Under review as a conference paper at ICLR 2021
Deep Networks from the Principle of Rate Reduction
Anonymous authors
Paper under double-blind review
Ab stract
This work attempts to interpret modern deep (convolutional) networks from the
principles of rate reduction and (shift) invariant classification. We show that the
basic iterative gradient ascent scheme for maximizing the rate reduction of learned
features naturally leads to a deep network, one iteration per layer. The architec-
tures, operators (linear or nonlinear), and parameters of the network are all explic-
itly constructed layer-by-layer in a forward propagation fashion. All components
of this “white box” network have precise optimization, statistical, and geomet-
ric interpretation. Our preliminary experiments indicate that such a network can
already learn a good discriminative deep representation without any back propa-
gation training. Moreover, all linear operators of the so-derived network naturally
become multi-channel convolutions when we enforce classification to be rigor-
ously shift-invariant. The derivation also indicates that such a convolutional net-
work is significantly more efficient to learn and construct in the spectral domain.
1	Introduction and Motivation
In recent years, various deep (convolution) network architectures such as AlexNet (Krizhevsky et al.,
2012), VGG (Simonyan & Zisserman, 2015), ResNet (He et al., 2016), DenseNet (Huang et al.,
2017), Recurrent CNN, LSTM (Hochreiter & Schmidhuber, 1997), Capsule Networks (Hinton et al.,
2011), etc., have demonstrated very good performance in classification tasks of real-world datasets
such as speeches or images. Nevertheless, almost all such networks are developed through years of
empirical trial and error, including both their architectures/operators and the ways they are to be ef-
fectively trained. Some recent practices even take to the extreme by searching for effective network
structures and training strategies through extensive random search techniques, such as Neural Archi-
tecture Search (Zoph & Le, 2017; Baker et al., 2017), AutoML (Hutter et al., 2019), and Learning
to Learn (Andrychowicz et al., 2016).
Despite tremendous empirical advances, there is still a lack of rigorous theoretical justification of
the need or reasons for “deep” network architectures and a lack of fundamental understanding of
the associated operators (e.g. multi-channel convolution and nonlinear activation) in each layer.
As a result, deep networks are often designed and trained heuristically and then used as a “black
box.” There have been a severe lack of guiding principles for each of the stages: For a given task,
how wide or deep the network should be? What are the roles and relationships among the multiple
(convolution) channels? Which parts of the networks need to be learned and trained and which
can be determined in advance? How to evaluate the optimality of the resulting network? As a
consequence, besides empirical evaluation, it is usually impossible to offer any rigorous guarantees
for certain performance of a trained network, such as invariance to transformation (Azulay & Weiss,
2018; Engstrom et al., 2017) or overfitting noisy or even arbitrary labels (Zhang et al., 2017).
In this paper, we do not intend to address all these questions but we would attempt to offer a plausible
interpretation of deep (convolution) neural networks by deriving a class of deep networks from first
principles. We contend that all key features and structures of modern deep (convolution) neural
networks can be naturally derived from optimizing a principled objective, namely the rate reduction
recently proposed by Yu et al. (2020), that seeks a compact discriminative (invariant) representation
of the data. More specifically, the basic iterative gradient ascent scheme for optimizing the objective
naturally takes the form of a deep neural network, one layer per iteration.
This principled approach brings a couple of nice surprises: First, architectures, operators, and param-
eters of the network can be constructed explicitly layer-by-layer in a forward propagation fashion,
and all inherit precise optimization, statistical and geometric interpretation. As result, the so con-
structed “white box” deep network already gives a good discriminative representation (and achieves
good classification performance) without any back propagation for training the deep network. Sec-
ond, in the case of seeking a representation rigorously invariant to shift or translation, the network
naturally lends itself to a multi-channel convolutional network. Moreover, the derivation indicates
such a convolutional network is computationally more efficient to learn and construct in the spectral
1
Under review as a conference paper at ICLR 2021
(Fourier) domain, analogous to how neurons in the visual cortex encode and transit information with
their spiking frequencies (Eliasmith & Anderson, 2003; Belitski et al., 2008).
2	Technical Approach
Consider a basic classification task: given a set of m samples X =. [x1, . . . , xm] ∈ Rn×m and
their associated memberships π(xi) ∈ [k] in k different classes, a deep network is typically used
to model a direct mapping from the input data x ∈ Rn to its class label f(x, θ) : x 7→ y ∈ Rk,
where y is typically a “one-hot” vector encoding the membership information π(x): the j-th entry
of y is 1 iff π(x) = j. The parameters θ of the network is typically learned to minimize certain
prediction loss, say the cross entropy loss, via gradient-descent type back propagation. Although
this popular approach provides people a direct and effective way to train a network that predicts the
class information, the so learned representation is however implicit and lacks clear interpretation.
2.1	Principle of Rate Reduction and Group Invariance
The Principle of Maximal Coding Rate Reduction. To help better understand features learned
in a deep network, the recent work of Yu et al. (2020) has argued that the goal of (deep) learning is
to learn a compact discriminative and diverse feature representation1 z = f (x) ∈ Rn of the data x
before any subsequent tasks such as classification: x -f-(-x→) z -h-(-z→) y. To be more precise, instead
of directly fitting the class label y, a principled objective is to learn a feature map f (x) : x 7→ z
which transforms the data x onto a set of most discriminative low-dimensional linear subspaces
{Sj }jk=1 ⊂ Rn, one subspace Sj per class j ∈ [k].
Let Z =. [z1, . . . , zm] = [f(x1), . . . , f (xm)] be the features of the given samples X. WLOG, we
may assume all features zi are normalized to be of unit norm: zi ∈ Sn-1. For convenience, let Πj ∈
Rm×m be a diagonal matrix whose diagonal entries encode the membership of samples/features
belong to the j-th class: Πj (i, i) = π(xi) = π(zi). Then based on principles from lossy data
compression (Ma et al., 2007), Yu et al. (2020) has suggested that the optimal representation Z? ⊂
Sn-1 should maximize the following coding rate reduction objective, known as the MCR2 principle:
Rate Reduction:	∆R(Z) = 2 log det(I + αZZ*
、	一_ 一
^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^-
k
--X Yj log det(I + αj ZΠjZ*), (1)
{z^^^
R(Z)
} j=1
、
where α = n/(me2), αj = n∕(tr(Πj)e2), Yj = tr(Πj)/m for j
{Z
Rc(Z,Π)
1, . . . , k. Given a prescribed
quantization error , the first term R of ∆R(Z) measures the total coding length for all the features
Z and the second term Rc is the sum of coding lengths for features in each of the k classes.
}
In Yu et al. (2020), the authors have shown the optimal representation Z? that maximizes the above
object indeed has desirable nice properties. Nevertheless, they adopted a conventional deep network
(e.g. the ResNet) as a black box to model and parameterize the feature mapping: z = f(x, θ). It
has empirically shown that with such a choice, one can effectively optimize the MCR2 objective and
obtain discriminative and diverse representations for classifying real image data sets.
However, there remain several unanswered problems. Although the resulting feature representation
is more interpretable, the network itself is still not. It is not clear why any chosen network is
able to optimize the desired MCR2 objective: Would there be any potential limitations? The good
empirical results (say with a ResNet) do not necessarily justify the particular choice in architectures
and operators of the network: Why is a layered model necessary, how wide and deep is adequate,
and is there any rigorous justification for the use of convolutions and nonlinear operators used? In
Section 2.2, we show that using gradient ascent to maximize the rate reduction ∆R(Z) naturally
leads to a “white box” deep network that represents such a mapping. All linear/nonlinear operators
and parameters of the network are explicitly constructed in a purely forward propagation fashion.
Group Invariant Rate Reduction. So far, we have considered the data and features as vectors. In
many applications, such as serial data or imagery data, the semantic meaning (labels) of the data and
their features are invariant to certain transformations g ∈ G (for some group G) (Cohen & Welling,
2016). For example, the meaning of an audio signal is invariant to shift in time; and the identity
of an object in an image is invariant to translation in the image plane. Hence, we prefer the feature
mapping f(x, θ) is rigorously invariant to such transformations:
1To simplify the presentation, we assume for now that the feature z and x have the same dimension n. But
in general they can be different as we will soon see, say in the case z is multi-channel extracted from x.
2
Under review as a conference paper at ICLR 2021
Group Invariance: f(x ◦ g, θ)〜f(x, θ), ∀g ∈ G,	(2)
where “〜” indicates two features belonging to the same equivalent class. The recent work of Zaheer
et al. (2017); Maron et al. (2020) characterize properties of networks and operators for set permu-
tation groups. Nevertheless, it remains challenging to learn features via a deep network that are
guaranteed to be invariant even to simple transformations such as translation and rotation (Azulay
& Weiss, 2018; Engstrom et al., 2017). In Section 2.3, we show that the MCR2 principle is com-
patible with invariance in a very natural and precise way: we only need to assign all transformed
versions {x ◦ g | g ∈ G} into the same class as x and map them all to the same subspace S.2 We
will rigorously show (in the Appendices) that, when the group G is (discrete) circular 1D shifting or
2D translation, the resulting deep network naturally becomes a multi-channel convolution network!
2.2	Deep Networks from Maximizing Rate Reduction
Gradient Ascent for Rate Reduction on the Training Samples. First let us directly try to max-
imize the objective ∆R(Z) as a function in the training samples Z ⊂ Sn-1. To this end, we may
adopt a (projected) gradient ascent scheme, for some step size η > 0:
∂∆R
Z'+ι Z z` + η ∙
∂Z
z`
subject to Z'+ι ⊂ Sn-1.
(3)
This scheme can be interpreted as how one should incrementally adjust locations of the current
features z` in order for the resulting Z'+ι to improve the rate reduction ∆R(Z). Simple calculation
shows that the gradient ∂∆R entails evaluating the following derivatives of the terms in (1):
1 ∂ logdet(I + αZZ,)
∂Z
1 ∂ (Yjlogdet(I + αjZΠjZ*))
∂Z
z`
z`
α(I + aZ'Z；)T z`	∈ Rn×m
'-------{----
e` ∈Rn×n
(4)
Yj aj(I + αjZ'ΠjZQT Z'Πj	∈ Rn×m
_____ ，
{z
Cj ∈Rn×n
(5)
Notice that in the above, the matrix e` only depends on z` hence it aims to expand all the features
to increase the overall coding rate; the matrix Cj depends on features from each class and aims Com-
press them to reduce the coding rate of each class. See Remark 1 in Appendix A for the geometric
and statistic meaning of e` and Cj. Then the complete gradient ∂∆R ∣ Zg
is of the following form:
∂∆R
^aZ^
z`
∣E} z` - EYj	Cj	Z'∏j	∈ Rn×m
Expansion j =1 Compression
(6)
2

2
|
Gradient-Guided Feature Map Increment. Notice that in the above, the gradient ascent consid-
ers all the features z` = [z，,...，zjm] as free variables. The increment Z'+ι — Zj = η∂∆R ∣ Zg does
not yet give a transform on the entire feature domain z` ∈ Rn . Hence, in order to find the optimal
f (x, θ) explicitly, we may consider constructing a small increment transform g(∙, θ`) on the '-th
layer feature z` to emulate the above (projected) gradient scheme:
Z'+1 Z z` + η ∙ g(z`, θ`) subject to Z'+1 ∈ Sn-1	(7)
such that: [g(z), θ`),..., g(zjm, θ`)] ≈ ∂∆R ∣z∕ That is, we need to approximate the gradient flow
∂∂∆R that locally deforms each (training) feature {zj}区 with a continuous mapping g(z) defined
on the entire feature space z` ∈ Rn . See Remark 2 in Appendix A for conceptual connection and
difference from the Neural ODE framework proposed by Chen et al. (2018).
By inspecting the structure of the gradient (6), it suggests that a natural candidate for the increment
transform g(z`, θ`) is of the form:
g(z`, θj) = EjZj - EYjCjz`nj(z`)	∈ Rn,	(8)
the probabilitjy=o1f
where πj (zj) ∈ [0, 1] indicates
zj belonging to the j-th class.3 Notice that the
increment depends on 1). A linear map represented by Ej that depends only on statistics of all
features from the preceding layer; 2). A set of linear maps {Cjj}jk=1 and memberships {πj(zj)}jk=1
of the features.
2Hence, any subsequent classifiers defined on the resulting set of subspaces will be automatically invariant
to such transformations.
3Notice that on the training samples z`, for which the memberships Πj are known, the so defined g(z`, θ)
gives exactly the values for the gradient d∂∆∆R ∣ Zg.
3
Under review as a conference paper at ICLR 2021
Since we only have the membership πj for the
training samples, the function g defined in (8) can
only be evaluated on the training samples. To ex-
trapolate the function g to the entire feature space,
we need to estimate πj (z`) in its second term. In
the conventional deep learning, this map is typi-
cally modeled as a deep network and learned from
the training data, say via back propagation. Nev-
ertheless, our goal here is not to learn a precise
classifier πj (z`) already. Instead, we only need a
good enough estimate of the class information in
order for g to approximate the gradient d⅛∆R well.
From the geometric interpretation of the linear
maps e` and C' given by Remark 1 in Appendix
Figure 1: Layer structure of the ReduNet: from
one iteration of gradient ascent for rate reduction.
A, the term p' = C' z` can be viewed as projection of z` onto the orthogonal complement of each
classj. Therefore, kpj` k2 is small ifz` is in class j and large otherwise. This motivates us to estimate
its membership based on the following softmax function: πb j (z`)
exp(一λ∣∣C' z`k)
Pk= exp(-λ∣∣Cjz'∣∣)
∈ [0, 1].
Hence the second term of (8) can be approximated by this estimated membership:4
kk
X YjC' z`n' (z`) ≈ X YjC' Z' ∙ ∏' (z`) = σ ([C1Z',...,CkZ'])	∈ Rn,	⑼
j=1	j=1
which is denoted as a nonlinear operator σ(∙) on outputs of the feature z` through k banks of filters:
[C'1 ,. . . , C'k]. Notice that the nonlinearality arises due to a “soft” assignment of class membership
based on the feature responses from those filters. Overall, combining (7), (8), and (9), the increment
feature transform from z' to z'+1 now becomes:
Z'+1 H z` + η ∙ E'Z' - η ∙ σ ([C1Z',...,CkZ']) subject to Z'+1 ∈ Sn-1,	(10)
with the nonlinear function σ(∙) defined above and θ` collecting all the layer-wise parameters in-
cluding E' , C'j, Yj and λ, and with features at each layer always “normalized” onto a sphere Sn-1,
denoted as PSn-1 . The form of increment in (10) can be illustrated by a diagram in Figure 1.
Deep Network from Rate Reduction. Notice that the increment is constructed to emulate the
gradient ascent for the rate reduction ∆R. Hence by transforming the features iteratively via the
above process, we expect the rate reduction to increase, as we will see in the experimental section.
This iterative process, once converged say after L iterations, gives the desired feature map f(x, θ)
on the input Z0 = x, precisely in the form of a deep network, in which each layer has the structure
shown in Figure 1:
f(x, θ) = ΦL ◦ ΦL-1 ◦•••◦ φ0(x), with φ'(Z', θ`) = Psn-1 [z` + η ∙ g(Z', θ`)]. (11)
As this deep network is derived from maximizing the rate reduction, we call it the ReduNet. Notice
that all parameters of the network are explicitly constructed layer by layer in a forward propagation
fashion. Once constructed, there is no need of any additional supervised learning, say via back
propagation. As suggested in Yu et al. (2020), the so learned features can be directly used for
classification via a nearest subspace classifier.
Comparison with Other Approaches and Architectures. Structural similarities between deep
networks and iterative optimization schemes, especially those for solving sparse coding, have been
long noticed. In particular, Gregor & LeCun (2010) has argued that algorithms for sparse cod-
ing, such as the FISTA algorithm (Beck & Teboulle, 2009), can be viewed as a deep network and be
trained for better coding performance, known as LISTA. Later Monga et al. (2019); Sun et al. (2020)
have proposed similar interpretation of deep networks as unrolling algorithms for sparse coding.
Like all networks that are inspired by unfolding certain iterative optimization schemes, the structure
of the ReduNet naturally contains a skip connection between adjacent layers as in the ResNet (He
4The choice of the softmax is mostly for its simplicity as it is widely used in other (forward components of)
deep networks for purposes such as selection, gating (Shazeer et al., 2017) and routing (Sabour et al., 2017). In
principle, this term can be approximated by other operators, say using ReLU that is more amenable to training
with back propagation, see Remark 3 in Appendix A.
4
Under review as a conference paper at ICLR 2021
et al., 2016). Remark 4 in Appendix A discusses possible improvement to the basic gradient scheme
that may introduce additional skip connections beyond adjacent layers. The remaining k+ 1 parallel
channels E , Cj of the ReduNet actual draw resemblance to the parallel structures that people later
found empirically beneficial for deep networks, e.g. ResNEXT (Xie et al., 2017) or the mixture
of experts (MoE) module adopted in Shazeer et al. (2017). But a major difference here is that all
components (layers, channels, and operators) of the ReduNet are by explicit construction from first
principles and they all have precise optimization, statistical and geometric interpretation. Further-
more, there is no need to learn them from back-propagation, although in principle one still could if
further fine-tuning of the network is needed (see Remark 3 of Appendix A for more discussions).
2.3	Deep Convolution Networks from Shift-Invariant Rate Reduction
We next examine ReduNet from the perspective of invariance to transformation. Using the basic and
important case of shift/translation invariance as an example, we will show that for data which are
compatible with an invariant classifier, the ReduNet construction automatically takes the form of a
(multi-channel) convolutional neural network, rather than heuristically imposed upon.
1D Serial Data and Shift Invariance. For one-dimensional data x ∈ Rn under shift symmetry,
we take G to be the group of circular shifts. Each observation xi generates a family {xi ◦g | g ∈ G}
of shifted copies, which are the columns of the circulant matrix circ(xi) ∈ Rn×n (see Appendix B.1
or Kra & Simanca (2012) for properties of circulant matrices).
What happens if we construct the ReduNet from these families Z1 = [circ(x1), . . . , circ(xm)]? The
data covariance matrix:
m
ZiZl = [circ(x1),..., CirC(Xm)] [circ(x1),..., CirC(Xm)] * = X CirC(Xi)CirC(Xi )* ∈ Rn×n
i=1
associated with this family of samples is automatically a (symmetric) circulant matrix. Moreover,
because the circulant property is preserved under sums, inverses, and products, the matrices E1
and C1j are also automatically circulant matrices, whose application to a feature vector z can be
implemented using cyclic convolution “~” (see Proposition B.1 of Appendix B):
Z2 H zi + η ∙ g(zi, θi) = zi + η ∙ ei ~ zi - η ∙ σ ([c1 ~ zi,..., Ck ~ zi]).	(12)
Because g(∙, θi) consists only of operations that co-vary with cyclic shifts, the features Z2 at the next
level again consist of families of shifts: Z2 = CirC(Xi+ηg(Xi, θi)), . . . , CirC(Xm +ηg(Xm, θm)) .
Continuing inductively, We see that all matrices e` and C' based on such z` are circulant. By virtue
of the properties of the data, ReduNet has taken the form of a convolutional network, with no need
to explicitly choose this structure!
The Role of Multiple Channels and Sparsity. There is one problem though: In general, the set of
all circular permutations of a vector z give a full-rank matrix. That is, the n “augmented” features
associated with each sample (hence each class) typically already span the entire space Rn . The
MCR2 objective (1) will not be able to distinguish classes as different subspaces.
One natural remedy is to improve the separability of the data by “lifting” the features to a higher
dimensional space, e.g., by taking their responses to multiple, filters ki, . . . , kC ∈ Rn:
z[c] = kc ~ z = CirC(kc)z ∈ Rn,	c = 1, . . . , C.	(13)
The filers can be pre-designed invariance-promoting filters,5 or adaptively learned from the data,6
or randomly selected as we do in our experiments. This operation lifts each original feature vector
Z ∈ Rn to a C-channel feature, denoted z = [z[1],..., z[C]]* ∈ RC×n. If we stack the multiple
channels of a feature Z as a column vector VeC(Z) ∈ RnC, the associated circulant version CirC(Z)
and its data covariance matrix, denoted as Σ, for all its shifted versions are given as:
circ(z[i])
circ(z) =	.	∈ RnC×n
.
circ(z[C])
circ(z[i])
∑ =	:	[ Circ(Z[1])*,…,circ(z[C])*] ∈ RnC×nC, (14)
.
circ(z[C])
5For 1D signals like audio, one may consider the conventional short time Fourier transform (STFT); for 2D
images, one may consider 2D wavelets as in the ScatteringNet (Bruna & Mallat, 2013).
6For learned filters, one can learn filters as the principal components of samples as in the PCANet (Chan
et al., 2015) or from convolution dictionary learning (Li & Bresler, 2019; Qu et al., 2019).
5
Under review as a conference paper at ICLR 2021
where Circ(Z[c]) ∈ Rn×n with C ∈ [C] is the CircUlant version of the c-th channel of the feature z.
Then the columns of circ(z) Will only span at most an n-dimensional proper subspace in RnC.
However, this operation does not yet render the classes separable - features associated with other
classes will span the same n-dimensional subspace. This reflects a fundamental conflict between
linear (subspace) modeling and invariance. One way of resolving this conflict is to leverage addi-
tional structure within each class, in the form of sparsity: signals within each class can be assumed
to be generated not as arbitrary linear combinations of basis vectors, but as sparse combinations of
atoms of different (incoherent) dictionaries Dj . Under this assumption, if the convolution kernels
{kc } match well the sparsifying dictionaries,7 the multi-channel responses should be sparse. Hence
we may take an entry-wise sparsity-promoting nonlinear thresholding, say T(∙), on the filter outputs
by setting small (say absolute value below ) or negative responses to be zero:8
Z = τ [circ(kι)z,..., circ(kc)z]	∈ Rn×C.	(15)
These features can be assumed to lie on a lower-dimensional (nonlinear) submanifold of Rn×C,
which can be linearized and separated from the other classes by subsequent ReduNet layers.
This multi-channel ReduNet retains the good invariance properties described above: all e` and C'
matrices are block circulant, and represent multi-channel 1D circular convolutions (see Proposition
B.2 of Appendix B for a rigorous statement and proof):
E(Z) = e ~ z, C' (z) = C ~ z ∈ Rn×C, j = i,...,k,	(16)
where e, C ∈ RC×C×n. Hence by construction, the resulting ReduNet is a deep convolutional
network for multi-channel 1D signals. Unlike Xception nets Chollet (2017), these multi-channel
convolutions in general are not depthwise separable.9
Fast Computation in the Spectral Domain. Since all circulant matrices can be simultaneously
diagonalized by the discrete Fourier transform matrix10 F: circ(z) = F*DF (see Fact 5 in Ap-
pendix B.1), all Σ of the form (14) can be converted to a standard “blocks of diagonals” form:
Σ
F * 0	0
0 ... 0
0	0 F*
Dii …Die -
..	..	..
.	..
Dei…Dcc」
F00
0 ... 0
00F
∈R
nC×nC
(17)
where each block Dkl is an n × n diagonal matrix. The middle of RHS of (17) is a block diagonal
matrix after a permutation of rows and columns. Hence, to compute E and C' ∈ RnC×nC, we only
have to compute in the frequency domain the inverse of C × C blocks for n times and the overall
complexity would be O(nC3) instead of O((nC)3) for inverting a generic nC × nC matrix.11
More details for implementing the network in the spectral domain can be found in Appendix B.3
(see Theorem B.3 for a rigorous statement and Algorithm 1 for implementation details).
Connections to Recurrent and Convolutional Sparse Coding. The sparse coding perspective of
Gregor & LeCun (2010) is later extended to recurrent and convolutional networks for serial data,
e.g. Wisdom et al. (2016); Papyan et al. (2016); Sulam et al. (2018); Monga et al. (2019). Although
both sparsity and convolution are advocated as desired characteristics for deep networks, they do
not explicitly justify the necessity of sparsity and convolutions from the objective of the network,
say classification. In our framework, we see how multi-channel convolutions (E, C'), different
nonlinear activations (πb j , τ), and the sparsity requirement are derived from, rather than heuristi-
cally proposed for, the objective of maximizing rate reduction of the features while enforcing shift
invariance.
2D Images and Translation Invariance. In the case of classifying images invariant to arbitrary
2D translation, we may view the image (feature) z ∈ R(W×H)×C as a function defined on a torus
T2 (discretized as a W × H grid) and consider G to be the (Abelian) group of all 2D (circular)
7There is a vast literature on how to learn the most compact and optimal sparsifying dictionaries from
sample data, e.g. (Li & Bresler, 2019; Qu et al., 2019). Nevertheless, in practice, often a sufficient number of
random filters suffice the purpose of ensuring features of different classes are separable (Chan et al., 2015).
8Here the nonlinear operator τ can be chosen to be a soft thresholding or a ReLU.
9It remains open what additional structures on the data would lead to depthwise separable convolutions.
10Here we scaled the matrix F to be unitary, hence it differs from the conventional DFT matrix by a 1 /√n.
11There are strong scientific evidences that neurons in the visual cortex encode and transmit information in
the rate of spiking, hence the so-called spiking neurons (Softky & Koch, 1993; Eliasmith & Anderson, 2003).
Nature might be exploiting the computational efficiency in the frequency domain for achieving shift invariance.
6
Under review as a conference paper at ICLR 2021
2.5
2.0 -
1.5 -
1.0∙
0.5 -
0.0 -
g
(a) Xtrain (2D)	(b) Ztrain (2D)
■ ∆R(tan) R R(tan) R Rc (tan)
∆R(ts) - R(ts)	R Rc (^∙sj
0 250 500 750 1C00125015817S)∞0
Layers
1.0
0.7
0.5
02
00
50500.eι.81
4.0
3.5∙
30
■ 2*
O 2.0∙
7 1.5∙
1.0∙
0.5∙
0.0∙
■ ∆R(⅛an) R R(Van)	∏ B (»»
—∆R(wJ ——- R(VS)	R Rc (test)
0 2S( S» 750 18012S>1S0>17S>20>0
Layers
(c)	Loss (2D)
(d)	Xtrain (3D)	(e) Ztrain (3D)	(f) Loss (3D)
Figure 2: Original samples and learned representations for 2D and 3D Mixture of Gaussians. We visualize
data points X (before mapping) and features Z (after mapping) by scatter plot. In each scatter plot, each color
represents one class of samples. We also show the plots for the progression of values of the objective functions.
translations on the torus. As We will show in the Appendix C, the associated linear operators E and
Cj 's act on the image feature Z as multi-channel 2D circular convolutions. The resulting network
will be a deep convolutional network that shares the same multi-channel convolution structures as
conventional CNNs for 2D images (LeCun et al., 1995; Krizhevsky et al., 2012). The difference
is that, again, the architectures and parameters of our network are derived from the rate reduction
objective, and so are the nonlinear activation πbj and τ. Again, our derivation in Appendix C shows
that this multi-channel 2D convolutional network can be constructed more efficiently in the spectral
domain (see Theorem C.1 of Appendix C for a rigorous statement and justification).
3	Experiments
We now verify whether the so constructed ReduNet achieves its design objectives through experi-
ments on synthetic data and real images. The datasets and experiments are chosen to clearly demon-
strate the behaviors of the network obtained by our algorithm, in terms of learning the correct dis-
criminative representation and truly achieving invariance. It is not the purpose of this work to push
the state of the art on any real datasets with highly engineered networks and systems, although we
believe this framework has this potential in the future. All code is implemented in Python mainly
using NumPy. All our experiments are conducted in a computer with 2.8 GHz Intel i7 CPU and
16GB of memory. Implementation details and more experiments and can be found in Appendix D.
Learning Mixture of Gaussians in S1 and S2. Consider a mixture of two Gaussian distributions
in R2 that is projected onto S1 . We first generate data points from these two distributions, X1 =
[x^1,..., χm] ∈ R2×m, Xi ~ N(μι, σιI), and ∏(x1) = 1; X2 = [x2,..., χm] ∈ R2×m, x∣ ~
N(μ2, σ2I), and ∏(x2) = 2. We set m = 500, σι = σ2 = 0.1 and μι, μ2 ∈ S1. Then we project
all the data points onto S1, i.e., xij /kxij k2 . To construct the network (computing E , C j for each
layer), we set the # of iterations/layers L = 2, 000,12 step size η = 0.5, and precision = 0.1.
As shown in Figure 2a-2b, we can observe that after the mapping f (∙, θ), samples from the same
class converge to a single cluster and the angle between two different clusters is approximately n/2,
which is well aligned with the optimal solution Z? of the MCR2 loss in S1. MCR2 loss of features
on different layers can be found in Figure 2c. Empirically, we find that our constructed network is
able to maximize MCR2 loss and converges stably. Similarly, we consider mixture of three Gaussian
distributions in R3 with means μι, μ2, μ3 uniformly in S2, and variance σι = σ2 = σ3 = 0.1, and
all data points are projected onto S2 (See Figure 2d-2f). We can observe similar behavior as in S2,
i.e., samples from the same class converge to one cluster and different clusters are orthogonal to
each other. Moreover, we sample new data points from the same distributions for both cases and
find that new samples form the same class consistently converge to the same cluster as the training
samples. More examples and details can be found in Appendix D.
Learning Shift Invariant Features. As described in § 2.3, by maximizing the rate reduction via
Eq. (12), we are able to explicitly construct operators that are invariant to (circular) shifts. To
verify the effectiveness of our proposed network on shift invariance tasks, we apply our network
to classify signals sampled from two different 1D functions. The underlying function of the first
class is sinusoidal signal h1(t) = sin(t) + , and the second class is a composition of sign and
Sin function, h2(t) = Sign(Sin(t)) + e, where e 〜 N(0,0.1). (See Figure 7 in Appendix D).
Each sample is generated by first picking t0 ∈ [0, 10π], then obtaining n equidistant point within
the boundaries [t0, t0 + 2π] with i.i.d Gaussian noise. Detailed implementations for sampling from
h1 and h2 can be found in Appendix D.3. We generate a dataset which contains m samples, with
m/2 samples in each class, i.e., X = [X1, X2] ∈ Rn×m. Then each sample is lifted to a C-
12It is remarkable to see how easily our framework leads to working deep networks with thousands of layers!
But this also indicates the efficiency of the layers is not so high. Remark 4 provides possible ways to improve.
7
Under review as a conference paper at ICLR 2021
(a) Xshift (1D)
3.0
2.5
2.0
1.5
1.0
0.5
0.0
×106
Similarity
(e) Xshift (MNIST)
(h) Similarity (MNIST)
(g) Loss (MNIST)
(f) Zshift(MNIST)

Figure 3: Heatmaps of cosine similarity between data Xshift/learned features Zshifb MCR2 loss, and distance
between shift samples and subspaces. For (a), (b), (e), (f), we pick one sample from each class and augment
the sample with its every possible shifted ones, then calculate the cosine similarity between these augmented
samples. For (d), (h), we first augment each samples in the dataset with its every possible shifted ones, then
we evaluate the cosine similarity (in absolute value) between pairs across classes: for each pair, one sample is
from training and one sample is from test which belong to different classes.
channel feature as defined in (13), i.e., X ∈ R(n∙C)×m. For training data, We set the number of
features n = 150, samples m = 400, channels C = 7, iterations/layers L = 2, 000, step size
η = 0.1, and precision = 0.1. We sample the same number of test data points followed by the
same procedure. As shown in Figure 8, we observe that the network can map the two classes of
signals to orthogonal subspaces both on training and test datasets. To verify invariance property
of the network, we first pick 5 signal samples from each class (from test dataset) and get their
corresponding augmented samples by shifting. Then we have m = 1, 500 augmented samples for
each original signal, Xshift ∈ R150×1,500, and we visualize the pairwise inner product of Xshift
and their representations, Zshift ∈ R(150Q×1,500 in Figure 3a-3b. Moreover, We augment every
sample (from the test dataset) with its all possible shifted versions and calculate the cosine similarity
between their representations and the representations of all the training samples from the other class
(in Figure 3d). We find that the proposed network can map different classes of signals (including all
shifted augmentations) to orthogonal subspaces, to increase the MCR2 loss (shown in Figure 3c).
Rotational Invariance on MNIST Digits. We study the ReduNet on learning rotation invariant
features on MNIST dataset (LeCun, 1998). We impose a polar grid on the image x ∈ RH×W, with
its geometric center being the center of the 2D polar grid. For each radius ri, i ∈ [C], we can sample
Γ pixels with respect to each angle Yl = l ∙ (2∏∕Γ) with l ∈ [Γ]. Then given an image sample X
from the dataset, we represent the image in a polar coordinate representation x(p) = (γl,i, rl,i) ∈
Rγ×c. Our goal is to learn rotation invariant features, i.e., we expect to learn f(∙, θ) such that
{f (x(p) ◦ g, θ)}g∈G lie in the same subspace, where g is the shift transformation in polar angle. By
performing polar coordinate transformation for images from digit ‘0’ and digit ‘1’ in the training
dataset, we can obtain the data matrix X(P) ∈ RTC) ×m. We use m = 2,000 training samples, set
Γ = 200 and C = 5 for polar transformation, and set iteration L = 3, 500, precision = 0.1, step-
size η = 0.5. We generate 1, 000 test samples followed by the same procedure. In Figure 10, we
can see that our proposed ReduNet is able to map most samples from different classes to orthogonal
subspaces (w.r.t. class) on test dataset. Meanwhile, in Figure 3e, 3f, and 3h, we observe that the
learnt features are invariant to shift transformation in polar angle (i.e., arbitrary rotation in x).
4	Conclusions and Future Work
This work offers an interpretation of deep (convolutional) networks by construction from first prin-
ciples. It provides a rigorous explanation for the deep architecture and components from the per-
spective of optimizing the rate reduction objective. Simulations and experiments on basic data sets
clearly verify the so-constructed ReduNet achieves the desired functionality and objective. Although
in this work the ReduNet is forward constructed, one may study how to effectively fine tune it via
back propagation. We believe rate reduction provides a principled framework for designing new
networks with interpretable architectures and operators that can scale up to real-world datasets and
problems, with better performance guarantees. This framework can also be naturally extended to
settings of online or unsupervised learning if Π is partially or not known and is to be optimized.
8
Under review as a conference paper at ICLR 2021
References
Mongi A Abidi, Andrei V Gribok, and Joonki Paik. Optimization Techniques in Computer Vision.
Springer, 2016.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981-3989, 2016.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? arXiv preprint arXiv:1805.12177, 2018.
Bowen Baker, Otkrist Gupta, N. Naik, and R. Raskar. Designing neural network architectures using
reinforcement learning. ArXiv, abs/1611.02167, 2017.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.
Andrei Belitski, Arthur Gretton, Cesare Magri, Yusuke Murayama, Marcelo A. Montemurro,
Nikos K. Logothetis, and Stefano Panzeri. Low-frequency local field potentials and spikes
in primary visual cortex convey independent visual information. Journal of Neuroscience, 28
(22):5696-5709, 2008. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.0009-08.2008. URL
https://www.jneurosci.org/content/28/22/5696.
Joan Bruna and StePhane Mallat. Invariant scattering convolution networks. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 35(8):1872-1886, 2013.
Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, and Yi Ma. PCANet: A simple
deep learning baseline for image classification? IEEE transactions on image processing, 24(12):
5017-5032, 2015.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differ-
ential equations. In Advances in Neural Information Processing Systems, pp. 6571-6583, 2018.
Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1800-1807, 2017.
Taco S. Cohen and Max Welling. Group equivariant convolutional networks.	CoRR,
abs/1602.07576, 2016. URL http://arxiv.org/abs/1602.07576.
Dheeru Dua and Casey Graff. Uci machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Chris Eliasmith and Charles Anderson. Neural Engineering: Computation, Representation and
Dynamics in Neurobiological Systems. Cambridge, MA, 01 2003.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation suffice: Fooling CNNs with simple transformations. arXiv preprint
arXiv:1712.02779, 2017.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the
27th International Conference on International Conference on Machine Learning, pp. 399-406,
2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Geoffrey E. Hinton, A. Krizhevsky, and S. Wang. Transforming auto-encoders. In ICANN, 2011.
Sepp Hochreiter and JUrgen SChmidhuber. Long short-term memory. Neural computation, 9:1735-
80, 12 1997. doi: 10.1162/neco.1997.9.8.1735.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2261-2269, 2017.
9
Under review as a conference paper at ICLR 2021
Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (eds.). Automatic Machine Learning: Methods,
Systems, Challenges. Springer, 2019.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. In Sebastien Bubeck, Vianney Perchet, and PhiliPPe Rigollet
(eds.), Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018, vol-
ume 75 of Proceedings of Machine Learning Research, pp. 1042-1085. PMLR, 2018. URL
http://proceedings.mlr.press/v75/jin18a.html.
Irwin Kra and Santiago R Simanca. On circulant matrices. Notices of the American Mathematical
Society, 59:368-377, 2012.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4013-4021, 2016.
Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yann LeCun, L.D. Jackel, Leon Bottou, Corinna Cortes, J. S. Denker, Harris Drucker, I. Guyon, U.A.
Muller, Eduard Sackinger, Patrice Simard, and V. Vapnik. Learning algorithms for classification:
A comparison on handwritten digit recognition, pp. 261-276. World Scientific, 1995.
Yanjun Li and Yoram Bresler. Multichannel sparse blind deconvolution on the sphere. IEEE Trans-
actions on Information Theory, 65(11):7415-7436, 2019.
Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data
via lossy data coding and compression. IEEE transactions on pattern analysis and machine
intelligence, 29(9):1546-1562, 2007.
Haggai Maron, O. Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
ArXiv, abs/2002.08599, 2020.
Michael Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks through
ffts, 2013.
Vishal Monga, Yuelong Li, and Yonina C Eldar. Algorithm unrolling: Interpretable, efficient deep
learning for signal and image processing. arXiv preprint arXiv:1912.10557, 2019.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of conver-
gence o (1∕k^ 2). In DokladyAN USSR, volume 269, pp. 543-547,1983.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via
convolutional sparse coding. Journal of Machine Learning Research, 18, 07 2016.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.
Qing Qu, Xiao Li, and Zhihui Zhu. A nonconvex approach for exact and efficient multichannel
sparse blind deconvolution. In Advances in Neural Information Processing Systems, pp. 4017-
4028, 2019.
Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Dynamic routing between capsules. CoRR,
abs/1710.09829, 2017. URL http://arxiv.org/abs/1710.09829.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. arXiv
preprint arXiv:1805.10408, 2018.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In
ICLR, 2017. URL https://openreview.net/pdf?id=B1ckMDqlg.
10
Under review as a conference paper at ICLR 2021
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
William R Softky and Christof Koch. The highly irregular firing of cortical cells is inconsistent with
temporal integration of random EPSPs. Journal OfNeuroscience, 13(1):334-350, 1993.
Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multilayer convolutional sparse
modeling: Pursuit and dictionary learning. IEEE Transactions on Signal Processing, 66(15):
4090-4104, 2018.
Xiaoxia Sun, Nasser M Nasrabadi, and Trac D Tran. Supervised deep sparse coding networks for
image classification. IEEE Transactions on Image Processing, 29:405-418, 2020.
Nicolas Vasilache, J. Johnson, Michael Mathieu, SoUmith Chintala, Serkan Piantino, and Y. LeCun.
Fast convolutional nets with fbfft: A gpu performance evaluation. CoRR, abs/1412.7580, 2015.
Scott Wisdom, Thomas Powers, James Pitton, and Les Atlas. Interpretable recurrent neural networks
using sequential sparse recovery. ArXiv, abs/1611.07252, 2016.
Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 5987-5995, 2017.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance
trade-off for generalization of neural networks. In International Conference on Machine Learning
(ICML), 2020.
Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and
discriminative representations via the principle of maximal coding rate reduction. arXiv preprint
arXiv:2006.08558, 2020.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 3391-3401. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
6931-deep-sets.pdf.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. 2017.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. 2017. URL
https://arxiv.org/abs/1611.01578.
11
Under review as a conference paper at ICLR 2021
A Additional Remarks and Extensions
Remark 1 (Interpretation of the Two Linear Operators) For any z` we have
(I + αZ'Z')-1 z` =	z` -	Z'q'	where	q'	= argminɑkz`	-	Z'q'∣2 +	∣∣q'∣2.	(18)
q'
Notice that q' is exactly the solution to the ridge regression by all the data points z` concerned.
Therefore, e` (Similarlyfor C') is approximately(i.e. when m is large enough) the projection onto
the orthogonal complement of the subspace SPanned by columns of z`. Another way to interpret
the matrix e` is through eigenvalue decomposition of the covariance matrix Z'Z'. Assuming that
Z'Z' = U'Λ'U' where a` = diag{σι,..., σd}, we have
e` = α U' diag[ —1—,…,	1— ∖u^	(19)
1 + ασ1	1 + ασd
Therefore, the matrix E' operates on a vector z` by stretching in a way that directions of large
variance are shrunk while directions of vanishing variance are kept. These are exactly the directions
(4) in which we move the features so that the overall volume expands and the coding rate will
increase, hence the positive sign. To the opposite effect, the directions associated with (5) are
exactly “residuals” of features of each class deviate from the subspace to which they are supposed
to belong. These are exactly the directions in which the features need to be compressed back onto
their respective subspace, hence the negative sign.
Essentially, all linear operations in the ReduNet are determined by data conducting “auto-
regressions” among themselves. The recent renewed understanding about ridge regression in an
over-parameterized setting (Yang et al., 2020) indicates that using seemingly redundantly sampled
data (from each subspaces) as regressors do not lead to overfitting.
Remark 2 (Connection and Difference from Neural ODE) Notice that one may interpret the in-
crement (7) as a discretized version of a continuous ordinary differential equation (ODE):
Z = g(z,θ).	(20)
Hence the (deep) network so constructed can be interpreted as certain neural ODE (Chen et al.,
2018). Nevertheless, unlike neural ODE where the flow g is chosen to be some generic structures,
here our g(z, θ) is to emulate the gradient flow of the rate reduction on the feature set
.
Z
∂∆R
∂Z ,
(21)
and its structure is entirely derived and fully determined from this objective, without any other priors
or heuristics.
Remark 3 (Approximate with a ReLU Network) In practice, there are many other simpler non-
linear activation functions that one can use to approximate the membership πb (∙) and subsequently
the nonlinear operation σ in (9). Notice that the geometric meaning of σ in (9) is to compute the
“residual” of each feature against the subspace to which it belongs. So when we restrict all our fea-
tures to be in the first (positive) quadrant of the feature space,13 one may approximate this residual
using the rectified linear units operation, ReLUs, on Pj = Cjz` or its orthogonal complement:
k
σ(z') Y z` - XReLU(Pjz`),	(22)
j=1
where P' = (Cj )⊥ is the projection onto the j-th class14 and ReLU(x) = max(0,x). The above
approximation is good under the more restrictive assumption that projection of z` on the correct
class via P' is mostly large and positive and yet small or negativefor other classes.
The resulting ReduNet will be a network primarily involving ReLU operations and feature normal-
ization (onto Sn-1) between each layer. Although in this work, we have argued that the forward-
constructed ReduNet network already works to a large extent, in practice one certainly can conduct
back-propagation to further fine tune the so-obtained network, say to correct some remaining errors
in predicting labels of the training data. Empirically, people have found that deep networks with
ReLU activations are easier to train via back propagation (Krizhevsky et al., 2012).
13Most current neural networks seem to adopt this regime.
14Pj can be viewed as the orthogonal complement to Cj.
12
Under review as a conference paper at ICLR 2021
Remark 4 (Accelerated Optimization via Additional Skip Connections) Empirically, people
have found that additional skip connections across multiple layers may improve the network
performance, e.g. the DenseNet (Huang et al., 2017). In our framework, the role of each layer is
precisely interpreted as one iterative gradient ascent step for the objective function ∆R. In our
experiments (see Section 3), we have observed that the basic gradient scheme sometimes converges
slowly, resulting in deep networks with thousands of layers (iterations)! To improve the efficiency
of the basic ReduNet, one may consider in the future accelerated gradient methods such as the
Nesterov acceleration (Nesterov, 1983) or perturbed accelerated gradient descent (Jin et al., 2018).
Say to minimize or maximize a function h(z), such accelerated methods usually take the form:
P P'+1	= z` + β' ∙ (z` - z`-i),
I Z'+1 = P'+1 + η ∙ Vh(p'+ι).
(23)
Hence they require introducing additional skip Connections among three layers '—1, ' and '+1. For
typical convex or nonconvex programs, the above accelerated schemes can often reduce the number
of iterations by a magnitude.
13
Under review as a conference paper at ICLR 2021
B 1D Circular Shift Invariance
It has been long known that to implement a convolutional neural network, one can achieve higher
computational efficiency by implementing the network in the spectral domain via the fast Fourier
transform (Mathieu et al., 2013; Lavin & Gray, 2016; Vasilache et al., 2015). However, our purpose
here is different: We want to show that the linear operators E and Cj derived from the gradient flow
of MCR2 are naturally convolutions when we enforce shift-invariance rigorously. Their convolution
structure is derived from the rate reduction objective, rather than heuristically imposed upon the net-
work. Furthermore, the computation involved in constructing these linear operators has a naturally
efficient implementation in the spectral domain via fast Fourier transform. Arguably this work is
the first to show multi-channel convolutions, together with other convolution-preserving nonlinear
operations in the ReduNet, are both necessary and sufficient to ensure shift invariance.
To be somewhat self-contained and self-consistent, in this section, we first introduce our notation
and review some of the key properties of circulant matrices which will be used to characterize the
properties of the linear operators E and Cj and to compute them efficiently. The reader may refer
to Kra & Simanca (2012) for a more rigorous exposition on circulant matrices.
B.1 Properties of Circulant Matrix and Circular Convolution
Given a vector Z = [zo, zι,..., zn-ι]* ∈ Rn,15 We may arrange all its circular shifted versions in a
circulant matrix form as
	z0 z1	zn-1 z0	... zn-1	z2 • ∙ ∙	z1 z2
circ(Z)	=.	. . .	z1	z0	. . .	. . .
	Zn-2	. . .	. . .	. . .	zn-1
	zn-1	zn-2	...	z1	z0
∈R
n×n
(24)
Fact 1 (Convolution as matrix multiplication via circulant matrix) The multiplication of a cir-
culant matrix circ(z) with a vector x ∈ Rn gives a circular (or cyclic) convolution, i.e.,
Circ(Z) ∙ X = z ~ x,	(25)
where
n-1
(Z ~ x)i =
xj zi+n-j mod n .	(26)
j=0
Fact 2 (Properties of circulant matrices) Circulant matrices have the following properties:
•	Transpose of a circulant matrix, say circ(z)*, is Circulant;
•	Multiplication oftwo circulant matrices is circulant, for example circ(z)circ(z)*;
•	For a non-singular circulant matrix, its inverse is also circulant (hence representing a
circular convolution).
These properties of circulant matrices are extensively used in this Work as for characterizing the
convolution structures of the operators E and Cj .
Given a set of vectors [Z1, . . . , Zm] ∈ Rn×m, let circ(Zi) ∈ Rn×n be the circulant matrix for Zi.
Then We have the folloWing:
Proposition B.1 (Convolution structures of E and Cj) Given a set of vectors Z = [Z1, . . . , Zm],
the matrix:
m
E = α(l + a X circ(zi)circ(zi)*) 1
i=1
is a circulant matrix and represents a circular convolution:
EZ = e ~ Z,
where e is the first column vector of E. Similarly, the matrices Cj associated with any subsets of Z
are also circular convolutions.
15We use superscript * to indicate (conjugate) transpose of a vector or a matrix
14
Under review as a conference paper at ICLR 2021
B.2 Circulant Matrix and Circulant Convolution for Multi-channel Signals
In the remainder of this section, we view z as a 1D signal such as an audio signal. Since we will deal
with the more general case of multi-channel signals, we will use the traditional notation T to denote
the temporal length of the signal and C for the number of channels. Conceptually, the “dimension”
n of such a multi-channel signal, if viewed as a vector, should be n = CT .16 As we will also reveal
additional interesting structures of the operators E and Cj in the spectral domain, we use t as the
index for time, p for the index of frequency, and c for the index of channel.
Given a multi-channel 1D signal Z ∈ RC×T, We denote
^z[1]*^
Z=	.	= [Z(0), Z(1),..., Z(T - 1)]= {z[c](t)}C=Ctt=0T-1.	(27)
Z[C]*
To compute the coding rate reduction for a collection of such multi-channel 1D signals, We may
flatten the matrix representation into a vector representation by stacking the multiple channels of Z
as a column vector. In particular, We let
VeC(Z) = [Z[1](0), Z[1](1),..., Z[1](T - 1), Z[2](0),...]	∈ R(CXT).	(28)
Furthermore, to obtain shift invariance for the coding rate reduction, We may generate a collection
of shifted copies of Z (along the temporal dimension). Stacking the vector representations for such
shifted copies as column vectors, We obtain
-CirC(Z国厂
CirC(Z) =I :	I ∈ R(CXT)×T.	(29)
CirC(Z[C])
In above, we overload the notation "drC(∙)“ defined in (24).
We now consider a collection of m multi-channel 1D signals {Zi ∈ Rc×t}mti： Compactly repre-
senting the data by Z ∈ RC×T×m in which the i-th slice on the last dimension is Zi, we denote
Z[c] = [Z1[c],..., Zm[c]] ∈ RT ×m,	Z(t) = [Z1(t),..., Zm(t)] ∈ RC×m.	(30)
In addition, we denote
VeC(Z) = [veC(Z1),..., VeC(Zm)] ∈ R(CXT)×m,
CirC(Z) = [CirC(Z1),..., CirC(Zm)] ∈ R(CXT)×(T×m).
Then, we define the shift invariant coding rate reduction for Z ∈ RCXT Xm as
(31)
△Rcirc(Z, Π) = T∆R(CirC(Z),Π)
2T log det
I + αj ∙ CirC(Z) ∙ Πj ∙ CirC(Z)*
I + α ∙ CirC(Z)∙ CirC(Z)*) - X 2T log det
j =1	(32)
where α = mCT2 = mC2, αj = tr(iCT⅛2 = tT(T⅛2, Yj =	, and πj is augmented member-
ship matrix in an obvious way. Note that we introduce the normalization factor T in (32) because
the circulant matrix CirC(Z) contains T (shifted) copies of each signal：
By applying (4) and (5), we obtain the derivative of ∆Rcirc(Z, Π) as
1 ∂ log det(I + αdrC(Z)drC(Z)*
2T	∂veC(Z)
1 ∂logdet(I + αdrC(Z)CirC(Z)*) ∂CirC(Z)
2T	∂CirC(Z)	∂ VeC(Z)
a(I + αdrC(Z)drC(Z)*)	VeC(Z),
(33)
E ∈R(C×T)×(C×T)
16Notice that in the main paper, for simplicity, we have used n to indicate both the 1D “temporal” or 2D
“spatial” dimension of a signal, just to be consistent with the vector case, which corresponds to T here. All
notation should be clear within the context.
15
Under review as a conference paper at ICLR 2021
Y. ∂ log det(I + αjCirc(Z)ΠjCirc(Z)")
2T	∂vec(Z)
Yjaj(I + aj circ(Z)Πjcirc(Z)*)	Vec(Z)Πj.
---------------{z---------------}
Cj ∈R(C×T)×(C×T)
(34)
In the following, We show that E ∙ Vec(Z) represents a multi-channel circular convolution. Note that
I+α Pm=I Circ(Zi[1])Circ(Zi[1])* …	Pm=I Circ(Zi[1])Circ(Zi[C])*
-1
E = a
(35)
Pm=I circ(zi[C])circ(zi[1])*	…I+Pmm=I αcirc(zi[C])circ(zi[C])* _
By using Fact 2, the matrix in the inverse above is a block circulant matrix, i.e., a block matrix where
each block is a circulant matrix. A useful fact about the inverse of such a matrix is the following.
Fact 3 (Inverse of block circulant matrices) The inverse of a block circulant matrix is a block cir-
culant matrix (with respect to the same block partition).
The main result of this subsection is the following.
Proposition B.2 (Convolution structures of E and Cj) Given a collection of multi-channel 1D
signals {Zi ∈ RC×T}m=ι, the matrix E is a block circulant matrix, i.e.,
E1,1 …Eι,c
E=	.	...	.	,	(36)
_Ec,i …Ec,C_
where each E" ∈ RT×T is a circulant matrix. Moreover, E represents a multi-channel circular
convolution, i.e.,for any multi-channel signal Z ∈ RC × T we have
E ∙ Vec(Z) = vec(e ~ Z).
In above, e ∈ RC×C×T is a multi-channel convolutional kernel with e[c, c0] ∈ RT being the first
column vector of Ec^, and e ~ Z ∈ RC×T is the multi-channel circular convolution (with “~''
overloading the notation from Eq. (26)) defined as
C
(e~ z)[c] = X e[c,c0] ~ z[c0],	∀c = 1,...,C.	(37)
c0=1
Similarly, the matrices Cj associated with any subsets of Z are also multi-channel circular convo-
lutions.
Note that the calculation of E in (35) requires inverting a matrix of size (C X T) X (C X T). In the
following, we show that this computation can be accelerated by working in the frequency domain.
B.3 Fast Computation in Spectral Domain
Circulant matrix and Discrete Fourier Transform. A remarkable property of circulant matrices
is that they all share the same set of eigenvectors that form a unitary matrix. We define the matrix:
.=
FT
ω
0T0T 0T0
0T1T
0 •	31	0 ωT	
T-2	T-1	
•	ωT . . ..	ωT . . .	∈ CT×t
(T-2)2 •	ωT	(T -2)(T -1) ωT	
(T -2)(T -1)	(T -1)2	
• ωT	ωT	
(38)
where ωT = exp(- 2πT-) is the roots of unit (as ωTT = 1). The matrix FT is a unitary matrix:
Ft FT = I and is the well known Vandermonde matrix. Multiplying a vector with Ft is known as
the discrete Fourier transform (DFT). Be aware that the conventional DFT matrix differs from our
definition of FT here by a scale: it does not have the 力 in front. Here for simplicity, we scale it so
that FT is a unitary matrix and its inverse is simply its conjugate transpose FT, columns of which
represent the eigenvectors ofa circulant matrix (Abidi et al., 2016).
16
Under review as a conference paper at ICLR 2021
Fact 4 (DFT as matrix-vector multiplication) The DFT of a vector z ∈ RT can be computed as
where
DFT(Z) = Ft ∙ Z ∈ Ct,
1 T-1
DFT(Z)(P) = √≡EZ⑴∙ωTt, ∀p = 0,1,...,T- 1.
T t=0
The Inverse Discrete Fourier Transform (IDFT) of a signal v ∈ CT can be computed as
IDFT(V) = FT ∙ v ∈ CT
where
1 T-1
IDFT(V)⑴=—=.E V(P) ∙ ωTpt, Vt = 0, 1,...,T - 1.
T p=0
(39)
(40)
(41)
(42)
Regarding the relationship between a circulant matrix (convolution) and discrete Fourier transform,
we have:
Fact 5 An n × n matrix M ∈ Cn×n is a circulant matrix if and only if it is diagonalizable by the
unitary matrix Fn :
FnMFn = D or M = FnDFn,	(43)
where D is a diagonal matrix of eigenvalues.
Fact 6 (DFT are eigenvalues of the circulant matrix) Given a vector Z ∈ CT, we have
FT ∙ Circ(Z) ∙ FT = diag(DFT(Z)) or Circ(Z) = FT ∙ diag(DFT(Z)) ∙ FT.	(44)
That is, the eigenvalues of the circulant matrix associated with a vector are given by its DFT.
Fact 7 (Parseval’s theorem) Given any Z ∈ CT, we have kZk2 = kDFT(Z)k2. More precisely,
T-1	T-1
X |Z[t]|2 = X |DFT(Z)[P]|2.	(45)
t=0	p=0
This property allows us to easily “normalize” features after each layer onto the sphere Sn-1 directly
in the spectral domain (see Eq. (10) and (62)).
Circulant matrix and Discrete Fourier Transform for multi-channel signals. We now consider
multi-channel 1D signals Z ∈ Rc×t. Let DFT(Z) ∈ Cc×t be a matrix where the c-th row is the
DFT of the corresponding signal Z [c], i.e.,
-DFT(Z[1])*-
DFT(Z) =	.	∈ Cc×t.	(46)
DFT(z[C ])*
Similar to the notation in (27), we denote
-DFT(Z)[1]*
DFT(Z)=	.
.
_DFT(Z)[C]*
[DFT(z)(0), DFT(Z)(1),..., DFT(z)(T - 1)]
={DFT(Z)[c](t)}C=Ct=τ T
(47)
As such, we have DFT(z[c]) = DFT(Z)[c].
By using Fact 6, circ(Z) and DFT(Z) are related as follows:
-FT ∙ diag(DFT(Z[1])) ∙ FTl	「FT …	0 "
circ(Z)=	.	=.....
FT ∙ diag(DFT(Z[C])) ∙ FT	0	…	FT
diag(DFT(Z[1]))
.	∙ FT .(48)
.
diag(DFT(Z[C]))
17
Under review as a conference paper at ICLR 2021
We now explain how this relationship can be leveraged to produce a fast computation of E defined
in (33). First, there exists a permutation matrix P such that
「diag(DFT(z[1]))]
diag(DFT(z[2]))
-DFT(Z)(0)
0	…	0
DFT(Z)(I)…	0
(49)
diag(DFT(z[C]))
0	…DFT(Z)(T - 1)
P ∙
0
0
Combining (48) and (49), we have
FT …	0
Circ(Z) ∙ circ(z)* =	.	...	.
0	… FT
where
-DFT(Z)(0) ∙ DFT(Z)(0)* …
D(Z)=	.	".
0	…
FT	.…0
. P ∙ D(Z) ∙ P* ......
0	…	FT
DFT(Z)(T — 1) ∙ DFT(Z)(T — 1)*
(50)
(51)
0
Now, consider a collection of m multi-channel 1D signals Z ∈ Rc×t×m. Similar to the notation in
(30), we denote
DFT(Z)[c] = [DFT(Z1)[c],..., DFT(Zm)[c]] ∈ RT ×m,
DFT(Z)(P) = [DFT(Z1)(p),...,DFT(Zm)(p)] ∈ RC×m.
By using (50), we have
(52)
FT*
E =	.
.
0
0
.
.
.
FT*
m
• P ∙ α ∙ I + α ∙ XD(Zi)
i=1
-1
• P*
FT
FT
(53)
0
0
Note that a • [I + a • Pm=I D(Zi)] 1 is equal to
一 .——... .——√.........
I+αDFT(Z)(0)∙DFT(Zi)(0)* …	0
α	..	..	..
..	.
0	… I+αDFT(Z)(T-1)∙DFT(Z)(T-1)*
^α(l+aDFT(Z)(0)∙DFT(Z)(0)*)-1 …	0
(54)
0	…α(l+aDFT(Z)(T -1)，DFT(Z)(T-1)*)-1
Therefore, the calculation of E only requires inverting T matrices of size C X C. This motivates US
to construct the ReduNet in the spectral domain for the purpose of accelerating the computation, as
we explain next.
Shift-invariant ReduNet in the Spectral Domain. Motivated by the result in (54), we introduce
the notations E(P) ∈ Rc×c×t and Cj(P) ∈ rc×c×t given by
E(P) = α • [I + α • DFT(Z)(p) . DFT(Z)(p)*]-1	∈ CC×C,	(55)
Cj(p) = αj • [I + αj∙ DFT(Z)(P) • Πj . DFT(Z)(P)*]-1 ∈ CC×C.	(56)
In above, E(P) (resp., Cj(P)) is the P-th slice of E (resp., Cj) on the last dimension. Then, the
gradient of ∆Rdrc(Z, Π) with respect to Z can be calculated by the following result.
Theorem B.3 (Computing multi-channel convolutions E and Cj) Let U ∈ Cc×t×m and
Wj ∈ Cc×t×m,j = 1,...,k be given by
U(P) = E(P) • DFT(Z)(p),	(57)
Wj(p) = Cj(P) • DFT(Z)(p), j = 1,...,k,	(58)
18
Under review as a conference paper at ICLR 2021
for each p ∈ {0, . . . , T - 1}. Then, we have
1 ∂logdet(I + α ∙ Circ(Z)Circ(Z)")
2T	dZ_ _ ɪ
Yj ∂ log det(I + αj ∙ circ(Z)Πjcirc(Z)")
2T	∂Z
IDFT(U),
Yj ∙IDFT(WjΠj).
(59)
(60)
By this result, the gradient ascent update in (3) (when applied_to ∆Rcirc(Z, Π)) can be equivalently
expressed as an update in frequency domain on V = DFT(Z`) as
k
V⅞+1 (P)	Y 忆(P) + η(£'(P)	∙ Ve(P)- X Yj C (P)	∙	VKpRj)	P =	0,...,t - 1.	(61)
j=1
Similarly, the gradient-guided feature map increment in (10) can be equivalently expressed as an
update in frequency domain on v` = DFT(Z') as
v`+I(P) B v'(P)+η ∙E' (P)Ve(P)-η ∙ σ([C1(P)v'(P),... ,ck (P)Ve(P)]), P = 0,...,t - 1, (62)
SUbjeCttotheConStraintthatkv'+ι ∣∣f = kZ'+ι∣∣F = 1 (the first equality follows from Fact 7).
We summarize the training, or construction to be more precise, of ReduNet in the spectral domain
in Algorithm 1.
Proof: [Proof to Theorem (B.3)]
From (4), (53) and (48), we have
1 ∂ log det(I + αcirc(Z)circ(Z)*
2	∂circ(zi)
E circ(zi) = E
-	FT …0 -
... ..
.....
L 0…FT」
diag(DFT(zi [1]))
.
.
.
diag(DFT(zi [C]))
-	FT …0 -I	r	IT
....∙ P ∙ α ∙ I + α ∙)：D(Zi)
0 …f"	L	i	_
-	FT …	0 -i	「W(0)∙DFT(zi)(0)
....∙P∙	.
.	..	.
L 0…FT」	0
DFT(Zi)(0)…	0	.
..	..	..
.	..
0	… DFT(Zi)(T-1)
0	-
.	∙ FT
.
E(T-1)∙DFT(Zi)(T-1)
-	FT …0 -i	Γ ui(0)
....∙ P •.
.	..	.
L 0…可」	0
0	「FT …0 -
.	∙ Ft =....
.	.	..
Ui(T-1) _|	L 0 …FT _
diag(Ui[1])
.	∙ FT
.
diag(Ui[C]) _
circ(IDFT(Ui)).
FT
(63)
(64)
(65)
(66)
(67)
Therefore, we have
1	∂ log det(I + α ∙ circ(Z) ∙ CirC(Z)*)	] ∂ log det(I + α ∙ circ(Z) ∙ CirC(Z)*) ∂circ(zi)
-------------------:--:-------------------------------------:------:---------------- ----:--:--
2	∂Zi	2	∂CirC(Zi)	∂ Zi
T ∙IDFT(Ui).
(68)
By collecting the results for all i, we have
∂ 2T log det I +
α ∙ CirC(Z) ∙ CirC(Z)*
∂Z
IDFT(U).
(69)


In a similar fashion, we get


∂2γT log det I + αj ∙ CirC(Z) ∙ Πj ∙ CirC(Z)*
∂ Z
γj∙ ∙IDFT(Wj ∙ Πj).
(70)

19
Under review as a conference paper at ICLR 2021
Algorithm 1 Training Algorithm (1D Signal, Shift Invariance, Spectral Domain)
Input: Z ∈ RCXT×m, Π, e > 0, λ, and a learning rate η.
1: Set α =品Naj = tr(IC W }k=1' {γj =吗口}k=1∙
2: Set % = {V0(P) ∈ Cc}T:0* 1 * * *,,imι = DFT(Z) ∈ Cc×t×m.
3: for ' = 1, 2,..., L do
4:	# Step 1: Compute E and C.
5:	for p = 0,1,..., T — 1 do
6:	Compute E'(p) ∈ Cc×c and {Cj(p) ∈ Cc×c}k=1 as
.	—1
E'(p) = a ∙ [I + α ∙ ^½-i(p) ∙ ^½-i(p)],
Cj(p) = αj ∙ [I + αj ∙ ½-i(p) ∙ Πj ∙ ½-i(p)*]-1;
7: end for
8:	# Step 2: Update Vrl for each i.
9: for i = 1,..., m do
10:	# Compute projection at eachfrequency P.
11:	for p = 0,1,..., T — 1 do
12:	Compute {p'j(p) = Cj(p) ∙ v`(p) ∈ Cc×1}k=1;
13:	end for
14:	# Compute overall projection by aggregating OVerfreqUenCy P.
15:	Let {P 'j = [pj (0),...,赍(T — 1)] ∈ Cc×τ}k=ι;
16:	# Compute soft assignment from projection.
17:	COmPUte {πj=P=pexpjι⅞就F)}：；
18:	# Compute update at eachfrequency P.
19:	for p = 0, 1, . . . , T — 1 do
20:	v`(P) = VLI(P) + η (Ee(P)v`(p) — Pj=I Yj ∙n∙ Cj (P) ∙ v`(p));
21:	end for
22:	v` = V' /IE";
23:	end for
24:	Set Z' = IDFT(Vj) as the feature at the '-th layer;
25:	# Evaluate the objective value.
26:	务 PT=CI (logdet[I + αV(p) ∙ V(P)*] — tr(ɪ^ logdet[I + % V(p) ∙ Πj ∙ V(P)*]
27: end for
Output: features Zl, the learned filters {&(p)}g,p and {Cj (p)}j,`,p.
20
Under review as a conference paper at ICLR 2021
∈ RHXW,	(71)
C 2D Circular Translation Invariance
To a large degree, both conceptually and technically, the 2D case is very similar to the 1D case that
we have studied carefully in the previous Appendix B. For the sake of consistency and completeness,
we here gives a brief account.
C.1 Doubly Block Circulant Matrix
In this section, we consider z as a 2D signal such as an image, and use H and W to denote its
“height” and “width”, respectively. It will be convenient to work with both a matrix representation
-z(0,0)	z(0,1)	…	z(0,W — 1)-
z(1,0)	z(1,1)	…	z(1,W — 1)
Z =	.	.	.	.
.	..	.
_Z(H — 1, 0) z(H — 1,1) …	z(H — 1,W — 1)_
as well as a vector representation
vec(z) =. hz(0, 0), . . . , z(0, W — 1), z(1, 0), . . . , z(1, W — 1), . . .
...,z(H — 1,0),...,z(H — 1,W — 1)「∈ R(HXW). (72)
We represent the circular translated version of z as transp,q(z) ∈ RH×W by an amount ofp and q
on the vertical and horizontal directions, respectively. That is, we let
transp,q (z)(h, w) =. z(h — p mod H, w — q mod W),
∀(h, w) ∈ {0, . . . , H — 1} × {0, . . . , W — 1}. (73)
It is obvious that trans0,0(z) = z. Moreover, there is a total number of H × W distinct translations
given by {transp,q(z), (p, q) ∈ {0, . . . , H — 1} × {0, . . . , W — 1}}. We may arrange the vector
representations of them into a matrix and obtain
circ(z) =. vec(trans0,0(z)), . . . , vec(trans0,W-1(z)),
vec(trans1,0(z)), . . . , vec(trans1,W-1(z)),
...,
vec(transH-1,0(z)), .. .,vec(transH-1,W-1(z))i ∈ R(H×W)×(H×W). (74)
The matrix circ(z) is known as the doubly block circulant matrix associated with z (see, e.g., Abidi
et al. (2016); Sedghi et al. (2018)).
We now consider a multi-channel 2D signal represented as a tensor Z ∈ rc×h×w, where C is
the number of channels. The c-th channel of Z is represented as Z[c] ∈ Rh×w, and the (h, w)-th
pixel is represented as z(h, w) ∈ RC. To compute the coding rate reduction for a collection of such
multi-channel 2D signals, we may flatten the tenor representation into a vector representation by
concatenating the vector representation of each channel, i.e., we let
VeC(Z) = [vec(z[1])*,...,vec(z[C])*]*	∈ R(C×h×W)	(75)
Furthermore, to obtain shift invariance for coding rate reduction, we may generate a collection of
translated versions of Z (along two spatial dimensions). Stacking the vector representation for such
translated copies as column vectors, we obtain
-CirC(Z[1])一
circ(Z) =	.	∈ R(C×h×W)X(H×w).	(76)
CirC(Z[C])
We can now define a translation invariant coding rate reduction for multi-channel 2D signals. Con-
sider a collection of m multi-channel 2D signals {Zi ∈ Rc×h×w}m=ι. Compactly representing the
data by Z ∈ rc×h×w×m where the i-th slice on the last dimension is Zi, we denote
CirC(Z) = [circ(Z1),..., CirC(Zm)]	∈ R(CXH×w)×(h×w×m).	(77)
21
Under review as a conference paper at ICLR 2021
Then, we define
∆Rcirc(Z, Π) = -1-∆R(circ(Z), H) = - ɪ log det J I + α ∙ Circ(Z) ∙ Circ(Z)" )
HW	2HW
-XX 2HHW logdet (l + j Circ(Z) ∙ Πj CirC(Z)*) , (78)
Where α = mCHW = mC2, % =	tr(1CHWWe2	=	tr(⅛，Yj	=镖)，and	πj	is augmented
membership matrix in an obvious way.
By folloWing an analogous argument as in the 1D case, one can shoW that ReduNet for multi-channel
2D signals naturally gives rise to the multi-channel 2D circulant convolution operations. We omit
the details, and focus on the construction of ReduNet in the frequency domain.
C.2 Fast Computation in Spectral Domain
Doubly block circulant matrix and 2D-DFT. Similar to the case of circulant matrices for 1D
signals, all doubly block circulant matrices share the same set of eigenvectors, and these eigenvectors
form a unitary matrix given by
F = Fh ③ Fw	∈ C(H×W八田×W),	(79)
where 0 denotes the Kronecker product and FH, FW are defined as in (38).
Analogous to Fact 4, F defines 2D-DFT as folloWs.
Fact 8 (2D-DFT as matrix-vector multiplication) The 2D-DFT of a signal z ∈ RH ×W can be
computed as
Vec(DFT(Z)) = F ∙ Vec(Z)	∈ C(HXW),	(80)
where
H-1 W-1
DFT(Z)(p, q) = √=== X X z(h, W) ∙ ωHhωWw,
VH ∙ W h=0 w=0
∀(p, q) ∈ {0, . . . , H - 1} × {0, . . . ,W - 1}. (81)
The 2D-IDFT of a signal v ∈ CH×W can be computed as
vec(IDFT(v)) = FT ∙ vec(v)	∈ C(H×w)	(82)
where
H-1 W-1
IDFT(V) (h,w) = √H .w X X V(P, q) ∙ ωHp hωw w
p=0 q=0
∀(h, w)	∈	{0, . . . ,H	-	1}	×	{0,	. . .,W-	1}.	(83)
Analogous to Fact 9, F relates DFT(Z) and circ(Z) as follows.
Fact 9 (2D-DFT are eigenvalues of the doubly block circulant matrix) Given a signal Z ∈
CH ×W, we have
F ∙ circ(z) ∙ F* = diag(Vec(DFT(Z))) or circ(z) = F* ∙ diag(Vec(DFT(Z))) ∙ F.	(84)
Doubly block circulant matrix and 2D-DFT for multi-channel signals. We now consider multi-
channel 2D signals Z ∈ rc×h×w. Let DFT(Z) ∈ Cc×h×w be a matrix where the c-th slice on
the first dimension is the DFT of the corresponding signal z[c]. That is, DFT(Z) [c] = DFT(z[c]) ∈
Ch×w. We use DFT(Z)(p, q) ∈ CC to denote slicing of Z on the frequency dimensions.
22
Under review as a conference paper at ICLR 2021
By using Fact 9, Circ(Z) and DFT(Z) are related as follows:
F * ∙ diag(Vec(DFT(Z[1]))) ∙ F
circ(z)=	.
F * ∙ diag(Vec(DFT(Z[C]))) ∙ F
F*
0
一 .
.
.
0
0 一
0
.
.
.
F*
-diag(Vec(DFT(Z[I])))-
diag(Vec(DFT(Z[2])))
.
.
.
diag(Vec(DFT(Z[C])))
• F. (85)
Similar to the 1D case, this relation can be leveraged to produce a fast implementation of ReduNet
in the spectral domain.
Translation-invariant ReduNet in the Spectral Domain. Given a collection of multi-channel
2D signals Z ∈ RC×H×W×m, We denote
DFT(Z)(p,q) = [DFT(Z1)(p,q),..., DFT(Zm)(p,q)]	∈ RC×m.	(86)
We introduce the notations E(p, q) ∈ RC×C×H×w and Cj(p,q) ∈ rc×c×η×w given by
E(p,q) = α • [I + α • DFT(Z)(p,q) ∙ DFT(Z)(p, q)*]-1	∈ CC×C,	(87)
Cj(p,q) = αj • [I + αj • DFT(Z)(p, q) • Πj . DFT(Z)(p,q)*]-1	∈ CC×C	(88)
In above, E(p, q) (resp., Cj(P, q)) is the (p, q)-th slice of E (resp., Cj) on the last two dimensions.
Then, the gradient of ∆Rdrc(Z, Π) with respect to Z can be calculated by the following result.
Theorem C.1 (Computing multi-channel 2D convolutions E and Cj) Let U ∈ CC×H×W×m
and Wj ∈ CC×h×w×m,j = 1,...,k be given by
U(p,q) = E(p,q) • DFT(Z)(p,q),
Wj(p,q) = Cj(p,q) • dft(Z)(P,q), j = 1,...,k,
(89)
(90)
for each (P, q) ∈ {0, . . . , H - 1} × {0, . . . , W - 1}. Then, we have
1	∂logdet(I + a • circ(Z)circ(Z)*)
2HW	∂Z
1	∂ (Yjlogdet(I + αj- • circ(Z)Πjcirc(Z)*))
2HW	∂Z
IDFT(U),
γj∙ ∙IDFT(WjΠj).
(91)
(92)
This result shows that the calculation of the derivatives for the 2D case is analogous to that of the
1D case. Therefore, the construction of the ReduNet for 2D translation invariance can be performed
using Algorithm 1 with straightforward extensions.
23
Under review as a conference paper at ICLR 2021
D	Implementation Details and Additional Experiments
Code for reproducing the results in this work will be made publicly available with the publication
of this paper. Disclaimer: in this work we do not particularly optimize any of the hyper parameters,
such as the number of initial channels, kernel sizes, and learning rate etc., for the best performance.
The choices are mostly for convenience and just minimally adequate to verify the concept, due to
limited computational resource.
D.1 ADDITIONAL EXPERIMENTS ON LEARNING MIXTURE OF GAUSSIANS IN S1 AND S2
We provide the cosine similarity results for the experiments described in Figure 2. The results are
shown in Figure 4. We can observe that the network can map the data points to orthogonal subspaces.
(e) Xtrain	(f) Ztrain	(g) Xtest	(h) Ztest
Figure 4: Cosine similarity (absolute value) for 2D and 3D Mixture of Gaussians. Lighter color
implies samples are more orthogonal.
Additional experiments on S1 and S2. We also provide additional experiments on learning mixture
of Gaussians in S1 and S2 in Figure 5. We can observe similar behavior of the proposed ReduNet:
the network can map data points from different classes to orthogonal subspaces.
Additional experiments on S1 with more than 2 classes. We try to apply ReduNet to learn mixture
of Gaussian distributions on S1 with the number of class is larger than 2. Notice that these are the
cases to which the existing theory about MCR2 (Yu et al., 2020) no longer applies. These exper-
iments suggest that the MCR2 still promotes between-class discriminativeness with so constructed
ReduNet. In particular, the case on the left of Figure 6 indicates that the ReduNet has “merged” two
linearly correlated clusters into one on the same line. This is consistent with the objective of rate
reduction to group data as linear subspaces.
D.2 Experiments on UCI datasets
We evaluate the proposed ReduNet on some real datasets, namely the two UCI tasks (Dua & Graff,
2017): iris and mice. There are 3 classes in iris dataset and the number of features is 4. For mice
dataset, there are 8 classes and the number of features is 82. We randomly select 70% data as the
training data, and use the rest for evaluation. The results are summarized in Table 1. We compare
our method with logistic regression, SVM, and random forest, and we use the implementations by
sklearn (Pedregosa et al., 2011). From Table 1, we find that the forward-constructed ReduNet is
able to achieve comparable performance with classic methods such as logistic regression, SVM, and
random forest.
24
Under review as a conference paper at ICLR 2021
(a) X (2D) (left: scatter plot; right: co- (b) Z(2D) (left: scatter plot; right: co-
sine similarity visualization)	sine similarity visualization)
S 1.5-
.
q ι.0-
0.5-
0.0-
0
250 500 750 1000 1250 1500 1750 2000
Layers
(c) Loss
sine similarity visualization)
sine similarity visualization)
(f) Loss
Figure 5: Learning mixture of Gaussians in S1 and S2. (Top) For S1, we set σ1 = σ2 = 0.1;
(Bottom) For S2, we set σ1 = σ2 = σ3 = 0.1.
(a) 3 classes. (Left) X ; (Right) Z
Figure 6: Learning mixture of Gaussian distributions with more than 2 classes. For both cases, we
use step size η = 0.5 and precision = 0.1. For (a), we set iteration L = 2, 500; for (b), we set
iteration L = 4, 000.
(b) 6 classes. (Left) X ; (Right) Z
D.3 Additional Experiments on Learning Shift Invariant Features
We provide additional experiments for Learning Shift Invariant Features in §3. The code for sam-
pling from h1(t) = sin(t) + and h2(t) = sign(sin(t)) + is described in Algorithm 2, and the
pseudocode for sampling from 2 classes {h1, h2} is described as follows, we sample training and
test signals using the same procedure.
t0 = np.random.uniform(low=0, high=10*np.pi, Size=Samples)
X = np.linspace(t0, t0+2*np.pi, time).T
noise1 = np.random.normal(0, 0.1, size=(samples, time))
X1 = np.sin(x) + noise1
noise2 = np.random.normal(0, 0.1, size=(samples, time))
X2 = np.sign(np.sin(x)) + noise2
data = np.vstack([X1, X2])
labels = np.hstack([np.ones(samples)*1,
np.ones(samples)*2]).astype(np.int32)
We also provide cosine similarities between samples in Figure 8. We visualize the cosine similarities
for the input Xtrain , Xtest as well as the learned representations Ztrain , Ztest. The cosine similarity
between sample pairs selected from different classes are shown in Figure 9. We can observe that
the original data is not orthogonal w.r.t. different classes, and the the ReduNet is able to learn
discriminative (orthogonal) representations.
25
Under review as a conference paper at ICLR 2021
Table 1: Performance (Accuracy) on iris and mice of the UCI datasets.
	REDUNET	Logistic Regression	SVM	Random Forest
	 IRIS	0.978	0.933	0.933	0.978
MICE	0.972	0.855	0.975	0.985
Algorithm 2 Pseudocode for sampling signals from 1D functions
Input: Number of samples m, number of classes k, number of features n, function {h1, . . . , hk}.
1:	for j = 1, 2, . . . , k do
2:	for i = 1, 2, . . . , m do
3:	to 〜Uniform[0,10π];
4:	t = [to, to + 2π∕n,to + (2π∕n) ∙ 2, to + (2π∕n) ∙ 3, .. .,to + (2π∕n) ∙ (n - 1)];
5:	xij = hj (t) + # broadcast over vector t;
6:	end for
7:	Xj = [xj1,xj2, . . . ,xjm];
8:	end for
9:	X = [X1,X2,...,Xk];
10:	shuffle X .
Output: outputs X.
D.4 Additional Experiments on learning rotational invariance on MNIST
We provide additional experiments for learning rotational invariance on MNIST in §3. Examples
of rotated images are shown in Figure 12. We compare the accuracy (both on the original test data
and the shifted test data) of the ReduNet (without considering invariance) and the shift invariant
ReduNet. For ReduNet (without considering invariance), we use the same training dataset as the
shift invariant ReduNet, we set iteration L = 3, 500, step size η = 0.5, and precision = 0.1.
The results are summarized in Table 2. With the invariant design, we can see from Table 2 that the
shift invariant ReduNet achieves better performance in terms of invariance on the MNIST binary
classification task.
We also provide cosine similarities between samples in Figure 10. We visualize the cosine similari-
ties for the input Xtrain, Xtest as well as the learned representations Ztrain , Ztest. The cosine similarity
between sample pairs selected from different classes are shown in Figure 11. We can observe that
the constructed ReduNet is able to learn discriminative (orthogonal) and invariant representations
for MNIST digits.
Table 2: Comparing network performance on learning rotational-invariant representations on MNIST.
	ReduNet	ReduNet (shift-invariant)
	 Acc (original test data)	0.983	0.996
Acc (test data with all possible shifts)	0.707	0.993
D.5 Experiments on learning 2D translation invariance on MNIST
In this part, we provide experimental results for verifying the invariance property of ReduNet under
2D translations. We construct 1). ReduNet (without considering invariance) and 2). 2D translation-
invariant ReduNet for classifying digit ‘0’ and digit ‘1’ on MNIST dataset. We use m = 1, 000
samples (500 samples from each class) for training the models, and use another 500 samples (250
samples from each class) for evaluation. To evaluate the 2D translational invariance, for each test
image xtest ∈ RH ×W , we consider all translation augmentations of the test image with a stride=7.
More specifically, for the MNIST dataset, we have H = W = 28. So for each image, the total
number of all cyclic translation augmentations (with stride=7) is 4 × 4 = 16. Examples of translated
images are shown in Figure 13. Notice that such translations are considerably larger than normally
considered in the literature since we consider invariance to the entire group of cyclic translations on
the H × W grid as a torus. See Figure 13 for some representative test samples.
26
Under review as a conference paper at ICLR 2021
Figure 7: Visualization of signals in 1D. Blue dots represent the sampled signal used for training
with dimension n = 150. Red curves represent the underlying 1D function (noiseless). (Left) One
sample from class 1; (Right) One sample from class 2.
(a) Xtrain	(b) Ztrain
(c) Xtest
(d) Ztest
Figure 8: Cosine similarity (absolute value) of training/test data as well as training/test representa-
tions for learning 1D functions.
For ReduNet (without considering translation invariance), we set iteration L = 2, 000, step size
η = 0.1, and precision = 0.1. For translation-invariant ReduNet, we set L = 2, 000, step size
η = 0.5, precision = 0.1, number of channels C = 5, and kernel size is set as 3 × 3. We
summarize the results in Table 3. Similar to the 1D rotational results on the MNIST dataset, the
translation-invariant ReduNet achieves better performance under translations compared with the
RedeNet without considering invariance. The accuracy drop of the translation-invariant ReduNet is
much less than the one of ReduNet without invariance design.
Table 3: Comparing network performance on learning 2D translation-invariant representations on MNIST.
I	ReduNet	ReduNet (translation-invariant)
ACC (Original Test Data)	0.980	0.975
ACC (test Data with ALL Possible Shifts)	0.540	0.909
27
Under review as a conference paper at ICLR 2021
0.5-
0.0-
5 0 5 0 5 0
- - - - - -
3 3 2 2 1 1
*unoo
14
12
1.0-
0.8-
06
0.4-
02
0.0-
0.0	0.2	0.4	0.6	0.8
Similarity
∙22∙86∙4
LL
*un。。
ɪθ ol ol ol ol U
Similarity
0.2-
0.0 0.0 0.2 0.4 0.6	0.8	1.0
Similarity
(a)	Xi (train) Vs. X2(test)
×104
(b)	X2(train) Vs Xi(test)
(c)	Xi (train) Vs X2 (test)
(d)	X2 (train) Vs Xi (test)
×106
.0.0	0.2	0.4	0.6	0.8	1.0
Similarity
(e)	Zi(train) Vs. Z2(test)
.0.0	0.2	0.4	0.6	0.8	1.0
Similarity
(f)	Z2(train) Vs Zi(test)
.0.0	0.2	0.4	0.6	0.8	1.0
Similarity
(g)	Zi (train) Vs Z2 (test)
.0.0	0.2	0.4	0.6	0.8	1.0
Similarity
(h)	Z2 (train) Vs Zi (test)
Figure 9: Histogram of Cosine similarity between pairs sampled from different Classes for learning
1D funCtion. The histogram of Cosine similarity between training data Xc as well as representations
Zc Vs. testing (shifted) data Xc0 as well as (shifted) representations Zc0, where we let c denote the
Class index and c 6= c0 .
(d) Ztest
(a) Xtrain	(b) Ztrain	(C) Xtest
Figure 10: Cosine similarity (absolute Value) of training/test data as well as traning/test representa-
tions for learning rotational inVariant representations on MNIST.
0 5 0 5
- - - -
2 110
*unoo
…0.0	0.2	0.4 0.6 0.8 1.0
Similarity
(a) X1 (train) Vs. X2(test)
6
8
1 O O O
*un。。
,	0.0	0.2	0.4	0.6	0.8	1.0
Similarity
(e) Zi(train) Vs. Z2(test)
∙5∙0∙5∙0∙5
ZZLL
*un。。
0.0	0.2	0.4	0.6	0.8
Similarity
1.0
*un。。
0.0	0.2	0.4	0.6	0.8	1.0
Similarity
(C) Xi (train) Vs X2 (test)
×107
*un。。
0.0	0.2	0.4	0.6	0.8	1.0
Similarity
(d) X2 (train) Vs Xi (test)
(b) X2(train) Vs X1(test)
*un。。
.0.0	0.2	0.4	0.6	0.8	1.0
Similarity
(f)	Z2(train) Vs Zi(test)
B 5 Q *P B 5
O. 7∙7∙7
2 1 1 1 1 O
*un。。
.	0.0 0.2	0.4 0.6 0.8 1.0
Similarity
(g)	Zi (train) Vs Z2 (test)
2.00-
1.75-
1.50-
之 1.25-
o 1.00-
U 0.75-
0.50-
0.25-
.	0.0	0.2	0.4 0.6 0.8 1.0
Similarity
(h) Z2 (train) Vs Zi (test)
Figure 11: Histogram of Cosine similarity between pairs sampled from different Classes for learning
rotational inVariant representations on MNIST. The histogram of Cosine similarity between training
data Xc as well as representations Zc Vs. testing (shifted) data Xc0 as well as (shifted) representa-
tions Zc0 , where we let c denote the Class index and c 6= c0 .
28
Under review as a conference paper at ICLR 2021

& 53
O 0 0 0
QQGO
QIOlOiQ
Figure 12: Examples of rotated images of MNIST digits for testing rotation invariance, each rotated
by18。.(Left) digit ‘0'. (Right) digit'1’.
0∖ φ 4。
Figure 13: Examples of translated images of MNIST digits (with stride=7) for testing cyclic trans-
lation invariance of the ReduNet. (Left) digit ‘0’. (Right) digit ‘1’.
29