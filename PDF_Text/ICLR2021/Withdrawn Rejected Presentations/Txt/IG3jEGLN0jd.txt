Under review as a conference paper at ICLR 2021
Contrastive estimation reveals topic
POSTERIOR INFORMATION TO LINEAR MODELS
Anonymous authors
Paper under double-blind review
Ab stract
Contrastive learning is an approach to representation learning that utilizes natu-
rally occurring similar and dissimilar pairs of data points to find useful embed-
dings of data. In the context of document classification under topic modeling
assumptions, we prove that contrastive learning is capable of recovering a repre-
sentation of documents that reveals their underlying topic posterior information to
linear models. We apply this procedure in a semi-supervised setup and demon-
strate empirically that linear classifiers with these representations perform well in
document classification tasks with very few training examples.
1 Introduction
Using unlabeled data to find useful embeddings is a central challenge in representation learning.
Classical approaches to this task often start by fitting some type of structure to the unlabeled data,
such as a generative model or a dictionary, and then embed future data via inference with the fit-
ted structure (Blei et al., 2003; Raina et al., 2007). While principled, this approach is not without
its drawbacks. One issue is that learning structures and performing inference is often hard in gen-
eral (Sontag & Roy, 2011; Arora et al., 2012). Another issue is that we must a priori choose a
structure and method for fitting the unlabeled data, and unsupervised methods for learning these
structures can be sensitive to model misspecification (Kulesza et al., 2014).
Contrastive learning (also called noise contrastive estimation, or NCE) is an alternative represen-
tation learning approach that tries to capture the latent structure in unlabeled data implicitly. At a
high level, these methods formulate a classification problem in which the goal is to distinguish ex-
amples that naturally occur in pairs, called positive samples, from randomly paired examples, called
negative samples. The particular choice of positive samples depends on the setting. In image repre-
sentation problems, for example, patches from the same image or neighboring frames from videos
may serve as positive examples (Wang & Gupta, 2015; Hjelm et al., 2018). In text modeling, the
positive samples may be neighboring sentences (Logeswaran & Lee, 2018; Devlin et al., 2018). The
idea is that in the course of learning to distinguish between semantically similar positive examples
and randomly chosen negative examples, we will capture some of the latent semantic information.
In this work, we look “under the hood” of contrastive learning and consider its application to docu-
ment modeling, where the goal is to construct useful vector representations of text documents in a
corpus. In this setting, there is a natural source of positive and negative examples: a positive exam-
ple is simply a document from the corpus, and a negative example is one formed by pasting together
the first half of one document and the second half of another (independently chosen) document. We
prove that when the corpus is generated by a topic model, learning to distinguish between these two
types of documents yields representations that are closely related to their underlying latent variables.
One potential application of contrastive learning is in a semi-supervised setting, where there is a
small amount of labeled data as well as a much larger collection of unlabeled data. In these situa-
tions, purely supervised methods that fit complicated models may have poor performance due to the
limited amount of labeled data. On the other hand, when the labels are well-approximated by some
function of the latent structure, our results show that an effective strategy is to fit linear functions,
which may be learned with relatively little labeled data, on top of contrastive representations. In our
experiments, we verify empirically that this approach produces reasonable results.
1
Under review as a conference paper at ICLR 2021
Contributions. The primary goal of this work is to shed light on what contrastive learning tech-
niques uncover in the presence of latent structure. To this end, we focus on the setting of document
modeling where latent structure is induced by a topic model. Here, our contrastive learning objec-
tive is to distinguish true documents from ‘fake’ documents that are composed by randomly pasting
together two document halves from the corpus. We consider two types of architectures or functional
forms of solutions for this problem, both trained with logistic loss.
The first architecture, on which our theoretical analysis will focus, consists of general functions of
the form f (∙, ∙). Here, We have trained f so that f (x, χ0) indicates the confidence of the model that
x and x0 are two halves of the same document. To embed a new document x using f, we propose a
landmark embedding procedure: fix documents l1, . . . , lM (our so-called landmarks) and create the
embedding φ(x) using a function of the predictions f(x, l1), . . . , f(x, lM). In Section 4, We shoW
that the embedding φ(x) is a linear transformation of the underlying topic posterior moments of x.
Moreover, under certain conditions this linear relationship is invertible, so that linear functions of
φ(x) correspond to polynomial functions of the topic posterior of document x. In Section 5, We
shoW that errors in f on the contrastive learning objective transfer smoothly to errors in φ(x) as a
linear transformation of the topic posterior ofx. Thus, as the quality of f improves, linear functions
of φ(x) more closely approximate polynomial functions of the topic posterior of document x.
Unfortunately, the landmark embedding can require quite a feW landmarks before our theoretical
results kick in. Moreover, embedding a document requires M evaluations of f, Which can be expen-
sive. To circumvent this, in Section 7 We introduce a direct embedding procedure that more closely
matches What is done in practice. We use an architecture of the form f1(x)Tf2(x0) Where f1, f2 are
functions With d-dimensional outputs, and We train this architecture on the same contrastive learn-
ing task as before. To embed a document x, We simply use the evaluation f1 (x). In Section 7,
We evaluate this embedding on a semi-supervised learning task, and We shoW that it has reasonable
performance. Indeed, the direct embedding method outperforms the landmark embedding method,
Which raises the question of Whether or not anything can be theoretically proven about the direct
embedding method. We leave this question to future Work.
Related work. Reducing an unsupervised problem to a synthetically-generated supervised prob-
lem is a Well-studied technique. In dynamical systems modeling, Langford et al. (2009) shoWed that
the solutions to a feW forWard prediction problems can be used to track the underlying state of a non-
linear dynamical system. For linear dynamics, the idea is also seen in autoregressive models (Yule,
1927). In anomaly/outlier detection, a useful technique is to learn a classifier that distinguishes
betWeen true samples from a distribution and fake samples from some synthetic distribution (Stein-
Wart et al., 2005; Abe et al., 2006). Similarly, estimating the parameters of a probabilistic model
can be reduced to learning to classify betWeen true data and randomly generated noise (Gutmann &
Hyvarinen, 2010).
In the context of natural language processing, methods such as skip-gram and continuous bag-of-
Words turn the problem of finding Word embeddings into a prediction problem (Mikolov et al.,
2013a;b). Modern language representation training algorithms such as BERT and QT also use nat-
urally occurring classification tasks such as predicting randomly masked elements of a sentence or
discriminating Whether or not tWo sentences are adjacent (Devlin et al., 2018; LogesWaran & Lee,
2018). Training these models often employs a technique called negative sampling, in Which softmax
prediction probabilities are estimated by randomly sampling examples; this bears close resemblance
to the Way that negative examples are produced in contrastive learning.
Most relevant to the current paper, Arora et al. (2019) gave a theoretical analysis of contrastive
learning. They considered the specific setting of trying to minimize the contrastive loss
L(f) = Eχ,χ+,χ- [' (f (x)T(f(x+)- f(x-)))]
Where (x, x+) is a positive pair and (x, x-) is a negative pair. They shoWed that if there is an
underlying collection of latent classes and positive examples are generated by draWs from the same
class, then minimizing the contrastive loss over embedding functions f yields good representations
for the classification task of distinguishing latent classes.
The main difference betWeen our Work and that of Arora et al. (2019) is that We adopt a generative
modeling perspective and induce the contrastive distribution naturally, While they do not make gen-
erative assumptions but assume the contrastive distribution is directly induced by the doWnstream
2
Under review as a conference paper at ICLR 2021
Algorithm 1 Contrastive Estimation with Documents
Input: Corpus U of unlabeled documents. Initialize: S = 0.
for i = 1, . . . , n do
Sample X and X independently from Unif(U); S J S ∪
end for
({(X(1) , X(2) , 1)} w.p. 1/2
[{(x(1),X(2),0)} w.p. 1∕2
Solve the optimization problem
f = minimize X ylog 1 + e-f(x(1),x(2)) + (1 - y) log 1 + ef(x(1),x(2))
(x(1),x(2) 3,y)∈S
Select landmark documents 1i,...,1m and embed φ(χ) = (exp (f(χ,li)) : i ∈
classification task. In particUlar, oUr contrastive distribUtion and sUpervised learning problem are
only indirectly related through the latent variables in the generative model, while Arora et al. as-
sume an explicit connection. The focus of our work is therefore complementary to theirs: we study
the types of functions that can be succinctly expressed with the contrastive representation in our
generative modeling setup. In addition, our results apply to semi-supervised regression, but it is un-
clear how to define their contrastive distribution in this setting; this makes it difficult to apply their
results here. Finally, Arora et al. point out the method they study has limitations that arise when
the number of latent classes is small and the probability of negative samples having the same class
is high. In our setting, class collisions turn out not to be a problem since our embeddings explicitly
utilize conditional probability information from the solution to our contrastive learning objective.
2 Setup
Let V denote a finite vocabulary. A topic is a distribution over V . We will assume that we have K
such topics, and denote the corresponding distributions as O(∙ | k) for k = 1,...,K .To generate a
length m document X, one first draws a vector w from ∆K, the K-dimensional probability simplex,
and then samples each of the m words xι,...,Xm by first sampling the latent variable Zi 〜 W and
drawing Xi 〜O(∙ | Zi). We note that documents are allowed to take different lengths.
We will also be interested in the case where each document has an associated label ` ∈ R. One
natural restriction to make on a label is that it is conditionally independent of the document given
the topic distribution of the document. Thus, we will assume that there is a joint distribution D of
triples (X, w, `), where (X, w) are generated according to the topic model described above, and then
` is drawn from some distribution conditioned on w. One of the goals of this paper is to characterize
the functional forms of this conditional distribution that are most suited to contrastive learning.
In the representation learning approach to the semi-supervised setting, we are given a large collection
U of documents with no labels, and a small collection L of labeled documents. Using U, we learn a
feature map φ that will form the basis of our predictions. Then, using L, we learn a simple predictor
based on φ, such as a linear function, to predict the label ` given φ(X).
3 Contrastive learning algorithm
In contrastive learning, examples come in the form of similar and dissimilar pairs of points, where
the exact definition of similar/dissimilar depends on the task at hand. Our construction of similar
pairs will take the form of randomly splitting a document into two documents, and our dissimilar
pairs will consist of subsampled documents from two randomly chosen documents. In the generative
modeling setup, since the words are i.i.d. conditional on the topic distribution, a natural way to split
a document X into two is to call the first half of the words X(1) and the second half X(2). In our
experiments, we split the documents by applying a random permutation to the word tokens and
partitioning in half, thus effectively ignoring the word ordering (as is common in topic models).
3
Under review as a conference paper at ICLR 2021
The contrastive representation learning procedure is displayed in Algorithm 1. It uses a finite-sample
approximation to the contrastive distribution Dcontrast described as follows: (a) sample a document
x and partition it into (x(1), x(2)), (b) with probability 1/2 output (x(1), x(2), 1), (c) with probability
1/2, sample a second document (X⑴，X(2)) and output (χ(1),X(2), 0). For (x, χ0, y)〜 Dcontrast, the
parts x and x0 are the two halves of a (possibly synthetic) document, and y is the binary label. Our
contrastive learning objective is to minimize the binary cross-entropy loss of discriminating between
positive and negative examples:
LCOntrast(f) := E(XH,y)〜DContraSt ylog(1 + e-f(χ,χ0)) +(1-y)log(1 + ef3χ0))] . (1)
In our algorithm, we approximate this expectation via sampling and optimize the empirical objective,
which yields an approximate minimizer f (chosen from some function class F).
To see why optimizing this contrastive learning objective is so useful, let f? be the global minimizer
of Eq. (1). By Bayes’ theorem we have that g? := exp(f?) satisfies the following:
g*(x,χ0) := exp(f*(x,χ0)) = P(y = 1 | x,x0) = P(x：= x： x0)、.
P(y = 0 | x,x0)	P(X⑴=x)P(χ(2) = x0)
Thus, g?(x, x0) captures the ratio of the probability of x and x0 co-occurring as the first and second
halves of the same document and the product of their marginal probabilities.
In Eq. (1), we have not imposed any constraints on the functions over which we are optimizing.
Thus, we seek to extract a useful embedding from g? using only black box access to g? . Our
approach to this problem is to select some set of fixed documents, which we call landmarks, and to
embed by utilizing the predictions of g? on these landmarks.
Formally, we select documents l1, . . . , lM and represent document x as1
φ*(x)=(g*(x,l1),...,g?(x,lM)).	⑵
This yields the final document-level representation, which can be used for downstream tasks. As
We shall see in Section 4, when the documents have an underlying topic structure, φ*(χ) is related
to the posterior information of the topics by a linear transformation and this linear transformation is
invertible whenever the landmarks l1 , . . . , lM are sufficiently diverse.
In practice, we only have access to an approximate minimizer f of Eq. (1). Thus, our embedding in
practice will be given by
φ(x)=卜Xp (f(x, 11)) ,..., exp (f(x, Im))).
In Section 5 we will see that, under some mild assumptions, our claims about φ? also hold true for
φ up to some small errors.
Finally, we point out that there is nothing special about the binary cross-entropy loss. We may
replace this loss in Eq. (1) with any proper scoring rule (Shuford et al., 1966; Buja et al., 2005), so
long as the appropriate non-linear transformation is applied to the resulting predictions.
4 Recovering topic structure
In this section, we focus on expressivity of the contrastive representation, showing that polynomial
functions of the topic posterior can be represented as linear functions of the representation. To do
so, we ignore statistical issues and assume that we have access to the oracle representations g?(x, ∙).
In Section 5, we address statistical issues.
Recall the generative topic model process for a document x. We first draw a topic vector w ∈ ∆K .
Then for each word i = 1,..., length(χ), we draw Zi 〜Categorical(w) and Xi 〜O(∙ | Zi). We
will show that when documents are generated according to the above model, the embedding of a
document X in Eq. (2) is closely related its underlying topic vector w.
1Strictly speaking, we should first partition x = (x(1) , x(2)), only use landmarks that occur as second-
halves of documents, and embed x 7→ (g?(x(1) , l1), . . . , g? (x(1) , lM)). For the sake of clarity, we will ignore
this small technical issue here and in the remainder of the paper.
4
Under review as a conference paper at ICLR 2021
4.1	The single topic case
To build intuition for the embedding in Eq. (2), we first consider the case where each document’s
probability vector w is supported on a single topic, i.e., w ∈ {e1, . . . , eK} where ei is the ith standard
basis element. Then we have the following lemma.
Lemma 1. For any documents x, x0, we can write g?(x, x0) = η(x)Tψ(x0), where η(x)k := P(w =
ek |x(1) = x) is the topic posterior distribution and ψ(x)k := P(x(2) = x|w = ek)/P(x(2) = x0).
Due to space constraints, all proofs are deferred to Appendix C and Appendix D.
The characterization from Lemma 1 shows that g? contains information about the posterior topic
distribution η(∙). To recover it, We must make sure that the ψ(∙) vectors for our landmark documents
span RK. Formally, if l1, . . . , lM are the landmarks, and we define the matrix L ∈ RK×M by
L := [ψ(II)…	ψ(IM)],	⑶
then our representation satisfies φ*(x) = LTη(x). If our landmarks are chosen so that L has rank
K, then Ltφ？(χ) = η(χ), where f denotes the matrix pseudo-inverse. Thus, there is a linear
transformation of φ? (x) that recovers the posterior distribution of W given x.
There are two observations to be made here. The first is that this argument naturally generalizes
beyond the single topic setting to any setting where w can take values in a finite set S, which may
include some mixtures of multiple topics, though the number of landmarks needed would grow at
least linearly with |S|. The second is that we have made no use of the structure of x(1) and x(2),
except for that they are independent conditioned on w. Thus, this argument applies to more exotic
ways of partitioning a document beyond the bag-of-words approach.
4.2	The general setting
In the general setting, we allow document vectors to be any probability vector in ∆K, and we do not
hope to recover the full posterior distribution over ∆K . However, the intuition from the single topic
case largely carries over, and we will show that we can still recover the posterior moments.
Let mmax be the length of the longest landmark document. Let SmK := {α ∈ Z+K : Pk αk = m}
denote the set of non-negative integer vectors that sum to mandlet SKmmax= SK ∪ …∪ SKmax ∙
Let π(w) denote the degree-mmax monomial vector in w: π(w) := (wf1 •…Wak : α ∈ SKmInaχ).
For a positive integer m and a vector α ∈ SmK , we let ([m]) ：= {z ∈ [K]m ： Pm=I 1I[zi = k]=
αk ∀k ∈ [K]}. For a document x of length m, the degree-m polynomial vector ψm is defined by
ψm(x) := X Y O(xi|zi) : α ∈ SmK ,
z∈([mα])i=1
and let ψd(x) = ~0 for all d 6= m. The cumulative polynomial vector ψ is given by
ψ(x) := P(χQl = χ) (ψ0(x),ψl(x), …，ψmmax(x)).	⑷
Given these definitions, we have the following general case analogue of Lemma 1.
Lemma 2. For any documents x, x0, we may write g?(x, x0) = η(x)Tψ(x0) where η(x) :=
E[π(w)∣x⑴=x] and ψ is defined in Eq (4).
Thus, we again have φ？(χ) = LTη(χ), but the columns of L are now vectors ψ(li) from Eq. (4).
Our analysis, so far, shows that if we choose the landmarks such that LLT is invertible, then our
representation captures all moments of the topic posterior up to degree mmax . As the next the-
orem shows, we can ensure that LLT is invertible whenever each topic has an associated anchor
word (Arora et al., 2012), i.e., a word that occurs with positive probability only within that topic. In
this case, there is a set of landmarks l1 , . . . , lM such that any polynomial of η(x) can be expressed
as a linear function of φ?(χ).
5
Under review as a conference paper at ICLR 2021
Theorem 3. Suppose that (i) each topic has an associated anchor word, and (ii) the marginal
distribution ofw has positive probability on the interior of ∆K. For any do ≥ 1, there is a collection
of M = O(Kdo) landmark documents l1, . . . , lM such that if Q(w) is a degree-do polynomial in w,
then there is a vector θ ∈ RM such that (θ, φ*(x)i = E[Q(w)∣x⑴=x] for all documents X.
Coupling Theorem 3 with the Stone-Weierstrass theorem (Stone, 1948) shows that, in principle, the
posterior mean of any continuous function of w can be approximated using our representation.
5 Error analysis
Given a finite amount of data, we cannot hope to solve Eq. (1) exactly. Thus, our solution f will
only be an approximation to f?. Since fis the basis of our representation, one may worry that errors
incurred in this approximation will cascade and cause the approximate representation φ(x) to differ
so wildly from φ*(x) that the results of Section 4 do not even approximately hold.
In this section, we will show that, under certain conditions, such fears are unfounded. Specifically,
we will show that there is an error transformation from the approximation error of f to the approx-
imation error of linear functions in φ. That is, if the target function is η(x)Tθ*, then We will show
that the best mean squared error achievable using our approximate representation φ, given by
R(φ) := minEx〜μ⑴(η(x) θ 一 φ(x) v)2,
is bounded in terms of the approximation quality of f as well as some other terms. Here, μ(1) is the
marginal distribution over first halves of documents drawn from D. Thus, for the specific setting of
semi-supervised learning, an approximate solution to Eq. (1) is good enough.
There are a number of reasonable ways to choose landmark documents. Here we consider a simple
method: randomly sample them from the marginal distribution μ(2) of x(2). We will assume that
this distribution satisfies certain regularity properties.
Assumption 1. There isa constant σmin > 0 such that for any δ ∈ (0, 1), there isa number M0 such
thatfor an i.i.d. sample l∖,...,lM from μ(2), with M ≥ Mo, with probability 1 一 δ, the matrix L in
Eq. (3) (with ψ as defined in Lemma 1 or Eq. (4)) has minimum singular value at least σmin √M.
Note that the smallest non-zero singular value of √= L is the square-root of the smallest eigenvalue
of a certain empirical second-moment matrix. Hence, Assumption 1 holds under appropriate condi-
tions on the landmark distribution, for instance via tail bounds for sums of random matrices (Tropp,
2012) combined with matrix perturbation analysis (e.g., Weyl’s inequality). In the single topic set-
ting with anchor words, it can be shown that for long enough documents, σmin is lower-bounded by
a constant when M0 grows polynomially with K . We defer a detailed proof of this to Appendix D.
We will also assume that the predictions of f and f? bounded above by some constant.
Assumption 2. There exists some gmax > 0 such that f(x, li), f?(x, li) ≤ log gmax for all docu-
ments x and landmarks li.
Note that if Assumption 2 holds for f?, then it can be made to hold for f by clipping. Moreover, it
holds for f? whenever the vocabulary and document sizes are constants:
?	0	P(x(1) = x, x(2) = x0)	P(x(2) = x0 | x(1) = x)	1
f (X，X ) = log	P(χ(1)	=	x)P(x(2)= χ0)	= log	P(χ(2) =	χ0)	≤ log	P(χ(2)	= χ0).
Since landmarks are sampled, and the number of possible documents is finite, there exists a constant
pmin > 0 such that P(x(2) = l) ≥ pmin. Thus, Assumption 2 holds for gmax ≤ 1/pmin.
Given these assumptions, we have the following error transformation guarantee.
Theorem 4. Fix any δ ∈ (0, 1), and suppose Assumption 1 and Assumption 2 hold (with M0,
σmin, and fmax). Let f be the function returned by the contrastive learning algorithm, and let
ε := Lcontrast (f) — Lcontrast (f ?) denote its excess ContraStive loss. If M ≥ Mo, then with probability
6
Under review as a conference paper at ICLR 2021
Topic Recovery
Small NN	Large NN
0.1
0.2
0.5
1.0
0.05	0.15	0.25	0.35	0.45	0.05	0.15	0.25	0.35	0.45
Topic similarity (Dirichlet parameter)	Topic similarity (Dirichlet parameter)
0.0	0.5	1.0
Accuracy
Figure 1:	Topic modeling simulations. Left: Average total variation distance between topics. Right:
Topic recovery accuracy for contrastive models. Total number of documents sampled = 6M × rate.
at least 1 - δ over the random sample of l1, . . . lM,
.^ .
R(φ) ≤
kθ* k2 (1+ gmax)4 Λ+ r 2lθg(2∕δ)
σmin	[ + V M
We make a few observations here. First, kθ? k22 is a measure of the complexity of the target function.
Thus, if the target function is some reasonable function (e.g., low-degree polynomial) of the posterior
document vector, then we can expect kθ? k22 to be small. Second, the dependence on gmax is probably
not very tight and can likely be improved. Third, note that M can grow andε can shrink with the
number of unlabeled documents; indeed, none of the terms in Theorem 4 deal with labeled data.
Finally, it is possible to establish guarantees in a semi-supervised setting using our analysis. If we
have Ul ι.ι.d. labeled examples, and We learn a linear predictor V With the representation φ using
ERM (say), then the bias-variance decomposition grants
mse(V)= Eχ~μ(l) (n(X)Tθ? - φ(X)Tv)2 = R(B) + Eχ~μ(l) (B(X)T(V* - V))2,
where v* is the minimizer of mse(∙). The final term 旧方〜兴⑴(φ(X)T(v*-V))2 is the excess risk in
linear regression, Which goes to zero as nL → ∞.
6	Topic modeling simulations
To test our theory, We ran simulation experiments With a single-topic generative model Where K =
20 topics are sampled from a symmetric Dirichlet(α) distribution over a vocabulary of size 5k. The
Dirichlet parameter α governs the sparsity of the topic distributions, effectively determining the
similarity of the topics: as α increases the prior concentrates on the interior of the simplex, forcing
the topic distributions to be more similar. This is visualized in the left panel of Figure 1.
In the experiments, We generate a dataset and solve the contrastive optimization problem, and then
We construct the landmark embeddings B(X) for each document X using 1k landmark documents,
folloWing Section 4. Using the true likelihood matrix L for the landmarks, We infer the MAP topic
estimate and measure accuracy as the fraction of test documents for Which this prediction matches
the generating topic. See Appendix A for additional details.
The results are displayed in the center and right panel of Figure 1 Where We vary the netWork
architecture and the amount of training data. The experiment identifies several interesting properties
of the contrastive learning approach. First, as a sanity check, the algorithm does accurately predict
the latent topics of the test documents in most experimental conditions and the accuracy is quite
high When the problem is relatively easy (e.g., α is small). Second, the performance degrades as
α increases, but this can be mitigated by increasing the model capacity (size of the netWork) or the
resampling rate (Which exposes the model to more unlabeled data). Specifically, We consistently
see that for a fixed model and α, increasing the resampling rate improves accuracy. A similar
trend emerges When We fix α and rate and increase model capacity. These findings suggest that
latent topics can be recovered by the contrastive learning approach, provided We have an expressive
enough model and enough unlabeled data.
7
Under review as a conference paper at ICLR 2021
0∙84
&
I 0^2
word2vec t-SNE
# of Landmarks Comparison
0.80
0.78
IgO 1500 20g 2 5g 30OT 3500 4000
Training examples
Contrastive error vs Supervised Accuracy
■ Business
Scl∕Γech
■ Sports
■ Worid
Figure 2:	Experiments with AG news dataset. Top left: test accuracy of methods as we increase the
number of supervised training examples. Bottom left: Landmark-NCE performance as we vary
number of landmarks. Top middle: Direct-NCE performance as we vary network depth. Bottom
middle: Relationship between contrastive error and test accuracy for Direct-NCE. Right: t-SNE
visualizations of Direct-NCE and word2vec embeddings.
7 Semi-supervised experiments
We also conducted experiments with our document-level contrastive representations in a semi-
supervised setting. The goal of these experiments is to demonstrate that the contrastive representa-
tions yield non-trivial performance, as consistent with the theory. Note that our intention is not to
show state-of-the-art performance using contrastive learning; that is beyond the scope of the paper.
We discuss the main findings here, with experimental details deferred to Appendix B.
A closely related representation. In the worst-case, the guarantees from Section 4 and Section 5
require the number of landmarks to be quite large. To develop a more practical representation, and
to more closely mirror what is done in practice, we consider training models of the form f1 , f2 :
X → Rd where (x, x0) 7→ f1(x)Tf2(x0). Plugging this into Eq (1), we solve the following bivariate
optimization problem:
minimizeEDcontrast y log(1 + exp (-fι(x)τf2(x0))) +(1 - y) log(1 + exp (fι(x)τf2(x0)))].
,	(5)
Given f1, f2, we can embed a document x according to f1 (x). We call the resulting scheme the
direct embedding approach to distinguish it from the landmark embedding approach from Section 3.
Methodology. We used the AG news topic classification dataset (Zhang et al., 2015), which has
4 classes and 30k training examples per class. We reserve 1k examples per class as labeled train-
ing data and use the remaining examples for representation learning. For all methods, we use `2-
regularized logistic regression to fit a linear classifier on the labeled data.
We compared the representations Landmark-NCE and Direct-NCE against the following
baselines: (1) standard bag-of-words (BOW), (2) bag-of-words with dimensionality reduction
(BOW+SVD), (3) representations from LDA (LDA), and (4) skip-gram word embeddings (Mikolov
et al., 2013b) (word2vec). For the NCE methods, we experiment with different neural network
architectures and numbers of landmarks but use standard settings for other training parameters. See
Appendix B for details. We note that all of these methods all of these methods ignore word order in
the final document-level representation, and all of them (with the exception of word2vec) ignore
word order in their training.
In all line plots in Figure 2, the training examples axis refers to the number of randomly selected
labeled examples used to train the linear classifier. The shaded regions denote 95% confidence
intervals computed over 10 replicates of this random selection procedure.
Baseline comparison. In the left panel of Figure 2, we visualize the semi-supervised perfomance
of NCE and the baselines. Direct-NCE outperforms all the other methods, with dramatic improve-
8
Under review as a conference paper at ICLR 2021
ments over all except word2vec in the low labeled data regime. BOW is quite competitive when
there is an abundance of labeled data, but as the dimensionality of this representation is quite large,
it performs poorly with limited samples. However, unsupervised dimensionality reduction on this
representation appears to be unhelpful and actually degrades performance uniformly. Finally, we
point out that word embedding representations (word2vec) perform quite well, but our document-
level Direct-NCE procedure is slightly better, particularly when there are few labeled examples.
This may reflect some advantage in learning document-level non-linear representations, as opposed
to averaging word-level ones.
Visualizing embeddings. For a qualitative perspective, we visualize the embeddings from NCE us-
ing t-SNE with the default scikit-learn parameters (van der Maaten & Hinton, 2008; Pedregosa et al.,
2011). To compare, we also used t-SNE to visualize the document-averaged word2vec embed-
dings. The right panels of Figure 2 shows these visualizations on the 7,600 test documents colored
according to their true label. While qualitiative, the visualization of the Direct-NCE embeddings
appear to be more clearly separated into label-homogeneous regions than that of word2vec.
Other results. We investigated the effect of the number of landmarks on the performance of
Landmark-NCE by embedding with 500, 1k, 4k, 8k, and 16k landmarks. The bottom left panel
of Figure 2 displays the results, which suggest that a larger number of landmarks is helpful, with
diminishing returns at the higher end of the scale.
We also looked into the effect of depth on the performance of Direct-NCE by training networks
with one, two, and three hidden layers. In each case, the first hidden layer has 300 nodes and the
rest have 256 nodes. The top center panel of Figure 2 displays the results, which suggest that using
deeper models for representation learning may improve downstream performance.
We also tracked the contrastive loss of the model on a holdout validation contrastive dataset. The
bottom center panel of Figure 2 plots how this loss evolves over training epochs. Along with this
contrastive loss, we checkpoint the model, train a linear classifier, and evaluate the supervised test
accuracy. We see that test accuracy steadily improves as contrastive loss decreases, suggesting that
in these settings, contrastive loss (which we can measure using an unlabeled validation set) is a good
surrogate for downstream performance (which may not be measurable until we have a task at hand).
References
Naoki Abe, Bianca Zadrozny, and John Langford. Outlier detection by active learning. In Interna-
tional Conference on Knowledge Discovery and Data Mining, 2006.
Sanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models-going beyond SVD. In Sym-
posium on Foundations of Computer Science, 2012.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi.
A theoretical analysis of contrastive unsupervised representation learning. In International Con-
ference on Machine Learning, 2019.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of Machine
Learning Research, 2003.
Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estimation
and classification: Structure and applications. Working draft, November, 3, 2005.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv:1810.04805, 2018.
Thomas L Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of the National
academy of Sciences, 101:5228-5235, 2004.
Michael Gutmann and AaPo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In International Conference on Artificial Intelligence and
Statistics, 2010.
9
Under review as a conference paper at ICLR 2021
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Matthew Hoffman, Francis R Bach, and David M Blei. Online learning for latent dirichlet allocation.
In Advances in Neural Information Processing Systems, 2010.
Alex Kulesza, N Raj Rao, and Satinder Singh. Low-rank spectral learning. In International Confer-
ence on Artificial Intelligence and Statistics, 2014.
John Langford, Ruslan Salakhutdinov, and Tong Zhang. Learning nonlinear dynamic models. In
International Conference on Machine Learning, 2009.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-
tions. In International Conference on Learning Representations, 2018.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv:1301.3781, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed repre-
sentations of words and phrases and their compositionality. In Advances in Neural Information
Processing Systems, 2013b.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems, 2019.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
2011.
Adityanarayanan Radhakrishnan, Mikhail Belkin, and Caroline Uhler. Overparameterized neural
networks can implement associative memory. arXiv:1909.12362, 2019.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y Ng. Self-taught learning:
transfer learning from unlabeled data. In International Conference on Machine Learning, 2007.
Emir H Shuford, Arthur Albert, and H Edward Massengill. Admissible probability measurement
procedures. Psychometrika, 31(2):125-145,1966.
David Sontag and Dan Roy. Complexity of inference in latent Dirichlet allocation. In Advances in
Neural Information Processing Systems, 2011.
Ingo Steinwart, Don Hush, and Clint Scovel. A classification framework for anomaly detection.
Journal of Machine Learning Research, 2005.
Marshall H Stone. The generalized Weierstrass approximation theorem. Mathematics Magazine,
1948.
Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-
tional Mathematics, 2012.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine
Learning Research, 2008.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.
In International Conference on Computer Vision, 2015.
George Udny Yule. On a method of investigating periodicities disturbed series, with special refer-
ence to Wolfer’s sunspot numbers. Philosophical Transactions of the Royal Society of London.
Series A, Containing Papers of a Mathematical or Physical Character, 226(636-646):267-298,
1927.
10
Under review as a conference paper at ICLR 2021
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Processing Systems, 2015.
11
Under review as a conference paper at ICLR 2021
A	Topic modeling simulations
The results of Section 4 show that if a model is trained to minimize the contrastive learning objective,
then that model must also recover certain topic posterior information in the corpus. However, there
are a few practical questions that remain: can we train such a model, how much capacity should it
have, and how much data is needed in order to train it? Our topic modeling simulations are designed
to study these questions.
Simulation setup. We considered a single-topic generative model where the topics θ1 , . . . , θK are
sampled from a symmetric Dirichlet(α∕K) distribution over ∆lVl and for each document, its length
is drawn from a Poisson(λ) and its topic is sampled uniformly from [K]. This model can be thought
of as a limiting case of the LDA model (Blei et al., 2003; Griffiths & Steyvers, 2004) when the
document-level topic distribution is symmetric Dirichlet(β) with β 1. In our experiments, we set
K = 20, |V| = 5000, and λ = 30, and we varied α from 1 to 10. Notice that as α increases, the
Dirichlet prior becomes more concentrated around the uniform distribution, so the topic distributions
are more likely to be similar. Thus, we expect the contrastive learning problem to be more difficult
with larger values of α.
We used contrastive models of the same form as Section 7, namely models of the form f1 , f2 where
the final prediction is f1 (x)Tf2(x0) and f1 and f2 are fully-connected neural networks with three
hidden layers. To measure the effect of model capacity, we trained two models - a smaller model
with 256 nodes per hidden layer and a larger model with 512 nodes per hidden layer. Both models
were trained for 100 epochs. We used all of the same optimization parameters as in Section 7 with
the exception of dropout, which we did not use.
To study the effect of training data, we varied the rate r at which we resampled our entire contrastive
training set from the ground truth topic model. Specifically, after every 1/r-th training epoch, we
resampled 60,000 new documents and constructed a contrastive dataset from these documents. We
varied the resampling rate r from 0.1 to 1.0, where larger values ofr imply more training data. The
total amount of training data varies from 600K documents to 6M documents.
Using the results from Section 4, we constructed the embedding φ(x) of a new document x using
1000 landmark documents, each sampled from the same generative model. We constructed the true
likelihood matrix L of the landmark documents using the underlying topic model and recovered
the model-based posterior Ltφ(χ). We measured accuracy as the fraction of testing documents for
which the MAP topic under the model-based posterior matched the generating topic. We used 5000
testing documents and performed 5 replicates for each setting of parameters.
B Semi-supervised experiment details
Methodology. We conducted semi-supervised experiments on the AG news topic classification
dataset as compiled by Zhang et al. (2015). This dataset contains news articles that belong to one of
four categories: world, sports, business, and sci/tech. There are 30,000 examples from each class in
the training set, and 1,900 examples from each class in the testing set. We minimally preprocessed
the dataset by removing punctuation and words that occurred in fewer than 10 documents, resulting
in a vocabulary of approximately 16,700 words.
We randomly selected 1,000 examples from each class to remain as our labeled training dataset,
and we used the remaining 116,000 examples as our unlabeled dataset for learning representations.
After computing representations on the unlabeled dataset, we fit a linear classifier on the labeled
training set using logistic regression with cross validation to choose the `2 regularization parameter
(nfolds = 3).
We compared our representations, Landmark-NCE and Direct-NCE, against several represen-
tation baselines.
•	BOW - The standard bag-of-words representation.
•	BOW+SVD — A bag of words representation with dimensionality reduction. We first per-
form SVD on the bag-of-words representation using the unsupervised dataset to compute
12
Under review as a conference paper at ICLR 2021
a low dimensional subspace, and train a linear classifier on the projected bag-of-words
representations with the labeled dataset.
•	LDA - A representation derived from LDA. We fit LDA on the unsupervised dataset Us-
ing online variational Bayes (Hoffman et al., 2010), and our representation is the inferred
posterior distribution over topics given training document.
•	word2vec - Skip-gram word embeddings (Mikolov et al., 2013b). We fit the skip-gram
word embeddings model on the unsupervised dataset and then averaged the word embed-
dings in each of the training documents to get their representation.
For our representation, to solve the optimization problem in Eq. (5), we considered neural network
architectures of various depths. We used fully-connected layers with between 250 and 300 nodes per
hidden layer. We used ReLU nonlinearities, dropout probability 1/2, batch normalization, and the
default PyTorch initialization (Paszke et al., 2019). We optimized using RMSProp with momentum
value 0.009 and weight decay 0.0001 as in Radhakrishnan et al. (2019). We started with learning
rate 10-4 which we halved after 250 epochs, and we trained for 600 epochs. Unless otherwise
stated, Landmark-NCE and Direct-NCE use a three-layer architecture, and Landmark-NCE
uses 8000 landmarks.
To sample a contrastive dataset, we first randomly partitioned each unlabeled document in half to
create the positive pairs. To create the negative pairs, we again randomly partitioned each unlabeled
document in half, randomly permuted one set of half documents, and discarded collisions. This
results in a contrastive dataset whose size is roughly twice the number of unlabeled documents. In
the course of training our models for the contrastive task, we resampled a contrastive dataset every
3 epochs to prevent overfitting on any one particular dataset.
Additional discussions. In the left panel of Figure 2, we additionally remark that LDA performs
quite poorly. This could be for several reasons, including that fitting a topic model directly could
be challenging on the relatively short documents in the corpus or that the document category is not
well-expressed by a linear function of the topic proportions.
C Proofs from Section 4
C.1 Proof of single topic representation lemma
Proof of Lemma 1. Conditioned on the topic vector w, x(1) and x(2) are independent. Thus,
=P(X(I) = x,x ⑵=XO)
，	P(X(I) = X)P(X⑵=x0)
K
X
k=1
K
X
k=1
P(w = ek)P(x(1) = x | w = ek)P(x(2) = x0 | w = ek)
P(X(I) = x)P(x⑵=x0)
P(w = ek | x(1) = x)P(x(2) = x0 | w = ek)
P(X⑵=x0)
η(X)Tψ(X0)
P(X⑵=X0)，
where the third equality follows from Bayes’ rule.
□
C.2 Proof of general representation lemma
Proof of Lemma 2. Fix a document X of length m and a document probability vector w. Conditioned
on the assignment of each word in the document to a topic, probability of a document factorizes as
m	mm
P(X | W)= X YWziO(Xi | Zi)= X	(Y Wzi)(Y O(Xi | Zi))= n(w)T@(X),
z∈[K]m i=1	z∈[K]m i=1	i=1
13
Under review as a conference paper at ICLR 2021
where the last line follows from collecting like terms. Using the form of g? from above, we have
=P(X ⑴=x,x ⑵=XO)
，	P(X(I) = X)P(X⑵=X0)
Rw P(X(1) = X | w)P(X(2) = X0 | w) dP(w)
P(X(I) = X)P(X⑵=X0)
Rw P(X(2) = X0 | w) dP(w | X(1) = X)
P(X⑵=X0)
Rw π(w)Tψ(X) dP(w | X(1) = X)
P(X⑵=X0)
= η(X)Tψ(XO)
=P(X⑵=X0).
C.3 Proof of polynomial representation theorem
Proof of Theorem 3. By assumption (i), there exists an anchor word ak for each topic k = 1, . . . , K.
By definition this means that O(ak | j) > 0 if and only if j = k. For each vector α ∈ Z+K such
that P αk ≤ do, create a landmark document consisting of αk copies of ak for k = 1, . . . , K. This
will result in Kd+do landmark documents. Moreover, from assumption (ii), we can see that each
do
of these landmark documents has positive probability of occurring under the marginal distribution
of x(2) for (X(I),X(2),y)〜Dcontrast, WhiCh implies g?(X,l) is well-defined for all our landmark
documents l.
Let l denote one of our landmark documents and let α ∈ Z+K be its associated vector. Since l only
contains anchor words, ψ(l)β > 0 if and only ifα = β. To see this, note that
mK
ψ(l)α =	X	Y O(li	|	zi)	≥ Y	O(ak	|	k)αk	>0.
z∈([m]) i=1	k=1
On the other hand, ifβ 6= α but Pk βk = Pk αk, then there exists an index k such that βk ≥ αk+1.
Thus, for any Z ∈ ([m]), there will be more than αk words in l assigned to topic k. Since every word
in l is an anchor word and at most αk of them correspond to topic k, we will have
m
Y O(li | zi) = 0.
i=1
Rebinding ψ(l) = (ψ0(l), . . . , ψd0 (l)) and forming the matrix L using this definition, we see that
LT can be diagonalized and inverted.
For any target degree-do polynomial Q(w), there exists a vector v such that Q(w) = hv, πd0 (w)i,
where πd0 (w) denotes the degree-d0 monomial vector. Thus, we may take θ = L-1v and get that
for any document X:
hθ,g*(x,li:M)i	= (LTv)TLTn(X)	=	E[hv,∏do(w)i	| X(I)= x] =	E[Q(w)	| X(I)=	x].	口
D	Proofs from Section 5
D. 1 Proof of error transformation guarantee
We first recall and setup some notations. For (X(I),x(2),y)〜DCOntraSt (our contrastive distribution
defined in Section 3), we let μi denote the marginal distribution of x(i). Furthermore, recall the
14
Under review as a conference paper at ICLR 2021
contrastive loss, conditional probability, odds ratio, and oracle representation functions:
Lcontrast(f) := E(χ,χo,y)〜DcoU U log(1 + ©-"*))+(1 - y) log(1 + ef (x,x0))]
?	0	P(y = 1 | x(1) = x, x(2) = x0)
f (x, X)= log 意——M (1)---------------(2)——X,
P(y = 0 | x(1) = x, x(2) = x0)
P(x(1) = x x(2) = x0)
g(XX ) := exp(f*(X,x )) = P(χ(1) = χ)P(χ(2) = χ0),
Φ? (x) := (g*(x,ll),∙∙∙,g?(x,lM))
where 1i,...,1m are landmark documents. The learned approximation to f? is f, and from it We
derive
g(x,x0) := exp 0(x,x0)),
7/ 、一 / ʌ /	7∖	ʌ /	7	∖∖
φ(x) := (g(x,lι),...,g(x,lM))
Let η(x), ψ(x) denote the posterior/likelihood vectors from Lemma 1 or the posterior/likelihood
polynomial vectors from Lemma 2. Say the length of this vector is N ≥ 1.
Our goal is to show that linear functions in the representation φ(x) can provide a good approximation
to the target function
x 7→ η (x)T θ?
where θ? ∈ RN is some fixed vector. To this end, define
R(φ) := mvinEx〜“(i)(n(x) θ - φ(x) v)2,
which is the best mean squared error achievable using the representation φ.
By Lemma 1 or Lemma 2, we know that for any x, x0 we have
g?(x, x0) = η(x)Tψ(x0).
Recall the matrix
L := ψ(l1), . . . ,ψ(lM) .
This matrix is in RN×M. If L has full row rank, then
η(x)Tθ? = η(x)T ll^θ? = φ*(x)Tv?
where
φ*(x) = (g*(x,lι),..∙,g?(x,lM))
and v? = Ltθ?. Thus, R(φ?) = 0. We will show that R(φ) can be bounded as well.
Theorem 5 (Restatement of Theorem 4). Suppose the following assumptions hold.
(1)	There is a constant σmin > 0 such that for any δ ∈ (0, 1), there is a number M0(δ) such
that for an i.i.d. sample l1, . . . , lM with M ≥ M0(δ), with probability 1 - δ, the matrix
L = [ψ(lι)… Ψ(1m )]
has minimum singular value at least σmin √M.
(2)	There exists a value gmax > 0 such that for all documents x and landmarks li
?
max{f (x, li), f? (x, li)} ≤ log gmax.
Let f be the function returned by the contrastive learning algorithm, and let
ε:
^	^
Lcontrast (f ) -
Lcontrast (f )
denote its excess Contrastive loss. For any δ ∈ (0,1) ,if M ≥ Mo(δ∕2), then with probability at
least 1 - δ over the random draw ofl1, . . . , lM, we have
,ʌ.
R(φ) ≤
kθ* k2 (1+ gmax)4 (一 r 2lθg(2∕δ)
σ2.	ε + V M
min
15
Under review as a conference paper at ICLR 2021
Proof. We first condition on two events based on the sample lι,...,lM. The first is the event that
L has full row rank and smallest non-zero singular value at least √MσmE > 0; this event has
probability at least 1 - δ∕2. The second is the event that
(p*(x,j) - p(x,j ))2 + 产Mδ
(6)
where we make the definitions
g(x,x0) := exp(f(x, x0))
P(x,xo):二	=1/(1 + e-/(X，xO))=	^(x,xo)
		1 + g(X, xo)
P? (X, XO) :	= 1/(1 + e-f?(x,x0))	=	g*(X,X0)
		1 + g?(X, XO)
By HOeffding's inequality and the fact that P and p? have range [0,1], this event also has probability
at least 1 - δ∕2. By the union bound, both events hold simultaneously with probability at least 1 - δ.
We henceforth condition on these two events for the remainder of the proof.
Since L has full row rank, via Cauchy-Schwarz, we have
R(φ) = min Ex〜μ(i)(η(x) θ - φ(x) v)2
≤ Ex〜μ(1)(η(x) θ - φ(χ) V )2
=Ex〜μ(1)((φ (x) - φ(x)) V )2
≤ Ex〜μ(1) kv*k2 l∣φ*(X)T- φ(x)∣∣2
=kv?k2 ∙ Ex〜μ(1) I。?(X)T- ^x)||2 .
We analyze the two factors on the right-hand side separately.
Analysis of V? . For V? , we have
kv?k2 ≤∣∣Lt∣∣jkθ?k2 ≤ Mσ12- kθ*k2,
where we have used the fact that L has smallest non-zero singular value at least √Mσma.
Analysis of φ? - φ. For the other term, we first note that
p?(X, X0) = 1/(1 + e-f?(x,x0)) = P(y = 1 | X(1) = X, X(2) = X0).
Thus, we have
^	∕^	7?\
ε = Lcontrast (f ) - Lcontrast (f )
=E(x，x0，y)~Dcoz ylog (⅜⅛x)) + (I-y)log ('ip((X；))]
= E(XH) ~Da卜χ,χ0)log(p^By) + (I- p? (χ,χ0))log (⅛p⅛⅛)#
E(X,xO)~Dcontrast KL(p(X,X0),p?(X,X0))
where KL(p, p0 ) denotes the KL-divergence between two Bernoulli distributions with biases p and
p0, respectively. Pinkser’s inequality tells us that KL(p,p0) ≥ 2(p - p0)2. Combining this with the
fact that Dcontrast is a mixture distribution that places half its probability mass in μ(1) 0 μ(2) implies
ε ≥ 2E(X,X0)〜DContraSt [(p(X,xO)- P* (X,XO)) E(X,X，)〜兴⑴即⑶[(P(X,XO)- P* (X,XO))	∙
16
Under review as a conference paper at ICLR 2021
Combining the above with Eq. (6) and the definitions of p^, p?, We have
Ex〜μ⑴(O*(X)- φ(X)IL
M
X Ex 〜μ(1)(g*(x,lj ) - g(X,lj ))2
j=1
≤
≤
≤
M
(1 + gmax)4 X Ex〜μ(D (P*(X,lj) - P(X,lj))2
j=1
2ɪ /2log(2∕δ)
+ V M
Wrapping up. To conclude, we have
□
D.2 Satisfying Assumption 1
Suppose we are in the single topic case where w ∈ {e1, . . . , eK}. Assume that mink Pr(w = ek) ≥
wmin. Further assumes that each topic k has an anchor word ak, satisfying O(ak | z = ek) ≥
amin . Then we will show that when M and m are large enough, the matrix L whose columns are
ψ(X)∕P(X) will have large singular values.
First note that if document X contains ak then ψ(X) is one sparse, and satisfies
if ak ∈ x:	ψ(x)
P(x | w = ek)
Pk0 P(W = k0)P(x | W = k0) ek
1	,
P(W = k0) ek.
Therefore, the second moment matrix satisfies
K
E ψ(X)ψ(X)T	XP(w = ek)P(ak ∈ X | ek)E ψ(X)ψ(X)T | ak ∈ X, w = ek
k=1
XP(ak ∈ X | ek ) T
一 P(W = ek) kkk..
Now, if the number of words per document is m ≥ 1/amin then
P(ak ∈ X | ek) = 1 - (1 - O(ak | ek))m ≥ 1 - exp(-mO(ak | ek))
≥ 1 - exp(-mamin )
≥ 1 - 1/e.
Finally, using the fact that P(w = ek) ≤ 1, we see that the second moment matrix satisfies
E [ψ(x)ψ(x)T]占(1 一 1∕e)Iκ×κ.
For the empirical matrix, we perform a crude analysis and apply the Matrix-Hoeffding inequal-
ity (Tropp, 2012). We have ψ(X)ψ(X)T2 ≤ Kwm-i2n and so with probability at least 1 一 δ, we
have
卜 K log(K∕δ)
≤ V	MWmin
2
If we take M ≥ Ω(Klotg(K∕δ)/w2nn) then we will have that the minimum eigenvalue of the
empirical second moment matrix will be at least 1/2.
17