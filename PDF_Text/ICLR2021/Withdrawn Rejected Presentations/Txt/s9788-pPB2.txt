Under review as a conference paper at ICLR 2021
LLBoost: Last Layer Perturbation to Boost
Pre-trained Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
While deep networks have produced state-of-the-art results in several domains
from image classification to machine translation, hyper-parameter selection re-
mains a significant computational bottleneck. In order to produce the best possible
model, practitioners often search across random seeds or use ensemble methods.
As models get larger, any method to improve neural network performance that
involves re-training becomes intractable. For example, computing the training ac-
curacy of FixResNext-101 (829 million parameters) on ImageNet takes roughly
1 day when using 1 GPU.
In this work, we present LLBoost, a theoretically-grounded, computationally-
efficient method to boost the validation accuracy of pre-trained over-
parameterized models without impacting the original training accuracy. LLBoost
adjusts the last layer of a neural network by adding a term that is orthogonal to the
training feature matrix, which is constructed by applying all layers but the last to
the training data. We provide an efficient implementation of LLBoost on the GPU
and demonstrate that LLBoost, run using only 1 GPU, improves the test/validation
accuracy of pre-trained models on CIFAR10, ImageNet32, and ImageNet. In the
over-parameterized linear regression setting, we prove that LLBoost reduces the
generalization error of any interpolating solution with high probability without
affecting training error.
1	Introduction
Over the past decade, deep networks have produced a number of state-of-the-art results including
surpassing human performance on the ImageNet classification task (26; 14). However, tuning hyper-
parameters to produce the best possible neural network in a given application remains a computa-
tionally expensive task. State-of-the-art results often involve selecting the best model using multiple
random seeds (14; 27; 12; 28) or ensembling (15), and training even a single model can take sev-
eral days even when using multiple GPUs. Hence, it is critical to identify computationally efficient
approaches to improve the performance of pre-trained models without the need of re-training them.
In this work, we present LLBoost, a theoretically-grounded, computationally-efficient method to
boost the validation accuracy of pre-trained, over-parameterized models without impacting the train-
ing accuracy. Figure 1 provides an overview of our method as well as the main results. As shown
in Figure 1A, our method adjusts the last fully-connected layer of a neural network by selecting
the best performing perturbation out of several orthogonal to the training feature matrix, which is
constructed by applying all but the last layer to the training data. In Figure 1B, we provide an ex-
ample showing how our method applied to a model trained under a poorly chosen random seed can
boost validation accuracy comparable to that of a model trained under a better seed. Lastly, Figure
1C shows that our method can significantly improve the validation accuracy of pre-trained neural
networks on large datasets using a fraction of the training time.
The intuition for our method is based on characterizing the benefit of random initialization in over-
parameterized linear regression. In particular, consider a dataset (X, y) ⊂ Rd×n × R1×n with n < d
for which there exists w* ∈ R1×d such that y = w*X. In order to estimate w* from the data, We
use gradient descent with learning rate η and initialization w(0) to minimize the squared loss, i.e. to
solve:
arg min 1 ∣∣y - WX∣∣2.
w∈Rd 2
1
Under review as a conference paper at ICLR 2021
It is well-known (8) that gradient descent converges to the following closed-form solution:
Wr = W⑼(I - X(XTX)tXT) + yXt,
where At is the pseudo-inverse of A (See Appendix A for a proof). In this work, We prove that
when n < d, sampling W(0) on a hyper-sphere leads to Wr with lower generalization error than
the minimum norm solution yXt with constant probability. Since the last layer of modern deep
networks is usually linear, we apply this result from regression to the last layer of networks to arrive
at our method, LLBoost.
We end the introduction with a summary of our main contributions:
1.	We present LLBoost, a computationally efficient method for boosting the validation accu-
racy of pre-trained neural networks without affecting training accuracy. Additionally, we
provide an efficient implementation of LLBoost on the GPU.
2.	We provide a wealth of empirical evidence that LLBoost improves the validation accuracy
of pre-trained neural networks and prove that it does not affect training accuracy for over-
parameterized models.
3.	We provide evidence that LLBoost yields a computational advantage over random seed
search for training neural networks.
4.	In the over-parameterized linear regression setting, we prove that LLBoost reduces the test
error of any interpolating solution with high probability without affecting training error.
2	Related Work
Understanding the benefits of over-parameterization is a recent topic of major interest in machine
learning. (2) introduced the double descent curve showing that when increasing model capacity
past the interpolation threshold, test error can drop a second time. (20) noted that this phenomenon
Training Images
Pre-trained Network
Dataset: 100 Ex. ImageNet32 Model: Pret-trained ResNet-18	
Seed	Val Acc.
200	80%
3	78%
70	85%
25	80%
500	82%
Our Method on Seed 3	85%
(B)
Dataset	Model	Train/Val. Acc. (Original)	Train/Val. Acc. (OUrs)	Training Time	Correction Time
100 Ex. ImageNet32	ResNet-50	100% / 83%	100% / 89%	164.07 sec	0.12 sec
CIFAR10	ResNet-18	99.99%∕95.05%	99.99%∕95.25%	1.35 hr	15.36 min
ImageNet	FixResNext-101	94.92%/86.26%	94.92%/86.34%	>1 day/epoch	7.59 hr
(C)
Figure 1: An overview of LLBoost and a demonstration of the benefits of our method in boosting
pre-trained models. (A) A breakdown of LLBoost, which improves the performance of pre-trained
neural networks by adjusting the last layer of a pre-trained network with a term orthogonal to the
training feature matrix, X . (B) LLBoost applied to a model trained on a poorly-chosen seed boosts
performance to that of a model trained on a well-chosen seed. (C) LLBoost provides a boost to the
pre-trained models at a fraction of the computational cost of training.
2
Under review as a conference paper at ICLR 2021
had been noticed empirically several decades prior to the work of (2). A line of recent work have
provided a theoretical analysis of double descent in linear regression (13; 3; 1; 22; 5; 21). In partic-
ular, (13; 3) analyzed the generalization error for the minimum norm solution in over-parameterized
linear models. These works proved that the minimum norm solution can yield lower generalization
error in the over-parameterized regime than in the under-parameterized regime. Our theoretical anal-
ysis primarily builds on that of (3), but we analyze the generalization error of random-initialization
interpolating solutions instead of the minimum-norm solution.
Various methods, including random seed search and ensembling, are standardly used to improve
the performance of machine learning models through training. (4) recommends using 5-10 random
seeds if computational power is available, and this is typically done in state-of-the-art models; for
example, (14) considered 5 random seeds for ResNet-110 and (12) considered at least 2 random
seeds on CIFAR10 (17). (9) rigorously analyzed the impact of random seeds on model performance
by considering between 50-400 random seeds for models of varying depth on MNIST (18). Figure
1 from their work demonstrates that random seeds can affect the validation accuracy of models by
up to a few percentage points on MNIST. Since this comes at a significant computational price, it
is critical to identify methods that can obtain such boosts without having to perform random seed
search, which is the topic of this paper.
Another popular approach to improving neural network performance is ensembling, for example via
bagging (6) or boosting (10). Recently, (15) presented an approach to ensembling, which involved
training a single network and saving the model parameters each time training stopped at a local
minimum. (15) built upon (30), which introduced horizontal and vertical voting to ensemble a
model across training epochs.
LLBoost can be used in combination with all the above approaches to further boost their perfor-
mance, since it only adjusts the last layer of a pre-trained model without requiring any training.
3	Preliminaries and Method
We now present the preliminaries relevant for formally introducing LLBoost. Since our method is
derived from over-parameterized linear regression, we first describe this problem setting.
Noiseless Regression. Let (X, y) ⊂ Rd×n ×R1×n denote the training dataset where dis the number
of dimensions and n is the number of samples. In noiseless linear regression there exists w* ∈ R1×d
such that y = w*X. In order to estimate w* from the data, We use gradient descent to minimize the
squared loss. Formally, we solve:
arg WmRd 1 ky -wXk2
(1)
using gradient descent With learning rate η, Which proceeds according to the updates:
w(t+1) = w(t) + η(y - w(t)X)XT.
The folloWing Well-knoWn theorem (8) gives the solution, w(∞), for the objective (1) given by
gradient descent.
Theorem 1. Let (X, y) ⊂ Rd×n × R1×n with X of rank r < d, let λmax denote the largest
eigenvalue of XXT, and let At denote the Pseudo-inverse ofa matrix A. Given initialization w(0) ∈
Rd, gradient descent used to solve:
arg WmRd 1 ky -wXk2
with learning rate η < λ-m1ax converges to w(∞) = w(0) (I - X(XT X)tXT) + yXt.
When w(0) = 0 (zero initialization), w(∞) is the minimum '2-norm solution for solving the linear
system wX = y. The folloWing property of w(∞) is used to demonstrate that LLBoost does not
impact training error.
Lemma 1. Let w(∞) = w(0)(I - X(XT X)tXT) + yXt. Then, w(0) (I - X(XTX)tXT) ⊥ yXt.
3
Under review as a conference paper at ICLR 2021
The proof follows directly by substituting in X = UΣVT given by the singular value decomposition
(SVD). Lemma 1 implies that LLBoost does not affect training predictions since it only ever adds a
component orthogonal to the span of the training feature matrix. We primarily focus on the noiseless
regression setting, but we note that our theory directly extends to the Gaussian model from (3) and
(7).
We now present a result from random matrix theory, which is at the basis of our results. The proof of
the following lemma relies on the rotation invariance of the multivariate Gaussian distribution and
is presented in Appendix B.
Lemma 2. If X ∈ Rd×n with columns x(i) 〜N (0, Id×d) ,then the singular vectors of X are uni-
formly distributed on the unit sphere Sd-ι. In particular, if X 〜N(0, Id×d), then 奇 is uniformly
distributed on the unit sphere Sd-1.
Method Overview. Given the above preliminaries, we present LLBoost in Algorithm 1. LLBoost
takes in the following inputs: (1) the training feature matrix X ; (2) the validation feature matrix
Xt ; (3) the validation labels yt; (4) the last layer weights w; (5) the number of samples to consider
T ; and (6) the radius of the hyper-sphere to sample from γ. LLBoost begins by constructing the
projection matrix P = I - X(XTX)*XT. Then, LLBoost samples Z 〜N(0, I), normalizes Z and
adds the perturbation γzP to w . LLBoost repeats this process and returns the perturbation that most
improves the validation accuracy.
Remarks. As shown in Figure 1A, X and Xt are constructed by applying all but the last layer of
the neural network to the training/test data. If X is not full rank (as is the case for neural networks
that are not over-parameterized), we can create a low rank X by truncating the lower singular values
of X . This correction only mildly affects training error as is shown in Section 4. For classification
settings, yt should be converted to 1-hot vectors. When the last layer has a bias, one can simply
append a 1 to w and append an extra row of 1’s to X and Xt . In most of our experiments, we
consider up to T = 100, 000 samples. When selecting γ, one can utilize a binary search approach
starting with Y = √ and either doubling or halving Y depending on whether the validation accuracy
increased or decreased respectively. However, as a simple heuristic one can start with γ =√1d kwk
(as is recommended by Theorem 3). Lastly, when the network has multiple outputs (as in multi-
class problems), we let the entries of the weight matrix be i.i.d. standard normal variables and then
normalize by the Frobenius norm.
LLBoost does not impact training error/accuracy. Since the perturbations constructed by LL-
Boost are always projected into the space orthogonal to the training features X, Lemma 1 implies
that LLBoost does not alter the training predictions. This is an important aspect of our algorithm
since otherwise, LLBoost could just overfit the test data.
Algorithm 1 LLBoost (X, Xt , yt , w, T, Y)
Input: X := rank r training feature matrix; Xt := val. feature matrix; yt := val. labels; w := last
layer weights; T := number of samples; Y := radius of hyper-sphere
Output: wbest := adjusted last layer weights
None
二 0; (1, d) = w.shape; U, Σ, VT — SVD(X)
1
2
3
4
5
6
7
8
9
10
11
12
13
wbest =
accbest
P J Id×d - UIr ×r UT
for t J 1 to T do
ZJN(0,Id×d)
Z J kzk
wr J YZP + w
acc = GetV alAcc(Xt, yt, wr )
if acc > accbest then
wbest = wr ; accbest = acc
end if
end for
return wbest
4
Under review as a conference paper at ICLR 2021
DataSet	Model	Train/Val. Acc. (Original)	Train∕Val. Acc. (Ours)	Train∕Val. Acc. (Standard Normal)
100 Ex. ImageNet32	ResNet-18	100%∕80%	100%∕84%	88%∕81%
2600 Ex. ImageNet32	ReSNet-18*	99.96%∕95%	99.96%∕97%	99.96%∕95%
100 Ex. ImageNet32	ResNet-34	100%∕85%	100%∕87%	74%∕82%
2600 Ex. ImageNet32	ResNet-34*	99.77%∕95%	99.77%∕98%	99.65%∕97%
100 Ex. ImageNet32	ResNet-50	100%∕83%	100%∕89%	84%∕83%
200 Ex. ImageNet32	ResNet-50	100%∕87%	100%∕93%	79.5%∕87%
500 Ex. ImageNet32	ResNet-50	99.6%∕91%	99.6%∕93%	93.2%∕92%
800 Ex. ImageNet32	ResNet-50	100%∕94%	100%∕98%	97.75%∕96%
1000 Ex. ImageNet32	ResNet-50	100%∕93%	100%∕97%	98.5%∕95%
2000 Ex. ImageNet32	ResNet-50	99.85%∕95%	99.85%∕97%	98.8%∕97%
2600 Ex. ImageNet32	ResNet-50*	99.88%∕95%	99.88%∕99%	99.69%∕98%
CIFAR10	ResNet-18*	99.99%∕95.05%	99.99%∕95.25%	42.53%∕36.46%
ImageNet	FixResNext-101*	94.924%∕86.26%	94.924%∕86.34%	.57%∕.56%
Figure 2: LLBoost consistently improves the performance of pre-trained neural networks across a
variety of experiments on ImageNet32, CIFAR10, and ImageNet without impacting training accu-
racy. The last column indicates that applying a standard perturbation instead of LLBoost to the last
layer of a neural network can significantly impact training and validation accuracy. *'s indicate that
the training feature matrix X was full rank, and thus, we used a low rank approximation of X for
LLBoost. See Appendix F for a description of how the rank was chosen, and an analysis of how low
rank approximations affect train/validation accuracy.
GPU Implementation. We can easily remove the loop in Algorithm 1 by using the GPU since
the dominating computations are vector-matrix products. A full implementation is provided using
PyTorch (25) and NumPy (23; 29) in the footnote below1. In practice, computing the validation
accuracy for each sample of wr is the limiting factor in LLBoost. For large datasets, the functions
used to compute the SVD and the projection matrix P need to be selected carefully. Note that with
respect to SVD, the reduced SVD is sufficient for our algorithm. While for the ImageNet training
features from FixResNext-101 the full SVD would require constructing a matrix with roughly 1012
entries, the reduced SVD only requires 108 entries.
4	Empirical Results
In Figure 2, we provide empirical evidence that LLBoost consistently improves the validation ac-
curacy of state-of-the-art models on CIFAR10, ImageNet32 (24), and ImageNet. For models fine-
tuned2 * on ImageNet32 or CIFAR10, we provide all training details in Appendix C. Importantly, as
proven by Lemma 1, we note that the training accuracy remains unchanged after applying LLBoost.
Handling under-parameterized models. If n denotes the number of training samples, the size of
the training feature matrix is 512 × n for ResNets-18 and 34 while it is 2048 × n for ResNet-50
and FixResNext-101 (28). Hence ResNet-18 and 34 are over-parameterized on the training set when
n < 512, while ResNet-50 and FixResNext-101 are over-parameterized for n < 2048. When the
training feature matrix, X , has full rank, we truncate the lower singular values of X in order to be
able to search the space orthogonal to X. Formally, if X = UΣV T, we construct Xb = UΣbVT
where we zero out the bottom entries of Σ. When Xb has rank k, we refer to it as the rank k
approximation of X .
Our experiments are summarized as follows:
1.	On ImageNet32, we fine-tune pre-trained ResNets of varying depth on subsets of varying
size of two classes (Kit-Fox vs. English-Setter). There are a total of 2600 examples (1300
of each class). We then apply LLBoost to these pre-trained models and use a rank 12
1 https://anonymous.4open.science/r/b11fa900- efbf- 45ae- aee8- a264d954cb51/
2By fine-tuning, we mean training a pre-trained model on a new dataset. This is discussed further in the
transfer learning tutorials provided by PyTorch (25).
5
Under review as a conference paper at ICLR 2021
approximation for the feature matrix when necessary. This set of experiments demonstrates
that LLBoost improves transfer learned models on small datasets.
2.	On CIFAR10, we fine-tune a pre-trained ResNet-18. We then apply LLBoost to the pre-
trained network. In this setting, there are 50, 000 training examples and so the feature
matrix is full rank. To make the model over-parameterized, we instead use the rank 212
approximation. Remarkably, this approximation does not impact the original training ac-
curacy.
3.	On ImageNet, we apply LLBoost to pre-trained FixResNext-101. ImageNet contains
1, 281, 167 training examples and the feature matrix is again full rank. In this case, we use
the rank 1000 approximation, and the original training accuracy decreases from 95.193%
to 94.924% when applying the original weights to the rank 1000 approximation.
In all settings, LLBoost improves validation accuracy. In addition, as proven by Lemma 1, there is
no impact on training accuracy in the over-parameterized setting. In under-parameterized settings
such as FixResNext-101 on ImageNet, our experiments indicate only minor decrease in training
accuracy when using a low rank approximation to X . In Appendix D, we additionally present: (1)
the difference in '2/Frobenius norm between the LLBoost weights and the original weights; (2) the
run-time of LLBoost; (3) the choice of γ used in the experiments. In Appendix F, we discuss how
the rank was chosen when approximating full rank feature matrices, and provide an analysis of how
low rank approximations affect train/validation accuracy.
Remarks. In the last column of Figure 2, we also provide the performance of perturbing the last
layer of a neural network with a usual distribution such as the standard normal. Note that such a
standard normal perturbation significantly impacts the original training accuracy; in fact for larger
datasets such as ImageNet, the training and validation accuracies drop below 1%. Hence, without
the projection operator, perturbations to the last layer can be devastating to network performance. In
Appendix E, we also demonstrate that when including the projection operator but using standard nor-
mal initialization (instead of uniform on the hyper-sphere as done in LLBoost) can similarly reduce
validation accuracy. While we have thus far concentrated on validation accuracy, in Appendix G we
show that in the setting of training, validation, and test splits, LLBoost not only improves validation
accuracy, but also improves test accuracy without impacting training accuracy.
Improvement over Random Seed Search. We demonstrate in Figure 3 that LLBoost provides a
significant computational advantage over random seed search. In particular, in Figure 3 we com-
pare the performance of fine-tuning pre-trained models on ImageNet32 and CIFAR10 using random
seeds. Naturally, LLBoost can be applied to boost the performance of all these models. Moreover,
Figure 3 illustrates that LLBoost can boost the performance of a model trained using a poor random
seed to that of a model trained using a well-selected seed. Since LLBoost only takes a fraction of
the training time, these experiments identify LLBoost as a computationally efficient alternative to
random seed search.
5	Theoretical Results
We now present our derivation and analysis of LLBoost for over-parameterized noiseless linear
regression. In the noiseless regression setting, we assume that the labels y ∈ R1×n are generated by
the product of w* ∈ R1×d and data X ∈ Rd×n. Let Wr = W(O)(I — X(XTX户XT) + yXt and
W = yX t denote interpolating solutions for the linear system wX = y given by gradient descent
from initialization W(0) and 0 respectively. In this section we establish the following:
1.	Generalization bounds on Wr .
2.	There exist infinitely many W(0) such that kWcr —W* k < kWb—W* k, i.e. there exist infinitely
many random initializations that out-perform the minimum norm solution. In particular, we
show that even if kW(0) — W* k is large, kWcr — W* k can be arbitrary close to 0.
3.	Sampling W(0) uniformly on the hyper-sphere of appropriate radius leads to Wcr out-
performing the minimum norm solution with constant probability.
The following proposition (with proof in Appendix H) compares the generalization error of the
random initialization solution, Wcr and the minimum norm solution, Wb.
6
Under review as a conference paper at ICLR 2021
Seed	Val Acc.
200	94%
3	94%
70	97%
25	94%
500	95%
LLBooston Seed 200	98%
Seed	Val Acc.
200	95%
3	95%
70	95%
25	97%
500	95%
LLBooston Seed 3	97%
Seed	Val Acc.
200	95.05%
3	94.98%
70	95.06%
25	95.14%
500	95.1%
LLBooston Seed 3	95.18%
(A)	800 ExjmageNet32
Classes 1,2
ResNet-50
(B)	2000 ExjmageNet32
Classes 1,2
ResNet-50
(C)	CIFAR10
ResNet-18
Figure 3: LLBoost provides a computational advantage over random seed search. While LLBoost
not only improves the performance of every pre-trained model above, our method boosts the per-
formance of a model trained using a poor random seed to at least that of a model trained using a
well-selected seed at a fraction of the computational cost.
Proposition 1. Let X = UΣVT denote the SVD of X and let Σ⊥ = I 一 Σ∑L Then,
(a)	M - w*∣∣ = ∣∣w*UΣ⊥UTk,
(b)	∣∣Wr — w*k = k(w(0) 一 w*)UΣ⊥UTk ≤ ∣∣w(0) 一 w*∣∣.
By using Proposition 1 and extending the proof technique of (3), we establish the following gener-
alization bounds for the solution starting from initialization w(0).
Theorem 2.	Assuming data x, x(i 〜N (0, Id×d) for i ∈ [n] ,then
⑴ Eχ,χ[(y 一 Wx)2] = EX[∣W - w*k2] = ∣∣w*k2(1 - 1),
⑵ Eχ,x[(y - Wrx)2]= EX[∣Wr - w*k2] = kw⑼一 w*k2(1 - d).
The proof is presented in Appendix I. From Theorem 2, the out of sample performance of the solu-
tion initialized at w(0) is governed by the distance between w(0) and w*, which matches intuition.
While this result is in expectation over the data drawn from an isotropic Gaussian, we now prove
the remarkable fact that even if ∣∣w(0) 一 w*∣ = ci is large, ∣C - w*k Can be any value between 0
and c1 .
Proposition 2. Let r denote the rank of the data matrix X and let c1 ≥ c2 ≥ 0. Then there exists
W(O) such that ∣∣w(0) — w*∣ = ci and kWr — w*k = c2∙
The full proof is presented in Appendix J. Importantly, Proposition 2 provides the following intuition
around the benefit of random initialization in over-parameterized models. When X has rank r for r
small, the space orthogonal to X is large with dimension dτ. There is thus a priori no reason for the
true w* to lie in the span of the training data. Hence, by sampling w(0) to add a term orthogonal to
X we can expect to obtain a boost in performance over the minimum norm solution. The following
proposition and theorem present a sampling method for W(0), which provably provides a boost over
the minimum norm solution with constant probability.
Proposition 3. Let Ud represent the uniform distribution on the sphere. Assume that the data
x,x(2)〜 N(0,Id×d) for i ∈ [n] and that w(0)〜lUd. Then,
Pw(0) (Eχ,x[(y - Wrx)2] ≤ Eχ,x[(y - Wbx)2])= Ar2-1、Z sind-2 θdθ, (2)
VZnr (~1~) Jo
where φ = cos-1 (2jWrj) and Γ(x) denotes the Gamma function, which satisfies Γ(x) = (x —
1)Γ(x — 1) with Γ(1) = 0 and Γ(2) = √π.
7
Under review as a conference paper at ICLR 2021
The proof is given in Appendix K. The benefit of Proposition 3 is that we can lower bound the right
hand side of Equation (2) by a constant greater than 0. This is computed explicitly in Theorem 3.
Theorem 3.	Let Zw(0) = Ex,X [(y - wbx)2] - Ex,X [(y - wbr x)2] denote the difference in expected
generalization error between the minimum '2 -norm solution and the random initialization solution.
,.	γ+ 工
If wo 〜Ud(Y), c ≥ 0, and K = 2jwγk, then
≤ Pw(0) (Zw(0) ≥ e (1 - d)) ≤ 2
In particular, if ∣∣w*k = 1, Y = 表,and E = d, then K = 2√d and
1
2
1
lim Pw(0)
d→∞
(3)
Remarks. Theorem 3 is at the core of LLBoost; it implies that the probability of wcr having lower
expected generalization error than wb is constant. Note that based on Equation (3) this constant
is lower bounded by roughly .3 (meaning we get a 30% chance of improvement). Importantly,
Theorem 3 can be trivially extended to demonstrate that LLBoost can reduce the generalization
error of any interpolating solution with constant probability (provided that the generalization error
is not already 0). We also remark that the lower bound in Equation (3) is nearly tight: in practice,
this constant is roughly .31. Lastly, note that all of the above theory holds also for the regression
setting with multiple outputs by replacing the usual `2 norm with the Frobenius norm.
While the full proof of Theorem 3 is presented in Appendix L, we here provide a sketch. The
proof provides an approximation for the integral on the right hand side of Equation (2) by using
Gautschi’s Inequality (11) and integration by parts. Note that while it may seem at first that φ should
be uniformly distributed when w(0) is uniformly distributed on the sphere (and thus the probability
should go to 1/2 as d goes to ∞), it is in fact not true that φ is uniformly distributed when w(0) is
uniformly distributed, which is why a more precise argument is required.
6 Discussion
In this work, we presented LLBoost, a theoretically-grounded, computationally-efficient method to
improve the validation accuracy of pre-trained neural networks without impacting training accuracy.
Through a variety of experiments on ImageNet32, CIFAR10, and ImageNet, we demonstrated that
our method is practically relevant and can be used to boost the performance of state-of-the-art pre-
trained models. A standard method to improve the performance of neural networks is random seed
search. We showed that LLBoost provides a significant computational advantage over random seed
search and can even boost the performance of a model trained on a poorly selected seed to that of
a model trained on a well-selected seed. Thus, LLBoost is a computationally efficient alternative
to random seed search. Lastly, we provided theoretical footing for our method by showing that
LLBoost provably reduces the test error of any interpolating solution in over-parameterized linear
regression. An interesting direction of future work is to identify alternate sampling schemes for
LLBoost (other than uniform on the hyper-sphere) that provably yield a greater increase in validation
accuracy without decreasing training accuracy.
8
Under review as a conference paper at ICLR 2021
References
[1]	Peter L. Bartlett, Philip M. Long, Ggbor Lugosi, and Alexander Tsigler. Benign overfitting in
linear regression. Proceedings of the National Academy of Sciences, 2020.
[2]	Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National
AcademyofSciences,116(32):15849-15854, 2019.
[3]	Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features.
arXiv:1903.07571, 2019.
[4]	Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures.
CoRR, abs/1206.5533, 2012.
[5]	K Bibas, Y. Fogel, and M. Feder. A new look at an old problem: A universal learning approach
to linear regression. arXiv preprint arXiv:1905.04708, 2019.
[6]	Leo Breiman. Bagging predictors. Machine Learning, 24(2):123-140, 1996.
[7]	Leo Breiman and David Freedman. How many variables should be entered in a regression
equation? Journal of the American Statistical Association, 78(381):131-136, 1983.
[8]	Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems,
volume 375. Springer Science & Business Media, 1996.
[9]	Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent,
and Samy Bengio. Why Does Unsupervised Pre-training Help Deep Learning? Journal of
Machine Learning Research (JMLR), 11:625-660, 2010.
[10]	Yoav Freund and Robert Schapire. A short introduction to boosting. Journal of Japanese
Society for Artificial Intelligence, 14(5):771-780, 1999.
[11]	Walter Gautschi. Some Elementary Inequalities Relating to the Gamma and Incomplete
Gamma Function. Journal of Mathematics and Physics, 38(1-4):77-81, 1959.
[12]	Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, and Mohammad
Sabokrou. Lets keep it simple, using simple architectures to outperform deeper and more
complex architectures. arXiv preprint arXiv:1608.06037, 2016.
[13]	Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
[14]	Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Computer Vision and Pattern Recognition (CVPR), 2016.
[15]	Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Wein-
berger. Snapshot ensembles: Train 1, get m for free. In International Conference on Learning
Representations (ICLR), 2017.
[16]	Like Hui and Mikhail Belkin. Evaluation of Neural Architectures Trained with Square Loss vs
Cross-Entropy in Classification Tasks. arXiv preprint arXiv:2006.07322, 2020.
[17]	Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis,
University of Toronto, 2009.
[18]	Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
[19]	Shengquiao Li. Concise formulas for the area and volume of a hyperspherical cap. Asian
Journal of Mathematics and Statistics, 4(1):66-70, 2011.
[20]	Marco Loog, Alexander Mey, Jesse H. Krijthe, and David M. J. Tax. A brief prehistory of
double descent. Proceedings of the National Academy of Sciences, 117(20):10625-10626,
2020.
9
Under review as a conference paper at ICLR 2021
[21]	Partha P. Mitra. Understanding overfitting peaks in generalization error: Analytical risk curves
for l2 and l1 penalized interpolation. arXiv preprint arXiv:1906.03667, 2019.
[22]	Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless
interpolation of noisy data in regression. IEEE Journal on Selected Areas in Information
Theory ,1(1):67-83,2020.
[23]	Travis E Oliphant. A guide to NumPy, volume 1. Trelgol Publishing USA, 2006.
[24]	Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuglu. Pixel recurrent neural net-
works. In International Conference on Machine Learning (ICML), 2016.
[25]	Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
[26]	Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Fei-Fei
Li. ImageNet large scale visual recognition challenge. International Journal of Computer
Vision, 2015.
[27]	Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
In Conference on Computer Vision and Pattern Recognition, 2015.
[28]	Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve J6gou. Fixing the train-test reso-
lution discrepancy. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[29]	Stefan Van Der Walt, S Chris Colbert, and Gael Varoquaux. The numpy array: a structure for
efficient numerical computation. Computing in Science & Engineering, 13(2):22, 2011.
[30]	Jingjing Xie, Bing Xu, and Zhang Chuang. Horizontal and vertical ensemble with deep repre-
sentation for classification. In ICML Workshop on Representation Learning, 2013.
10
Under review as a conference paper at ICLR 2021
Appendix
A Gradient Descent in Linear Regression
Theorem 4. Let (X, y) ⊂ Rd×n × Rn with X of rank r and X = UΣVT its singular value
decomposition (SVD). Given an initialization w(0) = 0, gradient descent used to solve:
arg min 1 ∣∣y - WX∣∣2.
w∈Rd 2
with learning rate η < λ-(XXT)converges to:
	上	0	... 0 σ1
	
w(∞) = yVΣ*UT, where Σ* =	0	σ12	... 0	0r×d-r
	0	...	ɪ σr
	
	0d-r×r	0d-r×d-r
Proof. Let S = XXT and S0 = yXT. Then, w(t+1) = w(t) (I - ηS) + ηS0. Now we directly solve
the recurrence relation; namely,
w(t) =ηS0((I-ηS)t-1+(I-ηS)t-2+...+(I-ηS)1+I).
Let X = UΣV T denote the singular value decomposition of X where {σ1, . . . , σr} are the non-
zero entries of Σ and r is the rank of X. Then, S = UΣ2UT, and S0 = yV ΣUT. Thus, we can
simplify the recurrence relation:
w(t) = ηS0U((I - ηΣ2)t-1 + (I - ηΣ2)t-2 +...+(I-ηΣ2)1+I)UT.
Since (I 一 η∑2)t-1 + (I 一 η∑2)t-2 + ... + (I 一 η∑2)1 + I is a geometric series, for η < σ12, we
have:
w(t) = ηS0UΣ+UT,
	-1-(1-ησ2)t ησ2	0	...0
Σ+ =	0	1-(1-ησ2)t ησ2	...0	0r × d—r
	0		1-(1-ησ2)t
		...	ησ2
		0d-r×r	tId-r×d-r
Now substituting in S0 = yV ΣUT gives us:
W㈤=yV ΣtU T,
	- 1—(1—ησ2)t σι	0	...0	-	
Σt =	0	1-(1—ησ2)t σ2	...0	0rxd-r
	0	...	1—(1-ησ" σ r	
		0d-r×r		0d—rxd—r_
Lastly, we can take the limit as t → ∞ to conclude that
w(∞) = lim WItt = yVΣ*UT, where ∑'
t→∞
1
σι
0
0
0	...0
σ^	…0	0r×d-r
1
...
σr
0d-r×r	0d-r×d-r
□
Note that the proof above can be easily extended to the setting of a random initialization W(0).
11
Under review as a conference paper at ICLR 2021
B Distribution of Singular Vectors of a Random Matrix
Proof. We use the rotational invariance of the multivariate isotropic Gaussian. If A is an orthonor-
mal matrix, then we have:
xTI-1x = xTATI-1Ax = (Ax)TI-1 (Ax).
Now, suppose A, B are both orthonormal matrices, then we have:
AXBT = (a 0 B)Xv,
where Xv ∈ Rdn is the row-major vectorization of X and 0 is the Kronecker product. Now, since
A, B are orthonormal, we have that A 0 B is orthonormal. Hence, AXBT must have the same
distribution as X, and thus the singular vectors of AXBT must have the same distribution as those
ofX. Since singular vectors lie on Sd-1 and since the distribution is rotation invariant, we conclude
that the singular vectors are uniformly distributed on Sd-1.	□
C Training Details
We now describe the training methodology we used to train pre-trained models on ImageNet32 and
CIFAR10. The optimizer, initialization, learning rate, and seeds used to train the ResNets in Figure 2
and 3 are presented in Figure 4. Note that all of our models were trained with mean squared error,
as discussed in (16). We trained models on ImageNet32 for 150 epochs and on CIFAR10 for 50
epochs. We then saved the model with the highest validation accuracy.
DataSet	Optimizer, Learning Rate	Initialization	Seed
Classes 1 and 2, ImageNet32	Adam, 1e-4	Default Pytorch Initialization	200
CIFAR10	Adam, 1e-4	Default Pytorch Initialization	200
Figure 4:	An overview of the optimizer, learning rate, initialization, and seeds used to fine-tune
pre-trained models on ImageNet32 and CIFAR10.
For all experiments, we used the PyTorch deep learning library (25). We trained our models on a
shared server with 1 Titan Xp and 2 GeForce GTX 1080 Ti’s. We only used 1 GPU at a time for
training neural networks and applying LLBoost.
D Additional Experimental Details
In this section, we provide the following additional details regarding the experiments in Figure 2
and 3:
1.	The number of components used in the low-rank approximations for a full rank training
feature matrix (Figure 5).
2.	The size of the perturbation produced by LLBoost and the values of γ used for the models
in Figure 2 (Figure 6).
3.	A comparison between training time and the time taken for LLBoost to improve the models
in Figure 2 (Figure 7).
E Performance of projected standard normal perturbations
In Figure 2, we demonstrated that perturbing the last layer without projecting to the space orthogonal
to the feature matrix provided a drastic decrease in the training and validation accuracy. In Figure
8, we illustrate the impact of using a perturbation that is randomly sampled from a standard normal
and then projected to the space orthogonal to the feature matrix. Again, we see that the validation
accuracies can drop significantly for larger datasets in this case. Note that including the projection
operator preserves the training accuracy in all cases, as is guaranteed by Lemma 1.
F Low Rank Approximations for Feature Matrices
As discussed in Section 4, when the feature matrix, X, is full rank, we needed to use a low-rank
approximation such that the space orthogonal to X . In this section, we discuss our method of
12
Under review as a conference paper at ICLR 2021
Dataset	Model	Number of Components
2600 Ex. ImageNet-32	ResNet-18	12
2600 Ex. Imagenet-32	ResNet-34	12
2600 Ex. Imagenet-32	ResNet-50	1548
CIFAR10	ResNet-18	212
ImageNet	FiXResNeXt-101	1000
Figure 5:	The rank of the approximation used for the training feature matrix, X, when X was full
rank.
Dataset	Model	y	Train/Val. Acc. (Original)	Train∕Val. Acc. (Ours)	Perturbation
100 Ex. ImageNet32	ResNet-18	0.226	100%∕80%	100%∕84%	0.21
2600 Ex. ImageNet32	ResNet-18*	11.314	99.96%∕95%	99.96%∕97%	11.210
100 Ex. ImageNet32	ResNet-34	2.263	100%∕85%	100%∕87%	2.263
2600 Ex. ImageNet32	ResNet-34*	4.525	99.77%∕95%	99.77%∕98%	4.474
100 Ex. ImageNet32	ResNet-50	0.453	100%∕83%	100%∕89%	0.440
200 Ex. ImageNet32	ResNet-50	0.453	100%∕87%	100%∕93%	0.431
500 Ex. ImageNet32	ResNet-50	4.525	99.6%∕91%	99.6%∕93%	3.962
800 Ex. ImageNet32	ResNet-50	9.051	100%∕94%	100%∕98%	7.215
1000 Ex. ImageNet32	ResNet-50	33.941	100%∕93%	100%∕97%	23.784
2000 Ex. ImageNet32	ResNet-50	45.255	99.85%∕95%	99.85%∕97%	6.338
2600 Ex. ImageNet32	ResNet-50*	36.204	99.88%∕95%	99.88%∕99%	17.364
ImageNet	FixResNext-101*	10	94.924%∕86.26%	94.924%∕86.34%	0.4428
CIFAR10	ResNet-18*	2.0	99.99%∕95.05%	99.99%∕95.25%	1.724
Figure 6:	An extended version of Figure 2 that includes the choice of γ considered and the size
of the perturbation (in FrobeniUs norm) produced by LLBoost. *'s indicate the use of low-rank
approximations for full rank training feature matrices.
choosing the number of components of the SVD to keep for producing the low-rank approximation
forX. We then present how the number of components selected affects the performance of LLBoost.
In Figure 9, we visualize the normalized singular values of the feature matrix for models from Figure
2. In Figure 9A, we do not use a low-rank approximation as the size of the dataset is already smaller
than the number of features. In Figure 9B, the feature matrices are full rank, and so we use a low-
rank approximation for the feature matrix with the number of components selected shown red. In
particular, we chose a number of components that is well past the elbow in the curve so that there
was not a significant drop in training accuracy.
In Figure 10, we demonstrate how the number of components selected for the low-rank approx-
imation affects the validation accuracy of LLBoost. In particular, we observe that using a lower
rank approximation generally increases the improvement provided by LLBoost. This matches the
intuition provided by Proposition 2: when the space orthogonal to the training feature matrix, X , is
large, there is no reason to believe that the best linear solution lies in the span ofX. Hence, sampling
the space orthogonal to X yields an improvement. We note that since only a few singular values of
X are large, there is no impact to the training accuracy when using a low-rank approximation for X
(shown in the second column of the tables in Figure 10).
G LLBoost applied to Train, validation, test splits
In Figure 2 and Figure 3, we demonstrated that LLBoost improves the validation accuracy of pre-
trained models without impacting the training accuracy. To ensure that LLBoost is not overfitting
13
Under review as a conference paper at ICLR 2021
DataSet	Model	Training Time	Correction Time (s)
100 Ex. ImageNet32	ResNet-18	77.753 sec	0.116 sec
2600 Ex. ImageNet32	ReSNet-18*	1020.413 sec	0.112 sec
100 Ex. ImageNet32	ResNet-34	122.85 sec	0.111 sec
2600 Ex. ImageNet32	ResNet-34*	1397.989 sec	0.098 sec
100 Ex. ImageNet32	ResNet-50	164.07 sec	0.113 sec
200 Ex. ImageNet32	ResNet-50	190.473 sec	0.111 sec
500 Ex. ImageNet32	ResNet-50	407.454 sec	0.105 sec
800 Ex. ImageNet32	ResNet-50	628.997 sec	0.137 sec
1000 Ex. ImageNet32	ResNet-50	1054.061 sec	0.087 sec
2000 Ex. ImageNet32	ResNet-50	1996.991 sec	0.129 sec
2600 Ex. ImageNet32	ResNet-50*	2488.621 sec	0.11 sec
ImageNet	FixResNext-101*	~1 day/epoch	7.59 hr
CIFAR10	ResNet-18*	1.35 hr	15.36 min
Figure 7:	A comparison between the training time and LLBoost correction time for models from
Figure 2. For the ImageNet32 models, the third column represents the time to compute the valida-
tion accuracy for 100, 000 samples from LLBoost. For CIFAR10 and ImageNet, the time addition-
ally includes the cost of computing the perturbation for LLBoost. *'s indicate the use of low-rank
approximations for full rank training feature matrices.
Dataset	Model	Train/Val. Acc. (Original)	Train∕Val. Acc. (Standard Normal @ Perp)
100 Ex. ImageNet32	ResNet-18	100%∕80%	100%∕76%
2600 Ex. ImageNet32	ResNet-18*	99.96%∕95%	99.96%∕96%
100 Ex. ImageNet32	ResNet-34	100%∕85%	100%∕75%
2600 Ex. ImageNet32	ResNet-34*	99.77%∕95%	99.77%∕88%
100 Ex. ImageNet32	ResNet-50	100%∕83%	100%∕76%
200 Ex. ImageNet32	ResNet-50	100%∕87%	100%∕77%
500 Ex. ImageNet32	ResNet-50	99.6%∕91%	99.6%∕79%
800 Ex. ImageNet32	ResNet-50	100%∕94%	100%∕87%
1000 Ex. ImageNet32	ResNet-50	100%∕93%	100%∕96%
2000 Ex. ImageNet32	ResNet-50	99.85%∕95%	99.85%∕97%
2600 Ex. ImageNet32	ResNet-50*	99.88%∕95%	99.88%∕99%
ImageNet	FixResNext-101*	94.924%∕86.26%	94.924∕18.54%
CIFAR10	ResNet-18*	99.99%∕95.05%	99.99%∕92.35%
Figure 8:	A demonstration that using samples from a standard normal projected onto the space
orthogonal to the training data leads to a decrease in validation accuracy but has no impact on
training accuracy. *'s indicate the use of low-rank approximations for full rank training feature
matrices.
the validation set, we additionally split the validation data into validation and test data and check
that LLBoost improves validation and test accuracy without impacting training accuracy3.
3For ImageNet32, the validation set size is only 100 examples, and so we split the training set and re-train.
14
Under review as a conference paper at ICLR 2021
ResNet-50, 100 Ex. ImageNet32
(B)
(A) ResNet-18, 100 Ex. ImageNet32
ωpn-u6e≡
Singular Value Index
ωp"u6e≡
ReSNet18, CIFAR10
3500-
ωpn-u6e≡
Singular Value Index
ResNet-50, 2000 Ex. ImageNet32
ωp"u6e≡
Singular Value Index
FiXReSNeXt-101, ImageNet
ωpn--u6e≡
Singular Value Index
Figure 9:	Visualizations of the singular values of training feature matrices for models from 2. (A)
The singular values of the training feature matrices for small datasets. (B) The singular values of
full rank training feature matrices from large datasets. The red vertical line indicates the size of the
approximation used for Figure 2.
ImageNet32 (2600 Ex.), ResNet-50	TrainAcc. (Original)	TrainAcc. (Approx.)	Val. Acc.
Original Feature Matrix	99.88%	99.88%	95%
Rank 1548 Approx.	99.88%	99.88%	99%
Rank 1648 Approx.	99.88%	99.88%	99%
Rank 1748 Approx.	99.88%	99.88%	98%
Rank 1848 Approx.	99.88%	99.88%	98%
Rank 1898 Approx.	99.88%	99.88%	97%
Rank 1948 Approx.	99.88%	99.88%	97%
Rank 1973 Approx.	99.88%	99.88%	96%
Rank 1998 Approx.	99.88%	99.88%	96%
Rank 2023 Approx.	99.88%	99.88%	96%
CIFAR10, ResNet-18	Train Acc. (Original)	Train Acc. (Approx.)	Val. Acc.
Original Feature Matrix	99.99%	99.99%	95.05%
Rank 50 Approx.	99.99%	99.99%	95.25%
Rank 75 Approx.	99.99%	99.99%	95.25%
Rank 100 Approx.	99.99%	99.99%	95.24%
Rank 150 Approx.	99.99%	99.99%	95.24%
Rank 200 Approx.	99.99%	99.99%	95.23%
Rank 212 Approx.	99.99%	99.99%	95.25%
Rank 300 Approx.	99.99%	99.99%	95.2%
Rank400 Approx.	99.99%	99.99%	95.19%
Rank 500 Approx.	99.99%	99.99%	95.13%
Figure 10: The impact of using approximations of varying rank for full rank training feature ma-
trices. The first row provides the training accuracy and validation of the original model. The first
column is the training accuracy of the model on the original dataset, the second column is the training
accuracy on the training data reconstructed from the low-rank approximation, and the third column
is the validation accuracy. We see that the validation accuracy generally increases when lowering
the rank of the approximation. Since only a few singular values of the training feature matrix are
large, there is no impact to the training accuracy when using a low-rank approximation for X .
In Figures 11 and 12, we present examples of how LLBoost (which selects the perturbation that
improves validation accuracy) improves both validation and test accuracy without impacting training
accuracy.
15
Under review as a conference paper at ICLR 2021
DataSet	Model	Train/Val./Test Acc.. (Original)	Train/Val./TeStAcc. (Ours)
2600 Ex. ImageNet32 (80/20 split)	ResNet-18	99.9%/91.7%/93%	99.9%/92.5%/94%
CIFAR10 (20/80 split)	ResNet-18	99.99%/95.2%/94.9%	99.99%/95.3%/94.93%
CIFAR10 (50/50 Split)	ResNet-18	99.99%/95.1%/94.82%	99.99%/95.16 %∕94.84%
CIFAR10 (90/10 split)	ResNet-18	99.99%/95.01%/94.50%	99.99%/95.04 %∕94.60%
Figure 11:	Using LLBoost to improve validation accuracy also leads to an improvement in test
accuracy (i.e. LLBoost does not overfit the validation set). We split the original validation set
of CIFAR10 into a validation and test set according to the splits indicated in parentheses. As the
validation set of ImageNet32 for 2 classes only has 100 images, we perform an 80/20 train/validation
split of the training set, use the 100 validation images as test data, and re-train our models on the
smaller training set.
Dataset	Model	Train/Val./TestAcc. (Original)	Train/Val./TestAcc. (Ours)
200 Dogs/Cats CIFAR10	ResNet-18	100%∕78%∕75.50%	100%/79%/75.53%
1000 Dogs/Cats CIFAR10	ResNet-18	100%/84.5%/86.37%	100%/86%/86.54%
2000 Dogs/Cats CIFAR10	ResNet-18	100%/88.65%/89.32%	100%/89.15%/89.42%
Figure 12:	Using LLBoost to improve validation accuracy also leads to an improvement in test
accuracy (i.e. LLBoost does not overfit the validation set). In our experiments, we use the same
number of examples for training and validation and use the entirety of the remaining examples for
testing. For example, in row 1, we use 200 examples for training, 200 for validation and 11600 for
testing.
H Proof of Propostion 1
Proof. We first consider Wb - w*:
W - W* = yVΣtUT - W*
=w*XVΣtUT - w* (since y = w*X)
=w*UΣΣtUT - w*(UΣ⊥UT + UΣ⊥⊥UT)
= w*UΣ⊥⊥UT - w*(UΣ⊥UT + UΣ⊥⊥UT)
= -w*UΣ⊥UT
Thus, we have shown (1). Now for (2), we have:
wbr -w* = w(0) UΣ⊥ UT +wb-w* = w(0) UΣ⊥ UT -w*UΣ⊥UT = (w(0) - w*)UΣ⊥UT.
Hence, (2) follows from (1).	□
I Proof of Theorem 2
Proof. The proof follows from Lemma 1. Since the columns of X are drawn from N(0, Id×d),
Lemma 2 implies that the columns of U are drawn from the uniform distribution on the sphere in
Rd . Hence we have that:
dd
EX [Uς⊥uT] =EX	X UiuT = X EX [uiuT] = 1 - d.
i=n+1	i=n+1
This implies (1) since:
EX[kwb - w*k2 = w*EX[UΣ⊥Ut]w*t = kw*k2(1 - d).
Similarly, We get (2), which completes the proof.	□
16
Under review as a conference paper at ICLR 2021
J Proof of Proposition 2
Proof. Let aτ = w(0) - w*. We need to find a such that:
d
(1)	kaτUΣ⊥Uτk2 = X |ha,uii|2=c22,
i=r+1
(2)	aτ a = c21 .
To do this, we instead first let a0 = c1a and show that there exists a solution to:
⑴ ka0TUΣ⊥UTk2= XX ∣ha0,Uii∣2 = c2,
c
i=r+1	1
(2)	a0T a0 = 1.
We will show that there is a solution to the above system by using the intermediate value theorem.
First, note that the unit sphere is path connected in Rd. Now for a0 = ur+1, we have ka0k = 1 and
∣∣a0TUΣ⊥UTk2 = 1. Next, note that for a0 = uι, ∣∣a0k = 1 and ∣a0TUΣ⊥UTk2 = 0. Thus, by
the intermediate value theorem we conclude that there exists some a0 on the unit sphere such that
2
IIa0TUΣ⊥UTk2 = -2, which completes the proof.	□
c1
K Proof of Proposition 3
Proof. Note that we have:
Pw(0) (Ex,X [(y - wbrx)2] ≤ Ex,X [(y - wbx)2])
^⇒ Pw(0) (kw(0)- w*k2 (1- d) ≤ kw*k2 (1- d))
^⇒ Pw(0) (hw(0), ||-7i∣ i ≥ 9∣∣1*∣I ).
w	kw*k	2kw*k
Since w(0) and ^^ are unit vectors on Sd-ι, the desired probability is equivalent to that of the
ratio of the area of the spherical cap (19) defined by the co-latitude angle φ = cos-1 Q∣W *∣∣) to the
surface area of Sd_1, which completes the proof.	□
L Proof of Theorem 3
Proof. We here present the proof for the case that γ = 1; however, the proof is easily extendable to
the case of arbitrary γ. The proof relies on the following inequalities, which are commonly used in
analysis.
Proposition 4 (Reduction Formula).
sind θdθ
-d cos θ(sin θ)d-1 +
d-1 ∕sind-2 θdθ
Proposition 5 (Gautschi’s Inequality).
x1-s < Γ(x + 1) < (X + 1)1-s ； S ∈ (O, 1) ； x > 0
Γ(x + s)
Corollary 1. For S ∈ (0, 1) and x > 0:
/1、 Γ Γ(x + 1) , --
⑴ B Γ(x∏) <λzxτ1,
(2) -^= <
√x + 1
Γ(x + 2)	1
Γ(x +1) < √x.
17
Under review as a conference paper at ICLR 2021
Proposition 6.
Let K = Rφ(sin θ)d-2dθ. We will lower bound this integral. For convenience of notation, we will
skip writing the limits of integration. By using the reduction formula for the powers of / (sin θ)ndθ,
and assuming d is even for convenience, we have:
K = — J cos φ(sin φ)d-3 —
d — 2
d—2 d―4 cos φ(sin φ尸 -
Y cos Φ sin φ +1⅛⅛ φ
(d - 2)!! φ φ + √∏Γ(d)φ
	
	
ɪeos φ Sin φ r(号)巴(T)
d — 2 φ	φ √∏Γ(得)[Γ(d-1)
(Sin Φ 尸 + ^Γ⅛T (Sin M- + …+ ” 1
H 2 )	r( 2)
, γ( ⅛1) φ
+ √∏γ( d)φ
d—2
cos
φ sin φ γ(⅛
Γ(得)
-d-4
~H~
X
i=1
(sin2 φ)i
2i+1
Γ(d-1)
T——2 d φ (by Gautschi,s Inequality)
√πr( 2)
d—2
cos
φ sin φ γ(⅛
Γ(得)
-d-4
~H~
X
i=1
(sin2 φ)i
2i
+1
-----cos
d — 2
φ sin φ γ(⅛
Γ(得)
-d-4
~7Γ~
+史幸φ
√∏γ( d)
>—ɪeos φ sin φ γ(⅛
-d — 2 φ	φ Γ(与2)
Id - 4
2V 丁十1
Γ( ⅛1)
√∏γ( d)
φ.
〉
〉
	
	
	
1
1
1
i I
√7 + 1
+ 1
+
, Γ(⅛1) φ
√∏γ(dV
Since φ = cos-1 (21+^), then
K > __1	1 + ^	 (1 + e)2 γ(与1)
> (d — 2)2∣∣w*∣∣V	4∣∣w*∣∣2 Γ(d-2)
-Id- 4
2V 丁十1
1 γ(号)
√πr( d)
cos-1
(1 + E
Again by Gautschi,s Inequality we obtain:
Γ(甘) < 尸
Γ^2y < v F
and hence,
K > - (d — 2)21∣+*∣∣ V1 - ⅛+⅜ʌ/diɪ
Id-4
2V ɪ + 1
.γ(d-1)
√∏γ(d)
cos-1
(1 + E
Thus, we have that:
/
r(d)
Z Sind-2 θdθ > —
Jo
d1
2π (d — 2) 2∣w*∣∣
1-
(1 + E)2
4∣w* ∣∣2
Id - 4
2V ι- + 1
1 + E
d — 1
2
+ 1cos-1 ( -1.e
π	12忖寸
Hence, assuming ∣∣w*∣ = W, we obtain:
1
2
Γ
lim
d-∞ Λ∕πΓ
φ Sind-2 θdθ >- ¾⅛2
1 π
H-----
π 2
	
c(1 + e)
2√2∏ .
18
Under review as a conference paper at ICLR 2021
Note that we have:
Pw(O) (hw⑼'岛〉≥ ⅛+f) ≤ Pw(O) (hw⑼'岛＞≥0) = 2
and hence, we conclude that:
1	c(1 + E)
2	— 2√2∏
≤
Iim
d-∞
□
19