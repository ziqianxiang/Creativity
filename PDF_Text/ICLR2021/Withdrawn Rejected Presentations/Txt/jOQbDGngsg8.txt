Under review as a conference paper at ICLR 2021
Secure Network Release with Link Privacy
Anonymous authors
Paper under double-blind review
Ab stract
Many data mining and analytical tasks rely on the abstraction of networks (graphs)
to summarize relational structures among individuals (nodes). Since relational
data are often sensitive, we aim to seek effective approaches to release utility-
preserved yet privacy-protected structured data. In this paper, we leverage the
differential privacy (DP) framework to formulate and enforce rigorous privacy
constraints on deep graph generation models, with a focus on edge-DP to guarantee
individual link privacy. In particular, we enforce edge-DP by injecting Gaussian
noise to the gradients of a link reconstruction based graph generation model, while
ensuring data utility by improving structure learning with structure-oriented graph
comparison. Extensive experiments on two real-world network datasets show
that our proposed DPGGan model is able to generate networks with effectively
preserved global structure and rigorously protected individual link privacy.
1 Introduction
Nowadays, open data of networks play a pivotal role in data mining and data analytics (Tang et al.,
2008; Sen et al., 2008; Blum et al., 2013; Leskovec & Krevl, 2014). By releasing and sharing
structured relational data with research facilities and enterprise partners, data companies harvest
the enormous potential value from their data, which benefits decision-making on various aspects,
including social, financial, environmental, through collectively improved ads, recommendation,
retention, and so on (Yang et al., 2017; 2018; Sigurbjornsson & Van Zwol, 2008; Kuhn, 2009).
However, network data usually encode sensitive information not only about individuals but also their
interactions, which makes direct release and exploitation rather unsafe. More importantly, even with
careful anonymization, individual privacy is still at stake under collective attack models facilitated by
the underlying network structure (Zhang et al., 2019; Cai et al., 2018). Can we find a way to securely
release network data without drastic sanitization that essentially renders the released data useless?
In dealing with such tension between the need to release utilizable data and the concern of data
owners’ privacy, quite a few models have been proposed recently, focusing on grid-based data like
images, texts and gene sequences (Frigerio et al., 2019; Papernot et al., 2018; Triastcyn & Faltings,
2018; Narayanan & Shmatikov, 2008; Xie et al., 2018; Chen et al., 2018; Boob et al., 2018; Dy &
Krause, 2018; Lecuyer et al., 2018; Zhang et al., 2018). However, none of the existing models can
be directly applied to the network (graph) setting. While a secure generative model on grid-based
data apparently aims to preserve high-level semantics (e.g., class distributions) and protect detailed
training data (e.g., exact images or sentences), it remains obtuse what to be preserved and what to be
protected for network data, due to its modeling of complex interactive objects.
Motivating scenario. In Figure 1, a bank aims to encourage public studies on its customers’
community structures. It does so by firstly anonymizing all customers and then sharing the network
(i.e., (a) in Figure 1) to the public. However, an attacker interested in knowing the financial interactions
(e.g., money transfer) between particular customers in the bank may happen to have access to another
network of a similar set of customers (e.g., a malicious employee of another financial company).
The similarity of simple graph properties like node degree distribution and triangle count between
the two networks can then be used to identify specific customers with high accuracy in the released
network (e.g., customer A as the only node with degree 5 and within 1 triangle, and customer B as
the only node with degree 2 and within 1 triangle). Thus, the attacker confidently knows the A and
B’s identities and the fact that they have financial interactions in the bank, which seriously harms
customers’ privacy and poses potential crises.
1
Under review as a conference paper at ICLR 2021
(a) Anonymized original net. (b) DPGGan generated net.
Figure 1: A toy pair of anonymized and generated networks.
As the first contribution in this work, we define and formulate secure network release goals as
preserving global network structure while protecting individual link privacy. Continue with the toy
example, the solution we propose is to train a graph neural network model on the original network and
release the generated networks (e.g., (b) in Figure 1). Towards the utility of generated networks, we
require them to be similar to the original networks from a global perspective, which can be measured
by various graph global properties (e.g., network (b) has very similar degree distribution and the same
triangle count as (a)). In this way, we expect many downstream data-mining and analytical tasks on
them to produce similar results as on the original networks. As for privacy protection, we require that
the information in the generated networks cannot confidently reveal the existence or absence of any
individual links in the original networks (e.g., the attacker may still identify customers A and B in
network (b), but their link structure has changed).
Subsequently, there are two unique challenges in learning such structure-preserved and privacy-
protected graph generation models, which have not been explored by existing literature so far.
Challenge 1: Rigorous protection of individual link privacy. The rich relational structures in
graph data often allow attackers to recover private information through various ways of collective
inference (Zhang et al., 2014; Narayanan & Shmatikov, 2009; Backstrom et al., 2007). Moreover,
graph structure can always be converted to numerical features such as spectral embedding, after
which most attacks on grid-based data like model inversion (Fredrikson et al., 2015) and membership
inference (Shokri et al., 2017) can be directly applied for link identification. How can we design an
effective mechanism with rigorous privacy protection on links in networks against various attacks?
Challenge 2: Effective preservation of global network structure. To capture the global network
structure, the model has to constantly compare the structures of the input graphs and currently
generated graphs during training. However, unlike images and other grid-based data, graphs have
flexible structures, and thus they lack efficient universal representations (Dong et al., 2019). How can
we allow a network generation model to effectively learn from the structural difference between two
graphs, without conducting very time-costly operations like isomorphism tests all the time?
Present work. In this work, for the first time, we draw attention to the secure release of network
data with deep generative models. Technically, towards the aforementioned two challenges, we
develop Differentially Private Graph Generative Nets (DPGGan), which imposes DP training
over a link reconstruction based network generation model for rigorous individual link privacy
protection, and further ensures structure-oriented graph comparison for effective global network
structure preservation. In particular, we first formulate and enforce edge-DP via Gaussian gradient
distortion by injecting designed noise into the sensitive modules during model training. Then we
leverage graph convolutional networks (Kipf & Welling, 2017) through a variational generative
adversarial network architecture (Gu et al., 2019; Larsen et al., 2016) to enable structure-oriented
network comparison.
To evaluate the effectiveness of DPGGan, we conduct extensive experiments on two real-world
network datasets. On one hand, we evaluate the utility of generated networks by computing a suite of
commonly concerned graph properties to compare the global structure of generated networks with
the original ones. On the other hand, we validate the privacy of individual links by evaluating links
predicted from the generated networks on the original networks. Consistent experimental results
show that DPGGan is able to effectively generate networks that are similar to the original ones
regarding global network structure, while at the same time useless towards individual link prediction.
2
Under review as a conference paper at ICLR 2021
2 Related Work
Differential Privacy (DP). Differential privacy is a statistical approach in addressing the paradox of
learning nothing about an individual while learning useful information about a population (Dwork
et al., 2006). Recent advances in deep learning have led to the rapid development of DP-oriented
learning schemes. Among them, the Gaussian Mechanism (Dwork et al., 2014), defined as follows,
provides a neat and compatible framework for DP analysis over machine learning models.
Definition 1 (Gaussian Mechanism (Dwork et al., 2014)). For a deterministic function f with its
'2 -norm sensitivity as ∆2f
kG-mGa0xk =1 kf(G) - f(G0)k2, we have:
Mf(G) ,f(G)+N(0,∆2f2σ2),	(1)
where N (0, ∆2f2σ2) is a random variable obeying the Gaussian distribution with mean 0
and standard deviation ∆2 fσ. The randomized mechanism Mf (G) is (ε, δ)-DP if σ ≥
∆2f p2 ln(1.25∕δ)∕ε and ε < 1.
Following this framework, (Abadi et al., 2016) proposes a general training strategy called DPSGD,
which looses the condition on the overall privacy loss than that in Definition 1 by tracking detailed
information of the SGD process to achieve an adaptive Gaussian Mechanism.
DP learning has also been widely adapted to generative models (Frigerio et al., 2019; Papernot et al.,
2018; Triastcyn & Faltings, 2018; Narayanan & Shmatikov, 2008; Mohammed et al., 2011; Xie et al.,
2018; Chen et al., 2018; Boob et al., 2018; Dy & Krause, 2018; Lecuyer et al., 2018; Zhang et al.,
2018). For example, (Frigerio et al., 2019; Chen et al., 2018; Boob et al., 2018; Zhang et al., 2018)
share the same spirit by enforcing DP on the discriminators, and thus inductively on the generators,
in a generative adversarial network (GAN) scheme. However, none of them can be directly applied to
graph data due to the lack of consideration of structure generation.
For graphs’ structural data, two types of privacy constraints can be applied, i.e., node-DP (Ka-
siviswanathan et al., 2013) and edge-DP (Blocki et al., 2012), which define two neighboring graphs
to differ by at most one node or edge. In this work, we aim at the secure release of network data, and
particularly, we focus on edge privacy because it is essential for the protection of object interactions
unique for network data compared with other types of data. Several existing works have studied
the protection of edge-DP. For example, (Sala et al., 2011) generates graphs based on the statistical
representations extracted from the original graphs blurred by designed noise, whereas (Wang & Wu,
2013) enforces the parameters of dK-graph models to be private. However, based on shallow graph
generation models, they do not flexibly capture global network structure that can support various
unknown downstream analytical tasks (Zhang et al., 2019; Wasserman & Zhou, 2010).
Graph Generation (GGen). GGen has been studied for decades and is widely used to synthesize
network data used to develop various collective analysis and mining models (Evans & Lambiotte,
2009; Hallac et al., 2017). Earlier works mainly use probabilistic models to generate graphs with
certain properties (Erdos & Renyi, 1960; Watts & Strogatz,1998; Barabasi & Albert,1999; Newman,
2001), which are manually designed based on sheer observations and prior assumptions.
Thanks to the surge of deep learning, many advanced GGen models have been developed recently,
which leverage different powerful neural networks in a learn-to-generate manner (Kipf & Welling,
2016; Bojchevski et al., 2018; You et al., 2018b; Simonovsky & Komodakis, 2018; Li et al., 2018;
You et al., 2018a; Jin et al., 2018; Grover et al., 2017; De Cao & Kipf, 2018; Zou & Lerman, 2018;
Ma et al., 2018). For example, NetGAN (Bojchevski et al., 2018) converts graphs into biased random
walks, learns the generation of walks with GAN, and assembles the generated walks into graphs;
GraphRNN (You et al., 2018b) regards the generation of graphs as node-and-edge addition sequences,
and models it with a heuristic breadth-first-search scheme and hierarchical RNN. These neural
network based models can often generate graphs with much richer properties, and flexible structures
learned from real-world graphs.
To the best of our knowledge, no existing work on deep GGen has looked into the potential privacy
threats laid during the learning and releasing of the powerful models. Such concerns are rather
urgent in the network setting, where sensitive information can often be more easily compromised in a
collective manner (Dai et al., 2018; Backstrom et al., 2007; Zhang et al., 2014) and privacy leakage
can easily further propagate (Narayanan & Shmatikov, 2009; Zugner et al., 2018).
3
Under review as a conference paper at ICLR 2021
3 DPGGAN
In this work, we propose DPGGan for the secure release of generated networks, whose global graph
structures are similar to the original sensitive networks, but the individual links (edges) between
objects (nodes) are safely protected. To provide robust privacy guarantees towards various graph
attacks, we propose to leverage the well-studied technique of differential privacy (DP) (Dwork et al.,
2014) by enforcing the edge-DP defined as follows.
Definition 2 (Edge Differential Privacy (Blocki et al., 2012)). A randomized mechanism M satisfies
(ε, δ)-edge-DP if for any two neighboring graphs G1, G2 ∈ G, which differ by at most one edge,
Pr[M(G1) ∈ S] ≤ exp(ε) × Pr[M(G2) ∈ S] +δ, where S ⊂ range(M).
Our key insight is, a graph generation model M satisfying the above edge-DP should learn to
generate similar graphs given the input of two neighboring graphs that differ by at most one edge; as
a consequence, the information in the generated graph does not confidently reveal the existence or
absence of any one particular edge in the original graph, thus protecting individual link privacy.
To ensure DP on individual links, we exploit the existing link reconstruction based graph generation
model GVAE (Kipf & Welling, 2016), and design a training algorithm to dynamically distort the
gradients of its sensitive model parameters by injecting proper amounts of Gaussian noise based
on the framework of DPSGD (Abadi et al., 2016). We provide theoretical analysis on applying
DPSGD to achieve edge-DP with GVAE based on the nature of graph data and the link reconstruction
loss. Moreover, to improve the capturing of global graph structures, we replace the direct binary
cross-entropy (BCE) loss on graph adjacency matrices in GVAE with a structure-oriented graph
discriminator based on GCN (Kipf & Welling, 2017) and the framework of VAEGAN (Gu et al.,
2019; Larsen et al., 2016). We further prove the improved model to maintain the same edge-DP.
Backbone GVAE. Recent research on graph models has been primarily focused around GCN (Kipf &
Welling, 2017), which is shown to be promising in calculating universal graph representations (Maron
et al., 2019; Xu et al., 2019; Chen et al., 2019; Keriven & Peyra 2019). In this work, We harness the
power of GCN under the consideration of edge-DP by adapting the link reconstruction based graph
variational autoencoder (GVAE) (Kipf & Welling, 2016) as our backbone graph generation model.
Notably, we are given a graph G = {V, E}, where V is the set of N nodes (vertices), and E is
the set of M links (edges), which can be further modeled by a binary adjacency matrix A. As a
common practice (Hamilton et al., 2017), we set the node features X simply as the one-hot node
identity matrix. The autoencoder architecture of GVAE consists of a GCN-based graph encoder to
guide the learning of a feedforward neural network (FNN) based adjacency matrix decoder, which
can be trained to directly reconstruct graphs with similar links as in the input graphs. A stochastic
latent variable Z is further introduced as the latent representation of A as
NN
q(Z∣X, A) = Y q(Zi∣X, A) = YN(Zi∣μi, diag(σ2)),	⑵
i=1	i
where μ = gμ(X, A) is the matrix of mean vectors μ% and σ = gσ (X, A) is the matrix of standard
deviation vectors σi. g∙(X, A) = AReLU(AXWo)Wi is a two-layer GCN model. gμ and gσ
share the first-layer parameters Wo. A = D- 1 AD-2 is the symmetrically normalized adjacency
matrix of G, with degree matrix Dii = PN=ι Aij. gμ and gσ form the encoder network.
To generate a graph G0, a reconstructed adjacency matrix A0 is computed from Z by an FNN decoder
NN	NN
P(A∣Z) = Y Y P(AjZe, Zj) = YY σ(f (Zi)T f (Zj)),	⑶
i=1 j=1	i=1 j=1
where σ(z) = 1/(1 + e-z), f is a two-layer FNN appended to Z before the logistic sigmoid function.
The whole model is trained through optimizing the following variational lower bound
Lvae
Lrec + Lprior
(4)
= Eq(Z|X,A)[log p(A|Z)] -DKL(q(Z|X,A)kp(Z)),
where Lrec is implemented as the sum of an element-wise binary cross entropy (BCE) loss between
the adjacency matrices of the input and generated graphs, and Lprior is a prior loss based on the
Kullback-Leibler divergence towards the Gaussian prior p(Z) = QiN=1p(Zi) = QiN N(Zi |0, I).
4
Under review as a conference paper at ICLR 2021
Enforcing DP. The probabilistic nature of Z allows the model to be generative, meaning that after
training the model with an input graph G, we can detach and disregard the encoder, and then freely
generate an unlimited amount of graphs G0 with similar links to G, by solely drawing random
samples of Z from the prior distribution N(0, I) and computing A0 with the learned decoder network
w.r.t. Eq. (3). However, as shown in (Kurakin et al., 2017; Gondim-Ribeiro et al., 2018), powerful
neural network models like VAE can easily overfit training data, so directly releasing a trained GVAE
model poses potential privacy threats, as links in its generated graphs may be highly indicative
towards links in the training graphs.
In this work, we care about the generation model’s rigorously protecting the privacy of individual
links in the training data, i.e., ensuring edge-DP. Particularly, in Definition 2, the inequality guarantees
that the distinguishability of any one edge in the graph will be restricted to the privacy leak level
proportional to ε, quantifying the absolute value of privacy information possibly to be leaked by a
graph generation model.
According to Eq. (3), GVAE essentially takes a graph G, in particular, the links E among the nodes
V in G, as input and generates a new graph G0 by reconstructing the links E0 among the same set of
nodes V. Therefore, if we regard GVAE as the mechanism M, as long as its model parameters are
properly randomized, the framework satisfies edge-DP. To be specific, any two input graphs G1 and
G2 differing by at most one link in principle lead to similar generated graphs G0, so information in
G0 does not confidently reveal the existence or absence of any particular link in G1 or G2.
To exploit the well-structured graph generation framework of GVAE, we leverage the Gaussian
mechanism (Definition 1) (Dwork et al., 2014) and DPSGD (Abadi et al., 2016) to enforce edge-DP on
it. In our setting, G is the original training graph. Then Eq. (1) tells us that a link reconstruction based
graph generation model M can be randomized to ensure (ε, δ)-edge-DP with properly parameterized
Gaussian noise. Prominently, we follow DPSGD (Abadi et al., 2016) to inject a designed Gaussian
noise to the gradients of our decoder network clipped by a hyper-parameter C as follows.
,θL∕max(1, ^viCLk2)) + N(0,σ2C2I)),	⑸
where L is the loss function of a link reconstruction based graph generation model, C is the clipping
hyper-parameter for the model’s original gradient to bound the influence of each link, and σ is the
noise scale hyper-parameter. In the following theorem, we analyze and prove that the noised clipped
gradient gθ,L applied as above guarantees the learned graph generation model to be edge-DP, with a
different condition from that in Definition 1 due to the learning process of link based graph generation
model.
Theorem 1. In training a link reconstruction based graph generation model on a graph with N
nodes with batch size B, given the sampling probability q = B∕N, and the number of steps T, there
exist explicit constants c1 and c2 that for any ε < c1q2T, iteratively updating the model T times with
gθ,L attains it with (ε, δ)-edge-DPfor any δ > 0 ifwe choose
QP log(1∕δ)
σ ≥ c2---------------,
ε
where ci ≥ c0 log qσ, c ≤ 1∕vzco(1 - co) forany co ∈ (0,1).
The proofs of Theorem 1 are detailed in Appendix A.
For the training of the DPGVAE decoder, L in Eq. (5) is specified as Lrec in Eq. (4). Due to the link
reconstruction nature of DPGVae, we derive Corollary 1.1 from Theorem 1 as follows.
Corollary 1.1 (DPGVAE edge-DP). Under the same conditions in Theorem 1, iteratively updating
the decoder in DPGVAE for T times with gθ,Lrec attains it with (ε, δ)-edge-DP
In the generation stage, we can disregard the encoder and only use the decoder to generate an
unlimited amount of graphs from randomly sampled vectors from the prior distribution N(0, I).
Due to the randomness of the normal Gaussian distribution, the sampling process can be regarded
as (0, 0)-DP. By the composability property of DP (Dwork et al., 2014), generating graphs from
random noises with the DPGVAE decoder satisfies (ε, δ)-edge-DP, whose release in principle does
not disclose sensitive information regarding individual links in the original sensitive networks. Since
we do not release the encoder network, we do not need to clip and perturb its gradients during training
to induce minimum interruptions.
5
Under review as a conference paper at ICLR 2021
Figure 2: Neural architecture of DPGGan (best viewed in color): Our novel graph generation
model consists of a GCN-based encoder, an FNN-based decoder (generator), and a GCN+FNN-based
discriminator. Sensitive data and modules are marked as red, while safe operations (i.e., gradient
clipping, noise injection and sampling) are marked as green, leading to DP modules and data.
Improving structure learning. Besides individual link privacy, we also aim to preserve the global
network structure to ensure the utility of released data. As we discuss before, original GVAE
computes the reconstruction loss between input and generated graphs based on the element-wise
BCE between their adjacency matrices. Such a computation is specified on each link, rather than the
graph structure as a whole. To improve the global graph structure learning, we leverage GCN again,
which has been shown universally powerful in capturing graph-level structures (Maron et al., 2019;
XU et al., 2019; Chen et al., 2019; Keriven & Peyra 2019). In particular, We borrow the framework of
VAEGAN from recent research (Gu et al., 2019; Larsen et al., 2016; Yang et al., 2019), and compute
a structure-oriented generative adversarial network (GAN) loss as
Lgan = log(D(A)) + log(1 - D(A0))
with D(A) = f0(g0(X,A)),	(6)
where g0 and f0 are GCN and FNN networks similar as defined before, besides at the end of g0
the node-level representations are summed up as the graph-level representation, which resembles
the recently proposed GIN model for graph-level representation learning (Xu et al., 2019). In this
DPGGAN framework, the decoder also serves as the generator, while D = f J g0 is the discriminator.
Following (Gu et al., 2019), the encoder is trained w.r.t. Lrec + λ1Lprior, the generator w.r.t. Lrec -
λ2Lgan, and the discriminator w.r.t. λ2 Lgan, where λ1 and λ2 are hyper-parameters. To enforce
DP constraints and complete our proposed DPGGAN framework, Eq. (5) with Lrec substituted by
Lrec - λ2Lgan is applied to distort the gradients of the generator and guarantee edge-DP, which can
be used to securely generate networks with the other parts disregarded after training. The overall
framework of DPGGan is shown in Figure 2, and the training process is detailed in Appendix B.
The intuition behind the novel design of DPGGAN is, the GCN encodings g0(A) and g0(A0) capture
the graph structures of G and G0, so a reconstruction loss Lrec = kg0(A) - g0(A0)k22 captures the
intrinsic structural difference between G and G0 instead of the simple sum of the differences over
their individual links. Note that the effectiveness of our structure-oriented discriminator is critical not
only because it can directly enforce effective training of the graph generator through the minimax
game in Eq. (6), but also because it can learn to relax the penalty on certain individual links through
flexible and diverse configurations of the whole graph as long as the global structures remain similar,
which exactly fulfills our goals of secure network release. The benefits of such diversity enabled by
the VAEGAN have also been discussed in image generation (Gu et al., 2019; Larsen et al., 2016).
Compared with DPGVae, DPGGan does not directly compute the link reconstruction loss based
on BCE in Eq. (4), but rather computes it based on the graph discriminator D. However, the link
reconstruction based graph generator of DPGGan is exactly the same as DPGVae. Since we also
do not release D after training, we can simply retrieve Corollary 1.2 from Theorem 1 as follows.
Corollary 1.2 (DPGGAN edge-DP). Under the same conditions in Theorem 1, iteratively updating
the generator in DPGGan for T times with gθ,(Lrec-λ2Lgan)attains it with (ε, δ)-edge-DP
With Corollary 1.2, we attain DPGGAN with the same (ε, δ)-edge-DP protection of DPGVAE. For
both DPGVae and DPGGan, the decoder/generator networks only get exposed to the noised and
clipped gradients, representing the partial sensitive information within the training graphs. Hence, it
prevents the inference of training graphs from both learned model parameters and generated graphs.
6
Under review as a conference paper at ICLR 2021
4	Experimental Evaluations
We conduct two sets of experiments to evaluate the effectiveness of DPGGAN in preserving global
network structure and protecting individual link privacy. All code and data are also in the submission.
Experimental settings. To provide a side-to-side comparison between the original networks and
generated networks, we use two standard datasets of real-world networks, i.e., DBLP, and IMDB.
DBLP includes 72 networks of author nodes and co-author links, where the average numbers of
nodes and links are 177.2 and 258; IMDB includes 1500 networks of actor/actress nodes and co-star
links, with average node and link numbers 13 and 65.9.
To show that DPGGan effectively captures global network structures, we compare it and DPG-
VAE under different privacy budgets (controlled by ε in Eq. (32)), regarding a suite of graph statistics
commonly used to evaluate the performance of graph generation models, especially from a global
perspective (Bojchevski et al., 2018; You et al., 2018b; Yang et al., 2019).1 In particular, we train all
models from scratch to convergence for K times, where K is the number of networks in the datasets.
Each time, the trained model is used to generate one network, which is compared with the original
network regarding the suite of graph statistics. Then we average the absolute differences between the
generated networks and the original networks, ensuring that the positive and negative differences do
not cancel out. The results are summarized in Table 1.
Beyond the single value statistics, we also compare the generated graph regarding degree distribution
and motif counts. For degree distribution, we convert each graph into a 50-dim vector (all nodes with
degree larger than 50 are binned together); for motif counts, we enumerate all 29 undirected motifs
with 3-5 nodes and convert each graph into a 29-dim vector by motif matching. We compute the
average cosine similarity between pairs of original graphs and generated graphs. Furthermore, we
use the most widely studied graph-level downstream task, i.e., graph classification, to evaluate the
global utilities of generated graphs. In particular, we evaluate the accuracy of the state-of-the-art
graph classification model, i.e., GIN (Xu et al., 2019), with the default parameter setting and 4:1
training-testing ratio. The results are summarized in Table 2.
To facilitate a better understanding towards how the graph statistics reflect the global network
structure captured by the models, we also provide results of two recent deep network generation
methods, i.e., NetGAN (Bojchevski et al., 2018) and GraphRNN (You et al., 2018b), with default
parameter settings and no DP constraints at all. In this experiment, we expect to see the more effective
structure-preserving models generate networks that are more similar to the original ones regarding
various global and distributional graph properties and achieve high graph classification accuracy,
thus maintaining high network data utility.
To show that DPGGan effectively guarantees individual link privacy, we train all models for another
K times on each dataset. Instead of complete networks, we randomly sample 80% of the original
networks’ links to train the models. After training and generation, we use degree distribution to
align the nodes in the generated networks with those in the original networks. Then we evaluate the
standard AUC metric on the task of individual link prediction by comparing links predicted in the
generated networks and links hidden during training in the original networks. In this experiment, we
expect to see the more effective privacy-protecting models generate networks that are less useful for
predicting individual links in the original networks, thus guaranteeing network data privacy.
All experiments are done with four GeForce GTX 1080 GPUs and a 12-core 2.2GHz CPU. The
training time of DP-enforced models is often slightly shorter due to early stops when the privacy
budget runs out, (e.g., a typical train of GVAE, DPGVAE, and DPGGAN takes 60, 42 and 53 seconds
on average on DBLP). The generation times of the three models are roughly the same (e.g., 0.02
second on average on DBLP). As a direct comparison, NetGAN and GraphRNN take longer times
under the same settings, especially for the generation (e.g., 89, and 4.5 seconds for NetGAN to train
and generate, and 75 and 2.4 seconds for GraphRNN, on DBLP). Although efficiency is not our
primary concern, short runtimes (especially for generation) are favorable for efficient data share.
Due to space limitation, detailed settings of the neural architectures and hyper-parameters of our
models are put into Appendix C.
1Statistics we use including LCC (size of the largest connected component), TC (triangle count), CPL
(characteristic path length), GINI (gini index) and REDE (relative edge distribution entropy).
7
Under review as a conference paper at ICLR 2021
	DBLP Networks					IMDB Networks				
Models	LCC	TC	CPL	GINI	REDE	LCC	TC	CPL	GINI	REDE
Original	107.5	59.90	3.6943	0.3248	0.9385	13.001	305.9	1.2275	0.1222	0.9894
GVAE(no DP)	7.51	66.93	0.1330	0.0213	0.0084	0.0145	25.83	0.0121	0.0030	0.0016
NetGAN(no DP)	9.66	39.87	0.1943	0.0105	0.0022	0.0083	27.54	0.0192	0.0042	0.0011
GraPhRNN(no DP)	10.27	57.43	0.2043	0.0415	0.0052	0.0594	27.26	0.0214	0.0155	0.0094
DPGVAE(ε=10)-	21.96	175.29	0.2471	0.0339	0.0153	0.0147	43.63	0.0367	0.0036	0.0030
DPGVAE(ε=1)	23.80	187.20	0.3059	0.0343	0.0156	0.0253	43.73	0.0373	0.0038	0.0031
DPGVAE(ε=0.1)	26.07	215.13	0.3342	0.0344	0.0158	0.0320	44.12	0.0392	0.0042	0.0032
DPGGAN(ε=10)-	10.61	64.75	0.2035	0.0224	0.0093	0.0040	22.89	0.0164	0.0010	0.0017
DPGGAN(ε=1)	12.38	70.97	0.2643	0.0353	0.0117	0.0053	23.81	0.0168	0.0029	0.0023
DPGGAN(ε=0.1)	24.62	77.41	0.2713	0.0485	0.0191	0.0113	24.91	0.0168	0.0029	0.0025
Table 1: Performance evaluation over compared models regarding a suite of important graph structural
statistics. The Original rows include the values of original networks, while the rest rows are the
average absolute difference between generated networks by different models and the original networks.
Therefore, smaller values indicate better capturing of global network structure and thus better global
data utility. Bold font is used for values ranked top-3.
		DBLP Networks				IMDB Networks		
Models	Degree dist.	Motif ct.	GIN acc.	Degree dist.	Motif ct.	GIN acc.
GVAE(no DP)	0.6171	0.4093	0.3029-	0.5132	0.4129	0.4698
NetGAN(no DP)	0.5754	0.4109	0.3471	0.4921	0.3891	0.4350
GraPhRNN(no DP)	0.5454	0.3672	0.3210	0.4635	0.3721	0.3875
DPGVAE(ε=1)	0.5476	0.4038	0.3043-	0.5081	0.4021	0.4625
DPGGAN(ε=1)	0.6092	0.4150	0.3261	0.5486	0.4150	0.4725
Table 2: Performance evaluation regarding degree distribution, motif counts and GIN accuracy.
Larger values for both cosine similarity and classification accuracy indicate better graph utility. Bold
font is used for values ranked top-2.
Preserving global structures. In Table 1, our strictly DP-constrained models constantly yield highly
competitive and even better results compared with the strongest DP-free baselines regarding global
network structural similarity between generated and original networks on both datasets, clearly
showing the effectiveness of our models on global network structure preservation. As we gradually
increase the privacy budget ε, our two models (especially DPGGAN) apparently perform better,
showing the effectiveness of our privacy constraints and a clear trade-off between privacy and utility.
Furthermore, as in Table 2, the graphs generated by DPGGan are competitively similar to the
original graphs regarding both degree distributions and motif counts, while achieving satisfactory
graph classification accuracy. The improvements of DPGGan all passed t-tests with p-value 0.01,
which corroborates our novel design of the structure-oriented graph generation framework.
Protecting individual links. For both datasets, links predicted on the networks generated by DPG-
GAN are much less accurate than those predicted on the original networks (26%-35% and 15%-20%
AUC drops on DBLP and IMDB, respectively) as well as the networks generated by all baselines.
This means even if the attackers identify nodes in the generated (released) networks of DPGGan,
they cannot leverage the information there to accurately infer the existence or absence of links
between particular pairs of nodes on the original networks. This directly corroborates our claim that
DPGGan is effective in protecting individual link privacy.
Due to space limit, more details and discussions regarding the experimental results are put into
Appendix D. In Appendix E, we also provide graph visualizations for qualitative visual inspections.
5	Conclusion
Due to the recent development of deep graph generation models, synthetic networks are generated and
released for granted, without the concern about possible privacy leakage over the original networks
used for model training. In this work, for the first time, we pay attention to the task of secure network
release and formulate its goals as preserving global network structure while protecting individual
link privacy. Subsequently, we adopt the well-studied DP framework and develop DPGGAN, which
protects individual link privacy by enforcing edge-DP on the graph generation model while preserving
global network structure with a structure-oriented graph discriminator. Comprehensive experiments
show that DPGGan is advantageous in generating networks that are globally similar to the original
ones (thus effectively maintaining network data utility), and at the same time, useless for predicting
individual links in the original network (thus rigorously protecting network data privacy).
8
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In SIGSAC, 2016.
Lars Backstrom, Cynthia Dwork, and Jon Kleinberg. Wherefore art thou r3579x?: anonymized social networks,
hidden patterns, and structural steganography. In WWW, 2007.
Albert-Ldszld BarabAsi and RCka Albert. Emergence of scaling in random networks. science, 286(5439):
509-512,1999.
Jeremiah Blocki, Avrim Blum, Anupam Datta, and Or Sheffet. The johnson-lindenstrauss transform itself
preserves differential privacy. FOCS, 2012.
Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to non-interactive database privacy.
JACM, 2013.
Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zugner, and StePhan Gunnemann. Netgan: Generating graphs
via random walks. In ICML, 2018.
Digvijay Boob, Rachel Cummings, Dhamma Kimpara, Uthaipon (Tao) Tantipongpipat, Chris Waites, and Kyle
Zimmerman. Private synthetic data generation via gans. arXiv preprint arXiv:1803.03148, 2018.
Z. Cai, Z. He, X. Guan, and Y. Li. Collective data-sanitization for preventing sensitive information inference
attacks in social networks. TDSC, 2018.
Qingrong Chen, Chong Xiang, Minhui Xue, Bo Li, Nikita Borisov, Dali Kaarfar, and Haojin Zhu. Differentially
private data generative models. arXiv preprint arXiv:1812.02274, 2018.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph isomorphism
testing and function approximation with gnns. In NIPS, 2019.
Quanyu Dai, Qiang Li, Jian Tang, and Dan Wang. Adversarial network embedding. In AAAI, 2018.
Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. arXiv
preprint arXiv:1805.11973, 2018.
Kun Dong, Austin R Benson, and David Bindel. Network density of states. KDD, 2019.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Theoretical Computer
Science, 9(3-4):211-407, 2014.
Jennifer G. Dy and Andreas Krause (eds.). Differentially private database release via kernel mean embeddings,
volume 80, 2018.
Pdl Erd6s and AlfrCd RCnyi. On the evolution of random graphs. PubL Math. Inst. Hungar Acad. Sci, 5:17-61,
1960.
TS Evans and Renaud Lambiotte. Line graphs, link partitions, and overlapping communities. Physical Review E,
80(1):016105, 2009.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence informa-
tion and basic countermeasures. In SIGSAC, 2015.
Lorenzo Frigerio, Anderson Santana de Oliveira, Laurent Gomez, and Patrick Duverger. Differentially private
generative adversarial networks for time series, continuous, and discrete open data. In IFIP SEC, 2019.
George Gondim-Ribeiro, Pedro Tabacof, and Eduardo Valle. Adversarial attacks on variational autoencoders.
arXiv preprint arXiv:1806.04646, 2018.
Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs. arXiv
preprint arXiv:1803.10459, 2017.
Xiaodong Gu, Kyunghyun Cho, Jungwoo Ha, and Sunghun Kim. Dialogwae: Multimodal response generation
with conditional wasserstein auto-encoder. In ICLR, 2019.
David Hallac, Youngsuk Park, Stephen Boyd, and Jure Leskovec. Network inference via the time-varying
graphical lasso. In KDD, 2017.
9
Under review as a conference paper at ICLR 2021
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NIPS,
2017.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph
generation. In ICML, 2018.
Shiva Prasad Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Analyzing graphs
with node differential privacy. In TCC, 2013.
NiColas Keriven and Gabriel Peyr6. Universal invariant and equivariant graph neural networks. In NIPS, 2019.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. In NIPS Workshop on Bayesian Deep
Learning, 2016.
Thomas N Kipf and Max Welling. Semi-supervised ClassifiCation with graph Convolutional networks. In ICLR,
2017.
Kristine M Kuhn. Compensation as a signal of organizational Culture: the effeCts of advertising individual or
collective incentives. IJHRM, 20(7):1634-1648, 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial maChine learning at sCale. ICLR, 2017.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond
pixels using a learned similarity metric. In ICML, 2016.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to
adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471, 2018.
Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.
stanford.edu/data, June 2014.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative models of
graphs. In ICML, 2018.
Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via regularizing
variational autoencoders. In NIPS, 2018.
Haggai Maron, Heli Ben-Hamu, Nadav Sharmir, and Lipman Yaron. Invariant and equivariant graph networks.
In ICLR, 2019.
Noman Mohammed, Rui Chen, Benjamin C. M. Fung, and Philip S. Yu. Differentially private data release for
data mining. In KDD, 2011.
Arvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large datasets (how to break anonymity
of the netflix prize dataset). SP, 2008.
Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. SP, 2009.
Mark EJ Newman. Clustering and preferential attachment in growing networks. Physical review E, 64(2):
025102, 2001.
Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Ulfar Erlingsson.
Scalable private learning with pate. In ICLR, 2018.
Alessandra Sala, Xiaohan Zhao, Christo Wilson, Haitao Zheng, and Ben Y. Zhao. Sharing graphs using
differentially private graph models. In SIGCOMM, 2011.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective
classification in network data. AI mag., 2008.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In SIGSAC, 2015.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against
machine learning models. In ISP, 2017.
Borkur Sigurbjornsson and Roelof Van Zwol. Flickr tag recommendation based on collective knowledge. In
WWW, 2008.
Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational
autoencoders. In ICANN, 2018.
10
Under review as a conference paper at ICLR 2021
Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of
academic social networks. In KDD, pp. 990-998, 2008.
Aleksei Triastcyn and Boi Faltings. Generating artificial data for private deep learning. AAAI, 2018.
Yue Wang and Xintao Wu. Preserving differential privacy in degree-correlation based graph generation. TDP, 6
(2):127-145, 2013.
Larry Wasserman and Shuheng Zhou. A statistical framework for differential privacy. JASA, 105(489):375-389,
2010.
Duncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’ networks. nature, 393(6684):440,
1998.
Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative adversarial
network. arXiv preprint arXiv:1802.06739, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In
ICLR, 2019.
Carl Yang, Lanxiao Bai, Chao Zhang, Quan Yuan, and Jiawei Han. Bridging collaborative filtering and
semi-supervised learning: a neural approach for poi recommendation. In KDD, 2017.
Carl Yang, Xiaolin Shi, Luo Jie, and Jiawei Han. I know you’ll be back: Interpretable new user clustering and
churn prediction on a mobile social application. In KDD, 2018.
Carl Yang, Peiye Zhuang, Wenhan Shi, Alan Luu, and Pan Li. Conditional structure generation through graph
variational generative adversarial nets. In NIPS, 2019.
Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for
goal-directed molecular graph generation. In NIPS, 2018a.
Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. Graphrnn: Generating realistic
graphs with deep auto-regressive models. In ICML, 2018b.
Aston Zhang, Xing Xie, Kevin Chen-Chuan Chang, Carl A Gunter, Jiawei Han, and XiaoFeng Wang. Privacy
risk in anonymized heterogeneous information networks. In EDBT, 2014.
Xinyang Zhang, Shouling Ji, and Ting Wang. Differentially private releasing via deep generative model (technical
report). arXiv preprint arXiv:1801.01594, 2018.
Yanjun Zhang, Xin Zhao, Xue Li, Mingyang Zhong, Caitlin Curtis, and Chen Chen. Enabling privacy-preserving
sharing of genomic data for gwass in decentralized networks. In WSDM, 2019.
Dongmian Zou and Gilad Lerman. Encoding robust representation for graph generation. arXiv preprint
arXiv:1809.10851, 2018.
Daniel Zugner, Amir Akbarnejad, and StePhan Gunnemann. Adversarial attacks on neural networks for graph
data. In KDD, 2018.
11
Under review as a conference paper at ICLR 2021
A APPENDIX: Proofs for Theorem 1
In this appendix, we provide proofs for Theorem 1, and derive Corollary 1.1 and Corollary 1.2. Theorem 1
indicates the link privacy protection achieved through updating model’s parameters with clipped and noised
gradient (latter referred to as DP learning) for link reconstruction based graph generation models. Corollary
1.1 and Corollary 1.2 derived from Theorem 1 support us to guarantee (, δ)-edge-DP for DPGVAE and
DPGGan with DP learning in Theorem 1.
The proof for Theorem 1 is divided into three steps. We first briefly introduce the definition of the moment
accountant privacy analysis and respective properties in Section A.1, for it being the fundamentals of our proof.
Note that in (Abadi et al., 2016), DPSGD is originally designed for classical machine learning tasks, such as
image classification. Therefore, in Section A.2, we leverage moment accountant to conduct the extended privacy
analysis of DPSGD for general types of data and loss functions. Then in Section A.3, we apply the conclusion
from Section A.2 on graph data and the link reconstruction loss function to derive the theoretical analysis over
edge-DP achieved by link reconstruction based graph generation models and finish our proof for Theorem
1. Following the conclusion in Theorem 1, we tune gradient representations to certain gradient functions
leveraged in training DPGVae decoder and DPGGan generator to derive Corollary 1.1 and Corollary 1.2, as
the theoretical support for the (ε, δ)-edge-DP held by respective models.
A. 1 Moment Accountant
Our proof for Theorem 1 is mainly based on moment accountant (Abadi et al., 2016). The definition of moment
accountant and the properties leveraged in our proof are listed below.
Definition 3. Let M : D → R be a randomized mechanism and d, d0 a pair of adjacent databases. Let aux
denote an auxiliary input. For an outcome o ∈ R, the privacy loss at o is defined as:
0	Pr[M(aux, d) = o]
C(°； M,aux,d，d), log PrM (aux,do) = o]	⑺
The privacy loss random variable C (M, aux, d, d0) is defined as c (M(d); M, aux, d, d0), i.e.the random
variable defined by evaluating the privacy loss at an outcome sampled from M(d).
Definition 4. Let M : D → R be a randomized mechanism and d, d0a pair of adjacent databases. Let aux
denote an auxiliary input. The moments accountant is defined as:
αM (λ) , max αM λ; aux, d, d0	(8)
aux,d,d0
where αM (λ; aux, d, d0) , log E [exp (λC (M, aux, d, d0))] is the moment generating function of the privacy
loss random variable.
The following properties of the moments accountant are proved in (Abadi et al., 2016).
Property 4.1. [Composability] Suppose that a mechanism M consists of a sequence of adaptive mechanisms
M1 , . . . , Mk where Mi : Qij-=11 Rj × D → Ri. Then, for any output sequence o1 , . . . , ok-1 and any λ, we
have
k
αM (λ; d,dz) = EaMi (λ; oi,... ,θi-ι,d,dz)	(9)
i=1
where αM is conditioned on Mi ’s output being oi for i < k.
Property 4.2. [Tail bound] For any ε > 0, the mechanism M is (ε, δ)-DP for
δ = min exp (αM (λ) - λε)	(10)
A.2 The Generalized Privacy Analysis of DPSGD
To achieve (, δ)-edge-DP for graph data, we exploit DPSGD (Abadi et al., 2016) with necessary adaptions
according to the special nature of graph data compared to other types of data (e.g., images), for which DPSGD
was originally designed. The original DPSGD only provides DP proof for gradient function f clipped by C with
its '2-norm sensitivity as ∆2f = 1 ∙ C = C. For classical tasks of machine learning like image classification,
∆2f = C is obvious. However, in a more complex task like graph learning, a minor change in the training
dataset can probably induce a different gap according to the chosen measurement. To explore the potential of
DPSGD with customized machine learning tasks, we further prove the privacy performance of DPSGD with a
gradient function f with '2-norm sensitivity ∆2f = s.
Therefore, to prepare for the proof for Theorem 1, we first leverage moments accountant to derive the upper
bound of privacy loss for a Gaussian Mechanism as below.
12
Under review as a conference paper at ICLR 2021
Lemma 1. Suppose that f : D → Rp with ∣∣f (∙)∣∣2 ≤ s. Let σ ≥ S and let J be a sample from [n] where each
2
i ∈ [n] is chosen independently with probability q < 16σ. Thenfor any positive integer λ ≤ 聂 ln qσ, the
Gaussian Mechanism M(d) = Pi∈J f (di) + N 0, σ2I satisfies
αM(λ) ≤ s2q2-(λ)+21) + O (s3q3λ3∕σ3)	(11)
Proof. Fix d and let d = d0 ∪ {dn}. Without loss of generality, We assume f (dn) = S ∙ eɪ and
Pi∈J \[n] f (di) = 0. Thus M(d) and M (d0) are distributed identically except for the first coordinate
and hence we have a one-dimensional problem. Let μo denote the pdf of N (0,σ2) and let μs denote the pdf of
N (s, σ2). We have
M (d0)〜μo,
(12)
M(d)〜μ，(1 一 q)μo + qμs.
We want to show that
Ez
〜μ [(μ(z)∕μo(z))[ ≤ α,
and Ez〜μo [(μo(z)∕μ(z))λ] ≤ α,
(13)
where α is a value to be determined. We will use the same method as in (Abadi et al., 2016) to prove both
bounds. Assume we have two distributions ν0 and νs , and we wish to bound
Ez〜V0 [(νo(z)/Vs(Z))λ] = Ez〜Vsh(Vo(z)/Vs(Z))λ+1] .	(14)
Leveraging binomial expansion, we obtain
Ez 〜Vsh(Vo(z)/Vs(Z))λ+1]
=Ez〜Vsh(I + (vo(z) — Vs(Z)) /Vs(z))λ+1]
=Ez〜Vsh(I + (vo(z) — Vs(Z)) /Vs(z))λ+1]
λ+1
=χ(	+1 ) Ez〜Vs [((“0(Z)-Vs(Z)) /Vs(Z))1.
(15)
The first term in Eq. (15) is 1, and the second term is
(λ + 1)E…/VO(Z) 一 Vs(Z)]=广 Vs(Z) V0(Z) -Vs(Z)dZ
s L	Vs(Z)	J -∞	Vs(Z)
= (λ+1) Z+∞V0(Z)dZ- Z+∞
-∞	-∞
Vs(Z)dZ
(16)
= (λ + 1)(1 - 1) = 0.
Regarding conditions stated in the lemma, for both cases, where Vo = μ, Vi = μo and Vo = μ0,V1 = μ, the
third term is bounded by q2λ(λ + 1)/(1 - q)σ2 and this bound dominates the sum of the remaining terms. We
provide the proof for the case of (vo = μo,Vs = μ), and the proof of the other case is similar.
13
Under review as a conference paper at ICLR 2021
To upper bound the third term in 15, we note that μ(z) ≥ (1 — q)μo(z), and write
E
μ0 (Z) ― μ(Z) ʌ 2
μ(z)
q2Ez 〜μ
μo(z) — μs(z) ʌ 2
μ (Z)
q2厂
—-co
(〃0(Z) 一 μs(Z))2 d
(17)
q2 尸(μo (Z) — μs (z))2
≤ ------ ----------------^τ∖------QZ
1 — q J-∞	μo(Z)
q2 E	I^(μ0(Z) - μs(z)∖2
口i	μo(Z))
Recalling the definition of μo and the normal distribution, we have
1 - exp (L) 了
EZ 〜μ0
μo(Z) 一 μι(Z) ∖ 2
μ0(Z)
=EZ 〜μ0
=1 — 2Ez 〜μo
(2sz — s2
exp(F-
+ EZ 〜μo
' ∕4sz — 2s2
exp(F2-
(18)
For the second term in Eq. (18) EZ〜μo [exp (2sZ-s2) ], we have
E
Z〜μo
exp
(2sz — s2 )
Z+∞
-∞
1
—尸 exp
σ vz2π
(19)
For the third term in Eq. (18), we have
EZ 〜μo
(20)
Thus, for Eq. (18), we have
EZ"(IJy )2 ] =exp (s2∕σ2) - 1.
(21)
Hence, the third term in the binomial expansion of Eq. (15) is
(1 + ʌ ) EZ∈“"(竿卓 )
‹ λ(λ + 1)q2
_ 2(1 — q)
(22)
For σ ≥ s, it is easy to get exp(冬)—1 ≤ 等.Therefore, we retrieve that
(1 + ʌ ) eg "(μo(Z)— μ(Z)2 ≤ λ(λ + 1)q2s2
k 2 J z∈m |_k	μ(Z)川—(I-q)σ2
(23)
By standard calculus, we get ∣Jo(z) — js(z)∣ = Rf-_s j0(z)Qz 卜 Note that j0(z) is monotonically decreasing
in (—∞, +∞). Thus, to bound the remaining terms, we derive
∀z ≤ 0 : ∣jo(z) — Js(z)∣ ≤ —s(z — s)μs(Z)∕σ2
∀z ≥ S : ∣Jo(z) — Js(z)∣ ≤ Zsμ0(Z)∕σ2
∀0 ≤ Z ≤ s : ∣μ0(Z) — Js(z)∣ ≤ μ0(Z) (exp (s2∕2σ2) — 1)
≤ s2μ0(Z)∕σ2.
(24)
14
Under review as a conference paper at ICLR 2021
We can then write
"f μo(Z) - μ(Z) Y
N	μ(z))
≤ Z μ(Z)
-∞
μo(z) - μ(z)
μ(Z)
tdZ
+ S μ(Z)
0
μo(z) - μ(z)
μ(Z)
tdZ
(25)
+ [ + μ(Z)
s
μo(z) - μ(z)
μ(Z)
tdZ.
We consider these terms individually. We repeatedly make use of three observations: (1) μo — μ =
q (μo — μs),(2)μ ≥ (1 — q)μo, (3)μ ≥ qμs, and (4) Eμ0 [∣Z∣t] ≤ σt(t — 1)!!. The first term can then
be bounded by
qt
(1 — q)t-1σ2t
Z0
-∞
μo(Z)∣Z — 1∣tdZ
qμs
(26)
≤Z0
-∞
qst(t - 1)!!
2σt
Then the second term is at most
qt
(1—>
S μ(Z)
0
I ( μo(Z) - μι(Z) Y
N	μo(Z))
dZ ≤
(1⅛ Z0 ”(Z)I(S22t|dZ
qts2t
(1 — q)tσ2t.
(27)
≤
Similarly, the third term is at most
tst	+∞
(1 — q)t-1 σ2t /	μ0(Z)IZldZ ≤
qtst(t - 1)!!
(1 — q)t-1σt
(28)
Under the assumptions on q, σ, and λ, itis easy to check that the three terms, and their sum, drop off geometrically
fast in t for t > 3. Hence the binomial expansion (5) is dominated by the t = 3 term, which is O s3q3λ3 /σ3 .
Therefore, the lemma is proved.	□
With Lemma 1, we retrieve the upper bound of privacy loss of the Gaussian Mechanism. Hence, based on
Lemma 1 and Property 4.1, we provide the generalized privacy analysis of DPSGD with different learning tasks,
which iteratively performs multiple times of the Gaussian Mechanism.
Lemma 2. Suppose that f : D → Rp with ∣∣f (∙)∣∣2 ≤ s. Lei J be a sample from [N ] that each i ∈ [N ] is
chosen independently in probability q = |J|/N, given the number of steps T, for any c0 ∈ (0, 1), there exist
explicit constants c1 and c2 that with any ε < c1q2T, iteratively computing T times of M(d) in Lemma 1
attains it with (ε, δ)-DP for any δ > 0 if we choose
σ ≥ C2 qs,T Iog(I@
ε
(29)
where ci ≥ 音 log qσ and c2 ≤
√C0(⅛ forany c0 ∈ (0,1)∙
Proof. Assume for now that σ, λ satisfy the conditions in Lemma 1. After T times of iteration, with Property
4.1 We derive that α(λ) ≤ Tq2s2λ2∕σ2. In order to to guarantee the whole training process to be (ε, δ) -DP,
combining α(λ) with Property 4.2, for any c0 ∈ (0, 1), we choose
Tq2 s2 λ2 /σ2 = c0λε,
exp((c0 — 1)λε) ≤ δ.
(30)
15
Under review as a conference paper at ICLR 2021
Plugging the condition λ ≤ σS2 log qσ into Eq. (30), We derive the bound for ε as ε < 专 log qs-q2T to
accomplish (ε, δ)-DP by setting
1
√co(1 - co)
qsvzT log(1∕δ)
ε
(31)
Where c0 ∈ (0, 1).
□
A.3 Privacy Analysis for the Link Reconstruction Based Graph Generation
Models with DPSGD
In this section, We conduct the theoretical privacy analysis for link reconstruction based graph generation model
based on Lemma 2 and obtain the conclusion of Theorem 1.
Theorem 1. In training a link reconstruction based graph generation model on a graph with N nodes with
batch size as B, given the sampling probability q = B∕N, and the number of steps T, there exist explicit
constants ci and c2 that for any ε < c1q2T, iteratively updating the model T times with gθ,L attains it with
(ε, δ)-edge-DP for any δ > 0 if we choose
q√T log(1∕δ)
σ ≥ c2-------------,
ε
where c1 ≥ * log q-, c2 ≤ 1/√co(1 - co) for any co ∈ (0,1).
Proof. Recall the expression of gθ,L as in Eq. (5)
gθ,L = NN (XX 卜Vi,θL∕max(1, k”Lk2 )) + N(0,σ2C2I)),
Where L is the loss function for a link reconstruction based graph generation model, C is the clipping hyper-
parameter for the model’s original gradient to bound the influence of each node, and σ is the noise scale
hyper-parameter. According to Definition 1, gθ,L is a Gaussian mechanism. Therefore, we first analyze the
'2-norm sensitivity of the clipped gradient function gθ,L, and then plug the sensitivity value to Lemma 2 and
conclude the privacy cost of training DPGVae, thus finishing the proof for Thereom 1.
Following the graph reconstruction procedure in (Simonovsky & Komodakis, 2018), a single value in the
adjacency matrix is sufficient to represent one edge in the respective graph for both directed and undirected
graphs. Therefore, referring to Definition 2, though changing an edge in the graph affects 2 nodes for node
classification tasks, for a structural inference task, i.e., graph reconstruction, as our work targeting at, adding or
removing an edge only results to at most 1 record difference. Together with Vvi,fL being clipped as its '2-norm
no more than C, we obtain the sensitivity of PN=I Vvi,fL∕ max(1, kqCfLk2) as C.
With plugging in the clipped Vvi,fL’s sensitivity (s = C ) into Lemma 2, we derive Theorem 1. We prove that,
given the sampling probability q = B/N and the number of steps T, with explicit constants ci ≥ 表 log q-
and c2 ≤
where co ∈ (0, 1), through iteratively updating model T times with Eq. (5), the outcome
generation model achieves (ε, δ)-edge-DP for any ε < ciq2T, and δ > 0 when we choose
σ ≥ c2 q√T!≡
ε
(32)
□
Recall the training process for the decoder in DPGVae and the generator in DPGGAN in Section 3. L in gθ,L
is substituted with Lrec and Lrec + λ2Lgan, respectively. For both Lrec and Lrec + λ2Lgan, their gradients
are clipped with C and adding Gaussian noises during the training process. Based on Theorem 1, we derive
Corollary 1.1 and 1.2 for the decoder in DPGVae and the generator in DPGGan respectively as below.
Corollary 1.1 (DPGVAE edge-DP). Under the same conditions in Theorem 1,iteratively updating the decoder
in DPGVae T times with gθ,Lrec attains it with (ε, δ)-edge-DP
Corollary 1.2 (DPGGAN edge-DP). Under the same conditions in Theorem 1, iteratively updating the generator
in DPGGan T times with gθ,(Lrec—λ2Lgan) attains it with (ε, δ)-edge-DP
With Corollary 1.1 and 1.2, under specified conditions, the public model (either the decoder in DPGVae or the
generator in DPGGAN) is guaranteed with (ε, δ)-edge-DP by the DP training process. For both DPGVAE de-
coder and DPGGan generator updated with noised and clipped representations of the sensitive training graph,
16
Under review as a conference paper at ICLR 2021
they only record noised and partial sensitive information. DPGVae decoder and DPGGan generator’s link
reconstruction procedures, reflecting its training information, only allude to the desensitize information rather
than the true sensitive training information. Thus, DPGVae decoder and DPGGan generator not only prevent
privacy leakage from their inner parameters with DP learning but also preserve the raw private training graphs
from being accurately inferred through the respective outputs.
B APPENDIX: Detailed Training Algorithm
The overall architecture of DPGGan is shown in Figure 2 in the main content. In Figure 2, the original graph is
fed into the GCN-based encoder network g to compute node embeddings, which is then compared with the prior
distribution N (0, I) and fed into the FNN-based decoder/generator network f to produce the reconstructed graph
and generated graph. After going through the GCN-based discriminator network part 1 (g0), a reconstruction
loss is computed between the reconstructed graph and the original graph, and a discrimination loss is computed
for the generated graph and original graph after the FNN-based discriminator network part 2 (f0).
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
Algorithm 1 DPGGAN
Input : Graph data G(A, X), clipping parameter C, decay ratio γ, privacy budget ε, noise scale σ, total
number of nodes N, batch size B = qN, learning rate η, maximum number of training epochs T, loss
weighing parameters λ1 and λ2
Output : Differentially private decoder f .
Initialize weights randomly for gμ, gσ, f, g0 and f0.
for epoch t = 0 to T do
for iteration i = 0 to N/B do
Sample a subgraph Gsub(Asub, Xsub) of size B
Mean vector: μ — gμ(Xsub, Asub)
Standard deviation vector: σ — gσ (Xsub, Asub)
Update q(Z∣X, A) — QN=I N(zi∖μi, diag(σ2))
Sample Zi, Zj 〜q(Z∖X, A)
Reconstruct adjacent matrix A0 - σ(f (Zi)T, f (Zj))
Lprior = DKL(q(Z∖X, A)kp(Z))
Lgan = log(D(A)) + log(1 - D(A0))
for node xi ∈ Gsub do
I Compute gθ(xi) 一 ∂(Lrec — λ2Lgan)∕∂Xi
end
Clip gradient: gθ(xi) — gθ(xi)∕max(1, kgθ(Ci)k2)
Perturb gradient 彰一B (Pi gθ(Xi) + N(0,σ2C2I))
Average gradient: gθ - -B Pi gθ (Xi)
Update f 4+— η ∙ gθ; f0, g0 <— η ∙ Vg0∙foλ2Lgan;	//apply DP learning to the generator
Update gμ, gσ 4— η ∙ Vg(Lrec + λ1 Lprior )
end
Update C = C * Y
end
Here, we give more details towards its training procedures in Algorithm 1. For a better description, we shorten
gθ,(Lrec-λ2Lgan)(%,(Lrec-入2Lgan) )as gθ ( gθ ).In the algorithm, for PrOPer gradient distortion, we devise
gradient clipping in Line 15 and noise injection in Line 16, which is only applied to the generator network f in
Line 18. In Line 21, we gradually reduce the clipping hyper-parameter C as the volume of gradients decreases
along training.
In the experimental analysis, existing works often fix δ as a dataset-specific value like 10-5, and then analyze
the performance of models based on fixed privacy budget ε (Abadi et al., 2016) or fixed noise scale σ (Papernot
et al., 2018). According to Theorem 1, our experiments are conducted with fixing the noise scale σ =
IqyJrTlog(1∕δ)∕ε, where ε < 2 log 亲q2T. In this work, other than the noise scale σ, to control the variance,
we also fix δ, the sampling ratio q, for better analysis of the model’s privacy loss. Note that we vary the number
of training iterations (query number) T to study the relation between the model’s performance and the privacy
budget.
17
Under review as a conference paper at ICLR 2021
(a) DBLP
Figure 3: Accuracy of links predicted based on networks generated by DPGGan with varying
hyper-parameters and evaluated on the original networks. Lower AUC means the information in the
generated networks is less useful in revealing the true existence or absence of links in the original
networks, thus better individual data privacy.
C APPENDIX: More details of experimental settings
(b) IMDB
For GVAE and our models, We use two-layer GCNs with sizes 32 → 16 for both gμ and gσ of the encoder
network, where the first layer is shared, and we use two-layer FNNs with sizes 16 → 32 for f of the decoder
(generator) network. For DPGGAN, we use another two-layer GCN with the same sizes for g0 and a three-layer
FNN with sizes 16 → 32 → 1 for f0. For DP-related hyper-parameters, we follow existing works (Abadi et al.,
2016; Shokri & Shmatikov, 2015) to fix δ to 10-5, σ to 5, and q to 0.01 (which determines the batch size B as
B = qN with N as the graph size). Then we vary ε from 0.1 to 10 to see how much graph-level utilities are
preserved under different privacy budgets. According to Eq. (32), we terminate the training of DPGGAN at T
iterations when ε is depleted. Other than the essential parameters in Eq. (32), we empirically set the clipping
parameter C to 5, decay ratio γ to 0.99, learning rate η to 10-3, and the loss weighing parameters λ1 and λ2
both to 0.1. We do not observe the model to be very sensitive to the setting of these non-essential parameters.
D APPENDIX: More details of experimental results
In this work, we define the goals of secure network release as preserving global network structure while
protecting individual link privacy. In the main content, we have presented experimental results to support the
effectiveness of DPGGan in both perspectives. That is, for global network structure preservation, we show
that the generated graphs of DPGGan are competitively similar to the original graphs in comparison with
the DP-free state-of-the-art graph generative models regarding a suite of commonly concerned global graph
statistics, and for individual link privacy protection, we show that the links predicted in the generated graphs of
DPGGan are useless (with low accuracy) when evaluated in the original graphs.
The suite of statistics measures the global network structure from different perspectives. As can be inferred from
TC, CPL, and GINI, the IMDB networks are in general smaller, tighter, and likely more structurally complex than
the DBLP networks, which favors link generation models (e.g., GVAE) over sequence generation models (e.g.,
NetGAN, and GraphRNN). Consequently, DPGGan also performs better on the IMDB networks, indicating its
advantages in modeling complex link structures as a whole.
In addition to the graph statistics, we further demonstrate the data utility of networks generated by DPGGan
with graph classification, which is the most widely studied graph-level downstream task. We deem this task
important towards evaluating network data utility, especially under our consideration of global network structure
preservation, because correct graph classification requires the generated graphs to share essential structural
properties with the original graphs. As we can see from Table 2 in the main paper, the data utilities evaluated
with graph classification are consistent with those evaluated with global graph statistics, as shown in Table 1
in the main paper. Our two DP-constrained models yield highly competitive performance compared with the
DP-free state-of-the-art graph generative models.
As for privacy protection, we conduct more detailed inspections of the performance of individual link prediction.
In particular, we vary two of the major hyper-parameters, i.e., the privacy budget ε, and sampling ratio q.
Consistently with the results in Table 1, larger privacy budgets lead to more privacy leakage, which allows
attackers to infer individual links in the original networks with higher accuracy. While some DP-constrained
deep learning models are observed to be sensitive to the sampling ratio during training (Abadi et al., 2016; Shokri
& Shmatikov, 2015), the privacy protection of DPGGAN is robust when q is changed in large ranges in practice.
18
Under review as a conference paper at ICLR 2021
E APPENDIX: Qualitative Graph Visualizations
To understand the behaviors and performances of compared algorithms, we conduct visualizations between the
original graphs and graphs generated graphs by different algorithms. We mainly focus on the visualization of
DBLP author networks, since they are in general smaller, sparser and thus easier to visually inspect. We draw
the graphs with NetworkX2.
In general, as we can observe in Figure 4-10:
1.	The graphs generated by algorithms without DP constraints like NetGAN and GraphRNN are more
similar to the original graphs, which is consistent with our results in Table 1 and 2 in the main paper.
2.	After enforcing the DP constraints, the graphs generated by DPGVae are significantly different from
the graphs generated by GVAE, especially regarding local structures around individual nodes.
3.	The graphs generated by DPGGan, while also having different local structures compared with those
generated by GVAE, have more similar global structures with the original graphs.
Figure 4: Visualizations on DBLP author network 1.
2https://networkx.github.io/
19
Under review as a conference paper at ICLR 2021
Figure 5: Visualizations on DBLP author network 2.
Figure 6: Visualizations on DBLP author network 3.
20
Under review as a conference paper at ICLR 2021
Figure 7: Visualizations on DBLP author network 4.
Figure 8: Visualizations on DBLP author network 5.
21
Under review as a conference paper at ICLR 2021
Figure 9: Visualizations on DBLP author network 7.
Figure 10: Visualizations on DBLP author network 7.
22