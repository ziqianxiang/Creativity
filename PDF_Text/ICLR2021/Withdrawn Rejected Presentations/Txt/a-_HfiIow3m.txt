Under review as a conference paper at ICLR 2021
Multimodal Variational Autoencoders for
Semi-Supervised Learning: In Defense of
Product-of-Experts
Anonymous authors
Paper under double-blind review
Ab stract
Multimodal generative models should be able to learn a meaningful latent rep-
resentation that enables a coherent joint generation of all modalities (e.g., im-
ages and text). Many applications also require the ability to accurately sample
modalities conditioned on observations of a subset of the modalities. Often not
all modalities may be observed for all training data points, so semi-supervised
learning should be possible. In this study, we evaluate a family of product-of-
experts (PoE) based variational autoencoders that have these desired properties.
We include a novel PoE based architecture and training procedure. An empirical
evaluation shows that the PoE based models can outperform an additive mixture-
of-experts (MoE) approach. Our experiments support the intuition that PoE mod-
els are more suited for a conjunctive combination of modalities while MoEs are
more suited for a disjunctive fusion.
1	Introduction
Multimodal generative modelling is important because information about real-world objects typi-
cally comes in different representations, or modalities. The information provided by each modality
may be erroneous and/or incomplete, and a complete reconstruction of the full information can often
only be achieved by combining several modalities. For example, in image- and video-guided trans-
lation (Caglayan et al., 2019), additional visual context can potentially resolve ambiguities (e.g.,
noun genders) when translating written text.
In many applications, modalities may be missing for a subset of the observed samples during train-
ing and deployment. Often the description of an object in one modality is easy to obtain, while
annotating it with another modality is slow and expensive. Given two modalities, we call samples
paired when both modalities are present, and unpaired if one is missing. The simplest way to deal
with paired and unpaired training examples is to discard the unpaired observations for learning. The
smaller the share of paired samples, the more important becomes the ability to additionally learn
from the unpaired data, referred to as semi-supervised learning in this context (following the termi-
nology from Wu & Goodman, 2019. Typically one would associate semi-supervised learning with
learning form labelled and unlabelled data to solve a classification or regression tasks). Our goal is
to provide a model that can leverage the information contained in unpaired samples and to inves-
tigate the capabilities of the model in situations of low levels of supervision, that is, when only a
few paired samples are available. While a modality can be as low dimensional as a label, which can
be handled by a variety of discriminative models (van Engelen & Hoos, 2020), we are interested in
high dimensional modalities, for example an image and a text caption.
Learning a representation of multimodal data that allows to generate high-quality samples requires
the following: 1) deriving meaningful representation in a joint latent space for each high dimen-
sional modality and 2) bridging the representations of different modalities in a way that the relations
between them are preserved. The latter means that we do not want the modalities to be represented
orthogonally in the latent space - ideally the latent space should encode the object's properties
independent of the input modality. Variational autoencoders (Kingma & Welling, 2014) using a
product-of-experts (PoE, Hinton, 2002; Welling, 2007) approach for combining input modalities are
a promising approach for multimodal generative modelling having the desired properties, in partic-
1
Under review as a conference paper at ICLR 2021
ular the VAEVAE model developed by Wu & Goodman (2018) and a novel model termed SVAE,
which we present in this study. Both models can handle multiple high dimensional modalities, which
may not all be observed at training time.
It has been argued that a product-of-experts (PoE) approach is not well suited for multimodal gener-
ative modelling using variational autoencoders (VAEs) in comparison to additive mixture-of-experts
(MoE). It has empirically been shown that the PoE-based MVAE (Wu & Goodman, 2018) fails to
properly model two high-dimensional modalities in contrast to an (additive) MoE approach referred
to as MMVAE, leading to the conclusion that “PoE factorisation does not appear to be practically
suited for multi-modal learning” (Shi et al., 2019). This study sets out to test this conjecture for
state-of-the-art multimodal VAEs.
The next section summarizes related work. Section 3 introduces SVAE as an alternative PoE based
VAE approach derived from axiomatic principles. Then we present our experimental evaluation of
multimodal VAEs before we conclude.
2	Background and Related Work
We consider multimodal generative modelling. We mainly restrict our considerations to two modal-
ities x1 ∈ X1 , x2 ∈ X2, where one modality may be missing at a time. Extensions to more modal-
ities are discussed in Experiments section and Appendix D. To address the problem of generative
cross-modal modeling, one modality x1 can be generated from another modality x2 by simply us-
ing independently trained generative models (x1 → x2 and x2 → x1 ) or a composed but non-
interchangeable representation (Wang et al., 2016; Sohn et al., 2015). However, the ultimate goal of
multimodal representation learning is to find a meaningful joint latent code distribution bridging the
two individual embeddings learned from x1 and x2 alone. This can be done by a two-step procedure
that models the individual representations first and then applies an additional learning step to link
them (Tian & Engel, 2019; Silberer & Lapata, 2014; Ngiam et al., 2011). In contrast, we focus on
approaches that learn individual and joint representations simultaneously. Furthermore, our model
should be able to learn in a semi-supervised setting. Kingma et al. (2014) introduced two models
suitable for the case when one modality is high dimensional (e.g., an image) and another is low
dimensional (e.g., a label) while our main interest are modalities of high complexity.
We consider models based on variational autoencoders (VAEs, Kingma & Welling, 2014; Rezende
et al., 2014). Standard VAEs learn a latent representation z ∈ Z for a set of observed variables
x ∈ X by modelling a joint distribution p(x, z) = p(z)p(x|z). In the original VAE, the intractable
posterior q(z|x) and conditional distribution p(x|z) are approximated by neural networks trained by
maximising the ELBO loss taking the form
L = Eq(z|x)[log p(x|z)] - DKL(q(z|x) k N (0, I))	(1)
with respect to the parameters of the networks modelling q(z|x) and p(x|z). Here Dkl(∙ ∣∣ ∙)
denotes the Kullback-Leibler divergence. Bi-modal VAEs that can handle a missing modality extend
this approach by modelling q(z|x1, x2) as well as q1(z|x1) and q2(z|x2), which replace the single
q(z|x). Multimodal VAEs may differ in 1) the way they approximate q(z|x1, x2), q1(z|x1) and
q2(z|x2) by neural networks and/or 2) the structure of the loss function, see Figure 1. Typically,
there are no conceptual differences in the decoding, and we model the decoding distributions in the
same way for all methods considered in this study.
Suzuki et al. (2017) introduced a model termed JMVAE (Joint Multimodal VAE), which belongs
to the class of approaches that can only learn from the paired training samples (what we refer to
as the (fully) supervised setting). It approximates q(z|x1, x2), q1(z|x1) and q2(z|x2) with three
corresponding neural networks and optimizes an ELBO-type loss of the form
L = Eq(z|x1,x2)[log p1(x1|z) + logp2(x2|z)] - DKL(q(z|x1,x2) ∣ N(0,I))
- DKL(q(z|x1,x2) ∣ q1(z|x1)) - DKL(q(z|x1, x2) ∣ q2(z|x2)) . (2)
The last two terms imply that during learning the joint network output must be generated which
requires paired samples.
The MVAE (Multimodal VAE) model (Wu & Goodman, 2018) is the first multimodal VAE-based
model allowing for missing modalities that does not require any additional network structures for
2
Under review as a conference paper at ICLR 2021
Gaussian prior
Latent space
Product ofexperts
-<——A KL divergence
Figure 1: Schematic overview bi-modal VAEs using a PoE and additional network structures that
are capable of semi-supervised learning without requiring a two step learning procedure. VAEVAE
(a) and (b) are by Wu & Goodman (2019), JMVAE is by Suzuki et al. (2017), MVAE is by Wu &
Goodman (2018), and SVAE is our newly proposed model. Each triangle stands for an individual
neural network, the colors indicate the two different modalities.
(proposed model)
Encoders
learning the joint latent code distribution. The joint posterior is modeled using a product-of-experts
(PoE) as q(z|x1:M) = Qm qm(z|xm). For the missing modality qk(z|xk) = 1 is assumed. The
model allows for semi-supervised learning while keeping the number of model parameters low.
The bridged model (Yadav et al., 2020) highlights the need for an additional network structure for
approximating the joint latent code distribution. It attempts to keep the advantages of the additional
encoding networks. It reduces the number of model parameters by introducing the bridge encoder
that consists of one fully connected layer which takes z1 and z2 latent code vectors generated from
x1 and x2 and outputs the mean and the variance of the joint latent code distribution.
The arguably most advanced multimodal VAE models is VAEVAE by Wu & Goodman (2019),
which we discuss in detail in the next section (see also Appendix C).
Shi et al. (2019) proposed a MoE model termed MMVAE (Mixture-of-experts Multimodal VAE). In
MMVAE model the joint variational posterior for M modalities is approximated as q(z|x1:M) =
Pm αmqm(z∣xm) Where am = M. The model utilizes a loss function from the importance
weighted autoencoder (IWAE, Burda et al., 2016) that computes a tighter lower bound compared
to the VAE ELBO loss. The MoE rule formulation alloWs in principle to train With a missing modal-
ity i by assuming αi = 0, hoWever, Shi et al. (2019) do not highlight or evaluate this feature. There
are benchmarks in the paper that compare MVAE (Wu & Goodman, 2018) and MMVAE, conclud-
ing that MVAE often fails to learn the joint latent code distribution. Because of these results and
those presented by Wu & Goodman (2019), We did not include MVAE as a benchmark model in our
experiments.
3	VAEVAE AND SVAE
We developed a neW approach as an alternative to VAEVAE. Both models 1) are VAE based; 2)
alloW for interchangeable cross-model generation as Well as a learning joint embedding; 3) alloW for
missing modalities at training time; and 4) can be applied to tWo similarly complex high dimensional
3
Under review as a conference paper at ICLR 2021
modalities. Next, we will briefly present our new model SVAE. Then we highlight the differences to
VAEVAE. Finally, we state a newly derived objective function for training the models. We consider
two modalities and refer to Appendix D for generalizations to more modalities.
SVAE Since both modalities might not be available for all the samples, it should be possible to
marginalize each of them out of q(z|x1, x2). While the individual encoding distributions q(z|x1)
and q(z|x2) can be approximated by neural networks as in the standard VAE, we need to define a
meaningful approximation of the joint encoding distribution q(z|x) = q(z|x1, x2). In the newly
proposed SVAE model, these distributions are defined as the following:
q(z∖xι,X2) = Z(χ1 X ) q1(z∣x1)q2(z∣x2)
q(z∣χι) = qι(z∖χι)q2(z∖χι)
q(z∖χ2) = q2(z∖χ2)q~z∖χ2)
q(z)=N(0,I)
(3)
(4)
(5)
(6)
The model is derived from an axiomatic proof that is given in Appendix A. The desired properties
of the model were that 1) when no modalities are observed the generating distribution for the latent
code is Gaussian, 2) the modalities are independent given the latent code, 3) both experts cover
the whole latent space with equal probabilities, and 4) the joint encoding distribution q(z∖x1, x2) is
modelled by a PoE.
The distributions qι(z∖χι), q2(z∖χ2), qg(z∖χι) and q^(z∖χ2) are approximated by neural networks.
In case both observations are available, q(z∖x1, x2) is approximated by applying the product-of-
experts rule with q1(z∖x1) and q2(z∖x2) being the experts for each modality. In case of a missing
modality, equation 4or5is used. If, for example, x2 is missing, the qg(z∖χι) distribution takes over
as a “replacement” expert, modelling marginalization over x2 .
SVAE vs. VAEVAE The VAEVAE model (Wu & Goodman, 2019) is the most similar to ours. Wu
& Goodman define two variants which can be derived from the SVAE model in the following way.
Variant (a) can be derived by setting q*(z∖χι) = q*(z∖χ2) = 1. Variant (b) is obtained from (a)
by additionally using a separate network to model q(z∖x1, x2). Having a joint network q(z∖x1, x2)
implements the most straightforward way of capturing the inter-dependencies of the two modalities.
However, the joint network cannot be trained on unpaired data - which can be relevant when the
share of supervised data gets smaller. Option (a) uses the product-of-experts rule to model the joint
distribution of the two modalities as well, but does not ensure that both experts cover the whole latent
space (in contrast to SVAE, see equation A.14 in the appendix), which can lead to individual latent
code distributions diverging. Based on this consideration and the experimental results from Wu &
Goodman (2019), we focused on benchmarking VAEVAE (b) and refer to it as simply VAEVAE in
Section 4.
SVAE resembles VAEVAE in the need for additional networks besides one encoder per each modal-
ity and the structure of ELBO loss. It does, however, solve the problem of learning the joint embed-
dings in a way that allows to learn the parameters of approximated q(z∖x1, x2) using all available
samples, i.e., both paired and unpaired. If q(z∖x1, x2) is approximated with the joint network that
accepts concatenated inputs, as in JMVAE and VAEVAE (b), the weights of q(z∖x1, x2) can only
be updated for the paired share of samples. If q(z∖x1, x2) is approximated with a PoE of decoupled
networks as in SvAe, the weights are updated for each sample whether paired or unpaired - which
is the key differentiating feature of SVAE compared to existing architectures.
4
Under review as a conference paper at ICLR 2021
A New Objective Function When developing SVAE, we devised a novel ELBO-type loss:
= ppaired (x1,x2) Eq(z|x1,x2)[log p1(x1|z) + logp2(x2|z)]
- DKL(q(z|x1, x2) k p(z|x1)) - DKL(q(z|x1,x2) k p(z|x2))
+ Eppaired(x1) Eq(z|x1)[log p1(x1|z)] - DKL(q(z|x1)	k p(z))
+ Eppaired(x2) Eq(z|x2)[log p2(x2|z)] - DKL(q(z|x2)	k p(z))	(7)
L1	=Epunpaired(x1) Eq(z|x1)[log p1(x1|z)] - DKL(q(z|x1) k	p(z))	(8)
L2	=Epunpaired(x2) Eq(z|x2)[log p2(x2|z)] - DKL(q(z|x2) k	p(z))	(9)
Lcomb =L + L1 + L2	(10)
Here ppaired and punpaired denote the distributions of the paired and unpaired training data, respectively.
The loss function is derived in Appendix B. The differences between this loss and the loss function
used to train VAEVAE by Wu & Goodman (2019) are highlighted in Appendix C.
4	Experiments
We conducted experiments to compare state-of-the-art PoE based
VAEs with the MoE approach MMVAE (Shi et al., 2019). We consid-
ered VAEVAE (b) as proposed by Wu & Goodman (2019) and SVAE
as described above. The two approaches differ both in the underlying
model as well as the objective function. For a better understanding
of these differences, we also considered an algorithm referred to as
VAEVAE*, which has the same model architecture as VAEVAE and
the same loss function as SVAE.1 The difference in the training proce-
dure for VAEVAE and VAEVAE* is described in Appendix C. Since
the VAEVAE implementation was not publicly available at the time
of writing, we used our own implementation of VAEVAE based on
the PiXYZ library.2 For details about the experiments we refer to Ap-
pendix F. The source code to reproduce the experiments can be found
in the supplementary material.
7
9
4
9
O
6
Oracle
Oracle
Figure 2: MNIST-SVHN
reconstruction for fully su-
pervised VAEVAE.

For an unbiased evaluation, we considered the same test problems and performance metrics as Shi
et al. (2019). In addition, we designed an experiment referred to as MNIST-Split that was supposed
to be well-suited for PoE. In all experiments we kept the network architectures as similar as possible
(see Appendix F). For the new benchmark problem, we constructed a multi-modal dataset where the
modalities are similar in dimensionality as well as complexity and are providing missing information
to each other rather than duplicating it. The latter should favor a PoE modelling, which suits an
“AND” combination of the modalities, and not a MoE modeling, which is more aligned with an
“OR” combination.
We measured performance for different supervision levels for each dataset (e.g., 10% supervision
level means that 10% of the training set samples were paired and the remaining 90% were unpaired).
Image and image: MNIST-Split We created an image reconstruction dataset based on MNIST
digits (LeCun et al., 1998). The images were split horizontally into equal parts, either two or three
depending on the experimental setting. These regions are considered as different input modalities.
In the above notion of “AND” and “OR” tasks we implicitly assume an additional modality, which
is the image label in this case. The fact that the correct digit can sometimes be guessed from only
one part of the image makes the new MNIST-Split benchmark a mixture of an “AND” and an “OR”
task. This is in contrast to the MNIST-SVHN task described below, which can be regarded as an
almost pure “OR” task.
Two modalities: MNIST-Split. In the bi-modal version referred to as MNIST-Split, the
MNIST images were split in top and bottom halves of equal size, and the halves were then used
1We also evaluated SVAE*, our model with the VAEVAE loss function, but it never outperformed other
models.
2https://github.com/masa-su/pixyz
5
Under review as a conference paper at ICLR 2021
Supervision level
100%	0.1%
Figure 3: MNIST-Split image reconstructions of a top half and a bottom half given (a) the top half;
(b) the bottom half of the original image.
as two modalities. We tested the quality of the image reconstruction given one or both modalities by
predicting the reconstructed image label with an independent oracle network, a ResNet-18 (He et al.,
2016) trained on the original MNIST dataset. The evaluation metrics were joint coherence, synergy,
and cross-coherence. For measuring joint coherence, 1000 latent space vectors were generated from
the prior and both halves of an image were then reconstructed with the corresponding decoding net-
works. The concatenated halves yield the fully reconstructed image. Since the ground truth class
labels do not exist for the randomly sampled latent vectors, we could only perform a qualitative
evaluation, see Figure 3. Synergy was defined as the accuracy of the image reconstruction given
both halves. Cross-coherence considered the reconstruction of the full image from one half and was
defined as the fraction of class labels correctly predicted by the oracle network.
Table 1: Evaluation of the models trained on the fully supervised datasets.
	Accuracy (both) Accuracy (top half) Accuracy (bottom half)
MMVAE SVAE VAEVAE VAEVAE*	0.539	0.221	0.283 0.948	0.872	0.816 0.956	0.887	0.830 0.958	0.863	0.778
Figure 4: MNIST-Split dataset. Accuracy of an oracle network applied to images reconstructed
given (a) the full image (both halves) (b) the top half (c) the bottom half.
The quantitative results are shown in Table 1 and Figure 4. All PoE architectures clearly outper-
formed MMVAE even when trained on the low supervision levels. In this experiment, it is important
that both experts agree on a class label. Thus, as expected, the multiplicative PoE fits the task much
better than the additive mixture. Utilizing the novel loss function (10) gave the best results for very
low supervision (SVAE and VAEVAE*).
Three modalities: MNIST-Split-3. We compared a simple generalization of the SVAE
model to more than two modalities with the canonical extension of the VAEVAE model (both de-
fined in Appendix D) on the MNIST-Split-3 data, the 3-modal version of MNIST-Split task. Figure 6
6
Under review as a conference paper at ICLR 2021
Encoders
∆x, ∆x2∆x3
∕∖xi, X2 ∕∖×2, X3 ∕∖xi, X3
^^^∖,×2,×3
Decoders
X1 X2 X3
Gaussian prior
Z) Latent space
4 > KL divergence
Figure 5: The SVAE and VAEVAE network architectures for 3-modalities case. The number of
parameters is kn2 for SVAE and kn2n-1 for VAEVAE, where n is the number of modalities and k
is the number of parameters in one encoding network.
Figure 6: MNIST-Split-3 dataset, reproducing the logic of MNIST-Split for the images splitted in
three parts.
shows that SVAE performed better when looking at the individual modalities reconstructions. While
the number of parameters in the bi-modal case is the same for SVAE and VAEVAE, it grows expo-
nentially for VAEVAE and stays in order of n2 for SVAE where n is the number of modalities, see
Figure 5 and Appendix D for details.
Image and image: MNIST-SVHN The first dataset considered by Shi et al. (2019) is constructed
by pairing MNIST and SVHN (Netzer et al., 2011) images showing the same digit. This dataset
shares some properties with MNIST-Split, but the relation between the two modalities is different:
the digit class is derived from a concatenation of two modalities in MNIST-Split, while in MNIST-
SVHN it could be derived from any modality alone, which corresponds to ”AND” combination of
the modalities and favors the MoE architecture. As before, oracle networks are trained to predict
the digit classes of MNIST and SVHN images. Joint coherence was again computed based on
1000 latent space vectors generated from the prior. Both images were then reconstructed with the
corresponding decoding networks. A reconstruction was considered correct if the predicted digit
classes of MNIST and SVHN were the same. Cross-coherence was measured as above.
Figure 2 shows examples of paired image reconstructions from the randomly sampled latent space of
the fully supervised VAEVAE model. The digit next to the each reconstruction shows the digit class
prediction for this image. The quantitative results in Figure 7 show that all three PoE based models
reached a similar joint coherence as MMVAE, VAEVAE scored even higher. The cross-coherence
7
Under review as a conference paper at ICLR 2021
Figure 7: Performance on MNIST-SVHN for different supervision levels. (a) Joint coherence, a
share of generated images with the same digit class; (b) Cross-coherence, accuracy of SVHN recon-
structions given MNIST; (c) Cross-coherence, accuracy of MNIST reconstructions given SVHN.
results were best for MMVAE, but the three PoE based models performed considerably better than
the MVAE baseline reported by Shi et al. (2019).
Image and text: CUB-Captions The second benchmark considered by Shi et al. (2019) is the
CUB Images-Captions dataset (Wah et al., 2011) containing photos of birds and their textual de-
scriptions. Here the modalities are of different nature but similar in dimensionality and information
content. We used the source code3 by Shi et al. to compute the same evaluation metrics as in
the MMVAE study. Canonical correlation analysis (CCA) was used for estimating joint and cross-
coherences of images and text (Massiceti et al., 2018). The projection matrices Wx for images and
Wy for captions were pre-computed using the training set of CUB Images-Captions and are avail-
able as part of the source code. Given a new image-caption pair x, y, We computed the correlation
hefween the f∖x∕c h∖r r*crr/φ 方) — 。(一) φ(y) Where ZArD — T∕TΛTZ→   ɑʌTy(TzjyTD
between the two by Corr(X,y)= ∣∣φ(χ)kkφ(y)k , where φ(k) = Wyk k - avg(Wk k).
We employed the same image generation procedure as in the MMVAE study. Instead of creating
the images directly, we generated 2048-d feature vectors using a pre-trained ResNet-101. In order
to find the resulting image, a nearest neighbours lookup with Euclidean distance was performed. A
CNN encoder and decoder was used for the (see Table F.5 and Table F.6). Prior to computing the
correlations, the captions were converted to 300-d vectors using FastText (Bojanowski et al., 2017).
As in the experiment before, we used the same network architectures and hyperparameters as Shi
et al. (2019). We sampled 1000 latent space vectors from the prior distribution. Images and captions
were then reconstructed with the decoding networks. The joint coherence was then computed as
the CCA for the resulting image and caption averaged over the 1000 samples. Cross-coherence was
computed from caption to image and vice versa using the CCA averaged over the whole test set.
As can be seen in Figure 8, VAEVAE showed the best performance among all models. With full
supervision the VAEVAE model outperformed MMVAE in all three metrics. The cross-coherence
of the three PoE models was higher or equal to MMVAE except for very low supervision levels. All
three PoE based models were consistently better than MVAE.
5 Discussion and Conclusions
We studied bi-modal variational autoencoders (VAEs) based on a product-of-experts
(PoE)architecture, in particular VAEVAE as proposed by Wu & Goodman (2019) and a new
model SVAE, which we derived in an axiomatic way, and represents a generalization of the VAE-
VAE architecture. The models learn representations that allow coherent sampling of the modalities
and accurate sampling of one modality given the other. They work well in the semi-supervised
setting, that is, not all modalities need to be always observed during training. It has been argued
that the mixture-of-experts (MoE) approach MMVAE is preferable to a PoE for multimodal VAEs
(Shi et al., 2019), in particular in the fully supervised setting (i.e., when all data are paired).
3https://github.com/iffsid/mmvae
8
Under review as a conference paper at ICLR 2021
Figure 8: CUB Images-Captions dataset. Performance metrics for different supervision levels. (a)
Joint coherence, the correlation between images and labels reconstructed from the randomly sam-
pled latent vectors; (b) Cross-coherence, the correlation of the reconstructed caption given the image;
(c) Cross-coherence, the correlation of the reconstructed image given the caption.
Original image
This bird has black
feathers with a bright
black bill on and black
This bird has wings
that are black and
has a red belly
SVAE VAEVAE
A large bird f has a
white ,and and and
black orange bill
The bird has a long
black bill that rest
and curved
Original caption
This bird is black with
white and has a long,
pointy beak
Reconstruction from the image
This bird is black with
white and has a long,
pointy beak
This bird is black with
white and has a long,
pointy beak
Reconstruction from the caption
Original image
Reconstruction from the image
Original caption Reconstruction from the caption
Figure 9: Examples of image and caption reconstructions given one modality input for SVAE and
VAEVAE. Given that the caption can be broad (e.g., ”this bird is black and white and has a long
pointy beak” in the example), it can fit many different images. In this case, the image from the
caption reconstruction tends to better fit the description than the original image. The same goes
for images: one of the reconstructed images has a bird with a red belly which got reflected in the
generated caption even though it was not a part of the original caption.
This conjecture was based on a comparison with the MVAE model (Wu & Goodman, 2018),
but is refuted by our experiments showing that VAEVAE and our newly proposed SVAE can
outperform MMVAE on experiments conducted by Shi et al. (2019). Intuitively, MoEs are more
tailored towards an “OR” (additive) combination of the information provided by the different
modalities, while PoEs are more tailored to towards an “AND” (multiplicative) combination. This
is demonstrated by our experiments on halved digit images, where a conjunctive combination
is helpful and the PoE models perform much better than MMVAE. We also expand SVAE and
VAEVAE to 3-modal case and show that SVAE demonstrates better performance on individual
modalities reconstructions while having less parameters than VAEVAE.
9
Under review as a conference paper at ICLR 2021
References
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with
subword information. Transactions ofthe Associationfor Computational Linguistics, 5:135-146,
2017.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Inter-
national Conference on Learning Representations (ICLR), 2016.
Ozan Caglayan, Pranava Madhyastha, LUcia SPecia, and Lolc Barrault. Probing the need for Vi-
sual context in multimodal machine translation. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies (NAACL HLT), volume 1, PP. 4159-4170, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), PP.
770-778, 2016.
Geoffrey E. Hinton. Training Products of exPerts by minimizing contrastive divergence. Neural
Computation, 14(8):1771-1800, 2002.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In International
Conference on Learning Representations (ICLR), 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Confer-
ence on Learning Representations (ICLR), 2014.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-suPervised
learning with deeP generative models. In Advances in Neural Information Processing Systems
(NeurIPS), PP. 3581-3589, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Daniela Massiceti, Puneet K. Dokania, N. Siddharth, and Philip H. S. Torr. Visual dialogue with-
out vision or dialogue. In NeurIPS Workshop on Critiquing and Correcting Trends in Machine
Learning, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011.
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y. Ng. Mul-
timodal deep learning. In Proceedings of the 28th International Conference on International
Conference on Machine Learning (ICML), pp. 689-696, 2011.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In Proceedings of the 31st International Confer-
ence on Machine Learning (ICML), volume 32(2) of Proceedings of Machine Learning Research,
pp. 1278-1286. PMLR, 2014.
Yuge Shi, Siddharth N, Brooks Paige, and Philip Torr. Variational mixture-of-experts autoencoders
for multi-modal deep generative models. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 15718-15729, 2019.
Carina Silberer and Mirella Lapata. Learning grounded meaning representations with autoencoders.
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics
(ACL), volume 1, pp. 721-732. Association for Computational Linguistics, 2014.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep
conditional generative models. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 3483-3491. 2015.
10
Under review as a conference paper at ICLR 2021
Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Joint Multimodal Learning with Deep
Generative Models. In International Conference on Learning Representations Workshop (ICLR)
Workshop Track, 2017.
Yingtao Tian and Jesse Engel. Latent translation: Crossing modalities by bridging generative mod-
els, 2019. arXiv:1902.08261.
Jesper E. van Engelen and Holger H. Hoos. A survey on semi-supervised learning. Machine Learn-
ing,109(2):373-440, 2020.
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-
UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of
Technology, 2011.
Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep variational canonical correla-
tion analysis, 2016. arXiv:1610.03454.
Max Welling. Product of experts. Scholarpedia, 2(10):3879, 2007.
Mike Wu and Noah Goodman. Multimodal generative models for scalable weakly-supervised learn-
ing. In Advances in Neural Information Processing Systems 31, pp. 5575-5585, 2018.
Mike Wu and Noah Goodman. Multimodal generative models for compositional representation
learning, 2019. arXiv:1912.05075.
Ravindra Nath Yadav, Ashish Sardana, Vinay P. Namboodiri, and Rajesh M. Hegde. Bridged vari-
ational autoencoders for joint modeling of images and attributes. 2020 IEEE Winter Conference
on Applications of Computer Vision (WACV), pp. 1468-1476, 2020.
A Derivation of the model architecture
We define our model in an axiomatic way, requiring the following properties:
1.	When no modalities are observed, the generating distribution for the latent code is Gaus-
sian:
q(z) = p(z) = N (0, I)	(A.11)
This property is well known from VAEs and allows easy sampling.
2.	The two modalities are independent given the latent code, so the decoder distribution is:
p(x1, x2|z) = p1(x1|z)p2(x2|z)	(A.12)
The second property formalizes our goal that the latent representation contains all relevant
information from all modalities.
The joint distribution p(z|x) = p(z|x1, x2) is given by
p(z|x1, x2)
p(z)pi(xi,X2∣z)
p(x1,x2)
p(z)p1(x1,x2lz)
p(z0)p(x1, x2 |z0)dz0
(A=2) P(z)Pl(xi∣z)P2 (X2∣Z)
p(z0)p(x1 |z0)p(x2 |z0)dz0
(A.13)
3.
Both experts cover the whole latent space with equal probabilities:
q1(z) =	q1(z|x1)p(x1)dx1 =	q2(z|x2)p(x2)dx2 = q2(z)	(A.14)
4.	The joint encoding distribution q(z|x) = q(z|x1, x2) is assumed to be given by the product-
of-experts rule (Hinton, 2002; Welling, 2007):
q(z∣x1,x2) = Z(χ1 X ) qι(z∣x1)q2(z∣x2)	(A.15)
with Z(x1, x2) = q1(z0|x1)q2(z0|x2)dz0. The modelling by a product-of-experts in equa-
tion A.15 is a simplification of equation A.13 to make the model tractable.
11
Under review as a conference paper at ICLR 2021
Given equation A.15 and equation A.14 we obtain
q(z) =	q(z|x)p(x)dx =	q(z|x1, x2)p(x1 , x2)dx1dx2
(A.15)	1
= 石-----------rq1(z∣x1)q2(z∣x2)p(x1)p(x2∣x1)dx1dx2 . (A.16)
Z(x1,x2)
Let us define	q；(ZIXi) = / Z J X ,)qj(ZIXj)p(XjIXi)dXj	(A.17)
and write (A.16) q(z) =	[qi(z∣Xl)ρ(Xl) j	——cq2(z∣X2)p(X2∣Xl)dX2dXl Z(X1, X2) =	P(X1)q1(ZIX1)q2；(ZIX1)dX1 . (A.18)
So the proposal distributions are:
q(ZIX1, X2)	= Z(X⅛) qi(z|Xl)q2(z|X2)	(A.19)
q(ZIX1)	q1(ZIX1)q2；(ZIX1)	(A.20)
q(ZIX2)	q2(ZIX2)q1；(ZIX2)	(A.21)
q(Z)=N(0,I)		(A.22)
B Derivation of the loss function
In the following, we derive the ELBO-type loss we use for training, see equation 7. Let consider the
optimization
EpData(X1 ,X2)[lθg P(X1,X2)] = 2 EpData(X1,X2) [Sg P(Xl M)+lθg P(X2)+lOg p(x2∣Xl)+lθg P(Xl)]
=2 EpData(X1,X2) [log P(XIM)] + 1 EpData(X1,X2) [log P(X2|XI)]
+ 1 EpData(X2)[lOg P(X2)] + 1 EpData(Xl ) [log P(XI)] .	(B.23)
We can now proceed by finding lower-bounds for each term. For the last two terms logP(Xi) we can
use the standard ELBO as given in equation 1. This gives the terms
Li = EpData(Xi) Eq(z|Xi)[log Pi(Xi|z)] - DKL(q(z|Xi) k P(z))	(B.24)
Next, we will derive log P(X1 |X2). This we can do in terms of a conditional VAE (Sohn et al.,
2015), where we condition all terms on X2 (or X1 if we model log P(X2 |X1)). So the model we
derive the log-likelihood for is P(X1 |X2) = P(X1 |z)P(z|X2)dz, whereP(z|X2) is now our prior. By
model assumption we further have P(X1, X2, z) = P(X1 |z)P(X2 |z)P(z) and therefore P(X1 |X2, z) =
P(X1|z). Thus we arrive at the ELBO losses
L12 = EpData(X1,X2) Eq(z|X1,X2)[log P1(X1|z)] - DKL(q(z|X1,X2) k P(z|X2))	(B.25)
and
L21 = EpData(X1,X2) Eq(z|X1,X2)[log P2(X2|z)] - DKL(q(z|X1, X2) k P(z|X1))	.	(B.26)
We now insert the terms in equation B.23 and arrive at:
2EpData(X1,X2) [logP(X1, X2)] ≤ L12 + L21 + L1 + L2
= EpData(X1,X2 ) Eq(z|X1 ,X2 ) [log P1 (X1 |z)] - DKL(q(z |X1 , X2 ) k P(z |X2 ))
+ EpData(X1,X2) Eq(z|X1,X2)[log P2(X2|z)] - DKL(q(z|X1, X2) k P(z|X1))
+EpData(X1) Eq(z|X1)[log P1(X1|z)] - DKL(q(z|X1) k P(z))
+EpData(X2) Eq(z|X2)[log P2(X2|z)] - DKL(q(z|X2) k P(z))
12
Under review as a conference paper at ICLR 2021
The first two terms together give
EpData(x1,x2) Eq(z|x1,x2)[log p1(x1|z) + logp2(x2|z)]
(B.27)
We do not know the conditional prior p(z|xi). By definition of the VAE, we are allowed to optimize
the prior, therefore we can parameterize it and optimize it. However, we know that in an optimal
model p(z|xi) ≈ q(z|xi) and it might be possible to prove that if p(z|xi) is learnt in the same
model-class as q(z|xi) we can find that the optimum is indeed p(z|xi) = q(z|xi). Inserting this
choice into the equation gives the end-result.
C Training procedure for SVAE and VAEVAE*
Algorithm 1: Training procedure for SVAE and VAEVAE*. In bold are terms that are different
from Wu & Goodman (2019)
Input: Supervised example (x1, x2), Unsupervised example x01, Unsupervised example x02
z0 = q(z|x1, x2)
zx1 = q1(z|x1)
zx2 = q2(z|x2)
d1 = DKL(q(z0|x1,x2)kq1(zx1 |x1)) + DKL(q1(zx1 |x1)kp(z))
d2 = DKL(q(z0|x1,x2)kq2(zx2|x2)) + DKL(q2(zx2|x2)kp(z))
L = logp1(x1|z) + logp2(x2|z) + logp1(x1|zx1) + logp2(x2|zx2) + d1 + d2
Lx1 = logp1(x01|zx1) + DKL(q1(zx1 |x01)kp(z))
Lx2 = logp2(x02|zx2) + DKL(q2(zx2 |x02)kp(z))
Lcomb
L0 + Lx1 + Lx
2
D SVAE and VAEVAE for more than two modalities
In the following, We formalize the VAEVAE model for three modalities and present a naive extension
of the SVAE model to more than two modalities.
In the canonical extension of VAEVAE to three modalities, the three- and two-modal relations are
captured by the corresponding networks q(z|x1, x2, x3), q(z|xi, xj) and q(z|xi) for i,j ∈ {1, 2, 3},
see Figure 5. In the general n-modal case, the model has 2n networks. For n = 3, the loss function
reads:
L1,2,3 =Eppaired(x1,x2,x3) Eq(z|x1,x2,x3)[log p1(x1|z) + log p2(x2|z) + logp3(x3|z)]
-	DKL(q(z|x1, x2, x3) k q(z|x1, x2))
-	DKL(q(z|x1, x2, x3) k q(z|x2, x3))
-	DKL(q(z|x1, x2, x3) k q(z|x1, x3))	(D.28)
ij = ppaired (x1 ,x2 ,x3 ) Eq(z|xi,xj)[log pi(xi|z) + logpj(xj|z)]
-	DKL(q(z|xi, xj) k q(z|x1)) - DKL(q(z|xi,xj) k q(z|x2))
-	DKL(q(z|xi, xj) k q(z|x3)) - DKL(q(z|xi,xj) k q(z))	(D.29)
Li =Epunpaired(xi) Eq(z|xi)[log pi(xi|z)] - DKL(q(z|xi) k q(z))	(D.30)
3
Lcomb =L1,2,3 +	X	Li,j + XLi	(D.31)
i,j∈{1,2,3},i6=j	i=1
In this study, we considered a simplifying extension of SVAE to n modalities using n2 networks
qij(z|xj) for i, j ∈ {1, . . . , n}. For the 3-modal case depicted in Figure 5, the PoE relations between
13
Under review as a conference paper at ICLR 2021
the modalities are defined in the following way:
q(z|x1, x2, x3)	二 司-1	Vq1(ZIxI)q2(Zlx2^3(Z|x3) Z(x1, x2, x3)	(D.32)
i,j, k ∈ {1,2,3},i 6= j 6= k :		(D.33)
qi(z|xi,xj)	ii	j	i 二 Z(χ x.)qi(z∖xi)qj(ZIxjIqk(ZIxi)	(D.34)
qj (z|xi, xj)	1i	j	j 二 Z(x. x.)qi(Z|xi)qj(ZIxj)qk(ZIxj)	(D.35)
q(z|xi)	qii(ZIxi)qji(ZIxi)qki (ZIxi)	(D.36)
q(z) =N(0,I)		(D.37)
The corresponding SVAE loss function has additional terms due to the fact that the relations between
pairs of modalities need to be captured with two PoE rules qi(z|xi, xj) and qi(z|xi, xj) in SVAE,
while there is only a single network q(z∣Xi,Xj) in VAEVAE. The loss functions equation D.28-
equation D.31 above are modified in a way that f (q(z|xi, xj)) = f(qi(z|xi, xj)) + f (qj (z|xi, xj))
for any function f .
This extension of the bi-modal case assumes that p(xi, xj |xk) = p(xi|xk)p(xj|xk) for i, j, k ∈
{1, 2, 3}, i 6= j 6= k, which implies that xi, xj and xk are independent of each other.
E Qualitative examples
E.1 SVAE
See Figure E.10
E.2 VAEVAE
See Figure E.11
F Training Details and Encoder/Decoder Architectures
The encoder and decoder architectures for each experiment and modality are listed below. To imple-
ment joint encoding network (VAEVAE architecture), an fully connected layer followed by ReLU
is added to the encoding architecture for each modality. Another fully connected layer accepts the
concatenated features from the two modalities as an input and outputs the latent space parameters.
Adam optimiser is used for learning in all the models (Kingma & Ba, 2015).
F.1 MNIST-Split
The models are trained for 200 epochs with the learning rate 2 ∙ 10-4. The best epoch is chosen by
the highest accuracy of the reconstruction from the top half evaluated on the validation set. We used
a latent space dimensionality of L = 64. The network architectures are described in Table F.7.
Encoder	Decoder
Input ∈ R3χ32x32 4x4 conv. 64 stride 2 pad 1 & ReLU 4x4 conv. 128 stride 2 pad 1 & ReLU 4x4 conv. 256 stride 2 pad 1 & ReLU FC. 786 & ReLU FC. L, FC. L	Input ∈ RL FC. L & ReLU FC. 512 & ReLU FC. 112 & ReLU 4x4 upconv. 56 stride 1 pad 0 & ReLU 4x4 upconv. 28 stride 2 pad 1 & ReLU
Table F.2: Network architectures for MNIST-Split for each image half.
14
Under review as a conference paper at ICLR 2021
Figure E.10: A. MNIST-Split image reconstructions of a top half and a bottom half given (a) the
top half; (b) the bottom half of the original image (c) both halves. B. Side-by-side MNIST-SVHN
reconstruction from randomly sampled latent space, with oracle predictions of a digit class. The joint
coherence from the Figure 7 is a share of classes predicted the same. The examples are generated
by SVAE for the supervision levels 100% and 0.1%
F.2 MNIST-SVHN
The models are trained for 50 epochs with the learning rate 10-3 for VAEVAE and VAEVAE* and
10-4 for SVAE. The best epoch is chosen by the highest joint coherence evaluated on the validation
set. We used a latent space dimensionality of L = 20. The network architectures are summarized in
Table F.4 and Table F.3 for the MNIST and SVHN modality, respectively.
Encoder
Decoder
Input ∈ R1x28x28
FC. 400 & ReLU
FC. L, FC. L
Input ∈ RL
FC. 400 & ReLU
FC. 1 x 28 x 28 Sigmoid
Table F.3: Network architectures for MNIST-SVHN: MNIST.
15
Under review as a conference paper at ICLR 2021
_EHHHHHHU_
Figure E.11: A. MNIST-Split image reconstructions of a top half and a bottom half given (a) the
top half; (b) the bottom half of the original image (c) both halves. B. Side-by-side MNIST-SVHN
reconstruction from randomly sampled latent space, with oracle predictions of a digit class. The joint
coherence from the Figure 7 is a share of classes predicted the same. The examples are generated
by VAEVAE for the supervision levels 100% and 0.1%
F.3 CUB-CAPTIONS
The models are trained for 200 epochs with the learning rate 10-4 . The best epoch is chosen by
the highest joint coherence evaluated on the validation set. We used a latent space dimensionality of
L = 64. The network architectures are described in Table F.5 and Table F.6 for the text and image
modality, respectively.
F.4 MNIST-Split-Three
The models are trained for 50 epochs with the learning rate 2 ∙ 10-4. The best epoch is chosen by
the highest accuracy of the reconstruction from the top half evaluated on the validation set. We used
a latent space dimensionality of L = 64. The network architectures are described in Table F.7.
16
Under review as a conference paper at ICLR 2021
Encoder
Input ∈ R3χ32χ32
4x4 conv. 32 stride 2 pad 1 & ReLU
4x4 conv. 64 stride 2 pad 1 & ReLU
4x4 conv. 128 stride 2 pad 1 & ReLU
4x4 conv. L stride 1 pad 0, 4x4 conv. L stride 1 pad 0
Decoder
Input ∈ RL
4x4 upconv. 128 stride 1 pad0&ReLU
4x4 upconv. 64 stride 2 pad 1 & ReLU
4x4 upconv. 32 stride 2 pad 1 & ReLU
4x4 upconv. 3 stride 2 pad 1 & Sigmoid
Table F.4: Network architectures for MNIST-SVHN: SVHN.
Encoder
Input ∈ R1590
Word Emb. 256
4x4 conv. 32 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 conv. 64 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 conv. 128 stride 2 pad 1 & BatchNorm2d & ReLU
1x4 conv. 256 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
1x4 conv. 512 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
4x4 conv. L stride 1 pad 0, 4x4 conv. L stride 1 pad 0
Decoder
Input ∈ RL
4x4 upconv. 512 stride 1 pad0&ReLU
1x4 upconv. 256 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
1x4 upconv. 128 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
4x4 upconv. 64 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 upconv. 32 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 upconv. 1 stride 2 pad 1 & ReLU
Word Emb.T 1590
Table F.5: Network architectures for CUB-Captions language processing.
Encoder	Decoder
Input ∈ R2048	Input ∈ RL
FC. 1024 ELU	FC. 256 ELU
FC. 512 ELU	FC. 512 ELU
FC. 256 ELU	FC. 1024 ELU
FC. L, FC. L	FC.2048
Table F.6: Network architectures for CUB-Captions image processing.
17
Under review as a conference paper at ICLR 2021
Encoder	Decoder
Input ∈ R3x32x32 4x4 conv. 64 stride 2 pad 1 & ReLU 4x4 conv. 128 stride 2 pad 1 & ReLU 4x4 conv. 256 stride 2 pad 1 & ReLU FC. 786 & ReLU FC. L, FC. L	Input ∈ RL FC. L & ReLU FC. 512 & ReLU FC. 112 & ReLU 4x4 upconv. 56 stride 1 pad 0 & ReLU 2x4 upconv. 28 stride 2 pad 1 & ReLU
Table F.7: Network architectures for MNIST-Split-Three for each image part.
18