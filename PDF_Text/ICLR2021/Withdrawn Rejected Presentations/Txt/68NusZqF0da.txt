Under review as a conference paper at ICLR 2021
Weakly Supervised Formula Learner for
Solving Mathematical Problems
Anonymous authors
Paper under double-blind review
Ab stract
Mathematical reasoning task is a subset of the natural language question answer-
ing task. Several approaches have been proposed in existing work to solve mathe-
matical reasoning problems. Among them, the two-phase solution to first predict
formulas from questions and then calculate answers from formulas has achieved
desirable performance. However, this design results in the reliance on annotated
formulas as the intermediate labels for training. In this work, we put forward
a brand-new idea to enable the models to explore the formulas by themselves
to eliminate the reliance on formula annotations. To realize this, we proposed
Weakly Supervised Formula Leaner, a learning framework that can autonomously
search for the optimal formulas through the training process and continuously
update itself. Our experiment is conducted on a typical mathematical dataset
MathQA. The result shows that our models learning with weak supervision out-
perform the baseline methods. 1
1 Introduction
With the rapid development of deep learning in recent years, super-human level performance has
been achieved on more and more tasks such as image classification, object detection, and machine
translation. However, there also exist some areas where human beings still hold the advantage like
commonsense reasoning, causal reasoning, and mathematical reasoning. A common characteristic
of these areas is that, unlike those naive classification and regression problems that end-to-end solu-
tions can be competent for, these tasks usually call for high-level logical reasoning capacity, which
is also the very reason that they are referred to as “reasoning” tasks.
In this work, the specific problem we focus on is mathematical reasoning. Generally, mathemat-
ical reasoning can be considered as a task where mathematical questions are described in natural
language or mathematical symbols, and the desired answers to these questions can be numbers,
expressions, or other mathematical representations. To solve mathematical reasoning problems, a
naive approach is to treat them simply as classification or regression problems when the desired
answers are single numbers, or sequence-to-sequence translation problems when the answers are
sequences of tokens. However, this approach usually results in the insufficiency of generalization
abilities and may not perform well when the questions come to unseen numbers (Saxton et al., 2019).
Another approach for solving mathematical reasoning problems is to adopt a two-phase methodol-
ogy. In the first phase, specific formulas are predicted for solving each question. Then in the second
phase, such formulas are calculated following pre-defined rules to produce the final answers. Note
that the formulas can also be referred to as equations in some previous work and they are function-
ally the same. For the unity of discussion, we simply call them formulas in the rest of this paper.
This approach is applied more widely in recent work, and has achieved desirable results in many
representative mathematical reasoning datasets (Wang et al., 2017; Amini et al., 2019). However,
this two-phase solution leads to the reliance on the annotated formulas. According to our survey,
this reliance can further result in two main weaknesses. Firstly, Ground-truth formula annotations
are not necessarily ready for every mathematical question and dataset. It is expensive to craft these
annotations manually. Secondly, The learning process can be misled when there is noise in these
formula annotations.
1The code of this work is available in the supplementary material.
1
Under review as a conference paper at ICLR 2021
Question:
Andy has 12 apples, Bob has 20 apples, Bob gives 2 apples to Andy, how many more apples does Bob have than Andy now?
Figure 1: To get rid of the dependence on annotated formulas, our learning framework conducts
a searching process to explore optimal formulas, and then such formulas can be fed back to the
formula predicting network for train.
In consideration of this, we are motivated to propose a new learning framework for solving mathe-
matical problems while not necessarily being dependent on the formula annotations. On the whole,
we followed the principle of the two-phase methodology. In our work, these two phases are imple-
mented by what we call a PolicyNet and a ActTaker. Figure 1 is an overview of our learning
framework. Our contributions to this framework can be summarized in the following aspects:
-	We proposed the idea to write the formulas in a style similar to the programs in Neural Module
Networks (NMNs), which provides more reasonable representations and enables more flexible
modifications on these formulas.
-	We proposed a set of modules as the basic units for solving mathematical problems. These
modules can be regarded as the substitute for the operators and number tokens that make up the
classic formulas.
-	We proposed the method to let the learning framework discover the optimal formulas by itself
through a heuristic search in the space of possible formulas, which is the key to getting rid of
the dependence on formula annotations. We also certified that the formulas found autonomously
through search can be even more beneficial to the training of the models than the annotated ones.
2	Related Work
2.1	Mathematical Reasoning
To study on the capacity of machine learning models in solving mathematical problems and doing
quantitative reasoning, various datasets have been published in recent years. For the convenience
of discussion, we use (q, [f,] a) to represent the elementary unit of data in such datasets. Here,
q, f, and a denotes the question, formula, and answer, respectively. Note that f is not necessarily
provided by every dataset.
Math23K Math23K (Wang et al., 2017) is a dataset with data of shape (q, f, a). This dataset
is crawled from a couple of online education websites and consists of 23,162 problems with full
formula annotations. These problems are mainly about algebra and linear equations. Their difficulty
is around elementary school level. Math23K can be considered as the first large-scale well-formed
public dataset for mathematical problems and has become the benchmark of much later work.
MathQA MathQA (Amini et al., 2019) is another dataset with data of shape (q, f, a). This dataset
is collected from another former dataset named AQuA and consists of 37,200 problems with full
formula annotations as well. Compared to Math23K, the questions in MathQA are asked on a wider
range of areas including probability and geometry. Correspondingly, More complex operations such
as Surface_cylinder and Volume_cube are also included into the formulas besides the basic
arithmetic operations. Besides, MathQA also provides the rationales for how problems are solved.
Mathematics Mathematics (Saxton et al., 2019) is a complicated large-scale dataset with data of
shape (q, a). Compared to other mathematical datasets like Math23K and MathQA, Mathematics is
2
Under review as a conference paper at ICLR 2021
much more challenging for its broad coverage on various areas of mathematics and its free-formed
questions and answers. The questions in Mathematics are generated in plenty of areas including
algebra, arithmetic, calculus, measurement, polynomial, etc. Besides, the answers in Mathematics
contain not only single real numbers but also polynomials or other expressions. This brought great
difficulty to the consistent learning of machine learning models.
2.2	Mathematical Problem S olvers
Generally, the existing approaches for solving mathematical problems can be categorized into two
different methodologies: end-to-end methods and formula-based methods. The essential divergence
between them is either to produce the final answer directly or to make use of the formula as an
intermediate label.
End-to-end methodology The end-to-end method is a concise approach for solving mathematical
problems. It simply regards both the questions and answers as sequences of alphabets, digits, and
symbols and conducts a sequence-to-sequence prediction (Saxton et al., 2019). The main advantage
of this method is that its application would not be restricted by the form of questions and answers
as long as they can be tokenized. However, the lack of the concept of complete numbers forces it
to receive and predict real numbers digit by digit, which leads to a relatively weak generalization
capacity.
Formula-based methodology The formula-based methods are a family of approaches to solve
mathematical problems with the help of formulas. Most elementary mathematical problems can be
solved by writing equations with unknowns and solving the unknowns to acquire the answer. As a
result, the formula-based methods put forward the idea to let the model first predict such equations or
what we called formulas, and then solve such formulas in a rule-based manner. This methodology
has the advantage of recognizing numbers in questions as entities and referring to such numbers
in predicted formulas to establish more steady relevance between questions and formulas. This
characteristic endows formula-based methods with better generalization capacity. These methods
were first implemented by Wang et al. (2017) and then improved by later work. Wang et al. (2018)
proposed the idea to represent the formulas in tree form. Zhang et al. (2020) proposed the idea to
encode the questions with graphs. Xie & Sun (2019) proposed a goal-driven tree decoder to achieve
better decoding performance.
2.3	Neural Module Networks
Neural Module Networks (NMN) is a model proposed by Andreas et al. (2016a) to solve the visual
question answering task. The general architecture of NMN can be described as a controller and a set
of modules. Given specific questions, the controller predicts the corresponding programs indicating
the required modules and their layout. Then, the modules are concatenated to calculate on given
images to obtain the final answers. In the sense of the general architecture, the existing formula-
based two-phase models for solving mathematical problems share a similar principle with NMN.
Both the formulas in these models and the programs in NMN can be considered as the intermediate
labels to guide the concrete actions for solving specific problems. On the other hand, this similar
architecture has also brought similar difficulty to these two models’ training. Considering that the
calculation from the formulas or the programs to the final answers is a non-differentiable process, the
loss defined on the final answer cannot be back-propagated to the formula or program predictor. To
overcome this difficulty, existing work on NMN applied reinforcement learning to train the program
predictor (Andreas et al., 2016b). Recent work has also proposed methods enabling more efficient
search in the space of possible programs (Wu & Nakayama, 2020).
3	Weakly Supervised Formula Learner
3.1	Preprocessing and symbols
To start with, we first present the necessary preprocessing and what every symbol refers to in our
learning framework. As shown in Figure 2, given the raw text of a question, the numbers appearing
in the text are extracted as num. Meanwhile, these numbers in the original text are replaced with
special token hNxi. We call the questions with replaced tokens “template” and they can also be
3
Under review as a conference paper at ICLR 2021
Question : Andy has 12 apples, Bob has 20 apples, Bob gives 2 apples to Andy, how many
more apples does Bob have than Andy now?
Equation : 20-12-2-2
q：
num :
Andy has <N0> apples, Bob has <N1>
apples, Bob gives <N2> apples to
Andy, how many more apples does
Bob have than Andy now?
(12, 20, 2)
4
ActTaker
modules
N_0
N_3
N_1
N_4
N_2
N_5
(-
N_0,〈End〉，N_1,〈End〉/—
N_2,〈End〉，N_2,〈End〉）
20 Z ∖	12
JILo N_1
c_0
C_1
C_100
a :
+
×

一∙一
- -
9	9	9
Figure 2: The preprocessing and the symbols applied in our work.
simply denoted by q. Note that multiple questions can share the same template if they only differ
on numbers. In this case, they are combined to a single template q. We let {num} denote the set
of extracted numbers num that comes from the questions corresponding to the same template, and
let {a} denote the set of answers a to these questions. Note that the num in {num} and a in {a}
should be kept paired. After preprocessing, the data visible to the following procedures should be
tuples of (q, {num}, {a}).
Besides, we let f denote the formula we apply. The basic form of the formula is a list of tokens.
Each of the tokens is the name of a module or a hEndi sign. The main purpose of this formula is to
guide ActTaker to solve specific question by providing the preorder traversal of desired modules.
As a result, with such formula, ActTaker can construct the tree with specified modules and conduct
a recursive calculation to obtain the answer. The underlying idea of writing formulas in this form
is to enable flexible modifications. Modifications can be easily conducted in a tree by insertion,
deletion, and substitution on specific nodes while not destroying the tree structures of other nodes.
This brings convenience to the mutation step in the following search.
3.2	General Architecture
Generally, our proposed learning framework consists of two main parts named PolicyNet and
ActTaker. As shown by Equation 1 and 2, PolicyNet takes the question q as input and predicts
the formula f . ActTaker takes the predicted formula f and the set of numbers {num} as inputs
and calculates a set of answers {^} corresponding to each num.
f = PolicyNet(q)
{^} = ActTaker(f, {num})
(1)
(2)
With these two models, Algorithm 1 shows the general training workflow of our Weakly Supervised
Formula Learner. Here, D denotes the original dataset consisting of tuples of training data as (q,
{num}, {a}). L denotes a dictionary initialized to be empty to store the optimal formulas found
through the learning process. After PolicyNet, ActTaker, and L get initialized, the learning
process consists of numerous basic loops. Within each loop, firstly, a tuple of training data is sam-
pled from the dataset D. Then, a search is conducted to try finding the optimal formula for solving
Algorithm 1 General Training Workflow
1:	PolicyNet, ActTaker — Initialize。
2:	L <— {}
3:	for loop in range(maXjoop) do
4:	(q, {num}, {a}) — Sample(D)
5:	f, accuf — Search(PolicyNet, ActTaker, q, {num}, {a})
6:	L.update(q: (f, accuf))
7:	PolicyNet.train(L)
8:	end for
4
Under review as a conference paper at ICLR 2021
the specific question. The detailed behavior of this Search function is presented in Section 3.4.
After the formula f together with its accuracy accuf on answering the question are obtained, the
dictionary L is updated with the newly found formulas. Concretely, if f is not None, and then if no
f has been recorded for q or accuf exceeded the previously recorded accuracy, f and accuf will be
recorded for q in L. At last, PolicyNet is trained with the q and f sampled from L. These steps
will be repeated until the max Joop is reached. By the time of inference, this workflow is no more
necessary and the answer can be acquired directly through Equation 1 and 2.
3.3	Modules
Modules are the basic calculating unit of ActTaker and the basic units to solve the mathematical
problems. As illustrated by Figure 2, ActTaker regards the formula as a tree and the nodes calcu-
late with the specified module recursively from leaves to roots to acquire the final answer. Here, the
three types of modules we adopted are Number, Operation, and Constant.
Number The Number modules, which can be denoted by N_x, are the modules to establish
references to the numbers extracted from questions. These modules need no input and return a
number. Here, x is the index of the number that is referred to. This index starts from 0. For
example, N_1 returns the second number in num. An error is raised if X exceeds the number of
numbers in num.
Operation The Operation modules, which can be denoted simply by their symbols, are the
modules to conduct specific mathematical calculations. These modules need a specific number
(commonly two) of numeric inputs and return the calculation result as a number. For example, a +
module takes two number a and b as inputs, and returns the real number (a+b). An error is raised
if the calculation is illegal such as division by zero.
Constant The Constant modules, which can be denoted by C_x, are the modules to generate
constant numbers. These modules need no input and return a number. Here, x is the specific real
number that is referred to. For example, C_10 0 returns the real number 100.
3.4	Search
To obtain the optimal formulas without prior knowledge, a searching process is necessary. As intro-
duced in Section 3.2, we conduct a Search procedure in each loop of learning. For this function,
we basically followed the Graph-based Heuristic Search algorithm proposed by Wu & Nakayama
(2020) and made some adjustments on it to make it capable of solving mathematical problems. The
basic workflow of this algorithm is presented as Algorithm 2.
Here, G denotes a graph to store the formulas under exploration. Each of its node represents a
unique formula. In every basic searching step, a formula to explore fexp is selected from G . Then,
ActTaker should try the formula to get its accuracy given {num} and {a}. Note that this calcula-
tion may not always succeed because the formulas being explored are not necessarily semantically
legal and errors may be raised in some cases as illustrated in Section 3.3. An answer a is consid-
ered correct only if no error is raised through calculation and the difference between ^ and a is
less than an acceptable floating-point error bound. After the accuracy is acquired, mutations are
generated from fexp to expand the graph G . When all the searching steps are finished, the formula
that achieved the highest accuracy together with its accuracy are returned as the formula searching
Algorithm 2 Formula Search
1:	function Search(PolicyNet, ActTaker, q, {num}, {a})
2:	G — Initialize。
3:	for step in range(max_step) do
4:	fexp — Sample(G)
5:	fexp.accu — Accuracy(ActTaker(fexp, {num}), {a})
6:	G.update(Mutate(fexp))
7:	end for
8:	fbest — arg maxf ∈G f.accu
9:	return fbest, fbest.accu
5
Under review as a conference paper at ICLR 2021
result. If none of the formula achieved a non-zero accuracy, this function returns None. See Wu &
Nakayama (2020) for more details about this searching algorithm.
4	Experiments and analyses
4.1	Experiment Settings
To prepare the training data, we first followed the preprocessing procedure presented in Section 3.1
to transform the original questions and answers into the form of (q, {num}, {a}).
For PolicyNet, we applied an encoder-decoder sequence-to-sequence model with LSTM as the
backbone (Hochreiter & Schmidhuber, 1997). Concretely, the encoder is a 2-layer Bidirectional
LSTM with hidden state size 256. The decoder is a 2-layer LSTM with hidden state size 512. Both
the encoder and the decoder have an input embedding size of 300. For its training, we adopted Adam
optimizer (Kingma & Ba, 2014) with learning rate 0.001. Note that as line 7 of Algorithm 1 indi-
cated, PolicyNet is trained continuously in every learning loop. Here, the batch size to sample
training data from L is 64. Within each loop, PolicyNet is trained by 500 batches. We utilized
part of the formula annotations to pre-train this PolicyNet. The pre-training data, which is or-
ganized as tuples of (q, f), is filtered from the training set to meet the following two requirements.
First, the formula should be able to solve the question and acquire a non-zero accuracy. Second,
the operations involved in the formula should only contain the four fundamental arithmetic opera-
tions. This means that the formulas with composite operations like Volume_cube are discarded.
Additionally, we provide a training option to load the pre-trained word vectors from Pennington
et al. (2014). This is totally optional and its result is referred to as “Ours + PW”. Considering
that the scale of MathQA is relatively small compared to those common natural language process-
ing datasets, some of the words appeared in the questions only one or a few times. We made this
attempt to measure and mitigate the influence brought by the insufficiency of word vector learning.
For ActTaker, the configurable setting is the composition of the modules. Here, we adopted
twenty Number modules N_0 to N_19, four Operation modules +, -, ×, ÷, and four Constant
modules C_1, C_2, C_3, and C_100.
For evaluation, MathQA provided five options to each question and the correct option is annotated.
To select an option, we calculate the difference between the answer acquired by our models and
each option, and select the option on which the difference is minimal. If an error is raised through
the calculation of ActTaker, we randomly select one of the options. The final accuracy we report
is the accuracy of option selection.
4.2	Results and analyses
Table 1 shows the accuracy achieved by our learning framework and the baselines. It is shown that
our proposed methods outperforms all the baselines on MathQA. Besides, as shown by Ours + PW,
improvements can further be made by using pre-trained word vectors. For this result, we attribute
our success to the autonomous formula finding capacity of our learning framework. With the inves-
tigation on the annotated formulas provided by MathQA, some noise was found. This means that
part of the annotated formulas cannot solve the corresponding questions correctly. Including this
noise into the training labels results in the degradation of performance in existing work. However,
our learning framework is capable of getting rid of this noise and find valid formulas for the cor-
Table 1: The option selecting accuracy achieved by our learning framework and baselines.
MathQA
Seq2prog (Amini et al., 2019)	51.9%
Seq2prog+cat (Amini et al., 2019)	54.2%
LSTM2TP (Chen et al., 2020)	54.61%
TP-N2F (Chen et al., 2020)	55.95%
Ours	59.5%
Ours+PW	59.6%
6
Under review as a conference paper at ICLR 2021
responding questions afresh through search with the weak supervision from answers. This enables
our learning framework to achieve higher accuracy in such datasets with noise in annotations. Table
2 shows some examples of the annotated formulas provided by MathQA together with the formulas
found by our learning framework. In these examples, the annotated formulas are invalid and become
noise for training. However, our learning framework finds the valid formula for the question instead.
Table 2: Examples of the annotated formulas and the formulas found by our learning framework
Example a
question index	14328	
question text	12.5 % of 192 = 50 %	of ?
annotated formula	((((50 × 192) ÷ 100)	× 12.5) ÷ 100)
answer	48	
our formula (raw)	(÷, ×, N,0, 〈End)，N_1	,(End), N_2,(End))
(flatten)	(12.5 × 192) ÷ 50	
Example b		
question index	1752	
question text	how many multiples of 160 ?	4 are there between 8 and
annotated formula	((160 - 8) ÷ 4) + 1	
answer	37	
our formula (raw)	(-,÷, -, N-2,(End), hEndi)	N_1, (End), N_0, (End), C_1,
(flatten)	((160 - 8) ÷ 4) - 1	
5	Further discussion
As presented by the sections above, our proposed learning framework can learn to solve mathemati-
cal problems while not being dependent on the formula annotations. However, through experiments
some of its weaknesses are also exposed. The main issue we are concerned about is the credibility
of the formula effectiveness judgment. Through search, we judge whether a newly found formula
is valid or not by the question answering accuracy. This metric is credible enough when there are
multiple num-a pairs for each question template. However, the confidence level drops when there is
only one pair of them for an invalid formula may also lead to a correct answer on particular numbers
accidentally. For example, for the question “Find the sum of 2 and 2.”, the true formula should be
N_0+N_1, but the formulas like N_0+N_0 or N_0 xn_1 also result in correct answerjUstby accident.
This mistake can be noticed while given num other than (2, 2), but it cannot be detected when there
is only one pair of num and a. For this reason, we also observe some wrongly found formulas in our
learning results. But fortunately, the probability of this happening is quite low when the numbers are
large enough, and this did not influence the performance of our learning framework significantly.
Besides, another potential weakness of our learning framework is that its performance is closely re-
lated to the performance of the searching algorithm. If the space of possible formulas is so huge that
a reasonable formula cannot be found through heuristic search in confined steps, the performance
of our models may drop because of the lack of training data. This is also the reason that we have
to utilize part of the annotated formulas in MathQA for its huge formula space. We leave further
improvements in the searching algorithm to future work.
6	Conclusion
This work addressed the issue of formula annotation dependence in existing work for solving mathe-
matical problems. To deal with this issue, this work put forward a brand-new idea to enable the mod-
els to explore the optimal formulas by themselves. Concretely, a new learning framework Weakly
Supervised Formula Learner is proposed as the main contribution of this work. To get rid of the de-
7
Under review as a conference paper at ICLR 2021
pendence on annotated formulas, the underlying principle of this learning framework is to conduct
a series of heuristic search in the space of possible formulas. Besides, we also proposed the idea to
write the formulas as the sequence of tokens indicating the desired modules, and a set of modules
as the basic units for solving mathematical problems. With all these proposals, our learning frame-
work is capable to learn the formulas for solving specific questions autonomously without formula
annotations, and to apply specified modules to obtain the final answer.
To verify the effectiveness of our proposed learning framework, our experiments are conducted
on a typical mathematical problems dataset MathQA. In the experiments, our learning framework
shows the capacity to discover the optimal formulas with weak supervision from answers. It is
also demonstrated that the formulas found by search can be even more beneficial for the training of
models than the formulas annotated manually. In view of these evidences, we consider our proposed
learning framework a valid and advanced approach for solving mathematical problems.
References
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. Mathqa: Towards interpretable math word problem solving with operation-based for-
malisms. arXiv preprint arXiv:1905.13319, 2019.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Pro-
Ceedings of the IEEE conference on computer vision and pattern recognition, pp. 39-48, 2016a.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural
networks for question answering. arXiv preprint arXiv:1601.01705, 2016b.
Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D Forbus, and Jianfeng
Gao. Mapping natural-language problems to formal-language solutions using structured neural
representations. In Proc. of ICML, volume 2020, 2020.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical rea-
soning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.
Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, and Xiaojiang Liu. Translating a math word
problem to an expression tree. arXiv preprint arXiv:1811.05632, 2018.
Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.
845-854, 2017.
Yuxuan Wu and Hideki Nakayama. Graph-based heuristic search for module selection procedure in
neural module network. arXiv preprint arXiv:2009.14759, 2020.
Zhipeng Xie and Shichao Sun. A goal-driven tree-structured neural model for math word problems.
In IJCAI, pp. 5299-5305, 2019.
Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, and Ee-Peng Lim. Graph-
to-tree learning for solving math word problems. Association for Computational Linguistics,
2020.
8