Modeling from Features: a Mean-field Framework for
Over-parameterized Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes a new mean-field framework for over-parameterized deep neural networks
(DNNs), which can be used to analyze neural network training. In this framework, a DNN is rep-
resented by probability measures and functions over its features (that is, the function values of the
hidden units over the training data) in the continuous limit, instead of the neural network parame-
ters as most existing studies have done. This new representation overcomes the degenerate situation
where all the hidden units essentially have only one meaningful hidden unit in each middle layer,
leading to a simpler representation of DNNs. Moreover, we construct a non-linear dynamics called
neural feature flow, which captures the evolution of an over-parameterized DNN trained by Gradient
Descent. We illustrate the framework via the Residual Network (Res-Net) architecture. It is shown
that when the neural feature flow process converges, it reaches a global minimal solution under
suitable conditions. Our analysis leads to the first global convergence proof for over-parameterized
neural network training with more than 3 layers in the mean-field regime.
1	Introduction
In recent years, deep neural networks (DNNs) have achieved great success empirically. However, the theoretical
understanding of the practical success is still limited. One main conceptual difficulty is the non-convexity of DNN
models. More recently, there has been remarkable progress in understanding the over-parameterized neural networks
(NNs), which are NNs with massive hidden units. The over-parameterization is capable of circumventing the hurdles
in analyzing non-convex functions under specific settings:
(i)	Under a specific scaling and initialization, it is sufficient to study the NN weights in a small region around the
initial values given sufficiently many hidden units - the aptly named “lazy training” regime (Jacot et al., 2018;
Li & Liang, 2018; Du et al., 2019a; Arora et al., 2019; Du et al., 2019b; Allen-Zhu et al., 2018; Allen-Zhu &
Li, 2019; Zou et al., 2018; Chizat et al., 2019). The NN in this regime is nearly a linear model fitted with a
random kernel in the tangent space, and provably achieves minimum training error. However, this regime does
not explain why NNs can effectively learn representative features, and the expressive power of random kernels
is limited Yehudai & Shamir (2019).
(ii)	Another line of research applies the mean-field analysis for NNs (Mei et al., 2018; Chizat & Bach, 2018; Sirig-
nano & Spiliopoulos, 2019a; Rotskoff & Vanden-Eijnden, 2018; Mei et al., 2019; Dou & Liang, 2019; Wei et al.,
2018; Sirignano & Spiliopoulos, 2019b; Fang etal., 2019; Araujo etal., 2019; Nguyen & Pham, 2020; Chen etal.,
2020). Learning a two-level over-parameterized NN can be approximately described as optimizing a functional
over probability distributions of the NN weights. The evolution of NN weights trained by the (noisy) Gradient
Descent algorithm corresponds to a Wasserstein gradient flow called “distributional dynamics”, solution to a
non-linear partial differential equation of McKean-Vlasov type (Sznitman, 1991). In the mean-field limit, the
Wasserstein gradient flow converges to the globally optimal solution for two-level NNs (Mei et al., 2018; Chizat
& Bach, 2018; Fang et al., 2019). Compared with lazy training, the mean-field view can characterize the entire
training process of NNs.
However, the mean-field analysis on DNNs is a challenging task. First of all, it is not easy to formulate the mean-field
limit of DNNs. As we will discuss in Section 2, extending existing formulations to DNNs, hidden units in a middle
layer essentially behave as a single unit along the training. This degenerate situation arguably cannot fully characterize
the training process of actual DNNs. Furthermore, understandings for the global convergence of DNNs are still limited
1
in the mean-field regime. Beyond two layers, the only result to the best of our knowledge came from (Nguyen & Pham,
2020) recently, in which they proved the global convergence for three-level DNNs under restrictive conditions. It is
not clear how to extend their analysis to deeper NNs.
In this paper, we propose a new mean-field framework for over-parameterized DNNs to analyze NN training. In
contrast to existing studies focusing on the NN weights, this framework represents a DNN in the continuous limit
by probability measures and functions over its features, that is, the outputs of the hidden units over the training data.
This new representation overcomes the degenerate situation in previous studies (Araujo et al., 2019; Nguyen & Pham,
2020).We also describe a non-linear dynamic called neural feature flow that captures the evolution of a DNN trained
by Gradient Descent.
We illustrate the framework by Res-Nets (He et al., 2016). Neural feature flow involves the evolution of the features and
does not require the boundedness of the weights. Under the standard initialization method of discrete Res-Nets (Glorot
& Bengio, 2010; He et al., 2015), the NN weights scale to infinity with the growth of the number of hidden units. There
are empirical studies, e.g. Zhang et al. (2019), which show that properly rescaling the standard initialization stabilizes
training. We introduce a simple '2-regression at initialization (see Algorithm 2). We prove that Gradient Descent from
the regularized initialization with a suitable time scale on Res-Nets can be well-approximated by its limit, i.e., neural
feature flow, when the number of hidden units is sufficiently large.
Finally, we consider the global convergence of neural feature flow for Res-Nets. More or less surprisingly, we show
that when the neural feature flow process converges, it reaches a globally optimal solution under suitable conditions. To
the best of our knowledge, our analysis leads to the first proof for the global convergence of training over-parameterized
DNNs with more than 3 layers in the mean-field regime. We conclude the contributions of the paper below:
(A)	We propose a new mean-field framework of DNNs which characterizes DNNs via probability measures and
functions over the features and introduce neural feature flow to capture the evolution of DNNs trained by the
Gradient Descent algorithm.
(B)	We illustrate our framework by Res-Net model. We show that neural feature flow can find a global minimal
solution of the learning task under certain conditions.
We also note that this paper presents the motivations and the primary outcomes of our new mean-field framework. We
provide the key proofs in the appendix. In the supplementary material, we also attach the extended version of this
paper, which similarly analyzes the fully-connected DNNs as well. We also consider general initialization conditions
for both DNNs and Res-Nets and provides all proofs and more discussions.
1.1	Notations
Let [m1 : m2] := {m1 , m1 + 1, . . . , m2} for m1, m2 ∈ N with m1 ≤ m2 and [m2] := [1 : m2] for m2 ≥ 1. Let
Pn be the set of probability distributions over Rn. For a matrix A ∈ Rn×m, let kAk2, kAkF, and kAk∞ denote
its operator, Frobenius, max norms, respectively. If A is symmetric, let λmin (A) be its smallest eigenvalue. Vectors
are treated as columns. For a vector a ∈ Rn, let ka∣∣2 and ∣∣a∣∣∞ denote its '2 and '∞ norms, respectively. The i-th
coordinate is denoted by a(i). For a, b ∈ Rn, denote the entrywise product by a ∙ b that [a ∙ b](i) := a(i) ∙ b(i) for
i ∈ [n]. For an unary function f : R → R, define f : Rn → Rn as the entrywise operation that f (a)(i) = f (a(i))
for i ∈ [n] and a ∈ Rn . Denote n-dimensional identity matrix by In . Denote m-by-n zero matrix and n-dimensional
zero vector by 0n×m and 0n, respectively. For two positive sequences {pn} and {qn}, pn = O(qn) ifpn ≤ Cqn for
some positive constant C, andPn = Ω(qn,) if qn = O(Pn). Moreover, Pn = O(qn if Pn = O(qn logk(qn)) for some
k > 0, andPn = Ω(qn if qn =O(Pn).
2	Challenges on Mean-field Theory for DNNs
We discuss related mean-field studies and point out the challenges in modeling DNNs. For two-level NNs, most of
the existing works Mei et al. (2018); Chizat & Bach (2018); Sirignano & Spiliopoulos (2019a); Rotskoff & Vanden-
Eijnden (2018) formulate the continuous limit as
f(x;P) = / W2 h (w>x) dP(W2, Wi),
2
where p is the probability distribution over the pair of weights (w2, w1). The weights of the second layer w2 can be
viewed as functions of w1 , which is a d-dimensional vector. However, this approach indexes higher-layer weights, say
w3 , by functions over features of the hidden layer, with a diverging dimensionality in the mean-field limit. For 3-level
NNs, w3 as the last hidden layer is indexed by the connection to the output units in Nguyen & Pham (2020), which is
not generalizable when middle layers present. An alternative approach is to model DNNs with nested measures (also
known as multi-level measures; see Dawson et al. (1982); Dawson (2018) and references therein), which however
suffers the closure problem to establish a well-defined limit (see discussions in (Sirignano & Spiliopoulos, 2019b,
Section 4.3)).
The continuous limit of DNNs is investigated by (Araujo et al., 2019; Nguyen & Pham, 2020) under the initialization
that all weights are i.i.d. realizations of a fixed distribution independent of the number of hidden units. However, under
that setting, all neurons in a middle layer essentially behave as a single neuron. Consider the output β of a middle-layer
neuron connecting to m hidden neurons in the previous layer:
1m
β = mm X h(β0) Wi,	(2.1)
i=1
where βi0 is the output of i-th hidden neuron in the previous layer with bounded variance, wi is the connecting weight,
and h is the activation function. If wi is initialized independently from N(0, 1), it is clear that var[β] → 0 as
m → ∞, and thus the hidden neurons in middle layers are indistinguishable at the initialization. Moreover, the
phenomenon sustains along the entire training process, as shown in Proposition 1. This phenomenon serves as the
basis of AraUjo et al. (2019); Nguyen & Pham (2020) to characterize the mean-field limit using finite-dimensional
probability distributions. This degenerate situation arguably does not fully characterize the actual DNN training. In
fact, similar calculations to equation 2.1 are carried out by Glorot & Bengio (2010); He et al. (2015) and motivate the
popular initialization strategy with N(0, O(m)) such that the variance ofβ is non-vanishing.
Proposition 1. Consider fully-connected L-layer DNNs with m units in each hidden layer trained by Gradient De-
scent. Suppose the activation and loss functions satisfy Assumption 1. Let βk,i denote the output of i-th hidden neuron
at' -th layer and k-th iteration, and define ∆',m := maxi=io,k∈[κ] kβki 一 βki∕∣∣∞. Then, for every ' ∈ [2 : L 一 1],
lim δ'⑪ = 0.
m→∞
3	Formulation of Continuous Res-Nets
We consider the empirical minimization problem over N training samples {xi, yi}iN=1, where xi ∈ Rd and yi ∈ Y.
For regression problems, Y is typically R; for classification problems, Y is often [K] for an integer K. We first present
the formulation of L-layer Res-Nets.
3.1	Discrete Res -Nets
For discrete Res-Nets, let m` denote the number of units at layer ` for ` = [0 : L + 1]. Suppose each hidden layer has
m hidden units that m` = m for ` ∈ [L]. Let m0 = d and node i outputs the value of i-th coordinate of the training
data for i ∈ [d]. Let mL+1 = 1 that is the unit of the final network output. For ` ∈ [L + 1], the output of node i for the
N training samples in layer ' is denoted by β',i ∈ RN; the weight that connects the node i at layer ' 一 1 to node j at
layer ' is denoted by v`,i,j ∈ R.
(1)	At the input layer, for i ∈ [d], let
(3.1)
(2)	At the first layer, for j ∈ [m], let
1	m0
β1,j = ^∑~ Ev1,i,j βo,i.
m0	i=1
(3.2)
3
(3)	We recursively define the upper layers for ' ∈ [2 : L]. Let α `j ∈ RN be the residual term at node j at layer ':
1m
α`j = m Xv`,i,j hι (β'-ιj, j ∈ m],	(3.3)
i=1
where hi : R → R is the activation function and hι : RN → RN is the entrywise operation for hi, which satisfies
hi(a)(i) = hi(a(i)) for i ∈ [N] and a ∈ RN. Furthermore, We consider the following coupling between the
residual and the previous feature:
β',j = h2 (α',j)+ β'-i,j, j ∈ [m].	(3.4)
where h2 : R → R.
(4)	At the output layer,
1m
fβL+1,1 = — ^X vL+1,i,1h 1 (∕‰,i) .	35)
m i=i
We collect weights, residuals, and features from all layers into single vectors V ∈ RD1, α ∈ RD2, and β ∈ RD2,
respectively, where Di := m2(L - 1) + (d + 1)m and D2 := NmL. The learning problem for Res-Nets is given by
1N
min LL(v, α, β) = — X φ (βL+ι,ι(n), yn) ,	(3.6)
v,α,β	N n=i `	)
where (v, α, β) satisfies equation 3.2 - equation 3.5, and φ : R ×Y → R denotes the loss function. One noteworthy
feature in the architecture is equation 3.4, where we introduce a mapping h? on the residual α`j before fusing it with
β'-1,j. We assume that h? is bounded by a constant Li, and hence ∣∣β',j - β'-1,j k ∞ ≤ Li. Therefore, the high-level
features can be regarded as perturbations of the low-level ones. Similar ideas have also appeared in Du et al. (2019a);
Hardt & Ma (2016), but realized in a different way. For example, in the lazy training regime, Du et al. (2019a) achieved
it by scaling α `j with a vanishing O( √m) factor.
3.2	Continuous Res -Nets Formulation
We propose our formulation for the continuous Res-Nets. The key structure of Res-Nets is the skip connections in
equation 3.4. However, in the continuous case, the discrete index j no longer makes sense and the skip connections
need to be properly parametrized by an infinite set. To overcome the hurdle of infinite skip connections, we introduce
Θ = (vi, α?, . . . , αL) ∈ RD for D = d + (N - 1)L to parametrize the skip connections that are described in
equation 3.8 and equation 3.9 below. Each Θ consists ofvi, α?, . . . , αL that can be regarded as an input-output path
vi → ɑ2 → ∙∙∙ → αL. and is called a skip-connected path. Our main technique is to characterize the overall state of
the continuous Res-Nets by the density p over skip-connected paths. Thus the joint distribution p can be regarded as a
description of the overall topological structure about the skip connections. We represent the features β' in the hidden
layer ` ∈ [L] as functions of Θ that we introduce next:
(1)	At the input layer, let X = xi, x?, . . . , xN> ∈ RN×d.
(2)	At the first layer, let the features be
βι (Θ) = 1 (Xvi).	(3.7)
d
(3)	At layer ` ∈ [2 : L], let v` : supp(p) × supp(p) → R denote the weights on the connections from layer ` - 1 to
`, then for all Θ = (vi, αi, α?, . . . , αL) ∈ RD, we have the forward-propagation constraint for v` and p:
ɑ` = / v` (Θ, Θ) hi (β'-i(Θ)) dp (Θ) ,	(3.8)
β' (Θ) = h2 (ɑ`) + β'-i (Θ).	(3.9)
Here, Θ takes on values in RD and for each Θ, ɑ` is one part of Θ and β' is a function of Θ. ɑ` represents the
residual at layer ' on the skip connected path described by Θ. And β'(Θ) represents the corresponding feature.
4
Algorithm 1 Scaled Gradient Descent for Training a Res-Net.
1 2 3 4 5	Input the data {xi, yi}NLι, step size η, and initial weights V0. : for k = 0, 1, . . . , K - 1 do Perform forward-propagation equation 3.2- equation 3.5 to compute βL十1「 Perform backward-propagation to compute the gradient Gkij = ∂VL . : Perform Scaled Gradient Descent: vk,+1	=	vk,i,j -[nm`-im`]	Gk,i,j,	' ∈	[L +	1],	i ∈	[m`-i],	j ∈	[m`].
6:	end for
7:	Output the weights VK.
(4) At the output layer, let vL+1 : supp(p) → R be the weights in the layer L + 1, and we have
βL+1 = / VL+1 (Θ) h 1 (βL (Θ)) dp (Θ).
(3.10)
In our continuous formulation, a static Res-Net is characterized by p and v2 . . . vL+1 . We will show in the next
section that the continuous Res-Net ({v'}L+1, P) that satisfies the equation 3.7 - equation 3.10 will serve as a feasible
initialization for neural feature flow.
4 S caled Gradient Descent and Neural Feature Flow for Res-Nets
We focus on the dynamic of the Res-Net trained by Gradient Descent. We consider the scaled Gradient Descent
algorithm1. Given initial weights V0, the meta-aIgorithm of scaled Gradient Descent is presented in Algorithm 1. Note
that Algorithm 1 differs from the standard Gradient Descent only on the step sizes (time scales). Such scaling is also
adopted in existing works (Araujo et al., 2019; Nguyen & Pham, 2020).
Now we describe the continuous limit of the Res-Net trained by Algorithm 1 by the continuous trajectories of the Res-Nets. A
trajectory is denoted by Φ that maps the initial Res-Nets at t = 0 to a Res-Net process over [0, T]. Specifically, it consists of the
following parts:
•	Φβ	: SuPP(P)	→ C([0,T], RN) is the trajectory of β' for ' ∈ [L];
•	Φα	: SuPP(P)	→ C([0,T], RN) is the trajectory of a` for ' ∈ [2 : L];
•	Φ1v	: suPP(p)	→ C ([0, T], Rd) and ΦvL+1 : suPP(p) → C([0, T], R) are	the trajectories of v1 and vL+1, respectively;
•	ΦV	: supp(p)	× supp(p) → C([0,T], R) is the trajectory of v` for ' ∈	[2 :	L].
The continuous gradient for the weight can be obtained from the backward-propagation algorithm. For a given trajectory Φ, the
gradients of weights at time t ∈ [0, T] can be obtained from the backward-propagation algorithm. Similar to the usual backward-
propagation, we first define gradients with respect to the features and residuals. Specifically, for all Θ = (v1 , α2, . . . , αL) ∈
SuPP(P), t ∈ [0, T], and ` ∈ [2 : L], let
βL+1 (Φ,t) := / ΦL+1 (Θ)(t) h 1 (φβ (Θ)(t)) dp (Θ),
DL+ι(Φ,t) := {Φ1 (βL+ι (Φ,t) (n),yn) : n ∈ [N]},
Dβ(θ;Φ,t) := [ΦL+ι (Θ)(t) Dz+ι(Φ,t)] ∙ h 1 (Φβ(θ)(t)),
Dα(θ; Φ,t) := De (Θ; Φ,t) ∙ h 2 (Φα(θ)(t)),
(4.1)
Dβτ (θιΦ,t) := De(θ=Φ,t)+
ΦΦV (Θ, Θ) (t) Dα(Θ; φ, t)dp (Θ) ∙ h 1 (Φβ-1 (Θ)(t)).
1In practice, one often use stochastic gradient instead of the full counterpart for training. Under mild conditions, the dynamic
of scaled Stochastic Gradient Descent will also converge to the neural feature flow in the continuous limit.
5
For all Θ, Θ ∈ SuPP(P),the drift term for the weights is given by
GL+ι (θ=Φ,t) ：=NN [Dz+ι(φ,t)]> hι (Φβ(θ)(t)),
GV(Θ, 6◎t) :=NN [DΓ(6® t)]> hι (Φβ-ι (Θ)(t)) ,	` ∈ [2 : L],
GV(θ^,t) ：=NNX De (6®t).
Moreover, the changes of the weights will induce a change of the residuals and features. By the chain rule, we can obtain the drift
term for the residuals and features: for ` ∈ [L - 1] and Θ ∈ suPP(p),
Ge (Θ^,t) ：= d [xGV(6；©,t)],
Ga+ι (θ；φ,t)：=
ΦΦV+1 (Θ, θ) (t) h⅛ 1 (φβ(Θ)(t)) ∙Gβ @®t)i dp(Θ) +
+ /h 1 (φβ (Θ) (t)) ∙ Gv+1 (Θ, Θ; φ, t) dp (Θ),
Ge+1 (Θ; φ, t) ：=Ge (Θ; φ, t)+Ga+1 (Θ; φ, t) h 2 (Φα+ι (Θ)(t)).
In all, the process of a continuous Res-Net trained by scaled Gradient Descent can be defined below.
Definition 1 (Neural Feature Flow for Res-Net). Given an initial continuous Res-Net ({v'}L=+2 ,p) that satisfies the equa-
tion 3.7 - equation 3.10 and T < ∞, we say a trajectory Φ* is a neural feature flow if for all Θ = (vι, α2,..., α" ∈
SuPP(P), Θ ∈ SuPP(P), and t ∈ [0, T],
Φβ,' (Θ)(t)
φ?,' (Θ)(t)
ΦV,ι (Θ)(t)
ΦV,' (Θ, Θ)(t) =
ΦV,L+1 (Θ)(t)
dXv1 + XX h2 (ai)
t
Ge (6；©*,s),	` ∈ [l],
_	i=2
ɑ` - [t Ga (Θ[Φ*,s) ds, ' ∈ [2: L],
0o
V1 - Z Gv (Θ; Φ*,s) ds,
0o
v`(Θ, Θ)—
VL+1(Θ)-
「Gv (Θ, Θ;Φt,s) ds, ' ∈ [2:L],
00
G GL+1 (Θ[Φ*,s) ds.
Jo
We call the process as neural feature flow because it characterizes the evolution of both weights and features.
5	Main Results
We first present the assumptions that are needed in our analysis.
Assumption 1 (Activation Functions and Loss Function). For the activation functions, we assume that there exist constants
L1 , L2 , L3 > 0 such that, for all x ∈ R,
|h1(x)| ≤ L1,	|h2(x)| ≤ L1,	h01(x) ≤ L2,	h02(x) ≤ L2.
Moreover, for all x, y ∈ R,
h01(x) - h01(y) ≤ L3|x - y|,	h02(x) - h02(y) ≤ L3|x - y|.
For the loss function, we assume that there exist constants L4 , L5 > 0 such that, for all y ∈ Y, x1 ∈ R, and x2 ∈ R,
φ01(x1, y) ≤ L4,	φ01(x1, y) - φ01(x2, y) ≤ L5|x1 - x2|.
Assumption 1 is easy to be satisfied. It only requires some boundedness, continuity, and smoothness for the activation and loss
functions. It is adopted in mostmean-field analysis, such as (Mei et al., 2018; Araujo et al., 2019).
6
Algorithm 2 Initialize a Discrete Res-Net.	
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15	: Input the data X, variance σ1 , and a constant C7 . Independently draw Vι,i,j 〜po = N(0, dσ2) for i ∈ [d] and j ∈ [m]. Set 8ι,j = 1 Pi=1 vι,i,j βo,i where j ∈ [m].	◊ Standard Initialization for layer 1 : for ` = 2, . . . , L do : Independently draw v`,i,j 〜N (0, mσ12 * for i, j ∈ [m]. Set a',j = m1 Pmm=I v`,i,j h 1(β'-1,i) where j ∈ [m]. Set β',j = β'-ι,j + h2 (α',j) for j ∈ [m].	◊ Standard Initialization for layer ' : end for Set vL+1,i,1 = C7 where i ∈ [m].	◊ Simply initialize {VL+1,i,1}m=1 by a constant : for ` = 2, . . . , L do : for j = 1, . . . , m do Solve convex optimization problem:	◊ Perform '2-regression to reduce redundancy 1m	1m Lminm - E(V',i,j)2 , s∙t∙ α',j = — ΣSv',i,j h 1(∕β'-1,i).	(5∙2) {v',i,j}i=ι m i=ι	m i=l : end for : end for Output the discrete Res-Net parameters (v, α, β).
Assumption 2 (Strong Universal Approximation Property). Assume that for any function f2 : Rd → RN that is bounded by CB,
i.e., for all v1 ∈ Rd, kf2 (v1)k∞ ≤ CB, we have
λmin
/ hι &Xvι + f2 (vι)
h 1 dXXvi + f2 (vι)
>
dp1 (v1)
(5.1)
≥ X > 0.
where λX only depends on X, CB, andh1, and p1 = N 0d, Id .
Assumption 2 is a technical assumption that we conjecture to hold under fairly general conditions. Notably when CB = 0, it is
shown in (Du et al., 2019a, Lemma F.1) that the assumption holds for all analytic non-polynomialh1. Lemma 1 affords many
examples that satisfy the assumption for constant CB .
Lemma 1. Suppose that the data is non-parallel, i.e., xi ∈/ Span(xj ) for all i 6= j.
(i)	If g: R → R is a non-polynomial function, then h1 (x) := g(cx) satisfies Assumption 2 when c > 0 is sufficiently small.
(ii)	The Relu-type function h1 (x) = (x)α+ for α > 0 satisfies Assumption 2.
(iii)	If hi (x) = c∣x∣-α or hi (x) = c(x)-α for |x| > C, where c,c0,α > 0, then hi satisfies Assumption 2.
We consider the Res-Net initialized by Algorithm 2, which is composed of a standard initialization (Glorot & Bengio, 2010; He
et al., 2015) and an additional regression procedure while preserving all initial features2. The standard initialization strategy scales
the weights to √m, which diverges in the mean-field limit. We perform the simple '2 -regression to reduce the redundancy of the
weights.
Now we start the analysis. First, we show in Theorem 1 that there is a neural feature flow that can capture the evolution of a Res-Net
that is initialized by Algorithm 2 and trained by Gradient Descent, i.e, Algorithm 1.
Theorem 1. UnderAssumptions 1 and 2, there is an initialization ({ V'}L=+1,p) Such that the continuous ReS-NethaS thefollowing
properties.
2In Algorithm 2, the weights in the last layer {vL+i,i,i}im=i can also be initialized by the standard initialization followed by an
'2-regression. The '2-regression equation 5.2 can be replaced by a soft version
m
2
min
{v',i,j }m=1
m
λm	2
m 上(v`,ij) + α `j-
Λ	7_	/ /ɪ	∖
v`,i,j hi(β'-i,i).
i=i
7
(1)	For any T < ∞, there exists an unique neural feature flow Φ* satisfying Definition 1.
2k
(2)	Suppose ε ≤ O⑴，δ ≤ 1, m ≥ Ω(ε	), the step size η ≤ O(ε). Let T be a constant and K := [T/n」.Let L :=
1 PN=ι φ(βL+1,1(n),yn) be the loss OfrUnning scaled Gradient Descent Algorithm Iona Res-Net initialized by Algorithm
2 at k-th step, and Lt := N〉2:=1 Φ (βL+ι (Φ*, t)(n), yn) be the loss of neural feature flow at time t. Then, with probability
1 - δ,
SuP ILk -Lknl ≤ O(ε).
k∈[0:K] '	'
The existence and uniqueness of Φ* follows from the technique of PiCard iterations (see, e.g., Hartman (1964)) With a special
consideration on the search space to deal with the unboundedness of parameters.
The second result shows that Φ* can approximate the training process of a sufficiently over-parametrized Res-Net. In the discrete
DNNs, although the connecting weights are independently initialized, the features θ`,j are not mutually independent since they all
depend on a common set of random outputs from the previous layer. Our key observation is that the skip-connected paths of the
discrete Res-Net {Vι,i, α2,i,..., αL,i}m=ι from the standard initialization are nearly independent when m is sufficiently large,
which makes it possible to construct an ideal initialization with independent skip-connected paths to approximate the discrete one.
Then, using a “propagation of chaos” argument Sznitman (1991), we compare an ideal process determined by Φ* with the discrete
one from Algorithm 1.
Finally, we consider the global convergence of the neural feature flow. We show in Theorem 2 that the neural feature flow always
finds a globally optimal solution when it converges.
Theorem 2. Under Assumptions 1 and 2, assume that the loss function φ is convex in the first argument. Let ({v'}L+1,p) be
the initial continuous Res-Net in Theorem 1, and Φ* and Lt be the solution and loss Ofthe neural feature flow, respectively. If
Φ, L (Θ)(t) converges in '∞ (p) and ΦV,l+i (Θ)(t) converges in 'ι(ρ) as t → ∞, where Θ 〜p, then we have
N
lim Lt = X
t→∞
n=1
Theorem 2 is an important application of our mean-field framework, which shows that neural feature flow can find a global mini-
mizer after it converges. We prove that the distribution of the weights in the first layer always have a full support in any finite time
by Brouwer’s fixed-point theorem. Then, using a similar argument to Chizat & Bach (2018), we show that all bad local minima
are unstable. Our global convergence holds for Res-Nets with arbitrary (finite) depth. Before us, the global convergence result was
proved only for two-level NNs (Mei et al., 2018; Chizat & Bach, 2018), and more recently for three-level ones (Nguyen & Pham,
2020) under a similar convergence assumption on the weights in the second layer.
6 Conclusions
This paper proposed a new mean-field framework for DNNs where features in hidden layers have non-vanishing variance. We
constructed a continuous dynamic called neural feature flow that captures the evolution of sufficiently over-parametrized Res-Nets
trained by Gradient Descent. Furthermore, the neural feature flow reaches a globally optimal solution after it converges. We hope
our new analytical tool pioneers better understandings for DNN training.
There are many interesting questions under this framework to be further investigated:
(A)	It is not clear whether the dynamics of DNNs trained by Gradient Descent can be characterized by PDEs of Mckean-Vlason
type. Recently AraujO et al. (2019) pointed out the difficulty lied in the potential discontinuity of the conditional distribution
under Wasserstein metric. From the viewpoint of our framework, the features of the hidden units potentially collide with others
along the evolution.
(B)	It is not answered in this paper how to analyze the evolution of DNN with special regularizers such as relative entropy regu-
larizer. Can we prove that Gradient Descent find a global minimum under such regularizers? More discussions about fusing
suitable regularizers is shown in Section F of supplementary material.
(C)	This paper only studies the optimization aspect of DNNs. It is interesting to study the generalization property. There are two
potential directions: we may incorporate regularizers on the DNNs to control the model complexity; implicit regularization is
often observed in practice, which is hopefully preserved in the neural feature flow.
We encourage the readers to see more discussions in the full version of this paper in the supplementary material.
8
References
Zeyuan Allen-Zhu and Yuanzhi Li. Can sgd learn recurrent neural networks with provable generalization? arXiv:1902.01028,
2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going
beyond two layers. arXiv:1811.04918, 2018.
Dyego AraUjo, Roberto I Oliveira, and Daniel Yukimura. A mean-field limit for certain deep neural networks. arXiv:1906.00193,
2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for
overparameterized two-layer neural networks. In International Conference on Machine Learning, 2019.
Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. Mean-field analysis of two-layer neural networks: Non-asymptotic rates
and generalization bounds. arXiv:2002.04026, 2020.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal
transport. In Advances in neural information processing systems, pp. 3036-3046, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In Advances in Neural
Information Processing Systems, pp. 2933-2943, 2019.
Donald A Dawson. Multilevel mutation-selection systems and set-valued duals. Journal of mathematical biology, 76(1-2):295-378,
2018.
Donald A Dawson, Kenneth J Hochberg, et al. Wandering random measures in the fleming-viot model. The Annals of Probability,
10(3):554-580, 1982.
Xialiang Dou and Tengyuan Liang. Training neural networks as learning data-adaptive kernels: Provable representation and ap-
proximation benefits. arXiv:1901.07114, 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural
networks. In International Conference on Machine Learning, 2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural
networks. In International Conference on Learning Representation, 2019b.
Cong Fang, Hanze Dong, and Tong Zhang. Over parameterized two-level neural networks can learn nearoptimal feature represen-
tations. arXiv:1910.11508, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of
the thirteenth international conference on artificial intelligence and statistics, pp. 249-256, 2010.
Andrzej Granas and James Dugundji. Fixed point theory. Springer Science & Business Media, 2013.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on Learning Representation, 2016.
Philip Hartman. Ordinary differential equations. Wiley, 1964.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In
Advances in neural information processing systems, 2018.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In
Advances in Neural Information Processing Systems, 2018.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceed-
ings of the National Academy of Sciences, 115(33):E7665-E7671, 2018. ISSN 0027-8424.
9
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds
and kernel limit. In Annual Conference on Learning Theory, 2019.
Phan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multilayer neural networks.
arXiv:2001.11443, 2020.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss
landscape and universal scaling of the approximation error. arXiv:1805.00915, 2018.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. Stochastic
Processes and their Applications, 2019a.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of deep neural networks. arXiv:1903.04440, 2019b.
Alain-Sol Sznitman. Topics in propagation of chaos. In Ecole dfete de Probabilites de Saint-Flour XIX_1989, pp. 165-251.
Springer, 1991.
Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and TrendsR in Machine Learning, 8(1-2):1-230,
2015.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv:1011.3027, 2010.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural nets v.s.
their induced kernel. arXiv:1810.05369, 2018.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. In Advances
in Neural Information Processing Systems, pp. 6594-6604, 2019.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv:1901.09321,
2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-parameterized deep relu
networks. In Advances in neural information Processing systems, 2018.
10
A Appendix
In this appendix, we present the key proofs for the results in the main text, and we defer the technical proofs to the full version
of this paper in the supplementary material. In the proof, we fix a set of non-parallel training data and treat the parameters in the
assumptions as constants. We use C to denote a generic constant; the value of C may change from line to line.
The appendix is organized as follows: Section A.1 introduces more notations that will be used in our proof. Section A.2 provides
the key proof for Theorem 1, in which the proofs of Theorems 3 -5 are deferred to supplementary material. Section A.3 proves
Theorem 2, in which an additional lemma (Lemma 16 in the supplementary material) is used. Finally, in Section A.4, we provide
the proofs for Lemma 1 and Proposition 1.
A. 1 Additional Notations
We introduce more notations which shall be used in our proof. For c > 0 and q ∈ [1, ∞], let Bq (a, C) denote the 'q-ball centered at
a of radius c. We say a univariate distribution P is σ-sub-gaussian if Ex 〜P exp(x2∕σ2) ≤ e3; We say a d-dimensional distribution P
is σ-sub-gaussian if the law of u>x is σ-sub-gaussian for X 〜P and any U ∈ Sd-1. Moreover, for distribution p, we use SuPP(P)
to denote its support.
A.2 Proof of Theorem 1
In Theorem 1, the well-posedness of the neural feature flow in Definition 1 can be established under mild conditions on the
initialization including the tail of initial distribution P and the continuity of the initial weights (see Theorem 3). To prove Theorem
1, it remains to construct an initial continuous Res-Net satisfying those conditions and show the corresponding neural feature flow
is close to the training trajectory of the scaled Gradient Descent on a sufficiently overparametrized discrete Res-Net initialized by
Algorithm 2. However, in the discrete Res-Nets, although the connecting weights are independently initialized, the features are not
mutually independent since they all depend on a common set of random outputs from the previous layer. Our crucial observation
is that those features are almost independent when the width m of the hidden layers are sufficiently large; namely, there exists an
ideal initial Res-Net with mutually independent features such that the distance to the actual discrete Res-Net is vanishing with m.
A precise definition is given in Definition 2. This allows us to construct an ideal process to bridge the trajectories of the discrete
Res-Net and the continuous Res-Net.
Road Map: In Section A.2.1, we propose the general initial conditions for continuous and discrete Res-Nets, respectively. In
Section A.2.2, we analyze the behaviors of the Res-Nets under those general initial conditions. We first study the continuous Res-
Net and show the existence and uniqueness of neural feature flow stated in Theorem 1 (1); then we bridge the trajectories of the
continuous and discrete Res-Nets as stated Theorem 1 (2). Lastly, in Section A.2.3, we verify that the initialization from Algorithm
2 satisfy the conditions in A.2.2 and also construct a continuous Res-Net to prove Theorem 1.
A.2.1 Definitions
We first propose the initial condition for the continuous Res-Nets. In our proof, unless otherwise specified, any initial continuous
Res-Net ({v'}L=+1 ,p) is needed to satisfies the forward propagation constraints, i.e., equation 3.7 - equation 3.10.
Assumption 3 (Initialization for Continuous Res-Net). We assume that P is σ-sub-gaussian distribution. We assume that, for all
' ∈ [2 : L], V'(∙, ∙) has sublinear growth on the second argument, that is, there is a constant C5 such that
∣v' (Θ, Θ)∣	≤ C5(1+ ∣∣(Θ∣∣∞) , forall Θ, Θ ∈ SuPP(P), ' ∈ [2 : L].
Moreover, we assume that V'(∙, ∙) are locally Lipschitz continuous where the Lipschitz constant has sub-linear growth on the SeCOnd
argument. In detail, there is a constant C6, such that for Θ1 ∈ SuPP(P), Θ1 ∈ SuPP(P) ∩ B∞ (Θ1 , 1), Θ2 ∈ SuPP(P), and
Θ2 ∈ SuPP(P) ∩ B∞ (Θ2 , 1), we have
Iv'(θ1, θ2) - v' (θ 1, θ2 )|	≤	C6 (1 + kθ2k∞ ) (llθ1 - θ 1∣l∞ + ∣∣θ2 - θ2∣l∞).
For the last layer, there exist constants C7 and C8, such that for all Θ, Θ ∈ SuPP(P), we have
∣vl+i(Θ)∣ ≤ C7 and ∣vl+i(Θ) - vl+i(Θ)∣	≤ Cg ∣∣Θ - Θ∣∣∞ .
Based on the initial condition for the continuous Res-Net, we then introduce the initial condition for discrete Res-Net.
Definition 2 (ει-independent Initial Discrete Res-Net). We say an initial discrete Res-Net (V, α, β) IS ει -independent ιfthere exist
a continuous initial Res-Net ({v'}L=+1 ,p) satisfying Assumption 3 and (V, α, β) such that
3Here the value e can be replaced by any number greater than one. See (Vershynin, 2010, Remark 5.6).
11
(1)	Θi = (vι,i,α2,i,...,αL,i)%p；
(2)	For β and v,
•	β',i = d (XVι,i) + P'1=2 h2(α'1,i) for ' ∈ [L] and i ∈ [m];
•	v`,i,j = v` (Θi, Θj) for ' ∈ [2 : L], i,j ∈ [m];
•	VL+1,i,1 = vl+i (Θi) for i ∈ [m];
(3)	ε1 -closeness:
•	l∣vι,i - vι,ik∞ ≤ (1 + ∣∣θi∣∣∞) ειfori ∈ [m]；
•	lv'+ι,i,j- v'+ι,i,j| ≤ (1 + ∣∣θi∣∣∞ + ∣∣θj∣∣∞) ειfor` ∈ [L - 1], i,j ∈ [m]；
•	lvL+1,i,1 - vL+1,i,1| ≤ (1 + ∣∣θi∣∣∞) ε1 for i ∈ [m].
A.2.2 General Analysis
We analyze the neural feature flow for the continuous Res-Net under the general conditions in Section A.2.1. The following theorem
guarantees the existence and uniqueness.
Theorem 3 (Existence and Uniqueness of Neural Feature Flow on Res-Net). Under Assumptions 1 and 3, for any T < ∞, there
exists an unique neural feature flow Φ*.
Moreover, We show that Φ* is a continuous mapping on Θ given time t, which is stated as follows. Theorem 4 will be used in the
proof of global convergence in Theorem 2.
Theorem 4 (Property of Φ*). Under Assumptions 1 and 3, let Φ* be the neural feature flow, there exist constants C, C0 ≥ 0 such
that for all t ∈ [0, T], Θ1 ∈ supp(p) and Θ1 ∈ supp(p) ∩ B∞ (Θ, 1), Θ2 ∈ supp(p), and Θ2 ∈ supp(p) ∩ B∞ (Θ2 , 1), we
have
∣∣Φβ,`(Θι)(t) - Φβ,'(Θι)(t)∣L ≤	Cec0t	(kΘιk∞	+1)∣∣Θι - Θ 1∣∣"	' ∈ [L],
∣∣Φα*(θι)(t) -Φα,'(θ 1)⑴L ≤	Cec0t(∣θιk∞	+ 1)∣∣Θ1 - θi∣^ ,	`∈ [2 ： l],
∣∣ΦV,1(θ1)(t) - ΦV,ι(Θι)(t)∣L ≤	CeC0t	(kΘιk∞	+ 1) ∣∣Θ1 - Θ 1∣L ,
∣ΦV,'(Θι, Θ2)(t) - ΦV,'(Θ 1, Θ2)(t)∣ ≤	Ce(Ct	(∣θιk∞	+ ∣∣Θ2k∞ + 1) ∣∣Θ1	- Θ 1∣L ,	'	∈	[2:	L],
∣ΦV,'(Θι, Θ2)(t) - ΦV,'(θι, Θ2)(t)∣ ≤	Ce(Ct	(∣θιk∞	+ ∣∣Θ2k∞ + 1) ∣∣Θ2	- Θ2∣L ,	'	∈	[2:	L],
∣ΦV,L+1(θ1)(t) - ΦV,l+i(Θι)(t)∣ ≤	CeCt	(∣θιk∞	+1) ∣∣Θ1 - Θι∣L .
The proofs of Theorems 3 and 4 follow from the technique of Picard iterations (see, e.g., Hartman (1964)) with a special consider-
ation on the search space to deal with the unboundedness of parameters. The latter differs from the former by introducing a more
restrictive space in which all the candidates satisfy the desired property. The details are shown in the proofs of Theorem 5 and 6 in
the full version of the paper in the supplementary material.
In the following, we analyze the trajectory of the discrete Res-Net from an ε1-independent initialization. Recall Definition 2 that
an ε1 -independent initialization induces a continuous Res-Net satisfying Assumption 3, which yields an unique neural feature
flow Φ* by Theorem 3. We show that scaled Gradient Descent from an ει-independent initialization is well-approximated by
2
the corresponding neural feature flow when the number of hidden units is Ω(ει ), where Ω hides poly-logarithmic factors. This
resembles a “propagation of chaos” argument Sznitman (1991). We compare the scaled Gradient Descent with an ideal discrete
process determined by Φ* as specified below:
• Actual process (v[0:K], α[0:K], β["K]) by executing Algorithm 1 in K = 工 steps on the discrete Res-Net from
(v,a, β);
12
•	Ideal process (V [0 ,T], α [0 ,T], β [0 ,T]) that evolves as neural feature flow:
β',i	=	Φ*	(Θi) (t),	'	∈	[L], i ∈ [m], t ∈ [0,T],
α',i	=	Φα,'	(Θi) (t),	'	∈	[2: L],i ∈ [m], t ∈ [0,T],
Vt,i	=	Φv,ι	(Θi) (t),	i ∈	[m], t ∈ [0,T],
vt,i,j	=	Φv,'	(Θi, Θj)	(t),	' ∈ [2 ： L], i ∈ [m],i ∈	[m], t ∈ [0,T],
VL+1,i,1	=	Φv,L+ι	(Θi) (t), i ∈ [m], t ∈ [0,T].
We also compare the discrete and	the continuous losses	denoted by LR := n PN=I	Φ(βL+ι,ι(n),yn)	and LR	：=
N PN=I φ (βL+ι(Φ*,t)(n),yn), respectively. We have the theorem below.
Theorem 5. Under Assumption 1, suppose ει ≤ O(1) and m ≥ Ω(ε-2), and teat the parameters in assumptions and T as
constants. Consider the actual process from an ε1 -independent initialization in Definition 2 with step size η ≤ O(ε1 ). Then, the
following holds with probability 1 - δ:
(1)	The two processes are close to each other:
Sup I SUpM,i- Vk"|	, SUp	∣vk,i,j - ∙⅛ηJ] ≤O(εD,
k∈[0.K] I i∈[m] 11	ll∞ 'e[2:L], i,j∈[m] I	,lJ
Sup	IlvL+1,i,1 -vL+1,i,1∣, Sup IlaIki-akηi∣∣	, Suphek,i-∕¾∣∣ [WO©).
k∈[0.K], i∈[m] I 1	1 '∈[2.L] 11	, ll∞ '∈[L] 11	, ll∞ J
(2)	The training losses are also close to each other:
Sup ILR - Lr ∣ ≤ O(ει).
k∈[0.K]
The proof of Theorem 5isa GrOnWall-type result by a similar argument to the “propagation of chaos”. The details are shown in the
proof of Theorem 7 in the full version of this paper in the supplementary material.
A.2.3 Verification of Algorithm 2
In this subsection, we verify that Algorithm 2 produces an ει-independent initial discrete Res-Net with ει ≤ O(1∕λ∕m). By
Definition 2, this entails the construction of an initial distribution p, weight functions {v∣ }L+21, and an ideal discrete Res-Net
satisfying the properties in Definition 2. We specify p, {v'}L=+2, and the ideal discrete Res-Net below, and then we verify the
properties in Lemma 2.
Initial distribution. We first define the distribution p:
(1)	At the first layer, βι 〜pβ = N (0N, σ2K0), where K0 := dXX>.
(2)	At the layer ' ∈ [L — 1], let Ke := R hɪ (e`) hɪ (β')> dp/ (β'). We define the distribution of the residuals at layer ' + 1 as
pO+1 = N(0N,σ2Kβ) .	(A.1)
Tʌ Γ∙	，1	■	7	/C	∖	zɔ .	7	/	\，1C，	. 1	/).-1- If 1	，1	1 C	1
Defining the mapping f∣+ι (e`, α∣+1) := β' + h2(ɑ'+1), the features at layer ' + 1 is defined as the pushforward measure
1 孑
by f'+i：
Pβ+ι = f'+i# (Pe × P?+).
Finally, let p be a multivariate Gaussian distribution of the form
P (vi, ai, α2,..., aL) := Pv(vi) × Pα(a2) × Pα(a3) × …× P?(«l).	(A.2)
Weight functions. Now we define the weight functions {v'}'L=+2i. Note that those gram matrices K'β are all positive definite
under Assumption 2 (see Lemma 3) and thus are invertible. For Θ = (vi , α2, . . . , αL), Θ0 = (v0i, α02 , . . . , α0L), we define the
connecting weights between consecutive layers by
v'(Θ, Θ) = h 1 (β'-i(Θ))> hκβ-iiT a',	' ∈ [2 : L],	(A.3)
where β'(Θ) = dXvi + P'=2 h2(ai) satisfies equation 3.9. The weights at the output layer are initialized as a constant c. The
forward propagation constraint equation 3.8 is also satisfied by equation A.3 and the definitions of K'β. Therefore ({v'}'L=+2i ,P)
constitutes a feasible continuous Res-Net.
13
Ideal discrete Res-Net. Finally We construct the initialization (V, α, β) of the ideal discrete Res-Net. Recall Algorithm 2 that
the corresponding variables are initialized as (v, α, β). Let vι,i := Vi,ι for i ∈ [m]. For ' ∈ [L — 1], define the empirical Gram
matrix as
m
K β = m X h 1 (β',i)h > (β',i).
i=1
Let α'+ι,j :=	(Ke)	(	(Kβ)	α	α'+ι,jfor all j ∈	[m]	when	Ke	is invertible, and otherwise let	α'+ι,ji吧⅛+ι∙ We use
Definition 2 (2) for the values of β' for ' ∈ [L] and v` for all in [2 : L + 1].
In the next lemma, we verify that the discrete Res-Net from Algorithm 2 has an O(1 /√m)-independent initialization, and the
corresponding continuous Res-Net satisfies Assumptions 3.
Lemma 2. Under Assumption 2, ({v'}L=+1 ,p) is a feasible continuous Res-Net that satisfies Assumptions 3. Moreover, under
Assumptions 1 and 2 treat the parameters in assumptions as constants. With probability at least 1 — δ, Algorithm 2 produces an
ει-independent initial discrete Res-Net based on ({v'}L=+2 ,p) with ει ≤ O(√m).
Proof. We first consider the continuous Res-Net. By definition p is a multivariate Gaussian distribution. By Lemma 3, we have
(Ke )	≤ C fora constant C. Since the activation function hi is bounded and Lipschitz continuous, the continuity conditions
in Assumption 3 are all satisfied.
Now we consider the ideal discrete Res-Net. We first verify the independence. By definition, α'+i,j are determined by the outputs
of previous layer β',i and the connecting weights V'+ι,i,j. Thus they are conditionally independent of vi,i and a2,i,..., α',i
for i ∈ [m] given {β',i}i∈[m]. Since V'+ι,i,j are independent Gaussian, α'+i,j and thus a'+i,j are conditionally independent
Gaussian given {β',i}i∈[m]. Furthermore, the conditional distribution of α'+i,j given {∕3',i}i∈[m] is N(0N,σ2⅛e) = pα+ι.
Therefore, marginally α'+i,ji^pO+i and they are independent of Vi,i and α2,i,..., α',i for i ∈ [m]. So {Θi}i∈[m]i^p. Since P
is a product distribution, all Vi,i, α2,i,..., αL,i are all mutually independent.
T ，1	1	1 1 ZA /r /	/-∖	1	∙ C 1 ∙ ɪʌ C ∙ , ∙	C /C' CT	Λ	1	.1 ɛ 11	■	，	∙，1	1	1 -T .
Lastly we show the O(1 /ʌ/m)-closeness specified in Definition 2 (3). By Lemma 4 we have the following events with probability
1 — δ:
kα'+1,ik2 ≤ O⑴，	(A.4)
IIKe—KeIL ≤，?,	(a.5)
ll<α'+i,i — α'+1,ik2 ≤ ε2 ∣l<Θi∖∣2,	(A.6)
I∣β'+ι,i- β'+1,i∖∖2 ≤ ε2∖∖θi∖∖2 ,	(A.7)
where ε2 ≤ O(1 /√m). Under equation A.4 the matrix K夕 is invertible, and it follows from Lemma 5 that
V'+i,i,j = α>+i,j Kei	hi(β',i),	' ∈ [L — 1], i,j ∈ [m].
By the triangle inequality,
lv'+1,i,j - v'+1,i,j |
=α >+ι,j [Kβi-ihIM) — a>+ι,j [Kβi-ihι(β',i)∖[
≤kα'+1,jk2 [Kβi-i J∖hi(β',i) — h 1(β',i)∖∖2 + ka'+1,jk2 [Kβi-i — [Kβ]1 Jhι(β',i)∖L
+ kα'+1,j — α'+1,jk2 hKβi	∖∖h 1(∕⅛',i)∖∖2 .
We upper bound three terms separately. By the Lipschitz continuity of hi and equation A.7, the first term is at most
，. ʃ f ∖ I I   I I	→	-	_ ,	， . J f ∖ I I   I I	.	I I	I I	. I I   I I
O(1∕λ∕m)k(Θ j∣∣2; the second term is also at most O(1∕λ∕m)k(Θ j∣2 since ∣α '+1,j∣∣2 ≤ ∣∣(Θ j∣2,
IhKe i T - hKβ i T∖∖2 ≤ ∖∖hKβ i T∖∖2 ∖∖Ke - K β ∖∖2 [[Kβ i T∖∖2 ≤ ℃2 ,
17	-1	Il ,1	,1 ∙	1 ,	■	,	, ZZΛ /-ɪ	/	/-∖ Il ʌ Il 1	,∙	. ʃ
and hi is bounded; the third term is at most O(1 /ʌ/m) ∣∣ Θj 12 by equation A.6.
□
14
With the above preparations, we are now ready to achieve Theorem 1.
Proof of Theorem 1. By Lemma 2, the discrete Res-Net and its corresponding continuous Res-Net satisfy our general initial con-
ditions in Definition 2 and Assumptions 3, respectively. Then, Theorem 3 and Theorem 5 (2) indicate Theorem 1 (1) and (2),
respectively.
□
A.2.4 Proof of additional lemmas
Lemma 3. min'∈[L-i] λmin(K?) ≥ C > 0.
Proof. Fix ` ∈ [L - 1]. For (v1 , α2, . . . , αL) ∈ supp(p), given α2, . . . , αL, we have Θ = Θ(v1) and
β'(Θ) - dXvi
`
T h2(α'1)	≤ LLi.
(A.8)
∞
Note that vi is independent of α2,..., aL, and vi 〜N(0, dσ2Id) which is equivalent to the standard Gaussian distribution. By
Assumption 2 with f2(vi) ≡ P'1 =2 h2(α'J and the constant CB = LLi, we have
E [h i (β'(Θ)) h > (β'(Θ)) | α2,..., az]之 CIN
Taking full expectation, We obtain Lemma 3.	□
In the sequel, We set λi :二 min'∈[L-i] λmin(K∕) that is strictly bounded away from zero.
Lemma 4. Let ε? ≤ O(1 /ʌ/m). With probability 1 — δ, for all' ∈ [L — 1] and i ∈ [m],
IKβ — KelL ≤ ε2,	ka'+i,i — α'+i,ik2 ≤ ε2 ∣∣Θ力、,
kα'+i,ik2 ≤ 0(1),	(β'+i,i - β'+i,i∣L ≤ ε2 ∣∣θi∣∣2 .
Proof. In the proof of Lemma 2 we verified that vi,i, α2,i, ..., αL,i for all i ∈ [m] are independent. Therefore, β',i吟p? by the
definitions of β',i and p?. Consider auxiliary random matrices
i=i
Since hi is bounded, by the matrix Bernstein inequality (Tropp, 2015), with probability 1 — 3
'∈maxi]∣Kβ - Kβ∣∣2
≤ ε3 = (9(1∕√m).
(A.9)
Due to the sub-gaussianness of p, we have ∣∣α'+i,i∣∣2 ≤ C,log(m∕δ) = O(1) with probability 1 — δ∕3 (Vershynin, 2010). We
will also use the following upper bound that happen with probability 1 — δ∕3 by the sub-gaussianness of p:
	1m m Xkθ ik2 i=i	≤ βi = O(1).	(A.10)
Next we inductively prove that, for ` ∈ [L — 1], ∣∣∣K	宵-Kβ∣∣2 ≤	(Cβi)'-iCε3,	(A.11)
kα '+i,i	—α'+i,ik2 ≤	(Cβi)'-iCε3∣∣Θi∣∣2,	(A.12)
∣β'+i,i	-β'+i,i∣∣2 ≤	(Cβi)'-iCε3∣∣Θ i∣∣2.	(A.13)
15
For ' = 1, by definition KKf = KKf. The upper bound of
Section V.3). Since I∣K<β
[蹬 Γ-[κβ i1/2
Kβ∣∣ ≤ λ1, then the eigenvalues of Kβ are at least
when X is the eigenvalue of K，. Applying (Bhatia, 2013, (V.20)) yields that
is achieved by matrix calculus (Bhatia, 2013,
2
λ1. Let f(x) := √x. Then ∣f0(x)∣ ≥ √1入
and
Ila2,i - α2,ik2
hκ^βi1/2 - hκβi1/2
≤ -ɪ
2λι
(A.14)
≤ Cε3kα 2,ik2 ≤ Cε3kΘik2.
2
(A.15)
—
2

Then by the Lipschitz continuity of h2, we have
∣∣β2,i - β2,i∣L = II [h2(α2,i) - h2(α2,i)i IL ≤ Cε3∣∣Θi∣2.
(A.16)
For ` ∈ [2 : L - 1], suppose that
IM- βe,i∣L ≤ (cβ1)'-2 Cε3∣∣θi∣∣2.
Then, by the boundedness of h1 and the triangle inequality, we have
m
IIκβ - KβII2 ≤ m XMM) -hι(β,,<.
i=1
Applying the Lipschitz continuity of h1 and equation A.17 yields that
`-2	m
IIKe - KeIL ≤ 3m_3 X IIθiII2 ≤ (Cβι )'-1Cε3.
i=1
(A.17)
(A.18)
where in the last inequality we used equation A.10. Then we obtain equation A.11 by triangle inequality from equation A.9 and
equation A.18. The upper bound in equation A.12 for ` + 1 follows from a similar argument of equation A.14 and equation A.15.
Finally equation A.13 for ` + 1 follows from equation A.12 and
^

β'+1,i - B'+]”]?
'+1
X [h2(aj,i) - h2(αj,i)]	≤ C(cβ1)'-1ε3 IIΘiII2.
j=2	I2
<τr C ∙1,1	-	1	,∙	C∙	C	ZZΛ∕-1 ∖	1	ZA ∕r∕	)-∖	1	,	,1	ɛ
We finish the induction. Since β = O(1) and ε3 = O(1∕λ∕m), We complete the proof.
□
β
Lemma 5. If K' is invertible, then
V'+ι,i,j = α>+ι,j [Kβi	hι(β',i),	' ∈ [L - 1], i,j ∈ [m].
Proof For a given layer ' and j, the '2-regression problem in Algorithm 2 can be equivalently written as
where V
(v'+1,1,j ,..., v'+1,m,j)ɪ and
min
V
s.t.
H= [h1(β',1),..
2 kvk2
1
-H V = α '+ι,j,
.,h 1(β',m)]
.Decompose V as
(A.19)
V = H Tz + V0,
where z ∈ Rm and HV0 = 0. Then equation A.19 is equivalent to
min
z, ^0
s.t.
1IIHT zII2+1IIV 0II2
mH H τz=α '+ι,j.
Since mm H H T = K β is invertible, the optimal solution is z
[Kβi	α'+ι,j and V0 = 0N.
□
16
A.3 Proof of Theorem 2
Proof of Theorem 2. In the proof we use the following abbreviated notations: for t ∈ [0, ∞) and Θ ∈ supp(p), let
βt(Θ)	=	Φβ,'(Θ)(t),	' ∈ [L],
α'(Θ)	= Φα,'(Θ)(t), ' ∈ [2:L],
v1(Θ)	= ΦV,ι(Θ)(t),
VL + 1(θ)	= φv,L+1(θ)(t).
From the convergence assumptions, it is clear that βLt +1 converges as t → ∞. Indeed, the convergence assumptions imply that,
for any ε2 > 0, there exists T , for any t ≥ T ,
βLt (Θ) - βL∞(Θ)∞ ≤ε2
holds p-almost surely and
Z vLt +1 (Θ) - vL∞+1 (Θ) p(Θ) ≤ ε2 .
Then, since h1 is bounded and Lipschitz continuous, we have
βLt +1 - βL∞+1∞
=||y VL+ι(Θ) h 1 (βL(Θ)) - v∞+ι(Θ) h 1 (β∞(Θ)) dp(Θ)
≤ Z ∣v∞+ι(θ)∣∣∣hι (βL(Θ)) - hι (β∞(Θ))∣L dp(Θ)
+ZM+i(θ) - v∞+ι(θ)∣∣h ι (βL (θ))∣∣ip(θ)
≤ Cε2.
(A.20)
(A.21)
(A.22)
∞
The goal of the proof is to show that
∣∣φ 1 (βL+1)∣∣2 = 0.	(A.23)
To this end, for any ε > 0, we will construct a function
fε(vi) := φ 1 (β∞+1 )> h 1 (dXv 1 + gε(vι)) ,	(A.24)
where the functions gε is uniformly bounded, SuCh that ∣fε(v1)∣ < ε. Then it follows from equation A.24 that
φ 1 (β∞+1) = KT / fε(V1) h 1 dXXv 1 + gε(v1)) dp1 (V1),
where p1 = N(0d, Id) and K := R h 1 (dXv1 + gε,η(v1)) h> (dXv1 + gε,η(v1)) dp1 (v1) whose minimum eigenvalue is at
least Λ1 > 0 by Assumption 2. The boundedness of h1 yields that
∣∣φ 1 (βL+1)∣∣2 ≤ Cλ-1ε.
Since Λ1 is independent of ε, by letting ε → 0, we obtain equation A.23.
Next we construct gε and f in equation A.24. Let T be the time such that equation A.20 and equation A.21 hold with ε2 ≤ cε
for a constant C to be specified. Note that VT is surjective by Lemma 6. Let g : Rd → SuPP(P) be the inverse function such that
VT (g(VI)) = v1. Define
L
gε(V1) = X h2 (αT (讥V1))) ,	fε(V1) = φ 1 (β∞+1)> h 1 (βT(g(V1))),
'=2
where gε is uniformly bounded by the boundedness of h2. Suppose on the contrary that there exists v1 such that |fε(v1 )| > ε. Let
Θ0 = g(v1). Since Θ → φ1 (β∞+1)> h 1 (βT(Θ)) is continuous by Theorem 4, there exists a ball around Θ0 denoted by S such
thatP(S) > 0 and ∣φ1 (β∞+1)> h 1 (βT(Θ)) | > ε∕2 with the same sign for all Θ ∈ S. However, for t > T,
∣∣∣VLt +1 (Θ) - VLT+1 (Θ)∣∣∣ dp(Θ)
≥ Nn ∕is∣( Φ1 (βL+1)> h 1 (βL(Θ)) dt dp(θ)
≥ Nn jIs (∣( Φ1 (β∞+1)> h 1 (β∞(Θ)) dt∣ - / Cε2dt) dp(Θ),	(A.25)
17
Where5'rhe IaSr SreP We USed equar5'n A∙2pequar5'n A∙22" and rhe boundedness and LiPSChirZ COnr5'UiryOf-and hl∙
c =^∙ TheWer bound in equation A∙25 diverges Wirh --which COnrradiCrS equation A∙2L
Finally from equarn A∙23 We ShOW rhe ConVergenCe SraSmenL SCe e is ConVeX On rhersr argumewe Obta
N Nl-
0lq) Uef∙
12=1 12=1 L 」
SCe+Ii 41 and eCOnrinUOUS- we Obtarh
N Nl-
-好 UU0lq) Un0J))
12 = 1 12=1 L 」
WhiCh completes rhe PrOOL
Lemma 6∙ ThefIlnCtn -A 8》v(iSUPP(F)，disa SIlFjeCtn.
proof. ReCallrharsrhe5'iriaHZarion Ier ①(V) = (F OD——d) m 毋DU SUPP(P) ∙ GiVen -八 ∞" consider ⅛≈ X as
“( V)Uλ(①(V))∙
Ir sufficesShOW rh(t is SUljeCHve∙ Nore rhft is COnrUOUS SCe ® I(①)is COnrinUOUS by TheOremFUrrhenlh
for any v md- by Lemma 16rhe supemeary mariaL WhiCh SraSSrha二he gradient: Ofrhe WeighrS bounded- We h
>(v)lvΓuf 百(TDL
e*3S) = CIS ≤ Cf.
FOr any X m 毋 d" COnSider g(v) ɪ= x — (j-(V) — V) WhiCh COnr5'UOUSly maps Bg 夕 Ct)δirself∙ By rhe BrOUWeySf
PoinrrheOrem (Se 尸 eg GranaS 行 DUgUndji (2013))"〔here exisV* moo(X) Cc SUehrhar g(v*) = v*; equivalenrly. We
"(F) U x∙
π)
□
□
A∙4 PRooF OF LEMMA IAND PRoPoSlTIoNI
PFOOfOfLemma 1. Wersr norhe following resulrs(DUal; 2019a- Lemma FLxSUPPOSe=rhe SuppoOf a random
VeCr V md denoted by R has PoSiHVe LebeSgUe measure- and h is an anaIyHC non—pynomial funcrn On R. Then
N 2
minaih(xi . V) H > V O-
T--H- I
2=1 2
Where a = (。一: •二。N) ∙ Lemma 1 ShOWSrhaL for vʃ 〜F = Af (0( Id)"rhe Same resuholds Wirh a COnSranr PeUrbarn Of
rhe funns F; nameIy- by Ierrg(v) = hi (Xm ∙ v + α(v)) Where 一≤ Qte-
N 2
minɪ2omgm(vʃ) =Vo" (A∙26)
-- 2 1 iu 2
whereis uniform over all PeUrbarions =≤ C⅛∙ H sucesprove equarn A∙26 for vʃ 〜q = UnifOrm(R) WhereKʃ
is dermed by h and C⅛" as rhe RadonlNikOdym derivaHVe 韭 is bounde
Wersr ProVe (consider a COmPaCr region R SUCh rhaL for V 〜UnifOrm(K) and any Unir VeCr a"
N 2
Bh3(x: V) ≥ >R V 0∙
=1 2
Then for any QVsce F is bounded and LiPSChirZ COnrUOUS- we have
N 2
HMɑ⅛∙3(xlv + aα(v∕α)) ≥ ∖r — Qαc⅛ ≥ *一
=1 2
Whena ≤ 2A. EqUiValenrly"=尸 Bhl(Xm∙ v`ɑ + ◎ (V`ɑ))啃，We achieve equarn A∙26 beng V =
Vm to
18
For (ii), consider R = {v : 1/2 ≤ ∣∣vk2 ≤ 1}. Then, for V 〜Uniform(R) and any unit vector a,
N	2
E X aihι(xi∙ V)	≥ λR > 0.
i=1	2
Since hι(βx) = βαx for any β > 0, then We have EkPN=I aihι(xi ∙ βV)∣2 ≥ β2αλR. Note that |xi ∙ βV∣ = Θ(β). For
x = Θ(β), we have |h1 (x)| ≤ Cβα and h1 is Cβα-1-Lipschitz continuous for a constant C. Therefore,
N	2
E X aihι(xi∙ βV + Ci(βV))	≥ β2αλR - C0β2α-1CB ≥ (CCCB产
i=1	2
for a constant C0 when β = 2；：B . We achieve equation A.26 by letting V0 = βV.
For (iii), We first shoW that there exists a compact set R such that, for all v ∈ R and xi ,
|xi ∙ v| ≥ c .	(A.27)
This can be done by a simple probabilistic argument. Let v be drawn from the uniform distribution on Sd-1, for any fixed x ∈ Rd,
we have
P{|v>x| < tkxk2}
2∏d-1 /Γ( d-1 ) F	2、d-3	厂
-----d i 2 ,	(1 - u2) F du < tVd.
2π 2/Γ(d)	J-t
Byaunion bound, we have |xi ∙ v| ≥ kxi√2 with probability 0.5. Denote the set of V ∈ Sd-1 by S. Since mini IlXik2 ：= Cx > 0,
we obtain equation A.27 with R = {tv : v ∈ S, 2c0N√d ≤ t ≤ 4cN√d }. Then, for V 〜Uniform(R) and any unit vector a,
N	2
E Xaihι(xi ∙ V)ll ≥ λR > 0.
i=1	2
Then, for any β > 0, we have EkPN=I &力1 (xi ∙ βV)∣2 ≥ β-2αλR. For X = Θ(β) we have ∣hι(x)∣ ≤ Ce-Ca and hi is
C β -α-1 -Lipschitz continuous for a constant C. Therefore,
l N	l2
E X aihι(xi ∙ βV + Ci(βV))	≥ β-2αλR - C0β-2a-1CB ≥ (CCCB)-2a
l i=1	l2
for a constant C0 when β = 2；：B . We achieve equation A.26 by letting V0 = βV.
□
Proof of Proposition 1. Explicitly shown in (Nguyen & Pham, 2020, Corollay 25), in the mean-field limit that m → ∞, the weights
remain mutually independent and follow a common distribution that only depends on time t in the intermediate layers. Therefore,
by the law of large numbers, the features are the same. We have Proposition 1.	□
19