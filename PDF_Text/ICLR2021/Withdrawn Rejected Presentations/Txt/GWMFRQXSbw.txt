Under review as a conference paper at ICLR 2021
Adaptive Learning Rates with Maximum Vari-
ation Averaging
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient methods such as RMSprop and Adam use exponential moving
estimate of the squared gradient to compute coordinate-wise adaptive step sizes,
achieving better convergence than SGD in face of noisy objectives. However,
Adam can have undesirable convergence behaviors due to unstable or extreme
adaptive learning rates. Methods such as AMSGrad and AdaBound have been
proposed to stabilize the adaptive learning rates of Adam in the later stage of
training, but they do not outperform Adam in some practical tasks such as train-
ing Transformers (Vaswani et al., 2017). In this paper, we propose an adaptive
learning rate principle, in which the running mean of squared gradient is replaced
by a weighted mean, with weights chosen to maximize the estimated variance of
each coordinate. This gives a worst-case estimate for the local gradient variance,
taking smaller steps when large curvatures or noisy gradients are present, which
leads to more desirable convergence behaviors than Adam. We prove the pro-
posed algorithm converges under mild assumptions for nonconvex stochastic op-
timization problems, and demonstrate the improved efficacy of our adaptive aver-
aging approach on image classification, machine translation and natural language
understanding tasks. Moreover, our method overcomes the non-convergence is-
sue of Adam in BERT pretraining at large batch sizes, while achieving better test
performance than Lamb in the same setting.
1	Introduction
Stochastic Gradient Descent (SGD) and its variants are commonly used for training deep neural
networks because of their effectiveness and efficiency. In their simplest form, gradient methods
train a network by iteratively moving each parameter in the direction of the negative gradient (or
the running average of gradients) of the loss function on a randomly sampled mini-batch of training
data. A scalar learning rate is also applied to control the size of the update. In contrast, adaptive
stochastic gradient methods use coordinate-specific learning rates, which are inversely proportional
to the square root of the running mean of squared gradients (Tieleman & Hinton, 2012; Duchi et al.,
2011; Kingma & Ba, 2015). Such methods are proposed to improve the stability of SGD on non-
stationary problems, and have achieved success in different fields across Speech, Computer Vision
(CV), and Natural Language Processing (NLP).
Despite the popularity of adaptive methods such as Adam (Kingma & Ba, 2015), the instability of
their adaptive learning rates sometimes leads to sub-optimal solutions or even non-convergent behav-
ior on some simple problems (Reddi et al., 2018; Luo et al., 2019). AMSGrad (Reddi et al., 2018)
was proposed to stabilize Adam by computing the adaptive learning rate with an update rule that
guarantees monotonically decaying adaptive learning rates for each coordinate. AdaB ound (Luo
et al., 2019) clips the adaptive learning rate of Adam with a decreasing upper bound and an in-
creasing lower bound, so that it transitions into SGD in the final stage of training. However, to
our knowledge, neither of the two approaches have been deployed to enhance Adam on recent
large-scale problems such as training Transformer-based language models (Devlin et al., 2019; Liu
et al., 2019; Lan et al., 2020; Raffel et al., 2019; Zhu et al., 2020). The stochastic gradients of
Transformer’s loss functions exhibit heavy-tailed statistics, making SGD converge to a much worse
solution than Adam. Zhang et al. (2019b) has to use gradient clipping to stablize SGD for training
transformers, indicating that the strategy of AdaB ound, which is to transition from Adam into
SGD, will fail on Transformers (see Appendix D for instance). RAdam (Liu et al., 2020) was re-
1
Under review as a conference paper at ICLR 2021
cently invented to free Adam from the warmup schedule for training Transformers, but its variance
rectification term does not really depend on the observed gradients during training, and Ma & Yarats
(2019) found that using a linear warmup over 2 ∙ (1 - β2)-1 iterations for ADAM achieves almost
the same convergence as RAdam.
In this work, we explore a different approach to improving the stability of adaptive learning rates. We
propose Maximum Variation Averaging (MaxVA), which computes the running average of squared
gradients using dynamic, rather than constant, coordinate-wise weights. These weights are chosen
so that the estimated variance of gradients is maximized. The MaxVA weights for maximizing
this variance have a simple closed-form solution that requires little storage or computational cost.
With this solution, MaxVA assigns a higher weight to a coordinate if the gradient at that coordinate
deviates too much (compared with the estimated variance) from the estimated mean, enabling a faster
adaptation to gradient change or the curvature. Extensive experiments on synthetic and practical
datasets demonstrate that this leads to an improved adaptability and stability for Adam, yielding
better test set performance than Adam on a variety of tasks. The effect is especially evident in the
large-batch optimization setting for BERT, where the total number of iterations is sharply reduced
and a faster adaptation in each step is more important, and Adam with MaxVA converges faster
than Adam and Lamb (You et al., 2020) with higher test scores in the experiments. We also prove
MaxVA converges under mild assumptions in the nonconvex stochastic optimization setting.
2	Preliminary and Definitions
By default, all vector-vector operators are element-wise in the following sections. Let θ ∈ Rd be the
parameters of the network to be trained, '(x; θ) is the loss of the model with parameters θ evaluated
at x. Our goal is to minimize the expected risk on the data distribution defined as:
f(θ)= Ex〜D ['(x; θ)].
(1)
In most deep learning problems, only a finite number of potentially noisy samples can be used
to approximate Eq. 1, and the gradients are computed on randomly sampled minibatches during
training. Stochastic regularizations such as Dropout (Srivastava et al., 2014) are commonly used for
training Transformers (Vaswani et al., 2017), which further adds to the randomness of the gradients.
Thus, it is important to design optimizers that tolerate noisy gradients. Adam (Kingma & Ba, 2015)
is an effective optimizer that adapts to such noisy gradients. It keeps exponential moving averages
to estimate the first and second moment of the gradient, mt and vt, defined as:
mt = amt-1 + (1 - α)gt,	mt
mt
vt = βvt-ι + (I - β)g2, vt
1 - αt+1
Vt
(2)
1 - βt+1 ,
where a, β ∈ [0,1], gt = Vθ'(xt； θt) is the gradient of the t-th minibatch xt, m0 = vo = 0, and
mt, vt corrects the initialization bias of mt, vt (Kingma & Ba, 2015). ADAM updates the parameters
with the estimated moments as θt+ι = θt - η √mζe, where e > 0 is a small constant for numerical
stability.
Ifwe assume that the distribution of the stochastic gradient is constant within the effective horizon of
the running average (Balles & Hennig, 2018), an assumption that is more accurate when the model
is closer to convergence, then mt and vt will be unbiased estimates of the first and second moments
of the gradient gt . Same as other adaptive methods such as ADAM and the recently proposed Ad-
aBelief (Zhuang et al., 2020), we use this assumption throughout training. Specifically, let σt2 be the
variance of gt. At time t, we assume E[mt] ≈ Vft, E[vt] ≈ Vft2 + σt2 . ADAM, RMSPROP and
other variants that divide the update steps by √vt can be seen as adapting to the gradient variance
under this assumption. These adaptive methods take smaller step sizes when the estimated variance
σt = vt - m2 is high. Higher local gradient variance indicates higher local curvature, and vice
versa. In certain quadratic approximations to the loss function, this variance is proportional to the
curvature (Schaul et al., 2013) (Eq. 13 of our paper). Therefore, like a diagonal approximation to
Newton’s method, using such adaptative learning rates to adapt to the curvature can accelerate the
convergence of first-order methods.
2
Under review as a conference paper at ICLR 2021
However, the adaptive learning rate η/(√t + E) of ADAM and RMSPROP can take extreme values,
causing convergence to undesirable solutions (Wilson et al., 2017; Chen et al., 2019). Reddi et al.
(2018) gave one such counter example where gradients in the correct direction are large but occur
at a low frequency, and Adam converges to the solution of maximum regret. They solve this issue
by keeping track of the maximum Vt for each coordinate throughout training with a new variable ^t,
and replace the adaptive learning rate with ηt∕√Vt to enforce monotonically descreasing learning
rates. Extremely small adaptive learning rates can also cause undesirable convergence behavior, as
demonstrated by a counter example from Luo et al. (2019).
3	Maximizing the Variance of Running Estimations
Algorithm 1 MADAM
1: Input: Learning rate {ηt}tT=1, parameter 0 <
α < 1, 0 <β<β < 1, e > 0
2:	Set m o	=Uo =万 o = wo = 0
3:	for t =	1 to T do
4:	Draw samples St from training set	
5:	COmPUte gt = τs1t∣ Pχk∈St ▽'(Xk； θt)	
6:	m t	=am t—1 + (1 — α)gt
7:	〜 βt 二	arg maxβ Vt (β) — Ut2 3 4 5 6 7 8 9 10 11 12 (β)	. see Eq 7
8:	βt	max(β, min(β, βt))
9:	Ut	二 fβtut-1 + (1 — βt)gt
10	Vt =	二 βtvt-1 + (1 — βt)gt
11	:	wt	=fβtwt-1 + (1 — βt)
12	:	θt	θ θ+ 1 _ √, Vwt	mt θ θt-1	ηt 1 —at √⅜+e
Algorithm 2 LAMADAM
1: Input: Learning rate {ηt}tT=1, parameter 0 < α <
1, 0 <β<β < 1, € > 0
2: Set mo = Uo = Vo = wo = 0
3: for t = 1 to T do
4:	Draw samples St from training set
5:	COmPUte gt = TStT pχk∈st ▽'(Xk; θt)
6:	βt = argmaxβvt(β) — u2(β)	. see Eq 7
7:	βt = max(β, min(β, βt))
8:	Ut = βtut-ι + (1 — βt)gt
9:	Vt = βt Vt-1 + (1 — βt)g2
10:	wt	= βtwt-1	+ (1 —	βt)
11:	mt	= amt-ι	+(1 —	α)	I g —
ʌ/ Vt/wt + e
12:	θt = θt-1 — 1—at mt
Motivation. We propose to mitigate the undesirable convergence issue of ADAM by changing the
constant running average coefficient β for the second moment into an adaptive one. The idea is to
allow βt to adopt the value that maximizes the estimated variance of the gradient at each iteration t.
Therefore, our algorithm can use βt as the adaptive running average coefficient to take steps that are
conservative enough to avoid instability but aggressive enough to make progress.
Maximum Variation Averaging. Formally, we estimate the variance of the gradient at each coor-
dinate by keeping track of the zeroth, first, and second moments of the gradient as functions of the
adaptive running average coefficient βt, denoted as wt(βt), Ut(βt) and Vt(βt), respectively:
wt(βt) = βtwt-1(βt-1) + (1 - βt),	(3)
ut(βt) = βtut-1(βt-1) + (I - βt)gt,	(4)
Vt(βt) = βtVt-ι(βt-ι) + (1 - βt)g2∙	(5)
The zeroth moment wt(βt) is used to normalize Ut(βt) and Vt(βt) to achieve bias-corrected esti-
mates ut(βt) = Ut(βt)∕wt(βt) and vt(βt) = Vt(βt)∕wt(βt) for the first and second moments, so
that the estimates are not biased towards mo = V。= 0 (Kingma & Ba, 2015).
Under our assumptions, the bias-corrected local estimate of the gradient variance is σ2 =
Vt(βt)∕wt(βt) - [Ut(βt)∕wt(βt)]2. Taking the arg max for σ2, We find the βt that achieves the
worst-case (maximal) variance for each coordinate i:
βt,i = arg max σ2i=arg max vt,i(β) - [ut,i(β)]2∙	(6)
β,	β
We call our approach to finding adaptive running average coefficient βt Maximum Variation Aver-
aging (MaxVA). We plug MaxVA into ADAM and its variant LAPROP (Ziyin et al., 2020), which
results in two novel algorithms, MAdam and LaMAdam, listed in Algorithm 1 and Algorithm 2.
Different from ADAM, LAPROP uses Vt to normalize the gradients before taking the running aver-
age, which results in higher empirical stability under various hyperparameters. Note, we only use
the MaxVA formula for the second moment ut (βt) used for scaling the learning rate; mt is still
an exponential moving average (with a constant coefficient α) of the gradient for MADAM or the
normalized gradient for LaMAdam.
3
Under review as a conference paper at ICLR 2021
Finding βt via a Closed-form Solution. The maximization for βt in Eq. 6 is quadratic and has a
relatively simple closed-form solution that produces maximal σ2 for each coordinate:
β = ___________________(gt - Ut-I)2 + vt-1 - Ut-I_________________
t	Wt-ι((gt - Ut-l)2 - Vt-1 + U2-l) + (gt - Ut-l)2 + Vt-1 - U2-1 ,
(7)
where all variables are vectors and all the operations are elementwise, and we have abbreviated
ut-1(βt-1), vt-1(βt-1) and wt-1(βt-1) into ut-1, vt-1 and wt-1. We use this abbreviation in the
following sections, and defer the derivation of Eq. 7 to Appendix A.
Implementation Notes. We apply MaxVA in every step except for the first step, where the gradient
variance one can observe is zero. So for Algorithm 1 and Algorithm 2 we define:
u1 = (I - βI)g1, v1 = (I - βI)g2,w1 = 1 - β1.	⑻
The coefficient β1 for t = 1 is set to a constant that is the same as typical values for ADAM.
To obtain a valid running average, we clip βt so that β ≤ βt ≤ β, where the typical values are
β = 0.5,0.98 ≤ β ≤ 1. For convenience, We set βι = β by default. For t > 1, since 0 < βt ≤ 1,
Wt will monotonically increase from (1 - βι) to 1. Before clipping, for any gt, ut-ι ,vt-ι satisfying
vt-1 - ut2-1 > 0 in Eq. 7, we have βt ∈ [1/(1 +wt-1), 1/(1 - wt-1)]. As a result, the lower bound
that we use (β = 0.5) is tight and does not really change the value of βt, and as t → ∞, Wt → 1
and βt ∈ [0.5, ∞]. We have a special case at t = 2, where βt is a constant 1/(2 - βι).
In practice, we also add a small coefficient δ > 0 to the denominator of Eq. 7 to prevent division
by zero, which will have negligible effect on the value of βt and does not violate the maximum
variation objective (Eq. 6). All the derivations for these conclusions are deferred to Appendix C.
Effect of Maximum Variation Averaging. By definition, we have vt-1 - ut2-1 ≥ 0, but in most
cases vt-1 - ut2-1 > 0. If we define a new variable Rt = (gt - ut-1)2/(vt-1 - ut2-1), which
represents the degree of deviation of gradient gt from the current estimated average, we can rewrite:
βt
Rt + 1
(1 + Wt )Rt + 1 — Wt
(9)
From Eq. 9, we can see βt monotonically decreases from 1/(1 - Wt) to 1/(1 + Wt) as Rt increases
from 0 to ∞, and equals to 1 when Rt = 1. As a result, for each entry, if Rt ≤ 1, or the deviation
of the gradient gt from the current running mean ut-1 is within the estimated standard deviation
σ, we will use β to update Vt, which is the smallest change we allow for Vt. If gt deviates much
more than σ from ut, MaxVA will find a smaller βt and therefore a higher weight (1 - βt) on
gt2 to adapt to the change faster. For example, when an abnormally large gradient entry occurs in
some coordinate i, MaxVA will assign a smaller βt,i to obtain a larger vt,i and smaller adaptive
step size compared with Adam with β = β. This allows a quick response to impede abnormally
large gradients, which enables a better handling for the heavy-tailed distribution of gradients in the
process of training Transformers (Zhang et al., 2019b). As a side effect, Vt tends to be larger than
Adam/LAPROP using a constant β, but as we will show in the experiments, using a larger learning
rate counters such an effect and achieves better results.
On the other hand, when the variance decreases in the later phase of training, gt tends to be within σt,
and MaxVA tends to find the slowest rate for decreasing Vt. This allows large values of Vt to last for
a longer horizon even compared with setting βt to a constant β on the same sequence, since we have
assigned more mass to large gradients, which can be seen as an adaptive version of AMSGrad.
Note that MaxVA and AMSGrad can be complementary approaches if applied together, which we
have found helpful for Image Classification on CIFAR10/100.
Convergence Analysis. We prove the convergence of MaxVA in the nonconvex stochastic opti-
mization setting. For the sake of simplicity, we analyze the case where α = 0, which is effectively
applying MaxVA to RMSPROP. We leave the analysis for α 6= 0 for future research. We assume the
function ` is L-smooth in θ, i.e., there exists a constant L such that
kVθ'(χ; θι) - Vθ'(χ; θ2)k ≤ Lkθι - θ2k, for all θ1,θ2 ∈ Rd,χ ∈ X.	(10)
This automatically implies that f (θ) = E['(χ; θ)] is L-smooth. Such a smoothness assumption
holds for networks with smooth activation functions, e.g., Transformers that use the GELU activa-
tion (Hendrycks & Gimpel, 2016). We also need to assume function ` has bounded gradient, i.e.,
4
Under review as a conference paper at ICLR 2021
∣∣Vθ'(x; θ)k∞ ≤ G for all θ ∈ Rd, x ∈ X. As typically used in the analysis of stochastic first-
order methods (Zaheer et al., 2018; Ghadimi & Lan, 2013), we assume the stochastic gradient has
bounded variance: E[[Vθ'(x; θ)]i — [Vθf (θ)]i]2 ≤ D2 for all θ ∈ Rd. Further, we assume the batch
size increases with time as bt = t, which is also adopted in the analysis of SIGNSGD (Bernstein
et al., 2018), and holds in our large batch experiments. Theorem 1 gives a “worst-case” convergence
rate of MaxVA to a stationary point under these assumptions, where the dependence of βt on gt is
ignored and we only consider the worst-case of βt in each step. MADAM still converges under this
setting. The proof is given in Appendix B.
Theorem 1. Define w0 = 1. Let ηt = η and bt = t for all t ∈ [T]. Furthermore, we assume
2
3	β, β, η are chosen such that η ≤ 2L, 1 — β ≤ 帚炉，and β ≤ 2β. Thenfor θt generated using
MADAM, we have the following bound:
E∣Vf(θa)k2 ≤ O ("θl)ηTf(θ*)
(11)
where θ* is an optimal solution to minimize the objective in Eq. 1, and θa is an iterate uniformly
randomly chosen from {θ1, ..., θT }.
4	Experiments on Synthetic Data
For a better control of data distribution and demonstrate the efficacy of MaxVA with statistical
significance on a larger number of instances, we compare MAdam and the baselines in two sets
of synthetic data. The first dataset simulates prevalent machine learning settings where mini-batch
stochastic gradient methods are applied on a finite set of samples, on which we show MAdam fixes
the nonconvergence issue of Adam and achieves faster convergence rate than AMSGrad. The
second dataset evaluates the algorithms under different curvatures and gradient noise levels, where
we show MAdam achieves both lower loss and variance than fine-tuned Adam at convergence.
4.1	Convergence with Stochastic Gradients
Iterations
IO1	IO3	IO5
Iterations
Adam
AMSGrad
MAdam
Figure 1: Median and standard error (over 100 runs) of objective value (f (θ)), accumulated update size
(PT=1 l∣gt/√vt||2) and total change in adaptive learning rate (PT=1 ||房一√v= ∣∣ι) for Adam, AMS-
Grad, MAdam on the problem defined in Eq. 12.
Since MaxVA maximizes the variance and the gradient converges to zero in most cases, MAdam bi-
ases towards larger vt than ADAM but does not require vt to be monotonically increasing, which
is like an adaptive version of AMSGrad. To highlight the difference, we compare Adam,
MAdam and AMSGrad on the synthetic dataset from Chen et al. (2019) simulating training with
StoChastiC mini batches on a finite set of samples. Formally, let I[∙] be the indicator function. We
consider the problem minθ f(θ) = Pi1=1 1 `i(θ) where
I[i = 1]5.5θ2 +I[i 6= 1](-0.5θ2),
'i(θ) = 1 I[i = 1](11∣θ∣ - 5.5) + I[i = 1](-∣θ∣ + 0.5),
if ∣θ| ≤ 1；
otherwise.
(12)
At every step, a random index i is sampled uniformly from i ∈ [11], and the gradient V'i(θ) is used
by the optimizer. The only stationary point where Vf (θ) = 0 is θ = 0. We set α = 0, β = 0.9 for
Adam and AMSGrad. For MAdam, we set α = 0, (β, β) = (0.5,1). We select the best constant
learning rates for the three algorithms, see Appendix E for details.
5
Under review as a conference paper at ICLR 2021
We plot the median and standard error of the objective (f (θ)), accumulated update size (S1 =
PT=1 l∣gt/√Vt||2), and total change in adaptive step Size (S2 = PT=1 ||√1vt - √v= ∣∣ι) over 100
runs in Figure 12.The optimal learning rates for these optimziers are different, so for fair compar-
isons, we have ignored the constant learning rate in S1 and S2. From the curves of f(θ), we can see
Adam diverges, and MAdam converges faster than AM S Grad in the later stage. As shown by
the S2 curves, the adaptive step sizes of MADAM and AM S GRAD all converged to some constant
values after about 10 steps, but MADAM converges faster on both f(θ) and S1, indicating the adap-
tive step size found by MAdam fits the geometry of the problem better than AMSGrad. This also
shows S1 + S2 of MADAM has a smaller slope than AM S GRAD in the log-scale plots after 10 iter-
ations, leading to a faster theoretical convergence rate in the bound given by Chen et al. (2019). The
slightly larger variation in adaptive step sizes of MAdam at the beginning of training, shown by the
larger S2 values, demonstrates MADAM adapts faster to the changing gradients than AMSGRAD,
achieved by dynamically selecting β < 0.9.
4.2	Convergence in the Noisy Quadratic Model
Figure 2: Results on NQM. The left figure shows the mean and standard error of the loss under different
learning rates η, computed over 100 runs at each point. We select the best β for ADAM at each η . The best
results (mean and variance) of Adam and MAdam are 1.84e-3 (2.51e-4) and 4.05e-3 (4.84e-4) respectively.
Figure on the right gives a qualitative example of the trajectories of two approaches.
We analyze the ability of MAdam to adapt to curvature and gradient noise on the simple but illus-
trative Noisy Quadratic Model (NQM), which has been widely adopted for analyzing optimization
dynamics (Schaul et al., 2013; Wu et al., 2018; Zhang et al., 2019a;c). The loss function is defined
as f (θ) = Ex〜N(0,σ2I)，Ped=I hi(θi - Xi)2], where X is a noisy observation of the ground-truth
parameter θ* = 0, simulating the gradient noise in stochastic optimization, and hi represents the
curvature of the system in d dimensions. In each step, the following noisy gradient for each coor-
dinate i are put into the algorithms, from which we can see the gradient noise is proportional to the
curvature hi :
Vθi'(σei; θi) = hi@ - σ6i),6i 〜N(0,1).
(13)
To validate the effectiveness of MaxVA, we compare MAdam with Adam under a variety of dif-
ferent curvatures h and noise level σ on a 2D NQM (setting d = 2). For each setting of h and σ,
we test both algorithms on a variety of learning rates. For ADAM, we additionally choose the best β
and report the best results. See Appendix F for details. We run each setting 100 times to report the
mean and standard error. MAdam consistently achieves 30-40% lower average loss with smaller
standard error in all settings. Figure 2 shows the results for one of the settings, from which we find
the best result of MADAM is better than ADAM under any choice of β , confirming the advantage
of choosing an adaptive βt . From the qualitative example, MaxVA also demonstrates smaller vari-
ance near convergence, enabled by a quicker response to impede the noise with a smaller βt . More
experimental results under other settings are provided in Appendix F.
5	Experiments on Practical Datasets
In this section, we thoroughly evaluate MAdam and LaMAdam on a variety of tasks against
well-calibrated baselines: CIFAR10/100 and ImageNet for image classification, IWSLT’14 DE-
EN/WMT’16 EN-DE for neural machine translation, and the GLUE benchmark for natural language
6
Under review as a conference paper at ICLR 2021
Model	CIFAR-10	CIFAR-100			
			ImageNet 	 	 MPthCd Method	IWSLT'14DE-EN	WMT'16EN-DE
SGD Adam LaProp MAdam LaMAdam	95.44 (.04) 95.37 (.03) 95.34 (.03) 95.51 (.09) 95.38 (.11)	79.62 (.07) 78.77 (.07) 78.36 (.07) 79.32 (.08) 79.21(.11)	7∏ 1 Q				
			70.18 66.54	LAPROP 70 02	LAMADAM	35.98 (0.06) 36.09 (0.04)	27.02 27.11
			69.96	Thl	C 70.16	Table 2: LaMAdam	BLEU score of LaProp and for training transformers on ma- on datasets. We report the median and for IWSLT’14 over 5 runs.	
Table 1: Accuracies on CIFAR10/100 and Ima- chine translati geNet. CIFAR10/100 experiments are the median standard error (standard error) over 4 runs.					
understanding. In both computer vision and NLP tasks, after careful tunings, we find the decou-
pled weight decay (Loshchilov & Hutter, 2018) gives much better results for Adam, MAdam,
LaProp and LaMAdam. Therefore, we use this approach in all our experiments. Across all the
plots in this section, We define the average step size at time t as the average of ∣ηtmt/(√vt + E) | for
Adam/MADAM and ∣ηtmt∣ for LAPROP/LaMAdam over all the entries.
5.1	Image Classification
To evaluate the effectiveness of MaxVA for image classification, We compare With SGD, Adam and
LaProp in training ResNet18 (He et al., 2016) on CIFAR10, CIFAR100 and ImageNet. On all
the datasets, We perform a grid search for the learning rate and Weight decay, and report the best
results for each method in Table 1. For CIFAR10/100, We train ResNet18 With a batch size of 128
for 200 epochs. We also find AMSGrad (Reddi et al., 2018) improves the classification accuracy
of all adaptive methods evaluated on CIFAR10/100, so We apply AMSGrad in all experiments With
adaptive methods. On ImageNet, We use the implementation from torchvision and the default multi-
step learning rate schedule. We do not use AMSGrad in this case. Further details are in Appendix G.
Despite achieving a marginal improvement on CIFAR10, adaptive methods often underperforms
carefully tuned SGD on CIFAR100 and ImageNet When training popular architectures such as
ResNet, as confirmed by Wilson et al. (2017); Zhang et al. (2019c); Liu et al. (2020). Neverthe-
less, With the proposed MaxVA, We shrink the gap betWeen adaptive methods and carefully tuned
SGD on these image classification datasets, and achieve top-1 accuracy very close to SGD on Ima-
geNet. Note our results With ResNet18 is better than the recent AdaBelief’s results With ResNet34
on CIFAR10/CIFAR100 (95.51/79.32 vs. 95.30/77.30 approximately), as Well as AdaBelief With
ResNet18 on ImageNet (70.16 vs. 70.08) (Zhuang et al., 2020).
5.2	Neural Machine Translation
We train Transformers from scratch With LaProp and LaMAdam on IWSLT’14 German-to-
English (DE-EN) translation (Cettolo et al., 2014) and WMT’16 English-to-German (EN-DE) trans-
lation, based on the implementation of fairseq.1 We do not compare With SGD, since it is unstable for
Transformers (Zhang et al., 2019b). We also shoW in Appendix D that AdaBound cannot achieve
any good result Without degenerating into Adam. More implementation details are in Appendix H.
IWSLT’14 DE-EN has 160k training examples, on Which We use a smaller Transformer With 512-
dimensional Word embeddings and 1024 FFN dimensions. We train it for 60k iterations, during
Which each batch has up to 4096 tokens. Results are listed in Table 2. Note the baseline’s BLEU
score is already 1.22 higher than the best results reported in Liu et al. (2020) using the same model.
Altough LaMAdam is only marginally better than LaProp, LaMAdam uses much smaller update
size than LaProp, and it is not able for LaProp to achieve better results When We scale its learning
rate to get similar update sizes as LaMAdam, as shoWn in Appendix H.
WMT’16 EN-DE has 4.5M training examples, Where same as Ott et al. (2018), We use a larger
Transformer With 1024-dimensional Word embeddings and 4096 FFN dimensions. Each batch has
up to 480k tokens. We train for 32k iterations using the same inverse square root learning rate
schedule as VasWani et al. (2017). We evaluate the single model BLEU on neWstest2013, unlike Liu
et al. (2020) Where models in the last 20 epochs are averaged to get the results. As shoWn in Table 2,
LaMAdam also achieves better results.
1https://github.com/pytorch/fairseq
7
Under review as a conference paper at ICLR 2021
Method	MNLI (Acc)	QNLI (ACC)	QQP (ACC)	RTE (Acc)	SST-2 (Acc)	MRPC (Acc)	CoLA (MCC)	STS-B (Pearson)
Reported	87.6	92.8	91.9	78.7	94.8	90.2	63.6	91.2
ADAM	87.70 (.03)	92.85 (.06)	91.80 (.03)	79.25 (.71)	94.75 (.08)	88.50 (.24)	61.92(1.1)	91.17 (.13)
LAPROP	87.80 (.04)	92.85 (.13)	91.80 (.03)	78.00 (.46)	94.65 (.11)	89.20 (.20)	63.01 (.61)	91.17 (.06)
MADAM	87.90 (.08)	92.95 (.07)	91.85 (.03)	79.60 (.66)	94.85 (.12)	89.70 (.17)	63.33 (.60)	91.28 (.03)
LAMADAM	87.80 (.03)	93.05 (.05)	91.85 (.05)	80.15 (.64)	95.15(.15)	90.20 (.20)	63.84 (.85)	91.36 (.04)
Table 3: Results (median and variance) on the dev sets of GLUE based on finetuning the RoBERTa-base model
(Liu et al. (2019)), from 4 runs with the same hyperparameter but different random seeds.
Figure 3: Training loss, validation accuracy and step size of various optimization methods on SST-2. All
optimizers here use λ = 0.1. ADAM and LAPROP use (η, β)=(1e-5, 0.98), MADAM and LAMADAM use
(η,β, β)=(4e-5, 0.5, 0.98), ADAM-η0 and LAPROP-η0 use (η, β)=(1.6e-5, 0.98).
5.3	General Language Understanding Evaluation (GLUE)
To evaluate MaxVA for transfer learning, we fine-tune pre-trained RoBERTa-base model (Liu et al.,
2019) on 8 of the 9 tasks of the GLEU benchmark (Wang et al., 2018). Following prevalent vali-
dation settings (Devlin et al., 2019; Lan et al., 2020; Raffel et al., 2019), we report the median and
standard error for fine-tuning the RoBERTa-base model (Liu et al., 2019) over 4 runs where only the
random seeds are changed. The results are in Table 3. MAdam and LaMAdam give better scores
than the corresponding baselines in the 8 tasks. More experimental details are in Appendix I.
To highlight the difference of the optimizers, we compare the training loss, dev set accuracy and the
average step size on SST-2, as shown in Figure 3. Different from Machine Translation experiments
where we train the Transformers from scratch, the adaptive step size of MAdam/LaMAdam is
higher in this transfer learning setting. The ratio of the learning rate and step size of MaxVA to
non-MaxVA optimizers are 4 and 1.8 respectively on GLUE, while on IWSLT’14 the two ratios
are 2 and (approximately) 0.875. Because we start from a pre-trained model, the heavy tail of the
gradient is alleviated, just as the BERT model in the later stage of training as shown by Zhang et al.
(2019b), and the curvature of the loss landscape should be smaller. Therefore, MaxVA selects larger
adaptive step sizes for better convergence. Same as in the Machine Translation experiments, the
highest test accuracy of Adam/LaProp cannot reach the same value as MAdam/LaMAdam by
simply scaling the base learning rate η to reach similar step sizes as MADAM/LAMADAM.
5.4	Large-batch Pretraining for BERT
We use the NVIDIA BERT pretraining repository to perform large-batch pretraining for BERT-Base
model on the Wikipedia Corpus only.2 Each run takes about 52 hours on 8 V100 GPUs. Training is
divided into two phases: the first phase uses a batch size of 64K with input sequence length 128 for
7,038 steps; the second phase uses a batch size 32K with input sequence length 512 for 1563 steps.
The total of steps is significantly smaller than the 1,000,000 steps used in the small-batch training
of Devlin et al. (2019). Therefore, a faster adaptation to curvature in each step is more important.
This point is validated by the faster convergence of MAdam in both phases, as shown in the training
loss curves in Figure 4. Contrary to the observation by You et al. (2020), Adam even converges
2https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT.
Note the results from the repository are for BERT-Large trained with additional data from BookCorpus. We
did not include BookCorpus due to the difficulty of obtaining the data.
8
Under review as a conference paper at ICLR 2021
faster than Lamb in the earlier iterations. You et al. (2020) only explored weight decay of up
to 0.01 for Adam, but we find using larger weight decay of 0.1 together with gradient clipping
(kgt k2 ≤ 1, same as LAMB) stabilizes ADAM in this large-batch setting. So we use a weight decay
of 0.1 and the same gradient clipping for both MAdam and Adam. For MAdam and Adam, we
do a grid search on the learning rate of phase 1 while keeping the ratios of learning rate in phase 1
and phase 2 to the same as Lamb. We use β = 0.999, β = 0.5 for MAdam. For Lamb, We use the
default setting from the aforementioned repository.
The faster adaptation of MaxVA improves the stability, Which enables MAdam to use a much
larger learning rate to achieve faster convergence than Adam. The best learning rate for MAdam is
3.4e-3. We tried learning rates in {7e-4, 8e-4, 9e-4, 1e-3} for ADAM, and find it alWays diverges
When the learning rate is higher or equal to 9e-4. The best result of Adam is achieved With learn-
ing rate 8e-4. MAdam achieves a training loss of 1.492, While Lamb achieves a training loss of
1.507, and Adam has the Worst training loss 1.568. The test scores of the models pretrained With
MADAM/LAMB/ADAM are 88.53/87.60/88.07 (F1) and 82.10/81.40/80.78 (Accuracy) on SQuAD
v1.1 and MNLI, respectively.
6	Related Work
Various adaptive methods have been proposed and broadly applied in deep learning (Kingma & Ba,
2015; Duchi et al., 2011; Tieleman & Hinton, 2012; Zeiler, 2012). Reddi et al. (2018) proposed to
compute the adaptive learning rate With the coordinate-Wise maximum value of the running squared
gradient so that the adaptive learning rate does not increase. AdaBound (Luo et al., 2019) clips
the adaptive learning rate of Adam With a decreasing upper bound and an increasing loWer bound.
Lookahead (Zhang et al., 2019c) computes Weight updates by looking ahead at the sequence of “fast
Weights” generated by another optimizer. Padam (Chen et al., 2018) improves the generalization of
adaptive methods by choosing a proper exponent for the vt of AMSGRAD. LAPROP (Ziyin et al.,
2020) uses local running estimation of the variance to normalize the gradients, resulting in higher
empirical stability. You et al. (2017) proposes Layer-Wise Adaptive Rate Scaling (LARS), and scales
the batch size to 16,384 for training ResNet50. LAMB (You et al., 2020) applies a similar layer-Wise
learning rate on Adam to improve LARS on training BERT. Starting from a similar motivation
of adapting to the curvature, the recent Work AdaBelief (Zhuang et al., 2020) directly estimates
the exponential running average of the gradient deviation to compute the adaptive step sizes. Our
approach finds the averaging coefficients βt automatically by maximizing the estimated variance
for a faster adaptation to the curvature, Which could be complementary to all the aforementioned
methods, and is the first to explore in this direction to our knoWledge.
7	Conclusion
In this paper, We present Maximum Variation Averaging (MaxVA), a novel adaptive learning rate
scheme that replaces the exponential running average of squared gradient With an adaptive Weighted
mean. In each step, MaxVA chooses the Weight βt for each coordinate, such that the esimated gra-
dient variance is maximized. This enables MaxVA to: (1) take smaller steps When large curvatures
or abnormal gradients are present, Which leads to more desirable convergence behaviors in face of
noisy gradients; (2) adapt faster to the geometry of the objective, achieving faster convergence in
the large-batch setting. We illustrate hoW our method improves convergence by a better adaptation
to variance, and demonstrate strong empirical results on a Wide range of tasks. We prove MaxVA
converges in the nonconvex stochastic optimization setting under mild assumptions.
9
Under review as a conference paper at ICLR 2021
References
Eneko Agirre, Llu’is M‘arquez, and Richard Wicentowski (eds.). Proceedings of the Fourth In-
ternational Workshop on Semantic Evaluations (SemEval-2007). Association for Computational
Linguistics, 2007.
Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic
gradients. In ICML,pp. 404-413, 2018.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The
fifth PASCAL recognizing textual entailment challenge. In TAC, 2009.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
Signsgd: Compressed optimisation for non-convex problems. In ICML, pp. 560-569, 2018.
MaUro Cettolo, Jan Niehues, Sebastian Stuker, LUiSa Bentivogli, and Marcello Federico. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In IWSLT, volume 57, 2014.
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, and Quanquan Gu. Closing the gener-
alization gap of adaptive gradient methods in training deep neural networks. arXiv preprint
arXiv:1806.06763, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type
algorithms for non-convex optimization. ICLR, 2019.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object clas-
sification, and recognising tectual entailment. Springer, 2006.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL, pp. 4171-4186, 2019.
William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
In Proceedings of the International Workshop on Paraphrasing, 2005.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, pp. 2121-2159, 2011.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing
textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment
and paraphrasing, 2007.
R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415, 2016.
Shankar Iyer, Nikhil Dandekar, and Kornl Csernai. First quora dataset release:
Question pairs, 2017.	URL https://www.quora.com/q/quoradata/
First- Quora- Dataset- Release- Question- Pairs.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), ICLR, 2015. URL http://arxiv.org/abs/1412.6980.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. ICLR, 2020.
10
Under review as a conference paper at ICLR 2021
Hector J Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In
AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, 2011.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. ICLR, 2020.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv:1907.11692, 2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. ICLR, 2019.
Jerry Ma and Denis Yarats. On the adequacy of untuned warmup for adaptive optimization.
arXiv:1910.04209, 2019.
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.
In WMT,,pp.1-9, 2018.
Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and
Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.
Interspeech, pp. 2613-2617, 20l9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv:1910.10683, 2019.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In EMNLP, 2016.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. ICLR,
2018.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In ICML, pp. 343-351,
2013.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In EMNLP, 2013.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. EMNLP, pp.
353, 2018.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments.
arXiv preprint 1805.12471, 2018.
Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In NAACL, 2018.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Neurips, pp. 4148-4158, 2017.
11
Under review as a conference paper at ICLR 2021
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in
stochastic meta-optimization. arXiv:1803.02021, 2018.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training.
CoRR, abs/1708.03888, 2017.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. In ICLR, 2020.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive meth-
ods for nonconvex optimization. In NeurIPS, pp. 9793-9803, 2018.
Matthew D. Zeiler. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701, 2012.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. In NeurIPS, pp. 8194-8205, 2019a.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv
Kumar, and Suvrit Sra. Why adam beats sgd for attention models. arXiv:1912.03194, 2019b.
Michael R. Zhang, James Lucas, Jimmy Ba, and Geoffrey E. Hinton. Lookahead optimizer: k steps
forward, 1 step back. In NeurIPS, pp. 9593-9604, 2019c.
Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced
adversarial training for natural language understanding. In ICLR, 2020.
Juntang Zhuang, Tommy Tang, Sekhar Tatikonda, Nicha Dvornek, Yifan Ding, Xenophon Pa-
pademetris, and James S. Duncan. Adabelief optimizer: Adapting stepsizes by the belief in
observed gradients. In NeurIPS, 2020.
Liu Ziyin, Zhikang T Wang, and Masahito Ueda. Laprop: a better way to combine momentum with
adaptive gradient. arXiv:2002.04839, 2020.
12
Under review as a conference paper at ICLR 2021
Adaptive Learning Rates with Maximum Variation Averaging
(Appendix)
A Deriving the closed form solution Eq.7
Plugging Eq. 3,4,5, and the unbiased estimations ut(β) = Ut(β)∕wt(β), vt(β) = Vt(β)/wt(β) into
Eq. 6, each coordinate is solving the same problem:
arg max f(β)
β
βwt-iVt-i + (1 - β)g2
βwt-1 + (I - β)
βwt-ιut-ι + (I - β)gt 2
βwt-1 + (I - β)	_
(14)
—
Let Y = 1∕[βwt-ι + (1 - β)] ∈ [1,1∕wt-ι], We can see f (β) can be represented as a quadratic
function of γ. Specifically,
f (β ) = h(Y) = w≡≡2+
gt2 -
wt-1vt-1 - g2
Wt-1 - 1
wt-1ut-1 - gt
YT---------i-
wt-1 - 1
wt-1ut-1 - gt
gt-----------;--
wt-1 - 1
+
Y
2
MeanWhile, β is a monotonic function of Y. Therefore, f(β) has a unique maximum value.
To find the maximum value, We return to Eq. 14, from Which We can find a stationary point
vt-1 - u2-l + (gt - ut-1)2
wt-1	(gt	-	ut-1)2	-	vt-1 +	ut2-1 +	vt-1 -	ut2-1 + (gt	-	ut-1)2
(15)
B Convergence Proof
FolloWing the convergence proofs of Yogi (Zaheer et al., 2018), We prove the convergence of
MAdam in the nonconvex setting.
Proof of Theorem 1.
Proof. Recall that We have assumed the update steps of MADAM as
“θt,i-ηt √vg⅛1,	(16)
for all i ∈ [d], and that f is L-smooth, Which results in the folloWing inequalities:
f (θt+1) ≤ f (θt) +(▽〃％), θt+1 - θti + ɪ kθt+1 - θtk2
d	L 2 d	g2	(17)
=f R)-小 X " √⅛±+Lt X E可.
13
Under review as a conference paper at ICLR 2021
Note the Stochastistic gradient is defined as gt = * P：=i Ne'(叼；θt). Given θt, we take expecta-
tion over the stochastic gradient gt in Eq. 17 (denoted as Et[∙] = E[∙∣θt]) to get
d /
Et[f(θt+ι)] ≤f (θt) - ηtf [V/(θt)]i × Et
i=1、
=f (θt) - ηt E ([Vf (θt)]i × Et
i=1 ∖
gt,i
—
gt,i
J βt,iVt-i,i + e
+
gt,i
√βt,iVt-i,i + e
))
M + e
d
+ 甘 EEt
d
g2,i
=f (θt) - ηtf	[Vf (θt)]i ×	Et
i=1
d
+ 半 EEt
i=1
d
≤f(θt)-ηt E
i=1
g2,i
[Vf(θt)]i
Jβ°t-i,i + e
σG
e√t
gt,i
+ Et
d
+ ηt E[Vf (θt)]iEt
i=1
gt,i
gt,i
gt,i
)))
gt,i
g2,i
^Z
Ti
(18)
where the second equality holds by applying Lemma 2 to the first expectation term, and taking the
absolute value of the second expectation term.
Next, we need to bound the term T11 to show convergence. First, we have the following upper bound
for Ti:
gt,i	gt,i
----------..—. ---------------
√vt,i + C	P βt,ivt-1,i + C
≤∣gt,i∣ ×
1	1
---------------,	----
√vj + C	√βt,iVt-i,i + C
.	∣gt,i ∣	vt,i - βt,ivt-1,i
(VV7 + C)( V βt,ivt-1,i + C)	√v7 + V βt,ivt-1,i
=∣gt,i∣X (I - βt,i)gt,i
(VV7 + C)( P βt,ivt-1,i + C)	√v7 + P βt,ivt-1,i
(19)
where the last equality comes from the definition of %,i = βt,i%-ι,i + (1 - βt,i)gt,i∙ We can further
bound T1 as
______________gt,i______________ χ 	(I - βt,i)∣gt,i∣_______________________________
(√vti + c)( P βt,ivt-1,i + C)___Jβt,iVt-ι,i + (1 - βt,i)gt,i + pβt,i%-ι,i
_________gM__________ χ (1 - βt,i)∣gt,i∣
(pβt,ivt-1,i + C)C	J(1 - βt,i)gt,i
P1 - βt,igt,i	≤ P1 - βgt,i
(pβt,iVt-i,i + c)c — (Pβvt-i,i + C)C
(20)
d
+ H E Et
—
—
—
|
,
14
Under review as a conference paper at ICLR 2021
Since the loss on each sample S satisfies ∣[V'(χ, s)]i∣ ≤ G, We will have |[Vf (x)]i| ≤ G for
∀i ∈ [d]. Substituting the coefficients of T1 in Eq. 18 with this gradient bound, we have
Et[f(θt+1)]
≤f(%)-ηtX JVf≡L + ηG^Ξβ X Et
i=1 Jβvt-1,i + C	i	i=1
σηdG
+ F
g2,i
Pevt-1,i + C
≤f(θtiX JVf≡L + ηGp1≡ X Et
i=1 Jβvt-1,i + C	i	i=1
+T X Et ⅛‰ ]+%
g2,i
pβvt-1,i + C
≤f(θt)+X (-	/_ηt
i=1	( βvt-1,i + C)
IntG a/i—β +	Ln
C(Pevt-1,i + C)	2c( pβvt-1,i + C)
[Vf(θt)]i2
-- /
‰z
T2
ntGpi — β Lnit! XX	σ2	σndG
C + 2c J -1 bt(PeVt-ι,i + C)+ C√t
(21)
where the second inequality comes from the fact that √Vξi + C ≥ C and vt,i = βt,iVt-ι,i + (1 一
βt,i)gt,i ≥ βvt-ι,i, and the third inequality comes from applying Lemma 1 by Zaheer et al. (2018)
to Et[gt2]. The application of Lemma 1 is possible because vt-1 is independent of the t-th batch. By
the assumptions for c, G, β, we have
Ln < n nG 1-βl <i
W ≤ 4，一C— ≤ 4n
(22)
Plugging these two results and the assumption β ≤ 2β into Tt, we have
T2 ≤
≤
n +	n
qβvt-j+C	2(PeVM+C)
n+	n
√2( Pβvt-1,i + c)	2(PeVt-1,i + c)
(23)
the main inequality, we have
Et[f(θt+1)] ≤f(θt) —
n
5( PeVt-1,i + c)
nX	[Vf(θt)]t
5 i=l Pevt- 1,i + C
+
d
y / E、
W t ∙√evt-ι,i+c)
σt
σndG
C√t
≤
≤f (θt) — 5G⅛ιy kvf(θt)k2 +
σ2 d σndG
T + 1√T,
(24)
where we have replaced bt with t by our assumption on the batch size, and the second inequality
comes from the fact that Vt-1,i ≤ G2 . Taking expectation on both the LHS and RHS for the
inequalities at t = i, ..., T, using telescope sum and rearranging the terms, we can conclude that
5(G 4 + e) XkVf(θt)k2 ≤ f(θι)-E[f(θτ +ι)]+ (nGP1≡ + T! σ2dlog(T +1)+2^√T.
5(GVe+c) i=ι	∖ C	2c )	C
(25)
15
Under review as a conference paper at ICLR 2021
Multiplying both sides with "GT^+'), and using the fact that f (x*) ≤ f (θt+ι), We conclude that
ɪ X 皿(θt)k2 ≤ 5(Gqβ+e) (fθJ + (Gp1≡ + 斗 σ⅛T÷2) + 2√g).
i=1	(26)
□
Lemma 2. Assume the gradient is bounded as ∣∣Vθ'(x; θ)k∞ ≤ G, and has bounded variance
E[[Vθ'(x; θ)]i — Vf (θt)]i]2 ≤ σ2, and the batch size b = t. For the t-th iteration of MADAM, we
have
— [Vf(θt)]iEt
gt,i
√βt,i vt-i,i + e
Vf(θt)]2	+ Gσ
qβvt-i,i + e	e√t
(27)
Proof. The LHS can be decomposed as
LHS = — [Vf(θt)]iEt	/Vf (θt)]i
βt,ivt-1,i +
≤ —-[vf四]2	— Vf(θt)]iEt
ʌ/βvt-1,i + E J
— [Vf(θt)]iEt
gt2i-[vf^t)i
√βt,iVt-i,i + e
gt2i-[vf(θt)i
pβt,ivt-1,i + E ，
L -	/
{z^^^^^^^^^^^^^^^^^
T3
(28)
where the inequality comes from taking the upper bound of βt,i, since the first term is non-positive.
Let [h(x)]+ and [h(x)]- be the operators for taking the positive and negative values of function h(x)
respectively, i.e.,
[h(x)]+ =	0h,(x),
if h(x) > 0
otherwise
[h(x)]- =	0h,(x),
if h(x) < 0
otherwise
(29)
It is obvious that E[[X]+] ≤ E[|X|] ≤ dE[X2], where the second inequality comes from Cauchy-
Schwarz inequality. Similarly, E[[X]-] ≥ —E[|X|] ≥ -PEX2]. With this in mind, we have
0 ≤ Et [[gt,i — [Vf (θt)]i]+] ≤ JEt [gt,i — Vf (θt)]i]2 ≤ √	(30)
where the last inequality comes from applying Lemma 1 from Zaheer et al. (2018) under the bounded
gradient variance assumption, and the assumption that the batch size grows as bt = t. Similarly, we
—√t ≤ Et [[gt,i — [Vf (θt)]i]-i ≤ 0.	(31)
Now we will decompose and bound T3 as
T3
—Et	Vf(θt)]i [gt,i- vf(θt)]i]+
βt,ivt-1,i + E
—Et	Vf(θt)]i [gt,i- vf(θt)]i]二
βt,ivt-1,i + E
Vf (θt )]i [gt√vXM-
[Vf (θt )]i [gt√7 1θ%+
if[Vf(θt)]i>0
otherwise
(32)
σ∣[Vf(θt)]i∣
(pgvt-1,i + E) ʌ/t
σG
Plugging this inequality back into Eq. 28 and we will get the RHS.
□
16
Under review as a conference paper at ICLR 2021
C PRACTICAL NOTES OF βt
Claims and arguments:
1.	For t > 1, since 0 < βt ≤ 1, wt will monotonically increase from (1 - β1) to 1.
This is obvious since in every step, wt is an interpolation between wt-1 and 1, and wt ≥
wt-1. We have also set w1 = 1 - β1.
2.	For any gt, ut-1 , vt-1 satisfying vt-1 - ut2-1 > 0 in Eq. 7, we have βt ∈ [1/(1 +
wt-1), 1/(1 - wt-1)].
Eq. 9 is monotonic in Rt .Since gt can be any value, Rt can be any value from 0 to ∞. If
Rt = 0, βt takes the largest value 1/(1 - wt). If Rt → ∞, βt → 1/(wt + 1).
3.	As t → ∞, wt → 1 and βt ∈ [0.5, ∞].
Combining Claims 1 and 2 to get this result.
4.	Adding a small coefficient δ > 0 to the denominator of Eq. 7 has negligibale effect on the
value of βt and does not violate the maximum variation objective (Eq. 6).
Since δ is small, it has negligible effect on βt when division by zero does not happen. We
only need to confirm adding δ will not affect the solution when division by zero happens.
We can re-write the dividend of Eq. 7 as
(wt-1 + 1)(gt - ut-1)2 + (1 - wt-1)(vt-1 - ut2-1).	(33)
Since E[X2] - (E[X])2 = Var[X] ≥ 0, we can conclude that vt-1 - ut2-1 ≥ 0.
When 1 - β1 ≤ wt-1 < 1, Eq. 33 can be 0 only when gt = ut-1 and vt-1 = ut2-1.
In this special case, We can set βt to any value in [0,1] without changing σ2; We will
always have Vt = vt-1∕wt-1 = vt-ι, ut = Ut-ι∕wt-ι = ut-ι, and σ2 = 0. Only
wt = (wt-1 - 1)βt + 1 is affected by βt, which takes a larger value when βt is smaller.
The solution given by adding δ to the denominator is βt = 0, and the following clipping
will set βt = β resulting in the largest possible Wt = (wt-ι - 1)β + 1. In the next step,
if Eq. 33 is not zero, then we have βt+1 = 1/(wt + 1), and we know gt+1 6= ut.3 In this
case, for 0.5 ≤ βt+ι < 1, σ2+ι increases as βt+ι decreases, so setting Wt to its maximum
will achieve the maximum variance at the next step. Otherwise if Eq. 33 is zero, doing this
will not change σ2+ι = 0.
When Wt-1 = 1, Eq. 33 is 0 if and only if gt = ut-1. As a result, if vt-1 = ut2-1, we
have the same conclusion as before. Otherwise, βt = (vt-ι - u2-J∕δ before clipping,
and βt = β after clipping. Also, any 0 < βt < 1 will not change the value of Ut =
βtUt-1 + (1 - βt)g^ = Ut-1. Since g2 = Ut-i < vt-ι, to maximize σ2 = vt(β) - u2-i,
we should set βt = β so that vt(β) takes the maximum value, which is consistent with the
solution after adding δ to the denominator.
D	AdaB ound might fail on Transformers ?
Since SGD often performs much worse than Adam on transformers, and AdaBound transitions
into SGD asymptotically, it is reasonable to believe that AdaBound would not converge well on
transformers. We did experiments on the IWSLT’14 dataset to evaluate AdaBound on Transform-
ers. AdaBound clips the effective step size to be within 0.1 - γ+ and 0.1 - 0γt1, and recommends
setting γ = 1 - β2 = 10-3. In practice, this setting only gives a < 24 test BLEU on IWSLT’14. To
explore the full potential of AdaBound, we tried γ ∈ {10-4, 10-5, 10-6, 10-7, 10-8}, and found
γ = 10-8 to give the best BLEU 35.99 (0.04). However, as shown in Figure 5, AdaBound does
not effectively clip most of the coordinates even in the last iteration with γ = 10-8, which means
AdaBound essentially degraded into Adam, yet it gives better results than those effectively doing
clipping. By comparison, the best result of MAdam and Adam with AMSGrad is 36.07(0.07) / 35.87
(0.05), respectively.
3Otherwise we will still have gt+1 = ut , gt2+1 = ut2 = vt and Eq. 33 is 0.
17
Under review as a conference paper at ICLR 2021
Figure 5: Distribution of effective step size of AdaBound and MAdam at iteration 10000, 30000 and 60000
on IWSLT’14. Red lines indicate the clipping range of AdaBound. On the top/bottom are results of Ad-
aBound/MAdam with learning rates 5e-4/1.25e-3.
E	Experimental Details on the Synthetic Finite-sample
Experiment
Same as Chen et al. (2019), we use constant learning rates η in every step, and set α = 0, β = 0.9 for
ADAM and AMSGrad. For MAdam, we set α = 0, (β, β) = (0.5,1). ADAM never converged for
a variety ofη we tried within [10-4, 1], consistent with Chen et al. (2019). Generally, a larger η gives
faster convergence for both AMSGrad and MAdam. For reproducibility, we repeat experiments
100 times with the same settings, and choose the η for AMSGRAD and MADAM where the solution
∣θ*∣ < 0.1 every time. η = 1.2 satisfies this requirement for MAdam, but AMSGrad only
satisfied it 1% of the times for η = 1.2 and 65% of the times for η = 0.9. η = 0.8 is the largest η we
find for AMSGRAD to achieve 100% satisfaction. Therefore, we use η = 0.8 for both ADAM and
AMSGRAD.
F Details and Additional Experimental Results on the Noisy
Quadratic Model
Figure 6: More results on the Noisy Quadratic Model.
Details of experimental settings We set β = 0.5, β = 0.99 for MAdam, and for fair comparison,
we do a grid search for ADAM with β ∈ [0.5, 0.6, 0.7, 0.8, 0.9, 0.99], and only report the results with
the best β . We repeat the experiments 100 times under each setting, where we select a random ini-
18
Under review as a conference paper at ICLR 2021
Figure 7: Training loss, test accuracy and average step size on CIFAR10.
tialization of θ 〜N(0, I) each time, and run MADAM and ADAM With different hyper-parameters
from this random initialization. Each run takes 1000 iterations by default.
Additional Results We give more results comparing ADAM and MADAM on the Noisy Quadratic
Model. The results are shoWn in Figure 6. Generally, the best result of MAdam has a more signif-
icant margin When h2 and σ are higher, i.e., the improvement is more significant When the problem
is Worse conditioned and the noise level is higher. Note that for each trial, We start both algorithms
from the same random initialization.
G Additional Experimental Results and Details on Image
Classification.
Additional Experimental Results In Figure 7, We plot the training loss, test accuracy and average
step size on CIFAR10. We see that LAMADAM and MADAM produce slightly better curves than
others.
Model, larning rate schedules and data augmentations On CIFAR10 and CIFAR100, the
ResNet18 comes from a public repository,4 Which has a base Width of 64 by default. We use random
cropping (4-pixel zero paddings on each side) and random horizontal flip as the data augmentations.
Instead of using the multi-step schedule, We find the cosine learning rate schedule to yield better
results for both SGD and adaptive methods. Therefore, We use the cosine learning rate schedule
and set a final learning rate of 2e-6 in all cases. On ImageNet, We use random resized crop and
random horizontal flip for data augmentation. For the multi-step learning rate schedule, multiply the
learning rate by 0.1 every 30 epochs, and train a total of 90 epochs, With a batch size of 256.
Hyperparameters of CIFAR10 For each optimizer, We do a grid search over the learning rate
and Weight decay for the best hyperparameters. For ADAM and LAPROP, We set β = 0.999. For
MAdam and LaMAdam, We set β = 0.5 and β = 0.999 in all cases. Except for SGD, We tried
learning rates from {5e-4, 1e-3, 2e-3, 3e-3, 4e-3, 6e-3, 8e-3} and weight decay from {0.025, 0.05,
0.1, 0.2, 0.4, 0.8, 1 }. The best learning rate and Weight decay for ADAM, LAPROP, MADAM and
LAMADAM are (3e-3, 0.2), (1e-3, 0.4), (8e-3, 0.05) and (6e-3, 0.05) respectively. As to SGD,
We tried learning rates from {3e-2, 5e-2, 1e-1, 2e-1, 3e-1} and Weight decays from {1e-4, 3e-4,
5e-4, 1e-3, 2e-3}, and the best result Was achieved With learning rate 2e-1 and Weight decay 3e-4.
These hyperparameters that gave the best results are also the hyperparameters We used for plotting
Figure 7.
Hyperparameters for CIFAR100 We use the same grid search configurations as for CIFAR10.
The best learning rate and Weight decay for Adam, LaProp, MAdam and LaMAdam are (2e-3,
0.4), (5e-4, 1), (4e-3, 0.2) and (3e-3, 0.2) respectively. For SGD, the best learning rate and Weight
decay are 3e-2 and 2e-3 respectively.
Hyperparameters for ImageNet Due to the heavy Workload and the time limit, We Were not able
to accomplish 4 runs for each hyperparameter in ImageNet, so We report the best results for each
4https://github.com/kuangliu/pytorch-cifar
19
Under review as a conference paper at ICLR 2021
Figure 8: Training loss, validation BLEU and average step size on IWSLT’14 DE-EN, trained with η=5e-
4, λ=1e-2, β=0.999 for LAPROP and η=1.5e-3, λ=1e-2, β=0.5, /3=0.999 for LaMAdam, and η=4.375e-4,
λ=1e-2, e=0.999 for LAPROPR.	一
optimizer in Table 5.1, except for the result of Adam , which was copied from Liu et al. (2020)
but uses the same hyperparameters except for the learning rate and weight decay. For LaProp,
MADAM and LAMADAM, we choose learning rates from {1e-3, 2e-3, 3e-3, 4e-3, 5e-3, 6e-3, 8e-3}
and weight decay from {0.003, 0.006, 0.01, 0.012, 0.02, 0.03}, and found the best combinations for
LAPROP, MADAM and LAMADAM are (2e-3, 0.03), (5e-3, 0.012) and (6e-3, 0.012). For SGD, we
choose learning rate from {0.05, 0.1, 0.2} and weight decay from {5e-5, 7e-5, 1e-4}, and found the
best combination to be (0.1, 7e-5).
H Additional Experimental Results and Details on Machine
Translation
Additional Experimental Results and analysis In Figure 8, we plot the training loss, valida-
tion BLEU and average step size on IWSLT’14 DE-EN. Although the average update size of
LaMAdam is smaller even when using 3 times higher learning rate than Adam, LaMAdam shows
slightly better convergence on the training set and better validation BLEU. This may be ex-
plained by the heavy-tailed distribution of the gradient in the process of training transformers from
scratch Zhang et al. (2019b). Smaller step sizes mitigate the effect of extreme gradient values on
the model’s performance. It is worth mentioning that LaProp diverges using the large learning rate
1.5e-3. Further, we find LaProp is unable to produce the same result as LaMAdam even when
their update sizes are similar. LaProp produces a similar step size curve as LaMAdam with learn-
ing rate 4.375e-4, but the performance is weaker than LaMAdam. LaMAdam uses the maximum
variation rule to select the adaptive learning rate for each dimension, creating benefit that is not
achievable by simply scaling the base learning rate η.
Hyperparameters for IWSLT’14 The transformer we use has 512-dimensional word embeddings
and 6 Transformer blocks with 4 attention heads and 1024 FFN dimensions for the encoder/decoder,
which is refered to as transformer_iwslt_de_en in fairseq. We do a grid search for the learn-
ing rate and weight decay for both optimizers. We tried η from {2.5e-4, 5e-4, 1e-3, 1.5e-3, 2e-
3}, and weight decay from {0.0001, 0.001, 0.01, 0.1}. The best combinations for LAPROP and
LaMAdam are (5e-4, 0.01) and (1.5e-3, 0.01). To demonstrate the full potential of adaptive meth-
ods under constant learning rates, we use the tri-stage learning rate schedule (Park et al., 2019),
linearly increase the learning rate from 0.01η to the full learning rate η in 4k iterations, hold it at
η for 32k iterations, and exponentially decay it to 0.01η in 24k iterations. For LAPROP, we tried
β from {0.98, 0.99, 0.997, 0.999}. We found 0.999 to work the best and used it for all the grid
search experiments. For LaMAdam, We set β = 0.5, β = 0.999. For other hyperparameters,
We use the default setting in the fairseq example, which sets dropout probability to 0.3, uses label
smoothed cross entropy loss with a smoothing coefficient 0.1, and shares the input and output token
embedding parameters.
Hyperparameters for WMT’16 The Transformer we use has 1024-dimensional word embed-
dings, 6 transformer blocks with 16 attention heads and 4096 FFN dimensions for the en-
coder/decoder, and is refered to as transformer_vaswani_wmt_en_de_big in fairseq. The
default implementation from fairseq did not use weight decay, so we also ignore weight decay in
all experiments. The learning rate schedule takes the first 4k steps to linearly increase the learning
20
Under review as a conference paper at ICLR 2021
rate to its maximum value. For LAPROP, we found β = 0.98 to give the best results, and we set
β = 0.95, β = 0.98 in all experiments. This takes around 8 hours on 16 V100 GPUs each run.
For grid search, We tried η from {5e-4, 1e-3, 1.5e-3, 2e-3}, and found 1e-3 and 1.5e-3 to work
the best for LaProp and LaMAdam respectively. Other hyperparameters are the defaults of the
corresponding fairseq example, which uses a dropout probability of 0.3, the label smoothed cross
entropy loss with a smoothing coefficient 0.1, and shares all embedding parameters.
I Additional Details of Experiments on the GLUE benchmark
The GLUE benchmark is a collection of 9 natural language understanding tasks, namely Corpus of
Linguistic Acceptability (CoLA; Warstadt et al. (2018)), Stanford Sentiment Treebank (SST; Socher
et al. (2013)), Microsoft Research Paraphrase Corpus (MRPC; Dolan & Brockett (2005)), Semantic
Textual Similarity Benchmark (STS; Agirre et al. (2007)), Quora Question Pairs (QQP; Iyer et al.
(2017)), Multi-Genre NLI (MNLI; Williams et al. (2018)), Question NLI (QNLI; Rajpurkar et al.
(2016)), Recognizing Textual Entailment (RTE; Dagan et al. (2006); Haim et al. (2006); Giampic-
colo et al. (2007); Bentivogli et al. (2009)) and Winograd NLI (WNLI; Levesque et al. (2011)).
It is reported in Liu et al. (2019) that ADAM is sensitive to the choice of on GLUE. Following
their settings, we set = 1e - 6 for both ADAM and MADAM. For LAPROP and LAMADAM,
however, we always set = 1e - 15, like all other experiments in this paper, which is consistent
with the observation in Ziyin et al. (2020) that LAPROP is robust to the choice of . We set β =
0.98 for ADAM and LAPROP, and β = 0.5, β = 0.98 for LAPROP and LaMAdam. All other
hyperparameters are set to the same as the example in fairseq.5 For each task, we do a grid search
over the learning rate and weight decay, which are chosen from {5e-6, 1e-5, 2e-5, 4e-5, 5e-5, 6e-
5} and {0.025, 0.05, 0.1, 0.2} respectively. We list the best combinations for ADAM, MADAM,
LaProp and LaMAdam on each task as below:
MNLI: (1e-5, 0.1), (1e-5, 0.1), (4e-5, 0.025), (4e-5, 0.025).
QQP: (1e-5, 0.1), (1e-5, 0.1), (4e-5, 0.025), (4e-5, 0.025).
QNLI: (1e-5, 0.1), (1e-5, 0.1), (4e-5, 0.05), (4e-5, 0.05).
SST-2: (1e-5, 0.1), (1e-5, 0.1), (4e-5, 0.1), (4e-5, 0.1).
RTE: (2e-5, 0.1), (2e-5, 0.1), (6e-5, 0.1), (6e-5, 0.1).
MRPC: (1e-5, 0.1), (1e-5, 0.1), (6e-5, 0.1), (6e-5, 0.1).
STS-B: (2e-5, 0.1), (2e-5, 0.1), (4e-5, 0.5), (4e-5, 0.5).
CoLA: (2e-5, 0.1), (2e-5, 0.1), (6e-5, 0.5), (6e-5, 0.5).
5https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md
21