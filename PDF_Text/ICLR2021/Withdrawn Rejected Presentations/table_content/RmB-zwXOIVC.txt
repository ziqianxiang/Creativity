Table 1: Comparison between different families of distribution matching IL algorithmsIL method	Learned Models	Relation between -divergence and optimized objective	Objective TypeSupport	policy πθ, support estimator f	Neither Upper nor Lower Bound	maxAdversarial	policy πθ , discriminator D	Tight Upper Bound	min maxNDI (ours)	policy πθ , critic f, density qφ	Loose Lower Bound	max max4	Trade-offs between Distribution Matching IL algorithmsAdversarial Imitation Learning (AIL) methods find a policy that maximizes an upperbound to theadditive inverse of an f -divergence between the expert and imitator occupancies (Ghasemipour et al.,2019; Ke et al., 2020). For example, if the f -divergence is reverse KL, then for any D : S×A→R,max -DKL(P∏θ∣∣P∏e) ≤ max log (E∏e[eD(s,a)]) - E∏g[D(s,a)]πθ	πθwhere the bound is tight at D(s, a) = log ：(；：) + C for any constant C. AIL alternates between,min log(E∏E [eD(S,a)]) - E∏θ [D(s,a)],max -Eπ [D(s, a)]πθThe discriminator update step in AIL minimizes the upper bound with respect to D, tightening theestimate of reverse KL, and the policy update step maximizes the tightened bound. We thus see thatby using an upper bound, AIL innevitably ends up with alternating min-max optimization wherepolicy and discriminator updates act in opposing directions. The key issue with such adversarialoptimization lies not in coordinate descent itself, but in its application to a min-max objective which
Table 2: Task Performance when provided with one demonstration. NDI (orange rows) outperformsall baselines on all tasks. See Appendix C.2 for results with varying demonstrations.
Table 3: Effect of varying MI reward weight λf on (1). Task performance of NDI-EBM(top row) and (2). Imitation performance of NDI-EBM (bottom row) measured as the averageKL divergence between π, πE on states s sampled by running π in the true environment, i.eEs〜∏ [Dkl(∏(∙∣s)∣∣∏e(∙∣s))], normalized by the average DKL between the random and expert poli-cies. Dkl(∏∣∣∏e) can be computed analytically since ∏, ∏e are conditional gaussians. Densitymodel qφ is trained with one demonstration. Setting λf too large hurts task performance while settingit too small is suboptimal for matching the expert occupancy. A middle point of λf = 0.005 achievesa balance between the two metrics.
