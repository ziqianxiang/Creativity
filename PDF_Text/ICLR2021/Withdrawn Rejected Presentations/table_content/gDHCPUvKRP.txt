Table 1: Data and the corresponding architectures used in the fast matrix multiplication using but-terfly matrices experiments.
Table 2: Data used in the truncated butterfly auto-encoder reconstruction experimentsFigure 2: Approximation error on data matrix with various methods for various values of k . Left:Gaussian 1 data, Right: MNIST data6.3	Two-phase Learning for Encoder-Decoder Butterfly NetworkThis experiment is similar to the experiment in Section 6.2 but the training in this case is done in twophases. In the first phase, B is fixed and the network is trained to determine an optimal D and E . Inthe second phase, the optimal D and E determined in phase one are used as the initialization, and the7Under review as a conference paper at ICLR 2021network is trained over D, E and B to minimize the loss. Theorem 1 ensures worst-case guaranteesfor this two phase training (see below the theorem). Figure 3 reports the approximation error of animage from Imagenet. The red and green lines in Figure 3 correspond to the approximation error atthe end of phase one and two respectively.
Table 3: Data used in the Sketching algorithm for low-rank matrix decomPosition exPeriments.
Table 4: Test error for different ` and kpreceding layer using at most 2' weights. By induction, for all i ≥ 0, the set of nodes S(p-i) ⊆V (p-i) is of size at most 2i ∙', and is connected to the set S(P-iT) ⊆ V (Pi-I) using at most 2i+1∙'weights.
