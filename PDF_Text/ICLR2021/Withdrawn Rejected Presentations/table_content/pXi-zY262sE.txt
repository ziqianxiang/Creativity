Table 1: The performance of TextCNNbase classifiers and the standard deviation with GraVeR ap-plied to various pretrained word vectors.
Table 2: The performance of TextCNNbase classifiers and the standard deviation with different reg-ularization techniques. We can observe that GraVeR positively matches with the classifier and withother regularization techniques. Bold indicates the largest performance gain.
Table 3: List of top-20 nearest words of a cue word (love) in initial embedding (Initial), afterfine-tuned once (FineTuned), and our method GraVeR in DBpedia dataset. The differences betweeninitial embedding and the other methods are marked in underlined. The differences between fine-tuned once and GraVeR are marked in bold. GraVeR further changes the distribution of word vectors.
Table 4: The performance of TextCNNtune and the standard deviation with GraVeR according to therandom noises and bias. B indicates our default setting noise scale 1, which means the randomvalues sampled from uniform distribution the range between -1 and 1.
Table 5: The performance of TextCNN and the standard deviation with GraVeR according to themasking rate, which determines how much the random maskers move to the next frequently usedwords. B indicates our default Setting that masks 20% of VocabUlary at once.________	DBpedia	Yah(Up)	Yah(LoW)	AGNews	Yelp	IMDBMaskRate 0.1	98.78±.06	74.86±.20	52.45±.21	91.06±.20	64.06±.22	88.50±.34BMaskRate 0.2	98.73±.07	75.09±.26	52.80±.38	91.40±.25	63.94±.17	88.77±.36MaskRate 0.5	98.60±.02	73.06±.33	49.87±.42	91.02±.28	63.60±.14	88.44±.20MaskRate 1.0	98.65±.02	70.89±.28	47.17±1.31	90.99±.18	63.75±.02	87.83±.34Masking Rate The amount of noise added by GraVeR is an important factor in that some noisesshould be small enough to be corrected during the re-training processes, while other noises shouldbe large enough to change the word vector distribution. We first change the masking rate of howmuch random maskers move in every re-training process. The larger the masking rate becomes,the more words are masked in a re-training process, so the noise increases. Conversely, the amountof noise decreases as the masking rate becomes small. The effect of the masking rate is presentedin Table 5. Masking Rate=0.1 also shows good performance, but it needs 2x re-training processesmore than Masking Rate=0.2. We thus use 0.2 as a default.
Table 6: The performance of TextCNN classifiers and the standard deviation with GraVeR accord-ing to the gradualness policy. The default method is to increase the number of maskers when thevalidation performance decreases.
Table 7: The performance of Transformer-based classifier and the standard deviation.
Table 8: The performance of TextCNNtune classifiers and the standard deviation with pretrainedword vectors trained from only training data and comparison with pretrained word vector trainedfrom external resources._____________________________________________________________	DBpedia	Yah(Up)	Yah(LoW)	AGNews	Yelp	IMDBGraVeR(SP)	98.73±.07	75.09±.26	52.80±.38	91.40±.25	63.94±.17	88.77±.36w2v (Gen)	98.61±.03	70.45±1.02	46.74±.66	91.27±.23	62.73±.23	85.07±1.54w2v (Sp)	+0.08(.04)	+3.72(.43)	+4.25(.51)	-0.32(.34)	+0.52(.33)	-0.02(1.18)glv (Gen)	98.79±.07	75.77±.41	52.83±.57	9L37±1.10	63.59±.32	87.67±.60glv (SP)	-0.54(.34)	-2.47(1.01)	-4.72(1.84)	-2.41(.77)	-0.59(.43)	-5.19(1.09)ftt (Gen)	98.43±.13	64.56±.31	40.91 ±.44	89.43±.82	62.75±.11	79.16±1.02ftt (SP)	+0.17(.11)	+9.46(.56)	+9.52(.58)	+ 1.49(.14)	+0.41(.23)	+4.57(1.53)BERT (Gen)	99.20±.01	81.75±.41	65.83±.37	93.58±.23	69.25±.15	93.24±.35BERT (Sp)	98.57±.13	77.19±.32	58.11±.58	90.35±.21	60.99±.22	87.17±.346.4 Pretraining by Training Data.
