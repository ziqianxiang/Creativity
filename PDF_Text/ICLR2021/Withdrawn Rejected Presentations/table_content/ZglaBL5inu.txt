Table 1: Test error rates of hyperbolic geometry related MNIST classifiersOntrup & Ritter (2005)	Grattarola et al. (2019) Khrulkov et al. (2020) This paper5.4%	42%	1%	0.35%7Under review as a conference paper at ICLR 20216.2	POINCAR宣 EMBEDDING SUBTREE CLASSIFICATIONGiven a Poincare embedding (Nickel & Kiela, 2017) PE : {WordNet noun} → HD of 82114nouns and given a node x ∈ {WordNet noun}, the task is to classify all other nodes as beingpart of the subtree rooted at x (Ganea et al., 2018a). Our model is logistic regression, wherethe horocycle feature p ∈ {WordNet noun} 7→ hPE(x) (P E (p)/s) (s is a hyperparameter lying in[1, 1.5]) is the only predictor, and the dependent variable is whether p is in the subtree rooted atx. The decision hypersurface of this model is a horocycle, as illustrated in Figure 3 (left). In theexperiment, we pre-train three different Poincare embeddings3 in each of H2, H3, H5, H10. For eachx ∈ {animal, group, location, mammal, worker} and D ∈ {2, 3, 5, 10}, we randomly select one ofthree pre-trained Poincare embedding PE : {WordNet noun} → HD and then test the model.
Table 2: Average test F1 classification scores (%) for five subtrees of WordNet noun treeRootNode	Model	H2	H3	H5	H10animal.n.01	Ganea et al. (2018a)	47.43±1.07	91.92±0.61	98.07±0.55	99.26±0.594016 nodes	Shimizu et al. (2020)	60.69±4.05	67.88±1.18	86.26±4.66	99.15±0.46	This paper	94.32±5.33	86.98±2.05	98.57±4.98	98.76±2.43group.n.01	Ganea et al. (2018a)	81.72±0.17	89.87±2.73	87.89±0.80	91.91±3.078376 nodes	Shimizu et al. (2020)	74.27±1.50	63.90±6.46	84.36±1.79	85.60±2.75	This paper	90.08±12.41	90.91±11.62	97.52±1.15	97.85±1.92location.n.01	Shimizu et al. (2020)	42.60±2.69	66.70±2.67	78.18±5.96	92.34±1.843362 nodes	This paper	93.19±11.76	94.66±4.65	95.23±4.11	97.37±1.75mamal.n.01	Ganea et al. (2018a)	32.01±17.14^^	87.54±4.55	87.73±3.22	91.37±6.091181 nodes	Shimizu et al. (2020)	63.48±3.76	94.98±3.87	99.30±0.30	99.17±1.55	This paper	98.74±1.93	96.37±2.42	93.34±8.86	95.80±1.62worker.n.01	Ganea et al. (2018a)	12.68±0.82	24.09±1.49	55.46±5.49	66.83±11.381115 nodes	This paper	94.47±4.30	92.53±6.36	95.47±2.71	94.75±2.186.3	End-based clustering for 2D dimension reductionIn this experiment, we use the horocycle MLR (Section 4.2) to construct end-based clusteringsNNθ : RD → H2 for MNIST, Fashion-MNIST(Xiao et al., 2017), and CIFAR-10(Krizhevsky, 2012).
Table 3: Test error rates on 2D embedded MNIST by dimensionality reduction techniquesGerDA CentroidenCoder dG-MCML dt-MCML CPL GCPL This paper3.2%	261%	213%	203%	072%^^067%^^0.63%6.4 Poisson MLRUsing a Poisson MLR whose feature desCriptor is a ResNet-32 struCture, we obtain a Classifierwith a test error rate of 6.46% on CIFAR-10. It is on par with other methods with similar networkstruCtures (Yang et al., 2018). Moreover, we apply Poisson MLR to the ClassifiCation task offlowers (Tensorflow), whiCh is a typiCal example of overfitting. ReplaCing the MLR part of theKeras model (Tensorflow) with a Poisson MLR, the new Poisson model shows better generalizationperformanCe (Figure 6). A.17 and Code Contain the details. This subseCtion provides evidenCe forfurther appliCations of horoCyCles.
