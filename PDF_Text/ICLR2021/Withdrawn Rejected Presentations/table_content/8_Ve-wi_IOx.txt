Table 1: Average over 10 runs of classification result using three architectures, one layer scattering followed bya linear classifier (Linear Scattering), one layer scattering followed by a two layer neural network (NonlinearScattering) and a two layer scattering with joint (2D) convolution for the second layer followed by a linearclassifier (Linear Joint Scattering). For each architecture and dataset, we experiment with the baseline, Morletwavelet fitler-bank (morlet), and learnable frameworks being ours (lwvd), learnable sinc based filters (sinc) andlearnable Morlet wavelet (lmorlet). As can be seen, across the dataset, architectures and learning rates, theproposed method provides significant performance gains (stds in Tab. 2).
Table 2: Std over 10 runs of classification result using three architectures, one layer scattering followed bya linear classifier (Linear Scattering), one layer scattering followed by a two layer neural network (NonlinearScattering) and a two layer scattering with joint (2D) convolution for the second layer followed by a linearclassifier (Linear Joint Scattering). For each architecture and dataset, we experiment with the baseline, Morletwavelet fitler-bank (morlet), and learnable frameworks being ours (lwvd), learnable sinc based filters (sinc) andlearnable Morlet wavelet (lmorlet). As can be seen, across the dataset, architectures and learning rates, theproposed method provides significant performance gains.
Table 3: Various hand picked K-transform parameters leading to known time invariant TFRs with their re-spective parameters. For the adaptive versions, such as the wavelet tree with various Gabor mother waveletparameters Ïƒ0 , then at each time t, the corresponding parameter is any of the optimal one based on the desiredcriterion.
