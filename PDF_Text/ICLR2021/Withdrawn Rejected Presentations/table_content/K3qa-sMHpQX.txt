Table 1: Comparison of ForceNet to existing GNN models. We mark as bold the best performanceand close ones, i.e., within 0.0005 MAE, which according to our preliminary experiments, is a goodthreshold to meaningfullly distinguish model performance. The final row represents the relative errorreduction of our best model (ForceNet-large) compared to the best existing model (GNS).
Table 2: Ablations on the archi- tecture of ForceNet. Avg Force Ablation		Table 3: The effect of model width D, depth K on ForceNet. All models are trained with a default batch size of 256, except the last row that uses 512.							MAE (eV/ )				V<αIidttfirkn Tr八A TVIAI7 MX7∕ΔA			ForceNet (1)	Only-dist (2)	No-atomic-radii (3)	No-node-emb (4)	Only-Fc (5)	Edge-linear-BN (6)	Node-linear-BN	0.0358 0.0698 0.0368 0.0412 0.0378 0.0427 0.0357	Width	Depth	ID	OOD Ads.	OOD Cat.	OOD Both	Average		512 768 512 768 768	5 5 7 7 7	0.0314 0.0302 0.0304 0.0296 0.0281	^^0.0348^^ 0.0338 0.0342 0.0335 0.0316	0.0336 0.0330 0.0330 0.0325 0.0318	0.0433 0.0427 0.0429 0.0422 0.0410	0.0358 0.0349 0.0351 0.0345 0.0331parameters) by 0.0112 eV/A (23.9% relative improvement). We also trained a shallower (depth of 3)variant of ForceNet with 7.1M parameters for comparison against SchNet (7.4M parameters). Theaverage MAE is reduced by 0.0109 eV/A (21.5% relative improvement) over SchNet.
Table 4: Percentage of relaxations that are successful, as measured by the Average Force belowThreshold (AFbT) and Average Distance within Threshold (ADwT) for the same models as in Table1. The higher, the better. The final row represents the relative performance improvement of our bestmodel compared to the best existing models.
Table 5: Ablations on basis and activation functions in the ForceNet architecture.
Table 6: Ablations on the message passing architecture of ForceNet.
Table 7: The effect of rotation augmentation and different weighting coefficients on fixed atoms whiletraining ForceNet.
