Table 1: WERs (%) on concatenated speech command dataset with various number of concatenated words.
Table 2: WERs (%) of offline speech recognition models without LM on LibriSpeech (with number ofparameters when available).
Table 3: WERs (%) of online speech recognition models without LM on LibriSpeech (with number of			parameters when available).		test-clean	test-otherCTC	Gated ConvNet (LiPtchinsky et al., 2017)	6.7	-variants	CTC (Chan et al., 2020)	4.6	13.0	ImPUter(DP)(Chan et al., 2020)	4.0	11.1	JasPer (333M)(Li et al., 2019)	3.86	11.95Transducers	Transformer-T (45.7M)(Yeh et al., 2019)~~	6.47	15.79	ConvT-T (67M)(HUang et al., 2020)	3.5	8.3	Transformer-T (139M) (Zhang et al., 2020)	3.0	7.7Streamable	Triggered Attention (Moritz et al., 2019)	7.4	19.2Seq2Seq	Mocha (Kim et al., 2019)	6.30	18.41	Mocha (Lee et al., 2020)	5.15	16.45	DecGRC (Lee et al., 2020)	5.87	17.04	CIF (Dong & Xu, 2020)	3.96	11.19	SAGMM-tr (121M) (single-head)	3.73	9.92	SAGMM-tr (121M) (multi-head)	3.52	9.29Table 4: Experimental results of (a) SAGMM-tr with fixed window width and (b) long-form speech recog-nition on “test-long” set. (sh) and (mh) denote single-head and multi-head attention, respectively.
Table 4: Experimental results of (a) SAGMM-tr with fixed window width and (b) long-form speech recog-nition on “test-long” set. (sh) and (mh) denote single-head and multi-head attention, respectively.
Table 5: BLEU scores on newstest2013 set forWMT EN-DE environment.
Table 6: The word error rate (%) of ablation experiments for SAGMM-tr attention mechanism in Lib-riSpeech.
Table 7: The example results for attention-based models in concatenated speech command dataset.
Table 8: The example results for attention-based models in test-clean-long set from LibriSpeech dataset.
