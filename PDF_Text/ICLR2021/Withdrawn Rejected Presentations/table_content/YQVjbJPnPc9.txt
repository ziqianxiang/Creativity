Table 1: Performance for natural language processing tasksModel	Avg	CoLA	SST-2	MRPC	STS-B	QQP	MNLI-m/-mm	QNLI	RTE	WNLIBERT-Base	77.4	51.7	93.5	87.2/82.1	86.7/85.4	91.1/89.0	84.3/83.7	90.4	67.2	65.1PA-BERT-Base	81.5	59.8	93.7	88.9/90.8	89.3/89.2	91.4/88.3	84.8/85.2	92.0	68.6	65.1BERT-Large	80.5	60.5	94.9	89.3/85.4	87.6/86.5	92.1/89.3	86.8/85.9	92.7	70.1	65.1PA-BERT-Large	83.0	63.1	95.4	90.4/88.9	88.9/88.0	92.4/89.9	87.7/86.2	93.5	70.9	65.1RoBERTa-Large	83.1	63.8	96.3	91.0/89.4	72.9/90.2	92.7/90.1	89.5/89.7	94.2	84.2	65.1PA-RoBERTa-Large	83.9	65.4	96.5	91.8/90.6	73.6/90.3	93.0/90.1	90.3/89.7	95.0	85.2	65.1T5-Base	83.2	51.1	95.2	90.7/87.5	89.4/88.6	72.6/89.4	87.1/86.2	93.7	80.1	-PA-T5-Base	84.5	53.3	92.8	92.4/89.5	89.6/89.1	89.4/91.5	86.0/85.1	92.9	80.7	-T5-Base (Dev)	83.5	53.1	92.2	92.0/88.7	89.1/88.9	88.2/91.2	84.7/85.0	91.7	76.9	-Synthesizer-T5-Base (Dev)	84.1	53.3	92.2	91.2/87.7	89.3/88.9	88.6/91.4	85.0/84.6	92.3	81.2	-PA-T5-Base (Dev)	84.4	53.6	92.4	92.2/89.1	89.7/89.1	89.7/91.7	85.4/84.9	92.5	80.7	-Table 2: Comparison of different model backbones on GLUE benchmark.
Table 2: Comparison of different model backbones on GLUE benchmark.
Table 3: Accuracy on CIFAR image classification datasetsModel	#Params	#GFLOPs	Top-1	Top-5ResNet-34	21.8M	7.4	73.79	91.43AA-ResNet-34	20.7M	7.1	74.33	91.92PA-AA-ResNet-34	20.7M	7.9	74.90	92.20ResNet-50	25.6M	8.2	75.99	93.00AA-ResNet-50	25.8M	8.3	77.15	93.52PA-AA-ResNet-50	25.8M	8.7	77.55	93.81ResNet-101	44.5M	15.6	77.40	93.65AA-ResNet-101	45.4M	16.1	78.31	94.16PA-AA-ResNet-101	45.4M	17.2	78.49	94.23Table 4: Accuracy on ImageNet dataseta global average pooling layer and feed the flattened representation to the classifier. Note that thememory is exhaustive for the original image size (32 × 32). Thus, we only leverage self-attentionafter the image size is reduced to 8 × 8. We set α = 0.01 empirically.
Table 4: Accuracy on ImageNet dataseta global average pooling layer and feed the flattened representation to the classifier. Note that thememory is exhaustive for the original image size (32 × 32). Thus, we only leverage self-attentionafter the image size is reduced to 8 × 8. We set α = 0.01 empirically.
Table 5: Analysis of PA-Transformer	Table 6: Analysis of AA-ResNet-34Transformer, Transformer with RC, and PA-Transformers consisting of different convolutional lay-ers are reported in Table 5. We can see that the residual connection is beneficial, and we generallyachieve strong performances on all datasets through one convolutional layer. Similar trends are ob-served in the ImageNet experiments, where we analyze the effectiveness of residual connections andconvolution-based prediction modules in Attention Augmented ResNet-34 architecture. As shownin Figure 6, the best performance is obtained with one predictive convolutional layer.
Table 7: Detailed hyper-parameter settings for GLUE fine-tuning.
