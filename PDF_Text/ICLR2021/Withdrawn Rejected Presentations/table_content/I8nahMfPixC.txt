Table 1: Performances on Fashion-MNIST (top), CIFAR-10 (middle) and SVHN (bottom).
Table 2: Performances of defenses against spatially-constrained attacks. We retrain our defense withthe adversarial examples generated from untargeted ST attack and targeted PGD attack, and obtainthe ADD-Defense-S.
Table 3: The summary of attacks: PGD (Madry et al., 2017), C&W (Carlini & Wagner, 2017),BA (Brendel et al., 2017), LS (Narodytska & Kasiviswanathan, 2016), DDN (Rony et al., 2019),RP2 (Eykholt et al., 2018), ST (Xiao et al., 2018) and SFW (Wu et al., 2020). The subscPIRt “u”indicates untargeted attack and the subscPIRt “t” indicates targeted attack.
Table 4: The architecture of our defense			E	G	DP	DIConv(128, 4, 2, 1)	Deconv(1024, 4, 2,1)	FC(1024)	Conv(32, 4, 2,1)LeakyReLU	LeakyReLU	LeakyReLU	LeakyReLUConv(256, 4, 2, 1)	Deconv(512, 4, 2, 1)	FC(256)	Conv(64, 4, 2, 1)LeakyReLU	LeakyReLU	LeakyReLU	LeakyReLUConv(512, 4, 2, 1)	Deconv(256, 4, 2, 1)	FC(64)	Conv(128, 4, 2, 1)LeakyReLU	LeakyReLU	LeakyReLU	LeakyReLUConv(1024, 4, 2, 1) LeakyReLU Conv(2048, 4, 2, 1)	Deconv(128,4, 2, 1) LeakyReLU Deconv(3, 4, 2, 1)	FC(3)	Conv(256, 4, 2, 1) LeakyReLU FC(512) LeakyReLU FC(1)D Defense against pixel-constrained perturbationFig 6 shows the change of distributions on MNIST dataset by using t-distributed stochastic neighborembedding (t-SNE) (Maaten & Hinton, 2008). The malicious perturbations generated from PGDattack modify the distribution of original examples, and our defense has largely rectified the mod-ification. The positive parameters are set as λ1 = e-4, λ2 = e0 , λ3 = e-3 , θ = e3 on MNIST,Fashion-MNIST and SVHN, and λ1 = e-6,λ2 = e-1,λ3 = e-3,θ = e1 on CIFAR-10.
Table 5: Performance of defenses against AutoAttack, including CROWN (Zhang et al., 2019), IBP(Gowal et al., 2018), Fast (Wong et al., 2020), Unlabled (Carmon et al., 2019) and HYDRA (Sehwaget al., 2020).
Table 6: Results of binary classification based on Lid. The attack for training is L2 - C&Wattack,the ”adv” indicates the identification results of the binary classifier based on Lid for the adversarialexamples. The ”def” indicates the identification results of the binary classifier based on Lid for therestored examples	Dataset	Data_ROC-AUC_Precision	Recall ~adv	0.9901	0.9787	0.8378 MNIST	def	0.5775	0.4098	0.0126 adv	0.9859	0.9268	0.9398 CIFAR	def	0.5874	0.5331	0.0848G.3 examples with non-malicious perturbationThe previous experiments are to eliminate perturbation and generate original example similar tooriginal examples. Our defense focuses on removing perturbations in latent space rather than justgenerating original examples. Moreover, each adversarial example is calculated by the attacker oneach single example, and an example with perturbation does not mean that it can interfere the targetmodel. We call the perturbation that cannot attack the target model as non-malicious perturbation.
Table 7: Accuracy of the target model to the examples with non-malicious perturbations. The targetimages on first row is the adversarial examples generated by untargeted C&W attack. The accuracyof reconstructed images (”-rec”) and restored images (”-def”) against untargeted C&W attack ispresented. Similarly, the accuracy against DDN attack is presented on the second row.
