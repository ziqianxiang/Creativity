Table 1: Results on a series of other environments and data settings from the D4RL benchmark (Fu et al.,2020a). Results are normalized to the range [0, 100], per the D4RL normalization scheme. For each method, foreach environment and data setting the results of the best hyperparameter setting are reported. The last columnindicates the best value of N in EMaQ amongst the considered hyperparameters (for the larger antmazedomains, we do not report this value since no value of N obtains nonzero returns). All the domains below theblue double-line are effectively unsolved by all methods. We have technical difficulties in evaluating BEAR onthe kitchen domains. This manuscript will be updated upon obtaining these results. Additional details can befound in Appendix G.3.
Table 2: Hyperparameters for Mujoco Experimentssize of Lini , number of bins for action discretization). Increasing the number of discretization binsfrom 40 (value for standard Mujoco experiments) to 80 was the most important change. Outputdimension of state-embedding MLP is the same as the hidden size.
Table 3: Hyperparameters for Table 1 Experiments21Under review as a conference paper at ICLR 2021RandomMediumMixed〃(a ⑸ 5 10 25 50 100 200 400	〃(a ⑸ 5 10 25 50 100 200 400	〃向S) 5 10 25 50 100 200 400	〃向S) 5 10 25 50 100 200 400EMaQ	EMaQ	EMaQ	EMaQFigure 6: Results for evaluating EMaQ on D4RL (Fu et al., 2020b) benchmark domains when using thedescribed VAE implementation, with N ∈ {5,10, 25, 50,100, 200, 400}. Values above μ(a∣s) represent theresult of evaluating the base behavior policies. Horizontal green lines represent the reported performance ofBEAR in the D4RL benchmark (apples to apples comparisons in Figure 7).
Table 4: Comparison of agent returns throughout training, under the variety of hyperparameters andand random seeds, in the small ant domains. We observe that EMaQ is significantly more stablethan BCQ in these domains, even though the values of N in EMaQ were fairly large for these plotsN ∈ {50, 100, 150, 200}.
