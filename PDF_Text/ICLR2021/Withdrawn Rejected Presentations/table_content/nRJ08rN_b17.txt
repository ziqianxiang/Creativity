Table 1: The two-pathway model outperforms others on diverse noise perturbations. FineNet-onlywith nfd = 0, 1, 3, refer to FineNet without feedback connection, with 1 loop of feedback inter-action, and with 3 loops of feedback interaction, respectively. The models of two-pathway-FFLand two-pathway-SFL refer to two different ways of fusing the features of FineNet and CoarseNet.
Table 2: CoarseNet facilitates the performance of FineNet. The images are taken from CIFAR-100, which form 20 super-classes and 100 sub-classes (for details, see Appendix A). CoarseNetand FineNet perform super- and sub- class classifications, respectively. The experiment settings thesame as in Tab. 1In the above, we have considered that the two pathways process the same categorical informationof images. In reality, the two pathways may process different levels of categorical information ofimages, and object recognition goes from rough to fine, for instance, CoarseNet may recognize thehigher category of an object (e.g., animal), and FineNet recognizes the lower category of the object(e.g., cat). In such a case, the result of CoarseNet can still serve as a cognitive bias to facilitate theperformance of FineNet. We carry out experiments to confirm this.
Table 3: The number of samples in each class of Pascalvoc-mask. Digits in each column meanthe training/testing numbers. The number of classes is 20 and the total number of training exam-ples/testing examples is 4512/375.
Table 4: The number of trainable parameters in different models. FineNet has more trainable pa-rameters than CoarseNet. Networks with FFL and SFL have more trainable parameters than ourtwo-pathway model.
Table 5: Performances of different models under different noise levels. CoarseNet in these modelstakes grayed and low-pass filtered inputs with std = 0, 1, 2, 3. FineNet takes clean inputs. Differentfrom FFL, FFL* consists of two FineNets. Mean and std are obtained by averaging over 4 trials.
