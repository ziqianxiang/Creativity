Table 1: Model performance on clean and perturbed inputs	Model	Atest(%)	Arand(%)	Aadv(%)	Capital ($)	Size (%)F	Linear	34.66	34.39	16.66	26, 808	0.4	MLP	36.56	33.12	7.22	24, 055	0.4	LSTM	36.40	37.26	21.76	10, 340	< 0.1	Model	Atest(%)	Arand(%)	Aadv(%)	Capital ($)	Size (%)GE	Linear	36.41	35.03	19.53	34, 513	0.6	MLP	36.87	33.65	9.87	23, 853	0.4	LSTM	35.65	33.01	20.59	12, 580	< 0.1	Model	Atest(%)	Arand(%)	Aadv(%)	Capital ($)	Size (%)NVDA	Linear	34.15	37.47	28.98	53, 805	2.5	MLP	35.69	34.39	27.60	49, 556	2.4	LSTM	37.11	35.88	35.56	5, 327	< 0.1levels are typically discarded by analysts as they are unlikely to be executed). Equivalently, weconsider perturbations to the size entries, but only to whole numbers, in raw order book data.
Table 2: Model performance on universal attacks (Ford data)Model	Ulinear		UMLP		Fooled	Size	Fooled	SizeLinear	9.5%	1.0%	11.90%	0.8%MLP	23.75%	1.1%	36.10%	0.8%LSTM	31.21%	0.9%	40.46%	0.8%6	Patterns in adversarial ordersWe observe patterns in adversarial attacks that can help explain the behavior of classifiers. Thenon-universal attacks on particular inputs highlight the vulnerability of the valuation models. Forexample, in Figure 3 in Appendix A.4, the perturbation is so small compared to the size on the bookthat a human would not distinguish the attacked signal from the clean one. The attack, however, isconcentrated on the fringes of the order book, much like a spoofing attack. Figure 1 shows a casein which a universal adversary has learned an interpretable perturbation in which it creates a localminimum in the size-weighted average by alternately placing perturbations on opposite sides of thebook. The perturbation on the left was computed without constraint, and when transferred to test dataand a different model, it fooled the victim on 157 inputs (out of 341 correctly classified) and accountsfor a relative size on the book of 3.8%. The perturbation on the right is the result of the same processwith an added penalty on relative size. The penalized attack shows a sparser perturbation with relativesize 0.9% while causing misclassification of almost the same number of inputs (123).
Table 3: Accuracy of each model on test data with confidence intervals of radius one standard error.
