Table 1: Top-1 accuracy on ImageNet of different BNN models with commonly used trainingstrategies, based on the ResNet-18 architecture (He et al., 2016). XIX denotes the correspondingstrategy is / is not utilized. KD denotes knowledge distillation, MS means using multiple trainingsteps, GA indicates applying gradient approximation in the backward pass. Note that BinaryDuo(Kim et al., 2020b) improved GA with a coupled ternary model. Scaling represents using explicitscaling factors to reweight the output of binary convolutions. N/A+n means only the number ofepochs n in the final step for multi-step training is given. Here Bi-Real Net was trained using twodifferent implementations. â†‘ indicates the double skip connections proposed by Liu et al. (2018)were used. It can be seen that FTBNN achieves promising accuracy even with a naive trainingscheme and can be further improved when combining the training strategies.
Table 2: Binary convolutions in FTBNN baseline based on ResNet-18 structure. BConvx-y indicatesyth block in the xth stage, where the size and number of filters are also given. The learnedcoefficients of FPReLU after each block are listed in the form of mean [minimum maximum].
Table 3: Ablation study for FTBNN baseline based on the ResNet 18 architecture.
Table 4: Left: Experimental results of the derived structures with different group and budgetconfigurations, with Top-1 accuracy (in %) recorded, where the widths of networks are adjustedto meet the budget requirements. For a clearer illustration, we also give the number of parametersfor each model: Params = number of floating point paramters + number of binary parameters / 32.
