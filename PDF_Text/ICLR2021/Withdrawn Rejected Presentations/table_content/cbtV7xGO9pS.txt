Table 1: TEAC HyperparametersParameter	Valueoptimizer learning rate for actor and critic learning rate for a learning rate for β discount (Y) replay buffer size number of hidden layers (all networks) number of hidden units per layer number of samples per minibatch target entropy (η) max divergence for KL (τ) nonlinearity target smoothing coefficient K target update interval gradient steps	Adam (Kingma & Ba, 2015) 1∙10-3 1∙10-4 1∙10-3 0.99 106 2 256 100 -dim(A))(e.g., -6 for HaIfCheetah-V3) 0.005 ReLU 0.005 1 	1	E The best performance of our model00.5	1-0	1-5	2.0	2.5	3.0times tsps	比68000E 60002000(c) Humanoid-v3HalfCheetah-v3	(b) Hopper-v3(d) Walker2d-v3(e) Swimmer-v3Figure 4: Performance comparisons on six MuJoCo tasks. Notice that the blue line is the perfor-mance of our model which setting different τ with respect to different tasks. In this figure, weset τ = 0.5 for HalfCheetah-v3, τ = 0.05 for Ant-v3, τ = 0.1 for Hopper-v3, τ = 0.001 forHumanoid-v3, τ = 0.005 for Swimmer-v3, and τ = 0.1 for Walker2d-v3.
