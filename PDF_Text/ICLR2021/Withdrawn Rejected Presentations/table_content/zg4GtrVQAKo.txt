Table 1: Neural network architectures used for training for the synthetic Gaussian dataset. FC standsfor fully connected layer. Size is the number of neurons or input size. ʌs activation function we usedthe scaled exponential linear unit (SeLU) (Klambauer et al., 2017).
Table 2: Neural network architectures used for training for the MNIST dataset. The input pixel valueis rescaled between -1.0 and 1.0. FC stands for a fully connected layer. Conv and ConvT stand for aconvolutional and a transposed convolutional layer, respectively, where “st” is shorthand for stride.
Table 3: Neural network architectures used for training for the Cifar-10 dataset. The input pixel valueis rescaled between -1.0 and 1.0. FC stands for a fully connected layer. Conv and ConvT standfor a convolutional and a transposed convolutional layer, respectively, where “st” is shorthand forstride. Size is the number of neurons or input size. As activation function we used SeLU, sigmoid,and hyperbolic tangent (tanh).
