Table 1: Human evaluation of action generation with respect to Semantic Accuracy and Visual Quality. Foreach metric, raters selected the better of two generation methods. In the results XX/Y Y means that AG2Vidwas selected as better for X% of the presented pairs. Image resolution is 256 × 256.
Table 2: Visual quality metrics of conditional video-generation methods in CATER and Smth. All methods useresolution 256 × 256 except for HG, which only supports 64 × 64.
Table 3: Ablation experiment for components of the frame generation. Losses are added one by one.
Table 4: Layout generation evaluation.
Table 6: HUman evalUation of timing in generated videos (see sec-tion 4.3). The table rePorts accUracy of hUman annotator ansWerWith resPect to the trUe ansWer.
Table 5: HUman evalUation ofthe semantic accuracy of the ac-tions in the generated videos.
Table 7: The semantic quality evaluated by humans of the generated and real action videos. We asked ratersto select the action described in the video for each synthesized video with a given action. The table reportsthe accuracy of the human annotators with respect to the true action underlying the video. Actions abovecorrespond to: ’Pushing [something] from left to right’, ’Moving [something] up’, ’Moving [something] down’,’Pushing [something] from right to left’, ’Putting [something] on a surface’, ’Taking [one of many similar thingson the table]’, ’Uncovering [something]’, ’Covering [something] with [something]’ .
