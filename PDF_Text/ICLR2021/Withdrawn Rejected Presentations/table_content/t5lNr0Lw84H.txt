Table 1: Average episode rewards in MPE.
Table 2: Eval Win rate in SMAC maps over 32 trials.
Table 3: Average score in the Hanabi-Small with different number of players. X’s represent trials thatwere not run due to underperformance.
Table 4: Success rate of each of the algorithms in the Hide-And-Seek domain.
Table 5: common hyperparameters used in MAPPO across all domains.
Table 6: sweeping procedure in MPE, tau denotes the target network update rate, the bold fontindicates the value adoPted.
Table 7: network hyPerParameters used in the MPE domain by all algorithmshyperparameters	valueepisode length	25last action layer gain	0.01gamma	0.99buffer size	5000batch size	32epsilon	from 1.0 to 0.05epsilon anneal time	50000Q function loss	MSE lossTable 8: HyPerParameters used in the MPE domain by MADDPG, MATD3, MASAC, and QMix.
Table 8: HyPerParameters used in the MPE domain by MADDPG, MATD3, MASAC, and QMix.
Table 9: HyPerParameters used in the MPE domain by MAPPO13Under review as a conference paper at ICLR 2021StarCraftII Micromanagement (SMAC): All algorithms are trained until convergence, 10Mtimesteps is reached, or for a maximum of 3 days. Since the maximum length of an episodevaries per map, we change the maximum episode length based on the maximum amount allowed bythe map.
Table 10: Sweeping procedure in the SMAC domain, the bold font indicates the value adopted.
Table 11: network hyperparameters used in the StarCraft domain by all algorithmshyperparameters	valueepisode length	depends on mapslast action layer gain	1gamma	0.99buffer size	5000batch size	32epsilon	from 1.0 to 0.05epsilon anneal time	50000Q-function loss	MSE lossTable 12: Hyperparameters used in SMAC by MADDPG, MATD3, MASAC, and QMix. buffer sizeand batch size is calculated by episodes.
Table 12: Hyperparameters used in SMAC by MADDPG, MATD3, MASAC, and QMix. buffer sizeand batch size is calculated by episodes.
Table 13: Hyperparameters used in the SMAC domain by MAPPO14Under review as a conference paper at ICLR 2021Maps	MAPPO	QMix	MASAC	MATD3	MADDPGepoch=15					2m_vs_1Z	100	100	90.88	89.84	89.063m	99.875	94.71	55.54	60.125	48.632s_vs_1sc	99.56	95.25	20.06	0	46.632s3z	98.5	94.07	23.52	3.125	14.593s_vs_3z	99.5625	97.03	0	0	03s_vs_4z	99.27	95.98	0	0	02c_vs_64zg	99.375	96.08	0	0	0so_many_baneling	99.68	93.18	62.43	45.28	56.158m	99.48	95.07	43.65	0	0MMM	98.59	96.95	16.83	0	01c3s5z	100	96.09	24	0	03s5z_vs_3s6z	80	79.03	0	0	08m_vs_9m	92.97	93.44	0	0	0bane_vs_bane	94.4792	91.97	0	0	025m	96.88	86.15	0	0	0
Table 14: Eval win rate in SMAC domain over 32 trials.
Table 15: Sweeping procedure in the Hanabi domain, the bold font indicates the value adopted.
Table 16: Network hyperparameters used in the Hanabi domain by all algorithms15Under review as a conference paper at ICLR 2021hyperparameters	valueepisode length	80last action layer gain	0.01gamma	0.99buffer size	5000batch size	64epsilon	from 1.0 to 0.05epsilon anneal time	80000Q function loss	MSE lossTable 17: Hyperparameters used in the Hanabi domain by MADDPG, MATD3, MASAC, and QMix.
Table 17: Hyperparameters used in the Hanabi domain by MADDPG, MATD3, MASAC, and QMix.
Table 18: Hyperparameters used in the Hanabi domain by MAPPOThe Hide-And-Seek (HNS) domainQMix	lr[1e-3, 5e-4,1e-4]; hard update interval [200, 500, 700, 900]MADDPG	lr[1e-3, 5e-4,1e-4]; tau [0.01, 0.005 0.001]MATD3	lr[1e-3, 5e-4,1e-4]; tau [0.01, 0.005 0.001]MASAC	lr[1e-3, 5e-4, 1e-4]; tau [0.01, 0.005, 0.001]MAPPO	lr[1e-3, 7e-4, 5e-4]; PPo epoch [5,10,15, 20, 25]Table 19: Sweeping procedure in the HNS domainnetwork hyperparameters	valueinitialization method	orthogonalnum GRU layers	1RNN hidden state dim	64fc layer dim	64num fc before	2num fc after	1use feature normalization	Trueoptimizer	Adamoptimizer eps	1e-5weight decay	0ReLU	True
Table 19: Sweeping procedure in the HNS domainnetwork hyperparameters	valueinitialization method	orthogonalnum GRU layers	1RNN hidden state dim	64fc layer dim	64num fc before	2num fc after	1use feature normalization	Trueoptimizer	Adamoptimizer eps	1e-5weight decay	0ReLU	True	ReLU gain	sqrt(2)Table 20: Network hyperparameters used in the HNS domain by all algorithms16Under review as a conference paper at ICLR 2021hyperparameters	valueepisode length	boxlocking: 120 blueprint construction: 200last action layer gain	0.01
Table 20: Network hyperparameters used in the HNS domain by all algorithms16Under review as a conference paper at ICLR 2021hyperparameters	valueepisode length	boxlocking: 120 blueprint construction: 200last action layer gain	0.01gamma	0.99buffer size	5000batch size	256epsilon	from 1.0 to 0.05epsilon anneal time	50000Q function loss	MSE loss	Table 21: Hyperparameters used in the HNS domain by MADDPG, MATD3, MASAC, and QMix.
Table 21: Hyperparameters used in the HNS domain by MADDPG, MATD3, MASAC, and QMix.
Table 22: hyperparameters used in the HNS domain by MAPPOB Algorithm DetailsWe use policies to refer to each unique actor and critic network pair and agents to refer to the actorsin the environment. We will use m to denote the number of policies being trained, and n to be thenumber of active agents in the environment. We will use g(。) to refer to the function which mapseach agent to the policy which controls it, and Ai to be the number of agents under the control ofpolicy i. In a setting where all agents share the same parameters, m = 1, as there is only a singlepolicy, and g would map all agents to this single policy. In a setting in which all agents have theirown critic and actor, m = n, as the number of policies equals the number of agents, and g wouldmap each agent to its unique policy containing the actor and critic networks.
