Table 1: Comparison of NLL and FID for different diffusion models on ImageNet 64 × 64. Lvlb andLhybrid were trained with learned sigmas using the parameterization from Section 3.1. For Lvlb, weused the resampling scheme from Section 3.3. Using our cosine schedule and Lhybrid improves bothlog-likelihood and FID over the baseline from Ho et al. (2020). Optimizing Lvlb further improveslog-likelihood at the cost of a higher FID.
Table 2: Comparison of NLL and FID for different diffusion models on CIFAR-10. Using ourcosine schedule and Lhybrid improves log-likelihood with a marginal impact on FID. OptimizingLvlb further improves log-likelihood at the cost of a significantly higher FID.
Table 3: Comparison of diffusion models to other likelihood-based models on CIFAR-10 and Un-conditional ImageNet 64 × 64. On ImageNet 64 × 64, our model is competitive with the bestconventional models, but is worse than fully transformer-based architectures.
Table 4: Sample quality comparison on class conditional ImageNet 64 × 64.
