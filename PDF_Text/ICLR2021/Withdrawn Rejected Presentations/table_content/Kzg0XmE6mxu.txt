Table 1: Performance of naturally-trained DML models against adversarial perturbations gener-ated using Algorithm 1 with PGD across various E for '∞. Results are an average of five randomseeds. Recall that, R@1 reflects a model’s inference accuracy, while mAP@R reflects its abilityto rank similar entities. Naturally-trained DML models are not robust to the generated adversarialperturbations.
Table 2: Performance of DML models trained using the proposed adversarial training objective(using PGD) compared to naturally-trained DML for adversarial perturbations within '∞(e = 0.01).
Table 3: Performance of DML models trained using the proposed adversarial training objective(using PGD) compared to naturally-trained DML for adversarial perturbations within '∞(e = 0.01)discovered using FGSM. Losses are denoted by C (contrastive) and T (triplet). The robustly trainedmodel attain both higher inference accuracy (R@1) and improved ability to rank similar entities(mAP@R) than the naturally-trained baseline model. Thereby, the proposed robust training objec-tive improves the robustness towards adversarial perturbations.
Table 4: Performance of DML models trained using the proposed adversarial training objective(using PGD) compared to naturally-trained DML for adversarial perturbations within '∞(e = 0.01)discovered using C&W. Losses are denoted by C (contrastive) and T (triplet). The robustly trainedmodel attain both higher inference accuracy (R@1) and improved ability to rank similar entities(mAP@R) than the naturally-trained baseline model. Thereby, the proposed robust training objec-tive improves the robustness towards adversarial perturbations.
Table 5: Performance of robust DML models on adversarial input for various specified (training)attack rates P(γ = 1) . These models were trained using the proposed adversarial training algorithm(covered in Section 3.5) with PGD for '∞(e = 0.01). Evaluations on conducted on adversarialinput generated using Algorithm 1. Naturally-trained marks the performance of DML models usingtraditional non-robust training objectives. Bold marks best performance for dataset, metric, losscombinations. Robust models reach higher inference accuracy (R@1) and better ability to ranksimilar entities (mAP@R) on adversarial input than naturally-trained DML models. Higher attackrates are often associated with higher robustness.
Table 6: Performance of robust DML models on benign input for various specified (training) attackrates P (γ = 1) . These models were trained using the proposed adversarial training algorithm(covered in Section 3.5) with PGD for '∞(e = 0.01). Evaluations on benign data. Naturally-trainedmarks the performance of DML models using traditional non-robust training objectives. Bold marksbest performance for dataset, metric, loss combinations. Higher attack rates are often associated withlower performance on benign input. For contrastive loss14Under review as a conference paper at ICLR 2021	'∞(e = 0.01)	CUB200-2011		CARS196		SOP			R@1	mAP@R	R@1	mAP@R	R@1	mAP@RC	Benign	59.1	21.0	74.0	20.9	71.8	44.7	FGSM	19.7	7.2	14.6	3.8	18.4	10.8	C&W	15.1	6.0	5.6	2.3	23.1	13.9T	Benign	59.3	21.7	74.0	21.4	69.6	42.1	FGSM	18.9	7.7	15.5	4.1	14.5	8.5	C&W	22.9	10.0	13.4	3.6	10.0	6.1Table 7: Performance of naturally-trained DML models against adversarial examples generatedusing Algorithm 1 with two alternative attack methods: FGSM and C&W. Losses are denoted by C(contrastive) and T (triplet). Recall that, R@1 reflects a model’s inference accuracy, while mAP@Rreflects its ability to rank similar entities.
Table 7: Performance of naturally-trained DML models against adversarial examples generatedusing Algorithm 1 with two alternative attack methods: FGSM and C&W. Losses are denoted by C(contrastive) and T (triplet). Recall that, R@1 reflects a model’s inference accuracy, while mAP@Rreflects its ability to rank similar entities.
