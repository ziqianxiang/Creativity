Table 1: Performance on the benchmarking GNN datasets. In bold: better performance between LRGA aug-mented and vanilla models; note the parameter (#) budget. Blue represents best performance with the 100Kbudget and red with the 500K budget.
Table 2: Performance on the link prediction tasks from theOGB benchmarkModel	ogbl-ppa		ogbl-collab		ogbl-ddi		# Param	Hits@100±std	# Param	Hits@50± std	# Param	Hits@20±stdNode2vec	7.3M	0.223 ± 0.008	30M	0.489 ± 0.005	645K	0.233 ± 0.021DeepWalk	150M	0.289 ± 0.015	61M	0.504 ± 0.003	11M	0.264 ± 0.061MF	147M	0.323 ± 0.009	60M	0.389 ± 0.003	1.2M	0.137 ± 0.047GraphSage	424K	0.165 ± 0.024	460K	0.481 ± 0.008	1.4M	0.539 ± 0.047GCN	278K	0.187 ± 0.013	296K	0.447 ± 0.011	1.2M	0.370 ± 0.050LRGA + GCN	814K	0.342 ± 0.016	1M	0.522 ± 0.007	1.5M	0.623 ± 0.091details in appendix H.2.
Table 3: Attention ablation table. Various GNNs augmented with attention variants on the ZINC dataset. Boldrepresent best performance and blue represent second best.
Table 4: Random FeaturesEvaluationModel	PATTERN	Acc ± StdGCN	74.891 ± 0.713LRGA + GCN	84.118 ± 1.216GAT	81.796 ± 0.661LRGA + GAT	85.905 ± 0.109GraphSage	85.039 ± 0.068LRGA + GraPhSage	85.229 ± 0.331GatedGCN	85.848 ± 0.065LRGA + GatedGCN	85.944 ± 0.664GIN	85.760 ± 0.001LRGA + GIN	86.765 ± 0.065pared to Table 1) and serve as an empirical validation to our main theorem.
