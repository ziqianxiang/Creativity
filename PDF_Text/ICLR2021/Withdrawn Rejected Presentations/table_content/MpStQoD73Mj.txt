Table 2: A comparison of bi-gram transition graphswith back-off and varying levels of pruning for let-ters and 1, 000 word pieces. We report CER on theIAM validation set and epoch time in seconds.
Table 1: A comparison of dense transitiongraphs from n = 0 (no transitions) up ton = 2 (bi-grams). We report CER on theIAM validation set using letter tokens.
Table 3: A comparison of the convolutionalWFST layer to a traditional convolution. Wereport the CER on the IAM validation set andcompare the two layers in number of param-eters and number of operations. Both con-volutional layers have a kernel width k = 5,a stride of 4, ci = 80 input channels, andco = 200 output channels.
Table 4: A comparison of the most frequent decomposition to the SentencePiece decompositionfor a given word. The words were selected by first computing the Viterbi decomposition for eachexample in the LibriSpeech training set. We then chose words for which the most frequently selecteddecomposition was different from the SentencePiece decomposition. The counts are the occurrencesof each decomposition in the Viterbi paths on the training set. In some cases the most frequentdecomposition appears to be more phonologically plausible than the SentencePiece decomposition.
