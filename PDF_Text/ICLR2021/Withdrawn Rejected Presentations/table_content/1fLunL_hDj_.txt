Table 1: Comparison between vocabularies search by Info-VOT and widely-used BPE vocabularies.
Table 2: Comparison between vocabularies search by Info-VOT and BPE-1K, recommendedby Ding et al. (2019) for low-resource datasets. Here we take TED En-X bilingual translation asan example. This table demonstrate that vocabularies searched by Info-VOT are on par with heuris-tically searched vocabularies in terms of BLEU scores.
Table 3: Comparison between Info-VOT and BPE-Search on bilingual settings. In BPE-Searchsolution, the best-performing vocabulary BPE-5K is selected based on its average performance fromBPE-1K, BPE-2K, BPE-3K, BPE-4K, BPE-5K, BPE-6K, BPE-7K, BPE-8K, BPE-9K, BPE-10K,BPE-20K, and BPE-30K. BPE-Search requires full training and takes 288 GPU hours to search forthe optimal vocabulary while Info-VOT only takes 0.5 CPU hours.
Table 4: Comparison between Info-VOT and strong baselines. Info-VOT achieves almost the bestperformance with a much smaller vocabulary.
Table 5: The correlation coefficient between AMD and BLEU scores. Among 45 tasks, almosttwo-thirds of tasks show obvious correlations (greater than 0.4) between AMD and BLEU scores.
Table 6: Info-VOT can find better vocabularies than widely-used vocabularies on normal-size ar-chitectures. Here “better” means competitive results but smaller sizes.
Table 7: Comparison between Info-VOT and widely-used BPE vocabularies on multilingual trans-lation. Here we show the results on 45 language pairs. BPE-60K is the most popular setting inmultilingual translation tasks. Info-VOT achieves better BLEU scores on 30 out of 45 datasets. The	size of vocabulary generated by Info-VOT is around 90K.	X-En	Es PT-br Fr Ru He Ar Ko Zh-Cn It Ja Zh-tw Nl RoBPE-60K	35.00 37.73 33.53 24.22 30.58 26.32 19.41 20.98 32.63 16.21 20.00 30.77 30.58Info-VOT	35.49 38.36 34.00 24.20 30.96 26.52 19.36 21.15 32.67 16.30 19.99 31.22 30.92X-En	Tr De Vi Pl Pt Bg El	Fa	Sr Hu Hr Uk CsBPE-60K	23.64 29.96 25.20 23.82 35.88 32.87 32.52 24.53 30.26 24.07 32.04 26.40 27.18Info-VOT	23.74 30.56 25.47 23.78 36.16 33.36 33.39 25.03 30.24 24.07 32.13 26.44 27.07X-En	Id Th Sv Sk Sq Lt Da My Sl Mk Fr-Ca Fi HyBPE-60K	27.51 22.34 33.21 29.31 31.78 23.73 37.72 17.82 24.99 29.92 30.42 21.13 21.70Info-VOT	28.05 22.24 33.94 29.36 31.78 22.91 37.72 17.43 24.83 31.49 31.04 21.30 21.40X-En	Hi Nb Ka Et Ku GlBPE-60K	23.87 38.36 21.51 22.05 18.45 31.39Info-VOT	22.69 39.72 20.91 21.27 17.85 31.3916Under review as a conference paper at ICLR 2021Appendix D:	S upplemental ExperimentsWe conduct bilingual experiments on 14 language pairs with the most training data from TED. The
Table 8: BPE-2K is not a universal solution. Different datasets have varying optimal sizes.
