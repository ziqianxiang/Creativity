Table 1: Peformance in CIFAR100Model	Test Accuracy %DenseNet121 (teacher)	75.09 ± 00.29AllCNN	67.64 ± 01.87AllCNN-KD	73.27 ± 00.20AllCNN-FitNet	72.03 ± 00.27AllCNN-AT	70.88 ± 0.29AllCNN-PKT	72.22 ± 0.35AllCNN-RKD	70.39 ± 0.17AllCNN-CRD	72.70 ± 0.18AllCNN-SRM (our)	74.73 ± 00.26supervisory information encoding the spatial information, image-level label provides global supervi-sory information, promoting the shift-invariance property. Image-level label bears some resemblancesto the Bag-of-Feature model (Passalis & Tefas, 2017), which aggregates the histograms of imagepatches to generate an image-level feature. The second knowledge transfer objective in our methodusing image-level labels is defined as follows:Inin X L	( Pijtn)ij Pi,j kS,n,i,jΘg,mn ? LBCE (^Wr,	Hl ∙ Wi(5)where LBCE denotes the binary cross-entropy loss. Here we should note that since most kernel
Table 2: Comparison (test accuracy %) between FitNet and SRM in terms of quality of intermediateknowledge transfer on CIFAR100AllCNN-FitNet AllCNN-SRM(our)Linear Probing	69.95 ± 00.20	69.10 ± 00.31Whole Network Update	68.06 ± 00.10	71.99 ± 00.08used DenseNet121 (Huang et al., 2017) as the teacher network (7.1M parameters and 900M FLOPs)and a non-residual architecture (a variant of AllCNN network (Springenberg et al., 2014)) as thestudent network (2.2M parameters and 161M FLOPs). Other details about training hyperparameterscan be found in our Appendix A.
Table 3: SRM test accuracy (%) on CIFAR100 with different dictionary sizes (parameterized by μ)and degree of sparsity (parameterized by λ)	μ = 1.5		μ = 2.0	μ = 3.0λ=	0.01	74.00±00.15	74.12±00.35	74.09±00.08λ=	0.02	74.34±00.07	74.73±00.26	74.20±00.27λ=	0.03	73.77±00.05	73.83±00.12	73.71±00.51Pixel-level label and image-level label: finally, to show the importance of combining both pixel-level and image-level label, we experimented with two other variants of SRM on CIFAR100: usingeither pixel-level or image-level label. The results are shown in Table 4. The student network6Under review as a conference paper at ICLR 2021performs poorly when only receiving intermediate knowledge via image-level labels, even thoughit was later optimized with the standard KD phase. Similar to the observations made from Table 2,this again suggests that the position in the parameter space, which is obtained after the intermediateknowledge transfer phase, and prior to the standard KD phase, heavily affects the final performance.
Table 4: Effects ofpixel-levellabel and image-levellabel in SRM on CIFAR100Pixel-level label	Image-level label	Test accuracy %X	73.16 ± 00.39	X	53.50 ± 09.96X	X	74.73 ± 00.264.2	Transfer Learning ExperimentsSince transfer learning is key in the success of many applications that utilize deep neural networks,we conducted experiments in 5 transfer learning problems (Flowers (Nilsback & Zisserman, 2008),CUB (Wah et al., 2011), Cars (Krause et al., 2013), Indoor-Scenes (Quattoni & Torralba, 2009) andPubFig83 (Pinto et al., 2011)) to assess how well the proposed method works under transfer learningsetting compared to others.
Table 5: Transfer learning using AllCNN (ACNN) and ResNet18 (RN18) (test accuracy %). The					standard deviation of the test accuracy in few-shot settings is reported in Table 7					Model	Flowers	CUB	Cars	Indoor-Scenes	PubFig83ResNext50	89.35 ± 00.62	69.53 ± 00.45	87.45 ± 00.27	63.51 ± 00.43	91.41 ± 00.14Full Shot					ACNN	40.80 ± 02.33	47.26 ± 00.18	61.93 ± 01.38	35.82 ± 00.43	78.47 ± 00.17ACNN-KD	46.14 ± 00.39	51.80 ± 00.41	66.12 ± 00.17	38.44 ± 00.99	81.54 ± 00.09ACNN-FitNet	30.10 ± 02.41	44.30 ± 01.25	60.20 ± 03.83	30.87 ± 00.35	77.61 ± 00.87ACNN-AT	51.62 ± 00.69	51.74 ± 00.39	73.89 ± 00.06	43.56 ± 00.52	81.11 ± 01.51ACNN-PKT	47.12 ± 00.52	47.60 ± 00.85	70.16 ± 00.51	37.71 ± 00.64	82.03 ± 00.26ACNN-RKD	42.00 ± 01.10	39.99 ± 00.61	56.99 ± 02.44	30.94 ± 00.45	75.44 ± 00.50ACNN-CRD	46.99 ± 01.13	51.12 ± 00.34	68.89 ± 00.47	42.82 ± 00.21	83.02 ± 00.11ACNN-SRM	51.72 ± 00.58	54.51 ± 01.72	71.44 ± 04.76	43.09 ± 00.60	82.89 ± 01.78RN18	44.25 ± 00.42	44.79 ± 00.68	57.17 ± 01.95	36.72 ± 00.29	79.08 ± 00.36RN18-KD	48.26 ± 00.33	54.91 ± 00.33	75.29 ± 00.46	43.84 ± 00.67	84.49 ± 00.28RN18-FitNet	48.29 ± 01.24	61.28 ± 00.48	85.01 ± 00.10	45.93 ± 01.00	89.78 ± 00.20RN18-AT	51.49 ± 00.42	53.13 ± 00.40	77.14 ± 00.15	44.13 ± 00.55	83.60 ± 00.19RN18-PKT	45.32 ± 00.51	45.24 ± 00.39	71.24 ± 02.23	37.27 ± 00.79	82.00 ± 00.10RN18-RKD	42.32 ± 00.28	36.29 ± 00.58	56.87 ± 02.64	29.57 ± 00.90	70.90 ± 01.79RN18-CRD	47.67 ± 00.05	53.25 ± 00.83	76.51 ± 00.74	43.76 ± 00.40	84.43 ± 00.40
Table 6: Top-1 Error of ResNet18 on ImageNet. (*) indicates results obtained by 110 epochs.
Table 7: Transfer learning: standard deviation (%) of test accuracyModel	Flowers	CUB	Cars	Indoor-Scenes	PubFig835-Shot					ACNN	00.94	00.46	00.11	00.81	00.45ACNN-KD	00.76	00.83	00.52	00.67	01.40ACNN-FitNet	01.18	00.60	00.37	00.52	01.36ACNN-AT	00.88	00.94	01.50	00.39	00.70ACNN-PKT	00.31	00.20	00.29	01.04	00.18ACNN-RKD	00.27	00.34	00.31	00.39	00.16ACNN-CRD	00.57	00.91	00.55	00.49	00.28ACNN-SRM	01.09	01.18	01.21	00.34	00.52RN18	00.37	00.10	00.27	00.44	00.22RN18-KD	01.47	00.35	00.58	00.88	00.99RN18-FitNet	00.62	00.46	01.00	01.07	00.66RN18-AT	01.23	00.47	00.92	00.73	00.34RN18-PKT	00.47	00.18	00.18	00.82	00.11RN18-RKD	00.55	00.40	00.13	00.49	00.24RN18-CRD	00.32	02.09	00.26	00.23	00.21RN18-SRM	00.48	00.63	01.82	01.29	00.5110-Shot					
