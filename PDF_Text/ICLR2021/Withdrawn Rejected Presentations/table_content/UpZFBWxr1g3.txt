Table 1: Tokenized BLEU on various test sets of WMT’18 En-De when adding skip connectionsby network morphism compared to back-translation sentence pairs obtained by various generationmethods.__________________________________________________________________________________Model	News2013	News2014	News2015	News2016	News2017	AverageBitext	27.84	30.88	31.82	34.98	29.46	31.00+beam	27.82	32.33	32.20	35.43	31.11	31.78+greedy	27.67	32.55	32.57	35.74	31.25	31.96+top10	28.25	33.94	34.00	36.45	32.08	32.94+sampling	28.81	34.46	34.87	37.08	32.35	33.51+beam+noise	29.28	33.53	33.79	37.89	32.66	33.43+sampling+network morphism (ours)	29.16	35.39	35.34	38.54	33.68	34.42the setup of Ott et al. (2018), we validate on newstest2013 and test on newstest2014. The vocabularyis a 32K joint source and target byte pair encoding (BPE; Sennrich et al., 2016).
Table 2: Tokenized BLEU on newstest2014 for WMT English-German (En-De) and English-French(En-Fr). a-h use only WMT bitext (WMT’14, except for b,c,e,f,g which train on WMT’16 in En-De).
Table 3:	Tokenized BLEU of adding skip connections by network morphism on three pre-trainedmodels. The first two models correspond to bitext models in Ott et al. (2018). The third modelcorresponds to back-translation (BT) model in EdUnov et al.(2018a).
Table 4:	The sentence numbers that network morphism fine-tuning needs on various datasets. Fine-tuning subset denotes how many sentences are used for fine-tuning. Each sentence is used only once.
Table 5: Detokenized BLEU of different transformer models on IWSLT’14 De-En. Different rowscorrespond to different architectures used to train new transferred models in IWSLT’14 De-En.
