Table 1: Choices of scoring function score(τ, π) investigated in this work.
Table 2: Comparison of test performance of policies trained on each method. Here PLR denotesvalue-based level replay with rank prioritization with β = 0.1 and ρ = 0.1. Following the evaluationprotocol in Raileanu et al. (2020), the reported mean and standard deviation per environment iscomputed by evaluating the final policy’s average return on 100 test episodes, aggregated acrossmultiple training runs (10 runs for Procgen Benchmark and 3 for MiniGrid). Each run is initializedwith a different training seed. Normalized test returns per run are computed by dividing the averagetest return per run for each environment by the corresponding average test return of the uniform-sampling baseline over all runs. We then report the means and standard deviations of normalizedtest returns aggregated across runs. Note, we report the normalized return statistics for Procgen andMiniGrid environments separately.
Table 3: Hyperparameters used for training.
Table 4: Comparison of test scores of PPO with value-based level replay (PLR) against PPO withuniform-sampling on the hard setting of Procgen Benchmark. As in Table 2, reported figures rep-resent the mean and standard deviation of average test scores over 100 episodes aggregated across10 runs, each initialized with a unique training seed. For each run, a normalized average return iscomputed by dividing the average test return for each game by the corresponding average test returnof the uniform-sampling baseline over all 1000 test episodes of that game, followed by averagingthese normalized returns over all 16 games. The final row reports the mean and standard deviationof the normalized returns aggregated across runs. Note that while the uniform-sampling baselineachieves a higher mean return on BigFish and StarPilot than value-based level replay, this differencewas not found to be statistically significant according to Welch’s t-test.
