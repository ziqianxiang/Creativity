Table 1: Performance of three citation networks. The ‘*’ indicates the best results of the baselines.
Table 2: Performance of NELLDataset MLP	Planetoid^^SGC	GNM-GCN^^GCN	GCN-VD^^GCN-DVDNELL~~02385^^03901	04128^^0.1589	04416^^04652	047348Under review as a conference paper at ICLR 20215	Related WorksIn the past few years, Graph Neural Networks (GNNs) (Scarselli et al., 2008; Kipf & Welling, 2016;Velickovic et al., 2017; XU et al., 2019; KlicPera et al., 2019) have become the major technologyto capture patterns encoded in the graph due to its powerful representation capacity. Although thecUrrent GNNs have achieved great sUccess, when aPPlied to indUctive setting, they all assUme thattraining nodes and test nodes follow the same distribUtion. However, this assUmPtion does not alwayshold in real aPPlications. GNM (ZhoU et al., 2019) first Pays attention on the label selection Problemon graPh learning, and it learns a IPW estimator to estimate the Probability of each node to be selectedand Uses this Probability to reweight the labeled nodes. However, it heavily relies on the accUracy ofIPW estimator, which dePends on the label assignment distribUtion of whole graPh, hence it is moresUitable for transdUctive setting.
Table 3: DataSet StatiSticSDataset	Type	Nodes	Edges	Classes	Features	Bias degree ()	Bias typeCora	Citation network	2,708	5,429	7	1,433	0.7/0.8/0.9	Label selection biasCiteseer	Citation network	3,327	4,732	6	3,703	0.7/0.8/0.9	Label selection biasPubmed	Citation network	19,717	44,338	3	500	0.7/0.8/0.9	Label selection biasNELL	Knowledge graph	65,755	266,144	210	5,414	One labeled node per class	Small sample selection biasSome statistics of datasets used in our paper are presented in Table 3, including the number of nodes,the number of edgeS, the number of claSSeS, the number of featureS, the biaS degree and biaS type.
