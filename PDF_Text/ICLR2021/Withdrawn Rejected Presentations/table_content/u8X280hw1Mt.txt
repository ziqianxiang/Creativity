Table 1: ImageNet accuracy (MoCo v2) vs. the number of queries per batch (N). The learning ratesduring training are adjusted with linear scaling rule.
Table 2: State-of-the-art InfoNCE-based frameworksWe propose SiMo, a simplified variant of MoCo v2 (Chen et al. (2020c)) equipped with EqCo. Wefollow most of the design in Chen et al. (2020c), where the key differences are as follows:Memory bank. MoCo, MoCo v2 and SimCLR v2 6 (Chen et al. (2020b)) employ memory bankto maintain large number of negative embeddings ki , in which there is a side effect: every positiveembedding k0 is always extracted from a “newer” network than the negatives’ in the same batch,which could harm the performance. In SiMo, we thus cancel the memory bank as we only rely on afew negative samples per batch. Instead, we use the momentum encoder to extract both positive andnegative key embeddings from the current batch.
Table 3: Ablation on momentum update.
Table 4: Sync BN vs. shuffling BN.
Table 5: SiMo with different α.
Table 6: MoCo v2 with different K .
Table 7: SiMo with wider models. All models are trained with 200 epochs.
Table 8: Object detection fine-tuned on COCO.
Table 9: Estimating mutual information by InfoNCE and EqCo with different batch size and variousground truth mutual information.
