Table 1: MNIST classification results (1NFE ≈ 0.0007 seconds for a test batch of100 images)Model	Accuracy	NFEResNet	-0.9959-	N/ARKNet	0.9953	N/ANo reg.	0.9960	26Kinetic energy reg.	0.9965	20L1 reg.	0.9956	20L2 reg.	0.9961	20Our reg.	0.9964	14No reg. & DISE	0.9958	11.36Our reg. & DISE	0.9963	5.791Under review as a conference paper at ICLR 2021Neural ODExFigure 1: The general architecture of Neural ODEs. We assume a classification task in this figure.
Table 2: The auxiliary network for MNIST where σ is ReLU, π isGroup Normalization, and ξ is Adaptive Average Pooling.
Table 3: The distribution ofthe integrator selection byour auxiliary network forMNISTIntegrator	PercentageDOPRI	37%RK4	34%Euler	29%the biggest portion for its higher accuracy than others, i.e., 37%. RK4 and the Euler method havemore numerical errors than DOPRI and as a result, their estimated costs are larger than that of DO-PRI in general (due to the large penalty M). We note that their rankings in terms of the selectionpercentage are the same as those in terms of the numerical error. One interesting point is that theEuler method also occupies a non-trivial portion in the table.
Table 4: The auxiliary network for PhysioNetwhere ψ is Drop Out.
Table 5: PhysioNet prediction results (1NFE ≈ 0.013 seconds for a test batch of60 records)Model	AUC	NFENo reg.	0.7190	74Kinetic energy reg.	0.7581	63.5L1 reg.	0.7630	68L2 reg.	0.7450	59Our reg.	0.7509	39.71No reg. & DISE	0.7513	57.57Our reg. & DISE	0.7604	34.16Under review as a conference paper at ICLR 20210.999. The exponent α is 0.05. The large penalty M is 1,000. The dropout ratio is {0, 0.25}. We useDOPRI as our default solver unless DISE is adopted. We use the recommended hyperparametersfor Latent-ODE in their paper or in their github repository.
Table 6: The distribution ofthe integrator selection byour auxiliary network forPhysioNet.
Table 9: The distributionof the integrator selectionby our auxiliary networkfor Continuous Normaliz-ing FlowsIntegrator	PercentageDOPRI	~87.5%~RK4	0%Euler	12.5%We tackled one critical problem of Neural ODEs, a delayed processof the forward-pass inference. Even though DOPRI is able to dynami-cally adjust its step-sizes, as described earlier, there exists a limitationin saving the forward-pass inference time. To address the problem, we suggested i) regularizing theDOPRI’s estimated error, which results in a reduced NFE, and ii) dynamically selecting an ap-propriate integrator for each input. We successfully showed that both the model accuracy and theforward-pass inference time can be improved at the same time in all tasks. We also showed that non-trivial percentages of test cases can be solved by the Euler method after adopting our regularizer.
