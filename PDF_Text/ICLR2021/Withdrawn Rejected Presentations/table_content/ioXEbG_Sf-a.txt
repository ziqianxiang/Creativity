Table 1: Results on OpenAI Gym environments when trained with 500k steps. ERE is only designedfor SAC, so its results on TD3 are not available.
Table 2: Results of SAC and TD3 trained from states on the DeepMind Control environments with andwithout LFIW after 100k and 250k environment steps. The results show significant improvementswhen the agents is trained with LFIW. Results are reported over 5 random seeds. The maximumpossible score for any environment is 1,000.
Table 3: Results of SAC and TD3 trained from states on the DeepMind Control environments with andwithout LFIW after 100k and 250k environment steps. The results show significant improvementswhen the agents is trained with LFIW. Results are reported over 5 random seeds. The maximumpossible score for any environment is 1,000.
Table 4: Results for DrQ (Kostrikov et al., 2020) on the image-based RL on the DeepMind ControlSuite. LFIW is applied to a state-of-the-art image-based RL algorithm in DrQ, and we are able to seeconsistent improvement over the DM Control Suite Benchmark.
Table 5: Additional hyperparameters for SAC (Haarnoja et al., 2018)Parameter	Valueoptimizer	Adam Kingma & B ( 01 )learning rate	3 X 10-4discount	0.99number of samples per minibatch	256nonlinearity	ReLUtarget smoothing coefficient	5 × 10-3Table 6: Additional hyperparameters for TD3 (Fujimoto et al., 2018)	Parameter	Valueoptimizer	Adam Kingma & B ( 01 )learning rate	10-3discount	0.99number of samples per minibatch	256nonlinearity	ReLUexploration policy	N(0,1)Temperature T The temperature T affects the variances of the weights assigned; a larger T makesthe weights more similar to each other, while a smaller T relies more on the outputs of the classifier.
Table 6: Additional hyperparameters for TD3 (Fujimoto et al., 2018)	Parameter	Valueoptimizer	Adam Kingma & B ( 01 )learning rate	10-3discount	0.99number of samples per minibatch	256nonlinearity	ReLUexploration policy	N(0,1)Temperature T The temperature T affects the variances of the weights assigned; a larger T makesthe weights more similar to each other, while a smaller T relies more on the outputs of the classifier.
Table 7: Additional hyperparameters for DrQ (Kostrikov et al., 2020)Parameter	Valueoptimizer	Adam Kingma & B ( 01 )learning rate	10-3discount	0.99critic target update frequency	2actor log stddev bounds	[-10, 2]actor update frequency	2number of samples per minibatch	512nonlinearity	ReLUimage dimensions	84 X 84(a) Temperature T	(b) Fast replay buffer size |Df |(c) Hidden layer size of wψFigure 2: Hyperparameter sensitivity analyses on Walker2d-v2 with SAC.
