Table 1: Cifar-10 on Resnet-18 trained for 200epochs with Momentum. An LR of 0.1 is usedfor the explore epochs. Half the remaining epochsare trained at 0.01 and the other half at 0.001. Re-ported results are average over 4 runs.
Table 2: Cifar-10 on Resnet-18 trained for 200 epochs. An LR of 0.1 is used for the first 100 epochs.
Table 3: ImageNet on Resnet-50 results. We report mean (stddev) over 3 runs.
Table 4: BERTLARGE results. We report the pre-training train loss, and the test F1 accuracy onSQuAD v1.1 after fine-tuning. See figure 7 in Appendix for training curves.
Table 5: Results for WMT’14 (EN-DE) on Transformer networks. The test BLEU scores are com-puted on the checkpoint with the best validation perplexity. We report mean (stdev) over 3 runs.
Table 6: We report the top-1 accuracy for ImageNet and Cifar-10, BLEU score for IWSLT’14 andWMT’14 and F1 score for BERT on SQuAD. All values are averaged over multiple runs.
Table 7: Shorter budget training: Test accuracy on all learning rate schedules tried in this paper, buttrained with a shortened budget. We report same metrics as Table 6. Knee schedule achieves thesame accuracy as baseline schedules in much lower budget, saving precious GPU-hours.
Table 8: Epochs required by different LR schedules to reach the target accuracy.
Table 9: IWSLT’14 (DE-EN) on the Transformer network trained with the Knee schedule. Theexplore duration is varied, while keeping the total training budget fixed at 50 epochs. We reportaverages over 3 runs.
Table 10: Training loss and Test accuracy for Cifar-10 on Resnet-18. We report mean (stddev) over7 runs.
Table 11: Training, validation perplexity and test BLEU scores for IWSLT on Transformer networks.
Table 12: Training, validation perplexity and test BLEU scores for IWSLT’14 DE-EN on MAT. Thetest BLEU scores are computed on the checkpoint with the best validation perplexity. Results areaveraged over 3 runs.
Table 13: Training, validation perplexity and test BLEU scores for WMT’14 DE-EN on MAT. Thetest BLEU scores are computed on the checkpoint with the best validation perplexity.
Table 14: SQuAD fine-tuning on BERTBASE. We report the average training loss, and average testEM, F1 scores over 3 runs.
Table 15: Keskar’s sharpness metric for Cifar-10 on Resnet-18 trained for 200 epochs with Momen-tum. An LR of 0.1 is used for the explore epochs. Half the remaining epochs are trained at 0.01 andthe other half at 0.001. We report the average sharpness over 50 different trials.
Table 16: FIM Score for Cifar-10 on Resnet-18 trained for 200 epochs with Momentum. A LR of0.1 is used for the explore epochs. Half the remaining epochs are trained at 0.01 and the other halfat 0.001. We report the average FIM score over 10 different trials.
Table 17: Seed LR sensitivity analysis. Cifar-10 on Resnet-18 trained for 150 epochs with Kneeschedule. We vary the seed LR and explore epochs to get the best test accuracy for the particularsetting. We report averages over 3 runs.
Table 18: Cifar-10 on Resnet-18 full budget training (200 epochs): Training loss and Test accuracyfor more learning rate schedules. We report the mean and standard deviation over 7 runs.
Table 19: Cifar-10 on Resnet-18 short budget training (150 epochs): Training loss and Test accuracyfor more learning rate schedules. We report the mean and standard deviation over 7 runs.
Table 20: ImageNet with ResNet-50 full budget training (90 epochs): Training loss, Test Top-1 andTest Top-5 for more learning rate schedules. We report the mean and standard deviation over 3 runs.
Table 21: ImageNet with ResNet-50 short budget training (50 epochs): Training loss, Test Top-1and Test Top-5 for more learning rate schedules. We report the mean and standard deviation over 3runs.
Table 22: WMT’14 (EN-DE) on Transformer networks full budget training (70 epochs): Training,validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scoresare computed on the checkpoint with the best validation perplexity. We report the mean and standarddeviation over 3 runs.
Table 23: WMT’14 (EN-DE) on Transformer networks short budget training (30 epochs): Training,validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scoresare computed on the checkpoint with the best validation perplexity. We report the mean and standarddeviation over 3 runs.
Table 24: IWSLT’14 (DE-EN) on Transformer networks full budget training (50 epochs): Training,validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scoresare computed on the checkpoint with the best validation perplexity. We report the mean and standarddeviation over 3 runs.
Table 25: IWSLT’14 (DE-EN) on Transformer networks short budget training (35 epochs): Training,validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scoresare computed on the checkpoint with the best validation perplexity. We report the mean and standarddeviation over 3 runs.
Table 26: SQuAD-v1.1 fine-tuning on BERTBASE for more learning rate schedules. We report theaverage training loss, average test EM, F1 scores over 3 runs.
Table 27: SQuAD fine-tuning on BERTLARGE. We report F1 scores for 3 different trials as well asthe maximum and average values.
