Table 1: A comparison of approaches for OOD generalization.
Table 2: Accuracy (percent) on ColoredMNIST. REx and IRM learn to ignore thespurious color feature. StrikethrOUgh resultsachieved via tuning on the test set.
Table 3: REx, IRM, and ERM all perform comparably on set of domain generalization benchmarks.
Table 4: Average mean-squared error between true and estimated weights on causal (X1) andnon-causal (X2) variables. Top 2: When the level of noise in the anti-causal features varies acrossdomains, REx performs well (FOU, FOS, POU, POS). Bottom 2: When the level of noise in thetargets varies instead, REx performs poorly (FEU, FES, PEU, PES). Using the baselines re = V(Y )does not solve the problem, and indeed, hurts performance on the homoskedastic domains.
Table 5: A complete overview of hyperparameters used for reinforcement learning experiments.
Table 6: Accuracy (percent) of different methods on the VLCS task. Results are test accuracy at thetime of the highest validation accuracy, averaged over 10 runs. On VLCS REx outperforms all othermethods. Numbers are shown in strike-through because we selected our hyperparameters based onhighest test set performance; the goal of this experiment was to find suitable hyperparameters for thePACS experiment.
Table 7: Accuracy (percent) of different methods on the PACS task. Results are test accuracy at thetime of the highest validation accuracy, averaged over 10 runs. REx outperforms ERM on average,and performs similar to IRM and Jigsaw (the state-of-the-art).
Table 8: Test accuracy of models trained on the financial domain dataset, averaged over all 20 tasks,as well as min./max. accuracy across the tasks.
