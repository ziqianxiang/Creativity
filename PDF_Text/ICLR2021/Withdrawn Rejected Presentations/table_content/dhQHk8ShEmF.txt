Table 1: Comparison with competitive OOD detection methods. We use DenseNet as network architecturefor all methods. We evaluate on four types of OOD inputs: (1) natural OOD, (2) corruption attacked OOD, (3)L∞ attacked OOD, and (4) compositionally attacked OOD inputs. The description of these OOD inputs canbe found in Section 5.1. ↑ indicates larger value is better, and ] indicates lower value is better. All values arepercentages and are averaged over six different OOD test datasets described in Section 5.1. Bold numbers aresuperior results. Additional results on a different architecture, WideResNet, are provided in Appendix B.9.
Table 2: Ablation study on informative outlier mining. We use DenseNet as network architecture. ↑ indicateslarger value is better, and ] indicates lower value is better. All values are percentages and are averaged over sixnatural OOD test datasets mentioned in section 5.1. We do not use OOD test set for tuning q. Please refer toTable 5 for validation results.
Table 3: The estimated average runtime for each result. We use DenseNet as network architecture. h meanshour. For MSP, ODIN, and Mahalanobis, we use standard training. The evaluation includes four OOD detectiontasks listed in Section 2.
Table 4: Common OOD detection methods and a family of natural and perturbed OOD examples we considered.
Table 5: Evaluate models on validation dataset. We use DenseNet as network architecture. ↑ indicates largervalue is better, and ] indicates lower value is better. All values are percentages and are averaged over six differentOOD test datasets mentioned in section 5.1. Bold numbers are superior results.
Table 6: Comparison with competitive OOD detection methods. We use ImageNet-RC as the auxiliary OODdataset (see section B.1 for the details) for SOFL, OE, ACET, CCU, NTOM and ATOM. We use DenseNet asnetwork architecture for all methods. We evaluate on four types of OOD inputs: (1) natural OOD, (2) corruptionattacked OOD, (3) L∞ attacked OOD, and (4) compositionally attacked OOD inputs. ↑ indicates larger value isbetter, and ] indicates lower value is better. All values are percentages and are averaged over six natural OODtest datasets described in section 5.1. Bold numbers are superior results.
Table 7: Comparison with competitive OOD detection methods. We use WideResNet as network architecturefor all methods. We evaluate on four types of OOD inputs: (1) natural OOD, (2) corruption attacked OOD,(3) L∞ attacked OOD, and (4) compositionally attacked OOD inputs. ↑ indicates larger value is better, and ]indicates lower value is better. All values are percentages and are averaged over six different OOD test datasetsdescribed in section 5.1. Bold numbers are superior results.
Table 8: Evaluation on L∞ attacked OOD and compositionally attacked OOD inputs with strong PGD attack(100 iterations and 5 random restarts). We use DenseNet as network architecture for all methods. ↑ indicateslarger value is better, and ] indicates lower value is better. All values are percentages and are averaged over sixdifferent OOD test datasets described in section 5.1. Bold numbers are superior results.
Table 9: Comparison with competitive OOD detection methods. We use SVHN as in-distribution dataset anduse DenseNet as network architecture for all methods. We evaluate the performance on all four types of OODinputs: (1) natural OOD, (2) corruption attacked OOD, (3) L∞ attacked OOD, and (4) compositionally attackedOOD inputs. ↑ indicates larger value is better, and J indicates lower value is better. All values are percentages.
Table 10: Comparison with competitive OOD detection methods. We use CIFAR-10 as in-distribution datasetand use DenseNet as network architecture for all methods. We evaluate the performance on all four types ofOOD inputs: (1) natural OOD, (2) corruption attacked OOD, (3) L∞ attacked OOD, and (4) compositionallyattacked OOD inputs. ↑ indicates larger value is better, and J indicates lower value is better. All values arepercentages.
Table 11: Comparison with competitive OOD detection methods. We use CIFAR-100 as in-distribution datasetand use DenseNet as network architecture for all methods. We evaluate the performance on all four types ofOOD inputs: (1) natural OOD, (2) corruption attacked OOD, (3) L∞ attacked OOD, and (4) compositionallyattacked OOD inputs. ↑ indicates larger value is better, and J indicates lower value is better. All values arepercentages.
Table 12: The performance of OOD detector and classifier on in-distribution test data. We use DenseNet for allmethods. We use three metrics: FNR, Prediction Accuracy and End-to-end Prediction Accuracy. We pick thethreshold for the OOD detectors such that 95% of in-distribution test data points are classified as in-distribution.
Table 13: The performance of OOD detector and classifier on in-distribution test data. We use WideResNet forall methods. We use three metrics: FNR, Prediction Accuracy and End-to-end Prediction Accuracy. We pick thethreshold for the OOD detectors such that 95% of in-distribution test data points are classified as in-distribution.
Table 14: Comparison with competitive OOD detection methods. We use SVHN as in-distribution dataset anduse DenseNet as network architecture for all methods. We evaluate the performance on all four types of OODinputs: (1) natural OOD, (2) corruption attacked OOD, (3) L∞ attacked OOD, and (4) compositionally attackedOOD inputs. ↑ indicates larger value is better, and J indicates lower value is better. All values are percentages.
Table 15: Comparison with competitive OOD detection methods. We use CIFAR-10 as in-distribution datasetand use DenseNet as network architecture for all methods. We evaluate the performance on all four types ofOOD inputs: (1) natural OOD, (2) corruption attacked OOD, (3) L∞ attacked OOD, and (4) compositionallyattacked OOD inputs. ↑ indicates larger value is better, and J indicates lower value is better. All values arepercentages. Bold numbers are superior results.
Table 16: Comparison with competitive OOD detection methods. We use CIFAR-100 as in-distribution datasetand use DenseNet as network architecture for all methods. We evaluate the performance on all four types ofOOD inputs: (1) natural OOD, (2) corruption attacked OOD, (3) L∞ attacked OOD, and (4) compositionallyattacked OOD inputs. ↑ indicates larger value is better, and J indicates lower value is better. All values arepercentages. Bold numbers are superior results.
