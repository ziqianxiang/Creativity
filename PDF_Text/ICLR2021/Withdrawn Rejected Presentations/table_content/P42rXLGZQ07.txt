Table 1: Denoising perfor-	σ=15		σ二25	σ=50TTizi nep irɪ PWNR (dR∖ fnr mance n	() or								the ‘house’ image under con-	MTMKL	34.29	31.88	28.08trolled conditions (D=8×8,	GSC	32.68	31.10	28.02H=64 for all algorithms).	var-BSC TVAE	32.25 34.27 ± .02	31.15 32.65 ± .06	28.62 29.61 ± .02timization based on Gaussian Processes (Nogueira, 2019). We will refer to the binary VAE trainedwith the method described above as Truncated Variational Autoencoder (TVAE) as the use of trun-cated posteriors is the main distinguishing feature.
Table 2: Denoising perfor- mance in PSNR (dB) for the		σ=15	σ二25	σ=50	n2v? MTMKL GSC S5C	32.05 34.29 33.78 33.50	29.20 31.88 32.01 32.08	25.42 28.08 28.35 28.35‘house’ image for different al- gorithms with different opti- mized hyper-parameters. The				top category only requires	VAR-BSC	33.50	32.32	28.91the noisy image. The mid- dle requires additional infor-	TVAE	34.27 ± .02	32.65 ± .06	29.98 ± .05	n2vt	33.91	32.10	28.94mation such as noise level				(KSVD, WNNM, BM3D) or additional noisy images with matched noise level (n2v*).	KSVD WNNM BM3D	34.32 35.13 34.94	32.15 33.22 32.86	27.95 30.33 29.37	EPLL	34.17	32.17	29.12The bottom requires large				clean datasets.	BDGAN	34.57	33.28	30.61	DPDNN	35.40	33.54	31.04increase with the number of data points. Above, we processed 100, 000 data points which is stillfeasible for the small DNNs used. However, larger DNNs increase computational load significantly(n)	(n)because N X (∣Φ | + ∣Φnew|) latent states have to be passed through the decoder. Furthermore,larger DNNs require more data points to not overfit which further increases computational loadof our N -dependent method. In many applications, there is, however, anyway relatively few dataavailable which makes the application of large DNNs prohibitive. One example is the task of ‘zero-shot’ denoising, i.e., denoising ofan image when only the image itself is available. Learning without
Table 3: Hyper-parametersfor the denoising experimentson the house image.
Table 4: Denoising performance of n2v in PSNR (dB) forthe ‘house’ image with AWG noise. For comparison, weadditionally list the performance of TVAE (numbers copiedfrom Tab. 2). PSNR values for n2v? are obtained by train-ing only on the noisy image (i.e., in the same setting as usedfor MTMKL, GSC, var-BSC and TVAE in Tab. 2. Moretraining data improves performance for n2v. PSNR valuesfor n2v* show performance if additional training data in theform of noisy images with AWG noise σ = 25 is used. Fur-ther improvements (especially for high noise) are obtainedif the n2v network is trained on training data with a noiselevel that matches the noise of the test set (see n2v) Forinstance, We used for n2v^ training data with σ = 50 to de-noise the ‘house’ with σ = 50. See text for further details.
