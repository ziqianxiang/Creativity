Table 1: Instance segmentation results on PartNet (part-category mAP%, IoU threshold 0.5,fine(3), middle(2), and coarse(1)-grained).
Table 2: Ablation study. Center, ExtDim, Prob refer to our proposed center-aware loss for theclustering step, the 6D deterministic embedding, and our proposed probabilistic embedding. Anisoand Hetero refer to the choice of Gaussian: anisotropic and heteroscedastic. AllAvg means takingall levels of granularity and categories into consideration. Large means fine-grained level of fourlargest categories. Others means fine-grained level of all the other categories. Here we also list theresults on four largest categories of fine-grained level. The top two results are marked bold.
Table 3: Activations for different output branches	Output	Activation	Centers	oi ∈ R3	μi 二	xi + tanh oiUncert	σi ∈ R3	σi	二 exp σiScores	Pi ∈ R3	Pi 二	s softmax(pi)2.	Second, we look at the inner summation of Eq. 10,LScore1NN Xi=1L X -Q[i,i]iog P[i,i]l=1(14)The inner summation inside the round bracket is the cross entropy between Q[i, :] andP[i, :]. And it is equivalent to replacing the one-hot vector in Equation 12 with Q[i, :]. Also,it is the form of label smoothing, a commonly used training trick in image classification(Szegedy et al. (2016); He et al. (2019)). The closer Q[i, :] is to a one-hot vector, the moreconfidence we give to the classification loss of point xi. By definition of Q[i, l], it can beeasily seen that the resulting classifier only classifies near-centers points correctly. Thuswe call our new loss function the center-aware loss.
Table 4: Instance segmentation results on PartNet. The metric is mAP (%) with IoU threshold0.25.
Table 5: Instance segmentation results on PartNet. The metric is mAP (%) with IoU threshold0.75.
Table 6: Results on ScanNet. We list the results on both validation and hidden test sets of ScanNet.
