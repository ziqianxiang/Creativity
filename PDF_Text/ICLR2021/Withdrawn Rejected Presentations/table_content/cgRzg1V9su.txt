Table 1: Error rate mean and std of IEN, maxout and the original model design on different deepmodel architectures. The lower, the better. The subscript we indicates results using the weightdownsizing method from section 4.2. +FC stands for IEN applied on both FC and CNN layers.
Table 2: Results of outer-ensemble of the models. Each model was trained 3 times and the predictionswere averaged. The metric reported is the mean of the error rate. The lower the better. The subscriptwe indicates results with the weight downsizing method from section 4.2. +FC stands for IEN appliedon Fully Connected layer. All IENs and Maxouts have m = 4. The base model ensemble results arefrom 4 trained models.
Table 3: Accuracies of four cell models based on NAS-Bench-201 configuration, using our discoveredcells and the original NAS-Bench-201 cells.
Table 4: To Reviewer#1: Mean error rate and std reported. Different variations are discussed.
Table 5: To Reviewer#4: Mean accuracy on CIFAR-10C and CIFAR-100C datasets (Hendrycks &Dietterich, 2019).
Table 6: To Reviewer#4: Comparison with SWA (Izmailov et al., 2018).
Table 7: To Reviewer#3 Avg pool layer applied after each CNN, same place where IEN was applied.
Table 8: To Reviewer#3 & #2. Accuracy on CIFAR-10 dataset.
Table 9: Model parameters count / Average training time per epoch in seconds. M stands for Million,K stands for Thousand. All models were trained on Nvidia V100 GPU. Models in italic title have thesame size at testing time. +FC stands for IEN applied on both FC and CNN layers. Maxout resultsare only on CNN layers. All models have m = 4Dataset	ResNet56		ResNet110	DenSeNet40-12	DenseNet100-12	VGG16	VGG19	IEN	4.25M / 55.16	8.60M / 97.17	854.94K/102.79	3.74M / 239.49	74.11M / 26.62	100.66M / 32.34CIFAR-10	IEN+FC	4.25M / 55.9	8.61M / 105.38	860.26K / 101.01	3.75M / 244.74	76.23M / 27.46	102.79M / 32.93	Maxout	4.25M / 57.45	8.60M / 110.53	854.94K / 106.33	3.74M / 244.74	74.11M / 28.20	100.66M / 33.98	Base	853.02K / 17.16	1.73M / 42.01	176.12K/51.96	769.16K / 115.16	15.25M / 7.61	20.57M / 9.21	IEN	4.25M / 58.56	8.61M / 94.75	866.91K/96.13-	3.77M / 239.63	74.16M / 26.68	100.71M / 32.29CIFAR-100	IEN+FC	4.28M / 58.30	8.64M / 96.38	920.11K / 98.71	3.75M / 244.74	76.46M / 27.32	103.02M / 33.10	-MaXOUt	4.25M / 62.64	8.61M / 111.1	866.91K / 104.53	3.77M / 253.85	74.16M / 28.18	100.71M / 34.02	Base	858.87k / 20.01	1.73M / 36.28	188.09K / 47.73	800.3K / 114.13	15.30M / 7.62	20.61M / 9.20From Table 9 we notice that Maxout training time is always more than IEN. There is also an increasein both training time and parameters size of IEN+FC against IEN only. Training time of IEN withm = 4 is much cheaper than training 4 ordinary models, but it comes at the cost of the memory onlyduring the training time.
Table 10: Results of applying dropout everywhere except the input and output layer. Mean error rateand std for multiple runs are reported. The lower the better. IEN uses m = 4.
