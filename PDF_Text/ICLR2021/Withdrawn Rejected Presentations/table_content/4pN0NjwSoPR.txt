Table 1: Comparisons between DDQ and state-of-the-art quantizers on ImageNet. “W/A” means bitwidthof weight and activation respectively. Mixed precision approaches are annotated as “mixed”. "-" denotes theabsence of data in previous papers. We see that DDQ outperforms prior arts with much less training epochs.
Table 2: Comparisons between PACT(Choi et al., 2018)+UQ, PACT(Choi et al., 2018)+PoT and DDQ onImageNet. "DDQ (fixed)" and "DDQ (mixed)" indicate DDQ trained with fixed / mixed bitwidth. We see thatDDQ+mixed surpasses all counterparts.
Table 3: Ablation studies of adaptive resolution and gradient correction. “UQ” and “PoT” denote uniform andpower-of-two quantization respectively. “DDQ(fix)+GradCorrect” refers to DDQ with gradient correction butfixed bitwidth. “-” denotes training diverged. “4/8” denotes training with 4-bit weights and 8-bit activations.
Table 4: Overall summary of state-of-art quantiztaion methods. "Differentiablity" column showswhether this method can be implemented with one-stage and gradient-based methods. "UQ" and"Non-UQ" indicate uniform / non-uniform quantization respectively. "Step Size" column denotesthe ability to adjust quantization step size. "Quantizer Calibration" means if the method calibratesthe quantizer with centre points and thresholds. "Gradient Calibration" shows if the quantizationgradients for parameters in quantizer are corrected.
Table 5: Comparison of Quantized MobileNetv2 runing on mobile DSPs. 1 Fixed precision DDQ. 2 Mixedprecision DDQ with uniform quantizer constraints. 3 Original DDQ. “w/a” means the bitwidth for networkweights and activations respectively.
Table 6: Comparison of Cifar-10 Top1-accuracy towards existing quantization methods. All the reported resultsuse 32-bit activation by following prior work.
