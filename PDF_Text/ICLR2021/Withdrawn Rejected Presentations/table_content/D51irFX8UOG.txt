Table 1: Examples and results of generalization tests (- indicates no problem is dynamically generated)Models & ResultsTest Type & Examples	SYMBOLIC (max_opt_len=5)	VISUAL (max_opt_len=l)% MLP LSTM TRAN TRAN+LSTM CNN+MLP CNN+TRAN SPACE		Pa I	5.22+4.11	12.12+2.14	14.57+6.77	13.05+3.09	14.39+7.22	10.29+2.61	16.45+2.65H	Training problems	Pg T	99.23+0.63	57.22+3.07	93.85+1.26	72.33+5.79	75.76+4.77	58.33+4.19	16.33+0.94		Pp T	71.67+1.73	50.91+3.54	67.89+0.63	63.97+5.84	63.77+2.68	35.31+3.00	12.02+1.17		Pa I	37.02±1.52	23.91+2.10	34.85+4.45	37.69+2.90	86.70±2.30	56.91+7.92	58.38+1.20	Random split	Pg T	51.00+2.21	57.78+3.49	82.82+0.96	54.00+2.94	7.58+0.43	14.00+4.24	3.67+0.47		Pp T	54.91+2.85	45.15+1.46	58.07+1.01	40.13+2.52	5.09 + 1.17	8.33+1.96	2.66+0.19	(3,5, I , 7)P MEM,	Pg T	55.00+7.07	50.00+8.16	41.67+8.50	66.67+13.12	0.00+0.00-	0.00+0.00	0.00+0.00H	test(3,5,4, d〉R MEM.	Pp T	19.90+2.18	24.02+7.20	16.34+3.90	35.74+5.85	0.00+0.00	0.00+0.00	0.00+0.00S	<^^pkBS；	Pg T	25.00+8.16	63.33+6.24	43.33+6.23	78.33+2.36	0.00+0.00-	0.00+0.00	0.00+0.00	test (3,5〉R KBs.	Pp T	7.37+2.33	26.31+2.34	12.22+1.83	34.79+4.25	0.00+0.00	0.00+0.00	0.00+0.00	-5=3 + <: 口 P KBa_c,	Pg T	41.67+2.36	60.00+10.80	36.67+8.50	58.33+10.27	0.00+0.00-	0.00+0.00	0.00+0.00	test (3, MyR KBs.	Pp T	15.10+0.35	28.91+7.62	14.01+3.75	27.11+2.12	0.00+0.00	0.00+0.00	0.00+0.00	{S = ▲: (⅛ + 0,3 = ▲:目}	Pg T	31.67+8.50	45.00+10.80	43.33±6.24	71.67+6.24	0.00+0.00-	0.00+0.00	0.00+0.00巳	U KBa_c, test ( 3,5〉R KBs.	Pp T	11.68+3.34	17.15+5.82	17.86+3.02	35.40+3.71	0.00+0.00	0.00+0.00	0.00+0.00<	5 =	+ ▲ ： Q P KBa_c,	Pg T	6.67+2.36	100.00+0.00	25.00+0.00	-	0.00+0.00-	0.00+0.00	0.00+0.00	test (3, MyR KBs.	Pp T	1.48+0.52	51.86+0.18	5.83+0.24	-	0.00+0.00	0.00+0.00	0.00+0.00
Table S1: Hyper-parameters of TD3Hyper-parameters	ValueOptimizer	Adam (Kingma & Ba, 2014)Learning rate for actor	1e-4Batch size	128of Adam	1e-8Discounting factor	0.95Initial for -greedy	0.1Ending for -greedy	0.95Decay steps for -greedy	100,000Policy update delay	5Target update rate	0.995Replay buffer size	10,000E.2 Architecture of AgentsFigure S7: Architecture of the actor model, where T is equal to max_opt_len.
Table S2: Architectural parameters of evaluated agents	Agent	ArchitectureShared Nonlinearity	ReLUMLP Agent Encoder Decoder	MLP with hidden units [128,128]. NoneLSTM Agent Encoder Decoder	MLP with hidden units [128,128]. LSTM with layer normalization (Ba et al., 2016) and hidden units [128].
Table S3: Task parameters of HALMATask parameters	ValueMaximum #steps in an episode (L)	500Maximum #trials in an episode (N)	10Maximum #steps in a trial(H)	200Discounting factor (γ)	0.95Goal reward (Rg)	100Penalty on invalid action (Ra)	-5Penalty on invalid action (Ra)	-5Auxiliary rewards (Rx)	1.0^(Lm(agenttτ, goal) ´Lm(agents goal)), where Lm is the Manhattan distance.
Table S4: Accuracy of color and MNIST category classification.
