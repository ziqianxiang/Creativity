Table 1: Comparison with concatenating all embeddings and random search baselines on 6 tasks.
Table 2: Comparison with state-of-the-art approaches in NER and POS tagging. *: Models aretrained on both train and development set. t Models are trained with document information. â—Š:Results are from Conneau et al. (2020).
Table 3: Comparison with state-of-the-art approaches in chunking and aspect extraction. *: Wereport the results reproduced by Wei et al. (2020).
Table 4: Comparison with state-of-the-art approaches in DP and SDP. 1: For reference, they addi-tionally used constituency dependencies in training. t For reference, We confirmed with the authorsof He & Choi (2020) that they used a different data pre-processing script with previous work.
Table 5: Comparison of rewardfunctions.
Table 6: A comparison among All, Random, ACE, Table 7: Results of models with documentAll+Weight and Ensemble. CHK: chunking. context on NER. +sent/+doc: models withsentence-/document-level embeddings.
Table 8: The embeddings we used in our experiments. The URL is where we downloaded theembeddings. Note that we have confirmed that the XLM-R models fine-tuned on CoNLL 2002/2003datasets are only trained on the training data.
Table 9: A comparison between ACE and the fine-tuned embeddings that are used in ACE for NERand POS tagging.
Table 10: A comparison between ACE and the fine-tuned embeddings we used in ACE for chunkingand AE.
Table 11: A comparison between ACE and the fine-tuned embeddings that are used in ACE for DPand SDP.
Table 12: A comparison among retrained models, All and ACE. We use the one dataset for eachtask.
Table 13: The percentage of each embedding candidate selected in the best concatenations fromACE. F and MF are monolingual and multilingual Flair embeddings. We count these two em-beddings are selected if one of the forward/backward (fw/bw) direction of Flair is selected in theconcatenation. We count the Word embedding is selected if one of the fastText/GloVe embeddingsis selected. SS: sequence-structured tasks. GS: graph-structured tasks. Sem.: Semantic-level tasks.
