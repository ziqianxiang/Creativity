Table 1: Comparison between MSGD and SNGM for a L-smooth objective function. C denotes thecomputation complexity (total number of gradient computation).
Table 2: Experimental results on CIFAR10. In LARS with warm-up, we adopt the gradual warm-upstrategy and a power of 2, which is the same setting as that in You et al. (2017). In the warm-up stage (5 epochs), the learning rate increases from 0.1 to the target (2.4 in ResNet20 and 2.8 inResNet56) gradually.
Table 3: Experimental results on ImageNet.
