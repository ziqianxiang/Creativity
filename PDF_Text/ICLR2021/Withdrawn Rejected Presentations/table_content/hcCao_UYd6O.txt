Table 1: Comparison of robust accuracy against various attacks on different datasets. For allattacks We used e = 0.3 and 蔡 for MNIST and CIFAR10/CIFAR100 datasets respectively. findicates replicated results. NT: natural training; AT: adversarial training; AFD: adversarial featuredesensitization; WB: White-box attack; BB: black-box attack Where the adversarial examples Wereproduced by running the attack on the NT ReSNet18 model. Numbers reported with μ ± σ denotemean and std values over three independent runs With different random initialization. * RST(Carmonet al., 2019) additionally uses 500K unlabeled images during training.
Table 2: AUC measures for different perturbations and methods on MNIST, CIFAR10, and CIFAR100datasets. AUC values are normalized to have a maximum allowable value of 1. Evaluations on ATand TRADES were made on networks trained using reimplemented or official code.
Table A1: Training hyperparameters for each dataset and network.
Table A2: Attack hyperparameters for each dataset and attack.
Table A3: Comparison of robust accuracy against AutoAttack (Croce & Hein, 2020), Boundary attack(Brendel et al., 2018) with 5000 steps and = 2, and B&B attack (Brendel et al., 2019). We testedthe robust performance of each model on 100 random samples from each dataset’s test-set.
Table A4: Transfer black-box attack from ResNet18 network trained with adversarially training (AT)and TRADES on different datasets.
Table A5: Dimensionality of the learned representation space on various datasets using differentmethods and measures. Units: number of non-zero feature dimensions over the test-set within eachdataset. Dims: number of PCA dimensions that account for 99% of the variance across all imageswithin the test-set of each dataset.___________________________________Dataset	MNIST		CIFAR10		CIFAR100	Network	RN18		RN18		RN18		Units	Dims	Units	Dims	Units	DimsNT	64	24	512	97	512	429AT	64	43	512	455	512	481TRADES	64	40	512	349	512	461AFD	18	6	417	9	500	76Table A6: Comparison of robust accuracy against PGD-L∞ with = 0.3 using different architecturesfor the adversarial discriminator, tested on MNIST dataset.
Table A6: Comparison of robust accuracy against PGD-L∞ with = 0.3 using different architecturesfor the adversarial discriminator, tested on MNIST dataset.
Table A7: Comparison of representation perturbations in response to different attacks. We computedthe cosine angle between representation perturbations due to each attack to those from PGD-Linf .
