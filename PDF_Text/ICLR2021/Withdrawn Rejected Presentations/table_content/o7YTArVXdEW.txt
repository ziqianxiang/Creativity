Table 1: Neighbor Cluster Results on Training SetCIFAR10	CIFAR100-20	STLMethods	10	50	100	10	50	100	10	50	100^^Rot + KNN	65.58	62.06	60.08^^	^^51.76	51.41	50.38^^	^^60.69	5471 ^^	50.02InstDisc 1 + KNN	74.73	73.96	73.08	56.66	56.41	54.28	71.52	61.82	55.38InstDisc 2 + KNN	71.33	67.78	64.38	57.76	56.36	55.68	66.24	59.71	54.78MoCo + KNN	75.28	70.72	68.27	56.92	56.49	54.83	71.97	57.66	59.89SimCLR + KNN	81.53	76.76	73.58	61.56	54.05	49.33	75.82	67.11	62.58VAeTkNN	75.71	73.34	72.32^^	^^59.27	52.04	48.38^^	^^74.71	72.83.08	58.92AC-VAE	95.25	87.72	82.27	68.75	65.51	62.48	92.75	87.93	85.42datasets. For all experiences, the same set of configurations is applied. A ResNet-34 is adopted asthe encoder network, and a two-layer full connection is utilized as the decoder network. The latentdistribution dimension is set as 512, and the decoded representation vector has a size of 64. Both theencoder and the decoder networks are initialized randomly. Besides, the NT_Xent loss (Chen et al.,2020a) and its augmentation strategy are used for the behavior loss implementation.
Table 2: The a and θ applied in the experimentsDataset Cluster Size	CIFAR10			CIFAR100-20				STL				10	50	100	10	50	100	10	50	100α	-052	0.94-	T28	T45-	1.62	^Γ72^^	057^	1.24	1.42	θ		.N/A	N/A	WA	WA		0.91	N/A	_^9_	To get the results mentioned in Table 1, the a and θ listed in Table 2 are applied. However, thesesuper parameters are precisely selected for easy compassion to the KNN based clustering results.
Table 3: The UnsuPervised Classfication resultsMethods	CIFAR10	CIFAR100-20	STLDEC (Xie et al.,2016)	^301	"T85	35.9ADC (Haeusser et al., 2018)	32.5	16.0	53.0DeepCluster (Caron et al., 2018)	37.4	18.9	33.4DAC (Chang et al., 2017)	52.2	23.8	47.0IIC (Ji et al., 2019)	61.7	25.7	59.6SCAN with KNN clustering	^"876	"ɪ^	76.7SCAN With AC-VAE	79.2	40.2	71.84.4	Other ADVANTAGESThe proposed method also has additional advantages, desired in the unsupervised setup, such asprototype selecting. As different representation has different boundary information, some sampleswill cluster far more neighbors than others under the same z-score range, so each class's prototypecan be easily identified as those who cluster the most neighbors. The selected prototypes of eachdataset are shown in Figure 6.
