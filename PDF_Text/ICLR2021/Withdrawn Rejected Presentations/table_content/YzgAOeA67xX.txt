Table 1: Test performance comparison of optimizers. We report the mean and the standard deviations(as the subscripts) of the optimal test errors computed over three runs of each experiment. AdamSgeneralizes better than popular adaptive gradient methods significantly and often compares favorablywith the baseline optimizer SGD.
Table 2: Test performance comparison of Adai, AdaiS, SGD, and SGDS. Stable/Decoupled WeightDecay often outperform L2 regularization for optimizers involving in momentum. We report themean and the standard deviations (as the subscripts) of the optimal test errors computed over threeruns of each experiment.
Table 3: Test performance comparison of optimizers with λL2 = λS = 0.0001 and λW = 0.1.
