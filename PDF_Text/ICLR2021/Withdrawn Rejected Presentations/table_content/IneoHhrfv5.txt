Table 1: (a) 2D vs 3D quantitative comparison. Eexp, Eldmk , PSNR, and SSIM comparison of 2D and3D parameter regression. (b) ID-removing quantitative comparison. “BL” is the 3D parameter regressionbaseline; “IR” is “id-removing”.
Table 2: ID-removing & Deflicker quantitative comparison. “BL” is the 3D parameter regression baseline;“IR” is “id-removing”; “DF” is “deflicker”. The metrics validate the effectiveness of the proposed componentsexcept for the deflicker algorithm. The deflicker algorithm mainly focus on removing temporal discontinuitythat can be viewed in the supplementary video.
Table 3: Training data size and training scheme. PSNR/SSIM of different amount of training data andtraining scheme. “Joint” means joint training one generator for all speakers and “Split” means training multipleperson-specific generators separately. One generation network contains 75 million parameters and N (N = 4in the table) is the speaker number.
Table 4: User study compared with ground truth videos. Results on generated and ground truth video clipsfor 7 poses.
Table 5: User study compared with state-of-the-art methods. Comparsion of our methods and recent state-of-the-art methods on realisticness, lip-sync accuracy and talking naturalness.
Table 6: Comparison on GRID dataset. SSIM, PSNR and Eldmk results of recent state-of-the-art methodsand ours. For a fair comparison, we generate the full face and do not apply any post-processing(e.g. temporalflicker removal, teeth proxy).
Table 7: Architecture of the Neural Video Rendering NetworkLayer Name	Resolution	Layer StructureInput	384 × 384	Sequences of Maked Face and HeatmaPE1	192 × 192	EncBlock ((3 + 1) × 7) → 64 + LeakyReLU(0.2)E2	96 × 96	EncBlock 64 → 128 + LeakyReLU(0.2)E3	48 × 48	EncBlock 128 → 256 + LeakyReLU(0.2)E4	24 × 24	EncBlock 256 → 512 +LeakyReLU(0.2)E5	12 × 12	EncBlock 512 → 512 + LeakyReLU(0.2)E6	6 X 6	EncBlock 512 → 512 + ReLUE7	3 × 3	EncBlock 512 → 512 + ReLUD7	6 × 6	DecBlock 512 → 512 + ReLUD6	12 × 12	SkiP(E6)+DecBlock (512 + 512) → 512 + ReLUD5	24 × 24	SkiP(E5)+DecBlock (512 + 512) → 512 + ReLUD4	48 × 48	SkiP(E4)+DecBlock (512 + 512) → 256 + ReLUD3	96 × 96	SkiP(E3)+DecBlock (256 + 256) → 128 + ReLUD2	192 × 192	SkiP(E2)+DecBlock (128 + 128) → 64+ ReLUD1	384 × 384	SkiP(E1)+DecBlock (64 + 64) → 3 × 7Table 8: Architecture of the Discriminator NetworkLayer Name	Resolution	Layer StructureInPut	384 × 384	Sequences of Generated/Real Face Images
Table 8: Architecture of the Discriminator NetworkLayer Name	Resolution	Layer StructureInPut	384 × 384	Sequences of Generated/Real Face ImagesE1	192 × 192	EncBlock (3 × 7) → 64 + LeakyReLU(0.2)E2	96 × 96	EncBlock 64 → 128 + LeakyReLU(0.2)E3	48 × 48	EncBlock 128 → 256 + LeakyReLU(0.2)E4	24 × 24	EncBlock 256 → 512 +LeakyReLU(0.2)E5	12 × 12	EncBlock 512 → 512 + LeakyReLU(0.2)E6	6 × 6	EncBlock 512 → 512 + ReLUE7	3 × 3	EncBlock 512 → 512 + ReLUOutPut	1 × 1		AvgPool + FC 512 → 1 × 7	E	Frame Selection AlgorithmIn the generated video, our generated mouth region is composited into face frames of a long videofootage. Our Frame Selection Algorithm aims at selecting face frames based on input audio to ensurethe head motion looks natural with the input audio. For example, it prefers more head motion andeyes blink during utterance (Suwajanakorn et al., 2017). Our Frame Selection Algorithm modifiesthe Video Re-timing Algorithm of Audio2Obama (Suwajanakorn et al., 2017) to adopt for our settingof multiple speakers . For more algorithm design considerations, please refer to the Video Re-timing Algorithm of Audio2Obama (Suwajanakorn et al., 2017). Here we formulate the dynamicprogramming objective and present the pseudo-code of our Frame Selection Algorithm.
