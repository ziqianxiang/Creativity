Table 1: Experimental environments and shaping reward functionsEnvironment	Shaping FunctionsGym - CartPole	φι = —10 ∙ pole_angle φ2 = -CartqositionGym - Pendulum	φi = Cos(Pole_angle) φ2 = -∖polejvelocity∖ ∙ Cos(Pole_angle)Gym - MountainCar	φi = CarJPos社 ion φ2 = ∖carjvelocity∖ ∙ 10 + car ^position/10MuJoCo - Reacher	0	— —distance distance ≤ 1.5 1	0	distanCe > 1.5 0	O0	distance ≤ 1.5 2	-distance distance > 1.5car horizontal position and velocity and was effective at leading the agent to the solution. For theReacher environment, each shaping reward signal provided information leading to the goal pointbut in different regions of the state space. The selector was required to learn when to listen to eachsignal. Further mathematical details are described in Table 1.
Table 2: Episodes until convergence for each automatic reward adaptation method (value of con-verged reward in parentheses)	Selection	FuCartPole	602 (1000.0)	849 (995.31)MountainCar	862 (-117.27^T	964 (-111.62T7Under review as a conference paper at ICLR 2021Figure 2: Comparisons in performance between our selection idea and the automatic reward adap-tation method presented in Fu et al. (2019)When applied to these versions of the problems, the selector was still able to identify the correctshaping reward signals to listen to. The selection agent converged to the correct policy in both theCartPole and MountainCar problems, often faster than the Fu agent. In CartPole, selection arrivedat a better solution than Fu earlier in learning. In MountainCar, selection learned a policy that per-formed slightly worse than that learned by Fu’s method, but discovered it earlier. Later in learning,the selection agent maintained competitive performance despite lacking the extra time to computeoptimal usage of available shaping reward signals. After reaching convergence, the selection agent’sperformance remained close to that of the Fu agent. While there was some fluctuation in the pol-icy, the selection method was consistently able to adjust the agent back towards the optimal policy.
Table 3: Performance Hyperparameters for each Environment	CartPole	Pendulum	MountainCar	ReacherCritic/DQN Learning Rate	一	0.001	-0.002-	0002	0.002Actor Learning Rate	-N/A-	-0.001-	0.001	0.001Selector Learning Rate	0.002	-0:01-	0003	0.003Target Network Update Rate	-N/A-	-0005-	0005	0.005Discount Factor	-095-	-099-	0.99	-095Exploration Factor	-0.9-	02	09	-0.2-Exploration Factor Decay Rate	-095-	-N/A-	N/A	-N/A-Batch Size	64-	64	64	-64-Replay Memory Size	50000	50000	50000	50000Optimizer	Adam	Adam	Adam	AdamDDPG used the Ornstein-Uhlenbeck process for exploration noise generation. Thus, the ExplorationFactor was actually the standard deviation for the process instead of the standard epsilon factor usedin, for example, DQN. For the same reason, the Exploration Factor Decay Rate for DDPG was listedas N/A.
