Table 1: Prototypical Networks fine-tuned on ImageNet (‘Standard’) with the addition of L2 regular-ization on the batch normalization weights (‘L2 BN’), EST (Cao et al., 2020), the ‘Fine-tune on bestk-shot’ baseline (‘Best k-shot’) and SCONE , including the ablation that omits the shot smoothing(‘SCONE w/o S’). The reported numbers are query set accuracies averaged over 600 test episodesand 95% confidence intervals. We also show the average ranks (lower is better). We report details inthe Appendix on rank computation and statistical testing.
Table 2: Our reproduction of the Classifier-Baseline (Chen et al., 2020) trained on all datasets, andtwo variants that freeze those weights and episodically fine-tune using Meta-Baseline (Chen et al.,2020) to optimize either only the batch norm parameters (‘Control’), or only SCONE ’s parameters(‘SCONE ’). In all cases, the reported numbers are query set accuracies averaged over 1K testepisodes and 95% confidence intervals. We also show the average ranks (lower is better).
Table 3: Comparison of our best SCONE model to recent state-of-the-art approaches on Meta-Dataset(for the setting of training on the training sets of all datasets).
Table 4: The same table as Table 1 additionally annotated with per-row ranks.
Table 5: The same table as Table 2 additionally annotated with per-row ranks.
Table 6: The same table as Table 3 additionally annotated with per-row ranks.
