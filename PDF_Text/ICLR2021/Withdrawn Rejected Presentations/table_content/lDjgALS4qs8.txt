Table 1: Multi-BLEU scores on De-En, En-De, En-Fr and En-Ro. The baselines are Transformer-small and Transformer-base, respectively.
Table 2: Multi-BLEU scores of ablations on De-En and En-De. #Para, #Speed, #Mem and #PPLdenote the size of model paragraphs, training speed (tokens/second), GPU memory model used (GB)and perplexity respectively.
Table 3: Results of NER and POS tagging	ROUGE-1	ROUGE-2	ROUGE-LTransformer (base)	3684	18.01	34.31Graph-Transformer (halfdim&gate)	37.38 (+0.54)~	18.59 (+0.58)	34.58 (+0.27)Table 4: Results of text summarization.
Table 4: Results of text summarization.
Table 5: Hyperparameters for our experiments. FF is short for feed-forward layer.
Table 6: Results of experiments on IWSLT 14 De-En about position-encoding. Word PE is short forposition-encoding.
