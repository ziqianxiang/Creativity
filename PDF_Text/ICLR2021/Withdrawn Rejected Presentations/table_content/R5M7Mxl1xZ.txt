Table 1: Classification accuracy for digits experiments.
Table 2: Parsing scores on Cityscapes. The scores with * are reproduced on a single GPU using thecodes provided by the authors. Qualitative results are given at the Appendix A.7.2.
Table 3: KID scores for style transfer tasks. The results of baselines (AGGAN (Tang et al., 2019) ,DRIT (Lee et al., 2019) , UNIT (Liu et al., 2017) , MUNIT (Huang et al., 2018)) are from (Kim et al.,2019). Here U (light) is the light Version ofU-GAT-IT.__________________________	Params	selfie2anime	horse2zebra photo2por	anime2selfie	zebra2horse	por2photoAGGAN	\	14.63±0.55	7.58±0.71 2.33±0.36	12.72±1.03	8.80±0.66	2.19±0.40DRIT	65.0M	15.08±0.62	9.79±0.62 5.85±0.54	14.85±0.60	10.98±0.55	4.76±0.72UNIT	\	14.71±0.59	10.44±0.67 1.20±0.31	26.32±0.92	14.93±0.75	1.42±0.24MUNIT	46.6M	13.85±0.41	11.41±0.83 4.75±0.52	13.94±0.72	16.47±1.04	3.30±0.47U-GAT-IT(full)	134.0M	11.61±0.57	7.06±0.8^^1.79±0.34	11.52±0.57	7.47±0.71	1.69±0.53U-GAT-IT(light)	74.0M	12.31±0.50	7.25±0.8 3.43±0.28	15.22±0.51	9.39±0.48	2.67±0.33U (light)+MGC	74.0M	10.37±0.32	5.19±0.46 3.19±0.26	10.30±0.47	7.80±0.48	2.18±0.26GAN+Contextual	588.1M	12.77±0.38	9.39±0.39 3.95±0.26	14.81±0.41	10.36±0.51	3.05±0.25GAN + MGC	14.1M	11.37±0.41	7.28±0.52 3.86±0.39	11.61±0.40	7.15±0.46	1.58±0.25CycleGAN	28.3M	13.08±0.49	8.05±0.72 1.84±0.34	11.84±0.74	8.0±0.66	1.82±0.36Cycle + MGC	28.3M	11.66±0.41	6.59±0.49 2.91±0.22	10.83±0.44	6.77±0.52	1.62±0.15GcGAN	16.9M	11.89±0.42	7.05±0.45 2.24±0.26	13.28±0.35	7.67±0.47	1.84±0.28GcGAN + MGC	16.9M	10.75±0.42	5.12±0.44 1.97±0.24	10.96±0.40	7.10±0.50	1.64±0.22CUT	18.1M	12.1±0.42	8.45±0.45 2.85±0.33	12.45±0.54	8.99±0.5	2.23±0.31CUT + MGC	18.1M	11.75±0.41	6.26±0.44	2.31 ±0.3	12.05±0.44	8.4±0.43	2.11±0.26Gc+Cycle+MGC	45.2M	10.61±0.44	4.82±0.68 1.64±0.24	10.92±0.35	6.28±0.52	1.31±0.27
Table 4: The results of User Study: the percentage of users prefer a particular model. To avoid theconcern of cherry-picking, qualitative results of U-GAT-IT and our results are used as the evaluationimages in the user study. Sample images are given in Appendix A.4.4.
Table 5: Sensitivity Analysis: the KID scores for different λmgc of the model CycleGAN + MGC inthe datasets horse2zebra and selfie2anime.
Table 6: Quantitative scores for Aerial photo→Map. ] indicates that the lower score is better and ↑denotes that the higher score is better.
Table 7: The network details of digits translation tasks, where C = Feature channel, K = Kernel size,S = Stride size, Deconv/Conv = Deconvolutional/Convolutional layer and "channels" donotes theimage channels of target domain, such as 1 for MNIST, 3 for MNIST-M.
Table 8: The network details of digits translation tasks, where C = Feature channel, K = Kernel size,S = Stride size, Deconv/Conv = Deconvolutional/Convolutional layer and ResBlk = A residual blockGeneratorindex	Layers	CKS1	Conv + ReLU2	Conv + ReLU3	Conv + ReLU4-9	ResBlk + ReLU10	Deconv + ReLU11	Deconv + ReLU12	Conv	3 7	113	Tanh	- --Discriminatorindex	Layers	CKS-1	Conv + LeakyReLU^^64~~4^^7T2	Conv + LeakyReLU	128	4	23	Conv + LeakyReLU	256	4	24	Conv + LeakyReLU	512	4	15	Conv	512	4	1Following all settings of the original models, the learning rate for all generators and discriminators is
Table 10: Falure cases on the horse2zebra dataset.
Table 11: Qualitative results on the Maps dataset.
Table 12: Qualitative results on the Cityscape Dataset.
Table 13:	Qualitative results on Selfie → Anime. Obviously, the geometrystructure, such as face shape, is better preserved by the translation modelfurther constrained by our MGC.
Table 14:	Qualitative results on photo → portrait. Obviously, the semanticinformation, such as face shape, is better preserved by the translationmodel further constrained by our MGC.
Table 15:	Qualitative results on Horse → Zebra. Obviously, the semantiCinformation, suCh as baCkground, is better preserved by the translationmodel further Constrained by our MGC.
Table 16: Qualitative comparisons on SVHN→MNIST.
Table 17: Qualitative comparisons on MNIST→MNIST-M.
