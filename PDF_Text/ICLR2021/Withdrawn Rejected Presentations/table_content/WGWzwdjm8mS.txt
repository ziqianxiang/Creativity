Table 1: The loss and area under the receiver operating characteristic curve (AUC score) on theMRNet test set (Bien et al., 2018), comparing 5-fold cross validation (5-fold CV) and gradient dis-parity (GD), when both are used as early stopping criteria for detecting the presence of abnormally,ACL tears, and meniscal tears from the sagittal plane MRI scans. The corresponding curves duringtraining are shown in Figure 10. The results of early stopping are given, both when the metric hasincreased for 5 epochs from the beginning of training and between parenthesis when the metric hasincreased for 5 consecutive epochs.
Table 2: The final test loss and accuracy when using gradient disparity (GD) and k-fold cross vali-dation (CV) as early stopping criteria, (top) when the available dataset is limited and (bottom) whenthe available data has noisy labeled samples. To simulate a limited-data scenario, we consider as atraining set a subset of 1280 samples of the CIFAR-100 dataset. The configurations in the top rowand the bottom row are ResNet-34 and ResNet-18, respectively. In both methods, the optimizationis stopped when the metric (validation loss or GD) increases for 5 epochs.
Table 3: The ratio of the magnitude of the network parameter vector at epoch t to the magnitudeof the network parameter vector at epoch 0, for t âˆˆ {0, GD, 200}, where GD stands for the epochwhen gradient disparity signals to stop the training.
Table 4: The loss and accuracy on the test set comparing 5-fold cross validation and gradient dispar-ity as early stopping criterion when the available dataset is limited. The corresponding curves duringtraining are presented in Figure 8. The above results are obtained by stopping the optimization whenthe metric (either validation loss or gradient disparity) has been increased for five epochs from thebeginning of training.
Table 5: The test accuracies achieved by using k-fold cross validation (CV) and by using gradientdisparity (GD) as early stopping criteria for different patience values. For a given patience value ofm, the training is stopped after m increases in the value of the validation loss in k-fold CV (top rows)and of GD (bottom rows). Throughout the paper, we have chosen m = 5 as the default patiencevalue for all methods without optimizing it even for GD. However, in this Table, we observe thateven if we tune the patience value for k-fold CV and for GD separately (which is indicated in bold),GD still outperforms k-fold CV.
Table 6: The loss and accuracy on the test set comparing 10-fold cross validation and gradient dis-parity as early stopping criterion when the available dataset is noisy. In all the experiments, 50%of the available data has random labels. The corresponding curves during training are presented inFigure 9. The above results are obtained by stopping the optimization when the metric (either vali-dation loss or gradient disparity) has been increased for five epochs from the beginning of training.
Table 7: The loss and AUC score on the test set, comparing 5-fold cross validation to gradientdisparity both as early stopping criterion for the MRNet dataset for three different tasks using thesagittal plane MRI scans. Note that an unassisted general radiologist gives on average 92%, 84%and 89% accuracy for detecting ACL tears, meniscal tears and abnormality, respectively (Bien et al.,2018). The corresponding curves during training are presented in Figure 10. We present the resultsof early stopping, both when the metric has increased for 5 epochs from the beginning of training,and in parenthesis when the metric has increased for 5 consecutive epochs.
Table 8: Test error (TE) and test loss (TL) achieved by using various metrics as early stoppingcriteria. On the leftmost column, the minimum values of TE and TL over all the iterations are re-ported (which is not accessible during training). The results of 5-fold cross validation are reportedon the right-most column, which serve as a baseline. For each experiment, we have underlined thosemetrics that result in a better performance than 5-fold cross validation. We observe that gradientdisparity (GD) and variance of gradients (Var) consistently outperform k-fold cross validation, un-like other metrics. On the rightmost column (No ES) we report the results without performing earlystopping (ES) (training is continued until the training loss is below 0.01).
Table 9: Pearson,s correlation coefficient between gradient disparity (D) and test loss (TL) over thetraining iterations is compared to the correlation between variance of gradients (Var) and test loss.
