Table 1: The statistics of datasetsDataset	#sample	#cluster	#dimMNIST	70, 000	10	1 × 28 × 28USPS	9, 298	10	1× 16×16YTF	10, 000	40	3 × 55 × 55Fashion	70, 000	10	1 × 28 × 28Reuters10K	10, 000	4	1 × 2, 000Metric: For all methods, we set the number of clusters to the ground truth categories and evaluateperformance with unsupervised clustering accuracy: Accuracy (ACC), Normalized mutual informa-tion (NMI). The best mapping can be efficiently computed by the Hungarian algorithm (Kuhn, 1955).
Table 2: Time Cost (s) of 103 iteration on MNISTBaseline JULE JULE(fast) DEPICT EDGaMTime(s)	1.44 X	105	3.00 X	104	1.09	X	104	9.28 X	103algorithms JULE and DEPICT in Table 2. Both JULE and its fast version JULE(fast) are evaluated.
Table 3: Comparisons of our method with popular clustering algorithms.
Table 4: Comparisons to third type of deep clustering methods.
Table 5: Network structure for all datasetNetwork	MNIST/USPS	YTF/FashionEncoder	(0): Dropout(p=0.25, inplace=False) (1): Linear(dim, 500, bias=True) (2): ReLU() (3): Linear(500, 500, bias=True) (4): ReLU() (5): Linear(500, 2000, bias=True) (6): ReLU() (7): Linear(2000, 10, bias=True)	(0): Conv2d(channel, 16, kernel_size=3, stride=1, Padding=1) (1): BatchNorm2d(16) (2): ReLU() (3): Conv2d(16, 32, kernel_size=3, stride=2, Padding=1) (4): BatchNorm2d(32,) (5): ReLU() (6): Conv2d(32, 32, kernel_size=3, stride=1, Padding=1) (7): BatchNorm2d(32) (8): ReLU() (9): Conv2d(32, 16, kernel_size=3, stride=2, Padding=1) (10): BatchNorm2d(16) (11): ReLU() (12): Linear(Inner-dim, 256, bias=True) (13): ReLU() (14): Linear(256, 10, bias=True)Streamlining EM	Iter 0: E-SteP (Eq.(3)) Iter 1:T-1: For i in range(T-1): Moment uPdate (Eq.(5)) Correction loss (Eq.(7)) E-steP (Eq.(3)) Iter T: Reconstruction (Eq.(9))	Decoder	(0)Linear(10, 2000, bias=True) (1): ReLU() (2): Linear(2000, 500, bias=True) (3): ReLU() (4): Linear(500, 500, bias=True) (5): ReLU() (6): Linear(500, dim, bias=True) (7): Sigmoid()	(0): Linear(10, 256, bias=TrUe) (1): ReLU() (2): Linear(256, Inner-dim, bias=True) (3): ConvTransPose2d(16, 32, kernel_size=3, stride=2, Padding=1) (4): BatchNorm2d(32) (5): ReLU() (6): ConvTransPose2d(32, 32, kernel_size=3, stride=1, Padding=1) (7): BatchNorm2d(32) (8): ReLU() (9): ConvTransPose2d(32, 16, kernel_size=3, stride=2, Padding=1) (10): BatchNorm2d(16) (11): ReLU() (12): ConVTransPose2d(16, channel, kerneLsiZe=3, Stride=1)D	Discussion on the advantages of streamlining EM for GMMWe streamline the EM algorithm of the Gaussian mixture model and derive a differential Gaussianmixture network for clustering. In the following, we compare it to the standard/stochastic EMalgorithm of the Gaussian mixture model to show its superiority.
Table 6: Hyperparameter setting for four datasetsHyperparameter	βl	β2	ηι	η2	η3MNIST	0.9	0.9	10-2	10-2	10-4USPS	0.9	0.9	10-1	5 × 10-2	10-4YTF	0.9	0.9	5 × 10-2	5 × 10-2	10-5Fashion	0.9	0.9	5 × 10-2	10-3	10-4The learnable parameters β1, β2 is initialized to 0.9 for all four datasets, which can be graduallyadjusted during the learning process.
