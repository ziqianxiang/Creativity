Table 1: Performance of the suite of methods (outlined in Section 4) across accuracy and compute metrics onsequence 5. We present several variants of accuracy - Overall, Mean-per-class as well as accuracy bucketed into4 categories: Novel-Head, Novel-Tail, Pretrain-Head and Pretrain-Tail (Pretrain refers to classes present in theImageNet-1K dataset). Sup. refers to Supervised and MoCo refers to He et al. (2019). The best technique onevery metric is in bold. Some methods could leverage caching of representations for efficiency, so, both GMACsare reported. GMACs do not include pretraining compute costs. See Table 4 for results with ResNet50 backbone.
Table 2: Statistics for the sequences of images used in NED. Sequences 1-2 are for validation and Sequence 3-5are for testing. The images from ImageNet-22k are approximately fit to a Zipfian distribution with 250 classesoverlapping with ImageNet-1k and 750 new classes.
Table 3: The results for fine-tuning various numbers of layers with a learning rate of .01 on Sequence 2. Trainingmore layers generally results in higher accuracy on novel classes, but lower accuracy on pretrain classes. Thetrade-off between novel and pretrain accuracy balances out so the overall accuracy is largely unaffected by thedepth of training.
Table 4: Continuation of Table 1 results on sequence 5 with ResNet50 backbone.
Table 5: Averaged results for all methods evaluated on Sequences 3-5. See Table 1 for the computational cost(GMACs) for each method and more information about each column.
Table 6: Our implementation of Prototypical Networks on MiniImageNet & NED. Results from Snell et al.
Table 7: The out-of-distribution performance for each method on sequence 5. We report the AUROC and the F1score achieved by choosing the best possible threshold value.
Table 8: Comparison of Weight Imprinting and Exemplar Tuning with different classifiers and initial temper-atures. Exemplar Tuning with a linear layer performs significantly better than all other variants.
