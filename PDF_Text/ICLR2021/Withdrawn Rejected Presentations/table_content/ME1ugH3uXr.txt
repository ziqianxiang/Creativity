Table 1: Scores of different anomaly detection algorithms for different tasks. RC: Robust Covariance (Nguyen& Welsch, 2010), kNN: Nearest neighbor (Gu et al., 2019), PCA: Principal Component Analysis (Harrou et al.,2015), OCS: One Class SVM (Scholkopf et al., 2000), LOF: Local Outlier Factor (Breunig et al., 2000), IF:Isolation Forest (Liu et al., 2008)Isolation Forest (Liu et al., 2008): For completeness, We provide a brief description of the IsolationForest algorithm. Isolation Forest is an unsupervised decision tree ensemble method that identifiesanomalies by isolating outliers of the data. It isolates anomalies in data points instead of profiling thenormal points. Algorithm Works by recursively partitioning the data using a random split betWeenthe minimum and maximum value of a random feature. It Works due to the observation that outliersare less frequent than the normal points and lie further aWay from normal points in the feature space.
Table 2: Filtering algorithm trained with Bio Abstracts and CS task data. We mix four corpora, filter out 80%of the data and retain the remaining 20% in both cases.
Table 3: Specification of task datasets. C refers to the number of classes. CHEMPROT (Kringelum et al.,2016) and RCT20k (Dernoncourt & Lee, 2017) are from biomedical domain. Hyperpartisan (Kiesel et al.,2019) and AGNews (Zhang et al., 2015) are from news domain. Helpfulness (McAuley et al., 2015) andIMDB (Maas et al., 2011) are from reviews domain. ACL-ARC (Jurgens et al., 2018) and SCIERC (Luanet al., 2018) are from CS domain. SST (Socher et al., 2013) is a general domain sentiment analysis task.
Table 4: Performance of DANA and five Baseline methods. Base corresponds to the pre-trained model ongeneral domain corpus with no further pre-training. Baseline methods are mentioned in previous subsection.
Table 5: Comparison of discrete vs continuous relevance weight setting. Base corresponds to the pre-trainedmodel on general domain corpus with no further pre-training. Discrete refers to DANA with discrete relevanceweights/filtered out text and pre-trained additionally for 50000 steps. Continuous-x refers to DANA withcontinuous relevance weights and pre-trained additionally for x*100, 000 more steps. Metrics used for differenttasks: accuracy for SST, micro f1 score for Chemprot and RCT20k, macro f1 score for ACL-ARC, SCIERC,Helpfulness, Hprpartisan, IMDB, and AGNews. Each model is finetuned eight times with differentseeds and the mean value is reported. Subscript correspond to the standard deviation in the finetuned modelperformance.
Table 6: Performance boost via DANA vs pre-training on domain specific corpus. DANA corresponds to ourmethod with discrete relevance weights/filtered out text and pre-trained additionally for 50000 steps. DomainCorpus refers to the model trained in Gururangan et al. (2020) over the domain same as the downstream task.
