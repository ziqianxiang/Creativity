Table 1: Functional Capability of Different Methods	ELIS,LIS AE,VAE,TopoAE ISOMAP,LLE,UMAP,t-SNEManifold learning without decoder Learned NLDR applicable to test data Generate data of learned manifolds Compatible with other NN architectures Scalable to large datasets	Yes	No	Yes Yes	Yes	No Yes	No	No Yes	No	No Yes	Yes	NoThe distinctive features of ELIS (and LIS) in comparison with related methods are summarized inTable 1. ELIS-based neural networks can accomplish all the functionalities in the general MLDLframework, for which none of the methods can achieve. Extensive experiments, comparisons, andablation study demonstrate that ELIS-based neural networks produce results not only superior tothe SOTA t-SNE and UMAP for NLDR and visualization but also better than other algorithms ofmanifold and autoencoder learning, including LIS, for NLDR and manifold data generation. Themain contributions of this paper are summarized below:(1)	Proposing the ELIS constraint in the MLDL framework, based on a similarity metric whichis nonlinear in distance. It inherits the metric-preserving property of LIS so that the resultinglayer-wise transformation is geometrically smooth, hence topologically homeomorphic,yet possesses more flexibility than LIS in handling highly nonlinear manifolds in highdimensional spaces.
Table 2: Comparison in performance metrics for four datasets		ELIS-Enc	LIS-EnC	UMAP	t-SNE	TopoAE	MLLESwiss Roll	Cont	1.0000	1.0000	0.9962	0.9969	0.9716	0.9956	Trust	1.0000	1.0000	0.9983	0.9993	0.9809	0.9948	Cont	0.9276	0.9255	0.9109	0.9155	0.9245	0.8943SpheresB	ACC(SVM) ACC(NN)	0.9273 0.9991	0.9100 0.9969	0.9100 0.8469	0.8478 0.9365	0.9581 0.9949	0.0965 0.8265	AUC	0.9711	0.9318	0.9570	0.9570	0.9870	0.9459	Cont	0.9956	0.9973	0.9962	0.9927	0.9901	0.9395Coil20	ACC(SVM)	0.8941	0.8301	0.8472	0.8014	0.7078	0.1556	NNACC	0.9965	0.9354	0.8917	0.9965	0.8160	0.6410	AUC	0.9770	0.9537	0.9842	0.9582	0.8916	0.8824	Cont	0.9639	0.9749	0.9646	0.9630	0.9618	0.9183MNIST	ACC(SVM)	0.9699	0.7468	0.9690	0.9525	0.7450	0.1100	ACC(NN)	0.9568	0.7035	0.9528	0.9567	0.7773	0.7423	AUC	0.9725	0.8779	0.9691	0.9314	0.8000	0.8575Figure 1: Comparison of visualization results for four datasetsManifold Data Generation. Fig. 3 compares images generated from interpolated points betweentwo nearest neighbors on the embedding using three autoencoders in comparison. The imagesgenerated by the ELIS-AE have clear boundaries and look sharper than those produced by the othertwo autoencoders. Results for several other objects are shown in Fig. A6 in Appendix A.4.
Table A1: Hyperparameters of ELIS for different datasetsDataset	Point	Network Structure (Number of parameters)	Q in Equ. (7)	BatchsiZeSwiss Roll	800	3, 500, 500, 2 (0.252M)	10	800S-Curve	800	3, 500, 500, 2 (0.252M)	10	800Servered Sphere	800	3, 500, 500, 2 (0.252M)	10	800SpheresA	10000	101, 500, 500, 2 (0.301M)	10	10000SpheresB	5500	101, 500, 500, 2 (0.301M)	10	5500Coli20	1440	16384, 500, 500, 2 (8.443M)	10	1440Coli100	7200	49152, 1000, 500, 250,2 (24.82M)	10	2400MNIST	60000	784, 1000, 500, 300, 2 (1.434M)	15	4000Fashion-MNIST	60000	784, 1000, 500, 2 (1.285M)	10	4000Continuation in ν(l0). In the training process, the parameter ν(l0)) in computing sample similaritiesfor the latent layer is graduated from a small number to a large number, e.g. ν(l0) : 0.01 → 100(see Equ. (5)), though fixed at a large value, e.g. ν(l0) = 100 for the input layer. Empirically, thecontinuation helps training converge to a good solution； the reasons behind are to be explained in afuture work.
Table A2: Comparison in performance metrics with 5 difference methods in eight datasets		ELIS-Enc	LIS-EnC	UMAP	t-SNE	TopoAE	MLLESwiss Roll	Cont	1.0000	1.0000	0.9962	0.9969	0.9716	0.9956	Trust	1.0000	1.0000	0.9983	0.9993	0.9809	0.9948SeveredSphere	Cont	0.9997	0.9932	0.9967	0.9985	0.9854	0.9958	Trust	0.9997	0.9755	0.9989	0.9995	0.9891	0.9836	Cont	0.7850	0.7892	0.7147	0.7548	0.8064	0.7272SpheresA	ACC(SVM)	0.5213	0.5000	0.5550	0.4992	0.4982	0.5000	ACC(NN)	0.9985	0.9912	0.5406	0.7837	0.9944	0.5205	AUC	0.5698	0.3362	0.5816	0.5603	0.3328	0.5961	Cont	0.9242	0.9255	0.9109	0.9155	0.9245	0.8943SpheresB	ACC(SVM)	0.9558	0.9100	0.9100	0.8478	0.9581	0.0965	ACC(NN)	0.9987	0.9969	0.8469	0.9365	0.9949	0.8265	AUC	0.9780	0.9318	0.9570	0.9570	0.9870	0.9459	Cont	0.9956	0.9973	0.9962	0.9927	0.9901	0.9395Coil20	ACC(SVM)	0.8941	0.8301	0.8472	0.8014	0.7078	0.1556	NNACC	0.9965	0.9354	0.8917	0.9965	0.8160	0.6410	AUC	0.9780	0.9537	0.9842	0.9582	0.8916	0.8824	Cont	0.9936	0.9967	0.9955	0.9950	0.9903	0.7898Coil100	ACC(SVM)	0.9372	0.7319	0.8299	0.8278	0.5540	0.0363
Table A3: Comparison in performance metrics of four different cross-layer schemes.
