Table 1: Error (%) on C-100.
Table 2: Refining energy.
Table 3: Normalization (%).
Table 4: Error (%) of OPT for MLPs and CNNs.
Table 5: ImageNet (%).
Table 6: Few-shot learning.
Table 7: Geometric networks.
Table 8: OPT vs. S-OPT on CIFAR-100 & ImageNet.
Table 9: Effect of sampling dimension.
Table 10: Our plain CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.xdenote convolution units that may contain multiple convolution layers. E.g., [3×3, 64]×3 denotes 3 cascadedconvolution layers with 64 filters of size 3×3.
Table 11: Our ResNet architectures with different convolutional layers. Conv0.x, Conv1.x, Conv2.x, Conv3.xand Conv4.x denote convolution units that may contain multiple convolutional layers, and residual units areshown in double-column brackets. Conv1.x, Conv2.x and Conv3.x usually operate on different size featuremaps. These networks are essentially the same as [20], but some may have a different number of filters in eachlayer. The downsampling is performed by convolutions with a stride of 2. E.g., [3×3, 64]×4 denotes 4 cascadedconvolution layers with 64 filters of size 3×3, S2 denotes stride 2.
Table 12: Architecture for few-shot learning. The number of classes is different for pretraining and finetuning.
Table 13: Our wide CNN-9 and wide ResNet-18 architectures with different convolutional layers.
Table 14: Testing error (%) on CIFAR-100 with different settings of PE-OPT (with block-shared orthogonalmatrix Rs ).
Table 15: Testing error (%) on CIFAR-100 with different settings of PE-OPT (with unconstrained blockorthogonal matrix Ru).
Table 16: Testing error for different initial energy.
Table 17: Training without BN on CIFAR-100.
Table 18: Error (%) with standard deviation on CIFAR-100.
Table 19: Training runtime (s) per 100 iterations.
