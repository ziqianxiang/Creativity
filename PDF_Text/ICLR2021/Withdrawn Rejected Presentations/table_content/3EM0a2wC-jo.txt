Table 1: Comparison of performance after training on one thousand Normal distributions for a thousanditerations. We use 3 components, and train models with 30 observations. We report standard error in parentheses.
Table 2: Comparison of performance on clustering after 30 iteration when training on 1000 different distributionsfor a thousand iterations. We use a total of 3 components, and train models with 30 observations. We reportstandard error in parentheses. Each cluster observation and center is drawn between -1 and 1, except for angularwhich is drawn between -Ï€ and pi with reported error as the L2 distance between predicted and ground truthmean.
Table 3: Quantitative evaluation of DAF-Net on distributions with different numbers of true components andhypothesis slots at test time with 30 observations. In all cases, DAF-Net is trained with 3-component problems,10 slots, and 30 observations. We compare with an offline set transformer trained with different numbers ofproblem components as well as with vector quantization.
Table 4: Comparison of performance on position estimation of 3 dynamically moving objects. All learningmodels are trained with 1000 sequences of 30 observations. We report standard error in parentheses. JPDA usesthe ground-truth observation and dynamics models.
Table 5: Comparison of performance of online clustering on MNIST and on rendered Airplane dataset. ForDAF-Net, LSTM and K-means (Learned) we use a convolutional encoder/decoder trained on the data; forK-means (Pixel) there is no encoding. We use a total of 3 components and train models with 30 observations.
Table 6: Comparison of performance under different settings after training on different distributionfor a thousand iterations. We use a total of 3 components, and train models with 30 observations. Wereport standard error in parentheses.
Table 7: Comparison of performance on Normal distribution. We use 30 components, and train models with 50observations. We report standard error in parentheses. Each cluster observation and center is drawn between -1and 1, with reported error as the L2 distance between predicted and ground truth means.
Table 8: Comparison of performance on regressing the ground truth un-rotated version of an MNIST digit wheredigits move over time. For DAF-Net, LSTM and K-means (Learned) we use a convolutional encoder/decodertrained on the data; for K-means (Pixel) there is no encoding. We use a total of 3 components and train modelswith 30 observations. We report MSE error with respect to ground truth unrotated images.
Table 9: We ablate each components of DAF-Net on the Normal distribution . When learned memoryis ablated, DAF-Net updates states based on observed values (appropriate in the Normal Distributiondataset). We report Lcluster of predictions and report standard error in parentheses. We find that eachproposed component of our model is important for improved performance.
