Table 1: The empirical results on continual pre-training setting, where RoBERTa and DAPT(RoBERTa continually pre-trained with the standard MLM objective) is leveraged as our baselineto facilitate comparison with (Gururangan et al., 2020). Specifically, ACL-ARC and SciERC areevaluated with the continually pre-trained model with CS domain corpus, while HyperPartisann andAGNews are based upon models trained with News domain corpus.
Table 2: The results on the dev sets of GLUE benchmarks, where MLM and FE-MLM are comparedwith the BERT-base model as the testbed.
Table 3: The comparison between the FE-MLMmodel with several baseline methods, based onthe averaged score (on the dev set) across differ-9 NLU datasets than the MLM baseline. Thisdemonstrates the wide applicability of the pro- ent tasks from the GLUE benchmark.
