Table 1: Summary of five exisiting nonlocal-based blocks in the spectral view.
Table 2: The Performances of Nonlocal-based Blocks with Different Number of Transferred Channelson CIFAR-100	No Reduction	Reduction by Two Times	Reduction by Four TimesModels	Topl (%) Top5 (%)	Top1(%) Top5 (%)	Top1 (%) Top5 (%)PreResNet56	75.33↑0∙00^^93.97↑0.00	75.3350^^93.972"	TS&oRo^^93.97200+ NL	75.29φo.o4 94.07↑o.1o	75.31即.02	92.84"13	75.50↑o.17	93.75^0.22+ NS	75.39↑0.06	93.00^0.97	75.83↑o∙5o 93.87φo.1o	75.61↑0∙28	93.66φ0.31+ A2	75.51↑0.18 92.90"07	75.58↑o∙25	94.27↑o∙3o	75.61↑0∙28	93.6∕0∙36+ CGNL	74.7/0.62	93.60Jo∙37	75.75↑o∙42	93.74φo.23	75.27^0.06	93.05^0.92+ Ours	76.34↑1.01 94.48↑0.51	76.41↑1.08 94.38↑0∙41	76.O2↑0.69 94.08↑0.11			The stage/position for adding the nonlocal-based blocks The nonlocal-based blocks can be addedinto the different stage of the preResNet to form the Nonlocal Network. In Tao et al. (2018), thenonlocal-based blocks are added into the early stage of the preResNet to catch the long-range relations.
Table 3: The Performances of Nonlocal-based Blocks Inserted into Different Position on CIFAR-100	Stage 1	Stage 2	Stage 3Models	Top1 (%) Top5 (%)	Top1(%) Top5 (%)	Top1 (%) Top5 (%)PreReSNet56	75.33↑o.oo 93.97↑o.oo	75.33↑0.00^^93.97↑0.00	75.33↑0.00 93.97↑0.00^^+ NL	75.3/0.02	92.84"13	75.64↑0.31	93.79^0.18	75.28^0.05 93.93^0.04+ NS	75.83↑0.50	93.87φ0.10	75.74↑0.41 94.02↑0.05	75.44↑0.11	93.8610.11+ A2	75.58↑o.25 94.27↑o.3o	75.60↑0.27	93.82φ0.15	75.2/0.12	93.65φ0.32+ CGNL	75.75↑0.42	93.74^0.23	74.54Φ0.79	92.65^1.32	74.9010.43 92.4611.51+ Ours	76.41↑1.08 94.38↑0∙41	76.29↑0.96 94.27↑0.30	75.68↑0.35	93.90J0.07The number of the nonlocal-based blocks We test the robustness for adding nonlocal-based blocksinto the backbone. The results are shown in Table 4. “x3" means three blocks are added into thestage 1, 2, and 3 respectively, and the accuracy in the brackets represent their results. We can seethat adding three proposed SNL operators into different stages of the backbone generates a largerimprovement (1.37%) than the NS operator and NL operator. This is because when adding NS andNL into the early stage, these two models cannot well aggregate the low-level features and interferewith the following blocks.
Table 4: Experiments for Adding Different Number of Blocks into PreResNet56 on CIFAR-100Models	Topl (%)	Top5 (%)PreResNet56	75.33↑0.00	93.97↑0.00+ NL(x3f)	75.31J0.02 (74.34299)	92.84Jj3 (93.∏286)+ NS (×3)	75.83↑0.50 (75.00233)	93.87,0.10 (93.57j0.40)+ A2 (×3)	75.58↑0.25 (75.63↑0.33)	94.27↑0.30 (94.12↑0.15)+ CGNL (×3)	75.75↑0.42 ( 75.96↑0.63)	93.74j0.23 (93.10287)+ Ours (×3)	76.41↑1.08 (76.70↑1∙37)	94.38↑0.41 (93.94,0.03)↑ The number in the bracket means the performance when adding 3 this kind of nonlocal-based blocks.
Table 5: Experiments for Adding Different Types of Nonlocal-based Blocks into PreResnet56 andResNet50 on CIFAR-10/100 Dataset	CIFAR-10/Resnet50				CIFAR-100/ReSnet50		CIFAR-100 / PreReSnet56	Models	Top1 (%)	Top5 (%)	Topl (%)	Top5 (%)	Top1 (%)	Top5 (%)Backbone	94.94↑0.00	99.87↑0∙00	76.5O↑0∙00	93.14↑0.00	75.33↑0.00	93.97↑0.00+ NL	94.01,0.93	99.82j0.05	76.77↑0.27	93.55↑0.41	75.31,0.02	92.84,1.33+ NS	95.15↑0.21	99.88↑0.01	77.90↑1.40	94.34↑1.20	75.83↑0.50	93.87,0.10+ A2	94.41,0.53	99.83j0.05	77.30↑0.80	93.4O↑0.26	75.58↑0.25	94.27↑0.30+ CGNL	94.49,0.45	99.92↑0∙05	74.88口.62	92.56,0.58	75.75↑0.42	93.74,0.23+ Ours	95.32↑0∙38	99.94↑0.07	78.17↑1.67-	94.17↑1.03	76.41↑1.08~	94.38↑0∙397Under review as a conference paper at ICLR 2021We also visualize the output feature maps of the ResNet50 with SNL and the original ResNet50 inFig. 3 A. Benefited from the rich and structured information considered in SNL, the response of thesimilar features between long-range spatial positions are enhanced as shown in the two mushroom,balls, and those animals. Moreover, Fig. 3 B shows the attention maps produced by our SNL and theoriginal NL block where the “Pink” and “Orange” dots are the central positions and the heatmapsrepresent the similarity between the central position and other positions. Compared with the originalNL block, SNL can pay more attention to the crucial parts than the original NL block profited by thebetter approximation formation as discussed in Sec. 2.3.
Table 6: Experiments for adding different types of Table 7: Experiments for adding NLs onnonocal-based blocks into Resnet50 on ImageNet CUB-200 and UCF-101 DatasetsModels	Top1(%)	Flops (G)	Size (M)ResNet50	76.15↑0.00	4.14	25.56+ CGD	76.90↑0.75	+0.01	+0.02+ SE	77.72↑1.57	+0.10	+2.62+ GE	78.00↑1.85	+0.10	+5.64+ NL	76.70↑0.55	^^+0.41 ^^	+2.09+ A2	77.00↑0.85	+0.41	+2.62+ CGNL	77.32↑1.17	+0.41	+2.09+ Ours	78.11↑1.96-	+0.51	+2.62	CUB-200	UCF-101Models	Top1(%)	Top1(%)-Backbone'	85.43↑0.00	81.57↑0.00+ NL	85.34φ0.09	82.88↑1.31+ NS	85.54↑0.11	82.50↑0.93+A2	85.91↑0.48	82.68↑1.11+ CGNL	86.14↑0.71	83.38↑1.81+ Ours	86.02↑0.59-	84.39↑2.82-f The ResNet50 is used for CUB-200 as the
Table 8: Experiments with state-of-the-art backboneModels	Top1(%)^^P3DQiu et al.(2017)	81.23↑0.00P3D + Ours	82.65↑1.42VTNKozlov et al.(2019)~~9O.06↑0.00VTN + Ours	90.34↑0.30MARS Crastoetal.(2019)^^92.29↑0.00MARS + Ours	92.79↑0.50C.2 Experiment on Video Person Re-identificationTable 9: Experiments on Video-Person ReidentificationMars			ILID-SVID		PRID-2011		Models	Rank1(%)	mAP(%)	Rank1(%)	mAP(%)	Rank1(%)	mAP(%)ResNet50tp	82.30↑0∙00	75.70↑0.00	74.70↑0.00	81.60↑0.00	86.50↑0∙00	90.50↑0.00+ NL	83.21↑0∙91	76.54↑0.84	75.30↑0∙60	83.00↑1.40	85.40"10	89.7θΦ0.80+ SNL	83.40↑1.10	76.80↑1.10	76.30↑1.60	84.80↑3.20	88.80↑2.30	92.40↑1.90For the backbone, we follow the strategy of Gao & Nevatia (2018) that use the pooling (RTMtp) tofuse the spatial-temporal features. (Note that the models are totally trained on ilidsvid and prid2011rather than fintuning the pretrained model on Mars.) We only insert the SNL block into the ResNet50(right before the last residual block of res4) in RTMtp. Other block setting are the same as the settingfor fine-grained image classification on CUB in our paper. For all those datasets we train the model
Table 9: Experiments on Video-Person ReidentificationMars			ILID-SVID		PRID-2011		Models	Rank1(%)	mAP(%)	Rank1(%)	mAP(%)	Rank1(%)	mAP(%)ResNet50tp	82.30↑0∙00	75.70↑0.00	74.70↑0.00	81.60↑0.00	86.50↑0∙00	90.50↑0.00+ NL	83.21↑0∙91	76.54↑0.84	75.30↑0∙60	83.00↑1.40	85.40"10	89.7θΦ0.80+ SNL	83.40↑1.10	76.80↑1.10	76.30↑1.60	84.80↑3.20	88.80↑2.30	92.40↑1.90For the backbone, we follow the strategy of Gao & Nevatia (2018) that use the pooling (RTMtp) tofuse the spatial-temporal features. (Note that the models are totally trained on ilidsvid and prid2011rather than fintuning the pretrained model on Mars.) We only insert the SNL block into the ResNet50(right before the last residual block of res4) in RTMtp. Other block setting are the same as the settingfor fine-grained image classification on CUB in our paper. For all those datasets we train the modelwith Adam with the initial learning rate 3e - 4, the weight decay 5e - 4. The learning rate is dividedby 200 at 400 epochs. All the models are trained for 500 epochs with the Cross Entropy and TripletLoss(margin = 0.3). We use rank-1 accuracy (Rank1) and men average precision (mAP) to evaluatethe performance for the models.
