Table 1: Size of evaluation sets. The		Table 2: Specifications of language models: pa-				number of relation types in LAMA are		rameters in the network layers (Net.P.#), in embed-				in brackets. All evaluations are zero-		dings (E.P.#), number of layers (L.#), and hidden				shot on their official testing data.		Dimension. GPT-2 from OpenAI is marked by (OAI).						Note that embeddings are		looked	up in	constantDataset	Items	time; the network capacity		are mostly defined by		Language Modeling		Net.P.# (17).				WikiText-103 (tokens)	270k					Lambada	5.1k	Model	Net.P.#	E.P.#	L.#	DimLAMA		Base				Google-Re	4.6k	GPT-2	90M	38M	12	768T-Rex 1-1 (2)	937	KALM	90M	458M	12	768T-Rex N-1 (23)	20k	Large				T-Rex N-M (16)	13k	GPT-2 (OAI)	304M	51M	24	1024ConceptNet (16)	11k	GPT-2	304M	51M	24	1024SQUAD (Statements)	305	KALM	304M	471M	24	1024Zero-Shot QA		eXntra Large				Trivia QA	11k	GPT-2 XL (OAI)	1.46B	80M	48	1600Natural Questions (Short)	3.7k	GPT-2 1.5B	1.46B	80M	48	1600WebQuestions	2.0k	GPT-2 17B	16.9B	214M	78	4256
Table 3: Results on language modeling tasks and LAMA knowledge probing tasks. The LAMAnumbers are average Precision@1.
Table 4: Zero-shot question answering performance of different models for three different questionanswering benchmarks. EM is exact match percent, and cover-EM is the percent counting whetherthe correct answer is a substring of the generated answer. All generated answers were generated by agreedy decoding of at most 20 tokens.
