Table 1: Main experimental results on the CIFAR-100 dataset. * means using learning rate warm-upto smooth the training process. We report top-1 “mean (std)” accuracies (%) over 3 runs.
Table 2: Results comparison with state-of-the-art two-stage and one-stage KD methods. See Appendixfor specific settings of these methods. We report top-1 “mean (std)” accuracies (%) over 3 runs.
Table 3: Results on the ImagNet dataset. We report top-1 accuracies (%).
Table 4: Comparison of ECD with dense feature connections added to different blocks. In the table,C2, C3 and C4 refer to Conv2_x, Conv3_x and Conv4_x, respectively. We report top-1 “mean (std)”accuracies (%) over 3 runs.
Table 5: Results comparison of our ECD and using feature losses. “stage-wise” means featuresupervision after each stage, which is consistent with many typical KD methods, and the distillationloss factor here is set to 10-2; “block-wise” refers to supervision on the same blocks where we addconnections in our ECD, and the distillation loss factor here is set to 10-4. We report top-1 “mean(std)” accuracies (%) over 3 runs.
Table 6: Comparison with adding Conv1×1 on each connection path. We report top-1 “mean (std)”accuracies (%) over 3 runs.
Table 7: Comparison of auxiliary teacher generation with different methods. DAConv refers to ourdefault setting. We report top-1 “mean (std)” accuracies (%) over 3 runs.
Table 8: Combination experiment with DML and ED, where S represents the student model, Trepresents the teacher model and T+ represents the teacher model connected with the features of thestudent model under ECD training. For the training time of ResNet20 in the experiment, DML cost5.40 gpu-hours and ECD cost 5.27 gpu-hours on a single NVIDIA TITAN RTX. Under the sameteacher-student model, our ECD will be slightly faster than DML because ECD does not need tocalculate additional loss.
Table 9: Details of the convolutional blocks of the ResNet110, ResNet164 and WRN-40-2 backbonesevaluated on the CIFAR-100 dataset.
Table 10: Structures of convolutional blocks of the ResNet backbones for the ImageNet dataset.
Table 11: Brief implementation details of KD methods and settings of the distillation loss factor (λ).
Table 12: Experiment results of applying Label Smoothing (LS) to ECD.
Table 13: Experiment results of ResNet32 on CIFAR100 in different number of multiplexing weightfor DAConv. We report top-1 “mean (std)” accuracies (%) over 3 runsI	Number of multiplexing weightModel Baseline ∣	2	4	8	16	32ResNet32^^70.80(0.15) ∣ 71.47(0.09)^^71.87 (0.16)^^72.12(0.07)^^72.40(0.14)^^71.86 (0.20)More comparison experiments with other KD methods. As shown in Table 14, the performanceof our ECD and ECD* is quite good compared to other methods especially when combining with theoriginal KD. Furthermore, we experiment with more dense cross-network “multi-layer-to-single-layer”feature connections in ECD, called ECD (dens), and the corresponding method combined with ED,called ECD (dens)*. Note that “multi-layer” indicates the student layer (at the same depth withthe teacher layer) as well as its all previous layers. As shown in Table 14, our ECD (dens) andECD (dens)* achieve the best performance among these KD methods, indicating that the featureconnection in our ECD can effectively transmit feature distillation information. Also, we evaluate theperformance of our ECD when combining with FitNet (Romero et al. (2015)) or CRD (Tian et al.
Table 14: Results comparison with state-of-the-art two-stage KD methods and results of combiningtwo-stage KD methods with the original KD. Bold and Underline denote the best and the second bestresults, respectively. We report top-1 “mean (std)” accuracies (%) over 3 runs.
Table 15: Results comparison with state-of-the-art two-stage KD methods reported in CRD (Tianet al. (2020)) under the training setting of the 240 epochs. Bold and UnderIine denote the best and thesecond best results, respectively.
Table 16: Detailed results on the CIFAR-100 dataset. * indicates using learning rate warm-up tosmooth the training process. S represents the student model, T represents the teacher model and T+represents the teacher model connected with the features of the student model under ECD training.
