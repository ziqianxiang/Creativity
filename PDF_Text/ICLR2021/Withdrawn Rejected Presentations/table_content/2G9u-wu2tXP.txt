Table 1: Hyperparameters’ ranges and observed impact (averaged over 10 runs) over accuracy andtraining/inference time, based on the Pairwise-MNIST scenario.
Table 2: VC: Pairwise-MNIST network architecture and hyperparametersLayer	Type	Parameters		1	Conv 2D	Filters: 16, Kernel: 5 × 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D, Activation: LeakyReLU					Hyperparameter	Value			batch size	162	Conv 2D	Filters: 32, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D, Activation: LeakyReLU	epochs	5,5,5,5,5			Optimizer	Adam3	Dense	Neurons: 60, Activation: LeakyReLU	learning rate	0.001			Fisher matrix samples	2004	DropOut	dropProb: 0.2		5	Dense	Neurons: 10, Activation: LeakyReLU	λ	10246	DropOut	dropProb: 0.2		7	Dense	Neurons: 10, Activation: None		Table 3: EWC: Pairwise-MNIST network architecture and hyperparametersLayer	Type	Parameters1	Conv 2D	Filters: 16, Kernel: 5 × 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D, Activation: LeakyReLU2	Conv 2D	Filters: 32, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D, Activation: LeakyReLU3	Dense	Neurons: 60, Activation: LeakyReLU4	DropOut	dropProb: 0.25	Dense	Neurons: 10, Activation: LeakyReLU
Table 3: EWC: Pairwise-MNIST network architecture and hyperparametersLayer	Type	Parameters1	Conv 2D	Filters: 16, Kernel: 5 × 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D, Activation: LeakyReLU2	Conv 2D	Filters: 32, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D, Activation: LeakyReLU3	Dense	Neurons: 60, Activation: LeakyReLU4	DropOut	dropProb: 0.25	Dense	Neurons: 10, Activation: LeakyReLU6	DropOut	dropProb: 0.27	Dense	Neurons: 10, Activation: NoneTable 4: ELL: Pairwise-MNIST network architecture and hyperparametersHyperparameter	Valuebatch size	16epochs	5,5,5,5,5Optimizer	Adamlearning rate	0.001Codes length	100Embedding Size	288Temperature	3.0Stabilization epochs	2Stabilization learning rate	0.001
Table 4: ELL: Pairwise-MNIST network architecture and hyperparametersHyperparameter	Valuebatch size	16epochs	5,5,5,5,5Optimizer	Adamlearning rate	0.001Codes length	100Embedding Size	288Temperature	3.0Stabilization epochs	2Stabilization learning rate	0.00113Under review as a conference paper at ICLR 2021Units Quantity	Unit Layers	Parameters		2	Conv 2D	Filters: 16, Kernel: 5 X 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D Activation: LeakyReLU	Hyperparameter	Value			batch size	16			epochs	5,5,5,5,52	Conv 2D	Filters: 16, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D Activation: LeakyReLU	extra units	0,0,0,0,0			Optimizer	Adam			learning rate	0.001
Table 5: HRN: Pairwise-MNIST units/classifiers architecture and hyperparametersLayer	Type	Parameters		1	Conv 2D	Filters: 32, Kernel: 5 × 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D, Activation: LeakyReLU					Hyperparameter	Value2	Conv 2D	Filters: 64, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D, Activation: LeakyReLU					batch size	16			epochs	20,103	Dense	Neurons: 60, Activation: LeakyReLU	Optimizer	Adam4	DropOut	dropProb: 0.2	learning rate	0.0015	Dense	Neurons: 10, Activation: LeakyReLU		6	DropOut	dropProb: 0.2		7	Dense	Neurons: 10, Activation: None		Table 6: VC: Fashion-MNIST/MNIST network architecture and hyperparametersLayer	TyPe	Parameters		1	Conv 2D	Filters: 32, Kernel: 5 × 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D, Activation: LeakyReLU					Hyperparameter	Value			batch size	162	Conv 2D	Filters: 64, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D, Activation: LeakyReLU	epochs	20,10			Optimizer	Adam3	Dense	Neurons: 60, Activation: LeakyReLU	learning rate	0.001
Table 6: VC: Fashion-MNIST/MNIST network architecture and hyperparametersLayer	TyPe	Parameters		1	Conv 2D	Filters: 32, Kernel: 5 × 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D, Activation: LeakyReLU					Hyperparameter	Value			batch size	162	Conv 2D	Filters: 64, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D, Activation: LeakyReLU	epochs	20,10			Optimizer	Adam3	Dense	Neurons: 60, Activation: LeakyReLU	learning rate	0.001			Fisher matrix samples	2004	DropOut	dropProb: 0.2		5	Dense	Neurons: 10, Activation: LeakyReLU	λ	4006	DropOut	dropProb: 0.2		7	Dense	Neurons: 10, Activation: None		Table 7: EWC: Fashion-MNIST/MNIST network architecture and hyperparameters14Under review as a conference paper at ICLR 2021Layer	Type	Parameters1	Conv 2D	Filters: 32, Kernel: 5 X 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D, Activation: LeakyReLU2	Conv 2D	Filters: 64, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D, Activation: LeakyReLU3	Dense	Neurons: 60, Activation: LeakyReLU
Table 7: EWC: Fashion-MNIST/MNIST network architecture and hyperparameters14Under review as a conference paper at ICLR 2021Layer	Type	Parameters1	Conv 2D	Filters: 32, Kernel: 5 X 5 Stride: 3 × 3, Padding: 2 × 2 BatchNorm2D, Activation: LeakyReLU2	Conv 2D	Filters: 64, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 BatchNorm2D, Activation: LeakyReLU3	Dense	Neurons: 60, Activation: LeakyReLU4	DroPOUt	dropProb: 0.25	Dense	Neurons: 10, Activation: LeakyReLU6	DropOut	dropProb: 0.27	Dense	Neurons: 10, Activation: NoneTable 8: ELL: Fashion-MNIST/MNIST network architecture and hyperparametersHyperparameter	Valuebatch size	16epochs	20,10Optimizer	Adamlearning rate	0.001Codes length	300Embedding Size	576Temperature	3.0
Table 8: ELL: Fashion-MNIST/MNIST network architecture and hyperparametersHyperparameter	Valuebatch size	16epochs	20,10Optimizer	Adamlearning rate	0.001Codes length	300Embedding Size	576Temperature	3.0Stabilization epochs	3Stabilization learning rate	0.001Units Quantity	Unit Layers	Parameters					Hyperparameter	Value3	Conv 2D	Filters: 6, Kernel: 5 × 5 Stride: 3 × 3, Padding: 2 × 2 Activation: LeakyReLU	batch size	16			epochs	20,10			extra units	0,23	Conv 2D	Filters: 8, Kernel: 2 × 2 Stride: 3 × 3, Padding: 0 Activation: LeakyReLU					Optimizer	Adam			learning rate	0.001Classifier Layer	Type	Parameters	d	3
Table 9: HRN: Fashion-MNIST/MNIST network architecture and hyperparameters15Under review as a conference paper at ICLR 2021Layer	Type	Parameters1	Conv 2D	Filters: 36, Kernel: 3 X 3 Stride: 2 × 2, Padding: 2 × 2 BatchNorm2D Activation: LeakyReLU2	Conv 2D	Filters: 99, Kernel: 2 × 2 Stride: 1 × 1, Padding: 1 × 1 BatchNorm2D Activation: LeakyReLU3	DroPOUt 2D-	dropProb: 0.54	Dense	Neurons: 200, BatchNorm Activation: ReLU5	DroPOUt	dropProb: 0.46	Dense	Neurons: 100, BatchNorm Activation: ReLU7	DroPOUt	dropProb: 0.48	Dense	Neurons: 100, BatchNorm Activation: None9	DroPOUt	dropProb: 0.2Table 10: VC: SVHN/incremental-Cifar100 network architecture and hyperparametersHyperparameter	Valuebatch size	32epochs	8,15,15,15 15,15,15,15 15,15,15Optimizer	Adamlearning rate	0.001	-Layer	Type	Parameters		
Table 10: VC: SVHN/incremental-Cifar100 network architecture and hyperparametersHyperparameter	Valuebatch size	32epochs	8,15,15,15 15,15,15,15 15,15,15Optimizer	Adamlearning rate	0.001	-Layer	Type	Parameters		1	Conv 2D	Filters: 36, Kernel: 3 × 3 Stride: 2 × 2, Padding: 2 × 2 BatchNorm2D Activation: LeakyReLU		2	DroPOUt 2D^	dropProb: 0.3	Hyperparameter	Value3	Conv 2D	-Filters: 99, Kernel: 2 × 2- Stride: 1 × 1, Padding: 1 × 1 BatchNorm2D Activation: LeakyReLU	batch size	32			epochs	8,15,15,15 15,15,15,15 15,15,154	DroPOUt 2D^	dropProb: 0.3	Optimizer	Adam5	Dense	Neurons: 200, BatchNorm Activation: ReLU	learning rate	0.001	-			Fisher matrix samples	2006	DropOut	dropProb: 0.5		7	Dense	Neurons: 100, BatchNorm Activation: ReLU	λ	250	-				8	DropOut	dropProb: 0.4		9	Dense	Neurons: 100, BatchNorm Activation: None		10	DropOut	dropProb: 0.2		
Table 11: EWC: SVHN/incremental-Cifar100 network architecture and hyperparameters16Under review as a conference paper at ICLR 2021Layer	Type	Parameters		1	Conv 2D	Filters: 36, Kernel: 3 X 3 Stride: 2 × 2, Padding: 2 × 2 BatchNorm2D Activation: LeakyReLU					Hyperparameter	Value			batch size	16			epochs	8,15,15,15 15,15,15,15 15,15,152	Conv 2D	Filters: 99, Kernel: 2 × 2 Stride: 1 × 1, Padding: 1 × 1 BatchNorm2D Activation: LeakyReLU					Optimizer	Adam3	DroPOUt 2D	dropProb: 0.5	learning rate	0.001	-4	Dense	Neurons: 200, BatchNorm Activation: ReLU	Codes length	2800	-			Embedding Size	32076	-5	DroPOUt	dropProb: 0.4	Temperature	3.0	-6	Dense	Neurons: 100, BatchNorm Activation: ReLU	Stabilization epochs	37	DroPOUt	dropProb: 0.4	Stabilization learning rate	0.0018	Dense	Neurons: 100, BatchNorm Activation: None						9	DroPOUt		dropProb: 0.2			Table 12: ELL: SVHN/incremental-Cifar100 network architecture and hyperparameters
Table 12: ELL: SVHN/incremental-Cifar100 network architecture and hyperparametersUnits Quantity	Unit Layers	Parameters		2	Conv 2D	Filters: 36, Kernel: 3 × 3 Stride: 2 × 2, Padding: 2 × 2 Activation: LeakyReLU			DropOut 2D	dropProb: 0.5	Hyperparameter	Value2	Conv 2D	Filters: 36, Kernel: 3 × 3 Stride: 1 × 1, Padding: 2 × 2 Activation: LeakyReLU	batch size	16			epochs	8,15,15,15 15,15,15,15 15,15,15	DroPOUt 2D-	dropProb: 0.5		1	Conv 2D	Filters: 12, Kernel: 2 × 2 Stride: 1 × 1, Padding: 1 × 1 Activation: ReLU	extra units	0,2,0,0 0,0,0,0 0,0,0	DroPOUt 2DT	dropProb: 0.5	Optimizer	Adam1	Conv 2D	Filters: 24, Kernel: 4 X 4 Stride: 2 × 2, Padding: 1 × 1 Activation: LeakyReLU	learning rate	0.001	-			d	3	-	DroPOUt 2D-	dropProb: 0.5	m	3	-Classifier Layer	Type	Parameters	S	2800	-				Td		1.10-5	-1	Dense	Neurons: 200 Activation: ReLU	Texpand	0.01	-			ɑ	5	-2	DropOut	dropProb: 0.4		P		10	-3	Dense	Neurons: 100 Activation: ReLU		λ		1.0	-				4	DropOut	dropProb: 0.4		
Table 13: HRN: SVHN/incremental-Cifar100 network architecture and hyperparameters17Under review as a conference paper at ICLR 2021D Runtime performanceThis section compares the training and inference run times of each algorithm, considering theSVHN/incremental-Cifar100 experiment. All runs were performed on a 32 CPUs machine withan Nvidia P100 GPU and 16 GB of RAM. Performance figures have been averaged over 10 runs.
Table 14: Training and inference run times (10 runs average) for each algorithm.
