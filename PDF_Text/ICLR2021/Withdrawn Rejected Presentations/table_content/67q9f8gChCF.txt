Table 1: Statistics of Atari environments.
Table 2: Average return of vPERL with two types network architectures (Large and Small) andtwo types of rewards (RMax and RMean), GIRIL (Yu et al., 2020), VIN and state-of-the-art IRLalgorithms GAIL (Ho & Ermon, 2016) and VAIL (Peng et al., 2019) with one-life demonstrationdata on eight Atari games. The results shown are the mean performance over five random seedswith better-than-expert performance in bold. The last two rows show the average performanceimprovements of IL algorithms versus demonstration (Demo.) and expert performance with greaterthan 100% in bold.
Table 3: Average return of vPERL-Small-RMean, and its components (i.e., Action Back-tracing andTransition Modeling, and PERL) with one-life demonstration data on eight Atari games. The resultsshown are the mean performance over five random seeds with better-than-eXPert performance in bold.
Table 4: Average return of vPERL, GIRIL, VIN and state-of-the-arts inverse reinforcement learningalgorithms GAIL (Ho & Ermon, 2016) and VAIL (Peng et al., 2019) with one demonstration data oncontinuous control tasks. The results shown are the mean performance over 3 random seeds with bestimitation performance in bold.
Table 5: Parameter Analysis of the vPERL versus other baselines with different numbers of full-episode demonstrations on Centipede game. The results shown are the mean performance over 5random seeds with best performance in bold.
Table 6: Parameter Analysis of the vPERL versus other baselines with different numbers of full-episode demonstrations on Qbert game. The results shown are the mean performance over 5 randomseeds with best performance in bold.
Table 7: Parameter Analysis of the vPERL versus other baselines with different numbers of full-episode demonstrations on Inverted Pendulum task. The results shown are the mean performanceover 5 random seeds with best performance in bold.
Table 8: Parameter Analysis of the vPERL versus other baselines with different numbers of full-episode demonstrations on InvertedDoublePendulum task. The results shown are the mean perfor-mance over 5 random seeds with best performance in bold.
Table 9: Average return of vPERL-Small-RMean with one-life demonstration data of expert policytrained under different simulations steps (NE=1 million, 5 million and 10 million). The results shownare the mean performance over five random seeds with better-than-eXPert performance in bold.
Table 10: Average return of vPERL-RMean with one demonstration data of expert policy trainedunder different simulations steps (NE=0.1 million, 0.5 million and 1 million). The results shown arethe mean performance over five random seeds with demonstration-level performance in bold.
Table 11: Average return of vPERL-Small-RMean with different choices of K, on one-life demonstra-tion data. The results shown are the mean performance over five random seeds with better-than-expertperformance in bold.
Table 12: Average return of vPERL-RMean with different choices of K, on one-life demonstrationdata. The results shown are the mean performance over five random seeds with demonstration-levelperformance in bold.
Table 13: Average return of vPERL-Small with RMean, GIRIL (Yu et al., 2020), VIN and state-of-the-art IRL algorithms GAIL (Ho & Ermon, 2016) and VAIL (Peng et al., 2019) with one-lifedemonstration data on additional Atari games. The results shown are the mean performance over fiverandom seeds with better-than-eXPert performance in bold.
Table 14: Average return of vPERL-Small-RMean with different choices of K on one-life demonstra-tion data. The results shown are the mean performance over five random seeds with better-than-expertperformance in bold.
