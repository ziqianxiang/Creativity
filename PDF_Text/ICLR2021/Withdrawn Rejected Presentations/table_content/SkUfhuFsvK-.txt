Table 1: Performance of GCN model trained with different ratio of bad training nodes.
Table 2: Performance of aggregation and classic methodsDataset	MLP	ChebyNet	GCN	GAT	feat5+svmCora	55.1%	81.2%	81.5%	83.0%	83.2%CiteSeer	46.5%	69.8%	70.3%	72.5%	72.3%PubMed	71.4%	74.4%	79.0%	79.0%	78.8%4.3	Multi-stage self-training frameworkThe overall framework of the proposed feature aggregation self-training GCN (FASG) algorithm isillustrated in Fig. 3. Instead of using deep cluster and aligning mechanism, we firstly apply featureaggregation and linear SVM classifier to build a checking part. After each training round we use4Under review as a conference paper at ICLR 2021Figure 2: Comparison of the number of falsely labeled nodes introduced by different checkingmechanism.
Table 3: Comparison of prediction accuracy on Cora datasetLabel Rate	0.5%	1%	2%	3%	4%Node2Vec	32.4%	44.4%	50.2%	54.5%	57.4%LP	57.6%	61.0%	63.5%	64.3%	65.7%Cheby	38.0%	52.0%	62.4%	70.8%	74.1%GCN	50.6%	58.4%	70.0%	75.7%	76.5%GAT	49.0%	60.7%	72.8%	77.7%	79.7%Co-training	53.9%	57.0%	69.7%	74.8%	75.6%Self-training	56.8%	60.4%	71.7%	76.8%	77.7%MultiStage	61.1%	63.7%	74.4%	76.1%	77.2%M3S	61.5%	67.2%	75.6%	77.8%	78.0%FASG	62.8%	685%	76.1%	78.0%	80.3%Table 4: Comparison of prediction accuracy on CiteSeer datasetLabel Rate	0.5%	1%	2%	3%	4%Node2Vec	24.9%	29.1%	34.4%	35.7%	38.8%LP	37.7%	41.6%	41.9%	44.4%	44.8%Cheby	31.7%	42.8%	59.9%	66.2%	68.3%GCN	44.8%	54.7%	61.2%	67.0%	69.0%GAT	44.6%	53.7%	64.6%	66.4%	69.3%Co-training	42.0%	50.0%	58.3%	64.7%	65.3%
Table 4: Comparison of prediction accuracy on CiteSeer datasetLabel Rate	0.5%	1%	2%	3%	4%Node2Vec	24.9%	29.1%	34.4%	35.7%	38.8%LP	37.7%	41.6%	41.9%	44.4%	44.8%Cheby	31.7%	42.8%	59.9%	66.2%	68.3%GCN	44.8%	54.7%	61.2%	67.0%	69.0%GAT	44.6%	53.7%	64.6%	66.4%	69.3%Co-training	42.0%	50.0%	58.3%	64.7%	65.3%Self-training	51.4%	57.1%	64.1%	67.8%	68.8%MultiStage	53.0%	57.8%	63.8%	68.0%	69.0%M3S	56.1%	62.1%	66.4%	70.3%	70.5%FASG	58.3%	66.4%	70.2%	70.3%	70.8%The comparison of these algorithms on the three benchmarks is summarized in Tables 3, 4 and 5respectively. It is observed that GNN-based approaches surpass traditional learning approaches ingeneral on all three datasets. By adopting multi-rounds training strategy and expanding the super-vised information iteratively, the algorithms based on self-training mechanism achieve remarkableimprovement in prediction accuracy, especially when the label rate is quite small. Furthermore, theproposed FASG algorithm outperforms all baseline algorithms in all tested scenarios. The superior-ity of our method derives from the delicate checking part based feature aggregation, which is ableto guarantee the high quality of the expanded supervised information as illustrated in Fig. 2.
Table 5: Comparison of prediction accuracy on PubMed datasetLabel Rate	0.03%	0.05%	0.1%Node2Vec	37.2%	38.2%	42.9%LP	58.3%	61.3%	63.8%Cheby	40.4%	47.3%	51.2%GCN	51.1%	58.0%	67.5%GAT	50.6%	59.1%	65.0%Co-training	55.5%	61.6%	67.8%Self-training	56.3%	63.6%	70.0%MultiStage	57.4%	64.3%	70.2%M3S	59.2%	64.4%	70.6%FASC	60.1%	65.2%	70.7%In order to reveal how our algorithm is affected by the number of training rounds K , we reportthe numbers of newly added nodes, bad training nodes and the prediction accuracy on the CiteSeerdataset for different label rates with increasing K from 0 to 50. Note that when K is 0, the frameworkdegrades to the plain GCN model. As shown in Fig. 4(a), accuracies grow rapidly during the first fewrounds for all label rates. For a small label rate (e.g. 0.005), the accuracy tends to grow continuouslywith K increasing. On the contrary, for a relatively large label rate (e.g. 0.04) the accuracy will reachthe peak rapidly with a small K and saturate afterward.
Table 6: Performance of our framework integrated with different base GNN modelsGNN Model	GCN	GAT	APPNP	GS-M	GS-PAcc of normal GNN	44.8%	46.8%	45.7%	40.9%	34.9%Acc in our framework	58.3%	59.8%	57.6%	59.1%	51.0%5.3.2 Integration with different base GCN modelsAs described in Sec. 4.3, the proposed FASG algorithm can be integrated with various base GCNmodels to improve their prediction performances. For validation, we combine FASG with severalpopular GCN models, and report their prediction accuracy on the CiteSeer dataset with label rate0.5% in Table 6, where GS-M and GS-P represent GraphSage with mean and maxpool aggregatorrespectively. It is shown that all tested base GCN models achieve similar performances, and theyare all benefitted significantly in prediction accuracy by applying our FASG to expand supervisedinformation iteratively.
