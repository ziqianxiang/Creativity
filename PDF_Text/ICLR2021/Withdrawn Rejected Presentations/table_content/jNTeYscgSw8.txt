Table 1: Regularizers and alternative losses improve ImageNet accuracy. Accuracy of models trained withdifferent losses/regularizers on the ImageNet validation (mean ± standard error of 8 models) and CIFAR-10 andCIFAR-100 test sets (mean ± standard error of 25 models). Losses are sorted from lowest to highest ImageNettop-1 accuracy. Accuracy values not significantly different from the best (p > 0.05, t-test) are bold-faced.
Table 2: Regularization and alternativelosses improve class separation in thepenultimate layer. Results averaged over8 ResNet-50 models per loss on the Image-Net training set.
Table 3: Regularized networks learn features specialized to ImageNet. Accuracy of linear classifiers (L2-regularized multinomial logistic regression) trained to classify different datasets using fixed penultimate layerfeatures. IN(50k) reflects accuracy of a classifier trained on 50,046 examples from the ImageNet training set andtested on the validation set. See Appendix A.2 for training details.
Table 4: Temperature of cosine softmax loss controls ImageNet top-1 accuracy, class separation (R2),and linear transfer accuracy.
Table A.1: Hyperparameters for ImageNet.
Table A.2: Hyperparameters for CIFAR. η is the learning rate and λ is the product of the learning rate andʌthe weight decay added to the loss, i.e., the weight decay loss is LWeight_decay = 卷 ∣∣w∣∣2∙Loss/regularizer CIFAR-10 (All-CNN-C + BN)	CIFAR-100 (WRN 16-8)Squared error	η=	_	r 二 0.1,Λ =	二 10-3.5,κ = 8,M = 0.83	η	二 0.1,Λ = 10-3.75, K = 6, M = 12Softmax	η=	10-0.75	,λ = 10-3.75	η	二 0.1,λ = 10—4Logit normalization	η=	_ _ r 二 0.01, λ	= 10-4,τ = 0.14	η	二 10-2.25,λ = 10-3.75,τ = 0.11Extra final layer L2	η=	_	r 二 0.1,Λ =	10-3.5 , λfinal = 10-1.5	η	= 0.1,λ = 10-3.75,λfinaι = 10-3.33Cosine softmax	η=	10-2.25	,λ = 10-4,τ = 0.08	η	二 0.011 = 10—3.75,τ = 0.1Dropout	η=	_	r 二 0.1,Λ =	10-3.75, ρ = 0.65	η	二 10T.25,λ = 10-3.75,ρ = 0.75Sigmoid	η=	〜 =1,λ =-	10-3.75	η	= 0.1,λ = 10-3.75Label smoothing	η=	_ ` 二 0.1,Λ =	10-3.75, α = 0.04	η	二 0.1,λ = 10-3.5,α = 0.18Logit penalty	η=	10-0.75	,λ = 10-3.75,β = 10-2.83	η	二 10T.25,λ = 10-3.75,β = 10—2.83A.2 Training and Tuning Multinomial Logistic Regression ClassifiersTo train multinomial logistic regression classifiers on fixed features, we follow a similar approach toKornblith et al. (2019b). We first extracted features for every image in the training set, by resizingthem to 224 pixels on the shortest side and taking a 224 × 224 pixel center crop. We held outa validation set from the training set, and used this validation set to select the L2 regularizationhyperparameter, which we selected from 45 logarithmically spaced values between 10-6 and 105,applied to the sum of the per-example losses. Because the optimization problem is convex, we used
Table B.1: Regularizers and alternative losses improve Inception v3 accuracy on ImageNet. Accuracy(mean ± standard error of 3 models) with different losses/regularizers on the ImageNet validation set. Losses aresorted from lowest to highest top-1 accuracy. Accuracy values not significantly different from the best (p > 0.05,t-test) are bold-faced.
Table B.2: Regularizers and alternative losses improve performance on out-of-distribution test sets. Ac-curacy averaged over 8 ResNet-50 models per loss.
Table B.3: Regularizers and alternative losses may or may not improve calibration. We report negative loglikelihood (NLL) and expected calibration error (ECE) for each loss on the ImageNet validation set, before andafter scaling the temperature of the probability of the distribution to minimize NLL, as in Guo et al. (2017). ECEis computed with 15 evenly spaced bins. For networks trained with sigmoid loss, we normalize the probabilitydistribution by summing probabilities over all classes.
Table B.4: Training accuracy of ResNet-50 models.
Table C.1: Combining final-layer regularizers and/or improved losses does not enhance performance.
Table C.2: AutoAugment and Mixup provide consistent accuracy gains beyond well-tuned losses andregularizers. Top-1 accuracy of ResNet-50 models trained with and without AutoAugment, averaged over3 (with AutoAugment) or 8 (without AutoAugment) runs. Models trained with AutoAugment use the losshyperparameters chosen for models trained without AutoAugment, but the point at which to stop training waschosen independently on our holdout set. For models trained with Mixup, the mixing parameter α is chosen from[0.1, 0.2, 0.3, 0.4] on the holdout set. Best results in each column, as well as results insignificantly differentfrom the best (p > 0.05, t-test), are bold-faced.
Table C.3: Comparison with state-of-the-art. All results are for ResNet-50 models trained with AutoAugment.
Table E.1: Comparison of class separation under different distance metrics. Cosine (mean-subtracted)subtracts the mean of the activations before computing the cosine distance. All results reported for ResNet-50 onthe ImageNet training set.
