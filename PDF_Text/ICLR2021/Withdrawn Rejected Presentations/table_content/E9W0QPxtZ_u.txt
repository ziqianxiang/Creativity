Table 1: The learned samplers above are all realized with BigGAN/BigGAN-deep architecture. Ona small compute budget, it is not only possible to train the Learned-W-64 sampler well, but alsothe nsb -GAN-W model (with Learned-W-64 sampler) beats the state-of-the-art VQVAE-2 model(before truncation/rejection sampling), which has an extremely high cost of training similar to thatof the BigGAN model. We defer a more detailed discussion about truncation and rejection samplingin Appendix B.1 and showcase a more detailed set of results in Table 4 in the Appendix B.1.
Table 2: All pretrained samplers above are realized with BigGAN. In higher dimensions, comparedto the baseline BigGAN models at the respective resolutions, nsb-GAN models reduce the trainingcompute budget by up to four times (last row) while suffering only a minor increase in FID. Allpre-trained samplers above are trained for approximately two days on the compute described. Notethat the decoder is only trained once, but generalizes across all the resolutions. This amortizationfurther reduces the training cost drastically. Pretrained-128-64, for example, indicates that the modelgenerates at 128 × 128 resolution and we down-sample it to 64 × 64 resolution with our encodersfor nsb-GAN models. As empirically tested and confirmed by Razavi et al. (2019), IS is highlysensitive to slight blurs and perturbations. Therefore, we include an expanded set of quantitativeresults with various truncation and rejection sampling levels in Table 5 in the Appendix B.2.
Table 3: MSE on training and validation set for UNet-P, UNet-W,and VQVAE-2 models. Smalldifference between the training and validation error suggests that the models generalize well.
Table 4: Minimum FID / IS (column 4) and FID / Minimum IS (column 5) attained with differentlevels of truncation [1.0, 0.8, 0.6, 0.4, 0.2, 0.1] and rejection sampling [0.70, 0.80, 0.90, 0.95].
Table 5: Minimum FID / IS (column 4) and FID / Minimum IS (column 5) attained with differentlevels of truncation [1.0, 0.8, 0.6, 0.4, 0.2, 0.1] and rejection sampling [0.70, 0.80, 0.90, 0.95].
Table 6: Hyperparameters of learned samplers (wavelet and pixel)20Under review as a conference paper at ICLR 2021Figure 11: Additional class-conditional random samples at 512 × 512. Classes from the top row:727 planetarium, 780 schooner, 853 thatch, 970 alp, and 992 agaric.
Table 7: Hyperparameters of ESRGAN decoders21Under review as a conference paper at ICLR 2021Figure 12: Full 256 × 256 resolution sample from NSB-GAN-W with pre-trained sampler.
Table 8: Hyperparameters of UNet-W decoders (level 1 and level 2)24Under review as a conference paper at ICLR 2021Figure 17: Full 512 × 512 resolution sample from NSB-GAN-W with pre-trained sampler.
Table 9: Hyperparameters of UNet-P decoders (level 1 and level 2)25Under review as a conference paper at ICLR 2021Figure 18: Full 512 × 512 resolution sample from NSB-GAN-W with pre-trained sampler.
Table 10: The above pre-trained samplers are StyleGAN-2 model trained on LSUN Church dataset.
