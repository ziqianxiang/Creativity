Table 1: Comparison of architectures found by searching ButterfLeNet on CIFAR-10. Results are averagesover five random seeds affecting both search and offline evaluation. When warm starting with convolutions,offline evaluation of Supernet SGD outperforms fixed convolutions by 1% on both CIFAR-10 and on transferto CIFAR-100. Furthermore, when search is initialized to a random operation, offline evaluation of SupernetSGDR matches the performance of fixed convolutions. Finally, we find that when warm starting with convolu-tions, SuPemet evaluation of ButterfLeNet using SuPernet SGDR attains the best performance.
Table 2: Comparison of fixed operations and K-operations found via Supernet SGD over ButterfLeNet-Text.
Table 3: Comparison of fixed operation baselines to ButterfLeNet trained using Supernet SGD on permutedCIFAR-10 and CIFAR-100. K-operations trained from scratch outperform all other methods on supernet eval-Uation, offline evaluation, and transfer to CIFAR-100.________________________________________	fixed operation baselines	Supernet SGD dataset	linear Conv K-layer from scratch warm start CIFAR-10 (SuPernet)	-	-	-	74.69	7022- CIFAR-10 (offline)	59.61	58.90	66.16	72.99	69.56 CIFAR-100(ofline*)	27.89	31.41	37.36	42.73	40.42 * For Supernet SGD, a K-Op found on CIFAR-10 is transferred to CIFAR-100.
Table 4: Comparison of spherical convolutions, convolutions, and NAS on the ButterfLeNet-Spherical searchspace, built atop the convolutional baseline of Cohen et al. (2018). We test on the stereographically projectedspherical MNIST as well as a rotated variant (Cohen et al., 2018).
Table 5: Performance on CIFAR-10 of a larger variant of LeNet (“Wide LeNet”) with a greaternumber of channels. Here, supernet evaluation of warm started Supernet SGD achieves the highestperformance on CIFAR-10.
