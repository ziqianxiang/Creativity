Table 1: Effect of regularization class-IL average accuracy, secondary information (on the first-task model)and forgetting rates (5 tasks), on CIFAR-100. All the values are averaged over 3 runs. Values that are betterthan the CCIL baseline are marked in green whereas the worse ones are marked in red. SD:self-distillation,LS:label-smoothing. Standard deviation in Appendix A.
Table 2: Drawing parallels between iCaRL and our proposed model. Average accuracy is reported for 5-taskclass-IL experiments on CIFAR-100 dataset. Last row highlights our proposed changes. All methods userandom exemplar selection, Dot: linear layer, KD: knowledge distillation, NME: nearest-mean-of-exemplars.
Table 3: Comparing average incremental accuracy computed using different methods on CIFAR-100,ImageNet-100 and ImageNet dataset. *as reported in Hou et al. (2019)7	ConclusionsWe presented a straightforward class-incremental learning system that focuses on the essential com-ponents and already exceeds the state of the art without integrating sophisticated modules. Thismakes it a good base model for future research on advancing class-incremental learning.
Table 4: The effect of overfitting on class-IL performance and its correlation with secondary information.
Table 5: Expected Calibration Error for different snapshots (every 100th epoch) of the overfitted model.
Table 6: Effect of regularization on secondary information. All the metrics are evaluated on the network trainedon the first task. ] and ↑ in the column headings indicate that lower and higher values are better respectively.
Table 7: Effect of regularization on class-IL performance. All the metrics are evaluated on the network trainedon the first task. ] and ↑ in the column headings indicate that lower and higher values are better respectively.
