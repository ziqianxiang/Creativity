Table 1: Properties of recent attention-based networksAttention moduleMethod	Global	Content	Positional	Attention + CNN combinationWang et al. (2018)	X	X		Chen et al. (2018)	X	X		Few attention modules are inserted inYue et al. (2018)	X	X		between residual blocksShen et al. (2018)	X	X		Huang et al. (2019)	X	X		Carion et al. (2020)	X	X	X	Attention modules are added at the endSun et al. (2019)	X	X	X	Bello et al. (2019)	X	X	X	Convolution layers are augmented with attention modules in parallelHu et al. (2019)		X	X	Ramachandran et al. (2019)		X	X	Convolution layers are replaced byZhao et al. (2020)		X	X	attention modulesWang et al. (2020)	X	X	X	This work	X	X	X	networks. All existing works except Wang et al. (2020) either insert their attention modules intoCNNs as auxiliary blocks (Bello et al., 2019; Chen et al., 2018; Huang et al., 2019; Shen et al., 2018;Wang et al., 2018; Yue et al., 2018; Carion et al., 2020; Sun et al., 2019) at later stages of the networkor constrain their attention mechanism to small local regions (Hu et al., 2019; Ramachandran et al.,
Table 2: Comparison of the proposed GSA module with axial attentionAttention module	Top-1 acc.	Top-5 acc.	Parameters	FLOPs	RuntimeProposed GSA module	78.5	93.9	18.1 M	7.2 G	31.7 msAxial content and positional attention	77.5	93.6	18.1 M	7.3 G	32.5 msOnly axial positional attention	77.4	93.5	16.8 M	6.4G	28.2 msTable 3: Comparison of GSA networks (3 × 3 convolutions replaced with GSA modules) with recentattention-based approaches. Note that Wang et al. (2020) is the conv-stem version since GSA-Netuses a conv stem. M-ResNet-50 is a modified ResNet-50 which halves the number of input andoutput channels of all residual blocks and scales the number of filters in every layer by 1.125Structure	Method	Top-1 acc.	Top-5 acc.	Params	FLOPs	RuntimeResNet-50	Chen et al. (2018)	77.0	93.5	-	-	-	Shen et al. (2018)	77.3	93.6	26.2 M	9.9 G	-	Hu et al. (2019)	77.3	93.6	23.3 M	8.6 G	-	Ramachandran et al. (2019)	77.6	-	18.0 M	7.0 G	131.9 ms	Yue et al. (2018)	77.7	93.6	-	-	-	Bello et al. (2019)	77.7	93.8	25.8 M	8.3 G	34.9 ms	Zhao et al. (2020)	78.2	93.9	20.5 M	6.6 G	-	This work	78.5	93.9	18.1 M	7.2 G	31.7 msResNet-101	Hu et al. (2019)	78.5	94.3	42.0 M	16.0 G	-	Bello et al. (2019)	78.7	94.4	45.4 M	16.1 G	61.9 ms
Table 3: Comparison of GSA networks (3 × 3 convolutions replaced with GSA modules) with recentattention-based approaches. Note that Wang et al. (2020) is the conv-stem version since GSA-Netuses a conv stem. M-ResNet-50 is a modified ResNet-50 which halves the number of input andoutput channels of all residual blocks and scales the number of filters in every layer by 1.125Structure	Method	Top-1 acc.	Top-5 acc.	Params	FLOPs	RuntimeResNet-50	Chen et al. (2018)	77.0	93.5	-	-	-	Shen et al. (2018)	77.3	93.6	26.2 M	9.9 G	-	Hu et al. (2019)	77.3	93.6	23.3 M	8.6 G	-	Ramachandran et al. (2019)	77.6	-	18.0 M	7.0 G	131.9 ms	Yue et al. (2018)	77.7	93.6	-	-	-	Bello et al. (2019)	77.7	93.8	25.8 M	8.3 G	34.9 ms	Zhao et al. (2020)	78.2	93.9	20.5 M	6.6 G	-	This work	78.5	93.9	18.1 M	7.2 G	31.7 msResNet-101	Hu et al. (2019)	78.5	94.3	42.0 M	16.0 G	-	Bello et al. (2019)	78.7	94.4	45.4 M	16.1 G	61.9 ms	This work	79.6	94.5	30.4 M	12.2 G	57.2 msM-ResNet-50	Wang et al. (2020)	77.5	-	12.4 M	5.6 G	41.4 ms	This work	78.2	93.9	12.7 M	6.0 G	31.1 ms4.3	Comparison with existing attention-based approachesTable 3 compares GSA networks with recent attention-based networks. The GSA networks achieve
Table 4: Comparison of different variants ofthe proposed GSA moduleAttention component AccuracyTable 5: Effect of replacing convolutions withGSA modules (indicated using X) at differentstages of ResNet-50Content	Col.	Row	Top-1	Top-5	Residual group	AccuracyX	X	X	78.5	93.9	12	3	4	Top-1 Top-5 Runtime	X	X	77.4	93.5	XXXX	78.5	93.9	31.7 msX	X		76.2	92.7	XXX	78.7	94.1	28.5 msX		X	75.4	92.2	XX	78.5	94.1	24.5 ms					X	77.7	93.7	23.2 ms	X		72.6	90.8		—		X	70.2	89.4	ResNet-50 CNN	76.9	93.5	22.6 msX			70.8	89.5	ResNet-101 CNN	78.7	94.4	39.3 ms(row3 vs row4 and row5 vs row6) suggesting that modeling pixel interactions along the verticaldimension is more important than the horizontal dimension for categories in the ImageNet dataset.
Table 5: Effect of replacing convolutions withGSA modules (indicated using X) at differentstages of ResNet-50Content	Col.	Row	Top-1	Top-5	Residual group	AccuracyX	X	X	78.5	93.9	12	3	4	Top-1 Top-5 Runtime	X	X	77.4	93.5	XXXX	78.5	93.9	31.7 msX	X		76.2	92.7	XXX	78.7	94.1	28.5 msX		X	75.4	92.2	XX	78.5	94.1	24.5 ms					X	77.7	93.7	23.2 ms	X		72.6	90.8		—		X	70.2	89.4	ResNet-50 CNN	76.9	93.5	22.6 msX			70.8	89.5	ResNet-101 CNN	78.7	94.4	39.3 ms(row3 vs row4 and row5 vs row6) suggesting that modeling pixel interactions along the verticaldimension is more important than the horizontal dimension for categories in the ImageNet dataset.
Table 6: Comparison between CNNs and GSA networksStructure	Operation	ImageNet		CIFAR-100	Params	FLOPs	Runtime		Top-1 acc.	Top-5 acc.	Top-1 acc.			ResNet-38	Convolution	75.9	92.9	80.8	19.6 M	6.4 G	17.2 ms	GSA	(+1.9) 77.8	93.6	(+2.5) 83.3	14.2 M	5.9 G	24.7 msResNet-50	Convolution	76.9	93.5	81.2	25.6 M	8.2 G	22.6 ms	GSA	(+1.6) 78.5	93.9	(+2.7) 83.9	18.1 M	7.2 G	31.7 msResNet-101	Convolution	78.7	94.4	82.8	44.5 M	15.6 G	39.3 ms	GSA	(+0.9) 79.6	94.5	(+1.6) 84.4	30.4 M	12.2 G	57.2 msB Mathematical implementation detailsThis section presents mathematical implementation details of the Global Self-Attention (GSA) mod-ule to supplement the high-level description in Section 3 of the paper.
