Table 1: Classification performance of a one hidden layer MLP classifier trained on frozen featuresfrom the mentioned encoder models (trained using Neural Bayes-MIM) and datasets (no dataaugmentation used in experiments we ran). STL-10 was resized to 32 × 32 in our runs instead of64 × 64 as in DIM due to memory restrictions. Performance reported from DIM paper are their bestnumbers (omitting STL-10 due to difference in image size).
Table 2: Effect of mini-batch size (MBS) and batch size (BS) on final test accuracy of NeuralBayes-MIM on CIFAR-10. Gradients are computed using batches of size MBS and accumulated untilBS samples are seen before parameter update. As expected, a sufficiently large MBS is needed forcomputing high fidelity gradients due to Ex[Lk(x)] term. Gradient accumulation using BS furtherhelps. All models are trained for the same number of epochs.
