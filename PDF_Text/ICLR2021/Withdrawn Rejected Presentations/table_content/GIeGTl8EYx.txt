Table 1: Comparison on test accuracy / F1-micro score and inference cost tuned with DropEdge)Method	Layers	Flickr		Reddit		Yelp		ogbn-arxiv		ogbn-products			Accuracy	Cost	Accuracy	Cost	F1-micro	Cost	Accuracy	Cost	Accuracy	CostGCN	3	0.516±0.002	2E0	0.953±0.000	6E1	0.402±0.002	2E1	0.717±0.003	1E0	0.756±0.002	5E0	5	0.522±0.002	2E2	0.949±0.001	1E3	ooM	1E3	0.719±0.002	2E0	ooM	9E2GraphSAGE	3	0.514±0.001	5E0	0.965±0.000	9E1	0.617±0.003	3E1	0.719±0.003	1E0	0.785±0.001	8E0	5	0.515±0.005	3E2	0.962±0.000	2E3	ooM	3E3	0.719±0.004	3E0	ooM	2E3GAT	3	0.507±0.003	3E1	ooM	5E2	OOM	3E2	0.720±0.001	1E0	ooM	6E1	5	0.516±0.003	4E2	ooM	3E3	ooM	4E3	ooM	5E0	ooM	4E3GraphSAGE	3	0.517±0.003	5E0	0.967±0.000	9E1	0.645±0.001	3E1	0.710±0.000	1E0	0.792±0.002	8E0+ GraphSAINT-RW	5	0.520±0.003	3E2	0.967±0.001	2E3	0.639±0.000	3E3	0.701±0.002	3E0	0.796±0.002	2E3GAT	3	0.522±0.005	3E1	0.967±0.000	5E2	0.645±0.000	3E2	0.697±0.000	1E0	0.802±0.003	6E1+ GraphSAINT-RW	5	0.515±0.003	4E2	0.965±0.002	3E3	0.647±0.001	4E3	0.695±0.001	5E0	0.799±0.007	4E3SHADOW-GCN	3	0.526±0.002	(1)	0.958±0.000	⑴	0.526±0.001	(1)	0.719±0.002	(1)	0.777±0.003	(1)+PPR	5	0.527±0.002	1E0	0.958±0.000	1E0	0.527±0.001	2E0	0.721±0.003	2E0	0.784±0.003	2E0shaDow-SAGE	3	0.529±0.001	2E0	0.966±0.000	2E0	0.649±0.000	2E0	0.716±0.001	2E0	0.799±0.001	2E0+ 2-hop	5	0.534±0.004	3E0	0.966±0.000	3E0	0.650±0.000	3E0	0.718±0.001	3E0	0.801±0.002	3E0shaDow-SAGE	3	0.534±0.002	2E0	0.969±0.000	2E0	0.651±0.000	2E0	0.723±0.003	3E0	0.794±0.002	2E0+ PPR	5	0.542±0.002	3E0	0.969±0.000	3E0	0.650±0.000	3E0	0.725±0.001	3E0	0.804±0.002	3E0shaDow-GAT	3	0.538±0.003	2E0	0.970±0.001	2E0	0.655±0.000	2E0	0.724±0.001	3E0	0.801±0.001	2E0
Table 2: Test accuracy (%) on other architectures	Flickr		Reddit		ogbn-arxiv		Normal	SHADOW	Normal	SHADOW	Normal	SHADOWJK (3)	49.45±0.70	53.17±0.27	96.49±0.10	96.82±0.03	71.30±0.26	72.01±0.17JK (5)	49.40±0.83	53.28±0.26	96.40±0.13	96.85±0.06	71.66±0.53	72.26±0.24GIN (3)	51.32±0.31	52.28±0.28	93.45±0.34	95.78±0.06	70.87±0.16	71.73±0.29GIN (5)	50.04±0.67	52.55±0.23	75.50±0.39	95.52±0.07	69.37±0.62	71.40±0.27accuracy improvement on deeper models. Compared with the normal JK-Net, increasing the depthbenefits shaDow-JK more. The GIN architecture theoretically does not oversmooth. However, weobserve that the GIN training is very sensitive to hyperparameter settings. We hypothesize that sucha challenge is due to the sensitivity of the sum aggregator on noisy neighbors (e.g., for GraphSAGE,a single noisy node can hardly cause a significant perturbation on the aggregation, due to the averag-ing over the entire neighborhood). The accuracy improvement of shaDow-GIN compared with thenormal GIN may thus be due to noise filtering by shallow sampling (see Section 3.1). The impactof noises / irrelevant neighbors can be critical, as reflected by the 5-layer GIN accuracy on Reddit.
Table 3: GCN test accuracy (%)L0	SAMPLE	FliCkr	ogbn-produCts3	PPR	52.57±0.21	77.73±0.32	2-hop	52.10±0.23	77.94±0.395	PPR	52.73±0.20	78.36±0.34	Ensemble	53.04±0.17	78.58±0.217	PPR	52.25 ±0.23	78.50±0.44does not degrade. In the middle and right plots, we train the standard GCN architecture without theSGC simplications. Since ogbn-products is too large, we compare shaDow-GCN with GCN-SAINT. For both methods, the deeper models converge slower. Deeper models (whether they areGNNs or other types of NNs) are generally harder to optimize. For shaDow-GCN, the validationperformance of the 15-layer model eventually catches up with the shallower ones. For GCN-SAINT,however, there consistently exists a large performance gap between the 15-layer model and the shal-lower ones. Both experiments clearly show that shallow sampling prevents oversmoothing. Finally,for ogbn-products in in Table 3, we note that even through > 98% of the subgraph nodes are with2 hops (see Figure 6), increasing the GNN depth can benefit accuracy for up to L0 = 7 layers.
Table 4: Average number of nodes touched by the approximate PPR computationDataset	Average 2-hop size	Average # nodes touched by PPRReddit	11093	27751Yelp	2472	5575ogbn-products	3961	5405D Detailed Experimental S etupD. 1 Additional Dataset DetailsThe statistics for the five benchmark graphs is listed in Table 5. Note that for Yelp, each nodemay have multiple labels, and thus we follow the original paper (Zeng et al., 2020) to report its16Under review as a conference paper at ICLR 2021F1-micro score. For all the other graphs, a node is only associated with a single label, and so wereport accuracy. Note that for Reddit and Flickr, other papers (Hamilton et al., 2017a; Zeng et al.,2020) also report F1-micro score as the metric. However, since each node only has a single label,“F1-micro score” is exactly the same as “accuracy”.
Table 5: Dataset statisticsDataset	Setting	Nodes	Edges	Degree	Feature	Classes	Train / Val / TestFlickr	Inductive	89,250	899,756	10	500	7	0.50 / 0.25 / 0.25Reddit	Inductive	232,965	11,606,919	50	602	41	0.66 / 0.10 / 0.24Yelp	Inductive	716,847	6,977,410	10	300	100	0.75 / 0.10 / 0.15ogbn-arxiv	Transductive	169,343	1,166,243	7	128	40	0.54 / 0.18 / 0.29ogbn-products	Transductive	2,449,029	61,859,140	25	100	47	0.10 / 0.02 / 0.88D.2 Hardware Specification and EnvironmentWe run our experiments on a DGX1 machine with Dual 20-core Intel Xeon CPUs (E5-2698 v4@ 2.2Ghz) and eight NVIDIA Tesla P100 GPUs (15GB of HBM2 memory). The main memory is512GB DDR4 memory. The code is written in Python 3.6.8 (where the sampling part is writtenwith C++ parallelized by OpenMP, and the interface between C++ and Python is via PyBind11).
Table 6: Training configuration of s haD ow- GNN for Table 1 (PPR sampler)							Arch.	Dataset	Layers	Learning Rate	Dropout	DropEdge	Budget p	Threshold θ	Flickr	3	0.001	0.40	0.10	200	-		5	0.001	0.40	0.10	200	-	Reddit	3	0.00002	0.20	0.05	150	-		5	0.00002	0.20	0.05	150	-shaDow-GCN	Yelp	3	0.001	0.10	0.00	100	-		5	0.001	0.10	0.00	100	-	ogbn-arxiv	3	0.00002	0.25	0.10	200	-		5	0.00002	0.25	0.10	200	-	ogbn-products	3	0.002	0.40	0.05	150	-		5	0.002	0.40	0.05	150	-	Flickr	3	0.001	0.40	0.00	200	-		5	0.001	0.40	0.00	200	-	Reddit	3	0.0002	0.20	0.10	150	-		5	0.0002	0.20	0.10	150	-shaDow-SAGE	Yelp	3	0.001	0.10	0.00	200	-		5	0.001	0.10	0.00	200	-	ogbn-arxiv	3	0.0001	0.30	0.10	500	0.01		5	0.00002	0.25	0.15	200	0.01
Table 7: Training configuration of SHADOW-GNN for Table 1 (k-hop sampler)Arch.	Dataset	Layers	Learning Rate	Dropout	DropEdge	Budget b	Depth k	Flickr	3	0.001	0.40	0.00	20	2		5	0.001	0.40	0.00	20	2	Reddit	3	0.0001	0.20	0.00	15	2		5	0.0001	0.20	0.00	15	2shaDow-SAGE	Yelp	3	0.01	0.10	0.00	5	2		5	0.01	0.10	0.00	5	2	ogbn-arxiv	3	0.0001	0.20	0.00	10	2		5	0.0001	0.20	0.00	10	2	ogbn-products	3	0.002	0.35	0.00	10	2		5	0.002	0.35	0.00	10	2•	Budget b ∈ {5, 10, 15, 20}The hyperparameters to reproduce the Table 1 shaDow-GNN results are listed in Tables 6 and 7.
