Table 1: Mean error (in degree) With different number of observations and different test sparsitype	q	ls	l1	LISTA-SS	EBT-LISTA-SS	15	3.41	0.678	5.50 × 10-2	4.09 × 10-20.8	25	3.05	0.408	7.48 × 10-3	3.17 × 10-3	35	2.78	0.336	1.89 × 10-3	5.95 × 10-4	15	1.94	0.232	6.67 × 10-3	2.57 × 10-30.9	25	0.145	2.03	1.33 × 10-3	1.64 × 10-4	35	1.61	0.088	2.93 X 10-4	4.91 × 10-56 ConclusionIn this paper, We have studied the thresholds in the shrinkage functions of LISTA. We have proposeda novel EBT mechanism that Well-disentangles the learnable parameter in the shrinkage functionon each layer of LISTA from its layer-Wise reconstruction error. We have proved theoretically that,in combination With LISTA and its variants, our EBT mechanism leads to faster convergence andachieves superior final sparse coding performance. Also, We have shoWn that the EBT mechanismsendoW deep unfolding models higher adaptivity to different observations With a variety of sparsity.
