Table 1: Details of datasets used in experimentsDataset	Images	Classes	Train-val-test	Resolution(after resize)miniImageNet	60000	-100-	-64-16-20-	80×80CUB-200-2011	11788	-200-	100-50-50	80×80Cars	16185	-196-	-98-49-49-	80×80Places	73000	-365-	183-91-91	80×80Plantae	47242	200	100-50-50 —	80×80compute meta loss again on the same task, and cast the loss as evaluation of update quality in thefirst stage. Then the loss is used to guide the update of the auto-view modules via gradient descent:Yt+1 = Yt- ηVγtLmeta(DS, DQ,θt+1),	(5)where η is the learning rate of the auto-view modules. We mention again that θt+1 can be cast asa function of Yt. Thus the loss depends on Yt through the computation graph of θt+1 in first stage.
Table 2: Comparative results for 5-way classification on miniImageNet. Average accuracies onthe meta-test set with 95 confidence interval are reported. f denotes methods using external textinformation.去 denotes result reported in (GidariS et al., 2019).
Table 3: Comparative results for 5-way classification on CUB. Average accuracies on the meta-testset with 95 confidence interval are reported.
Table 4: Results for 5-way few-shot classification on three fine-grained datasets: Cars, Places andPlantae. Average accuracies on the meta-test set are reported.
Table 5: Quantitative evaluation of auto-view module. PN+CL denotes model with traditionalrandom-view Contrastive learning.
Table 6: Effects of the β value in AVCL on model performances.
