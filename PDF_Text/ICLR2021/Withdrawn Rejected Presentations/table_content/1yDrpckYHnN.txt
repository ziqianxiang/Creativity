Table 1: Comparison with other objectives from the perspective of XEnDec with the inputs. Eachrow shows an input to XEnDec and its relation to existing work. ymask is a sentence of length |y|containing only “hmaski” tokens. n(y) is a sentence obtained by corrupting all the words in ythrough randomly shuffling and dropping. xadv and yadv are adversarial sentences in which all thewords are substituted with adversarial words.
Table 2: Experiments on WMT’14 English-German and WMT’14 English-French translation.
Table 4: Results on artificial noisy inputs. “CS”:code-switching noise. “DW”: drop-words noise.
Table 3: Results on F2 -XEnDec + BackTranslation. English-German is based on theTransformer-base model and English-French isbased on the Transformer-big model.
Table 5: Effect of monolingual corpora sizes.
Table 6: Finetuning vs. Joint Training.
Table 7: Ablation study on English-German.	Different Settings	BLEUTransformer	28.70F2 -XEnDeC	30.46without LF1	29.55without LF2	29.21without dropout A and model predictions	29.87without model predictions	30.24the second XEnDeC is replaced by Mixup	29.67LS with XEnDeC over parallel data	29.235	Related WorkThe recent past has witnessed an increasing interest in the research community on leveraging pre-training models to boost NMT model performance (Ramachandran et al., 2016; Lample & Conneau,2019; Song et al., 2019; Lewis et al., 2019; Edunov et al., 2018; Zhu et al., 2020; Yang et al., 2019).
