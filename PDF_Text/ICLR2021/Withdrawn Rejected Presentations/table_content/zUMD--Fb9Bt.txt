Table 1: Test accuracy (%) on transductive learning datasets. We report mean values and standarddeviations in 30 independent experiments. The best results are highlighted with boldface.
Table 2: Test Micro-F1 Score on indUctive learn-ing dataset. We rePort mean valUes and standarddeviations in 5 indePendent exPeriments.
Table 3: Comparison results on transductive learning datasets. We report mean values and standarddeviations in 30 independent experiments. The best results are highlighted with boldface. Thenumber in the brackets represent the number of GCN layers when achieving the best performance.
Table 4: Reformulation of convolution-based graph neural networks. D and di in the attention-basedmodules are normalization coefficients.
Table 5: DataSet StatiSticSDataSet	Cora	CiteSeer	Pubmed	PPINodeS	2,708	3,327	19,717	56,944(24 graphS)EdgeS	5,429	4,732	44,338	818,716FeatureS	1,433	3,703	500	50ClaSSeS	7	6	3	121(multilabel)Training NodeS	140	120	60	44,906(20 graphS)Validation NodeS	500	500	500	6,514(2 graphS)TeSt NodeS	1,000	1,000	1,000	5,524(2 graphS)C.	Data Statistics and Experimental SetupsWe conduct experimentS on four real-world graph dataSetS, whoSe StatiSticS are liSted in Table 5. FortranSductive learning, we evaluate our method on the Cora, CiteSeer, Pubmed dataSetS, followingthe experimental Setup in (Sen et al., 2008). There are 20 nodeS per claSS with labelS to be uSedfor training and all the nodeS’ featureS are available. 500 nodeS are uSed for validation and thegeneralization performance iS teSted on 1000 nodeS with unSeen labelS. PPI (Zitnik & LeSkovec,2017) iS adopted for inductive learning, which iS a protein-protein interaction dataSet containing 20graphS for training, 2 for validation and 2 for teSting while teSting graphS remain unobServed duringtraining.
Table 6: Test accuracy (%) on transductive learning datasets with random slits. We report meanvalues and standard deviations of the test accuracies over 100 random train/validation/test splits.
Table 7: Training and test time on Cora. We report mean values in 5 independent experiments. Thebest results are highlighted with boldface.
Table 8: Ablation study on the regularization strength. We report mean values and standard devia- tions in 30 independent experiments. The best results are highlighted with boldface.			Regularization Strength	Citeseer	Cora	Pubmed0	70.3±0.4	81.5±0.5	79.0±0.40.01	71.7±0.6	83.0±0.5	79.2±0.60.05	72.2±0.4	83.6±0.3	79.8±0.20.1	72.4±0.5	83.5±0.2	79.4±0.70.2	68.4±1.0	76.8±1.3	70.2±2.2F.	Ablation StudyTo analyze the effects of the regularization strength, we conduct experiments on three transductivedatasets and present the results in Table 8. As we can observe, with reasonable choice of the regu-larization strength, our approach can achieve consistent improvement under all settings. However,when the regularization strength is too large, the training procedure becomes unstable and the modelperformance suffers from a severe decrease.
