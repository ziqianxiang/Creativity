Table 1: Notation for Sec. 4.3 (binary symmetric HLTM).
Table 2: Normalized Correlation between the topmost latent variables in binary HLTM and topmost nodes indeep ReLU networks (L = 5) go up when training with SimCLR with NCE loss. We see higher correlations atboth initialization and end of training, with more over-parameterization (Left:ρμν〜Uniform[0.7,1]〜Uniform[θ.8, l]〜Uniform[θ.9, l]Initial0.510.650.811 epoch0.690.760.8520 epochs0.760.790.86ρμν
Table 3: Top-1 STL performance with different combination of predictor (P), EMA and BatchNorm usingBYOL. EMA means γema = 0.996. Batch size is 128 and all experiments run on 5 seeds and 100 epochs.
Table 4: Top-1 STL performance using different BatchNorm components in the predictor and the projectorof BYOL (Yema = 0.996, 100 epochs). There is no affine part. ''μ'' = zero-mean normalization only, ''μ,σ''=BN without affine, “'μ, σ才” =normalization with mean and Std but only backpropagating through mean. Allvariants with detached zero-mean normalization (in red) yield similar poor performance as no normalization.
Table 5: Top-1 performance of BYOL using reinitialization of the predictor every T epochs.
Table 6: Extended notation in HLTM.
Table 7: Training one-layer predictor with positive initial weights and no EMA (γema = 0). Allexperiments run for 3 seeds.
Table 8: Training one-layer predictor with positive initial weights with EMA (γema = 0.996) andpredictor resetting every T = 10 epochs. All experiments run for 3 seeds. Note that Xavier range isUniform[-0.15, 0.15] and our initialization range is much smaller than that.
Table 9: Same as Tbl. 8 but with different weight range. All experiments run for 3 seeds.
Table 10: Top-1 Performance on STL-10 with a two-layer predictor with BN and EMA(γema = 0.996). Learning rate is smaller (0.02) and predictor weight sampled fromUniform[-range, range]. Note that for this, Xavier range is Uniform[-0.097, 0.097] and ourrange is smaller.
