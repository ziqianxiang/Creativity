Table 1: Results on toy tasks. Char is char-acter accuracy, Seq is sequence accuracy.
Table 2: Results on WMT En-De compar-ing the Feedback Transformer to Recurrentarchitectures, hybrid Recurrent-Transformermodels, and standard Transformers.
Table 3: Results on WikiText-103. We re-port perplexity on test.
Table 4: Results on Enwiki8. We report bit-per-byte on test.
Table 5: Ablation on WikiText-103 of various modeing choices. Results are shown withoutfinetuning.
Table 6: An example program from the algorithmic task with 3 variables.
Table 7: Hyperparamers for sequence to sequence experiments.
Table 8: Hyperparamers for language modeling experiments. Here * indicates the adaptive span.
