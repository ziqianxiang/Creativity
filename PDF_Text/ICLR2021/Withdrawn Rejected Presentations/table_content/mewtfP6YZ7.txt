Table 1: The asymptotic performance of various model designs in the offline setting. In this settingthere is no exploration and the training dataset is pre-collected and fixed for all the models. Thereported numbers are the 90th percentile out of 100 evaluation episodes, averaged across threedifferent runs. This table indicates a significant performance improvement by predicting imagesalmost across all the tasks. Moreover, there is a meaningful difference between the numbers in thistable and Figure 2 signifying the importance of exploration in the online settings. Please note howsome of best-performing models in this table performed poorly in Figure 2.
Table 2: Median reward prediction error (LR) of each model across all of the trajectories in evaluationpartition of the offline dataset. This table demonstrates a generally better task performance formore accurate models in the offline setting, when compared with Table 1. The last row reports thePearson correlation coefficient between the reward prediction error and the asymptotic performancefor each task across models. This row demonstrates the strong correlation between reward predictionerror (LR) and task performance (S) in the absence of exploration. In cases which all models areclose to the maximum possible score of 1000 (such as baii_in_cup_catch) the correlation can bemisleading because a better prediction does not help the model anymore.
Table 3: Pearson correlation coefficient ρ between image prediction error LO, reward prediction errorLR and asymptotic score S . To calculate the correlation, we scaled down OT OR and LT LR at mul-tiple levels to limit their modeling capacity and thereby potentially increase their prediction error. μsand σS are the average and the standard deviation of asymptotic performances across different scalesof each model. In cases with low standard deviation of the scores (such as baii_in_Cupqat ch),meaning all version of the models did more or less the same, the correlation can be misleading. Thistable demonstrates the strong correlation between image prediction error and task performance.
Table 4: Summary of possible model designs based on whether or not to predict the future observations.
Table 5: The asymptotic performance of various model designs in the online and offline settingsand their differences. For the online setting the reported numbers are the average (and standarddeviation) across three runs after the training. For the offline setting, the reported numbers are thesame as Table 1 rounded up ± their standard deviation across three runs. This table demonstrates asignificant performance improvement by predicting images almost across all the tasks. Moreover,there is a meaningful difference between the results for the online and the offline settings signifyingthe importance of exploration. Please note how some of best-performing models in the offline settingperform poorly in the online setting and vice-versa. This is clear from the bottom section of the tablewhich includes the absolute difference of offline and online scores.
Table 6: Wall clock time of training and inference step of each model. The numbers are in seconds.
Table 7: Cost-normalized scores foe each model. These numbers are the online score achieved byeach model divided by the inference cost (time) of the corresponding model. As expected, fastermodels - i.e. R and LTLR which do not predict the images at inference time - get a big advantage.
Table 8: The architecture of Rrecurrent.		Layer Type	Filters Size Strides	ActivationLast Observation Encoder		Convolutional	32	3x3	2~~	LeakyReLUConvolutional	64	3x3	2	LeakyReLUConvolutional	16	3x3	2	LeakyReLUConvolutional	8	3x3	2	LeakyReLUAction Encoder		Dense	32	-	-	LeakyReLUDense	16	-	-	LeakyReLUDense	8	-	-	LeakyReLULast Reward Encoder		Dense	32	-	-	LeakyReLUDense	16	-	-	LeakyReLUDense	8	-	-	LeakyReLUReward Predictor		Dense	32	-	-	LeakyReLUDense	16	-	-	LeakyReLUDense	planning horizon	-	-	NoneTable 9: The architecture of Rconv
Table 9: The architecture of RconvParameter	ValuePlanning Horizon	12Optimization Iterations	10Number of Candidate Trajectories	128Number of Selected Trajectories	12Table 10: The hyper-parameters used for CEM. We used the same planning algorithm across allmodels and tasks.
Table 10: The hyper-parameters used for CEM. We used the same planning algorithm across allmodels and tasks.
Table 11: The hyper-parameter values used for SV2P (Babaeizadeh et al., 2018; Finn et al., 2016a)model (used in OT OR and OT LR). The number of ConvLSTM filters can be found in Table 14.
Table 12: The hyper-parameter values used for PlaNet (Hafner et al., 2018) model (used in LT ORand LT LR). The rest of parameters are the same as Hafner et al. (2018).
Table 13: The hyper-parameter values used for Rrecurrent (used as R). The hyper-parameters for theplanner is the same as other models (Table 10).
Table 14: The downsized version of OT OR. We down-scaled the model by reducing the number ofConvLSTM filters, limiting the modeling capacity and thereby potentially increasing their predictionerror. The detailed architecture of the model and layers can be found in Finn et al. (2016a);Babaeizadeh et al. (2018).
Table 15: The downsized version of LT LR . We down-scaled the model by reducing the numberof units in fully connected paths, limiting the modeling capacity and thereby potentially increasingtheir prediction error. The detailed architecture of the model and layers can be found in Hafner et al.
