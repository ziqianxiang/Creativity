Table 1: We trained TIM on the DNS speech enhancement dataset and evaluate on the DNS test-set (left). Toassess zero-shot transfer generalization we evaluate it on the voicebank test-set as well (right). The performanceis reported on wideband PESQ (higher is better). TIM reaches state-of-the-art performance and shows improvedgeneralization capabilities in mismatch conditions. Results with (*) used additional outside datasets for training.
Table 2: We compare the baseline BERT models to TIM with and without competition, on both validationlikelihood (perplexity) and NLP fine-tuning tasks (reported as accuracy, with median and standard deviationover five fine-tuning trials with different seeds). We also show that it is essential to make the first and last layersnormal Transformer layers and not TIM layers (TIM-All-Layers).
