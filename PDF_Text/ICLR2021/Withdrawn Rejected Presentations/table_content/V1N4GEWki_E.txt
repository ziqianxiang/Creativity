Table 1: Results of Trained Sparse/Dense Models from Different Initializations. The initializations proposed in Eq. (1) (Ours) andLiu et al. (2019) improve generalization consistently over masked dense (Original) except for in ResNet50. Note that VGG16 trainedwithout a sparsity-aware initialization fails to converge in some instances. Baseline corresponds to the original dense architecture,whereas Small Dense corresponds to a smaller dense model with approximately the same parameter count as the sparse models.
Table 2: Ensemble & Prediction Disagreement. We compare the function similarity (Fort et al.,2020) with the original pruning solution and ensemble generalization over 5 sparse models, trainedfrom random initializations and LTs. As a baseline, we also show results for 5 pruned modelstrained from different random initializations. See Appendix F for the complete results.
Table 3: ยง4.3: Experiment Details/Hyperparameters. Initial Learning Rate (LR), LR Schedule (Sched.),Batchsize (Batch.), Momentum (m), Weight Decay (WD), tstart, tend and f are the pruning starting iteration,end iteration, and mask update frequency respectively.
Table 4: ยง4.1: Experiment Details/Hyperparameters. Initial Learning Rate (LR), LR Schedule(Sched.), Batchsize (Batch.), Momentum (m), Weight Decay (WD), Initial Drop Fraction (Drop.), tendand f are the pruning mask update frequency and end iteration respectively. LeNet5+ row correspondsthe LeNet5 experiments with our sparse initialization, whereas LeNet5 is the regular masked initialization.
Table 5: Results of Trained Fully-Connected MNIST Model from Different Initializations.
Table 6: EnsembleZPrediction Disagreement. In order to show the function similarity of LTs to the pruning solution, we follow the analysis of (Fort et al., 2020), andcompare the function similarity and ensemble generalization over 5 sparse models trained using random initializations and LTs with the original pruning solution theyare derived from. The fractional disagreement is the pairwise disagreement of class predictions over the test set, as compared within the group of sparse models, andas compared to the pruned model whose mask they were derived from. Kullback-Leibler Divergence (KL) and Jensen-Shannon Divergence (JSD) compare the predictiondistributions over all the test samples.
