Table 1: Dataset statistics of signed graphs. |V| and |E| are the number of nodes and edges, respectively. Given sign s ∈ {+, -}, |Es| and ρ(s) are the number and percentage of edges with sign s, respectively.						Dataset		|V|_	|E|	∣E+I	|E- |	ρ(+)	ρ(-)Bitcoin-Alpha1	3,783	24,186	22,650	1,536	93.65%	6.35%Bitcoin-OTC1	5,881	35,592	32,029	3,563	89.99%	10.01%Slashdot2	79,120	515,397	392,326	123,071	76.12%	23.88%Epinions3	131,828	841,372	717,667	123,705	85.30%	14.70%1	https://snap.stanford.edu/data/soc- sign- bitcoin- otc.html 2	http://konect.uni- koblenz.de/networks/slashdot- zoo 3	http://www.trustlet.org/wiki/Extended_Epinions_dataset						3.4 Loss Function for Link Sign PredictionThe link sign prediction is to predict the missing sign of a given edge. As shown in Figure 1(a),SGDNet produces the final node embeddings H(L) . The embeddings are fed into a loss functionL(G, H(L); Θ) = Lsign(G, H(L)) + λLreg (Θ) where Θ is the set of model parameters, Lsign(∙) is thebinary cross entropy loss, and Lreg(∙) is the L? regularization loss with weight decay λ. For a signededge (u → v, s), the edge feature is zuv ∈ R1×2dL= h(uL) ||h(vL) where h(uL) is the u-th row vector ofH(L). Let E be the set of signed edges. Then, Lsign(∙) is represented as follows:Lsign(G, X) = - X X	I(t = s) log (softmaxt (zuvW))(u→v,s)∈E t∈{+,-}where W ∈ R2dL ×2 is a learnable weight matrix, Softmaxt(∙) is the probability for sign t aftersoftmax operation, and I(∙) returns 1 if a given predicate is true, and 0 otherwise.
Table 2: SGDNet gives the best link sign prediction performance in terms of AUC. The best modelis in bold, and the second best model is underlined. The % increase measures the best accuracyagainst the second best accuracy.
Table 3: S GDNet gives the best link sign prediction performance in terms of F1-macro.				F1-macro	Bitcoin-Alpha	Bitcoin-OTC	Slashdot	EpinionsAPPNP (Klicpera et al., 2019) ResGCN (Li et al., 2019a)	0.682±0.005^^ 0.658±0.006	0.762±0.009 0.735±0.015	0.748±0.003 0.609±0.004	0.773±0.004 0.784±0.003SIDE (Kim et al., 2018) SLF (Xu et al., 2019b)	0.663±0.008 0.615±0.027	0.709±0.008 0.641±0.025	0.685±0.009 0.733±0.008	0.785±0.006 0.810±0.008SGCN (Derr et al., 2018b)	0.690±0.014	0.776±0.008	0.752±0.013	0.844±0.002SNEA (Li et al., 2020)	0.670±0.005	0.742±0.011	0.690±0.005	0.805±0.005SGDNet (proposed)	0.757±0.012^^	0.799±0.007	0.778±0.002	0.854±0.002% increase	74%	1.6%	3.5%	1.2%Hyperparameter Settings. We set the dimension of final node embeddings to 32 for all methodsso that their embeddings have the same learning capacity (see its effect in Appendix A.6). Weperform 5-fold cross-validation for each method to find the best hyperparameters and measure thetest accuracy with the selected ones. In the cross-validation for SGDNET, the number L of SGDlayers is sought between 1 and 6, and the restart probability c is selected from 0.05 to 0.95 by stepsize 0.1. We set the number K of diffusion steps to 10 and the feature dimension dl of each layer to32. We follow the range of each hyperparameter recommended in its corresponding paper for thecross-validation of other models. Our model is trained by the Adam optimizer (Kingma & Ba, 2015),where the learning rate is 0.01, the weight decay λ is 0.001, and the number of epochs is 100. Wesummarize the hyperparameters used by SGDNet for each dataset in Appendix A.7.
Table 4: We summarize the configurations of SGDNet’s hyperparameters, which are used in theexperiments of this paper.
