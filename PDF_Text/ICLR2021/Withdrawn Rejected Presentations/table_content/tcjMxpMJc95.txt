Table 1: Test accuracy of various KD methods on CIFAR-100. ‘WRN’ indicates a family ofWide-ResNet. All student models share the same teacher model as WRN-28-4. SKD (Standard KD)and FKD (Full KD) represent the KD method (Hinton et al., 2015) with different hyperparametervalues (α, τ) used in Eq. (1) - (0.1, 5) and (1.0, 20), respectively. MSE represents the KD with L2regression loss between logits; see Appendix A for citations of other methods. Others are resultsreported in Heo et al. (2019a). Baseline indicates the model trained without teacher model.
Table 2: Top1 test accuracies on CIFAR-100. WRN-28-4is used as a teacher for LKD and MSE when α=1.0 in L.
Table 3: Pearson correlation coefficients (PCC)between entropy and TLD for each training in-stances. All models are trained with LCE onCIFAR-100. The bold indicates p-value < 0.05.
Table 4: Top1 test accuracies of WRN-16-2 models trained with LCE and with MSE (teacher: WRN-28-4) on CIFAR-100. We evaluate the accuracies of distilled training data, undistilled training data,and test data. Sampling is based on the quantile of pdf of TLD from WRN-28-4 trained with LCE .
Table 5: Comparison of top1 accuracies on various data sets with different pairs of teacher andstudent models. ‘CE’ indicates the model trained with LCE. All teacher models are trained withthe dataset whose (δ, ζ) is equal to (1.0, 1.0). We report the best result over 3 individual runs withdifferent initializations.
Table 6: Comparison of top1 accuracies on various data sets with different pairs of teacher andstudent models. ’CE’ indicates the models trained with LCE. All teacher models are trained withthe dataset whose (δ, ζ) is equal to (1.0, 1.0). We report the best result over 3 individual runs withdifferent initializations.
Table 7: Comparison of BLEU scores on WMT14 En-De with different pairs of teacher and studentmodels. In this task, since the data has no clear categories like image data, we only conduct anexperiment with hyperparameter ζ. We use the same settings in Zhou et al. (2019).
Table 8: Training accuracy on CIFAR-100 (Teacher: WRN-28-4 & Student: WRN-16-2).
Table 9: Testing accuracy on CIFAR-100 (Teacher: WRN-28-4 & Student: WRN-16-2).
Table 10: Hyperparameters of AT, NAT models. We utilize the same notation from Vaswani etal. Vaswani et al. (2017). dmodel indicates the dimension of key, value and query dimension.
Table 11: Ablation performance on various regularizations on CIFAR-100 dataset. ‘RN’ indicatesthe a family of ResNet. All teacher models are trained with such regularizations on the dataset whose(δ, ζ) is equal to (1.0, 1.0).
Table 12: Top1 accuracy of HCL on CIFAR-100 dataset. We keep the student the same as WRN-16-2and use the same settings described in Nayak et al. (2019).
Table 13: ECE of the training samples. Here, (student, teacher) is (WRN-28-4, WRN-16-2), and all student models are trained with α = 1.0.
Table 14: Top1 training accuracy and the number of samples whose TLD value from student isabove the mean TLD of teacher (i.e., mean = 7.99), when the learning rate is annealed. Here,(teacher, student) is (WRN-28-4, WRN-16-2). α in L is set to 1.0.
Table 15: Comparison of top1 accuracies on CIFAR-100 with a family of ResNets. All teachermodels are trained with the dataset whose (δ, ζ) is equal to (1.0, 1.0). We report the best result over3 individual runs with different initializations.
