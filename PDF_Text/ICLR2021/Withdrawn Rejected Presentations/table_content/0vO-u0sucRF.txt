Table 1: Few-shot sinusoid regression re-sults. We report the results (mean values and95% confidence intervals after 10 repeats) ofall methods (GP, GP+MAML, MAML and S.
Table 2: Classification test accuracy on Omniglot, mini-Imagenet and Augmented Omniglot. For all methods,mean performances with 95% confidence intervals are reported after repeating the experiments 10 times andeach experiments performs meta-testing in 1000 tasks. Best performance is with bold, while * indicates statis-tically significant better performance than MAML in Augmented Omniglot. For Augmented Omniglot we runMAML (see Appendix for the hyperparameters) since this dataset was not included in Finn et al. (2017).
Table 3: Classification negative log likelihood (NLL) test performance on Omniglot, mini-Imagenet and Aug-mented Omniglot. To obtain these scores for MAML we re-run MAML since only classification accuracy isreported in Finn et al. (2017). Again * indicates statistically significant better performance than MAML.
Table 4:	Hyperparameters for Stochastic MAML.
Table 5:	Hyperparameters for MAML in sinusoid regression and Augmented Omniglot.
Table 6: Hyperparameters for the GP methods.
Table 7: Detailed few-shot sinusoid regression results. We report the results of the GP model for K = 5, 10, 20and for MAML assuming different gradient adaptation steps (including also a result reported in the originalMAML paper by Finn et al. (2017)).
Table 8: Classification test accuracy performance on Augmented Omniglot for improved architectures. TheGP based methods use 4 additional simple convolutional layers where each of these layers is added after eachnetwork layer, similar to Flennerhag et al. (2020). We also report results with and without batch normalization.
Table 9: Additional results on mini-ImageNet and tiered-ImageNet. We re-use LEO Rusu et al. (2019) embed-dings from their GitHub code and apply a 2-layers MLP with ReLU activations and 128 hidden dimensions toconstruct the feature vector.
Table 10: Hyperparameters for the additional results on mini-ImageNet and tiered-ImageNet. For dropout rate,we considered values: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7.
