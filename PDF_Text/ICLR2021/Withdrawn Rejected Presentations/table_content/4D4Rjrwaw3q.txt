Table 1: Properties of selected benchmark collections (details in appendix A). “+” means thatthe feature is present, “-” that the feature is missing, and an empty case that it is not applicable.
Table 2: Nevergrad maintains a dashboard (Rapin & Teytaud, 2020). For each experiment, thereare many configurations (budget, objective function, possibly dimension, and noise level). We sep-arate benchmarks used for designing ABBO, benchmarks used for validation, and those only usedfor testing. “*” denotes benchmarks used for designing Shiwa (which is used inside ABBO). Wepresent the rank based on the winning rate of ABBO in the dashboard. Since the submission of thispaper, several variants of bandit-based algorithms have been added for high-dimensional noisy opti-mization. They outperform ABBO, hence its poor rank for these cases. Detailed plots are availablein Appendix B. As expected, DE variants are strong on LSGO and CMA-ES variants are strong forYABBOB. ABBO also performs well on YABBOB, which was used for designing its ancestor Shiwa(see Fig. 1). For the MuJoCo testbed, details are available in Table 3 and Figure 3. Our modificationsin the codebase implies an improvement of Shiwa compared to the version published in (Liu et al.,2020); for example, our chaining implies that the (k + 1)th code starts from the best point obtainedby the kth algorithm, which significantly improves in particular the chaining CMA-ES+Powell orCMA-ES+SQP. Experiments with “*" in the ranking of ShiWa correspond to this improved versionof Shiwa.
Table 3: Results for a linear policy in the black-box setting from the latest black-box paper (Wanget al., 2020) and references therein, compared to results from ABBO. Two last columns = averagereward for the maximum budget tested in (Wang et al., 2020), namely 1k, 4k, 4k, 40k, 30k, 40k,respectively. “ioa” = iterations on average for reaching the target. “iter” = iterations for targetreached for median run. “*” refers to problems for which the target was not reached by Wanget al. (2020): then BR means “best result in 10 runs”. ABBO reaches the target for Humanoidand Ant whereas previous (black-box) papers did not; we get nearly the same ioa for Hopper andHalfCheetah (Nevergrad computed the expected value instead of computing the ioa, so we cannotcompare exactly; see Figure 3 for curves). ABBO is slower than LA-MCTS on Swimmer. Notethat we keep the same method for all benchmarks whereas LA-MCTS modified the algorithm for3 rows. On HDMULTIMODAL, ABBO performs better than LA-MCTS, as detailed in the text,and as confirmed in (Wang et al., 2020), which acknowledges the poor results of LA-MCTS forhigh-dimensional Ackley and Rosenbrock.
