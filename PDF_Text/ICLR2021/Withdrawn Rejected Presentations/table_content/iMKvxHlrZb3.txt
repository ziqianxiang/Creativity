Table 1: Number of parameters (equivalently,FLOPs per prediction) in vanilla SIGN andNARS. If features are concatenated in NARS,the number of parameters grows multiplica-tively, but this is resolved by using 1D convolu-tion to reduce input dimension to the classifier.
Table 2: Statistics of three academic graph datasets. The node types are: Paper (P), Author (A), Field (F),Institute (I) and Venue (V).
Table 3: Training hyper-parameters & number of parameters for each model. For hyper-parameters # hiddenand # layers, we adopt values from HAN for ACM, and values from HGT for OGB-MAG and OAG. # subgraphssampled for NARS is picked to be small while producing good and stable results. TransE size is selected basedon the size of the graph.
Table 4: Performance of NARS vs. baseline models on different datasets and tasks. All numbers are averageand standard deviation over 5 runs. Bold numbers indicate best model(s).
Table 5: Comparison of different ways to featurize nodes with no input features on the OGB-MAG dataset.
