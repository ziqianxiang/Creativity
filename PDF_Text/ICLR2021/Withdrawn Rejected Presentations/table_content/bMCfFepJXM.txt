Table 1: Results for OpenAI gym (Brockman et al., 2016) environments in the D4RL (Fu et al.,2020) datasets. For each task, we train for 1 million gradient steps and report the performance byrunning the policy obtained at the last epoch of the training for 100 episodes, averaged over 4 randomseeds with standard deviation. Each number is the normalized score as proposed in (Fu et al., 2020).
Table 2: Results for Adroit tasks with human demonstrations in the D4RL (Fu et al., 2020) datasets.
Table 4: Default hyper-parametersHyper-parameter	Value (Gym/Adroit)Optimizer	Adam (Kingma & Ba, 2015)Policy learning rate	5e-6/5e-8Q network learning rate	3e-4α learning rate	1e-5/1e-7batch size	100Target update rate τ	1e-3Discount factor γ	0.99Initial β	10β learning rate	1e-3Steps per epoch T	2000Number of epochs	50017Under review as a conference paper at ICLR 2021Table 5: Task-specific hyper-parameters for OpenAI gym tasks.
Table 5: Task-specific hyper-parameters for OpenAI gym tasks.
Table 6: Task-specific hyper-parameters for Adroit tasks with human demonstrations.
