Table 1: Summary of normalized scores. See Table 3 in Appendix D for full results.
Table 2: Atari DQN HyperparametersHyperparameter	Value	DescriptioncV-DQN	0.1	Weighting factor of σ-stream in exploration policy in V-DQNcTD-DQN	0.1	Weighting factor of σ-stream in exploration policy in TD-DQNmini-batch size	32	Size of mini-batch sample for gradient stepreplay buffer size	1M	Maximum number of transitions stored in the replay bufferinitial replay buffer size	50K	Number of transitions stored in the replay buffer be- fore optimization startsoptimization frequency	4	Number of actions the agent takes between successive network optimization stepsupdate frequency	30000	Number of steps between consecutive target updatesinit	1.00	Initial exploration rate of -greedy methodfinal	0.01	Final exploration rate of -greedy methodN	1M	Number of actions that the exploration rate of - greedy method decays from initial value to final valueα	0.0000625	Adam optimizer learning rateADAM	0.00015	Adam optimizer parameterevaluation frequency	250K	Number of actions between successive evaluation runsevaluation length	125K	Number of actions per evaluation runevaluation episode length	27K	Maximum number of action in an episode in evalua- tion runsmax no-op	30	Maximum number of no-op actions before theepisode starts14
Table 3: Normalized scores.
Table 4: Raw Scores.
