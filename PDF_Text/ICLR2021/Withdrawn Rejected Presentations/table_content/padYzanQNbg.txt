Table 1: Classification loss. (Bold indicates best performance.)	Neural SDE	CTFP	Latent ODEStocks	0.357 ± 0.045	0.165 ± 0.087	0.000239 ± 0.000086Weights	0.507 ± 0.019	0.676 ± 0.014	0.0112 ± 0.0025Beijing Air Quality	0.589 ± 0.051	0.764 ± 0.064	0.392 ± 0.0118Under review as a conference paper at ICLR 2021Prediction is a train on synthetic, test on real (TSTR) metric (Hyland et al., 2017). We train asequence-to-sequence model to predict the latter part of a time series given the first part. We use aneural CDE/ODE as an encoder/decoder pair. Smaller losses, meaning ability to predict, are better.
Table 2: Prediction loss. (Bold indicates best performance.)	Neural SDE	cTFP	Latent ODEStocks	0.144 ± 0.0446	0.725 ± 0.233	46.2 ± 12.3Weights	0.00843 ± 0.00759	0.0808 ± 0.0514	0.127 ± 0.152Beijing Air Quality	0.395 ± 0.056	0.810 ± 0.083	0.456 ± 0.095Maximum mean discrepancy is a distance between probability distributions with respect to a kernelor feature map. We use the depth-5 signature transform as the feature map (Kiraly & Oberhauser,2019; Toth & Oberhauser, 2020). Smaller values, meaning closer distributions, are better.
Table 3: MMD loss. (Bold indicates best performance.)	Neural SDE	cTFP	Latent ODEStocks	1.92 ± 0.09	2.70 ± 0.47	60.4 ± 35.8Weights	5.28 ± 1.27	12.0 ± 0.5	23.2 ± 11.8Beijing Air Quality	0.000160 ± 0.000029	0.00198 ± 0.00001	0.000242 ± 0.000002Neural SDEs produce substantially better results with respect to both the predictive (forecasting) andMMD metrics. On the stocks data, they additionally perform substantially better on the classificationmetric - stocks being a regime in which SDE models have classically been applied. We see that bothneural SDEs and CTFPs consistently outperform Latent ODEs, which we attribute to the nature ofthese datasets: on these problems we expect to see some random fluctuations, and the underlyingdynamics are not those of a pure drift.
