Table 1: Summary statistics of the benchmark datasets used in the experiment.							Dataset	Nodes	Edges	Features	Classes	Training	Validation	TestingCora	2708	5429	1433	7	140	500	1000Citeseer	3327	4732	3703	6	120	500	1000Pubmed	19717	44338	500	3	60	500	1000Theorem 4.2 shows that as long as the number of iterations is large enough, the embeddings of nodeswill be close to linear correlation, and the representation ability of IGNNS will decline. However, inthe framework of IGNNS, because the spectral radius ρ(A0) = ρ(A1) = 1, IFS is not contractivein general, and IGNNS still has the ability of depth feature representation.
Table 2: hyper-parameters in experiment.
Table 3: Summary of classification accuracy (%) results on Cora, Citeseer and Pubmed. The resultsare taken from the corresponding papers. The first value in brackets indicates the total training timein seconds and the second value in brackets indicates the average training time in seconds per epoch.
Table 4:	Statistical characteristics of the networks. Bold for minimum.
Table 5:	Performance of completely linear IGNNS on Cora, Citeseer and Pubmed.
