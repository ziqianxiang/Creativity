Table 1: Train and test performance for the Procgen benchmark (aggregated over all 16 tasks, 10seeds). (a) compares PPO with two baselines specifically designed to improve generalization inRL and shows that they do not significantly help. (b) compares using the best augmentation fromour set with and without regularization, corresponding to DrAC and RAD respectively, and showsthat regularization improves performance on both train and test. (c) compares different approachesfor automatically finding an augmentation for each task, namely using UCB or RL2 for selectingthe best transformation from a given set, or meta-learning the weights of a convolutional network(Meta-DrAC). (d) shows additional ablations: Rand-DrAC selects an augmentation using a uniformdistribution, Crop-DrAC uses image crops for all tasks, and UCB-RAD is an ablation that does notuse the regularization losses. UCB-DrAC performs best on both train and test, and achieves a returncomparable with or better than DrAC (which uses the best augmentation).
Table 2: JSD and Cycle-Consistency (%) (aggregated across all Procgen tasks) for PPO, RADand UCB-DrAC, measured between observations that vary only in their background themes (i.e.
Table 3: List of hyperparameters used to obtain the results in this paper.
Table 4: Best augmentation type for each game, as evaluated on the test environments.
Table 5: Best augmentation type for each game, as evaluated on the test environments.
Table 6: Procgen scores on train levels after training on 25M environment steps. The mean andstandard deviation are computed using 10 runs. The best augmentation for each game is used whencomputing the results for DrAC and RAD.
Table 7: Procgen scores on test levels after training on 25M environment steps. The mean andstandard deviation are computed using 10 runs. The best augmentation for each game is used whencomputing the results for DrAC and RAD.
