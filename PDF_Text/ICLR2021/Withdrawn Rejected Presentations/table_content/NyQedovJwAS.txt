Table 1: Anomaly detection performance (Average ROC AUC %)Dataset	Self-Supervised		Pretrained				OE	OC-SVM	DeePSVDD	MHRot	UnadaPted	PANDA	MHRot	PANDA-OECIFAR10	64.7	64.8	90.1 -	92.5	96.2	95.6	98.9-CIFAR100	62.6	67.0	80.1	94.1	94.1	-	97.3FMNIST	92.8	84.8	93.2	94.5	95.6	-	91.8CatsVsDogs	51.7	50.5	86.0	96.0	97.3	-	94.5DIOR	70.7	70.0	73.3	93.0	94.3	-	95.9Table 2: Pretrained feature performance on various small datasets (Average ROC AUC %)Dataset	Self-SuPervised		Pretrained		OC-SVM	DeepSVDD	MHRot	UnadaptedBirds 200	62.0	60.8	64.4	95.3Flowers	74.5	78.1	65.9	94.1MvTec	70.8	77.9	65.5	86.5WBC	75.4	71.2	57.7	87.4appendix Sec. C, and represenative frames are shown in Fig. 3. For outlier exposure (OE), we fol-lowed Hendrycks et al. (2018) and used 50k randomly sampled images from 80M Tiny Images.
Table 2: Pretrained feature performance on various small datasets (Average ROC AUC %)Dataset	Self-SuPervised		Pretrained		OC-SVM	DeepSVDD	MHRot	UnadaptedBirds 200	62.0	60.8	64.4	95.3Flowers	74.5	78.1	65.9	94.1MvTec	70.8	77.9	65.5	86.5WBC	75.4	71.2	57.7	87.4appendix Sec. C, and represenative frames are shown in Fig. 3. For outlier exposure (OE), we fol-lowed Hendrycks et al. (2018) and used 50k randomly sampled images from 80M Tiny Images.
Table 3: Comparison of average transformation prediction accuracy (%)Method	Normal			Anomalous			Horizontal	Vertical	Rotation	Horizontal	Vertical	RotationSelf-supervised	94.0	91.4	94.0	67.9	67.5	51.6Pretrained	94.4	94.4	92.3	71.4	69.9	61.3On the different supervision settings for one-class anomaly detection: Anomaly detection meth-ods employ different levels of supervision. Within the one-class classification task, one may useoutlier exposure (OE) - an external dataset (e.g. ImageNet), pretrained features, or no external su-pervision at all. The most extensive supervision is used by OE, which requires a large externaldataset at training time, and performs well only when such a dataset is from a similar domain to theanomalies (see Tab. 1). In cases where the dataset used for OE has significantly different properties,the network may not learn to distinguish between normal and anomalous data, as the normal andanomalous data may have more in common than the OE dataset. E.g. both normal and anomalousclasses of Fashion MNIST are greyscale, OE using 80M Tiny Images will not be helpful. Pretrainedfeatures further improve OE, in cases where is suitable e.g. CIFAR10.
Table 4: A comparison of different feature adaptation methods (Avg. ROC AUC %)Dataset	Baseline		PANDA		JO	Early stopping	SES	EWCCIFAR10	93.2	96.2	95.9	96.2CIFAR100	91.1	94.8	94.6	94.2FMNIST	94.9	95.4	95.5	95.6CatsVsDogs	96.1	91.9	95.7	96.4DIOR	93.1	95.4	95.6	95.5Table 5: Performance of finetuning different ResNet blocks (CIFAR10 w. EWC, ROC AUC %)Trained Blocks	1,2,3,4 2,3,4 3,4	4Avg	94.9	95.9	96.2 94.8SES does not collapse as badly on CatsVsDogs dataset. We note that weighting equally the changesin all parameters ( Pi(θi - θ*)2) achieves similar results to early stopping.
Table 5: Performance of finetuning different ResNet blocks (CIFAR10 w. EWC, ROC AUC %)Trained Blocks	1,2,3,4 2,3,4 3,4	4Avg	94.9	95.9	96.2 94.8SES does not collapse as badly on CatsVsDogs dataset. We note that weighting equally the changesin all parameters ( Pi(θi - θ*)2) achieves similar results to early stopping.
Table 6: Pretrained vs. Raw Initialization Anomaly Detection Performance (ROC AUC %)CIFAR10 class	0	1	2	3	4	5	6	7	8	9	AvgPretrained MHRot	70.1	93.7	84.4	76.1	89.7	87.3	91.1	94.4	86.8	90.8	86.4MHRot	77.5	96.9	87.3	80.9	92.7	90.2	90.9	96.5	95.2	93.3	90.1To score an anomaly, the image is deemed anomalous if its rotation prediction accuracy is worse thanthat of a typical normal image.
Table 7: Deep SVDD vs. PCA Whitening Anomaly Detection Performance (ROC AUC %)CIFAR10 class	0	1	2	3	4	5	6	7	8	9	AvgPCA whitening	62.0	63.6	49.7	59.9	59.8	65.8	68.3	68.0	75.5	71.2	64.8Deep SVDD	59.7	64.3	48.4	61.5	61.3	65.5	70.1	68.9	75.3	72.5	64.6Table 8: Details of Datasets Used for EvaluationDataset	No. of classes	No. of train images (Avg.)	No. of test imagesCIFAR10	10	5,000	10,000Fashion MNIST	10	6,000	10,000CIFAR100	20	2,500	10,000102 Category Flowers	102	10	7,169Caltech-UCSD Birds 200	200	30	5,794CatsVsDogs	2	10,000	5,000MVTec	15	242	1,725WBC	4	59	62DIOR	19	649	9,243C Detailed Description of DatasetsStandard datasets: We evaluate our method on a set of commonly used datasets: CIFAR10(Krizhevsky et al., 2009): Consists of RGB images of 10 object classes. Fashion MNIST (Xiaoet al., 2017): Consists of grayscale images of 10 fashion item classes. CIFAR100 (Krizhevsky et al.,2009): We use the coarse-grained version that consists of 20 classes. DogsVsCats: High resolution
Table 8: Details of Datasets Used for EvaluationDataset	No. of classes	No. of train images (Avg.)	No. of test imagesCIFAR10	10	5,000	10,000Fashion MNIST	10	6,000	10,000CIFAR100	20	2,500	10,000102 Category Flowers	102	10	7,169Caltech-UCSD Birds 200	200	30	5,794CatsVsDogs	2	10,000	5,000MVTec	15	242	1,725WBC	4	59	62DIOR	19	649	9,243C Detailed Description of DatasetsStandard datasets: We evaluate our method on a set of commonly used datasets: CIFAR10(Krizhevsky et al., 2009): Consists of RGB images of 10 object classes. Fashion MNIST (Xiaoet al., 2017): Consists of grayscale images of 10 fashion item classes. CIFAR100 (Krizhevsky et al.,2009): We use the coarse-grained version that consists of 20 classes. DogsVsCats: High resolutioncolor images of two classes: cats and dogs. The data were extracted from the ASIRRA datasetElsonet al. (2007), we split each class to the first 10,000 images as train and the last 2,500 as test.
