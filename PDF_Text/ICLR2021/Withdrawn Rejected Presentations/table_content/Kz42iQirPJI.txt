Table 1: Compare to BaselinesAlgorithm	5 Way 5 Shots		Algorithm	5 Way 5 Shots		ACC	BWT		ACC	BWTProtonet-Sequential	54.74 ± 0.11	-24.68 ± 0.15	ANIL-SeqUentiaI	50.68 ± 0.25	-27.54 ± 0.34Protonet-EWC	59.38 ± 0.42	-11.15 ± 0.51	ANIL-EWC	48.18 ± 0.24	-30.24 ± 0.26Protonet-HAT	62.58 ± 0.24	-8.96 ± 0.30	ANIL-HAT	47.02 ± 0.26	-30.73 ± 0.32Protonet-UCB	57.82 ± 0.05	-13.81 ± 0.09	ANIL-UCB	51.58 ± 0.17	-25.96 ± 0.23Protonet-A-GEM	57.38 ± 0.32	-22.09 ± 0.26	ANIL-A-GEM	51.56 ± 0.38	-26.45 ± 0.40Protonet-ER-Ringbuffer	58.25 ± 0.31	-19.06 ± 0.36	ANIL-ER-Ringbuffer	45.14 ± 0.25	-35.29 ± 0.28Protonet-MER	60.79 ± 0.16	-11.96 ± 0.15	ANIL-MER	51.50 ± 0.23	-25.67 ± 0.26Protonet-Ours	68.72 ± 0.22	-3.22 ± 0.17	ANIL-Ours	56.62 ± 0.32	-15.28 ± 0.40Joint-training	71.81 ± 0.29	N/A	Joint-training	73.52 ± 0.20	N/Aresults of 5-way-1-shot setting. It can be seen that although there are some performance differencescompared to those of the previous order, our method still outperforms the baselines.
Table 2: Compare to Baselines with different domain orderingAlgorithm	5 Way 5 Shots		Algorithm	5 Way 5 Shots		ACC	BWT		ACC	BWTProtonet-Sequential	51.28 ± 0.31	-28.79 ± 0.39	ANIL-Sequential	44.18 ± 0.39	-33.79 ± 0.35Protonet-EWC	59.12 ± 0.35	-14.87 ± 0.27	ANIL-EWC	46.27 ± 0.37	-32.24 ± 0.40Protonet-HAT	60.61 ± 0.28	-10.89 ± 0.15	ANIL-HAT	42.52 ± 0.15	-34.79 ± 0.31Protonet-UCB	50.36 ± 0.18	-20.85 ± 0.23	ANIL-UCB	45.32 ± 0.25	-31.28 ± 0.36Protonet-A-GEM	59.75 ± 0.28	-17.85 ± 0.35	ANIL-A-GEM	45.87 ± 0.32	-32.01 ± 0.41Protonet-ER-Ringbuffer	61.52 ± 0.30	-14.21 ± 0.40	ANIL-ER-Ringbuffer	41.35 ± 0.30	-37.82 ± 0.37Protonet-MER	62.71 ± 0.18	-11.23 ± 0.10	ANIL-MER	47.19 ± 0.23	-28.45 ± 0.29Protonet-Ours	67.29 ± 0.32	-4.12 ± 0.30	ANIL-Our	55.89 ± 0.22	-12.36 ± 0.31Joint-training	71.81 ± 0.29	N/A	Joint-training	73.52 ± 0.20	N/AWe use another different domain-sequence order as: Omniglot, Aircraft, CUB, CIFARFS and Mini-Imagenet. Since Omniglot is used as first domain, the model cannot learn good representations forthe following domains, thus poses more challenges than the other two domain sequences. Table 3shows the results, where the baseline Protonet-fixfirst is the method that freeze the model parametersafter finishing training on the first domain. For this baseline, there is small BWT, this is becauseof random testing task variance at different time. In this case, all the baselines and our methodsperformance drop significantly compared to the sequence with CIFARFS or Miniimagenet as firstdomain. It indicates that this domain sequence is significantly more challenging than the other two
Table 3: Compare to Baselines with Omniglot as first training domainAlgorithm	5 Way 1 Shot		5 Way 5 Shots		ACC	BWT	ACC	BWTProtonet-Sequential	42.69 ± 0.26	-22.18 ± 0.32	57.92 ± 0.21	-18.32 ± 0.19Protonet-EWC	37.98 ± 0.19	-19.43 ± 0.22	54.02 ± 0.18	-13.24 ± 0.23Protonet-HAT	41.02 ± 0.12	-18.55 ± 0.17	58.35 ± 0.21	-16.14 ± 0.15Protonet-UCB	41.32 ± 0.20	-18.01 ± 0.32	58.73 ± 0.28	-17.28 ± 0.21Protonet-A-GEM	43.18 ± 0.27	-21.10 ± 0.21	59.96 ± 0.16	-14.86 ± 0.18Protonet-ER-Ringbuffer	41.92 ± 0.28	-22.54 ± 0.29	59.35 ± 0.23	-16.96 ± 0.20Protonet-MER	39.60 ± 0.28	-24.21 ± 0.31	58.10 ± 0.22	-16.26 ± 0.17Protonet-fixfirst	44.29 ± 0.19	-0.16 ± 0.14	56.29 ± 0.29	-0.22 ± 0.12Protonet-Ours	46.56 ± 0.18	-14.34 ± 0.25	62.68 ± 0.15	-11.12 ± 0.25Joint-training	53.92 ± 0.36	N/A	71.81 ± 0.29	N/A8Under review as a conference paper at ICLR 2021Table 4: Effect of domain-level meta loss5 Way 1 Shot	5 Way 5 ShotsAlgorithm	ACC	BWT	ACC	BWTProtonet-intra and inter meta loss	53.36 ± 0.27	-3.39 ± 0.15	68.72 ± 0.22	-3.22 ± 0.17Protonet-intra loss	52.29 ± 0.31	-4.02 ± 0.18	67.69 ± 0.21	-3.97 ± 0.23
Table 4: Effect of domain-level meta loss5 Way 1 Shot	5 Way 5 ShotsAlgorithm	ACC	BWT	ACC	BWTProtonet-intra and inter meta loss	53.36 ± 0.27	-3.39 ± 0.15	68.72 ± 0.22	-3.22 ± 0.17Protonet-intra loss	52.29 ± 0.31	-4.02 ± 0.18	67.69 ± 0.21	-3.97 ± 0.23ANIL-intra and inter meta loss	45.85 ± 0.22	-10.19 ± 0.27	56.62 ± 0.32	-15.28 ± 0.40ANIL-intra loss	44.16 ± 0.30	-11.38 ± 0.20	55.10 ± 0.19	-16.97 ± 0.33Table 5: Compare to our method with different number of learning blocksAlgorithm	5 Way 1 Shot		5 Way 5 Shots		ACC	BWT	ACC	BWTProtonet-Ours-2block	50.12 ± 0.19	-7.15 ± 0.12	65.25 ± 0.23	-6.64 ± 0.29Protonet-Ours-4block	53.36 ± 0.27	-3.39 ± 0.15	68.72 ± 0.22	-3.22 ± 0.17Protonet-Ours-8block	53.51 ± 0.33	-3.21 ± 0.55	68.82 ± 0.33	-3.01 ± 0.21Table 6: Effect of number of memory tasks. Top, memory with one task for each domain; Bottom,memory with six tasks for each domainAlgorithm	5 Way 1 Shot		5 Way 5 Shots		ACC	BWT	ACC	BWTProtonet-A-GEM	38.50 ± 0.09	-29.37 ± 0.17	55.22 ± 0.11	-24.01 ± 0.12Protonet-ER-Ringbuffer	39.41 ± 0.16	-27.41 ± 0.26	56.17 ± 0.28	-23.46 ± 0.25Protonet-MER	39.04 ± 0.19	-25.73 ± 0.25	58.37 ± 0.24	-17.52 ± 0.31
Table 5: Compare to our method with different number of learning blocksAlgorithm	5 Way 1 Shot		5 Way 5 Shots		ACC	BWT	ACC	BWTProtonet-Ours-2block	50.12 ± 0.19	-7.15 ± 0.12	65.25 ± 0.23	-6.64 ± 0.29Protonet-Ours-4block	53.36 ± 0.27	-3.39 ± 0.15	68.72 ± 0.22	-3.22 ± 0.17Protonet-Ours-8block	53.51 ± 0.33	-3.21 ± 0.55	68.82 ± 0.33	-3.01 ± 0.21Table 6: Effect of number of memory tasks. Top, memory with one task for each domain; Bottom,memory with six tasks for each domainAlgorithm	5 Way 1 Shot		5 Way 5 Shots		ACC	BWT	ACC	BWTProtonet-A-GEM	38.50 ± 0.09	-29.37 ± 0.17	55.22 ± 0.11	-24.01 ± 0.12Protonet-ER-Ringbuffer	39.41 ± 0.16	-27.41 ± 0.26	56.17 ± 0.28	-23.46 ± 0.25Protonet-MER	39.04 ± 0.19	-25.73 ± 0.25	58.37 ± 0.24	-17.52 ± 0.31Protonet-Ours	50.73 ± 0.17	-5.56 ± 0.32	66.92 ± 0.29	-4.96 ± 0.30Protonet-A-GEM	40.07 ± 0.40	-26.47 ± 0.46	57.38 ± 0.32	-22.09 ± 0.26Protonet-ER-Ringbuffer	44.12 ± 0.19	-21.83 ± 0.17	58.25 ± 0.31	-19.06 ± 0.36Protonet-MER	46.72 ± 0.25	-14.82 ± 0.11	60.79 ± 0.16	-11.96 ± 0.15Protonet-Ours	53.36 ± 0.27	-3.39 ± 0.15	68.72 ± 0.22	-3.22 ± 0.17Effect of number of learning block Our method assigns a different learning rate for each block ofthe network parameters. A finer grained division of learning blocks allows more flexible control, but
Table 6: Effect of number of memory tasks. Top, memory with one task for each domain; Bottom,memory with six tasks for each domainAlgorithm	5 Way 1 Shot		5 Way 5 Shots		ACC	BWT	ACC	BWTProtonet-A-GEM	38.50 ± 0.09	-29.37 ± 0.17	55.22 ± 0.11	-24.01 ± 0.12Protonet-ER-Ringbuffer	39.41 ± 0.16	-27.41 ± 0.26	56.17 ± 0.28	-23.46 ± 0.25Protonet-MER	39.04 ± 0.19	-25.73 ± 0.25	58.37 ± 0.24	-17.52 ± 0.31Protonet-Ours	50.73 ± 0.17	-5.56 ± 0.32	66.92 ± 0.29	-4.96 ± 0.30Protonet-A-GEM	40.07 ± 0.40	-26.47 ± 0.46	57.38 ± 0.32	-22.09 ± 0.26Protonet-ER-Ringbuffer	44.12 ± 0.19	-21.83 ± 0.17	58.25 ± 0.31	-19.06 ± 0.36Protonet-MER	46.72 ± 0.25	-14.82 ± 0.11	60.79 ± 0.16	-11.96 ± 0.15Protonet-Ours	53.36 ± 0.27	-3.39 ± 0.15	68.72 ± 0.22	-3.22 ± 0.17Effect of number of learning block Our method assigns a different learning rate for each block ofthe network parameters. A finer grained division of learning blocks allows more flexible control, butleads to higher computation cost. In this experiment, we investigate the impact of blocks numbersto model performance. Specifically, we implement each layer with 2, 4 and 8 blocks. Results areshown in Table 5. It can be seen that when the block number is relatively large (4 in our case), theaccuracies are relatively stable. Thus we can simply set the block number to 4 in practice.
Table 7: Compare to Baselines (5-way 1-shot)					Algorithm	5 Way 1 Shot		Algorithm	5 Way 1 Shot		ACC	BWT		ACC	BWTProtonet-Sequential	36.31 ± 0.18	-30.97 ± 0.12	ANIL-Sequential	37.55 ± 0.21	-25.12 ± 0.26Protonet-EWC	42.21 ± 0.21	-19.06 ± 0.36	ANIL-EWC	38.86 ± 0.22	-24.15 ± 0.31Protonet-HAT	49.98 ± 0.19	-6.67 ± 0.24	ANIL-HAT	39.90 ± 0.11	-20.02 ± 0.15Protonet-UCB	41.26 ± 0.17	-9.46 ± 0.07	ANIL-UCB	38.86 ± 0.23	-21.35 ± 0.29Protonet-A-GEM	40.07 ± 0.40	-26.47 ± 0.46	ANIL-A-GEM	34.62 ± 0.36	-29.04 ± 0.42Protonet-ER-Ringbuffer	44.12 ± 0.19	-21.83 ± 0.17	ANIL-ER-Ringbuffer	31.49 ± 0.47	-32.45 ± 0.56Protonet-MER	46.72 ± 0.25	-14.82 ± 0.11	ANIL-MER	41.40 ± 0.19	-19.24 ± 0.21Protonet-Ours	53.36 ± 0.27	-3.39 ± 0.15	ANIL-Ours	45.85 ± 0.22	-10.19 ± 0.27Joint-training	53.92 ± 0.36	N/A	Joint-training	57.18 ± 0.38	N/A5-way 1-shot learning We study the sensitivity of all methods on the order of domain arrivalsin the setting of 5-way 1-shot learning. We use a different domain-sequence order as: CIFARFS,MiniImagenet, Aircraft, CUB and Omniglot. Results are summarized in Table 8. Similar to theresults in Table 2 , it can be seen that although there are some performance differences compared tothose of the previous order, our method consistently outperforms the baselines.
Table 8: Compare to Baselines with different domain ordering (5-way 1-shot)Algorithm	5 Way 1 Shot			5 Way 1 Shot		ACC	BWT	Algorithm	ACC	BWTProtonet-Sequential	42.66 ± 0.14	-23.04 ± 0.12	ANIL-Sequential	39.61 ± 0.20	-20.95 ± 0.22Protonet-EWC	43.97 ± 0.20	-17.36 ± 0.33	ANIL-EWC	39.18 ± 0.29	-22.12 ± 0.35Protonet-HAT	48.92 ± 0.41	-9.18 ± 0.26	ANIL-HAT	39.02 ± 0.12	-22.62 ± 0.10Protonet-UCB	39.64 ± 0.14	-10.83 ± 0.09	ANIL-UCB	39.97 ± 0.37	-19.28 ± 0.41Protonet-A-GEM	46.81 ± 0.23	-18.78 ± 0.35	ANIL-A-GEM	37.76 ± 0.35	-24.91 ± 0.27Protonet-ER-Ringbuffer	48.60 ± 0.35	-14.85 ± 0.18	ANIL-ER-Ringbuffer	35.42 ± 0.31	-26.03 ± 0.37Protonet-MER	49.92 ± 0.33	-9.06 ± 0.32	ANIL-MER	40.86 ± 0.37	-18.34 ± 0.31Protonet-Ours	52.17 ± 0.12	-0.74 ± 0.19	ANIL-Ours	45.25 ± 0.20	-6.02 ± 0.28Joint-training	53.92 ± 0.36	N/A	Joint-training	57.18 ± 0.38	N/AMeta testing of current and all previous domains Algorithm 2 shows the algorithm of metatesting the model on current and all the previous domains.
