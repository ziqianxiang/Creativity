Table 1: Atari Suite comparisons. @N represents the amount of RL interaction with reward utilized,with four frames observed at each iteration. Mdn and M are median and mean human normalizedscores, respectively; > 0 is the number of games with better than random performance; and > H is thenumber of games with human-level performance as defined in Mnih et al. (2015). Top: unsupervisedlearning only. Mid: data-limited RL. Bottom: RL with unsupervised pre-training.
Table 2: Atari Suite comparisons for R2D2-based agents. @N represents the amount of RL interactionwith reward utilized, with four frames observed at each iteration. Mdn, M and CM are median, meanand mean capped human normalized scores, respectively.
Table 3: Hyperparameter values used in R2D2-based agents. The rest of hyperparameters use thevalues reported by Kapturowski et al. (2019).
Table 4: Atari Suite comparisons, adapted from Hansen et al. (2020). @N represents the amountof RL interaction with reward utilized, with four frames observed at each iteration. Mdn and Mare median and mean human normalized scores, respectively; > 0 is the number of games withbetter than random performance; and > H is the number of games with human-level performance asdefined in Mnih et al. (2015). Top: unsupervised learning only. Mid: data-limited RL. Bottom: RLwith unsupervised pre-training.
Table 5: Atari Suite comparisons for R2D2-based agents. @N represents the amount of RL interactionwith reward utilized, with four frames observed at each iteration. Mdn, M and CM are median, meanand mean capped human normalized scores, respectively.
Table 6: Results per game at 5B training frames.
Table 7: Final scores per game in our ablation study after 5B frames. We consider versions of CPT where the pre-trained policy is used for exploitation, exploration,or both.___________________________________________________________________________________________________________________________________________________________________Game	-greedy explore	z-greedy explore	CPT (exploration only)	CPT (exploitation only)	CPTasterix	347585.48 ± 244661.28	414183.29 ± 160442.25	717259.64 ± 94011.25	577034.99 ± 90735.79	649740.02 ± 56368.04bank heist	987.94 ± 163.14	1050.67 ± 143.36	13881.28 ± 2693.54	11417.81 ± 7006.75	11894.55 ± 984.46frostbite	9312.19 ± 286.04	8228.56 ± 1182.64	9132.02 ± 372.76	10650.34 ± 3690.80	17895.70 ± 2683.33gravitar	6169.63 ± 83.68	5781.13 ± 757.18	6473.56 ± 132.16	7231.54 ± 1994.41	8114.29 ± 1027.20jamesbond	5771.41 ± 2084.84	3633.35 ± 272.19	1713.43 ± 477.47	3898.66 ± 1044.03	1567.71 ± 472.73montezuma revenge	1483.33 ± 1118.65	1465.77 ± 1102.87	11216.13 ± 837.50	6433.33 ± 372.68	13429.28 ± 413.32ms pacman	11406.77 ± 121.72	8099.46 ± 868.26	10622.99 ± 504.86	10611.56 ± 821.30	10983.94 ± 665.04pong	20.95 ± 0.02	20.49 ± 0.06	20.88 ± 0.03	20.94 ± 0.05	20.89 ± 0.07private eye	23589.23 ± 11880.08	50907.99 ± 15174.57	40483.83 ± 18.21	37028.70 ± 2449.66	40437.40 ± 47.86space invaders	3617.12 ± 13.38	3559.49 ± 27.29	31513.65 ± 1093.84	3601.02 ± 38.46	27853.60 ± 5839.58tennis	8.02 ± 22.58	7.95 ± 22.55	7.95 ± 22.57	-7.62 ± 22.36	23.96 ± 0.03up n down	527933.55 ± 23035.67	383415.21 ± 79529.79	562248.72 ± 11617.29	549769.30 ± 3015.66	568148.23 ± 2861.37Under review as a conference paper at ICLR 2021Under review as a conference paper at ICLR 2021Table 8: Final scores per task in Atari games with modified reward functions. We report trainingresults for the standard game reward, a variant with sparse rewards (easy), and a task with deceptiverewards (hard). Despite the pre-trained policy might obtain low or even negative scores in some of
Table 8: Final scores per task in Atari games with modified reward functions. We report trainingresults for the standard game reward, a variant with sparse rewards (easy), and a task with deceptiverewards (hard). Despite the pre-trained policy might obtain low or even negative scores in some ofthe tasks, committing to its exploratory behavior eventually lets the agent discover strategies that leadto high returns.
