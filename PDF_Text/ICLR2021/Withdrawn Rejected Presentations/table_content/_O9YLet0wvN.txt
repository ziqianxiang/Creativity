Table 1: Dataset comparison. Thr. = Throughoutly annotated: every instance of every class isannotated in every image. *LVIS has potentially more objects and categories per image than areannotated due to the non-exhaustive labeling.
Table 2: On COCO and Pascal VOC there is a clear performance gap (AP50) between categoriesused during training (Train Cats.) and held-out categories (Held-Out Cats.). A baseline getting ablack image as reference which contains no information about the target category (- empty Refs.)performs surprisingly well on Pascal VOC but fails on COCO.
Table 3: Effect of a three times longer training schedule and a larger backbone (ResNeXt-101 32x4d)on model performance across datasets. While larger models and longer training times lead to no oronly minor improvements on held-out categories on COCO, they do have a larger effect on LVISand Objects365.
Table 4: Performance (AP50) on COCO can be improved by training on LVIS. Siamese Mask R-CNN and Siamese Cascade R-CNN are identical to Siamese Faster R-CNN except for an additionalmask head or cascaded bbox heads. (*Michaelis et al. (2018b), f Hsieh et al. (2019))5	Discussion5.1	Future datasets should focus on the diversity of categories.
Table 5: Performance (AP50) on GroZi-3.2k13Under review as a conference paper at ICLR 2021at first seem to contradict our previous findings. They however make sense when compared to thesituation on COCO: There training on the more diverse LVIS dataset leads to better results and asmaller generalization gap. We here have a very similar scenario where we pre-train on much largerand more diverse datasets and consequently find better generalization capabilities. This is also verysimilar to the findings of Kolesnikov et al. (2019) who achieve good results for few-shot learning onImageNet by training on a significantly larger dataset (Section 5.2).
