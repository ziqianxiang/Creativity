Table 1: Summary of test problems used in our experiments. The exact model configurations can befound in the work of Schneider et al. (2019).
Table 2:	List of optimizers we considered for our benchmark. Note, that this is still far from beinga complete list of all existing optimization methods applicable to deep learning, but only a subset,comprising of some of the most popular choices.
Table 3:	Overview of commonly used parameter schedules. Note, while we list the schedulesparameters, it isn’t clearly defined what aspects of a schedule are (tunable) parameters and what isa-priori fixed. In this column, α0 denotes the initial learning rate, αlo and αup the lower and upperbound, ∆t indicates an epoch count at which to switch decay styles, k denotes a decaying factor.
Table 4: Selected optimizers for our benchmarking process with their respective color, hyperparame-ters, default values, tuning distributions and scheduled hyperparameters. Here, LU(∙, ∙) denotes thelog-uniform distribution while U{∙, ∙} denotes the discrete uniform distribution.
Table 5: Tabular version of Figure 4. Mean test set performance and standard deviation over 10random seeds of all tested optimizers on all eight optimization problems using the large budgetfor tuning and no learning rate schedule. For comprehensability, mean and standard deviation arerounded.
