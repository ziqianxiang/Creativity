Table 1: Single-dataset open-set recognition (Setup-I) AUROC↑. Error bars are shown in gray rows. Boldednumbers mark the top-5 ranked methods on each dataset. Because GDM does not L2-normalize features, weadd a variant that does (denoted GDML2). L2-normalization clearly improves performance, particularly onCIFAR. Interestingly, GDML2 underperforms NCM, implying that a full-covariance Gaussian (used by GDM)overfits compared to an identity covariance. GMM makes use of low-dimensional covariances (learned viaPCA) and strikes a balance between flexibility and generalization, achieving comparable performance to manysophisticated approaches.
Table 2: Cross-dataset open-set image recognition (Setup-II) AUROC↑. In this setup, we train on Tiny-ImageNet, validate using outlier images from a second dataset, and test using open-set images from a thirddataset. For each open-set dataset, we compute the average AUROC over all results when using differentoutlier datasets. We study two Res50 models either trained from scratch (pink row), or fine-tuned from anImageNet-Pretrained model (blue row). Clearly, simple statistical models can handily outperform much priorwork. Pre-training boosts open-set recognition performance for all methods (see last row pair). Binary classifiersCLS2 do not generalize well, presumably due to overfitting. Somewhat surprisingly, OpenMax works quitepoorly. We conjecture that the regularized logit features on which it is based may too invariant to be effective forcross-dataset open-set recognition. Table 4 and 5 supplement this table with more details.
Table 3: Open-set semantic segmentation (Setup-III) AUROC↑. Simple statistical methods (GMMs) out-perform prior methods, with the notable exception of discriminative classifiers (CLS2 and CLS(K+1)) that haveaccess to open-set training examples. Figure 5 analyzes this further, demonstrating that GMMs can outperformsuch discriminative models when they have access to less open training examples, suggesting that GMMs maybetter generalize to never-before-seen open-world scenarios.
Table 4: Cross-dataset evaluation (Setup-II) with a K-way classification network that is trained-from-scratch. We report performance with the AUROC metric. This table supplements Table 2. Recall that we trainon TinyImageNet as the closed-set, use another dataset as outlier set to tune and select model, and report on thethird dataset as the open-set. All the methods operate on off-the-shelf features extracted from the underlyingclassification network. We report their averaged performance and standard deviation in the last two columns.
Table 5: Cross-dataset evaluation (Setup-II) with a K-way classification network that is finetuned froman ImageNet pre-trained model. We report performance with the AUROC metric. This table supplementsTable 2. Recall that we train on TinyImageNet as the closed-set, use another dataset as outlier set to tuneand select model, and report on the third dataset as the open-set. All the methods operate on off-the-shelffeatures extracted from the underlying classification network. We report their averaged performance and standarddeviation in the last two columns.
