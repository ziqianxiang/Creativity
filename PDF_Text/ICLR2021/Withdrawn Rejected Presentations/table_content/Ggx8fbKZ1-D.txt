Table 1: Hyperparameter Settings for ExperimentsArchitecture	Dataset	Batch size	lr (SGD/SGDN)	lr (Adam)	Hyper-grad lr (SGD/SGDN)	Hyper-grad lr (Adam)	CAM-HD-lrMLP 1		32	-	0.0003	-	1.00E-07	0.01MLP 2	MNIST	64	-	0.001	-	1.00E-07	0.01MLP 3		128	-	0.001	-	1.00E-07	0.01	MNIST	256	-	0.001	1.00E-03	1.00E-08	0.03LeNet-5	CIFAR10	256	-	0.001	1.00E-03	1.00E-08	0.03	SVHN	128	-	0.001	1.00E-03	1.00E-08	0.03ResNet-18		256	0.1	0.001	1.00E-06	1.00E-08	0.001ResNet-34	CIFAR10	256	0.1	0.001	1.00E-06	1.00E-08	0.0013.2	Combination Ratio and Model PerformancesFirst, we perform a study on the combination of different level learning rates. The simulations arebased on image classification tasks on MNIST and CIFAR10 (LeCun et al., 1998; Krizhevsky andHinton, 2012). One feed-forward neural network with three hidden layers of size [100, 100, 100] andtwo convolutional network models, including LeNet-5 (LeCun et al., 2015) and ResNet-18 (He et al.,2016), are implemented. We use full training sets of MNIST and CIFAR10 for training and full testsets for validation. In each case, two levels of learning rates are considered, which are the global andlayer-wise adaptation for FFNN, and global and filter-wise adaptation for CNNs. Adam-CAM-HDoptimizer is implemented in all three simulations. We change the combination weights of two levelsin each case to see the change of model performance in terms of test classification accuracy at epoch
Table 2: Summary of test performances with LeNet-5MNIST	CIFAR10	SVHN	Test acc	Test S.E	Test acc	Test S.E	Test acc	Test S.EAdam-CAM-HD	98.93	0.07	65.55	0.18	87.58	0.37Adam-HD	98.83	0.05	63.3	0.66	86.94	0.13Adam-L4	99.19	0.05	63.76	0.26	85.44	0.42Adabound	99.11	0.05	59.79	0.70	87.22	0.14RAdam	98.94	0.06	61.17	0.79	87.31	0.41Adam	98.89	0.05	63.88	0.45	86.82	0.163.5 ResNet for Image ClassificationIn the third experiment, we apply ResNets for image classification task on CIFAR10. We compareAdam and its adaptive optimizers, as well as SGD with Nestorov momentum (SGDN) and corre-sponding adaptive optimizers for training both ResNet-18 and ResNet-34. For SGDN methods, weapply a learning rate schedule, in which the learning rate is initialized to a default value of 0.1 andreduced to 0.01 or 10% (for SGDN-CAM-HD) after epoch 150. The momentum is set to be 0.9 forall SGDN methods. For Adam-CAM-HD SGDN-CAM-HD, we apply two-level CAM-HD with thesame setting as the second experiment. In addition, we apply an exponential decay function with adecay rate r = 0.001. The validation accuracy results, training loss, and validation loss are shownin Figure 4. We can see that the validation accuracy of Adam-CAM-HD reaches about 90% in 40epochs and consistently outperforms Adam, L4 and Adam-HD optimizers in a later stage. The L4
