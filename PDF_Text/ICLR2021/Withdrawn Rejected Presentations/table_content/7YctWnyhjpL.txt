Table 1: Main Hyperparametersdataset	epochs	l.r.	b.sM-MNIST	100	1e-3	512CLEVR	100	1e-4	128CELEB	50	1e-3	512CUB	200	1e-4	128We trained the full model end-to-end, using cross entropy loss at the end of BU2. In some of theexperiments of CLEVR and CUB-200 datasets we added an auxiliary loss at the end of the TDstream. The target in this case is a 224x224 mask, where a single pixel, blurred by a Gaussian kernel(s.d. 3 pixels) was labeled as the target location. Training one task at a time, we minimized thecross-entropy loss over the 224x224 image at the end of the TD softmax output (which encourages asmall detected area), for each visible ground-truth annotated object or part. This auxiliary loss, allowsus, at inference, to create task-dependent spatial maps of detected objects; examples of interest areshown in figure 4 and in the Supplementary material. For a fair comparison, we also trained anotherversion of the channel modulation architecture with an additional regression loss, calculated by a FClayer at the top of the network, using the same ground truth annotations.
Table 2: Mean of 5 repetitions on M-MNIST. Our architecture consistently achieves highest accuracies.
Table 3: Performance on CLEVR, CELEB-A and CUB200. Our approach yields better accuracy than alternatives(a) CLEVR(b) CELEB-A	(c) CUB-200CLEVR	loc	#P	Av. Acc. ± std	CELEB-A	#p	Av. Acc. ± std	CUB200	loc	Av. Acc. ± stdSingle task	×	x40	×	Single task	x40	×	Single task	×	74.34 ±0.07Uni-sc	×	x21.03	67.66 ±0.81	Uni-SC	x32.64	90.36 ±0.03	Uni-sc	×	77.49 ±0.05Uni-sc	√	x21.04	73.56 ±0.31	ch-mod	x1.013	90.06 ±0.02	ch-mod	×	79.87 ±0.14ch-mod	×	x1.002	87.05 ±0.59	task-routing	x1.013	89.80 ±0.04	ch-mod	√	79.91 ±0.18ch-mod	√	x1.003	89.87 ±0.40	ControlNet	x1.15	90.46 ±0.03	task-routing	√	80.04 ±0.22task-routing	√	x1.003	69.76 ±0.21				ControlNet	√	80.89 ±0.09ControlNet	√	x1.56	96.83 ±0.08						6Under review as a conference paper at ICLR 20214.5	Experiments discussionWe discuss below additional aspects of the experiments and generalconclusions.
Table 4: Adding task (Acc%)	before	afterexisting tasks	74.89	74.89added task	16.30	64.68the digit ‘9’ (9 tasks in all). We then extended the embedding layer and trained it, while keeping therest of the model fixed, on the new task examples. Table 4 shows the results. The obtained accuracyfor the added digit ‘9’ task are 64.68%, other tasks mean accuracy remains unchanged (74.89%).
Table 5: Hetrogenous tasks: executing recognition and segmentation with / without tasks selection.
Table 6: (a) Performance on CLEVR, Our approach is scalable with the number of tasks with an increasinggap over ch-mod. (b) Test of ablations and image content. ‘im’: image contribution.
