Table 1: Few-shot streaming label classification accuracy (F1 score) on Delicious with 95% confidence intervals. k-way Ns-shot denotes k new labels with Ns tagged examples per label for training.				Method	5-way 1-shot	5-way 5-shot	10-way 1-shot	10-way 5-shotStreaming label learning baselines				SLL (You et al., 2016)	17.59±1.22%	29.27±0.67%	7.84±1.59%	17.53±0.86%DSLL (Wang et al., 2020b)	24.42±1.84%	37.63±0.96%	11.22±1.47%	25.68±0.82%Few-shot learning baselines				MAML (Finn et al., 2017)	30.78±1.82%	40.76±0.95%	15.77±0.52%	27.69±0.37%ProtoNet (Snell et al., 2017)	36.09±0.78%	42.53±0.64%	25.42±0.71%	31.26±0.59%Reptile (Nichol et al., 2018)	29.09±0.13%	40.14±0.16%	19.53±0.21%	28.15±0.35%ATAML (Jiang et al., 2018)	35.53±0.82%	43.51±0.75%	20.82±0.67%	30.59±0.59%MAML++ (Antoniou et al., 2019)	41.21±0.64%	44.51±0.55%	23.91±0.57%	29.33±0.47%LEO (Rusu et al., 2019)	42.52±0.16%	45.07±0.11%	30.95±0.14%	33.01±0.08%Semantic-augment baselines				DSLL (+semantic embeddings)	29.15±0.85%	40.26±0.25%	15.63±0.74%	26.94 ±0.51%MAML (+semantic embeddings)	37.94±1.27%	44.03±0.62%	23.51±0.91%	32.56±0.84%AM3 (Xing et al., 2019)	44.67±0.61%	47.44±0.47%	32.09±0.53%	34.73±0.43%SIN (ours)	46.17±0∙35%	49.19±0.27%	35.06±0.31%	38.35±0.18%Theorem 1. Assume the loss function ` is bounded and C-Lipschitz. Let h be a hypothesis in termsof k new labels using formulation hN =argminh∈H RN (h) over a set of N training examples. Thenwith probability at least 1-δ, we have
Table 2: Ablation study of componentsthat we improve and enable few-shot learning methods to handle the scenario of multiple new labels.
Table 3: Definitions of streaming label learning performance measures.
Table 4: Few-shot streaming label classification accuracy (AUC) on Delicious with 95% confidence intervals. k-way Ns-shot denotes k new labels with Ns tagged examples per label for training.				Method	5-way 1-shot	5-way 5-shot	10-way 1-shot	10-way 5-shotStreaming label learning baselines				SLL (You et al., 2016)	60.11±1.81%	67.83±1.37%	61.10±1.53%	70.59±1.16%DSLL (Wang et al., 2020b)	63.59±2.36%	70.91±1.31%	62.70±2.07%	72.87±1.38%Few-shot learning baselines				MAML (Finn et al., 2017)	68.24±2.02%	73.82±1.35%	68.17±1.01%	74.42±0.74%ProtoNet (Snell et al., 2017)	65.96±1.53%	71.87±1.23%	69.79±1.45%	73.71±1.21%Reptile (Nichol et al., 2018)	66.65±0.53%	70.39±0.61%	68.54±0.49%	73.79±0.55%ATAML (Jiang et al., 2018)	70.14±1.54%	72.50±1.42%	68.68±1.20%	74.60±1.04%MAML++ (Antoniou et al., 2019)	72.50±1.31%	74.09±1.13%	71.38±1.26%	73.85±0.92%LEO (Rusu et al., 2019)	72.39±0.34%	74.87±0.26%	69.43±0.27%	74.10±0.18%Semantic-augment baselines				DSLL (+semantic embeddings)	67.35±1.75%	73.13±0.63%	68.38±1.39%	72.99 ±1.12%MAML (+semantic embeddings)	71.43±1.91%	74.30±1.28%	70.03±1.54%	74.59±1.32%AM3 (Xing et al., 2019)	72.58±1.31%	74.95±0.68%	72.25±1.13%	74.43±0.74%SIN (ours)	75∙38±0.61%	76.41±0.51%	73.90±0.71%	76.13±0.47%Figure 7: Performance comparison on Mir-Flickr. k-way Ns-shot denotes k new labels with Nstagged examples per label for training.
