Table 1: Experiments on the synthetic datasets with a GPU Nvidia Quadro P4000. Average(std)over 10 runs. All the models are trained for 25k iterations and the total training time is indicated inseconds.
Table 2: KL-divergence between the generated distribution and true distribution for an architecturewith 3 conv. layers for the Stacked MNIST dataset. The number of counted modes assesses modecollapse. The results are obtained after 25k iterations and we report the average(std) over 10 runs.
Table 3: Stacked MNIST experiment for an architecture with 4 conv. layers. All the models aretrained for 25k iterations with a batch size of 64, a learning rate of 2e-4 for Adam and a normallatent distribution. The evaluation is over 10k samples and we report an average(std) over 10 runs.
Table 4: Generation quality on CIFAR-10 and CIFAR-100 with DCGAN architecture. Average(std)over 10 runs. 100k iterations for each. For improving readability, SWD score was multiplied by100.
Table 5: Generation quality on STL-10 with DCGAN architecture. Average(std) over 5 runs. 150kiterations for each. SWD score was multiplied by 100 for improving readability.
Table 6: Best achieved IS and FID, using a ResNet architecture. Results with an asterisk are quotedfrom their respective papers (std in parenthesis). BuresGAN results were obtained after 300k itera-tions and averaged over 3 runs. The result indicated with f are taken from WU et al. (2018). For allthe methods, the STL-10 images are rescaled to 48 × 48 × 3 in contrast with Table 5.
Table 7: The generator and discriminator architectures for the synthetic examples.
Table 8: Respectively the MDGAN encoder model and VEEGAN stochastic inverse generator ar-chitectures for the synthetic examples. The output of the VEEGAN models are samples drawn froma normal distribution with scale 1 and where the location is learned.
Table 9: The generator and discriminator architectures for the Stacked MNIST experiments. TheBN column indicates whether batch normalization is used after the layer or not. For the experimentswith 2 convolution layers in Table 21, the final convolution layer is removed in the discriminator.
Table 10: The generator and discriminator architectures for the Stacked MNIST experiments with 4layers for both generator and discrimintor. The BN column indicates whether batch normalizationis used after the layer or not.
Table 11: The MDGAN encoder model architecture for the Stacked MNIST experiments. The BNcolumn indicates whether batch normalization is used after the layer or not.
Table 12: The hyperparameters and architectures of the stacked MNIST experiment performed bythe different methods are given here. This represents our best efforts at providing a comparisonbetween the different parameters used. The asterisks ** indicate that the parameters where obtainedby the Github repository. Notice that GDPP paper also used 30000 iterations for training DCGANand unrolled GAN (indicated by *). BUreSGAN's column refers to the settings of experiment ofTable 2 and Table 3 respectively, for which the different values are separated by the symbol &.
Table 13: The generator and discriminator architectures for the CIFAR-10 and CIFAR-100 experi-ments. The BN column indicates whether batch normalization is used after the layer or not.
Table 14: The MDGAN encoder model architecture for the CIFAR-10 and CIFAR-100 experiments.
Table 15: The generator and discriminator architectures for the STL-10 experiments. The BN col-umn indicates whether batch normalization is used after the layer or not.
Table 16: The MDGAN encoder model architecture for the STL-10 experiments. The BN columnindicates whether batch normalization is used after the layer or not.
Table 17: The generator (top) and discriminator (bottom) ResNet architectures for the CIFAR-10experiments.
Table 18: The generator (top) and discriminator (bottom) ResNet architectures for the STL-10 ex-periments. For the experiment with full-sized 96x96 images, an extra upsampling block was addedto the generator.
Table 19: Average time per iteration in seconds for the convolutional architecture. Averaged over5 runs, with std in parenthesis. The batch size is 256. For Stacked MNIST, we use a discriminatorarchitecture with 3 convolutional layers.
Table 20: Inception Score for the best trained models on CIFAR-10, CIFAR-100 and STL-10, witha DCGAN architecture (higher is better).
Table 21: KL-divergence between the generated distribution and true distribution (Quality, lower isbetter). The number of counted modes indicates the amount of mode collapse (higher is better). 25kiterations and average and std over 10 runs. Same architecture as in Table 2 with a discriminatorwith 2 convolutional layers.
