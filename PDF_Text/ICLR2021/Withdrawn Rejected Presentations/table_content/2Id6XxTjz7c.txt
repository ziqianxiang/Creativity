Table 1: Post-training (3 bits per weight) quantization comparison on MSE (quantization error),average scaling factor values, training loss, and training model accuracy. For importance metrics,E = 1.0 is used while P and C are not considered.
Table 2: Test score after post-training quantization with various E, C and P choices when thequantization bit is 3.
Table 3: Quantization results on various language models (see Appendix for details on modeldescriptions). Alternating quantization scheme significantly improves test scores when combinedwith our proposed importance metrics (described as ‘Ours’) that are searched by layer-wise BO.
Table 4: Dataset usages and maximum iterations for Bayesian optimization. In the case of largetraining dataset (such as MNLI and SQUAD v1.1), we use samples.
Table 5: 16 sets of hyper-parameters selected for our manual search of importance metric.											Setl	Set2	Set3	Set4	Set5	Set 6	Set 7	Set 8P=0.0 (default)	E	1.0	1.0	1.0	-^10-	~~Q3^^	0.2	0.5	0.5	C	-1 ^^	0.99	0.999	0.9999	-10^^	1.0	0.999	0.9999		Set9	Set 10	-Senr	Set 12	Set 13	Set 14	Set 15	Set 16C =1.0 (default)	E	0.0	0.0	0.0	-^0.0-	-10^^	1.0	1.0	1.0	P	0.02	0.05	0.1	0.2	0.02	0.05	0.1	0.2C.1 Fine-tuned BERT, DistilBERT and Longformer15Under review as a conference paper at ICLR 2021Table 6: Test score after post-training quantization with various E, C and P choices when thequantization bit is 4.
Table 6: Test score after post-training quantization with various E, C and P choices when thequantization bit is 4.
Table 7: Model accuracy of BERT on MRPC quantized into 3 bits per weight by Alternating methodwith different number of parameters sharing a scaling factor.
Table 8: Hyper-parameter search results of BERT-base (quantized into 3 bits per weight) using manualsearch, model-wise BO, and layer-wise BO.
Table 9: Hyper-parameter search results of DistilBERT-base (quantized into 3 bits per weight) usingmanual search, model-wise BO, and layer-wise BO.
Table 10: F1 scores of Longformer on SQUAD v1.1 after post-training quantization (4 bits perweight) with various E, C and P choices.
Table 11: Hyper-parameter search results of Longformer using manual search, model-wise BO, andlayer-wise BO.
Table 12: Perplexity of AWD-LSTM model on PTB test dataset after post-training quantization withvarious E, C and P choices.
Table 13: Hyper-parameter search results of fine-tuned AWD-LSTM (quantized into 2/3/4 bits perweight) using manual search, model-wise BO, and layer-wise BO.
Table 14: Quantizing Transformer by using 3 bits per weight with different quantization schemes andthe metric to be optimized by BO.
Table 15: Post-training quantization comparison on quantization MSE, average scaling factor values,training loss, and training model accuracy. For importance metrics, E=1.0 is used while P and Care not considered.
Table 16: Model accuracy(%) on test dataset after post-training quantization with various E and Cchoices. q is the number of quantization bits.
Table 17: The optimal hyper-parameters searched by Bayesian optimization when Alternatingquantization method is utilized and q is the number of quantization bits.
