Table 1: SQuAD v2.0 results for top answer predictionModel Name	EM	F1	total	HasAns-EM	HasAnS-FI	HasAns-total	NoAns-EMt	NoAns-F1	NoAns-totalbert-qa	63.6	67.2	11873	63.2	70.2	5928	64.1	64.1	5945bert-ms-softmax	64.0	66.8	11873	63.3	68.8	5928	64.7	64.7	5945bert-ms-softmax-sl	64.9	67.4	11873	62.5	67.5	5928	67.3	67.3	5945bert-ms-sigmoid	59.7	62.3	11873	59.4	64.6	5928	60.0	60.0	5945Since span-image architecture enables BERT to generate multi-span answers, we use top-K accuracyto compare bert-qa with bert-ms-softmax-sl for K=1,3,5, and 10. As shown in Table 2, both modelsperform similarly for K=1, but bert-ms-softmax-sl model significantly performs better in all metricsfor K > 1.
Table 2: SQuAD results using top-K answersRow Labels	EM	F1	HasAns-EM	HasAns-F1	NoAns-EM	NoAns-F1K=1 bert-qa	63.24±0.23	66.71±0.25	63.60±0.25	70.54±0.20	62.89±0.7	62.89±0.7bert-ms-softmax-sl K=3	64.12±0.37	66.71±0.33	62.90±0.48	68.07±0.48	65.35±0.49	65.35±0.49bert-qa	80.44±0.14	83.08±0.18	83.05±0.01	88.34±0.09	77.83±0.27	77.83±0.27bert-ms-softmax-sl K=5	88.42±0.28	90.53±0.19	86.04±0.49	90.25±0.27	90.80±0.39	90.80±0.39bert-qa	85.39±0.18	87.53±0.18	87.23±0.08	91.51±0.07	83.56±0.29	83.56±0.29bert-ms-softmax-sl K=10	93.44±0.16	94.96±0.15	90.74±0.14	93.79±0.08	96.13±0.24	96.13±0.24bert-qa	90.45±0.11	92.09±0.07	90.93±0.14	94.23±0.06	89.96±0.08	89.96±0.08bert-ms-softmax-sl	96.81±0.10	97.83±0.06	94.63±0.18	96.66±0.12	98.98±0.09	98.98±0.09fine-tune BERT with. We measure performances on multi-span answer prediction (i.e., whether allrelevant information to compute the total quantity has been extracted or not). On this task, since thespan count can be any number, we compare bert-ms-sigmoid and bert-ms-sigmoid-sl with bert-qa.
Table 3: Question-answer pairs in the multi-answer datasetParagraph	Question	AnswersBlackstrap Molasses Yummmy 5 Lbs, Kosher Certified, BPA free container, All Natural, Unsulfured Sale by weight	What is the weight?	-[5]-Sky Organics Grapeseed Oil 100% Pure, Natural & Cold-Pressed - Ideal for Massage, Cooking and Aromatherapy- Rich in Vitamin A, E and K- Helps Reduce Wrinkles, 8 oz (Pack of 2) (Packaging may vary)	What is the volume?	[8, 2]XUAN YUAN Fabric Shaver-Wool Clothes Pilling Trim- Rechargeable Shaving Hair Removal Artifact Cleaner Tick Suction Household Ball Machine Fabric razor (Size : Spare cutter head x1)	How many are there?	[]Model Name	Table 4: Multi-Answer Dataset Results EM F1 total HasAns-EM HasAns-F1 HasAns-total NoAns-EM NoAns-F1 NoAns-totalbert-qa bert-ms-sigmoid bert-ms-sigmoid-sl	79.2	85.0	35728	64.4	76.3	17510	93.5	93.5	18218 88.9	90.3	35728	84.9	87.6	17510	92.8	92.8	18218 89.1	90.4	35728	85.0	87.6	17510	93.0	93.0	18218To improve bert-qa’s multi-span prediction performance, we introduce a post-processing step wherea weighted penalty term for the span length is added to start/end index probabilities. Results fordifferent span-length penalty weights (λ's) are given in Table 5. Span-length penalization improvesthe performance of bert-qa, but it still performs worse than span-image models when an answer ispresent in the input text.
Table 5: Multi-answer dataset results using penalty weight λModel Name	λ	EM	F1	total	HasAns-EM	HasAns-F1	HasAns-total	NoAns-EM	NoAns-F1	NoAns-totalbert-base-uncased	0	79.2	85.0	35728	64.4	76.3	17510	93.5	93.5	18218bert-base-uncased	5	81.9	86.8	35728	69.8	79.9	17510	93.5	93.5	18218bert-base-uncased	10	87.3	89.7	35728	80.9	85.8	17510	93.5	93.5	18218bert-base-uncased	20	88.5	90.1	35728	83.3	86.5	17510	93.5	93.5	18218bert-base-uncased	25	87.5	89.2	35728	81.2	84.7	17510	93.5	93.5	18218bert-base-uncased	30	86.8	88.6	35728	79.8	83.4	17510	93.5	93.5	18218ReferencesJacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deepbidirectional transformers for language understanding. NAACL HLT 2019 - 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies - Proceedings ofthe Conference,1:4171-4186, 2019.
