Table 1: Ablation study of churn across 5 runs on CIFAR-10 with a ResNet-56. Holding theinitialization constant across models always decreases churn, but using identical mini-batch orderingand completely removing augmentation can increase churn with a decrease in accuracy.
Table 2: Estimate of Churn (cf. (3)) and SChurn (cf. (4)) on the test sets. For each setting, we reportthe mean and standard deviation over 10 independent runs, with random initialization, mini-batchesand data-augmentation. We report the values corresponding to the smallest churn for each method(see Table 5 in ยง A for the exact parameters). We boldface the best results in each column. First wenotice that both the proposed methods are effective at reducing churn and Schurn, with Co-distillSKLshowing significant reduction in churn. Additionally, these methods also improve the accuracy.
Table 3: Weight decay ablation studies. Similar to Table 2, we provide results on accuracy andchurn for baseline and the proposed approaches, with and without weight decay. We notice that theproposed approaches improve churn both with and without weight decay.
Table 4: Expected Calibration Error. We compute the Expected Calibration Error (ECE) (Guoet al., 2017) to evaluate the effect on calibration of logits by the churn reduction methods consideredin this paper, for different datasets. We report ECE for the predictions of the models used to reportaccuracy and churn in Table 2. We note that while the minimum entropy regularizers, predictably,increase the calibration error, our combined approach with co-distillation results in calibration errorcompetitive with 2-ensemble distillation method.
Table 5: In this table we list the hyper-parameter values corresponding to the settings for the resultsin Table 2. For each method we experiment with a range of hyper-parameters, and 10 independentruns, as described in the Section A and report the results for the setting with the best performance.
Table 6: Ablation study of churn across 5 runs on CIFAR-10 with a ResNet-32 on GPU. Holdingthe initialization constant across models decreases churn significantly, and using identical input data(keeping minibatch ordering and augmentation constant) lowers churn further. Unlike Table 1, theseexperiments were performed on GPU rather than TPU. Under this setting, using fixed initializationdoes achieve a lower churn at a much more expensive computation cost.
Table 7: Similar to Table 2, we provide results on accuracy and churn for 2-ensemble and proposedapproaches. We notice that proposed approaches are either competitive or improve churn over2-ensemble method, while keeping the inference costs low.
