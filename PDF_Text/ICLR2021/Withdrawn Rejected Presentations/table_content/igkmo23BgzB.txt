Table 1: Comparison of reduced-precision training for top-1 accuracy (%) using ResNet-50 (Im-ageNet). For works that did not evaluate on ResNet-50, we include AlexNet results (italicized).
Table 2: Results of LogBTQ training compared with the two most relevant low-precision trainingschemes. Note that the previous work listed here is not directly comparable to ours; as is shownin Table 1. Though these works perform the majority of computation in 8 bits, they still have theadvantage of FP16 or Hybrid FP8/16 when accumulating gradients, and dynamic quantization range.
