Table 1: Optimizers to be evaluated with their tunable hyperparameters. Specifically, α0 representsthe initial learning rate. μ is the decay factor of the first-order momentum for non-adaptive methodswhile β1 and β2 are coefficients to compute the running averages of first-order and second-ordermomentums. is a small scalar used to prevent division by 0.
Table 2: Tasks for benchmarking optimizers. Details are provided in Appendix D.
Table 3: CPE for different optimizers on benchmarking tasks. The best performance is highlightedin bold and blue and results within the 1% range of the best are emphasized in bold only.
Table 4: CPE of different optimizers computed under curves trained with Ωpartial on four full datasets.				Optimizer	CIFAR10 (%)↑	CIFAR100(%)↑	MRPC (%)↑	PPI↑SGD	90.04 ± 0.16	67.91 ± 0.23	66.62 ± 3.47	66.830 ± 0.010Adam	90.52 ± 0.03	67.04 ± 0.27	73.13 ± 1.16	70.420 ± 0.007RAdam	90.30 ± 0.14	67.06 ± 0.17	79.01 ± 3.10	70.840 ± 0.010Yogi	89.63 ± 0.39	67.58 ± 0.19	68.40 ± 1.68	67.990 ± 0.003LARS	90.17 ± 0.13	67.29 ± 0.14	64.43 ± 2.72	68.400 ± 0.005LAMB	90.51 ± 0.07	66.13 ± 0.02	78.94 ± 1.25	70.110 ± 0.008Lookahead	88.36 ± 0.06	67.10 ± 0.31	68.81 ± 1.22	69.710 ± 0.0035	Conclusions and DiscussionsIn conclusion, We found there is no strong evidence that newly proposed optimizers consistentlyoutperform Adam, While each of them may be good for some particular tasks. When deciding thechoice of the optimizer for a specific task, people can refer to results in Table 3 and 9. If the task iscontained in Table 2, he/she can directly choose the one With the best CPE or best peak performancebased on his/her goal of the task (easy to tune or high final performance). On the other hand, eventhough the desired task is covered, people can also gain some insights from the results of the mostsimilar task in Table 2, or refer to the performance profile in Figure 4 to pick adaptive methodslike Adam. Besides choosing the optimizer, it Will contribute to designing a neW optimizer as Well.
Table 5: A summary of popular optimization algorithms with different choices of mt, vt and rt.
Table 6: Hyperparamter search space and default valueTask	Hyperband parameter			R	n	ηImage Classification	200	172	VAE	50	62	GAN	200	172	η=3GLUE benchmark	10	200	Graph learning	200	200	RL	200	172	Table 7: Hyperband parameters for each task.
Table 7: Hyperband parameters for each task.
Table 8: CPE on CIFAR10 with different η. The value in the round brackets is peak performance.
Table 9: Peak performance during end-to-end training. The best one for each task is highlighted inbold.
Table 10: CPE on GAN for end-to-end training. The value in the bracket is peak performance.
