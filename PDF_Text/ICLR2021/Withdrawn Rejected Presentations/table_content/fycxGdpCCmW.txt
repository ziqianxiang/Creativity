Table 1: OOD Detection Results. The model is WideResNet-28-10 (without BN) following the settings ofJEM (Grathwohl et al., 2019). The comparison with JointLoss (Winkens et al., 2020) follows their setting to useResNet-50. The results of JointLoss are obtained from its paper. The training dataset is CIFAR-10. Values areAUROC. Standard deviations given in Table 4 (Appendix).
Table 2: Comparison on three standard image classification datasets: All models use the same batch sizeof 256 and step-wise learning rate decay, the number of training epochs is 200. The baselines SupervisedContrastive (Khosla et al., 2020), JEM (Grathwohl et al., 2019), and our method HDGE are based on WideResNet-28-10 (Zagoruyko & Komodakis, 2016).
Table 3: Hybrid modeling results on CIFAR-10. All models are based onWideResNet-28-10 (Zagoruyko & Komodakis, 2016)(without BN). ResidualFlow (Chen et al., 2019), Glow (Kingma & Dhariwal, 2018), IGEBM (Du &Mordatch, 2019), SNGAN (Miyato et al., 2018), NCSN (Song & Ermon, 2019),JEM (Grathwohl et al., 2019)Figure 3: Class-conditionalsamples generated byrunning HDGE+JEM onCIFAR-10.
Table 4: OOD Detection Results. The model is WideResNet-28-10 (without BN) following the settings ofJEM (Grathwohl et al., 2019), except ResNet-50 when comparing with JointLoss (Winkens et al., 2020). Thetraining dataset is CIFAR-10. Values are AUROC. Results of the baselines are from Grathwohl et al. (2019)and Winkens et al. (2020).
Table 5: OOD Detection Results. The model is WideResNet-28-10 (without BN) following the settings ofJEM (Grathwohl et al., 2019). The training dataset is CIFAR-10. Values are AUROC.
Table 6: Ablation of approximation on detecting OOD samples. We use CIFAR10 for in-distribution. N isthe batch size of JEM and HDGE. HDGE uses N = 64. K is the number of negative samples in contrastivelearning.
