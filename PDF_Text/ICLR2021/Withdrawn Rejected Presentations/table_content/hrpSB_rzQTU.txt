Table 1: Details of pre-training corporaDatasets	Hours	Speaking StyleOpen Mandarin	1150	ReadingDidi Callcenter - 1K	1150	SpontaneousDidi Callcenter	5000	SpontaneousDidi Dictation	5000	ReadingLibrispeech	960	ReadingFisher - 1K	960	Spontaneousmodeling units for HKUST and AISHELL like described in[37], while 2000 BPE subwords [38] is used for experimentson Switchboard.
Table 2: Character Error Rates(%) and Relative Error RatesReduction(%) on HKUST and AISHELL test setTask	Pre-training Data	Hours	CER	RERR	-	-	23.3	-	HKUST	170	22.8	2.14HKUST	Open Mandarin (8k)	1150	22.7	2.58	Didi Callcenter-1K	1150	22.0	5.58	Didi Callcenter	5000	21.5	7.73	Didi Dictation (8k)	5000	22.1	5.15	-	-	6.82	-AISHELL	AISHELL	178	6.61	3.07	Open Mandarin	1150	6.38	6.45	Didi Dictation	5000	6.26	8.21Table 3: Word Error Rates(%) and Relative Error Rates Reduc-tion(%) on Switchboard and CallHome test setPre-training Data	Hours	WER SWB CH		RERR SWB CH	-	-	8.8	17.8	-	-Switchboard	260	8.5	17.4	3.41	2.25Fisher - 1K	960	8.0	16.2	9.09	8.99Librispeech (8k)	960	8.4	17.2	4.55	3.37
Table 3: Word Error Rates(%) and Relative Error Rates Reduc-tion(%) on Switchboard and CallHome test setPre-training Data	Hours	WER SWB CH		RERR SWB CH	-	-	8.8	17.8	-	-Switchboard	260	8.5	17.4	3.41	2.25Fisher - 1K	960	8.0	16.2	9.09	8.99Librispeech (8k)	960	8.4	17.2	4.55	3.374.3.	Effect of pre-training data speaking styleThe results on HKUST and AISHELL with different pre-training data are listed in Table 2. Our baseline result with-out MPC matches the strong baseline in [39]. Comparing rel-ative error reduction of HKUST with same amounts of pre-training data, it is obvious MPC models pre-trained with match-ing speaking style data achieved lower error rates for down-stream tasks.
Table 4: Character Error Rates(%) and Relative Error RatesReduction(%) for uni-directional CTC and uni-directionalRNN-T with pre-trained MPCTask	Model	Pre-training Data	CER	RERR		-	29.3	-	CTC	Didi Callcenter - 1K	28.0	4.56HKUST		Didi Callcenter	27.6	5.80		-	28.1	-	RNN-T	Didi Callcenter - 1K	26.9	4.43		Didi Callcenter	26.6	5.20AISHELL	CTC	- Open Mandarin	9.91 9.42	- 4.91	RNN-T	- Open Mandarin	9.43 8.88	- 5.83Figure 2: CER(%) with each layer of pre-trained Transformerencoder. Results on HKUST is pre-trained with Didi Callcenterand results on AISHELL is pre-trained with Open MandarinTable 5: MPC + APC is the model pre-trained with APC 50% oftime and MPC 50% of time. Relative Error Rates Reduction(%)is calculated with HKUST baseline without MPCPre-training Data	Objective	CER	RERR	MPC	28.0	4.56
Table 5: MPC + APC is the model pre-trained with APC 50% oftime and MPC 50% of time. Relative Error Rates Reduction(%)is calculated with HKUST baseline without MPCPre-training Data	Objective	CER	RERR	MPC	28.0	4.56Didi Callcenter - 1K	APC	27.8	5.12	MPC + APC	27.2	7.32	MPC	27.6	5.80Didi Callcenter	APC	27.9	4.72	MPC + APC	26.8	8.46stage. Previous work on APC showed a future prediction stepof 5 gave best results on Transformer decoder [13]. We im-plemented APC with the same future prediction step and setswitching probability p to 0.5. Experiments were conducted onHKUST dataset with Transformer CTC model and the results ofdifferent pre-training objectives are presented in Table 5. Whenused alone, APC and MPC got similar improvements. Combin-ing APC with MPC results in consistent gains over them, whichechoes with the findings in [28].
Table 6: Results on HKUST and AISHELL with different knowl-edge transfer methods. HKUST is pre-trained with Didi Call-center. AISHELL is pre-trained with Didi Dictation. TargetData + Layer-wise is the combination of target data adaptionand layer-wise discriminative trainingTask	Knowledge Transfer Methods	CER	RERR	-	21.5	-	Target Data Adaption	21.2	1.40HKUST	Layer-wise Discriminative	21.3	0.93	Target Data + Layer-wise	20.8	3.26	Multi-Task MPC	21.1	1.86	-	6.26	-	Target Data Adaption	6.23	0.48AISHELL	Layer-wise Discriminative	6.21	0.80	Target Data + Layer-wise	6.01	3.99	Multi-Task MPC	6.13	2.08sults we used are HKUST pre-trained with Didi Callcenter andAISHELL pre-trained with Open Mandarin. Using target dataadaption or layer-wise discriminative training alone doesnâ€™thelp the knowledge transfer of MPC very much. But when com-
