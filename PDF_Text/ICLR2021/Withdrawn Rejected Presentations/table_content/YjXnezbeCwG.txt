Table 1: BLEU scores on IWSLT En→Vi simultaneous NMT tasks.
Table 2: Comparison of training speed (batch / sec) between wait-k and our methods.
Table 3: Ablation study for feature selection on IWSLT’14 En→De dataset.
Table 4: Ablation study for feature selection on IWSLT’14 En→De dataset.
Table 5: BLEU scores on IWSLT simultaneous NMT tasks.
Table 6: BLEU scores on WMT’15 En→De dataset.
Table 7: Example 1 for En→Zh wait-3 translation. In this example and the next example, differentcolors represent different meanings. Specifically, green and red represents information that does notexist in the source sentence (i.e., anticipated by the model), where green represents information thatis consistent with the input sentence (i.e. correctly anticipated), and red represents information that isinconsistent with the input sentence (i.e., wrongly anticipated).
Table 8: Example 2 for En→Zh wait-3 translation, where POS indicates possessive forms, and PROGindicates progressive tense. In this example, there is no need to anticipate. However, wait-3 stillanticipates "发现"(found) and makes a mistake. Wait-k* makes a mistake by repeating "打开 了 网站" (opened the website). Ours generates the best translation.
