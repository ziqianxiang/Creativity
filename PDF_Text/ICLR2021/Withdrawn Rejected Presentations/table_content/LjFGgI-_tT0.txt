Table 1: Predictive performance comparison on image classification. NLL denotes the negative log-likelihood.
Table 2: Accuracy ↑ comparison on open-set face recognition with MobileNetV2 architecture.
Table 3: Comparison on the quality of uncertainty estimates in terms of average precision (AP) ↑ of uncertaintybased binary classification. (CIFAR-10 and ImageNet)Method		CIFAR-10			ImageNet		Adversarial (PGD)	Fake (SNGAN)	Adversarial (PGD)	Fake (BigGAN)MAP	0.307	0.800	0.308	0.010Deep Ensemble	0.427	0.812	-	-SWAG	0.316	0.816	-	-Laplace Approx.	0.308	0.800	0.311	0.015MC dropout	0.308	0.803	0.309	0.012BNN	0.307	0.799	0.310	0.021BayesAdapter-	0.307	0.806	0.387	0.013BayesAdapter	0.993±0.003	0.994±0.001	0.964±0.009	0.848±0.037converged deterministic checkpoints is beneficial to bypass the local optimas potentially encoun-tered by direct Bayesian inference. The popular baselines MC dropout and SWAG show weakerperformance on ImageNet and CIFAR-10, respectively, revealing limited applicability. Also of notethat no method shows dominant performance on face recognition, probably due to the diversity ofthese validation sets. Across these tasks, BayesAdapter is slightly worse than its regularization-freeversion BayesAdapter-. This is reasonable since such a regularization enforces the model to tradepartial capacity for fidelity of uncertainty estimates. Nevertheless, BayesAdapter is substantiallybetter than its fine-tuning start point MAP and the BNN trained from scratch in most settings.
Table 4: Comparison on ECE ].
Table 5: Ablation study on γ .
Table 6: Comparison on the quality of uncertainty estimates in terms of AP ↑ on face recognition tasks. Theupper part is for adversarial instances and the other part is for DeepFake.
