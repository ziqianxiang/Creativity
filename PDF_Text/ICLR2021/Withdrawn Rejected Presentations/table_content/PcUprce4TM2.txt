Table 1: CAFE vs DLGIterations Datasets Batch siZe'^^^-≥χ^	CIFAR-10	CIFAR-100	Linnaeus1(DLG)	284.4	-266.9-	366.710 X 4	9.50-	-600-	9.5020 × 4	6.75	-386-	4.7530 × 4	4.83-	-341-	3.1740 × 4	3.75	3.75	2.375(a) Comparison of data leakage speed. Loweriteration count is faster.
Table 2: PSNR via LossPSNR∖∩atasets Loss^^^^^≥∖^	CIFAR-10	CIFAR-100	Linnaeus	PSNR'∖Datasets Loss^^	CIFAR-10	CIFAR-100	LinnaeusCAFE ((8))	35.03	36.90	36.37	CAFE ((8))	43.31	48.10	35.06CAFE with cosine similarity	30.15	31.38	30.76	CAFE with cosine similarity	30.96	43.68	34.90Loss in (GeiPing et al., 2020)	16.95	19.74	16.42	Loss in (GeiPing et al., 2020)	12.76	10.85	10.46(a) HFL	(b) VFL(4 workers, batch ratio = 0.1, batch size 10 × 4)	(4 workers, batch ratio = 0.1, batch size 40)4.2	CAFE in HFL settingsIn the HFL setting, we use a neural network consisting of 2 convolutional layers and 3 fully con-nected layers. The number of output channels of the convolutional layers are 64 and 128 respectively.
Table 3: PSNR via Batch sizePSNR Datasets BatCh size'"^^^^	CIFAR-10	CIFAR-100	Linnaeus10 per worker	35.03	36.90-	36.3720 per worker	33.14	33.99-	36.3230 per worker	32.31	33.21-	35.9640 per worker	30.59	30.70	35.49(a) HFL (4 workers, batch ratio = 0.1)PSNR Datasets Batch size'^'^^	CIFAR-10	CIFAR-100	Linnaeus8	41.80	44.42	39.9640	59.51	-65.00-	41.3780	57.20	-6310-	43.66160	54.74	64.75	38.72(b) VFL (4 workers, batch ratio = 0.2)Table 4: PSNR via Batch ratioPSNR Datasets Batch ratio'''∖×^	CIFAR-10 (HFL)	Linnaeus	CIFAR-10 (VFL)0.1	34.10	35.38	48.78005	34.49	32.92	55.46002	37.96	35.66	48.450.01	35.39	36.56	46.46Figure 5: PSNR and training loss curves
Table 4: PSNR via Batch ratioPSNR Datasets Batch ratio'''∖×^	CIFAR-10 (HFL)	Linnaeus	CIFAR-10 (VFL)0.1	34.10	35.38	48.78005	34.49	32.92	55.46002	37.96	35.66	48.450.01	35.39	36.56	46.46Figure 5: PSNR and training loss curves4.3	CAFE in VFL settingsSince DLG cannot be applied in VFL protocol, we test the performance of CAFE on various factors.
