Table 1 : resnet-20/cifar-10 training for 300 epochs on 2 gpus.
Table 2: ResNet-20/Cifar- 10 training for 300 epochs on 2GPUs by Lap-sgd. The LR is warmed up similar to Pl-sgd.Wefollow the same averaging schedule and other HPs as those of Pl-sgd. The listed results are average of 3 runs.
Table 3: Lpp-sgd performance. Other details are identical tothose in Table 2.
Table 4:	Lap-sgd perfor-mance.
Table 5:	Lap-sgd perfor-mance.
Table 6: WIDERESNET-16x8/CIFAR-10 training in the settings S1: Q = 2, S2: Q = 4, and S3: Q = 8. MB-SGD-128 indicates MB-SGD method with Bloc = 128 and similarly for others. For LAP-SGD, we spawned 4concurrent processes on each GPU: U = 4, whereas for Lpp-sgd U = 3 concurrent processes were spawned.
Table 7: Performance of RESNET-20 on CIFAR- 1 00 over the setting S1.															I Method	B	U	Tr.L.	Te.L.	Tr.A.	Te.A.	T Il Method		B	U	Tr.L.	Te.L.	Tr.A.	Te.A.	T ILap-sgd	64	4	0.007	1.341	99.98	75.97	1564	Mb-sgd	512	-	0.034	1.918	99.64	63.74	2076Lpp-sgd	64	3	0.008	1.554	99.97	75.42	1376	Pl-sgd	128	-	0.020	1.354	99.81	72.38	2108Mb-sgd	128	-	0.007	1.348	99.97	73.29	2115	Pl-sgd	512	-	0.393	1.758	88.91	60.47	1976Table 8: Performance of WIDERESNET-16x8 on CIFAR- 1 00 over the setting S3.
Table 8: Performance of WIDERESNET-16x8 on CIFAR- 1 00 over the setting S3.
Table 9: Performance of S QUEEZENET on CIFAR- 1 0 over the setting S1.
Table 10: RES NET-18/IMAGENET Training results.
Table 11: LARS performance.
