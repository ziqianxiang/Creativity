Table 1: Quantitative comparisons between the experimental group (tagged with “+”) and the controlgroup (tagged with “0”). All experiments are run across five different random seeds. ? denotes themost significant case where L-NMS does not generate obvious improvements which we will discussin the text.
Table 2: Training Configurations For MONetType	the trainings of MONet0 and MONet+Optimizer	RMSpropINITIAL LEARNING RATE η0	3e-4Batch size	8LEARNING RATE AT STEP s	N/ATotal gradient steps	600kGradient-norm clipping	5.0log-normal likelihood strength	1.0KL (GAUSSIAN PRIOR) STRENGTH β	0.5KL (attention prior) strength	0.5L-NMS (MONet+ only) strength	0.5	Table 3: Training Configurations of IODINE0 and IODINE+Type	the trainings of IODINE0 and IODINE+Optimizer	AdamINITIAL LEARNING RATE η0	2e-4Batch size	8LEARNING RATE AT STEP s	?max{0.1ηo + 0.9ηo ∙ (1.0 — s∕1e6), 0.1ηo}Total gradient steps	600kGradient-norm clipping	5.0
Table 3: Training Configurations of IODINE0 and IODINE+Type	the trainings of IODINE0 and IODINE+Optimizer	AdamINITIAL LEARNING RATE η0	2e-4Batch size	8LEARNING RATE AT STEP s	?max{0.1ηo + 0.9ηo ∙ (1.0 — s∕1e6), 0.1ηo}Total gradient steps	600kGradient-norm clipping	5.0inference iterations (Greff et al., 2019)	5log-normal likelihood strength	1.0KL (GAUSSIAN PRIOR) S TRENGTH β	1.0L-NMS (IODINE+ only) STRENGTH	1.0? : SAME SCHEDULER AS GQNS’.	Table 4: Training Configurations of MulMON0 and MulMON+Type	the trainings of MulMON0 and MulMON+Optimizer	AdamINITIAL LEARNING RATE η0	2e-4Batch size	8LEARNING RATE AT STEP s	?max{0.1ηo + 0.9ηo ∙ (1.0 — s∕1e6), 0.1ηo}Total gradient steps	600k
Table 4: Training Configurations of MulMON0 and MulMON+Type	the trainings of MulMON0 and MulMON+Optimizer	AdamINITIAL LEARNING RATE η0	2e-4Batch size	8LEARNING RATE AT STEP s	?max{0.1ηo + 0.9ηo ∙ (1.0 — s∕1e6), 0.1ηo}Total gradient steps	600kGradient-norm clipping	5.0inference iterations (Greff et al., 2019)	5log-normal likelihood strength	1.0KL (GAUSSIAN PRIOR) STRENGTH β	1.0L-NMS (IODINE+ only) strength	1.0	? : SAME SCHEDULER AS GQNS’.	Model Architecture Specifications As discussed in the main paper, we use three existing Comp-VAE models as our baselines and build our contributions on top of these architectures. Itis important11Under review as a conference paper at ICLR 2021to use the same architectures as the that of the original papers. However, we found it difficult to usea latent dimension of 64 as that of (Greff et al., 2019) for the CLEVR-based datasets as it trains tooslow, over one week for one run on two RTX2080TI, we thus reduce the dimension of IODINE to
