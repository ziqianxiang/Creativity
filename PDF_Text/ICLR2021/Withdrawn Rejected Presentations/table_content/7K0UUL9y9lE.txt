Table 1: Time/memory complexity of self-attention and YOSO-attention in forward/backward computation6Under review as a conference paper at ICLR 2021Figure 3: (a) The left two plots are results on MLM and SOP for 512 sequence length. We report MLMvalidation perplexity and SOP validation loss for each 2K training steps. (b) The middle two plots are resultsfor MLM and SOP when using different number of hashes on validation. Since the runtime of YOSO-Attentionis linear with respect to the number of hashes, these two plot directly reflect the equivalent relation betweenperformance vs inference time. (c) The right two plots are results on on MLM and SOP for 4096 sequencelength. YOSO-x means the model is pretrained with YOSO-Attention using x hashes with E being expectation.
Table 2: Dev set results on GLUE tasks. We report F1 score forMRPC and QQP and accuracy for others. YOSO-x means the sameas in Figure 3 and YOSO-x-E means that YOSO-x is finetuned ondownstream tasks using expectation.
Table 3: Dev set results on SST-2, QQP, andMNLI. We report both F1 score and accuracyQQP and accuracy for others. Reformer-x: Re-former using HuggingFace implementation (Wolfet al., 2019) using x hashes. Longformer-x: Long-former using HuggingFace implementation (Wolfet al., 2019) using sliding window of size x.
Table 4: The reported values pertain to a single instance. Time is estimated by averaging total runtime andthen dividing it by batch size, while memory is measured by dividing total memory consumption by batch size.
