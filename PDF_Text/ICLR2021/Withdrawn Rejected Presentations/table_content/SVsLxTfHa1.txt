Table 1: Summary of the main notationsNotation Descriptiontk	k-th token in sentencetk,s	s-th sense of tkk0	index of token tk in vocabularyL	number of LSTM/Transformer layersV	size of vocabularyS	maximum number of senses per tokenhk,j	contextual representation of token tk in layer jhk*,L	contextual representation used in softmaxfunction for predicting tkvi	i-th word in vocabularyvi,s	s-th sense of viwi	output embedding of viwi,s	context-dependent output embedding(i.e. sense vector) of vi,sci,s	sense cluster center of vi,sCi	sense cluster centers of vid	dimension of contextual representationsP	projection matrix for dimension reduction
Table 2: Word sense disambiguation (F1 scores)Model	SE2	SE3	SE07	SE13	SE15ELMo	0.555	0.576	0.446	0.544	0.538SaELMo (ours)	0.575	0.586	0.470	0.560	0.583BERT-Tiny	0.596	0.539	0.466	0.536	0.572SaBERT-Tiny (ours)	0.611	0.546	0.446	0.550	0.5794.2	Experiments using bilingual modelsTo verify the effectiveness of our cross-lingual framework, we implement the bilingual models ontop of ELMo, named Bi-SaELMo that does not use linear projection for further alignment andBi-SaELMo+Proj that uses the linear projection. Sense vectors and cluster center vectors arenot shared between the forward and backward language models. We use ELMo+Proj and Joint-ELMo+Proj as our baseline models, where ELMo+Proj is proposed by Schuster et al. (2019) andJoint-ELMo+Proj is implemented following the framework recently proposed by Wang et al. (2020).
Table 3: Zero-shot cross-lingual NER (F1)Model	de	es	zhELMo	16.30	16.14	0.28Joint-ELMo	56.49	58.91	53.47ELMo+Proj (Schuster et al., 2019)	69.57	60.02	63.15Joint-ELMo+Proj (Wang et al., 2020)	71.59	65.19	59.08Bi-SaELMo (ours)	63.83	60.65	55.83Bi-SaELMo+Proj (ours)	72.19	65.86	63.44For references, but not our baselines, since they are		trained on	muchlarger datasets and/or parallel sentences.			XLM Finetune (ConneaU & Lample, 2019)	67.55	63.18	-XLM-R Finetune (Conneau et al., 2019)	71.40	78.64	-M-BERT Finetune (Pires et al., 2019)	69.74	73.59	-M-BERT Finetune (WU & Dredze, 2019)	69.56	74.96	-M-BERT Finetune+Adv (Keung et al., 2019)	71.90	74.30	-M-BERT Feature+Proj (Wang et al., 2020)	70.54	75.77	-NER tasks. Our sense-aware pretraining makes sense distributions of two languages more isomorphic,which further improves linear projection performance. Our model Bi-SaELMo+Proj demonstratesconsistent performance improvement in all the three languages. Moreover, our model outperformsfinetuned XLM/XLM-R and Multilingual BERT on German data, and achieves state of the art even
Table 4: Zero-shot sentiment classification accuracyModel	de			jp			books	music	dvd	books	music	dvdELMo	52.94	63.61	57.78	50.37	51.59	54.32Joint-ELMo	71.72	75.22	64.25	66.64	68.50	58.54ELMo+Proj (Schuster et al., 2019)	49.92	50.29	49.94	50.57	49.59	50.65Joint-ELMo+Proj (Wang et al., 2020)	75.74	72.25	72.25	62.50	59.77	57.65Bi-SaELMo (ours)	77.46	75.32	74.97	68.16	69.48	64.04Bi-SaELMo+Proj (ours)	70.84	66.25	68.99	62.17	55.91	61.57Table 5: Zero-shot XNLI accuracyModel	de	es	zhELMo	34.07	33.41	35.77Joint-ELMo	60.12	63.73	57.82ELMo+Proj (Schuster et al., 2019)	55.51	58.92	53.17Joint-ELMo+Proj (Wang et al., 2020)	63.33	64.71	58.34Bi-SaELMo (ours)	60.98	62.75	60.40Bi-SaELMo+Proj (ours)	64.77	65.05	60.44Zero-shot cross-lingual natural language inference (XNLI) We use XNLI (Conneau et al., 2018)and MultiNLI (Williams et al., 2018) data for evaluation on this task. The Bi-LSTM baseline model5was trained on MultiNLI English training data, and then evaluated on XNLI German, Spanish,
Table 5: Zero-shot XNLI accuracyModel	de	es	zhELMo	34.07	33.41	35.77Joint-ELMo	60.12	63.73	57.82ELMo+Proj (Schuster et al., 2019)	55.51	58.92	53.17Joint-ELMo+Proj (Wang et al., 2020)	63.33	64.71	58.34Bi-SaELMo (ours)	60.98	62.75	60.40Bi-SaELMo+Proj (ours)	64.77	65.05	60.44Zero-shot cross-lingual natural language inference (XNLI) We use XNLI (Conneau et al., 2018)and MultiNLI (Williams et al., 2018) data for evaluation on this task. The Bi-LSTM baseline model5was trained on MultiNLI English training data, and then evaluated on XNLI German, Spanish,Chinese test data. We report the average zero-shot XNLI accuracy of 2 runs in Table 5. Our modelsshow consistent improvements over the baselines on all of the three data sets. For zero-shot transferto Chinese, both of our models outperform the best baseline by more than 2 points, which againdemonstrates the effectiveness of our framework on distant language pairs.
Table 6: Monolingual model hyperparameters: SaELMoHyperparameterValuemax_word_lengthbatchjsizen_gpusbidirectionalChar-Cnn:embedding:dimChar-Cnn:maX-CharaCterS _per_tokenChar-Cnn:n-CharaCterSchar_cnn:n_highwaydropoutlstm:CelLCliPlstm:dimlstm:n」ayerslstm:Proj-CliPlstm:projeCtion-dimlstm:USe-SkiP-ConneCtionSall_clip_norm_valn_epochs
Table 7: Monolingual model hyperparameters: SaBERT-TinyHyperparameterValueattention_probs_dropout_probdireCtionalityhidden_acthidden_dropout_probhiddenjsizeinitializer_rangeintermediate_sizemax-position_embeddingsnum_attention_headsnum_hidden_layersPoolerJcjsizepooler_num_attention_headspooler_num_fc Jayerspooler jsize_per_headpooler_typetype_VoCabjSizeVoCab-Size
Table 8: Bilingual model hyperparameters: Bi-SaELMo/Bi-SaELMo+ProjHyperparameter	Valuemax_word」ength	50batch_size	256n_gpus	2bidirectional	TrueChaJcnn:embedding:dim	16char_cnn:max_characters-PerJoken	50char_cnn:n_characters	261char_cnn:n_highWay	2dropout	0.1lstm:cell_clip	3lstm:dim	4096lstm:n_layers	2lstm:proj_clip	3lstm:prCjection_dim	512lstm:use_skip_connections	Trueall_clip_norm_val	10.0n_epochs	6UnrolLsteps	12
