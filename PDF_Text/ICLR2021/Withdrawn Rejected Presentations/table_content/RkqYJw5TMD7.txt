Table 1: Adversarial robustness under transfer attacks with '∞ attacks. For MNIST We use perturbationbudget 0.3. For MNIST-M we use perturbation budget 8/255 (we found robustness degrades fast on MNIST-M, even for adversarially trained models, so We choose this number to illustrate relative performance). For allCIFAR experiments We use perturbation budget 8/255. For transfer attacks, DANN remained robust (albeit ina Weaker threat model). Note that the numbers on adversarial training are only provided to qualitatively indicatethe robutness level of DANN. The main goal is to provide evidence for separation.
Table 2: Adversarial robustness under FPAMLL	adaptiveattacks, with F = AdvT (i.e., the attacker initializes the initialmodel as the adversarially trained target model). DANN exhibitssignificant robustness even for large k.
Table 3: Adversarial robustness under J-FPAMLL	adaptive attacks for CIFAR10 and CIFAR10c-fogtasks. For FPAMLLDANN We use k = 20 (early stopping according to Table 2). For J-FPAMLLDANN , We usek = 100. Joint FPAM is by far our most effective for DANN, though DANN still achieves non-trivial test timerobustness, and demonstrate a separation betWeen maximin and minimax threat models.
Table 4: Accuracy of adversarially trained models in the minimax threat model. For the Nat. testacc column we do not use the adversary A. Note that for the MNIST-M row it is the case that wedirectly do adversarial training on the clean labeled target data. This cannot be achieved in our threatmodel because defender only sees perturbed unlabeled target data. Nevertheless, this row gives thebest adversarial accuracy one can hope for.
Table 5: Transfer attacks ('∞ attack) with different corruption levels.
Table 6: Transfer attacks (`2 attacks with = 80/255) for all tasks.
