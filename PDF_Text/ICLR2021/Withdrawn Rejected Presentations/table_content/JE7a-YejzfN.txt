Table 1:	CNN, IMDb dataset: Unlike the difficult examples (larger eigenvalue), word substitutions areineffective in changing the classifier output for the easier examples (smaller eigenvalue). In difficultexamples synonym or change of name, changes classifier label. In easy examples, despite multiplesimultaneous antonym substitutions, the classifier sentiment does not change.
Table 2:	CNN, Counterfactual Examples: In difficult examples (larger eigenvalue), individualsynonym/antonym substitutions are effective in changing the classifier output. In easy examples(smaller eigenvalue) multiple antonym substitutions simultaneously have no effect on the classifieroutput.
Table 3: Statistics of difference in largest FIM eigenvalue (pre and post perturbation) for counterfactualexamples and contrast sets.
Table 4:	Perturbing an easy example on the IMDb dataset: The first row represents original sentence.
Table 5:	Perturbing a difficult example on the IMDb dataset: The first row represents originalsentence. Unlike easy examples, difficult examples tend to have a mixture of positive and negativetraits. Furthermore, a minor perturbation like removal of a sentence (row 1) or substitution of a word(row 2) causes a significant drop in FIM eigenvalue. The small FIM score indicates that the perturbedreview is not difficult for the classifier.
Table 6:	BERT: Difficult Examples change sentiment with a single word substituted. Easy examples,however, retain positive sentiment despite multiple substitutions of positive words with negativewords.
Table 7:	Counterfactual perturbations cause reduction in difficulty: The original example is moredifficult than the perturbed counterfactual example because of strong giveaway words like "amazing".
Table 8:	Difficult examples on the IMDB counterfactual dataset. Here the first row is the originalsentence and the next row is the counterfactual sentence. The counterfactual sentence is easier thanthe original sentence. Note the original example relied more on words like “unlucky”, “doesn’t”,and “absolutely” during classification. “unlucky” and “doesn’t” are associated with more negativesentences and thus the counterfactual example is much easier for the model along with very negativewords like “terrible” and “boring”.
Table 9:	Difficult examples on the IMDB counterfactual dataset. Here the first row is the originalsentence and the next row is the counterfactual sentence. Mix of positive and negative words makethe Sentences difficult for the model.
Table 10:	Difficult examples on the IMDB contrast dataset. Here the first row is the original sentenceand the next row is the contrast set sentence. The contrast set sentence is easier than the originalsentence. The difficulty in the first sentence is due to words like “irresponsible” and “sloppy”. Thusthe negative sentence is much easier for the model.
Table 11:	Easy and difficult examples in Contrast SetsPerturbed sen- Word substitutionstimentNegative →Negativeeasy example(λmax =0.034)Positive →Negative diffi-cult example(λmax =4.132)Negative →Positive diffi-cult example(λmax =3.98)This is a pathetic → excellent political satire. No wonder why it was largelyignored in the U.S.: it ridicules our foreign policy and misrepresents what itreally is.<br /><br />Another bad → good film from this era, Rendition, washowever totally dismissed simply because it showed, accurately, that the U.S.
Table 12:	Difficult examples for BERT. Sensitive to single word substitutions. In example 1, eachword WaS SUbStitUted one at a time.
