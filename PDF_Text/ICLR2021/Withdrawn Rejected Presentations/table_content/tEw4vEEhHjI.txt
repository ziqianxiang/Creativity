Table 1: Performances of RGPRs compared to their respective base methods on the detection offar-away outliers. Error bars are standard deviations of ten trials. For each dataset, best values overeach vanilla and RGPR-imbued method (e.g. LLL against LLL-RGPR) are in bold.
Table 2: OOD data detection. Datasets in bold face are the in-distribution datasets.
Table 3: Confidence over test sets (i.e. α = 1) in term of MMC. Values are averaged over ten trials.
Table 4: Regression far-away outlier detection. Values correspond to predictive error bars (averagedover ten trials), similar to what shades represent in Figures 1 and 2. “In” and “Out” correspond toinliers and outliers, respectively.
Table 5: The corresponding predictive performance to Table 4 in terms of the RMSE metric. Valuesare averaged over ten trials. Smaller is better.
Table 6: OOD data detection results using the hyperparameter tuning objective in (12). All valuesare averages and standard deviations over 10 trials.
Table 7: OOD data detection results using the hyperparameter tuning objective in (12) on DenseNet-BC-121 network. All values are averages and standard deviations over 10 trials.
Table 8: Calibration performance of RGPR on DenseNet-BC-121. Values are expected calibra-tion errors (ECEs), averaged over ten prediction runs. RGPR makes the base BNN (LLL) morecalibrated—even more than the “gold standard GP” in BNO.
Table 9: Optimal hyperparameter for each layer (or residual block and dense block for ResNet andDenseNet, respectively) on LLL.
Table 10: Comparison between Deep Ensemble (DE) and LLL-RGPR in terms of AUR. Results forDE are obtained from (Meinke & Hein, 2020) since we use the same networks and training protocol.
