Table 1: Confidence and accuracy metrics on the validation set of ImageNet with all the objectsremoved using bounding box annotation provided by Choe et al. (2020). Our approach has thebest performance under total uncertainty. ‘ACC’, ‘A.conf’, ‘O.conf’ and ’U.conf’ refer to accuracy,average confidence, mean overconfidence, and mean underconfidence scores. High underconfidenceand low overconfidence point to minimal reliance on context when no pertinent objects are in thegiven image. The last row of figure 6 in appendix provides a qualitative example.
Table 2: Classification and calibration results with ImageNet using ResNet-50. For a detailedexplanation of the metrics please refer to 4.2.‘O.conf’ and ’U.conf’ refer to overconfidence andunderconfidence scores. Our approach produces low overconfidence values. We use the single objectversion (mask) of ImageNet with 0.528M samples to train all the models below.
Table 3: Classification and calibration results with ImageNet using ResNet-101. For a detailedexplanation of the metrics please refer to 4.2.‘O.conf’ and ’U.conf’ refer to overconfidence andunderconfidence scores.
Table 4: Fine-tuning on MS COCO using FRCNN for object detection using ResNet-50 backbone.
Table 5: Classification and calibration results with ImageNet. For a detailed explanation of themetrics please refer to section 4.2 in the main paper. ‘A.conf’, ‘O.conf’ and ’U.conf’ refer to averageconfidence, overconfidence, and underconfidence scores. We provide ECE values for 100 bins and 15bins mean scores along with their standard deviation (std).
Table 6: Classification and calibration results with OpenImages. For a detailed explanation of themetrics please refer to section 4.2 in the main paper. ‘A.conf’, ‘O.conf’ and ’U.conf’ refer to averageconfidence, overconfidence, and underconfidence scores. We provide ECE values for 100 bins and 15bins mean scores along with their standard deviation (std).
Table 7: Fine-tuning on COCO using FRCNN for object detection. For a detailed explanation of theresults please refer to section 4.3 in the main paper. AP refers to average precision and AR refers toaverage recall at the specified Intersection over union (IoU) level. We also provide AP values forsmall, medium, and large objects using ‘S’, ‘M’, and ‘L’ respectively.
