Table 1: Comparison to prior work.		Leveraging baseline policies for RL. Prior work has used baseline policies to provide initial infor-mation to RL algorithms to reduce or avoid undesirable situations. This is done by either: initializingthe policy with the baseline policy (Driessens & Dzeroski, 2004; Smart & Kaelbling, 2000; KoPPejan& Whiteson, 2011; Abbeel et al., 2010; Gao et al., 2018; Le et al., 2019; Vecerik et al., 2017; Jaqueset al., 2019); or providing a teacher,s advice to the agent (Garcia & Fernandez, 2012; QuintIa Vidalet al., 2013; Abel et al., 2017; Zhang et al., 2019). However, such works often assume that thebaseline policy is constraint-satisfying (Sun et al., 2018; Balakrishna et al., 2019). In contrast, ourSPACE algorithm safely leverages the baseline policy without requiring it to satisfy the specifiedconstraints.
Table 2: Parameters used in all tasks. (PC: point circle, PG: point gather, AC: ant circle, AG: antgather, Gr: grid, BN: bottleneck, and CR: car-racing tasks)	PCPO		SPACE (Ours)		f-PCPO		f-CPO		d-PCPO		d-CPO		M/C	Time	M/C	Time	M/C	Time	M/C	Time	M/C	Time	M/C	TimePG	B	22.14	B	25.2	B	31.9	B	25.5	B	32.8	B	32.6PC	B	35.1	B	51.2	B	48.4	B	49.4	B	55.5	B	55.9AG	B	386.9	B	110.5	C	268.6	C	235.1	B	138.2	B	187.5AC	B	148.9	B	94.0	C	222.6	C	214.6	B	177.4	B	151.2Gr	A	105.3	A	91.4	A	88.2	A	58.7	A	116.8	A	115.3BN	A	257.7	A	181.1	A	162.9	A	161.6	A	259.3	A	275.6CR	C	993.5	C	971.6	C	1078.3	C	940.1	C	1000.4	C	981.0Table 3: Real-time in seconds for one policy update for all tested algorithms and tasks. (PC: pointcircle, PG: point gather, AC: ant circle, AG: ant gather, Gr: grid, BN: bottleneck, and CR: car-racingtasks)We use GAE-λ approach (Schulman et al., 2016) to estimate AπR(s, a), AπC (s, a), and AπD(s). Forthe simulations in the gather, circle, and car-racing tasks, we use neural network baselines with thesame architecture and activation functions as the policy networks. For the simulations in the grid andbottleneck tasks, we use linear baselines. The hyperparameters of all algorithms and all tasks are inTable 2.
Table 3: Real-time in seconds for one policy update for all tested algorithms and tasks. (PC: pointcircle, PG: point gather, AC: ant circle, AG: ant gather, Gr: grid, BN: bottleneck, and CR: car-racingtasks)We use GAE-λ approach (Schulman et al., 2016) to estimate AπR(s, a), AπC (s, a), and AπD(s). Forthe simulations in the gather, circle, and car-racing tasks, we use neural network baselines with thesame architecture and activation functions as the policy networks. For the simulations in the grid andbottleneck tasks, we use linear baselines. The hyperparameters of all algorithms and all tasks are inTable 2.
