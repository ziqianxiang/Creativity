Table 1: Comparisons against state-of-the-art distillation methods. For a fair comparison, all students are 6/768BERT models, distilled by BERTBASE (12/768) teachers. Other results are as reported by their authors. Resultsof development set are averaged over 5 runs and the best one of them are used for test server evaluation. “-”means the result is not reported.
Table 2: Comparisons against state-of-the-art distillation methods on the SQuAD 1.1v dataset (EM/F1 on devset). For a fair comparison, all students are 6/768 BERT models, distilled by BERTBASE (12/768) teachers. Theresults of PKD and TinyBERT are as reported by Jiao et al. (2019) and the result of DistilBERT is as reportedby the author (Sanh et al., 2019).
Table 3: Experimental results with the recently proposed MobileBERT. Following the author, a base-line is trained without distillation for downstream tasks. Results are averaged over 5 runs on thedevelopment set.
Table 4: Experiment results on the effect of model sizes for CKD and TinyBERT results are as re-ported in their papers. The results are evaluated on the test set of GLUE official benchmark. Detailedcomponent of model sizes are provided in the appendix C. f marks our runs with the official code.
Table 5: Ablation study using the subset of GLUE about the impactof each component of CKD.
Table 6: Overview of distillation objectives used for language model compression and their con-straint on architecture. Although TinyBERT acquires flexibility on the embedding size, using anadditional parameter, for attention matrices matching, the number of attention heads of the teacherand student must be the same. Although Sun et al. (2020) experiment with various sizes of models,they train the modified teacher architecture to distill the word representation and attention matrix.
Table 7: Details of architecture used for our experimentsModel	Layers	Hidden Size	Feed-forward Size	Attention Heads	Model SizeBERTBASE	-12^-	-^768^^	3072	12	110.1MBERTSMALL	4	^^512^^	2048	8	29.1MMobileBERT (Sun et al., 2020)	-24^^	^^512^^	512	4	25.3MBERTSHALLOW	-12^^	-^256^^	1024	4	17.6MTinyBERT (Jiao et al., 2019)	4	^^312^^	1200	12	14.5MBERTMINI	6	256	1024	4	12.5MIn this section, we describe the various models used in our experiments. Since pre-training eachmodel of various sizes costs a lot, pre-trained models of various sizes provided by PD (Turc et al.,2019) are used for our experiments. In addition, MobileBERT introduce the Inverted-BottleneckBERT. Excluding BERTBASE, a total of 3 student models are used, and each model name can bedifferent from Turc et al. (2019). Table 7 summarizes the number of layers, hidden size, feed-forwardsize, attention heads, and model size of each model.
