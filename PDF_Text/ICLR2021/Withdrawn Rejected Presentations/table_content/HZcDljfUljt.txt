Table 1: Top1 Accuracy comparison between pruned and no pruned modelsModel	Activation bitwidth	Evaluation	original	PfQ (w/o bias correction)	PfQMobileNetV1	4	Accuracy MAC	76.34 510M	76.67 367M	76.75 367MMobileNetV1	3	Accuracy MAC	74.17 510M	75.14 367M	75.26 367MMobileNetV2	4	Accuracy MAC	74.04 147M	74.67 93M	75.00 93MMobileNetV2	3	Accuracy MAC	71.36 147M	72.17 93M	72.28 93MThe experimental results are summarized in Table 1. Regarding the accuracy in Table 1, Table 1shows that the fine-tuning after using PfQ gives better performance in all experiments, and it can besolved that the bad effect on the fine-tuning of the quantized DNN described in 3.1. In particular,in PfQ (w/o bias correction), the accuracy is higher than that of the original, and the difference islarger for 3 bits than 4 bits, indicating that the influence of the quantization error (i.e. the 4th term in(16)) in the backward calculation is large. MobileNetV1 and MobileNetV2 in the columns of PfQin Table 1 are pruned with the weights by 31.86% and 35.74%, respectively. Thus, the number ofMACs is also reduced.
Table 2: Effect of proposed quantization workflowModel	Bitwidth (Activation/Weight)	Evaluation	fine-tune once (w/ PfQ)	Algorithm 1MobileNetV1	A4/W4	Accuracy	74.69	75.31		MAC	367M	367MMobileNetV1	A3/W3	Accuracy	71.56	70.85		MAC	367M	367MMobileNetV2	A4/W4	Accuracy	72.78	72.7		MAC	93M	92.4MMobileNetV2	A3W3	Accuracy	66.29	68.96		MAC	93M	93MTable 2 shows that Algorithm 1 is better. Although the performance of Algorithm 1 is lower thanthat of the fine-tuning once with PfQ for A3W3 of MobileNetV1 and A4W4 of MobileNetV2, thedifference is small, and Algorithm 1 which can often reduce the MAC is considered to be better.
Table 3: Top1 Accuracy comparison CIFAR-100Model	Bitwidth (Activation/Weight)	Evaluation	DFQ (w/ fine-tune)	OursMobileNetV1	A4/W4	Accuracy	71.57	75.31		MAC	510M	367MMobileNetV1	A3/W3	Accuracy	61.58	70.85		MAC	510M	367MMobileNetV2	A4/W4	Accuracy	72.14	72.7		MAC	147.2M	92.4MMobileNetV2	A3/W3	Accuracy	68.05	68.96		MAC	147.2M	93.0Mfor MobileNetV2, and DFQ. Next, the experimental settings are described. The experiment of theimagenet1000 was learned using 4 GPUs (tesla v100 x4), the optimizer was momentum SGD, themoment was 0.9, and the weight decay was 0.00001. The pre-trained model was learned by thefollowing settings. The number of epochs was 120, the batch size was 256, the learning rate settingsin (19) were l-1 = 0.256, w = 16, Î» = 104. The settings for the fine-tuning of the quantizedDNN are described below. In our method, in addition to the above settings, the batch size was 64,and the initial learning rate was l-1 = 0.01 in the first fine-tuning for quantized activations only inAlgorithm 1. In the second fine-tuning for quantized activations and weights in the Algorithm 1, theinitial learning rate was l-1 = 0.0001. In the experimental settings of DFQ, the batch size was 128,the initial learning rate was l-1 = 0.0001.
Table 4: Top1 Accuracy comparison imagenet1000Bitwidth (Activation/Weight)	Evaluation	DFQ (w/ fine-tune)	PACT	DSQ	OursA4/W4	Accuracy	57.44 =	61.39	64.8	67.48	MAC	313M	313M	313M	261MThe experimental results are summarized in Table 4, which shows that the quantization performanceof the proposed method is the best compared to the other quantization methods. In our approach,2 PfQ operations in the proposed quantization workflow pruned the weights by 10.29%. Thus, thenumber of MACs was also reduced.
Table 5: Using Accuracy DropModel	Bitwidth (ACtivationWeight)	Evaluation	DFQ	OursMobileNetV2	A4/W4	ACCuraCy	70.00	71.58		MAC	147.2M	92.4MA.2 Visualize Low Variance ChannelsWe claimed in subsection 3.1 that the outputs of some channels whose running variance close tozero are the constant values. In this subsection, we confirmed the fact and the result was Figure 3.
