Table 1: Number of gradients from positive and negative distance pairs contributing to the loss within a batchfor NCA and PNS. Respectively, w, n and m represent the number of ways, shots and queries. In Appendix A.8we show that the extra number of pairs NCA can exploit grows as O(w2 (m2 + n2)).
Table 2: Comparison between the different evaluation methods discussed in Sec. 3.4.
Table 3: Comparison of methods that use ResNet12 on miniImageNet and CIFAR-FS (test set).
Table 4: Comparison of methods that use ResNet12 on tieredImageNet (test set).
Table 5: Comparison between vanilla NCA, NCA using multiple evaluation layers and NCA performingoptimisation on the support set (ss). The NCA can only be optimised in the 5-shot case, since there are notenough positives distances in the 1-shot case. Support set is optimised for 5 epochs using Adam with learningrate 0.0001 and weight decay 0.0005. For details, see Sec. A.1A.3 Differences between the NCA and contrastive lossesEq. 3 is similar to the contrastive loss functions (Khosla et al., 2020; Chen et al., 2020a) that areused in self-supervised learning and representation learning. The main differences are that 1.) Incontrastive losses, the denominator only contains negative pairs and 2.) the inner sum in the nu-merator is moved outside of the logarithm in the supervised contrastive loss function from Khoslaet al. (2020). We opted to work with the NCA loss because we found it performs better than thesupervised constrastive loss in a few-shot learning setting. Using the supervised contrastive loss weonly managed to obtain 51.05% 1-shot and 63.36% 5-shot performance on the miniImagenet testset.
Table 6: Comparison of results on validation set of miniImageNet and CIFAR-FS using the hyperparametersused in SimpleShot(Wang et al., 2019) and the hyperparameters used in this paper (ours). Results are on batchsize 256 (as used in Wang et al. (2019)) with the PNS episodic batch being a=16, as it is the best performingepisodic setup we found.
Table 7: Number of positives and negatives used in the batch size 512 experiments of Fig. 2.
