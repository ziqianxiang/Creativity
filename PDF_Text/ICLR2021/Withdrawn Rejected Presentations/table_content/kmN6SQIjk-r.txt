Table 1: Table of AUC improvements achieved by MoCo-pretrained models against models withoutMoCo-pretraining on the CheXpert dataset4.2	Transfer Performance of end-to-end MoCo-pretrained Models onCheXpertWe investigate whether MoCo-pretraining translates to higher performance for models fine-tunedend-to-end. We visualize the performance of the MoCo and ImageNet-pretrained end-to-end modelsat different label fractions in Figure 3.
Table 2: Performance of MoCopretrained ResNet18 on Shen-zhen dataset at different pretrain-ing learning rates with 100% labelfraction.
Table 1: Data table corresponding to Figure 2 and Figure 3. AUC of models trained to detect pleuraleffusion on the CheXpert dataset.
Table 2: Data table corresponding to Figure 4. AUC of models trained to detect tuberculosis on theShenzhen dataset.
Table 3: AUC improvements achieved by MoCo-pretrained models against ImageNet-pretrainedmodels on the Shenzhen tuberculosis task.
