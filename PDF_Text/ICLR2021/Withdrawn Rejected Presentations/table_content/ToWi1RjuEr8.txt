Table 1: Final returns for different algorithms on the OpenAI Gym tasks, with Â± corresponding toone standard deviation of the average return across 10 random seeds. In terms of final performance,AWR is generally competitive with prior methods.
Table 2: Performance of algorithms on the motion Figure 4: Learning curves on motion imitationimitation tasks. Returns are normalized between tasks. On these challenging tasks, AWR generallythe minimum and maximum possible returns. learns faster than PPO and RWR.
