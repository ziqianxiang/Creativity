Table 1: The results on D4RL tasks compare BCQ (Fujimoto et al., 2019) with and without OVR, bootstrappingerror reduction (BEAR) (Kumar et al., 2019), behavior-regularized actor critic with policy (BRAC-p) (Wu et al.,2019a), AlgeaDICE (aDICE) (Nachum et al., 2019b) and offline SAC (SAC-off) (Haarnoja et al., 2018). Theresults presented are the normalized returns on the task as per Fu et al. (2020) (see Table 3 in Fu et al. (2020) forthe unnormalized scores on each task). We see that in most tasks we are able to significant gains using OVR. Ouralgorithm can be applied to any policy optimization baseline algorithm that trains the policy by maximizing theexpected rewards. Unlike BCQ, BEAR (Kumar et al., 2019) does not have the same objective, as they train thepolicy using and MMD objective.
Table 2: The results on D4RL tasks compare BCQ (Fujimoto et al., 2019) with and without OVR, bootstrappingerror reduction (BEAR) (Kumar et al., 2019), behavior-regularized actor critic with policy (BRAC-p) (?),AlgeaDICE (aDICE) (Nachum et al., 2019b) and offline SAC (SAC-off) (Haarnoja et al., 2018). The resultspresented are the normalized returns on the task as per Fu et al. (2020) (see Table 3 in Fu et al. (2020) for theunnormalized scores on each task).
Table 3: Results on the Safety-Gym envi-	I	PointGoalI	∣		PointGoal2	ronments Ray et al.. We report the mean	I	Reward	Cost I	Reward	Costand S.D. of episodic	BCQ	I	43.1 ± 0.3	137.0 ± 3.6 I	32.7± 0.7	468.2 ± 9.1returns and costs over five random seeds and	BCQ+OVR I	44.2 ± 0.3	127.1 ± 4.0 I	33.2 ± 0.7	453.9 ± 7.31 million timesteps. The goal of the agent is to maximize the	I	PointButtonI	∣		PointButton2		I	Reward	Cost I	Reward	Costepisodic return, while minimizing the cost in-	BCQ	I	30.9 ± 2.2	330.8 ± 8.3 I	18.1 ± 1.1	321.6 ± 4.1curred.	BCQ+OVR I	30.7 ± 2.3	321.5 ± 6.8 I	19.6 ± 1.0	305.7 ± 6.1and optimizing the resulting objective J (θ, ω). We further propose an overall policy optimizationobjective, where a single objective can be used for estimating the distribution ratio, evaluating thecritic and optimizing the resulting objective. We can write the off-policy optimization objective withits equivalent starting state formulation, such that we have :EdD (s,a) [ω∏θ∕D (s, a) ∙ r(s, a)] = (1 - Y )ES0~βo(s),a0~n(∙∣S0)Qπ(s0,(66)Furthermore, following Bellman equation, we expect to have E[r(s, a)] = E[Qn (s, a) — γQn (s0, a0)]EdD(s,a)[ω∏θ∕D(s,a)∙{Q (s,a)-YQ (s , a )}] = (1 -Y)Es0~βo(s),a0~n(∙∣so) [Q (s0,aθ)] (67)We can therefore write the overall objective as :J(ω,πθ,Q) = EdD(s,a) [ω∏θ/D(s,a) ∙{Qn(s,a) - YQn(s0,a0)}]-(I- Y)Es0~βo(s),a0~n(∙∣s0) [Q (s0,a。)] (68)
