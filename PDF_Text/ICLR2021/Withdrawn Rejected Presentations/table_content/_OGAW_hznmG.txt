Table 1: For n = 60 and zk = k (k = 1, ..., n) we show the average KL-distance (4) for vari-ous estimators. The full affine symmetry (24) holds for all shown probabilities. M is the lengthof sample (1). The initial prior Dirichlet density (5) holds (25) with ɑk = 2. Eq. (18) equalshK [q, q[0]]i = 0.225, i.e. values of the average KL-distance larger than 0.225 are meaningless.
Table 2: The same as in Table 1, but for αk = α = 0.1 in (25). Eq. (18) gives (K[q, q[0]]i = 1.798,i.e. values of the average KL-distance larger than 1.798 are meaningless.
Table 3: The same as in Table 1, but the initial prior density is a Dirichlet mixture given by (12, 30)with α0 = 0.3 and = 1.1. The average KL-distance hK[q, q[0]]i for the trivial estimator (17) equals0.212, i.e. values of the average KL-distance larger than 0.212 are meaningless; cf. (18). (KBayesiand (KBayesi refer to (14) and (32), respectively. (KML)b=bθφt and (KMLib=I refer to regularizedML estimator (15) under b = 1 and the optimal value of b found from numerically minimizing(Kml). The optimal value bopt of b changes from 2.46 for M = 35 to 2.65 for M = 1. We alsoreport the value of (KMLib=I with a sensible value of b to confirm that if b is not chosen properly,then the corresponding (regularized) ML estimator (15) is meaningless. (Ki) is defined via (4, 20).
Table 4: The same as in Table 3, but for different values of M. HerehK中〉refers to MAXENTestimator (23) with constraints (19, 22). The Bayesian estimator is found from (14, 30) Jor thisrange of sufficiently large M the MAXENT estimator (23) performs better than (23): hK 1+2i <hK 1i < hKMLib=bopt.______________________________________M	hK Bayesi	hK MLib=bopt	(K ML ib=1	(K ii	(K 1+2i45	0.015	0.172	0.196	0.045	0.04265	0.014	0.157	0.180	0.042	0.03585	0.014	0.145	0.164	0.040	0.031241	0.013	0.087	0.091	0.038	0.0245.2	Mixture of Dirichlet densitiesFor modeling more complex types of prior information about the unknown probabilities {qk}kn=1,we shall assume that the prior density is a mixture of two Dirichlet densities; see (12). Relationshqk i = n (k = 1,…，n) will be still kept, since they are necessary for applying MAXENT. Nowwe assume that that there are (prior) conditional rank correlation between the values (z1, ..., zn ) ofZ—ordered as (z1 < ... < zn )—and its probabilities (q1, ..., qn ). For one component of the mixture,the probabilities (q1, ..., qn ) prefer to be ordered as in (q1 < ... < qn ). For another component theytend to be ordered in the opposite way (q1 > ... > qn ). This type of prior knowledge can be modeledvia a mixture (12) of two Dirichlet priors with L = 2, ∏ι = ∏2 = ɪ, andα[11] < ... < α[n1] ,	α[12] > ... > α[n2] ,	(28)[1]	[1]	[2]	[2]
