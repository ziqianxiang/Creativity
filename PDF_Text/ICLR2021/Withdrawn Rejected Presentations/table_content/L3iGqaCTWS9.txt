Table 1: Quantization results using retro-synthesis data for models with and without Batch-Normalization layers on ImageNet dataset with weights and activations quantized to 8-bit (W8A8).
Table 2: Results for quantization method using retro-synthesis data with weights quantized to 6-bitand activations quantized to 8-bit (W6A8) on ImageNet datasetModel	ZeroQ	proposed method	FP32ResNet-18	70.76	7091	71.47Resnet-50	77.22	77.30	77.72MobileNet-V2	70.30	70.34	73.03The efficiency of the proposed quantization method for lower bit precision on the CIFAR-10 datasetfor ResNet-20 and ResNet-56 models is depicted in Table 3 below. From the results, it is evident7Under review as a conference paper at ICLR 2021that the proposed method outperforms the state-of-the-art methods even for lower precision 8, 6, and4 bit weights with 8 bit activations.
Table 3: Results for quantization method using retro-synthesis data with weights quantized to 8, 6,and 4-bit and activations quantized to 8-bit (W8A8, w6a8, and W4A8) on CIFAR-10 datasetW8A8			W6A8		W4A8	Model	ZeroQ	Proposed method	ZeroQ	Proposed method	ZeroQ	Proposed methodResNet-20	93.91	93.93	93.78	93.81	90.87	90.92ResNet-56	95.27	95.44	95.20	95.34	93.09	93.134.2	Results for Hybrid Quantization methodTable 4 demonstrates the benefits of the proposed Hybrid Quantization method in two folds, one isfor accuracy improvement and the other is for the reduction in inference time. From the results, it isobserved that the accuracy is improved for all the models as compared to the per-channel scheme.
Table 4: Results on ImageNet dataset with the proposed Hybrid quantization scheme 2 (W8A8),with threshold Th set to 0 for accuracy improvement and set to 0.001 for inference time benefit.
Table 5: Results with Non-Uniform Quantization method (W8A8) on ImageNet datasetModel/Method	Uniform quantization	Non-Uniform Quantization	FP32ResNet-18	69:70	70.60	71.47ResNet-50	77.1	77.30	77.725	Conclusion and Future scopeThis paper proposes a data independent post training quantization scheme using “retro sysnthesis”data, that does not depend on the Batch-Normalization layer statistics and outperforms the state-of-the-art methods in accuracy. Two futuristic post training quantization methods are also discussednamely “Hybrid Quantization” and “Non-Uniform Quantization” which resulted in better accuracyand inference time as compared to the state-of-the-art methods. These two methods unleashes a lotof scope for future research in similar lines. Also in future more experiments can be done on lowerprecision quantization such as 6-bit, 4-bit and 2-bit precision using these proposed approaches.
