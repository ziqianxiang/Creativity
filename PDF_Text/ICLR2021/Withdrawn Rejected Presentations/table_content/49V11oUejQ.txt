Table 1: Model robustness comparison amongFast Adversarial Training, Adversarial Train-ing, and TRADES, using ResNet-18 model onCIFAR-10 dataset.
Table 2: Performance comparison on CIFAR-10 Table 3: Performance comparison on CIFAR-using ResNet-18 model.
Table 4: Performance comparison on Tiny Ima-geNet dataset using ResNet-50 model.
Table 5: Performance comparison with state-of-the-art robust models on CIFAR-10 evaluated by Au-toAttack and RayS.
Table 7: Sensitivity analysis of the attack stepsize on the CIFAR-10 and CIFAR-100 datasetsTable 6: Sensitivity analysis ofγ on the CIFAR-10 and CIFAR-100 datasets using ResNet-18model.
Table 6: Sensitivity analysis ofγ on the CIFAR-10 and CIFAR-100 datasets using ResNet-18model.
Table 8: Performance comparison on CIFAR-10 using ResNet-18 model combined with cycliclearning rate and mix-precision training.
Table 9: Sensitivity analysis of β on CIFAR-10 and CIFAR-100 datasets using ResNet-18 model.
Table 10:	Performance of using BackwardSmoothing alone on CIFAR-10 dataset usingResNet-18 model.
Table 11:	Performance of using BackwardSmoothing alone on CIFAR-100 dataset usingResNet-18 model.
Table 12: Sensitivity analysis on the number of random points used in Back- ward Smoothing on the CIFAR-10 dataset using ResNet-18 model.	Table 13: Performance comparison on CIFAR- 10 dataset using ResNet-18 model (	= 16/255).				Method	Nat (%) Rob(%) Time (m)		# RandPointS Rob (%) Time(m)	AT	62.76	32.03	425—	Fast AT	53.72	20.12	891	52.50	164	TRADES	62.09	28.63	4702	52.67	204	Fast TRADES	56.55	17.47	1375	52.70	316	Fast TRADES (2-step)	53.36	19.11	16710	52.73	510	Backward Smoothing	63.47	25.04	164as multiple random points but saves more time. Note that our target is to improve the efficiency ofadversarial training, therefore, we only use a single random point for randomized smoothing in ourproposed method.
