Table 1: Zone prediction: Top-1 zone prediction accuracy on the validation walkthrough videos.
Table 2: Downstream task performance at the end of the episode. Gibson-S/L means small/large. MP3D-cat./inst. means categories/instances. All methods are evaluated on three random seeds. See Appendix forperformance vs. time step plots.
Table 3: Impact of noisy video data and a non-learned policy for video generation on EPC self-supervisedlearning.
Table 4: Comparing robustness to sensor noise on downstream tasks in Matterport3D. Note: NF denotes noisefree sensing, N-D denotes noisy depth (and noise-free pose), and N-D,P denotes noisy depth and pose. SeeAppendix G for full results.
Table 5: Hyperparameters for training our RL and self-supervised learning models.
Table 6: Comparing robustness to sensor noise on downstream tasks in Gibson and Matterport3D. Note: NFdenotes noise free sensing, N-D denotes noisy depth (and noise-free pose), and N-D,P denotes noisy depth andpose.
Table 7: Exploring spatial context for self-supervision in EPCas inputs, and predicts the average feature for the next 15 frames conditioned on the pose. We termthis as “EPC-local”. We compare the two EPC variants with SMT (Video) in Tab. 7.
