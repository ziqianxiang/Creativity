Table 1: Mean and standard deviation (w.r.t. last digits, in parentheses) of relative Sinkhorn distanceerror, IoU of top 0.1 % and correlation coefficient (PCC) of OT plan entries across 5 runs. SparseSinkhorn and LCN-Sinkhorn consistently achieve the best approximation in all 3 measures.
Table 2: Accuracy and standard deviation (w.r.t. last digits, in parentheses) across 5 runs for unsuper-vised word embedding alignment with Wasserstein Procrustes. LCN-Sinkhorn improves upon theoriginal by 3.1 pp. before and 2.0 pp. after iterative CSLS refinement. *Migrated and re-run on GPU via PyTorch	Time (s)	EN-ES	ES-EN	EN-FR	FR-EN	EN-DE	DE-EN	EN-RU	RU-EN	Avg.
Table 3: RMSE for GED regression across3 runs and the targets’ standard deviation σ.
Table 4: RMSE for graph distance regression across3 runs. Using LCN-Sinkhorn with GTN increasesthe error by only 10 % and allows log-linear scaling.
Table 5: Graph dataset statistics.
Table 6: Hyperparameters for the Linux dataset.
Table 8: Hyperparameters for the preferential attachment GED dataset.
Table 9: Runtimes (ms) of Sinkhorn approximations for EN-DE embeddings at different dataset sizes.
