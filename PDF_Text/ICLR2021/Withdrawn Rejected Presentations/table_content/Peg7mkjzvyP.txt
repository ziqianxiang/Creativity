Table 1: Comparison of different methods on program accuracy (PA) and supervision extentGenre	Method	Description	PA	Supervision ExtentData-driven	1pSMT	Phrase-based SMT	24.1%	fully supervisedprogram translation	mmpSMT	multi-phase phrase-based SMT	41.7%		Tree2tree	tree-to-tree neural networks	70.1%		TransCoder	weakly-supervised neural translation	49.9%	weakly supervisedCode search system	SourCerer	Lucene-based code search, free-text queries	13.5%	-	CodeHoW	free-text queries	13.5%		PTR 一		71.1%	no labels (directly retrieve translation with input)Variations of IPTR	PTRWORd2VEC	word2vec as queries	67.7%		PTRCODE2VEC	code2vec as queries	63.4%		PTR+QTM		78.6%		IPTRAL	-		87.1%	80 labels for QTM	IPTRFB -		79.3%	At most 1 correction per task	IPTRAL+FB -	the full system	89.6%	combination of IPTRal and IPTRFBTable 2: Comparing program accuracy with Transcoder on GeeksforGeeks dataset	C++-Java	C++-Python	Java-C++	Java-Python	Python-C++	Python-JavaTranscoder	3.1%	6.7%	^^24.7%^^	3.7%	4.9%	0.8%PTR	69.2%	65.3%	70.9%	59.3%	55.4%	54.2%PTR+QTM	76.6%	74.2%	78.1%	68.1%	59.2%	59.6%
Table 2: Comparing program accuracy with Transcoder on GeeksforGeeks dataset	C++-Java	C++-Python	Java-C++	Java-Python	Python-C++	Python-JavaTranscoder	3.1%	6.7%	^^24.7%^^	3.7%	4.9%	0.8%PTR	69.2%	65.3%	70.9%	59.3%	55.4%	54.2%PTR+QTM	76.6%	74.2%	78.1%	68.1%	59.2%	59.6%IPTRAL	87.2%	79.5%	84.8%	72.5%	66.2%	68.8%IPTRFB	75.9%	71.3%	76.5%	64.2%	59.5%	63.3%IPTRAL+FB	84.8%	83.0%	90.5%	77.5%	67.8%	68.1%We further explored the supervision impact on iPTR. In this experiment, we let the user give at mostone correction to each retrieved task. Without optimizing the QTM, iPTRFB slightly improves onPTR+QTM. The same improvement rate can be observed when comparing iPTRAL+FB to iPTRAL,suggesting that AL and FB have independent influence on the results. Compared to the fully super-vised methods 1pSMT, mmpSMT and Tree2tree, iPTR leverages very limited human supervisionto achieve better results. In the first retrieval round, the user does not make correction to any wrongresults, iPTR achieves 87.1% accuracy with only 80 labels for QTM. Also the reproduced weaklysupervised approach TransCoder does not achieve better results than iPTR. Although some of theperformance loss can be contributed to the fact that our training data is not as large as they reportin their paper, we also observed that their model often generates invalid translations with regard togrammar. For example, it often mistakes the input type of a function for different languages. Thisphenomenon is also acknowledged in their own paper and can be attributed to the fact that only
Table 3: The representation of the program from Figure 7Path type	Freq.	Text statisticsP1	1	{“data”:1,“type”:1,“example”:2}P2	2	{”data”:1,”type”:1,”example”:2, ”0.1”:1,”real”:1,”number”:1}P3	1	{”0.1”:1,”real”:1,”number”:1}P4	1	{”data”:1,”example”:1,”0.1”:1}P5	1	{"type”:1,"example”:1,"real”:1,"number”:1}A.3 Comprehensive program representationIn our comprehensive program representation, we not only consider the structural and textual fea-tures but also their dependencies.
Table 4: Statistics of the real-world datasetSiZe Files Lines	MethodS/Functions3.8GB	280,128	75,567,192	2,023,546	―A.5 Experiment on a Large Real-world DatasetDataset. We choose the four programming languages with the most pushes on github - JavaScript,Python, Java, and C++. They are also representative of programming languages with different char-acteristics. Based on the number of stargazers, we pick 1% files in these four languages fromPGA to be our raw dataset. Theoretically, iPTR works for programs of any length, but consideringthe practical value and the intuitiveness of the validation process, we aim to translate programs atmethod level in our experiment. Longer programs whose complete translations are not existing inthe database can be translated by merging translations of each part. We split all the files into meth-ods or functions as the input/output of iPTR. One limitation of PGA is that there are duplicate filesacross different repositories. To ensure the quality of the dataset and increase the efficiency of ourprogram translation task, we gradually remove duplicates at file level by taking hashes of these pro-grams and comparing their hashes. In addition, we also remove the data that cannot be successfullyparsed due to format, errors, version compatibility or other issues. Since our approach does notrequire additional explanation except the program itself, we remove annotations and descriptions.
Table 5: Comparison of different representations(a) JS as source languageRepresentation	Program accuracy			Mean	reciprocal rank					C++	Python	Java	C++	Python	Java			P TRSTRUCTURAL	53.1%	39.8%	46.7%	0.74	0.68	0.73PTRTEXTUAL	48.0%	23.4%	31.7%	0.36	0.42	0.57PTRS+T	56.2%	43.7%	49.9%	0.80	0.71	0.83PTRCODE2VEC	51.0%	32.0%	40.2%	0.53	0.62	0.61PTRWORD2VEC	61.4%	53.9%	60.2%	0.81	0.83	0.89PTR	61.4%	52.3%	59.8%	0.88	0.84	0.89(b) Python as source languageRepresentation	Program accuracy			Mean reciprocal rank			JS	C++	Java	JS	C++	JavaPTRSTRUCTURAL	41.8%	45.5%	43.3%	0.73	0.73	0.75PTRTEXTUAL	39.5%	31.2%	40.9%	0.63	0.58	0.64PTRS+T	44.3%	46.7%	43.8%	0.71	0.73	0.76PTRCODE2VEC	41.9%	43.4%	43.1%	0.66	0.53	0.71PTRWORD2VEC	47.9%	54.7%	48.3%	0.80	0.87	0.79PTR	48.4%	54.2%	47.9%	0.79	0.85	0.77
