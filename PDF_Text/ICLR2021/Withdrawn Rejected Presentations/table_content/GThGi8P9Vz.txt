Table 1: Comparison among stochastic (or online) PGD for solving the problem in Eq. (1).					Algorithm	Non-Convex	Non-Convex	Preconditioner	Momentum	ConvergenCe	Loss	Regularizer			GuaranteeADAGRAD (DuChi et al., 2011)			AdaGrad		✓Ghadimi et al. (2016)	✓		✓		✓Wang etal.(2018)	✓			✓	✓PhametaL(2019)	✓			✓	✓DaVis & Drusvyatskiy (2019)	✓			✓	✓Xu et al. (2019a)	✓	✓			✓Xu et al. (2019b)	✓	✓	AdaGrad		✓Prox-SGD (Yang et al., 2020)	✓		✓	✓	Davis et al. (2020)	✓	✓		✓	✓ProxGen (Ours)	∣	✓	✓	/	✓	✓learning, important instances include network pruning (Wen et al., 2016; Louizos et al., 2018), whichinduces a sparse network structure, and network quantization (Yang et al., 2019; Courbariaux et al.,2015; Bai et al., 2019), which gives hard constraints so that parameters have only discrete values.
Table 2: PROXQUANT versus revised PROXQUANTProxQuant	PrθxαtλR(∙) (θ	t-αt(Ct+δI)-1mtRevised ProxQuant	PrOXCt+RI .)(θ	t-αt(Ct+δI)-1mtθ in an element-wise manner. Using this regularizer, the main difference between PROXQUANTand our ProxGen approach is shown in Table 2. Note that ProxQuant (top in Table 2) does notconsider the effect of preconditioners when computing proximal mappings. Therefore, we revisethe proximal update in ProxQuant by considering preconditioners in proximal mappings withPROXGEN (bottom in Table 2). Moreover, we also propose generalized regularizers motivatedby 'q regularization for 0 < q < 1: Rqbin(θ) = ∣θ - sign(θ)∣q. In terms of theory, Bai et al.
Table 3: Comparison for binary neural networks. The best performance in mean value is highlighted.
Table 4: Preliminary results on revised PROXQUANT `0 for LSTM models.
