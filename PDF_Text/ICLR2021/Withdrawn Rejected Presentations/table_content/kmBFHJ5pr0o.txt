Table 1: Overall performance of DAT (in gray color), compared withbaselines, in TA (%), RA (%), communication time per epoch (seconds), andtotal training time (including communication time) per epoch (seconds). Forbrevity, ‘p × q’ represents ‘# nodes × # GPUs per node’, ‘Comm.’ representscommunication cost, and 'Tr. Time, represents training time.
Table 2: DAT in semi-supervised learning with unlabeled data,where the computing resource configuration and batch size areset the same as the 8th and 10th rows of (CIFAR-10, ResNet-18)in Table 1. The relative improvement over RA or TA obtained insupervised learning (CIFAR-10 only) is marked by red color.
Table 3: Effect of LALR on centralized and distributedtraining under CIAR-10 With same batch size (2048).
Table A1: TA/RA of DAT-FGSMunder (CIFAR-10, ResNet-18) using18x2048 batch size versus differentchoices of LALR hyperparameter CU.
Table A2: Overall performance of DAT (in gray color), compared with baselines, in TA (%), RA (%),communication time per epoch (seconds), and total training time (including communication time) per epoch (inseconds). For brevity, ‘p × q’ represents ‘# nodes × # GPUs per node’, ‘Comm.’ represents communicationcost, and 'Tr. Time, represents training time.
Table A3: Effect of gradient quantization on the performance of DAT for various numbers of bits. The trainingsettings of (CIFAR-10, ResNet-18) and (ImageNet, ResNet-50) are consistent With those in Table 1.
Table A4: Comparison to training over a high performance computing (HPC) cluster of nodesMethod	ImageNet, ResNet-50					# bits	TA(%)	RA (%)	Comm. per epoch (s)	Tr. time per epoch (s)DAT-PGD	32	63.75	38.45	898	1960DAT-FGSM	32	58.32	41.48	859	1109DAT-PGD (HPC)	32	63.43	38.55	15	1074DAT-FGSM (HPC)	32	57.60	41.70	15	310Table A5: Effect of 8-bit quantization on centralized robust training Fast AT w/ LALR.
Table A5: Effect of 8-bit quantization on centralized robust training Fast AT w/ LALR.
