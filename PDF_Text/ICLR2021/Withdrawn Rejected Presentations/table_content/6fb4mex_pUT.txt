Table 2: AUROC scores of ODIN on CIFAR10 vs OOD	T=1	T=10	T=100	T=1000-PM=0-	0.500	0.500	0.500	0.500PM=0.001	0.500	0.500	0.500	0.500PM=0.002	0.500	0.500	0.500	0.500PM=0.004	0.500	0.500	0.500	0.500Figure 13: the OOD score histograms of the in-distribution (blue) and OOD (red) samples whenT=1000 and PM=0.001D AppendixWe applied our OOD Attack algorithm to test the OOD detection method named Mahalanobis (Leeet al., 2018b).
Table 3: AUROC scores of Mahalanobis on CIFAR10 vs OOD	f (x) = OOD score of X	f (x) = feature concatenation-PM=0-	0.500	0.467PM=0.01	0.500	0.179	—Table 4: AUROC scores of Mahalanobis on CIFAR100 vs OOD	f (x)=OOD score of X	f (x) = feature concatenationPM=0	0.500	0.604PM=0.01	0.500	0.377	—16Under review as a conference paper at ICLR 2021Figure 14: The OOD score histograms of the in-distribution (blue) and OOD (red) samples. Thein-distribution dataset is CIFAR10, PM=0.01, and f (x) = OOD score of x.
Table 4: AUROC scores of Mahalanobis on CIFAR100 vs OOD	f (x)=OOD score of X	f (x) = feature concatenationPM=0	0.500	0.604PM=0.01	0.500	0.377	—16Under review as a conference paper at ICLR 2021Figure 14: The OOD score histograms of the in-distribution (blue) and OOD (red) samples. Thein-distribution dataset is CIFAR10, PM=0.01, and f (x) = OOD score of x.
Table 5: AUROC of Outlier Exposure on three datasetsSVHN VS OOD	CIFAR10 VS OOD	CIFAR100 VS OOD0.500 —	0.500	0.5003002001000j-----------1—-12Figure 16: The OOD score histograms of the in-distribution (blue) and OOD (red) samples. Thein-distribution dataset is CIFAR10.
Table 6: AUROC of the method on two datasetsSVHN vs OOD	CIFAR10 VS OOD0.501 —	0.576Figure 18: The OOD score histograms of the in-distribution (blue) and OOD (red) samples. Thein-distribution dataset is CIFAR10.
Table 7: AUROC of Gram on two datasets.
Table 8: AUROC of the OOD detection method on CIFAR10 vs OODOOD score	logp(x)	maxyp (y∣x)	-∣∣ ⅝21∣2AUROC	0.559	0.513	—	0.203Figure 21: The OOD score (logp(x) histograms of the in-distribution (blue) and OOD (red) sam-ples.
