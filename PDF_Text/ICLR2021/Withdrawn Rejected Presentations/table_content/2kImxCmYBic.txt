Table 1: NormalizationsType	ID	Formula	Scalingz-score	‘nmbr’	(Xi - μ)∕σ	scaled to sigma 1 and mu 0min-max	‘mnmx’	(xi - min)/(max - min)	scaled to unit intervalmean	‘mean’	(xi - mean)∕(max - min)	scaled and centered to meanMAD	‘MAD3’	(xi - max)∕(M AD)	scaled by median absolute deviationlognorm	‘lgnm’	ln(xi) → (Xi - μ)∕σ	log-normal scaled to GaussianUpon inspection a few points of differentiation become evident. The choice of denominator canbe material to the result, for while both (max - min) and standard deviation can have the result ofshrinking or enlarging the values to fall within a more uniform range, the (max - min) variety hasmore of a known returned range for the output that is independent of the feature set distributionproperties, thus allowing us to ensure all of the min-max returned values are non-negative for in-stance, as may be a pre-requisite for some kinds of algorithms. Of course this known range of outputrelies on the assumption that the range of values in subsequent test sets will correspond to the trainset properties that serve as a basis - to allow a user to prevent these type of outliers from interferingwith downstream applications, Automunge allows a user to pass parameters to the transformationfunctions, such as to activate floors or caps on the returned range.
Table 2: Retain Normalization (‘retn’)Min Max	Formula	Returned Min Returned Max≤ 0	≥	0 xi /(max - min)	min/(max - min) max/(max	-	min)> 0	>	0(xi - min)/(max - min)	01< 0	<	0(xi - max)/(max - min)	-1	02Under review as a conference paper at ICLR 20213	TransformationsIn many cases the application of a normalization procedure may be preceded by one or more typesof data transformations applied to the received numeric set. Examples of data transformations couldinclude basic mathematic operators like + - * /, log transforms, raising to a power, absolute values,etc. In some cases the transformations may also be tailored to the properties of the train set, forexample with a Box-Cox power law transformation (Box & Cox, 1964).
Table 3: Family Tree PrimitivesPrimitive	Upstream / Downstream	Applied to Generation	Column Action	Downstream Offspringparents	upstream	first	replace	yessiblings	upstream	first	supplement	yesauntsuncles	upstream	first	replace	nocousins	upstream	first	supplement	nochildren	downstream parents	offspring	replace	yesniecesnephews	downstream siblings	offspring	supplement	yescoworkers	downstream auntsuncles	offspring	replace	nofriends	downstream cousins	offspring	supplement	noThe convention for transformation functions in the Automunge library is that any kind of functionaccepts any kind of data, and in cases where an invalid entry is returned, for example when dividingby zero or taking a square root of a negative number, such entry may serve as a target for missingdata infill, with infill methods that may be applied to a column from a library of infill options -including “ML infill” in which random forest models (Breiman, 2001) are used to predict infillbased on properties of the train set. To facilitate the application of infill, transformation categoriesused as root categories are specified with a classification for the types of data that will be consideredvalid input, as for example may be non-negative numeric, non-zero numeric, integer numeric, etc.
Table 4: Binning Transformation Category OptionsTransform	One-Hot	Ordinal	Binary	ParametersNumber of standard deviations from the mean	‘bins’	‘bsor’	‘bsbn’	‘bincount’Powers of ten	‘pwrs’	‘pwor’	‘pwbn’	-Powers of ten (with support for negative)	‘pwr2’	‘por2’	‘por3’	-Fixed width bins	‘bnwd’	‘bnwo’	‘bnwb’	‘width’Equal population bins	‘bnep’	‘bneo’	‘bneb’	‘bincount’User specified bins (first/last unconstrained)	‘bkt1’	‘bkt3’	‘bkb3’	‘buckets’User specified bins (first/last bounded)	‘bkt2’	‘bkt4’	‘bkb4’	‘buckets’5	Noise InjectionFor most cases in the Automunge library, transformations applied to a train set feature set are appliedto the corresponding test set feature set using the same basis, such that if the same data is receivedfor both train and test sets, the same form will be returned (a useful point for validations). The noiseinjection options are a little different in that such injections may be intended just for the train databut not the corresponding test data.
Table 5: Numeric Noise InjectionsRoot Category	Normalization	Noise Type	Parameters‘DPnb’	‘nmbr’	Gaussian w/ Bernoulli ratio	‘mu’ / 'sigma' / 'flip_prob'‘DPmm’	‘mnmx’	scaled Gaussian w/ Bernoulli ratio	‘mu’ / 'sigma' / 'flip.prob'‘DPrt’	‘retn’	scaled Gaussian w/ Bernoulli ratio	‘mu’ / 'sigma' / 'flip.prob'Table 6: Categoric Noise InjectionsRoot Category	Encoding	Noise Type	Parameters‘DPbn’	‘bnry’ (boolean)	Bernoulli flip	'flip-prob'‘DPod’	‘ord3’ (ordinal)	Bernoulli flip to random activation	'flip-prob'‘DPoh’	‘onht’ (one-hot)	Bernoulli flip to random activation	'flip-prob'‘DP10’	‘1010’ (binary)	Bernoulli flip to random activation set	'flip-prob'Numeric noise injections [Table 5, Fig 1] are derived from a Gaussian source with configurableparameters. For noise intended to sets with a fixed range of values such as DPmm, although the4Under review as a conference paper at ICLR 2021	root Category						'DPmm,				'DPm2,					ParentS		[DPm2,]			[ ,DPm2, ]					SiblingS			U				U					auntsuncles			U				U					CoUSinS			[ ,NArw, ]				[,NArw,]	
Table 6: Categoric Noise InjectionsRoot Category	Encoding	Noise Type	Parameters‘DPbn’	‘bnry’ (boolean)	Bernoulli flip	'flip-prob'‘DPod’	‘ord3’ (ordinal)	Bernoulli flip to random activation	'flip-prob'‘DPoh’	‘onht’ (one-hot)	Bernoulli flip to random activation	'flip-prob'‘DP10’	‘1010’ (binary)	Bernoulli flip to random activation set	'flip-prob'Numeric noise injections [Table 5, Fig 1] are derived from a Gaussian source with configurableparameters. For noise intended to sets with a fixed range of values such as DPmm, although the4Under review as a conference paper at ICLR 2021	root Category						'DPmm,				'DPm2,					ParentS		[DPm2,]			[ ,DPm2, ]					SiblingS			U				U					auntsuncles			U				U					CoUSinS			[ ,NArw, ]				[,NArw,]			11		Children			U				U				Fiiecesnephews			U				U					CoWorkerS			U				[,DPmm,]				friends			U				U		ProCeSSdiCt						DPmrn				mnmX	
Table 8: Higgs Data Normalization Scenarios (AUC metric // compared to raw data)	Raw Data	Z-Score	Retain	Retain with Bins	Retain w/ Noise full	Retain w/ Noise partial	Retain w/ Augmentfull data	0.8670	0.8668	0.8667	0.8660	0.8268	0.8624	0.866742 epochs	-	(0.0002)	(0.0003)	(0.0010)	(0.0402)	(0.0046)	(0.0003)5% data	0.8436	0.8432	0.8431	0.8433	0.7679	0.8388	0.845314 epochs	-	(0.0004)	(0.0005)	(0.0003)	(0.0757)	(0.0048)	0.00170.25% data	0.7715	0.7716	0.7716	0.7718	0.7052	0.7668	0.78213 epochs	-	0.0001	0.0001	0.0003	(0.0663)	(0.0047)	0.0106noise injection. Both of the noise injections applied Gaussian noise with standard deviation of 0.03and with scaling to maintain the fixed range of the received data.
Table 9: Higgs Data Normalization Scenarios (Support Vector Classifier with Linear kernel)	Raw Data	Z-Score	Retain	Retain with Bins	Retain w/ Noise full	Retain w/ Noise partial	Retain w/ Augment partialAccuracy	0.6410	0.6410	0.6410	0.6821	0.6201	0.6393	0.6402	-	-	-	0.0411	(0.0209)	(0.0018)	(0.0008)9	DiscussionI would offer first that any consideration around benefits of feature engineering should distinguishfirst by scale of data available for training. When approaching big data scale input with infinitecomputational resources there may be less of a case to be made for much beyond basic normalizedinput. The nuance comes into play when we are targeting applications with real world constraints.
Table 10: Summary of Use CasesDesciption	Examples	Use CasesZ-Score Normalization	nmbr	general use, when distribution is unknownMin-Max Scaling	mnmx	when a fixed range is desired, e.g. nonnegativeRetain Normalization	retn	when interpretability is desired, sign retentionBins and Grainings	bins, pwrs, bnep	to aggregate sets into buckets, such as to sup- plement normalized data, helpful with linear modelsNoise Injection	DPnb, DPmm, DPrt, DPod, DP10	useful for data set augmentation, especially with underserved training data, also for differ- ential privacySequential	dxdt, d2dt, d3dt	useful for time-series data, supplements nor- malized sets with proxies for derivativesInteger Sets	ntgr, ntg2, ntg3	to redundantly encode integer sets of unknown interpretation9Under review as a conference paper at ICLR 2021AcknowledgmentsA thank you is owed to Alice Zheng and Amanda Casari whose 2018 book “Feature Engineeringfor Machine Learning” served as a helpful reference as I began to explore the practice of feature en-gineering. Thanks to Stack Overflow, Python, PyPI, GitHub, Colaboratory, Anaconda, and Jupyter.
Table 11: Higgs Experiment Details (AUC Average // Standard Deviation // Repetitions)	Raw Data	Z-Score	Retain	Retain with Bins	Retain w/ Noise full	Retain w/ Noise partial	Retain w/ Augmentfull data	0.8670	0.8668	0.8667	0.8660	0.8268	0.8624	0.8667St Dev	0.0002	0.0004	0.0005	0.0013	0.0007	0.0007	0.0015Repetitions	6	6	6	6	5	5	65% data	0.8436	0.8432	0.8431	0.8433	0.7679	0.8388	0.8453St Dev	0.0011	0.0011	0.0016	0.0014	0.0016	0.0011	0.0006Repetitions	30	30	30	30	30	30	300.25% data	0.7715	0.7716	0.7716	0.7718	0.7052	0.7668	0.7821St Dev	0.0024	0.0025	0.0026	0.0029	0.0034	0.0025	0.0020Repetitions	100	100	100	100	100	100	100Table 12: Higgs SVC Experiment Details (Accuracy Average // Standard Deviation // Repetitions)	Raw Data	Z-Score	Retain	Retain with Bins	Retain w/ Noise full	Retain w/ Noise partial	Retain w/ Augment partialAccuracy	0.6410	0.6410	0.6410	0.6821	0.6201	0.6393	0.6402St Dev	0.0003	0.0003	0.0003	0.0005	0.0006	0.0003	0.0003Repetitions	6	6	6	6	6	6	619Under review as a conference paper at ICLR 2021H	AFew Helpful HintsA few highlights that might make things easier for first-timers:
Table 12: Higgs SVC Experiment Details (Accuracy Average // Standard Deviation // Repetitions)	Raw Data	Z-Score	Retain	Retain with Bins	Retain w/ Noise full	Retain w/ Noise partial	Retain w/ Augment partialAccuracy	0.6410	0.6410	0.6410	0.6821	0.6201	0.6393	0.6402St Dev	0.0003	0.0003	0.0003	0.0005	0.0006	0.0003	0.0003Repetitions	6	6	6	6	6	6	619Under review as a conference paper at ICLR 2021H	AFew Helpful HintsA few highlights that might make things easier for first-timers:1)	automunge(.) returns sets as numpy arrays by default (for universal compatibility with ML plat-forms). A user can instead receive the returned sets as pandas dataframes by passing the parameterpandasoutput = True2)	Even if the sets are returned as numpy arrays, you can still inspect the returned column headerswith the returned list We demonstrate as finalcolumns _train3)	Printouts are turned on by default, they can be turned off with printstatus=False4)	Note for data sets with just a few rows, such as those demonstrated here, there is a PCA heuristicto apply dimensionality reduction when the number of features is more than 50% of the number ofobservations in the train set (this is a somewhat arbitrary heuristic). This can be turned off withML_cmnd = {'PCA_type':‘off’}.
