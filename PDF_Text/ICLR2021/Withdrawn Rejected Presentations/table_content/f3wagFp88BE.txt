Table 1: Comparison of models with and w/oBN on accuracy and robustness	Network	Acc	PGD l2 0.25	PGD l∞ 1/255	CW l2 0.25	CW l∞ 1/255	VGG16 (None)	71.59	15.55	1.79	16.66	0.23	VGG16 (BN)	73.37	6.04	0.55	6.82	0.02	VGG19 (None)	72.38	16.52	2.18	17.46	0.30⅞	VGG19 (BN)	74.24	6.94	0.69	7.66	0.03a) S	ResNet18 (None)	66.51	30.44	1.24	30.43	0.93S	ResNet18 (BN)	70.50	16.79	0.14	17.40	0.07	ResNet50 (None)	71.60	28.00	2.17	28.26	0.88	ResNet50 (BN)	76.54	19.50	0.53	20.19	0.19	VGG11 (None)	95.42	63.91	83.20	64.64	83.24	VGG11 (BN)	96.27	51.22	77.50	51.13	77.61	VGG16 (None)	95.76	62.24	82.76	62.97	82.92	VGG16 (BN)	96.43	52.90	80.24	52.88	79.93	VGG11 (None)	90.06	51.30	70.47	51.75	70.40o	VGG11 (BN)	92.48	39.31	63.87	39.04	63.852	VGG16 (None)	91.89	34.01	63.18	34.37	63.46爸	VGG16 (BN)	93.7	28.61	56.05	24.01	54.58U	ResNet50 (None) 92.15		29.24	49.33	17.09	49.24
Table 2: Influence of various normalization techniques on ac-curacy (left/) and robustness (/right).
Table 3: Cross-evaluation of the features extractedfrom source models on target models with the base-line VGG16. The left reports the results for standardmodels, and the right for adversarially trained models.
Table 4: Parameters to train a standard model on CIFAR10/SVHNParameter	ValueLearning rate	0.01Batch size	128Weight Decay	5∙10-4Epochs	300Learning rate decay epochs	200Learning rate decay factor	0.1For robust models in Table 3 We follow the adversarial training strategy from (Madry et al., 2017)with l2 -PGD and we list the parameters in Table 5.
Table 5: Training parameters for adversarial training for CIFAR10/SVHNParameter	ValueLearning rate	0.01Batch size	128Weight Decay	5∙10-4Epochs	150Learning rate decay epochs	100Learning rate decay factor	0.1PGD-variant	l2PGD step size (α)	0.1PGD perturbation magnitude ()	0.5PGD iterations	7B.3	Extracting features as a dataset from a modelTo demonstrate feature transferability in Table 3, we extract features from standard and adversariallytrained models as a dataset. We follow the procedure and hyperparameter choices in (Ilyas et al.,2019) and generate dataset DD, given a model C:EiD [y ∙f(Xn [y ∙ f(x)] XFFC,	⑵where FC is the set of features utilized by C. The set of activations in the penultimate layer g(x)corresponds to FC in the case of DNNs. Thus, to extract the robust features from C we perform thefollowing optimization:
Table 6: Hyperparameters for training the extracted datasetsDataset ∣ LR Batch size LR Drop Data Aug. Momentum Weight DecayDbR	0.01	128	Yes	Yes	0.9	5•10-4DbNR	0.01	128	Yes	Yes	0.9	5.10-4b Drand	0.01	128	Yes	Yes	0.9	5.10-4Dbdet	0.1	128	Yes	No	0.9	5.10-4b Dconf lict	0.1	128	Yes	No	0.9	5.10-4indicates a dataset containing mainly RFs relevant to a robust model, and DbNR indicates that withstandard model. During the extraction of DNR, the magnitude was not constraint, thus DNR has^ ^both RFs and NRFs. Drand and Ddet are datasets consisting of “useful” NRFs represented throughadversarial examples for a standard model. The target classes of Drand were chosen randomly, whilethe ones for Ddet were selected with an offset of t+ 1 to the original sample ground-truth class. Notethat these datasets are labeled with the target class for which the adversarial example was generated.
Table 7: Adapting the BN statistics on CIFAR10-CModel	Clean Accuracy	Adapted AccuracyVGG-16	71.9	79.4ResNet-18	73.9	82.3Table 8: Robust accuracy comparison of models with and w/o BN under FGSM attackNetworkAccFGSM4/255VGG11 (None)	90.06	32.51VGG11 (BN)	92.48	40.86VGG16 (None)	91.89	23.26VGG16 (BN)	93.7	51.28ResNet50 (None)	92.15	28.23ResNet50 (BN)	95.6	38.0715Under review as a conference paper at ICLR 2021H Visualization of optimization landscapeFollowing (Santurkar et al., 2018), we visualize the optimization landscape. The results on ResNet18and VGG16 are shown in Fig.14 and Fig.15, respectively. On ResNet18, only BN leads to more
Table 8: Robust accuracy comparison of models with and w/o BN under FGSM attackNetworkAccFGSM4/255VGG11 (None)	90.06	32.51VGG11 (BN)	92.48	40.86VGG16 (None)	91.89	23.26VGG16 (BN)	93.7	51.28ResNet50 (None)	92.15	28.23ResNet50 (BN)	95.6	38.0715Under review as a conference paper at ICLR 2021H Visualization of optimization landscapeFollowing (Santurkar et al., 2018), we visualize the optimization landscape. The results on ResNet18and VGG16 are shown in Fig.14 and Fig.15, respectively. On ResNet18, only BN leads to morepredictive and stable gradient; on ResNet50, BN/IN/LN/GN lead to a more stable gradient, however,the the effect of IN/LN/GN is significantly smaller than that of BN. The results demonstrating theinfluence of shortcut are shown in Fig.16 where shortcut is found to have trivial influence on thegradient stability. The results comparing FixupIni and BN are shown in Fig.17, where FixupIni
