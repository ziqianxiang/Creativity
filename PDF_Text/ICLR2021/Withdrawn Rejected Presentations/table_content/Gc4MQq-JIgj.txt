Table 1: Performance of trained policies on known and unknown Circuit environments. The values inthe table are the obtained rewards, and inside the parenthesis are the probabilities of crashing. Timestands for the computation time consumed in making 100 action steps. Note that our method nevercrashes in this experiment. In this experiment, every crash is penalized by 200pts. See Appendix forthe result with heavier crash penalty.
Table 2: Performance of trained policies on and known and unknown Jam environments. Values inthe table are reported in the same way as in Table 1. In this experiment, every crash is penalized by50 pts. Note that we achieve the lowest crashing rate while achieving high score. Our RP stands outmore if the crash is penalized more heavily (See Appendix)policy for the Circuit task, and evaluated its performance on the environments that are different fromthe original circuit used in the training, (1) narrowed circuit with original shape, and (2) differentlyshaped circuit with same width. For the second set of experiments, we trained a safe policy forthe Jam task, and tested its performance on other Jam tasks with different numbers of randomlymoving obstacles. Figs. 1 and 2 show the results. For the modified Jam, we have no results for MPCwith more than 3-step prediction since the exhaustive search cannot be completed within reasonabletime-frame. The 4-step MPC requires 18.2secs per each 100 steps for Circuit, and the 3-step MPCrequires 285secs per each 100 steps for the original Jam. We find that, even in varying environments,the policy obtained by our method can guarantee safety with high probability while seeking highreward. On the other hand, DQN in new environment is failing both in terms of reward and safety.
Table 3: Performance of trained policies on unknown Circuit environments. The values in the tableare the obtained rewards, and inside the parenthesis are the probabilities of crashing. Time stands forthe computation time consumed in making 100 action steps. Note that our method never crashes inthis experiment. In this experiment, every crash is penalized by 1500pts.
Table 4: Performance of trained policies on and unknown Jam environments. Values in the table arereported in the same way as in Table 3. In this experiment, every crash is penalized by 500 pts. Notethat we achieve the lowest crashing rate while achieving high score.
