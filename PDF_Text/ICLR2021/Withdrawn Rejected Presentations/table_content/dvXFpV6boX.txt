Table 1: Retrieval recall rate of DPR (Karpukhin et al., 2020), RAG (Lewis et al., 2020), and RDR(OUrs) on NQ-dev, NQ-test, and TriviaQA-test. ∣→ indicates which model RDR targets to enhance.
Table 2: The ablation study result, which showshow the retrieval recall rate on the test set of Nat-UralQuestions changes when RDR enhanced fromDPR-Single is trained without distillation. The re-call rate shows a consistent drop at all k's when nodistillation is used, and the gap is relatively largeat top-1.
Table 3: Enhancement in end-to-end QA accuracy on NaturalQuestions and TriviaQA achieved by utilizingRDR along with the readers of DPR-Single and RAG-Token. We finetune the readers for a few epochs. Thevalues in the “Reported” column for DPR-Single-related models are the best performance achieved amongk P t1, 10, 20, 30, 40, 50u, whereas those values for RAG-Token-related models are inferred with k “ 15.8We could not perform RAG-related experiments on TriviaQA because the model checkpoint is not publiclyavailable, and it was non-trivial to reproduce the with our limited computation resources.
Table 4: Retrieval recall rate, index search time, and file size according to the type of FAISS index.
Table 5: Latency of DPR reader with respect to the number of passages to read. The inference timeis averaged from the runs on 100 question-passage pairs of batch size 1. Two Xeon Gold 5120 CPUcores are used across the experiments. The latency linearly increases with respect to the number ofpassages.
Table 6: End-to-end QA (Exact Match) accuracy of recent and state-of-the-art models. Each of theresults in the left and right columns of TriviaQA is the score on the open domain test set and hiddentest set, respectively.
Table 7: Ablation result which shows the drop in the end-to-end QA accuracy (Exact Match) whenno finetuning is applied to the reader while the retriever is swapped with RDR.
