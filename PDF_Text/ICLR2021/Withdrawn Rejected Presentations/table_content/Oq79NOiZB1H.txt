Table 1: Comparison of the accuracy (F1-score) of SGCN, SGCN+, and SGCN++.
Table 2: Configuration of different sampling algorithms during training	GraphSAGE	VRGCN	Exact	FastGCN	LADIES	GraphSAINTMini-batch size	512	512	512	512	512	2048Sampled neighbors	5	2	All			Samples in layerwise				4096	512	20483https://github.com/tkipf/pygcn13Under review as a conference paper at ICLR 2021Figure 3: Comparison of doubly variance reduction and vanilla sampling-based GCN training onPPI dataset with SGD (learning rate 0.1) and Adam optimizer (learning rate 0.01). All other Config-urations are as default.
Table 3: Summary of dataset statistics. m stands for multi-class classification, and s stands forsingle-class.
Table 4: Comparison of average time (1 snapshot step and 10 regular steps) of doubly variancereduced LADIES++ with regular step batch size as 512. Full-batch is used for snapshot step on PPI,PPI-Large, and Flickr. 50% training set nodes are sampled for the snapshot step on Reddit, and 15%training set nodes are sampled for the snapshot step on Yelp.
Table 5: Comparison of average time (10 regular steps) of LADIES with regular step batch size as512	______________________________________________________________Time (second)	PPI	PPI-Large	Flickr	Reddit	YelpRegular step sampling	1.042	1.077	0.977	9.856	9.155Regular step transfer	0.036	0.047	0.016	0.496	0.041Computation	0.077	0.068	0.034	0.029	0.082Total time	1.156	1.192	1.028	10.381	9.278the comparison of training loss and validation loss with different number of inner-loop intervals forSGCN++ and SGCN+ on Reddit dataset, respectively. We can observe that the model with asmaller snapshot gap requires less number of iterations to reach the same training and validationloss, and gives us a better generalization performance (FI-Score).
