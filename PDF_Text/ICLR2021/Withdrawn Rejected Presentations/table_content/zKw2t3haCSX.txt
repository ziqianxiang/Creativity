Table 1: Hierarchies used to define targets in for the CIFAR-10 dataset using equation (4) - classstructures for the first three groupings were built by randomly sampling subgroups of cardinalitytwo or three from the 10 class categories . Networks were trained varying the smoothing factor, σ,in order to demonstrate the ability to embed information about root class as well as superclass intothe smoothed labels. Grouping four was chosen as a semantically-meaningful hierarchy while stillforcing some texturally dissimilar classes together.
Table 2: Results from models trained on mnist dataset for the model with the best mse as well as itscorresponding validation accuracy and the corresponding smoothing factorModel	CNN Layers	FC Layers	accuracy	top mse	σ	top accuracy	σ3-layer-0	2 - 3x3x2	1x2	.465	1.62	0.7	.782	0.13-layer-1	2 - 3x3x4	1x4	.848	.734	0.7	.959	0.03-layer-2	2 - 3x3x8	1x8	.981	.356	0.1	.981	0.1,0.23-layer-3	2 - 3x3x16	1x16	.986	.271	0.3	.986	0.36-layer-0	3 - 3x3x16	3x16	.985	.260	0.4	.986	0.26-layer-1	3 - 3x3x32	3x32	.986	.232	0.5	.987	0.2,0.3,0.46-layer-2	3 - 3x3x32	3x64	.988	.197	0.3	.988	0.39-layer	3 - 3x3x32	3x64, 3x128	.984	.262	0.3	.986	0.1dataset was also chosen because there is a straightforward way to consider error cost - i.e. the meansquared error between the true value and the predicted class. It is typically the case that only rawclass accuracy is considered in a model’s performance. However, in most real-world applicationsinvolving number classification errors closer to the true value would likely be preferable. For eachnetwork architecture, the smoothing parameter was varied from 0.0, the one-hot case, to 0.8, whichis the point at which the neighboring target classes have values near the true class and accuracybegins to drop off significantly. All networks were trained for 150 epochs with standard SGD witha constant learning rate of 10-2. The input layer was normalized but otherwise no regularizationwas used. All layers prior to the classification layer used ReLu activation functions, with a standard
Table 3: Architectures used for CIFAR-10 experiments - each consists of standard Residual Mod-ules (insert citation) with batch normalization applied before each activation function. Detaileddescriptions of architectures are included in Appendix A.
Table 4: Results showing best argmax and k-means accuracy for models trained on CIFAR-10dataset - Extended tabulated results for CIFAR-10 experiments can be found in appendix030.6QA0.21.0Model grouping top argmax σ top k-means σResNet-3-0	groupingo	.742	0.0	.636	0.3	grouping ι	.732	-0.1-	.681	^0^	grouping2	.738	-0.0-	.663	^03^	grouping3	.746	-0.0-	.719	^0^ResNet-3-1	groupingo	.873	-0.0-	.746	^0^	grouping ι	.870	-0.0-	.752	^0^	grouping2	.864	-0.0-	.722	^0^	groupings	.869	-0.0-	.797	^0^ResNet-9	groupingo	.924	-0.1-	.881	^0^	grouping ι	.926	-0.1-	.854	^0^	grouping2	.918	0.0,0.1	.838	^0^	groupings	.927	-0.1-	.870	^0^
Table 5: ResNet architecture details - All architectures consist of residual modules with batch nor-malization applied before each ReLu activation function, followed by a global average pooling layerbefore the final classification layer.
Table 6: Extended results from Table 4 - Below are the accuracy and mean-squared error results foreach architecture at for each value of σ applied, with best results emphasized for each model.
Table 7: Results from models trained on CIAFR-10 applying smoothed targets according to theclusters defined in Table 1 and clustering on two different levels. Note that even at σ = 0.1, with atrivial loss in accuracy, the model has effectively embedded two different levels of labels that can beused to cluster data on 2 different levels of granularity relative to the semantic hierarchy.
