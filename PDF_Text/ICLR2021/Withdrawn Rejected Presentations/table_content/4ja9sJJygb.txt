Table 1: Breakdown of the compaction ratio by layer for AlexNet for 60% sparsityLayer	Dense Matrix	Sparse Matrix (After Compaction)	Sparse Matrix (over BSR)	Compaction Ratioconv1	90.75kB	40.03kB	38.19kB	1.05xconv2	1200kB	626.7kB	386.9kB	1.62xconv3	2.53MB	1.38MB	0.80MB	1.73xconv4	3.38MB	1.12MB	0.67MB	1.67xconv5	2.25MB	0.68MB	0.41MB	1.66xfc6	144.0MB	83.60MB	28.69MB	2.91xfc7	64.00MB	36.98MB	16.12MB	2.29xfc8	15.63MB	8.51MB	4.20MB	2.03xTable 1 shows a more detailed breakdown of the compaction that is achieved in different layers ofAlexNet. We see that the level of block sharing in the first layer, which is an 11 × 11 convolution,is very small. However, the subsequent convolution layers, which use much smaller kernels, offermuch great opportunity for sharing blocks. Note that for convolution layers, we use a block size thatcorresponds to one row ofa convolution kernel (i.e. a vector of length 11 for an 11 × 11 kernel). Thesavings from sharing in the fully-connected layers are even larger. Table 2 (in Appendix A) showsthe same data for VGG16. There is a correlation between the size of the matrix, and thus the numberof blocks, and the opportunities for sharing identical blocks.
Table 2: Breakdown of the compaction ratio by layer for VGG16 for 60% sparsityLayer	Dense Matrix	Sparse Matrix	Sparse Matrix (After Compaction)	Compaction Ratio (over BSR)convL1	6.75kB	3.32kB	3.12kB	1.06xconvL2	144.0kB	71.93kB	50.10kB	1.44xConv2_1	288.0kB	146.7kB	95.57kB	1.53xconv2_2	576.0kB	203.5kB	179.5kB	1.13xConv3_1	1.13MB	0.57MB	0.34MB	1.70xconv3_2	2.25MB	1.17MB	0.68MB	1.74xConv3_3	2.25MB	1.18MB	0.68MB	1.74xConv4_1	4.50MB	2.43MB	1.37MB	1.77xConv4_2	9.00MB	4.78MB	2.69MB	1.76xConv4_3	9.00MB	4.48MB	2.53MB	1.77xConv5_1	9.00MB	4.75MB	2.64MB	1.80xConv5_2	9.00MB	4.68MB	2.58MB	1.81xConv5_3	9.00MB	4.51MB	1.51MB	2.99xfC6	392.0MB	225.7MB	72.51MB	3.11xfC7	64.00MB	37.30MB	14.99MB	2.49xfC8	15.63MB	8.73MB	4.65MB	1.88xB Compaction for fully connected layersFor convolutional layers, we select a block size that is equal to the size of one row of a kernel. This
Table 3: Optimal vector size for FC layers at 60% sparsityNetwork	Layer	Block SizeAlexNet	-^fc6^^	8AlexNet	fc7	8AlexNet	fc8	4VGG16	fc6	8VGG16	fc7	4VGG16	fc8	4ResNet	fc	2LeNet	fc3	4LeNet	fc4	2Figure 9 shows the trade-off between block storage and index storage for AlexNet layer fc6 andfc8. In both cases the best block size is a compromise between block and index storage, with asize of four for fc6 and eight for fc8. Table 3 shows the optimal block size for fully-connectedlayers across several different CNNs. In general it seems that FC layers with more parameters tendto benefit from larger block sizes.
