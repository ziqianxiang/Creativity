Table 1: Comparisons between existing NAS methods for GNN and the proposed EGAN.
Table 2: Performance comparisons in transductive and inductive tasks, whose evaluation metricsare mean classification accuracy and Micro-F1, respectively. For the first five GNN models in thetable, we present the better performance of each and its JK variants, and the detailed performanceof GNN models and their JK variants are in Table 10 in Appendix A.4.5. Note that the performanceof LGCN on CS and Computer is “-” due to the OOM (Out Of Memory) problem when running thereleased code in our GPUs. The results of Geom-GCN are copied from the original paper (Pei et al.,2020), since the data split ratio is the same to our experiments.
Table 3: Performance comparisons of the transfer task on Reddit and Arxiv, for which we use Micro-F1 and accuracy as evaluation metrics, respectively. Note that the OOM (Out Of Memory) problemoccurs when running GraphNAS on these two datasets, thus we do not report its performance.
Table 4: Performance comparisons on graph classificationtasks. We show the mean test accuracy (with standard de-viation) on these datasets. For the first five GNN models inthe table, we present the better performance of each and itsJK variants, and the detailed performance of GNN modelsand their JK variants are in Table 12 in Appendix A.5.3I Methods ∣ D&D ∣ PROTEINS	GCN	0.733(0.043)	0.730(0.026)	GraphSAGE	0.734(0.029)	0.734(0.041)Human-	GAT	0.716(0.056)	0.745(0.030)designed	GIN	0.733(0.033)	0.737(0.048)GNN	GeniePath	0.705(0.051)	0.694(0.035)	DiffPool	0.779(0.045)	0.738(0.040)	SAGPOOL	0.762(0.009)	0.724(0.041)	ASAP	0.748(0.024)	0.733(0.024)	Random	0.742(0.043)	0.731(0.031)NAS NAS	Bayesian	0.746(0.031)	0.676(0.041)metos	GraphNAS	0.719(0.045)	0.725(0.031)	GraphNAS-WS	0.758(0.044)	0.752(0.025)IEGAN	10.779(0.034) 10.761(0.039)
Table 5: The running time (s) of each method during the search phase.	Figure 3: Test accuracy w.r.t. elapsed time on Cora.
Table 6: Performance comparisons of EGANusing different search spaces.
Table 7: The operations We Use as node and layer aggregators for the search space of EGAN. Detailsof node aggregators are giVen in Table 8.
Table 8: More explanations to the node aggregators in Table 7.
Table 9: Dataset statistics of the datasets in the experiments. Here we use “CS”, “Computer”, as theabbreviations for Coauthor CS and Amazon Computers, respectively.
Table 10: Performance comparisons between the five GNN models and their JK variants. For sim-plicity, we report the better one of each in Table 2.
Table 11: The statistics of datasets in graph-level tasks.
Table 12: Performance comparisons between the five GNN models and their JK variants. For sim-plicity, we report the better one of each in Table 4.
Table 13: Performance comparisons of two search spaces on four benchmark datasets. We show themean classification accuracy (with STD).
Table 14: More implementing details of GNN baselines. Here we give hidden dimension size,activation function, and the number of heads (GAT models). For JK-Network, we further give thelayer aggregators.
Table 15: The hyperparameters obtained by hyperopt in the fine-tuning process for the searchedarchitectures in Figure 2, 6, and 7. For all GAT aggregators, we set the number of head to 2 forsimplicity, which empirically works well in our experiments.
