Table 1: The #params column is the ratio of number of parameters of the smaller network comparedagainst the standard full network. The speed gain column denotes the ratio of inference time. Thelast three columns compare the accuracies (in %) of the the smaller network trained using the adjointparadigm vs the the accuracy of the same network trained using the teacher student paradigm againstthe accuracy of the standard full network. aα is as defined in Eqn. 2 and rβ denotes a random matrixwith β fraction of its entries are zero. * denotes element-wise dot-product. Detailed results can befound in the appendix .
Table 2: The last two columns show the accuracies (in %) of the network trained in the adjointfashion and the same network trained in the standard way. In all cases the adjoint network exceedsthe accuracy of the standard full network. aα, rβ are as in Table 1. Detailed results can be found inthe appendix .
Table 3: The last two columns show the accuracies (in %) of the network trained in the adjoint fashionvs the same network trained in the standard way using dropouts. In all cases the adjoint networkexceeds the accuracy of the standard one. aα, rβ are as in Table 1. Details can be found in theappendix .
Table 4: Accuracy for the various training paradigms for resnet50 trained on Cifar-1014Under review as a conference paper at ICLR 2021Training with resnet18 on cifar-10					Training paradigm	Masking matrix (M)	top-1 full	top-5 full	top-1 small	top-5 smallStandard-full		88.17	99.41		Standard-full + dropout (0.5)		89.04	99.55		Adjoint-2	a，2	88.75	99.54	88.06	99.41Adjoint-4	a4	89.8	99.73	88.74	99.64Adjoint-8	08	90.26	99.72	88.62	99.62Adjoint-16	ai6	89.58	99.61	87.94	99.57Adjoint-4	a4 * r0.9	89.88	99.65	88.59	99.53Adjoint-4	a4 * r0.9	89.75	99.56	88.34	99.48Adjoint-8	a8 * r0.5	89.96	99.61	87.85	99.52Adjoint-8	a8 * r0.9	89.4	99.56	87.61	99.51Adjoint-16	ai6 * r0.9	89.66	99.52	87.67	99.35Table 5: Accuracy for the various training paradigms for resnet18 trained on Cifar-10D.2 Experiments on cifar- 1 00(a) Without random mask(b) With random mask
Table 5: Accuracy for the various training paradigms for resnet18 trained on Cifar-10D.2 Experiments on cifar- 1 00(a) Without random mask(b) With random maskFigure 11: Validation loss for the various training paradigms for resnet50 trained on Cifar-100. Allthe adjoint plots correspond to the bigger network.
Table 6: Accuracy for the various training paradigms for resnet50 trained on Cifar-10017Under review as a conference paper at ICLR 2021Training with resnet18 on cifar-100					Training paradigm	Masking matrix (M)	top-1 full	top-5 full	top-1 small	top-5 smallStandard-full		61.39	85.38		Standard-full + dropout		64.3	88.13		(0.5)					Adjoint-4	a4	66.84	89.2	62.73	87.4Adjoint-8	a，8	66.09	89.43	61.38	87.12Adjoint-16	a16	64.48	89.02	57.84	85.24Adjoint-4	a4 * r0.5	66.54	89.56	62.64	87.38Adjoint-4	a4 * r0.6	65.81	89.49	62.26	87.52Adjoint-4	a4 * r0.7	65.88	89.75	61.7	87.53Adjoint-4	a4 * r0.8	66.13	89.55	61.91	87.08Adjoint-4	a4 * ro.9	66.29	89.55	61.42	86.89Adjoint-8	a8 * r0.5	65.63	89.28	60.64	86.79Adjoint-8	a8 * r0.6	65.14	89.08	60.08	86.39Adjoint-8	a8 * r0.7	64.49	89.03	58.93	85.63Adjoint-8	a8 * r0.8	64.88	89.17	59.45	86.09
Table 7: Accuracy for the various training paradigms for resnet18 trained on Cifar-100D.3 Experiments on imagenetValid loss vs #Epochs----Adjoint-4----Adjoint-8----Standard---- Standard + dropoutEpochFigure 15:	Validation loss for the various training paradigms for resnet50 trained on Imagenet. Allthe adjoint plots correspond to the bigger network.
Table 8: Accuracy for the various training paradigms for resnet50 trained on imagenetD.4 Experiments on imagewoof(a) Without random maskFigure 16: Validation loss for the various training paradigms for resnet50 trained on imagewoof. Allthe adjoint plots correspond to the bigger network.
Table 9: Accuracy for the various training paradigms for resnet18 trained on imagewoof20Under review as a conference paper at ICLR 2021Training with resnet50 on imagewoof					Training paradigm	Masking matrix (M)	top-1 full	top-5 full	top-1 small	top-5 smallStandard-full		85.2	98.4		Standard-full + dropout		82.76	98.38		(0.75)					Adjoint-4	a4	85.52	98.3	85.26	98.17Adjoint-8	a，8	85.62	98.59	84.11	98.35Adjoint-16	a16	85.28	98.54	82.94	98.12Adjoint-4	a4 * r0.5	85.75	98.48	85	98.48Adjoint-4	a4 * r0.6	85.75	98.69	85.15	98.54Adjoint-4	a4 * r0.7	85.31	98.38	84.5	98.35Adjoint-4	a4 * r0.8	85.65	98.64	84.76	98.43Adjoint-4	a4 * r0.9	86.3	98.59	84.27	98.48Adjoint-8	。8 * r0.5	85.72	98.67	83.95	98.28Adjoint-8	。8 * r0.6	85.62	98.64	84.01	98.41Adjoint-8	。8 * r0.7	85.62	98.48	83.3	98.35Adjoint-8	。8 * r0.8	85.49	98.51	82.52	98.38
Table 10: Accuracy for the various training paradigms for resnet50 trained on imagewoofD.5 Experiments on oxford-pets(a) Without random maskFigure 20: Validation loss for the various training paradigms for resnet50 trained on oxford-pets Allthe adjoint plots correspond to the bigger network.
Table 11: Accuracy for the various training paradigms for resnet50 trained on oxford-petsTraining with resnet18 on oxford-pets					Training paradigm	Masking matrix (M)	top-1 full	top-5 full	top-1 small	top-5 smallStandard-full		84.84	97.76		Standard-full + dropout (0.5)		82.61	97.09		Adjoint-2	a2	84.5	97.56	83.89	97.36Adjoint-4	a4	86.33	98.3	83.08	97.76Adjoint-8	a8	85.52	98.71	82.61	97.97Adjoint-2	a，2 * r0.5	84.91	97.69	83.62	97.63Adjoint-2	a2 * r0.9	83.76	97.76	80.71	97.22Adjoint-4	a4 * r0.5	85.65	97.9	82.74	97.83Adjoint-4	a4 * r0.9	84.84	97.83	80.51	97.09Adjoint-8	a8 * r0.5	85.31	97.9	82.07	97.63Adjoint-8	a8 * r0.9	83.33	97.9	76.72	96.48Adjoint-16	a16 * r0.5	84.5	97.83	77.46	96.75Adjoint-16	a16 * r0.9	82.27	97.76	72.05	95.26Table 12: Accuracy for the various training paradigms for resnet18 trained on oxford-petsE Choosing the regularization functionFinally, we compare different choices of regularization function for the adjoint loss (Eqn. 3). For allthe previous experiments, we use the ‘quadratic’ function λ(t) = min{4t2, 1}. In this section, we fix
Table 12: Accuracy for the various training paradigms for resnet18 trained on oxford-petsE Choosing the regularization functionFinally, we compare different choices of regularization function for the adjoint loss (Eqn. 3). For allthe previous experiments, we use the ‘quadratic’ function λ(t) = min{4t2, 1}. In this section, we fixthe architecture, dataset and mask matrix as resnet18, cifar100 and Adj-4 respectively and vary theregularization function. We look at different functions which includes exponential and trigonometric23Under review as a conference paper at ICLR 2021Figure 24: Validation cross entropy loss for various regularization functions. The networks weretrained using Adj-4 mask matrix on cifar-100 using resnet-18.
Table 13: The effect of training with different regularization functions on the top-1 accuracies ofthe bigger and the smaller networks. The quadratic function min{4t2, 1} is used as the base forcomparison.
