Table 1: Left: Average test error of flat-minima architectures and sharp-minima architectures. Flatminima consistently outperform sharp minima on all three datasets. Right: Kendall’s Tau (rankcorrelation) of the standard criterion f(α) (baseline) and our criterion g (f (N (α))). Our criteriongives a more accurate ranking of architectures on all three datasets.
Table 2: Test error of NA-RS and the standard random search (RS). NA-RS consistently outperformsRS on all three datasets under the same computational budget.
Table 3: Test error of NA-DARTS and DARTS on CIFAR-10, CIFAR-100 and ImageNet. OurNA-DARTS consistently outperforms DARTS on all three datasets.
Table 4: Comparison with state-of-the-art NAS methods on CIFAR-10 and CIFAR-100. Our NA- DARTS achieves the lowest test error on CIFAR-100. As all the architectures are searched on CIFAR-10, this shows that architectures found by NA-DARTS generalize better.					Method	Test Error (%)		Params (M)	Search Cost (GPU days)	Search Method	CIFAR-10	CIFAR-100			NASNet-A (Zoph et al., 2018)	2.65	17.10*	3.3	1800	RLAmoebaNet-A (Real et al., 2019)	2.84*	17.16*	3.2	3150	EvolutionPNAS (Liu et al., 2018)	2.95*	17.29*	3.2	225	SMBOENAS (Pham et al., 2018)	2.54*	17.18*	3.9	0.5	RLSNAS (Xie et al., 2019)	2.85 ± 0.02	18.25*	2.8	1.5	GradientP-DARTS (Chen et al., 2019)	2.50	16.55	3.4	0.3	GradientPC-DARTS (Xu et al., 2020)	2.57 ± 0.07	16.74*	3.6	0.1	GradientDARTS+ (Liang et al., 2019)	2.72*	16.85*	4.3	0.6	GradientDARTS 1st (Liu et al., 2019)	2.90 ± 0.25	17.66 ± 0.83	2.9	0.3	GradientDARTS 2nd (Liu et al., 2019)	2.70 ± 0.08	17.72 ± 0.61	2.9	1.0	GradientNA-DARTS (Ours)	2.63 ± 0.12	16.48 ± 0.13	3.2	1.1	Gradient* We train the reported architecture following the training setup in DARTS (Liu et al., 2019).					Table 5: Comparison with state-of-the-art NAS methods on ImageNet. Our NA-DARTS obtainsthe second lowest test error on ImageNet. We expect further improvement since our contribution isorthogonal to other extensions of DARTS (e.g., P-DARTS, PC-DARTS and DARTS+).
Table 5: Comparison with state-of-the-art NAS methods on ImageNet. Our NA-DARTS obtainsthe second lowest test error on ImageNet. We expect further improvement since our contribution isorthogonal to other extensions of DARTS (e.g., P-DARTS, PC-DARTS and DARTS+).
Table 6: Test error of architectures found from the S3 search space on CIFAR-10 and CIFAR-100. Top: our NA-DARTS significantly outperforms DARTS on both CIFAR-10 and CIFAR-100.
Table 7: Average error of flat-minima architectures and sharp-minima architectures. “CIFAR-10-Validation” refers to the average validation error on CIFAR-10 used in search. CIFAR-10, CIFAR-100 and ImageNet-16-120 refer to the average test error on each dataset. Flat minima and sharpminima obtain a similar validation error on CIFAR-10. However, flat minima consistently achieveslower test error than sharp minima on all three datasets.
Table 8: Kendall’s Tau (rank correlation) obtained by the standard criterion f (α) (baseline) and ourcriterion g (f (N(α))) with different choices of g(∙).
Table 9: Neighborhood variance and test error of architectures found by by the standard criterionf (α) (baseline) and our criterion g (f (N(α))) with different choices of g(∙). Architectures foundby the mean validation error (‘Ours - mean’) have a much smaller neighborhood variance than thosefound by the baseline criterion, and also achieve lower classification error on all three datasets.
Table 10: Ablation study on the aggregation function in NA-RS. mean and median yield the lowertest error among all the choices for g(∙).
Table 11: Ablation study on nnbr in NA-RS. Sampling a subset of neighbors (nnbr = 10) is a goodapproximation for the entire neighborhood (nnbr = 25).
Table 12: Ablation study of NA-DARTS.
