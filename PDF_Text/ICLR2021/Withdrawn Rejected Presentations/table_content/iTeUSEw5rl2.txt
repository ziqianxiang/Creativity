Table 1: Summary of the benchmarks used in our experiments	#src domains	#tgt domains	#total domains	#train imgs	#test imgs	#classesiAnimal	23	10	33	4,500	1,500	10iVehicle	28	11	39	4,000	1,290	9iDomainNet	3	3	6	205,141	88,204	345optimization process. First, the critics are trained to maximize the critic loss in Eq. 4, or Eq. 5. Inthe second stage, the classifier and the feature extractor are trained to correctly classify the currentdata as well as learning conditional invariant features. Importantly, the feature extractor θf is trainedto minimize the domain’s discrepancy measure while the critics are trained to maximize it, whichimplements the minimax game to learn conditional invariant features described in Sec. 4.1. Notably,calculating the critics loss Ld requires y(t) forward passes, however, only one backward pass isrequired to update the critics’ parameters. We provide the pseudo-code of both CIER-JSD andCIER-W in Appendix A.
Table 2: Evaluation metrics on the iVehicle and iAnimal benchmarks, respectively. All methods usethe same pre-trained Resnet18 backbone. Scenario 1 (significant shift): train on source domains,test on target domains. Scenario 2 (mild shift): train on source domains, test on source and targetdomains. Scenario 3 (no shift): train on target domains, test on target domains.
Table 3: Evaluation metrics on the iDomainNet benchmarks with five and 15 tasks, respectively. Allmethods use the same pre-trained Resnet18 backbone. Scenario 1 (significant shift): train on sourcedomains, test on target domains. Scenario 2 (mild shift): train on source domains, test on source andtarget domains. Scenario 3 (no shift): train on target domains, test on target domainsiDomainNet - 5 tasks				iDomainNet - 15 tasks		Scenario 1	ACC	FM	LA	ACC	FM	LAMER	13.32±0.15	9.66±0.23	20.92±0.24	12.52±0.21	21.74±0.21	32.82±0.18ER	15.06±0.18	9.88±0.52	22.96±0.27	12.60±0.22	22.56±0.23	34.62±0.24MIR	15.18±0.30	10.26±0.29	23.40±0.16	12.28±0.27	23.14±0.13	34.88±0.30AER	15.04±0.17	9.76±0.24	22.88±0.19	12.30±0.20	22.20±0.35	33.02±0.36CIER-JSD	15.88±0.20	10.52±0.10	24.26±0.23	13.34±0.09	22.28±0.30	34.90±0.24CIER-W	16.02±0.15	10.72±0.39	24.66±0.29	13.54±0.19	22.44±0.22	35.18±0.10Scenario 2	ACC	FM	LA	ACC	FM	LAMER	23.96±0.09	13.06±0.13	34.42±0.19	23.28±0.15	25.98±0.21	47.24±0.12ER	26.96±0.09	12.50±0.13	36.92±0.12	23.98±0.21	25.90±0.24	49.16±0.19MIR	26.88±0.18	12.68±0.24	37.04±0.18	23.80±0.10	26.72±0.10	49.70±0.10AER	26.62±0.90	12.08±0.90	36.28±0.13	22.68±0.10	26.52±0.13	47.44±0.04CIER-JSD	27.96±0.17	13.14±0.27	38.48±0.07	24.74±0.04	26.12±0.09	49.82±0.07CIER-W	28.16±0.18	13.24±0.36	38.74±0.19	24.92±0.22	26.04±0.29	49.92±0.09Scenario 3	ACC	FM	LA	ACC	FM	LA
Table 4: Model and computational complexity of CIER compared to ER and MIR. Computationalcomplexity is the averaged training time per incoming batch, all methods have very small stddev,i.e., smaller than 0.01sER	MIR	CIER-JSD	CIER-W# params Time # params Time # params Time # params TimeiVehicle	11,181,129	0.42s	11,181,129	0.53s	11,443,013	0.53s	11,380,562	0.48siAnimal	11,181,642	0.42s	11,181,642	0.54s	11,437,872	0.54s	11,381,332	0.48siDomainNet	11,353,497	0.57s	11,353,497	0.59s	11,816,612	0.87s	11,639,282	0.68sWe study the model and computational complexity of CIER, and report the number of parametersand the averaged training time per incoming data on all benchmarks in Table 4. Among all methods,we observe significant additional computational cost in the iDomainNet benchmark. The overheadincurs because of the larger batch size and the larger number of classes: 345 classes compared tonine in iVhehicle and 10 in iAnimal. In CIER-JSD, each class’s critic is a D-ways classificationmodel where D is the number of domains observed, which results in more expensive parameters andcomputational complexity. On the other hand, CIER-W’s critics only predict domains scores, whichare scalars. Thefore, CIER-W enjoys much lower complexities compared to CIER-JSD. However,8Under review as a conference paper at ICLR 2021the complexity overhead in both methods are not significant compared to ER and especially MIR.
Table 5: Evaluation metrics of two experience replay implementations on the iVehicle benchmarks.
Table 6: Evaluation metrics of two experience replay implementations on the iAnimal benchmarks.
Table 7: Evaluation metrics on the iVehicle and iAnimal benchmarks. All methods use the same pre-trained Resnet18 backbone. Scenario 1: train on source domains, test on target domains. Scenario2: train on source domains, test on source and target domains. Scenario 3: train on target domains,test on target domains. This is the full version of Table 2 in the main paperiVehicle - 3 tasks				iAnimal - 5 tasks		Scenario 1	ACC	FM	LA	ACC	FM	LALwF	24.10±1.60	55.86±1.44	61.38±1.80	16.44±1.60	78.44±2.79	79.18±1.31AGEM	23.76±0.43	69.68±1.32	70.18±1.19	15.34±1.01	85.96±2.05	84.14±1.25MER	48.12±1.12	22.58±2.56	63.20±1.43	46.38±1.00	31.50±2.05	71.62±1.41ER	48.70±0.90	23.22±3.15	64.26±1.31	45.76±0.88	34.12±1.46	73.08±1.64MIR	45.88±2.00	30.00±2.28	65.88±1.54	44.02±0.91	41.96±1.55	77.56±1.22AER	48.96±0.39	24.30±1.32	64.46±0.98	45.98±1.47	32.82±2.02	72.24±0.99CIER-JSD	51.52±0.59	19.62±1.98	65.78±1.28	49.04±0.97	31.20±1.16	74.00±1.53CIER-W	51.38±1.10	22.04±2.63	66.08±1.33	48.36±0.67	31.02±1.77	73.16±1.57Scenario 2	ACC	FM	LA	ACC	FM	LALwF	29.76±0.86	64.98±4.46	73.08±2.14	14.28±2.69	83.62±1.20	81.18±1.91AGEM	25.86±0.65	77.18±0.99	77.34±1.20	15.16±0.90	88.34±1.01	85.80±1.10MER	56.90±1.05	18.90±1.40	69.22±1.21	52.66±1.53	28.28±1.26	75.30±1.25ER	56.30±1.29	21.04±2.44	70.34±1.20	51.60±0.80	31.68±1.52	76.92±1.74MIR	55.46±0.88	25.24±1.34	72.30±0.34	49.16±1.52	40.52±1.32	81.58±0.95
