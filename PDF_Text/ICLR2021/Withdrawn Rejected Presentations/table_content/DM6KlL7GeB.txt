Table 1: Test error (%) for LeNet-5 on MNIST andVGG-7 on CIFAR-10. “Ann.” stands for annealingthe temperature τ in RQ.
Table 2: Top-1/Top-5 error (%) with ResNet-18 and MobileNetV2 on ImageNet.
Table 3: Test error (%) for quantized sub-networks using LeNet-5 on MNIST, VGG-7 on CIFAR-10,andResNet-18 on ImageNet. Here, an underline means the learned bit-width and “T” stands for ternary precision.
Table 4: Top-1/ToP-5 error (%) with ResNet-18 and MobileNetV2 on ImageNet using 4-bit. f denotes the useof the full-precision first or last layer, and ∣ indicates our own implementation with all layers quantized by usingpretrained models available from the official PyTorch repository.
Table 5: Test error (%) for quantized sub-networks using LeNet-5 on MNIST, VGG-7 on CIFAR-10,andResNet-18 on ImageNet. Here, an underline means the learned bit-width and “T” stands for ternary precision.
Table 6: Test error (%) for LeNet-5 on MNIST and VGG-7 on CIFAR-10.
