Table 1: Absolute reward as well as improvement (in terms of reward) of Offline PEARL+FT andMACAW after 0, 20k, and 200k additional environment steps are gathered and used for online finetuning. Standard errors of the mean over the 13 test tasks are reported in parentheses. Averages aretaken over 10 rollouts of each policy. We find that MACAW achieves both better out-of-distributionperformance before online training as well as faster improvement during online fine-tuning. Notethat Offline PEARL+FT experiences an initial drop in average performance on the test task after 20ksteps, compared with the performance of the policy conditioned only on the initial batch of offlinedata. A similar effect has been reported in recent work in offline RL (Nair et al., 2020).
Table 2: Hyperparameters used for the PEARL experiments. For the MuJoCo tasks, we generallyused the same parameters as reported in (Rakelly et al., 2019), with some minor modifications. Thedifferent parameters used for the MetaWorld ML45 environment are reported above.
Table 3: Hyperparameters used for the multi-task learning + fine tuning baseline. *For the Walkerenvironment, the value learning rate was 1e-5 for stability.
Table 4: Hyperparameters used for MACAW. The Standard Configuration is used for all experimentsand all environments except for Meta-World (due to the extreme difference in magnitude of rewards inMeta-World, which has typical rewards 100-1000x larger than in the other tasks). For the Meta-Worldconfiguration, only parameters that differ from the standard configuration are listed.
