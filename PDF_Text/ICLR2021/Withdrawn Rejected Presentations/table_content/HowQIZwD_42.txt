Table 1: Test Accuracy on CelebA. Group 1 and Group 2 are co-trained with the task(s) in theleft column. Group 1 most benefits from co-training with A3 while Group 6 most benefits fromco-training with A6 .
Table 2: Test Accuracy on MultiMNIST and MultiFashion classification datasets. The test accuracyis averaged over 10 samples. We report standard error, and best results are highlighted in bold. Theapproximation described in Section 4.1 is denoted with T Our results indicate the IT augmentationcan improve the performance of traditional MTL, uncertainty weights (Kendall et al., 2018), andPCGrad (Yu et al., 2020).
Table 3: 13-class semantic segmentation, depth estimation, and surface normal prediction resultson the NYUv2 validation dataset. Performance of (Split, Wide), (Split, Deep), Dense, and Cross-Stitch (Misra et al., 2016) as reported in (LiU et al., 2019). The symbol ^ denotes the approximationdescribed in Section 4.1dataset is composed of RGB-D images of indoor scenes and sUpports modeling of 13-class semanticsegmentation, trUe depth estimation, and sUrface normal prediction. We follow the procedUre of(LiU et al., 2019) and directly Utilize their framework to evalUate the performance of IT-MTL. ForcompUtational efficiency, we form J = {semantic + depth, semantic + normal, depth + normal,semantic + depth + normal} in IT-MTAN and J = {semantic + depth + normal, PCGrad gradi-ent} in IT-PCGrad. Table 3 sUmmarizes oUr experimental findings. We find IT-MTAN improvesmodeling performance across all measUrements for segmentation, depth, and sUrface normal tasksas compared with the MTAN baseline. IT-PCGrad and the approximation IT-PCGrad^ demonstratesimilar improvements when compared with the PCGrad-MTAN baseline. This resUlt indicates thebenefit of IT-MTL can hold for complex neUral network architectUres on a challenging real worlddataset.
Table 4: Chosen hyperparameters for MUItiMNISTzFaShion experiments.
