Table 1: Comparison of different sparsity inducing approaches. Removal and Growth refer thestrategies used for adaptive sparse connectivity. Fixed is possible if the parameter count is fixedthroughout training. Backward Sparse means a clear sparse backward pass and no need to computeor store any information of the non-existing weights.
Table 2: Single model perplexity on validation and test sets for the Penn Treebank language modelingtask with Stacked LSTMs and RHNs. FLOPs required to train the entire model and to test on singlesample are reported. Methods with superscript ‘*’ indicates reported results in the correspondingpapers. “Static-ER” and “Static-uni” are the static sparse network trained from scratch with ERdistribution and uniform distribution, respectively. “Small” refers the small-dense network.
Table 3: Performance comparison of all DST methods trained with Adam, momentum SGD andSNT-ASGD with stacked LSTMs on PTB.
Table 4: Model ablation of test perplexity for Selfish-RNN for stacked LSTMs, RHNs, ON-LSTM onPenn Treebank and AWD-LSTM-MoS on WikiText-2.
Table 5: A small experiment about the comparison among different cell weight redistribution methods.
Table 6: Single model perplexity on validation and test sets for the Penn Treebank language modeling task with ON-LSTM. Methods with “ASGD” are trained with SNT-ASGD. The numbers reported are averaged over five runs.				Table 7: Single model perplexity on vali- dation and test sets for the WikiText-2 lan- guage modeling task with AWD-LSTM- MoS. Baseline is AWD-LSTM-MoS ob-							tained from Yang et al. (2018). ods with “ASGD” are trained wit ASGD.			Meth- h SNT-Models	#Param	Val	Test				Dense1000	25M	58.29 ± 0.10	56.17 ± 0.12								Models	#Param	Val	TestDense1300	25M	58.55 ± 0.11	56.28 ± 0.19				SET	11.3M	65.90 ± 0.08	63.56 ± 0.14	Dense	35M	66.01	63.33DSR	11.3M	65.22 ± 0.07	62.55 ± 0.06	SET	15.6M	72.82	69.61SNFS	11.3M	68.00 ± 0.10	65.52 ± 0.15	DSR	15.6M	69.95	66.93RigL	11.3M	64.41 ± 0.05	62.01 ± 0.13	SNFS	15.6M	79.97	76.18RigL1000 (ASGD)	11.3M	59.17 ± 0.08	57.23 ± 0.09	RigL	15.6M	71.36	68.52RigL1300 (ASGD)	11.3M	59.10 ± 0.05	57.44 ± 0.15	RigL (ASGD)	15.6M	68.84	65.18Selfish-RNN1000 Selfish-RNN1300	11.3M 11.3M	58.17 ± 0.06 57.67 ± 0.03	56.31 ± 0.10 55.82 ± 0.11	Selfish-RNN	15.6M	65.96	63.05							Proposed by Shen et al. (2019) recently, ON-LSTM can learn the latent tree structure of naturallanguage by learning the order of neurons. For a fair comparison, we use exactly the same modelhyper-parameters and regularization used in ON-LSTM. We set the sparsity of each layer to 55%and the initial removing rate to 0.5. We train the model for 1000 epochs and rerun SNT-ASGDas a fine-tuning step once at the 500th epoch, dubbed as Selfish-RNN1000. As shown in Table6, Selfish-RNN outperforms the dense model while reducing the model size to 11.3M. Without
Table 8: Training FLOPs analysis of different sparse training approaches. fD refers to the trainingFLOPs for a dense model to compute one single prediction in the forward pass and fS refers to thetraining FLOPs for a sparse model. ∆T is the number of iterations used by RigL to update sparseconnectivity. st is the sparsity level of the model at iteration t.
