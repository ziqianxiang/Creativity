Table 1: MQTransformer encoder and decoderEncoder	Decoder Contextsht1 = TEMPORALCONV(y:t , x:t , r:t) ht2 = FEEDFORWARD(s) ht = [ht1 ; ht2],	(3)	Chs = HSATTENTION(h:t,r)	(4) cta = FEEDFORWARD(ht , r) a	hs	a Ct = [ct,i；…；Ct,H； Ct] ct,h = DSATTENTION(C：t, h：t, r),Our decoder incorporates our horizon specific and decoder self-attention blocks, and consists of twobranches. The first (global) branch summarizes the encoded representations into horizon-specific(Chh) and horizon agnostic (Ca) contexts. Formally, the global branch ct := mo(∙) is given by (4).
Table 2: Attention weight and output computations for blocks introduced in Section 3.4Block	Attention Weights		Output			Decoder-Encoder Attention	Ath,s = h qt = lʃ —— ks =	qth,>Wq>Wkks [ht; rt; rt+h] [hs; rs]	(5)	t hs	h ct,h =	At,svs s=t-L		(6)	vs =	hs				Decoder Self-Attention	Ath,s,r qt,h ks,r vs,r	= qt>,hWqh,>Wkhks,r = [ht ; cth,sh; rt ; rt+h] = [csh,sr ; rs ; rs+r] hs =cs,r	(7)	hs	h ct,h =	As,t,r (s,r)∈H(t,h) H(t, h) := {(s, r)|s + r	vs,r, = t + h}	(8)Horizon-Specific Decoder-Encoder Attention Our horizon-specific attention mechanism is amulti-headed attention mechanism where the projection weights are shared across all horizons. Eachhead corresponds to a different horizon. It differs from a traditional multi-headed attention mechanismin that its purpose is to attend over representations of past time points to produce a representationspecific to the target period. In our architecture, the inputs to the block are the encoder hidden statesand position encodings. Mathematically, for time s and horizon h, the attention weight for the valueat time t is computed as (5).
Table 3: Aggregate Quantile Loss MetricsModel	ALL LTSP	LTSP 0/4	Seasonal Peak 1	Post-Peak Rampdown	Promotion Type 1Baseline	1.000	1.000	1.000	1.000	1.000Dec-Enc	0.984	0.93 1	0.748	0.712	0.706Dec-Enc + Dec-Self	0.989	0.908	0.698	0.639	0.6704	Empirical Results4.1	Large-Scale Demand ForecastingFirst, we evaluate our architecture on a demand forecasting problem for a large-scale e-commerceretailer with the objective of producing multi-horizon forecasts that span up to one year. Eachhorizon is specified by a lead time (LT), number of periods from the FCT to the start of the horizon,and a span (SP), number of periods covered by the forecast, combination. To assess the effectsof each innovation, we ablate by removing components one at a time. MQTransformer is denotedas Dec-Enc & Dec-Self Att, Dec-Enc Att - which contains only the horizon-specificdecoder-encoder unit - and Baseline - the vanilla MQ-CNN model. MQ-CNN is selected as thebaseline since prior work2 demonstrate that MQ-CNN outperforms MQ-RNN and DeepAR on thisdataset, and as can be seen in Table 4, MQ-CNN similarly outperforms MQ-RNN and DeepAR onpublic datasets.
Table 4: P50 (50th percentile) and P90 (90th percentile) QL on electricity and retail datasets with the best results on each task emphasized. For the retail task, MQTransformer has results in parentheses, which correspond to training without forking sequences on 450K trajectories only.						Task	P50 QL						DeepAR	ConvTrans	MQ-RNN MQ-CNN		TFT	MQTransformerElectricity	0.075	0.059	0.077	0.076	0.055	0.057Retail	0.574	0.429	0.379	0.269	0.354	0.256 (0.2645)Volatility	0.050	0.047	0.042	0.042	0.039	0.039Traffic	0.161	0.122	0.117	0.115	0.095	0.101				P90 QL		Task	DeepAR	ConvTrans	MQ-RNN MQ-CNN		TFT	MQTransformerElectricity	0.040	0.034	0.036	0.035	0.027	0.027Retail	0.230	0.192	0.152	0.118	0.147	0.106 (0. 109)Volatility	0.024	0.024	0.021	0.020	0.020	0.019Traffic	0.099	0.081	0.082	0.077	0.070	0.068accurate as it fails to incorporate newly available information. By contrast, MQTransformer is bothless volatile and more accurate when compared with MQ-CNN. See Appendix D for more details onthe experiment setup and training procedure used.
Table 5: Parameter settings for Large Scale Demand Forecasting ExperimentsParameterValueEncoder Convolution Dilation Rates	[1,2,4,8,16,32]Position Encoding Dilation Rates	[1,2,4,8,16,20]Static Categorical	One-HotTime-Series Categorical	One-HotStatic Encoder Dimension	64Convolution Filters	64Attention Block Heads	52Attention Block Head Dimension	8Dropout Rate	0.15Activation Function	ReLUC.2 ResultsTables 6, 7, and 8 contain the full set of results on the large scale demand forecasting task. Per thediscussion in Section 4, we use MQ-CNN as the baseline model and we did not compare to TFTdue to scaling issues and the fact that on the public task that was most similar (retail), MQ-CNNsignificantly outperformed TFT.
Table 6: 52-week aggregate quantile loss metrics with for a set of representative lead times and spansModel	ALLLTSP		LTSP 3/1		LTSP 0/4		P50	P90	P50	P90	P50	P90Baseline	1.000	1.000	1.000	1.000	1.000	1.000Dec-Enc	0.984	0.960	0.950	0.927	0.963	0.931Dec-Enc & Dec-Self	0.989	0.984	0.934	0.911	0.948	0.908Model	LTSP 12/3		LTSP 0/33				P50	P90	P50	P90		Baseline	1.000	1.000	1.000	1.000		Dec-Enc	0.975	0.957	0.982	0.963		Dec-Enc & Dec-Self	0.960	0.964	0.982	0.981		Table 7: P90 quantile loss metrics on seasonal peak target weeksModel	S eas onal Seasonal Seasonal	Post-Peak Peak 1	Peak 2	Peak 3	RampdownBaseline Dec-Enc Dec-Enc + Dec-Self	1.000	1.000	1.000	1.000 0.748	0.817	0.962	0.712 0.698	0.826	0.931	0.639Table 8: P90 quantile loss metrics on item, weeks with promotionsModel	Promotion Type 1	Promotion Type 2	Promotion Type 3Baseline	1.000	1.000	1.000Dec-Enc	0.706	0.769	0.865Dec-Enc + Dec-Self	0.670	0.763	0.85115
Table 7: P90 quantile loss metrics on seasonal peak target weeksModel	S eas onal Seasonal Seasonal	Post-Peak Peak 1	Peak 2	Peak 3	RampdownBaseline Dec-Enc Dec-Enc + Dec-Self	1.000	1.000	1.000	1.000 0.748	0.817	0.962	0.712 0.698	0.826	0.931	0.639Table 8: P90 quantile loss metrics on item, weeks with promotionsModel	Promotion Type 1	Promotion Type 2	Promotion Type 3Baseline	1.000	1.000	1.000Dec-Enc	0.706	0.769	0.865Dec-Enc + Dec-Self	0.670	0.763	0.85115Under review as a conference paper at ICLR 2021D Experiments on Public DatasetsIn this section we describe the experiment setup used for the public datasets in Section 4.
Table 8: P90 quantile loss metrics on item, weeks with promotionsModel	Promotion Type 1	Promotion Type 2	Promotion Type 3Baseline	1.000	1.000	1.000Dec-Enc	0.706	0.769	0.865Dec-Enc + Dec-Self	0.670	0.763	0.85115Under review as a conference paper at ICLR 2021D Experiments on Public DatasetsIn this section we describe the experiment setup used for the public datasets in Section 4.
Table 9: Parameter settings of reported MQTransformer model on each public dataset.
