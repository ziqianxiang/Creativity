Table 1: Popular optimizers in the paper on CIFAR10, CIFAR100, ImageNet32 and Speech Com-mands Dataset. η denotes momentum, γ is the weight decay coefficient,β is from Kingma & Ba(2014) and Pis fromZeiler(2012).______________________________________________Optimizer	Learning rate	Other Parameter TuningAdam	{0.0001, 0.001, 0.01}	β = (0.9, 0.999)AdaDelta	1.0	P = 0.9SGD	{0.001, 0.01, 0.05, 0.1}	γ = 0.005SGDM	{0.001, 0.01, 0.05, 0.1}	η=0.9,γ=0.005SGDL	≥ 10 * Ir(SGDM)	γ = 0.005A.3.2 Speech Commands DatasetOn Speech Commands Dataset, Similarly, the learning rate is 0.001 for Adam and 1.0 for AdaDeltaas above. We choose 0.01 from {0.001, 0.01, 0.05, 0.1} as the best for SGDM and 0.15 for SGDL15 times larger than SGDM.
