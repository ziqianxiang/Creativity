Table 1: Comparison of the different algebras we consider. Weight Size represents the number ofcomponents in each tuple, for example a 2 Ã— 2 matrix has 4 components. Weight Reuse denotes howmany output tuple components each input tuple component is involved in. For example, in a M2 (R)matrix multiply, each weight component is involved in the update of two components.
Table 2: Enwik-8: A parameter-efficient Transformer-XL with performance that matches the classicTransformer-XL on Enwik-8. Switching to M2 (R)permits both increasing the number of layers andreducing parameters at the same time - again while keeping the validation bits per character on par.
Table 3: WikiText-103: We replace a real-valued GRU (and the corresponding linear layers) withAlgebraNet counterparts. We report the minimum validation loss over the last 5% of training. Thehidden size is reported as number of tuples, e.g. M2(R) with 512 tuples has 2048 scalars in total.
Table 4: For 50%, 70%, and 90% sparsity, we show the performance relative to the Frobenius norm for differentmagnitude-based tuple pruning criterion.
Table 5: Performance different from pruning components and entire tuples for M2 (R) and H-AlgebraNets.
Table 6: A comparison of different AlgebraNets on CIFAR-10. BN denotes Batch Normalization, W denotesthe use of whitening.
