Figure 1: Number of times ArXiv titles and abstracts mention each optimizer per year. All non-selected optimizers from Table 2 in the appendix are grouped into Other.
Figure 2: The absolute test set performance improvement after switching from any untuned optimizer(y-axis, one-shot) to any tuned optimizer (x-axis, small budget) as an average over 10 random seedsfor the constant schedule. We discuss the unintuitive occurrence of negative diagonal entries inAppendix F. The colormap is capped at ±3 to improve presentation, although larger values occur.
Figure 3: Lines in gray (—, smoothed by cubic splines for visual guidance only) show the relativeimprovement for a certain tuning and schedule (compared to the one-shot tuning without schedule)for all 14 optimizers on all eight test problems. The median over all lines is plotted in orange (—)with the shaded area (!) indicating the area between the 25th and 75th percentile.
Figure 4: Mean test set performance over 10 random seeds of all tested optimizers on all eightoptimization problems using the large budget for tuning and no learning rate schedule. One standarddeviation for the tuned ADAM optimizer is shown with a red error bar (I; error bars for other methodsomitted for legibility). The performance of untuned ADAM (▼) and ADAB OUND (▲) are marked forreference. The upper bound of each axis represents the best performance achieved in the benchmark,while the lower bound is chosen in relation to the performance of Adam with default parameters.
Figure 5: Performance of SGD on a simple multilayer perceptron. For each learning rate, markers inorange (M) show the initial seed which would be used for tuning, blue markers (M) illustrate nineadditional seeds with otherwise unchanged settings. The mean over all seeds is plotted as a blue line(一),showing one standard deviation as a shaded area (■).
Figure 6: Mean test set performance of all 10 seeds of RMSPROP (—) on all eight optimizationproblems using the small budget for tuning and no learning rate schedule. The mean is shown with athicker line. We repeated the full tuning process on all eight test problems using different randomseeds, which is shown in dashed lines blue (- -). The mean performance of all other optimizers isshown in transparent gray lines.
Figure 7: Mean test set performance of all 10 seeds of ADADELTA (—) on all eight optimizationproblems using the small budget for tuning and no learning rate schedule. The mean is shown with athicker line. We repeated the full tuning process on all eight test problems using different randomseeds, which is shown in dashed lines blue (- -). The mean performance of all other optimizers isshown in transparent gray lines.
Figure 8:	Illustration of the selected learning rate schedules for a training duration of 150 epochs.
Figure 9:	The absolute test set performance improvement after switching from any untuned optimizer(y-axis, one-shot) to any tuned optimizer (x-axis, small budget) as an average over 10 random seedsfor the constant schedule. This is a detailed version of Figure 2 in the main text showing the first fourproblems.
Figure 10: The absolute test set performance improvement after switching from any untuned optimizer(y-axis, one-shot) to any tuned optimizer (x-axis, small budget) as an average over 10 random seedsfor the constant schedule. This is a detailed version of Figure 2 in the main text showing the last fourproblems.
Figure 11: The absolute test set performance improvement after switching from any untuned optimizer(y-axis, one-shot) to any tuned optimizer (x-axis, large budget) for the constant schedule. This isstructurally the same plot as Figure 9 but comparing to the large budget and only considering theseed that has been used for tuning.
Figure 12: The absolute test set performance improvement after switching from any untuned optimizer(y-axis, one-shot) to any tuned optimizer (x-axis, large budget) for the constant schedule. This isstructurally the same plot as Figure 10 but comparing to the large budget and only considering theseed that has been used for tuning.
Figure 13: Mean test set performance over 10 random seeds of all tested optimizers on all eightoptimization problems using the small budget for tuning and no learning rate schedule. One standarddeviation for the tuned ADAM optimizer is shown with a red error bar (I). The performance of theuntuned versions of ADAM (▼) and ADAB OUND (▲) are marked for reference. Note, the upperbound of each axis represents the best performance achieved in the benchmark, while the lower boundis chosen in relation to the performance of Adam with default parameters.
Figure 14: Mean test set performance over 10 random seeds of all tested optimizers on all eightoptimization problems using the large budget for tuning and the cosine learning rate schedule. Onestandard deviation for the tuned ADAM optimizer is shown with a red error bar (I). The performanceof the untuned versions of ADAM (▼) and ADAB OUND (▲) are marked for reference (this time withthe cosine learning rate schedule). Note, the upper bound of each axis represents the best performanceachieved in the benchmark, while the lower bound is chosen in relation to the performance of Adamwith default parameters (and no schedule).
Figure 15: Mean test set performance over 10 random seeds of all tested optimizers on all eightoptimization problems using the large budget for tuning and the trapezoidal learning rate schedule.
Figure 16: Mean training loss performance over 10 random seeds of all tested optimizers on alleight optimization problems using the large budget for tuning and no learning rate schedule. Onestandard deviation for the tuned ADAM optimizer is shown with a red error bar (I). The performanceof the untuned versions of ADAM (▼) and ADAB OUND (▲) are marked for reference. Note, theupper bound of each axis represents the best performance achieved in the benchmark, while the lowerbound is chosen in relation to the performance of Adam with default parameters (and no schedule).
