Figure 1: Samples and latent visualization from a flow based model with a fixed Gaussian priorwhen the intrinsic dimension is strictly lower than the true dimensionality of the data space.
Figure 2: The data (black dots) lies on a 2D man-ifold in 3D space. To have a model PX withIndim(PX) = 2, we learn a prior PZ that is a2D Gaussian in 3D space and an invertible func-tion f which maps from PZ to PX .
Figure 3: (a) shows the samples from the data distribution Pd and our model PX . (b) shows thesample from the learned prior PZ and the distribution QZ. (c) shows the eigenvalues of AAT.
Figure 4: (a) S-curve data samples X 〜 Pd. (b) The latent representation Z = g(x), pointscan be observed to lie on a linear subspace. (c) Eigenvalues of the matrix AAT, we deduce thatIndim(Pd) = 2. (d) Our representation after the dimensionality reduction zproj = zE.
Figure 5: (a) and (b) show the samples from the data distribution and our model, respectively. (c)shows a traditional flow based model with a fixed Gaussian prior fails to generate any valid samples.
Figure 6: This figure shows the eigenvalues of the AAT after fitting the (a) synthetic MNIST withintrinsic dimension 5; (b) synthetic MNIST with intrinsic dimension 10; (c) original MNIST dataset.
Figure 7: Figure (a) and (b) show the (approximated) entropy value using two different training(b) Fading square datasetobjectives, for two different experiments.
Figure 8: (a) shows the samples from the data distribution. (b) and (d) show samples from a flowwith a fixed Gaussian prior and our method, respectively. (c) and (d) show the latent space in bothmodels. In (f), we plot the eigenvalues of the matrix AAT .
Figure 9: (a) and (c) shows the ground truth ‘density allocation’ on the manifold for two toy datasets,(b) and (d) shows the ‘density allocation’ learned by our models.
Figure 10: Figure (a) and (b) plot the data distribution PZ of the S-Curve dataset and the samplesfrom our model and a traditional flow with a fixed Gaussian prior. Figure (c) shows the representa-tion distribution QZ and the learned prior PZ of our model. In Figure (d), we plot the representationdistribution QZ using the flow with a fixed Gaussian prior.
Figure 11: Training data for the flow model. Figure (a) and (b) are synthetic MNIST samples fromtwo implicit models with latent dimension 5 and 10. Figure (c) are samples from the original MNISTdataset.
Figure 12: Samples from our methods. Figure (a) and (b) are samples flow models trained onsynthetic MNIST data with intrinsic dimension 5 and 10. Figure (c) are samples from a flow modelthat trained on original MNIST data.
Figure 13: Samples from traditional non-volume preserving flow models with fixed Gaussian prior.
