Figure 1: Example of inference of a naturally-trained DML model and robustly trained variant onthe CUB200-2011 dataset. Each model infer the class of the natural data point x, and its perturbedx0 counterpart, using the class of the nearest anchor nn(âˆ™). Green and red borders indicate Correct-(same class) and incorrect inference, respectively. Both models infer the natural input correctly,however, the naturally-trained DML fails to infer the adversarial perturbed input correctly.
Figure 2: Effects of adversarial perturbations in the embedding space (embedding shift, inference)for 16 randomly sampled data points. Circles mark embeddings of unperturbed data points, whilecrosses are adversarial perturbations for the respective data points. (first row) Grey lines connectthe unperturbed data points to their perturbed counterpart, to highlight the shift in the embeddingspace. (second row) Green and red lines connect the embedding of the respective adversarial datapoint to its nearest (and unperturbed) neighbor. If the line is green, this neighbor is of the same class(correct) while red is the opposite (wrong). It can be seen that embeddings of the robustly-trainedmodel shifts much less, when faced with adversarial perturbed input, and are thus more robust.
Figure 3: Metrics (Loss, R@1, and mAP@R) for contrastive loss training procedure across pertur-bation targets.
Figure 4: Metrics (Loss, R@1, and mAP@R) for triplet loss training procedure across perturbationtargets.
