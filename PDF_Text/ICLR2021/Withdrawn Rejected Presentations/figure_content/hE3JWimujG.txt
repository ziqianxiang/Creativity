Figure 1: Cortico-cerebellar networks as decoupled neural interfaces (CC-DNI). (a) The cerebellumapproximates forward (black arrows) or feedback (blue arrows) cortical processing, effectivelydecoupling the respective brain areas as done by DNI Jaderberg et al. (2017). Here we highlight thecase of unlocking feedback processing. The cerebellum attempts to approximate feedback signals(g2 = {g2 ,g2}) which can be purely spatial (g2) or temporal g2. This means that area 2 does not needto wait for the exact feedback signals. The cerebellum receives neural activity from area 2, whichproject (sparsely) through mossy fibers (cyan) onto granule cells (PC; orange) and Purkinje cells (PC;orange). Cerebellar learning is mediated by the inferior olive, which compares estimated feedback g2with real feedback g2, computing LC = ||g2 - g2∣∣ (see main text). Sparse connectivity onto GCS(connections and pruned connections denoted by solid and dashed cyan lines, respectively) representsthe sparseDNI model. (b) Illustration of the cerebellum approximating temporal feedback signalsprovided by BPTT. By approximating future feedback (e.g. at gt and gt+3) the cerebellum reducesthe need for strong BPTT. Adapted from Jaderberg et al. (2017). (c) To be trained with temporalfeedback the cerebellum bootstraps its own teaching signals. This is consistent with the modularstructure of the cerebellum, where the each module’s predictions are used by the cerebellum itself tospeed up cerebellar learning.
Figure 2: Target reach-ing task. (a) Trajectoryproduced by LSTM (grey),CC-DNI (orange) and sCC-DNI (cyan) models to-wards 7 targets when usingBPTT T = 2. (b) ModelsMSE. (c) Distance betweenmodel output and the tar-gets. (d) MSE for multipletruncations T over the last5 epochs. (e) MSE and cor-50ess≡)relation of hidden activity(rh) at epoch 1. (f) Correla-tion of CC-DNI estimatedgradients (r^) over the first100 epochs of learning.
Figure 3: Sequential MNIST tasks. (a) Illustration of the four variants of sequential MNIST(seqMNIST). Top left: Standard classification sequential MNIST (row-by-row). Top right: givenseqMNIST input the model is asked to reach a target coordinates (c-seqMNIST); Bottom left: givenseqMNIST input the model is asked to draw the digit following a template (dd-seqMNIST); Bottomright: as in dd-seqMNIST, but the model is asked to draw a straight line (ld-seqMNIST). Targetdrawings in dotted grey and model output coloured by digit. (b) Validation learning curves for thefour tasks with gradient truncation T = 5. For ld-seqMNIST, different degrees of loss sparsity γare shown (see main text). (c) Model performance across T for seqMNIST (top) and respectivedifferences between DNI and LSTM (bottom). (d) Model performance after 5 epochs as a functionof the Pearson correlation rh of the hidden activity at epoch 1 in the main network (dots; top).
Figure 4: Caption generation task. (a) A graphical description of the model. The image is firstprocessed by a pretrained convolutional neural network (CNN). The resulting feature vector is thenprovided to a connected but distinct RNN which is trained to predict the next word given the previouswords of a provided “gold standard” caption to the image. The cerebellum module C is only appliedto the RNN. (b) Top: Learning curves in bits per word (BPW) on validation set for gradient truncationT = 4. Full learning curve shown as inset. Bottom: standard language metrics (BLEU, Rouge-L,METEOR, CIDEr, SPICE). (c) Two example images from the validation set with correspondingmodel captions and gold standard captions (black). (d) Top: Change in loss with respect to LSTMas a function of T . Middle: Standard deviation σloss over the average loss across all epochs as afunction of T . Bottom: Change in loss (i.e. CC-DNIs - LSTM) as a function of caption length, wherethe cross designates the mean (gold standard) caption length.
Figure 5: Cerebellarsparsity (K) andexpansion (M/N)parameters in se-qMNIST (top) andc-seqMNIST (bot-tom). Grey, orangeand cyan boxes de-note LSTM, CC-DNI,sCC-DNI modelsrespectively.
Figure 6: CC-DNI ablation results. (a) (top)Learning curve for the target reaching taskwith multiple total cerebellar ablations atspecific epochs (vertical line). (bottom)Summary of average error normalized to thecontrol model, for both CC-DNI (orange)and sCC-DNI (cyan). * denotes LSTM case.
Figure S1:	Equivalent to Fig. 1 for how cerebellar inverse models map onto forward DNI (forward-CC-DNI). (a) In the inverse model the cerebellum generates motor predictions (hM) given desiredtarget responses (hS). This allows the brain to break forward locks (dashed black arrows), for exampleto go directly from sensory areas to higher areas. The inferior olive plays a similar role to (a,b) buthere it allows the cerebellum to learn estimations of forward activities (e.g. L = ||hS - hM ||). (b)Example of forward CC-DNI model in a non-motor domain. The cerebellum computes an associativeprediction hA given a sensory input hS . Multiple backward and forward DNI-cerebellum modulescan co-exist working in parallel. h and g variables represent forward and gradient information as inDNI (see main text).
Figure S2:	Synthetic gradients in RNNs with eligibility traces. (a) An example of backpropagationthrough time with truncation every T=3 timesteps (dashed black arrows). A DNI can be used tobridge the gap between truncations allowing gradients to be sent further back in time (δt and δt+3).
Figure S3: (a) Learning curves for target reaching as in Fig. 2 over different divergence ratios M/N,where M is the number of hidden ‘granular’ units in cerebellar model, N the number of input units,and number of non-zero input connections K. LSTM performance (grey) shown along with fully(orange) and sparsely (cyan) connected CC-DNI models. Note that N is fixed here as 2 × 10 = 20(10 LSTM units with gradients calculated for both cell and output states), hence * denotes fullconnectivity. The smaller x-axis in each subplot represents the epoch number and the smaller y-axisrepresents mean squared error. (b) Average performance over last five epochs (295-300) againstdivergence ratio M/N and input connection sparsity K for target reaching as in Fig. 2. (c) Same as(a) but for complex input (input dimension =28) and seven 2-D target coordinates (30 LSTM unitswith gradients calculated for both cell and output states). (d) Same as (b) for target reaching withcomplex input (input dimension = 28) and seven 2-D target coordinates over last 5 epochs (95-100).
Figure S4: Learning curves for all seqMNIST based tasks over different cerebellum divergence ratiosM/N, where M is the number of hidden ‘granular’ units in cerebellar model, N the number of inputunits, and number of non-zero input connections K. LSTM performance (grey) shown as a referencealong with fully (orange) and sparsely (cyan) connected cc-DNI models. Note that N is fixed hereas 2 × 30 = 60 (30 LSTM units with gradients calculated for both cell and output states), hence *denotes full connectivity. The smaller x-axis in each subplot represents the epoch number and they-axis represents performance over validation data (error for seqMNIST, MSE for the others).
Figure S5: Related to Fig. S4. Average performance over last five epochs (21-25) against divergenceratio M/N and input connection sparsity K for each seqMNIST based task. * denotes full connectiv-ity. The divergence ratio and connectivity of the default LSTM, CC-DNI and sCC-DNI models usedfor the seqMNIST tasks (see Fig. 3) are illustrated by the grey, orange and cyan squares respectively.
Figure S6:	Equivalent to Fig. 2f for population correlation rpop (see equation ??). (top) Effect ofcorrelation of hidden (cortical) activity h on performance for cc-DNI (orange) and sCC-DNI (cyan)on the target reaching task, where population correlation and performance are recorded during the firstand last (300) epoch respectively. (bottom) Evolution of the population correlation of the syntheticgradient g over the first hundred epochs.
Figure S7:	Equivalent to Fig. 3d for population correlation rpop (see equation ??). (top) Effect ofcorrelation of hidden (cortical) activity h on performance for cc-DNI (orange) and sCC-DNI (cyan)on the (standard) seqMNIST task, where population correlation and performance are recorded duringthe first and fifth epoch respectively. (bottom) Evolution of the population correlation of the syntheticgradient g over the first five epochs.
Figure S8: (a, b) Target reaching task as in Fig. 2 with varying number of targets. (a) Given aone-dimensional input the network has to learn to drive its output towards one of the 3 targets over 10timesteps. For this variant divergence ratio for CC-DNI is M/N = 4 and K = 4 for sCC-DNI. (b)Given a one-dimensional input the network has to learn to drive its output towards one of the 9 targetsover 10 timesteps. For this variant divergence ratio for CC-DNI is M/N = 2 and K = 4 for sCC-DNI. (c, d) Two variants on the target reaching task with varying input dimension, two-dimensional(i.e. X = {x1, x2}) and and 28-dimensional (Fig. S8d) (i.e. X = {x1, ..., x28}) input respectively.
Figure S9: Learning curves for the target reaching task as in Fig. 2b across different truncation valuesT shown for the three models LSTM (grey), CC-DNI (orange) and sCC-DNI (cyan). Pararametersare the same as in Fig. 2b with divergence ratio M/N = 4 and sparse input connectivity K = 4 forsCC-DNI.
