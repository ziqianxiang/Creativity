Figure 1: Overview of UCB-DrAC. A UCB bandit selects an image transformation (e.g. random-conv) and applies it to the observations. The augmented and original observations are passed to aregularized actor-critic agent (i.e. DrAC) which uses them to learn a policy and value function whichare invariant to this transformation.
Figure 2: Comparison between RAD and DrAC with the same augmentations, grayscale and randomconvolution, on the test environments of Chaser (left), Miner (center), and StarPilot (right). WhileDrAC’s performance is comparable or better than PPO’s, not using the regularization terms, i.e. usingRAD, significantly hurts performance relative to PPO. This is because, in contrast to DrAC, RADdoes not use a principled (importance sampling) estimate of PPO’s objective.
Figure 3: Cumulative number of times UCB selects each augmentation over the course of training forNinja (a) and Dodgeball (c). Train and test performance for PPO, DrAC with the best augmentationfor each game (color-jitter and crop, respectively), and UCB-DrAC for Ninja (b) and Dodgeball (d).
Figure 4: Average return on DMC tasks with natural video backgrounds with mean and standard de-viation computed over 5 seeds. UCB-DrAC outperforms PPO and RAD with the best augmentations.
Figure 5: Behavior of UCB for different values of its exploration coefficient c on Dodgeball. When cis too small, UCB might converge to a suboptimal augmentation. On the other hand, when c is toolarge, UCB might take too long to converge.
Figure 6: Screenshots of multiple procedurally-generated levels from 15 Procgen environments:StarPilot, CaveFlyer, Dodgeball, FruitBot, Chaser, Miner, Jumper, Leaper, Maze, BigFish, Heist,Climber, Plunder, Ninja, BossFight (from left to right, top to bottom).
Figure 7: Train performance of various approaches that automatically select an augmentation, namelyUCB-DrAC, RL2-DrAC, and Meta-DrAC. The mean and standard deviation are computed using 10runs.
Figure 8:	Test performance of various approaches that automatically select an augmentation, namelyUCB-DrAC, RL2-DrAC, and Meta-DrAC. The mean and standard deviation are computed using 10runs.
Figure 9:	DMC environment examples. Top row: default backgrounds without any distractors.
Figure 10: Average return on DMC tasks with default (i.e. no distractor) backgrounds with mean andstandard deviation computed over 5 seeds. UCB-DrAC outperforms PPO and RAD with the bestaugmentations.
Figure 11: Average return on DMC tasks with simple (i.e. synthetic) distractor backgrounds withmean and standard deviation computed over 5 seeds. UCB-DrAC outperforms PPO and RAD withthe best augmentations.
