Figure 1: Overview of our framework. ci,cj,ck labelled with different shapes are embeddingssampled from a shared distribution Ψ. si,sj,sk labelled with different colors are embeddings fromthe style latent space. fσ and fμ are two fully-connected layers predicting the statistics parametersto scale and shift Ψ respectively to approximate the target image distributions via a Generator. Foreach generated image from 3 × 3 grid, the content and style embeddings are from the column androw respectively.
Figure 2: Verification of our design for content and style. The topmost row and leftmost column pro-vide the content and style embeddings, respectively. A good disentanglement is that: horizontally,the style (identity) of the images maintain very well when the content (pose) varies, and vertically,the content (pose) of the images align very well when the style (identity) varies.
Figure 3: Generated images at different training steps. The first and second rows share the samestyle embedding. The second and third rows share the same content embedding.
Figure 4: Comparison of the disentanglement with different normalizations. Instance Normalization(IN) achieves better result on CelebA, e.g. the face identities are more alike with the query. L2normalization outperforms on Chairs, where the shapes of chairs are more consistent in each row.
Figure 5: Demonstrations of the latent spaces by interpolation (a & b) and retrieval (c-f).
Figure 6: Comparison with MUNIT (Huang et al., 2018b) and Park et al. (2020). MUNIT (Huanget al., 2018b) and Park et al. (2020) learn the texture information which is different from Ours (c).
Figure 7: Inference for unseen images. Our method performs well on images from different do-mains: painting and cartoon.
Figure 8: Comparison of visual analogy results on Chairs, Car3D and CelebA (from top to bottom).
Figure 9: 3D reconstruction results on Chairs. We generate multi-view from Input. Single: theobject reconstructed by only Input. Ours: the object reconstructed by multi-view inputs. GT: theobject reconstructed by the ground truth of multi-view inputs.
Figure 10: Details of network structure. For every upsampling layer, there is a 3 X 3 convolutionallayer following it. The embedding s1 and c1 from style and content embedding space respectivelyare first processed by the reparametric model R, then fed into the network in different ways. Thenthe image Ii is generated. The style and content latent space and network arejointly optimized underthe supervision of the reconstruction loss between synthesized image I1 and ground truth image I1from the dataset.
Figure 11: Comparison between Wu et al. and our method. For Wu et al., the images are mainlydetermined by content embeddings, while style embeddings only change the tone.
Figure 12: Ablation study. R indicates reparametric module.
Figure 14: Comparison with MUNIT (Huang et al., 2018b) and Park et al. (2020). MUNIT (Huanget al., 2018b) and Park et al. (2020) learn the texture information which is different from Ours (c).
Figure 13: 3D face reconstruction. Given an image, we first generate multi-view images and thenuse them as augmented input.
Figure 15: Performance of StyleGAN2 (Karras et al., 2020) on human faces. For StyleGAN2, thecontent contains entangled semantic attribute, such as pose, hair and glasses. In our case, the contentis pose, which is a high-level semantic attribute of the object.
Figure 16: Examples of translating shoes to edge (left column) and translating edges to shoes (rightcolumn). Triplet order (left to right) is: content, style, translation.
Figure 17: Results of modified model on merged cross-domain dataset based on Celeba and Anime.
Figure 18: Study on the influence of size of embeddings. For (a), we set the size of the styleembedding ds to be 128, the size of the content embedding dc to be 256. The content embeddingscontain shape of face, facial expression and pose. For (b), ds = 256 and dc = 256, and the contentembeddings contain shape of face and facial expression. For (c), ds = 256 and dc = 128, which isthe setting used in our paper, the content embeddings contain pose.
Figure 19: Comparison of visual analogy results on Market-1501 dataset. Our method outperformssupervised method Lord Gabbay & Hoshen (2020) and unsupervised method FactorVAE Kim &Mnih (2018) significantly.
Figure 20: Results on Celeba with 128 × 128 resolution. Zoom in for better view.
Figure 21: Results on the MNIST dataset. Content indicates geometric attributes, and style indicatestexture.
Figure 22: Results on the Cat dataset. Content indicates pose, and style indicates identity. The resultfurther qualitatively demonstrates the ability of our method to disentangle on real-world data.
Figure 23: Results on the Anime dataset. Content indicates pose, and style indicates identity.
Figure 24: More visual anology of our method on Car3D.
Figure 25: More visual anology of our method on Car3D.
Figure 27: More visual anology of our method on Chairs.
Figure 28: More visual anology of our method on Celeba.
