Figure 1: The effect of doubly variance reduction on training loss, validation loss, and mean-squareerror (MSE) of gradient on Flickr dataset using LADIES proposed in Zou et al. (2019).
Figure 2: Comparing the validation loss of SGCN and SGCN++ on real world datasets.
Figure 3: Comparison of doubly variance reduction and vanilla sampling-based GCN training onPPI dataset with SGD (learning rate 0.1) and Adam optimizer (learning rate 0.01). All other Config-urations are as default.
Figure 4: Comparing the mean-square error of stochastic gradient to full gradient and training lossof SGCN, SGCN+, SGCN++ in the first 200 iterations of training process on Reddit dataset.
Figure 5: Comparison of GPU memory usage of SGCN and SGCN++ on Flickr and PPI dataset.
Figure 6: Comparison of training loss, validation loss, and F1-score of SGCN++ with differentsnapshot gap on Reddit dataset.
Figure 7: Comparison of training loss, validation loss, and F1-score of SGCN+ with different snap-shot gap on Reddit dataset.
Figure 8: Comparison of training loss, validation loss, and F1-score of SGCN++ with differentsnapshot large-batch size on Reddit dataset.
Figure 9: Comparison of training loss, validation loss, and F1-score of SGCN+ with different snap-shot large-batch size on Reddit dataset.
Figure 10: Comparison of training loss, validation loss, and F1-score of SGCN++ with differentmini-batch size on Reddit dataset.
Figure 11: Comparing the validation loss and F1-score of GraphSAINT and GraphSAINT+ +with different mini-batch size on Reddit datasetFigure 12: Effectiveness of gradually increasing snapshot gap K during training on wallclocktime (second) and accuracy on PPI dataset. We choose snapshot gap K = 10 for fixed-K. Forincreasing-K, we choose snapshot gap K = 10 + 0.1 × s, s = 1, 2, . . ., where s is the number ofsnapshot steps.
Figure 12: Effectiveness of gradually increasing snapshot gap K during training on wallclocktime (second) and accuracy on PPI dataset. We choose snapshot gap K = 10 for fixed-K. Forincreasing-K, we choose snapshot gap K = 10 + 0.1 × s, s = 1, 2, . . ., where s is the number ofsnapshot steps.
Figure 13: Relationship between the two types of variance with the training process, where em-bedding approximation variance (zeroth-order variance) happens during forward-propagation andlayerwise gradient variance (first-order variance) happens during backward-propagation.
