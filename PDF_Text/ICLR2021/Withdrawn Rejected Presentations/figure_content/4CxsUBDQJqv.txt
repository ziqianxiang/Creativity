Figure 1: LISR: agents discover latent rewards as symbolicfunctions and use it to train using standard Deep RL methodsjective to construct a surrogate reward. While the additional density leads to faster convergenceof a policy, creating a surrogate reward fundamentally changes the underlying Markov DecisionProcess (MDP) formulation central to many Deep RL solutions. Thus, the learned policy may differsignificantly from the optimal policy (Rajeswaran et al., 2017; Ng et al., 1999). Moreover, theachieved task performance depends on the heuristics used to construct the dense reward, and thespecific function used to mix the sparse and dense rewards.
Figure 2: LISR: EA (bottom) and SR (top) learners sharea common replay buffer. A set of symbolic trees sampleobservations from this buffer and map them into scalar re-wards. SR learners also sample the same observations andthe corresponding reward to train using policy gradients. Thechampion policy (circled) is selected by ranking all policies,EA and SR, based on a fitness function.
Figure 3: Evolution of symbolic trees. The colored polygons represent basic mathematical operators.
Figure 4: Results on continuous control tasks in Mujoco. LISR outperforms all baselines excepton the low-dimensional problem in Swimmer. Curiosity, with access to explicit as well as implicitrewards, is unable to learn an effective policy on any environment. SR on its own, with access only tointrinsic rewards, is also unable to scale but slightly outperforms Curiosity .
Figure 5: Results on discrete control tasks in Atari (top) and Pygames (bottom). LISR outperformsall baselines in all environments. Curiosity ’s performance is overall significantly better on thesetasks compared to continuous control. SR on its own, with no access to environment provided denserewards, is able to completely solve the Atari environments. SR on its own also outperforms Curiosityon all but the Catcher environment.
Figure 6: Google Research Football environments used in the experimentsWe test LISR on 3 environment in the Football Academy set of environments - which describe specificgame scenarios of varying difficulty. Specifically, we consider the following scenarios.
Figure 7: Experiments on Google Research Football environments. Numbers in parentheses indicatethe number of players controlled by LISRFigure 7 shows the performance on the four scenarios we evaluated. On the simpler environmentsinvolving an empty goal, all three algorithms were able to find performant solutions in less than 5Mtime steps. For the more difficult scenarios in involving 3 players vs 1, IMPALA does outperformLISR. However, LISR is able to find competitive strategies compared to IMPALA in both scenarios.
Figure 8: Pixel-Copter environmentestimators. In contrast, Curiosity ’s ICM module that generates an intrinsic reward is implemented asthree neural networks with 5634 parameters.
Figure 9: An example of a discovered symbolic reward on PixelCopter. We unroll the correspond-ing symbolic tree into Python-like code that can be parsed and debugged. {si } represent stateobservations.
