Figure 1: The architecture of Conditional Masked Language Modeling (CMLM).
Figure 2: Language distribution of retrieved sentences. The first and third columns are mBERT andour models. Our model already in general has a more uniform distribution than mBERT. The secondand fourth columns are mBERT and our model with PCR.
Figure 3: Visualizations of sentence embeddings in Tatoeba dataset in 2D. The target languages areall English and the source languages are French, German, Russian and Spanish from left to rightcolumns. The first and second rows are our model and mBERT respectively.
