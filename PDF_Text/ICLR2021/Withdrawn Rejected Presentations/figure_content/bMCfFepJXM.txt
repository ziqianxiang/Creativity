Figure 1: Left: learning curve of pen-human-v0 task. Right: the average Q value of the first ensem-ble over the course of training. Both curves are smoothed by a factor of 20.
Figure 2: Figures of ablation study. Each setting is repeated for 4 random seeds. The curve is themean and the shaded area is the standard deviation. The curves are smoothed by a factor of 20. Thenumber of gradient steps per epoch is 2000. To make fair comparison, we only substitute KL diver-gence with MMD-based measurement with additional MMD-specific hyperparameter tuning. Theother design choices are different from (Kumar et al., 2019). Details can be found in Appendix D.
Figure 3: Histogram of the KL divergence of all the states in the dataset with global and state-wiseLagrange multiplier.
Figure 4: Fitting a regression model with out of distribution dataground truthw/o gpw/ gpA	Toy Example to Demonstrate bounding the value of the out ofDISTRIBUTION INPUTS VIA GRADIENT PENALTYTo demonstrate the effectiveness of the gradient penalty to enforce the bound of the predicted valuesof a regression model, we conduct experiments by fitting a regression model with in-distribution datawhile minimizing the norm of the gradient at out of distribution inputs. Specifically, we generatedataset {(xi,yi)}N==100 as Xi 〜U(-0.8,0.8), yi = 5sin(∏Xi) + ei, where G 〜N(0,0.5). Inaddition, We generate out of distribution dataset {X}M=100 as X 〜 U(—2, —0.8) ∪ U(0.8, 2). Wefit a regression model f, represented as a two-layer feed-forward neural network. The size of thehidden layer is 64 and the activation is RELU. We use Adam (Kingma & Ba, 2015) optimizer withlearning rate 0.01. In addition to the standard MSE loss, we add a gradient penalty term inspiredfrom (Gulrajani et al., 2017) such that the gradient at the out of distribution inputs is penalized. Theoverall loss function is:1N	1ML = N χ(fE) - y)2 + λ ∙ MMX IVxf(Xj)∣∣2	(11)In our experiments, we set λ = 0.1. We fit f with and without the gradient penalty term. Figure 4shows the predicted value at both in-distribution and out-of-distribution inputs. The results suggest
Figure 5: The black curve in the two figures represents the behavior distribution πb . The orange,blue and green curves in the graph show forward KL, backward KL and MMD with Laplacian kernelbetween πb and πθ = N(x, σ), respectively, where x is a variable between [-10, 10] and σ is fixed.
Figure 6: Histogram of the L2 norm of the gradients for out-of-distribution actionsOut-of-distribution generalization Lastly, we discuss the out-of-distribution generalization. Inoffline RL, we rate the learned policy with three levels. Level I policies are able to strictly followthe behavior policy. Level II policies are able to combine sub-optimal policies in the baheviorpolicy. Level III policies are able to generalize to out-of-distribution actions. To visualize the out-of-distribution generalization, we plot the KL divergence of the learned policy on the testing statedistribution in Figure 7. In the hopper-mixed and halfcheetah-medium task, there are periodic spikes,suggesting that the agents visit states which are not in the training distribution at test time. Althoughthe results suggest that such generalization is correct, our approach is unable to explicitly quantifyit. Thus, we leave it as future work.
Figure 7: The KL divergence between the learned policy and the behavior policy on the testing datadistribution. The x axis is the step in an episode.
Figure 8: The performance of the halfcheetah-medium-expert task on various choices of the targetentropy.
