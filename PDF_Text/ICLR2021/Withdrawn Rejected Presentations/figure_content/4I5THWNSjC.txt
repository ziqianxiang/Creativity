Figure 1: An overview of the BasisNet and more details can be found in Sec. 3.2. For easy images(e.g., distinguishing cat from dogs), lightweight model can give sufficiently accurate predictions thusthe second stage could be bypassed. For more difficult images (e.g., distinguishing different breedsof dogs), a specialist model is synthesized following guidance from lightweight model, which isgood at recognizing subtle differences to make more accurate predictions about the given images.
Figure 2: Accuracy-MAdds trade-off compari-son of the proposed BasisNet and MobileNet onImageNet validation set.
Figure 3: Prediction accuracy monotonically in-creases when more bases are added to the basismodels. Details in Appendix D.3.
Figure 4: Performance boostwith various regularizationson BasisNet-MV2. Combin-ing them altogether gives thelargest improvement.
Figure 5: MobileNet and BasisNet training using different regu-larizations. BasisNet uses MV2-0.5x as its lightweight model and8 MV2-1.0x for basis models. Input image resolutions vary from{128, 160, 192, 224}. Note that basis model dropout (BMD)is not applicable to MobileNet because it has only one model.
Figure 6: Early exiting can further reduce computation cost without sacrificing accuracy. (Left)Prediction accuracy is comparable for more confident predictions (e.g. top 40%), and the synthe-sized specialist consistently outperforms regular MobileNet in all buckets; (Right) Simulation ofBasisNet-MV3 with early exiting under varying threshold.
Figure 7: (A,C) Sample images from visually similar or distinct categories. (B) Mean coefficientweights at 15-th layer for selected categories. (D) t-SNE visualization of combination coefficients.
Figure 8: BasisNet-MV3 with lightweight model of different sizes (#MAdds).
Figure 9: Categories with highest mean coefficients for different basis models.
Figure 10: Visualization of predicted combination coefficients for similar categories over all layers.
