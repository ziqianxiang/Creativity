Figure 1: Samples of latent shift along the saliency lighting direction.
Figure 2: Top: images G(z); Middle: images after the shift G(z + hbg); Bottom: SalienCy masks3.2	Adaptation to the particular segmentation task.
Figure 3: Schematic representation of our approach.
Figure 4: Top: Images from the DUTS-test dataset. Middle: Groundtruth masks. Bottom: Masksproduced by the E-BigBiGAN method.
Figure 5: Examples of interpretable directions discovered in the BigBiGAN latent space.
Figure 6: Latent directions and saliency masks from the StyleGAN2 trained on the LSUN-Churchdataset.
Figure 7: Examples of mask improvement. Left: sample rejected by the mask size filter. Middle:sample rejected by the histogram filtering. Right block: mask pixels removed by the connectedcomponents filter are shown in blue and the remaining mask pixels are shown in red.
Figure 8: Failure cases of masks generation. Top: BigBiGAN samples; Middle: masks produced bylatent manipulations; Bottom: masks produced by the supervised saliency model.
Figure 9: Segmentation directions shifts. Left: BigBiGAN; center: BigGAN (Voynov & Babenko,2020); right: StyleGAN2Now we address the problem to find these affine A1 , A2 given a latent direction h of a generatorG. In addition, we also show how to rank h with respect to the suitability for a segmentation maskgeneration task.
Figure 10: Rows from top to bottom: generated image; shifted image; shifted image approximationby pixelwice operators A1, A2; mask generated by these apperators assignement.
