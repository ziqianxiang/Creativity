Figure 1: The architecture of the proposed MV-TER in the fusion decoding scheme. E and Drepresent the feature representation module and transformation decoding module respectively, andT is a specific task, e.g., 3D object classification or retrieval.
Figure 2: Classification accuracy with different la-bel rates.
Figure 3: The architecture of the proposed MV-TER in the average decoding scheme. E and Drepresent the feature representation module and transformation decoding module respectively, andT is a specific task, e.g., 3D object classification and retrieval.
Figure 4: 3D object retrieval examples on ModelNet40 dataset. Top 10 matches are shown foreach query, with mistakes highlighted in red.
Figure 5: Illustration of multiple views projected from 3D objects in the same posture: (a) Carand (b) Bowl. The four rows of (a) and (b) demonstrate multiple views projected from the 3D objectwith the following 3D transformations applied respectively: 1) the ground-truth 3D transformation;2) the estimated 3D transformation of the fusion decoding scheme; 3) the individually estimated 3Dtransformations tiâ€™s from each view during the average decoding scheme (with ti applied to the ithview); and 4) the finally averaged 3D transformation of the average decoding scheme.
Figure 6: Transformation estimation error from the average scheme.
Figure 7: Illustration of feature maps of multiple views projected from 3D objects before andafter transformation in the same category Airplane. (a) and (b) demonstrate multiple viewsprojected from the 3D object before and after transformations, respectively; (c) and (d) show thefeature maps of the corresponding views above.
Figure 8: Illustration of feature maps of multiple views projected from 3D objects before andafter transformation in different categories Sofa (left) and TV Stand (right). (a) and (b) demon-strate multiple views projected from the 3D object before and after transformations, respectively;(c) and (d) show the feature maps of the corresponding views above.
