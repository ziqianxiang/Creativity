Figure 1: Illustration of the transformation from class labels to similarity labels. Note that y stands forthe noisy class label and y for the latent clean class label. The labels marked in red are incorrect labels.
Figure 2: An overview of the proposed method. We add a pairwise enumeration layer and similaritytransition matrix to calculate and correct the predicted similarity posterior. By minimizing theproposed loss Lc2s, a classifier f can be learned for assigning clean labels. The detailed structuresof the Neural Network are provided in Section 4. Note that for the noisy similarity labels, some ofthem are correct and some are not. The similarity label for dogs is correct and the similarity label forcats is incorrect. In practice, the input data is original class-labeled data, and the transformation isconducted during the training procedure rather than before training.
Figure 3: Examples of predicted noisy similarity. Assume class number is 10; f (Xi) and f (Xj)are categorical distribution of Xi and Xj respectively, which are shown above in the form of areacharts. Sij is the predicted similarity posterior between two instances, calculated by the inner productbetween two categorical distributions.
Figure 4: Means and Standard Deviations of Classification Accuracy over 5 trials on MNIST, CIFAR10and CIFAR100 with perturbational ground-truth Tc.
Figure 5: Means and Standard Deviations (Percentage) of Classification Accuracy over 5 trials onMNIST, CIFAR10 and CIFAR100 with symmetric noise. Forward and Class2Simi are trained withestimated T. ForWard-TrUeT is trained with ground-truth class TC and ClaSS2SimLTrueT is trainedwith similarity Ts calculated from ground-truth class Tc .
Figure 6: Means and Standard Deviations (Percentage) of Classification Accuracy over 5 trials onMNIST, CIFAR10 and CIFAR100 trained with different sampling rate of training data. The noise rateon training data is set to Sym-0.5.
