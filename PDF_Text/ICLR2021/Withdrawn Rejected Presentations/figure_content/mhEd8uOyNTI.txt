Figure 1: Do Transformers build complexity along their layers? (a) The representation of a word is a functionof its context, and this cartoon illustrates an hypothesis that deeper representations use larger contexts. (b) Anexample parse tree, illustrating our notion of phrase complexity. (c) Cartoon of the distortion metric, wherevectors are the z-scored feature vectors z, and color map vectors to words.
Figure 2:	Swapping n-grams and phrases. (a) Examples of basic n-gram shuffles, where colors indicate theunits of shuffling. (b) Distortion metric computed at each layer, conditioned on n-gram size. Error bars hereafterrepresent standard error across 400 examples. (c) An example parse tree, with phrase boundaries shown asgrey brackets, and two low-order phrases marked; and examples of a phrasal and control swap, with colorscorresponding to the phrases marked above. (d) Distortion, computed at each layer, using either the full sentence,the subsentence of unswapped words, or the subsentence of swapped words, conditioned on swap type.
Figure 3:	Syntactic distance affects representational distortion. (a) An example of adjacent swaps which doand do not cross a phrase boundary, with low-order phrases colored. Phrase boundaries are drawn in red. (b)Distortion in each layer, but conditioned on the tree distance. (c) For each head (column) of each layer (row), the(Spearman) rank correlation between distortion and tree distance of the swapped words. Colors are such that redis positive, blue negative. (d) Histogram of PMI values, for pairs in the same phrase and not. (e) Similar to b,but averaging all out-of-phrase swaps, and separating pairs above (‘high’) or below (‘low’) the median PMI.
Figure 4:	Distortion and inference impairment for increasing linguistic complexity. In each plot, a point isthe average (distortion, ‘impairment’) for a given layer and a given class of word swap distance. Points areconnected by lines according to their swap type (i.e. tree distance). The circles are colored according to layer(see right for a legend). Averages are taken over 600 test sentences, with one of each swap type per sentence,and both distortion and log-likelihood are computed for every word in the sentence.
Figure 5: Attention provides a possible explanation for the trends we observe in distortion. (a) An exampleof the attention matrices for all heads in a single layer (layer 8), given the above sentence as input. Phrasesin the sentence are drawn as blocks in the matrix. (b) The rank correlation of attention vs. tree distance forall the heads/layers. (c) The rank correlation coefficients of distortion (y-axis) and attention (x-axis) againsttree distance, colored by layer. Marker size is proportional to 1 minus the p-value of the distortion/distancecorrelation. (d) A comparison of p-values on the distortion vs. tree distance correlation without (x-axis) andwith attention splines as covariates (y-axis).
Figure 6: Additional experiments and clarifications (by reviewer).
Figure 7: Distortion per layer in n-gram shuffling experiments, using different transformer architectures. Linesare the mean Frobenius distance, and the shading is ±1 standard error of the mean.
Figure 8: Distortion per layer in the adjacent word swapping experiments, using different transformer architec-tures. Lines are the mean Frobenius distance, and the shading is ±1 standard error of the mean.
Figure 9: Results from the pretrained BERT model on the n-gram shuffling and phrase swap experiments, usingalternative distortion metrics described in the text.
