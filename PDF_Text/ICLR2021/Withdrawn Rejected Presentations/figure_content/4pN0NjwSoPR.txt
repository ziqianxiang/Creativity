Figure 1: Compare methods in 4-bit low-precision quantization. (a) compares quantization levels between theuniform and nonuniform (power-of-two) (Miyashita et al., 2016; Zhou et al., 2017; Liss et al., 2018; Zhang et al.,2018a) quantizers, where x- and y-axis represent values before and after quantization respectively (the floatvalues are scaled between 0 and 1 for illustration). We highlight the dense region with higher “resolution” byarrows. We see that there is no dense region in uniform quantization because the intervals between levels are thesame, while a single dense region in power-of-two quantization. (b), (c) and (d) show the weight distributionsof different layers of a MobileNetv2 (Sandler et al., 2018) trained on ImageNet (Russakovsky et al., 2015),and the corresponding quantization levels learned by the proposed DDQ. We see that the weight distributionsare Gaussian-like in (b), heavy-tailed in (c), and two-peak bell-shaped in (d). DDQ enables learning arbitraryquantization levels with different number of dense regions to model these distributions.
Figure 2: Illustrations of DDQ. (a) compares computations of DDQ with the round operator. Unlike roundingmethods that only learn the stepsize d, DDQ treats q as trainable variable, learning arbitrary quantizationlevels. (b) illustrates that DDQ enables mixed-precision training by using different binary block-diagonalmatrix U. The circles in light and dark indicate '0’ and '1’ respectively. For example, when q is of length8 entries (i.e. 3-bit) and U = Diag (I2×2, •一,I2×2), where Diag(∙) returns a matrix with the desireddiagonal blocks while the off-diagonal blocks are zeros and I2×2 denotes a 2-by-2 matrix of ones, we haveq = UTq that enables DDQ to represent a 2-bit quantizer by averaging neighboring discrete values in q. Anotherexample is a 1-bit quantizer when U = Diag (I4×4, I4×4). (c) shows relationship between gating variablesg = {gi}b=ι and U. For example, when the entries of g = [1,0, 0] are arranged in an descending order and letS = pb=ι gi = 1 + 0 + 0 = 1, U has 2s = 21 = 2 number of all-one diagonal blocks. In such case, DDQ is as = 1 bit quantizer.
Figure 3: Learned quantization policy of each layer for ResNet18 and MobileNetV2 trained by DDQ onImageNet. DDQ learns to allocate more bits to lower layers and depthwise layers of the networks.
Figure 4: Training dynamics of quantization error in MObileNetV2. (a) compares the quantization errors ofPACT+UQ, PACT+PoT, and DDQ with/without gradient correction. DDQ with gradient correction shows stableconvergence and lower quantization errors than counterparts. (b) compares the converged quantization levels ofDDQ for each channel with/without gradient correction, and dense regions are marked by arrows. Here we canalso see that quantization levels learned by DDQ with gradient correction could fit original data distributionbetter.
Figure 5: Evolution of bitwidth of each layer when training ResNet18. We can see that DDQ can learn to assignbitwidth to each layer under given memory footprint constraints.
Figure 6: Training dynamics of fixed precision DDQ w/ (or w/o) gradient correction.
