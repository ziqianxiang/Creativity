Figure 1: Left: kW Conv k2 of WD training Right: top-1 accuracy for Algo 1 and WD training, bothlearning rates are found by gridsearchFrom Fig 1 left, the curve can be divided into two phases. In the first few epochs, kWConvk2decreases rapidly, which will increase the ELR according to the ELR hypothesis. However, thiseffect is duplicated with the learning rate warmup strategy, thus can be discarded. In the secondphase, kWConvk2 changes slowly in a relatively stable range. We suppose that in this phase, the maineffect is to keep ELR stable, while exactly following the trajectory of kW Conv k2 is not necessary.
Figure 2: Training ResNet50 on ImageNet with Algo 1, Algo 1@WN-FC and Algo 1@FixNorm-FC. Left: top-1 accuracy Top right: weight norm of the final FC layer (kWFCk2 for FC, g forWN-FC and FixNorm-FC) Bottom right: MCBRWe compare Algo 1 and Algo 1@WN-FC by training ResNet50 on ImageNet. As can be seen inFig 2 left, there is still a clear gap between Algo 1 and Algo 1@WN-FC. Since Algo 1@WN-FCalready preserves ELR, this gap implies that weight decay has additional effects beyond preservingELR on the final FC layer.
Figure 3: Ilustration of feature spacealization. To quantitively verify this explaination, we define Mean Cross-Boundary Risk(MCBR)4Under review as a conference paper at ICLR 2021for a batch of training samples:MCBR(B, W)1|B| ∙ (#Class — 1)cos(x, Wj — Wy)(x,y)∈B j 6=y(9)MCBR shows how much x is lean to the class boundaries, ranging from -1 to 1. We suppose that thelarger weight norm of the final FC layer(or g for WN-FC layer) will lead to higher MCBR and worsegeneralization performance. We compare these metrics for Algo 1 and Algo 1@WN-FC in Fig 2.
Figure 4: Top-1 accuracy for differ-ent lr and α. ResNet50 trained for 50epochsFor FixNorm training, however, lr and α should not be coupled since they control the two effectsseparately. To verify this, we grid search lr and α and show the corresponding top-1 accuracy in Fig4. It clearly shows that the optimal value of lr does not depends on the value of α and vice versa.
Figure 5: Left: search progress for FixNorm and BO+WD Middle: FixNorm search details forResNet50-D Right: FixNorm search details for MobileNetV2To better understand our method, we show more details about FixNorm-tune in Fig 5 middle andright for ResNet50-D and MobileNetV2, respectively. There are two lr searching rounds and oneα searching round according to our setups. In the first lr round, both experiments starts with lrvalues in [0.8, 1.4, 2.0, 2.6, 3.2] (which are uniformly sampled from initial range [0.2, 3.2]) andtrain for 1/5 total epochs(24 epochs and 30 epochs respectively). Best lr values are very differentin this round: lr1 = 2.6 for ResNet50-D and lr1 = 0.8 for MobileNetV2. Taking these valuesas new upper bound, [0.2, lr1] are further split, and corresponding lr values are evaluated in thesecond round, for full total epochs. The best lr values are 1.4 and 0.5 in this round. These valuesare fixed, and then α is searched. For ResNet50-D, the initial α = 0.5 is already the best, while forMobileNetV2 a better α = 16.0 is found. From Fig 5 one can find that the patterns match two priorsintroduced in section 2.4.
