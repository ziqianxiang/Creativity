Figure 1: DASL integrates user-provided expert knowledge with training data to learn DNNs. It achieves this bycompiling a DNN from knowledge, expressed in first order logic, and domain-specific neural components. ThisDNN is trained using backpropagation, fitting both the data and knowledge. Here DASL applies commonsenseknowledge to the visual relationship detection task. ∧ and → refer to ‘and’ and ‘implies’ connectives respectively.
Figure 2: Figure showing the results for the MNIST toy example with a plot of accuracy of digit classificationversus number of samples per class used for creating the unlabeled knowledge triplets. The labels With-knowledgeand No-knowledge denote whether the training included the knowledge-augmented unlabeled triplets satisfyingthe given modular arithmetic (see subsection 4.1). Ntr refers to the number of labeled training examples perclass (all refers to the entire training set). Best seen in color.
Figure 3: Comparison of DASL with LTN [Garcez et al. (2019)] and Rocktaschel et al. [Demeester et al. (2016)]for training a NN to classify MNIST digits with conjunctions and negated conjunctions over training samples.
