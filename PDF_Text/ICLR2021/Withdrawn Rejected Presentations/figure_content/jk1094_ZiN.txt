Figure 1: Predicting the Optimal Slope of the Sigmoid Activation. Each blue diamond depicts thenormalized validation accuracy (ground-truth) of a 2-layer, 100-wide feed-forward network with aunique sigmoid slope value (mean of 20 runs). The validation accuracy peaks at a slope of 0.23. Boththe Synthetic Petri Dish and a neural network surrogate model that predicts performance as a functionof sigmoid slope are trained on a limited set of ground-truth points, restricted to the blue-shadedregion to the right of the peak. The normalized performance predictions for Synthetic Petri Dish areshown with green diamonds and those for the NN surrogate model are shown as red diamonds. Theplot shows that the NN model predictions overfits the training data. In contrast, because the SyntheticPetri Dish conducts experiments with small neural networks with these sigmoid slope values it ismore more accurate at inferring both that there is a peak and its approximate location.
Figure 2: (a) Synthetic Petri Dish Training. The left figure illustrates the inner-loop and outer-looptraining procedure. The motifs (in this example, activation functions) are extracted from the fullnetwork (e.g a 2-layer, 100 wide MLP) and instantiated in separate, much smaller motif-networks(e.g. a two-layer, single-neuron MLP). The motif-networks are trained in the inner-loop with thesynthetic training data and evaluated using synthetic validation data. In the outer-loop, an averagemean squared error loss is computed between the normalized Petri dish validation losses and thecorresponding normalized ground-truth losses. Synthetic training and validation data are optimizedby taking gradient steps w.r.t the outer-loop loss. (b) Combining Architecture Search with thePetri Dish. Functions are depicted inside rectangles and function outputs are depicted as arrows withtheir descriptions adjacent to them.
Figure 3: RNN Cell Search for PTB: This graph plots the test perplexity (mean of five runs) of thebest found cell for four NAS methods across variable numbers of NAS iterations. All the methodsare warmed UP at the beginning (Step 0 in Figure 2b) with 40 ground-truth evaluations - notice thetop-left point with best test perplexity of 63.1. For Synthetic Petri Dish with RS (blue curve) andSynthetic Petri Dish with NAO (green curve), the top 20 motifs with the best predicted performanceare selected for ground-truth evaluations in each NAS iteration. Original NAO (Luo et al., 2018)(not shown here) requires 1,000 ground-truth evaluations to achieve a test perplexity of 56.0. NAOwith Reduced Data (red-curve) shows the results obtained by running original NAO, but with fewerground-truth evaluations (the same number the Synthetic Petri Dish variants get). With such limiteddata, Synthetic Petri Dish with NAO outperforms other NAS methods and achieves a test perplexityof 57.1 after 100 ground-truth evaluations.
