Figure 1: Federated learning across K agents equipped with local datasets and assisted by a centralserver: (a) in DVI agents exchange the current model posterior q(i)(θ) with the server, while (b) inDSVGD agents exchange particles {θn}N=ι providing a non-parametric estimate of the posterior.
Figure 2: Gaussian toy example with uniform prior and K = 2. Dashed lines represent local poste-riors, the shaded area represents the true global posterior, while the solid blue line is the approximateposterior obtained using a KDE over the particles.DSVGD schedules agent 1 and 2 at odd and evennumber of communication rounds i, respectively.
Figure 4: Accuracy for Bayesian logistic regression with (left) K = 2 agents and (right) K = 20agents as function of the number of communication rounds i (N = 6 particles, L = L0 = 200).
Figure 3: KL divergence be-tween exact and approximateglobal posteriors as functionof the number of rounds i(L = L0 = 200).
Figure 5: Log-likelihood forBayesian logistic regressionwith non-iid data distributions(N = 6, L = L0 = 200).
Figure 6:	Average RMSE as a function of the number of communication rounds i for regressionusing BNN with a single hidden layer of ReLUs with (left) K = 2 agents and (right) K = 20 agents(N = 20, L = L0 = 200, 100 hidden neurons for the Year Prediction and 50 for Kin8nm).
Figure 7:	Multi-label classification accuracy using BNN with a single hidden layer of 100 neuronsas function of i, or number of communication rounds, using MNIST and Fashion MNIST with (left)K = 2 agents and (right) K = 20 agents (N = 20, L = L0 = 200).
Figure 8: Reliability plots for classification using BNNwith variable number of hidden neurons using fashionMNIST (N = 20,1 = 10, L = L0 = 200, K = 20).
Figure 9:	KL divergence between exact and approximate global posteriors (a) as function of thenumber of communication rounds i for L = L0 = 200; and (b) as function of the local iterationsnumber L for I = 5.
Figure 10:	Performance comparison of (a) GVI, (b) PVI, (c) SVGD and (d) DSVGD for a multi-variate Gaussian mixture model. Solid contour lines correspond to the approximate posterior whiledashed contour lines to the exact posterior (N = 200, I = 5, L = 200 and b = 0.1).
Figure 11: Binary classification with Bayesian logistic regression results using the setting in Ger-shman et al. (2012): accuracy and log-likelihood for U-DSVGD (upper row) and DSVGD (bottomrow), along with NPV and SVGD, for various datasets.
Figure 12: Accuracy as a function of the number of particles N for Bayesian logistic regressionon the Covertype dataset: (a) comparison with various benchmarks summarized in Table 1, and(b) performance for different number of rounds I. L = 2000 iterations were used for centralizedschemes while I × L = 10 × 200 total local iterations were used for decentralized schemes.
Figure 13: Bayesian logistic regression log-likelihood with K = 2 and K = 20 agents usingthe setting in Gershman et al. (2012) comparing DSVGD to distributed (DSGLD) and centralized(SVGD and SGLD) schemes as function of the number of communication rounds i. We use N = 6particles and fix L = L0 = 200. FedAvg has been removed as it has a log-likelihood lower than -1in all cases and to allow us to focus on relevant values for DSVGD.
Figure 14:	Bayesian logistic regression accuracy for K = 2 (top row) and K = 20 (bottom row)agents using the setting in Gershman et al. (2012) comparing U-DSVGD and DSVGD to distributed(DSGLD) and centralized (SVGD and SGLD) schemes as function of the local iterations number L.
Figure 15:	Accuracy for Bayesian logistic regression with P-DSVGD, FedAvg, DSGLD and SVGDusing different datasets with K = 100 agents and a proportion of C = 0.2 randomly scheduledagents. SVGD was executed for L = C × 100 × 4000 iterations while we fix L = L0 = LS = 200total local iterations for the remaining schemes. We use N = 6 particles.
Figure 16: Accuracy for Bayesian logistic regression with K = 2 (top row) and K = 20 (bottomrow) agents under the setting in Gershman et al. (2012) as function of the number of communicationrounds i, or number of communication rounds (N = 6 particles, L = L0 = 200).
Figure 17:	Average Root Mean square Error (RMsE) as a function of the number of communicationrounds i, or number of communication rounds, for regression using Bayesian neural networks with asingle hidden layer of ReLUs under the setting of Hernandez-Lobato & Adams (2015), with K = 2(top row) and K = 20 (bottom row) agents. (N = 20, L = L0 = 200 and 50 hidden neurons).
Figure 18:	Log-likelihood for multi-label classification using Bayesian neural networks with a singlehidden layer of 100 neurons as function of the number of communication rounds i, or number ofcommunication rounds, using MNIST and Fashion MNIST with K = 2 (top row) and K = 20(bottom row) agents (N = 20, L = L = 200.
Figure 19:	Reliability plots for classification using Bayesian neural networks for a variable numberof hidden neurons with FedAvg (top row), SVGD (middle row) and DSVGD (bottom row). We useN = 20 particles (I = 10, L = L0 = 200 and K = 20 agents).
Figure 20:	Accuracy and Maximum Calibration Error (MCE) as function of the number of particlesN for Bayesian neural networks. We fix I = 10, L = L0 = 200 and K = 20 agents in both figures.
