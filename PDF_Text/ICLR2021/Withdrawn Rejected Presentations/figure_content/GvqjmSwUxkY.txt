Figure 1: Levels of supervision. To perform image-to-image translation, existing methods needeither (a) a dataset with input-output pairs or, (b) a dataset with domain information. Our method iscapable of learning mappings among multiple domains using (c) a dataset without any supervision.
Figure 2: Overview of our proposed method. The figure illustrates how our model changes thebreed of the cat. (a) An estimated domain from our guiding network E is used to train the multi-taskdiscriminator D. (b) E provides the generator G with the style code of a reference image and theestimated domain is again used for GAN training.
Figure 3: Qualitative comparison of translation results using each configuration in Table 1. Here, Breflects the style feature (e.g. species or type of food) of the reference images while A does not. Themodel C performs much worse than A and B in that it overly adopts the source image, not adequatelymerging styles and contents from both sides. The model D generates more plausible images than Cbut fails to reflect the characteristics of the reference images. For example, D on fifth row does notlook like several pieces of dumpling due to its shape and dish color, meaning that the reference stylesare not properly reflected. Similarly, E also fails to generate the dumpling in the fifth row. TUNITwith sequential training F reflects the visual features of each reference on both datasets. However,in terms of visual fidelity, we observe that G consistently outperforms F. Akin to the quantitativeresults, TUNIT achieves equivalent or even better visual quality than the set-level supervised modelA and B.
Figure 4: Reference-guided image translation results on unlabeled datasets.
Figure 5: t-SNE visualization of the style space of our guiding network trained on AFHQ Wild. SinceAFHQ Wild does not have ground-truth labels, each point is colored with the guiding network’sprediction. Although We set the number of domains to be quite large (KK = 10), the network separatesone species into two domains, which are so closely located that the model creates six clusters.
Figure 6: Qualitative comparison on the number of pseudo domains KK. The performance varies1	・ / F -r'z- YY Tl	/	1	F /	1 1along with K . When we set K large enough, the results are reasonable.
Figure 7: t-SNE visualization and representative images of each domain.
Figure 8: t-SNE visualization and representative images of each domain.
Figure 9: t-SNE visualization and representative images of each domain.
Figure 10: t-SNE visualization and representative images of each domain.
Figure 11: AFHQ Cat, unsupervised reference-guided image-to-image translation results of FUNITand TUNIT. The content and the style are from the source and the reference, respectively. WhileFUNIT usually fails to reflect the style of the reference image, TUNIT generates the fake imageswith the style - color, fur texture.
Figure 12: AFHQ Wild, unsupervised reference-guided image-to-image translation results of FUNITand TUNIT. FUNIT rarely reflects the correct style of the reference image - the species, on the otherhand, TUNIT translates the source image to the correct species.
Figure 13: LSUN Car, unsupervised reference-guided image-to-image translation results of FUNITand TUNIT. While TUNIT generates plausible and changes the color of the source image to that ofthe reference image, FUNIT not also generates unrealistic image but also fails to changes the color.
Figure 14: FFHQ, unsupervised reference-guided image-to-image translation results of FUNIT andTUNIT. Our model, TUNIT can remove or add the glasses to the source while preserving the identitybetter than FUNIT. In addition, TUNIT can change the hair color (last column) and the hair style -especially, bang (fifth column). Itis hard to specify the definition of domains in the results of FUNITwhile domains of TUNIT are more interpretable.
Figure 15: AnimalFaces-10, unsupervised image-to-image translation results.
Figure 16: AFHQ Cat, unsupervised image-to-image translation results.
Figure 17: AFHQ Dogs, unsupervised image-to-image translation results.
Figure 18:	AFHQ Wild, unsupervised image-to-image translation results.
Figure 19:	FFHQ, unsupervised image-to-image translation results.
Figure 20:	LSUN Car, unsupervised image-to-image translation results.
Figure 21: Summer2Winter (S2W), unsupervised image-to-image translation results.
Figure 22: LPIPS and FID of models and their status.
