Figure 1: Schematic diagram of LSH. For an input,we compute hash codes and retrieve candidatesfrom the corresponding buckets.
Figure 2: Schematic diagram of our proposal for LSH-label scheme. 1) We first construct hashtables for the label vectors. The label vectors are the weights of the connections from a label to thepenultimate layer. In the figure, e.g. label vectors for node 1 (blue node) is a concatenation of itsconnection weights to the penultimate layer (blue lines). 2) For a training sample, we query the LSHtables with the true label weights (red lines) and obtain negative samples (blue and green nodes). Wecall the retrieved samples ‘hard’ negatives because they are very similar to the ‘true’ ones but aresupposed to be ‘false’.
Figure 3: HoW the negative SmaPling distribution (target), uniform negative sampling and LNS adaptsover iterations. Initially, when there is no learning all the sampling is close to uniform (Left Plots).
Figure 4: Comparison of our proposal LNS with two schemes (LSH label and LSH embedding)against full softmax, topK softmax, frequency based softmax, sampled softmax, NCE and D-softmaxfor all three datasets. Left Column: Precision@1 vs time, Right Column: Precision@1 vs iteration,Top Row: WikiLSH-325K dataset Middle Row: Amazon-670K dataset Bottom Row: Text8 dataset.
