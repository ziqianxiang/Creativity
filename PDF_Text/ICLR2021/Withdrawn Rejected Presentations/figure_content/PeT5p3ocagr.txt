Figure 1: Flat gradient and population-based search on piece-wise constantfunctionwhich includes “many Wide flat regions” since the gradient (Vθ J) is near zero at a flat point. Figure1 is an extreme case consisting of only flat regions, which is called a piece-wise constant function.
Figure 2: Architecture of TD3 and CEM. θi is sampled from N(μ, Σ), λi is a weight depending onthe rank of the Returni , and K is a fixed number about high performing actors.
Figure 3: Architecture of PGPS. π is used instead of πθ for the simplification.
Figure 4: Learning curves obtained on 5 continuous control tasks.
Figure 5: Learning curves of TD3with(out) EA guidance on Swimmer-v2even if both deceptive and general gradient occurs in the environment since It lets CEM lead TD3when a deceptive gradient occurs and lets TD3 lead CEM when the general gradient occurs.
Figure 6: Average Return of filtered ac-tors and sampled actors during runningThe effect of Q-critic filtering The proposed algorithmselects half of the population using Q-critic (Q-filter), andthe remaining half is sampled from a multivariate Gaussian(Random). Figure 6 shows the average Return of Q-filterand Random in HalfCheetah-v2. Q-filter shows a 7.7%higher average performance than Random. (Because theevaluation steps were controlled, the average Return inFigure 6 is lower than the test episode return in Figure 4.)This result is additional evidence that Q-critic filtering ismeaningful for sorting out promising actors. The remainingconcern is the effect of Q-critic filtering on the exploration.
Figure 7: The graphs represent the change of average episodic discounted reward average episodicreturn of a linear policy actor by the value change of a single parameter in the Swimmer, where theother parameters were fixed and we evaluated thirty times at different seeds for each policy.
