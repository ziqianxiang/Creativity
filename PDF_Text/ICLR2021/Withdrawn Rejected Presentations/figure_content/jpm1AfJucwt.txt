Figure 1: ∆L(θ, ∆θ) for different number of pruning stages π, as a function of λ, the step sizeconstraint strength, using either (left) LM, (middle) QM or (right) OBD criteria. MP, which isinvariant to λ and to the number of pruning stages, is displayed in dashed black. The curves arethe mean and the error bars the standard deviation over 5 random seeds. OBD with π = 1 andλ = 0 diverged for all of the 5 seeds. Increasing the number of pruning stages drastically reduces∆L(θ, ∆θ). A λ > 0 can also help improving performances. Figure 6 in Appendix contains thesame plots, but displaying the validation gap before/after pruning.
Figure 2: Linear vs exponential schedule using QM on VGG11. Left: ∆L(θ, ∆θ) vs sparsity,zoomed on the end. Markers denote the 14 pruning stages. Middle: ∣∆θ∣2 at each stage. Right:Same as Figure 1, but comparing exponential (solid) and linear (dotted) schedules at 95.6% sparsity,With π ∈ {14, 140}. We get smaller ∣∆θ∣2 per pruning stage When using exponential instead oflinear schedule, resulting in a smaller ∆L(θ, ∆θ). It is advantageous to use that schedule When thepruning budged is limited, i.e. When π is small. This advantage vanishes for larger values of π.
Figure 3: Gap of validation error after fine-tuning as a function of ∆L(θ, ∆θ). Each point is oneexperiment, i.e. one one random seed, one π and one λ. ρ is the Spearman’s rank correlationcoefficient computed on all the data points. Except for the MLP on MNIST, there is only a weakcorrelation between ∆L(θ, ∆θ) and the gap of validation after fine-tuning. Thus, the performanceafter pruning cannot be explained solely by the loss-preserving abilities of the pruning criteria.
Figure 4: Same as Figure 1 and Figure 3, for the ResNet50 on ImageNet, with a sparsity of 70 %.
Figure 5: Same as Figure 1, but using equally spaced pruning steps. Note the difference in number ofpruning iterations.
Figure 6: Same as Figure 1, but displaying the validation error gap before fine-tuning. With propernumber of pruning stages and step size regularization, LM and QM can produce pruned networksthat are drastically better than the ones pruned using MP.
Figure 7: Same as Figure 1, but displaying the validation error gap after fine-tuning.
Figure 8: Same as Figure 7, but using equally spaced pruning steps. Note the difference in number ofpruning stages.
Figure 10: Same as Figure 3, but showing L(θ m) after fine-tuning as a function of ∆L(θ, ∆θ).
Figure 9: Same as Figure 3, but showing the validation error gap after fine-tuning as a function ofthe validation error gap before fine-tuning. Networks with drastically different performance beforefine-tuning can still produce similar performances after fine-tuning.
Figure 11: Same as Figure 3, but for different sparsity levels on the VGG11 on CIFAR10. When thesparsity is low, the network has enough capacity to return to its original performances after fine-tuning.
Figure 12: Same as Figure 11 (left), but zoomed on smaller values of ∆L(θ, ∆θ).
Figure 13: Left: Using the same hyper-parameters for fine-tuning as the ones of the originaltraining. Right: Performing hyper-parameters optimisation for the fine-tuning. This figure showsthat optimizing the hyper-parameters for fine-tuning can improve the performances of the networkafter pruning. However, it reduces the correlation between the performances after fine-tuning and∆L(θ, ∆θ). The lack of correlation can thus not be explained by poor fine-tuning hyper-parameters.
Figure 14: Same as Figure 3, but zooming on the best performing networks in terms of ∆L(θ, ∆θ).
Figure 15: Fine-tuning losses (dotted is training, solid is validation) of networks pruned using MPand QM criteria. All the curves are the average over the 5 seeds. We do not show the standarddeviation for clarity. Left: MLP, middle: VGG11 and right: PreActResNet18. Except for MNIST, thedifference in loss right after pruning (i.e. at epoch 0) disappears after one epoch of fine-tuning.
Figure 16: Same as Figure 3 showing GraSP and SynFlow on VGG11 (left) and the PreActResnet18(right). This Figure shows that one can observe a large ∆L(θ, ∆θ) and yet obtain very goodperformance after fine-tuning. This is especially true in the case of GraSP for VGG11. Furthermore,we can observe similar behaviour on PreActResNet18 where two different methods can lead tosimilar performance after fine-tuning while having completely different ∆L(θ, ∆θ): GraSP with∆L(θ, ∆θ) ≈ 108 has fine-tuning performance similar to MP with ∆L(θ, ∆θ) < 101.
Figure 17: Same as Figure 4, but with 90 % sparsity. Increasing the number of pruning stagesand constraining the step size reduce ∆L(θ, ∆θ). However, the best-loss preserving criteria, whichmaximize the validation accuracy right after pruning, do not produce better networks after fine-tuning.
