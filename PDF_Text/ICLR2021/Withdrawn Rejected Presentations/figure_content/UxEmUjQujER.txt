Figure 1: Average power spectrum of a large-scale GAN trained on a fractal-based dataset clearlyreveals how low frequencies (closer to center) are matched much more accurately than the high fre-quencies (closer to corners). (Left) Average power spectrum of randomly rotated Koch snowflakesof level 5 and size 1024 X 1024. (Right) Average power spectrum of StyleGAN2 trained on thelatter. A representative patch from the perimeter of true and generated fractals are also displayed.
Figure 2: (Left) Dashed blue line shows the predicted correlation between diagonally adjacent SPa-tial frequencies in Eq. (5) for different layers (dl) given a fixed filter size (kl = 5), and the orangepoints show its empirical evaluation (average correlation with one standard deviation computed overthe filters of WGAN-GP trained on CelebA). (Right) The impulse response spectrum of effectivefilters operating on each layer in WGAN-GP trained on CelebA. Notice how the spectrum of theeffective filters that operate on inner layers, i.e. control the generation of low frequencies (top row),are much sharper than the ones that operate on outer layers, i.e. control the generation of high fre-quencies (bottom row). Smoothness is an indication of larger correlation in the effective filters.
Figure 3: FID Levels of GANs trained on CelebA and LSUN-Bedrooms. The farther to the righton the horizontal axis, the more low frequencies are removed prior to FID computation. Notice thetransient increase in FID (worsening performance) as low frequencies are removed. All plots willeventually decline to zero if we continue removing frequencies. In all figures, the blue curve depictsthe True FID Levels of the corresponding dataset as a baseline. All figures show average FID withone standard deviation error bars (dashed line), over three random training runs.
Figure 4: (Top) Two samples from WGAN-GP when enhanced by adding multiple FSGs. In eachsample, from left to right, the outputs correspond to the FSG with (Ut,^t) at (J,0), (0,七),(-16, ι6), and (十, 七), the WGAN-GP,s main generator, and the final compound output (SUm ofall the preceding generators). Notice how each FSG has learned to focus on specific spatial frequen-cies, without any explicit supervision during training. See Appendix for more samples. (Bottom)Improvement in the FID Levels of GANs when enhanced by multiple FSGs, trained on CelebA.
Figure 5: Each column corresponds to one sample of a WGAN-GP enhanced by adding multipleFSGs, trained on CelebA. The first four rows, from top to bottom, show the outputs of the FSGswith (Ut, Vt) at (ι16, 0), (0,116),(-专,右)，and (去,右)respectively. The fifth row shows the maingenerator, and the final row the output of the compound generator (sum of all the preceding rows).
Figure 6: Each column corresponds to one sample of a PG-GAN enhanced by adding multipleFSGs, trained on CelebA. The first four rows, from top to bottom, show the outputs of the FSGswith (Ut, Vt) at (116, 0), (0, R), (-R,焉)，and (R,*)respectively. The fifth row shows the maingenerator, and the final row the output of the compound generator (sum of all the preceding rows).
Figure 7: Each column corresponds to one sample of a StyleGAN2 enhanced by adding multipleFSGs, trained on CelebA. The first four rows, from top to bottom, show the outputs of the FSGswith (Ut, Vt) at (16, 0), (0,16), (-16,16), and (-16, -16) respectively. The fifth row shows the maingenerator, and the final row the output of the compound generator (sum of all the preceding rows).
Figure 8: WGAN-GP random samples compared to WGAN-GP-FSG on SCelebA and SBedrooms.
Figure 9: PG-GAN random samples compared to PG-GAN-FSG on SCelebA and SBedrooms. Sam-ples are re-shifted for visualization.
Figure 10: StyleGAN2 random samples compared to StyleGAN2-FSG on SCelebA and SBedrooms.
