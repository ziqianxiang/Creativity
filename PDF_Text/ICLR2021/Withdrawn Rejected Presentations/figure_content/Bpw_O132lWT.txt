Figure 1: Trace of covariance matrix of gradient noise in a region around local minimum w*. w*is selected by running gradient descent with small learning rate till it converges. The number athorizontal axis shows the distance of the point away from w*. (a),(b): Results for plain CNN.
Figure 3:	Approximating distribution of parameters (trained by SGD) by power-law dynamic.
Figure 4:	(a):Loss surface of L(w) for 2-D model. (b):Trace of covariance matrix around minimum(1, 1). (c)/(d): Success rate of escaping from the basin of L(w) / 0.9L(w) in repeated 100 runs.
Figure 5: Probability density for power-law dynamic.
Figure 6: A close-up of right tail distribution of the result for ResNet18 in Figure. 3(a), Which couldhelp to observe the heavy-tailed properties among different batchsize.
Figure 7: Approximating distribution of parameters (trained by SGD) by poWer-laW dynamic. ThesenetWorks use pre-trained models offered by PyTorch, and all of them are pre-trained on ImageNetdataset using SGD. The second line in each title shoWs Top-5 test error and the third line shoWsapproximated tail-index Îº.
Figure 8: Comparison between Q-Q plots of network parameters versus normal distribution andpower-law distribution. (upper): Q-Q plots of parameters versus normal distribution. (bottom):Q-Q plots of parameters versus power-law distribution.
Figure 9: Escaping experiment on corrupted FashionMNIST. Test accuracy versus iteration afterpretraining by GD. Model is pretrained by GD before the vertical dashed line and continued by GD,GLD and PLD (ours). Numbers in brackets are expected sharpness after model converging.
Figure 10: (a): Loss curve L(w) for 1-D model. (b): Mean escaping time versus different barrierheights. Mean escaping time is computed by average on 100 rounds, in which we record the numberof iterations when firstly escaping from the saddle point.
