Figure 1: Simple cliff-walk domain. Theagent starts in the bottom left, has to avoidthe cliff depicted in black and reach the statein the bottom right. a) depicts the transitionprobabilities of an optimal policy while b) de-picts a sub-optimal policy learned using HER.
Figure 2: Average and range of success rate across 3 seeds of TD3, UVD and HER on variations ofthe Fetch manipulation domains. FetchPush and FetchSlide correspond to the original domains. Fetch-Push (Sparse) shows the advantage of hindsight updates when the required precision is significantlyhigher while FetchSlide + Noise shows the effect of hindsight bias in stochastic domains.
Figure 3: Comparison of GAIL and VDI on variations of standard benchmark tasks. The x-axisshows the number of demonstrated trajectories times state-action pairs per trajectory. VDI requiresfewer demonstrations compared to GAIL.
Figure 4: Individual learning curves for each imitation learning experiment.
