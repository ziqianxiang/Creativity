Figure 1: The proposed PR-Net avoids solving integral problems by learning a regression modelthat conforms with a learned governing equation.
Figure 2: A neural network predicts solutionvalues at d, t given initial conditions, denotedh(d, 0) for various d, and a governing equation.
Figure 3: The general architecture and the training algorithm of PR-NetTraining Algorithm. Our overall training algorithm iS in Alg. 1. We alternately train θ, (d, t) ∈H, and αi,j for all i, j. The forWard problem to train θ becomeS a Well-poSed problem (i.e., itSSolution alWayS exiStS and iS unique) if the neural netWork f iS analytical or equivalently, uniformlyLipSchitz continuouS (Chen et al., 2018). Many neural netWork operatorS are analytical, Such aSSoftpluS, fully-connected, and exponential. Under the mild condition of analytical neural netWorkS,therefore, the Well-poSedneSS can be fulfilled. The inverSe problem can alSo be uniquely Solved inmany caSeS due to the SparSeneSS requirement. AS a reSult, our propoSed training algorithm canconverge to a cooperative equilibrium. Note that θ, (d, t) ∈ H, and αi,j for all i, j cooperateto minimize LT + Li + LG + Rg. Therefore, the proposed training method can be seen as acooperative game (MaS-Colell, 1989). After finiShing the training proceSS, αi,j , for all i, j, are notneeded any more (because θ already conforms With the learned governing equation at this point)and can be discarded during testing.
