Figure 1: MsIGN generates samples from coarse to fine scale. Each scale, as separated by verticaldash lines, takes in feature xl-1 from the coarser scale and Gaussian noise zl, and outputs a samplexl of finer scale. The prior conditioning layer PCl lifts up the coarser-scale sample xl-1 to a finerscale Xl, which is the best guess of Xl given its Coarse-SCale value xl-1 and the prior. An invertibleflow Fl further modifies Xl to better approximate xl. See Section 2.1 for detailed explanation.
Figure 2: Results of the synthetic BIP. (a): Distribution of 2500 samples along the critical directionwk*. MsIGN is more robust in capturing both modes, its samples are more balanced. (b): Errormean and its 95% confidence interval. MsIGN is more accurate in distribution approximation, es-pecially at finer scale when the problem dimension is high. The margin is statistical significant asshown by the confidence interval. For more experimental results, please refer to Appendix F.
Figure 3: Results of the elliptic BIP. (a): Distribution of 2500 samples along a critical direction.
Figure 4: Ablation study of the network architecture and training strategy. “MsIGN" means ourdefault setting: training MsIGN network with Jeffreys divergence and multi-stage strategy. Othermodels are named by a base model (MsIGN or Glow), followed by strings indicating its variancefrom the default setting. For example, “MsIGN-KL” refers to training MsIGN network with singleKL divergence in a multi-stage way, while “MsIGN-KL-S” means traininng in a single-stage way.
Figure 5: Left: Synthesized CelebA 64 images with temperature 0.9. Right: Linear interpolation inlatent space shows MsIGN’s parameterization of natural image manifold is semantically meaningful.
Figure 6: Visualization of internal activation shows the interpret-ability of MsIGN hidden neurons.
