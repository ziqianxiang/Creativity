Figure 1: (a) Cross-modal alignment architecture. We use a pre-trained ResNet-152 and BERT asfeature extractors with an in-batch hinge loss. (b) Sample query image and retrieved captions fromthe COCO dataset. Ground truth captions are colored in blue (best viewed in color).
Figure 2: Illustration of our end-to-end framework. The trained cross-modal alignment is used toextract features as queries to a FAISS indexer. The k retrieved indices are used to access data fromthe external knowledge source, and augment the input by appending each of the k retrievals to therelative modality. For VQA, we only query the input image and retrieve k captions.
Figure 3: Two Hot-Swap configurations of the knowledge source during inference. (a) both thealignment model and the knowledge source are replaced with new ones built using a new dataset.
Figure 4: Hot-Swap results. Each row corresponds to a different reader model. Each graph shows(a) Training with different amount of retrieved captions. (b) Using the trained model with 10-cap,we inference with different amount of captions. (c) Hot swapping between knowledge sources.
Figure 5: Sample top-4 result for “in-domain” Hot-Swap. The model was trained using COCO as theknowledge source, and 10 retrieved captions. Left - Query image from VQA val-set. Columnsrefer to the different hot-swaps, showing retrieved captions.
Figure 6: Ablation study of our method. (a) - Training with different amount of retrieved captions.
