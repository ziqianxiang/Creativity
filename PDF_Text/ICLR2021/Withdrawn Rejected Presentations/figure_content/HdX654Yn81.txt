Figure 1: The proposed VAE ensemble consists of multiple original VAE models. The encodersof the VAEs in the ensemble generate input encoding that can be linearly transformed among eachother. The decoders of the VAEs in the ensemble reconstruct the input data from both their corre-sponding encoder and the linearly transformed encodings from other encoders. The x and y axisof the circles on the left hand side of the plot represent two generative factors as an example. Thealigned arrows with x and y axis show a model with disentangled representation and unaligned onesshow a model with entangled representation.
Figure 2: Comparing the DtO of linear transformations in the VAE ensemble (γ=10) with the onebetween well-trained individual VAEs, as well as the well-trained individual state-of-the-art VAEmodels. The latent dimension for all models is set to 10 and evaluated on the dSprite dataset.
Figure 3: The “polarized regime” comparison between models in the VAE ensemble. The latentdimension is set to 10 and the results are over 10 runs of training on the dSprite dataset. (a) The“polarized regime” comparison by log( LKL-LLjKL ) of each latent variable of the two models inVAE-E2 as well as β-VAE model. (b) Similar to (a) but for the three models in VAE-E?.
Figure 4: Ablation study to understand the effect of cross-model reconstruction and linear transfor-mation in the VAE ensemble objective using the FactorVAE metric. (w.o. CR - without cross-modelreconstruction loss; w.o. LT - without linear transformation loss; org - original VAE ensemble loss)is due to the increased difficulty of balancing between the cross-model and within model objectivesof VAE ensemble for larger ensembles. The reduced alignment of the latent representations amongdifferent models can also be seen in Table 2 where difference in the performance among individualmodels in the ensemble increases as ensemble size increases.
Figure 5: Geometric interpretation of the cross-model reconstruction between a disentangled repre-sentation space and an entangled representation space.
Figure 6: Characteristics of the linear transform between latent representations. The latent dimen-sion is set to 10 and the results are over 10 runs of training on the dSprite dataset. (a) Comparingthe DtO of linear transformations in the VAE ensemble (γ=10) and the one between original VAEs.
Figure 7: Distance to Orthogonality (DtO) measurement of the linear transforms between latentrepresentations in VAE-≡2 during training on the CelebA dataset. We also compare models withdifferent latent dimensions of 10, 16 and 32 and the results are averaged over 5 runs. In the figurelegend, We use “VAE_Ei nd Y = g” to represent VAE Ensemble (VAE_E) model with i individualVAE models, n latent dimensions and γ value equal to g.
Figure 8: Ablation study to understand the effect of cross-model reconstruction and linear transfor-mation in the VAE ensemble objective using the DCI Disentanglement metric and DtO. (w.o. CR -without cross-model reconstruction loss; w.o. LT - without linear transformation loss; org - originalVAE ensemble loss)496497498499500501502503504505506507508509ensemble without the cross-model reconstruction, the linear transformations among models are closeto a trivial transformation (signed permutation). This implies the orthonormal transformation of the
Figure 9: The “polarized regime” comparison between models in VAE-E2. The results are over 5runs of training on the CelebA dataset.
Figure 10: The “polarized regime” comparison between models in the VAE_E3 on the CelebAdataset.
Figure 11: Latent traversal on three different input images using VAE_E?, a single VAE and thestate-of-the-art VAE models with 10 dimensional latent representation. The three input images areellipse, heart and square shapes as shown in the last column.
Figure 11: (cont.) Latent traversal on three different input images using VAE_E2, a single VAE andthe state-of-the-art VAE models with 10 dimensional latent representation. The three input imagesare ellipse, heart and square shapes as shown in the last column.
Figure 12: Latent traversal on two different input images of CelebA dataset using VAE旦 withlatent dimension of 16.
Figure 13: Latent traversal on two different input images of CelebA dataset using a single VAE withlatent dimension of 16.
