Figure 1: Illustrations of different language model architectures. (a) is the standard Transformerdecoder. (b) is a language model with memory. The decoder takes Mt and outputs the next timestep’smemory Mt+1 at the end of segment. (c) is a Transformer encoder-decoder model. The encoder ismodified to read and write memory.
Figure 2: Illustration of Memformer Encoder-Decoder and its sub-modules. (a) shows the over-all architecture of Memformer Encoder-Decoder. (b) demonstrates the memory cross attention forreading from memory. (c) is the memory slot attention module to write the next timestep’s memory.
Figure 3: Effects of different configurations. (a) shows how multi-task learning helps. (b) shows theeffects of changing time horizon. (c) shows the effects of changing memory size.
