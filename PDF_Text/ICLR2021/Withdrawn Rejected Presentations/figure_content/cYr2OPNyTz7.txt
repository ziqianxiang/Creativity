Figure 1: Illustration of the proposed fully-explored masking strategy with a specific example. Inthis case, the input sequence has been divided into 4 exclusive segments, where different colorsindicate which segment a certain token belongs to.
Figure 2: The distributions of gradient covariance Cov g(m1), g(m2) for different Hamming dis-tances H(m1, m2) based on a small CS domain corpus. Left: gradient covariance distribution ofselected parameters in RoBERTa-base model; Right: gradient covariance distribution of selectedparameters in RoBERTa-base model after continually pre-trained on CS domain corpus.
Figure 3: Empirical analysis of the correlation between gradient covariance Cov g(m1), g(m2)and Hamming distance H(m1, m2) based on a small CS domain corpus. For a sequence of length512, two masks m1, m2 are randomly sampled with 128 masked tokens, their Hamming distancesatisfying 0 ≤ H(m1, m2) ≤ 256. Left: gradient CoVariancecalcUlatedbasedonRoBERTa-basemodel; Right: gradient covariance calculated based on RoBERTa-base model after continually pre-trained on CS domain corpus.
Figure 4: Left: the efficiency comparison between the standard MLM and fully-explored MLMapproach. Specifically, the RoBERTa-base model and continual pre-training setting (on the CS do-main) are employed. The corresponding models are evaluated on the ACL-ARC dataset (at differenttraining steps). Right: the effect of split number (under the FE-MLM framework) on the generaliza-tion ability of pre-trained models, evaluated on two datasets in the CS domain.
