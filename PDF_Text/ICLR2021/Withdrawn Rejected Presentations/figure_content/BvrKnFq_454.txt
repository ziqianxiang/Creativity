Figure 1: We replicated the synthetic convex optimization problem from Reddi et al. (2019) thatcauses Adam to diverge. We also tested RMSProp and ADADELTA, two other EMA-based methods,and found that they diverge too. Compared to AMSGrad and Yogi, Expectigrad minimizes bothfunctions about 10 times faster despite using the same hyperparameters. (Note that AMSGrad ispartially occluded by Yogi in the graphs.) We clipped the domain to [-1, 1] for graphical purposes.
Figure 2: Using the MNIST dataset, we explored the effects of different momenta on Expectigradby training for a single epoch (left). We swept the momentum constant β ∈ [0, 1) for three variants:inner (Polyak, 1964), bias-corrected inner (Kingma & Ba, 2014), and bias-corrected outer (ours).
Figure 3: We compared five adaptive methods on CIFAR-10 by training two large convolutionalmodels, VGGNet-16 (left, center-left) and ResNet-50 (center-right, right). Hyperparameters werechosen by a small grid search (Appendix A). Expectigrad outperformed the baseline methods forboth models with respect to training loss and top-1 error rate. Results were averaged over 10 runs.
Figure 4: We compared Expectigrad and Yogi training a MobileNetV2 on the ImageNet classificationtask. Despite using the 10× larger learning rate for Yogi recommended by Zaheer et al. (2018),Expectigrad achieves a lower loss and training/validation error rates in the same amount of training.
