Figure 1: Architecture illustrationcontroller. Then, the controller interacts with the environment iteratively until achieving the taskor reaching maximum steps. At the same time, the controller collects extrinsic reward to the metacontroller for training. Instead, intrinsic reward is used to train the controller.
Figure 2:	Sample gameplay on Montezuma’s Revenge : (a) illustrates the optimal policy learnedby the agent. It firstly chooses the key as a task, then turn to reach the bottom-right ladder afterobtaining the key. Finally, the agent chooses top-right door and takes a set of low-level actions toreach the door. (b-d) presents the process of navigating through the first room. It is easy to see thekey, bottom-right ladder and the top-right door are the key tasks to choose.
Figure 3:	Training performance on Montezuma’s Revenge: (a) illustrates the training phase of NSRLand HDQN, of which the results are collected and averaged by 8 runs. (b) depicts the distributionof tasks chosen by the meta controller at different time steps. (c) presents the success ratio of tasksover time.
