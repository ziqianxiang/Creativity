Figure 1: BERT phases - circles are training steps which produce models, represented as rectangles.
Figure 3: Improvement by BERTIT:CLUST vs Normalized Mutual Information per dataset. x-axis:Normalized Mutual Information between the cluster labels and the class labels, calculated over theentire train set. y-axis: The reduction in classification error (percentage) of inter-training usingBERTIT:CLUST relative to no inter-training, when using 64 samples in the fine-tuning phase.
Figure 6: Evaluating the effect of different number of MLM epochs. Each point in the line is theaverage of five repetitions. X axis denotes the budget for training in log scale, and Y accuracy ofeach model.
