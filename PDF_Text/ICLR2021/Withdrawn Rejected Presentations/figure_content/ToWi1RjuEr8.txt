Figure 1: Snapshots of AWR policies trained on OpenAI Gym and motion imitation tasks. Our simplealgorithm learns effective policies for a diverse suite of control tasks.
Figure 2: Learning curves of the various algorithms when applied to OpenAI Gym tasks. Results areaveraged across 10 random seeds. AWR is generally competitive with the best current methods.
Figure 3: Left: Learning curves comparing AWR with various components removed. Each com-ponent contributes to performance improvements. Right: Learning curves comparing AWR withdifferent capacity replay buffers. AWR remains stable with large buffers containing primarilyoff-policy data from past iterations.
