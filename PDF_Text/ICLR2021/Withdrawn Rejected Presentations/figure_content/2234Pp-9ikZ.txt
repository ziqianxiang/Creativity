Figure 1: AutoKD leverages multi-fidelity Bayesian Optimization, a hierarchical graph-based searchspace coupled with an architecture generator optimization pipeline, to find the optimal student forknowledge distillation. See section 3 for a detailed description.
Figure 2: Top-1 accuracy distribution for AutoKD and standard NAGO at different budgets onCIFAR100. The histograms are tallied across 5 runs. Across all budgets, AutoKD samples architec-tures With improved performances in top-1 accuracy compared to NAGO.
Figure 3: Top-1 accuracy of the best model found during search at a given computation time onCIFAR100 for AutoKD (red) and NAGO (blue) across different budgets. Each method was run 8times with the bold curve showing the average performance and the shaded region the stdev.
Figure 4: Rank Correlations between different epoch budgets for AutoKD (KD; left) and StandardNAGO (right) computed for 5 runs of NAGO and AutoKD respectively. NAGO reports a rankcorrelation coefficient of 5 ∙ 10-1 for epoch pair 30-120, which is 3.3 ∙ 10-1 less than that of the KDrank correlation. These results show that the rank correlation across all budget pairs vastly improveswhen knowledge distillation is applied.
Figure 5: Networks sampled from the best generator parameters found by AUtoKD (left) and NAGO(right) on CIFAR10.The former is organized with 8 clusters of 3 nodes, while the latter has 5 clustersof 10 nodes, showcasing how the optimal configuration depends on teacher supervision. Arrowsindicate information flow between nodes. As NAGO’s search space is hierarchical, it contains anumber of sub-graphs; the yellow (blue) nodes are the input (output) nodes of these sub-graphswhile the grey nodes are the operation units (conv-norm-relu).
Figure 6: Accuracy vs memory per sample for the SOTA Model (the teacher), the AutoKD studentand best architecture found by vanilla NAS. This plot clearly shows how AutoKD finds a modelsuperior to NAS-only, managing to reach the performance of the large teacher model while usinga fraction of the per sample memory. Note that the MIT67 teacher has almost 10× the number ofparameters of the student (54M vs 6M).
Figure 7: Final training curves for the top generator found by NAGO and AutoKD, for CIFAR10,CIFAR100 and MIT67. Each generator was sampled 8 times and the 8 corresponding architecturestrained for 600 epochs. Bold line represents the average; shaded region represents std deviation.
Figure 8: Student model test accuracy for various temperature (τ) and loss weight (α) combinations.
