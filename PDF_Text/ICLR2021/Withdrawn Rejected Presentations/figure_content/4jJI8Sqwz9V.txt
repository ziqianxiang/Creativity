Figure 1: Some statistics when training the vanilla GAN on MNIST. When no penalty was applied,the training algorithm was unstable and unconvergent as shown in the last two subfigures. Whenspectral normalization was used for G only, the vanishing gradient problem appeared and G didnot learn anything from D, and hence the training process stopped early. In contrast, the trainingbehaved very well when spectral normalization was used for D. It can be seen from the middlesubfigure that the Lipschitz constant of G can be much smaller if we use spectral normalization forboth Players. ∣∣∙∣∣f denotes the FrobenioUs norm.
Figure 2: The architectures of G and D with the negative slope of LeakyRuLU is 0.2D	CHANGING BzIn this experiment, the input noise of G is sampled from uniform distribution U [-s, s] for s ∈{0.01, 1, 100}. Spectral normalization (Miyato et al., 2018) is used for D. Other setups are thesame as Appendix C. Figure 3 shows the results along the training progress.
Figure 3: Some behaviors of GAN when changing Bz, the lower the better. LVg was estimatedby max ∣∣ d∂Vg ∣∣f from 5000 testing noises, while the other quantities were averaged from from 5000testing noises. We can see that Bz = 0.01 provides the worst results, while the results for Bz = 1 areslightly worse than those for Bz = 100. Those results are consistent with our theoretical analysis.
Figure 4: Some behaviors of GAN with different σ for augmentation. It can be seen from the first twosubfigures that the higher σ provides smaller Jacobian norms. This is consistent with our theoreticalanalysis. However, the last three subfigures suggest that there is no clear difference between the twosettings of σ.
