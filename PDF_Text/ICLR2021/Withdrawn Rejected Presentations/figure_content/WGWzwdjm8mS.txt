Figure 1: An illustration of the penalty term R2,where the y-axis is the loss, and the x-axis indi-cates the parameters of the model. LS1 and LS2are the average losses over batches S1 and S2, re-spectively. w(t) is the parameter at iteration t andwi(t+1) is the parameter at iteration t + 1 if batchSi was selected for the update step at iteration t,with i ∈ {1, 2}.
Figure 2: The error percentage and D during training with different amounts of randomness in thetraining labels for an AlexNet trained on a subset of 12.8 k points of the MNIST training dataset.
Figure 3: The test error (TE) and average gradientdisparity (D) for networks that are trained (untilreaching the training loss value of 0.01) over train-ing sets with different sizes. We observe a verystrong positive correlation: PD TE = 0.984.
Figure 4: The test error and average gradient dis-parity for networks that are trained with differentbatch sizes. A ResNet-34 and a VGG-19 net-work that are trained on the CIFAR-10 dataset.
Figure 5: Normalizing versus re-scaling loss be-fore computing average gradient disparity D for aVGG-11 trained on 12.8 k points of the CIFAR-10dataset.
Figure 6: Average gradient disparity for differ-ent averaging parameter S for a ResNet-18 thathas been trained on 12.8k points of the CIFAR-10dataset.
Figure 7: Test error (TE), test loss (TL), and gradient disparity (D) for VGG-16 trained with differenttraining set sizes to minimize the mean square error criterion on the CIFAR-10 dataset. The PearSoncorrelation coefficient between TE and D and between TL and D are PDTE = 0.976 and PD TL =0.943, respectively.
Figure 8: ComParing 5-fold Cross validation (CV) with gradient disParity (GD) as an early stoPPingCriterion when the available dataset is limited. (left) Validation loss versus test loss in 5-fold Crossvalidation. (middle left) Gradient disParity versus test and generalization losses. (middle rightand right) PerformanCe on the unseen (test) data for GD versus 5-fold CV. (a) The Parameters areinitialized by Xavier teChniques with uniform distribution. (b, C) The Parameters are initialized usingHe teChnique with normal distribution. (C) The batCh size is 32. The gray and magenta vertiCal barsindiCate the ePoCh in whiCh the metriC (the validation loss or gradient disParity) has inCreased for 5ePoChs from the beginning of training and for 5 ConseCutive ePoChs, resPeCtively. In (b) the middleleft figure, these two bars meet eaCh other.
Figure 9:	Comparing 10-fold cross validation with gradient disparity as early stopping criteria whenthe available dataset is noisy. (left) Validation loss versus test loss in 10-fold cross validation. (mid-dle left) Gradient disparity versus test and generalization losses. (middle right and right) Perfor-mance on the unseen (test) data for GD versus 10-fold CV. (a) The parameters are initialized byXavier techniques with uniform distribution. (b, c, and d) The parameters are initialized using Hetechnique with normal distribution.
Figure 10:	Detecting three tasks from the MRNet dataset from the sagittal plane MRI scans. (left)Validation loss versus test loss in 5-fold cross validation. (middle) Gradient disparity versus gener-alization loss. (right) Performance comparison on the final unseen data when applying 5-fold CVversus gradient disparity.
Figure 11: The cross entropy loss, the error percentage and the average gradient disparity duringtraining. (a-c): A ResNet-18 trained on a subset of 12.8 k points of the CIFAR-10 training set(the parameter initialization is Xavier (Glorot & Bengio, 2010)). PearSon's correlation coefficientP between D and generalization loss/error over all the training iterations are PDgenloss = 0.755and PDgenerrOr = 0.846. (d-f): An AlexNet trained on a subset of 12.8 k points of the MNISTtraining set (the parameters are initialized according to the He (He et al., 2015) method with Normaldistributions). For this experiment, PDgenloss = 0.465 and PDgenerrOr = 0.457. The blue, orange,green, and red CurveS are the test loss/error, train loss/error, generalization loss/error, and the averagegradient disparity D, respectively.
Figure 12: The cross entropy loss, error percentage, and average gradient disparity during trainingwith different amounts of randomness in the training labels for a 4-layer fully connected neuralnetwork with 500 hidden units trained on the entire MNIST dataset. The parameter initialization isthe He initialization with normal distribution.
Figure 13: The cross entropy loss, error percentage, and average gradient disparity during trainingfor a 4-layer fully connected neural network with 500 hidden units trained on the entire MNISTdataset with 0% label noise. The parameter initialization is the He initialization with normal dis-tribution. Pearson's correlation coefficient P between D and generalization loss/error over all thetraining iterations are PDgenloss = 0.967 and PDgenerrOr = 0.734. The gray vertical bar indicateswhen GD increases for 5 epochs from the beginning of training. The magenta vertical bar indicateswhen GD increases for 5 consecutive epochs. We observe that the gray bar signals when overfittingis starting, which is when the training and testing curves are starting to diverge. The magenta barwould be a good stopping time, because ifwe train beyond this point, although the test error remainsthe same, the test loss would increase, which would result in overconfidence on wrong predictions.
Figure 14: Test error and normalized gradient disparity for networks trained on the CIFAR-10dataset with different number of channels and hidden units for convolutional neural networks (CNN)and fully connected neural networks (FCNN). The correlation between normalized gradient dispar-ity and test loss PD TL and between normalized gradient disparity and test error PD TE are reportedin the captions.
Figure 15: The cross entropy loss, error percentage, and average gradient disparity during trainingwith different amounts of randomness in the training labels for a 4-layer fully connected neuralnetwork with 500 hidden units trained on the entire CIFAR-10 dataset. The parameter initializationis the Xavier initialization with uniform distribution. The training is stopped when the training lossgets below 0.01.
Figure 16: The test error (bottom row) and gradient disparity (top row) for a ResNet-18 trained onthe CIFAR-10 dataset with different training set sizes. (a) Results with data augmentation (DA) (weuse random crop with padding = 4 and random horizontal flip with probability = 0.5). (b) Resultswithout using any data augmentation technique. (c) Combined results of (a) and (b). We observea strong positive correlation between gradient disparity (D) and test error (TE) regardless of usingdata augmentation or not. We also observe that using data augmentation decreases the values of bothgradient disparity and the test error.
Figure 17: Test error and gradient disparity for networks that are trained with different training setsizes. The training is stopped when the training loss is below 0.01.
Figure 18: Test error and gradient disparity for networks that are trained with different batch sizestrained on 12.8 k points of the CIFAR-10 and CIFAR-100 datasets. The training is stopped whenthe training loss is below 0.01.
Figure 19: The cross entropy loss, error percentage, and average gradient disparity during trainingwith different amounts of randomness in the training labels for a ResNet-18 trained on the CIFAR-100 training set. The parameter initialization is the Xavier initialization.
Figure 20: The cross entropy loss, error percentage, average gradient disparity and Tζ for ζ = 20during training with different amounts of randomness in the training labels for a ResNet-18 trainedon a subset of 12.8 k points of the CIFAR-10 training set. We can observe that, in this setting,average gradient disparity distinguishes different label noise levels from the beginning of training,unlike generalization error.
Figure 21: The cross entropy loss, average gradient disparity D, and T for Z = 10 during trainingfor an AlexNet trained on a subset of 12.8 k points of the MNIST training set.
Figure 22: (a-d) VGG-19 configuration trained on 12.8 k training points of CIFAR-10 dataset. (e-h)VGG-11 configuration trained on 12.8 k points of the CIFAR-10 dataset. The training is stoppedwhen the training loss gets below 0.01. The presented results are an average over 5 runs. Thecaptions below each figure give the epoch number where test loss and gradient disparity have re-spectively been increased for 5 epochs from the beginning of training. The Pearson correlationcoefficient ρ is presented in each figure between gradient disparity and test loss.
Figure 23: Test loss, gradient disparity, EB-criterion Mahsereci et al. (2017), and sign(gi ∙ gj∙) for aResNet-18 trained on the CIFAR-10 dataset, with 0% and 50% random labels. Gradient disparity,contrary to EB-criterion and sign(gi ∙ gj∙), clearly distinguishes the setting with real labels from thesetting with random labels.
Figure 24: The test error (TE), average gradient disparity (D), and cosine similarity (cos(gi ∙gj∙)) dur-ing training with different amounts of randomness in the training labels for two sets of experiments.
