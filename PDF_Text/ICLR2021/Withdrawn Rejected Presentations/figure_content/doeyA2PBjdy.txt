Figure 1: The examples of the imbalance in weight utilization. (left) We trained a network follow-ing the procedure for 1× network in Section 2.2.1. The weights were randomly ablated from thenetwork and the training accuracy was measured for 500 times. The average accuracy is plottedwith the minimum and maximum values in the bar. Note that the accuracy difference is about 60%when the ablation ratio is 0.1. This implies the utility imbalance among the weights. (right) Thetraining accuracy with respect to the ablation regarding the magnitude of the weights. Each legendindicates the ablation ratio. Note that the training accuracy drops only when the weights with thelarge magnitude is ablated. The utility of the weight is biased with regard to the magnitude.
Figure 2: The weight utility characteristics during optimization. Each legend indicates the ablationratio. (left) The measured value of the utility imbalance, i.e., the standard deviation of the weightutility of the sample set. (middle) The average weight utility of the sample set. (right) The minimumweight utility in the sample set. The corresponding training accuracy is in Figure 7Definition 2 (Utility imbalance). For Wi ⊂ W and Wj ⊂ W, we say there is δ utility imbalancebetween the subsets of weights when |U (Wi) - U (Wj )| > δ.
Figure 3: The characteristics of the weight utility of the networks with the various sizes. Each legendindicates the size of the network. (top) The characteristics when the weights are ablated by ratio.
Figure 4: The other characteristics of the weight utilization. Each legend indicates the size of thenetwork. (left) The maximum weight utility in the sample set. (middle) The minimum weight utilityin the sample set. (right) The histogram of the values of the weight utility when the ablation ratio is0.05. Please refer to the histograms with other ratios in Figure 14.
Figure 5: The characteristics of the weight utility in the pruned-and-retrained network. The legendsindicate the pruned-and-retrained network, the pretrained network and the small network, respec-tively. Note that the characteristics of the pruned-and-retrained network are similar to that of thepruned-and-retrained one, even though we used the large learning rate for retraining.
Figure 6: The visualization of the pretrained, pruned and retrained weights on the accuracy surface.
Figure 7: The training accuracy for each ablation in Figure 2. Each legend indicates the ablationratio.
Figure 8: The standard deviation values of the weight utility in the sample set.
Figure 9: The average values of the weight utility in the sample set..
Figure 10: The minimum values of the weight utility in the sample set..
Figure 11: The utility characteristics of the vanilla network when trained for 500 epochs. Note thesevere accuracy drop compared to that of Figure 7. This corresponds with the increase in the averageweight utility, as the increase in the difference between the outputs of the ablated and the originalnetwork might cause the accuracy drop. In particular, the tendency is shown after the early phaseof the training where the network undergoes an abrupt transition (approximately before the epoch1000). Each legend indicates the ablation ratio.
Figure 12: The weight utility of the ResNet20 (He et al., 2016) with Batch Normalization (Ioffe& Szegedy, 2015), trained by SGD with momentum and weight decay. Each legend indicates theablation ratio.
Figure 13: The weight utility of the ResNet20 (He et al., 2016) with Batch Normalization (Ioffe &Szegedy, 2015), trained by SGD with momentum and weight decay for 5000 epochs. Each legendindicates the ablation ratio.
Figure 14: The histograms of the weight utility for the different-sized networks. Note that theaverage and the standard deviation of the weight utility are higher in the smaller network when thenetworks have the ability to classify. And the distributions become similar as the networks outputrandom chances. Each legend indicates the size of the network.
Figure 15: The characteristics of the weight utility in the pruned-and-retrained network with varyingpruning ratio. (Top) the characteristics of the networks when the pruning ratio was 25%. (Bottom)the characteristics of the networks when the pruning ratio was 75%. Although the pruned-and-retrained network shows the similar characteristics to those of the pretrained network, it gets differ-ent as the pruning ratio is increased.
Figure 16: Loss surface visualization of each pruning method and retraining method. On each plot,W *, Wp, W refers to the pretrained weight, the pruned weight, and the retrained weight. Thenumber of pruning iteration is indicated by the subscript i. Note the transitions in the accuracysurface over the cycles.
