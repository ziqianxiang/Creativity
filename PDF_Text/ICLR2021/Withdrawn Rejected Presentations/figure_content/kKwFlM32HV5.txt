Figure 1: (a) Inter-class similarity vs. intra-class variation: Each column includes two instances of aspecific “Gull” category from the CUB-200-2011 dataset Wah et al. (2011). (b) The natural worlddistribution dataset iNaturalist2018 Van Horn et al. (2018).
Figure 2: Learning to FGVC with (a) conventional Cross Entropy (CE) loss, (b) + Pairwise Confusion(PC) energy and (c) + the proposed Batch Confusion Norm (BCN).
Figure 3: Baseline versus confusion energy models. (a) The l2-norm of each category corresponds tothe weight wi in the classifier. (b) The classification accuracy of Frequent, Common, and Rare fromthe baseline and the other two confusion energy methods, PC and BCN.
Figure 4: Heatmap-visualization of testing images by Grad-CAM (Selvaraju et al., 2017). The spatialheatmaps show the responses of the network to different images. For each image set, the first columnshows the input images; the remained three columns show the corresponding heatmap of each model.
Figure 5: The proposed neural network architecture Φ for tackling the task.
Figure 6: The attention gated ASPP architectureout the gated element-wise product to output the adjusted feature maps F, weighted by the predictedattentions. That is, the attention-gated ASPP feature maps F are derived by~ .. .. .. ..
Figure 7: All images in a training batch are of different class labels. (a): When inter-class similarityprevails in a batch, the BCN loss functions as regularization to avoid overfitting. (b): When inter-classsimilarity is not obvious, the batch would yield a significant BCN loss and the optimization wouldturn to further reduce the cross-entropy loss and thus is expect to boost the FGVC performance ininference.
Figure 8: Prediction-visualization of FGVC. The X-axis denotes the category index, and the Y-axisdenotes the classification confidence. This figure shows the predictions of ResNet-50 with/withoutcoupling BCN. Blue lines indicate coupling the proposed BCN with the vanilla ResNet-50, whichcorrectly classifies the two testing images. Green lines indicate the predictions of the vanilla ResNet-50, which only correctly classify the right testing image. One similar image is attached under eachtest image. With the aids of BCN, the classification model try to exert a regularization effect fromother images in the same batch, hence shows hesitation among similar categories. However, themodel trained without confusion loss seems very confident even though making a wrong prediction.
Figure 9: The impacts by BCN and PC on CUB-200-2011. (a): The regularization terms promotethe performance in the FGVC task by confusion energy. Each point represents a class in the task.
