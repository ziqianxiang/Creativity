Figure 1: Semi-supervised CheXpert model training pipeline with MoCo as self-supervised trainingagent.
Figure 2: AUC for linear models with MoCo-pretraining is consistently higher than AUC of linearmodels with ImageNet-pretraining, showing that MoCo-pretraining produces higher quality repre-sentations than ImageNet-pretraining does.
Figure 3: AUC for models fine-tuned end-to-end with MoCo-pretraining is consistently higher thanAUC of models fine-tuned end-to-end without MoCo-pretraining, showing that MoCo-pretrainingrepresentations are more transferable than representations produced by ImageNet-pretraining only.
Figure 4: AUC performance on the Shenzhen tuberculosistask for models with and without MoCo-pretraining shows thatMoCo pretraining still introduces significant performance im-provement despite being fine-tuned on an external dataset.
