Figure 1: Histogram of log-likelihoods from a VAE model trained on CIFAR10, SVHN, FashionM-NIST, and OMNIGLO. (see similar results in Nalisnick et al. (2019a); Choi et al. (2018); Serra et al.
Figure 2: (a) Histogram of log-likelihoods of VAE trained on Cifar10, FashionMnist, GTSRB,IMGAENET, KMNIST(Kuzushiji-MNIST), OMNIGLOT, and SVHN, respectively. (b) LikelihoodRatios of training and testing samples (the higher is better). The Likelihood Ratios < 1 is representedby the likelihood of testing simple higher than training samples2 Related workVarious works Nalisnick et al. (2019a); Choi et al. (2018); Hendrycks et al. (2019); Lee et al. (2017)have reported that the deep generative models are not able to correctly detect OOD samples untilthe models have an excellent understanding of OOD inputs. Maal0e et al. (2019) indicated thatBidirectional-Inference Variational Auto-encoder (BIVA) with the multiple latent layers could capturethe high-level semantic information of the data, with a better understanding of OOD representation.
Figure 3: Overview framework of standard VAE (a) and BPVAE(b). The encoder and decoder arerepresented by a green and blue trapezoid, respectively. The purple and yellow squares denote thelatent space of in-distribution(ID) and out-of-distribution(OOD) data, respectively. Compared thegenerator of standard VAE (c) and BPVAE(d). The latent space of standard VAE only learn andcapture ID-distribution data, while the latent space of BPVAE cover the features of ID and OOD dataconcurrently.
Figure 4: Reconstruction performance for MNIST and CIFAR10 by VAEs and BPVAEs. HereCIFAR10 is used as basic dataset and MNIST is used as simple dataset.
Figure 5:	Histogram of log-likelihoods from VAEs model, which are trained on different groups ofdatasets.
Figure 6:	Histogram of log-likelihoods from VAEs model, which are trained on different groups ofdatasets. (a) Trained on CIFAR10(Basic), FahionMINST(simple), and KMINST(simple); (b) Trainedon CIFAR10(Basic), FahionMINST(simple), and MINST(simple);5	DiscussionOOD problem has been increasingly gaining attention and interests, which remains an intriguingproperty and a challenging issue for likelihood-based generative models. In this work, we introducedexternal latent priors to assist VAEs in capturing more abstract representations for data which notbelong to in-distribution. Through building an effective synergistic mode, VAEs can obtain powerfulrepresentation ability for different data from various datasets. In this manner, VAEs can be well-calibrated by shifting the likelihood distribution of data with simpler complexity to lower-likelihoodintervals compared to basic dataset, in which way the high-likelihoods problem of OOD can beovercome to a large extent. Interestingly, we find there is a trivial trade-off when employing detectiontasks, that is, even this method can alleviate OOD problem to a great extent, the likelihood intervalscale which can be covered by bridging two latent priors is a little limited. Hence we introduce ahybrid VAE version with multiply latent priors, which can alleviate the trade-off greatly. Besides,we only impose the proposed approach on VAE model, designing the hybrid latent priors for othermodels like Glow, PixelCNN Van den Oord et al. (2016) will be an interesting research topic. Andwe are expected to continue related exploration further. Overall, from a brand-new perspective, thiswork provides a potential way to tackle the OOD problem intertwined with VAEs.
Figure 7: Presentation of preprocessed data from eight datasets, which are CIFAR10, SVHN, GTSRB,IMAGENET, KMNIST, MNIST, OMNIGLOT and FashionMNIST.
Figure 8: Histogram of log-likelihoods from a VAE model trained on GTSRB, IMAGENET, KMNISTand MNIST.
