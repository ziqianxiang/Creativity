Figure 1: Example application BaNNER. Fig. 1a shows functions f (∙,τt) based on samples Tt 〜P(T) for aparameterized Forrester function together with the 2σ confidence interval of the meta-learned prior. In Fig. 1bwe see the corresponding posterior distribution after two data points (blue circles) for a specific test-functionf(x, τ). The confidence intervals contain the true function and collapse quickly, which enables highly-efficientBayesian optimization. More plots in Fig. 6 (Appendix A.2).
Figure 2: Illustration of the generative models. We approximate the unknown function f (x, τ) with gθ (x, z)by meta-learning the parameters θ based on the noisy observations y = f (x, τ) + . We regularize the model sothat we can infer a reliable uncertainty prediction for a new function f (x, τ) through approximate inference onz in our model. Our method results in reliable confidence intervals and is able to adapt quickly, see Fig. 1.
Figure 3: Performance on two synthetic function ensembles. All show the mean ± the standard error of themean (95% confidence) for 256 independent evaluations. For the Forrester ensemble, a strong prior without anytest data leads to a strong performance right from the start for both variants of our method. For the quadraticfunctions, the strength of the meta-models lies in knowing the global shape of the functions, rather than knowinga-priory the location of the optimum. ABLR failed to learn useful features from the provided meta-data, possiblydue to the over-parametrized network and more broadly distributed y values. With growing complexity and asmaller ratio of number of points per task per dimension, the performance of the meta-learning models degradesand they fail to improve over time, especially for Hartmann6. In all cases, the GPBO method eventually catchesup and usually outperforms the other methods, but meta-learning leads to strong early performance in all cases.
Figure 4: Performance on two surrogate meta-learning problems. Both show the mean ± the standard error ofthe mean (95% confidence). We ran every method 256 times with different seeds.
