Figure 1: Cross-Node Federated Graph Neural Network. (a) In each round of training, we alternatelytrain models on nodes and the model on the server. More specifically, we sequentially execute: (1)Federated learning of on-node models. (2) Temporal encoding update. (3) Split Learning of GN.
Figure 2: Validation loss during the training stage of different training strategies.
Figure 3: Effect of client rounds and server rounds (Rc, Rs)on forecasting performance and communication cost.
Figure A1: Visualization of subgraphs visible in training under different ratios.
Figure A2: The histograms of data on the first 100 nodes ranked by ID.
