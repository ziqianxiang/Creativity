Figure 1: pe-graph2axiom System OverviewGraph-to-sequence network for pathfinding The system in Fig. 1 takes as input two programsrepresented as symbolic trees, and produces a sequence of axioms along with their position ofapplication (or node) that can be used to rewrite sequentially one input program into the other inputprogram. Node initialization initializes the graph neural network, converting the input programs text(e.g., (a + (b + c)) into nodes and edges in the Graph Neural Network Scarselli et al. (2009); Xuet al. (2017). The details of the network are covered in Sec. 5. In a nutshell, the key principle is tocombine a memory-based neural network approach, e.g., using Long-Short Term Memory (LSTM)Hochreiter & Schmidhuber (1997) neurons and a graph neural network design (which uses GatedRecurrent Units (GRUs) internally) Beck et al. (2018) that matches our program graph representation.
Figure 2: Distribution of axiom possibilities and proof complexity for test datasets.
Figure 3: Examples of ComputationsInput program representation We illustrate in Fig. 3 four very simple computations, representedas graphs, that are all equivalent under various axioms of natural arithmetic. For example, P1 modelsthe expression a(1b + 1c), one can imagine it to be the result of a(db + dc) after e.g. constant-propagation of 1 to d by a compiler. They are defined by a single root, have nodes which can beoperations consuming the value of their immediate predecessor or terminal/input values, and a nodeproduces a value that can be used by its immediate successors. In essence this is a classical dataflowrepresentation of the computation Buck & Lee (1993), and what our system uses as input programrepresentation.
Figure 4: pe-graph2axiom System Overviewgraph of an expression computation, and eventually produces a sequence of axioms along with theirposition of application (or node) that can be used to rewrite sequentially one input program into theother input program. As each axiom is produced, it is checked to insure it is a legal application withinthe grammar and if the transformed program matches the target then a correct proof of equivalencehas been found. To train the system, we generate pairs of equivalent programs by iterating the axiomswith random probability on one program, thereby generating both a path to equivalence and the targetprogram. Random programs are generated so as to respect the grammar defined. The training setis then appropriately selected from these random samples, as detailed in Sec. E. Node initializationinitializes the graph neural network, converting the input programs text (e.g., (a + (b + c)) into nodesand edges in the Graph Neural Network Scarselli et al. (2009); Xu et al. (2017). The details of thenetwork are covered in Sec. 5. In a nutshell, the key principle is to combine a memory-based neuralnetwork approach, e.g., using Long-Short Term Memory (LSTM) Hochreiter & Schmidhuber (1997)neurons and a graph neural network design (which uses Gated Recurrent Units (GRUs) internally)Beck et al. (2018) that matches our program graph representation. Token embedding is a neuralnetwork layer in which tokens are assigned a learnable multidimensional embedding vector Mikolovet al. (2013). Each layer in LSTM 2 layers has 256 neurons, which support sequence generation.
Figure 5: Graph-to-sequence neural network data flow details.
Figure 6: Model training percentage accuracy up to 300,000 iterations on AxiomStep10. Trainingand Validation accuracies are per-token on the target axioms in the samples. Test accuracies are forfull correct proofs of P1 to P2.
Figure 7: Adding loop constructs creates cycles in the program graph.
