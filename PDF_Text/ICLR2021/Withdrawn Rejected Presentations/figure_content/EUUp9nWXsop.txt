Figure 1: Training π to imitate experts E : (1) we pass samples from Dsub and Dlab through thecurrent classifier M ; (2) the embeddings and the predictions are input to π, whose output vectoris compared with the target vector predicted by the best expert; (3) we calculate a loss and back-propagate the error through π; (4) we extend the labeled pool data Dlab by Dsel and retrain M .
Figure 2: Examples for the three datasets MNIST, Fashion-MNIST, and Kuzushiji-MNIST.
Figure 3: Active learning performance of the trained policy (trained on MNIST), compared to thebaseline approaches and ALIL (Liu et al., 2018), and applied to MNIST, FMNIST and KMNISTdatasets.
Figure 5: Ablation studies: IALE performance for different (a) acq and (b) n=|Dsub| (on MNIST);(c) IALE performance for different expert sets (on KMNIST).
Figure 4: Overlap ratio.
Figure 6: The overlap plots for all datasets MNIST, FMNIST and KMNIST datasets.
Figure 7: We show the performance of three policies trained on different datasets in comparisonwith Random (on MNIST, FMNIST and KMNIST).
Figure 8: MLP and ResNet-18 classifiers, data averaged or median filtered. Active learning perfor-mance of the trained policy in comparison with the baseline approaches on MNIST, FMNIST andKMNIST datasets.
Figure 9:	Applying a policy trained using a Resnet-18 classifier (trained on MNIST) to a CNN-basedclassifier (on MNIST, FMNIST and KMNIST).
Figure 10:	Full and enlarged segments of learning curve: Applying π trained on a CNN (IALECNN) or Resnet-18 (IALE Resnet), trained on MNIST, to a Resnet-18 classifier on CIFAR10.
Figure 11:	Evaluating the acquisition sizes from 1 to 10 on FMNIST, and varying different poolsizes on MNIST.
Figure 12:	Some experts are more suitable to other acquisition sizes (evaluated on MNIST).
Figure 13:	The active learning performance for each (leave-one-out) set of experts.
Figure 14:	For partial state (without predictions, without gradients), we plot active learning perfor-mance for each (leave-one-out) set of experts.
Figure 15:	Increasingly difficult datasets: SVHN (complex structure, requires more samples, averageand smoothed plots) and CIFAR-100 (100 classes).
