Figure 1: (a) A real-world example where the ability to perform mental rotations can be of practicalutility. (b) Images from the CLEVR-MRT dataset.
Figure 2: The pipeline of our FILM baselines (Perez et al., 2017). An input view (which is randomlysampled from a scene) is fed through a pre-trained (on ImageNet) ResNet-101 model to produce ahigh-dimensional stack of feature maps. The dotted border on the ResNet-101 indicate it is frozenduring training. The question embedding is fed through a GRU which outputs an embedding vectorof the sentence. The viewpoint camera is run through its own embedding embed(φfilm) (c) before beingconcatenated to this vector (the dotted line indicates that this stage is optional, depending on whatbaseline is run). The resulting feature maps are fed through FILM-modulated residual blocks usingthe final embedding vector. (Please see the supplementary materials section for more details.)If we let S denote a scene consisting of all of its camera views (images) X, the camera c, the questionq, and its corresponding answer y, we can summarise this as the following:3Under review as a conference paper at ICLR 2021S = (X, q, c, y)〜 D (sample a scene)X 〜 X (sample a random view)h := ResNet(x)ecam := embed(φfilm)(c)egru := GRUφ(q)y ：= FILMφ(h, [egru, eɑam])'c∣s ：= '(y, y),	(1)where the encoder (a ResNet) is frozen and we do not update its parameters during training.
Figure 3: The pre-trained ResNet encoder outputs a stack of feature maps h of dimensions (1024,14, 14), as in Fig. 2, but now a post-processing module postprocψ(h), e.g. a set of 2D convolutions,processes the feature stack and reshapes it into a 4D tensor h0 of dimensions (64, 16, 14, 14) , i.e., astack of feature cubes. This entire block (inside the grey border) is the new encoder.
Figure 4: The 3D version of the FILM pipeline proposed in Section 2.2. The encoder can be eitherthe 2D-to-3D formulation in Section 2.2.1 (with the ResNet-101 inside it frozen but the postprocessorblock learnable, i.e. Figure 3) or the contrastive encoder in Section 2.2.3 (which has no post-processorand is completely frozen). A camera encoder embed(ψrot) (c) is trained to map the camera coordinatesof the scene to a transformation matrix which is used to transform the resulting 4D volume viaan explicit rotation and translation, and/or it can be embedded and concatenated with the GRUembedding via embed(φfilm) (the dotted lines for both indicate that either/or are optional). This volumeis then fed to FILM-modulated ResBlocks (which internally compute 3D convolutions since we aredealing with a volume) to produce the answer.
Figure 5: The contrastive-based encoder, inspired from (Chen et al., 2020). We sample two sets ofminibatches X(1) and X(2), where the i’th instance in each set comprises a positive pair (differentviews of the same scene). The H encoder generates a 3D volume for each view, and an additional Zencoder convolves this down to a summarisation vector over which the contrastive loss is applied.
Figure 6: Left-most image is the canonical view. The rest are randomly sampled views, of varyingazimuth and elevation. Compared to Figure 1b, occlusions here are more prevalent.
