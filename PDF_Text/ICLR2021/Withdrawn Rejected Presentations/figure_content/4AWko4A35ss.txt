Figure 1: Illustration of our constrained jigsaw and the surrogate pretext tasks using an image ex-ample (only spatial for clarity). Our constrained jigsaw can be easily extended to the spatiotemporaldomain as done in this work. (a): The raw image. (b),(c): Comparing an unconstrained puzzle(b) and our constrained one (c), it is clear that ours is much more continuous (hence interpretable)reflected by the size of the largest continuous cuboids (LCCs, rectangles in images here) shown inred. (d),(e): Illustration of the importance of the relative order of the top-2 LCCs for determiningthe global continuity level of the shuffled image. (d) and (e) have the same top-2 LCCs, but only(d) keeps the correct relative order between them. Locating these LCCs and predicting their relativeorder are thus the key objectives of our surrogate tasks.
Figure 2: (a) Illustration of our Constrained Spatiotemporal Jigsaw (CSJ) (see Sec. 3.2). (b) Thepipeline of our proposed framework for self-supervised video representation learning (see Sec. 3.3).
Figure 3:	Attention visualization of the last feature maps from the fine-tuned models on UCF101.
Figure 4:	Visualization of the LCCD predictions from the pre-trained models. Each row denotes theframes at time stamp = (0, 4, 8, 12) from one video clip. (a) raw frames (with color jittering); (b)shuffled frames; (c) the ground truth of LCCD; (d) networkâ€™s prediction.
Figure 5: Qualitative results for the temporal action segmentation task on the Breakfast dataset. Notethat the notation 0 denotes an unannotated segment in the ground truth.
