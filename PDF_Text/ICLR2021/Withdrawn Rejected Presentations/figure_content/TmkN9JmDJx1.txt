Figure 1: RASP program taking two sequences vals,keys and returning a sequence y sortingthe elements of vals according to keys, e.g.: if vals(x)=[a,b,c] and keys(x)=[0,4,2], theny(x)=[a,c,b].
Figure 2: RASP program sorting the unique elements of a sequence vals by decreasing frequency.
Figure 3: Implementation of the operation count_conditioned in terms of indices and the otherbase operations, assuming that other is indeed a tuple of sequences (and not a single one).
Figure 4: The computation flow for the sequence histogram(tokens_str) applied to input sequence“hello”, when implemented in terms of select, aggregate, and zipmap.
Figure 5: The attention (left) and selection (right) patterns of a single-head transformer and single-head RASP program both trained/written for the task of in-place histograms. We visualise both onthe input sequence §jibbiejig. On the y axis, the sequence acts as the query, describing the outputlocations for which new values must be computed, and on the x axis it acts as the key, describing theinput locations being selected to compute these new values. Specifically, each row in the attentionpattern describes the self-attention distribution of this head over the sequence §jibbiejig. Thetransformer has clearly learned the same pattern as that used by the RASP program.
Figure 6: Building the RASP sequence histogram_given_BOS that computes an in-place histogramon the input tokens, under the assumption that the first token is always a special beginning-of-sequencecharacter §. The selector same_and_0 builds on the same intuition as the s_with_0 selector from theimplementation of count_conditioned.
Figure 7: Function T_y0 that takes a description of a transformer and embedding function and returnsd RASP+score sequences mimicking the output values of that transformer exactly (where d is theoutput dimension of that transformer). Note that the loops in T_y0 and layer are ‘meta-loops’ withrespect to the generated RASP program: they generate multiple layers and heads, but the number ofthese is a function of the transformer being mimicked and not of the input it will receive.
Figure 8: The attention distribution in the only head of a transformer trained on Count-a. Thesequence is presented on the y-axis as the queries and on the x-axis as the keys, i.e., each row is adistribution over the input positions. As predicted by our RASP program (which solves this taskusing full_s), this distribution is relatively uniform.
Figure 9: The attention distribution in the single head transformer trained on Histogram. The inputsequence abcdeba is presented on the y-axis as the queries and on the x-axis as the keys, i.e., eachrow is a distribution over the input positions. This transformer does not have enough heads: despitetraining for 50 epochs, it has only reached test accuracy 55%, and generates the incorrect output(1, 2, 1, 1, 1, 2, 1) for this sequence. Similarly, it does not manage to recreate the selection patternpredicted by our RASP program for this task, which requires 2 heads.
Figure 10: The attention distribution in the 2-head, single-layer transformer trained on Histogram.
Figure 11: The attention distribution for a 1-head, 1-layer transformer trained on Histogram withBOS tokens. The input sequence jibbiejig is presented on the y-axis as the queries and on thex-axis as the keys, i.e., each row is a distribution over the input positions. This transformer appearsto have implemented Histogram (with BOS) using exactly the same s_with_0 selection patternas suggested in our count_conditioned implementation! (The second selection pattern in thecount_conditioned implementation, s_just_0, is unnecessary when the value at the 0 index isconstant.))16Under review as a conference paper at ICLR 2021We prepend all of the original Histogram inputs with a special BOS token § (and their outputs with1), and train a new 1-layer, 1-head transformer on the resulting data set. For gamma=0.99, the resultssatisfy our predictions perfectly: the transformer reaches 99.7% test accuracy in 20 epochs, anddrawing its attention distribution (Figure 11) shows that it follows exactly the pattern of the selectors_with_0! (For gamma=0.98 the attention distribution was also very similar to that of s_with_0,but the model reached only 86.4% test accuracy after 20 epochs.)Discussion of BOS The significance of such ‘non-input’ tokens in transformers has been previouslydiscussed, with different interpretations. For example, Vig & Belinkov (2019) refer to the attentionfocused on the initial token of a sequence - seemingly when there is nothing else to focus on - as nullattention. They report that the null token gathered as much as 97% of the attention of some of theirheads, and suggest that this is consistent with these heads being unimportant to the transformer’s
Figure 12: The attention distributions in the 2-layer, 1-head (per layer) transformer trained on Reverse.
Figure 13: The attention distributions in the 1-layer, 1-head transformer trained on Reverse withfixed input length 50, on input sequences of length 45, 50, and 55, in order. The y-axes representoutput locations (queries) and the x-axes input locations (keys), i.e., each row is a distribution overthe input positions. As expected, the transformer has learned a constant relation between pairs ofinput locations, and maintains all of the pairs it can find in each input sequence it gets. For missingpairs, such as (0, 49) through (4, 45) in the input of length 45 (left), it struggles to focus its attention.
Figure 14: The attention distributions of the 2 heads in the 1-layer transformer trained on Reverse,as applied to the sequence abcdeabcde. As expected from the RASP program, the transformer isunable to learn the reverse-match attention on any head of its first layer.
