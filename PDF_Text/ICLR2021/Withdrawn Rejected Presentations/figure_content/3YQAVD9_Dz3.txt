Figure 1:	No Search (NOSE) Augment Vs Search-based AugmentThe main contributions of this paper can be summarized as follows:1.	We present a no-search (NOSE) augmentation method as an alternative of computation-intensivesearch-based auto-augment methods. By jointly applying phased augmentation strategy and intro-ducing more augmentation operations on top of a simple stochastic augmentation mechanism, NOSEaugment achieves state-of-the-art (SOTA) accuracies on CIFAR 10, CIFAR 100 (Krizhevsky, 2009)and close-to-SOTA results on other benchmark datasets. Our ablation study demonstrates that allthe components of our methods should be combined together to achieve the best performance.
Figure 2:	Phased Augmentationrelatively smaller amount of data with limited diversity cannot support learning DNNs with largesizes and complex structures.
Figure 3:	Training with Augmentation: Stochastic VS Fast AAOur hypothesis is confirmed with experiment results on multiple datasets, as shown in Figure 4. Aswe can see in the figure, the SAS periods of stochastic method are shorter than the total epochsspent on achieving optimal results in search-based methods, thus the slower accumulation of defi-cient data with random policies can be ignored while the benefit is it completely skips the expensivepolicy searching. We have to mention that succeeding research in Adversarial AutoAugment ex-ploited adversarial network for generating dynamic policies which to some extent made up for thedisadvantages of static policies. But the time and cost of policy searching were still existing andconsiderable, compared to the stochastic approach without any searching.
Figure 4:	An abstract view of augmentation on deficient data. The black bars shows the data distri-bution of the original training data over the knowledge dimensions; while green, blue, yellow barsstand for data generated from the 1st to 3rd phases of augmentation. The red dotted lines showthe data amount of the most deficient dimension upon completion of certain training phase withaugmentation.
Figure 5:	Influence of number of operations in a sub-policy on stochastic augmentationA.5 MULTI-STAGE TRAINING LOSS PROFILEFigure 6 shows the training loss profile for different epoch allocation of multi-stage augmentation.
Figure 6:	Training loss with various stage-epoch allocation strategy15Under review as a conference paper at ICLR 2021A.6 FACE RECOGNITION TRAINING DETAILWe trained the system using MobileFaceNet (Chen et al., 2018) architecture with ArcFace loss(Deng et al., 2018). Training backbone is adapted to Fast AA system and the training conductedwith the same settings described in Fast AA study apart from the search depth parameter which setas 100. We adapt the learning rate of 0.1 with 512 batch size to speed up the training. We trainedthe model for 50 epochs and learning rate is divided by 10 on epoch numbers 10,20 and 30.
