Figure 1: (Top) Directed graph of functions in topological order. Any feed-forward neural networkscan be considered as a sequence of functions and can be executed from f1 to fn in order. (Bottom)A sequence of variables corresponding to the sequence of functions. Each function uses variableinternally and different functions can have the same variable (connection between variables). Weestimate distances between functions by sum of bytes of variables within two functions (e.g. weestimate the distance between fi and fi+3 as the sum of bytes of {Vi+1, Vi+2} in this picture).
Figure 2: An overview of our scheduling algorithm at function fi . Red rectangle shows schedule-window. Sliding this window from f1 to fn , our algorithm performs (a) → (b) → (c) at eachfunction to schedule Swap-in and Swap-out. (a) Schedule Swap-in for all new variables coming intothe window. (b) If scheduling exceeds physical memory budget by scheduling Swap-in, we allocateenough memory for Swap-in by determining Swap-out for previous variables. Specifically, Swap-out is reserved right after the previous function using the variable, and waits for the Swap-out rightbefore fi . (c) Schedule Swap-out for variables used by fi and move the window to the first variableof next function. (d) shows how we reduce unnecessary memory transfers. We skip Swap-out forthe variables used by fi if variables are never used in future or already reserved as Swap-in.
Figure 3: Maximum defined memory budget and actual memory usage. In the setting with schedul-ing alone, there are almost 2× differences at 928 batch size. This indicates memory fragmentation.
Figure 4: Training time as model size grows for various architectures. We evaluate average iterationtime including forward, backward, and update over 100 iterations.
Figure 5: Memory usage as model size grows for various architectures. We evaluate the peak mem-ory usage during forward, backward, and update for each network. Red dashed line representsphysical memory budget (16GB).
