Figure 1: Accuracy compared to training time. While some of the other methods perform slightlybetter than our 7 layer deep model in time per epoch, we achieve higher accuracies per trainingminute (in addition to achieving a higher overall accuracy). Of note, using a wider network is not apenalty in accuracy vs training time. Plots show the mean test accuracy/time over 10 runs — exceptGray et al. (2020), which shows the median results from 10 runs due to their higher variability.
Figure 2: (a) MNIST superpixel error rates as graph edges are dropped on input. Our model per-forms significantly better for a range of edge dropout rates. Gray et al. (2020) use a technique whichignores the input edges which leads to it being unaffected by edge dropout. Fey et al. (2017) seessome benefit from low edge dropout rates (probably from reduced overfitting). (b) Effect of hyper-parameters on performance of our architecture. We modify a default set of parameters — 7 layersdeep, 256 initial & max features, no pruning, and 0.5 edge dropout. Each was run for 5 epochs on anVidia V100 and average times per epoch are shown.
