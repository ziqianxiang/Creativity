Figure 1: Training loss for Cifar-10 onResnet-18. Orange plot uses a fixed LRof 0.1, while in blue plot, the LR is re-duced from 0.1 to 0.01 at epoch 50.
Figure 2:	Histogram of minima sharpness (Keskar et al., 2016) for 50 random trials of Cifar-10 onResnet-18. Each figure shows histograms for runs with different number of explore epochs. Thedistribution moves toward lower sharpness and tightens as the number of explore epochs increase.
Figure 3:	Histogram of test accuracy for 50 random trials of Cifar-10 on Resnet-18. Each figureshows histograms for runs with different number of explore epochs. The distribution moves towardhigher test accuracy and sharpens as the number of explore epochs increase.
Figure 4: LR Range test for selecting the maximum learning rate. A good choice is the learning rateis a bit before the minima in a region where the loss is still decreasing.
Figure 5: ImageNet on Resnet-50 trained with Momentum. Shown are the training loss, top-1/top-5test accuracy and learning rate as a function of epochs, for the baseline scheme (orange) vs the Kneeschedule scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.
Figure 6: Cifar-10 on Resnet-18 trained with Momentum. Shown are the training loss, test accuracyand learning rate as a function of epochs, for the baseline scheme (orange) vs the Knee schedulescheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.
Figure 7: BERTLARGE pretraining for batch size of 16k with LAMB optimizer for the short budgetruns. Shown are the training loss and learning rate as a function of steps, for the baseline schemeshort budget (orange) vs the Knee schedule scheme short budget (blue). The plot is split into 2 partsto give a clear picture of the two phases of training Devlin et al. (2018). Note that even thoughthe training loss curves look similar for the two runs, we see a significant gap in F1 score obtainedwhen we fine-tune the model checkpoints on SQuAD-v1.1 Rajpurkar et al. (2016). See Table 27 fordetails.
Figure 8: WMT’14 (EN-DE) on TransformerBASE network trained with RAdam. Shown are thetraining perplexity, validation perplexity and learning rate as a function of epochs, for the baselinescheme (orange) vs the Knee schedule scheme (blue). The plot is split into 3 parts to permit higherfidelity in the y-axis range.
Figure 9: IWSLT’14 (DE-EN) on TransformerBASE network trained with RAdam. Shown are thetraining perplexity, validation perplexity and learning rate as a function of epochs, for the baselinescheme (orange) vs the Knee schedule scheme (blue). The plot is split into 3 parts to permit higherfidelity in the y-axis range.
Figure 10: IWSLT’14 (DE-EN) on MAT network trained with Adam. Shown are the training per-plexity, validation perplexity and learning rate as a function of epochs, for the baseline scheme(orange) vs the Knee schedule scheme (blue). MAT training involves two training phases with 200epochs each, shown in separate columns above.
Figure 11: SQuAD-v1.1 fine-tuning on BERTBASE trained with Adam. Shown are the training loss,test EM score, and learning rate as a function of epochs, for the baseline scheme (orange) vs theKnee schedule scheme (blue). The plot is split into 2 parts to permit higher fidelity in the y-axisrange. It is clear that with Knee schedule the network starts to overfit after the 2nd epoch, where thetesting loss continues to go down, but generalization suffers. We saw similar behavior with differentseeds, and thus need to train with Knee schedule for only 2 epochs.
