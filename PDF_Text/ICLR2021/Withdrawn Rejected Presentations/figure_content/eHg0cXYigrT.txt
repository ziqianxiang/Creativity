Figure 1: A schematic overview of the general model structure that we screened during hyperpa-rameter search. Architectural features such as layer number and skip connections can vary. We notethat the architecture of our final model differs from the one depicted here.
Figure 2: Hyperparameter impor-tance. The bars show individual im-portance of each hyperparameter interms of the variance they explain.
Figure 3:	Marginal predictions of biochemical features and the label embedding based on optimiza-tion data and of the conditional mechanism based on data of the 27 best selected models. Predictionswere obtained training on MMD and MRR. Lower is better for blue and higher is better for red.
Figure 4:	Reciprocal rank for each individual la-bel. The structure represents the relations in theGO DAG. Nodes are colored by how well themodel can target them. Blue to light orange in-dicates that the model ranks the target functionin the first three positions in average. Values areaveraged over n = 5 different data splits.
Figure 5: (a-e) Distributions of some selected features of ProFET, dark blue is the reference. f) KSstatistics over ã€œ500 ProFET features, lower is better.
Figure A1: GO DAG of the 50 labels selected for this project.
Figure A2: Sequence feature analysis of the models trained on truncated data (Sequence length =32). KS statistics over ~ 500 ProFET features, lower is better.
Figure A3: Marginal predictions of hyperparameters based on optimization data in the second op-timization. Predictions were obtained training on MMD and MRR. Note that for MMD, lower isbetter.
Figure A4: Marginal predictions of hyperparameters based on the data of the 27 best selected modelsin the second optimization. Predictions were obtained training on MMD and MRR. Note that forMMD, lower is better.
Figure A5: Hyperparameter importance for the first BOHB optimization. Shown are all hyperpa-rameters subject to optimization for all models (left), and a manual selection of models that wastrained for 100 epochs (right).
Figure A6: Marginal predictions of hyperparameters based on data in the first optimization. Weshow some selected predictions that allowed for interpretation, all others were inconclusive. If nototherwise noted, data comes from all trials in the optimization. Predictions were obtained trainingon MMD and MRR. Note that for MMD, lower is better.
Figure A7: Losses and evaluations at training time. W = Wasserstein, AC = Auxiliary Classifier27Under review as a conference paper at ICLR 2021min. sq. euclidean distance between each sequence in thetest / generated set and all sequences in the training setFigure A8: Distributions of pairwise distances in kernel feature space between a testset and thetraining set (red) and a generated set of ProteoGAN and the training set (green). It can be seenthat the generated sequences are not closer to the training set than the testset (which would indicateoverfitting). Further the generated sequences are about as far, but not further, away from the trainingset than the testset.
Figure A8: Distributions of pairwise distances in kernel feature space between a testset and thetraining set (red) and a generated set of ProteoGAN and the training set (green). It can be seenthat the generated sequences are not closer to the training set than the testset (which would indicateoverfitting). Further the generated sequences are about as far, but not further, away from the trainingset than the testset.
