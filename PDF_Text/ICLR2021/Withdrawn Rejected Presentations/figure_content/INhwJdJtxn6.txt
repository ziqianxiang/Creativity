Figure 1: Comparison of transfer strategies on Montezuma’s Revenge (hard exploration) and Space In-VaderS (dense reward) from a task-agnostic policy pre-trained with NGU (Puigdomenech Badia et al.,2020b). Transferring representations provides a significant boost on dense reward games, but itdoes not seem to help in hard exploration ones. LeVeraging the behaVior of the pre-trained policyproVides important gains in hard exploration problems when compared to standard fine-tuning and iscomplementary to transferring representations. We refer the reader to Appendix F for details on thenetwork architecture.
Figure 2: Intuition behind CPT on a simple maze, where theagent needs to collect treasure chests (positive reward) whileavoiding skulls (negative reward). Trajectories that a policyπp trained to maximize coverage could produce are depictedin orange. Left: while πp ignores some of the rewardingobjects, many learning opportunities appear when followingit during training. Right: combining primitive actions (red)with actions from πp (orange) side-steps the need to learnbehavior that is already available through πp when solvingdownstream tasks.
Figure 3: Ablation results. Using the task-agnosticpolicy for exploitation and exploration seems toprovide complementary benefits, as combining thetwo techniques results in important gains.
Figure 4:	Effect of the pre-training budget, before and after adaptation, on Montezuma’s Re-venge (hard exploration) and Pong (dense reward).
Figure 5:	Final scores per task in the Atari games of Ms Pacman (top) and Hero (bottom) withmodified reward functions. We train a single task-agnostic policy per environment, and leverage it tosolve three different tasks: the standard game reward, a task with sparse rewards (easy), and a variantof the same task with deceptive rewards (hard). Despite the pre-trained policy might obtain low oreven negative scores in some of the tasks, committing to its exploratory behavior eventually lets theagent discover strategies that lead to high returns.
Figure 6: Q-NetWork architecture for the reinforcement learning stage. The pre-trained policy can beleveraged Without transferring representations (left), but sharing Weights generally provides efficiencygains early in training (right).
Figure 7: Training curves in all 57 Atari games after 5B frames. Shading shows maximum andminimum over 3 runs, while dark lines indicate the mean. Leveraging the pre-trained policy pro-vides important gains, particularly in hard exploration games such as Montezuma’s Revenge, whilemaintaining performance in games with denser rewards such as Pong or Asterix.
Figure 8: Training curves for ablation experiments after 5B frames. Shading shows maximum andminimum over 3 runs, while dark lines indicate the mean. Both methods offer benefits over thebaselines, but in different sets of games. Combining them retains the best of both methods, and boostsperformance even further in some games.
Figure 9: Alternative reward functions for MsPacman (top) and Hero (bottom). We report trainingcurves for the standard game reward (left), a variant with sparse rewards (center), and a task withdeceptive rewards (right). Despite the pre-trained policy might obtain low or even negative scores insome of the tasks, committing to its exploratory behavior eventually lets the agent discover strategiesthat lead to high returns.
Figure 10: Training curves after 50M frames on Montezuma’s Revenge, using 16 actors and the CNNencoder from the pre-trained policy. Pre-trained weights are not fine-tuned.
Figure 11: Influence of the pre-trained policy on the end performance. We evaluate different pre-training times: 8, 24, 40 and 56 hours of pre-training. 8 hours of training correspond roughly to 1.3Benvironmental steps. We show learning curves (top) and end performance (bottom) for the games ofMontezuma’s Revenge and Pong. When comparing end performance, we show the performance of πiand πe (after 5B environmental steps of adaptation). Longer pre-training times lead to policies thatcover more of the environment. As expected, the performance in Pong is independent of the qualityof the pre-trained policy while for Montezuma’s Revenge, longer pre-training times lead to dramaticimprovements after the adaptation stage.
