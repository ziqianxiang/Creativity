Figure 1: Mean SSIM over a test set of100 sequences of open-loop predictionwith Moving MNIST. All 3-level latentdynamics models, with temporal abstrac-tion factors 2, 4, 6, and 8, have the samenumber of model parameters.
Figure 2: Long-horizon open-loop video prediction for the MineRL Navigate dataset (Guss et al.,2019), using our 3-level TALD model with temporal abstraction factor 6, compared with VTA, RSSM,and SVG-LP. We observe that TALD accurately predicts the movement of the scene from the ocean tothe forest, and maintains that context until 420 frames. However, VTA predicts an implausible sceneafter 240 steps with blue and black skies in the same frame. RSSM predicts a plausible future as wellwhere the player stays in the ocean as the distant forest moves out of the scene, whereas SVG-LPlearns to copy the initial frame indefinitely and does not predict any new events in the future.
Figure 3: Temporally Abstract Latent Dynamics (TALD). Left is the structure of our recurrent model,in which each latent state St in the second level abstracts two latent states in the first level. The solidarrows represent the generative model, while both solid and broken arrows comprise the inferencemodel. On the right, we illustrate the internal components of the state variable, which comprisesa deterministic state ht and a stochastic state zt. The deterministic state processes all contextualinformation and passes it to the stochastic state to be used for either generation or inference.
Figure 4: Long-horizon open-loop prediction for Moving MNIST. (L for levels, F for abstractionfactor.) We illustrate samples from our TALD model with 3 levels and temporal abstraction factors:6 (3L-F6), and 1 (3L-F1) (i.e. no temporal abstraction). We compare those with samples fromthe RSSM and SVG-LP baselines. Red boxes show instances in time from where models loseaccurate object identity. We observe that TALD, with abstraction factor 6, is able to maintain accuratelong-term dependencies in the form of object identities for 900 frames into the future.
Figure 5: Open-loop video prediction for the KTH Action dataset. While TALD predicts accuratelyfor 50 time frames, we observe jumpy transitions in VTA, where in this example the person disappearsafter the 17th frame. SVG predicts accurately for 18 frames, but starts to forget the task thereafter, asthe person in the video starts to move in the opposite direction.
Figure 6: Quantitative comparison between our temporally abstract latent dynamics model (TALD)and baselines over open-loop video prediction of 300 frames for the KTH Action dataset.
Figure 8: KL divergence at each levelof the TALD model (3 levels, tem-poral abstraction factor 6), trainedon slower/faster versions of MovingMNIST. Observe that the KL termat higher levels decreases with an in-crease in the speed of the digits, sug-gesting less global information beingpushed up in the hierarchy.
Figure 7: Long horizon open-loop video prediction for the GQN mazes dataset (Eslami et al., 2018)using our 2-level TALD model with temporal abstraction factor of 6. RSSM does not use temporalabstraction and thus starts to forget wall and floor patterns after 40 frames, while TALD maintainsthis global information over 200 frames.
Figure 10: Entropy of the priorduring open-loop video genera-tion at different levels of TALD(3 levels, temporal abstraction2). (a) With GQN mazes, con-stant entropy at level 3 (storedwall and floor patterns). (b)With Moving MNIST, constantentropy at level 2 (stored digitidentities), and level 3 (sufferedposterior collapse).
Figure 9: Visualizing the information stored at the higher level of our 3-level model with temporalabstraction factor 2, using the GQN mazes dataset. We computed a posterior belief at each levelusing 8 observation frames, and set one of the levels to the prior (by not feeding it with observations),which were then used to condition open-loop predictions. Changing wall textures show that they arestored at the highest level.
Figure 11: Visualizing the information stored at different levels of TALD with 3-levels and temporalabstraction factor 6, for Moving MNIST. We computed a posterior belief at each level using 36observation frames, while setting one of the levels to the prior (by not feeding it with observations),which were then used to condition open-loop predictions.
Figure 12: Quantitative comparison between temporally abstract latent dynamics model (TALD)and other video prediction models, for long-horizon open-loop video prediction. Higher values arepreferred for SSIM and PSNR, whereas lower values are preferred for LPIPS and FVD.
Figure 13: Long-horizon open-loop prediction on Moving MNIST. (L for levels, F for abstractionfactor.) The top row shows the ground truth sequence. The next two rows illustrate samples generatedfrom our TALD model: 3 levels with temporal abstraction factor of 6 (3L-F6), and 3 levels withtemporal abstraction factor of 1 (3L-F1) (essentially a deeper model with no temporal abstractions).
Figure 14:	Long horizon open-loop video prediction for the GQN mazes dataset (Eslami et al., 2018)using our 2-level TALD model with a temporal abstraction factor of 6. In both the examples, themazes switch between two types of wall patterns as the camera moves around the maze. We observethat while TALD could remember the alternate wall patterns for 200 timesteps, RSSM could notpredict any frame with the previously generated wall patterns. This suggests that TALD was able tomaintain long-term information for at least 200 frames.
Figure 15:	Long-horizon open-loop video prediction for the MineRL Navigate dataset (Guss et al.,2019), using our 3-level TALD model with a temporal abstraction factor of 6, along with someestablished baselines.
Figure 15: (contd.) Long-horizon open-loop video prediction for the MineRL Navigate dataset (Gusset al., 2019), using our 3-level TALD model with a temporal abstraction factor of 6, along with someestablished baselines.
Figure 16: Visualizing the information stored at the higher level of our 3-level model with temporalabstraction factor 2, using the GQN mazes dataset. We computed a posterior belief at each levelusing 8 observation frames, and set one of the levels to the prior (by not feeding it with observations),which were then used to condition open-loop predictions. Changing wall textures show that they arestored at the highest level.
