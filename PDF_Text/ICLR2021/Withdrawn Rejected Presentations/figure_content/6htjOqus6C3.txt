Figure 1: (a) PI control algorithm. (b) Time response of KL-divergence when β = 130.
Figure 2: (a) shows the comparison of reconstruction error on dSprites using 5 random seeds.
Figure 3: Rows: latent traversals ordered by the value of KL-divergence with the prior in a descendingorder. We initialize the latent representation from a seed image, and then traverse a single latent codein a range of [-3, 3], while keeping the remaining latent code fiXed.
Figure 4: Latent traversals on MNIST for DynamicVAE. It can be seen that our method can disentanglefour different factors: rotation, thickness, size(width) and writing style.
Figure 7: Top: A hybrid annealing method that combines step function With ramp function.
Figure 8: (a) g(x(t)) on MNIST dataset. (b) g(x(t)) on dSprites dataset.
Figure 9: Time response of KL-divergence under different β on MNIST and dSprites datasetsrespectively(b) dSprites when β = 130C.1 Verification on Benchmark DatasetsWe first verify the validity of our assumption that g0(x) < 0 in Theorem 3.1. Fig. 8 illustrates therelationship between β(t) and the actual KL when model training converges on dSprites and MNISTdatasets. We can observe that the actual output KL-divergence and β(t) have a highly negativecorrelation, which means g0 (x) < 0.
Figure 10:	Performance comparison of different methods.
Figure 11:	Latent traversals on MNIST for ControlVAE.
Figure 12:	Latent traversals on MNIST for FactorVAE.
Figure 13:	Latent traversals on MNIST for β-VAEH (β = 10).
Figure 14:	Latent traversals on MNIST for β-VAEB (γ = 100).
Figure 15:	Latent traversals on MNIST for the basic VAE.
