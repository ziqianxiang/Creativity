Figure 1: Left - illustrative example of an action-value mapping (red line) and associated samples (reddots). The agent aims to maximize obtained value (green dot) and builds a model through interaction.
Figure 2: Bug trap environment. The agent starts inside the bug trap and explores for 100 episodes inthe absence of reward signals. Top row, left to right: initial configuration and occupancy traces foragents trained using LOVE (β > 0), LOVE without deep optimism (β = 0), and Dreamer. Bottomrow: results for a variation of the environment. LOVE’s ability to consider uncertain long-termperformance in driving exploration enables the agent to escape in both scenarios. We consider resultson 3 random seeds and highlight the number of escapes over episodes in the right panel. LOVEprovides the highest escape rate and the best area coverage (see Appendix E for occupancy maps).
Figure 3: DeepMind Control Suite. We evaluate performance over the first 300 episodes on 5 seeds.
Figure 4: Reward-free maze exploration task.
Figure 5:	Additional sparse reward tasks: Cheetah Run, Walker Run and Walker Walk providere-scaled rewards only above a threshold (0.25, 0.25 and 0.7, respectively). LOVE achieves thehighest performance, displaying significant improvement over LVE on Cheetah Run and Walker Run.
Figure 6:	Motion pattern of the Walker with low predictive uncertainty. The agent is provided with 5contextual images and predicts forward for 15 steps (preview horizon), at different stages of training.
Figure 7:	Motion pattern of the Walker with high predictive uncertainty. The agent is provided with 5contextual images and predicts forward for 15 steps (preview horizon), at different stages of training.
Figure 8: Motion pattern of the Cheetah with low predictive uncertainty. The agent is provided with 5contextual images and predicts forward for 15 steps (preview horizon), at different stages of training.
Figure 9: Motion pattern of the Cheetah with high predictive uncertainty. The agent is providedwith 5 contextual images and predicts forward for 15 steps (preview horizon), at different stages oftraining. The irregular falling pattern has not been extensively explored and uncertainty remains inthe ensemble. This motion is undesirable and the agent should not focus on reducing its uncertainty.
Figure 10: Occupancy maps of the bug trap environment for two scenarios and three random seeds.
Figure 11: Comparison to Dreamer with adapted policy learning rate and training steps (∆Dreamer).
Figure 12:	LOVE under variation of the planning horizon.
Figure 13:	LOVE under variation of the beta schedule.
Figure 14:	LOVE under variation of the ensemble size.
