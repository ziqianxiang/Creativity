Figure 1: Training of deep neural networks(DNN). Standard DNN transform inputs x intooutputs y through activation functions and linearlayers, which are tuned by an optimizer. In con-trast, P4 training operates on perturbations to theparameters. Those are defined to retain certainnetwork properties (here: invertibility as well astractable inversion and determinant computation).
Figure 2: Wall-clock times of forward and backward pass of linear normalizing flows includingdeterminant computation. Three methods are compared: (a) standard linear layers (“standard”),where the inverses and determinants are computed through PyTorch’s inverse and det functions; (b)LU decompositions (“LU”), where the determinants are products over the diagonal entries and thematrices are inverted through triangular上olve; (C) P4Inv updates that keep track of inverses anddeterminants through rank-one updates. Timings are compared for square matrices of dimension 64,512, and 4096.
Figure 3: Training towards a 32-dimensional positive definite target matrix T . Left: Losses duringtraining. Right: Eigenvalues during training. Final eigenvalues are shown as red crosses. Eigenval-ues of the target matrix are shown as black squares.
Figure 4: Training towards an orthogonal target matrix T ∈ SO(128). Left: Losses during training.
Figure 5: Training towards the matrixT = -I101 using no penalty. Residueof inversion (black line) and absolutedeterminants of the standard linear andP4Inv layer. Both converge to the targetin a similar number iterations (dashedline).
Figure 6: Density estimation for two-dimensional distribu-tions from RealNVP (RNVP) and P4Inv networks with sim-ilar numbers of tunable parameters.
Figure 7: Left: Energy distributions of generated samples in dimensionless units of kBT; the second(orange) violin plot shows energies when the training data was perturbed by normal distributedrandom noise with 0.004 nm standard deviation. The low-energy fraction for each column denotesthe fraction of samples that had potential energy u lower than the maximum energy from the trainingset (≈ 20 kBT). Right: Joint marginal distribution of the backbone torsions φ and ψ: training datacompared to samples from RealNVP Boltzmann generators with and without P4Inv swaps (denotedP4Inv and RNVP, respectively).
Figure 8: Samples from P4Inv training via the inverse.
Figure 9:	Neural network blocks used in the Boltzmann generator application. The baseline archi-tecture is a normalizing flow composed of Real NVP (RNVP) coupling blocks. RNVP uses inputfrom two channels x1 and x2. The input from the first channel is left untouched, y1 = x1 , while theoutput y2 from the second channel is conditioned on the first channel through two neural networkst and s. Each block of the baseline model contains two RNVP blocks and two swapping steps thatare bracketed by splitting and concatenation (cat) of the data. Instead of the swapping steps, theP4Inv model uses invertible linear layers that are trained through P4 updates.
Figure 10:	Training loss and test accuracy during MNIST training with a vanilla SGD optimizeraveraged over ten replicas. Standard multilayer perceptrons (MLP) are compared with rank-oneupdates.
