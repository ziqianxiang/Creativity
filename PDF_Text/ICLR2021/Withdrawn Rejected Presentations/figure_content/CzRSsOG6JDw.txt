Figure 1: We found that irrationality does not always hinder reward inference (section 3.2) - it isin many cases actually helpful. Here, We depict rational and myopic (short-sighted) behavior in amerging environment (section 4.2) for two different rewards, with higher and lower weights on goingfast. The rational car (white) exhibits similar behavior under both reward functions, while the myopiccar (blue) overtakes on the shoulder when the reward function places a high weight on speed. Thismakes it easier to differentiate what its reward is.
Figure 2: In section 2.3, we modify the components of the Bellman update to cover different types ofirrationalities: changing the max into a softmax to capture noise (Boltzmann), changing the transitionfunction to capture optimism/pessimism or the illusion of control, changing the reward values tocapture the nonlinear perception of gains and losses (Prospect), changing the average reward over timeinto a maximum (Extremal), and changing the discounting to capture more myopic decision-making.
Figure 3: The log loss (lower = better) of the posterior as a function of the parameter we vary foreach irrationality type, on the random MDP environments. For the irrationalities that interpolate tothe rational planner, we denote the value that is closest to rational using a dashed vertical line. Everyirrationality except Prospect Bias all have parameter settings that outperform the rational planner. Theerror bars show the standard error of the mean, calculated by 1000 bootstraps across environments.
Figure 4: A comparison of reward inference using a correct model of the irrationality type, versusalways using a Boltzmann-rational model (β = 10), on the random MDPs (left) and the car envi-ronment (right). The impairment due to model misspecification greatly outweighs the variation ininference performance caused by various irrationalities. The error bars show the standard error of themean, calculated by the bootstrap across environments.
Figure 5: The gridworld used in section 4.1.
Figure 6: The analog of Fig. 3 for the gridworld. Error bars are the standard error of the mean. Thefindings are surprisingly similar as with the random MDPs. Note the more limited x-axis ranges weused in this experiment.
Figure 9: Boltzmann (β = 100) produces different policies for θ* = (1,1) vs. θ* = (4,4): When∣∣θ∣∣ is larger, the policy becomes closer to that of the rational demonstrator, as the differences inQ-values becomes larger.
Figure 11: The log loss (lower = better) of various models under parameter misspecification. Eachx-axis shows the parameter that the learner assumes. The orange line represents the performancewhen the learner makes the faulty assumption that the demonstrator is Boltzmann-rational. In manycases, the learners perform better than by assuming Boltzmann-rational just by getting the type of theplanner correct, even if they don’t get the exact parameter correct. The error bars show the standarderror of the mean, calculated by the bootstrap across environments.
Figure 12: The log loss (lower = better) of two myopic demonstrators under type misspecification.
