Figure 1: We generate realistic renderings of cut-and-paste images. Our method is entirely image-based and can convincingly reshade/relight fragments with complex surface properties (a lego dozer,a plant and a chair in top row) and matte, glossy and specular fragments (a set of 16 different materialsin bottom row) added to a spatially varying illuminated target scene (indoor-outdoor and day-night)without requiring the geometry of the inserted fragment or the parameters of the target scene.
Figure 2: Cut-and-Paste Neural Renderer. Given two images, a target scene (top row) and anew fragment added to this target scene (bottom row), our approach generates a plausible, realisticrendering of the composite image by correcting the fragment’s shading. Our method uses DIPas a neural renderer trained to produce consistent image decomposition inferences. The resultingrendering from DIP should have an albedo same as the cut-and-paste albedo; it should have a shadingand gloss field that, outside the inserted fragment, is the same as the target scene’s shading and glossfield. This simple process produces convincing composite rendering for any cut-and-paste images.
Figure 3: Shading Consistency U-Net is trained separately to discrim-inate consistent and inconsistentpixel-level shading-normal pairs.
Figure 4: Image Decom-position. Left: sam-ples drawn from ourparadigms that are usedto train our image decom-position network. Right:examples showing MSCOCO image decompo-sitions after training onparadigms. Note our im-age decomposition net-work is trained withoutreal image labeled data.
Figure 5: Better WHDR does not mean better reshading. Reshaded images when using target infer-ences from different image decomposition networks. Paradigm I achieves (relatively weak) WHDRof 22%; Paradigm II achieves 19%, close to state-of-the-art (SOTA) for an image decompositionnetwork that does not see rendered images (Liu et al., 2020). However, Paradigm II produces signifi-cantly worse reshading results. Furthermore, reshading using CGIntrinsics (Li & Snavely, 2018) (asupervised SOTA method) is also qualitatively worse than using Paradigm I. This reflects that betterrecovery of albedo, as measured by WHDR, does not produce better reshading. The key issue isthat methods that get low WHDR do so by suppressing small spatial details in the albedo field (forexample, the surface detail on the lego), but the normal inference method cannot recover these details,and so they do not appear in the reshaded image. From the perspective of reshading, it is better tomodel them as fine detail in albedo than in shading.
Figure 6: Our method has some implicit notion of the 3D layout of the scene, which is required tochoose the appropriate shading. Our method shades the white discs as spheres (rather better than IH,implying it "knows" about shape). Furthermore, a depth network Ranftl et al. (2020) applied to ourreshaded results recovers depths that span the volume of the scene rather more than for others. Notethe curious fact that the depth network can choose depths for CP and IH images confidently, too.
Figure 7: Glossy effects in car paint (with glitter in them) make reshading cars a particularlychallenging case with their complex reflective properties. Our method successfully reshades carswithout a distinct shift in object and background color produced by IH.
Figure 8: Our method reshades objects with complex surface geometry and complex materialproperties, and appears to preserve the identity of the material without a distinct shift in object andbackground color produced by IH.
Figure 9: Ablation study showing how different components of our loss helps in correcting overallshading. Results improve moving from left to right. We observe that our LSNC helps in improvinglocal changes based on the near-by surroundings and LSHC takes into account the overall illuminationand hence provides complementary solutions.
Figure 10: Failure Examples. The wronglyshaded regions are marked in red. In the first exam-ple, DIP aggressively copies background shadingonto the chair. However, the lego and the plantare reshaded accurately. In the second, the reasonfor failure is surface normals. The scene has twodominant normal directions - ground (pointing UP-wards) and the sky (pointed towards viewer). Thelack of third direction forces DIP to copy shadingeither from the sky or the ground plane.
