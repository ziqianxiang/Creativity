Figure 1: Standard pure 16-bit training shows lower train-ing accuracy compared to 32-bit training on a BERT model.
Figure 2: Theory validation. Ona least square regression model,(smoothed) training losses with 16-bit nearest rounding for weight up-dates saturate at a higher level than32-bit training. With only usingnearest rounding for forward andbackward compute, the losses satu-rate much closer to 32-bit training.
Figure 3: Training accuracy gap imposed by the standard pure 16-bit training. The standardalgorithm fails to match the training accuracy of 32-bit training, especially in the middle-to-latestage. We close this accuracy gap by ablating nearest rounding for weight updates from the standardalgorithm. This indicates that nearest rounding for model weight update is the accuracy bottleneck.
Figure 5: Training accuracy for pure 16-bit training. With stochastic rounding or Kahan sum-mation enabled for model weight updates, pure 16-bit training matches 32-bit precision training interms of training accuracy with negligible differences across the applications in our experiments.
Figure 4: Efficiency and accuracytrade-off. With stochastic roundingand Kahan summation on differentparts of the DLRM-Kaggle, it attainshigher model accuracy at the cost ofmore weight memory.
