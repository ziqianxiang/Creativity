Figure 1: Given an anchor (circle with dark ring), our approach samples the closest positive example in theembedding space as the positive element. This results in pushing the anchor only towards the closest elementdirection (green arrow), which allows the embedding to have multiple clusters for each class.
Figure 2: Embedding examples from the MNIST validation set, after training using only even/odd labels.
Figure 3: Results on Omniglot-letters. (a) Recall@1 performance of each model per epoch. (b)performance of EPS + distance-margin model on the Omniglot dataset, as a function of the number ofpositive samples in batch (where zero is equivalent to only using only distance sampling). Increasingthe number the number of positive samples enhances the model performance.
Figure 4: Retrieval results for randomly chosen query images in Cars196 dataset. Using EPS createsmore homogeneous neighbourhood relationships with respect to the car viewpoint.
Figure 5: t-SNE visualization of Cars196 training classes (each class has a different color). Trainingwith EPS results in more diverse classes appearance.
Figure 6: Recall@1 performance with Trimmed loss across varying trimming percentage. Except forsmall improvement in the Distance-margin case on the Omniglot dataset, in all other cases there is noimprovement when applying the Trimmed loss.
