Figure 1: Accuracy of CNN models averaged over ten tasks in a lifelong learning setting with95% confidence interval. This empirically shows that the optimal transfer configuration varies, andchoosing the correct configuration is superior to transfer at all layers.
Figure 2: The Alternating {2, 4} transfer configuration for three different approaches using CNNswith four convolutional layers and one fully connected layer. Models are illustrated for two tasks, redand green, with shared or transfer-based layers denoted in blue.
Figure 3: (Left) Performance of LASEM applied to three methods and three lifelong scenarios.
Figure 4: Frequency of each layer being transfer-based according to the selection of LASEM.
Figure 5: Performance of LASEM DF-CNN compared to LASEM with a fixedposterior distribution over the optimalconfiguration (on Office-Home).
Figure 6: Details of the task model architectures used in the experiments. Text by each convolutionallayer describes the filter sizes and the number of channels. All convolutional layers are zero-padded.
Figure 7: Catastrophic forgetting ratio of transfer at all CNN layers (blue), best static transferconfiguration (black) and LASEM (red), exhibiting the benefit of LASEM. Note that the y-axis rangediffers for each data set.
Figure 8: (Top) Histogram of the most-selected configurations (i.e., the binary vectors ct, where1 denotes that a CNN layer employs transfer). (Bottom) The fraction of the time each layer wasselected to be transfer-based (red) or task-specific (blue).
