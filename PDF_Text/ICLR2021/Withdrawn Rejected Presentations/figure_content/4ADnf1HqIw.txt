Figure 1: Texture coordinate perturbations (tex-ture sliding) reduce shape inference errors: groundtruth (blue), prediction (orange).
Figure 2: Consider an extreme case, where theinferred cloth has a quite large triangle (shownin red). That triangle should encompass the non-linear texture region outlined in yellow (shownin pattern space). Note: the yellow curve wasgenerated by sampling the ground truth cloth’stexture coordinates along the projected edges ofthe red triangle. The linearity assumption im-plied by barycentric interpolation instead uses theregion outlined in green.
Figure 3: The method discussed in Section 4.1can fail near silhouettes of the inferred and groundtruth cloth meshes, in which case smoothnessassumptions are used (see Section 4.2). In (a),inferred triangles with at least one vertex fallingoutside the silhouette of the ground truth meshare colored red. In (b), ground truth triangles withat least one vertex falling outside the silhouetteof the inferred mesh are colored blue.
Figure 4: Illustration of the ray intersection methodfor transferring texture coordinates to the inferredcloth from the ground truth cloth. Texture coor-dinates for the inferred cloth vertex (red cross)are interpolated from the ground truth mesh to thepoint of ray intersection (red circle).
Figure 5: Texture sliding neural network (TSNN)architecture.
Figure 6: Per-pixel texture coordinate errors be-fore (a) and after (b) applying texture sliding tothe inferred cloth output by the network of Jinet al. (2020). The result of a two-step process (c)may well match the ground truth in a visual sense,whilst still having quite large errors in materialcoordinates. Blue = 0, red ≥ 0.04.
Figure 7: Local compression (blue) and extension(red) energies for a sample pose, comparing theground truth cloth (a), the inferred cloth (b), andthe result of a two-step process (c). In spite of thecloth mesh in (c) bearing visual resemblance tothe ground truth in (a), it still has quite erroneousdeformation energies.
Figure 8: As the inferred cloth mesh (a) is sub-divided, texture sliding (b-d) moves the inferredmesh’s appearance closer to the ground truth (e).
Figure 9: A typical test set example prediction. (a)CN. (b) CN. (C) Per-PiXel errors (blue = 0, red≥ 0.04).
Figure 10: The results of the TSNN before (b) andafter (c) subdivision, as compared to the groundtruth (d). In spite of Table 2, some wrinkles arebetter resolved by the TSNN after subdivision.
Figure 11: Given two camera views (far left and far right images), texture sliding can be linearlyinterpolated to novel views between them. The top row shows per-pixel errors (blue = 0, red ≥ 0.04),and the bottom row shows the cloth from a fixed front-facing view to illustrate how the interpolatedtexture changes as a function of the chosen novel view.
Figure 12: The ground truth (a) and inferred cloth Figure 13: Per-pixel errors (top) and local com-(b) compared to the 3D reconstructions obtained pression/extension energies (bottom) for Figureusing texture sliding (c) and the TSNN (d).	12c (a) and Figure 12d (b).
