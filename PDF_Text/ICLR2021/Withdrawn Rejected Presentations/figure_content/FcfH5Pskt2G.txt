Figure 1: (a) The axes of VAEs latent representation, when decoded back to data space, representnon-linear principal components, as formalized by Rolinek et al. (2019) (figure used with the per-mission of authors). (b) Distribution of latent encodings for an input distributed as depicted in theinset. The linear VAE’s encoding matches the PCA encoding remarkably well (left); both focus onaligning with axes based on explained (global) variance. The non-linear VAE is, however, moresensitive to local variance. It picks up on the natural axis alignment of the microscopic structure.
Figure 2: A schematic visualization ofthe image generation process. The set ofground truth generating factors stays un-touched by the modification.
Figure 3: Dataset modification. (a) Construction of the manipulation pattern (transposed visualiza-tion). (b,c) Visual comparison between original and perturbed samples side-by-side.
Figure 4: A comparison of the local expansiveness factor, the explained global variance and theβ-VAE noise distribution. (a) Sampled local variance for different values of ε. Restricted to theposition factors (curves are overlapping) since the sampling in the generating factor space is discreteand varying step sizes are inexpressive. (b) Variance for all generating factors and s. (c) The meanlatent noise standard deviation is an indicator for the relevance for reconstruction. Increasing εmakes s the prime encoded coordinate. For ε ≤ 0.075, s was not encoded due to it’s noisy nature.
Figure 5: MIG scores for scaled literature hyperparameters over 10 restarts. The dashed lines showthe average number of active units. Less than 5 (left, dSprites) or 6 (right, shapes3D) active unitsmean the architectures overpruned and can not recover the true generative factors.
