Figure 1: Mean class representation O , prototypes □ , and 2-dimensional embeddings 人 learnt onperturbed MNIST by a 3-layer convolutional net with three different classification modules: (a)cross-entropy, (b) learnt prototypes, and (c) learnt prototypes guided by a tree-shaped taxonomy(constructed according to the authors’ perceived visual similarity between digits). The guidedprototypes (c) embed more faithfully the class hierarchy: classes with low error cost are closer1. Thisis associated with a decrease in the Average Hierarchical Cost (AHC), as well as Error Rate (ER),indicating that our taxonomy may contain useful information for learning better visual features.
Figure 2: Error Rate (ER) in % and Average Hierarchical Cost (AHC) on four datasets for ourproposed method (framed), the Cross-Entropy baseline (in bold), and the competing approaches. Thebest performances on each dataset are plotted in green. Our guided prototype approach improvesboth the ER and AHC across the four datasets compared to the baseline. The metrics are computedwith the median over 5 runs for CIFAR100, the average over 5 cross-validation folds for S2-Agri,and a single run for NYUDv2 and iNat-19. The numeric values are given in the Appendix. (? notevaluated on S2-Agri).
