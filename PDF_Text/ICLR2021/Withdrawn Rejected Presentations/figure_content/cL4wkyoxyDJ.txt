Figure 1: An illustration that shows applying small perturbations generated for non-predicted labels to the adversarial example helps suppress the adversarial effect andimprove the prediction probability for the correct category. +Pertb(i) denotes applyingthe small perturbation generated for i-th class label to the adversarial example.
Figure 2: Impact of the size of perturbations generated for defense purposes on classification accuracy (%). Wereport performance of resisting both untargeted and targeted PGD10 attacks.
Figure 3: Impact of the number of randomly selected class labels in our method on classification accuracy (%).
