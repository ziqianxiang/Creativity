Figure 1: Learning disconnected manifolds leads to the apparition of an area in the latent spacegenerating points outside the target manifold. With the use of the importance weighter, one can avoidthis specific area and better fit the target distribution.
Figure 2: In this synthetic experiment, the target distribution has two modes slightly shifted from thegenerated distribution. When using an optimal discriminator, DRS only selects the intersection ofthe two supports, thus removing important data. Our method achieves a much better fit and does notsuppress any mode. The EMD metric confirms the interest of the method in this specific case.
Figure 3: Gradient ascent on latent importance weights (latentGA): the quality is gradually improvedas we move to higher importance weights. Each image is generated only for the purpose of visualiza-tion, and one can run this gradient ascent directly in the latent space with the importance weighter.
Figure 4: Ranking CelebA faces with the trained importance weighter. For each line, we compare theworst-5% vs best-5% of the generated samples. We see a significant difference in terms of quality.
Figure 5:	Synthetic examples on mixtures of Gaussians with respectively (per column) 9, 16 and 25components. Real samples in green and generated points in blue.
Figure 6:	Comparisons of concurrent methods on synthetic datasets: mixtures of 25 Gaussians andon 2D SWiss Roll. Experimental setting from Tanaka (2019). Real samples in green and generatedpoints in blue.
Figure 7:	We observe that the ranking with the discriminator does not perfectly correlate with humanjudgment, prohibiting the importance weighter to learn meaningful features. Consequently, the targetdistribution is not better approximated.
