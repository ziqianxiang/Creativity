Figure 1: Compressed Activation Replay (CAR): During sequential training, compressed feature maps arestored in addition to the input-output pairs in the replay buffer. While training future tasks, models are optimizedusing (1) task loss, (2) replay task loss using samples from the replay buffer and (3) activation matching lossbetween the compressed activation maps of the current encoder model and the replay samples. Activationmatching loss present distribution drift from happening in the feature space.
Figure 2: Visualizing t-SNE embeddings of model trained using vanilla experience replay (a) and Com-pressed Activation Replay (b) on Split-CIFAR dataset. We compare the features of the encoder of the fi-nal model (shown in blue) relative to the encoder representations obtained while training the respective tasks(shown in red). We observe that older tasks encounter significant drift in the feature space when trained withexperience replay. Using activation matching loss, our Compressed Activation Replay framework reduces thedrift as the blue dots better cover the support of red dots, especially on Task 5 and 10. We note that for the veryearly task (Task 0), feature drift can still be observed even if we use CAR.
Figure 3: Visualizations of predictions made on Taskonomy dataset. Compressed Activation Replay (CAR)drives the predictions to higher quality, compared with vanilla SGD and ER.
