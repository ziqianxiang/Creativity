Figure 1: Examples of global minima for learning the read-once DNF: (x1 ∧ x2 ∧ x3) ∨ (x4 ∧ x5 ∧x6) ∨ (x7 ∧x8 ∧x9) with a convex network. (a) Global minimum that memorizes the training points.
Figure 2: Illustration of pruning and reconstruction for the read-once DNF: (x1 ∧ x2 ∧ x3) ∨ (x4 ∧x5 ∧ x6) ∨ (x7 ∧ x8 ∧ x9). (a) The global minimum that GD converges to. (b) Global minimumafter pruning (Definition 6.1). (c) Global minimum after pruning followed by β-reconstruction(Definition 6.2).
Figure 3: Experiments for learning read-once DNFs. (a) Test performance of convex and standardtwo-layer networks for D = 9. (b) Reconstruction success rate for convex and standard two-layernetworks, for D = 27. (c) Model learned by a convex network for DNF with D = 100 and smallinitialization (d) Same setting as (c) with large initialization.
Figure 4: Test accuracy for the convex and standard networks. (a) D = 10 and the target DNF has 3terms of size: (2,3,3) (b) D = 27 and the target DNF has 3 terms of size 3, 2 terms of size 2 and oneterm of size 6. (c) D = 35 and the target DNF has 3 terms of size 2, 4 terms of size 4 and 2 terms ofsize 2.
Figure 5: Reconstruction success rate for the convex and standard networks. (a) D = 10 and thetarget DNF has 3 terms of size: (2,3,3) (b) D=30 and the target DNF has 5 terms of size 6. Standardnetwork fails to reconstruct (success rate below 5%). (c) D = 100 and the target DNF has 15 termsof size 5. Standard network fails to reconstruct (successs rate below 5%).
Figure 6: Model learned by a convex network for DNF with D = 27 for different initailization scales.
