Figure 1: Overview of the framework of our proposed methodNLU tasks. All these works focus on encouraging PrLMs to incorporate coarse-grained information.
Figure 2: Segmentation Examplescoarse-grained enhanced representation. Eventually, the fine-grained representation of [CLS] tokenprovided by BERT and the coarse-grained information enhanced representation are concatenated toform the final representation that makes the most of multi-grained information for downstream tasks.
Figure 3:	Influence of dictionary size on the average number of spans in the sentences8 7 65 5 5巴nb456.80Radom54.854.720k	50k 100k 200k 500k 1000kSize of n-gram dictionary(a) CoLA10k&号。。4RadOm 10k	20k	50k 100k 200k 500k 1000kSize of n-gram dictionary(b) MRPCFigure 4:	Quantitative study on the influence of the size of n-gram dictionary6Attention indicate the Self-attentive module (Lin et al., 2017).
Figure 4:	Quantitative study on the influence of the size of n-gram dictionary6Attention indicate the Self-attentive module (Lin et al., 2017).
