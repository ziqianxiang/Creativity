Figure 1: Training Curves of average return and 10% worst-Case return.
Figure 2: Heatmap of return in unseen environments on Waler2d, Hopper and Halfcheetah withpolicies trained by MRPO, PW-DR and DR in the training environments.
Figure 3:750.0 ■805.6 -861.1916.7 -972.21027.8 -1083.3 -1138.9 ■1194.4 ■1250.0 -3 一 suαjp(a) iteration k=300■"SUSP(b) iteration k=301750.0 ■805.6 -861.1916.7 -972.2
Figure 4: (a) Training curves of average return of MRPO on Hopper with different L; (b) Timeelapsed versus number of iterations curves during training.
Figure 5: (a) Training curves of (a) average return and (b) 10% worst-case return of MRPO onHopper with different κ.
Figure 6: Heatmap of return in unseen environments on Cartpole and InvertedDoublePendulum withpolicies trained by MRPO, PW-DR and DR in the training environments.
