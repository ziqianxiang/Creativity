Figure 1: Performance of constrained fine-tuning of all Transformer blocks for BERT-Large, BERT-Base, and ALBERT on T-REx.
Figure 2: Performance of fact modification for BERT-Base and BERT-Large on the T-REx benchmark. WeFigure 3: Performance of fact modification for a BERT-Base model on the zsRE benchmark, using the FT+FTMsetup with constrains during FTM. From left to right, the columns show the test accuracy for modifying 32, 128,512, and 2048 facts, respectively. In each column, we show the best accuracies of constrained finetuning 0th,5th, 11th, and all Transformer blocks of BERT-Base, which We achieve under different '∞ constraints. Theresults are averaged over 5 independent runs.
Figure 3: Performance of fact modification for a BERT-Base model on the zsRE benchmark, using the FT+FTMsetup with constrains during FTM. From left to right, the columns show the test accuracy for modifying 32, 128,512, and 2048 facts, respectively. In each column, we show the best accuracies of constrained finetuning 0th,5th, 11th, and all Transformer blocks of BERT-Base, which We achieve under different '∞ constraints. Theresults are averaged over 5 independent runs.
Figure 4: Mean and standard deviation of test accuracies after fine-tuning randomly initialized, pretrained, andfintuned pretrained models on different number of modified facts of T-REx dataset, denoted as RI+FTM, FTM,and FT+FTM, respectively. Here, RI refers to starting from a randomly initialized model, “fintuned pretrainedmodel” FT refers to starting from a off-the-shelf pretrained model and fine-tune on unmodified T-REx dataset.
