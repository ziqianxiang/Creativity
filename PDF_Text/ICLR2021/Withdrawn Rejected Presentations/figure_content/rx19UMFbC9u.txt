Figure 1: Model accuracy and percentage of dead connections under varying compression ratios:LeNet-300-100 on MNIST, ResNet-18 on CIFAR-10, and ResNet-50 on Tiny-ImageNet. We usedIMP with learning rate rewinding (Renda et al., 2020) as the base pruning method. The percentage ofdead connections is calculated by the number of dead connections divided by the total number ofconnections in a given network.
Figure 2: Overall procedure of all-alive pruning (AAP) with a magnitude-based criterion. Whileexisting pruning has a dead neuron (red circle) and its dead connections (red line), AAP safelyeliminates dead connections and revive other connections (green line).
Figure 3:	Accuracy comparison between IMP and IMP-AAP across various model architectures andcompression ratios by iterative pruning. We use weight rewinding for MNIST dataset and learningrate rewinding for the rest.
Figure 4:	Accuracy of applying AAP with various saliency-based pruning (i.e., one-shot magnitudepruning (One-shot MP) (Frankle & Carbin, 2019; Renda et al., 2020), SNIP (Lee et al., 2019),lookahead pruning (LAP) (Park et al., 2020), and dynamic pruning by RigL (Evci et al., 2019).
Figure 5: Visualization of remaining weights after applying AAP to one-shot MP at 128× compressionratio using the same unpruned network. The second fully-connected layer (300 × 100) of LeNet-300-100 on MNIST is chosen for the illustration. Each dot denotes the unpruned weight. Whereas 138out of 300 neurons and 93 out of 100 neurons have unpruned connections in one-shot MP, only 85out of 300 neurons and 44 out of 100 neurons have unpruned connections after applying AAP. Eachachieves 73.23% and 93.12% accuracy, respectively.
Figure 6: Visualization of remaining weights in the second fully-connected layer of LeNet-300-100after applying AAP to one-shot MP at 64× compression ratio. 250 out of 300 neurons and 99 outof 100 neurons have unpruned connections in one-shot MP, and 220 out of 300 neurons and 77 outof 100 neurons have unpruned connections after applying AAP. Each achieves 96.01% and 96.48%accuracy after pruning, respectively.
Figure 7: Visualization of remaining weights in the second fully-connected layer of LeNet-300-100after applying AAP to one-shot MP at 256× compression ratio. 88 out of 300 neurons and 64 out of100 neurons have unpruned connections in one-shot MP, and 36 out of 300 neurons and 29 out of 100neurons have unpruned connections after applying AAP. Each achieves 38.06% and 89.65% accuracyafter pruning, respectively.
Figure 8: Accuracy of applying AAP incrementally in one-shot magnitude pruning. AAP(1%) denotesthat the process of pruning n(θ)-τ connections and eliminating corresponding dead connections wasrepeated in order to reach the final sparsity threshold τ . Note that we only eliminate dead connectionswithout any training process differs from the iterative pruning.
