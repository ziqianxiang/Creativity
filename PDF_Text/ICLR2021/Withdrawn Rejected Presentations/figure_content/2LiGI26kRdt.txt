Figure 1: The framework of MSLT method. The green blocks mean that these layers only partici-pate in forward computation and are not updated. The red blocks mean that these layers are trainedin this stage and they participate in both forward and backward computations.
Figure 2: The attention distribution of a trained BERT-Base model given a sample sentence. Wecompare the attention distribution of the same heads from different layers. For example, “L5 H4”denotes the attention distribution from the forth heads of the fifth layer.
Figure 3: The first row is the attention distributions for a randomly chosen sample sentence onrandom 6 heads of the top 6 layers from the final trained 12-layer BERT-Base model. The secondrow is the attention distributions of the bottom 6 layers from the final trained 12-layer BERT-Basemodel. The third row is the attention distributions of the encoder layers from the trained 6-layermodel before stacking. The fourth row is the attention distributions of the bottom 6 layers from theBERT-Base model trained from scratch using the original method.
Figure 4: Pre-training loss curves of the BERT models.
