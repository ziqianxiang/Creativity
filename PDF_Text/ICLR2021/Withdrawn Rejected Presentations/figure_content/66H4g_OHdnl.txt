Figure 2: Verification of Remark 2.1. (a) Rank of the hidden layer weight matrix as a function of β and (b)rank of the hidden layer weights for different regularization parameters, i.e., β1 < β2 < β3 < β4.
Figure 3: Verification of Proposition 3.1 and 4.1. (a) Evolution of the operator and Frobenius norms for thelayer weights of a linear network and (b) Rank of the layer weights of a ReLU network with K = 1.
Figure 4: Training and test performance on whitened and sampled datasets, where (n, d) = (60, 90), K = 10,L = 3, 4, 5 with 50 neurons per layer and we use squared loss with one hot encoding. For Theory, we use thelayer weights in Theorem 4.3, which achieves the optimal performance as guaranteed by Theorem 4.3.
Figure 5: (a) Projection of the hidden neurons to the right singular vectors claimed in Remark 2.1 and (b)singular values of W1 with respect to β .
