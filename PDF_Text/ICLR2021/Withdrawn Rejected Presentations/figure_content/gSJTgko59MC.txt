Figure 1: Leading FWC to the mean IfU(x*)| (Eq. 11) and GP discrepancy(RMSE) as a function of train set size n for varying training noise σ2. Thetarget is quadratic g(x) = XTAx = O(1) with X ∈ Sd-I(Vzd) so the numberof parameters to be learnt is d(d + 1)/2 (vertical grey dashed line). The GPdiscrepancy is monotonically decreasing with n whereas IfU (x*)| increaseslinearly for small n (dashed-dotted lines in (a)) before it decays (best illustratedfor larger d and σ2). For sufficiently large n, both the GP discrepancy andIfU (x*)| scale as 1/n (diagonal dashed black lines in (b),(c)). This verifies ourprediction for the scaling of FWCs with n, Eq. 14 in the large n regime . Notably,it implies that at large N FWCs are only important at intermediate values of n.
Figure 2: Fully connected 2-layer network trained on a regression task. (a)Network outputs on a test point f (x*, t) vs. normalized time: the time-averagedDNN output f(x*) (dashed line) is much closer to the GP prediction fGP(x*)(dotted line) than to the ground truth y* (dashed-dotted line). (b) ACFs of thetime series of the 1st and 2nd layer weights, and of the outputs: the outputconverges to equilibrium faster than the weights. (c) Relative MSE between thenetwork outputs and the labels y (triangles), GP predictions f GP (x*) Eq. 6 (dots),and FWC predictions Eq. 10 (x’s), shown vs. width for quadratic (blue) andReLU (red) activations. For sufficiently large widths (N & 500) the slope of theGP-DNN MSE approaches -2.0 and the FWC-DNN MSE is further improvedby more than an order of magnitude.
Figure 3: DNN-GP MSE demonstrates con-vergence to a slope of -2.0, validating thetheoretically expected scaling. DNN-groundtruth (Y) MSE shows finite CNN can outper-form corresponding GP.
