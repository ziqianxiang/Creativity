Figure 1: Graphical illustration of the joint attention mechanisms used in each layer of JATs. Left:Joint attention mechanism using Implicit direction strategy. Right: Joint attention mechanism usingExplicit direction strategy. Both two mechanisms consider structural coefficients learned from graphadjacency.
Figure 2: Sensitivity test on and automatic determination of S/F significance(b) JAT-E5	ConclusionIn this paper, we propose novel attention-based GNNs, dubbed Graph Joint Attention Networks(JATs). Different from previous related approaches, JATs adopt novel joint attention mechanismsthat can smoothly infer the relative significance between graph structure and node features, so thatstructural properties and node features are appropriately preserved in node representations. Besides,the expressive power of JATs is theoretically analyzed and the improved strategy to ensure JATs tobe most powerful message-passing GNNs is also proposed. In future, we will further improve theeffectiveness of JATs by considering different structural properties hidden in the network data andexplore JATs’ applicability by using them in multi-view and heterogeneous network data.
Figure 3: Sensitivity test on βD.3 SENSITIVITY TEST ONβFor sensitivity test on β, which determines the relative significance of graph self-expression (Eq.
