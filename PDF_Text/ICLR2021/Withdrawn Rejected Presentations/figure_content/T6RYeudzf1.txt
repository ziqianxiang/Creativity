Figure 1: TextSETTR architecture for label-free style transfer. The Encoder, Decoder and StyleExtractor (Ex) are transformer stacks initialized from pretrained T5. During training, the modelreconstructs a corrupted input, conditioned on a fixed-width “style vector” extracted from the pre-ceding sentence. At inference time, a new style vector is formed via “targeted restyling”: addinga directional delta to the extracted style of the input text. Stochastic tuning ranges provide extraconditioning for the decoder, and enable fine-grained control of inference.
Figure 2:	Automatic evaluation metrics comparing our TextSETTR model, ablations, and previouswork. Up-and-right is better. We train for 10k steps and use add/delete:20-40% unless otherwisespecified. We recalculate metrics for previous approaches, using our BERT classifier for accuracy,ensuring direct comparability with our models.
Figure 3:	Comparison with Lample et al. (2019) on the evaluation setting that includes pos→posand neg→neg transfers. Note, a model that simply copies its input can achieve 50% accuracy.
Figure 4: 2D UMAP embeddings of the style vectors extracted by our TextSETTR model beforeand after training, for text inputs from Amazon reviews covering three product categories and twosentiment labels. Within each row, the same embeddings are visualized with product category labels(left) and sentiment labels (right).
Figure 5: BLEU scores between model outputs and human references provided by Li et al. (2018),along with self-BLEU for comparison. The first group of models in the table had access to labelsat training time, while the second group did not. TextSETTR (X-Y%) refers to our model withadd/delete rate ranges set to X-Y%.
Figure 6: Python code to extract adjacent lines of text from raw Amazon reviews, producing outputsin a similar style to Li et al. (2018).
