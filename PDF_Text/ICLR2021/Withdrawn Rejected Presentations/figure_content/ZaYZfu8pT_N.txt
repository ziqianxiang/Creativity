Figure 2: Illustration of the procedure of glancing samplingunidirectional way. NAT, on the other hand, incorporates conditional independence assumptionamong words in a sentence with the aim of enabling parallel generation:TLnat = X log p(yt∣X ； θ).	⑵t=1Note that, in NAT, different yt in Y are predicted simultaneously, which removes the interactionsamong target words in the NAT modeling. Thus, to generate fluent and faithful sentences, NAT has tomodel the target word dependencies implicitly, which makes the learning process quite challenging.
Figure 3: The trade-off between speed-up andBLEU on WMT14 DE-ENFigure 4: Performance under different sourceinput length on WMT14 DE-ENFigure 5: The BLEU scores of GLAT withdifferent decoding iterationsFigure 6: Training NAT with different initial-ized encoderthe BLEU score for each interval. The histogram of results is presented in Figure 4. NAT-base,sperformance drops sharply for long sentences, while the gradual learning process enables GLAT toboost the performance by a large margin, especially for long sentences. We also find that GLAToutperforms autoregressive Transformer when the source input length is smaller than 20.
Figure 4: Performance under different sourceinput length on WMT14 DE-ENFigure 5: The BLEU scores of GLAT withdifferent decoding iterationsFigure 6: Training NAT with different initial-ized encoderthe BLEU score for each interval. The histogram of results is presented in Figure 4. NAT-base,sperformance drops sharply for long sentences, while the gradual learning process enables GLAT toboost the performance by a large margin, especially for long sentences. We also find that GLAToutperforms autoregressive Transformer when the source input length is smaller than 20.
Figure 5: The BLEU scores of GLAT withdifferent decoding iterationsFigure 6: Training NAT with different initial-ized encoderthe BLEU score for each interval. The histogram of results is presented in Figure 4. NAT-base,sperformance drops sharply for long sentences, while the gradual learning process enables GLAT toboost the performance by a large margin, especially for long sentences. We also find that GLAToutperforms autoregressive Transformer when the source input length is smaller than 20.
Figure 6: Training NAT with different initial-ized encoderthe BLEU score for each interval. The histogram of results is presented in Figure 4. NAT-base,sperformance drops sharply for long sentences, while the gradual learning process enables GLAT toboost the performance by a large margin, especially for long sentences. We also find that GLAToutperforms autoregressive Transformer when the source input length is smaller than 20.
