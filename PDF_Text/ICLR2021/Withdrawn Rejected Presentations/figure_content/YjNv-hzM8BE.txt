Figure 1: The overview of VECO. During pre-training, We feed two masked segments X and y^ intodifferent modules to perform inner-sentence mask language modeling (IS-MLM) and cross-sentencemask language modeling (CS-MLM). More specifically, the masked segment X can only attend to itscontext via self-attention to recover the original tokens X (IS-MLM), while masked segment y^ canattend to its preceding tokens via self-attention and the context X via cross-attention to predict theoriginal tokens y (CS-MLM). For downstream NLU tasks, we throw out the cross-attention moduleand only fine-tune on the self-attention and FFN modules acted as an encoder. For NLG tasks, wekeep all modules to initialize the corresponding encoder and decoders.
