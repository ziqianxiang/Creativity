Figure 1: Schematic diagram of the Selfish-RNN. Wi , Wf , Wc, Wo refer to LSTM cell weights.
Figure 2: The number of weights whose mag-nitude is larger than 0.1 during training forON-LSTM. The solid lines represent SNT-ASGD and dashed lines represent standardNT-ASGD. ih1, ih2, and ih3 refer to inputweights in the first, second and third LSTMlayer.
Figure 3: (left) One random-initialized sparse network trained with Selfish-RNN end up with a verydifferent sparse connectivity topology. (right) Two same-initialized networks trained with differentrandom seeds end up with very different sparse connectivity topologies. ih is the input weight tensorcomprising four cell weights and hh is the hidden state weight tensor comprising four cell weights.
Figure 4: Comparison between random-based growth and gradient-based growth. (left) Modelstrained with SNT-ASGD. (right) Models trained with momentum SGD.
Figure 5: Sensitivity analysis for sparsity levels and the initial removing rate for Selfish stackedLSTMs, RHNs, ON-LSTM, and AWD-LSTM-MoS. (a) Test perplexity of all models with varioussparsity levels. The initial removing rate of stacked LSTMs is 0.7, the rest is 0.5. The dashed linesrepresent the performance of dense models. (b) Test perplexity of all models with different initialremoving rates. The sparsity level is 67%, 52.8%, 55% and 55% for Selfish stacked LSTMs, RHNs,ON-LSTM, and AWD-LSTM-MoS, respectively.
Figure 6: (left) A high-level overview of the difference between Selfish-RNN and the iterative pruningand re-training techniques. Blocks with light blue color represent optional pruning and retrainingsteps chosen based on specific approaches. (right) Comparison of the single model perplexity on testsets of Selfish-RNN, RigL and iterative magnitude pruning with stacked LSTMs on PTB.
Figure 7: (left) The topological distance between two different training runs of stacked LSTMstrained with random growth. (right) The topological distance between two different training runs ofstacked LSTMs trained with gradient growth.
Figure 8: (left) The topological distance between two stacked LSTMs with same initialization butdifferent training seeds trained with random growth. (right) The topological distance between twonetworks with same initialization but different training seeds trained with gradient growth. ih is theinput weight tensor comprising four cell weights and hh is the hidden state weight tensor comprisingfour cell weights.
Figure 9: Breakdown of the final sparsity level of cell weights for stacked LSTMs, RHNs, ON-LSTMon PTB, AWD-LSTM-MoS on Wikitext-2. W and R represent the weights tensors at each layer forRHNs; ih and hh refer to the input weight and the hidden weight in each layer, respectively for therest models.
