Figure 1: Left panel: a distribution pt is obtained by applying a transformation T to the support ofp-k . Right panel: three scenarios to consider when analyzing problem 5. Red distribution representsp-k and blue distribution represents pk . The data space X is represented by the whole space insidethe square, and the perturbation space S is represented by the gray area.
Figure 2: Plots of contours and gradient vector fields of the D functions (gradient vectors arenormalized to have the unit length). (a) The initial positions of p-k and pk. (b) The solution obtainedby the maximin problem solver. (c) The solution obtained by the maximin problem solver when p-kis a uniform distribution in the data space. (d) The solution obtained by the minimax problem solver.
Figure 3: Uncurated samples generated by our method; GANs results is in Figure 10. Seed imagesused to generated these results are in Figure 11. The training time for models used to produce thesegenerations are in Table 7.
Figure 4: The results ofpt and D in the first few iterations of a 2D simulation of Algorithm 1. SteP 2solves the inner minimization, causes suPPort of pt (red Points) to be concentrated in local maximaPoints. SteP 3 uPdate D by increasing its outPuts on the suPPort of pk and decreasing its outPuts onthe suPPort of pt , causes local maxima to be suPPressed.
Figure 5: Solutions obtained by the maximin problem solver (Algorithm 1) with different initializa-tions of D. First row are results when p-k is at bottom left (see Figure 2), and second row are resultswhen p-k are uniform distributions.
Figure 6: Solutions obtained by the minimax problem solver (Algorithm 2) with different initial-izations of D. Note that in all cases pt (red distribution) matches pk (blue distribution), but D hasunpredictable outputs on X \ Supp(pk). The initial position of the red distribution is in bottom leftcorner (see Figure 4(a)).
Figure 7: CIFAR-10 training curves of λ = 0.1. Left: training loss curves, middle: AUROC curves(pt vs. pk), and right: AUC curves (p-k vs. pk).
Figure 8: CIFAR-10 training curves of a failed training instance (λ = 0.6).
Figure 9: Generated samples after training with an increasing sequence of K values (K = 0, 1, 2).
Figure 10: Samples generated by GANs (Kurach et al., 2018); results of our method are in Figure 3.
Figure 11: Seed images used to generated samples in Figure 3. Seed images for CIFAR-10 andBedroom-128 are generated by applying Gaussian blur to random images from ImageNet test set.
Figure 12: Face retouching results. Top row are original images from the CelebA-HQ-128 test set,and bottom row are enhanced images. The strength of retouching could be increased by performingmore steps of gradient ascent on D .
Figure 13: Samples generated by CIFAR-10 models trained with different Ks. These generations alluse the seed images in Figure 11(a).
Figure 14: Samples generated by CelebA-HQ-128 models trained with different Ks. These genera-tions all use the seed images in Figure 11(b).
Figure 15: Samples generated by Bedroom-128 models trained with different Ks. These generationsall use the seed images in Figure 11(c).
Figure 16: Samples generated by the CIFAR-10 K = 40 model using seed images on the left. Seedimages are from OOD datasets (Table 6.
Figure 17: Samples generated by the CelebA-HQ-128 K = 80 model using seed images on the left.
Figure 18: Samples generated by the Bedroom-128 K = 55 model using seed images on the left.
Figure 19: Uncurated 256 × 256 generation results in the CelebA-HQ-256 dataset.
Figure 20: Uncurated 256 × 256 generation results in the Bedroom256 dataset. The state-of-the-artresults on this dataset can be found in Figure 10 of Karras et al. (2019).
Figure 21: Uncurated 256 × 256 generation results in the ImageNet Dog 256 dataset. The state-of-the-art results on this dataset can be found at Brock et al. (2018), although their results are ofresolution 128 × 128 and at the same time class-conditional. Unconditional generation results on thisdataset can be found at Zhang et al. (2018).
