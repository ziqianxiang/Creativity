Figure 1: (Left) An illustration of how an additive approximation bound can be used to guarantee label con-sistency. If each pre-softmax logit can only change by , then it is impossible for the second-largest logit tosurpass the largest logit under pertubation. However, the label may change if each logit can change by 0,which is more than half the distance between them. (Right) In the layer-by-layer approach, we start by pruningthe “first” layer (which is closest to the input) with some approximation guarantee on the first layer hiddenactivations. This epsilon error can be “bubbled up” to the output for an implied approximation guarantee onthe output g(). We proceed to prune the second hidden layer given the pruned first layer activations as input,again noting the guaranteed approximation error β and bubbling up to the output. This is repeated for eachlayer, accumulating approximation error for the deviations in each hidden layer.
Figure 2: Above are the results for epsilon-first pruning the final layer of LeNet-300-100 and VGG-16 undervarious training circumstances (Section 4.2), guaranteeing that 50% of the development set labels that arecorrect will remain correct. We show the calculated epsilon bounds (multiplicative or additive) required toguarantee 50% accuracy (Section 4.1) along with the actual observed approximation error, average and standarddeviation. We also show the layer size (number of neurons or number of weights) as well as the number ofsamples required by the theorem to meet the epsilon bound. Note that the sparsity may be different than thenumber of samples required, since sampling is done with replacement.
Figure 3: (Left) The results of pruning the final layer of “difficult case” LeNet-300-100 with sparsity-firstdeterministic weight pruning. The provided accuracy guarantees vastly under-estimate the accuracy after prun-ing, which can be attributed to loose bounds. Fine-tuning increases accuracy after pruning by more than 50%,which we call the fine-tuning gap. (Middle) Fine-tuning increases approximation error in the pre-softmax logitsrather than decrease it. This implies that approximation bounds would not be a good predictor of the size of thefine-tuning gap. (Right) An illustration of Softmax translation invariance, where y = y - C. Both Pre-Softmaxactivations lead to exactly the same probability distribution over classes, since the output of softmax dependsonly on the difference between logits, not their magnitude.
Figure 4: (Top, Middle) Algorithms from Baykal et al. (2019b) for pruning a single perceptronwith positive weights via a data-dependent coreset method. (Bottom) Algorithm from Mussayet al. (2020) for pruning a single hidden-layer feed-forward neural network data-independent coresetmethod.
Figure 5: The development accuracy of LeNet-300-100 models after pruning the final layer usingheuristic methods and also after fine-tuning. On the left is neuron pruning via L0 regularization, andon the right is weight pruning via magnitude weight pruning.
Figure 6: The full results of pruning the final layer of LeNet-300-100 under easy, moderate, anddifficult training circumstances (Section 4.2). Shown are the implied epsilon bounds required toreach each desired accuracy guarantee (additive for neuron pruning, multiplicative otherwise), aswell as the observed approximation (average and standard deviation). We also show the number ofsamples required to meet the epsilon bound as well as the sparsity achieved after sampling.
Figure 7: Samples required and sparsity (which is 0) for pruning LeNet-300-100 neurons via acoreset method, when dropout was applied during training at a rate of 0.5.
Figure 8: Results for pruning a “moderate case” LeNet-300-100 with sparsity-first methods. Shownare the accuracy and approximation error achieved during pruning, as well as the implied accuracyguarantee and implied approxiation error bound computed from the number of samples taken.
Figure 9: The development accuracy of VGG-16 models after pruning the final layer using heuristicmethods and also after fine-tuning. On the left is neuron pruning using L0 regularization, and on theright is weight pruning using magnitude weight pruning.
Figure 10: Full results of pruning the final layer of VGG-16. See Figure 6 and Section B.3. If thenumber of samples required is > 1M, we refrain from performing the computation and assume thatthe sparsity is 0. (Though in the easy and moderate cases, it would be > 0.)Samples ReqSparsity AchievedImplied Epsilon GuaranteeAVq Observed Errorsamples ReqSparsity AchievedSamples ReqSParSitY Achieved----samples Req----- Sparsity Achieved18Under review as a conference paper at ICLR 2021Figure 11: Approximation error of neuron pruning with a coreset method, when the inputs are nottaken from the training distribution but are instead drawn from the L2 ball with radius β, which isthe maximum L2 norm allowed by the assumptions of the approximation bound proof.
Figure 11: Approximation error of neuron pruning with a coreset method, when the inputs are nottaken from the training distribution but are instead drawn from the L2 ball with radius β, which isthe maximum L2 norm allowed by the assumptions of the approximation bound proof.
Figure 12: Results for pruning LeNet-300-100 with a coreset based neuron pruning method, aftermultiplying the pruned layer’s weight matrix by a constant α.
