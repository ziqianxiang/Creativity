Figure 1: Comparison of the benchmark implementation (Ovadia et al., 2019), versus our single andmultiple image methods. Mean Expected Calibration Error (ECE) across different corruptions types,for fixed corruption intensity going from 0 to 5. Each box represents a different uncertainty method.
Figure 2: Visualization of calibration errors for the Vanilla method on corrupted images on ImageNetand CIFAR-10 using the elastic transform corruption with intensity 4. The x-axis corresponds tothe pmax values and the y-axis to the confidence estimates of pcorrect . The blue histogram is thecalibration probabilities, the orange is the test probabilities, and the brown is where both overlap.
Figure 3: Histogram of the pmax values for the vanilla method on CIFAR-10. Top: calibration sets{0}, {0,1}, {0,2}, {0,3}, {0,4}, and {0,5} (left to right) with contrast corruption. Bottom: test setwith elastic transform corruption with increased intensities ranging from 1 to 5 (left to right) andSVHN dataset (right most picture). The multi image method outputs calibrated probabilities basedthe surrogate calibration set {0, 3},{0, 3},{0, 3},{0, 4},{0, 4},{0, 5}, respectively.
Figure 4: Comparison of the benchmark implementation (Ovadia et al., 2019) versus our single andmultiple image methods. Expected Calibration Error (ECE) distribution across different corruptionstypes, for fixed corruption intensity going from 0 to 5. Each box represents a different uncertaintymethod.
Figure 5: Comparison of the benchmark implementation (Ovadia et al., 2019) versus our single andmultiple image methods. Mean Brier score across different corruptions types, for fixed corruptionintensity going from 0 to 5. Each box represents a different uncertainty method. See Tables 3 and 4for numerical comparisons.
Figure 6: Comparison of the benchmark implementation (Ovadia et al., 2019) versus our singleand multiple image methods. Brier score distribution across different corruptions types, for fixedcorruption intensity going from 0 to 5. Each box represents a different uncertainty method. SeeTables 3 and 4 in the Appendix for numerical comparisons.
Figure 7: Confidence of CIFAR-10 (bottom) trained models on entirely OOD data (SVHN dataset(Netzer et al., 2011)). The benchmark method (blue) has highest confidence. The single and multiimage method are much less confidence on OOD data.
Figure 8: Confidence of MINST trained models on entirely OOD data: Fashion-MNIST (Xiao et al.,2017) and Not-MNIST (Bulatov, 2011). Our proposed methods are significantly less confident onOOD than the benchmark method.
Figure 9: Figure 1 shows us the mean ECE across different corruptions types, for fixed corruptionintensity going from 0 to 5 when contrast is used in the calibration sets. Here we show how thosemeans change when different corruptions are used in the calibration set. For CIFAR-10, our pro-posed methods are robust to the choice of corruption used in the calibration set, while for ImageNetthe choice of the corruption is import, in particular for the single image method.
Figure 10: Comparison of the benchmark implementation (Ovadia et al., 2019), versus our proposedsingle and multiple image methods and Top1 binning (Oberman et al., 2020) for CIFAR-10. MeanExpected Calibration Error (ECE) across different corruptions types, for fixed corruption intensitygoing from 0 to 5.
