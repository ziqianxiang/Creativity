Figure 1: Lower bounds on the robustness of an input example x. (a) Case 1: the input x (black) iscloser to the boundary of the polytope Q(x) (grey) than to the decision boundary (red). Our ACRterm improves robustness in this case. (b) Case 2: the input x is closer to the decision boundary thanto the boundary of Q(x). Smoothing f addresses this case. (c) The analytic center xac of a convexpolytope. The inner ellipsoid corresponds to the Dikin ellipsoid at xac. The dashed lines show levelcurves of the logarithmic barrier function.
Figure 2: Recovered linear regions and linearizations of a trained feedforward, single hidden layer(128 neurons) ReLU network on the two-moons dataset. (a): Polytopes defined by a vanilla networkfvanilla . (b): Linearization of fvanilla restricted to the polytopes in (a). (c): Polytopes defined bya robust network frobust. (d): Linearization of frobust restricted to the polytopes in (c). Note thedifference in the granularity of the partitions and the overall smoothness of the classifier. The flattransparent pink (b) and grey (d) planes correspond to the plane defined by y = 0. The intersectionof the networks with these planes correspond to the decision boundaries.
Figure 3: Distributions of MMR and GR robustness certificates for regularized networks. Fornetworks trained with GR and AT, the GR certification matches closely with MMR.
Figure 5: GR over robust training. As expected, the volumes of polytopes (certification of robustness)increases over training.
Figure 6: We plot decision boundaries via Eq. 11 for different values of λL. As expected, as λLincreases, the decision boundary exhibits less curvature.
Figure 7: We plot visualization of neuron weights for networks trained with different regularizers onMNIST.
Figure 8: We plot the analytic centers for trained networks. We see that the analytic centers for vanillanetworks roughly correspond to noise. However, the analytic centers of polytopes induced by robustnetworks are clearly interpretable and similar to the inputs they are conditioned on.
Figure 9: We evaluate the stochastic local Lipschitz constant (Eq. 12) networks trained in differentways on the Two Moons dataset uniformly over the grid. We see that GR exhibits local smoothnessin regions around potentially adversarial examples, while preserving nonsmoothness elsewhere. Incontrast, the networks trained with MMR and adversarial training exhibit smoothness globally.
Figure 10: We plot the decision boundaries for robust and non-robust networks on the Two Moonsdataset corresponding to Eq. 11 —the difference in likelihoods between the most likely class andthe second most likely class. We see that relative to networks trained with other methods, networkstrained with GR exhibits a larger gap - i.e. more confidence for regions around the decision boundary(Pa - Pb is large).
Figure 11: We evaluate the GR (top) and MMR (bottom) regularization terms uniformly over a gridfor networks trained in different ways on the Two Moons dataset.
Figure 12: We evaluate the GR (top) and MMR (bottom) regularization terms uniformly over a gridfor networks trained in different ways on the Two Moons dataset. Although the numerical scale ofthe costs differ, it seems clear that GR exhibits the desirable property smoothness over the inputs.
