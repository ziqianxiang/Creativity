Figure 1:	The number of steps needed for convergence for different values of K? , as given byTheorem 2. We use Cenv = 0.1, e = .001, I0 = 0.01, N = 3. This illustrates how larger values of K(which correspond to higher degrees of centralization) can dramatically improve convergence times,and how smaller values of K can dramatically lengthen them.
Figure 2:	Images of our benchmark environments, from Gupta et al. (2017).
Figure 3:	These illustrate the massive perfor-mance improvements of parameter sharing boot-strapping modern DRL methods for the firsttime, and the best documented average totalrewards and convergence rates, on the Guptaet al. (2017) MARL benchmark environments.
Figure 4:	These show the performance of thesame DRL methods as Figure 3, but with fullyindependent learning instead of parameter shar-ing. The maximum average total reward, as wellas convergence times, are worse, as predictedby our theoretical work in section 3.
Figure 5: QMIX reached a maximum average total reward of -16.935 after 17.8k episodes, MADDPGreached a maximum average total reward of -101.85 after 44.2k episodes. PPO (parameter sharing)reached a maximum reward of 41 after average total 9.2k episodes15Under review as a conference paper at ICLR 2021RL method	Hyperparameter	Value for Pursuit / Waterworld / MultiwalkerPPO	sample_batch_size	100	train_batch_size	5000	sgd_minibatch_size	500	lambda	0.95	kl_coeff	0.5	entropy_coeff	0.01	num_sgd_iter	10	vf_clip_param	10.0	clip_param	0.1	vf_share_layers	True	clip_rewards	True	batch_mode	truncate_episodesIMPALA	sample_batch_size	20	train_batch_size	512
