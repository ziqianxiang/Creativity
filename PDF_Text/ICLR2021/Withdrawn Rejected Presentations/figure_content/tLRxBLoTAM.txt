Figure 1: An overview of our method for creating an ensemble of diverse prediction paths.
Figure 3: How does our method work? Each network receives different information for making a pre-diction, due to going through different middle domains. Given a defocused image (left), the 2D edges extractedfrom it was the least affected by the distortion, and returned a prediction with the lowest overall uncertainty,which is reflected in the weights. Similarly, for an image with glass blur (right), the method successfully dis-regards the degraded output from the emboss path for the final prediction. The quality of the final predictiondepends on the following elements: 1. At least one middle domain is robust against the encountered distortion- the proposed inverse variance merging obtains significantly better results than learning from the RGB directly(leftmost column of each example). 2. The uncertainty estimates are well correlated with error, allowing us toselect regions from the best performing path - a uniform average (first row) of path predictions does not takeinto account the uncertainties and results in worse predictions.
Figure 4: Qualitative results under synthetic and natural distribution shifts for normal, reshading,and depth. The first four rows show the predictions from a query image from the Taskonomy test set underno distortion and increasing speckle noise. Our method degrades less than the other baselines, demonstratingthe effectiveness of using different cues to obtain a robust prediction. The last three rows shows the resultsfrom external queries (Zamir et al. (2020)). Again, our method demonstrates better generalization to imagesconsiderably different from the training dataset. Notable improvements in the accuracy can be seen especiallyin fine-grained regions.
Figure 5: Benefits of ensembling with multiple middle domains shown on a sample image from theReplica dataset for 10 distortions, shift intensity 3. Our method is resistant to distortions compared to thebaselines and provides better accuracy especially in the fine-grained regions. Best seen on screen.
Figure 6: Quantitative results: Average `1 loss over 11 unseen distortions. This does not includethe 2 distortions used during training and shows the performance for unseen distortions. Error bars correspondto the bootstrapped standard error. The losses are computed over two buildings of the Taskonomy test set.
Figure 7: Visuals of common corruptions used for a single image sample.
Figure 8: Loss for each distortion for our method against several baselines.
Figure 9: StatistiCally informed guesses (“Blind Guess”) on the Taskonomy dataset for depth, nor-mal, and reshading tasks. Top row shows the predicted mean, μ, while the bottom row correspondsto the prediCted standard deviation, σ, for these tasks. The blind guess prediCtions minimize theexpected NLL loss on the training dataset.
