Figure 1: (a) As explained by double descent, increasing width in ResNets trained on CIFAR10results in a decrease in test error. (b) In contrast, increasing the depth of ResNets trained on CIFAR10results in an increase in test loss (results are averaged across 3 random seeds).
Figure 2: Train and test accuracy of the Fully-Conv Net as a function of depth, for CIFAR10 inputimages. Increasing depth beyond a critical value leads to a decrease in test accuracy. As depthincreases, the performance of the Fully-Conv Net approaches that of a wide fully connected network(shown in red). All experiments are performed across 5 random seeds.
Figure 3: Train and test accuracy of the Fully-Conv Net as a function of depth, for ImageNet32input images. Increasing depth beyond a critical threshold again leads to a decrease in test accuracy.
Figure 4: Increasing depth well past the interpolation threshold leads to a decrease in test accuracy.
Figure 5: Test loss for ResNet models of width 32 (a) and width 16 (b) increases with depth.
Figure 6: Test loss and accuracy for the NTK and the CNTK of increasing depths (on a log scale),trained on a subset of planes and trucks from CIFAR10. Beyond a critical depth, the test error forthe CNTK increases and the accuracy decreases.
Figure 7: A toy example demonstrating that increasing depth in linear convolutional networks leadsto operators of decreasing `2 norm, which manifests as a decrease in test accuracy. (a) A visualiza-tion of samples from our toy dataset. (b) The training and test performance of linear convolutionalnetworks of varying depth across 5 random seeds. The test accuracy of the minimum `2 norm solu-tion for this problem is shown as a dashed black line. (c) The `2 norm of the operator with varyingdepth. The norm of the minimum `2 norm solution for this problem is shown as a dashed black line.
Figure 8: Training layer-constrained linear autoencoders of increasing depth leads to a decreasein the Frobenius norm and stable rank of the resulting operator. (a,d) We train convolutional net-works of varying depth on 1 example. (b,e) We train convolutional networks of varying depth on 5examples. (c,f) We train a network with Toeplitz layers to autoencode a 5 dimensional vector.
Figure 9: A diagram of the FUlly-Conv Net used with width w, depth d, and C classes. Each convo-lutional layer (except for the last one) is followed by batch norm and LeakyReLU.
Figure 10: A diagram of the custom ResNet models used. The ith block is repeated n times instage i. The first convolutional layer in stages 2, 3, and 4 downsamples the input using a stride of 2,while the remaining layers have a stride of 1.
Figure 11: Experimental details for all experiments conducted.
Figure 12: Train and Test losses of the ResNet models for all widths.
Figure 13: Train and Test accuracies for the ResNet models of width 8, 16, 32, and 64, for increasingdepths.
Figure 14: Train and test losses for the width 8 ResNet model. We see that test loss decreases asmodel depth increases, but train loss has still not reached 0, even for large depths.
Figure 15: Test losses for the width 32 ResNet model where the 1st stage blocks are increased versusthe model where the 3rd stage blocks are increased.
Figure 16: Test losses for the width 32 ResNet samples, trained on 500 samples per class.
Figure 17: Train and test losses for width 32 models for increasing kernel size. We observe that testloss increases as kernel size increases.
Figure 18: Test loss and test accuracy for the CNTK trained on subsets of Cifar10 with 2, 5 and 10classes. We observe that generalization improves up to a critical depth, after which it worsens.
Figure 19: An additional toy example demonstrating that increasing depth in linear convolutionalnetworks leads to operators of decreasing `2 norm, which manifests as a decrease in test accuracy.
Figure 20: Test losses for models in Figure 2 (on CIFAR10). The error for the convolutional net-works is in blue and that of full connected networks is in red.
Figure 21: Test losses for models in Figure 3 (on ImageNet32).
Figure 22: Test lossesfor the models in Figure 4.
