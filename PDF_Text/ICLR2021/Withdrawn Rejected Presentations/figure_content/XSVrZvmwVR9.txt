Figure 1: (a) MAML focus on the meta-weights, but ignores the architecture impact. (b) The currentnas-based meta-learning methods consists of two stages. The task-specific architectures and theirmeta-weights are obtained separately. It overlooks meta-weights during searching architectures formeta-learning. In addition, it is computationally intensive to retrain each architecture. (c) By co-optimizing the architecture and the network weights, CAML can obtain the adaptive architectureand the meta-weights simultaneously for all unseen tasks, requiring 130x less computational cost.
Figure 2: L is the loss function. θm, θn and φm are updated by the inner-learners, while θ0, θ1and φ1 are optimized by the meta-learners. (a) MAML (Finn et al., 2017) optimizes meta-weightsusing the same update direction as the inner-learner,s. (b) Our CAML optimizes both the connec-tion parameters φ and network weights θ using the same update direction as the inner-learners’,respectively.
Figure 3: In previous works (e.g., T-NAS andMeta-NAS), connection parameters and net-work weights are treated equally in optimiza-tion. Due to the unequal learning rates, the up-date direction of the meta-learner is not parallelto the inner-learner,s, which is against MAML.
Figure 4: The network weights distribution of the first convolution layer during the evaluation.
Figure 5: 5-shot, 5-way meta-test accuracy onMini-Imagenet during the evaluation.
Figure 6: Architecture searched in 5-way 5-shot setting of Mini-imagenet.
Figure 7: Architecture searched in 5-way 5-shot setting of FC100.
Figure 8: Heatmap while we do pruning. (a). We use standard CAML with progressive connectionconsolidation (PCC). (b). We treat connection parameters and network weights equally. (c). Weonly prune the supernet at the end of searching.
Figure 9: Dataset splitsD	DATASET SPLITSIn few-shot learning, the dataset is composed by train, validation and test classes. Under N-way K -shot setting, We sample N classes, of which each contains K examples as one task. Tasks sampledfrom train classes is denoted Dmeta-train. So as Dmeta-VaI and Dmeta-test. Each of them is divided intotwo subset: support set TS and query set Tq. The former is used for updating the inner-learners,while the later is for the meta-learners. In CAML, during one update step of each component of thearchitecture and weights, both the support set and query set data are needed. In NAS methods (LiUet al., 2019b), the architecture is considered a hyper-parameter selected to maximize the performanceon the validation set, based on the network weights trained with the data of the training set in theinner loop. So we split the Dmeta-train into Dmeta-train-split-arch and Dmeta-train-split-weights as the validationset and training set, respectively. As visualized in Figure 9, both splits are composed of the supportset and the query set. One iteration of CAML consists of two backpropagations, as shown in Figure2. During the backpropagation one, the architecture is optimized with the data of Dmeta-train-split-arch(including the support set and the query set), while the network weights are fixed and trained withthe data of Dmeta-train-split-weights in the previous iteration. During backpropagation two, the weightsare trained on the fixed architecture.
