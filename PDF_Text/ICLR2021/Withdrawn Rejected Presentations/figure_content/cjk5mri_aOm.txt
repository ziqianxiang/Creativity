Figure 1: Environment Predictive Coding: During self-supervised learning, our model is given video walk-throughs of various 3D environments. We mask portions out of the trajectory (dotted lines) and learn to inferthem from the unmasked parts (in red). We specifically mask out all overlapping views in a local neighborhoodto limit the content shared with the unmasked views. The resulting EPC encoder builds environment-level rep-resentations of the seen content that are predictive of the unseen content (marked with a “？”)，conditioned on thecamera poses. The agent then uses this learned encoder in multiple navigational tasks in novel environments.
Figure 2: We propose the zone prediction task for self-supervised learning of environment embeddings fromvideo walkthroughs generated by other agents. Each frame consists of the egocentric view and camera pose(top left). We group the frames in video V into seen zones in cyan {ZV,0, ∙ ∙ ∙ , Zsn} and unseen zones inyellow {ZU,0,…，ZU,m} (top row). The zones are generated automatically based on viewpoint overlap in 3Dspace (bottom left). Given a camera pose pU,i sampled from the unseen zone ZU,i, We use a transformer-basedencoder-decoder architecture that generates environment embeddings E from the seen zones, and predicts thefeature encoding fu,i of ZU,i conditioned on the pose pU,i (bottom center). The model is trained to distinguishthe positive fs,i from negatives in the same video {fs,j∙}j=i as well from other videos {fW }t=s (bottom right).
Figure 3: Integrating environment-level pre-training for navigation: Left: The first level of transfer occursfor the environment-level representations. We transfer the proposed EPC environment encoder and projectionfunction M that are pre-trained for zone prediction. Right: The second level of transfer occurs for the image-level representations. We transfer a pre-trained MidLevel image encoder (SaX et al., 2020) to generate visualfeatures for each input in the scene memory. Center: To train the SMT on a task, We keep the visual featuresfrozen, and finetune the environment encoder and projection function M with the rest of the SMT model.
Figure 4: Each row shows one zone prediction example. Left: Top-down view of the 3D environment fromwhich the video was sampled. The cyan viewing frusta correspond to the average pose for three input zones.
Figure 5: Sample efficiency on MatterPort3D Val split.OUr environment-level pre-training leads to 2-4 ×training sample efficiency when compared to SoTA image-level pre-training. See Appendix for Gibson plots.
Figure 6: We highlight the downstream task performance as a function of episode time on both Matterport3Dand Gibson.
Figure 7: Sample efficiency on Gibson Val split. Our environment-level pre-training leads to 2-4 × trainingsample efficiency when compared to SoTA image-level pre-training.
