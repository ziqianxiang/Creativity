Figure 1: Overview of the learner-evaluator framework, overcoming the static training and testingparadigms by explicitly modelling continual optimization and evaluation from data streams in thelearner and evaluator agents. The framework generalizes to both continual learning and concept driftwith resources transparently defined as the horizon D and operational memory M.
Figure 2: Main setup. The learner updates network fθ and prototypes py , ∀y ∈ Y continually. ThePPP-loss encourages inter-class variance (red arrows) and reduces intra-class variance (green arrows).
Figure 3: Balanced Split-MNIST first seed Sevalt-SNE (Maaten & Hinton,2008).
Figure 4: Accuracies over buffer sizes |M| for balanced Split-MNIST and Split-CIFAR10 sequences.
Figure 5: Accuracy (%) for imbalanced SPlit-MNIST (left), SPlit-CIFAR10 (center) and Split-CIFAR100 (right) sequences. The legend reports average accuracies over all the sequence variations.
Figure 6: CoPE and CoPE-CE confusion matrices at the end of learning averaged over all variationsS(Ti) for the imbalanced Split-MNIST setup in (a) and (b), and Split-CIFAR10 in (c) and (d).
Figure 7: Weighing (%) between the positive loss term Lpos compared to the full PPP-loss L,averaged over 5 runs of balanced Split-MNIST with standard deviation in blue.
Figure 8: Split-MNIST first seed t-SNE representation of the test data Seval, including (a) andexcluding (b) the pseudo-prototypes P in the PPP-loss.
Figure 9: Accuracies over buffer sizes |M| for balanced Split-CIFAR100 sequence.
