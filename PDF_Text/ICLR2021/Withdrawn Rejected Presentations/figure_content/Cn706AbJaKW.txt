Figure 1: Breakdown of papers and review outcomes by topic, ICLR 2020(a) Acceptance rate	(b) Topic distribution1See Appendix A.1 for details on the keywords that we chose.
Figure 2: Reproducibility by number of review-ers, ICLR 2020.
Figure 3: Citation rank vs score rank for paperssubmitted to ICLR 2020the paper was first published online. After examining the data, citation rates are approximately linearin time although rare papers with extremely high citation count may exhibit exponential behavior.
Figure 4: Papers statistics by time online before submission deadline(a) Acceptance rate	(b) Average reviewer scoreι.o	ionot visible online	not visible online> 0 & <= 3 months	> 0 & <= 3 months4 Has the review proces s gotten “worse” over time ?Our dataset shows that reproducibility scores, correlations with impact, and reviewer agreementhave all gone down over the years. Downward temporal trends were already seen in reproducibilityscores. Spearman correlations between scores and citations decreased from 0.582 in 2017 to 0.471 in2020. We also measure the “inter-rater reliability,” which is a well-studied statistical measure of theagreement between reviewers (Hallgren, 2012), using the Krippendorf alpha statistic (Krippendorff,2011; De Swert, 2012). We observe that alpha decreases from 56% in 2017 to 39% in 2020. Moredetails can be found in Appendix A.3.
Figure 5: Histogram of average scores by gender, first and last author, ICLR 2020.
Figure 6: Acceptance rate and Distribution by topic for male and female first authors(a) Acceptance rate(b) DistributionWe test for area chair bias with a regression predicting paper acceptance as a function of reviewscores and gender. We did not find evidence for AC bias (Figure 12, Appendix A.9).
Figure 7: Histogram for average scores of all papers. This plot demonstrates that the averagesCores in eaCh year follows a roughly Gaussian distribution. In some years there are partiCularsCores that appear to be Chosen relatively infrequently, Causing multi-modal behavior. This is whatinspired up to use a Monte-Carlo simulation in addition to the ANOVA results. Interestingly, ourMonte-Carlo simulations produCe similar results regardless of whether we sample the empiriCalsCore distribution, or the Gaussian approximation from ANOVA.
Figure 8: Bar plot for average standard deviation within each score range. This plot demon-strates that the standard deviations are roughly equal across different ranges in 2017, 2018 and 2019.
Figure 9: Reproducibility score over time. This plot compares the reproducibility score over timeusing the ANOVA model.
Figure 10: Reproducibility level by topic, ICLR 2020.
Figure 11: Reproducibility level by number of reviewers, ICLR 2020. The reproducibility leveldenotes the probability that a rejected paper is rejected a second time.
Figure 12: Reproducibility level by number of reviewers, ICLR 2020. The reproducibility leveldenotes the probability that a paper achieves the same outcome twice.
Figure 13: Institution rank vs average score for the top 100 institutions. For each institution andeach paper decision type, all reviewer scores were averaged to generate a data point. Shaded areasrepresent 95% confidence intervals.
Figure 14: Logistic Regression Assumptions Checking. We use a Hosmer-Lemeshow plot tocheck the assumption for our logistic regression model. The null hypothesis is that there is nodifference between observed deciles and those predicted by the logistic model. If the values are thesame, we have a model that fits perfectly. This graph compares the observed number of acceptedpapers and expected number of accepted papers across multiple groups.
Figure 15: Histogram for last author reputation index. This plot demonstrates that the transfor-mation log(citations/publication + 1) achieves a roughly Gaussian distribution witha mean of 2.95 and a standard deviation of 1.28. Last authors were taken from the 2020 conferenceonly.
Figure 16: Log-transformed citation rate rank vs score rank for papers submitted to ICLR2020.
