Figure 1: The overview of our proposed L2E framework. The entire training process is based onthe idea of adversarial learning (Alg. 1). The base policy training part maximizes the base policy’sadaptability by continually interacting with opponents of different strengths and styles (Section 2.1).
Figure 2: The trained base policy using our L2E framework can quickly adapt to different opponentsof different styles and strengths in the Leduc poker environment.
Figure 3: Visualization of the styles of the strategies generated with or without the MMD regular-ization term in the Leduc poker environment.
Figure 4: Each curve shows the aver-age normalized returns of the base policytrained with different variants of L2E in thegrid soccer environment.
Figure 5: The trained base policy using our L2E framework can quickly adapt to different opponentsof different styles and strengths in the BigLeduc poker environment.
Figure 6: The trained base policy using our L2E framework can quickly adapt to opponents withdifferent styles in a Grid Soccer environment.
Figure 7: A. Policy Gradient : Optimize iteratively for the initial opponent (the orange dot), eventu-ally converging to the best response of the initial opponent’s strategy. B. Self Play : Each iterationseeks the best response to the previous round of strategy, which does not converge in an intransitivegame like RPS. C. L2E’s adaptation process when facing a new opponent (the orange dot). D. Nashpolicy’s adaptation process when facing a new opponent (the orange dot).
Figure 8:	The performance of L2E and Nash strategy in RPS game when facing new opponents.
Figure 9:	The convergence properties of L2E.
