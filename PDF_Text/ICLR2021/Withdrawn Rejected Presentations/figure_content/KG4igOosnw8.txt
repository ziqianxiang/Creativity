Figure 1: 2-D classification examples, the x and y axis are the coordinates (also features) of samples.
Figure 2: Illustration of how Pr(2β > α - δ) in Theorem 1 behaves in various cases by drawingnegative pairs from different subsets of a 3-class feature space which are defined in Fig. 2a. Theclassifier is a linear model. y-axis in the right side of (b) & (c) is for the case of x ∈ S1 ∪ S2. We seethat α - δ behaves in a similar way with β but in a smaller range which makes β the key in studyingPr(2β > α - δ). In the case of x ∈ S3 the distribution of β has more mass on larger values thanother cases because the predicted probabilities are mostly on the two classes in a pair, and it causesall hgn, gmi having the opposite sign of hxn, xmi as shown in Tab. 1.
Figure 3: Similarities of gradients and representations of two classes in the MNIST dataset. The xand y axis are the cosine similarity of gradients and representations, respectively. Blue dots indicatethe similarity of negative pairs, while orange dots indicate that of positive pairs.
Figure 4: Effects of LDRL on reducing diveristy of gradients and ρ-spectrum. (a) and (b) displaydistributions of similarities of representations and gradients. shDR and sh denote similarities ofrepresentations with and without LDRL, respectively, sgDR and sg denote similarities of gradientswith and without LDRL, respectively. (c) demonstrates increasing α in LDRL can reduce ρ effectively.
Figure 5: Average accuracy of DRL+BER with different memory sizes. The x axis is the index oftasks, the shaded area is plotted by standard deviation of 5 runs.
