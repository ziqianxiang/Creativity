Figure 1: Visualization of entropy regularization methods and biases around terminal states. Thered vertices and edges represent absorbing states and maximum entropy action selection. Eachregularization is defined as Ht = H(∏(∙∣st)) and KLt = -DκL∣∏(∙∣st)∣∣Punif(.)], respectively.
Figure 3: Ant robot trajectories.
Figure 2: IRL trajectories of 2D Multi-goal environments.
Figure 4: Training curves of stochastic policies on imitation learning benchmarks.
Figure 5: Illustrations of transfer learning and total distance traveled by transfer learning agents.
Figure 6: Illustrations of multi-goal environments. From the initial agent position, four goals aresymmetrically located at the four cardinal directions. The ground-truth energy function of the 2Denvironment and the expert trajectory samples for each environment are displayed.
Figure 7: Multi-goal Ant trajectories with different random seedsE.3 Ablation Study on HyperparametersWe provide experiments on ablation study on three controllable hyperparameters in Fig. 8.
Figure 8: Experiments on hyperparameters of CAIRL. (a) If the shaping regularization λ is ex-cessively high, learning of the potential-based shaping is disabled, and the reward function showsunstable behavior. (b) The gradient penalty is applied for ensuring convergence of discriminators.
Figure 9: Visualization of trained agent on CrippledAnt environment. The model maximizes themovement speed toward the right side, without showing biased locomotion such as moving to theforward or to the backward. Also, the orientation of the torso is preserved throughout the episode,implying that the algorithm is capable of imitating experts even under variation in the dynamics.
Figure 10: Illustration of the CAIRL reward function in the 2D multi-goal environment. Left:recovered reward function which is averaged over action. The goals are located in the four cardinaldirection (6,0), (-6,0), (0,-6), (0,6). Right: visualization of each vector field of reward function andthe corresponding local contour map.
Figure 11: Illustration of the AIRL reward function.
