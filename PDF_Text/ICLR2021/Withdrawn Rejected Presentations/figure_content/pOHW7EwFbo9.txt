Figure 1: Top row: For harder-to-satisfy (i.e., lower magnitude) constraint thresholds, constrainedMO-MPO finds policies with higher task performance than those found by the Lagrangian baseline,in all tasks except point push. Middle row: Across all constraint thresholds, constrained MO-MPOperforms at least as well as the baseline. The plots in the top and middle row are Pareto plots: forboth axes, higher values are better. Each dot corresponds to a separate policy trained for a particularconstraint threshold. Bottom row: Constrained MO-MPO and MPO-Lagrangian perform comparablyin learning policies that satisfy the constraints. Policies that lie in the red region (below the dottedline) violate the constraint.
Figure 2: Plots are for humanoid run. (a) The difference in average reward obtained by constrainedMO-MPO versus MPO-Lagrangian (positive values mean constrained MO-MPO performs better),across sub-ranges of constraint thresholds and with standard error bars. Our approach significantlyoutperforms the baseline for more difficult (lower magnitude) constraint thresholds. (b) ConstrainedMO-MPO (bottom) is able to optimize for both objectives at once, whereas the baseline (top)alternates between the two, leading to large drops in action norm cost. The horizontal dashed linesindicate the cost threshold.
Figure 3: (a) Pareto fronts and (b) constraint violations for humanoid run from the middle of training.
Figure 4: (a) Pareto plots for constrained MO-MPO on humanoid run for three inequality constraints,with three random initializations for each. The vertical dashed lines indicate the cost threshold. (b)Policies trained by constrained MO-MPO with per-initial-state constraint satisfaction obtain highertask reward, while satisfying the constraints.
Figure 5: A comparison of the Pareto fronts for humanoid run found by MO-MPO, controllableMPO, and our approaches. For MO-MPO and constrained MO-MPO, each dot corresponds toa separately-trained policy. For controllable MPO and controllable MO-MPO, five policies areevaluated by conditioning on a range of linearly-spaced preferences (which correspond to weights orKL-divergence bounds, respectively). The gray triangles denote the MO-MPO Pareto front.
Figure 6: Top row: The task reward (averaged over episodes) achieved by policies trained with ourMPO-Lagrangian baseline, over the course of training. The horizontal solid and dashed grey linesdenote the final task reward obtained by the PPO-Lagrangian and TRPO-Lagrangian approaches,respectively, in Ray et al. (2019). Bottom row: The MPO-Lagrangian baseline meets the constraintof incurring less than 25 expected cumulative cost per episode, indicated by the dotted red line.
Figure 7: Constrained MO-MPO (in orange) achieves significantly higher task reward than thebaseline (in gray) for the harder-to-satisfy constraint thresholds in humanoid run (-1.4 and -1.2),humanoid walk (-1.2, -1.0, and -0.8), point goal (-4 and -2), and point button (-4 and -2). Forthe other constraint thresholds, both approaches achieve similar task reward. In terms of satisfyingconstraint thresholds, our approach performs on-par with or better than the baseline. Each bar showseither the average task reward (solid bars) or cost (clear bars) per episode, averaged over five seeds.
Figure 8: In the plots above, the Q-value estimate is obtained by averaging the Q-values for 100batches of (s, a) pairs from the replay buffer, after 200M actor steps of training. Each pointcorresponds to a separately-trained policy, for a different constraint thresholdapproaches achieve similar task reward. In terms of satisfying constraint thresholds, our approachperforms on-par with or better than the baseline.
