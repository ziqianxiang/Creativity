Figure 1: The first two layers of the multimodal convolutional long-short term memory network, andthe equations used to compute the gate and update values. ft is the forget gate, it is the input gate,gt is the cell gate, and ot is the output gate. W’s are the corresponding weight matrices, and b’s thecorresponding bias values. σ() is the sigmoid function. tanh() is the hyperbolic tangent function.
Figure 2: A single input example where white noise has been applied at various SNR values. AU-dio spectrograms have been truncated to the first 42 frequency bins for convenience, with the fullspectrograms available in Figure 8tractability. This allows us to combine them to create a multimodal task which we know will besolvable. This allows us to artificially manipulate the difficulty of the task via the addition of noise.
Figure 3:	Comparing the performance of initial layer fusion, second layer fusion, and FC fusionmodels. The first row shows raw test accuracy at various signal to noise ratios. The second rowshows the difference in accuracy between the late fusion models and the immediate fusion model atcorresponding signal to noise levels. All models were trained with an audio SNR of 0.5 and a visualSNR of 0.25. Orig signifies the original audio or visual input.
Figure 4:	Accuracy of multimodal models trained on data with a range of signal to noise ratios, eachfor a range of signal to noise ratios of the testing data.
Figure 5:	Performance of the C-LSTM model with only visual input trained on the original data aswell as at various visual signal to noise ratios.
Figure 6:	Performance of the C-LSTM model with only audio input trained on the original data aswell as at various audio signal to noise ratios.
Figure 7: The values of the final layer of the multimodal layer across timesteps of the C-LSTM fora representative example at various signal to noise ratios.
Figure 8: The same input example where white noise has been applied at various SNR values, withthe full audio spectrogram.
Figure 9: Late fusion model where audio SNR = 1, and visual SNR = 0.5. Fusion in the initial layeroutperforms the fusion in the fully connected layer.
Figure 10: Late fusion model where audio SNR = 2, and visual SNR = 0.25. Fusion in the initiallayer outperforms the fusion in the fully connected layer for audio SNR values greater than 1.
Figure 11: Late fusion model where audio SNR = 1.0, and visual SNR = 0.25. Fusion in the initiallayer outperforms the fusion in the fully connected layer for audio SNR values greater than 1.
Figure 12: Late fusion model where audio SNR = 1, and visual SNR = 0.125. Fusion in the initiallayer outperforms the fusion in the fully connected layer for audio SNR values greater than 1.
Figure 13: Additional example of values of the final layer of the multimodal layer across timestepsof the C-LSTM for a representative example at various signal to noise ratios.
Figure 14: Additional example of values of the final layer of the multimodal layer across timestepsof the C-LSTM for a representative example at various signal to noise ratios.
Figure 15: Additional example of values of the final layer of the multimodal layer across timestepsof the C-LSTM for a representative example at various signal to noise ratios.
Figure 16: Additional example of values of the final layer of the multimodal layer across timestepsof the C-LSTM for a representative example at various signal to noise ratios.
Figure 17: Additional example of values of the final layer of the multimodal layer across timestepsof the C-LSTM for a representative example at various signal to noise ratios.
