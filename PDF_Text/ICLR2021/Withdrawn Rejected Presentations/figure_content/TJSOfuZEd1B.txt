Figure 1: A toy example of how GeDi-guided generation uses Bayes rule to efficiently compute classificationprobabilities for possible next tokens at each generation timestep using only element-wise operations. Theseclassification probabilities can then be used to guide generation from a language model (e.g., GPT-2) to achieveattribute control across domains. If the GeDi was trained on movie reviews for sentiment control, its directclass-conditional predictions will be biased towards predicting movie review words (illustrated by next wordprediction of “cinematic”). However, by contrasting the predictions of opposing control codes via Bayes rule,the bias towards movie reviews can be canceled out.
Figure 2:	Instructions provided to the annotators on Mechanical Turk for labeling samples from the sentimentcontrol task.
Figure 3:	Example prompt shown to annotators on Mechanical Turk for samples from the sentiment controltask. The drop-down for the last 2 questions (on amazon review and movie review) consists of ‘Yes’ and ‘No’as options. Instructions from Figure 2 are provided above each such task.
Figure 4:	Example prompt shown to annotators on Mechanical Turk for samples from the detoxification task.
