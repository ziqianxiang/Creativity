Figure 1: (a) Q value landscape of a 1-dim continuous control task. Policy gradient methods optimizethe policy according to the local information of Q (b) For the same task, ZOSPI directly updates thepredicted actions to the sampled action with the largest Q value. (c) Simulation results, where in eachoptimization iteration 10 actions are uniformly sampled with different ranges, and reported resultsare averaged over 100 random seeds. It can be seen that a larger random sample range improves thechance of finding global optima. Similar phenomenon also exist in practice as shown in Appendix A.
Figure 2: Experimental results on the MuJoCo locomotion tasks. The shaded region represents half astandard deviation of the average evaluation over 10 trials.
Figure 3: Visualization of learned policies on the FSM environment. (a) the FSM environment andits optimal solution, where the policy should find the nearest reward region and move toward it; (b)learning curves of different approaches; (c)-(i) visualize the learned policies and corresponding valuefunctions.
Figure 4: Landscape of learned value function in the Pendulum-v0 environmentA.1 Discussion on the B enefits of Global SamplingIn this section, we discuss a type of structure, with which our zeroth-order optimization has aexponential convergence rate. To better explain our points, we include an Algorithm 3 in AppendixB, a modified version of Algorithm 1, which prevents the sampled action jumping too far acrossdifferent global regions.
