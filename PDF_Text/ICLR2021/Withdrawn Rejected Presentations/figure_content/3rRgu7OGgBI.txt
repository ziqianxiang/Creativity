Figure 1: The architecture of the proposed Bi-tuning approach, which includes an encoder forpre-trained representations, a classifer head and a projector head. Bi-tuning enables a dual fine-tuningmechanism: a contrastive cross-entropy loss (CCE) on the classifier head to exploit label informationand a categorical contrastive learning loss (CCL) on the projector head to model the intrinsic structure.
Figure 2: Sensitivity analysis of hyper-parameters K and L for Bi-tuning.
Figure 3: Interpretable visualization of learned representations via various training methods.
Figure 4: T-SNE (Maaten & Hinton, 2008) visualization of baselines on Pets (Parkhi et al., 2012).
