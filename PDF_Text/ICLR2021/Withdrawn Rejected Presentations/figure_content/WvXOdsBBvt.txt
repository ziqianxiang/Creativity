Figure 1: The main and auxiliary models.
Figure 2: Uncertainty estimation on Digits (left) and CIFAR-10-C (right). Our prediction of domain uncertainty is consis-tent with Bayesian uncertainty, while our method is an orderof magnitude faster since we forward data only once.
Figure 3: Visualization of feature perturbation |e| = ∣h+ - h|(Top) and embedding of domains (Bottom) at different trainingiterations T on MNIST. Left: Models w/o uncertainty; Right:Models w/ uncertainty. Most perturbations are located in the back-ground area and models w/ uncertainty can create large domaintransportation in a curriculum learning scheme.
Figure 4: Density of y+ onMNIST. Models w/ uncertaintycan encourage more smoothinglabels and significantly increasethe capacity of label space.
Figure 5: Architectures of main and auxiliary models. From left to right: (a) Digits (Volpi et al.,2018); (b) CIFAR-10-C (Hendrycks & Dietterich, 2019); (c) Google Commands (Warden, 2018), and(d) Amazon Reviews (Chen et al., 2012).
Figure 6: Classification accuracy on fifteen corruptions ofCIFAR-10-C using the backbone of WRN (40-2). FollowingFig. 7, the accuracy of each corruption with the highestlevel of severity is presented. Our method achieves 20%improvements on corruptions of noise.
Figure 7: Classification accuracy(%) on five levels of corruptionseverity. Our method has the small-est degradation under the highestlevel of corruption severity.
Figure 8: Ablation study on hyper-parameters K and β. The average accuracy on the four unseendomains (MNIST-M (Ganin & Lempitsky, 2015), SVHN (Netzer et al., 2011), SYN (Ganin & Lempit-sky, 2015), and USPS (Denker et al., 1989)) is presented. We set K = 15 and β = 1 according to thebest classification accuracy.
