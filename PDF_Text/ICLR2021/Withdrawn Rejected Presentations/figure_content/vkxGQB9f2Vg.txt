Figure 1: Training loss and log variance of the gradients for the different estimators for f (z) =Pjd=1(zj -)2ford∈ {1, 10, 100}.
Figure 2:	Training loss and log variance of the gradients for the different estimators for f (z) =Pjd=1 exp(-zj) for d ∈ {1, 10, 100}.
Figure 3:	Bayesian Logistic Regression with Laplacian priors6.2 Bayesian logistic regression with Laplacian PriorsWe evaluate the Laplace stochastic backpropagation estimator using a Bayesian logistic regressionmodel (Jaakkola & Jordan, 1997), similarly to (Mohamed et al., 2019). In our case, we substitute thenormal prior and posterior on the weights with Laplace priors and posteriors. We adopt the samenotations of (Murphy, 2012), where the data, target and weight variables are respectively: xn ∈ Rd ,yn ∈ {-1, 1}, and w. The probabilistic model in our case is the following:dp(w) =	L(wj, 0, 1)	p(y|x,w) = σ(yxTw),	(25)j=1where σ represents the sigmoid function. We consider Laplacian variational posteriors of the formpθ(W) = Qd=1 L(wj, μj, bj), with θ = {μ, b}. The evidence lower bound of a single sample isgiven by:L(Xn,yn；θ) = Ew〜Pθ [logσ(ynxTw)] - DKL[Pθ|[p],	(26)where the Kullback-Leibler divergence between the two Laplace distributions is the following:& (	- lμjl	]Dkl [Pθ ||p] =	M|+bje-bj TOg bj- 1 1j=1(27)We test the model on the UCI women’s breast cancer dataset (Dua & Graff, 2017), with a batch size
Figure 4: (Left) Bias and variance of the gradient for different values of the truncation level at a fixedparameter value. (Right-top) Mean square error between the Laplace gradient estimator and the scorefunction and reparameterization estimators across iterations. (Right-bottom) Norm of the gradientestimators.
Figure 5: A hidden variable probabilistic model, where the observed variables are the data x andtarget y, with L hidden stochastic layers h(1:L) .
Figure 6: The training evidence lower bound on the MNIST training set (top) and the log variance ofthe gradient (bottom) over 5 runs. Comparison with the REBAR and RELAX estimators.
Figure 7: Training evidence lower bound and the log vari-ance of the gradient for the categorical VAE on the MNISTdataset.
