Figure 1: An illustration example showing that different hyperparamter tuning methods are likely toaffect comparison of optimizers. Optimizer A is more sensitive to hyperparamters than optimizersB, but it may be prefered if bad hyperparameters can be terminated in the early stage.
Figure 2: Hyperband tuning used in our eval-uation protocol is closer to human behaviorthan random search.
Figure 3: End-to-end training curves(c)	(d)with Hyperband on CIFAR10 and CIFAR100.
Figure 4: Performance profile of 7 optimizers in the range [1.0, 1.3].
Figure 5: Training curves under。口血血 for both partial and full datasets.
Figure 6: End-to-end training curves for GAN on CIFAR10.
Figure 7: Probabilistic performance profile of 7 optimizers in the range [1.0, 1.3].
Figure 8: End-to-end training curves on CIFAR10116Under review as a conference paper at ICLR 2021CIFAR10 (tune learning rate only)“CTeJnoov1	10	100	1000Epoch“CTeJnoovCIFAR10 (tune every hyperparameter)1	10	100	1000Epoch(a)CIFAR100 (tune learning rate only)7 6 543 2 IOα α Qaa α ααAOJnoov1	10	100	1000Epoch(b)
Figure 9: End-to-end training curves on CIFAR10, CIFAR100, and CelebA.
Figure 10: End-to-end training curves on MRPC, PPI, and Walker2d-v3.
Figure 11: Data addition trainong on MRPC and PPI.
