Figure 1: Contrastive text-sequence-embedding-2-label-embedding matcher model: A text(‘measuring an interaction’), and positive (‘interaction’, R) or negative labels (‘p-value’) are en-coded by the same word embedding layer E ①,where labels have word IDs for lookup. The textembeddings are then encoded by a sequence encoder T ②，while C labels are encoded by a labelencoder L ③.Each text has multiple labels, so the text encoding t is repeated for, and concate-nated with, each label encoding l° l. The resulting batch of ‘text-embedding, label-embedding, pairs[[ti, C,/,..., [ti,，◦,/]④ is fed into a 'matcher, classifier ⑤ that trains a binary cross entropy loss⑥ on multiple (pseudo-)label (mis-)matches {0,1} for each text instance ti, resulting in a noise con-trastive estimation objective (NCE). Words like ‘measuring, provide self-supervised pseudo-labels(left). Positive and negative (pseudo-)labels are sampled from their own or other instances in a mini-batch. Unlike Zhang et al. (2018a) We use a CNN for ②,negative sampling and self-supervision.
Figure 2: Few-shot learning: Best training from scratch (left) vs. best fine-tuned (right):APmicro.test curves for different few-shot portions: 100%, 75%, 50%, 25%, and 10% of train-ing samples. ‘Dataset-internal’ pretraining via self-supervision (right) markedly improves few-shotlearning performance, speed and stability compared to training from scratch (left).
Figure 3: Zero-shot performance by model and signal size: Left plot: When using the samelabel and parameter amount as for the ‘joint self+supervised train from scratch’ reference model (*),allowing more self-supervision labels (left middle curve) and widening the network (left top curve)noticeably boosts zero-shot performance (supervised APmicro_dev and test). Right: When using lesstraining data text (few-shot on inputs X), zero-shot still works, but we need to wait much longer.
Figure 4: Head and long-tail as 5 label frequency balanced class buckets: We bucket classes bylabel frequency, into 5 buckets, so that each bucket contains equally many label occurrences - i.e.
Figure 5: Long-tail effects of base, label-embeddings and self-supervised pretraining (XL):When reporting APmacro of the five 20% head to tail class bins, the BCE multi-label objectiveperforms worst. Label-embedding NCE (LE, (4-6)) markedly boosts performance, especially onthe long-tail. When using label embeddings for self-supervised pretraining with the same networkparameters (*) for all LE models, there is no boost. However, when pretraining with more parameters((5.XL) - see larger net, 3.3x labels in the zero-shot learning Fig. 3), we see substantiallong-tail performance boosts (turquoise, upper most curve).
Figure 6: The long-tail is learned during later epochs: APmacro performance over five frequencybalanced 20% head to tail class buckets. All methods (2-5.XL) learn the head classes (0 - 20%)first - during early epochs. Self-supervised pretraining (5, 5.XL), via pseudo label-embedding NCE(LE, PT), learn the long-tail (see 60 - 100%) even in the first epoch (- - dashed line) of supervisedlearning. The baseline (2) struggles to learn this long-tail at epoch 10 and until its epoch 82 - i.e. itsoptimal dev set score epoch.
Figure 7: Few-shot training from scratch (top 2) vs. after pretraining (bottom 2): and usingonly supervision to fit the end-task (left) vs. jointly using self+supervision (right). Results are inAPmicro_test for different few-shot training set portions (1, 75%, 50%, 25%, 10%). Insight 1:self-supervision during end-task fitting makes no learning difference - i.e. when comparing top (orbottom) left (supervised) VSright (self+supervised) sub-figures, they look nearly the same. Insight 2:Pretraining (bottom figs.) via self-supervision markedly improves few-shot learning performance,speed and stability, independent of fine-tuning via supervision (left) or self+supervised (right).
