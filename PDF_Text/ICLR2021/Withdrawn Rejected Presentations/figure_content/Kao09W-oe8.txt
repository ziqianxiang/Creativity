Figure 1: Visualization of parameter tensor of convolutional layers trained on ImageNet. Frequentlywithin in layers (especially deeper layers), there is correlation of the weights along the output channeldirection. The table shows output correlation (invariant to re-scaling) relative to input direction. Ourmethod induces parameter correlations in the output direction.
Figure 2: Visualization of kernels applied to the H0gradient under different metrics for λ = 1. Thisillustrates the smoothing effect of the metrics. In com-putation, linear cost formulas are applied to computethe gradients not using the convolution interpretation.
Figure 3: Pytorch code to compute the re-weighted H0 (Hλ0) gradient from the H0 gradient.
Figure 4: Evolution of training and test accuracy on CIFAR-10: an example with batchsize = 8.
Figure 5: Distribution of results on CIFAR-10. Left: Histogram of test accuracy. Ours achieveshigher average with significantly reduced variance. Right: Results from different methods. Bestaccuracy obtained from our proposed direction. Ours: SGD+H1; LS-ChanDir: LS applied in ourproposed channel direction; Ours+O: output channel smoothing; Ours+R: parameters rasterizedinto a 1-D vector to perform smoothing; Ours+I: Input channel smoothing.
Figure 6: Results on MNIST and Fashion-MNIST with different choice of smoothness. Ourmethods improve classification accuracy over SGD (i.e., λ = 0) for a wide range of smoothness.
Figure 7: Semantic Segmentation Results on PascalVOe SoboleV H1 and re-weighted H0 (H：)improve segmentation accuracy by 8.5% and 7.8% respectively relative to SGD.
Figure 8: Training and test accuracy on CIFAR-10 with ADAM.
Figure 9: Training and test accuracy on CIFAR-10 with SGD.
Figure 10: Channel-Directed Smoothing Leads to Better Performance. Best accuracy obtainedfrom our proposed direction. A: Output-Channel Directed; B: Input-Channel Directed; All: parame-ters rasterized into a 1-D vector to perform smoothing; Ours: re-weighted L2.
Figure 11: Correlation of Final Tensor. Correlation between weights within different channeldirections in CIFAR trained ResNet 56 conv layers (over 10 trials). |i - j | is distance between weightlocations in tensor for correlation computation. Sobolev/re-weighted H0 show strong correlation inoutput direction, but not input. SGD shows correlation in output direction.
Figure 12: Pytorch code to compute the Sobolev (H1) gradient from the H0 gradient. The‘permute’, ‘repeat’ and ‘unsqueeze’ operations are due to standard library limitations, and can beavoided by further code optimization (e.g., writing the function in C++/Cuda that Pytorch calls).
Figure 13: Histogram of correlation of a representative tensor. While the input channel correlationis distributed as zero-mean Gaussian, the output channel shows positive correlation and sparsity. Theneural network prefers regularities in the output channel direction.
Figure 14: Scatter plot of mean and standard deviation of output channels. There is no positivecorrelation between channel mean and standard deviation, showing that the structure in output channeldirection is not due to scaling. Each color corresponds to a layer.
