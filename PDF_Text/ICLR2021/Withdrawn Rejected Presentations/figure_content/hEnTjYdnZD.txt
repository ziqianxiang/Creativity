Figure 1: ReLU, LReLU/PReLU, RReLU, and SE. For PReLU, k is learned and for LReLU k isfixed. For RReLU, k is randomized during training in a given range. For SE, the slope is decided bya trainable dynamic function.
Figure 2: BCPReLU: the k1 and k2 control the slopes of the negative input and the positive input,respectively. The -k1 μ and k2α control the lower bound and upper bound of output, respectively.
Figure 3: The validation error of PReLU and BCPReLU on CIFAR10-ResNet20.
Figure 4: The validation error of different bit-width BCPReLU on CIFAR10-ResNet20.
