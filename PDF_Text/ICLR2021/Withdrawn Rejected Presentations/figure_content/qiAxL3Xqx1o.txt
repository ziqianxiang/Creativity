Figure 1: (a) Methods that attempt to learn a mapping between a random input Z = [z1, . . . , zn]>and a target configuration X = [x1, . . . , xn]> need to ensure that no two input points are mapped tothe same output separately for each draw of Z. Learning to perform collision avoidance is non-trivialfor permutation equivariant g. (b) We instead learn a distribution over functions ggg(∙, Z) operating ona fixed point set Φ. Our method can learn to assign a different role to each point φi during training,side-stepping the need for collision avoidance at every random draw of z .
Figure 2: Mean-squared error per training step for three permutation equivariant networks tasked withgenerating a single target X . Even with direct supervision, the networks do not learn when startingfrom randomly drawn points Z (random Set-in green). The task is easily solved when starting from afixed point set Φ and a random vector Z (random context-in orange).
Figure 3: Examples (non-handpicked) of generated graphs on the QM9 dataset. Novel graphs arein green, graphs found in the training set are in orange, and duplicates are grayed out. GG-GANexhibits higher novelty than all methods that capture the QM9 distribution (see also Figures 6 and 7).
Figure 4: Median generation times for individual graphs of increasing size (lower is better). Dashedlines indicate that the timings are based on reports of previous papers. GG-GAN outperformsautoregressive baselines and the non-autoregressive CondGen by one or more orders of magnitude.
Figure 5: Random sampled graphs from GG-GAN, and GG-GAN at specific batch indices.
Figure 6: Random sampled graphs from the models and training dataset.
Figure 7: Comparison between random context (RC) and random set (RS) models.
