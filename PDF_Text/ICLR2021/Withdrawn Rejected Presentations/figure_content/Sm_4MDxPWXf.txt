Figure 1: An example of our parsing mechanism and dependency-constrained self-attention mecha-nism. The parsing network first predicts the syntactic distance T and syntactic height ∆ to representthe latent structure of the input sentence I like cats. Then the parent and dependent relationsare computed in a differentiable manner from T and ∆.
Figure 2: The Architecture of StructFormer. The parser takes shared word embeddings as input,outputs syntactic distances T, syntactic heights ∆, and dependency distributions between tokens.
Figure 3: An example of T, ∆ and respective de-pendency graph D.
Figure 4: Dependency relation weights learnt on different datasets. Row i constains relation weightsfor all attention heads in the i-th transformer layer. p represents the parent relation. d represents thedependent relation. We observe a clearer preference for each attention head in the model trained onBLLIP-SM. This probably due to BLLIP-SM has signficantly more training data. It’s also interestingto notice that the first layer tend to focus on parent relations.
Figure 5: Dependency distribution examples from WSJ test set. Each row is the parent distributionfor the respective word. The sum of each distribution may not equal to 1.
