Figure 1: SSGD (red) does not converge in the large batch setting. Figure 1a plots the heldout-loss,the lower the better. Figure 1b and Figure 1c plot the model accuracy, the higher the better. Byinjecting Gaussian noise, SSGD might escape early traps but result in much worse model (blue)compared to DPSGD (green) in the large batch setting. The detailed task descriptions and trainingrecipes are described in Section 4.3. BS stands for Batch-Size.
Figure 2: Comparison of different multi-learner algorithms, DPSGD (green), SSGD (red), andSSGD* (blue). (a) For a large learning rate α =1, the loWered effective learning rate αe in DPSGDin the beginning of training alloWs DPSGD to converge While SSGD fails to converge. SSGD* alsoconverges but to an inferior solution. (b) For a smaller α = 0.2, DPSGD finds a flatter minimum Witha loWer test error than SSGD. SSGD* has the Worst performance. See text for detailed description.
Figure 3: CIFAR-10 2D contour plot. The more widely spaced contours represent a flatter losslandscape and a more generalizable solution. The distance between each contour line is 0.005 acrossall the plots. We plot against the model trained at the end of 320th epoch. VGG: VGG-19, ResN:ResNet-18, DenseN: DenseNet-121, -S: -SSGD, -DP: -DPSGD(a) VGG-S (b) VGG-DP (c) ResN-S (d) ResN-DP (e) Dense-S (f) Dense-DPFigure 4: CIFAR-10 Hessian heatmap on a 4x4 grid. The lower value (i.e. a cooler color) indicatesthe corresponding point is less likely in a saddle. We plotted against the models at the end of the 16thepoch. DPSGD is much more effective at avoiding early traps (e.g., saddle points) than SSGD. VGG:VGG-19, ResN: ResNet-18, DenseN: DenseNet-121, -S: -SSGD, -DP: -DPSGDlearning rate w.r.t the baseline batch size for the first 10 epochs before annealing learning rate by efor the remaining 10 epochs. For example, when using a batch size 2048, we linearly warmup thelearning rate to 0.8 by the end of the 10th epoch before annealing. Table 4 illustrates heldout lossfor SWB-300 and SWB-2000. In the SWB-300 task, SSGD diverges beyond batch size 2048 andDPSGD converges well until batch size 8192. In the SWB-2000 task, SSGD diverges beyond batchsize 4096 and DPSGD converges well until batch size 8192. Figure 7 in Appendix C details heldoutloss progression w.r.t epochs.
Figure 4: CIFAR-10 Hessian heatmap on a 4x4 grid. The lower value (i.e. a cooler color) indicatesthe corresponding point is less likely in a saddle. We plotted against the models at the end of the 16thepoch. DPSGD is much more effective at avoiding early traps (e.g., saddle points) than SSGD. VGG:VGG-19, ResN: ResNet-18, DenseN: DenseNet-121, -S: -SSGD, -DP: -DPSGDlearning rate w.r.t the baseline batch size for the first 10 epochs before annealing learning rate by efor the remaining 10 epochs. For example, when using a batch size 2048, we linearly warmup thelearning rate to 0.8 by the end of the 10th epoch before annealing. Table 4 illustrates heldout lossfor SWB-300 and SWB-2000. In the SWB-300 task, SSGD diverges beyond batch size 2048 andDPSGD converges well until batch size 8192. In the SWB-2000 task, SSGD diverges beyond batchsize 4096 and DPSGD converges well until batch size 8192. Figure 7 in Appendix C details heldoutloss progression w.r.t epochs.
