Figure 1: AWR and QWR on the BitFlip environment. The maximum possible return is 5.
Figure 2: Ablation of QWR with respect to the margin, the number of action samples and the methodof training the critic. The results are shown on the Half-Cheetah environment. The plots show themedian of 5 runs with the shaded area denoting the interquartile range.
Figure 3: Figures 3a, 3b and 3c show offline trainings based on 50 trajectories of length 1000collected by diverse policies. The horizontal lines mark the average return of a policy from thedataset. The bars denote median returns out of 4 runs, and the vertical lines denote the interquartilerange. Data for figures 3d, 3e and 3f is borrowed from Peng et al. (2019) to cover a broader family ofalgorithms and show that offline training fails for many RL algorithms.
