Figure 1: Sketch for a hypothetical loss surface(original task loss to be minimized) and an ex-tra regularization term in 2-D weight space:for (a) weight decay, and (b) WaveQ.
Figure 2: (a) 3-D visualization of the proposed generalized objective WaveQ. (b) WaveQ 2-Dprofile, w.r.t weights, adapting for arbitrary bitwidths, (c) example of adapting to ternaryquantization. (d) WaveQ 2-D profile w.r.t bitwidth. (e) Regularization strengths profiles, λw,and λβ, across training iterations.
Figure 4: Quantization bitwidth assignments across layers. (a) AlexNet (average bitwidth =3.85 bits). (b) ResNet-18 (average bitwidth = 3.57 bits)DSQ (Gong et al., 2019), and DoReFa, which are current state-of-the-art methods that show re-sults with homogeneous 3-, and 4-bit weight/activation quantization for various networks (AlexNet,ResNet-18, and MobileNet).
Figure 5: Evolution of weight distributionsover training epochs at different layers andbitwidths for different networks. (a) CIFAR10,(b) SVHN, (c) AlexNet, (d) ResNet18.
Figure 6: Weight trajectories. The 10 coloredlines in each plot denote the trajectory of 10different weights.
Figure 8: Convergence behavior: accuracy and WaveQ regularization loss over fine-tuning ePochsfor (a) CIFAR10, (b) SVHN. ComParing convergence behavior with and without WaveQ duringtraining from scratch (c) accuracy, (d) training loss. Network: VGG-11, 2-bit DoReFa quantization1 for i in range (0, iterations):2	r = 200 # rise edge	starting iteration3	f = 500 # fall edge	starting iteration4	s = 10 # determines	the smoothness of the transitions (rise/fall)5	fl = 0.5 * (l+tanh(Ci-r)∕s));6	f2 = 0.5 * (l+tanh(Ci-d)∕s));7	IambdCl_w_VCIIIle = fl8	lambda_beta_value =	0.05*(fl-f2)(b)(a)Figure 9: Math formula for setting λw and λβ during training iterations.
Figure 9: Math formula for setting λw and λβ during training iterations.
