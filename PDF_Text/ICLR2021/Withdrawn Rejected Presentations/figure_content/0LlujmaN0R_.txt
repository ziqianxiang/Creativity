Figure 1: Experimental environments. a: Predator Prey (PP-n) (Barrett et al., 2011). Each agentreceives rti = -0.05 at each time step. After reaching the prey, they receive rti = 1/m, where mis the number of predators that have reached the prey. b: Traffic Junction (TJ-n) (Sukhbaatar et al.,2016; Singh et al., 2019). n cars with limited sight inform the other cars of each position to avoid acollision. The cars continue to receive a reward of rti = -0.05 at each time as an incentive to runfaster. If cars collide, each car involved in the collision will receive rti = -1. c: Combat task inStarCraft: Blood Wars (SC) (Synnaeve et al., 2016).
Figure 2: Learning curves in three-agent predator prey (PP-3). a Truthfulness, the fraction of stepsthat the agent shares a true pre-state (zij = hi), b the real part of the reward, and c the imaginary part.
Figure 3: Left: One-bit two-way communication game Gc2om. S = A = {0, 1}, X = S ∪ {◦},and p(s) = 1/2. Agents are given correct information from the environment with a probability ofλ > 0: P (s|s) = λ, P (◦|s) = 1 - λ. qi(s|xi, zj) follows a Bernoulli distribution and estimates thestate p(s) of the environment from observations and signals zj ∈ Z = X. Let hi = xi. Right: Anexample of the non-cooperative solutions: S = 1, X = (◦, 1), Z = (◦, 0), a = (0,1), r = (-1,1) andr1 + r2 = 0 < 2c.
Figure 4: Left: The information released by one agent i is verified by another agent, and if the infor-mation is incorrect, a penalty is given. Right: State-action value function of G2om[iI]: Qi (Si,〈zi, •〉).
