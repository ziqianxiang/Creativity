Figure 1: Example images for the fivedatasets.
Figure 2: OOD detection performance of the compared methods. ’ Dog 'indicates their performancewhen Dog is ID and all the other four datasets are OOD, etc. Each bar shows the average AUROCof a method, and the error bar indicates its minimum and maximum values. Upper: The networksare trained from scratch. Lower: Pre-trained models are fine-tuned.
Figure 3: OOD score vs. the true classification error for 95(= 19 × 5) Do’s (corrupted Amazon im-ages used for training the regressor f; in green), 18 Dt’s (subsets of DSLR and Webcam containing50 samples each; in blue), and Ds (original Amazon images; in red).
Figure 4: OOD score vs. classification error for 95(= 19 × 5) datasets, i.e., Do’s and Dt’s (corruptedFood-A images).
Figure 5: OOD score vs. classification error for 95(= 19 × 5) datasets, i.e., Do’s and Dt’s (corruptedImageNet images).
Figure 6: OOD score vs. classification error for 95(= 19 × 5) Do’s (corrupted Amazon imagesused for training the regressor f ; in green), 32 Dt ’s (subsets of DSLR and Webcam containing 30samples each; in blue), and Ds (original Amazon images; in red).
Figure 7: OOD score vs. classification error for 95(= 19 × 5) Do’s (corrupted Amazon imagesused for training the regressor f; in green), 8 Dt’s (subsets of DSLR and Webcam containing 100samples each; in blue), and Ds (original Amazon images; in red).
Figure 8: OOD score vs. classification error for 95(= 19 × 5) Do’s (corrupted Amazon images usedfor training the regressor f; in green), 2 Dt’s (the entire set of DSLR and Webcam; in blue), and Ds(original Amazon images; in red).
Figure 9: Examples of the corrupted images obtained by applying different types of image corruption(Hendrycks & Dietterich, 2019).
