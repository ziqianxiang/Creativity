Figure 1: Overall Approach. Phase 1: At each time-step, determine the actions to be stepped vialook-ahead search over latent dynamics and write the resulting transition data in a replay buffer.
Figure 2: Decision-time planning comparison: For a current state st, a) Dreamer simply samplesan action using the current policy. b) Rollout samples n actions and unrolls the dynamics for eachaction transition for a depth d using mode action of current stochastic policy. c) MCTS iterativelyextends a search tree via trade off over exploration and exploitation. In both (b) and (c), rolloutsare used to update action-value estimates of the root edges (actions) and Value estimates of leafnodes are used for bootstrapping. The blue line indicates path of backpropagted return for singlerollout/simulation4.2	MCTSUnlike the Rollout algorithm which follows current policy for rollout, MCTS iteratively extends asearch tree starting from a root node representing the current environment state. In this section, wedescribe how we adapt MCTS for continuous action spaces.
Figure 3: a) Mean performance across 20 Benchmark tasks of dm-control suite. Each task was runfor 3 random seeds. b) Time-taken by each explore method. MCTS is significantly slower thanrollout due to the increased complexity of its search.
Figure 4: Comparison of mcts varitions - progres-sive widening and fixed children. We share meanscores across 8 environments trained on 3 randomseeds.
Figure 5: Comparison of Dreamer with different search modes during exploration. Each curverepresents the mean value over 3 runs with different seeds.
