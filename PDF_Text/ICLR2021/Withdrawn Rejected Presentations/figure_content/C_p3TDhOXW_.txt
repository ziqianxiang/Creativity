Figure 1: Experiment results on three classical control environments: MountainCar-v0, Acrobot-v1,and CartPole-v1. The curves in the figure were averaged over 50 runs, and the standard deviationof 50 runs is given as a shaded area. Each policy was averaged out of 5 trials. All rewards of theenvironment follow the default settings of Open AI Gym.
Figure 2: Inverse RL Experiment results on MountainCar-v0 (left) and CartPole-v1 (right). Thecurves in the figure were averaged over 50 runs, and the standard deviation of 50 runs is given asa shaded area. Note that black and green dashed line on the right are overlapped. All rewards ofthe environment follow the default settings of Open AI Gym. (BC : Behavioral Cloning, MaxEnt :Maximum Entropy)7	ConclusionIn this paper, we introduced the use of active inference from the perspective of RL. Although activeinference emerged from the Bayesian model of cognitive process, we show that the concepts ofactive inference, especially for EFE, are highly related to RL using the bootstrapping method. Theonly difference is that, the value function of RL is based on a reward, while active inference isbased on the prior preference. We also show that active inference can provide insights to solve theinverse RL problems. Using expert simulations, an agent can learn a local prior preference, which ismore effective than the global preference. Furthermore, our proposed active inference based rewardwith a prior preference and a generative model makes the previous invser RL problems free from anill-posed state. Our work on active inference is complementary to RL because it can be applied tomodel-based RL for the design of reward and model-free RL for learning of generative models.
