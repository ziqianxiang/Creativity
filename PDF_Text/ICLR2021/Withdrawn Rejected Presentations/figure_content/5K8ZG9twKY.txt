Figure 1: Distribution of sampled gradient norms while trainingDCGAN(a) andReal-NVP(b) onthe CIFAR-10 dataset. (c) Distribution of norms of Gaussian random vectors and (d) Distribution ofnorms ofα-stable random vectors withα= 1.95. X-axis: norm, Y-axis: Density.
Figure 2: Variation of gradient distributions across different iterations for different models anddatasets. (a) and (b) show the variation of generator gradient norms over iterations forDCGANand (c) shows the variation of the complete gradient norms over iterations forReal-NVP. X-axis:Gradient norm, Y-axis: Density3 Existence of heavy-tails in deep generative modelsIn this section, we elaborate on the preliminary study discussed in Section 1. The evidence ofheavy-tailed gradient distributions observed inDCGANandReal-NVPsuggests that the issue ofheavy-tailedness is more pervasive than thought to be. While recent analyses have only consideredsupervised learning models focusing on image classification (Simsekli et al., 2019) and on attentionmodels (Zhang et al., 2019), they miss out a large family of models that benefit from gradientestimates, namely probabilistic models (Mohamed et al., 2019). Examples of probabilistic modelsare generative adversarial networks (GANs) (Goodfellow et al., 2014) and invertibleflow models(Dinh et al., 2014), and these are the models we have considered in our study.
Figure 3: Results for Heavy-Tailed Linear Regression. Smaller values for Qδ (θn,δ) are better.
Figure 4: Variation of TestBPDwith itera-tionsMethod	Average TestBPD over last 2000 iterationsStrFPd	12:97Mean (no-clipping)	64166Mean (with-clipping)	459∙16Table 2: Table of Average Test BPD over the last2000 iterations8Under review as a conference paper at ICLR 20216	DiscussionIn this work, we study methods for mean estimation, which are applicable for aggregating gradients.
