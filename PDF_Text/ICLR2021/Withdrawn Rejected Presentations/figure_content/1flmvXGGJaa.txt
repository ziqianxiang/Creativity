Figure 1: Number of architectures usedfor training the GIN surrogate model vsMAE on the NAS-Bench-101 dataset.
Figure 2: t-SNE visualization of theand the optimizers, respectively. We would like to point out that in hindsight adding training data ofwell-performing regions may be less important for a surrogate NAS benchmark than for a surrogateHPO benchmark, which we demonstrated in Appendix E.3. We argue that this is a result of HPO2We do note that the average estimation error of tabular benchmarks could be reduced by a factor of Vzk byperforming k runs for each architecture. The error of a surrogate model would also shrink when the model isbased on more data, but as k grows large tabular benchmarks would become competitive with surrogate models.
Figure 3: Number of parameters against val.
Figure 4: Anytime performance of different optimizers on the real benchmark (left) and the surrogatebenchmark (GIN (middle) and XGB (right)) when training ensembles on data collected from alloptimizers. Trajectories on the surrogate benchmark are averaged over 5 optimizer runs.
Figure 5: Anytime performance of blackbox optimizers, comparing performance achieved on thereal benchmark and on surrogate benchmarks built with GIN and XGB in an LOOO fashion.
Figure 6: Anytime performance of one-shot optimizers, comparing performance achieved on thereal benchmark and on surrogate benchmarks built with GIN and XGB in a LOOO fashion.
Figure 7: Case study results for LocalSearch. GT is the ground truth, GIN andXGB are results on NAS-Bench-301.
Figure 8: Empirical Cumulative Density Func-tion (ECDF) plot comparing all optimizers in thedataset. Optimizers which cover good regions ofthe search space feature higher values in the lowvalidation error region.
Figure 12: Standard deviation of the val. ac-curacy for multiple architecture evaluations.
Figure 9: Visualization of the exploration of different parts of the architectural t-SNE embeddingspace for all optimizers used for data collection. The architecture ranking by validation accuracy(lower is better) is global over the entire data collection of all optimizers.
Figure 10: Distribution of the validation error fordifferent cell depth.
Figure 13: t-SNE projection colored by the depthof the normal cell.
Figure 11: Comparison between the normal andreduction cell depth for the architectures foundby each optimizer.
Figure 14: Distribution of validation error in de-pendence of the number of parameter-free opera-tions in the reduction cell. Violin plots are cut offat the respective observed minimum and maxi-mum value.
Figure 15: Architecture with inputs ingreen, intermediate nodes in blue andoutputs in red.
Figure 16: Scatter plots of the predicted performance against the true performance of differentsurrogate models on the test set in a Leave-One-Optimizer-Out setting.
Figure 17: (continued) Scatter plots of the predicted performance against the true performance ofdifferent surrogate models on the test set in a Leave-One-Optimizer-Out setting.
Figure 18: (Left) Distribution of validation error in dependence of the number of parameter-freeoperations in the normal cell on the NAS-Bench-301 dataset. (Middle and Right) Predictions of theGIN and XGB surrogate model. The collected groundtruth data is shown as scatter plot. Violin plotsare cut off at the respective observed minimum and maximum value.
Figure 19: Comparison between GIN,XGB and LGB in the cell topology anal-ysis.
Figure 20: Ground truth (GT) and surrogate trajectories on a constrained search space where thesurrogates are trained with all data, leaving out the trajectories under consideration (LOTO), andleaving out all DARTS architectures (LOOO).
Figure 22: Comparison between the observed true trajectory of BANANAS and RS with the surro-gate benchmarks only trained on well performing regions of the spaceE Benchmark AnalysisE.1	One-Shot TrajectoriesTo obtain groundtruth trajectories for DARTS, PC-DARTS and GDAS, we performed 5 runs for eachoptimizer with 50 search epochs and evaluated the architecture obtained by discretizing the one-shotmodel at each search epoch. For DARTS, in addition to the default search space, we collectedtrajectories on the constrained search spaces from Zela et al. (2020a) to cover a failure case whereDARTS diverges and finds architectures that only contain skip connections in the normal cell. Toshow that our benchmark is able to predict this divergent behavior, we show surrogate trajectorieswhen training on all data, when leaving out the trajectories under consideration from the trainingdata, and when leaving out all DARTS data in Figure 20.
Figure 21: Scatter plot of GIN predic-tions on architectures that achieved be-low 92% validation accuracy.
Figure 23: Scatter plots of the predicted performance against the true performance of the GNNGIN/XGB surrogate models trained with different ratios of training data. ”RS” indicates that thetraining set only includes architectures from random search, ”mixed” indicates the training set in-cludes architectures from all optimizers. Training set sizes are identical for the two cases. The testset contains architectures from all optimizers. For better display, we show 1000 randomly sampledarchitectures (blue) and 1000 architectures sampled from the top 1000 architectures (orange). Foreach case we also show the R2 and Kendall-τ coefficients on the whole test set.
Figure 24: Anytime performance of different optimizers on the real benchmark (left) and the surro-gate benchmark (GIN (middle) and XGB (right)) when training ensembles only on data collected byrandom search. Trajectories on the surrogate benchmark are averaged over 5 optimizer runs.
Figure 25: Anytime performance of different optimizers on the real benchmark (left) and the sur-rogate benchmark (GIN (middle) and XGB (right)) when training ensembles on 47.3% of the datacollected from all optimizers. Trajectories on the surrogate benchmark are averaged over 5 optimizerruns.
