Figure 1: This is an example of grammar encoded tree LSTM at work. The top layer of canvases demonstratesthe image stack and the bottom layer demonstrates the grammar stack. The blue, orange, yellow and greencolored LSTM cell generates grammatical tokens according to the CFG rule 1, 2, 3 and 4 respectively. Inimplementation, we can constrain the output space by adding a mask to the output of the LSTM and render theinvalid options with close to zero probability of being sampled.
Figure 2: The left most image demonstrates the entropy value increases over 700 iterations by sampling 20distinct samples with and without replacement as well as sampling 40 samples with replacement. The secondimage shows the initial distribution. The third and fourth images show the final distributions.
Figure 3: (a) We show a target image from each dataset and attach its correct program below. To the right ofthe target program are the reconstructed output programs from our algorithm and three variants each missingone design feature. The reward is on top of the reconstructed images. (b) Some reconstructed example outputprograms of our algorithm. Each row represents one data point. The leftmost images of the five columns are thetarget images and the four columns to their right are the reconstructed outputs of four samples. The final outputis the program of the reconstruction with the highest reward (highlighted in red).
Figure 4: From left to right, we have reward per batch for programs of length 5, 7 and 9. It demonstratesthe performance of our algorithm and controlled comparison in performance with alternative algorithms byremoving one component at a time.
Figure 6: Compare the entropy estimation following the Equation 14 as a weighted sum of stepwise entropyversus taking the average of the sequence log probability. The number of samples, or beam size, varies from 2 to80 on the x-axis. The shaded area represents the standard deviation of each estimator. From left to right, wedemonstrate the result on datasets of three program lengths.
Figure 7: Each shape encoding is on top of the image it represents.
Figure 8: Additional test output with corresponding programs. The odd-numbered columns contain the targetimages and the images to their right are example outputs.
