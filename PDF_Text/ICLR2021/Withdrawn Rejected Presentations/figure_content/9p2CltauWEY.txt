Figure 1: The effect of graph size and d-pattern distribution on generalization in G(n,p) graphs.
Figure 2: Constant training size and varying test size with and without normalization. Left: constantp which leads to different d-patterns. Right: p is normalized to keep d-patterns distribution similar.
Figure 3: Left: a graph withnode features represented bycolors. Right: A tree that rep-resents the d-patterns for theblack node. The tree descrip-tor is the number of nodesfrom each class in each layerof the tree.
Figure 4: Mean accuracy over alldatasets in Tab. 1 for d-pattern pretraining and no SSL (Vanilla).
Figure 5: Two training procedures for learning with SSL tasks. Left: Learning with pretraining:Here, a GNN is trained on the SSL task with a specific SSL head. After training, the weights of theGNN are fixed, and only the main head is trained on the main task. Right: Multitask learning: Here,there is a shared GNN and two separate heads, one for the SSL task and one for the main task. TheGNN and both heads are trained in simultaneously.
Figure 6: Teacher-student setup with a 3-layer GNN. Training is on graphs drawn i.i.d from G(n, p)with n ∈ {40, ..., 50} uniformly andp = 0.3. Testing is done on graphs with n = 100 andp vary (x-axis). The ”Pattern tree” plot represent training with our pattern tree SSL task, using the pretrainingsetup.
Figure 7: Histogram in percentage of degrees of graphs. We used the 10% smallest and largestgraphs in each dataset.
