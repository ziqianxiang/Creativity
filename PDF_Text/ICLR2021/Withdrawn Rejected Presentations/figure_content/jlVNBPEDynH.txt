Figure 1: Architecture of the neuro-algorithmic policy. Two subsequent frames are processed by twosimplified ResNetl8s: the cost-predictor outputs a tensor (width × height × time) of vertex costs c⅛and the goal-predictor outputs heatmaps for start and goal. The time-dependent shortest path solverfinds the shortest path to the goal. Hamming distance between the proposed and expert trajectory isused as loss for training.
Figure 2: Differentiation of a piecewise constant loss resulting from incorporating a combinatorialsolver. A two-dimensional section of the loss landscape is shown (left) along with two differentiableinterpolations of increasing strengths (middle and right).
Figure 3: The Crash Jewel Hunt environment. The goal for the fox, see (a), is to obtain the jewelin the right most column, while avoiding the moving wooden boxes (arrows in (b)). When the agentcollides with a wooden box it instantly fails to solve the task. A prediction of the costs is shown in(c).
Figure 5: Performance of NAP against PPO on the Chaser environment (a) trained on 10, 20, 50,and 100 levels. In (b) we show the short-horizon plans (white) of the agent (blue) at step 5 and 110 inthe environment.
Figure 4: Generalization performance depending on the number of seen levels during training. Dashedlines indicate performance on training-levels and solid lines show the generalization to unseen levels.
Figure 6: Test success rate of our method with different horizon lengths. The solver assumes that thelast horizon step costs remain to infinity. In this sense, the horizon of length 1 corresponds to a staticsolver.
