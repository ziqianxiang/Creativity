Figure 1: The layer 2 encoder-decoder attention for the output digits 16 in the first simplified productfor the output (2θ * x5 + 16 * x1 * *3) + (12 * xj As expected, the digits 1 and 6 attends to thecoefficients of the first and third monomial in the input expression (4 * x2) * (5 * x3 + 4 * x1) +(12 * xι). Config: Coarse, Small Coeff, Infix, 1 variable.
Figure 2: The layer 1 encoder-decoder attention for the coefficient 12 in the last product (20 * x； +16 * xi * *3) + (12 * xι). It is expected, that in this step, this product remains unchanged andsimply copied o the output. Therefore, We see that the layers learn to copy the coefficients directlyby attending to the corresponding digits (i.e. 1 attends to 1 in the last product). Config: COARSE,Small Coeff, Infix, 1 variable.
