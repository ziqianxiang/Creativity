Figure 1: The simulation environments used in the experiments. Note that the racing car in (c) onlyhas a monocular camera, while in (d) it is equipped by a stereo camera and a Lidar.
Figure 2:	Evaluation performance for MuJoCo-Reacher, averaged across five runs. In each of (a) and(b), We consider both teacher task is similar to (left) and different from (right) the target task.
Figure 3:	Evaluation performance for DeepRacer single-car time-trial race, including mean accumu-lated rewards and mean progress (lap completion percentage), averaged across five runs.
Figure 4:	Evaluation performance for DeepRacer multi-car racing against bot cars, averaged acrossthree runs. The value at each iteration is smoothed by the mean value of nearest 3 iterations.
Figure 5: Evaluation performance for MuJoCo-Reacher, averaged across five runs. Left: Teacher taskis similar to the target task. Right: Teacher task is different from the target task.
Figure 6: Comparison of relevance-based transfer (RBT) and REPAINT on MuJoCo-Reacher,averaged across five runs. Left: Teacher task is similar to the target task. Right: Teacher task isdifferent from the target task.
Figure 7: Trajectories of policy evaluations. In each of (a) and (b), evaluation of the models trainedfrom REPAINT is visualized on the left and that trained with warm-start is on the right.
Figure 8: Evaluation performance with respect to different Z's, averaged across five runs.
Figure 9: Evaluation performance with respect to different initial β0 ’s, averaged across five runs.
Figure 10:	Evaluation performance With respect to different β schedules, averaged across five runs.
Figure 11:	Evaluation performance for DeepRacer multi-car racing against bot cars, using 4-layerCNN. The value at each iteration is smoothed by the mean value of nearest 3 iterations.
Figure 12:	Evaluation performance for DeepRacer multi-car racing against bot cars, using 5-layerCNN. The value at each iteration is smoothed by the mean value of nearest 3 iterations.
