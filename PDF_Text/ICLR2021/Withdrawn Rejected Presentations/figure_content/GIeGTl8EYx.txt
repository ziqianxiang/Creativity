Figure 1: ExampleneighborhoodSuppose G[v] consists of the star node, black nodes and black edges. Surprisingly, even with such a“regular” subgraph structure, a normal L-layer GraphSAGE cannot learn τ accurately. The reasonsare as follows. If L > 2, then normal GraphSAGE will include nodes outside V[v] , and the onlyway to always exclude the impact of these nodes is by setting the weights for that layer to 0 — thiseffectively reduces the number of layers to 2. Now since τ is linear, ReLU is not desirable, and shouldbe bypassed by shifting X with the bias parameters. The output of the neural net then simplifies toZ = [PL=1 A'v] X[v] W'] , where L = 2 for normal GraPhS AGE. For the given A[v], there doesnot exist a w` for GraPhSAGE to satisfy T = Z for any value of X[v]. On the other hand, withSAMPLE returning G[v], SHADOW-SAGE can learn τ with any Precision ifL is allowed to grow. Therequired L for SHADOW-SAGE to reach the desired Precision is determined by the mixing time ofA[v] . SHADOW-SAGE can learn the unweighted mean because the eigenvector corresPonding tothe largest eigenvalue of A[v] is a uniform vector. In fact, a deeP SHADOW-SAGE can learn theunweighted feature mean on any neighborhood.
Figure 2: Inference time after paral-lelization on commodity hardware)%( ycarucca tse8060......................................I........................................................I+......I4010-1	100	101	102	103Inference time per node (ms)SHADOWSAGESHADOWGAT■ NormalSAGE■ NormalGAT+ Flickr
Figure 3: Inference performance tradeoff. we test pre-trained models by subgraphs of various sizes.
Figure 4: Effect of increasing the model depth on the SGC and GCN architectures8Under review as a conference paper at ICLR 2021Evaluation on PPR sampler. We evaluate the PPR sampler in terms of its execution time overheadand accuracy-time tradeoff. In Figure 2, we parallelize the sampler on a 40-core Xeon CPU andthe GNN computation on a NVIDIA P100 GPU (See Appendix D.2). The sampler is lightweight:the sampling time is lower than the GNN computation time in most cases. Also, the sampling timeper node does not necessarily grow with the full graph size. By Section 3.2, the approximate PPRcomputation achieves efficiency and scalability by only traversing a local region around each targetnode. To evaluate the accuracy-time tradeoff, we take the 5-layer models of Table 1 as the pretrainedmodels. Then for SHADOW-GNN, we vary the PPR budget p from 50 to 200 with stride 50. InFigure 3, the inference time of shaDow-GNN has already included the PPR sampling time. Firstly,consistent with Table 1, inference of shaDow-GNNs achieves higher accuracy than the normalGNNs, with orders of magnitude speedup as well. In addition, based on the application requirements(e.g., latency constraint), SHADOW-GNNs have the flexibility of adjusting the sampling size withoutthe need of retraining. For example, on Reddit and ogbn-arxiv, directly reducing the subgraphsize from 200 to 50 speeds up inference by 2× to 4× at the cost of less than 1% accuracy drop.
Figure 5: Example graphon Which shaDow-GNN ismore expressive than 1-WL□Remark Note that even though We discuss the (L + 1)-layer case in the above proof, increasingthe layer L0 beyond L + 1 can still be benefitial in many cases. Consider the folloWing:CASE 1 We revisit the GraphSAGE example on Figure 1. Suppose We use a L0-layer GraphSAGEto approximate a function τ on the subgraph G[Lv] . We consider an example τ as computing theunWeighted mean of the subgraph node features. In this case, the error of approximating τ converges14Under review as a conference paper at ICLR 2021to zero when L0 goes to infinity. The error can still be significant if L0 is not sufficiently larger thanL. The desired depth L0 is determined by the mixing time of the subgraph adjacency matrix.
Figure 7: 4-hop sampler for normal 4-layerGNN: Composition of nodes u in V[v] under var-ious dist (u, v) = k3 k ≥ 41864...
Figure 6: PPR sampler for 5-layer SHADOW-GNN: Composition of nodes u in V[v] undervarious dist (u, v) = kE.2 Example on Subgraph Ensemble—PPR+1-hop4030200427.7 .7 0.
Figure 8: Subgraph ensemble: shaDow-SAGE on ogbn-arxivIn addition to the Table 3 results on shaDow-GCN, Figure 8 further shows how subgraph ensembleimproves convergence quality with the shaDow-SAGE model. We consider a PPR sampler withfixed threshold θ = 0.01, a 1-hop sampler (which returns the full 1-hop neighbors) and the ensembleof these two. The 1-hop sampler incurs significant information loss since it preserves no multi-hopneighbors. As expected, its accuracy is low. Surprisingly, we observe accuracy gain when weensemble the 1-hop sampler and the PPR sampler by Equation 3. As discussed, different samplerspreserve different kinds of local information. Rather than trying to design one “perfect” sampler, itmay be more reasonable to ensemble a few simpler ones.
