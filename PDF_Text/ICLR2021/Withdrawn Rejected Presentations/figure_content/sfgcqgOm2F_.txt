Figure 3: Randomized rounding for natural (left)and standard (right) dithering (s = 3 levels).
Figure 4: IEEE 754 single-precision binaryfloating-point format: binary32.
Figure 5: Train Loss and Test Accuracy of ResNet110 and Alexnet on CIFAR10. Speed-up isdisplayed with respect to time to execute fixed number of epochs, 320 and 200, respectively.
Figure 6: Training throughput speedup.
Figure 7: Accumulated transmission size of 1worker (CIFAR10 on AlexNet).
Figure 8: Train loss and test Aacuracy of VGG11 on CIFAR10. Green line: CWi = Ds2t,a27, CMidentity. Blue line: CWi = Dn2,a8t , CM = Cnat .
Figure 9: Violin plot of Aggregated Tensor Elements (ATE) persecond. Dashed lines denote the maximum ATE/s under line rate.
Figure 10: Convergence com-parison for weak scaling.
Figure 11: DenseNet40 (k = 12)AUeJn8e≡91O 20 40 60 80 IOO 120 140 160 180 200EpochsO 300	600	900Time(S)AUeJn8e≡91O 300	600	900Time(S)O IOO 200	300	400	500Time(S)O 20 40 60 80 IOO120 140160180 200	O IOO 200	300	400	500Epochs	Time(S)Figure 12:	AlexNet (Batch size: 256, 512 and 1024)A Extra ExperimentsA.1 Convergence Tests on CIFAR 10In order to validate that Cnat does not incur any loss in performance, we trained various DNNson the Tensorflow CNN Benchmark1 on the CIFAR 10 dataset with and without Cnat for the samenumber of epochs, and compared the test set accuracy, and training loss. As mentioned earlier, thebaseline for comparison is the default NCCL setting. We didn’t tune the hyperparameters. In all of
Figure 12:	AlexNet (Batch size: 256, 512 and 1024)A Extra ExperimentsA.1 Convergence Tests on CIFAR 10In order to validate that Cnat does not incur any loss in performance, we trained various DNNson the Tensorflow CNN Benchmark1 on the CIFAR 10 dataset with and without Cnat for the samenumber of epochs, and compared the test set accuracy, and training loss. As mentioned earlier, thebaseline for comparison is the default NCCL setting. We didn’t tune the hyperparameters. In all ofthe experiments, we used Batch Normalization, but no Dropout was used.
Figure 13:	ResNet (#layers: 20, 44 and 56)A.1.2 AlexNet Hyperparameters:For AlexNet, we chose the optimizer as SGD with momentum, with a momentum of 0.9. We trainedon three minibatch sizes: 256, 512 and 1024 for 200 epochs. The learning rate was initially set to be0.001, which was decreased by a factor of 10 after every 30 epoch.
Figure 14:	ResNet50 on ImageNetbatch-size of 220, and a dropout ratio of 0.5. No weigt decay is applied. Fig 15 shows that Cnatperforms similar to no compression both in terms of training loss and test hit rate.
Figure 15: Neural Collaborative Filtering on MovieLens-20MA.4 Dp,s VS. Dp,u: EMPIRICAL VARIANCEnat	staIn this section, we perform experiments to confirm that Dnp,ast level selection brings not just theoreticalbut also practical performance speedup in comparison to Dspt,au . We measure the empirical variance ofDspt,au and Dnp,ast . For Dnp,ast , we do not compress the norm, so we can compare just variance introducedby level selection. Our experimental setup is the following. We first generate a random vector x ofsize d = 105, with independent entries with Gaussian distribution of zero mean and unit variance (wetried other distributions, the results were similar, thus we report just this one) and then we measurenormalized empirical varianceω(x) :kc (X)-Xk2kxk2We provide boxplots, each for 100 randomly generated vectors X using the above procedure. Weperform this forp = 1, p = 2 and p = ∞. We report our findings in Fig 16, Fig 17 and Fig 18. Theseexperimental results support our theoretical findings.
Figure 16: Dnp,ast vs. Dspt,au with u = s.
Figure 17: Dnp,ast vs. Dspt,au with u = 2s-1.
Figure 18: When p = ∞ and s is very large, the empirical variance of Dspt,as can be smaller than thatof Dnp,ast . However, in this case, the variance of Dnp,ast is already negligible.
Figure 19:	CIFAR10 with VGG11.
Figure 20:	MNIST with 2 fully conected layers.
Figure 21:	Histogram of exponents of gradients exchanged during the entire training process forResNet110 (left) and Alexnet (right). Red lines denote the minimum and maximum exponent valuesof all gradients.
Figure 22: 1D visualization of the workings of natural dithering Dnp,ast and standard dither-ing Dspt,au with u = 2s-1, with s = 4. Notice that the numbers standard dithering roundsto, i.e., 0, 1/8, 2/8, . . . , 7/8, 1, form a superset of the numbers natural dithering rounds to, i.e.,0, 2-3, 2-2, 2-1, 1. Importantly, while standard dithering uses u = 24-1 = 8 levels (i.e., inter-vals) to achieve a certain fixed variance, natural dithering only needs s = 4 levels to achieve thesame variance. This is an exponential improvement in compression (see Theorem 5 for the formalstatement).
