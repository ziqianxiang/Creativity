Figure 1: Illustration of the MAML(a), and TreeMAML(b) algorithms. Both algorithms are de-signed to quickly adapt to new tasks with a small number of training samples. MAML achievesthis by introducing a gradient step in the direction of the single task. TreeMAML follows a similarapproach but exploiting the relationship between tasks by introducing a hierarchical aggregation ofthe gradients.
Figure 2: We show the tasks distribution over amplitude and phase of the original dataset used inFinn et al. (2017) (a) and in our modified dataset (b). The change in tasks distribution was introducedto simulate structured data. (c) Shows the equivalent fixed tree structure that was used to representthe data.
Figure 3: Results of sinusoidal regression task for Fixed TreeMAML, MAML and baseline. Theplots show the loss (MSE) at K shot regression for different numbers meta-testing samples. (a)shows the results for noise level 0.1 and (b) shows results for noise level 0.5. Fixed TreeMAMLalways performs performs better than or as good as MAML and the baseline, especially in the caseof a small number of data points being available.
Figure 4: Results of the multidimensional (N=64) linear regression task for Fixed TreeMAML,MAML and baseline for varying number of task data points K = 4, 8, 16, 32, 64 and 128. Similarlyto the sinusoidal regression task, TreeMAML performs better than MAML, especially when thenumber of tasks data points K is low.
Figure 5: This tree structure shows the task relationship that was logically designed and used forthe Fixed TreeMAML.
