Figure 1: Predictive distributions on the toy dataset. Shaded regions represent one standard deviation.
Figure 2: ELBO for different approximate posteriors as we change network depth/width on a datasetgenerated using a linear Gaussian model. The rand → gi line lies behind the global inducing line inwidth = 50 and width = 250.
Figure 3: Average test log likelihoods for BNNs on the UCI datasets (in nats). Error bars representone standard error. Shading represents different priors. We connect the factorised models with the fac→ gi models with a thin grey line as an aid for easier comparison. Further to the right is better.
Figure A1: Comparison of the graphical models for three approaches to inference in deep GPs:Salimbeni & Deisenroth (2017), Ustyuzhaninov et al. (2020), and ours.
Figure A2: Predictive distributions on the toy dataset as the number of inducing points changes.
Figure A3: Posterior distributions for 2-layer DGPs with local inducing and global inducing. Thefirst two columns show the predictive distributions for each layer taken individually, while the lastcolumn shows the predictive distribution of the output y.
Figure A4: ELBOs per datapoint and average test log likelihoods for BNNs on UCI datasets.
Figure A5: Calibration curves for CIFAR-10J.1 Experimental detailsThe architecture we considered for all BNN UCI experiments were fully-connected ReLU networkswith 2 hidden layers of 50 hidden units each. We performed a grid search to select the learningrate and minibatch size. For the fully factorised approximation, we selected the learning rate from{3e-4, 1e-3, 3e-3, 1e-2} and the minibatch size from {32, 100, 500}, optimising for 25000 gradientsteps; for the other methods we selected the learning rate from {3e-3, 1e-2} and fixed the minibatchsize to 10000 (as in Salimbeni & Deisenroth (2017)), optimising for 10000 gradient steps. For allmethods we selected the hyperparameters that gave the best ELBO. We trained the models using10 samples from the approximate posterior, while using 100 for evaluation. For the inducing pointmethods, we used the selected batch size for the number of inducing points per layer. For all methods,we initialised the log noise variance at -3, but use the scaling trick in Appendix G to accelerateconvergence, scaling by a factor of 10. Note that for the fully factorised method we used the localreparameterisation trick (Kingma et al., 2015); however, for fac → gi we cannot do so because theinducing point methods require that covariances be propagated through the network correctly. Forthe inducing point methods, we additionally use output channel-specific precisions, Λl,λ, whicheffectively allows the network to prune unnecessary neurons if that benefits the ELBO. However, weonly parameterise the diagonal of these precision matrices to save on computational and memorycost.
Figure A6: ELBOs per datapoint and average test log likelihoods for DGPs on UCI datasets. Thenumbers indicate the depths of the models.
Figure A7: The ELBO, test log likelihoods and classification accuracy with different priors andapproximate posteriors on a reduced MNIST dataset consisting of only the first 500 training examples.
