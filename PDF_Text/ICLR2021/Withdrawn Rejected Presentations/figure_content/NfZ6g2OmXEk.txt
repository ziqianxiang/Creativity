Figure 1: Overview of Prioritized Level Replay. The next level is either sampled from a distributionwith support over unseen levels (top), which could be the environment’s (perhaps implicit) fulltraining-level distribution, or alternatively, sampled from the replay distribution, which prioritizeslevels based on future learning potential (bottom). In either case, a trajectory τ is sampled from thenext level and used to update the replay distribution. This update depends on the lists of previouslyseen levels Λseen, their latest estimated learning potentials S, and last sampled timestamps C.
Figure 2: Mean episodic test returns (10 runs) while training under value-based level replay withrank prioritization, both with and without UCB-AutoDrAC, compared to baseline sampling meth-ods. Each colored F indicates statistically significant (p = 0.05) gains in final test performance orsample complexity along the curve, relative to uniform sampling for the condition of the same color.
Figure 3: Two example Procgen games, between which all scoring functions except for L1 value lossshow inconsistent improvements to generalization and sample efficiency, with rank prioritizationβ = 0.1, and ρ = 0.3. This inconsistency held across settings in our grid search.
Figure 4: Top: Mean episodic test returns of value-based level replay and the uniform-sampling baseline on MultiRoom-N4-Random (4 runs), ObstructedMazeGamut-Easy (3 runs), andObstructedMazeGamut-Medium (3 runs). Bottom: The probability mass assigned to levels of vary-ing difficulty over the course of training for the respective environment for a single run.
Figure 5: Example levels of each of the four difficulty levels of MultiRoom-N4-Random, in order ofincreasing difficulty from left to right. The agent (red triangle) must reach the goal (green square).
Figure 6: Example levels of each of the three difficulty levels of ObstructedMaze-Gamut-Easy, inorder of increasing difficulty from left to right. The agent must find the key, which may be hiddenunder a box, to unlock a door granting access to the goal object (blue circle).
Figure 7: Example levels in increasing difficulty from left to right of each additional difficulty settingintroduced by ObstructedMaze-Gamut-Hard in addition to those in ObstructedMaze-Gamut-Easy.
Figure 8: Mean test episodic returns on the Procgen Benchmark for value-based level replay withrank prioritization with β = 0.1 across a range of values of ρ, which weighs how much importancethe final replay distribution assigns to staleness. The replay distribution must consider both the L1value-loss and staleness values to realize improvements to generalization and sample complexity.
Figure 9: Mean test episodic returns on the Procgen Benchmark for value-based level replay withproportional prioritization with β = 0.1 across a range of values of ρ. As in the case of rankprioritization, the replay distribution must consider both the L1 value loss score and level stalenessvalues in order to realize performance improvements. Plots are averaged over 10 runs, and theshaded area indicates one standard deviation around the mean. A F next to the game name indicatesthe condition ρ = 0.1 exhibits statistically significantly better final test returns or sample efficiencyalong the test curve atp = 0.05, which we observe in 11 of 16 games.
Figure 10: Mean train episodic returns on the Procgen Benchmark for value-based level replay withβ = 0.1 and each of the prioritization methods investigated. On some games, value-based replayimproves both training sample efficiency and generalization performance (e.g. BigFish and Chaser),while on others, only generalization performance (e.g. CaveFlyer for rank prioritization). Plots areaveraged over 10 runs, and the shaded area indicates one standard deviation around the mean.
Figure 11: Comparison of mean train episodic returns on Procgen Benchmark,s hard setting forvalue-based level replay against the uniform-sampling baseline. These results were generated withthe best hyperparameters for value-based level replay found for Procgen Benchmark’s easy setting,where β = ρ = 0.01. Plots are averaged over 5 runs, and the shaded area indicates one standarddeviation around the mean. Note that even without tuning level-replay hyperparameters for ProcgenBenchmark’s hard setting, we reach statistically significant improvements in test performance on themajority of the benchmark (9 of 16 games).
Figure 12: Mean test episodic returns on MultiRoom-N4-Random (top) and ObstructedMazeGamut-Easy (bottom) with access to the full level distribution at training. We set PD to a Bernoulli param-eterized as p = 0.5 for MultiRoom-N4-Random and p = 0.95 for ObstructedMazeGamut-Easy (thebest values found via grid search). As with all MiniGrid experiments using Prioritized Level Replay,we set β = 0.1 and ρ = 0.3.
Figure 13: Mean test episodic returns of UCB-DrAC with value-based level replay on ProcgenBenchmark (easy), attaining statistically-significant (F) generalization gains on 14 of 16 games.
