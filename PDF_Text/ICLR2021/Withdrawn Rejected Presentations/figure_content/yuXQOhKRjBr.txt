Figure 1: The motivation of DS-GNN: constructing multiple sampled graphs rather than complexlayer-wise aggregation functions (the node with red circle is the central node and “FC” representsfully connected layer).
Figure 2: Architecture of DS-GNN.
Figure 3: The impact of sampling time KExperimental results are reported in Table 2. Additionally, we provide the corresponding standarddeviation in Appendix D. Similar to previous research, we report the mean classification accuracyover nodes in test set for quantitative evaluation. DS-GNN methods all achieve improvements overbase models (indicated by the bold numbers), e.g., in CiteSeer, DS-GCN (79.9%) achieves largeimprovement over GCN (75.9%). Furthermore, applying our proposed method to the base GNNmodels can achieve the better performance, outperforming the state-of-the-art models. Comparedwith DropEdge which takes GCN as base model, our DiverseSample (DS) mechanism achievesbetter or comparable results.
Figure 4: Diverse sampling VS Random samplingTo evaluate the effectiveness of the proposed diverse sampling strategy, we compare it with randomsampling strategy. Fig. 4 show the accuracy of Cora and CiteSeer with different initial node samplingprobability pinit respectively when sampling times K = 5. The results demonstrate that: 1) theperformance of random sampling will be significantly affected by pinit . The model performs poorwith a relatively small pinit , while the model effect is significantly improved when pinit increases.
Figure 5: One Sample of DBMovieMethod	MAP	F1@3	NDCG@3MLP	54.9%	39.4%	50.6%DeepWalk	61.6%	44.7%	59.1%GCN	83.2%	60.2%	82.1%DS-GCN	83.6%	60.5%	82.6%GAT	83.3%	60.5%	82.6%DS-GAT	84.0%	61.0%	83.0%Table 3: Results of Node-based Multi-label Clas-sificationPerformance on Node-based Multi-label ClassificationWe now validate the effectiveness of DS-GCN and DS-GAT. For node-based multi-label classifica-tion, we leverage the widely used ranking metrics to evaluate our method, including Mean AveragePrecision (MAP), F1, and Normalized Discounted Cumulative Gain (NDCG). These metrics en-courage the correct label to be placed ahead of irrelevant labels, and a larger value indicates betterperformance. Experimental results on DBMovie are reported in Table 3. In the multi-label clas-sification task, GNN models, i.e., both GCN and GAT, also perform much better than traditionalmethods, including MLP (only using attributes) and DeepWalk (only using only). GAT performsbetter than GCN, owing to its attention mechanism to learn edge weight. Under all evaluation met-rics, our proposed method achieves consistently an improvement over GCN and GAT, showing an
