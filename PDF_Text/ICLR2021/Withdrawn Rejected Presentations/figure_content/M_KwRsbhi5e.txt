Figure 1: (left) three policies ∏ι, ∏ and ∏3 produce three sets of polytopes bi, b2 and b3 respectively for thesame problem Q, (right) example cost matrix W and transportation matrix Γ.
Figure 2: Primal bounds versus the depth in search tree (number of branch constraints) they are foundallows pruning in line 5 Algorithm 1. We compare our RL agent with GCN, SVM, VFC on 100 maximumindependent set test instances under clean setting.
Figure 3: Average width versus depth Figure 4: Comparison of 4 RL agents5.5 Ablation S tudyWe present an ablation study of our method on maximum independent set problem by comparing four typesofRL agents: (1) PD policy + ES; (2) PD policy + NS-ES; (3) GCN +ES; (4) GCN + NS-ES. We sampleV = 200 instances as our validation set in and plot the average number of solving nodes under clean settingon the validation set during the training process for five random seeds. All agents are initialized by imitationlearning. The results are plotted in Figure 4. All curves obtain higher rewards shows that RL improves thevariable selection policy. (1) and (2) having larger rewards than (3) and (4) shows that PD policy can obtainmore improvement than GCN. Also, (2) and (4) having larger rewards than (1) and (3) shows that noveltysearch helps to find better policies. The results suggest that RL improves learning to branch and both PDpolicy, NS-ES are indispensable in the success of RL agent.
Figure 5: Illustration of splitting in B&B and the corresponding search tree(X1, X2) = (3,2) InfeaSibIex2 ≥ 3x2 ≤ 2(b) split on x2 and search tree t2A.2 IMPLEMENTATIONA.2.1 HARDWAREAll the experiments were run at a Ubuntu 18.04 machine with Intel(R) Xeon(R) Silver 4116 CPU @2.10GHz, 256 GB Memory and Nvidia RTX 2080Ti graphic cards.
