Figure 1: Heavy-tailedness in PPO during on-policy iterations. All plots show mean kurtosisaggregated over 8 MuJoCo environments. For other estimators, see App. F. For individual environ-ments with error bars, see App. H. Increases in Kurtosis implies an increase in heavy-tailedness.
Figure 2: Heavy-tailedness in PPO-NoClip during off-policy steps at various stages of trainingiterations in MuJoCo environments. All plots show mean kurtosis aggregated over 8 Mujoco envi-ronments. Plots for other estimators can be found in App. F. We also show trends with these estima-tors (with error bars) on individual environments in App H. Increases in Kurtosis implies an increasein heavy-tailedness. Dotted line represents the Kurtosis value for a Gaussian distribution. Note thatthe analysis is done with gradients taken on a fixed batch of data within a single iteration. As off-policyness increases, the actor gradients get substantially heavy-tailed. This trend is corroboratedby the increase of heavy-tailedness in ratios. Moreover, consistently we observe that the heavy-tailedness in “actor/ratios” stays constant. While initially during training, the heavy-tailedness inthe ratio’s increases substantially, during later stages the increase tapers off. The overall increaseacross training iterations is explained by the induced heavy-tailedness in the advantage estimates(cf. Sec. 3.1).
Figure 3: Heavy-tailedness in PPO-No Clip with PPO-heuristics applied progressively duringoff-policy steps, with kurtosis aggregated across 8 MuJoCo environments. For other estimators,see App. F. Dotted line represents the Kurtosis value for a Gaussian distribution. “-clip” denotesloss clipping on corresponding networks. “-gradclip” denotes both gradient clipping and loss clip-ping. Increases in Kurtosis implies an increase in heavy-tailedness. As training progresses duringoff-policy steps, the increased heavy-tailedness in actor and critic gradients is mitigated by PPO-heuristics.
Figure 4: Normalized rewards for Robust-PPO-NoClip and PPO-NoClip. Normal-ized w.r.t. the max reward obtained with PPO(with all heuristics enabled) and performance ofa random agent. (See App G for reward curveson individual environment.)Studying the behavior of SGD, Simsekli et al. (2019) questioned the Gaussianity of SGD noise, high-lighting its heavy-tailed nature. Subsequently, there has been a growing interest in understanding thenature of SGD noise. More recently, other works (SimSekli et al., 2020; Zhang et al., 2019b) explorethe heavy-tailed behavior of SGD noise, with emphasis on tasks that require adaptive methods. Inparticular, Zhang et al. (2019b) studied the nature of gradients in natural language processing (e.g.,BERT-pretraining). Later work (Panigrahi et al., 2019) studied results through Gaussianity tests onrandom projections. They showed that at least early on in training, gradients remain near Gaussianfor larger batch sizes. Some recent work has also made progress towards understanding the effec-tiveness of gradient clipping in convergence (Zhang et al., 2019b;a; SS imsSekli et al., 2020). However,its consequences modulo the bias- vs variance trade-off are not completely understood.
Figure 5: Kurtosis plots. Analysis on norms of 100-dimensional vectors such that each coordinateis sampled iid from Pareto distribution or normal distribution. (a) Variation in kurtosis (κ1/4) as thesample size is varied for samples from normal distribution and Pareto with tail index 2 (i.e, α = 2).
Figure 6: Heavy-tailedness in advantages grouped by their sign, rewards and value estimates.
Figure 7:	Heavy-tailedness in advantages for A2C and PPO during on-policy iterates. Clearly, asthe training progresses heavy-tailedness in PPO advantages increases rapidly when compared withA2C advantages. The observed behavior arises to the off-policy training of the agent in PPO. Thisexplains why we observe heightened heavy-tailedness in PPO during onpolicy iterations in Fig 1(a).
Figure 8:	Distribution of log(∣A∏θ |) over training grouped by sign of log(∣A∏θ |) for HalfCheetah-v2 . To elaborate, we collect the advantages and separately plot the grouped advantages with theirsign, i.e., we draw histograms separately for negative and positive advantages. As training proceeds,we clearly observe the increasing heavy-tailed behavior in negative advatanges as captured by thehigher fraction of log(∣A∏θ |) with large magnitude. Moreover, the histograms for positive advan-tages (which resembel Gaussain pdf) stay almost the same throughout training. This highlights theparticular heavy-tailed (outlier-rich) nature of negative advantages corroborating our experimentswith kurtosis and tail-index estimators.
Figure 9:	Heavy-tailedness in PPO during on-policy iterations. All plots show mean alpha indexaggregated over 8 MuJoCo environments. A decrease in alpha-index implies an increase in heavy-tailedness. (a) Alpha index vs on-policy iterations for A2C and PPO. Evidently, as training pro-ceeds, the gradients become more heavy-tailed for both the methods. (b) Alpha index vs on-policyiterations for actor networks in PPO. (c) Alpha index vs on-policy iterations for critic networksin PPO. Both critic and actor gradients become more heavy-tailed on-policy steps as the agent istrained. Note that as the gradients become more heavy-tailed, we observe a corresponding increaseof heavy-tailedness in the advantage estimates (A∏0 ). However, actor∕A∏o and CntiC/A∏0 (i.e.,actor or critic gradient norm divided by GAE estimates) remain light-tailed throughout the training.
Figure 10: Heavy-tailedness in PPO during on-policy iterations. All plots show mean fraCtion ofdireCtions aCCepted by Anderon-Darling test over 8 MuJoCo environments. A higher aCCepted fraC-tion indiCates a Gaussian behavior. (b) FraCtion aCCepted vs on-poliCy iterations for aCtor networksin PPO. (C) FraCtion aCCepted vs on-poliCy iterations for CritiC networks in PPO. Both CritiC andactor gradients remain non-Gaussian as the agent is trained. HoWeVer,“actor/An。” and “critic/A∏0”(i.e., aCtor or CritiC gradient norm divided by GAE estimates) have fairly high fraCtion of direCtionsaccepted, hinting their Gaussian nature.
Figure 11: Heavy-tailedness in PPO-NoClip during off-policy steps at various stages of train-ing iterations in MuJoCo environments. All plots show mean alpha index aggregated over 8 Mujocoenvironments. A decrease in alpha index implies an increase in heavy-tailedness. As off-policynessincreases, the actor gradients get substantially heavy-tailed. This trend is corroborated by the in-crease of heavy-tailedness in ratios. Moreover, consistently we observe that the heavy-tailedness in“actor/ratios” stays constant. While initially during training, the heavy-tailedness in the ratio’s in-creases substantially, during later stages the increase tapers off. The overall increase across trainingiterations is explained by the induced heavy-tailedness in the advantage estimates (cf. Sec. 3.1).
Figure 12: Reward curves as training progresses in 8 different Mujoco Environments aggre-gated across 30 random seeds and for hyperparameter setting tabulated in Table 1. The shadedregion denotes the one standard deviation across seeds. We observe that except in Hopper-v2 en-vironment, the mean reward with Robust-PPO-NoClip is significantly better than PPO-NoClipand close to that achieved by PPO with optimal hyperparameters. Aggregated results shown inFig. 4.
Figure 13: Heavy-tailedness in actor gradients for PPO during on-policy steps for 8 MuJoCoenvironments. All plots show mean and std of kurtosis aggregated over 30 random seeds. As theagent is trained, actor gradients become more heavy-tailed. Note that as the gradients become moreheavy-tailed, we observe a corresponding increase of heavy-tailedness in the advantage estimates(A∏o). However, actor/A∏0 (i.e., actor gradient norm divided by advantage) remain light-tailedthroughout the training.
Figure 14: Heavy-tailedness in critic gradients for PPO during on-policy steps for 8 MuJoCoenvironments. All plots show mean and std of kurtosis aggregated over 30 random seeds. As theagent is trained, critic gradients become more heavy-tailed. Note that as the gradients become moreheavy-tailed, we observe a corresponding increase of heavy-tailedness in the advantage estimates(A∏o). However, Critic/A∏0 (i.e., critic gradient norm divided by advantage) remain light-tailedthroughout the training.
Figure 15: Heavy-tailedness in PPO-NoClip during off-policy steps at Initialization for 8 Mu-JoCo environments. All plots show mean and std of kurtosis aggregated over 30 random seeds.
Figure 16: KL divergence between current and previous policies with the optimal hyperparam-eters (parameters in Table 1). We measure the mean empirical KL divergence between the policyobtained at the end of off-policy training (after every 320 gradient steps) and the sampling policyat the beginning of every training iteration. The quantities are measured over the state-action pairscollected in the training step (Engstrom et al. (2019) observed similar results with both unseen dataand training data). We observe that both the algorithms maintain a KL based trust region. The trendwith KL divergence in PPO matches with the observations made in Engstrom et al. (2019) wherethey also observed that it peeks in halfway in training.
Figure 17: Reward curves with advantage clipping in 8 different Mujoco Environments aggre-gated across 30 random seeds. The shaded region denotes the one standard deviation across seeds.
Figure 18: Heavy-tailedness in PPOadvantages with per-environmenttuned clipping threshold in MuJoCoenvironments. All plots show meankurtosis aggregated over 8 Mujocoenvironments. With clipping advan-tages at appropriate thresholds (tunedper environment), we observe that theheavy-tailedness in advantages remainsalmost constant with training.
Figure 19: Heavy-tailednessinPPO-NoClipadvantages throughout the training as the de-gree of off-policyness is varied in MuJoCo en-vironments. Kurtosis is aggregated over 8 Mu-joco environments. We plot kurtosis vs on-policy iterates. As the number of off-policyepochs increases, the heavy-tailedness in advan-tages remains the same showing an increase inthe number of offline epochs has a minor effecton the induced heavy-tailedness in the advan-tage estimates.
Figure 20: Heavy-tailedness in PPO-NoCliplikelihood-ratios as the degree of off-policyness is varied. at initialization inMuJoCo environments. Kurtosis is aggregatedover 8 Mujoco environments. We plot kurtosisvs the fraction of off-policy steps (i.e. numberof steps taken normalized by the total number ofgradients steps in one epoch). As the number ofoff-policy epochs increase, the heavy-tailednessin ratios increases substantially. The same trendholds at other training iterations, however, thedegree of increase tappers off.
Figure 21: (Top two rows) Reward curves with the varying number of offline epochs in 8 differentMujoco Environments aggregated across 10 random seeds. Bracketed quantity in the legend denotesthe number of offline epochs used for PPO-NoClip training. Clearly, as the number of offlineepochs increases, the performance of the agent drops (consistent behavior across all environments).
