Figure 1: Overview of the proposed pipeline. For training, both spectral consistency baseddisparity estimation and view prediction take stereo image pairs as input. In inference, a singleimage is used to estimate the depth. Then the fusion module takes both of the disparities to producea refined output which prevent blurred object boundary and wrong-matching regions.
Figure 2: Confidence maps from the corresponding predicted disparities for two approaches respec-tively. (a) Input color image. (b) Predicted disparity from spectral consistency constrained method.
Figure 3: Depth estimation samples from our refinement compared with depths from our two depthestimation separately without fusion. Left to right: Input color image; Output from spectral consis-tency based estimation only; Output from single image stereo network only; Our full output.
Figure 4: Visual performance of the proposed method compared with other recent methods (Godard& et al. (2019) Poggi & et al. (2018) Yin & Shi (2018)). First row: Input image; Second row:Disparity from GeoNet (Yin & Shi (2018)). Third row: Disparity from 3-Net (Poggi & et al. (2018));Fourth row: Disparity from Monodepth2 (Godard & et al. (2019)); Last row: Our pipeline output.
Figure 5: Comparison with other learning-based stereo matching methods from our synthesizedimage pairs. Left to right: Left input image; Synthesized right image; Disparity output from Luoet al. (2016); Disparity output from Chang & et al. (2018); Our single image stereo output.
Figure 6: Qualitative results of our pipeline on Cityscapes and DrivingStereo stereo dataset withouttraining or fine-tuning. Upper two images are from Cityscapes, the rest are from DrivingStereo.
Figure 7: Examples of 3D reconstruction from the original input color image and the estimateddisparity map from our full pipeline.
