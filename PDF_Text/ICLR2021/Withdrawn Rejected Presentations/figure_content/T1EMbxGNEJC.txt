Figure 1: Diagram of RankingMatch. In addition to Cross-Entropy loss, Ranking loss is used toencourage the model to produce the similar outputs for the images from the same class.
Figure 2:	t-SNE visualization of the “logits” scores of the methods on CIFAR-10 test set. Themodels were trained for 128 epochs with 4000 labels. The same color means the same class.
Figure 3:	Training time per epoch (seconds) during 128 epochs.
Figure 4: Illustration of image representation and model output on CIFAR-10.
Figure 5: t-SNE visualization of the image representations and model outputs on CIFAR-10 test set.
Figure 6: t-SNE visualization of the “logits” scores of the methods on CIFAR-10 test set. Themodels were trained for 128 epochs with 4000 labels. Note that this figure contains higher-resolutionversions of the figures shown in Figure 2.
Figure 7: Confusion matrices for models in Figure 6. Classes in Figure 6 are numbered from 0 to 9,respectively.
Figure 8: t-SNE visualization of the “logits” scores of RankingMatch with variants of Triplet losson CIFAR-10 test set. The models were trained for 128 epochs with 4000 labels.
Figure 9: Confusion matrices for models in Figure 8. Classes in Figure 8 are numbered from 0 to 9,respectively.
Figure 10: t-SNE visualization of the “logits” scores of RankingMatch with Contrastive loss onCIFAR-10 test set. The models were trained for 128 epochs with 4000 labels.
Figure 11: Confusion matrices for models in Figure 10. Classes in Figure 10 are numbered from 0to 9, respectively.
Figure 12: Training time per epoch (seconds) and GPU memory usage (MB) during 128 epochs onCIFAR-10, SVHN, and CIFAR-100.
Figure 13: Training time per batch (milliseconds) and GPU memory usage (MB) during the first5100 training steps on CIFAR-10 and SVHN.
Figure 14: Training time per batch (milliseconds) and GPU memory usage (MB) during the first31620 training steps on CIFAR-100.
