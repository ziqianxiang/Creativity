Figure 1: The pipeline of our cross-modal pair discrimination (CPD) framework. First, the visualand text are fed into modality-specific networks for feature extraction. Then, the visual and textualfeatures are mapped into a common 256-dimensional space. The cross-modal framework is learnedvia video and text pair discrimination, which tries to make corresponding pairs closer than otherinconsistent pairs using a softmax criteria. The learnt spatiotemporal features could be deployeddirectly or fine-tuned for downstream tasks.
Figure 2: List of top 10 and bottom 10 kinetics classes sorted by the frequency of at least one wordin label occurring in according title of Kinetics-title-clean dataset. Zoom in for more details.
Figure 3: Examples of video and title pairs from KinetiCs-210k.
Figure 4: Examples of videos and their associated captions from Instagram-300k.
