Figure 1: Deep Sea Benchmark. QEX, CTS, and RND use intrinsic rewards; BDQN, SU, and NNSuse posterior sampling (Section 5.1). Posterior sampling does well on the deterministic version, butstruggles on the stochastic version, suggesting an estimation bias (Section 2). Only TDU performs(near-)optimally on both the deterministic and the stochastic version of Deep Sea.
Figure 2: Deep Sea results. All models solve the deterministic version for prior scale λ = 3 (dashedline). TDU also solves it for λ = 1. Left: introducing stochasticity substantially deteriorates baselineperformance; including TDU (β > 0) recovers close to full performance. Center left: effect ofvarying λ, TDU benefits from diversity in Q estimates. Center right: effect of removing prior (λ=0).
Figure 3: Atari results with distributed training. We compare TDU with and without additive priorfunctions to R2D2 and Bootstrapped R2D2 (B-R2D2). Left: Results for montezuma_revenge.
Figure 4:	Performance on Binary Tree MDP. Top left: BootDQN (β = 0). Others: TDU with varyingstrenghts of intrinsic reward (β > 0). Results with prior strength λ = 3. Mean performance over 5seeds for each tree depth.
Figure 5:	Hyper-parameter sensitivity analysis on Binary Tree MDP. Top: Sweep over number ofexploration policies (N) for β = 0.5. Top: Sweep over number of exploration value functions (N)for β = 1. All results without prior functions (λ = 0). Mean performance over 5 seeds for each treedepth.
Figure 6: Overall performance scores on Bsuite. Left: Effect of varying β . Right: comparison ofTDU to exploration under σ = σ(Q) as intrinsic reward (QU) or as an immediate bonus (Q+UCB).
Figure 7: Bsuite per-task results. Results reported for different values of β with prior λ = 3. We alsoreport results under UCB1 policy sampling (“bandit”) for β = 1, λ = 3, η = 8.
Figure 8:	Ablation for prior scale, λ and the number of explorers, N , on the distributed setting. Wefix β = 1. Refer to the text for details on the ablation and exploration set of games.
Figure 9:	Ablation for the exploration bonus strength, β, on the distributed setting. We fix (N =5, λ = 0). We report the mean HNS for the ensemble of exploiter (solid lines) and the ensemble ofexplorers (dotted lines). All runs are average over three seeds per game. Refer to the text for detailson ablation and exploration set of games.
Figure 10: Ablation for the exploration bonus strength, β, on the distributed setting. We fix (N =5, λ = 0). We report the score on three different games for the ensemble of exploiter (solid lines) andthe ensemble of explorers (dotted lines). All runs are average over three seeds per game.
Figure 11:	Performance on each game in the main experiment in Section 5.2. Shading depictsstandard deviation over 8 seeds.
Figure 12:	Performance across all games in the main experiment in Section 5.2. We report meanHNS over the full set of games used in the main experiment, dense reward games, and explorationgames. Shading depicts standard deviation over 8 seeds.
Figure 13: Performance over the 57 atari games. We report mean and median HNS over the full suite,and mean HNS over the exploration games.
Figure 14: Results for each individual game. Shading depicts standard deviation over 3 seeds.
