Figure 1: (a) DNN setup: The DNN (depth dc, width wc) has layer-by-layer short-range connections(gray) with additional long-range links (purple/red). (b) Simulation of Gaussian matrices: Meansingular values vs. size of a matrix (wc + m/2, wc). Mean singular values increase as m increases(more simulations are given in Appendix D). (c) Convolutional layers form a similar topologicalstructure as MLP layers: All input channels contribute to all output channels.
Figure 2: MNIST results: (a) Models with different number of parameters (#Params) achieve similartest accuracy. (b) Test accuracy curves of models with different depths/#Params concentrate whenplotted against NN-Mass (test accuracy std. dev.〜0.05 - 0.34%). (c,d) Mean singular values ofJi,i-ι are much better correlated with NN-Mass (R2 = 0.79) than with #Params (R2 = 0.31).
Figure 3: Models A and C have thesame NN-Mass and achieve very sim-ilar training convergence, even thoughthey have highly different #Params anddepth. Model B has significantly fewerlayers than C but the same #Params, yetachieves a faster training convergencethan C (B has higher NN-Mass than C).
Figure 4: CIFAR-10 Width Multiplier wm = 2:(a) Models with very different #Params (box W)achieve similar test accuracies. (b) Models withsimilar accuracy often have similar NN-Mass:Test Accuracy vs. Number of ParametersMore results with different width multipliers aregiven in Appendix H.4. Again, the observationsare similar and for higher width, the models tendto cluster even more tightly for NN-Mass.
Figure 5: (a,b) ImageNet: (a) Models {P,Q}, {W,X}, and {Y,Z} have very different #ParamS butsimilar test accuracy. (b) When plotted against NN-Mass, the models with similar NN-Mass andaccuracy cluster together. (c,d) CIFAR-10 with DSConv: Again, models with similar NN-Massachieve similar accuracy but have quite different #ParamS/layers.
Figure 6: (a) Small-World Networks in traditional network science are modeled as a superpositionof a lattice network (G) and a random network R [Watts & Strogatz (1998); Newman & Watts(1999); Monasson (1999)]. (b) A DNN/CNN with both short-range and long-range links can besimilarly modeled as a random network superimposed on a lattice network. Not all links are shownfor simplicity.
Figure 7: Mean Singular Value E[σi] only increases with H while varying W. For small-enoughranges, the E[σi] vs. H relationship can be approximated by a linear trend.
Figure 8: To simulate more realistic Jacobian matrices, we calculate the mean singular value of matrixM with size [wc + m/2, wc] (wc is given by the Width in the title of each sub-figure). Clearly, E[σi]varies linearly with corresponding NN-Mass for all wc values. Moreover, as wc increases, the meansingular values (E[σi]) increase. Both observations show that E[σi] increases with k = wc + m/2(since the height of the Jacobian matrix H = k depends on both wc and m).
Figure 9: An example of CNN to calculate NN-Density and NN-Mass. Not all links are shown in themain figure for simplicity. The inset shows the contribution from all long-range and short-range links:The feature maps for randomly selected channels are concatenated at the current layer (similar toDensenets [Huang et al. (2017)]). At each layer in a given cell, the maximum number of channelsthat can contribute long-range links is given by tc .
Figure 10: More MNIST training convergence results (a, b are repeated from Fig. 2 but have beenannotated with different models (X, Y, Z, D, E, F)): (a) Models with different #Params achieve similartest accuracy. (b) Test accuracy curves of models with different depths/#Params concentrate whenplotted against NN-Mass (test accuracy std. dev.〜0.05-0.34%). (c,d) Models X and Y have thesame NN-Mass and achieve very similar training convergence, even though they have highly different#Params and depth. Model Z has significantly fewer layers than Y but the same #Params and yetachieves faster training convergence than Y (Z has higher NN-Mass than Y). The above conclusionshold true for models D, E, and F. Note that, the training convergence curves for similar NN-Massmodels are coinciding.
Figure 11: Illustration of synthetic datasets Seg4 and Circle4: (a). Seg20 (Seg30) dataset is similar toSeg4, but divides the [0, 1] range into 20 (30) segments. (b). Circle (or Circle20) dataset is similar toCircle4, but divides a unit circle into 20 concentric circles.
Figure 12: Synthetic results: (a, b, c) Models with different #Params achieve similar test accuracyacross all synthetic datasets. (d, e, f) Test accuracy curves for the same set of models come closertogether when plotted against NN-Mass.
Figure 13: Synthetic results (Circle20 datasets): Mean singular value of Ji,i-1 is much bettercorrelated with NN-Mass than with #Params.
Figure 14: CIFAR-10 Width Multiplier wm = 2: Shallower models with higher density can reachcomparable accuracy to deeper models with lower density. This does not help since models withdifferent depths achieve comparable accuracies at different densities.
Figure 15: Similar observations hold for low- (wm = 1) and high-width (wm = 3) models: (a,b) Many models with very different #Params (boxes U and V) cluster into buckets W and Z (see alsoother buckets). (c, d) For high-width, we observe a significantly tighter clustering compared to thelow-width case. Results are reported as the mean of three runs (std. dev.〜0.1%).
Figure 16: Impact of varying width: (a) Width multiplier, wm = 1, (b) wm = 2, and (c) wm = 3. Aswidth increases, the capacity of small (shallower) models increases and, therefore, the accuracy-gapbetween models of different depths reduces. Hence, the R2 for linear fit increases as width increases.
Figure 17: NN-Mass as an indicator of model performance compared to parameter counting. (a) Forwm = 2, log(#parameters) fits the test accuracy with an R2 = 0.76. (b) For the same wm = 2case, log(NN-Mass) fits the test accuracy with a higher R2 = 0.84. (c) For higher width (wm = 3),parameter counting completely fails to fit the test accuracy of various models (R2 = 0.14). (d) Incontrast, NN-Mass still fits the accuracies with a high R2 = 0.9.
Figure 18: Similar results are obtained for CIFAR-100 (wm = 2). (a) Models in box W have highlydifferent #parameters but achieve similar accuracy. (b) These models get clustered into buckets Yand Z. (c) The R2 value for fitting a linear regression model is 0.84 which shows that NN-Mass is agood predictor of test accuracy. Results are reported as the mean of three runs (std. dev.〜0.2%).
Figure 19: Models with highly different number of FLOPS achieve similar test accuracies. The CNNarchitectures are the same as those used in Figures 4, 15, and 18. The pattern for FLOPS is verysimilar to that for the number of parameters. Hence, these results show that models with both highlydifferent number of parameters and FLOPS can achieve similar test accuracy. Again, these modelscluster together when plotted against NN-Mass.
