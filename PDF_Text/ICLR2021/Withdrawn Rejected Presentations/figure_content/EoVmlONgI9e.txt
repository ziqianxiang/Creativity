Figure 1: EOIPφtι∖θ)more identifiableby PD&MIfitting with PD&MI<obl >,<o2,2>learning the classifier to enhance intrinsic reward signal and promote individuality. Unlike existingwork on behavior diversification, EOI directly correlates individuality with the task by intrinsicreward, and thus individuality emerges naturally during agents’ learning. EOI can be applied toDec-POMDP tasks and trained along with CTDE algorithms. We design practical techniques toimplement EOI on top of two popular MARL methods, MAAC and QMIX. We also propose anunbiased gradient estimation for learning from other agents’ experiences to accelerate learning, whichappears in Appendix A.1.
Figure 2: Illustration of EOI with MAAC and QMIX.
Figure 3: Illustration of scenarios: Pac-men (left), Windy Maze (mid), Firefighters (right).
Figure 4: Learning curves in Pac-Men: EOI+MAAC (left) and EOI+QMIX (right).
Figure 5: Distributions of agents’ positions of QMIX (left) and EOI+QMIX (mid), and kernel PCA of agentsobservations of EOI+QMIX (right) in Pac-Men. The darker color means the higher value.
Figure 6: Learning curves of intrinsic reward and environmental reward of EOI+MAAC (left) and EOI+QMIX(right) in Pac-Men.
Figure 7: Action distributions of theinitial action-value functions in QMIX.
Figure 8: Learning curves with EOI+QMIX. The dotted linesare the version with the same initial action-value functions.
Figure 9: Learning curves in Windy Maze: EOI+MAAC (left) and EOI+QMIX (right).
Figure 10: Learning curves in Firefighters (left) and 8m (right).
Figure 11: Learning curves of EOI with LFO on top of MAAC (a) and QMIX (b), and the curve of importanceweight during the training (c) in Pac-Men.
Figure 12: Comparison with ROMAand HC in Pac-Men.
Figure 13: The easy version of Pac-(right).
Figure 14: Learning curves of EOI+MAAC with different α (a), learning curves of EOI+PPO (b), and learningcurves of KL divergence+MAAC (c) in Pac-Men.
