Figure 1: Illustration of continuous transfer learn-ing. It learns a predictive function in DTt us-ing knowledge from both source domain DS andhistorical target domain DTi (i = 1, ∙∙∙ ,t - 1).
Figure 2: Comparison of domain discrepancy andtarget accuracyvations. (1) The classification accuracy on the target domain significantly decreases from targetdomain T1 to T8. One explanation is that the joint distribution p(x, y) on the time evolving targetdomain gradually shifted. (2) The A-distance increases from S1→T1 to S1→T4, and then decreasesfrom S1→T4 to S1→T8. That is because it only estimates the difference of the marginal feature dis-tribution p(x) between the source and target domains. (3) The C-divergence keeps increasing fromS1→T1 to S1→T8, which indicates the decreasing task relatedness between the source and the tar-get domains. Therefore, compared with A-distance3, the proposed C-divergence better characterizesthe transferability from the source to the target domains.
Figure 3: Comparison of error boundsWe use the 0-1 loss function as L and assume that the hypothesis space H consists of linear classi-fiers in the feature space. Figure 3 shows the estimated error bounds and target error with the timeevolving target domain (i.e., S1→T1, •一，S1→T8 in a new synthetic data set with a slower timeevolving target domain to ensure that the baseline bound is meaningful most of the time) where wechoose h = hT。. It demonstrates that our C-divergence based error bound is much tighter than thebaseline. Notice that when transferring source domain S1 to target domain T8, our error bound islargely determined by the C -divergence, whereas the baseline is determined by the difference be-tween the optimal source and target hypothesizes. Furthermore, given any hypothesis h ∈H, wemay not be able to estimate the baseline bound when the optimal hypothesis is not available.
