Figure 1: Overview of our approach. The model context variable (φ) is adapted using gradient descent, and theadapted context variable (φT ) is fed to the policy alongside state so the policy can be trained with standard RL(Model Identification). The adapted model is used to relabel the data from other tasks by predicting next stateand reward, generating synthetic experience to continue improving the policy (Experience Relabeling).
Figure 2: Performance on standard meta-RL benchmarks. Return is evaluated over the course of the meta-training process on meta-test tasks that are in-distribution.
Figure 3: Illustration of out-of-distribution adaptation tasks: (a) Cheetah-Velocity Medium (target velocitytraining set in blue, test set in red) and Cheetah-Velocity Hard (target velocity training set in green, test set inred), (b) Ant Direction (target direction training tasks in green, test tasks in red), (c) Cheetah Negated Jointsand (d) Ant Negated Joints. Training and test sets are indicated in the figure for (a) and (b). In the negated jointenvironments, the control is negated to a set of randomly selected joints, and the movement direction whencontrol is applied is depicted for both negated and normal joints.
Figure 4: Performance on out-of-distribution tasks. All algorithms are meta-trained with the same amount ofdata, and then evaluated on out-of-distribution tasks. Cheetah-Velocity and Ant-Direction environments havevarying reward functions, while Cheetah-Negated-Joints and Ant-Negated-Joints have different dynamics.
Figure 5: Extrapolation performance on OOD tasks. In all experiements, we see our method exceeds or matchesthe performance of previous state-of-the-art methods. We also observe that experience relabeling is crucial togetting good performance on out-of-distribution tasks.
Figure 6: Performance evaluated on validation tasks of varying difficulty. For Cheetah Velocity, the trainingdistribution consists of target speeds from 0 to 1.5 m/s, and so tasks become harder left to right along the x axis.
