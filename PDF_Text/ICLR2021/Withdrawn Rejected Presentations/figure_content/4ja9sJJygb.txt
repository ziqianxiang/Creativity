Figure 1:	Repeated kernel-width vectors in the second convolutional layer of LeNet-5, after pruningand quantizationTo illustrate this phenomenon, Figure 1 shows an example of the weight tensor of the second convo-lutional layer of LeNet-5 after pruning and quantization. In this convolutional layer the kernel sizeof is 5 Ã— 5, and for the purposes of illustration we show the kernel tensor as a 2D matrix of width 5.
Figure 2:	Parameter reduction in LeNet-5 by pruning followed by compression of redundant vectors.
Figure 3:	Kernels used for image processing (Gonzalez & Woods, 2008): (a) Edge detection (b) Boxblur (c) Gaussian blur. As highlighted by rows, kernel-vector redundancy exists within kernels.
Figure 4: Flow of our DNN model compaction methodIn the first step we prune the network to replace existing values with zero where possible. We usethe Scapel Yu et al. (2017) pruning method, which iteratively masks out values in the weight tensorsand then retrains the network to recover accuracy. The retraining step is critical to the accuracy ofthe pruned model. We iteratively prune and retrain in a similar way to other state-of-the-art Anwaret al. (2017); Li et al. (2016); Hu et al. (2016) pruning techniques.
Figure 5: Classic BSR formatAlthough BSR can result in more compact sparse matrix representations, our results in this currentpaper show that it nonetheless contains a great deal of redundancy. As we show in Section 5 manyinstances of the same dense blocks occur many times in BSR format. We apply block-wise sharingto the matrix in BSR format to eliminate this redundancy. We propose a new matrix format which wecall shared-block sparse row (SBSR) format, which allows repeated blocks to be shared betweendifferent entries in the sparse matrix.
Figure 6: SBSR format (ours)Figure 6 shows our SBSR format. For blocks that appear for the first time, the format is similar toBSR. The values of the block are stored in a block vector, which exists alongside the row pointersand column indices. However, when a block appears for a second or subsequent time, the valuesof the block are not represented. Instead, a reference is inserted into the block matrix, which refersback to the previous location where that block appeared. Thus, the values of a repeated block appearonly in the first appearance of that block, and subsequent appearances are replaced with an referenceto the shared block. Note that this format also requires a flag to indicate whether the block appearsfor the first time (F in Figure 6) or a repeat appearance (R). This flag can be represented as a singlebit.
Figure 7: Improvement of SBSR over BSR on CNNs for 40%, 60% and 80% sparsity.
Figure 8: Compaction per layer for AlexNet(b) Fully connected layersblocks even without pruning. The compaction ratio for these layers falls with very high levels ofsparsity simply because blocks that might otherwise be duplicates are eliminated entirely when allvalues are replaced with zeroes. The AlexNet fully-connected (FC) layers (Figure 8b) exhibit highlevels of block sharing, which is consistent with the large size and large numbers of blocks in theselayers. Appendix B and C analyze the sensitivity of vector granularity for fully connected layers androunding/truncation methods.
Figure 9: Impact of vector sizes on model compacting. The curves are the lower the better. Com-paction ratio goes worse with the vector size increasing. However, the cost introduced by indexdecreasing. Table 3 shows the best recorded vector sizes.
Figure 10: Comparison between the compaction ratio when using truncation and roundingD Vector Sharing vs Element S haringIt is straightforward to see how a block-sparse representation exploits patterns in nonzeroes to in-crease the efficiency of storage. Each index incurs some overhead, so when indices address blocks ofdata, rather than single elements, the overhead is amortized by the size of the blocks, at the expenseof a slight increase in the number of stored zeroes in partial blocks.
Figure 11: Comparison between element-wise and vector-wise Huffman coding on VGG-16Figure 11 presents the result of vector-wise over element-wise compaction. Limited by the length re-quirement of the paper, we only present the experiment carried out on the VGG-16 network. Similarresults have been found across other networks in fact. All of the 16 layers, including 13 convo-lutional layers (Conv) and 3 fully connected (FC) layers, have been examined in our experiment.
Figure 12: Using a larger vector to the fully connected layers. In this experiment, the size of vectoris 4.
Figure 13: Comparison between the element-wise and vector-wise sharingE Discussion B etween SBSR and Huffman codingThough both vector-wise Huffman coding and SBSR works better than element-wise sharing, thesetwo methods proposed in this paper have some fundamental difference. As the Huffman codingbuilds a binary tree on the vectors according to their repeated frequency, its performance complexityis O(nlogn). To the contrary, the SBSR does not require to sort the vector. It can be created by asingle scan of the tensor; therefore, its computation complexity is O(n). For extracting the value,14Under review as a conference paper at ICLR 2021both vector-wise Huffman coding and the SBSR has the complexity of O(1). For Huffman coding,the indices are used to get the code first and then decode itby checking the dictionary. For the SBSR,the indices are used to get the block in the SBSR. By checking the flag, we can acquire the valuedirectly or a pointer which leads us to the value.
