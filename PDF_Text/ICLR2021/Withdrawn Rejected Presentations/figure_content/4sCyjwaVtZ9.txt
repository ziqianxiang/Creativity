Figure 1: Whitening removes correlations between feature dimensions in a dataset. Whitening is a lineartransformation of a dataset that sets all non-zero eigenvalues of the covariance matrix to 1. ZCA whitening isa specific choice of the linear transformation that rescales the data in the directions given by the eigenvectorsof the covariance matrix, but without additional rotations or flips. (a) A toy 2d dataset before and after ZCAwhitening. Red arrows indicate the eigenvectors of the covariance matrix of the unwhitened data. (b) ZCAwhitening of CIFAR10 images preserves spatial and chromatic structure, while equalizing the variance across allfeature directions.
Figure 2: Activations and weights depend on the training data only through second moments. (a) Ourmodel class consists of a linear transformation Z = WX, followed by a nonlinear map gθ (Z) with parametersθ. Note that this model class includes fully connected neural networks, among other common machine learningmodels. (b) Causal dependencies for a single gradient descent update. The changes in weights, activations, andmodel output depend on the training data through the training sample second moment matrix, Ktrain, and thetargets, Ytrain. (c) Causal structure for the entire training trajectory. The final weights and training activationsonly depend on the training data through the training sample second moment matrix Ktrain , and the targets Ytrain,while the test predictions (in purple) also depend on the mixed second moment matrix, Ktrain×test.
Figure 3: Whitening and second order optimization reduce or prevent generalization. (a)-(c) Modelstrained on both fully whitened data (blue; panes a,b) and train-whitened data (green; panes a-c) consistentlyunderperform models trained by gradient descent on unwhitened data (purple; all panes). In (a), Newton’smethod on unwhitened data (pink circles) behaves identically to gradient descent on whitened data. (d) Secondorder optimization in a convolutional network results in poorer generalization properties than steepest descent.
Figure 4: Models trained on whitened data or with second order optimizers converge faster. (a) Linearmodels trained on whitened data optimize faster, but their best test accuracy was always worse. Data plottedhere is for a training set of size 2560. Similar results for smaller training set sizes are given in Fig. App.1. (b)Whitening the data significantly lowers the number of epochs needed to train an MLP to a fixed cutoff in trainingaccuracy, when the learning rate and all other training parameters are kept constant. Discrete jumps in the plotdata correspond to points at which the (constant) learning rate was changed. See Appendix E for details. (c)Second order optimization accelerates training on unwhitened data in a convolutional network, compared togradient descent. Data shown is for a training set of size 10240. Stars correspond to values of the validation lossat which test and training losses are plotted in Fig. 3(d).
Figure 5: Regularized second order methods can train faster than gradient descent, with minimal oreven positive impact on generalization. Models were trained on a size 10240 subset of CIFAR-10 byminimizing a cross entropy loss. Error bars indicate twice the standard error in the mean. (a) Test loss as afunction of regularizer strength. At intermediate values of λ, the second order optimizer produces lower values ofthe test loss than gradient descent. Test loss is measured at the training step corresponding to the best validationperformance for both algorithms. See text for further discussion. (b) At all values of λ < 1, the second orderoptimizer requires fewer training steps to achieve its best validation performance.
