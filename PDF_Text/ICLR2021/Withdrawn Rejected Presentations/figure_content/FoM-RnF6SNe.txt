Figure 1: Preprocessing used to assign input images to buckets. Similar to Go-Explore (Ecoffet et al.,2019), we resize the images to 8 Ã— 8 pixels and discretize each of the resulting cells to one of 4 values.
Figure 2: Overview of agent performance under the five metrics. Task Reward (R) and HumanSimilarity (S) are supervised metrics. Input Entropy (C), Empowerment (E), and Information Gain(I) are task-agnostic metrics. The two task-specific exploration agents achieve the highest taskreward and human similarity on average across Atari environments, and RND without reward inMinecraft. ICM or RND each achieve the highest input entropy and infogain value in three out offour environments according to our metrics. More unexpectedly, we find that PPO and task-agnosticICM achieve high empowerment, even in Montezuma where they achieve low task reward. The no-opagent achieves the lowest scores in all metrics does not show up in the normalized coordinates.
Figure 3: Pearson correlation coefficients between the five lifetime metrics considered in thisstudy: Task Reward (R), Human Similarity (S), Input Entropy (C), Empowerment (E), InformationGain (I). The metrics were computed for the lifetime of each agent and the correlation is takenacross agents. Aggregated across environments, all metrics correlate positively. Human similaritycorrelates substantially with both task reward (0.67) and the task-agnostic metrics (0.89, 0.79, 0.66).
Figure 4: Scatter plots showing correlations between the three task-agnostic metrics (X axis) and thetwo task-specific metrics (Y axis), normalized for each environment separately. It is visually evidentthat the task-agnostic metrics show significant correlations with human similarity but are only weaklycorrelated with task reward. Additionally, note that the no-op configurations are clearly distinguishedfrom the other agents along the input entropy, empowerment, infogain, and human similarity axes,while yielding similar task reward to the random agent and some RL agent configurations. Thisshows that input entropy, empowerment, and infogain capture aspects of behavior not captured bytask reward, in particular distinguishing between the random and no-op agents that behave veryqualitatively differently despite achieving similar task reward.
