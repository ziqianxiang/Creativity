Figure 1: Running Defuse on a MNIST classifier. The (handpicked) images are examples fromthree failure scenarios identified from running Defuse. The red digit in the upper right hand cornerof the image is the classifier’s prediction. Defuse initially identifies many model failures. Next,it aggregates these failures in the distillation step for annotator labeling. Last, Defuse tunes theclassifier so that it correctly classifies the images, with minimal change in classifier performance.
Figure 2: Providing intuition for failure scenarios through a t-SNE visualization of the latent spaceof MNIST. The black diamonds correspond to the latent codes of unrestricted adversarial examples.
Figure 3: Samples from three failure scenarios from each dataset. First row: The MNIST failurescenarios. These scenarios were labeled 4, 2, 6 in order from left to right. Second row: TheSVHN failure scenarios labeled 5, 8, and 5 from left to right. Third row: The German signs failurescenarios. The label 1 corresponds to 30km/h, 2 to 50km/h, and 5 to 80km/h. The first and secondwere labeled 2 while the third was labeled 1. Defuse finds significant bugs in the classifiers.
Figure 4: Results from the best models before finetuning, finetuning only on the unrestricted adver-sarial examples, and finetuning using Defuse. The numbers presented are accuracy on the validation,test set, and failure scenario test set and the absolute number of failure scenarios generated usingDefuse. We do not include finetuning on the unrestricted adversarial examples for German Signsbecause we, the authors, assigned failure scenarios for this data set and thus do not have groundtruth labels for individual examples. Critically, the test accuracy on the failure scenarios is high forDefuse indicating that the method successfully corrects the faulty behavior.
Figure 5: The tradeoff between test set and failure scenario accuracy running correction. Weassess both test set accuracy and accuracy on the test failure scenario data finetuning over a range ofλ's and plot the trade off. There is an optimal λ for each classifier where test set and failure scenarioaccuracy are both high. This result confirms that the correction step in Defuse adequately balancesboth generalization and accuracy on the failure scenarios .
Figure 6: Annotator agreement on the unrestricted adversarial examples. We plot the mean andstandard error of the percent of annotators that voted for the majority label in an unresricted adver-sarial example across all the annotated examples. We break this down into the failure scenario andnon-failure scenario unrestricted adversarial examples and the combination between the two. Theannotators are generally in agreement though less so for the failure scenario data, indicating thesetend to be more ambiguous examples.
Figure 7:	MNIST CNN Architectureβ-VAE training details We train a β-VAE on MNIST using the architectures in figure 8 and 9.
Figure 8:	MNIST data Set encoder architecture.
Figure 9:	MNIST data set decoder architecture.
Figure 10:	German signs data set encoder architecture.
Figure 11:	German signs data set decoder architecture.
Figure 12:	SVHN data set encoder architecture.
Figure 13:	SVHN data set decoder architecture.
Figure 14: Annotation interface.
Figure 15: Annotator label 6.
Figure 16: Annotator label 3.
Figure 17: Annotator label 4.
Figure 18: Annotator label 4.
Figure 19: Annotator label 6.
Figure 20: Annotator label 8.
Figure 21: Annotator label 6.
Figure 22: Annotator label 0.
Figure 23: Annotator label 6.
Figure 24: Annotator label 6.
Figure 25: German signs class labels.
Figure 33: Annotator label 1.
Figure 34: Annotator label 2.
Figure 37: Annotator label 8.
Figure 38: Annotator label 0.
