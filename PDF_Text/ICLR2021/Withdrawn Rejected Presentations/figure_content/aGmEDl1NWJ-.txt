Figure 1: Luring effect. (left) x0 fools M ◦ P byflipping a non-robust feature f ◦ P (toward thegreen class). (right) However, f can be a robustfeature, then M is not fooled (still in the blueclass), or a non-robust feature but switched dif-ferently than f ◦ P (towards the orange class).
Figure 2: Disagreement Rate (solid line) and Inefficient Adversarial examples Rate (dashed line) fordifferent attacks.
Figure 3: l0 adversarial distortion for MNIST (left) and CIFAR10 (middle). Saliency maps forMNIST (right): (top) clean image and gradient of the cross-entropy loss with respect to input; (bot-tom) mapping gradients RxP(x) for 3 augmented models.
Figure 4: (top) Clean image, (bottom) adversarial example for the maximum perturbation allowed(left to right: = 0.4, 0.08, 0.04).
Figure 5: Illustration of the black-box threat model with two scenarios. (top) In the inner transfer-ability scenario the system to attack and to defend is the same. Because the luring effect inducesdifferent behavior when facing adversarial perturbation, the defender is able to detect adversarial ex-amples by comparing M(x) and T (x). (bottom) In the distant transferability scenario, the systemto defend may suffer from transferable adversarial examples but the defender takes advantage of theweak transferability between T and M .
