Figure 1: Probability of ? being quantized to each grid point by RQ (τ = 1.0) and SRQ respectively. (a 〜e):RQ according to different GUmbel samples, (f): GUmbel samples used in (a 〜e), and (g): SRQ, the same as theoriginal categorical distribution. The x-axis denotes weight value (except (f)), ? denotes the original value α∕2.
Figure 2: Weight distributions for 3-bit quantized LeNet-5 by (a) RQ and (b) SRQ. The x-axis and y-axisrepresent the weight values and their frequencies, respectively. The vertical dashed lines denote grid points.
Figure 3: Two mask designs in 3-bit3.3	DropBitsAlthough our multi-class STE enjoys low variance of gradients, it is biased to the mode as the binaryone in Bengio et al. (2013). To reduce the bias of a STE, Chung et al. (2016) propose the slope anneal-ing trick, but this strategy is only applicable to the binary case. To address this limitation, we proposea novel method, DropBits, to decrease the distribution bias of a multi-class STE. Inspired by droppingneurons in Dropout (Srivastava et al., 2014), we drop an arbitrary number of grid points at randomevery iteration, where in effect the probability of being quantized to dropped grid points becomes zero.
Figure 4: The illustration of the effect of DropBits on SRQ. For a certain weight, (a) the categorical distributionindicates r = ∏i/∑j=0∏j for each grid (i = 0, ∙ ∙ ∙ , 7), (b) the distribution of SRQ is a sampling distributionafter taking the argmax of r = ∏ /∑j=0∏j, and (c) the distribution of SRQ + DropBits is a sampling distributionafter taking the argmax of r = ∏/∑j=0∏j. Here, ∏k,s are initialized to 0.7 for clear understanding.
Figure 5: Illustration of Semi-Relaxed Quantization (SRQ) framework with DropBits technique.
Figure 6: Comparison of RQ, SRQ, and SRQ + DropBits in quantization error/distribution bias/variance.
Figure 7: Learning curves of VGG-7 quantized byRQ, RQ with annealing τ , and SRQ in 3-bit.
Figure 8: The comparison of bias (a) between SRQ and SRQ + DropBits, and (b) between SRQ + DropBitsand RQ, when training LeNet-5 on MNIST in 3-bit. In this experiment, only weights are quantized. The x-axisdenotes the value of a weight in the last fully connected layer.
