Figure 1: Illustration of a spectral node attention layer on a 3-hop ego network of the central node Vfrom the CITESEER dataset. Node classes are indicated by shape and color. Passing the graph throughtwo learned spectral filters (adaptive spectral filters) place attention scores on nodes, including nodev itself. Nodes with positive attention scores are presented in color. Node features are aggregated fornode v according to attention scores (aggregation via global attention). The low-pass filter attendto local neighbors (filter 1), while the high-pass filter skips the first hop and attend the nodes in thesecond hop (filter K). The resulting embeddings from multiple heads are then concatenated beforebeing sent to the next layer (multi-head concatenation). Note that we have visualized learned filtersfrom experiments.
Figure 2: Micro-F1 results for classification accuracy on disassortative nodes (βv ≤ 0.5). GNANshows better accuracy on classifying disassortative nodes than the other methods.
Figure 3: Attention matrix density and training runtime with respect to k and t. Attention matrix iseffectively sparsified by both k and t, which improves runtime efficiency. Note that the density isnot monotonically increasing for GNAN-T since the threshold is applied to the "learnable" attentionweights. When all values of ψ are below t, the density becomes 100% as a result of the softmaxnormalization.
Figure 4: Micro-F1 with respect to an ablated frequency range on disassortative graphs. We dividethe frequency range into a set of sub-ranges with different lengths. The results (a) and (d) revealthe importance of high-frequency range (1 〜2). Further experiments show that there is a subtledifference in the most important range across datasets, but it ranges between (0.75 ~ 1.25).
Figure 5: Full details of the performances on frequency ablation at 0.25 level.
