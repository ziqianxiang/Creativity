Figure 1b: Terms of the VLB vs diffusion step.
Figure 1a: The ratio βt /βt for every diffusionstep for diffusion processes of different lengths.
Figure 2: Latent samples from linear (top) and cosine (bottom) schedules respectively at linearlyspaced values oft from 0 to T. The latents in the last quarter of the linear schedule are almost purelynoise, whereas the cosine schedule adds noise more slowlyThis suggests that, in the limit of infinite diffusion steps, the choice of σt might not matter at all forsample quality. In other words, as We add more diffusion steps, the model mean μθ (xt ,t) determinesthe distribution much more than Σθ (xt, t).
Figure 3a: FID when skipping a prefix of thereverse diffusion process on ImageNet 64 × 64.
Figure 3b: α throughout diffusion in the linearschedule and our proposed cosine schedule.
Figure 4b: Gradient noise scales for the Lvlb andLhybrid objectives on ImageNet 64 × 64.
Figure 4a: Learning curves comparing the log-likelihoods achieved by different objectives onImageNet 64 × 64.
Figure 5a: FID versus sampling steps on Ima-geNet 64 × 64.
Figure 5b: NLL versus evaluation steps on Ima-geNet 64 × 64.
Figure 5c: FID versus sampling steps on CIFAR-10.
Figure 5d: NLL versus evaluation steps onCIFAR-10.
Figure 5: NLL and FID versus number of evaluation/sampling steps, for models trained on ImageNet64 × 64 and CIFAR-10. All models were trained with 4000 diffusion steps. Models that learn sigmasusing our reparametrization and Lhybrid objective (Section 3.1) increase marginally in NLL and FIDas we reduce evaluation/sampling steps, while using fixed sigmas as in Ho et al. (2020) results in alarger increase.
Figure 6b: NLL throughout training on ImageNet64 × 64 for different model sizes.
Figure 6a: FID throughout training on ImageNet64 × 64 for different model sizes.
Figure 7a: 50 sampling stepsFigure 7b: 100 sampling stepsFigure 7:	Unconditional ImageNet 64 × 64 samples as we reduce number of sampling steps for aLhybrid model with 4K diffusion steps trained for 1.5M training iterations.
Figure 7b: 100 sampling stepsFigure 7:	Unconditional ImageNet 64 × 64 samples as we reduce number of sampling steps for aLhybrid model with 4K diffusion steps trained for 1.5M training iterations.
Figure 7:	Unconditional ImageNet 64 × 64 samples as we reduce number of sampling steps for aLhybrid model with 4K diffusion steps trained for 1.5M training iterations.
Figure 8a: 50 sampling stepsFigure 8b: 100 sampling stepsFigure 8d: 400 sampling stepsFigure 8c: 200 sampling stepsFigure 8e: 1000 sampling stepsFigure 8f: 4000 sampling stepsFigure 8:	Unconditional CIFAR-10 samples as we reduce number of sampling steps for a Lhybridmodel with 4K diffusion steps trained for 500K training iterations.
Figure 8b: 100 sampling stepsFigure 8d: 400 sampling stepsFigure 8c: 200 sampling stepsFigure 8e: 1000 sampling stepsFigure 8f: 4000 sampling stepsFigure 8:	Unconditional CIFAR-10 samples as we reduce number of sampling steps for a Lhybridmodel with 4K diffusion steps trained for 500K training iterations.
Figure 8d: 400 sampling stepsFigure 8c: 200 sampling stepsFigure 8e: 1000 sampling stepsFigure 8f: 4000 sampling stepsFigure 8:	Unconditional CIFAR-10 samples as we reduce number of sampling steps for a Lhybridmodel with 4K diffusion steps trained for 500K training iterations.
Figure 8c: 200 sampling stepsFigure 8e: 1000 sampling stepsFigure 8f: 4000 sampling stepsFigure 8:	Unconditional CIFAR-10 samples as we reduce number of sampling steps for a Lhybridmodel with 4K diffusion steps trained for 500K training iterations.
Figure 8e: 1000 sampling stepsFigure 8f: 4000 sampling stepsFigure 8:	Unconditional CIFAR-10 samples as we reduce number of sampling steps for a Lhybridmodel with 4K diffusion steps trained for 500K training iterations.
Figure 8f: 4000 sampling stepsFigure 8:	Unconditional CIFAR-10 samples as we reduce number of sampling steps for a Lhybridmodel with 4K diffusion steps trained for 500K training iterations.
Figure 8:	Unconditional CIFAR-10 samples as we reduce number of sampling steps for a Lhybridmodel with 4K diffusion steps trained for 500K training iterations.
Figure 9b: Samples from Lvlb modelFigure 9a: Samples from Lhybrid modelFigure 9:	Unconditional ImageNet 64 × 64 samples generated from an Lhybrid and Lvlb model respec-tively using the exact same random noise. Both models were trained for 1.5M training iterations.
Figure 9a: Samples from Lhybrid modelFigure 9:	Unconditional ImageNet 64 × 64 samples generated from an Lhybrid and Lvlb model respec-tively using the exact same random noise. Both models were trained for 1.5M training iterations.
Figure 9:	Unconditional ImageNet 64 × 64 samples generated from an Lhybrid and Lvlb model respec-tively using the exact same random noise. Both models were trained for 1.5M training iterations.
Figure 10:	Unconditional CIFAR-10 samples generated from an Lhybrid and Lvlb model respectivelyusing the exact same random noise. Both models were trained for 500K training iterations.
Figure 12:	Conditional ImageNet 64 × 64 samples generated from an Lhybrid model trained for1.7M training steps. The classes are 9: ostrich, 11: goldfinch, 130: flamingo, 141: redshank, 154:pekinese, 157: papillon, 97: drake and 28: spotted salamander. On right we fix the random noiseseed in each column to see how the class label affects the sampling process.
Figure 13:	Evaluation metrics over the course of training for two CIFAR-10 models, both withdropout 0.1. The model trained with the linear schedule learns more slowly, but does not overfit asquickly. When too much overfitting occurs, we observed overfitting artifacts similar to those fromSalimans et al. (2017), which is reflected by increasing FID.
