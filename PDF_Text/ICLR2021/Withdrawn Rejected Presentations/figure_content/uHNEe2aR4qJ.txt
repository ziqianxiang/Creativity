Figure 1: In the context of the negative pretraining effect, e.g. when pretraining on blurred imagesbefore training on unblurred images (Achille et al., 2017), we explore the effects of interventionson “learning paths”, shown as the blue path on the learning manifold M, on the generalizationperformance of neural networks. A point on the learning manifold is defined as a learning taskτ := (p(x, y), L), composed of a data distribution of input and labels and a loss function, and alearning process ω which determines how a model f is adapted given a task τ . Tasks can vary bychanging any part of the definition such as when the data distribution is changed along a blurringnuisance as depicted above with cat images. Learning processes can be adapted by changing theirupdate equation, e.g. via learning rate changes ∆η. The learning path T determines model pathchanges from initial to final trained model ffinal. Via three interventions in the learning path we showthat varying neural network learning paths can result in a removal of negative pretraining effects.
Figure 3: Final test loss on unblurred images as swarm and violin plot. The x-axis displays thelearning rates on the first task with blurred images and the second task with unblurred images. They-axis shows the test loss on the unblurred images with dots colored to distinguish different randomseeds. The dotted line indicates the mean baseline performance across 3 seeds. A clear trend thatnegative pretraining can be overcome when increasing the learning rate is visible, particularly whenmoving from 10e-4 to 20e-4.
Figure 4:	Final test loss on standard images as swarm and violin plots. The x-axis records the numberof discretization steps, where N = 2 refers to training first, e.g. on blurred then on sharp images.
Figure 5:	Left: Various two-dimensional sequential learning paths and discretizations from source totarget task. Right: Results of deterministic 2D-paths with different discretizations on CIFAR-10. Inmost cases finer discretization improves results. We remark the surprising path-dependence of results.
Figure 6:	Final test loss on unblurred images as swarm and violin plot. The x-axis denotes whetherbias resetting was applied before the first task with blurred images and/or the second task withunblurred images. The y-axis shows the test loss on unblurred images with dots colored to distinguishdifferent random seeds. The dotted line shows the mean baseline performance across 3 seeds. Aclear decrease of the negative pretraining effect is visible for CIFAR-10. On FashionMNIST andSVHN, overall a better performance is obtained however on average results do not change very muchcompared to the [No, No] negatively pretrained network.
