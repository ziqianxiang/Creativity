Figure 1:	Semi-formal illustration of AUA theorem. (Right is adapted from Baader et al. (2020).)The theorem of Baader et al. (2020) is restricted to networks that use rectified linear units (ReLU).
Figure 2:	Examples of squashable activation functions, including popular functions, and more recentones: sigmoid, tanh, rectified linear units (ReLU) (Nair & Hinton, 2010), exponential linear unit(ELU) (Clevert et al., 2016), softplus (Glorot et al., 2011), softsign (Bergstra et al., 2009), and smoothReLU (Xie et al., 2020), which is parameterized by a > 0.
Figure 3:	Two activation functions after applying construction in Proposition 3.3. Observe that theresulting function satisfies Eq. (1), and therefore ReLU and softplus are squashable.
Figure 4: Slicing example0.42This assumption eliminates the corner case where a point sits exactly on the classification boundary, 0.5.
Figure 5: Approximating indicator functions on [0, ∞), [0, 1] and [0, 1] × [0, 1] using the sigmoidactivation functionsDilation to approximate sign function. We now discuss how to dilate t to get a sign-function-like behavior. By definition of limit, we know the following lemma, which states that ∀θ > 0 bysufficiently increasing the input of t, we can get θ close to the right limit of 1, and analogously forthe left limit. Figs. 5a and 5b show how sigmoid can approximate an indicator function.
Figure 6: Loss of precision θ dueto use of squashable activation—(ai	—	0.5e))) — t	(μ(x —	(bi	+ 0.5e)))	(2) to	aPProximate a	Sign	function∙Length of red arrows is 6 θ .
