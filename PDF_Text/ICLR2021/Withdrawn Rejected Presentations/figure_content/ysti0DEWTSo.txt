Figure 1: Test error against the number of training data samples N for several network architecturesspecified by the depth and width for (a) the 1-local label and (b) the 1-global label. Test errorscalculated by the NTK of depth 1 and 7 are also plotted. Error bars are smaller than the symbols.
Figure 2: Test error against the number of training data samples N for several network architecturesspecified by the depth and the width for (a) the 2-local label, (b) the 3-local label, (c) 2-global label,and (d) 3-global label. Error bars indicate the standard deviation of the test error for 10 iterations ofthe network initialization and the training. Test errors calculated by the NTK of the depth of 1 and 7are also plotted.
Figure 3: Depth dependence of the test error for N = 104 training samples with 2-local and 2-globallabels. The dimension of input vectors is set to be d = 500 in the 2-local label and d = 100 in the2-global label. The network width is fixed to be 500. An error bar indicates the standard deviationover 10 iterations of the training using the same dataset.
Figure 4: Learning-rate dependence of the test error. (a) Numerical results for the 2-local and 2-global labels in the network with the depth of 1 and the width of 2000. (b) Numerical results for the3-global label in the networks with the depth of 1 and 7 (the network width is set at 2000 for bothcases). The dotted lines show the test error calculated by the NTK. Each data is plotted up to themaximum learning rate beyond which the zero training error is not achieved within 2500 epochs (insome cases training fails due to divergence of network parameters during the training). Error barsindicate the standard deviation over 10 iterations of the training.
Figure 5: Local stability s(v) for a shallow network (L = 1, H = 500), a deep network (L = 7,H = 500), the 2-local label, and the 2-global label. The networks are trained on training datasetwith randomized labels. We can see that s(v) for a deep network is smaller than that for a shallowone, which implies that a deeper network has a tendency to learning more local features even if thesame dataset is given.
Figure 6: Test error against the number of training data samples N . A shallow network of depth 1and width 500 shows better generalization compared with a deep network of depth 7 and width 500.
Figure 7: Test error against the number of training data samples N for several networks trainedwith the mean-square loss for (a) the 2-local label, (b) the 3-local label, (c) 2-global label, and (d)3-global label. Error bars indicate the standard deviation of the test error for 10 iterations of thenetwork initialization and the training. Test errors calculated by the NTK of the depth of 1 and 7 arealso plotted.
