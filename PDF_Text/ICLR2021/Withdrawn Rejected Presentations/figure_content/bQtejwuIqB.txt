Figure 1: Comparison between adversarial examples and hypocritical examples. Left: Conceptualdiagrams for the generation of an adversarial example xadv and a hypocritical example xhyp . Theinput space is (ground-truth) classified into the orange lined region (e.g., class “not panda”), andthe blue dotted region (e.g., class “panda”). The black solid line is the decision boundary of a non-robust model, which classifies the region above the boundary as “panda” and the region below theboundary as “not panda”. Red shadow and black shadow in the ball B (x) denote that the pointsin there are misclassified and correctly classified, respectively. As we can see, xadv or xhyp can beeasily found by perturbing a correctly classified x or a misclassified x across the model’s decisionboundary. Right: A demonstration of adversarial examples and hypocritical examples on real data.
Figure 2: Counterexample given by Equation 4.
Figure 3: Tradeoff between natural risk and hypocritical risk on real-world datasets.
Figure 4:	Hypocritical examples. In each subfigure, the first cloumn represents the clean examplessampled from original data distribution, the second cloumn represents the generated perturbations,the third cloumn represents the perturbed examples. Perturbations are rescaled for display. Redlabels and black labels below images denote misclassification and correct classification, respectively.
Figure 5:	More examples on ImageNet. In each subfigure, the first cloumn represents the clean ex-amples sampled from original data distribution, the second cloumn represents the generated pertur-bations, the third cloumn represents the perturbed examples. Perturbations are rescaled for display.
Figure 6:	Tradeoff between natural risk and hypocritical risk on real-world datasets.
Figure 7:	Transferability of hypocritical examples on MNIST. Attacks are bounded by l∞ normwith = 0.2. “(LeNet*)” means that it is the same architecture with “(LeNet)” but different randominitialization.
Figure 8:	Transferability of hypocritical examples on CIFAR-10. Attacks are bounded by l∞ normwith = 8/255.
Figure 9: A visualization of the toy example to illustrate the phenomenon of the tradeoff betweenadversarial and hypocritical risks. Oracle decision boundary is the circle given by Equation 9. Modeldecision boundary is the line given by Equation 10 with the threshold b = 0.5. Red shadow andgreen shadow region denote that the points in there are misclassified and correctly classified by themodel, respectively. The gray lined region denotes that the points in there can be perturbed withlittle perturbations to reverse the prediction of the model.
Figure 10: Visualization of the computing formulas of precision, recall, adversarial risk, and hypo-critical risk in the toy example. These values can be viewed as the proportion of the areas of differentregions.
Figure 11: The tradeoff between precision andrecall in the toy example.
Figure 12: The tradeoff between adversarial andhypocritical risks in the toy example.
