Figure 1: Predicting whether an image will be correctly classified is challenging. Left: Images fromthe train set. Middle: Adversarial images computed on the same images with FGSM-0.1 are indis-tinguishable from the clean images, yet, they fool our classifier into making incorrect predictions.
Figure 2: Comparing the predicted log likelihood distribution of correct (blue) and incorrect (orange)samples from the test set for models trained on CIFAR-100. Top row: Log likelihoods obtained fromtraining PixelCNN on images from the train set. Bottom row: Log likelihoods obtained from trainingGMM-100 on features extracted from the train set.
Figure 3: Comparison of ROC and PR curves for detecting classification mistakes using differentgenerative models. Using a PixelCNN to model the image space fails at reliably detecting classifica-tion mistakes. GMMs trained on the feature space achieve better detection than other more flexiblemodels like VAE and MAF and are comparable to temperature scaling on the WRN model.
Figure 4: Top: Distribution of log-likelihood predicted on clean (blue) and adversarial samples(green and orange) by a GMM-1000. The log-likelihood of features extracted from adversarialsamples is lower. The histograms are separated, which means it is possible to detect adversarialsamples using the log-likelihood of their features. Bottom: ROC curve for detecting adversarialsamples using predicted log-likelihood. Our method achieves a good trade-off between true positiveand false positive rate, significantly improves over chance and achieves between 76% and 100%AUC depending on attack methods and models.
Figure 5: Comparison of ROC curves for adversarial sample detection using different metrics. Logit-based scores (Logits and Calibrated) can not reliably detect adversarial samples properly while meth-ods that model the probability space of the feature space can (GMM1000, Mahalanobis, Zheng).
Figure 6:	Additional results for model trained on CIFAR-10. Left: comparison of log-likelihooddistributions. PixelCNN is not able to distinguish correct samples from incorrect ones while a GMMtrained on the feature space can. Middle: a GMM trained on the feature space can detect adversarialsamples reliably.
Figure 7:	Comparison of ROC curves for FGSM and BIM adversarial sample detection for CIFAR-10 model.
Figure 8:	Additional comparison of ROC and PR curves for detecting classification mistakes usingdifferent generative models. Using a PixelCNN to model the image space fails at reliably detectingclassification mistakes. GMMs trained on the feature space achieve better detection than other moreflexible models like VAE and MAF and are comparable to temperature scaling on the WRN model.
Figure 9:	Additional comparison of ROC curves for FGSM adversarial sample detection for CIFAR-100 models——GMM1000: 99.37 ±0.27—Logits: 63.93 ±5.93——Calibrated: 59.72 ± 5.87--- Mahalanobis: 99.27 ±0.20/---- Zheng: 98.35 ± 0.46——GMM1000: 76.95 ± 1.19— Logits: 71.73 ± 1.96——Calibrated: 69.02 ± 1.63---- Mahalanobis: 85.30 ± 1.74—— Zheng: 76.34 ± 1.25——GMM1000: 92.16±3.32—Logits: 38.03 ±3.38——Calibrated: 36.78 ± 3.28--- Mahalanobis: 90.19 ±4.39——Zheng: 73.13 ±6.42——GMM1000: 99.99 ±0.01—Logits: 65.06 ±5.29——Calibrated: 62.42 ±7.91
Figure 10:	Additional comparison of ROC curves for BIM adversarial sample detection for CIFAR-100 models.
