Figure 1: Diagram of LIOM architecture.
Figure 2: Episodic return and 95% confidence interval against opponent policies from Π-1.
Figure 3: First and second principal components of the learned opponent representations. Pointsrepresent individual episodes, colours represent the different opponent policies in Π-1.
Figure 4: Euclidean distance of embeddingsto the embedding at 25th time step (with 95%confidence interval).
Figure 5: Action prediction accuracy.
Figure 6: Episodic return and 95% confidence interval against opponent policies from Π-1 fordifferent combinations of input data for the encoder. (In speaker-listener, LIOM (Obs, Act) overlapsWith LIOM (Act, ReW).)5.6	Number of Opponent PoliciesFinally, We evaluate the effect of number of the opponentpolicies in the average achieved returns of FIOM, LIOM,and NOM in the double speaker-listener environment. Wetrain FIOM, LIOM, and NOM against a subset Π0-1 of theoriginal Π-1 set, Where the size of Π0-1 varies betWeenone and ten. Figure 7 presents the average episodic returnachieved at the end of training against different numbers ofopponent policies. We observe that When there is a singleopponent policy the performance of FIOM, LIOM, andNOM is equal. This is expected as there is not need foropponent modelling. As We increase the size of Π0-1, Weobserve a steep decrease in the returns of NOM, While thereturns of FIOM, and LIOM decrease at a much sloWerrate. This is a natural consequence of our problem formu-lation, since at the beginning of the episode LIOM doesnot have information about the opponent and because both
Figure 7: Average episodic returnagainst different numbers of opponentpolicies.
Figure 8: Multi-agent environments used in our evaluation.
