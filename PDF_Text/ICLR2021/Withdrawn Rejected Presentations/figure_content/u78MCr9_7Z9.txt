Figure 1: Overview: we propose a correlation prior, namely L-NMS, as an additional training con-straint to train a CompVAE’s (aka. multi-object VAE) inference model (left side of the Figure). Theproposed L-NMS prior is able to not only suppress duplicates (top right of the Figure), but alsotackle problems that are related to the CompVAEs’ suboptimalities such as the background splitting(bottom right example) problem which is a known issue of IODINE.
Figure 3: Qualitative comparisons between the experimental group (tagged with “+”) and the controlgroup (tagged with “0”). The Obs column is a source image, Rec is the corresponding reconstructedimage based on the inferred representation. The next 7 columns show the independent generationof the inferred scene components. The Seg column shows the pixel label for the component withhighest probability (the specific color of the pixel is not important). Top Training with the proposedL-NMS aids the original MONet model which suffers from local minima: obtains fair factoriza-tion and reconstruction while fails to learn clean object geometries and thus generates noisy scenecomponents whereas MONet+ produces cleaner inferred components. Middle Training with theproposed L-NMS aids IODINE: resolves duplicates (circled in yellow) and fixes the weak back-ground segmentation, as shown by the large colored regions in the Seg column, which is a knownissue of IODINE Greff et al. (2019). Bottom Training with the proposed L-NMS allows MulMONto suppress duplicates and thus produce a better segmentation map. (Colored boxes and circleshighlight the duplicates and failures caused by them.)lωN0IΛI NoIAI=IAlDolphindataFigure 4: A partial-failure example from the “outlier” model (MONet0) on Dolphin (tagged with“?” in Table 1). Top The model produces good factorization but fails badly to learn good-qualityobject representations and thus show noisy generations. The proposed L-NMS fails to fix it. BottomA good example shown by a model that achieves similar quantitative performance (MulMON+).
Figure 4: A partial-failure example from the “outlier” model (MONet0) on Dolphin (tagged with“?” in Table 1). Top The model produces good factorization but fails badly to learn good-qualityobject representations and thus show noisy generations. The proposed L-NMS fails to fix it. BottomA good example shown by a model that achieves similar quantitative performance (MulMON+).
Figure 5: Results of the suboptimality analysis. Left Yellow dots represents the ∆L+ for each testdata sample (2000 test images), and the green line is the mean ∆L+ , which is the change in theELBO (evidence lower bound) value from Eqn 6. Positive values are improvements. Observe thatmost dots lie above the “no improvement” line at 0, demonstrating that L-NMS generally producesimprovements. Middle The correlation between scene factorization performance difference and the∆L+, which shows a close-to-linear positive correlation, i.e. bigger improvements in the ELBOmeasure correlate with better object overlap. Right The correlation between scene reconstructionperformance difference and the ∆L+, which shows a perfect linear negative correlation, i.e. im-provements in ELBO mean better scene reconstruction.
Figure 6: Ablation study results. Top left Scene observation reconstruction performance vs. L-NMSprior precision (σ). Top right Scene decomposition performance vs. L-NMS prior precision (σ).
