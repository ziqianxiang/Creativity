Figure 1: Architecture of the proposed UM-encoder model. A separation mark “SEP” is insertedbetween them. Position embedding is introduced to distinguish the word order in each sentence;Illllllllhconcat-contextHΠJ[∏Jsource context TM-target contextFigure 2: Architecture of TM-concat decoderFigure 3: Architecture of TM-gate decoderwe only use one encoder to model the concatenated input. In this way, no additional parameterswill be introduced and can also keep the same source sentence’s semantic consistency. Meanwhile,to distinguish the context information from different sentences, a separation mark ”SEP” is insertedbetween sentences. We arrange its position embedding separately to keep the word order for eachsentence.
Figure 2: Architecture of TM-concat decoderFigure 3: Architecture of TM-gate decoderwe only use one encoder to model the concatenated input. In this way, no additional parameterswill be introduced and can also keep the same source sentence’s semantic consistency. Meanwhile,to distinguish the context information from different sentences, a separation mark ”SEP” is insertedbetween sentences. We arrange its position embedding separately to keep the word order for eachsentence.
Figure 3: Architecture of TM-gate decoderwe only use one encoder to model the concatenated input. In this way, no additional parameterswill be introduced and can also keep the same source sentence’s semantic consistency. Meanwhile,to distinguish the context information from different sentences, a separation mark ”SEP” is insertedbetween sentences. We arrange its position embedding separately to keep the word order for eachsentence.
Figure 4: Architecture of TM-point decoder.
