Figure 1: Neither Gradient Descent nor Ascent can traverse stationary points. An immediate conse-quence of Lemma 1 is that ifwe initialize in the above example θi(0) at (a), fi(θi(t)) can not escapethe purple section. This extends to cases where θi is vector of variables.
Figure 2: Level sets of Lyapunov function ofEquation (4) for both F and G being one dimen-sional sigmoid functions.
Figure 3: Comparison of`2 distance from the equilibrium and the Lyapunov function. Both of themconverge to zero as we state but `2 distance not monotonically to zero. For pdata we choose a fullymixed distribution of dimension d = 4. Given the sigmoid activations all the initializations are safe.
Figure 4: On the left, We show the trajectories of regularized GDA for α2 = 1 as well as the levelsets of Equation (4). All trajectories converge to one of the two equilibria (0, 1) and (0, -1) whereaswithout regularization, GDA would cycle on the level sets. In the right figure, we replace the exactexpectations in VWGAN with approximations via sampling and continuous time updates on α and vwith discrete ones. For small learning rates and large sample sizes, unregularized GDA continues tocycle. In contrast, the regularization approach of Lei et al. (2019) converges to the (0, 1) equilibrium.
Figure 5: Trajectories of a single player using gradient-descent-ascent dynamics for a hidden bilin-ear game L(F(θ), G(φ)) = F> (θ)AG(φ) where A is the classical Rock-Paper-Scissors table andF, G have the sigmoid activations. The two left figures present the Poincare recurrence for differentinitializations of the dynamics, a behavior consistent with the Lyapunov stability of Theorem 2. Onthe other hand, the two figures on the right illustrate convergent to the mixed Nash equilibrium exe-cutions which exploit the regularization tools as described in Section 3.3. The regularization termsadded are centered at the mixed equilbrium of the game, leading to convergence to the unmodifiedequilibrium of the Rock-Paper-Scissors game.
