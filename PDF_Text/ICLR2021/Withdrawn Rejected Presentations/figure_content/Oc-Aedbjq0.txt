Figure 1: Overview of our proposed method. The width and height dimension of weight tensors areomitted. The architecture vector v is firstly generated from fixed input ai, i = 1, . . . , L. Then, asub-network is sampled according to the architecture vector v. The parameters of HSN are updatedby using gradients from the loss function when evaluating the sub-network.
Figure 2: (a) For the original FLOPs regularization, some architectures may become unreachable. (b)After layer-wise scaling, the potential search space of architectures for a sub-network is increased.
Figure 3: (a,b): Effect of Î» on the performance of sub-networks. (c,d): Effect of layer-wise scalingon the performance of sub-network. All experiments are done on CIFAR-10.
Figure 4: Layer-wise preserved rate with or without layer-wise scaling (LWS) for ResNet-56 andMobileNetV2 on CIFAR-10.
Figure 5: (a,b): Effect of different scheme for the inputs of HSN ai . (c,d): Effect of different settingsof HSN. (e,f): Effect of different learning rates with LWS. (g,h): Effect of different optimizer on withLWS. For plots in (c,d,g,h), shaded areas represents variance from 5 trials.
Figure 6: (a,b): visualization of pruned architectures for ResNet-50 and MobileNetV2. (c,d): meanand variance of 20 generated sub-networks for pruning.
Figure 7: (a,b): Performance of sub-networks when using HSN or not using HSN (the setting inEq. 11). (c,d): Regularization loss for the same settings.
Figure 8: (a,b): Performance of sub-networks when training HSN given forward (c=0) and backwardpruning (c=3). (c,d): Regularization loss of sub-networks when training HSN given forward (c=0)and backward pruning (c=3). All experiments are done on CIFAR-10.
