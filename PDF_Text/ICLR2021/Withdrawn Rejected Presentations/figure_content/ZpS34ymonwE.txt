Figure 1: Illustration of a digital universal patch attackagainst an undefended model (left) and a model defendedwith meta adversarial training (MAT, right) on Bosch SmallTraffic Lights (Behrendt & Novak, 2017). A patch can leadthe undefended model to detect non-existent traffic lights andmiss real ones that would be detected without the patch (bot-tom left). In contrast, the same patch is ineffective againsta MAT model (bottom right). Moreover, a patch optimizedfor the MAT model (top right), which bears a resemblanceto traffic lights, does not cause the model to remove correctdetections.
Figure 2: Comparison of MAT modelsâ€™ learning curves against the transfer attack for generatinguniversal patches. (Left) meta-perturbation initialization in MAT between data initialization andrandom initialization. (Middle) MAT using targeted attacks and untargeted attacks for updatingmeta-perturbations. (Right) MAT uses the worst meta-perturbation chosen from different numbers ofsamples F.
Figure 3: Ablation study of MAT in the aspects where it differs from UAT (Shafahi et al., 2018).
Figure 4: Robustness against universal patch attacks: results correspond to Table 1 but show distribu-tion of accuracy over elements of the grid search rather than only the worst accuracy. Each value isaveraged across 5 runs (with 5 different seeds) for each configuration per training approach. MATris the MAT model trained with randomly initialized meta-perturbations, whereas MAT representsthe model trained with meta-perturbations through data initialization. The rows correspond to threedifferent universal patch attacks.
Figure 5: Comparison of learning curves of Standard training, AT, UAT, and MAT against the transferattack for generating universal perturbations.
Figure 6: Robustness against universal perturbation attacks: results correspond to Table 8 but showdistribution of accuracy over elements of the grid search rather than only the worst accuracy. Eachvalue is an average across 5 runs (with 5 different seeds) for each configuration per training approach.
Figure 7: Recall (left) and mean Average Precision (right) by model against universal patch attacks onBosch Small Traffic Lights Dataset. MAT (default): meta perturbation is initialized uniform-randomly.
Figure 8: Best random initialization, data initialization, and low-frequency attacks against therespective models from Table 1. The model prediction is given above each plot. The correct label ischimpanzee.
Figure 9: Best (in terms of mAP) random initialization, data-crop initialization and low-frequencyattacks against the models from Table 2. The patch location is fixed for a better comparison.
Figure 10: Training of patches against a MAT (Data Init) model. Attack from data-crop initializationwith step size of 0.01 (top). Attack from random initialization with step size of 0.1 (center). Low-frequency attack with cutoff frequency 100 from data-crop initialization with step size of 0.001.
Figure 11: Training of patches against standard model: Attack from data-crop initialization with stepsize of 0.01 (top). Attack from random initialization with step size of 0.01 (center). Low-frequencyattack with cutoff frequency 100 from data-crop initialization with step size of 0.001Low-Frequency Cutoff: 100, Learning Rate: 0.001Figure 12: Training of patch from Figure 1. This patch is generated with a low-frequency attack withcut-off frequency 100, learning rate of 0.001 and starting from data-crop initialization.
Figure 12: Training of patch from Figure 1. This patch is generated with a low-frequency attack withcut-off frequency 100, learning rate of 0.001 and starting from data-crop initialization.
Figure 13: Best random initialization, data initialization, and low-frequency perturbation attacksagainst the respective models from Table 8. The model prediction is given above each plot. Thecorrect label is chimpanzee.
