Figure 2: We use thickness of each layer to indicate its relative number of channels. The last layerindicates the output of the block. The dashed layer represents the ”bottleneck” of the block.
Figure 3: (1)-(2) are the experiment results for Theorem 1 with independent and dependent inputs,respectively. (3) is the experiment results for Tucker CNN in Theorem 2.
Figure 4: (1) 41-layer Residual Network for CIFAR-10&SVNH. (2) 30-layer 3D Residual Networkfor UCF101. Here, t = K/R represents the expansion ratio, and takes values of 1,4,8,16.
Figure 5: Formulating the process with an input vector x. Note that we combine the convo-lution, pooling and fully connected layers into the composite weight vector wX, where wX =q-1 pq=ι Ui(I)a + + q-1 Pq=m-q+ι Ui(I)a. The ~ represents the convolution operation.
Figure 6: Splitting matrix T(∆) based on its singular value decomposition.
Figure 7: Additional experiments for efficient number of kernels.
Figure 8: Equivalent ResNext building blocks. Both (1) and (2) represent a block of ResNeXt withcardinality g, With R = r ∙ g. Here, the expansion ratio t = K/R, and takes values of 1,2 and 4.
Figure 9: A basic block of Shufflenet v2, with its bottleneck structure depict explicitly. Here, theexpansion ratio t = K/R, and takes values of 1,2 and 4.
