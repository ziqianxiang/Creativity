Figure 1: Accuracy vs number of parameters (log scale) for compressing standard architec-ture (pretrained models) on few datasets. Comparative results of our approach (PSM), thehybrid SVD-Tucker (Tucker-SVD), the TT method (TT), the Hard (Hard Pruning) anditerative (Iterative Pruning) pruning techniques. Details on the hyperparameters for eachtested method are included in the name of the methods. Base stands for the uncompressedpretrained model.
Figure 2: Relative error approximation by layers using Palm4MSA for VGG19 architecture.
Figure 3: Illustration of the proposed sparse factorization method.
Figure 4: Description of the reshape operations and locally linear transformations computedin the convolutional layers. The grey box represent the receptive field of the convolutionfilters at the black dot coordinate. The white squares in the second step correspond to thezero padding of the input.
