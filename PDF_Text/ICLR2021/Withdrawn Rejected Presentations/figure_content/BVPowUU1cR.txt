Figure 1: FID scores throughout training for the WGAN-GP objective on MNIST, estimated using60 000 samples from the generator. We plot up to a maximum of 40 000 iterations. When plottingagainst time (right), this means some lines end before the two hours we show. The blue line showsthe results with AdvAs, while the others are baselines with different values of nadv .
Figure 2: FID scores on CIFAR10 using Auto-GAN from baselines and AdvAs plotted with alog y-axis against running time for different val-ues of nadv . We see that AdvAs with nadv = 2yields the lowest FID scores at every point duringtraining.
Figure 3: Bottom: FID scores throughout training estimated with 1000 samples, plotted againstnumber of epochs (left) and training time (right). FID scores for AdvAs decrease more on eachiteration at the start of training and converge to be 7.5% lower. Top: The left two columns showuncurated samples with and without AdvAs after 2 epochs. The rightmost two columns show uncu-rated samples from networks at the end of training. In each grid of images, each row is generated bya network with a different training seed and shows 3 images generated by passing a different randomvector through this network. AdvAs leads to obvious qualitative improvement early in training.
