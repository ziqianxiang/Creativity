Figure 1: (A) Examples of general object images. These include two categories (car and chair) eachwith two shape variations. In addition, the object of each shape is shown in three different views.
Figure 2: A diagrammatic outline of CIGMO learning algorithm. The structure of the entire work-flow consists of C modules (light-gray boxes) of group-based VAE corresponding to C categoriesand a classifier network (right-most). An input is a group of data instances xk (bottom). Given suchinput, each module c estimates an instance-specific view yk from each data instance xk using theencoders gc/rc . The module also estimates a group-common shape z by first estimating individualshapes for instances xk using the encoders hc/sc and then by taking their average. Then, new datainstances are generated by the decoder fc from the views and shape, and compared with the originalinput data for obtaining the reconstruction error (loss). This process is repeated for all modules. Inparallel, the posterior probability for category c is computed by the classifier u on each data instancexk and then by averaging over the instances. Each computed probability is multiplied with the re-construction error for the corresponding module. Other probabilistic mechanisms (e.g., priors) areomitted here for brevity.
Figure 3: Examples of invariant clustering from (A) a CIGMO, (B) a mixture of VAEs, (C) a GVAEwith k-means, and (D) an IIC, in the case of 3 categories. Random 24 images belonging to eachestimated category are shown in a box. Note that the categories quite precisely correspond to the car,chair, and table image classes in (A), whereas such correspondence is less clear in the other models(B-D; in particular, cars are mixed with many other objects).
Figure 4: Results from a CIGMO model trained for MultiPie dataset. (A) Invariant clustering.
Figure 5: Examples of feature manipulation tasks from a 3-category CIGMO model for ShapeNetdataset. (A) Swapping. For each category, we generate a matrix of images from two lists of sampleimages, where each generated image has the view of an image in the left list and the shape of animage in the top list. (B) Interpolation. For each category, we generate a matrix of images fromtwo sample images (corresponding to the top-left and bottom-right images), where each generatedimage has the view and shape that linearly interpolates those of the two images. (C) Random gener-ation. For each category, we generate images from shapes and views that are drawn from Gaussiandistributions.
