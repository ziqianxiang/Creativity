Figure 1: Examples of: (a) neglecting one vector due to magnitude difference; (b) partial cancella-tion of two vectors with equal magnitude; (c) a hard parameter sharing architecture.
Figure 2: Extended hard parameter sharing model with task-specific representations.
Figure 3: Figures (a) and (b) show the trajectories followed when using Rotograd in the spacesZ1 (blue) and Z2 (brown), versus not using it (red). Circular markers mean the final point of thetrajectory, whereas the triangle and star mark the global optimum of each task. Figure (c) shows thedeep logistic regression classifier learned using Rotograd to predict a binary label and its (Correct)and its negated (Wrong).
Figure 4: Tensorboard plots with the results of one run on the MNIST experiments during trainingfor uniform, gradnorm W, gradnorm Z, rotograd-sgd, and rotograd. Figure (a) shows the validationmetrics, where rotograd outperforms in the regression tasks. Figure (b) shows the (median) cosinesimilarity between the task gradient Nz'k (Z) and the averaged gradient Nz'(z), Clearly showinghow rotograd reduces the direction conflict among task gradients.
