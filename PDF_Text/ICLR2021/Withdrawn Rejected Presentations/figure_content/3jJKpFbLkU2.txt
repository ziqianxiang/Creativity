Figure 1: CNML probabilitieswith a logistic regression model.
Figure 2: Given the labeled training set (blue and orange dots), we want to predict the label at the queryinput (shown in pink in the left image), which the training set MLE θtrain confidently classifies as the blueclass. However, CNML assigns a near-uniform prediction on the query point, as it computes new MLEs /and θθι (center and right images) by assigning different labels to the query point, and finds both labels areconsistent with the training data.
Figure 3: CNMAP probability heatmaps with different levels of L2 regularization λ kw k22 . We seepredictions are less conservative as regularization increases.
Figure 4: Reliability diagrams plotting confidence vs. accuracy for VGG16 on in-distribution and out-of-distribution data. ACNML provides more conservative predictions than other methods, resulting inbetter calibration on out-of-distribution inputs. For the OOD task, we show results for the Gaussian blurcorruptions at levels 3 and 5, with level 5 corresponding to a higher amount of corruption. Each pointshows the mean confidence and mean accuracy within a bucket, so the spread of points along the x-axisshows that ACNML makes more low confidence predictions than other methods.
Figure 5: ACNML compared against their Bayesian counterparts and the deterministic MAP baseline onout-of-distribution CIFAR10-Corrupted datsets. We plot medians and 95% confidence intervals acrossall corruptions. We see that ACNML methods (solid lines) achieve much lower ECE at higher corruptionvalues, and ACNML with SWAGD also achieves better NLL than other methods.
Figure 6: CIFAR10-C performance with the VGG16 architecture. Instantations of our methods are shownin stripes. Boxplots show quartiles of each statistic over all different corruption types of the given intensity,with the mean indicated by a circle. The accuracy (a) and NLL (c) for most methods are similar, but bothACNML variants attain significantly better ECE (b) on the more severe corruptions, as the images movefurther out of distribution.
Figure 7: CIFAR10-C performance with the VGG16Drop architecture. Instantations of our methods areshown in stripes. Boxplots show quartiles of each statistic over all different corruption types of the givenintensity, with the mean indicated by a circle. Again, the accuracy (a) and NLL (c) for most methods aresimilar, but both ACNML variants attain significantly better ECE (b) on the more severe corruptions, asthe images move further out of distribution.
Figure 8: CIFAR10-C performance with the WideResNet28x10 architecture. Instantations of our methodsare shown in stripes. Boxplots show quartiles of each statistic over all different corruption types of the givenintensity, with the mean indicated by a circle. Again, we see that ACNML attains better ECE values thancomparable methods on the heavier corruptions (b). Note that the best performing prior method, SWAG,uses a substantially more expressive posterior than the diagonal approximation used by SWAGD+ACNML,whereas the comparable SWAGD method attains worse ECE.
Figure 9: In Distribution Comparisons between ACNML and naive CNML. We plot scatter plots ofthe values of each statistic for naive CNML (x-axis) vs ACNML (y-axis), with the red line indicatingLooking at the CNML normalizers, we see that the ACNML adaptation procedure using the approximateposterior is much less constraining than using the training set, resulting in the normalizers being higher forACNML than naive CNML for almost all inputs. This leads to excess conservatism, with ACNML almostalways having lower confidence its predictions, and many inputs with close to 0 NLL with naive CNMLhaving higher NLL with ACNML.
Figure 10: OOD Comparisons between ACNML and naive CNML. We plot scatter plots of the valuesof each statistic for naive CNML (x-axis) vs ACNML (y-axis). Looking at the CNML normalizers, weagain see that the ACNML adaptation procedure using the approximate posterior is less constraining thanusing the training set, with the normalizers being higher for ACNML than naive CNML for most inputs(though to lesser extent than the in-distribution data). ACNML again outputs more conservative predictionswith lower confidence on many inputs, which leads to better NLL and calibration on the OOD dataset,unlike with the in-distribution test set.
Figure 11: Reliability diagrams plotting confidence vs. accuracy for Bayes-by-Backprop experiments onthe MNIST test set and the randomly rotated MNIST test set (OOD). ACNML’s conservative predictionsprovided better calibrated predictions on the OOD test set.
