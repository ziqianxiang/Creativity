Figure 1: Left. Plot of average return v.s. training epochs ofour proposed method (red) v.s. baseline (brown) (Kumar et al.,2019) on the relocate-expert offline dataset. Right. Correspond-ing plot of Q-Target values v.s. training epochs. Our proposedmethod achieves much higher average return, with better train-ing stability, and more controlled Q-values.
Figure 2: Expert Trajectory Visualization. 2D heat maps of the expert,s action distribution with respect tohorizontal/VertiCaI displacement from the goal location. Warmer locations represent more observations.
Figure 3: Left. The training set with horizontal displacements (< 0.1) removed. This makes all states onthe left OOD. Right. Our model estimates higher uncertainty (brighter color) on the left and lower uncertainty(colder color) on the right.
Figure 4: Our learned policies successfully accomplishes manipulation tasks, SUCh as opening a door as shown.
Figure 5:	Plot of average return v.s. training epochs, together With the corresponding average Q Target overtraining epochs. Results are averaged across 5 random seeds. Left: Results of different types (human, cloned,expert) on the Adroit pen task. Right: Results on human demos on the 3 remaining tasks. The performance ofbaseline (BEAR) degrades over time (also noted in (Kumar et al., 2019)), and the Target Q value explodes.
Figure 6:	Left. The training dataset has observations with vertical displacements > 0.8 removed. This makesall states on the top OOD states. Right. Our model estimates higher uncertainty (brighter color) on the top andlower uncertainty (colder color) on the bottom.
Figure 7: Plot of average return v.s. training epochs, together With the corresponding average Q Target overtraining epochs on the D4RL Adroit hand offline data set. Results are averaged across 5 random seeds. Notethat the performance of baseline (BEAR) degrades over time (also noted in original paper Kumar et al. (2019)),and the Target Q value explodes. Our method, UWAC, achieves significantly better overall performance andtraining stability.
Figure 8: Plot of average return v.s. training epochs (zoomed-in). The figure is the same as 7, except that thesecond column is zoomed-in on the Q values of the UWAC critic.
Figure 9: Ablation: Plot of average return v.s. training epochs for BEAR v.s. BEAR+Spectral Norm, togetherWith the corresponding average Q Target over training epochs on the D4RL Adroit hand offline data set. Resultsare averaged across 5 random seeds. Although BEAR with Spectral Normalized Q function maintains stable Qestimate during training, BEAR with SN achieves significantly worse training performance in terms of averagereturn.
Figure 10: Ablation: Plot of average return v.s. training epochs for BEAR+Spectral Norm v.s.
Figure 11: Ablation: Plot of average return v.s. training epochs for UWAC (ours) v.s. ours without uncertaintyweighing but With dropout in the Q function, together With the corresponding average Q Target over trainingepochs on the D4RL Adroit hand offline data set. The results are averaged across 5 random seeds. Withoutthe weighing loss, performance of the agent drops drastically. Note that low performance on hammer-cloned,door-cloned, and relocated-cloned may be attributed to the bad quality of the datasets caused by data collection(explained in section 5.3)20Under review as a conference paper at ICLR 20211250>1000I 750f 500tυd 250o0Average ReturnoursSN+Dropout—SN
Figure 12: Ablation: Figure 9, 10, 11 plotted together.
Figure 13: Sequences of our offline agent trained from expert demonstrations executing learned poli-cies performing on the halfcheetah, walker2d, and hopper tasks in the MuJuCo Gym environment.
Figure 14: Sequences of the agent trained from human demonstrations executing learned policiesperforming the Adroit tasks of hammering a nail, twirling a pen and picking/moving a ball. The taskof opening a door is shown in Figure 4. See the videos attached in the supplementary.
