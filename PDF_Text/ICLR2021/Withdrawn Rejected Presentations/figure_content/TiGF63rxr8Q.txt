Figure 1: Performance for = 0.8 (left), = 0 (middle), and at N = 2000 with varying (right).
Figure 2: Scatter plots of the maximum absolutedeviation from Equal CRP vs. the change in re-wards every 50 steps (left) and the max. norm ofthe gradient for the minibatch (right).
Figure 3: Mean performance gain over Equal CRPof the learned policies when tested on 10 tasks us-ing out-of-sample instruments. Error bars denotethe standard deviation over 10 experiments.
Figure 4: The two left plots show mean annualized return in the testing period over 10 experiments(different instruments) each with 5 and 30 tasks. X-axes are scaled to make the curves comparable:each epoch has 1500 (5-tasks) and 9000 steps (30-tasks) and an evaluation. Shaded regions denotethe interquartile range. The rightmost figure shows, for each fraction of tasks, the gain over SingleTask Learning (STL). A curve further to the right shows higher gain over STL. From 30% on they-axis, the P-MTL gain is higher (more towards the right) than the MTL gain. As expected, when fewtasks are used, prioritizing tasks doesnâ€™t help much (y-axis from 0 to 0.2).
Figure 5: Comparison of a multi-task policy vs. a single-task policy on the testing period for a specifictask. The leftmost plot shows the percentage gain in portfolio value over time for both policies againstthat from the baseline Equal CRP policy. The right two plots show the asset allocations.
Figure 6: These plots compare the behavior of a multi-task policy and a single-task policy duringtesting. FedAvg denotes the accuracy of federated learning with uniform averaging. The left plotshows the accuracy of the aggregate model during federated learning. The right two plots show theweights produced by the policy for different clients. Note that clients 8 and 9 possess 40% of theunique labels.
