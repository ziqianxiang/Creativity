Figure 1: A graph neural network architecture design. The tree-pipeline successively includes globalwidth contraction, weight evolution, and linkâ€™s weight rewiring. In the initial dynamics, informationtransfer is from front to back. In supervised learning, information transfer is in the opposite directionsince it is subject to specific task constraints imposed by outputs, accompanied by declining transfercapacity in backpropagation caused by numerous local minimums or saddle points. To accelerate theinformational transfer, the global architecture should have a pyramid-like shrinkage shape to preventthe saddle points caused by over-parameterized settings. After the weight evolution forming themodularity in topological structure, one can use the topological structure as redundant informationto rewire possible erroneous topological links.
Figure 2: Performance analysis of the proposed framework. (Above) the observed shrinking property ofnetwork width after automatical pruning. (Bottom) the impact of learning rate on convergence.
Figure 3: Evolutionary dynamics in training process. Each figure contains the 5 prominent eigen-values of hidden states.
Figure 4: The effect of coupling coefficient on convergence.
