Figure 1: The possible model designs for visual MBRL, based on whether or not to predict images.
Figure 2: The performance of different models in the online setting. The x-axis is the number ofenvironment steps and the y-axis is the achieved score by the agent in an evaluation episode. Eachcurve represents the average score across three independent runs while the shadow is the standarddeviation. The model designs are from Figure 1. From this graph, it can be clearly seen that themodels which predict images can perform substantially better than the model which only predictsthe expected reward (R). In some cases, these models perform even better than the Oracle whichsuggests that there is not a clear relation between prediction accuracy and performance, given the factthat the Oracle is the perfect predictor. The curves are smoothed by averaging in a moving window ofsize ten.
Figure 3: This figure illustrates the distribution of the rewards in trajectories collected by bestperforming models and R in the online setting for Cheetah_run. Since the rewards for each frameis [0, 1] and the action repeat is set to four, the observed reward is always in [0, 4] (x-axis). Thisgraph demonstrates how LTLR and OTOR managed to explore a different subset of state space withhigher rewards (compared to R) which directly affected their performance (compare at Figure 2).
Figure 4: The effect of limiting the image prediction capacity on the performance of the agent. Thegraphs follow the same format as Figure 2. Each model is a variant of LT LR which predicts theentire image, or part of it (center cropped) or no image at all. This graph shows that predicting morepixels results in higher performance as well as more stable training and better sample efficiency.
Figure 5: The environments from DeepMind Control Suite (Tassa et al., 2018) used in our experiments.
Figure 6: The effect of limiting the observation prediction capacity on the performance of the agent.
Figure 7: Effect of sharing the learned latent space. The graphs follow the same format as Figure 2except the y-axis is the difference between achieved scores by each model with and without sharingthe learned latent space. In other words, a Positive delta Score on y-axis means that not Sharing thelearned 山tent SPaCe is improving the results for each PartiCUlar model. This figure demonstrateshow sharing the learned latent space can degrade the performance, particularly for models withobservation space dynamics.
Figure 8: Comparison of training and evaluation accuracy of models with and without imageprediction. Each graph demonstrates the heat map of the predicted vs. ground-truth rewardfor Cheetah_run. The training and evaluation dataset is fixed for all of the models with fixtrain/evaluation partitions. Since the rewards for each frame is [0, 1] and the action repeat is set tofour, the observed reward is always in [0, 4]. The green dash line is the perfect prediction. This figuredemonstrates better task performance for more accurate models when compared with Table 1.
Figure 9: Comparison of training and evaluation accuracy of models with and without imageprediction. Each graph demonstrates the heat map of the predicted vs. ground-truth reward forcheetahrun. The training and evaluation dataset is fixed for all of the models and designedspecifically to make sure that there are unseen high reward states in the evaluation (separated bythe dotted black line). Since the rewards for each frame is [0, 1] and the action repeat is set to four,the observed reward is always in [0, 4]. The green dash line is the perfect prediction. This figureillustrate better generalization to unseen data for models which predict images. It also demonstratesthe entanglement of exploration, task performance and prediction accuracy, when comparison withFigure 2 and Table 1.
Figure 10:	Comparison between different types of reward predictor models described in Section B.
Figure 11:	Correlation between accuracy of observation prediction and task performance, in the onlinesetting. For each graph, we scaled down the indicated model (left: OT OR, right: LT LR) by multiplelevels and report their median observation prediction error during training (x-axis) and medianachieved score (y-axis). The annotation on each data point is the scale multiplier: higher means abigger model (the detailed numbers can be found in appendix.) This graph clearly demonstrates thestrong correlation between observation prediction accuracy and asymptotic performance.
Figure 12: Architecture of SV2P. At training time, the inference network (top) estimates the posteriorqφ(z|x0:T) = N(μ(x0τ), c(x0:T)). The latent value Z 〜 qφ(z|x0:T) is passed to the generativenetwork along with the (optional) action. The generative network (from Finn et al. (2016a)) predictsthe next frame given the previous frames, latent values, and actions. At test time, z is sampled fromthe assumed prior N(0, I).
Figure 13: Architecture of PlaNet (Hafner et al., 2018). Learned Latent Dynamics Model: In a latentdynamics model, the information of the input images is integrated into the hidden states (green) usingthe encoder network (grey trapezoids). The hidden state is then projected forward in time to predictfuture images (blue trapezoids) and rewards (blue rectangle).
Figure 14: This figure illustrates the distribution of the rewards in trajectories collected by modelvariations in the online setting. Since the rewards for each frame is [0, 1] and the action repeat is setto four, the observed reward is always in [0, 4] (x-axis).
