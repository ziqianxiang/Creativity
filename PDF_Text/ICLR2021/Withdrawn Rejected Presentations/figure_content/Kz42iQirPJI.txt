Figure 1: Meta-Iearning over sequential domains. Data in each domain arrive sequentially. Ourmodel consists of a domain-shared part and a domain-specific part, all consist of a few convolutionallayers (and possibly fully connected layers). The domain-shared part is shared by all domains, andeach domain only owns one sub-network in the domain-specific part. Parameters (e.g., convolutionalfilters) in each domain-shared convolutional layer i (blue) are divided into n blocks, denoted asBio, Bi1,…，Bin. Each block is associated with one learnable learning rate when meta trainingeach domain on the network. The learning rates are updated by a loss defined on the memory tasksto enforce the memorization of previous domains.
Figure 2: The meta memory lossmechanism. For one task in do-main q from the memory pool ,we randomly sample the data fromsome tasks of other domains andcombine them with their own datato calculate the task-specific loss.
