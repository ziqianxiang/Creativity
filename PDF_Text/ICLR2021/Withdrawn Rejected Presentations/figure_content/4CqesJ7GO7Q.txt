Figure 1: Class-wise robustness at different epochs in test setImageNet, since the model uses the three-stage training method (Wong et al., 2019), its class-wiserobustness gap increases with the training epoch, and finally up to 80% (Figure 1(f)). Even for thesimplest dataset MNIST, on which model has achieved more than 95% overall robustness, the largestclass-wise robustness gap still has 6% (Figure 1(a)). Motivated by the above discovery, we naturallyraise the following three questions to better investigate the class-wise properties in the robust model:1)	Is there any relations among these different classes as they perform differently?2)	Are there any factors related to the above phenomenon?3)	Is the class-wise performance related to the strength of the attack?We conduct extensive analysis on the obtained robust models and gain the following insights:â€¢	Many examples from a certain class could only be maliciously flipped to some specificclasses. As long as we remove those specific classes and re-train the model, these exampleswill not exist adversarial counterparts in bounded -ball.
Figure 2: Confusion matrix of robustness in test setResults Analysis. An the confusion matrices in Figure 2 roughly demonstrate one kind of symme-try, indicating that similar classes could easily be maliciously flipped to each other. Specifically, forSVHN, digits with similar shapes are more likely to be flipped to each other, e.g., the number 6 andnumber 8 are similar in shape and the non-robustness between them (number 6 is misclassified to benumber 8 or vice versa) is very high as shown in Figure 2(d). For CIFAR-10 and STL-10, Figures2(b) and 2(e) clearly show that the classes belonging to the same superclass1 have high probabilityto be craftily misclassified to each other, for example, both class 3 (cat) and class 5 (dog) in CIFAR-10 belong to the superclass Animals, the non-robustness between them is very high in Figure 2(b).
Figure 3:	Misclassified and Homing Confusion matrix of CIFAR-10Results Analysis. Figure 3 clearly shows homing property is widely observed in many misclassifiedexamples. For example, we can focus on the 9th row and the 1st column of Figure 3(a) and 3(b).
Figure 4:	The correlation between the robustness of each class and its norm of classifier weightResults Analysis. From Figure 4, we can find that for most classes, their robustness is positivelycorrelated with their norm of classifier weight, i.e., higher (lower) robustness corresponds to a higher(lower) norm of classifier weight. For example, the robustness of class reduced as decreasing of thenorm of classifier weight across classes in CIFAR-100 as shown in 4(c). We also check this kind ofcorrelation in standard training, but the experimental results show no significant correlation betweenthe accuracy of each class and its corresponding norm of classifier weight in standard training. Themain reason might be that these datasets in standard training are sufficient for most classes to be welltrained, while adversarial training always requires abundant data for training (Schmidt et al. (2018)),hence the insufficient adversarial data cannot guarantee the classifier is well trained and lead to theabove experimental observation results.
Figure 5: Analysis of class-wise predicted probability distribution: (a) Class-wise variance ofpredicted probability of Madry, TRADES and MART. (b)-(d) The output probability distribution(Madry: 1/T=1 and MART: 1/T=1) change of image 127 (ground truth class 3) in the process ofgenerating adversarial examples (iteration step 1, 10 and 20). (e) Class-wise variance of predictedprobability of HE, HE-LIS and HE-CMP. (f)-(h) The output probability distribution (HE: 1/T=1 andHE: 1/T=10) change of image 46 (ground truth class 3) in the process of generating adversarialexamples (iteration step 1, 10 and 20).
Figure 6: Number of classes per robustness intervalDue to the large number of classes in CIFAR-100 and ImageNet, we randomly sample 12 classesfor analysis in the above paper. For the sake of experimental completeness, the number of classesin different robustness intervals is shown in Figure 6. Obviously, the robustness of the classes isdistributed at multiple intervals, which is consistent with the results shown in Figure 1.
Figure 7: Class-wise accuracy in standard training and class-wise robustness in adversarial trainingIn the above paper, we mainly focus on the robustness gap in the model obtained by adversarialtraining. Since Wang et al. (2019a) also report the emergence of unbalanced accuracy in the standardmodel, we compare this phenomenon with that in the robust model to highlight the differencesbetween adversarial training and standard training.
Figure 8: t-SNE visualization of CIFAR-10In the evaluation phase, we show the t-SNE (Maaten & Hinton, 2008) visualization of CIFAR-10 tofurther demonstrate the property of the two superclasses Transportation and Animals in this dataset.
Figure 9: The relation between the probability and gradient for an adversarial example(f) Gradient (1/T < 1)model can update more gradients based on this example. Similarly, when 1/T < 1 (Figures 9(c)and 9(f)), this example becomes simpler for the model and obtains a smaller gradient. Overall, thesefigures clearly show that we can adjust the gradient through the temperature factor 1/T .
