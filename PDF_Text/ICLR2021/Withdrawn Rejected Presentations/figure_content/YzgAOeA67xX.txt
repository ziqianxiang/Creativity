Figure 1: We compared Equation-1-based weightdecay and Equation-2-based weight decay bytraining ResNet18 on CIFAR-10 via vanilla SGD.
Figure 2: We compare the generalization ofVanilla SGD, SGD, SGDW, and SGDS under var-ious weight decay hyperparameters by trainingVGG16 on CIFAR-10. The optimal performanceof SGDS/SGDW is better than the widely usedVanilla SGD and SGD. For Vanilla SGD, SGD,and SGDS, we may choose λL2 = λS = 0.0005to maintain the optimal performance. But we haveto re-tune λW = 0.005 for SGDW. Hyperparame-ter Setting: β1 = 0 for Vanilla SGD; β1 = 0.9 forSGD, SGDW, and SGDS. We repeat each simula-tion for three runs. A similar experimental resultfor ResNet18 is presented in Appendix C.
Figure 3: The test errors of ResNet18 on CIFAR-10. SGDS has a deeper blue basin near dark points(≤ 4.83%). The optimal choices of η and λ are very close for SGDS and SGD.
Figure 4: The learning curves of AdamS, AdamW, and Adam on CIFAR-10 and CIFAR-100. TopRow: Test curves. Bottom Row: Training curves. AdamS shows significantly better generalizationthan AdamW and Adam.
Figure 5: The scatter plot oftraining losses and test errorsduring final 40 epochs of train-ing ResNet34 on CIFAR-100.
Figure 6: The learning curvesof all adaptive gradient methodsby training ResNet34 on CIFAR-100. AdamS outperforms otherAdam variants. The test perfor-mance of other models can befound in Table 1.
Figure 7: The test errors ofVGG16 on CIFAR-10 with var-ious weight decay rates. Thedisplayed weight decay valueof AdamW has been rescaledby the bias correction factor ≈0.001. A similar experimentalresult for ResNet34 is presentedin Appendix D.
Figure 8:	Language Modeling. The learning curves of AdamS, AdamW, and Adam for LSTM onPenn TreeBank. AdamS has better test performance than AdamW and Adam for LSTM. The optimaltest perplexity of AdamS, AdamW, and Adam are 69.90, 72.88, and 77.01, respectively. Note thatthe lower perplexity is better.
Figure 9:	The test errors of ResNet18 on CIFAR-10. AdamS has a much deeper and wider basin neardark points (≤ 4.9%). The optimal test error of AdamS, AdamW, and Adam are 4.52%, 4.90%, and5.49%, respectively. The displayed weight decay value of AdamW has been rescaled by the biascorrection factor ≈ 0.001.
Figure 10: We compare the generalization of Vanilla SGD, SGD, SGDW, and SGDS with variousweight decay hyperparameters by training ResNet18 on CIFAR-10. The optimal weight decay ratesare near 0.0005 for all three weight implementations. The optimal performance of SGDS/SGDWis better than Vanilla SGD and SGD. For Vanilla SGD, SGD, and SGDS, we may safely chooseλL2 = λS = 0.0005. But we have to re-tune λW = 0.005 for SGDW. Hyperparameter Setting:β1 = 0 for Vanilla SGD; β1 = 0.9 for SGD, SGDW, and SGDS.
Figure 11:	Generalization analysis on SGDS and SGD with L2 regularization. HyperparameterSetting: λS = λL2 = 0.0005 and β1 = 0.9.
Figure 12:	Generalization analysis on AdaiS and Adai with L2 regularization. HyperparameterSetting: λS = λL2 = 0.0005.
Figure 13: The learning curves of adaptive gradient methods.
Figure 14:	Even if with similar or higher training losses, AdamS still generalizes better than AdamWand Adam. The scatter plot of training losses and test errors during final 50 epochs of training VGG16on CIFAR-10 and DenseNet121 on CIFAR-100.
Figure 15:	We compare the generalization of Adam, AdamW, and AdamS with various weight decayrates by training ResNet34 on CIFAR-100. The displayed weight decay of AdamW in the figurehas been rescaled by the bias correction factor ≈ 0.001. The optimal test performance of AdamS issignificantly better than AdamW and Adam.
Figure 16:	We train ResNet18 on CIFAR-10 for 900 epochs to explore the performance limit ofAdamS, AdamW, and Adam. The learning rate is divided by 10 at the epoch of 300 and 600. AdamSachieves the most optimal test error, 4.70%.
Figure 17: The learning curves of ResNet18 and VGG16 on CIFAR-10 with cosine annealing andwarm restart schedulers. The weight decay hyperparameter: λL2 = λS = 0.0005 and λW = 0.5.
Figure 18: The test errors of ResNet18 and VGG16 on CIFAR-10 under various weight decaywith cosine annealing and warm restart schedulers. AdamS yields significantly better optimal testperformance than AdamW and Adam.
