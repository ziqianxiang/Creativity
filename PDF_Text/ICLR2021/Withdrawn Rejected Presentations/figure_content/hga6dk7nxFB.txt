Figure 1: Triplet loss, designed to fine-tune the original pre-trained embeddings produced by theBERT model to a downstream task. During training, a triplet of proteins our organised by calculatingthe Euclidean distance between each example. Once completed, proteins with similar propertyvalues (i.e. the values in the circles) should cluster closer together, producing a more meaningfulembedding space.
Figure 2: Overview of the Triplet-BERT Approach.
Figure 3: A set of t-SNE and cluster plots for both versions of the BERT model, thereby visualisingthe correlations within each learned embedding space (e.g. the number of modifications present andthe functional property of each protein) (see text for details).
Figure 4: Average Attentions within the BERT model. The least and most absorbent mutated ver-sions of the parent protein are coloured blue and red respectfully.
Figure 5:	A set of attention maps highlighting the importance of each amino-acid with three pro-teins from the peak absorption task. The parent protein is outlined in green, and its least and mostabsorbent versions are outlined in blue and red respectfully. Any modifications are represented byred lettering.
Figure 6:	Average Attentions within the BERT model. The least and most absorbent mutated ver-sions of the parent protein are coloured blue and red respectfully.
Figure 7: Average Attentions within the BERT model. The least and most absorbent mutated ver-sions of the parent protein are coloured blue and red respectfully.
Figure 8: The absolute difference in attention between a protein and its parent for each head of thefinal layer. The modification are highlighted by the white crosses.
