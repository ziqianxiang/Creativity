Figure 1: Illustration of DM in terms of optimisation and example weighting.
Figure 2: We train ResNet-56 on CIFAR-10. In (a) and (b), we observe noisy examples havemuch less Pi than clean ones, thus being more difficult examples in both CCE and DM. In (c), foreach label noise rate, we show the optimised ψDM from {0, 1/3, 1/2, 2/3}, i.e., λ ∈ {0, 1/2, 1, 2}.
Figure 3: Diverse semantically abnormal training examples highlighted by red boxes. The 1st rowshows synthetic abnormal examples from corrupted CIFAR-10 (Krizhevsky, 2009). The 2nd and3rd rows present realistic abnormal examples from video person re-identification benchmark MARS(Zheng et al., 2016).
Figure 4: A designed neural network with one 8-neuron hidden layer for sentiment analysis onIMDB. It is quite simple and our objective is not to represent the state-of-the-art.
Figure 5: The learning dynamics of ResNet-56 on CIFAR-10, i.e., training and testing accuraciesalong with training iterations. The legend in the top left is shared by all subfigures. ‘xxx: yyy’ means‘method: emphasis mode’. We have two key observations: 1) When noise rate increases, bettergeneralisation is obtained with higher emphasis mode, i.e., focusing on relatively easier examples;2) Both overfitting and underfitting lead to bad generalisation. For example, ‘CCE: 0’ fits trainingdata much better than the others while ‘DM: None’ generally fits it unstably or a lot worse. Betterviewed in colour.
Figure 6: ResNet-56 on CIFAR-10 (r = 20%). From left to right, the results of four emphasis modes0, 3, 0.5, 3 with different emphasis variances are displayed in each column respectively. When λ islarger, β should be larger. Specifically :1) when	λ	= 0:	we tried	β	= 0.5, 1, 2, 4;2) when	λ	= 0.5: we tried	β = 4, 8, 12, 16;3) when	λ	= 1:	we tried	β	= 8, 12, 16, 20;4) when	λ	= 2:	we tried	β	= 12, 16, 20, 24.
Figure 7: ResNet-56 on CIFAR-10 (r = 40%). From left to right, the results of four emphasis modes0, 3, 0.5, 3 with different emphasis variances are displayed in each column respectively. When λ islarger, β should be larger. Specifically :1) when	λ	= 0:	we tried	β	= 0.5, 1, 2, 4;2) when	λ	= 0.5: we tried	β = 4, 8, 12, 16;3) when	λ	= 1:	we tried	β	= 8, 12, 16, 20;4) when	λ	= 2:	we tried	β	= 12, 16, 20, 24.
Figure 8: ResNet-56 on CIFAR-10 (r = 60%). From left to right, the results of four emphasis modes0, 3, 0.5, 3 with different emphasis variances are displayed in each column respectively. When λ islarger, β should be larger. Specifically :1) when	λ	= 0:	we tried	β	= 0.5, 1, 2, 4;2) when	λ	= 0.5: we tried	β = 4, 8, 12, 16;3) when	λ	= 1:	we tried	β	= 8, 12, 16, 20;4) when	λ	= 2:	we tried	β	= 12, 16, 20, 24.
Figure 9: ResNet-56 on CIFAR-10 (r = 80%). From left to right, the results of four emphasis modes0, 3, 0.5, 2 With different emphasis variances are displayed in each column respectively. When λ islarger, β should be larger. Specifically :1) When	λ	= 0:	We tried	β	= 0.5, 1, 2, 4;2) When	λ	= 0.5: We tried	β = 4, 8, 12, 16;3) When	λ	= 1:	We tried	β	= 8, 12, 16, 20;4) When	λ	= 2:	We tried	β	= 12, 16, 20, 24.
Figure 10: The training and test accuracies on clean CIFAR-10 along With training iterations. Thetraining labels are clean. We fix λ = 0 to focus on harder examples While changing emphasisvariance controller β. The backbone is ResNet-20. The results of ResNet-56 are shoWn in Figure 11.
Figure 11: The training and test accuracies on clean CIFAR-10 along With training iterations. Thetraining labels are clean. We fix λ = 0 to focus on more difficult examples While changing emphasisvariance controller β. The backbone is ResNet-56. The results of ResNet-20 are shoWn in Figure 10.
