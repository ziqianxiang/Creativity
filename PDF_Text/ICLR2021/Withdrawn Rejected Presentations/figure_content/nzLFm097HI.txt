Figure 1: Sample and computational efficiency for pixel attention and object-level vision represen-tations. The sample efficiencies experiments just use a total percentage of the available QA pairsand report the validation score after convergence. The computational efficiency is evaluated on the10 % QA data and are run 5 times. The train samples indicate the number of examples seen by themodel over time (iteration × batch size).
Figure 2: Sample and computational efficiency for soft versus symbolic reasoning. We take softlogic NS-CL and modify the reasoning to become symbolic. Then we optimize these symbolicmethods through REINFORCE and abduction for NS-RL and NS-AB respectively.
Figure 3: A comparison of efficiencies during joint training of vision and language. We test DePewith 0.1 % directly supervised labels.
Figure 4: Our model ingests the image and extracts object-centric information. The question (textualinput) is used by the question parser, which first embeds the question through an LSTM encoder.
Figure 5: Here is a sample execution for the vision and text in DePe. For the simplicity of thediagram, we don’t include all the probability traces and just show the ones corresponding to the maxprobability. For the vision, the predictions correspond to the objects detected in the scene (markedwith 1, 2, 3 in the picture). For each execution, we store the multiplication in the correspondingmemory row, and retrieve the final memory to answer the question.
