Figure 1:	Distributions over all batch losses Lbatch (blue) on consecutive and representative crosssections during a training process of a ResNet32 on CIFAR-10. The empirical loss Lemp, is givenin red, the quartiles in black. The batch loss, whose negative gradient defines the search direction, isgiven in green. See Section 2.1 for interpretations.
Figure 2:	An exemplar ELF routine testing for the best fitting polynomial. The empirical loss ingiven in red, the distribution of batch losses in blue. The sampled losses are given in orange andgreen. The green losses are the test set of the current cross validation step. It can be seen, that thefifth-order polynomial (green) reaches the lowest test error.
Figure 3: Comparison of ELF against PLS and GOLS1 on a ResNet-32 trained on CIFAR-10. Thecorresponding test accuracies are: EFL: 0.900, PLS: 0.875, GOLS1: 0.872. ELF performs better inthis scenario and, intriguingly, PLS and ELF estimate similar step size schedules.
Figure 4: Robustness comparison of ELF, ADAM , SGD and PAL. The most robust optimizer-hyperparameters across several models were determined on CIFAR-10, which then tested on furtherdatasets and models. On those, the found hyperparameters of ELF, ADAM, SGD and PAL behaverobust. (β=momentum, λ=learning rate, δ=decrease factor,α=update step adaptation, μ=measuringstep size). Plots of the training loss and validation accuracy are given in Appendix C. (Results forPAL on ImageNet will be included in the final version.)rate has to be searched for each new problem. In addition, we note that ELF tends to perform betteron MobileNet-V2 and the 3-Layer-FC network, however, is not that robust on SVHN. The mostimportant insight, which has been obtained from our experiments, is that for all tested models anddataset ELF was able to fit lines by polynomials. Thus, we are able to directly measure step sizeson the empirical loss. Furthermore, we see that those step sizes are useful to guide optimizationon subsequent steps. To strengthen this statement, we plotted the sampled losses and the fittedpolynomials for each approximated line. Representative examples are given in Figure 5 and inAppendix D.
Figure 5:	Representative polynomial line approximations (red) obtained by training ResNet-18 onCIFAR-10. The samples losses are depicted in orange. The minimum of the approximation isrepresented by the green dot, whereas the update step adjusted by a decrease factor of 0.2 is depictedas the red dot.
Figure 6:	Left: Fraction of training steps used for step size estimation and the total amount of trainingsteps. The amount of steps needed depends strongly on the model and dataset. Right: Training timecomparison on CIFAR-10. One can observe, that ELF performs slightly faster.
