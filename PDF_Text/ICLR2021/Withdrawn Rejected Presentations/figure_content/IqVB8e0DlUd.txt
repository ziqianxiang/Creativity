Figure 1: Effect of clipping and noise in differentially private mechanism and τ(C, σ; θt) on MNISTdatasetwhere n and μt denote the batch size and step-size (learning rate) respectively; St denotes therandomly chosen instance set; the vector 1 denotes the vector filled with scalar value one; andgt(xi) denotes the gradient of the loss function in (1) at iteration t, i.e., VL(yi； θt, Xi). The twokey operations OfDPSGD are: i) clipping each gradient gt(xi) in '2-norm based on the thresholdparameter C; ii) adding noise ξ drawn from Gaussian distribution N(0, σ2C2 ) with a variance ofnoise scale σ and the clipping threshold parameter C . These operations enable training machinelearning models with non-convex objectives at a manageable privacy cost. Based on the result oftraditional SGD, we theoretically analyze the sufficient decrease type scheme of DPSGD, i.e.,E ff (θt+1)] 6 f (θt) + E [〈Vf (θt),θt+1 - θt>] + 2 E h∣∣θt+1 - θt∣∣2i + T(C, σ; θt),	(3)where the last term τ(C, σ; θt) denotes the gap of loss expectation compared with ideal SGD at this(t + 1)-th iteration, and related with parameters C, and σ. The term τ(C, σ; θ), which can be calledbias-variance term, can be calculated mathematically as2(1 + ɪ) kVf(θ)k∙ η + η2 + 3 ∙ σ2c2|1|,	(4)μt L	n2	/Clipping bias	NOise variancewhere L denotes the Lipschitz constant of f; |1| denotes the vector dimension; and we haveη := n X	(kgt(χi)k- C),Ikgt(xi)k>C
Figure 2: Main flowchart of the FairDP Method (Step 4-12 in Algorithm 1)other. The formulation can be denoted as follows, i.e.,min{Ck},θ`X τk (Ck, σ; θ),k=1s.t. θ ∈ arg min L(θ; {Gk }k=1),θ=(6a)(6b)where the upper-level problem (6a) aims to fairly adjust the clipping threshold parameters for allclasses, which is related to the classification model θ; as for the lower-level problem (6b), we aim tolearn the classification model based on the differential privacy schema with the self-adaptive clippingthreshold {Ck}. These two objectives are coupled together, although the model of the lower-levelproblem is determined only by θ. The effect of clipping is reflected through the DP calculationprocedure. Guided by the bias-variance term in (6a), the parameters of the DP learning can be finelyupdated simultaneously with the learning process of the classifiers in (6b).
Figure 3: Privacy vulnerability comparison4.1	Fairnes s performanceTable 1 and Table 2 provides the comparison results on model accuracy and fairness after imple-menting different DP methods. Table 1 presents the utility loss of different private learning methodsw.r.t. classical SGD. Table 2 presents the comparison on four fairness indexes (Bureau, 2016), in-cluding Atkinson Index, Gini Index, MLD and Theil Index (Appendix D.4). In most cases, FairDPhas the least accuracy loss from SGD than other methods and offers equal fair statistics as SGD (alower value is better). Although Opt-Q has little improvement in fairness on the Adult dataset, itreduces both per-class and overall accuracies. Overall, FairDP can outperform other private learningmethods on both model fairness and accuracy and balance model fairness and accuracy.
Figure 4: Effect of learning rate on training procedure&S3MW<0.76-512⅛074-0.72-64 128	256Batch size(b) MNIST (Class 8)0.78-Male (SGD)Male (DPSGD)--- Male (Opt-Q)-+- Male (DPSGD-F)-Male (FairDP)0.70-^64~UB	256Batch size(c) Adult (Male)«4 U8 25«	512
Figure 5:	Effect of batch size on training procedure1.000.980.96>0.94I 0.92< 0.900.880.860.841	2	5Class 2 (SGD)Class 2 (DPSGD)Class 2 (DP-FedAvg)-+- Class 2 (Opt-Q)Class 2 (DPSGD-F)Class 2 (FelrDP)&S3UU<0.910-0.905
Figure 6:	Effect of noise on training procedure4.3 Adaptive performance(McMahan et al., 2018) claimed that the proper choice for the clipping parameters might depend onthe learning rate. If the learning rate changes, the clipping parameter also needs to be re-evaluated.
