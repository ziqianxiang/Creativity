Figure 1: (a) the standard BN structure; (b) the structure with 4 BN branches, which constructdifferent mini-batches for different Dk to estimate the normalization statistics; (c) and (d): runningmeans and variances of multiple BN branches on 16 randomly sampled channels in a VGG-16’sconv2Λ layer, showing that different perturbation types induce different normalization statistics.
Figure 2: The training and inference procedures of GBN. During training, samples from differentdomains (denoted by different colors) are fed into corresponding BN branches to update the runningstatistics. During inference, given mini-batch data, the gated sub-network first predicts the domainof input X and then jointly normalizes it using multiple BN branches.
Figure 3: (a) demonstrates the results of adding GBN to different single layers. (b) shows the resultsof adding GBN to top-m layers. All the experiments are conducted using a VGG-16 on CIFAR-10.
Figure 4: The architecture of the gated sub-network used in this paper.
Figure 5: Model sensitivity to additive noise aligned With different Fourier basis vectors on CIFAR-10. From left to right: vanilla, '1-trained, '2-trained, and '∞-trained models. The numbers indicatethe model error rates.
Figure 6:	We use the GBN block at layer conv3Λ of a VGG-16 model. (a) demonstrates thevisualization of features before the GBN block; (b) shows the visualization of features after theGBN block.
Figure 7:	We use the BN block at layer conv3Λ of a VGG-16 model trained using AVG. (a) demon-strates the visualization of features before the BN block; (b) shows the visualization of features afterthe BN block.
Figure 8: Running statistics (running mean and running variance) of each BN in the multiple BNbranches at different layers of a VGG-16 model on CIFAR-10.
Figure 9: Running statistics (running mean and running variance) of each BN in the multiple BNbranches at different layers of a WideReSNet-2840 model on CIFAR-10.
