Figure 1: Flow of Latents for ReinforcementLearning (Flare) architecture. Input frames arefirst encoded individually by the same encoder.
Figure 2: (i) full-state SAC (blue) where input contains both pose and temporal information; (ii)position-only SAC (green) with only pose information as input; (iii) Flare applied to the state space(orange) with pose information and velocity approximations through pose offsets as input. Whilefull-state SAC efficiently learns the optimal policy, position-only SAC recovers suboptimal policiesand fails learning at all in some cases. Meanwhile, the fusion of approximated velocities in Flare isable to recover the optimal policy nearly as efficiently as the full state SAC in most cases. Results areaveraged over 3 seeds with standard deviation.
Figure 3: We compare Flare to 2 SAC variants: i) Stack SAC (green) receives consecutive positionalstates (St, st-1, st-2, st-3) as input, whereas positional-only SAC receives (St) and Flare receives(St, δt) where δt = (St - st-1, st-1 - st-2, st-2 - st-3). ii) Recurrent SAC (blue) uses recurrentlayers to process a series of states. Despite of the implicit access to temporal information betweenconsecutive states, Stack SAC and Recurrent perform significantly worse than Flare on most environ-ments, highlighting the benefit of explicit fusion of temporal information. Results are averaged overthree seeds.
Figure 4: Flow of Latents for Reinforcement Learning (Flare). In panel (a) We show the architecturefor the frame stacking heuristic, in (b) we show an alternative to the frame stacking hueristic byencoding each image individually, and in (c) we show the Flare architecture which encodes imagesindividually, computes the feature differences, and fuses the differences together with the latents.
Figure 5: We choose the following environments for our main experiments - (i) quadruped walk,which requires coordination of multiple joints, (ii) hopper hop, which requires hopping whilemaintaining balance, (iii) pendulum swingup, an environment with sparse rewards, (iv) walker run,which requires the agent to maintain balance at high speeds, and (v) finger turn hard, which requiresprecise manipulation of a rotating object. These environments are deemed challenging because priorstate-of-the-art model-free pixel-based methods (Laskin et al., 2020b; Kostrikov et al., 2020; Laskinet al., 2020a) either fail to reach the asymptotic performance of state SAC or learn less efficiently.
Figure 6: We compare the performance of Flare to RAD, a state-of-the-art algorithm and the basealgorithm used in Flare, on five challenging environments. Pendulum Swingup are trained over 1.5e6and the rest 2.5e6. We see that Flare substantially outperforms RAD on a majority (3 out of the 5) ofenvironments, while being competitive in the remaining. While not closing the gap between pixel andstate-based performance entirely, Flare is closer to state-based performance than prior methods, andis the state-of-the-art pixel-based model-free algorithm on most of these challenging environments.
Figure 7: We compare Rainbow DQN and Flare on 8 Atari games over 50M training steps. Flaresubstantially enhances a majority (5 out of 8) of the games over the baseline Rainbow DQN whilematching the rest. Results are averaged over 3 random seeds with standard deviation (shaded regions).
Figure 8: We perform three ablation studies. (a) pixel flow ablation: we compare Flare to a variantwhere the differences are computed directly in pixel space (pixel flow) and find that latent flow ismore stable and achieves better performance. (b) Latent stack ablation: in this experiment, we fusethe latent vectors without the temporal approximation. We find that this method performs significantlyworse than Flare, and on quadruped fails entirely, suggesting that fusing explicit temporal informationis crucial. (c) Frames count ablation: We test whether adding more frames increases performance forFlare. We find that including additional input frames either does not change or degrades performance.
