Figure 1: An example of supernetand two subgraphs with index i-.
Figure 2: Some examalphaple of supernets and the corresponding tensor networks (TNs). Informationflows in supernet (a) (c) follow the order of number in blocks. Circles in (b) and (d) represent core(a) Linear supernet.
Figure 3: Ablation studies on NAS-Bench-201, CIFAR-100 is used8Under review as a conference paper at ICLR 20214.3.2	Impact of rank of tensor networkWe also investigate the impact of Rn ’s, which are ranks in tensor network (TN) on supernets. Forsimplicity, we restrict Rn to be equal for all nodes n ∈ N (S) and compare the performance ofdifferent ranks with previous state-of-the-art REA (see Table 2) in Figure 3(b). Results demonstratethat while the rank can influence the final performance, it is easy to set rank properly for TRACE tobeat other methods. We also adopt Rn = 2 for all other experiments.
Figure 4: Correlation of T and ground-truth accuracy on NAS-Bench-201 for different datasets.
Figure 5: Architectures found by TRACE on NAS-Bench-201 for different datasets.
Figure 6: Architectures found by TRACE on weight-sharing setting.
Figure 7: Validation MRR during training on three KG datasets.
Figure 8: Validation macro F1 score during training on three HIN datasets.
Figure 9: Ablation studies on NAS-Bench-201, CIFAR-10 is used(a) Encoding methods(b) TN ranks(c) AlgorithmsFigure 10: Ablation studies on NAS-Bench-201, ImageNet-16-120 is used19Under review as a conference paper at ICLR 2021G Illustration of Tensorization Process for Supernets(d) Final tensor network (step 3).
Figure 10: Ablation studies on NAS-Bench-201, ImageNet-16-120 is used19Under review as a conference paper at ICLR 2021G Illustration of Tensorization Process for Supernets(d) Final tensor network (step 3).
Figure 11: A step-by-step illustration of supernet encoding process (Algorithm 1).
