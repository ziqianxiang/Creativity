Figure 1: Illustration of the BERT-base computational model. |V | denotes the number of tokens inthe model. #Classes denotes the number of classes in the down-stream classification task. Inputencoding, Feed-forward 3 and Feed-forward 4 are computed only once in the inference and thus donot contribute to overall computational time much.
Figure 2: Illustration of the empirical observation that weight matrices in BERT model are notlow-rank. The x-axis represents what percentage of the ranks are selected; the y-axis represents sumof singular values connected to the selected ranks divided by sum of all singular values. Ideally, alow-rank structure will have a larger area under curve, meaning that a small percentage of the rankscould explain most of singular values. We observe that the sum of top 50% of the ranks only accountabout 60% of all singular values for matrices in the BERT model. This shows that the matrices donot have a clear low-rank structure.
Figure 3: Illustration of efficiency and efficacy trade-off. Each point in this graph represents a specificratio of training loss increase after approximation.
