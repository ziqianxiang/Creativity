Figure 1: Overview of our NASOA. Our faster task-oriented online fine-tuning system has two parts: a) OfflineNAS to generate an efficient training model zoo with good accuracy and training speed; b) An online fine-tuningregime generator to perform a task-specific fine-tuning with a suitable model under user’s time constraint.
Figure 2: Our joint block/macro-level search space to find efficient training networks. Our block-level searchspace covers many popular designs such as ResNet, ResNext, MobileNet Block. Our macro-level search spaceallows small adjustment of the network in each stage thus the resulting models are more flexible and efficient.
Figure 3: Comparison of the training and inference efficiency of our searched models (ET-NAS) with SOTAmodels on ImageNet. Our searched models are considerably faster, e.g., ET-NAS-G is 6x training faster thanRegNetY-16GF, and ET-NAS-I is 1.5x training faster than EfficientNetB3. Although our models are optimizedfor fast training, the inference speed is comparable to EfficientNet and better than RegNet series.
Figure 4: Comparison of the training efficiency ofour searched models (ET-NAS) with 8 other NAS re-sults on ImageNet. It can be found that our methodis more training efficient than some recent evolution-based NAS methods such as AmoebaNet, OFANet be-cause of our effective search space.
Figure 5: Comparison of the final fine-tuning results under four time constraints for the testing dataset. Redsquare lines are the results of our NASOA in one-shot. The dots on the other solid line are the best performanceof all the models in that series can perform. The model and training regime generated by our NASOA canoutperform the upper bound of other methods in most cases. Our methods can improve around 2.1%~7.4%accuracy than the upper bound of RegNet/EfficientNet series on average.
Figure 6: (Left) Fine-tuning ResNet101 with different weight-frozen stages. “Freeze: k" means 0 tok stage'sparameters are not updated during training. The number of frozen stage will effect both training time and accu-racy. Its optimal frozen setting varies with datasets. (Right) Comparison of accuracy/time different fine-tuningmodels. Different models should be selected upon the request of different datasets and training constraints.
Figure 7: Fine-tune results along time with various networks on these datasets. It Can be seen that ifthe time constraints is short, We should choose a smaller network.
Figure 8: Block structure and two block samples.
Figure 9: Results of the block-level search inImageNet-100. The y-axis denotes the accuracy and x-axis denotes the latency. Blue dots are models searchedin this step, while the red ones are Basic Block withfirst channel 64, 128, 192; Inverted Bottleneck Block(expansion rate 4) with first channel 64, 128; Bottle-Neck Block (expansion rate 4) with first channel 256,320. It can be found that our algorithm can find moreefficient block in the block-level search.
Figure 10: Comparison of the training and inference efficiency of our searched models (ET-NAS) with SOTANAS models on ImageNet. We further compared our models with 8 other NAS results. It can be found that ourmethod is more training efficient than some recent evolution-based NAS methods such as AmoebaNet (Realet al., 2019b), OFANet (Cai et al., 2019a) because of our effective search space.
