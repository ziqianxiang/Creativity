Figure 1: The framework of learning-to-learn. The dashed line shows the computation graph of theobjective function Lopt for training the optimizer to learn a general update rule while the horizontalfull line is the one for few-shot learning. Note that m is the neural optimizer parameterized by φ,and St is the state of the optimizee taking the form of St = (θt,..., Vθ')t.
Figure 2: An illustration of the non-smoothness in the neural optimizer.
Figure 3: Learning curves of classification on MNIST. Training loss is shown in the first row andtesting loss in the second row. (a) and (d) are results of two neural optimizer structures to showthe compatibility of our proposed regularizer; (b) and (e) demonstrate performance of differentoptimizers for training LeNet of 200 steps, while (c) and (f) extend the optimization to 1000 steps.
Figure 4: Learning curves of classification on CIFAR-10. (a) and (d) show performance of training a3-layer CNN for 10000 steps while (b) and (e) are results of 10000-step optimization of GoogLeNet.
Figure 5: Learning curves of different optimizers in training and testing accuracy. (a)-(b) for MNISTwith LeNet and (c)-(d) for CIFAR-10 with a 3-layer CNN.
Figure 6: Learning curves of classification on tiny-ImageNet.
Figure 7: Loss difference of SimpleOptimizer and Smoothed-Simple.
