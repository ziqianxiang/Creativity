Figure 1: In goal-driven tasks, states on an expert trajectory have gradually increasing proximitytoward the goal as the expert proceeds and fulfills a task. Inspired by this intuition, We propose tolearn a proximity function fφ from expert demonstrations and agent experience, which provides anestimate of temporal distance to the goal of a task. Then, using this learned proximity function, wetrain a policy πθ to progressively move to states with higher proximity and eventually reach the goalto solve the task. We alternate these two learning phases to improve both the proximity function andthe policy, leading to not only better learning efficiency but also superior performance.
Figure 2: Four goal-driven tasks are used to evaluate our proposed method and the baselines. (a) Theagent (yellow) must navigate across rooms to reach the goal (green). (b, C) The robotic arm is requiredto pick up or push the yellow block towards the goal (red). (d) A quadruped ant agent must walktowards the green flag.
Figure 3: Goal completion rates of our method and baselines. The agent must generalize to a widerstate distribution than seen in the expert demonstrations. Demonstrations in (a,b) cover only 50% ofstates and in (c,d) are generated with less noise. Note that GAIL and BC (dashed lines) use expertactions whereas all other methods, including ours, learn from observations only. Our method learnsmore stably, faster and achieves higher goal completion rates than baseline LfO algorithms. Moreover,our method outperforms BC and GAIL in Navigation and Fetch Push, and achieves comparableresults in all other tasks.
Figure 4: Analyzing the effect of improved generalization as the cause for performance increase inour method. (a) performance with no generalization required (i.e., same start and goal distribution fordemonstrations and online learning). (b, c) performance with increasing difference between start andgoal distributions of demonstrations and online learning. (d) visualization of the learned proximityfunction for a fixed goal (green). The proximity function was evaluated for every state on the grid;lighter cells correspond to states which higher estimated proximity to the goal.
Figure 5: Ablation analysis of our method on ANT REACH. (a) Comparing different λ values to showthe effect of the uncertainty penalty. λ = 0 corresponds to no uncertainty penalty. (b) Contrastingtwo alternate formulations of the proximity function. (c) Analyzing the effect of online and offlineproximity function training.
Figure 6: Ablation analysis of the contribution of proximity, uncertainty penalty, and rewardformulation to our method’s performance. “Prox” uses the goal proximity function while “GAIfO-s”does not. “+Diff” uses R(st, st+1) = f (st+1) - f(st) and “+Abs” uses R(st) = f(st) as theper-time step reward. “+Uncert” adds the uncertainty penalty to the reward. Finally, “No Final”removes the sparse proximity reward at the final time step.
Figure 7: Examining different ways of training the proximity function. (a-c) Comparing training theproximity function with actions as input or with a ranking-based objective. (d) Analyzing differentchoices of δ for training the proximity function. The model learns similarly well over a range of δvalues around 1/H, which in ANT REACH is 0.02, but struggles for large δ as many proximity valueswill be clipped to 0.
Figure 8: Analyzing generalization under different demonstration coverages. The percentage refersto the percentage of regions the expert demonstrations cover. A higher coverage percentage indicatesless generalization, with 100% requiring no generalization.
Figure 9: Analyzing generalization to more noisy environments. The number indicates the amount ofadditional noise in agent learning compared to that in the expert demonstrations, with more noiserequiring harder generalization.
Figure 10: Visualizing the proximity predictions for a successful trajectory from agent learning in (a)Fetch Pick and (b) Fetch Push. Four informative frames are selected from the overall trajectoryand the predicted proximity value is displayed below.
Figure 11:	Ablation experiments across additional environments. Our method shows consistentlysuperior performance over all ablations.
Figure 12:	Comparing our method to SQIL across three tasks in the most challenging generalizationsettings. SQIL results are for 3 seeds while our method and BC are for 5 seeds. Note that SQIL andBC use actions whereas our method does not.
