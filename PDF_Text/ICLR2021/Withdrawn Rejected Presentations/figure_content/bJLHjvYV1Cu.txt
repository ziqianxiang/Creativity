Figure 1: The TaylorGLO method. Start-ing with a population of initially unbiasedloss functions, CMA-ES optimizes their Tay-lor expansion parameters in order to maxi-mize validation accuracy after partial train-ing. The candidate with the highest accuracyis chosen as the final, best solution.
Figure 2: The process of discovering loss func-tions in MNIST. Red dots mark generationswhere new improved loss functions were found.
Figure 3: The best loss functions (a) and their respective parameters (b) from each generation ofTaylorGLO on MNIST. The functions are plotted in a binary classification modality, showing loss fordifferent values of the network output (y0 in the horizontal axis) when the correct label is 1.0. Thefunctions are colored according to their generation from blue to red, and vertically shifted such thattheir loss at y0 = 1 is zero (the raw value of a loss function is not relevant; the derivative, however,is). TaylorGLO explores varying shapes of solutions before narrowing down on functions in thered band; this process can also be seen in (b), where parameters become more consistent over time,and in the population plot of Appendix B. The final functions decrease from left to right, but have asignificant increase in the end. This shape is likely to prevent overfitting during learning, which leadsto the observed improved accuracy.
Figure 4: Accuracy basins for AllCNN-C models trained with both cross-entropy and TaylorGLOloss functions. The TaylorGLO basins are both flatter and lower, indicating that they are more robustand generalize better (Keskar et al., 2017), which results in higher accuracy.
Figure 5: (a) Mean test accuracy across tenruns on MNIST. The TaylorGLO loss func-tion with the highest validation score signifi-cantly outperforms the cross-entropy loss (p =2.95 × 10-15 in a one-tailed Welch’s t-test) andthe BaikalCMA loss (Gonzalez & Miikkulainen,2020) (p = 0.0313). (b) Required partial train-ing evaluations for GLO and TaylorGLO onMNIST. The TaylorGLO loss function was dis-covered with 4% of the evaluations that GLOrequired to discover BaikalCMA.
Figure 6: Accuracy with reduced portions of theMNIST dataset. Progressively smaller portions ofthe dataset were used to train the models (averag-ing over ten runs). The TaylorGLO loss functionprovides significantly better performance than thecross-entropy loss on all training dataset sizes, andparticularly on the smaller datasets. Thus, its abil-ity to discourage overfitting is particularly useful inapplications where only limited data is available.
Figure 7: A visualization of all TaylorGLO loss function candidates using t-SNE (Maaten & Hinton,2008) on MNIST. Colors map to each candidate’s generation. Loss function populations showan evolutionary path and focus over time towards functions that perform well, consistent with theconvergence and settling in Figure 3.
Figure 8: Effect of varying learning rates in AllCNN-C when trained with the cross-entropy loss onCIFAR-10. For each learning rate, ten models were trained, with up to ten retries if training failed.
