Figure 1: Graphical models ofthe decoders. From top: CVAE,iVAE, and CFVAE. The encodersfor the VAEs are similar: theytake all observed variables andbuild approximate posteriors.
Figure 2: Plots of recovered (x) - true (y) la-tent on the nonlinear outcome. Blue: t = 0,Orange: t = 1. α, β = 0.4. “no.” indicatesindex among the 100 random models.
Figure 3: Pre-treatment √epehe on non-linear synthetic dataset. Error bar on 100random models. We adjust one of the noiselevels α, β in each panel, with another fixedto 0.2. See Appendix for results on lin-ear outcome. Results for ATE and post-treatment are similar.
Figure 4: Graphical models for generating synthetic datasets. From left: IV, ignorability given x, and non-balancing proxy x. Note that in the latter two cases, reversing the arrow between x, z does not change anyindependence relationships, and causal interpretations of the graphs remain the same.
Figure 5: √6pehe on linear syntheticdataset. Error bar on 100 random models.
Figure 6: Plots of recovered-true latent under unobserved confounding. Rows: first 10 nonlinear randommodels, columns: proxy noise level.
Figure 7: Plots of recovered-true latent under unobserved confounding. Rows: first 10 nonlinear randommodels, columns: outcome noise level.
Figure 8: Plots of recovered-true latent when ignorability holds. Rows: first 10 nonlinear random models,columns: proxy noise level.
Figure 9: Plots of recovered-true latent when ignorability holds. Rows: first 10 nonlinear random models,columns: outcome noise level.
Figure 10: Plots of recovered-true latent when ignorability holds. Conditional prior depends on t. Rows: first10 nonlinear random models, columns: outcome noise level. Compare to the previous figure, we can see thetransformations for t = 0, 1 are not the same, confirming our Theorem 3.
Figure 11: Plots of recovered-true latent on IVs. Rows: first 10 nonlinear random models, columns: outcomenoise level.
