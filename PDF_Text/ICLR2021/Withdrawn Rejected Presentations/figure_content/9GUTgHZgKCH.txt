Figure 4: Schematic representation of a sim-plified three-stacked networkFigure 5: Schematic representation of athree-stacked network which will be studiedin more detail in the following workWe recommend using a higher learning rate for the affine weights. In our experiments, we used aten-times higher learning rate for the affine weights.
Figure 5: Schematic representation of athree-stacked network which will be studiedin more detail in the following workWe recommend using a higher learning rate for the affine weights. In our experiments, we used aten-times higher learning rate for the affine weights.
Figure 6: The first two plots correspond to the deep network before reduction, respectively the lasttwo plots correspond to network after reduction(a) One kink(b) Absolute value(d) Sign Function(l) Reduced cubic(m) Reduced sine(n) Reduced exponentialFigure 7: The first seven plots correspond to the deep network before reduction, respectively the lastseven plots correspond to network after reduction7Under review as a conference paper at ICLR 20215.2 MNISTThe last one-dimensional example was hopefully very intuitive and visual, yet it would now beinteresting to test the architecture on a higher-dimensional dataset. We chose to apply it to MNIST,the very popular handwritten digit recognition dataset.
Figure 7: The first seven plots correspond to the deep network before reduction, respectively the lastseven plots correspond to network after reduction7Under review as a conference paper at ICLR 20215.2 MNISTThe last one-dimensional example was hopefully very intuitive and visual, yet it would now beinteresting to test the architecture on a higher-dimensional dataset. We chose to apply it to MNIST,the very popular handwritten digit recognition dataset.
Figure 8: Training and Testing error (in %) for ten different initial seeds (each seed represented bya different color)Figure 9 also shows two plots which compare the Reduction Algorithm and the pruning method, fordifferent number of neurons nj in the ReLU layer of every stack. As one can easily see, the com-pression factor goes up as the number of neurons increases (per stack, for three stack and dj = 16neurons in the bottleneck layer) and the plots help visualize that we can indeed reduce the networksto a rather constant number of neurons njj ≤ 100 per stack (no matter how many neurons nj weuse), which illustrate the result of the theory in section 3 about a finite number of neurons needed torepresent the learned function.
Figure 9:	Training and Testing error (in %) for different number of neurons per stack8Under review as a conference paper at ICLR 2021Cβπιpresslon factorFigure 10 compares the two methods for a different number of epochs using the same trainingscheme as explained above, where we just multiply the number of epochs by some increasing num-ber and using the same architecture with three stacks (nj = 1024 and dj = 16). One can also seefrom these plots the importance of training for some time, in order to approach a perfectly trainedmodel (which would be necessary in eq. (1) for the theory in section 3 to hold), indeed the cleartrend is that the compression factor increases as the training time increases.
Figure 10:	Training and Testing error (in %) for different number of epochs6 Conclusion and further workIn this paper, we have introduced a neural network architecture which allows to be reduced signifi-cantly in the sense that we can remove or replace many of its ReLU neurons by fewer ReLU neurons.
