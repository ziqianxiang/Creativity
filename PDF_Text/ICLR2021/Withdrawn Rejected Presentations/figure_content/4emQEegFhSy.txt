Figure 1: Adaptive fusion of K prediction errors from the multiple modelscases asmax {η(∏) + C E(s,a)〜∏[D(Pl∣Pφ)l(s, a)]}	⑴for some constant c > 0 and some divergence function D(∙∣∣∙), where η(∏) is the cumulative rewardassociated with policy π, and Pφ is the learning model parameterized by φ that the agent has regardingthe true unknown transition probability P of the environment. For the divergence, the mean squarederror (MSE) between the actual next state and the predicted next state can be used for the errormeasure when the learning model predicts the next state itself, or alternatively the Kullback-Leiblerdivergence (KLD) between the probability distribution for the next state st+1 and the predictedprobability distribution for st+1 can be used when the learning models learn the transition probability.
Figure 2: Performance comparison. All simulations were conducted over ten fixed random seeds.
Figure 3: (a) Learning curve of α during the proposed fusion learning in HalfCheetah for 1Mtimesteps. (b) The performance comparison with static fusion methods. (c, d) Mean performancefor 1M timesteps as a function of K for (c) Walker2d and (d) HalfCheetah. K = 0 means PPOwithout intrinsic reward, and K = 1 means the single-model surprise method. (K = 4 yieldedsimilar performance to that of K = 3, so we omitted the curve of K = 4 for simplicity.)7Under review as a conference paper at ICLR 20214.2.1	LEARNING B EHAVIOR OF FUSION PARAMETER αWe investigated how the fusion parameter α changed adaptively during the training. Fig. 3(a) showsthe learning curve of the fusion parameter α in HalfCheetah. It is seen that starting from the initialvalue α = 0, the fusion parameter α increases until it reaches approximately 5, maintains the leveluntil approximately 180 iterations (0.4 million timesteps), and then decreases monotonically. Theproposed fusion learning method takes relatively more aggressive fusion strategies with α beingaround 5 (but this is not the too aggressive maximum corresponding to α = ∞) in the early stage oflearning. Then, the fusion learning takes more and more conservative fusion strategy by decreasing αmore and more to large negative values (i.e., towards minimum taking). This observation is consistentwith the general behavior of RL that aggressive exploration is essential in the early stage of learningand conservative exploitation has a more considerable weight in the later stage of learning.
Figure 5:	Reproduced mean performance of the module method over 10 random seeds with ∆ = 40when B = 0.
Figure 6:	Performance comparison between our proposed method (blue curve) and fusion withneural network learning with nonlinear activation (green) when K = 2. (Fusion with neural networklearning with linear activation performed worse than the nonlinear activation, so we omitted theresult.) All simulations were conducted over ten fixed random seeds.
Figure 7: Visualizations of our fusion function and the fusion method based on neural networklearning (both were trained in the Hopper environment). The gray surface is the (x1, x2) input plane,and and the y-axis is the fusion output: (a) the proposed fusion function (blue) and the linear neuralnetwork fusion function (green). (b) The proposed fusion function (blue) and the nonlinear neuralnetwork fusion function (green).
Figure 8: Performance comparison between the proposed method using KLD (blue) and the proposedmethod using MSE (green). All simulations were conducted over ten fixed random seeds.
