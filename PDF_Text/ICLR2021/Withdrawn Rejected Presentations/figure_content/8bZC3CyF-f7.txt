Figure 1: Basic insight of RUDDER (left panel) and reward redistribution (right panel). Leftpanel, row 1: An agent has to take a key to open the door to a treasure. Both events increase theprobability of obtaining the treasure. row 2: Learning the Q-function by a fully connected networkrequires to predict the expected return from every state-action pair (red arrows). row 3: Learning theQ-functions by memorizing relevant state-action pairs requires only to predict the steps (red arrows).
Figure 2: Left: Alignment of biological sequences (triosephosphate isomerase) giving a conservationscore. Right: Alignment of demonstrations using the conservation score for reward redistribution.
Figure 3: The five steps of Align-RUDDERâ€™s reward redistribution.
Figure 4: Comparison of Align-RUDDER and other methods on Task (I) (left) and Task (II) (right)with respect to the number of episodes required for learning on different numbers of demonstrations.
Figure 5: Comparing the consensus frequencies between behavioral cloning (BC, orange), wherefine-tuning starts, the fine-tuned model (blue), and human demonstrations (green).
