Figure 1: Cumulative captured variance of the key query matrices per head separately (left) and perlayer with concatenated heads (right). Matrices are taken from a pre-trained BERT-base model withNh = 12 heads of dimension dk = 64. Bold lines show the means. Even though, by themselves,heads are not low rank (left), the product of their concatenation WQ WK> is low rank (right, in red).
Figure 2: Left: computation of the attention scores between tokens xn and ym using a standardconcatenated multi-head attention with Nh = 3 independent heads. The block structure of the mixingmatrix M enforces that each head dot products non overlapping dimensions. Right: we propose touse more general mixing matrices M than (a) heads concatenation, such as (b) allowing heads tohave different sizes; (c) sharing heads projections by learning the full matrix; (d) compressing thenumber of projections from Dk to Dk as heads can share redundant projections.
Figure 3: Comparison of the BLEU score on WMT14 EN-DE translation task for an encoder-decodertransformer-base (Vaswani et al., 2017) using collaborate vs. concatenate heads with key/querydimension Dk . We visualize performence as a function of number of parameters (middle) andtraining time (right). Collaborative attention consistently improves BLEU score, Dk can be decreasedby a factor of 4 without drop in performance.
Figure 4: Time to decompose_____- _ _____________ ~BERT-base from Dk = 768 to Dk.
Figure 5: Performance on MNLL MRPC and STS-B datasets of a …fine-tuned BERT-base model,—decomposed with collaborative heads of compressed dimension Dk (horizontal axis). — Repeatingfine-tuning after compression can make the model recover the original performance when compressionwas drastic. The GLUE baseline gives a reference for catastrophic failure.
