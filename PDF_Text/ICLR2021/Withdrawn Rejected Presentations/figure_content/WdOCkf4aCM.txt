Figure 1:	A SDT node. σ(∙) is the sigmoid function with function values on decision nodes as input.
Figure 2:	CDT methods. Left: a simple CDT architecture, consisting of a feature learning tree F anda decision making tree D; Right: two possible types of hierarchical CDT architectures, where (a) isan example architecture with hierarchical representation learning using three cascading F before oneD, and (b) is an example architecture with two F in parallel, potentially with different dimensions ofx as inputs.
Figure 3: Comparison of SDTs and CDTs on CartPole-v1 and LunarLander-v2 in terms of averagerewards in RL setting: (a)(c) use normalized input states while (b)(d) use unnormalized ones.
Figure 4: Comparison of SDTs and CDTs with different depths (state normalized). (a) and (b) aretrained on CartPole-v1, while (c) and (d) are on LunarLander-v2.
Figure 5: Left: learned CDT (after discretization). Right: game scene of CartPole-v1.
Figure 6: Left: learned CDT (after discretization). Right: game scene of MountainCar-v0.
Figure 7: Comparison of three different tree structures on a simple classification problem. From leftto right: (1) multivariate DT; (2) univariate DT; (3) differentiable rule lists.
Figure 8: Comparison of two different tree structures on a complicated classification problem.
Figure 9: The heuristic decision tree for LunarLander-v2. x is an 8-dimensional observation, a isthe univariate action given by the agent. Ot) ht are two intermediate variables, corresponding to the"angle-to-do" and "hover-to-do" in the heuristic solution.
Figure 10: A detailed architecture of simple CDT: the feature learning tree is parameterized by w andb, and its leaves are parameterized by w ; the inner nodes of decision making tree are parameterizedby w! and bf, while the leaves are parameterized by w!.
Figure 11: Comparison of numbers of model parameters in CDTs and SDTs. The left vertical axis isthe number of model parameters in log-scale. The right vertical axis is the ratio of model parameternumbers. CDT has a decreasing ratio of model parameters against SDT as the total depth of modelincreases.
Figure 12: Multiple soft decision boundaries (dashed lines) partition the space. The dots representinput data points, and different colored regions indicate different partitions in the input space. Theboundaries closer to the instance are less important in determining the feature importance since theyare less distinctive for the instance.
Figure 13: Comparison of feature importance (local explanation I) for SDT of depth 3, 5, 7 with HDTon an episodic decision making process.
Figure 14: Comparison of feature importance for three SDTs (depth=5, trained under the samesetting) with three different local explanations. All runs are conducted on the same offline episode.
Figure 15: Comparison of four runs with the same setting for SDT (before discretization) imitationlearning on LunarLander-v2. The dashed lines with different colors on the left top diagram indicatethe valid regions for each color bar, which is the default setting for the rest diagrams.
Figure 16: Comparison of four runs with the same setting for SDT (after discretization) imitationlearning on LunarLander-v2.
Figure 17: Comparison of four runs with the same setting for SDT (before discretization) imitationlearning on CartPole-v1.
Figure 18: Comparison of four runs with the same setting for SDT (after discretization) imitationlearning on CartPole-v1.
Figure 19: Comparison of four runs with the same setting for CDT (before discretization) imitationlearning on LunarLander-v2: feature learning trees (top) and decision making trees (bottom).
Figure 20: Comparison of four runs with the same setting for CDT (after discretization) imitationlearning on LunarLander-v2: feature learning trees (top) and decision making trees (bottom).
Figure 21: Comparison of four runs with the same setting for CDT (before discretization) imitationlearning on CartPole-v1: feature learning trees (top) and decision making trees (bottom).
Figure 22: Comparison of four runs with the same setting for CDT (after discretization) imitationlearning on CartPole-v1: feature learning trees (top) and decision making trees (bottom).
Figure 23: Comparison of SDTs and CDTs with different depths (state unnormalized). (a) and (b) aretrained on CartPole-v1, while (c) and (d) are on LunarLander-v2.
Figure 25: The learned CDT (before discretization)P(a[0]): 1.0p(a[l]): 0.0of depth 1+2 for CartPole-v1.
Figure 26: The learned SDT (before discretization) of depth 3 for CartPole-v1.
Figure 27: The learned CDT (before discretization) of depth 2+2 for MountainCar-v0.
Figure 28: The learned CDT (after discretization) of depth 2+2 for LunarLander-v2: two dimensionsare reserved for weight vectors in both F and D, as well as the intermediate features.
