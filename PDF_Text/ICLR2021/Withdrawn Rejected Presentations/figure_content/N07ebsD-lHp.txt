Figure 1: Minimum L2 image distortion as a function of ensemble size3.3	Substitute model black-box attacksOur modelâ€™s high distortion in CIFAR10 is reflected in substitute model black box attacks on thisdataset Papernot et al. (2016a). We train a two hidden layer neural network each with 200 nodes asthe substitute using the standard adversarial augmented algorithm Papernot et al. (2017) (describedfully in the Supplementary Material). In Figure 2 we that our models require a much higher dis-tortion than their gradient descent trained equivalents MLP and BNN in order to reach zero percentadversarial accuracy. We also see that all models attacked with random Gaussian noise of the samedistortion added to the test examples are barely affected thus showing the effectiveness of the blackbox adversarial examples.
Figure 2: Accuracy of adversarial examples and test data with Gaussian noise (denoted as -GNfor each model) for various distortion thresholds on CIFAR10. The substitute model adversarialexamples are far more effective than random noise. At distortion 0.125 both MLP and BNN havenear 0% accuracy whereas SCD01 has 40%.
Figure 3: Original and adversarial ECG examples3.6	DiscussionUsing ensembles of neural networks and promoting diverse ensembles has been previously proposedas a defense against adversarial attacks. Studies using ensembles with different initializations (likewe do), bootstrapping, and Gaussian noise have shown robustness but only in the white box settingStrauss et al. (2017) (which is somewhat unrealistic since it assumes the attacker has full knowledgeof the model and its parameters). Other studies combine the loss of all models in the classifier andadd a regularizer that promotes diversity.
