Figure 1: Overview of the GOAL steps. A GNN-based surrogate model f predicts the ranking scorey of the original architecture at, then back-propagating y through the GNN model to compute thegradient w.r.t. αt. A better architecture αt+1 is obtained by proximal gradient descent.
Figure 2:	Two typical architecture representations as DAGs. Left: nodes as operations, edges asconnections; right: nodes as intermediate outputs, edges as operations.
Figure 3:	Ranking results on architectures in NAS-Bench-201, where ρ stands for Spearman’s rankcorrelation coefficient, lower rank order stands for better architecture. Left: ranking results of dif-ferent models; right: comparison between models with varying training data sizes.
Figure 4: Comparisons of search efficiency. Left: validation (top) and test (bottom) error of archi-tectures searched on NAS-Bench-101; right: validation (top) and test (bottom) error of architecturessearched on NAS-Bench-201.
Figure 5: Comparisons of search efficiency with standard deviation. Left: validation (top) and test(bottom) error of architectures searched on NAS-Bench-101; right: validation (top) and test (bottom)error of architectures searched on NAS-Bench-201.
Figure 6: Left: the ranking result of the model accomplished search process on NAS-Bench-201;right: the ranking results of the model retrained with the same size of uniformly sampled trainingdata.
Figure 7: The 3-step evolution history of a found architecture on NAS-Bench-101. ’I’, ’O’, and ’C3’mean for input node, output node and 3 × 3 convolution node respectively. The green parts are newlyadded, while dashed parts indicate the deletion.
Figure 8: Normal (left) and reduction (right) cell found by GOAL on DARTS space.
