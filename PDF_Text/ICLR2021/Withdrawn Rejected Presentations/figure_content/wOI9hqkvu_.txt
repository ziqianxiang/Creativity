Figure 1: Architecture of NAGAN. (A) The generator converts the latent variable Z = [z1z2 . . . zL]to a sentence O = [o1o2 . . . oL], where each oi is a one-hot vector. The gradient of non-differentialoperation is estimated by the straight through estimator. (B) The discriminator produces a scoreD(O) for the sentence O. The gradient from the discriminator can be passed back to the generator.
Figure 2: Training curves on synthetic data.
Figure 3: Comparison to autoregressive (AR)generators on synthetic data of different lengths.
Figure 4: Connection between tokens and latent variables at different positions. The element in thei-th row and the j-th column is the probability that the token Xj changes when modifying the latentvariable zi. The last token is a full stop and never changed.
Figure 5: Cases of cosine similarities between intermediate hidden states and the last outputs. Thei-th row indicates the hidden states of the i-th Transformer layer (1 ≤ i ≤ 5).
Figure 6: Overview of NAGAN on unsupervised decipherment. (A) Three losses of generators:Lrec,X, Lcyc,X, Ladv,Y. The other three losses can be obtained by swapping the roles of X and Y.
