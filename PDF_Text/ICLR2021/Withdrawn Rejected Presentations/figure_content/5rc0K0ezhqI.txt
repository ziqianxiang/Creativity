Figure 1: Information plane plot of the training Figure 2:trajectories of ResNet18 models with our surro- formation quantities in the model p(x, y, z) =gate objective minθ Hθ[Y | Z] + YlE ∣∣Z∣∣2 on Ima- p^(羽y)pθ(z | X). X is for the data, Y for the la-genette. Color shows γ; transparency the training bels, Z for the latent encodings. See section 2 forepoch. Compression (Encoding Entropy J) trades- more details and section B.1 for a description ofoff with test performance (Residual Information all the quantities. I[Y; Z] = I[X; Y; Z] becauseJ). See section 4.
Figure 3: Decreasing the entropy of a noise-free Figure 4: Information plane plot of the latent Zlatent does not affect the training error. (Though similar to Tishby and Zaslavsky (2015) but using afloating-point issues start affecting it negatively ResNet18 model on CIFAR-10 using the differenteventually.) When adding zero-entropy noise, the regularizes from section 3.3. A larger version canerror rate increases as the entropy approaches zero. be found in figure G.1. See section 4 for moreSee section 3.3 and G.4 for more details.	details. Best viewed in color.
Figure 5: Adversarial robustness of ResNet18 models trained on CIFAR-10 with surrogate objectivesin comparison to regularization with L2 weight-decay as non-IB method. The robustness is evaluatedusing FGSM, PGD, DeepFool and BasicIterative attacks of varying values.
Figure 6: Information plane plot of the training trajectories of ResNet18 models with the IE kZk2surrogate objective orL2 weight-decay on CIFAR-10. The color shows γ; the transparency the trainingepoch. Compression (Preserved Information J) trades-off with performance (Residual Information ]).
