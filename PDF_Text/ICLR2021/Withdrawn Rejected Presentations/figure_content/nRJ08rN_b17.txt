Figure 1: Illustration of the two separated pathways for information processing in the visual system.
Figure 2: Illustration of the two-pathway model. (A) The architecture of the model. The blue and or-ange blocks represent the feedforward convolutional layers in FineNet and CoarseNet, respectively.
Figure 3: Imitation learning from FineNet improves the performance of CoarseNet. (A-B): perfor-mances of CoarseNet trained on low-pass filtered images from CIFAR10. (A) Performances vs. thenumber of convolution channels. (B) Performances vs. the size of convolution kernel. (C-D): per-formances of CoarseNet trained on the binarized Pascalvoc-mask. (C) Performance vs. the numberof convolution channels. (D) Performance vs. the size of convolution kernel. See Appendix A forthe details of the training and testing data.
Figure 4: Component analysis of the two-pathway model. FN: FineNet. CN: CoarseNet. SAM:static associative memory. FBt the long-range feedback from CoarseNet and the higher layer ofFineNet to the second layer of FineNet; without FBtmeanS a short-range feedback to the third layerof FineNet. Red dashed line: the performance of the two-pathway model. (A) Model performanceswith respect to Gaussian, impulse and shot noises. (B) Model performances with respect to adver-sarial noise. Experimental details are the same as in Tab. 1.
Figure 5: The two-pathway model reproduces the visual backward masking phenomenon. (A) Aparadigm of the backward masking experiment, adapted from Macknik & Martinez-Conde (2007).
Figure 6: Data examples of the dataset Pascalvoc-mask. (A) left panel: a raw image and the corre-sponding objects; right panel: the segment of the raw image and the corresponding object segments.
Figure 7: Examples of noise disruptions used in the experiments. (A) Examples of visual inputsused for training FineNet and CoarseNet. From up to down, a raw image to FineNet, the corre-sponding low-pass filtered image (blurred) to CoarseNet, and the corresponding binarized image(mask data) to CoarseNet. (B-E) Different kinds of noise disruptions. (B) Examples of Gaussiannoise with std = 0.04, 0.3, 0.6, respectively. (C) Examples of shot noise with c = 100, 3, 1, re-spectively. (D) Examples of impulse noise with p = 0.07, 0.15, 0.3, respectively. (E) Adversarialnoise. Up: the adversarial noise of the example image in (A)-up, obtained by the Fast Gradient SignMethod (Goodfellow et al., 2014); Middle and down: the adversarial examples with the noise levelsof 0.1 and 0.5, respectively.
Figure 8:	Performance of the two-pathway model against the parameter β in SMA. CoarseNet andthe two-pathway model are trained under different β settings. When β = 100, both the two-pathwaymodel and CoarseNet which reads out via SAM achieve the best performances.
Figure 9:	Model performances against noises and adversarial noise perturbations. In (A-C), modelperformances against noise perturbations. FineNet and CoarseNet are trained independently on thedataset. (D) Model performances against adversarial noise perturbations. FineNet-tp,FineNet-FFLand FineNet-FFL means FineNet in the two-pathway model, FFL, and SFL, respectively. Noiseperturbation details are shown in Appendix A.2. Experimental details are the same as that in Tab. 1.
Figure 10:	Learning in RBM. (A) Diagram of a RBM with three visible and three hidden units.
Figure 11:	DMA implemented by RBM. (A) The training phase (see Fig. 10 for the details). (B)The retrivela phase. The coarse probe gc (X) of a visual object is fed into the visible layer of thetrained RBM and the prediction O(x) is retrieved through the dynamics of RBM.
Figure 12:	Illustration of the backward masking with our two-pathway model. (A) The effect ofa small target duration to the network (small SOA). (B) The effect of a large target duration to thenetwork (large SOA).
Figure 13: Performances of human subjects in the backward masking task. The figure is adaptedfrom Tang et al. (2018). (A) Categorization performance on the partial stimuli on a set of 16 exem-plars belonging to 4 categories (chance = 25%, the dashed line). 14 subjects participated in this task.
