Figure 1: Learning to navigate with languageconstraints. The figure shows (1) three types oflanguage constraints, (2) items which provide re-wards when collected, and (3) a third-person viewof the environment. The objective is to maximizetotal reward without violating text constraints.
Figure 2: Model overview. Our model consists of two components. (1) The constraint interpreter takes anatural language constraint X and an observation ot as inputs and produces a constraint mask MMC and cost Con-straint threshold prediction hC. (2) a policy network takes an environment embedding, a constraint mask MC,and a cost budget mask MMB that specifies cost satisfaction at each step as inputs and produces an embedding.
Figure 3: Constraint interpreter. (a) For the budgetary and relational constraints, a constraint mask moduletakes the environment embedding and text vector representation as inputs and predicts MMC .(b) For the Sequen-tial constraints, we use an LSTM to store the information of the past visited states. For example, if the agenthas visited 'water,, the constraint mask MMC will begin to identify the cost entity (i.e., ‘grass,). In addition, forthese three types of constraints, We use another LSTM given text X to predict hC. (BeSt viewed in color.)an action at . This policy architecture enables us to encode the language constraint into the policynetwork and hence learn a constraint-satisfying policy.
Figure 4:	(a) The cell-level accuracy (ACC) and area-under-curve (AUC) of predicting MC, and mean-square-error (MSE) of predicting hC over three types of constraints on the test set. We observe that POLCO achievessuperior results compared to the rule-based baseline. (b) Visualization of the constraint mask MC for thesequential constraints. We show the probability of predicting the cost entity. Our model can accurately locatethe cost entity given the language constraints. The first two views are mostly black because the agent spawnedfacing a wall. (Best viewed in color.)hC = 0	hC = 2	hC = 4JR(n)f ZC J δRC↑ JR (n)f δC J δRC↑ JR(n)f δC J δRC↑RW	0.1	23.4 -23.3		0.1	21.4 -21.3		0.1	19.4	-19.3CF w/ TRPO	3.4	35.6 -32.2		3.4	33.6	-30.2	3.4	31.6 -28.2	CF w/ PCPO	0.7	7.4	-6.7	0.7	5.4	-4.7	0.7	3.4	-2.7PN w/ TRPO	4.5	54.7 -50.2		^Γ5^	52.7 -48.2		4.5	50.7 -46.2	PN w/ FPO	1.8	0.0	1.8	1.8	-2.0	1.8	1.8	-4.0	1.8Ours	3.7	2.9	0.8	"3Γ^	2.1	1.8	4.5	1.8	2.7(b) Generalization results in Lavawall(a) Lavawall (Chevalier-Boisvert et al., 2018a)Figure 5:	(a) Lavawall environment: Reward function and transition dynamics are same as Hazard World,but map contains only ‘lava’ entities. (b) Generalization performance in Lavawall over the tested models andalgorithms. (Arrows denote higher or lower scores being better.)constraints. Finally, the PN agent trained with FPO has a low reward, because simply treating the
Figure 5:	(a) Lavawall environment: Reward function and transition dynamics are same as Hazard World,but map contains only ‘lava’ entities. (b) Generalization performance in Lavawall over the tested models andalgorithms. (Arrows denote higher or lower scores being better.)constraints. Finally, the PN agent trained with FPO has a low reward, because simply treating thecost penalty as the negative reward hinders the agent’s exploration.
Figure 6: AMT workers receive the general prompt and one of the three specific prompts. They arethen asked to instruct another person for the given situation. This ensures that the texts we collectedare free-form.
Figure 7: Description of the policy network in POLCO.
Figure 8: Description of the constraint interpreter.
Figure 9: Baseline model-ConStraint Fusion (CF). It is composed of two parts - (1) a CNN takesot as an input and produce a vector representation, (2) an LSTM takes x as an input and producea vector representation. We then concatenate these two vectors, followed by a MLP to produce anaction at .
Figure 10: Description of our baseline model-Constraint Fusion (CF).
Figure 11: Learning curves of training the policy network. The undiscounted reward, the undis-counted cost violations (i.e., ∆C = JC (π) - hC), and the number of steps over policy updatesfor the tested algorithms and the constrains. In the undiscounted cost violations plots, we furtherinclude the numbers for the interpreter pre-training stage in the first 100 points. This is equal to5000 trajectories. The maximum allowable step for each trajectory is 200. We observe that POLCOsatisfies the cost constraints throughout training while improving the reward. In contrast, the pol-icy network trained with TRPO suffers from violating the constraints and the one trained with FPOcannot effectively improve the reward. (Best viewed in color.)A.4 POLCO for Pixel Observations/3D Ego-centric ObservationsTo deal with pixel observations ot , we can still use the proposed architecture to process ot as shownin Fig. 12. To predict the cost constraint mask MC, We use the object segmentation method to getthe bounding box of each object in the scene. As a result, the area of that bounding box will be oneif there is a cost entity (i.e., the forbidden states mentioned in the text). OtherWise, the bounding boxcontains a zero. For MMBB, we can use a similar approach to compute the cumulative cost violationsat each step. In addition, to deal With navigation environments With 3D ego-centric observations,we propose shifting the ot , MC and MB matrices to be the first-person view. The bounding boxapproach for image case can still be applied here. We leave this proposal to future work.
Figure 12: POLCO for pixel observations and 3D ego-centric observations. The red cloud arearepresents the bounding box of each object in ot .
