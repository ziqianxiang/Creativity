Figure 1: Backup diagram of (a) One-StepOptimality Operator; (b) Multi-Step Optimal-ity Operator; (c) Greedy Multi-Step OptimalityOperator.
Figure 2: Update process of GM-Q learning With N = ∞. The Q value With purple backgroundmeans that the value is changed at corresponding iteration, While one With the yelloW backgroundmeans that the value is unchanged. γ is the discount factor.
Figure 5: (Left) The chosen step size of GM-Q learning duringtraining process. (Right) The distribution of the chosen step sizeover all data, in which the percentage of step size 1 is 77%.
Figure 6: Episode rewards ofGM-Q learning with differentinitialized Q values.
Figure 9:	Value function update process of Q learning. The Q value with purple background meansthat the value is changed at corresponding iteration, while one with the yellow background meansthat the value is unchanged. The initial Q values are 0, which are not plotted to make it look clear.
Figure 10:	Final value function Qfinal of the algorithms and the trajectory executed by policy πQfinalτo , ∏1(a.St) = Z and Z < 1 (as at are chosen by the behavior policy ∏o and usually ∏ι(at∣st) <∏o (at∣st)). The estimated value function of off-policy Monte Carlo is shown in Figure 10c.
