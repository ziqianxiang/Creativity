Figure 1: Error rate vs. depth of ResNet models trained with SRSGD and the baseline SGD with constantmomemtum. Advantage of SRSGD continues to grow with depth.
Figure 2: Comparison between different schemes in optimizing the quadratic function in (8) with (a)exact gradient, (b) gradient with constant variance Gaussian noise, and (c) gradient with decayingvariance Gaussian noise. NAG, ARNAG, and SRNAG can speed up convergence remarkably whenexact gradient is used. Also, SRNAG is more robust to noisy gradient than NAG and ARNAG.
Figure 3: (a) Training loss comparison between different schemes in training logistic regression forMNIST classification. Here, SGD is the plain SGD without momentum, and SGD + Momentum thatfollows (3) and replaces gradient with the mini-batch stochastic gradient. NASGD is not robust tonoisy gradient, ARSGD almost degenerates to SGD, and SRSGD performs the best in this case. (b, c)Training loss vs. training epoch of ResNet models trained with SRSGD (blue) and the SGD baselinewith constant momentum as in PyTorch implementation, which is denoted by SGD in Section 4 (red).
Figure 4: Test error vs. number of training epochs.
Figure 5: Training loss (left) and test error (right) of Pre-ResNet-290 trained on CIFAR10 withdifferent initial restarting frequencies Fi (linear schedule). SRSGD with small Fi approximates SGDwithout momentum, while SrSGD with large Fi approximates NASGD. The training loss curveand test accuracy of NASGD are shown in red and confirm the result of Theorem 1 that NASGDaccumulates error due to the stochastic gradients.
Figure 6: Cardinality of the set A := {k ∈ Z+∣Ef (wk+1) ≥ Ef (wk)} (Top panels) and the valueof R = Pk∈A (Ef (wk+1) — Ef (wk)) (Bottom panels). We notice that when the training getsplateaued, E(f (wk)) still oscillates, but the magnitude of the oscillation diminishes as iterationsgoes, which is consistent with our plots that the cardinality of A increases linearly, but R convergesto a finite number under different restart frequencies. These results confirm that our assumption inTheorem 2 is reasonable.
Figure 8: Earth Moving distance estimate (i.e. discriminator loss) vs. training epochs of WGAN withgradient penalty trained with SGD (red), SGD + NM (green), and SRSGD (blue) on MNIST.
Figure 9:	MNIST digits generated by WGAN trained with gradient penalty by SGD (left), SGD +NM (middle), and SRSGD (right).
Figure 10:	Test error vs. number of training epochs. Dashed lines are test errors of SGD trainedin 200 epochs for CIFAR10 and 90 epochs for ImageNet. For CIFAR, SRSGD with fewer epochsachieves comparable results to SRSGD with 200 epochs. For ImageNet, training with less epochsslightly decreases the performance of SRSGD but still achieves comparable results to 200-epochSGD.
Figure 11: Test error vs. number of epoch reduction in CIFAR100 training. The dashed lines aretest errors of the SGD baseline. For CIFAR100, SRSGD training with fewer epochs can achievecomparable results to SRSGD training with full 200 epochs. In some cases, such as with Pre-ResNet-290 and 1001, SRSGD training with fewer epochs achieves even better results than SRSGD trainingwith full 200 epochs.
Figure 12: Training loss (left) and test error (right) of Pre-ReSNet-110 trained on CIFAR10 withdifferent growth rate r (linear schedule). Here, we fix the initial restarting frequency Fi = 30 for alltrainings. Increasing the restarting frequency during training yields better results than decreasing therestarting frequency, but increasing the restarting frequency too fast and too much also diminishes theperformance of SRSGD.
Figure 13: Training loss (left) and test error (right) of Pre-ResNet-290 trained on CIFAR10 withdifferent initial restarting frequencies Fi (linear schedule). SRSGD with small Fi approximatessgd without momentum, while SRSGD with large Fi approximates NASGD.The training loss curveand test accuracy of NASGD are shown in red and confirm the result of Theorem 1 that NASGDaccumulates error due to the stochastic gradients.
Figure 14: Training loss and test error of Pre-ReSNet-290 trained on CIFAR100 with differentinitial restarting frequencies Fi (linear schedule). SRSGD with small Fi approximates SGD withoutmomentum, while SrSGD with large Fi approximates NASGD. The training loss curve and testaccuracy of NASGD are shown in red and confirm the result of Theorem 1 that NASGD accumulateserror due to the stochastic gradients.
Figure 15: Training loss and test error of ResNet-101 trained on ImageNet with different initialrestarting frequencies Fi. We use linear schedule and linearly decrease the restarting frequency to1 at the last learning rate. SRSGD with small Fi approximates SGD without momentum, whileSRSGD with large Fi approximates NASGD.
Figure 16: Test error when using new learning rate schedules with less training epochs at the 2nd and3rd learning rate for CIFAR10. We still train in full 200 epochs in this experiment. On the x-axis, 10,for example, means We reduce the number of training epochs by 10 at each intermediate learning rate,i.e. the 2nd and 3rd learning rate. The dashed lines are test errors of the SGD baseline.
Figure 17: Test error when using new learning rate schedules with less training epochs at the 2nd and3rd learning rate for CIFAR100. We still train in full 200 epochs in this experiment. On the x-axis,10, for example, means we reduce the number of training epochs by 10 at each intermediate learningrate, i.e. the 2nd and 3rd learning rate. The dashed lines are test errors of the SGD baseline.
Figure 18: Test error when using new learning rate schedules with less training epochs at the 2ndlearning rate for ImageNet. We still train in full 90 epochs in this experiment. On the x-axis, 10, forexample, means We reduce the number of training epochs by 10 at the 2nd learning rate. The dashedlines are test errors of the SGD baseline.
Figure 19: Trajectory through bad minima of SGD, SGD with constant momentum, and SRSGDduring the training: we train a neural net classifier and plot the iterates of SGD after each ten epoch(red dots). We also plot locations of nearby “bad” minima with poor generalization (blue dots). Wevisualize these using PCA and t-SNE embedding. The blue color bar is for the test accuracy of badlocal minima while the red color bar is for the number of training epochs. All blue dots for SGD withconstant momentum and SRSGD achieve near perfect train accuracy, but with test accuracy below59%. All blue dots for SGD achieves average train accuracy of 73.11% and with test accuracy alsobelow 59%. The final iterate (yellow star) of SGD, SGD with constant momentum, and SRSGDachieve 73.13%, 99.25%, and 100.0% test accuracy, respectively.
