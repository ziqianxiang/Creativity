Figure 1: Example distorted images (b-g). We show an example Gaussian-noise-added image (c)out of all 15 ImageNet-C types. See Fig. A4 for more examples.
Figure 2: Left: For each AlexNet conv1 filter (top row), we show the highest-correlated filter inAlexNet-R (bottom row), their Spearman rank correlation (e.g. r: 0.93) and the Total Variation (TV)difference (e.g. 22) between the top kernel and the bottom. Here, the TV differences are all positivei.e. AlexNet filters have higher TV. Right: conv1 filters of AlexNet-R are smoother and less diversethan the counterparts. Similar two plots for all 64 conv1 filters are in Figs. A2 & A3.
Figure 3: In each subpanel, one point shows the mean Total Variation (TV) of one channel whenrunning clean ImageNet-CL images and their noisy versions through AlexNet (teal) or AlexNet-R(yellow). R channels have similar TV before and after adding noise, suggesting that conv1 kernelsfilter out the added noise. In higher layers (conv3 and conv5), R channels are consistently moreinvariant to the input noise than S channels (yellow circles are clustered around the diagonal linewhile teal circles have higher variance). See Fig. A5 for the same scatter plot (a) for all five layers.
Figure 4: Left: For all three architectures, the numbers of color and texture detectors in R modelsincrease, e.g. by 117% and 7%, respectively, for AlexNet, while the number of object units decreasesby 28%. See Fig. A9 for layer wise plots for detectors in other category. Right: The average Shape(O) and Texture (A) scores over all channels in the entire network (“All") or in a NetDissect category(“Object”, “Color”, and “Texture”). While AlexNet-R has more color and texture channels (□ above□), these R channels are not heavily shape- or texture-biased. In contrast, the corresponding channelsin AlexNet are heavily texture-biased (Δ is almost2× of O).
Figure 5: Left: Top-25 highest-activation images of the AlexNet unit conv419, which has a NetDis-sect label of spiralled under texture category. The unit prefer circular patterns e.g. car wheels andclock. Right: Example cue-conflict images originally labeled as shape (top) or texture (bottom)but that were given a different label after the unit is ablated. Qualitatively, the unit helps AlexNetdetect clocks and cars using shapes (top row) and reddish pink cars, birds, chairs, and bicycles usingtextures (bottom row). The unit has Shape and Texture scores of 18 and 22. “clock2-bicycle1”: theimage has the shape of a clock and a texture of a bicycle. See Fig. A20 for a full version.
Figure A1: Standard models substantially outperform R models When tested on scrambled imagesdue to their capability of recognizing images based on textures. See Fig. A6 for examples of scram-bled images and their top-5 predictions from ResNet-R and ResNet (Which achieves a remarkableaccuracy of 94.77%).
Figure A2: conv1 filters of AlexNet-R are smoother than the filters in standard AlexNet. In eachcolumn, We shoW an AlexNet filter conv1 filter and their nearest filter (bottom) from the AlexNet-R.
Figure A3: All 64 conv1 filters of in each standard network (left) and its counterpart (right). Thefilters ofR models (right) are smoother and less diverse compared to those in standard models (left).
Figure A4: Applying different transformation that remove shape/texture on real images. We ran-domly show an example of 7 out of 16 COCO coarser classes. See Table 3 for classification ac-curacy scores on different images distortion dataset in 1000 classes(Except for Silhouette). *Note:Silhouette are validate in 16 COCO coarse classes.
Figure A5: Each point shows the Total Variation (TV) of the activation maps on clean and noisyimages for an AlexNet or AlexNet-R channel. We observe a striking difference in conv1: Thesmoothness ofR channels remains unchanged before and after noise addition, explaining their supe-rior performance in classifying noisy images. While the channel smoothness differences (betweentwo networks) are gradually smaller in higher layers, we still observe R channels are consistentlysmoother.
Figure A6: ResNet-R, on average across the three patch sizes, underperforms the standard ResNetmodel. Surprisingly, we observe that ResNet correctly classifies the image to their ground truth classeven when the image is randomly shuffled into 16 patches, e.g., ResNet classifies the 4 × 4 case ofrule, safe with 〜100% confidence. The results are consistent with the strong texture bias of ResNetand shape bias of ResNet-R (described in Sec. 3.2.1).
Figure A7: For each network, we show the number of channels in each of the 6 NetDissect categories(color, texture, etc) in Bau et al. (2017). Across all three architectures, R models consistently havemore color and texture channels while substantially fewer object detectors.
Figure A8: In each bar plot, we column shows the difference in the number of channels (betweenAlexNet-R and AlexNet) for a given concept e.g. striped or banded. That is, yellow bars (i.e.
Figure A9: In higher layers (here, conv4 and conv5), AlexNet-R have fewer object detectors butmore color detector units compared to standard AlexNet. The differences between the two networksincrease as we go from lower to higher layers. Because both networks share an identical architecture,the plots here demonstrate a substantial shift in the functionality of the neurons as the result ofadversarial training—detecting more colors and textures and fewer objects. Similar trends were alsoobserved between standard and R models of GoogLeNet and ResNet-50 architectures.
