Figure 1: Different types of architectures.
Figure 2: Comparison among OPS, DOL (Decentralized Online Learning) and COL (CentralizedOnline Learning)(d) stochastic=50%(n = 20, #Neighb.=10)where regularization coefficient λ is set to 10-4. ξi,t is the stochastic component of the function fi,tintroduced in Section § 3, which is encoded in the random data sample (Ai,t, yi,t). We evaluate thelearning performance by measuring the average loss1	nTnτ EΞn,τ E E fi,t (xi,t; ξi,t),i=1 t=1instead of using the dynamic regret (1) directly, since the optimal reference point x* is the same forall the methods. The learning rate γ in Algorithm 1 is tuned to be optimal for each dataset separately.
Figure 3: Evaluation on different network sizes and densitiestrust network under the setting of federated learning. OPS also works better than DOL-Asymm.
Figure 5: Evaluation on the Network DensityFigure 4: Evaluation on the Network Sizes(b) stochastic=50%B.2	Comparison with local online gradient descentTo justify the necessity of communication, we also compare OPS with the local online gradientdescent (local OGD), where every node trains a local model without communicating with others.
Figure 4: Evaluation on the Network Sizes(b) stochastic=50%B.2	Comparison with local online gradient descentTo justify the necessity of communication, we also compare OPS with the local online gradientdescent (local OGD), where every node trains a local model without communicating with others.
Figure 6: Comparison between OPS and Local OGD.
