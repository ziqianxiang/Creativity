Figure 1: A snippet that demonstrates how explicitly guarded code is often equivalent to code withsalient implicit, invariant-like conditions. The code on the right was a real (bug) that was patchedby adding the conditional check on the left. We synthesize such samples to train our model byselectively removing if-statements. Our model correctly predicted this repair.
Figure 2: An overview of our learning approach. We extract samples from if statements in Javamethods by removing the guard and assigning it (or its negation, for the else block) as the targetinvariant of the previously-guarded block (if and else blocks separately, if both present) for a trans-lator. We train using the cross-entropy of the predictions given the target, as well as the contrast ofthis entropy to that of predicting the logical inversion (per sample) in a hinge loss, which encouragesB odyGuard to distinguish between syntactically similar, but logically distinct invariants.
Figure 3: Schematic overview of our model. Both encoder and decoder use 8 Transformer layers. In-put is provided as BPE tokens (not shown) augmented with program-graph edge information, whichthe encoder uses through relational self-attention from (Hellendoorn et al., 2020). The decoder usesboth masked self-attention and input attention biased towards the target scope (bold and underlined).
Figure 4: Model performance during and after training, focusing on the high-precision/low-recalldomain for the second (overall test accuracies: 33.8% base, 34.7% hinge-loss).
Figure 5: Results of the overlap and validity analysis of our invariants based on Daikon-extractedtrace data. Note the log-scaling on the x-axis.
