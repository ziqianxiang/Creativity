Figure 1: FOSAE++’s learning task compared to the action modeling by humans. All symbols areanonymous (gensym’d) due to the unsupervised nature. The produced PDDL files are then used toreason in the environment using classical planners.
Figure 2: Cube-Space AutoEncoder, Back-To-Logit, and First-Order State AutoEncoder.
Figure 3: move(c,a) should not affect p,s groundings with b and d, therefore BIND limits the ground-ing to a and c. move(c,a) and move(a,c) can then apply the same effect to each subspace due to theaxis reordering. (Note: move(7block, ?from,?to) is simplified to move(?from,?to) for illustration.)the objects that does not appear in the parameter list, e.g., action (move a b c) cannot affect (on d c)because d {a, b, c}. We represent this restriction as differentiable matrix operations.
Figure 4: (top) State transitions in Blocksworld and 8-puzzle, and the initial/goal state of a Soko-ban problem. In Sokoban and 8-puzzle, each tile is represented as an object. (bottom) Examplerendering of the ground-truth / reconstruction / errors. FOSAE++ reconstructs rectangular imagesand bounding boxes, then draw them on a black canvas. Due to the object-based input, it does notproduce the background. The depicted Sokoban reconstruction consists of 63 tile objects.
Figure 5: (Synopses:) The networks are trained on X objects, then tested on Y objects. (Top:Interpolation) 8-Puzzle: X = 9, Y ∈ {3,4,5,6,7}. Sokoban: X = 46, Y ∈ {13, 18, 23, 27, 32}.
Figure 6:	NLM uses EXPAND, REDUCE, PERMUTE tensor operations to combine predicates of dif-ferent arities while maintaining the object-ordering / size invariance. (For matrices, PERMUTE isequivalent to transposition.)proposed both min and max versions to represent ∀ and ≡, with enough number of layers only oneof them is necessary because Vx;p(-, x) = -^≡x;	x).
Figure 7:	Back-To-Logit architecture (repost of Fig. 2).
Figure 8: (Left) A graph representing a 3-dimensional cube which is a cube-like graph. (Right) Agraph whose shape is identical to the left, but whose unique node embeddings are randomly shuffled.
Figure 9: (left): An undirected graph consisting of 5 disconnected 2-star graphs It has c(G) = 2.
Figure 10:	The illustration of State AutoEncoder, Action AutoEncoder, and the end-to-end combi-nations of the two.
Figure 11:	A naive and a Back-to-Logit implementation of the apply module of the Cube-SpaceAE.
Figure 12:	Data array representing a single object.
Figure 13:	The encoder has two stages. The first stage extracts a compact but still continuous repre-sentation of each object. From this compressed representation, the second stage identifies propertiesof objects as well as relationships/predicates between objects.
Figure 14: params network using NLMs.
Figure 15: Training curves of 3 runs on the same optimized parameter found by the parameter tuner.
Figure 16: Visualized sokoban problems. The first 5 are used for the training, and the rest are usedfor evaluation. Pink dots depict the goals that the player pushes the blocks onto. Green boxes arealready on one of the goals.
