Figure 1: (a) Illustration of the message passing of intention propagation Λθ(a|s) (equation 5). (b)An instance of 2-layer GNN with the discrete action outputs (n agents).
Figure 2: Experimental scenarios. Cityflow: Manhattan, Predator-Prey and Cooperative-Navigation.
Figure 3: Performance on large-scale traffic lights control scenarios in CityFlow. Horizontal axis:environmental steps. Vertical axis: average episode reward (negative average travel time). Higherrewards are better. Our intention propagation (IP) performs best especially on large-scale tasks.
Figure 4: Experimental results on Cooperative Navigation, Heterogeneous Navigation, Prey andPredator. Our intention propagation (IP) beats all the baselines.
Figure 5: illustrate the message passing in intention propagation network Λθ(a|s).
Figure 6: Details of the graph neural networkmessage passing on state, since in practice the global state is not available. In the second layer, wedo similar things. Agent 1 receives the embedding information of a2 from its neighbors and get anew embedding μ2. Then this embedding passes a MLP+softmax layer and output probability ofaction, i.e. qι(αι∣s).
Figure 7: (a) a toy task on 2d-grid. (b) The performance of independent policy and intention propa-gation.
Figure 8: PerformanCe of the proposed method based on different ablation settings. (a) TraffiCgraph and fully ConneCted (fC) graph on CityFloW (N=49). (b) TraffiC graph and fully ConneCted (fC)graph on CityFloW (N=100). (C) Cooperative Nav. (N=30): Different number of neighbors. (d)Cooperative Nav. (N=30): Different hop size graph neural netWorks. (e) Cooperative Nav. (N=30):ConstruCt random graph vs k-nearest-neighbor graph (k = 3, 8, 10). (f) Cooperative Nav. (N=30):Update 8-nearest-neighbor graph every n environment steps (5 and 10 respeCtively.).
Figure 9: Further experimental results. Cooperative navigation (N=30) with assumption violation.
Figure 10: PerformanCe of different methods on traffiC lights Control sCenarios in CityFlow envi-ronment: (a) N=16 (4 × 4 grid), Gudang sub-distriCt, Hangzhou, China. (b) N=49 (7 × 7 grid), (C)N=96 (irregular grid map), Manhattan, USA. (d) N=100 (10 × 10 grid), (e) N=225 (15 × 15 grid),(f) N=1225 (35 × 35 grid). The horizontal axis is time steps (interaCtion with the environment). ThevertiCal axis is average episode reward, whiCh refers to negative average travel time. Higher rewardsare better. The proposed intention propagation (IP) obtains muCh better performanCe than all thebaselines on large-sCale tasks .
Figure 11: Comparison on instances with different number of agents: cooperative navigation (a)N=15, (b) N=30 and (c) N=200 respectively. (d) jungle (N=20, F=12), (e) prey and predator(N=100), and (f) cooperative push (N=100). The horizontal axis is environmental steps (numberof interactions with the enviroment). The vertical axis is average episode reward. The larger averagereward indicates better result. The proposed intention propagation (IP) beats all the baselines ondifferent scale of instances.
Figure 12: Policy interpreation on CityFlow task (N=49)22Under review as a conference paper at ICLR 2021Table 2: HyperparametersParameter	Valueoptimizer	Adamlearning rate of all networks	0.01discount of reward	0.95replay buffer size	106max episode length in MAgent	25max episode length in MPE, CityFlow	100number of hidden units per layer	128number of samples per minibatch	1024nonlinearity	ReLUtarget smoothing coefficient (T)	0.01target update interval	1gradient steps	8regularizer factor(α)	0.2I DerivationI.1 Proof of proposition 1
