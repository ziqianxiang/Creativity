Figure 1: Possible regularization functions fι(∙). From left to right, triangular, L2-inspired,piece-wise parabola and a 4th degree polynomial.
Figure 2:	Example of implementation of Binary Neural Networks and Sparse Binary Neural Net-works. The threshold in Binary Neural Networks is d ninput' ], with ninputs the dimension of eachinput sample at a given layer. In Sparse Binary Neural Networks, this accounts to dn1s], with nisthe number of connected inputs at each output neuron.
Figure 3:	Effective Connections (EC) as a function of the initial number of connections (p) in aSparse Binary Neural Network with 3 feed-forward hidden linear layers with 1024 neurons each,trained on MNIST dataset. Left. P = 0.1 % (O), P = 1.0 % (□), P = 10.0 % (4). Center. P = 50.0 %(*) and P = 100.0 % (×). Right. Corresponding test accuracy.
Figure 4:	Left. Effective Connections (EC) as a function of the learning rate (Ir) in a Sparse BinaryNeural Network with 3 feed-forward hidden linear layers with 1024 neurons each, trained on MNISTdataset. Each curve represents a different learning rate, lr = 0.01 (O), lr = 0.1 (□), lr = 1.0 (4), andlr=10.0 (*). Right. Corresponding test accuracy.
Figure 5:	Effects of fι and f2 in sparsity and test accuracy of a SBNN with convolutional topologytrained on CIFAR-10. Left. Effective Connections (EC). Right. Corresponding test accuracy.
Figure 6: Schematics of the index encoder and of the run-length encoder used to compress the sparsebinary weigth matrices这 Aoffl,lnooffl1S≡Figure 7: The blue curve shows the test accuracy as a function of dropout probability (pd∙op) in the3L-BNN with dropout, trained on MNIST dataset. The red curve represents the accuracy ofa SparseBinary Neural Network with same topology, no dropout.
Figure 7: The blue curve shows the test accuracy as a function of dropout probability (pd∙op) in the3L-BNN with dropout, trained on MNIST dataset. The red curve represents the accuracy ofa SparseBinary Neural Network with same topology, no dropout.
