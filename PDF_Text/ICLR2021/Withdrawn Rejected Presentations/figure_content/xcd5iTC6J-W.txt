Figure 1: Structure of the hidden Markov recurrent neural network (HMRNN). Solid lines indicatelearned weights that correspond to HMM parameters; dotted lines indicate weights fixed to 1. Theinner block initializes with the initial state probabilities then mimics multiplication by diag(Ψyt );connections between blocks mimic multiplication by P .
Figure 2: Estimated pii (left) and ψii (right) under Baum-Welch and HMRNN, shown by groundtruth parameter value. Results for each column are averaged across 9 simulations. Dashed linesindicate ground truth pii (left) and ψii (right) values, and error bars indicate 95% confidence intervals(but do not represent tests for significant differences). In line with Theorem 3.1, Baum-Welch and theHMRNN produce near-identical parameter solutions according to the Wasserstein distance metric.
Figure 3: Average probability placed on final MMSE scores, by score category. Recall that theHMRNN’s average performance significantly outperforms Baum-Welch (paired t-test p-value=2.396 × 10-6). As see in the Figure, this effect is consistent across score categories. Error barsindicate 95% confidence intervals, and do not represent tests for significant differences.
