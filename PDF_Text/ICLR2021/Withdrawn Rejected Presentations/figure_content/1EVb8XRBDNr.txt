Figure 1: CVaRThe α-percentile value is value at risk (VaR). For easeof notation, we write CVaR as a function of the CDF F,CVaRα(F).
Figure 2: The framework of RMIX (dotted arrow indicates that gradients are blocked during train-ing). (a) Agent network structure (bottom) and risk operator (top). (b) The overall architecture. (c)Mixing network structure. Each agent i applies an individual risk operator Παi on its return distri-bution Zi(∙, ∙) to calculate Ci(∙, ∙, ∙) for execution given risk level ai predicted by the dynamic risklevel predictor ψi. {Ci(∙, ∙, ∙)}n=-1 are fed into the mixing network for centralized training.
Figure 3: Agent architecture.
Figure 4: Risk level predictor ψi .
Figure 5: SMAC scenarios: 27m_vs_30m, 5m_vs_6m, 6h_vs_8z, corridor and MMM2.
Figure 6:	test_battle_won_mean summary of RMIX and QMIX on 17 SMAC scenarios.
Figure 8: test_battle_won_mean ofRMIX, QMIX, Qatten and MAVEN on veryhard corridor scenario.
Figure 7:	test_battle_won_mean of RMIX and baselines on 8 scenarios, the x-axis denotesthe training steps and the y-axis is the test battle won rate, ranging from 0 to 1. It applies for resultfigures in the rest of the paper, including figures in appendix.
Figure 9:	test_battle_won_mean of RMIX vs QR-MIX and QMIX.
Figure 10:	test_battle_won_mean of RMIX and baselines on 4 scenarios.
Figure 11: test_battle_won_mean of RMIX,RDN, QMIX and VDN on 5 SMAC scenarios.
Figure 12: The overall setup of QMIX (best viewed in colour), reproduced from the originalpaper (Rashid et al., 2018) (a) Mixing network structure. In red are the hypernetworks that producethe weights and biases for mixing network layers shown in blue. (b) The overall QMIX architecture.
Figure 13: test_battle_won_mean of RMIX, RMIX (α = 1) and QMIX on 3s5z_vs_3s6z(heterogeneous and asymmetric scenario, very hard game).
Figure 14: test_battle_won_mean of RMIX, RMIX (α = 1) and QMIX on 6h_vs_8z (homo-geneous and asymmetric scenario, very hard game)training stepsFigure 15: test_battle_won_mean of RMIX, RMIX (α = 1) and QMIX on 27m_vs_30m(homogeneous and asymmetric scenario, very hard game).
Figure 15: test_battle_won_mean of RMIX, RMIX (α = 1) and QMIX on 27m_vs_30m(homogeneous and asymmetric scenario, very hard game).
Figure 16: test_battle_won_mean of RMIX-static.
Figure 17: RMIX results analysis on corridor. We use trained model of RMIX and run the model to collect one episode data including game replay, states, actions,rewards and α values (risk level). We show rewards of one episode and the corresponding α value each agent predicts per time step in row one and row two. Weprovide description and analyses on how agents learn time-consistency α values for the rest rows. Pictures are screenshots from the game replay. Readers can watchthe game replay via this anonymous link: https://youtu.be/J-PG0loCDGk. Interestingly, it also shows emergent cooperation strategies between agents atdifferent time step during the episode, which demonstrate the superiority of RMIX.
Figure 18: test_battle_won_mean of RMIX vs QR-MIX and QMIX.
Figure 19:	test_battle_won_mean of RMIX, MAVEN, QMIX, Qatten, IQL, QTRAN, VDNand COMA on 17 SMAC StarCraft II scenarios.
Figure 20:	test_return_mean of RMIX, MAVEN, QMIX, Qatten, IQL, QTRAN, VDN,COMA on 17 SMAC StarCraft II scenarios.
