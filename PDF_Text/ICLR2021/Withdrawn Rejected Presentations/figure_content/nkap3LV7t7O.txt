Figure 1: Different types of calibrated decoders for Gaussian VAE, model parameters are denotedwith enclosing squares. Left: both the mean μ and the variance σ are output by a neural network withparameters θ. Center: σ-VAE with shared variance, the mean is output by a neural network withparameters θ, but the variance it iself a global parameter. Right: the proposed optimal σ-VAE, themean is output by a neural network with parameters θ, and the variance is computed analyticallyfrom the training data D.
Figure 2: Images or videos (bottom right) sampled from the proposed optimal σ-VAE and a unitvariance Gaussian VAE models. The Gaussian VAE does not have a means to control the expressivityof the latent variable and produces suboptimal, blurry samples. The σ-VAE controls the expressivityby learning a calibrated decoder, and produces higher quality sequences on all datasets.
Figure 4: Comparison of β-VAE and σ-VAE onSVHN in terms of mutual information Ie (x; z)and marginal KL divergence KL(m(z)||p(z)) (seeSec. 5.2). Ie(x; z) increases with lower β, yieldingexpressive representations and better reconstruc-tion. However, after a certain point, lowering βleads to a rapid increase in the marginal KL, yield-ing poor samples from the prior. The σ-VAE isable to automatically find the inflection point afterwhich the marginal KL begins to increase, cap-turing as much information as possible while stillproducing good samples.
Figure 5: Samples from the σ-VAE (left) and the Gaussian VAE (right) on the SVHN dataset. TheGaussian VAE produces blurry results with muted colors, while the σ-VAE is able to produce accurateimages of digits.
Figure 6: Samples from the σ-VAE (left) and the Gaussian VAE (right) on the CelebA dataset, imagescropped to the face for clarity. The Gaussian VAE produces blurry results with indistinct face features,while the σ-VAE is able to produce accurate images of faces.
Figure 7: Samples from the σ-VAE (top) and the Gaussian VAE (bottom) on the BAIR dataset.
Figure 8: Samples from the σ-VAE (left) and the Gaussian VAE (right) on the challenging CIFARdataset. The Gaussian VAE produces blurry results with muted colors, while the σ-VAE models thedistribution of shapes in the CIFAR data more faithfully.
Figure 9: Variance convergence speed on SVHN. We see that the shared σ-VAE which optimizes thevariance with gradient descent has an initial period of convergence when the variance converges tothe region of the optimal value. In contrast, σ-VAE with analytical (optimal) variance quickly learns agood estimate of the variance, which leads to better performance. The unit variance Gaussian β-VAEcan be interpreted as having a constant variance determined by β, shown here. Since the variancedoesn’t change throughout training, it achieves suboptimal performance.
