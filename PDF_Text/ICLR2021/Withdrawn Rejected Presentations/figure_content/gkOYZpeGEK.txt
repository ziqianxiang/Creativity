Figure 1: Points classifica-tion using Spheres dataset.
Figure 2: An illustration of UMATO pipeline using 10,000 data points (101 dimensions) of theSpheres. (A) UMATO first initializes hub points using PCA, (B) then optimize their positions usingthe cross entropy function. (C) Next, we embed the expanded nearest neighbors to the projectionand optimize their positions using sampling techniques for acceleration. (D) Lastly, we append theoutliers and achieve the final projection result. Best viewed in color.
Figure 3: 2D projections produced by UMATO and six baseline algorithms. t-SNE, At-SNE,and UMAP showed as if the points from a surrounding sphere were attached to inner spheres, notreflecting the dataâ€™s global structures. PCA, Isomap and topological autoencoders attempted to pre-serve the global structures, but failed to manifest the complicated hierarchical structures. UMATOwas the only algorithm to capture both the global and local structures among all different sphereclasses; this is best viewed in color.
Figure 4: UMATO results on the Spheres dataset using different initialization methods. Al-though the average value of the normalized Procrustes distance of UMATO results is higher than thebaselines because of the equidistant clusters of inner spheres, both global and local structures arewell-captured with all different initialization methods. Best viewed in color.
Figure 5: 2D projections of the Fashion MNIST dataset using UMATO and UMATO withmulti-phase optimizations. Although there was a small difference such as the locations of outliers,we observed that the projection results were quite similar to each other.
Figure 7: 2D projections produced by UMATO and six baseline algorithms UMATO generatedsimilar projections to PCA but with the points more locally connected; this is best viewed in color.
Figure 8: Comparing result of UMATO and UMAP with varying number of epochs (Toprow) UMAP is susceptible to the number of epochs so that the clusters get dispersed as the epochsincreases. (Bottom row) On the other hand, regardless of the number of epochs in the global opti-mization, UMATO results in almost the same embedding result.
Figure 9: The illustration of overall UMATO pipeline.
Figure 10: The effect of manipulating local learning rate of UMATO. Local learning rate wasset to 0.1 for all cases. Unlike previous embedding results in Figure 7, UMATO reveals more of thelocal aspects.
