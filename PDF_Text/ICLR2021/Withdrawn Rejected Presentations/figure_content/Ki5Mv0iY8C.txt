Figure 1: To visualize how 1 一 η(S(φ(θ, x))) represents the confidence of a model,s prediction weplot 1 一 η (a) for all a ∈ R3 such that a is a valid probability distribution over three classes (i.e. forall the elements of the standard 2-simplex). Since there are only two free variables, X and y in theplot represent a by a = [x, y, 1 - x - y]. We see that 1 - η(a) is only zero when a = ei for some i,namely when a model would be most confident. We also note that 1 - η(a) is largest when a modelwould be least confident in its prediction-i.e. when a = [1/3,1/3,1/3].
Figure 2: We consider the affine predictor φ((θ, b), x) = θtx + b with arbitrarily chosen parameters(θ, b) on 1, 000 randomly sampled datapoints from the MNIST dataset. Each scatter point representsa datapoint for which we compute the ACEHT and the empirical Hessian trace. The linear relation-ship between the ACEHT distribution and empirical Hessian trace both confirms our derivation ofthe ACEHT and provides a baseline for the numerical methods used for the empirical Hessian trace.
Figure 3: We observe how ACEHT and the softmax margin (SM) bound relate, via the PearsonCorrelation Coefficient, to the observed Hessian trace (HT) of LeNet trained on MNIST. In Figures3a 3b we see that for correctly classified datapoints (orange) the empirical Hessian trace correlateswell with the ACEHT and SM bound. In Figure 3c we observe that the increase in correlation occurswith an increase in training accuracy. To demonstrate the evolution of the distributions throughouttraining we plot the ACEHT and empirical HT distribution against each other in Figures 3d 3e 3f. Weobserve that while the most apparent outliers were removed, some still skew the linear regression.
Figure 4: We plot both small-batch method (orange) and large-batch method (blue). In Figures4a and 4b we plot the output margins against the Hessian trace for each datapoint. We observe astrong relationship between the Hessian trace and the output margins. In Figure 4c we plot both thehistogram and box-plot and display the skewness (the third standardized moment) for both the largeand small-batch method’s margin distributions. We observe that the distribution of the small-batchmethod is more left skewed which would indicate better generalizability independent of the flatness.
