Figure 1: Controllable ParetoMTL allows praCtitioners to Con-trol the trade-offs among tasks inreal time with a single model,whiCh Could be desirable for manyMTL appliCations.
Figure 2: Controllable Pareto MTL can directly generate a corresponding Pareto stationary solu-tion based on a given preference among tasks. (a) A practitioner adjusts the m-dimensional pref-erence vector p to specify a trade-off preference among m tasks. (b) A trained Pareto solutiongenerator directly generates a Pareto solution θp conditioned on p. (c) The generated solution θpshould have objective values L(θp) that satisfies the trade-off preference among m tasks. The num-ber of possible preference vectors and the corresponding Pareto stationary solutions could be infinitefor an MTL problem. We develop the idea in Section 4 and propose the MTL model in Section 5.
Figure 3: An MTL problem isdecomposed by a set of unitvectors in the loss space.
Figure 4: The Controllable Pareto Multi-Task Network: (a) The main MTL network has a fixedstructure, and all its parameters are generated by the hypernetwork conditioned on the preferencevector. (b) With the chunking method, the hypernetwork can iteratively generate parameters fordifferent parts of the main network.
Figure 5: Parameter Generation: The hypernetwork takes a preference vector p as input and gener-ates the parameter θp,k,j for part of the main MTL network, where e = Pim=1 piei is the preferenceembedding, ck is the chunk embedding, FC is the fully connected layers, a is the generated vector,and Wj is a parameter tensor. All blocks in color contain trainable parameters for the hypernetwork.
Figure 6: The algorithm framework and the continually learned trade-off curve for the trainingloss on MultiMNIST problem: Our proposed algorithm optimizes the hypernetwork parameters forall valid preferences, and continually learns the trade-off curve during the optimization process.
Figure 7: Point Set Approximation and Generated Pareto Front. (a) & (b): Traditional multi-objective optimization algorithms can only find a finite point set to approximate the Pareto set andPareto front. In addition, the simple linear scalarization method can not cover most part of the Paretoset/front when the ground truth Pareto front is a concave curve. (c) & (d): Our proposed controllablePareto MTL method can successfully generate the whole Pareto front from the preference vectors.
Figure 8:	Results on MultiMNIST, CityScapes and NYUv2: Our proposed algorithm can generatethe Pareto front for each problem with a single model respectively. For CityScapes and NYUv2, wereport pixel accuracy for segmentation and (1 - relative error) for depth estimation.
Figure 9:	The results of CIFAR-100 with 20 Tasks: The results for each baseline method is a singlesolution. Our proposed algorithms switch to the corresponding preference when making predictionfor each task (e.g., preference on task 1 for making prediction on task 1).
Figure 10: Results on NYUv2with ResNet-50 Backbone: Theproposed model can generate rea-sonable trade-off curves.
Figure 11: (a) Controllable Pareto MTL network with a pretrained feature extractor: The pretrainedencoder is preference-agnostic, and the hypernetwork generates the rest parts of the main MTLnetwork. (b) The generated MTL models share the same parameters for part of the encoder.
Figure 12: The training process of the Pareto solution generator. At each iteration, the proposedalgorithm randomly samples a preference vector and a set of reference vectors to decompose theloss space, and calculates a valid gradient direction to update the Pareto solution generator. Thealgorithm iteratively learns and improves the generated manifold during the optimization process.
Figure 13: For a preference vector,the valid gradient direction shouldreduce all losses and activated con-straints for the generated solution.
Figure 14: The training process of the Pareto solution generator with multiple preferences. Ateach iteration, the proposed algorithm randomly samples multiple preference vectors, and calculatesa valid gradient direction which can improve the performance for all subproblems.
