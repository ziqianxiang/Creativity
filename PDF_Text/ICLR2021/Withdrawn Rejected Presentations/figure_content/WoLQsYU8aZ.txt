Figure 1:	Basic Usage of Gymimport gymenv = gym.make(’CartPole-v0’)observation = env.reset()for _ in range(1000):env.render()action = policy(observation)observation, reward, done, info = env.step(action)env.close()3Under review as a conference paper at ICLR 2021Figure 2:	Basic Usage of PettingZoofrom pettingzoo.butterfly import pistonball_v0env = pistonball_v0.env()env.reset()for agent in env.agent_iter(1000):env.render()observation, reward, done, info = env.last()action = policy(observation, agent)env.step(action)
Figure 2:	Basic Usage of PettingZoofrom pettingzoo.butterfly import pistonball_v0env = pistonball_v0.env()env.reset()for agent in env.agent_iter(1000):env.render()observation, reward, done, info = env.last()action = policy(observation, agent)env.step(action)env.close()5.2	PARALLEL APIIn addition to our main API, for certain environments we offer a separate API based off of thePOSG model. It supports games and methods that assume this model very nicely. In style, it isvery similar to RLlib’s multi-agent API (Liang et al., 2018), accepting and returning dictionarieskeyed on agent names. The primary motivation for including this secondary API is because in gameswhere it’s applicable, it can allow for parallelization features which can lead to large performanceimprovements.
Figure 3: Example Environments From Each Class(b) Butterfly: PistonballrnbqkbnrPPPPP-PP.............P	. QP P P P . P P PRNB.KBNR(d) MAgent: Adversarial Pursuit(c) Classic: Chess(e) MPE: Simple Adversary(f) SISL: Multiwalkerdespite their popularity, have previously only existed as unmaintained “research grade” code, havenot been available for installation via pip, have required large amounts of maintenance to run at all,and have required large amounts of debugging, code review, code cleanup and documentation tobring to a production-grade state. The Multi-Agent Atari and Butterfly classes are new environmentsthat we believe pose important and novel challenges to multi-agent reinforcement learning. Finally,we include the Classic class — classic board and card games popular within the RL literature.
Figure 4: The beginning PettingZoo documentation for the Go environment, illustrating how we usedthe design metaphor of a Wikipedia page to include a large amount of detail in a manner that isn’toverwhelming8	BaselinesAll environments implemented in PettingZoo include baselines to provide a general sense of thedifficulty of the environment, and for something to initially compare against. We do this here for the7Under review as a conference paper at ICLR 2021Butterfly environments that this library introduces for the first time; similar baselines exist in thepapers introducing all other environments. We used parameter sharing (Terry et al., 2020c; Guptaet al., 2017) with Ape-X DQN (Horgan et al., 2018), with RLLib (Liang et al., 2018). Our resultsare shown in Figure 5. Preprocessing and hyperparameter details are included in Appendix A. Allpreprocessing was done with the SuperSuit wrapper library (Terry et al., 2020a), which has recentlyadded support for PettingZoo based multi-agent environments based. Code for the environments,training logs, and saved policies are available at https://github.com/pettingzoopaper/pettingzoopaper.
Figure 5: Total reward when learning on each Butterfly environment via parameter shared Ape-XDQN (a-d) and parameter shared PPO (e).
