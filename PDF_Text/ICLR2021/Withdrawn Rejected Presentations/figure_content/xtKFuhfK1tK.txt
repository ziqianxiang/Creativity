Figure 1: An example of distributed GCN training. Left: A graph with 8 nodes are divided into fourparts and stored on four GPUs. Right: For a two-layer gCn, to compute the embedding of nodeA, We need the feature vectors of node A, B, C, E and F; to compute the embedding of node B, Weneed the feature vectors of node B, C, D, E, F, G. Nodes that are not on the same GPU need to betransferred through the GPU connections.
Figure 2: Training loss over epochson Cora.
Figure 3: Results on Reddit graph.
Figure 4: Results on YouTube graph.
Figure 5: Results on Amazon graph.
