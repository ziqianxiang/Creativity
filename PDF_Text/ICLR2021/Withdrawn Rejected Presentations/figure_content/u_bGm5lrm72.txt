Figure 1: Training pipelineof the pre-activation distribution at each layer. The pre-activation for each neuron is the weighted sumof inputs Pj oj wij received by the neuron. During threshold computation, the leak in the hiddenlayers is set to unity, and the input layer employs direct input encoding. Finally, the converted SNNis trained with error-backpropagation to optimize the weights, the membrane leak, and the firingthresholds of each layer as described by the equations in Section 3. We evaluated the performance ofthese networks on VGG and ResNet architectures for CIFAR and ImageNet datasets. Column-2 inTable 1 shows the ANN accuracy; column-3 shows the accuracy after ANN-SNN conversion with Ttimesteps; column-4 shows the accuracy when only the weights in SNN are trained with spike-basedbackpropagation; column-5 shows the accuracy when the weights, threshold, and leak are jointlyoptimized (DIET-SNN). The performance of DIET-SNN compared to current state-of-the-art SNNsis shown in Table 2. DIET-SNN shows 5 - 100Ã— improvement in inference latency compared toother spiking networks. The authors in Wu et al. (2019b) achieved convergence in 12 timesteps witha special input encoding layer, but the extra overhead may affect the overall latency and energy.
Figure 2: (a) Layerwise spike rate for VGG16 during inference over entire test-set. Average spikerate is calculated as total #spikes/#neurons. An average spike rate of 1.65 indicates that every neuronfired on average 1.65 times for each image over all timesteps. (b) Layerwise leak and threshold forVGG16 on CIFAR100 dataset. The threshold before training represents the values obtained fromANN-SNN conversion process. The leak before training is unity for all layers.
