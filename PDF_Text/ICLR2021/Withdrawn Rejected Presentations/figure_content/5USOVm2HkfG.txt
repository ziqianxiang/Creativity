Figure 1: (a) Overall architecture, using the state embedding function φ, the internal policy ∏i, andthe action mapping function f . (b) Embedding model for learning φ and the auxiliary action em-bedding function g (the red arrows denote the gradient flow of the supervised loss in Equation (5)).
Figure 2: State and action embeddings for a discrete state gridworld with 1, 600 states and 25 * * * 9 actions(9 actuators). The embedding dimension for both states and actions is set to 2 for visualizationpurposes. (a) Original environment, i.e., a discrete gridworld as described in Section 5.1. The bluearea is the goal state and the red dot is the starting position of the agent. (b) Learned state embeddingsin 2-dimensional state embedding space. (c) Displacement in the Cartesian co-ordinates (c1 and c2)caused by actions. Each action is colored according to this displacement with [R = ∆c1, G =∆c2, B = 0.5]. (d) Learned action embeddings in continuous action embedding space.
Figure 3: Performance of our approach (JSAE, Joint State Action Embedding) compared againstbenchmarks without embeddings and/or benchmarks with action embeddings by RA (all results arethe average return over 10 episodes, with mean and std. dev. over 10 random seeds).
Figure 4: Illustration of πi0 in the combined model.
Figure 5: Gridworld environment as used for evaluation The goal state is in blue, the agent is in red,and the obstacles are shown in gray.
