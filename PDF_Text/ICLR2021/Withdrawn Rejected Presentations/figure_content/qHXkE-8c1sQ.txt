Figure 1: Distance dp changes with respect to changes in the parameterization of the networks. Wemeasure the distance between a pair of ResNet-56 models θA and θB, trained on two subsets ofTiny-ImageNet, respectively. We modify the configuration of θA while keeping θB the same. Mod-ifications include: scaling, where we multiply the weights of the network by a coefficient c, neuronswapping, where we randomly permute a fraction c of the units within a layer, perturbation, whereGaussian random noise of zero mean and standard deviation c is added to the weights of the network,and adversarial perturbation, where a vector of standard deviation c is added to the weights of thenetwork to maximize the change of second-to-last layer representations. We also include training θAwith different initializations and using different network architectures. Some commonly used dis-tance measures are used as baselines. These include measures in three different spaces: parameterspace distances, which measure distance using metrics on the parameter matrices, including plainL2 distance, cosine similarity, and Earth Mover Distance (EMD) (Monge, 1781; Rubner et al., 1998)that computes the cost to align neurons. Representation space distances measure distance on thesecond-to-last layer representation vectors, including plain L2, cosine, the EMD cost of aligningfeature dimensions, and Linear Centered Kernel Alignment (CKA) (Kornblith et al., 2019) which isbased on pairwise sample similarity and outperforms previously proposed similarity measures. Foroutput space distance, we use the KL-divergence between the probability distribution generated bythe final softmax layer. The baseline measures include both common and straightforward measuresand more sophisticated measures like EMD and CKA that possess invariance properties. Underlinedlabels on the x-axis denote the distances measured between unmodified θA and θB . Curves for each
Figure 2: Distance from the interpolation model θ to θB . Left: θA and θB are trained on differentbut related tasks (first 100-class and last 100-class of Tiny-ImageNet) from the same initialization.
Figure 3: Distance from the i-th epoch model θi to the initial model θ0 (left) and the final model θ14(middle). Right: whether the measure has monotonic trend with respect to the training progress, andwhether it can be used to identify models from different epochs.
Figure 4: Visualizing distances between datasets and models in three-dimensional space. Top-left:large datasets in VTAB. Top-right: small datasets in VTAB. Bottom: various model architecturestrained on ImageNet. The numbers on colored lines are pairwise distances.
Figure 5: Distance to an empty function dp(f, 0) for models with different kinds of regularizationand varying strength. From left to right: Weight decay (L2 regularization), self-distillation, labelsmoothing, dropout. For dropout, a different base model without batch normalization is used.
Figure 6: Relation between ensemble perfor-mance and model diversity given by dp(f1, f2).
Figure 7: Relation between generalization gapand model complexity dp (f, 0).
Figure 8:	Full results of Figure 2: Distance from the interpolation model θ to θB .
Figure 9:	Full results of Figure 3: Distance from the i-th epoch model θi to initial model θ0 and finalmodel θ14.
