Figure 1: Mean posterior entropy of the predictions after each acquisition on EMNIST.
Figure 2: Performance on MNIST and repeated-MNIST. Accuracy and NLL after each acquisition.
Figure 3: Histogram of the labels of all acquired points using different active learning methods onEMNIST (47 classes). ICAL acquires more diverse and balanced batches while all other methodshave overly/under-represented classes.
Figure 4: Performance on EMNIST and fashion-MNIST, ICAL significantly improves the accuracyand NLL.
Figure 5: Performance on CIFAR-10 and CIFAR-100 with batch size=3000 using 8 seeds7	ConclusionWe deVelop a noVel batch mode actiVe learning acquisition function ICAL that is model agnostic andapplicable to both classification and regression tasks (as it relies on only samples from the posteriorpredictiVe distribution). We deVelop key optimizations that enable us to scale our method to largeacquisition batch and unlabeled set sizes. We show that we are robustly able to outperform state ofthe art methods for batch mode actiVe learning on a Variety of image classification tasks in a deepneural network setting.
Figure 7: Relative performance of ICAL and ICAL-pointwise on CIFAR100 with different mini-batch size L.
Figure 8: Frequencies where different numbers of copies (1-3) of a same sample has been acquiredby each method.
Figure 9: CIFAR10 performance with different L. iter “ B is the number of iterations taken to build the entireacquisition batch of size B (note that the actual acquisition happens after the entire batch has been built)iter(B∕L)Figure 10: Runtime of ICAL on CIFAR10 with different minibatch size L.
Figure 10: Runtime of ICAL on CIFAR10 with different minibatch size L.
