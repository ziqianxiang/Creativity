Figure 1: Formally proving ∀x ∈ N : x + 0 = x.
Figure 2: Architecture for tactic and premise selection by Paliwal et al. (2020). Note that this work islargely agnostic to the model architectures.
Figure 3: The figure on the left gives a high-level overview of the components in the reinforcementlearning (RL) loop. The figure on the right shows how the model being trained is used for premiseselection. We propose a modification to the premise selection process to aid exploration in the RLloop.
Figure 4: Results of our main experiment. We report the percentage of validation theorems proven onthe HOList benchmark. The numbers in bold are the state-of-the-art in their respective categories(including results from this work). The main takeaway is that the best RL loop trained without humanproof data outperforms the model trained purely on human data, and approaches the performance ofthe best RL loop trained with human data.
Figure 5: Ablation experiments.
