Figure 1: Standard BNN training graph for fully connected layer l. “sgn,” “x" and “BN” are sign,matrix multiplication and batch normalization operations. Forward propagation dependencies areshown in black; those for backward passes are gray.
Figure 2: Batch Size vS training memory footprint, achieved teSt accuracy and per-batch trainingenergy consumption for BinaryNet with CIFAR-10. The upper plots show memory and accuracyresults for the standard and our proposed training flows. In the lower plots, total energy is split intocompute- and memory-related components. Annotations show reductions vs the standard approach.
Figure 3: Weight density of the sixth convolutional layer of BinaryNet trained with bool weightand po2_5 activation gradients using Adam and the CIFAR-10 dataset.
Figure 4: Achieved training accuracy over time for experiments reported in Table 3.
Figure 5: Achieved training accuracy over time for experiments reported in Table 5.
Figure 6: Achieved training accuracy over time for experiments reported in Figure 2.
Figure 7: Achieved training accuracy over time for experiments reported in Table 6.
