Figure 1: Illustrations of our Set-based Task-Adaptive Meta-Pruning (STAMP): STAMP meta-learns ageneral strategy to rapidly perform structural pruning of a reference network, for unseen tasks. If a learner givesa small fraction of information for his/her target tasks, STAMP almost instantly provides an optimally prunednetwork architecture which will train faster than the full network with minimal accuracy loss.
Figure 2: Set-based Task-Adaptive Pruning: We sample the subset X from D and train the model whilesimultaneously optimizing set-based binary masks through a set encoding function and mask generative functions.
Figure 3:	(a): Accuracy over the ratio of used parameters for CIFAR-10 on VGGNet. Full denotes theaccuracy of the VGGNet before pruning. (b): Accuracy over training time for CIFAR-10, SVHN and Aircraft.
Figure 4:	Left: Training time over the number of training instances. Middle: Accuracy over the number oftraining instances, Right: Accuracy over the number of instances. All experimental results are obtained onCIFAR-10 With VGG-19. We prune 96% 〜97% of the parameters for STAMP and SNIP.
Figure 5: We denote the indices of the remaining chan-nels at each convolution layer of VGGNet after pruning ondifferent datasets, CIFAR-10, SVHN, and Aircraft. All ofthem start from same meta-learned parameters φ.
Figure 6: Left: Accuracy over the ratio of used parameters for SVHN on ResNet-18. Full denotes the accuracyof the ResNet-18 before pruning. Right: Exploring different K (the number of epochs of pruning stage) ofSTAMP compared with BBDropout Lee et al. (2019a) (BBD). kl is a scale factor for the regularization term.
