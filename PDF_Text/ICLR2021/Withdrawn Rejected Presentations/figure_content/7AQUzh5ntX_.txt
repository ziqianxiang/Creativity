Figure 1: We present the steps required to complete two of our tasks. In the top panel, we present “Toast BreadSlice”, where an agent must pickup a bread slice, bring it to the toaster, place it in the toaster, and turn the toasteron. In order to complete the task, the agent needs to recognize the toaster across angles, and it needs to recognizethat when the bread is inside the toaster, turning the toaster on will cook the bread. In the bottom panel, wepresent “Place Apple on Plate & Both on Table”, where an agent must pickup an apple, place it on a plate, andmove the plate to a table. Like “Toast Bread Slice”, it must recognize that because the objects are combined,moving the plate to the table will also move the apple. We observe that learning to use objects together such asin the tasks above poses a representation learning challenge - and thus policy learning challenge - when learningfrom only a task-completion reward.
Figure 2: Full architecture and processing pipeline of ROMA. A scene is broken down into object-image-patches{xo,j} (e.g. of a pot, potato, and stove knob). The scene image is combined with the agent’s location to definethe context of the objects, xκ. The objects {xo,j } and their context xκ are processed by different encodingbranches and then recombined by an attention module R that selects relevant objects for computing Q-valueestimates. Here, R might select the pot image-patch when computing Q-values for interacting with the stove-knob image-patch. Actions are selected as (object-image-patch, base action) pairs a = (b, xo,c). The agent thenpredicts the consequences of its interactions with our attentive object-model fmodel which reuses R.
Figure 3: Top-panel: we present the success rate over learning for competing auxiliary tasks. We seek amethod that best enables our Relational Object-DQN (grey) to obtain the sample-efficiency it would from addingGround-Truth Object-Information (black). Bottom-panel: by measuring the % AUC achieved by each agentw.r.t to the agent with ground-truth information, we can more precisely measure how close each method isto ground-truth performance. We find learning with an Attentive Object-Model (ROMA, red) best closes theperformance gap on 6/8.
Figure 4: Ablation of Object-Attention. We show resultsfor DQN conditioned only on object-images without object-attention.
