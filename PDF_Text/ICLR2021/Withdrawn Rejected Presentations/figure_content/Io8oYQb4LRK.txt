Figure 1: Our method applied to the SGD optimizer to learn (from left to right) the learning rateschedule α, the momentum β, and weight decay μ for a WRN-16-1 on CIFAR-10. For each outerstep (color) we solve CIFAR-10 from scratch for 50 epochs, and update all hyperparameters such thatthe final weights minimize some validation loss. We use hyperparameter sharing over 10, 50 and 50epochs for α, β and μ respectively. All hyperparameters are initialized to zero and converge withinjust 10 outer steps to values that significantly outperform the online greedy alternative (Baydin et al.,2018), and match aggressively hand-tuned baselines for this setting (see Section 5.2).
Figure 2: The hypervariance (top row) and sign fluctuation (bottom row), when learning severallearning rates on the SVHN dataset. These are calculated as we perturb, from left to right, thechoice of training data, the choice of validation data, the initial weights and the initial learning rates.
Figure 3: The learning rate schedule α learned for the MNIST and SVHN datasets using a LeNetarchitecture over 500 inner gradient steps. We observe that on real-world datasets like SVHN, bothgreedy and non-greedy hyperparameter optimizations fail to learn decent learning rate schedules.
Figure 4:	Hypergradients have a reasonable range but fail to always converge to zero when thevalidation performance stops improving.
Figure 5: The combination of hyperparameters searched over for CIFAR-10 (top row) and the cor-responding distribution of test accuracies (bottom row).
