Figure 1: Dataset used for training data creationWord2vec is one of the most popular techniques to learn word embeddings using a neural network.
Figure 2: Classifier architecture2.2.2	Similarity-based featuresBy leveraging skill embeddings generated using embedding techniques, similarity-based featureswere created to capture the association between a group and skill. The details of those are givenbelow.
Figure 3: t-SNE plot of embeddings of "Customer Support" and "Electronics" competency group.
Figure 4: t-SNE plot of embeddings of "Logistic" and "Network" competency group. The left imageshows the projection generated using Word2vec embedding and the right image is the SkillBERTplot. The top cluster shown in SkillBERT t-SNE plot represents "Logistic" competency group whilethe bottom cluster represents "Network".
Figure 5: Adjacency matrix representation of Graph4=	ba12+ ba13+ ba14+ ba.		⅞=	b%	+ b¾	+ b⅛d3 =	b%	+ b¾	+ b⅛d,=	ba,.	+ ban.	十 ba,r4		v*24	45d5 =	ba15	+ ba35	+ ba4515	DigitaJ marketing	MedH buying	Publishing	Branding	AdvertisingDigrlal marketing		-ba1z	'bai3	^baw	-b⅛MecSa buying	-ba12	da	-ba23	-ba 24	□Publishmg	ba13	-咚	d3	O	-tj⅛Branding	-ba14	g	O	q	-ba,,Advertising	-ba IS	O	-ba33	-ba 43	djFigure 6: Graph laplacian for example in Figure 5and degree matrix D asD = diag(d(v1), ......., d(vm))(2)3.	Graph laplacian (L): If D is a diagonal matrix and A is affinity matrix then we can compute L asfollows :-
Figure 6: Graph laplacian for example in Figure 5and degree matrix D asD = diag(d(v1), ......., d(vm))(2)3.	Graph laplacian (L): If D is a diagonal matrix and A is affinity matrix then we can compute L asfollows :-L=D-A	(3)The Laplacian’s diagonal is the degree of our nodes, and the off-diagonal is the negative edge weights(similarity between nodes). For clustering the data in more than two clusters, we have to modify ourlaplacian to normalize it.
Figure 7: Scatter plot of eigenvalues to determine number of eigenvectors and clusters in spectralclusteringTable 6: Result for different embedding size (In this experiment, XGBoost was used as a classifierand bert-prob was used along with emdeddings of different sizes as independent variable. No otherfeature apart from these was used)SkillBERT embedding size	Precision		Recall		F1-score		Class 0	Class 1	Class 0	Class 1	Class 0	Class 132	98.12%	91.65%	95.47%	80.12%	96.78%	85.50%64	98.32%	91.80%	97.26%	81.10%	97.79%	86.12%128	99.12%	92.65%	97.47%	83.80%	98.29%	88.00%256	99.12%	92.56%	97.40%	83.79%	98.25%	87.96%11Under review as a conference paper at ICLR 2021Figure 8: Elbow method graph to determine the number of clusters in K-means clusteringTable 7: Examples of some candidate and job profilesCandidate or Job	Skill set	Probable competency groupsCandidate1	Design, knockoutjs, Corel draw	Tool design, mechanical design, front end, web developmentCandidate2	Statistical modeling, statistical process control	Statistics, production operationsJob1	Analytical skills, project execution, accounting	Financial operations, business analytics, statistics, accountsJob2	Digital marketing, cash management, ms office, ms excel, ms word, tally	Taxation, banking, statistics
Figure 8: Elbow method graph to determine the number of clusters in K-means clusteringTable 7: Examples of some candidate and job profilesCandidate or Job	Skill set	Probable competency groupsCandidate1	Design, knockoutjs, Corel draw	Tool design, mechanical design, front end, web developmentCandidate2	Statistical modeling, statistical process control	Statistics, production operationsJob1	Analytical skills, project execution, accounting	Financial operations, business analytics, statistics, accountsJob2	Digital marketing, cash management, ms office, ms excel, ms word, tally	Taxation, banking, statisticsA.3 SkillBERT trainingThe dataset used for training the SkillBERT model can be downloaded from here. It contains the listof skills present in job requisitions. Table 7 contains examples of some candidate and job profiles. Weleveraged Bert-Base architecture on the job-skill data to generate embeddings of size 768, details of itcan be found here. Finally, the embeddings generated using the SkillBERT model can be downloadedhere.
Figure 9: Data format used for creating bert_ prob featurefinance0.310.90.8 0.21A.5 FeaturesThe details of features used in the training of Bi-LSTM model, which gave us the best performanceare given in Table 8 .
Figure 10: Data format used for final model creationonly pre-trained BERT model and "skillbert_and_kmeans.py" for classifying skills using SkillBERTand k-means on SkillBERT embedding.
