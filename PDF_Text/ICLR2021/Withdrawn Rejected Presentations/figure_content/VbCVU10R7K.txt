Figure 1: The belief distributions of ρ1 and p2 de-pend on the uncertainty induced from the finite of-fline data (D and D0). A user might prefer π2 onlyif p(p2 < ρ1) < ξ (a choice of S).OPE basedon mean point estimates would select π2 in eithercase as ρ2 has the greater mean. Sampling fromthe posterior belief in OfflineSelect allowssimulation of any ranking score under S, aligningpolicy selection with the user’s choice ofS.
Figure 2: Confidence interval estimation on Bandit, FrozenLake, Taxi, and Reacher. The y-axisshows the empirical coverage and median log-interval width across 200 trials. BayesDICE exhibitsnear true coverage while maintaining narrow interval width, suggesting an accurate posterior ap-proximation.
Figure 3: Policy selection using top-k ranking scores compared to mean/confidence ranking ap-proaches on two-armed Bandit and Reacher. In these experiments, we fix the posterior to the oneapproximated by BayesDICE and evaluate different S used in Algorithm 1 to compute a policyranking. We find that using S = S (i.e., aligning the ranking score in posterior simulation with thegroundtruth evaluation) results in better performance than simple point estimates. Interestingly, thelower-bound point estimate almost always performs worse than the mean or the upper bound.
Figure 4: Policy selection evaluation under correlation and regret at top-k in two-armed Bandit (left)and Reacher (right) compared to other methods using point estimate (DualDICE) or high-confidencelower bounds. Please see Appendix C for more results with respect to other downstream metrics.
Figure 5: Confidence interval estimation on Bandit where the baselines are unnormalized (UN).
Figure 6: Confidence interval estimation with baselines computed from marginalized importancesampling.
Figure 7: Additional k values for top-k ranking on bandit. Ranking results based on Algorithm 1(blue lines) always perform better than using mean or high-confidence lower bound.
Figure 8: Additional k values for top-k ranking on reacher and additional scores (precision andregret). Ranking results based on Algorithm 1 (blue lines) generally perform much better than usingmean or high-confidence lower bound for top-k accuracy and correlation. Precision and regret aresimilar between posterior samples and the mean/confidence bound based ranking.
Figure 9:	Improved regret using BayesDICE across all trajectory lengths, behavior data, and top-kvalues considered for the bandit task.
Figure 10:	Improved correlation using BayesDICE across all trajectory lengths, behavior data, andtop-k values considered for the reacher task.
