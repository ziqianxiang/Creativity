Figure 1: Results produced by our model. The model was trained on a single training pair (first andthe second columns). The third column shows the inputs to the trained model at inference time. Firstrow- (left) lifting the nose, (right) flipping the eyebrows. Second row- (left) adding a wheel, (right)conversion to a sports car. Third row - modifying the shape of the starfish.
Figure 2: Results on three different image primitives (the leftmost column shows the source image,then each column demonstrate the result of our model when trained on the specified primitive). Wemanipulated the image primitives, adding a right eye, changing the point of view and shorteningthe beak. Our results are presented next to each manipulated primitive. Our SP performed best onhigh-level changes (e.g. the eye), and low-level changes (e.g. the background).
Figure 3: Results of our SP on challenging image manipulation tasks. left) the primitive-image pairused to train our method. center) switching the positions between the two rightmost cars. right)removing the leftmost car and inpainting the background. In both cases our method was able tosynthesize very attractive output images. See App. A for many more results.
Figure 4: Edges-to-image results. columns 1, 2 show the edges and images used for training. Column3 shows the edges used as input at inference time. We can see that Pix2PixHD-MI cannot generatethe correct shoe as there is not enough guidance. BicycleGAN has sufficient guidance but cannotreproduce the correct details. Our approach generates images of high quality and fidelity.
Figure 5: Paint-to-image results. Two leftmost columns show the input paint that was createdmanually and the input image. The third column shows the modified paint image used as input to thetrained models, the result by SinGAN was generated using the authors’ best practice. Our methodgenerates a novel tree corresponding to the segmentation map with high fidelity. Note that SinGANis not designed for conditional generation, hence the weak results.
Figure 6: Several sample results from the Cityscapes dataset. We train each model on thesegmentation-image pair on the left. We then use the models to predict the image, given thesegmentation maps (second column from left). Our method is shown to perform very well on thistask, generating novel configurations of people not seen in the training image.
Figure 7: Comparing Pix2PixHD trained on a single image pair with the ”flip-and-crop” warp vs.
Figure 8: Failure modes: (left) generating unseen objects - eyes of the dog (center) backgroundduplication - sea behind the turtle (right) empty space interpolation - nose of the catRuntime: As with other methods that train deep neural networks based on a single image, per-imagetraining times are higher than when trained on large datasets. Our runtime is a function of theneural architecture we use and the number of iterations. Here specifically, we use the Pix2Pix-HDframework which has roughly comparable runtime to SinGAN for the same image size. The preciseruntime depends on image size and the number of iterations used. When running all experiments onthe same hardware (NVIDIA RTX-2080 Ti), smaller images e.g. the “Balloons” image showcasedby SinGAN (size: 248X186) take SinGAN 50 minutes while DeepSIM (ours) takes 63 minutes.
Figure 9: Evaluation of the ability of our network to interpolate across empty space regions. Thetwo leftmost columns show the training image pair, we gradually increase the distance between theeyes and nose of the cat, and feed the test images to the network, the corresponding output of eachtest image is shown in the second row. Our method generates attractive interpolations for moderatechanges, the performance deteriorates for larger interpolations.
Figure 10: Ablation of SP on the cats dataset: (top) edges-only (center) segmentation-only (bottom)SP. We can see that edges-only creates wrong associations between objects, segmentation-only failsto generate the fine details correctly (e.g. building), whereas SP achieves strong results.
Figure 11: An analysis of the benefits of TPS. We show the kNN distance between patches in the testand train frames with and without TPS augmentations (top-right). We can see that TPS augmentationdecreases the kNN distance, in some image regions the decrease is drastic suggesting the patchesthere can be obtained by deformations of training patches. The kNN-TPS distance appears to becorrelated with the regions where the prediction error of our method is large. This analysis suggeststhat by artificially increasing the diversity of patches, single-image methods can generalize better tonovel images.
