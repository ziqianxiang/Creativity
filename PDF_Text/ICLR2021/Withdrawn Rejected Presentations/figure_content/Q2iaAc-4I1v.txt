Figure 1: Overview of Inference. The exploration loop produces a series of K experiments allowing the agentto infer the representations for K causal factors. After exploration, the agent utilizes the acquired knowledge fordownstream tasks.
Figure 2: Examples of discovered behaviors. The agent discovers experimental behaviors that allow it tocharacterize each environmental object in a binary manner, e.g., heavy/light, big small, rollable/not rollable, etc.
Figure 3: Utility of discovered behaviors. We find that the behaviors discovered by the agents while optimizingcausal curiosity show high zero-shot generalizability and converge to the same performance as conventionalplanners for downstream tasks. We also analyze the worst case performance and find that the pre-training ensuresbetter performance than random initialization. The table compares the time-steps of training required on anaverage to acquire a skill with the time steps required to learn a similar behavior using external reward. We findthat the unsupervised experimental behaviors are approximately 2.5 times more sample efficient. We also findthat maxizing both curiosity and external reward in our experimental setups results in sub-optimal results.
Figure 4: Discovered hierarchical latent space. The agent learns experiments that differentiate the full setof blocks in ShapeSizeMass into hierarchical binary clusters. At each level, the environments are dividedinto 2 clusters on the basis of the value of a single causal factor. We also show the principal components ofthe trajectories in the top left. For brevity, the full of extent of the tree is not depicted here. For each level ofhierarchy k, there are 2k number of clusters.
Figure 5: Knowledge of causal factors aids transfer. We find that knowledge of the causal representationallows agents to generalize to unseen environments with high zero-shot performance. The table depicts theextra timesteps required by the Generalist in each experimental setup to match the zero-shot performance ofcausally-curious agent. We find that as the number of varying causal factors increase, the difference in zero-shotperformance of the Causally-curious agent and the Generalist increases, showing that the CC agents are indeedrobust to multiple varying causal factors.
Figure 6: Overview of training. The experiment planner generates a trajectory of actions which is appliedto each of the environments with varying causal factors namely mass, shape and size of blocks. For eachenvironment, an observation trajectory or state S(i) âˆˆ S is obtained. A simple model with fixed low expressivepower is used to approximate the generative model for S. The "information overflow" L(S|M) is returned asnegative reward forcing S to be caused by few causal factors.
