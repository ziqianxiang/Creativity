Figure 1: Masking weights and hidden activations in BERT. We show a linear layer with weightsW , inputs h, and outputs h0 . We learn a mask for each disentangled factor, which is either appliedto the weights W or to intermediate representations h.
Figure 2: Average and worst main task performance across Sentiment/genre combinations. Maskedvariants (proposed in this paper) are cross-hatched. Larger gaps between the average performanceand the worst group performance indicate that the corresponding model is using the non-targetedattribute when making predictions for the main task.
Figure 3: t-SNE projection of sentiment representations and genre representations of different mod-els. Marker colors denote sentiment (blue for positive and yellow for negative); marker shapesdenote genre (× for drama and • for horror).
Figure 4: Differences between the performances achieved via BERT embeddings and the disen-tangled model variants considered on semantics-oriented (WC, STS) and syntax-oriented (Depth,TopCon) tasks compared with BERT embeddings. We plot this difference with respect to the se-mantics and syntax embeddings induced by the models in the left and right subplots, respectively.
Figure 5: Model performance as a function of the level of pruning. The x-axis corresponds to thesubnetwork sparsities (percent of weights dropped), while the y axes are performance measures— accuracy for all tasks except for STS, where we report Pearson’s correlation. We compare theperformance of models trained on the semantic (top) and syntax representations (bottom) learned bythe disentangling strategies considered, after pruning to varying levels of sparsity.
