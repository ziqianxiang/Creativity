Figure 1: Training curves of ResNet-18 without augmentation or weight decay on ImageNet datasetwith various levels of noise in training labels. We randomize 0%, 25%, 50%, 75%, and 100% of thetraining labels to get 5 variants of ImageNet. The training examples whose labels are unaffected byrandomization are called pristine and the rest are corrupt. We keep a fixed sample of 50k examplesfrom the pristine and corrupt sets to measure our accuracy and loss (except for 100% noise wherethere are fewer than 50k pristine). Figures (a) shows the training and test accuracy with various levelsof noise, while (b) shows the corresponding training loss. Figures (c) and (d) show the accuracyand loss plots on the pristine and corrupt samples. The plots confirm the predictions from CGH inSection 2. (We train with a mini-batch size of 256 and momentum of 0.9. The jumps at 30 and 60epochs are due to the usual lowering of the learning rate by 1/10 from an initial value of 0.1.)(c) Acc. on pristine and corrupt split of training data20	40	60	80epoch(d) Loss on pristine and corrupt split of training data3.1	Review of the Winsorization TechniqueThe test modified SGD by using the (coordinate-wise) winsorized mean of the per-example gradientsin a mini-batch (instead of the usual mean) to update the weights in each step. Everything else,including learning rate was kept the same. The winsorized mean limits the influence of outliers byclipping them to a specified percentile of the data (called the winsorization level). As expected fromCGH, they found that as the winsorization level increased, the rate of overfitting decreased.
Figure 2: First row: Using the rolling median of 3 mini-batCh gradients (RM3) to train a ResNet-18on ImageNet leads to a smaller generalization gap than using SGD. SinCe this reduCtion in overfittingis obtained by suppressing weak gradient direCtions, this provides strong evidenCe for CGH in thissetting. SeCond row: The use of median in RM3 is CritiCal; using average instead (RA3) does notsuppress (and resembles SGD). Last row: A Comparison with M3, another method for suppressingweak gradient direCtions based on splitting a mini-batCh into 3 groups and Computing the median.
Figure 3: The suppression of weak gradients by RM3 greatly inhibits learning of Corrupt examples asCompared to pristine. This shows that weak gradient direCtions are responsible for memorization andprovides Compelling evidenCe for CGH. We note again that median is what is CritiCal, and not therolling aspeCt sinCe RA3 (Figure (b)) behaves like SGD (Figure (a)), but M3 (Figure (C)) like RM3.
Figure 4: Using the original ImageNet training dataset, we create two datasets easy and hard. Theseare subsets of the original dataset and have a training set (e-train and h-train) and test set (e-test andh-test). We train on both with a ResNet-18 architecture. Figure (a) shows the accuracy curves of thefull, easy and hard training sets evaluated on the original, e-test and h-test test sets. Figure (b) showsthe corresponding training and test loss curves. The generalization of easy examples to other easyexamples is better than that of hard examples to other hard examples, in accordance with CGH, andprovides additional confirmation (Section 4.2).
Figure 5: (a) Training accuracy of RM3 on examples of varying difficulty. The suppression of weakgradients by RM3 disproportionately impacts the learning of more difficult examples which is asexpected from CGH (Section 4.3). (b) If SGD enumerates hypotheses of increasing complexitythen the examples learned early (i.e., easy examples) should be the ones far away from the decisionboundary since they can be separated by simpler hypotheses. However, the hard examples then,should generalize well to the easy examples, but that is not what we find (Section 4.4).
Figure 6: Training curves of Inception-V3 without augmentation or weight decay on ImageNetdataset with various levels of noise in training labels. We randomize 0%, 25%, 50%, 75%, and100% of the training labels to get 5 variants of ImageNet. The training examples whose labels areunaffected by randomization are called pristine and the rest are corrupt. We keep a fixed sample of50k examples from the pristine and corrupt sets to measure our accuracy and loss (except for 100%noise where there are fewer than 50k pristine). Figures (a) shows the training and test accuracy withvarious levels of noise, while (b) shows the corresponding training loss. Figures (c) and (d) show theaccuracy and loss plots on the pristine and corrupt samples. The plots confirm the predictions fromCG in Section 2.
Figure 7: Training curves of VGG-13 without augmentation or weight decay on ImageNet datasetwith various levels of noise in training labels. We randomize 0%, 25%, 50%, 75%, and 100% of thetraining labels to get 5 variants of ImageNet. The training examples whose labels are unaffected byrandomization are called pristine and the rest are corrupt. We keep a fixed sample of 50k examplesfrom the pristine and corrupt sets to measure our accuracy and loss (except for 100% noise wherethere are fewer than 50k pristine). Figures (a) shows the training and test accuracy with various levelsof noise, while (b) shows the corresponding training loss. Figures (c) and (d) show the accuracy andloss plots on the pristine and corrupt samples. The plots confirm the predictions from CG in Section2.
Figure 8: Performance of winsorization on the training of cifar-10 dataset on ResNet-32 architecture.
