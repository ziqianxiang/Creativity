Figure 1: (a) The over-all architecture of theproposed model, wherethe TE, SE and TA mod-ules are highlighted inyellow color. (b) Visu-alization of the proposedcontextual token embed-ding (TE) for token u,where WE and PE denoteword embedding and po-sition embedding. (c)Visualization of segmentembedding (SE), analo-gized as virtual tokensand placed before theword sequence. (d) Vi-sualization of the topicselect-attention module,interleaved into trans-
Figure 2: (a) (b) Comparisons of test perplexity as a function of fine-tuning time on WT2 based on GPT-2 andTransformer-XL (T-XL). (c) Visualizing of attention weights with different regularization.
Figure 3:	The generated sequences guided by multi-layer topics and preceding context. Words in the samecolor are semantic-consistent. The generated texts successfully capture both syntactic and semantics.
Figure 4:	Visualizing the attended topics of word "market" at layers 1, 2 and 3, respectively. X-axis denotes theindex of attended topics, and y-axis the index ofheads. Each row denotes the most activate topic of correspondinghead and we omit the other inactivate topics. Several top words of corresponding topics are listed at the bottom.
