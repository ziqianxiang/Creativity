Figure 1: The three tasks of the MNIST-Fellowship dataset.
Figure 2: Experiment on disjoint classes without vs with test task label. Left, the mean accuracy ofall 3 tasks, vertical lines are task transitions. Right, accuracy on the first task. Legends with ‘Lab_’indicate experiments with task labels for test. The expert model is trained with i.i.d. data from alltask and the baseline model is finetuned on each new task without any continual process.
Figure 3: Experiments with joint classes. Left, mean accuracy of all 3 tasks, vertical lines are tasktransitions. Right, accuracy on the first task.
Figure 4:	Simple case of continual learning classification in a multi-task setting. Left, the task T0:learning a hyperplane splitting two classes (red and blue dots). Right, the task T1: learning a linesplitting two classes (yellow and green squares) while remembering T0 models without rememberingT0 data (pale red and blue dots).
Figure 5:	D0 feature space before learning T0 (Left), D0 feature space after learning T0 with apossible decision boundary (Right). Data points are shown by blue and red dots. The line (right part)is the model learned to separate data into the feature space.
Figure 6:	Case of representation overlapping while continual learning classification in a multi-tasksetting. At task T1, feature space of D1 before learning T1 (Left), Feature space of D1 after learningT1 with a possible decision boundary (Right). New data are plotted as yellow and green squares andold data that are not available anymore to learn are shown with pale red and blue dots.
