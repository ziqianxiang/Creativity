Figure 1: Augmented Temporal Contrast—augmented observations are processedthrough a learned encoder fθ, compressor,gφ and residual predictor hψ , and areassociated through a contrastive loss witha positive example from k time steps later,processed through a momentum encoder.
Figure 2:	Online encoder training by ATC, fully detached from RL training, performs as well as end-to-endRL in DMControl, and better in sparse-reward environments (environment steps shown, see appendix for actionrepeats). Each curve is 10 random seeds.
Figure 3:	Online encoder training by ATC, fully detached from the RL agent, performs as well or better thanend-to-end RL in DMLab (1 agent step = 4 environment steps, the standard action repeat). Prioritized ATCreplay (Explore) or increased ATC training (Lasertag) addresses sparsities to nearly match performance ofRL with ATC as an auxiliary loss (RL+ATC). Each curve is 3 random seeds.
Figure 4: Online encoder training using ATC, fully detached from the RL agent, works well in 5 of 8 gamestested (1 agent step = 4 environment steps, the standard action repeat). 6 of 8 games benefit significantly fromusing ATC as an auxiliary loss or for weight initialization. Each curve is 8 random seeds.
Figure 5: RL in DMControl, using encoders pre-trained on expert demonstrations using UL, with weightsfrozen—across all domains, ATC outperforms prior methods and the end-to-end RL reference. Each curve is amininum of 4 random seeds.
Figure 6: RL in DMLab, using pre-trained en-coders With weights frozen-in Lasertag espe-cially, ATC outperforms leading prior UL algo-rithms.
Figure 7: RL in Atari, using pre-trained encoders with weights frozen—ATC outperforms several leading,prior UL algorithms and exceeds the end-to-end RL reference in 3 of the 4 games tested.
Figure 8: Separate RL agents using a single encoder with weights frozen after pre-training on expert demon-strations from the four top environments. The encoder generalizes to four new environments, bottom row,where sparse reward tasks especially benefit from the transfer. Each curve is minimum 4 random seeds.
Figure 9: BREAKOUT benefits fromcontrasting against negatives from sev-eral neighboring time steps.
Figure 10: An example scene from B REAKOUT, where a low-performance UL encoder (without shift) focuses on the paddle.
Figure 11: RL using multi-task encoders (all with weights frozen) for eight Atari games gives mixed perfor-mance, partially improved by increased network capacity (8-game-wide). Training on 7 games and testing onthe held-out one yields diminished but non-zero performance, showing some limited feature transfer betweengames.
Figure 12: Random shift augmentation helps in some Atari games and hurts in others, but applying withprobability 0.1 is a performant middle ground. DMLab benefits from random shift. (Offline pre-training.)ErU ① UEnv StepsBall-in-Cup: CatchRL Random ShiftLatent ±0.5 (subpixel)Observation ±4Latent ±1 (pixel)NoneFigure 13: Even after pre-training encoders for DMControl using random shift, RL requires augmentation—our subpixel augmentation acts on the (compressed) latent image, permitting its use in the replay buffer.
Figure 13: Even after pre-training encoders for DMControl using random shift, RL requires augmentation—our subpixel augmentation acts on the (compressed) latent image, permitting its use in the replay buffer.
Figure 14: Attention map in Breakout which shows the RL-trained encoder focusing on gamescore, whereas UL ATC encoder focuses properly on the paddle and ball.
Figure 15: Attention map in LASERTAG. UL encoder with pixel control focuses on the score, whileUL encoder with the proposed ATC focuses properly on the coin similar to RL-trained encoder.
Figure 16: Attention map in the LASERTAG which shows that UL encoders focus properly on theenemy similar to RL-trained encoder.
