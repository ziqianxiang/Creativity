Figure 1: Topology influence spectrum in the light of the model considered. From left to right weselected the models in order to smoothly transition from a case where only the point/node featuresare relevant (left, standard VAE) to the opposite end of the spectrum where only the topologicalproperties are considered (right, node2vec). In the middle we find the cases where the point nodefeatures and the topology are blended, either implicitly via a regularizer in the GR-VAE case orexplicitly in the DGI case.
Figure 2: Definition of the graph used to build the regularization term in the GR-VAE for-mulation. Relational information between the data points (A-D) is considered to define a datasetmetagraph that can be used to constraint distances in the learned latent space. The notation dνi inthe distance matrix abbreviates dD(ν, i) from Equation 1.
Figure 3: Learned representations on two synthetic datasets with diferent topology. This figuredisplays qualitatively how GR-VAE affects the latent space topology under different conditions,specifically when compared to its non-constrained counterpart. The plots on top show the latentspace, the ones in the bottom show the feature space, for the first two features. The two datasets(left and right) had different topologies, shown as a graph, next the color bar (all colors across plotscorrespond to the nodes’ ids). On the features plots both the original data (round marker) and thereconstructed data (cross marker) are shown.
Figure 4: Qualitative analysis of the latent representations learned in the MNIST case. A. PCAprojection of the samples in the latent space under different training regimes. The original latentspace has 16 dimensions. The metagraph is a chain connecting each class from 0 to 9 in order (arepresentation can be seen on the top right). The samples can be seen coloured by class pertinence.
Figure 5: Complete overview of the GR-VAE approach. Notice that, the notation dνi in thedistance matrix abbreviates dD(ν, i) from Equation 1.
Figure 6: Qualitative analysis of the latent representations learned in the MNIST case (with alatent space size 3). Figure with extended information about the MNIST results, it displays sameset of experiments run in Figure 4, but using a VAE with a latent space of 3 dimensions. For thatreason A. directly displays all the latent space dimensions (not a PCA projection). It also includesan extra setting (γ = 10) for the regularizer.
