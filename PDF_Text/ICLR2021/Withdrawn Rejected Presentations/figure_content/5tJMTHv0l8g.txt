Figure 1: Fraction bit distributions of image classification model parameters and distributions ofevaluated bytes of various file formats (JPEG, MP3, and PDF). We visualized the distributions offraction bits of parameters into three different plots. Three subsequences of23 fraction bits are eval-uated at their decimal values (top row, first three). The distributions of the entire parameters ofeach model is also presented (top row, last one). Next, we plotted byte distributions of the originaland the encrypted files (bottom row, first two). The plotted values are averaged over 100 files foreach file format. Finally, the probabilities of each bit in floating-point representation to be one isvisualized (bottom row, last one). An interesting observation is that the fraction bits of five archi-tectures follow nearly identical distributions, but the shape of parameter distributions are noticeablydifferent. Exponent bits of floating-point numbers mainly responsible for the discrepancy betweenparameter distributions.
Figure 2: Hiding information in neural network parameters. Before embedding, secrets should becompressed and encrypted to provide additional protection. A hiding procedure is to simply replacefraction bits of the parameter with the encoded message. In this paper, we convey information eitherin 1) the most significant 4 fraction bits (denoted by M4), orin 2) the least significant 8 or 16 fractionbits (denoted by L8 and L16, respectively) of fraction bits.
Figure 3: Validation accuracy sensitivity and gradient sensitivity of the layers of ResNet-18. Themore a layer is to the left, the closer it is to input. Due to significant scale differences between BNand convolutional layers, y-axes are represented in base-10 log scale. Overall, a layer closer to theoutput has a larger sensitivity in either metric.
Figure 4: Change in the probabilities of each bit of a floating-point number to be one. The plot inthe top shows the difference of probabilities between a perturbed model distribution and the originalmodel. The plot in the top shows the difference of probabilities between a perturbed and fine-tunedmodel and the original model.
