Figure 1: Audio-based video editing. Within a set of speech videos, speech audio of a randomly chosenspeaker, extracted from the video, can be used to drive any video featuring a random speaker.
Figure 2: Architecture. Our network contains an Audio-to-Expression Translation Network that learns facialexpression parameters from speech audio and a Neural Video Rendering Network that generates mouth regionguided by projected mouth landmarks.
Figure 3: Audio ID-Removing Network. We formu-late the speaker adaptation method from speech recog-nition (Visweswariah et al., 2002; Povey & Yao, 2012)as a neural network. The network removes identity inspeech MFCC spectrum by transferring it to the “globalspeaker” domain.
Figure 4: Many-to-many results. (a) One-to-many results: we use speech audio of one speaker to drive faceof three different speakers. (b) Many-to-one results: we use speech audio of two different speakers to drive thesame speaker.
Figure 5: Large pose results. We demonstrate four head poses including up, down, and left. Video results ofall of seven head poses can be viewed the supplementary video.
Figure 6: (a) Audio editing. We select “knowledge is” and “virtue” from “Knowledge is one thing, virtue isanother” in the source audio, then recombine them as “Knowledge is virtue” as input. (b) Singing. We evaluateour network on the singing audio clips. Video result can be viewed in the supplementary video.
Figure 7: Comparison to state-of-the-art. Comparison between our method with Audio2Obama (Suwa-janakorn et al., 2017), Face2Face (Thies et al., 2016), DVP (Kim et al., 2018), TBE (Fried et al., 2019) andNVP (Thies et al., 2020).
Figure 8: (a) 2D vs 3D qualitative comparison. 3D parameter regression outperforms 2D parameter regressionfor head motion and poses. (b) ID-removing qualitative comparison. Improvement can be observed on afailure case caused by not applying Audio ID-Removing Network.
Figure 9: tSNE before and after id-removing. 2D visualized distributions of input MFCC and normalizedMFCC. Different color represents different speaker. Our audio normalization erases the identity informationembedded in the MFCC spectrum.
Figure 10: Test on unseen subjects and sentences. The testing subjects and sentences are unseen duringtraining. For video results please refer to the supplementary video.
Figure 11: Architecture of Audio-to-Expression Network. The Audio ID-Removing Network eliminatesidentity information in speech audio. The Audio-to-Expression Translation Network estimate expression pa-rameters from input audio. We constrain the predicted expression parameters and the projected 2D mouthlandmark from the reconstructed facial mesh.
Figure 12: Failure cases. (a) Poor tongue generation result on phoneme ”Z” that require the use of tongue. (b)Poor lip-sync accuracy when we use normal speech audio to drive a speaker with strong Russian accent.
