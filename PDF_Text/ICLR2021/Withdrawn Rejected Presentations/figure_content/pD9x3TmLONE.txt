Figure 1: Overall Design of XMixup Algorithmfeatures using the pre-trained model Θpretrain for every sample xi in the class c, i.e.,Centroid(C) = ∣1∣ X Φ(xi, Θpretrain), for C ∈ S or C ∈ T.	(1)∀xi∈cGiven two classes Cs and Ct in the source and target domains respectively, we consider the similaritybetween the two classes as the potentials for knowledge transfer, while XMixup measures thesimilarity between the two classes using the cosine measures between the centroids of the two classes,such that dist(Cs, Ct) = cosine < centroid(Cs), centroid(Ct) >. In this way, the auxiliary sampleselection could be reduced to search the optimal transport between the sets of classes of S and Trespectively, via the pre-defined distance measure. Hereby XMixup intends to find a one-to-onemapping P * : T → S, such thatP * —	argmin	^X dist(q,P(Ct)),	(2)∀P ⊂(S ×T)∩O2O ∀c ∈Twhere S × T refers to the Cartesian product of the target and source class sets, O2O refers to theconstraint of the one-to-one mapping, P(Ct) maps the target class to a unique class from the sourcedomain. Note that P * refers to the optimal mapping that potentially exists to minimize the overalldistances, while XMixup solves the optimization problem using a simple Greedy search Cui et al.
Figure 2: Singular values of feature matrices extracted by different transfer learning models. Singularvalues are divided by the corresponding largest one for scale normalization. Top 10 smallest values arepresented. L2 models are trained with random sampling rates 10%, 30%, 50% and 100% respectively.
Figure 3: Influence of the auxiliary dataset size on the performance of XMixup.
Figure 4: Examples of Beta Distribution.
Figure 5: Influences of the choice of the hyperparameter α in log scale. Black nodes refer to thedefault value used in previous experiments. Doted lines refer to the accuracy of state-of-the-artbaselines.
