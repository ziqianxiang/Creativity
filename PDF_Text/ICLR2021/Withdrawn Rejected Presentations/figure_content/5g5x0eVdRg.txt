Figure 1: DHOG architecture. The skeleton is a ResNet18 (He et al., 2016). The final ResNetblock is repeated k - 3 times (k = 8 here).① Augmentations of each image, xa∙ d, are separatelyprocessed by the network. © Each shallow ResNet block (1... 3) constitutes shared computationfor deeper blocks, while also computing separate probability vectors, zι... z3. Each Zi is viewed asthe probability for each outcome of the random variable Ci that makes a discrete labelling choice. @The deepest ResNet blocks compute further z>3.④ The network is trained by maximising the MIbetween allocations Ci from all data augmentations, and © separately for each node i, minimisingthe MI between Ci and c<i for the same data augmentation. © This is implemented by stoppinggradients such that they are not back-propagated for later computation paths (red crosses).
Figure 2: Accuracy per head. DHOG causes heads to learn distinct solutions.
Figure 3: Confusion matrices from the same seed (a) without and (b) with DHOG cross-head MIminimisation. These networks struggle with distinguishing natural objects (birds, cats, deer, etc.),although DHOG does improve this.
