Figure 1: Activations of an SO(2,1)-EqUivanant neural network constructed using our framework.
Figure 2: Convergence to arbitrary precision group representations of three Lie groups:SO(3), SO(2, 1), and SO(3, 1). The multiplicative norm penalty is plotted in each lower subplot,and demonstrates that this penalty is important early on in preventing the learning of a trivial rep-resentation, but for later iterations stays at its clipped value of 1. Loss is plotted on each uppersubplot.
Figure 3: Tensor product structure of the learned group representations P with several known(analytically-derived) group representations ρι for the groups Sθ(3), SO(2,1), and SO(3,1). Eachcolumn is for the group indicated at the bottom, each row is for a different choice of ρι for that group,and the horizontal axis indicates the ρ(i) onto which We project the tensor product P 0 ρι =㊉i∈ιρ(i).
Figure 4: (Left) SO(2, 1)-equivariant neural network learning to recognize digits from the MNIST-Live dataset in 2 spatial dimensions. Error bars for train accuracy and loss are computed as the meanand standard deviation across a sliding window of 15 batches. (Right) SO(3, 1)-equivariant neuralnetwork training to recognize digits from the MNIST-Live dataset in 3 spatial dimensions. Error barsfor train accuracy and loss are computed as the mean and standard deviation across a sliding windowof 15 batches.
Figure 5: Our Lie Algebraic Networks (lan) module handles Lie algebra and Lie group representations,derives Clebsch-Gordan coefficients for the equivariant layer update, and computes the forward pass.
