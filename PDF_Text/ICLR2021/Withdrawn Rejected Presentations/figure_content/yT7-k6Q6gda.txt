Figure 1: The catastrophic Fisher explosion phenomenon demonstrated for Wide ResNet trained usingstochastic gradient descent on the TinyImageNet dataset. Training is done with either a learning rateoptimized using grid search (η1 = 0.0316, red), or a small learning rate (η2 = 0.001, blue). Trainingwith η2 leads to large overfitting (left) and a sharp increase in the trace of the Fisher InformationMatrix (FIM, middle). The trace of the FIM is closely related to the gradient norm (right).
Figure 2: Association between the value of Tr(F) in the initial phase of training (Tr(Fi)) and testaccuracy on ImageNet, CIFAR-10 and CIFAR-100 datasets. Each point corresponds to multipleseeds and a specific value of learning rate. Tr(Fi) is recorded during the early phase of training(2-7 epochs, see the main text for details). The plots show that early Tr(F) is predictive of finalgeneralization. Analogous results illustrating the influence of batch size are shown in Appendix A.1a relatively small learning rate, coincides with poor generalization. We call this phenomenon thecatastrophic Fisher explosion. Figure 1 illustrates this effect on the TinyImageNet dataset (Le &Yang, 2015).
Figure 3: Training with FP or GPx improves generalization and limits early peak of Tr(F). Eachsubfigure shows validation accuracy (left) and Tr(F) (right) for training with η* ora small learningrate (blue) and for training with either GPx or FP (red). Curves were smoothed for clarity.
Figure 4: Each subplot summarizes an experiment in which we apply Fisher Penalty starting from acertain epoch (x axis) and measure the final test accuracy (y axis). Fisher Penalty has to be appliedfrom the beginning of training to close the generalization gap to the optimal learning rate (c.f. the redhorizontal line to the blue horizontal line).
Figure 6: Small Tr(F) during the early phase of training is more likely to reach wider minima.
Figure 5: Fisher penalty slows down training on data with noisy labels more strongly than it slowsdown training on clean data for VGG-11 on CIFAR-100. This likely happens because FP penalizesmore strongly gradient norm on data with noisy labels. Left plot shows the training accuracy onexamples with clean/noisy labels (solid/dashed line). Middle plot shows the gradient norm evaluatedon examples with clean/noisy labels (solid/dashed). Right plot shows the ratio of gradient norm onclean to noisy data. Red to blue color represents the regularization coefficient (from 10-2 to 101).
Figure 7:	Association between early phase values of Tr(F) and generalization, holds on the CIFAR-10 and the CIFAR-100 datasets. Each point corresponds to multiple runs with randomly chosenseeds and a specific value of batch size. TrFi is recorded during early phase (2-7 epochs, see maintext for details), while the test accuracy is the maximum value along the entire optimization path(averaged across runs with the same batch size). The horizontal and vertical error bars show thestandard deviation of values across runs. The plots show that early phase Tr(F) is predictive of finalgeneralization.
Figure 8:	Same as Figure 3, but for DenseNet on CIFAR-100, and SimpleCNN on CIFAR-10. Curveswere smoothed for visual clarity.
Figure 9: Same as Figure 5, but for ResNet-50.
Figure 10: Small Tr(F) during the early phase of training is more likely to reach wider minima asmeasured by Tr(H). Left: 2 models are trained with different levels of regularization for 20 epochson CIFAR-10. Tr(F) at the end of 20 epochs (denoted as Tr(Fi)) is shown. Middle: Each model isthen used as initialization and trained until convergence using the low regularization configurationwith different random seeds. A histogram of Tr(H) at the point corresponding to the best testaccuracy along the trajectory (denoted by Tr(Hf )) is shown. Right: a histogram of the best testaccuracy corresponding to middle figure is shown.
Figure 11: The value of Tr(H) over the course of training. Each point corresponds to runs withdifferent seeds and a specific value of learning rate η and batch size S. ` and TA respectivelydenote the minimum training loss and the maximum test accuracy along the entire trajectory for thecorresponding runs (averaged across seeds). The plots show that flatter optimization trajectoriesbecome biased towards flatter minima early during training, at a coarse scale of hyper-parametervalues (red vs blue).
Figure 12: Correlation between Tr(F) and Tr(FB) for SimpleCNN trained on the CIFAR-10 dataset.
Figure 13: A comparison between the effect of recomputing Fisher Penalty gradient every 10 iterations(left) or every iteration (right), with respect to validation accuracy and Tr(F). We denote by f thefrequency with which we update the gradient. Both experiments result in approximately 80% testaccuracy with the best configuration.
Figure 14: Using Fisher Penalty without the approximation results in a similar generalizationperformance. We penalize the norm of the gradient rather than norm of the mini-batch gradient (asin Equation 2). We observe that this variant of Fisher Penalty improves generalization to a similardegree as the version of Fisher Penalty used in the paper (c.f. Figure 13.), achieving 79.7% testaccuracy.
Figure 15: Training with a large learning rate never (even for a single optimization step) enters aregion with as large value of Tr(F) as the maximum value of Tr(F) reached during training with asmall learning rate. We run the experiment using SimpleCNN on the CIFAR-10 dataset with twodifferent learning rates. The left plot shows the value of Tr(F) computed at each iteration, and theright plot shows training accuracy computed on the current mini-batch (curve has been smoothed forclarity).
Figure 16: Catastrophic Fisher explosion in large batch size training. Experiment run on the CIFAR-10 and dataset the SimpleCNN model. The left plot shows the value of Tr(F) computed at eachiteration, and the right plot shows training accuracy computed on the current mini-batch (curve hasbeen smoothed for clarity).
Figure 17: Fisher Penalty improves in large batch size training. Experiment run on the CIFAR-10dataset (without augmentation) and the SimpleCNN model. Warmer color corresponds to largercoefficient used in Fisher Penalty.
Figure 18: Correlation between Tr(F) and Tr(H).
