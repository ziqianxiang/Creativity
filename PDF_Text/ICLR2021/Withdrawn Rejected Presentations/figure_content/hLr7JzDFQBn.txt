Figure 1: Examples of mobile con-struction in nature (left), a termitemound section (Wikimedia Com-mons, 2018), and in our bench-mark (right), 1D brick-laying.
Figure 2: 1D environment: arobot moves along the x-axisand lays bricks (red). The twovertical blue dash lines indi-cate its sensing region for vec-tor o. The o = (5, 1, 1,0, 1)in this case. The blue curve isthe ground-truth plan P .
Figure 3: 2D environment:a robot moves in a 2d planeand lays bricks (red/yellow:in/correct laying). The bluebox is its sensing region for o,which is a 2d array with size(2Ws + 1)2 . The green ring isthe ground-truth plan P .
Figure 5: Example plans. The 3D red patches are the top-most surfaces of ground-truth plans.
Figure 4: 3D environment: a robot movesin a 2d x-y plane and lays bricks (dark bluecubes). Left/right is the 3Dâˆ•top view of theenvironment. The blue box is similar to Fig-ure 3. The red ring in the left image is thetop-most surface of ground-truth plan P .
Figure 6: Game GUI for measuring human performance. (a) 1DDynamic plan: user can see the ground truth plan in dynamicenvironments. (b) 2D Static plan in evaluation mode: only stepand brick count are shown, while rewards are hidden.
Figure 7: Benchmark results for all the deep RL and the human baselines.
Figure 8: Best testing results of all tasks (the same order as in Figure 7).
Figure 9: Influences of obstacles. Top rows shows the baselineresults on 2D and 3D static tasks. The first two images of bottomrow show results of adding obstacle to 2D world. Compared withbaseline, the performance drops notably. The last two images ofbottom row show results of removing obstacle from 3D world.
Figure 10: Influence of localization and environment uncertainty. Either adding GPS in (a) orremoving step size uncertainty in (b) significantly improves DRQN on sparse plan tasks comparedwith the first two columns of the top row of Figure 9.
Figure 11: Training history of IoU: thefull Rainbow algorithm does not alwaysoutperform its pruned configurations.
