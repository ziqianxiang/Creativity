Figure 1: The parameter changes as trainingis going on. From Iteration 0 to Iteration 2k,the effective displacement of this parameter isd(about 0.04), but the path length l is observ-ably much larger than d.
Figure 2: The Q value of AdaRem, Adam andSGDM for ResNet-18 on ImageNet. HigherQ value means more oscillations and uselessupdates. Compared with SGDM and Adam,AdaRem reduces oscillation more effectively.
Figure 3: Schematic diagram of spherically con-strained optimization. The length of the solidarrows shows the magnitude of the gradient vec-tors. According to Scale Invariance, networkat A and network at B are equivalent becausethey have the same output for any input. Simi-larly, network at A0 and network at B0 are equiv-alent. In addition, gt is larger than gt0 and theyare both perpendicular to OB . After the update,A0 is projected onto the point A00 on the sphere.
Figure 4: Train loss and test error of ResNet18 on ImageNet. Adam, AdamW, AdaBound andRMSProp have fast progress in the early stages, but their performance quickly enters a period ofslow growth. AdaRem achieves the fastest training speed among all methods and performs as well asSGDM on the test set.
Figure 5: Train loss and test error of three networks on ImageNet. Compared with SGDM, AdaRem-Ssignificantly reduces the training loss across all three networks. It generalizes as well as SGDM onResNet18 and ResNet50 and brings considerable improvement over SGDM on MobileNetV2.
