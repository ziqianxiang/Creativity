Figure 1: (a): DN input space partition for L = 3,D'=5, V', the boundaries induced by layer 1,2,3 aredepicted in blue, green and orange respectively. We can see how deeper layers successively subdivide the spaceto form the entire DN input space partition where in each region the DN behaves linearly. (b): K-means exper-iment on a toy mixture of 64 Gaussian in 2d (see Fig. 5). In all cases the number of final cluster is 64 but thenumber of starting clusters (x-axis) varies and pruning is applied during training to remove redundant centroidscomparing random centroid initialization and kmeans++. With overparametrization, random initialization andpruning reaches same performance as kmeans++.
Figure 2: (a) Difference between node and weight pruning. The former removes entire Subidivision lineswhile the latter simply quantise those partition lines to be colinear to the space axes. (b) Toy classificationtask pruning. Blue lines represent subdivisions in the first layer while red lines denote the last layer’s decisionboundary. We see that: 1) pruning indeed removes redundant subdivision lines so that the decision boundaryremains X -shape until 80% nodes are pruned; 2) ideally one blue subdivision line would be sufficient to providetwo turning points for the decision boundary, e.g., visualization at 80% sparsity, but the classification accuracydegrades a lot if further pruned. That aligns with the initialization dilemma for small DNs, i.e., blue lines arenot well initialized and all lines remain hard for training. (c) MNIST reproduction of (b). To produce thesevisuals, we choose two images from different classes to obtain a 2-dimensional slice of the 764-dimensionalinput space (grid depicted on the left). We thus obtain a low-dimensional depiction of the subdivision lines thatwe depict in blue for the first layer, green for the second convolutional layer, and red for the decision boundaryof 6 vs. 9 (based on the left grid). The observation consistently shows that only parts of subdivision lines areuseful for decision boundary; the goal of pruning is to remove those (redundant) subdivision lines.
Figure 3: Spline trajectory during training and visualization of the Early-Bird (EB) Phenomenon, which can beleveraged to largely reduce the total training costs due to the less training of costly overparametrized networks.
Figure 4: We depict on the left a small (L = 2, D1 = 5, D2 =8) DN input space partition, layer 1 trajectories in black andlayer 2 in blue. In the middle is the measure from 3 findingsimilar “partition trajectories” from layer 2 seen in the DN inputspace (comparing the green trajectory to the others with color-ing based on the induce similarity from dark to light). Based onthis measure, pruning can be done to remove the “grouped par-tition trajectoris” and obtain the pruned partition on the right.
Figure 5: Depiction of the dataset used for theK-means experiment with 64 centroids.
Figure 6: Depiction of a simple (toy) univariate regression task with target function being a sawtoothwith two peaks (left). On the right is the `2 training error based on how overparametrized is the initialDN. In all cases, the final DNs have the same number of units (pruning ratios are adapted from theinitial number of units).
Figure 7: Accuracy vs. efficiency trade-offs of EB training and layerwise pretraining.
Figure 8: Spline Early-Bird tickets in VGG-16 and PreResNet-101.
Figure 9: Abalation studies of the hyperparameter P in spline pruning.
