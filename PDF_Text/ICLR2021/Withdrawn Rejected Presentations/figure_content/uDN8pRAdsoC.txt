Figure 1: Computing hard masks for explaining the prediction of GNN.
Figure 2: Our approach is based on the rate-distortion framework, which assumes two parties, theencoder and decoder. The encoder has the original (restricted) feature matrix X and sends it througha noisy channel to the decoder, which receives YS. The goal is to determine, which parts of YS haveto have the original values from X, such that the fidelity is high, i.e., the decoder predicts the samelabel as the encoder with a high probability.
Figure 3: The number of explanations found with ZORRO at τ = 0.85.
Figure 4: Comparison of Zorro’s explanation size measured by the proportion of selected elements,i.e., the number of selected nodes is divided by the number of nodes in the computational graph,and the number of selected features is divided by the number of features. For the results of allcombinations, see Figure 7.
Figure 5: Comparison of fidelity, GNNEXPLAINER vs. ZORRO.Low: < 0.7, Med.: [0.7,0.85),High: [0.85,0.95) and V High: ≥ 0.954.3	HOMOPHILY OF THE EXPLANATIONSOne of the motivations of post-hoc interpretability is to use explanations to derive insights into amodel,s inner workings. Towards this, we investigate the homophily of the selected nodes fromour explanations since GNNs are known to exploit the homophily in the neighborhood to learnpowerful function approximators. We define homophily of the node as the fraction of the number ofits neighbors, which share the same label as the node itself. Intuitively, it should be easier to label anode with the correct label if a larger fraction of nodes in its computational graph shares its label.
Figure 6: Dataset - PubMed. The joint distribution of the homophily with respect to the nodesselected in the ZORRO’s explanation (τ = 0.85) with true and predicted labels. The orange contourlines correspond to the distributions for correctly predicted nodes, and the blue one corresponds toincorrectly predicted nodes.
Figure 7: Comparison on Zorro’s explanation size measured by the proportion of selected ele-ments, i.e. the number of selected nodes is divided by the number of nodes in the computationalgraph and the number of selected features is divided by the number of features.
Figure 8:	Dataset-Cora. Joint distribution of the homophily with respect to the selected subgraph (inthe explanation of ZORRO (τ = .85)) with true and predicted labels. The orange contour lines cor-respond to the distributions for correctly predicted nodes and the blue one correspond to incorrectlypredicted nodes.
Figure 9:	Dataset-CiteSeer. Joint distribution of the homophily with respect to the selected sub-graph (in the explanation of ZORRO (τ = .85)) with true and predicted labels. The orange contourlines correspond to the distributions for correctly predicted nodes and the blue one correspond toincorrectly predicted nodes.
Figure 10:	Distribution of GnnExplainer’ feature mask values corresponding to explanations ofa single node of GCN, where a fidelity of 1.0 is achieved. For the distribution of all nodes, models,and datasets, we refer to Figure 11.
Figure 11: Distribution of GnnExplainer’s feature mask values corresponding to explanations ofall explained nodes.
Figure 12: Comparison of the runtime by model and dataset. The cirlce and the cross mark the av-erage fidelity and average time of GnnExplainer and gradient baseline. The boxplots correspondto Zorro and visualize the runtime for retrieving the first explanation, which has at least the givenfidelity. For Zorro, we recorded the time after the initilization, which can be pre-computed for allnodes simultaneously. Time is measured in seconds.
Figure 13: Legend showing the symbols for the different classes of the synthetic dataset. Left andright the house graphs and both nodes (square and rotated square) represent the nodes in the BAgraphs of the two communities.
Figure 14: Two nodes of the synthetic example explained by Zorro and GnnExplainer, whichare correctly predicted. The first column shows the nodes with the ground truth labels. All nodesfrom the ground truth (house graph), as well as all the nodes selected in the explanation coloredas green (training set) and blue (test set). The second column shows the predicted classes and theexplanation(s), where we included in black all unselected nodes of the ground truth. All other colorscorrespond to each explanation, e.g., three disjoint explanations in a). The red circle highlights thenode, which is explained. The last column shows the computational graph of the explained nodeagain with the ground truth labels.
Figure 15: Two nodes of the synthetic example explained by Zorro and GnnExplainer, whichare wrongly predicted. Style same as Figure 14.
