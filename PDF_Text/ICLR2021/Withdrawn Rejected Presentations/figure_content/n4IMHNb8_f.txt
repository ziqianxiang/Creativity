Figure 1: Spatial Path Planning: The raw obser-vations (top left) and obstacles can be representedspatially via top-down map in navigation (left) andvia configuration space in manipulation (right).
Figure 2: Local vs Long-distance value propagation. Figure showing an example of number of iterationsrequired to propagate distance values over a map using local and long-distance value propagation. The obstaclemap and goal location shown on the left and the distance value predictions over 5 iterations is shown on theright (distance values increase from blue to yellow). Prior methods based on convolutional networks use localvalue propagation and require many iterations to propagate values accurately over the whole map (top right).
Figure 3: Spatial Planning Transformer (SPT). Figure showing an overview of the proposed Spatial PlanningTransformer model. It consists of 3 modules: an Encoder E to encode the input, a Transformer network Tresponsible for planning, and a Decoder D decoding the output of the Transformer into action distances.
Figure 4: End-to-end Mapping and Planning. Figure showing an overview of end-to-end mapping andplanning model for both the navigation and manipulation tasks.
Figure 5: Spatial Planning Examples. Figure showing 3 examples of the input, the predictions using theproposed SPT model and the baselines, and the ground truth for map size M = 30. The obstacles are shown inblue, free space in purple and goal in yellow in the leftmost input column. The predictions and ground truth inthe rest of the column are color-coded from blue to yellow to represent increasing action distance.
Figure 6: Sparse and Noisy Supervision. Figure showing examples of a map and goal with different levels ofsupervision. Noisy supervision adds gaussian noise to the ground truth distance values, and sparse supervisionsamples 5 trajectories for random starting locations to the goal location.
Figure 7: Navigation Mapper Architecture. Figure showing the architecture of the Navigation Mapper.
Figure 8: Attention Visualization. Visualization of the attention heads learned by Spatial Planning Transform-ers. SPTs learn an attention for each location in the map with respect to every other location.
Figure 9: Navigation in-distribution test set examples. Figure showing 3 examples of the input, the predic-tions using the proposed SPT model and the baselines, and the ground truth for the Navigation in-distributiontest set for map size M = 30.
Figure 10: Navigation out-of-distribution More Obstacles test set examples. Figure showing 3 examplesof the input, the predictions using the proposed SPT model and the baselines, and the ground truth for theNavigation out-of-distribution More Obstacles test set for map size M = 30.
Figure 11: Navigation out-of-distribution Real-World test set examples. Figure showing 3 examples of theinput, the predictions using the proposed SPT model and the baselines, and the ground truth for the Navigationout-of-distribution Real-World test set for map size M = 30.
Figure 12: Manipulation in-distribution test set examples. Figure showing 3 examples of the input, thepredictions using the proposed SPT model and the baselines, and the ground truth for the Manipulation in-distribution test set for map size M = 36.
Figure 13: Manipulation out-of-distribution More Obstacles test set examples. Figure showing 3 examplesof the input, the predictions using the proposed SPT model and the baselines, and the ground truth for theManipulation out-of-distribution More Obstacles test set for map size M = 36.
Figure 14: Dense and Perfect Supervision. Figure showing examples of map and distance predictions usingthe SPT model trained with dense and perfect action-level supervision.
Figure 15: Sparse and Noisy Supervision. Figure showing examples of map and distance predictions usingthe SPT model trained with sparse and noisy action-level supervision.
