Figure 1: The t-th layer of LISTA and EBT-LISTA.
Figure 2:	Disentanglement of the reconstruction error and learnable parameters in our EBT. z(t)here indicates ρ(t) and b(t) for networks with or without EBT, respectively.
Figure 3:	Thresholds obtained from different methods across layers.
Figure 4:	Validation of Theorem 2: there exist two convergence phases and our EBT accelerates theconvergence of LISTA-SS, in particular in the first phase.
Figure 5: NMSE of different methods when the test sparsity is different from the training sparsity.
Figure 6:	NMSE of different sparse coding methods under different noise levels. It can be seen thatour EBT performs favorably well under SNR=∞ and SNR=40dB.
Figure 7:	NMSE of different sparse coding methods in different settings where different sparsity anddifferent condition numbers are considered. When we vary the condition numbers, we fix pb=0.9.
Figure 8: Reconstruction 3D error maps of different methods in different settings. ζ here is the meanestimation error in degree. Note that the maximal error is 0.1 and 0.03 in theory when Pb = 0.8 andPb = 0.9, respectively.
Figure 9: NMSE of different sparse coding methods in different settings where different sparsityand different condition numbers are considered.
Figure 10: NMSE of different sparse coding methods when the sparsity of the data follows a certaindistribution.
Figure 11: NMSE of different sparse coding methods where different regularization coefficients λare considered.
