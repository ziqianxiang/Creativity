Figure 1:	Illustration of the multi-level scaling (MLS) low-bit tensor data format.
Figure 2:	Computation flow of the proposed low-bit training.
Figure 3: Average relative quantization errors (AREs) ofW, E, A (left, middle, right) in each layerwhen training a ResNet-20 on CIFAR-10. X axis: Layer index. Row 1: Different grouping dimensions(h0, 3 formatted X,〈8,1)formatted Sg); Row 2: Different Ex ({Eχ, 3〉formatted X, no group-wisescaling); Row 3: Different Ex (〈Ex, 3) formatted X,(8,1) formatted Sg, N X C groups).
Figure 4: The convolution hardware architecture. (a) Previous studies (Mellempudi et al., 2019)developed low-bit floating-point multiplication (FP MUL) (e.g., 8-bit), but FP32 accumulations arestill needed. (b) We not only makes FP MUL less than 8-bit, but also simplifies the local accumulator.
Figure 6: Performances with different (Eχ, Mxi configurations, no group-wise scaling is used.
