Figure 1: We focus on video synthesis from actions and propose a new task called Action Graph to Video. Torepresent input actions, we use a graph structure called Action Graph, and together with the first frame andfirst scene layout, our goal is to synthesize a video that matches the input actions. For illustration, we includeabove a (partial) example. Our model outperforms various baselines and can generalize to previously unseencompositions of actions.
Figure 2: Example of a partial Action Graph execution schedule in different time-steps.
Figure 3: Our AG2Vid Model. The AG At describes the execution stage of each action at time t. Togetherwith the previous layout `t-1, it is used to generate the next layout `t which has object representations that areenriched with At actions information. Then, `t, `t-1, vt-1 are used to generate the next frame.
Figure 4: Qualitative examples of generation on CATER and Something Something. AG2Vid generated videosof four and eight standard actions on CATER and Something Something, respectively. For CATER we alsoused AGs with multiple simultaneous actions, and the generated actions indeed correspond to those (verifiedmanually). For more examples please refer to Figure 1 and 2 in the Supp. Click the image to play the video clipin a browser.
Figure 5: Comparison of baselines methods. Thetop row are based on CATER videos, while thebottom row are based on Something Something.
Figure 6: Composing unseen actions in Something-Something and CATER. For example, the “swap” ac-tion is composed by combining the “Pick-Place” and“Slide” actions on frames 1 - 10 and their locations.
Figure 7: Qualitative examples for the generation of actions on the CATER dataset. We use the AG2Vid modelto generate videos of four standard actions and two composed unseen actions (“Swap” and “Huddle”). Theobjects involved in actions are highlighted. Click the image to play the video clip in a browser.
Figure 8: Qualitative examples for the generation of actions on the Something Something dataset. We use ourAG2Vid model to generate videos of eight standard actions and two composed unseen actions (“Right Up” and“Down Left”). Click the image to play the video clip in a browser.
Figure 10: Comparing Sg2Im and Ag2Vid results in CATER. Each column is a different sample. Click theimage to play the video clip in a browser.
