Figure 1: DC-VAE Reconstruction (top) and Sampling (bottom) on LSUN Bedroom Yu et al. (2015)at resolution 128 × 128 (left) and CelebA-HQ (Karras et al., 2018) at resolution 512 × 512 (right).
Figure 3: Qualitative results of CIFAR-10 (Krizhevsky et al., 2009) images (resolution 32 × 32) for experimentsin Table 1 (Krizhevsky et al., 2009).
Figure 4: Comparison of DC-VAE (resolution 512 × 512) with IntroVAE (Huang et al., 2018) (resolution1024 × 1024).
Figure 5: Interpolation results generated by DC-VAE (ours) on CelebA-HQ (Karras et al., 2018) images(512 × 512, left) and LSUN Bedroom Yu et al. (2015) images (128 × 128, right). More images can be seen inAppendix Figure 9. (Zoom in for a better visualization.)5.5	Latent Space Representation: ClssificationTo show that our model learns a good representation, we measure the performance on the downstreamMNIST classification task (Ding et al., 2020). The VAE models were trained on MNIST dataset(LeCun, 1998). We feed input images into our VAE encoder and get the latent representation. Then wetrain a linear classifier on the latent representation to classify the classes of the input images. Resultsin Table 5 show that our model gives the lowest classification error in most cases. This experimentdemonstrates that our model not only gains the ability to do faithful synthesis and reconstruction, butalso gains better representation ability on the VAE side.
Figure 6: Image editing on CelebA-HQ (Karras et al., 2018) validation set images (resolution 512 × 512). Themethod used to generate these is outlined in Appendix A.2.
Figure 7: Additional CelebA-HQ (Karras et al., 2018) reconstruction images (resolution 512 × 512)generated by DC-VAE (ours)14Under review as a conference paper at ICLR 2021Figure 8: Additional LSUN Bedroom (Yu et al., 2015) reconstruction images (resolution 128 × 128)A.2 Analysing the latent spaceIn this section we analyse the smoothness of the latent space learnt by DC-VAE. In Figure 9 wequalitatively show the high resolution (512 × 512) CelebA-HQ Karras et al. (2018) images generatedby an evenly spaced linear blending between two latent vectors. In Fig. 6 we show that DC-VAEis able to perform meaningful attribute editing on images while retaining the original identity. Toperform image editing, we first need to compute the direction vector in the latent space that correspondto a desired attribute (e.g. has glasses, has blonde hair, is a woman, has facial hair). We computethese attribute direction vectors by selecting 20 images that have the attribute and 20 images thatdo not have the attribute, obtaining the corresponding pairs of 20 latent vectors, and calculating the15Under review as a conference paper at ICLR 2021difference of the mean. The results in Fig. 6 show that these direction vectors can be added to a latentvector to add a diverse combination of desired image attributes while retaining the original identity ofthe individual.
Figure 8: Additional LSUN Bedroom (Yu et al., 2015) reconstruction images (resolution 128 × 128)A.2 Analysing the latent spaceIn this section we analyse the smoothness of the latent space learnt by DC-VAE. In Figure 9 wequalitatively show the high resolution (512 × 512) CelebA-HQ Karras et al. (2018) images generatedby an evenly spaced linear blending between two latent vectors. In Fig. 6 we show that DC-VAEis able to perform meaningful attribute editing on images while retaining the original identity. Toperform image editing, we first need to compute the direction vector in the latent space that correspondto a desired attribute (e.g. has glasses, has blonde hair, is a woman, has facial hair). We computethese attribute direction vectors by selecting 20 images that have the attribute and 20 images thatdo not have the attribute, obtaining the corresponding pairs of 20 latent vectors, and calculating the15Under review as a conference paper at ICLR 2021difference of the mean. The results in Fig. 6 show that these direction vectors can be added to a latentvector to add a diverse combination of desired image attributes while retaining the original identity ofthe individual.
Figure 9: Additional latent space interpolations on CelebA-HQ (Karras et al., 2018) (resolution512 × 512)17Under review as a conference paper at ICLR 2021Source Am ①BnoSFigure 10: Latent Mixing results on CelebA-HQ Karras et al. (2018). EaCh combined image in thegrid is generated by replacing an arbitrary subset of Source A latent with the corresponding Source Blatent.
Figure 10: Latent Mixing results on CelebA-HQ Karras et al. (2018). EaCh combined image in thegrid is generated by replacing an arbitrary subset of Source A latent with the corresponding Source Blatent.
Figure 11: Pixel reconstruction error on CIFAR-10 Krizhevsky et al. (2009) test set for varyingnumber of negative samplesimages at 96 × 96 resolution. We follow the procedure in AutoGAN(Gong et al., 2019) and resizethe STL-10 images to 32 × 32. The CelebA dataset has 162,770 training images and 19,962 testingimages, CelebA-HQ contains 29,000 training images with 1,000 test images of size 1024 × 1024,and LSUN Bedroom has approximately 3M images. We resize all images progressively in these threedatasets from (4 × 4) to (512 × 512) for the progressive training.
Figure 12: Visualization of the effect of adding each instance level and set level objectives. Table 1and Figure 3 contain FID (Heusel et al., 2017) results and qualitative comparisons on the CIFAR-10(Krizhevsky et al., 2009) that correspond to these settings.
Figure 13: DC-VAE synthesis images on LSUN images (Yu et al., 2015) (resolution 128 × 128)21Under review as a conference paper at ICLR 2021(a) STL10 Reconstructions generated by DC-VAE (b) STL10 Samples generated by DC-VAEFigure 14:	DC-VAE reconstruction (a) and synthesis results (b) on STL10 (Coates et al., 2011)images (resolution 32 × 32). In (a) the top two rows are input images and the bottom two rows arethe corresponding reconstruction images.
Figure 14:	DC-VAE reconstruction (a) and synthesis results (b) on STL10 (Coates et al., 2011)images (resolution 32 × 32). In (a) the top two rows are input images and the bottom two rows arethe corresponding reconstruction images.
Figure 15:	Network architecture of DC-VAE for resolution 32 × 32 for CIFAR-10 (Krizhevskyet al., 2009) and STL-10 (Coates et al., 2011). (a) is the Encoder. (b) is the Decoder. (c) is theDiscriminator.
