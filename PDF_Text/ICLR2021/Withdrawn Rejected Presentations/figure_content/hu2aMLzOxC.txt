Figure 1: (a) We train a policy that controls a robot arm operating in a table-top setting. (b) Ran-domly placed ShapeNet (Chang et al., 2015) objects constitute an initial state distribution for train-ing. (c) We use multiple manually designed holdout tasks to evaluate the learned policy.
Figure 2: (a) We train a goal-conditioned policy on a single training distribution and evaluate itsperformance on many unseen holdout tasks. (b) To construct a training distribution, we sample aninitial state from a predefined distribution, and run a goal setting policy (Alice) to generate a goal.
Figure 3: Holdout tasks in the environment using 1 or 2blocks. The transparent blocks denote the desired goalstate, while opaque blocks are the current state. (a) push:The blocks must be moved to their goal locations andorientations. There is no differentiation between the sixblock faces. (b) flip: Each side of the block is labelledwith a unique letter. The blocks must be moved to makeevery face correctly positioned as what the goal specifies.
Figure 4: Generalization to unseen holdout tasks for blocks. Baselines are trained over a mixtureof all holdout tasks. The solid lines represent 2-blocks, while the dashed lines are for 1-block. Thex-axis denotes the number of training steps via asymmetric self-play. The y-axis is the zero-shotgeneralization performance of Bob policy at corresponding training checkpoints. Note that successrate curves of completely failed baselines are occluded by others.
Figure 5: Goals discovered by asymmetric self-play. Alice discovers many goals that are not coveredby our manually designed holdout tasks on blocks.
Figure 6: The empirical payoff matrix be-tween Alice and Bob. Average success rateover multiple self-play episode is visualized.
Figure 7: Example holdout tasks involving unseen objects and complex goal states. The goal statesare illustrated here, and the initial states have randomly placed objects.
Figure 8: Success rates of a single goal-conditioned policy solving a variety of holdout tasks, aver-aged over 100 trials. The error bars indicate the 99% confidence intervals. Yellow, orange and bluebars correspond to success rates of manipulation tasks with blocks, YCB4objects and other uniquelybuilt objects, respectively. Videos are available at https://robotics-self-play.github.io.
Figure 9: The ablation studies compare four ablation runs each with one component disabled withthe full baseline. Solid lines are for 2-blocks, dashed lines are for 1-block. The x-axis denotesthe number of training steps via asymmetric self-play. The y-axis is the zero-shot generalizationperformance of Bob policy at corresponding training steps.
