Figure 1: The visualization of full precision weights distribution in BWNs. The X-axis indicates thefull precision weight value while Y-axis indicates the frequency that the value appears in a certainlayer of a binary weights network. In the figure captions, VGG is VGG-7, and R-20 is ResNet-20.
Figure 2: Inference accuracy on training sets after flipping a certain percentage of weights’ signs.
Figure 3: For each group, we display three figures whose X-axis is the training epoch and Y-axisis: Left: the overlapping percentage of those largest weights’ signs between the final weights andthe weights during training, Middle: the overlapping percentage of those largest weights’ normbetween the final weights and the weights during training, Right: the hamming distance divided bythe number of parameters between binarized weights during training and the final trained binarizedweights, which ranges between 1 (completely different from another) to 0 (the same).
Figure 4: The visualization of binary weight kernels in one Conv layer after assigning indices. TheX-axis indicates the index of a 3×3 binary weight kernel while Y-axis indicates the frequency thatthe binary kernel appears in one certain Conv layer. Left Figure is an example to illustrate how weindex a 3 × 3 kernel into the range of 0 to 511. Two figures on the right are from the last Convlayer of two networks. Right Figures are the visualization of binary weight kernels in XNor-BWNVGG-7’s last Conv layer and XNor-BWN ResNet-20’s last Conv layer after assigning indices. TheX-axis indicates the index of a 3 × 3 binary weight kernel while Y-axis indicates the frequency thatthe certain appears in Conv layer.
Figure 5: This is a pipeline to illustrate our compressing method on BWNs using 2-bit kernels.
Figure 6: The relation between the scaling factor of the LQ-BWN and the gamma in BatchNormlayer after the corresponding Conv layer channel by channel after normalizing their values. TheX-axis is the scaling factor value and Y-axis is the gamma in the corresponding BatchNorm layer.
Figure 7: L2 Distance between full-precision weights and binarized weights during training. We usethe L2 distance of the trained networks at last epoch as the unit norm, and other L2 distances aredivided by this unit distance to better display the increase or decrease trend. textbfw/o WD meanswe do the same experiments on training the network without using weight decay. Two figures aredisplayed together and the left one uses WD while the right one does not. The X-axis is trainingepochs while Y-axis is the re-scaled sum of L2 norm of all binarized layers.
Figure 8:	Full-precision weight histograms visualization in ResNet-18. We choose the weights fromthe second Conv in the first Block of each group, and the first Conv layer Conv0.
Figure 9:	XNor-BWN and LQ-BWN ResNet-20 Weights’ signs tracing and weights’ norm overlap-ping with positive weights. And the hamming distance between the weights during training and theweights after training.
Figure 10:	ResNet-18 Weights’ signs tracing and weights’ norm overlapping with both positiveweights and negative weights.
Figure 11: Hamming Distance between binary weights while training and binary weights after train-ing. X-axis is the epoch number, and Y-axis is the hamming distance over the total binary weightnumbers, zero means they become the same.
Figure 12:	Flip 10% of a trained BWN and fixing these weights, retrain with different learning rates,on VGG-7 and ResNet-20. The left two figures compare the flipping strategies, and the right twofigures compare the learning rate magnitude, which use small learning rate and large learning rate.
Figure 13:	The sum of frequency of top 2p binary-kernels of XNor-BWN and LQ-BWN ResNet-18.
Figure 14:	The ratio between the number of kernels in one layer that are in our selected binary-kernels, and that are in the top 2p frequent binary-kernels in that layer. Thus 100% is the upperbound, which means our selected binary-kernels can cover the same number of kernels that thislayer’s top frequent kernels can cover. The X-axis is log2 scale to visualize quantization bits. G0 toG3 are the groups of ResNet-18, and each group contains two blocks, two layers for each block.
Figure 16: The test accuracy of 5 experiments for each hyperparamter ∆ of different values.
