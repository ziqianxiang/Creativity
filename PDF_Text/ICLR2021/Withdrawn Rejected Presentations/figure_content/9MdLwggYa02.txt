Figure 1: Training on the Rosenbrock benchmark. ROMUL outperforms initiator and truncationselection because it can adapt its step size. Bottom plots: Trajectories of 100 PBT training steps (16jobs per step) on the Rosenbrock function with a = 1 and b = 100 (minimum at the red cross (1, 1),trajectories go from blue to green)(b) Loss R1,100 with respect to the number of steps(lower is better, minimum value is 0). ROMUL keepsadapting and decreasing while other optimizers arelocked to higher levels depending on their step sizes.
Figure 2: Trajectories of 100 Initiator PBT training steps (16 jobs per step) on the Rosenbrockfunction with a = 1 and b = 100 (minimum at the red cross (1, 1), trajectories go from blue togreen)Training	mean (log10)	std (log10)Initiator (0.8/1.2 mult. steps)	-1.18	0.045Initiator (big steps)	-0.707	0.069Initiator (small steps)	-0.992	0.143ROMUL	-2.101	0.678Truncation Selection	-0.834	0.327Table 3: Final loss (in log10) mean and standard deviation for independent runs on the Rosenbrocktestbed, computed over 20 runs (two-sample Welsh’s test provides p < 1.1e - 5 when comparingeach algorithm with ROMUL).
Figure 3: Score on the multi-variate Rosenbrock benchmark (explained in 3.1) over 20 experimentsfor each point. Lower is better, standard deviations are indicated. ROMUL performs well across theboard for a wide range of number of variables, being surpassed only by Truncated selection in someregime (n ∈ J8 . . . 14K).
Figure 4: Dropout schedule of the best run of ROMUL 32 workers on PTBA.2 Language modeling on Penn Tree BankA.3 Regularization schedules on Wikitext-103Figure 5: Lower dropout values are better early, but are outperformed by more strongly regularizedmodels later (red, orange and blue lines) - here on wikitext103 with a 247M parameters languagemodel from Fan et al. (2019) (Adaptive Inputs + LayerDrop). PBT algorithms would tend to reducedropout aggressively early on: after that, even if the dropout is increased later, the performanceremains worse than training with a high dropout from the beginning (red line). Perhaps counterin-tuitively, this hints against increasing regularization over the course of the training - in the opposite,we observe that fine-tuning the model without dropout significatively improves test performance(purple line reaches 17.98 test perplexity) compared to the baseline (green: 18.42 test perplexity)12Under review as a conference paper at ICLR 2021A.4 Slides for internal presentationTraining	Parallelism	Validation PPL	Test PPLTransformerXL SOTA	1	59.65	55.43ASHA	16	63.20	58.35Truncation Selection PBT	16	60.24	57.29Initiator PBT	16	59.42	55.80ROMUL PBT	16	57.83	55.16
Figure 5: Lower dropout values are better early, but are outperformed by more strongly regularizedmodels later (red, orange and blue lines) - here on wikitext103 with a 247M parameters languagemodel from Fan et al. (2019) (Adaptive Inputs + LayerDrop). PBT algorithms would tend to reducedropout aggressively early on: after that, even if the dropout is increased later, the performanceremains worse than training with a high dropout from the beginning (red line). Perhaps counterin-tuitively, this hints against increasing regularization over the course of the training - in the opposite,we observe that fine-tuning the model without dropout significatively improves test performance(purple line reaches 17.98 test perplexity) compared to the baseline (green: 18.42 test perplexity)12Under review as a conference paper at ICLR 2021A.4 Slides for internal presentationTraining	Parallelism	Validation PPL	Test PPLTransformerXL SOTA	1	59.65	55.43ASHA	16	63.20	58.35Truncation Selection PBT	16	60.24	57.29Initiator PBT	16	59.42	55.80ROMUL PBT	16	57.83	55.16Table 5: Perplexity (lower is better) on PTB for a Transformer-XL with 16 layers and 24M parame-tersAlgorithm	CIFAR-10	CIFAR-100
