Figure 1: Through Bractivate, we discover UNet architecture withhigh spatio-temporal efficiency by mimicking the brain,s dendriticbranching.
Figure 2: During each search iteration, Bractivate chooses a model from the randomly initialized search do-main. After We train on the dataset, We evaluate its performance. To mutate, We identify the most active blockin the ”best model” architecture, as per Equation 1; to that active block, we initialize new skip connections(branches) pointing from other blocks in the network to the highest-activation block, propagating their signalsthrough randomly chosen branches.
Figure 3: Bractivate samples from a randomly initialized domain,constrained by the model depth, D. The efficiency evaluator comparesa selected model with the current ”best model” genome. If the ”bestmodel” outperforms the current model, we mutate the ”best model”(Figure 2) and replace the chosen model with the mutated version inthe search space.
Figure 4: A sample of the discovered architecture through theefficiency loss scaling in Equation 3. The green block is themost active block, having the highest number of incoming den-dritic connections.
Figure 5: Comparing the effects of model depth on the corre-lations between the Dice coefficient performance metric anddifferent depth constraint D over three trials. Top: Dice vstime per epoch; Middle: Dice vs. number of parameters(spatial complexity) Bottom: Dice vs the number of modelparams.
Figure 6: Activation map comparison for the second deconv block's second convolutional layer for the archi-tecture described in Figure 4. Blue represents low saliency, and red represents high saliency. We ablate the mostsalient block (The green block containing Decode2 Conv2) in the network and display activation differencesafter ablation.
Figure 7: Comparing model performance measured by the Dice score as a function of different model archi-tectures. Note that these models are all Xavier initialized.
Figure 8: Each prism represents a layer in the overall repeating block motifs in the network. A) The con-tracting block micro-architecture for one bloc. Note that this motif repeats throughout the contracting phaseof the network. n, the number of channels, is factored by 1.5 for each subsequent contracting block. B) TheExpanding block of the micro-architecture. n, the number of channels, is factored by .75 for each subsequentexpanding block.
Figure 9: The efficiency loss scaling (ELS) loss function selects a smaller model (orange) that can perform atthe level as one selected simply by BCL alone (blue).
