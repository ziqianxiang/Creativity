Figure 1: Architecture of Proposed Semi-supervised CAEWe base our model off of a two-layer Convolutional Autoencoder (CAE) architecture. The CAEuses two 1-D convolutional layers to featurize the time series. The filter widths for both layers arecalculated at runtime from the length of the time series. Each layer of the encoder contains a 1Dconvolution operation, followed by a non-linear activation. These layers are paired with transposeconvolution layers in the decoder. After each convolution layer in the encoder, we also apply amax-pooling operation to further reduce the length of the featurized sequence. Each max-poolingoperation is “reversed“ using a nearest-neighbor upsampling operation in the decoder. Alternatively,large strides in the convolutional layer may be used instead of pooling operations. This accomplishesthe same goal of reducing the length of the featurized sequence, but does not require any up-samplingoperations in the decoder, since the transpose convolution operation with a stride will perform theupsampling. We found that the max-pooling and large stride methods produced similar results inpractice.
Figure 2: Overall performance results for selected datasets4.2.	1 Overall PerformanceThe results for FacesUCR are presented in Figure 2a. As shown in the figure, the semi-supervisedapproaches significantly improve the performance of the model, relative to the baseline CAE ARIof 0.35. According to Table 1, the CAE’s performance here is much better than the k-Means per-formance on raw data, but slightly worse than k-Shape, which achieves an average ARI of 0.441.
Figure 3: Hyperparameter Study ResultsIn the performance evaluation results from Section 4.2.1, we perform parameter updates at the end ofeach batch. In order to better understand how the frequency of parameter updates affects the overallperformance, we also experiment by applying the update for the semi-supervised loss at the end ofeach epoch, while still updating parameters from the autoencoder loss at the end of each batch. Inorder to accomplish this, we calculate the semi-supervised gradients at each batch, accumulatingthem and applying the sum as the gradient update at the end of each epoch. For this experiment, wetrain the model on the UWaveGestureLibrary dataset and choose 12 supervised examples per class.
Figure 4: Classification performance for three selected datasetsIn the results for FacesUCR as seen in Figure 4a, we see a clear distinction in performance betweenthe semi-supervised models and the unsupervised autoencoder. Even for low numbers of supervisedexamples, all semi-supervised models outperform the autoencoder by a large margin, and the au-toencoder never closes the gap in performance, even for larger numbers of supervised examples.
