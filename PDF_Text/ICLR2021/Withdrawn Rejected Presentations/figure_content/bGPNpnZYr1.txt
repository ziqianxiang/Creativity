Figure 1: An example of LPDR between asample x = x0 and a hypothesis h = hain binary classification using the hθ (x) =I[x > θ] on input X 〜U [0,1].
Figure 2: Empirical validation of Assumption 1. Left figure: Relationship between approximatedhypothesis distance and σ at step t = 0. Hypothesis distance is almost linearly proportional tolog(σ) in the ascension. Right figure: Relationship between variation ratio and σ (MNIST). Sampledistance to the decision boundary can be expressed as σ at which the variation ratio is not zero forthe first time (white arrow). The unlabeled samples are ordered in terms of LPDR.
Figure 3: Empirical validation of Assumption 2. Left figure: Spearman’s rank correlation coefficientbetween LPDR and the variation ratio in terms of σ showing that there exists a σ such that LPDRand the variation ratio have a strong rank correlation. Right figure: An example of strong negativecorrelation between both ranks when log(σ) = -5.0. Samples with increasing LPDR or variationratio are ranked from high to low.
Figure 4: The performance comparison of LPDR with the uncertainty based active learning algo-rithms on MNIST, CIFAR10, SVHN and EMNIST datasets (Random: random sampling, Entropy:entropy based uncertainty sampling, MC-BALD: MC dropout sampling with BALD, MC-VarR: MCdropout sampling with variation ratio, ENS-VarR: ensemble network with variation ratio). Over-all, LPDR consistently either performs best or comparable with all other algorithms regardless ofdataset. The performance of all algorithms except LPDR tend to be data dependent.
Figure 5: Performance comparison with respect to the network capacity on CIFAR100 dataset. Theperformances of all algorithms except LPDR are much worse than that of Random when using K-CNN, which has a relatively smaller network capacity than that of WRN-16-8. LPDR is able toperform consistently better than Random regardless of the network capacity.
Figure 6: The performance comparison on Tiny ImageNet and Food101 datasets with WRN-16-8.
Figure 7: The performance comparison onHAM10000 dataset with WRN-16-8. LPDRoutperforms all other algorithms on imbal-anced dataset.
Figure 8: An example of negative Spearman’s rank correlation between LPDR and the variationratio for each experimental setting.
Figure 9: The P0n and σ with respect to the labeling proceeds for all experimental settings. LPDRreliably guides the P0n to be the target value by increasing the variance of sampling as the number oflabeled samples increases.
Figure 10: The final accuracy with respect to the target ρ0n on MNIST dataset. LPDR performs bestin a wide range of the target ρ0n .
Figure 11: The performance comparison with respect to the hyperparameters of LPDR on MNISTand CIFAR10 datasets. LPDR is robust against β and N , and has no significant performance differ-ence whether the sampling is applied to the parameters of last layer or all layers.
Figure 12: The empirical errors of the learned and the sampled hypotheses with respect to theacquisition step for all experimental settings. It is observed that the empirical error of the learnedhypothesis or the sampled hypothesis is not zero.
Figure 13:	The test accuracy with respect to the number of labeled samples from initial to final stepfor all experimental settings.
Figure 14: The comparison of performance between LPDR and MC-BALD on MNIST datasetwhere the query size is 1 or 20. The performance of BatchBALD with q > 1 does not exceed thatof MC-BALD (q = 1) and LPDR (q = 20) outperforms MC-BALD (q = 1).
Figure 15: The comparison of AUC on HAM10000 dataset. LPDR performs comparable with thebest performing algorithms.
Figure 16: The rank and Dolan-More curves of the algorithms across all experimental settings andrepetitions. The left figure shows rank curve which is the mean of ranks on all datasets at each step.
