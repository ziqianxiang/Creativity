Figure 1: Google search images from WebVision (Li et al., 2017) dataset with keyword “orange”.
Figure 2: Our proposed framework for noise-robust contrastive learning. We project images into a low-dimensional subspace, and regularize the geometric structure of the subspace with (1)Lcc a consistency con-trastive loss which enforces images With perturbations to have similar embeddings; (2)LPjmix： a prototypicalcontrastive loss augmented with mixup, which encourages the embedding for a linearly-interpolated input tohave the same linear relationship w.r.t the class prototypes. The low-dimensional embeddings are also trained toreconstruct the high-dimensional features, which preserves the learned information and regularizes the classifier.
Figure 3: Curriculum learned by the proposed label correction method for training on CIFAR datasets with50% sym. noise. (a) Accuracy of pseudo-labels w.r.t to clean training labels. (b) Number of samples in theweakly-supervised subset Dstup. (c) Label noise ratio in the weakly-supervised subset.
Figure 4: Examples of input noise injected to CIFAR-10.
Figure 5: t-SNE visualization of low-dimensional embeddings for CIFAR-10 images (color represents the trueclass) + OOD images (gray points) from CIFAR-100 or SVHN. The model is trained on noisy CIFAR-10 (50kimages with 50% label noise) and 20k OOD images with random labels. Our method can effectively learn to (1)cluster CIFAR-10 images according to their true class, despite their noisy labels; (2) separate OOD samplesfrom in-distribution samples, such that their harm is reduced.
