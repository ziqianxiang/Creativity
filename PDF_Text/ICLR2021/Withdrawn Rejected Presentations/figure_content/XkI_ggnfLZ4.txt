Figure 1: Test accuracy at various sparsity levels, with the regular lottery ticket procedure, i.e. LRfind =LReval. “Lottery” is the selected mask (winning ticket), “global random” is a mask selected at random globally,and “preserve LPR” is a random mask that has the same LPR as the winning ticket. (a) LR = 0.5 performedvery poorly, even worse than the random baseline. (b) Curves we would expect from a properly functioninglottery ticket. The pruned performance is much better, even though training a regular model without pruning isbetter with LR as 0.5 than 0.2.
Figure 2: Test accuracy for different combinations of Hfind and HeVab adjusting learning rate (a, d), batch size(b, e), or weight decay (c, f). Top row (a-c): given a constant HeVab We compare different values of Hfind.
Figure 3: Comparison of the best coupled vs. decoupled run, for all three hyperparameters. For clarity, eachpoint at a given sparsity level is mean and standard deviation of the best configuration of any decoupled (blue)or coupled (orange) combination of Hfind, Heval at that sparsity level. This was done because the best Hevalchanges with sparsity level, and this was cleaner than plotting several lines.
Figure 4: Test accuracy on variants of LT, with two values of LRfind and two values of LReval each. (a) Laterewinding to 2 epochs instead of initialization. (b) Learning rate warmup. (c) Learning rate rewinding, whereweights continue training from their values during pruning rather than rewinding back to an earlier value. Thebetter LRfind for pruned models is the smaller one (blue is better than green, orange is better than red), despiteperforming worse on the unpruned model, and the best LReval starts high and then decreases.
Figure 5: Hfind should also be decoupled from Heval in one-shot structured pruning. Test accuracy for twovalues of Hfind and two Heval for four combinations each for learning rate (a), batch size (b), and weight decay(c). The best performing configuration requires decoupled values; though this only applies to low sparsity levelsfor batch size, the pattern is clearer for learning rate and weight decay. See Figures A11-A12 for full results.
Figure 6: Layerwise pruning ratios depend on Hfind. Shown here is the LPR of masks found by the threedifferent values of LRfind (a), BSfind (b), and WDfind (c). Small LR, large BS, and small WD values tend toprune more in the earlier layers. See Figure A17 in the appendix for equivalent plots at other sparsity levels.
Figure 7: Test accuracy when LPR is constrained. For each of learning rate (a), batch size (b), and weightdecay (c), the blue line represents pruning using bad Hfind but LPR fixed to a run from good Hfind. The orangeline represents pruning using good Hfind but LPR fixed to a run from bad Hfind, which performs much morepoorly. For comparison, the green lines are regular pruning runs (from Figure 2) that have the same LPR as theblue line, except with a different Hfind used for finding the masks within each layer. All three lines in each plotare evaluated with the same Heval.
Figure 8: LPR for different LRfind values at (a) 67% pruned (b) and 89% pruned. When weight decay isset to zero, LRfind has a significantly smaller effect on LPR. This is in contrast to Figure 6a, where the earlylayers get pruned much more at higher learning rates. As a result, overall performance is no longer harmed bya large LRfind: when comparing a large range of LRfind, all evaluated with a reasonable LReval = 0.5 and WDeval= 1e-4, the larger LRfind does better (c).
Figure 9: Early pruning: Pruning at epoch 20 (blue) is bad, but pruning at epochs 50 (orange) or 70 (green)is actually better than pruning at convergence (110 epochs, red) for LRfind = 0.2. Pruning at 70 for LRfind =0.05 also performs well. All masks are evaluated with LReval = 0.5, except for the points at 0% sparsity, whichrepresent an unpruned model trained for the corresponding number of epochs.
Figure A1: Test accuracy at various sparsity levels, with the regular lottery ticket procedure, i.e. Hfind= Heval.
Figure A2: All combinations of LRfind and LReval. Top (a-c): given a constant LReval, we compare differentvalues of LRfind. 0.1 does the best, followed closely by 0.2 then 0.05, and everything is significantly better than0.5. Bottom (d-f): given a constant LRfind, we compare different values of LReval. 0.5 does the best at lowersparsity levels and 0.2 at higher sparsity levels. Random baselines have the same LRfind and LReval as the valuein the title.
Figure A3: All combinations of learning rate plotted together for comparisona)Fraction of weights prunedMasks found with BSfInd =d)Masks evaluated with BSeva∣ = 512b)&A3e-nuuB⅛8J.
Figure A4: All combinations of BSfind and BSeval. Top (a-c): given a constant BSeval, we compare differentvalues of BSfind, showing that larger is better. Bottom (d-f): given a constant BSfind, we compare differentvalues of BSeval. 256 does the best at low to medium sparsity levels and 512 at higher sparsity levels. Randombaselines have the same BSfind and BSeval as the value in the title.
Figure A5: All combinations of batch size plotted together for comparison14Under review as a conference paper at ICLR 2021a)Aue-Inuue⅛8J.
Figure A6: All combinations of WDfind and WDeval. Top (a-c): given a constant WDeval, we compare differentvalues of WDfind , showing that smaller is better. Bottom (d-f): given a constant WDfind, we compare differentvalues of WDeval. 5e-4 does the best at lower sparsity levels and 1e-4 at higher sparsity levels. Random baselineshave the same WDfind and WDeval as the value in the title.
Figure A7: All combinations of weight decay plotted together for comparison15Under review as a conference paper at ICLR 2021A.2.1 Other models and datasetsMiniPlaces, LReva∣ = 0.02b)	MiniPIaCeS, LRevai = 0.1a)>UE⊃υυrattυlFraction of weights prunedMiniPIacesJRfind = 0.1Fraction of weights prunedMiniPIaces, LReva∣ = 0.5	d)Fraction of weights pruned	Fraction of weights prunedFigure A8: ResNet-50 on MiniPlaCes with LRfind and LReVaI from {0.02, 0.1, 0.5}. In (a), (b), and (c), Weuse a constant LReval of 0.02, 0.1, and 0.5 respectively, with each plot showing all three values of LRfind. Theyshow that LReval = 0.1 is the best. Thus, in (d) we compare the different values of LReVab all with LRfind = 0.1,to demonstrate that the best LReVaI is 0.5, until very high sparsity levels where 0.1 becomes better. In line withother results, the best LR for standard training (0.5) is not the best LRfind for pruning.
Figure A8: ResNet-50 on MiniPlaCes with LRfind and LReVaI from {0.02, 0.1, 0.5}. In (a), (b), and (c), Weuse a constant LReval of 0.02, 0.1, and 0.5 respectively, with each plot showing all three values of LRfind. Theyshow that LReval = 0.1 is the best. Thus, in (d) we compare the different values of LReVab all with LRfind = 0.1,to demonstrate that the best LReVaI is 0.5, until very high sparsity levels where 0.1 becomes better. In line withother results, the best LR for standard training (0.5) is not the best LRfind for pruning.
Figure A9: ResNet-18 on Tiny ImageNet with LRfind ∈ {0.05, 0.2, 0.5}, compared using a constant LReval of0.5 in (a) and 0.8 in (b). For both values of LReval, LRfind of 0.05 and 0.2 are very similar and both performsignificantly better than LRfind = 0.8, despite 0.8 performing the best on the unpruned model. LRfind of 0.8 alsodoes worse than the random baseline (red).
