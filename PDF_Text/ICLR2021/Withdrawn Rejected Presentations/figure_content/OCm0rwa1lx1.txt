Figure 1: The Feedback Transformer mergespast hidden representations from all layers intoa single vector and stores it in memory.
Figure 2: Difference between Feedback andTransformer. t indicates the timestep and lindicates the layer.
Figure 3: Results on the Corridor task. TheTransformer degrades as the memory size de-creases, but the Feedback Transformer main-tains performance.
Figure 4: (left) Machine Translation on WMT14 En-De, test set BLEU and decoding speed inwords-per-second for varying decoder depths. (right) Maze Navigation in Gridworld. We displayaverage reward comparing Feedback Transformer to standard Transformers.
Figure 5: Comparison of different memory com-position strategies on char-PTB. The recurrentconnection alone is not as effective as feedbackconnections from a higher layer.
Figure 6: Averaged cumulative reward during training on (left) Maze Navigation Easy and (right)Water Maze tasks.
Figure 7: Results on (left) the IWSLT De-En dataset, and (right) Summarization onCNN-Dailymail, test set ROUGE-L for varying decoder depths.
Figure 8: Ablation results on char-PTB: instead of a weighted sum of all layers as Feedbackmemory, only a single layer is used as memory for all layers. We also include a setting where theaverage of all layers is used.
Figure 9: The performance on (left) char-PTB and (right) Wikitext-103 as a function of themodel depth. The number of parameters is kept constant by increasing the width.
Figure 10: (left) Maze Navigation task and (right) Water Maze task.
