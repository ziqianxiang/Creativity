Figure 1: Semantic Inference Network (SIN) for Few-shot Streaming Label Learning. In thepreliminary stage, we learn a semantic-aware feature extractor to build a semantic embedding space.
Figure 2: Features are projected to semantic space.
Figure 3: SIN solves the FSLL problem.
Figure 4: Performance comparison on Mir-FlickrStudent	5-way 1-shot	5-way 5-shotSIN\l2	42.35±1.91%	44.47±1.26%SIN∖φ	45.25±0.83%	48.47±0.35%SIN\I	39.12±1.17%	45.93±0.62%SIN\If	45.01±0.47%	47.93±0.34%SIN\Ic	45.23±0.28%	48.51±0.19%SIN\Ia	44.83±0.31%	47.64±0.21%SIN	46.17±0∙35%	49.19±0.27%Table 2: Ablation study of componentsthat we improve and enable few-shot learning methods to handle the scenario of multiple new labels.
Figure 5: A photo is being tagged with new labelsP PaSt labelQneW labelOShirt0 woman「,Alice	0 grass/,catD dogQ LabradorFigure 6: Labels in semantic spaceA.2 Attention-level Semantic InferenceWe design an attention-level semantic inference to transfer the probabilistic predictions on past labelsto new labels. A basic idea of this mechanism is using the output score of past labels to computea weighted combination of new labels embeddings in the semantic space. Given example x andpast-label classifier, We can get a label prediction vector y= [y1 ,...,ym], where yj is the probabilisticprediction for the j-th label. Consequently, yj Wpast is the probabilistic weighted word vector for thej-th past label. More formally, by defining the convex combination of the past-label word embeddings:me(x) = Σ2 yj Wpast= Wpast y = Wpast Sigmoid(F (X)Wpast),j=1
Figure 6: Labels in semantic spaceA.2 Attention-level Semantic InferenceWe design an attention-level semantic inference to transfer the probabilistic predictions on past labelsto new labels. A basic idea of this mechanism is using the output score of past labels to computea weighted combination of new labels embeddings in the semantic space. Given example x andpast-label classifier, We can get a label prediction vector y= [y1 ,...,ym], where yj is the probabilisticprediction for the j-th label. Consequently, yj Wpast is the probabilistic weighted word vector for thej-th past label. More formally, by defining the convex combination of the past-label word embeddings:me(x) = Σ2 yj Wpast= Wpast y = Wpast Sigmoid(F (X)Wpast),j=1In that case, we can generate a probabilistic prediction of x on the corresponding new labels throughthis simple inference, i.e., cos(e(x),Wknew)=cos(e(x), W1new,...,Wknew ). However, the simpleinference mentioned above has not taken the full utilization of label-specific knowledge. In order toimprove the learning ability of the model, we further design the attention-level semantic inferencemechanism based on the above basic idea. We consider an attentional head a(z,Wpast) with z tocompute the query and base-label vectors used for keys and values, which takes the form: *sigmoid ( q⑶ W past)a(z，Wpast) = η	/ (N)W~ʒɪWpast ,11 Sigmoid (q(Z)Wp	J∣∣1
Figure 7: Performance comparison on Mir-Flickr. k-way Ns-shot denotes k new labels with Nstagged examples per label for training.
