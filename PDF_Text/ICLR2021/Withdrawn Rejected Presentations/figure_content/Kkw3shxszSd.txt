Figure 1: Diagram of data augmentations. We study randomly replacing residues (with probabilityp) with (a) a chemically-motivated dictionary replacement or (b) the single amino acid alanine. Wealso consider randomly shuffling either (c) the entire sequence or (d) a local region only. Finally, welook at (e) reversing the whole sequence and (f) subsampling to a subset of the original.
Figure 2: Diagram of experimental approach (see Sect. 3.1). We use dashed boxes to indicatedifferent steps: semi-supervised pre-training, augmented learning, linear evaluation, and finally fine-tuning the best performing augmented models on downstream tasks. In each box, we include thegeneral model architectures, with major sub-modules in different colors. The model freezer indicatesthe semi-supervised model is not updated during linear evaluation;.
Figure 3: Contrastive learning performance with pairwise & single augmentations in linear evalua-tion for 4 different tasks. The axes refer to different augmentations, with diagonal being a single aug-mentation. The values in the heatmaps are correlation (stability and fluorescence) and classificationaccuracy (remote homology and secondary structure). We do not consider two pairs: RD(p = 0.01)& RD(p = 0.5) and GRS & LRS, due to redundancy. The per-task performance of the masked-tokenTAPE Baseline model is colored white in each subfigure; red is better performance, blue is worse.
Figure 4: Top row: Effects of Binomial Replacement p for linear evaluation on constrastive learn-ing models for TAPE tasks. Bottom row: Effects of augmentation ratio γ for linear evaluationon TAPE’s self-supervised model with the best performing task-specific augmentations (see “BestAug.” row in Table 2) , using masked token prediction. The subfigures include linear evaluationresults with different augmentation ratios, Y. "γ=0.0” refers to fine-tuning with no data augmenta-tions, whereas “Baseline” refers to the TAPE pre-trained model with no further training. (Also seeAppendix C.)procedures that use subsampling tends to yield better performance than alternatives, with the bestperforming approach using subsampling alone. For complete heatmaps for the 6 remote homologyand secondary structure testing sets, please refer to Fig. 5 in Appendix B.
Figure 5: Contrastive Learning Performance with Pairwise & Single Augmentation in Linear Eval-uation. The heatmaps include the performance of contrastive learning models with pairwise/singleaugmentations for 4 different downstream tasks considering all possible testing sets. Both x andy axes refer to different augmentations. The diagonal of heatmaps refer to the single augmentationcases. All other cells refer to cases with pairwise augmentations. The values in the heatmaps refer toevaluation results: "p" for Stability and Fluorescence, and “Classification Accuracy" for SecondaryStructure and Remote Homology.
