Figure 1: Comparison between gradi-ent descent with tensor initialization andrandom initializationFigure 2: The sample complexity againstthe feature dimension dWe then fix d = 5 and study the impact on the sample complexity when the mean and variancein the GaUSSian mixture model change. In Fig. 3.(a), We fix σ1 = σ2 = 1 and let μ1 = μ ∙ 1,μ2 = -1. μ varies from 0 to 7.5. Fig. 3.(a) shows that when the mean increases, the samplecomplexity increaSeS. ThiS coincideS With our theoretical analySeS in Section 4. In Fig. 3.(b), Wefix μ1 = 1, μ2 = —1, and let σ1 = σ and σ2 = 1. σ varies from 10-1.4 to 101.4. The samplecomplexity increases both when σ increases and when σ approaches zero. The results match ourtheoretical prediction in Section 4.
Figure 2: The sample complexity againstthe feature dimension dWe then fix d = 5 and study the impact on the sample complexity when the mean and variancein the GaUSSian mixture model change. In Fig. 3.(a), We fix σ1 = σ2 = 1 and let μ1 = μ ∙ 1,μ2 = -1. μ varies from 0 to 7.5. Fig. 3.(a) shows that when the mean increases, the samplecomplexity increaSeS. ThiS coincideS With our theoretical analySeS in Section 4. In Fig. 3.(b), Wefix μ1 = 1, μ2 = —1, and let σ1 = σ and σ2 = 1. σ varies from 10-1.4 to 101.4. The samplecomplexity increases both when σ increases and when σ approaches zero. The results match ourtheoretical prediction in Section 4.
Figure 3: The sample complexity (a) when one mean changes, (b) when one variance changes.
Figure 4: (a) The convergence rate with different μ, (b) The convergence rate with different σ.
Figure 6: The relative error of the learned modelwith the ground-truth when n changesFigure 5: Convergence rate when thenumber of neurons K changesWe then evaluate the distance between Wcn，	11 A 1 LI Λ	1 TTΓ⅛	1 1 I Iττ7returned by Algorithm 1 and W *, measured by ∣∣Wn —W * ||F. d is 5. n ranges from 2 × 103 to 6 × 104. σ1 = σ2 = 3, μ1 = 1, μ2 = —1. Each point inFig. 6 is averaged over 100 independent experiments of different W * and the corresponding trainingset. k W * kF is normalized to 1. The error is indeed linear in ʌ/log(n)/n, as predicted by (12).
Figure 5: Convergence rate when thenumber of neurons K changesWe then evaluate the distance between Wcn，	11 A 1 LI Λ	1 TTΓ⅛	1 1 I Iττ7returned by Algorithm 1 and W *, measured by ∣∣Wn —W * ||F. d is 5. n ranges from 2 × 103 to 6 × 104. σ1 = σ2 = 3, μ1 = 1, μ2 = —1. Each point inFig. 6 is averaged over 100 independent experiments of different W * and the corresponding trainingset. k W * kF is normalized to 1. The error is indeed linear in ʌ/log(n)/n, as predicted by (12).
