Figure 1: Regression to (20) with varying mo-mentums (10 seeds per setting). Note that thedifference between μ and μ is not significant,but μ and μ* is.
Figure 2: Classification on SVHN with varyingmomentums (5 seeds per setting). Dotted linesare test losses. The only significant difference isbetween the training loss of μ* and μ.
Figure 4: TD(0) online policy evaluation onMountain Car, transitions are seen in-order. TheMSE is measured against a pretrained V π (50seeds per setting). At step 2θk, μ* and μ are notsignificantly different.
Figure 3: TD(0) policy evaluation on Moun-tain Car with varying momentums on a replaybuffer. The MSE is measured against a pre-trained V π (10 seeds per setting). At step 5k,all methods are significantly different.
Figure 6: MSE as a function of the width, σ2,of RBF kernels. The larger the kernel, the morevalue drift our method, μ, is able to correct (10seeds per setting).
Figure 5: Value drift of V (s0) when trainingwith TD(0) on a replay buffer. We see that RBFsbeing a sparse feature representation, the valuefunctions of recently seen data tend not to drift(10 seeds per setting). Here σ2 = 1.
Figure 7: MSE as a function of mean Valuedrift of V (s0) for RBFs of varying kernel size.
Figure 8: Average cosine similarity of the Tay-lor approximations gt with their true value gt forrecently seen data; Mountain Car, replay bufferpolicy evaluation (40 seeds per setting).
Figure 9: TD(0) policy evaluation on Atari(MsPacman) With varying momentums (20 seeds)on a replay buffer. The MSE is measured againstsampled returns Gπ .
Figure 11: TD(0) policy evaluation on Moun-tain Car with varying β on a replay buffer. TheMSE is measured after 5k SGD steps against apretrained V π . Shaded areas are bootstrapped95% confidence runs (10 seeds per setting). Weuse a minibatch size of 4 to reveal interestingtrends.
Figure 10: TD(0) policy evaluation on Moun-tain Car with varying minibatch size on a replaybuffer. The MSE is measured after 5k SGDsteps against a pretrained V π . Shaded areas arebootstrapped 95% confidence runs (20 seeds persetting).
Figure 13: TD(0) policy evaluation on Moun-tain Car with an MLP. We vary the number oflayers whose parameters are used for full μ Cor-rection (n2 params); e.g. when “bottom used” is3, the first 3 layers, those closest to the input, areused; when “top used” is 1, only the last layer,that predicts V from embeddings, is used. Theparameters of other layers are either correctedwith the diagonal correction or use normal mo-mentum. Correcting “both ends” is not betterthan just the bottom (not shown here).
Figure 12: TD(0) policy evaluation on Moun-tain Car with RBF on a variety of hyperparame-ter settings (10 seeds per setting). Here σ2 = 1.
Figure 15: TD(0) policy evaluation on Cartpolewith varying hyperparameters replay buffer.
Figure 14: TD(0) policy evaluation on Ac-robot with varying hyperparameters on a replaybuffer. The MSE is measured after 5k SGDsteps against a pretrained V π . Shaded areas arebootstrapped 95% confidence runs (10 seeds persetting).
Figure 16: Replication of Figure 3 including thefrozen targets baseline.
Figure 17: Replication of Figure 9 including thefrozen targets baseline. Interestingly the modelstrained with frozen targets eventually becomemore precise than those without, but this onlyhappens after a very long time. This is explainedby the stability required for bootstrapping whenTD errors become increasingly small, which iseasily addressed by keeping the target networkfixed.
Figure 18:	Replication of Figure 3 with 10×more training steps. Methods gradually con-verge to the value function.
Figure 19:	Effect of the learning rate on Moun-tain Car, replay buffer policy evaluation, MSEafter 5k training steps.
