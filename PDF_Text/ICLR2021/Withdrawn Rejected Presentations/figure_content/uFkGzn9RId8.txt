Figure 1: A diagram of a Memory-Augmented Environment.
Figure 2: Experiments in the gravity domain. We reported the avg. reward per 100 steps.
Figure 3:	Performance of a greedy policy every 10, 000 training steps in the recall task.
Figure 4:	Experiments in the gravity domain. We reported the avg. reward per 100 steps.
Figure 5:	Tabular experiments in a variation of the recall task (details in Appendix B).
Figure 6:	Results over partially observable benchmarks using PPO and different memories.
Figure 7: Variation of the recall task where Bk always converges to a suboptimal policy.
Figure 9: The minigrid environments.
Figure 10:	Atari environments.
Figure 11:	Tabular experiments in the gravity domain. We reported the avg. reward per 100 steps.
Figure 12:	Results over partially observable benchmarks using DDQN and different memories.
