Figure 1: Random crops of images are often used when training classification CNNs to help mitigatesize, position and scale bias (as shown in the left half of the figure along with the objectness valueslisted below them). Unfortunately, some of these crops miss the object as the process does not useany object location information. Traditional hard label and smooth label approaches do not accountfor the proportion of the object being classified and use a fixed label of 1 or 0.9 in the case of labelsmoothing. Our approach (right half) smooths the hard labels by accounting for the objectnessmeasure to compute an adaptive smoothing factor. The objectness is computed using boundingbox information as shown above. Our approach helps generate accurate labels during training andpenalizes low-entropy (high-confidence) predictions for context-only images.
Figure 2: Reliability diagrams help understand the calibration performance (DeGroot & Fienberg,1983; Niculescu-Mizil & Caruana, 2005) of classifiers. We compute ECE1 using the implementationof (Wenger et al., 2020) on the validation set of ImageNet. The deviation from the dashed line(shown in gray), weighted by the histogram of confidence values, is equal to Expected CalibrationError (Wenger et al., 2020). The top half of the figure shows classifiers trained using the same dataset(N =0.528M), but with different values of β. The leftmost reliability diagram is the classic hard labelsetting and the rightmost reliability diagram is the adaptive label setting. The bottom half of thefigure compares classifiers trained on the complete ImageNet (leftmost) with 3 classifiers trained onthe subset of ImageNet with bounding box labels using different values of the α hyperparameter.
Figure 3: Hard-label and label-smoothing based approaches (top half of the figure) do not take intoaccount the proportion of the object being classified. Our approach (bottom half) weights soft labelsusing the objectness measure to compute an adaptive smoothing factor during training, this helpsproduce peaks corresponding to object size during inference.
Figure 4: The first row of images in the left half of the figure are an example of the ImageNetdataset (N=0.474M) that have bounding box annotations. We match the images from the trainingset of ImageNet-1K dataset with the corresponding ‘.xml’ files included in the ImageNet objectdetection dataset. We then create object masks for each of the images. When applying any scalingand cropping operation to training samples, we apply the same transformation to the correspondingobject masks as well. By counting the number of white pixels, we can determine the object proportionpost transformation. We describe the two other approaches in the figure, the ‘mask’ version of ourapproach has a single object (for images with multiple bounding box annotations) and this versionhas 0.528M samples. Our approach helps generate accurate labels during training and penalizeslow-entropy (high-confidence) predictions for context-only images like the example on the right halfof the figure.
Figure 5:	Top half of the figure shows the count per class for the ImageNet dataset, the highestnumber of images in a given class is ‘1349’ and the lowest count is ‘190’. The distribution in thiscase is not as skewed as the OpenImages (bottom half) dataset. About 60 classes in our subset of theOpenImages dataset account for half the dataset. The maximum and minimum counts are, 55K and28 respectively.
Figure 6:	Examples of class activation maps (CAMs). These were obtained using the implementationof (Chattopadhay et al., 2018). The values under each CAM represent the top three probabilities, withgreen indicating the pertinent class and red indicating an incorrect prediction. Two columns on theleft show results for baseline CNNs using hard labels and standard label smoothing. Our approach,adaptive label smoothing (‘Adaptive l.s’), is illustrated in the three rightmost columns. Our techniqueproduces high-entropy predictions on images without any objects and shows an improved localizationperformance.
Figure 7:	Examples of class activation maps (CAMs). These were obtained using the implementationof (Chattopadhay et al., 2018). The second and third columns from the left show results for baselineCNNs using hard labels and standard label smoothing. Our approach, adaptive label smoothing(‘Adaptive l.s’), is illustrated in the three rightmost columns. Our technique produces high-entropypredictions and shows an improved localization performance. The values under each CAM representthe top three probabilities, with green indicating the pertinent class and red indicating an incorrectprediction.
Figure 8:	More examples of class activation maps (CAMs). These were obtained using theimplementation of (Chattopadhay et al., 2018). The second and third columns from the left showresults for baseline CNNs using hard labels and standard label smoothing. Our approach, adaptivelabel smoothing (‘Adaptive l.s’), is illustrated in the three rightmost columns. Our technique produceshigh-entropy predictions and shows an improved localization performance. The values under eachCAM represent the top three probabilities, with green indicating the pertinent class and red indicatingan incorrect prediction.
