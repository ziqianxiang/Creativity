Figure 1: Neural networks can be interpreted as layered chain graphs where activation functions aredetermined by node distributions. Left: An example neural network interpreted as a chain graphwith three chain components which represent its layers; Right: A variety of activation functions(softplus, ReLU, leaky ReLU) approximated by nodes following rectified Gaussian distributions(e, q as in Eq. (7)). We visualize the approximations stochastically by averaging over 200 samples.
Figure 2: Example of a refinement module (left) and its corresponding computational graph (right),composed of a base submodule Xl-1 → Xl (blue background) and a refining submodule Xl →Zl → X l (red background). In the computational graph each W, b represents a linear connection(Eq. (3)) and σ an activation function. Same color identifies corresponding parts in the two graphs.
Figure 3: Comparison of an simple recurrent layer (left) v.s. an IndRNN (right) recurrent layer.
Figure 4: Comparison of stochastic methods (None/Dropout/PCFF) in terms of image classificationtest errors (lower is better) under various settings. Left: MNIST/FashionMNIST datasets with asimple dense network and tanh/ReLU activation functions; Right: CIFAR-10 dataset with ResNet20and varying drop/sample rates. All reported results are average values of three runs. Compared todropout, PCFF can achieve comparable results, and tend to deliver more consistent improvements.
