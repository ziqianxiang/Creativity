Figure 1: The default PPO implementation with different discount factors.
Figure 2: Comparison between PPO and PPO-TD when γC = 1.
Figure 3: PPO-TD with different discount factors.
Figure 5: PPO-TD-Ex (γC = 1).
Figure 4: PPO-TD-Ex (γC = 0.99).
Figure 6: PPO-FHTD with the first parameterization. The best H and γC are used for each game.
Figure 7: PPO-FHTD with the second parameterization.
Figure 8: A simple MRP.	randomly generated in [0,1]. Let Xs ∈ RK be the feature vectorfor a state s; We set its i-th component as xs[i] = tanh(ξ), whereξ is a random variable uniformly distributed in [-2, -2]. We chosethis feature setup as we use tanh as the activation function in our PPO. We use X ∈ RN ×K todenote the feature matrix. To create state aliasing (McCallum, 1997), which is common under func-tion approximation, we first randomly split the N states into Si and S2 such that ∣Sι | = αΝ and|S2| = (1 - α)N, where α is the proportion of states to be aliased. Then for every s ∈ S1, werandomly select an s ∈ S2 and set Xs J χ^. Finally, we add Gaussian noise N(0,0.12) to eachelement ofX. We use N = 100 and K = 30 in our simulation and report the normalized represen-tation error (NRE) as a function of γ . For a feature matrix X , the NRE is computed analytically asNRE(Y) = minw ∣l∣lXw-vγ ||2, where Vγ is the analytically computed true value function of the MRP.
Figure 9: Normalized representation error as a function of the discount factor. Shaded regionsindicate one standard derivation. 4the widely recognized bias-variance trade-off. In the appendix, we provide additional experimentsinvolving distributional RL to further support the bias-variance-representation trade-off hypothe-sis, under the assumption that the benefits of distributional RL comes mainly from the improvedrepresentation learning (Bellemare et al., 2017; Munos, 2018; Petroski Such et al., 2019).
Figure 10: Comparison between PPO and DisPPO with γ = 0.995When our goal is to optimize the discounted objective Jγ<1 (π), theoretically we should have theγAt term in the actor update and use γC < 1. Practitioners, however, usually ignore this γAt (i.e.,set γA = 1), introducing bias. Figure 10 shows that even if we use the discounted return as theperformance metric, the biased implementation of PPO still outperforms the theoretically groundedimplementation DisPPO in the domains we tested. Here PPO refers to the default PPO implemen-tation where γA = 1, γC = γ < 1, and DisPPO (Alg. 6 in the appendix) adds the missing γAt termin PPO by using γA = γC = γ < 1. We propose to interpret the empirical advantages of PPO overDisPPO with Hypothesis 2. For all experiments in this section, we use γC = γ < 1.
Figure 11: Curves without any marker are obtained in the original Ant / HalfCheetah.
Figure 12: PPO-TD (γC = 0.99) with different learning rates. A curve labeled with (x, β) cor-responds to an initial learning rate for the actor and critic of αA = x × 3 ∙ 10-4 and αC = βαArespectively. The best learning rates for Ant 历=3 ∙ 10-4 and β = 3) yields reasonably goodperformance in all the other games except Humanoid.
Figure 13: Architectures of the algorithmsC	Additional Experimental ResultsC.1 Distributional RLHypothesis 1 and the previous empirical study suggest that representation learning may be the mainbottleneck of PPO-TD (γC = 1). To further support this, we benchmark PPO-C51 (γC = 1) (Al-gorithm 5 in the appendix), where the critic of PPO is trained with C51. C51 is usually consideredto improve representation learning by implicitly providing auxiliary tasks (Bellemare et al., 2017;Munos, 2018; Petroski Such et al., 2019). Figure 14 shows that training the critic with C51 in-deed leads to a performance improvement and PPO-C51 (γC = 1) sometimes outperforms PPO-TD(γC < 1) by a large margin. Figure 15 further shows that when Vmax is optimized for PPO-C51, thebenefit for using γC < 1 in PPO-C51 is less pronounced than that in PPO-TD, indicating the role ofγC < 1 and distributional learning may overlap. Figures 6, 7, & 9, suggest that the overlapping isrepresentation learning.
Figure 14: For PPO-C51, we set γC = 1.
Figure 15: For each game, Vmax is the same as the Vmax in Figure 14.
Figure 16: PPO-TD-Ex (γC = 0.995).
Figure 17: Unnormalized representation error (RE) as a function of the discount factor.
Figure 18: Curves without any marker are obtained in the original Ant. Diamond-marked curvesare obtained in Ant with r0 .
Figure 19: The default PPO implementation with different discount factors. The larger version ofFigure 1.
Figure 20: Comparison between PPO and PPO-TD when γC = 1. The larger version of Figure 2.
Figure 21: PPO-TD with different discount factors. The larger version of Figure 3.
Figure 22: PPO-TD-Ex (γC = 0.99). The larger version of Figure 4.
Figure 23: PPO-TD-Ex (γC = 1). The larger version of Figure 5.
Figure 24: PPO-FHTD with the first parameterization. The best H and γC are used for each game.
Figure 25: PPO-FHTD with the second parameterization. The larger version of Figure 7.
Figure 26:	Comparison between PPO and DisPPO with γ = 0.995. The larger version of Figure 10.
Figure 27:	Curves without any marker are obtained in the original Ant environment. Diamond-marked curves are obtained in Ant with r0. The larger version of Figure 11.
