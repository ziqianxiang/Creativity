Figure 1:	Quantization error (MSE) and test perplexity when one selected layer of an LSTM modelusing PTB dataset is quantized to be 1-bit. (Left): Embedding layer. (Right): LSTM layer.
Figure 2:	Test accuracy and quantization error (MSE) of fine-tuned BERT-base model on MRPCwhen weights are quantized into 3 bits by our proposed method while one of E, C, or P varies.
Figure 3: E, C, and P values searched by layer-wise BO for BERT-base on MRPC, MNLI, andSQUAD. X-axis shows layer index and y-axis shows hyper-parameters optimized differently for eachlayer. BO is necessary to efficiently and quickly find such diversified E, C, and P values.
Figure 4: Relationship of training accuracy achieved by weighted quantization and test accuracyusing BERT on MRPC (LEFT) and BERT on MNLI (RIGHT).
Figure 5: E, C, and P values searched by layer-wise BO for DistilBERT-base on MRPC, MNLI, andSQUAD. X-axis shows layer index and y-axis shows hyper-parameters optimized differently for eachlayer.
Figure 6: E, C, and P values searched by layer-wise BO for Longformer on SQUAD v1.1. X-axisshows layer index and y-axis shows hyper-parameters optimized differently for each layer.
Figure 7: Test score degradation by (post-training) pruning weights (based on the magnitude) usingvarious pre-trained language models. Weights of a layer are pruned by the same target pruning rate.
Figure 8: E, C, and P values searched by layer-wise BO for fine-tuned AWD-LSTM model. X-axisshows layer index and y-axis shows hyper-parameters optimized differently for each layer.
Figure 9: E, C, and P values searched by layer-wise BO for Transformer. BO is performed tooptimize PPL (Left) or BLEU (Right). X-axis shows layer index and y-axis shows hyper-parametersoptimized differently for each layer.
Figure 10: E, C , and P values searched by layer-wise BO for ResNet18 on ImageNet. X-axis showslayer index and y-axis shows hyper-parameters optimized differently for each layer.
