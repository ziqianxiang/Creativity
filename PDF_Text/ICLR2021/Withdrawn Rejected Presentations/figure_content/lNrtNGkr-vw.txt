Figure 1: Strong Experimental Results: We showcase the performance of meta-RL methods ontasks that are very different from the training tasks to assess the generalization ability of meth-ods. We also analyze the adaptation run-time speed of these methods on tasks that are similar (in-distribution) and tasks that are not very similar (out-of-distribution) to further evaluate these models.
Figure 2: Overview of our approach: In training, for different tasks {Ti}, we parametrize theirpolicy as πi = φ ∙ Wi, where φ ∈ Rd is the shared linear representation We hope to acquire. Intesting (adaptation), we fix the acquired linear representation φ and directly alter the weights wtestby using the output of the feed-forward adapter network.
Figure 3: Each task shares a feature mapping in the critic and a different shared feature mappingin the policy network. In this illustration, for the training task 1 input data x1, only the output y1from its corresponding output layer will be used in the meta-loss function. Similarly for task 2 inputdata x2, only the output y2 from its corresponding output layer will be used in the meta-loss. Thisis extended up to n points where n is the number of training tasks used.
Figure 4: Illustration of training the adapter network. The weights of the corresponding output layerfor the training task are flattened and concatenated to form a 1-D target for the output of the adaptermodel.
Figure 5: The first step taken is arbitrary. Each step following the first is fed into the feed-back loopwhere the SARS pair is inputted to the adapter network providing anew set of weights for the outputlayer of the policy.
Figure 6: Validation Test-Task performance over the course of the meta-training process on standardin-distribution meta-RL benchmarks. Our algorithm is comparable in all environments to prior meta-RL methods.
Figure 7: Validation Test-Task performance over the course of the meta-training process on out-of-distribution meta-RL benchmarks. Our algorithm is comparable if not better to prior methods in allenvironments except for the Ant-Negated task.
Figure 8: Our method clearly outperforms all other meta-RL algorithms in runtime during adapta-tion to new tasks.
Figure 9: Ablation study for FLAP with vs without adapter network. Although both algorithmsconverge to similar average return, the algorithm with adapter net converges much faster than theone without adapter.
Figure 10: Results of varying the input to the adapter network. In environments such as Half-Cheetah, where tasks differ in the reward function, the reward is a valuable input to the adapternetwork and helps in its effectiveness for predicting weights.
Figure 11: We add an extra layer, witha non-linear activation function, to theoriginal linear output layer to create thenon-linear output.
Figure 12: The linear FLAP algorithm outperformed the modified (non-linear) FLAP algorithm.
Figure 13: Results of Single Point tuple input vs Sequence of SARS tuples as inputs on the Half-Cheetah environment. The results showcase both methods are comparable in return.
Figure 14: Results of the sparse reward environment. FLAP, with the use of a sequence of inputsto the adapter network, is comparable with PEARL even with the FLAP algorithm using 20x lesssamples from the test task.
