Figure 1: While in princi-ple we consider the pres-ence of the objects (coffeecup, table, chair) to be inde-pendent mechanisms, theytend to appear together inobserved data.
Figure 2: Disentanglement metrics show no clear trend along different correlation strengths (eachviolin represents 180 models, higher σ indicates less correlation).
Figure 3: We show latent traversals (left) of the best DCI score model among all 180 trained modelswith strongest correlation (σ = 0.2) on Shapes3D (A). The traversals in latent code dimensions 4 and8 (highlighted in black), suggest that these dimensions encode a mixture of azimuth and object size,reflecting one major axis along the correlation line of the joint distribution and one smaller, locallyorthogonal axis. This is supported by a heat map of the GBT feature importance matrix of this model(right) indicating an entanglement of azimuth and object size encoded into both latent codes.
Figure 4: Left: Pairwise entanglement scores help to uncover still existent correlations in the latentrepresentation. Left: Mean of the pairwise entanglement scores for the correlated pair (red) and themedian of the uncorrelated pairs. We see that stronger correlation leads to statistically more entangledlatents compared to the baseline score without correlation (blue). Right: The same behavior can beseen for the unfairness score between the correlated pair of factors.
Figure 5: Generalization to out-of-distribution (OOD) test data. Left: Reconstructions of observationsthe model has never seen during training. Right: Latent space distribution of the two entangleddimensions. Circles without edges indicates encoded data from the (correlated) training distribution,while circles with edges indicate encoded OOD data where the correlation pattern is broken.
Figure 6: Fast adaption with few labels: Pairwise entanglement scores for correlated FoV pair inShapes3D (A). The correlated pair is highlighted (red). Zero labels reflects the unsupervised baselinewithout any fast adaptation. Growing number of labels show that fast adaption using linear regressionreduces these correlations with as little as 100 labels (blue). Reported pairwise scores are averagedover 180 models per correlation strength.
Figure 7: Weak supervision: Left: With weak supervision trained models on Shapes3D correlationobject size and azimuth learn consistently improved, often perfect, disentangled representationsacross all correlation strengths. Middle: Unfairness scores between correlated FoVs are muchsmaller (see scale). Right: Latent dimensions of a best DCI model with strong correlation (σ = 0.2).
Figure 8: Probability distributions for sampling training data in the correlated pair of FoVs in therespective datasets (A, B, C, D, E) considering correlation strengths of σ = 0.2, σ = 0.4, σ = 0.7and σ = ∞, the uncorrelated limit (from left to right).
Figure 10: We show latent traversals (left) of the best DCI score model among all 180 trained modelswith weak correlation (σ = 0.7) in object size and azimuth. The traversals in latent code no 3 and 7/8(highlighted in black), suggest that these dimensions encode no mixture of azimuth and object sizecompared to the models with stronger correlation. This is supported by the GBT feature importancematrix of this model (right).
Figure 11: Pairwise entanglement scores help to uncover still existent correlations in the latentrepresentation. Left: Mean of the pairwise entanglement scores for the correlated pair (red) andthe median of the uncorrelated pairs. We see that stronger correlation leads to statistically moreentanglement latents across all datasets studied compared to their baseline pairwise entanglementwhere the data exhibits no correlations (blue). Each pairwise score is the mean across 180 models foreach dataset and correlation strength. Scores in the left table are based on GBT feature importanceand scores presented in the right table are based on Mutual Information.
Figure 12: Disentangled representations trained on correlated data are anti-correlated with higherfairness properties. The plots show the mean unfairness scores between the correlated factors withdecreasing correlation strength for Shapes3D (A), dSprites (B) and MPI3D-real (C).
Figure 13: Generalization capabilities towards out-of-distribution test data. Latent traversals froman observations the model has never seen during training. The starting point corresponds to a factorconfiguration in point 1 from Fig. 5. Shown are the results of the model with highest DCI scoreamong all 180 trained models on Shapes3d (A) with a very restricted correlation strength σ = 0.2 inobject size and azimuthB.2 Section 4Post-hoc alignment correction with few labels In Fig. 15, we see the axis alignment of thecorrelated latent space after fast adaptation using linear regression on a model trained on Shapes3D(A). Fast adaptation with linear regression substitution fails in some settings: when no two latentdimensions encode the applied correlation isolated from the other latent codes, or when the correlatedvariables do not have a unique natural ordering (e.g. color or categorical variables). Additionally, thefunctional form of the latent manifolds beyond the training distribution is unknown and in generalexpected to be nonlinear. We test the possibility of fast adaptation in this case using as substitutionfunction a one-hidden layer MLP classifier of size 100 on the correlated Shapes3D variants. Underthis method, we sample the FoV from a uniform independent distribution. A small number of suchsamples could practically be labeled manually. Using only 1000 labeled data points for our fastadaptation method shows a significant reduction in disentanglement-thresholds for the correlated pair(Fig. 16).
Figure 14: Latent space distribution of the two entangled dimensions of the best DCI model inShapes3d (E) with σ = 0.2 (first column), in Shapes3d (E) with σ = 0.4 (second column), inShapes3d (D) with σ = 0.2 (third column) and in Shapes3d (D) with σ = 0.4 (fourth column). Latentcodes sampled from correlated observations (circle without edge) and (2) latent codes sampled withan object size-azimuth configuration not encountered during training(squares with black edge). Eachcolumn shows the ground truth values of the two correlated factors by color.
Figure 15: Latent space distribution of the two entangled dimensions of the best DCI model inShapes3D (A). Latent codes sampled from correlated observations (circle without edge) and (2) latentcodes sampled with an object size-azimuth configuration not encountered during training (squareswith black edge). Left column shows the ground truth values of the two correlated factors by color.
Figure 16:	Mean of the pairwise entanglement scores for the correlated pair (red) and the medianof the uncorrelated pairs (based on GBT feature importance) for all pairs of variables in Shapes3D(D) (top), Shapes3D (E) (middle) and Shapes3D (A) (bottom) all with correlation strength σ = 0.2.
Figure 17:	Standard disentanglement metrics evaluated for the weakly supervised scenario usingcorrelated observational data.
Figure 18: Left: For the weakly supervised scenario using correlated observational data trained modelson Shapes3D (A), (D) and (E) correlating object color and azimuth learn consistently improved, oftenperfect, disentangled representation across all correlation strengths. Right: Latent dimensions ofa best DCI model trained on strongly correlated observational data. Representations are perfectlyaxis-aligned with respect to both of the correlated variables ground truth values (right).
Figure 19: Standard disentanglement metrics evaluated on the correlated training sets for the weaklysupervised scenario with intervening capabilities (I-1).
Figure 20: Left: For the weakly supervised scenario with intervening capabilities (I-1) trained modelson Shapes3D (A), (D) and (E) correlating object color and azimuth learn consistently improved, oftenperfect, disentangled representation across all correlation strengths. Right: Latent dimensions of abest DCI model with strong correlation (0.2). Representations are perfectly axis-aligned with respectto both of the correlated variables ground truth values (right).
Figure 21: Disentanglement metrics (top), unfairness scores (middle) and latent spaces (bottom) showstrong disentanglement using weak supervision with intervening capabilities (I-2) - even under thestronger assumption that sampling of observation pairs follow its causal generative model. We showthe learnt latent space encoding of the two correlated factors of variation for a model with σ = 0.1and low reconstruction loss.
