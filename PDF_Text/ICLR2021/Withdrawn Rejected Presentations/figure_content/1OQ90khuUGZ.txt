Figure 1: The screenshot shows the typical learned behavior of agents in the task of DefeatRan-domEnemy. (a) shows that an agent trained with some shaped reward function RA1 learns manyhelpful behaviors such as building workers (grey circles), combat units (blue circles), and barracks(grey square) or using owned units (with red boarder) to attack enemy units (with blue border), butdoes not learn to win as fast as possible (i.e. it still does not win at internal time step t = 6000).
Figure 2: The faint lines are the actual sparse return of each seed for selected strategies in Produce-CombatUnits; solid lines are their means. The left figure showcase the sample-efficiency of actionguidance; the right figure is a motivating example for PLO.
Figure 3: The screenshot shows the typical learned behavior of agents in the task of ProduceCom-batUnits. (a) shows an agent trained with shaped reward function RA1 learn to only produce combatunits once the resources are exhausted (i.e. it produces three combat units at t = 1410). In contrary,(b) shows an agent trained with action guidance learn to produce units and harvest resources con-currently (i.e. it produces three combat units at t = 890). Click on the link below figures to see thefull videos of trained agents.
