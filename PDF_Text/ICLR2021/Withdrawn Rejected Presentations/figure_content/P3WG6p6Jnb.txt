Figure 1: Evaluation of the proposed approach and the baseline BCQ (Fujimoto et al., 2019) on a suite of threeOpenAI Gym environments. Details about the type of offline dataset used for training, namely random, medium,mixed, and expert are included in Appendix. Results are averaged over 5 random seeds (Henderson et al., 2018).
Figure 2: Ablation performed on Hopper. The mean and standard deviation are reported over 5 random seeds.
Figure 3: Evaluation of the proposed approach and the baseline BCQ on a suite of three OpenAI Gymenvironments. We consider the setting of rewards that are corrupted by a Gaussian noise. Results for theuncorrupted version are in Fig. 1. Experiment results are averaged over 5 random seedsE.3 Experimental Results on Safety Benchmark TasksSafety Benchmarks for Variance as Risk : We additionally consider safety benchmarks forcontrol tasks, to analyse the significance of variance regularizer as a risk constraint in offline policyoptimization algorithms. Our results are summarized in table 3.
