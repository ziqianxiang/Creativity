Figure 1: Memory modifications of Transformer architecture. (a) Transformer layer. For ev-ery element of a sequence (solid arrow), self-attention produces aggregate representation from allother elements (dashed arrow). Then this aggregate and the element representations are combinedand updated with a fully-connected feed-forward network layer. (b) Memory Transformer (Mem-Transformer) prepends input sequence with dedicated [mem] tokens. This extended sequence isprocessed with a standard Transformer layer without any distinction between [mem] and other ele-ments of the input. (c) Compared to MemTransformer MemCtrl Transforemer has dedicated mem-ory controller sub-network. (d) Memory Bottleneck Transformer (MemBottleneck Transformer)uses [mem] tokens but separates memory and input attention streams. At the first step, representa-tions of [mem] tokens are updated (2) with the attention span (1) covering both memory and inputsegments of the sequence. Then representations of input elements are updated (4) with memoryattention (3) only. Thus information flow is distributed to representations of elements only throughthe memory.
Figure 2: Operations with memory learned by MemTransformer 10. (a) The pattern of self-attention in the 3rd encoder layer. Here, [mem] tokens in the central segment of memory (on theleft) attend to the vector representations of tokens Technik, Entwicklung, Intelligenz(and some others). This attention values are consistent with the writing of selected token vectors tothe [mem] tokens. Activity in the left top corner that involves first four tokens might indicate fusionof neighbour vectors by pairwise summation of [mem] tokens. (b) In the next 4th layer of the sameencoder similar fusion operation with the same [mem]â€™s is repeated. A parallel diagonal activityjust below the fusion pattern can be attributed to copy operation. (c) Another attention head in thesame encoder layer demonstrates combination of fusion and store operations. Sharp self-attention ofthree tokens in the middle results in adding vectors to themselves. (d) Attention pattern in decoderlayer 4 over the output of 6th encoder layer suggest that vectors of [mem] tokens are selectivelyread and added to the output token representations during decoding.
Figure 3: How to read Memory Transformer attention map. Attention values indicate how ele-ments of input sequence (on the top) contribute to the update of representation for specific output el-ement (on the left). Attention map for memory augmented transformer can be split into four blocks:(1) update - [sequence] to [sequence]; (2) write - [sequence] to [memory]; (3) read - [memory] to[sequence]; (4) process - [memory] to [memory].
Figure 4: MemTransformer 10 encoder attention maps. As the model encodes an input se-quence the change of attention patterns related to memory can be interpreted as a read-process-storepipeline. Heads in layers 1 to 3 have many read to memory patterns. Patterns consistent with in mem-ory processing are more frequent in layers 3-6. The last layer is dominated by diagonal attentionthat can be seen as an amplification of calculated representations.
Figure 5: MemTransformer 10 decoder attention maps. Every layer of the decoder has headswith signs of memory reading activity. Reading patterns suggest that the representations in memoryare locally grouped in 3 blocks.
Figure 6: MemBottleneck 20 encoder attention maps. In the 1st layer, all attention heads ofmemory sub-layer ([memory+sequence] to [memory]) read from the input sequence. Only 2 headsof memory sub-layer in the layer 2 reads from the input, but all others are diagonal to amplify contentof the memory. No more input reading is present in layers 3 and 4. Notably, all heads of the 1stlayer memory attention have patterns that split into three blocks. The top block has sparse attentionover the whole sequence without preserving the order. The middle block reads the first half of thesequence in the reverse order, and the bottom block reads the rest in the proper order. This suggestsencoding of global information in the top block and local information in the middle and bottomblocks. Layer 3 of memory sub-layer has sharp amplifying diagonals, and something like shiftingoperations represented by broken diagonals. Layer 4 of memory sub-layer demonstrates mainlyheterogeneous patterns which indicates in memory processing. Maps of attention which belongs tothe sequence sub-layer ([memory] to [sequence]) of MemBottleneck layer degrade to vertical linesin layers 3 and 4. This is a sing that these attention heads are bypassed as follows from (Kobayashiet al., 2020).
Figure 7: MemBottleneck 20 decoder attention maps. At the decoding phase, almost all headsattend to the content of memory but not on the representations of sequence elements.
