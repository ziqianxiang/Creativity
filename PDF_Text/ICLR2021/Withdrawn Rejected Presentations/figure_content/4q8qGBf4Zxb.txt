Figure 1: An overview of NASDA: (a) Continuous relaxation of the research space by placing amixture of the candidate operations on each edge. (b) Inducing the final architecture by joint opti-mization of the neural architecture parameters α and network weights w , supervised by minimizingthe validation loss on the source domain and reducing the domain discrepancy. (c)(d) Adversarialtraining of the derive feature generator G and classifiers C.
Figure 2: (a) Neural architecture for STL→CIFAR10 task. (b) Neural architecture for MNIST→USPSresults. (c) Comparison between our NASDA model and state-of-the-art NAS models.
Figure 3: T-SNE for 4 classifiersIn this paper, we first formulate a novel dual-objective task of Neural Architecture Search for DomainAdaptation (NASDA) to invigorate the design of transfer-aware network architectures. Towardstackling the NASDA task, we have proposed a novel learning framework that leverages MK-MMDto guide the neural architecture search process. Instead of aligning the features from existinghandcrafted backbones, our model directly searches for the optimal neural architecture specific fordomain adaptation. Furthermore, we have introduced the ways to consolidate the feature generator,which is stacked from the searched architecture, in order to boost the UDA performance. Extensiveempirical evaluations on UDA benchmarks have demonstrated the efficacy of the proposed modelagainst several state-of-the-art domain adaptation algorithms.
