Figure 1: Dead Neuron Prevalence. The percentage of originally dead neurons in the selectedpaths of different methods reported for sparsity of 90% (left) and 99% (right). All paths selected bypruning objective contain originally dead (now active) neurons.
Figure 2: Path Analysis. Overlap between paths from different methods: a) overlap in entire net-work b) layer-wise overlap between paths of all methods and NeuronIntGrad. Among the pruning-based methods, only the path selected by DGR(init=1) overlaps with contribution-based methods.
Figure 3: Path Decoding. a) Maximizing the network response while restricting the network to aspecific path. b) Feature visualization of the top selected neuron in each path. Results confirm thatthe paths selected by the pruning objective (except for DGR(init=1)) do not encode image features.
Figure 4: Gradient Visualization. The gradients of the locally linear critical paths at differentsparsity levels for NeuronIntGrad (top) and NeuronMCT (bottom). More examples in Appendix D.
Figure 5: Comparison with Attribution Methods. Comparison between attribution maps derviedour proposed methods (right) vs. gradient-based attribution methods on VGG-16. Note the im-provement of integrated gradients on the neurons (NeuronIntGrad) over integrated gradients on input(InputIntGrad). More examples for VGG-16 and also ResNet-50 are provided in Appendix D.
Figure 6: Feature Importance. a,b,c) LeRF (ResNet-50) on Cifar10, Bridsnap, and ImageNet. d)Remove and retrain (ROAR) (ResNet-8, Cifar10). In all experiments our methods perform best.
Figure 7: Randomization-Sensitivity Sanity Check. Similarity of attributions before and after net-work (ResNet-50) parameter randomization. Our methods are as sensitive as the network gradient.
Figure 8: Dead Neuron Prevalence. The percentage of originally dead neurons in the selected pathsof different methods reported for sparsity of 80%. All paths selected by pruning objective containoriginally dead (now active) neurons15Under review as a conference paper at ICLR 2021D.4 Path Analysis - Entire NetworkFigure 9: Path Analysis. Overlap between paths from different methods in entire network. Amongthe pruning-based methods, only the path selected by DGR(init=1) overlaps with contribution-basedmethods.
Figure 9: Path Analysis. Overlap between paths from different methods in entire network. Amongthe pruning-based methods, only the path selected by DGR(init=1) overlaps with contribution-basedmethods.
Figure 10: Path Analysis. Overlap between paths from different methods in different layers ofVGG-16. Among the pruning-based methods, only the path selected by DGR(init=1) overlaps withNeuronIntGrad.
Figure 11: Gradient Visualization. The gradients of the locally linearsparsity levels for NeuronIntGrad (top) and NeuronMCT (bottom).
Figure 12: Comparison with Attribution Methods. Comparison between attribution maps derviedour proposed methods (right) vs. gradient-based attribution methods on ResNet-50. Note the im-provement of integrated gradients on the neurons (NeuronIntGrad) over integrated gradients on input(InputIntGrad).
Figure 13: Comparison with Attribution Methods. Comparison between attribution maps derviedour proposed methods (right) vs. gradient-based attribution methods on VGG-16. Note the im-provement of integrated gradients on the neurons (NeuronIntGrad) over integrated gradients on input(InputIntGrad).
