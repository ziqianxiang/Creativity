Figure 1: (a) CALM: A model’s loss (L) is tracked as it observes text pertaining to different classes,while catastrophic interference provokes spikes in this signal. (b-c) Mean cross-entropy for thefirst 15 batches after a switch averaged over all occurrences in MultiLingual and MultiDomain,respectively, under different random seeds. (d) Mixture weights produced by the PoE+PW 30 modelon multilingual data (λ = 10k). (e) Correlation coefficients between mixture weights correspondingto different languages for the PoE+PW 30 model collected during the last 100 batches (λ = 10k).
Figure 2:	Samples from the multilingual datasetbnceuronewswikiMultidomain dataset samplesGood weather for the crops. Have your sheep been suffering much from the staggers ?Have you contributed a great deal this year to the butter mountain ?I would like your advice about Rule 143 concerning inadmissibility.
Figure 3:	Samples from the multi-domain datasetB Further analysisB.1	PoE weights behaviourWe also inspected the gate values produced by LSTM-gated PoE models observing that the modelsare indeed not learning a class-switching mechanism. We hypothesized that this is due to the factthat when the experts are still untrained, the LSTM produces some arbitrary but consistent gatingvalues, making those selected modules being the only ones to be trained, and thus falling into avicious cycle. As a sanity check that supports this hypothesis, we first pre-trained a set of moduleswhile still using our simple gating mechanism. Then, we initialized with these modules a networkthat now used LSTM mixture weights, but training on very short sequences to avoid the effect ofcatastrophic forgetting affecting the network. In this context, the network learned the appropriategating as expected.
Figure 4:	MultiDomain (λ=10k) analysisC Transformer ExperimentsWe experimented extensively with Transformer models. One difference with respect to LSTM mod-els is that Trasnformers, at least in their vanilla versions, are not autoregressive, and thus they cannottransfer information from the past. In standard NLP tasks, they largely overcome this problem byusing a large context window on which they can operate effectively. Thus, to afford them similarmemory capabilities, we kept a buffer of the last b × 512 consecutive examples that was continuallyupdated with each incoming mini-batch.
Figure 5:	Generated text at different stagesof trainingE Model sizesAs it is shown in Table 2, the number of hidden units varies for most of the models. We vary thehidden size in order to keep a similar number of parameters across the models: around 22 millionfor the multilingual setup and around 600 million for the multidomain one.
