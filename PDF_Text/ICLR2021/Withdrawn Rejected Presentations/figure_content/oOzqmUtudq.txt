Figure 1: S2SD. We use a standard encoder Ï†, embedding f, and multiple auxiliary embeddingnetworks gi (used only during training) depending on the S2SD approach used. During training, foreach batch of embeddings produced by the respective embedding network gi , we compute DMLlosses while applying embedding distillation on the respective batch-similarity matrices (DSD/MSD).
Figure 3: S2SD study and ablations. (A) DSD outperforms comparable two-stage distillation onstudent S (Dist.) using teacher (T), with MSD(FA) even outperforming the teacher. We further see thatdistillation is essential for improvement - training multiple spaces in parallel (Joint.) or a detachedlower-dimensional base embedding (Concur.) gives little benefit. (B) We see benefits across basedimensionalities, especially in the low-dimensional regime. (C) We find KL-distillation betweensimilarity vectors (R-KL) to work best. (D) An additional non-linearity in aux. branches g gives aboost, but going deeper degenerates generalization. (E) Distilling each aux. embed. space (Multi) tothe reference space compares favourable against other distillation setups s.a. Nested and Chaineddistillation. (F) We find performance to be robust to changes in weight values.
Figure 2: Generalization metrics. S2SD increasesembed. space density and lowers spectral decay.
