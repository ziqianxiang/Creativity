Figure 1: Overview of our proposed method. We obtain a set of demonstrations of an unseen taskand we adapt to it through efficient demonstration-conditioned exploration.
Figure 2: Meta-training framework. Environment contains a set of tasks T 〜P(T), each with itsown ground truth task descriptor b. Samples generated by the policy overwrite the encoder bufferXT for the current task T. All samples are added to the replay buffer BT . concatenates contextfrom the demonstration and XT to produce a hybrid context cE . Red dotted line copies collectedtransitions into the demonstration buffer if these result in higher rewards than the demonstrations.
Figure 3: Snapshots of the 5 different task families used. We present two (top and bottom) distincttasks for each task family: Reach2D, Stick2D, Peg2D, Key2D, Reach3D (left to right). A roboticagent must navigate in order to find the unseen goal. In Stick2D and Peg2D the agent must insertthe effector. In Key2D the agent must also rotate the interactive (blue) handle to an arbitrary angle.
Figure 4: Test-task performance vs. number of collected transitions during meta-training. By ex-ploiting hybrid inference, PERIL consistently achieves better performance with respect to otherbaselines. Auxiliary systems are particularly useful in complex tasks which require efficient explo-ration. All results are 3-point averaged.
Figure 5: Test-task performance vs. number of collected transitions during meta-training (left andmiddle). PERIL-based methods are capable of generalising within task families. Latent task dis-tributions p(z) during adaptation (right) demonstrate MetaIL (top) is incapable of clustering taskswithin different task families. In contrast, PERIL (bottom) can discriminate them.
Figure 6: TSNE plot of collected z on adaptation.
Figure 7: Effect on k .
Figure 8: Computational graph of PERIL. Loss gradients (defined by blue circles) propagate totheir corresponding models. These include the task encoder qφ, actor πθπ, critic QθQ, and auxiliarymodule dλ. Black circles denote placeholders where latent task beliefs z can be updated (arrow headpointing inwards) or evaluated (arrow head pointing outwards). The operator denotes summationof the loss gradients. The colour of the arrows pointing outwards from the losses indicates the modeldependency for that loss. Notice critic and auxiliary losses are shared within two models. Last, theblack outlines denote the loss gradients which contribute to the meta-objective G(T |z).
