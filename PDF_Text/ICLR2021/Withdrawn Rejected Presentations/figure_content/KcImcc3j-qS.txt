Figure 1: Densities on the simplex of the true distribution (top row, computed by MC integration)and “Laplace Bridge” approximation constructed in this paper (bottom row). For column (a) and(b), two different Gaussians were constructed, such that the resulting MAP estimate is the same, butthe uncertainty differs. For (c), (d) and (e) the same mean with decreasing uncertainty was used. Wefind that in all cases the Laplace Bridge is a good approximation and captures the desired properties.
Figure 2: (Adapted from Hennig et al. (2012)). Visualization of the Laplace Bridge for the Betadistribution (1D special case of the Dirichlet). Left: “Generic” Laplace approximations of standardBeta distributions by Gaussians. Note that the Beta Distribution (red curve) does not even have avalid approximation because the Hessian is not positive semi-definite. Middle: Laplace approxima-tion to the same distributions after basis transformation through the softmax (4). The transformationmakes the distributions “more Gaussian” (i.e. uni-modal, bell-shaped, with support on the real line),thus making the Laplace approximation more accurate. Right: The same Beta distributions, withthe back-transformation of the Laplace approximations from the middle figure to the simplex, yield-ing a much improved approximate distribution. In particular, in contrast to the left-most image, thedashed lines now actually are probability distributions (they integrate to 1 on the simplex).
Figure 3: KL-divergence plotted against the number of samples (left) and wall-clock time (right).
Figure 4:	Upper row: images from the “laptop” class of ImageNet. Bottom row: Beta marginals ofthe top-k predictions for the respective image. In the first column, the overlap between the marginalof all classes is large, signifying high uncertainty, i.e. the prediction is “I do not know”. In thesecond column, “notebook” and “laptop” have confident, yet overlapping marginal densities andtherefore yield a top-2 prediction: “either a notebook or a laptop”. In the third column “desktopcomputer”, “screen” and “monitor” have overlapping marginal densities, yielding a top-3 estimate.
Figure 5:	A histogram of ImageNet predictions’ length using the proposed uncertainty-aware top-k.
Figure 6: Average variance of the Dirichlet distributions of each MNIST class. The in-distributionuncertainty (variance) is nearly nil, while out-of-distribution variance is higher. This implies usabil-ity for OOD detection.
