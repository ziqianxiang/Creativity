Figure 1: We target a California oak for reconstruction and simulation. (Inset) The drone and camerasetup used to collect video data of the tree.
Figure 2: Human labelers use our annotation tool to draw curves with positions, thicknesses, con-nectivities, and unique identifiers on images of the tree.
Figure 3: A set of anisotropic kernels is used to obtrain directional activations in segmentation masksfor both perceptual loss and flow field generation.
Figure 4: (Left) Image masks generated from image annotations and used as training data. (Right)Outputs of the segmentation network.
Figure 5: (Top left) A ground truth mask of the tree taken by flattening the image annotation datainto a simple binary mask. (Bottom left) A visualization of flow directions estimated by applyingdirectional filters to the ground truth mask. (Right) Medial axes of the tree branches estimated fromthe flow field.
Figure 6: The trained network infers a segmentation mask (bottom left) from an input image (topleft). We then estimate a flow field (right) by applying anisotropic filters to the segmentation mask.
Figure 7: Branches labeled from a stereo pair of cameras are visually plausible from the perspectiveof those cameras (top left), but they can exhibit severe error when viewed from a different angle(top right). By clamping these branch positions, one can achieve a virtually identical projection tothe original cameras (bottom left) while maintaining a nondegenerate albeit “flattened” appearancefrom a different angle (bottom right).
Figure 8: The tree model is deformed from its rest pose (left) to an exaggerated pose (right) viasimulation.
