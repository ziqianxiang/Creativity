Figure 1: Illustration of our LayoutTransformer Network (LT-Net) for layout generation. (a) In-ferring implicit relation across objects from the textual inputs. (b) Producing layouts from learnedcontextual features with diversity observed. (c) Image generation from the predicted layout.
Figure 2: LayoutTransformer Network (LT-Net) with Re山tion/Object Predictor P, Layout Gen-erator G, and Layout Refiner (Visual-Textual Co-Attention) Modules. Note that fi：T denote thecontextualized features of each object/relation, and f is that of the entire input. G contains a layoutfeature extractor F to incorporate contextual and bounding box features, followed by a predictionhead Hp for modeling the associated feature distribution θ. We have bτ and b0T indicate coarse andrefined layout outputs, respectively.
Figure 3: Architecture of ouf Visual-Textual Co-Attention (VT-CAtt) module. B denotes the coarselayout synthesized by G, and C represents the contextual vectors produced by F in LT-Net. Mθ in-dicates the refined attention weights, while Wq , WK, WV, and WP are to be learned for performingco-attention. Note that ∆B is the output describing the residual for each bounding box.
Figure 4: Qualitative evaluation on COCO-Stuff and VG-MSDN. Each row shows the textualinput, ground truth layout and those generated by different approaches. For visualization purposes,We apply the pretrained layout2im (Zhao et al., 2019) to convert the output layout into images. Notethat bounding boxes in red indicate layout components not matching the input relationships.
Figure 5: Examples of (a) diverse layout outputs and (b) layout with implicit relation/objectinferred. Note that all three outputs in (a) are conditioned on the same textual input, while objectsand relations from the incomplete textual inputs are recovered in (b).
Figure 6: Distribution visualization of relation priors generated by our LT-Net. Note that x and yaxes represent the differences between the associated bounding boxes of subject and object pair inhorizontal and vertical directions, respectively. Different colors denote each relation of interest. Forexample, the circles in green describe subject-object pairs with the relation word “right of”.
Figure 7: Qualitative comparison on COCO-Stuff (less than 5 objects). For each row We showthe textual input, ground truth layout, synthesized layout, and image converted from the layout bylayout2im (Zhao et al., 2019). Note that, for simplicity, we use the scene graph to represent thetextual input.
Figure 8: Qualitative comparison on COCO-Stuff (more than 5 objects). For each row We showthe textual input, ground truth layout, synthesized layout, and image converted from the layout bylayout2im (Zhao et al., 2019). Note that, for simplicity, we use the scene graph to represent thetextual input.
Figure 9:	Qualitative comparison on VG-MSDN (less than 6 objects). For each row We showthe textual input, ground truth layout, synthesized layout, and image converted from the layout bylayout2im (Zhao et al., 2019). Note that, for simplicity, we use the scene graph to represent thetextual input.
Figure 10:	Qualitative comparison on VG-MSDN (more than 6 objects). For each row We showthe textual input, ground truth layout, synthesized layout, and image converted from the layout bylayout2im (Zhao et al., 2019). Note that, for simplicity, we use the scene graph to represent thetextual input.
Figure 11: More example results of multi-model layout generation on COCO-Stuff. For sim-plicity, We take the scene graph to represent the textual input in each row, followed by three sampledlayout outputs produced by our LT-Net.
Figure 12: More example results of inferring implicit objects and relations on COCO-Stuff.
