Figure 1: Examples of images from the Cityscapes and Maps2sat datasets that received the lowest(first row) and highest (second row) image difficulty scores using our single-image approach. It canbe seen that detailed images with complicated patterns are ranked as difficult, while images withless details and lower contrast are ranked as easier.
Figure 2: Illustration of the proposed black-box membership inference attack on conditionalsegmentation-to-image generation models. We would like to determine if a given image and itssemantic segmentation were used in training. The victim model generates a reconstructed imagebased on the segmentation. In the top path the difference between the reconstructed image and theinput image gives the reconstruction error Lrec . In the bottom path we compute the difficulty scoreLdif f of the input image from the error of a linear predictor to predict pixel values of the ground-truth image from its deep features. Subtracting Ldif f from Lrec gives the membership error.
Figure 3: The proposed membership error Lmem can better separate train and test images by asimple threshold (i.e. a vertical line) compared to the reconstruction error Lrec.
Figure 4:	Comparison of MIA accuracy when using our single image vs. using multi-image diffi-culty scores, as a function of the number of training images. Note that the multi-image score assumesknowledge of the victim’s model, as well as the availability of many labeled training images.
Figure 5:	Comparison of the relation between the reconstruction error and the supervised difficultyscore (right) to the relation between the reconstruction error and our self-supervised difficulty score(left) on Pix2pixHD. Our difficulty score is better correlated to the reconstruction error.
Figure 6: Using the membership score, subtracting the difficulty score from the reconstruction error,makes train and test sets better separated by a vertical line on the x axis. Pix2pixHD for Maps2satand Cityscapes are presented in Fig. 3(c) Pix2pix-Cityscapes	Pix2pix Facades Maps2sat Cityscapes	Pix2pixHD Facades Maps2sat CityscapesOurs-base	96.62%	90.54%	82.23%	99.02%	99.89%	99.19%Ours-Aug	98.46%	92.21%	87.81%	99.09%	99.21%	99.91%Table 4: Augmentations improve the accuracy of our attack.
Figure 7: Comparison of the relation between the reconstruction error and the supervised difficultyscore (right) to the relation between the reconstruction error and our self-supervised difficulty score(left). Maps2sat and Cityscapes are presented in Fig. 5. As can be seen, our score is better correlatedto the reconstruction error.
Figure 8: Effect of α in Eq. (3) over the attack success.
Figure 9: Effect of overfitting on the attack success rate. (a) Pix2pix, (b) Pix2pixHDQ-R-JOoou□<uo±0.550	100	150	200#epochs(b)	Pix2pixHDwhich corrupts the generated image, is required in order to have a significant effect over our attacksuccess. Moreover, it can be seen that even with large amounts of noise, our attack still managesto succeed much better than random guessing. This implies that our attack is robust to the Gaussdefense.
Figure 10:	Effect of Gauss defense on the attack success rate. (a) pix2pix, (b) Pix2pixHDA.7 Shadow model trainingAfter having selected N images, denoted as shadow-train, which We used to train the shadow modeland another N images, ShadOw-test, which are not seen by the shadow model, we set the labels as:0, if X — shadow-trainlabel(x) =1, if x — shadowiest(4)The classifier C architecture and training procedure are similar to He et al. (2019). For each image,we compute the structured loss map between the ground-truth image and the generated image, andat every epoch we randomly crop 15 patches of size 90 × 90 from the structured loss map. We train aResNet-50 (He et al., 2016) from scratch on the 90 × 90 patches, modified for binary classification.
Figure 11:	Effect of the number of training images on shadow-model-based attacks. The shadow-model-based attack depends on the number of images used, we outperform this approach with noadditional data.
Figure 12: Evaluating our attack on the CelebA dataset, by training a pix2pixHD model from faciallandmarks to face images, achieves a high success rate of 99.04%.
Figure 13: Examples of images from the Cityscapes (first two rows) and Maps2sat (last two rows)datasets that received the lowest (first and third row) and highest (second and last row) difficultyscores using the supervised difficulty score.
Figure 14: Examples of images from the ImageNet dataset that received the lowest and highestdifficulty scores. First row - lowest scored train images. Second row - lowest scored test images.
Figure 15: Effect of different data partitions in the training of the difficulty score’s regression model,evaluated on the Cityscapes dataset and pix2pixHD model. Using less then 50% of the image pixelsresults with unstable performance, while all values of 50% or above result in similar attack successrates.
Figure 16: We evaluated the effect of different resize methods in the difficulty score on theCityscapes dataset and pix2pixHD model. The attack success rate is not very sensitive to the re-size method.
Figure 17: We evaluated the effect of incorporating our difficulty score in the training process of theconditional generative model. This encourages the model to pay more attention to difficult images.
