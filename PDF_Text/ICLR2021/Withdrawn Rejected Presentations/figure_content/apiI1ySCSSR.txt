Figure 1: Comparison of fine-tuning, joint training, and MeRLin on the semi-synthetic dataset. Left:The semi-synthetic dataset and the qualitative observations on the representations learned by threealgorithms. Right: Quantitative results on the target test accuracy. See more interpretations, analysis,and results in Section 3.
Figure 2: (a) T-SNE visualizations of features on the target train and test set. The representationsof pre-training work poorly on both target train and test set, indicating that transferable features arenot learned. Both joint training and fine-tuning work well on the target train set but poorly on thetest set, indicating overfitting. MeRLin works well on the target test set. (b) Evaluation of differentmethods on A and B. Joint-training and pre-training rely heavily on the source-specific feature Band learn the transferable feature A poorly compared to MeRLin. See more details in Section 3.
Figure 3: Comparison of intra-Classto inter-class variance ratio. Thisquantity is loWest for MeRLin, indicat-ing that it separates classes best.
Figure 4: (a) Analysis of Feature Quality. Comparison of feature-label correlation. A lowerquantity is better, and MeRLin has the lowest value. (b) Sensitivity of the proposed method tohyper-parameters. We test the accuracy on Food-101→CUB-200 with varying ρ and λ and providethe visualization.
Figure 5: Comparison of L2-sp and MeRLin. The results are averaged over 10 runs, with errorbars representing 95% confidence interval drawn by 1,000 bootstraps.
