Figure 1: (a) Standard Potts model requires constructing an MSA and optimizing parameters W.
Figure 2: Inductive generalization gain (illustration with a 1D loss landscape). W* is the standardPotts model, estimated on the finite observed MSA M. Though it minimizes the training objective, itdoes not achieve perfect generalization performance. However the Neural Potts Model Wθ (x) cangeneralize better than W * through transfer learning from related samples, guided by the inductiveʌbias of the model. We expect this especially when the estimate W * is far from W *, e.g. on small orbiased MSAs.
Figure 3: Contact prediction precision on Pfam families from the NADP Rossmann clan, at differentlevels of depth reduction. Columns show (from left to right) short, medium and long-range precisionfor top-L threshold. Across the metrics, NPM outperforms the independent Potts model trained onthe shallowest MSAs, as well as the Nearest Neighbor Potts model baseline.
Figure 4: Trajectory of training on the NADP Rossman clan, averaged over five-fold cross-evaluation.
Figure 5: UniRef50: contact prediction precisions (higher is better) with 95% bootstrapped confi-dence intervals, on medium range (left), long range (middle), binned by MSA depth Meff . Top row:sequences from the train set; bottom row: sequences from the test set. For shallow MSAs, averageperformance of NPM is higher than the independent Potts model. Right: scatter plot comparing longrange precision from NPM vs independent Potts model, each point is a protein. More metrics arepresented in Appendix D Fig. 10.
Figure 6: Examples where NPM outperforms the independent Potts model fit directly on the MSA.
Figure 7: A random sample of 1000 MSAs for 1000 sequences in UniRef50 were analyzed. Thegraph shows cumulative density plots with MSA depth in log-scale on the x axis, for different querysequence coverage requirements (-cov [20, 80]) specified by a call to HHfilter, applied to eachMSA. The fraction of MSAs with depth ≤ 10 is 19% (38% when a coverage of 80% is specified),while the fraction of MSAs with depth ≤ 100 is 30% (55% when a coverage of 80% is specified) .
Figure 8: Comparison of the main NPM model architecture choices, evaluated using the Pfamexperimental setup. We show precisions at fixed top-L threshold, while on the x-axis we vary sequenceseparation range and two levels of MSA depth reduction (10 and 1000). Standard deviations overthe five-fold cross-evaluation are shown. For the direct multi-head bilinear form (mhbf) prediction(tied or untied), we found an improvement from using U, V projection dimension 512 rather than128. Other hyper-parameters follow Table 2.
Figure 9: Contact prediction precision on Pfam families at different levels of depth reduction. Threeapproaches are compared: (1) the Neural Potts Model, (2) independent Potts Model, (3) NearestNeighbor Potts. On shallow MSAs, NPM outperforms both independent Potts and Nearest NeighborPotts for all four of the clans. On deep MSAs NPM matches the independent Potts baseline for 3 ofthe 4 clans. Clans are noted on the right hand side.
Figure 10: Additional metrics for the UniRef50 results. We show the sequence separation range ascolumns. On each row, we vary over cutoff thresholds L, L/5 and AUC defined in Appendix C.2. Thetop 3 rows are for train sequences, bottom 3 rows for test sequences.
