Figure 1: The XLSR approach. A shared quantization module over feature encoder representationsproduces multilingual quantized speech units whose embeddings are then used as targets for aTransformer trained by contrastive learning. The model learns to share discrete tokens acrosslanguages, creating bridges across languages. Our approach is inspired by Devlin et al. (2018);Lample & Conneau (2019) and builds on top of wav2vec 2.0 (Baevski et al., 2020c). It requires onlyraw unlabeled speech audio in multiple languages.
Figure 2: Visualization of language similarities learned by the model Figure (a) visualizes theshared discrete latent speech representations across languages for a model trained on 12 Common-Voice languages (CV-12). Figure (b) shows that adding Chinese-HongKong (zh-HK) shares relativelyfew latents with other languages. Figure (c) is for a model trained on 17 BABEL languages andillustrates that clusters can correspond to similar languages like Bengali and Assamese.
