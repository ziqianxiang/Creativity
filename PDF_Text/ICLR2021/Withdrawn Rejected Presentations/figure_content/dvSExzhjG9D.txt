Figure 1: Pre-set LR schedules for (a) image and (b) text classification. (c) Visualization of how weinput current loss Lt to MLR-SNet, which then outputs a proper LR αt to help SGD find a betterminima. LR schedules learned by MLR-SNet on (d) image and (e) text classification. (f) We transferLR schedules learned on CIFAR-10 to image (CIFAR-100) and text (Penn Treebank) classification,and the subfigure shows the predicted LR during training.
Figure 2: The structure of our proposed MLR-SNet.
Figure 3: (Above) Train loss and(Below) test loss as a function ofa point on a random ray starting atthe solutions for different methodson CIFAR-100 with ResNet-18.
Figure 4:	Test accuracy of our methods (train) and compared baselines on different datasets.
Figure 5:	Test accuracy on CIFAR-100 of ResNet-18 with varying epochs.
Figure 6:	Test accuracy of transferred LR schedules on different datasets.
Figure 7:	Test accuracy on CIFAR-10 of different network architecturesSince image tasks often use SGDM to train DNNs, Fig.4(d) and 4(e) show the results of baselinemethods trained with SGDM, and they obtain a remarkable improvement than SGD. Though notusing extra historical gradient information to help optimization, our method achieves comparableresults with baselines by finding a proper LR schedule for SGD.
Figure 8: Test accuracy on Ima-geNet with ResNet-50.
Figure 9: Comparion of differ-ent meta-learners.
Figure 10: Time consuming ofdifferent LR schedules methods.
Figure 11: Train loss (perplexity), test accuracy (perplexity) and learned LR schedules of our methods(train) and compared baselines on different tasks.
Figure 12: Ablation study. (a) Test accuracy on CIFAR-10 with ResNet-18 of different architecturesof MLR-SNet. ‘a-b’ denotes the configurations of MLR-SNet, where ‘a’ represents the number oflayers, and ‘b’ represents the number of hidden nodes. (b)Test accuracy on CIFAR-10 with ResNet-18 of different LRs of meta optimizer ’Adam’. (c) Test accuracy on CIFAR-10 with ResNet-18 ofdifferent gamma values of MLR-SNet.
Figure 13:	(a) We plot the LR variation curves along iterations with the same input for learnedMLR-SNet at different epochs. As is shown, when iteration increases, the LR is almost constant.
Figure 14:	Test accuracy on CIFAR-100 of different DenseNet architectures.
Figure 15: The meta-test performance on TinyImageNet (target task) of different meta-training tasks.
Figure 16:	Performance comparion on CIFAR-10 of the MLR-SNet and baselines.
Figure 17:	Train loss, test accuracy and learned LR schedules of our method(train) and LR Con-troller(train) on CIFAR-10 and CIFAR-100.
Figure 18:	Train loss, test accuracy of our method(test) and LR Controller(test) on CIFAR-100.
