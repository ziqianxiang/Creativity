Figure 1: (a) A histogram of parameter counts for each problems in the task suite. Our task suitespans more than 7 orders of magnitude in model size. (b) Percentage of optimizers (y-axis) capableof reaching a given loss value (color) for tasks (x-axis). We find there exists around 100 “easy” taskson which more than half of the optimizers perform well, and a large number of “difficult” tasks forwhich almost no optimizers perform well. (c) A histogram of training times. Almost all tasks can betrained in under an hour.
Figure 2: By learning a search space we achieve large speedups over random search. On the y-axis weshow J, or the best aggregated and normalized performance achieved given some number of optimizertrials (x-axis). This is computed on heldout tasks not used to train the hyperparameter list. In solidwe show median performance with 25-75 percentile shown with error bars over 50 resamplings ofthe train-test split of tasks, as well as random samplings. In black we show a learned search spacecomputed from the Adam8p family of optimizes. In color we show various random search baselines.
Figure 3: Using more tasks to train the search space results in improved performance on heldout tasks.
Figure 4: We find our learned optimizer list outperforms both learning rate tuned Adam anddefault training hyperparameters for ResNet50 and a 53M parameter Transformer trained on LM1B.
Figure S1: A 2D TSNE embedding of all 1162 tasks. This embedding is produced from a 1,000dimensional feature vector consisting of task loss evaluated with many different hyperparameterconfigurations. We find similar tasks - e.g. masked auto regressive flow models, and character / wordRNN models - cluster, suggesting similarity in the optimizers that perform well. See §?? for moredetails.
Figure S2:	We show learned search space generalization, measured as a ratio of the loss achievedin training and testing, versus the number of task parameters used during search space training.
Figure S3:	We find our learned hyperparameter lists performs about as well as random search onthe NAdam search space, and worse than the random search on the learning rate tuned Adam searchspace.
Figure S4:	We find our learned hyperparameter lists out performs learning rate tuned Adam withboth a constant, and a fixed learning rate schedule on a 53M parameter Transformer trained on LM1B.
Figure S5: Hyperparameter lists trained on short horizon data generalize remarkably well. On they-axis we show performance evaluated on the the full 10k training iterations for a given numberof optimizers tried (x-axis). In color we show different number of steps used when evaluating taskoptimizer performance when training the hyperparameter list.
Figure S6: Left: Aggregate performance (y-axis) vs number of optimizer tried (x-axis) for differentnormalization and aggregation techniques. In each curve we train the hyperparameter list with a dif-ferent normalization and aggregation strategy and test with the default normalization and aggregationtechnique described in 3.3. We find some some strategies are near identical in performance (e.g. minnorm), while others perform significantly worse - e.g. last quantile norm. In both cases, however,we still perform better than the underlying random search. Center: Correlation between default nor-malization and the quantile based normalization strategy. Correlation is quite low - 0.193 Pearson’scorrelation. Right: Correlation between the default normalization using a mean to aggregate overvalidation over the course of training vs using a min over validation over the course training. We finda much higher correlation of 0.911.
Figure S7:	Learning hyperparameter lists using one task family and testing on the remainder oftask families. We normalize each column from 0-1 to account for different mean losses across tasks.
Figure S8:	Timings computed for each task family. We find most task families have a narrowdistribution of compute times.
Figure S9: Performance continues to improve as more and more optimizers are used when training thesearch spaces. On the x-axis we show number of optimzers (size of Θ, the number of hyperparameterevaluations used in training the learned hyperparameter list) and y-axis we show test loss achievedwhen applying the learned search space for a given fixed length, e.g. different values of k shown incolor). We plot median with 25-75 percentile shaded over different random optimizer samples and iidtask splits. Stars (with horizontal guide lines) denote best search for the corresponding number ofhyperparameters for learning rate tuned Adam in half orders of magnitude.
