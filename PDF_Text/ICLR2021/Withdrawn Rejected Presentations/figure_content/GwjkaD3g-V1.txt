Figure 2: Multi-object 3D scene representation network. The image is sequentially encoded intoobject representations using an encoder network g0 . The object encoders additionally receive imageand mask compositions (∆I, M) generated from the previous object encodings. A differentiablerenderer based decoder F composes images and masks from the encodings of previous steps. Thebackground is encoded from the image in parallel and used in the final scene reconstruction.
Figure 3: Object-wise encoding and rendering. We feed the input image and scene composi-tion images and masks from the previously found objects to an object encoder network go whichregresses the encoding of the next object zi. The object encoding decomposes into shape zi,sh, ex-trinsics zi,ext and texture latents zi,tex. The shape latent parametrizes an SDF function network Φwhich we use in combination with the pose and scale of the object encoded in zi,ext for raycastingthe object depth and mask using our differentiable renderer f . Finally, the color of the pixels isfound with a texture function network Ψ parametrized by the texture latent.
Figure 7:	Evaluation on real images. We show preliminary results on real images by our model thatwas trained on synthetic data. We notice that our model is able to capture the coarse scene layout andshape properties of the objects. However, challenges arise due to domain, lighting, camera intrinsicsand view point changes indicating interesting directions for future research.
Figure 8:	Limitations. Input and output pairs for typical failure cases and limitations of our methoddue to ambiguities for self-supervised learning. See text for details.
