Figure 1: An illustration of quotient manifold modeling (QMM). Images of a cat moving aroundin a room would form an 1-D manifold due to the location of the cat; but the structure can becomemulti-manifold by having different discrete features (a chair or a vase in the background). Theidea of QMM is to consider the generic structure shared by the manifolds using an encoding map.
Figure 3:	Images generated from the trained Q-VAEs (first row) and Q-GANs. Each i-th rowpresents samples from the i-th manifold (only 8 out of 20 are shown for 3D-Chair) and each columnpresents the samples generated from the same latent code z, which is randomly sampled from p(z).
Figure 4:	Disentangled features. Images are arranged the same as Fig. 3, except the columns showlinear changes in the latent space along the first eigenvector from the disentanglement analysis (seeSec. 6.2). Slant, width (MNIST), height and brightness (3D-Chair) components are shown.
Figure 5: Manifolds derived for a noise-added data 1(highlighted in red boxes). Here, we also trainon RGB-MNIST dataset: a simple extension of MNIST by coloring with (R, G, or B).
Figure 6: Left: QMM-applied models trained on MNIST generates alphabet ’v’ included in EM-NIST dataset. Right: QMM-applied models trained on two-digit MNIST (multiples of 9) dataset,generating one-digit MNIST and non-multiples.
