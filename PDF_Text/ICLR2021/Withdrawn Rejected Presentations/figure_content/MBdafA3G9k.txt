Figure 1: Overview of our method: We aim to learn a distance function (1a) and then use that distancefunction as a reward function for RL (1b). At the current timestep, observations (o) of the referencemotion and the agent are encoded (e) and fed into LSTMs (leading to hidden states h). Fig. 1a showshow the reward model is trained using both Siamese and AE losses. There are: VAE reconstructionlosses on static images (LV AEI), sequence-to-sequence AE losses (LRAES), one for the referenceand one for the agent (which we do not show in pink to simplify the figure). There is a Siamese lossbetween encoded images (LSNI) and a Siamese loss that is computed between encoded states overtime (LSNS). Fig. 1b shows how the reward is calculated at every timestep. Reward for the agent atevery timestep consists of the distance between encoded images and encoded LSTM hidden states.
Figure 3: Rasterized frames of the agent’s motion aftertraining on humanoid3d walking and running. Addition-ally, a zombie walk and jumping policy can be found onthe project website: https://sites.google.com/view/virl1.
Figure 4: (a) Comparisons between VIRL, a simple VAE and GAIfO for the humanoid walking task.
Figure 5: (a) Ablation analysis of VIRL on the Walking Task showing the mean reward over of the number ofsimulated actions. The legend is the same as (b) where we examine the impact on our loss under the differentdistance metrics resulting from the ablation analysis. We find that including multi-task data (only available forthe humanoid3D) and both the VAE and recurrent AE losses provide the most performant models. (c) Ablatingthe recurrent autoencoder from VIRL dramatically impairs the ability to learn how to walk like a Zombie. (d)The use of multi-task training data helps learn better policies for running (away from Zombies if desired).
Figure 7: Rasterized frames of the imitation motions on humanoid3d walking (row 1), running (row 2), zombie(row 3) and jumping(row 4). https://sites.google.com/view/virl1Figure 8: The flow of control for the learning system.
Figure 8: The flow of control for the learning system.
Figure 9: We use a Siamese autoencoding network structure that can provide a reward signal at every stepto a reinforcement learning algorithm. For the Humanoid experiments here, the convolutional portion of ournetwork includes 2 convolution layers of 8 filters with size 6 × 6 and stride 2 × 2, 16 filters of size 4 × 4 andstride 2 × 2. The features are then flattened and followed by two dense layers of 256 and 64 units. The majorityof the network uses ReLU activations except for the last layer that uses a sigmoid activation. Dropout is usedbetween convolutional layers. The RNN-based model uses a LSTM layer with 128 hidden units, followed by adense layer of 64 units. The decoder model has the same structure in reverse with deconvolution in place ofconvolutional layers.
Figure 10: Training losses for the Siamese distance metric.
Figure 11: Ablation analysis of VIRL. We find that training RL policies is sensitive to the size anddistribution of rewards. We compare VIRL to a number of other simple baselines.
Figure 12: Variance and t-SNE Visualization7.9	Positive and Negative ExamplesWe use two methods to generate positive and negative examples. The first method is similar to TCN,where we can assume that sequences that overlap more in time are more similar. For each episode,two sequences are generated, one for the agent and one for the imitation motion. Here we list themethods used to alter sequences for positive pairs.
Figure 13: RL algorithm comparison on humanoid2d environment.
