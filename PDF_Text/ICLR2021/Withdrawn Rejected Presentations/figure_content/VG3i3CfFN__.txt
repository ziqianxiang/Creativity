Figure 1: Pharse alignments in PhraseTransformer.
Figure 2: PhraseTransformer Encoder architecture using n_gram LSTM in MUltiHead Layer. In thiscase, n-gramLSTM layer is built with n = [0,0,2,2,3, 3,4,4], 2-gram, 3-gram, 4-gram modelsapply to every two heads from head 3 to head 8.
Figure 3: The impact of BPE preprocess-ing to performance of PhraseTransformer onMSParS dev set.
Figure 4: Token-level accuracy (min, maxand average) of PhraseTransformer and theoriginal Transformer on Geo test set.
Figure 5:	Heatmap visualization of Attention. Figure a shows the difference of alignment Encoder-Decoder attention between the original Transformer (left) and PhraseTransformer (right). Consid-ering one row, the value in each column is corresponding to the rate of the attention of token in LFto the word in the sentence. Figure b shows Self-Attention in 8 heads of the last PhraseTransformerEncoder layer. TWo blue rectangles are zoomed-in separately of head 1 (not use n_gramLSTM),head 3 (use 2_gramLSTM).
Figure 6:	Figure a draws the representing vector of phrases in Selft-Attention Layer using PCA onAtis test set. Figures b, c are zoomed-in view of the blue and red clusters. The labels are annotatedfor each point in two figures show the information of the phrase corresponding to point followingthe template (sentence dd, Pharse.position) phrase-content.
Figure 7:	Heatmap visualization of Self-Attention in 8 heads at last layer of PhraseTransformerEncoder. Heads 1 - 8 are ordered from left to right. The gram sizes n = [0, 0, 2, 2, 3, 3, 4, 4].
Figure 8:	Heatmap visualization of Encoder-Decoder Attention of Transformer (left) and Phrase-Transformer (right). Two sentences are randomly chosen in the MSParS test set. PhraseTransformerpays more attention than Transformer to words that are entities name in the sentence. For example:words “tom cruise” and “tom san@@ ders” in sentence 6456 or “rick man@@ ning” in sentence440.
Figure 9:	Figure a draws the representing vector of words in Selft-Attention Layer using PCA onAtis test set. Figures b, c are zoomed-in view of the blue and cyan clusters. The labels are annotatedfor each point in two figures show the information of the word corresponding to point following thetemplate (sentence dd, word_position) phrase .context [considering *ord].
