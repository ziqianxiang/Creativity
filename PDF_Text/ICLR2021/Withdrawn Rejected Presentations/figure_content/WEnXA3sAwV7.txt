Figure 1: The extracted featuresimilarity of clips which appliedwith different degrees of tempo-ral augmentations from the sameinput. The number of strengthmean the buffer frames for po-tential frame cropping or drop-ping. As it becomes bigger, theaugmentation will be stronger.
Figure 2: (a) The distribution of cosine similarities of intra-video and inter-video clips. (b) The relativetemporal distances vs. feature similarities between clips with the same video. The shaded region shows the95% confidence interval.
Figure 3: Overview of the proposed self-supervised temporal learning (SSTL) framework. We randomlyone clip from a short video, and then get two processed clips through applying twice temporal augmentationand spatial augmentation for each clip. Finally, we feed it to a 3D backbone with an MLP head and a speedpredictor head. The contrastive loss is used to train the network to maximize agreement between differentlyaugmented views of the same clip and minimize agreement between different clips. Some repeated connectionlines are not drew in the figure.
Figure 4: Transfer learning evaluationof representations with different outputdimensions and batch size. Each bar isa single run from scratch. The repre-sentation before the projection is 512-dimensional here.
Figure 5: UCF classes with different gain using temporal augmentations. The plot shows UCF per-classaccuracy difference between the two pretrained models tested. It suggests that our temporal augmentations aremost useful for classes that require understanding motion, such as “Diving” and “VolleyballSpiking.”.
