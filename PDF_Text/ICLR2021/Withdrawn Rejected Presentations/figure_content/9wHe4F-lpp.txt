Figure 1: Left: The basic block in original Bi-Real Net vs. the simplified basic block in FTBNN,where we directly absorb the explicit scaling factors into the BN layer by leveraging BN’s scalingfactors. Right: The non-linear modules (ReLU or FPReLU) are explicitly added after each basicblock in FTBNN. To maximize the model’s discriminative power while keeping its training stability,the number of ReLU is controlled and the proposed FPReLU is connected with most blocks.
Figure 2: Feature visualization for 4 different models experimenting on MNIST, which are denotedas Linear: using real-valued convolution layers without non-linear module; Binary: using binaryconvolution layers without non-linear module; PReLU: using binary convolution layers with PReLUmodule; ReLU: using binary convolution layers with ReLU module. Testing accuracies are recordedbased on 5 runs for each model. The average discrepancy between output from binary convolutionsand that from its corresponding real-valued convolutions (using the proxy weights and original real-valued activations to conduct the convolution) are recorded as well (best viewed in color).
Figure 3: Left: The proposed FPReLU, slops in both sides are started from 1 (equal to anidentity map) and adaptively learned in channel-wise. Middle: Reduction block with group binaryconvolution and cheap downsampling layer (for normal block, the original binary convolutionlayer in Fig. 1 is also implemented with groups in the derived models). Right: Computationalbudget distributions for original and binary-purified FTBNN. The computational budget of a originalfloating point multiply operation (FLOP) in a convolution is reduced by 64 times if the operation isbinarized (using XNOR-Count instead). Based on that, when measuring the computational budget,the budget of binary operations (BOPs) are converted to equivalent of FLOPs with a factor of 1/64.
Figure 4: (a): Initial convolutional block for FTBNN baseline with a 7x7 convolutional layer, theoutput of which has 64 channels. (b): Initial convolutional block for the derived models with a3x3 convolutional layer, the number of channels in the output of which is halved compared withthe baseline. (c): Bridge convolutional block for the derived models, the number of channels in theoutput of which can be arbitrary. c and w represent the channel numbers. (d): Normal block for thederived models. (e): Reduction block for the derived models (in our experiments, we set the actualgroup number to 2 in the reduction block for settings Group = 1 and Group = 2 in Section 3.2).
Figure 5: Left: Training curves for Layer = 18 with computational budget 〜90M. Right: Trainingcurves for Layer = 34 with computational budget 〜135M. Group execution has a positive effect fordeeper BNN structures as discussed in Section 3.2.
