Figure 1: The mTSP MDP The black lined balls indicates the events of the mTSP MDP. The empty,dashed, and, filled rectangles represent the unassigned, assigned, and inactive cities, respectively.
Figure 2: Assignment action determination step of ScheduleNetthe entity is active or inactive In case of tasks, inactive indicates that the task is already visited;in case of worker, inactive means that worker returned to the depot. Similarly, ITSSigned indicateswhether worker is assigned to a task or not. We also define the environment state seτnv that containsthe current time of the environment, and the sequence of tasks visited by each worker, i.e., partialsolution of the mTSP. The state sτ of the MDP at the τ-th event becomes sτ = ({sT}m=+N, STnV) ∙The first state s0 corresponds to the empty solution of the given problem instance, i.e., no cities havebeen visited, and all salesmen are in the depot. The terminal state sT corresponds to a complete so-lution of the given mTSP instance, i.e., when every task has been visited, and every worker returnedto the depot (See Figure 1).
Figure 3: Gap distributions of the random mTSP instances [Left] Small-size random instanceresults, [Right] Medium-size random instance results, [Right] Large-size random instance results6	ExperimentsWe train the ScheduleNet using mTSP instances whose number m of workers and the number Nof tasks are sampled from m 〜 U(2,4) and N 〜 U(10, 20), respectively. This trained SchedU-leNet policy is then evaluated on the various dataset, including randomly generated uniform mTSPdatasets, mTSPLib (mTS), and randomly generated Uniform TSP dataset, TSPLib, and TSP (dai).
Figure 5: [Left] Effect of reward function The blue, orange, and green curve shows the traininggap of the sparse reward, the distance reward, and distance-utilization reward over the trainingsteps, respectively. The shadow regions visualize one standard deviation from the mean trends. Wereplicates 5 experiments per reward setup. [Right] Effect of training method The orange, green,and red curve shows the training gap of PPO model with the proposed spare, distance, and distance-utilization reward, respectively. We visualize the results of the clipped REINFORCE with the sparereward, which is denoted as blue curve, for clear comparisons. The shadow regions visualize onestandard deviation from the mean trends. We replicates 5 experiments per setup.
Figure 4: makespan dis-tribution7	conclusionWe proposed ScheduleNet for solving MinMax mTSP, the problem seek-ing to minimize the total completion time for multiple workers to com-plete the geographically distributed tasks. The use of type-aware graphs and the specially designedTGA graph node embedding allows the trained ScheduleNet policy to induce the coordinated strate-9Under review as a conference paper at ICLR 2021gic subroutes of the workers and to be well transferred to unseen mTSP with any numbers of workersand tasks. We have empirically shown that the proposed method achieves the performance compara-ble to Google OR-Tools, a highly optimized meta-heuristic baseline. All in all, this study has shownthe potential that the proposed ScheduleNet can be effectively used to schedule multiple vehicles forsolving large-scale, practical, real-world applications.
Figure 6: Computation time of ScheduleNet and OR-Tools. Figures show the computation time(in seconds) of ScheduleNet and OR-Tools as a function of number of cities (left) and numberof workers (right). We sample 10 mTSP instances per N-m combination for OR-Tools due tovariability in the runtime due to underlying instance’s topology. We measure all times on a CPUmachine equip AMD Ryzen Thereadripper 29990WX without any parallelization.
Figure 7: Type-aware graph attention embedding We omit the type-aware edge update for theclarity of visualization.
