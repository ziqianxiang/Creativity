Figure 1: A visualization of the behaviors of the agents with different backup lengths. (a) presents thelayout of the maze environment (denoted as Dense Maze), which contains a starting grid and a goalgrid. (b) and (c) illustrate the behaviors of the agents updated using 1-step return and 5-step return,respectively. It is observed that the agent trained with 5-step return reaches the goal through shorterdiagonal trajectories, while the agent trained with 1-step return explores more grids in the early stage.
Figure 2: Overview of the proposed MB-DQN framework.
Figure 3: Comparison of the evaluation curves of MB-DQN and the baselines in eight Atari games. 14	Experimental ResultsIn this section, we present the experimental results to demonstrate the advantages of the mixture usageof different backup lengths. We first evaluate the proposed MB-DQN on a collection of well-knownAtari 2600 (Bellemare et al., 2015) games, and compare its performance to different configurationsof boostrapped DQN both quantitatively and qualitatively in Section 4.1. Next, we investigate thequality of the data samples collected by MB-DQN, and demonstrate their advantages in training anRL agent in Section 4.2. Then, we validate the assumption made in Section 1 that unifying differentstep return targets to a single target value may not be as effective as the proposed MB-DQN approachin Section 4.3. Finally, we further provide a set of ablation analyses of MB-DQN on Atari games toinspect and discuss the impacts of different configurations on MB-DQN’s performance in Section 4.4.
Figure 4: Visualization of the agents’ attention areas (rendered in red). The attention areas are derivedbased on (Greydanus et al., 2018) and are obtained from the bootstrapped heads of MB-DQN.
Figure 5: The evaluation curves of (a) comparison of different configurations of data generationagents and learning-only agents for validating the quality of data samples collected by MB-DQN inSection 4.2, and (b) comparison between the single λ-target strategy adopted by DQN (λ) (Daley &Amato, 2019) and the multiple bootstrapped targets strategy adopted by MB-DQN in Section 4.3.
Figure 6: Impacts of (a) different configurations of step returns for the bootstrapped heads, and (b)different numbers of the bootstrapped heads on the proposed MB-DQN for four different Atari games.
Figure A1: Visualization of the agents’ attention areas (rendered in red) for six Atari games.
Figure A2: Curves of four more Atari games for (a) different configurations of step returns for thebootstrapped heads, and (b) different numbers of the bootstrapped heads on the proposed MB-DQN.
Figure A3: Experimental results of different configurations of step returns in four other Atari games.
