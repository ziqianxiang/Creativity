Figure 1: Some interesting observations on the structure of layer-wise Hessians. The eigenspaceoverlap is defined in Definition 4.1 and the reshape operation is defined in Definition 4.2Structure of Top Eigenspace for Hessians: Consider two neural networks trained with differentrandom initializations and potentially different hyper-parameters; their weights are usually nearlyorthogonal. One might expect that the top eigenspace of their layer-wise Hessians are also verydifferent. However, this is surprisingly false: the top eigenspace of the layer-wise Hessians have avery high overlap, and the overlap peaks at the dimension of the layer’s output (see Fig. 1a). Anotherinteresting phenomenon is that if we express the top eigenvectors of a layer-wise Hessian as a matrixwith the same dimensions as the weight matrix, then the matrix is approximately rank 1. In Fig. 1bwe show the singular values of several such reshaped eigenvectors.
Figure 2: Comparison between the approximated and true layer-wise Hessian of F-2002 .
Figure 3: Heatmap of Eigenvector Correspondence Matrices for fc1:LeNet5, which has 120 outputneurons. Here we take the top left corner of the eigenvector correspondence matrices. Similaritiesbetween (a)(c) and (b)(d) respectively verify the decoupling conjecture.
Figure 4: Eigenspectrum of the layer-wise output Hessian E[M] and the layer-wise weight HessianHL(w(p)). The vertical axes denote the eigenvalues. Similarity between the two eigenspectra is adirect consequence of a low rank E[xxT] and the decoupling conjecture.
Figure 5: Overlap between the top k dominating eigenspace of different independently trained models.
Figure 6: Ratio between top singular value and Frobenius norm of matricized dominatingeigenvectors. (LeNet5 on CIFAR10). The horizontal axes denote the index iof eigenvector hi, and the vertical axes denote k Mat(hi)k/k Mat(hi)kF.
Figure 7: Optimized posterior variance s using different algorithms (fc1:T-2002 trained on MNIST).
Figure 8: Top 50 Eigenvalues and Eigenspace approximation for full Hessian19C Computation of Hessian Eigenvalues and EigenvectorsFor Hessian approximated using Kronecker factorization, we compute E[M] and E[xxT] explicitly.
Figure 9: Eigenspace overlap of different models of LeNet5 trained with different hyper parameters.
Figure 10: Top Eigenspace overlap for varients of VGG11 on CIFAR10 and CIFAR100260.00.200.150.100.050.000.08 -0.06 -0.04 -0.02 -0.00 -0.3 -020.L0.0-0.5 -0.4-0.3 -
Figure 11:	Top Eigenspace overlap for variants of ResNet18 on CIFAR10027E.2.3 Failure CasesAs seen in Fig. 25 and Fig. 11, there is a small portion of layers, usually closer to the input, whoseeigenspace overlap does peak around the output dimensions. These layers can be clustered into thefollowing two general cases.
Figure 12:	Top eigenspace overlap for layers with an early low peak.
Figure 13: Top eigenspace overlap for layers with a delayed peak.
Figure 15: Eigenvector Correspondence for fc2:LeNet5. (m=84)25	50	75	100	125	150	17580 -0Figure 14: Eigenvector Correspondence for fc1:LeNet5. (m=120)20 -40 -60 -(b) Correspondence with E[M].
Figure 14: Eigenvector Correspondence for fc1:LeNet5. (m=120)20 -40 -60 -(b) Correspondence with E[M].
Figure 16: Eigenvector Correspondence for conv1:LeNet5. (m=6)0	10	20	30	40	0	10	20	30	40(a) Correspondence with E[xxT].	(b) Correspondence with E[M].
Figure 17: Eigenvector Correspondence for conv2:LeNet5. (m=16)29E.4 Structure of E[xxT] and E[M] During TrainingWe observed the pattern of E[xxT] matrix and E[M] matrix along the training trajectory (Fig. 18,Fig. 19). It shows that E[xxT] is always approximately rank 1, and E[M] always have around clarge eigenvalues. According to our analysis, since the nontrivial eigenspace overlap is likely to be aconsequence of a approximately rank 1 E[xxT], we would conjecture that the overlap phenomenonis likely to happen on the training trajectory as well.
Figure 18: Top eigenvalues of E[xxT] along training trajectory. (fc1:LeNet5)Figure 19: Top eigenvalues of E[M] along training trajectory. (fc1:LeNet5)30F Additional ExplanationsF.1 Outliers in Hessian EigenspectrumOne characteristic of Hessian that has been mentioned by many is the outliers in the spectrum ofeigenvalues. Sagun et al. (2018) suggests that there is a gap in Hessian eigenvalue distribution aroundthe number of classes c in most cases, where c = 10 in our case. A popular theory to explain thegap is the class / logit clustering of the logit gradients (Fort & Ganguli, 2019; Papyan, 2019, 2020).
Figure 19: Top eigenvalues of E[M] along training trajectory. (fc1:LeNet5)30F Additional ExplanationsF.1 Outliers in Hessian EigenspectrumOne characteristic of Hessian that has been mentioned by many is the outliers in the spectrum ofeigenvalues. Sagun et al. (2018) suggests that there is a gap in Hessian eigenvalue distribution aroundthe number of classes c in most cases, where c = 10 in our case. A popular theory to explain thegap is the class / logit clustering of the logit gradients (Fort & Ganguli, 2019; Papyan, 2019, 2020).
Figure 20:	Logit clustering behavior of ∆ and Γ at initialization (fc1:T-2002)Gradient Clustering at Minima Currently our theory does not provide an explanation to the lowrank structure of Hessian at the minima. However we have observed that the class clustering of logitgradients does not universally apply to all models at the minima, even when the models have aroundc significant large eigenvalues. As shown in Fig. 21, the class clustering is very weak but there arestill around c significant large eigenvalues. We conjecture that the class clustering of logit gradientsmay be a sufficient but not necessary condition for the Hessian to be low rank at minima.
Figure 21:	Class clustering behavior of ∆ and Γ at minimum. (fc1:T-2002)F.2 Dominating Eigenvectors of Layer-wise Hessian are Low RankA natural corollary for the Kronecker factorization approximation of layer-wise Hessians is that theeigenvectors of the layer-wise Hessians are low rank. Let hi be the i-th eigenvector of a layer-wiseHessian. The rank of Mat(hi) can be considered as an indicator of the complexity of the eigenvector.
Figure 22: Ratio between top singular value and Frobenius norm of matricized dominatingeigenvectors. (LeNet5 on CIFAR10). The horizontal axes denote the index i of eigenvector hi, andthe vertical axes denote k Mat(hi)k/k Mat(hi)kF .
Figure 23:	Top eigenspace overlap for the final fully connected layer.
Figure 24:	Eigenspace overlap, eigenspectrum, and cropped (upper 20 × 20 block)eigenvector correspondence matrices for fc2:F-2002 (MNIST)As shown in Fig. 24b, the second eigenvalue of the auto correlation E[xxT] is as large as approx-imately 1/10 of the first eigenvalue. With the output Hessian have c - 1 = 9 significant largeeigenvalues as described in , it has τ10γ1 < τ1γ2. Thus through the Kronecker factorization ap-ʌProXimation, the top m dimensional eigenspace is no longer simply Im 0 E[x], but a subset of topeigenvectors of the output Hessian Kroneckered with a subset of top eigenvectors of E[xxT] asreflected in Fig. 24d. This “miXture” of Kronecker product is moreover verified in Fig. 24c.
Figure 25:	Eigenspace overlap, eigenspectrum, and cropped (upper 20 × 20 block)eigenvector correspondence matrices for conv5:VGG11-W200 (CIFAR10)35We then consider the second phenomenon (delayed peak) and take conv2:VGG11-W200 (CIFAR10)in as an example. Here Fig. 26a is identical to Fig. 13a, which has the overlap peak later than theoutput dimension 200. In this case, the second eigenvalue of the auto correlation matrix is still notnegligible compared to the top eigenvalue. What differentiate this case from the first phenomenon isthat the eigenvalues of the output HeSSian no longer has a significant peak - instead it has a heavy tailwhich is necessary for high overlap.
Figure 26:	Eigenspace overlap, eigenspectrum, and cropped (upper 50 × 50 block)eigenvector correspondence matrices for conv2:VGG11-W200 (CIFAR10)Since the full correspondence matrices are too large to be visualized, we plotted their first rows upto 400 dimensions in Fig. 26e and Fig. 26f, in which each dot represents the average of correlationʌwith E[x] for the 10 eigenvector nearby. From these figures it is straightforward to see the gradualʌdecreasing correlation with E[x].
Figure 27:	EigenSpectrum and Eigenvector correSpondence matriceS with E[xxT] for LeNet5-BN.
Figure 28:	EigenSpace overlap of different modelS of LeNet5-BN.
Figure 29:	Comparison between the true and approximated layer-wise Hessians for LeNet5-BN.
Figure 30: Optimized posterior variance, s. (fc1:T-2002, trained on MNIST), the horizontal axis isordered with decreasing eigenvalues.
