Figure 1: A visualization of our algorithm in thecase of two interpretable directions n1 and n2 .
Figure 2: Modularity and MIG scores (higher is better) obtained for various encoders and datasetstrained via the two-stage procedure as described in Section 3 for StyleGAN 2. We observe that a)average results are on par or outperform most of the VAE-based models (Locatello et al., 2019b) b),on the other hand, for many methods, our approach provides smaller variance; the variance is due torandom seeds in generators and encoders, see Section 3.
Figure 3: Comparison of our MIG scores on Cars3D with various VAE based approaches; numbersare provided by the authors of Locatello et al. (2019b). Total denotes the entire distribution of VAEscores With respect to the hyperparameter/seed space; For Best mean for each VAE-based method,we select a single best hyperparameter according to the mean (over random seeds) MIG value andplot the distribution only With respect to seeds. For (c), We plot the distributions of MIG over allconfigurations (method, seed, hyperparameters).
Figure 4: (Left) An example of the abstract reasoning task. The goal of the learner is to correctlychoose the correct answer (marked with green in this example) from the answer panel, given thecontext panel. (Right) Accuracy obtained by training WReN with the (frozen) encoders obtainedusing one of the discussed methods. In most of the cases, we reliably obtain a sufficiently highaccuracy value.
Figure 5: Distribution of unfairness scores (the lower, the better); we can observe that the scores arerelatively low despite different hyperparameters and random seed setups.
Figure 6: Latent space traversal for 3D Shapes. We observe that all directions are almost perfectlydisentangled, except for shape (6th row) and scale (5th row).
Figure 8: Latent space traversal for MPI3D. We observe that samples are of excellent visual quality,and found directions are reasonably disentangled, except for shape and scale (5th row), and cameraheight and shape (1st row).
Figure 9: Latent space traversals for Isaac3D (selected directions). It appears that texture varia-tions, e.g., lighting, color, shape are easier to be inferred than ‘physical’ directions, such as the robothandle rotations.
Figure 10: Mutual information matrices for various methods and datasets obtained for the modelwith highest overal MIG score; a higher value indicates stronger mutual information.
Figure 11: Mutual information matrix for the best method on the Isaac3D dataset.
