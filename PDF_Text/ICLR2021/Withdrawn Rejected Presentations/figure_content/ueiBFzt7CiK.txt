Figure 1: DAD has 3 components: (1) augment problem instance graph With cheap global information; (2)learn graph neural networks with augmented graphs; (3) explain the learned GNN with a graph explainer model.
Figure 2:	Test approximation ratio for different algorithms. Foreground color (darker) shows theaverage test ratio, while background color (lighter) shows the maximum test ratio.
Figure 3:	Test approximation ratio for GNNs with other types of global information.
Figure 4:	Time and approximation ratio trade-off plot for different methods.
Figure 5: Greedy-like algorithm behavior on some nodes in MVC(left) and MDS(right) tasks. Se-lected nodes and edges are colored blue, Where the target node for explanation is colored red.
Figure 6: Anchor nodes v.s. node selection probability. From left to right: MVC, MDS, Max-Cut.
Figure 7: Algorithm selectioning algorithm, i.e., Greedy. From this we can learn two principles in instantiating our framework:1) better algorithm solution generally introduces better global information; 2) having diverse algo-rithmic solutions is better than multiple similar ones.
Figure 8: Test performance comparison with other classical algorithms on Erdos Renyi graphs.
Figure 9:	Test approximation ratio for GNNs with other types of global information.
Figure 10:	Extrapolation results with supervised learning, compared with the best polynomial base-line (greedy).
Figure 11:	Extrapolation results with unsupervised learning, compared with the best polynomialbaseline (greedy).
Figure 12:	Ablation studies on synthetic graphs. ER stands for Erdos Renyi random graphs, and BAstands for Barabasi Albert graphs.
Figure 13: Test approximation ratio by padding baseline features into same dimension.
Figure 14: Time and approximation ratio trade-off plot for different methods.
Figure 15: Comparing explanation results with GNNExplainer using different sparsity regularizationcoefficients.
