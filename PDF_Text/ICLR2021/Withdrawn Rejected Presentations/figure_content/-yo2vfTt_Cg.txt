Figure 1: Top: Iterates and implicit trust regions of GD and Adagrad on quadratic objectives With differentcondition number κ. Bottom: Average log suboptimality over iterations as well as 90% confidence intervals of30 runs with random initializationcenter). Particularly, once the method has reached the bottom of the valley, it struggles to makeprogress along the horizontal axis. Here is precisely where the advantage of adaptive stepsize methodscomes into play. As illustrated by the dashed lines, Adagrad,s search space is damped along thedirection of high curvature (vertical axis) and elongated along the low curvature direction (horizontalaxis). This allows the method to move further horizontally early on to enter the valley with a smallerdistance to the optimizer W along the low curvature direction which accelerates convergence.
Figure 2: Diagonal mass of neural network Hessian 6h relative to δffi[∣w∣] and 匹㈣口国卜口 of correspondingdimensionality for random inputs as well as at random initialization, middle and after reaching 90% trainingaccuracy with RMSProp on CIFAR-10. Mean and 95% confidence interval over 10 independent runs.
Figure 3: Mean and 95% confidence interval of 10 runs. Green dotted line indicates 99% training accuracy.
