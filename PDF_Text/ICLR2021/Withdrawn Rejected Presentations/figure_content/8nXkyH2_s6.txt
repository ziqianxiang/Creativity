Figure 1: t-SNE visualization of theneural codes of MNIST generated by aone-hidden-layer MLP with width 100.
Figure 2: (a) Plots of redundancy ratio as a function of the layer width of MLPs on both training set(blue) and test set (red). (b) Plots of test accuracies of K-Means (blue), K-NN (red), and logisticregression (LR, range) as functions of the layer widths of MLPs. (c) Plots of the average stochas-tic activation diameter (D) as a function of the layer width of MLPs on MNIST. (d) Histograms ofstochastic diameters (D) calculated on MNIST for an MLP of width 50 trained on MNIST for 10epochs (blue) and 500 epochs (red), respectively. (e) Histograms of stochastic diameters (D) calcu-lated on MNIST (blue) and randomly generated data with the same dimension (red), respectively,for an MLP of width 50 trained on MNIST. The two red histograms are identical. (f) Plots of redun-dancy ratio (R) calculated on MNIST (“True data”, solid lines) and randomly generated data (dottedlines) as functions of training time for MLPs of widths 40 (blue), 50 (red), and 60 (orange). Thedotted lines show networks trained on unaltered data, evaluated with random data. The models aretrained for 5 times on MNIST and 10 times on CIFAR-10 with different random seeds. The darkerlines show the average over seeds and the shaded area shows the standard deviations.
Figure 3: (a) Redundancy ratio (R ratio) as a function of training time on MNIST. (b) Redundancyratio (R ratio) as a function of training time on CIFAR-10. (c) Test accuracy of K-Means (left),K-NN (middle), and logistic regression (right) as functions of training time. The models are MLPsof widths 10 (blue), 20 (red), 40 (orange), and 80 (green). The dotted lines show networks trainedon unaltered data, evaluated with random data. The darker lines show the average over seeds andthe shaded area shows the standard deviations.
Figure 4: (a) Redundancy ratios (R ratios) on training set (dotted lines) and test set (solid lines) ofCIFAR-10 as functions of sample size. (b) Test accuracy of K-Means (left), K-NN (middle), andlogistic regression (right) as functions of sample size. The models are MLPs of widths 10 (blue),20 (red), 40 (orange), and 80 (green). All models are trained on CIFAR-10 for classification for 10times with different random seeds. The darker lines show the average over seeds and the shadedarea shows the standard deviations.
Figure 5: (a) Plots of redundancy ratio of neural codes formed by different single layers of MLPstrained on CIFAR-10 as a function of training time. (b) Test accuracy of K-Means (left), K-NN(middle), and logistic regression (right) as functions of training time. (c) Redundancy ratios ofneural codes formed by multiple layers of MLPs as functions of sample size. (d) Test accuracy ofK-Means (left), K-NN (middle), and logistic regression (right) as functions of sample size. Themodels are MLPs of width 40 on CIFAR-10.
Figure 6: (a) Scatter of redundancy ratios of MLPs trained on MNIST with (w/, y-axis) or without(w/o, x-asix) batch normalization (BN, left), gradient clipping (middle), or weight decay (right). (b)Scatter of logistic regression (LR) accuracy of MLPs trained on MNIST with (w/, y-axis) or without(w/o, x-asix) batch normalization (BN, left), gradient clipping (middle), or weight decay (right).
Figure 7: (a) Histograms of diameters of stochastic diameters calculated on MNIST for an MLP ofwidth 60 trained on MNIST for 10 epochs (red) and 500 epochs (blue), respectively. (b) Histogramsof stochastic diameters calculated on MNIST (blue) and randomly generated data with the same di-mension (red), respectively. The model is an MLP of width 60 trained on MNIST. (c) Histograms ofdiameters of stochastic diameters calculated on MNIST for an MLP of width 70 trained on MNISTfor 10 epochs (red) and 500 epochs (blue), respectively. (d) Histograms of stochastic diameters cal-culated on MNIST (blue) and randomly generated data with the same dimension (red), respectively,The model is an MLP of width 70 trained on MNIST.
Figure 8: (a) Redundancy ratio of MNIST as a function of training time. (b) Test accuracy ofK-Means (left), K-NN (middle), and logistic regression (right) as functions of training time. Themodels are MLPs of depths 40 (blue), 50 (red), and 60 (orange) on MNIST. The dotted lines shownetworks trained on unaltered data, evaluated with random data. All models are trained on MNISTfor classification for 5 times with different random seeds. The darker lines show the average overseeds and the shaded area shows the standard deviations.
Figure 9: (a) Redundancy ratios on training set (dotted lines) and test set (solid lines) of MNISTas functions of sample size. (b) Test accuracy of K-Means (left), K-NN (middle), and logisticregression (right) as functions of sample size. The models are MLPs of widths 40 (blue), 50 (red),and 60 (orange) on MNIST. All models are trained on MNIST for classification for 5 times withdifferent random seeds. The darker lines show the average over seeds and the shaded area shows thestandard deviations.
Figure 10: First row: Scatter of redundancy ratios and test accuracy of K-Means (blue), K-NN(violet), and logistic regression (LR, orange) for MLPs of depth from 3 to 100 with batch normal-ization (y-axis) and without gradient clipping (x-asix). We Perfer a smaller y - X in redundancyration, and larger ones in the test accuracies of K-Means, K-NN, and logistic regression. Totally,115 models are involved in one scatter. Second row: Scatter of redundancy ratios and test accuracyof K-Means (blue), K-NN (violet), and logistic regression (LR, orange) for MLPs of depth from 3to 100 with gradient clipping (y-axis) and without gradient clipping (x-asix). We perfer a smallery - X in redundancy ration, and larger ones in the test accuracies of K-means, K-Nn, and logisticregression. Totally, 115 models are involved in one scatter. Third row: Scatter of redundancy ratiosand test accuracy of K-Means (blue), K-NN (violet), and logistic regression (LR, orange) for MLPsof depth from 3 to 100 with weight decay (y-axis) and without weight decay (X-asix). We perfer asmaller y - X in redundancy ration, and larger ones in the test accuracies of K-Means, K-Nn, andlogistic regression. Totally, 115 models are involved in one scatter.
Figure 11: An example ofrandom dataTable 5: Training accuracy and loss on training random data withone-hidden-layer MLPsEpoch	0	100	300	500Training acc (%)	10.92	11.24	11.24	11.24Loss	230.56	230.13	230.13	230.13B.6 Additional results for random labelThe following figures show the impacts of random label on redundancy ratio and the test accuracyof K-Means, K -NN, and logistic regression. Please refer to Section 5.5.
Figure 12: (a) Redundancy ratios of MNIST as functions of layer width in different label noises.
