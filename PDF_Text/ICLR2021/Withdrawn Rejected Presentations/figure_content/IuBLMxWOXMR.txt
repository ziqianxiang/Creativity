Figure 1: Overview of the proposed framework for simultaneous training of depth-from-defocus and depth-from-focus. Besides depth estimation from two approaches, all-in-focus image is also generated from thenetwork with similar effect as light field cameras.
Figure 2: The thin lens model.
Figure 3: The DefocusNet and FocusNet structures.
Figure 5: Visual comparison between our method (trained on the synthetic dataset) and other recent single-viewdepth estimation methods (fine-tuned on the synthetic dataset). Left to right: Input defocus image at the focalplane of 1 meter; Ground truth depth map; Predicted depth map from Laina et al. (2016); Predicted depth mapfrom Alhashim & Wonka (2018); Our output from DefocusNet. Brighter color represents a farther distance.
Figure 6: Visual performance of our Focus-Net depth estimation method. From left to right: focal stack atdifferent focal plane: 1m, 3m, 5m, 9m, and estimated depth map from Focus-Net.
Figure 7: Further verification on realistic scenesfrom DSLR dataset. First column: Input real de-focus images; Second column: Predicted depthmaps from DefocusNet. Our model can alsowork well in real scenes with real defocus blur.
Figure 8: Depth estimation from our collected dataset with 1m and 5m focus. First column: raw in-focusimages; Second column: images focusing on 1m; Third column: estimated depth for images focusing at 1m;Fourth column: images focusing at 5m; Fifth column: estimated depth for images focusing at 5m.
Figure 9: Percentage of mean pixel errors in differentdistance range.
Figure 10: Visual result of our proposed hyper-spectral fusion to get all-clear image at different focal planes.
