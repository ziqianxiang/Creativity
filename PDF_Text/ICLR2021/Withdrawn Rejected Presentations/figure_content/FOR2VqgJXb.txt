Figure 1: Each measure for evaluating representation quality is a simple function of the “loss-data”curve shown here, which plots validation loss of a probe against evaluation dataset size. Left:Validation accuracy (VA), mutual information (MI), and minimum description length (MDL) measureproperties of a given dataset, with VA measuring the loss at a finite amount of data, MI measuringit at infinity, and MDL integrating it from zero to n. This dependence on dataset size can lead tomisleading conclusions as the amount of available data changes. Middle: Our proposed methodsinstead measure the complexity of learning a predictor with a particular loss tolerance. ε samplecomplexity (εSC) measures the number of samples required to reach that loss tolerance, while surplusdescription length (SDL) integrates the surplus loss incurred above that tolerance. Neither dependson the dataset size. Right: A simple example task which illustrates the issue. One representation,which consists of noisy labels, allows quick learning, while the other supports low loss in the limit ofdata. Evaluating either representation at a particular dataset size risks drawing the wrong conclusion.
Figure 2: Results using three representations on the MNIST dataset.
Figure 3: Results using three representations on a part of speech classification task.
