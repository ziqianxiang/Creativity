Figure 1: [Left] A typical Transformer architecture and its elements. [Right]Runtime/parameter-count breakdown of Transformers. At smaller context lengths, the feed-forward neural network is the time-dominant operation. As context length increases, self-attentionbecomes the time-dominant operation. In both cases, ATTN and FFN blocks together account for>75% of total parameters, and >85% of the total runtime on an NVIDIA GTX 1080 Ti GPU.
Figure 2: Illustration of the Transformer Approximation Methodology. Sign Matching is illus-trated with K=1oφUONσnτ-50≡σCreating TransEIement Queue,Q[encompassing_element,granularity]Speed Focus:	Size Focus: Lower layers more important: L1 ≥ L2T1 ≥ T2...	p1 > p2...	Top layers more important: L1 ≤ L2...
Figure 3: Tuning the SA Approximation Knobs with Sizeand Speed Focus. The average GLUE scores across the 9tasks using Q8BERT are reported for different acceptableaccuracy loss levels.
Figure 4: Illustration of pruning techniques used in FFN Blocks. ’A’ denotes activations and ’W’denotes weights, transposed for clarity. When optimizing for accuracy/size, unstructured pruning isused. When optimizing for speed, our greedy shrinking method is used to prune weight groups, sothat the remaining weight groups form a contiguous dense block.
Figure 5: Illustration of pruning techniques used in ATTN Blocks. ’A’ denotes activations and’W’ denotes weights, transposed for clarity. Pruning heads in the second step is illustrated this wayfor the sake of clarity. In the actual implementation, heads are pruned by pruning the correspondingweight groups along the y dimension of WQ/K/V in step 1. Greedy shrinking is not used whenoptimizing for speed for pruning heads.
Figure 6: Gains from different optimization techniques using Speed Focus.
Figure 7: Distribution of unimportant blocks across (a) downstream tasks and (b) Transform-ers.
