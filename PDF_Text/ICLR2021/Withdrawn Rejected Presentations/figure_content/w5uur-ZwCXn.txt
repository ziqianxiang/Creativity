Figure 1: Training flow diagram of XLA. After training the base task models θ(1) , θ(2) , and θ(3) onsource labeled data Ds (WarmUp), we use two of them (θ(j), θ(k)) to pseudo-label and co-distillthe unlabeled target language data (Dt0). A pretrained LM (Gen-LM) is used to generate new (vicinal)training samples for both source and target languages, which are also then pseudo-labeled and co-distilled using the two task models (θ(j), θ(k)) to generate Ds and DDt. The third model θ(l) is thenprogressively trained on these datasets: {Ds, D0} in epoch 1, Dt in epoch 2, and all in epoch 3.
Figure 2: Validation F1 results in XNER formulti-epoch co-teaching training of XLA.
Figure 3: Effect of training with confidence penalty in the warm-up step on target (Spanish) lan-guage XNER classification using t-SNE plots. From the visualization, it can be seen that the modeltrained with confidence penalty shows better inter-class separation which exhibits robustness of themultilingual model.
Figure 4: Histogram of loss distribution on target (Spanish) language XNER classification.
Figure 5: Scatter plot of loss distribution on target (Spanish) language XNER classification.
Figure 6: Distribution of selected sentence lengths on target (Spanish) language XNER classification.
