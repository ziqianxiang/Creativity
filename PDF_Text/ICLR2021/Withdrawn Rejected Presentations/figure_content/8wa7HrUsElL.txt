Figure 1: Agents estimate the price of anarchy assuming the joint strategy space, X, of the game isrestricted to a local linear region, Xτ , extending from the currently learned joint strategy, xt , to thenext, xt+1. ρ and ρx denote the global and local price of anarchy.
Figure 2: (a) Four drivers aim to minimize commute time from S to E. Commute time on eachedge depends on the number of commuters, nij . Without edge AB, drivers distribute evenly acrossSAE and SBE for a 65 min commute. After edge AB is added, switching to the shortcut, SABE,always decreases commute time given the other drivers maintain their routes, however, all drivers areincentivized to take the shortcut resulting in an 80 min commute. (b) The price of anarchy throughouttraining for 1000 randomly generated three road networks exhibiting Braess’s paradox. Gradientdescent and D3C are compared with shaded regions representing ± 1 standard devation.
Figure 3: Traffic Network (a) Without edge AB, agents are initialized with random strategies andtrain with either gradient descent (left) or D3C (right)—similar performance expected. Statisticsof 1000 runs are plotted over training. Median ρmax tracks the median over trials longest-commuteamong the four drivers. The shaded region captures ± 1 standard deviation around the mean. (b)After edge AB is added, agents are initialized with random strategies and trained with either gradientdescent (left) or D3C (right). Statistics of 1000 runs are plotted over training. Median ρmax tracksthe median over trials of the longest-commute among the four drivers. The shaded region captures ±1 standard deviation around the mean.
Figure 4: Coin Dilemma (a) Mean total return over ten training runs for agents. Mean return over allepochs is next to each method name in the legend. D3C hyperparameters were selected using fiveindependent validation runs. Cooperative agents trained to maximize total return represent the bestpossible baseline. Shaded region captures ± 1 standard deviation around the mean. (b) One trainingrun (Ai0i = 0.9): sum of agent returns (left); % of coins picked up that were the agent’s type (middle);relative reward attention measured as ln((n - 1)Aii) - ln( j6i Aji) (right).
Figure 5: (a) Mean total return over ten training runs for agents. D3C hyperparameters were selectedusing five independent validation runs. Cooperative agents trained to maximize total return representthe best possible baseline. Shaded region captures ± 1 standard deviation around the mean. (b) Threerandomly selected runs. Each curve shows the mean return up to the current epoch for 1 of 5 agents.
Figure 6: Average relative reward attention (RRA) over 10 runs for five agents in Cleanup (a) andHarvest Patch (b) and two agents in Mini-Cleanup (c).
