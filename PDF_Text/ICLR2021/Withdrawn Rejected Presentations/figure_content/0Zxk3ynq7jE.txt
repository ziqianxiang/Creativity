Figure 1: We motivate open-set recognition with safety concerns in autonomous systems. Left: State-of-the-art semantic segmentation networks (Wang et al., 2019) do not model “strollers”, which are outsidethe K closed-set categories in Cityscapes benchmark (Cordts et al., 2016). Here, the network misclassifiesthe “stroller” as a “motorcycle”，which can be a critical mistake when fed into an autonomy stackbecause the two objects exhibit different behaviours (and so require different plans for obstacle avoidance).
Figure 2: Flowchart for extracting off-the-shelf (OTS) features used for open-set recognition. We determinethe appropriate feature processing steps on validation set, including spatial pooling (sp) and L2 normalization(L2). Left: for open-set image recognition, we extract OTS features at the last convolution layer of a K-wayclassification network. Right: for open-set semantic segmentation, we extract OTS features from the “pyramidhead” module, which has sufficiently captured multi-scale information. We do not adopt spatial pooling andinstead use the per-pixel features to represent pixels. Note that, different from our practice, many other methodslike OpenMax (Bendale & Boult, 2016) and generative methods (Grathwohl et al., 2019) work on logit features,which are too invariant to be effective for open-set recognition (cf. Figure 3 and Table 2).
Figure 3: tSNE plots (Maaten & Hinton, 2008)of open-vs-closed data, as encoded by differ-ent features from a Res50 model (trained withpre-training in the closed world, cf. Table 2).
Figure 4: Two random images from Cityscapes-val, visualized with ground-truth and the predicted semanticsegmentation maps by the HRNet. We visualize open-world pixels in the ground-truth (white regions), as wellas predicted open-world pixels for a standard baseline (MSP) and our method (GMM). MSP tends to predictsegment boundaries as open-pixels, while our GMM tends to find open-set objects (street-shop and rollator aspointed by the red arrows).
Figure 5: Performance of CLS versus amount of (open) training data.
Figure 6: AUROC vs. memory cost (MB) for various statisticalmodels for open-set semantic segmentation. NN stores ~100k OTSfeatures, which is larger than the underlying network (HRNet). Weexplore GMMs with various covariance structures (spherical, diago-nal, full), feature dimensionality via PCA, and mixture components.
Figure 7: Study of open-set performance versus feature dimension reduced by PCA. We study this withthe experiment of open-set image classification, where TinyImageNet/Cityscapes are the closed/open sets.
Figure 8: Open-set performance w.r.t memory cost (MB) by different models. Memory cost means the spacerequired to store the parameters in these models. Left: open-set (200-way) image recognition for TinyImageNet-Vs-Cityscapes. Right: open-set semantic segmentation (19 classes) on the Cityscapes. NN memorizes trainingexamples for open-world recognition, and hence it consumes huge memory to store OTS features of trainingexamples (more memory consumption than the underlying SOTA models). Compared with the underlyingnetworks, GMM-spherical and k-means induce negligible computation cost. They also perform considerablybetter than NN and GMM-full/diag on open-set image classification, but not as well as NN and GMM-full onopen-world semantic segmentation. These plots clearly serVe as guidelines to choose the appropriate statisticalmodels on specific tasks. Note that the Validation performance shown here can be nicedly translated to test sets,as detailed in Figure 10 .
Figure 9: Visualization of per-class Gaussian means with medoid images (whose features are closest tothe means within their corresponding classes). As comparison, we show some random images sorted bytheir cosine similarity to the per-class Gaussian mean. We can see the medoid images capture “canonical”objects representing the corresponding classes, e.g., those with “standard” shape and cleaner background. Thisvisualization suggests our statistical models are quite interpretable.
Figure 10: Detailed results of (left) open-set image recognition using Cityscapes images as cross-datasetopen-world examples under Setup-II) and (right) open-set semantic segmentation (Setup-III). In each cell, thefirst and second (if existing) row numbers denote AUROC performance on the val and test sets, respectively. Wehighlight the best performance on the val set, on which we tune the hyper-parameter and report the performanceon the test set. As for notation, gGMM means we learn GMM “globally” on the whole closed train-set, agnosticto class labels; while cGMM means that we learn class-conditional GMMs. For open-set semantic segmentation,we only train GMM in a class-conditional fashion (i.e., cGMM), because using pixel features from all classesto train a global GMM is prohibitively time-consuming. “Raw feat.” means the feature we extract from thelast convolution layer without L2-normalization, and “w/ L2” means we L2-normalize the extracted features.
