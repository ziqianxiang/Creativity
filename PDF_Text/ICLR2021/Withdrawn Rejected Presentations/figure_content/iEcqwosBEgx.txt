Figure 1: The comparison of the standard policy gradient method without novelty seeking (left),multi-objective optimization method (middle), and our constrained optimization approach (right)for novel policy seeking. The standard policy gradient method does not try actively to find novelsolutions. The multi-objective optimization method may impede the learning procedure when thenovelty gradient is being applied all the time (Zhang et al., 2019), e.g., a random initialized policywill be penalized from getting closer to the previous policy due to the conflict of gradients, whichlimits the learning efficiency and the final performance. On the contrary, the novelty gradient of ourconstrained optimization approach will only be considered within a certain region to keep the policybeing optimized away from highly similar solutions. Such an approach is more flexible and includesthe multi-objective optimization method as its special case.
Figure 2: The performance and novelty comparison of different methods in Hopper-v3, Walker2d-v3and HalfCheetah-v3 environments. The value of novelty is normalized to relative novelty by regardingthe averaged novelty of PPO policies as the baseline. The results are from 10 policies of each method,with the points show their mean and lines show their standard deviation.
Figure 3: The performance under different novelty thresholds in the Hopper, Walker and HalfCheetahenvironments. The results are collected from 10 learned policies based on PPO. The box extendsfrom the lower to upper quartile values of the data, with a line at the median. The whiskers extendfrom the box to show the range of the data. Flier points are those past the end of the whiskers.
Figure 4: (Left): performance comparison in terms of novelty and task performance of PPO, WSRand IPD in the Ant-v3 environment. We also include the work of (Hong et al., 2018) as a comparison.
Figure 5: Experimental results on the Four Reward Maze Problem. We generate 5 policies withdifferent novelty seeking methods, and use the PPO with different random seeds as baseline. In eachfigure, the 5 lines indicate 5 trajectories when the game is started from the right hand side. It worthnoting that the results of WSR, CTNB and IPD are associated with the parameters of weights orthreshold. We set the weight parameter in WSR as 10 to make the two reward terms comparable, andset the thresholds in CTNB and IPD as the averaged novelty between policies trained with PPO. Allpolicies are trained with 6.1 Ã— 103 episodes.
Figure 6: The visualization of policy behaviors of agents trained by our method in Hopper-v3environment. Agents learn to jump with different strides.
Figure 7: The visualization of policy behaviors of agents trained by PPO in Hopper-v3 environment.
Figure 8: The visualization of policy behaviors of agents trained by our method in Walker2d-v3environment. Instead of bouncing at the ground using both legs, our agents learns to use both legs tostep forward.
Figure 9: The visualization of policy behaviors of agents trained by PPO in Walker2d-v3 environment.
Figure 10: The visualization of policy behaviors of agents trained by our method in HalfCheetah-v3environment. Our agents run much faster compared to PPO agents and at the mean time severalpatterns of motion have emerged.
Figure 11: The visualization of policy behaviors of agents trained by PPO in HalfCheetah-v3environment. Since we only draw fixed number of frames in each line, in the limited time steps thePPO agents can not run enough distance to leave the range of our drawing, which shows that ouragents run much faster.
