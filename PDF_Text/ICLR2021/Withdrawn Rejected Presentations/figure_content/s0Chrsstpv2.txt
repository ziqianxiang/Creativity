Figure 1: a) PCA based visualization of a part of the COMPAS dataset. The blue points show theoriginal instances, and the red points represent instances generated with the perturbation samplingused in the LIME method. The distributions are notably different. b) The idea of the attack onexplanation methods based on the difference of distributions. The attackerâ€™s adversarial model con-tains both the biased and unbiased model. The decision function that is part of the cheating modeldecides if the instance is outside the original distribution (i.e. used only for explanation) or an actualinstance. If the case of an actual instance, the result of the adversarial model is equal to the result ofthe biased model, otherwise it is equal to the result of the unbiased model.
Figure 2: The robustness results for gLIME (top), gSHAP (middle), and gIME (bottom). The graphsshow the proportion of evaluation set instances, where the sensitive feature was recognized as themost important by the used explanation method. Rows represent the generators used for explana-tions. The column labels consist of the name of the data set on which the experiment was performedand the name of the generator used for training of the adversarial model. Compas2 and CC2 de-note an attack with two independent features. Perturbation represents the original sampling used inLIME, SHAP, and IME, TEnsFillIn represents the TreeEnsemble variant where new instances aregenerated around the given one, and TreeEns represents the generation from the whole distribution.
Figure 3: Visual comparison of original and sampled distributions for the COMPASS dataset. TheSHAP k-means based generator (left) produces instances less similar to the original data, comparedto the MCD-VAE generator (right).
