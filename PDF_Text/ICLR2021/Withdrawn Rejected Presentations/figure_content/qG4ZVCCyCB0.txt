Figure 1: Panel (a) presents the optimal AsymMSE(wb 0tr,-Ttr) (blue) in Theorem 9 via grid search, and theoptimal AsymMSE(wb 0tr,-Tval) in Corollary 8 with n1 = 0 (orange) and n1 = 5 (green), as well as the upperbound of AsymMSE(wb 0tr,-Ttr) (magenta) in Corollary 4. The optimal AsymMSE(wb 0{,tTr-tr,tr-val}) are used asreference curves in plots (b) and (c). Panel (b) plots the '2-error of W0tT-tr,tr-val} as the total number of tasksincreases from 20 to 1000 with an increment of 20. We fix data dimension d = 60 and per-task sample sizen = 20. For the train-val method, we experiment on n1 = 0 and n1 = 5. Panel (c) shows the scaled (by T)'2-error of W0tTtr,tr-val} as the ratio d/n varies from 0 to 3 (n = 20 and T = 1000 are fixed).
Figure 2: The scaled (by T) '2-error of W0tTT-tr,tr-val,cv} as the ratio d/n varies from 0 to 3 (n = 20and T = 1000 are fixed). For the cross-validation method, the regularization coefficient λ = 0.5 istuned.
Figure 3: The scaled (by T) '2-error of Wb0tT-tr,cv} as the ratio d/n varies from 0 to 3 (n ∈ {40,200}and T = 1000 are fixed). For the cross-validation method, the regularization coefficient λ = 0.5.
Figure 4: The scaled (by T) '2 -error of Wb0tTtr,tr-val} as the ratio d/n varies from 0 to 3 in the generalscaling setting (n = 100 and T = 300 are fixed). The regularization coefficient λ is fine tuned forthe train-train method and λ = 2000 for the train-val method.
