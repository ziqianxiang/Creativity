Figure 1: A sliding window of the gradientdivergence (defined in Appendix D), on Ci-far10 in the setup of Section 4 for FedAvgand FedMix (K = 4).
Figure 2: FedMix graphical model. Thegenerative model is depicted with solid linesand the inference model with dashed lines.
Figure 3: The effect of specialisation (with-out H(q)) compared to an ensemble (withH(q)) and FedAvg on Cifar10. The experi-mental setup is identical to what is describedin Section 4.
Figure 4: Average accuracy across all clients (y-axis) as a function of the amount of GB commu-nicated (top row) and as a function of communication rounds (bottom row). Cifar 10 models aretrained on the standard 45k training split. x-axes have been truncated for improved visibility. Bestviewed in color.
Figure 5: Only rotationFigure 6: Rotation and labels7Under review as a conference paper at ICLR 2021Table 1: Average test-set accuracies across clients and communication costs (rounds and GB) forCifar10, Cifar100 and Femnist. For Cifar10 we report the performance after 2k rounds, for Cifar100we report the average over the last 10 evaluations at the end of training and for Femnist we reportthe average over the last 2k communication rounds.
Figure 6: Rotation and labels7Under review as a conference paper at ICLR 2021Table 1: Average test-set accuracies across clients and communication costs (rounds and GB) forCifar10, Cifar100 and Femnist. For Cifar10 we report the performance after 2k rounds, for Cifar100we report the average over the last 10 evaluations at the end of training and for Femnist we reportthe average over the last 2k communication rounds.
Figure 7: Ground truthand q(z|s) after 10rounds. Greyscale rep-resents probabilities(white: 0; black: 1)8Under review as a conference paper at ICLR 2021ReferencesKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, VladimirIvanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, H Brendan McMahan, et al. Towardsfederated learning at scale: System design. arXiv preprint arXiv:1902.01046, 2019.
Figure 8: q(z|s) at convergence for different number of experts K. Greyscale corresponds to prob-abilities (white: 0; black: 1)C Privacy ImplicationsPrivacy is one of the key motivations for research and deployment of Federated Learning. Eventhough privacy is not a focus of this paper, we briefly discuss some implications of making explicituse of q(y|s) in FedMix. The update rule described in equation 8 requires access to the marginalq(z|s) = Ey p(y ∣s)qφ(z∣y) at the server. At the same time, the server has access to the parametersφ that were used in computing p(z|s) before being sent to the server. Therefore, in principle, itcould solve q(z|s) = Ey p(y∣s)qφ(z|y) with respect to p(y|s) and thus obtain the marginal labeldistribution at the client. In practice this is not as straightforward to do as (a) the probability matrixqφ(z|y) is not always invertible and (b) solutions that use the pseudo-inverse, empirically, are notvery accurate in capturing the entire distribution. With the additional constraints that the marginalneeds to sum to one contains only positive elements and that NS ∙ p(y|s) ∈ Z, in some cases, areconstruction can become possible. As the number of classes exceeds the number of experts, thisreconstruction becomes more unlikely. We leave a thorough characterization of these properties tofuture work.
Figure 9: Histogram representation of p(y|s) as well as its server-side reconstruction for every clients for the Cifar10 setup described in Appendix A. The x-axis per sub-plot enumerates the 10 classes.
Figure 10: Histogram representation of p(y|s) as well as its server-side reconstruction for everyclient s for the Cifar10 setup described in Appendix A. The x-axis per sub-plot enumerates the 10classes. For every class c, the left bar in red represents p(y = c|s) and the right bar in blue representsits reconstruction. Each client performed multiple mini-batch update steps (on average 8).
Figure 11: Ablation studies on the effect of K on the average local accuracy (a) and new shardaccuracy (c) as well as on the influence on local (b) and new shard (d) accuracy of the threshold forpruning experts before communication for K = 4.
Figure 12: Accuracy on a new client (y-axis) as a function of the amount of GB communicated (toprow) and as a function of communication rounds (bottom row). Cifar 10 models are trained on thestandard 45k training split. x-axes have been truncated for improved visibility. Best viewed in color.
Figure 13: Average accuracy of models evaluated after E = 1 local epochs of fine-tuning0	500	1000	1500	2000Communication rounds17Under review as a conference paper at ICLR 2021H SpecializationIn Figure 14a We see how qφ(z|y) changes over time. The entropy of qφ(z|y = c), i.e. the weightwith which each expert is assigned to a specific label c decreases over time until each label is as-signed exactly one expert with probability 1. The original MoE formulation, however, quicklycollapses to a single expert, as can be see in Figure 14b. In the federated learning scenario, thiscorresponds to performing standard FedAvg on the single surviving expert.
Figure 14: Expert specialisationI FEDMIX with expert pruningThe FedMix algorithm (Algorithm 1) can be extended to ignore the communication of expertsbetween server and client if, for this given client, that particular expert is not updated during training.
