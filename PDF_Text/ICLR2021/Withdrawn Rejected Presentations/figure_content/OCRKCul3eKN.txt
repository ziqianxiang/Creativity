Figure 1: In online RL (left), the agent must interact with the environment to gather data to learn from. In offlineRL (right), the agent must learn from a logged dataset.
Figure 2: Two types of extrapolation error. Type A is most dangerous for offline RL, due to the max operation.
Figure 3: Bsuite Experiments: bsuite experi-mental results on two environments with respectto different levels of noise injected into the ac-tions in the dataset. The proposed method, BRr,outperforms all the baselines on cartpole. Meth-ods implementing a form of behavior constraining(BCQ, CQL and our methods BRr and QRr) excelon catch, stressing its importance.
Figure 4: Atari results: We compare our proposedQRr results against other recent State of Art offlineRL methods on the Atari offline policy selectiongames from RL Unplugged benchmark.
Figure 5: Ablations (online policy selectiongames): [Left] We compare behavior value esti-mation (B), DDQN and Monte Carlo approachesin offline RL Atari dataset. B and DDQN achievesimilar median episodic returns, but learning withMonte Carlo returns performs poorly. [Right]We show various ablation studies (in terms of us-ing regularization, reparametrization and behaviorvalue estimation). We found the most significantimprovement from the ranking regularization term.
Figure 6: Overestimation of Q-values with sub-sampled Atari datasets (% of Dataset). DDQNover-estimates the value of states severely whereasQr and B reduce over-estimation greatly. We reportmedian over-estimation error over online policyselection games on Atari.
Figure 7: DeepMind Lab Results: We compare the performance of different baselines on challenging DeepMindLab datasets coming from four different DeepMind Lab levels. Our method, BR, consistently performs the best.
Figure 8: Effect of coverage in the dataset: Wecompare offline RL models with varying the noiselevel in the environment. Increasing the noise levelincreases the coverage as well. BC performs wellwith low noise, however, BR performs significantlybetter as the noise increases. Let us note that, in allour experiments, R2D2 uses double Q-learning.
Figure 9: Robustness Experiments: (left) We compare DQN and B in terms of their robustness to the rewarddistribution on Atari online policy selection games. We split the datasets in two bins: the dataset that onlycontains transitions that are coming from episodes that have episodic return less than the mean episodic return inthe dataset (”Episodic Reward < Mean”), transitions coming from episodes with return higher than the meanreturn in the dataset (”Episodic Reward > Mean”). B performs better than DQN in both cases. (right) Normalizedscores of DQN and B on subsets of data from online policy selection games. B performs comparatively betterthan DQN . The Q-learning suffers more since the coverage of the dataset reduces with the subsampling whichcauses more severe extrapolation error.
Figure 10:	The Effect of increasing the ranking regularization on the action gap.
Figure 11:	The Effect of increasing the ranking regularization on the overestimation.
Figure 12: The Raw Returns obtained by each baseline on Atari online Policy Selection Games.
Figure 13: The value error computed in the environment by evaluating the agent and computed with respect tothe ground truth discounted returns. The negative values indicate under-estimation and positive values are forover-estimation.
Figure 14: The squared value error computed in the environment by evaluating the agent and computed withrespect to the ground truth discounted returns and reporting the mean squared values of the values.
Figure 15: DeepMind Lab Reward Distribution: We show the reward distributions for the DeepMind Labdatasets. The vertical red line indicates the average episodic return in the datasets.
