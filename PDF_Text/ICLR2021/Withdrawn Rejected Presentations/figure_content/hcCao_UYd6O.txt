Figure 1: Overview of the proposed AFD approach: (a) visual comparison of several adversarialrobustness methods (Adversarial training (Madry et al., 2017), TRADES (Zhang et al., 2019b), andAFD). The dotted black line corresponds to the decision boundary of the adversarial discriminator;(b) schematic of the proposed AFD paradigm.
Figure 2: Robust accuracy for different strengths of PGD-L∞ attack on different datasets.
Figure 3: Comparison of robust accuracy of different methods against white-box attacks on CIFAR10dataset with ResNet18 architecture.
Figure 4: (left) Comparison of normalized representation sensitivity on test-set of MNIST (top),CIFAR10 (middle), CIFAR100 (bottom) datasets under PGD-L∞ attack. Plots show the median(±std) sensitivity over test-set for each dataset. * denotes statistically significant difference betweensensitivity distributions for AFD and TRADES. (right) Logarithm of the average gradient magnitudesof class likelihoods with respect to input, evaluated at samples within the test-set of each dataset(log Ex~X (∂∂Xt))). For each matrix, rows correspond to ground truth (target) labels and columnscorrespond to non-target labels.
Figure A1: Comparison of robust accuracy of different methods against white-box attacks on MNISTdataset with ResNet18 architecture.
Figure A2: Comparison of robust accuracy of different methods against white-box attacks onCIFAR100 dataset with ResNet18 architecture.
Figure A3: Comparison of robust accuracy of AFD and representation matching against white-boxattacks on MNIST dataset with ResNet18 architecture.
Figure A4: Robust accuracy against various attacks at different training stages. First and second rowscorrespond to models trained on CIFAR10 and CIFAR100 respectively.
Figure A5: Classification accuracy of the adversarial discriminator Da at different training stages ondifferent datasets.
Figure A6: Robust accuracy of AFD-trained models on CIFAR10 dataset against various attackswhen using different levels of attack strength during training.
Figure A7: Feature visualization. Pixel values were changed in the direction of the gradients thatwould maximize either the ground truth class (left column) or a randomly selected class (rightcolumn). For each image, the original image (left), transformed image (middle) and gradient map(right) are shown.
Figure A8: Scatter plot of 2-dimensional t-SNE projection (Maaten & Hinton, 2008) of the rep-resentations derived from training the ResNet18 architecture on MNIST dataset. (top row) t-SNEprojection of representations of clean images for networks trained with different methods. Each pointcorresponds to the representation of one of the images from the MNIST test-set. (rows 2 to 5) t-SNEprojection of the representation of the clean and perturbed MNIST test-set images. Columns aresorted from left to right with the strength of the perturbation (left-most column corresponds to cleanimages and right-most column with highest tested perturbation). Perturbations are generated usingPGD-L∞ attack. NT: naturally trained; AT: adversarially trained(Madry et al., 2017); TRADES:(Zhang et al., 2019b); AFD: adversarial feature desensitization.
Figure A9: Scatter plot of 2-dimensional t-SNE projection (Maaten & Hinton, 2008) of the repre-sentations derived from training the ResNet5 architecture on CIFAR10 dataset. (top row) t-SNEprojection of representations of clean images for networks trained with different methods. Eachpoint corresponds to the embedding of one of the images from the CIFAR10 test-set. (rows 2 to 5)t-SNE projection of the embedding of the clean and perturbed CIFAR10 test-set images. Columnsare sorted from left to right with the strength of the perturbation (left-most column corresponds toclean images and right-most column with highest tested perturbation). NT: naturally trained; AT:adversarially trained(Madry et al., 2017); TRADES: (Zhang et al., 2019b);AFD: adversarial featuredesensitization.
