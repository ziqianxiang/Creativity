Figure 1: Example of Gaussian parametrization leading to a STFTwith high time resolution and poor frequency resolution (left) orSTFT with poor time resolution and high frequency resolution(middle) and finally the wavelet (or constant Q-transform) casewith adaptive time frequency resolution, for a chirplet transformconsider this last case with a non diagonal covariance matrix, withchirpness ρ, the support of each effective Gaussian is depicted. TheK-transform uses a 2D-Gaussian for computing each K(t, f) co-efficients, with learnable mean and covariance, thus allowing tocontinuously interpolate between known TFR or learn a new onebased on the data and task at hand.
Figure 2: Depiction of the learned filters Ψ[., f](Def. 1). The full filters banks can be found inAppendix B. Left, our learned representationon AudioMNIST dataset (Becker et al., 2018):This noiseless speech pushes the filters with highfrequency resolution (easily seen from the filtercovariance as per (7)). For some of these filtersthe time resolution is also very high (reachingsuper-resolution), while others favor local trans-lation invariance. For all the medium to lowfrequency filters, great frequency and time in-variance is preferred with large gaussian support,with a slight chirpness for the medium frequencyfilters. Thus, the low frequency filters tend to fa-vor time resolution. Right, our learned repre-sentation on Birdvox dataset (Lostanlen et al.,2018): We see that the detection of bird songsheavily relies on chirps. Indeed the characteris-tic sound of birds is increasing or decreasing infrequency over time. Moreover, the learned rep-
Figure 3: Audio MNIST: this dataset deals with spoken digit classification. A few key observations:the high frequency filters tend to take an horizontal shape greatly favoring frequency resolution, forsome of the filters the time resolution is also very high (reaching super-resolution) while others favorlocal translation invariance. For all the medium to low frequency filters, great frequency and timeinvariance is preferred (large gaussian support) with a slight chirpness for the medium frequencyfilters. The low frequency filters tend to favor time resolution.
Figure 4: FreeSound: this dataset contains various different classes ranging on different frequenciesand without an a priori prefered form of the events in the WV space. As opposed to the AudioMNISTcase, we can see that the kernels tend to have smaller covariance (support) hence preferring time andfrequency resolution to invariance. This becomes especially true for the high frequency atoms. Wealso see the clear chirpness for the medium/high frequency kernels with a specific (-30) angle, withdecreasing slope. This might be specific to some particular events involving moving objects such astrain, cars and so on.
Figure 5: Bird: This dataset proposes to predict the presence or absence of a bird in short audioclips. A priori, detection of such events heavily relies on chirps, the characteristic sound of birdswith increasing/decreasing frequency over time. We can see from the learned filters how the learnedrepresentation indeed focuses on such patterns, reach super-resolution and thus extreme sensitivityto the time and frequency position of the events, in particular for medium to low frequency kernels.
Figure 6: DOCC10 dataset samples, one class per column. The click is centered. Details in ENSDATA CHALLENGE WEB SITE.
