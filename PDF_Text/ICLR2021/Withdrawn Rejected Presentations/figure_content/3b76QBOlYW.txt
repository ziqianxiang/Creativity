Figure 1: Schematic depiction of a hologram plate in front of a wave emitter (a) and four amplitudemeasurements (b, c, d, e). The hologram plate has been created to display four numbers out of theMNIST-dataset, each 10mm apart from each other.
Figure 3: Different losses for 100 test images from the MNIST digit dataset and the fashion MNISTdataset, on which the network was not trained before. The top row contains the losses for the MNISTdigit dataset and the bottom row the losses for the fashion MNIST images on which the network wasnot trained. For the Lnmse and the Lstdmse smaller values are better, while for the ssim highervalues are better.
Figure 4: One example output from the five different algorithms for two test datasets. From topto bottom the rows contain: 1) The goal amplitude, 2) Images generated by the Gerchberg-Saxtonalgorithm, 3) Images generated by the L2-Grad algorithm, 4) Images generated by the NOVO-CGHalgorithm, 5) Images generated by the Lstdmse LRGS network and 6) Images generated by theLnmse LRGS networkExample amplitude images are displayed in figure 4a. The images are all distorted, illustrating thehard nature of the problem. In fact the NOVO-CGH algorithm failed to converge 4 times out of 100test images. The outliers are not shown in the plots.
Figure 5: Boxplots for the two different losses used in network training. Panel a) displays thenormalized mean squared error Lnmse for the digit MNIST dataset and the fashion MNIST datasetfor network depths of 1, 6 and10. Panel b) visualizes the standardized mean squared error Lstdmsefor the same network depths. The networks where each trained on the specific loss that is given inthe plots. It can be seen, that the loss decreases for higher network depths.
