Figure 1: (left) Meta learning with the information bottleneck. Zi 〜qw(Zi∣Dt) is the encoder We wish tooptimize to compress the task training set Dit (minimize the mutual information I(Zi, Dit)) and predict well thevalidation set Div (maximize I (Zi , Div )); see Section 2. (right) Specialization to supervised few-shot learningwhere for each task we have input-output data. Gradient-based algorithms, such as MAML, and Gaussianprocess memory-based methods (proposed in this paper) are instances of the framework; see Section 2.2 and 3.
Figure 2: (a) Sinusoid regression with GP and MAML in meta-testing as the number of shots K (x-axis)increases. On y-axis, we report the MSE. For MAML, we report the performance with different number ofinner loop steps, i.e. just SGD steps since we do meta-testing, specified in the legend. (b)-(c) Meta-testingclassification accuracy (y-axis) on mini-Imagenet, where each system has been meta-trained with either N =5, K = 1 or N = 5, K = 5, as the number K of observed examples per class (while always N = 5) growsfrom 1 to 20, e.g. for K = 20 each system sees N × K = 100 support examples. For MAML we showthe performance for different inner loop sizes, which in meta-testing is just SGD updates, where each SGDstep uses a random mini-batch of size 10 data points from the support set of size N × K. (d) Similarly to(b)-(c) for Augmented Omniglot, where instead of growing K we increase the amount of data augmentationin a pre-specified/fixed N = 20, K = 15 initial support set. This means in each SGD update of MAML orpredictive density GP update we sample a mini-batch from the fixed N × K support set, we apply randomtransformations in this mini-batch and then we use it to perform the actual update. The mini-batch size was 20and based on this data augmentation process we grow the amount of data (x-axis) processed by each methodfrom 20 up to 2000. Finally to create all plots we average performance under 10 repeats, where in each repeatthe systems are meta-trained from scratch and then are evaluated in a large number of meta-testing tasks.
Figure 3: The red curve represents a ground truth sinusoid function whereas the light blue one is the meanprediction of a GP. As K increases, the uncertainty (shown by the shaded area) of the GP decreases and itsmean predictions become closer to the ground truth. Given already K = 4 points, the mean prediction matcheswell the sinusoid function.
Figure 4: Qualitative difference between GP and GP+MAML on mini-Imagenet as a function of inner loopsteps. Because there is no inner loop for the GP, we simply report it as a reference to the GP+MAML.
Figure 5: Ablation analysis for different β values on sinusoid regression and few shot classification tasks. Forsinsusoid regression we report mean squared error (MSE): lower is better. For few-shot classification tasks, wereport the accuracy: higher is better.
