Figure 1: The relationship between RB(H`) - RB(h`) and the number of samples n and varianceσ2 when mixup is applied. Each data point was sampled from the normal distribution N (0, σ2)and the constant part was set to 1. It can be seen that as the number of samples n increases, theeffect of complexity mitigation by mixup decreases. We also find that the greater the variance in thedistribution of the data, the higher the effect of mixup.
Figure 2: The relationship between RB (HL,wl ) - RB (HL,wl ) and the number of samples n and thenoise of the outliers . When there are extreme outliers in the sample, we can see that mixup allowsthe neural network to make robust estimation. In addition, we can see that the effect of regularizationdecreases as the sample size n increases.
Figure 3: Beta distribution Bet a(α, α) foreach α. Here, the probability density func-tion of the Beta distribution is f(x; α,β) =B，R、xα-1(1 - x)β-1, where B(α, β) is theB(α,β )beta function. From this figure, it can be seenthat when α = 1, it is equivalent to a uniformdistribution, and when α > 1, it becomes bell-shaped. we can also see that when α < 1, sam-pled λ is close to 0 or 1.
Figure 4: Experimental results for CIFAR-10 dataset.We use ReSNet-18 as a classifier and applymixup With each parameter a for λ 〜Beta(α, α). Left: Learning curve of ResNet-18 With mixup.
Figure 5: Bregman divergence from θ0 to θ. This divergence derived from the convex function ψ(θ)and its supporting hyperplane with normal vector Vψ(θ0).
