Figure 1: Illustration of offline meta-RL: the task is to navigate to a goal position that can be anywhereon the semi-circle. The reward is sparse (light-blue), and the offline data (left) contains traininghistories of conventional RL agents trained to find individual goals. The meta-RL agent (right) needsto find a policy that quickly finds the unknown goal, here, by searching across the semi-circle. Notethat this search behavior is completely different than the dominant behaviors in the data.
Figure 2: Reward ambiguity: from thetwo trajectories, it is impossible to knowif there are two MDPs with different re-wards (blue and yellow circles), or oneMDP with rewards at both locations.
Figure 3: Offline performance on various domains. Blue: our method. Red: our method with rewardrelabelling ablated. Black: Thompson sampling baselines - calculated exactly in Gridworld, andusing online PEARL for the other continuous domains.
Figure 4: Ant-Semi-circle: trajectories from trained policy on a new goal. Left: Trajectory of thecenter of mass. Right: Visualization of the ant at different steps along the trajectories. Note that inthe first episode, the ant searches for the goal, and in the second one it directly moves toward thegoal it has previously found. This search behavior is different from the goal-reaching behaviors thatdominate the training data.
Figure 5: Online perfor-mance comparison. The off-policy optimization signif-icantly improved VariBADperformance.
Figure 6: Interaction of trained agent with Gridworld. For more details, see Zintgraf et al. (2020).
Figure 7: Semi-circle belief visualization. The plots show the reward belief over the 2-dimensionalstate space (obtained from the VAE) at different stages of interacting with the system. The red linemarks the agent trajectory, and the light blue circle marks the true reward location. Top: Once theagent finds the true goal, it reduces the belief over other possible goals from the task distribution.
Figure 8:	Initial state distributions. Red locations indicate non-zero sampling probability.
Figure 9:	Learning curves for the results presented in Table 1. In blue is our method and in red isour method, with critic network trained according to the CQL objective (Kumar et al., 2020). Left:Uniform initial state distribution. Middle: Uniform distribution, excluding states over the semi-circle.
Figure 10: Ant-Semi-circle: trajectories of trained agents for different offline datasets and for PEARL.
Figure 11: Adaptation performance.
Figure 12: Learning curves for online PEARL training. Performance is measured by average rewardin the first 2 episodes on unseen tasks from the task distribution.
Figure 13: Evaluation on Point-Robot-Wind - domain with varying transition function. In (a) isoffline performance of our method, compared to the best performance of online PEARL (similar toFigure 3), and in (b) is the learning curve for online PEARL training (similar to Figure 12).
Figure 14: Point-Robot-Wind: trajectories of trained agent on different test tasks from the taskdistribution.
