Figure 1: Comparison between DGCNN and the proposed method (Shuffle) on point cloud classi-fication and part segmentation. (a) Overall accuracy for classification. Mean and Variance reportedfor 5 runs. (b) Runtime, (c) GPU memory consumption, and (d) FLOPs are reported for comparison.
Figure 2: Overview of our pipeline and visualization of the feature space. The network architecturefor all tasks basically follows MLP structure. The visualization of feature space above the pipelineis a rendering of distance color map extracted from each layer of the neural network on the shape ofraw data. The two bottom sub-figures compare the basic operation of DGCNN and our method.
Figure 3: Qualitative result on Modelnet40. Left: (a), (b) Input space and last-layer feature spacerendered as colormap between the red point and the rest points at epoch 0. The green points areKNN of the red point. (c), (d) follow the same layout with (a), (b) at epoch 250. Right: (e) Ablationstudy of overall acc. w.r.t parameters K and P. Values calculated are the points on the grid, and thehotmap is derived by bilinear interpolation. (f) follows the same layout with (e) for balanced acc.
Figure 4: Surface reconstruction results. Both surface and normal map are visualized.
Figure 5: Renderings of input space and feature space as colormap between the red point and therest of the points on ModelNet40 dataset. The green points represent KNN of the red point. (a)represents the input space. (b) represents the feature space extracted from the second layer of thenetwork. (c) represents the feature space extracted from the last layer of the network. (d), (e),and (f)respectively follows the same layout with (a), (b), and (c).
Figure 6:	Comparison between DGCNN and the proposed method (Shuffle) on point cloud semanticsegmentation. (a) Runtime, (b) GPU memory consumption, and (c) FLOPs are reported for compar-ison.The proposed method can achieve significant reduction of computation resources.
Figure 7:	(a) Ablation study of overall acc. w.r.t parameters K and P. Values calculated are the pointson the grid, and the hotmap is derived by bilinear interpolation. (b) and (c) follows the same layoutwith (e) for balanced accuracy and mean IoU.
Figure 8: Qualitative results of surface reconstruction. (a) Input point cloud. (b), (c) Surface andnormal map reconstructed by Point2Mesh. (d), (e) Surface and normal map reconstructed by ourmethod.
