Figure 1: The effect of noise covariance in neural network and quadratically-parameterizedmodels. We demonstrate that label noise induces a stronger regularization effect than Gaussiannoise. In both real and synthetic data, adding label noise to large batch (or full batch) SGD updatescan recover small-batch generalization performance, whereas adding Gaussian noise with optimally-tuned variance Ïƒ2 cannot. Left: Training and validation errors on CIFAR100 for VGG19. AddingGaussian noise to large batch updates gives little improvement (around 2%), whereas adding labelnoise recovers the small-batch baseline (around 15% improvement). Right: Training and validationerror on a 100-dimensional quadratically-parameterized model defined in Section 2. Similar todeep models, label noise or mini-batch noise leads to better solutions than optimally-tuned sphericalGaussian noise. Moreover, Gaussian noise causes the parameter to diverge after sufficient mixing,as suggested by our negative result for Langevin dynamics (Theorem 2.2). More details are inSection A.
Figure 2: The norm of model weight along training trajectory. We demonstrate that the modelweight fails to converge when training with Gaussian noise. In contrast, the weigth norm convergesfor training with label noise or without noise.
