Figure 1: Illustration of the modelGiven input Xn, let x%ω* ∈ Rr represent the sub-vector that is connected to j-th neuron, where j ∈ [K].
Figure 2: The radius of the lo-cal convex region against √rJoɪɪə əaue一 əXNumber of iterations(a)Figure 3: Convergence rate when r changesVr(b)Convergence rate. Figure 3 shows the convergence rate of Algorithm 1 when r changes. N = 3000,d = 300, K = 5, η = 0.5, and β = 0.2. Figure 3(a) shows that the relative error decreases exponentiallyas the number of iterations increases, indicating the linear convergence of Algorithm 1. As shown in Figure3(b), the convergence rate is almost linear in √r, as predicted by Theorem 2. We also compare with GD bysetting β as 0. One can see that AGD has a smaller convergence rate than GD, indicating faster convergence.
Figure 3: Convergence rate when r changesVr(b)Convergence rate. Figure 3 shows the convergence rate of Algorithm 1 when r changes. N = 3000,d = 300, K = 5, η = 0.5, and β = 0.2. Figure 3(a) shows that the relative error decreases exponentiallyas the number of iterations increases, indicating the linear convergence of Algorithm 1. As shown in Figure3(b), the convergence rate is almost linear in √r, as predicted by Theorem 2. We also compare with GD bysetting β as 0. One can see that AGD has a smaller convergence rate than GD, indicating faster convergence.
Figure 4: Sample complexity When r changes Figure 5: Relative error against √r at different noise levelTheorem 2 that the difference of the learned model from the ground-truth model decreases as the numberof remaining Weights decreases. The second observation is that the test error decreases as N increases forany fixed number of remaining parameters. This verifies our result in Theorem 2 that the difference of thelearned model from the ground-truth model decreases as the number of training samples increases. When thenetWork is pruned significantly such that the percentage of reaming parameters is less than the ground-truth20%, the pruned netWork cannot explain the data properly, and thus the test error is large for all N . Whenthe number of samples is too small, N = 100, the test error is alWays large, because it does not meet thesample complexity requirement for estimating the sparse model even though the netWork is properly pruned.
Figure 6: Test error of pruned models on the syn- Figure 7: Test accuracy of pruned models on MNISTthetic dataset	dataset6	ConclusionsThis paper provides the first theoretical analysis of learning one-hidden-layer sparse neural netWorks, Whichoffers formal justification of the improved generalization of Winning ticket observed from empirical findingsin LTH. We characterize analytically the impact of the number of remaining Weights in a pruned netWork onthe required number of samples for training, the convergence rate of learning algorithm, and the accuracy ofthe learned model. We also provide extensive numerical validations of our theoretical findings. One desiredfuture Work Will be generalizing our theoretical analysis to the scenario of netWork pruning on multi-layerneural netWorks.
