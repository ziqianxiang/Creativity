Figure 1: Top row from left to right: input image, panoptic embeddings, panoptic predictions, and panopticlabels. We overlay panoptic embeddings with the resultant over-segmentation boundaries. Middle row: Afterextracting panoptic embeddings from a CNN and the resultant over-segmentation, we use the segment prototypefeatures to find nearest neighbors, within the image (middle) or across images (right), of each query segment (inred). These retrieval results probe whatâ€™s learned in the embedding space. Bottom row: an example of contexspecific instance retrieval results, where pedestrians crossing an intersection are discovered unsupervisedly.
Figure 2: The overall end-to-end diagram of our proposed Panoptic Segment Sorting (PSS). We first over-segment an image with pixel-wise embeddings, extracted from a CNN. Each segment is represented by aprototype feature (average of pixel embeddings), which is then used for classifying segments (semantic predic-tions) and/or merging segments into an instance (instance predictions), whose features from the embeddingsautomatically encode object-centric context. An extra center seeding branch can faciliate the merging pro-cess by designating seed segments. The overall losses include (1) the SegSort loss (Hwang et al., 2019b) forembeddings, (2) the cross-entropy softmax loss for classification, and (3) the regression loss for seed locations.
Figure 3: Illustration for evaluating the context similarity between two instaces by comparing their semanticdistributions in 8 extended regions. From left to right: whole image, instance of interest, semantic distributionof the middle right extended region. We calculate the symmetric KL-divergence between semantic distributionsfrom corresponding regions as the context error.
Figure 4: Visual comparison for context specific instance retrieval. We show 3 query examples (left) and theirtop retrieval results by our PSS (middle) and UPSNet (Xiong et al., 2019) (right), respectively. We observe thatretrieved instances by PSS are usually in similar context or sometimes even from the same training image.
Figure 5: Pedestrian-centric visual context cluster visualization (best viewed with zoom-in). We first collectall the pedestrian prototypes in the Cityscapes training set. We plot their surrounding ground truth mask at theirt-SNE feature locations and the aggregated density map (bottom left). We observe interesting clusters suchas pedestrians next to a car (center) and pedestrians alone on sidewalks (top left). We also notice some rarecontexts on the middle left by examining the density map: a pedestrian is behind a clutter of a motorbike and abike, which could possibly lead to collision.
Figure 6: Retrieved the K-Nearest Neighbors for query instances. The image and query are on the left, andto their right are the retrieved instances (in the order of left to right, top to bottom). We demonstrate one kindof contexts in each example: 1) Pedestrians crossing an intersection. 2) Pedestrians walking next to cars. 3)Riders riding bikes together. 4) Riders riding motorbikes next to cars. 5) Parked motorbikes. Note that thesecontexts are not given in the ground truth labels, yet our PSS can discover them automatically. We believe theseexamples are relevant in street scene understanding, especially for self-driving vehicles.
Figure 7: More visual comparisons for context specific instance retrieval between PSS (ours) and UPSNet. Itcan be observed that PSS captures more context when retrieving instances. For examples, riding bikes along asidewalk to its left, parked cars with a bike passing by, driving cars with pedestrians nearby, riding motorbikesbehind a car, etc.
Figure 8: Illustration of SegSort training with hybrid scale exemplars in Section 3.3 in the main paper. Theembeddings are overlayed with the predicted over-segmentation (with gray boundaries). We emphasize hereone normal sized instance (outlined in red) and one small instance (outlined in purple), the prototypes ofsegments of each are collected from corresponding embeddings.
Figure 9: Visualization of the merging processes, without and with seeds, described in Section 3.2 and 3.4in the main paper, respectively. On the left are the input images and approximated concentration per segment(overlayed with oversegmentation boundaries). To their right are merging processes without seeds (first row)and with seeds (second row). The red contours outline two segments to merge in this step. The viridis heatmaps indicate the cosine similarity between each segment and its nearest prototype. (The white segments areseed segments.) The merging starts from the pair with the highest cosine similarity. We visualize a few stepsin the beginning. Note that once an instance composes of a single segment, its cosine similarity to its nearestprototype drops significantly. It indicates this merging process with threshold works well with our trainingobjective.
Figure 11: We visualize the prototype features of cars on the Cityscapes training set using tSNE. We observethat the features cluster certain parts or types of cars automatically. For example, front wheels appear on thebottom left corner and red cars in the center. Best viewed with zoom-in.
