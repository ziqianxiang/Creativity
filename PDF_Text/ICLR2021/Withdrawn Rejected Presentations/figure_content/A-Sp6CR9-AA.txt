Figure 1: Illustration of (a) the original batch normalization (BN), composed of one normalization layer andone affine layer; (b) Categorical Conditional BN, composed of one normalization layer following a set of inde-pendent affine layers to intake conditional information; (c) our proposed Sandwich BN, sequentially composedof one normalization layer, one shared sandwich affine layer, and a a set of independent affine layers.
Figure 2: The CAPV value of Y (left) and the shared sandwichparameter γsa's value (right) along the network depth.
Figure 3: We depict two consecutive layers inthe super-network. By default, a BN is inte-grated into each operation in vanilla DARTS,except Zero and Skip-connection operation.
Figure 4: Results of architecture search on CIFAR-100 andImageNet16-120, based on DARTS. At the end of each searchingepoch, the architecture is derived from current a values. The x-axisis the searching epoch. The y-axis is the ground truth test accuracyof current epoch’s architecture, obtained via querying NAS-Bench-201. Each experiment is run for three times with different randomseeds. Each curve in the figure is averaged across them.
Figure 5: The content loss and the style loss of using AdaIN, ILM+INand SaAdaIN. The noisy shallow-color curves are the original data.
Figure 6: Pseudo Python code of BN, SaBN and SaAuxBN with TensorFlow. We highlight the main differencebetween our approaches with vanilla BN.
Figure 7: The search space of NAS-Bench-201.
Figure 8: Architecture parameters (after SoftmaX) on each edge in DARTS.
Figure 9: Architecture parameters (after softmax) on each edge in DARTS-CCBN.
Figure 10: Architecture parameters (after softmax) on each edge in our DARTS-SaBN.
Figure 11: The architectures searched by DARTS are dominated by "skip_connect” and the architectureof DARTS-CCBN is full of both “skip_connect” and “none”. In contrast, DARTS-SaBN highly prefers“nor_conv_3x3”.
Figure 12: The operation statistics of the searched architecture from DARTS and DARTS-SaBN.
Figure 13: The architecture loss LVal(ω, α) and the weight loss Ltrain(ω, α) of DARTS, DARTS-CCBN andDARTS-SaBN.
Figure 14: The architecture-weight loss gap of DARTS, DARTS-CCBN and DARTS-SaBN.
Figure 15: The CAPV for DARTS-CCBN and DARTS-SaBN. The shared sandwich affine layer is learned toreduce the feature heterogeneity across the whole network.
Figure 16: Results of architecture search on CIFAR-100 and ImageNet16-120, based on GDAS. EaCh curve isthe average result of three runs using different random seeds.
Figure 17: The adversarial branch loss L(fadv(XadV), y) and clean branch loss L(fclean(XCIean), y) on trainingset. f, x, y denote model, input and label respectively. The model with SaAuxBN has lower training loss.
Figure 18: The adversarial branch loss L(fadv(XadV), y) and clean branch loss L(fclean(XClean), y) on testing set.
Figure 19: The train-test loss gap for adversarialL(fclean(xclean), y).
Figure 20: The CAPV value for models With AuXBN and SaAuXBN. We can observe the shared sandwichaffine layer is learned to reduce CAPY i.e., the feature heterogeneity.
Figure 21: The image generation results of SNGAN and SNGAN-SaBN on ImageNet. Each column is corre-sponding to a specific image class.
Figure 22: The image generation results of SNGAN and SNGAN-SaBN on CIFAR-10.
Figure 23: The image generation results of AutoGAN and AutoGAN-SaBN on CIFAR-10.
Figure 24: The image generation results of BigGAN and BigGAN-SaBN on CIFAR-10.
Figure 25: The generator loss of SNGAN and SNGAN-SaBN on ImageNet. SNGAN-SaBN achieves lowerloss value.
Figure 26: The visual results of style transfer. An ideally stylized output should be semantically similar to thecontent image, while naturally incorporate the style information from the referenced style image.
