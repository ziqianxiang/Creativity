Figure 1: The optimization process of thevariational parameters Φ(n) using evolution-ary search. A. Some states are selected asparents. B. Each child undergoes mutation.
Figure 2: Graphical representation ofthe model architecture used in numeri-cal experiments.
Figure 3: ELBO gain of TVAE compared tolinear VAE with binary latents (on 16 × 16image patches).
Figure 4: TVAE denoising of house image withnoise level σ = 50. The denoised image hasPSNR=30.03, the best of the runs of Tab. 2.
Figure 5: From left to right: generic VAE decoding model, continuous-latent VAE model withGaussian noise and the binary-latent VAE model of Eqn. (1), in plate notation.
Figure 6: Standard series of methods applied to optimize the encoding model of VAEs. Left: meth-ods applied for encoding models of standard VAEs. Middle: additional methods applied to maintainthe standard procedure of encoding model optimization also for discrete latent variables. Right: al-ternative approach to optimize the VAE encoding model using direct discrete optimization.
Figure 7: TVAE training on simple bars data: noiseless output of the TVAE’s DNN for the 8 possibleone-hot input vectors over several training epochs. Generating parameters are in the last row.
Figure 8: Correlated bars test. The plot shows the ratio between inferred and ground-truth log-likelihoods log pΘ (~x) of datapoints with interesting bar combinations. The inferred values are re-ported below the datapoints themselves.
Figure 9: From left to right: generative parameters for the correlated bars test; ELBO values overepochs for 10 runs; example datapoints and samples from the generative model.
