Figure 1: The growing hypothesis space of potential interaction effects is balanced against theeffective learning rate imposed by Dropout. In this figure, we plot the product of the effective learningrate (rp(k)) and the number of potential interaction effects of order k (Hk) for a variety of Dropoutrates p. In a, we plot these values on a log scale for the entire range of potential interaction orders foran input of 25 features. In b, we plot up to order 4 on a linear scale.
Figure 2: In this experiment, we train fully-connected NNs on pure noise (details in Sec. 5.2).
Figure 3: Learned interaction effects of order 1, 2 and 3 (cols 1, 2, and 3 respectively), and modelerror on train and test (col 4) vs. epochs. Each row corresponds to a different generator as describedin Sec. 5.4.1: the generator in the top row has only 1-way interactions, the generator in the middlerow has only 2-way interactions, and the bottom row has only true 3-way interactions. The figure iscomplex; key findings are described in Sec. 5.4.1.
Figure 4: Strong weight decay can have a mild regularization effect against interaction effects;however, the regularization effect comparable to Dropout occurs at extremely strong weight decayfor which training is very unstable.
