Figure 1: Comparison of convergence with re-spect to time for the multi-layer perceptron trainedon the Boston Housing dataset. All networks aretrained for 4000 epochs with learning rate = 0.02and α = 0.68.
Figure 2: Comparison of training and test convergence with respect to time for different values oftuning parameter, k in a single neuron trained on the Iris dataset. We convert the problem into abinary classification problem by considering only two output classes. All networks are trained for600 epochs with 80 training examples and 20 test examples. The learning rate for all L1, L2 andLyapunov is set to the tuning parameter value k and α = 0.8From Table 2, we observe that the settling time for the training loss of our Lyapunov loss functionis always less than L1 and L2 . We also observe that the Accuracy of our method is either better7Under review as a conference paper at ICLR 2021or similar to the baselines, which shows that the optimized solution achieved by our method iseither better or on par with the baselines. For the multi-neuron case, we have defined kilj for everyweight. Similar to adaptive learning rate, we can either tune kilj for every weight (which can be quitecumbersome for larger neural networks) or we can set it arbitrarily to the same value for all weights.
Figure 3: Comparison of training loss convergence with respect to time for different values of inputperturbations, ∆x for a multi-layer perceptron trained on the IMDB Wiki Faces dataset.The a prioriupper bound on input perturbations are as follows: (a) ∆x = 0.1, (b) ∆x = 0.2, (c)∆x = 0.3Learning rate/k = 0.0009, α = 0.8, epochs=100In this section, we present results that demonstrate the robustness of proposed algorithm to boundedperturbations in the input while maintaining convergence of training dynamics. For this case, wetrain a multi-layer perceptron on a regression task of predicting the age given the image of a face.
Figure 4: Comparison of training convergence with respect to time for 0.5 million IMDB WikidatasetExperiment	Theoretical Upper	Exp. Convergence Time		Metric		Bound	(in seconds)		rmse		(in seconds)	L1	L2	Lyap.	L1	L2	Lyap.
Figure 5: Depiction of the numerical instability observed when we take α = 0. Effectively, at thispoint, the loss function contains a discontinuous signum function.
Figure 6: Comparison of convergence with respect to time for a single neuron trained on the Irisdataset. We convert the problem into a binary classification problem by considering only two of thethree output classes. All networks are trained for 2100 epochs with 80 training examples and 20 testexamples, α = 0.8, learning rate /k = 0.01From the Figure 6, we can clearly see that Lyapunov loss function with control weight update con-verges much faster towards zero as compared to L1 and L2 loss with standard gradient descentweight update. This shows that explicitly adding a control weight update (15) that drives the losstowards zero is helpful in reaching convergence faster.
Figure 7: Pictorial representation of the effect of random additive noise with an upper bound of 0.2on IMDB Wiki Faces Dataset. In this experiment, we work with monochrome images for the sakeof simplicity.
Figure 8: Comparison of training loss convergence with respect to time for different values of inputperturbations, ∆x for a multi-layer perceptron trained on the IMDB Wiki Faces dataset. The a prioriupper bound on input perturbations are as follows: (a) ∆x = 0.1, (b) ∆x = 0.2, (c)∆x = 0.3, (d)∆x = 0.4, (e) ∆x = 0.5, and (f) ∆x = 1.2.
Figure 9: Comparison of test loss convergence with respect to time for different values of inputperturbations, ∆x for a multi-layer perceptron trained on the IMDB Wiki Faces dataset.The a prioriupper bound on input perturbations are as follows: (a) ∆x = 0.1, (b) ∆x = 0.2, (c)∆x = 0.3, (d)∆x = 0.4, (e) ∆x = 0.5, and (f) ∆x = 1.2.
