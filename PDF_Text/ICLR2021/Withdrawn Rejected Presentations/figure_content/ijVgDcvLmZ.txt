Figure 1: The relative overgeneralization in discrete (a) and continuous (b) action space3 methodIn this section, we will first introduce the IGO (Individual-Global-Optimal), a new definition of fac-torizable MARL tasks with stochastic policies. Theoretical analysis shows that IGO degenerates intoIGM if the policy is greedy. With the energy-based policy, the structure between joint and individualaction values of IGO can be explicitly constructed, which is a novel factorization stochastic-basedpolicy solution we proposed, named FSV. Specifically, FSV realizes IGO using an efficient linearstructure and learns stochastic policies through maximum entropy reinforcement learning.
Figure 2: FSV network architectureFigure 2 shows the overall learning framework, which consists of two parts:(i) individual parts foreach agent i, which represents Qi, Vi and πi (ii)incorporation part that composes Qi, Vi to Qtot .
Figure 3: Max of Two Quadratics game:(a)reward function, (b)average reward for FSV,VDN,QMIXand MADDPGFig 3(b) is the training result averaged over 20 experiment runs and Table 7 gives a more detailedresult, where MADDPG and QMIX happened to find the optimal actions due to random initializationtwice. VDN never find the optimal actions and even fails to find the sub-optimal 4 times. These7Under review as a conference paper at ICLR 2021Table 7: training result for Max of Two Quadratics game	opt	sub-opt	otherFSV	^^0^^	0	-0-MADDPG	2^Γ~	-18-	-0--QMIX-	2^Γ~	-18-	-0-VDN	-0-	16	4results indicate that, a more explorative policy and correct estimation of Q-values are both neededto overcome the relative overgeneralization problem. Using a centralized critic like MADDPG toguide the decentralized actors will mislead the policy gradients because it averages the Q-valuesbased on others’ policies (?). Using individual Q-values to guide actors requires the full expressiveability of factorizable tasks where QMIX and VDN fail to estimate individual Q-values correctlydue to the structural limitation as shown in Sec5.1 and QTRAN losts its tractability for continuoustasks. To enable better exploration in joint action space, Wei et al. (2018) adopt multi-agent soft
Figure 4: test Win rate of FSV, VDN, QMIX and QTRAN6	conclusionIn this paper, We proposed a neW definition of factorizable tasks With stochastic policies named IGO.
