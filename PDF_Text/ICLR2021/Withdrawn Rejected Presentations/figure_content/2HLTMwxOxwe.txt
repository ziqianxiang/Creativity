Figure 1: Left: Transductive OOD detection setting. Labeled training set and an unlabeled test set withsamples both in- and outside the support of the training distribution. Right: Performance of RETO and somebaselines on data sets ranked by their difficulty (Appendix C contains details on the hardness metric). Theshaded area represents the gap in area under ROC curve (AUROC) between RETO and the next best baselines.
Figure 2: Cartoon illustration of an ensemble of two linear classifiers that fit both the training set and the testset. We assign either the blue label (Model 1) or the red label (Model 2) to the whole test set. Linear classifiersare smooth enough that they cannot fit both the correct labels of the training set and the arbitrary label on thetest ID samples. The classifiers disagree only on the OOD points in the test set (gray crosses) and agree topredict the correct label on the test ID samples.
Figure 3: Comparison between different kinds of ensembles used for OOD detection. Left: A vanilla ensem-ble will not be diverse enough outside the training distribution support. Middle: In comparison, the RETOclassifiers disagree only on the OOD points in the test set (gray crosses) and agree to predict the correct labelon the test ID samples. Right: However, if the models in the ensemble are too complex (i.e. not regularizedenough), then they can easily fit the arbitrary label on the ID test samples as well, thus identifying the wholetest set as OOD.
Figure 4: Importance of the right amount of regularization forpower. Left: If the model class is too small, then we cannotfit the arbitrary label on test OOD points, and the classifierswill agree on them, leading to low statistical power. Right: Inthis situation, the OOD samples can only be detected using alarger function class.
Figure 5: Accuracy measured on the correctly-labeled training set and on the arbitrarily-labeled test ID andOOD subsets. Validation accuracy is computed on a hold-out set with correctly-labeled ID samples. Thetraining set and the test OOD samples are fit first, while the test ID set reaches high accuracy much later. Atthe early stopping iteration (after around 50 epochs) the models tend to predict the arbitrary label for test OODsamples and the correct label for test ID. The model is a Resnet20 trained on CIFAR10 as ID and SVHN asOOD.
Figure 6: Left: AUROC averaged over all scenarios in the medical OOD detection benchmark. The values forthe baselines are computed using the code from the paper Cao et al. (2020). Right: The gap in AUROC betweenRETO and a vanilla ensemble, as the number and proportion of ID (CIFAR10) and OOD (CIFAR10-C/snow)samples in the test set is varied. The AUROCs are obtained with an ensemble of ResNet20 models.
Figure 7: (a) Data samples for the MNIST/FashionMNIST splits. (b) Data samples for the CI-FAR10/SVHN splits.
Figure 8: Samples from ImageNet and ObjectNet taken from the original paper by Barbu et al.
Figure 9: Data samples for the corrupted CIFAR10-C data set.
Figure 10: Effect of ensemble size on CIFAR100 vs. SVHN. Both methods are trained from scratchfor 100 epochs.
Figure 11: AUROCs obtained with an ensemble of ResNet20 models as the composition of the testset is changed. Only settings with at least 5 OOD samples have been considered. The ID samples arefrom CIFAR10, while the OOD samples are from CIFAR10-C/snow5. For comparison, we providein parentheses AUROC values for a vanilla ensemble trained once on the training set and evaluatedon each test set configuration. In 11a, the models are initialized from pretrained weights and finetuned. In 11b, the models are initialized with random weights and trained from scratch.
Figure 12: AUROCs obtained with an ensemble of WRN-28-10 models, as the initial learning rateand the training batch size are varied. For this we used the hardest setting, CIFAR100:0-50 as ID,and CIFAR100:50-100 as OOD.
Figure 13: AUROC averaged over all scenarios in the medical OOD detection benchmark (Caoet al., 2020). The values for all the baselines are computed using code made available by authors ofthe paper Cao et al. (2020). Notably, most of the baselines assume oracle knowledge of OOD dataat training time.
Figure 14: Comparison between RETO and the various baselines on the NIH chest X-ray data set,for use case 1 (top), use case 2 (middle) and use case 3 (bottom).
Figure 15: Comparison between RETO and the various baselines on the PC chest X-ray data set, foruse case 1 (top), use case 2 (middle) and use case 3 (bottom).
Figure 16: Comparison between RETO and the various baselines on the DRD fundus imaging dataset, for use case 1 (top), use case 2 (middle) and use case 3 (bottom).
Figure 17: Averaging the probability outputs of the models in an ensemble can lead to catastrophicinformation loss, in some cases. The softmax vectors of a 3-model ensemble are represented onthe 2D probability simplex. For an OOD sample, each model predicts with high confidence thearbitrary label it has seen during training. For the ID sample, the models predict the correct classwith moderate confidence. Therefore, the average probability vectors for the ID and the OOD sampleare close, which can make it hard to distinguish between them.
Figure 18: Distribution of confidence for a model trained on the training set alone (Top) and a modeltrained on both the training set and the arbitrarily labeled test set, with early stopping (Middle) andafter 100 epochs (Bottom). The models in the vanilla ensemble are confident on both ID and OODsamples. The model trained on training+test is only confident on OOD data early during training.
