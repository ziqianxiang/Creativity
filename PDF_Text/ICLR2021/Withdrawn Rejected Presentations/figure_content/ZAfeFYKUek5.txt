Figure 1: The expected test ZO loss and its bias and variance. The models were trained with 20%label noise. Adam optimizer with learning rate 0.0001 was used.
Figure 2: Test accuracy and OV. The models were trained with Adam optimizer (learning rate0.0001). The number in each legend indicates its percentage of label noise.
Figure 3: OV estimated from different number of training batches. The models were trained with20% label noise. Adam optimizer with learning rate 0.0001 was used.
Figure 4: Test accuracy and OV. The model was LeNet-5 trained on MNIST and FashionMNISTwith Adam optimizer (learning rate 0.0001).
Figure 5: Early stopping based on test error (True) and the corresponding OV (Found). The shapesrepresent different datasets, whereas the colors indicate different categories of DNNs (“CF" and“Res" are short for “CIFAR" and “ResNet", respectively).
Figure 6: Test accuracy and OV usingsmall sizes of training sets. The num-bers represent how many training sam-ples are used for training.
Figure 7: Test accuracy and OV w.r.t.
Figure 8:	Test accuracy and Vg . The models were trained with the Adam optimizer (learning rate0.0001). The number in each legend indicates its percentage of label noise.
Figure 9:	Different loss functions w.r.t. the training epoch. ResNet18 was trained on SVHN, CI-FAR10, and CIFAR100 with 20% label noise to introduce epoch-wise double descent. Adam opti-mizer with learning rate 0.0001 was used.
Figure 10:	Test accuracy and optimization variance (OV) of VGG11 on SVHN, w.r.t. different levelsof label noise. Adam optimizer with learning rate 0.001 was used.
Figure 11:	Loss, variance and bias w.r.t. different levels of label noise. The model was ResNet18trained on CIFAR10. Adam optimizer with learning rate 0.0001 was used.
Figure 12:	Test MSE loss and the corresponding bias/variance terms. The models were trained with20% label noise. Adam optimizer with learning rate 0.0001 was used.
Figure 13:	Test CE loss and the corresponding bias/variance terms. The models were trained with20% label noise. Adam optimizer with learning rate 0.0001 was used.
Figure 14:	The expected test ZO loss and its bias and variance. The models were trained with 20%label noise. Adam optimizer with learning rate 0.001 was used.
Figure 15:	Expected test ZO loss and its bias and variance. The models were trained with 20% labelnoise. SGD optimizer (momentum = 0.9) with learning rate 0.01 was used.
Figure 16:	Expected test ZO loss and its bias and variance. The models were trained with 20% labelnoise. SGD optimizer (momentum = 0.9) with learning rate 0.001 was used.
Figure 17: Test accuracy and optimization variance (OV). The models were trained with Adamoptimizer (learning rate 0.001). The number in each legend indicates its percentage of label noise.
Figure 18: Test accuracy and optimization variance (OV). The models were trained with SGD op-timizer (learning rate 0.01, momentum 0.9). The number in each legend indicates its percentage oflabel noise.
