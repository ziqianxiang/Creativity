Figure 1: An overview of our framework. Our model consists of two stages: sound clusteringand auditory event prediction. The agent starts to collect a diverse set of sound through limitedenvironment interactions (i.e. 10K) and then clusters them into auditory event classes. In thesecond stage, the agent uses errors of auditory events predictions as intrinsic reward to explore theenvironment.
Figure 2: The first and second rows show the auditory events that we discovered by the K-meansalgorithm in Frostbite and Assault respectively.
Figure 3: Average extrinsic rewards of our model against baselines in 20 Atari games. Intrinsicrewards are used for training, and the extrinsic rewards are used for evaluation only.
Figure 4: Comparisons on earned extrinsic re- Figure 5: Comparisons on earned extrinsic re-wards between our auditory event prediction mod- wards using random, RND, and our active explo-ule and sound feature prediction module.	ration strategy respectively.
Figure 6: Comparisons of stage coverageon Habitat.
Figure 7: Explorations on a multi-modal physics environment. From left to right: physical scene,collision events, and intrinsic reward changesSetup. We take an image observation of 84Ã—84 size and 50ms audio clip as input. We use a three-layer convolutional network to encode the image and extract sound textures from the audio clip. Sameas the previous experiment, we train the policy using the PPO algorithm. The action space consistsof moving to eight directions and stop. An action is repeated 4 times on each frame. We run all theexperiments for 200K steps with 8 parallel environments.
Figure 8: Ablated study on 3 Atari games, Habitat and TDW.
Figure 9: Average extrinsic rewards of our model against baselines combined with sound in 5 AtarigamesIn this section, we carry out ablated experiments to demonstrate that the gains in our method arecaused by the audio-event prediction, rather than the use of multi-modality information. For fourbaselines (i.e. RND, RFN, ICM, and DIS), instead of predicting audio-event, they consider soundinformation by concatenating both visual and sound features to predict the image embedding in thenext time step. As shown in Figure 9, our algorithm significantly outperforms other baselines infive Atari games. This indicates that it is non-trivial to exploit sound information for RL, and ouralgorithm benefits from the carefully designed audio-event prediction as an intrinsic reward.
Figure 10: Comparison of combining intrinsic and extrinsic reward on 6 hard exploration AtariGames.
