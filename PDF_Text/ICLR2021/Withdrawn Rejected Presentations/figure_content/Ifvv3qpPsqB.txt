Figure 1: For each image in (a)/(d), the left side is the normal data while the right side is perturbedby adversarial noise. (b)/(e) shows that adversarial attack could fail existing segmentation models.
Figure 2: Motivation and overall framework of DDC-AT. (a) Clean pixels in the output space aredivided into two categories by divide-and-conquer strategy. (b) Main branch fn is utilized to conqueradversarial pixels, and clean pixels stay far away from classification boundary. The auxiliary branchfa is employed to conquer clean pixels that are sensitive to perturbation. The mask branch fmdivides pixels into these two branches dynamically. The final output o is combined from the divisionduring training. In testing, both fa and fm are abandoned, and only fn is utilized to output on .
Figure 3: Visual comparison on VOC. Top row is obtained from models with PSPNet, and bottomrow is derived from models with DeepLabv3.
Figure 4: The training framework for the method without defense. Such training scheme does notadd adversarial samples into training.
Figure 5: The training framework for standard adversarial training SAT. Such training scheme al-ways uses one identical branch to align clean and adversarial samples.
Figure 6: The training framework for DDC-AT-MFigure 7: The training framework for DDC-AT-N15Under review as a conference paper at ICLR 2021Table 9: The evluation of our method and the baseline on white-box/black-box attack, experimentsare executed on the dataset of VOC/Cityscapes. We report the mIoU for different defense methodswith various model structures. Especially, we record the mean value (“Mean”) and standard de-viation (“Std”) of results through several repeated training process. The bold indicates the higherperformance between SAT and DDC-AT. Symbolic representation is the same as that of Table 1.
Figure 7: The training framework for DDC-AT-N15Under review as a conference paper at ICLR 2021Table 9: The evluation of our method and the baseline on white-box/black-box attack, experimentsare executed on the dataset of VOC/Cityscapes. We report the mIoU for different defense methodswith various model structures. Especially, we record the mean value (“Mean”) and standard de-viation (“Std”) of results through several repeated training process. The bold indicates the higherperformance between SAT and DDC-AT. Symbolic representation is the same as that of Table 1.
Figure 8: The t-SNE analysis for VOC and Cityscapes. a: clean samples in the model with nodefense, b: adversarial samples in the model with no defense, c: clean samples in the SAT model,d: adversarial samples in the SAT model, e: clean samples in the DDC-AT model, f: adversarialsamples in the DDC-AT model. The adversarial samples are generated with white-box BIM attack(L∞ constraint, n=2, = 0.03 × 255, α = 0.01 × 255)A.4 Analysist-SNE Analysis for SAT and DDC-AT To further analyze the defense effect of SAT and DDC-AT, we use t-SNE to visualize the corresponding feature distribution of trained models. As displayedin Fig. 8, we find: for the distribution of clean samples in models with no defense, features from dif-ferent classes are separated severally. This is helpful for segmentation. However, for the distributionof adversarial samples, features from different categories are mixed. For SAT and DDC-AT, featuresof adversarial samples and clean samples from different classes are both separated. Thus, SAT andDDC-AT can improve the robustness of models and keep great performance on clean samples.
Figure 9: The visual comparison for different defense methods, which are executed on VOC datasetwith model structure as PSPNet.
Figure 10: The visual comparison for different defense methods, which are executed on VOC datasetwith model structure as DeepLabv3.
Figure 11: The visual comparison for different defense methods, which are executed on Cityscapesdataset with model structure as PSPNet.
Figure 12: The visual comparison for different defense methods, which are executed on Cityscapesdataset with model structure as DeepLabv3.
