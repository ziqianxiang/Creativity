Figure 1: (a) shows the overview of our method and (b) shows the distribution of generated sam-ples of SNDCGAN on CIFAR-10 for varying values of N0. The percentage of class 0 (randomlychoosen) samples is determined by an annotator (i.e. high accuracy classifier). When N0 is large,the network tries to decrease fraction of class 0 samples whereas when N0 is small it tries to increasefraction of class 0 samples among the generated samples.
Figure 2: Distribution of classes and corresponding FID scores on long-tailed CIFAR-10 computedon samples generated by GANs with uniform distribution of labels in case of conditional GANs.
Figure 3: Overview of our UAP crafting approach using arbitrary data and a classifier in the loop.
Figure 4: Effect on FID with change in num steps for statistics update(For CIFAR-10 imbalanceratio ρ = 10)A.5 Hyperparameter Configuration (Image Generation Experiments)A.5.1 Lambda the Regularizer coeffecientThe λ hyperparameter is the only hyperparameter that we change across different imbalance scenar-ios. As the overall objective is composed of the two terms:Lg = -E(x,z) MPr,Pz) [log。(D(G(Z))- D(X))] - λLreg	QI)As the number of terms in the regularizer objective can increase with number of classes K . Formaking the regularizer term invariant of K and also keeping the scale of regularizer term similar toGAN loss, we normalize it by K. Then the loss is multiplied by λ. Hence the effective factor thatgets multiplied with regularize] term is K.
Figure 5: Plots of FID (y axis) vs Number of iteration steps. We observe a similar curve in both thecases.
Figure 6: Comparison of samples used by our approach vs approach by Zhang et al. (2020)(b) Samples from Comics DatasetDataset: We use the Comics dataset (Comics-Dataset) whereas approach (Zhang et al., 2020) useCOCO dataset. COCO dataset has overlap with ImageNet categories. The difference in imagesused shows that our procedure does not require natural images for generating effective attack. Thisincreases applicability of our method. We use a DCGAN architecture to generate 128 × 128 imagesfrom the GAN described in Table 15 and 16. In this experiment, we update the mode statistics afterevery epoch. Hyperparameters are present in the table 14. The images generated by our method ispresent in figure 10.
Figure 7: Number of distinct Modes observed while sampling from different cycles18Under review as a conference paper at ICLR 2021Layer	Input	Output	OperationInput Layer	(m, 100)	(m, 4 4,1024)	TCONV(100, 1024, 4, 1)Hidden Layer	(m, 4, 4, 1024)	(m,8,8,512)	TCONV(1024, 512, 4, 2),BN,LRELUHidden Layer	(m, 8, 8, 512)	(m, 16, 16, 256)	TCONV(256, 128, 4, 2),BN,LRELUHidden Layer	(m, 16, 16, 256)	(m, 32, 32, 128)	TCONV(256, 128, 4, 2),BN,LRELUHidden Layer	(m, 32, 32, 128)	(m, 64, 64, 64)	TCONV(128, 64, 4, 2),BN,LRELUHidden Layer	(m, 64, 64, 64)	(m, 128, 128, 3)	TCONV(64, 3, 4, 2),BN,LRELUOutput Layer	(m, 128, 128, 3)	(m,128,128, 3)	TanhTable 15: Generator of DCGAN (Radford et al., 2015) used for UAP Experiments.
Figure 8: Images from different GANs with imbalance ratio (ρ = 10)(d) Ours (Unconditional)19Under review as a conference paper at ICLR 2021(a) ACGAN (Conditional)(b) cGAN (Conditional)(c) SNDCGAN (Unconditional)Figure 9: Images generated by different GANs for CIFAR-10 with imbalance ratio (ρ = 10).
Figure 9: Images generated by different GANs for CIFAR-10 with imbalance ratio (ρ = 10).
Figure 10: Images generated for CIFAR-100 dataset with our method (GAN + Regularizer).
