Figure 2: Pretraining and fine-tuning with ResNet-18.
Figure 3: Pretraining sources (test onCifar100).
Figure 4: Depth, width, and ensembles on Cifar100.
Figure 5: Data augmentation on Places365.
Figure 6: Additional datasets4.3	Evaluation of Learning Curves Model and FittingWe validate our learning curve model using leave-one-size-out prediction error, for example, pre-dicting empirical mean performance with 400 samples per class based on observing error frommodels trained on 25, 50, 100, and 200 samples. We consider various choices of weighting schemes(w’s in Eq. 2) and estimating different parameters in a general form of the learning curve given bye(n) = α+ηnγ +δn2γ. Note that setting δ = 0 yields the learning curve model described in Sec. 3.
Figure 7: Learning curve model and weights validation. See text for explanation.
Figure 8: Stability under sparse measurements: Sampled learning curves for Places365 fine-tunedwithout pretraining are shown for four different learning curve parameterizations. In each case,means and standard deviations (shown by error bars) are estimated for n = 50, n = 100, n = 200,n = 400, using all the data points shown as white circles. Then, 100 times, we sample one pointeach from a Guassian distribution and fit a learning curve to the four points. In parantheses, thelegend shows the standard deviation of eN, βN, and γ. Note that the parameterization of {α, η, γ}extrapolates best to lower and higher data sizes while still producing stable estimates of eN and βN .
Figure 9: Optimization on Cifar10 with ResNet-18.
