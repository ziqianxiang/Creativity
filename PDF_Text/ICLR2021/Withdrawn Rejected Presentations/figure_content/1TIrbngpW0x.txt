Figure 1: We show a simplified version of the model at a single position to illustrate the difference between headsand mechanisms (left). Heads allow for parallel attention, but the differentiation between heads is transient: itbegins with the projection layer and ends immediately following the attention. As a result, most of the parametersare not head-specific. With independent mechanisms, the information is kept well-separated throughout theentire layer, and all of the layerâ€™s parameters are specific to a single mechanism. Competition patterns of anunsupervised image Transformer on CIFAR images (top right) with 2 mechanisms shows that mechanismslearn to specialize over foreground and background patterns on an early layer (center right) and become moreconfident in a later layer (bottom right).
Figure 2: We trained an Image Transformer (pixel-by-pixel, raster-order generative model) on a dataset wherewe construct new images in which the left-half of each image is an MNIST digit and the right-half of theimage is a random CIFAR example (left). We found that with ns = 2, the two mechanisms in TIM learn tospecialize over the two sides of the image, with one TIM only activating on the MNIST digit and one TIM onlyactivating on the CIFAR example (activation patterns shown in center). We measured the relative variance in themechanism-activation over the two sides of the image, showing that the specialization becomes much strongerover the course of training (right).
Figure 3: An examples of a speech signal (left) with their respective competition patterns over five successiveTIM layers (ordered from top to bottom). In the early layers, the competition is uncertain, but becomes morecertain in the deeper layers. This is further quantified in a correlation matrix of competition over layers (middle)and a plot showing that competition entropy drops in later layers, especially at the lowest percentiles (right).
Figure 4: On a BERT model, we show the minimum, average, and maximum mechanism competition values ofsome selected common tokens (left). One mechanism clearly specializes over the period between sentences, yetthe high difference between the minimum and maximum values suggest that these differences are contextual andnot a static function of the token. In particular we found that the modular activation for a period depends onwhether it is used to mark the end of a sentence or whether it is used as part of a number or a URL. Moreover,the mechanism activation is highly correlated between layers (scatter-plot in center, correlation matrix on right).
