Figure 1: (a) show cubic v.s. square function. (b) shows their absolute derivatives. (c) shows the hitting timeratio v.s. initial value x0 under different target value xt . (d) shows the ratio v.s. the target xt to reach underdifferent x0 . Note that a ratio larger than 1 indicates a longer time to reach the given xt for the square loss.
Figure 2: Testing RMSE v.s. number of mini-batch updates. (a)(b)(c)(d) show the learning curves withdifferent mini-batch size b or Guassian noises variance σ added to the training targets. (a) is using σ = 0.1 and asmaller training set (solid line for |T| = 800, dotted line for |T| = 1600) than others but has the same testingset size. (e) shows the a corresponding experiment in RL setting on the classical mountain car domain. Theresults are averaged over 50 random seeds on (a)-(d) and 30 on (e). The shade indicates standard error.
Figure 3: (a) shows the GridWorld taken from Pan et al. (2019). The state space is S = [0, 1]2, and the agentstarts from the left bottom and should learn to take action from A = {up, down, right, lef t} to reach the righttop within as few steps as possible. (b) shows the distance change as a function of training steps. The dashedline corresponds to our algorithm with an online learned model. The corresponding evaluation learning curve isin the Figure 5(c). (d) shows the policy evaluation performance as a function of running time (seconds). Allresults are averaged over 20 random seeds and the shade indicates standard error.
Figure 4:	(a) (b) shows the ER buffer state distributions trained by regular ER and prioritized ER respectively.
Figure 5:	Evaluation learning curves on benchmark domains with planning updates n = 10, 30. The dashedline denotes Dyna-TD with an online learned model. All results are averaged over 20 random seeds. Figure(g)shows MazeGridWorld(GW) taken from Pan et al. (2020) and the learning curves are in (h). On MazeGW, wedo not show model-free baselines as it is reported that model-free baselines do significantly worse than Dynavariants (Pan et al., 2020). We do reproduce the result of Dyna-Frequency from that paper.
Figure 6:	Evaluation learning curves on Mountain Car with different number of planning updates and differentreward noise variance. At each time step, the reward is sampled from the Gaussian N (-1, σ). σ = 0 indicatesdeterministic reward. All results are averaged over 20 random seeds.
Figure 7: (a) shows the round-about domain, where S ⊂ R90 .
Figure 8: The function f (x) = ln 1 — 1, x > 0. The function reaches maximum at X = 1.
Figure 9: Figure(a)(b)(c) show the training RMSE as a function of number of mini-batch updates with differentmini-batch sizes or Guassian noises with different σ added to the training targets. The results are averaged over50 random seeds. The standard error is small enough to get ignored.
Figure 10:	FigUre(a)(b)(c) show the testing RMSE as a function of number of mini-batch updates with differentmini-batch sizes or Guassian noises with different σ added to the training targets. (d)(e)(f) show the trainingRMSE. The results are averaged over 50 random seeds. The standard error is small enough to get ignored. Notethat the target variable in the testing set is not noise-contaminated.
Figure 11:	Figure(a)(b)(c) show the training RMSE as a function of number of mini-batch updates on the Bikesharing dataset. The results are averaged over 20 random seeds. The shade indicates standard error.
