Figure 1: Schematic diagram of our data representation encoding scheme in deep learning pipeline. We showa simple toy example of classifying data points of triangles, squares, and circles. In embedding space (themiddle block), data representations from different classes are constrained to a small ball with diameter ∆H,while they are separate from each other at least with distance δh.
Figure 2: Data hidden representation h(x) ∈ R2 from the 2nd fully-connected layer of synthetic data atdifferent epoch (E). Two settings of MixCon are given default λ = 0.1 but have different β. Compare toVanilla, MixCon squeezes data representations to a smaller space over training. When β = 0, MixCon map alldata to h(x) = (0, 0), which is not learnable.
Figure 3: Trade-off between data separability and data utility. We show testing accuracy and mean pairwisedistance (data separability) on three datasets with different λ and β . λ and β show complementary effort onadjusting data separability. A sweet-spot can be found at the (λ, β) resulting in small data separability and highdata utility.
Figure 4: Qualitative evaluation for image inversion results. (λ, β) settings of MixCon denoted on the header.
Figure 5: Qualitative evaluation for image inversion results.
Figure 6: Adding MixCon to the 1st layer of CNN on MNIST dataset. (a) The trade-off between data separa-bility and data utility . We show testing accuracy and mean pairwise distance (data separability) with differentλ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation of datarecovery results. We show SSIM and PSIM scores with different λ and β.
Figure 7: Adding MixCon to the 2nd layer of CNN on MNIST dataset. (a) The trade-off between dataseparability and data utility . We show testing accuracy and mean pairwise distance (data separability) withdifferent λ and β . λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluationof data recovery results. We show SSIM and PSIM scores with different λ and β.
Figure 8: Adding MixCon to the 3rd layer of CNN on MNIST dataset. (a) The trade-off between data separa-bility and data utility . We show testing accuracy and mean pairwise distance (data separability) with differentλ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation of datarecovery results. We show SSIM and PSIM scores with different λ and β.
Figure 9: Adding MixCon to the 4th layer of CNN on MNIST dataset. (a) The trade-off between data separa-bility and data utility . We show testing accuracy and mean pairwise distance (data separability) with differentλ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation of datarecovery results. We show SSIM and PSIM scores with different λ and β.
Figure 10: Adding MixCon to the 1st layer of CNN on FashionMNIST dataset. (a) The trade-off betweendata separability and data utility . We show testing accuracy and mean pairwise distance (data separability)with different λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitativeevaluation of data recovery results. We show SSIM and PSIM scores with different λ and β.
Figure 11: Adding MixCon to the 2nd layer of CNN on FashionMNIST dataset. (a) The trade-off betweendata separability and data utility . We show testing accuracy and mean pairwise distance (data separability)with different λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitativeevaluation of data recovery results. We show SSIM and PSIM scores with different λ and β.
Figure 12: Adding MixCon to the 3rd layer of CNN on FashionMNIST dataset. (a) The trade-off betweendata separability and data utility . We show testing accuracy and mean pairwise distance (data separability)with different λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitativeevaluation of data recovery results. We show SSIM and PSIM scores with different λ and β.
Figure 13: Adding MixCon to the 4th layer of CNN on FashionMNIST dataset. (a) The trade-off betweendata separability and data utility . We show testing accuracy and mean pairwise distance (data separability)with different λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitativeevaluation of data recovery results. We show SSIM and PSIM scores with different λ and β.
Figure 14: Adding MixCon to the 1st layer of CNN on SVHN dataset. (a) The trade-off between data separa-bility and data utility . We show testing accuracy and mean pairwise distance (data separability) with differentλ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation of datarecovery results. We show SSIM and PSIM scores with different λ and β.
Figure 15: Adding MixCon to the 2nd layer of CNN on SVHN dataset. (a) The trade-off between dataseparability and data utility . We show testing accuracy and mean pairwise distance (data separability) withdifferent λ and β . λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluationof data recovery results. We show SSIM and PSIM scores with different λ and β.
Figure 16: Adding MixCon to the 3rd layer of CNN on SVHN dataset. (a) The trade-off between dataseparability and data utility . We show testing accuracy and mean pairwise distance (data separability) withdifferent λ and β . λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluationof data recovery results. We show SSIM and PSIM scores with different λ and β.
Figure 17: Adding MixCon to the 4th layer of CNN on SVHN dataset. (a) The trade-off between dataseparability and data utility . We show testing accuracy and mean pairwise distance (data separability) withdifferent λ and β . λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluationof data recovery results. We show SSIM and PSIM scores with different λ and β.
