Table 2: Model Architecture and Training Details Ablation. Text→Video retrieval performanceon MSR-VTT. Recall@1, 5, and Median Recall are shown.
Table 3: Retrieval performance on the MSR-VTT dataset. Models in the second group areadditionally pretrained on HowTo100M.
Table 4: Retrieval performance on the VATEX dataset	Text → Video				Video →Text				R@1f R@5f R@10f			MdRl	R@1f R@5↑ R@10f MdRi			Random Baseline	0.2	0.7	1.05	2000.5	0.02	0.1	1.02	2100.5VSE (Kiros et al., 2014)	28.0	64.3	76.9	3.0	-	-	-	-VSE++ (Faghri et al., 2018)	33.7	70.1	81.0	2.0	-	-	-	-Dual (Dong et al., 2019)	31.1	67.4	78.9	3.0	-	-	-	-HGR (Chen et al., 2020b)	35.1	73.5	83.5	2.0	-	-	-	-Ours	44.6	81.8	89.5	1.0	58.1	83.8	90.9	1.0Ours-pretrained	45.9	82.4	90.4	1.0	61.2	85.2	91.8	1.0Table 5: Retrieval performance on ActivityNetText →Video	Video →TextR@1f R@5f R@50f MdR∣ R@1f R@5 ↑ R@50f MdR∣Random Baseline	0.02	0.1	1.02	2458	0.02	0.1	1.02	2458FSE(Zhang et al., 2018)	18.2	44.8	89.1	7.0	16.7	43.1	88.4	7.0CE (Liu et al., 2019)	18.2	47.7	91.4	6.0	17.7	46.6	90.9	6.0HSE (Zhang et al., 2018)	20.5	49.3	-	-	18.7	48.1	-	-MMT (Gabeur et al., 2020)	22.7	54.2	93.2	5.0	22.9	54.8	93.1	4.3Ours	26.8	58.1	93.5	3.0	25.5	57.3	93.5	3.0MMT-pretrained (Gabeur et al., 2020) 28.7		61.4	94.5	3.3	28.9	61.1	94.3	4.0
Table 5: Retrieval performance on ActivityNetText →Video	Video →TextR@1f R@5f R@50f MdR∣ R@1f R@5 ↑ R@50f MdR∣Random Baseline	0.02	0.1	1.02	2458	0.02	0.1	1.02	2458FSE(Zhang et al., 2018)	18.2	44.8	89.1	7.0	16.7	43.1	88.4	7.0CE (Liu et al., 2019)	18.2	47.7	91.4	6.0	17.7	46.6	90.9	6.0HSE (Zhang et al., 2018)	20.5	49.3	-	-	18.7	48.1	-	-MMT (Gabeur et al., 2020)	22.7	54.2	93.2	5.0	22.9	54.8	93.1	4.3Ours	26.8	58.1	93.5	3.0	25.5	57.3	93.5	3.0MMT-pretrained (Gabeur et al., 2020) 28.7		61.4	94.5	3.3	28.9	61.1	94.3	4.0Ours-pretrained	29.2	61.6	94.7	3.0	28.7	60.8	94.8	2.0Table 6: Retrieval performance on the MSVD datasetText →Video	Video →TextR@1f R@5f R@10f Md用 R@1f R@5f R@10f Md用VSE (Kiros et al., 2014)	12.3	30.1	42.3	14.0	-	-	-	-VSE++ (Faghri et al., 2018)	15.4	39.6	53.0	9.0	-	-	-	-Multi. Cues (Mithun et al., 2018) 20.3		47.8	61.1	6.0	-	-	-	-CE (Liu et al., 2019)	19.8	49.0	63.8	6.0	-	-	-	-Ours	23.0	52.8	65.8	5.0	27.3	50.7	60.8	5.0Ours-pretrained	28.4	60.0	72.9	4.0	34.7	59.9	70.0	3.0
Table 6: Retrieval performance on the MSVD datasetText →Video	Video →TextR@1f R@5f R@10f Md用 R@1f R@5f R@10f Md用VSE (Kiros et al., 2014)	12.3	30.1	42.3	14.0	-	-	-	-VSE++ (Faghri et al., 2018)	15.4	39.6	53.0	9.0	-	-	-	-Multi. Cues (Mithun et al., 2018) 20.3		47.8	61.1	6.0	-	-	-	-CE (Liu et al., 2019)	19.8	49.0	63.8	6.0	-	-	-	-Ours	23.0	52.8	65.8	5.0	27.3	50.7	60.8	5.0Ours-pretrained	28.4	60.0	72.9	4.0	34.7	59.9	70.0	3.04.3	Comparison to State-of-the-ArtIn this section, we compare the results of our method to other recent text-to-video and video-to-textretrieval approaches on various datasets. In Tab. 3 to 5, we show the results of our model appliedto text-to-video and video-to-text retrieval on MSR-VTT, VATEX, ActivityNet and MSVD with andwithout pre-trainig on HowTo100M. Without pre-training, our method outperforms all others in allmetrics and datasets. In particular, for the VATEX dataset, our retrieval performance at recall at1 and 5 is 45.9% and 82.4%, exceeding recent state-of-the-art methods (Chen et al., 2020b) by amargin of 9%. For ActivityNet, our model outperforms MMT by a margin of4% at recall at 1. Withpre-training on HowTo100M, our performance further increases across the board. Notably, unlikeMMT which uses 7 features, our model uses only 2 features and achieves state-of-the-art in mostmetrics.
Table 7: Captioning performance on the MSR-VTT dataset	Captioning				BLUE4	METEOR	Rogue-L	CIDErVidTranslate (Korbar et al., 2020)	41.7	28.5	-	-POS+VCT (Hou et al., 2019)	42.3	29.7	62.8	49.1ORG (Zhang et al., 2020)	43.6	28.8	62.1	50.9Ours, MSR-VTT only	39.7	28.3	60.5	46.5Ours, HT100M + MSR-VTT	38.9	28.2	59.8	48.6Table 8: Captioning performance on the VATEX dataset	Captioning				Blue@4	METEOR	Rogue-L	CIDErShared Enc-Dec (Wang et al., 2019)	28.4	21.7	47.0	45.1ORG (Zhang et al., 2020)	32.1	22.2	48.9	49.7Ours, VATEX only	32.8	24.4	49.1	51.2Ours, HT100M + Vatex	32.5	24.1	48.9	50.516Published as a conference paper at ICLR 2021Table 9: Captioning performance on the ActivtyNet dataset	Captioning				Blue@4	METEOR	Rogue-L	CIDEr
Table 8: Captioning performance on the VATEX dataset	Captioning				Blue@4	METEOR	Rogue-L	CIDErShared Enc-Dec (Wang et al., 2019)	28.4	21.7	47.0	45.1ORG (Zhang et al., 2020)	32.1	22.2	48.9	49.7Ours, VATEX only	32.8	24.4	49.1	51.2Ours, HT100M + Vatex	32.5	24.1	48.9	50.516Published as a conference paper at ICLR 2021Table 9: Captioning performance on the ActivtyNet dataset	Captioning				Blue@4	METEOR	Rogue-L	CIDErDENSE (Krishna et al., 2017)	1.6	8.9	-	-DVC-D-A (Li et al., 2018)	1.7	9.3	-	-Bi-LSTM+TempoAttn (Zhou et al., 2018b)	2.1	10.0	-	-Masked Transformer (Zhou et al., 2018b)	2.8	11.1	-	-Ours, ActivityNet only	1.5	6.9	17.8	3.2Ours, HT100M + ActivityNet	1.4	6.9	17.5	3.16.5	Zero-Shot Retrieval ExperimentsWe also evaluate our model in the zero-shot setting on MSR-VTT, Vatex, ActivityNet and MSVD,
Table 9: Captioning performance on the ActivtyNet dataset	Captioning				Blue@4	METEOR	Rogue-L	CIDErDENSE (Krishna et al., 2017)	1.6	8.9	-	-DVC-D-A (Li et al., 2018)	1.7	9.3	-	-Bi-LSTM+TempoAttn (Zhou et al., 2018b)	2.1	10.0	-	-Masked Transformer (Zhou et al., 2018b)	2.8	11.1	-	-Ours, ActivityNet only	1.5	6.9	17.8	3.2Ours, HT100M + ActivityNet	1.4	6.9	17.5	3.16.5	Zero-Shot Retrieval ExperimentsWe also evaluate our model in the zero-shot setting on MSR-VTT, Vatex, ActivityNet and MSVD,after pre-training on HT100M. While we are able to get reasonable results on MSR-VTT and MSVD,our results are not great on Vatex and Activity-Net due to significant domain gap.
Table 10: Zero-shot Retrieval performance on VATEX, MSR-VTT, MSVD and ActivityNet.
Table 11: Action recognition. Results of training only a linear-layer, on features extracted from our fixed backbone with or without a learned transformer-pooling head. We compare to the state-of-art supervised and self-supervised pretrainig methods on the HMDB-51 and UCF-101 action recogni- tion task, for different downstream training protocols (“FT?” stands for finetuned). We report aver- age Top-1 accuracy across all 3 folds. Dataset abbreviations: AudioSet, HMDB51, HowTo100M, Instagram65M, IMagenet-1000, KinetiCS400, OmniSource Images + Videos, SportslM, UCF101, YoUTUbe8M. Other abbreviations: Video modality, Flow modality, Image modality, Audio modal- ity, Transformer pooling, Average pooling						Method	Mod	Dataset	Model	FT?	H51	U101Self-Supervised Pre-training						MIL-NCE (Miech et al., 2020)	V,T	HM	S3D-G	X	53.1	82.7MIL-NCE (Miech et al., 2020)	V,T	HM	S3D-G	✓	61.0	91.3MMV (Alayrac et al., 2020)	V,T,A	HM+AS	TSM-50x2	X	67.1	91.8ELo (Piergiovanni et al., 2020)	V,F,A	YT8M	R(2+1)D-50x3	✓	67.4	93.8XDC (Alwassel et al., 2020)	V,A	IG65M	R(2+1)D-18	✓	68.9	95.5GDT (Patrick et al., 2020)	V,A	IG65M	R(2+1)D-18	✓	72.8	95.2MMV (Alayrac et al., 2020)	V,T,A	HM+AS	TSM-50x2	✓	75.0	95.2Supervised Pre-training						P3D (Qiu et al., 2017)	V,I	S1M+IM	P3D	✓	-	88.6TSN (Wang et al., 2018)	V,I	IM	TSN	✓	69.4	94.2I3D (Carreira & Zisserman, 2017)	V,I	K400+IM	I3D	✓	74.8	95.6R(2+1)D (Tran et al., 2018)	V	K400	R(2+1)D-34	✓	74.5	96.8S3D-G (Xie et al., 2018)	V,I	K400+IM	S3D-G	✓	75.9	96.8I3D (Carreira & Zisserman, 2017)	V,I	K400+IM	I3D	✓	77.1	96.7R(2+1)D (Tran et al., 2018)	V	K400	R(2+1)D-34	✓	76.4	95.5R(2+1)D (Tran et al., 2018)	V,F	K400	R(2+1)D-34x2	✓	78.7	97.3Omni (Duan et al., 2020)	V,I	K400+OS	Slow-8x8-R101	✓	79.0	97.3
Table 12: Retrieval performance on the VATEX dataset	Text → Video				Video →Text				R@1f	R@5f	R@10f	MdRl	R@1f	R@5f	R@10f	MdRlRandom Baseline	0.2	0.7	1.05	2000.5	0.02	0.1	1.02	2100.5VSE (Kiros et al., 2014)	28.0	64.3	76.9	3.0	-	-	-	-VSE++ (Faghri et al., 2018)	33.7	70.1	81.0	2.0	-	-	-	-Dual (Dong et al., 2019)	31.1	67.4	78.9	3.0	-	-	-	-HGR (Chen et al., 2020b)	35.1	73.5	83.5	2.0	-	-	-	-Ours	44.9±0.2	82.1±0.2	89.7±o.2	1.0	58.4±0.1	84.4±0.2	91.0±0.3	1.018Published as a conference paper at ICLR 20216.8	Additional Qualitative ResultsWe provide addition qualitative text-to-video retrieval results on MSR-VTT, VATEX, ActivityNet inFig. 5. Given a text query, in most cases, our model successfully retrieves the correct videos markedin green.
