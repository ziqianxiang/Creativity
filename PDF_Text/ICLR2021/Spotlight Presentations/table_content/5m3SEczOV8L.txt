Table 1: IS and FID scores for unconditional generation on CIFAR-10.
Table 2: Generative performance on CelebA 64		Table 3: Generative Performance on CelebA HQ 256	Model	FID；		VAEBM (ours)	5.31	Model	FID；NVAE (Vahdat & Kautz)	14.74				VAEBM (ours)	20.38FloW CE (Gao et al.) Divergence Triangle (Han et al.)	12.21 24.7				NVAE (Vahdat & Kautz) GLOW (Kingma & Dhariwal)	45.11 68.93NCSNv2 (Song & Ermon)	26.86	Advers. LAE (Pidhorskyi et al.) PGGAN (Karras et al.)		19.21 8.03COCO-GAN (Lin et al.) QA-GAN (Parimala & ChannaPPayya)	4.0 6.42		reduce the gap between likelihood based models and GANs on this dataset. On LSUN Church 64, weobtain FID 13.51, which significantly improves the NVAE baseline FID 41.3. We show qualitativesamples in Fig. 3. Appendix H contains additional samples and MCMC visualizations.
Table 4: Comparison for IS and FID on CIFAR-10 between several related training methods.
Table 5: Mode coverage on StackedMNIST.
Table 6: Table for AUROC↑ of log p(x) computed on several OOD datasets. In-distribution datasetis CIFAR-10. Interp. corresponds to linear interpolation between CIFAR-10 images.
Table 7: Network structures for the energy function Eψ (x)CIFAR-103 X 3 conv2d,128ResBlock down 128ResBlock 128ResBlock down 256ResBlock 256ResBlock down 256ResBlock 256Global Sum PoolingFC layer → scalarCelebA 643 × 3 conv2d, 64ResBlock down 64ResBlock 64ResBlock down 128ResBlock 128ResBlock down 128ResBlock 256ResBlock down 256
Table 8: Important hyper-parameters for training VAEBMDataset	Learning rate	Batch size	Persistent	# of LD steps	LD Step sizeCIFAR-10 w/o persistent chain	4e-5	32	No	10	8e-5CIFAR-10 w/ persistent chain	4e-5	32	Yes	6	6e-5CelebA 64	5e-5	32	No	10	5e-6LSUN Church 64	4e-5	32	Yes	10	4e-6CelebA HQ 256	4e-5	16	Yes	6	3e-6We summarize some key hyper-parameters we used to train VAEBM in Table 8.
