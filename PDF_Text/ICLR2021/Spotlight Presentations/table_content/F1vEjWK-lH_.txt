Table 1: BLEU scores on the WMT dataset. The best result for multilingual model is bolded whileUnderline signifies the overall best, and * means the gains over baseline multilingual models arestatistically significant with p < 0.05.
Table 2: Average BLEU scores of 25 languagepairs on our massively multilingual dataset.
Table 3: F1 on the NER tasks of the XTREME benchmark.
Table 4: Details of all languages considered in our dataset. Notice that since German (Germanic)is particularly similar to French and Spanish (Romance), we consider a larger language branch forthem named “Western European”. “Ex-Low” indicates extremely low-resource languages in ourdataset. We use BCP-47 language codes as labels (Phillips & Davis, 2006).
Table 5: Details of all languages selected from WMT for gradient analysis.
Table 6: Comparing which tasks to be included for GradVac. Parameter granularity fixed at all」ayerwhile β=1e-2.
Table 7: Comparing parameter granularity for GradVac. GradVac tasks fixed at LRL_only whileβ=1e-2.
Table 8: Comparing EMA decay rate β for GradVac. Parameter granularity fixed at alLlayer andGradVac tasks fixed at LRL_only.
Table 9: F1 on the POS tasks of the XTREME benchmark.
