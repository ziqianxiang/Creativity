Table 1: Test accuracy of models with handcrafted ScatterNet features compared to prior results withend-to-end CNNs for various DP budgets (ε, δ = 10-5). Lower ε values provide stronger privacy.
Table 2: Effect of feature normalization on the test accuracy of non-private ScatterNet models after20 epochs. We also report the maximal test accuracy upon convergence (mean and standard deviationover five runs).
Table 3: Variability across hyper-parameters. For each model, we report the minimum, maximum,median and median absolute deviation (MAD) in test accuracy (in %) achieved for a DP budget of(ε = 3, δ = 10-5 ). The maximum accuracy below may exceed those in Table 1 and Figure 1, whichare averages of five runs. SN stands for ScatterNet.
Table 4: Number of trainable parameters of our models. For CIFAR-10, we consider two differentend-to-end CNN architectures (see Appendix C.2), the smaller of which has approximately as manyparameters as the linear ScatterNet model.
Table 5: Size of linear ScatterNet classifiers.
Table 6: End-to-end CNN model for MNIST andFashion-MNIST, with Tanh activations (Papernotet al., 2020b).
Table 7: End-to-end CNN model for CIFAR-10,with Tanh activations (Papernot et al., 2020b).
Table 8: CNN model fine-tuned on ScatterNetfeatures for MNIST and Fashion-MNIST, withTanh activations.
Table 9: CNN model on ScatterNet features forCIFAR-10, with Tanh activations. In Section 4,we also use a smaller variant of this model withfour convolutional layers of 16-16-32-32 filters.
Table 10: Hyper-parameters for evaluating the effect of feature normalization in Table 2.
Table 11: Test accuracy (in %) for models trained without privacy. Average and standard deviationare computed over five runs.
Table 12: Hyper-parameters for the evaluation of private linear classifiers fine-tuned on ScatterNetfeatures, CNNs fine-tuned on ScatterNet features, and end-to-end CNNs in Section 3.
Table 13: Set of hyper-parameters resulting in the highest test accuracy for a privacy budget of(ε = 3, δ = 10-5). Note that we report the base learning rate (LR), before scaling by a factor ofB/512. SN stands for ScatterNet.
Table 14: Hyper-parameters for the experiments on model convergence rates in Figure 3 and Figure 11.
Table 15: Hyper-parameters for the evaluation of private classifiers on larger datasets in Section 5.1.
Table 16: Set of hyper-parameters resulting in the highest test accuracy for a privacy budget of(ε = 3, δ = 1/2N). The test accuracy for these models are in Figure 4. Epochs are normalized by thesize of the original CIFAR-10 dataset, so training for T epochs corresponds to training on T ∙ 50,000examples. Note that we report the base learning rate, before scaling by a factor of 8192/512.
Table 17: Hyper-parameters for the evaluation of private transfer learning from CIFAR-100 (using aResNeXt model) and from unlabeled ImageNet (using a SimCLR v2 model) in Section 5.2.
Table 18:	Hyper-parameters for comparing the convergence rate of DP-SGD with different batchsizes in Figure 7.
Table 19:	Comparison of DP-SGD with two different batch sampling schemes: (1) Poisson sampling,where a batch is formed by selecting each data point independently with probability B/N ; (2) Randomshuffle, where the training set is randomly shuffled at the beginning of each epoch, and split intoconsecutive batches of size B . For both sampling schemes, we report the best test accuracy (in %) ata DP budget of (ε = 3, δ = 10-5), with means and standard deviations over five runs.
Table 20:	Best test accuracy (in %) for two different model sizes on CIFAR-10 for a DP budget of(ε = 3, δ = 10-5 ). We compare two variants of the end-to-end CNN architecture from Table 7, withrespectively 551K and 168K parameters. Average and standard deviation computed over five runs.
