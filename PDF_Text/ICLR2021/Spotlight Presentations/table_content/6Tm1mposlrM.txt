Table 1: Results for SAM on state-of-the-art models on CIFAR-{10, 100} (WRN = WideResNet;AA = AutoAugment; SGD is the standard non-SAM procedure used to train these models).
Table 2: Test error rates for ResNets trained on ImageNet, with and without SAM.
Table 3: Top-1 error rates for finetuning EfficientNet-b7 (left; ImageNet pretraining only) andEfficientNet-L2 (right; pretraining on ImageNet plus additional data, such as JFT) on various down-stream tasks. Previous state-of-the-art (SOTA) includes EfficientNet (EffNet) (Tan & Le, 2019),Gpipe (Huang et al., 2018), DAT (Ngiam et al., 2018), BiT-M/L (Kolesnikov et al., 2020), KD-forAA (Wei et al., 2020), TBMSL-Net (Zhang et al., 2020), and ViT (Dosovitskiy et al., 2020).
Table 4: Test accuracy on the clean test setfor models trained on CIFAR-10 with noisy la-bels. Lower block is our implementation, up-per block gives scores from the literature, perJiang et al. (2019).
Table 5: Results on SVHN and Fashion-MNIST.
Table 6: Hyper-parameter used to produce the CIFAR-{10,100} resultsCIFAR Dataset	LR	WD	ρ (CIFAR-10)	P (CIFAR-100)WRN 28-10 (200 epochs)	0.1	0.0005	0.05	01WRN 28-10(1800epochs)	0.05	0.001	0.05	0.1WRN 26-2x6 ShakeShake	0.02	0.0010	0.02	0.05Pyramid vanilla	0.05	0.0005	0.05	0.2Pyramid ShakeDrop (CIFAR-10)	0.02	0.0005	0.05	-Pyramid ShakeDroP (CIFAR-100)	0.05	0.0005	-	0.05Table 7: Hyper-parameter used to produce the SVHN and Fashion-MNIST results		LR		WD	PSVHN	WRN	0.01	0.0005	0.01	ShakeShake	0.01	0.0005	0.01Fashion	WRN	0.1	0.0005	0.05	ShakeShake	0.1	0.0005	0.02decayed using a cosine schedule. Weight decay is set to 0.0001 with SGD optimizer and momentum= 0.9.
Table 7: Hyper-parameter used to produce the SVHN and Fashion-MNIST results		LR		WD	PSVHN	WRN	0.01	0.0005	0.01	ShakeShake	0.01	0.0005	0.01Fashion	WRN	0.1	0.0005	0.05	ShakeShake	0.1	0.0005	0.02decayed using a cosine schedule. Weight decay is set to 0.0001 with SGD optimizer and momentum= 0.9.
Table 8: Validation accuracy of the bootstrapped-SAM for different levels of noise and different ρC.2 Finetuning DetailsWeights are initialized to the values provided by the publicly available checkpoints, except the lastdense layer, which change size to accomodate the new number of classes, that is randomly initial-ized. We train all models with weight decay 1e-5 as suggested in (Tan & Le, 2019), but we reducethe learning rate to 0.016 as the models tend to diverge for higher values. We use a batch size of1024 on Google Cloud TPUv3 64 cores and cosine learning rate decay. Because other works trainwith batch size of 256, we train for 5k steps instead of 20k. We freeze the batch norm statistics anduse them for normalization, effectively using the batch norm as we would at test time 11. We trainthe models using SGD with momentum 0.9 and cosine learning rate decay. For Efficientnet-L2, weuse this time a batch size 512 to save memory and adjusted the number of training steps accord-ingly. For CIFAR, we use the same autoaugment policy as in the previous experiments. We do notuse data augmentation for the other datasets, applying the same preprocessing as for the Imagenetexperiments. We also scale down the learning rate to 0.008 as the batch size is now twice as small.
Table 9: Results for the Cifar10/Cifar100 experiments, using ρ = 0.05 for all mod-els/datasets/augmentationsDataset	Efficientnet-b7 + SAM (optimal)	Efficientnet-b7 + SAM (ρ = 0.05)	Efficientnet-b7FGVC-Aircraft	680	706	8T5Flowers	0.63	0.81	1.16OXfOrd_IIIT_Pets	3.97	4.15	4.24Stanford.Cars	5.18	5.57	5.94cifar10	0.88	0.88	0.95cifar100	7.44	7.56	7.68Birdsnap	13.64	13.64	14.30Food101	7.02	7.06	7.17Table 10: Results for the the finetuning experiments, using ρ = 0.05 for all datasets.
Table 10: Results for the the finetuning experiments, using ρ = 0.05 for all datasets.
Table 11: Test error rate and estimated sharpness (max L(w + ) - L(w)) at the end of the training.
