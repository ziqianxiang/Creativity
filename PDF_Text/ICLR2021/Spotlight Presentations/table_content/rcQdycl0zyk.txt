Table 1: Experimental results of natural language inference (accuracy) on five different datasets.
Table 2: Experimental results of machine translation (BLEU) on seven different datasets. Symbolf represents re-scaling the parameters with a factor of 2 by doubling the hidden size. The PHM-transformer does not lose much performance despite enjoying parameter savings. Re-scaling canlead to improvement in performance.
Table 3: Training time (seconds per 100 steps) and inference time (seconds to decode test sets) withbeam size of 4 and length penalty of 0.6 on the IWSLT'14 German-English dataset.
Table 4: Experimental results of text style transfer. The PHM-transformer may reduce the parame-ters of the standard transformer model and improve performance.
Table 5: Experimental results of subject verb agreement. The PHM-transformer may reduce theparameters of the standard transformer model and improve performance.
