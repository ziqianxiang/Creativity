Table 1: Language model perplexity (lower is better) on the WikiText-103 development and testsets. Bolded numbers outperform Base.
Table 2: Machine translation test set BLEU. The decoding speed (last column) is relative to Base.
Table 3: Accuracy (higher is better) of different models on LO, IMDb, and AAN, along with theirspeed (higher is better) and peak memory consumption (lower is better) varying sequence lengths(1-4K). Speed and memory are evaluated on the IMDb dataset and relative to the transformer,s.
Table 4: Time and space complexity comparisons between Rfa and its softmax counterpart ina sequence-to-sequence attentive model, assuming an infinite amount of available threads. Mand N denote the lengths of the source and target sequences respectively. Teacher forcing train-ing (Williams & Zipser, 1989) and autoregressive decoding are assumed. Blue color indicates thecases where Rfa asymptotically outperforms softmax attention.
Table 5: Some statistics for the datasets. WikiText-103 split sizes are in number of tokens, whileothers are in number of instances.
Table 6: WMT14 EN-DE development set performance varying the number of random matrices tosample from during training. No beam search or checkpoint averaging is used.
Table 7: Hyperparameters used in the language modeling experiments.
Table 8: Hyperparameters used in the machine translation experiments.
Table 9: WMT14 EN-DE development set performance of RFA-Gaussian (the size of ฯ is 2D; ยง2.2)varying the random feature sizes. N/A indicates training does not converge. No beam search orcheckpoint averaging is used.
