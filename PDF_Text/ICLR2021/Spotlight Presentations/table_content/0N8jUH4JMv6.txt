Table 1: CNN architectures and the corresponding norm regularization in our convex programs							2-layer equation 4	2-layer equation 7	2-layer equation 21 1	2-layer equation 24 1	3-layer equation 11	L-layer equation 9 2Architecture Implicit Regularization	Pj,k (XkUj) + Wj P k∙k2	Pj maxpool ({(χkUj)+ }) Wj P k∙k2		Pj,k Xk UjWjk k ∙ k* (nuclear norm)	Pj (Pk (XkUj) + Wljk) + W2j 	k_k		Pj XUjwj P k∙kF	Pj (X Qi UljwIj)+w2j P k∙kιShallow CNNs and their representational power: As opposed to their relatively simple and shal-low architecture, CNNs with two/three layers are very powerful and efficient models. Belilovskyet al. (2019) show that greedy training of two/three layer CNNs can achieve comparable performanceto deeper models, e.g., VGG-11(Simonyan & Zisserman, 2014). However, a full theoretical under-standing and interpretable description of CNNs even with a single hidden layer is lacking in theliterature.
Table 2: Computational complexity results for training CNNs to global optimality using a standardinterior-point solver (n: # of data samples, d: data dimensionality, K: # of patches, rc : maximal rankfor the patch matrices (rc ≤ h), rcc: rank for the circular convolution, h: filter size ,m: # of filters)I 2-layer equation 4 ∣ 2-layer equation 7 ∣	L-layer equation 9	∣	3-layer equation 11# of variables	2hPco∣nv	2hPcοnv	4dPcconv	4dP1P2K# of constraints	2nPcοnvK	2nPcοnv K2	2nPcconv	2n(PιK + l)P2Complexity	O (h3r3 (等『I	O (h3r3 (等广)	O (d3r3c (公广)	O (d3m3r3 (mnc)3mr)(see Table 2). More importantly, we extend this analysis to three-layer CNNs with two ReLU layersto achieve polynomial time convex training as proven in Theorem 4.1.
