Table 1: The statistics of the three largest public benchmark datasets for LTR models.
Table 2: All numbers are significantly worse than the corresponding number from λMARTGBM atthe p < 0.05 level using a two-tailed t-test. Best performing numbers are bold.
Table 3: Result on the Web30K, Yahoo, and Istella datasets. ↑ means significantly better result,performanced against λMARTGBM at the p < 0.05 level using a two-tailed t-test. Last row isrelative difference of DASALC-ens over λMARTGBM.
Table 4: NDCG@5 on Istella when different components are added.
Table 5: Results on Web30K and Istella using log1p input transformation.
Table 6: Results on the Web30k and Istella datasets with standard feed-forward network architecture.
Table 7: Results on the Web30K and Istella datasets using self-attention and latent cross.
Table 8: Results on the Web30K datasets using different architecture and random noise strength.
Table 9: Results on the Yahoo datasets using different architecture and random noise strength.
Table 10: Comparison of Catboost with other methods on the Web30K, Yahoo, and Istella datasets.
Table 11: Results on the Istella datasets using LambdaMART ensembles.
