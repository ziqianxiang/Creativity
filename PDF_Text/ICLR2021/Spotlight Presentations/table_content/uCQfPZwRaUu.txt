Table 1: Performance of different methods on the 26 Atari games considered by Kaiser et al. (2019)after 100k environment steps. Results are recorded at the end of training and averaged over 10random seeds for SPR, 20 for CURL, and 5 for other methods. SPR outperforms prior methods on allaggregate metrics, and exceeds expert human performance on 7 out of 26 games.
Table 2: Scores on the 26 Atari games under consideration for ablated variants of SPR. All variantslisted here use data augmentation.
Table 3: Hyperparameters for SPR on Atari, with and without aug-mentation.
Table 4: Mean episodic returns on the 26 Atari games considered by Kaiser et al. (2019) after 100kenvironment steps. The results are recorded at the end of training and averaged over 10 random seeds.
Table 5: Scores on the 26 Atari games under consideration for our controlled Rainbow implementationwith and without augmentation, compared to previous methods. The high mean DQN-normalizedscore of our DQN without augmentation is due to an atypically high score on Private Eye, a hardexploration game on which the original DQN achieves a low score.
Table 6: Scores on the 26 Atari games under consideration for various contrastive alternatives to SPRimplemented in our codebase. All variants listed here use data augmentation.
Table 7: Scores on the 26 Atari games under consideration for variants of SPR with different targetencoder schemes, without augmentation.
Table 8: Wall-clock runtimes for various algorithms for a complete training and evaluation run on asingle Atari game using a P100 GPU. Rainbow (controlled) is roughly comparable to DrQ, althoughits runtime will differ due different DQN hyperparameters. Runtime for SimPLe is taken from its v3version on Arxiv, although the latest version doesnâ€™t mention runtime. All runtimes are approximate,as exact running times vary from game to game.
