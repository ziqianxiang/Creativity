Table 1: Micro F1 (InKB) on the in-domain test set and five out-of-domain test sets for the namedentity disambiguation task. Bold indicates best model and Underline indicates second best (notfor ablations). *WIKI is usually considered out-of-domain but note that all methods use a partof Wikipedia to train. **results taken from https://github.com/facebookresearch/BLINK and normalized to accommodate entities not in KB.
Table 2: Micro F1 (InKB) on the in-domain test set and four out-of-domain test sets for the entitylinking task. Bold indicates best model and Underline indicates second best. "annotated with coref-erence (note that We do not train/evaluate our model to link pronouns and common nouns). * resultsfrom the Wikipedia 2019 setting as opposed to the 2014 setting (older dUmp and fewer entities).
Table 3: R-Precision for page-level retrieval on KILT test data. Bold indicates the best model andUnderline indicates the second best. For our model, We indicated What datasets We used for training.
Table 5: Different types of matches betWeen men-tions and their entity names on the WNED-KILT.
Table 4: Comparison between retrieval mod-els on memory (disk space) footprint andnumber of model/index parameters.
Table 6: Evaluation of GENRE on WikilinksNED Unseen-Mentions data (Onoe & Durrett, 2020).
Table 7: Additional results on AIDA. We report Micro InKB F1 on test sets.
Table 8: Ablation study on KILT retrieval. We report R-Precision. GENRE only BLINK IDsdenotes training on BLINK (Wu et al., 2020) data where instead of using the textual entity represen-tation as target we used a numerical ID.
