Table 1: AUtomatic evaluation results (in %) on the CCSD test set.
Table 2: HUman evaluation results on the CCSD test set.
Table 3: Examples of generated summaries on the CCSD test set.
Table 4: Automatic evaluation results (in %) on the PCSD test set.			Methods	BLEU-4	ROUGE-L	METEORNNGen	21.60-	31.61	15.96CODE-NN	16.39	28.99	13.68Transformer	17.06	31.16	14.37Rencos	24.02	36.21	18.07HGNN w/o static	24.06-	38.28	18.66HGNN w/o dynamic	24.13	38.64	18.93HGNN	24.42	39.91	19.48Transformer, which are either retrieval-based, generation-based or hybrid methods. The results areshown in Table 4. The results indicate that, compared with the best results from NNGen, CODE-NN,Rencos and Transformer, our method can improve the performance by 0.40, 3.70 and 1.41 in terms ofBLEU-4, ROUGE-L and METEOR. We also conduct the ablation study on PCSD to demonstratethe usefulness of the static graph (i.e., HGNN w/o dynamic) and dynamic graph (i.e., HGNN w/ostatic). The results also demonstrate that both static graph and dynamic graph can contribute to ourframework. In summary, the results on both our released benchmark (CCSD) and existing benchmark(PCSD) demonstrate the effectiveness and the scalability of our method.
Table 5: More Examples of generated summaries on the CCSD test set.
