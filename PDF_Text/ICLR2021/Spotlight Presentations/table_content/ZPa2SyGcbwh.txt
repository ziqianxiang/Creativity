Table 1: Test accuracy (%) on CIFAR-10 and CIFAR-100 under different feature-dependent noisetypes and levels. The average accuracy and standard deviation over 3 trials are reported.
Table 2: Test accuracy (%) on CIFAR-10 and CIFAR-100 under different hybrid noise types andlevels. The average accuracy and standard deviation over 3 trials are reported.
Table 3: The effect of θ0 on the performance.
Table 4: The effect of β on the performance.
Table 5: Test accuracy (%) on Clothing1M.					Method	Standard Forward D2L JO PENCIL		DY GCE SL MLNT LRT		PLC (ours)Accuracy	68.94	69.84	69.47~72.23	73.49		71.00 69.75	71.02	73.47	71.74		74:02Table 6: Test accuracy (%) on Food-101N.			Table 7: Test accuracy (%) on ANIMAL-10N.		Method		Accuracy	Method		Accuracy	Standard		81:67	Standard	79.4 ± 0.14	CleanNet (Lee et al., 2018)		83.95	SELFIE (Song et al., 2019)	81.8 ± 0.09	PLC (ours)			85.28 ± 0.04	PLC (OUrS)		83.4 ± 0.43	5	ConclusionWe propose a novel family of feature-dependent label noise that is much more general than thetraditional i.i.d. noise pattern. Building upon this noise assumption, we propose the first data-recalibrating method that is theoretically guaranteed to converge to a well-behaved classifier. On thesynthetic datasets, we show that our method outperforms various baselines under different feature-dependent noise patterns subject to our assumption. Also, we test our method on different real-worldnoisy datasets and observe superior performances over existing approaches. The proposed noisefamily offers a new theoretical setting for the study of label noise.
Table 6: Test accuracy (%) on Food-101N.			Table 7: Test accuracy (%) on ANIMAL-10N.		Method		Accuracy	Method		Accuracy	Standard		81:67	Standard	79.4 ± 0.14	CleanNet (Lee et al., 2018)		83.95	SELFIE (Song et al., 2019)	81.8 ± 0.09	PLC (ours)			85.28 ± 0.04	PLC (OUrS)		83.4 ± 0.43	5	ConclusionWe propose a novel family of feature-dependent label noise that is much more general than thetraditional i.i.d. noise pattern. Building upon this noise assumption, we propose the first data-recalibrating method that is theoretically guaranteed to converge to a well-behaved classifier. On thesynthetic datasets, we show that our method outperforms various baselines under different feature-dependent noise patterns subject to our assumption. Also, we test our method on different real-worldnoisy datasets and observe superior performances over existing approaches. The proposed noisefamily offers a new theoretical setting for the study of label noise.
