Table 1: Definition of content-based vs position-based interactions.
Table 2: Alternatives for capturing long-range interactions. The lambda layer captures contentand position-based interactions at a reduced memory cost compared to relative attention (Shaw et al.,2018; Bello et al., 2019). Using a multi-query lambda layer reduces complexities by a factor of |h|.
Table 3: Comparison of the lambda layer and attention mechanisms on ImageNet classificationwith a ResNet50 architecture. The lambda layer strongly outperforms attention alternatives at afraction of the parameter cost. All models are trained in mostly similar setups (see Appendix F.3)and we include the reported improvements compared to the convolution baseline in parentheses. SeeAppendix C.4 for a description of the |u| hyperparameter. * Our implementation.
Table 4: The lambda layer reaches higher ImageNet accuracies while being faster and morememory-efficient than self-attention alternatives. Memory is reported assuming full precisionfor a batch of 128 inputs using default hyperparameters. The memory cost for storing the lambdasmatches the memory cost of activations in the rest of the network and is therefore ignored. b: batchsize, h: number of heads/queries, n: input length, m: context length, k : query/key depth, l: numberof layers.
Table 5: Comparison of models trained on extra data. ViT-L/16 is pre-trained on JFT and fine-tuned on ImageNet at resolution 384x384, while EfficientNet and LambdaResNet are co-trained onImageNet and JFT pseudo-labels. Training and inference throughput is shown for 8 TPUv3 cores.
Table 6: Complexity comparison between a multi-head and a multi-query lambda layer. Usinga multi-query formulation reduces complexity by a factor |h| (the number of queries per lambda)compared to the standard multi-head formulation.
Table 7: Complexity for a multi-query lambda layer with intra-depth |u|.
Table 8: Ablations on the ImageNet classification task when using the lambda layer in aResNet50 architecture. All configurations outpeform the convolutional baseline at a lower pa-rameter cost. As expected, we get additional improvements by increasing the query depth |k | orintra-depth |u|. The number of heads is best set to intermediate values such as |h|=4. A large num-ber of heads |h| excessively decreases the value depth |v| = d/|h|, while a small number of headstranslates to too few queries, both of which hurt performance.
Table 9: Contributions of content and positional interactions. As expected, positional interac-tions are crucial to perform well on the image classification task.
Table 10: Impact of varying the scope size for positional lambdas on the ImageNet classificationtask. We replace the 3x3 spatial convolutions in the last 2 stages of a ResNet-50 with lambda layers(input image size is 224x224). Flops significantly increase with the scope size, however we stressthat larger scopes do not translate to slower latencies when using the einsum implementation (seeFigure 3).
Table 11: Impact of normalization schemes in the lambda layer. Normalization of the keys alongthe context spatial dimension m, normalization of the queries along the query depth k.
Table 12: Hybrid models achieve a better speed-accuracy trade-off. Inference throughput andtop-1 accuracy as a function of lambda (L) vs convolution (C) layersâ€™ placement in a ResNet50architecture on 224x224 inputs. Lambda layers in the c5 stage incur almost no speed decreasecompared to standard 3x3 convolutions. Lambda layers in the c4 stage are relatively slower thanstandard 3x3 convolutions but yield significant accuracy gains.
Table 13: Impact of number of lambda layers in the c4 stage of LambdaResNets. Most benefitsfrom lambda layers can be obtained by having a few lambda layers in the c4 stage. Such hybriddesigns maximize the speed-accuracy tradeoff. LambdaResNet-C4 architectures exclusively employlambda layers in c4 and c5. LambdaResNet block configurations can be found in Table 19. Modelsare trained for 350 epochs on the ImageNet classification task.
Table 14: COCO object detection and instance segmentation with Mask-RCNN architectureon 1024x1024 inputs. We compare LambdaResNets against ResNets with or without squeeze-and-excitation (SE) and report Mean Average Precision (AP) for small, medium, large objects (APs/m/l).
Table 15: Parameter-efficiency comparison between LambdaResNet-C4 and EfficientNet-B6.
Table 16: Flops-efficiency comparison between LambdaResNet-C4 and EfficientNet-B6. Weuse smaller local scopes (|m|=7x7) to reduce FLOPS in the lambda layers. Models are trained for350 epochs.
Table 17: Lambda layers improve ImageNet accuracy in a resource-constrained scenario.
Table 18: Detailed LambdaResNets results. Latency refers to the time per training step for a batchsize of 1024 on 8 TPU-v3 cores using bfloat16 activations.
Table 19: Block configurations and lambda layers placement of LambdaResNets in the Paretocurves. LambdaResNets use the block allocations from He et al. (2016); Bello et al. (2021).
Table 20: Hyperparameters used to train LambdaResNets. We train for 350 epochs with Ran-dAugment, dropout and stochastic depth.
