Table 2: Update rule at each layer t, Ut J Ut 一 ηM-1dt. (Expectation taken over batch data)Methods	Precondition matrix Mt	Update direction dtSGD RMSprop KFAC & EKFAC vanilla DDP	It	,	E[Jυt] diag(PE[Jut。Juj + e)	E[jut] E[xtxT]⑥ E[Jht JTt]	E[Jut] 	E[Quu ]	E[Qu + Quχδxt]DDPNOpt	Mt ∈ I diag(PE[Qu <Ξ> Qu], + e) , I	E[Qu + Quχδxt] [	E[xtxT] % E[VtVh t]	propagation (recall Eq. 6 where we set ft ≡ σt ◦ gt), we can rewrite Eq. 3 as:Qx = gXTVt, Qu = 'U + gUTVt, Qxx = gXTVthgx , Qux = gUTVhhgx ,	(12)where Vht , σht T Vxt+1 and Vhth , σht T Vxtx+1 σht absorb the computation of the non-parametrizedactivation function σ. Note that Eq. 12 expands the dynamics only up to first order, i.e. we omitt thetensor products which involves second-order expansions on dynamics, as the stability obtained bykeeping only the linearized dynamics is thoroughly discussed and widely adapted in practical DDPusages (Todorov & Li, 2005). The matrix-vector product with the Jacobian of the affine transform(i.e. gut , gxt ) can be evaluated efficiently for both feedforward (FF) and convolution (Conv) layers:ht = Wtxt + bt ⇒ gxTVt	=	WTVh ,	guTVt =	Xt	③ Vt,	(13)ht = ωt * xt ⇒ gx Vt	=	ωT f Vt,	gu Vt =	xt	f Vh,	(14)where & ^ , and * respectively denote the Kronecker product and (de-)convolution operator.
Table 3: Performance comparison on accuracy (%). All values averaged over 10 seeds.
Table 4: Computational complexity in backward pass.
Table 5: Hyper-parameter searchMethods	Learning RateSGD	(7e-2, 5e-1)Adam & RMSprop	(7e-4, 1e-2)EKFAC	(1e-2, 3e-1)kernels for all CNNs. The batch size is set 8-32 for dataset trained with feedforward networks,14Published as a conference paper at ICLR 2021and 128 for dataset trained with convolution networks. For each baseline we select its own hyper-parameter from an appropriate search space, which we detail in Table 5. We use the implementation inhttps://github.com/Thrandis/EKFAC-pytorch for EKFAC and implement our ownE-MSA in PyTorch since the official code released from Li et al. (2017) does not support GPUimplementation. We impose the GN factorization presented in Proposition 3 for all CNN training.
Table 6: Learning rate = 0.1I SGD	DDPNOPt with Mt = ItTrain Loss	0.035	0.032Accuracy (%)	95.36	95.52	Table 7: Learning rate =	0.001	RMSprop	DDPNOpt with Mt	=diag(pE[QU Θ QU]' + E)Train Loss	0.058	0.052Accuracy (%)	94.33	94.63Table 8: Learning rate = 0.03E EKFAC DDPNOpt with Mt = E[xtxT]③ E[VtVt t]Train Loss	0.074	0.067Accuracy (%)	95.24	95.19Numerical absolute values in ablation analysis (DIGITS). Fig. 4a rePorts the relative Performancebetween each baseline and its DDPNOpt counterpart under different learning rate and regularizationsetups. In Table 9 and 10, we report the absolute numerical values of this experiment. For instance,the most left-upper grid in Fig. 4a, i.e.the training loss difference between DDPNOpt and SGD withlearning rate 0.4 and Vxx regularization 5 × 10-5, corresponds to 0.1974 - 0.1662 in Table 9. Allvalues in these tables are averaged over 10 seeds.
Table 8: Learning rate = 0.03E EKFAC DDPNOpt with Mt = E[xtxT]③ E[VtVt t]Train Loss	0.074	0.067Accuracy (%)	95.24	95.19Numerical absolute values in ablation analysis (DIGITS). Fig. 4a rePorts the relative Performancebetween each baseline and its DDPNOpt counterpart under different learning rate and regularizationsetups. In Table 9 and 10, we report the absolute numerical values of this experiment. For instance,the most left-upper grid in Fig. 4a, i.e.the training loss difference between DDPNOpt and SGD withlearning rate 0.4 and Vxx regularization 5 × 10-5, corresponds to 0.1974 - 0.1662 in Table 9. Allvalues in these tables are averaged over 10 seeds.
Table 9: Training Loss. (Vxx denotes the Tikhonov regularization on Vxx .)	SGD	DDPNOpt with Mt = It					EVxx =5 ×	10-5	1 × 10-4	5 × 10-4	1 × 10-30.4	0.1974	0.1662	0.1444	0.1322	0.10670.6	0.5809	0.4989	0.4867	0.3263	0.27640.7	1.0493	0.9034	0.8240	0.6592	0.53810.8	1.7801	1.6898	1.4597	1.1784	1.316616Published as a conference paper at ICLR 2021			RMSProP		DDPNOpt with Mt = diag(√E[QU Θ QU]' + e)								Mx = 1 X 10-9	1 X 10-8	5 X 10-6	1 X 10-5Learn Rate	0.01 0.02 0.03 0.045		0.1949 0.4691 0.8156 1.3103		0.1638 0.4559 0.7675 1.2740	0.1714 0.4489 0.7736 1.2956	0.1746 0.4390 0.7790 1.2568	0.1588 0.4773 0.7893 1.2758			EKFAC	DDPNOPt With Mt = E[xtxT]⑥ E[VtVtt] evχx = 1 X 10-7	5 X 10-7	5 X 10-6	1 X 10-5				J Leam Rate I		0.05 0.09 0.1 0.3	0.0757 0.2274 0.3260 0.5959		0.0636 0.2087 0.2771 0.5462	0.0659 0.2164 0.3003 0.5282	0.0691 0.2091 0.2543 0.5299	0.0717 0.2223 0.2510 0.5858Table 10: Accuracy (%). (Vxx denotes the Vxx regularization.)				SGD		DDPNOpt with Mt Vxx = 5 X 10-5 1 X 10-4 5			= It X 10-4	1	X 10-3Learn Rate			0.4	91.46			91.98	92.71	92.90	93.12			0.6 0.7	81.73 70.48			83.64 73.42	84.09 75.44	88.39 80.62	89.39 82.87			0.8	55.76			57.70	62.82	70.23	65.01										
Table 10: Accuracy (%). (Vxx denotes the Vxx regularization.)				SGD		DDPNOpt with Mt Vxx = 5 X 10-5 1 X 10-4 5			= It X 10-4	1	X 10-3Learn Rate			0.4	91.46			91.98	92.71	92.90	93.12			0.6 0.7	81.73 70.48			83.64 73.42	84.09 75.44	88.39 80.62	89.39 82.87			0.8	55.76			57.70	62.82	70.23	65.01														RMSprop			DDPNOpt with Mt = diag(PE[QU Θ QU]' + e) Vxx = 1 X 10-9 1 X 10-8 5 X 10-6 1 X 10-5			Learn Rate		0.01		91.48			92.14	91.80	91.73	92.52		0.02		84.15			84.82	85.02	85.23	83.00		0.03		73.07			75.24	75.73	74.29	74.16		0.045		59.80			59.16	60.98	61.75	59.87														EKFAC		DDPNOpt with Mt = E[xtXT]⑥ E[½t½t t] evxx = 1 X 10-7 5 X 10-7 5 X 10-6 1 X 10-5				Learn Rate		0.05		93.70			93.84	93.88	94.31	94.06		0.09		90.84			91.13	91.45	91.23	91.24			0.1	88.88			89.69	89.89	90.18	90.94			0.3	81.82			83.79	84.09	84.15	82.55More experiments on vanishing gradient. Recall that Fig. 6 reports the training performance usingMMC loss on Sigmoid-activated networks. In Fig. 10a, we report the result when training the samenetworks but using CE loss (notice the numerical differences in the y axis for different objectives).
