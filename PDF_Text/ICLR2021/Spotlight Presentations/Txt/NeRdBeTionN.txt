Published as a conference paper at ICLR 2021
On Self-Supervised Image Representations
for GAN Evaluation
Stanislav Morozov
Marchuk Institute of Numerical Mathematics RAS
stanis-morozov@yandex.ru
Andrey Voynov
Yandex
avoin@yandex-team.ru
Artem Babenko
Yandex
HSE University
artem.babenko@phystech.edu
Ab stract
The embeddings from CNNs pretrained on Imagenet classification are de-facto
standard image representations for assessing GANs via FID, Precision and Re-
call measures. Despite broad previous criticism of their usage for non-Imagenet
domains, these embeddings are still the top choice in most of the GAN literature.
In this paper, we advocate using state-of-the-art self-supervised representations to
evaluate GANs on the established non-Imagenet benchmarks. These representa-
tions, typically obtained via contrastive or clustering-based approaches, provide
better transfer to new tasks and domains, therefore, can serve as more universal
embeddings of natural images. With extensive comparison of the recent GANs
on the standard datasets, we demonstrate that self-supervised representations pro-
duce a more reasonable ranking of models in terms of FID/Precision/Recall, while
the ranking with classification-pretrained embeddings often can be misleading.
Furthermore, using self-supervised representations often improves the sample-
efficiency of FID, which makes it more reliable in limited-data regimes.
1	Introduction
Generative adversarial networks (GANs) are an extremely active research direction in machine
learning. The intensive development of the field requires established quantitative measures to assess
constantly appearing models. While a large number of evaluation protocols were proposed (Borji,
2019; Xu et al., 2018; Zhou et al., 2019; Naeem et al., 2020), there is still no consensus regarding the
best evaluation measure. Across the existing measures, the Frechet Inception Distance (FID) (HeUseI
et al., 2017) and PreciSion/Recall (Kynkaanniemi et al., 2019) are the most widely adopted due to
their simplicity and decent consistency with human judgments. FID and Precision/Recall quantify
the discrepancy between distributions of real and generated images. Since these distributions are
complicated to describe in the original RGB space, the images are represented by embeddings, typ-
ically extracted with CNNs pretrained on the Imagenet classification (Deng et al., 2009). While
FID computed with these embeddings was shown to correlate with human evaluation (Heusel et al.,
2017), these observations were mostly obtained on datasets, semantically close to Imagenet. Mean-
while, on non-Imagenet datasets, FID can result in inadequate evaluation, as widely reported in the
literature (Rosca et al., 2017; Barratt & Sharma, 2018; Zhou et al., 2019).
In this work, we propose to employ the state-of-the-art self-supervised models (Chen et al., 2020a;
He et al., 2020; Caron et al., 2020) to extract image embeddings for GAN evaluation. These models
were shown to produce features that transfer better to new tasks, hence, they become a promising
candidate to provide a more universal representation. Intuitively, classification-pretrained embed-
dings by design can suppress the information, irrelevant for the Imagenet class labels, which, how-
ever, can be crucial for other domains, like human faces. On the contrary, self-supervised models,
mostly trained via contrastive or clustering-based learning, do not have such a bias since their main
goal is typically to learn invariances to common image augmentations.
1
Published as a conference paper at ICLR 2021
To justify the usage of self-supervised embeddings, we perform a thorough comparison of the re-
cent GAN models trained on the five most common benchmark datasets. We demonstrate that
classification-pretrained embeddings can lead to incorrect ranking in terms of FID, Precision, and
Recall, which are the most popular metrics. On the other hand, self-supervised representations pro-
duce more sensible ranking, advocating their advantage over “classification-oriented” counterparts.
Since all the checkpoints needed to compute self-supervised embeddings are publicly available,
they can serve as a handy instrument for GAN comparison, consistent between different papers.
We release the code for the “self-supervised” GAN evaluation along with data and human labeling
reported in the paper online1. To sum up, the contributions of this paper are as follows:
1.	To the best of our knowledge, our work is the first to employ self-supervised image repre-
sentations to evaluate GANs trained on natural images.
2.	By extensive experiments on the standard non-Imagenet benchmarks, we demonstrate that
the usage of self-supervised representations provides a more reliable GAN comparison.
3.	We show that the FID measure computed with self-supervised representations often has
higher sample-efficiency and analyze the sources of this advantage.
2	Related work
GAN evaluation measures. Over the last years, a variety of quantitative GAN evaluation methods
have been developed by the community, and the development process has yet to converge since
all the measures possess specific weaknesses (Borji, 2019; Xu et al., 2018). The Inception Score
(Salimans et al., 2016) was the first widely adopted measure but was shown to be hardly applicable
for non-Imagenet domains (Barratt & Sharma, 2018). The Frechet Inception Distance (FID) (HeUsel
et al., 2017) quantifies dissimilarity of real and generated distributions, computing the Wasserstein
distance between their GaUssian approximations, and is cUrrently the most popUlar scalar measUre
of GAN's quality. Several recent measures were proposed (Sajjadi et al., 2018; Kynkaanniemi et al.,
2019; Naeem et al., 2020) that separately evalUate fidelity and diversity of GAN-prodUced images.
All of them mostly use the embeddings produced by the Imagenet classification CNN. A recent work
(Zhou et al., 2019) has introduced a human-in-the-loop measure, which is more reliable compared
to automated ones but cannot be used, e.g., for monitoring the training process. We focus on three
the most widely used measures: FID, Precision, and Recall, which are discussed briefly below.
Frechet Inception Distance quantifies the discrepancy between the distributions of real and gener-
ated images, denoted by pD and pG . Both pD and pG are defined on the high-dimensional image
space forming nontrivial manifolds, which are challenging to approximate by simple functions. To
be practical, FID operates in the lower-dimensional space of image embeddings. Formally, the em-
beddings are defined by a map f : RN → Rd, where N and d correspond to the dimensionalities of
the images and embeddings spaces, respectively. By design, FID measures the dissimilarity between
the induced distributions fpD, fpG as follows. First, fpD and fpG are approximated by Gaussian
distributions. Then the Wasserstein distance between these distributions is evaluated. As was shown
in (Dowson & Landau, 1982), for distributions defined by the means μ0 ,μG and the covariance
matrices ∑d, ∑g, this quantity equals to ∣∣μD - μG∣∣2 + tr(∑D + ∑g - 2(∑d∑g)2). Lower FID
values correspond to higher similarity between pG and pD; hence, can be used to evaluate the per-
formance of generative models. As a common practice in the FID computation, one typically uses
the activations from the InceptionV3 (Szegedy et al., 2016) pretrained on Imagenet classification.
Precision and Recall. When assessing generative models, it is important to quantify both the visual
quality of generated images and the model diversity, e.g., to diagnose mode collapsing. However, the
scalar FID values were shown (Sajjadi et al., 2018; Kynkaanniemi et al., 2019) to sacrifice diversity
in favor of visual quality, therefore FID cannot serve as the only sufficient metric. To this end,
(Sajjadi et al., 2018) introduced Precision and Recall, which aim to measure the image realism and
the model diversity, respectively. A recent follow-up (Kynkaanniemi et al., 2019) elaborates on these
metrics and proposes a reasonable procedure to quantify both precision and recall based only on the
image embeddings. In a nutshell, (Kynkaanniemi et al., 2019) assumes that the visual quality of a
particular sample is high if its embedding is neighboring for the embeddings of the real images. On
1https://github.com/stanis-morozov/self-supervised-gan-eval
2
Published as a conference paper at ICLR 2021
the other hand, a given real image is considered covered by the model if its embedding belongs to
the neighborhood of embeddings of the generated images.
Self-supervised representations. Self-supervised learning is currently attracting much research
attention, especially to contrastive learning and clustering-based methods (Chen et al., 2020a; He
et al., 2020; Caron et al., 2020). The common idea behind these methods is to construct representa-
tions that are invariant to a wide range of common image augmentations. The recent self-supervised
methods were shown to provide more transferrable (He et al., 2020; Caron et al., 2020) and ro-
bust (Hendrycks et al., 2019) features, which implies their usage as more universal representations.
In this paper, we show them being a better alternative compared to established classifier-produced
embeddings in the context of GAN assessment.
3	GAN evaluation
Here we systematically compare the publicly available GANs to highlight the cases of mislead-
ing comparison with classification-pretrained embeddings. Our goal is to demonstrate that self-
supervised embeddings are a better alternative in these cases, while in other cases, the rankings with
both types of embeddings are mostly consistent. We examine open-sourced GAN models2 trained
on five popular benchmarks:
•	CelebaHQ 1024x1024 (Karras et al., 2017) with the following GAN models: StyleGAN with
truncation 0.7 (Karras et al., 2019a) and without it, MSG (Karnewar & Wang, 2020) with trunca-
tion 0.6 and without it, PGGAN (Karras et al., 2017). To compute the metrics, we use 30k real
and synthetic images;
•	FFHQ 1024x1024 (Karras et al., 2019a) with the following GAN models: StyleGAN (Karras
et al., 2019a), StyleGAN2 (Karras et al., 2019b), MSG (Karnewar & Wang, 2020) with truncation
0.6 and without it. To compute the metrics, we use 30k real and synthetic images;
•	LSUN Bedroom 256x256 (Yu et al., 2015) with the following GAN models: StyleGAN (Karras
et al., 2019a) with truncation 0.7 and without it, PGGAN (Karras et al., 2017), COCO-GAN (Lin
et al., 2019), RPGAN (Voynov & Babenko, 2019), RPGAN with high diversity (RPGAN div.).
RPGAN generates 128x128 images, so we upscale them to 256x256. To compute the metrics, we
use 30k real and synthetic images;
•	LSUN Church 256x256 (Yu et al., 2015) with the models: StyleGAN2 (Karras et al., 2019b) with
truncation 0.5 and without it, MSG (Karnewar & Wang, 2020) with truncation 0.6 and without it,
PGGAN (Karras et al., 2017), SNGAN (Miyato et al., 2018). SNGAN generates 128x128 images,
so we upscale them to 256x256. To compute the metrics, we use 100k real and synthetic images;
•	Imagenet 128x128 (Deng et al., 2009) with the following GAN models: BigGAN (Brock et al.,
2019), BigGAN-deep (Brock et al., 2019) (both with truncation 2.0), S3GAN (Lucic et al., 2019),
Your Local GAN (YLG) (Daras et al., 2020). To compute the metrics, we use 50k images (50
per class). We include this dataset to demonstrate that for Imagenet, the proposed self-supervised
representations provide consistent ranking with commonly used InceptionV3 embeddings.
To compute image embeddings, we use the following publicly available models:
•	InceptionV3 (Szegedy et al., 2016) pretrained on the ILSVRC-2012 task (Deng et al., 2009);
•	Resnet50 (He et al., 2016) pretrained on the ILSVRC-2012 task. We include this model since
self-supervised models employ Resnet50, therefore, it is important to demonstrate that better GAN
ranking comes from the training objective rather than the deeper architecture;
•	Imagenet21k (Kolesnikov et al., 2019) pretrained on the multi-label classification task on approx-
imately 14M images from the full Imagenet. Kolesnikov et al. (2019) have shown that supervised
pretraining on huge datasets provides more transferrable features, therefore, Imagenet21k can
also potentially provide more universal representations. The model architecture is Resnet50;
•	SwAV (Caron et al., 2020) is the state-of-the-art self-supervised image representation model
trained on ILSVRC-2012. The idea of SwAV is to simultaneously cluster the images while en-
forcing consistency between cluster assignments produced for different augmentations of the same
image. The model architecture is Resnet50;
2The URLs for all models are provided in Appendix.
3
Published as a conference paper at ICLR 2021
Table 1: FID values computed with different embeddings. The ‘*’ symbol indicates models with
truncation. The inconsistencies between InceptionV3 and SwAV rankings are highlighted in color.
CELEBAHQ	FFHQ
InCePtionV3	StyleGAN 5.958	MSG 7.041	PGGAN 7.747	StyleGAN* 12.761	MSG* 18.845	StyleGAN2 MSG StyleGAN MSG* 3.355	6.560	6.896	22.552
Resnet50	StyleGAN	^^MSG^^	PGGAN	StyleGAN*	MSG*	StyleGAN2 MSG StyleGAN MSG*
	5.981	7.427	9.395	13.650	19.048	3.813	7.252	8.230	25.076
Imagenet21k	MSG	StyleGAN	PGGAN	StyleGAN*	MSG*	StyleGAN2 MSG StyleGAN MSG*
	295.3	301.3	451.6	635.5	874.4	177.7	334.7	389.1	1092
SwAV	MSG	StyleGAN StyleGAN*		MSG*	PGGAN	StyleGAN2 MSG StyleGAN MSG*
	1.206	1.304	1.473	1.832	1.898	0.634	1.275	1.482	2.461
DeePClusterV2	MSG	StyleGAN StyleGAN*		PGGAN	MSG*	StyleGAN2 MSG StyleGAN MSG*
	1.847	2.255	2.680	2.865	2.935	0.978	1.890	2.076	3.926
MoCoV2	MSG	StyleGAN	PGGAN	StyleGAN*	MSG*	StyleGAN2 MSG StyleGAN MSG*
	0.008	0.009	0.012	0.016	0.023	0.005	0.009	0.010	0.035
LSUN-BEDROOM
InCePtionV3	StyleGAN 2.986	PGGAN 8.658	StyleGAN* 9.655	COCO-GAN 18.612	RPGAN 37.924188	RPGAN div. 40.165
Resnet50	StyleGAN	PGGAN	StyleGAN*	COCO-GAN	RPGAN	RPGAN div.
	6.263	17.689	20.042	34.045	44.850	51.850
Imagenet21k	StyleGAN StyleGAN*		PGGAN	COCO-GAN RPGAN div.		RPGAN
	321.7	633.3	940.6	1270	1589	1593
SwAV	StyleGAN StyleGAN*		PGGAN	-RPGAN	COCO-GAN	RPGAN div.
	1.475	1.776	4.160	5.608	6.289	6.460
DeePClusterV2	StyleGAN StyleGAN*		PGGAN	^^RPGAN	COCO-GAN RPGAN div.	
	2.095	2.958	5.855	8.757	9.151	10.13
MoCoV2	StyleGAN StyleGAN*		PGGAN	RPGAN	RPGAN div. COCO-GAN	
	0.012	0.031	0.037	0.072	0.083	0.085
LSUN-CHURCH
InCePtionV3	StyleGAN2 3.652	MSG 5.009	PGGAN 6.296	MSG* StyleGAN2*		SNGAN 32.661
				13.854	24.966	
Resnet50	StyleGAN2	MSG	PGGAN	MSG*	StyleGAN2*	SNGAN
	6.650	9.673	10.850	22.670	55.571	56.114
Imagenet21k	StyleGAN2	MSG	PGGAN	MSG*	SNGAN	StyleGAN2*
	558.1	856.8	946.0	1305.3	1715	2165
SwAV	StyleGAN2 PGGAN		MSG	MSG*	StyleGAN2*	SNGAN
	1.898	3.233	3.578	4.062	5.194	6.043
DeePClusterV2	StyleGAN2 PGGAN		MSG	MSG*	SNGAN	StyleGAN2*
	2.599	4.149	4.586	5.395	7.830	8.612
MoCoV2	StyleGAN2	MSG	PGGAN	MSG*	SNGAN	StyleGAN2*
	0.019	0.030	0.030	0.048	0.067	0.083
•	DeepClusterV2 (Caron et al., 2020) is another self-supervised model obtained by alternating
between pseudo-labels generation via k-means clustering and training the network with a classifi-
cation loss supervised by these pseudo-labels. The model architecture is Resnet50;
•	MoCoV2 (Chen et al., 2020b) is the state-of-the-art contrastive learning approach, which training
objective enforces the closeness of representations produced for different augmentations of the
same image while pushing apart the representations of unrelated images. The model architecture
is Resnet50.
Three self-supervised models listed above outperform supervised pretraining on a number of transfer
tasks (He et al., 2020; Caron et al., 2020), which implies that their embeddings capture more infor-
mation relevant for these tasks, compared to supervised models pretrained on Imagenet. Below, for
a large number of publicly available GANs, we present the values of FID, Precision, and Recall met-
rics computed with different embeddings. For the cases where the GANs ranking is inconsistent, we
aim to show that the ranking obtained with the self-supervised representations is more reasonable.
4
Published as a conference paper at ICLR 2021
Table 2: Prediction accuracy of CelebaHQ attributes from InceptionV3 and SwAV embeddings.
Model Mouth Slightly Open No Beard High Cheekbones Smiling
InceptionV3	0.802	0.906	0.811	0.837
SwAV	0.868	0.938	0.851	0.893
3.1	FRECHET Inception Distance
The FID values for the non-Imagenet datasets computed with different embeddings are shown in
Table 1. The cases of inconsistent ranking with supervised inceptionV3 and self-supervised SwAV
embeddings are highlighted in color. The key observations are listed below:
(a)	On celebaHQ, SwAV ranks StyleGAN* higher, while inceptionV3/Resnet50 prefer PGGAN.
Figure 1 shows random samples from both StyleGAN* and PGGAN and clearly demonstrates the
superiority of StyleGAN*. To investigate the reasons why SwAV produces a more adequate rank-
ing compared to inception/Resnet50, we perform two additional experiments. (I) First, we verify
that SwAV embeddings capture more information relevant for face images. The celeba dataset (Liu
et al., 2018) provides labels of 40 attributes for each image, describing various person properties
(gender, age, hairstyle, etc.). For each attribute, we train 4-layer feedforward neural network with
2048 neurons on each layer with cross-entropy loss, which learns to predict the attribute from the
SwAV/inception embedding. For all attributes, the predictions from SwAV embeddings appear to
be more accurate compared to inceptionV3 (several examples are given in Table 2). it confirms
the intuition that inceptionV3 representations partially suppress the information about small facial
details, which, however, is critical to identify more realistic images. (II) As a qualitative experiment,
we compare SwAV and supervised Resnet50 embeddings visually via a recent technique described
in Rombach et al. (2020). in a nutshell, this technique reveals the invariances learned by the par-
ticular representation model: for a given image, it visualizes several images having approximately
the same embedding. By inspecting these images, one can analyze what factors of variations are not
captured in the embedding (see the details in Section A.2). Two illustrative examples of such visu-
alization for SwAV and Resnet50 are shown in Figure 2, demonstrating that Resnet50 embeddings
are more invariant to sensitive information, like gender or race, compared to SwAV. Such ignorance
of sensitive information makes supervised embeddings less appealing to use as universal representa-
tions. One of the key ingredients of the visualization method is an autoencoder, which is expected to
capture all relevant information from an image. However, we argue that autoencoder representations
are not well-suited for evaluating generative models and elaborate on this in detail in Section D.
(b)	On Bedroom, there are two inconsistencies in inceptionV3 and SwAV ranking. The first is that
SwAV ranks StyleGAN higher than PGGAN and the second is that SwAV ranks RPGAN higher
than cOcO-GAN. Figure 3 shows the samples from StyleGAN, PGGAN, RPGAN, and cOcO-
GAN and demonstrates that the ranking according to SwAV embeddings is more adequate. Namely,
the quality of StyleGAN-generated images is substantially higher. Also, it is difficult to identify
a favorite among RPGAN and COCO-GAN visually, while InCePtionV3 embeddings claim strong
superiority of the COCO-GAN model. On the other hand, self-supervised embeddings consider
these models as comparable, which is better aligned with human perception.
(C) There are also cases of the inconsistent ranking of MSG and PGGAN on Church, and StyleGAN
and MSG on CelebaHQ. But since the difference of the FID values are small for both InceptionV3
and SwAV, We do not consider it as a strong disagreement.
StyleGAN-CelebaHQ
PGGAN-CeIebaHQ
Figure 1: Samples generated by StyleGAN* and PGGAN trained on CelebaHQ. The quality of
images generated by StyleGAN* is substantially higher.
5
Published as a conference paper at ICLR 2021
Figure 2: The reference image is top-left denoted by a green frame, while 8 others constitute a
diverse sample of images that have approximately the same embeddings as the reference one.
(d)	The rankings with the supervised InCePtionV3/ReSnet50 embeddings are consistent with each
other while being different from the rankings with the self-supervised ones. It indicates that the
model architecture does not affect GAN ranking, and it is the training objective that matters.
(e)	Imagenet21k corrects some cases of misleading ranking with InceptionV3, but not all of them.
Namely, it correctly ranks StyleGAN and PGGAN on Bedroom while being wrong on CelebaHQ.
(f)	SwAV and DeepClusterV2 have minimal inconsistencies in the ranking of MSG* vs PGGAN on
CelebaHQ and StyleGAN2* vs SNGAN on Church, but the differences in the absolute values of the
FID metric are negligible, so we consider these embedding models as mostly consistent.
(g)	MoCoV2 fixes some ranking mistakes with InceptionV3, but not all of them. While it fixes the
ranking of StyleGAN and PGGAN on Bedroom and reduces the gap between RPGAN and COCO-
GAN, the ranking of PGGAN and StyleGAN on CelebaHQ is still incorrect. Overall, the most
reasonable rankings are obtained using SwAV/DeepCluster, which have significantly higher transfer
performance compared to MoCoV2. In further experiments, we focus on the most transferable
SwAV/DeepCluster models.
Overall, self-supervised embeddings provide a more reasonable FID ranking across existing non-
Imagenet benchmarks. For completeness, we also report the FID values for the Imagenet dataset in
Table 6. In this case, rankings with all embeddings are the same, which confirms that the SwAV
representations can be used for Imagenet as well, while it is not the main focus of our work.
3.2	Precision
The values of the Precision metric are reported in Table 3. The main observations are listed below:
StyleGAN-Bedroom
PGGAN-Bedroom
Figure 3: Samples generated by StyIeGAN*, PGGAN, RPGAN and COCO-GAN trained on Bed-
room. The quality of images generated by StyleGAN* is substantially higher, while the quality of
the images generated by RPGAN and COCO-GAN is approximately the same.
6
Published as a conference paper at ICLR 2021
Table 3: Precision (k=5) for different embedding models. The ‘*’ symbol indicates models with
truncation. Inconsistencies between InceptionV3 and SwAV models are highlighted in color.
CELEBAHQ	FFHQ
InceptionV3	MSG* 0.894	StyleGAN* 0.882	StyleGAN 0.803	MSG 0.7963	PGGAN 0.778	MSG* 0.843	StyleGAN2 StyleGAN		MSG 0.776
							0.778	0.778	
Resnet50	StyleGAN*	MSG*	StyleGAN	MSG	PGGAN	MSG*	StyleGAN2 StyleGAN MSG		
	0.921	0.916	0.857	0.839	0.839	0.865	0.821	0.807	0.789
Imagenet21k	StyleGAN*	MSG*	PGGAN	StyleGAN	MSG	StyleGAN2	MSG*	StyleGAN MSG	
	0.923	0.912	0.904	0.896	0.868	0.883	0.875	0.871	0.851
SwAV	MSG*	StyleGAN*	MSG	StyleGANPGGAN		MSG*	StyleGAN	StyleGAN2MSG	
	0.935	0.928	0.879	0.875	0.851	0.911	0.905	0.889	0.870
DeepClusterV2	MSG*	StyleGAN*StyleGAN PGGAN			MSG	MSG*	StyleGAN StyleGAN2 MSG		
	0.945	0.937	0.892	0.890	0.887	0.920	0.908	0.882	0.873
LSUN-BEDROOM
StyleGAN* StyleGAN InceptionV3 0.799	0.649	PGGAN 0.541	COCO-GAN RPGAN div. RPGAN 0.443	0.092	0.086
Resnet50 StyleGAN* StyleGAN Resnet50 0.849	0.722	PGGAN 0.646	COCO-GAN RPGAN div. RPGAN 0.596	0.545	0.515
StyleGAN* StyleGAN COCO-GAN Imagenet21k 。723	。653	。622		PGGAN RPGAN div. RPGAN 0.434	0.171	0.114
SAV StyleGAN* StyleGAN PGGAN COCO-GAN RPGAN div. RPGAN
SwAV 0.849	0.760	0.732	0.725	0.540	0.440
D^^	2 StyleGAN* StyleGAN PGGAN COCO-GAN RPGAN div.RPGAN
DeepClusterV2	0.817	0.728	0.722	0.706	0.540	0.423
LSUN-CHURCH
StyleGAN2*MSG* InCePtionV3	0.909	0.837	MSG 0.698	PGGAN 0.692	StyleGAN2SNGAN 0.689	0.193	
Resnet50StyleGAN2*	MSG* Resnet50 0.910	0.828	^^MSG^^ 0.679	StyleGAN2 0.672	PGGAN 0.639	SNGAN 0.403
Imnt21kStyleGAN2*StyleGAN2 Imagenet21k 0.567	0.492	MSG* 0.458	MSG 0.448	PGGAN 0.399	SNGAN 0.183
SWAVStyleGAN2*	MSG* 0.929	0.647	StyleGAN2 0.619	PGGAN 0.533	^^MSG^^ 0.491	SNGAN 0.465
^^^C , ∖,°StyleGAN2*^^MSG* DeepClusterV2 0.936	0.779	StyleGAN2 0.630	^^MSG^^ 0.593	PGGAN 0.589	SNGAN 0.438
(a)	As with FID, all supervised InceptionV3/Resnet50 embeddings provide the same ranking, except
minor differences between MSG with truncation and StyleGAN on CelebaHQ, and StyleGAN2 and
PGGAN on Church. Self-supervised SwAV and DeepClusterV2 are also consistent except for the
negligible difference in the ranking of PGGAN and MSG on CelebaHQ and Church;
(b)	The most notable inconsistency between supervised and self-supervised embeddings is revealed
on LSUN-Church, where InceptionV3 considers MSG to be comparable to StyleGAN2, while
SwAV ranks StyleGAN2 significantly higher. (I) To analyze which ranking of two GANs is more
reasonable, we perform the following. On the synthetic data from the first GAN, we train a classifier
that aims to distinguish between real and synthetic images. This classifier is then evaluated on the
synthetic data from the second GAN. Concretely, we train a classifier to detect synthetic images on
real LSUN-Church and the images generated by MSG. Then we evaluate this model on hold-out
real images and images produced by StyleGAN2. Intuitively, if a model was trained on high-quality
synthetic samples, it will easily detect lower-quality ones. On the other hand, if the model learns to
detect only low-quality synthetics, it will be harder to discriminate real images from the high-quality
ones. In this experiment, we employ a Resnet50 classifier with a binary cross-entropy loss. The re-
sults for Church are provided in Table 4, meaning that the StyleGAN2 images are of higher quality,
therefore, the SwAV ranking is more reasonable. (II) We also conduct a human study to determine
which of the generative models gives more realistic images in terms of human perception. For each
generative model, we show ten assessors a real or randomly generated (fake) image and ask them to
choose whether it is real or fake. The error rate reflects the visual quality of the generative model.
For both models, MSG and StyleGAN2, we demonstrate to assessors 500 images, and the error rate
is 0.4% for MSG and 2.8% for StyleGAN2, which clearly shows the superiority of StyleGAN2.
7
Published as a conference paper at ICLR 2021
Table 5: Recall (k=5) for different GAN and embedding models. The ‘*’ symbol indicates models
with truncation. Inconsistencies between InceptionV3 and SwAV models are highlighted in color.
	CELEBAHQ				FFHQ
-"	.∙ mMSGStyleGAN IncpnticnVV	' P	O.477 0.405	PGGAN 0.398	StyleGAN* 0.281	MSG* 0.276		StyleGAN2MSGStyleGANMSG* 0.632	0.538 0.474	0.339
Resnet50MSGStyleGAN Resnet500.502 0.419	PGGAN 0.330	StyleGAN* 0.284	MSG* 0.277		StyleGAN2MSGStyleGANMSG* 0.624	0.547 0.456	0.299
MSG StyleGAN Imagenet21k0.546 0.414	MSG* 0.321	StyleGAN* 0.315	PGGAN 0.287		StyleGAN2MSGStyleGANMSG* 0.688	0.551 0.452	0.335
MSGStyleGAN SWAV0.151 0.061	MSG* 0.055	StyleGAN* 0.035	PGGAN 0.027		StyleGAN2MSGStyleGANMSG* 0.399	0.193 0.095	0.063
Z^^C , SMSG MSG* DeepClusterV20.185 0.074	StyleGANStyleGAN* PGGAN 0.072	0.042	0.039				StyleGAN2MSGStyleGANMSG* 0.409	0.220 0.132	0.078
	LSUN-BEDROOM						
StyleGAN InceptionV3 0.592	PGGAN 0.516	COCO-GAN 0.476		StyleGAN* RPGANRPGANdiv. 0.397	0.190	0.110	
StyleGAN Resnet50 0.519	PGGAN 0.352	StyleGAN* 0.304		COCO-GANRPGANRPGAN div. 0.258	0.058	0.030	
ɪ	S1 I^StyleGANStyleGAN* PGGAN Imagenet21k 0.498	O.	0.240				COCO-GANRPGANRPGAN div. 0.157	0.008	0.003	
AV StyleGANStyleGAN* PGGAN SWAV 0.153	0.069	0.015				COCO-GANRPGANRPGAN div. 0.003	0.0003	0.00003	
DeePClusterV2StyleGANStyleGAN* PGGAN DeepClUsterV2 0.210	0.109	0.039				COCO-GANRPGANRPGAN div. 0.009	0.001	0.0	
LSUN-CHURCH
InceptionV3	StyleGAN2	MSG	PGGANMSG*		SNGAN	StyleGAN2*
	0.519	0.491	0.461	0.271	0.090	0.071
Resnet50	StyleGAN2 PGGAN		MSG	MSG*	SNGAN	StyleGAN2*
	0.460	0.413	0.406	0.182	0.038	0.021
Imagenet21k	StyleGAN2	MSG	PGGANMSG*		StyleGAN2*	SNGAN
	0.350	0.260	0.251	0.105	0.013	0.010
SWAV	StyleGAN2PGGAN		MSG	MSG*	StyleGAN2*	SNGAN
	0.073	0.019	0.008	0.002	0.0014	0.0002
DeepClusterV2	StyleGAN2 PGGAN		MSG	MSG*	SNGAN	StyleGAN2*
	0.112	0.058	0.031	0.009	0.002	0.002
Table 4: The accuracy of fake images detection on
Church. The rows correspond to GANs producing
the train synthetics, while the columns correspond
to GANs producing the test.
Train/Test MSG StyleGAN2
MSG^^0.999	0610
StyleGAN2^^0.967	0.979
(c) Imagenet21k ranks GANs less reliably com-
pared to SwAV. The most notable mistake is a
ranking of COCO-GAN and PGGAN on Bed-
room, where PGGAN produces more visually
appealing images, see Figure 3. Another case is
comparison on CelebaHQ, where Imagenet21k
ranks PGGAN higher than the more powerful
MSG, see samples in Section B.
3.3	Recall
The values of the Recall metric are shown in Table 5, and the main observations are provided below:
(a)	As in previous experiments, there are only minor inconsistencies between supervised Incep-
tionV3 and Resnet50 models, namely, StyleGAN vs COCO-GAN on Bedroom and MSG vs PG-
GAN on Church. The only insignificant difference between the self-supervised methods is the rank-
ing of StyleGAN with truncation vs SNGAN on Church, however, Recall values for both models
are negligible. In terms of Recall, Imagenet21k ranking always coincides with the ranking obtained
by self-supervised methods, except for the negligible discrepancy between MSG and PGGAN on
Church;
8
Published as a conference paper at ICLR 2021
(b)	The absolute Recall values for SwAV/DeepClusterV2 are smaller compared to Incep-
tionV3/Resnet50. We attribute this behavior to the fact that GANs tend to simplify images omitting
the details (Bau et al., 2019), e.g., people in front of buildings, cars, fences, etc. The classifier-
pretrained embeddings are less sensitive to these details since they are not crucial for correct classi-
fication. In contrast, self-supervised embeddings are more susceptible to small details (see Figure 2
and Table 4), hence, more images are considered “not covered”. Figure 6 in Section C shows exam-
ples of real LSUN-Church images that are “definitely covered” by StyleGAN2 from the standpoint
of InceptionV3 embeddings, but are “not covered” if SwAV embeddings are used. More formally,
we say that a real image is covered by a synthetic one with the neighborhood size k, if the distance
between their embeddings does not exceed the distance from the embedding of the synthetic image
to its k-th nearest neighbor in the set of all synthetic embeddings. The images from Figure 6 are
covered by at least 10 synthetic images with neighborhood size 5 with InceptionV3 embeddings,
while being not covered even by the neighborhood of size 100 for SwAV embeddings. These im-
ages possess many small details, such as monuments, cars, people, branches in the foreground, and
so on, that GANs usually omit to generate.
(c)	Interestingly, the Precision values for SwAV are
quite high, therefore, SwAV considers the state-
of-the-art generative models being able to produce
high-fidelity images, but failing to generate diverse
images with small details.
(d)	There are two significant inconsistencies in rank-
ings based on InceptionV3 and SwAV. First, on
CelebaHQ, SwAV prefers MSG* over PGGAN. Sec-
ond, on LSUN-Bedroom, InceptionV3 ranks PG-
GAN higher than StyleGAN. Since StyleGAN is a
more powerful model compared to PGGAN, we also
consider it as a case of a more reliable ranking with
SwAV, even though it is difficult to confirm quan-
titatively due to the lack of an “oracle” measure of
generation diversity.
Figure 4: FID values for different sample
sizes for StyleGAN2 on Church. Since FID
values for SwAV and InceptionV3 have dif-
ferent scales, they normalized by the FID
value computed for a sample of size 100k.
3.4	Sample efficiency of SwAV-based FID
As an additional practical advantage of SwAV, we highlight that computing FID becomes more
sample-efficient compared to InceptionV3. Namely, to obtain a reliable estimation of FID values,
one requires much fewer samples when using SwAV embeddings. We illustrate this effect in Fig-
ure 4, which plots FID values w.r.t. sample size for StyleGAN2 trained on Church. Since the FID
values for SwAV and InceptionV3 have different typical scales, we normalize both curves by the
corresponding FID value computed for a sample of size 100k. FID based on SwAV embeddings
converges faster, i.e., using SwAV always achieves more reliable FID estimates for a fixed sample
size.
We attribute this benefit of SwAV to the fact that its representations capture more information needed
to distinguish between real and fake distributions. Intuitively, the covariance matrices for real and
synthetic data computed from SwAV embeddings are more dissimilar compared to IncePtionV3-
based ones. Quantitatively, the magnitude of the covariance term in FID tr(CR + CS - 2√CrCs)
is larger for SwAV, which leads to smaller relative errors of its stochastic estimates. We elaborate
on this issue more rigorously in Section E.
4	Conclusion
In this paper, we have investigated if the state-of-the-art self-supervised models can produce more
appropriate representations for GAN evaluation. With extensive experiments, we have shown
that using these representations often corrects the cases of misleading ranking obtained with
classification-pretrained embeddings. Overall, self-supervised representations provide a more ad-
equate GAN comparison on the four established non-Imagenet benchmarks of natural images. Of
course, we do not claim that they should be used universally for all areas, e.g., for spectrograms
or medical images. But our work indicates that obtaining good representations needed for proper
GAN evaluation does not require supervision, therefore, domain-specific self-supervised learning
becomes a promising direction for further study.
9
Published as a conference paper at ICLR 2021
5	Acknowledgements
We thank the Anonymous Reviewers for their reviews. We also thank Xun Huang for commenting
on his experience with SwAV on OpenReview.
References
Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,
2018.
David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and An-
tonio Torralba. Seeing what a gan cannot generate. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 4502-4511, 2019.
Ali Borji. Pros and cons of gan evaluation measures. Computer Vision and Image Understanding,
179:41-65, 2019.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Giannis Daras, Augustus Odena, Han Zhang, and Alexandros G Dimakis. Your local gan: Design-
ing two dimensional local attention mechanisms for generative models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14531-14539, 2020.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
D.C. DoWson and B.V. Landau. The frechet distance between multivariate normal distributions.
Journal of multivariate analysis, 12(3):450-455, 1982.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning
can improve model robustness and uncertainty. In Advances in Neural Information Processing
Systems, pp. 15663-15674, 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Animesh Karnewar and Oliver Wang. Msg-gan: Multi-scale gradients for generative adversarial
networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pp. 7799-7808, 2020.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
10
Published as a conference paper at ICLR 2021
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition,pp. 4401-4410, 2019a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of stylegan. arXiv preprint arXiv:1912.04958, 2019b.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
arXiv:1912.11370, 2019.
TUomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. In Advances in Neural Information
Processing Systems, pp. 3929-3938, 2019.
Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-Tzong
Chen. COCO-GAN: generation by parts via conditional coordinating. In IEEE International
Conference on Computer Vision (ICCV), 2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba)
dataset. Retrieved August, 15:2018, 2018.
Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain Gelly.
High-fidelity image generation with fewer labels. arXiv preprint arXiv:1903.02271, 2019.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=B1QRgziT-.
Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable
fidelity and diversity metrics for generative models. arXiv preprint arXiv:2002.09797, 2020.
Stanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
14104-14113, 2020.
Robin Rombach, Patrick Esser, and Bjorn Ommer. Making sense of cnns: Interpreting deep repre-
sentations & their invariances with inns. arXiv preprint arXiv:2008.01777, 2020.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational
approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987,
2017.
Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assess-
ing generative models via precision and recall. In Advances in Neural Information Processing
Systems, pp. 5228-5237, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
Andrey Voynov and Artem Babenko. Rpgan: Gans interpretability via random routing. arXiv
preprint arXiv:1912.10920, 2019.
Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun, Felix Wu, and Kilian Weinberger.
An empirical study on evaluation metrics of generative adversarial networks. arXiv preprint
arXiv:1806.07755, 2018.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.
11
Published as a conference paper at ICLR 2021
Sharon Zhou, Mitchell Gordon, Ranjay Krishna, Austin Narcomey, Li F Fei-Fei, and Michael Bern-
stein. Hype: A benchmark for human eye perceptual evaluation of generative models. In Advances
in Neural Information Processing Systems, pp. 3449-3461, 2019.
A Appendix
A.1 Imagenet
Table 6: FID values for GANs on Imagenet.
IMAGENET
InceptionV3	BigGAN-deep BigGAN S3GAN YLG 6.923	7.659	10.85 20.64
Resnet50	BigGAN-deep BigGAN S3GAN YLG 11.22	19.50	20.05 34.11
Imagenet21k	BigGAN-deep BigGAN S3GAN YLG 334.8	601.5	1202 1697
SwAV	BigGAN-deep BigGAN S3GAN YLG 2.374	4.162	4.282 5.965
DeepClusterV2	BigGAN-deep BigGAN S3GAN YLG 3.937	6.240	6.555 9.101
A.2 The details of the visualization technique in Section 3.1
The technique from Rombach et al. (2020) proposes to construct the embedding visualization in
the following way. Let us have an autoencoder A(x), whose latent representations are denoted by
z. It is assumed that a latent representation z=A(x) captures all the information from the image
x, since the autoencoder’s goal is to fully reconstruct x from z. Let us denote the embedding
model we want to study by E(x). For a particular image x, Rombach et al. (2020) compute an
autoencoder representation z = A(x) and aims to disentangle the information contained in the
autoencoder representation z into the information contained in the image embedding e = E(x) and
the information v to which the embedding model E(x) is invariant. To this end, an invertible neural
network (INN) is trained, which predicts the invariant part v = INN(z, e) based on the autoencoder
representation and embedding. Moreover, it is trained in such a way that the invariant part has
a normal distribution V 〜N(0,1). Finally, a bunch of images having the same e but different
invariant parts v are produced. Namely, several samples of invariant parts vb are produced from
N(0, 1) and the corresponding latent representations are computed from vb and embedding e, that is
zb = INN.reverse(v, e). Then zb can be decoded into an image using the autoencoder. As a result,
for a given embedding model E(x) and a given image x, it allows to obtain a set of images with
approximately the same embeddings but varying invariant part. For the CelebaHQ dataset, We use
ALAE autoencoder (PidhorSkyi et al., 2020) and train INN with the hyperparameters provided in
(RombaCh et al., 2020).
B	CelebaHQ samples
MSG-CelebaHQ
PGGAN-CeIebaHQ
Figure 5: Random sample of images generated by MSG without truncation and PGGAN trained on
CelebaHQ dataset. One can see that the quality of images generated by MSG is substantially higher.
12
Published as a conference paper at ICLR 2021
C Church images
Figure 6: Examples of real images that are confidently covered by StyleGANv2 in terms of Incep-
tionV3 embeddings, but not covered in terms of SwAV.
D Autoencoder embeddings
Figure 7: Examples of nearest neighbors in terms of SWAV and ALAE representations.
In this section, we compare SwAV and ALAE autoencoder (PidhorSkyi et al., 2020) embeddings.
ALAE was trained on Celeba (Liu et al., 2018) dataset, therefore, is expected to work correctly
for the ranking of generative models on CelebaHQ. To investigate what information is important
for each of the embeddings, we build a dataset containing 30k real images from CelebaHQ and
30k synthetic images generated by PGGAN. Then we select 1.5k real and 1.5k synthetic images as
queries and leave the remaining 57k pictures as a database. For each query, we compute three nearest
neighbors from the database. Typical examples of the nearest neighbors are shown on Figure 7.
13
Published as a conference paper at ICLR 2021
One can see that ALAE places a strong emphasis on the exact spatial arrangement, while sparsely
sampled manifolds rarely include near-exact matches in terms of spatial structure (Kynkaanniemi
et al., 2019). This makes ALAE embeddings poor for GAN evaluation, for instance, via Precision
and Recall metrics. We also perform real/fake classification of queries using 1-NN classification
on the constructed database. The classification accuracy is 0.729 for ALAE embeddings and 0.839
for SwAV. Overall, the distance in the space of the autoencoder’s embeddings is less informative to
distinguish between the real/fake distributions.
(2)
(3)
(4)
E	On the estimate of FID values
Let us denote CR the covariance matrix for real data, CS for synthetic data and CeR and CeS their
estimates with a finite number of samples. We also denote
d(CR,Cs) = tr(CR + CS - 2,CRCS)	(1)
In (Dowson & Landau, 1982) it was shown that d(CR, CS) defines a metric on the space of all
covariance matrices of order n. Our goal is to assess the relative error of the FID estimate. Under
the assumption that the means of real and synthetic data distributions are estimated quite accurately
(which is the case already for a few thousand of samples) we need to estimate only
d(CR,CS) -dCeR,CeS
d(CR, CS)
Due to the metric properties
dCeR,CeS ≤dCeR,CR +d(CR,CS)+dCS,CeS
Then
d (Cr,Cs) — d(CR,CS) < d (Cr,CR) + d (CS,CS)
d(CR,Cs)	≤	dCRCS
Due to the symmetry the same inequality can be obtained with the opposite sign and as the result we
get
d(CeR,CeS) -d(CR,CS)	d(CeR,CR) +d(CeS,CS)
d(CR, CS)	≤	d(CR, CS)	,	⑸
where the numerator corresponds to the accuracy of the estimation of covariance matrices and the
denominator corresponds to the distance between covariance matrices for real and synthetic data.
Thus, larger values of d(CR, CS) result in a smaller relative error of the FID estimation. Experi-
mentally, We compute the distances d(Cr, CS) = tr(CR + CS - 2√CRCS) based on SwAV and
InceptionV3 embeddings for StyleGAN2 trained on the Church dataset.3 We obtain 0.083 for SwAV
and 0.028 for InceptionV3, which confirms the validity of calculations above and explains the better
sample-efficiency of SwAV presented in Figure 4.
F Human evaluation
As an additional evidence that inter-image distances induced by SwAV are better aligned with human
perception compared to InceptionV3, we perform two crowdsourcing experiments. All the data and
labellings are released on the GitHub.4.
SwAV-based Precision and Recall have higher agreement with human judgements. The key
step of computing Recall is checking, if for a given real embedding r there exists a generated em-
bedding g that is closer to r than its k-th real neighbor. To verify, if a particular embedding agrees
well with human perception, we perform the following procedure. We form a triplet of an anchor
3Since the absolute values of d(CR, CS) depend on the scale of SwAV/InceptionV3 activations, we nor-
malize them by the geometric mean of norms CR and CS .
4https://github.com/stanis-morozov/self-supervised-gan-eval
14
Published as a conference paper at ICLR 2021
	CELEBAHQ, StyleGAN*		CELEBAHQ, PGGAN	
	Precision	Recall	Precision	Recall
InceptionV3	-0.396	-0.497	-0.399	-0.482
SwAV	0.418	0.525	—	0.482	0.479
	LSUN-Bedroom, StyleGAN*		LSUN-Bedroom, PGGAN	
	Precision	Recall	Precision	Recall
InceptionV3	-0.417	-0.522	-0.375	^0444
SwAV	0.450	0.530	0.446	0.429	—
	LSUN-Church, MSG		LSUN-Church, PGGAN	
	Precision	Recall	Precision	Recall
InceptionV3	-0.386	-0.473	-0.400	-0.449
SwAV	0.477	0.497	0.445	0.458
Table 7: Human evaluation of the precision and recall agreement with different embeddings (higher
is better).
real image Ianchor that contributes to the Recall value, its 5-th closest neighbor among the real im-
ages I5th and the generated image Igen that appears to be closer to Ianchor in terms of the considered
embedding. A human assessor is then asked to choose an image between Igen and I5th that is
more similar to Ianchor . Once the assessor chooses the generated one, we consider it as a case of
agreement with the embedding. The embeddings with higher agreement rate are more suitable for
computing Recall.
For Precision, we similarly form the triplets consisting ofa generated image, its 5-th neighbor among
the generated images and a real image Ireal closer to it Ianchor, I5th, Ireal. Once an assessor answers
that Ireal is more similar to Ianchor then I5th, we consider this is as an agreement with the embedding.
Here we always use the same real and generated samples as for the evaluation of the metrics in
Section 3. We label three datasets with two GAN models for each. For each pair of a dataset and
a generator, we label 200 different triplets, each by ten different assessors. An assessor is also able
to choose the options “equally similar” or “both completely dissimilar”. Once the “equally similar”
is chosen, we suppose that the agreement happens with the probability 0.5. The user interface is
illustrated on Figure 8 (left). All the labeling was performed in Yandex Toloka 5. The results are
presented on Table 7 and confirm that SwAV emebddings mostly have higher agreement with human
perception.
Quality of neighbors. As a more simple experiment, we also ask human assessors to compare
quality of top-5 neighbors produced by InceptionV3 and SwAV embeddings. Namely, we take a set
of N real images I, same as in Section 3. For a given real image r ∈ I we form two lists of its
5 nearest neighbors BIV3 ⊂ I and BSwAV ⊂ I based on InceptionV3 and SwAV embeddings. An
assessor is asked to assign r either to BIV3 or to BSwAV . Same as above, the assessor may also label
it as “equal” which is treated as an equal probability of each set to be chosen. The user interface
is illustrated on Figure 8 (right). For each dataset we form 500 different triplets r, BSwAV , BIV3,
each labeled by ten different assessors. Table 8 presents probabilities that assessors prefer a SwAV-
based set of neighbors, indicating that SwAV-induced distances better capture perceptual similarity
compared to InceptionV3.
	InCePtionV3	SwAV
CELEBAHQ~~	0.37	^063-
LSUN-Bedroom	-0.39	^061-
LSUN-ChuTch-	0.41	—	0.59
Table 8: Comparison of top-5 neighbor lists quality based on SwAV/InceptionV3 representations.
5https://toloka.ai
15
Published as a conference paper at ICLR 2021
Figure 8: User interface for precision / recall agreement labeling (left) and top-5 neighbours labeling
(right).
model	Source	I truncation
	CELEBAHQ			
StyleGAN	https://github.com/NVlabs/stylegan	-
StyleGAN*	https://github.com/NVlabs/stylegan	0.7
^SG	https://github.com/akanimax/msg-stylegan-tf	-
MSG*	https://github.com/akanimax/msg-stylegan-tf	0.6
PGGAN	https://github.com/tkarras/progressive_growing_ of_gans	-
FFHQ		
StyleGAN2	https://github.com/NVlabs/stylegan2	-
StyleGAN	https://github.com/rosinality/style-based-gan- pytorch	-
^SG	https://github.com/akanimax/msg-stylegan-tf	-
MSG*	一	https://github.com/akanimax/msg-stylegan-tf	0.6
LSUN-Bedroom		
StyleGAN	https://github.com/NVlabs/stylegan	-
StyleGAN*	https://github.com/NVlabs/stylegan	0.7
PGGAN	https://github.com/tkarras/progressive_growing_ of_gans	-
RPGAN	https://github.com/anvoynov/RandomPathGAN, model: generator_lsun_2	-
RPGAN-div	https://github.com/anvoynov/RandomPathGAN, model: generator_lsun_high_diversity	-
COCO-GAN	https://github.com/hubert0527/COCO-GAN/tree/ 12b90e26e23214c2072c9701644e9724e052743c, model: LSUN_2 5 6x2 5 6_N2M2S12 8		-
LSUN-ChUrCh		
StyleGAN2	https://github.com/NVlabs/stylegan2	-
StyleGAN2*	https://github.com/NVlabs/stylegan2	0.5
PGGAN	https://github.com/tkarras/progressive_growing_ of_gans	-
^SG	https://github.com/akanimax/msg-stylegan-tf	-
MSG*	https://github.com/akanimax/msg-stylegan-tf	0.6
SNGAN 一	Submission authors implementation	-
Table 9: The URLs with GAN checkpoints and truncation levels used in our experiments.
16
Published as a conference paper at ICLR 2021
Representation	Dim	URL	Checkpoint
InceptionV3	2048	https://github. com/mseitzer/ pytorch-fid	https://github.com/ mseitzer/pytorch-fid/ releases/download/fid_ weights/pt_inception-2015- 12-05-6726825d.pth
Resnet-50	2048	https://pytorch. org/docs/stable/ torchvision/ models.html	https://download.pytorch. org/models/resnet50- 19c8e357.pth
Imagenet21k	2048	https://tfhub. dev/google/bit/m- r50x1/1	https://tfhub.dev/google/ bit/m-r50x1/1
SwAV	2048	https:// github.com/ facebookresearch/ swav	https://dl.fbaipublicfiles. com/deepcluster/swav_800ep_ pretrain.pth.tar
DeepClusterV2	2048	https:// github.com/ facebookresearch/ swav	https://dl.fbaipublicfiles. com/deepcluster/ deepclusterv2_800ep_ pretrain.pth.tar
MoCoV2	2048	https:// github.com/ facebookresearch/ moco	https://dl.fbaipublicfiles. com/moco/moco_checkpoints/ moco_v2_800ep/moco_v2_ 800ep_pretrain.pth.tar
Table 10: The URLs with checkpoints used in our experiments.
17