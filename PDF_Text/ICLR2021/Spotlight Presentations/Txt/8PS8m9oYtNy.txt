Published as a conference paper at ICLR 2021
Implicit Normalizing Flows
Cheng Lu*, Jianfei Chen*, Chongxuan Lit, Qiuhao Wangr Jun Zhut*
±Dept. of Comp. Sci. & Tech., Institute for AL BNRist Center
*Tsinghua-Bosch Joint ML Center, THBI Lab,Tsinghua University, Beijing, 100084 China
^Center for Data Science, Peking University, Beijing, 100871 China
{lucheng.lc15,chris.jianfei.chen,chongxuanli1991}@gmail.com,
dcszj@tsinghua.edu.cn, wqh19@pku.edu.cn
Ab stract
Normalizing flows define a probability distribution by an explicit invertible trans-
formation z = f (x). In this work, we present implicit normalizing flows
(ImpFlows), which generalize normalizing flows by allowing the mapping to be
implicitly defined by the roots of an equation F (z, x) = 0. ImpFlows build
on residual flows (ResFlows) with a proper balance between expressiveness and
tractability. Through theoretical analysis, we show that the function space of
ImpFlow is strictly richer than that of ResFlows. Furthermore, for any ResFlow
with a fixed number of blocks, there exists some function that ResFlow has a non-
negligible approximation error. However, the function is exactly representable by
a single-block ImpFlow. We propose a scalable algorithm to train and draw sam-
ples from ImpFlows. Empirically, we evaluate ImpFlow on several classification
and density modeling tasks, and ImpFlow outperforms ResFlow with a compara-
ble amount of parameters on all the benchmarks.
1	Introduction
Normalizing flows (NFs) (Rezende & Mohamed, 2015; Dinh et al., 2014) are promising methods for
density modeling. NFs define a model distribution px (x) by specifying an invertible transformation
f (x) from x to another random variable z. By change-of-variable formula, the model density is
ln px (x) = lnpz(f(x)) + ln |det(Jf (x))| ,
(1)
where pz (z) follows a simple distribution, such as Gaussian. NFs are particularly attractive due to
their tractability, i.e., the model density px (x) can be directly evaluated as Eqn. (1). To achieve such
tractability, NF models should satisfy two requirements: (i) the mapping between x and z is invert-
ible; (ii) the log-determinant of the Jacobian Jf (x) is tractable. Searching for rich model families
that satisfy these tractability constraints is crucial for the advance of normalizing flow research. For
the second requirement, earlier works such as inverse autoregressive flow (Kingma et al., 2016) and
RealNVP (Dinh et al., 2017) restrict the model family to those with triangular Jacobian matrices.
More recently, there emerge some free-form Jacobian approaches, such as Residual Flows (Res-
Flows) (Behrmann et al., 2019; Chen et al., 2019). They relax the triangular Jacobian constraint by
utilizing a stochastic estimator of the log-determinant, enriching the model family. However, the
Lipschitz constant of each transformation block is constrained for invertibility. In general, this is
not preferable because mapping a simple prior distribution to a potentially complex data distribution
may require a transformation with a very large Lipschitz constant (See Fig. 3 for a 2D example).
Moreover, all the aforementioned methods assume that there exists an explicit forward mapping
z = f (x). Bijections with explicit forward mapping only covers a fraction of the broad class of
invertible functions suggested by the first requirement, which may limit the model capacity.
In this paper, we propose implicit flows (ImpFlows) to generalize NFs, allowing the transformation to
be implicitly defined by an equation F(z, x) = 0. Given x (or z), the other variable can be computed
by an implicit root-finding procedure Z = RootFind(F(∙, x)). An explicit mapping Z = f (x) used
in prior NFs can viewed as a special case of ImpFlows in the form of F(z, x) = f(x) - z =
* Corresponding Author.
1
Published as a conference paper at ICLR 2021
0. To balance between expressiveness and tractability, we present a specific from of ImpFlows,
where each block is the composition of a ResFlow block and the inverse of another ResFlow block.
We theoretically study the model capacity of ResFlows and ImpFlows in the function space. We
show that the function family of single-block ImpFlows is strictly richer than that of two-block
ResFlows by relaxing the Lipschitz constraints. Furthermore, for any ResFlow with a fixed number
of blocks, there exists some invertible function that ResFlow has non-negligible approximation error,
but ImpFlow can exactly model.
On the practical side, we develop a scalable algorithm to estimate the probability density and its
gradients, and draw samples from ImpFlows. The algorithm leverages the implicit differentiation
formula. Despite being more powerful, the gradient computation of ImpFlow is mostly similar with
that of ResFlows, except some additional overhead on root finding. We test the effectiveness of
ImpFlow on several classification and generative modeling tasks. ImpFlow outperforms ResFlow
on all the benchmarks, with comparable model sizes and computational cost.
2	Related Work
Expressive Normalizing Flows There are many works focusing on improving the capacity of NFs.
For example, Dinh et al. (2014; 2017); Kingma & Dhariwal (2018); Ho et al. (2019); Song et al.
(2019); Hoogeboom et al. (2019); De Cao et al. (2020); Durkan et al. (2019) design dedicated
model architectures with tractable Jacobian. More recently, Grathwohl et al. (2019); Behrmann
et al. (2019); Chen et al. (2019) propose NFs with free-form Jacobian, which approximate the de-
terminant with stochastic estimators. In parallel with architecture design, Chen et al. (2020); Huang
et al. (2020); Cornish et al. (2020); Nielsen et al. (2020) improve the capacity of NFs by operating in
a higher-dimensional space. As mentioned in the introduction, all these existing works adopt explicit
forward mappings, which is only a subset of the broad class of invertible functions. In contrast, the
implicit function family we consider is richer. While we primarily discuss the implicit generaliza-
tion of ResFlows (Chen et al., 2019) in this paper, the general idea of utilizing implicit invertible
functions could be potentially applied to other models as well. Finally, Zhang et al. (2020) formally
prove that the model capacity of ResFlows is restricted by the dimension of the residual blocks. In
comparison, we study another limitation of ResFlows in terms of the bounded Lipschitz constant,
and compare the function family of ResFlows and ImpFlows with a comparable depth.
Continuous Time Flows (CTFs) (Chen et al., 2018b; Grathwohl et al., 2019; Chen et al., 2018a) are
flexible alternative to discrete time flows for generative modeling. They typically treat the invertible
transformation as a dynamical system, which is approximately simulated by ordinary differential
equation (ODE) solvers. In contrast, the implicit function family considered in this paper does
not contain differential equations, and only requires fixed point solvers. Moreover, the theoretical
guarantee is different. While CTFs typically study the universal approximation capacity under the
continuous time case (i.e., “infinite depth” limit), we consider the model capacity of ImpFlows and
ResFlows under a finite number of transformation steps. Finally, while CTFs are flexible, their learn-
ing is challenging due to instability (Liu et al., 2020; Massaroli et al., 2020) and exceedingly many
ODE solver steps (Finlay et al., 2020), making their large-scale application still an open problem.
Implicit Deep Learning Utilizing implicit functions enhances the flexibility of neural networks,
enabling the design of network layers in a problem-specific way. For instance, Bai et al. (2019)
propose a deep equilibrium model as a compact replacement of recurrent networks; Amos & Kolter
(2017) generalize each layer to solve an optimization problem; Wang et al. (2019) integrate logical
reasoning into neural networks; Reshniak & Webster (2019) utilize the implicit Euler method to
improve the stability of both forward and backward processes for residual blocks; and Sitzmann
et al. (2020) incorporate periodic functions for representation learning. Different from these works,
which consider implicit functions as a replacement to feed-forward networks, we develop invertible
implicit functions for normalizing flows, discuss the conditions of the existence of such functions,
and theoretically study the model capacity of our proposed ImpFlow in the function space.
3	Implicit Normalizing Flows
We now present implicit normalizing flows, by starting with a brief overview of existing work.
2
Published as a conference paper at ICLR 2021
3.1	Normalizing Flows
As shown in Eqn. (1), a normalizing flow f : x 7→ z is an invertible function that defines a proba-
bility distribution with the change-of-variable formula. The modeling capacity of normalizing flows
depends on the expressiveness of the invertible function f. Residual flows (ResFlows) (Chen et al.,
2019; Behrmann et al., 2019) are a particular powerful class of NFs due to their free-form Jacobian.
ResFloWs use f =%◦…。fι to construct the invertible mapping, where each layer f is an
invertible residual network with Lipschitz constraints bounded by a fixed constant κ:
fl(x) = x +gl(x), Lip(gl) ≤ κ < 1,	(2)
where Lip(g) is the Lipschitz constant of a function g (see Sec. 4.1 for details). Despite their
free-form Jacobian, the model capacity of ResFlows is still limited by the Lipschitz constant of the
invertible function. The Lipschitz constant of each ResFlow block fl cannot exceed 2 (Behrmann
et al., 2019), so the Lipschitz constant of an L-block ResFlow cannot exceed 2L. However, to
transfer a simple prior distribution to a potentially complex data distribution, the Lipschitz constant
of the transformation can be required to be sufficiently large in general. Therefore, ResFlows can be
undesirably deep simply to meet the Lipschitz constraints (see Fig. 3 for a 2D example). Below, we
present implicit flows (ImpFlows) to relax the Lipschitz constraints.
3.2	Model Specification
In general, an implicit flow (ImpFlow) is defined as an invertible mapping between random variables
x and z of dimension d by finding the roots of F (z, x) = 0, where F is a function from R2d to Rd.
In particular, the explicit mappings z = f(x) used in prior flow instances (Chen et al., 2019; Kingma
& Dhariwal, 2018) can be expressed as an implicit function in the form F (z, x) = f(x) - z = 0.
While ImpFlows are a powerful family to explore, generally they are not guaranteed to satisfy the
invertibility and the tractability of the log-determinant as required by NFs. In this paper, we focus on
the following specific form, which achieves a good balance between expressiveness and tractability,
and leave other possibilities for future studies.
Definition 1. Let gz : Rd → Rd and gx : Rd → Rd be two functions such that Lip(gx) < 1 and
Lip(gz) < 1, where Lip(g) is the Lipschitz constant of a function g. A specific form of ImpFlows
is defined by
F(z,x) = 0, where F (z, x) = gx(x) - gz(z) +x - z.	(3)
The root pairs of Eqn. (3) form a subset in Rd × Rd, which actually defines the assignment rule of
a unique invertible function f. To see this, for any x0, according to Definition 1, we can construct
a contraction hx0 (z) = F(z, x0) + z with a unique fixed point, which corresponds to a unique root
(w.r.t. z) of F(z, x0) = 0, denoted by f(x0). Similarly, in the reverse process, given a z0, the root
(w.r.t. x) of F(z0, x) = 0 also exists and is unique, denoted by f-1(z0). These two properties are
sufficient to ensure the existence and the invertibility of f, as summarized in the following theorem.
Theorem 1. Eqn.(3) defines a unique mapping f : Rd → Rd, z = f (x), and f is invertible.
See proof in Appendix A.1. Theorem 1 characterizes the validness of the ImpFlows introduced in
Definition 1. In fact, a single ImpFlow is a stack of a single ResFlow and the inverse of another
single ResFlow, which will be formally stated in Sec 4. We will investigate the expressiveness of the
function family of the ImpFlows in Sec 4, and present a scalable algorithm to learn a deep generative
model built upon ImpFlows in Sec. 5.
4	Expressiveness Power
We first present some preliminaries on Lipschitz continuous functions in Sec. 4.1 and then formally
study the expressiveness power of ImpFlows, especially in comparison to ResFlows. In particular,
we prove that the function space of ImpFlows is strictly richer than that of ResFlows in Sec. 4.2
(see an illustration in Fig. 1 (a)). Furthermore, for any ResFlow with a fixed number of blocks,
there exists some function that ResFlow has a non-negligible approximation error. However, the
function is exactly representable by a single-block ImpFlow. The results are illustrated in Fig. 1 (b)
and formally presented in Sec. 4.3.
3
Published as a conference paper at ICLR 2021
Lemma 1
R$F
Equation (5)	Theorem 2
(2-composition) Corollary 1 (2-composition)
∙ψ∙	-ψ-
R2	$ I
(a) Relationship between R2 and I.
(b) Relationship between r` and I.
(a) Target function
Figure 1: An illustration of our main theoretical results on the expressiveness power of ImpFlows
and ResFlows. Panel (a) and Panel (b) correspond to results in Sec. 4.2 and Sec. 4.3 respectively.
Figure 2: A 1-D motivating example. (a) Plot of the target function. (b) Results of fitting the
target function using ResFlows with different number of blocks. All functions have non-negligible
approximation error due to the Lipschtiz constraint. (c) An ImpFlow that can exactly represent
the target function. (d) A visualization of compositing a ResFlow block and the inverse of another
ResFlow block to construct an ImpFlow block. The detailed settings can be found in Appendix D.
4.1	Lipschitz Continuous Functions
For any differentiable function f : Rd → Rd and any x ∈ Rd , we denote the Jacobian matrix of f
atx as Jf (x) ∈ Rd×d.
Definition 2. A function Rd → Rd is called Lipschitz continuous if there exists a constant L, s.t.
kf(x1) - f(x2)k ≤ Lkx1 - x2k, ∀x1,x2 ∈ Rd.
The smallest L that satisfies the inequality is called the Lipschitz constant of f, denoted as Lip(f).
Generally, the definition of Lip(f) depends on the choice of the norm ∣∣∙∣∣, while we use L2 -norm
by default in this paper for simplicity.
Definition 3. A function Rd → Rd is called bi-Lipschitz continuous if it is Lipschitz continuous
and has an inverse mapping f-1 which is also Lipschitz continuous.
It is useful to consider an equivalent definition of the Lipschitz constant in our following analysis.
Proposition 1. (Rademacher (Federer (1969), Theorem 3.1.6)) If f : Rd → Rd is Lipschitz contin-
uous, then f is differentiable almost everywhere, and
Lip(f) = sup kJf (x)k2,
x∈Rd
where kMk2 = sup{v:kvk2=1} kM vk2 is the operator norm of the matrix M ∈ Rd×d.
4.2	Comparison to two-block ResFlows
We formally compare the expressive power of a single-block ImpFlow and a two-block ResFlow.
We highlight the structure of the theoretical results in this subsection in Fig. 1 (a) and present a 1D
motivating example in Fig. 2. All the proofs can be found in Appendix. A.
On the one hand, according to the definition of ResFlow, the function family of the single-block
ResFlow is
R :={f:f=g+Id, g∈C1(Rd,Rd),Lip(g) < 1},	(4)
4
Published as a conference paper at ICLR 2021
where C1(Rd, Rd) consists of all functions from Rd to Rd with continuous derivatives and Id de-
notes the identity map. Besides, the function family of '-block ResFloWs is defined by composition:
R' := {f : f = f` ◦ ∙ ∙ ∙ ◦ fι for some fι,…，f` ∈ R}.	(5)
By definition of Eqn. (4) and Eqn. (5), R1 = R.
On the other hand, according to the definition of the ImpFloW in Eqn. (3), We can obtain (gx +
Id)(x) = gx(x) + x = gz(z) + z = (gz + Id)(z), Where ◦ denotes the composition of functions.
Equivalently, We have z = (gz + Id)-1 ◦ (gx + Id) (x), Which implies the function family of the
single-block ImpFloW is
I = {f : f = f2-1 ◦ f1 for some f1,f2 ∈ R}.	(6)
Intuitively, a single-block ImpFloW can be interpreted as the composition of a ResFloW block and
the inverse function of another ResFloW block, Which may not have an explicit form (see Fig. 2 (c)
and (d) for a 1D example). Therefore, it is natural to investigate the relationship betWeen I and R2 .
Before that, We first introduce a family of “monotonically increasing functions” that does not have
an explicit Lipschitz constraint, and shoW that it is strictly larger than R.
Lemma 1.
R$F:
{f∈D:
inf	vT Jf (x)v > 0},
x∈Rd,v∈Rd,kvk2=1
(7)
where D is the set of all bi-Lipschitz C 1 -diffeomorphisms from Rd to Rd, and A $ B means A is a
proper subset of B.
Note that it folloWs from Behrmann et al. (2019, Lemma 2) that all functions in R are bi-Lipschitz, so
R $ D. In the 1D input case, We can getR = {f ∈ C1(R) : infx∈R f0 (x) > 0, supx∈R f0(x) < 2},
and F = {f ∈ C1(R) : infx∈R f0 (x) > 0}. In the high dimensional cases, R and F are hard to
illustrate. Nevertheless, the Lipschitz constants of the functions in R is less than 2 (Behrmann et al.,
2019), but those of the functions in F can be arbitrarily large. Based on Lemma 1, We prove that the
function family of ImpFloWs I consists of the compositions of tWo functions in F , and therefore is
a strictly larger than R2, as summarized in the folloWing theorem.
Theorem 2.	(Equivalent form of the function family of a single-block ImpFlow).
I = F2 := {f : f = f2 ◦ f1 for some f1, f2 ∈ F}.	(8)
Note that the identity mapping Id ∈ F , and it is easy to get F ⊂ I . Thus, the Lipschitz constant of
a single ImpFloW (and its reverse) can be arbitrarily large. Because R $ F and there exists some
functions in I \ R2 (see a constructed example in Sec. 4.3), We can get the folloWing corollary.
Corollary 1. R $ R2 $ F2 = I.
The results on the 1D example in Fig. 2 (b) and (c) accord With Corollary 1. Besides, Corollary 1 can
be generalized to the cases with 2'-block ResFlows and '-block ImpFlows, which strongly motivates
the usage of implicit layers in normalizing floWs.
4.3 Comparison with multi-block ResFlows
We further investigate the relationship between r` for ' > 2 and I, as illustrated in Fig. 1 (b). For
a fixed ', the Lipschitz constant of functions in r` is still bounded, and there exist infinite functions
that are not in R' but in I. We construct one such function family: for any L,r ∈ R+, define
P(L,r) = {f : f ∈ F, ∃Br ⊂ Rd, ∀x, y ∈ Br, kf(x) - f(y)k2 ≥ LkX - yk2},	(9)
where Br is an d-dimensional ball with radius of r. Obviously, P(L, r) is an infinite set. Below,
we will show that ∀ 0 < ' < log2 (L), R' has a non-negligible approximation error for functions in
P(L, r). However, they are exactly representable by functions in I.
Theorem 3.	Given L > 0 and r > 0, we have
• P(L, r) ⊂ I.
5
Published as a conference paper at ICLR 2021
• ∀ 0 < ' < log2 (L), P(L, r) ∩R' = 0. Moreover, for any f ∈ P(L, r) with d-dimensional
ball Br, the minimal error for fitting f in Br byfunctions in R' satisfies
inf sup kf (X) -g(X)k2 ≥ r(L - 2')	(IO)
g∈R' X∈Br	2
It follows Theorem 3 that to model f ∈ P(L, r), we need only a single-block ImpFlow but at least
a log2(L)-block ResFlow. In Fig. 2 (b), we show a 1D case where a 3-block ResFlow cannot fit a
function that is exactly representable by a single-block ImpFlow. In addition, we also prove some
other properties of ImpFlows. In particular, R3 6⊂ I. We formally present the results in Appendix B.
5 Generative Modeling with ImpFlows
ImpFlows can be parameterized by neural networks and stacked to form a deep generative model to
model high-dimensional data distributions. We develop a scalable algorithm to perform inference,
sampling and learning in such models. For simplicity, we focus on a single-block during derivation.
Formally, a parametric ImpFlow block z = f(X; θ) is defined by
F(z, X; θ) = 0, where F(z, X; θ) = gx(X; θ) - gz(z; θ) + X - z,	(11)
and Lip(gx) < 1, Lip(gz) < 1. Let θ denote all the parameters in gx and gz (which does NOT mean
gx and gz share parameters). Note that X refers to the input of the layer, not the input data.
The inference process to compute z given X in a single ImpFlow block is solved by finding the root
of F(z, X; θ) = 0 w.r.t. z, which cannot be explicitly computed because of the implicit formulation.
Instead, we adopt a quasi-Newton method (i.e. Broyden’s method (Broyden, 1965)) to solve this
problem iteratively, as follows:
z[i+1] = z[i] — αBF(z[i], x; θ), for i = 0,1,…，	(12)
where B is a low-rank approximation of the Jacobian inverse1 and α is the step size which we use
line search method to dynamically compute. The stop criterion is kF (z[i] , X; θ)k2 < f, where f
is a hyperparameter that balances the computation time and precision. As Theorem 1 guarantees the
existence and uniqueness of the root, the convergence of the Broyden’s method is also guaranteed,
which is typically faster than a linear rate.
Another inference problem is to estimate the log-likelihood. Assume that Z 〜p(z) where p(z) is a
simple prior distribution (e.g. standard Gaussian). The log-likelihood ofX can be written by
ln p(X) = ln p(z) + ln det(I + Jgx (X)) - ln det(I + Jgz (z)),	(13)
where Jf (X) denotes the Jacobian matrix of a function f at X. See Appendix. A.4 for the detailed
derivation. Exact calculation of the log-determinant term requires O(d3) time cost and is hard to
scale up to high-dimensional data. Instead, we propose the following unbiased estimator of ln p(X)
using the same technique in Chen et al. (2019) with Skilling-Hutchinson trace estimator (Skilling,
1989; Hutchinson, 1989):
11	[S (-1)k+1 (vT[Jgx(X)k]v-vT[Jgz(Z)k]v)]	/14、
lnp(x)=lnp(z)+ En〜P(N),v〜N(0,I) T —k-----------------------P(N ≥ k)------------- , (14)
k=1
where p(N) is a distribution supported over the positive integers.
The sampling process to compute X given Z can also be solved by the Broyden’s method, and the
hyperparameters are shared with the inference process.
In the learning process, we perform stochastic gradient descent to minimize the negative log-
likelihood of the data, denoted as L. For efficiency, we estimate the gradient w.r.t. the model
parameters in the backpropagation manner. According to the chain rule and the additivity of the
log-determinant, in each layer we need to estimate the gradients w.r.t. X and θ of Eqn. (13). In
particular, the gradients computation involves two terms: one is 翁 lndet(I + Jg(x; θ)) and the
1We refer readers to Broyden (1965) for the calculation details for B .
6
Published as a conference paper at ICLR 2021
Table 1: Classification error rate (%) on test set of vanilla ResNet, ResFlow and ImpFlow of ResNet-
18 architecture, with varying Lipschitz coefficients c.
			Vanilla	C = 0.99	c = 0.9	c = 0.8	c=0.7	c = 0.6
	ResFlow ImpFlow		8.24	8.39	8.69	9.25	9.94
CIFAR10		6.61(±0.02)	(±0.03) 7.29	(±0.01) 7.41	(±0.03) 7.94	(±0.02) 8.44	(±0.02) 9.22
			(±0.03)	(±0.03)	(±0.06)	(±0.04)	(±0.02)
	ResFlow ImpFlow		31.02	31.88	32.21	33.58	34.48
CIFAR100		27.83(±0.03)	(±0.05) 29.06	(±0.02) 30.47	(±0.03) 31.40	(±0.02) 32.64	(±0.03) 34.17
			(±0.03)	(±0.03)	(±0.03)	(±0.01)	(±0.02)
other is d∂L焉,where g is a function satisfying Lip(g) < 1 and (∙) denotes X or θ. On the one
hand, for the log-determinant term, we can use the same technique as Chen et al. (2019), and obtain
an unbiased gradient estimator as follows.
dlndet(I + Jg(X； θ^	IYX (-1)k	τ J Λ Jg(X； θ
∂(∙)	= En 〜P(N ),v 〜N (O,I) [[ JLp(N ≥ k)V Jg (X ⑼ J ∂ (∙) V ,
=	(15)
where P(N) is a distribution supported over the positive integers. On the other hand, ∂∂Z 箫 can be
computed according to the implicit function theorem as follows (See details in Appendix A.5):
∂L ∂z	∂L -1	∂F (z, X; θ)
∂Z 丽=∂Z JG (Z) -∂0-, where G(z； θ) = gz(z； θ) + z∙	(16)
In comparision to directly calculate the gradient through the quasi-Newton iterations of the forward
pass, the implicit gradient above is simple and memory-efficient, treating the root solvers as a black-
box. Following Bai et al. (2019), We compute ∂∂Z JGI(Z) by solving a linear system iteratively, as
detailed in Appendix C.1. The training algorithm is formally presented in Appendix C.4.
6	Experiments
We demonstrate the model capacity of ImpFlows on the classification and density modeling tasks2.
In all experiments, we use spectral normalization (Miyato et al., 2018) to enforce the Lipschitz
constrants, where the Lipschitz constant upper bound of each layer (called Lipschitz coefficient) is
denoted as c. For the Broyden’s method, we use f = 10-6 and b = 10-10 for training and testing
to numerically ensure the invertibility and the stability during training. Please see other detailed
settings including the method of estimating the log-determinant, the network architecture, learning
rate, batch size, and so on in Appendix D.
6.1	Verifying Capacity on Classification
We first empirically compare ResFlows and ImpFlows on classification tasks. Compared with gen-
erative modeling, classification is a more direct measure of the richness of the functional family,
because it isolates the function fitting from generative modeling subtleties, such as log-determinant
estimation. We train both models in the same settings on CIFAR10 and CIFAR100 (Krizhevsky &
Hinton, 2009). Specifically, we use an architecture similar to ResNet-18 (He et al., 2016). Over-
all, the amount of parameters of ResNet-18 with vanilla ResBlocks, ResFlows and ImpFlows are the
same of 6.5M. The detailed network structure can be found in Appendix D. The classification results
are shown in Table 1. To see the impact of the Lipschitz constraints, we vary the Lipschitz coefficient
c to show the difference between ResFlows and ImpFlows under the condition of a fixed Lipschitz
upper bound. Given different values of c, the classification results of ImpFlows are consistently
better than those of ResFlows. These results empirically validate Corollary 1, which claims that the
2See https://github.com/thu-ml/implicit-normalizing-flows for details.
7
Published as a conference paper at ICLR 2021
Table 2: Average test log-likelihood (in nats) of tabular datasets. Higher is better.
		POWER	GAS	HEPMASS	MINIBOONE	BSDS300
RealNVP (Dinh et al., 2017)		0.17	8.33	-18.71	-13.55	153.28
FFJORD (Grathwohl et al., 2019)		0.46	8.59	-14.92	-10.43	157.40
MAF (Papamakarios et al., 2017)		0.24	10.08	-17.70	-11.75	155.69
NAF (Huang et al., 2018)		0.62	11.96	-15.09	-8.86	157.73
ImpFlow (L	20)	0.61	12.11	-13.95	-13.32	155.68
ResFlow (L =	10)	0.26	6.20	-18.91	-21.81	104.63
ImpFlow (L	5)	0.30	6.94	-18.52	-21.50	113.72
Table 3: Average bits per dimension of ResFlow and ImpFlow on CIFAR10, with varying Lipschitz
coefficients c. Lower is better.
	c=0.9	C = 0.8	c=0.7	c=0.6
ResFlow (L =	12)	3.469(±0.0004)	3.533(±0.0002)	3.627(±0.0004)	3.820(±0.0003)
ImpFlow (L	= 6)	3.452(±0.0003)	3.511(±0.0002)	3.607(±0.0003)	3.814(±0.0005)
functional family of ImpFlows is richer than ResFlows. Besides, for a large Lipschitz constant upper
bound c, ImpFlow blocks are comparable with the vanilla ResBlocks in terms of classification.
6.2	Density Modeling On 2D Toy Data
For the density modeling tasks, we first
evaluate ImpFlows on the Checkerboard
data whose density is multi-modal, as
shown in Fig. 3 (a). For fairness, we
follow the same experiment settings as
Chen et al. (2019) (which are specified
in Appendix D), except that we adopt
a Sine (Sitzmann et al., 2020) activation
function for all models. We note that the
(a) Checkerboard (b) ResFlow, (c) ImpFlow,
data (5.00 bits)	L = 8 (5.08 bits) L = 4 (5.05 bits)
data distribution has a bounded support Figure 3: Checkerboard data density and the results of
while we want to fit a transformation f a 8-block ResFlow and a 4-block ImpFlow.
mapping it to the standard Gaussian dis-
tribution, whose support is unbounded. A perfect f requires a sufficiently large kJf (x)k2 for some
x mapped far from the mean of the Gaussian. Therefore, the Lipschtiz constant of such f is too
large to be fitted by a ResFlow with 8 blocks (See Fig. 3 (b)). A 4-block ImpFlow can achieve a
result of 5.05 bits, which outperforms the 5.08 bits of a 8-block ResFlow with the same number
of parameters. Such results accord with our theoretical results in Theorem 2 and strongly motivate
ImpFlows.
6.3	Density Modeling On Real Data
We also train ImpFlows on some real density modeling datasets, including the tabular datasets (used
by Papamakarios et al. (2017)), CIFAR10 and 5-bit 64 × 64 CelebA (Kingma & Dhariwal, 2018).
For all the real datasets, we use the scalable algorithm proposed in Sec. 5.
We test our performance on five tabular datasets: POWER (d = 6), GAS (d = 8), HEPMASS
(d = 21), MINIBOONE (d = 43) and BSDS300 (d = 63) from the UCI repository (Dua & Graff,
2017), where d is the data dimension. For a fair comparison, on each dataset we use a 10-block
ResFlow and a 5-block ImpFlow with the same amount of parameters, and a 20-block ImpFlow for
a better result. The detailed network architecture and hyperparameters can be found in Appendix D.
Table 2 shows the average test log-likelihood for ResFlows and ImpFlows. ImpFlows achieves
better density estimation performance than ResFlow consistently on all datasets. Again, the results
demonstrate the effectiveness of ImpFlows.
8
Published as a conference paper at ICLR 2021
Then we test ImpFlows on the CIFAR10 dataset. We train a multi-scale convolutional version for
both ImpFlows and ResFlows, following the same settings as Chen et al. (2019) except that we use a
smaller network of 5.5M parameters for both ImpFlows and ResFlows (see details in Appendix D).
As shown in Table 3, Impflow achieves better results than ResFlow consistently given different
values of the Lipschitz coefficient c. Moreover, the computation time of ImpFlow is comparable to
that of ResFlow. See Appendix C.2 for detailed results. Besides, there is a trade-off between the
expressiveness and the numerical optimization of ImpFlows in larger models. Based on the above
experiments, we believe that advances including an lower-variance estimate of the log-determinant
can benefit ImpFlows in larger models, which is left for future work.
We also train ImpFlows on the 5-bit 64 × 64 CelebA. For a fair comparison, we use the same settings
as Chen et al. (2019). The samples from our model are shown in Appendix E.
7	Conclusions
We propose implicit normalizing flows (ImpFlows), which generalize normalizing flows via utilizing
an implicit invertible mapping defined by the roots of the equation F (z, x) = 0. ImpFlows build on
Residual Flows (ResFlows) with a good balance between tractability and expressiveness. We show
that the functional family of ImpFlows is richer than that of ResFlows, particularly for modeling
functions with large Lipschitz constants. Based on the implicit differentiation formula, we present
a scalable algorithm to train and evaluate ImpFlows. Empirically, ImpFlows outperform ResFlows
on several classification and density modeling benchmarks. Finally, while this paper mostly focuses
on the implicit generalization of ResFlows, the general idea of utilizing implicit functions for NFs
could be extended to a wider scope. We leave it as a future work.
Acknowledgement
We thank Yuhao Zhou, Shuyu Cheng, Jiaming Li, Kun Xu, Fan Bao, Shihong Song and Qi’An Fu for
proofreading. This work was supported by the National Key Research and Development Program of
China (Nos. 2020AAA0104304), NSFC Projects (Nos. 61620106010, 62061136001, U19B2034,
U181146, 62076145), Beijing NSF Project (No. JQ19016), Beijing Academy of Artificial Intel-
ligence (BAAI), Tsinghua-Huawei Joint Research Program, Huawei Hisilicon Kirin Intelligence
Engineering Development, the MindSpore team, a grant from Tsinghua Institute for Guo Qiang,
Tiangong Institute for Intelligent Computing, and the NVIDIA NVAIL Program with GPU/DGX
Acceleration. C. Li was supported by the fellowship of China postdoctoral Science Foundation
(2020M680572), and the fellowship of China national postdoctoral program for innovative talents
(BX20190172) and Shuimu Tsinghua Scholar. J. Chen was supported by Shuimu Tsinghua Scholar.
References
Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.
In International Conference on Machine Learning, pp. 136-145, 2017.
Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural
Information Processing Systems, 2019.
Jens Behrmann, Will GrathWohL Ricky TQ Chen, David Duvenaud, and Jorn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning, pp. 573-582,
2019.
Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics
of Computation, 19(92):577-593, 1965.
Changyou Chen, Chunyuan Li, Liqun Chen, Wenlin Wang, Yunchen Pu, and LaWrence Carin Duke.
Continuous-time floWs for efficient inference and density estimation. In International Conference
on Machine Learning, pp. 824-833, 2018a.
Jianfei Chen, Cheng Lu, Biqi Chenli, Jun Zhu, and Tian Tian. VfloW: More expressive generative
floWs With variational data augmentation. In International Conference on Machine Learning,
2020.
9
Published as a conference paper at ICLR 2021
Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Jorn-Henrik Jacobsen. Residual flows
for invertible generative modeling. In Advances in Neural Information Processing Systems, pp.
9916-9926, 2019.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differ-
ential equations. In Advances in Neural Information Processing Systems, pp. 6571-6583, 2018b.
Rob Cornish, Anthony L Caterini, George Deligiannidis, and Arnaud Doucet. Relaxing bijectivity
constraints with continuously indexed normalising flows. In International Conference on Machine
Learning, 2020.
Nicola De Cao, Wilker Aziz, and Ivan Titov. Block neural autoregressive flow. In Uncertainty in
Artificial Intelligence, pp. 1263-1273. PMLR, 2020.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. In International Conference on Learning Representations Workshop, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In
International Conference on Learning Representations, 2017.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. In
Advances in Neural Information Processing Systems, pp. 7511-7522, 2019.
Herbert Federer. Grundlehren der mathematischen wissenschaften. In Geometric measure theory,
volume 153. Springer New York, 1969.
Chris Finlay, Jorn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman. HoW to train your
neural ode: the world of jacobian and kinetic regularization. In International Conference on
Machine Learning, 2020.
Will Grathwohl, Ricky TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. In International Con-
ference on Learning Representations, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-
based generative models with variational dequantization and architecture design. In International
Conference on Machine Learning, pp. 2722-2730, 2019.
Emiel Hoogeboom, Rianne Van Den Berg, and Max Welling. Emerging convolutions for generative
normalizing flows. In International Conference on Machine Learning, pp. 2771-2780, 2019.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. In International Conference on Machine Learning, pp. 2078-2087, 2018.
Chin-Wei Huang, Laurent Dinh, and Aaron Courville. Augmented normalizing flows: Bridging the
gap between generative flows and latent variable models. arXiv:2002.07101, 2020.
Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian
smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059-1076,
1989.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-
proved variational inference with inverse autoregressive flow. In Advances in Neural Information
Processing Systems, pp. 4743-4751, 2016.
10
Published as a conference paper at ICLR 2021
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, University of Toronto, 2009.
Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. How does noise help
robustness? explanation and exploration under the neural sde framework. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 282-290, 2020.
Stefano Massaroli, Michael Poli, Michelangelo Bin, Jinkyoo Park, Atsushi Yamashita, and Hajime
Asama. Stable neural flows. arXiv preprint arXiv:2003.08063, 2020.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.
Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther, and Max Welling. Survae flows:
Surjections to bridge the gap between vaes and flows. arXiv preprint arXiv:2007.02731, 2020.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In Advances in Neural Information Processing Systems, pp. 2338-2347, 2017.
Viktor Reshniak and Clayton Webster. Robust learning with implicit residual networks. arXiv
preprint arXiv:1905.10479, 2019.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional Conference on Machine Learning, pp. 1530-1538, 2015.
Vincent Sitzmann, Julien NP Martel, Alexander W Bergman, David B Lindell, and Gordon
Wetzstein. Implicit neural representations with periodic activation functions. arXiv preprint
arXiv:2006.09661, 2020.
John Skilling. The eigenvalues of mega-dimensional matrices. In Maximum Entropy and Bayesian
Methods, pp. 455-466. Springer, 1989.
Yang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks with
masked convolutions. In Advances in Neural Information Processing Systems, pp. 11002-11012,
2019.
Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and log-
ical reasoning using a differentiable satisfiability solver. In International Conference on Machine
Learning, pp. 6545-6554, 2019.
Han Zhang, Xi Gao, Jacob Unterman, and Tom Arodz. Approximation capabilities of neural odes
and invertible residual networks. In International Conference on Machine Learning, 2020.
A Additional Lemmas and Proofs
A.1 Proof For Theorem 1
Proof. (Theorem 1)
Firstly, ∀x0 ∈ Rd, the mapping
hx0 (z) = F (z, x0) +z
is a contrative mapping, which can be shown by Lipschitz condition of gz :
k(F(z1,x0) +z1) - (F(z2,x0) +z2)k = kgz(z1) -gz(z2)k < kz1 - z2k.
Therefore, hx0 (z) has an unique fixed point, denoted by f(x0) :
hx0 (f (x0)) = f(x0) ⇔ F(f(x0),x0) = 0
Similarly, we also have: ∀z0 ∈ Rd, there exists an unique g(z0) satisfying F (z0, g(z0)) = 0.
Moreover, Let z0 = f (x0), we have F (f (x0), g(f (x0))) = 0. By the uniqueness, we have
g(f (xo)) = xo, ∀xo ∈ Rd . Similarly, f (g(xo)) = x0,∀x° ∈ Rd. Therefore, f is unique and
invertible.	□
11
Published as a conference paper at ICLR 2021
A.2 Proof For Theorem 2
We denote D as the set of all bi-Lipschitz C 1 -diffeomorphisms from Rd to Rd.
Firstly, we prove Lemma 1 in the main text.
Proof. (Lemma 1). ∀f ∈ R, we have
sup kJf (x) -Ik22 < 1,
x∈Rd
which is equivalent to
sup	k(Jf (x) - I)vk22 < 1 (Definition of operator norm.)
x∈Rd,v∈Rd,kvk2=1
sup	vT (JfT (x) -I)(Jf(x) - I)v < 1
x∈Rd,v∈Rd,kvk2=1
sup	vT JfT (x)Jf (x)v - 2vT Jf (x)v < 0
x∈Rd,v∈Rd,kvk2=1
Note that Jf (x) is nonsingular, so ∀x, v ∈ Rd, kvk2 = 1, we have vT JfT (x)Jf (x)v > 0. Thus,
0 > sup	vT JfT (x)Jf (x)v - 2vT Jf (x)v ≥ sup	-2vT Jf (x)v
x∈Rd,v∈Rd,kvk2=1	x∈Rd,v∈Rd,kvk2=1
So we have
inf	vT Jf (x)v > 0.
x∈Rd,v∈Rd,kvk2=1
Note that the converse is not true, because vT Jf (x)v > 0 does not restrict the upper bound of
Lipschitz constant of f. For example, when f (x) = mx where m is a positive real number, we have
inf	vT Jf (x)v = inf	vT (mI)v = m > 0
x∈Rd,v∈Rd,kvk2=1	x∈Rd,v∈Rd,kvk2=1
However, m can be any large positive number. So We have R $ F.	□
Lemma 2. ∀f ∈ D, if
inf	vT Jf (x)v > 0,	(17)
x∈Rd,v∈Rd,
kvk2=1
then
inf	vT Jf -1 (x)v > 0,	(18)
x∈Rd,v∈Rd,
kvk2=1
Proof. (Proof of Lemma 2). By Inverse Function Theorem,
Jf-1(x)=Jf-1(f-1(x)).
Because f is from Rd to Rd, we have
inf	vT Jf -1 (x)v = inf
x∈Rd,v∈Rd,kvk2=1	x∈Rd,v∈Rd,kvk2=1
= inf
x∈Rd,v∈Rd,kvk2=1
vT Jf-1(f -1(x))v
vT Jf-1 (x)v
Let U = J-I(X)V and vo = ^^, we have ∣∣v0∣∣2 = 1, and
vT Jf-1(x)v = uT JfT (x)u = uTJf (x)u = kuk22v0TJf (x)v0.
The above equation uses this fact: for a real d×d matrix A, ∀X ∈ Rd, XTAX = (XT AX)T = XTATX
because XT AX ∈ R.
Note that f is Lipschitz continuous, kJf (X)k2 ≤ Lip(f). So
1=kvk2≤ kJf(X)k2kuk2 ≤Lip(f)kuk2,
12
Published as a conference paper at ICLR 2021
which means
kuk2 ≥ W).
Thus,
inf	vT Jf-1 (x)v =
x∈Rd,v∈Rd,kvk2=1	f
inf
x∈Rd,v∈Rd,kvk2=1
inf
kuk22v0T Jf (x)v0
x∈Rd,u∈Rd,kJf (x)uk2=1
kuk22	inf	v0T Jf (x)v0
2 x∈Rd,v0∈Rd,kv0 k2=1	0
≥
≥
>
Lip(f)2 χ∈Rd,v∈Rd,kvk2=1	Jf(x)v
0
□
Lemma 3. ∀f ∈ D, if f-1 ∈ R, we have
inf	vT Jf (x)v > 0.
x∈Rd,v∈Rd,
kvk2=1
(19)
Proof. (Proof of Lemma 3). ∀f ∈ D, if f-1 ∈ R, then from Lemma 1, we have
inf	vT Jf -1 (x)v > 0.
x∈Rd,v∈Rd,kvk2=1
Note that f -1 ∈ D, from Lemma 2 we have
inf	vT Jf (x)v > 0.
x∈Rd,v∈Rd,kvk2=1
□
Lemma 4. ∀f ∈ D, if
inf	vT Jf (x)v > 0,
x∈Rd,v∈Rd,kvk2=1
then ∃ α0 > 0, s.t. ∀ 0 < α < α0,
sup kαJf (x) - Ik2 < 1.
x∈Rd
Proof. (Proof of Lemma 4). Note that f is Lipschitz continuous, so Lip(f) = supx∈Rd kJf(x)k2.
Denote
β = inf	vT Jf (x)v.
x∈Rd ,v∈Rd ,kvk2 =1
And let
β
> 0.
α0
Lip(f)2
13
Published as a conference paper at ICLR 2021
∀ 0 < α < α0, we have
sup kαJf (x) - I k22 = sup	vT (αJfT (x) - I)(αJf (x) - I)v
x∈Rd	x∈Rd,v∈Rd,kvk2=1
= 1 + sup	α2vT JfT (x)Jf (x)v - 2αvT Jf (x)v
x∈Rd ,v∈Rd ,kvk2 =1
≤ 1 + α2	sup	vT JfT (x)Jf (x)v
x∈Rd,v∈Rd,kvk2 =1
+ 2α	sup	-vT Jf (x)v
x∈Rd,v∈Rd,kvk2=1
= 1 + α2	sup	vT JfT (x)Jf (x)v
x∈Rd,v∈Rd,kvk2 =1
- 2α	inf	vT Jf (x)v
x∈Rd,v∈Rd,kvk2=1
= 1 + α2 sup kJf (x)k22 - 2αβ
x∈Rd
= 1 + α(αLip(f)2 - 2β)
<	1 + α(α0 Lip(f)2 - 2β)
= 1 - αβ
<	1.
The above equation uses this fact: for a real d×d matrix A, ∀x ∈ Rd, xTAx = (xT Ax)T = xTATx
because XTAX ∈ R.	□
Proof. (Theorem 2) Denote
P = {f ∈ D | ∃f1, f2 ∈ D,f = f2 ◦ f1, where
inf	vT Jf1 (X)v > 0,	inf	vT Jf2 (X)v > 0}.
x∈Rd,v∈Rd,kvk2=1	1	x∈Rd,v∈Rd,kvk2=1	2
Firstly, we show that I ⊂ P. ∀f ∈ I, assume f = f2 ◦ f1, where f1 ∈ R and f2-1 ∈ R. By
Lemma 1 and Lemma 3, we have
inf	vT Jf1 (X)v > 0,
x∈Rd,v∈Rd,kvk2=1	1
inf	vT Jf2 (X)v > 0.
x∈Rd ,v∈Rd ,kvk2 =1
Thus, f ∈ P. So I ⊂ P.
Next, we show that P ⊂ I. ∀f ∈ P, assume f = f2 ◦ f1, where
inf	vT Jf1 (X)v > 0,
x∈Rd ,v∈Rd ,kvk2 =1
inf	vT Jf2 (X)v > 0.
x∈Rd,v∈Rd,kvk2=1	2
From Lemma 2, we have
inf	vT Jf -1 (X)v > 0.
x∈Rd,v∈Rd,kvk2=1	f2
From Lemma 4, ∃ α1 > 0, α2 > 0, s.t. ∀ 0 < α < min{α1, α2},
sup kαJf1 (X) - Ik2 < 1,
x∈Rd
sup kαJf -1 (X) - Ik2 < 1.
x∈Rd	2
Let α = 2 min{αι, α2}. Let g = g2 ◦ gι, where
g1(X) = αf1(X),
X
g2(x) = f2(-).
α
14
Published as a conference paper at ICLR 2021
Wehave g(x) = f2( αfα(x)) = f(x),and
Jg1 (x) = αJf1 (x),
g2-1 (x) = αf2-1 (x),
Jg2-1 (x) = αJf2-1 (x).
So we have
sup kJg1 (x) - Ik2 = sup kαJf1 (x) - Ik2 < 1,
x∈Rd	x∈Rd
sup kJg-1 (x) - Ik2 = sup kαJf-1 (x) - Ik2 < 1.
x∈Rd	2	x∈Rd	2
Thus, g1 ∈ R and g2-1 ∈ R and f = g2 ◦ g1. So f ∈ I. Therefore, P ⊂ I.
In conclusion, I = P.	口
A.3 Proof For Theorem 3
Firstly, we prove a lemma of bi-Lipschitz continuous functions.
Lemma 5. If f : (Rd, k」)→ (Rd, k」)is bi-Lipschitz continuous, then
1	≤ kf(xι)- f(x2)k
Lip(f-1) — l∣xι — x2k
≤ Lip(f), ∀x1, x2 ∈ Rd,x1 6= x2.
Proof. (Proof of Lemma 5). ∀x1, x2 ∈ Rd, x1 6= x2, we have
kf(x1) -f(x2)k ≤Lip(f)kx1 -x2k
kxι — x2k = kf-1(f(xι)) — f-1 (f(x2))k ≤ Lip(f-1)kf(xι) — f(x2)k
Thus, We get the results.	口
Assume a residual flow f = 九 ◦•••◦ f ι where each layer 力 is an invertible residual network:
fl(x) = x +gl(x), Lip(gl) ≤ κ < 1.
Thus, each layer fl is bi-Lipschitz and it follows by Behrmann et al. (2019) and Lemma 5 that
1	kfl (XI) - fl(x2) k	1 1 试/ lL ʊ	U Iuld Y -≠ γ	/ɔm
1 一 K ≤ ----η-------≤---- ≤ 1 + K < 2 , ∀xι, x2 ∈ R , xι = x2.	(20)
kx1 — x2 k
By multiplying all the inequalities, we can get a bound of the bi-Lipschitz property for ResFlows,
as shown in Lemma 6.
Lemma 6. For ResFlows built by f =九◦•••◦ fι, where fι(x) = X + gι(x), Lip(gι) ≤ κ < 1,
then
(1 - k)l ≤ kf (XI)- f(x2)k ≤ (1 + k)l, ∀xι, x2 ∈ Rd, xι= x2.
kx1 — x2 k
Next, we prove Theorem 3.
Proof. (Theorem 3) According to the definition of P(L, r), we have P(L, r) ⊂ F ⊂ I.
∀ 0 < ' < log2(L), we have L - 2' > 0. ∀ g ∈ r`, by Lemma 6, we have
kg(x) -g(y)k2 ≤ 2'kx - y∣2,∀x,y ∈ Br.
Thus, ∀ x0 ∈ Br , we have
kf (x) -g(x)k2 = kf (x) - f(x0) + g(x0) -g(x) +f(x0) -g(x0)k2
≥ kf (x) - f(x0)k2 - kg(x0) -g(x) +f(x0) -g(x0)k2
≥ kf(x) -f(x0)k2 - kg(x0) -g(x)k2 - kf(x0) - g(x0)k2
≥ (L - 2')kx - x0k2 - kf (x0) - g(x0)k2
15
Published as a conference paper at ICLR 2021
So
SUp kf (X)- g(x)k2 ≥ SUp (L - 2')kx - X01∣2 - kf(X0)- g(x0)k2
x∈Br	x∈Br
≥ (L - 2')r - kf (XO)- g(XO)k2
Notice that the inequality above is true for any XO ∈ Br , so we have
sup kf (x) - g(x)k2 ≥ sup (L - 2')r - kf (xo) - g(x0)k2
x∈Br
x0 ∈Br
=(L - 2')r - infkf (XO)- g(X0)k2
x0 ∈Br
≥ (L - 2')r - sup kf (xo) - g(X0)k2
x0 ∈Br
Therefore,
sup kf (x) - g(x)k2 ≥ r(L - 2'),∀g ∈R'
x∈Br	2
So we get
gi∈R ` XUPrkf(X)-g(χ)k2 ≥ 2(L - 2')
Because ∀f ∈ P(L, r), infg∈R supχ∈Br kf (x) - g(x)∣∣2 > 0, We have R' ∩ P(L, r) = 0.	□
A.4 Proof for Equation 13
Proof. (Equation 13) By Change of Variable formula:
logρ(x) = logp(z) + log ∣∂z∕∂x∣,
Since z = f(x) is defined by the equation
F(z,x) = gχ(x) - gz(z) +x - z = 0,
by Implicit function theorem, We have
∂z∕∂ X = Jf(X) = -[Jf,z(z)]-1[Jf,x(x)] = (I + Jgz (Z))T(I + Jgx(X)).
Thus,
log ∣∂z∕∂x∣ = ln | det(I + Jgx(X))I- ln | det(I + Jgz(Z))|
Note that any eigenvalue λ of Jgx(X) satisfies ∣λ∣ < σ(Jgx(x)) = kJgx(x)k2 < 1, so λ ∈ (-1,1).
Thus, det(I + Jgx (x)) > 0. Similarly, det(I + Jgz (Z)) > 0. Therefore,
log I∂Z∕∂xI = ln det(I + Jgx (x)) - ln det(I + Jgz (Z))
□
A.5 Proof for Equation 16
Proof. (Equation 16) By implicitly differentiating tWo sides of F(Z, x; θ) = 0 by x, We have
∂gχ(x; θ) _ ∂gz(z; θ) ∂z
∂x
∂Z	∂x
∂Z
+ I - ∂X = 0,
So We have
∂Z = (I + ∂gz(z; θ) ) T (I + ∂gχ(x; θ))
= JG-1 (Z)
∂F(z, x; θ)
∂x
16
Published as a conference paper at ICLR 2021
then
1-K
----≤
1+K
By implicitly differentiating two sides of F (z, x; θ) = 0 by θ, we have
∂gχ(x; θ)	∂gz(z; θ)	∂gz(z; θ) ∂z ∂z _ 0
-∂θ	∂θ	∂Z- ∂θ - ∂θ =,
So we have
∂ Z =(I + ∂gz(z; θ) ∖ T ( ∂gχ(x; θ) _ ∂gz(z; θ) λ
∂θ =v+ -∂Z-)	V-∂θ	∂θ-)
∂F(z, x; θ)
∂θ
Therefore, the gradient from Z to (∙) is
∂L ∂z	∂L ι ∂F(z, x; θ)
∂Z ∂∏ = ∂Z JG (Z) —∂(∙)一.
□
B	Other Properties of Implicit Flows
In this section, we propose some other properties of ImpFlows.
Lemma 7. For a single implicit flow f ∈ I, assume that f = f2-1 ◦ f1, where
f1(x) = x + g1(x), Lip(g1) ≤ κ < 1,	(21)
f2(x) = x + g2(x), Lip(g2) ≤ κ < 1,	(22)
kf (XI)- f (x2)k	1 + K W	^d	必	。外
≤	, ∀X1, X2 ∈ R , Xl = X2.	(23)
kx1 - x2 k 1 - κ
Proof. (Proof of Lemma 7) According to Eqn. (20), we have
1 - K ≤ kf1 (XI)- f1(X2)k ≤ 1 + K, ∀X1,X2 ∈ Rd,X1= X2,	(24)
kX1 - X2 k
1	∕kf-1(XI)- f-1(X2)k /	1	∀.. 一. LRd- S-	(25
E ≤----------^T-^----------≤ E, ∀X1, x2 ∈ R ,X1 = X2.	(25)
By multiplying these two inequalities, We can get the results.	□
Theorem 4. (Limitation of the single ImpFlow).
I⊂{f : f ∈D,∀x ∈ Rd, λ(Jf(x)) ∩ R- = 0},	(26)
where λ(A) denotes the set of all eigenvalues of matrix A.
Proof. (Proof of Theorem 4)
Proof by contradiction. Assume ∃f ∈ I and X ∈ Rd, s.t. ∃λ ∈ λ(Jf (X)), λ < 0.
There exists a vector u 6= 0, Jf (X)u = λu. By Theorem 2, ∃f1, f2 ∈ F, f = f2 ◦ f1, hence
Jf (X) = Jf2 (f1(X))Jf1 (X). We denote A := Jf2 (f1(X)), B := Jf1 (X). Since f1, f2 ∈ F, we have
vT Av > 0, wT Bw > 0, ∀v, w 6= 0, v, w ∈ Rd .
Note that B is the Jacobian of a bi-Lipschitz function at a single point, so B is non-singular. As
u 6= 0, we have Bu 6= 0. Thus,
(Bu)TA(Bu) = (Bu)T ((AB)u) = λuTBTu = λuTBu
The last equation uses this fact: for a real d × d matrix A, ∀X ∈ Rd, XTAX = (XT AX)T =
XTATX because XTAX ∈ R. Note that the left side is positive, and the right side is negative. It’s a
contradiction.	□
17
Published as a conference paper at ICLR 2021
Therefore, I cannot include all the bi-Lipschitz C1-diffeomorphisms. As a corollary, we have R3 6⊂
I.
Corollary 2. R3 6⊂ I.
Proof. (Proof for Corollary 2) Consider three linear functions in R:
f1(x)	-0.46 -0.20 x +	0.85	0.00	x
f2(x)	-0.20 -0.70 x +	0.30	-0.60	x
f3(x)	-0.50 -0.60 x +	-0.20 -0.55 x
0.2776 -0.4293
0.5290 -0.6757
We can get that f = f1 ◦ f2 ◦ f3 is in R3, and f is also a linear function with Jacobian
However, this is a matrix with two negative eigenvalues: -0.1881, -0.2100.
Hence f is not in I. Therefore, R3 ⊂ I.	□
C Computation
C.1 Approximate Inverse Jacobian
The exact computation for the Jacobian inverse term costs much for high dimension tasks. We use
the similar technique in Bai et al. (2019) to compute d∂L J-1 (z): solving the linear system of variable
y:
JT (z)yτ = (∂L )T,	(27)
∂z
where the left hand side is a vector-Jacobian product and it can be efficiently computed by autograd
packages foy any y without computing the Jacobian matrix. In this work, we also use Broyden’s
method to solve the root, the same as methods in the forward pass, where the tolerance bound for
the stop criterion is b.
Remark. Although the forward, inverse and backward pass of ImpFlows all need to solve the root
of some equation, we can choose small enough f and b to ensure the approximation error is small
enough. Thus, there is a trade-off between computation costs and approximation error. In practice,
we use f = 10-6 and b = 10-10 and empirically does not observe any error accumulation. Note
that such approximation is rather different from the variational inference technique in Chen et al.
(2020); Nielsen et al. (2020), because we only focus on the exact log density itself.
C.2 Computation Time
We evaluate the average computation time for the model trained on CIFAR10 in Table 3 on a sin-
gle Tesla P100 (SXM2-16GB). See Table 4 for the details. For a fair comparision, the forward
(inference) time in the training phase of ImpFlow is comparable to that of ResFlow because the
log-determinant term is the main cost. The backward time of ImpFlow costs more than that of Res-
Flow because it requires to rewrite the backward method in PyTorch to solve the linear equation.
The training time includes forward, backward and other operations (such as the Lipschitz iterations
for spectral normalization). We use the same method as the release code of ResFlows (fixed-point
iterations with tolerance 10-5) for the sample phase. The sample time of ImpFlow is less than that
of ResFlow because the inverse of L-block ImpFlow needs to solve L fixed points while the inverse
of 2L-block ResFlow needs to solve 2L fixed points. Fast sampling is particularly desirable since it
is the main advantage of flow-based models over autoregressive models.
Also, we evaluate the average Broyden’s method iterations and the average function evaluation times
during the Broyden’s method. See Table 5 for the details.
18
Published as a conference paper at ICLR 2021
Table 4: Single-batch computation time (seconds) for ResFlow and ImpFlow in Table 3 on a single
Tesla P100 (SXM2-16GB).
c	Model	Forward (Inference)			Backward		Training ∣ Sample	
0.5	ImPFloW	Fixed-point	Log-det	Others	Inv-Jacob	Others	4.152	0.138
		0.445	2.370	0.090	0.562	0.441		
		2.905			1.003			
	ResFloW	2.656			0.031		2.910	0.229
0.6	ImpFlow	Fixed-point	Log-det	Others	Inv-Jacob	Others	4.415	0.159
		0.497	2.356	0.120	0.451	0.800		
		2.973			1.251			
	ResFloW	2.649			0.033		2.908	0.253
0.7	ImpFlow	Fixed-point	Log-det	Others	Inv-Jacob	Others	4.644	0.181
		0.533	2.351	0.157	0.525	0.887		
		3.041			1.412			
	ResFloW	2.650			0.030		2.908	0.312
0.8	ImpFlow	Fixed-point	Log-det	Others	Inv-Jacob	Others	4.881	0.206
		0.602	2.364	0.139	0.641	0.943		
		3.105			1.584			
	ResFloW	2.655			0.030		2.910	0.374
0.9	ImpFlow	Fixed-point	Log-det	Others	Inv-Jacob	Others	5.197	0.258
		0.707	2.357	0.137	0.774	1.033		
		3.201			1.807			
	ResFloW	2.653			0.030		2.916	0.458
Table 5: Single-batch iterations of Broyden’s method during forward and backward pass for a single
block of ImpFlow in Table 3.
c	I Broyden,s Method Iterations		Function Evaluations
0.5	Forward	7.2	8.2
	Backward	12.5	13.5
0.6	Forward	8.3	9.3
	Backward	14.9	15.9
0.7	Forward	9.4	10.4
	Backward	17.9	18.9
0.8	Forward	10.8	11.8
	Backward	22.4	23.4
0.9	Forward	12.9	13.9
	Backward	27.4	28.4
C.3 Numerical Sensitivity
We train a 20-block ImpFlow on POWER dataset with f = 10-6 (see Appendix. D for detailed
settings), and then test this model with different f to see the numerical sensitivity of the fixed-point
iterations. Table 6 shows that our model is not sensitive to f in a fair range.
C.4 Training algorithm
We state the training algorithms for both forward and backward processes in Algorithm 1 and Algo-
rithm 2
19
Published as a conference paper at ICLR 2021
Algorithm 1: Forward Algorithm For a Single-Block ImpFlow
Require: gx”, gz” in Eqn. (3), stop criterion Ef.
Input: x.
Output: Z = f (x) and lnP(X), where f is the implicit function defined by gx” and gz”.
Define h(z) = F(z, x; θ)
z — 0
while kh(z)k2 ≥ Ef do
B J The estimated inverse Jacobian of h(z) (e.g. by Broyden's method)
a J LineSearch(z, h, B)
_ z J z — αBh(z)
if training then
Esitamate lndet(I + Jgx(x; θ)) by Eqn. (15)
Esitamate lndet(l + Jgz(z; θ)) by Eqn. (15)
else
Esitamate lndet(I + Jgx (x; θ)) by Eqn. (14)
_ Esitamate lndet(l + Jgz(z; θ)) by Eqn. (14)
Compute ln p(x) by Eqn. (13)
Algorithm 2: Backward Algorithm For a Single-Block ImpFlow
Require: gx”, gz” in Eqn. (3), stop criterion q.
Input： x, z, dL.
Output: The gradient for X and θ from z, i.e.除 ∂z and 除 ∂z.
Define G(z; θ) = gz(z; θ) + Z and h(y) = NJG⑦-dL
yJ0
while kh(y)k2 ≥ Eb do
B J The estimated inverse Jacobian of h(y) (e.g. by Broyden’s method)
α J LineSearch(y, h, B)
_ y J y — αBh(y)
Compute dFdy and dF¾Γθ) by autograd packages.
∂L ∂z J ∂F(z,x;6)
∂z ∂x	y	∂x
∂L ∂z J X ∂F(z2xiθ)
∂z ∂θ J y	∂θ
20
Published as a conference paper at ICLR 2021
Table 6: Average test log-likelihood (in nats) for different f of ImpFlow on POWER dataset.
f	I 10-8	10-7	10-6	10-5	10-4	10-3	10-2	10-1
log-likelihood	∣ 0.606	0.603	0.607	0.611	0.607	0.607	0.602	0.596
D Network Structures
D.1	1-D example
We specify the function (data) to be fitted is
f(x)
0.1x,
10x,
x<0
x≥0
ForI, we can construct a fully-connected neural network with ReLU activation and 3 parameters as
following:
gx(x) = ReLU(-0.9x)
gz(z) = -√09ReLU(√09z)
The two networks can be implemented by spectral normalization. Assume the implicit function
defined by Eqn. (3) using the above gx(x) and gz(z) is fI. Next we show that f = fI.
Let fι(x) = X + ReLU(-0.9x) and f2(x) = X — √0.9ReLU(√0.9x), We have f-1(x) = X +
ReLU(9x). Therefore, fI = f2-1 ◦ f1 = f.
For every residual block of R,R2 and R3, We train a 4-layer MLP With ReLU activation and 128
hidden units, and the Lipschitz coefficient for the spectral normalization is 0.99, and the iteration
number for the spectral computation is 200. The objective function is
minEx〜Unif(-ι,i) [(fθ(x) - f (x))2],
θ
Where fθ is the function of 1 or 2 or 3 residual blocks. We use a batch size of 5000. We use the Adam
optimizer, With learning rate 10-3 and Weight decay 10-5. We train the model until convergence,
on a single NVIDIA GeForce GTX 1080Ti.
The losses for R,R2 and R3 are 5.25, 2.47, 0.32, respectively.
D.2 Classification
For the classification tasks, We remove all the BatchNorm layers Which are inside of a certain Res-
Block, and only maintain the BatchNorm layer in the doWnsampling layer. Moreover, as a single
ImpFloW consists of tWo residual blocks With the same dimension of input and output, We replace
the doWnsampling shortcut by a identity shortcut in each scale of ResNet-18, and add a doWnsam-
pling layer (a convolutional layer) With BatchNorm after the tWo residual blocks of each scale. Thus,
each scale consists of tWo ResBlocks With the same dimension of input and output, Which (6.5M
parameters) is different from the vanilla ResNet-18 architecture (11.2M parameters). Note that the
“vanilla ResNet-18” in our main text is refered to the 6.5M-parameter architecture, Which is the
same as the versions for ResFloW and ImpFloW.
We use the comman settings: batch size of 128, Adam optimizer With learning rate 10-3 and no
Weight decay, and total epoch of 150. For the spectral normalization iterations, We use a error bound
of 10-3, the same as Chen et al. (2019). We train every experiment on a single NVIDIA GeForce
GTX 2080Ti.
D.3 Density Modeling on Toy 2D Data
FolloWing the same settings as Chen et al. (2019), We use 4-layer multilayer perceptrons (MLP)
With fully-connected layers of 128 hidden units. We use the Adam optimizer With learning rate of
10-3 and weight decay of 10-5. Moreover, we find that 2∏ sin(2∏x) is a better activation for this
21
Published as a conference paper at ICLR 2021
task while maintain the property of 1-Lipschitz constant, so we use this activation function for all
experiments, which can lead to faster convergence and better log-likelihood for both ResFlows and
ImpFlows, as shown in Fig. 4.
We do not use any ActNorm or BatchNorm layers. For the log-determinant term, we use brute-force
computation as in Chen et al. (2019). For the forward and backward, we use the Broyden’s method
to compute the roots, with f = 10-6. The Lipschitz coefficient for spectral normalization is 0.999,
and the iteration number for spectral normalization is 20. The batch size is 5000, and we train 50000
iterations. The test batch size is 10000.
Also, we vary the network depth to see the difference between ImpFlow and ResFlow. For every
depth L, we use an L-block ImpFlow and a 2L-block ResFlow with the same settings as stated
above, and train 3 times with different random seeds. As shown in Figure 5, the gap between
ImpFlow and ResFlow shrinks as the depth grows deep, because the Lipschitz constant of ResFlow
grows exponentially. Note that the dashed line is a 200-block ResFlow in Chen et al. (2019), and we
tune our model better so that our models perform better with lower depth.
(a) LiPSWish (5.22 bits)
(b) Sine (5.08 bits)
Figure 4: 8-block ReSFloW with different activation function trained on Checkerboard dataset.
Figure 5: Test NLL (in bits) by varying the network depth. Lower is better.
D.4 Density Modeling on Tabular Datasets
We use the same data preprocessing as Papamakarios et al. (2017), including the train/valid/test
datasets splits. For all models, we use a batch size of 1000 (both training and testing) and learning
rate of 10-3 for the Adam optimizer. The main settings are the same as Chen et al. (2019) on the
toy2D dataset. The residual blocks are 4-layer MLPs with 128 hidden units. The ResFlows use 10
22
Published as a conference paper at ICLR 2021
blocks and ImpFlows use 5 blocks to ensure the same amount of parameters. And we use a 20-block
ImPFloW for a better result. Also, We use the Sine activation as 2∏ sin(2∏x). We do not use any
ActNorm or BatchNorm layers. For the Lipschitz coefficient, we use c = 0.9 and the iteration error
bound for sPectral normalization is 10-3.
For the settings of our scalable algorithms, We use brute-force comPutation of the log-determinant
term for POWER and GAS datasets and use the same estimation settings as Chen et al. (2019) for
HEPMASS, MINIBOONE and BSDS300 datasets. In Particular, for the estimation settings, We
alWays exactly comPute 2 terms in training Process and 20 terms in testing Process for the log-
determinant series. We use a geometric distribution of p = 0.5 for the distribution p(N) for the
log-determinant term. We use a single samPle of (n, v) for the log-determinant estimators for both
training and testing.
We train each exPeirment on a single NVIDIA GeForce GTX 2080Ti for about 4 days for ResFloWs
and 6 days for ImPFloWs. For 20-block ImPFloW, We train our model for about 2 Weeks. HoWever,
We find that the 20-block ImPFloW Will overfit the training dastaset for MINIBOONE because this
dataset is quite small, so We use the early-stoPPing technique.
D.5 Density Modeling on Image Datasets
For the CIFAR10 dataset, We folloW the same settings and architectures as Chen et al. (2019). In
Particular, every convolutional residual block is
LipSwish → 3 × 3 Conv → LipSwish → 1 × 1 Conv → LipSwish → 3 × 3 Conv.
The total architecture is
Image → LogitTransform(α) → k × ConvBlock → [Squeeze → k × ConvBlock] × 2,
Where ConvBlock is i-ResBlock for ResFloWs and ImPBlock for ImPBlock, and k = 4 for ResFloWs
and k = 2 for ImPFloWs. And the first ConvBlock does not have LiPSWish as Pre-activation,
folloWed as Chen et al. (2019). We use ActNorm2d after every ConvBlock. We do not use the FC
layers (Chen et al., 2019). We use hidden channels as 512. We use batch size of 64 and the Adam
oPtimizer of learning rate 10-3. The iteration error bound for sPectral normalization is 10-3. We
use α = 0.05 for CIFAR10.
For the settings of our scalable algorithms, We use the same as Chen et al. (2019) for the log-
determinant terms. In Particular, We alWays exactly comPute 10 terms in training Process and 20
terms in testing Process for the log-determinant series. We use a Possion distribution for the distri-
bution p(N) for the log-determinant term. We use a single samPle of (n, v) for the log-determinant
estimators for both training and testing.
We train each ResFloW on a single NVIDIA GeForce GTX 2080Ti and each ImPFloW on tWo cards
of NVIDIA GeForce GTX 2080Ti for about 6 days for ResFloWs and8 days for ImPFloWs. Although
the amount of Parameters are the same, ImPFloWs need more GPU memory due to the imPlementa-
tion of PyTorch for the backWard Pass of imPlicit function.
For the CelebA dataset, We use exactly the same settings as the final version of ResFloWs in Chen
et al. (2019), except that we use the Sine activation of the form as ɪ sin(2∏x).
E	ImpFlow S amples
23
Published as a conference paper at ICLR 2021
Figure 6: Qualitative samples on 5bit 64×64 CelebA by ImpFlow, with a temperature of 0.8(Kingma
& Dhariwal, 2018)
24