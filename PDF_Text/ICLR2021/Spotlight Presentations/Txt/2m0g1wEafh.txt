Published as a conference paper at ICLR 2021
B enefit of deep learning with non-convex
noisy gradient descent: Provable excess risk
BOUND AND SUPERIORITY TO KERNEL METHODS
Taiji Suzuki
Graduate School of Information Science and Technology, The University of Tokyo, Japan
Center for Advanced Intelligence Project, RIKEN, Japan
E-mail: taiji@mist.i.u-tokyo.ac.jp
Shunta Akiyama
Graduate School of Information Science and Technology, The University of Tokyo, Japan
E-mail: akiyama@mist.i.u-tokyo.ac.jp
Ab stract
Establishing a theoretical analysis that explains why deep learning can outperform
shallow learning such as kernel methods is one of the biggest issues in the deep
learning literature. Towards answering this question, we evaluate excess risk of
a deep learning estimator trained by a noisy gradient descent with ridge regular-
ization on a mildly overparameterized neural network, and discuss its superiority
to a class of linear estimators that includes neural tangent kernel approach, ran-
dom feature model, other kernel methods, k-NN estimator and so on. We consider
a teacher-student regression model, and eventually show that any linear estimator
can be outperformed by deep learning in a sense of the minimax optimal rate espe-
Cially for a high dimension setting. The obtained excess bounds are so-called fast
learning rate which is faster than O(1/√n) that is obtained by usual Rademacher
complexity analysis. This discrepancy is induced by the non-convex geometry of
the model and the noisy gradient descent used for neural network training provably
reaches a near global optimal solution even though the loss landscape is highly
non-convex. Although the noisy gradient descent does not employ any explicit
or implicit sparsity inducing regularization, it shows a preferable generalization
performance that dominates linear estimators.
1	Introduction
In the deep learning theory literature, clarifying the mechanism by which deep learning can out-
perform shallow approaches has been gathering most attention for a long time. In particular, it is
quite important to show that a tractable algorithm for deep learning can provably achieve a better
generalization performance than shallow methods. Towards that goal, we study the rate of conver-
gence of excess risk of both deep and shallow methods in a setting of a nonparametric regression
problem. One of the difficulties to show generalization ability of deep learning with certain opti-
mization methods is that the solution is likely to be stacked in a bad local minimum, which prevents
us to show its preferable performances. Recent studies tackled this problem by considering opti-
mization on overparameterized networks as in neural tangent kernel (NTK) (Jacot et al., 2018; Du
et al., 2019a) and mean field analysis (Nitanda & Suzuki, 2017; Chizat & Bach, 2018; Rotskoff &
Vanden-Eijnden, 2018; 2019; Mei et al., 2018; 2019), or analyzing the noisy gradient descent such
as stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011; Raginsky et al., 2017;
Erdogdu et al., 2018).
The NTK analysis deals with a relatively large scale initialization so that the model is well approxi-
mated by the tangent space at the initial solution, and eventually, all analyses can be reduced to those
of kernel methods (Jacot et al., 2018; Du et al., 2019b; Allen-Zhu et al., 2019; Du et al., 2019a; Arora
et al., 2019; Cao & Gu, 2019; Zou et al., 2020). Although this regime is useful to show its global
1
Published as a conference paper at ICLR 2021
convergence, the obtained estimator looses large advantage of deep learning approaches because the
estimation ability is reduced to the corresponding kernel methods. To overcome this issue, there are
several “beyond-kernel” type analyses. For example, Allen-Zhu & Li (2019; 2020) showed benefit
of depth by analyzing ResNet type networks. Li et al. (2020) showed global optimality of gradient
descent by reducing the optimization problem to a tensor decomposition problem for a specific re-
gression problem, and showed the “ideal” estimator on a linear model has worse dependency on the
input dimensionality. Bai & Lee (2020) considered a second order Taylor expansion and showed
that the sample complexity of deep approaches has better dependency on the input dimensionality
than kernel methods. Chen et al. (2020) also derived a similar conclusion by considering a hierarchi-
Cal representation. The analyses mentioned above actually show some superiority of deep learning,
but all of these bounds are essentially Ω(1∕√n) where n is the sample size, which is not optimal
for regression problems with squared loss (Caponnetto & de Vito, 2007). The reason why only
such a sub-optimal rate is considered is that the target of their analyses is mostly the Rademacher
complexity of the set in which estimators exist for bounding the generalization gap. However, to
derive a tight excess risk bound instead of the generalization gap, we need to evaluate so called local
Rademacher complexity (Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2006) (see Eq. (2)
for the definition of excess risk). Moreover, some of the existing analyses should change the target
function class as the sample size n increases, for example, the input dimensionality is increased
against the sample size, which makes it difficult to see how the rate of convergence is affected by
the choice of estimators.
Another promising approach is the mean field analysis. There are also some work that showed
superiority of deep learning against kernel methods. Ghorbani et al. (2019) showed that, when
the dimensionality d of input is polynomially increasing with respect to n, the kernel methods is
outperformed by neural network approaches. Although the situation of increasing d explains well
the modern high dimensional situations, this setting blurs the rate of convergence. Actually, we can
show the superiority of deep learning even in a fixed dimension setting.
There are several studies about approximation abilities of deep and shallow models. Ghorbani et al.
(2020) showed adaptivity of kernel methods to the intrinsic dimensionality in terms of approxi-
mation error and discuss difference between deep and kernel methods. Yehudai & Shamir (2019)
showed that the random feature method requires exponentially large number of nodes against the
input dimension to obtain a good approximation fora single neuron target function. These are only
for approximation errors and estimation errors are not compared.
Recently, the superiority of deep learning against kernel methods has been discussed also in the
nonparametric statistics literature where the minimax optimality of deep learning in terms of excess
risk is shown. Especially, it is shown that deep learning achieves better rate of convergence than
linear estimators in several settings (Schmidt-Hieber, 2020; Suzuki, 2019; Imaizumi & Fukumizu,
2019; Suzuki & Nitanda, 2019; Hayakawa & Suzuki, 2020). Here, the linear estimators are a general
class of estimators that includes kernel ridge regression, k-NN regression and Nadaraya-Watson
estimator. Although these analyses give clear statistical characterization on estimation ability of
deep learning, they are not compatible with tractable optimization algorithms.
In this paper, we give a theoretical analysis that unifies these analyses and shows the superiority of a
deep learning method trained by a tractable noisy gradient descent algorithm. We evaluate the excess
risks of the deep learning approach and linear estimators in a nonparametric regression setting, and
show that the minimax optimal convergence rate of the linear estimators can be dominated by the
noisy gradient descent on neural networks. In our analysis, the model is fixed and no explicit sparse
regularization is employed. Our contributions can be summarized as follows:
•	A refined analysis of excess risks for a fixed model with a fixed input dimension is given to com-
pare deep and shallow estimators. Although several studies pointed out the curse of dimensionality
is a key factor that separates shallow and deep approaches, we point out that such a separation ap-
pears in a rather low dimensional setting, and more importantly, the non-convexity of the model
essentially makes the two regimes different.
•	A lower bound of the excess risk which is valid for any linear estimator is derived. The analysis is
considerably general because the class of linear estimators includes kernel ridge regression with
any kernel and thus it also includes estimators in the NTK regime.
•	All derived convergence rate is a fast learning rate that is faster than O(1∕√n). We show that
simple noisy gradient descent on a sufficiently wide two-layer neural network achieves a fast
2
Published as a conference paper at ICLR 2021
learning rate by using a fact that the solution converges to a Bayes estimator with a Gaussian
process prior, and the derived convergence rate can be faster than that of linear estimators. This
is much different from such existing work that compared only coefficients with the same rate of
convergence with respect to the sample size n.
Other related work Bach (2017) analyzed the model capacity of neural networks and its corre-
sponding reproducing kernel Hilbert space (RKHS), and showed that the RKHS is much larger than
the neural network model. However, separation of the estimation abilities between shallow and deep
is not proven. Moreover, the analyzed algorithm is basically the Frank-Wolfe type method which is
not typically used in practical deep learning. The same technique is also employed by Barron (1993).
The Frank-Wolfe algorithm is a kind of sparsity inducing algorithm that is effective for estimating
a function in a model with an L1-norm constraint. It has been shown that explicit or implicit sparse
regularization such as L1 -regularization is beneficial to obtain better performances of deep learning
under certain situations (Chizat & Bach, 2020; Chizat, 2019; Gunasekar et al., 2018; Woodworth
et al., 2020; Klusowski & Barron, 2016). For example, E et al. (2019b;a) showed that the approxi-
mation error of a linear model suffers from the curse of dimensionality in a setting where the target
function is in the Barron class (Barron, 1993), and showed an L1 -type regularization avoids the curse
of dimensionality. However, our analysis goes in a different direction where a sparse regularization
is not required.
2	Problem setting and model
In this section, we give the problem setting and notations that will be used in the theoretical analy-
sis. We consider the standard nonparametric regression problem where data are generated from the
following model for an unknown true function fo : Rd → R:
yi = f (xi ) + i (i = 1, . . . , n),	(1)
where Xi is independently identically distributed from Pχ whose support is included in Ω = [0,1]d,
and i is an observation noise that is independent of xi and satisfies E[i] = 0 and i ∈ [-U, U]
almost surely. The n i.i.d. observations are denoted by Dn = (xi, yi)in=1. We want to estimate the
true function fo through the training data Dn. To achieve this purpose, we employ the squared loss
'(y, f (x)) = (y - f (x))2 and accordingly We define the expected and empirical risks as L(f):=
Eγ,χ['(Y,f(X))] and L(f ):= 1 Pn=I '(yi,f(xi)) respectively. Throughout this paper, we are
interested in the excess (expected) risk of an estimator f defined by
(Excess risk)	L(fb) - inf	L(f).	(2)
f:measurable
Since the loss function ` is the squared loss, the infimum of inff:measurable L(f) is achieved by
fo: inff:measUrabIeL(f) = L(f°)∙ The population L2(Pχ)-norm is denoted by IIfllL2(Pχ):=
,Eχ~Px [f(X)2] and the sup-norm on the support of Pχ is denoted by ∣∣fk∞	:=
supx∈supp(PX) |f (x)|. We can easily check that for an estimator f, the L2-distance ∣f - fo∣2L (P )
between the estimator f and the true function fo is identical to the excess risk: L(f) - L(fo) =
∣f - fo∣2L (P ). Note that the excess risk is different from the generalization gap L(f) - L(f).
Indeed, the generalization gap typically converges with the rate of O(1/√n) which is optimal in a
typical setting (Mohri et al., 2012). On the other hand, the excess risk can be faster than O(1/√n),
which is known as a fast learning rate (Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2006;
Gine & Koltchinskii, 2006).
2.1	Model of true functions
To analyze the excess risk, we need to specify a function class (in other words, model) in which
the true function fo is included. In this paper, we only consider a two layer neural network model,
whereas the techniques adapted in this paper can be directly extended to deeper neural network
models. We consider a teacher-student setting, that is, the true function fo can be represented by
a neural network defined as follows. For W ∈ R, let W be a “clipping” of W defined as w :=
3
Published as a conference paper at ICLR 2021
R × tanh(w/R) where R ≥ 1 is a fixed constant, and let [x; 1] := [x>, 1]> for x ∈ Rd. Then, the
teacher network is given by
fW(X) = Pm=I amw2,mσm(W>m[x； 1]),
where w1,m ∈ Rd+1 and w2,m ∈ R (m ∈ N) are the trainable parameters (where W =
(w1,m, w2,m)m∞=1), am ∈ R (m ∈ N) is a fixed scaling parameter, and σm : R → R is an activation
function for the m-th node. The reason why we applied the clipping operation to the parameter of
the second layer is just for a technical reason to ensure convergence of Langevin dynamics. The dy-
namics is bounded in high probability in practical situations and the boundedness condition would
be removed if further theoretical development of infinite dimensional Langevin dynamics would be
achieved.
Let H be a set of parameters W such that its squared norm is bounded: H := {W =
(w1,m,w2,m)m∞=1 | Pm∞=1(kw1,mk2 + w22,m) < ∞}. Define kWkH := [Pm∞=1(kw1,mk2 +
w2,m)]1/2 for W ∈ H. Let (μm)∞=1 be a regularization parameter SUCh that μm & 0. Accordingly
Wedefine HY = {w ∈ H | kW∣∣Hγ < ∞} where kW∣∣Hγ := P∞=1 μmγ(Ilw1,mk2 + w2,m)]1/2
for a given 0 < γ. Throughout this paper, we analyze an estimation problem in which the true
function is included in the folloWing model:
Fγ={fW | W∈Hγ, kW kHγ ≤ 1}.
This is basically tWo layer neural netWork With infinite Width. As assumed later, am is assumed
to decrease as m → ∞. Its decreasing rate controls the capacity of the model. If the first layer
parameters (w1,m)m are fixed, this model can be regarded as a variant of the unit ball of some
reproducing kernel Hilbert space (RKHS) With basis functions amσm(w1>,m[x; 1]). HoWever, since
the first layer (w1,m) is also trainable, there appears significant difference betWeen deep and kernel
approaches. The Barron class (Barron, 1993; E et al., 2019b) is relevant to this function class.
Indeed, it is defined as the convex hull of w2σ(w1> [x; 1]) With norm constraints on (w1, w2) Where
σ is an activation function. On the other hand, We Will put an explicit decay rate on am and the
parameter W has an L2-norm constraint, Which makes the model Fγ smaller than the Barron class.
3	Estimators
We consider tWo classes of estimators and discuss their differences: linear estimators and deep
learning estimator With noisy gradient descent (NGD).
Linear estimator A class of linear estimators, Which We consider as a representative of “shalloW”
learning approach, consists of all estimators that have the folloWing form:
f(X) = Pn=1 y Wi(X1,...,xn ,x).
Here, (wi)n=ι can be any measurable function (and L2(Pχ)-integrable so that the excess risk can
be defined). Thus, they could be selected as the “optimal” one so that the corresponding linear esti-
mator minimizes the Worst case excess risk. Even if We chose such an optimal one, the Worst case
excess risk should be loWer bounded by our loWer bound given in Theorem 1. It should be noted that
the linear estimator does not necessarily imply “linear model.” The most relevant linear estimator
in the machine learning literature is the kernel ridge regression: f(X) = Y >(Kχ + λI)-1k(X)
Where Kχ = (k(Xi, Xj))in,j =1 ∈ Rn×n, k(X) = [k(X, X1), . . . , k(X, Xn)]> ∈ Rn and Y =
[y1, . . . , yn]> ∈ Rn for a kernel function k : Rd × Rd → R. Therefore, the ridge regression
estimator in the NTK regime or the random feature model is also included in the class of linear
estimators. The solution obtained in the early stopping criteria instead of regularization in the NTK
regime under the squared loss is also included in the linear estimators. Other examples include the
k-NN estimator and the Nadaraya-Watson estimator. All of them do not train the basis function in a
nonlinear Way, Which makes difference from the deep learning approach. In the nonparametric statis-
tics literature, linear estimators have been studied for estimating a Wavelet series model. Donoho
et al. (1990; 1996) have shoWn that a Wavelet shrinkage estimator can outperform any linear estima-
tor by shoWing suboptimality of linear estimators. Suzuki (2019) utilized such an argument to shoW
superiority of deep learning but did not present any tractable optimization algorithm.
4
Published as a conference paper at ICLR 2021
Noisy Gradient Descent with regularization As for the neural network approach, we consider a
noisy gradient descent algorithm. Basically, we minimize the following regularized empirical risk:
Lb(fw) + 2 kW kHι.
Here, we employ H1-norm as the regularizer. We note that the constant γ controls the
relative complexity of the true function fo compared to the typical solution obtained un-
der the regularization. Here, we define a linear operator A as λkW kH1 = W>AW, that
is, AW = (λμm1wι,m, λμmlw2,m)∞=1. The regularized empirical risk can be minimized
by noisy gradient descent as	Wk+1 = Wk -	ηV(L(fw%)	+ 2 kWkkH1) + ʌ/2Fξk,	where
η > 0 is a step size and ξk = (ξk,(1,m), ξk,(2,m))m∞=1	is an infinite-dimensional	Gaus-
sian noise, i.e., ξk,(1,m) and ξk,(2,m) are independently identically distributed from the stan-
dard normal distribution (Da Prato & Zabczyk, 1996). Here, VLb(fw) = § Pi=ι 2(fw(Xi)-
yi)(W2,mam[xi; 1]σm(w>m[xi； 1]), am tanh0(w2,m∕R)σm(w>m[Xi； 1]))∞=1. However, since
VkWk-1k2H is unbounded which makes it difficult to show convergence, we employ the semi-
implicit Euler scheme defined by
Wk+1=Wk-ηvLb(fwk)-ηAWk+ι+腮ξk ⇔ Wk+ι=Sn (Wk-ηvL(fwk)+腮ξ,,⑶
where Sη := (I+ηA)-1. It is easy to check that this is equivalent to the following update rule: Wk =
Wk-ι - η (VL(fwk-ι) + SnAWk-ι + ^2∙ξk-ι) . Therefore, the implicit Euler scheme can be
seen as a naive noisy gradient descent for minimizing the empirical risk with a slightly modified
ridge regularization. This can be interpreted as a discrete time approximation of the following
infinite dimensional Langevin dynamics:
dWt = -V(L(fwt) + 2 kWtkHι)dt + P2∕βdξt,	(4)
where (ξt)t≥0 is the so-called cylindrical Brownian motion (see Da Prato & Zabczyk (1996) for
the details). Its application and analysis for machine learning problems with non-convex objectives
have been recently studied by, for example, Muzellec et al. (2020); Suzuki (2020).
The above mentioned algorithm is executed on an infinite dimensional parameter space. In practice,
we should deal with a finite width network. To do so, we approximate the solution by a finite
dimensional one: W(M) = (w1,m, w2,m)mM=1 where M corresponds to the width of the network.
We identify W(M) to the “zero-padded” infinite dimensional one, W = (w1,m, w2,m)m∞=1 with
w1,m = 0 and w2,m = 0 for all m > M. Accordingly, we use the same notation fW(M) to indicate
fw with zero padded vector W. Then, the finite dimensional version of the update rule is given by
WkM) = SnM) (WkM) - ηVLb(fw(M)) + qnξkM)) , where ξM is the Gaussian noise vector
obtained by projecting ξk to the first M components and Sn(M) is also obtained in a similar way.
4	Convergence rate of estimators
In this section, we present the excess risk bounds for linear estimators and the deep learning estima-
tor. As for the linear estimators, we give its lower bound while we give an upper bound for the deep
learning approach. To obtain the result, we setup some assumptions on the model.
Assumption 1.
(i)	There exists a constant Cμ such that μm, ≤ c*m-2 (m ∈ N).
(ii)	There exists αι > 1/2 such that am, ≤ μm (m ∈ N).
(iii)	The activation functions (σm)m is bounded as kσmk∞ ≤ 1. Moreover, they are three times
differentiable and their derivatives upto third order differentiation are uniformly bounded:
∃Cσ such that kσmk1,3 := max{kσm0 k∞, kσm00 k∞, kσm000k∞} ≤ Cσ (∀m ∈ N).
The first assumption (i) controls the strength of the regularization, and combined with the second
assumption (ii) and definition of the model Fγ, complexity of the model is controlled. If α1 and γ
are large, the model is less complicated. Indeed, the convergence rate of the excess risk becomes
5
Published as a conference paper at ICLR 2021
faster if these parameters are large as seen later. The decay rate μm, ≤ c*m-2 can be generalized as
m-p with p > 1 but we employ this setting just for a technical simplicity for ensuring convergence
of the Langevin dynamics. The third assumption (iii) can be satisfied by several activation functions
such as the sigmoid function and the hyperbolic tangent. The assumption kσm k∞ ≤ 1 could be
replaced by another one like kσm k∞ ≤ C , but we fix this scaling for simple presentation.
4.1	Minimax lower bound for linear estimators
Here, we analyze a lower bound of excess risk of linear estimators, and eventually we show that
any linear estimator suffers from curse of dimensionality. To rigorously show that, we consider the
following minimax excess risk over the class of linear estimators:
Rlin(Fγ) := inf sup EDn[kfb- fok2L2(PX)],
fb:linear f o ∈Fγ
where inf is taken over all linear estimators and EDn [∙] is taken with respect to the training data Dn.
This expresses the best achievable worst case error over the class of linear estimators to estimate a
function in Fγ . To evaluate it, we additionally assume the following condition.
Assumption 2. We assume that μm = m-2 and am = μm (m ∈ N) (and hence Cμ = 1). There
exists a monotonically decreasing Sequence (bm,)∞= and S ≥ 3 such that bm, = μm (Vm) with
a2 > γ∕2 andσm,(u) = bmlσ(bm1u)(u ∈ R) where σ isthesigmoidfunction: σ(u) = 1/(1 + e-u).
Intuitively, the parameter s controls the “resolution” of each basis function σm , and the relation
between parameter α1 and α2 controls the magnitude of coefficient for each basis σm . Note that the
condition s ≥ 3 ensures kσmk1,3 is uniformly bounded and 0 < bm ≤ 1 ensures kσmk∞ ≤ 1. Our
main strategy to obtain the lower bound is to make use of the so-called convex-hull argument. That
is, it is known that, for a function class F, the minimax risk R(F) over a class of linear estimators
is identical to that for the convex hull ofF (Hayakawa & Suzuki, 2020; Donoho et al., 1990):
Rlin(F) = Rlin(Conv(F)),
where conv(F) = {PN=ι λif | f ∈ F, PN=I λ% = 1, λi ≥ 0, N ∈ N} and cθnv(∙) is the
closure of conv(∙) with respect to L2(Pχ)-norm. Intuitively, since the linear estimator is linear to
the observations (yi)in=1 of outputs, a simple application of Jensen’s inequality yields that its worst
case error on the convex hull of the function class F does not increase compared with that on the
original one F (see Hayakawa & Suzuki (2020) for the details). This indicates that the linear es-
timators cannot distinguish the original hypothesis class F and its convex hull. Therefore, if the
class F is highly non-convex, then the linear estimators suffer from much slower convergence rate
because its convex hull ConV(F) becomes much “fatter” than the original one F. To make use of
this argument, for each sample size n, we pick up appropriate mn and consider a subset generated
by the basis function σmn, i.e., Fn) := {amn W2,mnσm(w>mn [x; 1]) ∈ FY}. By applying the con-
vex hull argument to this set, We obtain the relation Rlin(FY) ≥ Rlin(Fn)) = Rlin(COnV(Fn))).
Since Fn) is highly non-convex, its convex hull ConV(Fn)) is much larger than the original set
FY(n) and thus the minimax risk over the linear estimators would be much larger than that over all
estimators including deep learning. More intuitively, linear estimators do not adaptively select the
basis functions and thus they should prepare redundantly large class of basis functions to approx-
imate functions in the target function class. The following theorem gives the lower bound of the
minimax optimal excess risk over the class of linear estimators.
Theorem 1. Suppose that Var() > 0, PX is the uniform distribution on [0, 1]d, and Assumption 2
is satisfied. Let β = α1+(-+∕202. Thenfor arbitrary small K > 0, we have that
2β + d	0
Rlin(FY) & n-…n-κ .	(5)
The proof is in Appendix A. We utilized the Irie-Miyake integral representation (Irie & Miyake,
1988; Hornik et al., 1990) to show there exists a “complicated” function in the convex hull, and
then we adopted the technique of Zhang et al. (2002) to show the lower bound. The lower bound is
characterized by the decaying rate (α1) of am relative to that (α2) of the scaling factor bm. Indeed,
the faster am decays with increasing m, the faster the rate of the minimax lower bound becomes.
6
Published as a conference paper at ICLR 2021
We can see that the minimax rate of linear estimators is quite sensitive to the dimension d. Actually,
for relatively high dimensional settings, this lower bound becomes close to a slow rate Ω(1∕√n),
which corresponds to the curse of dimensionality.
It has been pointed out that the sample complexity of kernel methods suffers from the curse of
dimensionality while deep learning can avoid that by a tractable algorithms (e.g., Ghorbani et al.
(2019); Bach (2017)). Among them, Ghorbani et al. (2019) showed that if the dimensionality d is
polynomial against n, then the excess risk of kernel methods is bounded away from 0 for all n. On
the other hand, our analysis can be applied to any linear estimator including kernel methods, and
it shows that even if the dimensionality d is fixed, the convergence rate of their excess risk suffers
from the curse of dimensionality. This can be accomplished thanks to a careful analysis of the rate
of convergence. Bach (2017) derived an upper bound of the Rademacher complexity of the unit ball
of the RKHS corresponding to a neural network model. However, it is just an upper bound and there
is still a large gap from excess risk estimates. Allen-Zhu & Li (2019; 2020); Bai & Lee (2020); Chen
et al. (2020) also analyzed a lower bound of sample complexity of kernel methods. However, their
lower bound is not for the excess risk of the squared loss. Eventually, the sample complexities of all
methods including deep learning take a form of O(C∕√n) and dependency of coefficient C to the
dimensionality or other factors such as magnitude of residual components is compared. On the other
hand, our lower bound properly involves the properties of squared loss such as strong convexity and
smoothness and the bound shows the curse of dimensionality occurs even in the rate of convergence
instead of just the coefficient. Finally, we would like to point out that several existing work (e.g.,
Ghorbani et al. (2019); Allen-Zhu & Li (2019)) considered a situation where the target function
class changes as the sample size n increases. However, our analysis reveals that separation between
deep and shallow occurs even if the target function class Fγ is fixed.
4.2	Upper bound for deep learning
Here, we analyze the excess risk of deep learning trained by NGD and its algorithmic convergence
rate. Our analysis heavily relies on the weak convergence of the discrete time gradient Langevin dy-
namics to the stationary distribution of the continuous time one (Eq. (4)). Under some assumptions,
the continuous time dynamics has a stationary distribution (Da Prato & Zabczyk, 1992; Maslowski,
1989; Sowers, 1992; Jacquot & Royer, 1995; Shardlow, 1999; Hairer, 2002). Ifwe denote the prob-
ability measure on H corresponding to the stationary distribution by π∞, then it is given by
dd∏Vβ(W) X exp(-βLb(fw)),
where νβ is the Gaussian measure in H with mean 0 and covariance (βA)-1 (see Da Prato &
Zabczyk (1996) for the rigorous definition of the Gaussian measure on a Hilbert space). Remarkably,
this can be seen as the Bayes posterior for a prior distribution νβ and a “log-likelihood” function
exp(-β L(W)). Through this view point, we can obtain an excess risk bound of the solution Wk.
The proofs of all theorems in this section are in Appendix B.
Under Assumption 1, the distribution ofWk derived by the discrete time gradient Langevin synamics
satisfies the following weak convergence property to the stationary distribution π∞ . This conver-
gence rate analysis depends on the techniques by Brehier & Kopec (2016); Muzellec et al. (2020).
Proposition 1. Assume Assumption 1 holds and β > η. Then, there exist spectral gaps Λ and
ʌo (defined in Eq. (10) of Appendix B.1) and a constant C such that, for any 0 < a < 1/4, the
following convergence bound holds for almost sure observation Dn:
IEWk[L(fwk)Dn] - EW5∞[L(fw)∣Dn]∣ ≤ C0exp(-Ληηk) + Ci圣L- =： Ξk,	(6)
ʌ0
where Ci is a ConStant depending only on c*,R,αι,Cσ, U, a (independent of η,k,β,λ,n).
This proposition indicates that the expected risk of Wk can be almost identical to that of the “Bayes
posterior solution” obeying π∞ after sufficiently large iterations k with sufficiently small step size
η even though L(fw) is not convex. The definition of ʌ can be found in Eq. (10). We should
note that its dependency on β is exponential. Thus, if We take β = Ω(n), then the computational
cost until a sufficiently small error could be exponential with respect to the sample size n. The same
convergence holds also for finite dimensional one Wk(M) with a modified stationary distribution. The
7
Published as a conference paper at ICLR 2021
constants appearing in the bound are independent of the model size M (see the proof of Proposition
1 in Appendix B). In particular, the convergence can be guaranteed even if W is infinite dimensional.
This is quite different from usual finite dimensional analyses (Raginsky et al., 2017; Erdogdu et al.,
2018; Xu et al., 2018) which requires exponential dependency on the dimension, but thanks to the
regularization term, we can obtain the model size independent convergence rate. Xu et al. (2018)
also analyzed a finite dimensional gradient Langevin dynamics and obtained a similar bound where
O(η) appears in place of the second term n1/2-a which corresponds to time discretization error. In
our setting the regularization term is ∣∣ W∣∣Hι = Pm(Ilw1,mk2 + w2,m)∕μm With μm . m-2, but
if we employ kW k2H ■2 = Pm(kw1,mk2 + w],m","2 for P > 1, then the time discretization
error term would be modified to η(P-1)/p-a (Andersson et al., 2016). We can interpret the finite
dimensional setting as the limit of P → ∞ which leads to n(p-1)/p → η that recovers the finite
dimensional result (O(η)) as shown by Xu et al. (2018).
In addition to the above algorithmic convergence, we also have the following convergence rate for
the excess risk bound of the finite dimensional solution Wk(M).
Theorem 2. Assume Assumption 1 holds, assume η < β ≤ min{n/(2U 2), n}, and 0 < γ <
1/2 + αι. Then, if the width satisfies M ≥ min {X1/4Y(ai + 1)e1/2Y,AT/2(ai+1),n1/2Y}, the
expected excess risk of Wk is bounded as
[ι∕γ	1	1	T	γ 、
EW(M)[kfw(M)-f o kL2(Pχ )∖Dn]∖ ≤ C max{(λβ) 1+T2γ	γλ 2(a1+1) β-1λ E } + Ξk,
where C is a constant independent ofn, β, λ, η, k. In particular, ifwe set β = min{n/(2U 2 ), n}
and λ = β-1, thenfor M ≥ n1/2(a1+1), we obtain
EDn hEW(M) [kfW(M)- fokL2(Pχ)∖Dn]i . n-α+1 +Ek.
In addition to this theorem, ifwe further assume Assumption 2, we obtain a refined bound as follows.
Corollary 1. Assume Assumptions 1 and 2 hold and η < β, and let β = min{n/(2U 2), n} and
λ = β-1. Suppose that there exists 0 ≤ q ≤ s - 3 such that 0 < γ < 1/2 + α1 + qα2. Then, the
excess risk bound of WkM) for M ≥ n1/2(ai+qa2+1) can be refined as
EDnhEW(M) [∣∣fW(M)- f okL2(Pχ )∖Dn]i . n α1+qα2 + 1 + Ek .	⑺
These theorem and corollary shows that the tractable NGD algorithm achieves a fast convergence
rate of the excess risk bound. Indeed, if q is chosen so that Y > (αι +qɑ2 + 1)/2, then the excess risk
bound converges faster than O (1 / √n). Remarkably, the convergence rate is not affected by the input
dimension d, which makes discrepancy from linear estimators. The bound of Theorem 2 is tightest
when γ is close to 1/2 + α1 (γ ≈ 1/2 + α1 + 3α2 for Corollary 1), and a smaller γ yields looser
bound. This relation between γ and α1 reflects misspecification of the “prior” distribution. When
γ is small, the regularization λ∣W ∣2H is not strong enough so that the variance of the posterior
distribution becomes unnecessarily large for estimating the true function fo ∈ FY . Therefore, the
best achievable bound can be obtained when the regularization is correctly specified. The analysis
of fast rate is in contrast to some existing work (Allen-Zhu & Li, 2019; 2020; Li et al., 2020; Bai
& Lee, 2020) that basically evaluated the Rademacher complexity. This is because we essentially
evaluated a local Rademacher complexity instead.
4.3	Comparison between linear estimators and deep learning
Here, we compare the convergence rate of excess risks between the linear estimators and the neural
network method trained by NGD using the bounds obtained in Theorem 1 and Corollary 1 respec-
tively. We write the lower bound (5) of the minimax excess risk of linear estimators as Rjin and the
excess risk of the neural network approach (7) as RNn∙ To make the discussion concise, we consider
a specific situation where S = 3, αι = Y = 1 a2. In this case, β = 17/3 ≈ 5.667, which gives
≈ - (1+π⅛d
≈n
-1
n-κ0
8
Published as a conference paper at ICLR 2021
On the other hand, by setting q = 0, we have
RNN . n-α1+1 = n- (1+ α1).
Thus, as long as aι > 11.3/d + 1 ≈ 2β∕d + 1, We have that
Rlin & RNN, and limn→∞ RNN = 0.
In particular, as d gets larger, R[ approaches to Ω(n-1/2) while RNN is not affected by d and it
gets close to O(n-1) as α1 gets larger. Moreover, the inequality α1 > 11.3/d + 1 can be satis-
fied by a relatively low dimensional setting; for example, d = 10 is sufficient when α1 = 3. As
α1 becomes large, the model becomes “simpler” because (am )m∞=1 decays faster. However, the
linear estimators cannot take advantage of this information whereas deep learning can. From the
convex hull argument, this discrepancy stems from the non-convexity of the model. We also note
that the superiority of deep learning is shown without sparse regularization while several existing
work showed favorable estimation property of deep learning though sparsity inducing regularization
(Bach, 2017; Chizat, 2019; Hayakawa & Suzuki, 2020). However, our analysis indicates that sparse
regularization is not necessarily as long as the model has non-convex geometry, i.e., sparsity is just
one sufficient condition for non-convexity but not a necessarily condition. The parameter setting
above is just a sufficient condition and the lower bound Rllin would not be tight. The superiority of
deep learning would hold in much wider situations.
5	Conclusion
In this paper, we studied excess risks of linear estimators, as a representative of shallow methods,
and a neural network estimator trained by a noisy gradient descent where the model is fixed and no
sparsity inducing regularization is imposed. Our analysis revealed that deep learning can outperform
any linear estimator even for a relatively low dimensional setting. Essentially, non-convexity of the
model induces this difference and the curse of dimensionality for linear estimators is a consequence
of a fact that the geometry of the model becomes more “non-convex” as the dimension of input
gets higher. All derived bounds are fast rate because the analyses are about the excess risk with the
squared loss, which made it possible to compare the rate of convergence. The fast learning rate of
the deep learning approach is derived through the fact that the noisy gradient descent behaves like a
Bayes estimator with model size independent convergence rate.
Acknowledgments
TS was partially supported by JSPS Kakenhi (18K19793, 18H03201, and 20H00576), Japan Digital
Design and JST-CREST.
References
Z. Allen-Zhu and Y. Li. What can ResNet learn efficiently, going beyond kernels? In Advances in
Neural Information Processing Systems 32,pp. 9017-9028. Curran Associates, Inc., 2019.
Z. Allen-Zhu and Y. Li. Backward feature correction: How deep learning performs deep learning.
arXiv preprint arXiv:2001.04413, 2020.
Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In Proceedings of International Conference on Machine Learning, pp. 242-252, 2019.
A.	Andersson, R. Kruse, and S. Larsson. Duality in refined Sobolev-Malliavin spaces and weak ap-
proximation of SPDE. Stochastics and Partial Differential Equations Analysis and Computations,
4(1):113-149, 2016.
S.	Arora, S. S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and gen-
eralization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584,
2019.
9
Published as a conference paper at ICLR 2021
F. Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research,18(19):1-53, 2017.
Y. Bai and J. D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide
neural networks. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=rkllGyBFPH.
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930-945, 1993.
P. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. The Annals of Statis-
tics, 33:1487-1537, 2005.
C.-E. Brehier and M. Kopec. Approximation of the invariant law of SPDEs: error analysis using
a Poisson equation for a full-discretization scheme. IMA Journal of Numerical Analysis, 37(3):
1375-1410, 07 2016.
Y. Cao and Q. Gu. A generalization theory of gradient descent for learning over-parameterized deep
ReLU networks. arXiv preprint arXiv:1902.01384, 2019.
A. Caponnetto and E. de Vito. Optimal rates for regularized least-squares algorithm. Foundations
of Computational Mathematics, 7(3):331-368, 2007.
M. Chen, Y. Bai, J. D. Lee, T. Zhao, H. Wang, C. Xiong, and R. Socher. Towards understanding hier-
archical learning: Benefits of neural representations. Advances in Neural Information Processing
Systems, 33, 2020.
L. Chizat. Sparse optimization on measures with over-parameterized gradient descent. arXiv
preprint arXiv:1907.10300, 2019.
L. Chizat and F. Bach. A note on lazy training in supervised differentiable programming. arXiv
preprint arXiv:1812.07956, 2018.
L. Chizat and F. Bach. Implicit bias of gradient descent for wide two-layer neural networks trained
with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
G. Da Prato and J. Zabczyk. Non-explosion, boundedness and ergodicity for stochastic semilinear
equations. Journal of Differential Equations, 98:181-195, 1992.
G. Da Prato and J. Zabczyk. Ergodicity for Infinite Dimensional Systems. London Mathematical
Society Lecture Note Series. Cambridge University Press, 1996.
D. L. Donoho, R. C. Liu, and B. MacGibbon. Minimax risk over hyperrectangles, and implications.
The Annal of Statistics, 18(3):1416-1437, 09 1990. doi: 10.1214/aos/1176347758.
D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Density estimation by wavelet
thresholding. The Annals of Statistics, 24(2):508-539, 1996.
S.	Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural
networks. In International Conference on Machine Learning, pp. 1675-1685, 2019a.
S.	S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. International Conference on Learning Representations 7, 2019b.
W. E, C. Ma, and L. Wu. A priori estimates of the population risk for two-layer neural networks.
Communications in Mathematical Sciences, 17(5):1407-1425, 2019a.
W. E, C. Ma, and L. Wu. A comparative analysis of optimization and generalization properties of
two-layer neural network and random feature models under gradient descent dynamics. Science
China Mathematics, pp. 1-24, 2019b.
M. A. Erdogdu, L. Mackey, and O. Shamir. Global non-convex optimization with discretized diffu-
sions. In Advances in Neural Information Processing Systems 31, pp. 9671-9680. 2018.
10
Published as a conference paper at ICLR 2021
B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Linearized two-layers neural networks in
high dimension. arXiv preprint arXiv:1904.12191, 2019.
B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. When do neural networks outperform
kernel methods? arXiv preprint arXiv:2006.13409, 2020.
E. Gine and V. Koltchinskii. Concentration inequalities and asymptotic results for ratio type empir-
ical processes. TheAnnalsofProbabilily, 34(3):1143-1216, 2006.
S. Gunasekar, J. D. Lee, D. Soudry, and N. Srebro. Implicit bias of gradient descent on linear
convolutional networks. In Advances in Neural Information Processing Systems, pp. 9482-9491,
2018.
M. Hairer. Exponential mixing properties of stochastic PDEs through asymptotic coupling. Probab.
Theory Related Fields, 124(3):345-380, 2002.
S. Hayakawa and T. Suzuki. On the minimax optimality and superiority of deep neural network
learning over sparse parameter spaces. Neural Networks, 123:343-361, 2020. ISSN 0893-6080.
K. Hornik, M. Stinchcombe, and H. White. Universal approximation of an unknown mapping and
its derivatives using multilayer feedforward networks. Neural Networks, 3(5):551-560, 1990.
M. Imaizumi and K. Fukumizu. Deep neural networks learn non-smooth functions effectively. In
K. Chaudhuri and M. Sugiyama (eds.), Proceedings of Machine Learning Research, volume 89
of Proceedings of Machine Learning Research, pp. 869-878. PMLR, 16-18 Apr 2019.
B. Irie and S. Miyake. Capabilities of three-layered perceptrons. In IEEE 1988 International Con-
ference on Neural Networks, pp. 641-648, 1988.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in
neural networks. In Advances in Neural Information Processing Systems 31, pp. 8580-8589,
2018.
S. Jacquot and G. Royer. Ergodicite d'une classe d'equations aux derivees partielles stochastiques.
ComptesRendus de l,Academie des Sciences. Serie I. Mathematique, 320(2):231-236, 1995.
J. M. Klusowski and A. R. Barron. Risk bounds for high-dimensional ridge function combinations
including neural networks. arXiv preprint arXiv:1607.01434, 2016.
V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The
Annals of Statistics, 34:2593-2656, 2006.
Y. Li, T. Ma, and H. R. Zhang. Learning over-parametrized two-layer neural networks beyond ntk.
volume 125 of Proceedings of Machine Learning Research, pp. 2613-2682. PMLR, 09-12 Jul
2020.
B. Maslowski. Strong Feller property for semilinear stochastic evolution equations and applications.
In Stochastic systems and optimization (Warsaw, 1988), volume 136 of Lect. Notes Control Inf.
Sci., pp. 210-224. Springer, Berlin, 1989.
S. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural
networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671, 2018. doi:
10.1073/pnas.1806579115.
S. Mei, T. Misiakiewicz, and A. Montanari. Mean-field theory of two-layers neural networks:
dimension-free bounds and kernel limit. In A. Beygelzimer and D. Hsu (eds.), Proceedings of
the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learn-
ing Research, pp. 2388-2464, Phoenix, USA, 25-28 Jun 2019. PMLR.
S.	Mendelson. Improving the sample complexity using global data. IEEE Transactions on Informa-
tion Theory, 48:1977-1991, 2002.
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. The MIT Press,
2012.
11
Published as a conference paper at ICLR 2021
B. Muzellec, K. Sato, M. Massias, and T. Suzuki. Dimension-free convergence rates for gradient
Langevin dynamics in RKHS. arXiv preprint 2003.00306, 2020.
A. Nitanda and T. Suzuki. Stochastic particle gradient descent for infinite ensembles. arXiv preprint
arXiv:1712.05438, 2017.
M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via Stochastic Gradient Langevin
Dynamics: a nonasymptotic analysis. arXiv e-prints, pp. arXiv:1702.03849, 2017.
G. Rotskoff and E. Vanden-Eijnden. Parameters as interacting particles: long time convergence
and asymptotic error scaling of neural networks. In Advances in Neural Information Processing
Systems 31, pp. 7146-7155. Curran Associates, Inc., 2018.
G. M. Rotskoff and E. Vanden-Eijnden. Trainability and accuracy of neural networks: An interacting
particle system approach. arXiv preprint arXiv:1805.00915, 2019.
W. Rudin. Real and Complex Analysis (third edition). Mathematics series. McGraw-Hill, 1987.
J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation
function. The Annals of Statistics, 48(4), 2020.
T.	Shardlow. Geometric ergodicity for stochastic PDEs. Stochastic Analysis and Applications, 17
(5):857-869, 1999.
R.	Sowers. Large deviations for the invariant measure of a reaction-diffusion equation with non-
Gaussian perturbations. Probab. Theory Related Fields, 92(3):393-421, 1992. ISSN 0178-8051.
T. Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:
optimal rate and curse of dimensionality. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=H1ebTsActm.
T. Suzuki. Generalization bound of globally optimal non-convex neural network training: Trans-
portation map estimation by infinite dimensional langevin dynamics. In Advances in Neural
Information Processing Systems 33, pp. to appear. Curran Associates, Inc., 2020.
T. Suzuki and A. Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness
in anisotropic Besov space. arXiv preprint arXiv:1910.12799, 2019.
M. Welling and Y.-W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML,
pp. 681-688, 2011.
B. Woodworth, S. Gunasekar, J. D. Lee, E. Moroshko, P. Savarese, I. Golan, D. Soudry, and N. Sre-
bro. Kernel and rich regimes in overparametrized models. volume 125 of Proceedings of Machine
Learning Research, pp. 3635-3673. PMLR, 09-12 Jul 2020.
P. Xu, J. Chen, D. Zou, and Q. Gu. Global convergence of langevin dynamics based algorithms for
nonconvex optimization. In Advances in Neural Information Processing Systems, volume 31, pp.
3122-3133. Curran Associates, Inc., 2018.
G. Yehudai and O. Shamir. On the power and limitations of random features for understanding
neural networks. In Advances in Neural Information Processing Systems 32, pp. 6598-6608.
Curran Associates, Inc., 2019.
S.	Zhang, M.-Y. Wong, and Z. Zheng. Wavelet threshold estimation of a regression function with
random design. Journal of Multivariate Analysis, 80(2):256-284, 2002.
D. Zou, Y. Cao, D. Zhou, and Q. Gu. Gradient descent optimizes over-parameterized deep ReLU
networks. Machine Learning, 109(3):467-492, 2020.
12
Published as a conference paper at ICLR 2021
A Proof of Theorem 1
We basically combine the “convex hull argument” and the minimax optimal rate analysis for linear
estimators developed by Zhang et al. (2002).
Zhang et al. (2002) essentially showed the following statement in their Theorem 1.
Proposition 2 (Theorem 1 of Zhang et al. (2002)). Let μ be the Lebesgue measure. Suppose that
the space Ω has even partition A such that |A| = 2K for an integer K ∈ N, each A has equivalent
measure μ(A) = 2-K for all A ∈ A, and A is indeed a partition of Ω, i.e., ∪a∈a = Ω, A ∩ A0 = 0
for A, A0 ∈ Ω and A = A0. Then, if K is chosen as n-γ1 ≤ 2-K ≤ n-γ2 for constants γ1,γ2 › 0
that are independent of n, then there exists an event E such that, for a constant C0 > 0,
P(E) ≥ 1 + o(1) and |{xi | xi ∈ A (i ∈ {1, . . . , n})}| ≤ C0n/2K (∀A ∈ A).
Moreover, suppose that, for a class Fo of functions on Ω, there exists ∆ > 0 that satisfies the
following conditions:
1.	There exists F > 0 such that, for any A ∈ A, there exists g ∈ Fo that satisfies g(x) ≥
2 ∆F forall X ∈ A,
2.	There exists K0 and C00 > 0 such that 1 PZi g(xi)2 ≤ C00∆22-K0 for any g ∈ Fo on
the event E.
Then, there exists a constant F1 such that at least one of the following inequalities holds:
F2	2K0
k - ≤ Rlin(FO)，
F3
132∆22-K ≤ Rlm(Fo),
(8a)
(8b)
for sufficiently large n.
Before we show the main assertion, we prepare some additional lemmas. For a sigmoid function σ,
let JFCσT := {x ∈ Rd → aσ(τ(w>x + b))) | |a| ≤ 2C, kw∣∣ ≤ 1, |b| ≤ 2 (a, b ∈ R, w ∈ Rd)} for
C > 0,,τ > 0.
1
Lemma 1. Let ψ(x) = ∙2(σ(x + 1) 一 σ(x - 1)) and ψ be its Fourier transform: ψ(ω):=
(2π)-1 R e-iωxψ(x)dx. Let h > 0 and Dw > 0. Then, by setting T = h-1(2√d + 1)Dw and
C =(27^d+1)Dw, the Gaussian RBF kernel can be approximated by
inf sup
g∈conv(F(^T) χ∈[0,i]d
≤ ∣2πψ(1)∣ hCdDw(d-2) eXP(-Dw /2)+exP(-Dw)i
for any c ∈ [0, 1]d, where Cd is a constant depending only on d. In particular, the right hand side is
O(eXp(-nκ)) ifDw = nκ.
kx - ck2
g(x)-exp(--k
Proof of Lemma 1. Let ψh(x) = ψ(h-1x). Suppose that, for f ∈ L1(Rd), its Fourier transform
f(ω) = (2π)-d R e-iω>xf (χ)dχ (ω ∈ Rd) gives
/ exp(iw>x)f(w)dw
f (x),
for every x ∈ Rd1. Then the Irie-Miyake itegral representation (Irie & Miyake (1988); see also the
proof of Theorem 3.1 in Hornik et al. (1990)) gives
f(x) =	ψ(a>x + b)dν (a, b)
(a.e.),
1d
1If f is integrable, this inversion formula holds for almost every x ∈ Rd (Rudin, 1987). However, we
assume a stronger condition that it holds for every x ∈ Rd.
13
Published as a conference paper at ICLR 2021
where dν(a, b) is given by
(∣ω|de-iwb ʌ 八
------X----- f(wa)dadb
2πψ(ω)
for any ω 6= 0. Since the characteristic function of the multivariate normal distribution gives that
>	> >/ Nl h2d	( h2kwk2A r,	( kx-ck2∖
JR exp(iw (x - c))V(2∏F P (--- dw= = exp 卜 ~2hr~ )
S---------V----------}
ʌ , .
=f(W)
: f(x)
(∀x ∈ Rd),
we have that
exp
(—kx - ck2 ʌ
V 2h2 )
ψh(a>(x - c) + b)Re
e e-iwb
∖2πψh(ω)
∣ωh∣2d
Wexp
(ωh)2kak2
2
dadb,
i' 11	_ -ΓTT> rl I-I ∙	I / ∖	I / 7 一 1 ∖	∙>7∕∖	7 ? / 7	\ 1	∙ . Ir.1	•	< . <
for all X ∈ Rd. Since ψh(∙) = ψ(h 1∙) and ψh(∙) = hψ(h∙) by its definition, the right hand Side is
equivalent to
Z Z ψ(h-1[a>(x — c) + b])Re ( -‰；------
JaEdd √b∈R	∖2∏hψ(hω)
∣ωh∣2d
72πF exp
(ωh)2kak2 ) dadb.
—
2
Here, we set ω = h-1. Let Nσ2 be the probability measure corresponding to the multivariate
normal with mean 0 and covariance σ2I, and let AD := {w ∈ Rd | kwk ≤ D}. Let Da > 0 and
Db = Da(√2d + 1), and define
1	e-ib/h
fDa (X) := 2Db N (ADa ) ∕αk≤Da ,网 ≤ D. 他 ( ^ ^ (X-	^h^
Then, we can see that, for any X ∈ [0, 1]d, it holds that
2DbN (ADa)f (X)- fDa (X)
≤ 2DbNι(AD[∣2πhψ(1)∣ 卜I(ADa)∕2exp(-hτ…+	2exp(ThT(IbI-2√dDa)Ddb
≤ 2DbNι(AD1∣2πhψ(1)∣ S(ADa)+ 4hexp(-Dα)]
=2DbNι(ADh)∣2πhψ(1)∣ hCdDa(d-2)exp(-D2/2)+exp(-Da)i，
where Cd > 0 is a constant depending on only d, and we used Ia>(X - c) + bI ≥ IbI - Ia>(X -
c)| ≥ ∣b∣ - 2VdDa and ≠(x) ≤ 2exρ(-∣X∣). Note that if Da = nκ, then the right hand side is
O(hexp(-nκ)). Therefore, since NI(ADa) ≤ 1, by setting T = h-1Db, C =冗九DibDl, We have
that
inf sup
g∈conv(jFCσ,T) χ∈[o,i]d
g(X) - exp
(-⅛f )∣
≤ IC '、1 hcdDa(d-2) exp(-D2/2) + exp(-Da)i .
I2πψ(1)I
Hence, by rewriting Dw J Da, we obtain the assertion. As noted above, the right hand is
O(exp(-nκ)) if Da = nκ.	□
14
Published as a conference paper at ICLR 2021
Proof of Theorem 1. For a sample size n, we fix mn which will be determined later and use
Proposition 2 with F ° =	FYn).	If	w2,mn =	bpμmn∕2 with |b| ≤	1 and	wι,m =
μγZ12[u; -u>c]∕(p2(d +1))	for U	∈	Rd such	that ∣∣uk ≤ 1 and C	∈ [0,1]d, then
k(w1,mn ,w2,mn)k2 ≤ μγmn (1/2 + (1 + |u>c|2)/2(d + 1)) ≤ μγmn. Therefore, 0u,c(X)=
amn w2,mn σmn (W>mn[x；1]) = μm1n (>μm)√)μlmn σ {μmn+/Q(X- c)/p2(d + 1) ) ∈
Fγ(n) ⊂ Fγ for all b ∈ R with |b| ≤ 1, u ∈ Rd with ∣u∣ ≤ 1, and c ∈ [0, 1]d. In other words,
μm1n+γ∕2+sα2 (2C)TFCT ⊂ FYn) forany C > 0 and T = √⅛N「/.
Therefore, by setting C = (√2d + 1)Dw/(πh∣ψ⑴|) for Dw > 0, Lemma 1 yields that for any
c ∈ [0, 1]d and given h > 0, there exists g ∈ Conv(Fn)) such that
α1+γ∕2+sα2
μmn
(I!-1 exp (-⅛f ) - g
α1+γ∕2+sα2
≤ μmn
2(√2d +1)Dw
.^ ...
πhlψ(I)I
-1
∣2πψ(1)∣ hCdDw(d-2) eXP(—Dw/2) + eXP(—Dw )i
∞
〃m：Y/2+sa2 (√d + 1)D hCdD2(d-2) exp(-Dw/2)+exp(—Dw)].
We let Dw = nκ for any κ > 0 and choose μmn as τ ` μ-^+^/2 = Dw h-1 = h-1nκ. We write
a1+γ∕2+sa2∕nf -1 α1+sα2+γ/2 +1	-κ( α1+sα2+γ/2 +1)
△ := μmn u 2 2 (2C) 1 ` h α2-γ/2 Tn , α2-γ/2 ɪ 7. Then, it holds that
∆ exp
. ∆ exp(-nκ).
∞
(9)
—
g
Here, we set h as h = 2-k with a positive integer k. Accordingly, We define a partition A of Ω so
that any A ∈ A can be represented as A = [2-kj1,2-k(j + 1)] X …X [2-kjd, 2-k(jd	+	1)]	by
non-negative integers 0 ≤ ji ≤ 2k - 1 (i = 1, . . . , d). Note that |A| = 2dk = h-d.
For each A ∈ A, we define cA as cA = (2-k(j1 + 1/2), . . . , 2-k(jd + 1/2))> where (j1, . . . ,jd)
is a set of indexes that satisfies A = [2-k九 2-k(j1 + 1)] X … X [2-kjd, 2-k(jd + 1)].	For	each
A ∈ A, we define gA ∈ Conv(Fn)) as a function that satisfies Eq. (9) for C = ca.
Now, we apply Proposition 2 with F ° = Conv(FYn)) and K = K 0 = dk. Let R :=
Rlin(Conv(Fn))). First, we can see that there exits a constant F > 0 such that
gA(X) ≥ F∆ (∀X ∈ A),
where we used exp(-nκ)	1.
Second, in the event E introduced in the statement of Proposition 2, there exists C such that |{i ∈
{1, . . . , n} | Xi ∈ A0}| ≤ Cn/2-dk for all A0 ∈ A. In this case, we can check that
n XX Qexp (-kxi 2h2Ak2 )]2 . δ2* = 詈2-kd,
by the uniform continuity of the Gaussian RBF. Therefore, we also have
1 XXgA(xi)2 ≤ 2 XX [∆exp (-kxi -cA∣2 )] + c∆2 exp(-2nκ)
n	n	2h2
i=1	i=1
. ∆2(hd + exp(-2nκ)),
where C > 0 is a constant. Thus, as long as h is polynomial to n like h = Θ(n-a), the right hand
side is O(∆2hd).
15
Published as a conference paper at ICLR 2021
Now, if we write
αι + Sa2 + γ∕2 + ]
a - γ∕2
α1 + (s + 1)α2
α2 - γ∕2
〜
β
then we have ∆ ' hβ n-κβ by its definition.
Here, We choose k as a maximum integer that satisfies FF3∆22-dk > R*. In this situation, it holds
that
h2β+dn-2κβ ` r*.
Since Eq. (8b) is not satisfied, Eq. (8a) must hold, and hence We have
n-1h-d . R* ` h2β+dn-2κβ

1-2κβ
⇒ h ` n	2β+2d
Therefore, We obtain that
2^	_	_ -Z-
2β + d	2κdβ
R* & n~ 2β+2d n~ 2β + 2d
-2β + d _ ,
≥ n	2β+2d n-κ
G
by setting K = K 2-+22 ∙ ThiS gives the assertion.
□
B Proofs of Proposition 1, Theorem 2 and Corollary 1
Proposition 1, Theorem 2 and Corollary 1 can be shoWn by using Propositions 3 and 4 given in
Appendix B.1 shoWn beloW.
Let TαW = (μθnwι,m, μθnw2,m)m=ι for W = (wι,m, w2,m)∞=1 for a > 0, and let US consider a
model hw := fr-α∕2w. Then, the training error can be rewritten as
^
^
L(fW ) = L(hT ɑ∕2W ).
1-1	, , ∙	1	∙	1∙ ∙ ,	1 , 6∕TT7∙∖	*/ "	∖
For notational simplicity, we let L(W) := L(fW).
Let H(M) be {W(M) = (w1,m, w2,m)mM=1 | w1,m ∈ Rd+1, w2,m ∈ R, 1 ≤ m ≤ M} and
ι : H(M) → H be the zero padding of W(M), that is, ι(W(M)) = (w10,m, w20,m)m∞=1 ∈ H satisfies
w10,m = w1,m, w20,m = w2,m (m ≤ M) and w10,m = 0, w20,m = 0 (m > M). Moreover, we
define ι* : H → H(M) as the map that extracts first M components. By abuse of notation, we write
fW(M) for W(M) ∈ H(M) to indicate fι(W(M)) . Finally, let A(M) : H(M) → H(M) be a linear
operator such that A(M)W(M) = ι* (Aι(W (M) )), which is just a truncation of A. Similarly, let
TMa W(M) for W(M) ∈ H(M) be the operator corresponding to TaW for W ∈ H, i.e., TMa W(M) =
ι*(Taι(W(M))).
B.1	Auxiliary lemmas
First, we show some key propositions to show the main results. To do so, we utilize the result by
Muzellec et al. (2020) and Suzuki (2020).
Assumption 3.
(i)	There exists a constant Cμ such that μm ≤ c*m-2.
(ii)	There exist B, U > 0 such that the following two inequalities hold for some a ∈ (1∕4, 1)
almost surely:
^.... . .
∣∣VL(W)kH ≤ B (VW ∈H),
^. ^. ,... .. ,.. . , .
kVLb(W) - VLb(W 0)kH ≤ LkW - W0kH-a (∀W,W0 ∈H).
16
Published as a conference paper at ICLR 2021
(iii)	For any data Dn, Lb is three times differentiable. Let V3Lb(W) be the third-order derivative
of L(W). This can be identified with a third-order linearform and V3Lb(W )∙(h, k) denotes
the Riesz representor of l ∈ H → V3bb(W) ∙ (h,k,l). There exists α0 ∈ [0,1), Cɑo ∈
(0, ∞) such that∀W,h,k ∈ H, kV3Lb(W) ∙(九国后一 ≤。。，|向|丸|同|丸，∣∣V3Lb(W) ∙
(h, k)kH ≤ Cα0 khkHα0 kkkH (a.s.).
Remark 1. In the analysis of Brehier & Kopec (2016); Muzellec et al. (2020); Suzuki (2020),
Assumption 3-(iii) is imposed for any finite dimensional projection L(W(M)) as a function on H(M))
for all M ≥ 1 instead of L(W) as a function of H. However, the condition on L(W) gives a
sufficient condition for any finite dimensional projection in our setting. Thus, we employed the
current version.
Assumption 4. For the loss function '(y, f (X)) = (y 一 f (x))2, thefollowing conditions holds:
(i)	There exists C > 0 such that for any fW (W ∈ H), it holds that
Eχ,γ[('(Y,fw(X)) - '(Y,f*(X)))2] ≤ C(L(fw) -L(f *)).
(ii)	β > 0 is chosen so that, for any h : Rd → R and x ∈ supp(PX), it holds that
Eγ∣χ=χ[exp ( - n('(Y,h(x))-'(Y,f *(x))))] ≤ 1.
(iii)	There exists Lh > 0 such that ∣∣Vw'(Y,hw(X)) — VW'(Y,hwo(X))∣∣h ≤ LhkW —
W0 kH (∀W, W0 ∈ H) almost surely.
(iv)	There exists Ch such that khW - hW0 k∞ ≤ ChkW - W0kH (W, W0 ∈ H).
Proposition 3. Assume Assumption 3 holds and β > η. Suppose that ∃R > 0, 0 ≤ '(Y,fw(X)) ≤
R for any W ∈ H (a.s.). Let P =[+工；/*] and b =μB + 敬∙ ACCOrdingly, let b = max{b, 1},
K = b + 1 and V = 4b∕(√(i+ρ1∕η)/2-p1/n). Then, the SPeCtraI gap ofthe dynamics is given by
min (⅛,1) δ
4log(κ(V+ 1)/(1 - δ))δ
(10)
where 0 < δ < 1 is a real number satisfying δ = Ω(exp(-Θ(poly(λ-1)β))). We define AG =
limn→0 An (i.e., V is replaced by 4b∕(J(i+exp(-a))/2-exp(—7λ))). We also define Cw0 = κ[V +
1] + vz2√R+b). Then, for any 0 < a < 1/4, the following convergence bound holds for almost sure
observation Dn : for either L = L or L = L,
∣Ewk[L(Wk)∣Dn] - EW~∏∞[L(W)|Dn]|	(11)
≤ CJCWO exp(-Anηk) + ɪβη1/2-^ = Ξk,	(12)
A0
where Ci is a COnStant depending only on Cμ, B, L, Cαj,a, R (independent of η,k,β,λ).
Proposition 4. Assume that Assumptions 3 and 4 hold. Let a := 1∕{2(α + 1)} for a given
a > 0 and θ be an arbitrary real number satisfying 0 < θ < 1 — α. Assume that the
true function fo can be represented by hw * = fo for W * ∈ Hθ(α+i). Then, if M ≥
min {λα∕2[θ(α+1)] β1∕2[θ(α+1)], λ-1∕2(α+1), n1∕2[θ(α+1)] }, the expected excess risk is bounded by
EDn [EW(M)[L(hτα∕2w(M))IDn] -L(f°)]
-	2α∕θ	ι
≤ C max {(λβ) 1+a∕θn- 1+α∕θ ,λ-aβ-i, λθ, 1/n} + Ξk,
where C is a constant independent of n, β , λ, η, k.
(13)
Proof. Repeating the same argument in Proposition 1 and using the same notation, Proposition 3
gives
IEW(M) [L(WkM))∣Dn] - Ew~∏(m) [L(W)∣Dn]∣ ≤ Ξk,
k	π∞
17
Published as a conference paper at ICLR 2021
for any 1 ≤
IEDn IEW(M)〜π
M ≤ ∞.
(M) [L(hTM/2w(M
Therefore, we just need to bound the following quantity:
))|Dn]i -L(fo)III.
We define ∣∣W(M)kH(M) := ∣∣∣*(W(M))IlH for W(M) ∈ H(M). For a > 0, we define HaM) be
the projection of Ha to the first M components, H(aM) = {ι(W) | W ∈ Ha}, and we define
k W(M) IlH(M) := ∣∣∣*(W(M))∣∣Ha (note that since HaM) is a finite dimensional linear space, it is
same as H as a set). Let νβ(M) be the Gaussian measure on H(M) with mean 0 and covariance
(βA(M))-1, and VeM) be the Gaussian measure corresponding to the random variable TM/2W(M)
with W(M) 〜VeM). Let the concentration function be
φ(βM,λ)() :=	inf βλ∣W∣2H(M) -logνVβ(M)({W∈H(M) : ∣W∣H(M) ≤ }) + log(2),
W∈H(α+)1:	α+1
L(hW)-L(fo)≤2
where, if there does not exist W ∈ Hα(M+1) that satisfies the condition inf, then we define φ(βM,λ) ()
∞, then Let e* > 0 be
e" := max{inf{e > 0 | φβ,λ(e) ≤ βe2}, 1/n}.
Then, Suzuki (2020) showed the following bound:
EDn Ew(M)〜∏(M)[L(hτ0∕2w(M))∣Dn] -L(fo)
≤ C max {e*2, (ne*2 + n- 1+α∕θ (λβ) 1+α7θ
1
(14)
n
They also showed that, for M = ∞, it holds that
e*2 . max {(λβ)-αβ~(1-a'), λθ, n-1} = max {λ-αβ-1, λθ, n-1}.
Substituting this bound of e* to Eq. (14), we obtain Eq. (13) for M = ∞. Moreover, in their proof,
if M ≥ (e*)-1∕[θ(α+1)],then
W∈¾1:	βλkW∣HαMι.…
L(hW)-L(f o)≤2
Finally, since νVβ(M) is a marginal distribution of νVβ(∞), it holds that
-logνVβ(M)({W∈H(M) : ∣W∣H(M) ≤e}) ≤ -logνVβ(∞)({W∈H: ∣W∣H ≤ e}).
Therefore, as long as M ≥ (e*)-1∕[θ(α+1)], the rate of e* is not deteriorated from M = ∞. In
other words, if M ≥ min {λα∕2[θ(α+1)]β1∕2[θ(α+1)], λ-θ∕2[θ(α+1)], n1∕2[θ(α+1)]}, the bound (13)
holds.	□
Remark 2. Suzuki (2020) showed Proposition 4 under a condition α > 1/2. However, this is used
only to ensure Assumption 3. In our setting, we can show Assumption 3 holds directly and thus we
may omit the condition α > 1/2.
B.2 Proofs of Proposition 1, Theorem 2 and Corollary 1
Here, we give the proofs of Proposition 1 and Theorem 2 simultaneously.
Proofof Proposition 1 and Theorem 2. Let R = (2 P∞=1 amR + U)2. Then, we can easily check
that (yi 一 fw(xi))2 ≤ R. As stated above, we use Propositions 3 and 4 to show the statements.
First, we show Proposition 1 for the dynamics of Wk(M) for any 1 ≤ M ≤ ∞. However, it suffices
to show the statement only for M = ∞ because the finite dimensional version can be seen as a
18
Published as a conference paper at ICLR 2021
specific case of the infinite dimensional one. Actually, the dynamics of Wk(M) is same as that of
/ T^T T ∖ 1	τV T -C/ K	< 1 Γ∙ 11 ∙ F	∙
ι(Wk) where Wk ∈ H obeys the following dynamics:
〜
Wk+1 = Sn
- NL(f∣(wk))+∖∩η
El ∙	∙	1	(-	∙	1 .	∙	1 1	F,,τt∕	∕τ7τ- ∖	/ LZ 介/ "	∖ ∖
This is because f“w.)is determined by only the first M components ∣(Wk), ∣(VL(f∣(WfcJ)=
▽w(M)L(fw(M))∣w(M)=∣(Wk) and Sn is a diagonal operator. Since the components of Wk With
indexes higher than M does not affect the objective, smoothness of the objective is not lost. The
stationary distribution π∞(M) of the continuous dynamics corresponding to W(M) is a probability
measure on H(M) that satisfies
—∞M)(W(M)) H exp(-βLb(fw(M))),
dνβ
Where νβ(M) is the Gaussian measure on RM×(d+2) With mean 0 and covariance (βA(M))-1. We
can notice that this is the marginal distribution of the stationary distribution of the continuous time
counterpart of Wk: d∏∞(W) h exp(一βL(f∣(W)))dνβ. Therefore, we just need to consider an
infinite dimensional one. For this reasoning, We shoW the convergence for the original infinite
dimensional dynamics (Wk)k∞=1. The convergence of the finite dimensional one (Wk(M))k∞=1 can be
shown by the same manner using the argument above.
To show Proposition 1, we use Propositions 3. To do so, we need to check validity of Assumptions
3. First, we check Assumption 3. Assumption 3-(i) is ensured by Assumption 1. Next, we check
Assumption 3-(ii). The boundedness of the gradient can be shown as follows:
kvLb(fw )kH
∞n	2
=X(11X2(fw(Xi)- yi)W2,mam[xiUσ0n(w>m[xi; 1])∣∣
m=1 i=1
+ I 万 X : 2(fW (Xi) - yi)am tanh (w2,m/R)σm(wι,m [xi; 1]))m=1 | ^
n i=1
∞
≤ X 4RR2am(d +1)cσ +4Ram
m=1
(v |fw(Xi) - yi| ≤ R, kσ0nk∞ ≤ Cσ, k tanh0 k∞ ≤ 1)
∞
≤4R[R2cσ (d+i)+i] x am < ∞.
m=1
Similarly, we can show the Lipschitz continuity of the gradient as
kVL(fw )-VL(fw 0 )kH
∞
≤ X μm2α1 μm11 {4Ram (d + 1)Cσ [(w2,m - w2 ,m)2 + R2kw1,m - w1,mk2]
m=1
+ 4Ram[(W2,m - w2 ,m) /R + Cσ (d + 1)kw1,m - w1,mk2]} (Vk tanh" k∞ ≤ 1)
≤ 4R[(d +1)cσ(1 + R2) + 1/R2 + C(d +1)] maχ[μm2αιamm}
m∈N
∞
X E μmα 1 [(w2,m - w2,m)2 + kw1,m - w1,mk2]
m=1
. kW - W0k2H-α1 .
We can also verify Assumption 3-(iii) in a similar way. Then, we have verified Assumption 3.
Therefore, we may apply Proposition 3, and then we obtain Proposition 1.
19
Published as a conference paper at ICLR 2021
Next, we show Theorem 2 by using Proposition 4. For that purpose, we need to we verify Assump-
tion 4. The first condition can be verified as
EX,Y[((Y-fW(X))2-(Y-fo(X))2)2]
= EX,[((fo(X) + - fW (X))2 - 2)2]
=EX[((fo(X) - fW(X))2 + 2(fo(X) - fW(X)))2]
= EX[(fo(X) - fW(X))4 + 2(fo(X) - fW(X))(fo(X) - fW(X))2 + 2(fo(X) - fW(X))2]
= kfo-fWk2∞EX[(fo(X)-fW(X))2]+U2EX[(fo(X)-fW(X))2]
≤ REX [(fo(X) - fw(X))2] = R(LfW)- L(F)).
The second condition can be checked as follows. Note that
Eγ∣χ=X kXp [n[(Y - fw(χ))2 - (Y - fo(x))2]})
=Ee 卜Xp -n(f。(X)- fw(x))2 - 2e(fw(X)- fo(x))]})
=exp -β(fo(x) - fw(x))2 Ee [exp 2βe(fw(x) - fo(x))]
nn
≤ eχp - J(fo(X) - fw(X))2 eχp 8~2r4U2(fw(X) - fo(X))2
Thus, under the condition β ≤ n/(2U2), the right hand side can be upper bounded by
exp -~ 11 - 2--(fW(X) - fo(X))2 ≤ 1.
nn
Next, we check the third and fourth conditions. Noting that
▽w hw (X)
= (am(μm W w2,m)μmα/2 [Xi;1]0"(从以。/%[m [Xi;1]),
∞
am〃ma/2 tanh0(μmα∕2w2,m IR)Om (〃^^〜鼠 E；1D)∞=1),
m=1
we have that
IVw hw (χ )kH
∞
≤ X amμmα[(d +i)R2cσ + 1]
m=1
∞
≤ [(d +i)R2cσ + 1] X μmα+201
m=1
∞
≤ [(d + 1)R2Cσ + 1]c-α+2α1 X m-2(-α+2α1) =: C1 <∞
m=1
(二 一α + 2αι = αι > 1/2),
and
∣∣Vwhw(X) - Vwhw0(X)kH
∞
≤ X amμmα(d + 1)[μmα(w2,m - w2,m)2 + Rμmαkw1,m - w1,mk2]
m=1
+ amμmα[μmα (W2,m - w2 ,m)2∕R2 + CIJ (d +1)-°忖1,m-w1 ,m∣2]
∞
≤ X amμm2α[(d + 1)(1 + R2) + 1/R2 + Cσ (d +1)Hkw1,m - w1,mk2 + (W2,m - w2 ,m )2]
m=1
20
Published as a conference paper at ICLR 2021
≤ cμα1 max{μmαLa)}[(d +1)(1 + R2) + 1/R2 + C(d +1)]∣∣W - W0kH =: C2∣∣W - W0kH,
m
for a constant 0 < C2 < ∞. Therefore, it holds that
|hW(X) -hW0(X)|2 ≤ C1kW-W0k2H,
which yields the forth condition, and we also have
∣∣Vw'(Y hw(X)) - Vw'(Y hw0(X))kH
= k2(hw(X) - Y)Vwhw(X) - 2(hw0(X) - Y)Vwhw0(X)∣H
≤2∣2(hw (X) - Y)(Vw hw (X) - Vwhw (X))∣2H
+ 2∣2(hw(X) - hw0 (X))Vwhw0 (X)∣2H
≤ 8RC2kw - W0kH + 8c2∣w - W0kH . kW - W0kH,
which yields the third condition.
Since fo ∈ FY, there exists W * ∈ HY such that fo = fw *. Therefore, applying Proposition 4 with
α = αι (α = 1∕[2(aι + 1)]) and θ = γ∕(1 + αι) (since γ ‹ 1/2 + αι, the condition θ < 1 一 α
is satisfied), We obtain that for M ≥ min {X1/4Y(ai + 1)/1/2Y, X-1/2(a1 + 1),n1/2Y}, the following
excess risk bound holds:
EDn hEw(M)[L(WkM ))∣Dn] - L(f *)i . max {(λβ) 1+αθθ n-小,k&β-1,λθ, 1∕n} + Ξk.
Finally, by noting L(Wz(M)) 一 L(f *) = ||/印(M) — f * ∣L2(px), we obtain the assertion.	□
Finally, we give the proof of Corollary 1.
Proof of Corollary 1. Note that
fw (x)
∞
=E amW2,mbm(w>m[x；1])
m=1
∞
=X	μm w2,mμma μ-mqa2 μmaσ(μmaw>m[x;	1])	(；	am	=	μm,	bm	=	μm)
m=1
∞
=X μml+qα2 W2,m〃m(S-q)a2 σ(μma2 w>m[x; 1]).
m=1
Therefore, we may redefine ɑj J αι + qα? and s0 J S 一 q so that we obtain another representation
of the model FY :
FY = S fw (x) = X μm W2,mbm(w>m[x；1]) J W ∈ HY, ∣∣W IlHY ≤ 1 > ,
m=1
where σm,(∙) = μms α2σ(μma2 ∙). Note that the condition 0 ≤ q ≤ S 一 3 gives S 一 q ≥ 3. Therefore,
Assumptions 3 and 4 are valid even for the redefined parameters a；, s0 and σm, instead of αι, S and
Qm. Therefore, we can apply Theorem 2 by simply replacing a； by a； = a； + qa2.	□
21