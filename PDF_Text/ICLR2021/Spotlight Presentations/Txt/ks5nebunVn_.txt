Published as a conference paper at ICLR 2021
Towards Robustness Against
Natural Language Word Substitutions
Xinshuai Dong
Nanyang Technological University, Singapore
dongxinshuai@outlook.com
Rongrong Ji *
Xiamen University, China
rrji@xmu.edu.cn
Anh Tuan Luu
Nanyang Technological University, Singapore
VinAI Research, Vietnam
anhtuan.luu@ntu.edu.sg
Hong Liu
National Institute of Informatics, Japan
hliu@nii.ac.jp
Ab stract
Robustness against word substitutions has a well-defined and widely acceptable
form, i.e., using semantically similar words as substitutions, and thus it is con-
sidered as a fundamental stepping-stone towards broader robustness in natural
language processing. Previous defense methods capture word substitutions in
vector space by using either l2-ball or hyper-rectangle, which results in pertur-
bation sets that are not inclusive enough or unnecessarily large, and thus impedes
mimicry of worst cases for robust training. In this paper, we introduce a novel
Adversarial Sparse Convex Combination (ASCC) method. We model the word
substitution attack space as a convex hull and leverages a regularization term to
enforce perturbation towards an actual substitution, thus aligning our modeling
better with the discrete textual space. Based on the ASCC method, we further pro-
pose ASCC-defense, which leverages ASCC to generate worst-case perturbations
and incorporates adversarial training towards robustness. Experiments show that
ASCC-defense outperforms the current state-of-the-arts in terms of robustness on
two prevailing NLP tasks, i.e., sentiment analysis and natural language inference,
concerning several attacks across multiple model architectures. Besides, we also
envision a new class of defense towards robustness in NLP, where our robustly
trained word vectors can be plugged into a normally trained model and enforce its
robustness without applying any other defense techniques. * 1
1	Introduction
Recent extensive studies have shown that deep neural networks (DNNs) are vulnerable to adversarial
attacks (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016a; Kurakin et al., 2017;
Alzantot et al., 2018); e.g., minor phrase modification can easily deceive Google’s toxic comment
detection systems (Hosseini et al., 2017). This raises grand security challenges to advanced natural
language processing (NLP) systems, such as malware detection and spam filtering, where DNNs
have been broadly deployed (Stringhini et al., 2010; Kolter & Maloof, 2006). As a consequence, the
research on defending against natural language adversarial attacks has attracted increasing attention.
Existing adversarial attacks in NLP can be categorized into three folds: (i) character-level modifica-
tions (Belinkov & Bisk, 2018; Gao et al., 2018; Eger et al., 2019), (ii) deleting, adding, or swapping
words (Liang et al., 2017; Jia & Liang, 2017; Iyyer et al., 2018), and (iii) word substitutions using
semantically similar words (Alzantot et al., 2018; Ren et al., 2019; Zang et al., 2020). The first two
attack types usually break the grammaticality and naturality of the original input sentences, and thus
can be detected by spell or grammar checker (Pruthi et al., 2019). In contrast, the third attack type
only substitutes words with semantically similar words, thus preserves the syntactic and semantics
* Corresponding author.
1Our code will be available at https://github.com/dongxinshuai/ASCC.
1
Published as a conference paper at ICLR 2021
• Vector of word
Vector of substitution
Figure 1: Visualization of how different methods capture the word substitutions in the vector space.
of the original input to the most considerable extent and are very hard to discern, even from a hu-
man’s perspective. Therefore, building robustness against such word substitutions is a fundamental
stepping stone towards robustness in NLP, which is the focus of this paper.
Adversarial attack by word substitution is a combinatorial optimization problem. Solving this prob-
lem in the discrete textual space is considered NP-hard as the searching space increases exponen-
tially with the length of the input. As such, many methods have been proposed to model word
substitutions in the continuous word vector space (Sato et al., 2018; Gong et al., 2018; Jia et al.,
2019; Huang et al., 2019), so that they can leverage the gradients generated by a victim model either
for attack or robust training. However, previous methods capture word substitutions in the vector
space by using either l2 -ball or hyper-rectangle, which results in perturbation sets that are not inclu-
sive enough or unnecessarily large, and thus impedes precise mimicry of the worst cases for robust
training (see Fig. 1 for an illustration).
In this paper, we introduce a novel Adversarial Sparse Convex Combination (ASCC) method, whose
key idea is to model the solution space as a convex hull of word vectors. Using a convex hull brings
two advantages: (i) a continuous convex space is beneficial for gradient-based adversary generation,
and (ii) the convex hull is, by definition, the smallest convex set that contains all substitutions, thus is
inclusive enough to cover all possible substitutions while ruling out unnecessary cases. In particular,
we leverage a regularization term to encourage adversary towards an actual substitution, which
aligns our modeling better with the discrete textual space. We further propose ASCC-defense, which
employs the ASCC to generate adversaries and incorporates adversarial training to gain robustness.
We evaluate ASCC-defense on two prevailing NLP tasks, i.e., sentiment analysis on IMDB and
natural language inference on SNLI, across four model architectures, concerning two common attack
methods. Experimental results show that our method consistently yields models that are more robust
than the state-of-the-arts with significant margins; e.g., we achieve 79.0% accuracy under Genetic
attacks on IMDB while the state-of-the-art performance is 75.0%. Besides, our robustly trained word
vectors can be easily plugged into standard NLP models and enforce robustness without applying
any other defense techniques, which envisions a new class of approach towards NLP robustness.
For instance, using our pre-trained word vectors as initialization enhances a normal LSTM model to
achieve 73.4% robust accuracy, while the state-of-the-art defense and the undefended model achieve
72.5% and 7.9%, respectively.
2	Preliminaries
2.1	Notations and Problem Setting
In this paper, we focus on text classification problem to introduce our method, while it can also be
extended to other NLP tasks. We assume we are interested in training classifier X → Y that predicts
label y ∈ Y given input x ∈ X. The input x is a textual sequence of L words {xi}iL=1. We consider
the most common practice for NLP tasks where the first step is to map x into a sequence of vectors
in a low-dimensional embedding space, which is denoted as v(x). The classifier is then formulated
as p(y|v(x)), where p can be parameterized by using a neural network, e.g., CNN or LSTM model.
2
Published as a conference paper at ICLR 2021
We examine the robustness of a model against adversarial word substitutions (Alzantot et al., 2018;
Ren et al., 2019). Specifically, any word Xi in X can be substituted with any word Xi in S(Xi)=
{S(xi)j }jT=1, where S(xi) represents a predefined substitution set for xi (including itself) and T
denotes the number of elements in S(Xi). To ensure that X is likely to be grammatical and has the
same label as X, S(Xi) is often comprised of semantically similar words of Xi, e.g., its synonyms.
Attack algorithms such as Genetic attack (Alzantot et al., 2018) and PWWS attack (Ren et al., 2019)
aim to find the worst-case X to fool a victim model, whereas our defense methods aim to build
robustness against such substitutions.
2.2	Perturbation Set at Vector Level
Gradients provide crucial information about a victim model for adversary generation (Szegedy et al.,
2013; Goodfellow et al., 2015). However, in NLP, the textual input space is neither continuous nor
convex, which impedes effective use of gradients. Therefore, previous methods capture perturba-
tions in the vector space instead, by using the following simplexes (see Fig.1 for an illustration):
L2-ball with a fixed radius. Miyato et al. (2017) first introduced adversarial training to NLP tasks.
They use a l2-ball with radius to constrain the perturbation, which is formulated as:
V(X) = V(X) + r, s.t. ∣∣r∣∣2 ≤ €,	(1)
where r denotes sequence-level perturbation in the word vector space and V denotes the adversar-
ial sequence of word vectors. While such modeling initially considers l2-ball at the sentence-level,
it can also be extended to word-level to capture substitutions. Following that, Sato et al. (2018)
and Barham & Feizi (2019) propose to additionally consider the directions towards each substitu-
tion. However, they still use the l2-ball, which often fails to capture the geometry of substitutions
precisely.
Axis aligned bounds. Jia et al. (2019) and Huang et al. (2019) use axis-aligned bound to capture
perturbations at the vector level. They consider the smallest axis-aligned hyper-rectangular that
contains all possible substitutions. Such perturbation set provides useful properties for bound prop-
agation towards robustness. However, the volume of the unnecessary space it captures can grow
with the depth of the model and grow exponentially with the dimension of the word vector space.
Thus it fits shallow architectures but often fails to utilize the capacity of neural networks fully.
Besides, instead of fully defining the vector-level geometry of substitutions, Ebrahimi et al. (2018)
propose to find substitutions by first-order approximation using directional gradients. It is effective
in bridging the gap between continuous embedding space and discrete textual space. However, it is
based on local approximation, which often fails to find global worst cases for robust training.
3 Methodology
In this section, we first introduce the intuition of using a convex hull to capture substitutions. Then,
we propose how Adversarial Sparse Convex Combination (ASCC) generates adversaries. Finally,
we introduce ASCC-defense that incorporates adversarial training towards robustness.
3.1	Optimality of Using Convex Hull
From the perspective of adversarial defense, it is crucial to well capture the attack space of word
substitutions. There are three aspects we need to consider: (i) Inclusiveness: the space should in-
clude all vectors of allowed substitutions to cover all possible cases. (ii) Exclusiveness: on the basis
of satisfying inclusiveness, the space should be as small as possible since a loose set can generate
unnecessarily intricate perturbations, which impede a model from learning useful information. (iii)
Optimization: the space should be convex and continuous to facilitate effective gradient-based op-
timization, whether the objective function is convex or not (Bertsekas, 1997; Jain & Kar, 2017).
Inspired by archetypal analysis (Cutler & Breiman, 1994), we propose to use a convex hull to build
the attack space: the convex hull is a continuous space and, by definition, the minimal convex set
containing all vectors of substitutions. We argue that using a convex hull can satisfy all the above
aspects (as illustrated in Fig.1), and thus it is considered as theoretical optimum.
3
Published as a conference paper at ICLR 2021
Figure 2: An illustration of the training process of the ASCC-defense. Step 1: Generate adversaries
by ASCC with regularization. Step 2: Take adversaries as input to perform adversarial training.
terrific
fantastic
amazing
Adversary
generation
3.2	Adversarial Sparse Convex Combination
Efficient representation of and optimization over a convex hull. A classical workaround in liter-
ature for optimization over a constraint set is the projected gradient descent (Cauchy, 1847; Frank &
Wolfe, 1956; Bubeck, 2014; Madry et al., 2018). As for optimization over a convex hull, it necessi-
tates characterizing the convex hull, e.g., by vertexes, to perform projections. However, computing
vertexes is computationally unfavorable because we need to recalculate the vertexes whenever word
embeddings change, which frequently occurs during the training process.
In this paper, we propose a more efficient fashion for optimization over the concerning convex hull,
based on the following proposition (the proof of which lies in the definition of convex hull):
Proposition 1. Let S(u) = {S(u)1, ..., S(u)T} be the set of all substitutions of word u, convS(u) be
the convex hull of word vectors of all elements in S(U), and v(∙) be the word vectorfunction. Then,
we have convS(u) = {X	WiV(S(u)i) | X	Wi
i=1	i=1
1, wi ≥ 0}.
According to Proposition 1, we can formulate V(xi), which denotes any vector in the convex hull
around v(xi), as:
T
j=1
Wij v (S(xi)j), s.t. X	Wij = 1, Wij ≥ 0.
(2)
As such, We use Eq.2 to transform the original optimization on v(Xi) to the optimization on wi,
the coefficient of convex combination. Considering that Wi still belongs to a set with constraint
{∣Wi∣ι = 1, Wij ≥ 0}, to achieve better flexibilities of optimization, We introduce a variable W ∈ R
to relax the constraint on W by the following equation:
Wij=Prp(Wij L ,W,, ∈ R
工j=ι eχp(Wij)
(3)
After such relaxation in Eqs.2 and 3, we are able to optimize the objective function over the convex
hull by optimizing W ∈ R. It provides a projection-free way to generate any adversaries inside the
convex hull using gradients. .
Gradient-based adversary generation. Let L be a loss function concerning a classifier. We can
generate the worst-case convex combinations V(x) by finding the worst-case W:
max L(V(X), ^(x), y)
W
where L is classification-related, e.g., the cross-entropy loss over V(x):
L(V(X),V(x),y) = — log p(y∣V(x)).
(4)
(5)
However, since we relax the discrete textual space to a convex hull in the vector space, any wi that
∣∣Wiko > 1 is highly possible to give rise to V(Xi) that does not correspond to a real substitution.
4
Published as a conference paper at ICLR 2021
Algorithm 1 ASCC-defense
Input: dataset D, parameters of Adam optimizer.
Output: parameters θ and φ.
1:	repeat
2:	for random mini-batch 〜D do
3:	for every x, y in the mini-batch (in parallel) do
4:	Solve the inner maximization in Eq.11 to find the optimal W by Adam;
5:	Compute V(x) by Eq.10 using W and then compute the inner-maximum in Eq.11;
6:	end for
7:	Update θ and φ by Adam to minimize the calculated inner-maximum;
8:	end for
9:	until the training converges.
To align better with the discrete nature of textual input, we propose to impose a regularizer on the
coefficient of convex combination, Wi . To be specific, we take Wi as a probability distribution and
minimize the entropy function of Wi to softly encourage the l0 sparsity of Wi . We formulate this
word-level entropy-based regularization term as:
H(Wi) = X	-Wij log(Wij).	(6)
j=1
Combining loss function L and the entropy-based regularizer H, we here formulate Adversarial
Sparse Convex Combination (ASCC) for adversary generation as:
XL1
,1 TH(Wi),	(7)
w	i=1 L
where α ≥ 0 is the weight controlling the regularization term (the effectiveness of which is validated
in Sec.4.3).
3.3 ASCC-Defense Towards Robustness
We here introduce ASCC-defense, which uses ASCC for adversaries and employs adversarial train-
ing towards robustness. We denote θ and φ as the parameters of p(y|v(x)) and v(x), respectively.
Adversarial training paradigm for NLP. Adversarial training (Szegedy et al., 2013; Goodfellow
et al., 2015; Madry et al., 2018) is currently one of the most effective ways to build robustness.
Miyato et al. (2017) are the first to use adversarial training for text classification. They use l2-ball
with radius to restrict perturbations and the training objective can be defined as:
min[ E	[maxL(V(X), V(x), y, θ, φ)]], s.t. V(x) = v(x) + r, ∣∣r∣∣2 ≤ 3	(8)
θ,φ (x,y)〜D r
where r denotes the perturbations in the vector space and L denotes a classification-related loss.
Therefore, maximizing L can generate adversarial perturbations r to fool a victim model, whereas
minimizing L can let the model learn to predict under perturbations.
ASCC-Defense. Instead of using l2-ball in Eq.8, we leverage ASCC to capture perturbations inside
the convex hull to perform adversarial training. This is to re-define V(χ) in Eq.8 using ASCC, and
the resulting training objective is formulated as:
min[ E [maxL(V(X), V(x), y, θ, φ)]],	(9)
θ,φ (x,y)〜D W
V(Xi)=XTJj V(S(Xi)j ),wij=j¾
(10)
To specify L in Eq.9 for ASCC-defense, we consider the KL-divergence between the prediction
by vanilla input and the prediction under perturbations (Miyato et al., 2018; Zhang et al., 2019a).
In the meantime, we also encourage the sparsity of Wi by the proposed regularizer for adversary
generation. Taking these together, we formulate the training objective of ASCC-defense as follows:
L1
min[ E [ max - logp(y|V(X)) - a£ 了H(Wi) + βKL(p(∙∣v(x))∣∣p(∙∣V(x))) ]],	(11)
θ,φ (x,y)〜D	W	i=： L
5
Published as a conference paper at ICLR 2021
Table 1: Accuracy(%) of different defense methods under attacks on IMDB (a) and SNLI (b). “First-
order aprx” denotes Ebrahimi et al. (2018). “Adv l2-ball” denotes Miyato et al. (2017). “Axis-
aligned” denotes Jia et al. (2019). “ASCC-defense” denotes the proposed method.
Method	Model	Genetic	PWWS	Method	Model	Genetic	PWWS
Standard	LSTM	1.0	0.2	Standard	BOW	28.8	15.4
	CNN	7.0	11.3		DCOM	30.2	9.0
First-order aprx	LSTM	72.5	66.7	First-order aprx	BOW	65.6	57.2
	CNN	51.2	74.1		DCOM	66.7	58.6
Adv l2-ball	LSTM	20.1	11.7	Adv l2-ball	BOW	35.0	16.7
	CNN	36.7	46.2		DCOM	33.1	15.4
Axis-aligned	LSTM	64.7	59.6	Axis-aligned	BOW	75.0	72.1
	CNN	75.0	69.5		DCOM	73.7	67.9
ASCC-defense	LSTM	79.0	77.1	ASCC-defense	BOW	76.3	75.1
	CNN	78.2	76.2		DCOM	74.5	72.8
(a) Accuracy (%) under attacks on IMDB.
(b) Accuracy (%) under attacks on SNLI.
(a) Accuracy (%) under Genetic attacks.
Figure 3: Tradeoff between robustness and accuracy on IMDB under Genetic and PWWS attacks.
(b) Accuracy (%) under PWWS attacks.
where α, β ≥ 0 control the weight of regularization and KL term, respectively. Noted that term H(∙)
has no gradient with respect to θ and φ, so it only works during inner-max adversary generation.
Robust word vectors. We here explain why ASCC-defense can yield more robust word vectors.
Previous defenses such as Miyato et al. (2017) fail to train word vectors in a robust way, as they
update φ by only using the clean data flow. Specifically, Miyato et al. (2017) obtains V(Xi) through
Eq.1, where perturbation r has no gradient with respect to φ, and thus Vφ^(xi) = Vφv(xi). On the
contrary, v(xi) modeled by ASCC-defense has gradient w.r.t. φ concerning all substitutions, as:
Vφv(xi) = X 一 Wij VφV(S(Xi)j).
j=1
(12)
Therefore, ASCC-defense updates the word vector considering all potential adversarial substitutions
simultaneously, which gives rise to more robust word vectors (we validate our claim in Sec.4.4).
Optimization. We employ Adam (Kingma & Ba, 2014) to solve both inner-max and outer-min
problems in Eq.11. Our training process is illustrated in Fig. 2 and presented in Algorithm 1.
4 Experiments
4.1 Experimental Setting
Tasks and datasets. We focus on two prevailing NLP tasks to evaluate the robustness and compare
our method to the state-of-the-arts: (i) Sentiment analysis on the IMDB dataset (Maas et al., 2011).
(ii) Natural language inference on the SNLI dataset (Bowman et al., 2015).
Model architectures. We examine robustness on the following four architectures to show the scala-
bility of our method: (i) BOW, the bag-of-words model which sums up the word vectors and predicts
6
Published as a conference paper at ICLR 2021
Table 2: Ablation study on the sparsity regularization term.
Regul weight	Vanilla	Genetic	PWWS
α=0	80.6	75.1	61.6
α=5	81.9	76.8	71.7
α=10	82.2	78.2	75.7
α=15	81.2	76.1	78.3
(a) Acc (%) of CNN-based ASCC-defense on IMDB.
Regul weight	Vanilla	Genetic	PWWS
α=0	76.7^^	73.4	73.7
α=5	77.4	75.1	74.0
α=10	77.8	76.3	75.1
α=15	76.7	73.7	73.3
(b) Acc (%) of BOW-based ASCC-defense on SNLI.
(a) V generated without the regularization term.
Figure 4: An illustration to show the effectiveness of the proposed sparsity regularization. We
randomly choose 300 adversaries of word “actually” from the test set of IMDB. Vectors are projected
into a 2-dimensional space by SVD. Best view in color with zooming in.
(b) V generated with the regularization when α = 10.
by a multilayer perceptron (MLP), (ii) CNN model, (iii) Bi-LSTM model, (iv) DCOM, decompos-
able attention model (Parikh et al., 2016), which generates context-aware vectors and predicts by a
MLP. We align our implementation details with Jia et al. (2019) for fair comparisons.
Comparative methods. (i) Standard training. It uses the cross-entropy loss as the main loss func-
tion. (ii) First-order approximation (Ebrahimi et al., 2018). While it is initially proposed to model
char flip, it can also be applied to word substitutions. We implement its word-level version for
comparison. (iii) Adv l2-ball (Miyato et al., 2017). It first normalizes the word vectors and then
generates perturbations inside a l2-ball for adversarial training. We implement it with word level
l2-ball and radius varying from 0.1 to 1. We only plot the best performance among different .
(iv) Axis aligned bound (Jia et al., 2019). It models perturbations by an axis-aligned box and uses
bound propagation for robust training. (v) ASCC-defense. We set the hyper-parameters α = 10
and β = 4. For fair comparisons, KL term is employed for all compared adversarial training based
methods. More implementation details as well as runtime analysis can be found in the Appendix A.
Attack algorithms. We employ the following two powerful attack methods to examine the ro-
bustness: (i) Genetic attack (Alzantot et al., 2018) maintains a population to generate attacks in an
evolving way. Aligned with Jia et al. (2019), we set the population size as 60 to run for 40 itera-
tions. (ii) PWWS attack (Ren et al., 2019) calculates the saliency of each word and then substitutes
greedily. Aligned with Alzantot et al. (2018) and Jia et al. (2019), we do not attack premise on SNLI.
Substitution set. For fair comparisons with state-of-the-art defense Jia et al. (2019), we follow
their setting to use substitution set from Alzantot et al. (2018). We apply the same language model
constraint on Genetic as in Jia et al. (2019), while do not apply it on PWWS attacks. As we aim at
using prevailing setting to compare with the state-of-the-arts, we do not focus on how to construct
the substitution set in this work and leave it for future exploration.
4.2	Main Result
Aligned with Jia et al. (2019), we evaluate robustness on 1000 randomly selected examples from the
test set of IMDB, and all 9824 test examples from SNLI. As shown in Tab.1, our method achieves
leading robustness across all architectures with significant margins. For example, on IMDB, we
surpass LSTM-based runner-up method by 6.5% under Genetic and 10.4% under PWWS attacks.
7
Published as a conference paper at ICLR 2021
Table 3: Accuracy (%) of models initialized with different word vectors without any other defense
technique. GloVe denotes the word vectors from Pennington et al. (2014). “First order V” denotes
word vectors trained by Ebrahimi et al. (2018). “ASCC-V” denotes word vectors trained by ASCC-
defense. We freeze the pre-trained word vectors during normal training.
Word vector	Model	Vanilla	Genetic
GloVe	LSTM	88.5	7.9
First order V	LSTM	85.3	65.6
ASCC-V	LSTM	84.1	73.4
GloVe	CNN	86.4	8.6
First order V	CNN	83.1	44.7
ASCC-V	CNN	84.2	72.0
(a) Accuracy (%) under attacks on IMDB.
Word vector	-^Model^^	Vanilla	Genetic
GloVe	-^BOW^^	80.1	35.8
First order V	BOW	79.3	62.1
ASCC-V	BOW	77.9	69.6
GloVe	DCOMP	82.6	41.8
First order V	DCOMP	78.7	62.8
ASCC-V	DCOMP	77.8	72.1
(b) Accuracy (%) under attacks on SNLI.
Plus, the robust performance of ASCC-defense is consistent against different attacks: e.g., on IMDB,
LSTM-based ASCC-defense achieves 79.0% under Genetic attacks and 77.1% under PWWS at-
tacks, which shows ASCC-defense does not rely on over-fitting to a specific attack algorithm. In
addition to robust accuracy, we also plot the tradeoff between robustness and accuracy in Fig.3,
which shows our method can trade off some accuracy for more robustness compared to the state-
of-the-art. More detailed vanilla accuracy and our performance on BERT (Devlin et al., 2019) are
shown in Appendix B.
4.3	On the Regularization and Other Discussions
Fig.4 qualitatively shows how the proposed regularizer encourages sparsity. After applying the
regularization, the resulting V is close to a substitution, corresponding better with the discrete nature
of textual input. Tab.2 quantitatively shows the influence of the regularization term on robustness.
Specifically, when α = 10 our method performs the best. As α keeps increasing, ASSC focus too
much on the sparsity and thus fail to find strong enough perturbations for robust training.
We now discuss some other intuitive defense methods. The thought of enumerating all combi-
nations during training is natural and yet impractical on benchmark datasets; e.g., on IMDB the
average number of combinations per input is 6108. Augmenting training with random combinations
is also ineffective, since it fails to find hard cases in the exponentially large attack space; e.g., under
Genetic ASCC-defense surpasses random augmentation by 46.0% on IMDB and by 7.8% on SNLI
(more significant margin owes to larger attack space on IMDB). Besides, though simply grouping
all substitutions can achieve ensured robustness, it sacrifices discriminative powerness: two words
that are not semantically similar will be mapped together just because they are indirectly related by
one or more mediators. For instance, grouping defense achieves 71.3% robust accuracy and 71.3%
vanilla accuracy on IMDB while ASCC-defense achieves 79.0% and 82.5% respectively.
4.4	Robust Word Vectors
As mentioned in Sec.3.3, ASCC-defense updates the vector of a word by considering all its substi-
tutions, and thus the obtained word vectors are robust in nature. To validate, we use the standard
training process to train models but with different pre-trained word vectors as initialization. We
compare word vectors pre-trained by ASCC-defense with Glove and word vectors pre-trained by
Ebrahimi et al. (2018) (the best performing setting for Miyato et al. (2017) and Jia et al. (2019) is to
freeze the word vectors as Glove, which laterally validates our claim about robust word vectors in
Sec.3.3). As shown in Tab.3, the models initialized by our robustly trained word vectors (and fixed
during normal training) are robust to attacks without applying any other defense techniques. For
example, armed with our robust word vectors, a normally trained LSTM-based model can achieve
73.4% under Genetic attacks, whereas using GloVe achieves 7.9%.
In addition, this result also implies a new perspective towards robustness in NLP: the vulnerabilities
of NLP models relate to word vectors significantly, and transferring pre-trained robust word vectors
can be a more scalable way towards NLP robustness. For more result, please refer to Appendix B.
8
Published as a conference paper at ICLR 2021
5	Related Work
Though achieved success in many fields, DNNs appear to be susceptible to adversarial examples
(Szegedy et al., 2013). Initially introduced to attack CV models, attack algorithms vary from Lp
bounded Goodfellow et al. (2015); Carlini & Wagner (2017); Madry et al. (2018), universal pertur-
bations (Moosavi-Dezfooli et al., 2017; Liu et al., 2019), to wasserstein distance-based attack (Wong
et al., 2019), while defense techniques for CV models include adversarial training(Goodfellow et al.,
2015; Kurakin et al., 2017; Madry et al., 2018), preprocessing (Chen et al., 2018; Yang et al., 2019),
and generative classifiers (Li et al., 2019; Schott et al., 2019; Dong et al., 2020).
Recently, various classes of NLP adversarial attacks have been proposed. Typical methods consider
char-level manipulations (Hosseini et al., 2017; Ebrahimi et al., 2018; Belinkov & Bisk, 2018; Gao
et al., 2018; Eger et al., 2019; Pruthi et al., 2019). Another line of thought focus on deleting, adding,
or swapping words (Iyyer et al., 2018; Ribeiro et al., 2018; Jia & Liang, 2017; Zhao et al., 2018).
In contrast to char-level and sequence-level manipulations, word substitutions consider prior knowl-
edge to preserve semantics and syntactics, such as synonyms from WordNet (Miller, 1998), Se-
memes (Bloomfield; Dong et al., 2006), and neighborhood relationships (Alzantot et al., 2018),
Some focus on heuristic searching in the textual space (Alzantot et al., 2018; Liang et al., 2017; Ren
et al., 2019; Jin et al., 2020; Zhang et al., 2019b; Zang et al., 2020), while (Papernot et al., 2016b;
Gong et al., 2018) propose to leverage gradients for adversary generation in the vector space.
As for defense against adversarial word substitutions, Ebrahimi et al. (2018) find substitutions by
first-order approximation. Miyato et al. (2017), Barham & Feizi (2019) and Sato et al. (2018) use
l2-ball to model perturbations, while Jia et al. (2019) and Huang et al. (2019) use axis-aligned
bound. Zhou et al. (2020) sample from Dirichlet distribution to initialize a convex combination of
substitutions, but the sparsity might be lost during adversary generation. Our work differs as we
model the convex hull with sparsity by entropy function, and the sparsity is enforced during the
whole process, which makes our captured geometry of substitutions more precise.
6	Conclusion
In this paper, we proposed a novel method to use the convex hull to capture and defense against
adversarial word substitutions. Our method yields models that consistently surpass the state-of-the-
arts across datasets and architectures. The experimental results further demonstrated that the word
vectors themselves can be vulnerable and our method gives rise to robust word vectors that can
enforce robustness without applying any other defense techniques. As such, we hope this work can
be a stepping stone towards even broader robustness in NLP.
Acknowledgements
This work is supported by the National Science Fund for Distinguished Young Scholars
(No. 62025603), and the National Natural Science Foundation of China (No. U1705262,
No. 62072386, No. 62072387, No. 62072389, No. 62002305, No. 61772443, No. 61802324, and
No. 61702136).
References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei
Chang. Generating natural language adversarial examples. In EMNLP, 2018.
Samuel Barham and Soheil Feizi. Interpretable adversarial training for text. arXiv preprint
arXiv:1905.12864, 2019.
Yonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine trans-
lation. In ICLR, 2018.
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):
334-334,1997.
9
Published as a conference paper at ICLR 2021
Leonard Bloomfield. A set of postulates for the science of language. Language, 2(3).
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno-
tated corpus for learning natural language inference. In EMNLP, 2015.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint
arXiv:1405.4980, 2014.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In SP.
IEEE, 2017.
AUgUstin Cauchy. Methode generale PoUr la resolution des systemes d'equations simultanees.
Comp. Rend. Sci. Paris, 25(1847):536-538,1847.
Jiefeng Chen, Xi Wu, Yingyu Liang, and Somesh Jha. Improving adversarial robustness by data-
specific discretization. CoRR, abs/1805.07816, 2018.
Adele Cutler and Leo Breiman. Archetypal analysis. Technometrics, 36(4):338-347, 1994.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL, 2019.
Xinshuai Dong, Hong Liu, Rongrong Ji, Liujuan Cao, Qixiang Ye, Jianzhuang Liu, and Qi Tian.
Api-net: Robust generative classifier via a single discriminator. In ECCV, 2020.
Zhendong Dong, Qiang Dong, and Changling Hao. Hownet and the computation of meaning. 2006.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples
for text classification. In ACL, 2018.
Steffen Eger, Gozde Gul Sahin, Andreas Ruckle, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar,
Krishnkant Swarnkar, Edwin Simpson, and Iryna Gurevych. Text processing like humans do:
Visually attacking and shielding nlp systems. arXiv preprint arXiv:1903.11508, 2019.
Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research
logistics quarterly, 3(1-2):95-110, 1956.
Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text
sequences to evade deep learning classifiers. In SPW. IEEE, 2018.
Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, and Wei-Shinn Ku. Adversarial texts with gradient
methods. arXiv preprint arXiv:1801.07175, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015.
Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. Deceiving google’s
perspective api built for detecting toxic comments. arXiv preprint arXiv:1702.08138, 2017.
Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krish-
namurthy Dvijotham, and Pushmeet Kohli. Achieving verified robustness to symbol substitutions
via interval bound propagation. In EMNLP, 2019.
Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example generation
with syntactically controlled paraphrase networks. In NAACL, 2018.
Prateek Jain and Purushottam Kar. Non-convex optimization for machine learning. arXiv preprint
arXiv:1712.07897, 2017.
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.
In EMNLP, 2017.
Robin Jia, Aditi Raghunathan, Kerem Goksel, and Percy Liang. Certified robustness to adversarial
word substitutions. In EMNLP, 2019.
10
Published as a conference paper at ICLR 2021
Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language
attack on text classification and entailment. AAAI, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
J Zico Kolter and Marcus A Maloof. Learning to detect and classify malicious executables in the
wild. Journal of Machine Learning Research, 7(Dec):2721-2744, 2006.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In
ICLR, 2017.
Yingzhen Li, John Bradshaw, and Yash Sharma. Are generative classifiers more robust to adversarial
attacks? In ICML, 2019.
Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text
classification can be fooled. arXiv preprint arXiv:1704.08006, 2017.
Hong Liu, Rongrong Ji, Jie Li, Baochang Zhang, Yue Gao, Yongjian Wu, and Feiyue Huang. Uni-
versal adversarial perturbation via prior driven uncertainty approximation. In ICCV, 2019.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In ACL, 2011.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
George A Miller. WordNet: An electronic lexical database. MIT press, 1998.
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-
supervised text classification. In ICLR, 2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In CVPR, 2017.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In EuroS&P. IEEE, 2016a.
Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial
input sequences for recurrent neural networks. In MILCOM, 2016b.
AnkUr Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model for natural language inference. In EMNLP, 2016.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In EMNLP, pp. 1532-1543, 2014.
Danish PrUthi, BhUwan Dhingra, and Zachary C Lipton. Combating adversarial misspellings with
robUst word recognition. In ACL, 2019.
ShUhUai Ren, Yihe Deng, KUn He, and Wanxiang Che. Generating natUral langUage adversarial
examples throUgh probability weighted word saliency. In ACL, 2019.
Marco TUlio Ribeiro, Sameer Singh, and Carlos GUestrin. Semantically eqUivalent adversarial rUles
for debUgging nlp models. In ACL, 2018.
Motoki Sato, JUn SUzUki, HiroyUki Shindo, and YUji MatsUmoto. Interpretable adversarial pertUr-
bation in inpUt embedding space for text. In IJCAI, 2018.
LUkas Schott, Jonas RaUber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially
robUst neUral network model on mnist. In ICLR, 2019.
11
Published as a conference paper at ICLR 2021
Gianluca Stringhini, Christopher Kruegel, and Giovanni Vigna. Detecting spammers on social net-
works. In ACSAC,pp.1-9, 2010.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2013.
Eric Wong, Frank R Schmidt, and J Zico Kolter. Wasserstein adversarial examples via projected
sinkhorn iterations. In ICML, 2019.
Yuzhe Yang, Guo Zhang, Dina Katabi, and Zhi Xu. Me-net: Towards effective adversarial robustness
with matrix estimation. In ICML, 2019.
Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun.
Word-level textual adversarial attacking as combinatorial optimization. In ACL, 2020.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019a.
Huangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li. Generating fluent adversarial examples for
natural languages. In ACL, 2019b.
Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. In ICLR,
2018.
Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-wei Chang, and Xuanjing Huang. Defense against
adversarial attacks in nlp via dirichlet neighborhood ensemble. arXiv preprint arXiv:2006.11627,
2020.
A Appendix
A. 1 Implementation Details
Text processing. We employ the tokenizer from NLTK and ignore all punctuation marks when
tokenizing the textual input. We set the maximum length of input as 300 for IMDB and 80 for
SNLI. For unknown words, we set them as ”NULL” token.
Hyper-parameters and optimization. We set α as 10 and β as 4 for the training procedure defined
in Eq.12. To generate adversaries for robust training, we employ Adam optimizer with a learning
rate of 10 and a weight decay of 0.00002 to run for 10 iterations To update φ and θ, we also employ
Adam optimizer, the parameters of which differ between architectures and will be discussed as
follows.
Architecture parameters (i) CNN for IMDB: We use a 1-d convolutional layer with kernal size of
3 to extract features and then make predictions. We set the batch-size as 64 and use Adam optimizer
with a learning rate of 0.005 and a weight decay of 0.0002. (ii) Bi-LSTM for IMDB: We use a
bi-directional LSTM layer to process the input sequence, and then use the last hidden state to make
predictions. We set the batch-size as 64 and use Adam optimizer with a learning rate of 0.005 and
a weight decay of 0.0002. (iii) BOW for SNLI: We first sum up the word vectors at the dimension
of sequence and concat the encoding of the premise and the hypothesis. Then we employ a MLP of
3 layers to predict the label. We set the batch-size as 512 and use Adam optimizer with a learning
rate of 0.0005 and a weight decay of 0.0002. (iv) DECOMPATTN for SNLI: We first generates
context-aware vectors and then employ a MLP of 2 layers to make predictions given the context-
aware vectors. We set the batch-size as 256 and use Adam with a learning rate of 0.0005 and a
weight decay of 0.
A.2 Runtime Analysis
All models are trained using the GeForce GTX1080 GPU. (i) As for IMDB, it takes about 1.5 GPU
hours to train a CNN-based model and 2 GPU hours for a LSTM-based model. (ii) As for SNLI,
it takes about 12 GPU hours to train a BOW-based model and 15 GPU hours for DECOMPATTN-
based model.
12
Published as a conference paper at ICLR 2021
Table 4: Vanilla accuracy(%) of different defense methods on IMDB (a) and SNLI (b).
Method	Model	Vanilla accuracy	Method	Model	Vanilla accuracy
Standard	LSTM	88.5	Standard	BOW	80.1
	CNN	87.2		DECOMP	82.6
First-order aprx	LSTM	83.2	First-order aprx	BOW	78.2
	CNN	80.3		DECOMP	77.6
Adv l2-ball	LSTM	84.6	Adv l2-ball	BOW^~	74.8
	CNN	84.5		DECOMP	73.5
Axis-aligned	LSTM	76.8	Axis-aligned	-^BOW^~	79.4
	CNN	81.0		DECOMP	77.1
ASCC-defense	LSTM	82.5	ASCC-defense	-^BOW^~	77.2
	CNN	81.7		DECOMP	76.3
(a) Vanilla accuracy (%) on IMDB.	(b) Vanilla accuracy (%) on SNLI.
Table 5: Vanilla and robust accuracy (%) of the proposed method on BERT (bert-base-uncased).
Method	Dataset	Model	Vanilla accuracy	Under Genetic attack
Standard	IMDB	BERT	92.2	16.4
ASCC-defense	IMDB	BERT	77.5	70.2
B	Additional Experimental Result
B.1	Vanilla Accuracy and Robust Accuracy under Genetic attacks without
Constraint
In Tab.1, we have plotted the robust accuracy under attacks to compare with state-of-the-arts. Here
we plot the vanilla accuracy of all compared methods in Tab.4 as an addition. Aligned with Tab.3,
the parameters of each compared method here are chosen to have the best robust accuracy instead of
vanilla accuracy.
In our main result in Tab.1, for fair comparisons we align our setting with SOTA defense Jia et
al., where genetic attacks are constrained by a language model. Here we plot our performance
under genetic attacks without any language model constraint as an addition: ASCC-defense achieves
76.7% robust accuracy under Genetic without constraint on IMDB based on LSTM, and 72.8% on
SNLI based on BOW.
B.2	Performance on BERT
ASCC-defense models perturbations at word-vecotr level, and thus it can be applied to architec-
tures like Transformers, as long as it uses word embeding as its first layer. To validate, we conduct
experiments on BERT (bert-base-uncased) using standard training and ASCC-defense respectively.
As shown in Tab.5, ASCC-defense enhances the robustness of BERT model significantly. Specifi-
cally, BERT finetuned by standard method on IMDB achieves 16.4% robust accuracy under Genetic
attacks, while using the proposed ASCC-defense achieves 70.2%.
Table 6: Accuracy (%) of normally trained models initialized with and freezed by the proposed
robust word vectors. For example, pre-trained on LSTM means the word vectors are pre-trained by
LSTM-based ASCC-defense and applied to CNN means the pre-trained word vectors are used to
initialize a CNN model to perform normal training.
Pre-trained	Applied to	Vanilla	Genetic	Pre-trained	APPlied to	Vanilla	Genetic
LSTM	-^LSTM^^	84.1	73.4	DECOMP	DECOMP	77.8	72.1
LSTM	CNN	78.5	71.9	DECOMP	BOW	77.2	70.5
(a) Accuracy (%) under attacks on IMDB.	(b) Accuracy (%) under attacks on SNLI.
13
Published as a conference paper at ICLR 2021
B.3	Reduced Perturbation Region
In this section, we show the reduced perturbation region by using convex hull compared to l2-ball
and hyper-rectangle. To make the result more intuitive, we first project word vectors into a 2D
space by SVD, and than calculate the average area of each modeling (to rule out irrelevant factors,
we use word vectors from GloVe and consider words whose substitution sets are of the same size).
We choose the smallest l2-ball and hyper-rectangle that contain all substitutions to compare with
convex hull. The result shows that using convex hull reduce the perturbation region significantly.
Specificaly, the average ratio of the area modeled by convex hull to the area modeled by hyper-
rectangle is 29.2%, and the average ratio of the area modeled by convex hull to the area modeled by
l2-ball area is 8.4%.
B.4	Cross-Architectures Performance of Robust Word Vectors
As discussed in Sec.4.4, our robustly trained word vectors can enforce the robustness of a normally
trained model without applying any other defense techniques. In this section, we aim to examine
whether such a gain of robustness over-fits to a specific architecture. To this end, we first employ
ASCC-defense to obtain robust word vectors and then fix the word vectors as the initialization of an-
other model based on a different architecture to perform normal training. We examine the accuracy
under attacks. As shown in Tab.6, our robustly trained word vectors consistently enhance the robust-
ness of a normally trained model based on different architectures. For example, though trained by a
LSTM-based model, the robust word vectors can still enforce a CNN-based model to achieve robust
accuracy of 71.9% under Genetic attacks (whereas initializing by GloVe achieves 8.6%), demon-
strating the across-architecture transferability of the robustness of our pre-trained word vectors .
14