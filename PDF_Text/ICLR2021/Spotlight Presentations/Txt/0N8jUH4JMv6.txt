Published as a conference paper at ICLR 2021
Implicit Convex Regularizers of CNN Archi-
tectures: Convex Optimization of Two- and
Three-Layer Networks in Polynomial Time
Tolga Ergen & Mert Pilanci
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
{ergen,pilanci}@stanford.edu
Ab stract
We study training of Convolutional Neural Networks (CNNs) with ReLU activa-
tions and introduce exact convex optimization formulations with a polynomial
complexity with respect to the number of data samples, the number of neurons,
and data dimension. More specifically, we develop a convex analytic framework
utilizing semi-infinite duality to obtain equivalent convex optimization problems
for several two- and three-layer CNN architectures. We first prove that two-layer
CNNs can be globally optimized via an `2 norm regularized convex program. We
then show that multi-layer circular CNN training problems with a single ReLU
layer are equivalent to an `1 regularized convex program that encourages sparsity
in the spectral domain. We also extend these results to three-layer CNNs with
two ReLU layers. Furthermore, we present extensions of our approach to differ-
ent pooling methods, which elucidates the implicit architectural bias as convex
regularizers.
1 Introduction
Convolutional Neural Networks (CNNs) have shown a remarkable success across various machine
learning problems (LeCun et al., 2015). However, our theoretical understanding of CNNs still
remains restricted, where the main challenge arises from the highly non-convex and nonlinear
structure of CNNs with nonlinear activations such as ReLU. Hence, we study the training problem for
various CNN architectures with ReLU activations and introduce equivalent finite dimensional convex
formulations that can be used to globally optimize these architectures. Our results characterize the
role of network architecture in terms of equivalent convex regularizers. Remarkably, we prove that
the proposed methods are polynomial time with respect to all problem parameters.
Convex neural network training was previously considered in Bengio et al. (2006); Bach (2017).
However, these studies are restricted to two-layer fully connected networks with infinite width, thus,
the optimization problem involves infinite dimensional variables. Moreover, it has been shown
that even adding a single neuron to a neural network leads to a non-convex optimization problem
which cannot be solved efficiently (Bach, 2017). Another line of research in Parhi & Nowak (2019);
Ergen & Pilanci (2019; 2020a;b;c;d); Pilanci & Ergen (2020); Savarese et al. (2019); Gunasekar
et al. (2018); Maennel et al. (2018); Blanc et al. (2019); Zhang et al. (2016) focuses on the effect of
implicit and explicit regularization in neural network training and aims to explain why the resulting
network generalizes well. Among these studies, Parhi & Nowak (2019); Ergen & Pilanci (2020b;c;d);
Savarese et al. (2019) proved that the minimum `2 norm two-layer network that perfectly fits a one
dimensional dataset outputs the linear spline interpolation. Moreover, Gunasekar et al. (2018) studied
certain linear convolutional networks and revealed an implicit non-convex quasi-norm regularization.
However, as the number of layers increases, the regularization approaches to `0 quasi-norm, which is
not computationally tractable. Recently, Pilanci & Ergen (2020) showed that two-layer CNNs with
linear activations can be equivalently optimized as nuclear and `1 norm regularized convex problems.
Although all the norm characterizations provided by these studies are insightful for future research,
existing results are quite restricted due to linear activations, simple settings or intractable problems.
1
Published as a conference paper at ICLR 2021
Table 1: CNN architectures and the corresponding norm regularization in our convex programs						
	2-layer equation 4	2-layer equation 7	2-layer equation 21 1	2-layer equation 24 1	3-layer equation 11	L-layer equation 9 2
Architecture Implicit Regularization	Pj,k (XkUj) + Wj P k∙k2	Pj maxpool ({(χkUj)+ }) Wj P k∙k2		Pj,k Xk UjWjk k ∙ k* (nuclear norm)	Pj (Pk (XkUj) + Wljk) + W2j 	k_k		Pj XUjwj P k∙kF	Pj (X Qi UljwIj)+w2j P k∙kι
Shallow CNNs and their representational power: As opposed to their relatively simple and shal-
low architecture, CNNs with two/three layers are very powerful and efficient models. Belilovsky
et al. (2019) show that greedy training of two/three layer CNNs can achieve comparable performance
to deeper models, e.g., VGG-11(Simonyan & Zisserman, 2014). However, a full theoretical under-
standing and interpretable description of CNNs even with a single hidden layer is lacking in the
literature.
Our contributions: Our contributions can be summarized as follows:
•	We develop convex programs that are polynomial time with respect to all input parameters: the
number of samples, data dimension, and the number of neurons to globally train CNNs. To the
best of our knowledge, this is the first work characterizing polynomial time trainability of non-
convex CNN models. More importantly, we achieve this complexity with explicit and interpretable
convex optimization problems. Consequently, training CNNs, especially in practice, can be further
accelerated by leveraging extensive tools available from convex optimization theory.
•	Our work reveals a hidden regularization mechanism behind CNNs and characterizes how the
architecture and pooling strategies, e.g., max-pooling, average pooling, and flattening, dramatically
alter the regularizer. As we show, ranging from `1 and `2 norm to nuclear norm (see Table 1
for details), ReLU CNNs exhibit an extremely rich and elegant regularization structure which is
implicitly enforced by architectural choices. In convex optimization and signal processing, `1, `2
and nuclear norm regularizations are well studied, where these structures have been applied in
compressed sensing, inverse problems, and matrix completion. Our results bring light to unexplored
and promising connections of ReLU CNNs with these established disciplines.
Notation and preliminaries: We denote matrices/vectors as uppercase/lowercase bold letters, for
which a subscript indicates a certain element/column. We use Ik for the identity matrix of size k . We
denote the set of integers from 1 to n as [n]. Moreover, k ∙ ∣∣f and k∙∣∣* are FrobeniUs and nuclear
norms and Bp := {u ∈ Cd : kUkp ≤ 1} is the unit 'p ball. We also use l[x ≥ 0] as an indicator.
To keep the presentation simple, we will use a regression framework with scalar outputs and squared
loss. However, we also note that all of our results can be extended to vector outputs and arbitrary
convex regression and classification loss functions. We present these extensions in Appendix. In
our regression framework, we denote the input data matrix and the corresponding label vector as
X ∈ Rn×d and y ∈ Rn, respectively. Moreover, we represent the patch matrices, i.e., subsets of
columns, extracted from X as Xk ∈ Rn×h, k ∈ [K], where h denotes the filter size. With this
notation, {Xku}kK=1 describes a convolution operation between the filter u ∈ Rh and the data matrix
X. Throughout the paper, we will use the ReLU activation function defined as (x)+ = max{0, x}.
However, since CNN training problems with ReLUs are not convex in their conventional form, below
we introduce an alternative formulation for this activation, which will be crucial for our derivations.
Prior Work (Pilanci & Ergen, 2020): Recently, Pilanci & Ergen (2020) introduced an exact convex
formulation for training two-layer fully connected ReLU networks in polynomial time for training
data X ∈ Rn×d of constant rank, where the model is a standard two-layer scalar output network
fθ (X) := Pj=1 (Xuj)+ αj. However, this model has three main limitations. First, as noted by the
authors, even though the algorithm is polynomial time, i.e., O(nr), provided that r := rank(X),
the complexity is exponential in r = d, i.e., O(nd), if X is full rank. Additionally, as a direct
consequence of their model, the analysis is limited to fully connected architectures. Although they
briefly analyzed some CNN architectures in Section 4, as emphasized by the authors, these are either
fully linear (without ReLU) or separable over the patch index k as fully connected models, which do
not correspond to weight sharing in classical CNN architectures in practice. Finally, their analysis
does not extend to three-layer architectures with two ReLU layers since the analysis of two ReLU
layers is significantly more challenging. On the contrary, we prove that classical CNN architectures
can be globally optimized by standard convex solvers in polynomial time independent of the rank
1The results on two-layer CNNs are presented in Appendix A.4.
2This refers to an L-layer network with only one ReLU layer and circular convolutions.
2
Published as a conference paper at ICLR 2021
Table 2: Computational complexity results for training CNNs to global optimality using a standard
interior-point solver (n: # of data samples, d: data dimensionality, K: # of patches, rc : maximal rank
for the patch matrices (rc ≤ h), rcc: rank for the circular convolution, h: filter size ,m: # of filters)
I 2-layer equation 4 ∣ 2-layer equation 7 ∣	L-layer equation 9	∣	3-layer equation 11
# of variables	2hPco∣nv	2hPcοnv	4dPcconv	4dP1P2K
# of constraints	2nPcοnvK	2nPcοnv K2	2nPcconv	2n(PιK + l)P2
Complexity	O (h3r3 (等『I	O (h3r3 (等广)	O (d3r3c (公广)	O (d3m3r3 (mnc)3mr)
(see Table 2). More importantly, we extend this analysis to three-layer CNNs with two ReLU layers
to achieve polynomial time convex training as proven in Theorem 4.1.
1.1	Hyperplane arrangements
Let H be the set of all hyperplane arrangement patterns of X, defined as the following set
H := U {{sign(Xw)} : W ∈ Rd},
which has finitely many elements, i.e., |H| ≤ NH < ∞, NH ∈ N. We now define a collection
of sets that correspond to positive signs for each element in H, by S := {∪hi=1{i}} : h ∈ H .
We first note that ReLU is an elementwise function that masks the negative entries of a vector or
matrix. Hence, given a set S ∈ S, we define a diagonal mask matrix D(S) ∈ Rn×n defined as
D(S)ii ：= l[i ∈ S]. Then, We have an alternative representation for ReLU as (Xw)+ = D(S)XW
given D(S)XW ≥ 0 and (In - D(S)) XW ≤ 0. Note that these constraints can be compactly
defined as (2D(S) - In) Xw ≥ 0. If We denote the cardinality of S as P, i.e., the number of regions
in a partition of Rd by hyperplanes passing through the origin and are perpendicular to the roWs of
the data matrix X With r := rank(X) ≤ min(n, d), then P can be upper-bounded as folloW
P≤2rkX-=10n-k1≤2r
e(n - 1) r
r
(Ojha, 2000; Stanley et al., 2004; Winder, 1966; Cover, 1965) (see Appendix A.2 for details).
1.2	Convolutional hyperplane arrangements
We noW define a notion of hyperplane arrangements for CNNs, Where We introduce the patch
matrices {Xk}kK=1 instead of directly operating on X. We first construct a neW data matrix as
M = [X1; X2; . . . XK] ∈ RnK×h. We then define convolutional hyperplane arrangements as the
hyperplane arrangements for M and denote the cardinality of this set as Pconv . Then, We have
Pconv
rc-1
≤2X
k=0
nKk-1
≤ 2rc (enK - 1
rc
where 屋:=rank(M) ≤ h and K = stride_l + 1∙ Note that when the filter size h is fixed, Pconv is
polynomial in n and d. Similarly, We consider
by a linear pooling layer, i.e., XUw, where U
hyperplane arrangements for circular CNNs followed
d×d
∈ R is a circulant matrix generated by the elements
u ∈ Rh . Then, we define circular convolutional hyperplane arrangements and denote the cardinality
of this set as Pcconv, which is exponential in the rank of the circular patch matrices, i.e., rcc.
Remark 1.1. There exist P hyperplane arrangements of X where P is exponential in r. Thus, if
X is full rank, r = d, then P can be exponentially large in the dimension d. As we will show, this
makes the training problem for fully connected networks challenging. On the other hand, for CNNs,
the number of relevant hyperplane arrangements Pconv is exponential in rc. If M is full rank, then
rc = h d and accordingly Pconv P. This shows that the parameter sharing structure in CNNs
enables a significant reduction in the number of possible hyperplane arrangements. Consequently,
as shown in the sequel and Table 2, our results imply that the complexity of training problem is
significantly lower compared to fully connected networks.
2 Two-layer CNNs
In this section, we present exact convex formulation for two-layer CNN architectures.
3
Published as a conference paper at ICLR 2021
2.1	Two-layer CNNs with average pooling
We first consider an architecture with m filters, average pooling3, i.e., is defined as fθ(X) :=
j k(Xkuj)+wj with parameters θ := {uj , wj }, and standard weight decay regularization,
which can be trained via the following problem
Pi = min 1
{uj ,wj }jm=1 2
mK
(Xkuj)+ wj - y
j=1 k=1
2
βm
+ 2 X (kuj k2+ Wj
2 j=1
(1)
where uj ∈ Rh and w ∈ Rm are the filter and output weights, respectively, and β > 0 is a
regularization parameter. After a rescaling (see Appendix A.3), we obtain the following problem
2
pi1
min 一
{uj ,wj }im=1 2
uj ∈B2,∀j
mK
(Xkuj)+wj -y
j=1 k=1
+ β kwk1 .
2
(2)
1
Then, taking dual with respect to w and changing the order of min-max yields the weak dual
Pi ≥ di=max - 2 kv-yk2 + 2 kyk2 s.t. UmB
K
X vT (Xku)+
k=1
≤ β,	(3)
which is a semi-infinite optimization problem and the dual can be obtained as a finite dimensional
convex program using semi-infinite optimization theory (Goberna & Ldpez-Cerdd, 1998). The same
dual also corresponds to the bidual of equation 1. Surprisingly, strong duality holds when m exceeds
a threshold. Then, using strong duality, we characterize a set of optimal filter weights as the extreme
point of the constraint in equation 3. Below, we use this characterization to derive an exact convex
formulation for equation 1.
Theorem 2.1. Let m be a number such that m ≥ mi for some mi ∈ N, mi ≤ n + 1, then strong
duality holds for equation 3, i.e., P1i = d1i, and the equivalent convex program for equation 1 is
1
min -
{ci ,c0i}iP=co1nv 2
ci,c0i∈Rh ,∀i
Pconv K
XXD(Sik)Xk(c0i-ci) -y
i=1 k=1
2P
conv
+β X (kcik2+kc0ik2)
2	i=1
(4)
s.t. (2D(Sik) - In)Xkci ≥ 0, (2D(Sik) - In)Xkc0i ≥ 0, ∀i, k.
Moreover, an optimal solution to equation 1 with mi filters can be constructed as follows
(ujii,wjii) = (p⅛, qκi)	if kci*k2 > 0
(uj∙2i,wj2i) = (pk=, -qt⅛)	if	kcik2 > 0,
where {ci*, cji }PCιnv are optimal, mi := PP¾nv l[kc"∣2 = 0] + PP=T l[∣∣ci*k2 = 0] ,and
jsi ∈ [|Js|] given the definitions J1 := {i1 : kc0i1 k > 0} and J2 := {i2 : kci2 k > 0}.4
Therefore, we obtain a finite dimensional convex formulation with 2hPconv variables and 2nPconvK
constraints for the non-convex problem in equation 1. Since Pconv is polynomial in n and d given a
fixed rc ≤ h, equation 4 can be solved by a standard convex optimization solver in polynomial time.
Remark 2.1. Table 2 shows that for fixed rank rc, or fixed filter size h, the complexity is polynomial
in all problem parameters: n (number of samples), m (number of filters, i.e., neurons), and d
(dimension). The filter size h is typically a small constant, e.g., h = 9 for 3 × 3 filters. We also note
that for fixed n and rank(X) = d, the complexity of fully connected networks is exponential in d,
which cannot be improved unless P = NP even for m = 2 (Boob et al., 2018; Pilanci & Ergen,
2020). However, this result shows that CNNs can be trained to global optimality with polynomial
complexity as a convex program.
3We define the average pooling operation as PkK=1 (Xkuj)+, which is also known as global average pooling.
4Since our proof technique is similar for different CNNs, we present only the proof of Theorem 2.1 in Section
5. The rest of the proofs can be found in Appendix (including the strong duality results in A.7).
4
Published as a conference paper at ICLR 2021
Interpreting non-convex CNNs as convex variable selection models: Interestingly, we have the
sum of the squared `2 norms of the weights (i.e., weight decay regularization) in the non-convex
problem equation 1 as the regularizer, however, the equivalent convex program in equation 4 is
regularized by the sum of the `2 norms of the weights. This particular regularizer is known as
group `1 norm, and is well-studied in the context of sparse recovery and variable selection (Yuan
& Lin, 2006; Meier et al., 2008). Hence, our convex program reveals an implicit variable selection
mechanism in the original non-convex problem. More specifically, the original features in X are
mapped to higher dimensions via convolutional hyperplane arrangements as {D(Sik)Xk }iP=co1nv and
followed by a convex variable selection strategy using the group `1 norm. Below, we show that this
implicit regularization changes significantly with the CNN architecture and pooling strategies and
can range from `1 and `2 norms to nuclear norm.
2.2	Two-layer CNNs with max pooling
Here, we consider the architecture with max pooling, i.e., fθ (X) = Pj maxpool {(Xkuj )+}k wj,
which is trained as follows
2
*
Pl
1
min -
{uj ,wj }im=1 2
uj ∈B2,∀j
m
X maxpool {(Xk uj )+ }kK=1 wj - y
j=1
+ βkwk1,
2
(5)
where maxpool(∙) is an elementwise max function over the patch index k. Then, taking dual with
respect to w and changing the order of min-max yields
p* ≥ d* = mvax -1 l∣v - yk2 + 2l∣yk2 3 * s∙t∙ max IvTmaxpool ({(XkU)+}K=I)I ≤ β ⑹
Theorem 2.2. Let m be a number such that m ≥ m* for some m* ∈ N, m* ≤ n + 1, then strong
duality holds for equation 6, i.e., p1* = d1*, and the equivalent convex program for equation 5 is
1
mm
{ci ,c0i}iP=co1nv 2
ci,c0i ∈Rh ,∀i
Pconv K
XXD(Sik)Xk(c0i-ci) -y
i=1 k=1
2P
conv
+β X (lcil2+lc0il2)
2	i=1
(7)
s.t. (2D(Sik)-In)Xkci ≥0, (2D(Sik) - In)Xkc0i ≥0,∀i,k,
D(Sik)Xkci ≥D(Sik)Xjci, D(Sik)Xkc0i ≥D(Sik)Xjc0i,∀i,j,k.
Moreover, an optimal solution to equation 5 can be constructed from equation 7 as in Theorem 2.1.
We note that max pooling corresponds to the last two linear constraints of the above program. Hence,
max pooling can be interpreted as additional regularization, which constraints the parameters further.
3 Multi-layer Circular CNNs
In this section, we first consider L-layer circular CNNs with L - 2 pooling layers before ReLU, i.e.,
fθ(X) =	j (X	l Uljw1j)+ w2j, which is trained via the following non-convex problem
*1
P2 =Lmin
{{ulj }lL=-12,w1j ,w2j }jm=1 2
ulj ∈UL ,∀l,j
m L-2
X X Y Uljw1j	w2j-y
2
m
+ 2 X (kwjk2 + w2j),⑻
2 j=1
where Ulj ∈ Rd×d is a circulant matrix generated using ulj ∈ Rhl and UL := {(u1 , . . . , uL-2) :
ul ∈Rhl,∀l∈ [L-2];	QlL=-12
Ul	≤ 1} and we include unit norm constraints w.l.o.g.
Theorem 3.1. Let m be a number such that m ≥ m* for some m* ∈ N, m* ≤ n + 1, then strong
duality holds for equation 8, i.e., p2* = d2*, and the equivalent convex problem is
1
mm	一
{ci ,c0i}iP=cc1onv 2
ci ,c0i ∈Cd,∀i
Pcconv
X D(Si)X(Ci - Ci)- y
i=1
2P
cconv
+ ⅛ X (kCikι + kcikι)	(9)
2	d F i=1


. 一 ,. . , 一 , . . . ..
s.t. (2D(Si)- In)X Ci ≥ 0, (2D(Si)- In)X Ci ≥ 0,∀i,
5
Published as a conference paper at ICLR 2021
where X = XF and F ∈ Cd×d is the DFT matrix. Additionally, as in Theorem 2.1, we can construct
an optimal solution to equation 8 from equation 9.5
Remarkably, although the sum of the squared `2 norms in the non-convex problem in equation 8 stand
for the standard weight decay regularizer, the equivalent convex program in equation 9 is regularized
by the sum of the `1 norms which encourages sparsity in the spectral domain X. Thus, even with
the simple choice of the weight decay in the non-convex problem, the architectural choice for a
CNN implicitly employs a more sophisticated regularizer that is revealed by our convex optimization
approach. We further note that in the above problem D(Si)X are the spectral features of a subset
of data points which are seperated by a hyperplane from all the other spectral features. While such
spectral features can be very predictive for images in many applications, we believe that our convex
program also sheds light into the undesirable bias of CNNs, e.g., towards certain textures and low
frequencies (Geirhos et al., 2018; Rahaman et al., 2019).
4	Three-layer CNNs with two ReLU layers
Here, we consider three-layer CNNs with two ReLU layers, which has the following primal problem
*	…∙	1
PR =	min	-
{uj ,w1j ,w2j}jm=1 2
uj ∈B2
mK
(Xkuj)+w1jk	w2j-y
2
m
+ 2 X (kwjk2 + w2j)
2 j=1
(10)
with fθ(X) = Pj Pk (Xkuj)+
w1jk	w2j and the following convex equivalent problem.
Theorem 4.1. Let m be a number such that m ≥ m* for some m* ∈ N, m* ≤ n + 1, then strong
duality holds for equation 8, i.e., p3* = d3*, and the equivalent convex problem is
1
min 一
{cijk,cijk}ijk 2
cijk ,cijk ∈R
P2	P1 K
XDs2j XXIijkD(Ski)Xk (Cijk-Cijk) -y
j=1	i=1 k=1
2
P1 P2
+ β XX(kCij kF + kCijkF)
i=1 j=1
2
P1 K
s.t. (2D(S2j) - In) XXIijkD(S1ki)XkCijk ≥ 0, (2D(S1ki) - In)XkCijk ≥ 0, ∀i, j, k (11)
i=1 k=1
P1 K
(2D(S2j)-In)XXIijkD(S1ki)XkC0ijk ≥0, (2D(S1ki) - In)XkC0ijk ≥0,∀i,j,k.
i=1 k=1
where P1 and P2 are the number hyperplane arrangements for the first and second layers, Iijk ∈
{±1} are sign patterns to enumerate all possible sign patterns of the second layer weights, and
Cij = [Cij1 . . . CijK] (see Appendix A.10 for further details).
It is interesting to note that, although the sum of the squared `2 norms in the non-convex problem
equation 10 is the standard weight decay regularizer, the equivalent convex program equation 11 is
regularized by the sum of the Frobenius norms that promote matrix group sparsity, where the groups
are over the patch indices. Note that this is similar to equation 4 except an extra summation due to
having one more ReLU layer. Therefore, we observe that adding more convolutional layers with
ReLU implicitly regularizes for group sparsity over a richer hierarchical representation of the data
via two consecutive hyperplane arrangements.
5	Proof of the main res ult (Theorem 2.1)
Here, we provide our proof technique for Theorem 2.1. We first focus on the single-sided constraint
K
max	vT (Xku)+ ≤ β,
u∈B2
2 k=1
(12)
5The details are presented in Appendix A.9
6
Published as a conference paper at ICLR 2021
where the maximization problem can be written as
K
max max X VT D(Sk)Xku s.t. (2D(Sk) - In)Xku ≥ 0, ∀k.
Sk⊆[n] u∈B2 k=1
Sk∈S
(13)
Since the maximization is convex and strictly feasible for fixed D(Sk), equation 13 can be written as
K
max min max X (vtD(Sk)Xk + αT(2D(Sk) - In)Xk) U
S ⊆[n] k	2 k=1
Sk∈S
max min
Sk⊆[n] αk≥0
Sk∈S
K
XVTD(Sk)Xk+αkT(2D(Sk) -In)Xk
k=1
2
We now enumerate all hyperplane arrangements and index them in an arbitrary order, i.e., denoted as
(Si1,..., SK), where i ∈ [Pconv ], Pconv = |Sk |, Sk ：= {(S1,…，Si): Sk ∈ S,∀k, i}.Then,
equation 12 ^⇒ ∀i ∈ [Pconv], min
αk≥0
K
XVTD(Sik)Xk+αkT(2D(Sik) -In)Xk
k=1
≤β
2
< ⇒ ∀i ∈ [Pconv], ∃αik ≥ 0 s.t.
K
X VT D(Sik)Xk + αiTk(2D(Sik) - In)Xk
k=1
≤ β.
2
We now use the same approach for the two-sided constraint in equation 3 to obtain the following
max -1 kv - yk2 +1 l∣yk2 s∙t∙
αik ,α0ik ≥0
K
XVTD(Sik)Xk+αiTk(2D(Sik)-In)Xk
k=1
K
≤ β (14)
2
X -vT D(Sik)Xk + α0iTk (2D(Sik) - In)Xk
k=1
≤ β, ∀i.
2
Note that this problem is convex and strictly feasible for v = αik = α0ik = 0. Therefore, Slater’s
conditions and consequently strong duality holds, and equation 14 can be written as
min max -LllV
λi ,λ0i ≥0	v0	2
i	αik ,α0ik ≥0
Pconv	K
-yk2 + 2 kyk2+ X λi	β -	XVTD(Sk)Xk
i=1	k=1
K
+αiTk(2D(Sik) - In)Xk
2
Pconv
+	λ0i β -	-
VTD(Sik)Xk+α0iTk(2D(Sik) -In)Xk
(15)
i=1
k=1
Next, we first introduce new variables zi, z0i ∈ Rh. Then, by recalling Sion’s minimax theorem (Sion,
1958), we change the order of the inner max-min as follows
min min max
λi ,λ0i ≥0 zi ∈B2	v0
i	z0i∈B2 αik,α0ik≥0
Pconv
+ X λ0i
i=1
1	1	Pconv	K
-2 kv - yk2 + 2 kyk2 + X λi (β + (X VTD(Sk)Xk + αT(2D(Sk) - In)Xk/Zi)
β+	XK -VT D(Sik)Xk + α0iTk (2D(Sik) -In)Xk z0i .
(16)
We now compute the maximum with respect to V, αik, α0ik analytically to obtain the following
2
min mm -
λi ,λ0i ≥0 zi ∈B2 2
z0i∈B2
Pconv K
D(Sik)Xk (λ0iz0i - λizi) - y
i=1 k=1
2P
conv
+ β	(λi + λ0i )
2	i=1
s.t. (2D(Sik) - In)Xkzi ≥0, (2D(Sik)-In)Xkz0i ≥0,∀i,k.
(17)
2
7
Published as a conference paper at ICLR 2021
10-4
严-1-2-3
Wooo
ΘB> θ≥qo
0	20	40	60	80
Time(S)
°0-1-2-3
1 Www
ΘB> θ≥qo
---Trial #1
---Trial #2
---Trial #3
---Trial #4
---Trial #5
-•^Convex opt. val.
-O-Non-convex feas. val
10-4 --------------------1-------------------E
0	20	40	60	80	100	120
Time(s)
(a) Independent realizations with m = 5	(b) Independent realizations with m = 15
Figure 1: Training cost of the three-layer circular CNN trained with SGD (5 initialization trials)
on a synthetic dataset (n = 6, d = 20, h = 3, stride = 1), where the green and red line with a
marker represent the objective value obtained by the proposed convex program in equation 9 and the
non-convex objective value in equation 8 of a feasible network with the weights found by the convex
program, respectively. We use markers to denote the total computation time of the convex solver.
O
1 2
11
①n-e> ①≥loqo
0.95
0.9
0.85
0.8
0.75
Ao≡noo∙1IS①一
	
Γ-	——TriaI #丁 -Trial #2 —Trial #3 -Trial #4 二-Trial #5	_ …- Convex opt. val. ONon-convex feas. val.
0	100	200	300	400	500
Time(S)
T-2
W W
①n-e> ①≥10qo
100	200	300	400	500
Time(S)
(a) MNIST-Training objective
500	1000	1500
Time(S)
(c) CIFAR10-Training objective
(b) MNIST-Test accuracy
Ao≡nooq IS①一
—Trial #1
-Trial #2
—Trial #3
—Trial #4
-Trial #5
-••-Convex opt. val.
-O-Non-convex feas. val.
1000	1500
Time(s)
(d) CIFAR10-Test accuracy
Figure 2: Evaluation of the three-layer circular CNN trained with SGD (5 initialization trials) on a
subset of MNIST (n = 99, d = 50, m = 20, h = 3, stride = 1) and CIFAR10 (n = 99, d = 50,
m = 40, h = 3, stride = 1).
Then, we apply a change of variables and define ci = λizi and c0i = λ0iz0i . Thus, we obtain
1
mm -
ci ,c0i ∈Rh 2
Pconv K
X XD(Sik)Xk(c0i-ci)-y
i=1 k=1
2P
conv
+β X (kcik2 + kc0ik2)
2 i=1
s.t. (2D(Sik) - In)Xkci ≥ 0, (2D(Sik) - In)Xkc0i ≥ 0, ∀i, k,
(18)
since λ% = ∣∣Ci∣∣2 and λi = ∣∣ci∣∣2 are feasible and optimal. Then, using the prescribed {u*,w*}m=^ι,
we evaluate the non-convex objective in equation 1 as follows
Pi ≤ 2
m* K
X X(Xkuji)+wji -y
j=1 k=1
2
Pconv
+2, X
2	i=1,c0i 6=0
ci
ci
Pconv
+2 x
i=1,ci*6=0
vW7
which has the same objective value with equation 18. Since strong duality holds for the convex
program, Pi1 = d1i , which is equal to the value of equation 18 achieved by the prescribed parameters.
8
Published as a conference paper at ICLR 2021
6	Numerical experiments
In this section6,7, we present numerical experiments to verify our claims. We first consider a synthetic
dataset, where (n, d) = (6, 20), X ∈ R6×20 is generated using a multivariate normal distribution
with zero mean and identity covariance, and y = [1 - 1 1 - 1 - 1 1]T. We then train the three-
layer circular CNN model in equation 8 using SGD and the convex program equation 9. In Figure
1, we plot the regularized objective value with respect to the computation time with 5 different
independent realizations for SGD. We also plot both the non-convex objective in equation 8 and the
convex objective in equation 9 for our convex program, where optimal prescribed parameters are
used to convert the solution of the convex program to the original non-convex CNN architecture
(see Appendix A.9). In Figure 1a, we use 5 filters with h = 3 and stride 1, where only one
trial converges to the optimal objective value achieved by both our convex program and feasible
network. As m increases, all the trials are able to converge to the optimal objective value in Figure
1b. We also evaluate the same model on a subset of MNIST (LeCun) and CIFAR10 (Krizhevsky
et al., 2014) for binary classification. Here, we first randomly sample the dataset and then select
(n, d, m, h, stride) = (99, 50, 20, 3, 1) and a batch size of 10 for SGD. Similarly for CIFAR10, we
select (n, d, m, h, stride) = (99, 50, 40, 3, 1) and use a batch size of 10 for SGD. In Figure 2, we
plot both the regularized objective values in equation 8 and equation 9, and the corresponding test
accuracies with the computation time. Since the number of filters is large enough, all the SGD trials
converge the optimal value provided by our convex program.
7	Concluding remarks
We studied various non-convex CNN training problems and introduced exact finite dimensional
convex programs. Particularly, we provide equivalent convex characterizations for ReLU CNN
architectures in a higher dimensional space. Unlike the previous studies, we prove that these
equivalent characterizations have polynomial complexity in all input parameters and can be globally
optimized via convex optimization solvers. Furthermore, we show that depending on the type
of a CNN architecture, equivalent convex programs might exhibit different norm regularization
structure, e.g., `1, `2, and nuclear norm. Thus, we claim that the implicit regularization phenomenon
in modern neural networks architectures can be precisely characterized as convex regularizers.
Therefore, extending our results to deeper networks is a promising direction. We also conjecture
that the proposed convex approach can also be used to analyze popular heuristic techniques to train
modern deep learning architectures. For example, after our work, Ergen et al. (2021) studied batch
normalization through our convex framework and revealed an implicit patchwise whitening effect.
Similarly, Sahiner et al. (2021) extended our model to vector outputs. More importantly, in the light of
our results, efficient optimization algorithms can be developed to exactly (or approximately) optimize
deep CNN architectures for large scale experiments in practice, which is left for future research.
Acknowledgements
This work was partially supported by the National Science Foundation under grants IIS-1838179 and
ECCS-2037304, Facebook Research, Adobe Research and Stanford SystemX Alliance.
References
Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for
convex optimization problems. Journal ofControl and Decision, 5(1):42-60, 2018.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629-681, 2017.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale
to imagenet. In International Conference on Machine Learning, pp. 583-593, 2019.
6Additional numerical results can be found in Appendix A.1.
7We use CVX (Grant & Boyd, 2014) and CVXPY (Diamond & Boyd, 2016; Agrawal et al., 2018) with the
SDPT3 solver (Tutuncu et al., 2001) to solve convex optimization problems.
9
Published as a conference paper at ICLR 2021
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in neural information processing Systems, pp. 123-130, 2006.
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural
networks driven by an ornstein-uhlenbeck like process. CoRR, abs/1904.09080, 2019. URL
http://arxiv.org/abs/1904.09080.
Digvijay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training relu neural network.
arXiv preprint arXiv:1809.10787, 2018.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers, (3):326-334, 1965.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1-5, 2016.
Herbert Edelsbrunner, Joseph O’Rourke, and Raimund Seidel. Constructing arrangements of lines
and hyperplanes with applications. SIAM Journal on Computing, 15(2):341-363, 1986.
Tolga Ergen and Mert Pilanci. Convex duality and cutting plane methods for over-parameterized
neural networks. In OPT-ML workshop, 2019.
Tolga Ergen and Mert Pilanci. Convex programs for global optimization of convolutional neural
networks in polynomial-time. In OPT-ML workshop, 2020a.
Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
arXiv preprint arXiv:2002.11219, 2020b.
Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
arXiv preprint arXiv:2002.11219, 2020c.
Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the
Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of
Proceedings of Machine Learning Research, pp. 4024-4033, Online, 26-28 Aug 2020d. PMLR.
URL http://proceedings.mlr.press/v108/ergen20a.html.
Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demys-
tifying batch normalization in relu networks: Equivalent convex optimization models and implicit
regularization. 2021.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. In International Conference on Learning Representations, 2018.
Miguel Angel Goberna and Marco Ldpez-Cerdd. Linear semi-infinite optimization. 01 1998. doi:
10.1007/978-1-4899-8044-1_3.
Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,
version 2.1. http://cvxr.com/cvx, March 2014.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on
linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
9461-9471. Curran Associates, Inc., 2018.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. http://www.cs.
toronto.edu/kriz/cifar.html, 2014.
Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/
mnist/.
10
Published as a conference paper at ICLR 2021
Yann LeCun, YoshUa Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu network
features. arXiv preprint arXiv:1803.08367, 2018.
L. Meier, S. van de Geer, and P Buhlmann. The group lasso for logistic regression. Journal of the
Royal Statistical Society, Series B, 70:53-71, 2008.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Piyush C Ojha. Enumeration of linear threshold functions from the lattice of hyperplane intersections.
IEEE Transactions on Neural Networks, 11(4):839-850, 2000.
Rahul Parhi and Robert D. Nowak. Minimum "norm" neural networks are splines, 2019.
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks, 2020.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Conference
on Machine Learning, pp. 5301-5310. PMLR, 2019.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.
Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. L1 regularization in infinite
dimensional feature spaces. In International Conference on Computational Learning Theory, pp.
544-558. Springer, 2007.
Walter Rudin. Principles of Mathematical Analysis. McGraw-Hill, New York, 1964.
Arda Sahiner, Tolga Ergen, John M. Pauly, and Mert Pilanci. Vector-output relu neural network
problems are copositive programs: Convex analysis of two layer networks and polynomial-time
algorithms. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=fGF8qAqpXXG.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm
networks look in function space? CoRR, abs/1902.05040, 2019. URL http://arxiv.org/
abs/1902.05040.
Alexander Shapiro. Semi-infinite programming, duality, discretization and optimality conditions.
Optimization, 58(2):133-161, 2009.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Maurice Sion. On general minimax theorems. Pacific J. Math., 8(1):171-176, 1958. URL https:
//projecteuclid.org:443/euclid.pjm/1103040253.
Richard P Stanley et al. An introduction to hyperplane arrangements. Geometric combinatorics, 13:
389-496, 2004.
RH Tutuncu, KC Toh, and MJ Todd. Sdpt3—a matlab software package for semidefinite-quadratic-
linear programming, version 3.0. Web page http://www. math. nus. edu. sg/mattohkc/sdpt3. html,
2001.
RO Winder. Partitions of n-space by hyperplanes. SIAM Journal on Applied Mathematics, 14(4):
811-818, 1966.
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
11
Published as a conference paper at ICLR 2021
Table of Contents
A Appendix	12
A.1 Additional numerical results ......................................... 12
A.2	Constructing hyperplane arrangements in polynomial time ............ 12
A.3	Equivalence of the `1 penalized objectives .......................... 14
A.4	Two-layer linear CNNs .............................................. 15
A.5	Extensions to vector outputs ........................................ 17
A.6	Extensions to arbitrary convex loss functions ....................... 17
A.7	Strong duality results .............................................. 18
A.8	Proof of Theorem 2.2 ................................................ 19
A.9	Proof of Theorem 3.1 ................................................ 20
A.10 Proof of Theorem 4.1 ................................................ 23
A Appendix
In this section, we present additional materials and proofs of the main results that are not included in
the main paper due to the page limit.
A.1 Additional numerical results
Here, we present additional numerical experiments to further verify our theory. We first perform
an experiment with another synthetic dataset, where X ∈ R6×15 is generated using a multivariate
normal distribution with zero mean and identity covariance, and y = [1 - 1 1 1 1 - 1]T. In this case,
we use the two-layer CNN model in equation 1 and the corresponding convex program in equation 4.
In Figure 3, we perform the experiment using m = 5, 8, 15 filters of size h = 10 and stride 5, where
we observe that as the number of filters increases, the ratio of the trials converging to the optimal
objective value increases as well.
In order to apply our convex approach in Theorem 2.1 to larger scale experiments, we now introduce
an unconstrained version of the convex program in equation 4 as follows
1
mm	-
{ci ,c0i}iP=co1nv 2
ci,c0i∈Rh ,∀i
Pconv K
D(Sik)Xk(c0i-ci)-y
i=1 k=1
2P
conv
+β	(kcik2 + kc0ik2)
2	i=1
(19)
Pconv K
+ PIT XX ((-(2D(Sk) — In)Xkci)+ + (-(2D(Sk) — In)Xkci)+),
i=1 k=1
where ρ > 0 is a trade-off parameter. Since the problem in equation 19 is in an unconstrained form,
we can directly optimize its parameters using conventional algorithms such as SGD. Hence, we use
PyTorch to optimize the parameters of a two-layer CNN architecture using both the non-convex
objective in equation 1 and the convex objective in equation 19, where we use the full CIFAR-10
dataset for binary classification, i.e., (n, d) = (10000, 3072). In Figure 4, we provide the training
objective and the test accuracy of each approach with respect to the number of epochs. Here, we
observe that the optimization on the convex formulation achieves lower training objective and higher
test accuracy compared to the classical optimization on the non-convex problem.
A.2 Constructing hyperplane arrangements in polynomial time
In this section, we discuss the number of distinct hyperplane arrangements, i.e., P, and present
algorithm that enumerates all the distinct arrangements in polynomial time.
We first consider the number of all distinct sign patterns sign(Xw) for all w ∈ Rd. This number
corresponds to the number of regions in a partition of Rd by hyperplanes passing through the origin,
12
Published as a conference paper at ICLR 2021
0	50	100	150	200
Time(S)
(a) Independent realizations
with m = 3
0	50	100	150
Time(S)
(b) Independent realizations
with m = 8
0°
12 3
- - -
Ooo
θn-> θθ一q。
0	50	100	150	200
Time(S)
(c) Independent realizations
with m = 15
O IO 20	30	40	50
Epochs
Figure 3: Training cost of a two-layer CNN (with average pooling) trained with SGD (5 initialization
trials) on a synthetic dataset (n = 6, d = 15, h = 10, stride = 5), where the green line with a marker
represents the objective value obtained by the proposed convex program in equation 4 and the red
line with a marker represents the non-convex objective value in equation 1 of a feasible network with
the weights found by the convex program. Here, we use markers to denote the total computation time
of the convex optimization solver.
-* SGD
♦ Convex
(b) Test accuracy
Figure 4: Evaluation of two-layer CNNs trained with SGD on full CIFAR-10 (n = 10000, d = 3072,
m = 50, h = 12, stride = 4).
(a) Objective value
and are perpendicular to the rows of X. Here, one can replace the dimensionality d with the rank of
the data matrix X, i.e., denoted as r, without loss of generality. Let us first introduce the Singular
Value Decomposition of X in a compact form as X = UΣVT, where U ∈ Rn×r, Σ ∈ Rr×r, and
V ∈ Rr×d. Then, for a given vector w ∈ Rd, Xw = Uw0, where w0 = ΣVT w, w0 ∈ Rr. Hence,
the number of distinct sign patterns sign(Xw) for all possible w ∈ Rd is equal to the number of sign
patterns sign(Uw0) for all possible w0 ∈ Rr.
Consider an arrangement of n hyperplanes in Rr , where n ≥ r. Let us denote the number of regions
in this arrangement by Pn,r. In Ojha (2000); Cover (1965), it is shown that this number satisfies
Pn,r
r-1
≤2X
k=0
n-1
k
For hyperplanes in general position, the above inequality is in fact an equality. In Edelsbrunner et al.
(1986), the authors present an algorithm that enumerates all possible hyperplane arrangements O(nr)
time, which can be used to construct the data for the convex programs we present throughout the
paper.
13
Published as a conference paper at ICLR 2021
A.3 EQUIVALENCE OF THE `1 PENALIZED OBJECTIVES
In this section, we prove the equivalence between the original problems with `2 regularization and
their `1 penalized versions. We also note that similar equivalence results were also presented in
Savarese et al. (2019); Neyshabur et al. (2014); Ergen & Pilanci (2019; 2020c;d). We start with the
equivalence between equation 1 and equation 2.
Lemma A.1. The following two problems are equivalent:
1
min -
{uj ,wj }jm=1 2
mK
(Xkuj)+ wj - y
j=1 k=1
2
m
+ 2 X (kuj k2+ Wj)
2	j=1
1
mm	一
{uj ,wj}jm=1 2
uj ∈B2,∀j
mK
(Xkuj)+wj -y
j=1 k=1
2
m
+ β X kwk1.
2	j=1
ProofofLemma A.1. We rescale the parameters as Uj = YjUj and Wj = Wj/γj, for any Yj > 0.
Then, the output becomes
mK	mK	mK
XX(XkUj )+Wj = XX(XkUj Yj) + W = XX(XUj )+wj,
which proves that the scaling does not change the network output. In addition to this, we have the
following basic inequality
mm
2 X(MI∣2 + Wj) ≥ X(IWj|Ml∣2),
j=1	j=1
where the equality is achieved with the scaling choice Yj = (『；")2 is used. Since the scaling
operation does not change the right-hand side of the inequality, we can set IUj Ij = 1, ∀j. Therefore,
the right-hand side becomes IwI1.
Now, let us consider a modified version of the problem, where the unit norm equality constraint
is relaxed as IUj Ij ≤ 1. Let us also assume that for a certain index j, we obtain IUj Ij < 1 with
Wj 6= 0 as an optimal solution. This shows that the unit norm inequality constraint is not active for
Uj , and hence removing the constraint for Uj will not change the optimal solution. However, when
we remove the constraint, IUj Ij → ∞ reduces the objective value since it yields Wj = 0. Therefore,
we have a contradiction, which proves that all the constraints that correspond to a nonzero Wj must
be active for an optimal solution. This also shows that replacing IUj Ij = 1 with IUj Ij ≤ 1 does not
change the solution to the problem.	口
Next, we prove the equivalence between equation 8 for L = 3 and equation 30.
Lemma A.2. The following two problems are equivalent:
1
min
{uj,w1j,w2j}jm=1 2
uj ∈B2 ,∀j
m
(XUjw1j)+Wjj -y
j=1
j
βm
+ 2 X (kwijk2 + Wij)
j j=1
1
mm
{uj,w1j,w2j}jm=1 2
uj ,w1j ∈B2 ,∀j
m
(XUjw1j)+ Wjj -y
j=1
j
+ β Iwj I1 .
j
ProofofLemma A.2. We rescale the parameters as Wij = YjWj and Wij = Wij/γj-, for any
Yj > 0. Then, the output becomes
mm	m
X(XUj WIj )+W2j = X(XUj WIjYj) + Wjj = X(XUj wij )+Wij,
j =1	j=1	Yj	j=1
14
Published as a conference paper at ICLR 2021
which proves that the scaling does not change the network output. In addition to this, we have the
following basic inequality
mm
2 X(HWIj k2 + w2j ) ≥ X(HWIj l∣2 lw2j |),
j=1	j=1
where the equality is achieved with the scaling choice Yj = (JWj^) 2 is used. Since the scaling
operation does not change the right-hand side of the inequality, we can set kW1j k2 = 1, ∀j. Therefore,
the right-hand side becomes kW2 k1. The rest of the proof directly follows from the proof of Lemma
A.1.	□
A.4 Two-layer linear CNNs
We now consider two-layer linear CNNs, for which the training problem is
1
min
{uj ,wj }jm=1 2
Km
Xkujwjk - y
2
βm
+ 2 X (kuj k2 + kwjk2
(20)
Theorem A.1. (Pilanci & Ergen, 2020) The equivalent convex program for equation 20 is
1
mm 一
{zk}kK=1,zk ∈Rh 2
K
Xkzk
k=1
2
—y + β k[zι,..., zκ ]k*.
2
(21)
Proof of Theorem A.1. We first apply a rescaling (as in Lemma A.1) to the primal problem in
equation 20 as follows
min 1
{uj ,wj }jm=1 2
uj ∈B2
Km
Xkujwjk - y
k=1 j=1
2
m
+ β X kWjk2.
2 j=1
Then, taking the dual with respect to the output layer weights Wj yields
max- 2 kv-yk2 + 1 kyk2
s.t. max
u∈B2
jX(vTXkU)2 ≤ β.
(22)
Let us then reparameterize the problem above as follows
max — 1 kv — yk2 + 1 kyk2 s.t. σmaχ (M) ≤ β, M = [XTV …XKv],
M,v 2	2
where σmax(M) represent the maximum singular value of M. Then the Lagrangian is as follows
L(λ, Z, M, v) = —1 ∣∣v — yk2 + 2 kyk2 + λ (β — σmaχ (M)) + trace(ZTM) - trace(ZT[XTV ... XKv])
11	K
=-2 kv — yk2 + 2 kyk2 + λ (B — σmax (M)) + trace(Z M)- V ɪ2 XkZk
k=1
where λ ≥ 0. Then maximizing over M and v yields the following dual form
1
mm	一
{zk}kK=1,zk ∈Rh 2
K	2
EXk Zk — y	+ β k[zι ... ZK ]k*,
k=1	2
where ∣∣[zι ... ZK]k* = ∣∣Zk* = Ei σi(Z) is the 'ι norm of singular values, i.e., nuclear norm
(ReChtetal.,2010).	□
15
Published as a conference paper at ICLR 2021
The regularized training problem for two-layer circular CNNs as follows
1
min -
{uj ,wj }im=1 2
m
X XUjwj - y
j=1
2
m
+ 2 X (kuj I∣2 + Ilwj k2)
2 j=1
(23)
where Uj ∈ Rd×d is a circulant matrix generated by a circular shift modulo d using uj ∈ Rh.
Theorem A.2. (Pilanci & Ergen, 2020) The equivalent convex program for equation 23 is
men 2 区 z-H2+√βd kzk1,	(24)
where X = XF and F ∈ Cd×d is the DFT matrix.
Proof of Theorem A.2. We first apply a rescaling (as in Lemma A.1) to the primal problem in
equation 23 as follows
min
{uj ,wj }im=
uj ∈B2
m
X XUj wj - y
j=1
2
m
+ βXIwjI2
2 j=1
1
2
and then taking the dual with respect to the output layer weights wj yields
max- 1kv - yk2 + 1 ∣∣yk2 s.t. max IIvTXFDFH∣∣2 ≤ β,
v 2	2	D∈D
where D := {D : IDI2F ≤ d}. In the problem above, we use the eigenvalue decomposition
U = FDFH, where F ∈ Cd×d is the DFT matrix and D ∈ Cd×d is a diagonal matrix defined as
D := diag(√dFu). We also note that the unit norm constraint in the primal problem, i.e., Uj ∈ B?,
is equivalent to Dj ∈ D since Dj = diag(√dFuj) and ∣∣Dj∣∣F = dIuj I22 due the properties of
circulant matrices.
Now let us first define a variable change as X = XF. Then, the problem above can be equivalently
written as
maχ -1 l∣v - y∣2 + 1 l∣y∣2 s∙t∙ max IIvTXDk2 ≤ β.
v 2	2	D∈D
Since D is a diagonal matrix with a norm constraint on its diagonal entries, for an arbitrary vector
s ∈ Cn, we have
∣sT D∣2
n
|si|2|Dii|2 ≤ s
max
i=1
n
X |Dii|2
i=1
smax
√d,
∖
∖
where smax := maxi |si|. If we denote the maximum index as imax := arg maxi |si|, then the
upper-bound is achieved when
D . = ∕ VZd	if i = imax
ii 0, otherwise .
Using this observation, the problem above can be further simplified as
maχ -2l∣v - y∣2 + 2l∣y∣2 s.t. ∣vTX∣∣∞ ≤ √ .
Then, taking the dual of this problem gives the following
mcd IBx Z Τ2+√βd ∣z∣1.
□
16
Published as a conference paper at ICLR 2021
A.5 Extensions to vector outputs
Here, we present the extensions of our approach to vector output. To keep the notation and presentation
simple, we consider the vector output version of the two-layer linear CNN model in Section A.4. The
training problem is as follows
1
mm	一
{uj,{wjk}kK=1}jm=1 2
Km
XXXkujwjTk-Y
2
βm	K
+ 2 X ( kujk2 + X kwjk k2
j=1	k=1
The corresponding dual problem is given by
max — 2 kV - YkF + 1 kYkF
s.t.
max
u∈B2
K
X kVT Xk uk22 ≤ β.
k=1
The maximizers of the dual are the maximal eigenvectors of PkK=1 XkT VVT Xk, which are optimal
filters.
We now focus on the dual constraint as in Proof of Theorem 2.1.
uK	K
max	kVT Xk uk22 = max	skgkT VT Xku = max
u∈B2	u,s,gk ∈B2	u,s,gk ∈
k=1	k=1
K
Xsk V, XkugkT
k=1
K
max	sk hV,XkGki = max
kGkk*≤1 J	kGk k*≤sk
s∈B2 k=1	s∈B2
K
XhV,XkGki
k=1
Then, the rest of the derivations directly follow Section A.4.
A.6 Extensions to arbitrary convex loss functions
In this section, we first show the procedure to create an optimal standard CNN architecture using the
optimal weights provided by the convex program in. Then, we extend our derivations to arbitrary
convex loss functions.
In order to keep our derivations simple and clear, we use the regularized two-layer architecture in
equation 1. For a given convex loss function '(∙, y), the regularized training problem can be stated as
follows
mK	m
XX(Xk Uj )+wj, y	+ 2 X(Mk2 + Wji).
j=1 k=1	2j=1
(25)
Then, the corresponding finite dimensional convex equivalent is
Pconv K	Pconv K
XXD(Sik)Xk(c0i-ci),y +β XX(kcik2+kc0ik2)
i=1 k=1	i=1 k=1
ci,c0i∈Rh,∀i
s.t. (2D(Sik) -In)Xkci ≥ 0, (2D(Sik) - In)Xkci ≥ 0, ∀i, k.
We now define m* := PP=Tv l[∣∣c"∣2 =0] + PP=Tv l[∣∣c"∣2 =0], Where {c*, ci*}P=1nv
optimal weights in equation 26.
(26)
are the
Theorem A.3. The convex program equation 26 and the non-convex problem equation 25, where
m ≥ m* has identical optimal values. Moreover, an optimal solution to equation 25 can be
constructed from an optimal solution to equation 26 as follows
(u*ii,w*ii) = (pk=, qκi)	if kci*k2 > 0
(u;2i,Wju= (p⅛, -q^)	if kc*k2 > 0,
where {ci*, c* }P=1nv are the optimal solutions to equation 26.
17
Published as a conference paper at ICLR 2021
_	_	_ ,	_	一	一	一一 一	-.*,、	_
ProofofTheorem A.3. We first note that there Will be m* vectors {ci , c*}. Constructing
{u*,w*}m=*ι as stated in the theorem, and plugging in the non-convex objective equation 25, we
obtain the value
(m* K	∖ p	Pconv
XX(Xk u；)+w；, y∣ + 2	X
j=1 k=1	i=1,c0i* 6=0
Pconv
+2 x
i=1,ci*6=0
which is identical to the objective value of the convex program equation 26. Since the value of the
convex program is equal to the value of it,s dual d↑ in the dual, we conclude that p↑ =曲,which is
equal to the value of the convex program equation 26 achieved by the prescribed parameters.
We also show that our dual characterization holds for arbitrary convex loss functions
(m K	∖
XX(Xkuj)+wj,y + βkwk1,
j=1 k=1
=
where '(∙, y) is a convex loss function.
Theorem A.4. The dual of equation 27 is given by
K
max —'*(v) s.t.	VT(Xku)+ ≤ β, ∀u ∈B ,
v
k=1
(27)
□
where '* is the Fenchel conjugate function defined as
'* (V) = max ZTv — '(z, y).
z
Proof of Theorem A.4. The proof follows from classical Fenchel duality (Boyd & Vandenberghe,
2004). We first describe equation 27 in an equivalent form as follows
mK
min	`(z, y) + βkwk1 s.t. z
{uj ,wj}jm=1,z
uj ∈B2,∀j
(Xkuj)+wj,.
j=1 k=1
Then the dual function is
mK
g(V)
min
{uj ,wj}jm=1,z
uj ∈B2,∀j
`(z, y) —
VTz+VT	(Xkuj)+wj + βkwk1.
j=1 k=1
Therefore, using the classical Fenchel duality (Boyd & Vandenberghe, 2004) yields the claimed dual
form.
□
A.7 Strong duality results
Proposition A.1. Given m ≥ m*, strong duality holdsfor equation 3, i.e., p； = d；.
We first review the basic properties of infinite size neural networks and introduce technical details to
derive the dual of equation 3. We refer the reader to Rosset et al. (2007); Bach (2017) for further
details. Let us first consider a measurable input space X with a set of continuous basis functions
(i.e., neurons or filters in our context) ψu : X → R, which are parameterized by u ∈ B2. Next, we
use real-valued Radon measures with the uniform norms (Rudin, 1964). Let us consider a signed
Radon measure denoted as μ. Now, we can use μ to formulate an infinite size neural network as
f (x) = Ju∈b2 ψu(x)dμ(u), where X ∈ X is the input. The norm for μ is usually defined as its
18
Published as a conference paper at ICLR 2021
total variation norm, which is the supremum of Ju∈b2 g(u)dμ(u) over all continuous functions
g(u) that satisfy |g(u)| ≤ 1. Now, we consider the case where the basis functions are ReLUs, i.e.,
ψu = xT u + . Then, the output of a network with finitely many neurons, say m neurons, can be
written as
m
f(x) =	ψuj wj
j=1
which can be obtained by selecting μ as a weighted sum of Dirac delta functions, i.e., μ =
Pjm=I Wjδ(u - Uj). In this case, the total variation norm, denoted as ∣∣μ∣∣τv, corresponds to
the `1 norm kwk1 .
Now, we ready to derive the dual of equation 3, which can be stated as follows (see Section 8.6 of
Goberna & Ldpez-Cerdd (1998) and Section 2 of Shapiro (2009) for further details)
2
d； ≤ Pι,∞
K
E(Xku)+ dμ(u) - y
k=1
+ βkμkτv.
2
(28)
Although equation 28 involves an infinite dimensional integral form, by Caratheodory’s theorem, we
know that the integral can be represented as a finite summation, to be more precise, a summation
of at most n + 1 Dirac delta functions (Rosset et al., 2007). If we denote the number of Dirac delta
functions as m； , where m； ≤ n + 1, then we have
1
Pι,∞ =	min * w
{Uj ,Wj }m=l 2
uj ∈B2,∀j
= p；1
m* K
XX(Xkuj)+
j=1 k=1
2
wj - y	+ β kwk1
2
provided that m ≥ m；. We now need to show that strong duality holds, i.e., p1； = d1；.
We first note that the semi-infinite problem equation 3 is convex. Then, we prove that the optimal
value is finite. Since β > 0, we know that v = 0 is strictly feasible, and achieves 0 objective value.
Moreover, since -ky - vk22 ≤ 0, the optimal objective value p1； is finite. Therefore, by Theorem 2.2
of Shapiro (2009), strong duality holds, i.e., p↑,∞ = d； provided that the solution set of equation 3 is
nonempty and bounded. We also note that the solution set of equation 3 is the Euclidean projection
of y onto a convex, closed and bounded set since (Xku)+ can be expressed as the union of finitely
many convex closed and bounded sets.	□
A.8 Proof of Theorem 2.2
The proof follows the proof of Proposition A.1. The dual of equation 6 is as follows
d1 ≤ Pι,∞ = min 2 1/ maxpool ({(Xku)+}K=ι) dμ(u) - y
2
+ β l∣μkτv,
2
which has the following finite equivalent
2
p1,∞
1
= min 一
{uj,wj}jm=1 2
uj ∈B2,∀j
= p；1
*
m*
X maxpool {(Xk uj )+	}kK=1 wj - y
j=1
+ β lwl1
2
provided that m ≥ m；. We now need to show that strong duality holds, i.e., p1； = d1；.
Since maxpool(∙) can be expressed as the union of finitely many convex, closed and bounded sets,
the rest of the strong duality results directly follow from the proof of Proposition A.1.
We now focus on the single-sided dual constraint
max vT maxpool( (Xku)+ }kK=1 ≤ β,
19
Published as a conference paper at ICLR 2021
which can be written as
K
max max X vTD(Sk)Xku s.t. (2D(Sk) - In)Xku ≥ 0, ∀k,
Sk⊆[n] u∈B2 k=1
Sk∈S
K
D(Sk)Xku ≥ D(Sk)Xju,∀j,k ∈ [K],XD(Sk) =In.
k=1
We again enumerate all hyperplane arrangements and index them in an arbitrary order, where we
define the overall set as SK := {(Si1, . . . , SiK) : Sik ∈ S, ∀k, i; PkK=1 D(Sik) = In, ∀i} and
Pconv = |Sk|. Then, following the same steps in (13)-(17) gives the following convex problem
1
min -
wi ,w0i ∈Rh 2
Pconv K
XXD(Sik)Xk(w0i -wi) -y
i=1 k=1
2P
conv
+ β X (kwik2 + kwi0k2)
2	i=1
(29)
s.t. (2D(Sik) - In)Xkwi ≥0, (2D(Sik) - In)Xkwi0 ≥0,∀i,k,
D(Sik)Xkwi ≥ D(Sik)Xjwi, D(Sik)Xkwi0 ≥ D(Sik)Xj wi0, ∀i, j, k.
__	-	-	一一一 一	, 一 ∙-	，*	, 、	__ _	- -
We now note that there Will be m* pairs {wi , w*}. Then, we can construct a set of weights
*
{uj,Wj}m=ι as defined in the theorem and evaluate the non-convex objective in equation 5 using
these weights as follows
p1 ≤ 2
*
m*
Xmaxpool ({(xku*}3i) wj- y
j=1
2
Pconv
X
i=1,wi0* 6=0
Pconv
X
i=1,wi* 6=0
+β
2
+β
which has the same objective value with equation 29. Since strong duality holds for the convex
program, we have p1j = d1j , which is equal to the value of the convex program equation 29 achieved
by the prescribed parameters above.	□
A.9 Proof of Theorem 3.1
By using a rescaling for each w1j and w2j, equation 8 can be equivalently stated as
2
j1
P2 =	L-min
{{ulj }l=1 ,w1j ,w2j }jm=1 2
w1j ∈B2,ulj ∈UL ,∀l,j
m	L-2
X xYUljw1j
j=1	l=1	+
w2j - y	+βkw2k1.
2
(30)
Let us denote the eigenvalue decomposition of Ulj as Ulj = FDlj FH, where F ∈ Cd×d is the DFT
matrix and Dlj ∈ Cd×d is a diagonal matrix. Then, we again take the dual with respect to w2 and
change the order of min-max as follows
Pj ≥ dd = maχ - 1 kv-yk2 + 2 kyk2 s.t.
max
Dlj∈DL
w1j ∈B2
vT xF LY-2 Dlj FH w1j
≤ β, ∀j,
(31)
where DL := {(D1, . . . , DL-2 : Dl ∈ Cd×d, ∀l ∈ [L - 2]; QlL=-12 Dl	≤ dL-2}. Below we
prove that strong duality holds for equation 31.
In order to obtain the dual of the semi-infinite problem in equation 31, we again take dual with respect
to V (see Appendix A.7 and Goberna & Ldpez-Cerdd (1998); Shapiro (2009) for further details),
which yields
2
d2 ≤ p2,∞=min1
/
θL ∈ΘL
L-2
xF Y DlFHw1
l=1	+
dμ(θL) - y + β∣∣μ∣∣τv
2
20
Published as a conference paper at ICLR 2021
where ΘL :=	{(D1, . . . , DL-2,	w1)	: Dl ∈ DL, ∀l	∈	[L	- 2];	w1	∈	B2}.	Then, selecting
μ = Pm=I W2j δ(θL - θLj), where m* ≤ n +1, gives
m
*
2
p2,∞ =	min *
{{Dlj}lL=-12,w1j,w2j}jm=*1
Dlj ∈DL,w1j ∈B2,∀j,l
=p2
j=1
L-2
XF Y DljFH w1j
l=1	+
w2j -y	+β kw2k1
2
m
2 x
provided that m ≥ m* holds. Then, the rest of the strong duality proof directly follows from Proof
of Proposition A.1.
We now focus on the single-sided dual constraint
DmDLVT (X Y DlwI)	≤ β,
W 1∈B2	∖	l=1	+ +
which can be written as
L-2
L-2
max max v
S⊆[n] Dl∈DL
S∈S W 1∈B2
TD(SI)X ∏ DlWι s.t. (2D(S) - In)X ɪɪ Dlw 1 ≥ 0.
l=1
l=1
(32)
Since the inner maximization is convex (after a variable change as q = QL-12 Dl Wι) and there exists
a strictly feasible solution for a fixed D(S) matrix, equation 32 can also be written as
L-2
max min max vTD(Si)XZ + αT(2D(Si) — In)X Y Dlwι
S∈S 一 W 1∈B2	l = 1
=max min ∣∣vTD(S)X + αT(2D(S) - In)X∣∣∞dL-2.
S⊆[n] α≥0
S∈S
We now enumerate all hyperplane arrangements and index them in an arbitrary order, which are
denoted as D(Si), where i ∈ [Pcconv]. Then, we have
equation 32 ^⇒ ∀i ∈ [Pcconv], min ∣∣vTD(Si)X + αT(2D(Si) — In)Xk∞d 2 ≤ β
α≥0
^⇒ ∀i ∈ [Pcconv], ∃αi ≥ 0 s.t. kvTD(Si)X + αT(2D(Si)- In)X∣∣∞d 2	≤ β.
We now use the same approach for the two-sided constraint in equation 31 to represent equation 31
as a finite dimensional convex problem as follows
max -2l∣v -yk2 + 2kyk2
αi ,α0i ≥0	2	2
s.t. kvTD(Si)X + αT(2D(Si)- In)Xk∞dL-2 ≤ β, k - VTD(Si)X + α广
(33)
L-2
(2D(Si) - In)Xk∞dF ≤ β,∀i.
We note that the above problem is convex and strictly feasible for v = αi = α0i = 0. Therefore,
equation 33 can be written as
Pcconv
ʌmin	max -2kv - yk2 + 2l∣yk2 + X λi (β -kvTD(Si)X + αT(2D(Si)- In)Xk∞dɪ
λi,λi ≥0	0	2	2
i	αi ,α0i ≥0	i=1
Pcconv
XT	L-2
λi (β -k - VT D(Si)X + αi (2D(Si) - In)X k∞d F ) .	(34)
i=1
Next, we introduce new variables Zi , Z0i ∈ Cd to represent equation 34 as
Pcconv
λminn	max	min	-5kv	- yk2 + 5ky∣∣2+ E	λi	(β	+	dɪ kTD(Si)X +	aT(2D(Si)-	In)X)	Zi
λi,λ0i≥0αi,αv0i≥0zz0i∈∈BB11 2	2	i=1
Pcconv
cconv
+ X λi (β + dL-2 (-vTD(Si)X + αiT (2D(Si) - In)X) Zi)
i=1
(35)
21
Published as a conference paper at ICLR 2021
We note that the objective is concave in v, αi , α0i and convex in Zi , Z0i . Moreover the set B1 is convex
and compact. We recall Sion’s minimax theorem (Sion, 1958) for the inner max-min problem and
express the strong dual of the problem equation 35 as
Pcconv
min min max --kv - y|2 + 弓|切|2 + £% [β + dɪ kTD(Si)X + αT(2D(Si)-In)X) Zi
λi,λ0i≥0zz0i∈∈BB11αi,αv0i≥0	2	2	i=1
Pcconv
cconv
+ X λi (β + d-ɪ (-vTD(Si)X + αi (2D(Si)- In)X) zi
i=1
(36)
Now, we can compute the maximum with respect to v, αi , α0i analytically to obtain the following
problem
1
mm	mm 一
λi ,λ0i ≥0 zi ∈B1 2
z0i∈B1
Pcconv
dL-2 X D(Si)X (λizi
i=1
- λizi) - y
2P
cconv
+β X (λi +λ0i)	(37)
2	i=1
〜
〜
s.t. (2D(Si) - In)XZi ≥ 0, (2D(Si)- In)XZi ≥ 0,∀i.
L-2	L-2
Now We apply a change of variables and define Ci = d"ɪ 入网 and Ci = d-ɪ λizi. Thus, We obtain
1
mm 一
ci ,c0i 2
Pcconv
X D(Si)X(Ci - Ci)- y
i=1
2P
cconv
+ -L-2 X (kCikl + kCikl)
2 d F i=1
~ .
(38)
,—,. .	_ , .ʌ , — , _. . , ..
s.t. (2D(Si) - In)XCi ≥ 0, (2D(Si)- In)XCi ≥ 0,∀i,
L-2	L-2
where we eliminate the variables λi, λi, since λi = kCik 1 /d-ɪ and λi = IlCikI/d"ɪ are feasible
and optimal.
Optimal weight construction for equation 8:
Given the optimal weights for the convex program in equation 38, i.e., denoted as {c*, C； }P=conv,
we use the following relation
〜
~	~ L l-2 I |c*| ∖ I I |c*| ∖ ∙φ ∕∣∣C*kι
XCi=Xdiag卜Fm^diagHd⅛)e iy大
=X (Y dia"(旦、2(LF!! dia",可!冲 S≡≡
=md g〔UclkJ dddiag〔VdL-2)e VdF
where φi is defined such that Cil = diag (|Cil |) ejφi. Thus, we can directly set the parameters as
follows
Dli = diag d1
,Wli = diag IJ-lL⅛ ) ejφi, wli = J⅛⅛
1	1 i	L TL — 2	2	2 i	TL — 2
d V d	V d "ɪ
which can be equivalently written as
Ul — Fdia Jd 1 /"∣c^^! FH w* — Γc^ w/ — ∕1c^
Uli=Fdiag	讦产,w1i=V可，w2i=v 丁
to exactly match with the problem formulation in equation 8. We first note that k QlL=-12 Ulli k2F =
k QlL=-12Dllik2F = dL-2, ∀i, l, therefore, this set of parameters is feasible for equation 8. Now, we
prove the optimality by showing that these parameters have the same regularization cost with the
convex program in equation 38 as follows
Pccov	Pcconv
2 X kw*ik2 + w*2 = ɪ X kC*kι.
2	2	2
i=1	d 2 i=1
1*
The same steps can also be applied to C0i . Then, the rest of the proof directly follows from The-
orem 2.1. Therefore, we prove that a set of optimal layer weights for equation 8, denoted as
L2	*
{{Ullj}lL=-12, w1lj, w2lj}jm=1, can be obtained from the optimal solution to equation 38, denoted as
ʃe* N* IPCConv	I~I
{Ci , Ci }i=1	.	LJ
22
Published as a conference paper at ICLR 2021
A.10 Proof of Theorem 4.1
We now provide the proof for the three-layer CNN architecture with two ReLU layers, which has the
following primal optimization problem
Pq = mm
3 uj,w1j,w2j
uj ∈B2
mK
X X (Xk uj)+ w1jk	w2j - y
1
2
2
βm
+ 2 X (kwjk2 + w2j)
2 j=1
Then, the dual is
p3 ≥ d3 =mVax -1 kv-yk2 + 2 kyk2 s.t. u,m⅛
(Xku)+ w1k
≤ β,
(39)
Dual of equation 39
2
d3 ≤ p3,∞=mμn1
Z	XK (Xku)+ w1k
u,w1∈B2	k=1	+
dμ(u, wι) - y + βkμk
2
TV ,
(40)
where strong duality holds, i.e., d3q = p3,∞, by Proof of Proposition A.1. Then, the finite equivalent
is as follows
2
p3,∞
p3
1
min -
uj,w1j ∈B2 2
w2
m* K K	∖
(Xkuj)+w1jk	w2j -y
+ βkw2 k1
2
We now focus on a single-sided dual constraint
T
max vT
u,w1 ∈B2
XK (Xku)+ w1k	≤β
(41)
which can be written as
KK
max max	vTD(S2)XD(S1k)Xkuw1k	s.t.	(2D(S2)	-	In)	X D(S1k)Xkuw1k ≥	0,	(2D(S1k)	-In)Xkuw1k	≥ 0
S2,S1k⊆[n] u,w1∈B2	k=1	k=1
S2,S1k∈S
K
max max max max
Ik∈{±1} S2,Sk⊆[n] w1∈B2 I∣qkk2≤lwikl
S2 ,S1k ∈S
vT D(S2)	IkD(S1k)Xkqk
k=1
(42)
K
s.t. (2D(S2) - In) XIkD(S1k)Xkqk ≥ 0, (2D(S1k) -In)Xkqk ≥ 0,
k=1
where we introduce the notation Ik ∈ {±1} to enumerate all possible sign patterns for w1k. Since
the inner maximization is convex and there exists a strictly feasible solution for fixed D(S1k), D(S2),
and Ik equation 42 can also be written as
K
max max min max max X v D(S1 )Xkqk + αk (2D(S1 ) - In )Xkqk + Ikγ (2D(S2) - In)D(S1 )Xkqk
Ik∈{±1} S2,S1k⊆[n] αk,γ≥0 w1 ∈B2 Iqk I2 ≤w1k k 1
S2 ,S1k ∈S
K
= max max min max X vT D(S1k)Xkqk + αkT (2D(S1k) -In)Xkqk +IkγT(2D(S2) -In)D(S1k)Xk	|w1k|
Ik∈{±1} S2,S1k⊆[n] αk,γ≥0 w1∈B2 k=1	2
S2 ,S1k ∈S
1
= max max min
Ik∈{±1} S2 ,S1k⊆[n] αk ,γ≥0
S2 ,S1k ∈S
TD(Sk)Xkqk + aT(2D(Sk) - In)Xkqfc + IkYT(2D(S?) - In)D(Sk)X^；)
23
Published as a conference paper at ICLR 2021
Then, we have
equation 41 V⇒
∀i, j, min
αk,γ≥0
TD(S1ki)Xkqk +αkT(2D(S1ki) - In)Xkqk + IijkγT (2D(S2j) -In)D(S1ki)Xk2
1
2
≤β
^⇒
∀i, j, ∃αijk , γij ≥ 0 s.t.
T D(S1ki)Xkqk + αiTjk(2D(S1ki) - In)Xkqk + IijkγiTj(2D(S2j) -In)D(S1ki)Xk2
1
2
≤ β.
We now use the same approach for the two-sided constraint as follows
max -2Ilv - yk2 + 2llyk2
αijk ,α0ijk ≥0
γij ,γi0j ≥0
(43)
s.t.
TD(S1ki)Xkqk +αiTjk(2D(S1ki) - In)Xkqk + IijkγiTj(2D(S2j) -In)D(S1ki)Xk2
D(S1ki)Xkqk + αiTjk(2D(S1ki) -In)Xkqk+IijkγiTj(2D(S2j)-In)D(S1ki)Xk2
1
2
≤ β,
1
2
≤ β, ∀i, j.
We note that the above problem is convex and strictly feasible for v = αijk = α0ijk = Yij = Yi0j = 0.
Therefore, Slater’s conditions and consequently strong duality holds (Boyd & Vandenberghe, 2004),
and equation 43 can be written as
ʌ minη	max - 1 kv - yk2 + 1 I∣yk2
λij ,λ ≥0	2	2
ij	αijk ,α0ijk≥0
γij ,γi0j ≥0
(44)
P1 P2	K
+ Σ∑λij Iβ-	∑llvT D(S1ki)Xkqk + αiTjk(2D(S1ki) - In)Xkqk + IijkγiTj(2D(S2j) - In)D(S1ki)Xk22
i=1 j=1	k=1
1
2
P1 P2	K
+ Σ∑>i∕β- Ell-vTD(Ski)Xkqk+ αiTjk (2D(S1ki) - In)Xk qk + Iij k γiTj (2D(S2j ) - In)D(S1ki)Xk ll2
i=1 j=1	k=1
Next, we introduce new variables zijk , z0ijk ∈ Rh to represent equation 44 as
1
2
min max
λij,λ0 ≥0	v0
ij	αijk ,α0ijk≥0
γij ,γi0j ≥0
P1	P2
,l min ,	-2Ilv - yk2 + 1 I∣yk2
zijk :kzij k k2 ≤qij k	2	2
zijk :kzij k k2 ≤qij k
qij ,q0ij ∈B2
K
(45)
+ ∑ ∑λij β + £ (VT D(Ski)Xk qfc + aTk (2D(Ski)- In)Xk Q® + IijkYT (2D(S2j)-In)D(Ski)Xk) Zijk
i=1 j=1
P1	P2
k=1
K
+ ∑ ∑λij-	β + £ (-vTD(Ski)Xkqk + αT%(2D(Ski) - In)XkQk + IijkYT(2D(S2j∙) - In)D(Ski)Xk) Zijk
i=1 j=1
k=1
Then, the strong dual of the problem equation 45 as
min
min
λij ,λij ≥0 zijk :kzijk k2 ≤qijk
zijk :kzijk k2 ≤qijk
P1	P2
qij ,q0ij ∈B2
K
max	— 2 kv — yk2 + 1 l∣yk2
αijk,α0ijk≥0
γij ,γi0j ≥0
(46)
+ ∑ ∑λij- β + £ (vT D(Ski)Xk qk + aTk (2D(Ski)- In)Xk Qk + IijkYT (2DSj)-In)D(Ski)Xk) Zijk
i=1 j=1
P1	P2
k=1
K
+ ∑ ∑λij-	β + £ (-vTD(Ski)XkQk + αT%(2D(Ski) - In)XkQk + IijkYT(2D(S2j∙) - In)D(Ski)Xk) Zijk
i=1 j=1
k=1
24
Published as a conference paper at ICLR 2021
Now, we can compute the maximum with respect to v, αijk, α0ijk, γij, γi0j analytically to obtain the
following problem
1
mm	min -
λij ,λij ≥0 zijk :kzijk k2 ≤qijk 2
z0ijk :kz0ijk k2 ≤qi0jk
qij ,q0ij ∈B2
P2	P1 K
XDs2j XXIijkD(SIi)Xk (λjzijk - λijZijk) - y
j=1	i=1 k=1
(47)
P1 K
s.t. (2D( S2j) - In) X XIijkD(S1ki)Xkzijk ≥ 0, (2D(S1ki) - In)Xkzijk ≥ 0, ∀i, j, k
i=1 k=1
P1 K
(2D(S2j)-In)XXIijkD(S1ki)Xkz0ijk ≥0, (2D(S1ki) - In)Xkz0ijk ≥0,∀i,j,k.
i=1 k=1
Now we apply a change of variables and define cijk = λij zijk and c0ijk = λ0ijz0ijk. Thus, we obtain
1
mm	一
cijk ,c0ijk 2
P2	P1 K
XDs2j XXTijkD(Ski)Xk(Cijk -Cijk) -y
j=1	i=1 k=1
2
P1 P2
+βXX(kCijkF+kCijkF)
i=1 j=1
2
P1 K
s.t. (2D(S2j) - In) X XIijkD(S1ki)XkCijk ≥ 0, (2D(S1ki) - In)XkCijk ≥ 0, ∀i, j, k
i=1 k=1
P1 K
(2D(S2j)-In)XXIijkD(S1ki)XkC0ijk ≥0, (2D(S1ki) - In)XkC0ijk ≥0,∀i,j,k.
i=1 k=1
□
25