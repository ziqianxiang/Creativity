Published as a conference paper at ICLR 2021
How Does Mixup Help With Robustness and
Generalization ?
Linjun Zhang*
Rutgers University
linjun.zhang@rutgers.edu
Kenji Kawaguchi*
Harvard University
kkawaguchi@fas.harvard.edu
Amirata Ghorbani
Stanford University
amiratag@stanford.edu
Zhun Deng*
Harvard University
zhundeng@g.harvard.edu
James Zou
Stanford University
jamesz@stanford.edu
Ab stract
Mixup is a popular data augmentation technique based on taking convex combina-
tions of pairs of examples and their labels. This simple technique has been shown
to substantially improve both the robustness and the generalization of the trained
model. However, it is not well-understood why such improvement occurs. In this
paper, we provide theoretical analysis to demonstrate how using Mixup in training
helps model robustness and generalization. For robustness, we show that minimiz-
ing the Mixup loss corresponds to approximately minimizing an upper bound of
the adversarial loss. This explains why models obtained by Mixup training ex-
hibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign
Method (FGSM). For generalization, we prove that Mixup augmentation corre-
sponds to a specific type of data-adaptive regularization which reduces overfitting.
Our analysis provides new insights and a framework to understand Mixup.
1	Introduction
Mixup was introduced by Zhang et al. (2018) as a data augmentation technique. It has been em-
pirically shown to substantially improve test performance and robustness to adversarial noise of
state-of-the-art neural network architectures (Zhang et al., 2018; Lamb et al., 2019; Thulasidasan
et al., 2019; Zhang et al., 2018; Arazo et al., 2019). Despite the impressive empirical performance,
it is still not fully understood why Mixup leads to such improvement across the different aspects
mentioned above. We first provide more background about robustness and generalization properties
of deep networks and Mixup. Then we give an overview of our main contributions.
Adversarial robustness. Although neural networks have achieved remarkable success in many areas
such as natural language processing (Devlin et al., 2018) and image recognition (He et al., 2016a),
it has been observed that neural networks are very sensitive to adversarial examples — prediction
can be easily flipped by human imperceptible perturbations (Goodfellow et al., 2014; Szegedy et al.,
2013). Specifically, in Goodfellow et al. (2014), the authors use fast gradient sign method (FGSM)
to generate adversarial examples, which makes an image of panda to be classified as gibbon with
high confidence. Although various defense mechanisms have been proposed against adversarial
attacks, those mechanisms typically sacrifice test accuracy in turn for robustness (Tsipras et al.,
2018) and many of them require a significant amount of additional computation time. In contrast,
Mixup training tends to improve test accuracy and at the same time also exhibits a certain degree of
resistance to adversarial examples, such as those generated by FGSM (Lamb et al., 2019). Moreover,
the corresponding training time is relatively modest. As an illustration, we compare the robust test
* Equal contribution.
1
Published as a conference paper at ICLR 2021
Attack Size
00°∙°'°,°∙°s°,°j
SSo-+s ①+
30
Och
2 P
e
dro6 UoAπj---πjj① ue°
(a) Robustness	(b) Generalization
Figure 1: Illustrative examples of the impact of Mixup on robustness and generalization. (a) Adver-
sarial robustness on the SVHN data under FGSM attacks. (b) Generalization gap between test and
train loss. More details regarding the experimental setup are included in Appendix C.1, C.2.
accuracy between a model trained with Mixup and a model trained with standard empirical risk
minimization (ERM) under adversarial attacks generated by FGSM (Fig. 1a). The model trained
with Mixup loss has much better robust accuracy. Robustness of Mixup under other attacks have
also been empirically studied in Lamb et al. (2019).
Generalization. Generalization theory has been a central focus of learning theory (Vapnik, 1979;
2013; Bartlett et al., 2002; Bartlett & Mendelson, 2002; Bousquet & Elisseeff, 2002; Xu & Mannor,
2012), but it still remains a mystery for many modern deep learning algorithms (Zhang et al., 2016;
Kawaguchi et al., 2017). For Mixup, from Fig. (1b), we observe that Mixup training results in
better test performance than the standard empirical risk minimization. That is mainly due to its good
generalization property since the training errors are small for both Mixup training and empirical risk
minimization (experiments with training error results are included in the appendix). While there
have been many enlightening studies trying to establish generalization theory for modern machine
learning algorithms (Sun et al., 2015; Neyshabur et al., 2015; Hardt et al., 2016; Bartlett et al.,
2017; Kawaguchi et al., 2017; Arora et al., 2018; Neyshabur & Li, 2019), few existing studies have
illustrated the generalization behavior of Mixup training in theory.
Our contributions. In this paper, we theoretically investigate how Mixup improves both adver-
sarial robustness and generalization. We begin by relating the loss function induced by Mixup to
the standard loss with additional adaptive regularization terms. Based on the derived regulariza-
tion terms, we show that Mixup training minimizes an upper bound on the adversarial loss,which
leads to the robustness against single-step adversarial attacks. For generalization, we show how the
regularization terms can reduce over-fitting and lead to better generalization behaviors than those of
standard training. Our analyses provides insights and framework to understand the impact of Mixup.
Outline of the paper. Section 2 introduces the notations and problem setup. In Section 3, we
present our main theoretical results, including the regularization effect of Mixup and the subse-
quent analysis to show that such regularization improves adversarial robustness and generalization.
Section 4 concludes with a discussion of future work. Proofs are deferred to the Appendix.
1.1 Related work
Since its advent, Mixup training (Zhang et al., 2018) has been shown to substantially improve gen-
eralization and single-step adversarial robustness among a wide rage of tasks, on both supervised
(Lamb et al., 2019; Verma et al., 2019a; Guo et al., 2019), and semi-supervised settings (Berth-
elot et al., 2019; Verma et al., 2019b). This has motivated a recent line of work for developing a
number of variants of Mixup, including Manifold Mixup (Verma et al., 2019a), Puzzle Mix (Kim
et al., 2020), CutMix (Yun et al., 2019), Adversarial Mixup Resynthesis (Beckham et al., 2019),
and PatchUp (Faramarzi et al., 2020). However, theoretical understanding of the underlying mech-
anism of why Mixup and its variants perform well on generalization and adversarial robustness is
still limited.
2
Published as a conference paper at ICLR 2021
Some of the theoretical tools we use in this paper are related to Wang & Manning (2013) and Wager
et al. (2013), where the authors use second-order Taylor approximation to derive a regularized loss
function for Dropout training. This technique is then extended to drive more properties of Dropout,
including the inductive bias of Dropout (Helmbold & Long, 2015), the regularization effect in ma-
trix factorization (Mianjy et al., 2018), and the implicit regularization in neural networks (Wei et al.,
2020). This technique has been recently applied to Mixup in a parallel and independent work (Car-
ratino et al., 2020) to derive regularization terms. Compared with the results in Carratino et al.
(2020), our derived regularization enjoys a simpler form and therefore enables the subsequent anal-
ysis of adversarial robustness and generalization. We clarify the detailed differences in Section 3.
To the best of our knowledge, our paper is the first to provide a theoretical treatment to connect the
regularization, adversarial robustness, and generalization for Mixup training.
2	Preliminaries
In this section, we state our notations and briefly recap the definition of Mixup.
Notations. We denote the general parameterized loss as l(θ, z), where θ ∈ Θ ⊆ Rd and z =
(x, y) is the input and output pair. We consider a training dataset S = {(χι, yι),…，(xn, yn)},
where xi ∈ X ⊆ Rp and yi ∈ Y ⊆ Rm are i.i.d. drawn from a joint distribution Px,y . We
further denote Xi,j(λ) = λx% + (1 - λ)xj, y%,j(λ) = λy% + (1 - λ)y7- for λ ∈ [0,1] and let
Zi j(λ) = (Xi j(λ),yi j(λ)). Let L(θ) = Ez~px yl(θ, Z) denote the standard population loss and
L3 * sntd(θ,S)= in=1 l(θ,Zi)/n denote the standard empirical loss. For the two distributions D1 and
D2, we use pD1 + (1 - p)D2 forp ∈ (0, 1) to denote the mixture distribution such that a sample is
drawn with probabilities p and (1 - p) from D1 and D2 respectively. For a parameterized function
fθ(x), We use Vfθ(x) and Vθfθ(x) to respectively denote the gradient with respect to X and θ. For
two vectors a and b, we use cos(x, y) to denotehx, y)∕(∣∣x∣∣∙ Ilyl∣).
Mixup. Generally, for classification cases, the output yi is the embedding of the class of xi, i.e.
the one-hot encoding by taking m as the total number of classes and letting yi ∈ {0, 1}m be the
binary vector with all entries equal to zero except for the one corresponding to the class of xi . In
particular, if we take m = 1, it degenerates to the binary classification. For regression cases, yi can
be any real number/vector. The Mixup loss is defined in the following form:
1n
Lm (θ,S)=溟 ∑ Eλ~Dλl(θ,Zij(λ)),	⑴
n i,j=1
where Dλ is a distribution supported on [0, 1]. Throughout the paper, we consider the most com-
monly used Dλ 一 Beta distribution Beta(α, β) for α,β > 0.
3 Main Results
In this section, we first introduce a lemma that characterizes the regularization effect of Mixup.
Based on this lemma, we then derive our main theoretical results on adversarial robustness and
generalization error bound in Sections 3.2 and 3.3 respectively.
3.1 The regularization effect of Mixup
As a starting point, we demonstrate how Mixup training is approximately equivalent to optimizing
a regularized version of standard empirical loss Lsntd(θ, S). Throughout the paper, we consider the
following class of loss functions for the prediction function fθ(x) and target y:
L = {l(θ, (x, y))∣l(θ, (x, y)) = h(fθ(x)) - yfθ(x) for some function h}.	(2)
This function class L includes many commonly used losses, including the loss function induced by
Generalized Linear Models (GLMs), such as linear regression and logistic regression, and also cross-
entropy for neural networks. In the following, we introduce a lemma stating that the Mixup training
with λ 〜Dλ = Beta(α, β) induces a regularized loss function with the weights of each regulariza-
tion specified by a mixture of Beta distributions Dλ = Oae Beta(α +1,β) + α+^ Beta(β +1,α).
3
Published as a conference paper at ICLR 2021
Lemma 3.1. Consider the loss function l(θ, (x,y)) = h(fθ (x)) — yfθ(x), where h(∙) and fθ (∙)
for all θ ∈ Θ are twice differentiable. We further denote Dλ as a uniform mixture of two Beta
distributions, i.e.,工+ɑ彳Beta(α + 1,β) + α+βBeta(B + 1, α), and DX as the empirical distribution
of the training dataset S = (xι, •…,Xn), the corresponding Mixup loss LmX(θ, S), as defined in
Eq. (1) with λ 〜Dλ = Beta(α, β), can be rewritten as
3
Lmix (θ,S) = Lntd(θ, S) + X Ri(θ, S) + Eλ 〜Dλ [(1 — λ)2 以1 — λ)],
i=1
where lima→0 夕(a) = 0 and
Eλ D [1 — λ] P ,	丁
Rι(θ,S) = ^Dλl——-V(h0(fθ(Xi))- yi)Vfθ(xi)>Erχ"χ [rx — Xi],
nx
i=1
EX 京[(1 — λ)2] "	,,	丁	丁
R2(θ,S)=	———Eh'0(fθ(xi))Vfθ(xi)>E%〜DX [(rχ - Xi)(rχ — Xi)>]Vfθ(xi),
2n
i=1
Eλ 力[(1 — λ)2] S ,	9	丁
R3(θ,S) =	λ~H———f(h(fθ (Xi))- yi)Erχ 〜DX [(rχ — Xi)V2fθ (Xi)(rχ — g)>].
2n
i=1
By putting the higher order terms of approximation in 夕(∙)，this result shows that MixUP is related
to regularizing Vfθ (Xi) and V2fθ (Xi), which are the first and second directional derivatives with
respect to Xi . Throughout the paper, our theory is mainly built upon analysis of the quadratic ap-
proximation of Lnmix(θ, S), which we further denote as
3
Lmix(θ,S) := Lnd(θ,S) + XRi(θ,S).
i=1
(3)
Comparison with related work. The result in Lemma 3.1 relies on the second-order Taylor ex-
pansion of the loss function Eq. (1). Similar approximations have been proposed before to study the
regularization effect of Dropout training, see Wang & Manning (2013); Wager et al. (2013); Mianjy
et al. (2018); Wei et al. (2020). Recently, Carratino et al. (2020) independently used similar approx-
imation to study the regularization effect of Mixup. However, the regularization terms derived in
Carratino et al. (2020) is much more complicated than those in Lemma 3.1. For example, in GLM,
our technique yields the regularization term as shown in Lemma 3.3, which is much simpler than
those in Corollaries 2 and 3 in Carratino et al. (2020). One technical step we use here to simplify the
regularization expression is to equalize Mixup with input perturbation, see more details in the proof
in the Appendix. This simpler expression enables us to study the robustness and generalization of
Mixup in the subsequent sections.
Validity of the approximation. In the following, we present numerical experiments to support
the approximation in Eq. (3). Following the setup of numerical validations in Wager et al. (2013);
Carratino et al. (2020), we experimentally show that the quadratic approximation is generally very
accurate. Specifically, we train a Logistic Regression model (as one example of a GLM model,
which we study later) and a two layer neural network with ReLU activations. We use the two-moons
dataset (Buitinck et al., 2013). Fig. 2 shows the training and test data’s loss functions for training
two models with different loss functions: the original Mixup loss and the approximate Mixup loss.
Both models had the same random initialization scheme. Throughout training, we compute the
test and training loss of each model using its own loss function. The empirical results shows the
approximation of Mixup loss is quite close to the original Mixup loss.
3.2 Mixup and Adversarial Robustness
Having introduced Lnmix(θ, S) in Eq. (3), we are now ready to state our main theoretical results. In
this subsection, we illustrate how Mixup helps adversarial robustness. We prove that minimizing
Lnmix(θ, S) is equivalent to minimizing an upper bound of the second order Taylor expansion of an
adversarial loss.
4
Published as a conference paper at ICLR 2021
Logistic Regression
Two Layer ReLU Neural Network
Figure 2: Comparison of the original Mixup loss with the approximate Mixup loss function.
Throughout this subsection, we study the logistic loss function
l(θ, z) = log(1 +exp(fθ(x))) -yfθ(x),
where y ∈ Y = {0, 1}. In addition, let g be the logistic function such that g(s) = es/(1 + es) and
consider the case where θ is in the data-dependent space Θ, defined as
Θ = {θ ∈ Rd : yifθ(xi) + (yi - 1)fθ(xi) ≥ 0 for all i = 1, . . . ,n}.
Notice that Θ contains the set of all θ with zero training errors:
Θ ⊇ {θ ∈ Rq : the label prediction ^i = 1{fθ (xi) ≥ 0} is equal to y% for all i = 1,...,n }. (4)
In many practical cases, the training error (0-1 loss) becomes zero in finite time although the training
loss does not. Equation (4) shows that the condition of θ ∈ Θ is satisfied in finite time in such
practical cases with zero training errors.
Logistic regression. As a starting point, we study the logistic regression with fθ(x) = θ>x,
in which case the number of parameters coincides with the data dimension, i.e. P = d. For a
given ε > 0, we consider the adversarial loss with '2-attack of size εy∕d, that is, Lndv(θ,S) =
1/n Pn=I max∣∣δw∣∣2≤ε√d l(θ, (Xi + δi, yi)). We first present the following second order Taylor ap-
proximation of Landv (θ, S).
Lemma 3.2. The second order Taylor approximation of Lndv (θ, S) is Pn=ι Iadv(εVd, (Xi ,yi))∕n,
where for any η > 0, x ∈ Rp and y∈ {0, 1},
Iadv(η, (X, y)) = l(θ, (X,y)) + n|g(XTO) - y| ∙ kθk2 + % ∙ g(XTo)(I - g(XTo)) ∙ kθk2.⑸
ʃʌ	∙	7	/ c /	∖ ∖ i τ miv / n n∖ ι ∙ ι , ι ∙ , ∙	∙	,ι r n
By comparing ladv(δ, (X, y)) and Lnmix(o, S) applied to logistic regression, we prove the following.
Theorem 3.1.	Suppose that fθ (X) = XTo and there exists a constant cx > 0 such that kXi k2 ≥
Cxʌ/d for all i ∈ {1,..., n}. Then, for any θ ∈ Θ, we have
1 n	1 n
Lnix (θ, S) ≥ n E iadv (εi√d, (Xi, yi)) ≥ n£ Iadv (εmiχ√d,⑵,%))
i=1
i=1
where εi = RiCxE、〜介入[1 - λ] with Ri = | cos(θ,Xi)∣, and εmiχ = R ∙ cxE、〜生[1 - λ] with
R = mini∈{1,...,n} |cos(o,Xi)|.
Theorem 3.1 suggests that Lnmix(o, S) is an upper bound of the second order Taylor expansion of the
adversarial loss with '2-attack of size εmiχʌ/d. Note that εmiχ depends on θ; one can think the final
radius is taken at the minimizer of Lnmix(o, S). Therefore, minimizing the Mixup loss would result in
a small adversarial loss. Our analysis suggests that Mixup by itself can improve robustness against
small attacks, which tend to be single-step attacks (Lamb et al., 2019). An interesting direction
for future work is to explore whether combining Mixup with adversarial training is able to provide
robustness against larger and more sophisticated attacks such as iterative projected gradient descent
and other multiple-step attacks.
5
Published as a conference paper at ICLR 2021
(a) Linear
200
epoch
(b) ANN
(c) ANN: epoch = 0 (d) ANN: epoch = 400
Figure 3: The behaviors of the values of R and Ri during training for linear models and artificial
neural network with ReLU (ANN). The subplots (c) and (d) show the histogram of (R1, R2, . . . , Rn)
for ANN before and after training. R and Ri control the radii of adversarial attacks that Mixup
training protects for.
Remark 3.1. Note that Theorem 3.1 also implies adversarial robustness against '∞ attacks with size
ε sincefor any attack δ, kδ∣∣∞ ≤ ε implies ∣∣δk2 ≤ ε√d, and therefore maxkδ∣∣∞≤e l(θ, (X+δ,y)) ≤
maxkδ∣∣26√d∙e l(θ, (X + δ,y)).
In the following we provide more discussion about the range of R = mini∈{1,...,n} | cos(θ, xi)|. We
first show that under additional regularity conditions, we can obtain a high probability lower bound
that does not depend on sample size. We then numerically demonstrate that R tends to increase
during training for both cases of linear models and neural networks at the end of this subsection.
A constant lower bound for logistic regression. Now, we show how to obtain a constant lower bound
by adding some additional conditions.
Assumption 3.1. Let us denote Θn ⊆ Θ as the set of minimizers of Lnmix(θ, S). We assume there
exists a set Θ* 1, such that for all n ≥ N, where N is a positive integer, Θn ⊆ Θ* With probability
at least 1 - δn where δn → 0 as n → 0. Moreover, there exists a τ ∈ (0, 1) such that
Pτ = P ({x ∈ X : | cos(x, θ) | ≥ T for all θ ∈ Θ*}) ∈ (0,1].
Such condition generally holds for regular optimization problems, where the minimizers are not lo-
cated too dispersedly in the sense of solid angle (instead of Euclidean distance). More specifically,
if we normalize all the minimizers’ `2 norm to 1, this assumption requires that the set of minimizers
should not be located all over the sphere. In addition, Assumption 3.1 only requires that the proba-
bility pτ and the threshold τ to be non-zero. In particular, if the distribution of X has positive mass
in all solid angles, then when the set of minimizers is discrete, this assumption holds. For more
complicated cases in which the set of minimizers consists of sub-manifolds, as long as there exists
a solid angle in X that is disjoint with the set of minimizers, the assumption still holds.
Theorem 3.2.	Under Assumption 3.1, for fθ (X) = X>θ, if there exists constants bx, cx > 0 such
that Cx√d ≤ ∣∣Xi∣2 ≤ bχ√d for all i ∈ {1,...,n}. Then, with probability at least 1 一 δn 一
2 exp(-npT/2), there exists constants κ > 0,κ2 > κι > 0, such thatfor any θ ∈ Θn, we have
1 n
Lmi(θ,S) ≥ -YjIadv (εmix√d, (Xi,yi))
n i=1
where εmix = RcxEλ 〜Dλ [1 - λ] and R = min n 2K2 -PTK12-κι), q 2-P⅛κpτ }厂
Neural networks with ReLU / Max-pooling. The results in the above subsection can be extended
to the case of neural networks with ReLU activation functions and max-pooling. Specifically, we
IUnder some well-separation and smoothness conditions, We would expect all elements in Θn will fall into
mix
a neighborhood Nn of minimizers of ES Lmnix(θ, S), and Nn will shrink as n increases, i.e. Nn+1 ⊂ Nn . One
can think Θ* is a set containing all Nn for n ≥ N.
6
Published as a conference paper at ICLR 2021
consider the logistic loss, l(θ, z) = log(1 + exp(fθ (x))) - yfθ(x) with y ∈ {0, 1}, where fθ(x)
represents a fully connected neural network with ReLU activation function or max-pooling:
fθ(x) = β>σ(WN-1 …(W2σ(W1x)).
Here, σ represents nonlinearity via ReLU and max pooling, each Wi is a matrix, and β is a column
vector: i.e., θ consists of {Wi}iN=-11 and β. With the nonlinearity σ for ReLU and max-pooling,
the function fθ satisfies that fθ(x) = Vfθ(χ)>χ and V2fθ(x) = 0 almost everywhere, where
the gradient is taken with respect to input x. Under such conditions, similar to Lemma 3.2, the
adversarial loss function Pn=ι maxkδ. k2≤ε√d l(θ, (Xi + δi, yi))∕n can be written as
.,	L 1 二	ε2∙ d 1 3..	C
Lnd(θ,S )+εmix√d(- E∣g(fθ (Xi))-yi∣kVfθ (Xi)k2)+ -mix- (- f∖h"(fθ (Xi))∣kVfθ (Xi)k2).
n i=1	n i=1
(6)
With a little abuse of notations, we also denote
δ2-
Iadv(δ, (χ,y)) = l(θ, (χ,y)) + s|g(fe(X))-y|kVfe(x)k2 + ɪ|h (f (X))IkVfθ(χ)k2∙
The following theorem suggests that minimizing the Mixup loss in neural nets also lead to a small
adversarial loss.
Theorem 3.3. Assume that fθ(Xi) = Vfθ(Xi)>Xi, V2fθ(Xi) = 0 (which are satisfied by the ReLU
and max-pooling activation functions) and there exists a constant Cx > 0 such that ∣∣Xi∣∣2 ≥ Cx √-
for all i ∈ {1, . . . , n}. Then, for any θ ∈ Θ, we have
1 n	- n
Lna (θ,S) ≥ n X ladv (-i√d, (xi,yi)) ≥ n X ladv (-mix√-,(Xi,yi))
i=1
i=1
where -i =尼”旧入〜方入[1 — λ], -mix = R ∙ cxEλ〜方入[1 — λ] and Ri = I Cos(Vfθ(xi),Xi)∣, R
mini∈{1,...,n} | cos(Vfθ(Xi), Xi)|.
Similar constant lower bound can be derived to the setting of neural networsk. Due to limited space,
please see the detailed discussion in the appendix.
On the value of R = mini Ri via experiments. For both linear models and neural networks, after
training accuracy reaches 100%, the logistic loss is further minimized when kfθ(Xi)k2 increases.
Since kfθ(Xi)k2 = kVfθ(Xi)>Xik2 = kVfθ(Xi)k2kXik2Ri, this suggests that Ri and R tend to
increase after training accuracy reaches 100% (e.g., Vfθ(Xi) = θ in the case of linear models). We
confirm this phenomenon in Fig. 3. In the figure, R is initially small but tends to increase after
training accuracy reaches 100%, as expected. For example, for ANN, the value of R was initially
2.27 × 10-5 but increased to 6.11 × 10-2 after training. Fig. 3 (c) and (d) also show that Ri for each
i-th data point tends to increase during training and that the values of Ri for many points are much
larger than the pessimistic lower bound R: e.g., whereas R = 6.11 × 10-2, we have Ri > 0.8 for
several data points in Fig. 3 (d). For this experiment, We generated 100 data points as Xi 〜N(0, I)
and yi = 1{x>θ* > 0} where Xi ∈ R10 and θ* 〜N(0,I). We used SGD to train linear models
and ANNs with ReLU activations and 50 neurons per each of two hidden layers. We set the learning
rate to be 0.1 and the momentum coefficient to be 0.9. We turned off weight decay so that R is not
maximized as a result of bounding kVfθ(Xi)k, which is a trivial case from the discussion above.
3.3 Mixup and Generalization
In this section, we show that the data-dependent regularization induced by Mixup directly controls
the Rademacher complexity of the underlying function classes, and therefore yields concrete gener-
alization error bounds. We study two models - the Generalized Linear Model (GLM) and two-layer
ReLU nets with squared loss.
Generalized linear model. A Generalized Linear Model is a flexible generalization of ordinary
linear regression, where the corresponding loss takes the following form:
l(θ, (X, y)) = A(θ>X) - yθ>X,
7
Published as a conference paper at ICLR 2021
where A(∙) is the log-partition function, X ∈ Rp and y ∈ R. For instance, if We take A(θ>χ)=
log(1 + eθ>x) and y ∈ {0, 1}, then the model corresponds to the logistic regression. In this para-
graph, we consider the case where Θ, X and Y are all bounded.
By further taking advantage of the property of shift and scaling invariance of GLM, we can further
simplify the regularization terms in Lemma 3.1 and obtain the following results.
n
Lemma 3.3. Consider the centralized dataset S, that is, 1/n i=1 xi = 0. and denote ΣX =
nXiX>. For a GLM, if A(∙) is twice differentiable, then the regularization term obtained by the
second-order approximation of Lmmix(θ, S) is given by
1 n	(1	λ)2
2n[∑ A00(θ>xi)]∙ Eλ〜Dλ [½2~L]θ>∑xθ,	⑺
i=1
where Dλ = α二. Beta(α + 1, β) + α：. Beta(β + 1, α).
Given the above regularization term, we are ready to investigate the corresponding generalization
gap. Following similar approaches in Arora et al. (2020), we shed light upon the generalization
problem by investigating the following function class that is closely related to the dual problem of
Eq. (7):
Wγ := {x → θ>x, such that θ satisfying EχA00(θ>x) ∙ θ>Σχθ 6 γ},
where α > 0 and ΣX = E[xixi>]. Further, we assume that the distribution of x is ρ-retentive
for some P ∈ (0,1/2], that is, if for any non-zero vector V ∈ Rd, [Eχ [A00(x>v)]]2 ≥ P ∙
min{1, Ex (v>x)2}. Such an assumption has been similarly assumed in Arora et al. (2020) and
is satisfied by general GLMs when θ has bounded `2 norm. We then have the following theorem.
Theorem 3.4. Assume that the distribution ofxi is P-retentive, and let ΣX = E[xx>]. Then the
empirical Rademacher complexity of Wγ satisfies
Rad(WY, S) ≤ max{(2)", (Y)"}∙ /rankM.
PP	n
The above bound on Rademacher complexity directly implies the following generalization gap of
Mixup training.
Corollary 3.1. Suppose A(∙) is LA-Lipchitz continuous, X, Y and Θ are all bounded, then there
exists constants L,B > 0, such thatforall θ satisfying EχA00(θ>x) ∙θ>Σχθ 6 Y (the regularization
induced by Mixup), we have
L(θ) 6 Lnd(θ,S) + 2L ∙ La ∙ (max{(Y)1/4, (Y)1/2} ∙ J…取)) + BrlogpM,
n	P P	n	2n
with probability at least 1 - δ.
Remark 3.2. This result shows that the Mixup training would adapt to the intrinsic dimension of
x and therefore has a smaller generalization error. Specifically, if we consider the general ridge
penalty and consider the function class Wγridge := {x → θ>x, kθk2 6 γ}, then the similar tech-
nique would yield a Rademacher complexity bound Rad(WY, S) ≤ max{(γ∕ρ)1/4, (γ∕ρ)1∕2} ∙
,p/n, where P is the dimension of X. This bound is much larger than the result in Theorem 3.4
when the intrinsic dimension rank(ΣX) is small.
Non-linear cases. The above results on GLM can be extended to the non-linear neural network
case with Manifold Mixup (Verma et al., 2019a). In this section, we consider the two-layer ReLU
neural networks with the squared loss L(θ, S) = n Pn=ι(yi — fθ(Xi))2, where y ∈ R and fθ(x) is
a two-layer ReLU neural network, with the form of
fθ(x) = θ>σ(Wx) + θo.
where W ∈ Rp×d, θ1 ∈ Rd, and θ0 denotes the bias term. Here, θ consists of W, θ0 and θ1.
If we perform Mixup on the second layer (i.e., mix neurons on the hidden layer as proposed by
Verma et al. (2019a)), we then have the following result on the induced regularization.
8
Published as a conference paper at ICLR 2021
ɪ . ___∙~t λ ʃʌ . S(T . 1	1	■	. ■ r C ∕τrτ^ XTn .ι .ι	ι	. ■
Lemma 3.4. Denote ΣσX as the sample covariance matrix of {σ(W xi)}in=1, then the regularization
term obtained by the Second-order approximation of Lmix(θ, S) is given by
(1 - λ)2
Eλ~Dι [(-λ2-h]θ>∑ X θι,	(8)
where Dλ 〜@:. Beta(α + 1, β) + α：. Beta(β + 1, α).
To show the generalization property of this regularizer, similar to the last section, we consider the
following distribution-dependent class of functions indexed by θ:
WγNN := {x → fθ(x), such that θ satisfying θ1>ΣσXθ1 6 γ},
where ΣσX = E[ΣσX] and α > 0. We then have the following result.
Theorem 3.5. Let μσ = E[σ(Wx)] and denote the generalized inverse of ∑χ by ΣcX. Suppose X,
Y and Θ are all bounded, then there exists constants L, B > 0, such that for all fθ in WγNN (the
regularization induced by Manifold Mixup), we have, with probability at least 1 - δ,
L(θ) 6 Ln叫θ, S)+4L ∙ SmæXH + Brrlo尹I.
n	n	2n
4	Conclusion and Future Work
Mixup is a data augmentation technique that generates new samples by linear interpolation of multi-
ple samples and their labels. The Mixup training method has been empirically shown to have better
generalization and robustness against attacks with adversarial examples than the traditional training
method, but there is a lack of rigorous theoretical understanding. In this paper, we prove that the
Mixup training is approximately a regularized loss minimization. The derived regularization terms
are then used to demonstrate why Mixup has improved generalization and robustness against one-
step adversarial examples. One interesting future direction is to extend our analysis to other Mixup
variants, for example, Puzzle Mix (Kim et al., 2020) and Adversarial Mixup Resynthesis (Beckham
et al., 2019), and investigate if the generalization performance and adversarial robustness can be
further improved by these newly developed Mixup methods.
Acknowledgments
The research of Linjun Zhang is supported by NSF DMS-2015378. The research of James Zou is
supported by NSF CCF 1763191, NSF CAREER 1942926 and grants from the Silicon Valley Foun-
dation and the Chan-Zuckerberg Initiative. The research of Kenji Kawaguchi is partially supported
by the Center of Mathematical Sciences and Applications at Harvard University. This work is also
in part supported by NSF award 1763665.
References
Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Unsupervised
label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.
Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro. Dropout: Explicit forms and
capacity control. arXiv preprint arXiv:2003.03397, 2020.
Sanjeev Arora, R Ge, B Neyshabur, and Y Zhang. Stronger generalization bounds for deep nets via
a compression approach. In 35th International Conference on Machine Learning, ICML 2018,
2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized rademacher complexities. In
International Conference on Computational Learning Theory, pp. 44—58. Springer, 2002.
9
Published as a conference paper at ICLR 2021
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Christopher Beckham, Sina Honari, Vikas Verma, Alex M Lamb, Farnoosh Ghadiri, R Devon Hjelm,
Yoshua Bengio, and Chris Pal. On adversarial mixup resynthesis. In Advances in neural informa-
tion processing systems, pp. 4346-4357, 2019.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural
Information Processing Systems, pp. 5049-5059, 2019.
Olivier BoUsqUet and Andre Elisseeff. Stability and generalization. Journal of machine learning
research, 2(Mar):499-526, 2002.
Lars BUitinck, Gilles LoUppe, MathieU Blondel, Fabian Pedregosa, Andreas MUeller, Olivier Grisel,
Vlad NicUlae, Peter Prettenhofer, Alexandre Gramfort, JaqUes Grobler, Robert Layton, Jake Van-
derPlas, Arnaud Joly, Brian Holt, and Gael Varoquaux. API design for machine learning software:
experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Min-
ing and Machine Learning, pp. 108-122, 2013.
Luigi Carratino, Moustapha Cisse, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regular-
ization. arXiv preprint arXiv:2006.06049, 2020.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature. In NeurIPS Creativity Workshop 2019, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Mojtaba Faramarzi, Mohammad Amini, Akilesh Badrinaaraayanan, Vikas Verma, and Sarath Chan-
dar. Patchup: A regularization technique for convolutional neural networks. arXiv preprint
arXiv:2006.07794, 2020.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regulariza-
tion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3714-3722,
2019.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234. PMLR,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630-645. Springer, 2016b.
David P Helmbold and Philip M Long. On the inductive bias of dropout. The Journal of Machine
Learning Research, 16(1):3403-3454, 2015.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. International Conference on Machine Learning, 2020.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
10
Published as a conference paper at ICLR 2021
Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training:
Achieving robust neural networks without sacrificing too much accuracy. In Proceedings of the
12th ACM Workshop on Artificial Intelligence and Security, pp. 95-103, 2019.
Poorya Mianjy, Raman Arora, and Rene Vidal. On the implicit bias of dropout. In International
Conference on Machine Learning, pp. 3540-3548, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Behnam Neyshabur and Zhiyuan Li. Towards understanding the role of over-parametrization in gen-
eralization of neural networks. In International Conference on Learning Representations (ICLR),
2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural
networks: A theoretical view. arXiv preprint arXiv:1506.05232, 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Micha-
lak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks.
In Advances in Neural Information Processing Systems, pp. 13888-13899, 2019.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.
V Vapnik. Estimation of dependences based on empirical data nauka, 1979.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,
2013.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-
Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states.
In International Conference on Machine Learning, pp. 6438-6447. PMLR, 2019a.
Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation con-
sistency training for semi-supervised learning. In Proceedings of the 28th International Joint
Conference on Artificial Intelligence, pp. 3635-3641. AAAI Press, 2019b.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
Advances in neural information processing systems, pp. 351-359, 2013.
Sida Wang and Christopher Manning. Fast dropout training. In international conference on machine
learning, pp. 118-126, 2013.
Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of
dropout. arXiv preprint arXiv:2002.12915, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,
2012.
11
Published as a conference paper at ICLR 2021
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceed-
ings of the IEEE International Conference on Computer Vision,pp. 6023-6032, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018.
12
Published as a conference paper at ICLR 2021
Appendix
In this appendix, we provide proofs of the main theorems and the corresponding technical lemmas.
Additional discussion on the range of R in the case of neural nets, and some further numerical
experiments are also provided.
A Technique Proofs
A.1 Proof of Lemma 3.1
Consider the following problem with loss function lx,y (θ) := l(θ, (x, y)) = h(fθ(x)) -yfθ(x), that
is
1n
Lnd (θ, S) = - E[h(fθ (Xiy)- yfθ (Xi)].
ni=1
The corresponding Mixup version, as defined in Eq.(1), is
1n
Lmix(θ,S) = nEλ〜Beta(α,β) E [h(fθ(Xij(λ))) - (λyi + (1- λ)yj)fθ(Xij(λ))],
n	i,j=1
where Xij (λ) = λxi + (1 - λ)xj. Further transformation leads to
1n
Lmix(θ,S) = nn2 Eλ 〜Beta(α,β) X {λh(fθ (Xij (λ)))) - λyifθ (Xij (λ))
n	i,j=1
+ (1 - λ)h(fθ (Xij (λ))) - (1 - λ)yj fθ (Xij (λ))}
1n
=—Eλ 〜Beta(α,β)EB 〜Bern(λ) X {B[h(fθ (Xij (λ))) - Vifθ (Xij (λ))]
n	i,j=1
+ (1- B)[h(fθ (Xi,j (λ))) - yj fθ (Xi,j (λ))]}
Note that λ 〜 Beta(α, β),B∣λ 〜 Bern(λ), by conjugacy, We can exchange them in order and
have
α
B 〜Bern(—~^~β), λ | B 〜Beta(α + B,β + 1 — B).
As a result,
n
Lmix (θ,S )=F X { ʌ Eλ 〜Beta(α+1,β)[h(fθ (Xij (λ))) -yifθ (Xi,j (λ))]
n i,j=1 α + β
+ α+β Eλ 〜Beta(α,β+1) [h(fθ (Xij (λ))) - y fθ (Xij (λ))]}.
Using the fact 1 - Beta(α, β + 1) and Beta(β +1, α) are of the same distribution and Xij (1 - λ)=
Xji(λ), we have
X Eλ 〜Beta(α,β+1)[h(fθ (Xij (λ))) - yj fθ (Xij (λ))]
i,j
=X Eλ 〜Beta(β+1,α) [h(fθ (Xij (λ))) - yifθ (Xij (λ))].
i,j
Thus, let DDλ = α++βBeta(α + 1,β) + a+β Beta(B + 1, α)
1n
LmiX(θ, S) = - £E1 〜DDλErx〜Dxh(f (θ, λXi + (1- λ)rx))) - yf(θ, λ” + (1- λ)rx)
n i=1
1n
=n £旧1 〜DλErx~DxMyi(θ)	(9)
n i=1
13
Published as a conference paper at ICLR 2021
where Dx is the empirical distribution induced by training samples, and Xi = λxi + (1 - λ)rχ.
In the following, denote S = {(xi, yi)}n=ι, and let us analyze Lntd(θ, S) = 1 ∑2i=ι lχiyi(θ), and
compare it with Lntd(θ, S). Let α = 1 - λ and ψi(α) = lχi^i (θ). Then, using the definition of the
twice-differentiability of function ψi ,
lxi,yi (θ) = ψi(α) = ψi(0) + ψi (0)α + 1 ψi0(0)α2 + α2 ψi (α),	(IO)
where limz→o 夕i(z) = 0. By linearity and chain rule,
/0/ ʌ	NTcfθ(Xi) ∂Xi	∂fθ(Xi) ∂Xi
ψi S) = h (fθ (Xi))	石-yi	石
=h0(fθ(Xi)) dfθ(xi)(rx - Xi) - yi dfθ(Xi) (rx - Xi)
∂Xi	∂Xi
where We used d∂α = (rx - Xi). Since
,dfθ(Xi) (rx-Xi) = ɪ(rx-Xi)>[dfθ(Xj]> = (rx-x∕>V2fθ(Xi)dXi = (rx-Xi)>V2fθ(Xi)(rx-Xi)
∂α ∂Xi	∂α	∂Xi	∂α
we have
ψ00(α) =h0(fθ(Xi))(rx - Xi)>V2fθ(Xi)(rx - Xi)
+ h”(fθ(Xi))[df∂XXi) (rx - Xi)]2 - yi(rx - Xi)>V2fθ(Xi)(rx — Xi).
Thus,
ψi0(0) = h0(fθ(Xi))Vfθ(Xi)> (rx-Xi)-yiVfθ(Xi)> (rx-Xi) = (h0(fθ(Xi))-yi)Vfθ(Xi)> (rx-Xi)
ψi00(0) =h0(fθ(Xi))(rx - Xi)>V2fθ(Xi)(rx - Xi) + h00(fθ(Xi))[Vfθ(Xi)> (rx - Xi)]2
- yi (rx - Xi)>V2fθ (Xi)(rx - Xi).
=h00(fθ(Xi))Vfθ(Xi)>(rx - Xi)(rx - Xi)>Vfθ(Xi) + (h0(fθ(Xi)) - yi)(rx - Xi)>V2fθ(Xi)(rx - Xi)
By substituting these into equation 10 with 夕(α) = n Pn=ι 夕i(α), We obtain the desired statement.
A.2 Proofs related to adversarial robustness
A.2. 1 Proof of Lemma 3.2
Recall that Lndv(θ,S) = n PNI maXkδik2≤ε√d l(θ, E + δi,yi)) and g(u) = 1/(1 + e-u). Then
the second-order Taylor expansion of l(θ, (X + δ, t)) is given by
l(θ, (x + δ,y)) = l(θ, (X,y)) + (g(θ>x) - y) ∙ δ>θ + 2g(X>θ)(1 - g(X>θ)) ∙ (δ>θ)2.
Consequently, for any given η > 0,
max l(θ, (x + δ, y)) = max l(θ, (x, y)) + (g(θ>x) - y) ∙ δ>θ + 1 g(x>θ)(1 - g(x>θ)) ∙ (δ>θ)2
kδk2≤η	kδk2≤η	2
2
=l(θ, (χ,y)) + Mg(χ>θ) - y| ∙ ∣∣θk2 + }(g(χ>θ)(1 - g(χ>θ))) ∙ ∣∣θ∣∣2,
where the maximum is taken when δ = sgn(g(x>θ) - y) ∙ 儡 ∙ η.
A.2.2 Proof of Theorem 3.1
Since fθ(X) = X>θ, we have Vfθ(Xi) = θ and V2fθ(Xi) = 0. Since h(z) = log(1 + ez), we have
h0(z) = ι+Zez = g(z) ≥ 0 and h00(z) =(i+；z)2 = g(z)(1 - g(z)) ≥ 0. By substituting these into
the equation of Lemma 3.1 with Erx [rx] = 0,
~ ___ . ~ .
Lmix(θ, S) = Lmix(θ, S) + Rι(θ, S) + R2(θ, S),	(11)
14
Published as a conference paper at ICLR 2021
where
Rι(θ, S) = Eλ[(1- λ)] X(yi - g(χ>θ))θ>χi
n
i=1
R2(θ,S )= Eλ [(12- λ)2] X ∣g(x> θ)(1-g(x> θ))∣θτErχ [(rχ - xi)(rχ - xi)τ]θ
n	i=1
≥ Eλ[(1- λ)]2 X ∣g(x>θ)(1-g(x>θ))∣θτErχ [(rx - Xi)(rχ — Xi)>]θ
2n	i=1	i	i	x
where we used E[z2] = E[z]2 + Var(z) ≥ E[z]2 and θτ Erx [(rx - xi)(rx - xi)τ]θ ≥ 0. Since
Erx [(rx - xi)(rx - xi)τ] = Erx [rxrxτ - rxxiτ - xirxτ + xixiτ] = Erx [rxrxτ] + xixiτ where
Erx [rxrxτ] is positive semidefinite,
R2(θ, S) ≥ Eλ[(1-λ)]2 X ∣g(x>θ)(1 - g(x>θ))∣θT(Erx [rχr>]+ XixT)θ.
2n
i=1
≥ Eλ[(12- λ)]2 X ∣g(xτΘ)(1 - g(xτθ))l(θτxi)2
n	i=1
=叫了)]2 X ∣g(x>θ)(i - g(x>θ))lkθk2kxik2(cos(θ,xi))2
n	i=1
R2 c2xEλ [(1 - λ)]2 d	τ	τ 2
≥ ——— E∣g(x>θ)(i - g(x>θ))∣kθk2
2n	i=1 i	i	2
Now We bound E = E"(：-')] P3(yi - g(x>θ))(θTxi) by using θ ∈ Θ. Since θ ∈ Θ, we have
yifθ(xi) + (yi - 1)fθ(xi) ≥ 0, which implies that (θτxi) ≥ 0 ifyi = 1 and (θτxi) ≤ 0 ifyi = 0.
Thus, if yi = 1,
(yi - g(xiτθ))(θτxi) = (1 - g(xiτθ))(θτxi) ≥ 0,
since (θτxi) ≥ 0 and (1 - g(xiτ θ)) ≥ 0 due to g(xiτθ) ∈ (0, 1). If yi = 0,
(yi - g(xiτθ))(θτxi) = -g(xiτθ)(θτxi) ≥ 0,
since (θτxi) ≤ 0 and -g(xiτθ) < 0. Therefore, for all i = 1, . . . , n,
(yi - g(xiτθ))(θτxi) ≥ 0,
which implies that, since Eλ [(1 - λ)] ≥ 0,
Ri(θ, S) = Eλ[(In- λ)] X |yi - g(x>θ)l∣θτxiI
i=1
=Eλ[(1-λ)] X ∣g(xτθ) - yi∣kθk2kxik2∣ cos(θ,xi)∣
n
i=1
≥ RcxEλ[(n-λ)]√dX ∣g(xτθ) - yi∣kθk2
n	i=1
By substituting these lower bounds of R1(θ, S) and R2(θ, S) into equation 11, we obtain the desired
statement.
A.2.3 Proof of Theorem 3.2
Recall we assume that θn will fall into the set Θ* with probability at least 1 - δn, and δn → 0 as
n → ∞. In addition, define the set
Xθ* (τ) = {x ∈ X : ∣ cos(x,θ)∣ > T for all θ ∈ Θ*},
15
Published as a conference paper at ICLR 2021
there is T ∈ (0,1) such that X8* (τ) = 0, and
Pτ ：= P(X ∈ Xθ* (τ)) ∈ (0,1).
Let us first study
1n
—E Ig(X>θ)(I - g(X>θ))l(COSGXi))2
n i=1
Since We assume Θ* is bounded and Cx√d 6 IlXik2 6 bχ Vd for all i, there exists κ > 0, such that
Ig(X>θ)(1 — g(x>θ))∣ > κ.
If we denote P= {number of Xi S such that Xi ∈ Xθ*(t )}/n. Then, it is easy to see
n Pxi∈χθ * (τ)Ig(X>θ)(I- g(X>θ)X < (1 — p)/4
n >^xi∈xθ* (T) ∣g(X>θ)(1 — g(X>θ))∣ 6 pκ
For η2 satisfying
η2(i +上p/4) 6 τ2
pκ
we have
1n	1
n£m(/[θ)(1 -g(X>e))|(cos(4Xi))2 > n E	m(e1θ)(1-g(X>e))|T2
i=1	xi∈XΘ*(τ)
> 1 X	i9(x>θ)(1 - g(X> θ))lη2 +1 X	b(/jθ)(1- g(ejθ))的2.
nn
xi∈XΘ* (τ)	xi∈XΘc * (τ)
Lastly by Hoeffding’s inequality, if we take ε = pτ /2
(1 +
pκ
) 6 (1 +
(1— Pτ∕2)∕4)
(PT/2)K
with probability at least 1 一 2 exp(-2nε2)
I	4κpτ
η、2- 2 — Pt + 4κpτ
Similarly, ifwe study
n
Ig(Xi>θ) — yiII cos(θ, X)I
i=1
By boundedness of θ, X and y ∈ {0, 1}, we know there are constants κ1, κ2 > 0, such that
κ1 6 Ig(fθ(Xi)) — yiI 6 κ2
Similarly, we know
6	PT Ki	T
η	2κ2 — PT (κ2 — κ1) .
Combined together, we can obtain the result:
/ …∙ p	PT κ1	/	4κpT	1
η 6 mιn1------------；--------, A ------------}τ
2K2 — PT (K2 — K1)	2 —PT + 4KPT
16
Published as a conference paper at ICLR 2021
A.2.4 Proof of Theorem 3.3
From the assumption, we have fθ(Xi) = Vfθ(x∕>xi and V2fθ(Xi) = 0. Since h(z) = log(1 + ez),
We have h0(Z) = ι^ez = g(z) ≥ 0 and h00(z) = .+:之产 =g(z)(1 一 g(z)) ≥ 0. By substituting
these into the equation of Lemma 3.1 with Erx [rx] = 0,
~ ___ . ~ .
LmX(θ, S) = LmX(θ, S) + Rι(θ, S) + R2(θ, S),	(12)
where
Rι(θ,S) = Eλ[(1- λ)] Xyi- g(fθ(Xi)))fθ(Xi)
n i=1
R2(θ,S) = Eλ[(12- λ) ] X ∣g(fθ(Xi))(I- g(fθ(Xi))) Vfθ(xi)>Erχ [(rχ 一 Xi)(rχ - Xi)>]Vfθ(xi)
n	i=1
≥ Eλ[(I- λ" X ∣g(fθ(Xi))(I- g(fθ(Xi))) Vfθ(Xi)>Erχ [(rχ - Xi)(rχ - Xi)>]Vfθ(Xi)
n	i=1
whereweusedE[z2] = E[z]2+Var(z) ≥ E[z]2 andVfθ(Xi)>Erx [(rx-Xi)(rx-Xi)>]Vfθ(Xi) ≥
0. Since Erx [(rx - Xi)(rx - Xi)>] = Erx [rxrx> - rxXi> - Xirx> + XiXi>] = Erx [rxrx>] + XiXi>
where Erx [rxrx>] is positive semidefinite,
R2(θ,S) ≥ Eλ[(1 λ)] X ∖g(fθ(Xi))(1 - g(fθ(Xi)))∣Vfθ(Xi)>(Erx[rχr>] + XiX>)Vfθ(Xi).
2n
i=1
≥ Eλ[(1- λ)]2 X ∣g(fθ(Xi))(1 - g(fθ(Xi)))l(Vfθ(Xi)>Xi)2
2n
i=1
=Eλ[(1- λ)]2 X ∣g(fθ(Xi))(1 - g(fθ(Xi)))lkVfθ(Xi)k2kXik2(cos(Vfθ(Xi),Xi))2
2n
i=1
≥ RCcEλ;1-λ)]2d X ∣g(fθ(Xi))(1 - g(fθ(Xi)))∣kVfθ(Xi)k2
2n	i=1	2
Now we bound E = Eλ[α-λ)] P=(yi - g(fθ(Xi)))fθ(Xi) by using θ ∈ Θ. Since θ ∈ Θ, we have
yifθ(Xi) + (yi - 1)fθ(Xi) ≥ 0, which implies that fθ(Xi) ≥ 0 ifyi = 1 and fθ(Xi) ≤ 0 ifyi = 0.
Thus, if yi = 1,
(yi - g(fθ(Xi)))(fθ(Xi)) = (1 - g(fθ(Xi)))(fθ(Xi)) ≥ 0,
since (fθ(Xi)) ≥ 0 and (1 - g(fθ(Xi))) ≥ 0 duetog(fθ(Xi)) ∈ (0, 1). Ifyi = 0,
(yi - g(fθ(Xi)))(fθ(Xi)) = -g(fθ(Xi))(fθ(Xi)) ≥ 0,
since (fθ(Xi)) ≤ 0 and -g(fθ(Xi)) < 0. Therefore, for all i = 1, . . . , n,
(yi - g(fθ(Xi)))(fθ(Xi)) ≥ 0,
which implies that, since Eλ [(1 - λ)] ≥ 0,
Rι(θ,S) = Eλ[(1- λ)] X |yi - g(fθ(Xi))llfθ(Xi)1
n
i=1
=Eλ[(1- λ)] X ∣g(fθ(Xi))-yi∣kVfθ(Xi)k2kXik21cos(Vfθ(Xi),Xi)∣
n	i=1
RcxEλ[(1- λ)]√d XI /F/ C mʊʃ( Ml
≥-------------------T∖g(fθ (Xi)) — yi |kVf6 (Xi)k2
n
i=1
By substituting these lower bounds of E and F into equation 12, we obtain the desired statement.
17
Published as a conference paper at ICLR 2021
A.3 Proofs related to generalization
A.3.1 Proof of Lemma 3.3 and Lemma 3.4
We first prove Lemma 3.3. The proof of Lemma 3.4 is similar.
By Eq. (9), We have Lmix(θ, S) = Lntd(θ, S), where S = {(Xi, yi)}f=ι With Xi = λx% + (1 - λ)人
and λ 〜Dλ = α⅛Beta(α + 1,β) + αβ^Beta(β + 1, α). Since for Generalized Linear Model
(GLM), the prediction is invariant to the scaling of the training data, so it suffices to consider S =
{(Xi,yi)}i=ι with Xi = ɪ(λxi + (1 - λ)rχ).
In the following, we analyze Lsntd(θ, S). For GLM the loss function is
1n
Lntd(θ,S) = - EMyi (θ)
n
i=1
1n
—E-(yiX>θ - A(X>θ)),
n i=1
where A(∙) is the log-partition function in GLMs.
Denote the randomness (of λ and rx) by ξ, then the second order Taylor expansion yields
Eξ[A(X>θ) - A(x>θ)] 2nd-ord= approx Eξ[A0(χ>θ)(Xi - χi)>θ + A00(χ>θ)Var(X>θ)]
Notice Eξ[Xi - Xi] = 0 and Varξ(Xi) = n Pn=ι XiX> = ΣX, then we have the RHS of the last
equation equal to
A00(X>θ)(E(1λ2 λ)2 )θ>∑xθ.
As a result, the second-order Taylor approximation of the Mixup loss Lsntd(θ, S) is
n	1 n	(1 λ)2
E Tyixθ - A(x>θ)) + 2nQT A00(x>θ)]E((-^^)θτΣXθ
i=1	n i=1
1 n	(1	λ)2
=Lnd(θ, S) + 2n[∑ A00(X>θ)]E(k^L)θ>∑xθ.
i=1
This completes the proof of Lemma 3.3. For Lemma 3.4, since the Mixup is performed on the final
layer of the neural nets, the setting is the same as the least square with covariates σ(wjτX). Moreover,
since we include both the linear coefficients vector θ1 and bias term θ0, the prediction is invariant to
the shifting and scaling of σ(wjτX). Therefore, we can consider training θ1 and θ0 on the covariates
{(σ(WXi) - σw) + 1-λ(σ(Wrχ) - σw)}i=「where σw = n Pn=ι σ(WXi). Moreover, since
we consider the least square loss, which is a special case of GLM loss with A(U) = 11 u2, we have
A00 = 1. Plugging these quantities into Lemma 3.3, we get the desired result of Lemma 3.4.
A.3.2 Proof of Theorem 3.4 and Corollary 3.1
By definition, given n i.i.d. Rademacher rv. ξ1, ..., ξn, the empirical Rademacher complexity is
1n
Rad(WY, S) = Eξ	sup	- VξiθτXi
a(θ)∙θ>Σχθ≤γ n i=ι
Let Xi = Σχ2Xi, a(θ) = Eχ[A00(x>Θ)] and V = Σ1^2θ, then ρ-retentiveness condition implies
a(θ)2 ≥ P ∙ min{1, Ex(Θtx)2} ≥ P ∙ min{1, θτΣχθ} and therefore a(θ) ∙ θτΣχθ ≤ Y implies that
kvk2 = θ>∑xθ ≤ max{(P)1/2, P}.
18
Published as a conference paper at ICLR 2021
As a result,
1n
Rad(WY, S) =Eξ	sup	— V ξ∕>g
a(θ)∙θ>Σχθ≤γ n i=ι
1n
=Eξ	SUP	-Σ2ξiVτXi
a(θ)∙θ>Σχθ≤γ n『1
1n
≤Eξ	SUp —	ξivτXi
kvk2≤( P )1∕2∨ P n 士
≤^ ∙ ( Y )1/4 ∨ ( Y )1/2 ∙ Eξ k X ξixi k
nρ ρ
≤1 ∙ (Y )1/4 ∨ (Y )1/2 ∙∖ Eξ k X8诵 k2
n ρ ρ	i=1
≤1 ∙ ( Y)1/4 ∨ ( Y )1/2 •
nρ ρ
n
Xxτx
xixi.
=1
Consequently,

Rad(Wγ, S) = ES [Rad(Wγ, S)]
un
≤1 ∙ (γ)1/4 ∨ (γ)1/2 飞 XEχi[χ>Xi]
n ρ	ρ i=1 ii
1
^√n
• (γ)1/4 ∨ (Y)1/2 ∙ rank(∑χ).
ρ
ρ
Based on this bound on Rademacher complexity, Corollary 3.1 can be proved by directly applying
the following theorem.
Lemma A.1 (Result from Bartlett & Mendelson (2002)). For any B-uniformly bounded and L-
Lipchitz function ζ, for all φ ∈ Φ, with probability at least 1 - δ,
∕log(1∕δ)
V	2n
1n
EZ(。(Xi)) ≤ n XZ -—⑼”
A.3.3 Proof of Theorem 3.5
To prove Theorem 3.5, by Lemma A.1, it suffices to show the following bound on Rademacher
complexity.
Theorem A.1. The empirical Rademacher complexity of WγNN satisfies
Rad(WNN,S) ≤ 2；Y YrankNX)/网!111.
By definition, given n i.i.d. Rademacher rv. ξ1, ...,ξn, the empirical Rademacher complexity is
1n
Rad(Wγ,S) =Eξ sup — Vξiθ>σ(Wxi).
Wγ ni=1
19
Published as a conference paper at ICLR 2021
Let θι = ∑X1∕2θι and μσ = E[σ(Wx)], then
nn
RS(WNN) =Eξ sup —X ξi时∑P2(σ(Wxi)-μσ)+Eξ sup - X ξi时Σp2μσ
WγNN n i=1	WγNN n i=1
-n	-
≤kθ1k2 ∙kEξ[- Eξi∑p2σ(Wxi)]k + ∣∣θιk∙ -√= k∑p2μσk
i=1
≤2SY ∙ (rank(∑X) + |因?/2μσk2)
n,
where the last inequality is obtained by using the same technique as in the proof of Lemma 3.4.
Combining all the pieces, we get
Rad(Wγ,S) ≤ J ∙ ra*).
B Discussion of R in the Neural Network case
(B.1).	On the value of R = mini Ri via experiments for neural networks. After training accuracy
reaches 100%, the loss is further minimized when kfθ(xi)k2 increases. Since
kfθ (xi)k2 = kVfθ (xi)>xik2 = kVfθ (Xi )k2kxik2Ri,
this suggests that Ri and R tend to increase after training accuracy reaches 100%. We confirm this
phenomenon in Figure 3. In the figure, R is initially small but tend to increase after training accuracy
reaches 100%, as expected. For example, for ANN, the values of R were initially 2.27 × -0-5 but
increased to 6.-- × -0-2 after training. Figure 3 (c) and (d) also show that Ri for each i-th data
point tends to increase during training and that the values of Ri for many points are much larger
than the pessimistic lower bound R: e.g., whereas R = 6.-- × -0-2, we have Ri > 0.8 for several
data points in Figure 3 (d). For this experiment, We generated 100 data points as Xi 〜N(0, I) and
yi = 1{x>θ* > 0} where Xi ∈ R10 and θ* 〜N(0,I). We used SGD to train linear models and
ANNs With ReLU activations and 50 neurons per each of tWo hidden layers. We set the learning
rate to be 0.1 and the momentum coefficient to be 0.9. We turned off weight decay so that R is not
maximized as a result of bounding kVfθ(Xi)k, which is a trivial case from the above discussion.
(B.2).	A constant lower bound for neural networks. Similarly, we can obtain a constant lower bound
by adding some additional conditions.
Assumption B.1. Let Us denote Θn ⊆ Θ as the set of minimizers of Lmix(θ, S). We assume there
exists a set Θ*, such that for all n ≥ N, where N ISa positive integer, Θn ⊆ Θ* with probability at
least - - δn and δn → 0 as n → 0. Moreover, there exists τ, τ0 ∈ (0, -) such that
Xθ*(τ,τ0) = {x ∈ X : | cos(x, Vfθ(x))| > τ, kVfθ(x)k > T0, forall θ ∈ Θ*},
has probability pτ,τ0 ∈ (0, -).
Theorem B.1. Define
Fθ ：= {fθ∣fθ(Xi) = Vfθ(xi)>xi, V2fθ(Xi) = 0 almosteverywhere,θ ∈ Θ}.
Under Assumption B.1, for any fθ (x) ∈ Fθ, if there exists constants bχ, Cx > 0 such that Cx √d ≤
IlXiIl2 ≤ bx√d for all i ∈ {1,..., n}. Then, with probability at least 1 — δn — 2exp(-npT T0/2),
there exist constants κ > 0, κ2 > κ1 > 0, if we further have θ ∈ Θn, then
1 n
LX (θ,S) ≥ - V/adv (Mmix √d, (Xi,%))
n i=1
where Mmix = R cxEλ 〜Dλ [1-λ] and R = min{ 4 (2-pτ,τ opT 匚 κ+2Pτ,τ 0 κτ 02 , pτ,τo κιW+T2Mτ,τo )κ2τ 〃 }厂
20
Published as a conference paper at ICLR 2021
B.1 Proof of Theorem B.1
Notice if we assume for
Xθ*(τ,τ0) = {x ∈ X : | cos(x, Vfθ(x))| > τ, kVfθ(x)k > T0, for all θ ∈ Θ*},
there is τ,τ0 ∈ (0,1) such that Xθ* (τ,τ0) = 0, and
pτ,τ0 := P(x ∈ XΘ* (τ, τ0)) ∈ (0, 1).
Let us first study
1n
—E ∖g(fθ(Xiy) - yi∣kVfθ(χi)k2∣ Cos(Vfθ(χi),χi)∣
n i=1
By boundedness of θ, x and y ∈ {0, 1}, we know there is κ1, κ2 > 0, such that
κ1 6 |g (fθ (xi)) - yi | 6 κ2
If we denote P= {number of XiS such that Xi ∈ Xθ*(τ,τ0)}∕n. Then, it is easy to see
1 Pxi∈xθ *(τ,τ0) l∣g(fθ (Xi))-yi∣kVfθ (Xi)k2	(1 - p)κ2τ 00
1 Pxi∈Xθ*(τ,τo) ∣g(fθ(Xi))-yi∣kVfθ(Xi)k2 6 pκιτ0
For η2 satisfying
η(i +里包贮)
pK]T0
6τ
we have
1
n
n
X
i=1
1n
∣g(fθ(Xi))-yi∣kVfθ(Xi)k2Cos(Vfθ(Xi),Xi)| > -£|g(fe(Xi))-yi∣kVfθ(Xi)k2η
n i=1
Besides, if we consider
n
X ∣g(fθ(Xi))(1-g(fθ(Xi)))lkVfθ(Xi)k2(cos(Vfθ(Xi),Xi))2
i=1
Thus, we have
η2(1 +
(1- P)∕4τ002 )
ʌ _o2	)
pκτ 02
6 τ2
With probability at least 1 - 2 exp(-2nε2), for ε = pτ,τ0∕2, we have
6 i { ____________Pτ,τ0 KT02________________Pτ,τ0 KlT0________}
η ` min V (2 一 Ry0)∕4τ002 + pτ,τ0κτ02 ,pτ,τ0κιτ0 + (2 一 pτ,τ0)κ2τ00 ʃʃ
B.2 PROOFS OF THE CLAIM fθ (X) = Vfθ (X)>X AND V2fθ (X) = 0 FOR NN WITH
ReLU/Max-pooling
Consider the neural networks with ReLU and max-pooling:
fθ(x) = W[L]σ[Lτ] (z[Lτ]),	z[l](X,θ) = W[l]σ(lT) (z[l-1](X,θ)) , l = 1, 2,...,L - 1,
with σ(0) z[0] (X, θ) = X, where σ represents nonlinear function due to ReLU and/or max-pooling,
and W[l] ∈ RNl ×Nl-1 is a matrix of weight parameters connecting the (l - 1)-th layer to the l-
th layer. For the nonlinear function σ due to ReLU and/or max-pooling, We can define σ[l] (x, θ)
21
Published as a conference paper at ICLR 2021
such that σ[l] (x, θ) is a diagonal matrix with each element being 0 or 1, and σ[l] z[l] (X, θ) =
σ[l] (x, θ)z[l] (x, θ). Using this, we can rewrite the model as:
fθ (x) = W [L] σ [L-1] (x, θ )W [L-1] σ [L-2] (x, θ)…W [2] σ [1] (x, θ )W [1] x.
Since d'；,⑼ = 0 almost everywhere for all l, which will cancel all derivatives except for
dx W [1] χ, we then have that
dfθ(x) = W [L] σ [L-1] (x, θ )W [L-1] σ [L-2] (x, θ)…W [2] σ [1] (x, θ )W [1].
∂x
Therefore,
dfθ(x) X = W [L] σ [L-1] (x,θ)W [L-1] σ [L-2] (x,θ)…W [2] σ [1] (x,θ)W [1] X = f (x).
∂x
This proves that fθ(x) = V∕θ(x)>X for deep neural networks With ReLU/MaX-Pooling.
Moreover, from equation 13, we have that
▽2 fθ (x) = vx (W [L] σ [L-1] (x,θ)W [L-1] σ [L-2] (χ,θ)…W [2] σ [1] (x,θ)W [1]) = o,
(13)
since d，；,,θ) = 0 almost everywhere for all l. This proves that V2 fθ(x) = 0 for deep neural
networks with ReLU/MaX-pooling.
C More About Experiments
C.1 Adversarial Attack and Mixup
We demonstrate the comparison between Mixup and standard training against adversarial attacks
created by FGSM. We train two WideResNet-16-8 (Zagoruyko & Komodakis, 2016) architectures
on the Street View House Numbers SVHN (Netzer et al., 2011)) dataset; one model with regular
empirical risk minimization and the other one with Mixup loss (α = 5, β = 0.5). We create FGSM
adversarial attacks (Goodfellow et al., 2014) for 1000 randomly selected test images. Fig. (1a)
describes the results for the two models. It can be observed that the model trained with Mixup loss
has better robustness.
C.2 Validity of the approximation of adversarial loss
In this subsection, we present numerical experiments to support the approximation in Eq. (5) and (6).
Under the same setup of our numerical experiments of Figure 2, we experimentally show that the
quadratic approximation of the adversarial loss is valid. Specifically, we train a Logistic Regression
model (as one example of a GLM model, which we study later) and a two layer neural network with
ReLU activations. We use the two-moons dataset (Buitinck et al., 2013). Fig. 4, and compare the
approximated adversarial loss and the original one along the iterations of computing the original
adversarial loss against '2 attacks. The attack size is chosen such that eʌ/d = 0.5, and both models
had the same random initialization scheme. This experiment shows that using second order Taylor
expansion yields a good approximation of the original adversarial loss.
Adversarial
Approximate
LogiStC Regression
Two Layer ReLu Neural Network
Figure 4: Comparison of the original adversarial loss with the approximate adversarial loss function.

22
Published as a conference paper at ICLR 2021
C.3 Generalization and Mixup
Figures 5-8 show the results of experiments for generalization with various datasets that moti-
vated us to mathematically study Mixup. We followed the standard experimental setups without
any modification as follows. We adopted the standard image datasets, CIFAR-10 (Krizhevsky &
Hinton, 2009), CIFAR-100 (Krizhevsky & Hinton, 2009), Fashion-MNIST (Xiao et al., 2017), and
Kuzushiji-MNIST (Clanuwat et al., 2019). For each dataset, we consider two cases: with and with-
out standard additional data augmentation for each dataset. We used the standard pre-activation
ResNet with 18 layers (He et al., 2016b). Stochastic gradient descent (SGD) was used to train the
models with mini-batch size = 64, the momentum coefficient = 0.9, and the learning rate = 0.1. All
experiments were implemented in PyTorch (Paszke et al., 2019).
-mi×up
ERM
ss-ws
(SSo-) de6 UO-N 一»」3U36
O 50	100	1 50	200
epoch
UOt3) dsUOqZ-=3us
(sso-) de6 UOnZ-»」3U36
(a)	without extra data augmentation
(b)	With extra data augmentation
,J0t3ws
0	50	100	150	200
epoch
Figure 5: Generalization: CIFAR-10
0	50	100	150	200
epoch
0	1∞	200	300	400
epoch
ss-ws
0	100	200	3∞	400
epoch
*∞g~:。
」」3 c-nb
ss-u≡l
50	100	150	200
epoch
:60:。
,J0t3 c-nb
SSO-U»匕
100	200	300	400
epoch
(SSo-) de6 UO-N-»」3U36
50	100	150	200
epoch
(SSo-) de6 UOqZ-»」3U36
1∞	200	3∞	400
epoch
(a)	Without extra data augmentation
(b)	With extra data augmentation
Figure 6: Generalization: CIFAR-100
23
Published as a conference paper at ICLR 2021
10,
general⅛ion gap (error) train error	- test,rror -	generaliztion gap (error) Enerror	评 terror
-mixup
—ERM
epoch
epoch
6xl0^1
4 x 10^1
3 x IO-1
2 x IO-1
ss-ws
ss-u≡l
O IOO 200	300	400
epoch
O IOO 200	3∞	400
epoch
(a) Without extra data augmentation
(a) Without extra data augmentation
Figure 8: Generalization: Kuzushiji-MNIST


W
」s」3ws
SSO-U»匕
(b) With extra data augmentation
Figure 7: Generalization: Fashion-MNIST
(b) With extra data augmentation
24