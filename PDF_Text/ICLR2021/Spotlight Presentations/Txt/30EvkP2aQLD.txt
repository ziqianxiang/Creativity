Published as a conference paper at ICLR 2021
What are the Statistical Limits of Offline RL
with Linear Function Approximation?
Ruosong Wang
Carnegie Mellon University
ruosongw@andrew.cmu.edu
Dean P. Foster
University of Pennsylvania and Amazon
dean@foster.net
Sham M. Kakade
University of Washington, Seattle and Microsoft Research
sham@cs.washington.edu
Ab stract
Offline reinforcement learning seeks to utilize offline (observational) data to guide
the learning of (causal) sequential decision making strategies. The hope is that
offline reinforcement learning coupled with function approximation methods (to
deal with the curse of dimensionality) can provide a means to help alleviate the
excessive sample complexity burden in modern sequential decision making prob-
lems. However, the extent to which this broader approach can be effective is not
well understood, where the literature largely consists of sufficient conditions.
This work focuses on the basic question of what are necessary representational and
distributional conditions that permit provable sample-efficient offline reinforce-
ment learning. Perhaps surprisingly, our main result shows that even if: i) we have
realizability in that the true value function of every policy is linear in a given set
of features and 2) our off-policy data has good coverage over all features (under a
strong spectral condition), any algorithm still (information-theoretically) requires
a number of offline samples that is exponential in the problem horizon to non-
trivially estimate the value of any given policy. Our results highlight that sample-
efficient offline policy evaluation is not possible unless significantly stronger con-
ditions hold; such conditions include either having low distribution shift (where
the offline data distribution is close to the distribution of the policy tobe evaluated)
or significantly stronger representational conditions (beyond realizability).
1 Introduction
Offline methods (also known as off-policy methods or batch methods) are a promising methodol-
ogy to alleviate the sample complexity burden in challenging reinforcement learning (RL) settings,
particularly those where sample efficiency is paramount (Mandel et al., 2014; Gottesman et al.,
2018; Wang et al., 2018; Yu et al., 2019). Off-policy methods are often applied together with func-
tion approximation schemes; such methods take sample transition data and reward values as inputs,
and approximate the value of a target policy or the value function of the optimal policy. Indeed,
many practical deep RL algorithms find their prototypes in the literature of offline RL. For exam-
ple, when running on off-policy data (sometimes termed as “experience replay”), deep Q-networks
(DQN) (Mnih et al., 2015) can be viewed as an analog of Fitted Q-Iteration (Gordon, 1999) with
neural networks being the function approximators. More recently, there are an increasing number
of both model-free (Laroche et al., 2019; Fujimoto et al., 2019; Jaques et al., 2020; Kumar et al.,
2019; Agarwal et al., 2020) and model-based (Ross & Bagnell, 2012; Kidambi et al., 2020) offline
RL methods, with steady improvements in performance (Fujimoto et al., 2019; Kumar et al., 2019;
Wu et al., 2020; Kidambi et al., 2020).
However, despite the importance of these methods, the extent to which data reuse is possible, espe-
cially when off-policy methods are combined with function approximation, is not well understood.
For example, deep Q-network requires millions of samples to solve certain Atari games (Mnih et al.,
2015). Also important is that in some safety-critical settings, we seek guarantees when offline-
1
Published as a conference paper at ICLR 2021
trained policies can be effective (Thomas, 2014; Thomas et al., 2019). A basic question here is
that if there are fundamental statistical limits on such methods, where sample-efficient offline RL is
simply not possible without further restrictions on the problem.
In the context of supervised learning, it is well-known that empirical risk minimization is sample-
efficient if the hypothesis class has bounded complexity. For example, suppose the agent is given
a d-dimensional feature extractor, and the ground truth labeling function is a (realizable) linear
function with respect to the feature mapping. Here, it is well-known that a polynomial number
of samples in d suffice for a given target accuracy. Furthermore, in this realizable case, provided
the training data has a good feature coverage, then we will have good accuracy against any test
distribution.1
In the more challenging offline RL setting, it is unclear if sample-efficient methods are possible,
even under analogous assumptions. This is our motivation to consider the following question:
What are the statistical limits for offline RL with linear function approximation?
Here, one may hope that value estimation for a given policy is possible in the offline RL setting
under the analogous set of assumptions that enable sample-efficient supervised learning, i.e., 1)
(realizability) the features can perfectly represent the value functions and 2) (good coverage) the
feature covariance matrix of our off-policy data has lower bounded eigenvalues.
The extant body of provable methods on offline RL either make representational assumptions that are
far stronger than realizability or assume distribution shift conditions that are far stronger than having
coverage with regards to the spectrum of the feature covariance matrix of the data distribution. For
example, Szepesvari & Munos (2005) analyze offline RL methods by assuming a representational
condition where the features satisfy (approximate) closedness under Bellman updates, which is a far
stronger representation condition than realizability. Recently, Xie & Jiang (2020a) propose a offline
RL algorithm that only requires realizability as the representation condition. However, the algorithm
in (Xie & Jiang, 2020a) requires a more stringent data distribution condition. Whether it is possible
to design a sample-efficient offline RL method under the realizability assumption and a reasonable
data coverage assumption — an open problem in (Chen & Jiang, 2019) — is the focus of this work.
Our Contributions. Perhaps surprisingly, our main result shows that, under only the above two
assumptions, it is information-theoretically not possible to design a sample-efficient algorithm to
non-trivially estimate the value of a given policy. The following theorem is an informal version of
the result in Section 4.
Theorem 1.1 (Informal). In the offline RL setting, suppose the data distributions have (polynomi-
ally) lower bounded eigenvalues, and the Q-functions of every policy are linear with respect to a
given feature mapping. Any algorithm requires an exponential number of samples in the horizon
H to output a non-trivially accurate estimate of the value of any given policy π, with constant
probability.
This hardness result states that even if the Q-functions of all polices are linear with respect to the
given feature mapping, we still require an exponential number of samples to evaluate any given
policy. Note that this representation condition is significantly stronger than assuming realizability
with regards to only a single target policy; it assumes realizability for all policies. Regardless, even
under this stronger representation condition, it is hard to evaluate any policy, as specified in our
hardness result.
This result also formalizes a key issue in offline reinforcement learning with function approxima-
tion: geometric error amplification. To better illustrate the issue, in Section 5, we analyze the clas-
sical Least-Squares Policy Evaluation (LSPE) algorithm under the realizability assumption, which
demonstrates how the error propagates as the algorithm proceeds. Here, our analysis shows that,
if we only rely on the realizability assumption, then a far more stringent condition is required for
sample-efficient offline policy evaluation: the off-policy data distribution must be quite close to the
distribution induced by the policy to be evaluated.
1Specifically, if the features have a uniformly bounded norm and if the minimum eigenvalue of the feature
covariance matrix of our data is bounded away from 0, say by 1/poly(d), then we have good accuracy on any
test distribution. See Assumption 2 and the comments thereafter.
2
Published as a conference paper at ICLR 2021
Our results highlight that sample-efficient offline RL is simply not possible unless either the distri-
bution shift condition is sufficiently mild or we have stronger representation conditions that go well
beyond realizability. See Section 5 for more details.
Furthermore, our hardness result implies an exponential separation on the sample complexity be-
tween offline RL and supervised learning, since supervised learning (which is equivalent to offline
RL with H = 1) is possible with polynomial number of samples under the same set of assumptions.
A few additional points are worth emphasizing with regards to our lower bound construction:
•	Our results imply that Least-Squares Policy Evaluation (LSPE, i.e., using Bellman backups with
linear regression) will fail. Interestingly, while LSPE will provide an unbiased estimator, our
results imply that it will have exponential variance in the problem horizon.
•	Our construction is simple and does not rely on having a large state or action space: the size of
the state space is only O(d ∙ H) where d is the feature dimension and H is the planning horizon,
and the size of the action space is only is 2. This stands in contrast to other RL lower bounds,
which typically require state spaces that are exponential in the problem horizon (e.g. see (Du
et al., 2020)).
•	We provide two hard instances, one with a sparse reward (and stochastic transitions) and another
with deterministic dynamics (and stochastic rewards). These two hard instances jointly imply that
both the estimation error on reward values and the estimation error on the transition probabilities
could be geometrically amplified in offline RL.
•	Of possibly broader interest is that our hard instances are, to our knowledge, the first concrete
examples showing that geometric error amplification is real in RL problems (even with realizabil-
ity). While this is a known concern in the analysis of RL algorithms, there have been no concrete
examples exhibiting such behavior under only a realizability assumption.
2	Related Work
We now survey prior work on offline RL, largely focusing on theoretical results. We also discuss
results on the error amplification issue in RL. Concurrent to this work, Xie & Jiang (2020a) propose
a offline RL algorithm under the realizability assumption, which requires stronger distribution shift
conditions. We will discuss this work shortly.
Existing Algorithms and Analysis. Offline RL with value function approximation is closely re-
lated to Approximate Dynamic Programming (Bertsekas & Tsitsiklis, 1995). Existing work (Munos,
2003; Szepesvari & Munos, 2005; Antos et al., 2008; Munos & Szepesvari, 2008; Tosatto et al.,
2017; Xie & Jiang, 2020b; Duan & Wang, 2020) that analyze the sample complexity of approximate
dynamic programming-based approaches usually make the following two categories of assumptions:
(i) representation conditions that assume the function class approximates the value functions well
and (ii) distribution shift conditions that assume the given data distribution has sufficient coverage
over the state-action space. As mentioned in the introduction, the desired representation condition
would be realizability, which only assumes the value function of the policy to be evaluated lies in
the function class (for the case of offline policy evaluation) or the optimal value function lies in
the function class (for the case of finding near-optimal policies), and existing works usually make
stronger assumptions. For example, Szepesvari & Munos (2005); Duan & Wang (2020) assume (ap-
proximate) closedness under Bellman updates, which is much stronger than realizability. Whether
it is possible to design a sample-efficient offline RL method under the realizability assumption and
reasonable data coverage assumption, is left as an open problem in (Chen & Jiang, 2019).
To measure the coverage over the state-action space of the given data distribution, existing works
assume the concentrability coefficient (introduced by Munos (2003)) to be bounded. The concen-
trability coefficient, informally speaking, is the largest possible ratio between the probability for a
state-action pair (s, a) to be visited by a policy, and the probability that (s, a) appears on the data
distribution. Since we work with linear function approximation in this work, we measure the distri-
bution shift in terms of the spectrum of the feature covariance matrices (see Assumption 2), which
is a well-known sufficient condition in the context of supervised learning and is much more natural
for the case of linear function approximation.
3
Published as a conference paper at ICLR 2021
Concurrent to this work, Xie & Jiang (2020a) propose an algorithm that works under the realizabil-
ity assumption instead of other stronger representation conditions used in prior work. However, the
algorithm in (Xie & Jiang, 2020a) requires a much stronger data distribution condition which as-
sumes a stringent version of concentrability coefficient introduced by (Munos, 2003) to be bounded.
In contrast, in this work we measure the distribution shift in terms of the spectrum of the feature
covariance matrix of the data distribution, which is more natural than the concentrability coefficient
for the case of linear function approximation.
Recently, there has been great interest in approaching offline policy evaluation (Precup, 2000) via
importance sampling. For recent work on this topic, see (Dudik et al., 2011; Mandel et al., 2014;
Thomas et al., 2015; Li et al., 2015; Jiang & Li, 2016; Thomas & Brunskill, 2016; Guo et al.,
2017; Wang et al., 2017; Liu et al., 2018; Farajtabar et al., 2018; Xie et al., 2019; Kallus & Ue-
hara, 2019; Liu et al., 2019; Uehara & Jiang, 2019; Kallus & Uehara, 2020; Jiang & Huang, 2020;
Feng et al., 2020). Offline policy evaluation with importance sampling incurs exponential variance
in the planning horizon when the behavior policy is significantly different from the policy to be
evaluated. Bypassing such exponential dependency requires non-trivial function approximation as-
sumptions (Jiang & Huang, 2020; Feng et al., 2020; Liu et al., 2018). Finally, Kidambi et al. (2020)
provide a model-based offline RL algorithm, with a theoretical analysis based on hitting times.
Hardness Results. Algorithm-specific hardness results have been known for a long time in the
literature of Approximate Dynamic Programming. See Chapter 4 in (Van Roy, 1994) and also (Gor-
don, 1995; Tsitsiklis & Van Roy, 1996). These works demonstrate that certain approximate dynamic
programming-based methods will diverge on hard cases. However, such hardness results only hold
for a restricted class of algorithms, and to demonstrate the fundamental difficulty of offline RL, it is
more desirable to obtain information-theoretic lower bounds as initiated by Chen & Jiang (2019).
Existing (information-theoretic) exponential lower bounds (Krishnamurthy et al., 2016; Sun et al.,
2017; Chen & Jiang, 2019) usually construct unstructured MDPs with an exponentially large state
space. Du et al. (2020) prove an exponential lower bound under the assumption that the optimal Q-
function is approximately linear. The condition that the optimal Q-function is only approximately
linear is crucial for the hardness result in Du et al. (2020). The techniques in (Du et al., 2020) are
later generalized to other settings (Kumar et al., 2020; Wang et al., 2020; Mou et al., 2020).
Error Amplification In RL. Error amplification induced by distribution shift and long planning
horizon is a known issue in the theoretical analysis of RL algorithms. See (Gordon, 1995; 1996;
Munos & Moore, 1999; Ormoneit & Sen, 2002; Kakade, 2003; Zanette et al., 2019) for papers on
this topic and additional assumptions that mitigate this issue. Error amplification in offline RL is
also observed in empirical works (see e.g. (Fujimoto et al., 2019)). In this work, we provide the first
information-theoretic lower bound showing that geometric error amplification is real in offline RL.
3	The Offline Policy Evaluation Problem
Throughout this paper, for a given integer H, we use [H] to denote the set {1, 2, . . . , H}.
Episodic Reinforcement Learning. Let M = (S, A, P, R, H) be a Markov Decision Process
(MDP) where S is the state space, A is the action space, P : S × A → ∆ (S) is the transition
operator which takes a state-action pair and returns a distribution over states, R : S × A → ∆ (R)
is the reward distribution, H ∈ Z+ is the planning horizon. For simplicity, we assume a fixed initial
state s1 ∈ S. A (stochastic) policy π : S → ∆ (A) chooses an action a randomly based on the
current state s. The policy π induces a (random) trajectory s1, a1, r1, s2, a2, r2, . . . , sH, aH, rH,
where aι 〜∏1(s1), ri 〜R(sι,αι), s2 〜P(s1,a1), a2 〜∏2(s2), etc. To streamline our analysis,
for each h ∈ [H], we use Sh ⊆ S to denote the set of states at level h, and we assume Sh do not
intersect with each other. We assume, almost surely, that rh ∈ [-1, 1] for all h ∈ [H].
Value Functions. Given a policy	π,	h ∈	[H]	and (s, a)	∈	Sh	×	A,	define	Qπh (s, a)
E PhH0=h
define V π
rh0 | sh = s, ah = a, π and Vhπ(s) = E PhH0=h rh0 | sh
V1π(s1) to be the value of π from the fixed initial state s1.
s,π
For a policy π , we
4
Published as a conference paper at ICLR 2021
Linear Function Approximation. When applying linear function approximation schemes, it is
commonly assumed that the agent is given a feature extractor φ : S × A → Rd which can either be
hand-crafted or a pre-trained neural network that transforms a state-action pair to a d-dimensional
embedding, and the Q-functions can be predicted by linear functions of the features. In this paper,
we are interested in the following realizability assumption.
Assumption 1 (Realizable Linear Function Approximation). For every policy π : S → ∆(A), there
exists θ1π, . . . θHπ ∈ Rd such that for all (s, a) ∈ S × A and h ∈ [H], Qhπ (s, a) = (θhπ)> φ(s, a).
Note that our assumption is much stronger than assuming realizability with regards to a single policy
π (say the policy that we wish to evaluate); our assumption imposes realizability for all policies.
Offline Reinforcement Learning. This paper is concerned with the offline RL setting. In this
setting, the agent does not have direct access to the MDP and instead is given access to data dis-
tributions {μh}H=ι where for each h ∈ [H], μh ∈ ∆(Sh X A). The inputs of the agent are H
datasets {Dh}hH=1, and for each h ∈ [H], Dh consists i.i.d. samples of the form (s, a, r, s0) ∈
Sh × A× R × Sh+ι tuples, where (s, a)〜μh, r 〜r(s, a), s0 〜P (s, a).
In this paper, we focus on the offline policy evaluation problem with linear function approximation:
given a policy π : S → ∆ (A) and a feature extractor φ : S × A → Rd, the goal is to output an
accurate estimate of the value of π (i.e., Vπ) approximately, using the collected datasets {Dh}hH=1,
with as few samples as possible.
Notation. For a vector x ∈ Rd, we use kxk2 to denote its `2 norm. For a positive semidefinite
matrix A, we use kAk2 to denote its operator norm, and σmin(A) to denote its smallest eigenvalue.
For two positive Semidefinite matrices A and B, We write A 占 B to denote the LoWner partial
ordering of matrices, i.e, A B if and only ifA - B is positive semidefinite. For a policy π : S →
∆ (A), we use μ∏ to denote the marginal distribution of Sh under π, i.e., μ∏(S) = Pr[sh = S | π].
For a vector X ∈ Rd and a positive semidefinite matrix A ∈ Rd×d, we use IlxkA to denote √χ>Aχ.
4	The Lower Bound: Realizab ility and C overage are Insufficient
We now present our main hardness result for offline policy evaluation with linear function approxi-
mation. It should be evident that without feature coverage in our dataset, realizability alone is clearly
not sufficient for sample-efficient estimation. Here, we will make the strongest possible assumption,
with regards to the conditioning of the feature covariance matrix.
Assumption 2 (Feature Coverage). For all (S, a) ∈ S × A, assume our feature map is bounded
such that kφ(s, a)k2 ≤ L Furthermore, suppose for each h ∈ [H ], the data distributions μh satisfy
thefollowing minimum eigenvalue condition: σmin(E(s,0)〜*九[φ(s,a)φ(s,a)>]) = 1/d.2
Clearly, for the case where H = 1, the realizability assumption (Assumption 1), and feature cov-
erage assumption (Assumption 2) imply that the ordinary least squares estimator will accurately
estimate θ1.3 Our main result now shows that these assumptions are not sufficient for offline policy
evaluation for long horizon problems.
Theorem 4.1. Suppose Assumption 2 holds. Fix an algorithm that takes as input both a policy and
a feature mapping. There exists a (deterministic) MDP satisfying Assumption 1, such that for any
policy π : S → ∆(A), the algorithm requires Ω((d∕2)H) samples to output the value of π UP to
constant additive approximation error with probability at least 0.9.
Although we focus on offline policy evaluation in this work, our hardness result also holds for
finding near-optimal policies under Assumption 1 in the offline RL setting. Below we give a simple
reduction. At the initial state, if the agent chooses action a1, then the agent receives a fixed reward
value (say 0.5) and terminates. If the agent chooses action a2, then the agent transits to our hard
2Note that 1/d is the largest possible minimum eigenvalue due to that, for any data distribution μh,
σmin(E(s,a)〜eh[φ(s,a)φ(s,a)>]) ≤ 1/d since ∣∣φ(s,a)k2 ≤ 1 forall (s,a) ∈S×A.
32
3For H = 1, the ordinary least squares estimator will satisfy that ∣θ1 - θOLS ∣22 ≤ O(d/n) with high
probability. See e.g. (Hsu et al., 2012b).
5
Published as a conference paper at ICLR 2021
instance. In order to find a policy with suboptimality at most 0.5, the agent must evaluate the value
of the optimal policy in our hard instance up to an error of 0.5, and hence the hardness result holds.
Remark 1 (The sparse reward case). As stated, the theorem uses a deterministic MDP (with stochas-
tic rewards). See Appendix C for another hard case where the transition is stochastic and the reward
is deterministic and sparse (only occurring at two states at h = H).
Remark 2 (Least-Squares Policy Evaluation (LSPE) has exponential variance). For offline policy
evaluation with linear function approximation, the most naive algorithm here would be LSPE, i.e.,
using ordinary least squares (OLS) to estimate θπ , starting at level h = H and then proceeding
backwards to level h = 1, using the plug-in estimator from the previous level. Here, LSPE will pro-
vide an unbiased estimate (provided the feature covariance matrices are full rank, which will occur
with high probability). As a direct corollary, the above theorem implies that LSPE has exponential
variance in H. See Section 5 for a more detailed discussion on LSPE. More generally, our theorem
implies that there is no estimator that can avoid such exponential dependence in the offline setting.
Remark 3 (Least-Squares Value Iteration (LSVI) versus Least-Squares Policy Iteration (LSPI)). In
the offline setting, under Assumptions 1 and 2, in order to find a near-optimal policy, the most naive
algorithm would be LSVL i.e., using ordinary least squares (OLS) to estimate θ*, starting at level
h = H and then proceeding backwards to level h = 1, using the plug-in estimator from the previous
level and the bellman operator. The above theorem implies that LSVI will require an exponential
number of samples to find a near-optimal policy. On the other hand, if the regression targets are
collected by using rollouts (i.e. on-policy sampling) as in LSPI (Lagoudakis & Parr, 2003), then a
polynomial number of samples suffice. See Section D in (Du et al., 2020) for an analysis. Thus,
Theorem 4.1 implies an exponential separation on the sample complexity between LSVI and LSPI.
Of course, LSPI requires adaptive data samples and thus does not work in the offline setting.
One may wonder if Theorem 4.1 still holds when the data distributions {μh}H=∖ are induced by
a policy. In Appendix C, we prove another exponential sample complexity lower bound under the
additional assumption that the data distributions are induced by a fixed policy π. However, under
such an assumption, it is impossible to prove a hardness result as strong as Theorem 4.1 (which
shows that evaluating any policy is hard), since one can at least evaluate the policy π that induces
the data distributions. Nevertheless, we are able to prove the hardness of offline policy evaluation,
under a weaker version of Assumption 1. See Appendix C for more details.
In the remaining part of this section, we give the hard instance construction and the proof of Theo-
rem 4.1. We use d the denote the feature dimension, and we assume d is even for simplicity. We use
d to denote d/2 for convenience. We also provide an illustration of the construction in Figure 1.
State Space, Action Space and Transition Operator. The action space A = {a1, a2}. For each
^-r-ι ^	.	^	^	^
h ∈ [H], Sh contains d+ 1 states sh, sh,...,sh and Sd+ . For each h ∈ [H - 1], for each
d + d	d+1
c ∈ {1, 2, . . . , d+ 1}, we have P(sch, a1) = sdh++11 and P(sch, a1) = sch+1.
Reward Distributions. Let 0 ≤ ro ≤ drH/2 be a parameter to be determined. For each (h, C) ∈
[H - 1] X [d] and a ∈ A, we set R(Sh, a) = 0 and R(sdh+1, a) = r0 ∙ (d1/2 - 1) ∙ d(H-h)/2. For
c	1 with probability (1 + r0)/2
the last level, for each c ∈ [d] and a ∈ A, we set R(scH , a) =
H	-1 with probability (1 - r0)/2
d,+ 1	$1 /2
so that E[R(sH, a)] = r0. Moreover, for all actions a ∈ A, R(Sd++1,a) = r0 ∙ d1/2.
Feature Mapping. Let e1 , e2 , . . . , ed be a set of orthonormal vectors in Rd. Here, one possible
choice is to set e1 , e2, . . . , ed to be the standard basis vectors. For each (h, c) ∈ [H] × [d], we set
Φ(sh, aι) = e°,Φ(sh, a2) = ec+d, and φ(sh+1,a) = pc∈t^ ejd1∕2 for all a ∈ A.
Verifying Assumption 1. The following lemma shows that Assumption 1 holds for our construc-
tion. The formal proof can be found in Appendix A.
Lemma 4.2. For every policy π : S → ∆(A), for each h ∈ [H], for all (S, a) ∈ Sh × A, we have
Qπh (S, a) = (θhπ)> φ(S, a) for some θhπ ∈ Rd.
6
Published as a conference paper at ICLR 2021
d/2. States on the top are those in
Figure 1: An illustration of the hard instance. Recall that d
the first level (h = 1), while states at the bottom are those in the last level (h = H). Solid line
(with arrow) corresponds to transitions associated with action a1, while dotted line (with arrow)
corresponds to transitions associated with action a2. For each level h ∈ [H], reward values and
Q-values associated with s，sh,..., Sh are marked on the left, while reward values and Q-values
associated with s∕+1 are mark on the right. Rewards and transitions are all deterministic, except
^	^
for the reward distributions associated with SH, SH,..., SH. We mark the expectation of the reward
value when it is stochastic. For each level h ∈ [H], for the data distribution μh, the state is chosen
uniformly at random from those states in the dashed rectangle, i.e., {s}, Sh,..., sdι}, while the action
^
is chosen uniformly at random from {α1, a2}. Suppose the initial state is sd+ . When ro = 0, the
value of the policy is 0. When ro = d-H/2, the value of the policy is r0 ∙ 2H/2 = 1.
The Data Distributions. For each level h ∈ [H], the data distribution μh is a uniform distribution
2 2 1	、/1	、/2	、/2	d ∖ d d	,ι,∕d+1
over {(sh, α1), (Sh, a2), (sh, a1), (sh, a2),..., (Sh a1), (sh, a2)}. Notice that (s∕+1 ,a) is not in the
support of μh for all a ∈ A. It can be seen that, E(s,α)〜*九[φ(s, a)φ(s, a)τ] = d Pd=1 ece> = ɪI.
The Lower Bound. We show that it is information-theoretically hard for any algorithm to distin-
guish the case ro = 0 and ro = 2-H/2. We fix the initial state tobe sd+1, and consider any policy π.
When ro = 0, all reward values will be zero, and thus the value of π is zero. On the other hand, when
ro = 2-H/2, the value of π would be ro ∙ dH/2 = 1. Thus, if the algorithm approximates the value
of the policy up to an error of 1/2, then it must distinguish the case that ro = 0 and ro =才-H/2.
We first notice that for the case ro = 0 and ro = 2-H/2, the data distributions {μh}H=1, the fea-
ture mapping φ : S ×A → Rd, the policy π to be evaluated and the transition operator P are the
same. Thus, in order to distinguish the case ro = 0 and ro = 2-H/2, the only way is to query the
reward distribution by using sampling taken from the data distributions. For all state-action pairs
(s, a) in the support of the data distributions of the first H - 1 levels, the reward distributions will
be identical. This is because for all S ∈ Sh \ {sg+1} and a ∈ A, we have R(s,a) = 0. For
the case ro = 0 and ro = d-H/2, for all state-action pairs (s, a) in the support of the data dis-
+ x +∙	1 -	1 d/ ʌ 11 with probability (1 + ro)∕2 Thf + 孑 +∙	∙ ɪ.
tribution of the last level, R(s, a) = < T .,	,,…二	、(.Therefore, to distinguish
-1 with probability (1 - ro)/2
7
Published as a conference paper at ICLR 2021
Algorithm 1 Least-Squares Policy Evaluation
1:	Input: policy ∏ to be evaluated, number of samples N, regularization parameter λ > 0
2:	Let Qh+i(∙, ∙)=0 and VH+ι(∙) = 0
3:	forh = H,H - 1, . . ., 1 do
4:	Take samples (sh,ah)〜μκ, rh 〜r(sh,ah) and Sh 〜P(Sh, ah) for each i ∈ [N]
5:	Let λ h = Pi∈[N ]φ(SL ah)φ(sh, ah)> + λI
6:	Let θh = Λ-1 (P3 φ(sh, ah) ∙ (rh + ‰ι(sh)))
7:	Let Qh(∙, ∙) = φ(∙, ∙)>θh and Vh(∙) = Q(∙,∏(∙))
the case that ro = 0 and ro = 才-H/2, the agent needs to distinguish two reward distributions
1 with probability 1/2
r1	-1 with probability 1/2 an r2
11	with probability (1 + 2-H/2)/2	.
ɪ-l with probability (1 - 2-H/2)/2 . 0w we in
voke Lemma B.1 in Section B by setting ε = 2-H/2/2 and δ = 0.9. By Lemma B.1, in order to
distinguish r1 and r with probability at least 0.9, any algorithm requires Ω(dH) samples.
Remark 4. The key in our construction is the state Sdh+1 in each level, whose feature vector is
defined to be Pc∈^ ej才 1/2. In each level, sdh+1 amplifies the Q-value by a 蕾/2 factor, due to the
linearity of the Q-function. After all the H levels, the value will be amplified by a dH/2 factor. Since
z^∣1.
Sdh+1 is not in the support of the data distribution, the only way to estimate the value of the policy is
to estimate the expected reward value in the last level. Our construction forces the estimation error
of the last level to be amplified exponentially and thus implies an exponential lower bound.
5	Upper Bounds: Low Distribution Shift or Policy Completeness
are S ufficient
In order to illustrate the error amplification issue and discuss conditions that permit sample-efficient
offline RL, in this section, we analyze Least-Squares Policy Evaluation when applied to the offline
policy evaluation problem under the realizability assumption. The algorithm is presented in Algo-
rithm 1. For simplicity here we assume the policy π to be evaluated is deterministic.
Notation. For each h ∈ [H], define Λh = E(s,a)〜*h[φ(s,a)φ(s,a)>] to be the feature covariance
matrix at level h. Foreach h ∈ [H-1], define Λh+1 = E(s,a)〜μh,s〜P(∙∣s,a) [φ(s, π(s))φ(s, π(s))>]
to be the feature covariance matrix of the one-step lookahead distribution at level h. Moreover, define
Λ1 = φ(s1, ∏(s1))φ(s1,∏(s1))>. We define Φh to be a N X d matrix, whose i-th row is φ(sh, ah),
and define Φh+1 to be another N × d matrix whose i-th row is φ(sh, ∏(Sh)). For each h ∈ [H] and
i ∈ [N], define ξ* ih = rih + V(Sh) — Q(Sh, ah). We use ξh to denote a vector whose i-th entry is ξih.
Now we present a general lemma that characterizes the estimation error of Algorithm 1 by an equal-
ity. The proof can be found in Appendix D. Later, we apply this general lemma to special cases.
Lemma 5.1. Suppose λ > 0 in Algorithm 1, and for the given policy π, there exists θ1, θ2, . . . , θd ∈
Rd such that for each h ∈ [H], Qπh (S, a) = φ(S, a)> θh for all (S, a) ∈ Sh × A. Then we have
H	2
(Qn(S1,π(s1)) - Q(s1,π(s1)))2= XΛ-1φ>Φ2Λ-1Φ> …(Λ-1Φ>ξh- λΛ-1θh)	. (1)
h=1	Λι
Now we consider two special cases where the estimation error in Equation (1) can be upper bounded.
Low Distribution Shift. The first special we focus on is the case where the distribution shift
between the data distributions and the distribution induced by the policy to be evaluated is low. To
measure the distribution shift formally, our main assumption is as follows.
Assumption 3. We assume that for each h ∈ [H ] ,there exists Ch ≥ 1 such that Λh W ChAh/.
8
Published as a conference paper at ICLR 2021
Remark 5. For each h ∈ [H], if σmin(Λh)占 击I for some Ch ≥ 1, We have Λh W I W ChΛh.
Therefore, Assumption 3 can be replaced with the assumption that ChΛh I. However, we stick
to the original version of Assumption 3 as it gives a tighter characterization of the distribution shift.
NoW We state the theoretical guarantee of Algorithm 1. The proof can be found in Appendix D.
Theorem 5.2. Suppose for the given policy π, there exists θ1,θ2,...,θd ∈ Rd such that for
each h ∈	[H],	Qh(s,a)	=	φ(s, a)>θh for all	(s, a)	∈	Sh	X A and	∣∣θh∣∣2	≤	HVd.4	Let
λ = CHyJdlog(dH∕δ)N for some C > 0. With probability at least 1 一 δ, for some c > 0,
(Qπ (s1,π(s1)) 一 Qι(sι,∏(sι)))2 ≤ C ∙ QH=ι Ch∙ dH5 ∙ Pd log(dH∕δ)∕N.
Remark 6. The factor QhH=1 Ch in Theorem 5.2 implies that the estimation error Will be amplified
geometrically. NoW We discuss hoW the error is amplified When running Algorithm 1 on the instance
in Section 4 to better illustrate the issue. If We run Algorithm 1 on the hard instance in Section 4,
when h = H, the estimation error on V(SH) would be roughly NT/2 for each C ∈ [d]. When using
the linear predictor at level H to predict the value of SH, the error will be amplified by 缜/2. When
h = H 一 1, the dataset contains only SH-I for C ∈ [d], and the estimation error on the value of SH-I
will be the same as that of SH, which is roughly (d∕N)1/2. Again, the estimation error on the value
of SH-I will be (d2/N)1/2 when using the linear predictor at level H 一 1. The error will eventually
be amplified by a factor of dH/2, which corresponds to the factor QH=I Ch in Theorem 5.2.
Policy Completeness. In offline RL, another representation condition is closedness under Bellman
update (Szepesvari & Munos, 2005; Duan & Wang, 2020), which is stronger than realizability. In
the context of offline policy evaluation, we have the following policy completeness assumption.
Assumption 4. For the given policy π, for any h > 1 and θh ∈ Rd, there exists θ0 ∈ Rd such that
for any (S, a) ∈ Sh-1 × A, E[R(S, a)] + Ps0∈Sh P(S0 | S, a)φ(S0, π(S0))>θh = φ(S, a)>θ0.
Under Assumption 4 and the assumption that σmin (Λh) ≥ λ0 for all h ∈ [H] for some λ0 > 0,
Duan & Wang (2020) have shown that for Algorithm 1, by taking N = poly(H, d, 1∕ε, 1∕λ0), we
have (Q1π(S1, π(S1)) 一 Q1(S1, π(S1)))2 ≤ ε. We refer interested readers to (Duan & Wang, 2020).
We remark that the above analysis again implies that geometric error amplification is a real issue in
offline RL, and sample-efficient offline RL is impossible unless the distribution shift is sufficiently
low, i.e., QhH=1 Ch is bounded, or strong representation condition (e.g. policy completeness) holds.
6	Conclusion
While the extant body of provable results in the literature largely focus on sufficient conditions for
sample-efficient offline RL, this work focuses on obtaining a better understanding of the necessary
conditions, where we seek to understand to what extent mild assumptions can imply sample-efficient
offline RL. This work shows that for off-policy evaluation, even ifwe are given a representation that
can perfectly represent the value function of the given policy and the data distribution has good
coverage over the features, any provable algorithm still requires an exponential number of samples
to non-trivially approximate the value of the given policy. These results highlight that provable
sample-efficient offline RL is simply not possible unless either the distribution shift condition is
sufficiently mild or we have stronger representation conditions that go well beyond realizability.
Acknowledgments
The authors would like to thank Akshay Krishnamurthy, Alekh Agarwal, Wen Sun, and Nan Jiang
for numerous helpful discussion. Sham M. Kakade gratefully acknowledges funding from the
ONR award N00014-18-1-2247, and NSF Awards CCF-1703574 and CCF-1740551. Ruosong
Wang was supported in part by the NSF IIS1763562, US Army W911NF1920104, and ONR Grant
N000141812861. Research performed while Ruosong Wang was an intern at Microsoft Research. * 1
4Without loss of generality, we can work in a coordinate system such that ∣∣θh∣∣2 ≤ HVd and ∣∣φ(s,a)k2 ≤
1 for all (s, a) ∈ S × A. This follows due to John’s theorem (e.g. see (Ball, 1997; Bubeck et al., 2012)).
9
Published as a conference paper at ICLR 2021
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, 2020.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
71(1):89-129, 2008.
Keith Ball. An elementary introduction to modern convex geometry. Flavors ofgeometry, 31:1-58,
1997.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Pro-
ceedings of 1995 34th IEEE Conference on Decision and Control, volume 1, pp. 560-564. IEEE,
1995.
Sebastien Bubeck, Nicolo Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies for
online linear optimization with bandit feedback. In COLT 2012 - The 25th Annual Conference on
Learning Theory, June 25-27, 2012, Edinburgh, Scotland, volume 23 of JMLR Proceedings, pp.
41.1-41.14, 2012.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In International Conference on Machine Learning, pp. 1042-1051, 2019.
Herman Chernoff. Sequential analysis and optimal design. SIAM, 1972.
Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation suf-
ficient for sample efficient reinforcement learning? In International Conference on Learning
Representations, 2020.
Yaqi Duan and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approx-
imation. arXiv preprint arXiv:2002.09516, 2020.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, pp. 1097-1104, 2011.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In International Conference on Machine Learning, pp. 1447-1456, 2018.
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation with
kernel bellman statistics. arXiv preprint arXiv:2008.06668, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062, 2019.
Geoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning
Proceedings 1995, pp. 261-268. Elsevier, 1995.
Geoffrey J Gordon. Stable fitted reinforcement learning. In Advances in neural information pro-
cessing systems, pp. 1052-1058, 1996.
Geoffrey J Gordon. Approximate solutions to markov decision processes. Technical report,
CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE, 1999.
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan,
Linying Zhang, Yi Ding, David Wihl, Xuefeng Peng, Jiayu Yao, Isaac Lage, Christopher Mosch,
Li wei H. Lehman, Matthieu Komorowski, Matthieu Komorowski, Aldo Faisal, Leo Anthony
Celi, David Sontag, and Finale Doshi-Velez. Evaluating reinforcement learning algorithms in
observational health settings, 2018.
10
Published as a conference paper at ICLR 2021
Zhaohan Guo, Philip S Thomas, and Emma Brunskill. Using options and covariance testing for long
horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems, pp.
2492-2501, 2017.
Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability, 17, 2012a.
Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pp. 9-1, 2012b.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning
of human preferences in dialog, 2020. URL https://openreview.net/forum?id=
rJl5rRVFvH.
Nan Jiang and Jiawei Huang. Minimax confidence interval for off-policy evaluation and policy
optimization. arXiv preprint arXiv:2002.02081, 2020.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In
International Conference on Machine Learning, pp. 652-661. PMLR, 2016.
Sham Machandranath Kakade. On the sample complexity of reinforcement learning. PhD thesis,
University of London London, England, 2003.
Nathan Kallus and Masatoshi Uehara. Efficiently breaking the curse of horizon in off-policy evalu-
ation with double reinforcement learning. arXiv preprint arXiv:1909.05850, 2019.
Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evalu-
ation in markov decision processes. Journal of Machine Learning Research, 21(167):1-63, 2020.
Emilie Kaufmann, Olivier Cappe, and AUrelien Garivier. On the complexity of best-arm identifi-
cation in multi-armed bandit models. The Journal of Machine Learning Research, 17(1):1-42,
2016.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel : Model-
based offline reinforcement learning, 2020.
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich
observations. In Advances in Neural Information Processing Systems, pp. 1840-1848, 2016.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. neural information processing systems, pp. 11761-
11771, 2019.
Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement
learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107-1149, 2003.
Romain Laroche, Paul Trichelair, and Remi Tachet des Combes. Safe policy improvement with
baseline bootstrapping. In Proceedings of the 36th International Conference on Machine Learning
(ICML), 2019.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In
Artificial Intelligence and Statistics, pp. 608-616, 2015.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356-
5366, 2018.
Yao Liu, Pierre-Luc Bacon, and Emma Brunskill. Understanding the curse of horizon in off-policy
evaluation via conditional importance sampling. arXiv preprint arXiv:1910.06508, 2019.
11
Published as a conference paper at ICLR 2021
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy
evaluation across representations with applications to educational games. In AAMAS, pp. 1077-
1084, 2014.
Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit
problem. Journal of Machine Learning Research, 5(Jun):623-648, 2004.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Wenlong Mou, Zheng Wen, and Xi Chen. On the sample complexity of reinforcement learning with
policy space generalization. arXiv preprint arXiv:2008.07353, 2020.
Remi Munos. Error bounds for approximate policy iteration. In ICML, volume 3, pp. 560-567,
2003.
Remi Munos and Andrew W Moore. Barycentric interpolators for continuous space and time re-
inforcement learning. In Advances in neural information processing systems, pp. 1024-1030,
1999.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(May):815-857, 2008.
Dirk Ormoneit and Saunak Sen. Kernel-based reinforcement learning. Machine learning, 49(2-3):
161-178, 2002.
Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department
Faculty Publication Series, pp. 80, 2000.
Stephane Ross and Drew Bagnell. Agnostic system identification for model-based reinforcement
learning. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012,
Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress, 2012.
Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply ag-
grevated: Differentiable imitation learning for sequential prediction. In International Conference
on Machine Learning, pp. 3309-3318, 2017.
Csaba Szepesvari and Remi Munos. Finite time bounds for sampling based fitted value iteration. In
Proceedings of the 22nd international conference on Machine learning, pp. 880-887, 2005.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, pp. 2139-2148, 2016.
Philip S. Thomas. Safe reinforcement learning. PhD thesis, University of Massachusetts, Amherst,
2014.
Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-
policy evaluation. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Philip S. Thomas, Bruno Castro da Silva, Andrew G. Barto, Stephen Giguere, Yuriy Brun, and
Emma Brunskill. Preventing undesirable behavior of intelligent machines. Science, 366(6468):
999-1004, 2019. ISSN 0036-8075. doi: 10.1126/science.aag3311.
Samuele Tosatto, Matteo Pirotta, Carlo d’Eramo, and Marcello Restelli. Boosted fitted q-iteration.
In International Conference on Machine Learning, pp. 3434-3443. PMLR, 2017.
Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and TrendsR in
Machine Learning, 8(1-2):1-230, 2015.
J Tsitsiklis andB Van Roy. An analysis of temporal-difference learning with function approximation
(technical report lids-p-2322). Laboratory for Information and Decision Systems, 1996.
Masatoshi Uehara and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation.
arXiv preprint arXiv:1910.12809, 2019.
12
Published as a conference paper at ICLR 2021
Benjamin Van Roy. Feature-based methods for large scale dynamic programming. PhD thesis,
Massachusetts Institute of Technology, 1994.
L. Wang, Wei Zhang, Xiaofeng He, and H. Zha. Supervised reinforcement learning with recurrent
neural network for dynamic treatment recommendation. Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, 2018.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. arXiv preprint arXiv:2006.11274, 2020.
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudik. Optimal and adaptive off-policy evaluation
in contextual bandits. In International Conference on Machine Learning, pp. 3589-3597. PMLR,
2017.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning,
2020. URL https://openreview.net/forum?id=BJg9hTNKPH.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. arXiv
preprint arXiv:2008.04990, 2020a.
Tengyang Xie and Nan Jiang. Q? approximation schemes for batch reinforcement learning: A
theoretical comparison. arXiv preprint arXiv:2003.03924, 2020b.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for rein-
forcement learning with marginalized importance sampling. In Advances in Neural Information
Processing Systems, pp. 9668-9678, 2019.
C. Yu, G. Ren, and J. Liu. Deep inverse reinforcement learning for sepsis treatment. In 2019 IEEE
International Conference on Healthcare Informatics (ICHI), pp. 1-3, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Limiting ex-
trapolation in linear approximate value iteration. In Advances in Neural Information Processing
Systems, pp. 5616-5625, 2019.
13
Published as a conference paper at ICLR 2021
A	Proof of Lemma 4.2
>Λ	Z- -I-V T C .	∙ i'	∙ 1 ∙	i' .Λ Γ- .TT	Tl	1 I -	1/7	∖ _ Γ TT	τ^l	Γ Tl
Proof. We first verify Qπ is linear for the first H - 1 levels. For each (h, c) ∈ [H - 1] × [d], we
have
Qh (sh, a1) =R(sh, a1)	+	R(Sh+1,	a1)	+ R(Sh+2, aI)	+ ... +	R(SH	,aI)=	r0	∙ d ”.
Moreover, for all a ∈ A,
Qn(sh+1, a) =R(sh+1,a) + R(sh+1,aι) + 冗阂+2，。1)+ …+ R(s#,ai) = ro ∙ d(H-h+1”2.
Therefore, if we define
ʌ ʌ
d	d
θ∏ = X ro /(H-h"2 ∙ ec + X Qn(Sh,a2) ∙ 5，
c=1	c=1
thenQnh(S,a) = (θhn)>φ(S,a) for all (S, a) ∈ Sh × A.
Now we verify that the Q-function is linear for the last level. Clearly, for all c ∈ [d] and a ∈ A,
QH(SH, a) = ro and QH(sd+1,a) = ro ∙ PL Thus by defining θ% = Pd=ι ro • ec, We have
QnH (S, a) = (θHn )> φ(S, a) for all (S, a) ∈ SH × A.
□
B A Technical Lemma
We need the folloWing lemma in the proof of our hardness results.
Lemma B.1. Let α be a random variable uniformly distributed on {α+, α-}, where α- = 1/2 and
α+ = 1/2 + ε with 0 < ε < 1. Suppose that ξ1, ξ2, . . . , ξm are i.i.d. {+1, -1}-valued random
variables with Pr[ξi = +1] = α for all i ∈ [m]. Let f be a function from {+1, -1}m to {α+, α-}.
Suppose m ≤ C∕ε2 log(1∕δ) forsome fixed constant C. Then
Pr[f(ξ1,ξ2,...,ξm) 6=α] >δ.
To our best knoWledge, Lemma B.1 Was first proved in (Chernoff, 1972) and has enormous ap-
plications in statistical learning theory (see, e.g., Chapter 5 in (Anthony & Bartlett, 2009)) and
bandits (Mannor & Tsitsiklis, 2004).
To prove Lemma B.1, one can first prove that the maximum likelihood estimator (MLE) is optimal
and then show that MLE requires Ω(1∕ε2 log(1∕δ)) samples to correctly output α with probability
1 - δ by anti-concentration. Lemma B.1 can also be proved by using information theory. See, e.g.,
(Kaufmann et al., 2016) for such a proof.
C Another Hard Instance
In this section, we present another hard case under a weaker version of Assumption 1. Here the
transition operator is stochastic and the reward is deterministic and sparse, meaning that the reward
value is non-zero only for the last level. Moreover, the data distributions {μh}H=ι are induced by a
fixed policy πdata . We also illustrate the construction in Figure 2. Throughout this section, we use
d the denote the feature dimension, and we assume d is an even integer for simplicity. We use d to
denote d/2 - 1.
In this section, we adopt the following realizability assumption, which is a weaker version of As-
sumption 1.
Assumption 5 (Realizability). For the policy π : S → ∆(A) to be evaluated, there exists
θ1, . . . θH ∈ Rd such that for all (S, a) ∈ S × A and h ∈ [H],
Qhn (S, a) = θh>φ(S,a).
14
Published as a conference paper at ICLR 2021
--►
ad+i'act+2,…g
P3 I，产Ig) = (1+桐(H-M)/2
P3 I s#1, %) = (1- rοd(H-”2)/2
Q(S 尹Ig)=^(~2)/2
P0+1 I 铲1,3 = (1+ 相(Hf)/2)/2
P(S-+ι I，”，¾) = (1-e(Hf)/2)/2
QW+1g) =r0 加 Hf)/2
P(，+|，H-1g) = (1 + W1/2)/2
P(S- I SH-1g) = (1- r()d1/2)/2
Q(SH-1，%) = r()d1/2
Figure 2: An illustration of the hard instance. Recall that d = d/2 - 1. States on the top are those
in the first level (h = 1), while states at the bottom are those in the last level (h = H). Dotted
line (with arrow) corresponds to transitions associated with actions α1,α2,…,a^, while solid line
(with arrow) corresponds to transitions associated with actions af^+1, a^+2, ∙∙∙,ad. We omit the
transition associated with a1,α2, ∙∙∙,α^ in the figure if all actions give the same transition. For
each level h ∈ [H], Q-values associated with s1, ∙⅛.∙∙, s，, s，s- are marked on the left, while
transition distributions and Q-values associated with s，+1 are marked on the right. Rewards are all
deterministic, and the only two states (SH and SH) with non-zero reward values are marked in black
and grey. Consider the fixed policy that returns ad for all input states. When r0 = 0, the value of the
policy is 0. When p = 2-(H-2)/2, the value of the policy is = p疝H-2)/2 = 1.
State Space, Action Space and Transition Operator. In this hard case, the action space A =
{α1, a2,…,ad} contains d elements. Si contains a single state s1. For each h ≥ 2, Sh contains
(d+3 states s，, s，，...，Sh s，+1, s，and s-.
Let 0 ≤ ro ≤ "H-2)/2 be a parameter to be determined. We first define the transition operator for
the first level. We have
a = ac, c ∈ [d]
a = ad+i
a = a d+2
a ∈ {a^+3,a^+4,...,ad}
Now we define the transition operator when h ∈ {2, 3,...,H - 2}. For each h ∈ {2,3,...,H - 2},
a ∈ A and C ∈ [d], we have P(Sh, a) = s，，；，P(s，，a) = s∖ι and P(s-, a) = s-+ι. For each
h ∈ {2, 3,...,H - 2} and C ∈ [d], we have P (s，+1, aj = s，，；. For all a ∈ {a 3十；，a f^+2, ...,a d },
15
Published as a conference paper at ICLR 2021
we have
with probability (1 + r0 ∙疝H-h"2)∕2
with probability (1 一 r0 ∙疝H-h"2)∕2
Now we define the transition operator for the second last level. For all c ∈ [d] and a ∈ A, we have
P(scH -1, a) =	s+-H
sH
with probability (1 + r0)∕2
with probability (1 一 r0)∕2
For all a ∈ A, We have P(s∏-ι,a) = SH and P(sH_「a) = SH. For each C ∈ [d], We have
P(SH1ι,ac) = SH. For all a ∈ {af^+ι, af^+2,..., a√}, We have
with probability
With probability
Reward Values. In this hard case, all reWard values are deterministic, and reWard values can be
non-zero only for the last level. Formally, We have
R”1-1
S = S+H
S = S-H
otherWise
Feature Mapping. As in the in hard instance in Section 4, let e1, e2, . . . , edbe a set of of orthonor-
mal vectors in Rd. For the initial state, for each c ∈ [d], We have φ(S1, ac) = ec.
NoW We define the feature mapping When h ∈ {2, 3, . . . , H}. For each h ∈ {2, 3, . . . , H}, a ∈ A
and c ∈ [d], φ(sh, a) = ec, φ(s[,a) = e^+ι and φ(sh, a) = ef^+2∙ Moreover, for all actions a ∈ A,
φ(sd+1, a)= fed+2+c	a =	ac,c ∈[d]	.
h ，	[d⅛ (e1	+ e2	+ ... + e^	a ∈	{ad+1, ad+2,...,ad}
Clearly, for all (S, a) ∈ S × A, kφ(S, a)k2 ≤ 1.
Verifying Assumption 5. NoW We consider the deterministic policy π : S → A, Which is defined
to be π(S) = ad for all S ∈ S. We shoW that Assumption 5 holds.
When h = 1, define
ʌ	ʌ
d	d
θι = Xro /(H-3"2 ∙ ec + ed+1 - ed+2 + X「0 ∙才(HH/ ∙ ed+2+c∙
For each h ∈ {2, 3, . . . , H 一 2}, define
ʌ ʌ
d	d
θh = X ro ∙ d(H—…/ ∙ ec + ed+ι - e* + Xr0 ∙加H-h-2"2 ∙ ed+2+c∙
c=1	c=1
For the second last level h = H 一 1, define
ʌ
d
θH-1 = Σ2r0 ∙ ec + ed+1 - ed+2∙
c=1
Finally, for the last level h = H, define
θH = ed+1 - ed+2∙
It can be verified that for each h ∈ [H], Qπh(S, a) = θh>φ(S, a) for all (S, a) ∈ Sh × A.
16
Published as a conference paper at ICLR 2021
The Data Distributions. For the first level, the data distribution μι is defined to be the uniform
distribution over {(sι,ac) | C ∈ [d]}. For each h ≥ 2, the data distribution μh is a uniform
distribution over
^	z^L	z^L	z^L
{(Sh,aI), (Sh,aI),..., (Sh,aI), (S+,aI), (Sh,aI), (Sh+1,aI), (Sh『azb..., (sh+1,ad)}.
Notice that again (Sh+', a) is not in the support of μh for all actions a ∈ {ad+「a^+2,..., ad}∙ It
can be seen that for all h ∈ [H],
>	1d >	1
E(s,a)〜μh[φ(s,a)φ(s,a)>] = dΣSec ec = dI
c=1
Moreover, by defining
	'Uniform(A) aι	S= S∈	S1 {Sch | h ∈ {2,3,...	-	-人-- ,h},c∈ [d]}
πdata(S) =	a1	S∈	{Sh+ | h ∈ {2,3,..	. , H}}
	aι	S∈	{Sh- | h∈ {2,3,..	. , H}}
	、Uniform({aι, a2,.	.,af^})	S ∈	{Sh+1 | h ∈ {2, 3,	...,H}}
We have μh = μ∏data for all h ∈ [H].
The Lower Bound. Now we show that it is information-theoretically hard for any algorithm to
distinguish the case ro = 0 and ro = d-(H-2)/2 in the offline setting by taking samples from the
data distributions {μh}H=∖. Here we consider the above policy ∏ defined above which returns action
ad for all input states. Notice that When r0 = 0, the value of the policy Would be zero. On the other
hand, when r0 = 2-(H-2"2, the value of the policy would be ro ∙才(H-2)/2 = 1. Therefore, if the
algorithm approximates the value of the policy up to an approximation error of 1/2, then it must
distinguish the case that ro = 0 and ro = d-(H-2)/2.
We first notice that for the case ro = 0 and ro = 2-(H-2)/2, the data distributions {μh}H=ι, the
feature mapping φ : S × A → Rd, the policy π to be evaluated and the reward distributions R are
the same. Thus, in order to distinguish the case ro = 0 and ro = d-(H-2)/2, the only way is to
query the transition operator P by using sampling taken from the data distributions.
Now, for all state-action pairs (S, a) in the support of the data distributions of the first H - 2 lev-
els (namely μι, μ2,..., μH-2), the transition operator will be identical. This is because chang-
z^L1	z^L1	^L1
ing ro only changes the transition distributions of (Shh+1, a^+ι), (Sh+1,af^+2),..., (Sh+1,ad), and
such state-actions are not in the support of μh for all h ∈ [H - 2]. Moreover, for any
(s, a) ∈ {sH-i, sH-i, SdH-1ι} XA in the support of μH-ι, P(s, a) will also be identical no mat-
ter ro = 0 or ro =才-(H-2)/2. For those state-action pairs (s, a) in the support of μH-ι with
S ∈/ {SH-i,Sh-i,SH3ι}, we have
S
+
H
P(S, a)
SH

with probability (1 + ro)/2
with probability (1 - ro)/2
Again, this is because (SH+ʌ,a) is not in the support of μH-ι for all a ∈ {af+「af^+2,..., ad}.
Therefore, in order to distinguish the case ro = 0 and ro = 2-(H-2)/2, the agent needs distinguish
two transition distributions
= S+H with probability 1/2
p1	S-H with probability 1/2
and
=ISH with probability (1 + 2-(H-2)/2)/2
p2	(SH with probability (1 - 2-(H-2)/2)/2 .
Again, by Lemma B.1, in order to distinguish p1 and p2 with probability at least 0.9, one needs
Ω(dH-2) samples. Formally, we have the following theorem.
17
Published as a conference paper at ICLR 2021
Theorem C.1. Suppose Assumption 2 holds, and rewards are deterministic and could be none-
zero only for state-action pairs in the last level. Fix an algorithm that takes as input both a policy
and a feature mapping. There exists an MDP satisfying Assumption 5, such that for a fixed policy
π : S → A, the algorithm requires Ω((d∕2 — I)H/2) samples to output the value of π UP to constant
additive approximation error with probability at least 0.9.
D Analysis of Algorithm 1
D.1 Proof of Lemma 5.1
Clearly,
ʌ	Λ -1
θh = Λh
(Xφ(sh,ah) ∙ (rh + Vh+1(sh)))
(Xφ(sh,ah) ∙ (rh + Qh+1(sh,∏国)))
Λ-1
:` , , . :` ` ~1~ ʌ .
ah) ∙ (rh + Φ(Sh,∏国))>θh+ι)
Λ-1
N
ah) ∙ (rh + Φ(Sh, ∏(Sh))>θh+1) + E Φ(sh, ah) ∙ Φ(sh, ∏(sh))>(θh+1 — θh+1)
=1
(X Φ(sh,ah) ∙ (rh + Φ(sh,∏(SJ)>θh+1)) +Λ-1 (X φ(sih,aih) ∙ φ(sh,∏(sh ))>(θh+1 — θh+1)).
For the first term, we have
Λ-1
ʌ -1
=Λ-1
ʌ -1
=Λ-1
ʌ -1
=Λ-1
(X φ(sh, ah) ∙ (rh + φ(sh, ∏(sh))>θh+1)
(X φ(sh,ah) ∙ (rh + Qn (sh,∏(sh))))
(X φ(sh,ah) ∙ (rh + Vπ (Sh)))
(Xφ(sh,ah) ∙ (Qn(sh,ah) + ξh))
N
N
=Λ-1 E φ(sh, ah) ∙ ξh + Λ-1£ φ(sh, ah) ∙ φ(sh, ah)>θh
i=1
N
i=1
=λ-1 fΦ(sh,ah) ∙ξh+λ-1(Φ>Φh)θh
i=1
=Λ -1Φhξh + θh — λΛ -1θh.
Therefore,
ʌ	, Λ -1	Λ -1	.	Λ -1 ~Γ--	,	ʌ .
θ1 — θ1 = (Λ-1Φ1ξ1 — λΛ-1θ1) + Λ-1Φ>Φ2(θ2 — θ2)
=(Λ-1Φ1ξ1 — λΛ-1θ1) + Λ-1Φ>Φ2(Λ-1Φ>ξ2 — λΛ—1θ2)
+ Λ1 1Φ> Φ2Λ 2 1Φ> Φ3(θ3 — θ3)
H
XΛ-1φ>Φ2Λ-1Φ>Φ3 …(Λ-1Φ>ξh - λΛ-1θh).
h=1
18
Published as a conference paper at ICLR 2021
Also note that
(Qn(S1,π(SI))- Q(S1,π(SI)))2 = kθ1 - θ1kΛι.
D.2 Proof of Theorem 5.2
By matrix concentration inequality (Tropp, 2015), we have the following lemma.
Lemma D.1. For each h ∈ [H], with probability 1 一 δ∕(4H), for some universal constant C, we
have
"φ>Φh — Ah	≤ CPdlog(dH∕δ)∕N.
2
and
1 *	=	7
NΦh+ιΦh+ι 一 Λh+ι
≤ CVZd log(dH∕δ)∕N.
2
Therefore, since λ = CHjd log(dH∕δ)N, with probability 1 — δ∕(4H), we have
Λh =Φ>Φh + λI 占 NΛh.
Note that
(Q (s1,∏(s1)) — Q(s1,∏(s1)))2
≤H ∙
A-1Φ>Φ2A—1Φ>Φ3 …(A—1Φ>ξh — λA—1θh)∣∣;
≤2H ∙
A-1Φ>Φ2A—1Φ>Φ3∙∙∙ A -
1Φ>ξh∣∣2ι+X ∣∣a-1Φ>Φ2A—1Φ>Φ3
h=1
∙∙∙λ—1θhBΛι
For each h ∈ [H],
Ii ^ —1而>而 ^ —1而丁而	^ —1下>金Il2
llA1 φ1 φ2A2 φ2 φ3 …Ah φh ξhkΛι
≤kΦιΛ—1 AiA—1Φ>k2 ∙ kΦ2Λ—1Φ>Φ3 …A—1Φ>ξhk2
≤kΛ-1∕2ΛιΛ-1∕2
≤kΛ-1∕2ΛιΛ-1/2
k2 ∙kΦlΛ-1Φ> k2 ∙ kΦ2Λ-1Φ>Φ2 …Λ-1Φ>ξhk2
h-1
k2 ∙ Y (kΦh0 Λ -01Φ>0 k2 ∙kΛ-+2(Φ>0 + ιΦh0 + ι)Λ -0%k2) ∙ kξh kφfcΛ-iφ>.
Φh h Φh
h0=1
Similarly,
kA—1Φ>Φ2A—1Φ>Φ3 …λA—1θhkΛ1
h—1
≤kA-1/2A1A-1/2k2 ∙ Y (kΦh,A —01Φ>0k2 ∙ kA —1+2(Φ>0+1Φh0+1)A —+12k2) ∙ λ2 ∙kθhkΛ-1
h0=1
h—1
≤kA-1/2A1A—1/2k2 ∙ γ (kΦh,A —01Φ>0k2 ∙ kA —1+2(Φ>0+1Φh0+1)A —,由2)∙ λ ∙ H2d.
h0=1
For all h ∈ [H], we have
kΦhΛ-1Φ>k2 ≤ 1
H
and
kA —1∕2(Φ>Φh)A—1/2k2 ≤ kN A -1/2 AhA-1/2k2 + kA—1∕2(Φ>Φh — NAh)A—1/2k2.
Conditioned on the event in Lemma D.1,
ʌ … NT
λh 占 NAh 占 ~^~kh,
Ch
19
Published as a conference paper at ICLR 2021
which implies ∣∣NΛ-1/2AhA-1/2|| ≤ Ch Moreover, conditioned on the event in Lemma D.1,
kΛ-1∕2(Φ>Φh - NΛh)Λ-1∕2∣∣2 ≤ CPdlog(dH∕δ)N∕λ.
Thus,
kΛ-"ΛιΛ-1/2k2 ≤ Ci/N.
and
kΛ-1分(Φ>Φh)Λ-1/2k2 ≤ Ch + CPdlog(dH∕δ)N∕λ.
Finally, by Theorem 1.2 in (HsU et al., 2012a), with probability 1 - δ∕(4H), for some constant C0,
we have
kξhkφfcΛ-1φ> ≤ C0H2dlog(H∕δ).
hh h
Therefore,
I∣Λ-1φ>ΦdΛ-1φ>Φq ・…Λ-1Φ>ξJ2 + l∣Λ-1φ>ΦdΛ-1φ>Φq ・…λΛ-1θJ2
H八1 ±1 *2八2 ±2 ±3	八h 义h ξh∣∣Λ + H八1 ±1 *2八2 ±2 ±3	八八h h∣∣Λ
≤C(C2 + CPdlog(d∕δ)N∕λ) X …X (Ch + CPdlog(d∕δ)N∕λ) × (CIH2dlog(H∕δ) + λH2d)
≤C(C2 + 1∕H) X …X (Ch + 1∕H) X (C0H2dlog(H∕δ) + λH2d)
≤-eC1 X C2 X …X Ch X (C0H2dlog(H∕δ) + CdH3Pdlog(dH∕δ)N).
Let c > 0 be a large enoUgh constant. We now have
Esi [(QΠ(sι,∏(sι)) — Qι(sι,∏(sι)))2] ≤ C • (" Ch) ∙ dH5 ∙ JdlogCH∕δ).
20