Published as a conference paper at ICLR 2021
DDPNOpt: Differential Dynamic Programming
Neural Optimizer
Guan-Horng Liu1 2, Tianrong Chen3, and Evangelos A. Theodorou1 2
1 Center for Machine Learning, 2 School of Aerospace Engineering,
3School of Electrical and Computer Engineering, Georgia Institute of Technology, USA
{ghliu,tianrong.chen,evangelos.theodorou}@gatech.edu
Ab stract
Interpretation of Deep Neural Networks (DNNs) training as an optimal control
problem with nonlinear dynamical systems has received considerable attention
recently, yet the algorithmic development remains relatively limited. In this work,
we make an attempt along this line by reformulating the training procedure from the
trajectory optimization perspective. We first show that most widely-used algorithms
for training DNNs can be linked to the Differential Dynamic Programming (DDP), a
celebrated second-order method rooted in the Approximate Dynamic Programming.
In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDP-
NOpt), for training feedforward and convolution networks. DDPNOpt features
layer-wise feedback policies which improve convergence and reduce sensitivity
to hyper-parameter over existing methods. It outperforms other optimal-control
inspired training methods in both convergence and complexity, and is competitive
against state-of-the-art first and second order methods. We also observe DDPNOpt
has surprising benefit in preventing gradient vanishing. Our work opens up new
avenues for principled algorithmic design built upon the optimal control theory.
1	Introduction
In this work, we consider the following optimal control problem (OCP) in the discrete-time setting:
T-1
min J(U; xo):
φ(xτ) + E't(Xt,Ut)
t=0
s.t. xt+1 = ft(xt, ut) ,
(OCP)
u
where Xt ∈ Rn and Ut ∈ Rm represent the state and control at each time step t. ft(∙, ∙), 't(∙, ∙) and
φ(∙) respectively denote the nonlinear dynamics, intermediate cost and terminal cost functions.OCP
aims to find a control trajectory, U，{ut}T=o1, such that the accumulated cost J over the finite
horizon t ∈ {0,1,…，T} is minimized. Problems with the form of OCP appear in multidisciplinary
areas since it describes a generic multi-stage decision making problem (Gamkrelidze, 2013), and have
gained commensurate interest recently in deep learning (Weinan, 2017; Liu & Theodorou, 2019).
Central to the research along this line is the interpretation of DNNs as discrete-time nonlinear
dynamical systems, where each layer is viewed as a distinct time step (Weinan, 2017). The dynamical
system perspective provides a mathematically-sound explanation for existing DNN models (Lu et al.,
2019). It also leads to new architectures inspired by numerical differential equations and physics (Lu
et al., 2017; Chen et al., 2018; Greydanus et al., 2019). In this vein, one may interpret the training as
the parameter identification (PI) of nonlinear dynamics. However, PI typically involves (i) searching
time-independent parameters (ii) given trajectory measurements at each time step (Voss et al., 2004;
Peifer & Timmer, 2007). Neither setup holds in piratical DNNs training, which instead optimizes
time- (i.e. layer-) varying parameters given the target measurements only at the final stage.
An alternative perspective, which often leads to a richer analysis, is to recast network weights as
control variables. Through this lens, OCP describes w.l.o.g. the training objective composed of layer-
wise loss (e.g. weight decay) and terminal loss (e.g. cross-entropy). This perspective (see Table 1)
has been explored recently to provide theoretical statements for convergence and generalization
(Weinan et al., 2018; Seidman et al., 2020). On the algorithmic side, while OCP has motivated new
1
Published as a conference paper at ICLR 2021
architectures (Benning et al., 2019) and methods for breaking sequential computation (Gunther et al.,
2020; Zhang et al., 2019), OCP-inspired optimizers remain relatively limited, often restricted to either
specific network class (e.g. discrete weight) (Li & Hao, 2018) or small-size dataset (Li et al., 2017).
The aforementioned works are primarily inspired by the Pontryagin Maximum Principle (PMP,
Boltyanskii et al. (1960)), which characterizes the first-order optimality conditions to OCP. Another
parallel methodology which receives little attention is the Approximate Dynamic Programming (ADP,
Bertsekas et al. (1995)). Despite both originate from the optimal control theory, ADP differs from
PMP in that at each time step a locally optimal feedback policy (as a function of state xt) is computed.
These policies, as opposed to the vector update from PMP, are known to enhance the numerical
stability of the optimization process when models admit chain structures (e.g. in DNNs) (Liao &
Shoemaker, 1992; Tassa et al., 2012). Practical ADP algorithms such as the Differential Dynamic
Programming (DDP, Jacobson & Mayne (1970)) appear extensively in modern autonomous systems
for complex trajectory optimization (Tassa et al., 2014; Gu, 2017). However, whether they can be
lifted to large-scale stochastic optimization, as in the DNN training, remains unclear.
In this work, we make a significant advance to-
ward 叩timal-Control-theoretic training algorithms __________Table 1： TerminologymaPPing
inspired by ADP. We first show that most exist- ing first- and second-order optimizers can be de-	Deep Learning		Optimal Control
rived from DDP as special cases. Built upon	J	Total Loss	Trajectory Cost
this intriguing connection, we present a new	Xt	Activation Vector	State Vector
class of optimizer which marries the best of both.	ut	Weight Parameter	Control Vector
The proposed method, DDP Neural Optimizer	f	Layer Propagation	Dynamical System
(DDPNOpt), features layer-wise feedback poli-	φ	End-goal Loss	Terminal Cost
cies, which, as we will show through experiments,	`	Weight Decay	Intermediate Cost
improve convergence and robustness. To enable		—	
efficient training, DDPNOpt adapts key components including (i) curvature adaption from existing
methods, (ii) stabilization techniques used in trajectory oPtimization, and (iii) an efficient factoriza-
tion to OCP. These lift the comPlexity by orders of magnitude comPared with other OCP-insPired
baselines, without sacrificing the Performance. In summary, we Present the following contributions.
•	We draw a novel PersPective of DNN training from the trajectory oPtimization viewPoint, based
on a theoretical connection between existing training methods and the DDP algorithm.
•	We Present a new class of oPtimizer, DDPNOpt, that Performs a distinct backward Pass inherited
with Bellman oPtimality and generates layer-wise feedback Policies to robustify the training
against unstable hyPerParameter (e.g. large learning rate) setuPs.
•	We show that DDPNOPt achieves comPetitive Performance against existing training methods
on classification datasets and outPerforms Previous OCP-insPired methods in both training
Performance and runtime comPlexity. We also identify DDPNOPt can mitigate vanishing gradient.
2	Preliminaries
We will start with the Bellman PrinciPle to OCP and leave discussions on PMP in APPendix A.1.
Theorem 1 (Dynamic Programming (DP) (Bellman, 1954)). Define a value function Vt : Rn 7→ R
at each time step that is computed backward in time using the Bellman equation
Vt(xt) =	min
ut(xt)∈Γ
xt |
't(xt, ut) + Vt+ι(ft(xt, Ut)), Vt(XT) = φ(xτ),
Qt(xt,ut)≡Qt
(1)
z
where Γxt : Rn 7→ Rm denotes a set of mapping from state to control space. Then, we have
V0(x0) = J* (x0) be the optimal objective value to OCP FUrther let μt (Xt) ∈ Γχt be the minimizer
ofEq. 1 for each t, then the policy π* = {μ* (xt)}T- is globally optimal in the closed-loop system.
Notation: We will always use t as the time steP of dynamics and denote a subsequence trajectory until
time S as Xs，{xt}S=o, with X，{xt}T=o as the whole. For any real-valued time-dependent function Ft,
We denote its derivatives evaluated on a given state-control pair (Xt ∈ Rn and Ut ∈ Rm) as Vxt Ft ∈ Rn,
▽XtFt ∈ Rn×n, VxtutFt ∈ Rn×m,or simply Fx, Fxχ, and FxU for brevity. The vector-tensor product, i.e.
the contraction mapping on the dimension of the vector space, is denoted by Vx ∙ fxx，Pn=ι[V⅛ [fxx]i,
where [Vx]i is the i-th element of the vector Vx and [fxx]i is the Hessian matrix corresponding to that element.
2
Published as a conference paper at ICLR 2021
Algorithm 1 Differential Dynamic Programming
Algorithm 2 Back-propagation (BP) with GD
1:	Input: U , {ut}T=-1, X , {xt}T=0
2:	Set Vx = Vχφ and VxE = vXφ
3:	for t = T - 1 to 0 do
4:	Compute δut (δxt) using Vt+1, Vt+1 (Eq. 3, 4)
5:	Compute Vxt and Vxtx using Eq. 5
6:	end for
7:	Set xo = xo
8:	for t = 0 to T - 1 do
9:	Ut = ut + δu↑(δxt), where δxt = Xt — Xt
10:	Xt+ι = ft(Xt, Ut)
11:	end for
12:	U ― {ut}TT-1
1:	Input: U，{ut}T=-1, X，{xt}T=o, learning
rate η
2:	Set pT ≡ VxT JT = Vxφ
3:	for t = T — 1 to 0 do
4:	6Ut = -ηvut Jt = -η('U + fU ,pt+1)
5:	pt ≡ Vxt Jt = fx pt+1
6:	end for
7:	for t = 0 to T — 1 do
8:	Utt = Ut + δUtt
9:	end for
10:	U ― {Ut}TT-1
Hereafter we refer Qt(xt, ut) to the Bellman objective. The Bellman principle recasts minimization
over a control sequence to a sequence of minimization over each control. The value function Vt
summarizes the optimal cost-to-go at each stage, provided all afterward stages also being minimized.
Differential Dynamic Programming (DDP). Despite providing the sufficient conditions to OCP,
solving Eq. 1 for high-dimensional problems appears to be infeasible, well-known as the Bellman
curse of dimensionality. To mitigate the computational burden of the minimization involved at each
stage, one can approximate the Bellman objective in Eq. 1 with its second-order Taylor expansion.
Such an approximation is central to DDP, a second-order trajectory optimization method that inherits
a similar Bellman optimality structure while being computationally efficient.
Alg. 1 summarizes the DDP algorithm. Given a nominal trajectory (x, U) with its initial cost J, DDP
iteratively optimizes the objective value, where each iteration consists a backward (lines 2-6) and
forward pass (lines 7-11). During the backward pass, DDP performs second-order expansion on the
Bellman objective Qt at each stage and computes the updates through the following minimization,
{2
1
T
1
δuft(δχt) =	arg min
δut(δxt)∈Γ0δxt
Q	Qx = 'x + fx TVt+1
x x xx
where Qtu= `tu+ futTVxt+1
δut
xx
δxt	} ,
δut
(2)
(3)
'Xx + fXτVt+1fX + Vt+1 ∙ fX X
xx
,Quu= 'Uu + fUTW+1fU + vχ+1 ∙ fUu.
Qux= ' + ft TVt+1 ft + Vt+1 ∙ ft
ux u xx x x ux
Qtx
We note that all derivatives in Eq. 3 are evaluated at the state-control pair (xt, ut) at time t among the
nominal trajectory. The derivatives of Qt follow standard chain rule and the dot notation represents
the product of a vector with a 3D tensor. Γ0δx ={bt+Atδxt : bt ∈ Rm, At ∈ Rm×n} denotes the
set of all affine mapping from δxt . The analytic solution to Eq. 2 admits a linear form given by
δ遍(δxt) = kt + Ktδxt, where k，-(Quu)TQu and Kt，-(Quu)TQux	(4)
denote the open and feedback gains, respectively. δxt is called the state differential, which will
play an important role later in our analysis. Note that this policy is only optimal locally around the
nominal trajectory where the second order approximation remains valid. Substituting Eq. 4 back to
Eq. 2 gives us the backward update for Vx and Vxx ,
Vxt = Qtx -QtuxT(Qtuu)-1Qtu , and Vxtx = Qtxx -QtuxT(Qtuu)-1Qtux .	(5)
In the forward pass, DDP applies the feedback policy sequentially from the initial time step while
keeping track of the state differential between the new simulated trajectory and the nominal trajectory.
3	Differential Dynamic Programming Neural Optimizer
3.1	Training DNNs as Trajectory Optimization
Recall that DNNs can be interpreted as dynamical systems where each layer is viewed as a distinct
time step. Consider e.g. the propagation rule in feedforward layers,
xt+1 = σt (ht) , ht = gt (xt, ut) = Wtxt + bt .
(6)
3
Published as a conference paper at ICLR 2021
xt ∈ Rnt and xt+1 ∈ Rnt+1 represent the activation vector at layer t and t + 1, with ht ∈ Rnt+1
being the pre-activation vector. σt and gt respectively denote the nonlinear activation function and
the affine transform parametrized by the vectorized weight ut , [vec(Wt), bt]T. Eq. 6 can be seen
as a dynamical system (by setting ft ≡ σt ◦ gt in OCP) propagating the activation vector xt using ut .
Next, notice that the gradient descent (GD) update, denoted δU* ≡ —ηXlu J With η being the learning
rate, can be break down into each layer, i.e. δU*，{δu* }T=01, and computed backward by
δ u* = arg min {Jt + Nut JT δ Ut + 2 δ UT (11t )δ Ut} ,	(7)
δut∈Rmt
where Jt (xt, ut) , `t (ut) + Jt+1 (ft (xt, ut), ut+1 ) , JT (xT) , φ(xT)	(8)
is the per-layer objective1 at layer t. It can be readily verified that pt ≡ Xxt Jt gives the exact
Back-propagation dynamics. Eq. 8 suggests that GD minimizes the quadratic expansion of Jt with
the Hessian Vut Jt replaced by 11t. Similarly, adaptive first-order methods, such as RMSProP and
Adam, approximate the Hessian with the diagonal of the covariance matrix. Second-order methods,
such as KFAC and EKFAC (Martens & Grosse, 2015; George et al., 2018), compute full matrices
using Gauss-Newton (GN) approximation:
VuJt ≈ E[Jut JTt] = E[(xt ③ Jht)(Xt ③ Jht)T] ≈ E[(xtXT)]③ E[(Jht JTt)] .	(9)
We now draw a novel connection between the training procedure of DNNs and DDP. Let us first
summarize the Back-propagation (BP) with gradient descent in Alg. 2 and compare it with DDP
(Alg. 1). At each training iteration, we treat the current weight as the control U that simulates the
activation sequence X. Starting from this nominal trajectory (X, U), both algorithms recursively
define some layer-wise objectives (Jt in Eq. 8 vs Vt in Eq. 1), compute the weight/control update from
the quadratic expansions (Eq. 7 vs Eq. 2), and then carry certain information (Vxt Jt vs (Vxt, Vxtx))
backward to the preceding layer. The computation graph between the two approaches is summarized
in Fig. 1. In the following proposition, we make this connection formally and provide conditions
when the two algorithms become equivalent.
Proposition 2. Assume Qtux = 0 at all stages,
then the backward dynamics of the value deriva-
tive can be described by the Back-propagation,
∀t,Vxt =VxtJ, Qtu =VutJ, Qtuu =V2utJ.
(10)
In this case, the DDP policy is equivalent to the
stage-wise Newton, in which the gradient is pre-
conditioned by the block-wise inverse Hessian at
each layer:
kt +KtδXt = —(V2utJ)-1VutJ.	(11)
Iffurther we have QuU ≈ 11, then DDP degener-
ates to Back-propagation with gradient descent.
Figure 1: DDP backward propagates the value
derivatives (Vx, Vxx) instead of Vxt J and up-
dates weight using layer-wise feedback policy,
δUt*(δXt), with additional forward propagation.
Proof is left in Appendix A.2. Proposition 2 states that the backward pass in DDP collapses to BP
when Qux vanishes at all stages. In other words, existing training methods can be seen as special
cases of DDP when the mixed derivatives (i.e. Vxtut) of the layer-wise objective are discarded.
3.2	Efficient Approximation and Factorization
Motivated by Proposition 2, we now present a new class of optimizer, DDP Neural Optimizer
(DDPNOpt), on training feedforward and convolution networks. DDPNOpt follows the same pro-
cedure in vanilla DDP (Alg. 1) yet adapts several key traits arising from DNN training, which we
highlight below.
Evaluate derivatives of Qt with layer dynamics. The primary computation in DDPNOpt comes
from constructing the derivatives of Qt at each layer. When the dynamics is represented by the layer
1 Hereafter we drop Xt in all't(∙) as the layer-wise loss typically involves weight regularization alone.
4
Published as a conference paper at ICLR 2021
Table 2: Update rule at each layer t, Ut J Ut 一 ηM-1dt. (Expectation taken over batch data)
Methods	Precondition matrix Mt	Update direction dt
SGD RMSprop KFAC & EKFAC vanilla DDP	It	,	E[Jυt] diag(PE[Jut。Juj + e)	E[jut] E[xtxT]⑥ E[Jht JTt]	E[Jut] 	E[Quu ]	E[Qu + Quχδxt]
DDPNOpt	Mt ∈ I diag(PE[Qu <Ξ> Qu], + e) , I	E[Qu + Quχδxt] [	E[xtxT] % E[VtVh t]	
propagation (recall Eq. 6 where we set ft ≡ σt ◦ gt), we can rewrite Eq. 3 as:
Qx = gXTVt, Qu = 'U + gUTVt, Qxx = gXTVthgx , Qux = gUTVhhgx ,	(12)
where Vht , σht T Vxt+1 and Vhth , σht T Vxtx+1 σht absorb the computation of the non-parametrized
activation function σ. Note that Eq. 12 expands the dynamics only up to first order, i.e. we omitt the
tensor products which involves second-order expansions on dynamics, as the stability obtained by
keeping only the linearized dynamics is thoroughly discussed and widely adapted in practical DDP
usages (Todorov & Li, 2005). The matrix-vector product with the Jacobian of the affine transform
(i.e. gut , gxt ) can be evaluated efficiently for both feedforward (FF) and convolution (Conv) layers:
ht = Wtxt + bt ⇒ gxTVt	=	WTVh ,	guTVt =	Xt	③ Vt,	(13)
ht = ωt * xt ⇒ gx Vt	=	ωT f Vt,	gu Vt =	xt	f Vh,	(14)
where & ^ , and * respectively denote the Kronecker product and (de-)convolution operator.
Curvature approximation. Next, since DNNs are highly over-parametrized models, Ut (i.e. the
layer weight) will be in high-dimensional space. This makes Qtuu and (Qtuu)-1 computationally
intractable to solve; thus requires approximation. Recall the interpretation we draw in Eq. 8 where
existing optimizers differ in approximating the Hessian Vut Jt DDPNOPt adapts the same curvature
approximation to Qtuu. For instance, we can approximate Qtuu simply with an identity matrix It,
adaptive diagonal matrix diag(,E[Qu Θ Qu「), or the GN matrix:
Quu ≈ E[QuQUT] = E[(xt 乳 Vh)(xt 乳 Vt)t] ≈ E[xtxT]乳 EMVtT] .	(15)
Table 2 summarizes the difference in curvature approximation (i.e. the precondition Mt ) for different
methods. Note that DDPNOpt constructs these approximations using (V, Q) rather than J since they
consider different layer-wise objectives. As a direct implication from Proposition 2, DDPNOpt will
degenerate to the optimizer it adapts for curvature approximation whenever all Qtux vanish.
Outer-product factorization. When the memory efficiency becomes nonnegligible as the problem
scales, we make GN approximation to V2φ, since the low-rank structure at the prediction layer has
been observed for problems concerned in this work (Nar et al., 2019; Lezama et al., 2018). In the
following proposition, we show that for a specific type of OCP, which happens to be the case of DNN
training, such a low-rank structure preserves throughout the DDP backward pass.
Proposition 3 (Outer-product factorization in DDPNOpt). Consider the OCP where `t ≡ `t (Ut) is
independent of xt, If the terminal-stage Hessian can be expressed by the outer product of vector zxT,
V2φ(xτ) = ZT 0 ZT (for instance, ZT = Vφ for GN), then we have the faCtorizationforall t:
Qux =	qu	0 qx ,	Qtxx	= qx 0	qx	,	Vxx	=	zx	0	zx	∙	(16)
qut , qxt , and Zxt are outer-product vectors which are also computed along the backward pass.
qu =	fuTzx+1,	qx	=	fxTzx+1,	zx	=	J1 一	quT(Quu)Tqu' qx ∙	(17)
The derivation is left in Appendix A.3. In other words, the outer-product factorization at the final
layer can be backward propagated to all proceeding layers. Thus, large matrices, such as Qtux , Qtxx ,
Vxtx, and even feedback policies Kt, can be factorized accordingly, greatly reducing the complexity.
5
Published as a conference paper at ICLR 2021
Algorithm 3 Differential Dynamic Programming Neural Optimizer (DDPNOpt)
1:	Input: dataset D, learning rate η, training iteration K, batch size B, regularization Vxx
2:	Initialize the network weights U(O) (i.e. nominal control trajectory)
3:	for k = 1 to K do
4:	Sample batch initial state from dataset, Xo ≡ {x0"}B=ι 〜 D
5:	Forward propagate to generate nominal batch trajectory Xt	B Forward simulation
6:	Set VxTw = Vx(i)Φ(χTi)) and VxTx⑸=V^(i)Φ(χTi))
7:	for t = T - 1 to 0 do	B Backward Bellman pass
8:	Compute	Qtu, Qtx , Qtxx , Qtux with Eq. 12 (or Eq. 16-17 if factorization is	used)
9:	Compute	E[Qtuu] with one of the precondition matrices in Table 2
10:	Store the layer-wise feedback policy δ遍(δXt) = -^ PB=I k(i) + K(i)δxti)
11:	Compute	Vxt(i) and Vxtx(i) with Eq. 5 (or Eq. 16-17 if factorization is used)
12:	Vxx⑸ J	VXx(i) + EVxx It if regularization is used
13:	end for
14:	Set X0i) = x0i)
15:	for t = 0 to T - 1 do	B Additional forward pass
16:	遍=Ut + δuj(δXt), where δXt = {X? — Xti)}B=ι
17:	x(+ι = ft(xt∖ ut)
18:	end for
19:	U(k+1) J {u*}T=o1
20:	end for
Regularization on Vxx . Finally, we apply Tikhonov regularization to the value Hessian Vxtx (line 12
in Alg. 3). This can be seen as placing a quadratic state-cost and has been shown to improve stability
on optimizing complex humanoid behavior (Tassa et al., 2012). For the application of DNN where
the dimension of the state (i.e.the vectorized activation) varies during forward/backward pass, the
Tikhonov regularization prevents the value Hessian from low rank (throught gut TVhthgxt ); hence we
also observe similar stabilization effect in practice.
4	The Role of Feedback Policies
DDPNOpt differs from existing methods in the use of feedback Kt and state differential δxt . The
presence of these terms result in a distinct backward pass inherited with the Bellman optimality.
As shown in Table 2, the two frameworks differ in computing the update directions dt, where the
Bellman formulation applies the feedback policy through additional forward pass with δxt . We have
built the connection between these two dt in Proposition 2. In this section, we further characterize
the role of the feedback policy Kt and state differential δxt during optimization.
First we discuss the relation of DDPNOpt with other second-order methods and highlight the role
feedback during training. To do so let us consider the example in Fig. 2a. Given an objective L
expanded at (x0, u0), standard second-order methods compute the Hessian w.r.t. u then apply the
update δu = —Lu-u1 Lu (shown as green arrows). DDPNOpt differs in that it also computes the mixed
partial derivatives, i.e. Lux . The resulting update law has the same intercept but with an additional
feedback term linear in δx (shown as red arrows). Thus, DDPNOpt searches for an update from the
affine mapping Γ0δx (Eq. 2), rather than the vector space Rmt (Eq. 7).
Next, to show how the state differential δxt arises during optimization, notice from Alg. 1 that Xt can
be compactly expressed as Xt = Ft(xo, U + δu*(δx))2. Therefore, δxt = Xt — Xt captures the state
difference when new updates δu* (δX) are applied until layer t — 1. Now, consider the 2D example in
Fig 2b. Back-propagation proposes the update directions (shown as blue arrows) from the first-order
derivatives expanded along the nominal trajectory (X, U). However, as the weight at each layer is
correlated, parameter updates fromprevious layers δu* affect proceeding states {Xt : t > s},thus the
trustworthiness of their descending directions. As shown in Fig 2c, cascading these (green) updates
may cause an over-shoot w.r.t. the designed target. From the trajectory optimization perspective,
a much stabler direction will be instead Vut Jt(Xt, Ut) (shown as orange), where the derivative is
2 Ft，ft。…。fo denotes the compositional dynamics propagating xo with the control sequence {u§ }S=0.
6
Published as a conference paper at ICLR 2021
Figure 2: (a) A toy illustration of the standard update (green) and the DDP feedback (red). The DDP
policy in this case is a line lying at the valley of objective L. (bc) Trajectory optimization viewpoint
of DNN training. Green and orange arrows represent the proposed updates from GD and DDP.
evaluated at the new cascading state Xt, which accounts for previous updates, rather than the original
state xt . This is exactly what DDPNOpt proposes, as we can derive the relation (see Appendix A.5),
Kt δ Xt ≈	arg min	∣∣Vut J (X t, Ut + δ Ut (δ Xt)) - Nut J (Xt, Ut )k .	(18)
δut (δxt)∈Γ0δxt
Thus, the feedback direction compensates the over-shoot by steering the GD update toward
Vut Jt(Xt, Ut) after observing δXt. The difference between Vut J(Xt, υt) and Vut J(Xt, υt) cannot
be neglected especially during early training when the loss landscape contains nontrivial curvature
everywhere (Alain et al., 2019). In short, the use of feedback Kt and state differential δXt arises from
the fact that deep nets exhibit chain structures. DDPNOpt feedback policies thus have a stabilization
effect on robustifying the training dynamics against e.g.improper hyper-parameters which may cause
unstable training. This perspective (i.e. optimizing chained parameters) is explored rigorously in
trajectory optimization, where DDP is shown to be numerically stabler than direct optimization such
as Newton method (Liao & Shoemaker, 1992).
Remarks on other optimizers. Our discussions so far rigorously explore the connection between
DDP and stage/layer-wise Newton, thus include many popular second-order training methods. Gen-
eral Newton method coincides with DDP only for linear dynamics (Murray & Yakowitz, 1984),
despite both share the same convergence rate when the dynamics is fully expanded to second order.
We note that computing layer-wise value Hessians with only first-order expansion on the dynamics
(Eq. 12) resembles the computation in Gauss-Newton method (Botev et al., 2017). For other control-
theoretic methods, e.g. PID optimizers (An et al., 2018), they mostly consider the dynamics over
training iterations. DDPNOpt instead focuses on the dynamics inherited in the DNN architecture.
5 Experiments
5.1	Performance on Classification Dataset
Networks & Baselines Setup. We first validate the performance of training fully-connected (FCN)
and convolution networks (CNN) using DDPNOpt on classification datasets. FCN consists of 5
fully-connected layers with the hidden dimension ranging from 10 to 32, depending on the size of
the dataset. CNN consists of 4 convolution layers (with 3×3 kernel, 32 channels), followed by 2
fully-connected layers. We use ReLU activation on all datasets except Tanh for WINE and DIGITS
to better distinguish the differences between optimizers. The batch size is set to 8-32 for datasets
trained with FCN, and 128 for datasets trained with CNN. As DDPNOpt combines strengths from
both standard training methods and OCP framework, we select baselines from both sides. This
includes first-order methods, i.e. SGD (with tuned momentum), RMSprop, Adam, and second-order
method EKFAC (George et al., 2018), which is a recent extension of the popular KFAC (Martens &
Grosse, 2015). For OCP-inspired methods, we compare DDPNOpt with vanilla DDP and E-MSA (Li
et al., 2017), which is also a second-order method yet built upon the PMP framework. Regarding the
curvature approximation used in DDPNOpt (Mt in Table 2), we found that using adaptive diagonal
and GN matrices respectively for FCNs and CNNs give the best performance in practice. We leave
the complete experiment setup and additional results in Appendix A.6.
Training Results. Table 3 presents the results over 10 random trials. It is clear that DDPNOpt
outperforms two OCP baselines on all datasets and network types. In practice, both baselines suffer
from unstable training and require careful tuning on the hyper-parameters. In fact, we are not able
to obtain results for vanilla DDP with any reasonable amount of computational resources when the
problem size goes beyond FC networks. This is in contrast to DDPNOpt which adapts amortized
7
Published as a conference paper at ICLR 2021
Table 3: Performance comparison on accuracy (%). All values averaged over 10 seeds.
	DataSet	SGD-m	Standard baselines RMSProp Adam		EKFAC	OCP-inspired baselines E-MSA vanilla DDP		DDPNOpt (ours)
Feed-forward	WINE	94.35	98.10	98.13	94.60	93.56	98.00	98.18
	DIGITS	95.36	94.33	94.98	95.24	94.87	91.68	95.13
	MNIST	92.65	91.89	92.54	92.73	90.24	90.42	93.30
	F-MNIST	82.49	83.87	84.36	84.12	82.04	81.98	84.98
N N U	MNIST	97.94	98.05	98.04	98.02	96.48		98.09
	SVHN	89.00	88.41	87.76	90.63	79.45	N/A	90.70
	CIFAR-10	71.26	70.52	70.04	71.85	61.42		71.92
Figure 3: Runtime comparison on MNIST.
Table 4: Computational complexity in backward pass.
(B: batch size, X : hidden state dim., L: # of layers)
Method	Adam	Vanilla DDP	DDPNOpt
Memory	O(X 2L)	O(BX3L)	O(X 2L + BX)
Speed	O(BX2L)	O(B3X3L)	O(BX2L)
curvature estimation from widely-used methods; thus exhibits much stabler training dynamics with
superior convergence. In Table 4, we provide the analytic runtime and memory complexity among
different methods. While vanilla DDP grows cubic w.r.t. BX, DDPNOpt reduces the computation by
orders of magnitude with efficient approximation presented in Sec. 3. As a result, when measuring the
actual computational performance with GPU parallelism, DDPNOpt runs nearly as fast as standard
methods and outperforms E-MSA by a large margin. The additional memory complexity, when
comparing DDP-inspired methods with Back-propagation methods, comes from the layer-wise
feedback policies. However, DDPNOpt is much memory-efficient compared with vanilla DDP by
exploiting the factorization in Proposition 3.
Ablation Analysis. On the other hand, the performance gain between DDPNOpt and standard
methods appear comparatively small. We conjecture this is due to the inevitable use of similar
curvature adaptation, as the local geometry of the landscape directly affects the convergence behavior.
To identify scenarios where DDPNOpt best shows its effectiveness, we conduct an ablation analysis
on the feedback mechanism. This is done by recalling Proposition 2: when Qtux vanishes, DDPNOpt
degenerates to the method associated with each precondition matrix. For instance, DDPNOpt with
identity (resp. adaptive diagonal and GN) precondition Mt will generate the same updates as SGD
(resp. RMSprop and EKFAC) when all Qtux are zeroed out. In other words, these DDPNOpt variants
can be viewed as the DDP-extension to existing baselines.
In Fig. 4a we report the performance difference between each baseline and its associated DDPNOpt
variant. Each grid corresponds to a distinct training configuration that is averaged over 10 random
trails, and we keep all hyper-parameters (e.g. learning rate and weight decay) the same between
baselines and their DDPNOpt variants. Thus, the performance gap only comes from the feedback
policies, or equivalently the update directions in Table 2. Blue (resp. red) indicates an improvement
(resp. degradation) when the feedback policies are presented. Clearly, the improvement over baselines
remains consistent across most hyper-parameters setups, and the performance gap tends to become
obvious as the learning rate increases. This aligns with the previous study on numerical stability (Liao
& Shoemaker, 1992), which suggests the feedback can stabilize the optimization when e.g. larger
control updates are taken. Since larger control corresponds to a further step size in the application of
DNN training, one should expect DDPNOpt to show its robustness as the learning rate increases. As
shown in Fig. 4b, such a stabilization can also lead to smaller variance and faster convergence. This
sheds light on the benefit gained by bridging two seemly disconnected methodologies between DNN
training and trajectory optimization.
5.2	Discussion on Feedback Policies
Visualization of Feedback Policies. To understand the effect of feedback policies more perceptually,
in Fig. 5 we visualize the feedback policy when training CNNs. This is done by first conducting
8
Published as a conference paper at ICLR 2021
Config: Ir 0.6f Vxx=le-3
——SGD
——DDPNOpt
SScrlU-alh->U2⊃UU<
O 500 IOOO 1500
Iterations
Config: Ir 0.01f Vxx= le-5
Vxx regularization
DDPNOpt vs EKFAC (accuracy %)
sso, U-B-IJ.
RMSprop
DDPNOpt
Iterations
Figure 4: (a) Performance difference between DDPNOpt and baselines on DIGITS across hyper-
parameter grid. Blue (resp. red) indicates an improvement (resp. degradation) over baselines. We
observe similar behaviors on other datasets. (b) Examples of the actual training dynamics.


Figure 5: Visualization of the
feedback policies on MNIST.

(a)
10
o 9
c 8
2 7
6
0
(b)
——SGD-VGR
一EKFAC	⅛⅛⅜ k∣h⅛M⅜⅛⅛∣⅛⅛l⅛⅝
- DDPNOpt	叫哪WRnlFP甲［
——DDPN0pt2nd
1000	2000	3000	4000
Iterations
Figure 6: Training a 9-layer sigmoid-activated FCN on
DIGITS using MMC loss. DDPNOpt2nd denotes when the layer
dynamics is fully expanded to the second order.
singular-value decomposition on the feedback matrices Kt , then projecting the leading right-singular
vector back to image space (see Alg. 4 and Fig. 7 in Appendix for the pseudo-code). These feature
maps, denoted δxmax in Fig. 5, correspond to the dominating differential image that the policy shall
respond with during weight update. Fig. 5 shows that the feedback policies indeed capture non-trivial
visual features related to the pixel-wise difference between spatially similar classes, e.g. (8, 3) or
(7, 1). These differential maps differ from adversarial perturbation (Goodfellow et al., 2014) as the
former directly links the parameter update to the change in activation; thus being more interpretable.
Vanishing Gradient. Lastly, we present an interesting finding on how the feedback policies help
mitigate vanishing gradient (VG), a notorious effect when DNNs become impossible to train as
gradients vanish along Back-propagation. Fig. 6a reports results on training a sigmoid-activated
DNN on DIGITS. We select SGD-VGR, which imposes a specific regularization to mitigate VG
(Pascanu et al., 2013), and EKFAC as our baselines. While both baselines suffer to make any progress,
DDPNOpt continues to generate non-trivial updates as the state-dependent feedback, i.e. Ktδxt,
remains active. The effect becomes significant when dynamics is fully expanded to the second order.
As shown in Fig. 6b, the update norm from DDPNOpt is typically 5-10 times larger. We note that in
this experiment, we replace the cross-entropy (CE) with Max-Mahalanobis center (MMC) loss, a new
classification objective that improves robustness on standard vision datasets (Pang et al., 2019). MMC
casts classification to distributional regression, providing denser Hessian and making problems similar
to original trajectory optimization. None of the algorithms escape from VG using CE. We highlight
that while VG is typically mitigated on the architecture basis, by having either unbounded activation
function or residual blocks, DDPNOpt provides an alternative from the algorithmic perspective.
6 Conclusion
In this work, we introduce DDPNOpt, a new class of optimizer arising from a novel perspective by
bridging DNN training to optimal control and trajectory optimization. DDPNOpt features layer-wise
feedback policies which improve convergence and robustness to hyper-parameters over existing
optimizers. It outperforms other OCP-inspired methods in both training performance and scalability.
This work provides a new algorithmic insight and bridges between deep learning and optimal control.
9
Published as a conference paper at ICLR 2021
Acknowledgments
The authors would like to thank Chen-Hsuan Lin, Yunpeng Pan, Yen-Cheng Liu, and Chia-Wen Kuo
for many helpful discussions on the paper. This research was supported by NSF Award Number
1932288.
References
Guillaume Alain, Nicolas Le Roux, and Pierre-Antoine Manzagol. Negative eigenvalues of the
hessian in deep neural networks. arXiv preprint arXiv:1902.02366, 2019.
Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. A pid controller
approach for stochastic optimization of deep networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 8522-8531, 2018.
Richard Bellman. The theory of dynamic programming. Technical report, Rand corp santa monica
ca, 1954.
Martin Benning, Elena Celledoni, Matthias J Ehrhardt, Brynjulf Owren, and Carola-Bibiane Schonlieb.
Deep learning as optimal control problems: models and numerical methods. arXiv preprint
arXiv:1904.05657, 2019.
Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic
programming and optimal control. Athena scientific Belmont, MA, 1995.
Vladimir Grigor’evich Boltyanskii, Revaz Valer’yanovich Gamkrelidze, and Lev Semenovich Pon-
tryagin. The theory of optimal processes. i. the maximum principle. Technical report, TRW SPACE
TECHNOLOGY LABS LOS ANGELES CALIF, 1960.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 557-565. JMLR. org, 2017.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary dif-
ferential equations. In Advances in Neural Information Processing Systems, pp. 6572-6583,
2018.
R Gamkrelidze. Principles of optimal control theory, volume 7. Springer Science & Business Media,
2013.
Thomas George, CeSar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast
approximate natural gradient descent in a kronecker factored eigenbasis. In Advances in Neural
Information Processing Systems, pp. 9550-9560, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Advances
in Neural Information Processing Systems, pp. 15353-15363, 2019.
Tianyu Gu. Improved trajectory planning for on-road self-driving vehicles via combined graph
search, optimization & topology analysis. PhD thesis, Carnegie Mellon University, 2017.
Stefanie Gunther, Lars Ruthotto, Jacob B Schroder, Eric C Cyr, and Nicolas R Gauger. Layer-parallel
training of deep residual neural networks. SIAM Journal on Mathematics of Data Science, 2(1):
1-23, 2020.
D.H. Jacobson and D.Q. Mayne. Differential Dynamic Programming. Modern analytic and computa-
tional methods in science and mathematics. American Elsevier Publishing Company, 1970. URL
https://books.google.com/books?id=tA-oAAAAIAAJ.
Jose Lezama, Qiang Qiu, Pablo Muse, and Guillermo Sapiro. Ole: Orthogonal low-rank embedding-
a plug and play geometric loss for deep learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 8109-8118, 2018.
10
Published as a conference paper at ICLR 2021
Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applications to
discrete-weight neural networks. arXiv preprint arXiv:1803.01299, 2018.
Qianxiao Li, Long Chen, Cheng Tai, and E Weinan. Maximum principle based algorithms for deep
learning. The Journal of Machine Learning Research ,18(1):5998-6026, 2017.
Li-zhi Liao and Christine A Shoemaker. Advantages of differential dynamic programming over
newton’s method for discrete-time optimal control problems. Technical report, Cornell University,
1992.
Guan-Horng Liu and Evangelos A Theodorou. Deep learning theory review: An optimal control and
dynamical systems perspective. arXiv preprint arXiv:1908.10920, 2019.
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks:
Bridging deep architectures and numerical differential equations. arXiv preprint arXiv:1710.10121,
2017.
Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu.
Understanding and improving transformer from a multi-particle dynamic system point of view.
arXiv preprint arXiv:1906.02762, 2019.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
DM Murray and SJ Yakowitz. Differential dynamic programming and newton’s method for discrete
optimal control problems. Journal of Optimization Theory and Applications, 43(3):395-414, 1984.
Kamil Nar, Orhan Ocal, S Shankar Sastry, and Kannan Ramchandran. Cross-entropy loss and
low-rank features have responsibility for adversarial examples. arXiv preprint arXiv:1901.08360,
2019.
Tianyu Pang, Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, and Jun Zhu. Rethinking softmax
cross-entropy loss for adversarial robustness. arXiv preprint arXiv:1905.10626, 2019.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International conference on machine learning, pp. 1310-1318, 2013.
M Peifer and J Timmer. Parameter estimation in ordinary differential equations for biochemical
processes using the method of multiple shooting. IET Systems Biology, 1(2):78-88, 2007.
Lev Semenovich Pontryagin, EF Mishchenko, VG Boltyanskii, and RV Gamkrelidze. The mathemat-
ical theory of optimal processes. 1962.
Jacob H Seidman, Mahyar Fazlyab, Victor M Preciado, and George J Pappas. Robust deep learning as
optimal control: Insights and convergence guarantees. Proceedings of Machine Learning Research
vol xxx, 1:14, 2020.
Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors
through online trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent
Robots and Systems, pp. 4906-4913. IEEE, 2012.
Yuval Tassa, Nicolas Mansard, and Emo Todorov. Control-limited differential dynamic programming.
In 2014 IEEE International Conference on Robotics and Automation (ICRA), pp. 1168-1175.
IEEE, 2014.
Emanuel Todorov and Weiwei Li. A generalized iterative lqg method for locally-optimal feedback
control of constrained nonlinear stochastic systems. In Proceedings of the 2005, American Control
Conference, 2005., pp. 300-306. IEEE, 2005.
Henning U VOss, Jens Timmer, and Jurgen Kurths. Nonlinear dynamical system identification from
uncertain and indirect measurements. International Journal of Bifurcation and Chaos, 14(06):
1905-1933, 2004.
E Weinan. A proposal on machine learning via dynamical systems. Communications in Mathematics
and Statistics, 5(1):1-11, 2017.
11
Published as a conference paper at ICLR 2021
E Weinan, Jiequn Han, and Qianxiao Li. A mean-field optimal control formulation of deep learning.
arXiv preprint arXiv:1807.01083, 2018.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. arXiv preprint arXiv:1905.00877,
2019.
A Appendix
A.1 Connection between Pontryagin Maximum Principle and DNNs Training
Development of the optimality conditions to OCP can be dated back to 1960s, characterized by both
the Pontryagin's Maximum Principle (PMP) and the Dynamic Programming (DP). Here we review
Theorem of PMP and its connection to training DNNs.
Theorem 4 (Discrete-time PMP (Pontryagin et al., 1962)). Let U* be the optimal control trajectory
for OCP and X* be the corresponding State trajectory. Then, there exists a co-state trajectory
p* , {p*}T=ι, such that
x*+1 = vpHt (x*, p*+i, u*) , x0 = xO , Pt = SVxHt (x°, p*+1, UO) , p*T = vxφ (xt) , U* = arg min Ht (x*, P*+ι, V). v∈Rm	(19a) (19b) (19c)
where Ht : Rn × Rn × Rm 7→ R is the discrete-time Hamiltonian given by
Ht (xt, Pt+1, Ut)，'t(xt, Ut) + PT+ιft(xt, Ut) ,	(20)
and Eq. 19b is called the adjoint equation.
The discrete-time PMP theorem can be derived using KKT conditions, in which the co-state Pt is
equivalent to the Lagrange multiplier. Note that the solution to Eq. 19c admits an open-loop process
in the sense that it does not depend on state variables. This is in contrast to the Dynamic Programming
principle, in which a feedback policy is considered.
It is natural to ask whether the necessary condition in the PMP theorem relates to first-order optimiza-
tion methods in DNN training. This is indeed the case as pointed out in Li et al. (2017):
Lemma 5 (Li et al. (2017)). Back-propagation satisfies Eq. 19b and gradient descent iteratively
solves Eq. 19c.
Lemma 5 follows by first expanding the derivative of Hamiltonian w.r.t. xt,
▽ XtHt(Xt, pt+ι, Ut) = Rxt't(χt, Ut) + Rxt ft(xt, ut)TPt+ι = RxtJ(U; XO).	(21)
Thus, Eq. 19b is simply the chain rule used in the Back-propagation. When Ht is differentiable w.r.t.
Ut, one can attempt to solve Eq. 19c by iteratively taking the gradient descent. This will lead to
Utk+1) = U(k) — ηRutHt(xt,Pt+1, Ut) = U(k)—n▽%J(U; x0) ,	(22)
where k and η denote the update iteration and step size. Thus, existing optimization methods can be
interpreted as iterative processes to match the PMP optimality conditions.
Inspired from Lemma 5, Li et al. (2017) proposed a PMP-inspired method, named Extended Method
of Successive Approximations (E-MSA), which solves the following augmented Hamiltonian
Ht (xt, Pt+1, Ut, Xt+1, Pt) , Ht (xt, Pt+1, Ut)
+ 2ρ kxt+ι - ft(xt,Ut)Il + 2P IlPt - VxtHtk .
12
Published as a conference paper at ICLR 2021
Ht is the original Hamiltonian augmented with the feasibility constraints on both forward states and
backward co-states. E-MSA solves the minimization
*
Ut = argmin Ht (xt, Pt+ι, ut, xt+ι,Pt)	(24)
ut∈Rmt
with L-BFGS per layer and per training iteration. As a result, we consider E-MSA also as second-order
method.
A.2 Proof of Proposition 2
Proof. We first prove the following lemma which connects the backward pass between two frame-
works in the degenerate case.
Lemma 6. Assume Qtux = 0 at all stages, then we have
Vxt = Vxt J ,and Vtx = WtJ, Vt.	(25)
Proof. It is obvious to see that Eq. 25 holds at t = T. Now, assume the relation holds at t + 1 and
observe that at the time t, the backward passes take the form of
VX = QX - QuX(Quu)TQu = 'X + fXTVχt+ι J = Vxt J,
VxX = QXX- QuX(Quu)TQuX = VXt {'X + fX^τ V Xt+ι J} = VXt J,
where we recall Jt = `t + Jt+1 (ft) in Eq. 8.
□
Now, Eq. 11 follows by substituting Eq. 25 to the definition of Qtu and Qtuu
Qu = 'U + fU Tvt+1 = `u + fU τVχt+ι J = Vut J,
Qtuu = Ctuu + fuTVXXIfu + vt+1 ∙ fuu
='uu + fuT(VXt+ι J)fu + Vxt+i J ∙ fuu
=Vut {'u + fu τVχt+ι j } = VutJ.
Consequently, the DDP feedback policy degenerates to layer-wise Newton update.
□
A.3 Proof of Proposition 3
Proof. We will prove Proposition 3 by backward induction. Suppose at layer t + 1, we have
VXt+1 = zX+1 0 ztt+1 and't ≡ 't(ut), then Eq. 3 becomes
QXx =	fX TVXX1 fX	= fXT(zX+10 zX+1)fX	=	(fXTzX+1) 0	(fXTzX+1)
QtuX =	fut TVXtX+1fXt	= fut T(zXt+1 0 zXt+1)fXt	=	(fut TzXt+1) 0	(fXt TzXt+1).
Setting qXt := fXtTzXt+1 and qut := fut TzXt+1 will give the first part of Proposition 3.
Next, to show the same factorization structure preserves through the preceding layer, it is sufficient to
show VXtX = zXt 0 zXt for some vector zXt . This is indeed the case.
VXtX = QtXX - QtuXT(Qtuu)-1QtuX
= qXt 0qXt - (qut 0qXt )T(Qtuu)-1(qut 0 qXt )
= qXt 0qXt - (qutT(Qtuu)-1qut )(qXt 0qXt ) ,
where the last equality follows by observing qut T(Qtuu)-1qut is a scalar.
Set ZX = y 1 一 quT(Quu)Tqu qX Will give the desired factorization.
□
13
Published as a conference paper at ICLR 2021
A.4 Derivation of Eq. 12
For notational simplicity, we drop the superscript t and denote Vχ0,，VxVt+1(xt+1) as the derivative
of the value function at the next state.
Qu = 'u + AT— 'u + gU σTvχθ 0,
∂
Quu = 'uu + ∂U {guσhVx0}
∂∂	∂
='uu + gu σh ∂- {Vχ0} + gu( ∂ {σh}) Vx0 + ( ∂ {gu}) σhVx0
∂u	∂u	∂u
= `uu + guσhVx0x0σhgu + gu(Vx0 σhhgu) + guuσhVx0
='uu + gT(Vhh + Vxo ∙ σhh)gu + Vh ∙ guu
The last equation follows by recalling Vh , σhTVx00 and Vhh , σhTVx00x0σh. Follow similar derivation,
we have
Qx = 'x + gxTVh
QXX = 'xx + gT(Vhh + Vx0 ∙ σhh)gx + Vh ∙ gxx	(26)
Qux = 'ux + gu(Vhh + VX，∙ σhh )gx + Vh ∙ gux
Remarks. For feedforward networks, the computational overhead in Eq. 12 and 26 can be mitigated
by leveraging its affine structure. Since g is bilinear in xt and ut, the terms gxt x and gut u vanish. The
tensor gut x admits a sparse structure, whose computation can be simplified to
[gut x](i,j,k) = 1 iff j = (k - 1)nt+1 + i ,
[Vh ∙ gux]((k-1)nt+i：knt+i,k) = Vht .
(27)
For the coordinate-wise nonlinear transform, σht and σht h are diagonal matrix and tensor. In most
learning instances, stage-wise losses typically involved with weight decay alone; thus the terms
'tx , 'txx , 'tux also vanish.
A.5 Derivation of Eq. 18
Eq. 18 follows by an observation that the feedback policy Ktδxt = -(Qtuu)-1Qtux δxt stands as
the minimizer of the following objective
Ktδxt =	arg min	kVut Q(xt + δxt,ut + δut(δxt)) - VutQ(xt,ut)k ,	(28)
δut(δxt)∈Γ0(δxt)
where Γ0(δxt) denotes all affine mappings from δxt to δut and ∣∣∙k can be any proper norm in the
Euclidean space. Eq. 28 follows by the Taylor expansion of Q(xt + δxt, ut + δut) to its first order,
Vut Q(xt + δxt, ut + δut) = Vut Q(xt, ut) + Qux δxt + Quuδut .
When Q = J, we will arrive at Eq. 18. From Proposition 2, we know the equality holds when all Qsxu
vanish for s > t. In other words, the approximation in Eq. 18 becomes equality when all aferward
layer-wise objectives s > t are expanded only w.r.t. us .
A.6 Experiment Detail
A.6. 1 Setup
Clarification Dataset. All networks in the classification
experiments are composed of 5-6 layers. For the inter-
mediate layers, we use ReLU activation on all dataset,
except Tanh on WINE and DIGITS. We use identity
mapping at the last prediction layer on all dataset except
WINE, where we use sigmoid instead to help distinguish
the performance among optimizers. For feedforward net-
works, the dimension of the hidden state is set to 10-32.
On the other hand, we use standard 3 × 3 convolution
Table 5: Hyper-parameter search
Methods	Learning Rate
SGD	(7e-2, 5e-1)
Adam & RMSprop	(7e-4, 1e-2)
EKFAC	(1e-2, 3e-1)
kernels for all CNNs. The batch size is set 8-32 for dataset trained with feedforward networks,
14
Published as a conference paper at ICLR 2021
and 128 for dataset trained with convolution networks. For each baseline we select its own hyper-
parameter from an appropriate search space, which we detail in Table 5. We use the implementation in
https://github.com/Thrandis/EKFAC-pytorch for EKFAC and implement our own
E-MSA in PyTorch since the official code released from Li et al. (2017) does not support GPU
implementation. We impose the GN factorization presented in Proposition 3 for all CNN training.
Regarding the machine information, we conduct our experiments on GTX 1080 TI, RTX TITAN, and
four Tesla V100 SXM2 16GB.
Procedure to Generate Fig. 5. First, we perform standard DDPNOpt steps to compute layer-wise
policies. Next, we conduct singular-value decomposition on the feedback matrix (kt , Kt). In this
way, the leading right-singular vector corresponding to the dominating that the feedback policy shall
respond with. Since this vector is with the same dimension as the hidden state, which is most likely
not the same as the image space, we project the vector back to image space using the techniques
proposed in (Zeiler & Fergus, 2014). The pseudo code and computation diagram are included in
Alg. 4 and Fig. 7.
Algorithm 4 Visualizing the Feedback Policies
1:	Input: Image x (we drop the time subscript for nota-
tional simplicity, i.e.x ≡ x0)
2:	Perform backward pass of DDPNOpt. Compute
(kt , Kt ) backward
3:	Perform SVD on Kt
4:	Extract the right-singular vector corresponding to the
largest singular value, denoted vmax ∈ Rnt
5:	Project vmax back to the image space using decon-
volution procedures introduced in (Zeiler & Fergus,
2014)
Figure 7: Pictorial illustration for Alg. 4.
A.6.2 Additional Experiment and Discussion
Batch trajectory optimization on synthetic dataset. One of the difference between DNN training
and trajectory optimization is that for the former, we aim to find an ultimate control law that can drive
every data point in the training set, or sampled batch, to its designed target. Despite seemly trivial
from the ML perspective, this is a distinct formulation to OCP since the optimal policy typically
varies at different initial state. As such, we validate performance of DDPNOpt in batch trajectories
optimization on a synthetic dataset, where we sample data from k ∈ {5, 8, 12, 15} Gaussian clusters
in R30 . Since conceptually a DNN classifier can be thought of as a dynamical system guiding
trajectories of samples toward the target regions belong to their classes, we hypothesize that for the
DDPNOpt to show its effectiveness on batch training, the feedback policy must act as an ensemble
policy that combines the locally optimal policy of each class. Fig. 8 shows the spectrum distribution,
sorted in a descending order, of the feedback policy in the prediction layer. The result shows that the
number of nontrivial eigenvalues matches exactly the number of classes in each setup (indicated by
the vertical dashed line). As the distribution in the prediction layer concentrates to k bulks through
training, the eigenvalues also increase, providing stronger feedback to the weight update.
Order of Largest Eigenvalues
Figure 8: Spectrum distribution on synthetic dataset.
Ablation analysis on Adam. Fig. 9 reports the ablation analysis on Adam using the same setup as
in Fig. 4a, i.e. we keep all hyper-parameters the same for each experiment so that the performance
15
Published as a conference paper at ICLR 2021
difference only comes from the existence of feedback policies. It is clear that the improvements from
the feedback policies remain consistent for Adam optimizer.
Figure 9: Additional experiment for Fig. 4a where we compare the performance difference between
DDPNOpt and Adam. Again, all grids report values averaged over 10 random seeds.
Ablation analysis on DIGITS compared with best-tuned baselines. Fig. 4 reports the performance
difference between baselines and DDPNOpt under different hyperparameter setupts. Here, we report
the numerical values when each baseline uses its best-tuned learning rate (which is the values we
report in Table 3) and compare with its DDPNOpt counterpart using the same learning rate. As
shown in Tables 6, 7, and 8, for most cases extending the baseline to accept the Bellman framework
improves the performance.
Table 6: Learning rate = 0.1
I SGD	DDPNOPt with Mt = It
Train Loss	0.035	0.032
Accuracy (%)	95.36	95.52
	Table 7: Learning rate =	0.001
	RMSprop	DDPNOpt with Mt	=diag(pE[QU Θ QU]' + E)
Train Loss	0.058	0.052
Accuracy (%)	94.33	94.63
Table 8: Learning rate = 0.03
E EKFAC DDPNOpt with Mt = E[xtxT]③ E[VtVt t]
Train Loss	0.074	0.067
Accuracy (%)	95.24	95.19
Numerical absolute values in ablation analysis (DIGITS). Fig. 4a rePorts the relative Performance
between each baseline and its DDPNOpt counterpart under different learning rate and regularization
setups. In Table 9 and 10, we report the absolute numerical values of this experiment. For instance,
the most left-upper grid in Fig. 4a, i.e.the training loss difference between DDPNOpt and SGD with
learning rate 0.4 and Vxx regularization 5 × 10-5, corresponds to 0.1974 - 0.1662 in Table 9. All
values in these tables are averaged over 10 seeds.
Table 9: Training Loss. (Vxx denotes the Tikhonov regularization on Vxx .)
	SGD	DDPNOpt with Mt = It			
		EVxx =5 ×	10-5	1 × 10-4	5 × 10-4	1 × 10-3
0.4	0.1974	0.1662	0.1444	0.1322	0.1067
0.6	0.5809	0.4989	0.4867	0.3263	0.2764
0.7	1.0493	0.9034	0.8240	0.6592	0.5381
0.8	1.7801	1.6898	1.4597	1.1784	1.3166
16
Published as a conference paper at ICLR 2021
			RMSProP		DDPNOpt with Mt = diag(√E[QU Θ QU]' + e)			
					Mx = 1 X 10-9	1 X 10-8	5 X 10-6	1 X 10-5
Learn Rate	0.01 0.02 0.03 0.045		0.1949 0.4691 0.8156 1.3103		0.1638 0.4559 0.7675 1.2740	0.1714 0.4489 0.7736 1.2956	0.1746 0.4390 0.7790 1.2568	0.1588 0.4773 0.7893 1.2758
			EKFAC	DDPNOPt With Mt = E[xtxT]⑥ E[VtVtt] evχx = 1 X 10-7	5 X 10-7	5 X 10-6	1 X 10-5				
J Leam Rate I		0.05 0.09 0.1 0.3	0.0757 0.2274 0.3260 0.5959		0.0636 0.2087 0.2771 0.5462	0.0659 0.2164 0.3003 0.5282	0.0691 0.2091 0.2543 0.5299	0.0717 0.2223 0.2510 0.5858
Table 10: Accuracy (%). (Vxx denotes the Vxx regularization.)
				SGD		DDPNOpt with Mt Vxx = 5 X 10-5 1 X 10-4 5			= It X 10-4	1	X 10-3
Learn Rate			0.4	91.46			91.98	92.71	92.90	93.12
			0.6 0.7	81.73 70.48			83.64 73.42	84.09 75.44	88.39 80.62	89.39 82.87
			0.8	55.76			57.70	62.82	70.23	65.01
										
				RMSprop			DDPNOpt with Mt = diag(PE[QU Θ QU]' + e) Vxx = 1 X 10-9 1 X 10-8 5 X 10-6 1 X 10-5			
Learn Rate		0.01		91.48			92.14	91.80	91.73	92.52
		0.02		84.15			84.82	85.02	85.23	83.00
		0.03		73.07			75.24	75.73	74.29	74.16
		0.045		59.80			59.16	60.98	61.75	59.87
										
				EKFAC		DDPNOpt with Mt = E[xtXT]⑥ E[½t½t t] evxx = 1 X 10-7 5 X 10-7 5 X 10-6 1 X 10-5				
Learn Rate		0.05		93.70			93.84	93.88	94.31	94.06
		0.09		90.84			91.13	91.45	91.23	91.24
			0.1	88.88			89.69	89.89	90.18	90.94
			0.3	81.82			83.79	84.09	84.15	82.55
More experiments on vanishing gradient. Recall that Fig. 6 reports the training performance using
MMC loss on Sigmoid-activated networks. In Fig. 10a, we report the result when training the same
networks but using CE loss (notice the numerical differences in the y axis for different objectives).
None of the presented optimizers were able to escape from vanishing gradient, as evidenced by the
vanishing update magnitude. On the other hands, changing the networks to ReLU-activated networks
eliminates the vanishing gradient, as shown in Fig. 10b.
Fig. 11	reports the performance with other first-order adaptive optimizers including Adam and
RMSprop. In general, adaptive first-order optimizers are more likely to escape from vanishing
gradient since the diagonal precondition matrix (recall Mt = E[Jut Jut] in Table 2) rescales
the vanishing update to a fixed norm. However, as shown in Fig. 11, DDPNOpt* (the variant of
DDPNOpt that utilizes similar adaptive first-order precondition matrix) converges faster compared
with these adaptive baselines.
Fig. 12	illustrates the selecting process on the learing-rate tuning when we report Fig. 6. The training
performance for both SGD-VGR and EKFAC remains unchanged when tuning the learning rate. In
17
Published as a conference paper at ICLR 2021
practice, we observe unstable training with SGD-VGR when the learing rate goes too large. On the
other hands, DDPNOpt and DDPNOpt2nd are able to escape from VG with all tested learning rates.
Hence, Fig. 6 combines Fig. 12a (SGD-VGR-lr0.1) and Fig. 12c (EKFAC-lr0.03, DDPNOpt-lr0.03,
DDPNOpt2nd-lr0.03) for best visualization.
(a)
CE Lx)ss + Sigmoid-Activated Nets
2.400
£ 2.300
2.275
2.375
K
5 2.350
,£ 2.325
2.250
O 500 IOOO 1500 2000 2500 3000 3500 4000
Iterations
Layer
MMC Loss + ReLU-Activated Nets
(b)
sso^∣ ctc≡-2h
Figure 10: Vanishing gradient experiment for different losses and nonlinear activation functions.
MMC Loss + Sigmoid-Activated Nets
15.0
812∙5
-I
CT 10.0
`æ 7.5
-5.0
2.5
0.25
0.20
0.15
0.10
0.05
0.00
・ SGD-VGR
IMl EKFAC
DDPNOpt
_L
7
9
5
Layer
τT
MMC Loss + Sigmoid-Activated Nets
■ SGD
I	I Adam
I	I RMSprop
l~l DDPNOpt*
T卓可


IOOO 2000	3000	4000
Iterations
1	3	5	7	9
Layer
Figure 11:	Vanishing gradient experiment for other optimizers. The legend “DDPNOpt*” denotes
DDPNOpt with adaptive diagonal matrix.
109 8 7
)SSo-I 6ucraJ.L
0	1000	2000	3000	4000
109 8 7
b)SSo-I 6ucraJ-L
(C)
10
9
8
7
6
(d)
O	IOOO 2000	3000	4000
Iterations
SSOl 6u-UIeJ-L
Iterations
EKFAC-lr0.03
DDPNOpt-lrO.03
DDPNOpt2nd-lr0.03
O IOOO 2000	3000	4000
Iterations
109 8 7
SSOI 6U⊂∙≡JJ.
Figure 12:	Vanishing gradient experiment for different learning rate setups.
18