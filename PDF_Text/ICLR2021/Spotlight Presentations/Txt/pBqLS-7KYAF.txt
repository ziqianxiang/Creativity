Published as a conference paper at ICLR 2021
Sparse Quantized Spectral Clustering
Zhenyu Liao
ICSI and Department of Statistics
University of California, Berkeley, USA
zhenyu.liao@berkeley.edu
Romain Couillet
G-STATS Data Science Chair, GIPSA-lab
University Grenobles-Alpes, France
romain.couillet@gipsa-lab.grenoble-inp.fr
Michael W. Mahoney
ICSI and Department of Statistics
University of California, Berkeley, USA
mmahoney@stat.berkeley.edu
Ab stract
Given a large data matrix, sparsifying, quantizing, and/or performing other entry-
wise nonlinear operations can have numerous benefits, ranging from speeding up
iterative algorithms for core numerical linear algebra problems to providing non-
linear filters to design state-of-the-art neural network models. Here, we exploit
tools from random matrix theory to make precise statements about how the eigen-
spectrum of a matrix changes under such nonlinear transformations. In particular,
we show that very little change occurs in the informative eigenstructure, even
under drastic sparsification/quantization, and consequently that very little down-
stream performance loss occurs when working with very aggressively sparsified
or quantized spectral clustering problems. We illustrate how these results depend
on the nonlinearity, we characterize a phase transition beyond which spectral clus-
tering becomes possible, and we show when such nonlinear transformations can
introduce spurious non-informative eigenvectors.
1 Introduction
Sparsifying, quantizing, and/or performing other entry-wise nonlinear operations on large matrices
can have many benefits. Historically, this has been used to develop iterative algorithms for core
numerical linear algebra problems (Achlioptas & McSherry, 2007; Drineas & Zouzias, 2011). More
recently, this has been used to design better neural network models (Srivastava et al., 2014; Dong
et al., 2019; Shen et al., 2020). A concrete example, amenable to theoretical analysis and ubiquitous
in practice, is provided by spectral clustering, which can be solved by retrieving the dominant eigen-
vectors of XTX, for X = [x1, . . . , xn] ∈ Rp×n a large data matrix (Von Luxburg, 2007). When
the amount of data n is large, the Gram “kernel” matrix XT X can be enormous, impractical even to
form and leading to computationally unaffordable algorithms. For instance, Lanczos iteration that
operates through repeated matrix-vector multiplication suffers from an O(n2) complexity (Golub &
Loan, 2013) and quickly becomes burdensome.
One approach to overcoming this limitation is simple subsampling: dividing X into subsamples of
size εn, for some ε ∈ (0, 1), on which one performs parallel computation, and then recombining.
This leads to computational gain, but at the cost of degraded performance, since each data point xi
looses the cumulative effect of comparing to the whole dataset. An alternative cost-reduction proce-
dure consists in uniformly randomly “zeroing-out” entries from the whole matrix XT X, resulting in
a sparse matrix with only an ε fraction of nonzero entries. For spectral clustering, by focusing on the
eigenspectrum of the “zeroed-out” matrix, Zarrouk et al. (2020) showed that the same computational
gain can be achieved at the cost of a much less degraded performance: for n/p rather large, almost
no degradation is observed down to very small values of ε (e.g., ε ≈ 2% for n/p & 100).
Previous efforts showed that it is often advantageous to perform sparsification/quantization in a non-
uniform manner, rather than uniformly (Achlioptas & McSherry, 2007; Drineas & Zouzias, 2011).
The focus there, however, is often on (non-asymptotic bounds of) the approximation error between
1
Published as a conference paper at ICLR 2021
the original and the sparsified/quantized matrices. This, however, does not provide a direct access to
the actual performance for spectral clustering or other downstream tasks of interest, e.g., since the
top eigenvectors are known to exhibit a phase transition phenomenon (Baik et al., 2005; Saade et al.,
2014). That is, they can behave very differently from those of the original matrix, even if the matrix
after treatment is close in operator or Frobenius norm to the original matrix.
Here, we focus on a precise characterization of the eigenstructure of XTX after entry-wise non-
linear transformation such as sparsification or quantization, in the large n, p regime, by performing
simultaneously non-uniform sparsification and/or quantization (down to binarization). We consider
a simple mixture data model with X 〜 N(±μ,Ip) and let K ≡ f (XTX/√p)/√p, where f is
an entry-wise thresholding/quantization operator (thereby zeroing-out/quantizing entries of XT X);
and we prove that this leads to significantly improved performances, with the same computational
cost, in spectral clustering as uniform sparsification, but for a much reduced cost in storage induced
by quantization. The only (non-negligible) additional cost arises from the extra need for evaluating
each entry of XTX. Our main technical contribution (of independent interest, e.g., for those inter-
ested in entry-wise nonlinear transformations of feature matrices) consists in using random matrix
theory (RMT) to derive the large n, P asymptotics of the eigensPectrUm of K = f (XTX/√p)/√p
for a wide range of functions f, and then comparing to previously-established results for uniform
subsampling and sparsification in (Zarrouk et al., 2020). Experiments on real-world data further
corroborate our findings.
Our main contributions are the following.
1.	We derive the limiting eigenvalue distribution of K as n, p → ∞ (Theorem 1), and we identify:
(a)	the existence of non-informative and isolated eigenvectors of K for some f (Corollary 1);
(b)	in the absence of such eigenvectors, a phase transition in the dominant eigenvalue-eigenvector
(^，V) pair (Corollary 2): if the Signal-to-noise ratio (SNR) kμk2 of the data exceeds a certain
threshold γ, then λ becomes isolated from the main bulk (Von Luxburg, 2007; Joseph & Yu,
2016; Baik et al., 2005) and V contains data class-structure information exploitable for cluster-
ing; if not, then V contains only noise and is asymptotically orthogonal to the class-label vector.
2.	Letting f be a sparsification, quantization, or binarization operator, we show:
(a)	a selective non-uniform sparsification operator, such that XT X can be drastically sparsified
with very little degradation in clustering performance (Proposition 1 and Section 4.2), which
significantly outperforms the random uniform sparsification scheme in (Zarrouk et al., 2020);
(b)	for a given matrix storage budget (i.e., fixed number of bits to store K), an optimal design of the
quantization/binarization operators (Proposition 2 and Section 4.3), the performances of which
are compared against the original XT X and its sparsified but not quantized version.
For spectral clustering, the surprisingly small performance drop, accompanied by a huge reduction
in computational cost, contributes to improved algorithms for large-scale problems. More generally,
our proposed analysis sheds light on the effect of entry-wise nonlinear transformations on the eigen-
spectra of data/feature matrices. Thus, looking forward (and perhaps more importantly, given the
use of nonlinear transformations in designing modern neural network models as well as the recent
interest in applying RMT to neural network analyses (Dobriban et al., 2018; Li & Nguyen, 2018;
Seddik et al., 2018; Jacot et al., 2019; Liu & Dobriban, 2019)), we expect that our analysis opens
the door to improved analysis of computationally efficient methods for large dimensional machine
learning and neural network models more generally.
2 System model and preliminaries
Basic setup. Let X1, . . . , Xn ∈ Rp be independently drawn (not necessarily uniformly) from a
two-class mixture of C1 and C2 with
Ci :	Xi	=	-μ +	Zi,	C :	Xi	=+μ +	Zi	(1)
with Zi ∈ Rp having i.i.d. zero-mean, unit-variance, κ-kurtosis, sub-exponential entries, μ ∈ Rp
such that kμk2 → P ≥ 0 as P → ∞, and V ∈ {±1}n with [v]i = -1 for Xi ∈ Ci and +1 for
Xi ∈ C2.1 The data matrix X = [xi,..., xn] ∈ Rp×n can be compactly written as X = Z + μvT
1The norm ∣∣ ∙ k is the Euclidean norm for vectors and the operator norm for matrices.
2
Published as a conference paper at ICLR 2021
for Z = [zι,..., Zn] so that kvk = √n and both Z, μvτ have operator norm of order O(√p) in the
n 〜p regime. In this setting, the Gram (or linear kernel) matrix XTX achieves optimal clustering
performance on the mixture model (1); see Remark 1 below. However, it consists of a dense n × n
matrix, which becomes quickly expensive to store or to perform computation on, as n increases.
Thus, we consider instead the following entry-wise nonlinear transformation of XTX:
K = {δi=j f (XTXj/6)/^}nj=i	⑵
for f : R → R satisfying some regularity conditions (see Assumption 1 below), where δi6=j equals
1 for i = j and equals 0 otherwise. The diagonal elements f (XTXi/√p) (i) bring no additional
information for clustering and (ii) do not scale properly for P large (xtXi / √p = O (√p) instead of
O(1)). Thus, following (El Karoui, 2010; Cheng & Singer, 2013), they are discarded.
Most of our technical results hold for rather generic functions f, e.g., those of interest beyond
sparse quantized spectral clustering, but we are particularly interested in f with nontrivial numerical
properties (e.g., promoting quantization and sparsity):
Sparsification:	fι(t) = t ∙ 1∣t∣>√2s	(3)
Quantization:	f2⑴=22-M(bt ∙ 2M-^√2sc +1/2) ∙ 1∣t∣≤√2s + Sign⑴∙ 1∣t∣>√2s (4)
Binarization:	f3(t) = sign(t) ∙ 1∣t∣>√2s .	(5)
Here, s ≥ 0 is some truncation threshold, and M ≥ 2 is a number of information bits.2 The
visual representations of these fs are given in Figure 1-(left). For f3, taking s → 0 leads to the sign
function sign(t). In terms of storage, the quantization f2 consumes 2M-2 + 1 bits per nonzero entry,
while the binarization f3 takes values in {±1, 0} and thus consumes 1 bit per nonzero entry.
Random matrix theory. To provide a precise description of the eigenspectrum of K for the non-
linear f of interest, to be used in the context of spectral clustering, we will provide a large dimen-
sional characterization for the resolvent of K, defined for z ∈ C \ R+ as
Q(z) ≡ (K-zIn)-1.	(6)
This matrix, which plays a central role in RMT analysis (Bai & Silverstein, 2010), will be used in
two primary ways. First, the normalized trace ɪ tr Q(Z) is the so-called Stieltjes transform of the
eigenvalue distribution ofK, from which the eigenvalue distribution can be recovered, and be further
used to characterize the phase transition beyond which spectral clustering becomes theoretically
possible (Corollary 2). Second, for (λ, v), an isolated eigenvalue-eigenvector Pair of K, and a ∈
Rn, a deterministic vector, by Cauchy,s integral formula, the “angle” between V and a is given by
IvTa|2 = - 2∏ι jΓ(^) aτQ(z)a dz, where「(入)is a positively oriented contour surrounding 入 only.
Letting a = v, this will be exploited to characterize the spectral clustering error rate (Proposition 1).
From a technical perspective, unlike linear random matrix models, K (and thus Q(z)) involves non-
linear dependence between its entries. To break this difficulty, following the ideas of (Cheng &
Singer, 2013), We exploit the fact that, by the central limit theorem, ZTZj/√p → N(0,1) in distri-
bution as p → ∞. As such, up to μvT, which is treated separately with a perturbation argument,
the entries of K asymptotically behave like a family of dependent standard Gaussian variables to
which f is applied. Expanding f in a series of orthogonal polynomials with respect to the Gaus-
sian measure allows for “unwrapping” this dependence. A few words on the theory of orthogonal
polynomials (Andrews et al., 1999) are thus convenient to pursue our analysis.
Orthogonal polynomial framework. For a probability measure μ, let {Pl(χ), l ≥ 0} be the
orthonormal polynomials with respect to hf, g〉≡ f fgdμ obtained by the Gram-Schmidt procedure
on the monomials {1, x, x2, . . .}, such that P0(x) = 1, Pl is of degree l and hPl1 , Pl2i = δl1-l2. By
the Riesz-Fischer theorem (Rudin, 1964, Theorem 11.43), for any function f ∈ L2(μ), the set of
square-integrable functions with respect to〈•，•)，one can formally expand f as
∞
f (X)〜X alPI(X),
l=0
al
/ f(x)Pι (x)μ(dx)
(7)
2Also, here, 1•」denotes the floor function, while erf(x) = √∏ Rx e-t2dt and erfc(x) = 1 — erf(x) denotes
the error function and complementary error function, respectively.
3
Published as a conference paper at ICLR 2021
-√2s	0	√2s
Figure 1: Visual representations of different functions f defined in (3)-(5) (left) and their corre-
sponding parameters a1 and ν (right) (a2 = 0 for each of these fs) defined in Assumption 1.
f	aι	V
ɪ	-erfc(s) + 2se-s2 ∕√π ~	erfc(s) + 2se-s2 ∕√π
f2	《∏ 21-M(1 + e-s2 M-2	k2s2 + Pk=1 -1 2e-…)	M 1 - 24M-1 erf(s) P2M-2-1 k erf(ks∙22-M) -2^k = 1	22M-5
f	e-s2 p2∕π	erfc(s)
where “f 〜P∞=o aιPi ” indicates that kf - PL=O aιPιk → 0 as L → ∞ With ∣∣f k2 = hf, f).
To investigate the asymptotic behavior of K, as n, p → ∞, we make the following assumption on f.
Assumption 1 (Square-integrable in Gauss space). Let ξp = ZTZj/√p and Pι,p(x) be the orthonor-
mal polynomials with respect to the measure μp of ξp. For f ∈ L2 (μp), f (x)〜 Pi∞=0 ai,pPi,p(x)
with aι,p in (7) such that (i) E∞=0 aι,pPι,p(x)μp (dx) converges in L2(μp) to f (x) uniformly over
large p; and (ii) asp → ∞, i∞=1 ai2,p → ν and, for l = 0, 1, 2, ai,p → ai converges with a0 = 0.
Since ξp → N(0, 1) in distribution, the parameters a0, a1, a2 and ν are simply moments of the
standard Gaussian measure involving f. More precisely, for ξ 〜N(0,1),
ao =	E[f(ξ)],	aι	= E[ξf(ξ)],	√‰ =	E[ξ2f(ξ)]	- a。, V =	E[f2(ξ)	- a0]	≥	af	+ a].	(8)
Imposing the condition a。= 0 simply discards the non-informative rank-one matrix a01n1T∕√p
from K. The three parameters (a1, a2, ν) are of crucial significance in determining the spectral
behavior of K (see Theorem 1 below).The sparse f1, quantized f2, and binary f3 of our primary
interest all satisfy Assumption 1 (counterexample exists though, for example f(x) = eP(x) for
polynomial P(x) of degree greater than two), with their corresponding a2 = 0 (as we shall see in
Corollary 1 below, this is important for the spectral clustering use case) and a1, ν given in Figure 1-
(right). With these preliminary comments, we are in position to present our main technical results.
3 Main Technical Results
Our main technical result, from which our performance-complexity trade-off analysis will follow,
provides an asymptotic deterministic equivalent Q(z) for the random resolvent Q, defined in (6).
(A deterministic equivalent is a deterministic matrix Q(Z) such that, for any deterministic sequence
of matrices An ∈ Rn×n and vectors an , bn ∈ Rn of bounded (spectral and Euclidean) norms,
n tr An(Q(Z) - Q(Z)) → 0 and aT(Q(z) - Q(z))bn → 0 almost surely as n,p → ∞. We denote
this relation Q(Z) — Q(z).) This is given in the following theorem. The proof is in Appendix A.1.
Theorem 1 (Deterministic equivalent). Let n,p → ∞ with p/n → c ∈ (0, ∞), let Q(Z) be defined
in (6), and let =[∙] denote the imaginary part ofa complex number. Then, under Assumption 1,
Q(Z)- Q(Z)= m(z)I -VΛ(z)Vt Λ(z) =	θ(Z)m2(Z)	V In")\" m(Z)
Q(Z) — Q(Z)= m(Z)In V 八(Z) V ,八(Z)= [vTιnθ(z)Ω(z) m(Z)	(VT 1n)R(z)Q2(z) - Ω(z)J ,
with √nV = [v，1n], ω(Z) = 2c2⅛(K-mm2)(z), Θ(z) = c+αιm(z)(1+kμk2)λ-+αμklμk2Ω(z)(vτ 1n)2∕n2 ,
for K the kurtosis ofthe entries of Z, and m(Z) the unique solution, such that =[m(Z)] ∙=[z] ≥ 0, to
—
1	a12 m(Z)	ν - a21
m(Z) = z + C + aιm(z) + ^^m(Z).
(9)
The major implication of Theorem 1 is that, the spectral behavior of the matrix K (e.g., its eigenvalue
distribution and the isolated eigenpairs as discussed after (6)) depends on the nonlinear f only via
the three parameters a1, a2, ν defined in (8). More concretely, the empirical spectral measure ωn =
1 ∑n=ι δλ. i(K) ofK has a deterministic limit ω as n,p → ∞, uniquely defined through its Stieltjes
transform m(Z) ≡	(t - Z)-1ω(dt) as the solution to (9).3 This limiting measure ω does not
3For m(z) the Stieltjes transform of a measure ω, ω can be obtained via ω([a, b]) = limy/ ∏ Ra =[m(x +
ιy)]dx for all a < b continuity points of ω.
4
Published as a conference paper at ICLR 2021
Figure 2: (Left) Histogram of eigenvalues of K (blue) versus the limiting spectrum and spikes
(red). (Right) Eigenvectors of the largest (top) and second largest (bottom) eigenvalues of K
(blue), versus the rescaled class label av/√n (red, from Corollary 2). f (t) = sin(t) - 3 cos(t) +
3∕√e, P = 800, n = 6400, μ = 1.1 ∙ 1p∕√p, V = [-1n/2； ln/2] on Student-t data with K = 5.
depend on the law of the independent entries of Z, so long that they are sub-exponential, with zero
mean and unit variance. In particular, taking a1 = 0 in (9) gives the (rescaled) Wigner semi-circle
law ω (Wigner, 1955), and taking V = al (i.e., a = 0 for l ≥ 2) gives the Marcenko-Pastur law
ω (Marcenko & Pastur, 1967). See Remark 2 in Appendix A.2 for more discussions.
Spurious non-informative spikes. Going beyond just the limiting spectral measure of K, Theo-
rem 1 also shows that isolated eigenvalues (often referred to as spikes) may be found in the eigen-
spectrum ofK at very specific locations. Such spikes and their associated eigenvectors are typically
thought to provide information on the data class-structure (and they do when f is linear). However,
when f is nonlinear, this is not always the case: it is possible that not all these spikes are “useful”
in the sense of being informative about the class structure. This is made precise in the following,
where We assume μ = 0, i.e., that there is no class structure. The proof is in Appendix A.2.
Corollary 1 (Non-informative jpikes). Assume μ = 0, κ = 1 and a2 = 0, so that in the notations
of Theorem 1, Θ(z) = 0 and Q(Z) = m(z) + Ω(z) 11n1T for Ω(z) defined in Theorem 1. Then,
2 ~2
if x± ≡ ±α1 Jκ-1 satisfies aιx± = ±1 and (V — a2)x± +(1+：[±)2 < 1/c, two eigenvalues
of K converge to z± = — cχ1^ — 1；：：：± — (V — a1)x± away from the support of ω. If instead
x± = ±1∕aι for aι = 0, then a single eigenvalue of K isolates with limit Z =—言—
：1 (2-c)
-2C-
Corollary 1 (combined with Theorem 1) says that, while the limiting spectrum ω is universal with re-
spect to the distribution of the entries ofZ, the existence and position of the spurious non-informative
spikes are not universal and depend on the kurtosis κ of the distribution. (See Figure 12 in Ap-
pendix A.2 for an example.) This is far from a mathematical curiosity. Given how nonlinear
transformations are used in machine learning practice, being aware of the existence of spurious
non-informative spikes in the eigenspectrum of K (well separated from the bulk of eigenvalues, but
that correspond to random noise instead of signal), as a function of properties of the nonlinear f, is
of fundamental importance for downstream tasks. For example, for spectral clustering, their associ-
ated eigenvectors may be mistaken as informative ones by spectral clustering algorithms, even in the
complete absence of classes. This is confirmed by Figure 2 where two isolated eigenvalues (on the
right side of the bulk) are observed, with only the second largest one corresponding to an eigenvector
that contains class-label information. For further discussions, see Appendix A.2 and A.3.
Informative spikes. From Theorem 1, we see that the eigenspectrum of K depends on f only via
a1, a2, and V. In particular, a1 and V determine the limiting spectral measure ω. From Corollary 1,
we see that a2 contributes by (i) introducing (at most two) non-informative spikes and (ii) reducing
the ratio a1/V (since V = Pi≥1 ai2), thereby necessarily enlarging the support of ω (see Remark 2
in Appendix A.2 for more details). Taking a2 = 0 thus reduces the length of the support ofω and, as
such, maximizes the “chance” of the appearance of an informative spike (the eigenvector of which
is positively correlated with the label vector V). See Remark 1 below for a more precise statement.
In particular, by taking a2 = 0 and a1 6= 0, we obtain only informative spikes, and we can charac-
terize a phase transition depending on the SNR ρ. The proof of the following is in Appendix A.3.
Corollary 2 (Informative spike and a phase transition). For a1 > 0 and a2 = 0, let
F(x) = x4 + 2x3 +(1 — 2) x2 — 2cx — c,	G(X) = — (1 + x) + — + ——— —--, (10)
a12	c	x a1	1 + x
5
Published as a conference paper at ICLR 2021
and let Y be the largest real solution to F(Y) = 0. Then, under Assumption 1, we have √c ≤ Y ≤
√ΣV∕aι, and the largest eιgenpaιr (λ, V) of K satisfies
ʌ
λ → λ
G(ρ)
G(Y)
ρ>γ,	lvTvl2 → α = /P(F+))3	P>Y，
ρ ≤ Y; n	0	ρ ≤ Y;
(11)
almostSurelyaS n,p → ∞, p/n → c ∈ (0, ∞), where we recall P ≡ limp→∞ ∣∣μk2 and ∣∣vk = √n.
Without loss of generality, we discuss only the case a1 > 0.4 For a1 > 0, both F(x) and G(x)
are increasing functions on x ∈ (Y, ∞). Then, as expected, both λ and α increase with the SNR
ρ. Moreover, the phase transition point Y is an increasing function of C and of ν∕O2. As such, the
optimal f in the sense of the smallest phase transition point is the linear function f (t) = t with
aι = V = 1 and Y = √C. This recovers the classical random matrix result in (Baik et al., 2005).
4 Clustering performance of sparse and quantized operators
We start, in Section 4.1, by providing a sharp asymptotic characterization of the clustering error
rate and demonstrating the optimality of the linear function under (1). Then, in Section 4.2, we
discuss the advantageous performance of the proposed selective sparsification approach (with f1)
versus the uniform or subsampling approach studied previously in (Zarrouk et al., 2020). Finally, in
Section 4.3, we derive the optimal truncation threshold sopt, for both quantized f2 and binary f3, so
as to achieve an optimal performance-complexity trade-off for a given storage budget.
4.1	Performance of spectral clustering
The technical results in Section 3 provide conditions under which K admits an informative eigen-
vector V that is non-trivially correlated with the class label vector V (and thus that is exploitable for
spectral clustering) in the n,p → ∞ limit. Since the exact (limiting) alignment ∣vτV∣ is known,
along with an additional argument on the normal fluctuations of V, We have the following result for
the performance of the spectral clustering method. The proof is in Appendix A.4.
Proposition 1 (Performance of spectral clustering). Let Assumption 1 hold, let a1 > 0, a2 = 0, and
let Ci = sign([V]i) be the estimate ofthe underlying class Ci of Xi, with the convention V TV ≥ 0 ,for
V the top eigenvector of K. Then, the (average) misclassification rate satisfies ɪ En=I δg=c, →
2 erfc( ∖J ɑ∕(2 - 2a)), almost surely, as n,p → ∞ ,for α ∈ [0,1) defined in (11).
Recall from Corollary 2 that, for a2 = 0, the nonlinear function f (e.g., f1, f2, f3) acts on the
(statistical behavior of the) isolated eigenvector V, and thus on the spectral clustering performance
per Proposition 1, only through the ratio ν∕a12. It thus suffices to evaluate the ratio ν∕a12 of different
f and compare, for instance to that of the linear f(t) = t corresponding to the original XTX matrix.
Despite being asymptotic results valid in the n, p → ∞ limit, the results of Proposition 1 and
Corollary 2 closely match empirical results for moderately large n, p only in the hundreds. This
is illustrated in Figure 3. Proposition 1 further confirms that the misclassification rate, being a
decreasing function ofα, increases with ν∕a12 (for C andρ fixed). This leads to the following remark.
Remark 1 (Optimality of linear function). Since both the phase transition point Y and the misclas-
sification rate grow with ν∕a12, the linear function f(t) = t with the minimal ν∕a12 = 1 is optimal
in the sense of (i) achieving the smallest SNR ρ or the largest ratio C = lim p∕n (i.e., the fewest
samples n) necessary to observe an informative (isolated) eigenvector, and (ii) upon existence of
such an isolated eigenvector, reaching the lowest classification error rate.
According to Remark 1, any f with ν∕a12 > 1 induces performance degeneration (compared to
the optimal linear function). However, by choosing f to be one of the functions f1 , f2 , f3 defined
in (3)-(5), one may trade clustering performance optimality for reduced storage size and computa-
tional time. Figure 4 displays the theoretical decay in clustering performance and the gain in storage
size of the sparse f1, quantized f2, and binary f3, when compared to the optimal but “dense” linear
function. As s → 0, both performance and storage size under f1 naturally approach those of the
4Otherwise we could consider -K instead of K and the largest eigenvalue becomes the smallest one.
6
Published as a conference paper at ICLR 2021
2PJ .Jissei。-2≡
--A
SNR ρ
Truncation threshold s
Figure 3: (Left) Empirical alignment ∣Vτv∣2∕n (green crosses) and misclassification rate (purple
circles) in markers versus their limiting behaviors in lines, for f(t) = sign(t), as a function of
SNR ρ. (Right) Misclassification rate as a function of the truncation thresholds S of sparse f1
(blue crosses) and binary f3 (red circles) with P = 4. Here, P = 512, n = 256, μ a N(0, Ip),
v = [-1n/2; 1n/2] on Gaussian data. The results are obtained by averaging over 250 runs.
O
42
11
..
00
0.14
0.13
0.12
0	0.43 0.69	1
Truncation threshold s
101
0 0 -
110
1
ezis egarotS
Truncation threshold s
5
*
1
1
Truncation threshold s
2
.
0
5
*
O
Figure 4: Clustering performance (left, a zoom-in in middle) and storage size (MB) (right) of f1
(blue), f2 with M = 2 (green), f3 (red), and linear f(t) = t (black), versus the truncation threshold
s, for SNR ρ = 2, c = 1/2 and n = 103, with 64 bits per entry for non-quantized matrices.
linear function. This is unlike f2 or f3, which approach the sign function. For s 1, the perfor-
mance under sparse f1 becomes comparable to that of binary f3 (which is significantly worse than
quantized but non-sparse f2) but for a larger storage cost. In particular, using f2 or f3 in the setting
of Figure 4, one can reduce the storage size by a factor of 32 or 64 (in the case of IEEE standard
single- or double-precision floating-point format), at the price of a performance drop less than 1%.
4.2	Comparison to uniform sparsification and subsampling
From Figure 4, we see that the classification error and storage gain of the sparse f1 increase mono-
tonically, as the truncation threshold s grows. For f1, the number of nonzero entries of K is approx-
imately erfc(s)n2 with truncation threshold s. Thus, the sparsity level εselec = erfc(s) ∈ [0, 1] can
be defined and compared to uniform sparsification or subsampling approaches.
Recall (from the introduction) that the cost of spectral clustering may be reduced by subsampling the
whole dataset in 1∕εsub chunks of nεsub data vectors each. Alternatively, as investigated recently
in (Zarrouk et al., 2020), the cost can be reduced by uniformly zeroing-out XTX with a symmetric
random mask matrix B, with Bij 〜 Bern(εunif) for 1 ≤ i < j ≤ n and Bii = 0.On average,
a proportion 1 - εunif of the entries of XTX is set to zero, so that εunif ∈ [0, 1] controls the
sparsity level (and thus the storage size as well as computational time). Similar to our Corollary 2,
the associated eigenvector alignment α (and thus the clustering accuracy per Proposition 1) in both
cases can be derived. Specifically, taking εunif = a12∕ν in (Zarrouk et al., 2020, Theorem 3.2), we
obtain the same F (x) as in our Corollary 2 and therefore the same phase transition point γ and
eigenvector alignment α. As for subsampling, its performance can be obtained by letting a21 = ν
and changing c into c∕εsub in the formulas ofF(x) and G(x) of our Corollary 2. Consequently, the
same clustering performance is achieved by either uniform or selective sparsification (using f1) with
εunif = ai/v = erfc(s) + 2se /√π > erfc(S) = εselec,	(12)
and our proposed selective sparsification thus leads to strictly sparser matrices. Moreover, their
ratio r(s) = erfc(s)∕(erfc(s) + 2se-s2∕√π) is a decreasing function of S and approximates as
7
Published as a conference paper at ICLR 2021
oitar / noitropor
Figure 5: (Left) Proportion of nonzero entries with uniform versus selective sparsification f1 and
their ratio r(s), as a function of the truncation threshold s. (Right) Comparison of 1%, 10% clas-
sification error and phase transition (i.e., 50% error) curves between subsampling (green), uniform
(blue) and selective sparsification f1 (red), as a function of sparsity level ε and SNR ρ, for c = 2.
r(s)〜(1 + s2)-1∕2 for S》1,5 meaning that the gain in storage size and computational time is
more significant as the matrix becomes sparser. This is depicted in Figure 5-(left).
Fixing α in Corollary 2 to achieve a given clustering performance level (via Proposition 1), one
may then retrieve “equi-performance” curves in the (ε, ρ)-plane, for uniform sparsification, selec-
tive sparsification, and subsampling. This is displayed in Figure 5-(right), showing that a dramatic
performance gain is achieved by the proposed selective sparsification f1 . Besides, here for c = 2, as
much as 80% sparsity could be obtained with selective sparsification at constant SNR ρ, with virtu-
ally no performance loss (red curves are almost flat on ε ∈ [0.2, 1]). This fails to hold for uniform
sparsification (Zarrouk et al. (2020) obtain such a result only when c . 0.1) or subsampling.
4.3	Optimally quantized and binarized matrices
From Figure 4, we see that the classification errors of the quantized f2(M; s; t) and binarized f3(s; t)
do not increase monotonically with the truncation threshold s. It can be shown (and also visually
confirmed in Figure 4) that, for a given M ≥ 2, the ratio ν∕a2 of both f2 and f3 is convex in S and
has a unique minimum. This leads to the following optimal design result for f2 and f3 , respectively,
the proof of which is straightforward.
Proposition 2 (Optimal design of quantized and binarized functions). Under the assumptions and
notations of Proposition 1, the classification error rate is minimized at S = Sopt with
1.	Sopt the unique solution to a1 (Sopt)ν 0 (Sopt) = 2a01 (Sopt)ν (Sopt) for quantized f2, with a01(S) and
ν0(S) the corresponding derivatives with respect to S in Figure 1-(right); and
2.	SoPt = exp(-s2pt)∕(2√∏ erfc(sopt)) ≈ 0.43 for binary f3, with level of sparsity ε ≈ 0.54.
Therefore, the optimal threshold Sopt for quantized f2 or binary f3 under (1) is problem-independent,
as it depends neither on P nor on c. In particular, note that (i) the binary f3(sopt; ∙) is consistently
better than f (t) = sign(t) for which ν∕a2 = π∕2 ≈ 1.57 > 1.24; and (ii) the performance of
quantized f2 can be worse, though very slightly, than that of binary f3 for small S, but significantly
better for not-too-small S. These are visually confirmed in the left and middle displays of Figure 4.
As already observed in Figure 4-(right), a significant gain in storage size can be achieved by using
f2 or f3 , versus the performance-optimal but dense linear function, with virtually no performance
loss. Figure 6 compares the performance of the optimally designed f2 and f3, to sparse f1 that has
approximately the same storage size.* 5 6 A significant drop in classification error is observed by using
quantization f2 or binarization f3 rather than sparsification f1. Also, the performances of f2 and
f3 are extremely close to the theoretical optimal (met by f(t) = t). This is further confirmed by
Figure 6-(right) where, for the optimal f2, the ratio ν∕a2 gets close to 1, for all M ≥ 5.
Figure 7 next evaluates the clustering performance, the proportion of nonzero entries in K, and
the computational time of the top eigenvector, for sparse f1 and binary f3, versus linear f(t) = t,
2
5We use here the asymptotic expansion erfc(s) = e√∏ [l + Pk=式-1)k ∙ ∙ 芸)—)].
6We set the truncation threshold s of f1 such that erfc(s) = 3/64, so that the storage size of the sparse f1
(64 bits per nonzero entry) is the same as the quantized f2 with M = 3 (with 3 bits per nonzero entry), which
is three times that of the binary f3 .
8
Published as a conference paper at ICLR 2021
1
-
0
1
00c
1
1
10
Figure 6:	Performance of f1 (blue), f2 with M = 3 (green) and f3 (red) of the same storage size,
versus SNR for c = 4 (left) and versus c for SNR ρ = 4 (middle). (Right) Optimal threshold sopt
(green) and (ν∕a1)min (purple) of f2 versus M. Curves for linear f (t) = t are displayed in black.
O
420
..
00
etar.fissalcsiM
---Sparse .♦
一-Binary ʃ
..J......
O
2
.porp oreznoN
O
186420
..............
0000
15
)s( emit.etupmo
1
.
00
5
O
1
5
1
5
O
1
Truncation threshold s	Truncation threshold s	Truncation threshold s
Figure 7:	Clustering performance of sparse f1 and binary f3 (left and middle), proportion of
nonzero entries and computational time of the top eigenvector for f3 (right), as a function of the
truncation threshold s on the MNIST dataset: digits (0, 1) (left) and (5, 6) (middle and right) with
n = 2 048 and performance of the linear function in black. Results averaged over 100 runs.
as a function of the truncation threshold s, on the popular MNIST datasets (LeCun et al., 1998).
Depending on (the SNR ρ of) the task, up to 90% of the entries can be discarded almost “for free”.
Moreover, the curves of the binary f3 appear strikingly close to those of the sparse f1, showing the
additional advantage of using the former to further reduce the storage size of K. More empirical
results on various datasets are provided in Appendix B to confirm our observations in Figure 7.
5 Concluding remarks
We have evaluated performance-complexity trade-offs when sparsifying, quantizing, and binarizing
a linear kernel matrix via a thresholding operator. Our main technical result characterizes the change
in the eigenspectrum under these operations; and we have shown that, under an information-plus-
noise model, sparsification and quantization, when carefully employed, maintain the informative
eigenstructure and incur almost negligible performance loss in spectral clustering. Empirical results
on real data demonstrate that these conclusions hold far beyond the present statistical model.
The proposed analysis can be extended in many ways, for instance by considering a multi-cluster
and more involved model than (1) as in (Liao & Couillet, 2019) (i.e., “generic” K-class Gaussian
mixture N(N。,Ca) for a ∈ {1,...,K}, which may help better interpret the empirical observations
in Figure 7 and Appendix B), by focusing on more general kernels beyond the current inner-product
type in (2), or by deriving non-asymptotic guarantees as in (Vankadara & Ghoshdastidar, 2020).
Our results open the door to theoretical investigation of a broad range of cost-efficient linear alge-
bra methods in machine learning, including subsampling techniques (Mensch et al., 2017; Roosta-
Khorasani & Mahoney, 2019), distributed optimization (Wang et al., 2018), randomized linear alge-
bra algorithms (Mahoney, 2011; Drineas & Mahoney, 2016), and quantization for improved training
and/or inference (Dong et al., 2019; Shen et al., 2020). Also, given recent interest in viewing neural
networks from the perspective of RMT (Li & Nguyen, 2018; Seddik et al., 2018; Jacot et al., 2019;
Liu & Dobriban, 2019; Martin & Mahoney, 2019; 2020), our results open the door to understand-
ing and improving performance-complexity trade-offs far beyond kernel methods (Rahimi & Recht,
2008; Jacot et al., 2018; Liu et al., 2020), e.g., to sparse, quantized, or even binary neural networks
(Hubara et al., 2016; Lin et al., 2017).
9
Published as a conference paper at ICLR 2021
Acknowledgments
We would like to acknowledge DARPA, IARPA (contract W911NF20C0035), NSF, and ONR via
its BRC on RandNLA for providing partial support of this work. Our conclusions do not necessarily
reflect the position or the policy of our sponsors, and no official endorsement should be inferred.
Couillet’s work is partially supported by MIAI at University Grenoble-Alpes (ANR-19-P3IA-0003)
and the HUAWEI LarDist project.
References
Dimitris Achlioptas and Frank McSherry. Fast computation of low-rank matrix approximations.
Journal ofthe ACM (JACM), 54(2):9-es, 2007.
George E. Andrews, Richard Askey, and Ranjan Roy. Special Functions, volume 71. Cambridge
University Press, 1999.
Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices,
volume 20. Springer, 2010.
Jinho Baik, Gerard Ben Arous, and Sandrine Peche. Phase transition of the largest eigenvalue for
nonnull complex sample covariance matrices. TheAnnalsofProbability, 33(5):1643-1697, 2005.
Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices. Random
Matrices: Theory and Applications, 2(04):1350010, 2013.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Yen Do and Van Vu. The spectrum of random kernel matrices: universality results for rough and
varying kernels. Random Matrices: Theory and Applications, 2(03):1350005, 2013.
Edgar Dobriban, Stefan Wager, et al. High-dimensional asymptotics of prediction: Ridge regression
and classification. The Annals of Statistics, 46(1):247-279, 2018.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. HAWQ: Hes-
sian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 293-302, 2019.
Petros Drineas and Michael W Mahoney. RandNLA: randomized numerical linear algebra. Com-
munications of the ACM, 59(6):80-90, 2016.
Petros Drineas and Anastasios Zouzias. A note on element-wise matrix sparsification via a matrix-
valued Bernstein inequality. Information Processing Letters, 111(8):385-389, 2011.
Noureddine El Karoui. On information plus noise kernel random matrices. The Annals of Statistics,
38(5):3191-3216, 2010.
Zhou Fan and Andrea Montanari. The spectral norm of random inner-product kernel matrices.
Probability Theory and Related Fields, 173(1-2):27-85, 2019.
Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University
Press, third edition, 2013.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
Neural Networks. In Advances in Neural Information Processing Systems, volume 29, pp. 4107-
4115, 2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
10
Published as a conference paper at ICLR 2021
Arthur Jacot, Franck Gabriel, and Clement Hongler. The asymptotic spectrum of the Hessian of
DNN throughout training. In International Conference on Learning Representations, 2019.
Antony Joseph and Bin Yu. Impact of regularization on spectral clustering. The Annals of Statistics,
44(4):1765-1791,2016.
Arun Kadavankandy and Romain Couillet. Asymptotic gaussian fluctuations of spectral clustering
eigenvectors. In 2019 IEEE 8th International Workshop on Computational Advances in Multi-
Sensor Adaptive Processing (CAMSAP), pp. 694-698. IEEE, 2019.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Ping Li and Phan-Minh Nguyen. On random deep weight-tied autoencoders: Exact asymptotic
analysis, phase transitions, and implications to training. In International Conference on Learning
Representations, 2018.
Zhenyu Liao and Romain Couillet. Inner-product kernels are asymptotically equivalent to binary
discrete kernels. arXiv preprint arXiv:1909.06788, 2019.
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In
Advances in Neural Information Processing Systems, pp. 345-353, 2017.
Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan AK Suykens. Random features for kernel
approximation: A survey in algorithms, theory, and beyond. arXiv preprint arXiv:2004.11154,
2020.
Sifan Liu and Edgar Dobriban. Ridge Regression: Structure, Cross-Validation, and Sketching. In
International Conference on Learning Representations, 2019.
Anna Lytova and Leonid Pastur. Central limit theorem for linear eigenvalue statistics of random
matrices with independent entries. The Annals of Probability, 37(5):1778-1840, 2009.
Michael W Mahoney. Randomized Algorithms for Matrices and Data. Foundations and Trends® in
Machine Learning, 3(2):123-224, 2011.
Vladimir A Marcenko and Leonid Andreevich Pastur. Distribution of eigenvalues for some sets of
random matrices. Mathematics of the USSR-Sbornik, 1(4):457, 1967.
Charles H Martin and Michael W Mahoney. Traditional and heavy tailed self regularization in neural
network models. In International Conference on Machine Learning, pp. 4284-4293, 2019.
Charles H Martin and Michael W Mahoney. Heavy-tailed universality predicts trends in test accu-
racies for very large pre-trained deep neural networks. In Proceedings of the 2020 SIAM Interna-
tional Conference on Data Mining, pp. 505-513. SIAM, 2020.
Arthur Mensch, Julien Mairal, Bertrand Thirion, and Gael Varoquaux. Stochastic subsampling for
factorizing huge matrices. IEEE Transactions on Signal Processing, 66(1):113-128, 2017.
Vinay Uday Prabhu. Kannada-MNIST: A new handwritten digits dataset for the Kannada language.
arXiv preprint arXiv:1908.01242, 2019.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177-1184, 2008.
Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled Newton methods. Mathematical
Programming, 174(1-2):293-326, 2019.
Walter Rudin. Principles of mathematical analysis, volume 3. McGraw-hill New York, 1964.
Alaa Saade, Florent Krzakala, and Lenka Zdeborova. Spectral clustering of graphs with the Bethe
Hessian. In Advances in Neural Information Processing Systems, pp. 406-414, 2014.
Mohamed El Amine Seddik, Mohamed Tamaazousti, and Romain Couillet. A kernel random matrix-
based approach for sparse PCA. In International Conference on Learning Representations, 2018.
11
Published as a conference paper at ICLR 2021
Mohamed El Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Romain Couillet. Random
matrix theory proves that deep learning representations of GAN-data behave as gaussian mixtures.
arXiv preprint arXiv:2001.08370, 2020.
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney,
and Kurt Keutzer. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT. In AAAI,
pp. 8815-8821,2020.
Jack W Silverstein and ZD Bai. On the empirical distribution of eigenvalues of a class of large
dimensional random matrices. Journal of Multivariate analysis, 54(2):175-192, 1995.
Jack W Silverstein and Sang-Il Choi. Analysis of the limiting spectral distribution of large dimen-
sional random matrices. Journal of Multivariate Analysis, 54(2):295-309, 1995.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Terence Tao, Van Vu, and Manjunath Krishnapur. Random matrices: Universality of ESDs and the
circular law. The Annals of Probability, 38(5):2023-2065, 2010.
Leena C Vankadara and Debarghya Ghoshdastidar. On the optimality of kernels for high-
dimensional clustering. In International Conference on Artificial Intelligence and Statistics, pp.
2185-2195. PMLR, 2020.
Dan Voiculescu. Addition of certain non-commuting random variables. Journal of functional anal-
ysis, 66(3):323-346, 1986.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395-416,
2007.
Shusen Wang, Fred Roosta, Peng Xu, and Michael W Mahoney. GIANT: Globally improved approx-
imate Newton method for distributed optimization. In Advances in Neural Information Processing
Systems, pp. 2332-2342, 2018.
Eugene P. Wigner. Characteristic vectors of bordered matrices with infinite dimensions. Annals of
Mathematics, 62(3):548-564, 1955.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Bench-
marking Machine Learning Algorithms, 2017.
Tayeb Zarrouk, Romain Couillet, Florent Chatelain, and Nicolas Le Bihan. Performance-complexity
trade-off in large dimensional statistics. In 2020 IEEE International Workshop on Machine Learn-
ing for Signal Processing (MLSP). IEEE, 2020.
12
Published as a conference paper at ICLR 2021
A Proofs and related discussions
Under the mixture model (1), the data matrix X ∈ Rp×n can be compactly written as
X = Z + μvT,	(13)
for Z ∈ Rp ×n having i.i.d. zero-mean, unit-variance, κ-kurtosis, sub-exponential entries and V ∈
{±1}n so that kvk = √n. Recall also the following notations:
K = {δi=jf(xTxj∕√P)∕√P}%=1, Q(Z) ≡ (K -ZIn)-1.	(14)
A.1 Proof of Theorem 1
The proof of Theorem 1 comes in the following two steps:
1.	show that the random quantities n tr AnQ(Z) and aTQ(z)bn of interest concentrate
around their expectations in the sense that
1tr An(Q(Z)- E[Q(z)]) → 0,	aT(Q(z) - E[Q(z)])bn → 0,	(15)
nn
almost surely as n,p → ∞; and
2.	show that the sought-for deterministic equivalent Q(Z) given in Theorem 1isan asymptotic
approximation for the expectation of the resolvent Q(Z) defined in (6) in the sense that
kE[Q] - Qk→ 0,	(16)
as n, p → ∞.
The concentration of trace forms in the first item has been established in (Cheng & Singer, 2013;
Do & Vu, 2013), and the bilinear forms follow similarly. Here we focus on the second item to show
that kE[Q] - Qk → 0 in the large n,p limit.
In the sequel, We use o⑴ and o∣∣∙∣∣ ⑴ for scalars or matrices of (almost surely if being random)
vanishing absolute values or operator norms as n,p → ∞.
To establish ∣∣E[Q] - Q∣∣ → 0, we need to show subsequently that:
1.	under (13), the random matrix K defined in (2) admits a spiked-model approximation, that is
K = K o + UAUT + Ok∙k(1),	(17)
for some full rank random (noise) matrix Ko and low rank (information) matrix UAUT to be
specified; and
2.	the matrix inverse (Ko - UAUT - ZIn)-1 can be decomposed with the Woodbury identity, so that
Q = (Ko -UAUT-ZIn)-1+ oHl(I) = Q0 -QOU(A-1 + UTQOU)TUTQo + oHl (1), (18)
with Qo(Z) ≡ (Ko - ZIn)-1; and
3.	the expectation of the right-hand side of (18) is close to Q in the large n,p limit, allowing us to
conclude the proof of Theorem 1.
To establish (17), we denote the “noise-only” null model with ∣∣μ∣ = 0 by writing K = Ko such that
[Ko]j = δi=j f(zTzj ∕√P)∕√p.	(19)
With a combinatorial argument, it has been shown in (Fan & Montanari, 2019) that
Ko - Ko----⅛-(ψ1T + 1nψT) → 0,	(20)
2p n
almost surely as n,p → ∞, for Ko such that (Ko - ZIn) 1 ≡ Qo(z) 什 m(∕)In and the random
vector ψ ∈ Rn with its i-th entries given by
[ψ]i = -p (kzik2 - E[kzik2]) = 7(Izik2 - p).
13
Published as a conference paper at ICLR 2021
Consider now the informative-plus-noise model K for X = Z + μvτ as in (13) with [v]i = ±1 and
∣∣vk = √n. It follows from (Liao & Couillet, 2019) that
K - K0 -	ap1	[v	zTμ]	kμk	0	μTz]	I →	0,	QI)
almost surely as n, p → ∞.
Combining (20) with (21), we obtain ∣∣K 一 Ko 一 UΛUT∣ → 0 almost surely as n,p → ∞, with
	U=	1 √P	[1n, v, ψ, Zτμ] ∈ Rn×4, Λ =	0 0 a √2 0	0	√2 aιkμk2	0 00 a1	0	0 ap 0 0	(22)
,~ and (K	0 一 ZIn)-1	≡	Qo(z)什 m(z)In. By the Woodbury identity, we write				
		Q	=(K 一 ZIn)T = (K0 + UΛUt	ZIn	JT + ok∙k ⑴		
			=Q o - Q oU(Λ-1 + UTQ oU)-1	UTQ o + 0k∙k(1)			(23)
with							
			-	m(z)	m(z) VTIn	0		0		
	-T~ ~		c	cn m(z) VTIn	m(z)	∩		0		
Λ-1	+ UTQ0U =		Cn	C	j、 0	0	(κ - 1) mz		0		+ ok∙k ⑴
			00	0		μT (P E[ZQ oZT])μ-		
where we use the fact that
E[ψ] = 0, E[ψψT] = (κ 一 1)In.
We need to evaluate the expectation PE[Z(Ko 一 ZIn)TZT]. This is given in the following lemma.
Lemma 1. Under the assumptions and notations of Theorem 1, we have
PE[Z(K0 - zIn)-1Zτ] = C+mα‰IP + ok∙k⑴.	(24)
ProofofLemma 1. For Qo = (Ko 一 ZIn)-1, we aim to approximate the expectation E[ZQ°ZT]∕p.
Consider first the case where the entries of Z are i.i.d. Gaussian, we can write the (i, i0) entry of
E[ZQoZt] with Stein,s lemma (i.e., E[xf (x)] = E[f 0(x)] for X 〜N(0,1)) as
E[ZQ oZT]ii0 = XX E[Zij [Q oZT]ji0 ] = XX e d[QZT]ji0
∂Zij
j=1	j=1
XX E [[Q0]jjδii0 + X 耍ZT
∂Zij
j =1	k=1	ij
We first focus on the term ∂Q0 jk by writing
∂Zij
r ~ r
∂[Q ojk
∂Zij
Q 0 ∂Zj Q 0
=XX -[Q0]jl d∂KZ0]m [Q0]mk
∂Z
jk	l,m=1	ij
where we recall [Ko]ij = δi=j f (ZTZ∕√p)j / √p so that for l = m we have
d[K0]m = 1 f0(ZTZ∕√P)lm dZTZm = 1 f0(ZTZ∕√P)im(δjiZim + ZTδjm)
∂Zij	p	∂Zij	p	i
and d[Kl]lm = 0 for l = m. We get
ΛΓO.1
X dQ0jk ZTiO = - ɪ X [Q 0]jj f 0(ZTZ∕√P)jm Zim [Q 0]mkZTi0 - 1 £ [Q θ]jlf 0(ZτZ∕√P)ljZT [Q θ]jk ZT
j,k ij	p j,k,m	p j,k,l
=一 1[Zdiag(f0(ZTZ∕√P)Qo1n)Q0Zτ]ii0 - 1[Z(Q0 Θ f0(ZτZ∕√P))QoZ%，
pp
14
Published as a conference paper at ICLR 2021
where f 0(ZτZ∕√p) indeed represents f 0(ZτZ∕√p) 一 diag(∙) in both cases.
For the first term, since f 0(ZτZ∕√p) 一 diag(∙) = aι1n1TT +。卜口 (√p), We have
1 f0(ZτZ∕√P)Q0 1n = a11TQ0ln ∙ 1n + O(PT/2) =	1n + O(PT/2)	(25)
p	pn	c
where O(P-1/2) is understood entry-wise. As a result,
1Zdiag(f0(ZTZ∕√P)Q0ln)Q0Zτ = aιmz) ∙ 1ZQ0ZT + o∣h∣(1).
P	cP
For the second term, since f (ZτZ∕√p) has O(1) entries and ∣∣ A Θ Bk ≤ √n∣∣A∣UBk for
A, B ∈ Rn×n , we deduce that
P∣Z(Q0 Θ f0(ZτZ∕√P))Q0Zτk = O(√p).
As a consequence, we conclude that
1	E[ZQ0Zτ] = 1 tr Q0 ∙ Ip 一 aimz) ∙ 1 E[ZQ0Zτ] + θk∙k ⑴
P	P	cP
that is
1	E[ZQ0Zτ]=	m(Z)	Ip + oHl (1)
P	c + a1m(z)
where we recall that tr Q0∕P = m(Z)∕c and thus the conclusion of Lemma 1 for the Gaussian case.
The interpolation trick (Lytova & Pastur, 2009, Corollaray 3.1) can then be applied to extend the
result beyond Gaussian distribution. This concludes the proof OfLemma 1.	口
Denote A = (Λ-1 + UTQ0U)-1, it follows from Lemma 1 that
EUAUT] = l(A11lnin + A12lnVT + A21viτ + A22vvτ + A33(κ — 1)In + A44kμk2In)
Pn	n
=p(A111n1n + A121nVT + A21v1T + A22vvT) + o∣H∣ ⑴
since ∣∣μ∣ = O(1) and ∣∣v∣ = O(√n). We thus deduce from (23) that
Q(z) » Q(z) = m(z)In-cm2(z)V A；； A；； VT
with √nV = [v, 1n]. Rearranging the expression we conclude the proof of Theorem 1.	口
A.2 Proof of Corollary 1 and related discussions
Consider the noise-only model by taking μ = 0 in Theorem 1. Then, we have K = K0 and
Θ(z) = 0, so that
Q(Z)= m(z) + Ω(z) ∙ 11n1TT,
nn
C(Z) =	a2(K — 1)m3(Z)
2c2 一 a2(κ 一 1)m2(z)
(26)
where we recall m(z ) is the solution to
m(z)
Z +	a；m(z)	+」m(z)) T
c + a1m(z)	c
(27)
Since the resolvent Q(z) is undefined for z ∈ R within the eigensupport ofK that consists of (i) the
main bulk characterized by the Stieltjes transform m(z) defined in (27) and (ii) the possible spikes,
we need to find the poles of Q(z) but not those of m(z) to determine the asymptotic locations of
the spikes that are away from the main bulk. Direct calculations show that the Stieltjes transforms
of the possible non-informative spikes satisfy
m± = ± J-2-j--
κ 一1 a2
(28)
15
Published as a conference paper at ICLR 2021
that are in fact the poles of Ω(z), for a2 = 0 and κ = 1. For K = 1 or a? = 0, Ω(z) has no
(additional) poles, so that there is (almost surely) no spike outside the limiting spectrum.
It is however not guaranteed that z ∈ R corresponding to (28) isolates from the main bulk. To this
end, we introduce the following characterization of the limiting spectral measure in (27), the proof
of which follows from previous work.
Corollary 3 (Limiting spectrum). Under the notations and conditions of Theorem 1, with proba-
bility one, the empirical spectral measure ωn = 1 En=I 方工认长。)of the noise-only model K° (and
therefore that of K as a low rank additive perturbation of K0 via (21)) converges weakly to a prob-
ability measure ω of compact support as n, p → ∞, with ω uniquely defined through its Stieltjes
transform m(z) solution to (27). Moreover,
1.	ifwe let supp(ω) be the support ofω, then
supp(ω) ∪ {0} = R \ {x(m) | m ∈ R \ {{-c/a1} ∪ {0}} and x0(m) > 0}	(29)
for x(m) the functional inverse of (27) explicitly given by
x(m) = -ɪ -	a2m	- Um,	(30)
m c+ a1m	c
2.	the measure ω has a density and its support may have up to four edges, with the associated
Stieltjes transforms given by the roots ofx0(m) = 0, i.e.,
x0(m)
m2
a12c	ν - a21
----------~^τ ---------
(c+ a1m)2 c
0.
(31)
1
—
The limiting spectral measure ω of the null model K0 was first derived in (Cheng & Singer, 2013)
for Gaussian distribution and then extended to sub-exponential distribution in (Do & Vu, 2013). The
fact that a finite rank perturbation does not affect the limiting spectrum follows from (Silverstein &
Bai, 1995, Lemma 2.6).
The characterization in (29) above follows the same idea as in (Silverstein & Choi, 1995, Theo-
rem 1.1), which arises from the crucial fact that the Stieltjes transform m(x) = (t - x)-1ω(dt)
of a measure ω is an increasing function on its domain of definition and so must be its functional
inverse x(m) given explicitly in (30). In plain words, Corollary 3 tells us that (i) depending on the
number of real solutions to (31), the support ofω may contain two disjoint regions with four edges,
and (ii) x ∈ R is outside the support of ω if and only if its associated Stieltjes transform m satisfies
x0(m) > 0, i.e., belonging to the increasing region of the functional inverse x(m) in (30). This
is depicted in Figure 8, where for the same function f (t) = max(t, 0) - 1/√2π with a1 = 1/2,
a2 = 1∕(2√π) and V = (π - 1)∕(2π), We observe in the top display a single region of ω for C = 2
and in the bottom display two disjoint regions (with thus four edges) for c = 1/10. The correspond-
ing (empirical) eigenvalue histograms and limiting laws are given in Figure 9. Note, in particular,
that the local extrema of the functional inverse x(m) in Figure 8 characterize the (possibly up to
four) edges of the support of ω in Figure 9.
According to the discussion above, it remains to check the sign of x0(m) for m = ± JK-ɪ ac- to see
if they correspond to isolated eigenvalues away from the support of ω . This, after some algebraic
manipulations, concludes the proof of Corollary 1.	□
Discussions. The limiting spectral measure in Corollary 3 is indeed a “mix” between the popular
MarCenkO-PaStUr and the Wigner,s semicircle law.
Remark 2 (From MarCenkO-PaStUr to semicircle law). As already pointed out in (Fan & Montanari,
2019), here the limiting spectral measure ω is the so-called free additive convolution (Voiculescu,
1986) ofthe SemiCirCIe and Marcenko-Pastur laws, weighted respectively by aι and ,ν — a2, i.e.,
ω = a1(ωMP,c-1
-1)田 J(V - a2)∕c ∙ ωsc
(32)
where we denote a1(ωMp,c-1 — 1) the law of aι(x — 1) for X 〜ωMp,c-ι and，(v — a2)∕c ∙ ωsc
the law of，(v — a2)∕c ∙ X for X 〜ωsc. Figure 10 COmPareS the eigenvalue distributions of Ko
16
Published as a conference paper at ICLR 2021
15
10
5
0
-5
m
Figure 8: Functional inverse x(m) for m ∈ R\{{—c∕αι} ∪ {0}}, With f (t) = max(t, 0) — 1/λ∕2π,
for c = 2 (above, with two edges) and c = 1/10 (bottom, with four edges). The support of ω can
be read on the vertical axes and the values ofx such that x0(m) = 0 are marked in green.
Figure 9: Eigenvalues of K with μ = 0 (blue) versus the limiting laws in Theorem 1 and
Corollary 3 (red) for P = 3 200, n = 1 600 (left) and P = 400, n = 4 000 (right), with
f (t) = max(t, 0) — 1/λ∕2∏ and Gaussian data. The values of X such that χ0(m) = 0 in Figure 8 are
marked in green.
for f (t) = ait + a2(t2 — 1)/√2 (so that V — a2 = a2) with different pairs of (aι, a2). We observe
a transition from the Marcenko-Pastur law (in the left display, with ai = 0 and α2 = 0) to the
semicircle law (in the right display, with a1 = 0 and a2 6= 0).
Iik
-1	0	1	2
Figure 10: Eigenvalues of K with μ = 0 (blue) versus the limiting laws in Theorem 1 and
Corollary 3 (red) for Gaussian data, P = 1024, n = 512 and f(t) = ait + a2(t2 — 1)∕√2 with
a1 = 1, a2 = 0 (left), a1 = 1, a2 = 1/2 (middle), and a1 = 0, a2 = 1 (right).
17
Published as a conference paper at ICLR 2021
Remark 2 tells Us that, depending on the ratio ν∕a2, the eigensPectrUm of K exhibits a transition
from the Marcenko-Pastur to semicircle-like shape. Note from Figure 1-(right) that, for the sparse
fι, the ratio ν∕a2 is an increasing function of the truncation threshold S and therefore, as the matrix
K become sparser, it eigenspectrum changes from a MarcenkO-PaStUr-type (at S = 0) to be more
semicircle-like. This is depicted in Figure 11 and similar conclusions hold for quantized f2 and
binary f3 in the S ≥ Sopt regime.
Figure 11: Eigenvalues of K with μ = 0 (blue) versus the limiting laws in Theorem 1 and
Corollary 3 (red) for Gaussian data,P = 1024, n = 512 and f(t) = t ∙ 1∣t∣>√¾ with S = 0.1 (left),
s = .75 (middle), and S = 1.5 (right).
As discussed after Theorem 1 and in the proof above, while the limiting eigenvalue distribution ω
is universal and independent of the law of the entries of Z, so long as they are independent, sub-
exponential, of zero mean and unit variance, as commonly observed in RMT (Tao et al., 2010), this
is no longer the case for the isolated eigenvalues. In particular, according to Corollary 1, the possible
non-informative spikes do depend on the kurtosis K of the distribution. In Figure 12 We observe a
farther (left) spike for Student-t (with K = 5 and is thus not sub-exponential) than Gaussian distri-
bution (with κ = 3), while no spike can be observed for the symmetric Bernoulli distribution (that
takes values ±1 with probability 1/2 so that K = 1), with the same limiting eigenvalue distribution
for f(t) = max(t, 0) — 1∕√2π.
Figure 12: Eigenvalues of K with μ = 0 (blue) versus the limiting laws and spikes in Theo-
rem 1 and Corollary 1 (red) for Student-t (with 7 degrees of freedom, left), Gaussian (middle) and
Rademacher distribution (right), P = 512, n = 2 048, f (t) = max(t, 0) 一 1/√2π. Emphasis on
the non-informative spikes at different locations: at -2.10 for Student-t and -1.77 for Gaussian.
Remark 3 (Non-informative spike in-between). When the support of ω consists of two disjoint
regions (e.g., in the right plot of Figure 9), a non-informative spike may appear between these two
regions, with the associated Stieltjes transform m < — c∕aι in the setting ofFigure 8-(bottom). This
is only possibly when aι ʌ∕K⅛ >。2. An example is provided in Figure 13.
A.3 Proof of Corollary 2 and related discussions
Similar to our discussions in Section A.2, we need to find the zeros of det Λ(z), that are real solu-
tions to H(x) = 0 with
H (x) = aιa2(κ-1)
P 一
1
(ρ+1)m(x)+2c
一 P
3
0
(33)
18
Published as a conference paper at ICLR 2021
Figure 13:	Eigenvalues of K with μ = 0 (blue) versus the limiting laws and spikes in Corol-
lary 3 and 1 (red) for P = 400, n = 6 000, with f(t) = max(t, 0) - 1/√2π and Gaussian data.
for m(z) the unique solution to (9) and P = limp ∣∣μk2. Note that
1.	for aιa2(κ - 1)( (VnIn) P - 1 - ρ) = 0, there can be up to three spikes;
2.	with aι = 0 and a? = 0, We get m2(x) = a：；；-劫 and there are at most two spikes:
this is equivalent to the case of Corollary 1 with P = 0; in fact, taking a1 we discard the
information in the signal μ, as has been pointed out in (Liao & Couillet, 2019);
3.	with a； = 0 and aι = 0 We obtain m(x)=-。](；+]),this is the case of Corollary 2.
For a given isolated eigenvalue-eigenvector pair (λ, V) (assumed to be of multiplicity one), the
projection ∣Vτv∣2 onto the label vector V can be evaluated via the Cauchy,s integral formula and
our Theorem 1. More precisely, consider a positively oriented contour Γ that circles around only the
isolated λ, we write
工VTVVTv = —-ɪ- V 工VT(K — ZIn)TV dz
n	2πι Jr n
=——ɪ- V -ɪVT(m(z)In — VΛ(z)Vt)v dz + o(1)
2πι Jr n
=1VTV ( — Λ A(Z) dz ) VTV + o⑴=1VTV (ResA(Z)) VTV + o⑴
n	∖2πι Jri	n
1
lim(z - λ)h	e(z)m：(Z)	dz.?⅞n2m(z)
z→λ	Θ(z)Ω(z)vn1nm(z) Θ(z)Ω2(z) (V ；n) -Ω(z)
1
VTIn
- n -
+ o(1)
where we use Theorem 1 for the second line and recall that the asymptotic location λ of λ is away
from the support of limiting spectral measure ω so that -1 外 m(z) dz = 0 in the third line.
Interestingly, note at this point that taking VTIn = 0 or a； = 0 (so that Ω(z) = 0) leads to the same
simplification
3VTV|2 = lim(z — λ)Θ(z)m2(z) + o(1) = lim(z — λ)---a1pm (Z)------ + o(1)	(34)
n	z→λ	z→λ	c + a1m(z)(1 + P)
aip m2(λ)
1 + P m0(λ)
+ (1) = aιP (1 _	a1cm2 ―)
0	1 + P	(C + aιm(λ))2
V J m2(λ)) + o(1) (35)
with l,Hospital,s rule and the fact that m0(z) = (m2(ZJ
α2c
(c+aιm(z))2
2	-1
ν-ca1)	by differenti-
ating (9). The particularly means that, in the absence of the (noisy) non-informative spikes due to
a； 6= 0 orin the case of balanced class VT1n = 0 (in fact “almost” balanced class VT1n = o(n)), we
obtain the same asymptotic alignment (with respect to V) for the informative eigenvector. However,
there may appear spurious non-informative and isolated eigenvectors in the latter case.
In the setting of Corollary 2 with a1 > 0 and a；
into (35) and then the change-of-variable m
0, with the substitution m(λ)
mρ
C
aι (ρ+1)
-ac-1+X, we obtain the expression of F(x) in
Corollary 2. The phase transition condition can be similarly obtained, as discussed in Section A.2, by
checking the sign of the derivative of the functional inverse x0(m) as in Corollary 3. This concludes
the proof of Corollary 2.
—
—
—
□
19
Published as a conference paper at ICLR 2021
Discussions. Note that, while with either a2 = 0 or vT1n = 0 we obtain the same expression for
the projection IvTv|2, the possible spike of interest 入(and its asymptotic location λ) in these two
scenarios can be rather different. More precisely,
1.	with a2 = 0, there is a single possible spike λ with m(λ) = mρ = - a](；+]);
2.	with vτ1n = 0, there can be UP to three spikes that correspond to m。= - a](；+])and
This observation leads to the following remark.
Remark 4 (Noisy top eigenvector with a2 6= 0). For vT1n = 0 and a2 6= 0, one may have
m- = — 0c∙ JK-1 > — a](p+1)= mρ for instance with large a? and small aι. Since m(x) is an
increasing function, the top eigenvalue-eigenvector pair of K can be non-informative, independent
of the SNR ρ, and totally useless for clustering purposes. An example is provided in Figure 2 where
one observes that (i) the largest spike (on the right-hand side) corresponds to a noisy eigenvector
while the second largest spike has its eigenvector positively aligned with the label vector v; and (ii)
the theoretical prediction of the eigen-alignment α in Corollary 2 still holds here due to vT1n = 0.
This extends our Corollary 1 to the signal-plus-noise scenario and confirms the advantage and
necessity of taking a2 = 0.
As a side remark, in contrast to Remark 3 and FigUre 13, where we observe that the non-informative
spike can be lying between the two disjoint regions of the limiting measUre ω, in the case of a1 > 0,
the informative spike m。= - a](；+])can only appear on the right-hand side of the support of ω,
since -言 < -a](；+])< 0 for P = limp ∣∣μk2 ≥ 0. See Figure 8-(bottom) for an illustration.
A.4 Proof of Proposition 1
Note that, for v the top isolated eigenvector of K, with Corollary 2 we can write
v = ʌ/ɑv/ʌ/n + σw
(36)
for some σ ∈ R, w ∈ Rn a zero-mean random vector, orthogonal to v, and of unit norm. To
evaluate the asymptotic clustering performance in the setting of Proposition 1 (i.e., with the estimate
Ci = sign([v]i) for vTv ≥ 0), we need to assess the probability Pr(Sign([v]i) < 0) for Xi ∈ Ci and
Pr(Sign([v]i) > 0) for Xi ∈ C? (recall that the class-label [v]i = -1 for Xi ∈ Ci and [v]i = +1 for
Xi ∈ C2), and it thus remains to derive σ. Note that
1 = v Tv = α + 2σ√αwτv∕√n + σ2 = α + σ2 + o ⑴
(37)
(Kadavankandy & Couillet, 2019), concludes the proof.
where we recall ∣∣v∣ = √n, which, together with an argument on the normal fluctuations of v
B Additional empirical results on real-world datasets
In Figure 14, we compare the clustering performance, the level of sparsity, and the computational
time of the top eigenvector, of the sparse function fi and quantized f? with M = 2 (so 2 bits per
nonzero entry), on the MNIST dataset. We see that, different from the binary f3 with which small
entries ofK are set to zero, the quantized function f?, by letting the small entries ofK to take certain
nonzero values, yields surprisingly good performance on the MNIST dataset. This performance gain
comes, however, at the price of somewhat heavy computational burden that is approximately the
same as the original dense matrix XTX, since we lose the sparsity with f?, see Figure 1-(left). This
may be interpreted as a trade-off between storage size and computational time.
Also, from the left and middle displays of Figure 7 and Figure 14, we see that for MNIST data, while
the classification error rates on the pair of digits (0, 1) can be as low as 1%, the performances on the
pair (5, 6) are far from satisfactory, with the linear f(t) = t and the proposed fi, f? or f3. This is
the limitation of the proposed statistical model in (1), which only takes into account the first-order
discriminative statistics. Indeed, it has been shown in (Liao & Couillet, 2019) that, taking a? = 0 (as
20
Published as a conference paper at ICLR 2021
O
420
..
00
etar.fissalcsiM
O
420
..
00
15
O
150
.
0
oreznon fo.porP
)s( emit.etupmoC
1
.
00
Truncation threshold s
Truncation threshold s
Truncation threshold s
5
O
1
5
1
O
2
5
O
1
O
2
5
O
1
5
1
Figure 14:	Clustering performance (left), proportion of nonzero entries, and computational time of
the top eigenvector (right, in markers) of sparse f1 and quantized f2 with M = 2, as a function of
the truncation threshold s on the MNIST dataset: digits (0, 1) (left) and (5, 6) (middle and right)
with n = 2 048 and performance of the linear function in black. Results averaged over 100 runs.
in the case of the proposed f1 , f2 and f3) asymptotically discards the second-order discriminative
statistics in the covariance structure, and may thus result in suboptimal performance in the case of
non-identity covariance. It would be of future interest to extend the current analysis to the “generic”
Gaussian mixture classification: N(μι, Ci) versus N(μ2, C2) by considering the impact of (i)
asymmetric means μγ and μ2 = -μγ and (ii) statistical information in the covariance structure Ci
versus C2 and (iii) possibly a multi-class mixture model with number of classes K ≥ 3.
O
420
..
00
etar.fissalcsiM
20
5
1
O
1
5
O
420
..
00
20
5
1
5
Truncation threshold s	Truncation threshold s	Truncation threshold s
Figure 15: Clustering performance of sparse f1, quantized f2 (with M = 2) and binary f3 as
a function of the truncation threshold s on: (left) Kuzushiji-MNIST class 3 versus 4, (middle)
Fashion-MNIST class 0 versus 9, and (right) Kannada-MNIST class 4 versus 8, for n = 2 048 and
performance of the linear function in black. Results averaged over 100 runs.
Figure 15 compares the clustering performances of the proposed f1 , f2, and f3 on other MNIST-
like datasets including the Fashion-MNIST (Xiao et al., 2017), Kuzushiji-MNIST (Clanuwat et al.,
2018), and Kannada-MNIST (Prabhu, 2019) datasets. Then, Figure 16 compares the performances
on the representations of the ImageNet dataset (Deng et al., 2009) from the popular GoogLeNet
(Szegedy et al., 2015) of feature dimension p = 2 048. On various real-world data or features, we
made similar observations as in the case of MNIST data in Figure 7 and Figure 14: the performances
of sparse f1 and binary f3 are very similar and generally degrade as the threshold s becomes large,
while the quantized f2 yields consistently good performances that are extremely close to that of the
linear function. This is in line with the (theoretically sustained) observation in (Seddik et al., 2020)
that the “deep” representations of real-world datasets behave, in the large n, p regime, very similar to
simple Gaussian mixtures, thereby conveying a strong practical motivation for the present analysis.
O
420
..
00
etar.fissalcsiM
O
420
..
00
20
40
20
40
Truncation threshold s
Truncation threshold s
Figure 16: Clustering performance of sparse f1, quantized f2 (with M = 2) and binary f3 as
a function of the truncation threshold s on GoogLeNet features of the ImageNet datasets: (left)
class “pizza” versus “daisy” and (right) class “hamburger” versus “coffee”, for n = 1 024 and
performance of the linear function in black. Results averaged over 10 runs.
21