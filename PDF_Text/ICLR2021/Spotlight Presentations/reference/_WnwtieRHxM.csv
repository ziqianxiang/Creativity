title,year,conference
 Regularized learningfor domain adaptation under label shifts,2019, arXiv preprint arXiv:1903
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Curriculum learning,2009, InProceedings of the 26th annual international conference on machine learning
 Concentration inequalities: A nonasymptotictheory of independence,2013, Oxford university press
 Convex optimization: Algorithms and complexity,2014, arXiv preprint arXiv:1405
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, arXiv preprint arXiv:2002
 Class-balanced loss based oneffective number of samples,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Efficientand accurate estimation of lipschitz constants for deep neural networks,2019, In Advances in NeuralInformation Processing Systems
 Size-independent sample complexity ofneural networks,2018, In Conference On Learning Theory
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InAdvances in neural information processing systems
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Learning deep representation forimbalanced classification,2016, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Deep imbalanced learning forface recognition and attribute prediction,2019, IEEE transactions on pattern analysis and machineintelligence
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Risk and parameter convergence of logistic regression,2018, arXiv preprintarXiv:1803
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Doubly robust off-policy value evaluation for reinforcement learning,2016, InInternational Conference on Machine Learning
 Empirical margin distributions and bounding thegeneralization error of combined classifiers,2002, The Annals of Statistics
 Gradient harmonized single-stage detector,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Learning fromnoisy labels with distillation,2017, In Proceedings of the IEEE International Conference on ComputerVision
 Focal loss for dense objectdetection,2017, In Proceedings ofthe IEEE international conference on computer vision
 Detecting and correcting for label shift withblack box predictors,2018, In International Conference on Machine Learning
 Lexico-graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models,2019, arXivpreprint arXiv:1905
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2018, arXiv preprintarXiv:1805
 Margin maximizing loss functions,2004, In Advances inneural information processing systems
 Simulation and the Monte Carlo Method,1118, Wiley Publishing
 Boosting: Foundations and algorithms,2013, Kybernetes
 Rec-ommendations as treatments: Debiasing learning and evaluation,2016, arXiv preprint arXiv:1602
 Learning from noisy labels withdeep neural networks: A survey,2020, arXiv preprint arXiv:2007
 Counterfactual risk minimization: Learning from loggedbandit feedback,2015, In International Conference on Machine Learning
 Lipschitz regularity of deep neural networks: analysis andefficient estimation,2018, In Advances in Neural Information Processing Systems
 Learning to model the tail,2017, In Advances inNeural Information Processing Systems
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, In Advances in Neural Information ProcessingSystems
 Adversarialcounterfactual learning and evaluation for recommender system,2020, Advances in Neural InformationProcessing Systems
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
2 PRACTICAL IMPLICATIONS OF CONDITION C3C3 asserts the Lipschitz and smoothness properties,2018, The Lipschitz condition is rather mild assumptionfor neural networks
 From Theorem 1 of Golowich et al,2021, (2018)
