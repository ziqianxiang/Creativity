title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, 2018
 Spectrally-normalized margin bounds forneural networks,2017, In NIPS
 Nearly-tight vc-dimension andpseudodimension bounds for piecewise linear neural networks,2017, 2017b
 Dawnbench: An end-to-end deep learningbenchmark and competition,2017, In NIPS ML Systems Workshop
 In search of robust measures of generaliza-tion,2020, In NeurIPS
 'âˆž vector contraction for rademacher complexity,2019, 2019
 Size-independent sample complexity ofneural networks,2018, In COLT
 Distilling the knowledge in a neural network,2015, 2015
 Uniform convergence rates for kernel density estimation,2017, In ICML
 Predicting the generaliza-tion gap in deep networks with margin distributions,2019, In ICLR
 Fantasticgeneralization measures and where to find them,2019, 2019b
 Uniform convergence may be unable to explain general-ization in deep learning,2019, 2019
 Remarques sur un resultat non publie de b,1980, maurey
 Improved approximation algorithms for large matrices via random projections,2006, InFOCS
 Boosting: Foundations and Algorithms,2012, MIT Press
 Understanding Machine Learning: From Theory toAlgorithms,2014, Cambridge University Press
 Sanity-checking pruning methods: Random tickets can win the jackpot,2020, 2020
 Compression based bound for non-compressednetwork: unified generalization error analysis of large compressible deep neural network,2019, 2019
 Data-dependent sample complexity of deep neural networks via lipschitzaugmentation,2019, 2019
 Understandingdeep learning requires rethinking generalization,2016, 2016
1 was mostly a reduction to Lemma A,2017,1
