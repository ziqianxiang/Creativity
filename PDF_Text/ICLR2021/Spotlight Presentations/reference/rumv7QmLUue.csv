title,year,conference
 Global Sparse Momentum SGD for PruningVery Deep Neural Networks,2019, In Proc
 Fast Sparse ConvNets,2020, In Proc
 Rate Distortion For Model Compression: From Theory ToPractice,2019, In Proc
 Understanding the Difficulty of Training Deep Feedforward NeuralNetworks,2010, In Proc
 EIE: Efficient InferenceEngine on Compressed Deep Neural Network,2016, In Proc
 Learning Both Weights and Connections For EfficientNeural Networks,2016, In Proc
 Robust Pruning at Initialization,2021, In Proc
 Delving Deep into Rectifiers: Surpassing Human-LevelPerformance on ImageNet Classification,2015, In Proc
 Soft Filter Pruning For Accelerating Deep ConvolutionalNeural Networks,2018, In Proc
 Distilling the Knowledge in a Neural Network,2015, In NeurIPS DeepLearning and Representation Learning Workshop
 Optimal Brain Damage,1990, In Proc
 A Signal Propagation Perspective For Pruning NeuralNetworks at Initialization,2020, In Proc
 Pruning Filters For Efficient ConvNets,2017, InProc
 Learning Efficient Convolutional NetworksThrough Network Slimming,2017, In Proc
 Rethinking the Value of Network Pruning,2019, InProc
 Proving the Lottery Ticket Hypothesis:Pruning is All You Need,2020, In Proc
 Importance Estimation For NeuralNetwork Pruning,2019, In Proc
 Generalization in Deep Networks: The Role of Distance from Initializa-tion,2017, In Workshop on Deep Learning: Bridging Theory and Practice
 Optimal Lottery Tickets viaSubset Sum: Logarithmic Over-Parameterization is Sufficient,2020, In Proc
 SVCCA: Singular Vector CanonicalCorrelation Analysis For Deep Learning Dynamics and Interpretability,2017, In Proc
 Pruning Neural Networks Without Any Data byIteratively Conserving Synaptic Flow,2020, In Proc
 In Proc,2018, Euro
 Picking Winning Tickets Before Training by Preserving GradientFlow,2020, In Proc
 Good Subnetworks Provably Exist: Pruningvia Greedy Forward Selection,2020, In Proc
 DrawingEarly-Bird Tickets: Towards More Efficient Training of Deep Networks,2020, In Proc
