title,year,conference
 Justifying and generalizing contrastive divergence,2009, NeuralComputation
 On contrastive divergence learning,2005, In Societyfor Artificial Intelligence and Statistics
 Conditional noise-contrastive estimation of unnormalisedmodels,2018, In International Conference on Machine Learning
 Which processes satisfy the second law,1994, Physical Origins of TimeAsymmetry
 Implicit generation and generalization in energy-based models,2019, InAdvances in Neural Information Processing Systems
 Learning generative con-vnets via multi-grid modeling and sampling,2018, In Conference on Computer Vision and PatternRecognition
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Societyfor Artificial Intelligence and Statistics
 Training products of experts by minimizing contrastive divergence,2002, NeuralComputation
 Reducing the dimensionality of data with neuralnetworks,2006, Science
 Replicated softmax: an undirected topic model,2009, InAdvances in Neural Information Processing Systems
 A fast learning algorithm for deep beliefnets,2006, Neural Computation
 Learning deep energy models: Contrastive divergence vs,2017, amortizedmle
 Deep Boltzmann machines,2009, In Artificial Intelligenceand Statistics
 Restricted Boltzmann machines forcollaborative filtering,2007, In International Conference on Machine Learning
 Minimum probability flow learn-ing,2011, In ICML
 On the convergence properties of contrastive divergence,2010, InInternational Conference on Artificial Intelligence and Statistics
 Some investigations into energy-based models,2007, PhD thesis
 A theory of generative convnet,2016, InInternational Conference on Machine Learning
9 and anexponential decaying learning rate,1000, Except for the training of the third configuration
