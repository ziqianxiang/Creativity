title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In International Conference on Learning Representations
 Neural combinatorialoptimization with reinforcement learning,2016, 2016
 Attention augmentedconvolutional networks,2019, CoRR
 Efficient attention using a fixed-size mem-ory representation,2017, CoRR
 Large scale gan training for high fidelity naturalimage synthesis,2019, 2019
 End-to-end object detection with transformers,2020, 2020
 Generative pretraining from pixels,2020, 2020a
 Uniter: Universal image-text representation learning,2020, 2020b
 A2-nets: Doubleattention networks,2018, CoRR
 Rethinking attention with performers,2020, 2020
 Randaugment: Practical automateddata augmentation with a reduced search space,2019, 2019
 A cheap linear attention mechanism with fast lookupsand fixed-size representations,2016, 2016
 An image is worth 16x16 words: Transformers for image recognition atscale,2020, 2020
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, 2021
 Deep residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Bag of tricks forimage classification with convolutional neural networks,2018, 2018
 Axial attention in multidi-mensional transformers,2019, arXiv preprint arXiv:1912
 Searchingfor mobilenetv3,2019, 2019
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Relation networks for objectdetection,2018, 2018a
 Gather-excite: Exploiting featurecontext in convolutional neural networks,2018, In Advances in Neural Information Processing Systems
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Deep networks with stochas-tic depth,2016, 2016
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Learning Representations
 In-datacenter performance analysis of a tensor processingunit,2017, SIGARCHComPut
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, 2020
 Reformer: The efficient transformer,2020, arXivPrePrint arXiv:2001
 Visualbert: A simpleand performant baseline for vision and language,2019, 2019
 Video-based person re-identificationvia 3d convolutional networks and non-local attention,2019, 2019
 Microsoft coco: Common objects in context,2014, In EuropeanConference on ComPuter Vision
 Object-centric learning with slot atten-tion,2020, 2020
 Vilbert: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, 2019
 The gpu computing era,2010, IEEE micro
 Bam: bottleneck attentionmodule,2018, In British Machine Vision Conference
 Image transformer,2018, In International Conference on Machine Learning
 Film: Visualreasoning with a general conditioning layer,2017, CoRR
 Learning transferable visual models from natural language supervision,2021, 2021
 Stand-alone self-attention in vision models,2019, CoRR
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Self-attention with relative position representa-tions,2018, arXiv preprint arXiv:1803
 Fast transformer decoding: One write-head is all you need,2019, 2019
 Efficient attention: Self-attention with linear complexities,2018, CoRR
 Videobert: A jointmodel for video and language representation learning,2019, 2019
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, CoRR
 Efficient transformers: A survey,2020, 2020
 Pointer networks,2015, In Advances in NeuralInformation Processing Systems
 Linformer: Self-attentionwith linear complexity,2020, 2020b
 Non-local neural networks,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Visual transformers: Token-basedimage representation and processing for computer vision,2020, 2020
 Nystromformer: A nystrom-based algorithm for approximating self-attention,2021, 2021
 Self-attention generativeadversarial networks,2019, 2019
