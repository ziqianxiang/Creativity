title,year,conference
 Preference-based policy learning,2011, InMachine Learning and Knowledge Discovery in Databases
 Learning to understand goal specifications by modelling reward,2019, InICLR
 Extrapolating beyondsuboptimal demonstrations via inverse reinforcement learning from observations,2019, In ICML
 Scaling data-drivenrobotics with reward sketching and batch reinforcement learning,2019, arXiv: 1909
 Deepreinforcement learning from human preferences,2017, In NIPS
 Guided cost learning: Deep inverse optimalcontrol via policy optimization,2016, In ICML
 Learning robust rewards with adverserial inversereinforcement learning,2018, In ICLR
 Rewardlearning from human preferences and demonstrations in Atari,2018, In NeurIPS
 Solving Least Squares Problems,1995, SIAM
 Understanding learned reward functions,2020, InProceedings of the Workshop on Deep Reinforcement Learning at NeurIPS
 Algorithms for inverse reinforcement learning,2000, In ICML
 Policy invariance under reward transforma-tions: theory and application to reward shaping,1999, In NIPS
 Solving Rubikâ€™s Cube with a robot hand,2019, arXiv: 1910
 Bayesian inverse reinforcement learning,2007, In IJCAI
 Active preference-basedlearning of reward functions,2017, In RSS
 Proximalpolicy optimization algorithms,2017, arXiv:1707
 An upper bound on the loss from approximate optimal-value functions,1994, Machine Learning
 Grandmaster level in StarCraft II using multi-agent reinforcementlearning,2019, Nature
 A Bayesian approach for policy learning fromtrajectory preference queries,2012, In NIPS
 Maximum entropyinverse reinforcement learning,2008, In AAAI
 By the same argument as lemma A,2021,11 up toeq
