title,year,conference
 Quantifying Distributional Model Risk via OptimalTransport,2016, arXiv:1604
 Robust Wasserstein Profile Inference and Applica-tions to Machine Learning,2016, arXiv:1610
 Debiasing representations byremoving unwanted variation due to protected attributes,2018, arXiv:1807
 Gender Shades: Intersectional Accuracy Disparities in Commer-Cial Gender Classification,2018, In Proceedings of Machine Learning Research
 Robust decision trees againstadversarial examples,2019, ArXiv
 Flexibly fair representation learning by disentanglement,2019, ArXiv
 Bias in Bios:A Case Study of Semantic Representation Bias in a High-Stakes Setting,2019, Proceedings of theConference on Fairness
 Data-driven Distributionally Robust Optimization Us-ing the Wasserstein Metric: Performance Guarantees and Tractable Reformulations,2015, MathematicalProgramming
 Stochastic Optimization for Large-scale Optimal Transport,2016, In NIPS (ed
 Online Learning with anUnknown Fairness Metric,2018, arXiv:1802
 Explaining and Harnessing AdversarialExamples,2014, arXiv preprint arXiv:1412
 Fairness WithoutDemographics in Repeated Loss Minimization,2018, arXiv:1806
 Metric Learning for Individual Fairness,2019, arXiv:1906
 Eliciting and enforcing subjective individual fairness,2019, CoRR
 Classifying without discriminating,2009, In 2009 2nd InternationalConference on Computer
 Data preprocessing techniques for classification withoutdiscrimination,2011, Knowledge and Information Systems
 An empirical study of rich subgroupfairness for machine learning,2019, In Proceedings of the Conference on Fairness
 Fairness through computationally-boundedawareness,2018, In S
 Operationalizing individual fairness withpairwise fair representations,2019, ArXiv
 Functional gradient techniquesfor combining hypotheses,1999, Advances in Neural Information Processing Systems
 Two simple waysto learn individual fairness metrics from data,2020, In International Conference on Machine Learning
 Numerical Optimization,2006, Springer Series in OperationsResearch
 Debiasing Embeddings for Reduced Gender Biasin Text Classification,2019, arXiv:1908
 Certifying Some Distributional Robustnesswith Principled Adversarial Training,2017, arXiv:1710
 SenSeI: Sensitive Set Invariance for Enforcing IndividualFairness,2020, arXiv:2006
 Training individually fair ML models withsensitive subspace robustness,2020, In International Conference on Learning Representations
 Mitigating unwanted biases with adversariallearning,2018, In AIES â€™18
 Projecting always produces a classifierwith small group fairness gaps,2021, Unlike projecting
