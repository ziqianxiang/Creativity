title,year,conference
 ETC: Encoding long and structuredinputs in transformers,2020, In Proc
 Anempirical study on the properties of random bases for kernel methods,2017, In Proc
 Faster kernel ridge regression usingsketching and preconditioning,2017, SIAM J
 Adaptive input representations for neural language modeling,2019, InProc
 Neural machine translation by jointlylearning to align and translate,2015, In Proc
 Harmonic Analysis and the Theory of Probability,1955, University of California Press
 Language models are few-shot learners,2020, arXiv: 2005
 Report onthe 11th IWSLT evaluation campaign,2014, In Proc
 Recurrent positional embedding forneural machine translation,2019, In Proc
 Generating long sequences with sparsetransformers,2019, arXiv: 1904
 Learning phrase representations using RNN encoder-decoderfor statistical machine translation,2014, In Proc
 Kernel methods for deep learning,2009, In Proc
 Rethinking attention with per-formers,2021, In Proc
 In Proc,2019, of ACL
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proc
 Classicalstructured prediction losses for sequence to sequence learning,2018, In Proc
 Exploring kernel functions in thesoftmax layer for contextual word classification,2019, In International Workshop on Spoken LanguageTranslation
 Modelingrecurrence for transformer,2019, In Proc
 Distilling the knowledge in a neural network,2015, InNeurIPs Deep Learning and Representation Learning Workshop
 Axial attention in multidi-mensional transformers,2020, arXiv: 1912
 Long short-term memory,1997, Neural Computation
 Kernel methods in machine learn-ing,2008, Annals of Statistics
 Parameter-efficient transfer learning forNLP,2019, In Proc
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, In Proc
 Adam: A method for stochastic oPtimization,2015, In Proc
 Auto-encoding variational bayes,2014, In Proc
 Settransformer: A framework for attention-based Permutation-invariant neural networks,2019, In Proc
 Enhancing the locality and breaking the memory bottleneck of transformer on time seriesforecasting,2019, In Proc
 Generating wikiPedia by summarizing long sequences,2018, In Proc
 Learning word vectors for sentiment analysis,2011, In Proc
 Pointer sentinel mixturemodels,2017, In Proc
 Regularizing and Optimizing LSTMLanguage Models,2018, In Proc
 Differentiable plasticity: training plastic neuralnetworks with backpropagation,2018, In Proc
 Document-level neuralmachine translation with hierarchical attention networks,2018, In Proc
 Transformers with convolutionalcontext for ASR,2019, arXiv: 1904
 Fast function to function regression,2015, In Proc
 Stabilizing transformers for reinforcement learn-ing,2020, In Proc
 Image transformer,2018, In Proc
 Rational recurrences,2018, In Proc
 A mixture of h - 1 heads is better than hheads,2020, In Proc
 A call for clarity in reporting BLEU scores,2018, In Proc
 Blockwise self-attention for long document understanding,2020, In Findings of EMNLP
 In Proc,2020, of ICLR
 Random features for large-scale kernel machines,2007, In Proc
 In Proc,2019, of NeurIPS
 Efficient content-basedsparse attention with routing transformers,2020, arXiv: 2003
 Neural machine translation of rare words withsubword units,2016, In Proc
 Q-BERT: Hessian based ultra low precision quantization of BERT,2020, In Proc
 Adaptive attentionspan in transformers,2019, In Proc
 Random Features Methods in Supervised Learning,2019, PhD thesis
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv: 2005
 Efficient transformers: A survey,2020, arXiv:2009
 Long range arena: A benchmark for efficienttransformers,2021, In Proc
 Transformer dissection: An unified understanding for transformerâ€™s attention viathe lens of kernel,2019, In Proc
 Attention is all you need,2017, In Proc
 Linformer: Self-attentionwith linear complexity,2020, arXiv: 2006
 A learning algorithm for continually running fully recurrentneural networks,1989, Neural Computation
 Lite transformer with long-short rangeattention,2020, In Proc
 Hard-coded Gaussian attention for neural machinetranslation,2020, In Proc
 Orthogonal random features,2016, In Proc
 Big bird: Transformersfor longer sequences,2020, arXiv: 2007
