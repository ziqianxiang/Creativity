title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In International Conference on Machine Learning
 A convergence analysis of gradient de-scent for deep linear neural networks,2019, In International Conference on Learning Representations
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Deep equilibrium models,2019, In Advances in NeuralInformation Processing Systems
 Gradient descent with identity initializa-tion efficiently learns positive-definite linear transformations by deep residual networks,2019, Neuralcomputation
 Algorithmic differentiation of implicit functions and optimalvalues,2008, In Advances in Automatic Differentiation
 Reverse accumulation and attractive fixed points,1994, Optimization Methods andSoftware
 Deep learning for classical japanese literature,2019, In NeurIPS Creativity Workshop 2019
 Recurrent stacking of layers for compact neural machine translationmodels,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Width provably matters in optimization for deep linear neural networks,2019, InInternational Conference on Machine Learning
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Modeling from features: a mean-fieldframework for over-parameterized deep neural networks,2020, arXiv preprint arXiv:2007
 Uber matnzen aus nicht negativen elementen,1912, Sitzungsberichte der KOmgIichPreussischen Akademie der Wissenschaften
 Evaluating derivatives: principles and techniques Of algO-rithmic differentiatiOn,2008, SIAM
 Identity matters in deep learning,2017, In InternatiOnal COnference OnLearning RepresentatiOns
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural infOrmatiOn prOcessing systems
 Locally adaptive activation func-tions with slope recovery for deep and physics-informed neural networks,2020, PrOceedings Of theROyal SOciety A
 Adaptive activation functionsaccelerate convergence in deep and physics-informed neural networks,2020, JOurnal Of COmputatiOnalPhysics
 Directional convergence and alignment in deep learning,2020, arXivpreprint arXiv:2006
 Deep learning Without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Depth With nonlinearity creates no bad local minima inresnets,2019, Neural Networks
 Gradient descent finds global minima for generalizable deepneural netWorks of practical sizes,2019, In 2019 57th Annual Allerton Conference on Communication
 Elimination of all bad local minima in deep learning,2020, InInternational Conference on Artificial Intelligence and Statistics
 Effect of depth and Width on localminima in deep learning,2019, Neural computation
 Deep linear netWorks With arbitrary loss: All local minima areglobal,2018, In International conference on machine learning
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Adding one neuron can eliminate all badlocal minima,2018, In Advances in Neural Information Processing Systems
 The interpolation phase transition in neural networks: mem-orization and generalization under lazy training,2020, preprint arXiv:2007
 Bounds for norms of the matrix inverse and the smallest singular value,2008, Linearalgebra and its applications
 Implicit bias in deep linear classification: Initialization scale vs training accuracy,2020, arXivpreprint arXiv:2007
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS workshop on deep learningand unsupervised feature learning
 On connected sublevel sets in deep learning,2019, In International Conference onMachine Learning
 A note on connectivity of sublevel sets in deep learning,2021, arXiv preprintarXiv:2101
 Zur theorie der matrices,1907, Mathematische Annalen
 Theory of deep learning iii: explaining the non-overfittingpuzzle,2017, arXiv preprint arXiv:1801
 Gradient methods for minimizing functionals,1963, Zhurnal Vychislitelâ€™noiMatematiki i Matematicheskoi Fiziki
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural networks,2014, In International Conference on LearningRepresentations
 Semeion handwritten digit data set,1994, Semeion Research Center ofSciences of Communication
 Kernel and rich regimes in overparametrized models,2020, arXivpreprint arXiv:2002
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Gradient descent optimizes over-parameterized deep ReLU networks,2020, Machine Learning
 On the global convergence of training deep linearresnets,2020, In International Conference on Learning Representations
