title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Measuringabstract reasoning in neural networks,2018, arXiv preprint arXiv:1807
 Neural mechanisms of binding in the hippocampus andneocortex: insights from computational models,2006,
 Learning to perform role-filler binding with schematic knowledge,2019, arXiv preprintarXiv:1902
 Generalization of reinforcementlearners with working and episodic memory,2019, In Advances in Neural Information ProcessingSystems
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Hybrid computing using a neural network with dynamic external memory,2016, Nature
 Draw: Arecurrent neural network for image generation,2015, arXiv preprint arXiv:1502
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Grounded language learning fast and slow,2020, arXiv preprint arXiv:2009
 Long short-term memory,1997, Neural computation
 The proper treatment of symbols in a connectionist architec-ture,2000, Cognitive dynamics: Conceptual change in humans and machines
 Distributed rePresentations of structure: A theory of analog-ical access and maPPing,1997, Psychological review
 Solving raven’s Progressive matrices with multi-layer rela-tion networks,2020, arXiv preprint arXiv:2003
 Not-so-clevr: learning same-different relationsstrains feedforward neural networks,2018, Interface focus
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Indirection andsymbol-like Processing in the Prefrontal cortex and basal ganglia,2013, Proceedings of the NationalAcademy of Sciences
 Generalization without systematicity: On the comPositional skillsof sequence-to-sequence recurrent networks,2018, In International Conference on Machine Learning
 Object-centric learning with slot atten-tion,2020, arXiv preprint arXiv:2006
 Rule learning by seven-month-old infants,1999, Science
 Why there are comPlementarylearning systems in the hiPPocamPus and neocortex: insights from the successes and failures ofconnectionist models of learning and memory,1995, Psychological review
 Metalearned neuralmemory,2019, In Advances in Neural Information Processing Systems
 LearningcomPositional rules via neural Program synthesis,2020, arXiv preprint arXiv:2003
 The codes of man and beasts,1983, Behavioral and Brain Sciences
 Neural ePisodic control,2017, arXiv preprintarXiv:1703
 Raven’s progressive matrices,1938, Western Psychological Services LosAngeles
 ComPositional generalization in adeeP seq2seq model by seParating syntax and semantics,2019, arXiv preprint arXiv:1904
 A simple neural network module for relational reasoning,2017, InAdvances in neural information processing Systems
 Analysing mathematical rea-soning abilities of neural models,2019, arXiv preprint arXiv:1904
 An explicitly relational neural network architecture,2019, arXiv preprintarXiv:1905
 A memory-augmented neural network modelof abstract rule learning,2020, arXiv preprint arXiv:2012
 Tensor product variable binding and the representation of symbolic structures inconnectionist systems,1990, Artificial intelligence
 The topography of ability and learningcorrelations,1984, Advances in the psychology of human intelligence
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Learning representations that support extrapolation,2020, arXiv preprintarXiv:2007
 The tolman-eichenbaum machine: Unifying space and relational mem-ory through generalisation in the hippocampal formation,2019, BioRxiv
 Raven: A dataset for relationaland analogical visual reasoning,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Temporal relational reasoning invideos,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
2 Task output layerAll models had an output layer for generating y,2010, The number of units and the nonlinearity dependedon the task
0 ± 0,2021,0	100
6 ± 0,2021,2	99
