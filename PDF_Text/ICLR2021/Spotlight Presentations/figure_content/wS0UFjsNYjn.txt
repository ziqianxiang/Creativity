Figure 1: During meta-training, Meta-GMVAE learns multi-modal latent space that can best explain the unla-beled data using EM algorithm. At meta-test time, we use semi-supervised EM to map both the support (labeleddata) and queries (unlabeled data) to each mode learned during meta-training.
Figure 2: The graphical illustration of Meta-GMVAE. The dotted lines denote either variational inference orExpectation Maximization. (a): We introduce the multimodal distribution pψ (z) into prior distribution, and itsoptimal task-specific parameter ψ* is obtained by EM in an episodic manner. (b): For meta-test, We obtaintask-specific parameter ψ* by semi-supervised EM using Xs, ys, and Xq.
Figure 3: The samples obtained and generated for each mode at unsupervised meta-training and supervisedmeta-test step of Meta-GMVAE. Samples in each row are in the same modality obtained by EM.
