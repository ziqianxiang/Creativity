Figure 1: Averaging feature maps per input (column marginal) and per filter (row marginal) in thelast convolutional layer of a traffic light detector. Color indicates activation strength (the brighter,the higher), revealing line artifacts in the maps. These artifacts are the manifestation of spatial bias.
Figure 2: Activation maps for a 0 input, averaged over each layer’s filters (title format: H×W×C).
Figure 3: Mean kernel per convolutional layer. All kernels are 3 × 3, the titles show their counts.
Figure 4: The formation of blind spots in SSD, illustrated via its box predictor internals with azero-valued input. The predictor uses spatial anchors to detect and localize the target object at45 × 80 possible locations based on 512 feature maps. Certain anchors are predisposed to predictbackground due to feature-map artifacts, as evident in the logit maps. Traffic lights at the corre-sponding location cannot be detected as demonstrated with a real scene (middle one in the bottom).
Figure 5: (a) A map showing via color the detection score the SSD computes for a traffic light whenpresent at various locations. The detection is muted when the stimulus lies in the area impacted bythe artifacts. (b) The same map after changing the padding method to SYMMETRIC. The detectionscores are rather constant except for periodic variations due to the SSD’s reliance on anchors.
Figure 6: (a) Illustrating the problem of uneven padding when down-sampling at a stride of 2. Thepadding along x-axis is consumed only at the left side. (b) Mean 3 × 3 filters in three ResNet models,trained on ImageNet with two input sizes. Color encodes average weight (green is positive). A sizethat induces uneven padding (top row) can lead to asymmetries, esp. around down-sampling layers.
Figure 7: Foveation behavior of different padding methods applied to VGG-19 [37], and illustratedin a 512 × 512 input space (unless otherwise stated). Color represents the number of paths to theoutput for each input pixel. (a) The difference between VALID, FULL, and SAME 0-padding. (b)SAME alternatives to 0-padding. (c) Dilation amplifies foveation of SAME 0-padding. (d) Stridescan lead to checkerboard patterns. (e) Foveation effects are more extensive in smaller inputs (relativeto input size) and are sensitive to uneven padding.
Figure 8: The same feature maps in Figure 2, generated under mirror padding and averaged over 30randomly-generated input samples. The line artifacts induced by 0-padding are largely mitigated.
Figure 9: The foveation maps of two ResNet architectures under 0 padding, illustrated with a225×225 input. Compared with ResNet-50, ResNet-101 has twice the number of convolutionallayers with non-unitary filter sizes. Accordingly, the extent of the foveation effect is doubled.
Figure 10: The foveation maps of ResNet-50 under 0 padding, illustrated with inputs of differentsize. The smaller the input, the larger the relative extent of foveation.
Figure 11: The foveation maps of ResNet-50 under 0 padding, illustrated with two input sizes. Witha 257 × 257 input, the padding is evenly applied at all downsampling layers, leading to a symmetricfoveation map. With a 256 × 256 input, the padding is applied only to the left and top sides of featuremaps at all downsampling layers, which limits the number of convolutional input-output paths forpixels in the bottom and right sides as evident in the skewed foveation map.
Figure 12: Mean filters of two ResNet models trained on ImageNet with 224×224 images. Theinput size causes uneven application of padding, leading to frequent asymmetries in the mean filtersunder 0 padding. We illustrate how two alternatives, circular padding and PartialConv [23], enablelearning highly-symmetric mean filters despite the uneven application of padding.
Figure 13: Mean filters of ResNet-101 trained on ImageNet with 224×224 images under both 0-padding and PartialConv [23]. The input size causes uneven application of padding, leading tofrequent asymmetries in the mean filters under 0 padding. In contrast, PartialConv produces highlysymmetric mean filters, thanks for its treatment of pixels outside the feature map as missing values.
Figure 14: Mean filters of VGG-16 trained on ImageNet under different conditions. Most meanfilters exhibit high symmetry when trained with 225×225 images where the size violates Eq. 4.
Figure 16: Circular padding largely preserves the randomness and mitigates line artifacts.
Figure 17: SYMMETRIC mirror padding also preserves the randomness and mitigates line artifacts.
Figure 18: REFLECT mirror padding also preserves the randomness and mitigates line artifacts.
Figure 20: Feature map artifacts of a VGG-19 model under Distribution Padding (interpolationmode) [30]. Due to multiple resize operations used to fill the padding area, the artifacts grow fromthe boundary inwards. We use a saturated constant input to make the effect visible.
Figure 21:	Mean filters of four models trained on ImageNet with 224×224 images under 0-paddingboth without and with antialiasing.
Figure 22:	Mean filters of two models trained on ImageNet with 224×224 images under 0-paddingboth without and with antialiasing.
