Figure 1: Examples of entities correctly retrieved from GENRE (we show only the top-3 rank). Onthe top three entity disambiguation instances and on the bottom three document retrieval instances,two for open-domain question answering and one for fact checking. All of them are cast as sequence-to-sequence problems while inference is done using constrained beam search. Gold entities in bold.
Figure 2: Example of dynamically constrained Markup decoding for entity linking using “In 1503,Leonardo began painting the Mona Lisa.” as input. There are 3 cases: when we are outside a men-tion/entity (a), inside a mention generation step (b), and inside an entity link generation step (c). Themodel is supposed to output the input source annotating mentions and pointing them to the respectiveentities: “ In 1503, [Leonardo] (Leonardo da Vinci) began Painting the [Mona Lisa] (MonaLisa) ”.
Figure 3: Accuracy per mention-entity pair frequency (in Wikipedia) on the validation sets of allEntity Disambiguation tasks in KILT.
Figure 4: Accuracy per number of BPE tokens of the Wikipedia title to generate on the validationsets of all KILT datasets except ELI5 (as it is fundamentally different from the others). We alsoshow the data distribution of token lengths. Most of the titles have less than 15 BPE tokens whilethe mode of the distribution is 5. Here GENRE has an average accuracy of 78.6% but it is higherfor short titles (e.g., <10) and it is lower for long titles (e.g., ≥10). Degradation in performancedoes not directly follow the data distribution of the token lengths. Indeed, even if long titles are rareperformance is not heavily affected (e.g., for length >15).
Figure 5: Accuracy per number of incoming links in Wikipedia on the validation sets of all KILTdatasets except ELI5 (as it is fundamentally different from the others). We also show the datadistribution of the number of incoming links. Intuitively, a page/entity with few incoming links hasbeen observed less than highly connected pages/entities. Indeed, for pages/entities never linked (firstbin on the left) the average accuracy is 20% lower than the global average (78.6%). However, forpages/entities linked at least once it is above the global average. This indicates that GENRE seemseffective on linking rare entities.
Figure 6:	Example of a GENRE prediction for named entity disambiguation on KILT WNED. Theinput is plain text where a mention is flagged with two special start and end tokens [START_ENT]and [END-ENT]. The output is a ranked list of entity (where we report the log-likelihood as well).
Figure 7:	Example of GENRE predictions for the retrieval task on KILT. The input is a query and theoutput is a ranked list of Wikipedia article titles (we also report the log-likelihood of the solutions).
Figure 8: Example of a GENRE prediction for end-to-end entity linking on AIDA. The input isplain text and the output is a Markup string where the links are Wikipedia titles. Spans are in theformat hsi, li, tii: start of the mention, length of the mention, and title respectively.
Figure 9: Example of prefix tree (trie) structure where the allowed entities identifiers are ‘Englishlanguage’, ‘English literature’ and ‘France’. Note that at the root there is the start-of-sequencetoken SOS and all leaves are end-of-sequence tokens EOS. Since more that one sequence has thesame prefix (i.e., ‘English’), this end up being an internal node where branches are the possiblecontinuations.
