Figure 1: Cosine similarities of encoder gradients between xx-en language pairs averaged across alltraining steps. Darker cell indicates pair-wise gradients are more similar. Best viewed in color.42.2	ObservationsWe make the following three main observations. Our findings are consistent across different modelarchitectures and settings (see Appendix C and D for more results and additional discussions).
Figure 2: Comparing gradient similarity versus model performance. (a): Similarity of model gradi-ents between xx-en (left) and en-xx (right) language pairs in a single Any→Any model. (b): BLEUscores on en-fr of a set of trilingual models versus their gradient similarities. Each model is trainedon en-fr and another en-xx language pair.
Figure 3: Counts of active PCGrad (left) andGradVac (right) during the training process.
Figure 4: Evaluating gradient similarity across model architecture and training steps. (a): Differ-ence between gradient similarities in the encoder and decoder. Positive value (darker) indicates theencoder has more similar gradient similarities. (b): Gradient similarities across layers. (c): Gradientsimilarities of different components and tasks across training steps.
Figure 5: Comparing PCGrad (left) with GradVac (right) in two cases. (a): For negative similarity,both methods are effective but GradVac can utilize adaptive objectives between different tasks. (b):For positive similarity, only GradVac is active while PCGrad stays “idle”.
Figure 6: Comparing multilingual models with bilingual baselines on our dataset. Language pairsare listed in the order of training data sizes (high-resource languages on the left).
Figure 7: Per language pair data distribution of the dataset used to train our multilingual model. Theyaxis depicts the number of training examples available per language pair on a logarithmic scale.
Figure 8: Cosine similarities on WMT dataset averaged across all training steps.
Figure 9: Cosine similarities (on Transformer-Base models) of xx-en language pairs on WMT datasetaveraged across all training steps.
Figure 10: Cosine similarities of decoder gradients between en-xx language pairs averaged acrossall training steps. Darker cell indicates pair-wise gradients are more similar.
Figure 11: Cosine similarities of decoder gradients between en-xx language pairs averaged acrossall training steps. Darker cell indicates pair-wise gradients are more similar. Model trained withsmaller batch sizes.
Figure 12: Pictorial description of our method.
