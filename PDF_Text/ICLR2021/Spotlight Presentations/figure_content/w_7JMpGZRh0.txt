Figure 1:	Overview of the Watch-And-Help challenge. The challenge has two stages: i) in theWatch stage, Bob will watch a single demonstration of Alice performing a task and infer her goal; ii)then in the Help stage, based on the inferred goal, Bob will work with Alice to help finish the sametask as fast as possible in a different environment.
Figure 2:	The system setup for the WAH challenge. An AI agent (Bob) watches a demonstration ofa human-like agent (Alice) performing a task, and infers the goal (a set of predicates) that Alice wastrying to achieve. Afterwards, the AI agent is asked to work together with Alice to achieve the samegoal in a new environment as fast as possible. To do that, Bob needs to plan its actions based on i)its understanding of Alice’s goal, and ii) a partial observation of the environment. It also needs toadapt to Alice’s plan. We simulate environment dynamics and provide observations for both agentsin our VirtualHome-Social multi-agent platform. The platform includes a built-in agent as Alicewhich is able to plan its actions based on the ground-truth goal, and can react to any world statechange caused by Bob through re-planning at every step based on its latest observation. Our systemalso offers an interface for real humans to control Alice and work with an AI agent in the challenge.
Figure 3: Overviewof the human-likeagent.
Figure 4: The overall design of the baseline models. A goal inferencemodel infers the goal from a demonstration D and feeds it to a helping pol-icy (for learning-based baselines) or to a planner to generate Bob’s action.
Figure 4: a goal inference model and a goal-conditioned helping planner / policy. In this paper, weassume that the AI agent has access to the ground-truth states of objects within its field of view (butone could also use raw pixels as input). We describe our approach for the two components below.
Figure 5: a) Success rate (X axis) and speedup (y axis) of all baselines and oracles. The performanceof an effective Bob agent should fall into the upper-right side of the Alice-alone baseline in this plot.
Figure 6: Example helping plan. The arrows indicate moving directions and the circles with blackborders indicate moments when agents interacted with objects. When working alone (left), Alicehad to search different rooms; but with Bob’s help (right), Alice could finish the task much faster.
Figure 7: Example helping behaviors. We show more examples in the supplementary video.
Figure 8: a) Success rate (x axis) and speedup (y axis). b) Cumulative reward with real humans orwith the human-like agent. c) Subjective ratings from Exp. 2. Here, Alice refers to humans or thehuman-like agent acting alone, whereas HP, Hybrid, and HPRG indicate different AI agents helpingeither humans or the human-like agent. All results are based on the same 30 tasks in the test set.
Figure 9: Apartments used in VirtualHome-Social. The last two apartments are uniquely used ashelping environments during the testing phase.
Figure 10: Avatars available in VirtualHome-Social.
Figure 11:	a) VirtualHome-Social provides egocentric views, third-person views and scene graphswith symbolic state representations of objects and agents. It also offers multi-modal inputs (RGB,segmentation, depth, 3D boxes and skeletons). b) Illustration of the action space at one step.
Figure 12:	Schematic of the human-like agent. Based on the state graph sampled from the belief,the hierarchical planner searches for a high-level plan over subgoals using MCTS; then RP searchesfor a low-level plan over actions for each subgoal. The first action of each plan is sent back to theenvironment for execution.
Figure 13:	The agent’s belief is represented as the location distribution of objects, and is updated ateach step based on the previous belief and the latest observation. In the example, the open cabinetreveals that the wine glass can not be in there, and that there is an apple inside, updating the beliefaccordingly.
Figure 14: Initial location distributions of all objects in the environment. Rows are objects andcolumns are locations. The color indicates the frequency.
Figure 15: Initial location distributions of the goal objects. Rows are objects and columns arelocations. The color indicates the frequency.
Figure 16: Network architecture of the goal inference model, which encodes the symbolic statesequence in demonstrations and infers the count for each predicate.
Figure 17: Network architecture of the low-level policy in the HRL baseline. Note that the objectselection policy also considers “Null” as a dummy object node for actions that do not involve anobject, which is not visualized here.
Figure 18: Network architecture the high-level policy for the Hybrid and the HRL baselines.
Figure 19: Success rate (X axis) and speedup (yaxis) of all the baselines and oraclesshould be 2 plates on the dinnertable). Similar to the low-level policy, We get an attention vector σgfrom the go, encoding to reshape the state representation. In total, the network has three outputs:the object subgoal policy for sampling the object class name in the subgoal, the location subgoalpolicy for sampling the target location class name in the subgoal, and a value function.
Figure 20: An example of how real human differs from the human-like agent when working withan AI agent (i.e., HPRG) with a conflicting goal. In this example, Bob incorrectly thinks that Alicewants to put the wine glass to the dishwasher whereas Alice actually wants to put it to the dinnertable. When controlled by a human-like agent, Alice enters into a loop with Bob trying to changethe location of the same object. The real human player, on the other hand, avoids this conflict byfirst focusing on other objects in the goal, and going back to the conflicting object after all the othergoal objects have been placed on the dinner table. Consequently, the real human completes the fulltask successfully within the time limit.
