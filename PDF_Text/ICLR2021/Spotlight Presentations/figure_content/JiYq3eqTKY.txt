Figure 1: Illustrative linear regression. Ac-tive learning deliberately over-samples unusualpoints (red x’s) which no longer match the pop-ulation (black dots). Common practice uses thebiased unweighted estimator R which puts toomuch emphasis on unusual points. Our unbiasedestimators RPURE and RLURE fix this, learninga function using only Dtrain nearly equal to theideal you would get if you had labels for thewhole of Dpool, despite only using a few points.
Figure 2: RPURE and RLURE remove bias introduced by active learning, while unweighted R, whichmost active learning work uses, is biased. Note the sign: R overestimates risk because active learningsamples the hardest points. Variance for RPURE and RLURE depends on the acquisition distributionplacing high weight on high-expected-loss points. In (b), the BALD-style distribution means that thevariance of the unbiased estimators is smaller. For FashionMNIST, (c), active learning bias is smalland high variance in all cases. Shading is ±1 standard deviation.
Figure 3: For linear regression, the models trained with RPURE or RLURE have lower ‘population’risk. In contrast, BNNs trained with RLURE or RPURE perform either similarly (e) or slightly worse(b,c,d,f), even though they remove bias and have lower variance. Shading is one standard deviation.
Figure 4: Overfitting bias—BOFB—for models trained using the three objectives. (a) Linear regression,BOFB is small compared to ALB (c.f. Figure 2a). Shading IQR. 1000 trajectories. (b) BNN, BOFB issimilar scale and opposite magnitude to ALB (c.f. Figure 2b). (c) BNN on FashionMNIST, OFB issomewhat larger than with MNIST, particularly for R (i.e. our approaches reduce overfitting) anddominates active learning bias (c.f. Figure 2c). Shading ±1 standard error. 150 trajectories.
Figure 5:	For linear regression (a) the biased estimator has the lowest variance, and RLURE improves7^^i	∕1∖TΓA . i' .l TΓA-ΛT-1LT.1	♦	1 1	.,1%	, 1	1	,on RPURE. (b) But for the BNN the variances are more comparable, with RLURE the lowest.
Figure 6:	Adopting an alternative proposal distribution—here an epsilon-greedy adaptation of adistance-based measure—does not change the overall picture for linear regression.
Figure 7:	We contrast the effect of using RLURE throughout the entire acquisition procedure andtraining (rather than using the same acquisition procedure based on R for all estimators). The purpletest performance and orange are nearly identical, suggesting the result is not sensitive to this choice.
Figure 8: Versions of Figures 3b and 3c shown with standard errors (45 points) instead of standarddeviations. This makes it clearer that the biased R has better performance, even if only marginally so.
Figure 9: Higher temperatures approach a deterministic acquisition function. These also tend toincrease the variance of the risk estimator because the weight associated with unlikely points increases,when it happens to be selected. The overall pattern seems fairly consistent, however.
Figure 10: Further downstream performance experiments. (a)-(c) are partners to Figures 3d, 3e, and3f. (d) and (e) show similar results for a smaller multi-layer perceptron (with one hidden layer of 50units). In all cases the results broadly mirror the results in the main paper.
