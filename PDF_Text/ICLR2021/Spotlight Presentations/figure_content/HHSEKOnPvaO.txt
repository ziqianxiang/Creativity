Figure 1: Illustration of Experiment Replay (ER) (Chaudhry et al., 2019) on the left and our model (GCL) onthe right. While ER independently processes context images from the episodic memory and target images fromthe current task, GCL models pairwise similarities between the images via the random graphs G and A.
Figure 2: t-SNE visualization of image embeddings (small circles) from the penultimate layers and classembeddings (large circles) from the weights of the last layers on S PLIT SVHN. The left figure shows thatFinetune, a model naively trained on the data stream, fails to recognize the class-based clustering structure andbias the image embeddings toward the last task (class 8 & 9). In contrast, the right figure shows that GCL (ourmodel) maintains the relational structure and is more robust to the distributional shifts incurred by task changes.
Figure 3: Average accuracy as a function of the number of tasks trained.
Figure 4: Effects of episodic memory sizes.
Figure 5: Training and testing time.
Figure 6: Context graph G on Split CIFAR10.
Figure 7: Effects of graph regularization (Î»G).
Figure 8:	Average accuracy as a function of the number of tasks trained on Permuted MNIST, RotatedMNIST, SPLIT SVHN, and SPLIT CIFAR10.
Figure 9:	Average accuracy as a function of memory size on Split SVHN and S plit CIFAR10.
Figure 10: Average forgetting as a function of memory size on Split SVHN and Split CIFAR 1 0.
