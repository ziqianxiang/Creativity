Figure 1: Top: The two phases of a fast-mapping episode. Bottom: Screenshots of the task fromthe agent’s perspective at important moments (including the contents of the language channel).
Figure 2: Accuracy of agents trained on probe trials involving a different number of total objectsfor agents meta-trained with different numbers of total objects.
Figure 3: Accuracy during training and evaluation trials involving unfamiliar objects, for differentsizes of global training set G. Curves show mean ± S.E. over 3 agent seeds in each condition.
Figure 4: Accuracy of agents in fast-mapping trials requiring the extension of ShapeNet categoriesfrom a single exemplar. Curves show the mean ± S.E. over three agent seeds in each condition.
Figure 5: Accuracy of agents trained without shaping reward on the 3-object fast-mapping taskwith |G| = 30. Curves show mean ± S.E. across three seeds in each condition.
Figure 6: Right: The accuracy of the agent (accuracy ±S.E.) on evaluation trials when exposed todifferent training regimes. Left: Schematic of the most impoverished training regime.
Figure 7: Training success comparison between DCEM, DCEM with selective writing and Trans-formerXL for different sizes of memory-buffer (DCEM) or window (TransformerXL).
Figure 8: Training and test accuracy on two types of generalization tasks for agents that readdifferent numbers of frames from their memory per query.
Figure 9: Agent architecture. See figure 10 for details of the dual coding episodic memory compo-nent. Dashed lines correspond to connections across timesteps.
Figure 10: DCEM architecture. This corresponds to the ‘memory’ component in the agent archi-tecture, Figure 9. NB: the similarity computation and selection of nearest neighbours is replicatedfor each read head, but not depicted here to avoid clutter. Dashed lines correspond to connectionsacross timesteps.
