Figure 1: Illustration of Recurrent Independent Mechanisms (RIMs). A single step under the proposedmodel occurs in four stages (left figure shows two steps). In the first stage, individual RIMs produce a querywhich is used to read from the current input. In the second stage, an attention based competition mechanism isused to select which RIMs to activate (right figure) based on encoded visual input (blue RIMs are active, basedon an attention score, white RIMs remain inactive). In the third stage, individual activated RIMs follow theirown default transition dynamics while non-activated RIMs remain unchanged. In the fourth stage, the RIMssparsely communicate information between themselves, also using key-value attention.
Figure 2: Visualizing Activation Patterns. For the copying task, one can see that the RIM activation patternis distinct during the dormant part of the sequence in the middle (activated RIMs black, non-activated white).
Figure 3: Handling Novel Out-of-Distribution Variations. We study the performanCe of RIMs Compared toan LSTM baseline (4 left plots). The first 15 frames of ground truth (yellow,orange) are fed in and then thesystem is rolled out for the next 35 time steps (blue,purple). During the rollout phase, RIMs perform betterthan the LSTMs in aCCurately prediCting the dynamiCs of the balls as refleCted by the lower Cross Entropy(CE) [blue for RIMs, purple for LSTMs]. NotiCe the substantially better out-of-distribution generalization ofRIMs when testing on a number of objeCts different from the one seen during training. (2nd to 4th plot). Wealso show (right plot) improved out-of-distribution generalization (F1 score) as compared to LSTM andRMC (Santoro et al., 2018) on another partial observation video prediction task. X-axis = number of balls.
Figure 4: Robustness to Novel Distractors:. Left: performance ofthe proposed method (blue) compared to an LSTM baseline (red)in solving the object picking task in the presence of distractors.
Figure 5: RIMs-PPO relative score improvement over LSTM-PPO baseline (Schulman et al., 2017) across allAtari games averaged over 3 trials per game. In both cases, PPO was used with the exact same settings, and theonly change is the choice of recurrent architecture. More detailed experiments with learning curves as well ascomparisons with external baselines are in Appendix C.
Figure 6: Different RIMs attending to Different Balls. For understanding what each RIM is actually doing,we associate each with a separate encoder, which are spatially masked. Only 4 encoders can be active at anyparticular instant and there are four different balls. We did this to check if there would be the expected geometricactivation of RIMs. 1.) Early in training, RIM activations correlated more strongly with the locations of thefour different balls. Later in training, this correlation decreased and the active strips did not correlate as stronglywith the location of balls. As the model got better at predicting the location, it needed to attend less to the actualobjects. The top row shows every 5th frame when the truth is fed in and the bottom shows the results duringrollout. The gray region shows the active block. In the top row, the orange corresponds to the prediction and inthe bottom, green corresponds to the prediction.
Figure 7: Example of the other LSTM baselines. For the 2 other experiments that we consider, here we showexample outputs of our LSTM baselines. In each row, the top panel represents the ground truth and the bottomrepresents the prediction. All shown examples use an LSTM with 250 hidden units, as shown in Fig. 3. Framesare plotted every 3rd time step. The red line marks 10 rollout frames. This is marked because after this we donot find BCE to be a reliable measure of dissimilarity.
Figure 8: Comparison of RIMs to LSTM baseline. For 4 different experiments in the text, we compare RIMsto two different LSTM baselines. In all cases we find that during rollout, RIMs perform better than the LSTMsat accurately capturing the trajectories of the balls through time. Due to the number of hard collisions, accuratemodeling is very difficult. In all cases, the first 15 frames of ground truth are fed in (last 6 shown) and then thesystem is rolled out for the next 15 time steps, computing the binary cross entropy between the prediction andthe true balls at each instant, as in Van Steenkiste et al. (2018). In the predictions, the transparent blue shows theground truth, overlaid to help guide the eye.
Figure 9: Comparison between RIMs and LSTM baseline. For the 4 ball task and the 6-8 ball extrapolationtask, here we show an example output of from our LSTM baseline and from RIMs. All shown examples use anLSTM with 250 hidden units, as shown in Fig. 3. Frames are plotted every 3rd time step. The red line marks 10rollout frames. This is marked because after this we do not find BCE to be a reliable measure of dissimilarity.
Figure 10: Comparison of RIMs to LSTM baseline. For 4 different experiments in the text, we compare RIMsto two different LSTM baselines. In all cases we find that during rollout, RIMs perform better than the LSTMsat accurately capturing the trajectories of the balls through time. Due to the number of hard collisions, accuratemodeling is very difficult.
Figure 11: RIMs on dataset with an occlusion. We show two trajectories (top and bottom) of three balls. Forthe left frames, at each step the true frame is used as input. On the right, outlined in black, the previous output isused as input.
Figure 12: RIMs transferred on new data. We train the RIMs model on the 6-8 ball dataset (as shown in thetop row). Then, we apply the model to the 4 ball dataset, as shown in the bottom.
Figure 13: Performance metrics on OOD one-step forward prediction task. Gist: RIMs outperforms all RNNbaselines OOD.
Figure 15: Ablation loss For the normal, a one-head model, and without input attention, we show the loss duringtraining and the loss for the 4th and 5th frame of rollout. We find that the one-head and without input attentionmodels perform worse than the normal RIMs model during the rollout phase.
Figure 16: One head and no attention Using one head and no attention models, we show the rollout predictionsin blue. On top we show results on the 4 ball dataset and on the bottom we show results on the curtains dataset.
Figure 17: A comparison showing relative improvement of RIMs with kA = 5 over a kA = 4 baseline. UsingkA = 5 performs slightly worse than kA = 4 but still outperforms PPO, and has similar results across themajority of games.
Figure 18: RIMs-PPO relative score improvement over LSTM-PPO baseline (Schulman et al., 2017) across allAtari games averaged over 3 trials per game. In both cases PPO was used with the exact same settings with theonly change being the choice of the recurrent architecture (RIMs with kA = 5).
Figure 19: Comparing RIMs-PPO with LSTM-PPO: Learning curves for kA = 4, kA = 5 RIMs-PPOmodels and the LSTM-PPO baseline across all Atari games.
Figure 20: Baseline agent with no input attention mechanism: Here we compare the RIMs to the baseline,where their is no input attention (i.e., top down attention) as well as all the RIMs communicate with each otherat all the time steps. Learning curves for RIMs-PPO models, Baseline Agent, the LSTM-PPO baseline across 30Atari games.
Figure 22: 4 RIMs, (top k = 3). Each sub-figure shows the effect of masking a particular RIM and studying theeffect of masking on the other RIMs. For example, the top figure shows the effect of masking the first RIM, thesecond figure shows the effect of masking the second RIM etc.
Figure 23: 400dim, kT = 5, kA = 2. Each sub-figure shows the effect of masking a particular RIM and studyingthe effect of masking on the other RIMs. For example, the top figure shows the effect of masking the first RIM,the second figure shows the effect of masking the second RIM etc.
Figure 24: 400dim, kT = 5, kA = 3. Each sub-figure shows the effect of masking a particular RIM and studyingthe effect of masking on the other RIMs. For examples, the top figure shows the effect of masking the first RIM,the second figure shows the effect of masking the second RIM etc.
Figure 25: 400dim, kT = 5, kA = 4. Each sub-figure shows the effect of masking a particular RIM and studyingthe effect of masking on the other RIMs. For example, the top figure shows the effect of masking the first RIM,the second figure shows the effect of masking the second RIM etc.
Figure 26: The core RIMs module for a single recurrent step using GRU independent dynamics. We show howthe attention scores from input attention are used to re-weight the input to the RIMs (called att_inp) and constructa mask which controls which RIMs are allowed to update.
