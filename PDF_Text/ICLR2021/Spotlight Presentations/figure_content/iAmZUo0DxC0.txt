Figure 1: The unlearnable effectiveness of different types of noise: random, adversarial (error-maximizing) and our proposed error-minimizing noise on CIFAR-10 dataset. The lower the clean testaccuracy the more effective of the noise.
Figure 2: (a-b): For both sample-wise (a) and class-wise (b) noise, learning curves of RN-18on CIFAR-10 dataset with different types of training data: 1) only 20% clean data, 2) only 80%unlearnable data, and 3) both clean and unlearnable data. (c-d): Prediction confusion matrices (onthe clean test set) of two RN-18s trained on CIFAR-10 with the ‘Bird’ unlearnable class created bysample-wise (c) or class-wise (d) error-minimizing noise.
Figure 3: (a): Comparison between ‘bus’ (clean) class, ‘ship’ (unlearnable) class and the overallaccuracy. (b): Clean test accuracy of RN-18/RN-50/DN-121 on unlearnable CIFAR-10 with error-minimizing noise crafted on ImageNet. (c): Prediction confusion matrix of RN-18 trained on CIFAR-10 with only 4 classes (‘airplane’, ‘car’, ‘ship’, ‘truck’) are unlearnable by ImageNet transferrednoise, and the confusion matrix is computed on CIFAR-10 clean test set.
Figure 4: Preventing exploitation of face data using error-minimizing noise.
Figure 5: Prediction confusion matrix (on the clean test set) of two RN-18s trained on CIFAR-10with the unlearnable classes in bold by (a) sample-wise or (b) class-wise error-minimizing noise.
Figure 6: Clean test accuracy of adversarially trained RN-18 on unlearnable CIFAR-10 by differentsizes () of error-minimizing noise, and the dashed line indicates the performance of RN-18 trainedon clean CIFAR-10.
Figure 7:	Results on face verification: the Receiver Operating Characteristic (ROC) of two Inception-ResNet models trained on partially unlearnable WebFace (left) and fully unlearnable WebFace (right).
Figure 8:	Backdoor attack success rates when 3 types of class-wise noises including random, error-maximizing and error-minimizing noise are applied as the backdoor triggers. The x-axis shows 6RN-18 models trained on CIFAR-10 training set with different percentages of the data were poisonedby the class-wise noise. The y-axis shows the attack success rate: the percentage of all non-targetclass test images are predicted to the target class when attached with the target-class class-wise noise.
Figure 9:	The unlearnable effectiveness of different types of noises on CIFAR-10 dataset: random,error-maximizing (adversarial), error-minimizing noise generated using PGD and error-minimizingnoise generated using L-BFGS. The lower the clean test accuracy the more effective the noise inmaking training examples unlearnable. This is tested in the 100% unlearnable setting with thesample-wise noise.
