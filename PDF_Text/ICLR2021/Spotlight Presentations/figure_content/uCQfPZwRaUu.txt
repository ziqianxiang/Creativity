Figure 1: Median and Mean Human-Normalized scores of different methods across 26 games in theAtari 100k benchmark (Kaiser et al., 2019), averaged over 10 random seeds for SPR, and 5 seedsfor most other methods except CURL, which uses 20. Each method is allowed access to only 100kenvironment steps or 400k frames per game. (*) indicates that the method uses data augmentation.
Figure 2: An illustration of the full SPR method. Representations from the online encoder are used inthe reinforcement learning task and for prediction of future representations from the target encodervia the transition model. The target encoder and projection head are defined as an exponential movingaverage of their online counterparts and are not updated via gradient descent. For brevity, we illustrateonly the kth step of future prediction, but in practice we compute the loss over all steps from 1 to K .
Figure 3: A boxplot of the distribution of human-normalized scores across the 26 Atari gamesunder consideration, after 100k environment steps. The whiskers represent the interquartile rangeof human-normalized scores over the 26 games. Scores for each game are recorded at the end oftraining and averaged over 10 random seeds for SPR, 20 for CURL, and 5 for other methods.
Figure 4: Performance of SPR with various predictiondepths. Results are averaged across ten seeds per game,for all 26 games. To equalize the importance of games,we calculate an SPR-normalized score analogouslyto human-normalized scores, and show its mean andmedian across all 26 games. All other hyperparametersare identical to those used for SPR with augmentation.
Figure 5: Performance on a subset of 10 Atari games for different values of the EMA parameter τwith augmentation (left) and without (right). Scores are averaged across 10 seeds per game for eachvalue of τ . Self-normalized score is calculated separately for the augmentation and no-augmentationcases.
