Figure 1: Estimates of the intrinsic dimension of commonly used datasets obtained using the MLE methodwith k = 3, 5, 10, 20 nearest neighbors (left to right). The trends are consistent using different k’s.
Figure 2: Visualization of basenji GAN samples of varying intrinsic dimension.
Figure 3: Validation of MLE estimate on synthetic basenji data With (d = 10 free entries. We observe theestimates to converge around the expected dimensionality of 10. Standard errors plotted with N = 5 replicatesover random samples of the data.
Figure 4: Sample complexity of synthetic datasets of varying intrinsic dimensionality.
Figure 5:	Sample complexity of synthetic datasets of varying extrinsic dimensionality.
Figure 6:	Sample complexity of real datasets. Standard errors are shown N = 5 class pairs.
Figure 7: Sample complexity of noisy datasets. Standard errors are shown N = 5 random subsets of the data.
Figure 8: Sample complexity of FONTS datasets.
Figure 10: Validation of MLE estimate on synthetic soap-bubbles data with d = 10.
Figure 11:	Validation of MLE estimate on synthetic coffee data with 10 free entries. Note that the estimatesdo not converge around the upper bound of d = 10, which suggests that data generated from this class is not offull dimension.
Figure 12:	Validation of anchor approximation on basenji with d = 10.
Figure 13: Validation of anchor approximation on tree-frog With N= 32 and α = 0.001.
Figure 14: The TwoNN, Shortest-Path, and GeoMLE methods on d-dimensional Hypercube data.
Figure 15: The TwoNN, Shortest-Path, and GeoMLE methods on basenji_10 data. The esti-mates do not converge around the expected value of I= 10 in this sample regime.
