Figure 1: (a). Linearly separable data; (b). Non-separable data; (c): Balanced moon-shaped non-linear separable data; (d). Unbalance moon-shaped data after down-sampling both classes (20% forthe blue class, and 80% for the orange class). We use solid line to denote the separating hyperplaneof the trained linear model and shades to represent the decision boundary of trained nonlinear model.
Figure 2: (a): Epoch-wise training performances measured by the angle between the decisionboundary (at that epoch) and the max-margin solution, using linear predictor on the linear separabledata of Figure 1a; (b): Epoch-wise training performances measured by the average margin in the samesetting as (a); (c). The generalization error on testing data (the remaining 80% of the orange classand 20% of the blue class that are not part of the down-sampling in Figure 1d) when the nonlinearmodel is trained under different class weights, as the training progresses; (d). The average margin forthe nonlinear model on the non-linearly separable training data shown in Figure 1c, under differentclass weights, as the training progresses.
Figure 3: The role of importance weighting on defining the intercept term in addition to the implicitbias for the linearly separable case, where the hyperplane shifts in the non-separable subspacedepending on the class weights.
Figure 4: The left-five figures show that the distribution of the learned weights concentrates to aconstant as the training progresses. The rightmost figure indicates the correlation pattern betweenmargin and the learned weights: the correlation increases rapidly in the beginning, and then slowlydecreases to zero (the process is much slower for nonlinear predictor so we only show the first part).
