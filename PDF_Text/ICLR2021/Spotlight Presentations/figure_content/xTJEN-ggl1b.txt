Figure 1: Comparison between self-attention and lambda layers. (Left) An example of 3 queriesand their local contexts within a global context. (Middle) Self-attention associates each query withan attention distribution over its context. (Right) The lambda layer transforms each context into alinear function lambda that is applied to the corresponding query.
Figure 2:	Computational graph of the lambda layer. Contextual information for query positionn is summarized into a lambda λn ∈ Rlkl×lvl. Applying the lambda dynamically distributes Con-textual features to produce the output as yn = λTn qn . This process captures content-based andposition-based interactions without producing attention maps.
Figure 3:	Pseudo-code for the multi-query lambda layer. The position embeddings can be madeto satisfy various conditions, such as translation equivariance, when computing positional lambdas(not shown). The lambda layer can be adapted to other tasks/modalities by adjusting the choice ofembeddings (Section A.2).
Figure 4: Speed-accuracy comparison between LambdaResNets and EfficientNets with match-ing training and regularization setups. LambdaResNets (annotated with (depth, image size)) are 3.2-4.4x faster than EfficientNets and 1.6-2.3x faster than ResNet-RS with squeeze-and-excitation, thussignificantly improving the speed-accuracy Pareto curve of image classification. LambdaResNet-420 (image size 320), reaches a strong 84.9% top-1 accuracy, 0.9% over the corresponding architec-ture with standard 3x3 convolutions and 0.65% over the corresponding architecture with squeeze-and-excitation.
Figure 5:	Pseudo-code for the multi-query lambda layer and the 1d lambda convolution. An-d lambda convolution can equivalently be implemented via a regular (n+1)-d convolution or a n-d depthwise convolution with channel multiplier. The embeddings can be made to satisfy variousconditions (e.g. translation equivariance and masking) when computing positional lambdas with theeinsum implementation.
Figure 6:	Pseudo-code for masked multi-query lambda layer.
Figure 7:	Pseudo-code for the multi-head lambda layer. This is only shown as an example as werecommend the multi-query lambda laayer instead.
Figure 8: Pseudo-code for the multi-query lambda layer with intra-depth |u|. Lambdas areobtained by reducing over the context positions and the intra-depth dimension. This variant allocatesmore computation for generating the lambdas while keeping the cost of applying them constant. Theequivalent n-d lambda convolution can be implemented with a regular (n+1)-d convolution.
