Figure 1: Computation graphs for softmax attention (left) and random feature attention (right). Here,we assume cross attention with source length M and target length N .
Figure 2: Conditional decoding speed (left) and memory overhead (right) varying the output lengths.
Figure 3: Unconditional decoding speed (left) and memory overhead (right) varying the outputlengths. All models are tested on a single TPU v2 accelerator, with greedy decoding and batch size16.
Figure 4: Finetuning an Rfa-Gaussian model with its parameters initialized from a pretrainedsoftmax-transformer. “Reset” indicates resetting the multihead attention parameters to randomly-initialized ones. The dashed line indicates the training loss of the pretrained model.
