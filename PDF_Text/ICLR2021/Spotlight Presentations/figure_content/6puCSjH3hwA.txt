Figure 1: Left: Given an initial latent code z1 , a trajectory t , and a PCA basis V, the motiongenerator GM encodes z1 using LSTMenc to get the initial hidden state and uses LSTMdec toestimate hidden states for future frames. The image generator GI synthesizes images using thepredicted latent codes. The discriminator DV is trained on both real and generated video sequences.
Figure 2: Example generated videos from a modeltrained on FaceForensics. We can generate natu-ral and photo-realistic videos with various motionpatterns, such as eye blink and talking. Four ex-amples show frames 2, 7, 11, and 16.
Figure 3: Sample generated frames at severaltime steps (t) for the Sky Time-lapse dataset.
Figure 4: Example sequences for cross-domain video generation. First Row: (FFHQ, VoxCeleb).
Figure 6: The first and second row (also the thirdand fourth row) share the same motion code butwith different content codes.
Figure 5: The first and second row (also the thirdand fourth row) share the same initial contentcode but with different motion codes.
Figure 7: The generation ofa 64-frame video using a model trained with 16-frame on FaceForensics.
Figure 8: The generation of a 32-frame video on (AFHQ-Dog, VoxCeleb) by doing the interpolationon motion trajectory.
Figure 9: Example videos generated by our ap-proach on the UCF-101 dataset.
Figure 10: Example videos generated by our ap-proach on the FaceForensics dataset.
Figure 11: The generated videos on the Face-Forensics dataset consisting of 32 frames.
Figure 12: The generated videos on the Face-Forensics dataset consisting of 64 frames.
Figure 13: Each row is synthesized using thesame content code to generate diverse motionpatterns. Please see the corresponding supple-mentary video for a better illustration.
Figure 14: Each row is synthesized with thesame motion trajectory but different contentcodes. Please see the corresponding supplemen-tary video for a better illustration.
Figure 15: Example videos generated by ourapproach on the Sky Time-lapse dataset. Thevideos have a resolution of 128 × 128.
Figure 16: Cross-domain video generation for(FFHQ, Vox). The videos have a resolution of128 × 128.
Figure 17: Cross-domain video generation for(FFHQ, Vox). The videos have a resolution of256 × 256.
Figure 18: Cross-domain video generation for(FFHQ, Vox). The videos have a resolution of1024 × 1024.
Figure 19: Cross-domain video generation for(AFHQ-Dog, Vox). The videos have a resolutionof 512 × 512.
Figure 20: Cross-domain video generation for(AFHQ-Dog, Vox). We interpolate every twoframes to get 32 sequential frames. The videoshave a resolution of 512 × 512.
Figure 21: Cross-domain video generation for(AnimeFaces, Vox). The videos have a resolu-tion of 512 × 512.
Figure 22: Cross-domain video generation for(LSUN-Church, TLVDB). The videos have a res-olution of 256 × 256.
Figure 23: Row 1 and 3: The last frame of the mean-video and per-pixel std of w/o Lm model. Row2 and 4: The last frame of the mean-video and per-pixel std of the Full model. The Full model hasa more blurry mean-video and higher per-pixel std, which indicates more diverse motion.
Figure 24: A synthesized video using BAIR dataset. Note the background changing of the firstframe (upper-left) and the last frame (bottom-right).
Figure 25: Percentage of variations captured by top PCA components on different models.
Figure 26: Visualization of top 20 principle components of BAIR (left) and FFHQ (right).
