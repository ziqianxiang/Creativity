Figure 1: Structures of our proposed sequential probabilistic models. Sequence x1:T is disentangledinto static part zc and dynamic parts {ztm}. (a) Sequence is generated by randomly sampling{zc , ztm } from priors and concatenating them as input into an LSTM to get hidden state ht for thedecoder; (b) zc is inferred from x1:T with an LSTM, and ztm is inferred from ht and ztm-1 withanother LSTM; (c) is the same as (a) except concatenating additional categorical a; (d) A categoricallatent variable a is inferred from the dynamic latent codes. The detailed structures of the encoder anddecoder are in the supplementary material.
Figure 2: Illustration of disentangling the motions and contents of two videos on the test data ofSM-MNIST (T = 100), Sprites (T = 8) and MUG dataset (T = 8). The first row and fourth row areoriginal videos. The second row and third row are generated sequences by swapping the respectivemotion variables while keeping content variable the same (sampled at 4 time steps for illustrations).
Figure 3: Unconditional video generation on MUG dataset, where the sample at time step T = 10 ischosen for clear comparison. DS-VAE in (b) is improved by incorporating categorical latent variables.
Figure 4: Unconditional video generation on SM-MNIST: (a) Sequences (length=20) in R-WAE(MMD) are randomly taken from generated samples with T = 100 to save pdf size; (b)Generated videos by DS-VAE (Yingzhen & Mandt, 2018) with T = 20; (c) Generated videos byMoCoGAN (Tulyakov et al., 2018) with T = 20. The figures should be viewed with Adobe Readerto see video.
Figure 6: Visualizing 2D manifold of content code {zc} encoded from R-WAE(MMD) on SM-MNIST by t-SNE (Maaten & Hinton, 2008).
Figure 7: Structures of the encoder network and decoder network. (a) The ResBlock in the encodernetwork consists of convolutional network adopted from Brock et al. (2019), named "ResBlockdown". After each Resblock, We use a FC network to get latent feature hi, for i = 0, ∙∙∙ , 5(Ladder Network (S0nderby et al., 2016; Zhao et al., 2017)), whose dimensions are the same.
Figure 8: Network architectures in addition to encoder/decoder network with ht defined in Fig. 7.
Figure 9: Network parameters on encoder network and decoder network on SM-MNIST and Spritesdatasets. We adopt ResBlock down and up from Brock et al. (2019). The dimensions of zc, ztm, htare 120, 12 and 150 respectively. The batch size on both SM-MNIST and Sprites dataset are 60 andthe length of video sequence for training is T = 8.
Figure 11: Cross generation of 16 audio clips forms a 17 × 17 matrix. The first column and the firstrow are spectrum visualization of the original sequences. Subplot at the (i + 1)-th row and (j + 1)-thcolumn represents the reconstruction of i-th static factor and j-th dynamic factor.
Figure 12: Results of fix Zc and sample Zm using TFGAN (Balaji et al., 2018) architectures. The firstrow in each subfigure are real video sequences. The generated motion of moving objects by DS-VAEcontains abruptjumps and is not smooth, while R-WAE is able to generate motion of various typesincluding zig-zag, diagonal and straight line.
