Figure 1: Abstract rule learning tasks. Each task involves generalizing rules to objects not seenduring training. (a) Same/different discrimination task. (b) Relational match-to-sample task (answeris 2). (c) Distribution-of-three task (answer is 2). (d) Identity rules task (ABA pattern, answer is 1).
Figure 2: Emergent Symbol Binding Network. fs consists of an LSTM controller plus output layersfor y, kw, and g (not shown). fe is a multilayer feedforward encoder that translates an image Xinto a low-dimensional embedding z . These two pathways only interact indirectly via a key/valuememory.
Figure 3: Results for all four tasks with m objects withheld (out ofn = 100) during training. Resultsreflect test accuracy averaged over 10 trained networks (± the standard error of the mean).
Figure 4: Training accuracy time courses for all models on the m = 0 regime. Each time coursereflects an average over 10 trained networks. Error bars reflect the standard error of the mean.
Figure 5: Training accuracy time courses on m = 0 regime of the same/different task. Each timecourse reflects an average over 10 trained networks. Error bars reflect the standard error of the mean.
Figure 6: Results for all four tasks with convolutional (conv), multilayer perceptron (MLP), orrandom (rand) encoders. Results reflect test accuracy averaged over 10 trained networks (± thestandard error of the mean).
Figure 7: Training accuracy time courses for the ESBN model without confidence values on them = 0 regime, shown with the time courses for all other models for comparison. Each time coursereflects an average over 10 trained networks. Error bars reflect the standard error of the mean.
Figure 8: Training accuracy time courses on m = 0 regime of the RMTS task for the ESBN modelwith a learned default memory instead of confidence values. Each time course reflects an averageover 10 trained networks. Error bars reflect the standard error of the mean.
Figure 9: Representations learned by ESBN (projected along first two principal components). (a)Keys written to memory during time steps 1-9 (training set). (b) Keys written to memory during timesteps 1-3 (training set vs. test set). (c) Keys retrieved from memory following second appearance ofobjects that first appeared during time steps 1-3 (training set vs. test set).
