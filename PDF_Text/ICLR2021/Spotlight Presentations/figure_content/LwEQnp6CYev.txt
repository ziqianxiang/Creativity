Figure 1: Heatmaps of four reward functions for a 3 × 3 gridworld. Sparse and Dense lookdifferent but are actually equivalent with DEPIC (Sparse, Dense) = 0. By contrast, the optimalpolicies for Path and Cliff are the same if the gridworld is deterministic but different if it is“slippery”. EPIC recognizes this difference with DEPIC (Path, Cliff) = 0.27. Key: RewardR(s, s0) for moving from s to s0 is given by the triangular wedge in cell s that is adjacent to cells0. R(s, s) is given by the central circle in cell s. Optimal action(s) (deterministic, infinite horizon,discount γ = 0.99) have bold labels. See Figure A.2 for the distances between all reward pairs.
Figure 2: Approximate distances between hand-designed reward functions in PointMass, where theagent moves on a line trying to reach the origin. EPIC correctly assigns 0 distance between equivalentrewards SUch as (D盘',S∙^) while DNPEC(D猾',S∙∙^) = 0.58 and DERC(D鳏'，S鳏.)=0.56. Thecoverage distribution D is sampled from rollouts of a policy πuni taking actions uniformly at random.
