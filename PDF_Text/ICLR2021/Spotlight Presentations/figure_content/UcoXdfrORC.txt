Figure 1: The robot must find actions thatquickly achieve the desired goal. State tran-sitions and the true optimal distances be-tween states are unknown, so our methodlearns an approximate shortest distancefunction and dynamics model directly onimages. These models allow the robot to findthe shortest path to the goal at test-time.
Figure 2: Model-based visual goal reaching: (Left) During offline learning, we train an image-based predictive model and distance function on the same random dataset. (Right) At test time, weuse the learned distance model for MPC, plugging in the learned distance as a cost function.
Figure 3: Comparative evaluation results: (Left) Example initial states and task definitions for Sawyerobject pushing and Franka door sliding simulated environments, as well as the real-world drawer closing task.
Figure 4: Real-world robot evaluation: (Left) Third-person view of an example task setting and (Right)results. Success rates are computed using 10 trials for each task. Each task is specified by a goal image, and asin previous experiments, the same trained models are used across tasks. Task success is determined by the finalposition of the drawer only.
Figure 5: Comparisons on the simplereaching task, where most methods attaingood performance.
Figure 6: Heatmap visualizations of our distance functions. Each pixel in every heatmap represents thedistance between a generated starting image containing the object at that (x, y) coordinate and the fixed goalimage (pictured on left). All three distance functions show a minimum when the object position is near the goalposition of (0.1, -0.05). However, our Q-function produces a better-shaped signal than the direct regressionmodel, and avoids occlusion errors - like the local minimum at high y-values, which plague pixel-wise MSE.
Figure 7: Our learned distance function yieldshigher success rates than alternative approaches fromprior work, such as the `2 distance of a VAE latentspace (Nair et al., 2018) and temporal distance regres-sion (Hartikainen et al., 2019). We also see consistentimprovements from using negative transition mining,especially on “hard” tasks.
Figure 8: Results for planning horizon ablations.
