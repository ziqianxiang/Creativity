Figure 1: Preliminary observations for additional motivation to theoretically understand deep equi-librium linear models. The figure shows test and train losses versus the number of epochs for linearmodels, deep equilibrium linear models (DELMs), and deep neural networks with ReLU (DNNs).
Figure 2: (a)-(b): Convergence performances for deep equilibrium linear models (DELMs) withidentity initialization and random initialization of three random trials, and linear ResNet with iden-tity initialization. (c) the numerical training trajectory of DELMs with random initialization alongwith theoretical upper bounds with initialization-independent 位T and initialization-dependent 位T .
Figure 3: Logistic loss and theoreticalbounds with initialization-independent位T and initialization-dependent 位T .
Figure 4: Test and train losses (in log scales) Versus the number of epochs for linear models, deepequilibrium linear models (DELMs), and deep neural networks with ReLU (DNNs). The plottedlines indicate the mean Values oVer fiVe random trials whereas the shaded regions represent errorbars with one standard deViations for both test and train losses. The plots for linear models andDELMs are shown with the best and worst learning rates (in terms of the final test errors at epoch =5000). The plots for DNNs are shown with the best learning rates for each depth H = 2, 3, and 4(in terms of the final test errors at epoch = 5000).
Figure 5: Test and train losses (in log scales) versus the number of epochs for deep equilibrium linearmodel (DELM) and deep neural network with no bias term (DNN-NB). The plotted lines indicatethe mean values over five random trials whereas the shaded regions represent error bars with onestandard deviations for both test and train losses. The plots for DELM are shown with the best andworst learning rates (in terms of the final test errors at epoch = 5000). The plots for DNN-NB areshown with the best learning rates for each depth H = 2, 3, and 4 (in terms of the final test errors atepoch = 5000).
Figure 6: Test and train losses (in log scales) versus the number of epochs for deep equilibriumlinear model (DELM) and deeper neural network with no bias term (DNN-NB). The legend is thesame for all subplots and shown in subplot (a). The plotted lines indicate the mean values over fiverandom trials whereas the shaded regions represent error bars with one standard deviations for bothtest and train losses. The plots for DELM are shown with the best and worst learning rates (in termsof the final test errors at epoch = 5000). The plots for DNN-NB are shown with the best learningrates for each depth H = 10, 100, and 200 (in terms of the final test errors at epoch = 5000). Thevalues for DNNs with depth H = 100 and 200 coincide in the plots.
Figure 7: Test and train losses (in log scales) versus the number of epochs for deep equilibriumlinear model (DELM) and deeper neural networks with ReLU (DNNs). The legend is the same forall subplots and shown in subplot (a). The plotted lines indicate the mean values over five randomtrials whereas the shaded regions represent error bars with one standard deviations for both test andtrain losses. The plots for DELM are shown with the best and worst learning rates (in terms of thefinal test errors at epoch = 5000). The plots for DNNs are shown with the best learning rates foreach depth H = 10, 100, and 200 (in terms of the final test errors at epoch = 5000). The values forDNNs with depth H = 100 and 200 coincide in the plots.
Figure 8: Test and train losses (in log scales) versus the number of epochs for linear models, deepequilibrium linear models (DELMs), and deep neural networks with ReLU (DNNs). The legend isthe same for all subplots and shown in subplot (a). The plotted lines indicate the mean values overthree random trials whereas the shaded regions represent error bars with one standard deviations.
