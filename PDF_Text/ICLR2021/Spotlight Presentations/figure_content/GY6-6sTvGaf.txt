Figure 1: The performance of SAC trained from pixels on the DeepMind control suite using imageencoder networks of different capacity (network architectures taken from recent RL algorithms, withparameter count indicated). (a): unmodified SAC. Task performance can be seen to get worse asthe capacity of the encoder increases. For Walker Walk (right), all architectures provide mediocreperformance, demonstrating the inability of SAC to train directly from pixels on harder problems.
Figure 2: Different combinations of our three regularization techniques on tasks from (Tassa et al.,2018) using SAC. Black: standard SAC. Blue: DrQ [K=1,M=1], SAC augmented with random shifts.
Figure 3: The PlaNet benchmark. Our algorithm (DrQ [K=2,M=2]) outperforms the other methodsand demonstrates the state-of-the-art performance. Furthermore, on several tasks DrQ is able tomatch the upper-bound performance of SAC trained directly on internal state, rather than images.
Figure 4: The Dreamer benchmark. Our method (DrQ [K=2,M=2]) again demonstrates superiorperformance over Dreamer on 12 out 15 selected tasks. In many cases it also reaches the upper-boundperformance of SAC that learns directly from states.
Figure 5: The Atari 100k benchmark. Compared to a set of leading baselines, our method (DrQ[K=1,M=1], combined with Efficient DQN) achieves the state-of-the-art performance, despite beingconsiderably simpler. Note the large improvement that results from adding DrQ to Efficient DQN(pink vs cyan). By contrast, the gains from CURL, that utilizes tricks from both Data EfficientRainbow and OTRainbow, are more modest over the underlying RL methods.
Figure 6: Various image augmentations have different effect on the agent’s performance. Overall, weconclude that using image augmentations helps to fight overfitting. Moreover, we notice that randomshifts proven to be the most effective technique for tasks from the DeepMind control suite.
Figure 7: Increasing values of K,M hyper-parameters generally correlates positively with the agent’sperformance, especially on the harder tasks, such as Cheetah Run.
Figure 8: A robustness study of our algorithm (DrQ) to changes in mini-batch size, learning rate,and initial temperature hyper-parameters on three different tasks from (Tassa et al., 2018). Each rowcorresponds to a different mini-batch size. The low variance of the curves and heat-maps shows DrQto be generally robust to exact hyper-parameter settings.
Figure 9: In the data-efficient regime, where we measure performance at 100k environment steps,DrQ is able to enhance its efficiency by performing more training iterations per an environment step.
