Figure 1: GRAPHMASK uses vertex hidden states and messages at layer k (left) as input to a classi-fier g that predicts a mask z(`). We use this to mask the messages of the kth layer and re-compute theforward pass with modified node states (right). The classifier g is trained to mask as many hiddenstates as possible without changing the output of the gated model.
Figure 2: Toy example: a model predicts whether there are more black edges (→) than blue edges.
Figure 3:	Subgraph of retained edges (21% ofthe original) for the query “recordJabel Phi”.
Figure 4:	Percentage of paths used in pre-dictions as a function of the distance betweenthe predicate and the predicted role for theLSTM+GNN model (on the left) and the GNNonly model (on the right).
Figure 5: Example analysis on SRL from the GNN+LSTM model (superfluous arcs are excluded).
Figure 6: Binary Concrete distributions: (a) a Concrete pC and its stretched version pSC ; (b) arectified and stretched (Hard) Concrete pHC.
Figure 8:	Distribution over edge types for retained edges (left) and probability of keeping eachedge type (right); in both cases split by nominal (N) and verbal (V) predicates; edge types are adependency function including computation directionality: flow from the head, (->) or flow to thehead (<-). Excludes edges that occur in less than 10 % cases, and edges judged superfluous in morethan 99 % cases.
Figure 9:	Mean percentage of messages assigned attribution scores above a certain level in theQA model of Section 5, separated by layer. We report scores for GNNExPlainer (—×-), IntegratedGradients (—■—), and GRAPHMASK (-4-).
