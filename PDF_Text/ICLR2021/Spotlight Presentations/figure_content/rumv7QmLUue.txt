Figure 1: Train/test accuracy curves for pruned ResNet-56 models on CIFAR-10 (left) and CIFAR-100 (right) over 25 rounds. Models are pruned using magnitude-based pruning (Magnitude), the proposed extension to loss preservation (Proposed), and loss-preservation based pruning (Loss-pres.).Magnitude-based pruning converges fastest, followed by the proposed measure. Curves for other models and number of rounds are shown in Appendix F.
Figure 2: Correlation between |σ∆σ| and loss-preservation based importance (see Equation 3) at every 10th epoch. Also plotted is the distance between pruning masks (target ratio: 20% filters), as used by You et al. (2020) to decide when to prune a model. As the distance between pruning masks over consecutive epochs reduces, |σ∆σ| becomes more correlated with loss-preservation importance.Results for a similar experiment on Tiny-ImageNet are shown in Appendix I and in an unstructured pruning setting are shown in Section H.1.
Figure 3: ΘTp (t)H(Θ(t))g(Θ(t)) versus ΘTp (t)g(Θ(t)) for ResNet-56 models trained on CIFAR-100. Plots are shown for filters (a) at initialization, (b) after 40 epochs of training, and (c) after complete (160 epochs) training. The correlation is averaged over 3 seeds and plots are for 1 seed.As shown, the measures are highly correlated throughout model training, indicating gradient-norm increase may severely affect model loss if a partially or completely trained model is pruned using ΘTp (t)H(Θ(t))g(Θ(t)). Plots for other models are shown in Appendix G. Results of this experiment in an unstructured pruning setting are shown in Section H.1.
Figure 4: VGG-13: 1 round of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 5: VGG-13: 5 rounds of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 6: VGG-13: 25 rounds of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 7: MobileNet-V1: 1 round of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 8: MobileNet-V1: 5 rounds of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 9: MobileNet-V1: 25 rounds of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 10: ResNet-56: 1 round of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 11: ResNet-56: 5 rounds of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 12: ResNet-56: 25 rounds of pruning. CIFAR-10 (left); CIFAR-100 (right).
Figure 13: ΘTp (t)H(Θ(t))g(Θ(t)) versus ΘTp (t)g(Θ(t)) for VGG-13 models trained on CIFAR-100. Plots are shown for filters (a) at initialization, (b) after 40 epochs of training, and (c) after complete (160 epochs) training.
Figure 14: ΘTp (t)H(Θ(t))g(Θ(t)) versus ΘTp (t)g(Θ(t)) for MobileNet-V1 models trained on CIFAR-100. Plots are shown for filters (a) at initialization, (b) after 40 epochs of training, and (c) after complete (160 epochs) training.
Figure 15: ΘTp (t)H(Θ(t))g(Θ(t)) versus ΘTp (t)g(Θ(t)) for ResNet-56 models trained on CIFAR-100. Plots are shown for filters (a) at initialization, (b) after 40 epochs of training, and (c) after complete (160 epochs) training.
Figure 16: Layer-wise pruning ratios for VGG-13 with a target ratio of 65% pruning. Results are provided for pruning over (a) 1 round, (b) 5 rounds, and (c) 25 rounds. When the model is allowed to train even slightly, GraSP without temperature prunes earlier layers very aggressively.
Figure 17: Layer-wise pruning ratios for MobileNet-V1 with a target ratio of 65% pruning. Results are provided for pruning over (a) 1 round, (b) 5 rounds, and (c) 25 rounds. When the model is allowed to train even slightly, GraSP without temperature prunes earlier layers very aggressively.


