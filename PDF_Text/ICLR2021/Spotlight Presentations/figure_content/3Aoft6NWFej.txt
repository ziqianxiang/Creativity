Figure 1: SQUAD2.0 development setF1 scores oFBERTbasemodels trained with different masking schemes, evaluated ev-ery 200K steps during pretraining.
Figure 2: Scores on SQuAD2.0 development set of BERTBASE models trained for 2.4M steps, asdone by Joshi et al. (2020) when proposing Random-Span Masking. Left: PMI-Masking efficientlyelicits information from limited data. Right: More data, PMI-Masking continues to improve. Seenumerical scores in the appendix, along with the same trends on the RACE benchmark.
Figure 3: Quality measures of top ranking PMIn n-grams lists increased in increments of 50K. Themasking vocabulary size was chosen such that it includes as many n-grams labeled as collocationas possible, while not including too many n-grams labeled as not a collocation, in an internallyconstructed test set detailed below. r is the percent of all positively labeled examples from the testset that appear within the given list (recall), c is the percent of all negatively labeled examples fromthe test set that do not appear within the given list (complement-recall). We aim for a list size forwhich both r and c are high enough, and employ f as a measure for this, finally choosing a list sizeof 800K.
