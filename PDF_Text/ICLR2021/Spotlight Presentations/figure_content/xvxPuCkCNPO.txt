Figure 1: Consider a multi-agent experience tuple for a Listener agent receiving communicationfrom a Speaker agent. In this simplified illustration the Speaker agent receives only environmentobservations, the Listener only receives communication. Our communication correction relabelsthe Listener's experience by generating a new message using the Speaker's current policy π(atm |of)and then generating the new Listener observation using the communication model p(otm+1 |atm). Weshade in red the parts of the experience tuple which we relabel. Note that this relabelling only takesplace for the Listener's sampled multi-agent experience, and not for the Speaker, as in this examplethe Speaker is not itself a Listener.
Figure 2: Cooperative Commu-nication With 5 landmarks. Onlythe Speaker knoWs the targetcolour and must guide the Lis-tener to the target landmark.
Figure 3: Cooperative Communication with 5 landmarks. (Left) MADDPG with communicationcorrection (MADDPG+CC) substantially outperforms MADDPG (n=20, shaded region is standarderror in the mean). (Right) Smoothed traces of individual MADDPG and MADDPG+CC runs.
Figure 4: Hierarchical Commu-nication. Three Speakers, withlimited information and commu-nicating in a chain, must guidethe Listener to the target.
Figure 5: Hierarchical Communication. (Left) MADDPG+OCC substantially outperforms alter-natives on this task (n=20). (Right) Correlation matrices for joint communication actions. Pastcommunication from before learning is a poor reflection of communication after learning. OCCapplied to past samples recovers this correlation structure whereas FCC only partially recovers this.
Figure 6: Covert Communication. When theAllies use the CC their performance is im-proved, whereas when their adversary uses ittheir performance is diminished (n=20).
Figure 7: Cooperative Communication with Multiple Targets. (Left) One mobile Speaker agentand two Listener agents must each move to cover the target of their own colour. However, only theSpeaker can see the target colours. For visualisation we overlay the Speaker’s two discrete messages.
Figure 8: Cooperative Communication with Dropped Messages. (Left) Correcting communicationwhilst accounting for error improves the performance of MADDPG (n=20) (Right) We comparethe performance of a model trained without dropped messages to one trained with 25% droppedmessages. The model trained with dropped messages learns to be more robust to it than the modeltrained without (mean reward for each bar was averaged over 5000 episodes).
Figure 9: Covert Communication without the key(n=20). MADDPG+CC Allies outperform MAD-DPG Allies.
