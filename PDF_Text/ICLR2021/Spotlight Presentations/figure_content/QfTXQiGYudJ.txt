Figure 1: Adversarial attacks on medical images. A clean fundus image is shown in (a) and correctlyclassified as “None” during diabetic retinopathy grading. The perturbations from FGSM (Goodfel-low et al., 2014) attack successfully (i.e., grading as “Mild”) in (b) while PGD (Madry et al., 2017)fails (i.e., grading still as “None”). A clean CT slice is shown in (e) where the lung is correctly seg-mented. The perturbations from FGSM do not attack completely (i.e., cyan mask is still accurate) in(f) while PGD works in (g). A clean endoscopic image detection result is shown in (i). FGSM andPGD are not effective to fail the detector completely. The perturbations produced by SMIA decreasethe analysis performance across different medical image datasets as shown in (d), (h) and (l).
Figure 2: Visualizations of adversarial perturbations. We show how perturbation varies on the first,fourth, and ninth iterations, which is generated via LDEV and LDEV + LSTA on an input fundusimage. The perturbations produced via LDEV + LSTA have low variance and consistent cosinedistance values, which accords with the analysis of the loss stabilization term via KL-divergence.
