Figure 1: Our TANL model translates between input and output text in augmented naturallanguage, and the output is then decoded into structured objects.
Figure 2: Experiments on the CoNLL04 dataset. (a) Our model outperforms the previousstate-of-the-art model SpERT, in low-resource scenarios. (b) Ablation studies where weremove label semantics (numeric labels), augmented natural language format (abridgedoutput) or dynamic programming alignment (no DP alignment), and plot the score differ-ence with the non-ablated TANL.
Figure 3: Low-resource experiments on the CoNLL04 dataset.
Figure 4: Ablation studies on CoNLL04, using different portions of the training dataset.
Figure 5: Percentage of output sentences presenting different kinds of errors, when train-ing with a variable portion of the CoNLL04 training dataset.
