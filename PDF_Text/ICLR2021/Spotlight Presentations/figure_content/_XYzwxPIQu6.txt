Figure 1: A)-B): Illustration of the state space of a 2-unit RNN with flow field (grey) and nullclines(set of points at which the flow of one of the variables vanishes, in blue and red). Insets: Timegraphs of z1 for T = 30 000. A) Perfect line attractor. The flow converges to the line attractor, thusretaining states indefinitely in the absence of perturbations, as illustrated for 3 example trajectories(green). B) Slightly detuned line attractor. The system’s state still converges toward the ”attractorghost”, but then very slowly crawls up within the ‘attractor tunnel’ (green trajectory) until it hitsthe stable fixed point at the intersection of nullclines. Within the tunnel, flow velocity is smoothlyregulated by the gap between nullclines, thus enabling arbitrary time constants. C) Simple 2-unitsolution to the addition problem exploiting the line attractor properties of ReLUs. The output unitserves as a perfect integrator (see Suppl. 6.1.1 for complete parameters).
Figure 2: Comparison of rPLRNN (τ = 5, MMreg = 0.5, cf. Fig. S3) to other methods for A)addition problem, B) multiplication problem and C) sequential MNIST. Top row gives loss as afunction of time series length T (error bars = SEM, n ≥ 5), bottom row shows relative frequency ofcorrect trials. Note that better performance (lower values in top row, higher values in bottom row)is reflected in a more rightward shift of curves. Dashed lines indicate chance level, black dots in Cindicate individual repetitions.
Figure 3: Reconstruction of a 2-time scale DS in limit cycle regime. A) KL divergence (DKL)between true and generated state space distributions. Globally diverging system estimates were re-moved. B) Average MSE between power spectra of true and reconstructed DS and C) split accordingto low (≤ 50 Hz) and high (> 50 Hz) frequency components. Error bars = SEM (n = 33). D) Ex-ample of (best) generated time series (red=reconstruction with T = 2). See Fig. S5A for variable n.
Figure 4: A) Distribution of maximum absolute eigenvalues λ of Jacobians around fixed points forrPLRNN for different τ and L2PLRNN trained on bursting neuron DS. B) Absolute deviations ofmax. ∣λ∣ from 1 (using for each system the one eigenvalue with smallest deviation). C) Same asA for addition problem for rPLRNN (τ = 5) vs. standard, fully L2- (L2f), and partially L2 (L2p)-regularized PLRNN. D) Same as B for the models from C. Error bars = stdv. See also Fig. S8.
Figure S1:	Illustration of the ‘manifold-attractor-regularization’ for the PLRNN’s auto-regressionmatrix A, coupling matrix W , and bias terms h. Regularized values are indicated in red, crossesmark arbitrary values (all other values set to 0 as indicated).
Figure S2:	MSE evaluated between time series is not a good measure for DS reconstruction. A) Timegraph (top) and state space (bottom) for the single neuron model (see section 4.2 and Suppl. 6.1.8)with parameters in the chaotic regime (blue curves) and with simple fixed point dynamics in the limit(red line). Although the system has vastly different limiting behaviors (attractor geometries) in thesetwo cases, as visualized in the state space, the agreement in time series initially seems to indicatea perfect fit. B) Same as in A) for two trajectories drawn from exactly the same DS (i.e., sameparameters) with slightly different initial conditions. Despite identical dynamics, the trajectoriesimmediately diverge, resulting in a high MSE. Dash-dotted grey lines in top graphs indicate thepoint from which onward the state space trajectories were depicted.
Figure S3:	Performance of the rPLRNN for different A) numbers of latent states M, B) values of τ ,and C-E) proportions Mreg/M of regularized states. A-C are for the addition problem, D for themultiplication problem, and E for sequential MNIST. Dashed lines denote the values used for theresults reported in section 4.1.
Figure S4: A) 20-step-ahead prediction error between true and generated observations for rPLRNNas a function of regularization τ . B) KL divergence (DKL) between true and generated state spacedistributions for orthogonal PLRNN (oPLRNN; i.e., the PLRNN with the ‘manifold attractor regu-larization’ replaced by an orthogonality regularization, (A + W)(A + W)T → I), as well as forthe partially (L2p) and fully (L2f) standard L2-regularized PLRNNs (i.e., with all weight parametersfor all (L2f) or only a fraction Mreg/M of states (L2p) driven to 0). Note that the quality of the DSreconstruction does not significantly depend on the strength of regularization τ, or becomes evenslightly worse, for the oPLRNN, L2pPLRNN and L2fPLRNN. Globally diverging estimates wereremoved.
Figure S5:	A) Reconstruction of fast gating variable n (rightmost) not shown in Fig. 3D. For com-pleteness and comparison, other variables have been re-plotted from Fig. 3D as well. B) Exampleof reconstruction of voltage (V , left) and slow gating (h, center) observations, and underlying latentstate dynamics (right) for oPLRNN (with orthogonality regularization on A + W, see Fig. S4 leg-end). C) Example of V (left) and h (center) observations for standard PLRNN, and underlying latentstate dynamics (right). In general, both the standard and the oPLRNN tended to produce many fixedpoint solutions. In those cases where this was not the case, the standard PLRNN tended to reproduceonly the fast components of the dynamics as in the example in C (in agreement with the results inFigs. 3C & 3E), while the oPLRNN tended to capture only the slow components as in the examplein B (as expected from the fact that the orthogonality constraint tends to produce solutions similarto those obtained for the regularized states only, cf. Fig. 3E).
Figure S6:	Reconstruction of aDS with multiple time scales like fast spikes and slow T-waves (simu-lated ECG signal, see McSharry et al. (2003)). A) KL divergence (DKL) between true and generatedstate space distributions as a function of τ . Unstable (globally diverging) system estimates were re-moved. B) Average MSE between power spectra (slightly smoothed) of true and reconstructed DS.
Figure S7:	Same as Fig. 2, illustrating performance for L2RNN (vanilla RNN with L2 regulariza-tion on all weights) and L2fPLRNN (PLRNN with L2 regularization on all weights) on the threeproblems shown in Fig. 2. Note that the L2fPLRNN is essentially not able to learn any of thetasks, likely because a conventional L2 norm drives the PLRNN parameters away from a manifoldattractor configuration (as supported by Fig. 4 and Fig. S8). Results for rPLRNN, vanilla RNN,L2pPLRNN, and LSTM have been re-plotted from Fig. 2 for comparison.
Figure S8:	Same as Fig. 4 for A, B) multiplication problem, and C, D) sequential MNIST. Errorbars = stdv.
Figure S9:	Effect of regularization strength T on rPLRNN network parameters (cf. eq. 1) (regular-ized parameters for states m ≤ Mreg, eq. 1, in red). Note that some of the non-regularized networkparameters (in blue) appear to systematically change as well as T is varied.
