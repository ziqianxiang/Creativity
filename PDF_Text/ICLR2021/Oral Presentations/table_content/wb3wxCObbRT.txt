Table 1: Comparison with the channel pruning methods L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018), ThiNet (Luo et al., 2017), Provable (Liebenwein et al., 2020) and BAR (Lemaire et al., 2019) on CIFAR-10.					Model	Method	Val Acc(%)	Params(M)	FLOPs(%)	Train-Cost Savings(×)	Original	92.9 ± 0.16 (-0.0)	14.99 (100%)	100	1.0×	L1-Pruning	91.8 ± 0.12 (-1.1)	2.98 (19.9%)	19.9	2.5×VGG	SoftNet	92.1 ± 0.09 (-0.8)	5.40 (36.0%)	36.1	1.6×-16	ThiNet	90.8 ± 0.11 (-2.1)	5.40 (36.0%)	36.1	1.6×	Provable	92.4 ± 0.12(-0.5)	0.85 (5.7%)	15.0	3.5×	Ours	92.50 ± 0.10 (-0.4)	0.754 ± 0.005(5.0%)	13.55 ± 0.03	4.95±0.17 X	Original	91.3 ± 0.12 (-0.0)	0.27 (100%)	100	1.0×	L1-Pruning	90.9 ± 0.10(-0.4)	0.15 (55.6%)	55.4	1.1XResNet	SoftNet	90.8 ± 0.13 (-0.5)	0.14 (53.6%)	50.6	1.2X-20	ThiNet	89.2 ± 0.18 (-2.1)	0.18 (67.1%)	67.3	1.1X	Provable	90.8 ± 0.08 (-0.5)	0.10(37.3%)	54.5	17X	Ours	90.91 ± 0.07 (-0.4)	0.096 ± 0.002 (35.8%)	50.20 ± 0.01	2.40 ± 0.09 XWRN	Original	96.2 ± 0.10 (-0.0)	36.5 (100%)	100	1.0X-28	L1-Pruning	95.2 ± 0.10(-1.0)	7.6 (20.8%)	49.5	1.5X-10	BAR(16x V)	92.0 ± 0.08 (-4.2)	2.3 (6.3%)	1.5	26x	Ours	95.32 ± 0.11 (-0.9)	3.443 ± 0.010(9.3%)	28.25 ± 0.04	3.12 ± 0.11xat which the temperature increases during training. Continuation methods with global temperatureschedulers have been successfully applied in pruning and NAS. However, in our case, a global
Table 2: Comparison with channel pruning methods: L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018) andProvable (Liebenwein et al., 2020) on ImageNet.
Table 3: Results comparing with AutoGrow (Wen et al., 2020) on CIFAR-10 and ImageNet.
Table 4: Performance comparing with NAS methods AmoebaNet-A (Real et al., 2019), MnasNet (Tan et al., 2019), EfficientNet-B0 (Tan & Le, 2019), DARTS (Liu et al., 2019) and Proxyless-Net (Cai et al., 2019) on ImageNet.
Table 5: Comparison with random pruning baseline on CIFAR-10.
Table 6: Overview of the pruning performance of each algorithm on MobileNetV1 ImageNet.
Table 7: Results on the PASCAL VOC dataset.
Table 8: Results on the PTB dataset.
Table 9: A detailed comparison among seed architecture variants of our method and AutoGrow (Wenet al., 2020). In growing units term, “Basic” and “Bottleneck” refer to ResNets with standard basicand bottleneck residual blocks while “PlainLayers” refers to standard convolutional layer, BN, andReLu layer combinations in VGG-like networks without shortcuts.
