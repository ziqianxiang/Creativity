Table 1: Mean Opinion Scores (MOS) for our final EATS model and the ablations described inSection 4, sorted by MOS. The middle columns indicate which components of our final modelare enabled or ablated. Data describes the training set as Multispeaker (MS) or Single Speaker(SS). Inputs describes the inputs as raw characters (Ch) or phonemes (Ph) produced by Phonemizer.
Table 2: Mean Opinion Scores (MOS) for the top four speakers With the most data in our training set.
Table 3: EATS batched inference benchmarks, timing inference (speech generation) on a GoogleCloud TPU v3 (1 chip with 2 cores), a single NVIDIA V100 GPU, or an Intel Xeon E5-1650 v4CPU at 3.60 GHz (6 physical cores). We use a batch size of 2, 8, or 16 utterances (Utt.), each 30seconds long (input length of 600 phoneme tokens, padded if necessary). One “run” consists of 10consecutive forward passes at the given batch size. We perform 101 such runs and report the medianrun time (Med. Run Time (s)) and the resulting Realtime Factor, the ratio of the total duration of thegenerated speech (Length / Run (s)) to the run time. (Note: GPU benchmarking is done using singleprecision (IEEE FP32) floating point; switching to half precision (IEEE FP16) could yield furtherspeedups.)A Hyperparameters and other detailsOur models are trained for 5 ∙ 105 steps, where a single step consists of one discriminator updatefollowed by one generator update, each using a minibatch size of 1024, with batches sampled indepen-dently in each of these two updates. Both updates are computed using the Adam optimizer (Kingma &Ba, 2015) with β1 = 0 and β2 = 0.999, and a learning rate of 10-3 with a cosine decay (Loshchilov& Hutter, 2017) schedule used such that the learning rate is 0 at step 500K. We apply spectralnormalisation (Miyato et al., 2018) to the weights of the generator’s decoder module and to thediscriminators (but not to the generator’s aligner module). Parameters are initialised orthogonallyand off-diagonal orthogonal regularisation with weight 10-4 is applied to the generator, followingBigGAN (Brock et al., 2018). Minibatches are split over 64 or 128 cores (32 or 64 chips) of GoogleCloud TPU v3 Pods, which allows training of a single model within up to 58 hours. We use cross-
Table 4: The symbols in this table are replaced or removed when they appear in phonemizer’s output.
Table 5: Mean Opinion Scores (MOS) and Frechet DeePSPeech Distances (FDSD) for our finalEATS model and the ablations described in Section 4, sorted by MOS. FDSD scores presented herewere comPuted on held-out validation multi-sPeaker set and therefore could not be obtained for theSingle SPeaker ablation. Due to dataset differences, these are also not comParable with the FDSDvalues reported for GAN-TTS by BinkoWSki et al. (2020).
Table 6: A comparison of TTS methods. The model stages described in each paper are shownby linking together the inputs, outputs and intermediate representations that are used: characters(Ch), phonemes (Ph), mel-spectrograms (MelS), magnitude spectrograms (MagS), cepstral fea-tures (Cep), linguistic features (Ling, such as phoneme durations and fundamental frequencies, orWORLD (Morise et al., 2016) features for Char2wav (Sotelo et al., 2017) and VoiceLoop (Taigmanet al., 2017)), and audio (Au). Arrows with various superscripts describe model components: autore-gressive (AR), feed-forward (FF), or feed-forward requiring distillation (FF*). Arrows without asuperscript indicate components that do not require learning. 1 Stage means the model is trained ina single stage to map from unaligned text/phonemes to audio (without, e.g., distillation or separatevocoder training). EATS is the only feed-forward model that fulfills this requirement.
