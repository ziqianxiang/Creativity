Table 1: Top-1 error rate on various datasets and models. For CIFAR-100, we train eachmodel with three different random seeds and report the mean error.
Table 2: WSOL results on ImageNet and ECE (%) measurements of CIFAR-100 classifiers.
Table 3: Top-1 error rates of various mixup methods for background corrupted ImageNetvalidation set. The values in the parentheses indicate the error rate increment by corruptedinputs compared to clean inputs.
Table 4: Top-1 error rates of mixup baselines with multiple mixing inputs on CIFAR-100and PreActResNet18. We report the mean values of three different random seeds. Notethat Co-Mixup optimally determines the number of inputs for each output by solving theoptimization problem.
Table 5: A summary of notations.
Table 6: Mean function values of the solutions over 100 different random seeds. Rel. errormeans the relative error between ours and random guess.
Table 7: Mean function values of the solutions over 100 different random seeds. We reportthe standard deviations in the parenthesis. Random represents the random guess algorithm.
Table 8: Convergence time (s) of the algorithms.
Table 9: Top-1 classification test error on the Google commands dataset. We stop training ifvalidation accuracy does not increase for 5 consecutive epochs.
Table 10: Expected calibration error (%) of classifiers trained with various mixup methodson CIFAR-100, Tiny-ImageNet and ImageNet. Note that, at all of three datasets, Co-Mixupoutperforms all of the baselines in Top-1 accuracy.
Table 11: Hyperparameter sensitivity results (Top-1 error rates) on CIFAR-100 with PreAc-tResNet18. We report the mean values of three different random seeds.
Table 12: Top-1 error rates of VAT on CIFAR-100 dataset with PreActResNet18.
