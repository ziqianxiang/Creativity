Table 1: Details of Vision Transformer model variants.
Table 2: Comparison with state of the art on popular image classification benchmarks. We re-port mean and standard deviation of the accuracies, averaged over three fine-tuning runs. VisionTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on alldatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on thesmaller public ImageNet-21k dataset performs well too. * Slightly improved 88.5% result reportedin Touvron et al. (2020).
Table 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-ing rate warmup of 10k steps. For ImageNet we found it beneficial to additionally apply gradientclipping at global norm 1. Training resolution is 224.
Table 4: Hyperparameters for fine-tuning. All models are fine-tuned with cosine learning rate decay,a batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,fine-tuning resolution is 384.
Table 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-ageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Modelsare fine-tuned at 384 resolution. Note that the ImageNet results are computed without additionaltechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.
Table 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the mainpaper.
Table 7: Fine-tuning ResNet models pre-trained with Adam and SGD.
Table 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated onImageNet 5-shot linear.
Table 9: Breakdown of VTAB-1k performance across tasks.
