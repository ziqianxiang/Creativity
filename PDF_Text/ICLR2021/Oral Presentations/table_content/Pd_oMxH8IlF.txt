Table 1: SHAPES-SyGeT accuracies. NMNs are trained with 20 or 135 ground-truth programs;FiLM (Perez et al., 2018) and MAC (Hudson & Manning, 2018) do not use program supervision.
Table 2: Task accuracy on the CLEVR validation set and each CLOSURE category for modelstrained with and without IL, using 100 ground-truth programs.
Table 3: IL hyperparameters used in our experiments.
Table 4: Split of questions in the SHAPES and SHAPES-SyGeT datasets.
Table 5: Overview and splits of the SHAPES-SyGeT templates. The placeholder COLOR cantake values in ‘red,’ ‘green,’ and ‘blue,’ and SHAPE can be either ‘circle,’ ‘triangle,’ or ‘square.’RELATIVE(1) is a placeholder for one of the positional prepositions ‘above,’ ‘below,’ ‘left of,’ or‘right of,’ whereas RELATIVE(2) is a combination of two RELATIVE(1)s, such as ‘above leftof.’the dataset. However, our goal is not to pursue state-of-the-art performance on GQA, but insteadto study if we can still observe the advantage of IL in a limited ground-truth program setting forour NMN setup. Still, due to the significantly higher complexity of GQA, a few modifications andspecial considerations become necessary.
Table 6: Ablation experiments for models on SHAPES-SyGeT trained with 20 ground-truth pro-grams. ‘FullSN’: spectral normalization applied throughout training;“NoSN’: no spectral normal-ization in any phase; ‘NoRetrain’: new program generator is taken from the previous generationwithout any re-training; ‘Seeded’: new execution engine is initialized to the state after the previouslearning phase; ‘NoReset’: execution engine is not re-initialized at the start of a generation.
Table 7: Vocabulary sizes for SHAPES-SyGeT, CLEVR, and GQA token types after pre-processing.
Table 8: GQA validation accuracies. In Vector-NMN on GT programs, the execution engine alwaysassembles modules according to the ground-truth programs, including at test time. For all the modelsexcept Vector-NMN+IL, the program generator is retained between generations. ‘ResetEE’: resetparameters except the FiLM embeddings; ‘FinetuneEE’: train the execution engine on a frozenprogram generator before joint training; ‘IL’: the full IL algorithm.
