Figure 1: Our problem setting. Our training dataset consists of near-optimal state-action trajectories (withoutreward labels) from a wide range of tasks. Each task might involve interacting with a different set of objects.Even for the same set of objects, the task can be different depending on our objective. For example, in the upper right corner, the objective could be picking up a cup, or it could be to place the bottle on the yellow cube.
Figure 2: PARROT. Using successful trials from a large variety of tasks, we learn an invertible mapping fφ that maps noise z to useful actions a. This mapping is conditioned on the current observation, which in our case is an RGB image. The image is passed through a stack of convolutional layers and flattened to obtain an image encoding ψ(s), and this image encoding is then used to condition each individual transformation fi of our overall mapping function fφ. The parameters of the mapping (including the convolutional encoder) are learned through maximizing the conditional log-likelihood of state-action pairs observed in the dataset. When learning a new task, this mapping can simplify the MDP for an RL agent by mapping actions sampled from a randomly initialized policy to actions that are likely to lead to useful behavior in the current scene. Since the mapping is invertible, the RL agent still retains full control over the action space of the original MDP, simply the likelihood of executing a useful action is increased through use of the pre-trained mapping.
Figure 3: Tasks. A subset of our evaluation tasks,with one task shown in each row. In the first task(first row), the objective is to pick up a can and place it in the pan. In the second task, the robot must pick up the vase and put it in the basket. In the third task, the goal is to place the chair on top of the checkerboard. In the fourth task, the robot must pick up the mug and hold it above a certain height.Initial positions of all objects are randomized, and must be inferred from visual observations. Not all objects in the scene are relevant to the current task.
Figure 4: We plot trajectories from executing a random policy, with and without the behavioral prior. We see that the behavioral prior substantially increases the likelihood of executing an action that is likely to lead to a meaningful interaction with an object, while still exploring a diverse set of actions.
Figure 5: Results. The lines represent average performance across multiple random seeds, and the shaded areas represent the standard deviation. PARROT is able to learn much faster than prior methods on a majority of the tasks, and shows little variance across runs (all experiments were run with three random seeds, computational constraints of image-based RL make it difficult to run more seeds). Note that some methods that failed to make any progress on certain tasks (such as “Place Sculpture in Basket”) overlap each other with a success rate of zero. SAC and VAE-features fail to make progress on any of the tasks.
Figure 6: Impact of dataset size on performance. We observe that training on 10K, 25K or 50K trajectories yields similar performance.
Figure 7: Impact of train/test mismatch on performance. Each plot shows results for four tasks. Note that for the pick and place tasks, the performance is close to zero, and the curves mostly overlap each other on the x-axis.
Figure 8: Coupling layer architecture.
Figure 9: Policy and Q-function network architectures.Figure 10: In the first row, the objective is to grasp a can and lift it above a certain height. Rows two and three are similar, except the objective is to grasp a vase and a baseball cap, respectively. The final row depicts a task where the goal is to pick the baseball cap and place it on the marble cube.
Figure 10: In the first row, the objective is to grasp a can and lift it above a certain height. Rows two and three are similar, except the objective is to grasp a vase and a baseball cap, respectively. The final row depicts a task where the goal is to pick the baseball cap and place it on the marble cube.
Figure 11: Train objects.
Figure 12: Test objects