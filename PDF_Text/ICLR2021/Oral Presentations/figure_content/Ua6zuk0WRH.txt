Figure 1: Approximation of the regular attention mechanism AV (before D -1 -renormalization) via (random)feature maps. Dashed-blocks indicate order of computation with corresponding time complexities attached.
Figure 2: Left: Symmetrized (around origin) utility function r (defined as the ratio of the mean squared errors(MSEs) of estimators built on: trigonometric and positive random features) as a function of the angle φ (inradians) between input feature vectors and their lengths l. Larger values indicate regions of (φ, l)-space withbetter performance of positive random features. We see that for critical regions with φ large enough (smallenough softmax-kernel values) our method is arbitrarily more accurate than trigonometric random features. Plotpresented for domain [-π, π] × [-2, 2]. Right: The slice of function r for fixed l = 1 and varying angle φ.
Figure 3: Comparison of Transformer and Performer in terms of forward and backward PaSS speed andmaximum L allowed. "X" (OPT) denotes the maximum possible speedup achievable, when attention simplyreturns the V-matrix. Plots shown up to when a model produces an out of memory error on a V100 GPU with16GB. Vocabulary size used was 256. Best in color.
Figure 4: MSE of the approximation output when comparing Orthogonal Vs IID features and trigonometricsin/cos vs positive features. We took L = 4096, d = 16, and varied the number of random samples m. StandarddeViations shown across 15 samples of appropriately normalized random matrix input data.
Figure 5: We transferred the original pretrained Transformer's weights into the Performer, which producesan initial non-zero 0.07 accuracy (dotted orange line), but quickly recoVers accuracy in a small fraction of theoriginal number of gradient steps. HoweVer on PG-19, Trigonometric (TRIG) softmax approximation becomeshighly unstable (full curVe in Appendix D.2), while positiVe features (POS) (without redrawing) and Linformer(which also approximates softmax) even with redrawn projections, plateau at the same perplexity. PositiVesoftmax with feature redrawing is necessary to match the Transformer, with SMREG (regularization from Sec.
Figure 6: Train = Dashed, Validation = Solid. For TrEMBL, we used the exact same model parameters(nheads, nlayers, dff, d) = (8, 36, 1024, 512) from (Madani et al., 2020) for all runs. For fairness, all TrEMBLPerformer-RELU (U)Transformer (U)Reformer (U)Performer-RELU (B)Performer-SOFTMAX (B)Transformer (B)Linformer (B)experiments used 16x16 TPU-v2’s. Batch sizes were maximized for each separate run given the computeconstraints. Hyperparameters can be found in Appendix A. Extended results including dataset statistics, out ofdistribution evaluations, and visualizations, can be found in Appendix C.
Figure 7: Train = Dashed, Validation = Solid. For ImageNet64, all models used the standard (nheads, dff, d) =(8, 2048, 512). We further show that our positive softmax approximation achieves the same performance asReLU in Appendix D.2. For concatenated TrEMBL, we varied nlayers ∈ {1, 2, 3} for the smaller Transformer.
Figure 8: Visual representation of the prefix-sum algorithm for unidirectional attention. For clarity, we omitattention normalization in this visualization. The algorithm keeps the prefix-sum which is a matrix obtainedby summing the outer products of random features corresponding to keys with value-vectors. At each giveniteration of the prefix-sum algorithm, a random feature vector corresponding to a query is multiplied by the mostrecent prefix-sum (obtained by summing all outer-products corresponding to preceding tokens) to obtain a newrow of the matrix AV which is output by the attention mechanism.
Figure 9: Visualization of the estimated empirical distribution for the 20 standard amino acids, colored by theirclass. Note the consistency with the statistics on the TrEMBL web page.
Figure 10: We show the attention matrices for the first 4 layers and all 8 heads (each row is a layer, each columnis head index, each cell contains the attention matrix across the entire BPT1_BOVIN protein sequence). Notethat many heads show a diagonal pattern, where each node attends to its neighbors, and some heads show avertical pattern, where each head attends to the same fixed positions.
Figure 11: We illustrate in more detail two attention heads. The sub-figures correspond respectively to: (1)Head 1-2 (second layer, third head), (2) Head 4-1 (fifth layer, second head). Note the block attention in Head 1-2and the vertical attention (to the start token (‘M’) and the 85th token (‘C’)) in Head 4-1.
Figure 13: Amino acid similarity matrix estimated from attention matrices aggregated across a small subsetof sequences, as described in Vig et al. (Vig et al., 2020). The sub-figures correspond respectively to: (1) thenormalized BLOSUM matrix, (2) the amino acid similarity estimated via a trained Performer model. Note thatthe Performer recognizes highly similar amino acid pairs such as (D, E) and (F, Y).
Figure 14: Output approximation errors between a vanilla Transformer and a Performer (withorthogonal features) for varying numbers of layers.
Figure 15: Best viewed zoomed in. Left: The importance of redrawing features. If redrawing is not50FTMAX (B) No Redraw, Seed 150FTMAX (B) No Redraw, Seed 250FTMAX (B) Redrawused, an "unlucky" set of random features may cause training degradation, shown by the early-stoppedcurve with Seed 1, while a ‘lucky’ set of random features may cause no issue, shown by the curvewith Seed 2. Redrawing allows the training to correct itself, as seen at the black vertical line. Middle:Using the same 8x8 TPU-v2 compute and same 6-layer standard model, approximate softmax withpositive features achieves the same result as generalized ReLU attention. Right: Zoomed out view ofright subfigure of Fig. 5, showing that Trigonometric softmax causes very unstable training behaviors.
Figure 16: To emphasize the highest accuracy runs but also show the NaN issues with certain kernelswhich caused runs to stop early, we set both x and y axes to be log-scale. We tested kernels definedby different functions f (see: Sec. 2.2): sigmoid, exponential, ReLU, absolute, gelu, cosine (originalsoftmax approximation), tanh, and identity. All training runs were performed on 2x2 TPU-v2’s, 128Figure 17: We also performed a similar setup as Fig. 16 for 4x4 TPU-v2’s.
Figure 17: We also performed a similar setup as Fig. 16 for 4x4 TPU-v2’s.
Figure 18: Left: In the unidirectional 36-ProGen setting, we ran 3 seeds of the Linear Transformer,Performer-RELU (B)Performer-SOFTMAX (B)Transformer (B)Linear Transformer (B)and found that all 3 seeds produced exploding gradients very early on, stopping the training run.
Figure 19: Upper Table: Results on Long-Range Arena benchmark. Best model is in boldface andsecond best is underlined. Lower Table: Benchmark results of all X-former models with a consistentbatch size of 32 across all models. The authors report relative speed increase/decrease in comparisonwith the vanilla Transformer in brackets besides the steps per second. Memory usage refers to perdevice memory usage across each TPU device. Benchmarks are run on 4x4 TPU-v3 chips. RightFig: Performance (y-axis), speed (x-axis), and memory footprint (size of the circles) of differentmodels.
Figure 20: CaPtionS (1) and (2) for each 2x2 subfigure mentioned above.
Figure 21: Caption (3) for this 2x2 subfigure mentioned above.
