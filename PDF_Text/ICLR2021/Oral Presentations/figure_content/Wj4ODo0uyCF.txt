Figure 1: The model architecture used for our experiments. We introduce a CLSR layer after every transformersub-layer in the encoder and the decoder. The gating layer learns to route every input through either the LSprojection layer, or a shared projection layer. We analyze the outputs of the gating layers to develop a MNMTarchitecture with LS projections.
Figure 2: Average BLEU 2(a),2(b) and win ratio 2(c),2(d) over all test language pairs for O2M and M2O onOPUS-100 when varying the budget p. Baseline: multilingual baseline on the original training data; Oversam-ple: oversampling low-resourCe data with a temperature of 5.
Figure 3: LSScore of encoder and decoder sub-layers for O2M 3(a), 3(b) and M2O 3(c), 3(d) on OPUS-100.
Figure 4: Average BLEU 4(a),4(b) and win ratio 4(c),4(d) over all test language pairs for O2M and M2O onWMT-14 when varying the budget p.
Figure 5: Training data distribution over language pairs for OPUS-100 and WMT-14.
Figure 6: Heatmap of LSScore distribution on OPUS-100 trained w/ and w/o oversampling. X-axis denoteslanguage pairs ranked by training data size, and y-axis denotes encoder (enc) and decoder (dec) sub-layers witha format of “enc/dec.layer types.layer index”. Darker color indicates a larger LSScore.
Figure 7:	Impact of the budget p on average test BLEU for High/Med/Low-resource languages on OPUS-100.
Figure 8:	Impact of the budget p on average test BLEU for High/Med/Low-resource languages on WMT-14.
Figure 9: Heatmap of LSScore distribution on WMT-14.
Figure 10: LSScore of encoder and decoder sub-layers for O2M 10(a), 10(b) and M2O 10(c), 10(d) on WMT-14.
