Figure 1: We employ two “renderers”: a GAN (StyleGAN in our work), and a differentiable graphics renderer(DIB-R in our work). We exploit StyleGAN as a synthetic data generator, and we label this data extremelyefficiently. This “dataset” is used to train an inverse graphics network that predicts 3D properties from images.
Figure 2: We show examples of cars (first two rows) synthesized in chosen viewpoints (columns). To getthese, We fix the latent code Wv that controls the viewpoint (one code per column) and randomly sample theremaining dimensions of (Style)GAN’s latent code (to get rows). Notice how well aligned the two cars are ineach column. In the third row we show the same approach applied to horse and bird StyleGAN.
Figure 3: A mapping network mapscamera, shape, texture and backgroundinto a disentangled code that is passedto StyleGAN for “rendering”. We referto this network as StyleGAN-R.
Figure 4: 3D Reconstruction Results: Given input images (1st column), we predict 3D shape, texture, andrender them into the same viewpoint (2nd column). We also show renderings in 3 other views in remainingcolumns to showcase 3D quality. Our model is able to reconstruct cars with various shapes, textures andviewpoints. We also show the same approach on harder (articulated) objects, i.e., bird and horse.
Figure 5: Comparisonon Pascal3D test set: Wecompare inverse graphicsnetworks trained on Pas-cal3D and our StyleGANdataset. Notice consider-ably higher quality of pre-diction when training onthe StyleGAN dataset.
Figure 6: Ablation Study: We ablate the use of multi-view consistency loss. Both texture are shape areworse without this loss, especially in the invisible parts (rows 2, 5, denoted by "w.o M. V.” - no multi-viewconsistency used during training), showcasing the importance of our StyleGAN-multivew dataset.
Figure 7: Dual Renderer: Given input images (1st column), we first predict mesh and texture, and renderthem with the graphics renderer (2nd column), and our StyleGAN-R (3rd column).
Figure 8: Latent code manipulation: Given an input image (col 1), we predict 3D properties and synthesizea new image with StyleGAN-R, by manipulating the viewpoint (col 2, 3, 4). Alternatively, we directly opti-mize the (original) StyleGAN latent code w.r.t. image, however this leads to a blurry reconstruction (col 5).
Figure 10: 3D Manipulation:We sample 3 cars in column 1.
Figure 11: Real Image Manipulation: Given input images (1st col), we predict 3D properties and use ourStyleGAN-R to render them back (2nd col). We swap out shape, texture & background in cols 3-5.
Figure A: Layer Visualization for Each Block: Notice that the car contour starts to appear in blocks4 and higher. This supports some of our findings that the early blocks control viewpoint (and otherglobal properties), while shape, texture and background are controlled in the higher layers.
Figure B: All Viewpoints: We show an example of a car, bird and a horse synthesized in all of our chosenviewpoints. While shape and texture are not perfectly consistent across views, they are sufficiently accurate toenable training accurate inverse graphics networks in our downstream tasks. Horses and birds are especiallychallenging due to articulation. One can notice small changes in articulation across viewpoints. Dealing witharticulated objects is subject to future work.
Figure C: Dataset Overview: We synthesize multi-view datasets for three classes: car, horse, and bird. Ourdatasets contain objects with various shapes, textures and viewpoints. Notice the consistency of pose of objectin each column (for each class). Challenges include the fact that for all of these objects StyleGAN has notlearned to synthesize views that overlook the object from above due to the photographer bias in the originaldataset that StyleGAN was trained on.
Figure D: We show examples of cars synthesized in chosen viewpoints (columns) along with annotations. Toprow shows the pose bin annotation, while the images show the annotated keypoints. We annotated keypoints forthe car example in the first image-row based on which we compute the accurate camera parameters using SfM.
Figure E: Comparison of Different Camera Initializations: The first row shows predictions from keypoint-Initialization (cameras computed by running SFM on annotated keypoints) and the second row show resultsobtained by training with view-Initialization (cameras are coarsely annotated into 12 view bins). Notice howclose the two predictions are, indicating that coarse viewpoint annotation is sufficient for training accurateinverse graphics networks. Coarse viewpoint annotation can be done in 1 minute.
Figure F: Comparison on PASCAL3D imagery: We compare PASCAL-model with StyleGAN-model onPASCAL3D test set. While the predictions from both models are visually good in the corresponding imageview, the prediction from StyleGAN-model have much better shapes and textures as observed in other views.
Figure G: Comparison on Images from the Web: We compare the PASCAL-model with our StyleGAN-model on images downloaded from the web. While the predictions from both models are visually good inthe corresponding image view, the prediction from StyleGAN-model have much better shapes and textures asobserved in other views.
Figure H: 3D Reconstruction Results for Car, Horse and Bird Classes: We show car, horse and birdexamples tested on the images from the StyleGAN dataset test sets. Notice that the model struggles a little inreconstructing the top of the back of the horse, since such views are lacking in training.
Figure I:	User Study Interface (AMT): Predictions are rendered in 6 views and we ask users to choose theresult with a more realistic shape and texture that is relevant to the input object. We compare both the baseline(trained on Pascal3D dataset) and ours (trained on StyleGAN dataset). We randomize their order in each HIT.
Figure J:	Dual Rendering: Given the input image, we show the DIB-R-rendered predictions in rows (1, 4)and StyleGAN-R’s results in rows (2, 5). We further shows the neural rendering results from the originalStyleGAN model, where we only learn the mapping network but keep the StyleGAN weights fixed. Clearly,after fine-tuning, StyleGAN-R produces more consistent results.
Figure K: Light Prediction: Given the input image, we show rendering (using the OpenGL renderer used inDIB-R) results with light (columns 2, 5) and results with just textures (columns 3, 6). We find that the tworesults are quite similar, which indicates that we did not learn a good predictor for lighting. Moreover, we findthat higher order lighting, such as reflection, high-specular light are merged into texture, as shown in the secondrow. We aim to resolve this limitation in future work.
Figure L: Real Image Editing. Given an input image (column 1), we use our inverse graphics network topredict the 3D properties and apply StyleGAN-R to re-render these (column 2, 3). We manipulate the carsize/scale (row 1-3), azimuth (row 4-6) and elevation (Row 7-9).
Figure M: Bird Camera Controller: We manipulate azimuth, scale, elevation parameters with StyleGAN-Rto synthesize images in new viewpoints while keeping content code fixed.
Figure N:	Bird 3D Manipulation: We sample 3 birds in column 1. We replace the shape of all birds with theshape of Bird 1 (red box) in 2nd column. We transfer texture of Bird 2 (green box) to other birds (3rd col). Inlast column, we paste background of Bird 3 (cyan box) to the other birds. Examples indicated with boxes areunchanged.
Figure O:	3D Reconstruction Failure Cases: We show examples of failure cases for car, bird and horse. Ourmethod tends to fail to produce relevant shapes for objects with out-of-distribution shapes (or textures).
Figure P: Ablation Study: We ablate the use of multi-view consistency and perceptual losses by showingresults of 3D predictions. Clearly, the texture becomes worse in the invisible part if we remove the multi-view consistency loss (rows 2, 5, denoted by “w.o M. V.”, which denotes that no multi-view consistency wasused during training), showcasing the importance of our StyleGAN-multivew dataset. Moreover, the texturesbecome quite smooth and lose details if we do not use the perceptual loss (rows 3, 6, noted by “w.o P.”, whichdenotes that no perceptual loss was used during training).
