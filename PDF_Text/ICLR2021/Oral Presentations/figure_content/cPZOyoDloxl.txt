Figure 1: Left: SMiRL observes a state st and computes a reward rt as the negative surprise under its currentmodel pθt-1 (st), given by log pθt-1 (st). Then the model is updated on the agents state history, including st,to yield pθt. The policy ∏φ (at |st ,θt, t) then generates the action at. Right: This procedure leads to complexbehavior in environments where surprising events happen on their own. In this cartoon, the robot experiences awide variety of weather conditions when standing outside, but can avoid these surprising conditions by buildinga shelter, where it can reach a stable and predictable states in the long run.
Figure 2: Evaluation environments. Top row, left to right: Tetris environment, VizDoom TakeCoverand DefendTheLine, HauntedHouse with pursuing “enemies,” where the agent can reach a morestable state by finding the doors and leaving the region with enemies. Bottom row, left to right:Humanoid next to a Cliff , Humanoid on a Treadmill, Pedestal, Humanoid learning to walk.
Figure 3: Comparison between SMiRL, ICM, RND, and an Oracle baseline that uses the true reward,evaluated on Tetris with (top-left) number of deaths per episode (lower is better), (top-center) rowscleared per episode (higher is better), and in TakeCover (top-right) and DefendTheLine (bottom-left)on amount of damage taken (lower is better). In all cases, the RL algorithm used for training isDQN, and all results are averaged over 6 random seeds, with the shaded areas indicating the standarddeviation. In Tetris (bottom-center) and TakeCover (bottom-right) methods are evaluated on how theyimprove learning when added to the environment reward function.
Figure 4: Cliff , Treadmill and Pedestal results. In all cases, SMiRL reduces episodes with falls (loweris better). SMiRL that uses the VAE for representation learning typically attains better performance.
Figure 5: Here we show SMiRL’s incentive for longer-term planning in the HauntedHouse envi-ronment. On the top-left, we see that SMiRL on its own does not explore well enough to reach thesafe room on the right. Adding exploration via Counts (bottom-left) allows SMiRL to discover moreoptimal entropy reducing policies, shown on the right.
Figure 6: Tetris imitation bystarting pθ (s) with left image.
Figure 7: Left: We combine SMiRL with the survival time task reward in the DefendTheLine task.
Figure 9: SMiRL results when the Humanoid environments are trained without early terminationbased resets (fixed episode lengths). Cliff and Pedestal still produce entropy minimizing policies thatreduce falls. RL has difficulty with optimizing the more challenging Treadmill environment.
