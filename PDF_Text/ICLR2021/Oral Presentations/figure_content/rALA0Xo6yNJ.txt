Figure 1: Goal-conditioned supervised learning (GCSL): The agent learns how to reach goals bysampling trajectories, relabeling the trajectories to be optimal in hindsight and treating them as expertdata, and then performing supervised learning via behavioral cloning.
Figure 2: Evaluation Tasks: We study the following goal-reaching tasks: (from left to right)2D navigation, robotic pushing, Lunar Lander, robotic door opening, dexterous object manipulation.
Figure 3: On a majority of tasks, GCSL performs well or better compared to more complex RLalgorithms like PPO (Schulman et al., 2017) or TD3-HER (Andrychowicz et al., 2017). Shadedregions denote the standard deviation across 5 random seeds (lower is better).
Figure 4: Hyperparameter Robustness: Distribution of final performance of GCSL and TD3-HERacross nine hyperparameter configurations in each environment (see Section 5.4 for details). Highervalues indicate better performance, and tightly clustered distributions indicate lower sensitivity tohyperparameters. GCSL is more performant and robust to hyperparameters than TD3-HER.
Figure 5: Ablations of GCSL on Lunar Lan-der and pushing. Other domains in AppendixA.4.
Figure 6: Demonstrations: GCSL incorpo-rates expert demonstrations more effectivelythan TD3-HER.
Figure 7:	Performance across variations of GCSL (Section 5.3) for all experimental domains.
Figure 8:	Policy update frequency: Performance when varying the ratio of policy update steps toenvironment steps. GCSL performs well even when significantly more gradient steps are taken on thereplay buffer data.
Figure 9:	Optimizing discounted return vs final-timestep objective: Performance of TD3-HERand PPO when optimizing for the final time-step objective and the discounted return objective, asmeasured by median final distance to the goal. Both RL methods perform better with the discountedreturn objective uniformly across environments, so we use the discounted-return comparisons in themain paper.
Figure 10: GCSL converges faster andlearns a more accurate goal-reaching policythan HPG (Rauber et al., 2017)Figure 11: Exploration strategies: Whencombined with the exploration strategy fromEcoffet et al. (2020), GCSL explores moreand learns a more accurate goal-reaching pol-icy when navigating a 2x larger four-roomsdomain.
Figure 11: Exploration strategies: Whencombined with the exploration strategy fromEcoffet et al. (2020), GCSL explores moreand learns a more accurate goal-reaching pol-icy when navigating a 2x larger four-roomsdomain.
Figure 12: Alternative Evaluation Metrics: Here we present plots reporting (1) median distance togoal at the final timestep, (2) median minimum distance to goal within a trajectory (3) proportionof trajectories that were at the desired goal at the final timestep (3) proportion of trajectories thatwere at the desired goal at any timestep. As mentioned in Appendix A.2, we additionally providecomparisons to a version of TD3-HER that uses discretized actions. See Table 1 for time to goalmetrics17Published as a conference paper at ICLR 2021B Theoretical AnalysisB.1 Proof of Theorem 4.1We will assume a discrete state space in this proof, and denote a trajectory as τ ={s0, a0, . . . , sT , aT }. Let the notation G(τ) = sT denote the final state of a trajectory, whichrepresents the goal that the trajectory reached. As there can be multiple paths to a goal, we letτg = {τ : G(τ) = g} denote the set of trajectories that reach a particular goal g. We abbreviate apolicy's trajectory distribution as π(τ|g) = p(so) QT=0 ∏(at∣st, g)T(st+ι∣st, at). The target goal-reaching objective we wish to optimize is the probability of reaching a commanded goal, when goalsare sampled from a pre-specified distribution p(g).
Figure 13: Examples of trajectories generated by GCSL for the Lunar Lander and 2D Roomenvironments. Stars indicate the goal state.
