Figure 1: Method overview. We use regularized evolution to evolve a population of RL algorithms. A mutatoralters top performing algorithms to produce a new algorithm. The performance of the algorithm is evaluatedover a set of training environments and the population is updated. Our method can incorporating existingknowledge by starting the population from known RL algorithms instead of purely from scratch.
Figure 2: Visualization of a RL algorithm,DQN, as a computational graph which com-PUtes the loss L = (Q(St, at) - (rt + Y *maxa Qtarg (st+1 , a)))2. Input nodes are in blue,parameter nodes in gray, operation nodes in or-ange, and output in green.
Figure 3: Left: Meta-training performance over different number of environments from scratch, and boot-strapping. Plotted as RL evaluation performance (sum of normalized training return across the training en-vironments) over the number of candidate algorithms. Shaded region represents one standard deviation over10 random seeds. More training environments leads to better algorithms. Bootstrapping from DQN speedsup convergence and higher final performance. Right: Meta-training performance histogram for bootstrappedtraining. Many of the top programs have similar structure (Appendix D).
Figure 4: Performance of learned algorithms (DQNClipped and DQNReg) versus baselines (DQN and DDQN)on training and test environments as measured by episode return over 10 training seeds. A dashed line indi-cates that the algorithm was meta-trained on that environment while a solid line indicates a test environment.
Figure 5: Overestimated value estimates is generally problematic in value-based RL. Our method learns algo-rithms which regularize the Q-values helping with overestimation. We compare the estimated Q-values for ourlearned algorithms and baselines with the optimal ground truth Q-values across several environments duringtraining. Estimate is for taking action zero from the initial state of the environment. While DQN overestimatesthe Q-values, our learned algorithms DQNClipped and DQNReg underestimate the Q-values.
Figure 6: Our learned algorithm,DQNClipped, can be broken down intofour update rules where each rule isactive under certain conditions. Case3 corresponds to normal TD learningwhile case 2 corresponds to minimizingthe Q-values. Case 2 is more active inthe beginning when value overestima-tion is a problem and then becomes lessactive as it is no longer needed.
Figure 7: Meta-training performance for boot-strapping on 4 training environments for 10 randomseeds.
