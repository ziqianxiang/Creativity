Figure 1: Illustration of the basic concepts of the linearized computational graph and Bauer’s formula. (a) asimple Python function with intermediate variables; (b) the primal computational graph, a DAG with variablesas vertices and flow moving upwards to the output; (c) the linearized computational graph (LCG) in which theedges are labeled with the values of the local derivatives; (d) illustration of the four paths that must be evaluatedto compute the Jacobian. (Example from Paul D. Hovland.)The goals and assumptions of automatic differentiation as performed in classical and modern systemsare mismatched with those required by stochastic optimization. Traditional AD computes thederivative or Jacobian of a function accurately to numerical precision. This accuracy is required formany problems in applied mathematics which AD has served, e.g., solving systems of differentialequations. But in stochastic optimization we can make do with inaccurate gradients, as long asour estimator is unbiased and has reasonable variance. We ask the same question that motivatesmini-batch SGD: why compute an exact gradient if we can get noisy estimates cheaply? By thinkingof this question in the context of AD, we can go beyond mini-batch SGD to more general schemesfor developing cheap gradient estimators: in this paper, we focus on developing gradient estimatorswith low memory cost. Although previous research has investigated approximations in the forward orreverse pass of neural networks to reduce computational requirements, here we replace deterministicAD with randomized automatic differentiation (RAD), trading off of computation for variance insideAD routines when imprecise gradient estimates are tolerable, while retaining unbiasedness.
Figure 2: Common compu-tational graph patterns. Thegraphs may be arbitrarily deepand wide. (a) A small numberof independent paths. Path sam-pling has constant variance withdepth. (b) The number of paths in-creases exponentially with depth;path sampling gives high variance.
Figure 3: NN computation graphs.
Figure 4: Convnet activation sampling for one mini-batch element. X is the image, H is the pre-activation,and A is the activation. A is the output of a ReLU, soWe can store the Jacobian dAι∕∂Hι With 1 bit per entry.
Figure 6: Training curves for neural networks. The legend in (c) applies to all plots. For the convolutional andfully connected neural networks, the loss decreases faster using activation sampling, compared to reducing themini-batch size further to match the memory usage. For the fully connected NN on MNIST, it is important tosample different activations for each mini-batch element, since otherwise only part of the weight vector willget updated with each iteration. For the convolutional NN on CIFAR-10, this is not an issue due to weighttying. As expected, the full memory baseline converges quicker than the low memory versions. For the RNNon Sequential-MNIST, sampling different activations at each time-step matches the performance obtained byreducing the mini-batch size.
Figure 5: We visualize the gradient noisefor each stochastic gradient method by com-puting the full gradient (over all mini-batchesin the training set) and computing the meansquared error deviation for the gradient es-timated by each method for each layer inthe convolutional net. RAD has significantlyless variance vs memory than reducing mini-batch size. Furthermore, combining RADwith an increased mini-batch size achievessimilar variance to baseline 150 mini-batchelements while saving memory.
Figure 7: Reaction-diffusion PDE expt. (b)RAD saves up to 99% of memory withoutsignificant slowdown in convergence.
Figure 8: Full train/test curves and runtime per iteration. Note that training time is heavily dependenton implementation, which We did not optimize. In terms of FLOPs, “project” should be significantlyhigher than all the others and “reduced batch” should be significantly smaller. The “baseline”, “samesample”, and “different sample” should theoretically have approximately the same number of FLOPs.
Figure 9: Full train/test curves and runtime per 400 iterations. We also include results for randomprojections with shared and different random matrices for each mini-batch element.
Figure 10: Full train/test curves and runtime per iteration for various fractions for the “differentsample” experiment. Note that the reason 0.8 does not quite converge to baseline in the trainingcurve is because We sample with replacement. This is an implementation detail; our method could bemodified to sample without replacement, and at fraction 1.0 would be equivalent to baseline. Theweight decay and initial learning rate for the RAD experiments above are all the same as the onestuned for 0.1 fraction “different sample” experiment. The baseline experiments are tuned for baseline.
