Figure 1: mongoose workflow - For each layer in the NN, besides the usual setup for LSH, We add twocomponents: (i) a scheduler deciding if an LSH update should be performed in this iteration (ii) if updating, alearning (data-adaptive) mechanism will be triggered for tuning the LSH hash functions based on the currentweights, and then LSH updates will be performed.
Figure 2: Visualization of whyLSH updates are essential whendata changes. h is a hyperplaneseparating two hash buckets. Ifx1 is updated in training, itshash value may change.
Figure 3: ∆W and ∆H both start out relatively highat the beginning of training, but quickly drop off andflatten out.
Figure 4: Attention distribution examples in different layers:Y-axis is the least number of attentions summing to 90% offull attentions. The annotations represent the median. The totalnumber of attentions (or sequence length) is 512.
Figure 5: Left plot: P@1 versus training iteration on Wiki-325k. Rightplot: Loss versus training iteration on a synthetic copying task.
Figure 6: The normalized inner product computed by NNS-ds neu-rons. In both Wiki-325k (left) and enwik8 (right), mongoose hasmuch higher average inner product than baselines.
Figure 7: We show the average change of weight (left), cosine similarity (middle), weight’s hash code (right).
Figure 8: Illustration of the smart schedulerpartition boundaries, the scheduler updates the hash functions.
Figure 9: Illustration of learning hash functionsThen, the loss would update the hash functions to accommodate these new partitions.
Figure 10: The distribution of the minimal quantifies of neurons that sum up to have 0.9 softmax values inattention in each head of attention in each layer of transformer for Enwiki827Published as a conference paper at ICLR 2021D Experiments detailsD. 1 Data statisticsTable 5: Statistics for our benchmark datasetDataset	Wiki10-31k	DelicioUS-200K	Wiki-325K	Amz-670KOutput Dimension	30938	205443	325056	670091Input Dimension	101938	782585	1617899	135909Training Samples	14146	^6616	1778351	490449Testing Samples	196606	—	100095	587084	—	153025	—We present statistics on the 3 datasets we test on from the Extreme Classification Repository (Bhatiaet al., 2016). While the number of datapoints in each dataset is not large (on the order of 200Kat most), the key feature is the sheer size of the input and output dimensions. In particular, eachdataset has over 10,000 output classes, which, using a conventional nueral network, requires a matrixmultiplication involving over 10,000 neurons at the final layer.
Figure 11: Comparison of mongoose against SLIDE and FULL during the training. The two metrics (P@1in the top row and P@5 in the bottom row) are the same as in (Bhatia et al., 2016).
Figure 12: Comparison of MONGOOSE against Reformer during training on the synthetic copy task.
Figure 13: Comparison of MONGOOSE against Reformer during training on enwik8.
Figure 14: Visualization of how unionselection within the batch instead of in-dependent sets of neurons with variantlength can avoid irregular memory ac-cess.
