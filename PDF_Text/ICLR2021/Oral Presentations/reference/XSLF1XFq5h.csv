title,year,conference
 Debugging tests for model explana-tions,2020, Advances in Neural Information Processing Systems
 Estimating or propagating gradients through stochastic neurons,2013,	CoRR
 Evaluating and aggregating feature-basedmodel explanations,2020, In Christian Bessiere (ed
 Explainable machine learning indeployment,2020, In Proceedings of the 2020 Conference on Fairness
 Bayes-probe: Distribution-guided samplingfor prediction level sets,2020, arXiv preprint arXiv:2002
 Explaining imageclassifiers by counterfactual generation,2019, In International Conference on Learning Representations
 Avoiding resentment via monotonic fairness,2019, ArXiv
 Modeling Epistemic and Aleatoric Uncertainty with Bayesian Neural Networks andLatent Variables,2019, PhD thesis
 Sensitivityanalysis for predictive uncertainty,2017, In ESANN
 EXplanations based on the missing: Towards contrastive eXplanations with pertinentnegatives,2018, In Advances in Neural Information Processing Systems
 Uncertainty in Deep Learning,2016, PhD thesis
 EXplaining and harnessing adversarialeXamples,2015, In International Conference on Learning Representations
 Human perceptionsof fairness in algorithmic decision making: A case study of criminal risk prediction,2018, In Proceedingsof the 2018 World Wide Web Conference
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Metrics for eXplainable ai:Challenges and prospects,2018, arXiv preprint arXiv:1812
 Variational autoencoder with arbitrary con-ditioning,2019, In International Conference on Learning Representations
 xGEMs: Generating examplarsto explain black-box models,2018, arXiv preprint arXiv:1806
 Counterfactual fairness,2017, In I
 MNIST handwritten digit database,2010, 2010
 On the variance of the adaptive learning rate and beyond,2020, In Proceedings of the EighthInternational Conference on Learning Representations (ICLR 2020)
 A unified approach to interpreting model predic-tions,2017, In I
 A practical Bayesian framework for backpropagation networks,0899, NeuralComput
 Methods for interpreting andunderstanding deep neural networks,1051, Digital Signal Processing
 Variational continuallearning,2018, In 6th International Conference on Learning Representations
" ""why should i trust you?"": Explaining thepredictions of any classifier",2016, In Proceedings of the 22Nd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 Stop explaining black box machine learning models for high stakes decisions and useinterpretable models instead,2019, Nature Machine Intelligence
 Efficient search for diverse coherent explanations,2019, In Proceedings of the Conferenceon Fairness
 Transparency: Motivations and challenges,2019, In Explainable AI: Interpreting
 LSAC national longitudinalbar passage study,1998, LSAC research report series
 AuxiliaryVAEs and VAEACs used for tabular data use fully connected encoders and decoders with residualconnections and batch normalization at every layer,2016, For MNIST
 A negative CLUE attribution means that the the absence of that featurewill make the model more certain,2021, A negative feature importance attribution means the absence ofthat feature would serve as evidence for a particular prediction
