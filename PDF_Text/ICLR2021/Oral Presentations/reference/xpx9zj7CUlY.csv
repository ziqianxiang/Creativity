title,year,conference
 Tensorflow: A system for large-scalemachine learning,2016, In 12th USENIX Symposium on Operating Systems Design and Implementation(OSD116)
 Faster neural network training with approximate tensoroperations,2018, arXiv preprint arXiv:1805
 Efficient optimization of loops and limits with randomizedtelescoping sums,2019, In International Conference on Machine Learning
 Theano: a CPU and GPUmath expression compiler,2010, In Proceedings of the Python for Scientific Computing Conference(SciPy)
 ADIFOR-generating derivative codes from Fortran programs,1992, Scientific Programming
 Neural ordinarydifferential equations,2018, Advances in Neural Information Processing Systems
 Training deep nets with sublinearmemory cost,2016, arXiv preprint arXiv:1604
 The reversible residual network:Backpropagation without storing activations,2017, In Advances in Neural Information ProcessingSystems
 Algorithm 799: revolve: an implementation of check-pointing for the reverse or adjoint mode of computational differentiation,2000, ACM Transactions onMathematical Software (TOMS)
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Training products of experts by minimizing contrastive divergence,2002, Neuralcomputation
 Flat minima,1997, Neural Computation
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2017, InInternational Conference on Learning Representations
 Adam: A method for stochastic oPtimization,2015, In InternationalConference on Learning Representations
 Auto-encoding variational Bayes,2013, arXiv preprintarXiv:1312
 A simple way to initialize recurrent networks ofrectified linear units,2015, arXiv preprint arXiv:1504
 Gradient-based hyperparameter optimizationthrough reversible learning,2015, In International Conference on Machine Learning
 Computational Nuclear Engineering and Radiological Science Using Python:Chapter 18 - One-Group Diffusion Equation,2018, Academic Press
 Optimal accumulation of Jacobian matrices by elimination methods on the dualcomputational graph,2004, Mathematical Programming
 Optimal Jacobian accumulation is NP-complete,2008, Mathematical Programming
 A Bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 meprop: Sparsified back propagationfor accelerated deep learning with reduced overfitting,2017, In Proceedings of the 34th InternationalConference on Machine Learning
 Unbiasing truncated backpropagation through time,2017, arXiv preprintarXiv:1705
 Getting started with ADOL-C,2009, Combinatorial ScientificComputing
 Minimal effort back propagation forconvolutional neural networks,2017, arXiv preprint arXiv:1709
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
