title,year,conference
 Backward feature correction: How deep learning performs deeplearning,2020, arXiv preprint arXiv:2001
 On the convergence rate of training recurrent neuralnetworks,2019, In Advances in neural information processing systems
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In Proceedingsof International Conference on Machine Learning 36
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Spherical harmonics and approximations on the unit sphere: anintroduction,2012, Springer
 Beyond linearization: On quadratic and higher-order approximation of wideneural networks,2019, In International Conference on Learning Representations
 On the inductive bias of neural tangent kernels,2019, In Advances inNeural Information Processing Systems
 Optimal rates for regularization of statistical inverse learningproblems,2018, Foundations of Computational Mathematics
 A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, arXiv preprint arXiv:1905
 Towards understanding thespectral bias of deep learning,2019, arXiv preprint arXiv:1912
 Learning with sgd and random features,2018, InAdvances in Neural Information Processing Systems 31
 On the generalization ability of on-linelearning algorithms,2004, IEEE Transactions on Information Theory
 On the mathematical foundations of learning,2002, Bulletin of theAmerican mathematical society
 Gradient descent finds globalminima of deep neural networks,2019, In Proceedings of International Conference on Machine Learning36
 Gradient descent provably optimizesover-parameterized neural networks,2019, International Conference on Learning Representations 7
 Limitations of lazytraining of two-layers neural network,2019, In Advances in Neural Information Processing Systems 32
 Polylogarithmic width suffices for gradient descent to achievearbitrarily small test error with shallow relu networks,2019, arXiv preprint arXiv:1909
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 Generalized leverage score sampling forneural networks,2020, In Advances in Neural Information Processing Systems
 Learning over-parametrized two-layer neuralnetworks beyond ntk,2020, In Proceedings of Conference on Learning Theory 33
 Optimal rates for spectralalgorithms with least-squares regression over hilbert spaces,2020, Applied and Computational HarmonicAnalysis
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings ofthe National Academy of Sciences
 Beating sgd saturation with tail-averaging andminibatching,2019, In Advances in Neural Information Processing Systems
 Stochastic particle gradient descent for infinite ensembles,2017, arXivpreprint arXiv:1712
 Stochastic gradient descent with exponential convergence rates ofexpected classification errors,2019, In Proceedings of International Conference on Artificial Intelligenceand Statistics 22
 Gradient descent can learn lessover-parameterized two-layer neural networks on classification problems,2019, arXiv preprintarXiv:1905
 Exponential convergence of testing errorfor stochastic gradient methods,2018, In Proceedings of Conference on Learning Theory 31
 Statistical optimality of stochasticgradient descent on hard learning problems through multiple passes,2018, In Advances in NeuralInformation Processing Systems
 On the spectral bias of neural networks,2019, In Proceedings ofInternational Conference on Machine Learning 36
 Random features for large-scale kernel machines,2007, In Advances inNeural Information Processing Systems 20
 Making gradient descent optimal forstrongly convex stochastic optimization,2012, In Proceedings of International Conference on MachineLearning 29
 Searching for activation functions,2017, arXiv preprintarXiv:1710
 The convergence rate of neuralnetworks for learned functions of different frequencies,2019, In Advances in Neural InformationProcessing Systems
 Online learning algorithms,2006, Foundations of computational mathematics
 On learning over-parameterized neural networks: A functional approx-imation perspective,2019, In Advances in Neural Information Processing Systems
 Generalization bound of globally optimal non-convex neural network training: Trans-portation map estimation by infinite dimensional langevin dynamics,2020, In Advances in NeuralInformation Processing Systems
 Global convergence of adaptive gradient methods for anover-parameterized neural network,2019, arXiv preprint arXiv:1902
 Online regularized classification algorithms,2006, IEEE Transactions onInformation Theory
 Fast convergence of natural gradient descentfor over-parameterized neural networks,2019, In Advances in Neural Information Processing Systems
 Statistical behavior and consistency of classification methods based on convex risminimization,2004, The Annals of Statistics
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In Advances in Neural Information Processing Systems
