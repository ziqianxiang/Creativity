title,year,conference
 Curriculum learning,2009, InProceedings of International Conference on Machine Learning
 Food-101-mining discriminative compo-nents with random forests,2014, In European conference on computer vision
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Learning and memorization,2018, In International Conference on Machine Learning
 Visualizing and understanding curriculumlearning for long short-term memory networks,2016, arXiv preprint arXiv:1611
 Learning with average top-k loss,2017, InAdvances in neural information processing systems
 Catastrophic forgetting in connectionist networks,1999, Trends in cognitive sciences
 Mildly overparametrized neural nets can memorizetraining data efficiently,2019, arXiv preprint arXiv:1909
 An empiricalinvestigation of catastrophic forgeting in gradientbased neural networks,2014, In In Proceedings ofInternational Conference on Learning Representations (ICLR
 Covariate shift by kernel mean matching,2009, Dataset shift in machine learning
 Neural network memorization dissection,2019, arXiv preprintarXiv:1911
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Acceleratingdeep learning by focusing on the biggest losers,2019, arXiv preprint arXiv:1910
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Beyond synthetic noise: Deep learning oncontrolled noisy labels,2020, In ICML
 Exploring the memorization-generalization continuum in deep learning,2020, arXiv preprint arXiv:2002
 How do humans teach: On curriculum learning andteaching dimension,2011, In Advances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Curriculum learning and minibatch bucketing in neural machinetranslation,2017, In Proceedings of the International Conference Recent Advances in Natural LanguageProcessing
 Learning multiple layers of features from tiny images,2009, 2009
 Self-paced learning for latent variablemodels,2010, In Advances in neural information processing systems
 Cleannet: Transfer learning forscalable image classifier training with label noise,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 Curriculum loss: Robust learning and generalization against labelcorruption,2020, In International Conference on Learning Representations
 Teacher-student curriculum learn-ing,2019, IEEE transactions on neural networks and learning systems
 Learning withnoisy labels,2013, In Advances in neural information processing systems
 Toward understanding catastrophic forgetting in continual learning,2019, arXiv preprintarXiv:1908
 Towards robustlearning with different label noise distributions,2019, arXiv preprint arXiv:1912
 Curriculum learning of multi-ple tasks,2015, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Anatomy of catastrophic forgetting: Hiddenrepresentations and task semantics,2020, arXiv preprint arXiv:2007
 Neural network learning control of robot manipulators using gradually increasingtask difficulty,1994, IEEE transactions on Robotics and Automation
 Data parameters: A new family of parametersfor learning a differentiable curriculum,2019, In Advances in Neural Information Processing Systems
 Learning with bad training data via iterative trimmed loss mini-mization,2019, In International Conference on Machine Learning
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Learning from noisy labels with deep neural networks,2014, arXivpreprint arXiv:1406
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 An empirical study of example forgetting during deep neural networklearning,2019, In ICLR
 Potential downside of high initial visual acuity,2018, Proceedings of theNational Academy of Sciences
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference (BMVC)
 Learning to execute,2014, arXiv preprint arXiv:1410
 Understandingdeep learning requires rethinking generalization,2017, 2017
 Autoassist: A framework to accelerate trainingof deep neural networks,2019, In Advances in Neural Information Processing Systems
 An empirical exploration of curriculumlearning for neural machine translation,2018, arXiv preprint arXiv:1811
