Published as a conference paper at ICLR 2021
Why Are Convolutional Nets More Sample-
Efficient than Fully-Connected Nets ?
Zhiyuan Li, Yi Zhang
Princeton University
zhiyuanli,y.zhang@cs.princeton.edu
Sanjeev Arora
Princeton University & IAS
arora@cs.princeton.edu
Ab stract
Convolutional neural networks often dominate fully-connected counterparts in
generalization performance, especially on image classification tasks. This is often
explained in terms of “better inductive bias.” However, this has not been made
mathematically rigorous, and the hurdle is that the sufficiently wide fully-connected
net can always simulate the convolutional net. Thus the training algorithm plays
a role. The current work describes a natural task on which a provable sample
complexity gap can be shown, for standard training algorithms. We construct
a single natural distribution on Rd × {±1} on which any orthogonal-invariant
algorithm (i.e. fully-connected networks trained with most gradient-based methods
from gaussian initialization) requires Ω(d2) samples to generalize while O(1)
samples suffice for convolutional architectures. Furthermore, we demonstrate
a single target function, learning which on all possible distributions leads to an
O⑴ Vs Ω(d2∕ε) gap. The proof relies on the fact that SGD on fully-connected
network is orthogonal equivariant. Similar results are achieved for `2 regression and
adaptiVe training algorithms, e.g. Adam and AdaGrad, which are only permutation
equiVariant.
1	Introduction
Deep conVolutional nets (“ConVNets”) are at the center of the deep learning reVolution (KrizheVsky
et al., 2012; He et al., 2016; Huang et al., 2017). For many tasks, especially in Vision, conVolutional
architectures perform significantly better their fully-connected (“FC”) counterparts, at least giVen
the same amount of training data. Practitioners explain this phenomenon at an intuitiVe leVel by
pointing out that conVolutional architectures haVe better “inductiVe bias”, which intuitiVely means
the following: (i) ConVNet is a better match to the underlying structure of image data, and thus are
able to achieVe low training loss with far fewer parameters (ii) models with fewer total number of
parameters generalize better.
Surprisingly, the aboVe intuition about the better inductiVe bias of ConVNets oVer FC nets has
neVer been made mathematically rigorous. The natural way to make it rigorous would be to show
explicit learning tasks that require far more training samples on FC nets than for ConVNets. (Here
“task”means, as usual in learning theory, a distribution on data points, and binary labels for them
generated giVen using a fixed labeling function.) Surprisingly, the standard repertoire of lower bound
techniques in ML theory does not seem capable of demonstrating such a separation. The reason is
that any ConVNet can be simulated by an FC net of sufficient width, since a training algorithm can
just zero out unneeded connections and do weight sharing as needed. Thus the key issue is not an
expressiVeness per se, but the combination of architecture plus the training algorithm. But if the
training algorithm must be accounted for, the usual hurdle arises that we lack good mathematical
understanding of the dynamics of deep net training (whether FC or ConVNet). How then can one
establish the limitations of “FC nets + current training algorithms”? (Indeed, many lower bound
techniques in PAC learning theory are information theoretic and ignore the training algorithm.)
The current paper makes significant progress on the aboVe problem by exhibiting simple tasks that
require Ω(d2) factor more training samples for FC nets than for ConvNets, where d is the data
dimension. (In fact this is shown eVen for 1-dimensional ConVNets; the lowerbound easily extends
to 2-D ConvNets.) The lower bound holds for FC nets trained with any of the popular algorithms
1
Published as a conference paper at ICLR 2021
0.8
0.5 N
102
0.6
0.7
0.9
1.0
1O3
104
# training data
Gauss
105
106
0.5
102
0.6
0.9
0.7
0.8
1.0
103	104
# training data
cifar-10
2-layer cnn w/ quadratic
3-layer cnn w/ relu
resπet14 cππ
hybrid w/quadratic
hybrid w/ relu
2-layer fc w/quadratic
3-layer fc w/quadratic
3-layer fc w/ relu
3-layer fc w/ relu + bn
105
Figure 1: Comparison of generalization performance of convolutional versus fully-connected models trained by
SGD. The grey dotted lines indicate separation, and we can see convolutional networks consistently outperform
fully-connected networks. Here the input data are 3 × 32 × 32 RGB images and the binary label indicates for
each image whether the first channel has larger `2 norm than the second one. The input images are drawn from
entry-wise independent Gaussian (left) and CIFAR-10 (right). In both cases, the 3-layer convolutional networks
consist of two 3 × 3 convolutions with 10 hidden channels, and a 3 × 3 convolution with a single output channel
followed by global average pooling. The 3-layer fully-connected networks consist of two fully-connected layers
with 10000 hidden channels and another fully-connected layer with a single output. The 2-layer versions have
one less intermediate layer and have only 3072 hidden channels for each layer. The hybrid networks consist of a
single fully-connected layer with 3072 channels followed by two convolutional layers with 10 channels each.
bn stands for batch-normalization Ioffe & Szegedy (2015).
listed in Table 1. (The reader can concretely think of vanilla SGD with Gaussian initialization of
network weights, though the proof allows use of momentum, `2 regularization, and various learning
rate schedules.) Our proof relies on the fact that these popular algorithms lead to an orthogonal-
equivariance property on the trained FC nets, which says that at the end of training the FC net
—no matter how deep or how wide — will make the same predictions even if we apply orthogonal
transformation on all datapoints (i.e., both training and test). This notion is inspired by Ng (2004)
(where it is named “orthogonal invariant”), which showed the power of logistic regression with `1
regularization versus other learners. For a variety of learners (including kernels and FC nets) that
paper described explicit tasks where the learner has Ω(d) higher sample complexity than logistic
regression with `1 regularization. The lower bound example and technique can also be extended to
show a (weak) separation between FC nets and ConvNets. (See Section 4.2)
Our separation is quantitatively stronger than the result one gets using Ng (2004) because the sample
complexity gap is Ω(d2) vs O(1), and not Ω(d) vs O(1). But in a more subtle way our result is
conceptually far stronger: the technique of Ng (2004) seems incapable of exhibiting a sample gap of
more than O(1) between Convnets and FC nets in our framework. The reason is that the technique of
Ng (2004) can exhibit a hard task for FC nets only after fixing the training algorithm. But there are
infinitely many training algorithms once we account for hyperparameters associated in various epochs
with LR schedules, `2 regularizer and momentum. Thus Ng (2004)’s technique cannot exclude the
possibility that the hard task for “FC net + Algorithm 1” is easy for “FC net + Algorithm 2”. Note
that we do not claim any issues with the results claimed in Ng (2004); merely that the technique
cannot lead to a proper separation between ConvNets and FC nets, when the FC nets are allowed to
be trained with any of the infinitely many training algorithms. (Section 4.2 spells out in more detail
the technical difference between our technique and Ng’s idea.)
The reader may now be wondering what is the single task that is easy for ConvNets but hard for FC
nets trained with any standard algorithm? A simple example is the following: data distribution in Rd
is standard Gaussian, and target labeling function is the sign of Pid=/21 xi2 - Pid=d/2+1 xi2 . Figure 1
shows that this task is indeed much more difficult for FC nets. Furthermore, the task is also hard in
practice for data distributions other than Gaussian; the figure shows that a sizeable performance gap
exists even on CIFAR images with such a target label.
Extension to broader class of algorithms. The orthogonal-equivariance property holds for many
types of practical training algorithms, but not all. Notable exceptions are adaptive gradient methods
(e.g. Adam and AdaGrad), `1 regularizer, and initialization methods that are not spherically symmetric.
To prove a lower bound against FC nets with these algorithms, we identify a property, permutation-
invariance, which is satisfied by nets trained using such algorithms. We then demonstrate a single
2
Published as a conference paper at ICLR 2021
and natural task on Rd × {±1} that resembles real-life image texture classification, on which we
prove any permutation-invariant learning algorithm requires Ω(d) training examples to generalize,
while Empirical Risk Minimization with O(1) examples can learn a convolutional net.
Paper structure. In Section 2 we discuss about related works. In section 3, we define the notation
and terminologies. In Section 4, we give two warmup examples and an overview for the proof
technique for the main theorem. In Section 5, we present our main results on the lower bound of
orthogonal and permutation equivariant algorithms.
2	Related Works
Du et al. (2018) attempted to investigate the reason why convolutional nets are more sample efficient.
Specifically they prove O⑴ samples suffice for learning a convolutional filter and also proved a Ω(d)
min-max lower bound for learning the class of linear classifiers. Their lower bound is against learning
a class of distributions, and their work fails to serve as a sample complexity separation, because their
upper and lower bounds are proved on different classes of tasks.
Arjevani & Shamir (2016) also considered the notion of distribution-specific hardness of learning
neural nets. They focused on proving running time complexity lower bounds against so-called
"orthogonally invariant" and "linearly invariant" algorithms. However, here we focus on sample
complexity.
Recently, there has been progress in showing lower bounds against learning with kernels. Wei et al.
(2019) constructed a single task on which they proved a sample complexity separation between
learning with neural networks vs. with neural tangent kernels. Notably the lower bound is specific
to neural tangent kernels (Jacot et al., 2018). Relatedly, Allen-Zhu & Li (2019) showed a sample
complexity lower bound against all kernels for a family of tasks, i.e., learning k-XOR on the
hypercube.
3	Notation and Preliminaries
We will use X = Rd, Y = {-1, 1} to denote the domain of the data and label and H = {h |
h : X → Y} to denote the hypothesis class. Formally, given a joint distribution P, the error of a
hypothesis h ∈ H is defined as errp(h) := Pχ,y〜P [h(x) = y]. If h is a random hypothesis, We
define errp (h) := Pχ,y〜p,h [h(x) = y] for convenience. A class of joint distributions supported on
X × Y is referred as a problem, P .
We use ∣∣∙k2 to denote the spectrum norm and ∣∣∙kf to denote the Frobenius norm of a matrix. We
use A ≤ B to denote that B - A is a semi-definite positive matrix. We also use O(d) and GL(d)
to denote the d-dimensional orthogonal group and general linear group respectively. We use Bpd2 to
denote the unit Schatten-p norm ball in Rd×d .
We use N(μ, Σ) to denote Gaussian distribution with mean μ and covariance Σ. For random variables
X and Y , We denote X is equal to Y in distribution by X =d Y . In this Work, We also alWays use
PX to denote the distributions on X and P to denote the distributions supported jointly on X × Y .
Given an input distribution PX and a hypothesis h, we define PX h as the joint distribution on
X × Y, such that (PX h)(S) = P ({x|(x, h(x)) ∈ S}), ∀S ⊂ X × Y. In other words, to sample
(X,Y) 〜PX ◊ h means to first sample X 〜PX, and then set Y = h(X). For a family of input
distributions PX and a hypothesis class H, we define PX H = {PX h | PX ∈ PX , h ∈ H}. In
this work all joint distribution P can be written as PX ◊ h for some h, i.e. PY|X is deterministic.
For set S ⊂ X and 1-1 map g : X → X, we define g(S) = {g(x)|x ∈ S}. We use ◦ to
denote function composition. (f ◦ g)(x) is defined as f(g(x)), and for function classes F, G,
F ◦ G = {f ◦ g | f ∈ F, g ∈ G}. For any distribution PX supported on X , we define PX ◦ g as the
distribution such that (PX ◦ g)(S) = PX(g(S)), In other words, if X 〜 PX Q⇒ g-1(X) 〜PX ◦ g,
because
VS ⊆X,	P	[g-1(X) ∈	S]	= P	[X	∈	g(S)]	=	[Px	◦ g](S).
X〜PX	X〜PX
3
Published as a conference paper at ICLR 2021
Algorithm 1 Iterative algorithm A
Require: Initial parameter distribution Pinit supported in W = Rm, total iterations T , training
dataset {xi, yi}in=1, parametric model M : W → H, iterative update rule F(W, M, {xi, yi}in=1)
Ensure: Hypothesis h : X → Y .
Sample W⑼〜Pinit.
for t = 0 to T - 1 do
W(t+1) = F (W(t), M, {xi, yi}in=1).
return h = sign M[W(T)].
For any joint distribution P of form P = PX h, we define P ◦ g = (PX ◦ g) (h ◦ g). In other
words, (X, Y)〜 P ^⇒ (g-1 (X), Y)〜 P ◦ g. For any distribution class P and group G acting on
X, We define PoG as {P ◦ g | P ∈ P,g ∈ G}.
Definition 3.1. A deterministic supervised Learning Algorithm A is a mapping from a sequence
of training data, {(xi, yi)}in=1 ∈ (X × Y)n, to a hypothesis A({(xi, yi)}in=1) ∈ H ⊆ YX . The
algorithm A could also be randomized, in which case the output A({(xi, yi)}in=1) is a distribution on
hypotheses. Two randomized algorithms A and A0 are the same if for any input, their outputs have
the same distribution in function space, which is denoted by A({xi, yi}in=1) =d A0({xi, yi}in=1).
Definition 3.2 (Equivariant Algorithms). A learning algorithm is equivariant under group GX (or
GX -equivariant) if and only if for any dataset {xi, yi}in=1 ∈ (X × Y)n and ∀g ∈ GX , x ∈ X,
A({g(xi), yi}in=1) og = A({xi,yi}in=1), or A({g(xi), yi}in=1)(g(x)) = [A({xi,yi}in=1)](x). 1
Definition 3.3 (Sample Complexity). Given a problem P and a randomized learning algorithm A,
δ, ε ∈ [0, 1], we define the (ε, δ)-sample complexity, denoted N(A, P, ε, δ), as the smallest number
n ∈ N such that ∀P ∈ P, w.p. 1 - δ over the randomness of {xi, yi}in=1, errP (A({xi, yi}in=1)) ≤ ε.
We also define the ε-expected sample complexity for a problem P, denoted N*(A, P,ε), as the
smallest number n ∈ N such that ∀P ∈ P, E [errP(A({xi, yi}in=1))] ≤ ε. By definition, we
(Xi,yi gp
have N * (A, P ,ε + δ) &N (A,P,ε,δ) ≤N *(A,P,εδ), ∀ε,δ ∈ [0,1].
3.1	Parametric Models and Iterative Algorithms
A parametric model M : W → H is a functional mapping from weight W to a hypothesis
M(∙) : X → Y. Given a specific parametric model M, a general iterative algorithm is defined as
Algorithm 1. In this work, we will only use the two parametric models below, FC-NN and CNN.
FC Nets: A L-layer Fully-connected Neural Network parameterized by its weights W =
(Wι, W2,..., Wl) is a function FC-NNH : Rd → R, where Wi ∈ Rdi-1×di, d° = d, and dL = 1:
FC-NN[W](x) = Wl0(Wl-i ∙∙∙σ(W2σ(W1x))).
Here, σ : R → R can be any function, and we abuse the notation such that σ is also defined for vector
inputs, in the sense that [σ(x)]i = σ(xi).
ConvNets (CNN): In this paper we will only use two layer Convolutional Neural Networks with
one channel. Suppose d = d0r for some integer d0 , r, a 2-layer CNN parameterized by its weights
W = (w, a,b) ∈ Rk X Rr X R is a function CNN[∙] : Rd → R:
r
CNN[W](x) = X arσ([w * x]d，(i-i)+i：d，i) + b,
i=1
where * : Rk XRd → Rd is the convolution operator, defined as [w*x]i = Pjk=1 wjx[i-j-1 modd]+1,
and σ : Rd0 → R is the composition of pooling and element-wise non-linearity.
3.2	Equivariance and training algorithms
This section gives an informal sketch of why FC nets trained with standard algorithms have certain
equivariance properties. The high level idea here is if update rule of the network, or more generally,
1For randomized algorithms, the condition becomes A({g(xi), yi}in=1) ◦ g =d A({xi, yi}in=1), which is
stronger than A({g(xi), yi}in=1)(g(x)) =d [A({xi,yi}in=1)](x), ∀x ∈ X.
4
Published as a conference paper at ICLR 2021
Symmetry	Sign FliP	Permutation	Orthogonal	Linear
Matrix Group	Diagonal, | Mii| = 1 一	Permutation	Orthogonal	Invertible
Algorithms	AdaGrad, Adam	AdaGrad, Adam	SGD Momentum	Newton,s method
Initialization	Symmetric distribution	i.i.d.	i.i.d. Gaussian	All zero
Regularization	'p norm	'p norm	'1 norm	None
Table 1: Examples of gradient-based equivariant training algorithms for FC networks. The initializa-
tion requirement is only for the first layer of the network.
the parametrized model, exhibits certain symmetry per step, i.e., property 2 in Theorem C.1, then by
induction it will hold till the last iteration.
Taking linear regression as an example, let xi ∈ Rd, i ∈ [n] be the data and y ∈ Rn be the
labels, the GD update for L(W) = 11 Pn=ι(x>w - yi)2 = 1 ∣∣Xτw - y∣∣2 would be wt+ι =
F(wt, X, y) := wt - ηX(X>wt - y). Now suppose there’s another person trying to solve the same
problem using GD with the same initial linear function, but he observes everything in a different
basis, i.e., X0 = UX and w00 = Uw0, for some orthogonal matrix U. Not surprisingly, he would get
the same solution for GD, just in a different basis. Mathematically, this is because w0t = Uwt =⇒
w0t+1 = F (wt0 , UX, y) = UF (wt, X, y) = Uwt+1. In other words, he would make the same
prediction for unseen data. Thus if the initial distribution of w0 is the same under all basis (i.e., under
rotations), e.g., gaussian N(0, Id), then w0 =d Uw0 =⇒ Ft(w0, UX, y) = UFt(w0, X, y), for
any iteration t, which means GD for linear regression is orthogonal invariant.
To show orthogonal equivariance for gradient descent on general deep FC nets, it suffices to apply
the above argument on each neuron in the first layer of the FC nets. Equivariance for other training
algorithms (see Table 1) can be derived in the exact same method. The rigorous statement and the
proofs are deferred into Appendix C.
4	Warm-up examples and Proof Overview
4.1	Example 1: Ω(d) lower bound against orthogonal equivariant methods
We start with a simple but insightful example to how equivariance alone could suffice for some
non-trivial lower bounds.
We consider a task on Rd × {±1} which is a uniform distribution on the set {(eiy, y)|i ∈
{1, 2, . . . , d}, y = ±1}, denoted by P . Each sample from P is a one-hot vector in Rd and the
sign of the non-zero coordinate determines its label. Now imagine our goal is to learn this task using
an algorithm A. After observing a training set of n labeled points S := {(xi, yi)}in=1, the algorithm
is asked to make a prediction on an unseen test data x, i.e., A(S)(x). Here we are concerned with
orthogonal equivariant algorithms ——the prediction of the algorithm on the test point remains the
same even if we rotate every xi and the test point x by any orthogonal matrix R, i.e.,
A({(Rxi,yi)}in=1)(Rx)=d A({(xi,yi)}in=1)(x)
Now we show this algorithm fails to generalize on task P, if it observes only d/2 training examples.
The main idea here is that, for a fixed training set S, the prediction A({(xi, yi)}in=1)(x) is determined
solely by the inner products between x and xi ’s due to orthogonal equivariance, i.e., there exists a
random function f (which may depend on S) such that2
A({(xi,yi)}in=1)(x) =d f(xτx1, . . . , xτxn)
But the input distribution for this task is supported on 1-hot vectors. suppose n < d/2. Then at test
time the probability is at least 1/2 that the new data point (x, y)〜P, is such that X has zero inner
product with all n points seen in the training set S. This fact alone fixes the prediction of A to the
value f(0, . . . , 0) whereas y is independently and randomly chosen to be ±1. We conclude that A
outputs the wrong answer with probability at least 1/4.
2this can be made formal using the fact that Gram matrix determine a set of vectors up to an orthogonal
transformation.
5
Published as a conference paper at ICLR 2021
4.2	Example 2: Ω(d2) lower bound in the weak sense
The warm up example illustrates the main insight of (Ng, 2004), namely, that when an orthogonal
equivariant algorithm is used to do learning on a certain task, it is actually being forced to simulta-
neously learn all orthogonal transformations of this task. Intuitively, this should make the learning
much more sample-hungry compared to even simple sGD on ConvNets, which is not orthogonal
equivariant. Now we sketch why the obvious way to make this intuition precise using VC dimension
(Theorem B.1) does not give a proper separation between ConvNets and FC nets, as mentioned in the
Introduction.
We first fix the ground truth labeling function h = sign [pd=ι X 一 Pi2=dd+1 xi2 . Algorithm
A is orthogonal equivariant (Definition 3.2) means that for any task P = PX ◊ h*, where PX is
the input distribution and h is the labeling function, A must have the same performance on P
and its rotated version P ◦ U = (PX ◦ U) ◊ (h* ◦ U), where U can be any orthogonal matrix.
Therefore if there,s an orthogonal equivariant learning algorithm A that learns h on all distributions,
then A will also learn every the rotated copy of h*, h ◦ U, on every distribution PX, simply
because A learns h on distribution PX ◦ U-1. Thus A learns the class of labeling functions
h ◦ O(d) := {h(x) = h*(U(x)) | U ∈ O(d)} on all distributions. (See formal statement in
Theorem 5.1) By the standard lower bounds with VC dimension (see Theorem B.1), it takes at
least Ω(VCdim(H°°(dd)) samples for A to guarantee 1 一 ε accuracy. Thus it suffices to show the
VC dimension VCdim(H ◦ O(d)) = Ω(d2), towards a Ω(d2) sample complexity lower bound. (Ng
(2004) picks a linear thresholding function as h*, and thus VCdim(h* ◦ O(d)) is only O(d).)
Formally, we have the following theorem, whose proof is deferred into Appendix D.2:
Theorem 4.1 (All distributions, single hypothesis). Let P = {all distributions} ◊ {h*}. For any
orthogonal equivariant algorithms A, N(A, P, ε, δ) = Ω((d2 + ln 1 )∕ε), while there,s a 2-layer
ConvNet architecture, such thatN(ERMCNn, P, ε,δ) = O (ɪ (log ɪ + log ɪ)).
As noted in the introduction, this doesn,t imply there is some task hard for every training algorithm
for the FC net. The VC dimension based lower bound implies for each algorithm A the existence of a
fixed distribution PX ∈ P and some orthogonal matrix UA such that the task (PX ◦ U-1) ◊ h is
hard for it. However, this does not preclude (PX ◦ U-1) ◊ h being easy for some other algorithm A0.
4.3	Proof overview for fixed distribution lower bounds
At first sight, the issue highlighted above (and in the Introduction) seems difficult to get around. one
possible avenue is if the hard input distribution PX in the task were invariant under all orthogonal
transformations, i.e., PX = PX ◦ U for all orthogonal matrices U. Unfortunately, the distribution
constructed in the proof of lower bound with VC dimension is inherently discrete and cannot be made
invariant to orthogonal transformations.
our proof uses a fixed PX, the standard Gaussian distribution, which is indeed invariant under
orthogonal transformations. The proof also uses the Benedek-Itai,s lower bound, Theorem 4.2, and
the main technical part of our proof is the lower bound for the the packing number D(H, ρ, ε) defined
below (also see Equation (2)).
For function class H, we use ΠH(n) to denote the growth function of H, i.e. ΠH(n) :=
sup	|{(h(x1), h(x2), . . . , h(xn)) | h ∈ H}| . Denote the VC-Dimension of H by VCdim(H),
x1 ,...,xn ∈X
VCdim(H)
by Sauer-Shelah Lemma, We know ∏H(n) ≤ (VCden(H))	for n ≥ VCdim(H).
Let ρ be a metric on H, We define N(H, ρ, ε) as the ε-covering number of H w.r.t. ρ, and
D(H, ρ, ε) as the ε-packing number of H w.r.t. ρ. For distribution PX, we use ρX (h, h0) :=
Px~Px [h(X) = h0(X)] to denote the discrepancy between hypothesis h and h0 w.r.t. PX.
Theorem 4.2. [Benedek-Itai,s lower bound] For any algorithm A that (ε, δ)-learns H with n i.i.d.
samples from a fixed distribution PX, it must hold for every
∏H(n) ≥ (1 — δ)D(H,PX, 2ε)
(1)
Since ΠH(n) ≤ 2n, we have N(A, PX ◊ H, ε, δ) ≥ log2 D(H, ρX, 2ε) + log2(1 一 δ), which is the
original bound from Benedek & Itai (1991). Later Long (1995) improved this bound for the regime
6
Published as a conference paper at ICLR 2021
n ≥ VCdim(H) using Sauer-Shelah lemma, i.e.,
N(APX,ε,δ) ≥ VCdim(H) ((1 - δ)D(H,ρχ, 2ε)) vcdi⅛H)
e
(2)
Intuition behind Benedek-Itai’s lower bound. We first fix the data distribution as PX . Suppose
the 2ε-packing is labeled as {h1, . . . , hD(H,ρX,2ε)} and ground truth is chosen from this 2ε-packing,
(ε, δ)-learns the hypothesis H means the algorithm is able to recover the index of the ground truth w.p.
1 - δ. Thus one can think this learning process as a noisy channel which delivers log2 D(H, ρX , 2ε)
bits of information. Since the data distribution is fixed, unlabeled data is independent of the ground
truth, and the only information source is the labels. With some information-theoretic inequalities,
we can show the number of labels, or samples (i.e., bits of information) N (A, PX H, ε, δ) ≥
log2 D(H, ρX, 2ε)+log2(1-δ). A more closer look yields Equation (2), because when VCdim(H) <
∞, then only log2 ΠH (n) instead of n bits information can be delivered.
5 Lower Bounds
Below we first present a reduction from a special subclass of PAC learning to equivariant learning
(Theorem 5.1), based on which we prove our main separation results, Theorem 4.1, 5.2, 5.3 and 5.4.
Theorem 5.1. IfPX is a set of data distributions that is invariant under group GX, i.e., PX ◦GX = PX,
then the following inequality holds. (Furthermore it becomes an equality when GX is a compact
group.)
inf N*(A,Px oH,ε) ≥ inf N*(A,Pχ O(HoGX),ε)	(3)
A∈AGX	A∈A
Remark 5.1. The sample complexity in standard PAC learning is usually defined again hypothesis
class H only, i.e., PX is the set of all the possible input distributions. In that case, PX is always
invariant under group GX, and thus Theorem 5.1 says that GX -equivariant learning against hypothesis
class H is as hard as learning against hypothesis H o GX without equivariance constraint.
5.1	Ω(d2) LOWER BOUND FOR ORTHOGONAL EQUIVARIANCE WITH A FIXED DISTRIBUTION
In this subsection We show Ω(d2) Vs O(I) separation on a single task in our main theorem (Theo-
rem 5.2). With the same proof technique, we further show we can get correct dependency on ε for
the lower bound, i.e., Ω(d), by considering a slightly larger function class, which can be learnt by
ConvNets with O(d) samples. We also generalize this Ω(d2) vs O(d) separation to the case of '2
regression with a different proof technique.
Theorem 5.2.	There,s a single task, Pχ ◊ h*, where h = sign [pd=ι x2 一 P2=d+ι x，] and
PX = N(0, I2d) and a constant ε0 > 0, independent of d, such that for any orthogonal equivariant
algorithm A, we have
N*(A,Pχ ◊ h*,ε0) = Ω(d2),	(4)
while there,s a 2-layer ConvNet, such that N(ERMCNn,PX ◊ h*,ε, δ) = O (ε (log ε + log 1)).
Moreover, ERMCNN could be realized by gradient descent (on the second layer only).
Proof of Theorem 5.2. Upper bound: implied by upper bound in Theorem 4.1. Lower bound:
Note that the PX = N(0, I2d) is invariant under O(2d), by Theorem 5.1, it suffices to show that
there,s a constant ε0 > 0 (independent of d), for any algorithm A, it takes Ω(d2) samples to learn the
augmented function class h ◦ O(2d) w.r.t. Pχ = N(0, I2d). Define hu = sign [x>dU xd+i：2d],
∀U ∈ Rd×d, and by Lemma D.2, we have H = {hu | U ∈ O(d)} ⊆ h ◦ O (2d). Thus it suffices to
a Ω(d2) sample complexity lower bound for the sub function class H, i.e.,
N * (A, N(0, I2d) ◊ {sign [x>dU xd+12d]},ε0) = Ω(d2).	(5)
By Benedek&Itai,s lower bound, (Benedek & Itai, 1991) (Equation (1)), we know
N(A, P, ε0, δ) ≥ log2 ((1 - δ)D(H, ρX, 2ε0)) .	(6)
d(d — 1)
By Lemma D.4, there,s some constant C, such that D(H,PX,ε) ≥ (Cε)	, ∀ε > 0.
7
Published as a conference paper at ICLR 2021
The high-level idea for Lemma D.4 is to first show that PX(hu, hv) ≥ Ω( kU√VkF ), and then We
show the packing number of orthogonal matrices in a small neighborhood of Id w.r.t.《F is roughly
the same as that in the tangent space of orthogonal manifold at Id, i.e., the set of skew matrices,
which is of dimension d(d-1) and has packing number (C) d(d- 1). The advantage of working in the
tangent space is that we can apply the standard volume argument.
Setting δ = 1, we have N *(A,P,ε°) ≥N (A, P, ɪ, 2ε°) ≥ d(d-1) log2 4C- - 1 = Ω(d2).	□
Indeed, we can improve the above lower bound by applying Equation (2), and get
1	1___ι
N (A, P ,ε, 2) ≥ d (1 )'2 (CC )2 2'=Ω(d2 ε-1 + 2).	⑺
Note that the dependency in ε in Equation (7) is ε- 2+2 is not optimal, as opposed to ε-1 in upper
bounds and other lower bounds. A possible reason for this might be that Theorem 4.2 (Long’s
improved version) is still not tight and it might require a tighter probabilistic upper bound for the
growth number ΠH(n), at least taking PX into consideration, as opposed to the current upper bound
using VC dimension only. We left it as an open problem to show a single task P with Ω(d) sample
complexity for all orthogonal equivariant algorithms.
However, if the hypothesis is of VC dimension O(d), using a similar idea, we can prove a Ω(d2∕ε)
sample complexity lower bound for equivariant algorithms, and O(d) upper bounds for ConvNets.
Theorem 5.3	(Single distribution, multiple functions). There is a problem with single input distribu-
tion, P = {PX} H = {N(0, Id)} {sign Pid=1αixi2 | αi ∈ R}, such that for any orthogonal
equivariant algorithms A and ε > 0, N*(A, P, ε) = Ω(d2∕ε), while there,s a 2-layer ConvNets
architecture, such that N(ERMCNn, P, ε, δ) = O(dlog εg+log $ ).
Interestingly, we can show an analog of Theorem 5.3 for `2 regression, i.e., the algorithm not only
observes the signs but also the values of labels yi . Here we define the `2 loss of function h : Rd → R
as 'p(h) = E [(h(x) 一 y)2] and the sample complexity N*(A, P,ε) for '2 loss similarly as
(χ,y)〜P
the smallest number n ∈ N such that ∀P ∈ P,	E	['p (A({xi, yi }n=1))] ≤ ε E	[y2]. The
(χi,ya)~P	(χ,y)~P
last term E	y2 is added for normalization to avoid the scaling issue and thus any ε > 1 could
(X,y)〜P
be achieved trivially by predicting 0 for all data.
Theorem 5.4	(Single distribution, multiple functions, `2 regression). There is a problem with single
input distribution, P =	{PX}	H =	{N (0, Id)}	{Pid=1αixi2	|	αi	∈	R}	, such that for any
orthogonal equivariant algorithms A and ε › 0, N* (A, P, ε) ≥ d(d+3) (1 一 ε) 一 1, while there,s a
2-layer ConvNet architecture, such that N*(ERMCNn, P, ε) ≤ d for any ε > 0.
5.2 Ω(d) LOWER BOUND FOR PERMUTATION EQUIVARIANCE
In this subsection we will present Ω(d) lower bound for permutation equivariance via a different
proof technique — direct coupling. The high-level idea of direct coupling is to show with constant
probability over (Xn , x), we can find a g ∈ GX, such that g(Xn ) = Xn, but x and g(x) has different
labels, in which case no equivariant algorithm could make the correct prediction.
Theorem 5.5. Let ti = ei + ei+1 and si = ei + ei+23 and P be the uniform distribution on
{⑸,1)}n=1 ∪ {(ti, -1)}n=1, which is the classification problem for local textures in a 1-dimensional
image with d pixels. Then for any permutation equivariant algorithm A, N(A, P, 1, ɪ) ≥
N*(A, P, 4) ≥ 10. Meanwhile, N(ERMCNN, P, 0,δ) ≤ log2δ +2, where ERMCNN stands
for ERMCNN for function class of 2-layer ConvNets.
Remark 5.2. The task could be understood as detecting if there are two consecutive white pixels in
the black background. For proof simplicity, we take texture of length 2 as an illustrative example. It
3For vector x ∈ Rd, we define xi = x(i-1) mod d+1.
8
Published as a conference paper at ICLR 2021
is straightforward to extend the same proof to more sophisticated local pattern detection problem of
any constant length and to 2-dimensional images.
6	Conclusion
We rigorously justify the common intuition that ConvNets can have better inductive bias than FC
nets, by constructing a single natural distribution on which any FC net requires Ω(d2) samples to
generalize if trained with most gradient-based methods starting with gaussian initialization. On the
same task, O(1) samples suffice for convolutional architectures. We further extend our results to
permutation equivariant algorithms, including adaptive training algorithms like Adam and AdaGrad,
'ι regularization, etc. The separation becomes Ω(d) VS O(1) in this case.
References
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? In
Advances in Neural Information Processing Systems,pp. 9015-9025, 2019.
Yossi ArjeVani and Ohad Shamir. On the iteration complexity of obliVious first-order optimization
algorithms. In International Conference on Machine Learning, pp. 908-916, 2016.
Gyora M Benedek and Alon Itai. Learnability with respect to fixed distributions. Theoretical
Computer Science, 86(2):377-389, 1991.
Anselm Blumer, A. Ehrenfeucht, DaVid Haussler, and Manfred K. Warmuth. Learnability and the
Vapnik-cherVonenkis dimension. J. ACM, 36(4):929-965, October 1989. ISSN 0004-5411. doi:
10.1145/76359.76371. URL https://doi.org/10.1145/76359.76371.
Simon S Du, Yining Wang, Xiyu Zhai, SiVaraman Balakrishnan, Russ R SalakhutdinoV, and Aarti
Singh. How many samples are needed to estimate a conVolutional neural network? In Advances in
Neural Information Processing Systems, pp. 373-383, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
conVolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal coVariate shift. arXiv preprint arXiv:1502.03167, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Philip M. Long. On the sample complexity of PAC learning half-spaces against the uniform distribu-
tion. IEEE Transactions on Neural Networks, 6(6):1556-1559, 1995.
Zongming Ma and Yihong Wu. Volume ratio, sparsity, and minimaxity under unitarily invariant
norms. IEEE Transactions on Information Theory, 61(12):6939-6956, 2015.
Andrew Y Ng. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings
of the twenty-first international conference on Machine learning, pp. 78, 2004.
Stanislaw J Szarek. Metric entropy of homogeneous spaces. arXiv preprint math/9701213, 1997.
9
Published as a conference paper at ICLR 2021
Michel Talagrand. Upper and lower bounds for stochastic processes: modern methods and classical
problems, volume 60. Springer Science & Business Media, 2014.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
doi: 10.1017/9781108231596.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing
Systems,pp. 9709-9721, 2019.
10
Published as a conference paper at ICLR 2021
arccos x
≥= ≥
—X
t
A Some basic inequalities
Lemma A.1.
∀x ∈ [-1, 1],
Proof. Let x = cos(t), t ∈ [-π, π], we have
arccos(x)	t	> /?
√1 — X pl - cos(t)	√2sin(t∕2)-
□
Lemma A.2. ∃C > 0, ∀d ∈ N+,M ∈ Rd×d,
CkMkF /√d ≤ E	[kMxk2] ≤∣M∣f/√d.	(8)
x~Sd-ι
Proof of Lemma A.2.
Upper Bound: By Cauchy-Schwarz inequality, we have
E	[kMxk2] ≤ r E	[kMxk2] = ∕tr[M E	[xx>] M>] = Jtr[MM>] =吗F.
x~Sd-ι	V x~Sd-ι L	」 VL x~Sd-ι	_| d d	dd
Lower Bound: Let M = U ΣV > be the singular value decomposition of M, where U, V are orthog-
onal matrices and Σ is diagonal. Since kMkF = kΣkF, and E [kMxk2] = E [kΣxk2],
x~Sd-1	x~Sd-1
w.l.o.g., we only need to prove the lower bound for all diagonal matrices.
By Proposition 2.5.1 in (Talagrand, 2014), there’s some constant C, such that
Ck九=1£ σ2 ≤x~NE0,Id) t
d
X x2σ2 =— xk]2 .
By Cauchy-Schwarz Inequality, we have E	[kxk2] ≤
x~N (0,Id)
we have
E ∣∣xk2 = √d. Therefore,
V x~N(0,Id)
C kΣkF
≤ E [kMxk]2
x~N (0,Id)
=E [∣M xk]2	E [kxk2]
X~Sd-ι	x~N (0,Id)
≤ E [∣∣Mx∣∣]2 √d,
X~Sd-ι
(9)
which completes the proof.
□
Lemma A.1. For any z > 0, we have
2z
Pr (|x| ≤ Z) ≤ 十一
x~N (0,σ)	π σ
Proof.
Pr (|X| ≤ z)
x~N (0,σ)
Z √^eχp
-z 2π σ
X2
----o	dx ≤
2σ2J	≤
2z
πσ
□
11
Published as a conference paper at ICLR 2021
B Upperand lower bound for sample complexity with VC
DIMENSION
Theorem B.	1. [Blumer et al. (1989)] If learning algorithm A is consistent and ranged in H, i.e.
A({xi, yi}in=1) ∈ H and A({xi, yi}in=1)(xi) = yi, ∀i ∈ [n], then for any distribution PX and
0 < ε, δ < 1, we have
N(A,Px oH," O(VCdm(Hln ε+ ln 1).
(10)
Meanwhile, there’s a distribution PX supported on any subsets {x0, . . . , xd-1} which can be shat-
tered by H, such that for any 0 < ε, δ < 1 and any algorithm A, it holds
N(APX oH,ε,δ) = Ω(331).
ε
(11)
C	Equivariance in Algorithms
In this section, we give sufficient conditions for an iterative algorithm to be equivariant (as defined in
Algorithm 1).
Theorem C.	1. Suppose GX is a group acting on X = Rd, the iterative algorithm Ais GX -equivariant
(as defined in Algorithm 1) if the following conditions are met: (proof in appendix)
1.	There’s a group GW acting on W and a group isomorphism τ : GX → GW, such that
M[τ (g)(W)](g(x)) = M[W](x), ∀x ∈ X,W ∈ W,g ∈ G. (One can think g as the
rotation U applied on data x in linear regression and τ(U) as the rotation U applied on w.)
2.	Update rule F is invariant under any joint group action (g, τ (g)), ∀g ∈ G. In other words,
[τ (g)](F (W, M, {xi, yi}in=1)) = F ([τ (g)](W), M, {g(xi), yi}in=1).
3.	The initialization Pinit is invariant under group GW, i.e. ∀g ∈ GW, Pinit = Pinit ◦ g-1.
Here we want to address that the three conditions in Theorem C.1 are natural and almost necessary.
Condition 1 is the minimal expressiveness requirement for model M to allow equivariance. Condition
3 is required for equivariance at initialization. Condition 2 is necessary for induction.
ProofofTheorem C.1. ∀g ∈ GX, We sample W(O)〜Pinit, and W(0) = T(g)(W(0)).
By property (3), W⑼=W(O)〜Pinit. Let W(HI) = F (W⑶，M, {q, yi}ii=1) and W(HI)=
F Wf (t), M, {g(xi), yi}in=1 for 0 ≤ t ≤ T - 1, We can shoW Wf (t) = τ(g)W(t)) by induction
using property (2). By definition of Algorithm 1, We have
A{xi,yi}in=1 =d M[W(T)],
and
M[W(T)] ◦ g = A({g(χi),yi}i=1) ◦ g.
By property (1), We have M[Wf (T)](g(x)) = M[τ (g)(W(T)](g(x)) = M[W(T)](x). There-
fore, A({xi,yi}n=1) = M[W(T)] = M[W(T)] ◦ g = A({g(xi),yi}n=1) ◦ g, meaning A is
Gx-equivariant.	□
Remark C.1. Theorem C.1 can be extended to the stochastic case and the adaptive case Which alloWs
the algorithm to use information of the Whole trajectory, i.e., the update rule could be generalized
as W(t+1) = Ft({W(s)}ts=1, M, {xi, yi}in=1), as long as (the distribution of) each Ft is invariant
under joint transformations.
BeloW are tWo example applications of Theorem C.1. Other results in Table 1 could be achieved in
the same Way.
12
Published as a conference paper at ICLR 2021
For classification tasks, optimization algorithms often work with a differentiable surrogate loss
' :R → R instead the 0-1 loss, such that '(yh(x)) ≥ 1 [yh(x) ≤ 0], and the total loss for hypothesis
h and training, L(M(W); {xi, yi}rn=↑) is defined as Pn=ι '(yi[M(W)](xi)). It’s also denoted by
L(W) when there’s no confusion.
Definition C.1 (Gradient Descent for FC nets). We call Algorithm 1 Gradient Descent if M =
FC-NN and F = GDL , where GDL(W) = W - ηVL(W) is called the one-step GradientDescent
update and η > 0 is the learning rate.
Algorithm 2 Gradient Descent for FC-NN (FC networks)
Require: Initial parameter distribution Pirit , total iterations T, training dataset {xi, yi}ir=1, loss
function `
Ensure: Hypothesis h : X → Y .
Sample W(O)〜Pirit.
for t = 0 to T - 1 do
r
W(t+I) = W㈤ — η P V'(FC-NN(W⑴)(xj yi)
i=1
return h = sign FC-NN[W(T)].
Corollary C.2. Fully-connected networks trained with (stochastic) gradient descent from i.i.d.
Gaussian initialization is equivariant under the orthogonal group.
Proof of Corollary C.2. We will verify the three conditions required in Theorem C.1 one by one.
The only place we use the FC structure is for the first condition.
Lemma C.3. There’s a subgroup GW of O(m), and a group isomorphism τ : GX = O(d) → GW,
such that FC-NN[τ (R)(W)] ◦R= FC-NN[W], ∀W ∈ W,R ∈ GX.
ProofofLemma C.3. By definition, FC-NN[W](x) could be written FC-NN[W2z](σ(W1x)),
which implies FC-NN[W](x) = FC-NN[W1R-1, W2:L](Rx), ∀R ∈ O(d), and thus we can
pick T(R) = O ∈ O(m), where O(W) = [W1R-1, W2z], and GW = T(O(d)).	□
A notable property of Gradient Descent is that it is invariant under orthogonal re-parametrization. For-
mally, given loss function L : Rm → R and parameters W ∈ Rm, an orthogonal re-parametrization
of the problem is to replace (L, W) by (L ◦ O-1, OW), where O ∈ Rm×m is an orthogonal matrix.
Lemma C.4 (Gradient Descent is invariant under orthogonal re-parametization). For any L, W and
orthogonal matrix O ∈ Rm×m,we have OGDL(W) = GDLoO-I (OW).
Proof of Lemma C.4. By definition, it suffices to show that for each i ∈ [n], and every W and
W0 = OW,
OVw '(FC-NN(W)(xi),yi) = NW '(FC-NN(O-1W0)(xi),yi),
which holds by chain rule.	□
For any R ∈ O(d), and set O =	T(R) by Lemma C.3, [L ◦ O-1](W)	=
Pn=I '(yiFC-NN[O-1 (W)](xi)) = Pn=I '(yiFC-NN[W](Rxi)). The second condition in Theo-
rem C.1 is satisfied by plugging above equality into Lemma C.4.
The third condition is also satisfied since the initialization distribution is i.i.d. Gaussian, which is
known to be orthogonal invariant. In fact, from the proof, it suffices to have the initialization of the
first layer invariant under GX.	□
Corollary C.5. FC nets trained with newton’s method from zero initialization for the first layer and
any initialization for the rest parameters is GL(d)-equivariant, or equivariant under the group of
invertible linear transformations.
Here, Netwon’s method means to use NT(W) = W - η(V2L(W))-1VL(W) as the update rule
and we assume V2L(W) is invertible. Proof is deferred into Appendix, .
13
Published as a conference paper at ICLR 2021
Proof of Corollary C.5. The proof is almost the same as that of Corollary C.2, except the following
modifications.
Condition 1: If we replace the O(d), O(m) by GL(d), GL(m) in the statement and proof Lemma C.3,
the lemma still holds.
Condition 2:By chain rule, one can verify the update rule Newton’s method is invariant under
invertible linear re-parametization, i.e. OGDL(W) = NTLoO-I (OW), for all invertible matrix O.
Condition 3: Since the first layer is initialized to be 0, it is invariant under any linear transformation.
□
Remark C.2. The above results can be easily extended to the case of momentum and Lp reg-
ularization. For momentum, we only need to ensure that the following update rule, W(t+1) =
GDM(W⑶,W(t-1), M,{xi,yi}i=ι) = (1 + Y)W⑴—YW(JI) — ηVL(W(t)), also satisfies
the property in Lemma C.4. For Lp regularization, because kWkp is independent of {xi, yi}in=1,
we only need to ensure kWkp = kτ (R)(W)kp , ∀R ∈ GX, which is easy to check when GX only
contains permutation or sign-flip.
C.1 Examples of Equivariance for non-iterative algorithms
To demonstrate the wide application of our lower bounds, we give two more examples of algorithmic
equivariance where the algorithm is not iterative. The proofs are folklore.
Definition C.2. Given a positive semi-definite kernel K, the Kernel Regression algorithm REGK is
defined as:
REGK({xi, yi}n=ι)(x) := 1 [K(x, XN) ∙ K(Xn, XN)ty ≥ 0]
where	K(XN,XN)	∈	Rn×n,	[K (XN, XN)]i,j	= K(xi,xj),	y =	[y1,y2,...,yN]>	and
K(x,XN) = [K(x,x1),.. .,K(x,xN)].
Kernel Regression: If kernel K is GX -equivariant, i.e., ∀g ∈ GX , x, y ∈ X, K(g(x), g(y)) =
K(x, y), then algorithm REGK is GX -equivariant.
ERM: If F = F ◦ GX, and argminh∈F Pin=1 1 [h(xi) 6= yi] is unique, then ERMF is GX-
equivariant.
D Omitted proofs
D.1 Proofs of sample complexity reduction for general equivariance
Given GX-equivariant algorithm A, by definition, N*(A, P, ε) = N*(A, P ◦ g-1, ε), ∀g ∈ GX.
Consequently, we have
N * (A, P ,ε)= N *(A, PoGX ,ε).	(12)
Lemma D.1. Let A be the set of all algorithms and AGX be the set of all GX -equivariant algorithms,
the following inequality holds. The equality is attained when GX is a compact group.
inf N*(A,P,ε) ≥ inf N*(A,PoGX,ε)	(13)
A∈AGX	A∈A
Proof of Lemma D.1. Take infimum over AGX over the both side of Equation 12, and note that
AGX ⊂ A, Inequality 13 is immediate.
Suppose the group GX is compact and let μ be the Haar measure on it, i.e, ∀S ⊂ Gx , g ∈ GX ,μ(S)=
μ(goS). We claim for each algorithm A, the sample complexity of the following equivariant algorithm
A0 is no higher than that of A on P GX :
A0({xi,yi}n=ι) = A({g(xi),yi}n=ι) ◦ g, where g 〜μ.
By the definition of Haar measure, A0 is GX -equivariant. Moreover, for any fixed n ≥ 0, we have
inf E	[errP(A0({xi,yi}in=1))] = inf E E	[errP(A({xi,yi}in=1))]
P∈p (χi,ya)~P	P∈P g~μ (xi,ya)~Pog-1
14
Published as a conference paper at ICLR 2021
≥ ρnPg∈nfχ (Ri)IEP ◦g-i [errP (AHXi,yi}n=I))I = P ∈pM (-"MP (AHXi,yi}n=I))I，
which implies infA∈Ag% N*(A, P, ε) ≤ inf∕∈AN*(A, P ◦ Gx,ε).	口
Proofof Theorem 5.1. Simply note that (PX oH) ◦Gx = ∪g∈Gχ (PX ◦ g) o (HOg-1) = ∪g∈Gχ PX◊
(H ◦ g-1) = Pχ ◊ (H ◦ GX), the theorem is immediate from Lemma D.1.	口
D.2 Proof of Theorem 4.1
Lemma D.2. Define hU = sign X1>:dU Xd+1:2d, ∀U ∈ Rd×d, we have H = {hU | U ∈ O(d)} ⊆
signhPid=1xi2-Pi2=dd+1xi2i ◦ O(2d).
Proof. Note that
and
0U
U>	0
Id	0
0 U>
0	Id
Id	0
Id	0
0U
0	Id _	√22 Id	- √√2 Id
[Id	0] = [√Id	√Id
Id	0
0	-Id
察 Id	√22 Id]
-√√2 Id	挈 IJ
thus for any U ∈ O(d), ∀X ∈ R2d,
hU (X) = sign X1>:dU Xd+1:2d = sign X> U0> U0 X
=Sign gu(X)T Id -Id gu(x) ∈ h* oO(2d),
(14)
where gU (X)
Id 0
0U
-等Id
√2 Id
• x is an orthogonal transformation on R2d.
□
Lemma D.3. Define hu = sign X1>:dU Xd+1:2d, ∀U ∈ Rd×d, and H = {hu | U ∈ O(d)}, we
have
VCdim(H) ≥ d(d- 1.
Proof. Now we claim H shatters {ei + ed+j }1≤i<j≤d, i.e. O(d) can shatter {eiej> }1≤i<j≤d, or
for any sign pattern {σij}1≤i<j≤d, there exists U ∈ O(d), such that sign U, eiej> = σij, which
implies VCdim(H) ≥ d(d-1).
Let so(d) = {M | M = -M >, M ∈ Rd×d}, we know
exp(u) = Id + U +	+ • ∙ • ∈ S O (d), ∀u ∈ so(d).
Thus for any sign pattern {σij}1≤i<j≤d, let u = P	σij(eiej> - ejei>) and λ → 0+,
1≤i<j≤d
sign exp(λu), eiej>	= sign 0 + λσij + O(λ2) = sign [σij + O(λ)] = σij.
□
Theorem 4.1 (All distributions, single hypothesis). Let P = {all distributions} ◊ {h*}. For any
orthogonal equivariant algorithms A, N(A, P, ε, δ) = Ω((d2 + ln ɪ)∕ε), while there,s a 2-layer
ConvNet architecture, such thatN(ERMCNn, P, ε,δ) = O (ε (log ε + log 1)).
15
Published as a conference paper at ICLR 2021
Proof of Theorem 4.1. Lower bound: Suppose d = 2d0 for some integer d0, we construct
P = PX H, where PX is the set of all possible distributions on X = R3k, and H =
{sign hPid=0 1xi2 - Pi2=d0d0+1xi2i}. By Lemma D.2, H0 = {sign x1>:dU xd+1:2d | U ∈ O(d0)} ⊆
H ◦ O(d). By Theorem 5.1, we have
A∈nGx N *(A Pχ °Hε ≥ Af N *(A Pχ O(HoGX)，° ≥ Af N(A，Pχ ° H0，ε)	(15)
By the lower bound in Theorem B.1, We have infa∈aN*(A, PX ° H0, ε) ≥ VCdim(H )+ln 1. By
LemmaD.3 VCdim(H0) ≥ 飞-1) = Ω(d2).
Upper Bound: Take CNN as defined in Section 3.1 with d = 2d0, r = 2, k =
1, σ : Rd0 → R, σ(x) = Pid=1 xi2 (square activation + average pooling), we have
FCNN = sign Pi2=1 ai Pjd=0 1
x(2i-1)d0+jw12 + b |a1, a2,w1, b ∈
R.
Note that min errP (h) = 0, ∀P ∈ P, and the VC dimension of F is 3, by Theorem B.1, we have
h∈FCNN
VP ∈ P, W.p. 1 - δ, err P (ERMFCNN ({xi,yi}n=ι)) ≤ ε, if n = Ω (1 (log 1 + log 1))).
Convergence guarantee for Gradient Descent: We initialize all the parameters by i.i.d. standard
gaussian and train the second layer by gradient descent only, i.e. set the LR of w1 as 0. (Note
training the second layer only is still a orthogonal-equivariant algorithm for FC nets, thus it’s a valid
separation.)
For any convex non-increasing surrogate loss of 0-1 loss l satisfying l(0) ≥ 1, limx→∞ l(x) = 0 e.g.
logistic loss, We define the loss of the Weight W as (xk,i is the kth coordinate of xi)
n	n	2	d0
L(W)= X l(FcNN[W](Xi)yi)= X l((X a，Xx(2k-1)d0+j,iw2 + b) yi),
i=1	i=1 k=1 j =1
Which is convex in ai and b. Note w1 6= 0 With probability 1, Which means the data are separable
even with fixed first layer, i.e. mina,b L(W) = L(W) b=a*,b=o= 0, where a* is the ground truth.
Thus With sufficiently small step size, GD converges to 0 loss solution. By the definition of surrogate
loss, L(W) < 1 implies for Xi, /(Xiyi) < 1 and thus the training error is 0.	□
D.3 Proofs of Lemmas for Theorem 5.2
Lemma D.4. Define hU = sign X1>:dU Xd+1:2d, H = {hU | U ∈ O(d)}, and ρ(U, V ) :=
PX(hu, hv) = Px〜N(0,i2d) [hu(x) = hv(x)]. There exists a constant C, such that the packing
number D(H, PX, ε) = D(O(d), ρ, ε) ≥ ㈢
d(d-1)
2
16
Published as a conference paper at ICLR 2021
ProofofLemma D.4. The key idea here is to first lower bound ρχ(U, V) by ∣∣U - V∣∣f /√d and
apply volume argument in the tangent space of Id in O(d). We have
ρ(hU, hV) = P	[hU (x) 6= hV (x)]
X〜N (0,l2d)
= P	[(x>dU xd+L2d) (x>dv xd+L2d) < 0]
X〜N (0,l2d)
1
一 E
π XLd〜N(0,Id)
arccos
x>d UV >xi：d
kxi：d『
))
≥1 E
π XLd〜N(0,Id)
2-2
x>dUV >x1：d
kxi：d k2
(by Lemma A.1)
(16)
1
E
∏ X〜Sd-1
1 E h，2 - 2x>UV> X
∏ X〜Sd-1 L	-
≥Cι ∣∣U - V∣∣f /√d (by Lemma A.2)
Below we show it suffices to pack in the 0.4 '∞ neighborhood of Id. Let so(d) be the Lie algebra
of SO(d), i.e., {M ∈ Rd×d | M = -M>}. We also define the matrix exponential mapping exp :
Rd×d → Rd×d, where exp(A) = A + A22 + A33 + ∙∙∙. It holds that exp(so(d)) = SO(d) ⊆ O(d).
The benefit of covering in such neighborhood is that it allows us to translate the problem into the
tangent space of Id by the following lemma.
Lemma D.5 (Implication of Lemma 4 in (Szarek, 1997)). For any matrix A, B ∈ so(d), satisfying
that kAk∞ ≤ 4, kBk∞ ≤ 4, We have
0.4∣A-B∣F ≤ ∣exp(A) - exp(B)∣F ≤ ∣A-B∣F.	(17)
Therefore, we have
D(H,ρx,ε) ≥ D(O(d),C |卜|恨/√d,ε) ≥ D(so(d) ∩ ∏B∞,Cι |卜|恨/√d, 2.5ε).	(18)
Note that so(d) is a d(d-1) -dimensional subspace of Rd2, by Inverse Santalo,s inequality (Lemma 3,
(Ma & Wu, 2015)), we have
vol(so(d) ∩ B∞d2 )
2
d(d-1)
vol(so(d) ∩ B22)
≥ C	VZdim(So(d))
一 2 G—d2 W")/]
where vol(∙) is the d(d-1) volume defined in the space of so(d) and ∏so(d)(G) = g-g> is the
projection operator onto the subspace so(d). We further have
GJk2 J凡(d)(G<∞]=GJk2 )
G - G>
-2-
≤G〜NEθ,Id2 ) [kGk∞]≤ C3逐
∞
where the last inequality is by Theorem 4.4.5, Vershynin (2018).
17
Published as a conference paper at ICLR 2021
Finally, we have
D(so(d) ∩ 4B∞2,Cι k∙kF/√d, 2.5ε)
=D(so(d) ∩ B∞2,k∙∣∣F , 10√dε)
C1 π
2	d(d—1)
≥VOl(Som) ∩ B∞)X( Cιπ}	2
—vol(so(d) ∩ Bd)	∖10√dε )
d(d-1)
(19)
□
D.4 Proof of Theorem 5.3
Theorem 5.3 (Single distribution, multiple functions). There is a problem with single input distribu-
tion, P =	{PX}	H = {N (0, Id	)}	{sign Pid=1	αixi2	|	αi	∈	R},	such that for any orthogonal
equivariant algorithms A and ε > 0, N*(A, P, ε) = Ω(d2∕ε), while there,s a 2-layer COnVNets
architecture, such that N(ERMCNn, P, ε, δ) = O(dlog ε+log $ ).
Proof of Theorem 5.3. Lower bound: NOte P = {N (0, Id)} H, where H = {sign Pid=1 αixi2 |
αi ∈ R}. Since N(0, Id) is invariant under all OrthOgOnal transfOrmatiOns, by TheOrem 5.1,
inf	N*(A,N(0,Id) ◦ H, εο) = inf N*(A,N(0, Id) ◊ (H ◦ O(d)), ε°). Furthermore, it can
equivariant A	A
be show that H ◦ O(d) = {sign Pi,j βijxixj | βij ∈ R}, the sign functions of all quadratics in
Rd. Thus it suffices to show learning quadratic functions on Gaussian distribution needs Ω(d2∕ε)
samples for any algorithm (see Lemma D.6, where we assume the dimension d can be divided by 4).
Upper bound:Take CNN as defined in Section 3.1 with d = d0, r = 1, k = 1, σ : R → R, σ(x) =
x2 (square activatiOn + nO pOOling), we have FCNN
nsign hPid=1 aixi2 + bi |ai, b ∈ Ro.
sign Pid=1 aiw12xi2 + b |ai, w1, b ∈ R
Note that min errP (h) = 0, ∀P ∈ P, and the VC dimension of F is d + 1, by Theorem B.1, we
h∈FCNN
have ∀P ∈ P, w.p. 1 - δ, errp (ERMFCNN({xi, yi}n=ι)) ≤ ε, if n = Ω (ε (dlog 1 + log 1))).
Convergence guarantee for Gradient Descent: We initialize all the parameters by i.i.d. standard
gaussian and train the second layer by gradient descent only, i.e. set the LR of w1 as 0. (Note
training the second layer only is still a orthogonal-equivariant algorithm for FC nets, thus it’s a valid
separation.)
For any convex non-increasing surrogate loss of 0-1 loss l satisfying l(0) ≥ 1, limx→∞ l(x) = 0 e.g.
logistic loss, we define the loss of the weight W as (xk,i is the kth coordinate of xi)
n	nd
L(W) = X l(FCNN[W](xi)yi) =Xl (X w12aix2k,i + b)yi ,
i=1	i=1	k=1
which is convex in ai and b. Note w1 6= 0 with probability 1, which means the data are separable
even with fixed first layer, i.e. mina,b L(W) = L(W) ∣a=a*,b=o= 0, where a* is the ground truth.
18
Published as a conference paper at ICLR 2021
Thus with sufficiently small step size, GD converges to 0 loss solution. By the definition of surrogate
loss, L(W) < 1 implies for xi, l(xiyi) < 1 and thus the training error is 0.
□
D.5 Proof of Lemma D.6
Lemma D.6. For A ∈	Rd×d ,	we define	MA	∈	R2d×2d	as	MA	= A 0 , and	hA :
0 Id
R4d → {-1, 1} as hA (x) = sign x1>:2dMAx2d+1:4d. Then for H = {hA | ∀A ∈ Rd×d} ⊆
{sign [x>Ax]∣∀A ∈ R4d×4d]}, satisfies that it holds that for any d, algorithm A and ε > 0,
d2
N *(A,{N (0,l4d)}oH,ε) = Ω( 三).
d2
ProofofLemma D.6. Below We will prove a Ω((十) ) lower bound for packing number, i.e.
D(H, ρX, 2ε0) = D(Rd×d, ρ, 2ε0), where ρ(U, V ) = ρX(hU, hV ). Then we can apply Long’s
improved version Equation (2) of Benedek-Itai,s lower bound and geta Ω(d2∕ε) sample complexity
lower bound. The reason that we can get the correct rate of ε is that the VCdim(H) is exactly equal
to the exponent of the packing number. (cf. the proof of Theorem 5.2)
Similar to the proof of Theorem 5.2, the key idea here is to first lower bound ρ(U, V ) by
∣∣U 一 V∣∣f/√d and apply volume argument. Recall for A ∈ Rd×d, we define MA ∈ R2d×2d
as MA = A0 I0 , and hA : R4d → {-1, 1} as hA(x) = sign [x1>:2dMAx2d+1:4d]. Then for
H = {hA | ∀A ∈ Rd×d} . Below we will see it suffices to lower bound the packing num-
ber of a subset of Rd×d, i.e. Id + 0.1B∞d2, where B∞d2 is the unit spectral norm ball. Clearly
∀x, ∣x∣2 = 1, ∀U ∈ Id + 0.1B∞d2, 0.9 ≤ ∣U x∣2 ≤ 1.1.
Thus ∀U, V ∈ Id + 0.1B∞d2 we have,
ρX(hU, hV ) = P	[hU (x) 6= hV (x)]
X〜N (0,l4d)
P	[(x>2dMu X2d+1:4d) (x>2dMv X2d+1:4d) < θ]
X〜N (0,l4d)
1
E
π Xl：2d 〜N (0,I2d )
arccos
x>2dMu M>xi：2d
∣∣M>xL2d∣∣2 ∣∣M>χL2d∣∣2
))
≥1 E
π Xl:2d 〜N (0,I2d )
」2一 2 U χ>2dMUMVχL2d J (byLemmaA.1)
V	∣∣M>XL2d∣□∣M>XL2d□ j	J
√2
≥ ∏π xi:2d 〜E(0,i2d)
JUM>Xl：2d|[2 ∣∣M>XL2dU2 - X>2dMuM>Xl：2d
=n^ Es r、q∣l(M> - M>)X1：2dll2- (|lM>Xi：2dll2- llM>Xi：2dll2)2
1.1π Xl:2d〜N(0,l2d) L
≥TT~( E [ll(M> - M>)X1：2dH2]
l.lπ Xi：2d〜N(0,l2d)
一 E	[llMU>X1：2d ll2 一 llMV>X1：2d ll2])
Xi：2d 〜N (0,l2d)
≥C0-	E	[『M> - M>)Xi：2dll2] (by LemmaD.7)
1.1π Xi：2d〜N(0,l2d )
≥Cι ∣∣Mu 一 MV∣∣f /√d (by Lemma A.2)
=Ci IlU - V IlF /√d
(20)
19
Published as a conference paper at ICLR 2021
It remains to lower bound the packing number. We have
M(0.1B∞2,Cι k∙kF/√d,ε)
≥vol(B∞2) × (Sλd2
-vol(Bd2)	V √dε 7
(21)
for some constant C. The proof is completed by plugging the above bound and VCdim(H) = d2
into Equation (2).
□
Lemma D.7. Suppose x, X 〜N(0,Id), then ∀R, S ∈ Rd×d, We have
E [k(R - S)xk2] - E [ q∣∣Rxk2 + kyk2 - q∣∣Sxk2 + ky∣∣2 1 ≥ Co E [k(R - S)xk2], (22)
x	x,y	x
for some constants C0 independent of R, S and d.
Proof of Lemma D.7. Note that
JkRχk2 + kyk2 - Jksχk2 + kyk2
= |kRxk2 - kSxk2|
_______kRxk2 + ksxk2______
JkRχk2 + kyk2 + JkS xk2 + kyk2
≤k(R-S)xk2
_______kRxk2 + kSxk2______
JkRxk2 + kyk2 + JkS xk2 + kyk2
Let F (x, d) be the cdf of chi-square distribution, i.e. F (x, d) = Px
have F(zd, d) ≤ (ze1-z)d/2 ≤ (ze1-z)1/2. Thus Py hkyk22 ≤ d/2i
kxk2 ≤ 10√d,
[kx∣∣2 ≤ x]. Let Z = d, We
< 1, Which implies for any
√kRxk2 + kyk2 -√kSxk2 + kyk2
≤k(R-S)xk2E
y
kRxk2 + kSxk2
.√kRxk2 + kyk2 + √kS xk2 + kyk2 -
≤(1-α1)k(R-S)xk2,
for some 0 < α1 .
Therefore, We have
E[k(R-S)xk2]- E
x
≥E
x
x,y
√kRxk2 + kyk2 -√kSxk2 + kyk2
[k(R - S)xk21 [kxk ≤ 10√d]]
E
y
-E
x,y
≥α1 E
x
JkRXk2 + kyk2 -JkSXk2 + kyk2 1
[k(R-S)xk2IhkXk2 ≤ 10√dii
[kx∣∣2 ≤ I0√d]]
20
Published as a conference paper at ICLR 2021
≥α1α2 E [k(R - S)xk2] ,
x
for some constant α2 > 0. Here we use the other side of the tail bound of cdf of chi-square, i.e. for
z > 1, 1 -F(zd,d) < (ze1-z)d/2 < (ze1-z)1/2.
□
D.6 Proofs of Theorem 5.4
LemmaD.8. Let M ∈ Rd×d,wehave	E	∖(x>Mx)2] = Il M +M> 112 + (tr[M])2.
x~N (0,Id)	F
Proof of Lemma D.8.
E	(x>Mx)2
x~N (0,Id)
E
x~N(0,Id)
xixj xi0 xj0 Mij Mi0j0
i,j,i0j0
X(Mi2j+MijMji+MiiMjj)	E x22 + X Mi2i	E x4
i6=j	x~N(0,1)	i	x~N(0,1)
(Mi2j+MijMji+MiiMjj)+3	Mi2i
i6=j	i
M + M >
2
□
Theorem 5.4 (Single distribution, multiple functions, `2 regression). There is a problem with single
input distribution, P =	{PX }	H = {N(0, Id)}	{Pid=1	αixi2	|	αi	∈	R}	, such that for any
orthogonal equivariant algorithms A and ε > 0, N*(A, P, ε) ≥ d(d+3) (1 一 ε) — 1, while there,s a
2-layer ConvNet architecture, such that N*(ERMCNn, P, ε) ≤ d for any ε > 0.
Proof of Theorem 5.4. Lower bound: Similar to the proof of Theorem 5.3, it suffices to for any
algorithm A, N*(A, HoO(d),ε) ≥ d(d+3) (1 — ε) -1. Note that HoO(d) = {Pi,j βijXiXj | βij ∈
R} is the set of all quadratic functions. For convenience we denote hM (x) = x> Mx, ∀M ∈ Rd×d.
Now we claim quadratic functions such that any learning algorithm A taking at most n samples must
suffer d(d+1) — n loss if the ground truth quadratic function is sampled from i.i.d. gaussian. Moreover,
the loss is at most d(d+3) for the trivial algorithm always predicting 0. In other words, if the expected
d(d+1) -n
relative error ε ≤	壮.+3), we must have the expected sample complexity N* (A, P, ε) ≥ n. That
2
isN*(A,P,ε) ≥ d(d+3)(1 — ε) — 1.
(1)	. Upper bound for E y2 . By Lemma D.8,
E	E	y2 = E
M ~N (0,Id2 ) x~PX ,y=x> Mx	M ~N (0,Id2 )
M±M> |2 + (tr[M])21 = d+d+ F =中.
(2)	. Lower bound for expected loss.
The infimum of the test loss over all possible algorithms A is
呼 M~NE0,ld2 )[(χi,yi)EPχ.hM['P(AHXi,yi}n=I))I
21
Published as a conference paper at ICLR 2021
inf E	E	E	([A({xi,yi}in=1)](x) - y)2
A M〜N(0,加2 ) L(χi,y,)〜PXohM |_x,y〜PXohm
inf E	E
A M〜N(0,Id2 ) Lxi〜PX
E	([A({xi, hM(xi)}in=1)](x) - hM (x))2
x〜PX
≥ E Var [hM (x) | {xi, hM(xi)}in=1, x]
xi,x 〜PX Lx,xi,M
M〜N(0,Id2 )
= E	hVar [hM (x) | {hM(xi)}in=1]i ,
M〜N(0,Id2)
where the inequality is achieved when [A({xi, yi}in=1)](x) = E [hM (x) | {xi, yi}in=1].
i=1	M	i=1
Thus it suffices to lower bound VarM [hM (x) | {hM (xi)}in=1], for fixed {xi}in=1 and x. For conve-
nience we define Sd = {A ∈ Rd×d | A = A>} be the linear space of all d × d symmetric matrices,
where the inner product hA, Bi := tr[A>B] and Πn : Rd×d → Rd×d as the projection operator for
the orthogonal complement of the n-dimensional space spanned by xixi> in Sd . By definition, we
can expand
n
xx> =	αixixi> + Πn (xx>).
i=1
Thus even conditioned on {xi , yi }in=1 and x,
n
hM (x) = tr[xx>] = Xαitr[xixi>M] + tr[Πn(xx>)M],
i=1
still follows a gaussian distribution, N(0, Πn(xx>)2F).
Note we can always find symmetric matrices Ei with kEi kF = 1 and tr[Ei> Ej] = 0 such that
∏n(Α) = Pk=I Eitr[E> A], where the rank of ∏n, is at least d(d+1) - n. Thus We have
Ex Πn(xx>)F
=E [χEitr[E>χχ>]
k
= X Ex hEitr[Ei>χχ>]Fi
i=1
k
= X E (χ> Ei> χ)2 (byLemma D.8)
i=1 x
k
≥XkEik2F ≥k
i=1
≥ d(d+D - n
2
Thus the infimum of the expected test loss is
呼 M Jk2 )[(xi,yi)EPX°hM['P (AHXi,yi}n=1力
22
Published as a conference paper at ICLR 2021
≥E
xi,x~PX
M ~N (0,Id2)
VMar [hM (x) | {hM(xi)}in=1] .
E
χi~Pχ
M~N(0,Id2 )
Ex	Πn(xx>)2F	.
≥ d(d+D -n.
2	2
Upper bound: We use the same CNN construction as in the proof of Theorem 5.3, i.e., the function
class is
FCNN = Pid=1 aiw12xi2+b|ai, w1, b ∈
R = Pid=1 aixi2 + b|ai , b ∈ R . Thus given
d+1 samples, w.p. 1, (x21, x22, . . . , x2d, 1) will be linear independent, which means ERMCNN could
recover the ground truth and thus have 0 loss.
□
D.7 Proof of Theorem 5.5
Theorem 5.5. Let ti = ei + ei+1 and si = ei + ei+24 and P be the uniform distribution on
{(si, 1)}in=1 ∪ {(ti, -1)}in=1, which is the classification problem for local textures in a 1-dimensional
image with d pixels. Then for any permutation equivariant algorithm A, N(A, P, 1, 8) ≥
N*(A,P, 4) ≥ 10. Meanwhile, N(ERMCNN, P,0,δ) ≤ log2 δ + 2, where ERMCNN stands
for ERMCNN for function class of 2-layer ConvNets.
Proof of Theorem 5.5. Lower Bound: We further define permutation gi as gi (x) = x - (ei+1 -
ei+2)>(ei+1 - ei+2)x for i ∈ [d]. Clearly, gi(ti) = si, gi(si) = ti. For i,j ∈ {1, 2, . . . , d}, we
define d(i, j) = min{(i - j) mod d, (j - i) mod d}. It can be verified that if d(i, j) ≥ 3, then
gi(sj) = sj, gi(tj) = tj. For x = si or ti, x0 = sj or tj, we define d(x, x0) = d(i, j).
Given Xn, yn, we define B := {d(x, xk) ≥ 3, ∀k ∈ [n]} and we have P [B] =
d— W *5
Px [d(x, Xk) ≥ 3, ∀k ∈ [n]] ≥ —d— = 21. Therefore, we have
errP(A(Xn,yn)) = P [A(Xn,yn)(x) 6= y] ≥ P [A(Xn,yn)(x) 6= y | B]P[B]
x,y,A	x,y,A
≥ 1 P [A(Xn, yn)(χ) = y | B]
2 x,y,A
=P P [A(Xn, Yn)(Si) = 1 | B] + 彳 P [A(Xn, ynXti) = -1 | B]
4 i,A	4 i,A
(=) 1 P [A(gi(Xn), yn)(gi(Si)) = 1 | B] + 1 P [A(Xn, Yn)(ti) = -1 | B]
4 i,A	4 i,A
=P P [A(Xn, ynXti) = I | B] + - P [A(Xn, ynXti) = -1 | B]=-.
4 i,A	4 i,A	4
Thus for any permutation equivariant algorithm A, N*(A, {P}, 4) ≥ 10.
Upper Bound: Take CNN as defined in Section 3.1 with d0 = d, r = 1, k = 2, σ : Rd → R,
sign a1 Pid=1
σ(χ) = Pid=1xi2, we have FCNN
(w1xi + w2xi-1)2 + b|a1,w1,w2,b ∈ R
.
Note that ∀h ∈ FCNN, ∀1 ≤ i ≤ d, h(si) = a1(2w12+2w22)+b, h(ti) = a1(w12+w22+(w1+w2)2)+b,
thus the probability of ERMFCNN not achieving 0 error is at most the probability that all data in the
training dataset are ti or si: (note the training error of ERMFCNN is 0)
P χi ∈ {sj}jd=1,∀i ∈ [n] +P χi ∈ {tj}jd=1,∀i ∈ [n] = 2-n × 2 = 2-n+1.
4For vector x ∈ Rd, we define xi = x(i-1) mod d+1.
23
Published as a conference paper at ICLR 2021
Convergence guarantee for Gradient Descent: We initialize all the parameters by i.i.d. standard
gaussian and train the second layer by gradient descent only, i.e. set the LR of w1, w2 as 0. (Note
training the second layer only is still a permutation-equivariant algorithm for FC nets, thus it’s a valid
separation.)
For any convex non-increasing surrogate loss of 0-1 loss l satisfying l(0) ≥ 1, limx→∞ l(x) = 0 e.g.
logistic loss, we define the loss of the weight W as
n
L(W) = X l(FCNN[W](xi)yi)
i=1
=NS × l (aι (2w； + 2w2) + b) + Nt × l (—aι (wɪ + w2 + (wι + w2 产)+ b).
Note w1w2 6= 0 with probability 1, which means the data are separable even with fixed first layer, i.e.
infa1,b L(W) = 0. Further note L(W) is convex in a1 and b, which implies with sufficiently small
step size, GD converges to 0 loss solution. By the definition of surrogate loss, L(W) < 1 implies for
Xi, l(xiyi) < 1 and thus the training error is 0.	□
24