Published as a conference paper at ICLR 2021
On the mapping between Hopfield networks
and Restricted B oltzmann Machines
Matthew Smart
Department of Physics
University of Toronto
msmart@physics.utoronto.ca
Anton Zilman
Department of Physics
and Institute for Biomedical Engingeering
University of Toronto
zilmana@physics.utoronto.ca
Ab stract
Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two
important models at the interface of statistical physics, machine learning, and neu-
roscience. Recently, there has been interest in the relationship between HNs and
RBMs, due to their similarity under the statistical mechanics formalism. An exact
mapping between HNs and RBMs has been previously noted for the special case
of orthogonal (“uncorrelated”) encoded patterns. We present here an exact map-
ping in the case of correlated pattern HNs, which are more broadly applicable to
existing datasets. Specifically, we show that any HN with N binary variables and
p < N arbitrary binary patterns can be transformed into an RBM with N binary
visible variables and p gaussian hidden variables. We outline the conditions under
which the reverse mapping exists, and conduct experiments on the MNIST dataset
which suggest the mapping provides a useful initialization to the RBM weights.
We discuss extensions, the potential importance of this correspondence for the
training of RBMs, and for understanding the performance of deep architectures
which utilize RBMs.
1	Introduction
Hopfield networks (HNs) (Hopfield, 1982; Amit, 1989) are a classical neural network architecture
that can store prescribed patterns as fixed-point attractors of a dynamical system. In their standard
formulation with binary valued units, HNs can be regarded as spin glasses with pairwise interactions
Jij that are fully determined by the patterns to be encoded. HNs have been extensively studied in
the statistical mechanics literature (e.g. (Kanter & Sompolinsky, 1987; Amit et al., 1985)), where
they can be seen as an interpolation between the ferromagnetic Ising model (p = 1 pattern) and
the Sherrington-Kirkpatrick spin glass model (many random patterns) (Kirkpatrick & Sherrington,
1978; Barra & Guerra, 2008). By encoding patterns as dynamical attractors which are robust to
perturbations, HNs provide an elegant solution to pattern recognition and classification tasks. They
are considered the prototypical attractor neural network, and are the historical precursor to modern
recurrent neural networks.
Concurrently, spin glasses have been used extensively in the historical machine learning literature
where they comprise a sub-class of “Boltzmann machines” (BMs) (Ackley et al., 1985). Given a
collection of data samples drawn from a data distribution, one is generally interested in “training”
a BM by tuning its weights Jij such that its equilibrium distribution can reproduce the data distri-
bution as closely as possible (Hinton, 2012). The resulting optimization problem is dramatically
simplified when the network has a two-layer structure where each layer has no self-interactions, so
that there are only inter-layer connections (Hinton, 2012) (see Fig. 1). This architecture is known as
a Restricted Boltzmann Machine (RBM), and the two layers are sometimes called the visible layer
and the hidden layer. The visible layer characteristics (dimension, type of units) are determined by
the training data, whereas the hidden layer can have binary or continuous units and the dimension
is chosen somewhat arbitrarily. In addition to generative modelling, RBMs and their multi-layer
extensions have been used for a variety of learning tasks, such as classification, feature extraction,
and dimension reduction (e.g. Salakhutdinov et al. (2007); Hinton & Salakhutdinov (2006)).
1
Published as a conference paper at ICLR 2021
There has been extensive interest in the relationship between HNs and RBMs, as both are built
on the Ising model formalism and fulfill similar roles, with the aim of better understanding RBM
behaviour and potentially improving performance. Various results in this area have been recently
reviewed (Marullo & Agliari, 2021). In particular, an exact mapping between HNs and RBMs has
been previously noted for the special case of uncorrelated (orthogonal) patterns (Barra et al., 2012).
Several related models have since been studied (AgIiari et al., 2013; Mezard, 2017), which partially
relax the uncorrelated pattern constraint. However, the patterns observed in most real datasets exhibit
significant correlations, precluding the use of these approaches.
In this paper, we demonstrate exact correspondence between HNs and RBMs in the case of corre-
lated pattern HNs. Specifically, we show that any HN with N binary units and p < N arbitrary
(i.e. non-orthogonal) binary patterns encoded via the projection rule (Kanter & Sompolinsky, 1987;
Personnaz et al., 1986), can be transformed into an RBM with N binary and p gaussian variables.
We then characterize when the reverse map from RBMs to HNs can be made. We consider a practi-
cal example using the mapping, and discuss the potential importance of this correspondence for the
training and interpretability of RBMs.
2	Results
We first introduce the classical solution to the problem of encoding N -dimensional binary {-1, +1}
vectors {ξμ}μ=ι, termed “patterns”, as global minima of a pairwise spin glass H(S) = - 11 ST Js.
This is often framed as a pattern retrieval problem, where the goal is to specify or learn Jij such
that an energy-decreasing update rule for H(S) converges to the patterns (i.e. they are stable fixed
points). Consider the N × p matrix ξ with the p patterns as its columns. Then the classical prescrip-
tion known as the projection rule (or pseudo-inverse rule) (Kanter & Sompolinsky, 1987; Personnaz
et al., 1986), J = ξ(ξTξ)-1ξT, guarantees that the p patterns will be global minima of H(S). This
resulting spin model is commonly called a (projection) Hopfield network, and has the Hamiltonian
H(S) = - 1 ST ξ(ξT ξ)-1ξT s.	(1)
Note that ξTξ invertibility is guaranteed as long as the patterns are linearly independent (we there-
fore require P ≤ N). Also note that in the special (rare) case of orthogonal patterns ξμ ∙ ξν = Nδμν
(also called “uncorrelated”), studied in the previous work (Barra et al., 2012), one has ξTξ = NI
and so the pseudo-inverse interactions reduce to the well-known Hebbian form J = NξξT (the
properties of which are studied extensively in Amit et al. (1985)). Additional details on the pro-
jection HN Eq. (1) are provided in Appendix A. To make progress in analyzing Eq. (1), we first
consider a transformation of ξ which eliminates the inverse factor.
2.1	Mapping a Hopfield Network to a Restricted B oltzmann Machine
In order to obtain a more useful representation of the quadratic form Eq. (1) (for our purposes), we
utilize the QR-decomposition (Schott & Stewart, 1999) ofξ to “orthogonalize” the patterns,
ξ=QR,	(2)
with Q ∈ RN ×p , R ∈ Rp×p . The columns of Q are the orthogonalized patterns, and form an
orthonormal basis (of non-binary vectors) for the p-dimensional subspace spanned by the binary
patterns. R is upper triangular, and if its diagonals are held positive then Q and R are both unique
(Schott & Stewart, 1999). Note both the order and sign of the columns of ξ are irrelevant for HN
pattern recall, so there are n = 2p ∙ p! possible Q, R pairs. Fixing a pattern ordering, We can use the
orthogonality of Q to re-write the interaction matrix as
J = ξ(ξT ξ)-1ξT = QR(RTR)-1RTQT = QQT	(3)
(the last equality follows from (RT R)-1 = R-1(RT)-1). Eq. (3) resembles the simple Hebbian
rule but with non-binary orthogonal patterns. Defining q ≡ QTS in analogy to the classical pattern
overlap parameter m ≡ NNξτS (Amit et al., 1985), We have
H(S) = -1 STQQTS = -2q(S)∙ q(S).	(4)
2
Published as a conference paper at ICLR 2021
Using a Gaussian integral as in Amit et al. (1985); Barra et al. (2012); Mezard (2017) to transform
(exactly) the partition function Z ≡ P{s} e-βH(s) of Eq. (1), we get
Z = X e 1(βq)T (β-1i)(βq)
{s}
X / e-2 Pμ λμ+β Pμ λμ Pi Qe Y
dλ“
(5)
P2n/e.
The second line can be seen as the partition function of an expanded Hamiltonian for the N (binary)
original variables {si} and the P (continuous) auxiliary variables {λμ}, i.e.
HRBM({si}, {λμ}) = 2 X λμ - XXQiμ Siλμ.	(6)
Note that this is the Hamiltonian of a binary-continuous RBM with inter-layer weights Qiμ. The
original HN is therefore equivalent to an RBM described by Eq. (6) (depicted in Fig. 1). As
mentioned above, there are many RBMs which correspond to the same HN due to the combinatorics
of choosing Q. In fact, instead of QR factorization one can use any decomposition which satisfies
J = UUT, with orthogonal U ∈ RN×p (see Appendix B), in which case U acts as the RBM
weights. Also note the inclusion of an applied field term - Pi bisi in Eq. (1) trivially carries
through the procedure, i.e. HRBM({si}, {λ“}) = 2 pμ *l - Pi biSi - pμ Pi QiμSiλμ.
I = ξ(ξτξ)^1ξτ
Figure 1: Correspondence between Hop-
field Networks (HNs) with correlated pat-
terns and binary-gaussian restricted boltz-
mann machines (RBMs). The HN has N
binary units and pairwise interactions J de-
fined by p < N (possibly correlated) pat-
terns {ξμ}μ=ι. The patterns are encoded as
minima of Eq. (1) through the projection
rule J = ξ(ξTξ)-1ξT, where ξμ form the
columns of ξ. We orthogonalize the patterns
through a QR decomposition ξ = QR. The
HN is equivalent to an RBM with N binary
visible units and p gaussian hidden units with
inter-layer weights defined as the orthogonal-
ized patterns Qiμ, and Hamiltonian Eq. (6).
See (Agliari et al., 2017) for the analogous
mapping in the uncorrelated case.
Instead of working with the joint form Eq. (6), one could take a different direction from Eq. (5) and
sum out the original variables {si}, i.e.
Z = e --2 Pμ λ 2N Y
i
Cosh β∑>μλμ Π
dλμ
P2∏∕β.
(7)
μ
μ
This continuous, p-dimensional representation is useful for numerical estimation ofZ (Section 3.1).
We may write Eq. (7) as Z = R --F0(λ)dλμ, where
Fθ({λμ}) = 2 X λμ- 1 X lncosh (β X Qiμλμ) .	(8)
μ	" i	μ	/
Eq. (8) is an approximate Lyapunov function for the mean dynamics of {λμ}; VλF0 describes the
effective behaviour of the stochastic dynamics of the N binary variables {si} at temperature β-1.
2.2	Comments on the reverse mapping
With the mapping from HNs (with correlated patterns) to RBMs established, we now consider the
reverse direction. Consider a binary-continuous RBM with inter-layer weights Wiμ which couple a
3
Published as a conference paper at ICLR 2021
visible layer of N binary variables {si} to a hidden layer of P continuous variables {λμ},
H ER = ! X λμ — X biSi- XX WiμSiλμ.	(9)
Here we use W instead of Q for the RBM weights to emphasize that the RBM is not necessarily
an HN. First, following Mehta et al. (2019), we transform the RBM to a BM with binary states by
integrating out the hidden variables. The corresponding Hamiltonian for the visible units alone is
(see Appendix D.1 for details),
H(S) = - X bi Si- 2 XXX WiμWjμSiSj ,	(10)
i	i j μ
a pairwise Ising model with a particular coupling structure Jij = P* WiμWjμ, which in vector
form is
J = X WμwT = WW T,	(11)
μ
where {wμ} are the P columns of W.
In general, this Ising model Eq. (10) produced by integrating out the hidden variables need not
have Hopfield structure (discussed below). However, it automatically does (as noted in Barra et al.
(2012)), in the very special case where Wiμ ∈{-1, +1}. In that case, the binary patterns are simply
{wμ}, so that Eq. (11) represents a Hopfield network with the Hebbian prescription. This situation
is likely rare and may only arise as a by-product of constrained training; for a generically trained
RBM the weights will not be binary. It is therefore interesting to clarify when and how real-valued
RBM interactions W can be associated with HNs.
Approximate binary representation of W: In Section 2.1, we orthogonalized the binary matrix
ξ via the QR decomposition ξ = QR, where Q is an orthogonal (but non-binary) matrix, which
allowed us to map a projection HN (defined by its patterns ξ, Eq. (1)) to an RBM (defined by its
inter-layer weights Q, Eq. (6)).
Here we consider the reverse map. Given a trained RBM with weights W ∈ RN ×p, we look for an
invertible transformation X ∈ Rp×p which binarizes W. We make the mild assumption that W is
rank P. If we find such an X, then B = WX will be the Hopfield pattern matrix (analogous to ξ),
with Biμ ∈ { —1, +1}.
This is a non-trivial problem, and an exact solution is not guaranteed. As a first step to study the
problem, we relax it to that of finding a matrix X ∈ GLp(R) (i.e. invertible, P × P, real) which
minimizes the binarization error
arg min ||WX -sgn(WX)||F.	(12)
X∈GLp(R)
We denote the approximately binary transformation of W via a particular solution X by
Bp = WX.	(13)
We also define the associated error matrix E ≡ Bp - sgn(Bp). We stress that Bp is non-binary and
approximates B ≡ sgn(Bp), the columns of which will be HN patterns under certain conditions on
E . We provide an initial characterization and example in Appendix D.
3	Experiments on MNIST Dataset
Next we investigate whether the Hopfield-RBM correspondence can provide an advantage for train-
ing binary-gaussian RBMs. We consider the popular MNIST dataset of handwritten digits (LeCun
et al., 1998) which consists of28 × 28 images of handwritten images, with greyscale pixel values 0
to 255. We treat the sample images as N ≡ 784 dimensional binary vectors of {-1, +1} by setting
all non-zero values to +1. The dataset includes M ≡ 60, 000 training images and 10, 000 testing
images, as well as their class labels μ ∈ {0,..., 9}.
4
Published as a conference paper at ICLR 2021
3.1	Generative objective
The primary task for generative models such as RBMs is to reproduce a data distribution. Given
a data distribution pdata, the generative objective is to train a model (here an RBM defined by its
parameters θ), such that the model distribution pθ is as close to pdata as possible. This is often quan-
tified by the KUllbaCk-Leibler(KL) divergence DKL(Pdatakpθ) = PsPdata(s)ln (pp^)One
generally does not have access to the actual data distribution, instead there is usually a representa-
tive training set S = {sa}aM=1 sampled from it. As the data distribUtion is constant with respect to
θ, the generative objective is equivalent to maximizing L(θ)= 吉 Pa lnpθ(s。).
3.1.1	Hopfield RBM specification
With labelled classes of training data, specification of an RBM via a one-shot Hopfield rule (“Hop-
field RBM”) is straightforward. In the simplest approach, we define P = 10 representative patterns
via the (binarized) class means
"Sgn (⅛ X
(14)
where μ ∈ {0,..., 9} and Sμ is the set of sample images for class μ. These patterns comprise the
columns of the N × P pattern matrix ξ, which is then orthogonalized as in Eq. (2) to obtain the RBM
weights W which couple N binary visible units to P gaussian hidden units.
We also consider refining this approach by considering sub-classes within each class, representing,
for example, the different ways one might draw a “7”. As a proof of principle, we split each digit
class into k sub-patterns using hierarchical clustering. We found good results with Agglomera-
tive clustering using Ward linkage and Euclidean distance (see Murtagh & Contreras (2012) for an
overview of this and related methods). In this way, we can define a hierarchy of Hopfield RBMs. At
one end, k = 1, we have our simplest RBM which has P = 10 hidden units and encodes 10 patterns
(using Eq. (14)), one for each digit class. At the other end, 10k/N → 1, we can specify increas-
ingly refined RBMs that encode k sub-patterns for each of the 10 digit classes, for a total ofP = 10k
patterns and hidden units. This approach has an additional cost of identifying the sub-classes, but is
still typically faster than training the RBM weights directly (discussed below).
The generative performance as a function ofk and β is shown in Fig. 2, and increases monotonically
with k in the range plotted. If β is too high (very low temperature) the free energy basins will be
very deep directly at the patterns, and so the model distribution will not capture the diversity of
images from the data. If β is too low (high temperature), there is a “melting transition” where the
original pattern basins disappear entirely, and the data will therefore be poorly modelled. Taking
α = p/N 〜0.1 (roughly k = 8), Fig. 1 of Kanter & Sompolinsky (1987) predicts βm ≈ 1.5
for the theoretical melting transition for the pattern basins. Interestingly, this is quite close to our
observed peak near β = 2. Note also as k is increased, the generative performance is sustained at
lower temperatures.
(A) *=M小
-200
-400
-600
-800
-1000
-1200
2	4	6	8	10
Figure 2: Hopfield RBM generative perfor-
mance as a function of β for varying numbers
of encoded sub-patterns k per digit. The num-
ber of hidden units in each RBM is p = 10k,
corresponding to the total number of encoded
patterns. ln Z is computed using annealed im-
portance sampling (AIS) (Neal, 2001) on the
continuous representation of Z, Eq. (7), with
500 chains for 1000 steps (see Appendix E).
Each curve displays the mean of three runs.
In situations where one already has access to the class labels, this approach to obtain RBM weights
is very fast. The class averaging has negligible computational cost O(MN) for the whole training
5
Published as a conference paper at ICLR 2021
set (M samples), and the QR decomposition has a modest complexity of O(Np2) (Schott & Stewart,
1999). Conventional RBM training, discussed below, requires significantly more computation.
3.1.2	Conventional RBM training
RBM training is performed through gradient ascent on the log-likelihood of the data, L(θ) =
M Pa lnpθ(Sa) (equivalent here to minimizing KL divergence, as mentioned above). We are fo-
cused here on the weights W in order to compare to the Hopfield RBM weights, and so we neglect
the biases on both layers. As is common (Hinton, 2012), we approximate the total gradient by split-
ting the training dataset into “mini-batches”, denoted B. The resulting gradient ascent rule for the
weights is (see Appendix E)
Wit+1 = Wit + η
iμ	μ ʃ
(15)
where hsiλμimodel ≡ Z-1 P{s} / siλ*e-βH(SK) Q* dλ* is an average over the model distribution.
The first bracketed term of Eq. (15) is simple to calculate at each iteration of the weights. The second
term, however, is intractable as it requires one to calculate the partition function Z. We instead
approximate it using contrastive divergence (CD-K) (Carreira-Perpinan & Hinton, 2005; Hinton,
2012). See Appendix E for details. Each full step of RBM weight updates involves O(KBN p)
operations (Melchior et al., 2017). Training generally involves many mini-batch iterations, such that
the entire dataset is iterated over (one epoch) many times. In our experiments we train for 50 epochs
with mini-batches of size 100 (3 ∙ 105 weight updates), so the overall training time can be extensive
compared to the one-shot Hopfield approach presented above. For further details on RBM training
see e.g. Hinton (2012); Melchior et al. (2017).
In Fig. 3, we give an example of the Hopfield RBM weights (for k = 1), as well as how they evolve
during conventional RBM training. Note Fig. 3(a), (b) appear qualitatively similar, suggesting that
the proposed initialization Q from Eqs. (2), (14) may be near a local optimum of the objective.
(a) Hopfield initialized (0 epochs)
(b) Hopfield initialized (50 epochs)
Figure 3: Binary-gaussian RBM weights for p = 10 hidden units prior to and during generative
training. (a) Initial values of the columns of W (specified as the orthogonalized Hopfield patterns
via Eqs. (2), (14)). (b) Same columns of W after 50 epochs of CD-20 training (see Fig. 4(a)).
0.15
0.10
0.05
0.00
-0.05
-0.10
-0.15
In Fig. 4(a), (b), we compare conventional RBM training on four different weight initializations: (i)
random Wiμ 〜N(0,0.01) (purple), commonly used in the literature; (ii) our proposed weights from
the projection rule Hopfield mapping for correlated patterns (blue); (iii) the “Hebbian” Hopfield
mapping described in previous work for uncorrelated patterns, W = N-1∕2ξ (Barra et al., 2012)
(green); and (iv) the top p PCA components of the training data (pink). In Fig. 4(c), (d) we compare
generated sample images from two RBMs, each with p = 50 hidden units but different initial
weights (random in (c) and the HN mapping in (d)). The quality of samples in Fig. 4(d) reflect the
efficient training of the Hopfield initialized RBM.
Fig. 4(a), (b) show that the Hopfield initialized weights provide an advantage over other approaches
during the early stages of training. The PCA and Hebbian initializations start at much lower values
of the objective and require one or more epochs of training to perform similarly (Fig. 4(a) inset),
while the randomly initialized RBM takes > 25 epochs to reach a similar level. All initializations
ultimately reach the same value. This is noteworthy because the proposed weight initialization is
fast compared to conventional RBM training. PCA performs best for intermediate training times.
6
Published as a conference paper at ICLR 2021
(b) 50 hidden units
(c) Normal initialization, 15 epochs
(d) Hopfield initialization, 15 epochs
Figure 4: Generative performance of binary-gaussian RBMs trained with (a) p = 10 and (b) p = 50
hidden units. Curves are colored according to the choice of weight initialization (see legend in (b),
and further detail in the preceding text). Each curve shows the mean and standard deviation over
5 runs. The inset in (a) details the first two epochs. We compute lnpθ as in Fig. 2, but with 100
AIS chains. The learning rate is η0 = 10-4 except the first 25 epochs of the randomly initialized
weights in (b), where we used η = 5η0 due to slow training. The mini-batch size is B = 100 for all
curves in (b) and the purple curve in (a), and B = 1000 otherwise. (c), (d) Samples from two RBMs
from (b) (projection HN and random) after 15 epochs, generated by initializing the visible state to
an example image from the desired class and performing 20 RBM updates with β = 2. Training
parameters: β = 2, and CD-20.
Despite being a common choice, the random initialization trains surprisingly slowly, taking roughly
40 epochs in Fig. 4(a), and in Fig. 4(b) we had to increase the basal learning rate η0 = 10-4
by a factor of 5 for the first 25 epochs due to slow training. The non-random initializations, by
comparison, arrive at the same maximum value much sooner. The relatively small change over
training for the Hopfield initialized weights supports the idea that they may be near a local optimum
of the objective, and that conventional training may simply be mildly tuning them (Fig. 3).
That the HN initialization performs well at 0 epochs suggests that the p Hopfield patterns concisely
summarize the dataset. This is intuitive, as the projection rule encodes the patterns (and nearby
states) as high probability basins in the free energy landscape of Eq. (1). As the data itself is
clustered near the patterns, these basins should model the true data distribution well. Overall, our
results suggest that the HN correspondence provides a useful initialization for generative modelling
with binary-gaussian RBMs, displaying excellent performance with minimal training.
3.2	Classification objective
As with the generative objective, we find that the Hopfield initialization provides an advantage for
classification tasks. Here we consider the closely related MNIST classification problem. The goal
is to train a model on the MNIST Training dataset which accurately predicts the class of presented
images. The key statistic is the number of misclassified images on the MNIST Testing dataset.
We found relatively poor classification results with the single (large) RBM architecture from the
preceding Section 3.1. Instead, we use a minimal product-of-experts (PoE) architecture as described
in Hinton (2002): the input data is first passed to 10 RBMs, one for each class μ. This “layer of
RBMs” functions as a pre-processing layer which maps the high dimensional sample s to a feature
vector f(s) ∈ R10. This feature vector is then passed to a logistic regression layer in order to predict
the class of s. The RBM layer and the classification layer are trained separately.
The first step is to train the RBM layer to produce useful features for classification. As in Hinton
(2002), each small RBM is trained to model the distribution of samples from a specific digit class μ.
We use CD-20 generative training as in Section 3.1, with the caveat that each expert is trained solely
on examples from their respective class. Each RBM connects N binary visible units to k gaussian
hidden units, and becomes an “expert” at generating samples from one class. To focus on the effect
of interlayer weight initialization, we set the layer biases to 0.
After generative training, each expert should have relatively high probability pθμ)(sa) for sample
digits Sa of the corresponding class μ, and lower probability for digits from other classes. This idea
is used to define 10 features, one from each expert, based on the log-probability of a given sample
7
Published as a conference paper at ICLR 2021
under each expert, lnpθμ) (Sa) = -βH")(Sa) - ln Z(μ). Note that β and ln Z(μ) are constants with
respect to the data and thus irrelevant for classification. For a binary-gaussian RBM, H(μ)(sa) has
the simple form Eq. (10), so the features we use are
f (μ)(s) = XXX W(μ) WjV)Sisj = IlsT W (μ)l∣2.	(16)
ijν
With the feature map defined, we then train a standard logistic regression classifier (using scikit-
learn (Pedregosa et al., 2011)) on these features. In Fig. 5, we report the classification error on the
MNIST Testing set of 10,000 images (held out during both generative and classification training).
Note the size p = 10 of the feature vector is independent of the hidden dimension k of each RBM,
so the classifier is very efficient.
119 7 5 3
S JON。⅛¾L
0α⅛
50
40
30Ch
O
Oθp
2

Figure 5: Product-of-experts classification
performance for the various weight initializa-
tions. For each digit model (expert), we per-
form CD-20 training according to Eq. (15)
(as in Fig. 4) for a fixed number of epochs.
A given sample image is mapped to the 10-
dimensional feature vector with elements Eq.
(16). These features are used to train a logistic
regression classifier, and the average MNIST
Test set errors are reported. The initializations
considered for each expert's weights W(μ ∈
RN×k are Wiμ 〜N(0,0.01) (purple, dash-
dot), PCA (pink, dashed), and the orthogo-
nalized Hopfield sub-patterns for digit class μ
(blue, solid). The error bars show the min/max
of three runs.
Despite this relatively simple approach, the PoE initialized using the orthogonalized Hopfield pat-
terns (“Hopfield PoE”) performs fairly well (Fig. 5, blue curves), especially as the number of sub-
patterns is increased. We found that generative training beyond 50 epochs did not significantly
improve performance for the projection HN or PCA. (in Fig. E.1, we train to 100 epochs and also
display the aforementioned “Hebbian” initial condition, which performs much worse for classifi-
cation). Intuitively, increasing the number of hidden units k increases classification performance
independent of weight initialization (with sufficient training).
For k fixed, the Hopfield initialization provides a significant benefit to classification performance
compared to the randomly initialized weights (purple curves). For few sub-patterns (circles k = 10
and squares k = 20), the Hopfield initialized models perform best without additional training and
until 1 epoch, after which PCA (pink) performs better. When each RBM has k = 100 hidden
features (triangles), the Hopfield and PCA PoE reach 3.0% training error, whereas the randomly
initialized PoE reaches 4.5%. However, the Hopfield PoE performs much better than PCA with
minimal training, and maintains its advantage until 10 epochs, after which they are similar. Inter-
estingly, both the Hopfield and PCA initialized PoE with just k = 10 encoded patterns performs
better than or equal to the k = 100 randomly initialized PoE at each epoch despite having an order
of magnitude fewer trainable parameters. Finally, without any generative training (0 epochs), the
k = 100 Hopfield PoE performs slightly better (4.4%) than the k = 100 randomly initialized PoE
with 50 epochs of training.
4 Discussion
We have presented an explicit, exact mapping from projection rule Hopfield networks to Restricted
Boltzmann Machines with binary visible units and gaussian hidden units. This provides a gener-
alization of previous results which considered uncorrelated patterns (Barra et al., 2012), or special
cases of correlated patterns (Agliari et al., 2013; Mezard, 2017). We provide an initial Characteriza-
tion of the reverse map from RBMs to HNs, along with a matrix-factorization approach to construct
approximate associated HNs when the exact reverse map is not possible. Importantly, our HN to
8
Published as a conference paper at ICLR 2021
RBM mapping can be applied to correlated patterns such as those found in real world datasets. As
a result, we are able to conduct experiments (Section 3) on the MNIST dataset which suggest the
mapping provides several advantages.
The conversion of an HN to an equivalent RBM has practical utility: it trades simplicity of presen-
tation for faster processing. The weight matrix of the RBM is potentially much smaller than the HN
(Np elements instead of N(N - 1)/2). More importantly, proper sampling of stochastic trajectories
in HNs requires asynchronous updates of the units, whereas RBM dynamics can be simulated in a
parallelizable, layer-wise fashion. We also utilized the mapping to efficiently estimate the partition
function of the Hopfield network (Fig. 2) by summing out the spins after representing it as an RBM.
This mapping also has another practical utility. When used as an RBM weight initialization, the
HN correspondence enables efficient training of generative models (Section 3.1, Fig. 4). RBMs
initialized with random weights and trained for a moderate amount of time perform worse than
RBMs initialized to orthogonalized Hopfield patterns and not trained at all. Further, with mild
training of just a few epochs, Hopfield RBMs outperform conventionally initialized RBMs trained
several times longer. The revealed initialization also shows advantages over alternative non-random
initializations (PCA and the “Hebbian” Hopfield mapping) during early training. By leveraging
this advantage for generative tasks, we show that the correspondence can also be used to improve
classification performance (Section 3.2, Fig. 5, Appendix E.3).
Overall, the RBM initialization revealed by the mapping allows for smaller models which perform
better despite shorter training time (for instance, using fewer hidden units to achieve similar clas-
sification performance). Reducing the size and training time of models is critical, as more realistic
datasets (e.g. gene expression data from single-cell RNA sequencing) may require orders of mag-
nitude more visible units. For generative modelling of such high dimensional data, our proposed
weight initialization based on orthogonalized Hopfield patterns could be of practical use. Our the-
ory and experiments are a proof-of-principle; if they can be extended to the large family of deep
architectures which are built upon RBMs, such as deep belief networks (Hinton et al., 2006) and
deep Boltzmann machines (Salakhutdinov & Hinton, 2009), it would be of great benefit. This will
be explored in future work.
More broadly, exposing the relationship between RBMs and their representative HNs helps to ad-
dress the infamous interpretability problem of machine learning which criticizes trained models
as “black boxes”. HNs are relatively transparent models, where the role of the patterns as robust
dynamical attractors is theoretically well-understood. We believe this correspondence, along with
future work to further characterize the reverse map, will be especially fruitful for explaining the
performance of deep architectures constructed from RBMs.
Acknowledgments
The authors thank Duncan Kirby and Jeremy Rothschild for helpful comments and discussions.
This work is supported by the National Science and Engineering Research Council of Canada
(NSERC) through Discovery Grant RGPIN 402591 to A.Z. and CGS-D Graduate Fellowship to M.S.
References
David H Ackley, Geoffrey E Hinton, and Terrence J. Sejnowski. A learning algorithm for boltz-
mann machines. Cognitive Science, 9(1):147-169, 1985. ISSN 03640213. doi: 10.1016/
S0364-0213(85)80012-4.
Elena Agliari, Adriano Barra, Andrea De Antoni, and Andrea Galluzzi. Parallel retrieval of cor-
related patterns: From Hopfield networks to Boltzmann machines. Neural Networks, 38:52-63,
2013. ISSN 08936080. doi: 10.1016/j.neunet.2012.11.010. URL http://dx.doi.org/10.
1016/j.neunet.2012.11.010.
Elena Agliari, Adriano Barra, Chiara Longo, and Daniele Tantari. Neural Networks Retrieving
Boolean Patterns in a Sea of Gaussian Ones. Journal of Statistical Physics, 168(5):1085-1104,
2017. ISSN 00224715. doi: 10.1007/s10955-017-1840-9.
9
Published as a conference paper at ICLR 2021
Daniel J. Amit. Modeling Brain Function: The World of Attractor Neural Networks. Cambridge
University Press, sep 1989. doi: 10.1017/cbo9780511623257.
Daniel J. Amit, Hanoch Gutfreund, and H. Sompolinsky. Spin-glass models of neural networks.
PhysicalReviewA, 32(2):1007-1018,1985. ISSN 10502947. doi: 10.1103∕PhysRevA.32.1007.
Adriano Barra and Francesco Guerra. About the ergodic regime in the analogical Hopfield neural
networks: Moments of the partition function. Journal of Mathematical Physics, 49(12), 2008.
ISSN 00222488. doi: 10.1063/1.3039083.
Adriano Barra, Alberto Bernacchia, Enrica Santucci, and Pierluigi Contucci. On the equivalence of
Hopfield networks and Boltzmann Machines. Neural Networks, 34:1-9, 2012. ISSN 08936080.
doi: 10.1016/j.neunet.2012.06.003. URL http://dx.doi.org/10.1016/j.neunet.
2012.06.003.
Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. In Aistats,
volume 10, pp. 33-40. Citeseer, 2005.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504-507, 2006. ISSN 0036-8075. doi: 10.1126/science.1127647. URL
https://science.sciencemag.org/content/313/5786/504.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14(8):1771-1800, 2002. ISSN 08997667. doi: 10.1162/089976602760128018.
Geoffrey E. Hinton. A practical guide to training restricted boltzmann machines. Lecture Notes
in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture
Notes in Bioinformatics), 7700 LECTU:599-619, 2012. ISSN 03029743. doi: 10.1007/
978-3-642-35289-8-32.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief
nets. Neural Computation, 2006. ISSN 08997667. doi: 10.1162/neco.2006.18.7.1527.
J J Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the National Academy of Sciences of the United States of America, 79(8):2554-
2558, 1982. ISSN 00278424. doi: 10.1073/pnas.79.8.2554.
I. Kanter and H. Sompolinsky. Associative recall of memory without errors. Physical Review A, 35
(1):380-392, 1987. ISSN 10502947. doi: 10.1103/PhysRevA.35.380.
Scott Kirkpatrick and David Sherrington. Infinite-ranged models of spin-glasses. Physical Review
B, 17(11):4384-4403, 1978. ISSN 01631829. doi: 10.1103/PhysRevB.17.4384.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2323, 1998. ISSN 00189219. doi:
10.1109/5.726791.
Per Olov Lowdin. On the Nonorthogonality Problem. Advances in Quantum Chemistry, 5(C):
185-199, 1970. ISSN 00653276. doi: 10.1016/S0065-3276(08)60339-1.
Chiara Marullo and Elena Agliari. Boltzmann machines as generalized hopfield networks: A review
of recent results and outlooks. Entropy, 23(1):1-16, 2021. ISSN 10994300. doi: 10.3390/
e23010034.
Pankaj Mehta, Marin Bukov, Ching Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K
Fisher, and David J Schwab. A high-bias, low-variance introduction to Machine Learning for
physicists, 2019. ISSN 03701573.
Jan Melchior, Nan Wang, and Laurenz Wiskott. Gaussian-binary restricted Boltzmann machines
for modeling natural image statistics. PLoS ONE, 12(2), 2017. ISSN 19326203. doi: 10.1371/
journal.pone.0171015.
Marc Mezard. Mean-field message-passing equations in the Hopfield model and its generalizations.
Physical Review E, 95(2):1-15, 2017. ISSN 24700053. doi: 10.1103/PhysRevE.95.022117.
10
Published as a conference paper at ICLR 2021
Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clustering: An overview. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2(1):86-97, 2012. ISSN
19424787. doi: 10.1002/widm.53.
Radford M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125-139, 2001.
ISSN 09603174. doi: 10.1023/A:1008923215028.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,
Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duch-
esnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:
2825-2830, 2011. ISSN 15324435.
L. Personnaz, I. Guyon, and G. Dreyfus. Collective computational properties of neural networks:
New learning mechanisms. Physical Review A, 34(5):4217-4228, 1986. ISSN 10502947. doi:
10.1103/PhysRevA.34.4217.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep Boltzmann machines. In Journal of Machine
Learning Research, volume 5, pp. 448-455, 2009.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for
collaborative filtering. In Proceedings of the 24th International Conference on Machine Learn-
ing, ICML ’07, pp. 791-798, New York, NY, USA, 2007. Association for Computing Machin-
ery. ISBN 9781595937933. doi: 10.1145/1273496.1273596. URL https://doi.org/10.
1145/1273496.1273596.
James R. Schott and G. W. Stewart. Matrix Algorithms, Volume 1: Basic Decompositions. Journal
of the American Statistical Association, 94(448):1388, 1999. ISSN 01621459. doi: 10.2307/
2669960.
A	Hopfield network details
Consider p < N N-dimensional binary patterns {ξμ}力=[that are to be “stored”. From them,
construct the N × p matrix ξ whose columns are thep patterns. If they are mutually orthogonal (e.g.
randomly sampled patterns in the large N → ∞ limit), then choosing interactions according to the
Hebbian rule, JHebb = 得ξξT, guarantees that they will all be stable minima of H(S) = - 2ST Js,
provided α ≡ p/N < αc, where αc ≈ 0.14 (Amit et al., 1985). If they are not mutually orthogonal
(referred to as correlated), then using the “projection rule” JProj = ξ(ξT ξ)-1ξT guarantees that
they will all be stable minima of H(S), provided p < N (Kanter & Sompolinsky, 1987; Personnaz
et al., 1986). Note JProj → JHebb in the limit of orthogonal patterns. In the main text, we use J as
shorthand for JProj.
We provide some relevant notation from Kanter & Sompolinsky (1987). Define the overlap ofa state
s with the P patterns as m(s) ≡ Nξτs, and define the projection of a state S onto the P patterns as
a(s) ≡ (ξτξ)-1ξτs ≡ A-1m(s). Note A ≡ ξτξ is the overlap matrix, and m*, a* ∈ [-1,1].
We can re-write the projection rule Hamiltonian Eq. (1) as
H(S) = — Nm(s) ∙ a(s).	(A.1)
For simplicity, we include the self-interactions rather than keeping track of their omission; the results
are the same as N → ∞. From Eq. (A.1), several quadratic forms can be written depending on
which variables one wants to work with:
I.	H(S) = - Nmτ(ξτξ)-1m
II.	H(S) = -1 aτ(ξτξ)a
These are the starting points for the alternative Boltzmann Machines (i.e. not RBMs) presented in
Appendix C.
11
Published as a conference paper at ICLR 2021
B Additional HN to RBM mappings
We used QR factorization in the main text to establish the HN to RBM mapping. However, one can
use any decomposition which satisfies
JProj = UUT	(B.1)
such that U ∈ RN×p is orthogonal (orthogonal for tall matrices means UTU = I). In that case, U
becomes the RBM weights. We provide two simple alternatives below, and show they are all part of
the same family of orthogonal decompositions.
“Square root” decomposition: Define the matrix K ≡ ξ(ξTξ)-1/2. Note that K is orthogonal, and
that JProj = KKT .
Singular value decomposition: More generally, consider the SVD of the pattern matrix ξ :
ξ = UΣVT	(B.2)
where U ∈ RN×p, V ∈ Rp×p store the left and right singular vectors (respectively) of ξ as orthog-
onal columns, and Σ ∈ Rp×p is diagonal and contains the singular values of ξ. Note in the limit of
orthogonal patterns, We have Σ = √NI. This decomposition gives several relations for quantities
of interest:
A≡ξTξ	=VΣ2VT
JHebb ≡ NξξT	= NUC2UT
JProj ≡ ξ(ξTξ)-1ξT	= UUT.
(B.3)
The last line is simply the diagonalization of JProj, and shoWs that our RBM mapping is preserved
if We sWap the Q from QR With U from SVD. HoWever, since there are p degenerate eigenvalues
σ2 = 1, U is not unique - any orthogonal basis for the 1-eigenspace can be chosen. Thus U0 = UO
Where O is orthogonal is also valid, and the QR decomposition and “square root” decomposition
correspond to particular choices of O .
C HN to BM mappings using alternative representations of the
Hopfield Hamiltonian
In addition to the orthogonalized representation in the main text, there are tWo natural representations
to consider based on the pattern overlaps and projections introduced in Appendix A. These lead to
generalized Boltzmann Machines (BMs) consisting of N original binary spins and p continuous
variables. These representations are not RBMs as the continuous variables interact With each other.
We present them for completeness.
“Overlap” Boltzmann Machine: Writing H(S) = 一N22mτ(ξτξ)-1m, we have
Z = exp
{s}
Applying the gaussian integral identity,
Z = qdet (ξrξ) ^X Z e-2N Pμ,ν(ξTξ"λμλν+√N Pμλμ Pi ξiμsi γ , dλ" ,
from which we identify the BM Hamiltonian
H(S, λ) = 2N X(STξ)μνλμλν - √N XX ξiμsiλμ.
(C.1)
(C.2)
(C.3)
This is the analog of Eq. (6) in the main text for the “overlap” representation. Note we can also sum
out the binary variables in Eq. (C.2), which allows for an analogous expression to Eq. (8),
F0({λμ}) = 2N X(ST ξ)μν λμλν - 1 X ln cosh √N^ X ξiμλμ j ∙ (C，4)
μ,ν	i i	μ μ μ	)
12
Published as a conference paper at ICLR 2021
Curiously, we note that we may perform a second Gaussian integral on Eq. (C.2), introducing new
auxiliary variables TV to remove the interactions between the λμ variables:
Z=Hn X ZZ e-2 τTτ+品 λTξTs+i √N λT(ξTξi YY √∏ YY—
Eq. (C.5) describes a three-layer RBM with complex interactions between the λμ and TV variables,
a representation which could be useful in some contexts.
“Projection” Boltzmann Machine: Proceeding as above but for H(S) = -2ατ(ξTξ)a, one finds
Z = det(ξTξ)-1/2 X ∣' e-2λT(ξTξ)-1λ+βλT(ξTξ)-1ξTs π	,	(C.6)
{s} J	μ，2n/e
which corresponds to the BM Hamiltonian
H(s, λ) = 2λτ(ξτξ)-1λ - λ(ξτξ)-1ξτs.	(C.7)
The analogous expression to Eq. (8) in this case is
Fo(λ) = 1 λτ(ξτξ)-1λ - 1 Xlncosh (β[ξ(ξτξ)-1λ]i) .	(C.8)
2β
D RBM to HN details
D. 1 Integrating out the hidden variables
The explanation from Mehta et al. (2019) for integrating out the hidden variables of an RBM is
presented here for completeness. For a given binary-gaussian RBM defined by HRBM(s, λ) (as in
Eq. (9)), we have p(s) = Z-1 R e-βHRBM(s,λ) dλ. Consider also that p(s) = Z-1e-βH(s) for some
unknown H(s). Equating these expressions gives,
H(S) = - X biSi- 1 X ln (J e-β1 λμ+ 上^”但dλj.	(D.1)
i	μ '	z
Decompose the argument of ln(∙) in Eq. (D.1) by defining q*(λ*), a gaussian with zero mean and
variance β-1. Writing t* = β Pi WiμSi, one observes that the second term (UP to a constant) is a
sum of cumulant generating functions, i.e.
Kμ(tμ) ≡ ln hetμλμ iqμ =m(/ qμ^^λ dλμ
(D.2)
These can be written as a cumulant expansion, Kμ(tμ) = Pn=ι Kμn *, where κμn = d∂Kμ 卜仙=0
is the nth cumulant of q*. However, since q*(λμ) is gaussian, only the second term remains, leaving
Kμ(tμ) = β (β Pi WMSi) . Putting this all together, we have
H(s) = - X biSi - 2 XXX
WiμWjμSiSj,	(D.3)
i	i j μ
Note that in general, q*(λμ) need not be gaussian, in which case the resultant Hamiltonian H(s)
can have higher order interactions (expressed via the cumulant expansion).
D.2 Approximate reverse mapping
Suppose one has a trial solution Bp = WX to the approximate binarization problem Eq. (12), with
error matrix E ≡ Bp - sgn(Bp). We consider two cases depending on if W is orthogonal.
13
Published as a conference paper at ICLR 2021
Case 1:	If W is orthogonal, then starting from Eq. (11) and applying Eq. (13), we have J =
WWT = Bp(XTX)-1BpT. Using I = WTW,weget
J = Bp(BpTBp)-1BpT .	(D.4)
Thus, the interactions between the visible units is the familiar projection rule used to store the
approximately binary patterns Bp. “Storage” means the patterns are stable fixed points of the deter-
ministic update rule st+1 ≡ sgn(Jst).
We cannot initialize the network to a non-binary state. Therefore, the columns of B = sgn(Bp) are
our candidate patterns. To test if they are fixed points, consider
sgn(JB) = sgn(JBp -JE) = sgn(Bp -JE).
We need the error E to be such that JE will not alter the sign of Bp. Two sufficient conditions are:
(a)	Smanerror: ∣(JE)iμ∣ < ∣(Bp)iμ∣, or
(b)	error with compatible sign: (JE')iμ(Bp)iμ < 0.
When either of these conditions hold for each element, we have sgn(JB) = sgn(Bp) = B, so that
the candidate patterns B are fixed points. It remains to show that they are also stable (i.e. minima).
Case 2:	If W is not orthogonal but its singular values remain close to one, then LoWdin Orthogo-
nalization (also known as Symmetric Orthogonalization) (LoWdin, 1970) provides a way to preserve
the HN mapping from Case 1 above.
Consider the SVD of W: W = UΣV T. The closest matrix to W (in terms of Frobenius norm)
with orthogonal columns is L = UVT, and the approximation W ≈ L is called the LOWdin
Orthogonalization of W. Note the approximation becomes exact when all the singular values are
one. We then write WWT ≈ UUT, and the orthogonal W approach of Case 1 can then be applied.
On the other hand, W may be strongly not orthogonal (singular values far from one). If it is still
full rank, then its pseudo-inverse W * = (W T W )-1W T = V Σ-1U T is well-defined. Repeating
the steps from the orthogonal case, we note here XTX = BT(W*)TW*Bp. Defining C ≡
(W*)TW* = UΣ-2UT, we arrive at the corresponding result,
J = Bp(BpTCBp)-1BpT.	(D.5)
This is analogous to the projection rule but with a “correction factor” C . However, it is not imme-
diately clear how C affects pattern storage. Given the resemblance between JProj and Eq. (D.4)
(relative to Eq. (D.5)), we expect that RBMs trained with an orthogonality constraint on the weights
may be more readily mapped to HNs.
D.3 Example of the approximate reverse mapping
In the main text we introduced the approximate binarization problem Eq. (12), the solutions of
which provide approximately binary patterns through Eq. (13). To numerically solve Eq. (12) and
obtain a candidate solution X*, we perform gradient descent on a differentiable variant.
Specifically, we replace sgn(u) with tanh(αu) for large α. Define E = WX - tanh(αW X) as
in the main text. Then the derivative of the “softened” Eq. (12) with respect to X is
G(X) = 2WT(E- αEsech2(WX))).	(D.6)
Given an initial condition X0 , we apply the update rule
Xt+1 = Xt - γG(Xt)	(D.7)
until convergence to a local minimum X*.
In the absence of prior information, we consider randomly initialized X0 . Our preliminary experi-
ments using Eq. (D.7) to binarize arbitrary RBM weights W have generally led to high binarization
error. This is due in part to the difficulty in choosing a good initial condition X0 , which will be
explored in future work.
14
Published as a conference paper at ICLR 2021
To avoid this issue, we consider the case of a Hopfield-initialized RBM following CD-k training. At
epoch 0, We in fact have the exact binarization solution X * = R (from the QR decomposition, Eq.
(2)), which recovers the encoded binary patterns ξ. We may use X0 = R, as an informed initial
condition to Eq. (D.7), to approximately binarize the Weight at later epochs and monitor hoW the
learned patterns change.
In Fig. D.1, We give an example of this approximate reverse mapping for a Hopfield-initialized
RBM folloWing generative training (Fig. 4). Fig. D.1(a) shoWs the p = 10 encoded binary patterns,
denoted beloW by ξ0, and Fig. D.1(b) shoWs the approximate reverse mapping applied to the RBM
Weights at epoch 10. We denote these nearly binary patterns by ξ10. Interestingly, some of the
non-binary regions in Fig. D.1(b) coincide With features that distinguish the respective pattern. For
example, the strongly “off” area in the top-right of the “six” pattern.
(a) Encoded Hopfield patterns
(b) Recovered Hopfield patterns (10 epochs)
(2.50
1.50
0.75
-0.25
--0.25
--0.75
[-1.50
-2.50
Figure D.1: Reverse mapping example. (a) The p = 10 encoded binary patterns used to initialize
the RBM in Fig. 4(a). (b) The approximate reverse mapping applied to the RBM Weights after 10
epochs of CD-k training. Parameters: α = 200 and γ = 0.05.
We also considered the associative memory performance of the projection HNs built from the the
patterns ξ0, ξ10 from Fig. D.1. Specifically, We test the extent to Which images from the MNIST
Test dataset are attracted to the correct patterns. Ideally, each pattern should attract all test images
of the corresponding class. The results are displayed in Fig. D.2 and elaborated in the caption.
Net accuracy 76.8%
Net accuracy 73.7%
O	I 883 I	o	4	10	5	14	27	2	31	4		o	I 772	12	8	19	7	59	58	2	32	7		-1000
1-I	o I	1106	I 0	3	1	8	2	0	14	1		T-I		1112	0	3	0	4	2	0	14	0		
CXJ	31	117	711	17	31	0	39	22	50	14		CN	22	120	744	14	33	2	33	17	41	5		一 800
co	6	34	29 I	793	1	37	8	15	72	15		CO	2	46	57	691	2	94	9	8	84	16		
寸	4	34	0	1 I	664	0	44	3	20	212		寸	0	39	0	1 I	728	0	38	2	18	156		-600
m	28	107	4	147	21	465	33	14	30	43		LO	15	115	0	161	25	469	28	8	34	35	■	
9	28	53	15	2	77	28	749	I 2	3	1		9	18	61	25	1	80	28	739	0	3	0		-400
z	8	80	11	0	16	1	3	830	18	61		Z	4	106	12	4	18	1	2	779	126	73		
OO	20	45	10	88	5	16	14	101	726			OO	14	51	14	129	6	36	17	12	649	46		-200
6	15	34	5	8	99	10	9	47	31	751 I		6	6	35	6	11	158	14	2	45	45	685 H		-o
	0	1	2	3	4	5	6	7	8	9			0	1	2	3	4	5	6	7	8	9		
				Fixed point												Fixed point								
(a) Epoch 0 patterns
(b) Epoch 10 patterns
Figure D.2: Associative memory task using (a) ξ0, the initial Hopfield patterns, and (b) ξ10, the
patterns recovered from the reverse mapping after 10 epochs. The patterns are used to construct a
projection HN as in Eq. (1). Each sample from the MNIST Test set is updated deterministically
until it reaches a local minimum of the HN. If the fixed point is one of the encoded patterns, the
sample contributes value 1 to the table. OtherWise, We perform stochastic updates With β = 2.0
and ensemble size n = 20 until one of the encoded patterns is reached, defined as an overlap
NTsTsgn(ξμ) > 0.7, with each trajectory contributing 1/n to the table.
15
Published as a conference paper at ICLR 2021
The results in Fig. D.2 suggest that p = 10 patterns may be too crude for the associative memory
network to be used as an accurate MNIST classifier (as compared to e.g. Fig. 5). Notably, the HN
constructed from ξ10 performs about 3% worse than the HN constructed from ξ0, although this per-
formance might improve with a more sophisticated method for the associative memory task. There
may therefore be a cost, in terms of associative memory performance, to increasing the generative
functionality of such models (Fig. 4). Our results from Appendix D.2 indicate that incorporating
an orthogonality constraint on the weights during CD-k generative training may provide a way to
preserve or increase the associative memory functionality. This will be explored in future work.
E RBM training
Consider a general binary-gaussian RBM with N visible and p hidden units. The energy function is
H (S, λ)=2 X(λμ - Cμ)2 - X biSi - XX WiμSiλμ.	(E.1)
First, we note the Gibbs-block update distributions for sampling one layer of a binary-gaussian RBM
given the other (see e.g. Melchior et al. (2017)),
Visible units: p(si = 1∣λ)= …乙匕,where Xi ≡ £* Wi*λ* + b defines input to s%,
Hidden units: p(λμ = λ∣s)〜N(hμ, β-1), where hμ ≡ Pi WiμSi + Cμ defines the input to λμ.
E.1 Generative training
For completeness, we re-derive the binary-gaussian RBM weight updates, along the lines of Mel-
Chior et al. (2017). We want to maximize L ≡ M Pa lnpθ(sa). The contribution for a single
datapoint Sa has the form lnpθ(Sa) = ln(CT R e-βH(Sa,λ)dλ) - ln Z with C ≡ (2π∕β)p/2. The
gradient with respect to the model is
∂ ln Pθ (Sa ) = R (-β d⅛λi )e-βH(Sa ,λ) Qμ dλμ	P{s}R Siλμe-βH(SaN Qμ dλμ
—∂θ — =	R e-βH(Sa,λ) Qμ dλμ	β	Z
(E.2)
We are focused on the interlayer weights, with dfW,λ) = -s%λμ, so
∂ ln Pθ (Sa) = R , 、R(λμe-βH(SaNQμdλμ	P{s} R $,"眈 (Se)Q*dλμ
∂θ= e(Sa)i R eTH(Sa,λ) Qμdλμ	- β	Z	(E.3)
=e(Sa)i hλμ |S = Sai model	- B hsi λμimodel∙
The first term is straightforward to compute: (λμ∣S = SaimOdeI = £, Wiμ(Sa)i + Cμ. The second
term is intractable and needs to be approximated. We use contrastive divergence (Carreira-Perpinan
& Hinton, 2005; Hinton, 2012): hs,λμimodei ≈ Sik)入优). Here k denotes CD-k 一 k steps of Gibbs-
(k)	(k)
block updates (introduced above) 一 from which Si ,λμ ' comprise the final state. We evaluate both
terms over mini-batches of the training data to arrive at the weight update rule Eq. (15).
E.2 Generative performance
We are interested in estimating the objective function L ≡ M Pa lnpθ(Sa) during training. As
above, we split L into two terms,
L = ln (CTJ e-βH(Sa,λ ∩dλμj - ln Z,
with C ≡ (2∏∕β)p/2. The first term evaluates to
M X(bT Sa + 2 ||c + W T Sa||2) - 2llcll2,
a=1
(E.4)
(E.5)
16
Published as a conference paper at ICLR 2021
which can be computed deterministically. ln Z, on the other hand, needs to be estimated. For this
we perform Annealed Importance Sampling (AIS) (Neal, 2001) on its continuous representation
Z = C -qN e e-2 Pμ(λμ-Cμ)2 +Pi lnCOsh(β(Pμ Wiμλμ + bi)) Y 4入口.
μ
(E.6)
For AIS we need to specify the target distribution’s un-normalized log-probabilities
ln(Zp(λ))
N ln2 - P ln (瓦)-2 X(λμ - cμ)2 + X lncosh
μ	i
Wiμλμ + b))
(E.7)
as well as an initial proposal distribution to anneal from, which we fix as a p-dimensional unit normal
distribution N(0, I).
E.3 Classification performance
Here we provide extended data (Fig. E.1) on the classification performance shown in Fig. 5. As
in the main text, color denotes initial condition (introduced in Section 3.1) and shape denotes the
number of sub-patterns. Here we train for each 100 epochs, which allows convergence of most of
the curves. We also include the “Hebbian” initialization (green curves).
9 8 7 6 5 4
(鸵 ɪoɪɪə ^səɪ
φ τmτ□
HΘKb
Initial RBM weights WiIIit
Wiμ ~ -ʌf(θ, 0.01)
φ Hopfield (Hebbian)
0 PCA
0 Hopfield (projection)
Number of sub-patterns k
0	□	△
10	20	100
τφx
&
S
国
Δ
≡至
Q
φ
U
≡∆i
⅛ΞΔ-⅛
△
C
δ Ss Λ□



□
E

O IO 20	30	40	50	60	70	80	90 IOO
epoch
Figure E.1: Product-of-experts classification performance (extended data).
Notably, the Hebbian initialization performs quite poorly in the classification task (as compared to
direct generative objective, Fig. 4). In particular, for the 100 sub-patterns case, where the projection
17
Published as a conference paper at ICLR 2021
HN performs best, the Hebbian curve trains very slowly (still not converged after 100 epochs) and
lags behind even the 10 sub-pattern Hebbian curve for most of the training. This emphasizes the
benefits of the projection rule HN over the Hebbian HN when the data is composed of correlated
patterns, which applies to most real-world datasets.
18