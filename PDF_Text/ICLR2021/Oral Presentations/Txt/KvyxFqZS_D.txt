Published as a conference paper at ICLR 2021
Global Convergence of Three-layer Neural
Networks in THE MEAN Field Regime* * * §
Huy Tuan Phamt and Phan-Minh Nguyen^ §
Ab stract
in the mean field regime, neural networks are appropriately scaled so that as the
width tends to infinity, the learning dynamics tends to a nonlinear and nontriv-
ial dynamical limit, known as the mean field limit. This lends a way to study
large-width neural networks via analyzing the mean field limit. Recent works have
successfully applied such analysis to two-layer networks and provided global con-
vergence guarantees. The extension to multilayer ones however has been a highly
challenging puzzle, and little is known about the optimization efficiency in the
mean field regime when there are more than two layers.
in this work, we prove a global convergence result for unregularized feedforward
three-layer networks in the mean field regime. We first develop a rigorous frame-
work to establish the mean field limit of three-layer networks under stochastic gra-
dient descent training. To that end, we propose the idea of a neuronal embedding,
which comprises of a fixed probability space that encapsulates neural networks of
arbitrary sizes. The identified mean field limit is then used to prove a global con-
vergence guarantee under suitable regularity and convergence mode assumptions,
which - unlike previous works on two-layer networks - does not rely critically
on convexity. Underlying the result is a universal approximation property, natural
of neural networks, which importantly is shown to hold at any finite training time
(not necessarily at convergence) via an algebraic topology argument.
1	Introduction
interests in the theoretical understanding of the training of neural networks have led to the recent
discovery of a new operating regime: the neural network and its learning rates are scaled appro-
priately, such that as the width tends to infinity, the network admits a limiting learning dynamics in
which all parameters evolve nonlinearly with time1. This is known as the mean field (MF) limit (Mei
et al. (2018); Chizat & Bach (2018); Rotskoff & Vanden-Eijnden (2018); Sirignano & Spiliopoulos
(2018); Nguyen (2019); Araujo et al. (2019); Sirignano & Spiliopoulos (2019)). The four works Mei
et al. (2018); Chizat & Bach (2018); Rotskoff & Vanden-Eijnden (2018); Sirignano & Spiliopoulos
(2018) led the first wave of efforts in 2018 and analyzed two-layer neural networks. They estab-
lished a connection between the network under training and its MF limit. They then used the MF
limit to prove that two-layer networks could be trained to find (near) global optima using variants
of gradient descent, despite non-convexity (Mei et al. (2018); Chizat & Bach (2018)). The MF limit
identified by these works assumes the form of gradient flows in the measure space, which factors
out the invariance from the action of a symmetry group on the model. interestingly, by lifting to
the measure space, with a convex loss function (e.g. squared loss), one obtains a limiting optimiza-
tion problem that is convex (Bengio et al. (2006); Bach (2017)). The analyses of Mei et al. (2018);
*This paper is a conference submission. We refer to the work Nguyen & Pham (2020) and its companion
note Pham & Nguyen (2020) for generalizations as well as other conditions for global convergence in the case
of multilayer neural networks.
,Department of Mathematics, Stanford University. This work was done in parts while H. T. Pham was at
the University of Cambridge.
^The Voleon Group. This work was done while P.-M. Nguyen was at Stanford University.
§ The author ordering is randomized.
1 This is to be contrasted with another major operating regime (the NTK regime) where parameters essen-
tially do not evolve and the model behaves like a kernel method (Jacot et al. (2018); Chizat et al. (2019); Du
et al. (2019); Allen-Zhu et al. (2019); Zou et al. (2018); Lee et al. (2019)).
1
Published as a conference paper at ICLR 2021
Chizat & Bach (2018) utilize convexity, although the mechanisms to attain global convergence in
these works are more sophisticated than the usual convex optimization setup in Euclidean spaces.
The extension to multilayer networks has enjoyed much less progresses. The works Nguyen (2019);
Araujo et al. (2019); Sirignano & SPiIioPoUIos (2019) argued, heuristically or rigorously, for the
existence of a MF limiting behavior under gradient descent training with different assumptions. In
fact, it has been argued that the difficulty is not simPly technical, but rather concePtual (Nguyen
(2019)): for instance, the Presence of intermediate layers exhibits multiPle symmetry grouPs with
intertwined actions on the model. Convergence to the global oPtimum of the model under gradient-
based oPtimization has not been established when there are more than two layers.
In this work, we Prove a global convergence guarantee for feedforward three-layer networks trained
with unregularized stochastic gradient descent (SGD) in the MF regime. After an introduction of
the three-layer setuP and its MF limit in Section 2, our develoPment Proceeds in two main stePs:
Step 1 (Theorem 3 in Section 3): We first develoP a rigorous framework that describes the MF
limit and establishes its connection with a large-width SGD-trained three-layer network. Here we
ProPose the new idea of a neuronal embedding, which comPrises of an aPProPriate non-evolving
Probability sPace that encaPsulates neural networks of arbitrary sizes. This Probability sPace is in
general abstract and is constructed according to the (not necessarily i.i.d.) initialization scheme of
the neural network. This idea addresses directly the intertwined action of multiPle symmetry grouPs,
which is the aforementioned concePtual obstacle (Nguyen (2019)), thereby covering setuPs that
cannot be handled by formulations in Araujo et al. (2019); Sirignano & SPilioPoulos (2019) (see also
Section 5 for a comParison). Our analysis follows the technique from Sznitman (1991); Mei et al.
(2018) and gives a quantitative statement: in Particular, the MF limit yields a good aPProximation
of the neural network as long as n-m1in log nmax 1 indePendent of the data dimension, where nmin
and nmax are the minimum and maximum of the widths.
Step 2 (Theorem 8 in Section 4): We Prove that the MF limit, given by our framework, con-
verges to the global oPtimum under suitable regularity and convergence mode assumPtions. Several
elements of our Proof are insPired by Chizat & Bach (2018); the technique in their work however
does not generalize to our three-layer setuP. Unlike Previous two-layer analyses, we do not exPloit
convexity; instead we make use of a new element: a universal aPProximation ProPerty. The result
turns out to be concePtually new: global convergence can be achieved even when the loss function
is non-convex. An imPortant crux of the Proof is to show that the universal aPProximation ProPerty
holds at any finite training time (but not necessarily at convergence, i.e. at infinite time, since the
ProPerty may not realistically hold at convergence).
Together these two results imPly a Positive statement on the oPtimization efficiency of SGD-trained
unregularized feedforward three-layer networks (Corollary 10). Our results can be extended to the
general multilayer case - with new ideas on top and significantly more technical works - or used to
obtain new global convergence guarantees in the two-layer case (Nguyen & Pham (2020); Pham &
Nguyen (2020)). We choose to keeP the current PaPer concise with the three-layer case being a Pro-
totyPical setuP that conveys several of the basic ideas. ComPlete Proofs are Presented in aPPendices.
Notations. K denotes a generic constant that may change from line to line. ∣∙∣ denotes the absolute
value for a scalar and the Euclidean norm for a vector. For an integer n, we let [n] = {1, ..., n}.
2	Three-layer neural networks and the mean field limit
2.1	Three-layer neural network
We consider the following three-layer network at time k ∈ N≥0 that takes as inPut x ∈ Rd:
y(x; W (k)) = Ψ3 (H3 (x; W (k))),	(1)
1 n2
H3 (x; W (k)) = X w3 (k, j2)中2 (H2 (x, j2； W (k))),
n2
j2=1
2
Published as a conference paper at ICLR 2021
1 n1
H2 (x, j2； W (k))=	X W2 (k, jl, j2)2 1 (hwι (k,jι) ,Xi,
n1
j1 =1
in which W (k) = (wι (k, ∙), W2 (k, ∙, ∙), W (k, ∙)) consists of the weights2 wι (k,jι) ∈ Rd,
w2 (k,j1,j2) ∈ R and w3 (k,j2) ∈ R. Here 21 : R → R,22 : R → R and 23 : R → R are the
activation functions, and the network has widths {n1, n2}.
We train the network with SGD w.r.t. the loss L : R × R → R≥0. We assume that at each time k,
we draw independently a fresh sample z (k) = (x (k) , y (k)) ∈ Rd × R from a training distribution
P. Given an initialization W (0), we update W (k) according to
w3 (k + 1,j2) = w3 (k,j2) - ξ3 (k) Grad3 (z (k) ,j2;W (k)) ,
w2 (k + 1,j1,j2) = w2 (k, j1, j2) - ξ2 (k) Grad2 (z (k) ,j1,j2;W (k)) ,
w1 (k + 1,j1) = w1 (k,j1) - ξ1 (k) Grad1 (z (k) ,j1;W (k)) ,
in which j1 = 1, ..., n1, j2 = 1, ..., n2, ∈ R>0 is the learning rate, ξi : R≥0 7→ R≥0 is the learning
rate schedule for wi, and for z = (x, y), we define
Grad3 (z,j2; W (k)) = ∂2L (y, y(x; W (k)))夕3 (H (x; W (k)))夕2(H2(x,j2; W (k))),
Grad2 (z,j1,j2; W (k)) = ∆H (z, j2; W (k))2 1 (hwι (k, jι) ,xi),
Gradi (z,ji； W (k)) = (11^ X ∆H (z, j2; W (k)) W (k,j1,j2))P1 (〈wi (k,jι) ,x〉)x,
∆H (z, j2; W (k)) = ∂2L (y, y(x; W (k)))q3 (H3 (x; W (k))) W (k, j2)q2 (H (x, j2; W (k))).
We note that this setup follows the same scaling w.r.t. n1 and n2, which is applied to both the
forward pass and the learning rates in the backward pass, as Nguyen (2019).
2.2 Mean field limit
The MF limit is a continuous-time infinite-width analog of the neural network under training. To
describe it, we first introduce the concept ofa neuronal ensemble. Given a product probability space
(Ω, F, P) = (Ωι X Ω2, Fi X F1,P1 X P2), we independently sample Ci 〜Pi, i = 1,2. In the
following, We use ECi to denote the expectation w.r.t. the random variable Ci 〜 Pi and Ci to denote
an arbitrary point Ci ∈ Ωi. The space (Ω, F, P) is referred to as a neuronal ensemble.
Given a neuronal ensemble (Ω, F, P), the MF limit is described by a time-evolving system with
state/parameter W (t), where the time t ∈ R≥o and W (t) = (wi (t, ∙), w2 (t, ∙, ∙), w3 (t, ∙)) with
wi : R≥o x Ωι → Rd, w2 : R≥o x Ωι X Ω2 → R and w3 : R≥0 X Ω2 → R. It entails the
quantities:
y (x; W (t))	= Ψ3 (H3 (x; W (t))),
H3 (x; W (t))	= EC2 [w3 (t, C2)的(H (x, C2; W (t)))],
H (x, C2； W (t))	= Eci [w2 (t, C1,c2) 0 (hwι (t, Ci), Xi)].
Here for each t ∈ R≥o, Wi (t, ∙) is (Ωι, Fi)-measurable, and similar for w2 (t, ∙, ∙), w3 (t, ∙).
The MF	limit	evolves according	to a continuous-time dynamics, described	by	a	system	of
ODEs,	which	we refer to as the	MF ODEs. Specifically, given an initialization	W (0)	=
(wi (0, ∙), w2 (0, ∙, ∙), w3 (0, ∙)), the dynamics solves:
∂tw3 (t,C2) = -ξ3 (t) ∆3 (C2; W (t)) ,
∂tw2 (t,Ci,C2) = -ξ2 (t) ∆2 (Ci, C2; W (t)) ,
∂twi (t,ci) = -ξi (t)∆i (ci; W (t)).
Here Ci ∈ Ωi, c2 ∈ Ω2, EZ denotes the expectation w.r.t. the data Z = (X, Y)〜P, and for
z = (x, y), we define
△3 (C2； W (t)) = EZ [∂2L (Y, y (X; W (t)))夕 3 (H (X; W (t)))夕 2 (H (X, C2； W (t)))],
2To absorb first layer’s bias term to w1, we assume the input x to have 1 appended to the last entry.
3
Published as a conference paper at ICLR 2021
∆ (ci, C2； W (t)) = EZ [∆H (Z, C2； W (t))中 1 (hwι (t, ci) ,Xi)],
∆ι (ci; W (t)) = EZ [Ec2 [∆H (Z, C2; W (t)) w2 (t, c1,C2)] H (hwi (t, ci), Xi) X],
∆H (z, C2； W (t)) = ∂2L (y, y (x; W (t))) φ,3 (H3 (x; W (t))) W3 (t, c2) H (H (x, c2; W (t))).
In Appendix B, we show well-posedness of MF ODEs under the following regularity conditions.
Assumption 1 (Regularity). We assume that Hi and H2 are K -bounded, H0i, H02 and H03 are K-
bounded and K -Lipschitz, H and H are non-zero everywhere, ∂2L (∙, ∙) is K-Lipschitz in the
second variable and K -bounded, and |X| ≤ K with probability 1. Furthermore ξi, ξ2 and ξ3 are
K -bounded and K -Lipschitz.
Theorem 1. Under Assumption 1, given any neuronal ensemble and an initialization W (0) such
that3 ess-sup |w2 (0, Ci, C2)| , ess-sup |w3 (0, C2)| ≤ K, there exists a unique solution W to the
MF ODEs on t ∈ [0, ∞).
An example ofa suitable setup is Hi = H2 = tanh, H3 is the identity, L is the Huber loss, although a
non-convex sufficiently smooth loss function suffices. In fact, all of our developments can be easily
modified to treat the squared loss with an additional assumption |Y | ≤ K with probability 1.
So far, given an arbitrary neuronal ensemble (Ω, F, P), for each initialization W (0), We have de-
fined a MF limit W (t). The connection with the neural network’s dynamics W (k) is established in
the next section.
3 Connection between neural network and its mean field limit
3.1	Neuronal embedding and the coupling procedure
To formalize a connection betWeen the neural netWork and its MF limit, We consider their initializa-
tions. In practical scenarios, to set the initial parameters W (0) of the neural netWork, one typically
randomizes W (0) according to some distributional laW ρ. We note that since the neural netWork
is defined W.r.t. a set of finite integers {ni, n2}, so is ρ. We consider a family Init of initialization
laWs, each of Which is indexed by the set {ni, n2}:
Init = {ρ : ρ is the initialization laW ofa neural netWork of size {ni, n2} , ni, n2 ∈ N>0}.
This is helpful When one is to take a limit that sends ni , n2 → ∞, in Which case the size of this
family |Init| is infinite. More generally We alloW |Init| < ∞ (for example, Init contains a single laW
ρ ofa netWork of size {ni, n2} and hence |Init| = 1). We make the folloWing crucial definition.
Definition 2. Given a family of initialization laws Init, we call (Ω, F, P, {w0卜=[② 3) a neuronal
embedding of Init if the folloWing holds:
1.	(Ω,F,P) = (Ωι X Ω2,Fi X F2,P1 X P2) a product measurable space. As a reminder,
we call it a neuronal ensemble.
2.	The deterministic functions w0 : Ωι → Rd, w0 : Ωι X Ω2 → R and w0 : Ω2 → R are
such that, for each index {ni, n2} of Init and the law ρ of this index, if— with an abuse of
notations —— we independently sample {Ci (ji)} j∈[n.]〜 Pi i.i.d. for each i = 1,2, then
LaW (w0 (Ci (jι)), w0 (Ci(ji), C2 (j2)), w0 (C2(j2)), ji ∈ [ni], i = 1, 2)= ρ.
To proceed, given Init and {ni, n2} in its index set, we perform the following coupling procedure:
1.	Let (Ω, F, P, {w0 },=1 2 3) be a neuronal embedding of Init.
2.	We form the MF limit W (t) (for t ∈ R≥0) associated with the neuronal ensemble
(Ω, F, P) by setting the initialization W (0) to Wi (0, ∙) = w0 (∙), w2 (0, ∙, ∙) = w0 (∙, ∙)
and w3 (0, ∙) = w0 (∙) and running the MF ODES described in Section 2.2.
3We recall the definition of ess-sup in Appendix A.
4
Published as a conference paper at ICLR 2021
3.	We independently sample Ci (ji)〜 Pi for i = 1, 2 and ji = 1,..., ni. We then form
the neural network initialization W (0) with w1 (0, j1) = w10 (C1 (j1)), w2 (0,j1,j2) =
w20 (C1 (j1) , C2 (j2)) and w3 (0,j2) = w30 (C2 (j2)) for j1 ∈ [n1], j2 ∈ [n2]. We obtain
the network’s trajectory W (k) for k ∈ N≥0 as in Section 2.1, with the data z (k) generated
independently of {Ci (ji)}i=1,2 and hence W (0).
We can then define a measure of closeness between W (bt/c) and W (t) for t ∈ [0, T]:
DT(W,W) =sup|w1(bt/c,j1)-w1(t,C1(j1))|, |w2(bt/c,j1,j2)-w2(t,C1(j1),C2(j2))|,
∣W3 ([t/e] , j2) - w3 (t, C2 (j2))∣ ： t ≤ T, ji ≤ nι, j2 ≤ n2}.	(2)
Note that W (t) is a deterministic trajectory independent of {n1, n2}, whereas W (k) is random for
all k ∈ N≥0 due to the randomness of {Ci (ji)}i=1,2 and the generation of the training data z (k).
Similarly DT (W, W) is a random quantity.
The idea of the coupling procedure is closely related to the coupling argument in Sznitman (1991);
Mei et al. (2018). Here, instead of playing the role of a proof technique, the coupling serves as a
vehicle to establish the connection between W and W on the basis of the neuronal embedding. This
connection is shown in Theorem 3 below, which gives an upper bound on DT (W, W).
We note that the coupling procedure can be carried out to provide a connection between W and
W as long as there exists a neuronal embedding for Init. Later in Section 4.1, we show that for
a common initialization scheme (in particular, i.i.d. initialization) for Init, there exists a neuronal
embedding. Theorem 3 applies to, but is not restricted to, this initialization scheme.
3.2 Main result: approximation by the MF limit
Assumption 2 (Initialization of second and third layers). We assume that ess-sup w20 (C1, C2),
ess-sup w30 (C2) ≤ K, where w20 and w30 are as described in Definition 2.
Theorem 3. Given a family Init of initialization laws and a tuple {n1, n2} that is in the index set
of Init, perform the coupling procedure as described in Section 3.1. Fix a terminal time T ∈ eN≥0.
Under Assumptions 1 and 2, for e ≤ 1, we have with probability at least 1 - 2δ,
Dt (W, W) ≤ eKT (√^ + √e) log1/2 (3 (T + /max + e) ≡ errδ,τ (e, n1,n2),
in which nmin = min {n1, n2}, nmax = max {n1, n2}, and KT = K 1 + TK .
The theorem gives a connection between W (bt/ec), which is defined upon finite widths n1 and n2,
and the MF limit W (t), whose description is independent of n1 and n2 . It lends a way to extract
properties of the neural network in the large-width regime.
Corollary 4. Under the same setting as Theorem 3, consider any test function ψ : R × R → R
which is K -Lipschitz in the second variable uniformly in the first variable (an example of ψ is the
loss L). For anyδ > 0, with probability at least 1 - 3δ,
SUp |Ez [ψ (Y, y (X; W (bt/eC)))] - EZ [ψ (Y, y (X; W (t))川 ≤ eKTerrδ,τ (e,n1,n2).
t≤T
These bounds hold for any ni and n, similar to Mei et al. (2018); Araujo et al. (2019), in contrast
with non-quantitative results in Chizat & Bach (2018); Sirignano & Spiliopoulos (2019). These
bounds suggest that n1 and n2 can be chosen independent of the data dimension d. This agrees with
the experiments in Nguyen (2019), which found width ≈ 1000 to be typically sufficient to observe
MF behaviors in networks trained with real-life high-dimensional data.
We observe that the MF trajectory W (t) is defined as per the choice of the neuronal embedding
(Ω, F ,P, {w0 }i=i 23), which may not be unique. On the other hand, the neural network,s trajectory
W (k) depends on the randomization of the initial parameters W (0) according to an initialization
law from the family Init (as well as the data z (k)) and hence is independent of this choice. Another
corollary of Theorem 3 is that given the same family Init, the law of the MF trajectory is insensitive
to the choice of the neuronal embedding of Init.
5
Published as a conference paper at ICLR 2021
Corollary 5. Consider a family Init of initialization laws, indexed by a set of tuples {m1, m2}
that contains a sequence of indices {m1 (m) , m2 (m) : m ∈ N} in which as m → ∞,
min {mι (m), m2 (m)}-1 log (max {mι (m), m2 (m)}) → 0. Let W (t) and W (t) be two MF
trajectories associated with two choices ofneuronal embeddings of Init, (Ω, F, P, {w0 卜=12 3) and
(Ω, F,P, {W0 }i=ι 2 3). Thefollowing statement holdsfor any T ≥ 0 and any two positive integers
nι and n: if we independently sample Ci (ji)〜 Pi and Ci (ji)〜 Pi for ji ∈ [n∕, i = 1, 2,
then Law (W (n1, n2, T)) = Law(W (n1, n2, T)), where we define W (n1, n2, T) as the below
collection w.r.t. W (t), and similarly define W (n1, n2, T) w.r.t. W (t):
W (n1, n2,	T)	=	w1	(t, C1	(j1))	,	w2 (t,	C1	(j1) , C2 (j2)) , w3	(t, C2 (j2)) :
j1 ∈ [n1] , j2 ∈ [n2] , t ∈ [0, T] }.
The proofs are deferred to Appendix C.
4 Convergence to global optima
In this section, we prove a global convergence guarantee for three-layer neural networks via the MF
limit. We consider a common class of initialization: i.i.d. initialization.
4.1	I.i.d. initialization
Definition 6. An initialization law ρ for a neural network of size {n1, n2} is called ρ1, ρ2, ρ3 -
i.i.d. initialization (or i.i.d. initialization, for brevity), where ρ1, ρ2 and ρ3 are probability mea-
sures over Rd, R and R respectively, if {w1 (0, j1)}j ∈[n ] are generated i.i.d. according to ρ1,
{w2 (0,j1,j2)}j1∈[n1],j2∈[n2] are generated i.i.d. according to ρ2 and {w3 (0,j2)}j2∈[n2] are gen-
erated i.i.d. according to ρ3, and w1, w2 and w3 are independent.
Observe that given ρ1, ρ2, ρ3 , one can build a family Init of i.i.d. initialization laws that contains
any index set {n1, n2}. Furthermore i.i.d. initializations are supported by our framework, as stated
in the following proposition and proven in Appendix D.
Proposition 7. There exists a neuronal embedding (ω, F, P, {w?},=[② 3) for any family Init of
initialization laws, which are ρ1 , ρ2, ρ3 -i.i.d.
4.2	Main result: global convergence
To measure the learning quality, We consider the loss averaged over the data Z ~ P:
L (V) = EZ [L (Y,y(X; V))],
where V = (vι, v2,v3) is a set of three measurable functions vι : Ω[ → Rd, v : Ω[ X Ω2 → R,
V3 : Ω2 → R.
Assumption 3. Consider a neuronal embedding (ω, F, P, {w0}i=[② 3) of the (ρ1,ρ2,ρ3) -i.i.d.
initialization, and the associated MF limit with initialization W (0) such that wι (0, ∙) = w0 (∙),
w2 (0, ∙, ∙) = w0 (∙, ∙) and w3 (0, ∙) = w0 (∙). Assume:
1.	Support: The support ofρ1 is Rd.
2.	Convergence mode: There exist limits W[, W2 and W3 such that as t → ∞,
E [(1 + ∣W3(C2)∣)	∣W3(C2)∣∣W2(C1,C2)∣∣w1(t,C1)- W1(C1)∣]	→	0,	(3)
E [(1	+ ∣W3(C2)∣) ∣W3(C2)∣∣w2(t,C1,C2)- W2(C1,C2)∣]	→	0,	(4)
E [(1 + ∣W3(C2)∣) ∣w3(t,C2) - W3(C2)∣]	→	0,	(5)
ess - supEc2 [∣∂tw2 (t,C1,C2)∣]	→	0.	(6)
6
Published as a conference paper at ICLR 2021
3.	Universal approximation: {夕1 ((u, •)) ： U ∈ Rd} has dense span in L2 (PX) (the space
of square integrable functions w.r.t. PX the distribution of the input X).
Assumption 3 is inspired by the work Chizat & Bach (2018) on two-layer networks, with certain
differences. Assumptions 3.1 and 3.3 are natural in neural network learning (Cybenko (1989); Chen
& Chen (1995)), while we note Chizat & Bach (2018) does not utilize universal approximation.
Similar to Chizat & Bach (2018), Assumption 3.2 is technical and does not seem removable. Note
that this assumption specifies the mode of convergence and is not an assumption on the limits Wι, W2
and W3. Specifically conditions (3)-(5) are similar to the convergence assumption in Chizat & Bach
(2018). We differ from Chizat & Bach (2018) fundamentally in the essential supremum condition
(6). On one hand, this condition helps avoid the Morse-Sard type condition in Chizat & Bach (2018),
which is difficult to verify in general and not simple to generalize to the three-layer case. On the
other hand, it turns out to be a natural assumption to make, in light of Remark 9 below.
We now state the main result of the section. The proof is in Appendix D.
Theorem 8. Consider a neuronal embedding (ω, F, P, {w0}i=123) of (ρ1, ρ2, ρ3)-i.i.d. initial-
ization. Consider the MF limit corresponding to the network (1), such that they are coupled together
by the coupling procedure in Section 3.1, under Assumptions 1, 2 and 3. For simplicity, assume
ξι (∙) = ξ (∙) = 1. Further assume either:
•	(untrained third layer) ξ3 (∙) = 0 and w0 (C2) = 0 with a positive probability, or
•	(trained third layer) ξ3 (∙) = 1 andL (w0, w0, wɜ) < EZ [L (Y, 夕3 (0))].
Then the following hold:
•	Case 1 (convex loss): If L is convex in the second variable, then
lim L (W (t)) = inf L (V) = inf EZ [L (Y, y (X))].
t→∞	v	y： Rd→R
•	Case 2 (generic non-negative loss): Suppose that ∂2L (y, y) = 0 implies L (y, y) = 0. If
y = y(x) is a function ofx, then L (W (t)) → 0 as t → ∞.
Remarkably here the theorem allows for non-convex losses. A further inspection of the proof shows
that no convexity-based property is used in Case 2 (see, for instance, the high-level proof sketch
in Section 4.3); in Case 1, the key steps in the proof are the same, and the convexity of the loss
function serves as a convenient technical assumption to handle the arbitrary extra randomness of Y
conditional on X . We also remark that the same proof of global convergence should extend beyond
the specific fully-connected architecture considered here. Similar to previous results on SGD-trained
two-layer networks Mei et al. (2018); Chizat & Bach (2018), our current result in the three-layer case
is non-quantitative.
Remark 9. Interestingly there is a converse relation between global convergence and the essential
supremum condition (6): under the same setting, global convergence is unattainable if condition
(6) does not hold. A similar observation was made in Wojtowytsch (2020) for two-layer ReLU
networks. A precise statement and its proof can be found in Appendix E.
The following result is straightforward from Theorem 8 and Corollary 4, establishing the optimiza-
tion efficiency of the neural network with SGD.
Corollary 10. Consider the neural network (1). Under the same setting as Theorem 8, in Case 1,
lim lim lim EZ [L (Y, y(X; W ([〃4)))] = , i?f, L (fι,f2,f3)=inf Ez [L (Y,y(X))]
t→∞ n1,n2 e→0	f1,f2,f3	y
in probability, where the limit of the widths is such that min {n1, n2}-1 log (max {n1, n2}) → 0.
In Case 2, the same holds with the right-hand side being 0.
4.3 High-level idea of the proof
We give a high-level discussion of the proof. This is meant to provide intuitions and explain the
technical crux, so our discussion may simplify and deviate from the actual proof.
7
Published as a conference paper at ICLR 2021
Our first insight is to look at the second layer’s weight w2 . At convergence time t = ∞, we expect
to have zero movement and hence, denoting W (∞) = (Wι, W2,W3):
△2 (c1,c2; W (∞)) = EZ [∆H (Z, C2； W (∞)) φι (hwι (ci) ,X i)] = 0,
for P -almost every c1, c2. Suppose for the moment that we are allowed to make an additional
(strong) assumption on the limit Wi： SuPP(WI (Ci)) = Rd. It implies that the universal ap-
proximation property, described in Assumption 3, holds at t = ∞; more specifically, it implies
{夕1 (〈Wi (ci), •))： ci ∈ Ωι} has dense span in L2 (PX). This thus yields
EZ [∆H (Z,c2; W (∞))∣X = x] =0,
for P-almost every x. Recalling the definition of △2H, one can then easily show that
EZ [∂2L (Y, y (X; W (∞)))∣X = x] =0.
Global convergence follows immediately; for example, in Case 2 of Theorem 8, this is equivalent to
that ∂2L (y (x), y (x; W (∞))) = 0 and hence L (y (x), y (x; W (∞))) = 0 for P-almost every x.
In short, the gradient flow structure of the dynamics ofW2 provides a seamless way to obtain global
convergence. Furthermore there is no critical reliance on convexity.
However this plan of attack has a potential flaw in the strong assumption that SuPP (Wi (Ci))=
Rd, i.e. the universal approximation property holds at convergence time. Indeed there are setups
where it is desirable that SuPP (WI (Ci)) = Rd (Mei et al. (2018); Chizat (2019)); for instance,
it is the case where the neural network is to learn some “sparse and spiky” solution, and hence
the weight distribution at convergence time, if successfully trained, cannot have full support. On
the other hand, one can entirely expect that if SuPP (Wi (0, Ci )) = Rd initially at t = 0, then
SuPP (Wi (t, Ci )) = Rd at any finite t ≥ 0. The crux of our proof is to show the latter without
assuming SuPP (Wi (Ci)) = Rd.
This task is the more major technical step of the proof. To that end, we first show that there exists
a mapping (t, u) 7→ M (t, u) that maps from (t, Wi (0, ci)) = (t, u) to Wi (t, ci) via a careful
measurability argument. This argument rests on a scheme that exploits the symmetry in the network
evolution. Furthermore the map M is shown to be continuous. The desired conclusion then follows
from an algebraic topology argument that the map M preserves a homotopic structure through time.
5 Discussion
The MF literature is fairly recent. A long line of works (Nitanda & Suzuki (2017); Mei et al. (2018);
Chizat & Bach (2018); Rotskoff & Vanden-Eijnden (2018); Sirignano & Spiliopoulos (2018); Wei
et al. (2019); Javanmard et al. (2019); Mei et al. (2019); Shevchenko & Mondelli (2019); Woj-
towytsch (2020)) have focused mainly on two-layer neural networks, taking an interacting particle
system approach to describe the MF limiting dynamics as Wasserstein gradient flows. The three
works Nguyen (2019); Araujo et al. (2019); Sirignano & Spiliopoulos (2019) independently develop
different formulations for the MF limit in multilayer neural networks, under different assumptions.
These works take perspectives that are different from ours. In particular, while the central object
in Nguyen (2019) is a new abstract representation of each individual neuron, our neuronal embed-
ding idea instead takes a keen view on a whole ensemble of neurons. Likewise our idea is also
distant from Araujo et al. (2019); Sirignano & Spiliopoulos (2019): the central objects in Araujo
et al. (2019) are paths over the weights across layers; those in Sirignano & Spiliopoulos (2019) are
time-dependent functions of the initialization, which are simplified upon i.i.d. initializations.
The result of our perspective is a neuronal embedding framework that allows one to describe the MF
limit in a clean and rigorous manner. In particular, it avoids extra assumptions made in Araujo et al.
(2019); Sirignano & Spiliopoulos (2019): unlike our work, Araujo et al. (2019) assumes untrained
first and last layers and requires non-trivial technical tools; Sirignano & Spiliopoulos (2019) takes
an unnatural sequential limit ni → ∞ before n2 → ∞ and proves a non-quantitative result, unlike
Theorem 3 which only requires sufficiently large min {ni, n2}. We note that Theorem 3 can be
extended to general multilayer networks using the neuronal embedding idea. The advantages of
our framework come from the fact that while MF formulations in Araujo et al. (2019); Sirignano
& Spiliopoulos (2019) are specific to and exploit i.i.d. initializations, our formulation does not.
Remarkably as shown in Araujo et al. (2019), when there are more than three layers and no biases,
8
Published as a conference paper at ICLR 2021
i.i.d. initializations lead to a certain simplifying effect on the MF limit. On the other hand, our
framework supports non-i.i.d. initializations which avoid the simplifying effect, as long as there
exist suitable neuronal embeddings (Nguyen & Pham (2020)). Although our global convergence
result in Theorem 8 is proven in the context of i.i.d. initializations for three-layer networks, in the
general multilayer case, it turns out that the use of a special type of non-i.i.d. initialization allows
one to prove a global convergence guarantee (Pham & Nguyen (2020)).
In this aspect, our framework follows closely the spirit of the work Nguyen (2019), whose MF
formulation is also not specific to i.i.d. initializations. Yet though similar in the spirit, Nguyen
(2019) develops a heuristic formalism and does not prove global convergence.
Global convergence in the two-layer case with convex losses has enjoyed multiple efforts with a lot
of new and interesting results (Mei et al. (2018); Chizat & Bach (2018); Javanmard et al. (2019);
Rotskoff et al. (2019); Wei et al. (2019)). Our work is the first to establish a global convergence guar-
antee for SGD-trained three-layer networks in the MF regime. Our proof sends a new message that
the crucial factor is not necessarily convexity, but rather that the whole learning trajectory maintains
the universal approximation property of the function class represented by the first layer’s neurons,
together with the gradient flow structure of the second layer’s weights. As a remark, our approach
can also be applied to prove a similar global convergence guarantee for two-layer networks, remov-
ing the convex loss assumption in previous works (Nguyen & Pham (2020)). The recent work Lu
et al. (2020) on a MF resnet model (a composition of many two-layer MF networks) and a recent
update of Sirignano & Spiliopoulos (2019) essentially establish conditions of stationary points to be
global optima. They however require strong assumptions on the support of the limit point. As ex-
plained in Section 4.3, we analyze the training dynamics without such assumption and in fact allow
it to be violated.
Our global convergence result is non-quantitative. An important, highly challenging future direction
is to develop a quantitative version of global convergence; previous works on two-layer networks
Javanmard et al. (2019); Wei et al. (2019); Rotskoff et al. (2019); Chizat (2019) have done so under
sophisticated modifications of the architecture and training algorithms.
Finally we remark that our insights here can be applied to prove similar global convergence guaran-
tees and derive other sufficient conditions for global convergence of two-layer or multilayer networks
(Nguyen & Pham (2020); Pham & Nguyen (2020)).
Acknowledgement
H. T. Pham would like to thank Jan Vondrak for many helpful discussions and in particular for the
shorter proof of Lemma 19. We would like to thank Andrea Montanari for the succinct description
of the difficulty in extending the mean field formulation to the multilayer case, in that there are
multiple symmetry group actions in a multilayer network.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the 36th International Conference on Machine Learning,
volume 97,pp. 242-252, 2019.1
Dyego Araujo, Roberto I Oliveira, and Daniel Yukimura. A mean-field limit for certain deep neural
networks. arXiv preprint arXiv:1906.00193, 2019. 1, 1, 3.2, 5
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629-681, 2017. 1
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in neural information processing systems, pp. 123-130, 2006. 1
Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks
with arbitrary activation functions and its application to dynamical systems. IEEE Transactions
on Neural Networks, 6(4):911-917, 1995. 4.2
9
Published as a conference paper at ICLR 2021
Lenaic Chizat. Sparse optimization on measures with over-parameterized gradient descent. arXiv
preprint arXiv:1907.10300, 2019. 4.3, 5
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in Neural Information Processing
Systems,pp. 3040-3050. 2018. 1,1, 3.2, 4.2, 4.2, 5, D.2
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems 32, pp. 2937-2947. 2019. 1
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989. 4.2
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=S1eK3i09YQ. 1
Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Ad-
vances in Neural Information Processing Systems, pp. 9747-9757, 2018. F
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems 31, pp.
8580-8589. 2018. 1
Adel Javanmard, Marco Mondelli, and Andrea Montanari. Analysis of a two-layer neural network
via displacement convexity. arXiv preprint arXiv:1901.01375, 2019. 5
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems 32, pp. 8572-
8583. 2019. 1
Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean-field analysis of deep
resnet and beyond: Towards provable optimization via overparameterization from depth. arXiv
preprint arXiv:2003.05508, 2020. 5
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layers neural networks. In Proceedings of the National Academy of Sciences, volume 115, pp.
7665-7671, 2018. 1, 1, 3.1, 3.2, 4.2, 4.3, 5
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019. 5
Phan-Minh Nguyen. Mean field limit of the learning dynamics of multilayer neural networks. arXiv
preprint arXiv:1902.02880, 2019. 1, 1, 2.1, 3.2, 5
Phan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multi-
layer neural networks. arXiv preprint arXiv:2001.11443, 2020. *, 1, 5
Atsushi Nitanda and Taiji Suzuki. Stochastic particle gradient descent for infinite ensembles. arXiv
preprint arXiv:1712.05438, 2017. 5
Huy Tuan Pham and Phan-Minh Nguyen. A note on the global convergence of multilayer neural
networks in the mean field regime. arXivpreprint arXiv:2006.09355, 2020. *,1, 5
Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of
Probability, 22(4):1679-1706, 1994. F
Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence
and asymptotic error scaling of neural networks. In Advances in Neural Information Processing
Systems 31, pp. 7146-7155. 2018. 1, 5
Grant Rotskoff, Samy Jelassi, Joan Bruna, and Eric Vanden-Eijnden. Neuron birth-death dynamics
accelerates gradient descent and converges asymptotically. In Proceedings of the 36th Interna-
tional Conference on Machine Learning, volume 97, pp. 5508-5517, 2019. 5
10
Published as a conference paper at ICLR 2021
Alexander Shevchenko and Marco Mondelli. Landscape connectivity and dropout stability of sgd
solutions for over-parameterized neural networks. arXiv preprint arXiv:1912.10095, 2019. 5
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv
preprint arXiv:1805.01053, 2018. 1, 5
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of deep neural networks. arXiv
preprint arXiv:1903.04440, 2019. 1, 1, 3.2, 5
Alain-Sol Sznitman. Topics in propagation of chaos. In Ecole d,ete de ProbabilitGs de Saint-Flour
XIX—1989, pp. 165—251. SPringer,1991. 1, 3.1
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and op-
timization of neural nets v.s. their induced kernel. In Advances in Neural Information Processing
Systems 32,pp. 9712-9724. 2019. 5
Stephan Wojtowytsch. On the convergence of gradient descent training for two-layer relu-networks
in the mean field regime. arXiv PrePrint arXiv:2005.13530, 2020. 9, 5
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv PrePrint arXiv:1811.08888, 2018. 1
11
Published as a conference paper at ICLR 2021
A Notational preliminaries
For a real-valued random variable Z defined on a probability space (Ω, F, P), we recall
ess-supZ = inf {z ∈ R : P (Z > z) = 0} .
We also introduce some convenient definitions which we use throughout the appendices. For a set
of neural network’s parameter W, we define
9W9T = max	max	sup |w2 (bt/ec ,j1,j2)| , max sup |w3 (bt/ec ,j2)| .
j1≤n1, j2≤n2 t≤T	j2≤n2 t≤T
Similarly for a set ofMF parameters W, we define:
9W 9T = max ess-sup sup |w2 (t, C1, C2)| , ess-sup sup |w3 (t, C2)| .
t≤T	t≤T
For two sets of neural network’s parameters W0, W00, we define their distance:
kW0 - W00kT =sup|w10	(bt/c, j1)	- w010	(bt/c, j1)|, |w20	(bt/c,j1,j2)-w020(bt/c,j1,j2)|,
|w3 (bt/ec , j2) - w3z (bt/ec , j2)1 : t ∈ [0, T], jι ∈ [nι], j2 ∈ [n2] }∙
Similarly for two sets of MF parameters W0, W00, we define their distance:
kW0 - W00kT = ess-sup sup n |w10 (t,C1) - w100 (t, C1)|, |w20 (t,C1,C2) - w200 (t, C1, C2)|,
t∈[0,T]
|w30 (t, C2 ) - w300 (t, C2 )| o.
B	Existence and uniquenes s of the s olution to MF ODEs
We first collect some a priori estimates.
Lemma 11. Under Assumption 1, consider a solution W to the MF ODEs with initialization W (0)
such that 9W 90 < ∞. If this solution exists, it satisfies the following a priori bounds, for any
T≥0:
ess-sup sup |w3 (t,C2)| ≤ 9W 90 +KT ≡ 9W 90 +K0,3 (T) ,
ess-sup sup |w2 (t, C1, C2)| ≤ 9W 90 +K T K0,3 (T) ≡ 9W 90 +K0,2 (T) ,
t≤T
and consequently, 9W 9T ≤ 1 + max {K0,2 (T) , K0,3 (T)} .
Proof. The bounds can be obtained easily by bounding the respective initializations and update
quantities separately. In particular,
∂
ess-sup sup ∣w3 (t,C2)∣ ≤ ess-sup ∣w3 (0, C2)∣ + T ess-sup sup — w3 (t, C2) ≤ 9 W90 +KT,
t≤T	t≤T ∂t
∂
ess-sup sup ∣w2 (t, Ci, C2)∣ ≤ ess-sup ∣w2 (0, Ci, C2)∣ + T ess-sup sup —w2 (t, Ci, C2)
t≤T	t≤T ∂t
≤ ess-sup |w2 (0, Ci , C2 )| + K T ess-sup sup |w3 (t, C2)|
t≤T
≤ 9W 90 +KTK0,3 (T) .
□
Inspired by the a priori bounds in Lemma 11, given an arbitrary terminal time T and the initialization
W (0), let us consider:
12
Published as a conference paper at ICLR 2021
•	for	a	tuple (a, b)	∈ R2≥0, a space WT	(a, b)	of W0	=	(W0 (t))t≤T	=
(w1	(t,	∙), w2	(t, ∙, ∙), w3	(t, ∙))t≤τ such that
ess-sup sup |w30 (t, C2)| ≤ b,
t≤T
ess-sup sup |w20 (t, C1, C2)| ≤ a,
t≤T
where wj :	R≥o	X Ω1 → Rd, w2 : R≥o X Ω1 X Ω2 → R,	w∖ :	R≥o	X Ω3 → R,
•	for a tuple	(a, b)	∈ R2≥0 and W (0), a space WT+ (a, b, W	(0))	of W	0 ∈ WT (a, b)	such
that W 0 (0) = W (0) additionally (and hence every W 0 in this space shares the same
initialization W (0)).
We equip the spaces with the metric kW 0 - W 00kT. It is easy to see that both spaces are complete.
Note that Lemma 11 implies, under Assumption 1 and 9W 90 < ∞, we have any MF solution W,
if exists, is in WT (9W 90 +K0,2 (T) , 9W 90 +K0,3 (T)). For the proof of Theorem 1, we work
mainly with WT+ (9W 90 +K0,2 (T) , 9W 90 +K0,3 (T) , W (0)), although several intermediate
lemmas are proven in more generality for other uses.
Lemma 12. Under Assumption 1, for T ≥ 0, any W0, W00 ∈ WT (a, b) and almost ^very Z 〜P:
ess-sup sup ∆2H (z,C2; W 0 (t)) ≤ Ka,b,
ess-sup sup |H2 (x, C2; W 0 (t)) -H2(x,C2;W00(t))| ≤Ka,bkW0-W00kT,
sup|H3(x;W0(t)) -H3(x;W00(t))| ≤Ka,bkW0-W00kT,
sup ∣∂2L (y, y (x; W0 (t))) - ∂2L (y, y (x; W00 (t)))∣ ≤ Ka,b kW0 - W00kτ ,
ess - supsup ∣∆H (z, C2; W0 (t)) - ∆ (z, C2； W00 (t))∣ ≤ K0,b ∣∣W0 - W00∣∣t ,
where Ka,b ≥ 1 is a generic constant that grows polynomially with a and b.
Proof. The first bound is easy to see:
ess-sup sup ∣∆2H (z, C2; W0 (t))∣ ≤ ess-sup sup |w30 (t, C2)| ≤ b.
t≤T	t≤T
We prove the second bound, invoking Assumption 1:
|H2(x,C2;W0(t)) -H2(x,C2;W00(t))|
≤ K |w2 (t,C1 ,C2)| |?1 (hw1 (t,C1) , Xi)- Ψ1 (hw1' (t,C1) , Xi)I
+K|w20 (t,C1,C2) -w200(t,C1,C2)|
≤ K (|w20 (t,C1,C2)|+1)kW0-W00kT,
which yields by the fact W0 ∈ WT (a, b):
ess-sup sup |H2 (x,C2;W0 (t)) - H2 (x, C2; W 00 (t))| ≤ K(a+ 1) kW0 - W00kT .
Consequently, we have:
IH3 (x; W0 (t)) - H (x; W00 (t))∣≤ K |w3 (t,C2)k2 (H (χ,C2; W0 (t)))-22 (H (χ,C2; W00 (t)))∣
+K|w30 (t, C2) - w300 (t, C2)|
≤ K |w3 (t,C2)∣∣H2 (x, C2; W0 (t)) - H2(x, C2; W00 (t))|
+KkW0 -W00kT,
∣∂2L (y, y (x; W0 (t))) - ∂2L (y, y (x; W00 (t)))∣ ≤ K ∣y (x; W0 (t)) - y (x; w00 (t))∣
≤ K IH (x; W0 (t)) - H3 (x; W00 (t))∣,
13
Published as a conference paper at ICLR 2021
which then yield the third and fourth bounds by the fact W0, W00 ∈ WT (a, b). Using these bounds,
we obtain the last bound:
∣∆H (z,C2; w 0 (t)) - ∆H (z,C2; w00(t))|
≤ K |w3 (t,C2)l (∣∂2L(y,y (x； W0(t))) - ∂2L(y,y (x； W00 (t)))∣
+ |H3(x;w0(t)) -H3(x;w00(t))|+ |H2(x,C2;w0(t)) -H2(x,C2;w00(t))|
+K|w30 (t,C2) - w300 (t, C2)|,
from which the last bound follows.	□
To prove Theorem 1, for a given W (0), we define a mapping FW (0) that maps from W0 =
(w1 ,w2,w3) ∈ WT (a,b) to Fw(0)(W0) = W0 = (wj,w2,w∖), defined by W0 (0) = W (0)
and
∂
∂tw3 (t, c2) = -ξ3 ⑴ δ3 (C2； W (t)),
∂
∂tw2 (t, c1, c2) = -ξ2 ⑴ δ2 (C1, c2； W (t)),
∂
∂twι (t, ci) = -ξι (t) ∆ι (C1； W (t)).
Notice that the right-hand sides do not involve W0. Note that the MF ODEs' solution, initialized at
W (0), is a fixed point of this mapping.
We establish the following estimates for this mapping.
Lemma 13. Under Assumption 1, for T ≥ 0, any initialization W (0) and any W0, W00 ∈
WT (a, b),
ess-supsup ∣∆3 (C2; W0 (S))- ∆3 (C2； W00 (s))| ≤ Ka,b ∣∣W0 -	W,
ess-supsup ∣∆2 (C1,C2; W0 (S))- ∆2 (C1,C2; W00 (s))| ≤ " ∣∣ W0 -	W,
ess-supsup ∣∆ι (C; W0 (s)) - ∆ι (Ci； W00 (s))| ≤ " ∣∣ W0 -	W,
and consequently, if in addition W0 (0) = W00 (0) (not necessarily equal W (0)), then
ZT∣W0-W00∣sdS,
0
ZT∣W0-W00∣sdS,
0
ZT∣W0-W00∣sdS,
0
ess - sup sup |w3 (t,C2) - w3 (t,C2)∣ ≤ Ka,b
t≤T
ess - sup sup |w2 (t,C1,C2) - Wa (t,C1,C2)∣ ≤ Ka,b
t≤T
ess - sup sup |wi (t, Ci ) — W? (t, Cl)∣ ≤ Kα,b
t≤T	,
in which W0 = (wj, w2,w3) = Fw(o)(W0), W00 = (W7, w2', WR = Fw(o)(W00) and Kab ≥ 1 is
a generic constant that grows polynomially with a and b.
Proof. From Assumption 1 and the fact W0, W00 ∈ WT (a, b), we get:
∣∆3 (C2； W0 (s)) - ∆3 (C2； W00(s))| ≤ κEZ [∣∂2L(Yy (X； W0(S)))- ∂2L(Yy (X； W00 (s)))∣]
+KEZ[|H3(X；W0(S))-H3(X；W00(S))|]
+KEZ[|H2(X,C2；W0(S)) -H2(X,C2；W00(S))|],
∣∆2 (C1,C2; W0 (s)) - ∆2 (C1,C2; W00 (s))| ≤ Kα,b |w1 (s,Ci) - w10 (s,Cl)∣
+ K ∣Ez [∆H (Z, C2； W0 (s)) - ∆H (Z, C2； W00 (s))]∣,
∣∆i (Ci； W0 (s)) - ∆i (Ci； W00 (s))| ≤ Ka,bEz [∣∆H (Z, C2； W0 (s)) - ∆∏ (Z, C2； W00 (s))∣]
14
Published as a conference paper at ICLR 2021
+ Ka,b |w20 (s,C1,C2)-w200(s,C1,C2)|
+ Ka,b |w10 (s, C1) - w100 (s, C1)| ,
from which the first three estimates then follow, in light of Lemma 12. The last three estimates then
follow from the fact that WW0 (0) = WW00 (0) and Assumption 1; for instance,
∂	0	∂	00
P ∂tw3 (S,C^ - ∂tw3 (s, C2) ds
ess-sup sup |w3 (t, C2) — W00 (t, C2)∣ ≤	ess-su]
t≤T	0
≤K
ZT
0
ess-sup ∣∆3 (C2; W0 (s)) — ∆3 (C2; W00 (s))| ds.
□
We are now ready to prove Theorem 1.
Proof of Theorem 1. We will use a Picard-type iteration. To lighten notations:
WT+ ≡ WT+ (9W 90 +K0,2 (T), 9W 90+K0,3(T),W(0)),	F ≡ FW (0) .
Since 9W90 ≤ K by assumption, we have 9W 90 +K0,2 (T) + K0,3 (T) ≤ KT. Recall that WT+
is complete. For an arbitrary T > 0, consider W0, W00 ∈ WT+ . Lemma 13 yields:
kF (W0) — F (W00)kτ ≤ KT ' k W0 — W00ks ds.
0
Note that F maps to WT+ under Assumption 1 by the same argument as Lemma 11. Hence we are
allowed to iterating this inequality and get, for an arbitrary T > 0,
F(k) (W0) — F(k) (W00)∣∣	≤ KT	∣∣F (k-1) (W0) — F (k-1) (W00)∣∣	dT2
≤ KT2 Z TZ	T2	∣∣∣F (k-2) (W0)	— F(k-2)	(W00)∣∣∣	I(T2	≤	T)dT3dT2
By substituting W00
∞
ZTZT2	Tk
... kW0—W00kTk+1I(Tk ≤ ... ≤T2 ≤T)dTk+1...dT2
≤	KT kW0 — W00kτ.
k!
F (W0), we have:
X F (k+1) (W0) — F(k) (W0)	= X F (k) (W00) —F(k) (W0)
k=1
k=1
∞
≤ X k KT kW0 — W 00kτ
k=1
< ∞.
∞
Hence as k → ∞, F(k) (W0) converges to a limit in WT+, which is a fixed point of F. The unique-
ness of a fixed point follows from the above estimate, since if W0 and W00 are fixed points then
kW0 — W00kτ = ∣∣F(k) (W0) — F(k) (W00)[ ≤ ɪKT kW0 — W00kτ ,
while one can take k arbitrarily large. This proves that the solution exists and is unique on t ∈
[0, T ]. Since T is arbitrary, we have existence and uniqueness of the solution on the time interval
[0, ∞).
□
15
Published as a conference paper at ICLR 2021
C Connection between the neural net and its MF limit: proofs
for Section 3
C.1 Proof of Theorem 3
We construct an auxiliary trajectory, which we call the particle ODEs:
∂tW3 (t,j2) = -ξ3 (t)EZ [∂2L (y,y (x； W(t))"3(H3 (X； W(t))"2(H2(X, j2； W(t)))i,
∂
∂tw2 (t, jl, j2) = -ξ2 Ct) EZ
∂
∂iw1 (t,j1) = -ξ1(t) EZ
Z,j2； W(t))n (hW1 (t,jι) ,Xi)],
n2
—X ∆H(Z,j2； W(t)) W2 (t,ji ,j2) ψ'ι (hW1 (t,jι) ,X i)X ,
nz j2=1
in which jι = 1,..., nι, j2 = 1,..., n2, W (t) = (Wι (t, ∙), W⅛ (t, ∙, ∙), W3 (t, ∙)), and t ∈ R≥o. We
specify the initialization W (0): Wi (0, jι) = w0 (Ci (jι)), W (0, jι, j2) = w0 (Ci (jι), C2 (j2))
and W3 (0, j3) = w0 (C2 (j2)). That is, it shares the same initialization with the neural network
one W (0), and hence is coupled with the neural network and the MF ODEs. Roughly speaking,
the particle ODEs are continuous-time trajectories of finitely many neurons, averaged over the data
distribution. We note that W (t) is random for all t ∈ R≥0 due to the randomness of {Ci (ji)}i=i,z.
The existence and uniqueness of the solution to the particle ODEs follows from the same proof as
in Theorem 1, which We shall not repeat here. We equip W (t) with the norm
/W 9 9 9 t = max ∖ max	SUp ∣W⅛ (t,j1,j2)∣, max sup ∣W3 (t,j2)∣，.
j1 ≤n1 , j2≤n2 t≤T	j2≤n2 t≤T
One can also define the measures DT (w, W) and DT (W, W) similar to Eq. (2):
DT (w,W) =SUp { ∣wι (t,Cι (jι))
|w3 (t, Cz (jz))
DT (W, W) = sup { ∣wι ([t/e] , jι)
|w3 (bt/c ,jz)
-w1 (t, C1 (j1 ))| , |w2 (t, C1 CjI) , C2 Cjz))- w2 (t, Cl CjI) , C2 Cjz))I ,
-	w⅛ (t,C2 (j2))∣ : t ≤ T, jι ≤ nι, j2 ≤ n2},
-	Wl (t,C1 (jι))I, ∣W2 ([t/d ,j1,j2) - W2 (t,C1 (jl) ,C2 (j2))I,
-	W3 (t,C2 (j2))∣ : t ≤ T, jι ≤ nι, j2 ≤ n2}.
We have the following results:
Theorem 14.	Under the same setting as Theorem 3, for any δ > 0, with probability at least 1 - δ,
Dt (w，w"√⅛ log1/2
3(T +1)n2m
ax + e eKT
δ
in which nmin = min {n1, nz}, nmax = max {n1, nz}, and KT = K 1 + TK .
Theorem 15.	Under the same setting as Theorem 3, for any δ > 0 and e ≤ 1, with probability at
least 1 - δ,
DT (W, W) ≤ je log (2nδn2 f eKT
in which KT = K(1 + TK).
Proof of Theorem 3. Using the fact
DT (W, W) ≤ DT (W, W) + DT (W, W)
the thesis is immediate from Theorems 14 and 15.
□
16
Published as a conference paper at ICLR 2021
C.2 Proof of Theorems 14 and 15
Proof of Theorem 14. In the following, let Kt denote an generic positive constant that may change
from line to line and takes the form
Kt = K (1 + tκ),
such that Kt ≥ 1 and Kt ≤ KT for all t ≤ T. We first note that at initialization, Do (w, W)= 0.
Since ∣∣∣W9o ≤ K, ∣∣∣W∣∣∣t ≤ KT by Lemma 11. Furthermore it is easy to see that ∣∣∣W9o ≤
9W 90 ≤ K almost surely. By the same argument as in Lemma 11, 9W 9T ≤ KT almost surely.
We shall use all above bounds repeatedly in the proof. We decompose the proof into several steps.
Step 1 - Main proof. Let us define, for brevity
q3 (t, x) = H3 卜;W (t)) - H (x; W (t)),
q2 (t,χ,j2,c2) = H2 1,j2; W (t)) - H (χ,c2; W (t)),
q∆ (t,z, jl, j2,Cl,C2) = ∆H (z, j2； W (t)) W2 (t,jl,j2) - ∆H (z,c2; W (t)) W2 (t,C1,C2).
Consider t ≥ 0. We first bound the difference in the updates between W and W. Let us start with
w3 and W3. By Assumption 1, We have:
∂	∂
∂tw3 (t,j2) -	∂tw3	(t,	C2	(j2))	≤	KEz	[|q3	(t, X)|	+ |q2 (t,X,j2, C2 (j2))|].
Similarly, for w2 and W2,
∂∂
∂tw2 (t,j1,j2)- ∂tw2(t,C1 (jI) ,C2 (j2))
≤ KEz h∣∆H(Z,j2； W(t)) -∆H(Z,C2 (j2); W(t))∣i
+ K |w3 (t, C2 缶川 |wi (t, jI)- w1 (t, CI (jI))I
≤ KtEz [|q3 (t, X)| + |q2 (t, X, j2, C2 (j2))|]
+ Kt (∣wι (t,ji) - wι (t, Ci (jι))∣ + 网(t,j2) - w3 (t, C2 (j2))∣)
≤ KtEZ [∣q3 (t, X)1 + Iq2 (t, X,j2, C2 (j2))∣] + KtDt(W, W),
and for wi and wi, by Lemma 12,
∂∂
∂twι (t,jι) - ∂twi(t, Ci (ji))
∣ 1	n2
≤ KEz	一 X EC2 [q∆ (t, Z,ji,j2,C1 (ji) ,C2)]
∣ n2	2
∣ j2 =i
+ EC2 [∣∆H (Z,C2; W (t))∣ ∣w2 (t, Ci (ji) ,C2)∣] w (t,ji) - wi (t, Ci (ji))∣
∣ 1 n2
≤ KEz	一 X EC2 [q∆ (t, Z,ji,j2,Ci (ji) ,C2)]
∣ n2	2
∣	j2 =i
+ KtDt (w,W).
To further the bounding, we now make the following two claims:
• Claim 1: For any ξ > 0,
max
∂∂
max 了w3 (t + ξ,C2 (j2 )) -方w3 (t,C2 (j2)) ≤ Kt+ξξ,
j2 ≤n2 ∣ ∂t	∂t	∣
17
Published as a conference paper at ICLR 2021
max
j1≤n1, j2≤n2
∂	， 一、…、、∂	一,、一、、
∂tw2 (t+ξ, C1 (j1), C2 S))- ∂tw2 g CI (j1), C2 (j2))
max
j1≤n1
∂twι (t+ ξ, C1 (ji)) - ∂twι (t C1 (jι))
≤ Kt+ξ &
≤ Kt+ξ &
and similarly,
max
j1≤n1, j2≤n2
max
j2≤n2
σ ~
σtw2
∂	,	_	、	∂	,、
率w3 (t + ξ,j2) -泥w3 (t,j2)
，	■	、	∂	，	、
(+ ξ,j1,j2) -石w2 (t,j1,j2)
σt
max
j1≤n1
σσ
σtw1(t+ξ,jI)-σtw1 (t,jI)
≤ Kt+ξξ,
≤ Kt+ξξ,
≤ Kt+ξξ.
• Claim 2: For any 71,72, Y3 > 0 and t ≥ 0,
max < max EZ [∣q2 (t,X, j2,C2 (j2))∣], EZ [∣q3 (t, X)|],
I j2≤n2
1 n2
max EZ —
j1≤n1	n2 —
j2 = 1
n2
Es [q∆ (t,Z, j1,j2,C1 (j1) ,C2)]
)
≥ Kt (Dt (w,W)+ Y1 + Y2 + Y3)
with probability at most
n1	( n27
K P k-不
+ n2 exp (-
72
n17:
不
+ Lxp (-
73
n2Y3
Kt
Combining these claims with the previous bounds, taking a union bound over t ∈
{0,ξ, 2ξ,..., ∖T∕ξ∖ ξ} for some ξ ∈ (0,1), we obtain that
max
σσ
Mw3 (t,j2) 一 ∣tw3 (t,C2 (j2)),
max -
j2≤n2 σt
j1≤m,ax ≤n2 ItW2 (t，j1，j2)-
max
σ
∂tw2 (t,C1 CjI) , C2 (j’2)),
max -
j1 ≤n1 σt
Mw1 (t，jI)-
σ
σtw1(t, C1 (j1))
≤ Kt	(Dt	(w, W) +	Y1 +	Y2 +	Y3 +	ξ)	,	∀t ∈	[0,T],
with probability at least
1 -中[nɪ exp
ξ	L71
The above event in turn implies
+ n2 exp (-
72
KT
+ Lxp (-
73
n2Y2
KT
Dt (w,W) ≤ KT
(w, W) + Y1 + Y2 + Y3 + ξ) ds,
n1Y
and hence by Gronwall,s lemma and the fact D0 (w, W) = 0, we get
DT (w, W) ≤ (γ1 + γ2 + Y3 + ξ) eKT.
The theorem then follows from the choice
ξ
nmax
72 = -K^ log
√nr
1/2 ( 3(T +1) nm
ax	KT
---+ e ,	71 = 73 = —;= log
n	√n2
1/2 ( 3(T +1) nm
1
δ
δ
"+ e
We are left with proving the claims.
18
Published as a conference paper at ICLR 2021
Step 2 - Proof of Claim 1. We have from Assumption 1,
ess-sup ∣w3 (t + ξ, C2) - w3 (t, C2)∣
ess-sup ∣W2 (t + ξ,C1,C2) - W2 (t,C1,C2)∣
≤K /
≤ Kξ,
r
≤K L
f
≤K /
ess-su
∂	，…，
P —W3 (s,C2) ds
ess-su
∂
P ∂tw2 (SCO ds
ess-sup ∣w3 (s C2)∣ ds
≤ Kt+ξξ,
ess-sup ∣wι (t + ξ, Ci) — wι (t, Ci)∣
r
≤K /
r
≤K L
ess-su
∂
P ∂tw1 (S, CI) ds
ess-sup ∣W3 (s , C2) w2 (s , Ci, C2)∣ ds
≤ Kt+ξξ.
By Lemma 12, we then obtain that
ess-supEz [∣H (X, C2; W (t + ξ)) - H (X, C2; W (t))∣] ≤ Kt+ξξ,
EZ [∣H3 (X； W (t + ξ)) - H3 (X; W (t))∣] ≤ Kt+ξξ,
ess-supEz [ J ∆H (Z,C2; W (t + ξ)) - ∆∏ (Z, C2; W (t)) ∣ ] ≤ Kt+ξξ.
Using these estimates, We thus have, by Assumption 1,
max
j2≤n2
∂tw3 (t + ξ, C2 (j2)) - ∂tw3 (t, C2 (j2 ))
≤ Kt+ξξ + KEZ [∣H3 (X; W (t + ξ)) - H3 (X; W (t))∣]
+ Kess-supEz [∣H (X, C2; W (t + ξ)) - H (X, C2; W (t))∣]
≤ Kt+ξξ,
∂	∂
max	7j7w2 (t + ξ, C1 CjI) , C2 (j2)) - 7T7w2 (t, C1 CjI) , C2 (j2))
j1≤n1, j2≤n2 ∂t	∂t
≤ Kt+ξξ + Kess-supEz [ ∣ ∆H (Z, C2； W (t + ξ)) - ∆∏ (Z, C2； W (t)) ∣ ]
+ K ess-sup ∣w3 (t, C2)∣∣w1 (t + ξ, Ci) - w1 (t, Cι)∣
≤ Kt+ξξ,
maχ MWI (t + ξ, C1 (j1)) - MWI (t,C1 (j1))
j1≤n1 ∂t	∂t
≤ Kt+ξξ + Kess-supEz [Eg [ ∣ ∆H (Z, C2; W (t + ξ)) - ∆∏ (Z, C2; W (t)) ∣ ∣w2 (t, Ci,C2)∣]]
+ Kess-supEc [∣w3 (t, C2)∣ ∣w2 (t + ξ, Ci, C2) - w2 (t, Ci, C2)∣]
+ Kess-supEc [∣w3 (t, C2) w2 (t, Ci,C2)∣] ∣wi (t + ξ, Ci)- wi (t, Ci)∣
≤ Kt+ξξ.
The proof of the rest of the claim is similar.
Step 3 - Proof of Claim 2. We recall the definitions of q∆, q2 and q3. Let us decompose them as
follows. We start with q2:
∣q2 (t,x, j2,C2 (j2 ))∣
1 n1
=一 X w2 (t, ji, j2) 2i (〈wi (t, ji) , xi) - EC1 [w2 (t, Ci, C2 (j’2))Pi (hwi (t, Ci) , xi)]
niy
31 =i
≤ max ∣<2 (t, ji, j2) 0 (〈wi (t, ji) ,x〉)一 w2 (t, Ci (ji), C2 (j2)) 0 (hwi (t, Ci (ji)) ,xi)∣
31≤n1
19
Published as a conference paper at ICLR 2021
+
1 n1
n X w2 (t, C1 (jl) ,C2 (j2)) 夕 1 ((WI (t,C1 CjI)) , Xi)- ECi [w2 (t, C1,C2 (j2)) 夕 1 ((WI (t,C1) , Xi)]
n1 j1=1
≡ Q2,1 (x, j2) + Q2,2 (x, j2) .
Similarly, we have for q3 :
|q3 (t, X)|
n2
=n X w3 (t,j2) ψ2 (H2 (X,j2； W (t))) — EC2 [w3 (t,C2) Ψ2 (H2 (x, C2； W (t)))]
n2 j2 =1
≤ max ,3 (t,j2) Ψ2 (H (x,j2； W (t))) — W3 (t,C2 (j2))中2 (H (x,C2 (j2) ； W (t)))∣
1 n2
+ — X W3 (t, C2 (j2))疹(H2 (x, C2 (j2) ； W (t))) — EC2 [W3 (t, C2)吐(H2 (x, C2； W (t)))]
∣ n2 j2=1
≡ Q3,1 (X) + Q3,2 (X) .
Finally we have for q∆ :
∣ 1 n2	∣
/ X EC2 [q∆ (t, z,j1,j2, Cl (j1) , C2)]
∣ n2 j2=1	∣
≤ max ∣∆H (z, j2； W (t)) W (t,jι, j2) — ∆H (z,C2 (j2) ； W (t)) w2 (t, Ci (jι), C2 (j2))
+
1 n2
X δ2 (Z,C2 (j2)； W (t)) w2 (t,C1 (jl) ,C2 (j2)) - EC2 [△? (z,C2； W Ct)) w2 (t, C1 (j1) , C2)]
n2 j2=1
≡ Q1,1 (z,j1) + Q1,2 (z, j1) .
Now let us analyze each of the terms.
•	We start with Q2,1. We have from Assumption 1,
max EZ [Q2,1 (X, j2)]
j2 ≤n2
≤ K max	∣W2 (t, j1,j2) — W2 (t,Ci (ji) ,C2 (j2))∣
j1 ≤n1 , j2 ≤n2
+ K	一 max 一	∣W2	(t,	Ci	(ji) ,C2 (j2))∣ |Wi	(t, ji)	—	wi	(t, Ci	(ji))|
j1 ≤n1 , j2 ≤n2
≤ KtDt (W,W).
•	To bound Q2,2, let us write:
Z (x, ci,c2) = W2 (t, ci,c2)2 1 ((wi (t, ci) , Xi).
Recall that Ci (ji ) and C2 (j2 ) are independent. We thus have:
E [Z2 (X, Ci (ji), C2 (j2))|X, C2 (j2)] =EC1 [Z2 (X, Ci, C2 (j2))].
Furthermore {Z2 (Ci (ji) , C2 (j2))}j1∈[n1] are independent, conditional on C2 (j2). We
also have, almost surely
|Z2 (X, Ci (ji), C2 (j2))| ≤Kt,
by Assumption 1. Then by Lemma 19,
P (EZ [Q2,2 (X,j2)] ≥ KtI2) ≤ (I∕γ2)exp (-niY2∕Kt).
20
Published as a conference paper at ICLR 2021
•	To bound Q3,1, we have from Assumption 1,
EZ [Q3,1 (X)] ≤ max (K ∣W3 (t, j2) - w3 (t,C2(j2))∣ + KtEZ [∣q2 (t,X, j2,C2 (j2))∣])
j2 ≤n2
≤ K Dt (W W) + Kt max EZ [∣q2 (t, X,j2, C (j2))∣].
j2 ≤n2
•	To bound Q3,2, noticing that almost surely
W (t,C2 (j2))P2 (H (x,C2 (j2)； W (t)))∣ ≤ Kt
by Assumption 1, we obtain
P (EZ [Q3,2 (X)] ≥ KtY3) ≤ (I∕γ3) exp (-∏2γ3∕Kt),
similar to the treatment of Q2,2 .
•	To bound Q1,1, using Assumption 1,
EZ [Q1,1 (Z,jι)] ≤ K max W (t, C j , C? (j2))∣ EZ [∣∆H (zj W (t)) - ∆ (Z, C? j ; W (t))∣]
+ Kmx ∣W2 (t,j1,j2) - W2 (t, Cl (jl) , C (j2))| EZ h∣∆H (z, j2； W (t)) ∣i
≤ K max ∣W2 (t, Cl (jι), C? (j2))∣ ( ∣W3 (t, j2) - w3 (t, C? (j2))∣
j2 ≤n2
+ |W3 (t, C? (j?))| EZ [|q3 (t, X)| + |q? (t, X, j?, C? (j?))|]
+ K max W (t,j1,j2) - W2 (t, C1 (jι) ,C (j2))l∣W3 (t,j2)l
j2 ≤n2
≤ Kt (Dt (w,W ) + EZ ∣q3 (t,X )| +max ∣q2 (t,X,j2,C2 (j2))∣ ).
•	To bound Q1,?, we note that almost surely
∣∆H (Z, C? (j2); W (t)) W2 (t, Cl (jι) ,C2 (j2))∣ ≤ K ∣W3 (t, C2 (j2))∣∣W2 (t, Cl (jl) , C2 (j2))∣
≤ Kt .
Then similar to the bounding of Q?,?, we get:
P(EZ [Qι,? (Z,jι)] ≥ Ktγι) ≤ (1∕γι)exp (-n?Y?/Kt).
Finally, combining all of these bounds together, applying suitably the union bound over j1 ∈ [n1]
and j? ∈ [n?], We obtain the claim.	□
Proof of Theorem 15. We consider t ≤ T, for a given terminal time T ∈ N≥0. We again reuse the
notation Kt from the proof of Theorem 3. Note that Kt ≤ KT for all t ≤ T . We also note that at
initialization, Do (w, W)= 0. We also recall from the proof of Theorem 3 that 9 W 9t ≤ KT
almost surely.
For brevity, let us define several quantities that relate to the difference in the gradient updates be-
tWeen w and W :
q3(k, z,z, j?) = d?L (y, y (x;W (k))).(H3 (x;W (k))) ψ?(H?(x, j?;W (k)))
-	d?L (y, y (x； W (ke))).3(H3 (x； W (屣))).? (h? (x,j?; W (ke))),
r3 (k,z, j?) = ξ3 (ke) d?L (y, y (x; W (ke))) .3 (H (x; W (ke))) φ? (h? (x, j?； W (ke)))
-	ξ3 (ke) Ez [d?L (y, y (x ; W(ke))). (h (x ; W(ke))).? (h? (Xj?; W(ke)))],
q? (k, z,z, jι, j?) = ∆H (z, j?; W (k)) .1 (hwι (k, jι) ,xi)
21
Published as a conference paper at ICLR 2021
-	∆H (z,j2； W (ke))Hi (hWι (ke,jι) ,£)),
r2 (k,z, j1, j2)= ξ2 (ke) δh (z, j2； W (ke)"1 (hw1 (ke,j1) ,xi)
-	ξ2 (ke) Ez ∆(Z,j2； W(ke)"1(hW1 (ke,j1) ,X))],
ɪ n2
q1 (k,z,z, j1) = - E ∆H (z,j2； W (k)) W2 (k,j1,j2) H (〈W1 (k,j1) ,x)) x
n2『
j2=1
1 &
-----E δ21 (z,j2； W (ke)) w2 (ke,j1,j2) Ψ1 (hw1 (ke,j1) ,xi) x,
n2 j2 = 1
1 n2
r1 (k,z,j1) = ξ1 (ke)	E δ21 (z, j2； W (ke)) w2 (ke, j1, j2) HI (hw1 (ke,j1) , x)) X
n2 j2 = 1
一 ξι (ke) Ez
J X	∆H(Z,j2； W(ke))	W2	(ke,九 j2)	H	(hWι	Ikejl ,X〉) X	.
2 j2 = 1
Let us also define:
q3 (k, x) = H3 (x； W (k)) - H3 (x； W (ke)),
qH (k,x, j2) = H2 (x, j2； W (k)) - H2 (x, j2； W (ke)
We proceed in several steps.
Step 1: Decomposition. As shown in the proof of Theorem 3:
max
j2≤n2
∂ ~ , 一 ∂ ~	,、
瓦w3 (t + ξ,j2)-瓦w3 (t,j2)
∂	,	,	、	∂	,	、
^max	入W2 (t + ξ,j1,j2)-工W2 (t,j1,j2)
jι≤nι, j2≤n2 ∂t	Ot
max
j1≤n1
O ,	」、O ,、
Otw1 (t + ξ,j1) - Otw1 (t,j1)
≤ Kt+ξξ,
≤ Kt+ξξ,
≤ Kt+ξξ∙
for any t ≥ 0 and ξ ≥ 0. These time-interpolation estimates, along with Assumption 1, allow to
derive the following. We first have:
max ∣W3 (Lt/eJ ,j2) - w3 (t, j2)∣
j2≤n2
≤ K max
j2 ≤n2
Lt/eJ—1
e E
k=0
ξ3 (ke)Ez [q3 (k,z (k) ,Z,j2)] + tKte
≤ K max [Q3,1 (Lt/eJ , j2) + Q3,2 ([t/e] j)]+ tKte,
j2≤n2
where we define
Lt/eJ —1
Q3,1 (Lt/eJ , j2) = e E ∣q3 (k,z (k) ,z (k) , j2)∣ ,
k=0
Lt/eJ —1
Q3,2 (Lt/eJ , j2) = e E r3 (k, z (k) , j2) ∙
k=0
(Here Pk=0J —1 = 0 if Lt/eJ = 0.) We have similarly:
max ∣W1 ([t/eJ ,j1) - w1 (t, j1)∣ ≤ K max [Q1,1 (Lt/eJ , j1) + Q1,2 (Lt/eJ , j1)] + tKte,
j1≤n1	jι ≤nι
22
Published as a conference paper at ICLR 2021
..max	∣w2 ([t/e] , ji, j2) - W (t,jι, j2)| ≤ K max [Q2,1 ([〃4 ,j1,j2) + Q2,2 ([〃4 , jι, j2)] + iKte,
j1≤n1, j2≤n2	jι ≤nι, j2≤n2
in which
Lt/eJ—1
Q1,1 ([t∕ec ,jι) = e X ©1 (k,z (k) ,z (k) ,ji)1,
k=0
Lt/eJ —1
Q1,2 ([t∕ec ,j1) = e X r1 (k, z (k) ,j1),
k=0
Lt/eJ —1
Q2,1 (Lt/eJ ,j1,j2) = e X 02(k,z (k) ,z (k) ,j1,j2)∖ ,
k=0
Lt/eJ —1
Q2,2 (Lt/ec ,j1,j2') = e X r2 (k, z (k) ,j1,j2).
k=0
The task is now to bound Q1,1, Q1,2, Q2,1, Q2,2, Q3,1 and Q3,2.
Step 2: Bounding the terms. Before We proceed, let us give some bounds for q∏ and q∏, which
hold for any x ∈ Rd:
I qH (k,x, j2) I ≤ K max (∣W2 (ke, j1, j2)∣∣w1 (k, j1) - W1 (ke, j1)∣ + ∣w2 (k,j1,j2) - W (ke, j1, j2)∣)
j1≤n1
≤ KkeDke (W, W),
1	qH (k,x) ∣ ≤ K max (∣w3 (k, j2) - W3 (ke, j2)∣ + ∣W3 (ke, j2)∣1 qH (k,x, j2)1)
≤ KkeDke (W, W).
With these, we have the following:
•	Let us bound Q3,1. By Assumption 1,
©3 (k, z (k) ,z (k) )2» ≤ K (qH (k,x (k), j2) + 1 qH (k,x (k)) 1 ).
We then get:
Lt/eJ-1
Q3,1 (Lt/eJ ,j2) ≤ Kte X Dke (W, W).
-	k=0
•	Similarly to Q3,1, we consider Q2,1:
∣q2 (k,z (k) ,z (k) ,j1 ,j2)∣ ≤ K I ∆H (z (k) ,j2； W (k)) - ∆H (z (k) ,j2； W (ke))∣
+ K 1 ∆H (z (k), j2； W (ke)) 1 ∣w1 (k, j1) - W (ke, j1)∣
≤ Kke ( 1 qH (k,x (k), j2)∣ + 1 qH (k,x (k))1)
+ K ∣w3 (k, j2) - W3 (ke, j2)∣ + Kke ∣w1 (k, j1) - W1 (ke, j1 )∣,
which yields
Lt/eJ—1
^max Q2,1 (Lt/eJ ,ji, j2) ≤ Kte X Dke(W, W)
31 ≤n1,j2≤n2	f~~1	∖	)
k=0
•	Again we get a similar bound for Q1,1:
...........................K	£
∣q1 (k,z (k) ,z (k), j1 )∣ ≤ 一 X ∣w2 (ke,j1,j2)∣l ∆H (z (k) j W (k)) - ∆H (Z (k) ,j2；W(ke))|
n2 32 = 1
23
Published as a conference paper at ICLR 2021
n2
+——X ∣∆H (Z (k), j2； WW (ke)) I ∣W2 (k,j1,j2) - W2 (ke,ji, j2)|
n2 j2=1
K n2	I	I
+----X |w2 (ke, jι, j2)| UH (Z (k) ,j2； W (ke))∣ |wi (k,ji) - wι (ke,jι)1
n2 j2=1
≤ Kk
max IqH (k,x (k), j'2)∣ + IqH (k,x (k))∖} + KkeDke (Wiz, W)
which yields
bt/ec-1
max Q1,1 (bt/c	, j1)	≤	Kt	X	Dke	Wi	, W .
1	1	k=0
•	Let us bound Q3,2 . Let us define:
k-1
r3 (k, j2) = X r3 (k,z (k) ,j2),	r (O,j2) =0.
'=0
Let Fk be the sigma-algebra generated by {Z (`) : ` ∈ {0, ..., k - 1}}. Note that
{r3 (k, j2)}k∈N is a martingale adapted to {Fk}破闪.Furthermore, for k ≤ T/e, the mar-
tingale difference is bounded: |r3 (k, Z (k) ,j2)| ≤ K by Assumption 1. Therefore, by
Theorem 20 and the union bound, we have:
P (max '∈{0maxτ∕e}Q3,2 ('，j2) ≥ ξ) ≤ 2n2 exp (-K (Tξ+ 1) e).
•	The bounding of Q2,2 is similar: |r2 (k, Z (k) ,j1,j2)| ≤ Kke almost surely by Assumption
1, and thus
P max	max Q2 2 (', jι, j2) ≥ ξ∖ ≤ 2nιn2 exp  --------77ξ----ʌʌ .
j1≤n1,j2≤n2 '∈{0,ι,...,τ∕e} q2,2 ( ,j1 ,j2) ≥ ξ) ≤	1 2 Pk KT (T +1) e
•	Again the bounding of Q1,2 is also similar: |r1 (k, Z (k) , j1)| ≤ Kke almost surely by
Assumption 1, and thus
P m max max Q1 2	(', j1)	≥	ξ]	≤ 2n1 exp	-------ξ-----ʌʌ	.
j1≤n1 '∈{0,1,...,T∕e} Q1,2	( ,j1)	≥	ξ)	≤	1 pV KT	(T +1) e
Step 3: Putting everything together. All the above results give us
bt/ec-1
Dbt/ece	Wi	,W ≤KTe	X	Dke	Wi	,W +ξ+TKTe	∀t	≤T,
k=0
which hold with probability at least
1 - 2n1n2exp (-不TE).
The above event implies, by Gronwall’s lemma,
DT Wi ,W ≤ (ξ+e)eKT.
Choosing ξ = KT，(T + 1) e log(2n1n2∕δ) completes the proof.	口
C.3 Proofs of Corollaries 4 and 5
Proof of Corollary 4. By the assumption on ψ and Assumption 1, we have:
|Ez [ψ (Y,y(x; W(bt/eC))) - ψ (Y,y (X; W(t)))]∣
24
Published as a conference paper at ICLR 2021
≤ KEZ[|H3(X;W(bt/c)) -H3(X;W(t))|]
≤ KEZ [|H (X； W (t)) - H (X； W ([t∕eC e))|] + KEZ [∣H3 (X; W ([t/e])) - H3 (X; W ([t/e[ e))∣].
An inspection of the proof of Theorem 3 (in particular, the proofs of Theorems 14 and 15) reveals
that firstly,
supEZ[|H3(X;W(t)) -H3(X;W(bt/ece))|] ≤KTe,
t≤T
and secondly,
supEZ[|H3(X;W(bt/ece)) - H3 (X; W (bt/ec))|]
t≤T
≤ KTDT (W, W) + —1= log1/2 ( 3Tnmax + e) eκT
ʌ/nmin	δ
with probability at least 1 一 δ. Together with Theorem 3, We obtain the claim.	□
Proof of Corollary 5. Observe that for each index {N1, N2} of Init, one obtains a neural network
initialization W(0) with law ρ by setting
w1(0,j1) = w1(0,C1(j1)), w2(0,j1,j2) = w2 (0, C1 (j1) , C2 (j2)) ,
w3(0,j2) = w3 (0, C2 (j2)) , j1 ∈ [N1] , j2 ∈ [N2] .
We consider the evolution W (k) starting from W (0), which is independent of W. Note that
W (k) is a deterministic function of its initialization W (0) and the data {z (j)}j≤k. Similarly, we
consider the counterpart for W: the evolution W (k) as a function of the initialization W (0) and
the data {Z(j)}j≤k. Due to sharing the same distribution for both the initialization and the data,
these evolutions have the same law; to be specific, W (n1,n2,T) and W (nι, n2, T) has the same
distribution for any n1 , n2 and T, where we define
W(n1,n2,T) = {wι (k,jι), W2 (k,j1,j2), W3 (k,j2):
ji ∈ [nι], j2 ∈ [n2], k ≤ [T∕eC },
and a similar definition for W (n1,n2,T). In other words,
W(W, W) ≡ inf E	max	1 |wi (k,ji) — Wi (k,jι)∣,
∖	C	CoUPlingof(W,亚)[k≤[T∕eC, j1≤n1, j2≤n2 I
∣W2 (k,jl,j2 ) — W 2 (k,jl,j2)l , ∣W3 (k,j2) — W 3 (k,j2)l }] =0.
Theorem 3 implies that for any tuple {n1, n2} such that n1 ≤ N1 and n2 ≤ N2, with probability at
least 1 - 2δ,
DT(n1,n2) (W, W) ≡ max	sup |W2 (t,j1,j2) - w2 (t, C1 (j1) ,C2 (j2))| ,
t≤T, j1≤n1, j2≤n2
sup	|W3 (t,j2) - w3 (t, C2 (j2))| ,
t≤T, j2 ≤n2
sup	∣W1 (t, ji) — Wi (t, Ci (jι))M ≤ Oδ,τ (e, N1,N2),
t≤T, j1≤n1
where Oδ,τ (e, Ni, N2) → 0 as e → 0 and Nmin log NmaX → 0 with Nmin = min {N1,N2} and
Nmax = max {Ni, N2}. We also have a similar result for W and W. As such, with probability at
least 1 - 4δ,
W (W, W) ≡ inf , e[ SUp n ∣Wi (t,Ci(ji))- Wi S,Ci(ji))∣,
∖	C CouPlingof (W,W) L t≤T, ji ≤nι, j2≤n2	'
∣w2 (t, Ci (ji), C2 (j2)) — W2 k,Ci (ji), C2 (j2)) ∣, ∣w3 (t, C2 (j2)) — W3 k,C2 (j2)
≤ DTn1,n2) (W, W) + DTn1 ,n2) (W, W) + W (W, W)
-Y	,	_ ____.
≤ 2Oδ,T (e, Ni, n2 ) .
By fixing the tuple {ni, n2} while letting e → 0, Nm-iin log Nmax → 0 and δ → 0, we obtain the
claim.	□
25
Published as a conference paper at ICLR 2021
D Global convergence: proofs for Section 4
D.1 Proof of Proposition 7
Proof of Proposition 7. Consider a probability space (Λ, G, P0) with random processes Rd-valued
p1 (θ1), R-valued p2 (θ1, θ2) and R-valued p3 (θ2), which are indexed by (θ1, θ2) ∈ [0, 1] × [0, 1],
such that the following holds. Let m1 and m2 be two arbitrary finite positive integers and,
with these integers, let θi(ki) ∈ [0, 1] : ki ∈ [mi] , i = 1, 2 be an arbitrary collection. For each
i = 1, 2, let Si be the set of unique elements in θi(ki) : ki ∈ [mi] . Similarly, let R2 be the set
of unique pairs in nθ1(k1), θ2(k2) : k1 ∈ [m1] , k2 ∈ [m2]o. We have that {p1 (θ1) : θ1 ∈ S1},
{p3 (θ2) : θ2 ∈ S2}and {p2 (θ1, θ2) : (θ1, θ2) ∈ R2} are all mutually independent. In addition, we
also have Law (p1 (θ1)) = ρ1, Law (p3 (θ2)) = ρ3 and Law (p2 (θ10 , θ20 )) = ρ2 for any θ1 ∈ S1,
θ2 ∈ S2 and (θ10 , θ20 ) ∈ R2 . Such a space exists by Kolmogorov’s extension theorem.
We now construct the desired neuronal embedding. For i = 1, 2, consider Ωi = A × [0,1] and
Fi = G × B ([0, 1]), equipped with the product measure P0 × Unif ([0, 1]) in which Unif ([0, 1])
is the uniform measure over [0, 1] equipped with the Borel sigma-algebra B ([0, 1]). We construct
Ω = Ωι × Ω2 and F = Fi × F2, equipped with the product measure P = (Po × Unif ([0,1]))2.
Define the deterministic functions w0 : Ωι → Rd, w0 : Ωι X Ω2 → R and w0 : Ω2 → R:
w10((λ1,θ1)) =p1(θ1)(λ1),
w20 ((λ1, θ1) , (λ2,θ2)) = p2 (θ1,θ2) (λ2) ,
w30 ((λ2 , θ2)) = p3 (θ2) (λ2) .
It is easy to check that this construction yields the desired neuronal embedding.
□
D.2 Proof of Theorem 8
We first present a measurability argument, which is crucial to showing that a certain universal ap-
proximation property holds throughout the course of training.
Lemma 16 (Measurability argument). Consider a family Init of initialization laws, which are
ρ1, ρ2, ρ3 -i.i.d., such that ρ2-almost surely |w2 | ≤ K and ρ3 -almost surely |w3 | ≤ K. There
exists a neuronal embedding (ω, F, P, {w0 卜=[② 3^ of Init such that there exist Borelfunctions Wi
and ∆Hi for which P-almost surely, for all t ≥ 0,
wi (t,Ci) = w； (t, w0 (Ci)),
△H (z,C2; W (t)) = ∆H* (t, z, w0 (C2)),
where W (t) is the MF dynamics formed under the coupling procedure with this neuronal embedding
as described in Section 3.1. Furthermore,
"(t,ui)
-ξi (t)	EZ △2H； (t, Z,
u3)u2^1 (hw； (t,uι) ,Xi)X] P (du2)ρ3 (du3)
+ ξi (t)
0
t
ξ2 (s) EZ,Z0
△2H； (t, Z, u3) △2H； (s, Z0, u3) ρ3 (du3)
× 2 1 (hw； (s,uι) ,X0i) Ψ1 (hw； (t,ui) ,Xi) X ds,
with initialization wi； (0, ui) = ui for all ui ∈ supp ρi and t ≥ 0, where Z0 is an independent
copy of Z.
Proof. We denote by Kt a constant that may depend on t and is finite with finite t. By Proposition
7, there exists a neuronal embedding that accommodates Init. We recall its construction and reuse
the notations from the proof of Proposition 7; in particular:
wi0 ((λi, θi)) = pi (θi) (λi) ,
26
Published as a conference paper at ICLR 2021
W20 ((λ1, θ1) , (λ2, θ2)) = p2 (θ1, θ2) (λ2) ,
W30 ((λ2, θ2 )) = p3 (θ2 ) (λ2) .
Let S1 , S3 and S2 denote the sigma-algebras generated by W10 (C1), W30 (C2) and
W10 (C1 ) , W20 (C1, C2 ) , W30 (C2) respectively. Let S13 denote the sigma-algebra generated by S1
and S3 . We also let S1Z to denote the sigma-algebra generated by S1 and the sigma-algebra of the
data Z. We define similarly for S2Z and S3Z.
Step 1: Reduced dynamics. Given the MF dynamics W (t), let us define
∆岂(t,C2)= E [∆3 (C2; W (t))∣S3](c2),
∆2 (t, ci, c2) = E [∆2 (Ci,C2; W (t))∣S2] (ci, c2),
∆i (t,ci)= E [∆i (Ci; W (t))∣Si](ci).
We recall from the proof of Theorem 3 that for any t, s ≥ 0,
ess-sup |W3 (t, C2) - W3 (s, C2)| ≤ K |t - s| ,
ess-sup |W2 (t, Ci, C2) - W2 (s, Ci, C2)| ≤ Kt∨s |t - s| ,
ess-sup |Wi (t, Ci) - Wi (s, Ci)| ≤ Kt∨s |t - s| .
Then by Lemma 13,
E [∣∆3 (t, C2) -	∆3 (s, C2)∣2]	≤	Kt∨s	|t -	s|2	,
E h∣∆2 (t, C1,C2) - ∆2	(s, C1,C2)∣2i	≤	Kt∨s	|t -	s|2	,
E h∣∆ 1 (t, Ci) -	∆ 1 (s, Ci)∣2i	≤	Kt∨s	|t -	s|2	.
Therefore, by Kolmogorov continuity theorem, there exist continuous modifications of the (time-
indexed) processes ∆ 1, ∆2 and ∆3. We thus replace them with their continuous modifications,
written by the same notations.
Given these continuous modifications, we consider the following reduced dynamics:
∂
W3 (t, c2) = -ξ3 ⑴ & (t, c2) ,
∂t
∂
w2 (t, c1,c2)= -ξ2 (t) A (t, c1,c2),
∂t
∂
w1 (t, c1) = -ξ1 (t) Ai (t, c1) ,
∂t
in which:
•	Wι : R≥o X Ωι → Rd, W? : R≥o X Ωι X Ω2 → R, W3 : R≥o X Ω3 → R.
•	W (t) = {W1 (t, ∙), W2 (t, ∙, ∙), W3 (t, ∙)} is the collection of reduced parameters at time t,
•	the initialization is Wi (0, ∙) = w0 (∙), W2 (0, ∙, ∙) = w0 (∙, ∙) and W3 (0, ∙) = w0 (∙), i.e.
W(0) = W (0).
Step 2: Measurability of the reduced dynamics. It is easy to see that W3 (t,C2) is S3-measurable
by its construction and the fact W3 (0, C2) = w0 (C2) is S3-measurable. Similarly, W2 (t, Ci, C2) is
S2-measurable and Wi (t, Ci) is Si-measurable.
Notice that there exist Borel functions W；, W2 and W3 for which P-almost surely,
Wi (t,Ci) = W； (t, W0 (Ci)),
W2 (t,Ci,C2) = W2 (t, W0 (Ci), w0 (Ci,C2) ,w0 (C2)),
W3 (t,C2) = w3 (t, W0 (C2)).
Indeed, since W2 (t, Ci,C2) is S2-measurable, there exists a function W； (t, ∙) for each rational t
such that the desired identity holds for P-almost every (Ci ,C2) and for all rational t ≥ 0. Since W2
is continuous in time, there is a unique continuous (in time) function W； (t, ∙) such that the identity
holds for all t ≥ 0 and for P -almost every (Ci, C2). The same argument yields the construction of
W； and W3.
27
Published as a conference paper at ICLR 2021
Step 3: Measurability of constituent quantities. We show that H (X,C2; W (t)) is SZ-
measurable. Recall that
H (X,C2; W(t)) = Eci [W2 (t,Ci,C2) 0 (hWi(t,Ci) ,Xi)].
By the existence of Wi and w2, for each t ≥ 0, there exists a Borel function ft such that almost
surely
W2 (t, Ci,C2)例(hWi (t, Ci) ,Xi) = ft (X, W0 (Ci), W0 (Ci, C2) ,W0 (C2)).
We recall that Pi and P2 are the laWs of wi0 (Ci) and w20 (Ci, C2). We analyze the folloWing:
E H (X,C2; W (t)) - / ft (X,Ui,U2,W0 (C2)) ρi(dui) P2 (du2)∣ 1
E h∣H2 (X,C2; W(t))∣2i + E /ft (X,
ui, u2, w30 (C2) ρi (dui) ρ2 (du2)∣
-2E H (X,C2; W(t)) ∕ft(X,ui,U2,w0 (C2)) ρi (dui) P2 (du2).
Let us evaluate the first term:
E h∣H2 (X,C2; W(t))∣2]
=E h∣E5 [ft(X, w0 (Ci) ,w0 (Ci, C2) ,w0 (C2))] ∣2i
(a)
(b)
(c)
E [ft(X, w0 (Ci), w0 (C1,C2), w0 (C2)) ft(X, w0 (Ci), w0 (Ci,C2) ,w0 (C2))]
Ehft (X, p1 (θ1) (λ1), p2 (θ1, θ2)(λ2), p3(θ2) (λ2))
× ft(X,pi(θi0)(λ0i),p2(θi0,θ2)(λ2),p3(θ2)(λ2))i
Ehft (X, pi (θi) (λi),p2 (θi, θ2) (λ2),p3(θ2) (λ2))
× ft(X,pi(θi0)(λ0i),p2(θi0,θ2)(λ2),p3(θ2)(λ2))I(θi 6=θi0)i
(=d) EZ
ft (X, ui,u2,u3) ft (X, u0i,u02,u3) ρi (dui) ρi (du0i) ρ2 (du2) ρ2 (du02) ρ3(du3) ,
where in step (a), we define C10 to be an independent copy of C1; in step (b), we recall C1 =
(λ1,θ1); in step (c), We recall θι,θ1 〜 Unif ([0,1]) and since Ci is independent of C1, We have
θ1 6= θ10 almost surely; step (d) is owing to the independence property of the construction of the
functions p1 , p2 and p3. We calculate the second term:
E
Zft(X,ui, u2, w30 (C2)) ρi (dui) ρ2 (du2)∣∣∣
=J E [ft(X, uι,U2,w0 (C2)) ft(X, u1,u2, w0 (C2))] ρ1 (dui) ρ2 (du2) ρ1 (du'i) P Idu2)
=	EZ	[ft	(X,ui, u2,u3)	ft (X,	u0i, u02,u3)] ρi	(dui)	ρ2	(du2) ρi (du0i) ρ2	(du02) ρ3 (du3)	,
as Well as the last term:
E H (X,C2; W(t)) ∕ft(X,ui,U2,w0 (C2)) Pi (dui) P2 (du2)
=E ft(X,w0 (Ci) ,w0 (Ci,C2) ,w0 (C2)) / ft (X,ui,u2,w0 (C2)) ρi(dui) ρ2 (du2)
= EZ [ft (X, ui, u2, u3) ft (X, u0i, u02, u3)] Pi (dui) P2 (du2) Pi (du0i) P2 (du02) P3(du3) .
28
Published as a conference paper at ICLR 2021
It is then easy to see that
E H (X,C; W(t)) - / ft(X,u1,U2,w3 (C)) ρ1 (duι)ρ2 (du2)l =0.
That is, we have almost surely
H2(X,。2； W (t)) = / ft (X, U1, U2, W0 (C2)) ρ1 (duι) ρ2 (du2).
Note that the right-hand side is SZ-measurable, and hence so is H2 (X , C2； W (t)).
Next we consider Af (Z, C¾; W (t)). Recall that
△H (z, C2； W (t)) = d2L (y, y (x; W ⑴))φ,3 (H3 (x; W (t))) w3 (t, C2)H (H2 (x, c2； W (t))).
Then together with the existence of w3, We have AH (Z, C⅛; W (t)) is SZ-measurable.
Now we consider E/ [AH (Z, C2； W (t)) W2 (t, Ci, C2)]. With the existence of W2, there exists a
Borel function gt such that
△H (Z, C2; W (t)) W2 (t, Ci, C2)=贝(Z, w0 (Ci),遥(Ci, C2), w0 (C2)).
Then with the same argument as the treatment of Η2(X, C2; W (t)), one can show that
Ec2 [AH (Z,C2; W(t)) W (t,C1,C2)]
/ gt (Z, w0 (Ci) ,u2,u3) ρ2 (du2) ρ3 (du3),
which is SZ -measurable.
Using these facts together with the existence of wj, Wg and w3, we have A3(C2； W (t)) is S3-
measurable, A2 (C1, C2; W (t)) is Si3-measurable and Ai (Ci； W (t)) is Si-measurable.
Step 4: Closeness between the MF dynamics and the reduced dynamics. We shall use
IlW - Wlit with the same meaning as the distance between two sets of MF parameters. Recall
by Lemma 11 that『W9丁 ≤ KT since『W『0 ≤ K. BytheSameargUment, ∣∣ W||丁 ≤ KT. Then
by Lemma 13, we have for any t ≤ T,
ess-sup sup I A3 (C2； W (S))- A3(C2； W (s))∣ ≤ KT ∣∣W - W1 ,
s≤t
ess-supsup∣A2 (C1,C2; W (S))- A2 (C1,C2; W(S))I ≤ Kt ∣∣ W - W∣∣t,
s≤t
ess-sup sup ∣ Al (Ci； W (s)) — Ai (Ci； W (s)) ∣ ≤ KT ∣∣ W — W1.
s≤t
We have:
ess-sup ∣A2 (t, C1, C2) - A2 (C1, C2； W (t)) ∣
=ess-sup ∣ E [A2 (C1, C2； W (t))∣S2] - A2 (C1,C2; W (t)) ∣
≤ ess-sup ∣ E [A2 (C1, C2； W (t)) ∣ S2] - A2 (C1,C2; W (t)) ∣
+ ess-sup ∣ E [A2 (C1,C2; W (t)) - A2 (C1,C2; W (t)) ∣ S2]∣
=ess-sup ∣ E [A2 (C1,C2; W (t)) - A2 (C1, C2； W (t)) ∣S2 ]∣
≤ ess-sup ∣ A2 (C1,C2; W(t)) - A2 (C1,C2; W (t)) ∣,
where step (a) is because A2 (C1, C2； W (t)) is Si3-measurable from Step 3 and Si3 ⊆ S2. As
such,
ess-sup ∣ A2 (t, C1,C2) - A2 (C1, C2； W (t)) ∣ ≤ 2Kt ∣∣W - W∣∣t
almost surely for all rational t ≤ T. By continuity in t of both sides, the same holds for all t ≤ T.
Hence by Assumption 1,
年w2 (t, Cl, C2)— —w2 (t, Cl, C2) ≤ K ∣ A2 (t, c1, C2) - A2 (C1, C2； W (t)) ∣
∂t	∂t	1	1
29
Published as a conference paper at ICLR 2021
≤ 2Kt IIW - WIlt,
for all t ≤ T almost surely, which leads to
∣w2 (t,C1,C2) - w2 (t,C1 ,C2)∣ ≤ 2Kt Z11∣W - W∣∣s ds
0
almost surely. One can obtain similar results for wι versus wι and w3 versus w3. Therefore,
∣∣w - W∣∣t ≤ KT L ∣∣w - W∣∣s ds.
Since W (0) = W (0), by Gronwall,s inequality, ∣∣W - W∣∣t = 0 for all t ≤ T. In other words,
since T is arbitrary,
wι (t,Cι) =	wι (t,Cι) ,	w2	(t,C1,C2)	=	w2	(t,C1,C2),	w3	(t,C2)	= w3	(t,C2),
for all t ≥ 0 almost surely.
Step 5: Concluding. The first claim of the lemma is proven by the conclusion of Step 4 and by
choosing w； = W；, w； = Wg and w3 = W3, as well as the measurability facts from Step 3. To
prove the second claim, since ∆H (Z, C2; W (t)) is SZ-measurable and 「∣ W — W||t=0 for all
t ≥ 0, there exists a Borel function ∆H； such that
△H (z, c2 ； W (t)) = ∆H (Z, c2 ； W (t)) = ∆H* (t, z, w0 (C2))
for all t ≥ 0 almost surely, by the same argument in Step 2. These facts, together with the dynamics
ofw1 and w2, imply that almost surely, for all t ≥ 0,
∂
由w2 (t, C1,C2 )
∂t
=-ξ2 (t) Ez [∆H* (t, Z, w0 (C2))包(〈w； (t, w0 (Cι)), X))],
∂
∂tw1 (t, w1 (CI))
=-ξι (t) EZ [Ec2 [∆H* (t, Z, w0 (C2)) w2 (t,C1,C2)] H (〈w； (t, w0 (Cι)), X)) X],
with initialization w1； 0, w10 (C1) = w10 (C1). Substituting the first equation into the second one,
we get:
∂
dtw； (t, w0 (Cι)) = -ξι (t) EZ [Ec2 [∆H； (t, Z, w0 (C2)) w0 (C1,C2)] H (〈w； (t, w0 (Cι)) , X)) X]
+ ξ1 (t) Z ξ2 (s) EZ,Z0 E
0
Note that by an argument similar to Step 3,
EC2 [∆H*(t,Z,w0 (C2)) w0 (C1,C2)] = / ∆H (t,Z,u3) u2ρ2 (du2) ρ3 (du3),
which holds for all t ≥ 0 almost surely by the same argument in Step 2. We thus obtain:
∂
∂tw1 (t,uι) = -ξι (t) J EZ [∆H (t,Z,u3) U2为(hw1 (t,uι) ,Xi) X] ρ2 (du2) P (du3)
+ ξ1 (t)	ξ2 (s) EZ,Z0
0
/ ∆H* (t, Z, U3)∆H* (s, Z0, U3)P3 (du3)
X 夕 1 (〈w； (s,uι) ,X0i) H (〈w； (t,uι) ,Xi) X ds,
with initialization Wi (t, uι) = uι for all uι ∈ SuPP (ρ1) and t ≥ 0.
□
30
Published as a conference paper at ICLR 2021
An important ingredient of the proof is that the distribution of w1 (t, C1) has full support at all time
t ≥ 0, even though we only need to assume this property at initialization t = 0. This key property is
proven by a topology argument, supported by the measurability result of Lemma 16. We remark that
a similar property for two-layer networks is established in Chizat & Bach (2018) using a different
topology argument.
Lemma 17. Consider the same setting as Theorem 8. For all finite time t ≥ 0, the support of
Law (w1 (t, C1)) is Rd.
Proof. By Lemma 16, one can choose a neural embedding such that there exists Borel functions wɪ
and ∆H* for which almost surely, for all t ≥ 0,
wι (t,Cι)= w； (t, w0 (Cι)),
△H (z,C2; W (t)) = ∆H*(t,z,w0 (C2)),
where W (t) is the MF dynamics formed under the coupling procedure with this neuronal embedding
as described in Section 3.1. Furthermore,
Itw； (t,uι)
/ EZ [∆H; (t,Z, u3) u2H (hw； (t,ui) ,X i) X ] P2 (du2) P (du3)
Z EZ,Z0
0
+
△2H； (t, Z, u3) △2H； (s,Z0,u3)ρ3(du3)
X 夕 1 (hw； (s, uι), X0i) H (hw； (t, uι), Xi) X ds,
with initialization w1； (0, u1) = u1 for all u1 ∈ supp ρ1 and t ≥ 0, where Z0 is an independent
copy of Z . We recall from Lemma 12 that
ess-sup∆H； (t, Z, w0 (C2)) = ess-sup∆H (Z, C2； W (t)) ≤ Kt,
where Kt denotes a generic constant that depends on t and is finite with finite t. Therefore, by
Assumption 1, for t ≤ T and u1 , u01 ∈ supp ρ1 ,
∂∂
∂tw1 (t,u1)- ∂tw1 (t,u1) ≤
≤
Kt |w1；	(t, u1)	- w1；	(t,	u01)|	+	Kt Z	|w1；	(s, u1) -	w1；	(s,	u01)|	ds
0
KT sup |w1； (s, u1) - w1； (s, u01)| ,
s≤t
∂	； ,	、1
∂twl (t,ul) ≤
KT .
Applying Gronwall’s lemma to the first bound:
sup |w1； (t, u1) - w1； (t, u01)| ≤ eKT |w1； (0, u1) - w1； (0, u01)|
t≤T
= eKT |u1 - u01 | .
Furthermore the second bound implies
sup |w1； (t, u1) - w1； (t0, u1)| ≤ KT |t - t0| .
t,t0≤T
Therefore (t, u1) 7→ w1； (t, u1) is a continuous mapping on [0, T] × Rd for an arbitrary T ≥ 0.
Given this continuity, we show the thesis by a topology argument. Consider the sphere Sd which is
a compactification of Rd. We can extend w1； to a function M : [0, T] × Sd → Sd fixing the point at
infinity, which remains a continuous map since |M (t, u1) - u1 | = |M (t, u1) - M (0, u1)| ≤ KTt.
Let Mt : Rd → Rd be defined by Mt (u1) = M (t, u1). We claim that Mt is surjective for all finite
t. Indeed, if Mt fails to be surjective for some t, then for some p ∈ Sd, Mt : Sd → Sd\ {p} → Sd
is homotopic to the constant map, but M then gives a homotopy from the identity map M0 on
the sphere to a constant map, which is a contradiction as the sphere Sd is not contractible. Hence
w； (t, ∙) is surjective for all finite t. Recall that wι (t,Cι) = w； (t,w10 (C1) almost surely and
w10 (C1) has full support. Now let us assume that w1 (t, C1) does not have full support at some
31
Published as a conference paper at ICLR 2021
time t, which implies there is an open ball B in Rd for which P (w1 (t, C1) ∈ B) = 0. Then
P (wɪ (t, w0 (Cι)) ∈ B) = 0. Since w； (t, ∙) has full support, there is an open set U such that
wɪ (t,uι) ∈ B for all uι ∈ U. Then P (w0 (Ci) ∈ U) = 0, contradicting the assumption that
w0 (Ci) has full support. Therefore wi (t, Ci) must have full support at all t ≥ 0.	口
With this, we are ready to prove Theorem 8.
Proof of Theorem 8. Recall, by Theorem 1, the solution to the MF ODEs exists uniquely, and by
Lemma 17, the support of Law (wi (t, Ci)) is Rd at all t. By the convergence assumption, we have
that for any > 0, there exists T () such that for all t ≥ T () and P -almost every ci:
EC2 [∣Ez [∆H (N, C W (t))处(hwι (t, ci), Xi)]∣] ≤ e.
Since Law (wi (t, Ci)) has full support, we obtain that for u in a dense subset of Rd,
Ec [∣Ez [∆H (N, C2; W (t))中11hu Xi)] ∣] ≤ e.
By continuity of u → 夕i(〈u, x))，We extend the above to all U ∈ Rd. Since 夕i is bounded,
Ec2 [∣Ez [(∆H (z, C2; W (t)) - ∆H (Z, C2; Wi, W2, W3))中i Ihu X))] ∣]
≤ KE [∣∆H (z, C2; W (t)) - ∆H (Z, C2; Wi, W2, W3)∣]
≤ KEh (I + |W3(C2)|) ( |w3(t, C2) - w3(C2)| + |W3(C2)| |w2(t, Ci, C2) - w2(Ci, C2)|
+ ∣w3(C2)∣∣w2(Ci, C2)∣∣Wi(t,Ci) - Wi(Ci)I )i,
Where the last step is by Assumption 1. Recall that the right-hand side converges to 0 as t → ∞.
We thus obtain that for all u ∈ Rd,
EC2 [|〈EZ [δh (Z,C2； wi, w2, w3) IX = χ] Ni (hu,xi)〉L2(Px)|i
=Ec2 [∣Ez [∆H (Z, C2； Wi,W2,W3) ifii (hu,X))] |]
= 0,
Which yields that for all u ∈ Rd and P -almost every c2,
|〈EZ [δh (Z,C2； wi, w2, w3) |X = χ],中i (hU,x))〉L2(Px)| = 0.
Here We note that by Assumption 1,
|EZ [∆H (Z, C2； Wi, W2, W3)	∣X = x] | ≤ K ∣W3 (c2)∣	,
and so EZ	[∆H	(Z, c2; Wi,W2,W3) ∣X = x] is in L2 (PX)	for	P-almost	ev-
ery c2. Since {夕i ((u, ∙)) : U ∈ Rd} has dense span in L2 (PX), we have
EZ [∆h (N, c2; Wi,W2,W3) ∣X = x] = 0 for PX-almost every X and P-almost every c2,
and hence
Ez [∂2L (Y,y (X; Wi, W2, W3))∣X = x]夕3 (H (x; Wi, W2, W3)) W3 (c2)夕2 (H (x, c2; Wi, W2)) = 0.
We note that our assumptions guarantee that P (W3 (C2) = 0) is positive. Indeed:
•	In the case w0 (C2) = 0 with positive probability and ξ3 (∙) = 0, the conclusion is obvious.
•	In the case L (w0, w0, w0) < EZ [L (Y 夕3 (0))], we recall the following standard prop-
erty of gradient flows:
L (Wi (t, ∙) , W2 (t, ∙, ∙) , W3 (t, ∙)) ≤ L (Wi (t0, ∙) , W2 (t0, ∙, ∙) , W3 (t0, ∙)),
for t ≥ t0. In particular, setting t0 = 0 and taking t → ∞, it is easy to see that
L (Wi, W2, W3) ≤ L (w0, w0, w0) < Ez [L (Y 23 (0))].
If P (W3 (C2) = 0) = 1 then L (Wi,W2,W3) = EZ [L (Y33 (0))], a contradiction.
32
Published as a conference paper at ICLR 2021
Then since 夕2 and 夕3 are strictly non-zero, We have EZ [∂2L (Y, y (X; Wι, W2,W3))∣X = x] = 0
for PX -almost every x.
In Case 1, since L convex in the second variable, for any measurable function y(x),
L (y,y(χ)) - L (y,y(χ; w1,w2,w3)) ≥ ∂2L(y,y(x； w1,w2,w3))(y (x) - y(x； w1,w2,w3)).
Taking expectation, we get EZ [L (Y, y (X))] ≥ L (W1,W2,W3), i.e. (W1,W2,W3) is a global mini-
mizer of L .
In Case 2, since y is a function of x, we obtain ∂2L (y,y (x; W1,W2,W3)) = 0 and hence
L (y, y(x; W1,W2,W3)) = 0 for PX-almost every x.
Finally we have from Assumptions 1, 3:
|L (W (t)) - L (W1,W2,W3)∣ = |Ez [L (Y,y(X; W (t))) -L (Y,y(X; W1,W2,W3))]∣
≤ KEZ [|y (X; W (t)) - y(X; W1,W2,W3)∣]
≤ KEh |w3 (t, C2) - w3 (C2)| + |w3 (C2)| |w2 (t, C1,C2) - w2 (C1,C2)1
+ ∣W3 (C2)∣∣W2 (C,C2)∣∣w1 (t,C1) - Wi (Ci)| i
which tends to 0 as t → ∞. This completes the proof.	□
E Converse for global convergence: Remark 9
We prove a converse statement for global convergence in relation with the essential supremum con-
dition (6).
Proposition 18. Consider a neuronal embedding (ω, F, P, {w?},= [ 2 3) of (ρ1,ρ2,ρ3)-i.i.d. ini-
tialization. Consider the MF limit corresponding to the network (1), such that they are coupled
together by the coupling procedure in Section 3.1, under Assumptions 1, 2, ξ[ (∙) = ξ (∙) = 1.
Assume that L(y, y) → ∞ as |y| → ∞ for each y. Further assume that there exists W3 such that as
t → ∞,
Ec2 [∣w3(t,C2) - W3(C2)∣] → 0.
Then the following hold:
•	Case 1 (convex loss): If L is convex in the second variable and
lim L (W (t)) = infL(V),
t→∞	V
then it must be that
sup EC2
ci ∈Ωι
∂
∂tw2 (t, c1,C2)
→ 0 as t → ∞.
•	Case 2 (generic non-negative loss): Suppose that ∂2L (y, y) = 0 implies L (y, y) = 0, and
y = y(x) is a function ofx. IfL (W (t)) → 0 as t → ∞, then the same conclusion also
holds.
Proof. We recall
∂
—W2 (t, C1,C2) = -EZ ∂2L (Y, y (X; W (t))) W3 (t, C2)
∂t
X 尻(H3 (X; W (t))) & (H2 (X, C2； W (t)))中 1 (hwι (t, ci) ,Xi) i,
for ci ∈ Ωι, c2 ∈ Ω2. By Assumption 1,
∂
—W2 (t, C1,C2) ≤ KEZ [∣∂2L (Y,y(X; W (t)))l] ∣W3 (t,c2)∣.
∂t
33
Published as a conference paper at ICLR 2021
Note that the right-hand side is independent of ci. Since E。？ [∣w3(t, C2) - W3(C2)∣] → 0 as t →
∞, we have for some finite t0 ≤ K,
EC2 [∣W3(C2)∣] ≤ EC2 [∣w3(t0,C2)∣]+ K ≤ K,
where the last step is by Lemma 11 and Assumption 2. As such, for all t sufficiently large, we have:
∂
sup EC2	.W2 (t, c1,C2)	≤ KEZ [∣∂2L (Y, y (X; W (t)))∣] Ec2 [∣W3 (t,C2)∣]
c1∈Ω1	L ∂t	_
≤ KEZ [∣∂2L (Y,y (X; W (t)))∣](K + EC2 [|W3 (C2)∣])
≤ K EZ [∣∂2L (Y,y(X; W (t)))∣].
The proof concludes once we show that EZ [∣∂2L (Y, y (X; W (t)))∣] → 0 as t → ∞.
For a fixed Z = (x, y),let us write L (t, Z) = L(y, y(x; W(t))) and ∂2L(t, Z) = ∂2L(y, y(x; W(t)))
for brevity. Consider Case 1. We claim that if there is an increasing sequence of time ti so
that limi→∞ [L(ti,z) - infy L(y, y)] = 0, then limi→∞ ∣∂2L(ti, z)| = 0. Indeed, it suffices
to show that for any subsequence tij of ti , there exists a further subsequence tij such that
limk→∞ ∂2L(tij , Z) = 0. In any subsequence tij of ti, using that L(tij , Z) is convergent and
the fact L(y,y) → ∞ as |y| → ∞, We have y(x; W(tj)) is bounded. Hence, We obtain a
subsequence 办九 for which y(x; W (tj)) converges to some limit y*. By continuity, We have
L(y, y*) = limk→∞ L(tj, z) = inf ^ L(y, y). Thus, since L is convex in the second variable, we
have ∂2L(y,y*) = 0. Thus, limk→∞ ∣∂2L(tj, z)∣ = ∣∂2L(y,y*)∣ = 0, as claimed. Similarly, we
obtain in Case 2 that if there is an increasing sequence of time ti so that limi→∞ [L(ti, Z)] = 0, then
limi→∞ ∣∂2L(ti, z)| = 0.
To show that EZ [∣∂2L (t, Z )|] → 0 as t → ∞, it suffices to show that for any increasing sequence of
times ti tending to infinity, there exists a subsequence tij ofti such that EZ ∣∂2L tij , Z ∣ → 0. In
Case 1, we have limi→∞ L (W (ti)) = infV L (V), so limi→∞ EZ [l (ti, Z) - infY L(Y, Y)]=
0. Since L (ti,Z) - infY L(Y,Y) is nonnegative, this implies that L (ti,Z) - infY L(Y,Y)
converges to 0 in probability. Thus, there is a further subsequence tij for which L tij , Z -
infY L(Y,Y) converges to 0 P-almost surely. By the previous claim, ∣∂2L (tj ,Z)∣ con-
verges to 0 P -almost surely. Since ∣∂2L tij , Z ∣ is bounded P-almost surely, we obtain that
EZ ∣∂2 L tij , Z ∣ → 0 from the bounded convergence theorem. The result in Case 2 can be
established similarly.	口
F Useful tools
We first present a useful concentration result. In fact, the tail bound can be improved using the
argument in Feldman & Vondrak (2018), but the following simpler version is sufficient for our
purposes.
Lemma 19. Consider an integer n ≥ 1 and let x, c1, ..., cn be mutually independent random
variables. Let Ex and Ec denote the expectations w.r.t. x only and {ci}i∈[n] only, respectively.
Consider a collection of mappings {fi}i∈[n], which map to a separable Hilbert space F. Let fi (x) =
Ec [fi (ci, x)]. Assume that for some R > 0, |fi (ci, x) - fi (x)| ≤ R almost surely, then for any
δ > 0,
1n
反 Efi(Ci ,x) - fi(X)
n i=1
8R
丁exp
Proof. For brevity, let us define
n
Zn (x) = X (fi (ci,x) -fi (x)) .
i=1
34
Published as a conference paper at ICLR 2021
By Theorem 21,
P (|Zn (x)| ≥ nδ∣x) ≤ 2exp (—nδ2/(4R2)),
and therefore,
P (| Zn (x) | ≥ nδ) ≤ 2 exp (—nδ2/(4R2)),
since the right-hand side is uniform in x. Next note that, w.r.t. the randomness of x only,
Ex [|Zn (x)∣] = Ex [|Zn (x)| I (|Zn (x)| ≥ nδ∕2)] + Ex [|Zn (x)| I (|Zn (x)| < nδ∕2)]
≤ Ex [|Zn (x)| I (|Zn (x)∣≥ nδ∕2)] + nδ∕2.
As such, by Markov’s inequality and Cauchy-Schwarz’s inequality,
P (Ex [|Zn (x)|] ≥nδ) ≤ P (Ex [|Zn (x)| I (|Zn (x)| ≥ nδ∕2)] ≥ nδ∕2)
≤ -2E [|Zn (x)| I (|Zn (x)| ≥ nδ∕2)]
nδ
≤ nδE h∣Zn (x)l2i1/2 P(|Zn (x)l ≥ nδ∕2)1/2
≤ nδE h|Zn (X)|2i /exp (-8⅛).
Notice that since c1, ..., cn are independent and fi (x) = Ec [fi (ci, x)],
n
E |Zn (X)|2 =X E |fi(ci,X) — fi (X)|2 ≤ 2nR2.
i=1
We thus get:
8R
P (Ex [|zn(X) 1] ≥ nδ) ≤√nδexp
This proves the claim.
□
We state a martingale concentration result, which is a special case of (Pinelis, 1994, Theorem 3.5)
which applies to a more general Banach space.
Theorem 20	(Concentration of martingales in Hilbert spaces.). Consider a martingale Zn ∈ Z a
separable Hilbert space such that |Zn — Zn-1 | ≤ R and Z0 = 0. Then for any t > 0,
P (max |Zk| ≥ t) ≤ 2 inf exp (—λt + ess-supXXE heλlZk-Zk-1l — 1 — λ∣Zk — Zk-ι∣ | Fk-Ii) .
In particular, for any δ > 0,
P max |Zk | ≥ nδ ≤ 2 exp
The following concentration result for i.i.d. random variables in Hilbert spaces is a corollary.
Theorem 21	(Concentration of i.i.d. sum in Hilbert spaces.). Consider n i.i.d. random variables
X1 , ..., Xn in a separable Hilbert space. Suppose that there exists a constant R > 0 such that
|Xi — E [Xi ]| ≤ R almost surely. Then for any δ > 0,
n
P
n XX Xi- E [Xi] ≥ δ
i=1
≤ 2 exp
35