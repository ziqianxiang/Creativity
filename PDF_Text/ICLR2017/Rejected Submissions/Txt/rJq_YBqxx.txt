Under review as a conference paper at ICLR 2017
Deep Character-Level Neural Machine
Translation By Learning Morphology
Shenjian Zhao
Department of Computer Science and Engineering
Shanghai Jiao Tong University
Shanghai 200240, China
sword.york@gmail.com
Zhihua Zhang
School of Mathematical Sciences
Peking University
Beijing 100871, China
zhzhang@math.pku.edu.cn
Ab stract
Neural machine translation aims at building a single large neural network that can
be trained to maximize translation performance. The encoder-decoder architecture
with an attention mechanism achieves a translation performance comparable to the
existing state-of-the-art phrase-based systems. However, the use of large vocabulary
becomes the bottleneck in both training and improving the performance. In this
paper, we propose a novel architecture which learns morphology by using two
recurrent networks and a hierarchical decoder which translates at character level.
This gives rise to a deep character-level model consisting of six recurrent networks.
Such a deep model has two major advantages. It avoids the large vocabulary issue
radically; at the same time, it is more efficient in training than word-based models.
Our model obtains a higher BLEU score than the bpe-based model after training
for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that
our model is able to learn morphology.
1	Introduction
Neural machine translation (NMT) attempts to build a single large neural network that reads a
sentence and outputs a translation (Sutskever et al., 2014). Most of the extant neural machine
translations models belong to a family of word-level encoder-decoders (Sutskever et al., 2014; Cho
et al., 2014). Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which
automatically searches the alignments and greatly improves the performance. However, the use of a
large vocabulary seems necessary for the word-level neural machine translation models to improve
performance (Sutskever et al., 2014; Cho et al., 2015).
Chung et al. (2016a) listed three reasons behind the wide adoption of word-level modeling: (i) word
is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling.
Consider that a language itself is an evolving system. So it is impossible to cover all words in the
language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can
effect the performance of neural machine translation. In particular, using larger vocabulary does
improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes
much harder and the vocabulary is often filled with many similar words that share a lexeme but have
different morphology.
There are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre
et al. (2016); Luong et al. (2015); Cho et al. (2015) proposed to obtain the alignment information of
target unknown words, after which simple word dictionary lookup or identity copy can be performed
to replace the unknown words in translation. However, these approaches ignore several important
properties of languages such as monolinguality and crosslinguality as pointed out by Luong and
1
Under review as a conference paper at ICLR 2017
Manning (2016). Thus, Luong and Manning (2016) proposed a hybrid neural machine translation
model which leverages the power of both words and characters to achieve the goal of open vocabulary
neural machine translation.
Intuitively, it is elegant to directly model pure characters. However, as the length of sequence
grows significantly, character-level translation models have failed to produce competitive results
compared with word-based models. In addition, they require more memory and computation resource.
Especially, it is much difficult to train the attention component. For example, Ling et al. (2015a)
proposed a compositional character to word (C2W) model and applied it to machine translation (Ling
et al., 2015b). They also used a hierarchical decoder which has been explored before in other context
(Serban et al., 2015). However, they found it slow and difficult to train the character-level models, and
one has to resort to layer-wise training the neural network and applying supervision for the attention
component. In fact, such RNNs often struggle with separating words that have similar morphologies
but very different meanings.
In order to address the issues mentioned earlier, we introduce a novel architecture by exploiting the
structure of words. It is built on two recurrent neural networks: one for learning the representation
of preceding characters and another for learning the weight of this representation of the whole
word. Unlike subword-level model based on the byte pair encoding (BPE) algorithm (Sennrich et al.,
2016), we learn the subword unit automatically. Compared with CNN word encoder (Kim et al.,
2016; Lee et al., 2016), our model is able to generate a meaningful representation of the word. To
decode at character level, we devise a hierarchical decoder which sets the state of the second-level
RNN (character-level decoder) to the output of the first-level RNN (word-level decoder), which will
generate a character sequence until generating a delimiter. In this way, our model almost keeps the
same encoding length for encoder as word-based models but eliminates the use of a large vocabulary.
Furthermore, we are able to efficiently train the deep model which consists of six recurrent networks,
achieving higher performance.
In summary, we propose a hierarchical architecture (character -> subword -> word -> source sentence
-> target word -> target character) to train a deep character-level neural machine translator. We show
that the model achieves a high translation performance which is comparable to the state-of-the-art
neural machine translation model on the task of En-Fr, En-Cs and Cs-En translation. The experiments
and analyses further support the statement that our model is able to learn the morphology.
2	Neural Machine Translation
Neural machine translation is often implemented as an encoder-decoder architecture. The encoder
usually uses a recurrent neural network (RNN) or a bidirectional recurrent neural network (BiRNN)
(Schuster and Paliwal, 1997) to encode the input sentence x = {x1, . . . , xTx} into a sequence of
hidden states h = {h1, . . . , hTx }:
ht = f1(e(xt), ht-1),
where e(xt) ∈ Rm is an m-dimensional embedding of xt . The decoder, another RNN, is often
trained to predict next word yt given previous predicted words {y1, . . . , yt-1} and the context vector
ct ; that is,
p(yt | {y1, . . . , yt-1}) = g(e(yt-1), st, ct),
where
st = f2(e(yt-1),st-1,ct)	(1)
and g is a nonlinear and potentially multi-layered function that computes the probability of yt . The
context ct depends on the sequence of {h1, . . . , hTx}. Sutskever et al. (2014) encoded all information
in the source sentence into a fixed-length vector, i.e., ct = hTx. Bahdanau et al. (2015) computed ct
by the alignment model which handles the bottleneck that the former approach meets.
The whole model is jointly trained by maximizing the conditional log-probability of the correct
translation given a source sentence with respect to the parameters of the model θ:
Ty
θ* = argmax ɪ2 log P(yt I {yι,…，yt-i}, x, θ)∙
t=1
For the detailed description of the implementation, we refer the reader to the papers (Sutskever et al.,
2014; Bahdanau et al., 2015).
2
Under review as a conference paper at ICLR 2017
3 Deep Character-Level Neural Machine Translation
We consider two problems in the word-level neural machine translation models. First, how can
we map a word to a vector? It is usually done by a lookup table (embedding matrix) where the
size of vocabulary is limited. Second, how do we map a vector to a word when predicting? It is
usually done via a softmax function. However, the large vocabulary will make the softmax intractable
computationally.
We correspondingly devise two novel architectures, a word encoder which utilizes the morphology
and a hierarchical decoder which decodes at character level. Accordingly, we propose a deep
character-level neural machine translation model (DCNMT).
3.1	Learning Morphology in a Word Encoder
Many words can be subdivided into smaller meaningful units called morphemes, such as “any-one”,
“any-thing” and “every-one.” At the basic level, words are made of morphemes which are recognized
as grammatically significant or meaningful. Different combinations of morphemes lead to different
meanings. Based on these facts, we introduce a word encoder to learn the morphemes and the rules
of how they are combined. Even if the word encoder had never seen “everything” before, with a
understanding of English morphology, the word encoder could gather the meaning easily. Thus
learning morphology in a word encoder might speedup training.
The word encoder is based on two recurrent neural networks,
as illustrated in Figure 1. We compute the representation of the
word ‘anyone, as	ranyonc = tanh(^ Wtrt)
T t=ι
6
ranyone = tanh(ɪ^ Wtrt),
t=1	∖
where rt is an RNN hidden state at time t, computed by
rt = f(e(xt), rt-1).
Each rt contains information about the preceding characters.
The weight wt of each representation rt is computed by
wt = exp(aff(ht)),
where ht is another RNN hidden state at time t and aff() is
an affine function which maps ht to a scalar. Here, we use a
BiRNN to compute ht as shown in Figure 1. Instead of nor-
malizing it by Pt exp(aff(ht)), we use an activation function
tanh as it performs best in experiments.
Figure 1: The representation of the
word ’anyone.’

We can regard the weight wi as the energy that determines whether ri is a representation of a
morpheme and how it contributes to the representation of the word. Compared with an embedding
lookup table, the decoupled RNNs learn the representation of morphemes and the rules of how they
are combined respectively, which may be viewed as learning distributed representations of words
explicitly. For example, we are able to translate “convenienter” correctly which validates our idea.
After obtaining the representation of the word, we could encode the sentence using a bidirectional
RNN as RNNsearch (Bahdanau et al., 2015). The detailed architecture is shown in Figure 2.
3.2	Hierarchical Decoder
To decode at the character level, we introduce a hierarchical decoder. The first-level decoder is similar
to RNNsearch which contains the information of the target word. Specifically, st in Eqn. (1) contains
the information of target word at time t. Instead of using a multi-layer network following a softmax
function to compute the probability of each target word using st , we employ a second-level decoder
which generates a character sequence based on st .
We proposed a variant of the gate recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) that used
in the second-level decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter
3
Under review as a conference paper at ICLR 2017
and Schmidhuber, 1997) units instead of the GRU described here). HGRU has a settable state and
generates character sequence based on the given state until generating a delimiter. In our model, the
state is initialized by the output of the first-level decoder. Once HGRU generates a delimiter, it will
set the state to the next output of the first-level decoder. Given the previous output character sequence
{y0, y1, . . . , yt-1} where y0 is a token representing the start of sentence, and the auxiliary sequence
{a0, a1, . . . , at-1} which only contains 0 and 1 to indicate whether yi is a delimiter (a0 is set to 1),
HGRU updates the state as follows:
gt-1 = (1 - at-1)gt-1 + at-1 sit,	(2)
qtj = σ([Wqe(yt-1)]j + [Uqgt-1]j),	(3)
ztj = σ([Wz e(yt-1)]j + [Uzgt-1]j),	(4)
gj = Φ([We(yt-i)]j + [U(qt Θ gt-i)]j),	(5)
gj = Zjgj-I + (I- Zt )gj,	⑹
where sit is the output of the first-level decoder which calculated as Eqn. (8). We can compute the
probability of each target character yt based on gt with a softmax function:
p(yt | {y1, . . . ,yt-1},x) = softmax(gt).	(7)
The current problem is that the number of outputs of the first-level decoder is much fewer than the
target character sequence. It will be intractable to conditionally pick outputs from the the first-level
decoder when training in batch manner (at least intractable for Theano (Bastien et al., 2012) and
other symbolic deep learning frameworks to build symbolic expressions). Luong and Manning (2016)
uses two forward passes (one for word-level and another for character-level) in batch training which
is less efficient. However, in our model, we use a matrix to unfold the outputs of the first-level
decoder, which makes the batch training process more efficient. It is a Ty × T matrix R, where Ty is
the number of delimiter (number of words) in the target character sequence and T is the length of
the target character sequence. R[i, j1 + 1] to R[i, j2] are set as 1 if j1 is the index of the (i-1)-th
delimiter and j2 is the index of the i-th delimiter in the target character sequence. The index of the
0-th delimiter is set as 0. For example, when the target output is “go」_” and the output of the
first-level decoder is [si, s], the unfolding step will be:
1
0
0
1
0
1
1
0
[s1, s1, s1, s2, s2],
therefore {si1 , si2, si3, si4, si5} is correspondingly set to {s1, s1, s1, s2, s2} in HGRU iterations.
After this procedure, we can compute the probability of each target character by the second-level
decoder according to Eqns. (2) to (7).
3.3	Model Architectures
There are totally six recurrent neural networks in our model, which can be divided into four layers as
shown in Figure 2. Figure 2 illustrates the training procedure of a basic deep character-level neural
machine translation. It is possible to use multi-layer recurrent neural networks to make the model
deeper. The first layer is a source word encoder which contains two RNNs as shown in Figure 1. The
second layer is a bidirectional RNN sentence encoder which is identical to that of (Bahdanau et al.,
2015). The third layer is the first-level decoder. It takes the representation of previous target word
as a feedback, which is produced by the target word encoder in our model. As the feedback is less
important, we use an ordinary RNN to encode the target word. The feedback rYt-1 then combines the
previous hidden state ut-1 and the context ct from the sentence encoder to generate the vector st:
st = W1ct + W2rYt-1 + W3ut-1 + b.	(8)
With the state of HGRU in the second-level decoder setting to st and the information of previous
generated character, the second-level decoder generates the next character until generating an end of
sentence token (denoted as </s> in Figure 2). With such a hierarchical architecture, we can train our
character-level neural translation model perfectly well in an end-to-end fashion.
4
Under review as a conference paper at ICLR 2017
HGRU.
HGRU.
HGR uɪ
>
HGR u
>
HGR u
HGR
HGR U 不
H
Second-level Decoder
matrix R
A
Source Word Encoder
Figure 2: Deep character-level neural machine translation. The HGRUs with red border indicate that
the state should be set to the output of the first-level decoder.
3.4 Generation Procedure
We first encode the source sequence as in the training procedure, then we generate the target sequence
character by character based on the output st of the first-level decoder. Once we generate a delimiter,
we should compute next vector st+1 according to Eqn. (8) by combining feedback rYt from the target
word encoder, the context ct+1 from the sentence encoder and the hidden state ut . The generation
procedure will terminate once an end of sentence (EOS) token is produced.
4	Experiments
We implement the model using Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van
Merrienboer et al., 2015), the source code and the trained models are available at github 1. We train
our model on a single GTX Titan X with 12GB RAM. First we evaluate our model on English-to-
French translation task where the languages are morphologically poor. For fair comparison, we
use the same dataset as in RNNsearch which is the bilingual, parallel corpora provided by ACL
WMT’14. In order to show the strengths of our model, we conduct on the English-to-Czech and
Czech-to-English translation tasks where Czech is a morphologically rich language. We use the same
dataset as (Chung et al., 2016a; Lee et al., 2016) which is provided by ACL WMT’152.
4.1	Dataset
We use the parallel corpora for two language pairs from WMT: En-Cs and En-Fr. They consist of
15.8M and 12.1M sentence pairs, respectively. In terms of preprocessing, we only apply the usual
tokenization. We choose a list of 120 most frequent characters for each language which coveres nearly
100% of the training data. Those characters not included in the list are mapped to a special token
1 https://github.com/SwordYork/DCNMT
2http://www.statmt.org/wmt15/translation-task.html
5
Under review as a conference paper at ICLR 2017
(<unk>). We use newstest2013(Dev) as the development set and evaluate the models on newstest2015
(Test). We do not use any monolingual corpus.
4.2	Training Details
We follow (Bahdanau et al., 2015) to use similar hyperparameters. The bidirectional RNN sentence
encoder and the hierarchical decoder both consists of two-layer RNNs, each has 1024 hidden units;
We choose 120 most frequent characters for DCNMT and the character embedding dimensionality is
64. The source word is encoded into a 600-dimensional vector. The other GRUs in our model have
512 hidden units.
We use the ADAM optimizer (Kingma and Ba, 2015) with minibatch of 56 sentences to train each
model (for En-Fr we use a minibatch of 72 examples). The learning rate is first set to 10-3 and then
annealed to 10-4 .
We use a beam search to find a translation that approximately maximizes the conditional log-
probability which is a commonly used approach in neural machine translation (Sutskever et al., 2014;
Bahdanau et al., 2015). In our DCNMT model, it is reasonable to search directly on character level to
generate a translation.
5	Result and Analysis
We conduct comparison of quantitative results on the En-Fr, En-Cs and Cs-En translation tasks in
Section 5.1. Apart from measuring translation quality, we analyze the efficiency of our model and
effects of character-level modeling in more details.
5.1	Quantitative Results
We illustrate the efficiency of the deep character-level neural machine translation by comparing with
the bpe-based subword model (Sennrich et al., 2016) and other character-level models. We measure
the performance by BLEU score (Papineni et al., 2002).
Table 1: BLEU scores of different models on three language pairs.
	Model	Size	Src Trgt	Length	Epochs Days	Dev Test
	bpe2bpe⑴	-	bpe bpe	50 50	--	26.91 ^^29.70
	-C2W⑵	~ 54 M	char char	300^^300^	~ 2.8^^~ 27	25.89^^27.04
	-CNMT	~ 52 M	char char	300	300	~ 3.8^^~ 21	28.19^^29.38
	DCNMT	~ 54 M	char char	300 300	1	~ 7 ~ 2.8	~ 19	27.02^^28.13 29.31 30.56
	bpe2bpe⑴	-	bpe bpe	50 50	--	15.90^^13.84
	bpe2char⑶	-	bpe char	50 500	--	-	16.86
	Char⑸	-	char char	600 600	->4	~ 90	-	17.5
	hybrid⑸	~ 250 M	hybrid hybrid	50 50	->4	~ 21	-	19.6
	DCNMT	~ 54 M	char char	450 450	1	~ 5 ~ 2.9	~ 15	15.50^^14.87 17.89	16.96
	bpe2bpe⑴	-	bpe bpe	50 50	--	21.24^^20.32
	bpe2char⑶	~ 76 M	bpe char	50 500	~ 6.1	~ 14	23.27^^22.42
	char2char⑷	~ 69 M	char char	450 450	~ 7.9	~ 30	23.38 22.46
	DCNMT	~ 54 M	char char	450 450	1	~ 5 ~ 4.6	~ 22	20.50^^19.75 23.24 22.48
In Table 1, “Length” indicates the maximum sentence length in training (based on the number of
words or characters), “Size” is the total number of parameters in the models. We report the BLEU
6
Under review as a conference paper at ICLR 2017
scores of DCNMT when trained after one epoch in the above line and the final scores in the following
line. The results of other models are taken from (1)Firat et al. (2016), (3)Chung et al. (2016a), (4)Lee
et al. (2016) and (5)Luong and Manning (2016) respectively, except (2) is trained according to Ling
et al. (2015b). The only difference between CNMT and DCNMT is CNMT uses an ordinary RNN
to encode source words (takes the last hidden state). The training time for (3) and (4) is calculated
based on the training speed in (Lee et al., 2016). For each test set, the best scores among the models
per language pair are bold-faced. Obviously, character-level models are better than the subword-level
models, and our model is comparable to the start-of-the-art character-level models. Note that, the
purely character model of (5)(Luong and Manning, 2016) took 3 months to train and yielded +0.5
BLEU points compared to our result. We have analyzed the efficiency of our decoder in Section 3.2.
Besides, our model is the simplest and the smallest one in terms of the model size.
5.2	Learning Morphology
1.5
1
0.5
0
-0.5
-1
-1.5
-2
■— reliable
，—solvable
J flexible J reliability
■一 solvabili j flexibility
J CaJanl⅛able
j JpaotatyIity
J possible
J possibility
-2	-1.5	-1	-0.5	0	0.5	1	1.5	2
2
1.5
1
0.5
0
-0.5
-1
-1.5
-2
—notable
■— capable
■— capability
—possi蜂Ossibility
—notability
—solvable
■— solvability
一 目rabability
—Hexi慢Xibility
-2	-1.5	-1	-0.5	0	0.5	1	1.5	2
(a) ordinary RNN word encoder
(b) our word encoder
Figure 3:	Two-dimensional PCA projection of the 600-dimensional representation of the words.
In this section, we investigate whether our model could learn morphology. First we want to figure out
the difference between an ordinary RNN word encoder and our word encoder. We choose some words
with similar meaning but different in morphology as shown in Figure 3. We could find in Figure
3(a) that the words ending with “ability”, which are encoded by the ordinary RNN word encoder, are
jammed together. In contrast, the representations produced by our encoder are more reasonable and
the words with similar meaning are closer.
0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0.15
0.1
0.05
0
-0.05
-0.1
-0.15
■— anybody
J anyone
JanyWthyng
J anywhere
—everyway
J everything
J everywhere
-0.2	-0.15	-0.1	-0.05	0	0.05	0.1	0.15	0.2
(b) two-dimensional PCA projection
Figure 4:	The learnt morphemes
Then we analyze how our word encoder learns morphemes and the rules of how they are combined.
We demonstrate the encoding details on “any*” and “every*”. Figure 4(a) shows the energy of each
character, more precisely, the energy of preceding characters. We could see that the last character
of a morpheme will result a relative large energy (weight) like “any” and “every” in these words.
Moreover, even the preceding characters are different, it will produce a similar weight for the same
morpheme like “way” in “anyway” and “everyway”. The two-dimensional PCA projection in Figure
7
Under review as a conference paper at ICLR 2017
4(b) further validates our idea. The word encoder may be able to guess the meaning of “everything”
even it had never seen “everything” before, thus speedup learning. More interestingly, we find that
not only the ending letter has high energy, but also the beginning letter is important. It matches the
behavior of human perception (White et al., 2008).
Peak
energy
Peak
energy
Peak
energy
Figure 5: Subword-level boundary detected by our word encoder.
Moreover, we apply our trained word encoder to Penn Treebank Line 1. Unlike Chung et al. (2016b),
we are able to detect the boundary of the subword units. As shown in Figure 5, “consumers”,
“monday”, “football” and “greatest” are segmented into “consum-er-s”,“mon-day”, “foot-ball” and
“great-est” respectively. Since there are no explicit delimiters, it may be more difficult to detect the
subword units.
5.3 Benefiting from learning morphology
As analyzed in Section 5.2, learning morphology could speedup learning. This has also been shown
in Table 1 (En-Fr and En-Cs task) from which we see that when we train our model just for one
epoch, the obtained result even outperforms the final result with bpe baseline.
Another advantage of our model is the ability to translate the misspelled words or the nonce words.
The character-level model has a much better chance recovering the original word or sentence. In
Table 2, we list some examples where the source sentences are taken from newstest2013 but we
change some words to misspelled words or nonce words. We also list the translations from Google
translate 3 and online demo of neural machine translation by LISA.
Table 2: Sample translations.
(a) Misspelled words
Source	For the time being howeve their research is UnConCIUSiVe.
Reference	LeUrs recherches ne Sont toutefois Pas ConclUantes PoUr l'instαnt.
Google translate	PoUr le moment, leurs recherches ne sont PaS concluantes.
LISA	Pour le moment UNK leur recherche est UNK.
DCNMT	Pour le moment, cependant, leur recherche n,est pas concluante.
(b) Nonce words (morphological change)
SoUrce	Then We will be able to supplement the real world with virtual objects in a much convenienter form .
Reference	Ainsi, nous pourrons ComPleter le monde r6el par des objets virtuels dans une forme plus pratique .
Google translate	Ensuite, nous serons en mesure de ComPleter le monde r6el avec des objets virtuels dans une forme beaucoup plus pratique.
LISA	Ensuite, nous serons en mesure de completer le vrai monde avec des objets virtuels sous une forme bien UNK.
DCNMT	Ensuite, nous serons en mesure de completer le monde reel avec des objets virtuels dans une forme beaucoup plus pratique.
As listed in Table 2(a), DCNMT is able to translate out the misspelled words correctly. For a
word-based translator, it is never possible because the misspelled words are mapped into <unk>
3The translations by Google translate were made on Nov 4, 2016.
8
Under review as a conference paper at ICLR 2017
token before translating. Thus, it will produce an <unk> token or just take the word from source
sentence (Gulcehre et al., 2016; Luong et al., 2015). More interestingly, DCNMT could translate
“convenienter” correctly as shown in Table 2(b). By concatenating “convenient” and “er”, we get the
comparative adjective form of “convenient” which never appears in the training set; however, our
model guessed it correctly based on the morphemes and the rules.
6 Conclusion
In this paper we have proposed an hierarchical architecture to train the deep character-level neural
machine translation model by introducing a novel word encoder and a multi-leveled decoder. We have
demonstrated the efficiency of the training process and the effectiveness of the model in comparison
with the word-level and other character-level models. The BLEU score implies that our deep character-
level neural machine translation model likely outperforms the word-level models and is competitive
with the state-of-the-art character-based models. It is possible to further improve performance by
using deeper recurrent networks (Wu et al., 2016), training for more epochs and training with longer
sentence pairs.
As a result of the character-level modeling, we have solved the out-of-vocabulary (OOV) issue that
word-level models suffer from, and we have obtained a new functionality to translate the misspelled or
the nonce words. More importantly, the deep character-level is able to learn the similar embedding of
the words with similar meanings like the word-level models. Finally, it would be potentially possible
that the idea behind our approach could be applied to many other tasks such as speech recognition
and text summarization.
References
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems, pages 3104-3112, 2014.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing, 2014.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. International Conference on Learning Representation, 2015.
SebaStien Jean Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large
target vocabulary for neural machine translation. Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics, 2015.
Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. A character-level decoder without explicit
segmentation for neural machine translation. Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics, 2016a.
Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointing the
unknown words. Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics, 2016.
Minh-Thang Luong, Ilya Sutskever, Quoc V Le, Oriol Vinyals, and Wojciech Zaremba. Addressing
the rare word problem in neural machine translation. Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics, 2015.
Minh-Thang Luong and Christopher D Manning. Achieving open vocabulary neural machine
translation with hybrid word-character models. Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics, 2016.
Wang Ling, Tiago Luis, LUiS Marujo, Ram6n Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. Finding function in form: Compositional character models for open
vocabulary word representation. Empirical Methods in Natural Language Processing, 2015a.
9
Under review as a conference paper at ICLR 2017
Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W Black. Character-based neural machine
translation. arXiv preprint arXiv:1511.04586, 2015b.
Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. Hierar-
chical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808,
2015.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics, 2016.
Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language
models. Association for the Advancement of Artificial Intelligence, 2016.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation
without explicit segmentation. arXiv preprint arXiv:1610.03017, 2016.
Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673-2681,1997.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Frederic Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud
Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and sPeed imProvements.
DeeP Learning and UnsuPervised Feature Learning NIPS 2012 WorkshoP, 2012.
James Bergstra, Olivier Breuleux, Frederic Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume
Desjardins, JosePh Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU
math exPression comPiler. In Proceedings of the Python for Scientific Computing Conference
(SciPy), June 2010. Oral Presentation.
Bart van Merrienboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley,
Jan Chorowski, and Yoshua Bengio. Blocks and fuel: Frameworks for deeP learning. arXiv
preprint arXiv:1506.00619, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. International
Conference on Learning Representation, 2015.
Kishore PaPineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. Pages 311-318. Association for ComPutational Linguistics,
2002.
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine translation
with a shared attention mechanism. In Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies., 2016.
Sarah J White, Rebecca L Johnson, Simon P Liversedge, and Keith Rayner. Eye movements when
reading transPosed text: the imPortance of word-beginning letters. Journal of Experimental
Psychology: Human Perception and Performance, 34(5):1261, 2008.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.
arXiv preprint arXiv:1609.01704, 2016b.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation sys-
tem: Bridging the gaP between human and machine translation. arXiv preprint arXiv:1609.08144,
2016.
10
Under review as a conference paper at ICLR 2017
A Detailed description of the model
Here we describe the implementation using Theano, it should be applicable to other symbolic deep
learning frameworks. We use f to denote the transition of the recurrent network.
A.1 Source word encoder
As illustrated in Section 3.1, the word encoder is based on two recurrent neural networks. We compute
the representation of the word ‘anyone’ as
6
ranyone = tanh(	wtrt),
t=1
where rt ∈ Rn is an RNN hidden state at time t, computed by
rt = f(e(xt), rt-1).
Each rt contains information about the preceding characters. The weight wt of each representation
rt is computed by
wt = exp(Wwht + bw),
where Ww ∈ R1×2l maps the vector ht ∈ R2l to a scalar and ht is the state of the BiRNN at time t:
"→t #
tt.
-→h t ∈ Rl is the forward state of the BiRNN which is computed by
-→h t = f(e(xt), -→ht-1).
The backward state Irt ∈ Rl is computed similarly, however in a reverse order.
(9)
(10)
A.2 Source sentence encoder
After encoding the words by the source word encoder, we feed the representations to the
source sentence encoder. For example, the source “Hello world </s>” is encoded into a vector
[rHello, rworld, r</s>], then the BiRNN sentence encoder encodes this vector into [v1, v2, v3]. The com-
putation is the same as Eqn. (9) and Eqn. (10), however the input now changes to the representation
of the words.
A.3 First-level decoder
The first-level decoder is similar to Bahdanau et al. (2015) which utilizes the attention mechanism.
Given the context vector ct from encoder, the hidden state ut ∈ Rm of the GRU is computed by
Ut = (1 - Zt) ◦ ut-ι + Zt ◦ Ut,
where
Ut = tanh(WrYt-I + U[qt ◦ ut-ι] + Cct)
Zt = σ(WzrYt-1 + UzUt-1 + Czct)
qt = σ(WqrYt-1 + UqUt-1 + Cqct).
rYt-1 is the representation of the target word which is produced by an ordinary RNN (take the last
state). The context vector ct is computed by the attention mechanism at each step:
Tx
ct =	αtjvj ,
j=1
11
Under review as a conference paper at ICLR 2017
where
=exp(etj)
P	ΣT= I exp(etfc)
etj = E tanh(Weut-1 +Hehj).
E ∈ R1×m which maps the vector into a scalar. Then the hidden state ut is further processed as
Eqn. (8) before feeding to the second-level decoder:
st+1 =	W1	ct+1 +W2rYt	+W3ut +b.
A.4 Second-level decoder
As described in Section 3.2, the number of outputs of the first-level decoder is much fewer than the
target character sequence. It will be intractable to conditionally pick outputs from the the first-level
decoder when training in batch manner (at least intractable for Theano (Bastien et al., 2012) and other
symbolic deep learning frameworks to build symbolic expressions). We use a matrix R ∈ RTy×T
to unfold the outputs [s1 , . . . , sTy] of the first-level decoder (Ty is the number of words in the target
sentence and T is the number of characters). R is a symbolic matrix in the final loss, it is constructed
according the delimiters in the target sentences when training (see Section 3.2 for the detailed
construction, note that R is a tensor in batch training. ). After unfolding, the input of HGRU becomes
[si1 , . . . , siT], that is
[si1 , . . . , siT] = [s1 , . . . , sTy]R.
According to Eqns.(2) to (7), we can compute the probability of each target character :
p(yt | {y1 , . . . , yt-1 }, x) = softmax(gt).
Finally, we could compute the cross-entroy loss and train with SGD algorithm.
B S ample translations
We show additional sample translations in the following Tables.
	Table 3: SamPle translations of En-Fr.	
Source	This " disturbance " Produces an electromagnetic wave ( of light , infrared , ultraviolet etc . ) , and this wave is nothing other than a Photon - and thus one of the " force carrier " bosons .
Reference	QUand, en effet, Une particule ayant Une charge Clectrique acc6lere ou change de direction , cela " derange "le champ electromagnetique en cet endroit prCcis , Un peU comme Un cailloU lancC dans Un Ctang .
DCNMT	Lorsque , en fait , une particule ayant une charge electrique accelere ou change de direction , cela " perturbe " le champ electromagnetique dans cet endroit SPeCifiqUe , plutðt comme un galetjetC dans un Ctang .
Source	Since October , a manifesto , signed by palliative care lUminaries inclUd- ing Dr BalfoUr MoUnt and Dr Bernard Lapointe , has been circUlating to demonstrate their opposition to sUch an initiative .
Reference	DepUis le mois d’ octobre , Un manifeste , signe de sommites des soins palliatifs dont le Dr BalfoUr MoUnt et le Dr Bernard Lapointe , circUle pour tCmoigner de leur opposition a une telle initiative .
DCNMT	DepUis octobre , Un manifeste , signe par des liminaires de soins palliatifs , dont le Dr Balfour Mount et le Dr Bernard Lapointe , a circule pour demontrer leur opposition a une telle initiative .
12
Under review as a conference paper at ICLR 2017
	Table 4: Sample translations of En-Cs.
Source	French troops have left their area of responsibility in Afghanistan ( Kapisa and Surobi ) .
Reference	FranCOUzskejednOtky opustily svou oblast odpovednosti V Afghanistanu ( Kapisa a Surobi ) .
DCNMT	Francouzske jednotky opustily svou oblast odpovednosti V Afghanistanu ( Kapisa a Surois ) .
Source	" All the guests were made to feel important and loved " recalls the top model , who started working with him during Haute Couture Week Paris ,in 1995 .	
Reference	Vsichni POzVani se d^ky nemu mohli c^tit dule为ti a milovani," VzPOmina top modelka, ktera S nιm zacala pracovat V PrUbehU Parizskeho tydne vrcholne m6dy V roce 1995 .
DCNMT	"Vsichni hoste byli provedeni , aby se cιtili dulezitι a milovanι " PriPOmina nejvyssι model, ktery S nιm zacal pracovat V PrUbehU ty- denfku Haute Coutupe V Paffzi V roce 1995 .
Source	" There are so many priVate weapons factories now , which do not endure competition on the international market and throw weapons from under the counter to the black market , including in Moscow , " says the expert .
Reference	"V SOUCaSnOSti vznikajι soukrome zbrojafske podniky , ktere nejsou konkurenceschopne na mezindrodnιm trhu , a vyfazujι Zbrane , ktere doddvaji na Cerny trh VCetne Moskvy ," rfkd tento odbornιk .
DCNMT	"V SOUCaSnOSti existuje tolik soukromych zbranι , ktere nevydrzι hospodarskou SOUteZ na mezindrodnim trhu a hod^ zbrane pod pultem k CernemU trhu , VCetne Moskvy ," rfka odbornιk .
	Table 5: Sample translations of Cs-En.
Source	Prezident Karzai nechce zahraniCn^ kontroly , zejmena ne pfi pffleZitOSti voleb planovanych na duben 2014 .
Reference	President Karzai does not want any foreign controls , particularly on the occasion of the elections in April 2014 .
DCNMT	President Karzai does not want foreign controls , particularly in the opportunity of elections planned on April 2014 .
Source	Manzelsky par mefl dvef defti , Prestona a Heidi , a dlouhou dobu zili v kalifornskem mefstef Malibu , kde pobyva mnoho celebrit .
Reference	The couple had two sons , Preston and Heidi , and lived for a long time in the Californian city Malibu , home to many celebrities .
DCNMT	The married couple had two children , Preston and Heidi , and long lived in the California city of Malibu , where many celebrities resided .
Source	Trestny Cin rouhðni je zachovan a UrdZkaje nadale Zakazana , coz by mohlo mιt VdZne dUsledky pro svobodu VyjadfOVdni, zejmena pak pro tisk .
Reference	The offence of blasphemy is maintained and insults are now prohibited , which could have serious consequences on freedom of expression , particularly for the press .
DCNMT	The criminal action of blasphemy is maintained and insult is still prohib- ited , which could have serious consequences for freedom of expression , especially for the press .
13