Under review as a conference paper at ICLR 2017
An Information Retrieval Approach for Find-
ing Dependent Subspaces of Multiple Views
Ziyuan Lin & Jaakko Peltonen *
Department of Computer Science, Aalto University, Finland, and
School of Information Sciences, University of Tampere, Finland
{ziyuan.lin,jaakko.peltonen}@uta.fi
Ab stract
Finding relationships between multiple views of data is essential both in ex-
ploratory analysis and as pre-processing for predictive tasks. A prominent ap-
proach is to apply variants of Canonical Correlation Analysis (CCA), a classical
method seeking correlated components between views. The basic CCA is re-
stricted to maximizing a simple dependency criterion, correlation, measured di-
rectly between data coordinates. We introduce a new method that finds dependent
subspaces of views directly optimized for the data analysis task of neighbor re-
trieval between multiple views. We optimize mappings for each view such as lin-
ear transformations to maximize cross-view similarity between neighborhoods of
data samples. The criterion arises directly from the well-defined retrieval task, de-
tects nonlinear and local similarities, measures dependency of data relationships
rather than only individual data coordinates, and is related to well understood
measures of information retrieval quality. In experiments the proposed method
outperforms alternatives in preserving cross-view neighborhood similarities, and
yields insights into local dependencies between multiple views.
1	Introduction
Finding dependent subspaces across views (subspaces where some property of data is statistically
related or similar across views) is a common data analysis need, where Canonical Correlation Anal-
ysis (CCA) (Hotelling, 1936) is a standard unsupervised tool. Preprocessing to find dependent
subspaces is useful both for prediction and for analysis: in predictive tasks, such subspaces help
if non-dependent parts of each view may arise from noise and distortions. In some data analysis
tasks, finding the dependent subspaces may itself be the main goal; for example in bioinformatics
domains dependency seeking projections have been used to identify relationships between different
views of cell activity (Tripathi et al., 2008; Klami et al., 2013); in signal processing a similar task
could be identifying optimal filters for dependent signals of different nature, e.g., speech and the
corresponding tongue movements of the speakers as in Westbury (1994).
Methods like CCA maximize simple correlations between data point coordinate features across the
projected subspaces. However, in many data domains the coordinates may not be of main interest
but rather the data relationships that they reveal. It is then of great interest to develop dependency
seeking methods that directly focus on the data relationships. For example, consider a database of
scientists, defined in one view by their level of interest in various research topics, and in another
view by their level of interest in various hobbies. In a database like this, finding relationships of
people is the common interest, e.g. to find nearest colleagues for a scientist, having the most similar
(neighboring) research interests; or to find hobby partners having the most similar (neighboring)
hobby interests; the question is then, can we predict the research colleagues from hobby partners
or vice versa? Research topics and hobbies are very dissimilar views, and not all of their variation
will be related, but we can try to find subspaces of research and hobby interests, so that research
neighbors and hobby neighbors are as highly related as possible in those subspaces.
In this paper we propose a method that solves this task: we present a novel method for seeking
dependent subspaces across multiple views, preserving neighborhood relationships of data. Our
* Ziyuan Lin and Jaakko Peltonen contributed equally to the paper.
1
Under review as a conference paper at ICLR 2017
method directly maximizes the between-view similarity of neighborhoods of data samples, a natural
measure for similarity of data relationships among the views. The method detects nonlinear and
local dependencies, has strong invariance properties, is related to an information retrieval task of the
analyst, and performs well in experiments.
Relating data items is one of the main elementary tasks in Shneiderman’s taxonomy of tasks in visual
data analysis (Shneiderman, 1996). Our method is optimized for finding related (neighboring) data
items, formulated as optimizing an information retrieval task. Since our method directly serves the
task of relating data items (across views) in Shneiderman’s taxonomy, in this sense it arguably comes
closer to needs of data analysts than maximizing some variant of coordinate correlation.
We find linear projections (linear subspaces) of views. Linear projections have advantages of sim-
plicity and easy interpretability with respect to original data features. Even if projections are linear,
the dependency criterion we optimize is flexible and detects nonlinear dependencies across views.
We present our method in Section 2, properties and extensions in Section 3, related work in Section
4, experiments in Section 5, and conclusions in Section 6.
2	Method: Dependent Neighborhoods of Views
Our method finds similar neighborhood relationships between views. We define the neighborhood
relationships and then discuss how to measure their cross-view similarity. Instead of hard neighbor-
hoods where two points are or are not neighbors, we use more realistic probabilistic neighborhoods.
Assume input data items xi = (xi,1, . . . , xi,NViews) have paired features xi,V in each view V. We con-
sider transformations of each view by a mapping fV which is typically a dimensionality reducing
transformation to a subspace of interest; in this paper, for simplicity and interpretability we use lin-
ear mappings fV (xi,V) = WVTxi,V with WVT ∈ Rdimlow(V)×dimorig(V) as the to-be-optimized parameters,
where dimorig(V) and dimlow(V) are the number of dimensions of V and its subspace respectively.
The local neighborhood of a data item i in any transformation of view V can be represented by the
conditional probability distribution pi,V = {pV(j|i; fV)} where pV ( j|i; fV) tells the probability that
data item j 6= i is picked as a representative neighbor of i; that is, the probability that an analyst who
inspected item i will next pick j for inspection. The pV(j|i; fV) can be defined in several ways, as a
decreasing function of distance dV (i, j; fV) between features of i and j in view V. Here we define it
by a simple exponential falloff with respect to squared distance of i and j, as
PV(j|i； fv) = exP(-dV(i,j; fv"σ2V) ∙ (∑exP(-dV(i,k； fv"σ2V))-I	⑴
k6=i
where σi,V sets the falloff rate around i in the view. We tried two simple ways to set the σi,V : one is
as a fraction of maximal pairwise distance so σi,v = 0.05 ∙ maxj,k kXj,v - XkV k, or alternatively, set
σi,V = (dimlow(V)/dimOrig(V))1/2 ∙meanj,ι=kNN(j)kxιV -Xj,v∣∣	(2)
i.e., calculate the average distance between Xj,V and its k-th nearest neighbor Xl,V, then give the
average a heuristic correction factor，dimlow (V) /dimorig (V) since the average distance is obtained
from the original space yet σi,V is used in a subspace. We use the first simple σi,V for artificial data
experiments and the more data-driven second σi,V from (2) with k = 5 for the other experiments.
Both choices give good results. Other local choices to e.g. achieve a desired entropy are possible,
see Venna et al. (2010). With linear mappings the probabilities become
PV(j|i; fv) = exp(-k/(xi,v -χj,v)k^σ2v) ∙ (∑exp(-kWvT(Xi,v -xk,v)k^σ2V))-1	⑶
k6=i
where the matrix WV defines the subspace of interest for the view and also the distance metric within
the subspace. Our method learns the mapping parameters WV for each view.
2.1 Comparison of Neighborhoods Across Views
Neighborhoods represented as probability distributions can be compared by difference measures.
We discuss two measures for different purposes, and their information retrieval interpretations.
2
Under review as a conference paper at ICLR 2017
Kullback-Leibler divergence. For two distributions p = {p(j)} and q = {q( j)}, the Kullback-
Leibler (KL) divergence is an information-theoretic asymmetric difference measure defined as
DKL(p,q) = ∑ p( j)(log p( j)/q( j)) .	(4)
j
The KL divergence is nonnegative and zero if and only if p = q. Traditionally it is interpreted to
measure the amount of extra coding length needed when coding examples with codes generated for
distribution q when the samples actually come from distribution p. We treat views symmetrically
and compute the symmetrized divergence (DKL(p,q) +DKL(q,p))/2.
KL divergence is related to an information retrieval criterion: DKL ( p, q) is the cost of misses in
information retrieval of neighbors, when neighbors are retrieved using retrieval distribution q but
they actually follow distribution p. DKL ( p, q) is also the cost of false neighbors when neighbors are
retrieved using p but they actually follow q. The relationships were shown in Venna et al. (2010)
and used to compare a reduced-dimensional neighborhood to an original one; we use it in a novel
way to compare neighborhoods across (transformed) views of data. The symmetrized divergence is
the total cost of both misses and false neighbors when neighbors following the distribution in one
transformed view are retrieved from the other transformed view with its neighbor distribution.
The value of the KL divergence can depend highly on differences between individual probabilities
p( j) and q( j). A single missed neighbor can yield a high divergence value: for any index j if
p( j) > ε for some ε > 0, DKL ( p, q) → ∞ as q( j) → 0. In real-life multi-view data differences
between views may be unavoidable, so we prefer a less strict measure focusing more on overall
similarity of neighborhoods than severity of individual misses. We discuss such a measure below.
Angle cosine. A simple similarity measure between discrete distributions is the angle cosine be-
tween the distributions as vectors, that
is, Cos(P,q) = (∑jP(j)q(j))/ J(∑j(P(j))2)(∑j(q(j))2),
which can be seen as the Pearson correlation coefficient between elements of P and q; it is thus a
neighborhood correlation—a neighborhood based analogue of the coordinate correlation cost func-
tion of CCA.1 The angle cosine is bounded above and below: it has highest value 1 if and only if
P = q and lowest value 0 if supports ofP and q are nonoverlapping.
Similarity of neighborhoods by itself is not enough. The KL divergence and angle cosine (neigh-
borhood correlation) measures only compare similarity of neighborhoods but not potential useful-
ness of the found subspaces. In high-dimensional data it is often possible to find subspaces where
neighborhoods are trivially similar. For example, in data with sparse features it is often possible to
find two dimensions where all data is reduced to a single value; in such dimensions neighborhood
distributions would become uniform across all data since, hence any two such dimensions appear
similar. To avoid discovering trivial similarities we wish to complement the measures of similarity
between neighborhoods with terms favoring nontrivial (sparse) neighborhoods. A simple way to
prefer sparse neighborhoods is to omit the normalization from neighborhood correlation, yielding
Sim(P,q) = ∑P(j)q(j)	(5)
j
which is the inner product between the vectors of neighborhood probabilities. Unlike Cos(P, q),
Sim( P, q) favors sparse neighborhoods: it has highest value 1 if and only ifP = q andP(j) = q(j) = 1
for only one element j, and lowest value 0 if the supports of P and q are nonoverlapping.
The information retrieval interpretation is: Sim(P, q) is a proportional count of true neighbors from
P retrieved from q or vice versa. If P has K neighbors with near-uniform high probabilities P(j) ≈
1/K and other neighbors have near-zero probabilities, and q has L neighbors with high probability
q(j) ≈ 1/L, then Sim( P, q) ≈ M/KL where M is the number of neighbors for which both P and
q are high (retrieved true neighbors). Thus Sim(P, q) rewards matching neighborhoods and favors
sparse neighborhoods (small K and L). One advantage of this formulation is to avoid matching two
neighborhoods that seem to match only because they are highly uninformative: for example if P and
q are both uniform over all neighbors, they have the same probability values and would be “similar”
in a naive comparison of probability values, but both are actually simply uninformative about the
choice of neighbors. Sim(P, q) would prefer a more sparse, more informative match, as desired.
1To make the connection exact, typically correlation is computed after substracting the mean from coordi-
nates; for neighbor distributions of n data items, the mean neighborhood probability is the data-independent
value 1/(n - 1)2 which can be substracted from each sum term if an exact analogue to correlation is desired.
3
Under review as a conference paper at ICLR 2017
2.2 Final Cost and Optimization Technique
We wish to evaluate similarity of neighborhoods between subspaces of each view, and optimize the
subspaces to maximize the similarity, while favoring subspaces having sparse (informative) neigh-
borhoods for data items. We then evaluate similarities as Sim(pi,V, pi,U) where pi,V = {pV(j|i; fV)}
is the neighborhood distribution around data item i in the dependent subspace of view V and fV is the
mapping (parameters) of the subspace, and pi,U = {pU(j|i; fU)} is the corresponding neighborhood
distribution in the dependent subspace of view U having the mapping fU. As the objective function
for finding dependent projections, we sum the above over each pair of views (U, V) and over the
neighborhoods of each data item i, yielding
Nviews Nviews Ndata Ndata
C(f1,...,fNviews) = ∑	∑	∑ ∑ pV(j|i;fV)pU(j|i;fU)	(6)
V=1 U=1,U6=V i=1 j=1, j6=i
where, in the setting of linear mappings and neighborhoods with Gaussian falloffs, pV is defined by
(3) and is parameterized by the projection matrix WV of the linear mapping.
Optimization. The function C( f1 , . . . , fNviews ) is a well-defined objective for dependent projections
and can be maximized with respect to the projection matrices WV of each view. We use gradient
techniques for optimization, specifically limited memory Broyden-Fletcher-Goldfarb-Shanno (L-
BFGS). Even with L-BFGS, (6) can be hard to optimize due to several local optima. To find a
good local optimum, we optimize over L-BFGS rounds with a shrinking added penalty driving
the objective away from the worst local optima during the first rounds; we use the optimum in each
round as initialization of the next. For the penalty we use KL divergence based dissimilarity between
neighborhoods, summed over neighborhoods of all data items i and view pairs (U, V), giving
r ff f K NvVs N管 Ndata (DKL(PiV,Pi,u)+ DKL(Pi,u,Pi,v))	G
cPenalty(f1,…,fNViews)= ∑	∑	∑ -----------------2---------------- (7)
V=1 U=1,U6=V i=1	2
which is a function of all mapping parameters and can be optimized by L-BFGS; (7) penalizes severe
misses (pairs (i, j) with nonzero neighborhood probability in one view but near-zero in another)
driving the objective away from bad local optima. In practice KL divergence is too strict about
misses; we use two remedies below.
Bounding KL divergence by neighbor distribution smoothing. To bound the KL divergence, one
way is to give the neighbor distributions (1) a positive lower bound. In the spirit of the well-known
Laplace smoothing in statistics, we revise the neighbor probabilities (1) as
Pv(j|i； fv) = (exp(-dV(i, j； fv)∕σ22v) + ε) ∙ (∑exp(-dV(i, k； fv)∕σ2V) + (Ndata - 1)ε)-1	(8)
k6=i
where ε > 0 is a small positive number. To keep notations simple, we still denote this smoothed
neighbor distribution as Pv(j|i；fv). To avoid over-complicated formulation and for consistency, we
also use this version of neighbor probabilities in our objective function (6), even though the value of
the objective is bounded by itself. We simply set ε = 1e - 6 which empirically works well.
Shrinking the penalty. Even with bounded KL divergence, optimization stages need different
amounts of penalty. At end of optimization, nearly no penalty is preferred, as views may not fully
agree even with the best mapping. We shrink the penalty during optimization； the objective becomes
cTotal(f1,...,fNviews) =c(f1,. ..,fNviews) - γcPenalty( f1,..., fNviews)	(9)
where γ controls the penalty. We initially set γ so the two parts of the objective function are equal for
the initial mappings, c(f1,...,fNviews) = γcPenalty( f1, ...,fNviews), and multiply γ by a small factor
(0.9 in experiments) at the start of each L-BFGS round to yield exponential shrinkage.
Time complexity. We calculate the neighbor distributions for all views, and optimize the objec-
tive function involving each pairs of views, thus the naive implementation takes O(dNd2ataNv2iews)
time, with d the maximal dimensionality among views. Acceleration techniques (Yang et al., 2013；
Van Der Maaten, 2014; Vladymyrov & Carreira-Perpinan, 2014) from neighbor embedding could
be adopted to reduce time complexity of a single view from O(Nd2ata) to O(Ndata log Ndata) or even
O(Ndata). But scalability is not our first concern in this paper, so we use the naive O(Nd2ata) imple-
mentation for calculating the neighbor distributions for each view involved.
4
Under review as a conference paper at ICLR 2017
3	Properties of the Method and Extensions
Information retrieval. Our objective measures success in a neighbor retrieval task of the analyst:
we maximize count of retrieved true neighbors across views, and penalize by severity of misses.
Invariances. For any subspace of any view, (1) and (3) depend only on pairwise distances and are
thus invariant to global translation, rotation, and mirroring of data in that subspace. The cost is
then invariant to differences of global translation, rotation, and mirroring between views and finds
view dependencies despite such differences. If in any subspace the data has isolated subsets (where
data pairs from different subsets have zero neighbor probability) invariance holds for local transla-
tion/rotation/mirroring of the subsets as long as they preserve the isolation.
Dependency is measured between whole subspaces. Unlike CCA where each canonical com-
ponent of one view has a particular correlated pair in the other view, we maximize dependency
with respect to the entire subspaces (transformed representations) of each view, as neighborhoods of
data depend on all coordinates within the dependent subspace. Our method thus takes into account
within-view feature dependencies when measuring dependency. Moreover, dependent subspaces do
not need to be same-dimensional, and in some views we can choose not to reduce dimensionality
but to learn a metric (full-rank linear transformation).
Finding dependent neighborhoods between feature-based views and views external neighbor-
hoods. In some domains, some data views may directly provide neighborhood relationships or
similarities between data items, e.g., friendships in a social network, followerships in Twitter, or
citations between scientific papers. Such relationships or similarities can be used in place of the
feature-based neighborhood probabilities pV ( j|i; fV) above. This shows an interesting similarity to
a method (Peltonen, 2009) used to find similarities of one view to an external neighborhood defini-
tion; our method contains this task as one special case.
Other falloffs. Exponential falloff in (1) and (3) can be replaced with other forms like t-distributed
neighborhoods (van der Maaten & Hinton, 2008). Such replacement preserves the invariances.
Other transformations. Our criterion is extensible to nonlinear transformations in future work; re-
place linear projections by another parametric form, e.g. neural networks, optimize (9) with respect
to its parameters; the transformation can be chosen on a view-by-view basis. Optimization difficulty
of transformations varies; the best form of nonlinear transformation is outside the paper scope.
4	Related Work
In general, multi-view learning (Xu et al., 2013) denotes learning models by leveraging multiple
potentially dependent data views; such models could be built either for unsupervised tasks based on
the features in the views or for supervised tasks that involve additional annotations like categories
of samples. In this paper we concentrate on unsupervised multi-view learning, and our prediction
tasks of interest are predicting neighbors across views.
The standard Canonical Correlation Analysis (CCA) (Hotelling, 1936) iteratively finds component
pairs maximizing correlation between data points in the projected subspaces. Such correlation is
a simple restricted measure of linear and global dependency. To measure dependency in a more
flexible way and handle nonlinear local dependency, linear and nonlinear CCA variants have been
proposed: Local CCA (LCCA, Wei & Xu 2012) seeks linear projections for local patches in both
views that maximize correlation locally, and aligns local projections into a global nonlinear projec-
tion; its variant Linear Local CCA (LLCCA) finds a linear approximation for the global nonlinear
projection; Locality Preserving CCA (LPCCA, Sun & Chen 2007) maximizes reweighted corre-
lation between data coordinate differences in both views. In experiments we compare to the well
known traditional CCA and LPCCA as an example of recent state of the art.
As a more general framework, Canonical Divergence Analysis (Nguyen & Vreeken, 2015) mini-
mizes a general divergence between distributions of data coordinates in the projected subspace.
The methods mentioned above work on data coordinates in the original spaces. There are also
nonlinear CCA variants (e.g., Lai & Fyfe 2000; Bach & Jordan 2003; Verbeek et al. 2003; Andrew
et al. 2013; Wang et al. 2015; Hodosh et al. 2013) for detecting nonlinear dependency between
multiple views. Although some variants above are locality-aware, they introduce locality from the
original space before maximizing correlation or other similarity measures in the low-dimensional
5
Under review as a conference paper at ICLR 2017
subspaces. Since locality in the original space may not reflect locality in the subspaces due to noise
or distortions, such criteria may not be suited for finding local dependencies in subspaces.
The CODE method (Globerson et al., 2007) creates an embedding of co-occurrence data of pairs
of original categorical variables, mapping both variables into a shared space. Our method is not
restricted to categorical inputs - its main applicability is to high-dimensional vectorial data, with
several other advantages. In contrast to CODE, we find dependent subspaces (mappings) from
multiple high-dimensional real-valued data views. Instead of restricting to a single mapping space
we find several mappings, one from each view, which do not need to go into the same space; our
output spaces can even be different dimensional for each view. Unlike CODE our method is not
restricted to maximizing coordinate similarity: we only need to make neighborhoods similar which
is more invariant to various transformations.
The above methods and several in Xu et al. (2013) all maximize correlation or alternative depen-
dency measures between data coordinates across views. As we pointed out, in many domains coordi-
nates are not of main interest but rather the data relationships they reveal; we consider neighborhood
relationships and our method directly finds subspaces having similar neighborhoods.
5	Experiments
We demonstrate our method on artificial data with multiple dependent groups between views, and
three real data sets: a variant of MNIST digits (LeCun & Cortes, 2010), video data, and stock prices.
In this paper we restrict our method to find linear subspaces, important in many applications for
interpretability, and compare with two prominent linear subspace methods for multiple views, CCA
and LPCCA. To our knowledge, no other information retrieval based approaches for finding linear
subspaces is known up to the time when we did the experiment. Future work could compare methods
for nonlinear mappings (Xu et al., 2013) to variants of ours for the same mapping; we do not focus
on the mapping choice, and focus on showing the benefit or our neighborhood based objective.
On the artificial data set, we measure performance by correspondence between found projections and
the ground truth. On the real data we use mean precision-mean recall curves, a natural performance
measure for information retrieval tasks, and a measure for dependency as argued in Section 2.
Experiment on artificial data sets. We generate an artificial data set with 2 views with multiple
dependent groups in each pair of corresponding dimensions as follows. Denote view V(∈ {1, 2})
as X(V) ∈ R5×1000, and its i-th dimension as Xi(V). For each i, we divide the 1000 data points in
that dimension into 20 groups {gi j}2j=0 1 with 50 data points each. For each gij and view V, we let
戈 j，Fijmj + ε ijk (1 ≤ k ≤ 50), with mj 〜N (0,5), εj 〜U [-0.5,0.5] and Fij ∈ {-1,1} a
random variable allowing positive or negative correlation inside the group. We assemble Xj into
X(V) ∈ R5 ×1000, and randomly permute entries of XF) and Xi⑵ in the same way but differently for
different i, to ensure cross-dimension independency. Lastly We perform a PCA between X(1) and
Xj) for each i, to remove cross-dimension correlation. We assemble the resulting XiV) into X(V).
We pursue 2 transformations mapping from the 5D original space to a 1D latent space for each of
the two views. Ground truth projections for both views will then be W(i) = (δi j)5j=1 ∈ R1×5. Results
are in Fig. 1: compared with CCA, our method successfully finds one of the ground truth transfor-
mations (= the 5th one), despite mirroring and scale, recovering the between-view dependency.
We measure performance by correspondence between the found projections and the ground truth
transformation: let W1,W2∈ R1×5 be projections found by a method, define
Corr(W1,W2)=max|W(i)W1T|/kW1k2+|W(i)W2T|/kW2k2/2	(10)
as the correspondence score. High score means good match between the found projections and
ground truth. We repeat the experiment calculating correspondence on 20 artificial data sets gen-
erated in the same way. The table in Figure 1 summarizes the statistics. Our method outperforms
CCA and LPCCA (with k = 5), finding the dependency on all 20 data sets.
6
Under review as a conference paper at ICLR 2017
2 weiv ni 5 mid
dim 5 in view 1
12
Transformed coordinates found
Z Me->J0 esu-pooo PeEo"ueb
-10000	-5000	0	5000	10000
transformed coordinate of view 1
Transformed coordinates
found by CCA
transformed coordinate of View 1
	Mean	Std
Our method	1.00	0.00
CCA	0.51	0.043
LPCCA 一	0.51	0.043
Figure 1:	Result for artificial data with dependent groups. Figures (left to right): one of the ground
truths; 1D subspace pair recovered by our method; 1D subspace pair recovered by CCA. Our method
recovers the dependency between views in the 5th dimension despite mirroring and scale differences;
CCA fails to do so. Table: means and standard deviations of the correspondence measure (10); our
method outperforms CCA and LPCCA, recovering the dependency in all artificial data sets.
Experiment on real data sets with two views. We show our method helps match neighbors between
the subspaces of two views after transformation. We use the following 3 data sets.
MNIST handwritten digit database (MNIST). MNIST contains 70000 gray-scale hand-written digit
images of size 28 × 28. We create a training set and a testing set with 2000 images each. In the
training set, we randomly choose 200 images from each digit to balance the distribution, while the
testing set is another 2000 random images without balancing. We apply nonlinear dimensionality
algorithm on both the left half and the right half of the images to create the two views to simulate
a scenario where views have nonlinear dependency; we use Neighbor Retrieval Visualizer (NeRV,
Venna et al. 2010) embedding to 5 dimensions with λ = 0.1 and λ = 0.9 respectively. The experi-
ment is repeated 17 times, covering 68000 images.
Image patches from video (Video). We take image patches from a subset of frames in a video
(db.tt/rcAS5tII). Starting from frame 50, we take 5 consecutive frames as a short clip at every
50 frames until frame 5200, then create two views from image patches in two fixed rectangles in
those frames, rect1 = [800, 900] × [250, 350] and rect2 = [1820, 1920] × [800, 900]. We measure
5-fold cross-validation performance after randomly permuting the clips.
Stock prices (Stock), from the Kaggle “Winton stock market challenge” (goo.gl/eqdhKK). It
contains prices of a stock at different times. We split the time series in the given training set into
two halves, and let view 1 be the amplitudes from the Fourier transform results of the first half, and
view 2 be the phases from the Fourier transform results of the second half.
For each data set we seek a pair of transformations onto 2D subspaces for the views. We measure per-
formance by mean precision-mean recall curves of neighbor retrieval between 1) the two subspaces
from the transformations, and 2) one of the original views and the subspace from the transformation
for the other view. The better the performance is, the more to the top (better mean precision) and
right (better mean recall) the curve will be in the figure. We set the number of neighbors in the
ground truth as 5 for MNIST and Stock, 4 for Video, and let the number of retrieved neighbors vary
from 1 to 10 as we focus on the performance of the matching for the nearest neighbors. We compare
the performance with CCA and LPCCA. Figure 2 (column 1-3) shows the results.
We now show our method can find dependent subspaces for multiple (more than two) views. In
this experiment we use Cell-Cycle data with five views. The views are from different measurements
of cell cycle-regulated gene expression for the same set of 5670 genes (Spellman et al., 1998). We
preprocess data as in Tripathi et al. (2008) with an extra feature normalization step. We seek five
two-dimensional subspaces from the five views, comparing to the PCA baseline with 2 components.
We again use mean precision-mean recalls curves as the performance measure, additionally average
the curves across the 10 view pairs or view-transformed coordinate pairs, besides averaging over the
five repetitions in 5-fold cross-validation. Figure 2 (column 4) shows we outperform the baseline.
Finding subspaces with different dimensions. We show our method can find dependent sub-
spaces with different dimensions. We create three two-dimensional Lissajous curves Lk (t) =
(CoS √2k — 11 + 2π(k - 1)/3, cos √2kt + 2π(k - 1)/3), k = 1,2,3. We create the first view X⑴ ∈
r6×1000 as X(11):1000 = (0,..., 999) and X(≥)2,1:1000 i N N (0,1), and the second view X ⑵ ∈ R6×10∞
as the concatenation of the coordinates in the Lissajous curves. We seek a one-dimensional subspace
7
Under review as a conference paper at ICLR 2017
MNIST	Video
Cell-Cycle
×10-3
0	0005	001	0.015	0.02	0.025	0.03	0035	004
recall
.×10-3
Uo_s_o3」d
0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	09	1
recall
Stock
0.017	0.018	0.019
0.02	0021	0022	0.023	0.024
recall
-- I- I- I-
UO-S-OaJd
1	23456789	10	11
recall	×10-3
Uo_s_o3」d
6
55
0	0.002	0.004	0006	0008	0.01	0.012	0.014
recall
Uo-S-Oald
0.4
0.3
0.2
0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	09	1
recall
UO-Soald
008
007
006
005
004
003
002
001
0.017	0.01S	0.019	0.02	0021	0022	0.023	0.024
recall
5
4.95
4.9
O 4.85
W 4B
J.75
d 4.7
4.65
4.6
4.55
×10-3
Figure 2:	Mean precision-mean recalls curves from different real data on the test sets. Top row:
view 1 as the ground truth; bottom row: subspace from view 1 as the ground truth. We can see
curves from our method are to the top and/or right of the curves from other methods in most parts of
all sub-figures, meaning our method achieves better precision and recall on average. Column 1-3:
our method outperforms CCA and LPCCA; column 4: our method outperforms PCA.
from X(1), and a two-dimensional subspace from X(2); the aim is to find the nonlinear dependency
between one-dimensional timestamps, and a two-dimensional representation for the three trajecto-
ries summarizing the two-dimensional movements of the three points along Lissajous curves. Figure
3 shows the Lissajous curves, found subspaces, and optimized projection pair. Our method success-
fully finds the informative feature in X(1), and keeps transformed coordinates ofX(2) smooth, with
roughly the same amount of “quick turns” as in original Lissajous curves. The magnitudes in the
optimized projections also suggest they capture the correct dependency.
(a) the studied Lissajous
curves


(b) optimized 1D subspace (c) optimized 2D subspace (d) entry magnitudes in the
of X(1)	of X(2)	optimized projections
Figure 3: Lissajous curves (a) and found subspaces from our method. (b) - (d) show We find the
correct dependency: (b): perfect linear correlation shows the time dimension was found. (c): the
number of “quick turns” (14 in total) in the smooth curves roughly matches that in the original
curves. (d): projection weights, darker color means smaller magnitude; high magnitude of P(1) ’s
first entry and the complementary pattern in P(2) suggest we capture the dependency correctly.
6 Conclusions
We presented a novel method for seeking dependent subspaces across multiple views, preserving
neighborhood relationships of data. It has strong invariance properties, detects nonlinear dependen-
cies, is related to an information retrieval task of the analyst, and performs well in experiments.
References
G. Andrew, R. Arora, K. Livescu, and J. Bilmes. Deep canonical correlation analysis. In ICML,
2013.
8
Under review as a conference paper at ICLR 2017
F. R. Bach and M. I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res., 3:1-48,
March 2003. ISSN 1532-4435.
A. Globerson, G. Chechik, F. Pereira, and N. Tishby. Euclidean embedding of co-occurrence data.
J. Mach. Learn. Res., 8:2265-2295, 2007.
M. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task: Data,
models and evaluation metrics. Journal of Artificial Intelligence Research, 47(1):853-899, May
2013. ISSN 1076-9757.
H. Hotelling. Relations between two sets of variates. Biometrika, 28(3-4):321-377, 1936.
A. Klami, S. Virtanen, and S. Kaski. Bayesian canonical correlation analysis. J. Mach. Learn. Res.,
14:965-1003, 2013.
P. Lai and C. Fyfe. Kernel and nonlinear canonical correlation analysis. Int. J. of Neu. Sys., 10(5):
365-377, 2000.
Y. LeCun and C. Cortes. MNIST handwritten digit database, 2010.
H. Nguyen and J. Vreeken. Canonical divergence analysis. CoRR, abs/1510.08370, 2015.
J. Peltonen. Visualization by linear projections as information retrieval. In Advances in Self-
Organizing Maps, pp. 237-245, Berlin Heidelberg, 2009. Springer.
Ben Shneiderman. The eyes have it: A task by data type taxonomy for information visualizations. In
Proc. IEEE Symposium on Visual Languages, pp. 336-343. IEEE Computer Society Press, 1996.
Paul T. Spellman, G. Sherlock, M. Q. Zhang, V. R. Iyer, K. Anders, M. B. Eisen, P. O. Brown,
D. Botstein, and B. Futcher. Comprehensive identification of cell cycle-regulated genes of the
yeast saccharomyces cerevisiae by microarray hybridization. Molecular Biology of the Cell, 9
(12):3273-3297, 12 1998. ISSN 1059-1524.
T. Sun and S. Chen. Locality preserving cca with applications to data visualization and pose estima-
tion. Image Vision Comput., 25(5):531-543, 2007.
A. Tripathi, A. Klami, and S. Kaski. Simple integrative preprocessing preserves what is shared in
data sources. BMC Bioinformatics, 9:111, 2008.
L. Van Der Maaten. Accelerating t-sne using tree-based algorithms. J. Mach. Learn. Res., 15(1):
3221-3245, January 2014. ISSN 1532-4435.
L. van der Maaten and G. Hinton. Visualizing data using t-SNE. J. Mach. Learn. Res., 9:2579-2605,
2008.
J. Venna, J. Peltonen, K. Nybo, H. Aidos, and S. Kaski. Information retrieval perspective to nonlinear
dimensionality reduction for data visualization. J. Mach. Learn. Res., 11:451-490, 2010.
J. J. Verbeek, S. T. Roweis, and N. A. Vlassis. Non-linear cca and pca by alignment of local models.
In NIPS, pp. 297-304. MIT Press, 2003.
M. VIadymyrov and M. A. Carreira-Perpinan. Linear-time training of nonlinear low-dimensional
embeddings. In AISTATS, volume 33, 2014.
W. Wang, R. Arora, K. Livescu, and J. Bilmes. On deep multi-view representation learning. In
ICML, 2015.
L. Wei and F. Xu. Local cca alignment and its applications. Neurocomputing, 89:78-88, 2012.
J. R. Westbury. X-ray Microbeam Speech Production Database User’s Handbook. Waisman Center
on Mental Retardation & Human Development, University of Wisconsin, 1.0 edition, June 1994.
C. Xu, D. Tao, and C. Xu. A survey on multi-view learning. CoRR, abs/1304.5634, 2013.
Z. Yang, J. Peltonen, and S. Kaski. Scalable optimization of neighbor embedding for visualization.
In ICML, pp. 127-135, 2013.
9