Under review as a conference paper at ICLR 2017
The Predictron:
End-To-End Learning and Planning
David Silver*, Hado van Hasselt*, Matteo Hessel*, Tom Schaul*, Arthur Guez*, Tim Harley,
Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris
DeepMind, London
{davidsilver,hado,mtthss,schaul,aguez}@google.com
Ab stract
One of the key challenges of artificial intelligence is to learn models that are ef-
fective in the context of planning. In this document we introduce the predictron
architecture. The predictron consists of a fully abstract model, represented by a
Markov reward process, that can be rolled forward multiple “imagined” planning
steps. Each forward pass of the predictron accumulates internal rewards and val-
ues over multiple planning depths. The predictron is trained end-to-end so as to
make these accumulated values accurately approximate the true value function.
We applied the predictron to procedurally generated random mazes and a sim-
ulator for the game of pool. The predictron yielded significantly more accurate
predictions than conventional deep neural network architectures.
1	Introduction
The central idea of model-based reinforcement learning is to decompose the RL problem into two
subproblems: learning a model of the environment, and then planning with this model. The model
is typically represented by a Markov reward process (MRP) or decision process (MDP). The plan-
ning component uses this model to evaluate and select among possible strategies. This is typically
achieved by rolling forward the model to construct a value function that estimates cumulative re-
ward. In prior work, the model is trained essentially independently of its use within the planner.
As a result, the model is not well-matched with the overall objective of the agent. Prior deep rein-
forcement learning methods have successfully constructed models that can unroll near pixel-perfect
reconstructions (Oh et al., 2015; Chiappa et al., 2016); but are yet to surpass state-of-the-art model-
free methods in challenging RL domains with raw inputs (e.g., Mnih et al., 2015; 2016; Lillicrap
et al., 2016).
In this paper we introduce a new architecture, which we call the predictron, that integrates learning
and planning into one end-to-end training procedure. At every step, a model is applied to an internal
state, to produce a next state, reward, discount, and value estimate. This model is completely abstract
and its only goal is to facilitate accurate value prediction. For example, to plan effectively in a game,
an agent must be able to predict the score. If our model makes accurate predictions, then an optimal
plan with respect to our model will also be an optimal plan for the underlying game - even if that
model uses a different state space (e.g., an abstract representation of enemy positions, ignoring
their shapes and colours), action space (e.g., a high-level action to move away from an enemy),
rewards (e.g., a single abstract step could have a higher value than any real reward), or even time-
step (e.g., a single abstract step could “jump” the agent to the end of a corridor). All we require
is that trajectories through the abstract model produce scores that are consistent with trajectories
through the real environment. This is achieved by training the predictron end-to-end, so as to make
its value estimates as accurate as possible.
An ideal model could generalise to many different prediction tasks, rather than overfitting to a single
task; and could learn from a rich variety of feedback signals, not just a single extrinsic reward. We
therefore train the predictron to predict a host of different value functions for a variety of pseudo-
reward functions and discount factors. These pseudo-rewards can encode any event or aspect of the
environment that the agent may care about, e.g., staying alive or reaching the next room.
We focus upon the prediction task: estimating value functions in MRP environments with uncon-
trolled dynamics. In this case, the predictron can be implemented as a deep neural network with an
*Primary contributors
1
Under review as a conference paper at ICLR 2017
MRP as a recurrent core. The predictron unrolls this core multiple steps and accumulates rewards
into an overall estimate of value.
We applied the predictron to procedurally generated random mazes, and a simulated pool domain,
directly from pixel inputs. In both cases, the predictron significantly outperformed model-free al-
gorithms with conventional deep network architectures; and was much more robust to architectural
choices such as depth.
2	Background
We consider environments defined by an MRP with states s ∈ S . The MRP is defined by a function,
s0, r, γ = p(s, α), where s0 is the next state, r is the reward, and γ is the discount factor, which
can for instance represent the non-termination probability for this transition. The process may be
stochastic, given IID noise α.
The return of an MRP is the cumulative discounted reward over a single trajectory, gt = rt+1 +
γt+1rt+2 +γt+1γt+2rt+3 + ... , where γt can vary per time-step. We consider a generalisation of the
MRP setting that includes vector-valued rewards r, diagonal-matrix discounts γ, and vector-valued
returns g; definitions are otherwise identical to the above. We use this bold font notation to closely
match the more familiar scalar MRP case; the majority of the paper can be comfortably understood
by reading all rewards as scalars, and all discount factors as scalar and constant, i.e., γt = γ.
The value function of an MRP p is the expected return from state s, vp(s) = Ep [gt | st = s]. In
the vector case, these are known as general value functions (Sutton et al., 2011). We will say that a
(general) value function v(∙) is consistent with environment P if and only if V = Vp which satisfies
the following Bellman equation (Bellman, 1957),
Vp(s) = Ep [r +γVp(s0) | s] .	(1)
In model-based reinforcement learning (Sutton and Barto, 1998), an approximation m ≈ p to the
environment is learned. In the uncontrolled setting this model is normally an MRP s0, r, γ = m(s, β)
that maps from state s to subsequent state s0 and additionally outputs rewards r and discounts γ;
the model may be stochastic given an IID source of noise β. A (general) value function Vm(∙) is
consistent with model m (or valid, (Sutton, 1995)), if and only if it satisfies a Bellman equation
Vm(s) = Em [r + γVm(s0) | s] with respect to model m. Conventionally, model-based RL methods
focus on finding a value function V that is consistent with a separately learned model m.
3	Predictron Architecture
The predictron is composed of four main components. First, a state representation s = f (s) that
encodes raw input s (this could be a history of observations, in the partially observed setting, for
example when f is a recurrent network) into an internal (abstract, hidden) state s. Second, a model
s0, r, γ = m(s, β) that maps from internal state s to subsequent internal state s0, internal rewards r,
and internal discounts γ. Third, a value function V that outputs internal values v = V(s) representing
the future, internal return from internal state s onwards. The predictron is applied by unrolling its
model m multiple “planning” steps to produce internal rewards, discounts and values. We use
superscripts •k to indicate internal steps of the model (which have no necessary connection to time
steps •t of the environment). Finally, these internal rewards, discounts and values are combined
together by an accumulator into an overall estimate of value g. The whole predictron, from input
state s to output g, may be viewed as a value function approximator for external targets (i.e. the
returns in the real environment). We consider both k-step and λ-weighted accumulators.
The k-step predictron rolls its internal model forward k steps. Specifically, the k-step predictron
return gk (henceforth abbreviated as preturn) is the internal return obtained by accumulating k
model steps, plus a final value vk from the kth step,
gk = r1 +γ1(r2 + γ2(. . . (rk-1 +γk-1(rk + γkvk)) . . .)).	(2)
The 0-step preturn is simply the first value g0 = v0. The 1-step preturn is g1 = r1 + γ1v1, and so
on (see Fig. 1a).
The λ-predictron combines together many k-step preturns. Specifically, it computes a diagonal
weight matrix λk from each internal state sk. The accumulator uses weights λ0, ..., λK to aggregate
2
Under review as a conference paper at ICLR 2017
a) k-step predictron
I	(■■- b) λ-predictron -∣
卜
Figure 1: a) The k-step predictron architecture. The first three columns illustrate 0, 1 and 2-step
pathways through the predictron. The 0-step preturn reduces to standard model-free value function
approximation; other preturns “imagine” additional steps with an internal model. Each pathway
outputs a k-step preturn gk that accumulates discounted rewards along with a final value estimate. In
practice all k-step preturns are computed in a single forward pass. b) The λ-predictron architecture.
The λ-parameters gate between the different preturns. The output is a λ-preturn gλ that is a mixture
over the k-step preturns. For example, if λ0 = 1, λ1 = 1, λ2 = 0 then we recover the 2-step preturn,
gλ = g2. Discount factors γk and λ-parameters λk are dependent on state sk ; this dependence is
not shown in the figure.
over k-step preturns g0, ..., gK and output a combined value that we call the λ-preturn gλ,
K	[ (1 - λk) Qk-1 λj	if k<K
gλ = X wkgk	where	wk =	(3)
k=0	I QK=01 λj	otherwise.
where 1 is the identity matrix. This λ-preturn is analogous to the λ-return in the forward-view
TD(λ) algorithm (Sutton, 1988; Sutton and Barto, 1998). It may also be computed by a backward
accumulation through intermediate steps gk,λ,
gk,λ = (1 - λk)vk + λk (rk+1 + γk+igk+ι,λ) ,	(4)
where gK,λ = vK, and then using gλ = g0,λ. Computation in the λ-predictron operates in a sweep,
iterating first through the model from k = 0 . . . K and then back through the accumulator from
k = K . . . 0 in a single “forward” pass of the network (see Figure 1b). Each λk weight acts as a
gate on the computation of the λ-preturn: a value of λk = 0 will truncate the λ-preturn at layer k,
while a value ofλk = 1 will utilise deeper layers based on additional steps of the model m; the final
weight is always λK = 0. The individual λk weights may depend on the corresponding abstract
state sk and can differ per prediction. This enables the predictron to compute to an adaptive depth
(Graves, 2016) depending on the internal state and learning dynamics of the network.
4 Predictron Learning Updates
We first consider updates that optimise the joint parameters θ of the state representation, model, and
value function. We begin with the k-step predictron. We update the k-step predictron gk towards
a target outcome g, such as the Monte-Carlo return from the real environment, by minimising a
mean-squared error loss,
Lk = I IlEp	[g	| s] - Em	[gk	|	s]『.	∂k	=	(g	-gk) ∂k	.	(5)
2
where lk = 2 ∣∣g - gk∣∣2 is the sample loss. We can use the gradient of the sample loss to update
parameters, e.g. by stochastic gradient descent. For stochastic models, two independent samples are
k	∂gk	k
required for gk and 潴 to get unbiased samples for the gradient of Lk.
3
Under review as a conference paper at ICLR 2017
The λ-predictron combines together many k-step preturns. To update the joint parameters θ, we can
uniformly average the losses on the individual preturns gk,
L0：K = 2K XX 同[g|s]-Em [gk 川2 ,	d∂K = KK XX (g -gk) % .⑹
k=0	k=0
Alternative, we could weight each loss by the usage wk of the corresponding preturn, such that the
gradientis PK=0 wk (g - gk) d∂θ.
The λ-predictron uses an accumulator with additional parameters η that determine the relative
weighting of the k-step preturns. These weights are also updated so as to minimise a mean-squared
error loss Lλ,
Lλ = 2 IlEp	[g |	s]	-Em	[gλ	|	s]	||2 ,	∂-	= (g - gλ)	∂g-.	⑺
2	ηη
In summary, the joint parameters θ of the state representation f, the model m, and the value function
v are updated to make each of the k-step preturns gk more similar to the target g, and the parameters
η of the λ-accumulator are updated to make the aggregate λ-preturn gλ more similar to the target g.
4.1	CONSISTENCY (SEMI-SUPERVISED) LEARNING WITH THE λ-PREDICTRON
Ideally, the predictron (f, m, v) learns preturns that are all equal in expectation to the true value
function of the environment, Em gk | s = Ep [gt | s] = vp(s), in which case the preturns must
be equal in expectation, Em g0 | s = Em g1 | s = ... = Em gK | s . In addition, each k-step
preturn must then be equal in expectation to the λ-preturn, Em gk | s = Em gλ | s , for any λ
parameters. All these consistency relations between preturns give rise to additional constraints upon
the predictron. Specifically, we may adjust the parameters of the predictron to lead to consistent
preturns, even in the absence of labelled targets.
Concretely, we can adjust each preturn gk towards the λ-preturn gλ ; in other words, we can update
each individual value estimate towards the best aggregated estimate by minimizing
L =2 X ||Em	[gλ	| s] -	Em	[gk	| s] l∣2 ,	∂θ	= X	(gλ	- gk)	∂gθr	.⑻
2
k=0	k=0
Here gλ is considered fixed; the parameters θ are only updated to make gk more similar to gλ , not
vice versa. This consistency update does not require any labels g or samples from the environment.
As a result, it can be applied to (potentially hypothetical) states that have no associated ‘real’ (e.g.
Monte-Carlo) outcome: we update the value estimates to be self-consistent with each other. Note
the similarity with the semi-supervised setting, where we may have unlabelled inputs.
5	Experiments
We conducted experiments on two domains. The first domain consists of randomly generated 20×20
mazes in which each location either is empty or contains a wall. Two locations in a maze are consid-
ered connected if they are both empty and we can reach one from the other by moving horizontally
or vertically through adjacent empty cells. The goal is to predict, for each of the locations on the
diagonal from top-left to bottom-right of the maze, whether the bottom-right corner is connected to
that location, given the entire maze as an input image. Some of these predictions will be straightfor-
ward, for instance for locations on the diagonal that contain a wall themselves and for locations close
to the bottom right. Many other predictive questions seem to require a simple algorithm, such as
some form of a flood fill or search; our hypothesis is that an internal model can learn to emulate such
algorithms, where naive approximation may struggle. A few example mazes are shown in Figure 2.
Our second domain is a simulation of the game of pool, using four balls and four pockets. The simu-
lator is implemented in the physics engine Mujoco (Todorov et al., 2012). We generate sequences of
RGB frames starting from a random arrangement of balls on the table. The goal is to simultaneously
learn to predict future events for each of the four balls, given 5 RGB frames as input. These events
include: collision with any other ball, collision with any boundary of the table, entering a quadrant
(×4, for each quadrant), being located in a quadrant (×4, for each quadrant), and entering a pocket
4
Under review as a conference paper at ICLR 2017
Figure 2: Left: Two sample mazes from the random-maze domain. Light blue cells are empty,
darker blue cells contain a wall. One maze is connected from top-left to bottom-right (indicated in
black), the other is not. Right: An example trajectory in the pool domain (before downsampling).
It was selected by maximising the prediction of pocketing balls, using the predictron.
φ8s 60-)
SBZElU LUoPUeJ co 山SlAla
0	500K	IM 0	500K	IM
Updates	Updates
Figure 3: Exploring predictron variants. Aggregated prediction errors over all predictions (20
for mazes, 280 for pool) for the eight predictron variants corresponding to the cube on the left (as
described in the main text), for both random mazes (top) and pool (bottom). Each line is the median
of RMSE over five seeds; shaded regions encompass all seeds. The full (r, γ, λ)-prediction (red)
consistently performed best.
(×4, for each pocket). Each of these 14 × 4 events provides a binary pseudo-reward that we combine
with 5 different discount factors {0, 0.5, 0.9, 0.98, 1} and predict their cumulative discounted sum
over various time spans. This yields a total of 280 general value functions. An example trajectory is
shown in Figure 2. In both domains, inputs are presented as minibatches of i.i.d. samples with their
regression targets. Additional domain details are provided in Appendix E.
5.1	Exploring the Predictron Architecture
Our first set of experiments examines three binary dimensions that differentiate the predictron from
standard deep networks. We compare eight predictron variants corresponding to the corners of the
cube on the left in Figure 3.
The first dimension corresponds to whether or not the predictron architecture utilises the structure of
an MRP model. In the MRP case, labelled r, γ, internal rewards and discounts are both learned. In
the non-r, γ case, which corresponds to a vanilla hidden-to-hidden neural network module, internal
rewards and discounts are ignored by fixing their values to rk = 0 and γk = 1.
The second dimension is whether a K -step accumulator or λ-accumulator is used to aggregate over
preturns. When a λ-accumulator is used, a λ-preturn is computed as described in Section 3. Other-
wise, intermediate preturns are ignored by fixing their values to λk = 1 for k < K . In this case, the
overall output of the predictron is simply the maximum-depth preturn gK .
The third dimension, labelled usage weighting, defines the loss that is used to update the parameters
θ. On this dimension, we consider two options: the preturn losses can either be weighted uniformly
(see Equation 6), or the update for each preturn gk can be weighted according to the weight wk that
determines how much it is used in the λ-prediction's overall output. We call the latter loss ‘usage
weighted‘. Note that for architectures without a λ-accumulator, wk = 0 for k < K, and wK = 1,
thus usage weighting then implies backpropagating only the loss on the final preturn gK.
All variants utilise a convolutional core with 2 intermediate hidden layers (see Appendix A); param-
eters were updated by supervised learning (see Appendix B for more details). Root mean squared
prediction errors for each architecture, aggregated over all predictions, are shown in Figure 3. The
5
Under review as a conference paper at ICLR 2017
recurrent ResNet
recurrent ConvNet
ResNet
weight Shar-ng
(r, γ, λ)-predictron
ConvNet
0.01
0.001
0.0001
500K
Updates
φ-8s 8)
s<υzelu EoPU0」co 山 Swa
Figure 4: Comparing predictron to baselines. Aggregated prediction errors on random mazes
(top) and pool (bottom) over all predictions for the eight architectures corresponding to the cube on
the left. Each line is the median of RMSE over five seeds; shaded regions encompass all seeds. The
full (r, γ, λ)-predictron (red), consistently outperformed conventional deep network architectures
(black), with and without skips and with and without weight sharing.
top row corresponds to the random mazes and the bottom row to the pool domain. The main con-
clusion is that learning an MRP model improved performance greatly. The inclusion of λ weights
helped as well, especially on pool. Usage weighting further improved performance.
5.2	Comparing the Predictron to Other Deep Networks
Our second set of experiments compares the predictron to feedforward and recurrent deep learning
architectures, with and without skip connections. We compare the corners of a new cube, as depicted
on the left in Figure 4, based on three different binary dimensions.
The first dimension of this second cube is whether we use a predictron, or a (non-λ, non-r, γ) deep
network that does not have an internal model and does not output or learn from intermediate predic-
tions. We use the most effective predictron from the previous section, i.e., the (r, γ, λ)-predictron
with usage weighting.
The second dimension is whether weights are shared between all cores (as in a recurrent network),
or whether each core uses separate weights (as in a feedforward network). We note that the non-
λ, non-r, γ variants of the predictron then correspond to standard (convolutional) feedforward and
(unrolled) recurrent neural networks respectively.
The third dimension is whether we include skip connections. This is equivalent to defining the model
step to output a change to the current state, ∆s, and then defining sk+1 = h(sk + ∆sk), where h
is the non-linear function—in our case a ReLU, h(x) = max(0, x). The deep network with skip
connections is a variant of ResNet (He et al., 2015).
Root mean squared prediction errors for each architecture are shown in Figure 4. All (r, γ, λ)-
predictrons (red lines) outperformed the corresponding feedforward or recurrent neural network
baselines (black lines) both in the random mazes and in pool. We also investigated the effect of
changing the depth of the networks (see Appendix C). The predictron outperformed the correspond-
ing feedforward or recurrent baselines for all depths, with and without skip connections.
5.3	Semi-supervised learning by consistency
We now consider how to use the predictron for semi-supervised learning, training the model on
a combination of labelled and unlabelled random mazes. Semi-supervised learning is important
because a common bottleneck in applying machine learning in the real world is the difficulty of
collecting labelled data, whereas often large quantities of unlabelled data exist.
We trained a full (r, γ , λ)-predictron by alternating standard supervised updates with consistency
updates, obtained by stochastically minimizing the consistency loss (8), on the unlabelled samples.
For each supervised update we apply either 0, 1, or 9 consistency updates. Figure 5 shows that
6
Under review as a conference paper at ICLR 2017
the performance improved monotonically with the number of consistency updates, measured as a
function of the number of labelled samples consumed.
φmuS 6o)
WΦNroE EoPUeJ co 山SWa
Figure 5: Semi-supervised learning. Prediction errors of the (r, γ, λ)-predictrons (shared core, no
skips) using 0, 1, or 9 consistency updates for every update with labelled data, plotted as function of
the number of labels consumed. Learning performance improves with more consistency updates.
5.4	Analysis of adaptive depth
In principle, the predictron can adapt its depth to ‘think more’ about some predictions than others,
perhaps depending on the complexity of the underlying target. We investigate this by looking at
qualitatively different prediction types in pool: ball collisions, rail collisions, pocketing balls, and
entering or staying in quadrants. For each prediction type we consider several different time-spans
(determined by the real-world discount factors associated with each pseudo-reward). Figure 6 shows
distributions of depth for each type of prediction. The ‘depth’ of a predictron is here defined as the
effective number of model steps. If the predictron relies fully on the very first value (i.e., λ0 = 0),
this counts as 0 steps. If, instead, it learns to place equal weight on all rewards and on the final
value, this counts as 16 steps. Concretely, the depth d can be defined recursively as d = d0 where
dk = λk(1 + γkdk+1) and dK = 0. Note that even for the same input state, each prediction has a
separate depth.
The depth distributions exhibit three properties. First, different types of predictions used different
depths. Second, depth was correlated with the real-world discount for the first four prediction types.
Third, the distributions are not strongly peaked, which implies that the depth can differ per input
even for a single real-world discount and prediction type. In a control experiment (not shown) we
used a scalar λ shared among all predictions, which reduced performance in all scenarios, indicating
that the heterogeneous depth is a valuable form of flexibility.
5.5	Visualizing the predictions in the pool domain
We test the quality of the predictions in the pool domain to evaluate whether they are well-suited to
making decisions. For each sampled pool position, we consider a set I of different initial conditions
(different angles and velocity of the white ball), and ask which is more likely to lead to pocketing
coloured balls. For each initial condition s ∈ I, we apply the (r, γ, λ)-predictron (shared cores, 16
model steps, no skip connections) to obtain predictions gλ . We sum the predictions that correspond
16
14
12
10
8
6
4
2
0
rails
16
enter
0 0.5 0.9 0.98 1
Real-world discounts
14
12
10
0.5 0.9 0.98 1
Rea I-world discounts
16
14
12
10
8
6
4
2
0
stay
to
0 0.5 0.9 0.98 1
Real-world discounts


Figure 6: Thinking depth. Distributions of thinking depth on pool for different types of predictions
and for different real-world discounts.
7
Under review as a conference paper at ICLR 2017
to pocketing any ball except the white ball, and to real-world discounts γ = 0.98 and γ = 1. We
select the condition s* that maximises this sum.
We then roll forward the pool simulator from s* and log the number of pocketing events. Figure 2
shows a sampled rollout, using the predictron to pick s*. When providing the choice of 128 an-
gles and two velocities for initial conditions (|I | = 256), this procedure resulted in pocketing 27
coloured balls in 50 episodes. Using the same procedure with an equally deep convolutional net-
work only resulted in 10 pocketing events. These results suggest that the lower loss of the learned
(r, γ, λ)-predictron translated into meaningful improvements when informing decisions. A video of
the rollouts selected by the predictron is available here: https://youtu.be/BeaLdaN2C3Q.
6	Related work
Lee et al. (2015) introduced a neural network architecture where classifications branch off interme-
diate hidden layers. An important difference with respect to the λ-predictron, is that the weights are
hand-tuned as hyper-parameters, whereas in the predictron the λ weights are learnt and, more im-
portantly, conditional on the input. Another difference is that the loss on the auxiliary classifications
is used to speed up learning, but the classifications themselves are not combined into an aggregate
prediction; the output of the model itself is the deepest prediction.
Graves (2016) introduced an architecture with adaptive computation time (ACT), with a discrete
(but differentiable) decision on when to halt, and aggregating over the outputs at each pondering
step. This is related to our λ weights, but obtains depth in a different way; one notable difference is
that the λ-predictron can choose different pondering depths for each of its predictions.
Value iteration networks (VINs) (Tamar et al., 2016) also learn value functions end-to-end using an
internal model, similar to the (non-λ) predictron. However, VINs plan via convolutional operations
over the full input state space; whereas the predictron plans via imagined trajectories through an
abstract state space. This may allow the predictron architecture to scale much more effectively in
domains that do not have a natural two-dimensional encoding of the state space.
The notion of learning about many predictions of the future relates to work on predictive state
representations (PSRs; Littman et al., 2001), general value functions (GVFs; Sutton et al., 2011),
and nexting (Modayil et al., 2012). Such predictions have been shown to be useful as representa-
tions (Schaul and Ring, 2013) and for transfer (Schaul et al., 2015). So far, however, none of these
have been considered for learning abstract models.
Schmidhuber (2015) discusses learning abstract models, but maintains separate losses for the model
anda controller, and suggests training the model unsupervised to compactly encode the entire history
of observations, through predictive coding. The predictron’s abstract model is instead trained end-
to-end to obtain accurate values.
7	Conclusion
The predictron is a single differentiable architecture that rolls forward an internal model to estimate
external values. This internal model may be given both the structure and the semantics of tradi-
tional reinforcement learning models. But unlike most approaches to model-based reinforcement
learning, the model is fully abstract: it need not correspond to the real environment in any human
understandable fashion, so long as its rolled-forward “plans” accurately predict outcomes in the true
environment.
The predictron may be viewed as a novel network architecture that incorporates several separable
ideas. First, the predictron outputs a value by accumulating rewards over a series of internal planning
steps. Second, each forward pass of the predictron outputs values at multiple planning depths. Third,
these values may be combined together, also within a single forward pass, to output an overall
ensemble value. Finally, the different values output by the predictron may be encouraged to be
self-consistent with each other, to provide an additional signal during learning. Our experiments
demonstrate that these differences result in more accurate predictions of value, in reinforcement
learning environments, than more conventional network architectures.
We have focused on value prediction tasks in uncontrolled environments. However, these ideas may
transfer to the control setting, for example by using the predictron as a Q-network (Mnih et al.,
2015). Even more intriguing is the possibility of learning an internal MDP with abstract internal
actions, rather than the MRP considered in this paper. We aim to explore these ideas in future work.
8
Under review as a conference paper at ICLR 2017
References
R.	Bellman. Dynamic programming. Princeton University Press, 1957.
S.	Chiappa, S. Racaniere, D. Wierstra, and S. Mohamed. Recurrent environment simulators. 2016.
X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Aistats, volume 15,
page 275, 2011.
A. Graves. Adaptive computation time for recurrent neural networks. CoRR, abs/1603.08983, 2016.
URL http://arxiv.org/abs/1603.08983.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint
arXiv:1512.03385, 2015.
S.	Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
D. P. Kingma and J. Ba. A method for stochastic optimization. In International Conference on
Learning Representation, 2015.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings ofthe IEEE, 86(11):2278-2324,1998.
C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. In AISTATS, volume 2,
page 6, 2015.
T.	Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. In ICLR, 2016.
M.	L. Littman, R. S. Sutton, and S. P. Singh. Predictive representations of state. In NIPS, volume 14,
pages 1555-1561, 2001.
V.	Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforce-
ment learning. Nature, 518(7540):529-533, 2015.
V.	Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In International Conference on Machine
Learning, 2016.
J. Modayil, A. White, and R. S. Sutton. Multi-timescale nexting in a reinforcement learning robot.
In International Conference on Simulation of Adaptive Behavior, pages 299-309. Springer, 2012.
J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep
networks in atari games. In Advances in Neural Information Processing Systems, pages 2863-
2871, 2015.
T. Schaul and M. B. Ring. Better Generalization with Forecasts. In Proceedings of the International
Joint Conference on Artificial Intelligence (IJCAI), Beijing, China, 2013.
T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal Value Function Approximators. In
International Conference on Machine Learning (ICML), 2015.
J. Schmidhuber. On learning to think: Algorithmic information theory for novel combina-
tions of reinforcement learning controllers and recurrent neural world models. arXiv preprint
arXiv:1511.09249, 2015.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:
9-44, 1988.
R. S. Sutton. TD models: Modeling the world at a mixture of time scales. In Proceedings of the
Twelfth International Conference on Machine Learning, pages 531-539, 1995.
9
Under review as a conference paper at ICLR 2017
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. The MIT press, Cambridge
MA, 1998.
R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde: A scal-
able real-time architecture for learning knowledge from unsupervised sensorimotor interaction.
In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2,
pages 761-768. International Foundation for Autonomous Agents and MUltiagent Systems, 2011.
A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel. Value iteration networks. In Neural
Information Processing Systems (NIPS), 2016.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE,
2012.
10
Under review as a conference paper at ICLR 2017
Figure 7: The predictron core used in our experiments.
A	Architecture
The state representation f is a two-layer convolutional neural network (LeCun et al., 1998). There
is a core c, again based on convolutions, that combines both MRP model and λ-network into a
single repeatable module, such that sk+1 , rk+1 , γk+1, λk = c(sk). This core is deterministic, and
is duplicated K times in the predictron with shared weights. (The predictron with unshared weights
has K distinct cores.) Finally, the value network v is a fully connected neural network that computes
vk = v(sk).
Concretely, the core (Figure 7) consists first of a convolutional layer that maps into an intermediate
(hidden) layer. From this layer, another two convolutions compute the next abstract state of the
predictron. Additionally, this same hidden layer is flattened and fed into three separate networks,
with two fully connected layers each. The outputs of these three networks represent the internal
rewards, discounts, and lambdas. A similar small network also hangs off the internal states, in
addition to the core, and computes the values. All convolutions use 3×3 filters and a stride of one,
and use padding to retain the size of the feature maps. All feature maps have 32 channels. The
hidden layers within the MLPs have 32 hidden units.
In Figure 7 the convolutional layers are schematically drawn with three channels, flattening is repre-
sented by curly brakets, while the arrows represent the small multi-layer perceptrons which compute
values, rewards, discounts and lambdas.
We allow up to 16 model steps in our experiments, resulting in 52-layer deep networks—two convo-
lutional layers for the state representations, 3 × 16 = 48 convolutional layers for the core steps, and
two fully-connected layers for the values on top of the final state. Between each two layers we ap-
ply batch normalization (Ioffe and Szegedy, 2015) followed by a ReLU non-linearity (Glorot et al.,
2011). The value and reward networks end with a linear layer, whereas the discount and λ-networks
additionally add a sigmoid non-linearity to ensure that these quantities are in [0, 1].
B	Training
All experiments used the supervised (Monte-Carlo) update described in Section 4 except for the
semi-supervised experiment which used the consistency update described in Section 4.1. We update
all parameters by applying the Adam optimiser (Kingma and Ba, 2015) to stochastic gradients of
the corresponding loss functions. Each return is normalised by dividing it by its standard deviation
(as measured, prior to the experiment, on a set of 20,000 episodes). In all experiments, the learning
rate was 0.001, and the other parameters of the Adam optimiser were β1 = 0.9, β2 = 0.999, and
= 10-8. We used mini-batches of 100 samples.
C Comparing architectures of different depths
We investigated the effect of changing the depth of the networks, with and without skip connec-
tions. Figure 8 in shows that skip connections (dashed lines) make the conventional architectures
11
Under review as a conference paper at ICLR 2017
Φ-BUS 6。-)
WΦNroE EoPUeJ co 山SWa
Unshared cores
0 IM 2M 3M 4M 5M
Updates
Figure 8: Comparing depths. Comparing the (r, γ, λ)-predictron (red) against more conventional
deep networks (black) for various depths (2, 4, 8, or 16 model steps, corresponding to 10, 16, 28, or
52 total layers of depth). Lighter colours correspond to shallower networks. Dashed lines correspond
to networks with skip connections.
500K
Updates
IM
(black/grey lines) more robust to the depth (i.e., the black/grey dashed lines almost overlap, es-
pecially on pool), and that the predictron outperforms the corresponding feedforward or recurrent
baselines for all depths, with and without skips.
D Capacity comparisons
In this section, we present some additional experiments comparing the predictron to more con-
ventional deep networks. The purposes of these experiments are 1) to show that the conclusions
obtained above do not depend on the precise architecture used, and 2) to show that the structure
of the network—whether we use a predictron or not—is more important than the raw number of
parameters.
Specifically, we again consider the same 20 by 20 random mazes, and the pool task described in the
main text. As described in Section A, for the results in the paper we used an encoder that preserved
the size of the input plans, 20 × 20 for the mazes and 28 × 28 for pool. Each convolution had 32
channels and therefore the abstract states were 20 × 20 × 32 for the mazes and 28 × 28 × 32 for
pool.
We now consider a different architecture, where we no longer pad the convolutions used in the
encoder. For the mazes, we still use two layers of 3 × 3 stride-1 convolutions, which means the
planes reduce in size to 16 × 16. This means that the abstract states are about one third smaller. For
pool, we use three 5 × 5 stride-1 convolutions, which bring us from 28 × 28 down to 16 × 16 as well.
So, the abstract states are now of equal size for both experiments. For pool, this is approximately a
two-thirds reduction, which helps reduce the compute needed to run the model.
Most of the parameters in the predictron are in the fully connected layers. Previously, the first fully
connected layer for each of the internal values, rewards, discounts, and λ-parameters would take a
flattened abstract state, and then go into 32 hidden nodes. This means the number of parameters in
this layer were 20 × 20 × 32 × 32 = 409, 600 for the mazes and 28 × 28 × 32 × 32 = 802, 816 for
pool. The predictron with shared core would have four of these layers, one for each of the internal
values, rewards, discounts, and λs, compared to one for the deep network which only has values. We
change this in two ways. First, we add a 1 × 1 convolution with a stride of 1 and 8 channels before
the first fully connected layer for each of these outputs. This reduces the number of channels, and
therefore the number of parameters in the subsequent fully-connected layer, by one fourth. Second,
we tested three different numbers of hidden nodes: 32, 128, or 512.
12
Under review as a conference paper at ICLR 2017
ω-eus Bo-)ood co 山 SW
Figure 9: Comparing depths. Comparing the (r, γ, λ)-predictron (red) against more conventional
deep networks (blue) for different numbers hidden nodes in the fully connected layers, and therefore
different total numbers of parameters. The deep networks with 32, 128, and 512 nodes respectively
have 381,416, 1,275,752, and 4,853,096 parameters in total. The predictrons with 32 and 128 nodes
respectively have 1,275,752, and 4,853,096 parameters in total. Note that the number of parameters
for the 32 and 128 node predictrons are exactly equal to the number of parameters for the 128 and
512 node deep networks.
deep net, 32 hidden nodes
deep net, 128 hidden nodes
deep net, 512 hidden nodes
predictron, 32 hidden nodes
predictron, 128 hidden nodes
IM 2M 3M 4M 5M
Updates
The deep network with 128 hidden nodes for its values has the exact same number of parameters
as the (r, γ, λ)-predictron with 32 hidden nodes for each of its outputs. Before, the deep network
had fewer parameters, because we kept this number fixed at 32 across experiments. This opens the
question of whether the improved performance of the predictron was not just an artifact of having
more parameters. We tested this hypothesis, and the results are shown in Figure 9.
Figure 9 shows that in each setting—on the mazes and pool, and with or without shared cores—
both. The predictrons always performed better than all the deep networks. This includes the 32
node predictron (darkest red) compared to the 512 node deep network (lightest blue), even though
the latter has approximately 4 times as many parameters (1.27M vs 4.85M). This means that the
number of parameters mattered less than whether or not we use a predictron.
E	Additional domain details
We now provide some additional details of domains.
E.1 POOL
To generate sequences in the Pool domain, the initial loca-
tions of 4 balls of different colours are sampled at random.
The white ball is the only one moving initially. Its velocity
has a norm sampled uniformly between 7 and 14. The ini-
tial angle is sampled uniformly in the range (0, 2π). From
the initial condition, the Mujoco simulation is run forward
until all balls have stopped moving; sequences that last
more than 151 frames are rejected, and a new one is gen-
erated as replacement. Each frame is rendered by Mujoco
as a 280x280 RGB image, and subsequently downsam-
pled through bilinear interpolation to a 28x28 RGB input
(see Figure 10 for an example). Since the 280 signals de-
scribed in Section 6.1 as targets for the Pool experiments
have very different levels of sparsity, resulting in values
Figure 10: Pool input frame. An ex-
ample of a 28x28 RGB input frame in
the pool domain.
13
Under review as a conference paper at ICLR 2017
with very different scales, we have normalised the pseudo returns. The normalization procedure
consisted in dividing all targets by their standard deviation, as empirically measured across an initial
set of 20,000 sequences.
E.2 Random Mazes
To generate mazes we first determine, with a stochastic line search, a number of walls so that the top-
left corner is connected to the bottom-right corner (both always forced to be empty) in approximately
50% of the mazes. We then shuffle the walls uniformly randomly. For 20 by 20 mazes this means
70% of locations are empty and 30% contain walls. More than a googol different such 20-by-20
mazes exist (as (398) > 10100).
14