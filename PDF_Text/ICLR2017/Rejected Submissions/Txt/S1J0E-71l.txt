Under review as a conference paper at ICLR 2017
Surprisal-Driven Feedback in Recurrent Net-
WORKS
Kamil Rocki
IBM Research
San Jose, CA 95120, USA
kmrocki@us.ibm.com
Ab stract
Recurrent neural nets are widely used for predicting temporal data. Their inher-
ent deep feedforward structure allows learning complex sequential patterns. It is
believed that top-down feedback might be an important missing ingredient which
in theory could help disambiguate similar patterns depending on broader context.
In this paper, we introduce surprisal-driven recurrent networks, which take into
account past error information when making new predictions. This is achieved
by continuously monitoring the discrepancy between most recent predictions and
the actual observations. Furthermore, we show that it outperforms other stochas-
tic and fully deterministic approaches on enwik8 character level prediction task
achieving 1.37 BPC.
1	Introduction
Based on human performance on the same task, it is believed that an important ingredient which is
missing in state-of-the-art variants of recurrent networks is top-down feedback. Despite evidence
of its existence, it is not entirely clear how mammalian brain might implement such a mechanism.
It is important to understand what kind of top-down interaction contributes to improved prediction
capability in order to tackle more challenging AI problems requiring interpretation of deeper con-
textual information. Furthermore, it might provide clues as what makes human cognitive abilities
so unique. Existing approaches which consider top-down feedback in neural networks are primar-
ily focused on stacked layers of neurons, where higher-level representations constitute a top-down
signal source. In this paper, we propose that the discrepancy between most recent predictions and
observations might be effectively used as a feedback signal affecting further predictions. It is very
common to use such a discrepancy during learning phase as the error which is subject to minimiza-
tion, but not during inference. We show that is also possible to use such top-down signal without
losing generality of the algorithm and that it improves generalization capabilities when applied to
Long-Short Term Memory (Hochreiter & Schmidhuber, 1997) architecture. It is important to point
out that the feedback idea presented here applies only to temporal data.
1.1	Summary of Contributions
The main contributions of this work are:
•	the introduction of a novel way of incorporating most recent misprediction measure as an
additional input signal
•	extending state-of-the-art performance on character-level text modeling using Hutter
Wikipedia dataset.
1.2	Related Work
There exist other approaches which attempted to introduce top-down input for improving predic-
tions. One such architecture is Gated-Feedback RNN (Chung et al., 2015). An important difference
between architecture proposed here and theirs is the source of the feedback signal. In GF-RNN it is
assumed that there exist higher level representation layers and they constitute the feedback source.
1
Under review as a conference paper at ICLR 2017
On the other hand, here, feedback depends directly on the discrepancy between past predictions and
current observation and operates even within a single layer. Another related concept is Ladder Net-
works (Rasmus et al., 2015), where top-down connections contribute to improved semi-supervised
learning performance.
2	Feedback： Misprediction-driven prediction
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
4
2
0
4
2
0
20
10
0
4
2
0
2
1
0
20
10
0
10
5
0
10
5
0
10
5
0
10
5
0
10
5
0
10
5
0
10
5
0
5
0
10
5
0
;&amp;#1489;&amp;#1500;&amp;#1512; ] ] [ [id:AsSembler] ] [ [lt:AsembIeris] ] [[pl:ASembler] ] [ [ru:&amp;#
八人∕∖〜J∖N
16
≡≡-≡≡J0
at, sandy soils. Granites sometimes occur in circular depressions surrounded by a range of hills, f
Figure 1: Illustration of St signal on a typical batch of 16 sequences of length 100 from enwik8
dataset. y-axis is negative log probability in bits. Intuitively surprise signal is low when a text
fragment is highly predictable (i.e. in the < timestamp > part - sequence no 10, the tag itself
is highly predictable, whereas the exact date cannot be predicted and should not be the focus of
attention). The main idea presented in this paper is that feedback signal st should be able to help in
distinguishing predictable and inherently unpredictable parts during the inference phase.
2
Under review as a conference paper at ICLR 2017
2.1	Notation
The following notation is used throughout the section:
x - inputs
h - hidden units
y - outputs
p - output probabilities (normalized y)
s - surprisal
t - time step
W - feedforward x → h connection matrix
U - recurrent h → h connection matrix
V - feedback s → h connection matrix
S - truncated BPTT length
M - number of inputs
N - number of hidden units
• denotes matrix multiplication
denotes elementwise multiplication
σ(∙),tanh(∙) - elementwise nonlinearities
δx = dE
∂x
In case of LSTM, the following concatenated representations are used:
=
W
- 一
iUUfUoUu
=
U
- -
ib bfbobu
=
b
itftotut
=
gt
=
V
- 一
iVVfVoVu
)
1
(
2.2	Simple RNN without feedback
First, we show a simple recurrent neural network architecture without feedback which serves as a
basis for demonstrating our approach. It is illustrated in Fig. 2 and formulated as follows:
(2)
ht = tanh(W • xt + U • ht-1 + b)
Figure 2: Simple RNN; h - internal (hidden) states; x are inputs, y are optional outputs to be emitted
3
Under review as a conference paper at ICLR 2017
2.3	Feedback augmented recurrent networks
error feedback
feedforward input
Figure 3: Surprisal-Feedback RNN; st represents surprisal (in information theory sense) - the dis-
crepancy between prediction at time step t- 1 and the actual observation at time step t; it constitutes
additional input signal to be considered when making a prediction for the next time step.
Figure 3 presents the main idea of surprisal-driven feedback in recurrent networks. In addition to
feedforward and recurrent connections W and U, we added one additional matrix V . One more
input signal, namely V ∙ St is being considered when updating hidden states of the network. We
propose that the discrepancy st between most recent predictions pt-1 and observations xt might
be effectively used as a feedback signal affecting further predictions. Such information is usually
used during learning phase as an error signal, but not during inference. Our hypothesis is that it
represents an important source of information which can be used during the inference phase, should
be used and that it bring benefits in the form of improved generalization capability. Figure 1 presents
examples of feedback signal being considered. Intuitively, when surprisal is near zero, the sum of
input signals is the same as in a typical RNN. Next subsections provide mathematical description
of the feedback architecture in terms of forward and backward passes for the Back Propagation
Through Time (BPTT) (Werbos, 1990) algorithm.
2.4	Forward pass
Set h0, c0 to zero and p0 to uniform distribution or carry over the last state to emulate full BPTT.
1
∀i,p0 = M，i ∈ {0,1,..,M - 1},t = 0	(3)
for t = 1:1:S-1
I.	Surprisal part
i
st = - X log pit-1	xit	(4)
IIa. Computing hidden activities, Simple RNN
ht = tanh(W ∙ Xt + U ∙ ht-ι + V ∙ St + b)	(5)
IIb. Computing hidden activities, LSTM (to be used instead of IIa)
ft = σ(Wf ∙ Xt + Uf ∙ ht-i + Vf ∙ St + bf)	(6)
it = σ(Wi ∙ xt + Ui ∙ ht-i + Vi ∙ St + bi)	(7)
4
Under review as a conference paper at ICLR 2017
Ot = σ(Wo ∙ Xt + Uo ∙ ht-i + Vo ∙ St + bo)	(8)
Ut = tanh(Wu ∙ Xt + Uu ∙ ht-i + 匕∙ St + bu)	(9)
ct = (1 - ft) ct-1 + it ut	(10)
^t = tanh(ct)	(11)
ht = ot Θ ^t	(12)
III. Outputs	yi = Wy ∙ ht + by	(13)
Softmax normalization	i	eyti Pt = Pieyi	(14)
2.5 Backward pass
for t = S-1:-1:1
I. Backprop through predictions
Backprop through softmax, cross-entropy error, accumulate
	∂Et 		= ∂yt	∂Et 二	+ Pt-1 - xt ∂yt	(15)
δy → δWy , δby	∂E -	= ∂Wy	∂E	T ∂Et -∂Wy+ t •西	(16)
	∂E -	= dby	_ ∂E XX ∂Ei =dby +i=1 dyt	(17)
δy → δh	∂Et	∂Et	∂Et	T 二	1	∙ W T ∂ht	∂yt	y	(18)
			= ∂ht		
IIa. Backprop through hidden nonlinearity (simple RNN version)
∂Et
∂ht
∂Et ∂Et	0
西 + ∂ht θ tanh (ht)
(19)
∂Et ∂Et
---=------
∂gt ∂ht
(20)
IIb. Backprop through c, h, g (LSTM version)
5
Under review as a conference paper at ICLR 2017
Backprop through memory cells, (keep gradients from the previous iteration)
∂Et
∂ct
∂Et ∂Et	7一、
西 + 西 θ 0t θ tanh (Q)
(21)
Carry error over to ∂∕∖
(	∂Et = de1	∂Et ∂ct-1	+四 ∂ct	θ (1 - ft)	(22)
Propagate error through the gates					
	∂Et ——： dot	∂Et =—— ∂ht	Θ Ct θ	σ'(ot)	(23)
	∂Et ——： ∂it	∂Et =—— ∂ct	θ Ut Θ	σ (it)	(24)
	∂Et 	= ∂ft	∂Et —：— ∂ct	Θ ct-i	Θ σ'(ft)	(25)
	∂Et _ -	= ∂ut	咨G ∂ct	i it Θ tanh0(ut)		(26)
Carry error over to ^t1	∂Et		泪J	UT	(27)
	∂h-1 =		∂gt		
III. Backprop through linearities					
	∂Et ~∂b	=M+X 2=1		∂Et ∂gt	(28)
	∂E --= ∂U	_ ∂E =∂U	+ hT-1	∂Et •-	 ∂gt	(29)
	∂E ∂W	∂E	T =∂W + X '		∂Et ■-	 ∂gt	(30)
	∂E ∂x	∂E =——— ∂x	l ∂Et ∂gt	. WT	(31)
IV. Surprisal part	∂E ∂V	∂E =—— ∂V	+ ST -	∂Et ∂gt	(32)
	∂E ——= ∂st		dE . VT ∂gt		(33)
	∂Et = ∂pt-1		岩Θ	xt	(34)
Adjust apE∖ according to the sum of gradients and carry over to 债∖
∂Et _ ∂Et	∂E ∂Et
∂yt-ι ∂pt-ι PtT ∙ = ∂pi-1
(35)
6
Under review as a conference paper at ICLR 2017
1,04u∙,1,∙,qo∕s4τ8
4h 8h 16h	24h	32h	40h	48h	60h	72h
Time
Figure 4: Training progress on enwik8 corpus, bits/character
uφ4ju∙,u∙,q"∕"4τnutbuh
1.6
1.7
1.75	1.7
1.65	1.6	1.55	1.5	1.45	1.4
Test Bits/Character
3 Experiments
We ran experiments on the enwik8 dataset. It constitutes first 108 bytes of English Wikipedia dump
(with all extra symbols present in XML), also known as Hutter Prize challenge dataset1 2 3. First 90%
of each corpus was used for training, the next 5% for validation and the last 5% for reporting test
accuracy. In each iteration sequences of length 10000 were randomly selected. The learning algo-
rithm used was Adagrad1 with a learning rate of 0.001. Weights were initialized using so-called
Xavier initialization Glorot & Bengio (2010). Sequence length for BPTT was 100 and batch size
128, states were carried over for the entire sequence of 10000 emulating full BPTT. Forget bias was
set initially to 1. Other parameters set to zero. The algorithm was written in C++ and CUDA 8 and
ran on GTX Titan GPU for up to 10 days. Table 1 presents results comparing existing state-of-the-
art approaches to the introduced Feedback LSTM algorithm which outperforms all other methods
despite not having any regularizer.
Table 1: Bits per character on the Hutter Wikipedia dataset (test data).
BPC	
mRNN(Sutskever et al., 2011)	1.60
GF-RNN (Chung et al., 2015)	1.58
Grid LSTM (Kalchbrenner et al., 2015)	1.47
Standard LSTM4	1.45
MI-LSTM (Wu et al., 2016)	1.44
Recurrent Highway Networks (Zilly et al., 2016)	1.42
Array LSTM (Rocki, 2016)	1.40
Feedback LSTM	1.39
Hypernetworks (Ha et al., 2016)	1.38
Feedback LSTM + Zoneout (Krueger et al., 2016)	1.37
4	Summary
We introduced feedback recurrent network architecture, which takes advantage of temporal nature
of the data and monitors the discrepancy between predictions and observations. This prediction error
1with a modification taking into consideration only recent window of gradient updates
2http://mattmahoney.net/dc/text.html
3This method does not belong to the ’dynamic evaluation’ group: 1. It never actually sees test data during
training. 2. It does not adapt weights during testing
4our implementation
7
Under review as a conference paper at ICLR 2017
information, also known as surprisal, is used when making new guesses. We showed that combining
commonly used feedforward, recurrent and such feedback signals improves generalization capabil-
ities of Long-Short Term Memory network. It outperforms other stochastic and fully deterministic
approaches on enwik8 character level prediction achieving 1.37 BPC.
5	Further work
It is still an open question what the feedback should really constitute as well as how it should
interact with lower-level neurons (additive, multiplicative or another type of connection). Further
improvements may be possible with the addition of regularization. Another research direction is
incorporating sparsity in order improve disentangling sources of variation in temporal data.
Acknowledgements
This work has been supported in part by the Defense Advanced Research Projects Agency (DARPA).
References
JUnyoUng Chung, Caglar Gulcehre, KyUngHyUn Cho, and Yoshua Bengio. Gated feedback recurrent
neural networks. CoRR, abs/1502.02367, 2015. URL http://arxiv.org/abs/1502.
02367.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neu-
ral networks. In In Proceedings of the International Conference on Artificial Intelligence and
Statistics (AISTATS10). Society for Artificial Intelligence and Statistics, 2010.
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.
SePP Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735—
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. CoRR,
abs/1507.01526, 2015. URL http://arxiv.org/abs/1507.01526.
David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rose-
mary Ke, AnirUdh Goyal, YoshUa Bengio, HUgo Larochelle, Aaron C. CoUrville, and Chris Pal.
Zoneout: Regularizing rnns by randomly Preserving hidden activations. CoRR, abs/1606.01305,
2016. URL http://arxiv.org/abs/1606.01305.
Antti Rasmus, Harri ValPola, Mikko Honkala, Mathias Berglund, and TaPani Raiko. Semi-
suPervised learning with ladder network. CoRR, abs/1507.02672, 2015. URL http://arxiv.
org/abs/1507.02672.
Kamil Rocki. Recurrent memory array structures. arXiv preprint arXiv:1607.03085, 2016.
Ilya Sutskever, James Martens, and Geoffrey Hinton. Generating text with recurrent neural networks.
In Lise Getoor and Tobias Scheffer (eds.), Proceedings of the 28th International Conference on
Machine Learning (ICML-11), ICML ,11, pp. 1017-1024, New York, NY, USA, June 2011.
ACM. ISBN 978-1-4503-0619-5.
P. Werbos. Backpropagation through time: what does it do and how to do it. In Proceedings of
IEEE, volume 78, pp. 1550-1560, 1990.
Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan Salakhutdinov. On mul-
tiplicative integration with recurrent neural networks. CoRR, abs/1606.06630, 2016. URL
http://arxiv.org/abs/1606.06630.
Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutnk, and Jrgen Schmidhuber. Recurrent high-
way networks, 2016.
8