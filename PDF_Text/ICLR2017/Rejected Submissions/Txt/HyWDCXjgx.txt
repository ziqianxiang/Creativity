Under review as a conference paper at ICLR 2017
Multi-label Learning with the RNNs
for Fashion Search
Se-Yeoung Kim, Sang-Il Na, Ha-Yoon Kim, Moon-Ki Kim, Byoung-Ki Jeon
Machine Intelligence Lab., SK Planet
Seongnam City, South Korea
{seyeong,sang.il.na,hayoon,moonki,standard}@sk.com
TaeWan Kim *
Naver Labs, Naver Corp.
Seongnam City, South Korea
{taey.16@navercorp.com}
Ab stract
We build a large-scale visual search system which finds similar product images
given a fashion item. Defining similarity among arbitrary fashion-products is
still remains a challenging problem, even there is no exact ground-truth. To re-
solve this problem, we define more than 90 fashion-related attributes, and com-
bination of these attributes can represent thousands of unique fashion-styles. We
then introduce to use the recurrent neural networks (RNNs) recognising multiple
fashion-attributes with the end-to-end manner. To build our system at scale, these
fashion-attributes are again used to build an inverted indexing scheme. In addition
to these fashion-attributes for semantic similarity, we extract colour and appear-
ance features in a region-of-interest (ROI) of a fashion item for visual similarity.
By sharing our approach, we expect active discussion on that how to apply current
deep learning researches into the e-commerce industry.
1	Introduction
Online commerce has been a great impact on our life over the past decade. We focus on an online
market for fashion related items* 1. Finding similar fashion-product images for a given image query
is a classical problem in an application to computer vision, however, still challenging due to the
absence of an absolute definition of the similarity between arbitrary fashion items.
Deep learning technology has given great success in computer vision tasks such as efficient feature
representation (Razavian et al., 2014; Babenko et al., 2014), classification (He et al., 2016a; Szegedy
et al., 2016b), detection (Ren et al., 2015; Zhang et al., 2016), and segmentation (Long et al., 2015).
Furthermore, image to caption generation (Vinyals et al., 2015; Xu et al., 2015) and visual ques-
tion answering (VQA) (Antol et al., 2015) are emerging research fields combining vision, language
(Mikolov et al., 2010), sequence to sequence (Sutskever et al., 2014), long-term memory (Xiong
et al., 2016) based modelling technologies.
These computer vision researches mainly concern about general object recognition. However, in
our fashion-product search domain, we need to build a very specialised model which can mimic
human's perception of fashion-product similarity. To this end, we start by brainstorming about
what makes two fashion items are similar or dissimilar. Fashion-specialist and merchandisers are
also involved. We then compose fashion-attribute dataset for our fashion-product images. Table
1 explains a part of our fashion-attributes. Conventionally, each of the columns in Table 1 can be
modelled as a multi-class classification. Therefore, our fashion-attributes naturally is modelled as a
multi-label classification.
*This work was done by the author at SK Planet.
1In our e-commerce platform, 11st (http://english.11st.co.kr/html/en/main.html), al-
most a half of user-queries are related to the fashion styles, and clothes.
1
Under review as a conference paper at ICLR 2017
Table 1: An example of fashion-attributes.
Great-category (3 classes)	Fashion-category (19 classes)	Gender (2 classes)	Silhouette (14 classes)	Collar (18 classes)	sleeve-length . . . (6 classes)
bottom	T-shirts	male	normal	shirt	. long	..
top	pants	female	A-line	turtle	. a half	..
. . .	bags . . .		. . . . . .	round . . .	. sleeveless	.. .. .. ..
Multi-label classification has a long history in the machine learning field. To address this problem, a
straightforward idea is to split such multi-labels into a set of multi-class classification problems. In
our fashion-attributes, there are more than 90 attributes. Consequently, we need to build more than
90 classifiers for each attribute. It is worth noting that, for example, collar attribute can represent the
upper-garments, but it is absent to represent bottom-garments such as skirts or pants, which means
some attributes are conditioned on other attributes. This is the reason that the learning tree structure
of the attributes dependency can be more efficient (Zhang & Zhang, 2010; Fu et al., 2012; Gibaja &
Ventura, 2015).
Recently, recurrent neural networks (RNN) are very commonly used in automatic speech recognition
(ASR) (Graves et al., 2013; Graves & Jaitly, 2014), language modelling (Mikolov et al., 2010),
word dependency parsing (Mirowski & Vlachos, 2015), machine translation (Cho et al., 2014), and
dialog modelling (Henderson et al., 2014; Serban et al., 2016). To preserve long-term dependency
in hidden context, Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and its
variants (Zaremba et al., 2014; Cooijmans et al., 2016) are breakthroughs in such fields. We use this
LSTM to learn fashion-attribute dependency structure implicitly. By using the LSTM, our attribute
recognition problem is regarded to as a sequence classification. There is a similar work in Wang
et al. (2016), however, we do not use the VGG16 network (Simonyan & Zisserman, 2014) as an
image encoder but use our own encoder. To the best of our knowledge, it is the first work applying
LSTM into a multi-label classification task in the commercial fashion-product search domain.
The remaining of this paper is organized as follows. In Sec. 2, We describe details about our
fashion-attribute dataset. Sec. 3 describes the proposed fashion-product search system in detail.
Sec. 4 explains empirical results given image queries. Finally, we draw our conclusion in Sec. 5.
2	Building the fashion-attribute dataset
We start by building large-scale fashion-attribute dataset in the last year. We employ maximum 100
man-months and take almost one year for completion. There are 19 fashion-categories and more
than 90 attributes for representing a specific fashion-style. For example, top garments have the T-
shirts, blouse, bag etc. The T-shirts category has the collar, sleeve-length, gender, etc. The gender
attribute has binary classes (i.e. female and male). Sleeve-length attribute has multiple classes (i.e.
long, a half, sleeveless etc.). Theoretically, the combination of our attributes can represent thousands
of unique fashion-styles. A part of our attributes are in Table 1. ROIs for each fashion item in an
image are also included in this dataset. Finally, we collect 1 million images in total. This internal
dataset is to be used for training our fashion-attribute recognition model and fashion-product ROI
detector respectively.
3	Fashion-product search system
In this section, we describe the details of our system. The whole pipeline is illustrated in Fig. 3.
As a conventional information retrieval system, our system has offline and online phase. In offline
process, we take both an image and its textual meta-information as the inputs. The reason we take
additional textual meta-information is that, for example, in Fig. 1a dominant fashion item in the
image is a white dress however, our merchandiser enrolled it to sell the brown cardigan as described
2
Under review as a conference paper at ICLR 2017
women,s clothes/
cardigan and knit/
round-neck cardigan
brend-new/
women,s shirts, blouse/
see-through blouse
Figure 1: Examples of image and its textual meta-information.
in its meta-information. In Fig. 1b, there is no way of finding which fashion item is to be sold with-
out referring the textual meta-information seller typed manually. Therefore, knowing intension (i.e.
what to sell) for our merchandisers is very important in practice. To catch up with these intension,
we extract fashion-category information from the textual meta. The extracted fashion-category in-
formation is fed to the fashion-attribute recognition model. The fashion-attribute recognition model
predicts a set of fashion-attributes for the given image. (see Fig. 2) These fashion-attributes are
used as keys in the inverted indexing scheme. On the next stage, our fashion-product ROI detector
finds where the fashion-category item is in the image. (see Fig. 8) We extract colour and appear-
ance features for the detected ROI. These visual features are stored in a postings list. In these
processes, it is worth noting that, as shown in Fig. 8, our system can generate different results in
the fashion-attribute recognition and the ROI detection for the same image by guiding the fashion-
category information. In online process, there is two options for processing a user-query. We can
#shoes, #male, #leather,
#under-ankle, #low-heel,
#monochrom, #shoelace
#top-bottom, #dress, #female,
#slim, #mini, #pencil,
⅛ound-neck, #1Ong-SleeVed
Il
#bottom, #pants, #male,
#long, #sweetpants, #EIaStiC ∙waist,
#in-pocket, #SibOri, #brend-logo
#bottom, #pants, #female,
#long, #Skiny ∙shi∏outte,
#nOrmal-WaiSL #belt-type,
#botton-lock, #in-pocket, #fading
#top, #SUitTaCket, #male,
#tailored-collar, #long-sleeved,
#modem-fit, #tWO-button
#bag, #female, #midimum-size,
#1Iandbag, //zipper-lock, #leather
#shoes, #female, #leather,
#ankle-boot, #high-heel,
#monochrom, ⅛buckle
[⅛
#top, #COat, #female,
#lon.g-sleeved, #iTlonoChrOm,
⅛tailored-collar, ⅛car-coat,
#normal-fit #double-button-type
Figure 2: Examples of recognized fashion-attributes for given images.
take a guided information, what the user wants to find, or the fashion-attribute recognition model
automatically finds what fashion-category item is the most likely to be queried. This is up to the
user's choice. For the given image by the user, the fashion-attribute recognition model generates
fashion-attributes, and the results are fed into the fashion-product ROI detector. We extract colour
and appearance features in the ROI resulting from the detector. We access to the inverted index
addressed by the generated a set of fashion-attributes, and then get a postings list for each fashion-
attribute. We perform nearest-neighbor retrieval in the postings lists so that the search complexity is
reduced drastically while preserving the semantic similarity. To reduce memory capacity and speed
up this nearest-neighbor retrieval process once more, our features are binarized and CPU depen-
3
Under review as a conference paper at ICLR 2017
Offline
textual meta-infb.r
hrand-new I
Affribufe
recognition
Rol defection
ColoUrand appearence
feafure extraction
Figure 3: The whole pipeline of the proposed fashion-product search system. (Dashed lines denote
the flows of the guided information.)
postings
postings
COIOUrand appearence
feafure extraction
dent intrinsic instruction (i.e. assembly popcnt instruction2) is used for computing the hamming
distance.
3.1	Vision encoder network
We build our own vision encoder network (ResCeption) which is based on inception-v3 architecture
(Szegedy et al., 2016b). To improve both speed of convergence and generalization, we introduce a
shortcut path (He et al., 2016a;b) for each data-flow stream (except streams containing one convo-
lutional layer at most) in all inception-v3 modules. Denote input of l-th layer , xl ∈ R , output of
the l-th layer, xl+1, a l-th layer is a function, H : xl 7→ xl+1 and a loss function, L(θ; xL). Then
forward and back(ward)propagation is derived such that
xl+1 = H(xl) + xl
∂xl+1	∂H(xl)
-----=-----------+ + 1
∂xl	∂ xl
(1)
(2)
Imposing gradients from the loss function to l-th layer to Eq. (2),
∂L
∂x

∂L	∂xl+2 ∂xl+1
∂xL ... ∂xl+1 ∂xl
∂L	∂H(xL-2)	∂H(xL-1)
∂XL C + …+	∂χi	+	∂xl
i=L-1
(3)
As in the Eq. (3), the error signal, ∂∂L, goes down to the l-th layer directly through the shortcut
path, and then the gradient signals from (L - 1)-th layer to l-th layer are added consecutively (i.e.
Pi=L-I dHXXχ )). Consequently, all of terms in Eq. (3) are aggregated by the additive operation
instead of the multiplicative operation except initial error from the loss (i.e. -∂=l). It prevents
from vanishing or exploding gradient problem. Fig. 4 depicts network architecture for shortcut
2http://www.gregbugaj.com/?tag=assembly (accessed at Aug. 2016)
4
Under review as a conference paper at ICLR 2017
Figure 4: Network architecture for shortcut paths (depicted in two red lines) in an inception-v3
module.
paths in an inception-v3 module. We use projection shortcuts throughout the original inception-v3
modules due to the dimension constraint.3 To demonstrate the effectiveness of the shortcut paths in
the inception modules, we reproduce ILSVRC2012 classification benchmark (Russakovsky et al.,
2015) for inception-v3 and our ResCeption network. As in Fig. 5a, we verify that residual shortcut
paths are beneficial for fast training and slight better generalization.4 The whole of the training
curve is shown in Fig. 5b. The best validation error is reached at 23.37% and 6.17% at top-1
and top-5, respectively. That is a competitive result.5 To demonstrate the representation power of
our ResCeption, we employ the transfer learning strategy for applying the pre-trained ResCeption
as an image encoder to generate captions. In this experiment, we verify our ResCeption encoder
outperforms the existing VGG16 network6 on MS-COCO challenge benchmark (Chen et al., 2015).
The best validation CIDEr-D score (Vedantam et al., 2015) for c5 is 0.923 (see Fig. 5c) and test
CIDEr-D score for c40 is 0.937.7
(a) Early validation curve on ILSVRC2012 dataset.
(b) The whole training curve on ILSVRC2012
dataset.
(c) Validation curve on MS-COCO dataset.
Figure 5: Training curves on ILSVRC2012 and MS-COCO dataset with our ResCeption model.
3.2	Multi-label learning as sequence prediction by using the RNN
The traditional multi-class classification associates an instance x with a single label a from previ-
ously defined a finite set of labels A. The multi-label classification task associates several finite
sets of labels An ⊂ A. The most well known method in the multi-label literature are the binary
relevance method (BM) and the label combination method (CM). There are drawbacks in both BM
3If the input and output dimension of the main-branch is not the same, projection shortcut should be used
instead of identity shortcut.
4This is almost the same finding from Szegedy et al. (2016a) but our work was done independently.
5http://image-net.org/challenges/LSVRC/2015/results
6https://github.com/torch/torch7/wiki/ModelZoo
7We submitted our final result with beam search on MS-COCO evaluation server and found out the beam
search improves final CIDEr-D for c40 score by 0.02.
5
Under review as a conference paper at ICLR 2017
Figure 6: An example of the fashion-attribute dependence tree for a given image and the objective
function of our fashion-attribute recognition model.
and CM. The BM ignores label correlations that exist in the training data. The CM directly takes
into account label correlations, however, a disadvantage is its worst-case time complexity (Read
et al., 2009). To tackle these drawbacks, we introduce to use the RNN. Suppose we have ran-
dom variables a ∈ An , An ⊂ A. The objective of the RNN is to maximise the joint probability,
p(at, at-1, at-2, . . . a0), where t is a sequence (time) index. This joint probability is factorized as a
product of conditional probabilities recursively,
p(at, at-1, . . . a0)
p(ao)p(aι∣ao)p(a2∣a1, ao)…
'-----{------}
p(a0,a1)
(4)
z
p(a0,a1,a2)
p(a0,a1,a2,... )
p(a0) QtT=1p(at|at-1, . . . , a0).
Following the Eq. 4, we can handle multi-label classification as sequence classification which is
illustrated in Fig. 6. There are many label dependencies among our fashion-attributes. Direct mod-
elling of such label dependencies in the training data using the RNN is our key idea. We use the
ResCeption as a vision encoder θI , LSTM and softmax regression as our sequence classifier θseq,
and negative log-likelihood (NLL) as the loss function. We backpropagage gradient signal from the
sequence classifier to vision encoder.8 Empirical results of our ResCeption-LSTM based attribute
recognition are in Fig. 2. Many fashion-category dependent attributes such as sweetpants, fad-
ing, zipper-lock, mini, and tailored-collar are recognized quite well. Fashion-category independent
attributes (e.g., male, female) are also recognizable. It is worth noting we do not model the fashion-
attribute dependance tree at all. We demonstrate the RNN learns attribute dependency structure
implicitly. We evaluate our attribute recognition model on the fashion-attribute dataset. We split
this dataset into 721544, 40000, and 40000 images for training, validating, and testing. We employ
the early-stopping strategy to preventing over-fitting using the validation set. We measure precision
and recall for a set of ground-truth attributes and a set of predicted attributes for each image. The
quantitative results are in Table 2.
8Our attribute recognition model is parameterized as θ = [θI ; θseq]. In our case, updating θI as well as θseq
in the gradient descent step helps for much better performance.
6
Under review as a conference paper at ICLR 2017
Table 2: A quantitative evaluation of the ResCeption-LSTM based attribute recognition model.
Measurement	Train	Validation	Test
Precision	0.866	0.842	0.841
Recall	0.867	0.841	0.842
NLL	0.298	0.363	0.363
3.3	Guided ATTRIBUTE-SEQUENCE GENERATION
Our prediction model of the fashion-attribute recognition is based on the sequence generation pro-
cess in the RNN (Graves, 2013). The attribute-sequence generation process is illustrated in Fig.
7. First, we predict a probability of the first attribute for a given internal representation of the im-
age i.e. paseq(ao∣gθι(I)), and then sample from the estimated probability of the attribute, a0 〜
Pθseq(ao∣gθι(I)). The sampled symbol is fed to as the next input to compute pθseq(aι∣ao,gθι(I)).
This sequential process is repeated recursively until a sampled result is reached at the special end-
of-sequence (EOS) symbol. In case that we generate a set of attributes for a guided fashion-category,
we do not sample from the previously estimated probability, but select the guided fashion-category,
and then we feed into it as the next input deterministically. It is the key to considering for each
seller's intention. Results for the guided attribute-sequence generation is shown in Fig. 8.
ResCeption
g。I(D
LSTM
LSTM
0。Seq(Qo 血I (1))
Figure 7: Guided sequence generation process.
3.4	Guided ROI DETECTION
Our fashion-product ROI detection is based on the Faster R-CNN (Ren et al., 2015). In the conven-
tional multi-class Faster R-CNN detection pipeline, one takes an image and outputs a tuple of (ROI
coordinate, object-class, class-score). In our ROI detection pipeline, we take additional informa-
tion, guided fashion-category from the ResCeption-LSTM based attribute-sequence generator. Our
fashion-product ROI detector finds where the guided fashion-category item is in a given image. Jing
et al. (2015) also uses a similar idea, but they train several detectors for each category independently
so that their works do not scale well. We train a detector for all fashion-categories jointly. Our
detector produces ROIs for all of the fashion-categories at once. In post-processing, we reject ROIs
that their object-classes are not matched to the guided fashion-category. We demonstrate that the
guided fashion-category information contributes to higher performance in terms of mean average
precision (mAP) on the fashion-attribute dataset. We measure the mAP for the intersection-of-union
(IoU) between ground-truth ROIs and predicted ROIs. (see Table 3) That is due to the fact that our
guided fashion-category information reduces the false positive rate. In our fashion-product search
pipeline, the colour and appearance features are extracted in the detected ROIs.
7
Under review as a conference paper at ICLR 2017
Guided fashion-category:
Guided fashion-category:
skirt
blouse
Recognition results:
#top, #blous, #woman,
#waistline, #SIeeVeless,
#rOUnd-neck
Guided fashion-category:
T-shirt
Recognition results:
#topT ⅛tshirts #wOman,
#nOrmaI-fit, #waistline,
#rOUnd-neck,
⅛long-sleeved, #striped
Guided fashion-category:
pants
Recognition results:
#bottom, #Pants, ⅛woman,
#1Ong-Iine, #SkiiIy-ShiHoUtte,
#nOrmaI-Waist, ⅛belt-type7
#button-lock, ⅛in-pockett
#roll-up cuff, ⅛fading
Recognition results:
#bottoms, #Skirts, #wOman,
#maxi, #PIeated-Skirts,
⅛no-slit
■ ■
Giiidedfashion-Category.
dress
Recognition results:
#top-bottom, ⅛dress, #wOmen,
#mini, #slim-fit, #Straight-Skirt,
#roUnd-neck, #1Ong-SIeeVed
Recognition results:
#top-bottom, #dress,
#woman, #mini,
#regular-fit, ⅛ound-neck,
#1Ong-SIeeVed
Recognition results:
#bottom, ⅛leggings, #wOmen,
#long
Recognition results:
#top, ⅛shirt, #women,
#loose-fit, #button-lock,
#PUlIOVer, #COlIared shirt,
#1Ong-SleeVed
Figure 8: Examples of the consecutive process in the guided sequence generation and the guided
ROI detection. Although we take the same input image, results can be totally different guiding the
fashion-category information.
Table 3: Fashion-product ROI Detector evaluation. (mAP)
IoU	0.5	0.6	0.7	0.8	0.9
Guided	0.877	0.872	0.855	0.716	0.225
Non-guided	0.849	0.842	0.818	0.684	0.223
3.5	Visual feature extraction
To extract appearance feature for a given ROI, we use pre-trained GoogleNet (Szegedy et al., 2015).
In this network, both inception4 and inception5 layer's activation maps are used. We evaluate
this feature on two similar image retrieval benchmarks, i.e. Holidays (Jegou et al., 2008) and
UK-benchmark (UKB) (Nister & Stewenius, 2006). In this experiment, We do not use any Post-
processing method or fine-tuning at all. The mAP on Holidays is 0.783, and the precision@4 and
recall@4 on UKB is 0.907 and 0.908 respectively. These scores are competitive against several deep
feature representation methods (Razavian et al., 2014; Babenko et al., 2014). Examples of queries
and resulting nearest-neighbors are in Fig. 9. On the next step, we binarize this appearance fea-
ture by simply thresholding at 0. The reason we take this simple thresholding to generate the hash
code is twofold. The neural activation feature map at a higher layer is a sparse and distributed code
in nature. Furthermore, the bias term in a linear layer (e.g., convolutional layer) compensates for
8
Under review as a conference paper at ICLR 2017
Nearest neighbors (in order from left to right)
Figure 9: Examples of retrieved results on Holidays and UKB. The violet rectangles denote the
ground-truth nearest-neighbors corresponding queries.
aligning zero-centering of the output feature space weakly. Therefore, we believe that a code from
a well-trained neural model, itself, can be a good feature even to be binarized. In our experiment,
such simple thresholding degrades mAP by 0.02 on the Holidays dataset, but this method makes
it possible to scaling up in the retrieval. In addition to the appearance feature, we extract colour
feature using the simple (bins) colour histogram in HSV space, and distance between a query and
a reference image is computed by using the weighted combination of the two distances from the
colour and the appearance feature.
4 Empirical results
To evaluate empirical results of the proposed fashion-product search system, we select 3 million
fashion-product images in our e-commerce platform at random. These images are mutually ex-
clusive to the fashion-attribute dataset. We have again selected images from the web used for the
queries. All of the reference images pass through the offline process as described in Sec. 3, and
resulting inverted indexing database is loaded into main-memory (RAM) by our daemon system.
We send the pre-selected queries to the daemon system with the RESTful API. The daemon system
then performs the online process and returns nearest-neighbor images correspond to the queries.
In this scenario, there are three options to get similar fashion-product images. Option 1 is that
the fashion-attribute recognition model automatically selects fashion-category, the most likely to be
queried in the given image. Option 2 is that a user manually selects a fashion-category given a query
image. (see Fig. 10) Option 3 is that a user draw a rectangle to be queried by hand like Jing et al.
(2015). (see Fig. 11) By the recognized fashion-attributes, the retrieved results reflect the user's
main needs, e.g. gender, season, utility as well as the fashion-style, that could be lacking when using
visual feature representation only.
9
Under review as a conference paper at ICLR 2017
(a) For the Option2, the guided information is “pants”.
(b) For the option 2, the guided information is “blouse”.
Figure 10: Similar fashion-product search for the Option 1 and the Option 2.
10
Under review as a conference paper at ICLR 2017
Figure 11: Similar fashion-product search for the Option 3.
5 Conclusions
Today's deep learning technology has given great impact on various research fields. Such a success
story is about to be applied to many industries. Following this trend, we traced the start-of-the art
computer vision and language modelling research and then, used these technologies to create value
for our customers especially in the e-commerce platform. We expect active discussion on that how
to apply many existing research works into the e-commerce industry.
References
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence
Zitnick, and Devi Parikh. Vqa: Visual question answering. In The International Conference on
Computer Vision, 2015.
Artem Babenko, Anton Slesarev, Alexander Chigorin, and Victor S. Lempitsky. Neural codes for
image retrieval. CoRR, abs/1404.1777, 2014.
Xinlei Chen, Hao Fang, TsUng-Yi Lin, Ramakrishna Vedantam, SaUrabh Gupta, Piotr Dollar, and
C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR,
abs/1504.00325, 2015.
KyUngHyUn Cho, Bart van Merrienboer, Dzmitry BahdanaU, and YoshUa Bengio. On the properties
of neUral machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259, 2014.
Tim Cooijmans, Nicolas Ballas, Cesar Laurent, and Aaron C. Courville. Recurrent batch normal-
ization. CoRR, abs/1603.09025, 2016.
Bin Fu, Zhihai Wang, Rong Pan, Guandong Xu, and Peter Dolog. Learning tree structure of label
dependency for multi-label learning. Advances in Knowledge Discovery and Data Mining, 2012.
EVa Gibaja and Sebastian Ventura. A tutorial on multilabel learning. The ACM Computing Surveys,
2015.
11
Under review as a conference paper at ICLR 2017
Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.
Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural net-
works. In The International Conference on Machine Learning. JMLR Workshop and Conference
Proceedings, 2014.
Alex Graves, Abdel-Rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep
recurrent neural networks. In The IEEE International Conference on Acoustics, Speech and Signal
Processing, 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. arXiv preprint arXiv:1603.05027, 2016b.
M. Henderson, B. Thomson, and S. J. Young. Word-based Dialog State Tracking with Recurrent
Neural Networks. In The Annual SIGdial Meeting on Discourse and Dialogue, 2014.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Hamming embedding and weak geometric
consistency for large scale image search. In The European Conference on Computer Vision,
2008.
Yushi Jing, David Liu, Dmitry Kislyuk, Andrew Zhai, Jiajing Xu, Jeff Donahue, and Sarah Tavel.
Visual search at Pinterest. In ACM International Conference on Knowledge Discovery and Data
Mining, 2015.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition, 2015.
Tomas Mikolov, Martin Karafiat, LUkas BUrgeL Jan Cernocky, and Sanjeev Khudanpur. Recurrent
neural network based language model. In The Annual Conference of the International Speech
Communication Association, 2010.
Piotr Mirowski and Andreas Vlachos. DePendency recurrent neural language models for sentence
comPletion. CoRR, abs/1507.01193, 2015.
D. Nister and H. Stewenius. Scalable recognition with a vocabulary tree. In The IEEE Conference
on Computer Vision and Pattern Recognition, 2006.
Ali Sharif Razavian, Hossein AzizPour, JosePhine Sullivan, and Stefan Carlsson. Cnn features off-
the-shelf: An astounding baseline for recognition. In The IEEE Conference on Computer Vision
and Pattern Recognition Workshops, 2014.
Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. Classifier chains for multi-label
classification. In The European Conference on Machine Learning and Knowledge Discovery in
Databases, 2009.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region ProPosal networks. In Advances in Neural Information Processing Systems
28, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej KarPathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. The International Journal of Computer
Vision, 2015.
Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C. Courville, and Joelle Pineau.
Building end-to-end dialogue systems using generative hierarchical neural network models. In
The AAAI Conference on Artificial Intelligence, 2016.
K. Simonyan and A. Zisserman. Very deeP convolutional networks for large-scale image recogni-
tion. CoRR, abs/1409.1556, 2014.
12
Under review as a conference paper at ICLR 2017
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
The IEEE Conference on Computer Vision and Pattern Recognition, 2015.
Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. Inception-v4, inception-resnet and the
impact of residual connections on learning. In The International Conference on Learning Repre-
sentation Workshop, 2016a.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In The IEEE Conference on Computer Vision and
Pattern Recognition, 2016b.
Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In The IEEE Conference on Computer Vision and Pattern Recognition,
2015.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. In The IEEE Conference on Computer Vision and Pattern Recognition, 2015.
Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, and Wei Xu. CNN-RNN: A
unified framework for multi-label image classification. CoRR, abs/1604.04573, 2016.
Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and
textual question answering. In The International Conference on Machine Learning, 2016.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual at-
tention. In The International Conference on Machine Learning,pp. 2048-2057. JMLR Workshop
and Conference Proceedings, 2015.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
CoRR, abs/1409.2329, 2014.
Liliang Zhang, Liang Lin, Xiaodan Liang, and Kaiming He. Is faster R-CNN doing well for pedes-
trian detection? CoRR, abs/1607.07032, 2016.
Min-Ling Zhang and Kun Zhang. Multi-label learning by exploiting label dependency. In The ACM
International Conference on Knowledge Discovery and Data Mining, 2010.
13