Under review as a conference paper at ICLR 2017
Rectified Factor Networks for Biclustering
Djork-Arne Clevert, Thomas Unterthiner, & Sepp Hochreiter
Institute of Bioinformatics
Johannes Kepler University, Linz, Austria
{okko,unterthiner,hochreit}@bioinf.jku.at
Abstract
Biclustering is evolving into one of the major tools for analyzing large datasets
given as matrix of samples times features. Biclustering has several noteworthy
applications and has been successfully applied in life sciences and e-commerce
for drug design and recommender systems, respectively.
FABIA is one of the most successful biclustering methods and is used by compa-
nies like Bayer, Janssen, or Zalando. FABIA is a generative model that represents
each bicluster by two sparse membership vectors: one for the samples and one
for the features. However, FABIA is restricted to about 20 code units because of
the high computational complexity of computing the posterior. Furthermore, code
units are sometimes insufficiently decorrelated. Sample membership is difficult to
determine because vectors do not have exact zero entries and can have both large
positive and large negative values.
We propose to use the recently introduced unsupervised Deep Learning approach
Rectified Factor Networks (RFNs) to overcome the drawbacks of existing bi-
clustering methods. RFNs efficiently construct very sparse, non-linear, high-
dimensional representations of the input via their posterior means. RFN learning
is a generalized alternating minimization algorithm based on the posterior regu-
larization method which enforces non-negative and normalized posterior means.
Each code unit represents a bicluster, where samples for which the code unit is
active belong to the bicluster and features that have activating weights to the code
unit belong to the bicluster.
On 400 benchmark datasets with artificially implanted biclusters, RFN signifi-
cantly outperformed 13 other biclustering competitors including FABIA. In bi-
clustering experiments on three gene expression datasets with known clusters that
were determined by separate measurements, RFN biclustering was two times sig-
nificantly better than the other 13 methods and once on second place. On data
of the 1000 Genomes Project, RFN could identify DNA segments which indicate,
that interbreeding with other hominins starting already before ancestors of modern
humans left Africa.
1	Introduction
Biclustering is widely-used in statistics (A. Kasim & Talloen, 2016), and recently it also became
popular in the machine learning community (θ´ Connor & Feizi, 2014; Lee et al., 2015; Kolar
et al., 2011), e.g., for analyzing large dyadic data given in matrix form, where one dimension are
the samples and the other the features. A matrix entry is a feature value for the according sample.
A bicluster is a pair of a sample set and a feature set for which the samples are similar to each
other on the features and vice versa. Biclustering simultaneously clusters rows and columns of a
matrix. In particular, it clusters row elements that are similar to each other on a subset of column
elements. In contrast to standard clustering, the samples of a bicluster are only similar to each other
on a subset of features. Furthermore, a sample may belong to different biclusters or to no bicluster
at all. Thus, biclusters can overlap in both dimensions. For example, in drug design biclusters are
compounds which activate the same gene module and thereby indicate a side effect. In this example
different chemical compounds are added to a cell line and the gene expression is measured (Verbist
et al., 2015). If multiple pathways are active in a sample, it belongs to different biclusters and may
1
Under review as a conference paper at ICLR 2017
have different side effects. In e-commerce often matrices of costumers times products are available,
where an entry indicates whether a customer bought the product or not. Biclusters are costumers
which buy the same subset of products. In a collaboration with the internet retailer Zalando the
biclusters revealed outfits which were created by customers which selected certain clothes for a
particular outfit.
FABIA (factor analysis for bicluster acquisition, (Hochreiter et al., 2010)) evolved into one of the
most successful biclustering methods. A detailed comparison has shown FABIA’s superiority over
existing biclustering methods both on simulated data and real-world gene expression data (Hochre-
iter et al., 2010). In particular FABIA outperformed non-negative matrix factorization with sparse-
ness constraints and state-of-the-art biclustering methods. It has been applied to genomics, where it
identified in gene expression data task-relevant biological modules (Xiong et al., 2014). In the large
drug design project QSTAR, FABIA was used to extract biclusters from a data matrix that contains
bioactivity measurements across compounds (Verbist et al., 2015). Due to its successes, FABIA
has become part of the standard microarray data processing pipeline at the pharmaceutical company
Janssen Pharmaceuticals. FABIA has been applied to genetics, where it has been used to identify
DNA regions that are identical by descent in different individuals. These individuals inherited an
IBD region from a common ancestor (Hochreiter, 2013; Povysil & Hochreiter, 2014). FABIA is a
generative model that enforces sparse codes (Hochreiter et al., 2010) and, thereby, detects biclus-
ters. Sparseness of code units and parameters is essential for FABIA to find biclusters, since only
few samples and few features belong to a bicluster. Each FABIA bicluster is represented by two
membership vectors: one for the samples and one for the features. These membership vectors are
both sparse since only few samples and only few features belong to the bicluster.
However, FABIA has shortcomings, too. A disadvantage of FABIA is that it is only feasible with
about 20 code units (the biclusters) because of the high computational complexity which depends
cubically on the number of biclusters, i.e. the code units. If less code units were used, only the
large and common input structures would be detected, thereby, occluding the small and rare ones.
Another shortcoming of FABIA is that units are insufficiently decorrelated and, therefore, multiple
units may encode the same event or part of it. A third shortcoming of FABIA is that the membership
vectors do not have exact zero entries, that is the membership is continuous and a threshold have to
be determined. This threshold is difficult to adjust. A forth shortcoming is that biclusters can have
large positive but also large negative members of samples (that is positive or negative code values).
In this case it is not clear whether the positive pattern or the negative pattern has been recognized.
Rectified Factor Networks (RFNs; (Clevert et al., 2015)) RFNs overcome the shortcomings of
FABIA. The first shortcoming of only few code units is avoided by extending FABIA to thousands
of code units. RFNs introduce rectified units to FABIA’s posterior distribution and, thereby, allow
for fast computations on GPUs. They are the first methods which apply rectification to the posterior
distribution of factor analysis and matrix factorization, though rectification it is well established in
Deep Learning by rectified linear units (ReLUs). RFNs transfer the methods for rectification from
the neural network field to latent variable models. Addressing the second shortcoming of FABIA,
RFNs achieve decorrelation by increasing the sparsity of the code units using dropout from field of
Deep Learning. RFNs also address the third FABIA shortcoming, since the rectified posterior means
yield exact zero values. Therefore, memberships to biclusters are readily obtained by values that are
not zero. Since RFNs only have non-negative code units, the problem of separating the negative
from the positive pattern disappears.
2	Identifying Biclusters by Rectified Factor Networks
2.1	Rectified Factor Networks
We propose to use the recently introduced Rectified Factor Networks (RFNs; (Clevert et al., 2015))
for biclustering to overcome the drawbacks of the FABIA model. The factor analysis model and the
construction of a bicluster matrix are depicted in Fig. 1. RFNs efficiently construct very sparse, non-
linear, high-dimensional representations of the input. RFN models identify rare and small events in
the input, have a low interference between code units, have a small reconstruction error, and explain
the data covariance structure.
2
Under review as a conference paper at ICLR 2017
w
w *
0
0
0
0
0
0
0
0
4 8 12 16 20
0
0
0
0
1
2
3
4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 0 0 0 0 0 0
0	0	0 0	0	0	0
0	0	0 0	0	0	0
0	0	0	0	0
1	2	3	4	5
2	4	6	8	10
3	6	9	12	15
(a) Factor analysis model
(b) Outer product of two sparse vectors
Figure 1: Left: Factor analysis model: hidden units (factors) h, visible units v, weight matrix W,
noise e. Right: The outer product W hτ of two sparse vectors results in a matrix with a bicluster.
Note that the non-zero entries in the vectors are adjacent to each other for visualization purposes
only.
RFN learning is a generalized alternating minimization algorithm derived from the posterior reg-
ularization method which enforces non-negative and normalized posterior means. These posterior
means are the code of the input data. The RFN code can be computed very efficiently. For non-
Gaussian priors, the computation of the posterior mean ofanew input requires either to numerically
solve an integral or to iteratively update variational parameters. In contrast, for Gaussian priors the
posterior mean is the product between the input and a matrix that is independent of the input. RFNs
use a rectified Gaussian posterior, therefore, they have the speed of Gaussian posteriors but lead to
sparse codes via rectification. RFNs are implemented on GPUs.
The RFN model is a factor analysis model
V = Wh + e , (1)
which extracts the covariance structure of the data. The prior h 〜N (0, I) of the hidden units (fac-
tors) h ∈ Rl and the noise e 〜N (0, Ψ) of visible units (observations) V ∈ Rm are independent.
The model parameters are the weight (factor loading) matrix W ∈ Rm×l and the noise covariance
matrix Ψ ∈ Rm×m .
RFN models are selected via the posterior regularization method (Ganchev et al., 2010). For data
{V} = {V1,...,Vn}, it maximizes the objective F:
1n	1n
F = -XlogP(Vi)- - XDκL(Q(hi | Vi) k p(h | Vi)),	(2)
n i=1	n i=1
where DκL is the Kullback-Leibler distance. Maximizing F achieves two goals simultaneously: (1)
extracting desired structures and information from the data as imposed by the generative model and
(2) ensuring sparse codes via Q from the set of rectified Gaussians.
For Gaussian posterior distributions, and mean-centered data {V} = {V1,...,Vn}, the posterior
p(hi | Vi) is Gaussian with mean vector (μp)i and covariance matrix Σp:
(μp)i = (I + W T Ψ-1 W )-1 W T Ψ-1 Vi , Σp = (I + W T Ψ-1W )-1 . (3)
For rectified Gaussian posterior distributions, Σp remains as in the Gaussian case, but minimizing
the second DκL of Eq. (2) leads to constrained optimization problem (see Clevert et al. (2015))
1n
min 一 £(〃i - (μp)i)τ ς-1 (μi - (μp)i)
μi n
i=1
1n
s.t. Vi : μi ≥ 0 , ∀j : - E μij = 1 , (4)
n i=1
3
Under review as a conference paper at ICLR 2017
where "≥"is component-wise. In the E-SteP of the generalized alternating minimization algorithm
(Ganchev et al., 2010), which is used for RFN model selection, we only perform a step of the gradi-
ent projection algorithm (Bertsekas, 1976; Kelley, 1999), in particular a step of the projected Newton
method for solving Eq. (4) (Clevert et al., 2015). Therefore, RFN model selection is extremely effi-
cient but still guarantees the correct solution.
2.2 RFN Biclustering
For a RFN model, each code unit represents a bicluster, where samples, for which the code unit is
active, belong to the bicluster. On the other hand features that activates the code unit belong to the
bicluster, too. The vector of activations of a unit across all samples is the sample membership vector.
The weight vector which activates the unit is the feature membership vector. The un-constraint
posterior mean vector is computed by multiplying the input with a matrix according to Eq. (3). The
constraint posterior of a code unit is obtained by multiplying the input by a vector and subsequently
rectifying and normalizing the code unit (Clevert et al., 2015).
To keep feature membership vector sparse, we introduce a Laplace prior on the parameters. There-
fore only few features contribute to activating a code unit, that is, only few features belong to a
bicluster. Sparse weights Wi are achieved by a component-wise independent Laplace prior for the
weights:
n
P(Wi)= (√2)" Y e-√2 WkiI	(5)
k=1
The weight update for RFN (Laplace prior on the weights) is
W = W + η US-1 - W -α sign(W) .
(6)
Whereby the sparseness of the weight matrix can be controlled by the hyper-parameter α and U
1 En=I viμT and S =
of the sample membership
and S are defined as U =
to enforce more sparseness
n En=I μiμT + ∑, respectively. In order
vectors, we introduce dropout of code units.
Dropout means that during training some code units are set to zero at the same time as they get
rectified. Dropout avoids co-adaptation of code units and reduces correlation of code units — a
problem of FABIA which is solved.
RFN biclustering does not require a threshold for determining sample memberships to a bicluster
since rectification sets code units to zero. Further crosstalk between biclusters via mixing up negative
and positive memberships is avoided, therefore spurious biclusters do less often appear.
3	Experiments
In this section, we will present numerical results on multiple synthetic and real data sets to verify
the performance of our RFN biclustering algorithm, and compare it with various other biclustering
methods.
3.1	Methods Compared
To assess the performance of rectified factor networks (RFNs) as unsupervised biclustering methods,
we compare the following 14 biclustering methods:
(1)	RFN: rectified factor networks (Clevert et al., 2015), (2) FABIA: factor analysis with Laplace
prior on the hidden units (Hochreiter et al., 2010; Hochreiter, 2013), (3) FABIAS: factor analysis
with sparseness projection (Hochreiter et al., 2010), (4) MFSC: matrix factorization with sparse-
ness constraints (Hoyer, 2004), (5) plaid: plaid model (Lazzeroni & Owen, 2002; T. Chekouo &
Raffelsberger, 2015), (6) ISA: iterative signature algorithm (Ihmels et al., 2004), (7) OPSM: order-
preserving sub-matrices (Ben-Dor et al., 2003), (8) SAMBA: statistical-algorithmic method for bi-
cluster analysis (Tanay et al., 2002), (9) xMOTIF: conserved motifs (Murali & Kasif, 2003), (10)
Bimax: divide-and-conquer algorithm (Prelic et al., 2006), (11) CC: Cheng-Church δ-biclusters
4
Under review as a conference paper at ICLR 2017
(Cheng & Church, 2000), (12) plaid_t: improved plaid model (Turner et al., 2003), (13) FLOC:
flexible overlapped biclustering, a generalization of CC (Yang et al., 2005), and (14) spec: spectral
biclustering (Kluger et al., 2003).
For a fair comparison, the parameters of the methods were optimized on auxiliary toy data sets. If
more than one setting was close to the optimum, all near optimal parameter settings were tested. In
the following, these variants are denoted as method-variant (e.g. plaid_ss). For RFN We used the
following parameter setting: 13 hidden units, a dropout rate of 0.1, 500 iterations with a learning
rate of 0.1, and set the parameter α (controlling the sparseness on the weights) to 0.01.
3.2	Simulated Data Sets with Known Biclusters
In the following subsections, we describe the data generation process and results for synthetically
generated data according to either a multiplicative or additive model structure.
3.2.1	Data with Multiplicative Biclusters
We assumed n = 1000 genes and l = 100 samples and implanted p = 10 multiplicative biclusters.
The bicluster datasets with p biclusters are generated by following model:
p
X =	λi ziT + Υ , (7)
i=1
where Υ ∈ Rn×l is additive noise; λi ∈ Rn and zi ∈ Rl are the bicluster membership vectors
for the i-th bicluster. The λi ’s are generated by (i) randomly choosing the number Niλ of genes in
bicluster i from {10,...,210}, (ii) choosing Niλ genes randomly from {1,...,1000}, (iii) setting
λi components not in bicluster i to N(0, 0.22) random values, and (iv) setting λi components that
are in bicluster i to N (±3, 1) random values, where the sign is chosen randomly for each gene.
The zi ’s are generated by (i) randomly choosing the number Niz of samples in bicluster i from
{5,...,25}, (ii) choosing Niz samples randomly from {1,...,100}, (iii) setting zi components not
in bicluster i to N(0, 0.22) random values, and (iv) setting zi components that are in bicluster i to
N(2, 1) random values. Finally, we draw the Υ entries (additive noise on all entries) according to
N(0, 32) and compute the data X according to Eq. (7). Using these settings, noisy biclusters of
random sizes between 10 × 5 and 210 × 25 (genes×samples) are generated. In all experiments, rows
(genes) were standardized to mean 0 and variance 1.
3.2.2	Data with Additive Biclusters
In this experiment we generated biclustering data where biclusters stem from an additive two-way
ANOVA model:
p
X = »i	© (λi	ZT)	+ Y ,	θikj =	μi	+	αik	+	βij	, (8)
i=1
where © is the element-wise product of matrices and both λi and Zi are binary indicator vectors
which indicate the rows and columns belonging to bicluster i. The i-th bicluster is described by
an ANOVA model with mean μ%, k-th row effect aik (first factor of the ANOVA model), and j-
th column effect βij (second factor of the ANOVA model). The ANOVA model does not have
interaction effects. While the ANOVA model is described for the whole data matrix, only the effects
on rows and columns belonging to the bicluster are used in data generation. Noise and bicluster
sizes are generated as in previous Subsection 3.2.1.
Data was generated for three different signal-to-noise ratios which are determined by distribution
from which μ% is chosen: A1 (low signal) N(0, 22), A2 (moderate signal) N(±2,0.52), and A3
(high signal) N (±4, 0.52), where the sign of the mean is randomly chosen. The row effects αki are
chosen from N (0.5, 0.22) and the column effects βij are chosen from N(1, 0.52).
3.2.3	Results on Simulated Data Sets
For method evaluation, we use the previously introduced biclustering consensus score for two sets
of biclusters (Hochreiter et al., 2010), which is computed as follows:
5
Under review as a conference paper at ICLR 2017
Table 1: Results are the mean of 100 instances for each simulated data sets. Data sets M1 and A1-
A3 were multiplicative and additive bicluster, respectively. The numbers denote average consensus
scores with the true biclusters together with their standard deviations in parentheses. The best results
are printed bold and the second best in italics (“better” means significantly better according to both
a paired t-test and a McNemar test of correct elements in biclusters).
Method	multiplic. model M1	additive model		
		A1	A2	A3
RFN	0.643±7e-4	0.475±9e-4	0.640±1e-2	0.816±6e-7
FABIA	0.478±1e-2	0.109±6e-2	0.196±8e-2	0.475±1e-1
FABIAS	0.564 ± 3e-3	0.150±7e-2	0.268± r7e-2	0.546±1e-1
SAMBA	0.006±5e-5	0.002±6e-4	0.002±5e-4	0.003±8e-4
xMOTIF	0.002±6e-5	0.002±4e-4	0.002±4e-4	0.001±4e-4
MFSC	0.057±2e-3	0.000±0e-0	0.000±0e-0	0.000±0e-0
Bimax	0.004±2e-4	0.009±8e-3	0.010±9e-3	0.014±1e-2
plaid_ss	0.045±9e-4	0.039±2e-2	0.041±1e-2	0.074±3e-2
CC	0.001 ±7e-6	4e-4±3e-4	3e-4±2e-4	1e-4±1e-4
Plaid-ms	0.072±4e-4	0.064±3e-2	0.072±2e-2	0.112±3e-2
plaid_t_ab	0.046±5e-3	0.021±2e-2	0.005±6e-3	0.022±2e-2
Plaid_ms_5	0.083±6e-4	0.098±4e-2	0.143±4e-2	0.221±5e-2
plaid_t_a	0.037±4e-3	0.039±3e-2	0.010±9e-3	0.051±4e-2
FLOC	0.006±3e-5	0.005±9e-4	0.005±1e-3	0.003±9e-4
ISA	0.333±5e-2	0.039±4e-2	0.033±2e-2	0.140±7e-2
spec	0.032±5e-4	0.000±0e-0	0.000±0e-0	0.000±0e-0
OPSM	0.012±1e-4	0.007±2e-3	0.007±2e-3	0.008±2e-3
1.	Compute similarities between all pairs of biclusters by the Jaccard index, where one is from
the first set and the other from the second set;
2.	Assign the biclusters of one set to biclusters of the other set by maximizing the assignment
by the Munkres algorithm;
3.	Divide the sum of similarities of the assigned biclusters by the number of biclusters of the
larger set.
Step (3) penalizes different numbers of biclusters in the sets. The highest consensus score is 1 and
only obtained for identical sets of biclusters.
Table 1 shows the biclustering results for these data sets. RFN significantly outperformed all other
methods (t-test and McNemar test of correct elements in biclusters).
3.3	Gene Expression Data Sets
In this experiment, we test the biclustering methods on gene expression data sets, where the bi-
clusters are gene modules. The genes that are in a particular gene module belong to the according
bicluster and samples for which the gene module is activated belong to the bicluster. We consider
three gene expression data sets which have been provided by the Broad Institute and were previ-
ously clustered by Hoshida et al. (2007) using additional data sets. Our goal was to study how well
biclustering methods are able to recover these clusters without any additional information.
(A)	The “breast cancer” data set (van’t Veer et al., 2002) was aimed at a predictive gene signature
for the outcome of a breast cancer therapy. We removed the outlier array S54 which leads to a data
set with 97 samples and 1213 genes. In Hoshida et al. (2007), three biologically meaningful sub-
classes were found that should be re-identified.
(B)	The “multiple tissue types” data set (Su et al., 2002) are gene expression profiles from human
cancer samples from diverse tissues and cell lines. The data set contains 102 samples with 5565
genes. Biclustering should be able to re-identify the tissue types.
(C)	The “diffuse large-B-cell lymphoma (DLBCL)” data set (Rosenwald et al., 2002) was aimed at
predicting the survival after chemotherapy. It contains 180 samples and 661 genes. The three classes
found by Hoshida et al. (2007) should be re-identified.
6
Under review as a conference paper at ICLR 2017
Table 2: Results on the (A) breast cancer, (B) multiPle tissue samPles, (C) diffuse large-B-cell
lymPhoma (DLBCL) data sets measured by the consensus score. An “nc” entry means that the
method did not converge for this data set. The best results are in bold and the second best in italics
(“better” means significantly better according to a McNemar test of correct samPles in clusters).
The columns “#bc”, “#g”, “#s” Provide the numbers of biclusters, their average numbers of genes,
and their average numbers of samPles, resPectively. RFN is two times the best method and once on
second Place.
method
RFN
FABIA
FABIAS
MFSC
plaid_ss
plaid_ms
plaid_ms_5
ISA」
OPSM
SAMBA
xMOTIF
Bimax
CC
plaid_t_ab
Plaid工a
spec
FLOC
(A) breast cancer
(B) multiPle tissues
score #bc #g #s score #bc #g #s
72279993427114324
55513320000012210
aa
33355555285152235
2 13 1
3
32470565271120483
79480755736214294
CJ lɪ lɪ lɪ lɪ oʌ
11248894876723085
333233291222
734160354310c897
754355200011nc333
0.0.0.0.0.0.0.0.0.0.0.0. 0.0.0.
555555599954
215
555
3
565101103385c545
753397734523n579
34415 26 6 223
390452662865
3232342 1
(C) DLBCL
score	#bc	#g	#s
0.35	2	59	72
0.37	2	59	62
0.35	2	104	60
018	5	50	42
0,30	5	339	72
0,28	5	143	63
021	5	68	47
001	56	26	8
003	6	162	4
002	38	19	15
005	5	9	9
007	5	73	5
005	5	10	10
017	1	3	44
011	3	6	24
005	28	133	32
003	5	167	5
For methods assuming a fixed number of biclusters, we chose five biclusters — slightly higher than
the number of known clusters to avoid biases towards Prior knowledge about the number of actual
clusters. Besides the number of hidden units (biclusters) we used the same Parameters as described
in Sec. 3.1. The Performance was assessed by comParing known classes of samPles in the data sets
with the samPle sets identified by biclustering using the consensus score defined in Subsection 3.2.3
— here the score is evaluated for samPle clusters instead of biclusters. The biclustering results are
summarized in Table 2. RFN biclustering yielded in two out of three datasets significantly better
results than all other methods and was on second Place for the third dataset (significantly according
to a McNemar test of correct samPles in clusters).
3.4	1000 Genomes Data Sets
In this exPeriment, we used RFN for detecting DNA segments that are identical by descent (IBD). A
DNA segment is IBD in two or more individuals, if they have inherited it from a common ancestor,
that is, the segment has the same ancestral origin in these individuals. Biclustering is well-suited
to detect such IBD segments in a genotyPe matrix (Hochreiter, 2013; Povysil & Hochreiter, 2014),
which has individuals as row elements and genomic structural variations (SNVs) as column ele-
ments. Entries in the genotyPe matrix usually count how often the minor allele of a Particular SNV
is Present in a Particular individual. Individuals that share an IBD segment are similar to each other
because they also share minor alleles of SNVs (tagSNVs) within the IBD segment. Individuals that
share an IBD segment rePresent a bicluster.
For our IBD-analysis we used the next generation sequencing data from the 1000 Genomes Phase
3. This data set consists of low-coverage whole genome sequences from 2,504 individuals of the
main continental PoPulation grouPs (Africans (AFR), Asians (ASN), EuroPeans (EUR), and Ad-
mixed Americans (AMR)). Individuals that showed cryPtic first degree relatedness to others were
removed, so that the final data set consisted of 2,493 individuals. Furthermore, we also included
archaic human and human ancestor genomes, in order to gain insights into the genetic relationshiPs
between humans, Neandertals and Denisovans. The common ancestor genome was reconstructed
from human, chimPanzee, gorilla, orang-utan, macaque, and marmoset genomes. RFN IBD detec-

Under review as a conference paper at ICLR 2017
HG00114_GBR
HG00121_GBR
HG00131_GBR
HG00133_GBR
HG00137_GBR
HG00149_GBR
HG00258_GBR
HG00265_GBR
HG00325_FIN
HG00373_FIN
HG01051_PUR
HG01060_PUR
HG01148_CLM
HG01171_PUR
HG01204_PUR
HG01390_CLM
HG01626_IBS
NA07000_CEU
NA11829_CEU
NA11919_CEU
NA11993_CEU
NA12144_CEU
NA12144_CEU
NA12275_CEU
NA12282_CEU
NA12341_CEU
NA19707_ASW
NA19783_MXL
NA19788_MXL
NA19900_ASW
NA20334_ASW
NA20513_TSI
NA20515_TSI
NA20520_TSI
NA20520_TSI
NA20534_TSI
NA20585_TSI
NA20765_TSI
NA20795_TSI
model L
Ancestor
Neandertal
Denisova
11,877,884
11,883,084
11,888,284
11,893,484	11,896,70
11,872,737
Figure 2: Example of an IBD segment matching the Neandertal genome shared among multiple
populations. The rows give all individuals that contain the IBD segment and columns consecutive
SNVs. Major alleles are shown in yellow, minor alleles of tagSNVs in violet, and minor alleles of
other SNVs in cyan. The row labeled model L indicates tagSNVs identified by RFN in violet. The
rows Ancestor, Neandertal, and Denisova show bases of the respective genomes in violet if they
match the minor allele of the tagSNVs (in yellow otherwise). For the Ancestor genome we used the
reconstructed common ancestor sequence that was provided as part of the 1000 Genomes Project
data.
tion is based on low frequency and rare variants, therefore we removed common and private variants
prior to the analysis. Afterwards, all chromosomes were divided into intervals of 10,000 variants
with adjacent intervals overlapping by 5,000 variants
In the data of the 1000 Genomes Project, we found IBD-based indications of interbreeding between
ancestors of humans and other ancient hominins within Africa (see Fig. 2 as an example of an IBD
segment that matches the Neandertal genome).
4	Conclusion
We have introduced rectified factor networks (RFNs) for biclustering and benchmarked it with 13
other biclustering methods on artificial and real-world data sets.
On 400 benchmark data sets with artificially implanted biclusters, RFN significantly outperformed
all other biclustering competitors including FABIA. On three gene expression data sets with pre-
viously verified ground-truth, RFN biclustering yielded twice significantly better results than all
other methods and was once the second best performing method. On data of the 1000 Genomes
Project, RFN could identify IBD segments which support the hypothesis that interbreeding between
ancestors of humans and other ancient hominins already have taken place in Africa.
RFN biclustering is geared to large data sets, sparse coding, many coding units, and distinct mem-
bership assignment. Thereby RFN biclustering overcomes the shortcomings of FABIA and has the
potential to become the new state of the art biclustering algorithm.
Acknowledgment. We thank the NVIDIA Corporation for supporting this research with several
Titan X GPUs.
8
Under review as a conference paper at ICLR 2017
References
S	. Kaiser S. Hochreiter A. Kasim, Z. Shkedy and W. Talloen. Applied Biclustering Methods for Big
and High-Dimensional Data Using R. Chapman and Hall/CRC, 2016.
A. Ben-Dor, B. Chor, R. Karp, and Z. Yakhini. Discovering local structure in gene expression data:
the order-preserving submatrix problem. J. Comput Biol.,10(3-4):373-384, 2003.
D. P. Bertsekas. On the Goldstein-Levitin-Polyak gradient projection method. IEEE Trans. Automat.
Control, 21:174-184, 1976.
Y. Cheng and G. M. Church. Biclustering of expression data. In Proc. Int. Conf. on Intelligent
Systems for Molecular Biology, volume 8, pp. 93-103, 2000.
D.-A. Clevert, T. Unterthiner, A. Mayr, and S. Hochreiter. Rectified factor networks. In C. Cortes,
N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 28. Curran Associates, Inc., 2015.
K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar. Posterior regularization for structured latent
variable models. Journal of Machine Learning Research, 11:2001-2049, 2010.
S. Hochreiter. HapFABIA: Identification of very short segments of identity by descent characterized
by rare variants in large sequencing data. Nucleic Acids Res., 41(22):e202, 2013. doi: 10.1093/
nar/gkt1013.
S. Hochreiter, U. Bodenhofer, M. Heusel, A. Mayr, A. Mitterecker, A. Kasim, S. VanSanden, D. Lin,
W. Talloen, L. Bijnens, H. W. H. Gohlmann, Z. Shkedy, and D.-A. Clevert. FABIA: factor analysis
for bicluster acquisition. Bioinformatics, 26(12):1520-1527, 2010. doi: 10.1093/bioinformatics/
btq227.
Y. Hoshida, J.-P. Brunet, P. Tamayo, T. R. Golub, and J. P. Mesirov. Subclass mapping: Identifying
common subtypes in independent disease data sets. PLoS ONE, 2(11):e1195, 2007.
P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. J. Mach. Learn. Res., 5:
1457-1469, 2004.
J. Ihmels, S. Bergmann, and N. Barkai. Defining transcription modules using large-scale gene
expression data. Bioinformatics, 20(13):1993-2003, 2004.
C. T. Kelley. Iterative Methods for Optimization. Society for Industrial and Applied Mathematics
(SIAM), Philadelphia, 1999.
Y. Kluger, R. Basri, J. T. Chang, and M. B. Gerstein. Spectral biclustering of microarray data:
Coclustering genes and conditions. Genome Res., 13:703-716, 2003.
Mladen Kolar, Sivaraman Balakrishnan, Alessandro Rinaldo, and Aarti Singh. Minimax localization
of structural information in large noisy matrices. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett,
F. Pereira, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 24,
pp. 909-917. Curran Associates, Inc., 2011.
L. Lazzeroni and A. Owen. Plaid models for gene expression data. Stat. Sinica, 12(1):61-86, 2002.
Jason D Lee, Yuekai Sun, and Jonathan E Taylor. Evaluating the statistical significance of biclusters.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 28, pp. 1324-1332. Curran Associates, Inc., 2015.
T. M. Murali and S. Kasif. Extracting conserved gene expression motifs from gene expression data.
In Pac. Symp. Biocomputing, pp. 77-88, 2003.
Luke θ´ Connor and Soheil Feizi. Biclustering using message passing. In Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural In-
formation Processing Systems 27, pp. 3617-3625. Curran Associates, Inc., 2014.
G. Povysil and S. Hochreiter. Sharing of very short IBD segments between humans, neandertals,
and denisovans. In bioRxiv. WWW, 2014. doi: 10.1101/003988.
9
Under review as a conference paper at ICLR 2017
A. Prelic, S. Bleuler, P. Zimmermann, A. Wille, P. Buhlmann, W. Gruissem, L. Hennig, L. Thiele,
and E. Zitzler. A systematic comparison and evaluation of biclustering methods for gene expres-
Sion data. Bioinformatics, 22(9):1122-1129, 2006.
A. Rosenwald et al. The use of molecular profiling to predict survival after chemotherapy for diffuse
large-B-cell lymphoma. New Engl. J. Med., 346:1937-1947, 2002.
A. I. Su et al. Large-scale analysis of the human and mouse transcriptomes. P. Natl. Acad. Sci. USA,
99(7):4465-4470, 2002.
A. Murua T. Chekouo and W. Raffelsberger. The gibbs-plaid biclustering model. Ann. Appl. Stat., 9
(3):1643-1670, 2015.
A. Tanay, R. Sharan, and R. Shamir. Discovering statistically significant biclusters in gene expres-
sion data. Bioinformatics, 18(Suppl. 1):S136-S144, 2002.
H. Turner, T. Bailey, and W. Krzanowski. Improved biclustering of microarray data demonstrated
through systematic performance tests. Comput. Stat. Data An., 48(2):235-254, 2003.
L.	J. van’t Veer et al. Gene expression profiling predicts clinical outcome of breast cancer. Nature,
415:530-536, 2002.
B. Verbist, G. Klambauer, L. Vervoort, W. Talloen, Z. Shkedy, O. Thas, A. Bender, H. W. H.
Gohlmann, and S. Hochreiter. Using transcriptomics to guide lead optimization in drug discovery
projects: Lessons learned from the QSTAR project. Drug Discovery Today, 20(5):505-513, 2015.
ISSN 1359-6446.
M.	Xiong, B. Li, Q. Zhu, Y.-X. Wang, and H.-Y. Zhang. Identification of transcription factors
for drug-associated gene modules and biomedical implications. Bioinformatics, 30(3):305-309,
2014.
J.	Yang, H. Wang, W. Wang, and P. S. Yu. An improved biclustering method for analyzing gene
expression profiles. Int. J. Artif. Intell. T., 14(5):771-790, 2005.
10