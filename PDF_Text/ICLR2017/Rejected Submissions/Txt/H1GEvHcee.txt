Under review as a conference paper at ICLR 2017
Annealing Gaussian into ReLU: a New Sam-
pling Strategy for Leaky-ReLU RBM
Chun-Liang Li Siamak Ravanbakhsh Barnabas Poczos
Department of Machine Learning
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{chunlial,mravanba,bapoczos}@cs.cmu.edu
Ab stract
Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used
as the building block in energy-based deep generative models. Due to its numer-
ical stability and quantifiability of its likelihood, RBM is commonly used with
Bernoulli units. Here, we consider an alternative member of the exponential fam-
ily RBM with leaky rectified linear units - called leaky RBM. We first study the
joint and marginal distributions of the leaky RBM under different leakiness, which
leads to interesting interpretation of the leaky RBM model as truncated Gaussian
distribution. We then propose a simple yet efficient method for sampling from
this model, where the basic idea is to anneal the leakiness rather than the energy;
- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness
over iterations. This serves as an alternative to the annealing of the temperature
parameter and enables numerical estimation of the likelihood that are more effi-
cient and far more accurate than the commonly used annealed importance sam-
pling (AIS). We further demonstrate that the proposed sampling algorithm enjoys
relatively faster mixing than contrastive divergence algorithm, which improves the
training procedure without any additional computational cost.
1	Introduction
In this paper, we are interested in deep generative models. One may naively classify these models
into a family of directed deep generative models trainable by back-propagation (e.g., Kingma &
Welling, 2013; Goodfellow et al., 2014), and deep energy-based models, such as deep belief net-
work (Hinton et al., 2006) and deep Boltzmann machine (Salakhutdinov & Hinton, 2009). The
building block of deep energy-based models is a bipartite graphical model called restricted Boltz-
mann machine (RBM). The RBM model consists of two layers, visible and hidden. The resulting
graphical model which can account for higher-order interactions of the visible units (visible layer)
using the hidden units (hidden layer). It also makes the inference easier that there are no interactions
between the variables in each layer.
The conventional RBM uses Bernoulli units for both the hidden and visible units (Smolensky, 1986).
One extension is using Gaussian visible units to model general natural images (Freund & Haussler,
1994). For hidden units, we can also generalize Bernoulli units to the exponential family (Welling
et al., 2004; Ravanbakhsh et al., 2016).
Nair & Hinton (2010) propose a variation using Rectified Linear Unit (ReLU) for the hidden layer
with a heuristic sampling procedure, which has promising performance in terms of reconstruction
error and classification accuracy. Unfortunately, due to its lack of strict monotonicity, ReLU RBM
does not fit within the framework of exponential family RBMs (Ravanbakhsh et al., 2016). In-
stead we study leaky-ReLU RBM (leaky RBM) in this work and address two important issues i) a
better training (sampling) algorithm for ReLU RBM and; ii) a better quantification of leaky RBM
-i.e., evaluation of its performance in terms of likelihood.
We study some of the fundamental properties of leaky RBM, including its joint and marginal dis-
tributions (Section 2). By analyzing these distributions, we show that the leaky RBM is a union of
1
Under review as a conference paper at ICLR 2017
truncated Gaussian distributions. In this paper, we show that training leaky RBM involves under-
lying positive definite constraints. Because of this, the training can diverge if these constrains are
not satisfied. This is an issue that was previously ignored in ReLU RBM, as it was mainly used for
pre-training rather than generative modeling.
Our contribution in this paper is three-fold: I) we systematically identify and address model con-
straints in leaky RBM (Section 3); II) for the training of leaky RBM, we propose a meta algo-
rithm for sampling, which anneals leakiness during the Gibbs sampling procedure (Section 3) and
empirically show that it can boost contrastive divergence with faster mixing (Section 5); III) We
demonstrate the power of the proposed sampling algorithm on estimating the partition function. In
particular, comparison on several benchmark datasets shows that the proposed method outperforms
the conventional AIS (Salakhutdinov & Murray, 2008) in terms of efficiency and accuracy (Sec-
tion 4). Moreover, we provide an incentive for using leaky RBM by showing that the leaky ReLU
hidden units perform better than the Bernoulli units in terms of the model log-likelihood (Section 4).
2	Restricted B oltzmann Machine and ReLU
The Boltzmann distribution is defined as p(x) = e-E(x) /Z where Z = Px e-E(x) is the partition
function. Restricted Boltzmann Machine (RBM) is a Boltzmann distribution with a bipartite struc-
ture It is also the building block for many deep models (e.g., Hinton et al., 2006; Salakhutdinov &
Hinton, 2009; Lee et al., 2009), which are widely used in numerous applications (Bengio, 2009). The
conventional Bernoulli RBM, models the joint probability p(V, h) for the visible units V ∈ [0, 1]I and
the hidden units h ∈ [0,1]J as p(v, h) H exp(-E(v, h)), where E(v, h) = a>V — v>Wh + b>h.
The parameters are a ∈ RI, b ∈ RJ and W ∈ RI×J. We can derive the conditional probabilities as
p(vi = 1|h) = σ (X Wjhj + a)	and p(hj = 1|v) = σ	WijVi + bjj ,	(1)
where σ(x) = (1 + e-x)-1 is the sigmoid function.
One extension of Bernoulli RBM is replacing the binary visible units by linear units v ∈ RI with
independent Gaussian noise. The energy function in this case is given by
E(V, h) = X (Vi2-F - XX viWijhj + b>h.
i=1	2σi	i=1 j=1 σi
To simplify the notation, we assume a normalized data so that ai and σi is no longer required.
The energy function is accordingly simplified to E(v,h) = kVk v>Wh + b> h (Note that the
elimination does not influence the discussion and one can easily extend all the results in this paper
to the model that includes ai and σi .).
The conditional distributions are as follows:
p(vi∣h) = N (XX Wijhj,l)	and p(hj = 1|v) = σ (XXWij vi + bj ,
(2)
where N(μ, V) is a Gaussian distribution with mean μ and variance V. To simplify the notation,
in the following We define ηj∙ = PI=ι WijVi + bj - that is ηj∙ is the input to the jth hidden layer
neuron and similarly define νi = PjJ=1 Wij hj + ai . Using this notation the conditionals in the (2)
arep(Vi|%) = N(%, 1) andp(hj = 1∣ηj-) = σ(ηj-).
2.1	ReLU RBM with Continuous Visible Units
From (1) and (2), we can see that the mean of the p(hj|V) is the nonlinearity of the hidden unit
at ηj = PI=ι WijVi + bj - e.g., mean of the Bernoulli unit is the sigmoid function. From this
perspective, we can extend the sigmoid function to other functions and thus allow RBM to have
more expressive power (Ravanbakhsh et al., 2016). In particular, it would be interesting to use
rectified linear unit (ReLU) nonlinearity, f(ηj) = max(0, ηj), for generative modeling.
2
Under review as a conference paper at ICLR 2017
Nair & Hinton (2010) use an RBM with visible Gaussian unit and ReLU hidden activation functions
for pretraining. They suggest sampling from max(0, ηj +N (0, σ(ηj)) for conditional sampling from
the hidden units (compare to (2)). However, this sampling heuristic does not suggest the parametric
form of the joint ReLU-Gaussian distribution. This also means we cannot evaluate it using methods
such as Annealed Importance Sampling that require access to this parametric form. In fact, only
strictly monotonic activation functions can derive feasible joint and conditional distributions in the
exponential familly RBM and ReLU is not strictly monotonic Ravanbakhsh et al. (2016). Similar
activation functions that are monotonic are Softplus, f(ηj) = log(1 + eηj) and leaky ReLU (Maas
et al., 2013), defined as f(ηj) = max(cηj, ηj), where c ∈ (0, 1) is the leakiness parameter. In con-
trast to the ReLU RBM the joint parametric form of these two distributions are available. However,
the energy (logarithm of the joint probability) in the case of Softplus activation function contains a
polylogarithmic term that requires evaluation of an infinite series; see Table 1 in Ravanbakhsh et al.
(2016). For this reason, here we focus on Leaky-ReLU activation function.
By Ravanbakhsh et al. (2016), the conditional probability of the activation, assuming the nonlinear-
ity f(ηj), is generally defined as p(hj |v) = exp (-Df (ηj khj) + g(hj)), where Df (ηj khj) is the
Bregman Divergence associated with f, and g(hj) is the base (or carrier) measure in the exponential
family which ensures the distribution is well-defined. The Bergman divergence, for strictly mono-
tonic function f, is Df(η ∣∣h7-) = -%hj + F (n) + F *(h7-), where F with 看 F (%) = f(%) is
the anti-derivative (integral) of f and F* is the anti-derivative of f-1 (i.e., f-1(f (η)) = η); Note
that due to the strict monotonicity of f, f T is well-defined, and F and F* are commonly referred
to as conjugate duals.
Considering the leaky ReLU activation function f(η) = max(cη, η), using this formalism, the
conditional distributions of hidden units in the leaky RBM simplifies to (see Appendix A.1 for
details)
双=)={*j 1c,, ifηj≤ 0.	⑶
Since the visible units uses the identity function, the corresponding conditional distribution is a
Gaussian1
P(ViIh)= N (X Wij hj, I^ ,	⑷
Having these two conditional distributions is enough for training a leaky RBM model using con-
trastive divergence (Hinton, 2002) or some other alternatives (e.g., Tieleman, 2008; Tieleman &
Hinton, 2009).
3	Training and Sampling from leaky RBM
Given the conditional distributions p(v|h) andp(h|v), the joint distribution p(v, h) from the general
treatment for MRF model is (Yang et al., 2012; Ravanbakhsh et al., 2016)
P(v,h) H eχp (v>Wh — X(F*(Vi) + g(Vi))- X(F*(hj) + g(hj))) ,	(5)
i=1	j=1
where F* (Vi) and F* (hj) are anti-derivatives of the inverses of the activation functions f(Vi)
and f(hj) for visible units Vi and hidden units hj, respectively (see Section 2.1). Assuming
f(ηj) = max(cηj, c) and f(νi) = νi in leaky-ReLU RBM, the joint distribution above becomes
(see Appendix A.2 for details)
-X (h2+log
ηj ≤0
p(v, h) H exp ( v>Wh —	----^X j -j- + log
ηj>0
+ b> h ,
1which can also be written as ρ(vi∣h) = exp (—D∕(νi∣∣vi) + g(vi)), where Vi = Pj=ι Wijhj and
f(νi) = Vi and。/(⑷心)=(Vi — vi)2 and g(vi) =  log √2π.
3
Under review as a conference paper at ICLR 2017
Figure 1: A two dimensional
example with 3 hidden units.
Figure 2: An one dimensional
example of truncated Gaussian
distributions with different vari-
ances.
Figure 3: A three dimensional
example with 3 hidden units,
where Wj are orthogonal to
each other.
and the corresponding visible marginal distribution is
p(v)	H exp —』v>	I — ^X	WjW>	— C ^X Wj W>	V + ^X bj W>V +	C ^X	bjW>V
ηj>0	ηj≤0	ηj>0	ηj≤0
(6)
where Wj is the j-th column of W .
3.1 Leaky RBM as Union of Truncated Gaussian Distributions
From (6) we see that the marginal probability is determined by the affine constraints ηj > 0 or
ηj ≤ 0 for all hidden units j . By combinatorics, these constraints divide RI (the visible domain)
into at most M = PI=ι (J) convex regions Ri, ∙一RM. An example with I = 2 and J = 3 is
shown in Figure 1. If I > J, then we have at most 2J regions.
We discuss the two types of these regions. For bounded regions, such as R1 in Figure 1, the integra-
tion of (6) is also bounded, which results in a valid distribution. Before we discuss the unbounded
cases, We define Ω = I — Pj=I ajWjWj>, where aj = Inj>0 + CInj≤0. For the unbounded
region, if Ω ∈ Ri×i is a positive definite (PD) matrix, then the probability density is proportional to
a multivariate Gaussian distribution with mean μ = Ω-i (PJ=I αjbjWj) and precision matrix Ω
(covariance matrix Ω-i) but over an affine-constrained region. Therefore, the distribution of each
unbounded region can be treated as a truncated Gaussian distribution. The marginal distrubution can
be treated as a union of truncated Gaussain distribution. Note that leaky RBM is different from Su
et al. (2017), which use single truncated Gaussian distribution to model joint (conditional) distribu-
tions and require approximated and more complicated sampling algorithms for truncated Gaussian
distribution, while leaky RBM only requires to sample from Gaussian distributions.
On the other hand, if Ω is not PD, and the region Ri contains the eigenvectors with negative eigen-
values of Ω, the integration of (6) over Ri is divergent (infinite), which can not result in a valid
probability distribution. In practice, with this type of parameter, when we do Gibbs sampling on the
conditional distributions, the sampling will diverge. However, it is unfeasible to check exponentially
many regions for each gradient update.
Theorem 1. If I — WW> is positive definite, then I —	j αj Wj Wj> is also positive definite, for
all αj ∈ [0, 1].
The proof is shown in Appendix 1. From Theorem 1 we can see that if the constraint I — WW>
is PD, then one can guarantee that the distribution of every region is a valid truncated Gaussian
distribution. Therefore, we introduce the following projection step for each W after the gradient
update.
• IIttt-	τ^τ r ɪ ɪ 9
argmin kW — W k2F
W	(7)
~	~ -I-	、'
s.t. I — WW> 之 0
Theorem 2. The above projection step (7) can be done by shrinking the singular values to be less
than 1.
4
Under review as a conference paper at ICLR 2017
The proof is shown in Appendix C. The training algorithm of the leaky RBM is shown in Algo-
rithm 1. By using the projection step (7), we could treat the leaky RBM as the union of truncated
Gaussian distributions, which uses weight vectors to divide the space of visible units into several
regions and use a truncated Gaussian distribution to model each region. Note that the leaky RBM
model is different from Su et al. (2016), which uses a truncated Gaussian distribution to model the
conditional distribution p(h|v) instead of the marginal distribution.
The empirical study about the divergent values and the necessity of the projection step is shown in
Appendix D. Without the projection step, when we run Gibbs sampling for several iterations from the
model, the sampled values will diverge because the model does not have a valid marginal distribution
p(v). It also implies that we cannot train leaky RBM with larger CD steps without projection, which
would result in divergent gradients. The detailed discussion is shown in Appendix D.
Algorithm 1 Training Leaky RBM
for t = 1,... ,T do
Estimate gradient gθ by CD or other algorithms with (13) and (4), where θ = {W, a, b}.
θ⑴—θ(t-1) + ηgθ.
Project W (t) by (7).
end for
3.2 Sampling from Leaky-ReLU RBM
Gibbs sampling is the core procedure for RBM, including training, inference, and estimating the
partition function (Fischer & Igel, 2012; Tieleman, 2008; Salakhutdinov & Murray, 2008). For ev-
ery task, we start from randomly initializing v by an arbitrary distribution q, and iteratively sample
from the conditional distributions. Gibbs sampling guarantees the procedure result in the stationary
distribution in the long run for any initialized distribution q. However, if q is close to the target dis-
tribution p, it can significantly shorten the number of iterations to achieve the stationary distribution.
If we set the leakiness c to be 1, then (6) becomes a simple multivariate Gaussian distribution
N ((I - WW>)-1Wb, (I - WW>)-1), which can be easily sampled without Gibbs sampling.
Also, the projection step (7) guarantees it is a valid Gaussian distribution. Then we decrease the
leakiness with a small , and use samples from the multivariate Gaussian distribution when c = 1
as the initialization to do Gibbs sampling. Note that the distribution of each region is a truncated
Gaussian distribution. When we only decrease the leakiness with a small amount, the resulted dis-
tribution is a “similar” truncated Gaussian distribution with more concentrated density. From this
observation, we could expect the original multivariate Gaussian distribution serves as a good initial-
ization. The one-dimensional example is shown in Figure 2. We then repeat this procedure until we
reach the target leakiness. The algorithm can be seen as annealing the leakiness during the Gibbs
sampling procedure. The meta algorithm is shown in Algorithm 2. Next, we show the proposed
sampling algorithm can help both the partition function estimation and the training of leaky RBM.
Algorithm 2 Meta Algorithm for Sampling from Leaky RBM.
Sample v from N ((I - WW>)-1Wb, (I - WW>)-1)
= (1 - c)/T and c0 = 1
for t = 1, . . . , T do
Decrease c0 = c0 - and perform Gibbs sampling by using (13) and (4) with leakiness c0
end for
4 Partition Function Estimation
It is known that estimating the partition function of RBM is intractable (Salakhutdinov & Murray,
2008). Existing approaches, including Salakhutdinov & Murray (2008); Grosse et al. (2013); Liu
et al. (2015); Carlson et al. (2016) focus on using sampling to approximate the partition function of
the conventional Bernoulli RBM instead of the RBM with Gaussian visible units and non-Bernoulli
hidden units. In this paper, we focus on extending the classic annealed importance sampling (AIS)
algorithm (Salakhutdinov & Murray, 2008) to leaky RBM.
5
Under review as a conference paper at ICLR 2017
	J = 5	J = 10	J = 20	J =30
Log partition function	2825.48	2827.98	2832.98	2837.99
Table 1: The true partition function for Leaky-ReLU RBM with different number of hidden units.
Assuming that We want to estimate the partition function Z of p(v) with p(v) = p* (v)/Z and
p*(v) a Phexp(-E(v,h)), Salakhutdinov & Murray (2008) start from a initial distribution
po(V) a Ph exp(-Eo(v, h)), where computing the partition Zo of po(V) is tractable and we can
draw samples fromp0(v). They then use the “geometric path” to anneal the intermediate distribution
as Pk(V) a Pk(V) = Ph exp(一βkEo(v, h) — (1 — βk)E(v, h)), where they grid βk from 1 to 0. If
we let β0 = 1, we can draw samples Vk frompk(V) by using samples Vk-1 from pk-1(V) for k ≥ 1
via Gibbs sampling. The partition function is then estimated via Z =答 PM=I ω(i), where
ω(i) = pKvW Pi(V"…PK-I(VK-2) PK (VK-I)	d 氏 =0
Po(voi)) Pi(Vii))	pK-2(VK)-2)PK-I(VK-1)，	K
Salakhutdinov & Murray (2008) use the initial distribution with independent visible units and with-
out hidden units. We consider application of AIS to the Ieaky-ReLU case with Eo(v, h) = kv^,
which results in a multivariate Gaussian distribution P0 (V). Compared with the meta algorithm
shown in Algorithm 2 which anneals between leakiness, AIS anneals between energy functions.
4.1	Study on Toy Examples
As we discussed in Section 3.1, leaky RBM with J hidden units is a union of 2J truncated Gaussian
distributions. Here we perform a study on the leaky RBM with a small number hidden units. Since
in this example the number of hidden units is small, we can integrate out all possible configurations
of h. However, integrating a truncated Gaussian distribution with general affine constraints does
not have analytical solutions, and several approximations have been developed (e.g., Pakman &
Paninski, 2014). To compare our results with the exact partition function, we consider a special case
that has the following form:
p(v) a exp (— 2v> ( I - X Wj Wjr - c X Wj Wjr ) v ) .	(8)
ηj >0	ηj ≤0
Compared to (6), it is equivalent to the setting where b = 0. Geometrically, every Wj passes through
the origin. We further put the additional constraint Wi ⊥ Wj , ∀i 6= j . Therefore. we divide the
whole space into 2J equally-sized regions. A three dimensional example is shown in Figure 3. Then
the partition function of this special case has the analytical form
Z = J X	(2π)-21 (I- XɑjWjWJ	.
αj ∈{1,c},∀j	j=1
We randomly initialize W and use SVD to make columns orthogonal. Also, we scale kWj k to
satisfy I - WW r 0. The leakiness parameter is set to be 0.01. For Salakhutdinov & Murray
(2008) (AIS-Energy), we use 105 particles with 105 intermediate distributions. For the proposed
method (AIS-Leaky), we use only 104 particles with 103 intermediate distributions. In this small
problem we study the cases when the model has 5, 10, 20 and 30 hidden units and 3072 visible units.
The true log partition function log Z is shown in Table 1 and the difference between log Z and the
estimates given by the two algorithms are shown in Table 2.
From Table 1, we observe that AIS-Leaky has significantly better and more stable estimations
than AIS-Energy especially and this gap increases as we increase the number of hidden units.
AIS-Leaky achieves this with orders magnitude reduced computation 一e.g., here it uses 〜.1%
of resources used by conventional AIS. For example, when we increase J from 5 to 30, the bias (dif-
ference) of AIS-Leaky only increases from 0.02 to 0.13; however, the bias of AIS-Energy increases
from 1.76 to 9.6. We further study the implicit connection between the proposed AIS-Leaky and
AIS-Energy in Appendix E, which shows AIS-Leaky is a special case of AIS-Energy under certain
conditions.
6
Under review as a conference paper at ICLR 2017
	J = 5	J = 10	J=20	J = 30
AIS-Energy	1.76 ± 0.011	3.56 ± 0.039	7.95 ± 0.363	9.60 ± 0.229
AIS-Leaky	0.02 ± 0.001^	0.04 ± 0.0O2~	0.08 ± 0.003	0.13 ± 0.004^
Table 2: The difference between the true partition function and the estimations of two algorithms
with standard deviation.
	CIFAR-10	SVHN
Bernoulli-Gaussian RBM	-2548.3	-2284.2
Leaky-ReLURBN 一	-1031.1	-182.4
Table 3: The log-likelihood performance of Bernoulli-Gaussian RBM and leaky RBM.
4.2 Comparison between leaky-ReLU RBM and Bernoulli-Gaussian RBM
It is known that the reconstruction error is not a proper approximation of the likelihood (Hinton,
2012). One commonly adopted way to compare generative models is to sample from the model,
and visualize the images to check the quality. However, Theis et al. (2016) show the better visu-
alization does not imply better likelihood. Also, the single layer model cannot adequately model
the complicated natural images (the result for Bernoulli-Gaussian RBM has been shown in Ran-
zato & Hinton (2010)), which makes the visualization comparison difficult (Appendix F has few
visualization results).
Fortunately, our accurate estimate of the partition function for leaky RBM can produce a reli-
able quantitative estimate of the representation power of leaky RBM. We compare the Bernoulli-
Gaussian RBM2 , which has Bernoulli hidden units and Gaussian visible units. We trained both
models with CD-203 and momentum. For both model, we all used 500 hidden units. We initialized
W by sampling from Unif(0, 0.01), a = 0, b = 0 and σ = 1. The momentum parameter was 0.9 and
the batch size was set to 100. We tuned the learning rate between 10-1 and 10-6. We studied two
benchmark data sets, including CIFAR10 and SVHN. The data was normalized to have zero mean
and standard deviation of 1 for each pixel. The results of the log-likelihood are reported in Table 3.
From Table 3, leaky RBM outperforms Bernoulli-Gaussian RBM significantly. The unsatisfactory
performance of Bernoulli-Gaussian RBM may be in part due to the optimization procedure. If we
tune the decay schedule of the learning-rate for each dataset in an ad-hoc way, we observe the
performance of Bernoulli-Gaussian RBM can be improved by 〜300 nats for both datasets. Also,
increasing CD-steps brings slight improvement. The other possibility is the bad mixing during the
CD iterations. The advanced algorithms Tieleman (2008); Tieleman & Hinton (2009) may help.
Although Nair & Hinton (2010) demonstrate the power of ReLU in terms of reconstruction error
and classification accuracy, it does not imply its superior generative capability. Our study confirms
leaky RBM could have much better generative performance compared to Bernoulli-Gaussian
RBM.
5	B etter Mixing by Annealing Leakiness
In this section, we show the idea of annealing between leakiness benefit the mixing in Gibbs sam-
pling in other settings. A common procedure for comparison of sampling methods for RBM is
through visualization. Here, we are interested in more quantitative metrics and the practical benefits
of improved sampling. For this, we consider optimization performance as the evaluation metric.
The gradient of the log-likelihood function L(θ∖vdata) of general RBM models is
dL⑹Vdata) _E	IdE(V,h)] _E	IdE(V,h)]	(9)
∂θ	h∣vdata	∂θ	v,h ∂θ .	(
Since the second expectation in (9) is usually intractable, different approximation algorithms are
used (Fischer & Igel, 2012).
2Our GPU implementation with gnumpy and cudamat can reproduce the results of
http://www.cs.toronto.edu/ tang/code/GaussianRBM.m
3CD-n means that contrastive divergence was run for n steps
7
Under review as a conference paper at ICLR 2017
Iterations
(a) SVHN
Iterations
(b) CIFAR10
×104
×104
Figure 4: Training leaky RBM with different sampling algorithms.
In this section, we compare two gradient approximation procedures. The baselines are the conven-
tional contrastive divergence (CD) (Hinton, 2002) and persistent contrastive divergence (Tieleman,
2008) (PCD). The second method is using Algorithm 2 (Leaky) with the same number of mixing
steps as CD. The experiment setup is the same as that of Section 4.
The results are shown in Figure 4. The proposed sampling procedure is slightly better than typical
CD steps. The reason is we only anneals the leakiness for 20 steps. To get accurate estimation
requires thousands of steps as shown in Section 4 when we estimate the partition function. There-
fore, the estimated gradient is still inaccurate. However, it still outperforms the conventional CD
algorithm. On the other hand, unlike the binary RBM case shown in Tieleman (2008), PCD does
not outperform CD with 20 mixing steps for leaky RBM.
The drawback of Algorithm 2 is that sampling V from N ((I 一 WW>)-1Wb, (I 一 WW>)-1)
requires computing mean, covariance and the Cholesky decomposition of the covariance matrix in
every iteration, which are computationally expensive. We study a mixture algorithm by combin-
ing CD and the idea of annealing leakiness. The mixture algorithm replaces the sampling from
N ((I 一 WW>)-1Wb, (I 一 WW>)-1) with sampling from the empirical data distribution. The
resulted mix algorithm is almost the same as CD algorithm while it anneals the leakiness over the
iterations as Algorithm 2. The results of the mix algorithm is also shown in Figure 4.
The mix algorithm is slightly worse than the original leaky algorithm, but it also outperforms the
conventional CD algorithm without additional computation cost. The comparison in terms of CPU
time is shown in Appendix F. Annealing the leakiness helps the mix algorithm explore different
modes of the distribution, thereby improves the training. The idea could also be combined with
more advanced algorithms (Tieleman, 2008; Tieleman & Hinton, 2009)4.
6	Conclusion
In this paper, we study the properties of the exponential family distribution produced by leaky RBM.
This study relates the leaky RBM model and truncated Gaussian distribution and reveals an under-
lying positive definite constraint of training leaky RBM. We further proposed a meta sampling algo-
rithm, which anneals between leakiness during the Gibbs sampling procedure. We first demonstrate
the proposed sampling algorithm is significantly more effective and efficient in estimating the par-
tition function than the conventional AIS algorithm. Second, we show that the proposed sampling
algorithm has comparatively better mixing properties (compared to CD). A few direction are worth
further study; in particular we are investigating on speeding up the naive projection step; either us-
ing the barrier function as shown in Hsieh et al. (2011) or by eliminating the need for projection by
artificially bounding the domain via additional constraints.
4We studied the PCD extension of the proposed sampling algorithm. However, the performance is not as
stable as CD.
8
Under review as a conference paper at ICLR 2017
References
Y. Bengio. Learning deep architectures for ai. Found. Trends Mach. Learn., 2009.
Jorg Bomschein and Yoshua Bengio. ReWeighted wake-sleep. In ICLR, 2015.
Y. Burda, R. B. Grosse, and R. Salakhutdinov. Accurate and conservative estimates of mrf log-
likelihood using reverse annealing. In AISTATS, 2015.
D. E. Carlson, P. Stinson, A. Pakman, and L. Paninski. Partition functions from rao-blackwellized
tempered sampling. In ICML, 2016.
KyungHyun Cho, Tapani Raiko, and Alexander Ilin. Enhanced gradient for training restricted boltz-
mann machines. Neural Computation, 2013.
A. Fischer and C. Igel. An introduction to restricted boltzmann machines. In CIARP, 2012.
Y. Freund and D. Haussler. Unsupervised learning of distributions on binary vectors using two layer
networks. Technical report, 1994.
I.	Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In ICML. 2014.
R. B. Grosse, C. J. Maddison, and R. Salakhutdinov. Annealing between distributions by averaging
moments. In NIPS, 2013.
G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computa-
tion, 2002.
G. E. Hinton. A practical guide to training restricted boltzmann machines. In Neural Networks:
Tricks of the Trade (2nd ed.). 2012.
G.	E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural
Computation, 2006.
C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar. Sparse inverse covariance matrix estima-
tion using quadratic approximation. In NIPS, 2011.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. CoRR, 2013.
H.	Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable
unsupervised learning of hierarchical representations. In ICML, 2009.
Q.	Liu, J. Peng, A. Ihler, and J. Fisher III. Estimating the partition function by discriminance
sampling. In UAI, 2015.
A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectifier nonlinearities improve neural network acoustic
models. In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing,
2013.
V.	Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML,
2010.
A. Pakman and L. Paninski. Exact hamiltonian monte carlo for truncated multivariate gaussians.
Journal of Computational and Graphical Statistics, 2014.
N. Parikh and S. Boyd. Proximal algorithms. Found. Trends Optim., 2014.
M. Ranzato and G. E. Hinton. Modeling pixel means and covariances using factorized third-order
boltzmann machines. In CVPR, 2010.
S. Ravanbakhsh, B. Poczos, J. G. Schneider, D. Schuurmans, and R. Greiner. Stochastic neural
networks with monotonic activation functions. In AISTATS, 2016.
R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
9
Under review as a conference paper at ICLR 2017
R. Salakhutdinov and I. Murray. On the quantitative analysis of Deep Belief Networks. In ICML,
2008.
P.	Smolensky. Parallel distributed processing: Explorations in the microstructure of cognition, vol.
1. 1986.
Q.	Su, X. Liao, C. Chen, and L. Carin. Nonlinear statistical learning with truncated gaussian graph-
ical models. In ICML, 2016.
Qinliang Su, Xuejun Liao, Chunyuan Li, Zhe Gan, and Lawrence Carin. Unsupervised learning
with truncated gaussian graphical models. In AAAI, 2017.
L.	Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In ICLR,
2016.
T. Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradi-
ent. In ICML, 2008.
T. Tieleman and G.E. Hinton. Using Fast Weights to Improve Persistent Contrastive Divergence. In
ICML, 2009.
M.	Welling, M. Rosen-Zvi, and G. E. Hinton. Exponential family harmoniums with an application
to information retrieval. In NIPS, 2004.
E.	Yang, P. Ravikumar, G. I. Allen, and Z. Liu. Graphical models via generalized linear models. In
NIPS, 2012.
A Derivation of leaky (ReLU) RBM
A.1 Conditional Distributions
For leaky RBM, the activation function of hidden units is defined as f (ηj) = max(cηj , ηj), where
c ∈ (0, 1) and ηj = PiI=1 Wij vi + bj. The inverse function of f is f-1(hj) = min(hj, hj /c).
Therefore, the anti-derivatives are
F(η) = [ Cn2, ifj 0	(10)
j	I2ηj, else,
and
F*/{j j 0	(II)
The activation function of Gaussian visible units can be treated as the linear unit f(νi) = νi, where
Vi = Pj=ι Wijhj. Following the similar steps for deriving F and F*, we get the anti-derivatives
F(Vi) = 1V2 and F*(vJ = 11 v2.
From Ravanbakhsh et al. (2016), the conditional distribution is defined as
P(hj∖η) = exp (-ηjhj + F(ηj) + F*(hj))
By plugging F and F * into (12), we get the conditional distribution for leaky RBM
p(hj|v)
∣N(%, i)with g(hj) = - log(√2∏),
IN(Cnj, c)with g(hj) = - log(√2cπ),
ifηj > 0
ifηj ≤ 0.
(12)
(13)
Similarly, we have p(v∕νi) = N(νi, 1) with g(vi) = - log(√2π).
10
Under review as a conference paper at ICLR 2017
A.2 Joint and Marginal Distributions
Given the conditional distributions p(v |h) andp(h|v), the joint distribution p(v, h) from the general
treatment for MRF model given by Yang et al. (2012) is
I
J
p(v,h) Y exp v>Wh — E(F*(vi) + g(vi)) - E(F*(hj) + g(hj)),
(14)
i=1
j=1
By plugging F *, F * and g from Section A.1 into (14), We have
p(v, h) Y exp vrWh — kvk-----------^X j -j- + log
2	ηj>0 2
Then the marginal distribution is
-X h2 +log
ηj ≤0
+ b> h ,
p(v)	Y
exp
p(v, h)dh
h
Z
h
hj2
eχp - -2 + ηj hj - log
ηj>0	2
IeXp ( η2 ! Y
ηj >0	ηj ≤0
hj2
∏ - 2C + hj ηj- log
ηj≤0
dh
exp I-2v> (I- X WjWjr -c X WjWjT)V + X bjW>V + Cχ bjW>V
ηj>0
ηj≤0
ηj>0
ηj≤0
Y
Y
Y
B	Proof of Theorem 1
Proof. Since WW r -PjαjWjWj = Pj(1-αj)WjWjr	0,WehaveWWr	PjαjWjWj.
Therefore, I — PjajWjWjr 上 I — WW> 上 0.	□
C Proof of Theorem 2
.	-	-.	..	一一_ _	___ ______-T 一二-	~ C--T 一	一
Proof. Let the SVD decomposition of W and W as W = USV> and W = USV>. Then we have
I
kW - WllF = IlUSV> - USV>∣∣F ≥ X(Sii- Sii)2,	(15)
i=1
1 .1	.	∙	. T T V T-T-T 7^-Γ ∖ C	1	∙ , ,	l^∖	, K /r∖∕' r-ɪ-ɪl ,	C∙	F 1 1
and the constraint I - WW r 0 can be rewritten as 0 ≤ Sii ≤ 1, ∀i. The transformed problem
has a Lasso-like formulation and we can solve itby Sii = min(Sii, 1) (Parikh & Boyd, 2014). Also,
I
the lower bound	i=1(Sii - Sii)2 in (15) becomes a tight bound when we set U = U and V = V,
which completes the proof.
D Necessity of the Projection Step
We conduct a short comparison to demonstrate the projection step is necessary for the leaky RBM
on generative tasks. We train two leaky RBM as follows. The first model is trained by the same
setting in Section 4. We use the convergence of log likelihood as the stopping criteria. The second
model is trained by CD-1 with weight decay and without the projection step. We stop the training
when the reconstruction error is less then 10-2. After we train these two models, we run Gibbs
sampling with 1000 independent chains for several steps and output the average value of the visible
units. Note that the visible units are normalized to zero mean. The results on SVHN and CIFAR10
are shown in Figure 5.
From Figure 5, the model trained by weight decay without projection step is suffered by the problem
of the diverged values. It confirms the study shown in Section 3.1. It also implies that we cannot
11
Under review as a conference paper at ICLR 2017
5
7
4
e
ω
S
6 3
o
ω
,⊂
ɔ
φ
Z
ω
>
Φ
昼-1
Φ
>
< -2 L
0
■Q Weight Decay
迷 Projection
-6
φ
ra 5
o 5
S
6 4
O
_ C Weight Decay
课 Projection
c，Q
/0
O
0
e
0
0',
断.黎加***米-米
20
40
60
80
100
φ -1 -
6
2-2-
Φ
>
< -3 L
0
£
Q
0，
j* _米-*_米_并咪_ * _ ⅛- -米
Gibbs Sampling Iterations
(a)	SVHN
GibbS Sampling Iterations
(b)	CIFAR10
4	5
×104
0
2
1
0
O
0
3
2
1
0

1
2
3
Figure 5: Divergence results on two datasets.
train leaky RBM with larger CD steps when we do not do projection; otherwise, we would have the
diverged gradients. Therefore, the projection is necessary for training leaky RBM for the generative
purpose. However, we also oberseve that the projection step is not necessary for the classification
and reconstruction tasks. he reason may be the independency of different evaluation criteria (Hinton,
2012; Theis et al., 2016) or other implicit reasons to be studied.
E Equivalence between Annealing the Energy and Leakines s
We analyze the performance gap between AIS-Leaky and AIS-Energy. One major difference is the
initial distribution. The intermediate marginal distribution of AIS-Energy has the following form:
Pk (V) (X exp ( -1 v>(I - (1 - βk) X Wj Wjr - (1 - βk)c X Wj Wjr ) v ) .	(16)
ηj>0	ηj≤0
Here we eliminated the bias terms b for simplicity. Compared with Algorithm 2, (16) not
only anneals the leakiness (1 - βk)cPη ≤0 Wj Wjr when ηj ≤ 0, but also in the case (1 -
βk ) Pη >0 Wj Wjr when ηj > 0, which brings more bias to the estimation. In other words,
AIS-Leaky is a one-sided leakiness annealing while AIS-Energy is a two-sided leakiness annealing
method.
To address the higher bias problem of AIS-Energy, we replace the initial distribution with the one
used in Algorithm 2. By elementary calculation, the marginal distribution becomes
Pk(V) X exp [ -1 v> ( I - X WjWjr - (βk + (1- βk)c) X WjWjr ) v ) ,	(17)
ηj>0	ηj≤0
which recovers the proposed Algorithm 2. From this analysis, we understand AIS-Leaky is a special
case of conventional AIS-Energy with better initialization inspired by the study in Section 3. Also,
by this connection between AIS-Energy and AIS-Leaky, we note that AIS-Leaky can be combined
with other extensions of AIS (Grosse et al., 2013; Burda et al., 2015) as well.
F	More Experimental Results for Sampling
F.1 Sampled Images
We show the sampled images from leaky RBM train on CIFAR10 and SVHN datasets. We randomly
initialize 20 chains and run Gibbs sampling for 1000 iterations. The sampled results are shown in
Figure 6 The results shows that single layer RBM does not adequately model CIFAR10 and SVHN
12
Under review as a conference paper at ICLR 2017
Figure 6: Sampled images from leaky RBM.
Figure 7: Sampled images in gray-scale from Bernoulli-Gaussian RBM trained on CIFAR10 (Ran-
zato & Hinton, 2010).
when compared to multilayer models. The similar results for single layer Bernoulli-Gaussian RBM
from Ranzato & Hinton (2010) (in gray scale) is shown in Figure 7. Therefore, we instead focused
on quantitative evaluation of the log-likelihood in Table 3.
F.2 Computational Time between Different Sampling Strategies
The comparison in terms of CPU time of different sampling algorithms discussed in Section 5 is
shown in Figure 8. Please note that the complexity of CD and Mix are the almost the same. Mix
only need a few more constant time steps which can be ignored compared with sampling steps.
Leaky is more time-consuming because of computing and decomposing the covariance matrix as we
discussed in Section 5. We also report the execution time of each step of algorithms in Table 4.
F.3 Study on ReLU-Bernoulli RBM
We study the idea of annealing leakiness on the RBM model with leaky ReLU hidden units and
Bernoulli visible units. We create the toy dataset with 20, 25 and 30 visible units as shown in
Figure 9. The small datasets allow exact computation of the partition function. For each dataset, we
sample 60,000 images for training and 10,000 images for testing. We use 100 hidden units and PCD
to train the model. The log likelihood results are shown in Table 5.
Compared to the Gaussian visible units case we study in Section 3, where p(v) is a multi-variate
Gaussian distribution when c = 1, the partition function of p(v) in ReLU-Bernoulli when c = 1
does not have the analytical form. Therefore, we do the following two-stage alternative. We first
run the standard AIS algorithm, which anneals the energy, to the distribution with leakiness c = 1.
We then change to anneals the leakiness from 1 to the target value. For the typical AIS algorithm
(AIS-Energy), we use 104 chains with 2 × 104 intermediate distributions. For the proposed two-
staged algorithm (AIS-Leaky), we use 104 chains with 104 intermediate distributions for annealing
to c = 1 and the other 104 distributions for annealing the leakiness. The results are shown in Table 6.
In Table 6, the standard AIS algorithm (AIS-Energy) has unsatisfactory performance. We show
the performance of AIS for estimating the partition function of models with different leakiness on
Toy20. We use the 104 independent chains and 2 × 104 intermediate distributions. The results are
shown in Table 7. From Table 7, we observe that the AIS performances worse when the leakiness
is closer to 0. Although we observed that increasing chains and intermediate distributions could
improve the performance, but the improvements are limited. The study demonstrates when the
	SVHN	CIFAR10
Inner Loop of CD	96.16	96.15
Inner Loop of annealing leaky	96.17	96.16
Projection	438.56	439.47
Table 4: The execution time (s) of each step of algorithms (1000 iterations).
13
Under review as a conference paper at ICLR 2017

-1520
-1540
-1560
P
O
o -1580
^Φ
*
口-1600
σ)
o
"j -1620
-1640
-1660
Running Time (S)
(a) SVHN
Running Time (s)
(b) CIFAR10
Figure 8: Training leaky RBM with different sampling algorithms.
Figure 9: Toy Datasets with different number of visible units.
	I = 20	I = 25	I = 30
Log likelihood	-5.660821	-6.846937	-8.448907
Log partition function	21.626300 -	22.363024 -	27.937846 -
Table 5: The log lokelihood and true partition function for ReLU-Bernoulli RBM with different
number of visible units.
	I = 20	I = 25	I = 30
AIS-Energy	46.7 ± 1.26	60.9 ± 1.29	48.2 ± 1.18
AIS-Leaky	0.04 ± 0.0θ2-	0.04 ± 0.003~	0.10 ± 0.05-
Table 6: The difference between the true partition function and the estimations of two algorithms
with standard deviation.
14
Under review as a conference paper at ICLR 2017
non-linearity of the distribution increases (the leakiness value c decreases), the standard AIS cannot
effectively estimate the partition function within feasible computational time. On the other hand, it
also confirm the proposed idea, annealing the leakiness, can serve as an effective building block for
algorithms without enhancing the algorithm complexity. Note that the unsatisfactory performance
of AIS may be addressed by Grosse et al. (2013). From Appendix E, the two-stage algorithm used
here can also be improved by applying Grosse et al. (2013).
	c = 1	C = 0.9	c = 0.5	c = 0.1	c = 0.01
AIS-Energy	0.001 ± 0.0OT~	0.32 ± 0.00Γ~	3.69 ± 0.015~	19.18 ± 0.26-	46.7 ± 1.26 一
Table 7: The difference (with standard deviation) between the true partition function and the esti-
mations of AIS-Energy under different leakiness.
F.3.1 MNIST and Caltech datasets
We study MNIST and Caltech 101 Silhouettes datasets with 500 hidden units and train the model
with CD-25. The results are shown in Table 8 and Table 9. The leaky RBM is better than con-
ventional Bernoulli RBM and some deep models on MNIST data. Although leaky RBM deos not
outperform Su et al. (2017), but it enjoys the advantage of the simpler sampling procedure (Gaussian
distribution vs truncated Gaussian distribution) in the binary visible unit case.
Model	Dim	Test lld
RBM (Salakhutdinov & Murray, 2008)	500	-86.3
SBN (Bornschein & Bengio, 2015)	10-100-200-300-400	-85.4
DBN (Salakhutdinov & Murray, 2008)	2000-500	-86.2
Truncated Gaussian (Su et al., 2017)	500	-83.2
	Leaky RBM			500		-84.5
Table 8: The testing log-likelihood result on MNIST.
Model	Dim	Test lld
RBM (Cho etal., 2013)	500	-114.7
RBM (Cho etal., 2013)	4000	-107.7
SBN (Bornschein & Bengio, 2015)	10-100-200-300	-113.3
Truncated Gaussian (Su et al., 2017)	500	-105.1
	Leaky RBM		500	-107.6
Table 9: The testing log-likelihood result on Caltech 101 Silhouettes.
15