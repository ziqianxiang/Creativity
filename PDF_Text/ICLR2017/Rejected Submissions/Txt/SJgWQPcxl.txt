Under review as a conference paper at ICLR 2017
Multi-view	Generative	Adversarial	Net-
WORKS
Mickael Chen
Sorbonne UniversitCs, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France
mickael.chen@lip6.fr
Ludovic Denoyer
Sorbonne UniversitCs, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France
ludovic.denoyer@lip6.fr
Ab stract
Learning over multi-view data is a challenging problem with strong practical
applications. Most related studies focus on the classification point of view and
assume that all the views are available at any time. We consider an extension of this
framework in two directions. First, based on the BiGAN model, the Multi-view
BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs.
Second, it can deal with missing views and is able to update its prediction when
additional views are provided. We illustrate these properties on a set of experiments
over different datasets.
1 Introduction
Many concrete applications involve multiple sources of information generating different views on the
same object (Cesa-Bianchi et al., 2010). If we consider human activities for example, GPS values
from a mobile phone, navigation traces over the Internet, or even photos published on social networks
are different views on a particular user. In multimedia applications, views can correspond to different
modalities (Atrey et al., 2010) such as sounds, images, videos, sequences of previous frames, etc...
The problem of multi-view machine learning has been extensively studied during the last decade,
mainly from the classification point of view. In that case, one wants to predict an output y based
on multiple views acquired on an unknown object x. Different strategies have been explored but a
general common idea is based on the (early or late) fusion of the different views at a particular level
of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).
The existing literature mainly explores problems where outputs are chosen in a discrete set (e.g
categorization), and where all the views are available. An extension of this problem is to consider the
density estimation problem where one wants to estimate the conditional probabilities of the outputs
given the available views. As noted by Mathieu et al. (2015), minimizing classical prediction losses
(e.g Mean square error) will not capture the different output distribution modalities.
In this article, we propose a new model able to estimate a distribution over the possible outputs given
any subset of views on a particular input. This model is based on the (Bidirectional) Generative
Adversarial Networks (BiGAN) formalism. More precisely, we bring two main contributions: first,
We propose the CV-BiGAN (Conditional Views BiGAN - Section 3) architecture that allows one to
model a conditional distribution P (y|.) in an original way. Second, on top of this architecture, we
build the Multi-view BiGANs (MV-BiGAN - Section 4) which is able to both predict when only
one or few views are available, and to update its prediction if new views are added. We evaluate
this model on different multi-views problems and different datasets (Section 5). The related work is
provided in Section 6 and we propose some future research directions in Section 7.
1
Under review as a conference paper at ICLR 2017
2 Background and General Idea
2.1	Notations and Task
Let us denote X the space of objects on which different views will be acquired. Each possible input
x ∈ X is associated to a target prediction y ∈ Rn. A classical machine learning problem is to
estimate P (y|x) based on the training set. But we consider instead a multi-view problem in which
different views on x are available, x being unknown. Let us denote V the number of possible views
and Xk the k-th view over x. The description space for view k is Rnk where n is the number of
features in view k. Moreover, we consider that some of the V views can be missing. The subset
of available views for input xi will be represented by an index vector si ∈ S = {0, 1}V so that
sik = 1 if view k is available and sik = 0 elsewhere. Note that all the V views will not be available
for each input x, and the prediction model must be able to predict an output given any subset of views
S ∈ {0；1}V.
In this configuration, our objective is to estimate the distributions p(y|v(s, x)) where v(s, x) is the
set of views Xk so that Sk = 1. This distribution P will be estimated based on a training set D of
N training examples. Each example is composed of a subset of views si , v(si , xi) associated to an
output yi, so that D = { y1, S1, v(S1, X1) , ..., yN, SN, v(SN, XN) } where Si is the index vector
of views available for Xi . Note that Xi is not directly known in the training set but only observed
through its associated views.
2.2	Bidirectional Generative Adversarial Nets (BiGAN)
We quickly remind the principle of BiGANs since our model is an extension of this technique.
Generative Adversarial Networks (GAN) have been introduced by Goodfellow et al. (2014) and
have demonstrated their ability to model complex distributions. They have been used to produce
compelling natural images from a simple latent distribution (Radford et al., 2015; Denton et al.,
2015). Exploring the latent space has uncovered interesting, meaningful patterns in the resulting
outputs. However, GANs lack the ability to retrieve a latent representation given an output, missing
out an opportunity to exploit the learned manifold. Bidirectional Generative Adversarial Networks
(BiGANs) have been proposed by Donahue et al. (2016) and Dumoulin et al. (2016), independently,
to fill that gap. BiGANs simultaneously learn both an encoder function E that models the encoding
process PE(z|y) from the space Rn to a latent space RZ, and a generator function G that models the
mapping distribution PG(y|z) of any latent point z ∈ RZ to a possible object y ∈ Rn. From both the
encoder distribution and the generator distribution, we can model two joint distributions, respectively
denoted PE(y, z) and PG(y, z):
PG(y, z) = P (z)PG(y|z)
PE(y, z) = P (y)PE (z|y)
(1)
assuming that P (z) = N (0, 1) and P(y) can be estimated over the training set by a uniform
sampling. The BiGAN framework also introduces a discriminator network D1 whose task is to
determine whether a pair (y, z) is sampled from pG (y, z) or frompE(y, z), while E and G are trained
to fool D1 , resulting in the following learning problem:
minmax Ey〜P(y),z〜PE(z∖y) [logDι(y,z)] + Ez〜P(z),y〜PG(y∖z) [1 - logDι(y,z)]	(2)
G,E D1
It can be shown, by following the same steps as in Goodfellow et al. (2014), that the optimization
problem described in Equation 2 minimizes the Jensen-Shanon divergence between PE (y, z) and
PG(y, z), allowing the model to learn both a decoder and a generator over a training set that will
model the joint distribution of (y, z) pairs. As proposed by Dumoulin et al. (2016), we consider in
the following that PG(y|z) is modeled by a deterministic non-linear model G so that G(z) = y, and
PE as a diagonal Gaussian distribution E(Z) = (μ(y), σ(y)). G, μ and σ are estimated by using
gradient-based descent techniques.
2.3	General Idea
We propose a model based on the Generative Adversarial Networks paradigm adapted to the multi-
view prediction problem. Our model is based on two different principles:
2
Under review as a conference paper at ICLR 2017
Figure 1: The CV-BiGAN Architecture. The two top levels correspond to the BiGAN model, while the
third level is added to model the distribution over the latent space given the input of the CV-BiGAN.
The discriminator D? is used to constraint P(z|y) and P(z|X) to be as close as possible.
Conditional Views BiGANs (CV-BiGAN): First, since one wants to model an output distribution
based on observations, our first contribution is to propose an adaptation of BiGANs to model
conditional probabilities, resulting in a model able to learn P(y|X) where X can be either a single
view or an aggregation of multiple views. If conditional GANs have already been proposed in the
literature (see Section 6) they are not adapted to our problem which require explicit mappings between
input space to latent space, and from latent space to output space.
Multi-View BiGANs (MV-BiGAN): On top of the CV-BiGAN model, we build a multi-view model
able to estimate the distribution of possible outputs based on any subset of views v(s, x). If a natural
way to extend the Conditional BiGANS for handling multi-view is to define a mapping function
which map the set of views to a representation space (see Section 4.1) the resulting model has shown
undesirable behaviors (see Section 5.1). Therefore, we propose to constrain the model based on
the idea that adding one more view to any subset of views must decrease the uncertainty on the
output distribution i.e the more views are provided, the less variance the output distribution has. This
behavior is encouraged by using a Kullback-Leibler divergence (KL) regularization (see Section 4.2).
3	The Conditional BiGAN Model (CV-BiGAN)
Our first objective is to extend the BiGAN formalism to handle an input space (e.g a single observed
view) in addition to the output space Rn. We will denote X the observation and y the output
to predict. In other words, We wish to capture the conditional probability P(y|X) from a given
training dataset. Assuming one possesses a bidirectional mapping between the input space and an
associated representation space, ie. PE(z|y) and Pg(y⑶,one can equivalently capture P(z|X).
The CV-BiGAN model keeps the encoder E and generator G defined previously but also includes
an additional encoder function denoted H which goal is to map a value X to the latent space RZ.
Applying H on any value of X results in a distribution PH(z|X) = N(μH(X),σH(X)) so that a value
of z can be sampled from this distribution. This would then allow one to recover a distribution
P(y∣X).
Given a pair (X, y), we wish a latent representation Z sampled from PH(z|X) to be similar to one
from PE(z|y). As our goal here is to learn P(z∣X),we define twojoint distributions between X and
z:
PH (X,z)= PH (z∣X)P(X)
PE (X,z) = X PE (z∣y)P (X,y)	(3)
y
3
Under review as a conference paper at ICLR 2017
/
KL constraint
∖.
Figure 2: The MV-BiGAN additional components. In this example, we consider a case where only
X1 is available (top level) and a second case where both X1 and X3 are available. The distribution
P(z∣X1,X3) is encouraged to be "included"in P(z|X1) by the KL constraint. The aggregation of the
views is made by the φk functions that are learned conjointly with the rest of the model.
Minimizing the Jensen-Shanon divergence between these two distributions is equivalent to solving
the following adversarial problem:
min max EX,y〜p(行,y),z〜PE(z|y) [log D2 (X, Z)]+ E或y〜P(X,y),z〜ph(z∣x) [1 - log D2 (X, Z)]	(4)
E,H D2
Note that when applying stochastic gradient-based descent techniques over this objective function,
the probability P(X, y) is approximated by sampling uniformly from the training set. We can sample
from PH(X, Z) and PE (X, Z) by forwarding the pair (X, y) into the corresponding network.
By merging the two objective functions defined in Equation 2 and 4, the final learning problem for
our Conditionnal BiGANs is defined as:
min maχ EX,y 〜p(X,y),z 〜PE (z|y) [log D1(y,z)] + Ez 〜P (z),y 〜PG(y∣z) [1 - log DI"z)]
G,E,H D1,D2	(5)
+EX,y 〜P (X,y),z 〜PE (z|y) [log D2 (X, Z)] + EX,y 〜P (X,y),z 〜PH (z|X) [1 - log D2 (X, Z)]
The general idea of CV-BiGAN is illustrated in Figure 1.
4	Multi-View BiGAN
4.1	Aggregating Multi-views for CV-BiGAN
We now consider the problem of computing an output distribution conditioned by multiple different
views. In that case, we can use the CV-BiGAN Model (or other conditional approaches) conjointly
with a model able to aggregate the different views where A is the size of the aggregation space. Instead
of considering the input X, we define an aggregation model Ψ. Ψ(v(s, x)) will be the representation
of the aggregation of all the available views Xk 1:
V
Ψ(v(s,x)) = X Skφk(Xk)	(6)
k=1
where φk is a function that will be learned that maps a particular view in Rnk to the aggregation
space RA. By replacing X in Equation 5, one can then simultaneously learn the functions φk and the
distributions PH, PE and PD, resulting in a multi-view model able to deal with any subset of views.
4.2	Uncertainty reduction assumption
However, the previous idea suffers from a very high instability when learning, as it is usually noted
with complex GANs architectures (see Section 5). In order to stabilize our model, we propose to add
a regularization based on the idea that adding new views to an existing subset of views should reduce
the uncertainty over the outputs. Indeed, under the assumption that views are consistent one another,
adding a new view should allow to refine the predictions and reduce the variance of the distribution
of the outputs.
1 Note that other aggregation scheme can be used like recurrent neural networks for example.
4
Under review as a conference paper at ICLR 2017
Let us consider an object x and two index vectors s and s0 such that v(x, s) ⊂ v(x, s0) ie.
∀k, s0k ≥ sk. Then, intuitively, P (x|v(x, s0)) should be ”included” in P (x|v(x, s)). In the CV-
GAN model, since P (y|z) is deterministic, this can be enforced at a latent level by minimizing
KL(P (z|v(x, s0) || P(z|v(x, s)). By assuming those two distributions are diagonal gaussian distribu-
tions (ie. P(z∣v(x, s0) = N(μι, ∑ι) and P(z∣v(x, S) = N(μ2, ∑2) where ∑k are diagonal matrices
with diagonal elements σk(i)), the KL divergence can be computed as in Equation 7 and differentiated.
KL(P(Z|v(x, s0))∣∣P(z∣v(x, S))) = g XX (-1 - log (⅛i)) + Φ) + MIii- 42⑴)
2 i=1	σ2(i)	σ2(i)	σ2(i)
(7)
Note that this divergence is written on the estimation made by the function H and will act as a
regularization over the latent conditional distribution.
The final objective function of the MV-BiGAN can be written as:
^min max Es,x,y~P (s,χ,y),z 〜PE (z|y) [log DI (y,Z)] + Ez~P (z),y~PG(y|z) [1 - log DI (y,Z)]
G,E,H D1,D2
+Es,x,y 〜P (s,x,y),z 〜PE (z|y) [log D2 (v(x, s), z)] + Es,x,y〜P (s,x,y),z「、jPH (z| v(x,s)) [1 - log D2 (v(x, s), z)]
+ λEχ 〜P(X)	X	KL(H (V(X, S0))||H (V(X, s)))
s,s0∈Sx
∀k,s0k ≥sk
(8)
where λ controls the strength of the regularization. Note that aggregation models Ψ are included into
H and D2 and can be optimized conjointly in this objective function.
4.3	Learning the MV-BiGAN
The different functions E, G, H, D1 and D2 are implemented as parametric neural networks and
trained by mini-batch stochastic gradient descent (see Appendix for more details concerning the
architectures).We first update the discriminators networks D1 and D2, then we update the generator
and encoders G, E and H with gradient steps in the opposite direction.
As with most other implementation of GAN-based models, we find that using an alternative objective
proposed by Goodfellow et al. (2014) for E, G and H instead leads to more stable training. The new
objective consist of swapping the labels for the discriminators instead of reversing the gradient. We
also find that we can update all the modules in one pass instead of taking alternate gradient steps
while obtaining similar results.
Note that the MV-BiGAN model is trained based on datasets where all the V views are available
for each data point. In order to generate examples where only subsets of views are available, the
ideal procedure would be to consider all the possible subsets of views. Due to the number of data
points that would be generated by such a procedure, we build random sequences of incremental sets
of views and enforce the KL regularization over successive sets.
5 Experiments
We evaluate our model on three different types of experiments, and on two differents datasets. The
first dataset we experiment on is the MNIST dataset of handwritten digits. The second dataset is the
CelebA (Liu et al., 2015) dataset composed of both images of faces and corresponding attributes. The
MNIST dataset is used to illustrate the ability of the MV-BiGAN to handle different subset of views,
and to update its prediction when integrating new incoming views. The CelebA dataset is used to
demonstrate the ability of MV-BiGAN to deal with different types (heterogeneous) of views.
5.1 MNIST, 4 VIEWS
We consider the problem where 4 different views can be available, each view corresponding to a
particular quarter of the final image to predict - each view is a vector of R(14×14). The MV-BiGAN
5
Under review as a conference paper at ICLR 2017
3 √ 今 夕 OGq q $，a r £ £
OOO
0 5 0
O 0OooCoS
00^0^000
0ðG OODqOOoq
。”。〃£含0 ðθθ
CD^obOOo&O&
ajsryy
? 7/ O
夕Egg
^ΓΓg
Γ Λ /
7 9ʃg
，制 2 00
7 夕 OOg
夕£鲁2
7 X夕q
qFoo¾
Oo Z ? Oo
2/?Sp
S 夕 W?
Qf 2 -Y ?
尸夕Voc
0©夕S平
“ 7g 9
Γg 9 q
Figure 3: Results of the MV-BiGAN on sequences of 4 different views. The first column corresponds
to the provided views, while the other columns correspond to outputs sampled by the MV-BiGAN.
is used here to recover the original image. The model is trained on the MNIST training digits, and
results are provided on the MNIST testing dataset.
Figure 3 illustrates the results obtained for some digits. In this figure, the first column displays
the input (the subset of views), while the other columns shows predicted outputs sampled by the
MV-BiGAN. An additional view is added between each row. This experiment shows that when new
views are added, the diversity in the predicted outputs decreases due to the KL-contraint introduced in
the model, which is the desired behavior i.e more information implied less variance. When removing
the KL constraint (Figure 4), the diversity still remains important, even if many views are provided to
the model. This show the importance of the KL regularization term in the MV-BiGAN objective.
Figure 4: MV-BiGAN without KL-constraint (i.e λ = 0).
5.2	MNIST, sequence of incoming views
We made another set of experiments where the views correspond to images with missing values
(missing values are replaced by 0.5). This can be viewed as a data imputation problem - Figure 5.
Here also, the behavior of the MV-BiGAN exhibits interesting properties: the model is able to predict
the desired output as long as enough information has been provided. When only non-informative
views are provided, the model produces digits with a high diversity, the diversity decreasing when
new information is added.
5.3	CelebA, integrating heterogeneous information
At last, the third experiment aims at measuring the ability of MV-BiGAN to handle heterogeneous
inputs. We consider two views: (i) the attribute vector containing information about the person in
the picture (hair color, sex, ...), and (ii) a incomplete face. Figure 9 illustrates the results obtained
on two faces. The first line corresponds to the faces generated based on the attribute vector. One
can see that the attribute information has been captured by the model: for example, the sex of the
6
Under review as a conference paper at ICLR 2017
□©BQS同©ΞΞΞ
QQQQS HΞΞΞΞ
ħξ□qqbξξξξ
QQQ"ΞHΞΞΞ固
□ΞΞ“Dhqξξξ
QQQQQHΞSΞΞ
ΞB□QQH目©©图
□Q□QQHQΞΞΞ
QQSQBBEIΞΞΞ
・闻闻nπ∙ ∙⅞rsH
∣/M∙∕∣ ∕∖4∖'
√∙
Figure 5: MV-BiGAN with sequences of incoming views. Here, each view is a 28 × 28 matrix (values
are between 0 and 1 with missing values replaced by 0.5).
the images generated based on the attribute vector, the second line corresponds to images generated
based on the incomplete face, the third line corresponds to the images generated based on the two
views. The groundthruth face is given in the bottom-left corner, while the incomplete face is given in
the top-left corner.
夕
夕


N
√
y

/

√

√

夕

generated face is constant (only women) showing that MV-BiGan has captured this information from
the attribute vector. The second line corresponds to the faces generated when using the incomplete
face as an input. One can also see that the generated outputs are ”compatible” with the incomplete
information provided to the model. But the attribute are not considered (for example, women and
men are generated). At last, the third line corresponds to images generated based on the two partial
views (attributes and incomplete face) which are close to the ground-truth image (bottom left). Note
that, in this set of experiments, the convergence of the MV-BiGAN was quite difficult to obtain, and
the quality of the generated faces is still not satisfying.
6	Related work
Multi-view and Representation Learning: Many application fields naturally deal with multi-view
data with true advantages. For example, in the multimedia domain, dealing with a bunch of views is
usual (Atrey et al., 2010): text, audio, images (different framings from videos) are starting points of
these views. Besides, multimedia learning tasks from multi-views led to a large amount of fusion-
based ad-hoc approaches and experimental results. The success of multi-view supervised learning
approaches in the multimedia community seems to rely on the ability of the systems to deal with the
complementary of the information carried by each modality. Comparable studies are of importance
7
Under review as a conference paper at ICLR 2017
in many domains, such as bioinformatics (Sokolov & Ben-Hur, 2011), speech recognition (Arora
& Livescu, 2012; Kogo et al., 2012), signal-based multimodal integration (WU et al., 1999), gesture
recognition (Wu et al., 2013), etc.
Moreover, multi-view learning has been theoretically studied mainly under the semi-supervised
setting, but only with two facing views (Chapelle et al., 2006; Sun, 2013; Sun & Taylor, 2014; Johnson
& Zhang, 2015). In parallel, ensemble-based learning approaches have been theoretically studied, in
the supervised setting: many interesting results should concern multi-view learning, as long as the
ensemble is built upon many views (Rokach, 2010; Zhang & Zhang, 2011). From the representation
learning point of view, recent models are based on the incorporation of some ”fusion” layers in the
deep neural network architecture as in (Ngiam et al., 2011) or (Srivastava & Salakhutdinov, 2012) for
example. Some other interesting models include the multiview perceptron(Zhu et al., 2014).
Estimating Complex Distributions: While deep learning has shown great results in many classifi-
cation task for a decade, training deep generative models still remains a challenge. Deep Boltzmann
Machines (Salakhutdinov & Hinton, 2009) are un-directed graphical models organized in a succes-
sion of layers of hidden variables. In a multi-view setting, they are able to deal with missing views
and have been used to capture the joint distribution in bi-modal text and image data (Srivastava &
Salakhutdinov, 2012; Sohn et al., 2014). Another trend started with denoising autoencoder (Vincent
et al., 2008), which aims to reconstruct a data from a noisy input have been proved to possess some
desirable properties for data generation (Bengio et al., 2013). The model have been generalized under
the name Generative Stochastic Networks by replacing the noise function C with a mapping to a
latent space (Thibodeau-Laufer et al., 2014). Pulling away from the mixing problems encountered
in previous approaches, Variational Autoencoders (Kingma & Welling, 2013) attempts to map the
input distribution to a latent distribution which is easy to sample from. The model is trained by
optimizing a variational bound on the likelihood, using stochastic gradient descent methods. The
Kullback-Leibler regularizer on the latent Gaussian representations used in our model is reminiscent
of the one introduced in the variational lower bound used by the VAE.
The BiGAN model (Donahue et al., 2016; Dumoulin et al., 2016) that serves as a basis for our work
is an extension of the Generative Adversarial Nets (Goodfellow et al., 2014). A GAN extension
that captures conditional probabilities (CGAN) has been proposed in (Mirza & Osindero, 2014).
However, as noted by (Mathieu et al., 2015) and (Pathak et al., 2016), they display very unstable
behavior. More specifically, CGAN have been able to generate image of faces conditioned on an
attribute vector (Gauthier, 2014), but fail to model image distribution conditioned on a part of the
image or on previous frames. In both CGAN and CVBiGAN, the generation process uses random
noise to be able to generate a diversity of outputs from the same input. However, in a CGAN, the
generator concatenate an independent random vector to the input while CV-BiGAN learns a stochastic
latent representation of the input. Also, some of the difficulties of CGAN in handling images as both
inputs X and outputs y stem from the fact that CGAN's discriminator directly compares X and y. In
cV-BiGAN, neither discriminators has access to both X and y but only to a latent representation Z
and either X or y.
7	Conclusion and Perspectives
We have proposed the cV-BiGAN model for estimating conditional densities, and its extension
MV-BiGAN to handle multi-view inputs. The MV-BiGAN model is able to both handle subsets of
views, but also to update its prediction when new views are added. It is based on the idea that the
uncertainty of the prediction must decrease when additional information is provided, this idea being
handled through a KL constraint in the latent space. This work opens different research directions:
the first one concerns the architecture of the model itself since the convergence of MV-BiGAN is still
difficult to obtain and has a particularly high training cost. Another direction would be to see if this
family of model could be used on data streams for anytime prediction.
Acknowledgments
This work was supported by the French project LIVES ANR-15-cE23-0026-03.
8
Under review as a conference paper at ICLR 2017
References
R. Arora and K. Livescu. Kernel cca for multi-view acoustic feature learning using articulatory
measurements. In MLSP, 2012.
Pradeep K. Atrey, M. Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S. Kankanhalli. Mul-
timodal fusion for multimedia analysis: A survey. Multimedia Syst., 16(6):345-379, November
2010. ISSN 0942-4962.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders
as generative models. In Advances in Neural Information Processing Systems, pp. 899-907, 2013.
Nicold Cesa-Bianchi, David R. Hardoon, and Gayle Leen. Guest editorial: Learning from multiple
sources. Machine Learning, 79(1):1-3, 2010.
O. Chapelle, B. Scholkopf, and A. Zien. Semi-SuperViSed Learning. MIT Press, Cambridge, MA,
2006.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a
laplacian pyramid of adversarial networks. In Advances in neural information processing systems,
pp. 1486-1494, 2015.
JeffDonahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro,
and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Jon Gauthier. Conditional generative adversarial nets for convolutional face generation. Class Project
for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester,
2014, 2014.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014.
R. Johnson and T. Zhang. Semi-supervised learning with multi-view embedding: theory and
application with convolutional neural networks. CoRR, abs/1504.012555v1, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the 2nd
International Conference on Learning Representations (ICLR), number 2014, 2013.
Sokol Kogo, CeCiIe Capponi, and FrederiC BeChet. Applying multiview learning algorithms to
human-human conversation classification. In INTERSPEECH, 2012.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), 2015.
Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond
mean square error. arXiv preprint arXiv:1511.05440, 2015.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multi-
modal deep learning. In Proceedings of the 28th International Conference on Machine Learning
(ICML), 2011.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context
encoders: Feature learning by inpainting. arXiv preprint arXiv:1604.07379, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
L. Rokach. Ensemble-based classifiers. Artif. IntelL Rev., 33(1-2):1-39, 2010.
9
Under review as a conference paper at ICLR 2017
Ruslan Salakhutdinov and Geoffrey E Hinton. Deep boltzmann machines. In AISTATS, volume 1, pp.
3, 2009.
Kihyuk Sohn, Wenling Shang, and Honglak Lee. Improved multimodal deep learning with variation
of information. In Advances in Neural Information Processing Systems, pp. 2141-2149, 2014.
Artem Sokolov and Asa Ben-Hur. Multi-view prediction of protein function. In ACM-BCB, pp.
135-142, 2011.
Nitish Srivastava and Ruslan R Salakhutdinov. Multimodal learning with deep boltzmann machines.
In Advances in neural information processing systems, pp. 2222-2230, 2012.
Shiliang Sun. A survey of multi-view machine learning. Neural Comput. Appl., 23(7-8):2031-2038,
2013.
Shiliang Sun and John-Shawe Taylor. PAC-Bayes analysis of multi-view learning.	CoRR,
abs/1406.5614, 2014.
Eric Thibodeau-Laufer, Guillaume Alain, and Jason Yosinski. Deep generative stochastic networks
trainable by backprop. 2014.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103. ACM, 2008.
Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view representation
learning. In Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), pp. 1083-1092, 2015.
Jiaxiang Wu, Jian Cheng, Chaoyang Zhao, and Hanqing Lu. Fusing multi-modal features for gesture
recognition. In ICMI, pp. 453-460, 2013.
L. Wu, S.L. Oviatt, and P.R. Cohen. Multimodal integration: a statistical view. MM, 1(4):334-341,
1999.
Jianchun Zhang and Daoqiang Zhang. A novel ensemble construction method for multi-view data
using random cross-view correlation between within-class examples. Pattern Recogn., 44(6):
1162-1171, 2011.
Zhenyao Zhu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Multi-view perceptron: a deep model
for learning face identity and view representations. In Advances in Neural Information Processing
Systems, pp. 217-225, 2014.
10
Under review as a conference paper at ICLR 2017
Appendix
Model architecture and hyperparameters
Operation	Kernel	Strides	Padding	Feature maps	BN	Nonlinearity
Convolution	4×4	2 X 2	1×1	64	×	Leaky ReLU
Convolution	4×4	2×2	1×1	128	√	Leaky ReLU
Convolution	4×4	2×2	1×1	256	√	Leaky ReLU
Convolution	4×4	2×2	1×1	512	√	Leaky ReLU
Convolution	4×4	1×1		output size	×	Linear
Fractionally-strided convolution	4×4	1 × 1		512	√	ReLU
Fractionally-strided convolution	4×4	2×2	1×1	256	√	ReLU
Fractionally-strided convolution	4×4	2×2	1×1	128	√	ReLU
Fractionally-strided convolution	4×4	2×2	1×1	64	√	ReLU
Fractionally-strided convolution	4×4	2×2	1×1	3	×	Tanh
Table 1: Convolution architectures used in our experiments on the CelebA dataset. The top part is
used for encoding images to the aggregation space. The bottom part is used in G to generate images
from a vector z.
All models are optimized using Adam with hyperparameters α = 2 ∙ 10-5, βι = 0.5, β2 = 10-3 and
a learning rate of 2 ∙ 10-5. The slope of leaky rectified linear units is set to 0.2. Latent representations
(μ, log(σ2)) are of size 128.
MNIST experiments
The generator function G has three fully connected layers. The second and the third are followed
by parameter free batch normalizations. All layers uses a rectified linear unit (ReLU) non linearity
except the output layer that has a sigmoid.
The aggregation model Ψ is the sum of mapping functions φk that map each input views to the
aggregation space. Each φk is a linear transformation of the inputs. The encoding functions E and
H are both neural networks that include an aggregation model and adds two hidden fully connected
layers. They takes a set of views as inputs, and outputs a pairs of vectors (μ, log(σ2)). The first
layer is followed by a ReLU non linearity. The second layer is followed by a parameter free batch
normalization and a ReLU. The output layer has a tanh layer for μ and a negative exponential linear
unit (nELU) for log σ2 to help stabilizing σ at the beginning of the training procedure.
The discriminator D1 has three fully connected layers with parameter free batch normalization at the
third layer, Leaky ReLU non linearity at the first three layers and a sigmoid for the output layer. The
vector z is concatenated to the representation at the second layer.
The discriminator D2 is similar to E and H except it uses Leaky ReLUs instead of ReLUs and a
sigmoid at the output level. z is concatenated directly to the aggregation vector Ψ(v(x, s)).
All hidden layers and the aggregation space are of size 1500. λ is set to 1 ∙ 10-5. Minibatch size is
set to 128. The models have been trained for 300 epochs.
CelebA experiments
The generator function G is a network of fractionally-strided convolution layers (often called decon-
volution) discribed in table 7.
The mapping function φk for image views is a convolutional network (Table 7). For attribute vectors,
it is a simple linear transformation. E and H builds a neural networks with one hidden layer on top
of the aggregation space. The hidden layer has a parameter free batch normalization followed by a
ReLU. The output layer is the same as the encoders used in the MNIST experiments.
11
Under review as a conference paper at ICLR 2017
The discriminator D1 has two fully connected layers with a Leaky ReLU after the first one, and a
sigmoid at the output, build on top of a fractionally-strided convolutions network. z is concatenated
at the output level of the convolutional network.
As in the MNIST experiments, the discriminator D2 is similar to E and H except it uses Leaky ReLUs
instead of ReLUs and a sigmoid at the output level. z is concatenated directly to the aggregation
vector Ψ(v(x, s)).
Aggregation space is of size 1000. λ is set to 1 ∙ 10-3, and minibatch size is 16. The model has been
trained for 15 epochs.
Additional results
12
Under review as a conference paper at ICLR 2017
Qqqq
Qqq=T
q q q q
e∖q q 4
a* 01 A al
q q q q
q cr -ʃ q
q qτTq
『q q q
G<Γ q g
Tqql
3 q 4 rr
qj q<i
q qu∕ =F
q & q q
∖ HQ q
q q ≤r q
qMq q
Cr q 4 4
q q Q q
;出“
3 3 3 3
6 3，？
33m 3
3 3 3 3
333 3
63 33
一?3 3 3
5 0*3
3 3212
3333
3 3 $ 3
3 3 y 3
5 3 33
53 33
s-4 3 M
33 3 3
73 3
5 3 3 3
3 -⅛>* 3
3 3JB
X。。。
ODOO
70 0 O
O 6
O 5
O O
D ð
∖ Q
V。
5 6
6 O
O O
e
G
O

6
O
0
O O
π√ 7 λπ 7
/ Λ; 7 7
弓7 'I
^77
7 7 1
Z7 7
「7
7 ? T 7
7/17
Z 1 7

1 7 T 1
π
7
7
〃?3-<?
4。q夕
q夕99
•7(?夕夕
cl3 q
39夕
5 CΓ- 9
5夕夕
q g q
V ⅛ 4r
彳 CT O/
r q夕
Γ ? ?
S1夕9
TG-9 q
，■:T 夕 q
√q q9
√/ q q
Σru>o∕Q'
Ql
7
Uf
9
O'
G1
9
Tr S 5 ʃ
q。5 5
q g 5 5
6 5 5 5
5 5 5 3
/855
8 8 5 3
SgS 5
g S 5 5
g S 5 5
q S 5 5
5355
q 3 5 3
S S 5 5
5 5 3 5
5 5 5 5
SS 5 5
3 S S 5
5 ʃ* 3 5
S S 5 5
b⅛⅛5
Figure 7: Additional results on the MNIST in four parts task.
13
Under review as a conference paper at ICLR 2017
Figure 8: Additional results on the MNIST in streams task.
14
Under review as a conference paper at ICLR 2017
Figure 9: Additional results on the CelebA dataset.
15