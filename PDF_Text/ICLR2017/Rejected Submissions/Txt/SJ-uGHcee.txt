Under review as a conference paper at ICLR 2017
Efficient iterative policy optimization
Nicolas Le Roux
Criteo Research
nicolas@le-roux.name
Ab stract
We tackle the issue of finding a good policy when the number of policy updates is
limited. This is done by approximating the expected policy reward as a sequence
of concave lower bounds which can be efficiently maximized, drastically reducing
the number of policy updates required to achieve good performance. We also
extend existing methods to negative rewards, enabling the use of control variates.
1 Introduction
Recently, reinforcement learning has seen a surge in popularity, in part because of its successes
in playing Atari games (Mnih et al., 2013) and Go (Silver et al., 2016). Due to its ability to act in
settings where the actions taken influence the environment and, more generally, the input distribution
of examples, reinforcement learning is now used in other domains, such as online advertising.
The goal is to learn a good policy, i.e. a good mapping from states to actions, which will maximize
the final score, in the case of Atari games, the probability of winning, in Go, or the number of sales,
in online advertising. We have at our disposal logs of past events, where we know the states we
were in, the actions we took and the resulting rewards we obtained. In this paper, we shall focus on
how to efficiently use these logs to obtain a good policy. In some cases, in addition to (state, action,
reward) triplets, we have access to a teacher which provides the optimal, or at least a good, action
for a given state. The use of such a teacher to find a good initial policy is outside the scope of this
paper.
There are many ways to learn good policies using past data. The two most popular are Q-learning
and direct policy optimization. In Q-learning (Sutton & Barto, 1998), we are trying to learn a
mapping from a (state, action) pair to the reward. Given this mapping and a state, we can then find
the action which leads to the maximal predicted reward. This method has been very successful,
especially when the action space is small, since we need to test all the actions, and the reward
somewhat predictable, since taking the maximum is unstable and a small error can lead to suboptimal
actions.
Direct policy optimization, rather than trying to estimate the value of a (state, action) pair, directly
parameterizes a policy, i.e. a conditional distribution over actions given the current state. More
precisely, and using the notation from Kober (2014), we wish to maximize the expected return of a
policy p(∙∣θ) with parameters θ, i.e.
J(θ) = T
p(τ∣θ)R(τ) dτ ,
(1.1)
where T is the set of all paths τ and R(τ) is the reward associated with path τ.
A rollout τ = [s1:T+1, a1:T] is a series of states s1:T +1 = [s1, . . . , sT+1] and actions a1:T =
[aι,..., aτ]. p(τ∣θ) is the probability of rollout T when using a policy with parameters θ and R(T)
is the aggregated return of τ. If we make the Markov assumption that a state only depends on the
T
previous state and the action chosen, we have P(T∣θ) = p(sι) ]^[p(st+ι∣st, at)∏(at∣st,t, θ) where
t=1
p(st+1|st, at) is the next state distribution and is independent of our policy. The action space can be
discrete or continuous.
1
Under review as a conference paper at ICLR 2017
Without the ability to exactly compute Ja, we must resort to sampling to get an estimate of both J
and its gradient. These samples can Come fromp(∙∣θ) or from another distribution. In the latter case,
we need to use importance sampling to keep our estimator unbiasedb. Whether they use importance
sampling or not, all methods which directly optimize the policy rely on iterative procedures. First,
rollouts are performed under a policy to gather samples. Then, these samples are used to update the
policy, which is in turn used to gather new samples, until convergence of the policy. These methods
continuously gather new samples, mostly because the updates to the policy are more reliable when
they are based on fresh samples. However, in a production environment, as we will see in Sec. 2, we
will typically release a new policy to many users at once, gathering millions or billions of samples,
but the delay between two updates of a policy is of the order of hours or even days. Thus, there is
a strong need for policy learning techniques which can achieve a good performance with a limited
number of policy updates. There is an analogous issue in robotics where each new rollout induces
wear and tear on the robot, making such a method which limits the number of rollouts desirable.
In this paper, we shall thus present a method dedicated to achieving high performance while limiting
the number of different policies under which samples are gathered. Sec. 3 reviews the relevant liter-
ature. Sec. 4 presents a first version of our algorithm, proving its theoretical efficiency and providing
a common framework to several existing techniques. Then, Sec. 5 shows how the positiveness as-
sumption for the rewards can slow down learning and proposes an improvement which circumvents
the issue. Sec. 6 shows results of the proposed algorithm on both a synthetic dataset and a real-life,
large scale dataset in online advertising. Finally, Sec. 7.3 reflects on the current state and the future
venues of research.
2	Display advertising
Retargeting is a type of advertising where ads are displayed to users who have already expressed
interest in one or multiple products, generally by browsing on merchants’ websites. Since the infor-
mation used to know which ad to display are not related to a query, like in search advertising, these
ads can be displayed on any website, for instance news sites or personal pages. More precisely,
every time a user lands on such a website, the website contacts an ad-exchange platform which runs
a real-time auction. The highest bidder gets the right to display an ad for this particular user and
pays a price depending on many factors, including the opponents’ bids. If the user then clicks on
the ad, the retargeter is paid by the merchant whose ad was displayed. To maximize its revenue,
the retargeter must thus only display ads leading to a click, and do so at the lowest possible price.
Historically, these auctions were second-price, which means that the price paid by the highest bidder
was the second highest bid. From a bidder perspective, the optimal strategy was straightforward as
the optimal bid was the expected gain of displaying an ad. However, ad-exchanges have recently
moved to other types of auctions, where the optimal strategy depends on the (unknown) bids of the
other bidders. Worse, the exact type of auction is unknown to the bidders who only know the price
they pay when they win the auction.
The bidding problem thus fits nicely in the reinforcement learning framework where the state is the
set of information known about the user and the current website, the action is the bid and the reward
is the payment (if there is a click) minus the cost of displaying the ad. As the reward depends mostly
on whether there is a click or not, an event which is highly unpredictable, techniques such as doubly
robust estimation (Dudik et al., 2011) or based on carefully crafted Q-functions (LillicraP et al.,
2015; Schulman et al., 2015b; Gu et al., 2016) are unlikely to yield significant improvements.
There is another major difference with other reinforcement learning works. For quality control, a
new policy can only be put in production every few hours or even days. Since large advertising
companies display several billion ads each day, each new policy is used to gather about a billion
samples. We are thus in a very specific setting where the number of samples is very large but the
number of policies with which samples are gathered is limited.
We will now review the existing literature and show how no existing work addresses the constraints
we face. We will then present our solution which is both simple and leads to good policies. To
aComputing J would require visiting every possible τ at least once, which is impossible, even for moder-
ately long rollouts.
bUsing a biased estimator can be useful but this is outside the scope of this paper.
2
Under review as a conference paper at ICLR 2017
show its efficiency, we report results on both the Cartpole problem, a synthetic problem common in
reinforcement learning, and a real-world example.
3	Related work
We review here some of the most common techniques in reinforcement learning, limiting ourselves
to those who try to directly optimize the policy.
The first such method is REINFORCE (Williams, 1992), which performs a single gradient step
between two rollouts of the policy. This method has multiple issues. First, one has to do rollouts
after each update, ultimately resulting in many rollouts before reaching a good solution. This is
further emphasized with the potential poor quality of the gradient update which is not insensitive to
a reparametrization of the parameter space. Finally, as with any stochastic method, the choice of the
stepsize has a strong influence of the convergence speed. Each of these problems has been treated in
separate works. The need to perform rollouts after each update was alleviated by using importance
sampling policy gradient (Swaminathan & Joachims, 2015). The update direction can be improved
by using natural gradient (Amari, 1998; Kakade, 2001) and doing a line search helps in choosing a
correct stepsize (Jie & Abbeel, 2010). These improvements can be computationally expensive and
the additional hyperparameters make them less suited to a production environment where simplicity
and robustness are key.
Another line of research used concave lower bounds on J (θ), which could then be optimized using
off-the-shelf classifiers such as L-BFGS (Liu & Nocedal, 1989). Examples of such methods are
PoWER (Kober & Peters, 2009) and Natural actor-critic (Peters & Schaal, 2008). These bounds
were obtained using an analogy with EM (Dempster et al., 1977; Minka) which required the rewards
R(τ) to be positive. In settings where multiple policies can achieve high accuracy, Neumann (2011)
proposed another EM-based method which focuses on one of these policies rather than trying to
loosely cover all of them, at the expense of a larger computational cost.
We will show how these works can be extended to better optimize the policy between two updates.
We will then show how the positiveness requirement hurts the optimization, then propose an exten-
sion which allows us to use any reward.
Since, in practice, we do not have access to the full distribution but rather to a set of N samples, we
shall optimize a Monte-Carlo estimate of J:
J(θ) = ɪ	R(Ti) P(TijQ ,	Ti 〜p(τ∣θo) ,	(3.1)
() N ( )p(τi∣θo)	, i P( 1 0) ,	()
where p(τ∣Θ0) is the probability of rollout T under the distribution used to generate samples. This is
the standard importance sampling trick commonly used in policy gradient (Sutton et al., 1999).
4 Concave approximation of the expected loss
In this section, we assume that all returns are nonnegativec. Due to this nonnegativity, the non-
concavity in J stems from the nonconcavity of each p(τ∣θ). However, if p(τ∣θ) belongs to the
exponential family (Wainwright & Jordan, 2008), then it is a log-concave function of its parameters
and logp(τ∣θ) is a concave function of θ. This suggests the following lower bound:
Lemma 1. Let
Pq (τ lθ) = q(T)
1 + log
(4.1)
with q such that q(τ) = 0 when p(τ∣θ) = 0. Then we have Pq(τ∣θ) ≤ p(τ∣θ).
cOr at least bounded below, in which case they need to be adequately shifted.
3
Under review as a conference paper at ICLR 2017
Proof.
p"" ) $
≥ q(τ) 1 + log
=Pq(Tlθ) .
The second line stems from the inequality x ≥ 1 + log x.
□
Lemma 1 shows that, regardless of the function q chosen, Pq(T∣θ) is a lower bound of p(τ∣θ) for
any value of θ. Thus, provided that P(T∣θ) belongs to the exponential family, we have obtained a
log-concave lower bound. Lemma 1, however, does not guarantee the quality of that lower bound.
This is addressed by the following lemma:
Lemma 2. Ifthere is a V such that q(τ) = P(T∣ν), we have
Pq (T IV ) = P(T IV )
∂pq (T ∣θ)
∂θ
θ=ν
∂p(t ∣θ)
∂θ
θ=ν
Proof. Pq(TIV) = P(TIV) is immediate when setting θ = V in Eq. 4.1. Deriving Pq (T Iθ) with respect
to θ yields
∂Pq (t ∣θ)	∂ I ʌd lθg P(T ∣θ)
=P(T IV) 一∂θ一
p(t∣v) ∂p(t∣θ)
p(t ∣θ)	∂θ .
Taking θ = V on both sides yields dpq∂Tlθ)	=已喝⑼	.
θ=ν	θ=ν
□
To simplify further notations, we will write directly
Pν (T Iθ) =P(TIV) 1 + log
P(T ⑼)
P(T IV )√
(4.2)
to explicitly show the dependency of the bound on the parameter V.
The following result is a direct consequence of these two lemmas:
Lemma 3. (lower bound of the expected reward estimator): Let
JV(θ) = NN X R(Ti)
i
P(TiIV)
P(Ti 仇)
1+ log
P(Ti m))
P(TiIV)J
(4.3)
^ ^ 0
Then we have JV(θ) ≤ J(θ) ∀θ , JV(v) = J(v) ,	—V^-
∂θ
p(tI∙) is a log-concavefunction, then JV is Concaveforany V.
ʌ..
∂J(θ) I	L , Y
= I . Further, if
θ=V ∂θ Iθ=V
Proof. Since each R(Ti) is nonnegative, so is the ratio PR(T：)). The sum of lower bounds being a
lower bound, this concludes the proof.	□
It is now worth going into more detail on the three parameters of Eq. 4.3:
•	θ is the current value of the parameter we are trying to optimize over;
•	θ0 is the parameter value used to gather samples;
•	V is the parameter used to create the lower bound. Any value of V is valid.
4
Under review as a conference paper at ICLR 2017
There are two special cases of this bound. First, when ν = θ = θ0, this bound becomes an equality
and we recover the policy gradient of Williams (1992). However, this equality only holds for the
first update of θ. Second, a more interesting case is ν = θ0 6= θ. In this case, Eq. 4.3 simplifies and
we get
Jθ0 (θ) = 1- X R(Ti)(1 + log P(T:叩.	(4.4)
0J N V iV	P(TiWoV
This bound is used by multiple authors (Dayan & Hinton, 1997; Peters & Schaal, 2007; 2008; Kober,
2014; Schulman et al., 2015a) and has the attractive property that it is tight at the beginning of the
optimization since we have θ = θ0 . When we optimize this bound without ever changing the value
of ν, we end up with exactly the PoWER algorithm. However, as we optimize θ, this bound becomes
looser and it might be useful to change the value of ν.
This suggest an iterative scheme where, after the optimization of Eq. 4.4 has ended in θ = θ1,
we recompute the bound of Eq. 4.3 with ν = θ1. This yields an iterative version of the PoWER
algorithm as described in Algorithm 1.
The data: Rewards R(Ti), probabilities p(τi∣θo), initial parameters θo
The result: Final parameters θT
for t = 1 to T do
θt = arg max Jθt-1 (θ)
θ
= arg 产 X R(Ti)" ("^ PB)
end
Algorithm 1: Iterative PoWER
We recall that PoWER is equivalent to Algorithm 1 but with T = 1. As we shall see in the experi-
ments, larger values of T lead to vast improvements.
One can also see that Algorithm 1 performs the same optimizations as the PoWER algorithm where
new samples would be gathered at each iteration, with the difference that importance sampling is
used rather than true samples. Thus, in the spirit of off-policy learning, we have replaced extra
sampling with importance sampling. When, and only when, variance starts to be too high, can we
sample from the new distribution.
5 Convex upper bound
Lemma 3 requires positive returns. This has two undesirable consequences. First, this can lead to
very slow convergence. One can see this by creating a setting where all rollouts lead to a positive
return except for one which leads to a return of -β < 0. To maintain the positivity of the returns,
we need to shift all the returns by β which does not change the optimal policy using the following
transformation: J(θ) = / P(T∣Θ)R(t) dr = J P(T∣Θ)(R(t) + β) dr — β. WemaynoW apply
lemma 3 and optimize the following lower bound:
Je 阴=ZTP(TIV )(ι+log p≡)(r(t )+⑶而-*
Without the rollout with a negative return, the returns would not need to be shifted by β and we
could have optimized Jν0 (θ) instead. The difference between the two is equal to Jνβ (θ) — Jν0 (θ) =
-BKL(p(t|v)∣∣p(t∣θ)) where KL is the KUllback-Leibler divergence, which encourages P(T∣θ) to
be close to P(T∣v). Hence, one rollout with a negative return would slow down our optimization
with a regularization term proportional to that return. A simple heuristic would be to discard such
rollouts but we would lose all guarantees about the improvement of the expected return.
5
Under review as a conference paper at ICLR 2017
Further, the positivity of the returns precludes the use of control variates which are especially useful
when the shifted rewards are approximately centered on 0. Thus, it prevents us from benefiting of all
the existing techniques based around these control variates which would help reducing the variance.
The positivity assumption is required since we multiply our lower bound with the returns. If, instead,
We have convex upper bounds of p(τ∣θ), then this would provide Us with a concave lower bound
whenever it is associated with a negative return. Lemma 4 provides such a bound.
Lemma 4. Let
UV(T∣θ) = p(τ∣ν)exp (θ - ν)>dlogp(τl"	,	(5.1)
∂θ	θ=ν
where p(τ ∣θ) is a log-concave function of θ. Then Uv (τ ∣θ) is a convex function of θ and we have:
UV(T IV)= P(TIV) ,	auvdjθ)	= dp∂τθ)	,	UV (T W) ≥ P(T ⑻ ∀θ .
∂θ	θ=ν	∂θ	θ=ν
Proof. Both equalities can be verified by setting θ = V. Since UV is the exponential of a linear
function in its argument, it is convex. Finally, using the concavity of log P, we have:
logp(t∣θ) ≤ logp(t∣v) + (θ - V)> dlogp(T")
∂θ	θ=V
P(T Iθ) ≤ P(TIV) exp
(θ-V)
> ∂ log p(τ∣θ)
∂θ
θ=ν .
This concludes the proof.
□
We may now combine the two bounds to get the following lower bound on J(θ) without any con-
straint on the rewards:
J(θ) ≥ ɪ X R(Ti)Zi(θ)	(5.2)
N i	P(TiIθ0)
加 _ 1	P ( , 1	p(Ti∣θ)∖	>∂	、> ∂ log p(t ∣θ)-
zi(θ~) = 1R(τi)≥0 (l+log P(T IV) J + 1R(Ti)<0 exp ](θ - V) -∂θ-- θ .
Further, when P(TIθ) is log-concave in θ, then zi(θ) is concave in θ.
This allows us to deal with positive and negative rewards, which means that the choice of control
variate is now free, which can help both in reducing the variance and improving the lower bound
and thus the convergence.
6 Experiments
To demonstrate the effectiveness of Iterative PoWER, we provide results obtained on the Cartpole
benchmark using the Gym d toolkit. We also show experiments on real online advertising data where
we try to maximize advertisers’ revenue while keeping our costs constant.
6.1	Cartpole benchmark
To show how we can achieve good performance with limited rollouts, we ran our experiments on
the Cartpole benchmark, which is solved by the PoWER method. We used a stochastic linear policy
with a 4-dimensional state (positions and velocities of the cart and the pole) where, at each time step,
the cart moves right with probability σ(s>θ) where s is the vector representation of the state. Each
experiment was 250 rollouts, in 10 batches of 25 rollouts, of length 400. In between each batch, we
retrained the parameters of the model. The average performance over 350 repetitions was computed
every batch, varying the number T of iterations of PoWER e. We capped the importance weights at
20.
dhttps://gym.openai.com/
eEach iteration of PoWER consisted in 5 steps of Newton method
6
Under review as a conference paper at ICLR 2017
Figure 1: Reward as a function of the number of rollouts for the Cartpole problem, for various
values of iterations of Power and various values of the control variate. The light areas represent 3
standard deviations. The performance greatly increases with higher values of T , especially with a
proper control variate. Except at the very beginning, there is no major difference between T = 5 and
T = 20, leading us to think that we might suffer from variance issues despite the control variates.
Figure 2: Reward as a function of the number of rollouts for the Cartpole problem, for various values
of iterations of Power and various values of the control variate. The light areas represent 3 standard
deviations. Using a control variate drastically improves the performance when using large values of
T while it has very little impact for T = 1 and T = 2. This demonstrates the importance of being
able to deal with negative rewards.
7
Under review as a conference paper at ICLR 2017
Figure 3: Dashed blue: improvement in expected mer-
chant value as a function of the number of iterations (ar-
bitrary linear scale). The optimization is very slow at the
beginning and the improvement is close to linear for the
first 50 iterations.
Solid green: Relative change of the expected cost as a
function of the number of iterations. The expected spend
increased up to 0.0086% but reached 0.0001% when we
stopped the optimization.
We also evaluated the impact of using control variates on the convergence. To that effect, at each
iteration, we computed the control variate minimizing the variance of the total estimator. We then
used as control variate fractions of that value. For instance, cv = 0.5 means that the control variate
used was half of the “optimal” control variate found by minimizing the variance of the estimator.
The results can be seen in Fig. 2 and Fig. 1. We see that doing several iterations of PoWER leads to
an increase in performance, up to a point where the variance is too high.
We can also see that, without using control variates (cv = 0), it is not beneficial to do too several
iterations. The stagnation in performance for cv = 0.25 and cv = 0.5 might be due to the poorer
quality of the upper bound compared to the lower bound. Thus, to get the full benefit of the control
variate, we need to use larger values, such as cv = 0.99, which yields the highest performance for
any value of T .
6.2 Real world data - Online advertising
We also tested Iterative PoWER using real data gathered from a production system. In this system,
every time a user lands on a webpage, an auction is run to determine who gets to display an ad to that
user. Each participant to the auction chooses in real-time how much to bid for the right to display
that ad. If the ad is clicked and then the user buys an item on the merchant’s website, a reward equal
to the value of the item is observed. Otherwise, a reward of 0 is observed. The cost of displaying an
ad depends on the bid in a way unknown to us as we do not have access to the type of auction nor to
the bids of the other participants.
We gathered data for over 1.3 billion displays over a period of 2 weeks in April 2016. This data was
comprised of information available at the time of the bid, for instance the history of the user, the
current URL, or the size of the ad to display. The aggregation of all this data represented the states.
When we won the auction, we also logged our bid, which are our actions, the cost of the display and
the value generated if there was a sale, both of which are used for the reward.
Rather than learning a full bidding strategy mapping from states to a bid, we used our production
system as baseline and learnt a small modifier to account for the non-truthfulness of the auctions.
Thus, only a few thousand parameters were learnt, a small number compared to the number of
training samples. This allowed us to run many iterations of PoWER without fear of high variance.
Since our aim was to maximize the value generated, this could lead to the undesirable solution
where all ads are bought regardless of their price. Thus, we included the constraint that the total
cost of buying the ads had to remain constant. The details of iPoWER with added constraints are in
section 7.1
Fig. 3 shows the relative improvement in advertiser value generated as a function of the number
T of iterations. We can readily see that the final improvement obtained by Iterative Power is far
greater than that obtained after one iteration of PoWER (about 60 times greater). The green curve
shows the change in total cost. Since we also used a lower bound for the constraint, it is initially not
satisfied, but running the algorithm to convergence leads to a solution in the feasible set. The final
improvement represents an increase of the net gain by several percentage points.
8
Under review as a conference paper at ICLR 2017
7 Extensions and conclusion
7.1	Application to constrained optimization
There are cases where one wishes to maximize the expected reward while satisfying a constraint
on another quantity. In online advertising, for instance, it is interesting to maximize the number of
clicks (or sales) while keeping the number of ads displayed constant as this reduces the potential
long-term effects on user behaviour, something not captured at the scale of a rollout. To maximize
the expected reward Eθ [R] while satisfying the constraint Eθ [S] = S0, we may add a Lagrangian
term to Jν (θ) and iteratively solve the following problem:
.v-^ p(、p(TiIV)	A , 1	P(Tim)(p-τ	p(、p(TiIV)	A 1	P(Tiιθp⅛	J
maxmin ? R(Ti)而两(1+log 而m+α (∑S(Ti)而而(1+log 西西)-SS,
where α is the Lagrange multiplier associated with the constraint. Due to the approximation, the
constraint will not be exactly satisfied for the first few iterations but the convergence of this algorithm
guarantees that the constraint will eventually be satisfied.
7.2	Extension to non log-concave policies
The results of this paper rely on a log-concavity assumption on the policy which can be too strong a
constraint. Indeed, in many cases, the policy depends in a complex manner on the state, for instance
through a deep network. However, most of these policies can still be written as a log-concave policy
on a non-linear transformation of the states, for instance when the last layer of the deep network uses
a softmax. iPoWER can then be used to transform the optimization problem into a simpler, albeit
still not concave, maximization problem where the non-concavity of the output of the network has
been removed and only remains the non-concavity of the nonlinear transformation of the state.
7.3	Conclusion
We proposed a modification to the PoWER algorithm which allows to improve a policy with a re-
duced number of rollouts. This method is particularly interesting when there are constraints on the
number of rollouts, for instance in a robotic environment or when each policy has to be deployed
in an industrial production system. We also proposed an extension to existing EM-based methods
which allows for the use of control variates, a potentially useful tool to reduce the variance of the
estimator. However, several questions remain. In particular, experiments on the Cartpole bench-
mark indicate that, despite the use of capped importance weights and control variates, as we do
more iterations, we might end up in regions of the space with high variance. It is thus important
to use additional regularizers, such as normalized weights or penalizing the standard deviation of
the estimator. To maintain the simplicity of the overall algorithm, concave lower bounds of these
regularizers must also be found, which is still an open problem.
Acknowledgments
The author would like to thank Vianney Perchet for helpful discussions.
References
ShUn-Ichi Amari. Natural gradient works efficiently in learning. Neural COmputatiOn, 10(2):251-
276, February 1998.
Peter Dayan and Geoffrey E Hinton. Using expectation-maximization for reinforcement learning.
Neural COmputatiOn, 9(2):271-278, 1997.
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete
data via the em algorithm. JOurnal Of the rOyal statistical sOciety. Series B (methOdOlOgical), pp.
1-38, 1977.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning.
CORR, abs:1103.4601, 2011. URL http://arxiv.org/abs/1103.4601.
9
Under review as a conference paper at ICLR 2017
Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning
with model-based acceleration. CoRR, abs/1603.00748, 2016. URL http://arxiv.org/
abs/1603.00748.
Tang Jie and Pieter Abbeel. On a connection between importance sampling and the likelihood ratio
policy gradient. In Advances in Neural Information Processing Systems, pp. 1000-1008, 2010.
Sham Kakade. A natural policy gradient. In NIPS, volume 14, pp. 1531-1538, 2001.
Jens Kober. Learning motor skills: from algorithms to robot experiments. it-Information Technology,
56(3):141-146, 2014.
Jens Kober and Jan R Peters. Policy search for motor primitives in robotics. In Advances in neural
information processing systems, pp. 849-856, 2009.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR,
abs/1509.02971, 2015. URL http://arxiv.org/abs/1509.02971.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989.
Thomas Minka. Expectation-maximization as lower bound maximization. URL http://
www-white.media.mit.edu/tpminka/papers/em.html.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.
Gerhard Neumann. Variational inference for policy search in changing situations. In Proceedings
of the 28th international conference on machine learning (ICML-11), pp. 817-824, 2011.
Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational
space control. In Proceedings of the 24th international conference on Machine learning, pp. 745-
750. ACM, 2007.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7):1180-1190, 2008.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using
stochastic computation graphs. CoRR, abs/1506.05254, 2015a. URL http://arxiv.org/
abs/1506.05254.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438,
2015b. URL http://arxiv.org/abs/1506.02438.
David Silver et al. Mastering the game of go with deep neural networks and tree search. Na-
ture, 529:484-503, 2016. URL http://www.nature.com/nature/journal/v529/
n7587/full/nature16961.html.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT
press Cambridge, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPS, volume 99, pp. 1057-
1063, 1999.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from
logged bandit feedback. CoRR, abs:1502.02362, 2015. URL http://arxiv.org/abs/
1502.02362.
Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational
inference. Foundations and TrendsR in Machine Learning, 1(1-2):1-305, 2008.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
10