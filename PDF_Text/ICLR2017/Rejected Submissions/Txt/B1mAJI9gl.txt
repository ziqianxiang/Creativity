Under review as a conference paper at ICLR 2017
Towards Understanding the Invertibility of
Convolutional Neural Networks
Anna C. Gilbert1	Yi Zhang1	Kibok Lee1 Yuting Zhang1	Honglak Lee1,2
1University of Michigan, Ann Arbor, MI 48109
2GoogleBrain, Mountain View, CA 94043
{annacg,yeezhang,kibok,yutingzh,honglak}@umich.edu
Ab stract
Several recent works have empirically observed that Convolutional Neural Nets
(CNNs) are (approximately) invertible. To understand this approximate invertibility
phenomenon and how to leverage it more effectively, we focus on a theoretical
explanation and develop a mathematical model of sparse signal recovery that is
consistent with CNNs with random weights. We give an exact connection to a
particular model of model-based compressive sensing (and its recovery algorithms)
and random-weight CNNs. We show empirically that several learned networks are
consistent with our mathematical analysis and then demonstrate that with such a
simple theoretical framework, we can obtain reasonable reconstruction results on
real images. We also discuss gaps between our model assumptions and the CNN
trained for classification in practical scenarios.
1	Introduction
Deep learning has achieved remarkable success in many technological areas (Bengio et al., 2013;
Schmidhuber, 2015), including computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015;
Simonyan and Zisserman, 2015), automatic speech recognition (Hinton et al., 2012; Hannun et al.,
2014), natural language processing (Collobert et al., 2011; Mikolov et al., 2013; Cho et al., 2014),
bioinformatics (Chicco et al., 2014), even high energy particle physics (Baldi et al., 2014). In
particular, deep Convolutional Neural Networks (CNNs) (LeCun et al., 1989; Krizhevsky et al., 2012;
Simonyan and Zisserman, 2015) have been a critical enabling technique for analyzing images and
sequential data.
Following the unprecedented success of deep networks, there has been some theoretical work
(e.g., Arora et al. (2014; 2015); Paul and Venkatasubramanian (2014)) that suggest several mathemat-
ical models for different deep learning architectures. However, theoretical analysis and understanding
lag behind the very rapid evolution and empirical success of deep architectures, and more theoretical
analysis is needed to better understand the state-of-the-art deep architectures, and possibly to improve
them further.
In this paper, we attempt to address the gap between the empirical success and theoretical understand-
ing of the Convolutional Neural Nets, in particular its invertibility (i.e., reconstructing the input from
the hidden activations), by analyzing a simplified mathematical model using random weights.1
This property is intriguing because convolutional neural networks are typically trained with discrimi-
native objectives (i.e., unrelated to reconstruction) with a large amount of labels, such as the ImageNet
dataset. For example, Dosovitskiy and Brox (2016) used upsampling-deconvolutional architectures
to invert the hidden activations of feedforward CNNs to the input domain. In other related work,
Zhao et al. (2016) proposed stacked a what-where network via a (deconvolutional) decoder and
demonstrate its promise in unsupervised and semi-supervised settings. Bruna et al. (2014) studied
signal discovery from generalized pooling operators using image patches on non-convolutional small
scale networks and datasets. Zhang et al. (2016) showed that CNNs discriminately trained for image
classification (e.g., VGG Net (Simonyan and Zisserman, 2015)) are almost fully invertible using
pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why
CNNs are invertible yet.
We introduce three new concepts that, coupled with the accepted notion that images have sparse
representations, guide our understanding of CNNs:
1For more discussion about random filters, see Sections 2.1 and 4.1.
1
Under review as a conference paper at ICLR 2017
1.	we provide a particular model of sparse linear combinations of the learned filters that are
consistent with natural images; also, this model of sparsity is itself consistent with the
feedforward network;
2.	we show that the effective matrices that capture explicitly the convolution of multiple filters
exhibit a model-Restricted Isometry Property (model-RIP) (Baraniuk et al., 2010); and
3.	our model can explain each layer of the feedforward CNN algorithm as one iteration
of Iterative Hard Thresholding (IHT) (Blumensath and Davies, 2009) for model-based
compressive sensing and, hence, we can reconstruct the input simply and accurately.
In other words, we give a theoretical connection to a particular model of model-based compressive
sensing (and its recovery algorithms) and CNNs. We show empirically that large-scale deep con-
volution networks are consistent with our mathematical analysis. We then demonstrate that with
such a simple theoretical framework, we can obtain reasonable reconstruction results on real images,
using filters from trained networks. Finally, we observe that it makes a significant difference which
filters one uses for encoding and decoding, whether they are trained specifically for reconstruction, or
random, or the same for both procedures. This paper explores these properties and elucidate specific
empirical aspects that any more sophisticated mathematical model should take into account.2
2	Preliminaries
In this section, we set the stage for our mathematical analysis in Section 3. We begin with discussion
on the use of random weights in (convolutional) neural networks, and then provide the definitions
and models for CNNs. Then, we discuss compressive sensing and sparse signal recovery. We define a
particular model of sparsity that we will use throughout our analysis and detail the Iterative Hard
Thresholding (IHT) algorithm which is the basis of our reconstruction analysis.
In order to simplify our notation and to make clear our analysis, we focus on a single layer in the
analysis instead of multiple layers.3 Also, we assume that all of our input signals are vectors rather
than matrices and that any operations we would ordinarily carry out on images (e.g., convolving
with a filter bank, dividing into regions over which we pool coefficients), we do on vectors with the
appropriate modifications for a simplified structure. While these assumptions ease our exposition,
they do not change the nature of our arguments nor their implications for images. Furthermore, we
demonstrate the validity of our results in two-dimensional natural images.
2.1 Effectiveness of Gaussian Random Filters
We analyze theoretically CNNs with Gaussian random filters, which have been surprisingly effective
in unsupervised and supervised deep learning tasks. Jarrett et al. (2009) showed that random filters
in 2-layer CNNs work well for image classification. In addition, Saxe et al. (2011) observed that
convolutional layer followed by pooling layer is frequency selective and translation invariant, even
with random filters, and these properties lead to good performance for object recognition tasks.
On the other hand, Giryes et al. (2016) proved that CNNs with random Gaussian filters have
metric preservation property, and they argued that the role of training is to select better hyperplanes
discriminating classes by distorting boundary points among classes. According to their observation,
random filters are in fact a good choice if training data are initially well-separated. Also, He et al.
(2016) empirically showed that random weight CNNs can do image reconstruction well.
To better demonstrate the effectiveness of Gaussian random CNNs, we evaluate their classification
performance on CIFAR-10; see Section 4.1 for details. We find that a 3-layer Gaussian random
CNN is able to achieve 〜75% accuracy on the test set, with only the last classifier layer optimized,
(see Table 1 for more details). Even though this number is far from the state-of-the-art results, it is
surprisingly good considering the networks are almost untrained. Our theoretical results may provide
another new perspective on explaining these phenomena.
2We note that our model may not be an exact replica of a real setting, but for mathematical analysis, it is
a simplified but representative abstraction of practical settings. A number of works show that random weight
CNNs still achieve surprisingly good classification accuracy although they may not match the state-of-the-art
results; see Sections 2.1 and 4.1 for more discussion.
3We can extend the equivalency on a single layer of CNNs to multiple layer CNNs simply by using the output
on one layer as the input to another, still using the steps of the inner loop of IHT.
2
Under review as a conference paper at ICLR 2017
W	x	= h
l
■ -1 F¾--;
ItF	I.... I	l
lʃ 二二 1"~J 二二■! 匚二二 I=T=G
(wιH	I I (wι,M)τ
,___ ⊂=l	⊂=l
造一]隼
I ×____ I.... I **∙_ l
匚二二 J=T 二二4 lʃ二二 J=T二二∙∣
(WK,1产、I [	(WKM产，I
j=0,…,n-1
D
n
-MD
κ Kn
Figure 1: One-dimensional CNN architecture where W ∈ RKn×MD is the matrix instantiation of convolution
over M channels with a filter bank consisting of K different filters. Note that a filter bank has K filters of size
l × M, such that there are lMK parameters in this architecture.
2.2	Convolutional Neural Nets
We define a single layer of our CNN as follows. We assume that the input signal x consists of M
channels, each of length D, and we write x ∈ RMD. For each of the input channels, m = 1, . . . , M,
let wi,m, i = 1, . . . , K denote one of K filters, each of length `. Let t be the stride length, the
number of indices by which we shift each filter. Note that t can be larger than 1.
We assume that the number of shifts, n = (D - `)/t + 1, is an integer. Let wij,m be a vector of length
D that consists of the (i, m)-th filter shifted by jt, j = 0, . . . , n - 1 (i.e., wij,m has at most ` non-zero
entries). We will concatenate over the M channels each of these vectors (as row vectors) to form a
large matrix, W , which is the Kn × MD matrix made up of K blocks of the n shifts of each filter
in each of M channels. We assume that Kn ≥ M D. We also assume that the Kn row vectors of W
span RMD and that we have normalized the rows so that they have unit `2 norm. We assume that the
hidden units of the feed-forward CNN are computed by multiplying an input signal x ∈ RMD by
the matrix W (i.e., convolving, in each channel, by a filter bank of size K, and summing over the
channels to obtain Kn outputs), applying the ReLU function to the Kn outputs, and then selecting
the value with maximum absolute value in each of the K blocks; i.e., we perform max pooling over
each of the convolved filters and sum over the channels.4 We use h = Wx for the hidden activation
computed by a single layer CNN without pooling. Figure 1 illustrates the architecture.
2.3	Compressive sensing
Let Φ be a i × j matrix with j > i. We say that Φ satisfies the Restricted Isometry Property RIP(k, δk)
(or, just RIP) if there is a distortion factor δk > 0 such that for all z ∈ Rj with exactly k non-zero
entries, (1 - δk)kzk22 ≤ kΦzk22 ≤ (1 + δk)kzk22. If Φ satisfies RIP (for appropriate sparsity level
k and sufficiently small δk) and if z ∈ Rj is k-sparse, then, given the vector x = Φz ∈ Ri, we
can efficiently recover Z (See Candes (2008) for more details)5. There are many efficient algorithms
for doing so, including `1 sparse coding (e.g., `2 minimization with `1 regularization) and greedy,
iterative algorithms (such as Iterative Hard Thresholding or IHT).
Model-based compressive sensing. While sparse signals are a natural model for some applications,
they are less realistic for CNNs. We consider a vector z ∈ RKn as the true sparse code for generating
the CNN input x with a particular model of sparsity. Rather than permitting k non-zero entries
anywhere in the vector z, we divide the support of z into K contiguous blocks of size n and we
stipulate that from each block there is at most one non-zero entry in z with a total of k non-zero
4The convolution can be computed more efficiently than a straight-forward matrix multiplication, but they
are mathematically equivalent.
5We note that this is a sufficient condition and that there are other, less restrictive sufficient conditions, as
well as more complicated necessary conditions. Furthermore, we have not given the exact, quantitative relations
amongst the parameters. For simplicity, we stick with this definition.
3
Under review as a conference paper at ICLR 2017
entries. We call a vector with this sparsity model model-k-sparse and denote the union of all k-
sparse subspaces with this structure Mk. It is clear that Mk contains nk Kk subspaces. In our
analysis, we consider linear combinations of two model-k-sparse signals. To be precise, suppose that
z = α1z1 + α2z2 is the linear combination of two elements in Mk. Then, we say that z lies in the
linear subspace M2k that consists of all linear combinations of vectors from Mk .
We say that a matrix Φ satisfies the model-RIP condition for parameter k if, there is a distortion
factor δk > 0 such that, for all z ∈ Mk,
(1-δk)kzk22 ≤ kΦzk22 ≤ (1 + δk)kzk22.	(1)
See Baraniuk et al. (2010) for the definitions of model sparse and model-RIP, as well as the necessary
modifications to account for signal noise and compressible (as opposed to exactly sparse) signals
(which we have neglected to consider to keep our analysis simple). Intuitively speaking, a matrix that
satisfies the model-RIP is a nearly an orthonormal matrix for a particular set of sparse vectors with a
particular sparsity model or pattern.
For our analysis, we also need matrices Φ that satisfy the model-RIP condition for vectors z ∈ M2k .
We denote the distortion factor δ2k for such matrices. Note that δk ≤ δ2k < 1.
Algorithm 1 Model-based IHT
Input: model-RIP matrix Φ, measure-
ments x = Φz, structured sparse
approximation algorithm M
Output: k-sparse approximation z
1:	Initialize z0 = 0, d = x, i = 0
2:	while stopping criteria not met do
3:	i J i + 1
4:	b J Zi-ι + ΦTd
5:	zi J M(b, k)
6:	d J x - Φzi
7:	end while
8:	return z J zi
Many efficient algorithms have been proposed for sparse
coding and compressive sensing (Olshausen et al., 1996;
Mallat and Zhang, 1993; Beck and Teboulle, 2009). As
with traditional compressive sensing, there are efficient
algorithms for recovering model-k-sparse signals from
measurements (see Baraniuk et al. (2010)), assuming the
existence of an efficient structured sparse approximation
algorithm M, that given an input vector and the sparsity
parameter, returns the vector closest to the input with the
specified sparsity structure.
In convolutional neural networks, the max pooling oper-
ator finds the downsampled activations that are closest to
the activations of the original size by retaining the most
significant values. The max pooling can be viewed as two
steps: 1) zeroing out the locally non-maximum values;
2	) downsampling the activations with the locally maximum values retained. To study the pooled
activations with sparsity structures, we can recover dimension loss from the second step (downsam-
pling step) by an unsampling operator. This procedure defines our structured sparse approximation
algorithm M(z, k), where z is the original (unpooled) code, and k is the sparsity parameter for further
sparsification, which guarantees that M(z , k) is a model-k-sparse signal. With the standard layered
formulation for neural networks, we have
M(z, k) = block-sparsify(upsample(max-pool(z), s), k),	(2)
where s denotes the upsampling switches that indicate where to place the non-zero values in the
upsampled activations. Taking the pooling switches known from the max pooling operation as s, we
specifically define M as the nesting of the max pooling and the unpooling with known switch. We
define this special case as
Mknown(z, k) = block-sparsify(upsample(max-pool(z), max-pool-switch(z)), k).	(3)
Alternatively, using the fixed uniform switches as s, we specifically define M as the nesting of the
max pooling and the naive unsampling, denoted by Mfixed . In the rest of this paper, our theoretical
analysis are generic to any type of valid upsampling switches6, so we use M(z , k) to denote the
structured sparse approximation algorithm without worrying about s. The two special cases Mknown
and Mfixed are used in the empirical analysis when we need to specify M(z, k) as a fully concrete
operator.
The main recovery algorithm that we focus onis a model-sparse version of Iterative Hard Thresholding
(IHT) (see Blumensath and Davies (2009)), not because we are interested in recovering model-
sparse signals, per se, but because one iteration of IHT for our model of sparsity captures exactly
6Valid switches should place a non-zero value at exactly one location.
4
Under review as a conference paper at ICLR 2017
a feedforward CNN.7 Algorithm 1 describes the model-based IHT algorithm. In particular, the
sequence of steps 4-6 in the middle IHT (without the outer iterative loop) is exactly one layer of a
feedforward CNN. As a result, the theoretical analysis of IHT for model-based sparse signal recovery
serves as a guide for how to analyze the approximation activations of a CNN.
3	Analysis
To motivate our more formal analysis, we begin with a simple example. Suppose that the matrix W
is an orthonormal basis for RMD and define Ψ = WT -WT .
Proposition 1. A one-layer CNN using the matrix ΨT, with no pooling, gives perfect reconstruction
(with the matrix Ψ) for any input vector x ∈ RMD .
Proof. Because we have both the positive and the negative dot products of the signal with the basis
T	Wx
vectors in ReLU(ΨT x) = ReLU -Wx , we have positive and negative versions of the hidden
units h+ = ReLU(W x) and h- = ReLU(-W x) where we decompose h = Wx = h+ - h-
into the difference of two non-negative vectors, the positive and the negative entries of h. From this
decomposition, we can easily reconstruct the original signal via
Ψ hh+ = WT	-WT	hh+ = WT(h+ - h-) = WTh = WTWx = x.
□
In the example above, we have pairs of vectors (w, -w) in our matrix Ψ. This settings allow us to
turn what would ordinarily be a nonlinear function, ReLU, into a linear one. In fact, the assumption
that trained CNN filters come in positive and negative is validated by Shang et al. (2016), which
makes a CNN much easier to analyze within the model compressed sensing framework.
Suppose that we have a vector z that we split into positive and negative components, z = z+ - z-,
and that we synthesize (or construct) a signal x from z using the matrix WT -WT . Then, we
have
WT -WT	zz+ = WT (z+ - z-) = WTz = x.
Next, suppose that we multiply x = WTz by the transpose of the same matrix, we find
W
-W
WWTz
-WWTz
and, if we apply ReLU to this vector, we produce
(WWTz)+
(WWTz)-
a vector that is split
into its positive and negative components. To determine whether or not we have “reconstructed” the
vector z, the structure of the product WWT is crucial. In addition, this calculation shows that if we
have both positive and negative pairs of filters or vectors, then the ReLU function applied to both the
positive and negative dot products simply splits the vector into the positive and negative components.
These components are then reassembled in the next computation. For this reason, in the analysis
in the following sections, it is sufficient to consider WTz = x and Wx = h with max pooling
alone applied to h, assuming that all of the entries in the vectors are real numbers, rather than only
non-negative.
3.1	Model-RIP and random filters
Our first main result says that if we use Gaussian random filters in our CNN, then, with high
probability, the transpose of the matrix W formed by the convolutions with these filters has the
model-RIP property. In other words, Gaussian random filters generate a matrix whose transpose
W T is almost an orthonormal transform for sparse signals with a particular sparsity pattern (that
is consistent with our pooling procedure). The bounds in the theorem tell us that we must balance
the size of the filters ` and the number of channels M against the sparsity of the hidden units k, the
number of the filter banks K, the number of shifts n, the distortion parameter δk, and the failure
probability . The proof is in Appendix A.
7Multiple iterations of IHT can improve the quality of signal recovery. However, it is rather equivalent to the
recurrent version of CNNs and does not fit to the scope of this work.
5
Under review as a conference paper at ICLR 2017
Theorem 3.1. Assume that we have MK vectors wi,m of length ` in which each entry is a scaled
i.i.d. (Sub-)Gaussian random variable with mean zero and variance 1 (the scaling factor is 1∕√M').
Let t be the stride length (where n = (D - `)/t + 1) and build the structured random matrix W as
the weight matrix in a single layer CNN for M -channel input dimension D. If
M' 、 C" ”八」、	」〕
Djj- ≥ 阳(Iog(K) + log(n) - log(c)}
then, with probability 1 - , the MD × Kn matrix WT satisfies the model-RIP for model Mk with
parameterδk.
We also note that the same analysis can be applied to the sum of two model-k-sparse signals, with
changes in the constants (that we do not track here).
Corollary 3.2. Random matrices with the CNN structure have, with high probability, the model-RIP
property for M2k.
Other examples of matrices that satisfy model-RIP (both empirically and via a less sophisticated
analysis on the dot products between any two columns) include wavelets and localized Fourier bases;
both examples that can be easily and efficiently implemented via convolutions in a CNN.
3.2	Reconstruction bounds
To distinguish the true sparse code Z and its reconstruction, We use Z = M(h, k) = M(Wx, k) for
the reconstruction by CNN. Our next result tells us that if we compute the hidden units h from an
input signal x using a Weight matrix W Whose transpose has the model-RIP and using max pooling
over each filter (Z), then we can reconstruct (approximately) the input signal X simply by multiplying
the hidden units by W. This result bounds the relative error betWeen the approximate reconstruction
X and the input as a function of the distortion for the model-RIP. In our analysis, we assume that the
input signal x = WTZ is a sparse linear combination of hidden activations, captured approximately
by the filters in W. See Appendix B for the detailed proofs. Part of our analysis also shows that the
hidden units Z are approximately the putative coefficient vector Z in the sparse linear representation
for the input signal.
Theorem 3.3. We assume that WT satisfies the M2k-RIP with constant δk ≤δ2k < 1. If we use W
in a single layer CNN both to compute the hidden units Z and to reconstruct the input X from these
hidden units as X so that X = WTM(Wx, k), the error in our reconstruction is
kx - Xk2 ≤
5δ2k √1 + δ2k
1 - δk √1 — δ2k
kXk2.
Recall that the structured sparsity approximation algorithm M includes the downsampling caused by
pooling and an unsampling operator. Theorem 3.3 is applicable to any type of upsampling switches,
so our reconstruction bound is generic to the particular design choice on how to recover the activation
size in a decoding neural network.
4	Experimental Evidence and Analysis
In this section, we provide experimental validation of our theoretical model and analysis. We
first validate experimentally the relevance of our assumption by examining the effectiveness of
random filter CNNs. We then provide an experimental validation of our theoretical analysis on the
synthetic 1D case, then we provide experimental results on more realistic scenarios. In particular,
we study popular deep neural networks trained for image classification on the ImageNet ILSVRC
2012 dataset (Deng et al., 2009). We calculate empirical model-RIP bounds for WT, showing
that they are consistent with theory. Our results are also consistent with a long line of research
shows that it is reasonable to model real, natural images as sparse linear combinations over learned
dictionaries (e.g., Boureau et al. (2008); Le et al. (2013); Lee et al. (2008); Olshausen et al. (1996);
Ranzato et al. (2007); Yang et al. (2010)). In addition, we verify our theoretical bounds for the
reconstruction error ∣∣x 一 WTZk2∕kX∣∣2 on real images. (This is the relative '2 distance between the
original image and the reconstruction.) We investigate both randomly sampled filters and empirically
learned filters in these experiments. Our implementation is based on the Caffe (Jia et al., 2014) and
MatConvNet (Vedaldi and Lenc, 2015) toolboxes.
6
Under review as a conference paper at ICLR 2017
4.1	Evaluation of Gaussian random CNNs on CIFAR-10
To show the practical relevance of our theoretical assumptions on using random filters for CNNs
as stated in Section 2.1, we evaluate simple CNNs with Gaussian random filters (with i.i.d. zero-
mean unit-variance entries) on the CIFAR-10 dataset. The goal of this experiment is not to achieve
state-of-the-art results, but to examine practical relevance of our assumption on random filter CNNs.
Once the CNNs weights are initialized (randomly), they are fixed during the training of the classifiers.
Specifically, we test random CNNs with 1, 2, and 3 convolutional layers, where we use ReLU as the ac-
tivation. A 2 × 2 max pooling layer follows each convolutional layer to down-sample the feature map.8
We experiment with different filter sizes (3, 5, 7) and numbers of channels (64, 128, 256, 1024, 2048)
and report the classification accuracy of the best-performing architectures based on cross-validation
in Table 1. We also report the best performance using learnable filters for comparison. More details
about the architectures can be found in Section C.1 of the supplementary materials. We observe the
CNNs with Gaussian random filters achieve surprisingly good classification performance (implying
that they serve as reasonable representation of input data), although fully learnable CNN counterparts
perform better. Our experimental results are also consistent with the observations made by Jarrett et al.
(2009) and Saxe et al. (2011). Overall, these results seem to suggest that the CNNs with Gaussian
random filters might be a reasonable setup which is amenable to mathematical analysis while not
being too far off in terms of practical relevance.
Method	1 layer	2 layers	3 layers
Random filters	66.5%	74.6%	74.8%
Learned filters	68.1%	83.3%	89.3%
Table 1: Classification accuracy of CNNs with random and learnable filters on CIFAR-10. A typical layer
consists of four operators: convolution, ReLU, batch normalization and max pooling. Networks with optimal
filter size and numbers of output channels are used (see Section C.1 in the supplementary materials for the
architecture details). The random filters, assumed in our theoretical analysis, perform reasonably well, not far
off the learned filters.
4.2	Experimental Validation of the Analysis in 1D Synthetic Data
We use 1-D synthetic data to empirically show the basic validity of our theory in terms of the model-
RIP condition in Equation (1) and reconstruction bound in Theorem 3.3. We plot the histograms
of the empirical model-RIP values of 1D Gaussian random filters W ( scaled by 1∕√7M ) with
size l × 1 × M × K = 5 × 1 × 32 × 96 on 1D Mk sparse signal z with size D = 32 and sparsity
k = 10, whose non-zero elements are drawn from a uniform distribution on [-1, 1]. The histograms
in Figure 2a and 2b are tightly centered around 1, suggesting that WT satisfies the model-RIP
condition in Equation (1) and its corollary from Lemma B.1 in the supplementary materials. We
also empirically show the reconstruction bound in Theorem 3.3 on synthetic vectors x = WT z
(Figure 2c). The reconstruction error is concentrated at around 0.1-0.2 and bound under 0.5. Results
in Figure 2 suggests the practical validity of our theory when the model assumptions hold.
4.3	Architectures and Dataset
We conduct the rest of our experimental evaluations on the 16-layer VGGNet (Model D in Simonyan
and Zisserman (2015)),9 where the computation is carried out on images; e.g., convolution with a
2-D filter bank and pooling on square regions. In contrast to the theory, the realistic network does not
pool activations over all the possible shifts for each filter, but rather on non-overlapping patches. The
networks are trained for the large-scale ImageNet classification task, which is important for extending
to other supervised tasks in vision. The main findings on VGGNet are presented in the rest of this
section; we also provide some analysis on AlexNet (Krizhevsky et al., 2012) in the supplementary
materials.
8Implementation detail: We add a batch normalization layer together with a learnable scale and bias before
the activation so that we do not need to tune the scale of the filters. The filter weights of the intermediate layers
in the CNNs are not trained after random initialization. On top of the network, we use an optional average
pooling layer to reduce the feature map size to 4 × 4 and a dropout layer for better regularization before feeding
the feature to a learnable soft-max classifier for image classification.
9VGGNet is practically important as it is popularly used in the community and is one of the best-performing
“single-pathway” networks (i.e., no skip connections). We expect that the ResNet (e.g., trained from ImagNet)
can also reconstruct images from its activations well in practice. However, the ResNet architectures are too
complicated to be in the scope of our theory without further nontrivial customization.
7
Under review as a conference paper at ICLR 2017
(a)
(b)
(c)
Figure 2: For 1D scaled Gaussian random filters W, we plot the histogram of ratios (a) kW T zk2/kzk2 (model-
RIP condition in Equation (1); supposed to be concentrated at 1), (b) kWWTzk2/kzk2 (model-RIP corollary
from LemmaB.1 in the supplementary materials; supposed to be concentrated at 1), and (C) ∣∣x — x∣∣2/∣∣x∣∣2
(reconstruction bound in Theorem 3.3, supposed to be small), where z is a Mk sparse signal that generates
the vector X and X = WTMfiXed(Wx, k) is the reconstruction of x, where We use the naive unsampling to
recover the reduced dimension due to pooling (see Section 2.3).
layer	c(1,1)	c(1,2)	p(1)	c(2,1)	c(2,2)	p(2)	c(3,1)	c(3,2)	c(3,3)	p(3)
% of non-zeros	49.1	69.7	80.8	67.4	49.7	70.7	53.4	51.9	28.7	45.9
layer	c(4,1)	c(4,2)	c(4,3)	p(4)	c(5,1)	c(5,2)	c(5,3)	p(5)		
% of non-zeros	35.6	29.6	12.6	23.1	23.9	20.6	7.3	13.1		
Table 2: Layer-wise sparsity of VGGNet on ILSVRC-2012 validation set. “c” stands for convolutional layers
while “p” represents pooling layers. CNN with random filters in Section 4.4 can be simulated with the same
sparsity.
VGGNet contains five groups of convolution and pooling layers, each group has 2~3 convolutional
layers followed by a pooling layer. We denote the j-th convolutional layer in the i-th group “conv(i, j),”
and the pooling layer “pool(i).” When we say the activations/features are from i-th layer, we mean
they are the output of pool(i). Our analysis is for single convolutional layers. When evaluating the
i-th layer, we take the activations from the (i - 1)-th layer, and investigate the filters and output of
conv(i, 1).
4.4	2D MODEL-RIP
The key to our reconstruction bound is Theorem 3.3 is the model-RIP condition for our particular
model of sparsity in Equation (1). We empirically evaluate the model-RIP property, i.e., kW T zk/kzk,
for real CNN filters of the pretrained VGGNet. We use two-dimensional coefficients (or hidden units)
z (each block of coefficients is of size D × D), K filters of size ` × `, and pool the coefficients over
smaller pooling regions (i.e., not over all possible shifts of each filter). The following experimental
evidence suggest that the sparsity model and the model-RIP property of the filters are consistent with
what we conclude from the mathematical analysis on the simpler one-dimensional case.
To check the significance of the model-RIP property (i.e., how close kWTzk/kzk is to 1) in
controlled settings, we first synthesize the hidden activations z with sparse uniform random variables,
which fully agree with our model assumptions. The sparsity of z is constrained to the average level of
the real CNN activations (refer to Table 2). Given the filters of a certain convolutional layer, we use
the synthetic z (in equal position to this layer’s output activations) to get statistics for the model-RIP
property. To be consistent with succeeding experiments, we choose conv(5, 2), while other layers
(a) Random
(b) After ReLU
0.
0.
0.
0.
0.
,05
,04
,03
,02
,01
0
.9	0.95	1	1.05	1.
(c) Before ReLU
Figure 3: For VGGNet's conv(5, 2) filters W, we plot the histogram of ratios ∣∣ WTzk2∕kz∣∣2 (the model-RIP
value derived from Equation (1); supposed to be concentrated at 1) where z is a Mk sparse signal. (a) z is
randomly generated with the same sparsity as the conv(5, 2) activations and from a uniform distribution for the
non-zero magnitude. (b) z is recovered by Algorithm 2 from the conv(5,1) activations before applying ReLU.
(c) z is recovered by Algorithm 2 from the conv(5,1) activations after applying ReLU. The learned filters admits
similar model-RIP value distributions to the random filters except for a bit larger bandwidth, which means the
model-RIP condition in Equation (1) can empirically hold even when the filters do not necessarily subject to the
i.i.d Gaussian random assumption.
8
Under review as a conference paper at ICLR 2017
show similar results. Figure 3 (a) summarizes the distribution of empirical model-RIP values, which
is clearly centered around 1 and satisfies Equation (1) with a short tail roughly bounded by δk < 1.
For more details of the algorithm, we normalize the filters from the conv(5, 2) layer, which are ` × `
(` = 3). All K = 512 filters with M = 512 input channels are used.10 We set D = 15 (the same as
the output activations of conv(5, 2)) and use 2 × 2 pooling regions11 (commonly used in recent deep
networks). We generate 1000 Mk randomly sampled sparse activation (z) maps by first sampling
their non-zero supports and then filling elements on the supports uniformly from [-1, 1]. The sparsity
is the same as that in conv(5, 1) activations.
To conduct more realistic experiments,
we observe the actual conv(5, 2) acti-
vations from VGGNet are not neces-
sarily drawn from a model-sparse uni-
form distribution. This motivates us
to evaluate the empirical model-RIP
property on the hidden activations z
that reconstruct the actual input activa-
tions x from conv(5, 1) by WTz. Per
theory, the x is given by a max pool-
ing layer, so we constrain the sparsity
Algorithm 2 Sparse hidden activation recovery
Input: convolution matrix W , input activation/image x
Output: hidden code z, satisfying our model-RIP assump-
tion with Mk and reconstructing x with W
1:	zinit = argminz kx - WTzk22 + λkzk1
2:	zmodel = Mknown(zinit, k)
3:	z = argminz kx - WTzk22 + λkzk1,
s.t. zi = 0 if zimodel = 0
(i.e., the size of the support set is no more than 1 in a pooling region for a single channel). We use a
simple and efficient algorithm to recover Z from X in Algorithm 2. The algorithm is inspired by “'1
heuristic" method that are commonly used in practice (e.g. Boyd (2015)). As shown in Algorithm 2,
we first do `1 -regularized least squares without constraining the support set. Max pooling is then
applied to figure out the support set for each pooling region. In particular, we use Mknown, defined
in (3), to zero out the locally non-maximum values without messing up the support structures. We
perform `1 -regularized least squares again on the fixed support set to recover the hidden activations
satisfying the model sparsity. As shown in Figures 3 (b)-(c), the empirical model-RIP property values
for visual activations x from conv(5, 1) with/without ReLU are both close to 1. The center offset to
1 is less than 0.05 and the range bound δk is rough less then 0.05, which agrees with the theoretical
bound (1) quite well.
To gain more insight, we summarize the learned filter coherence in Table 4 for all the convolutional
layers in VGGNet.12 This measures the correlation or similarity between the columns of WT and is a
proxy for the value of the model-RIP parameter δk (which we can only estimate computationally).
The smaller the coherence, the smaller δk is, and the better the reconstruction. The coherence of
the learned filters is not low, which is inconsistent with our theoretical assumptions. However,
the model-RIP property turns out to be robust to this mismatch. It also demonstrates the strong
invertibility of CNN in practice.
4.5 Reconstruction B ounds
With model-RIP as a sufficient condition, Theorem 3.3 provides theoretical bounds for layer-wise
reconstruction via X = W T M(Wx, k). This operator consists of the projection andreconstruction in
one IHT iteration. Without confusion, we refer to it as IHT for notational convenience. We investigate
the practical reconstruction errors on Layer 1~4 activations (i.e., pool(1)~(4)) of VGGNet.
To encode and reconstruct intermediate activations of CNNs, we employ IHT with sparsity estimated
from the real CNN activations on ILSVRC-2012 validation set (see Table 2). We also reconstruct
input images, since CNN inversion is not limited to a single layer, and images are easier to visualize
than hidden activations. To implement image reconstruction, we project the reconstructed activations
into the image space via a pretrained decoding network as in (Zhang et al., 2016), which extends
a similar autoencoder architecture as in (Dosovitskiy and Brox, 2016) to a stacked “what-where”
autoencoder (Zhao et al., 2016). The reconstructed activations were scaled to have the same norm as
the original activations so that we can feed them into the decoding network.
10We do not remove any filters including those in approximate positive/negative pairs (refer to 3).
11In VGGNet, no pooling layer follows conv(5, 2). Here, we just use it in this way to analyze the convolution-
pooling pair targeted by our theory.
12The coherence is defined as the maximum (in absolute value) dot product between distinct pairs of columns
of the matrix WT, i.e. μ = maxi=j ∣WiWT |, where Wi denote the i-th row of matrix W.
9
Under review as a conference paper at ICLR 2017
Figure 4: Visualization of images reconstructed by a pretrained decoding network with VGGNet’s pool(4)
activation reconstructed using different methods: (a) original image, (b) output of the 5-layer decoding network
with original activation, (c) output of the decoding net with reconstructed activation by IHT with learned filters,
(d) output of the decoding net with reconstructed activation by IHT with Gaussian random filters, (e) output of
the decoding net with Gaussian random activation.
As an example, Figure 4 illustrates the image reconstruction results for the hidden activations of the
4-th layer, the ground truth of which is obtained by feeding natural images to the CNNs. Interestingly,
the decoding network itself is powerful, since it can reconstruct the glimpse of images with Gaussian
random input, as shown in Figure 4 (e). Object shapes are recovered by using the pooling switches
only in the “what-where” autoencoder. This result suggests that it is important to determine which
pooling units are active and then to estimate these values accurately. These steps are consistent with
the steps in the inner loop of any iterative sparse signal reconstruction algorithm.
In Figure 4 (c), we take the pretrained conv(5, 1) filters for IHT. The images recovered from the IHT
reconstructed 4-th layer activations are reasonable and the reconstruction quality is significantly better
than the random input baseline. We also try Gaussian random filters (Figure 4 (d)), which agree more
with the model assumptions (e.g., lower coherence, see Table 4). The learned filters from VGGNet
perform equally well visually. IHT ties the encoder and decoder weights (no filter learning for the
decoder), so it does not perform as well as the decoding network trained with a huge batch of data
(Figure 4 (b)). Nevertheless, we show both theoretically and experimentally decent reconstruction
bounds for these simple reconstruction methods on real CNNs. More visualization results for more
layers are in the supplementary materials (Figure 5 in Section C.3).
In Table 3, we summarize reconstruction performance for all 4 layers. With random filters, the model
assumptions hold and the IHT reconstruction is the best quantitatively. IHT with real CNN filters
performs comparable to the best case and much better than the baseline established by the randomly
sampled activations.
Additionally, reconstruction performance of IHT is strongly related to the filter coherence, sum-
marized in Table 4. Lower coherence agrees more closely with the model assumptions and leads
to higher reconstruction quality. Higher coherence yields worse recovery of the hidden activation
(i.e., large ∣∣Z - z∣∣, where Z is the hidden activations recovered by IHT, Z is the true activation).
Compared to Algorithm 2, (one-step) IHT is not so robust to high coherence.
In summary, when the assumption of i.i.d Gaussian randomness of the CNN filters holds, our
theoretical reconstruction bound strictly match with the empirical observations. More importantly,
we demonstrate that the bound can still reasonably hold in practice for discriminatively learned CNN
layers, which is particularly true for layers with relatively lower coherence.
5 Conclusion
We introduce three concepts that tie together a particular model of compressive sensing (and the
associated recovery algorithms), the properties of learned filters, and the empirical observation
13The relative error in activation space of random activations (the last column) are identical (1.414) for all
i	1	Il r r Il / Il r Il	/rʌ	c /	■	ι 含	∙ ι ι Il r Il Il r Il
layers because kf - f k/kf k = 2 on average for Gaussian random f provided kf k = kf k.
10
Under review as a conference paper at ICLR 2017
layer	image space relative error			activation space relative error		
	learned filters	random filters	random activations	learned filters	random filters	random activations
-1-	0.423	0.380	-0.610-	0.895	0.872	1.414
-2-	0.692	0.438	-0.864-	0.961	0.926	1.414
-3-	0.326	0.345	-0.652-	0.912	0.862	1.414
4	0.379	0.357	0.436	1.051	0.992	1.414	-
Table 3: Layer-wise relative reconstruction errors by different methods in activation space and image space
between reconstructed and original activations. For layer i, we take its activation after pooling from that layer
and reconstruct it with different methods (using learned filters from the layer above or scaled Gaussian random
filters) and feed the reconstructed activation to a pretrained corresponding decoding network.13
layer	(1,1)	(1,2)	(2,1)	(2,2)	(3,1)	(3,2)	(3,3)
coherence of learned filters	0.9427	0.7340	0.6435	0.7465	0.5838	0.4844	0.5194
coherence of random filters	0.6701	0.1218	0.1546	0.1053	0.1099	0.0895	0.0802
layer	(4,1)	(4,2)	(4,3)	(5,1)	(5,2)	(5,3)	
coherence of learned filters	0.4596	0.4574	0.4043	0.4099	0.4099	0.4046	
coherence of random filters	0.0920	0.0619	0.0617	0.0696	0.0674	0.0674	
Table 4: Comparison of coherence between learned filters in each convolutional layer of VGGNet and Gaussian
random filters with corresponding sizes.
that CNNs are (approximately) invertible. Our experiments show that filters in trained CNNs are
consistent with the mathematical properties we present while the hidden units exhibit a much richer
structure than mathematical analysis suggests. Perhaps simply moving towards a compressive, rather
than exactly sparse, model for the hidden units will capture the sophisticated structure in these layers
of a CNN or, perhaps, we need a more sophisticated model. Our experiments also demonstrate that
there is considerable information captured in the switch units (or the identities of the non-zeros in the
hidden units after pooling) that no mathematical model has yet expressed or explored thoroughly.
References
S.	Arora, A. Bhaskara, R. Ge, and T. Ma. Provable Bounds for Learning Some Deep Representations. ICML,
Pages 584-592, 2014.
S.	Arora, Y. Liang, and T. Ma. Why are deep nets reversible: A simple theory, with implications for training.
arXiv:1511.05653, 2015.
P. Baldi, P. Sadowski, and D. Whiteson. Searching for exotic Particles in high-energy Physics with deeP learning.
Nature communications, 5, 2014.
R. G. Baraniuk, V. Cevher, M. F. Duarte, and C. Hegde. Model-Based ComPressive Sensing. IEEE Transactions
on Information Theory, 56(4):1982-2001, 2010.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse Problems. SIAM
Journal of Imaging Science, 2:183-202, 2009.
Y. Bengio, A. Courville, and P. Vincent. RePresentation Learning: A Review and New PersPectives. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 35(8), 2013.
T.	Blumensath and M. E. Davies. Iterative hard thresholding for comPressed sensing. Applied and Computational
Harmonic Analysis, 27(3):265-274, 2009.
Y.-l. Boureau, Y. L. Cun, et al. SParse feature learning for deeP belief networks. In NIPS, 2008.
S. Boyd. l1-norm methods for convex-cardinality Problems, ee364b: Convex oPtimization ii lecture notes,
2014-2015 sPring. 2015.
J.	Bruna, A. Szlam, and Y. LeCun. Signal recovery from Pooling rePresentations. In ICML, Pages 307-315,
2014.
E. J. Cand6s. The restricted isometry property and its implications for compressed sensing. COmptes Rendus
Mathematique, 346(9):589-592, 2008.
D. Chicco, P. Sadowski, and P. Baldi. Deep autoencoder neural networks for gene ontology annotation predictions.
In PrOceedings Of the 5th ACM COnference BiOinfOrmatics, COmputatiOnal BiOlOgy, and Health InfOrmatics,
pages 533-540, 2014.
K.	Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learn-
ing phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint
arXiv:1406.1078, 2014.
11
Under review as a conference paper at ICLR 2017
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing
(almost) from scratch. Journal of Machine Learning Research,12(AUg):2493-2537, 2011.
J.	Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database.
In CVPR, pages 248-255, JUne 2009.
A. Dosovitskiy and T. Brox. Inverting visUal representations with convolUtional networks. CVPR, 2016.
R. Giryes, G. Sapiro, and A. M. Bronstein. Deep neUral networks with random gaUssian weights: A Universal
classification strategy? IEEE Transactions on Signal Processing, 64(13):3444-3457, 2016.
A. HannUn, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. SengUpta,
A. Coates, et al. Deep speech: Scaling Up end-to-end speech recognition. arXiv preprint arXiv:1412.5567,
2014.
K.	He, Y. Wang, and J. Hopcroft. A powerfUl generative model Using random weights for the deep image
representation. arXiv preprint arXiv:1606.04801, 2016.
G.	Hinton, L. Deng, D. YU, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. VanhoUcke, P. NgUyen, T. N.
Sainath, et al. Deep neUral networks for acoUstic modeling in speech recognition: The shared views of foUr
research groUps. IEEE Signal Processing Magazine, 29(6):82-97, 2012.
K. Jarrett, K. KavUkcUoglU, M. Ranzato, and Y. LeCUn. What is the best mUlti-stage architectUre for object
recognition? In ICCV, pages 2146-2153, 2009.
Y. Jia, E. Shelhamer, J. DonahUe, S. Karayev, J. Long, R. Girshick, S. GUadarrama, and T. Darrell. Caffe:
ConvolUtional architectUre for fast featUre embedding. arXiv:1408.5093, 2014.
A. Krizhevsky, I. SUtskever, and G. E. Hinton. Imagenet classification with deep convolUtional neUral networks.
In NIPS, pages 1097-1105, 2012.
Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng. BUilding high-level
featUres Using large scale UnsUpervised learning. In ICML, 2013.
Y. LeCUn, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. HUbbard, and L. D. Jackel. Backpropagation
applied to handwritten zip code recognition. Neural computation, 1(4):541-551, 1989.
H.	Lee, C. Ekanadham, and A. Y. Ng. Sparse deep belief net model for visUal area v2. In NIPS, 2008.
S.	Mallat and Z. Zhang. Matching pUrsUits with time-freqUency dictionaries. IEEE Transactions on Signal
Processing, 41:3397 - 3415, 1993.
T.	Mikolov, I. SUtskever, K. Chen, G. S. Corrado, and J. Dean. DistribUted representations of words and phrases
and their compositionality. In Advances in neural information processing systems, pages 3111-3119, 2013.
B. A. OlshaUsen et al. Emergence of simple-cell receptive field properties by learning a sparse code for natUral
images. Nature, 381(6583):607-609, 1996.
J. Y. Park, H. L. Yap, C. Rozell, and M. B. Wakin. Concentration of MeasUre for Block Diagonal Matrices With
Applications to Compressive Signal Processing. IEEE Transactions on Signal Processing, 59(12):5859-5875,
2011.
A. PaUl and S. VenkatasUbramanian. Why does Deep Learning work? - A perspective from GroUp Theory.
arXiv.org, Dec. 2014.
M. A. Ranzato, F. J. HUang, Y.-L. BoUreaU, and Y. LeCUn. UnsUpervised learning of invariant featUre hierarchies
with applications to object recognition. In CVPR, 2007.
A. Saxe, P. W. Koh, Z. Chen, M. Bhand, B. SUresh, and A. Y. Ng. On random weights and UnsUpervised featUre
learning. In ICML, pages 1089-1096, 2011.
J.	SchmidhUber. Deep learning in neUral networks: An overview. Neural Networks, 2015.
W. Shang, K. Sohn, D. Almeida, and H. Lee. Understanding and improving convolUtional neUral networks via
concatenated rectified linear Units. In ICML, 2016.
K.	Simonyan and A. Zisserman. Very deep convolUtional networks for large-scale image recognition. In ICLR,
2015.
C. Szegedy, W. LiU, Y. Jia, P. Sermanet, S. Reed, D. AngUelov, D. Erhan, V. VanhoUcke, and A. Rabinovich.
Going deeper with convolUtions. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1-9, 2015.
A. Vedaldi and K. Lenc. Matconvnet - convolUtional neUral networks for matlab. In Proceeding of the ACM Int.
Conf. on Multimedia, 2015.
R. Vershynin. IntrodUction to the non-asymptotic analysis of random matrices. arXiv.org, Nov. 2010.
J. Yang, J. Wright, T. S. HUang, and Y. Ma. Image sUper-resolUtion via sparse representation. Image Processing,
IEEE Transactions on, 19(11):2861-2873, 2010.
Y. Zhang, K. Lee, and H. Lee. AUgmenting neUral networks with reconstrUctive decoding pathways for
large-scale image classification. In ICML, 2016.
J. Zhao, M. MathieU, R. Goroshin, and Y. LecUn. Stacked what-where aUto-encoders. arXiv:1506.02351, 2016.
12
Under review as a conference paper at ICLR 2017
Supplementary Materials:
Towards Understanding the Invertibility of
Convolutional Neural Networks
A Mathematical Analysis: Model-RIP and random filters
Theorem 3.1(Restated) Assume that we have MK vectors wi,m of length ` in which each entry is
a scaled i.i.d. (sub-)Gaussian random variable with mean zero and variance 1 (the scaling factor
is 1/a/M'). Let t be the Stride length (where n = (D 一 ')/t + 1) and build the structured random
matrix W as the weight matrix in a single layer CNN for M -channel input dimension D. If
M' 、 C" ”八」、	」〕
-ɪ ≥ 碌(log(K) + log(n) — log(e)j,
then, with probability 1 一 , the MD × Kn matrix WT satisfies the model-RIP for model Mk with
parameter δk.
Proof. We note that this result follows the same structure of that for many proofs of the RIP for
(structured) random matrices (see Park et al. (2011); Vershynin (2010) for details) although we make
minor tweaks to account for the particular structure of WT.
Suppose that z ∈ Mk which means that z consists of at most k non-zero entries that each appear in
a distinct block of size n (there are a total of K blocks). First, we observe that the norm of WTz is
preserved in expectation.
Lemma A.1.
E(kWTzk22)=kzk22
Proof. Note that each entry of WT is either zero or Gaussian random variable W 〜 N(0,1)
(suitably normalized). Therefore, it is obvious that E(WWT) = I since each row of W satisfies
E	wij1m
i,m1
that E
1 for all j . Finally, we have
0 ifj1 6= j2 or m1 6= m2, and we normalized the random variables so
E (k WTz∣∣2) = E (ZTWWTZ) = ZTE (WWT) Z = ZTZ = ∣∣zk2.
□
Let y = WTz . We aim to show that the square norm of the random variable ky k22 concentrates
tightly about its mean; i.e., with exceedingly low probability
∣y∣22 一 ∣Z ∣22 > δ∣Z ∣22 .
To do so, we need several properties of sub-Gaussian and sub-exponential random variables. A
mean-zero sub-Gaussian random variable Z has a moment generating function that satisfies
E(exp(tZ)) ≤ exp(t2C2)
for all t ∈ R and some constant C. The sub-Gaussian norm of Z, denoted ∣Z ∣ψ2 is
∣Z ∣ψ2
p≥p √⅛(E|Z1 厂
If Z 〜N(0, σ2), then ∣∣Z∣∣ψ2 ≤ cσ.
A sub-exponential random variable X satisfies14
P |X| > t ≤ exp(1 一 t/C)
14There are two other equivalent properties. See Vershynin (2010) for details.
13
Under review as a conference paper at ICLR 2017
for all t ≥ 0.
Let yi denote the ith entry of the vector y = WT z. We can write
Kn
yi = X WiT,jzj
j=1
and observe that yi is a linear combination of i.i.d. sub-Gaussian random variables (or it is identically
equal to 0) and, as such, is itself a SUb-GaUssian random variable with mean zero and SUb-GaUssian
norm Ilyikψ⅛ ≤ C∕√M'∣∣wkψ2 ∣∣zk2 (see Vershynin (2010), Lemma 5.9). The structure of the
random matrix and how many non-zero entries are in row i of W do enter the more refined boUnd on
the sub-Gaussian norm of kyikψ2 (again, see Vershynin (2010), Lemma 5.9 for details) but we ignore
such details for this estimate as they are not necessary for the next estimate.
To obtain a concentration bound for kyik22, we recall from Park et al. (2011); Vershynin (2010) that
the sum of squares of sub-Gaussian random variables tightly concentrate.
Theorem A.2. Let Y1, . . . , YMD be independent sub-Gaussian random variables with sub-Gaussian
norms kYikψ2 for all i = 1, . . . , MD. Let T = maxi kYi kψ2. The for every t ≥ 0 and every
a∈RMD,
MD
X ai (Yi - EYi2) ≥ t
i=1
≤ 2 exp
We note that although some entries yi may be identically zero, depending on the sparsity pattern of
z, not all entries are. Let us define 欧i= ∣yyi^- so that ki∕ikψ2 = 1 and observe that
MD
P(Ikyk2 -kz∣2∣ >δ∣zk2) = P(IXkyikψ2(y2 -Ey2)∣ >δ∣∣zk2)
We apply Theorem A.2 to the sub-Gaussian random variables ryi with the weights ∣∣yikψ2. We have
ll ll2 MD11 ll4 NCDkwkψjzk4 a l, l, NCkwkψjzk2
kak2 = TkyikΨ2 ≤ ——Mf- and kak∞ ≤ —M—.
i=1
If we set T = 1, t = δkzk22, and use the above estimates for the norms of a, we have
Pakyk2 -kzk2∣ > δkzk2) ≤2eχp( - C min( Cδ2M2, CwM-)).⑷
Finally, we use the concentration of measure result in a crude union bound to bound the failure
probability over all vectors z ∈ Mk. We take nk Kk ≈ (nK)k and for a desired constant failure
probability. Using the smaller term in Equation (4), (note that δ < 1, -∕D < 1, and kwkψ2 ≥ 1) we
have
M -2δ2
exp — C E ∣∣4 exp k(log(K) + log(n)) ≤ exp(log(e))
Dkwk4ψ2
which implies
M -2	k	k
-D- ≥ δ2kwkψ2 ( log(K) + log(n) — log(e) ) = °辟(log(K) + log(n) — log(e)).
Therefore, if design our matrix W as described and with the parameter relationship as above, the
matrix WT with satisfy the model-RIP for Mk and parameter δ with probability 1 — e.	□
Let us discuss the relationship amongst the parameters in our result. First, if we have only one channel
M = 1 and the filter length - = D, then our bound on the number of measurements D matches those
of traditional (model-based) compressive sensing; namely,
D ≥ cδ2 (Iog(K) + log(n) — log(e)).
If ' < D (i.e., the filters are much shorter than the length of the input signal as in a CNN), then We
can compensate by adding more channels; i.e., the filter length ' needs to be larger than √D, or, if
add more channels, /D∕M.
14
Under review as a conference paper at ICLR 2017
B Mathematical Analysis: Reconstruction B ounds
The consequences of having model-RIP are two-fold. The first is that if we assume that an input
image is the structured sparse linear combination of filters, x = WTz (where z ∈ Mk and WT
satisfies the model-RIP property), then we know an upper and lower bound on the norm of x in terms
of the norm of its sparse coefficients, kxk2 ≤ (1 ± δ)kzk2. Additionally,
kzk2 ≤
kxk2.
More importantly, when we calculate the hidden units of x,
h= ReLU(W x) =ReLU(WWTz)
we can see that the computation of h is nothing other than the first step of a reconstruction algorithm
analogous to that of model-based compressed sensing. As a result, we have a bound on the error
between h and z and we see that we can analyze the approximation properties of a feedfoward CNN
and its linear reconstruction algorithm. In particular, we can conclude that a feedforward CNN and a
linear reconstruction algorithm provide a good approximation to the original input image.
Theorem 3.3(Restated) We assume that WT satisfies the M2k -RIP with constant δk ≤ δ2k < 1. If
we use W in a single layer CNN both to compute the hidden units Z and to reconstruct the input X
from these hidden units as X so that X = WTM( Wx, k), the error in our reconstruction is
kx - xk2 ≤
5δ2k √1+ δ2k
1 - δk √1 — δ2k
kXk2.
Proof. To show this result, we recall the two following lemmas from Baraniuk et al. (2010) and
rephrase them in the setting of a feedforward CNN.
Lemma B.1. Suppose WT has Mk-RIP with constant δk. Let Ω be a support corresponding to a
subspace in Mk. Then we have the following bounds:
kWΩxk2 ≤ √1 + δkkxk2	(5)
∣∣WωWTzk2 ≤ (1 + δk)kzk2	(6)
∣∣WωWTzk2 ≥ (1 — δk)kzk2	(7)
Lemma B.2. Suppose that WT has Mk-RIP with constant δ2k. Let Ω be a SUPPOrt COrreSPOnding
to a subspace of Mk and suppose that Z ∈ Mk (not necessarily SUPPOrted on Ω). Then
k wωw T zhc k2 ≤ δ2k Ilzhc ∣∣2∙
Let Π denote the support of the Mk sparse vector z. Set h = Wx and set Z to be the result of max
pooling applied to the vector h, or the best fit (with respect to the `2 norm) to h in the model Mk.
Let Ω denote the support set of Z ∈ Mk. For simplicity, We assume ∣Π∣ = k = ∣Ω∣.
Lemma B.3 (Identification). The support set, Ω, ofthe switch units captures a significant fraction of
the total energy in the coefficient vector Z
kzlΩc k2 ≤
2δ2k
1 — δk
kZk2.
Proof. Let h∙Ω and h∏ be the vector h restricted to the support sets Ω and Π, respectively. Since
both are support sets for Mk and since Ω is the best support set for h,
kh — hΩk2 ≤ kh — hΠk2,
and, after several calculations, We have
khlΩ∖Πk2 ≥ khl∏∖Ωk2.
15
Under review as a conference paper at ICLR 2017
Using Lemma B.2 and the size | (Ω \ Π) U Π∣ ≤ 2k, We have
∣∣hΩ∖Πk2 = kWΩ∖ΠW T zk2 ≤ δ2k ∣∣z∣∣2∙
We can bound the other side of the inequality as
∣∣hΠ∖Ωk2 ≥ kWΠ∖Ω(WTzl∏∖Ω)∣2 - kWΠ∖Ω(WTZh)Il2
≥ (I- δk)kzl∏∖Ωk2 - δ2kkz∣Ω∣∣2 .
Since the support of Z is the set Π, Π \ Ω = Ωc and we can conclude that
δ2kkz∣∣2 ≥ (1 - δk )∣z∣Ωc ∣∣2 - δ2k kz∣Ω∣∣2,
and with some rearrangement, we have
llzlΩc ∣∣2 ≤
2δ2k
1 - δk
∣Z∣2.
□
To set the value of Z on its support set Ω, we simply set Z = h∣Ω and Z|q。= 0. Then
Lemma B.4 (Estimation).
kz - zk2 ≤
5δ2k
1 - δk
∣Z∣2
Proof. First, note that 11 — WcWT∣∣2 ≤ δk since
(I- δk) ≤ kZUPo jtZ⅛γ(= σmax(WT)=σmax(WQWT))≤ (ι+δk),
where σmax is the maximum singular value. Therefore,
∣z - Z∣2 ≤ ∣Z∣ΩC ∣2 + ∣Z∣Ω - Z∣Ω∣2
=∣Z∣Ωc ∣2 + ∣Z∣Ω - Wω(WTZ∣Ω + WTZ∣Ωc)∣2
≤ ∣Z∣ΩC ∣2 + ∣(I - WωWT )z∣ω∣2 + ∣WωW TZhC ∣2
≤ ∣Z∣ΩC ∣2 + ∣I - WωWΩ ∣2∣Z∣ω∣2 + δ2k ∣Z∣Ωc ∣2
≤ ∣Z∣Ωc ∣2 + δk ∣Z∣ω∣2 + δ2k∣Z∣Ωc ∣2
≤ ((1 + δ2k) 1 -2δk + δk)惊心
≤
5δ2k
1 - δk
∣Z∣2.
□
Finally, if we use the autoencoder formulation to reconstruct the original image X by setting X =
W T Z, we can estimate the reconstruction error. We note that Z is Mk-SParSe by construction and
remind the reader that WT satisfies M2k-model-RIP with constants δk ≤ δ2k	1. Then, using
Lemma B.4 as well as the M2k-sparse properties of WT,
IlX - x∣∣2 = IlWT(z - Z)∣∣2 ≤ √i+^2k∣z - z∣2
≤
5δ2k
1 - δk
√i+^2k ∣z∣2
≤
5δ2k √1+ δ2k
1 - δk √1 - δ2k
∣X∣2.
This proves that a feedforward CNN with a linear reconstruction algorithm is an approximate
autoencoder and bounds the reconstruction error of the input image in terms of the geometric
properties of the filters.	□
16
Under review as a conference paper at ICLR 2017
C More experimental results
C.1 More details on evaluation of CNNs with Gaussian random filters
In this section, we provide more details on the network architectures that we used in Table 1. In
particular, we describe the best performing architectures for all cases in Table 5.
Method	#LayerS	1 layers	2 layers	3 layers
Random filters	Best param.	(2048)5c-2pmaχ-4pave	(2048)3c-2pmax-- (2048)3c-2pmax -2pave	(2048)3c-2pmx-(2048)3c- 2pmx-(1024)3c-2pmx
	Accuracy	66.5%	74.6%	74.8%
Learned filters	Best param.	(1024)5c-2pmaχ-4pave	(1024)3c-2pmax-- (1024)3c-2pmax -2pave	(1024)3c-2pmx-(1024)3c- 2pmx-(1024)3c-2pmx
	Accuracy	68.1%	83.3%	89.3%	一
Table 5: Best-performing architecture and classification accuracy of random CNNs on CIFAR-10. “([n])[k]c”
denotes a convolution layer with a stride 1, a kernel size [k] and [n] output channels, “[k]pmax” denotes a max
pooling layer with a kernel size [k] and a stride [k], and “[k]pave” denotes a average pooling layer. A typical layer
consists of four operations, namely convolution, ReLU, batch normalization, and max pooling.
C.2 Layer-wise coherence and sparsity for AlexNet
We present coherence (see Table 6) and sparsity level (see Table 7) for each layer in AlexNet.
layer	1	2	3	4	5
coherence of learned filters	0.9172	0.6643	0.6200	0.6382	0.3390
coherence of random filters	0.1996	0.1263	0.0929	0.1073	0.1026
Table 6: Comparison of coherence between learned filters in each layer of AlexNet and Gaussian random filters
with corresponding sizes.
layer	conv1	pool1	conv2	pool2	conv3	conv4	conv5	pool5
% of non-zeros	49.41	87.79	18.97	44.13	31.08	30.95	9.78	28.15
Table 7: Layer-wise sparsity of AlexNet on ILSVRC-2012 validation set.
C.3 Visualization of image reconstruction for VGGNet
In Figure 5, we show reconstructed images from each layer using different reconstruction methods
via a pretrained decoding network.
17
Under review as a conference paper at ICLR 2017
Figure 5: Visualization of images reconstructed by a pretrained decoding network with VGGNet’s pool(4)
activation reconstructed using different methods: (a) original image, (b) output of the 5-layer decoding network
with original activation, (c) output of the decoding net with reconstructed activation by IHT with learned filters,
(d) output of the decoding net with reconstructed activation by IHT with Gaussian random filters, (e) output of
the decoding net with Gaussian random activation.
18