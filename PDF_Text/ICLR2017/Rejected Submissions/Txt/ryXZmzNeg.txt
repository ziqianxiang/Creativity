Under review as a conference paper at ICLR 2017
Improving Sampling from Generative
Autoencoders with Markov Chains
Antonia Creswell, Kai Arulkumaran & Anil A. Bharath
Department of Bioengineering
Imperial College London
London SW7 2BP, UK
{ac2211,ka709,aab01}@ic.ac.uk
Ab stract
We focus on generative autoencoders, such as variational or adversarial autoen-
coders, which jointly learn a generative model alongside an inference model. Gen-
erative autoencoders are those which are trained to softly enforce a prior on the
latent distribution learned by the inference model. We call the distribution to
which the inference model maps observed samples, the learned latent distribu-
tion, which may not be consistent with the prior. We formulate a Markov chain
Monte Carlo (MCMC) sampling process, equivalent to iteratively decoding and
encoding, which allows us to sample from the learned latent distribution. Since,
the generative model learns to map from the learned latent distribution, rather
than the prior, we may use MCMC to improve the quality of samples drawn from
the generative model, especially when the learned latent distribution is far from
the prior. Using MCMC sampling, we are able to reveal previously unseen differ-
ences between generative autoencoders trained either with or without a denoising
criterion.
1	Introduction
Unsupervised learning has benefited greatly from the introduction of deep generative models. In
particular, the introduction of generative adversarial networks (GANs) (Goodfellow et al., 2014)
and variational autoencoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014) has led to a
plethora of research into learning latent variable models that are capable of generating data from
complex distributions, including the space of natural images (Radford et al., 2015). Both of these
models, and their extensions, operate by placing a prior distribution, P (Z), over a latent space
Z ⊆ Rb, and learn mappings from the latent space, Z, to the space of the observed data, X ⊆ Ra.
We are interested in autoencoding generative models, models which learn not just the generative
mapping Z 7→ X, but also the inferential mapping X 7→ Z. Specifically, we define generative
autoencoders as autoencoders which softly constrain their latent distribution, to match a specified
prior distribution, P (Z). This is achieved by minimising a loss, Lprior, between the latent distribu-
tion and the prior. This includes VAEs (Kingma & Welling, 2014; Rezende et al., 2014), extensions
of VAEs (Kingma et al., 2016), and also adversarial autoencoders (AAEs) (Makhzani et al., 2015).
Whilst other autoencoders also learn an encoding function, e : Ra → Z , together with a decoding
function, d : Rb → X, the latent space is not necessarily constrained to conform to a specified
probability distribution. This is the key distinction for generative autoencoders; both e and d can
still be deterministic functions (Makhzani et al., 2015).
The functions e and d are defined for any input from Ra and Rb respectively, however the outputs
of the functions may be constrained practically by the type of functions that e and d are, such that e
maps to Z ⊆ Rb and d maps to X ⊆ Ra . During training however, the encoder, e is only fed with
training data samples, x ∈ X and the decoder, d is only fed with samples from the encoder, z ∈ Z,
and so the encoder and decoder learn mappings between X and Z .
The process of encoding and decoding may be interpreted as sampling the conditional probabilities
Qφ(Z|X) and Pθ(X|Z) respectively. The conditional distributions may be sampled using the en-
coding and decoding functions e(X; φ) and d(Z; θ), where φ and θ are learned parameters of the
1
Under review as a conference paper at ICLR 2017
encoding and decoding functions respectively. The decoder of a generative autoencoder may be
used to generate new samples that are consistent with the data. There are two traditional approaches
for sampling generative autoencoders:
Approach 1 (Bengio et al., 2014):
X0 〜P(X),	Z0 〜Qφ(Z|X = X0),	Xi 〜Pθ(X|Z = z0)
where P (X ) is the data generating distribution. However, this approach is likely to generate samples
similar to those in the training data, rather than generating novel samples that are consistent with the
training data.
Approach 2 (Kingma & Welling, 2014; Makhzani et al., 2015; Rezende et al., 2014):
zo 〜P(Z),	X0 〜Pθ(X|Z = zo)
where P(Z) is the prior distribution enforced during training and Pθ(X|Z) is the decoder trained to
map samples drawn from Qφ(Z|X) to samples consistent with P(X). This approach assumes that
/ Qφ(Z|X)P(X)dX = P(Z), suggesting that the encoder maps all data samples from P(X)
to a distribution that matches the prior distribution, P(Z). However, it is not always true that
/ Qφ(Z|X)P(X)dX = P(Z). Rather Qφ(Z|X) maps data samples to a distribution which We
ʌ ,
call, P(Z):
1Qφ(Z∣X)P(X)dX = P(Z)
where it is not necessarily true that P(Z) = P(Z) because the prior is only softly enforced.
The decoder, on the other hand, is trained to map encoded data samples (i.e. samples from
/ Qφ(Z|X)P(X)dX) to samples from X which have the distribution P(X). If the encoder maps
observed samples to latent samples with the distribution P(Z), rather than the desired prior distri-
bution, P(Z), then:
/ Pθ(X∣Z)P(Z)dZ = P (X)
This suggests that samples drawn from the decoder, Pθ(X|Z), conditioned on samples drawn from
the prior, P(Z), may not be consistent with the data generating distribution, P(X). However, by
conditioning on P(Z):
/ Pθ (X|Z)P(Z)dZ = P (X)
This suggests that to obtain more realistic generations, latent samples should be drawn via z 〜 P(Z)
rather than Z 〜 P(Z), followed by X 〜 Pθ(X|Z). A limited number of latent samples may be
drawn from P(Z) using the first two steps in Approach 1 - however this has the drawbacks discussed
in Approach 1. We introduce an alternative method for sampling from P(Z) which does not have
the same drawbacks.
Our main contribution is the formulation of a Markov chain Monte Carlo (MCMC) sampling process
for generative autoencoders, which allows us to sample from P(Z). By iteratively sampling the
chain, starting from an arbitrary zt=o ∈ Rb, the chain converges to zt→∞ 〜P(Z), allowing
us to draw latent samples from P(Z) after several steps of MCMC sampling. From a practical
perspective, this is achieved by iteratively decoding and encoding, which may be easily applied to
existing generative autoencoders. Because P(Z) is optimised to be close to P(Z), the initial sample,
zt=0 can be drawn from P(Z), improving the quality of the samples within a few iterations.
When interpolating between latent encodings, there is no guarantee that z stays within high density
regions of P(Z). Previously, this has been addressed by using spherical, rather than linear interpo-
lation of the high dimensional Z space (White, 2016). However, this approach attempts to keep z
2
Under review as a conference paper at ICLR 2017
Figure 1: P (X) is the data generating distribution. We may access some samples from P (X) by
drawing samples from the training data. Qφ (Z|X) is the conditional distribution, modeled by an
encoder, which maps samples from Ra to samples in Rb. An ideal encoder maps samples from
P (X) to a known, prior distribution P (Z): in reality the encoder maps samples from P (X) to an
unknown distribution P(Z). P0(X|Z) is a conditional distribution, modeled by a decoder, which
maps samples from Rb to Ra. During training the decoder learns to map samples drawn from P(Z)
to P(X) rather than samples drawn from P(Z) because the decoder only sees samples from P(Z).
Regularisation on the latent space only encourages P(Z) to be close to P(Z). Note that if Lprior is
optimal, then P(Z) overlaps fully with P(Z).
(a) VAE (initial)
(b) VAE (5 steps)
(c) VAE (initial)
(d) VAE (5 steps)
Figure 2: Prior work: Spherically interpolating (White, 2016) between two faces using a VAE (a,
c). In (a), the attempt to gradually generate sunglasses results in visual artifacts around the eyes.
In (c), the model fails to properly capture the desired change in orientation of the face, resulting
in three partial faces in the middle of the interpolation. This work: (b) and (d) are the result of
5 steps of MCMC sampling applied to the latent samples that were used to generate the original
interpolations, (a) and (c). In (b), the discolouration around the eyes disappears, with the model
settling on either generating or not generating glasses. In (d), the model moves away from multiple
faces in the interpolation by producing new faces with appropriate orientations.
3
Under review as a conference paper at ICLR 2017
within P(Z), rather than trying to sample from P(Z). By instead applying several steps of MCMC
sampling to the interpolated Z samples before sampling Pθ(X|Z), unrealistic artifacts can be re-
duced (see Figure 2). Whilst most methods that aim to generate realistic samples from X rely on
adjusting encodings of the observed data (White, 2016), our use of MCMC allows us to walk any
latent sample to more probable regions of the learned latent distribution, resulting in more convinc-
ing generations. We demonstrate that the use of MCMC sampling improves generations from both
VAEs and AAEs with high-dimensional Z; this is important as previous studies have shown that the
dimensionality of Z should be scaled with the intrinsic latent dimensionality of the observed data.
Our second contribution is the modification of the proposed transition operator for the MCMC sam-
pling process to denoising generative autoencoders. These are generative autoencoders trained us-
ing a denoising criterion, (Seung, 1997; Vincent et al., 2008). We reformulate our original MCMC
sampling process to incorporate the noising and denoising processes, allowing us to use MCMC
sampling on denoising generative autoencoders. We apply this sampling technique to two models.
The first is the denoising VAE (DVAE) introduced by Im et al. (2015). We found that MCMC sam-
pling revealed benefits of the denoising criterion. The second model is a denoising AAE (DAAE),
constructed by applying the denoising criterion to the AAE. There were no modifications to the cost
function. For both the DVAE and the DAAE, the effects of the denoising crtierion were not immedi-
ately obvious from the initial samples. Training generative autoencoders with a denoising criterion
reduced visual artefacts found both in generations and in interpolations. The effect of the denoising
criterion was revealed when sampling the denoising models using MCMC sampling.
2	Background
One of the main tasks in machine learning is to learn explanatory factors for observed data,
commonly known as inference. That is, given a data sample x ∈ X ⊆ Ra , we would like
to find a corresponding latent encoding z ∈ Z ⊆ Rb. Another task is to learn the inverse,
generative mapping from a given z to a corresponding x. In general, coming up with a suit-
able criterion for learning these mappings is difficult. Autoencoders solve both tasks efficiently
by jointly learning an inferential mapping e(X; φ) and generative mapping d(Z; θ), using unla-
belled data from X in a self-supervised fashion (Kingma & Welling, 2014). The basic objec-
tive of all autoencoders is to minimise a reconstruction cost, Lreconstruct , between the original
data, X, and its reconstruction, d(e(X; φ); θ). Examples of Lreconstruct include the squared error
loss, 1 PnN=I ∣∣d(e(xn; φ); θ) - Xnk2, and the cross-entropy loss, H[P(X)∣∣P(d(e(X; φ); θ))]=
-PnN=IXn log(d(e(xn; φ); θ)) + (1 - Xn) log(1 - d(e(xn； φ); θ)).
Autoencoders may be cast into a probablistic framework, by considering samples X 〜 P (X) and
z 〜P(Z), and attempting to learn the conditional distributions Qφ(Z∣X) and P§(X|Z) as e(X; φ)
and d(Z; θ) respectively, with Lreconstruct representing the negative log-likelihood of the recon-
struction given the encoding (Bengio, 2009). With any autoencoder, it is possible to create novel
x ∈ X by passing a z ∈ Z through d(Z; θ), but we have no knowledge of appropriate choices of z
beyond those obtained via e(X; φ). One solution is to constrain the latent space to which the encod-
ing model maps observed samples. This can be achieved by an additional loss, Lprior, that penalises
encodings far away from a specified prior distribution, P(Z). We now review two types of gener-
ative autoencoders, VAEs (Kingma & Welling, 2014; Rezende et al., 2014) and AAEs (Makhzani
et al., 2015), which each take different approaches to formulating Lprior.
2.1	Generative autoencoders
Consider the case where e is constructed with stochastic neurons that can produce outputs from a
specified probability distribution, and Lprior is used to constrain the distribution of outputs to P(Z).
This leaves the problem of estimating the gradient of the autoencoder over the expectation Eqφ(z∣x),
which would typically be addressed with a Monte Carlo method. VAEs sidestep this by constructing
latent samples using a deterministic function and a source of noise, moving the source of stochas-
ticity to an input, and leaving the network itself deterministic for standard gradient calculations—a
technique commonly known as the reparameterisation trick (Kingma & Welling, 2014). e(X; φ)
then consists ofa deterministic function, erep (X; φ), that outputs parameters for a probability distri-
bution, plus a source of noise. In the case where P(Z) is a diagonal covariance Gaussian, erep(X; φ)
4
Under review as a conference paper at ICLR 2017
Figure 3: Reconstructions of faces from a DVAE trained with additive Gaussian noise: Q(X|X) =
N(X, 0.25I). The model successfully recovers much of the detail from the noise-corrupted images.
maps X to a vector of means, μ ∈ Rb, and a vector of standard deviations, σ ∈ R[, with the noise
E 〜N(0, I). Put together, the encoder outputs samples Z = μ + E Θ σ, where Θ is the Hadamard
product. VAEs attempt to make these samples from the encoder match up with P (Z) by using the
KL divergence between the parameters for a probability distribution outputted by erep(X; φ), and
the parameters for the prior distribution, giving Lprior = Dkl[Qφ(ZX)∣∣P(Z)]. A multivariate
Gaussian has an analytical KL divergence that can be further simplified when considering the unit
Gaussian, resulting in Lprior = 1 PnN=I μ2 + σ2 一 log(σ2) 一 1.
Another approach is to deterministically output the encodings z. Rather than minimising a met-
ric between probability distributions using their parameters, we can turn this into a density ratio
estimation problem where the goal is to learn a conditional distribution, Qφ(Z∣X), such that the
distribution of the encoded data samples, P(Z) = J Qφ(Z|X)P(X)dX, matches the prior distri-
bution, P (Z). The GAN framework solves this density ratio estimation problem by transforming it
into a class estimation problem using two networks (Goodfellow et al., 2014). The first network in
GAN training is the discriminator network, Dψ , which is trained to maximise the log probability of
samples from the “real” distribution, Z 〜P(Z), and minimise the log probability of samples from
the “fake” distribution, Z 〜Qφ(Z|X). In our case e(X; φ) plays the role of the second network,
the generator network, Gφ, which generates the “fake” samples.1 The two networks compete in a
minimax game, where Gφ receives gradients from Dψ such that it learns to better fool Dψ . The
training objective for both networks is given by Lprior = argminφ argmaxψ EP (Z) [log(Dψ(Z))] +
EP(X)[log(1 一 Dψ(Gφ(X)))] = argmin° argmaxψ EP(z)[log(Dψ(Z))] + Eq、(z|x)p(x) log[1 一
Dψ(Z)]. This formulation can create problems during training, so instead Gφ is trained to minimise
一 log(Dψ (Gφ(X))), which provides the same fixed point of the dynamics of Gφ and Dψ. The
result of applying the GAN framework to the encoder of an autoencoder is the deterministic AAE
(Makhzani et al., 2015).
2.2	Denoising autoencoders
In a more general viewpoint, generative autoencoders fulfill the purpose of learning useful repre-
sentations of the observed data. Another widely used class of autoencoders that achieve this are
denoising autoencoders (DAEs), which are motivated by the idea that learned features should be
robust to “partial destruction of the input” (Vincent et al., 2008). Not only does this require en-
coding the inputs, but capturing the statistical dependencies between the inputs so that corrupted
data can be recovered (see Figure 3). DAEs are presented with a corrupted version of the input,
x ∈ X, but must still reconstruct the original input, X ∈ X, where the noisy inputs are cre-
ated through sampling X 〜C(X|X), a corruption process. The denoising criterion, Ldenoise,
can be applied to any type of autoencoder by replacing the straightforward reconstruction cri-
terion, Lreconstruct (X, d(e(X; φ); θ)), with the reconstruction criterion applied to noisy inputs:
Lreconstruct(X, d(e(X; φ); θ)). The encoder is now used to model samples drawn from Qφ(Z|X).
As such, we can construct denoising generative autoencoders by training autoencoders to minimise
+ Lprior .
Ldenoise
One might expect to see differences in samples drawn from denoising generative autoencoders and
their non-denoising counterparts. However, Figures 4 and 6 show that this is not the case. Im et al.
1We adapt the variables to better fit the conventions used in the context of autoencoders.
5
Under review as a conference paper at ICLR 2017
(2015) address the case of DVAEs, claiming that the noise mapping requires adjusting the original
VAE objective function. Our work is orthogonal to theirs, and others which adjust the training or
model (Kingma et al., 2016), as we focus purely on sampling from generative autoencoders after
training. We claim that the existing practice of drawing samples from generative autoencoders
conditioned on Z 〜 P (Z) is SUboPtimaL and the quality of samples can be improved by instead
conditioning on Z 〜 P(Z) Via MCMC sampling.
3	Markov sampling
We now consider the case of sampling from generative autoencoders, where d(Z; θ) is used to draw
samples from Pθ(X|Z). In Section 1, we showed that it was important, when sampling Pθ(X|Z),
to condition on Z’s drawn from P(Z), rather than P(Z) as is often done in practice. However, we
now show that for any initial Z0 ∈ Z0 = Rb, Markov sampling can be used to produce a chain of
samples Zt, such that as t → ∞, produces samples Zt that are from the distribution P(Z), which
may be used to draw meaningful samples from Pθ(X|Z), conditioned on Z 〜P(Z). To speed up
convergence We can initialise zo from a distribution close to P(Z), by drawing zo 〜P(Z).
3.1	Markov sampling process
A generative autoencoder can be sampled by the following process:
z0 ∈ ZO = Rb,	xt+1 〜Pθ(X|Zt),	zt+1 〜Qφ(ZIXt+1)
This allows us to define a Markov chain with the transition operator
T(Zt+1IZt)=	Qφ(Zt+1IX)Pθ(XIZt)dX	(1)
for t ≥ 0.
Drawing samples according to the transition operator T (Zt+1 IZt) produces a Markov chain. For the
transition operator to be homogeneous, the parameters of the encoding and decoding functions are
fixed during sampling.
3.2	Convergence properties
We now show that the stationary distribution of sampling from the Markov chain is P(Z).
Theorem 1. If T (Zt+1 IZt) defines an ergodic Markov chain, {Z1, Z2...Zt}, then the chain will
converge to a stationary distribution, Π(Z), from any arbitrary initial distribution. The stationary
distribution Π(Z) = P(Z).
The proof of Theorem 1 can be found in (Rosenthal, 2001).
Lemma 1. T(Zt+1IZt) defines an ergodic Markov chain.
Proof. For a Markov chain to be ergodic it must be both irreducible (it is possible to get from any
state to any other state in a finite number of steps) and aperiodic (it is possible to get from any state
to any other state without having to pass through a cycle). To satisfy these requirements, it is more
than sufficient to show that T (Zt+1 IZt) > 0, since every Z ∈ Z would be reachable from every
other z ∈ Z. We show that Pθ(X∣Z) > 0 and Qφ(Z∣X) > 0, giving T(Zt+ι∣Zt) > 0, providing
the proof of this in Section A of the supplementary material.	□
Lemma 2. The stationary distribution of the chain defined by T (Zt+1 IZt) is Π(Z) = P(Z).
Proof. For the transition operator defined in Equation (1), the asymptotic distribution to which
T(Zt+1IZt) converges to is P(Z), because P(Z) is, by definition, the marginal of the joint distribu-
tion Qφ(Z∣X)P(X), over which the Lprior used to learn the conditional distribution Qφ(Z∣X). □
6
Under review as a conference paper at ICLR 2017
Using Lemmas 1 and 2 with Theorem 1, we can say that the Markov chain defined by the transition
operator in Equation (1) will produce a Markov chain that converges to the stationary distribution
ʌ ,
∏(Z) = P(Z).
3.3	Extension to denoising generative autoencoders
A denoising generative autoencoder can be sampled by the following process:
7_	...	. ~ .	~
z0 ∈ Z0 = R ,	xt+1 〜Pθ (X lZt),	xt+1 〜C(X Xt+1),	zt+1 〜Q0(Z|Xt+1).
This allows us to define a Markov chain with the transition operator
C
T(Zt+ι∣Zt) = J Qφ(Zt+ι∣X)C(XX)Pθ(X∣Zt)dXdX	⑵
for t ≥ 0.
The same arguments for the proof of convergence of Equation (1) can be applied to Equation (2).
3.4	Related work
Our work is inspired by that of Bengio et al. (2013); denoising autoencoders are cast into a proba-
bilistic framework, where Pθ(X|X) is the denoιsιng (decoder) distribution and C(X|X) is the Cor-
ruption (encoding) distribution. X represents the space of corrupted samples. Bengio et al. (2013)
define a transition operator of a Markov chain - using these conditional distributions - whose sta-
tionary distribution is P(X) under the assumption that Pθ(X|X) perfectly denoises samples. The
chain is initialised with samples from the training data, and used to generate a chain of samples
from P(X). This work was generalised to include a corruption process that mapped data samples to
latent variables (Bengio et al., 2014), to create a new type of network called Generative Stochastic
Networks (GSNs). However in GSNs (Bengio et al., 2014) the latent space is not regularised with a
prior.
Our work is similar to several approaches proposed by Bengio et al. (2013; 2014) and Rezende et
al. (Rezende et al., 2014). Both Bengio et al. and Rezende et al. define a transition operator in terms
of Xt and Xt-1. Bengio et al. generate samples with an initial X0 drawn from the observed data,
while Rezende et al. reconstruct samples from an X0 which is a corrupted version of a data sample.
In contrasts to Bengio et al. and Rezende et al., in this work we define the transition operator in
terms of Zt+1 and Zt, initialise samples with a Z0 that is drawn from a prior distribution we can
directly sample from, and then sample X1 conditioned on Z0 . Although the initial samples may be
poor, we are likely to generate a novel X1 on the first step of MCMC sampling, which would not be
achieved using Bengio et al.’s or Rezende et al.’s approach. We are able draw initial Z0 from a prior
because we constrain P(Z) to be close to a prior distribution P(Z); in Bengio et al. a latent space
is either not explicitly modeled (Bengio et al., 2013) or it is not constrained (Bengio et al., 2014).
Further, Rezende et al. (2014) explicitly assume that the distribution of latent samples drawn from
Qφ(Z|X) matches the prior, P(Z). Instead, We assume that samples drawn from Qφ(Z|X) have a
distribution P(Z) that does not necessarily match the prior, P(Z). We propose an alternative method
for sampling P(Z) in order to improve the quality of generated image samples. Our motivation is
also different to Rezende et al. (2014) since we use sampling to generate improved, novel data
samples, while they use sampling to denoise corrupted samples.
3.5	Effect of regularisation method
The choice of Lprior may effect how much improvement can be gained when using MCMC sam-
pling, assuming that the optimisation process converges to a reasonable solution. We first consider
the case of VAEs, which minimise Dkl[Qφ(Z|X)[∣P(Z)]. Minimising this KL divergence pe-
nalises the model P(Z) if it contains samples that are outside the support of the true distribution
P(Z), which might mean that P(Z) captures only a part of P(Z). This means that when sampling
7
Under review as a conference paper at ICLR 2017
I -> / Γ7∖	1	i'	∙ .Λ	.	.	1 1 ∙A / Γ7∖ EI ♦	. .1 . > « XΛ» « r Λ	1 ∙
P (Z), we may draw from a region that is not captured by P(Z). This suggests that MCMC sampling
can improve samples from trained VAEs by walking them towards denser regions in P (Z).
Generally speaking, using the reverse KL divergence during training, Dkl[P(Z)kQφ(ZX)], pe-
nalises the model Qφ(Z∣X) if P(Z) produces samples that are outside of the support of P(Z). By
minimising this KL divergence, most samples in P(Z) will likely be in P(Z) as well. AAEs, on the
other hand are regularised using the JS entropy, given by 1 Dkl[P(Z)k 1 (P(Z) + Qφ(ZX))] +
1 Dkl[Qφ(Z∖X)k2(P(Z) + Qφ(Z|X))]. Minimising this cost function attempts to find a com-
promise between the aforementioned extremes. However, this still suggests that some samples from
P(Z) may lie outside P(Z), and so we expect AAEs to also benefit from MCMC sampling.
4	Experiments
4.1	Models
We utilise the deep convolutional GAN (DCGAN) (Radford et al., 2015) as a basis for our autoen-
coder models. Although the recommendations from Radford et al. (2015) are for standard GAN
architectures, we adopt them as sensible defaults for an autoencoder, with our encoder mimicking
the DCGAN’s discriminator, and our decoder mimicking the generator. The encoder uses strided
convolutions rather than max-pooling, and the decoder uses fractionally-strided convolutions rather
than a fixed upsampling. Each convolutional layer is succeeded by spatial batch normalisation (Ioffe
& Szegedy, 2015) and ReLU nonlinearities, except for the top of the decoder which utilises a sig-
moid function to constrain the output values between 0 and 1. We minimise the cross-entropy
between the original and reconstructed images. Although this results in blurry images in regions
which are ambiguous, such as hair detail, we opt not to use extra loss functions that improve the
visual quality of generations (Larsen et al., 2015; Dosovitskiy & Brox, 2016; Lamb et al., 2016) to
avoid confounding our results.
Although the AAE is capable of approximating complex probabilistic posteriors (Makhzani et al.,
2015), we construct ours to output a deterministic Qφ(Z∖X). As such, the final layer of the encoder
part of our AAEs is a convolutional layer that deterministically outputs a latent sample, z. The
adversary is a fully-connected network with dropout and leaky ReLU nonlinearities. erep(X; φ)
of our VAEs have an output of twice the size, which corresponds to the means, μ, and standard
deviations, σ, of a diagonal covariance Gaussian distribution. For all models our prior, P(Z), is a
200D isotropic Gaussian with zero mean and unit variance: N (0, I).
4.2	Datasets
Our primary dataset is the (aligned and cropped) CelebA dataset, which consists of 200,000 images
of celebrities (Liu et al., 2015). The DCGAN (Radford et al., 2015) was the first generative neural
network model to show convincing novel samples from this dataset, and it has been used ever since
as a qualitative benchmark due to the amount and quality of samples. In Figures 7 and 8 of the
supplementary material, we also include results on the SVHN dataset, which consists of 100,000
images of house numbers extracted from Google Street view images (Netzer et al., 2011).
4.3	Training & evaluation
For all datasets we perform the same preprocessing: cropping the centre to create a square image,
then resizing to 64 × 64px. We train our generative autoencoders for 20 epochs on the training
split of the datasets, using Adam (Kingma & Ba, 2014) with α = 0.0002, β1 = 0.5 and β2 =
0.999. The denoising generative autoencoders use the additive Gaussian noise mapping C(X ∖X)=
N(X, 0.25I). All of our experiments were run using the Torch library (Collobert et al., 2011).2
For evaluation, we generate novel samples from the decoder using z initially sampled from P(Z);
we also show spherical interpolations (White, 2016) between four images of the testing split, as
depicted in Figure 2. We then perform several steps of MCMC sampling on the novel samples
and interpolations. During this process, we use the training mode of batch normalisation (Ioffe &
2Example code is available at https://github.com/Kaixhin/Autoencoders.
8
Under review as a conference paper at ICLR 2017
Szegedy, 2015), i.e., we normalise the inputs using minibatch rather than population statistics, as
the normalisation can partially compensate for poor initial inputs (see Figure 4) that are far from
the training distribution. We compare novel samples between all models below, and leave further
interpolation results to Figures 5 and 6 of the supplementary material.
9
Under review as a conference paper at ICLR 2017
4.4 Samples
(a) VAE (initial)	(b) VAE (1 step)	(c) VAE (5 steps)	(d) VAE (10 steps)
(e) DVAE (initial)	(f) DVAE (1 step)	(g) DVAE (5 steps)	(h) DVAE (10 steps)
Figure 4: Samples from a VAE (a-d), DVAE (e-h), AAE (i-l) and DAAE (m-p) trained on the CelebA
dataset. (a), (e), (i) and (m) show initial samples conditioned on Z 〜 P(Z), which mainly result in
recognisable faces emerging from noisy backgrounds. After 1 step of MCMC sampling, the more
unrealistic generations change noticeably, and continue to do so with further steps. On the other
hand, realistic generations, i.e. samples from a region with high probability, do not change as much.
The adversarial criterion for deterministic AAEs is difficult to optimise when the dimensionality of
Z is high. We observe that during training our AAEs and DAAEs, the empirical standard deviation
of z 〜 Qφ(Z|X) is less than 1, which means that P(Z) fails to approximate P(Z) as closely as was
achieved with the VAE and DVAE. However, this means that the effect of MCMC sampling is more
pronounced, with the quality of all samples noticeably improving after a few steps. As a side-effect
of the suboptimal solution learned by the networks, the denoising properties of the DAAE are more
noticeable with the novel samples.
10
Under review as a conference paper at ICLR 2017
5	Conclusion
Autoencoders consist of a decoder, d(Z; θ) and an encoder, e(X; φ) function, where φ and θ are
learned parameters. Functions e(X; φ) and d(Z; θ) may be used to draw samples from the condi-
tional distributions Pθ(X|Z) and Qφ(ZX) (Bengio etal., 2014; 2013; Rezendeetal., 2014), where
X refers to the space of observed samples and Z refers to the space of latent samples. The encoder
distribution, Qφ(Z|X), maps data samples from the data generating distribution, P(X), to a latent
「，♦F	. ∙	τ∖ / Γ7∖ EI 1	1	∙>♦,♦1	. ∙	I 1	/ TT- I Γ7∖	1 Γ∙	τ∖ / Γ7∖	.	7-⅝∕P7∖	5 T
distribution, P(Z). The decoder distribution, Pθ(X|Z), maps samples from P(Z) to P(X). We
are concerned with generative autoencoders, which we define to be a family of autoencoders where
regularisation is used during training to encourage P(Z) to be close to a known prior P(Z). Com-
monly it is assumed that P(Z) and P(Z) are similar, such that samples from P(Z) may be used to
sample a decoder Pθ(X|Z); We do not make the assumption that P(Z) and P(Z) are sufficiently
close” (Rezende et al., 2014). Instead, we derive an MCMC process, whose stationary distribution
is P(Z), allowing us to directly draw samples from P(Z). By conditioning on samples from P(Z),
samples drawn from X 〜 Pθ(X|Z) are more consistent with the training data.
In our experiments, we compare samples X 〜Pθ (X |Z = z0), z0 〜P(Z) to X 〜P§(X|Z = Zi) for
i = {1, 5, 10}, where zi’s are obtained through MCMC sampling, to show that MCMC sampling
improves initially poor samples (see Figure 4). We also show that artifacts in X samples induced
by interpolations across the latent space can also be corrected by MCMC sampling see (Figure
2). We further validate our work by showing that the denoising properties of denoising generative
autoencoders are best revealed by the use of MCMC sampling.
Our MCMC sampling process is straightforward, and can be applied easily to existing generative au-
toencoders. This technique is orthogonal to the use of more powerful posteriors in AAEs (Makhzani
et al., 2015) and VAEs (Kingma et al., 2016), and the combination of both could result in further
improvements in generative modeling. Finally, our basic MCMC process opens the doors to apply a
large existing body of research on sampling methods to generative autoencoders.
Acknowledgements
We would like to acknowledge the EPSRC for funding through a Doctoral Training studentship and
the support of the EPSRC CDT in Neurotechnology.
References
Yoshua Bengio. Learning deep architectures for AI. Foundations and trendsR in Machine Learning, 2(1):
1-127, 2009.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as gener-
ative models. In Advances in Neural Information Processing Systems, pp. 899-907, 2013.
Yoshua Bengio, Eric Thibodeau-Laufer, Guillaume Alain, and Jason Yosinski. Deep generative stochastic
networks trainable by backprop. In Journal of Machine Learning Research: Proceedings of the 31st Inter-
national Conference on Machine Learning, volume 32, 2014.
Ronan Collobert, Koray Kavukcuoglu, and Clement Farabet. Torch7: A matlab-like environment for machine
learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376, 2011.
Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep
networks. arXiv preprint arXiv:1602.02644, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing
Systems, pp. 2672-2680, 2014.
Daniel Jiwoong Im, Sungjin Ahn, Roland Memisevic, and Yoshua Bengio. Denoising criterion for variational
auto-encoding framework. arXiv preprint arXiv:1511.06406, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning (ICML-
15), pp. 448-456, 2015.
11
Under review as a conference paper at ICLR 2017
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 2015
International Conference on Learning Representations (ICLR-2015), arXiv preprint arXiv:1412.6980, 2014.
URL https://arxiv.org/pdf/1412.6980v8.pdf.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the 2015 Interna-
tional Conference on Learning Representations (ICLR-2015), arXiv preprint arXiv:1312.6114, 2014. URL
https://arxiv.org/abs/1312.6114.
Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregres-
sive flow. arXiv preprint arXiv:1606.04934, 2016.
Alex Lamb, Vincent Dumoulin, and Aaron Courville. Discriminative regularization for generative models.
arXiv preprint arXiv:1602.03220, 2016.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, and Ole Winther. AutoenCoding beyond pixels using
a learned similarity metric. In Proceedings of The 33rd International Conference on Machine Learning,
arXiv preprint arXiv:1512.09300, pp. 1558-1566, 2015. URL http://jmlr.org/proceedings/
papers/v48/larsen16.pdf.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning faCe attributes in the wild. In Proceed-
ings of the IEEE International Conference on Computer Vision, pp. 3730-3738, 2015.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoenCoders. arXiv
preprint arXiv:1511.05644, 2015.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro BissaCCo, Bo Wu, and Andrew Y Ng. Reading dig-
its in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning, 2011. URL https://static.googleusercontent.com/media/
research.google.com/en//pubs/archive/37648.pdf.
AleC Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep Convolutional
generative adversarial networks. In International Conference on Learning Representations (ICLR) 2016,
arXiv preprint arXiv:1511.06434, 2015. URL https://arxiv.org/pdf/1511.06434.pdf.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. StoChastiC baCkpropagation and approximate
inferenCe in deep generative models. In Proceedings of the 31st International Conference on Machine Learn-
ing, arXiv preprint arXiv:1401.4082, 2014. URL https://arxiv.org/pdf/1401.4082.pdf.
Jeffrey S Rosenthal. A review of asymptotiC ConvergenCe for general state spaCe markov Chains. Far East J.
Theor. Stat, 5(1):37-50, 2001.
H Sebastian Seung. Learning Continuous attraCtors in reCurrent networks. In NIPS Proceedings, volume 97,
pp. 654-660, 1997.
PasCal VinCent, Hugo LaroChelle, Yoshua Bengio, and Pierre-Antoine Manzagol. ExtraCting and Composing ro-
bust features with denoising autoenCoders. In Proceedings of the 25th International Conference on Machine
Learning, pp. 1096-1103. ACM, 2008.
Tom White. Sampling generative networks: Notes on a few effeCtive teChniques. arXiv preprint
arXiv:1609.04468, 2016.
12
Under review as a conference paper at ICLR 2017
Supplementary Material
A PROOF THAT T(Zt+1|Zt) > 0
For Pθ (X |Z) > 0 We require that all possible X ∈ X ⊆ Ra may be generated by the net-
work. Assuming that the model Pθ(X|Z) is trained using a sufficient number of training samples,
X ∈ Xtrain = X , and that the model has infinite capacity to model Xtrain = X , then we should
be able to draw any sample X ∈ Xtrain = X from Pθ(X|Z). In reality Xtrain ⊆ X and it is not
possible to have a model with infinite capacity. However, Pθ(X|Z) is modeled using a deep neural
network, which we assume has sufficient capacity to capture the training data well. Further, deep
neural networks are able to interpolate between samples in very high dimensional spaces (Radford
et al., 2015); we therefore further assume that if we have a large number of training samples (as well
as large model capacity), that almost any X ∈ X can be drawn from P§(X|Z).
Note that if we wish to generate human faces, we define Xall to be the space of all possible faces,
with distribution P(Xall), while Xtrain is the space of faces made up by the training data. Then,
practically even a well trained model which learns to interpolate well only captures an X, with distri-
bution J Pθ(X|Z)P(Z)dZ, where Xtrain ⊆ X ⊆ Xall, because X additionally contains examples
of interpolated versions of X 〜P(Xtrain).
For Qφ(Z|X) > 0 it must be possible to generate all possible Z ∈ Z ⊆ Rb. Qφ(Z|X) is
described by the function e(∙; φ) : X → Z .To ensure that Qφ(Z |X) > 0, We want to show that
the function e(X; φ) allows us to represent all samples of z ∈ Z. VAEs and AAEs each construct
e(X; φ) to produce z ∈ Z in different ways.
The output of the encoder of a VAE, evAE (X; φ) is Z = μ + E Θ σ, where E 〜N(0, I). The output
of a VAE is then always Gaussian, and hence there is no limitation on the z’s that eVAE (X; φ) can
produce. This ensures that Qφ (Z|X) > 0, provided that σ = 0.
The encoder of our AAE, eAAE(X; φ), is a deep neural network consisting of multiple convolutional
and batch normalisation layers. The final layer of the eAAE(X; φ) is a fully connected layer without
an activation function. The input to each of the M nodes in the fully connected layer is a function
fi=1...M (X). This means that Z is given by: Z = a1f1 (X) + a2f2(X) + ... + aM fM (X), where
ai=1...M are the learned weights of the fully connected layer. We now consider three cases:
Case 1:	If ai are a complete set of bases for Z then it is possible to generate any Z ∈ Z from an
X ∈ X with a one-to-one mapping, provided that fi(X) is not restricted in the values that it can take.
Case 2:	If ai are an overcomplete set of bases for Z, then the same holds, provided that fi (X) is not
restricted in the values that it can take.
Case 3:	If ai are an undercomplete set of bases for Z then it is not possible to generate all Z ∈ Z
from X ∈ X . Instead there is a many (X) to one (Z) mapping.
For Qφ(Z|X) > 0 our network must learn a complete or overcomplete set of bases and fi(x) must
not be restricted in the values that it can take ∀i. The network is encouraged to learn an overcomplete
set of bases by learning a large number of ai’s—specifically M = 8192 when basing our network
on the DCGAN architecture (Radford et al., 2015)—more that 40 times the dimensionality of Z.
By using batch normalisation layers throughout the network, we ensure that values of fi (x) are
spread out, capturing a close-to-Gaussian distribution (Ioffe & Szegedy, 2015), encouraging infinite
support.
We have now shown that, under certain reasonable assumptions, Pθ(X|Z) > 0 and Qφ(Z|X) > 0,
which means that T (Zt+1 |Zt) > 0, and hence we can get from any Z to any another Z in only
one step. Therefore the Markov chain described by the transition operator T (Zt+1 |Zt) defined in
Equation (1) is both irreducible and aperiodic, which are the necessary conditions for ergodicity.
13
Under review as a conference paper at ICLR 2017
B CelebA
B.1 Interpolations
(a) DVAE (initial)
(b) DVAE (5 steps)
J
Figure 5: Interpolating between two faces using (a-d) a DVAE. The top rows (a, c) for each face is
the original interpolation, whilst the second rows (b, d) are the result of 5 steps of MCMC sampling
applied to the latent samples that were used to generate the original interpolation. The only qualita-
tive difference when compared to VAEs (see Figure 4) is a desaturation of the generated images.
14
Under review as a conference paper at ICLR 2017
(a) AAE (initial)
(b) AAE (5 steps)
(c) AAE (initial)
(d) AAE (5 steps)
(e) DAAE (initial)
(f) DAAE (5 steps)
(g) DAAE (initial)
(h) DAAE (5 steps)
Figure 6: Interpolating between two faces using (a-d) an AAE and (e-h) a DAAE. The top rows (a,
c, e, g) for each face is the original interpolation, whilst the second rows (b, d, f, h) are the result
of 5 steps of MCMC sampling applied to the latent samples that were used to generate the original
interpolation. Although the AAE performs poorly (b, d), the regularisation effect of denoising can
be clearly seen with the DAAE after applying MCMC sampling (f, h).
15
Under review as a conference paper at ICLR 2017
C S treet View House Numbers
C.1 Samples
(a) VAE (initial)
(b) VAE (1 step)
(c) VAE (5 steps)
(d) VAE (10 steps)
(g) DVAE (5 steps)
(h) DVAE (10 steps)
(e) DVAE (initial)
(i) AAE (initial)
(f) DVAE (1 step)
(j) AAE (1 step)
(k) AAE (5 steps)
(l) AAE (10 steps)
(m) DAAE (initial)	(n) DAAE (1 step)	(o) DAAE (5 steps) (p) DAAE (10 steps)
Figure 7: Samples from a VAE (a-d), DVAE (e-h), AAE (i-l) and DAAE (m-p) trained on the SVHN
dataset. The samples from the models imitate the blurriness present in the dataset. Although very
few numbers are visible in the initial sample, the VAE and DVAE produce recognisable numbers
from most of the initial samples after a few steps of MCMC sampling. Although the AAE and
DAAE fail to produce recognisable numbers, the final samples are still a clear improvement over the
initial samples.


16
Under review as a conference paper at ICLR 2017
C.2 Interpolations
(a) VAE (initial)
(b) VAE (5 steps)
(c) VAE (initial)
(d) VAE (5 steps)
(e) DVAE (initial)
(f) DVAE (5 steps)
rm∣
(g) DVAE (initial)
(h) DVAE (5 steps)
Figure 8: Interpolating between Google Street View house numbers using (a-d) a VAE and (e-h) a
DVAE. The top rows (a, c, e, g) for each house number are the original interpolations, whilst the
second rows (b, d, f, h) are the result of 5 steps of MCMC sampling. If the original interpolation
produces symbols that do not resemble numbers, as observed in (a) and (e), the models will attempt
to move the samples towards more realistic numbers (b, f). Interpolation between 1- and 2-digit
numbers in an image (c, g) results in a meaningless blur in the middle of the interpolation. After a
few steps of MCMC sampling the models instead produce more recognisable 1- or 2-digit numbers
(d, h). We note that when the contrast is poor, denoising models in particular can struggle to recover
meaningful images (h).
17