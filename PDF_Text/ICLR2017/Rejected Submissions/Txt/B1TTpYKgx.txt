Under review as a conference paper at ICLR 2017
On the expressive power of deep neural net-
WORKS
Maithra Raghu
Google Brain and Cornell University
Ben Poole
Stanford University and Google Brain
Jon Kleinberg
Cornell University
Surya Ganguli
Stanford University
Jascha Sohl-Dickstein
Google Brain
Ab stract
We study the expressive power of deep neural networks before and after training.
Considering neural nets after random initialization, we show that three natural
measures of expressivity all display an exponential dependence on the depth of
the network. We prove, theoretically and experimentally, that all of these mea-
sures are in fact related to a fourth quantity, trajectory length. This quantity grows
exponentially in the depth of the network, and is responsible for the depth sen-
sitivity observed. These results translate to consequences for networks during
and after training. They suggest that parameters earlier in a network have greater
influence on its expressive power - in particular, given a layer, its influence on
expressivity is determined by the remaining depth of the network after that layer.
This is verified with experiments on MNIST and CIFAR-10. We also explore the
effect of training on the input-output map, and find that it trades off between the
stability and expressivity of the input-output map.
1 Introduction
Neural network architectures have proven “unreasonably effective” (LeCun, 2014; Karpathy, 2015)
on many tasks, including image classification (Krizhevsky et al., 2012), identifying particles in high
energy physics (Baldi et al., 2014), playing Go (Silver et al., 2016), and modeling human student
learning (Piech et al., 2015). Despite their power, we have limited knowledge of how and why neural
networks work, and much of this understanding is qualitative and heuristic.
To aim for a more precise understanding, we must disentangle factors influencing their effectiveness,
trainability, or how well they can be fit to data; generalizability, or how well they perform on novel
examples; and expressivity, or the set of functions they can compute.
All three of these properties are crucial for understanding the performance of neural networks. In-
deed, for success at a particular task, neural nets must first be effectively trained on a dataset, which
has prompted investigation into properties of objective function landscapes (Dauphin et al., 2014;
Goodfellow et al., 2014; Choromanska et al., 2014), and the design of optimization procedures
specifically suited to neural networks (Martens and Grosse, 2015). Trained networks must also be
capable of generalizing to unseen data, and understanding generalization in neural networks is also
an active line of research: (Hardt et al., 2015) bounds generalization error in terms of stochastic
gradient descent steps, (Sontag, 1998; Bartlett and Maass, 2003; Bartlett et al., 1998) study gener-
alization error through VC dimension, and (Hinton et al., 2015) looks at developing smaller models
with better generalization.
In this paper, we focus on the third of these properties, expressivity — the capability of neural
networks to accurately represent different kinds of functions. As the class of functions achievable
by a neural network is dependent on properties of its architecture, e.g. depth, width, fully connected,
convolutional, etc; a better understanding of expressivity may greatly inform architectural choice and
inspire more tailored training methods.
Prior work on expressivity has yielded many fascinating results by directly examining the achiev-
able functions of a particular architecture. Through this, neural networks have been shown to be
1
Under review as a conference paper at ICLR 2017
universal approximators (Hornik et al., 1989; Cybenko, 1989), and connections between boolean
and threshold networks and ReLU networks developed in (Maass et al., 1994; Pan and Srikumar,
2015). The inherent expressivity due to increased depth has also been studied in (Eldan and Shamir,
2015; Telgarsky, 2015; Martens et al., 2013; Bianchini and Scarselli, 2014), and (Pascanu et al.,
2013; Montufar et al., 2014), with the latter introducing the number of linear regions as a measure
of expressivity.
These results, while compelling, also highlight limitations of much of the existing work on ex-
pressivity. Much of the work examining achievable functions relies on unrealistic architectural
assumptions, such as layers being exponentially wide (in the universal approximation theorem).
Furthermore, architectures are often compared via 'hardcoded' weight values - a specific function
that can be represented efficiently by one architecture is shown to only be inefficiently approximated
by another.
Comparing architectures in such a fashion limits the generality of the conclusions, and does not
entirely address the goal of understanding expressivity — to provide characteristic properties of a
typical set of networks arising from a particular architecture, and extrapolate to practical conse-
quences.
Random networks To address this, we begin our analysis of network expressivity on a family of
networks arising in practice — the behaviour of networks after random initialization. As random
initialization is the starting point to most training methods, results on random networks provide
natural baselines to compare trained networks with, and are also useful in highlighting properties of
trained networks (see Section 3). The expressivity of these random networks is largely unexplored.
In previous work (Poole et al., 2016) we studied the propagation of Riemannian curvature through
random networks by developing a mean field theory approach, which quantitatively supports the
conjecture that deep networks can disentangle curved manifolds in input space. Here, we take a
more direct approach, exactly relating the architectural properties of the network to measures of
expressivity and exploring the consequences for trained networks.
Measures of Expressivity In particular, we examine the effect of the depth and width of a net-
work architecture on three different natural measures of functional richness: number of transitions,
activation patterns, and number of dichotomies.
Transitions: Counting neuron transitions is introduced indirectly via linear regions in (Pascanu
et al., 2013), and provides a tractable method to estimate the degree of non-linearity of the computed
function.
Activation Patterns: Transitions ofa single neuron can be extended to the outputs of all neurons in
all layers, leading to the (global) definition of a network activation pattern, also a measure of non-
linearity. Network activation patterns directly show how the network partitions input space (into
convex polytopes), through connections to the theory of hyperplane arrangements.
Dichotomies: We also measure the heterogeneity of a generic class of functions from a particular
architecture by counting dichotomies, ‘statistically dual’ to sweeping input in some cases. This
measure reveals the importance of remaining depth in expressivity, in both simulation and practice.
Connection to Trajectory Length All three measures display an exponential increase with depth,
but not width (most strikingly in Figure 4). We discover and prove the underlying reason for this -
all three measures are directly proportional to a fourth quantity, trajectory length. In Theorem 1) we
show that trajectory length grows exponentially with depth (also supported by experiments, Figure
1) which explains the depth sensitivity of the other three measures.
Consequences for Trained Networks Our empirical and theoretical results connecting transitions
and dichotomies to trajectory length also suggest that parameters earlier in the network should have
exponentially greater influence on parameters later in the network. In other words, the influence on
expressivity of parameters, and thus layers, is directly related to the remaining depth of the network
after that layer. Experiments on MNIST and CIFAR-10 support this hypothesis — training only
earlier layers leads to higher accuracy than training only later layers. We also find, with experiments
on MNIST, that the training process trades off between the stability of the input-output map and its
expressivity.
2
Under review as a conference paper at ICLR 2017
Trajectory Length, fc=32
5 4 3 2 1
Ooooo
Illll
£39 XJowfe」！
0	2	4	6	8	10	12
2 10
Ooo
111
£39 XJaP©」！
IO0
Figure 1: The exponential growth of trajectory length with depth, in a random deep network with
hard-tanh nonlinearities. A circular trajectory is chosen between two random vectors. The image of
that trajectory is taken at each layer of the network, and its length measured. (a,b) The trajectory
length vs. layer, in terms of the network width k and weight variance σw2 , both of which determine
its growth rate. (c,d) The average ratio of a trajectory’s length in layer d + 1 relative to its length in
layer d. The solid line shows simulated data, while the dashed lines show upper and lower bounds
(Theorem 1). Growth rate is a function of layer width k, and weight variance σw2 .
2 Growth of Trajectory Length and Measures of Expressivity
0	2	4	6	8	10	12
In this section we examine random networks, proving and empirically verifing the exponential
growth of trajectory length with depth. We then relate trajectory length to transitions, activation
patterns and dichotomies, and show their exponential increase with depth.
2.1	Notation and Definitions
Let FW denote a neural network. In this section, we consider architectures with input dimension m,
n hidden layers all of width k, and (for convenience) a scalar readout layer. (So, FW : Rm → R.)
Our results mostly examine the cases where φ is a hard-tanh (Collobert and Bengio, 2004) or ReLU
nonlinearity. All hard-tanh results carry over to tanh with additional technical steps.
We use vi(d) to denote the ith neuron in hidden layer d. We also let x = z(0) be an input, h(d) be the
hidden representation at layer d, and φ the non-linearity. The weights and bias are called W (d) and
b(d) respectively. So we have the relations
h(d) = W (d) z(d) + b(d),	z(d+1) = φ(h(d)).	(1)
Definitions Say a neuron transitions when it switches linear region in its activation function (i.e.
for ReLU, switching between zero and linear regimes, for hard-tanh, switching between negative
saturation, unsaturated and positive saturation). For hard-tanh, we refer to a sign transition as the
neuron switching sign, and a saturation transition as switching from being saturated between ±1.
The Activation Pattern of the entire network is defined by the output regions of every neuron. More
precisely, given an input x, we let A(FW , x) be a vector representing the activation region of every
hidden neuron in the network. So for a ReLU network FW, we can take A(FW , x) ∈ {-1, 1}nk
with -1 meaning the neuron is in the zero regime, and 1 meaning it is in the linear regime. For
3
Under review as a conference paper at ICLR 2017
hard-tanh network FW, we can (overloading notation slightly) take A(FW , x) ∈ {-1, 0, 1}nk. The
use of this notation will be clear by context. Given a set of inputs S, we say a dichotomy over S is a
labeling of each point in S as ±1.
We assume the weights of our neural networks are initialized as random Gaussians, with appropriate
variance scaling to account for width, i.e. W(d 〜N(0, σW/k), and biases b(d) 〜 N(0, σb2). In
the analysis below, we sweep through a one dimensional input trajectory x(t). The results hold
for almost any such smooth x(t), provided that at any point x(t), the trajectory direction has some
non-zero magnitude perpendicular to x(t).
2.2	Trajectory Length and Neuron Transitions
We first prove how the trajectory length grows, and relate it to neuron transitions.
2.2.1	Bound on trajectory length growth
We prove (with a more exact lower bound in the Appendix):
Theorem 1. Bound on Growth of Trajectory Length Let FW be a hard tanh random neural network
and x(t) a one dimensional trajectory in input space. Define z(d)(x(t)) = z(d)(t) to be the image
of the trajectory in layer d of FW, and let l(z(d) (t)) = Rjl dzz；『，11 dt be the arc length of Z⑷(t).
Then
E hl（Z（d）（t））i ≥ Ou⅛V4
d
y√σw+σ2 +k
l(x(t))
This bound is tight in the limits of large σw and k. An immediate Corollary for σb = 0, i.e. no bias,
is
Corollary 1. Bound on Growth of Trajectory Length Without Bias For FW with zero bias, we have
E
…≥O((√σ⅛))
l(x(t))
The theorem shows that the image of a trajectory in layer d has grown exponentially in d, with the
scaling σw and width of the network k determining the base. We additionally state and prove a sim-
ple O(σwd ) growth upper bound in the Appendix. Figure 1 demonstrates this behavior in simulation,
and compares against the bounds. Note also that if the variance of the bias is comparatively too large
i.e. σb >> σw , then we no longer see exponential growth. This corresponds to the phase transition
described in (Poole et al., 2016).
The proof can be found in the Appendix. A rough outline is as follows: we look at the expected
growth of the difference between a point z(d) (t) on the curve and a small perturbation z(d) (t + dt),
from layer d to layer d + 1. Denoting this quantity llδz(d)(t)ll, we derive a recurrence relating
llδz(d+1)(t)ll and llδz(d)(t)ll which can be composed to give the desired growth rate.
The analysis is complicated by the statistical dependence on the image of the input z(d+1)(t). So we
instead form a recursion by looking at the component of the difference perpendicular to the image
of the input in that layer, i.e. llllδz⊥(d+1)(t)llll. For a typical trajectory, the perpendicular component
preserves a fraction Jk-1 of the total trajectory length, and our derived growth rate thus provides
a close lower bound, as demonstrated in Figure 1(c,d).
2.2.2	Relation to number of transitions
Further experiments (Figure 2) show:
Observation 1. The number of sign transitions in a network FW is directly proportional to the
length of the latent image of the curve, z(n) (t).
4
Under review as a conference paper at ICLR 2017
Length/法
4=8就) =2
fc=64,σ^ =2
k=8,4 =8
A =64,端=8
fc=512,σ^ =8
fc=8,σ≡ =32
fc=64,σ^ =32
七=512,4 =32
Figure 2: The number of transitions is linear in trajectory length. Here we compare the empirical
number of sign changes to the length of the trajectory, for images of the same trajectory at different
layers of a hard-tanh network. We repeat this comparison for a variety of network architectures,
with different network width k and weight variance σw2 .
We intuit a reason for this observation as follows: note that for a network FW with n hidden layers,
the linear, one dimensional, readout layer outputs a value by computing the inner product W (n) z(n) .
The sign of the output is then determined by whether this quantity is ≥ 0 or not. In particular, the
decision boundary is a hyperplane, with equation W (n) z(n) = 0. So, the number of transitions the
output neuron makes as x(t) is traced is exactly the number of times z(n) (t) crosses the decision
boundary. As FW is a random neural network, with signs of weight entries split purely randomly
between ±1, it would suggest that points far enough away from each other would have independent
signs, i.e. a direct proportionality between the length of z(n) (t) and the number of times it crosses
the decision boundary.
We can also prove this in the special case when σw is very large. Note that by Theorem 1, very large
σw results in a trajectory growth rate of
((√k	∖n∖
g(k,σw ,σb,n) = O	=
厂,
Large σw also means that for any input (bounded away from zero), almost all neurons are saturated.
Furthermore, any neuron transitioning from 1to -1 (or vice versa) does so almost instantaneously.
In particular, at most one neuron within a layer is transitioning for any input. We can then show that
in the large σw limit the number of transitions matches the trajectory length (proof in the Appendix,
via a reduction to magnitudes of independent Gaussians):
Theorem 2. Number of transitions in large weight limit Given FW, in the very large σw regime, the
number of sign transitions of the network as an input x(t) is swept is of the order of g(k, σw, σb, n).
2.3	Transitions and Activation Patterns
We can generalize the ’local’ notion of expressivity of a neuron‘s sign transitions to a ’global’
measure of activation patterns over the entire network. We can formally relate network activation
patterns to specific hyperplane arrangements, which allows proof of three exciting results.
First, we can precisely state the effect of a neural network on input space, also visualized in Figure
3
Theorem 3. Regions in Input Space Given a network FW with with ReLU or hard-tanh activations,
input space is partitioned into convex regions (polytopes), with FW corresponding to a different
linear function on each region.
This results in a bijection between transitions and activation patterns for ‘well-behaved’ trajectories,
see the proof of Theorem 3 and Corollary 2 in Appendix.
Finally, returning to the goal of understanding expressivity, we can upper bound the expressive
power of a particular architecture according to the activation patterns measure:
5
Under review as a conference paper at ICLR 2017
(a)
Figure 3: Deep networks with piecewise linear activations subdivide input space into convex poly-
topes. Here we plot the boundaries in input space separating unit activation and inactivation for all
units in a three layer ReLU network, with four units in each layer. The left pane shows activation
boundaries (corresponding to a hyperplane arrangement) in gray for the first layer only, partitioning
the plane into regions. The center pane shows activation boundaries for the first two layers. Inside
every first layer region, the second layer activation boundaries form a different hyperplane arrange-
ment. The right pane shows activation boundaries for the first three layers, with different hyperplane
arrangements inside all first and second layer regions. This final set of convex regions correspond to
different activation patterns of the network - i.e. different linear functions.
Dichotomies vs. Remaining Depth
04
SEα>t!3dα>nb-un
3 2 1
Ooo
111
2	4	6	8	10 12 14 16 18
Remaining Depth dr
Dichotomies vs. Width
03
2 1
O O
1 1
SEα>t!3dα>nb-un
0	100	200	300	400	500	600
Width k
(b)

Figure 4: The number of functions achievable in a deep hard-tanh network by sweeping a single
layer’s weights along a one dimensional trajectory is exponential in the remaining depth, but in-
creases only slowly with network width. Here we plot the number of classification dichotomies over
s = 15 input vectors achieved by sweeping the first layer weights in a hard-tanh network along a
one-dimensional great circle trajectory. We show this (a) as a function of remaining depth for several
widths, and (b) as a function of width for several remaining depths. All networks were generated
with weight variance σw2 = 8, and bias variance σb2 = 0.
Theorem 4. (Tight) Upper bound for Number of Activation Patterns Given a neural network FW,
inputs in Rm, with ReLU or hard-tanh activations, and with n hidden layers of width k, the number
of activation patterns grows at most like O(kmn) for ReLU, or O((2k)mn) for hard-tanh.
2.4	Dichotomies: a natural dual
So far, we have looked at the effects of depth and width on the expressiveness (measured through
transitions and activations) of a generic function computed by that network architecture. These
measures are directly related to trajectory length, which is the underlying reason for exponential
depth dependence.
A natural extension is to study a class of functions that might arise from a particular architecture.
One such class of functions is formed by sweeping the weights of a network instead of the input.
More formally, we pick random matrices, W, W0, and consider the weight interpolation W cos(t) +
W0 sin(t), each choice of weights giving a different function. When this process is applied to just
the first layer, we have a statistical duality with sweeping a circular input.
6
Under review as a conference paper at ICLR 2017
Dichotomies vs. Remaining Depth
sφ一Eosuyɑφnb 一 Un
4	6	8	10	12	14	16
Remaining Depth dτ


Figure 5: Expressive power depends only on remaining network depth. Here we plot the number of
dichotomies achieved by sweeping the weights in different network layers through a 1-dimensional
great circle trajectory, as a function of the remaining network depth. The number of achievable
dichotomies does not depend on the total network depth, only on the number of layers above the
layer swept. All networks had width k = 128, weight variance σw2 = 8, number of datapoints
s = 15, and hard-tanh nonlinearities. The blue dashed line indicates all 2s possible dichotomies for
this random dataset.
Figure 6: Demonstration of expressive power of remaining depth on MNIST. Here we plot train
and test accuracy achieved by training exactly one layer of a fully connected neural net on MNIST.
The different lines are generated by varying the hidden layer chosen to train. All other layers are
kept frozen after random initialization. We see that training lower hidden layers leads to better
performance. The networks had width k = 100, weight variance σw2 = 2, and hard-tanh nonlin-
earities. Note that we only train from the second hidden layer (weights W (1)) onwards, so that the
number of parameters trained remains fixed. While the theory addresses training accuracy and not
generalization accuracy, the same monotonic pattern is seen for both.
Given this class of functions, one useful measure of expressivity is determining how heteroge-
neous this class is. Inspired by classification tasks we formalize it as: given a set of inputs,
S = {x1, .., xs} ⊂ Rm, how many of the 2s possible dichotomies does this function class pro-
duce on S?
For non-random inputs and non-random functions, this is a well known question upper bounded by
the Sauer-Shelah lemma (Sauer, 1972). We discuss this further in Appendix D.1. In the random
setting, the statistical duality of weight sweeping and input sweeping suggests a direct proportion
to transitions and trajectory length for a fixed input. Furthermore, if the xi ∈ S are sufficiently
uncorrelated (e.g. random) class label transitions should occur independently for each xi Indeed,
we show this in Figure 4 (more figures, e.g. dichotomies vs transitions and observations, are included
in the Appendix).
Observation 2. Depth and Expressivity in a Function Class. Given the function class F as above,
the number of dichotomies expressible by F over a set of random inputs S by sweeping the first
layer weights along a one dimensional trajectory W(0) (t) is exponential in the network depth n.
7
Under review as a conference paper at ICLR 2017
Figure 7: We repeat a similar experiment in Figure 6 with a fully connected network on CIFAR-10,
and mostly observe that training lower layers again leads to better performance. The networks had
width k = 200, weight variance σw2 = 1, and hard-tanh nonlinearities. We again only train from the
second hidden layer on so that the number of parameters remains fixed.
Property	Architecture	Results
Trajectory length	hard-tanh	Asymptotically tight lower bound (Thm 1) Upper bound (Appendix Section A) Simulation (Fig 1)
Neuron transitions	hard-tanh	Expectation in large weight limit (Thm 2) Simulation (Fig 2)
Dichotomies	hard-tanh	Simulation (Figs 4 and 10)
Regions in input space	hard-tanh	and ReLU		Consist of convex polytopes (Thm 3)
Network activation patterns	hard-tanh	and ReLU		Tight upper bound (Thm 6)
Effect of remaining depth	hard-tanh	Simulation (Fig 5) Experiment on MNIST (Fig 6) Experiments on CIFAR-10 (Fig 7)
Effect of training on trajec- tory length	hard-tanh	Experiment on MNIST (Fig 8, 9)
Table 1: List and location of key theoretical and experimental results.
3 Trained Networks
Remaining Depth The results from Section 2, particularly those linking dichotomies to trajectory
length, suggest that earlier layers in the network might have more expressive power. In particular,
the remaining depth of the network beyond the layer might directly influence its expressive power.
We see that this holds in the random network case (Figure 5), and also for networks trained on
MNIST and CIFAR-10. In Figures 6, 7 we randomly initialized a neural network, and froze all the
layers except for one, which we trained.
Training trades off between input-output map stability and expressivity We also look at the
effect of training on measures of expressivity by plotting the change in trajectory length and number
of transitions (see Appendix) during the training process. We find that for a network initialized with
large。加,the training process appears to stabilize the input-output map - monotonically decreasing
trajectory length (Figure 8) except for the final few steps. Interestingly, this happens at a faster rate
in the vicinity of the data than for random inputs, and is accomplished without reducing weight
magnitudes.
For a network closer to the boundary of the exponential regime σw2 = 3, where trajectory length
growth is still exponential but with a much smaller base, the training process increases the trajectory
length, enabiling greater expressivity in the resulting input-output map, Figure 9
8
Under review as a conference paper at ICLR 2017
Trajectory Length during training
MNIST inputs
I4βusXJOP*51
101
O 2	4	6	8	10	12
Layer number
Random inputs
O 2	4	6	8	10	12
Layer number
Figure 8:	Training acts to stabilize the input-output map by decreasing trajectory length for σw
large. The left pane plots the growth of trajectory length as a circular interpolation between two
MNIST datapoints is propagated through the network, at different train steps. Red indicates the
start of training, with purple the end of training. Interestingly, and supporting the observation on
remaining depth, the first layer appears to increase trajectory length, in contrast with all later layers,
suggesting it is being primarily used to fit the data. The right pane shows an identical plot but for an
interpolation between random points, which also display decreasing trajectory length, but at a slower
rate. Note the output layer is not plotted, due to artificial scaling of length through normalization.
The network is initialized with σw2 = 16. A similar plot is observed for the number of transitions
(see Appendix.)
Trajectory Length during training
MNIST inputs
u8ue-l XJOt9s'ffl
2	4	6	8	10	12
Layer number
Random inputs
O 2	4	6 S 10	12
Layer number
Figure 9:	Training increases expressivity of input-output map for σw small. The left pane plots the
growth of trajectory length as a circular interpolation between two MNIST datapoints is propagated
through the network, at different train steps. Red indicates the start of training, with purple the
end of training. We see that the training process increases trajectory length, likely to increase the
expressivity of the input-output map to enable greater accuracy. The right pane shows an identical
plot but for an interpolation between random points, which also displays increasing trajectory length,
but at a slower rate. Note the output layer is not plotted, due to artificial scaling of length through
normalization. The network is initialized with σw2 = 3.
4 Conclusion
In this paper, we studied the expressivity of neural networks through three measures, neuron tran-
sitions, activation patterns and dichotomies, and explained the observed exponential dependence on
depth of all three measures by demonstrating the underlying link to latent trajectory length. Having
explored these results in the context of random networks, we then looked at the consequences for
trained networks (see Table 1). We find that the remaining depth above a network layer influences
its expressive power, which might inspire new pre-training or initialization schemes. Furthermore,
we see that training interpolates between expressive power and better generalization. This relation
between initial and final parameters might inform early stopping and warm starting rules.
9
Under review as a conference paper at ICLR 2017
Acknowledgements
We thank Samy Bengio, Ian Goodfellow, Laurent Dinh, and Quoc Le for extremely helpful discus-
sion.
References
Yann LeCun. The unreasonable effectiveness of deep learning. In Seminar. Johns Hopkins University, 2014.
Andrej Karpathy. The unreasonable effectiveness of recurrent neural networks. In Andrej Karpathy blog, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097-1105, 2012.
Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with
deep learning. Nature communications, 5, 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, and Jascha
Sohl-Dickstein. Deep knowledge tracing. In Advances in Neural Information Processing Systems, pages
505-513, 2015.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.
Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Ad-
vances in neural information processing systems, pages 2933-2941, 2014.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization
problems. arXiv preprint arXiv:1412.6544, 2014.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The loss surfaces
of multilayer networks. arXiv preprint arXiv:1412.0233, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature.
arXiv preprint arXiv:1503.05671, 2015.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient
descent. arXiv preprint arXiv:1509.01240, 2015.
Eduardo D Sontag. Vc dimension of neural networks. NATO ASI SERIES F COMPUTER AND SYSTEMS
SCIENCES, 168:69-96, 1998.
Peter L Bartlett and Wolfgang Maass. Vapnik-chervonenkis dimension of neural nets. The handbook of brain
theory and neural networks, pages 1188-1192, 2003.
Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc-dimension bounds for piecewise polynomial
networks. Neural computation, 10(8):2159-2173, 1998.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal ap-
proximators. Neural networks, 2(5):359-366, 1989.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals
and systems, 2(4):303-314, 1989.
Wolfgang Maass, Georg Schnitger, and Eduardo D Sontag. A comparison of the computational power of
sigmoid and Boolean threshold circuits. Springer, 1994.
Xingyuan Pan and Vivek Srikumar. Expressiveness of rectifier networks. arXiv preprint arXiv:1511.05678,
2015.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv preprint
arXiv:1512.03965, 2015.
Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101,
2015.
James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of
restricted boltzmann machines. In Advances in Neural Information Processing Systems, pages 2877-2885,
2013.
Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between
shallow and deep architectures. Neural Networks and Learning Systems, IEEE Transactions on, 25(8):1553-
1565, 2014.
Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep feed forward
networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098, 2013.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of
deep neural networks. In Advances in neural information processing systems, pages 2924-2932, 2014.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expres-
sivity in deep neural networks through transient chaos. arXiv preprint, 2016.
Ronan Collobert and Samy Bengio. Links between perceptrons, mlps and svms. In Proceedings of the twenty-
first international conference on Machine learning, page 23. ACM, 2004.
10
Under review as a conference paper at ICLR 2017
Norbert Sauer. On the density of families of sets. Journal of Combinatorial Theory, Series A, 13(1):145-147,
1972.
D. Kershaw. Some extensions of w. gautschi’s inequalities for the gamma function. Mathematics of Computa-
tion, 41(164):607-611, 1983.
Andrea Laforgia and Pierpaolo Natalini. On some inequalities for the gamma function. Advances in Dynamical
Systems and Applications, 8(2):261-267, 2013.
Richard Stanley. Hyperplane arrangements. Enumerative Combinatorics, 2011.
Vladimir Naumovich Vapnik and Vlamimir Vapnik. Statistical learning theory, volume 1. Wiley New York,
1998.
11
Under review as a conference paper at ICLR 2017
Appendix
Here we include the full proofs from sections in the paper.
A Proofs and additional results from Section 2.2
Proof of Theorem 1 We prove this result for FW with zero bias for technical simplicity. The
result also translates over to FW with bias with a couple of technical modifications.
A. 1 Notation and Preliminary Results
Difference of points on trajectory Given x(t) = x, x(t + dt) = x + δx in the trajectory, let δz(d) =
z(d)(x + δx) - z(d)(x)
Parallel and Perpendicular Components: Given vectors x, y, we can write y = y⊥ + yk where y⊥
is the component of y perpendicular to x, and yk is the component parallel to x. (Strictly speaking,
these components should also have a subscript x, but we suppress it as the direction with respect to
which parallel and perpendicular components are being taken will be explicitly stated.)
This notation can also be used with a matrix W, see Lemma 1.
Before stating and proving the main theorem, we need a few preliminary results.
Lemma 1. Matrix Decomposition Let x, y ∈ Rk be fixed non-zero vectors, and let W be a (full
rank) matrix. Then, we can write
W = k Wk + k W⊥ + ⊥Wk + ⊥W⊥
such that
k W⊥x = 0	⊥W⊥x = 0
yT ⊥Wk = 0	yT ⊥W⊥ = 0
i	.e. the row space of W is decomposed to perpendicular and parallel components with respect to x
(subscript on right), and the column space is decomposed to perpendicular and parallel components
of y (superscript on left).
Proof. Let V, U be rotations such that Vx = (||x|| , 0..., 0)T and Uy = (||y|| , 0...0)T. Now let
W = UWVT, and let W = k W∣∣ + kW⊥ + ⊥W∣∣ + ⊥W⊥, with k W∣∣ having non-zero term exactly
W11, kW⊥ having non-zero entries exactly W1i for 2 ≤ i ≤ k. Finally, we let ⊥ Wk have non-zero
entries exactly Wi1 , with 2 ≤ i ≤ k and ⊥ W⊥ have the remaining entries non-zero.
If We define X = Vx and y = Uy, then We see that
kW⊥x = o	⊥W⊥x = o
yT ⊥W k =o	yT ⊥Wɪ = o
as X, y have only one non-zero term, which does not correspond to a non-zero term in the compo-
nents of W in the equations.
Then, defining k W∣∣ = UTkW∣∣ V, and the other components analogously, we get equations of the
form
k W⊥x = U TkW⊥ Vx = U TkWIX = 0
□
Observation 3. Given W, X as before, and considering Wk , W⊥ with respect to X (wlog a unit
vector) we can express them directly in terms of W as follows: Letting W(i) be the ith row of W,
we have
12
Under review as a conference paper at ICLR 2017
/((W(O))T ∙ x)x
Wk=	.
V(W(k))T ∙ x)x
i	.e. the projection of each row in the direction of x. And of course
W⊥ = W - Wk
The motivation to consider such a decomposition of W is for the resulting independence between
different components, as shown in the following lemma.
Lemma 2. Independence of Projections Let x be a given vector (wlog of unit norm.) If W is a
random matrix with Wij 〜 N(0, σ2), then Wk and W⊥ with respect to X are independent random
variables.
Proof. There are two possible proof methods:
(a)	We use the rotational invariance of random Gaussian matrices, i.e. if W is a Gaussian
matrix, iid entries N(0, σ2 ), and R is a rotation, then RW is also iid Gaussian, entries
N(0, σ2). (This follows easily from affine transformation rules for multivariate Gaussians.)
Let V be a rotation as in Lemma 1. Then W = W V T is also iid Gaussian, and furthermore,
Wk and W⊥ partition the entries of W, so are evidently independent. But then Wk =
Wk V T and W⊥ = W⊥ V T are also independent.
(b)	From the observation note that Wk and W⊥ have a centered multivariate joint Gaussian
distribution (both consist of linear combinations of the entries Wij in W.) So it suffices
to show that Wk and W⊥ have covariance 0. Because both are centered Gaussians, this is
equivalent to showing E(< Wk , W⊥ >) = 0. We have that
E(< Wk,W⊥ >) = E(WkW⊥T) = E(WkWT) -E(WkWkT)
As any two rows of W are independent, we see from the observation that E(Wk WT) is a
diagonal matrix, with the ith diagonal entry just ((W(O) )t ∙ x)2. But similarly, E(Wk WiT)
is also a diagonal matrix, with the same diagonal entries - so the claim follows.
□
In the following two lemmas, we use the rotational invariance of Gaussians as well as the chi distri-
bution to prove results about the expected norm of a random Gaussian vector.
Lemma 3. Norm of a Gaussian vector Let X ∈ Rk be a random Gaussian vector, with Xi iid,
~ N(0, σ2). Then
E[||X||]
σ√2
Γ((k + 1)/2)
Γ(k∕2)
Proof. We use the fact that if Y is a random Gaussian, and 匕 ~ N(0,1) then ||Y|| follows a Chi
distribution. This means that E(∣∣X∕σ∣∣) = √2Γ((k + 1)∕2)∕Γ(k∕2), the mean ofachi distribution
with k degrees of freedom, and the result follows by noting that the expectation in the lemma is σ
multiplied by the above expectation.	口
We will find it useful to bound ratios of the Gamma function (as appear in Lemma 3) and so introduce
the following inequality, from (Kershaw, 1983) that provides an extension of Gautschi’s Inequality.
Theorem 5. An Extension of Gautschi’s Inequality For 0 < s < 1, we have
s 1-s	Γ(x + 1)
(X+2)	≤ r⅛πy
≤
13
Under review as a conference paper at ICLR 2017
We now show:
Lemma 4. Norm of Projections Let W be a k by k random Gaussian matrix with iid entries 〜
N(0, σ2), and x, y two given vectors. Partition W into components as in Lemma 1 and let x⊥ be a
nonzero vector perpendicular to x. Then
(a)
E [∣l⊥W⊥χ⊥l∣] = ∣∣χ⊥∣∣ σ√2r(M)/2
≥ ∣∣x⊥∣∣ σ√2
(b)	If IA is an identity matrix with non-zeros diagonal entry i iff i ∈ A ⊂ [k], and |A| > 2,
then
E [∣∣ l∕⊥W⊥x⊥∣∣] ≥ ∣∣X⊥∣∣ σ √2
Γ(∣A∣∕2)
Γ((∣A∣- 1)/2)
≥ ∣∣x⊥∣∣ σ√2
（程-3广
Proof. (a) Let U, V, W be as in Lemma 1. As U, V are rotations, W is also iid Gaussian.
Furthermore for any fixed W, with α = Va, by taking inner products, and square-rooting,
We Seethat ∣ ∣Wa∣ ∣ = ||Wa||. So in particular
E[∣∣∣∣⊥W⊥x⊥∣∣∣∣ =E h∣∣∣∣∣∣⊥Wα⊥xα⊥∣∣∣∣∣∣i
But from the definition of non-zero entries of ⊥W⊥, and the form of xα⊥ (a zero entry in the
first coordinate), it follows that ⊥Wα ⊥xα⊥ has exactly k - 1 non zero entries, each a centered
Gaussian with variance (k - 1)σ2 ∣∣x⊥∣∣2. By Lemma 3, the expected norm is as in the
statement. We then apply Theorem 5 to get the lower bound.
(b)	First note we can view 1∕⊥W⊥ = ⊥1∕W⊥. (Projecting down to a random (as W is
random) subspace of fixed size |A| = m and then making perpendicular commutes with
making perpendicular and then projecting everything down to the subspace.)
So we can view W as a random mby k matrix, and for x, y as in Lemma 1 (with y projected
down onto m dimensions), we can again define U, V as k by k and m by m rotation
matrices respectively, and W = UWV T, with analogous properties to Lemma 1. Now we
can finish as in part (a), except that ⊥Wα ⊥xα may have only m - 1 entries, (depending on
whether y is annihilated by projecting down by IA) each of variance (k - 1)σ2 ∣∣x⊥∣∣2.
□
Lemma 5. Norm and Translation Let X be a centered multivariate Gaussian, with diagonal covari-
ance matrix, and μ a constant vector.
E(||X - μ∣∣) ≥ E(||X||)
Proof. The inequality can be seen intuitively geometrically: as X has diagonal covariance matrix,
the contours of the pdf of ||X || are circular centered at 0, decreasing radially. However, the contours
of the Pdf of ||X - μ∣∣ are shifted to be centered around ∣∣μ∣∣, and so shifting back μ to 0 reduces
the norm.
A more formal proof can be seen as follows: let the pdf of X be fχ (∙). Then we wish to show
/ ||x - μ∣∣ fχ (x)dx ≥ /
||x|| fX (x)dx
Now we can pair points x, -x, using the fact that fX (x) = fX (-x) and the triangle inequality on
the integrand to get
/ I (||x - μ∣∣ + ||-x - μ∣∣) fx(x)dx ≥ / J|2x|| fχ(x)dx = / ∣ (||x|| + ||-x||) fχ(x)dx
□
14
Under review as a conference paper at ICLR 2017
A.2 Proof of Theorem
Proof. We first prove the zero bias case, Theorem 1. To do so, it is sufficient to prove that
E "δz(d+1)Wi ≥ O ((√√+k! + )lkz(0)(t)ll	(**)
as integrating over t gives us the statement of the theorem.
For ease of notation, we will suppress the t in z(d) (t).
We first write
W (d) = W⊥(d) + Wk(d)
where the division is done with respect to z(d). Note that this means h(d+1) = Wk(d)z(d) as the other
component annihilates (maps to 0) z(d).
We can also define A
Wk(d)
{i : i ∈ [k], |hi(d+1) | < 1} i.e. the set of indices for which the hidden
representation is not saturated. Letting Wi denote the ith row of matrix W, we now claim that:
EW(d) hllllδz(d+1)lllli =EW(d)EW(d)
(
X
i∈AWk(d)
ʌ
((W⊥⊥d-)iδz^dd + (W(d))iδz ⑷)2
)
1/2
(*)
Indeed, by Lemma 2 we first split the expectation over W(d) into a tower of expectations over the
two independent parts of W to get
EW(d) hllllllδz(d+1)lllllli =EW(d)EW(d) hllllllφ(W (d)δz(d))lllllli
But conditioning on Wk(d)
in the inner expectation gives us h(d+1) and A
W(d), allowing us to replace
the norm over φ(W (d)δz(d)) with the sum in the term on the right hand side of the claim.
Till now, we have mostly focused on partitioning the matrix W(d). But we can also set δz(d) =
δzk(d) + δz⊥(d) where the perpendicular and parallel are with respect to z(d). In fact, to get the
expression in (**), we derive a recurrence as below:
EW (d) hllδz⊥d+1)lli ≥ O (√σ¾) EW (d) hHδz⊥d)lli
To get this, We first need to define z(d+1) = IA ⑷ h(d+1) - the latent vector h(d+1) With all
saturated units zeroed out.
We then split the column space of W(d) = ⊥W(d) + kW(d), where the split is with respect to z(d+1).
Letting δz⊥(d+1) be the part perpendicular to z(d+1), and A the set of units that are unsaturated, We
have an important relation:
Claim
llδz(d+1)l∣ ≥ ∣l⊥w(d)δz(d) 1a∣∣
(where the indicator in the right hand side zeros out coordinates not in the active set.)
To see this, first note, by definition,
δz(d+1) = W⑷δz⑷∙ IA - hW⑷δz⑷∙ 1a, z(d+1)iz(d+1)
where the: indicates a unit vector.
(1)
15
Under review as a conference paper at ICLR 2017
Similarly
⊥W(d)δz(d) = W(d)δz(d) - hW(d)δz(d), ≡(d+1)i≡(d+1)	⑵
Now note that for any index i ∈ A, the right hand sides of (1) and (2) are identical, and so the
vectors on the left hand side agree for all i ∈ A. In particular,
δz(d+1) ∙ IA = ⊥W(d)δz(d) ∙ IA
Now the claim follows easily by noting that ∣ ∣ δz(d+1) ∣ ∣ ≥ ∣ 卜z(d+1) ∙ IAl ∣ .
Returning to (*), we split δz(d) = δz(d) + δz(d), W⊥d) = k W⊥d) + Wv)( (and Wfd) analogously),
and after some cancellation, we have
EW(d) [∣ ∣ δzfd+1) ∣ ∣ ] = EWF)EW(d)
(
X ((ɪ W⊥d) + kW⊥d))iδz (d)
(∈AWF)
∖ 1/2-
+ (⊥ w(d) + kw(d))iδz(d) )2
)
We would like a recurrence in terms of only perpendicular components however, so we first drop
the k Wjd) J Wld) (which can be done without decreasing the norm as they are perpendicular to the
remaining terms) and using the above claim, have
EW(d) [ ∣ ∣ δz(d+1) ∣ ∣ ] ≥ EWF) EWf)
(
X ((⊥ WId))iδz⊥)
CeAWIf)
1/2
But in the inner expectation, the term ⊥ Wld) δz(d) is just a constant, as we are conditioning on Wld).
So using Lemma 5 we have
ι
X ((⊥ W⊥d))iδz ⊥d) + (⊥ W∣(d))iδz(d))
(eAwf)
1/2
2
≥ EWf)
X(什Wnδz'y
(eAWF)
1/2
/
/
We can then apply Lemma 4 to get
1/2
EWF
VeAWF)
E [∣∣δz⊥d)∣∣]
The outer expectation on the right hand side only affects the term in the expectation through the size
of the non-saturated set of units. Letting p = P(∣h(d+1) | < 1), and noting that we get a non-zero
norm
only if ∣Aw⑷ ∣ ≥ 2 (else we cannot project down a dimension), and for ∣A
Wf)
∣ ≥ 2,
/
∖∕2∣ AW产 ∣ - 3	ι _________
√2	2	≥ √2 q∣AWf) ∣
we get
EW ⑷[∣∣δz⊥d+1) ∣ ∣ ] ≥√2 (X C )pj (I- P)I √⅜ P)E [ ∣ ∣ δz⊥d) ∣ ∣ ]
16
Under review as a conference paper at ICLR 2017
We use the fact that we have the probability mass function for an (k, p) binomial random variable to
bound the √j term:
X C)pj(I- P)I j√ pj = - C)P(1 - P)I √k + X C)pj(I- P)I j√ P
=-σ√kp(1 - p)k-1 + kp ∙ √σ= X √j C-I)PjT(1 - P)k-j
But by using Jensen,s inequality with 1 /√χ, we get
X ±C-1) PTI-P)I ≥
1	_	1
JPk=Ij(k-；)PjT(I-p)k-j	p(k - I)P+1
where the last equality follows by recognising the expectation of a binomial(k - 1,p) random vari-
able. So putting together, we get
EW<"kτn ≥√2 (-σ√kp(1 -p)k-1+σ∙ √1+√kp- 1)p)E胫叫](a)
To lower bound p, we first note that as h(d+1) is a normal random variable with variance ≤ σ2, if
A 〜N(0, σ2)
P(|瓜d+1)|< 1) ≥ P(|A| < 1) ≥3	(b)
σ √2π
where the last inequality holds for σ ≥ 1 and follows by Taylor expanding e-x2/2 around 0. Simi-
larly, we can also show that P ≤ ɪ.
So this becomes
E h" I I i≥ (√2 (/ τσ√i⅛
=O (√¾) E M II]
Finally, we can compose this, to get
E h 帜"I) lli ≥( M 4 Jk- 1)
with the constant C being the ratio of ||Jx(t)± || to ||6x(t) ||. So if our trajectory direction is almost
orthogonal to χ(t) (which will be the case for e.g. random circular arcs, C can be seen to be ≈ 1 by
splitting into components as in Lemma 1, and using Lemmas 3,4.)
□
Result for non-zero bias In fact, we can easily extend the above result to the case of non-zero
bias. The insight is to note that because δz(d+1) involves taking a difference between z(d+1) (t + dt)
and z(d+1) (t), the bias term does not enter at all into the expression for δz(d+1). So the computations
above hold, and equation (a) becomes
EW(d)[ i i δz(d+ι) i i ] ≥√2 f-σw √kp(I- p)k-1+σw ∙ √1+(kp- 1)p)E " H) i I ]
17
Under review as a conference paper at ICLR 2017
We also now have that hi(d+1) is a normal random variable with variance ≤ σw2 + σb2 (as the bias is
drawn from N(0, σb2)). So equation (b) becomes
P(|hi(d+1)| < 1) ≥
1
Zσ + σ) √2π
This gives Theorem 1
EWTli ≥O((σwτ⅛∕4∙qpfcbMdYi
Statement and Proof of Upper Bound for Trajectory Growth Replace hard-tanh with a linear
coordinate-wise identity map, hi(d+1) = (W(d)z(d))i + bi. This provides an upper bound on the
norm. We also then recover a Chi distribution with k terms, each with standard deviation σw,
k 2
Eh∣∣δz(d+1)∣∣i ≤ √2∙
≤ σw
r((k + 1)/2)σw
Γ(k∕2)	k
(k +
k~^
(2)
(3)
where the second step follows from (Laforgia and Natalini, 2013), and holds for k > 1.
B Proofs and additional results from Section 2.2.2
Proof of Theorem 2
Proof. For σb = 0:
For hidden layer d < n, consider neuron v1(d). This has as input Pik=1 Wi(1d-1)zi(d-1). As we are in
the large σ case, we assume that |zi(d-1) | = 1. Furthermore, as signs for zi(d-1) and Wi(1d-1) are both
completely random, we can also assume wlog that zi(d-1) = 1. For a particular input, we can define
v1(d) as sensitive to vi(d-1) if vi(d-1) transitioning (to wlog -1) will induce a transition in node v1(d).
A sufficient condition for this to happen is if |W∏| ≥ | Pj/ Wji∖. But X = Wn 〜N(0, σ2∕k)
and Pj=i Wjι = Y0 〜N(0, (k 一 1)σ2∕k). So We want to compute P(∖X∖ > ∖Y0∖). For ease of
computation, we instead look at P(∖X∖ > ∖Y∖), where Y 〜N(0,σ2).
But this is the same as computing P(∖X∖∕∖Y∖ > 1) = P(X∕Y < 一1) + P(X∕Y > 1). But
the ratio of two centered independent normals with variances σ12 , σ22 follows a Cauchy distribution,
with parameter σ1∕σ2, which in this case is 1∕√k. Substituting this in to the cdf of the Cauchy
distribution, we get that
P (jɪɪ > 1)= 1 — — arctan(√k)
Finally, using the identity arctan(x) + arctan(1∕x) and the Laurent series for arctan(1∕x), we can
evaluate the right hand side to be O(1∕√k). In particular
P (F > 1)≥ O (√⅛)	(C)
This means that in expectation, any neuron in layer d will be sensitive to the transitions of √k
neurons in the layer below. Using this, and the fact the while vi(d-1) might flip very quickly from
say 一1 to 1, the gradation in the transition ensures that neurons in layer d sensitive to vi(d-1) will
transition at distinct times, we get the desired growth rate in expectation as follows:
Let T (d) be a random variable denoting the number of transitions in layer d. And let Ti(d) be a
random variable denoting the number of transitions of neuron i in layer d. Note that by linearity of
expectation and symmetry, E T(d) = Pi E hTi(d)i = kE hT1(d)i
18
Under review as a conference paper at ICLR 2017
Now, E T1(d+1) ≥ E Pi 1(1,i)Ti(d) = kE 1(1,1)T1(d) where 1(1,i) is the indicator function of
neuron 1 in layer d + 1 being sensitive to neuron i in layer d.
But by the independence of these two events, E
1(1,1)T1(d)
time on the right hand side is O(1/√k) by (c), so putting it all
=E [l(i,i)] ∙ E [τ(d)]. Butthefirt
together, E [r叶1)] ≥ √kE [t*].
Written in terms of the entire layer, we have E [T(d+1)] ≥ √kE [T⑷]as desired.
For σb > 0:
We replace √k with y∕k(1 + σb2∕σW), by noting that Y 〜N(0, σW + σ2). This results in a growth
rate of form O(√k/ Jl + M).	口
C Proofs and additional results from Section 2.3
Proof of Theorem 3
Proof. We show inductively that FW partitions the input space into convex polytopes via hyper-
planes. Consider the image of the input space under the first hidden layer. Each neuron vi(1) defines
hyperplane(s) on the input space: letting Wi(0) be the ith row of W(0), bi(0) the bias, we have the
hyperplane Wi(0)x + bi = 0 for a ReLU and hyperplanes Wi(0)x + bi = ±1 for a hard-tanh. Con-
sidering all such hyperplanes over neurons in the first layer, we get a hyperplane arrangement in the
input space, each polytope corresponding to a specific activation pattern in the first hidden layer.
Now, assume we have partitioned our input space into convex polytopes with hyperplanes from
layers ≤ d-1. Consider vi(d) anda specific polytope Ri. Then the activation pattern on layers ≤ d-1
is constant on Ri, and so the input to vi(d) on Ri is a linear function of the inputs Pj λjxj + b and
some constant term, comprising of the bias and the output of saturated units. Setting this expression
to zero (for ReLUs) or to ±1 (for hard-tanh) again gives a hyperplane equation, but this time, the
equation is only valid in Ri (as we get a different linear function of the inputs in a different region.)
So the defined hyperplane(s) either partition Ri (if they intersect Ri) or the output pattern of vi(d) is
also constant on Ri The theorem then follows.	口
This implies that any one dimensional trajectory x(t), that does not ‘double back’ on itself (i.e.
reenter a polytope it has previously passed through), will not repeat activation patterns. In particular,
after seeing a transition (crossing a hyperplane to a different region in input space) we will never
return to the region we left. A simple example of such a trajectory is a straight line:
Corollary 2. Transitions and Output Patterns in an Affine Trajectory For any affine one dimensional
trajectory x(t) = x0 + t(x1 - x0) input into a neural network FW, we partition R 3 t into intervals
every time a neuron transitions. Every interval has a unique network activation pattern on FW.
Generalizing from a one dimensional trajectory, we can ask how many regions are achieved over
the entire input 一 i.e. how many distinct activation patterns are seen? We first prove a bound on the
number of regions formed by k hyperplanes in Rm (in a purely elementary fashion, unlike the proof
presented in (Stanley, 2011))
Theorem 6. Upper Bound on Regions in a Hyperplane Arrangement Suppose we have k hyper-
planes in Rm - i.e. k equations of form αix = βi. for αi ∈ Rm, βi ∈ R. Let the number of regions
(connected open sets bounded on some sides by the hyperplanes) be r(k, m). Then
m
r(k, m) ≤
i=0
Proof of Theorem 6
Proof. Let the hyperplane arrangement be denoted H, and let H ∈ H be one specific hyperplane.
Then the number of regions in H is precisely the number of regions in H - H plus the number of
19
Under review as a conference paper at ICLR 2017
regions in H ∩ H . (This follows from the fact that H subdivides into two regions exactly all of the
regions in H ∩ H, and does not affect any of the other regions.)
In particular, we have the recursive formula
r(k, m) = r(k - 1, m) + r(k - 1, m - 1)
We now induct on k + m to assert the claim. The base cases of r(1, 0) = r(0, 1) = 1 are trivial, and
assuming the claim for ≤ k + m - 1 as the induction hypothesis, we have
m	k-1 m-1 k-1
r(k- 1,m)+r(k- 1,m-1) ≤	i +	i
i=0 i	i=0 i
≤k-01+dXi=-01k-i1+ki+-11
≤k0+mXi=-01i+k1
where the last equality follows by the well known identity
a	a	a+1
b + b+1 = b+1
This concludes the proof.	口
With this result, we can easily prove Theorem 4 as follows:
Proof. First consider the ReLU case. Each neuron has one hyperplane associated with it, and so by
Theorem 6, the first hidden layer divides up the inputs space into r(k, m) regions, with r(k, m) ≤
O(km).
Now consider the second hidden layer. For every region in the first hidden layer, there is a different
activation pattern in the first layer, and so (as described in the proof of Theorem 3) a different
hyperplane arrangement of k hyperplanes in an m dimensional space, contributing at most r(k, m)
regions.
In particular, the total number of regions in input space as a result of the first and second hidden
layers is ≤ r(k, m) * r(k, m) ≤ O(k2m). Continuing in this way for each of the n hidden layers
gives the O(kmn) bound.
A very similar method works for hard tanh, but here each neuron produces two hyperplanes, result-
ing in a bound of O((2k)mn).
□
D Proofs and additional results from Section 2.4
D.1 Upper B ound for Dichotomies
The Vapnik-Chervonenkis (VC) dimension of a function class is the cardinality of the largest set of
points that it can shatter. The VC dimension provides an upper (worst case) bound on the gener-
alization error for a function class (Vapnik and Vapnik, 1998). Motivated by generalization error,
VC dimension has been studied for neural networks (Sontag, 1998; Bartlett and Maass, 2003). In
(Bartlett et al., 1998) an upper bound on the VC dimension v of a neural network with piecewise
polynomial activation function and binary output is derived. For hard-tanh units, this bound is
v = 2 |W | nlog (4e |W | nk) + 2 |W| n2 log 2 + 2n,	(4)
where |W | is the total number of weights, n is the depth, and k is the width of the network. The
VC dimension provides an upper bound on the number of achievable dichotomies |F| by way of the
20
Under review as a conference paper at ICLR 2017
Figure 10: Here we plot the number of unique dichotomies that have been observed as a function of
the number of transitions the network has undergone. Each datapoint corresponds to the number of
transitions and dichotomies for a hard-tanh network of a different depth, with the weights in the first
layer undergoing interpolation along a great circle trajectory W(0) (t). We compare these plots to a
random walk simulation, where at each transition a single class label is flipped uniformly at random.
Dichotomies are measured over a dataset consisting of s = 15 random samples, and all networks
had weight variance σw2 = 16. The blue dashed line indicates all 2s possible dichotomies.
SaUer-Shelah lemma (Sauer, 1972),
∣F∣≤ (/)v.	(5)
By combining Equations 4 and 5 an upper bound on the number of dichotomies is found, with a
growth rate which is exponential in a low order polynomial of the network size.
Our results further suggest the following conjectures:
Conjecture 1. As network width k increases, the exploration of the space of dichotomies increas-
ingly resembles a simple random walk on a hypercube with dimension equal to the number of inputs
|S|.
This conjecture is supported by Figure 10, which compares the number of unique dichotomies
achieved by networks of various widths to the number of unique dichotomies achieved by a ran-
dom walk. This is further supported by an exponential decrease in autocorrelation length in function
space, derived in our prior work (Poole et al., 2016).
Conjecture 2. The expressive power of a single weight Wi(jd) at layer d in a random network F,
and for a set of random inputs S, is exponential in the remaining network depth dr = (n - d). Here
expressive power is the number of dichotomies achievable by adjusting only that weight.
That is, the expressive power of weights in early layers in a deep hard-tanh network is exponentially
greater than the expressive power of weights in later layers. This is supported by the invariance to
layer number in the recurrence relations used in all proofs directly involving depth. It is also directly
supported by simulation, as illustrated in Figure 5, and by experiments on MNIST and CIFAR10 as
illustrated in Figures 6, 7.
E	Further Results and implementation details from Section 3
We implemented the random network architecture described in Section 2.1. In separate experiments
we then swept an input vector along a great circle trajectory (a rotation) for fixed weights, and swept
weights along a great circle trajectory for a fixed set of inputs, as described in Section 2.4. In both
cases, the trajectory was subdivided into 106 segments. We repeated this for a grid of network
widths k, weight variances σw2 , and number of inputs s. Unless otherwise noted, σb = 0 for all
experiments. We repeated each experiment 10 times and averaged over the results. The simulation
results are discussed and plotted throughout the text.
The networks trained on MNIST and CIFAR-10 were implemented using Keras and Tensorflow, and
trained for a fixed number of epochs with the ADAM optimizer.
21
Under review as a conference paper at ICLR 2017
20
Transitions Count during Training
MNIST data	Random data
,5W
-C=OO suo8u
6	8	10	12 0	2	4	6	8	10	12
Layers
An identical plot to Figure 8 but for transition count.
Train AccuracyAgainst Epoch
9ejn∞v
Figure 12: We repeat the experiment in Figure 6 for a convolutional network trained on CIFAR-
10. The network has eight convolutional hidden layers, with three by three filters and 64 filters in
each layer, all with ReLU activations. The final layer is a fully connected softmax, and is trained in
addition to the single convolutional layer being trained. The results again support greater expressive
power with remaining depth. Note the final three convolutional layers failed to effectively train, and
performed at chance level.
We also have preliminary experimental results on Convolutional Networks. To try and make the
comparisons fair, we implemented a fully convolutional network (no fully connected layers except
for the last layer).
We also include the plot showing the effect of training on number of transitions for interpolated
MNIST and interpolated random points.
Ep8h Number
Test Accuracy Against Epoch
100	200	3∞	400	500
Epoch Number
22