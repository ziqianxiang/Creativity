Under review as a conference paper at ICLR 2017
Learning to Understand: Incorporating Lo-
cal Contexts with Global Attention for Sen-
timent Classification
Zhigang Yuan & Yuting Hu
Department of Electronic Engineering
Tsinghua University
Beijing, China
{yuanzg14, hu-yt12}@mails.tsinghua.edu.cn
Yongfeng Huang
Tsinghua National Laboratory for Information Science and Technology
Department of Electronic Engineering
Tsinghua University
Beijing, China
yfhuang@tsinghua.edu.cn
Ab stract
Recurrent neural networks have shown their ability to construct sentence or para-
graph representations. Variants such as LSTM overcome the problem of vanishing
gradients to some degree, thus being able to model long-time dependency. Still,
these recurrent based models lack the ability of capturing complex semantic com-
positions. To address this problem, we propose a model which can incorporate
local contexts with the guide of global context attention. Both the local and global
contexts are obtained through LSTM networks. The working procedure of this
model is just like how we human beings read a text and then answer a related
question. Empirical studies show that the proposed model can achieve state of the
art on some benchmark datasets. Attention visualization also verifies our intuition.
Meanwhile, this model does not need pretrained embeddings to get good results.
1	Introduction
Sentiment classification, a key problem in sentiment analysis, has drawn lots of interest since 2000s
(Pang & Lee, 2008). Traditional machine learning based methods often use one-hot features to
represent a text. In these methods, each word (or n-gram) is treated as an independent token, and a
text is represented as the words within. Though convenient intuitively, these methods cannot model
the semantic dependencies between words, and often lead to the curse of dimensionality problem.
Neural models have shown their great performance in learning representation. Bengio et al. (2003)
proposed the feed forward neural network language model. Ever since, many neural network based
models have been widely used in many natural language processing (NLP) tasks, such as named en-
tity recognization (Collobert & Weston, 2008), machine translation (Cho et al., 2014) and sentiment
analysis (dos Santos & Gatti, 2014). Compared to one-hot representation, words are represented
as distributed, dense vectors (also called word embeddings) in these neural models. Mikolov et al.
(2013b) showed that, through proper training, these vectors can model syntactic and semantic rela-
tionships between words. More importantly, these vectors can be calculated naturally, which leads
to another problem — semantic composition.
For many NLP tasks, we often face the problem of representing a sentence or paragragh using word
embeddings of its containing words. Simply methods such as concatenation or weighted sum cannot
give us satisfactory results. Many neural models, such as recursive neural network (Socher et al.,
2011), recurrent neural network (Mikolov et al., 2010) and convolutional neural networks (Collobert
& Weston, 2008) have been proposed to solve this problem.
1
Under review as a conference paper at ICLR 2017
Since a sentence or paragraph is naturally a sequence of word tokens, recurrent neural networks
(RNN) can be used to composite word embeddings into a sentence or paragraph embedding. Recur-
rent networks utilize special units which are connected to themselves to maintain a hidden state. The
internal hidden state can be seen as a representation of the contexts. However, basic recurrent net-
work suffers from the vanishing or exploding gradients problem (Hochreiter et al., 2001), due to its
direct connection between states in adjacent timesteps. Long short term memory (LSTM) network
alleviates this problem by introducing a cell that has the ability to read, write or reset its internal
state (Graves, 2012). Even so, LSTM has the bias of prefering recent inputs, making it difficult to
capture the important information within a long sequence.
Recently, many studies have been done to help RNNs improve their semantic composition perfor-
mance. A common way is to introduce another kind of network, such as CNN into RNN (Lai et al.,
2015). In these combined models, different parts share complementary advantages. Lately, the at-
tention mechanism (Bahdanau et al., 2014) has drawn a lot of attention. In an attention model, we
utilize states in all timesteps, rather than simply force the network to represent the context into one
fixed-length vector. More importantly, by visualizing the attention weights, we can get a deeper
insight of how the network works (BahdanaU et al., 2014; Rocktaschel et al., 2015).
Inspired by the attention thought, we propose a framework which is similar to how we human beings
read a text and then answer an related qUestion. This framework consists of two parts: the first part
is a bidirectional LSTM (Bi-LSTM) to learn a roUgh representation of the whole text, the second
part is another Bi-LSTM with attention, which can learn local contexts and incorporate them to get
a refined context representation.
2	Related Work
In this section, we introdUce the related work concerning sentiment analysis, neUral semantic com-
position and attention mechanism.
2.1	Sentiment Analysis
Sentiment analysis, also called opinion mining or sUbjectivity analysis, is the field of stUdy that
analyzes people’s sentiments or emotions towards varioUs entities (LiU, 2012). DUe to its great
importance, sentiment analysis has a wide range of applications, sUch as finding opinions of certain
prodUcts, predicting the stock market or political elections.
Generally, existing methods for sentiment classification fall into two categories: machine-learning-
based methods and semantic-based methods. Semantic methods relies more on lexcial and lingUistic
resoUrces. Pang et al. (2002) first tried Using machine learning methods to classifiy texts based on
their sentiments. Ever since, a lot more classification models and featUres have been tested. For
machine learning procedUres, featUre design is always the key step for a good resUlt. While in most
cases, a well-designed featUre needs mUch hUman effort and domain knowledge, which severely
limits its applications.
Deep learning models have drawn lots of attention dUe to their aUtomatic representation learning.
Deep learning is aboUt learning mUltiple levels of representations, obtained by combining non-linear
modUles (LeCUn et al., 2015). VarioUs models have been tested for sentiment analysis task, sUch as
recUrsive aUtoencoder (Socher et al., 2011), paragraph vector (Le & Mikolov, 2014), convolUtional
network (Kim, 2014), etc.
2.2	Neural Language Models and Semantic Composition
Bengio et al. (2003) proposed a feed-forward neUral network langUage model, which Used distribUted
representations for words. With the rapid progress of neUral langUage models for NLP tasks, the
distribUted representations of words have become more and more important.
In most neUral langUage models, the words (or even characters) are represented as dense, distribUted
vectors. These embeddings are UsUally Used as the inpUts of a network. The sUbseqUent layers then
condUct semantic composition operations over these word/character embeddings to obtain higher
level semantic represetations. This kind of representations based on embeddings of different seman-
2
Under review as a conference paper at ICLR 2017
tic levels (character, word, phrase, sentence, paragraph, etc.) have shown their great performance on
many NLP tasks.
After Bengio’s pioneering work, Collobert & Weston (2008) replaced the probabilistic output with
a scoring scheme, and provided a unified architecture for many NLP tasks. Mnih & Hinton (2007)
used a log-bilinear model to get the hidden representations. They future provided a hierarchical
version (Mnih & Hinton, 2009), which uses a hierarchical softmax at the output layer to reduce
calculation amount. Huang et al. (2012) added the global context information into C&W model,
which can improve word embeddings as well as solve the polysemy problem. Their idea of using
global context inspired our work, while in our work we use the global context as attention for the
following training. Socher et al. (2011; 2012; 2013) proposed a family of recursive autoencoder
models, which construct higher level represetations using a tree structure. Mikolov et al. (2013a)
provided their word2vec (CBOW and Skip-gram) model. The word2vec model is actually a simple
but effective log-linear model, imbibing advantages of previous neural models. Word embeddings
pretrained using word2vec on a large corpus can be used for many other NLP tasks, often used as
initializations or auxiliary features. Pennington et al. (2014) performed the training on aggregated
global word-word co-occurrence statistics from a corpus, and their resulting global vectors (GloVe)
showcase interesting linear substructures of the word vector space. Le & Mikolov (2014) extended
the idea of word2vec model from word to paragragh, and the paragraph vectors they got can improve
the performance on various NLP tasks.
Although seemingly different, these models all conduct semantic compositions directly or indirectly
at intermediate layers. Currently, various neural models have been tried for semantic composition,
such as multi-layer perceptron, recusive network, convolutional network and recurrent network.
When representing higher level language units, convolutinal netwoks are better at capturing local
information, while recurrent networks excel at storing history information. The network structure
has also been explored, such as sequencial or tree structure (Li et al., 2015).
Of all these models, recurrent networks are highly valued due to its human alike reading style.
More importantly, recurrent networks are capable of modeling sequencial inputs or outputs, which is
essential for tasks such as machine translation (Cho et al., 2014). When applying recurrent networks
for NLP tasks, it is typical to encode the semantic information of all words within a sentence into
one fixed-length vector. The resulting representation is then used for generating related output words
or simply used for classification.
2.3	Attention Mechanism
Attention mechanism is first used for machine translation (Bahdanau et al., 2014) in a encoder-
decoder framework. The key idea of attention is to allow the network revisit all parts of a source
sentence for an output decision, instead of trying to encode all information of a source sentence into
a fixed-length vector. By this mechanism, we can get a deeper insight of how the network works.
In the work of Bahdanau et al. (2014), they found the alignment between words by visualizing the
attention weights.
Successive studies on attention mechanism include image caption (Xu et al., 2015), semantic entail-
ment (Rocktaschel et al., 2015), aspect level sentiment analysis (Tang et al., 2016), etc. The most
related work to ours is an attention model for sequence classification (Shen & Lee, 2016), in which
they used LSTM output to weight word embeddings. While in our work, we use the global context
to selectively choose the most important local contexts, which provides mutual promotions for both
global and local context representations.
3	Learning to Understand: The Model
It’s widely accepted that RNN alone, even with LSTM cells, cannot store long-time information.
But, we may ask, do we really have to get a refined document or text embedding? Taking the reading
comprehension task as an example, after a quick look at the text given, what we human beings got is
also a rough memory of this text. Even so, this did not affect us to make a right decision. What we
typically do next is to look at the problem, and rescan the text with a rough memory of the whole
text in mind. With this global context information, we can easily locate the important parts for our
3
Under review as a conference paper at ICLR 2017
problem. In the end, even if we have not understood the whole text thoroughly, we can make right
decisions for the problem in hand.
This thought inspired us to propose this attention model, which incorporates local contexts with a
rough global context attention.
3.1	BASIC MODEL: BI-LSTM
Long short term memory network can be seen as a variant of the basic recurrent network, mainly to
address the long-time dependency probem. It has special units that can pass their states to the next
timestep. LSTM network replaces the hidden units with more flexible cells. The cell itself is a small
network, which can decide its own reading, writing or resetting behavior.
The LSTM network we use is similar to Graves (2012). We don’t consider peephole connections.
Given an input sequence S = (χ1,χ2, ∙ ∙ ∙ ,Xn), (Xi ∈ Rm), the update procedure from timestep
t - 1 to t can be described as follows:
it	=	σ(Wixxt + Uihht-1 + bi)	(1)
ft	=	σ(Wfxxt + Ufhht-1 + bf)	(2)
ot	=	σ(Woxxt + Uohht-1 + bo)	(3)
Ct	=	φ(w Cxxt + U chht-1 + bc)	(4)
Ct	=	ft Θ Ct-1 + it © Ct	(5)
ht	=	ot φ(ct)	(6)
where Θ denotes the element-wise multiplication, it , ft, ot ∈ Rh stands for the input gate, forget
gate, output gate respectively. The LSTM cell maintains an internal cell state. In one timestep, the
input gate determines how much the outside information can come in, the output gate determines
how much the internal information can go out, and the forget gate determines how much the internal
information should be forgotten when passed to next timestep.
For a given sequence S = (x1,x2, ∙ ∙ ∙ ,xn), if we feed the token from left to right one by one into
the LSTM network, the final internal state -→h will contain more information of the latter tokens due
to the model bias. We feed the sequences backwards into the LSTM again, thus getting a backward-
propagating internal state 汇 The final representation of the global context is calculated by
h = →㊉力	(7)
where ㊉ denotes concatenation. This is a typical bidirectional LSTM (Bi-LSTM) network. Again,
we point out that this gloal context representation may be just a rough one. We use it as attention
rather than directly treat it as a refined representation for the task afterwards.
3.2	Two-Scan Approach with Attention
In this part, we introduce our Two-Scan Approach with Attention (TS-ATT) model.
The TS-ATT model contains two parts: a Bi-LSTM network first scans the whole text to get a global
context represetation, another Bi-LSTM with attention scans the text again to get local contexts
around each token, and composite them using the global context attention. The model are shown in
Figure (1).
First Scan: we first use a Bi-LSTM network to gain a global context representation. As described
in Sec. (3.1), the final global context vector h is a concatention of the final output of left-LSTM 力
and right-LSTM -→h . This global context serves as attention for the second scan. To distinguish the
final context representation in the second scan, we denote h as hscan1, which is
hscani =%㊉ →	(8)
4
Under review as a conference paper at ICLR 2017
hscan1
Global Attention I
I l	Hxn■
Ila
Word
Embedding
Figure 1: Two-Scan Approach Attention Model (TS-ATT)
Second Scan: During the second scan of the text, we use another Bi-LSTM networks to learn local
context for each word. Since already with the rough global context in mind, when looking at one
word, we actually see its context meaning rather than the word itself. Thus, we use a word’s local
context vector instead of its word embedding here. Inspired by the work of Lai et al. (2015), for
(l)	(r)
word wi, we concatenate its left context ci , right context ci and embedding ei to construct its
final local context represetation ci , which is:
ci(l)	=	f W (lc)ci(+l)1 +W(le)ei
ci(r)	=	f W(rc)ci(-r)1+W(re)ei
Ci =	c(l ㊉ ei ㊉ c(r)
(9)
(10)
(11)
Attention: As we may expect, during the second scan, we don’t have to encode every word into
the text representation. Just paying attention to the parts which are important for our problem in
hand would be enough. Thus, we use the global context vector hscan1 as the attention of all the local
contexts. Attention weights are acquired by one-layer feed-forward network, which is:
yatt = f (Watt(hscan1 ㊉ Ci) + batt)
(12)
where Watt and batt are attention parameters. All the attention weights are then fed into a soft-
max layer to obtain probabilistic attention weights. The final representation used for classification
(denoted as hscan2) is the weighted sum of all the local context vectors, which is:
αi
n
αiCi
i=1
hscan2
(13)
(14)
The context vector hscan2 is then fed into a fully-connected layer and a softmax layer to do the
classification task.
The whole network is training to minimize the cross-entropy error E = 焉 PN=I H(y|p) =
--N PN=I pn=1 yj log Pj over all training examples, where P denotes the predictions and y the
true labels. Parameters are updated using the RMSProp (Tieleman & Hinton, 2012) rule.
5
Under review as a conference paper at ICLR 2017
3.3	Single-Scan Approach with Attention
In the proposed TS-ATT model, we use two Bi-LSTM netwoks to conduct these two scans. However,
sometimes we may adopt another faster, but may equally good approach: a single scan. In this way,
we first look at the problems to be solved, then scan the original text looking for clues. During this
process, we maintain a rough gloal context and pay attention to the important parts simultaneously.
Inspired by this thought, we proposed the Single-Scan Approach with Attention (SS-ATT) model,
which is a reduced version of TS-ATT model. In this model, the final outputs of the second Bi-
LSTM are directly used as attention, saving the need of the first Bi-LSTM network. The SS-ATT
model is shown in Figure (2).
Word
Embedding
Figure 2: Single-Scan Approach Attention Model (TS-ATT)
4	Experiments
We evaluated our models on some benchmark datasets for sentiment classification. These datasets
includes:
•	Amazon1 by Blitzer et al. (2007). Amazon dataset contains product reviews from Ama-
zon.com from many product types. This dataset is origially used for domain adaption, here,
we only use it for open-domain classification, regardless of the domain difference.
•	IMDB2 by Maas et al. (2011), a large movie review dataset collected from imdb.com web-
site.
•	Yelp 20133 User reviews and recommendations dataset, which are built by Tang et al.
(2015).
The statistical information of these datasets are listed in Table (1).
Table 1: Statistical Information of Datasets
dataset	# classes	# docs	Avg doc length	# words	# Train/(Val/)/Test
Amazon	2	8,000	^^25	42,266	6,400 / 1,600
IMDB	2	50,000	228	88,584	25,000 / 25,000
Yelp2013	5	78,966	161	73,835	62,522 / 7,773 / 8,671
1https://www.cs.jhu.edu/~mdredze∕datasets∕sentiment∕
2http://ai.stanford.edu/~amaas∕data∕sentiment∕
3http://ir.hit.edu.cn/~dytang/
6
Under review as a conference paper at ICLR 2017
For IMDB, the original dataset already has a train/test split, we stick to this behavior in our exper-
iments. For Amazon, we split 20% of the data as the test set. For Yelp datasets, they also have a
train/validation/test split.
For datasets without an explicit validation split (like IMDB and Amazon), we randomly select 10%
of the training data as the validation data, which is used for hyperparameter tuning and early-stop in
training.
4.1	Baselines
We compare our attention model with some models related to our work. These models contain tra-
ditional machine learning methods such as support vector machine (SVM) as well as neural models
such as LSTM.
Traditional machine learning methods:
(1)	SVM+BOW: Support Vector Machine with Bag Of Words representation. SVM uses hinge loss
to maximize the margin between different classes.
(2)	NB+BOW: Naive Bayes with Bag Of Words representation. NB chooses the class with the
largest posterior probability, which is calculated through priori probability and likelihood based on
training data.
Neural model methods:
(3)	CNN: Convolutional Neural Network for sentence classification model proposed by Kim (2014).
We use 100 filters to evaluation this model.
(4)	LSTM: Long Short Term Memory network (Graves, 2012)
(5)	Bi-LSTM: Bi-directional LSTM network (Graves, 2012)
(6)	RCNN: Recurrent Convolutional Neural Network proposed by Lai et al. (2015), which uses
a bi-directional recurrent network to replace the convolution operation in a typical convolutional
network.
(7)	NAM: Neural Attention Model proposed by Shen & Lee (2016). This model uses LSTM output
as attention directly for word embeddings.
4.2	Setup
For texts of these datasets, we split them into word tokens using punctuations and spaces as delim-
iters. Unlike some studies earlier, we process each text as a single unit rather than split them into
sentences, which actually provides us more challenges. We process texts this way mainly to test the
model’s ability to construct representation of long texts, which requires the model to learn long-time
dependency.
For the neural models, we set the word embedding dimention to be 300, and LSTM state dimention
to be 300. For training, we use stochastic gradient descent based on mini-batches with a batch size of
32. The training process is monitored by the validation accuracy for early-stopping. After training,
the model is evaluated by a separate test dataset.
Due to the different pre-processing and model parameters selecting scheme, our results may not
look the same as they are reported in other papers. For a fair comparison, we rerun all the models
using the same platform of our experiment. No tricks such as dropout, batch-normaliztion are used,
since we want a comparison of these models themselves. Similarly, we use random initial word
embeddings instead of pretrained embeddings such as word2vec (Mikolov et al., 2013a).
4.3	Results and Discussions
Our experimental results are shown in Table (2).
7
Under review as a conference paper at ICLR 2017
Table 2: Classification Accuracy (in percentage)
Model	Amazon	IMDB	Yelp2013
SVM+BOW	80.31	84.88	-
NB+BOW	80.75	82.98	-
CNN (Kim, 2014)	83.94	85.68	55.67
LSTM (Graves, 2012)	79.50	82.00	56.74
Bi-LSTM (Graves, 2012)	80.13	82.54	56.35
RCNN (Lai et al., 2015)	81.38	82.75	57.21
NAM (Shen & Lee, 2016)	79.23	86.02	57.05
TS-ATT	82.19	86.25	58.66
SS-ATT	83.25	86.74	58.38
As shown in Table (2), our methods (TS-ATT or SS-ATT) achieve the best accuracy on two of three
datasets (IMDN and Yelp2013). For Amazon dataset, the result of our method is almost as good as
the best model (CNN). These results demonstrate the effectiveness of our proposed models.
From Tabel (2), we can see CNN shows good performance in these sentiment classification tasks.
But, CNN has many hyper-parameters to tune, such as filter numbers, filter size, pooling size, etc.
Changes of these hyper-parameters affect CNN’s performance to a large scale. Zhang & Wallace
(2015) did a sensitivity analysis of CNNs for sentence classification. Compared with CNNs, recur-
rent based networks have fewer hypermeters to fine-tune. Even so, basic neural network such as
LSTM doesn’t perform so well, as shown in Table (2). Bi-LSTM fails to make significant improve-
ments on these tasks.
Comparing with RCNN, which uses a max-pooling operation after an Bi-LSTM, our model surpass
it on all three datasets (1.87%, 3.99% and 1.45% respectively). This result demonstrates the effec-
tiveness of using global context (though a rough one) as attention. Further, our SS-ATT model can
be seen as a single Bi-LSTM network with global attention. The great improvement compared to
Bi-LSTM also witnesses the key role that global attention plays.
Comparing with NAM, which use global attention directly on word embeddings, our model also
achieves big or small improvements (4.02%, 0.72% and 1.61% respectively). These improvements
over NAM model indicate that incorporating local contexts with attention would be more suitable
for text understanding.
As for our proposed models, we can see TS-ATT and SS-ATT can achieve approximately equally
good results. The key difference of these two models is that SS-ATT uses only one Bi-LSTM, im-
proving global and local context representation mutually when training. These results are consistent
with our intuition of human-like reading style. Imagine when in a reading comprehension test, we
may skip reading the whole texts at first, but directly look at the texts after knowing the questions.
In this way, we are actually using single-scan approach to save time. But as long as we have the big
picture (global context) in mind, this kind of process won’t harm our right decisions.
5 Attention Visualization and Case Study
In order to get a deeper insight into our global-local context attention model, we track the attention
weights when the model is trying to make a decision. Take the IMDB dataset as example, when
evaluating our model, we accumulate the attention weights for each word appeared, then calculate
their average attentions. By this way, we can get a better knowledge about what kind of words this
model favors when trying to predict the sentiment of a text. The words with largest average attention
are shown in Table (3). We select those words with a minimal appearing frequency (for IMDB we
set it to 100, change of this number won’t affect the results so much).
From Table (3), we can see that our attention model can effectively find the most discriminative
features. Word such as surprised is actually negative in most cases, while in our model it’s
classified as positive correctly, since we usually use surprised to express our pleasance when
some movie is beyond our expectation. Also, we notice that the word recommended is favored by
8
Under review as a conference paper at ICLR 2017
Table 3: Words with the largest average attention
Sentiment	Words with largest average attention in IMDB dataset
positive	recommended, surprised, underrated, excellent, fascinating favorites, incredible, terrific, beautifully, funniest, amazing
negative	avoid, blah, disappointment, waste, awful, disappointing, recommended dreadful, worst, lousy, miserably, horrible, boredom, pretentious
both polarities. In this case, a model has to rely its local context (such as not recommended or
recommended) to make a right decision. This will be illustrated in following case study.
Since our method incorporates local contexts to work, we next provide some case studies where
important local contexts are automatically selected for right decisions. We use cases where the word
recommended serving a high attention and then dive into its surrounding words to support our
thought above.
Figure (3) shows the attention weights of each word in the sample sentence segment or music of the
and greeks - this movie is strongly recommended for lovers of the music. Figure (4) shows another
negative sentence segment the new zealand film commission fourthly - a friend recommended it to
me - however i was utterly disappointed.
Figure 4: A negtive example
Figure 3: A positive example
From Figure (3) and Figure (4), we can see that the model actually concentrations on important
local contexts rather than single embeddings. This further verifies our initial idea that, using a
global context to incorporate local contexts is a suitable way for sentiment or more general text
classification task.
6 Conclusions
In this paper, we proposed a global-local context attention framework for sentiment analysis. This
method is similar to human’s reading behavior in a reading comprehension situation. First, this
model uses a Bi-LSTM network to extract a global context representation. This global representation
maybe inaccurate, but we only use it as attention for important local parts. In a second scan of the
text, the model automatically finds the most important local contexts and incorporates them to make
a final decision. A simple version of this model only needs one-scan of text, which can reduce the
model complexity but almost equally effective. Experimental results demonstrate that our model
performs better than existing related models. Using global context representation as attention, our
model can effectively find the most important local parts to make a right decision.
9
Under review as a conference paper at ICLR 2017
Acknowledgments
This research is supported by the Key Program of National Natural Science Foundation of China
(Grant nos. U1536201 and U1405254), the National Natural Science Foundation of China (Grant
no. 61472092), the National High Technology Research and Development Program of China (863
Program) (Grant no. 2015AA020101), the National Science and Technology Support Program of
China (Grant no. 2014BAH41B00), and the Initiative Scientific Research Program of Tsinghua
University.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
language model. journal ofmaChine learning research, 3(Feb):1137-1155, 2003.
John Blitzer, Mark Dredze, Fernando Pereira, et al. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classification. In ACL, volume 7, pp. 440-447, 2007.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th international conference on
Machine learning, pp. 160-167. ACM, 2008.
Cicero Nogueira dos Santos and Maira Gatti. Deep convolutional neural networks for sentiment
analysis of short texts. In COLING, pp. 69-78, 2014.
Alex Graves. Neural networks. In Supervised Sequence Labelling with Recurrent Neural Networks,
pp. 15-35. Springer, 2012.
SePP Hochreiter, Yoshua Bengio, Paolo Frasconi, and JUrgen Schmidhuber. Gradient flow in recur-
rent nets: the difficulty of learning long-term dependencies, 2001.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word rep-
resentations via global context and multiple word prototypes. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pp. 873-882.
Association for Computational Linguistics, 2012.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint
arXiv:1408.5882, 2014.
Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. Recurrent convolutional neural networks for text
classification. In AAAI, pp. 2267-2273, 2015.
Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML,
volume 14, pp. 1188-1196, 2014.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Jiwei Li, Minh-Thang Luong, Dan Jurafsky, and Eudard Hovy. When are tree structures necessary
for deep learning of representations? arXiv preprint arXiv:1503.00185, 2015.
Bing Liu. Sentiment analysis and opinion mining. Synthesis lectures on human language technolo-
gies, 5(1):1-167, 2012.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp.
142-150. Association for Computational Linguistics, 2011.
10
Under review as a conference paper at ICLR 2017
Tomas Mikolov, Martin Karafiat, LUkas BUrgeL Jan Cernocky, and Sanjeev KhUdanpur. Recurrent
neural network based language model. In Interspeech, volume 2, pp. 3, 2010.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013a.
Tomas Mikolov, Ilya SUtskever, Kai Chen, Greg S Corrado, and Jeff Dean. DistribUted represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing Systems, pp. 3111-3119, 2013b.
Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical langUage modelling.
In Proceedings of the 24th international conference on Machine learning, pp. 641-648. ACM,
2007.
Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distribUted langUage model. In Ad-
vances in neural information processing systems, pp. 1081-1088, 2009.
Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1-135, 2008.
Bo Pang, Lillian Lee, and ShivakUmar Vaithyanathan. ThUmbs Up?: sentiment classification Using
machine learning techniqUes. In Proceedings of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pp. 79-86. Association for CompUtational LingUistics,
2002.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In EMNLP, volUme 14, pp. 1532-43, 2014.
Tim RocktascheL Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.
Reasoning aboUt entailment with neUral attention. arXiv preprint arXiv:1509.06664, 2015.
Sheng-syun Shen and Hung-yi Lee. Neural attention models for sequence classification: Analysis
and application to key term extraction and dialogue act detection. CoRR, abs/1604.00077, 2016.
Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning.
Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of
the Conference on Empirical Methods in Natural Language Processing, pp. 151-161. Association
for Computational Linguistics, 2011.
Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. Semantic composi-
tionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Processing and Computational Natural Language
Learning, pp. 1201-1211. Association for Computational Linguistics, 2012.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on empirical methods in natural language processing
(EMNLP), volume 1631, pp. 1642. Citeseer, 2013.
Duyu Tang, Bing Qin, and Ting Liu. Learning semantic representations of users and products for
document level sentiment classification. In Proc. ACL, 2015.
Duyu Tang, Bing Qin, and Ting Liu. Aspect level sentiment classification with deep memory net-
work. arXiv preprint arXiv:1605.08900, 2016.
T Tieleman and G Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning.
Technical report, Technical report, 2012.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov,
Richard S Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. arXiv preprint arXiv:1502.03044, 2(3):5, 2015.
Ye Zhang and Byron Wallace. A sensitivity analysis of (and practitioners’ guide to) convolutional
neural networks for sentence classification. arXiv preprint arXiv:1510.03820, 2015.
11