Under review as a conference paper at ICLR 2017
Memory-augmented Attention Modelling for
Videos
Rasool Fakoor*, Abdel-rahman MohamedJ*, Margaret Mitchell计,Sing Bing Kangtt,
Pushmeet Kohli**
**Microsoft Research tUniversity of Texas at Arlington 计 Google
*rasool.fakoor@mavs.uta.edu,计{asamir, Singbing.kang, PkOhli}@microsoft.com 料mmitchellai@google.com,
Ab stract
Recent works on neural architectures have demonstrated the utility of attention
mechanisms for a wide variety of tasks. Attention models used for Problems such
as image caPtioning tyPically dePend on the image under consideration, as well
as the Previous sequence of words that come before the word currently being gen-
erated. While these tyPes of models have Produced imPressive results, they are
not able to model the higher-order interactions involved in Problems such as video
descriPtion/caPtioning, where the relationshiP between Parts of the video and the
concePts being dePicted is comPlex. Motivated by these observations, we ProPose
a novel memory-based attention model for video descriPtion. Our model utilizes
memories of Past attention when reasoning about where to attend to in the current
time steP, similar to the central executive system ProPosed in human cognition
(Baddeley & Hitch, 1974). This allows the model to not only reason about lo-
cal attention more effectively, it allows it to consider the entire sequence of video
frames while generating each word. Evaluation on the challenging and PoPular
MSVD and Charades datasets show that the ProPosed architecture outPerforms all
Previously ProPosed methods and leads to anew state of the art results in the video
descriPtion.
1	Introduction
DeeP neural architectures have led to remarkable Progress in comPuter vision and natural language
understanding Problems. Image caPtioning is one such aPPlication that has seen the combination
of convolutional structures (Krizhevsky et al., 2012; LeCun et al., 1998), which have been shown
to be very effective for Problems like object detection, with sequential recurrent structures, which
have been shown to be very effective for Problems like machine translation (Sutskever et al., 2014).
One of the key modelling Paradigms shared by most models for image caPtioning is the notion ofan
attention mechanism that guide the model to attend to certain Parts of the image while generating.
The attention models used for Problems such as image caPtioning tyPically dePend on the single
image under consideration and the Partial outPut generated so far, jointly caPturing one region of an
image and the words being generated. However, such models cannot caPture the temPoral reasoning
necessary to effectively Produce words that refer to actions and events taking Place over multiPle
frames in a video. For examPle, in a video dePicting “someone waving a hand”, the “waving” action
can start from any frame and can continue on for a variable number of following frames. More
imPortantly, it is likely in a given video quite few frames do not contain any useful information or
motion in regard to a given task. Given this, it is not surPrising that even with recent advancements
in image caPtioning Xu et al. (2015a); Johnson et al. (2016); Vinyals et al. (2015), video caPtioning
has remained challenging.
Motivated by these observations, we introduce a memory-based attention mechanism for video caP-
tioning and descriPtion. Our model utilizes memories of Past attention in the video when reasoning
about where to attend to in a current time steP. This allows the model to not only effectively lever-
age local attention, but also to consider the entire video as it generates each word. This mechanism
is similar to the ProPosed central executive system in human cognition, which is thought to Permit
human Performance on two simultaneous tasks (e.g., seeing and saying) using two seParate PerceP-
1
Under review as a conference paper at ICLR 2017
tual domains (e.g., visual and linguistic) by binding information from both sources into coherent
structure that enables coordination, selective attention, and inhibition.
Our work shares the same goals as recent work on attention mechanisms for sequence-to-sequence
architectures, such as Rocktaschel et al. (2016) and Yang et al. (2016). However, there are major
differences between this work and our current work. RoCktaSCheI et al. (2016) considers the domain
of entailment relations, where the goal is to determine entailment given two input sentences. They
propose a soft attention model that is not only focused on the current state, but the previous as
well. In our model, we explicitly store all previous attention into memory. In addition, our memory
memorizes the encoded version of the input videos conditioned on previously seen words. Even
though Yang et al. (2016) and our work both try to solve the problem of locality of attention, our
work is different from them in how the memory architecture is modelled. More specifically, they
incorporate discriminative supervision into their ”reviewer” mechanism, which is not the case in our
model. Further, their model is applied to image caption generation, which is to some extent simpler
than video caption generation because there is no temporal structure to model.
We apply our model on the video captioning problem and evaluate it on the MSVD (Chen & Dolan,
2011) and the Charades (Sigurdsson et al., 2016) datasets. Experimental results show that our pro-
posed architecture outperforms all previous methods and leads to new state of the art results. While
we have chosen the video captioning problem for our experiment, the model is general enough that
it can be applied to other problems where attention models are used.
2	Related Work
One of the primary challenges in learning a mapping from a visual space (i.e., video or image)
to a language space is learning a representation that not only effectively represents each of these
modalities, but is also able to translate a representation from one space to the other. Rohrbach et al.
(2013) developed a model that generates a semantic representation of visual content that can be used
as the source language for the language generation module. Venugopalan et al. (2015b) proposed a
deep method to translate a video into a sentence where an entire video is represented with a single
vector based on the mean pool of frame features. However, representing a video by an average of its
frames misses the temporal structure of the video. To address this problem, recent work (Yao et al.,
2015; Pan et al., 2016a; Venugopalan et al., 2015a; Andrew Shin, 2016; Pan et al., 2016b; Xu et al.,
2015b; Ballas et al., 2016; Yu et al., 2016) proposed methods to model temporal structure of video
as well as language.
The majority of these methods are inspired by sequence to sequence (Sutskever et al., 2014) and
attention (Bahdanau et al., 2015) models. Sequence learning (Sutskever et al., 2014) was originally
proposed to map the input sequence ofa source language to a target language. Even though applying
this method with a combination of attention to the problem of translating a video to a description
shows promising results, there are some shortcomings. First of all, modelling the video content with
a fixed-length vector in order to map it to a language space is a much harder problem than mapping
from a language to a language given the complexity of visual content. Since not all frames in a video
are equally salient for a short description, and an event can happen in multiple frames, it is important
for a model to identify which frames are most salient. Further, the model should be able to focus on
points of interest within these frames to select what to talk about. Even using a variable-length vector
to represent a video using attention (Yao et al., 2015) can have some problems. More specifically,
current attention methods are local Yang et al. (2016), since the attention mechanism works in a
sequential structure, and lacks the ability to capture global structure. Moreover, combining a video
and a language description as a sequence to sequence is usually done by some variant of a recurrent
neural network (RNN) (Hochreiter & Schmidhuber, 1997). Given the limited capacity of a recurrent
network to model very long sequences, memory networks (Weston et al., 2014; Sukhbaatar et al.,
2015) have been introduced to help the RNN memorize sequences. However, one problem these
memory networks suffer from is the difficulty in training the model. The model in Weston et al.
(2014) requires supervision at each layer which makes training with backpropagation a challenging
task. Even though Sukhbaatar et al. (2015) proposed a memory network that can be trained end-
to-end, working with memory is still a challenging problem in deep learning especially with write
operation (Graves et al., 2014).
2
Under review as a conference paper at ICLR 2017
Figure 1: Our proposed architecture. Each component of our model is described in 3.1 through 3.3
To address these problems, we propose a memory-based attention sequence-to-sequence model that
not only can learn hierarchical attention relationships, but provides a simple and effective memory
structure. In the next section, we explain our model in more detail.
3	Learning to Attend and Memorize
Our goal is to design an architecture that learns where to look and what to look for in a video, in
order to talk about it in the description. To achieve this goal, we formulate the problem as sequence
learning to maximize the probability of generating a correct description given a video:
Θ* = argmax X	log P(S∣f1,f2,…，/n；。)	⑴
Θ
Θ	(S,f1,f2, ,fN)
where S is the description, f1, f2, ..., fN are the input video frames, and Θ is the model parameter
vector. The main modelling challenges in video description are to develop a system that can model
the temporal structure of the video, learn to attend to the important parts of a video, how to memorize
the video that was described given all the generated words so far, then generate a new word by
looking at the entire video. To address these issues, we propose an end-to-end network that has
three components (Figure 1): Temporal Model (TEM), Hierarchical Attention/Memory (HAM),
and the Decoder. The goal in TEM is to capture the temporal structure and track motion in a video.
The HAM component acts as a hierarchical attention or memory between an input video and the
description. More specifically, the HAM learns a hierarchical attention structure that learns where
to attend in video given all previously generated words and previous states. The HAM can be
interpreted as a memory structure as well, where it learns to memorize an encoded version of a
video with language. HAM provides the decoder with the ability to look at an entire video plus all
the previously generated words before generating any new words. This is important because a single
action normally exhibits multiple frames in the input video. By employing the HAM, the model can
effectively model the action over these frames. One of the main contributions of this work is to use
a global state to generate any new word. This global state aggregates information from previously
generated words and all input frames. We will first describe each component of our model, then
explain details of training and inference.
3.1	Temporal Modeler (TEM)
One important question is how to encode the temporal structure of the input video for caption gener-
ation. Recently, it has been shown that Recurrent Neural Networks (RNN) has the ability to model
the temporal structure in sequential data such as video (Ballas et al., 2016; Sharma et al., 2015;
Venugopalan et al., 2015a) and speech (Graves & Jaitly, 2014). Moreover, since frame-to-frame
temporal variation tend to be local (Brox & Malik, 2011) and critical in the motion modeling (Bal-
las et al., 2016), it is important to consider a frame representation that can preserve frame-to-frame
temporal variation. Even though using features extracted from the fully connected layers of Con-
volutional Neural Networks (CNNs) have shown state of the art results in image classification and
3
Under review as a conference paper at ICLR 2017
recognition (Simonyan & Zisserman, 2014; He et al., 2016), these features tend to discard the low
level information useful in modeling the motion in the video (Ballas et al., 2016).
To address the temporal modeling and video representation problems, we use an RNN to model
the temporal structure of the video where at each time step, a frame encoding with size of RD
is used as an input to the RNN. Instead of extracting features from a top layer of the pretrained
CNN, intermediate convolutional maps have been extracted for the video frames. Specifically, for a
given video, X, with N frames X = [X 1,X2,…，XN], N convolutional maps of size RL×D are
extracted where D is dimension ofa feature corresponding to L locations in the input frame.
In order to let the network selectively focus on these L locations of each frame given the hidden
state of RNN, we apply a soft attention model Bahdanau et al. (2015); Xu et al. (2015a); Sharma
et al. (2015), called “Location Attention (Latt)”. More specifically, by using a softmax, each hidden
state produces L probabilities to specify which part of the input is more important, and then creates
an input map for the RNN using these probabilities. The fLatt is defined as follow:
t =	exP((hVT)T Wp)	(2)
PL PL= exp((hV-1)TWk)
L
Ft = XρjtXjt	(3)
j=1
where htv-1 ∈ RK is hidden state of RNN at t - 1, Wp ∈ RK×L, and Ft ∈ RD. At each time
step, TEM learns a vector representation for each frame, looking at the frame convolution map, and
applying the location attention on this map conditioned on all previously seen frames.
Ft = fLatt(Xt,htv-1;Wp)	(4)
hV = fv(Ft, hV-1; Θv)	⑸
where fv can be a vanilla RNN, LSTM, or GRU and Θv is the parameters of the fv . Due to the
fact that vanilla RNNs have gradient vanishing and exploding problems (Pascanu et al., 2013), we
use gradient clipping to deal with gradient exploding, and an LSTM with the following flow to deal
with the gradient vanishing problem:
it = σ(FtWxi + htv-1Whi)
ft = σ(FtWxf +htv-1Whf)
ot = σ(FtWxo +htv-1Who)
gt = tanh(FtWxg +htv-1Whg)
ctv = ft ctv-1 +it gt
htv = ot	tanh(ct)
where Wh* ∈ RK×K, Wχ* ∈ RD×K, and We define Θv = {W%*, Wχ*}.
3.2	Hierarchical Attention/Memory (HAM)
One problem with using sequence-to-sequence style architectures (Sutskever et al., 2014), to model
a task such as video language description, is how to find a mapping from a video space to a language
space that can capture the relationship between a word and video, or more specifically, the con-
nection between an entire video and an entire sentence where there might not be a clear alignment
between the two sequences, as opposed to machine translation and speech recognition. Furthermore,
the model should be able to identify which part of the video is more relevant to the description be-
cause captions normally focus on a tiny fraction of the facts present in the video. More importantly,
once the model starts generating the description, it should still be reminded with the video frames to
generate meaningful descriptions. In order to address these problems, we propose a memory-based
attention that encodes a video into memory, built as a function of the state of the language genera-
tion network (a.k.a. Decoder) and the state of the TEM network. More specifically, our Hierarchical
Attention/Memory can be formulated as two following steps:
4
Under review as a conference paper at ICLR 2017
•	Attention update [ F(Θa)]:
QA = tanh(HvWv + Htg0-1Wg + Htm0-1Wm)	(6)
αt0 = softmax(UT QA)	(7)
F = αf Hv	(8)
•	Memory update:
hm = fm(hmτ, F； Θm)	(9)
where Hv ∈ RN×K, Wv and Wg are ∈ RK×K, U ∈ RK, Wm ∈ RM×K, and Θa =
{Wv, Wg, Wm, U}. N is number of frames in a given video, Hv = [hv1 , ..., hvN], Hgt0-1 =
[htg-1, ..., htg-1], and Hmt -1 = [htm-1, ..., htm-1]. αt0 is the set of probabilities in a given time
step that specifies the attention over an input video state (Hv), memory state (Hm), and decoder
state (Hg). In order to let the network remember what has been attended before and the temporal
structure of a video, we propose fm to memorize the previous attention and encoded version of an
input video with language model. Using fm not only enables the network to memorize previous
attention and frames, but also to learn multi-layer attention over an input video and corresponding
language. The output of the memory-attention is then used as input to the Decoder.
3.3	Decoder
In order to generate a new word conditioned on all previous words and HAM states, a recurrent
structure is modelled as follows:
hg0 = fg( st0, hm, hg0-1; Θg)	(10)
^t = Softmax(Wehgj	(11)
where st is a word vector at t0, We ∈ RK× C and C is the vocabulary size. In addition,即，assigns
a probability to each word in the language. We use LSTMs for both fm and fg .
3.4	Training and Optimization
The goal in our network is to predict the next word given all previously seen words and an input
video. In order to optimize our network parameters, Θ = {Wp, Θv , Θa, Θm , We}, we minimize a
negative log likelihood loss function, formulated as follow:
T |V|
L(S, X; Θ) = - XXSj,ilog(Sj,i) + λ k Θ k2	(12)
ji
where |V | is the dictionary size. We fully train our network in an end-to-end fashion using first-order
stochastic gradient-based optimization method with an adaptive learning rate. More specifically, in
order to optimize our network parameters, we use Adam Kingma & Ba (2015) with learning rate
2 × 10-5 and set β1, β2 to 0.8 and 0.999, respectively. At the training, we use a batch size of 16.
4	Experiments
DATASET We evaluate our proposed model on the Charades (Sigurdsson et al., 2016) dataset
and the Microsoft Video Description Corpus (MSVD) (Chen & Dolan, 2011). Charades contains
9,848 videos (in total) and provides 27, 8471 video descriptions, with 7569 training, 1, 863 test,
400 for the validation. We follow the same split (i.e. training and test splits) as Sigurdsson et al.
(2016). It is worth noting that one major difference between this dataset and others is that they use a
“Hollywood in Homes” approach to collecting the data (Sigurdsson et al., 2016), where “actors” are
crowdsourced, yielding a diverse scene and actor videos. One reason that we report results on this
1Only 16087 out of 27, 847 are used as captions for our evaluation since the 27, 847 refers to script of the
video as well as captions.
5
Under review as a conference paper at ICLR 2017
dataset is because each video has a specific action in it and would be a suitable testbed to evaluate
our model.
MSVD is a set of Youtube videos that are annotated by a Mechanical Turker,2 who was asked to
pick a clip from a video that represents an activity. In this dataset, each clip is annotated by multiple
workers with a single sentence. This dataset contains 1, 970 videos and about 80, 000 descriptions,
where 1, 200 of the videos are training data, 670 are test data, and the rest (i.e., 100 videos) are
assigned for validation. In order to make the results comparable with other papers, we follow the
exact training/validation/test split provided by Venugopalan et al. (2015b).
Evaluation metrics Below, we report results on the video caption generation task. In order
to evaluate captions generated by our model, we use model-free automatic evaluation metrics. We
adopt METEOR, BLEU@N, and CIDEr metrics available from the Microsoft COCO Caption Eval-
uation code3 to score the system.
Video and Caption preprocessing We preprocess the captions for both datasets using the
Natural Language Toolkit (NLTK)4. Beyond this, no other type of preprocessing is used.
We extract sample frames for each video and pass each frame through VGGnet (Simonyan & Zisser-
man, 2014) without any fine-tuning. For the experiments in this paper, we use the feature maps from
conv5^ layer after applying ReLU. The feature map in this layer is 14 X 14 X 512. Our TEM Com-
ponent operates on the flattened 196 × 512 of this feature cubes. For the ablation studies, features
from fully connected layer are used as well where the features in this layer have 4096 dimension.
Hyper-parameter optimization We use random search (Bergstra & Bengio, 2012) on vali-
dation set to select hyper-parameters on both datasets. The word-embedding size, hidden layer size
(for both TEM and Decoder), and memory size of the best model on Charades are: 237, 1316, and
437, respectively. These values are 402, 1479, and 797 for the model on MSVD dataset. A stack of
two LSTMs are used in the Decoder and TEM.
4.1	Video Caption Generation
We first present an ablation analysis to elucidate the contribution of the different components of
our proposed model. Next, we compare the overall performance of our model on video caption
generation task to other models.
Ablation Analysis
We first perform a series of ablation studies in order to show the contributions of the different
components of our model. Specifically, we show that the importance of each components in our
model in caption generation task on MSVD dataset. One ablation (denoted as Att + No TEM)
corresponds to a simpler version of our model in which we remove the TEM component and instead
we pass each frame of a video through a CNN and extract features from the last fully-connected
hidden layer (e.g., fc7). In addition, we replace our HAM component with a simpler version where
the model only memorizes the current step instead of all previous steps. In another ablation (denoted
as No HAM + TEM), we remove the HAM component from our model and keep the rest of our
model as it is. In the next variation (denoted as HAM + No TEM), we remove the TEM component
and calculate features for each frame, similar to Att + No TEM. Finally, the last row in the table is
our proposed model (denoted HAM + TEM) with all its components.
Table 1 reports the result of this study. In this experiment, we sample 40 frames per video and use
them as the inputs to a network. As the results show, HAM plays a critical role in our proposed
model, and removing it causes a drop in performance. On the other hand, removing TEM by itself
does not drop performance as much as dropping the HAM. When we put the two together, they
complement one another, resulting in better performance.
2https://www.mturk.com/mturk/welcome
3https://github.com/tylin/coco-caption
4http://www.nltk.org/
6
Under review as a conference paper at ICLR 2017
Table 1: Ablation of our model with and without the HAM component on the test set of 670 videos
Method	METEOR	BLEU@1	BLEU@2	BLEU@3	BLEU@4	CIDEr
Att + No TEM	31.20	77.90	65.10	55.3	44.90	63.90
No HAM + TEM	30.5	78.10	65.20	55.10	44.60	60.50
HAM + No TEM	31.0	78.70	66.90	57.40	47.0	62.10
HAM + TEM	31.70	79.0	66.20	56.0	45.6	62.20
Performance Comparison
Next, to extensively evaluate our model, we compare our model with state-of-the-art models and
baselines for the video caption generation task on the MSVD dataset. In this experiment, we use 8
frames per video as the inputs to the TEM module. Table 25 shows the results for this experiment. As
the results show, our model gets state-of-the-art scores either in BLEU-4 or METEOR, compared to
other methods. This is particularly noteworthy because we do not use external features for the video,
such as Optical Flow (Brox et al., 2004) (denoted as Flow in table), 3-Dimensional Convolutional
Network features (Tran et al., 2015) (denoted as C3D), or fine-tuned CNN features (denoted as FT)
on the action recognition task with dataset such as UCF-101. The only exception happens when
we compare our model with (Yu et al., 2016), who uses C3D features. In this method, adding C3D
features leads to a huge improvement in their results (compare row 4 with 11 in Table 2). On the
other hand, our method without using any external features can achieve better results in comparison
with all other methods. This is important because our proposed architecture can alone not only learn
a representation for video that can model the temporal structure of a video sequence, but also a
representation that can effectively map visual space to the language space.
Table 2: Video captioning evaluation on the test set of 670 videos in MSVD.
Method	METEOR	BLEU@1	BLEU@2	BLEU@3	BLEU@4	CIDEr
Venugopalan et al. (2015b)	27.7	-	-	-	-	-
Venugopalan et al. (2015a)	29.2	-	-	-	-	-
Pan et al. (2016b)	29.5	74.9	60.9	50.6	40.2	-
Yu et al. (2016)	31.10	77.30	64.50	54.60	44.30	-
Pan et al. (2016a)	33.10	79.20	66.30	55.10	43.80	-
Our Model	31.80	79.40	67.10	56.80	46.10	62.70
Yao et al. (2015) +C3D	29.60	-	-	-	41.92	51.67
Venugopalan et al. (2015a)	29.8	-	-	-	-	-
+ Flow						
Ballas et al. (2016) +FT	30.75	-	-	-	49.0	59.37
Pan et al. (2016b) + C3D	31.0	78.80	66.0	55.4	45.3	-
Yu et al. (2016) +C3D	32.60	81.50	70.40	60.4	49.90	-
In addition, we report results on the Charades dataset for video caption generation. This dataset is
challenging because only a few captions per video (about 2 per video) are available. In this experi-
ment, we use 16 frames per video as the inputs to the TEM module. Table 3 shows the performance
of our method on this dataset. Our method can achieve 10% improvement over Venugopalan et al.
(2015a) in the caption generation task. It is worth noting that a human can only achieve a score of
24 in METEOR for this dataset, which illustrated the level of difficulty in this dataset.
Qualitative results
We show some captions generated by our model in 2. The model mostly generates correct captions
for cases where content and ground truth captions are consistent. There are some cases in which
our model makes some mistakes. For example, in a ‘a dog is on a trampoline’ video, our model
generated “a man is washing a bath” as a caption. This is interesting because the ’man’ object only
appears in a few frames (1 or 2), but our model can still recognize the man object in the video.
5 - in the Table 2 means, that score was not reported by the corresponding paper.
7
Under review as a conference paper at ICLR 2017
Table 3: Video captioning evaluation on the test set of 1863 videos in Charades.
Method	METEOR	BLEU@1 BLEU@2		BLEU@3 BLEU@4		CIDEr
Human(Sigurdsson et al., 2016)	24	62	43	29	20	53
Sigurdsson et al. (2016)	16	49	30	18	11	14
Our Model	17.6	50	31.1	18.8	11.5	16.7
Our CaPtions Ground Truth
A group of
people are
dancing
A group of
young children
performing
together
A person is
cutting the
vegetable
A man is
playing a
guitar
A young girl is
playing the
flute
A woman is
cutting garlic
A man is
playing the
guitar
A little girl is
talking on a
cordless
telephone
A cat is eating
Figure 2: Example captions generated by our model on the test video for MSVD. Incorrect caption
cases are shown in red.
A woman is pouring eggs into a bowl	A woman is pouring ingredients into a bowl
A man is	A man is
playing a flute	playing a large
	flute
A woman is	A woman is
applying	putting on
makeup	makeup
A man is	A guy is
cutting a gun	shooting a gun
A man is	A dog is on a
washing a bath trampoline
Hamsters are
eating
5	Conclusion
We introduce an end-to-end memory-based attention model to describe an input video using natural
language description, similar to the central executive system proposed in human cognition. Our
model utilizes memories of past attention when reasoning about where to attend to in the current
time step. This allows the model to not only reason about local attention more effectively, but
also allows it to consider the entire sequence of video frames while generating each word. Our
experiments have confirmed that the memory components in our architecture play a significant role
in improving the performance of the entire network. It is worth noting that in this paper, we consider
the problem of video caption generation, but our architecture can be applied to any sequence learning
problem, which we hope to explore in the future.
8
Under review as a conference paper at ICLR 2017
References
Tatsuya Harada Andrew Shin, Katsunori Ohnishi. Beyond caption to narrative: Video captioning
with multiple sentences. ICIP, 2016.
A.D. Baddeley and G. Hitch. Working memory. G.A. Bower (Ed.), The psychology of learning and
motivation, 8(4):47-89,1974.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. ICLR, 2015.
Nicolas Ballas, Li Yao, Chris Pal, and Aaron C. Courville. Delving deeper into convolutional net-
works for learning video representations. In ICLR, 2016.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach.
Learn. Res., 13:281-305, 2012.
T. Brox and J. Malik. Large displacement optical flow: Descriptor matching in variational motion
estimation. TPAMI, 33(3):500-513, March 2011. ISSN 0162-8828.
T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High accuracy optical flow estimation based on
a theory for warping. In ECCV, 2004.
David L. Chen and William B. Dolan. Collecting highly parallel data for paraphrase evaluation. In
ACL, Portland, OR, June 2011.
Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural
networks. In ICML-14, pp. 1764-1772, 2014.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401,
2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, June 2016.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-
1780, November 1997. ISSN 0899-7667.
Justin Johnson, Andrej KarPathy, and Li Fei-Fei. DensecaP: Fully convolutional localization net-
works for dense caPtioning. In CVPR, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In ICLR, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deeP convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
NIPS, PP. 1097-1105. 2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yueting Zhuang. Hierarchical recurrent neural
encoder for video representation with application to captioning. In CVPR, June 2016a.
Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong Rui. Jointly modeling embedding and
translation to bridge video and language. CVPR, 2016b.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. ICML-13, 28:1310-1318, 2013.
Tim Rocktaschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.
Reasoning about entailment with neural attention. In ICLR, 2016.
M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and B. Schiele. Translating video content to
natural language descriptions. In ICCV, pp. 433-440, Dec 2013.
9
Under review as a conference paper at ICLR 2017
Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition using visual attention.
CoRR, abs/1511.04119, 2015.
Gunnar A. Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta.
Hollywood in homes: Crowdsourcing data collection for activity understanding. In ECCV, 2016.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. CoRR, abs/1409.1556, 2014.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks.
NIPS, 2015.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In NIPS,pp. 3104-3112. 2014.
Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-
tiotemporal features with 3d convolutional networks. In ICCV, 2015.
Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, and
Kate Saenko. Sequence to sequence - video to text. In ICCV, 2015a.
Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and
Kate Saenko. Translating videos to natural language using deep recurrent neural networks. In
NAACL HLT, 2015b.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. In CVPR, June 2015.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. CoRR, abs/1410.3916, 2014.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In ICML-15, pp. 2048-2057, 2015a.
Ran Xu, Caiming Xiong, Wei Chen, and Jason J. Corso. Jointly modeling deep video and composi-
tional text to bridge vision and language in a unified framework. In AAAI, 2015b.
Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, and William W. Cohen. Encode, review,
and decode: Reviewer module for caption generation. CoRR, abs/1605.07912, 2016.
Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and
Aaron Courville. Describing videos by exploiting temporal structure. In ICCV, 2015.
Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. Video paragraph captioning using
hierarchical recurrent neural networks. In CVPR, June 2016.
10