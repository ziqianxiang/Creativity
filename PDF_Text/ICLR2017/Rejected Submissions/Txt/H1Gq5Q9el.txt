Under review as a conference paper at ICLR 2017
Unsupervised Pretraining for
Sequence to Sequence Learning
Prajit Ramachandran*
University of Illinois at Urbana-Champaign
prajitram@gmail.com
Peter J. Liu, Quoc V. Le
Google Brain
{peterjliu,qvl}@google.com
Ab stract
This work presents a general unsupervised learning method to improve the accu-
racy of sequence to sequence (seq2seq) models. In our method, the weights of
the encoder and decoder of a seq2seq model are initialized with the pretrained
weights of two language models and then fine-tuned with labeled data. We ap-
ply this method to challenging benchmarks in machine translation and abstractive
summarization and find that it significantly improves the subsequent supervised
models. Our main result is that the pretraining accelerates training and improves
generalization of seq2seq models, achieving state-of-the-art results on the WMT
English→German task, surpassing a range of methods using both phrase-based
machine translation and neural machine translation. Our method achieves an im-
provement of 1.3 BLEU from the previous best models on both WMT’14 and
WMT’15 English→German. On summarization, our method beats the supervised
learning baseline.
1	Introduction
Sequence to sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner
& Blunsom, 2013; Allen, 1987; Neco & Forcada, 1997) are extremely effective on a variety of
tasks that require a mapping between a variable-length input sequence to a variable-length output
sequence. The main weakness of sequence to sequence models, and deep networks in general, lies
in the fact that they can easily overfit when the amount of supervised training data is small.
In this work, we propose a simple and effective technique for using unsupervised pretraining to
improve seq2seq models. Our proposal is to initialize both encoder and decoder networks with
pretrained weights of two language models. These pretrained weights are then fine-tuned with the
labeled corpus.
We benchmark this method on machine translation for English→German and abstractive summa-
rization on CNN and Daily Mail articles. Our main result is that a seq2seq model, with pretraining,
exceeds the strongest possible baseline in both neural machine translation and phrase-based machine
translation. Our model obtains an improvement of 1.3 BLEU from the previous best models on both
WMT’14 and WMT’15 English→German. On abstractive summarization, our method achieves
competitive results to the strongest baselines.
We also perform ablation study to understand the behaviors of the pretraining method. Our study
confirms that among many other possible choices of using a language model in seq2seq with atten-
tion, the above proposal works best. Our study also shows that, for translation, the main gains come
from the improved generalization due to the pretrained features, whereas for summarization the
gains come from the improved optimization due to pretraining the encoder which has been unrolled
for hundreds of timesteps. On both tasks, our proposed method always improves generalization on
the test sets.
*Work done as an intern on Google Brain.
1
Under review as a conference paper at ICLR 2017
2	Unsupervised Pretraining for Sequence to Sequence Learning
In the following section, we will describe our basic unsupervised pretraining procedure for sequence
to sequence learning and how to modify sequence to sequence learning to effectively make use of
the pretrained weights. We then show several extensions to improve the basic model.
2.1	Basic Procedure
Given an input sequence x1, x2, ..., xm and an output sequence yn, yn-1, ..., y1, the objective of se-
quence to sequence learning is to maximize the likelihood p(yn, yn-1, ..., y1 |x1, x2, ..., xm).
Common sequence to sequence learning methods decompose this objective as
p(yn, yn-1, ..., y1|x1,x2, ..., xm) = tn=1p(yt|yt-1, ..., y1;x1,x2, ..., xm).
In sequence to sequence learning, an RNN encoder is used to represent x1, ..., xm as a hidden vector,
which is given to an RNN decoder to produce the output sequence. Our method is based on the
observation that without the encoder, the decoder essentially acts like a language model on y’s.
Similarly, the encoder with an additional output layer also acts like a language model. Thus it is
natural to use trained languages models to initialize the encoder and decoder.
Therefore, the basic procedure of our approach is to pretrain both the seq2seq encoder and decoder
networks with language models, which can be trained on large amounts of unlabeled text data. This
can be seen in Figure 1, where the parameters in the shaded boxes are pretrained. In the following
we will describe the method in detail using machine translation as an example application.
Softmax
Second RNN Layer
First RNN Layer
Embedding
Figure 1: Pretrained sequence to sequence model. The red parameters are the encoder and the blue
parameters are the decoder. All parameters in a shaded box are pretrained, either from the source
side (light red) or target side (light blue) language model. Otherwise, they are randomly initialized.
First, two monolingual datasets are collected, one for the source side language, and one for the
target side language. A language model (LM) is trained on each dataset independently, giving an
LM trained on the source side corpus and an LM trained on the target side corpus.
After two language models are trained, a multi-layer seq2seq model M is constructed. The embed-
ding and first LSTM layers of the encoder and decoder are initialized with the pretrained weights.
To be even more efficient, the softmax of the decoder is initialized with the softmax of the pretrained
target side LM.
2.2	Improving the model
We also employ three additional methods to further improve the model above. The three meth-
ods are: a) Monolingual language modeling losses, b) Residual connections and c) Attention over
multiple layers (see Figure 2).
Monolingual language modeling losses: After the seq2seq model M is initialized with the two
LMs, it is fine-tuned with a labeled dataset. To ensure that the model does not overfit the labeled
data, we regularize the parameters that were pretrained by continuing to train with the monolingual
language modeling losses. The seq2seq and language modeling losses are weighted equally.
2
Under review as a conference paper at ICLR 2017
X
W
(a)
ABC	<EOS>
(b)
Figure 2: Two improvements to the baseline model: (a) residual connection, and (b) attention over
multiple layers.
Residual connections: As described, the input vector to the decoder softmax layer is a random
vector because the high level (non-first) layers of the LSTM are randomly initialized. This slows
down training and introduces random gradients to the pretrained parameters, reducing the effective-
ness of pretraining. To circumvent this issue, we use a residual connection from the output of the
first LSTM layer directly to the input of the softmax (see Figure 2-a).
Attention over multiple layers: In all our models, we use an attention mechanism (Bahdanau
et al., 2015), where the model attends over both top and first layer (see Figure 2-b). More concretely,
given a query vector qt from the decoder, encoder states from the first layer h11, . . . , h1T, and encoder
states from the last layer h1L, . . . , hTL, we compute the attention context vector ct as follows:
a = VjeXPSt ∙ hi)N、	Ct = X αihi	CN = X aihN	Ct = [c1;CN]
Ej=I eχp(qt ∙ hN)	i=ι	i=ι
Note that attention weights αi are only computed once using the top level encoder states.
We also experiment with passing the attention vector Ct as input into the next timestep (Luong et al.,
2015b). Instead of passing C into the first LSTM layer, we pass it as input to the second LSTM layer
by concatenating it with the output of the first LSTM layer.
We use all three improvements in our experiments. However, in general we notice that the benefits
of the attention modifications are minor in comparison with the benefits of the additional language
modeling objectives and residual connections.
3	Experiments
In the following section, we apply our approach to two important tasks in seq2seq learning: machine
translation and abstractive summarization. On each task, we compare against the previous best
systems. We also perform ablation experiments to understand the behavior of each component of
our method.
3.1	Machine Translation
Dataset and Evaluation: For machine translation, we evaluate our method on the WMT
English→German task (Bojar et al., 2015). We used the WMT 14 training dataset, which is slightly
smaller than the WMT 15 dataset. Because the dataset has some noisy examples, we used a lan-
guage detection system to filter the training examples. Sentences pairs where either the source was
not English or the target was not German were thrown away. This resulted in around 4 million
training examples. Following Sennrich et al. (2015b), we use subword units (Sennrich et al., 2015a)
3
Under review as a conference paper at ICLR 2017
with 89500 merge operations, giving a vocabulary size around 90000. The validation set is the
concatenated newstest2012 and newstest2013, and our test sets are newstest2014 and newstest2015.
Evaluation on the validation set was with case-sensitive BLEU (Papineni et al., 2002) on tokenized
text using multi-bleu.perl. Evaluation on the test sets was with case-sensitive BLEU on
detokenized text using mteval-v13a.pl. The monolingual training datasets are the News Crawl
English and German corpora, each of which has more than a billion tokens.
Experimental settings: The language models were trained in the same fashion as (Jozefowicz
et al., 2016) We used a 1 layer 4096 dimensional LSTM with the hidden state projected down
to 1024 units (Sak et al., 2014) and trained for one week on 32 Tesla K40 GPUs. Our seq2seq
model was a 3 layer model, where the second and third layers each have 1000 hidden units. The
monolingual objectives, residual connection, and the modified attention were all used. We used the
Adam optimizer (Kingma & Ba, 2015) and train with asynchronous SGD on 16 GPUs for speed.
We used a learning rate of 5e-5 which is multiplied by 0.8 every 50K steps after an initial 400K
steps, gradient clipping with norm 5.0 (Pascanu et al., 2013), and dropout of 0.2 on non-recurrent
connections (Zaremba et al., 2014). We used early stopping on validation set perplexity. A beam
size of 10 was used for decoding. Our ensemble is constructed with the 5 best performing models
on the validation set, which are trained with different hyperparameters.
Results: Table 1 shows the results of our method in comparison with other baselines. Our
method achieves a new state-of-the-art for single model performance on both newstest2014 and
newstest2015, significantly outperforming the competitive semi-supervised backtranslation tech-
nique (Sennrich et al., 2015b). Equally impressive is the fact that our best single model outperforms
the previous state of the art ensemble of 4 models. Our ensemble of 5 models matches or exceeds
the previous best ensemble of 12 models.
System	ensemble?	BLEU	
		newstest2014	newstest2015
Phrase Based MT (Williams etal.,2016)	-	21.9	23.7
Supervised NMT (Jean et al., 2015)	single	-	22.4
Edit Distance Transducer NMT (Stahlberg et al., 2016)	single	21.7	24.1
Edit Distance Transducer NMT (Stahlberg et al., 2016)	ensemble 8	22.9	25.7
BaCktranSlation (Sennrich et al., 2015b)	single	22.7	25.7
Backtranslation (Sennrich et al., 2015b)	ensemble 4	23.8	26.5
Backtranslation (Sennrich et al., 2015b)	ensemble 12	24.7	27.6
No pretraining	single	21.3	24.3
Pretrained seq2seq	single	24.0	27.0
Pretrained seq2seq	ensemble 5	24.7	28.1
Table 1: English→German performance on WMT test sets. Our pretrained model outperforms all
other models. Note that the model without pretraining uses the LM objective.
Ablation study: In order to better understand the effects of pretraining, we conducted an ablation
study by modifying the pretraining scheme. Figure 3 shows the drop in validation BLEU of various
ablations compared with the full model. The full model uses LMs trained with monolingual data to
initialize the encoder and decoder, in addition to the language modeling objective. In the following,
we interpret the findings of the study. Note that some findings are specific to the translation task.
Given the results from the ablation study, we can make the following observations:
•	Pretraining the decoder is better than pretraining the encoder: Only pretraining the encoder
leads to a 1.6 BLEU point drop while only pretraining the decoder leads to a 1.0 BLEU
point drop.
•	Pretrain as much as possible because the benefits compound: given the drops of no pre-
training at all (-2.0) and only pretraining the encoder (-1.6), the additive estimate of the
drop of only pretraining the decoder side is -2.0 - (-1.6) = -0.4; however the actual
drop is -1.0 which is a much larger drop than the additive estimate.
•	Pretraining the softmax is important: Pretraining only the embeddings and first LSTM layer
gives a large drop of 1.6 BLEU points.
4
Under review as a conference paper at ICLR 2017
Figure 3: English→German ablation study measuring the difference in validation BLEU between
various ablations and the full model. More negative is worse. The full model uses LMs trained with
monolingual data to initialize the encoder and decoder, plus the language modeling objective.
•	The language modeling objective is a strong regularizer: The drop in BLEU points of
pretraining the entire model and not using the LM objective is as bad as using the LM
objective without pretraining.
•	Pretraining on a lot of unlabeled data is essential for learning to extract powerful features:
If the model is initialized with LMs that are pretrained on the source part and target part of
the parallel corpus, the drop in performance is as large as not pretraining at all. However,
performance remains strong when pretrained on the large, non-news Wikipedia corpus.
To understand the contributions of unsupervised pretraining vs. supervised training, we track the
performance of pretraining as a function of dataset size. For this, we trained a a model with and
without pretraining on random subsets of the English→German corpus. Both models use the ad-
ditional LM objective. The results are summarized in Figure 4. When a 100% of the labeled data
is used, the gap between the pretrained and no pretrain model is 2.0 BLEU points. However, that
gap grows when less data is available. When trained on 20% of the labeled data, the gap becomes
3.8 BLEU points. This demonstrates that the pretrained models degrade less as the labeled dataset
becomes smaller.
222120191801615
∩W-JC0
20	40	60	80	100
Percent of entire labeled dataset used for training
Figure 4: Validation performance of pretraining vs. no pretraining when trained on a subset of the
entire labeled dataset for English→German translation.
5
Under review as a conference paper at ICLR 2017
3.2	Abstractive S ummarization
Dataset and Evaluation: For a low-resource abstractive summarization task, we use the
CNN/Daily Mail corpus from (Hermann et al., 2015). Following Nallapati et al. (2016), we modify
the data collection scripts to restore the bullet point summaries. The task is to predict the bullet
point summaries from a news article. The dataset has fewer than 300K document-summary pairs.
To compare against Nallapati et al. (2016), we used the anonymized corpus. However, for our abla-
tion study, we used the non-anonymized corpus.1 We evaluate our system using full length ROUGE
(Lin, 2004). For the anonymized corpus in particular, we considered each highlight as a separate
sentence following Nallapati et al. (2016). In this setting, we used the English Gigaword corpus
(Napoles et al., 2012) as our larger, unlabeled “monolingual” corpus, although all data used in this
task is in English.
Experimental settings: We use subword units (Sennrich et al., 2015a) with 31500 merges, result-
ing in a vocabulary size of about 32000. We use up to the first 600 tokens of the document and
predict the entire summary. Only one language model is trained and it is used to initialize both the
encoder and decoder, since the source and target languages are the same. However, the encoder
and decoder are not tied. The LM is a one-layer LSTM of size 1024 trained in a similar fashion to
Jozefowicz et al. (2016). For the seq2seq model, we use the same settings as the machine translation
experiments. The only differences are that we use a 2 layer model with the second layer having
1024 hidden units, and that the learning rate is multiplied by 0.8 every 30K steps after an initial
100K steps.
Results: Table 2 summarizes our results on the anonymized version of the corpus. Our pretrained
model is only able to match the previous baseline seq2seq of Nallapati et al. (2016). However, our
model is a unidirectional LSTM while they use a bidirectional LSTM. They also use a longer context
of 800 tokens, whereas we used a context of 600 tokens due to GPU memory issues. Furthermore,
they use pretrained word2vec (Mikolov et al., 2013) vectors to initialize their word embeddings. As
we show in our ablation study, just pretraining the embeddings itself gives a large improvement.
System	ROUGE-1	ROUGE-2	ROUGE-L
Seq2seq + pretrained embeddings (Nallapati et al., 2016)	-3249-	-1184-	29.47
+ temporal attention (Nallapati et al., 2016)	35.46	13.30	32.65
Pretrained seq2seq	-3256-	1189	29.44
Table 2: Results on the anonymized CNN/Daily Mail dataset.
Ablation study: We performed an ablation study similar to the one performed on the machine
translation model. The results are reported in Figure 5. Here we report the drops on ROUGE-1,
ROUGE-2, and ROUGE-L on the non-anonymized validation set.
Given the results from our ablation study, we can make the following observations:
•	Pretraining improves optimization: in contrast with the machine translation model, it is
more beneficial to only pretrain the encoder than only the decoder of the summarization
model. One interpretation is that pretraining enables the gradient to flow much further
back in time than randomly initialized weights. This may also explain why pretraining on
the parallel corpus is no worse than pretraining on a larger monolingual corpus.
•	The language modeling objective is a strong regularizer: A model without the LM objective
has a significant drop in ROUGE scores.
Human evaluation: As ROUGE may not be able to capture the quality of summarization, we
also performed a small qualitative study to understand the human impression of the summaries
produced by different models. We took 200 random documents and compared the performance of
1We encourage future researchers to use the non-anonymized version because it is a more realistic summa-
rization setting with a larger vocabulary. Our numbers on the non-anonymized test set are 35.56 ROUGE-1,
14.60 ROUGE-2, and 25.08 ROUGE-L. We did not consider highlights as separate sentences.
6
Under review as a conference paper at ICLR 2017
ROUGE-I
ROUGE-2
ROUGE-L
Figure 5: Summarization ablation study measuring the difference in validation ROUGE between
various ablations and the full model. More negative is worse. The full model uses LMs trained with
unlabeled data to initialize the encoder and decoder, plus the language modeling objective.
a pretrained and non-pretrained system. The document, gold summary, and the two system outputs
were presented to a human evaluator who was asked to rate each system output on a scale of 1-5
with 5 being the best score. The system outputs were presented in random order and the evaluator
did not know the identity of either output. The evaluator noted if there were repetitive phrases or
sentences in either system outputs. Unwanted repetition was also noticed by Nallapati et al. (2016).
Table 3 and 4 show the results of the study. In both cases, the pretrained system outperforms the
system without pretraining in a statistically significant manner. The better optimization enabled by
pretraining improves the generated summaries and decreases unwanted repetition in the output.
NP > P NP = P
-^29	88^^
NP < P
83
Table 3:	The count of how often the no pretrain system (NP) achieves a higher, equal, and lower
score than the pretrained system (P) in the side-by-side study where the human evaluator gave each
system a score from 1-5. The sign statistical test gives a p-value of < 0.0001 for rejecting the null
hypothesis that there is no difference in the score obtained by either system.
Pretrain
No repeats
Repeats
No pretrain
No repeats Repeats
67	65
24	44
Table 4:	The count of how often the pretrain and no pretrain systems contain repeated phrases or
sentences in their outputs in the side-by-side study. McNemar’s test gives a p-value of < 0.0001
for rejecting the null hypothesis that the two systems repeat the same proportion of times. The
pretrained system clearly repeats less than the system without pretraining.
4 Related Work
Unsupervised pretraining has been intensively studied in the past years, most notably is the work
by Dahl et al. (2012) who found that pretraining with deep belief networks improved feedforward
acoustic models. More recent acoustic models have found pretraining unnecessary (Xiong et al.,
7
Under review as a conference paper at ICLR 2017
2016; Zhang et al., 2016; Chan et al., 2015), probably because the reconstruction objective of deep
belief networks is too easy. In contrast, we find that pretraining language models by next step
prediction significantly improves seq2seq on challenging real world datasets.
Despite its appeal, unsupervised learning is rarely shown to improve supervised training. Dai & Le
(2015) was amongst the rare studies which showed the benefits of pretraining in a semi-supervised
learning setting. Their method is similar to our method except that they did not have a decoder
network and thus could not apply to seq2seq learning. Similarly, Zhang & Zong (2016) found it
useful to add an additional task of sentence reordering of source-side monolingual data for neural
machine translation. Various forms of transfer or multitask learning with seq2seq framework also
have the flavors of our algorithm (Zoph et al., 2016; Luong et al., 2015a; Firat et al., 2016).
Perhaps most closely related to our method is the work by Gulcehre et al. (2015), who combined a
language model with an already trained seq2seq model by fine-tuning additional deep output layers.
Empirically, their method produces small improvements over the supervised baseline. We suspect
that their method does not produce significant gains because (i) the models are trained independently
of each other and are not fine-tuned (ii) the LM is combined with the seq2seq model after the last
layer, wasting the benefit of the low level LM features, and (iii) only using the LM on the decoder
side. Venugopalan et al. (2016) addressed (i) but still experienced minor improvements. Using
pretrained GloVe embedding vectors (Pennington et al., 2014) had more impact.
Related to our approach in principle is the work by Chen et al. (2016) who proposed a two-term,
theoretically motivated unsupervised objective for unpaired input-output samples. Though they did
not apply their method to seq2seq learning, their framework can be modified to do so. In that case,
the first term pushes the output to be highly probable under some scoring model, and the second
term ensures that the output depends on the input. In the seq2seq setting, we interpret the first term
as a pretrained language model scoring the output sequence. In our work, we fold the pretrained
language model into the decoder. We believe that using the pretrained language model only for
scoring is less efficient that using all the pretrained weights. Our use of labeled examples satisfies
the second term. These connections provide a theoretical grounding for our work.
In our experiments, we benchmark our method on machine translation, where other unsupervised
methods are shown to give promising results (Sennrich et al., 2015b; Cheng et al., 2016). In back-
translation (Sennrich et al., 2015b), the trained model is used to decode unlabeled data to yield extra
labeled data. One can argue that this method may not have a natural analogue to other tasks such as
summarization. We note that their technique is complementary to ours, and may lead to additional
gains in machine translation. The method of using autoencoders in Cheng et al. (2016) is promising,
though it can be argued that autoencoding is an easy objective and language modeling may force the
unsupervised models to learn better features.
5 Conclusion
We presented a novel unsupervised pretraining method to improve sequence to sequence learning.
The method can aid in both generalization and optimization. Our scheme involves pretraining two
language models in the source and target domain, and initializing the embeddings, first LSTM layers,
and softmax of a sequence to sequence model with the weights of the language models. Using our
method, we achieved state-of-the-art machine translation results on both WMT’14 and WMT’15
English to German.
A key advantage of this technique is that it is flexible and can be applied to a large variety of tasks,
such as summarization, where it surpasses the supervised learning baseline.
Acknowledgments
We thank George Dahl, Andrew Dai, Laurent Dinh, Stephan Gouws, Geoffrey Hinton, Rafal Joze-
fowicz, Pooya Khorrami, Phillip Louis, Ramesh Nallapati, Arvind Neelakantan, Xin Pan, Abi See,
Rico Sennrich, Luke Vilnis, Yuan Yu and the Google Brain team for their help with the project.
8
Under review as a conference paper at ICLR 2017
References
Robert B. Allen. Several studies on natural language and back-propagation. IEEE First International Confer-
ence on Neural Networks, 1987.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to
align and translate. In ICLR, 2015.
Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, PhiliPP
Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and
Marco Turchi. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth
Workshop on Statistical Machine Translation, 2015.
William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals. Listen, attend and spell. arXiv preprint
arXiv:1508.01211, 2015.
Jianshu Chen, Po-Sen Huang, Xiaodong He, Jianfeng Gao, and Li Deng. Unsupervised learning of predictors
from unpaired input-output samples. abs/1606.04646, 2016.
Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Semi-supervised learning
for neural machine translation. arXiv preprint arXiv:1606.04596, 2016.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statisti-
cal machine translation. In EMNLP, 2014.
G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large-
vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):
30-42, 2012. ISSN 1558-7916.
Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In NIPS. 2015.
Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos T. Yarman-Vural, and Kyunghyun Cho. Zero-
resource translation with multi-lingual neural machine translation. arXiv preprint arXiv:1606.04164, 2016.
Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. On using monolingual corpora in neural machine translation. arXiv
preprint arXiv:1503.03535, 2015.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. Teaching machines to read and comprehend. In NIPS. 2015.
Sebastien Jean, Orhan Firat, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. Montreal neural ma-
chine translation systems for WMT’15. In Proceedings of the Tenth Workshop on Statistical Machine Trans-
lation, 2015.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of
language modeling. arXiv preprint arXiv:1602.02410, 2016.
Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP, 2013.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Chin-Yew Lin. ROUGE: a package for automatic evaluation of summaries. In Proceedings of the Workshop on
Text Summarization Branches Out (WAS 2004), 2004.
Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to
sequence learning. In ICLR, 2015a.
Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural
machine translation. In EMNLP, 2015b.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In NIPS. 2013.
Ramesh Nallapati, Bing Xiang, and Bowen Zhou. Sequence-to-sequence RNNs for text summarization. arXiv
preprint arXiv:1602.06023, 2016.
Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. Annotated gigaword. In Proceedings of the
Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction. ACL,
2012.
9
Under review as a conference paper at ICLR 2017
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic evaluation of
machine translation. In ACL, 2002.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks.
ICML, 2013.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representa-
tion. In EMNLP, 2014.
Hasim Sak, Andrew W. Senior, and Francoise Beaufays. Long short-term memory recurrent neural network
architectures for large scale acoustic modeling. In INTERSPEECH, 2014.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword
units. arXiv preprint arXiv:1508.07909, 2015a.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with mono-
lingual data. arXiv preprint arXiv:1511.06709, 2015b.
Felix Stahlberg, Eva Hasler, and Bill Byrne. The edit distance transducer in action: The university of cambridge
english-german system at wmt16. In Proceedings ofthe First Conference on Machine Translation, pp. 377—
384, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.
aclweb.org/anthology/W/W16/W16-2324.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS.
2014.
Subhashini Venugopalan, Lisa Anne Hendricks, Raymond Mooney, and Kate Saenko. Improving LSTM-based
video description with linguistic knowledge mined from text. arXiv preprint arXiv:1604.01729, 2016.
Philip Williams, RiCo Sennrich, Maria Nadejde, Matthias Huck, Barry Haddow, and Ondrej Bojar. Edinburgh's
statistical machine translation systems for wmt16. In Proceedings of the First Conference on Machine
Translation, pp. 399-410, Berlin, Germany, August 2016. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/W/W16/W16-2327.
W. Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey
Zweig. Achieving human parity in conversational speech recognition. abs/1610.05256, 2016.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint
arXiv:1409.2329, 2014.
Jiajun Zhang and Chengqing Zong. Exploiting source-side monolingual data in neural machine translation. In
EMNLP, 2016.
Yu Zhang, William Chan, and Navdeep Jaitly. Very deep convolutional networks for end-to-end speech recog-
nition. abs/1610.03022, 2016. URL http://arxiv.org/abs/1610.03022.
Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low-resource neural machine
translation. In EMNLP, 2016.
Ramon P. Neco and Mikel L. Forcada. Asynchronous translations with recurrent neural nets. Neural Networks,
1997.
10
Under review as a conference paper at ICLR 2017
Appendix
Selected Summarization Outputs
Source Document
(Cnn) like phone booths and typewriters, record stores are a vanishing breed - another victim
of the digital age . camelot music . virgin megastores . wherehouse music . tower records
. all of them gone . corporate america has largely abandoned brick - and - mortar music
retailing to a scattering of independent stores , many of them in scruffy urban neighborhoods
. and that s not necessarily a bad thing . yes , it s harder in the spotify era to find a place to go
buy physical music . but many of the remaining record stores are succeeding - even thriving
- by catering to a passionate core of customers and collectors . on saturday , hundreds of
music retailers will hold events to commemorate record store day , an annual celebration of
, well , your neighborhood record store . many stores will host live performances , drawings
, book signings , special sales of rare or autographed vinyl and other happenings . some will
even serve beer . to their diehard customers , these places are more than mere stores : they
are cultural institutions that celebrate music history ( the entire duran duran oeuvre , all in
one place ! ) , display artifacts ( aretha franklin on vinyl ! ) , and nurture the local music
scene ( hey , here s a cd by your brother s metal band ! ) . they also employ knowledgeable
clerks who will be happy to debate the relative merits of blood on the tracks and blonde on
blonde . or maybe , like jack black in high fidelity , just mock your lousy taste in music . so
if you re a music geek , drop by . but you might think twice before asking if they stock i just
called to say i love you .
Ground Truth summary
saturday is record store day , celebrated at music stores around the world . many stores will
host live performances, drawings and special sales of rare vinyl.
No pretrain
corporate america has largely abandoned brick - brick - mortar music. many of the remaining
record stores are succeeding - even thriving - by catering to a passionate core of customers.
Pretrained
hundreds of music retailers will hold events to commemorate record store day . many stores
will host live performances , drawings , book signings , special sales of rare or autographed
vinyl.
Table 5:	The pretrained model outputs a highly informative summary, while the no pretrain model
outputs irrelevant details.
11
Under review as a conference paper at ICLR 2017
Source Document
(Cnn ) hey , look What i did . that small boast on social media can trigger a whirlwind that
spins into real - life grief , as a texas veterinarian found out after shooting a cat . dr. kristen
lindsey allegedly shot an arrow into the back of an orange tabby s head and posted a proud
photo this week on facebook of herself smiling , as she dangled its limp body by the arrow s
shaft . lindsey added a comment , cnn affiliate kbtx reported . my first bow kill , lol . the only
good feral tomcat is one with an arrow through it s head ! vet of the year award ... gladly
accepted . callers rang the phones hot at washington county s animal clinic , where lindsey
worked , to vent their outrage . web traffic crashed its website . high price of public shaming
on the internet then an animal rescuer said that lindsey s prey was probably not a feral cat
but the pet of an elderly couple , who called him tiger . he had gone missing on wednesday ,
the same day that lindsey posted the photo of the slain cat . cnn has not been able to confirm
the claim . as the firestorm grew , lindsey wrote in the comments underneath her post : no i
did not lose my job . lol . psshh . like someone would get rid of me . i m awesome ! that
prediction was wrong . the clinic fired lindsey , covered her name on its marquee with duct
tape , and publicly distanced itself from her actions . our goal now is to go on and try to fix
our black eye and hope that people are reasonable and understand that those actions do nt
anyway portray what we re for here at washington animal clinic , said dr. bruce buenger . we
put our heart and soul into this place . the clinic told wbtx that lindsey was not available for
comment . cnn is reaching out to her . she removed her controversial post then eventually
shut down her facebook page . callers also complained to the brenham police department and
washington county animal control , as her facebook post went viral . the sheriff s office in
austin county , where the cat was apparently shot , is investigating , and lindsey could face
charges . its dispatchers were overloaded with calls , the sheriff posted on facebook . we are
asking you to please take it easy on our dispatchers . as soon as the investigation is complete ,
we will post the relevant information here on this page , the post read . animal rights activists
are pushing for charges . animal cruelty must be taken seriously , and the guilty parties should
be punished to the fullest extent of the law , said cat advocacy activist becky robinson . her
organization , alley cat allies , is offering a $ 7,500 reward for evidence leading to the arrest
and conviction of the person who shot the cat . but others stood up for lindsey . she s amazing
. she s caring , said customer shannon stoddard . she s a good vet , so maybe her bad choice
of posting something on facebook was not good . but i do nt think she should be judged for
it . she dropped off balloons at the animal clinic for lindsey with a thank you note . cnn s
jeremy grisham contributed to this report.
Ground Truth summary
dr. kristen lindsey has since removed the post of her holding the dead cat by an arrow . her
employer fired her ; the sheriff S office is investigating . activist offers $ 7,500 reward .
No pretrain
dr. kristen lindsey allegedly shot an arrow into the back of an orange orange tabby S head . it
s the only good good tomcat is one with an arrow through it s head ! vet vet of the year award
Pretrained
lindsey lindsey , a texas veterinarian , shot an arrow into the back of an orange tabby S head
. she posted a photo of herself smiling , as she dangled its limp body by the arrow s shaft .
lindsey could face charges , the sheriff s department says .
Table 6:	The pretrained model outputs a highly relevant summary but makes a mistake on the feline
executioner’s name. The no pretrain model degenerates into irrelevant details and repeats itself.
12
Under review as a conference paper at ICLR 2017
Source Document
eugenie bouchard S run of poor form continued as the top seed was beaten 6 - 3,6 - 1 by
american lauren davis in the second round at the family circle cup in charleston on wednesday
. davis , 21 , had lost her only career meeting with bouchard , but was in control this time
against the world no 7 . davis won nine of the final 11 games of the match and broke bouchard
s serve twice in the final set to pull off the upset . eugenie bouchard fires down a serve during
her second - round match at the family circle cup bouchard shows her frustrations during
her straight - sets defeat by lauren davis on wednesday i ve never beaten here before , so i
came out knowing i had nothing to lose , said davis , ranked 66th in the world . bouchard
was a semi-finalist last year but had struggled in the lead - up to the charlston green - clay
event , losing three of her last five matches to lower - ranked opponents . davis used precise
groundstrokes to keep bouchard on her heels throughout the second set . davis broke bouchard
s serve to love to take a 3 - 1 lead as the 21 - year - old canadian had a double fault , then
sailed a forehand long to fall behind . two games later , davis hit a backhand to send back
bouchard s 102mph serve for a winner down the left sideline and a 5 - 1 lead . davis hits
a forehand on her way to an impressive win against the canadian top seed bouchard has
struggled recently , this time slumping to her fourth defeat in six matches the match ended
on bouchard s forehand into the net , davis waving to the cheering crowd . bouchard said
: as soon as it was over , definitely a bit of anger , but also kind of this confusion , slash ,
like quest to find what s wrong , like this kind of searching feeling that , ok , like i know
something s off . i know something s not right . so i want to find it . i was definitely a little
bit slow today , overpowered . usually , i m the one dominating . so it was definitely , just , i
do nt know ,just not good .
Ground Truth summary
eugenie bouchard suffered her fourth defeat in six matches . the Canadian top seed lost to
lauren davis at the family circle cup . world no 66 davis won 6 - 3 , 6 - 1 in the second round
in charleston . davis won nine of the final 11 games of the match to seal victory . click here
for all the latest news from charleston .
No pretrain
bouchard beat american lauren davis 6 - 3,6 -1in the second round of the family circle cup
in charleston on wednesday . bouchard had lost her only career meeting with bouchard but
was in control this time against the world no 7 . bouchard won nine of the final 11 games of
the match and broke bouchard S serve twice in the final set to pull off the upset.
Pretrained
eugenie bouchard was beaten 6 - 3 , 6 - 1 by american lauren davis in the second round .
davis had lost her only career meeting with bouchard , but was in control this time against the
world no 7 . davis hit a backhand to send back bouchard s 102mph serve for a winner down
the left sideline .
Table 7:	Both models output a relevant summary, but the no pretrain model uses the same name to
refer to both players.
13
Under review as a conference paper at ICLR 2017
Source Document
(Cnn ) mike rowe is coming to a river near you . sometimes, you hear about a person Who
makes you feel good about humanity , but bad about yourself , rowe says . on thursday s
episode of somebody s got ta do it , rowe meets up with chad pregracke , the founder of
living lands & waters , who does just that . pregracke wants to clean up the nation s rivers
one piece of detritus at a time . his quota ? always more . read mike rowe s facebook post
on how to break our litter habit . since he founded the nonprofit in 1998 at the ripe age of 23
, pregracke and more than 87,000 volunteers have collected 8.4 million pounds of trash from
u.s. waterways . those efforts helped him earn the 2013 cnn hero of the year award , along
with numerous other honors . wherever you are , no matter if there s a stream , a creek , a
lake , whatever , that needs to be cleaned up , you can do it . just organize it and do it , he told
cnn s anderson cooper after his win . pregracke also gives rowe a tour of the 150 - foot , solar
- powered barge that the living lands & waters staff calls home during lengthy cleanups . the
part - home , part - office , part - dumpster has seven bedrooms , two bathrooms , a classroom
and a kitchen - and just happens to be made from a recycled strip club . according to the
organization s latest annual report , pregracke has made it his mission in 2015 to remove
500,000 more pounds of trash . if you d like to help achieve this goal , visit his website to
learn how to help : livinglandsandwaters.org / get - involved /.
Ground Truth summary
chad pregracke was the 2013 cnn hero of the year. mike rowe visited pregracke for an episode
of somebody S got ta do it.
No pretrain
rowe meets up with chad pregracke , founder of living lands & waters . pregracke and more
than 87,000 volunteers collected 8.4 million pounds of trash from u.s. waterways.
Pretrained
rowe is the founder of living lands & waters, who does just that. pregracke also gives rowe
a tour of the 150 - foot barge that the living lands & waters gets .
Table 8:	A failure case. The pretrained model outputs irrelevant details while the no pretrain model
successfully summarizes the document.
14
Under review as a conference paper at ICLR 2017
SELECTED ENGLISH→GERMAN OUTPUTS
Source
Mayor Bloomberg told reporters that, because of that court order, the city had suspended the
reopening of the public space and protesters were informed, however, that local laws do not
allow them to re-install with camping shops and sleeping bags.
Ground Truth
Burgermeister Bloomberg stellt vor der Presse klar, das aufgrund dieser richterlichen Anord-
nung die erneute Offnung des Platzes fur den Publikumsverkehr und die Demonstranten
aufgehoben worden sei . Die Demonstranten wies er darauf hin , dass die Stadtgesetze ihnen
nicht erlaubten , sich erneut mit Zelten und SChIafSaCken an diesem Ort einzurichten .
No pretrain
Der Burgermeister Bloomberg sagte den Reportern , dass die Stadt aufgrund dieser Gericht-
sentscheidung die Wiedereroffnung des offentlichen Raumes und die Information der
Demonstranten ausgesetzt habe , dass die lokalen Gesetze ihnen nicht erlauben , mit den
Campingplatzen und SChIafSaCken neu ZU installieren .
Pretrained
Burgermeister Bloomberg erklarte gegenuber Journalisten , dass die Stadt aufgrund dieser
Gerichtsentscheidung die Wiedereroffnung des offentlichen Raums ausgesetzt habe und dass
die Demonstranten daruber informiert wurden , dass die ortlichen Gesetze es ihnen nicht
erlauben wurden , sich mit Campingplatzen und Schlafsalen neu zu installieren .
Table 9:	The no pretrain model makes a complete mistranslation when outputting ”und die Infor-
mation der Demonstranten ausgesetzt habe”. That translates to ”the reopening of the public space
and the information [noun] of the protesters were suspended”, instead of informing the protesters.
Furthermore, it wrongly separated the two sentences, so the first sentence has extra words and the
second sentence is left without a subject. The pretrained model does not make any of these mistakes.
However, both models make a vocabulary mistake of ”zu installieren”, which is typically only used
to refer to installing software. A human evaluator fluent in both German and English said that the
pretrained version was better.
15
Under review as a conference paper at ICLR 2017
Source
The low February temperatures, not only did they cause losses of millions for the agricultural
sector, but they limited the possibilities of the state economy to grow, causing a contraction of
the economic activity in general of 3.6 percent in the first half of the year, mainly supported
by the historic fall of 31.16 per cent in agriculture, which affected the dynamics of other
economic sectors.
Ground Truth
Die niedrigen Temperaturen im Februar verursachten nicht nur Verluste in Millionenhohe in
der Landwirtschaft , sondern steckten daruber hinaus dem Wachstum der Staatswirtschaft
enge Grenzen und verursachten im ersten Vierteljahr einen allgemeinen RUCkgang der
Wirtschaftstatigkeit um 3,6 Prozent Dieser geht hauptSaChIiCh auf den historischen Abbau
der landwirtschaftlichen Entwicklung Um 31,16 Prozent ZurUCk, der sich bremsend auf weit-
ere Wirtschaftssektoren auswirkte .
No pretrain
Die niedrigen Temperaturen im Februar fuhrten nicht nur ZU Verlusten fur die Landwirtschaft
,sondern sie beschrankten die Moglichkeiten der staatlichen Wirtschaft, wachsen zu wach-
sen , wodurch die Wirtschaftstatigkeit insgesamt von 3,6 Prozent in der ersten Halfte des
Jahres, hauptSaChIiCh durch den historischen Ruckgang von 31.16 % in der Landwirtschaft,
beeinflusst wurde, was die Dynamik anderer Wirtschaftssektoren betraf.
Pretrained
Die niedrigen Temperaturen im Februar fuhrten nicht nur zu Verlusten von Millionen fur den
Agrarsektor, sondern beschrankten die Moglichkeiten der Staatswirtschaft, zu wachsen, was
zu einer Schrumpfung der Wirtschaftstatigkeit im Allgemeinen von 3,6 Prozent in der ersten
Halfte des Jahres fuhrte , hauptSaChIiCh durch den historischen Einbruch von 316 Prozent in
der Landwirtschaft , der die Dynamik anderer Wirtschaftsbereiche beeinflusst hatte .
Table 10:	The human evaluator noted that the pretrained version is better, as it correctly captures the
meaning and sentence structure of the middle. The no pretrain model does not misses translating
the word ”million”, repeats itself in ”wachsen zu wachsen”, and puts the verb ”beeinflusst wurde”
is an unnatural position. However, the pretrained model makes a mistake in the percentage (316%
instead of 31.16%).
16
Under review as a conference paper at ICLR 2017
Source
To facilitate the inception of the Second World War, they allowed bankers and politicians
to create a latent conflict situation by saddling Germany with huge war reparations, thereby
making a radicalist example of the impoverished masses, it remained only to introduce a
sufficiently convincing culprit and a leader with a simple solution, while also creating a multi-
racial Czechoslovakia with a strong German minority to play, and indeed did, the role of a
fifth colony, once the war had been ignited.
Ground Truth
Um den Zweiten Weltkrieg einfacher entfachen ZU konnen , lieβen die Banker durch die
Politik eine latente Konfliktsituation schaffen , indem sie Deutschland mit gigantischen
Kriegsreparationen belegten ; dadUrch schUfen sie die VoraUssetzUng verarmter Massen , so
dass sie den Deutschen nur noch einen ausreichend starken FUhrer unterjubeln mussten , der
die Schuldigen benannte und einfache Losungen anbot; ein weiterer Faktor war die Schaf-
fung des Vielvolkerstaates Tschechoslowakei mit einer starken deutschen Minderheit, die die
Funktion einer funften Kolonne einnehmen sollte und auch einnahm , um den Kriegsbrand
zu entfachen .
No pretrain
Um die Grundung des Zweiten Weltkriegs zu erleichtern , ermoglichte es den Bankern und
Politikern , eine latente Konfliktlage zu schaffen , indem sie Deutschland mit enormen Repa-
rationsforderungen konfrontierte , wodurch ein radikalislamistisches Beispiel der verarmten
Massen entstand , es blieb nur , einen ausreichend aussagekraftigen Schuldigen und einen
Fuhrer mit einer einfachen LoSUng ZU etablieren , wahrend gleichzeitig eine multi-ethnische
Tschechoslowakei mit einer starken deutschen Minderheit zu spielen war und tatsaChIiCh die
Rolle einer funften Kolonie war .
Pretrained
Um die Einfuhrung des Zweiten Weltkrieges zu erleichtern , lieβen sie Banker und Politiker
eine latente Konfliktlage schaffen , indem sie Deutschland mit riesigen Reparationszahlungen
belieferten , wodurch ein radikalislamistisches Beispiel der verarmten Massen entstand , es
blieb nur, einen ausreichend uberzeugenden Schuldigen und einen Fuhrer mit einer einfachen
Losung zu prasentieren , wahrend gleichzeitig eine multiethnische Tschechoslowakei mit
einer starken deutschen Minderheit geschaffen wurde , um zu spielen , und tatsaChIiCh , die
Rolle einer funften Kolonie , sobald der Krieg entfacht worden war .
Table 11:	An example where the English source is poorly worded. Both models output poor trans-
lations, but the evaluator noted that the pretrained version is still better than the no pretrain version.
Interestingly, both models mistranslate ”radical” as ”radikalislamistisches”, which means ”radical
Islam”, which is probably a bias in the training data.
17
Under review as a conference paper at ICLR 2017
Source
The total vote count will also be done if at the end of the ordinary calculation is established
that the difference between the winner and the candidate placed on second position is equal
to or less than one percentage point, as long as there is a request of the representative of
the political party whose candidate came on the second position, case in which there will be
excluded the electoral boxes that have been considered during the partial recount.
Ground Truth
Die StimmenaUSzahIung kann auch in ihrer Gesamtheit erfolgen , wenn nach Abschluss
der ordentlichen Berechnung festgestellt wird , dass der Unterschied zwischen dem mut-
maβlichen Gewinner und dem Kandidaten auf dem zweiten Platz gleich oder geringer als ein
Prozent ist, vorausgesetzt es liegt ein ausdrucklicher Antrag von einem Vertreter der Partei,
deren Kandidat Zweiter geworden ist, vor . In diesem Fall wurden die Wahlpakete, die einer
teilweisen Auszahlung ausgesetzt wurden , ausgeschlossen .
No pretrain
Die gesamte Stimmenanzahl wird auch dann erreicht , wenn am Ende der ordentlichen
Berechnung festgestellt wird , dass der Unterschied zwischen dem Sieger und dem Kandi-
daten , der auf der zweiten Position liegt, gleich oder weniger als einen Prozentpunkt betragt
, vorausgesetzt , dass der Vertreter der Partei , deren Kandidat auf der zweiten Position ist , der
Fall ist, in dem die Wahlunterlagen, die wahrend der teilweisen RUCkzahIUng berucksichtigt
wurden , ausgeschlossen werden .
Pretrained
Die Gesamtzahl der Stimmzettel wird auch dann durchgefuhrt , wenn am Ende der or-
dentlichen Berechnung festgestellt wird , dass der Unterschied zwischen dem Gewinner und
dem auf den zweiten Platz platzierten Kandidaten gleich oder weniger als einen Prozent-
punkt betragt, solange es einen Antrag des Vertreters der politischen Partei gibt, dessen
Kandidat auf die zweite Position kam , in dem es die Wahlzettel ausklammert, die wahrend
der Teilzahlung berucksichtigt wurden .
Table 12:	Another example where the English source is poorly worded. Both models get the struc-
ture right, but have a variety of problematic translations. Both models miss the meaning of ”total
vote count”. They both also translate ”electoral boxes” poorly - the no pretrain model calls it ”elec-
toral paperwork” while the pretrained model calls it ”ballots”. These failures may be because of the
poorly worded English source. The human evaluator found them both equally poor.
18