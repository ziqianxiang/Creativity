Under review as a conference paper at ICLR 2017
The Loss Surface of Residual Networks:
Ensembles & the Role of Batch Normalization
Etai Littwin & Lior Wolf
The School of Computer Science
Tel Aviv University, Israel
{etailittwin,liorwolf}@gmail.com
Ab stract
Deep Residual Networks present a premium in performance in comparison to con-
ventional networks of the same depth and are trainable at extreme depths. It has
recently been shown that Residual Networks behave like ensembles of relatively
shallow networks. We show that these ensembles are dynamic: while initially
the virtual ensemble is mostly at depths lower than half the network’s depth, as
training progresses, it becomes deeper and deeper. The main mechanism that con-
trols the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch
Normalization technique. We explain this behavior and demonstrate the driving
force behind it. As a main tool in our analysis, we employ generalized spin glass
models, which we also use in order to study the number of critical points in the
optimization of Residual Networks.
1	Introduction
Residual Networks (He et al., 2015) (ResNets) are neural networks with skip connections. These
networks, which are a specific case of Highway Networks (Srivastava et al., 2015), present state
of the art results in the most competitive computer vision tasks including image classification and
object detection.
The success of residual networks was attributed to the ability to train very deep networks when
employing skip connections (He et al., 2016). A complementary view is presented by Veit et al.
(2016), who attribute it to the power of ensembles and present an unraveled view of ResNets that
depicts ResNets as an ensemble of networks that share weights, with a binomial depth distribution
around half depth. They also present experimental evidence that short paths of lengths shorter than
half-depth dominate the ResNet gradient during training.
The analysis presented here shows that ResNets are ensembles with a dynamic depth behavior.
When starting the training process, the ensemble is dominated by shallow networks, with depths
lower than half-depth. As training progresses, the effective depth of the ensemble increases. This
increase in depth allows the ResNet to increase its effective capacity as the network becomes more
and more accurate.
Our analysis reveals the mechanism for this dynamic behavior and explains the driving force behind
it. This mechanism remarkably takes place within the parameters of Batch Normalization (Ioffe
& Szegedy, 2015), which is mostly considered as a normalization and a fine-grained whitening
mechanism that addresses the problem of internal covariate shift and allows for faster learning rates.
We show that the scaling introduced by batch normalization determines the depth distribution in the
virtual ensemble of the ResNet. These scales dynamically grow as training progresses, shifting the
effective ensemble distribution to bigger depths.
The main tool we employ in our analysis is spin glass models. Choromanska et al. (2015a) have
created a link between conventional networks and such models, which leads to a comprehensive
study of the critical points of neural networks based on the spin glass analysis of Auffinger et al.
(2013). In our work, we generalize these results and link ResNets to generalized spin glass models.
These models allow us to analyze the dynamic behavior presented above. Finally, we apply the
results of Auffinger & Arous (2013) in order to study the loss surface of ResNets.
1
Under review as a conference paper at ICLR 2017
2	A recap of Choromanska et al. (20 1 5a)
We briefly summarize Choromanska et al. (2015a), which connects the loss function of multilayer
networks with the hamiltonian of the p spherical spin glass model, and state their main contributions
and results. The notations of our paper are summarized in Appendix A and slightly differ from those
in Choromanska et al. (2015a).
A simple feed forward fully connected network N , with p layers and a single output unit is consid-
ered. Let ni be the number of units in layer i, such that n0 is the dimension of the input, and np = 1.
It is further assumed that the ReLU activation functions denoted by R() are used. The output Y of
the network given an input vector x ∈ Rd can be expressed as
dγ	p
Y=XXxij Aij Y wij ,	(1)
i=1 j=1	k=1
where the first summation is over the network inputs x1...xd, and the second is over all paths from
input to output. There are γ = ip=1 ni such paths and ∀i, xi1 = xi2 = ...xiγ. The variable
Aij ∈ {0, 1} denotes whether the path is active, i.e., whether all of the ReLU units along this
path are producing positive activations, and the product Qkp=1 wi(jk) represents the specific weight
configuration wi1j...wikj multiplying xi given path j. Itis assumed throughout the paper that the input
variables are sampled i.i.d from a normal Gaussian distribution.
Definition 1. The mass of the network N is defined as ψ = Qip=0 ni.
The variables Aij are modeled as independent Bernoulli random variables with a success probability
ρ, i.e., each path is equally likely to be active. Therefore,
dγ	p
EA[Y] = XXxijρ Y wi(jk).	(2)
i=1 j=1	k=1
The task of binary classification using the network N with parameters w is considered, using either
the hinge loss LhN or the absolute loss LaN :
LhN (w) = EA[max(0, 1 - YxY)], LaN (w) = EA[|Yx -Y|]	(3)
where Yx is a random variable corresponding to the true label of sample x. In order to equate either
loss with the hamiltonian of the p-spherical spin glass model, a few key approximations are made:
A1 Variable independence - The inputs xij are modeled as independent normal Gaussian random
variables.
A2 Redundancy in network parameterization - It is assumed the set of all the network weights
[w1, w2...wN] contains only Λ unique weights such that Λ < N.
A3 Uniformity - It is assumed that all unique weights are close to being evenly distributed on the
graph of connections defining the network N . Practically, this means that we assume every
node is adjacent to an edge with any one of the Λ unique weights.
A4 Spherical constraint - The following is assumed:
1Λ
1X w2 = C2	(4)
i=1
for some constant C > 0.
These assumptions are made for the sake of analysis, and do not necessarily hold. The validity of
these assumption was posed as an open problem in Choromanska et al. (2015b), where a different
degree of plausibility was assigned to each. Specifically, A1, as well as the independence assumption
of Aij , were deemed unrealistic, and A2 - A4 as plausible. For example, A1 does not hold since
each input xi is associated with many different paths and xi1 = xi2 = ...xiγ. See Choromanska
et al. (2015a) for further justification of these approximations.
2
Under review as a conference paper at ICLR 2017
Under A1-A4, the loss takes the form of a centered Gaussian process on the sphere Sλ-1(√Λ).
Specifically, it is shown to resemble the hamiltonian of the a spherical p-spin glass model given by:
1Λ	r
Hp,Λ(W) = -7-1 X xiι...ip Y Wik	⑸
2 iι …ip	k=1
with spherical constraint
1Λ
-X w2 = 1	⑹
i=1
where xi1...ip are independent normal Gaussian variables.
In Auffinger et al. (2013), the asymptotic complexity of spherical p spin glass model is analyzed
based on random matrix theory. In Choromanska et al. (2015a) these results are used in order to
shed light on the optimization process of neural networks. For example, the asymptotic complexity
of spherical spin glasses reveals a layered structure of low-index critical points near the global op-
timum. These findings are then given as a possible explanation to several central phenomena found
in neural networks optimization, such as similar performance of large nets, and the improbability of
getting stuck in a “bad” local minima.
As part of our work, we follow a similar path. First, a link is formed between residual networks and
the hamiltonian of a general multi-interaction spherical spin glass model as given by:
pΛ	r
Hp,Λ(w)=E —r-i	E	xi1,i2...ir Πwik	⑺
r=1	2 iι,i2 …ir = 1	k=1
where 1...p are positive constants. Then, using Auffinger & Arous (2013), we obtain insights on
residual networks. The other part of our work studies the dynamic behavior of residual networks,
where we relax the assumptions made for the spin glass model.
3 Residual nets and general spin glass models
We begin by establishing a connection between the loss function of deep residual networks and the
hamiltonian of the general spherical spin glass model. We consider a simple feed forward fully
connected network N , with ReLU activation functions and residual connections. For simplicity of
notations without the loss of generality, we assume n1 = ... = np = n. n0 = d as before. In our
ResNet model, there exist p - 1 identity connections skipping a single layer each, starting from the
first hidden layer. The output of layer l > 1 is given by:
Nl(x) =R(Wl>Nl-1(x))+Nl-1(x)	(8)
where Wl denotes the weight matrix connecting layer l - 1 with layer l. Notice that the first hidden
layer has no parallel skip connection, and so N1(x) = R(W1>x). Without loss of generality, the
scalar output of the network is the sum of the outputs of the output layer p and is expressed as
p d γr	r
Y = X X X xi(jr)Ai(jr) Y wi(jr)(k)
r=1 i=1 j=1	k=1
(9)
where Ai(jr) ∈ {0, 1} denotes whether path j of length r is open, and ∀j,j0, r, r0 xirj = xirj00. The
residual connections in N imply that the output Y is now the sum of products of different lengths,
indexed by r. Since our ResNet model attaches a skip connection to every layer except the first,
1 ≤ r ≤ p. See Sec. 6 regarding models with less frequent skip connections.
Each path of length r includes r - 1 non-skip connections (those involving the first term in Eq. 8
and not the second, identity term) out of layers l = 2..p. Therefore, γr = rp--11 nr. We define the
following measure on the network:
Definition 2. The mass of a depth r subnetwork in N is defined as ψr = dγr.
The properties of redundancy in network parameters and their uniform distribution, as described in
Sec. 2, allow us to re-index Eq. 9.
3
Under review as a conference paper at ICLR 2017
Lemma 1. Assuming assumptions A2 — A4 hold, and Λ ∈ Z, then the output Can be expressed
after reindexing as:
P	Λ	Λr	r
Y = X X	Xxi(1j,)i2...irAi(1j,)i2...irYwik.	(10)
r=1 i1 ,i2 ...ir =1 j=1	k=1
All proofs can be found in Appendix B.
Making the modeling assumption that the ReLU gates are independent Bernoulli random variables
with probability ρ, we obtain that for every path of length r, E Ai(j,)i ...i = ρr and
P	Λ	ψΛr	r
EA[Y] =X X	Xxi(1j,)i2...irρrYwik.	(11)
r=1 i1 ,i2 ...ir =1 j=1	k=1
In order to connect ResNets to generalized spherical spin glass models, we denote the variables:
ξi1 ,i2 ...i
ψr
Λr
Xj
xi1 ,i2 ...ir,	xi1 ,i2...ir
j=1
ξil ,i2 …ir
Ex [ξ2ι,i2...ir ] 2
(12)
r
Note that since the input variables x1...xd are sampled from a centered Gaussian distribution (de-
pendent or not), then the set of variables X汨 ,i2...ir are dependent normal Gaussian variables.
Lemma 2. Assuming A2 — A3 hold, and Λ ∈ N then ∀r,iι...ir thefollowing holds:
d (Λrr )2 ≤ E[ξ2ι,i2 …“]≤ (Λrr )2.	(13)
We approximate the expected output EA (Y) with Y by assuming the minimal value in 13 holds
such that Vr,iι…ir E[ξ2ι i2...^] = d(ψ)2. This approximation holds exactly when Λ = n, since
all weight configurations of a particular length in Eq. 10 will appear the same number of times.
When Λ 6= n, the uniformity assumption dictates that each configuration of weights would appear
approximately equally regardless of the inputs, and the expectation values would be very close to
the lower bound. The following expression for Y is thus obtained:
PΛr
Y = X(P)r√rd	X	Xii,i2…Y Wik.	(14)
r=1	i1 ,i2 ...ir =1	k=1
The independence assumption A1 was not assumed yet, and 14 holds regardless. Assuming A4 and
F , ∙	.1	IF ∙ 1 , ~	1	1∙ 1 ,1 T ,∙1	i' -X T- . ,1 T , ∙1 , ∙	~
denoting the scaled weights Wi =吉wi, We can link the distribution of Y to the distribution on x:
P	Λr
Y= X √d (ɪ)r	X	xil,i2...ir Y Wik
r=1	i1 ,i2 ...ir =1	k=1
PΛ	r
=Z X Λ≡Γ	X	xil ,i2...ir Y Wik (15)
r=2	2 iι,i2...ir = 1	k = 1
where e『=e/=ɪ (P-：) (PnC)r and Z is a normalization factor such that PP=ι ^T = 1.
The following lemma gives a generalized expression for the binary and hinge losses of the network.
Lemma 3 ( Choromanska et al. (2015a)). Assuming assumptions A2 — A4 hold, then both the
losses LhN (x) and LaN (x) can be generalized to a distribution of the form:
LN (x) = Ci + C2Y	(16)
where C1 , C2 are positive constants that do not affect the optimization process.
4
Under review as a conference paper at ICLR 2017
The model in Eq. 16 has the form of a spin glass model, except for the dependency between the
variables Xi1 河…ir. We later use an assumption similar to A1 of independence between these vari-
ables in order to link the two binary classification losses and the general spherical spin glass model.
However, for the results in this section, this is not necessary.
We denote the important quantities:
β
ρnC
√Λ
(17)
The series (r)rp=1 determines the weight of interactions ofa specific length in the loss surface. No-
tice that for constant depth p and large enough β, argmaxr(r) = p. Therefore, for wide networks,
where n and, therefore, β are large, interactions of order p dominate the loss surface, and the effect
of the residual connections diminishes. Conversely, for constant β and a large enough p (deep net-
works), we have that argmaxr(r) < p, and can expect interactions of order r < p to dominate the
loss. The asymptotic behavior of is captured by the following lemma:
Theorem 1.	Assuming ι+βeP ∈ N, we have that:
lim 1argmax(er) = β-
p→∞ p r	1 + β
(18)
As the next theorem shows, the epsilons are concentrated in a narrow band near the maximal value.
Theorem 2.	For any ɑɪ < 1+^ < α2, and assuming aɪp, α2p, 1+^P ∈ N, it holds that:
α2p
lim X er2
p→∞
r=α1p
(19)
1
Thm. 2 implies that for deep residual networks, the contribution of weight products of order far
away from the maximum 1+^P is negligible. The loss is, therefor, similar in complexity to that of
an ensemble of potentially shallow conventional nets. The next Lemma shows that we can shift the
effective depth to any value by simply controlling C .
Lemma 4. For any integer 1 ≤ k ≤ P there exists a global scaling parameter C such that
arg maxr (er (C)) = k.
A simple global scaling of the weights is, therefore, enough to change the loss surface, from an
ensemble of shallow conventional nets, to an ensemble of deep nets. This is illustrated in Fig. 1(a-c)
for various values of β. Ina common weight initialization scheme for neural networks, C = √n (Orr
& Muller, 2003; Glorot & Bengio, 2010). With this initialization and Λ = n, β = P and the maximal
weight is obtained at less than half the network,s depth limp→∞ argmax『(er) < 2. Therefore, at
the initialization, the loss function is primarily influenced by interactions of considerably lower order
than the depth P, which facilitates easier optimization.
4 Dynamic behavior of residual nets
The expression for the output of a residual net in Eq. 15 provides valuable insights into the machinery
at work when optimizing such models. Thm. 1 and 2 imply that the loss surface resembles that ofan
ensemble of shallow nets (although not a real ensemble due to obvious dependencies), with various
depths concentrated in a narrow band. As noticed in Veit et al. (2016), viewing ResNets as ensembles
of relatively shallow networks helps in explaining some of the apparent advantages of these models,
particularly the apparent ease of optimization of extremely deep models, since deep paths barely
affect the overall loss of the network. However, this alone does not explain the increase in accuracy
of deep residual nets over actual ensembles of standard networks. In order to explain the improved
performance of ResNets, we make the following claims:
1.	The distribution of the depths of the networks within the ensemble is controlled by the
scaling parameter C.
5
Under review as a conference paper at ICLR 2017
2.	During training, C changes and causes a shift of focus from a shallow ensemble to deeper
and deeper ensembles, which leads to an additional capacity.
3.	In networks that employ batch normalization, C is directly embodied as the scale parameter
λ. The starting condition of λ = 1 offers a good starting condition that involves extremely
shallow nets.
For the remainder of Sec.4, we relax all assumptions, and assume that at some point in time
Λι Pλ=1 w2 = C2, and A = N. Using Eq. 9 for the output of the network Y in Lemma. 3, the
loss can be expressed:
p d γr	r
LN (x, w) = C1+ C2 X X X xi(jr)Ai(jr) Y wi(jr)(k)	(20)
r=1 i=1 j=1	k=1
where C1, C2 are some constants that do not affect the optimization process. In order to gain addi-
tional insight into this dynamic mechanism, we investigate the derivative of the loss with respect to
the scale parameter C. Using Eq. 9 for the output, we obtain:
dLNdCw) = (C X r X X x(r)A(r) Y! W(r)(k)	(21)
r=1 i=1 j=1	k=1
Notice that the addition of a multiplier r indicates that the derivative is increasingly influenced by
deeper networks.
4.1	Batch normalization
Batch normalization has shown to be a crucial factor in the successful training of deep residual
networks. As we will show, batch normalization layers offer an easy starting condition for the
network, such that the gradients from early in the training process will originate from extremely
shallow paths.
We consider a simple batch normalization procedure, which ignores the additive terms, has the out-
put of each ReLU unit in layer l normalized by a factor σl and then is multiplied by some parameter
λl. The output of layer l > 1 is therefore:
Nl(x) = λl R(W>Nl-ι(χ)) + Nl-ι(χ)	(22)
σl
where σl is the mean of the estimated standard deviations of various elements in the vector
R(Wl>Nl-1(x)). Furthermore, a typical initialization of batch normalization parameters is to set
∀l, λl = 1. In this case, providing that units in the same layer have equal variance σl, the recursive
relation E[Nl+1(x)j2] = 1 + E[Nl(x)j2] holds for any unit j in layer l. This, in turn, implies that the
output of the ReLU units should have increasing variance σl2 as a function of depth. Multiplying the
weight parameters in deep layers with an increasingly small scaling factor 言,effectively reduces
the influence of deeper paths, so that extremely short paths will dominate the early stages of opti-
mization. We next analyze how the weight scaling, as introduced by batch normalization, provides
a driving force for the effective ensemble to become deeper as training progresses.
4.2	The driving force behind the scale increase
The analysis below focuses on a single residual connection skipping a block of one or more layers.
Since it holds for each block individually, it holds also for a residual network of multiple skipped
blocks of arbitrary depth.
We consider a simple network of depth p, with a single residual connection skipping p - m layers.
We further assume that batch normalization is applied at the output of each ReLU unit as described
in Eq. 22. We denote by l1...lm the indices of layers that are not skipped by the residual connection,
and *m = Qm=I σii,耳=Qp=1 λσi. Since every path of length m is multiplied by *m, and every
path of length p is multiplied by λp, the expression for the loss can be expressed using Eq. 20 and
6
Under review as a conference paper at ICLR 2017
(a)
(d)
Figure 1: (a) A histogram of r(β), r = 1..p, for β = 0.1 and p = 100 . (b) Same for β = 0.5
(c) Same for β = 2. (d) Values (y-axis) of the batch normalization parameters λl (x-axis) for
10 layers ResNet trained to discriminate between 50 multivariate Gaussians (see Appendix C for
more details). Higher plot lines indicate later stages of training. (e) The norm of the weights of a
residual network, which does not employ batch normalization, as a function of the iteration. (f) The
asymptotic of the mean number of critical points of a finite index as a function of β .
ignoring constant terms:
d γm	r	d γp	p
LN (x, w) = λmXX Xj Aj YY w(m)(k)+λp XXxi(jm)Ai(jp) Y wi(jp)( )
i=1 j=1	k=1	i=1 j=1	k=1
= Lm(x, w) + Lp(x, w) (23)
We denote by Vw the derivative operator with respect to the parameters w, and the gradient g =
VwLN(x, W)= gm + gp evaluated at point w.
Theorem 3.	Considering the loss in 23, and assuming dL^Zd(X,w) = 0, then for a small learning
rate 0 < μ << 1 the following hold:
1.	For any λl∈l1...lm then:
λι- μdLN(Ww -μg) > ∣λι|	(24)
2.	For any λl6∈l1...lm, if kgpk22 + gp>gm > 0 then:
λι- μd LN - W -"g) > ∣λι |	(25)
Thm. 3 suggests that ∣λl | will increase for layers l that do not have skip-connections. Conversely,
if layer l has a parallel skip connection, then ∣λl | will increase if ∣∣gp∣∣2 > ∣∣gm ∣∣2, where the later
condition implies that shallow paths are nearing a local minima. Notice that an increase in | λl∈l1 …lm |
results in an increase in ∣λp∣, while ∣λm | remains unchanged, therefore shifting the balance into
deeper ensembles.
This steady increase of ∣λl|, as predicted in our theoretical analysis, is also backed in experimen-
tal results, as depicted in Fig. 1(d). Note that the first layer, which cannot be skipped, behaves
differently than the other layers. More experiments can be found in Appendix C.
7
Under review as a conference paper at ICLR 2017
It is worth noting that the mechanism for this dynamic property of residual networks can also be
observed without the use of batch normalization, as a steady increase in the L2 norm of the weights,
as shown in Fig. 1(e). In order to model this, consider the residual network as discussed above,
without batch normalization layers. Recalling, kwk2 = C√Λ, W = C, the loss of this network is
expressed as:
d γm	r	d γp	p
LN (x, w) = Cm XXm
χ(m)A(m) Y w(m)(k) + CP XXp
x(m)A(p) Y w(p)(k)
i=1 j =1	k=1	i=1 j =1	k=1
= Lm(x, w) + Lp(x, w) (26)
Theorem 4.	Considering the loss in 26, and assuming Ldxw = 0，then for a small learning
rate 0 < μ << 1 the following hold:
dLN(X,Wμg) ≈ -μJ(m∣∣gmk2 + PIIgpI∣2 + (m + p)g>gm)	(27)
∂C	C
Thm. 4 indicates that if either Igp I2 or Igm I2 is dominant (for example, near local minimas of
the shallow network, or at the start of training), the scaling of the weights C will increase. This
expansion will, in turn, emphasize the contribution of deeper paths over shallow paths, and in-
crease the overall capacity of the residual network. This dynamic behavior of the effective depth of
residual networks is of key importance in understanding the effectiveness of these models. While
optimization starts off rather easily with gradients largely originating from shallow paths, the overall
advantage of depth is still maintained by the dynamic increase of the effective depth.
5 The loss surface of Ensembles
We now present the results of Auffinger & Arous (2013) regarding the asymptotic complexity in the
case of limΛ→∞ of the multi-spherical spin glass model given by:
∞Λ
He,A = - X -T-1	X Jr …“ wi2 …Wir	(28)
r=2 λ 2 iι,...ir = 1
where Jir ...i are independent centered standard Gaussian variables, and = (r )r>2 are positive
real numbers such that Pr∞=2 r 2r < ∞. A configuration W of the spin spherical spin-glass model
is a vector in RΛ satisfying the spherical constraint:
∞
Xr2=1
=2
Note that the variance of the process is independent of :
∞Λ	∞
E[H2,Λ] = X Λ1-r r2(Xwi2)r =ΛXr2=Λ
=2	i=1	=1
(29)
(30)
Definition 3. We define the following:
∞
v0 =Xr2r,
=2
∞
v00 = Xr2r(r - 1),
=2
α2 = v00 + v0 - v02
(31)
Note that for the single interaction spherical spin model α2 = 0. The index of a critical point of
HaA is defined as the number of negative eigenvalues in the hessian V2He,Λ evaluated at the critical
point W.
Definition 4. For any 0 ≤ k < Λ and u ∈ R, we denote the random number C rtλ,k (u, ) as the
number of critical points of the hamiltonian in the set BX = {ΛX |X ∈ (-∞, u)} with index k.
That is:
CrtA,k(u, e) =	E	1 {He,A ∈ Λu} 1 {i(V2He,A) = k}	(32)
w: VHe,Λ = 0
8
Under review as a conference paper at ICLR 2017
Furthermore, define θk(u, e) = limΛ→∞ -χlogE[Crt^k(ue)]. Corollary 1.1 of Auffinger & Arous
(2013) states that for any k > 0:
θk (R, )
1	v00	v00 - v0
-loq(——)-------------
2	v0 '	v00 + v0
(33)
Eq. 33 provides the asymptotic mean total number of critical points with non-diverging index k. Itis
presumed that the SGD algorithm will easily avoid critical points with a high index that have many
descent directions, and maneuver towards low index critical points. We, therefore, investigate how
the mean total number of low index critical points vary as the ensemble distribution embodied in
(r )r>2 changes its shape by a steady increase in β.
Fig. 1(f) shows that as the ensemble progresses towards deeper networks, the mean amount of low
index critical points increases, which might cause the SGD optimizer to get stuck in local minima.
This is, however, resolved by the the fact that by the time the ensemble becomes deep enough,
the loss function has already reached a point of low energy as shallower ensembles were more
dominant earlier in the training. In the following theorem, we assume a finite ensemble such that
Pr∞=p+1 r 2r ≈0.
Theorem 5. For any k ∈ N,p > 1, we denote the solution to the following constrained optimization
problems:
p
It holds that:
e* = arg max θk (R, e) s.t ɪ2 V = 1
r=2
* = 1, r=p
r 0, otherwise
(34)
(35)
Thm. 5 implies that any heterogeneous mixture of spin glasses contains fewer critical points of a
finite index, than a mixture in which only p interactions are considered. Therefore, for any distribu-
tion of e that is attainable during the training ofa ResNet of depth p, the number of critical points is
lower than the number of critical points for a conventional network of depth p.
6	Discussion
In this work, we use spin glass analysis in order to understand the dynamic behavior ResNets dis-
play during training and to study their loss surface. In particular, we use at one point or another the
assumptions of redundancy in network parameters, near uniform distribution of network weights, in-
dependence between the inputs and the paths and independence between the different copies of the
input as described in Choromanska et al. (2015a). The last two assumptions, i.e., the two indepen-
dence assumptions, are deemed in Choromanska et al. (2015b) as unrealistic, while the remaining
are considered plausible.
Our analysis of critical points in ensembles (Sec. 5) requires all of the above assumptions. However,
Thm. 1 and 2, as well as Lemma. 4, do not assume the last assumption, i.e., the independence
between the different copies of the input. Moreover, the analysis of the dynamic behavior of residual
nets (Sec. 4) does not assume any of the above assumptions.
Our results are well aligned with some of the results shown in Larsson et al. (2016), where it is
noted empirically that the deepest column trains last. This is reminiscent of our claim that the deeper
networks of the ensemble become more prominent as training progresses. The authors of Larsson
et al. (2016) hypothesize that this is a result of the shallower columns being stabilized at a certain
point of the training process. In our work, we discover the exact driving force that comes into play.
In addition, our work offers an insight into the mechanics of the recently proposed densely connected
networks (Huang et al., 2016). Following the analysis we provide in Sec. 3, the additional shortcut
paths decrease the initial capacity of the network by offering many more short paths from input
to output, thereby contributing to the ease of optimization when training starts. The driving force
mechanism described in Sec. 4.2 will then cause the effective capacity of the network to increase.
Note that the analysis presented in Sec. 3 can be generalized to architectures with arbitrary skip
connections, including dense nets. This is done directly by including all of the induced sub networks
in Eq. 9. The reformulation of Eq. 10 would still holds, given that Ψr is modified accordingly.
9
Under review as a conference paper at ICLR 2017
7	Conclusion
Ensembles are a powerful model for ResNets, which unravels some of the key questions that have
surrounded ResNets since their introduction. Here, we show that ResNets display a dynamic en-
semble behavior, which explains the ease of training such networks even at very large depths, while
still maintaining the advantage of depth. As far as we know, the dynamic behavior of the effective
capacity is unlike anything documented in the deep learning literature. Surprisingly, the dynamic
mechanism typically takes place within the outer multiplicative factor of the batch normalization
module.
References
Antonio Auffinger and Gerard Ben Arous. Complexity of random smooth functions on the high-
dimensional sphere. Annals of Probability, 41(6):4214-4247,11 2013.
Antonio Auffinger, Grard Ben Arous, and Ji ern. Random matrices and complexity of spin glasses.
Communications on Pure and Applied Mathematics, 66(2):165-201, 2 2013. doi: 10.1002/cpa.
21422.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In AISTATS, 2015a.
Anna Choromanska, Yann LeCun, and Gerard Ben Arous. Open problem: The landscape of the loss
surfaces of multilayer networks. In COLT, pp. 1756-1760, 2015b.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. arXiv preprint arXiv:1512.03385, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. arXiv preprint arXiv:1603.05027, 2016.
Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks.
arXiv preprint arXiv:1608.06993, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, pp. 448-456, 2015.
Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, 2009.
URL http://www.cs.toronto.edu/〜{}kriz∕learning-features-2009-TR.
pdf.
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural net-
works without residuals. arXiv preprint arXiv:1605.07648, 2016.
Genevieve B Orr and Klaus-Robert Muller. Neural networks: tricks of the trade. Springer, 2003.
Rupesh Kumar Srivastava, Klaus Greff, and Jurgen Schmidhuber. Highway networks. arXivpreprint
arXiv:1505.00387, 2015.
Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles of
relatively shallow networks. In NIPS, 2016.
A Summary of notations
Table 1 presents the various symbols used throughout this work and their meaning.
10
Under review as a conference paper at ICLR 2017
SYMBOL	Table 1: Notations DESCRIPTION	
x d Ni(x) Y Yx LN LhN LaN p w C W) n Λ N Wl Hp,λ H,Λ γ ψ γr ψr R(∙) Aij ρ r(β) β z λl σl	Input vector ∈ Rd, sampled from a normal distribution The dimensionality of the input x The output of layer i of network N given input x The final output of the network N True label of input x Loss function of network N Hinge loss Absolute loss The depth of network N Weights of the network w ∈ RΛ A positive scale factor such that kw∣∣2 = √ΛC Scaled weights such that W = = W The number of units in layers l > 0 The number of unique weights in the network The total number of weights in the network N The weight matrix connecting layer l - 1 to layer l in N. The hamiltonian of the p interaction spherical spin glass model. The hamiltonian of the general spherical spin glass model. Total number of paths from input to output in network N γd Total number of paths from input to output in network N of length r γrd ReLU activation function Bernoulli random variable associated with the ReLU activation function, indexed by ij . Parameter of the Bernoulli distribution associated with the ReLU unit multiplier associated with paths of length r in N. ρnC √Λ . Normalization factor. Batch normalization multiplicative factor in layer l. The mean of the estimated standard deviation various elements in R(Wl>Nl-1(x)).
B Proofs
Proof of Lemma 1. There are a total of ψr paths of length r from input to output, and a total of
Λr unique r length configurations of weights. The uniformity assumption then implies that each
configuration of weights is repeated ψr times. By summing over the unique configurations, and re
indexing the input We arrive at Eq. 10.	口
Proofof Lemma 2. From 12, we have that ξi1 缄…％ is defined as a sum of 'ψ inputs. Since there are
only p distinct inputs, it holds that for each ξi1,i2...ir there exists a sequence α = (αi)ip=1 ∈ N such
that Pd=1 αi = ψr, and £/面...“ =Pd=1。，/，. We, therefore, have that E[ξ21,i2…ir] = ||。花.
Note that the minimum value of E[ξi2 ,i ...i ] is a solution to the following:
	min(E[ξ2ι,i2 …ir]) = minα(kαk2) s.t kαk1 = Λ, (αi))p=1 ∈ N,	(36)
which achieves its minimal value at ∀i, αi = 1ψr. Similarly, the maximum value is achieved at
αi = Λr δi for some index i.	口
11
Under review as a conference paper at ICLR 2017
ProofofThm. 1. We use the Stirling approximation, which states limp→∞ Plog((Op)) = H(α),
where H(α) = -αlog(α) - (1 - α)log(1 - α). Ignoring the constants which do not depend on α,
lim ɪlog( I P) βαp) = H(α) + αlog(β)	(37)
p→∞ p αp
which achieves its maximum value at α = α*.	□
Proof of Thm. 2. For brevity, we provide a sketch of the proof. It is enough to show that
limp→∞ Prα=1p1 r2 = 0 for β < 1. Ignoring the constants in the binomial terms, we have:
α1p
lim	i2 = lim
p→∞	p→∞
α1p
i=1
(p)2β
z2
2r
≤ lim
p→∞
αlP(αpp)2β2α1p
z2
(38)
Σ
Where z2 = Pp=ι (P)2β2r, which can be expressed using the Legendre polynomial of order p:
1 + β2
z2 = (1- β2)pPp(τ^⅛)	(39)
1-β
In order to compute the limit of Eq. 38, we use the asymptotic of the Legendre polynomial of order
p for x > 1, Pp(X)〜√2∏p (x+√χ2-11P+2. For the term in the nominator of Eq. 38 , We use the
Stirling approximation for factorials p!〜 √2πp(p)p. Substituting both approximations in Eq. 38
and taking the limit completes the proof.	□
Proof of Lemma 4. For simplicity, we ignore the constants in the binomial coefficient, and assume
Cr = 1 (p)βr. Notice that for β* = (P), we have that arg maxr(∈r (β*)) = p, argmax『(金(表))=
1 and argmaxr&(1)) = p. From the monotonicity and continuity of βr, any value 1 ≥ k ≥ P can
be attained. The linear dependency β(C) = ρ√C completes the proof.	□
Proof of Thm. 3. 1. Notice that by definition, layer l is not skipped by the residual connection,
and therefore λι multiplies every path in the network. Therefore, dLN^w，= -1 (Lm(x, W) +
Lp(x, w)). Using taylor series expansion:
∂Ln(x, w - μg)	∂Ln(x, w)	∂LN(x, w)
—∂λ— ≈ ^^λ^ - μvw	g
Substituting Vw 'LNxxW = -■ (gm + gp) in 40 We have:
--N'，-----W9w≈ ≈ 0 - μ (gm + gp)>(gm + gp) = -μ-kgm + gp k 2 < 0
∂λl	λl	λl
And hence:
dLN(X, W - μgw) ι 2 1......................
λl - μ-------∂λ--------- = λl + μ λ kgm + gPk 2
=λl(I + μ2 * kgm + gpk2)
Finally:
lλl(1 + μ2 父 kgm + gPk2)1 =内|(I + μ2 λ ) ≥ 内|
2. Since paths of length m skip layer l, we have that Vw "NX；W = -■ gp. Therefore:
'£N(χ,W μg) ≈ 0 - μ-L(gm + gp)>gp = -μγ(gmbgp + Ilgpk2)
∂λl	λl	λl
The condition kgpk2 > kgmk2 implies that gm> gp + kgpk22 > 0, completing the proof.
(40)
(41)
(42)
(43)
(44)
□
12
Under review as a conference paper at ICLR 2017
ProofofThm 4. Notice that "N*,W) = dLNχ,w) ∂∂⅛√Λ = g> W = 0, and hence the gradient
is orthogonal to the weights. We have that dLNχ,W) = C (mLm(χ, w) + pLp(χ, w)). Using taylor
series expansion we have:
∂Lν(x, w - μg)
∂C
∂LN (x, w)	∂LN(x, w)
≈ ―τc-----闷包 ∂C g
(45)
For the last term we have:
▽w d LN(X, W) g = (mLm(x, w) + pLp(x, w))Vw -^-^ g + ɪ(mgm + Pgp )>g
∂C	kwk2 C
wg 1	>	1	>
=(mLm(x, w) + PLp(X, W)) —3- + κ(mgm + Pgp)Tg = K(mgm + Pgp)>g, (46)
C 2 C	C
where the last step stems from the fact that w>g = 0. Substituting VW dLNχ,W) = CC (mgm + Pgp)
in 45 we have:
dLN(x,W "gw) ≈ 0 - μɪ(mgm + Pgp)>(gm + gp)
∂C	C
=-μ ^(mkgpk2 + Pkgpk2 + (m + P)g>gm) (47)
C
□
Proof of Thm 5. Inserting Eq. 31 into Eq. 33 we have that:
θk (R, e) = 1 log( PpP『『2 - 1)-
2	r=2 r r
Pp=2 e"(r - 2)
P=不
(48)
We denote the matrices V 0 and V00 such that Vi0j = rδij and Vi0j0 = r(r-1)δij. We then have:
θk(R,)
1	e> V00e e>(V00 - V0)e
2log( e>V0e ) - e> (V00 + V0)e
(49)
1	>V00	>(V00-V0)
maxeθk(R, e) ≤ maxe(2log( e>V0e )) - min√ 5丁〃 +『把)
=1 log^maχi(Vii0⅞i-1/) - mini (Vvii - VOi)(V00 + Vii)T)
12
=ROg(P - I)-(I- -) = θk(R, C (5O)
2P
□
C Additional experiments
Fig. 1(d) and 1(e) report the experimental results of a straightforward setting, in which the task is
to classify a mixture of 10 multivariate Gaussians in 50D. The input is therefore of size 50. The
loss employed is the cross entropy loss of ten classes. The network has 10 blocks, each containing
20 hidden neurons, a batch normalization layer, and a skip connection. Training was performed on
10,000 samples, using SGD with minibatches of 50 samples.
Next, we provide additional experiments performed on the public CIFAR-10 and CIFAR-100 data
sets (Krizhevsky, 2009). The public ResNet code of https://github.com/facebook/fb.
resnet.torch is used for networks of depth 32.
As noted in Sec. 4.2, the dynamic behavior can be present in the Batch Normalization multiplica-
tive coefficient or in the weight matrices themselves. In the following experiments, it seems that
13
Under review as a conference paper at ICLR 2017
until the learning rate is reduced, the dynamic behavior is manifested in the Batch Normaliza-
tion multiplicative coefficients and then it moves to the convolution layers themselves. We there-
fore absorb the BN coefficients into the convolutional layer using the public code of https:
//github.com/e-lab/torch-toolbox/tree/master/BN-absorber. Note that the
multiplicative coefficient of Batch Normalization is typically refereed to as γ. However, throughout
our paper, since we follow the notation of Choromanska et al. (2015a), γ refers to the number of
paths. The multiplicative factor of Batch normalization appears as λ in Sec. 4.
Fig. 2 depicts the results. There are two types of plots: Fig. 2(a,c) presents for CIFAR-10 and
CIFAR-100 respectively the magnitude of the various convolutional layers for multiple epochs (sim-
ilar in type to Fig. 1(d) in the paper). Fig. 2(b,d) depict for the two datasets the mean of these norms
over all convolutional layers as a function of epoch (similar to Fig. 1(e)).
As can be seen, the dynamic phenomenon we describe is very prominent in the public ResNet
implementation when applied to these conventional datasets: the dominance of paths with fewer
skip connections increases over time. Moreover, once the learning rate is reduced in epoch 81 the
phenomenon we describe speeds up.
In Fig. 3 we present the multiplicative coefficient of the Batch Normalization when not absorbed.
As future work, we would like to better understand why these coefficients start to decrease once the
learning rate is reduced. As shown above, taking the magnitude of the convolutions into account,
the dynamic phenomenon we study becomes even more prominent at this point. The change of
location from the multiplicative coefficient of the Batch Normalization layers to the convolutions
themselves might indicate that Batch Normalization is no longer required at this point. Indeed,
Batch Normalization enables larger training rates and this shift happens exactly when the training
rate is reduced. A complete analysis is left for future work.
14
Under review as a conference paper at ICLR 2017
Figure 2: (a,c) The Norm of the convolutional layers once the factors of the subsequent Batch
Normalization layers are absorbed, shown for CIFAR-10 and CIFAR-100 respectively. Each graph
is a different epoch, see legend. Waving is due to the interleaving architecture of the convolutional
layers. (b,d) Respectively for CIFAR-10 and CIFAR-100, the mean of the norm of the convolutional
layers’ weights per epoch.
(d)
15
Under review as a conference paper at ICLR 2017
Figure 3: The norms of the multiplicative Batch Normalization coefficient vectors. (a,c) The Norm
of the coefficients, shown for CIFAR-10 and CIFAR-100 respectively. Each graph is a different
epoch (see legend). Since there is no monotonic increase between the epochs in this graph, it is
harder to interpret. (b,d) Respectively for CIFAR-10 and CIFAR-100, the mean of the norm of the
multiplicative factors per epoch.
(d)
16