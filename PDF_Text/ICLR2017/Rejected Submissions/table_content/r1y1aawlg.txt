Table 1: Accuracy of the error detection model f compared to baselines on the concatenation of theWMT test sets from 2008 to 2015. For precision, recall and F1 we consider a positive predictionas labeling a word as a mistake. Baseline fcor labels all words as correct, fwrong labels all words asincorrect, fstat labels a word from yg based on the prior probability estimated on the training data.
Table 2: Validation BLEU (selecting substitution heuristics, decision thresholds t, and number ofmaximum allowed modifications N). BLEU is reported on a 3,041 validation sentences.
Table 3: Test accuracy on WMT test sets after applying our refinement procedure.
Table 4: Examples of good refinements performed by our system on our test sets. The model clearlyimproves the quality of the initial guess translations.
Table 5: Refinements of mixed quality. Our model is not able to insert new words, and so sometimesit replaces a relevant word with another relevant word. In other cases, improvements are insignifi-cant, or good word replacements are mixed with poor ones.
Table 6: Examples of poor refinements. Our model does not improve the translation or decreasesthe quality of the translation.
