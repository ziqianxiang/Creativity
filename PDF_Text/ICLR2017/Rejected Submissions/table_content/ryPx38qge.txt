Table 1: Accuracy of scattering compared to similar architectures on CIFAR10		Table 2: Accuracy of scattering compared to similar architectures on CIFAR100	Architecture	Accuracy	Architecture	AccuracyUnsupervised architectures Roto-translation scattering	82.3	Unsupervised architectures Roto-translation scattering	56.8Scattering (ours) + 3 FC +no data augmentation	78.6	Scattering (ours) + 3 FC + no data augmentation	47.9Scattering (ours) + 3FC	83.0	Scattering (ours) + 3FC	56.7Supervised hybrid architectures Scattering (ours) + CNN, K = 128	89.4	Supervised hybrid architectures Scattering (ours) +CNN, K = 128	64.4SCattering (OUrs) + CNN, K = 512	91.4	SCattering (OUrs) +CNN, K = 512	69.5Supervised architectures Highway network	92.4	Supervised architectures Highway network	67.8All-CNN	92.8	All-CNN	66.3Wide ResNet	96.2	Wide ResNet	81.7data augmentation. Besides, since our convnet is kept as simple as possible, we also compare ourarchitecture to the All-CNN (Springenberg et al., 2014) work. The latter performs slightly betteron CIFAR10, but our hybrid network has a better generalization on CIFAR100. This indicates thatsupervision is essential, but that a geometric initialization of the first layers of a deep network leadsto a discriminative representation as well. It is also interesting to observe that we could not find anyarchitecture that was performing worse on CIFAR10 than ours, but better on CIFAR100. Since thenumber of samples available per class is lower, this could indicate that learning is easier in this casewith a scattering initialization.
Table 3: Accuracy of a hybrid scattering in a limited sample situation on CIFAR10 dataset. N.A.
Table 4: Accuracy of a hybrid scattering on the STL-10 datasetArchitecture	AccuracySupervised architecturesScattering+CNN, K = 512	77.4± 0.4CNN	70.1~_Semi-Supervised andunsupervised architectureExemplar CNN	75.4±0.3Unsup. Discr. CNN	76.8±0.3of samples decreases. Nonetheless, a GAN performs better than a translation scattering with 3 fullyconnected layers; its performances is almost constant equal to 80%, which shows that this algorithmcan adapt itself to the bias of the dataset.
Table 5: Computation time (in seconds) for different input size, one being with MATLAB on CPUsand the other with Torch on GPUsInput sizeJ I ScatNetLight (in S) ScatWave (in S)32 × 32 × 3 × 12832 × 32 × 3 × 12832 × 32 × 3 × 128128 × 128 × 3 × 128128 × 128 × 3 × 128128 × 128 × 3 × 128256 × 256 × 3 × 1282452452.513381652120659141037001123Appendix A:	Implementation details of ScatwaveCascade of multi-resolution computations, such as perfomed in a scattering transform, are delicate.
