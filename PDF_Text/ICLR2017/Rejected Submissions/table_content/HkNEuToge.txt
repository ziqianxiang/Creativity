Table 1: Underlying block architecture common across all models we evaluated. SSC networksadd an extra normalization layer after the non-linearity. And EB-SSC networks insert class-specificbias layers between the convolution layer and the non-linearity. Concatenated ReLU (CReLU) splitspositive and negative activations into two separate channels rather than discarding the negative com-ponent as in the standard ReLU.
Table 2: Comparison of the baseline ReLU+LC7 model, its derivative models, and our proposedmodel on CIFAR-10.
