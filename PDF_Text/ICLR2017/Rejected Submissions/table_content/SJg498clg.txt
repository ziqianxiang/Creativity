Table 1: Results for BlogCatalog dataset averaged over 10 random splits. The higher is better.
Table 2: Settings of CNNs for the text classification experimentSetting	I Baseline “small” CNN ∣ “Tiny” CNN ∣# of convolutional layers	6	3Frame size in conv. layers	256	32# of fully-connected layers	3	3Hidden units in fully-connected layers	1024	256The network is trained with the same parameters as Zhang et al. (2015) but only for 20 epochs. Wecompare the final outputs using the cross entropy loss, that is d = cross_entropy(g(xu), g(xv)).
Table 3: Results for News Categorization using CNNsNetwork	Accuracy %Baseline: “small” CNN	84.35Baseline: “small” CNN with thesaurus augmentation	85.20Baseline: “tiny” CNN	85.07“Tiny” CNN With NGM	86:904.3	Semantic Intent Classification using LSTM RNNsFinally, we compare the performance of our approach for training RNN sequence models (LSTM)fora semantic intent classification task as described in the recent work on SmartReply (Kannan et al.,2016) for automatically generating short email responses. One of the underlying tasks in SmartReplyis to discover and map short response messages to semantic intent clusters.1 We choose 20 intentclasses and created a dataset comprised of 5,483 samples (3,832 for training, 560 for validation and1,091 for testing). Each sample instance corresponds to a short response message text paired witha semantic intent category that was manually verified by human annotators. For example, “That1For details regarding SmartReply and how the semantic intent clusters are generated, refer Kannan et al.
Table 4: Results for Semantic Intent Classification using LSTM RNNsModel	Mean Reciprocal Rank (MRR)Random	0.175Frequency	0.258LSTM	0.276NGM-LSTM	02845	ConclusionsWe have proposed a training objective for neural network architectures that can leverage both labeledand unlabeled data. Inspired by the label propagation objective function, the proposed objective bi-ases the neural networks to learn similar hidden representations for nodes connected by an edgeon the graph. Importantly, this objective can be trained by stochastic gradient descent, as in super-vised neural network training. We validate the efficacy of the graph-augmented objective on variousstate-of-the-art neural network architectures on bloggers’ interest, text category and semantic in-tent classification problems. Additionally, the node-level input features can be combine with graphfeatures as inputs to the neural network. We showed that a neural network that simply takes theadjacency matrix of a graph and produces node labels, can perform better than a recently proposedtwo-stage approach using sophisticated graph embeddings and a linear classifier.
