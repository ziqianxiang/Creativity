Table 1: Comparative test perplexities on various document datasets (50 latent variables). Note thatdocument probabilities were calculated using 10 samples to estimate the variational lower bound.
Table 2: Word query similarity test, where each (20 News-Group) document model’s decoder isgiven a query and must return the top 10 most relevant words. The first query was “government”while the second was “space”. It appears that the models with piecewise variables tend to associatemore general/abstract terms to the query, which may or may not always be what is desired.
Table 3: Approximate posterior word encoding on Twitter. The numbers are computed by countingthe number of times each word is among the 5 words with the largest sum of squared gradients ofthe Gaussian KL divergence (G-KL) and piecewise constant KL divergence (P-KL)Similar to Serban et al. (2016b), we use a bidirectional GRU RNN encoder, where the forward andbackward RNNs each have 1000 hidden units. We experiment with context RNN encoders with500 and 1000 hidden units, and find that that 1000 hidden units reach better performance w.r.t. thevariational lower-bound on the validation set. The encoder and context RNNs use layer normaliza-tion (Ba et al., 2016). We experiment with decoder RNNs with 1000, 2000 and 4000 hidden units(LSTM cells), and find that 2000 hidden units reach better performance. For the G-VHRED model,we experiment with latent multivariate Gaussian variables with 100 and 300 dimensions, and findthat 100 dimensions reach better performance. For the H-VHRED model, we experiment with latentmultivariate Gaussian and piecewise constant variables each with 100 and 300 dimensions, and findthat 100 dimensions reach better performance. We follow the training procedure of Serban et al.
Table 4: Approximate posterior word encodings on 20 News-Groups. For P-KL, we also bold everycase where the piecewise variables showed greater sensitivity to the word than the Gaussian variableswithin the same hybrid model.
