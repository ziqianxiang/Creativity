Table 1: SqueezeNet architectural dimensions. (The formatting of this table was inspired by theInception2 paper (Ioffe & Szegedy, 2015).)layer name/type	output size	filter size / stride (if not a fire layer)	depth	Slxl (#1x1 squeeze)	e1x1 (#1x1 expand)	e3x3 (#3x3 expand)	Slxl sparsity	e1x1 sparsity	e3x3 sparsity	#bits	#Parameter before pruning	#Parameter after pruninginput image	224x224x3										-	-conv1	111x111x96	7x7/2 (x96)	1				100% (7x7)			6bit	14,208	14,208maxpool1	55x55x96	3x3/2	0									fire2	55x55x128		2	16	64	64	100%	100%	33%	6bit	11,920	5,746fire3	55x55x128		2	16	64	64	100%	100%	33%	6bit	12,432	6,258fire4	55x55x256		2	32	128	128	100%	100%	33%	6bit	45,344	20,646maxpool4	27x27x256	3x3/2	0									fire5	27x27x256		2	32	128	128	100%	100%	33%	6bit	49,440	24,742fire6	27x27x384		2	48	192	192	100%	50%	33%	6bit	104,880	44,700fire7	27x27x384		2	48	192	192	50%	100%	33%	6bit	111,024	46,236fire8	27x27x512		2	64	256	256	100%	50%	33%	6bit	188,992	77,581maxpool8	13x12x512	3x3/2	0									fire9	13x13x512		2	64	256	256	50%	100%	30%	6bit	197,184	77,581conv10	13x13x1000	1x1/1 (x1000)	1				20% (3x3)			6bit	513,000	103,400avgpool10	1x1x1000	13x13/1	0									I	,	I I	,	I I	.	， activations	parameters	∞mpression info											1,248,424 (total)	421,098 (total)In Table 2, we review SqueezeNet in the context of recent model compression results. The SVD-
Table 2: Comparing SqueezeNet to model compression approaches. By model size, we mean thenumber of bytes required to store all of the parameters in the trained model.
Table 3: SqUeezeNet accuracy and model size using different macroarchitecture configurationsArchitecture	Top-1 Accuracy	Top-5 Accuracy	Model SizeVanilla SqueezeNet	575%	803%	-4.8MB-SqueezeNet + Simple Bypass	604%	825%	-4.8MB-SqueezeNet + Complex Bypass	58.8%	82.0%	7.7MB -simple bypass connections yielded an increase of 2.9 percentage-points in top-1 accuracy and 2.2percentage-points in top-5 accuracy without increasing model size.
