Table 1: Evaluation Result on MUlti30k dataset (English-German). The scores in parentheses arecomputed with ‘-norm’ parameter. NMT is dl4mt’s NMT (in the session3 directory). The score ofthe CMU is from (Huang et al., 2016).
Table 2: Hyperparameters. The name is the variable name of dl4mt except for dimv and dim_pic,which are the dimension of the latent variables and image embeddings. We set dim (number ofLSTM unit size) and dim^word (dimensions of word embeddings) 256, batchsize 32, maxlen (maxoutput length) 50 and lr (learning rate) 1.0 for all models. decay-c is weights on L2 regularization.
Table 3: Hyperparameters using the experiments in the Figure 8 and 9	dim	dim_word	lr	decay-c	maxlen	batchsie1	~566~	256	1.0	0.0005	30	1282	256	256	1.0	0.001	50	3212Under review as a conference paper at ICLR 2017Figure 10	presents the English word length histogram of the Multi30k test dataset. Most sentencesin the Multi30k are less than 20 words. We assume that this is one of the reasons why Multi30k iseasy to overfit.
