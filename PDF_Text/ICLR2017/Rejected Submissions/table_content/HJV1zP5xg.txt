Table 1: Oracle accuracy and distinct n-grams on COCO and PASCAL-50S datasets for image captioning atB = 20. While We report SPICE, We observe similar trends in other metrics (reported in supplement).
Table 2: Quantitative results on English-German translation on the newstest-2013 and newstest-2014 datasetscombined (at B = 20).
Table 3: Qualitative examples of dialog generation comparing best-first search, BS and DBSPrompt	Beam Search	Diverse Beam Search	I don’t know.	I don’t know.
Table 4: CIDEr Oracle accuracy on COCO and PASCAL-50S datasets for image captioning at B = 20.
Table 5: METEOR Oracle accuracy on COCO and PASCAL-50S datasets for image captioning at B = 20.
Table 6: ROUGE Oracle accuracy on COCO and PASCAL-50S datasets for image captioning at B = 20.
Table 7: Frequency table for image difficulty and human preference for DBS captions on PASCAL50S datasetdifficulty score # images % images DBSbin range	was preffered≤ μ - σ	481	50.51%[μ-σ, μ+σ]	409	69.92%≥ μ + σ	110	83.63%As mentioned in Section 5, we observe that difficulty score of an image and human preference forDBS captions are positively correlated. The dataset contains more images that are less difficultyand so, we analyze the correlation by dividing the data into three bins. For each bin, we report the% of images for which DBS captions were preferred after a majority vote (i.e. at least 3/5 turkersvoted in favor ofDBS) in Table 7. At low difficulty scores consisting mostly of iconic images - onemight expect that BS would be preferred more often than chance. However, mismatch between thestatistics of the training and testing data results in a better performance of DBS. Some examples forthis case are provided in Fig. 7. More general qualitative examples are provided in Fig. 8.
