Table 1: Performance of our proposed models and other state-of-the-art methods on COCO, where B@N,M, R, C and S are short for BLEU@N, METEOR, ROUGE-L, CIDEr-D and SPICE scores. All values arereported as percentage (%).
Table 2: Leaderboard of the published state-of-the-art image captioning models on the online COCO testingserver (http://mscoco.org/dataset/#captions-leaderboard), where B@N, M, R, and C areshort for BLEU@N, METEOR, ROUGE-L, and CIDEr-D scores. All values are reported as percentage (%).
Table 3: The user study using two criteria: M1 - percentage of captions generated by different methods thatare evaluated as better or equal to human caption and M2 - percentage of captions that pass the Turing Test.
