Table 1: The performance of simple parameter transformations from DNN to SFNN on the MNISTand synthetic datasets, where each layer of neural networks contains 800 and 50 hidden units fortwo datasets, respectively. For all experiments, the only first hidden layer of DNN is replaced bystochastic one. We report negative log-likelihood (NLL) and classification error rates.
Table 2: Classification test error rates [%] on MNIST, where each layer of neural networks contains800 hidden units. All Simplified-SFNNs are constructed by replacing the first hidden layer of a base-line DNN with stochastic hidden layer. We also consider training DNN and fine-tuning Simplified-SFNN using batch normalization (BN) and dropout (DO). The performance improvements beyondbaseline DNN (due to fine-tuning DNN parameters under Simplified-SFNN) are calculated in thebracket.
Table 3: Test negative log-likelihood (NLL) on MNIST-half and TFD datasets, where each layer ofneural networks contains 200 hidden units. All Simplified-SFNNs are constructed by replacing thefirst hidden layer of a baseline DNN with stochastic hidden layer.
Table 4: Test error rates [%] on CIFAR-10, CIFAR-100 andSVHN. The error rates for WRN are from our experiments,where original ones reported in (Zagoruyko & Komodakis,2016) are in the brackets. Results with f are obtained usingthe horizontal flipping and random cropping augmentation.
