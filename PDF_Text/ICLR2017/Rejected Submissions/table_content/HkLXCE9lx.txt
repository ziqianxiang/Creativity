Table 1: MAB Results. Each grid cell records the total reward averaged over 1000 different instancesof the bandit problem. We consider k ∈ {5, 10, 50} bandits and n ∈ {10, 100, 500} episodes ofinteraction. We highlight the best-performing algorithms in each setup according to the computedmean, and we also highlight the other algorithms in that row whose performance is not significantlydifferent from the best one (determined by a one-sided t-test with p = 0.05).
Table 2: Random MDP ResultsSetup		Random	PSRL	OPSRL	UCRL2	BEB	-Greedy	Greedy	RL2n	10	100.1	138.1	144.1	146.6	150.2	132.8	134.8	156.2n	25	250.2	408.8	425.2	424.1	427.8	377.3	368.8	445.7n	50	499.7	904.4	930.7	918.9	917.8	823.3	769.3	936.1n	75	749.9	1417.1	1449.2	1427.6	1422.6	1293.9	1172.9	1428.8n	100	999.4	1939.5	1973.9	1942.1	1935.1	1778.2	1578.5	1913.7The distribution over MDPs is constructed with |S| = 10, |A| = 5. The rewards follow a Gaus-sian distribution with unit variance, and the mean parameters are sampled independently fromNormal(1, 1). The transitions are sampled from a flat Dirichlet distribution. This constructionmatches the commonly used prior in Bayesian RL methods. We set the horizon for each episode tobe T = 10, and an episode always starts on the first state.
Table 3: Results for visual navigation. These metrics are computed using the best run among allruns shown in Figure 5. In 3c, we measure the proportion of mazes where the trajectory length inthe second episode does not exceed the trajectory length in the first episode.
Table 1: Hyperparameters for TRPO: multi-armed banditsDiscountGAE λPolicy Iters#GRU UnitsMean KLBatch size0.990.3Up to 10002560.01250000A.2 TABULAR MDPSThe parameters for TRPO are shown in Table 2. We use a one-hot embedding for the states andactions separately, which are then concatenated together.
Table 2: Hyperparameters for TRPO: tabular MDPsDiscount	0.99GAE λ	0.3Policy Iters	UP to 10000#GRU Units	256Mean KL	0.01Batch size	250000A.3 Visual NavigationThe parameters for TRPO are shown in Table 3. For this task, we use a neural network to formthe joint embedding. We rescale the images to have width 40 and height 30 with RGB channelspreserved, and we recenter the RGB values to lie within range [-1, 1]. Then, this preprocessed14Under review as a conference paper at ICLR 2017image is passed through 2 convolution layers, each with 16 filters of size 5 × 5 and stride 2. Theaction is first embedded into a 256-dimensional vector where the embedding is learned, and thenconcatenated with the flattened output of the final convolution layer. The joint vector is then fed toa fully connected layer with 256 hidden units.
Table 3: Hyperparameters for TRPO: visual navigationDiscount	0.99GAE λ	0.99Policy Iters	UP to 5000#GRU Units	256Mean KL	0.01Batch size	50000B Hyperparameters for baseline algorithmsB.1	Multi-armed banditsThere are 3 algorithms with hyperparameters: UCB1, Optimistic Thompson Sampling (OTS), and-Greedy. We perform a coarse grid search to find the best hyperparameter for each of them. Morespecifically:• UCB1: We test c ∈ {0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. The best found pa-rameter for each setting is given in Table 4.
Table 4: Best hyperparameter for UCB1Setting	Best Cn = 10, k = 5	ɪin= 10,k= 10	0.1n = 10, k = 50	0.1n = 100, k = 5	0.2n = 100,k= 10	0.2n = 100,k= 50	0.2n = 500, k = 5	0.2n = 500,k= 10	0.2n = 500, k = 50	0.2•	Optimistic Thompson Sampling (OTS): The hyperparameter is the number of posteriorsamples. We use up to 20 samples. The best found parameter for each setting is given inTable 5.
Table 5: Best hyperparameter for OTSSetting	Best #SamPlesn = 10, k = 5	15n = 10, k = 10	14n = 10, k = 50	19n = 100, k = 5	8n = 100, k = 10	20n= 100,k=50	16n = 500, k = 5	7n = 500, k = 10	20n = 500, k = 50	20	•	-Greedy: The hyperparameter is the parameter. We test ∈{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.	The best found parameter for eachsetting is given in Table 6.
Table 6: Best hyperparameter for -GreedySetting			Best En=	10, k =	5	^00n=	10, k =	10	0.0n=	10, k =	50	0.0n=	100, k	=5	0.0n=	100, k	= 10	0.0n=	100, k	= 50	0.1n=	500, k	=5	0.1n=	500, k	= 10	0.1n=	500, k	二50	0.1B.2	TABULAR MDPSThere are 4 algorithms with hyperparameters: Optimistic PSRL (OPSRL), BEB, -Greedy, UCRL2.
Table 7: Best hyperparameter for OPSRLSetting	Best #SamPlesn = 10	"14n =25	14n =50	14n = 75	14n = 100	17	•	BEB: We search for the scaling factor in front of the exploration bonus, in the log-linearspan of [log(0.0001), log(1.0)] with 21 way points. The actual searched parameters are0.0001, 0.000158, 0.000251, 0.000398, 0.000631, 0.001, 0.001585, 0.002512, 0.003981,0.00631, 0.01, 0.015849, 0.025119, 0.039811, 0.063096, 0.1, 0.158489, 0.251189,0.398107, 0.630957, 1.0. The best found parameter for each setting is given in Table 8.
Table 8: Best hyperparameter for BEBSetting		Best scalingn	10	0.002512n	25	0.001585n	50	0.001585n	75	0.001585n	100	0.001585•	-Greedy: We test ∈ {0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. The best foundparameter for each setting is given in Table 9.
Table 9: Best hyperparameter for -GreedySettingBestn = 10n=25n=50n = 75n= 1000.10.10.10.10.1• UCRL2: We search for the scaling factor of exploration bonus among the same values asBEB. The best found parameter for each setting is given in Table 10.
Table 10: Best hyperparameter for UCRL2Setting		Best scalingn	10	0.398107n	25	0.398107n	50	0.398107n	75	0.398107n	100	0.398107C Further analysis on multi-armed banditsIn this section, we provide further analysis of the behavior of RL2 agent in comparison with thebaseline algorithms, on the multi-armed bandit task. Certain algorithms such as UCB1 are designednot in the Bayesian context; instead they are tailored to be robust in adversarial cases. To highlightthis aspect, we evaluate the algorithms on a different metric, namely the percentage of trials wherethe best arm is recovered. We treat the best arm chosen by the policy to be the arm that has beenpulled most often, and the ground truth best arm is the arm with the highest mean parameter. Inaddition, we split the set of all possible bandit tasks into simpler and harder tasks, where the difficultyis measured by the -gap between the mean parameter of the best arm and the second best arm. Wecompare the percentage of recovering the best arm separately according to the gap, as shown inTable 11.
Table 11: Percentage of tasks where the best arm is chosen most frequently, with k = 5 arms andn = 500 episodes of interaction.
