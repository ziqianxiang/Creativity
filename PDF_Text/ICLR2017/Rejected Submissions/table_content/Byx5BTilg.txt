Table 1: Analyzing the differences in accuracy for the different architecture parent-child pairs foreach dataset. __________________________________________________________________Dataset	Max difference	Average differenceContraceptive	^3%	T.8%Seismic bumps	4.9%	1.1%Page Blocks	7.4%	1.4%Wind	35%	3.2%Puma_32	19.2%	1.8%CPUaCt	40%	3.3%Delta elevators	39.5%	2.7%Mammography	3%	1.1%Ailerons	17.4%	5.7%Bank marketing	3.5%	0.8%German Credit	5%	1%Space	11.5%	2.5%Cardiography	11.5%		1%	Table 2: Analyzing the differences in accuracy for the different architecture parent-child pairs foreach dataset. __________________________________________________________________Component type	Number of appearancesDropout	2≥
Table 2: Analyzing the differences in accuracy for the different architecture parent-child pairs foreach dataset. __________________________________________________________________Component type	Number of appearancesDropout	2≥Sigmoid	3TanH	2Fully connected	2ReLU	1Batchnorm	3	Comparing the performance of DNN architectures to those of “conventional classifiers”. Asa point of reference to “classical” machine learning approaches for classifying tabular data, in Table3 we also presents the performance of the Random Forest algorithm (using the Weka Hall et al.
Table 3: Comparison of the accuracy performance of the best average-ranking architectures to thetop-ranking architecture found by our approach for each dataset.
Table 4: Comparison of the performance of parallel architectures to their serial counterparts.
Table 5: The evaluation results of different approaches using the precision@X metric. f ull, trainand d + t denote MLfull (all meta-features), M Ltrain (training-based meta-features only) andM Ldata+top (dataset-based and topological meta-features) respectively. Best results are in bold.
Table 6: The probabilities of finding an architecture that outperforms all those in the ranked list whenrandomly sampling a set of architectures. The size of the ranked list by our algorithm is always 10(i.e. for sample size 20 We test a set two times the size of the ranked list.)Dataset	Sample size -10	Sample size - 20Contraceptive	1.7%	^2%Seismic bumps	11.5	22%Page Blocks	14.8%	27.7%Wind	24.3%	41.5%Puma_32	20.7%	36.5%CPUaCt	3.4%	6.7%Delta elevators	33.3%	55.5%Mammography	7.5%	14.3%Ailerons	13.9%	25.5%Bank marketing	5.6%	10.4%German Credit	11.9%	22.9%Space	20.2%	36.3%Cardiography	5.6%		11.2%	scheme along the lines presented in Li et al. (2016) to enable us to efficiently explore larger archi-tecture spaces.
Table 7: Description of the dataset-based meta-features used by our approach.
Table 8: Description of the topological meta-features used by our approach.
Table 9: Description of the training-based meta-features used by our approach.
Table 10: The characteristics of the datasets used in the experimentsName	Num of Data Points	% of Class	Minority	Num of Features	% of Numeric Fea- turesGerman Credit	1,000	"30%		^^0	"30%Contraceptive	1,473	22.6%		9	66.6%Cardiography	2,126	22.1%		22	100%Seismic bumps	2,584	6.5%		18	77%Space	3,107	49.5%		6	100%Page Blocks	5,473	9.3%		10	100%Wind	6,574	46.7%		14	100%Puma_32	8,192	49.6%		32	100%CPUaCt	8,192	30.2%		21	100%Delta elevators	9,517	49.7%		6	100%MammOgraPhy	11,183	2.3%		6	100%Ailerons	13,750	42.3%		40	100%Bank marketing	45211	11.6%		16		43.75%	C Analysis of the performance parallel layersFor each dataset, we analyze the 100 top-performing architectures and determine the percentage ofarchitectures with parallel layers. The results, presented in Table 11, show that the percentage issignificant. In table 12 we analyze the component composition of these architectures. The mostinteresting point (in our view) is that the number of fully-connected layers is about half of the
Table 11: The percentage of architectures with parallel layers in the 100 top-performing architecturesfor each dataset. _______________________________________________________Dataset	% of architectures with parallel layersContraceptive	^61%Seismic bumps	60%Page Blocks	65%Wind	61%Puma_32	59%CPUaCt	64%Delta elevators	73%Mammography	61%Ailerons	61%Bank marketing	62%German Credit	59%Space	49.6%Cardiography	64%Average	61%	-16Under review as a conference paper at ICLR 2017Table 12: The average number of component types per architecture for 100 top-performing archi-
Table 12: The average number of component types per architecture for 100 top-performing archi-tectures of each dataset.
