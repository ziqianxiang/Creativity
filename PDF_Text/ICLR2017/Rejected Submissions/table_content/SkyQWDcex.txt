Table 1: Two examples of QA problems. Left is an ideal QA example, where the question is veryclear. Right is an example with incomplete information, where the question is ambiguous and it isdifficult to provide an answer only using the input statements.
Table 2: An example of interactive mechanism. “SQ” denotes supplementary question.
Table 3: Examples of three different tasks on the generated ibAbI datasets. “Q” indicates the targetquestion. “SQ” is the supplementary question. “FB” refers to user’s feedback. “A” is the answer.
Table 4: Performance comparison of various models in terms of test error rate (%) in QA dataset.
Table 5: Examples of bAbI task 17 (left) and 18 (right), where our model predicts correct answerswhile MemN2N makes wrong predictions.
Table 6: Examples of our model’s results on QA tasks. Supporting facts are shown in the datasetswhich our model does not use during training. “Weight” indicates the attention weight for sentence.
Table 7: Performance comparison of various models in terms of test error rate (%) based on interac-tive question answering datasets with different IQA ratios.
Table 8: Examples of sentence attention weights obtained by our model in both QA and IQA data.
Table 9: Performance comparison of the generated supplementary question quality with RIQA as0.8. Both two methods achieve 100% under all metrics in all tasks with other different RIQA values.
