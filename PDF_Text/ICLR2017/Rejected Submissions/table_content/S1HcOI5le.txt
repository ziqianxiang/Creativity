Table 1: The effect of different hyperparameters α and β5.2	FinetuningIn this section, we prove that OMG forces deep neural network learn more discriminative features,and the OMG works smoothly with pre-train neural networks. We finetune existing pre-trainednetworks on ImageNet with the help of OMG. The original network is fully trained on ImageNetdataset, so directly finetune on the same dataset should not change any thing significantly. Evenfinetune on different datasets, empirically the parameters in the very first layers are not going tochange. However, we observe the significant changing after finetuning on the same dataset for only5 epoches.
Table 2: Comparison of accuracy with other models on k-shot learning tasks. The proposed OMGachieves better performance on both one-shot learning and 5-shot learning case.
Table 3: One shot learning result on Office dataset, the numbers of baseline are borrow from Judyet al. (2013b).
