Table 1: Visualization of the loss surface near and between local minima found by different opti-mization methods. Each box corresponds to a pair of optimization methods. In the lower triangle,we plot the projection of the loss surface at weight vectors between the initial weight and the learnedweights found by the two optimization methods. Color as well as height of the surface indicate theloss function value. In the upper triangle, we plot the functional difference between the networkcorresponding to the learned weights for the first algorithm and networks corresponding to weightslinearly interpolated between the first and second algorithm’s learned weights. (Best viewed inzoom)5Under review as a conference paper at ICLR 201799.095.598.598.0> 97,5< 97.096.596.0SGD(a) Train
Table 2: Visualization of the Loss Surface with and without batch-normalization.
Table 3: The coefficients of vari-ous second order Runge-Kutta methodsHairer et al. (1987)Method Name	aι	a2	qιMidpoint	~δ~	~~Γ	~r~ 2Heun	1 2	-r~ 2	1Ralston	1 3	2 3	-3- 4A.3.1 Augmenting Optimization with RungeKuttaFor a given timestep, explicit integrators can be seen as a morphism over vector fields X → Xh.
