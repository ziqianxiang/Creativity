Table 1: Results of pruning Lenet-300-100 on MNIST. The accuracy of all the compressed networksare the same as the original network.
Table 2: Pruning Lenet-5 on MNIST. The accuracy of all the compressed networks are the same asthe original network.
Table 4: Base model architecture for SVHNwith 1236250 parameters.
Table 3: Pruning the reference network in Table 4,	Layer	Filters	Kernel	Weightson SVHN dataset.	conv1	32	3 X 3	-896-^	conv2	32	3 × 3	9246	conv3	32	3 × 3	9246Dense	d Method	Layer Parameters Removed	pool1	-	2 × 2	-Parameters NeUronS		conv4	48	3 × 3	13872Ground Truth 1024	1236250	-	conv5	48	3 × 3	20784	conv6	48	3 × 3	20784				No_Noise	132	313030	74.67%	pool2	-	2 × 2	-GauSSian	4	180550	85.39%	conv7	64	3 × 3	27712ConStant	25	202285	83.63%	conv8	64	3 × 3	36928Bionomial	17	194005	84.30%	conv9	64	3 × 3	36928	pool3	-	2 × 2	-	denSe	1024	-	1049600	Softmax	10	-	10250Pruning results of LeNet-300-100 on MNIST1.005Pruning results of LeNet-5 on MNlST>UE3UU⅞
