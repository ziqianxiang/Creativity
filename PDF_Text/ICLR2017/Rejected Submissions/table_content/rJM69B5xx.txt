Table 1: The mean performance across 11 bAbI tasks. The first two columns show a random baseline2and a baseline that selects the most frequent word from the context which also appears as an answerin the training data for the task. The following three columns show performance of the AS Readertrained on different datasets, the last column shows the results of DMN+ (Xiong et al., 2016), thestate-of-the-art-model on the bAbI 10k dataset. For more detailed results listing per task accuraciessee Appendix C.
Table 2: The effect of pre-training different components of the model for selected tasks. The first rowshows performance (average test accuracy across all trained model instances in each category) of arandomly initialized baseline model. The following three rows show increase in accuracy (measuredin percent absolute) when the model is initialized with weights pre-trained on the BookTest. The lastline shows results for models initialized with Google News word2vec word embeddings (Mikolovet al., 2013).
Table 3: Hyperparameters for both the randomly initialized and the pre-trained models.
Table 4: Performance of the AS Reader when trained on the bAbl 10k, BookTest and CNN/DM datasets and then evaluated on bAbl test data. The DynamicMemory Network (DMN+) is the state-of-the-art model in a weakly supervised setting on the bAbl IOk dataset. Its results are taken from (Xiong et al., 2016).
Table 5: Mean test accuracy for each combination of task, model type and target-adjustment set size.
Table 6: Standard deviation in accuracies for each combination of task, model type and target-adjustment set size.
Table 7: One-sided p-value whether the mean accuracy of pre-trained models is greater than the accuracy of the randomly initialized ones for each combination oftask pre-training dataset, p-values below 0.05 are marked in green.
