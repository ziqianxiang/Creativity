Table 1: ENet architecture. Output sizes are givenfor an example input of 512 × 512.
Table 2: Performance comparison. Image size is W×HModel	NVIDIA TX1	NVIDIA Titan X 480 × 320	640×360	1280 × 720	640 × 360 1280×720 1920×1080 ms fps ms fps ms fps	ms fps ms fps ms fpsSegNet ENet	757	1.3	1251	0.8	-	-	69	14.6	289	3.5	637	1.6 47	21.1	69	14.6	262	3.8	7	135.4	21	46.8	46	21.6Inference time: Table 2 compares inference time for a single input frame of varying resolution.
Table 3: Hardware requirements. FLOPs are estimated for an input of 3 × 640 × 360 (C,W,H).
Table 4: Cityscapes test set resultsModel	Class IoU	Class iIoU	Category IoU	Category iIoUSegNet	56.1	34.2	79.8	66.4ENet	58.3	34.4	80.4	64.0Cityscapes: This dataset consists of 5000 fine-annotated images, out of which 2975 are availablefor training, 500 for validation, and the remaining 1525 have been selected as test set (Cordts et al.
Table 5: Results on CamVid test set of (1) SegNet-Basic, (2) SegNet, and (3) ENet∩0I S3。.gva ssaltsilcyciBklawediSeloPecneFnairtsedePdaoRngiSraCykSeerTgnidliuBledoM1	75.0	84.6	91.2	82.7	36.9	93.3	55.0	47.5	44.8	74.1	16.0	62.9	n/a2	88.8	87.3	92.4	82.1	20.5	97.2	57.1	49.3	27.5	84.4	30.7	65.2	55.63	74.7	77.8	95.1	82.4	51.0	95.1	67.2	51.7	35.4	86.7	34.1	68.3	51.3CamVid: Another automotive dataset, on which we have tested ENet, was CamVid. It contains367 training and 233 testing images (Brostow et al. (2008)). There are eleven different classes such
Table 6: SUN RGB-D test set resultsModel	Global avg.	Class avg.	Mean IoUSegNet	70.3	35.6	26.3ENet	59.5	32.6	19.77Under review as a conference paper at ICLR 2017SUN RGB-D: The SUN dataset consists of 5285 training images and 5050 testing images with37 indoor object classes. We did not make any use of depth information in this work and trainedthe network only on RGB data. In Table 6 we compare the performance of ENet with SegNet(Badrinarayanan et al. (2015b)), which is the only neural network model that reports accuracy onthis dataset. Our results, though inferior in global average accuracy and IoU, are comparable in classaverage accuracy. Since global average accuracy and IoU are metrics that favor correct classificationof classes occupying large image patches, researchers generally emphasize the importance of othermetrics in case of semantic segmentation. One notable example is introduction of iIoU metric (Cordtset al. (2016)). Comparable result in class average accuracy indicates, that our network is capable ofdifferentiating smaller objects nearly as well as SegNet. Moreover, the difference in accuracy shouldnot overshadow the huge performance gap between these two networks. ENet can process the imagesin real-time, and is nearly 20× faster than SegNet on embedded platforms.
