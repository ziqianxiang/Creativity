Table 1: Percentage of Forwarding Time on Non-tensor LayersNetwork	Intel x86	Arm	Titan XAlexNet	32.08%	25.08%	22.37%GoogLeNet	62.03%	37.81%	26.14%ResNet-50	55.66%	36.61%	47.87%ResNet-152	49.77%	N/A	44.49%Average	49.89%	33.17%	35.22%Figure 1: Time Decomposition for each layer. Non-tensor layers (e.g., dropout, ReLU, LRN, soft-max, pooling, etc) shown in red color while tensor layers (e.g., convolution, inner-product) shownin black color.
Table 2: GoogLeNet Accuracy on each layer after mergingStep	Merged Layer(S) Top-5 Accuracy0	N/A	88.89%1	conv1	88.73%2	conv2	88.82%3	inception_3a	88.50%4	inception_3b	88.27%5	inception_4a	88.60%6	inception_4b-4d	88.61%7	inception_4e	88.43%8	inception_5a	88.41%9	inception_5b	88.43%Tucker Decomposition	N/A	86.54%is indicated as “GoogLeNet-Tucker”. Thus, we have 4 models to compare, namely GoogLeNet,GoogLeNet-Merge, GoogLeNet-Tucker and GoogLeNet-Merge-Tucker.
Table 3: Breakdown of GoogLeNet forwarding time cost using different methods on each layer.
Table 4: Execution time using different methods (including SqueezeNet) on different mobile devicesDevice	GoogLeNet	GoogLeNet -Tucker	GoogLeNet -Merge	GoogLeNet -Merge-Tucker	SqueezeNetMoto E	1168.8 ms	897.9 ms	406.7 ms	213.3 ms	291.4 msSamsung Galaxy S5	651.4 ms	614.9 ms	210.6 ms	106.3 ms	136.3 msSamsung Galaxy S6	424.7 ms	342.5 ms	107.7 ms	65.34 ms	75.34 msMacbook Pro (CPU)	91.77 ms	78.22 ms	23.69 ms	15.18 ms	17.63 msTitan X	10.17 ms	10.74 ms	6.57 ms	7.68 ms	3.29 msNot limited to mobile platform of Samsung Galaxy S5, we also apply the speed-up schemes onother popular processors. These mobile devices include (1) Moto E: a low-end mobile ARM CPU,(2) Samsung Galaxy S5: a middle-end mobile ARM CPU, (3) Samsung Galaxy S6: a high-endmobile ARM CPU, (4) Macbook Pro: an Intel x86 CPU, and (5) Titan X: a powerful server GPU.
Table 5: GoogLeNet Execution Storage vs. Engery vs. Runtime-Memory CostModel	Energy	Storage	Runtime Memory	Max Batch Size on Titan XGoogLeNet	984 mJ	26.72 MB	33.2 MB	350GoogLeNet-Tucker	902 mJ	14.38 MB	35.8 MB	323GoogLeNet-Merge	447 mJ (2.2x)	23.77 MB	13.2 MB	882 (2.52x)GoogLeNet-Merge-Tucker	226 mJ (4.4x)	11.99 MB	14.8 MB	785 (2.24x)SqueezeNet	288 mJ	4.72 MB	36.5 MB	321Table 6: AlexNet Result (Accuracy vs. Speed vs. Energy cost)Step	Merged Layer(s)	Top-5 Accuracy	Speed-up	Energy Cost0	N/A	80.03%	445 ms	688 mJ1	conv1+norm1 → conv1	79.99%	343 ms (1.29x)	555 mJ (1.24x)2	conv2+norm2 → conv2	79.57%	274 ms (1.63x)	458 mJ (1.51x)Another benefit of layer merging is run-time memory saving. The generated GoogLeNet-Mergemodel reduces the number of layers and consumes only 13.2 MB to process one image. This featureis also very useful for the cloud based deep learning service which can process a much larger batchat one run. As shown in table 5, one Titan X GPU can run a batch size of 882 with the GoogLeNet-Merge model while the original GoogLeNet can only allow a batch size of 350. On the other hand,SqueezeNet though has much less trained parameters, it has much larger run-time memory impactdue to the increased number of layers.
Table 6: AlexNet Result (Accuracy vs. Speed vs. Energy cost)Step	Merged Layer(s)	Top-5 Accuracy	Speed-up	Energy Cost0	N/A	80.03%	445 ms	688 mJ1	conv1+norm1 → conv1	79.99%	343 ms (1.29x)	555 mJ (1.24x)2	conv2+norm2 → conv2	79.57%	274 ms (1.63x)	458 mJ (1.51x)Another benefit of layer merging is run-time memory saving. The generated GoogLeNet-Mergemodel reduces the number of layers and consumes only 13.2 MB to process one image. This featureis also very useful for the cloud based deep learning service which can process a much larger batchat one run. As shown in table 5, one Titan X GPU can run a batch size of 882 with the GoogLeNet-Merge model while the original GoogLeNet can only allow a batch size of 350. On the other hand,SqueezeNet though has much less trained parameters, it has much larger run-time memory impactdue to the increased number of layers.
Table 7: ResNet (conv1-res2a) Result (Accuracy vs. Speed up).
