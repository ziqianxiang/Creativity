Table 1: Classification error of different learners against various adversary models based on MNISTModel	A(fgs0.1)	A(fgs0.5)	A(cg)	A(adam)	No adversaryLeNet-5	1.2%	46.1%	48.2%	48.9%	0.045%RAD(fgs0.1)	0.1%	0.5%	0.4%	3.0%	0.045%RAD(fgs0.5)	0.5%	0.1%	0	2.5%	0.045%RAD(cg)	0.1%	1.4%	0.4%	2.9%	0.045%RAD(adam)	0	0.1%	0.1%	0.1%	0.045%AEC(adam)	3.2%	20.6%	9.7%	2.6%	4.5%IAEC(fgs0.1)	1.3%	28.0%	18.3%	9.6%	1.1%IAEC(f gs0.5)	1.2%	1.4%	2.6%	5.5%	1.0%IAEC(cg)	1.6%	1.6%	1.5%	7.4%	1.2%IAEC(adam)	1.2%	5.2%	7.3%	2.3%	1.7%Distillation(T = 1)	0.6%	47.2%	29.4%	41.9%	0.2%Distillation(T = 100)	0.3%	42.3%	12.4%	28.5%	0.2%Similarly, we show the classification error comparison results of RAD across different adversarymodels in Table 2 compared with Distillation. As CIFAR-10 images are more complex, the errorrates for adversarial retraining framework get larger compared with that on MNIST. However, over-all the classification error for the retraining framework on different adversarial examples are below13% with zero knowledge of the adversary model, while the classification error on normal datais around 6%. Therefore, even on CIFAR-10 dataset, the adversarial retraining framework is still
Table 2: Comparisons for the error rate of RAD based on different adversary models on CIFAR-10Model	A(fgs0.1)	A(fgs0.5)	A(cg ) A(adam)		No adversaryLeNet-5	1.2%	46.1%	54.0%	52.7%	5.5%RAD(fgs0.1)	2.35%	2.0%	4.65%	3.0%	5.3%RAD(fgs0.5)	4.4%	2.7%	5.6%	2.6%	5.8%RAD(cg)	7.5%	2.45%	5.05%	2.2%	5.7%RAD(adam)	16.2%	2.8%	6.15%	2.4%	5.9%Distillation(T = 1)	21.3%	30.8%	13.8%	22.0%	11.0%Distillation(T = 100)	19.3%	25.2%	9.2%	20.2%	7.2%4.4	Robustness Against additional AttacksIn order to test the robustness of the learner against the repeated attacks, where the attacker canagain conduct attacks on the robust learners, here we evaluate how the robust learner behaves givenadditional attacks based on different adversary models. Table 3 presents the test error rate compar-ison when the attacker generates adversarial examples to attack the robust RAD learner, IAEC, andDistillation on MNIST. It is shown that the coordinate greedy (cg) and adam are somehow efficientto attack RAD, while the fast gradient sign methods fail to attack the robust RAD. So if the RAD isretrained with instances generated by arbitrary adversary models, it can be resilient against adversar-ial examples produced by the fast gradient sign method with various values. This means the RADcan confer robustness to single-step attack methods but not the iterative ones. However, adversariesbased on cg and adam can still find the vulnerabilities to attack the model. Compared with the per-
Table 3: Error rate of attacking the robust learners with additional attacks on MNISTModel	A(fgso.ι)	A(f gs0.5)	A(cg)	A(adam)RAD(fgs0.1)	0.3%	9.6%	48.1%	49.0%RAD(fgs0.5)	0.8%	0.1%	45.7%	49.0%RAD(cg)	0.8%	3.4%	44.6%	49.0%RAD(adam)	0.1%	0.1%	40.2%	48.7%IAEC(fgs0.1)	4.2%	10.3%	49.9%	49.5%IAEC(fgs0.5)	5.2%	3.8%	49.8%	49.9%IAEC(cg )	5.3%	3.9%	49.9%	49.4%IAEC(adam)	4.6%	7.0%	49.9%	49.9%Distillation(T = 100)	0.2%	0.2%	49.0%	48.7%Table 4: Error rate of attacking the robust learners with additional attacks on CIFAR-10Model	A(fgs0.1)	A(f gs0.5)	A(cg)	A(adam)RAD(fgs0.1)	3.7%	2.7%	42.0%	52.7%RAD(fgs0.5)	5.3%	2.8%	49.0%	52.4%RAD(cg)	7.9%	2.8%	52.0%	52.7%RAD(adam)	6.3%	3.1%	54.0%	52.7%Distillation(T = 100)	9.05%	8.6%	54.0%	54.1%T = 100) given diverse adversarial attacking strategies. What is worth to mention is that theserobust learners all perform accurately on the normal dataset without adversarial manipulation, which
Table 4: Error rate of attacking the robust learners with additional attacks on CIFAR-10Model	A(fgs0.1)	A(f gs0.5)	A(cg)	A(adam)RAD(fgs0.1)	3.7%	2.7%	42.0%	52.7%RAD(fgs0.5)	5.3%	2.8%	49.0%	52.4%RAD(cg)	7.9%	2.8%	52.0%	52.7%RAD(adam)	6.3%	3.1%	54.0%	52.7%Distillation(T = 100)	9.05%	8.6%	54.0%	54.1%T = 100) given diverse adversarial attacking strategies. What is worth to mention is that theserobust learners all perform accurately on the normal dataset without adversarial manipulation, whichoffers more potentials for the robust learners.
Table 5: Adversarial distortion required for attacking different models on MNISTModel	A(cg)	A(adam)LeNet-5	0.0118	0.0060RAD (.)	0.0118	0.0060IAECfgs0.ι)	0.0042	0.0031IAEC(fgs0.5)	0.0058	0.0028IAEC(cg )	0.0069	0.0023IAEC(adam)	0.0064	0.0029Distillation(T = 100)	0.0106	0.0060(a)	(b)	(c)	(d)	(e)Figure 3: Visualization of adversarial examples generated by different attacker models based onMNIST. (a) Original image, (b) attacked by fgs0.1, (c) attacked by f gs0.5, (d) attacked by cg, (e)attacked by adam.
Table 6: Adversarial distortion required for attacking different models on CIFAR-10Model	A(cg)	A(adam)LeNet-5	0.0025	0.0015RAD (.)	0.0025	0.0015Distillation(T = 100)	0.0025	0.0015The visual attacking results by injecting malicious noise are shown in Figure 4. It is clear that fgswith = 0.5 can distort the original images the most compared with other adversary algorithms.
