Figure 1: The hard attention network architecture. A round tip expresses concatenation of theinputs it receives. The attention is promoted to the next input element once a step action ispredicted.
Figure 2: Learning curves for the soft andhard attention models on the first fold of theCELEX datasetFigure 3: A comparison of the alignmentsas predicted by the soft attention (left) andthe hard attention (right) models on examplesfrom the CELEX dataset.
Figure 3: A comparison of the alignmentsas predicted by the soft attention (left) andthe hard attention (right) models on examplesfrom the CELEX dataset.
Figure 4: SVD dimension reduction to2D of 500 character representations incontext from the encoder, for both thesoft attention (top) and the hard atten-tion (bottom) models. Colors indicatewhich character is encoded.
Figure 5: SVD dimension reduction to2D of 500 character representations incontext from the encoder, for both thesoft attention (top) and hard attention(bottom) models. Colors indicate the lo-cation of the character.
