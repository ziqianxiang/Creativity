Figure 1: (left) Full Hessian matrix for a 784-2-10 system after convergence. (right) Eigenvalueprofile for increasingly bigger networks. For k hidden networks there are (784 + 1) * k + (k + 1) *k + (k + 1) * 10 eigenvalues.
Figure 2: Comparing random input (last two) with the MNIST data (first). Initial eigenvalue profilesare very different, as well as the final profile when compared to figure 1.
Figure 3: The input data for the simple case.
Figure 4: Increasing the network size: Systems with 18, 74, 162, 282, and 434 parameters, respec-tively. And a network with MSE loss.
Figure 5: Spectrum for the loss With the mean square loss.
Figure 6: Hessian heatmaps for 18, 74 and 162 paramters systems after training. The plots are 90degrees rotates counter-clock wise.
Figure 8: Top eigenvalue fluctuations over 5000 runs of the same system with same data and algo-rithm but different initial points.
Figure 9: Response of the top eigenvalues to the increasingly less-separable data. The numbers ontop of the figures indicate the standard deviation of the Gaussian blobs. Their means are kept thesame at (1, 1) and (-1, -1), respectively.
Figure 10: z-axis is the distance between points. The left most and right most curves in each plotare actual training profiles, and the lines in between are interpolations only. (left figure) same initialpoint (right figure) random (hence orthogonal) initial points.
