Figure 1: Case 1: an empirical study of the XOR classification task. The left figure: the networkstructure we use: h1 is a linear transformation, h2 is a non-linear transform of h1 and y is theprediction; The right column: The linear transformation maps x to h1. As shown in the red arrow,an L2 norm penalty on h2 centers the feature of h1 and make the points from different sets separable.
Figure 2: Case 2: a comprehensive study of a two-layer linear neural network for regression task.
Figure 3: Evaluation on the XOR classification task. The red, blue and green lines stand for theaccuracy of the Neural Network without any regularization, only the L2 feature penalty, and ourmodel with both weight and feature regularizer, respectively.
Figure 4: Classification accuracy comparison of our feature penalty (termed “PN”) with batch-normalization (termed “BN”) Ioffe & Szegedy (2015) on MNIST, CIFAR-10, Omniglot and Im-ageNet benchmarks. We compare baseline methods with neither batch-normalization nor featurepenalty, with each module added, and with modules included.
