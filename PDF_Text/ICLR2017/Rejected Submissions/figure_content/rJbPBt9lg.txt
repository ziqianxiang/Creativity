Figure 1: Code Completion Example in In-telliJ IDEAFigure 2: Correct prediction of theprogram in Figure 1programs; and (3) in the scenario that the code completion engine suggests a list of candidates, ourRNN-based models allow users to choose from a list of 5 candidates rather than inputting manuallyfor over 96% of all time when this is possible.
Figure 2: Correct prediction of theprogram in Figure 1programs; and (3) in the scenario that the code completion engine suggests a list of candidates, ourRNN-based models allow users to choose from a list of 5 candidates rather than inputting manuallyfor over 96% of all time when this is possible.
Figure 3:	AST example (part)EXPreSSionStatement	Partial ASTAssignmentStatementMembe rStatementIdentifier__WebPaCk_require__Property	CIdentifier	installedModUleSExpressionStatementAssignmentStatementMemberStatementIdentifierRight-most node-------------------	♦ webpack requireNeXt node following	Zar Property -------1 P ∣the partial ASTFigure 4:	Partial AST exampleInput: a partial AST. Given a complete AST T , we define a partial AST to be a subtree T0of T , such that for each node n in T0, its left set LT (n) with respect to T is a subset of T0, i.e.,LT (n) ⊆ T0. Here, the left set LT (n) of a node n with respect to T is defined as the set of all nodesin the in-order sequence during the depth-first search of T that are visited earlier than n .
Figure 4:	Partial AST exampleInput: a partial AST. Given a complete AST T , we define a partial AST to be a subtree T0of T , such that for each node n in T0, its left set LT (n) with respect to T is a subset of T0, i.e.,LT (n) ⊆ T0. Here, the left set LT (n) of a node n with respect to T is defined as the set of all nodesin the in-order sequence during the depth-first search of T that are visited earlier than n .
Figure 5: Architecture (NT2N) for predicting the next non-terminal.
Figure 6: Architecture (NTN2T) for predicting the next terminal.
Figure 7: Training epoch illustrationCategories	Previous work	Our considered models			RayCheV et al.(2016a)	N2N	NT2N	NT2NTOne model accuracy	83.9%	79.4 ± 0.2%	84.8 ± 0.1%	84.0 ± 0.1%Ensemble accuracy		82.3%	87.7% 一	86.2% 一Table 2: Next non-terminal prediction resultsbucket, we stop adding whole programs, and start adding only the first segment of each program:when a bucket is empty, a program is chosen randomly, and its first segment is added to the bucket.
Figure 8: Percentage of UNK tokens in theentire test data and the sampled subset of thetest data by varying the UNK threshold from10000 to 80000.
Figure 9: Accuracies of different modelstrained over the sampled subset of train-ing data by varying the UNK threshold from10000 to 80000.
Figure 10: Overall accuracies and accuracies on non-UNK terminals by varying α.
