Figure 1: (A) Graphical illustration of the recurrent structure of MAP estimation (compare equation(6)). The three boxes represent likelihood model p(y|x) (A omitted), prior pθ(x), and updatefunction Γ, respectively. In each iteration, likelihood and prior collect the current estimate of x,to send a gradient to update function Γ (see text). Γ then produces a new estimate of x. Typically,prior pθ (x) and update function Γ are modeled as two distinct model components. Here they areboth depicted in gray boxes because they each represent model internal information which we wishto be transferable between different observations, i.e. they are observation independent. Likelihoodterm p(y|x) is depicted in blue to emphasize it as a model extrinsic term, some aspects of thelikelihood term can change from one observation to the other (such as matrix A). The likelihoodterm is observation-dependent. (B) Model simplification. The central insight of this work is to mergeprior pθ (x) and update function Γ into one model with trainable parameters φ. The model theniteratively produces new estimates through feedback from likelihood model p(y|x) and previousupdates. (C) A Recurrent Inference Machine unrolled in time. Here we have added an additionalstate variable which represents information that is carried over time, but is not directly subjectedto constraints through the likelihood term p(y|x). During training, estimates at each time step aresubject to an error signal from the ground truth signal x (dashed two-sided arrows) in order toperform backpropagation. The intermittent error signal will force the model to perform well as soonas possible during iterations. At test time, there is no error signal from x.
Figure 2: Reconstruction performance over time on random projections. Shown are results of thethree reconstruction tasks from random projections (see text) on 5000 random patches from theBSD-300 test set. Value of p represent the the reduction in dimensionality through the random pro-jection. Noise standard deviation was chosen to be σ = 1. Solid lines correspond to the mean peaksignal-to-noise-ration (PSNR) over time, and shaded areas correspond to one standard deviationaround the mean. Vertical dashed lines mark the last time step that was used during training.
Figure 3: Denoising performance on example image use in Zoran & Weiss (2011). σ = 50. Noisyimage was 8-bit quantized before reconstruction.
Figure 4: Super-resolution example with factor 3. Comparison with the same methods as in table 3.
