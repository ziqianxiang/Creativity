Figure 1: This graph illustrates how a sample from experience replay is used in training. We use anumber of frames to fill the hidden state of the RNN. Then, for the states used for training, we havethe RNN output the Q-values. Finally, we calculate each n-step return and weight them accordingto Î», where the arrows represent the forward view of each trace. All states are passed though theCNN before entering the RNN.
Figure 2: Test scores on Pong by training models with RMSprop vs Adam.
Figure 3: Test scores on Tennis comparing RMSprop and Adam.
