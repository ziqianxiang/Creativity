Figure 1: Our network compression modelThis article addresses the above issues. Specifically, we aimed to reduce the test-time computationalload of a pre-trained network. Since our approach does not depend on a network configuration(e.g. a choice of an activation function, layer structures, and a number of neurons) and acts as apost-processing of network training, pre-trained networks shared in a download site of MatConvNet(Vedaldi & Lenc, 2015) and Model Zoo (BVLC) can be compressed and accelerated. Our methodis outlined in Figure 1. The main idea is to factorize both weights and activations into integer andnon-integer components. Our method is composed of two building blocks, as shown below.
Figure 2:	Results of MNIST. The first fully connected layer was decomposed.
Figure 3:	Results of VGG-16. The last three fully connected layers were decomposed.
Figure 4:	Results of VGG-Face. The last two fully connected layers were decomposed.
Figure 5: 4096 Ã— 1000 weight matrix of last fully connected layer in VGG-16 model (Simonyan& Zisserman, 2015) is decomposed under two different constraints: (blue) {-1, +1} and (red){-1, 0, +1}.
