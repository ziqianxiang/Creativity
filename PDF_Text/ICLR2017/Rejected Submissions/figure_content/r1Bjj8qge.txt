Figure 1: A complete and decomposable SPN S with leaves over univariate distributions labeledby their scopes (1a); the MPN M obtained from S (1b); and its bottom-up evaluation to solveargmaXq~Q p(q, Xi = 0,X2 = 1, X6 = 0) (1c) with Q = {X3, X4, X5}. Orange (resp. blue) forinner (resp. leaf) activations. A tree path highlighted by MPEAssignment in the top-down traversalof M (1d). The assignment for RVs Q (resp. O = {X1, X2, X6}) is the violet (resp. purple) leaves.
Figure 2: Visualizing features learned by an SPN trained on a binarized version of MNIST: 4 clustersof 9 images generated from randomly chosen nodes from different parts of the network but havingsimilar scope lengths. The checkerboard pattern indicates pixels out of a node scope.
Figure 3: Average test EXACT MATCH scores (y axis) obtained by imputing different percentagesof missing random embedding components (x axis) for the X → EY setting on all datasets byemploying MPE inference (orange crosses) or the bottom-up evaluation imputation schemes (bluesquares). Results for Cal dataset are not reported since they are all zeros (see Table 9).
Figure 4:	Average test JACCARD scores (y axis) obtained by imputing different percentages ofmissing random embedding components (x axis) for the X → EY setting on all datasets by employingMPE inference (orange crosses) or the bottom-up evaluation imputation schemes (blue squares).
Figure 5:	Average test HAMMING scores (y axis) obtained by imputing different percentages ofmissing random embedding components (x axis) for the X → EY setting on all datasets by employingMPE inference (orange crosses) or the bottom-up evaluation imputation schemes (blue squares).
