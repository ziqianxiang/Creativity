Figure 1: The ISAN has near identical performance to other RNN architectures, and makes fulleruse of its latent space. a) Performance of RNN architectures on Text8 one-step-ahead prediction,measured as cross-entropy loss on a held-out test set, in bits per character. The loss is shown asa function of the maximum number of parameters a model is allowed. The values reported for allother architectures are taken from (Collins et al., 2016). b) The explained variance ratio of the first210 most significant PCA dimensions of the hidden states across several architectures. The legendprovides the number of latent units for each architecture. We find the ISAN model uses the hiddenspace more uniformly than the vanilla RNN or GRU.
Figure 2:	Using the linearity of the hidden state dynamics, predictions at step t can be broken outinto contributions, κts, from previous steps. Accordingly, each row of the top pane corresponds to thepropagated contribution (κts) of the input character at time s, to the prediction at time t (summed tocreate the logit at time t). The penultimate row contains the output bias vector replicated at everytime step. The last row contains the logits of the predicted next character, which is the sum of allrows above. The bottom pane contains the corresponding softmax probabilities at each time t for allcharacters (time is separated by gray lines). Labeled is the character with the maximum predictedprobability. The timestep boxed in red is examined in more detail in Figure 3.
Figure 3:	Detailed view of the prediction stack for the final 'n' in '_annuaLreVenue’. In a) all Ksare shown, in b) only the contributions to the ‘n’ logit and ‘r’ logits are shown, in orange and redrespectively, from each earlier character in the string. This corresponds to a zoom in view of thecolumns highlighted in orange and red in a). In c) we show how the sum of the contributions fromthe string ‘_annual’, K‘t_annual’, pushes the prediction at ‘_annual_reve’ from ‘r’ to ‘n’. Without thiscontribution the model decodes based only on K‘t_reve’, leading to a MAP prediction of ‘reverse’. Withthe contribution from K‘t annual’ it instead predicts ‘revenue’. The contribution of K‘t annual’ to the ‘n’and ‘r’ logits is purely linear.
Figure 4:	The time decay of the contributions from each character to prediction. a) Average normof Ks across training text, E [∣∣κS |卜],plotted as a function of t - s, and averaged across all sourcecharacters. The norm appears to decay exponentially at two rates, a faster rate for the first ten or socharacters, and then a slower rate for more long term contributions. b) The median cross entropyas a function of the position in the word under three different circumstances: the red line uses allof the κs (baseline), the green line sets all Ks apart from κt ’ to zero, while the blue line only setsKt ’ to zero. The results from panel C demonstrate the disproportionately large importance of ‘_, indecoding, especially at the onset of a word. c) Shown is the cross-entropy as a function of historywhen artificially limiting the number of characters available for prediction. This corresponds to onlyconsidering the most recent n of the K, where n is the length of the history.
Figure 5: The ISAN architecture can be used to precisely characterize the relationship between wordsand characters. The top pane shows how exploiting the linearity of the network’s operation we cancombine the κts ..κts in a word to a single contribution, κtword, for each word. Shown is the norm ofκtword, a measure of the magnitude of the effect of the previous word on the selection of the currentcharacter (red corresponds to a norm of 10, blue to 0). The bottom pane shows the probabilitiesassigned by the network to the next sequence character. Lighter lines show predictions conditionedon a decreasing number of preceding words. For example, when predicting the characters of ‘than’there is a large contribution from both 尤1强’ and Kthighe° as ShoWn in the top pane. The effect on thelog probabilities can be seen in the bottom pane as the model becomes less confident when excludingKtwas’ and significantly less confident when excluding both Ktwas’ and Kthighe「’. This word basedrepresentation clearly shoWs that the system leverages contextual information across multiple Words.
Figure 6: We can use the Ktword as an embedding space. Although the model was only trained oncharacter level representations, the Ktword show clear semantic clustering under t-SNE (Maaten &Hinton, 2008). Shown is an overview of the 4000 most common words in a). In b) a zoomed inversion is shown, a region that is primarily filled with numbers. In c) the zoom captures a variety ofdifferent professions.
Figure 7: By transforming the ISAN dynamics into a new basis, we can better understand the actionof the input-dependent biases. a) We observe a strong correlation between the norms of the inputdependent biases, bx , and the log-probability of the unigram x in the training data. We can begin tounderstand this correlation structure using a basis transform into the ‘readout basis’. Breaking out thenorm into its components in Prko and Pr⊥o in b) and c) respectively, shows that the correlation is dueto the component orthogonal to Wro. This implies a connection between information or ‘surprise’and distance in the ’computational’ subspace of state space.
Figure 8: By transforming ISAN dynamics into a new basis, we can better interpret structure in theinput-dependent biases. In a) we show the cosine distance between the input dependent bias vectors,split between vowels and consonants (‘ ’ is first). In b) we show the correlation only consideringthe components in the subspace Prko spanned by the rows of the readout matrix Wro. c) shows theFigure 9: The predictions of ISAN for one and two characters well approximate the predictions ofunigram and bigram models. In a) we compare softmax(bro) to the empirical unigram distributionP (x). In b) we compare softmax(Wrob‘_’ + bro) with the empirical distribution P (xt+1 |‘_’). In c)we show the correlation of softmax(Wrobx + bro) with P(xt+1|xt) for all 27 characters (y-axis),and compare this to the correlation between the empirical unigram probabilities P(x) to P (xt+1 |xt)(x-axis). The plot shows that the readout of the bias vector is a better predictor of the conditionaldistribution than the unigram probability.
Figure 9: The predictions of ISAN for one and two characters well approximate the predictions ofunigram and bigram models. In a) we compare softmax(bro) to the empirical unigram distributionP (x). In b) we compare softmax(Wrob‘_’ + bro) with the empirical distribution P (xt+1 |‘_’). In c)we show the correlation of softmax(Wrobx + bro) with P(xt+1|xt) for all 27 characters (y-axis),and compare this to the correlation between the empirical unigram probabilities P(x) to P (xt+1 |xt)(x-axis). The plot shows that the readout of the bias vector is a better predictor of the conditionaldistribution than the unigram probability.
Figure 10: A visualization of the dynamics of an ISAN for the parenthesis counting task. In a) theweight matrix for ‘(’ is shown in the original basis. In c) it is shown transformed to highlight thedelay-line dynamics. The activations of the hidden units are also shown b) in the original basis, andd) rotated to the same basis as in c), to highlight the delay-line dynamics in a more intelligible way.
