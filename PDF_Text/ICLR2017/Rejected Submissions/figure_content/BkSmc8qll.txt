Figure 1: A graphical illustration of the proposed dynamic neural Turing machine with therecurrent-controller. The controller receives the fact as a continuous vector encoded by a recurrentneural network, computes the read and write weights for addressing the memory. If the D-NTMautomatically detects that a query has been received, it returns an answer and terminates.
Figure 2: An example view of the discrete attention over the memory slots for both read (left) and writeheads(right). x-axis the denotes the memory locations that are being accessed and y-axis correspondsto the content in the particular memory location. In this figure, we visualize the discrete-attention modelwith 3-reading steps and on task-20. It is easy to see that the NTM with discrete-attention accessesto the relevant part of the memory. We only visualize the last-step of the 3-steps writing. Becausewith discrete attention usually the model just reads the empty slots of the memory.
Figure 3: A visualization for the learning curves of continuous and discrete D-NTM models trainedon Task 1 using 3 steps. In most tasks, we observe that the discrete attention model with GRU controllerdoes converge faster than the continuous-attention model.
Figure 4: We compare the learning curves of our D-NTM model using discrete attention on pMNISTtask with input-based baseline and regular REINFORCE baseline. The x-axis is the loss and y-axisis the number of epochs.
