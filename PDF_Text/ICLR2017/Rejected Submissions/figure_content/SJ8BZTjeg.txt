Figure 1: Figure shows the InfoGAN architecture that was used in all our experiments. Notice thatthe input to G(.) is a combination of Z and c. Also notice that most of the parameters are sharedbetween the Q(.) network and the D(.) network, thus improving the computational efficiency.
Figure 2: Figure shows samples generated from InfoGAN trained on the CIFAR-10 dataset whenthe system was encouraged to identify 10 categories. Each row corresponds to a different clusteridentified by InfoGAN. Each column corresponds to a different sample from that clusters. Wecan see that while InfoGAN can identify clusters that are different from each other, they do notcorrespond to the CIFAR-10 categories. See Sec. 4.1 for quantitative results.
Figure 3: This figure shows all the 64 filters from the first layer of the discriminative network trainedon CIFAR-10. The visualization on the left corresponds to the filters learned using adversarialtraining. The visualization on the right corresponds to the filters learned for the same architectureusing supervised learning. It is interesting to see that there the filters on the left have more highfrequency components and the filters on the right are more smooth.
Figure 4: CIFAR-10: (a) Plots the performance of the grouping algorithm when using the featureslearned from InfoGAN training when trained over multiple categories. Zero groups correspondsto vanilla GAN. -32 and -64 correspond to the output sizes of the generated images. -InfoGANcorresponds to the results obtained with direct prediction using the recognition model in InfoGAN.
Figure 5: CIFAR-100: (a) # of groups used to train InfoGAN has less ofan effect on CIFAR-100 thanit had on CIFAR-10. However, the performance of k-means++ clustering is still better than directprediction using the recognition model of InfoGAN. Please see Fig. 4a for labeling conventions.
Figure 6: STL-10: (a) InfoGAN’s performance drops with increase in the number of groups. (b)Vanilla GAN’s features outperform InfoGAN-trained features. Also, notice that, with just 5000labeled training images, supervised learning starts to reach its limits. However, our model makesuse of the additional 100000 unlabeled images and is able to learn representations that surpass theperformance of features learned using the supervised model.
