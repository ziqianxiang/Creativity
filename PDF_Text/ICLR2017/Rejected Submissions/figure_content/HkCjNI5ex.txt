Figure 1: Distribution of the magnitude of softmax probabilities on the MNIST validation set. Afully-connected, 2-layer, 1024-unit neural network was trained with dropout (left), label smoothing(center), and the confidence penalty (right). Dropout leads to a softmax distribution where probabil-ities are either 0 or 1. By contrast, both label smoothing and the confidence penalty lead to smootheroutput distributions, which results in better generalization.
Figure 2: Norm of the gradient as training proceeds on the MNIST dataset. We plot the norm ofthe gradient while training with confidence penalty, dropout, label smoothing, and without regular-ization. We use early stopping on the validation set, which explains the difference in training stepsbetween methods. Both confidence penalty and label smoothing result in smaller gradient norm.
