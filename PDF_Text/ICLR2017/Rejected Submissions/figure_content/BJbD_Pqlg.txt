Figure 1: Predicting perturbation thresholds. a, For a fixed image perturbation, perceptual detectionthreshold (visualized by red arrow) depends on image context. b, Measured perceptual threshold iscorrelated with the average L1 change in DNN computation due to image perturbation (for DNNmodel VGG-19, image scale=100%). c, Explained variability (R2) of perceptual threshold datawhen L1 change is based on isolated computational layers for different input image scales. SameVGG-19modelas in (b). X-axis labels: data refers to raw image pixel data, conv*_1 and fc_* arethe before-ReLU output of a convolution and a fully-connected operation, respectively, and probis the output class label probabilities vector. d, Example images for whcih predicted threshold in bis much higher than perceptually measured (”Overshoot”, where perturbation saliency is better thanpredicted), or vise versa (”Undershoot”). Examples are considered from several perceptual thresholdranges (±2 dB of shown number).
Figure 2:	Background context. a-c, Illustrations of reproduced discrimination stimuli for threepsychophysical experiments (actual images used were white-on-black rather than black-on-white,and pattern size was smaller, see Figures 12-14). d, Number of configurations for which order-of-difficulty in discrimination is qualitatively consistency with perception according to a mutualinformation DNN metric. Configurations vary in pattern (element size, target location, and jittermagnitude; see Section 8.4) and in DNN architecture used (CaffeNet, GoogLeNet, VGG-19, andResNet-152). DNN metric is the average across neurons of the isolated neuron target-discriminativeinformation (averaged first within, and then across computational layer stages), where performanceis limited by location jittering (e.g. evident jitter in illustrations). e-g, The value of the MI metricacross computational layers of model VGG-19 for a typical pattern configuration. The six ”hard”(gray) lines in Shape MI correspond to six different layouts (see Section 8.4.3). Analysis shows thatfor isolated computation stages, similarity to perception is evident only at the final DNN computationstages. h, A caricature summarizing the similarity and discrepancy of perception and the MI-basedDNN prediction for Shape (see Figure 9).
Figure 3:	Contrast sensitivity. a. Perceived contrast is strongly affected by spatial frequency atlow contrast, but less so at high contrast (which preserves the physical quantity of contrast and thustermed constancy). b. The Li change in VGG-19 representation between a gray image and imagesdepicting sinusoidal gratings at each combination of sine spatial frequency (x-axis) and contrast(color) (random orientation, random phase), considering the raw image pixel data representation(data), the before-ReLU output of the first convolutional layer representation (Conv1_1),the out-put of the last fully-connected layer representation (fc8), and the output class label probabilitiesrepresentation (prob).
Figure 4: Predicting perceptual sensitivity to image changes (following Figure 1). a-c, The Lichange in CaffeNet, GoogLeNet, and ResNet-152 DNN architectures as a function of perceptualthreshold. d, The Li change in GoogLeNet as a function of the Li change in VGG-19.
Figure 5: Prediction accuracy as a function of computational stage. a, Predicting perceptual sen-sitivity for model VGG-19 using the best single kernel (i.e. using one fitting parameter, no crossvalidation), vs. the standard Li metric (reproduced from Figure 1). b, For non-branch computa-tional stages of model ResNet-152.
Figure 6: Images where predicted threshold is too high (”Overshoot”, where perturbation saliencyis better than predicted) or too low (”Undershoot”), considered from several perceptual thresholdranges (±2 dB of shown number). Some images are reproduced from Figure 1.
Figure 7:	Background context for different DNN models (following figure 2).
Figure 8:	Background context for baseline DNN models (following figure 2). ''CaffeNet iter310K”is reproduced from Figure 7.
Figure 9:	Background context for Shape. Shown for each model is the measured MI for the six“Hard" shapes as a function of the MI for the “Easy" shape. The last panel shows an analagouscomparison measured in human subjects by Weisstein & Harris (1974). A data point which liesbelow the dashed diagonal indicates a configuration for which discriminating line location is easierfor the Easy shape compared with the relevant Hard shape.
Figure 10:	Contrast sensitivity (following Figure 3) for DNN architectures CaffeNet, GoogLeNet,and ResNet-152.
Figure 11:	Comparison of contrast sensitivity. Shown are iso-output curves, for which perceivedcontrast is the same (Human), or for which the Li change relative to a gray image is the same (DNNmodel VGG-19). To obtain a correspondence between human frequency values (given in cycles perdegree of visual field) to DNN frequency values (given in cycles per image), a scaling was chosensuch that the minima of the blue curve is given at the same frequency value. Human data is forsubject M.A.G. as measured by Georgeson & Sullivan (1975).
Figure 12: Pattern scales used in the different configurations of the Segmentation condition. Actualimages used were white-on-black rather than black-on-white.
Figure 13:	Pattern scales used in the different configurations of the Crowding condition. Actualimages used were white-on-black rather than black-on-white.
Figure 14:	Pattern scales used in the different configurations of the Shape condition. Actual imagesused were white-on-black rather than black-on-white.
