Figure 1: Network architecture for joint video frame and reward prediction. The architecture com-prises three stages: an encoding stage mapping current input frames to some compressed latentrepresentation, a transformation stage integrating the current action into the latent representationthrough element-wise vector multiplication denoted by ' × ’，and a final predictive stage for recon-structing the frame of the next time step and the current reward. The network uses three differenttypes of neuron layers (’Conv’ for convolutional, ’Deconv’ for deconvolutional and ’Fc’ for forwardconnection) in combination with three different types of activation functions (’ReLU’, ’Softmax’ and’Lin’ for linear activations). The dimensional extend of individual layers is either depicted beneathor within layers. The network part coloured in red highlights the extension for reward prediction.
Figure 2: Cumulative reward error over look ahead steps in five different Atari games. There aretwo plots for each game. The top plot per game shows how the median and the 5 to 95 percentilesof the cumulative reward error evolve over look ahead steps for both our model (in blue) and a base-line model that samples rewards from the marginal reward distribution of the test set (in red). Eachvertical slice of this concise representation corresponds to a single empirical distribution over thecumulative reward error. We depict these for every fifth look ahead step in the compound plots be-low for both models. These empirical error distributions demonstrate successful cumulative rewardprediction over at least 20 steps (80 frames) in all five games as evidenced by their zero-centeredand unimodal shape in the first column of each compound plot per game.
Figure 3: Example predictions in Seaquest. Ground truth video frames, model predictions and errormaps emphasizing differences between ground truth and predicted frames—in form of the squarederror between pixel values—are compared column-wise. Error maps highlight objects in black orwhite respectively depending on whether these objects are absent by mistake or present by mistakein the model’s prediction. Actions taken by the agent as well as ground truth rewards (’rew’) andreward predictions (’pred’) are shown below video and error frames. Peculiarities in the predictionprocess are marked in red. The figure demonstrates how our predictive model fails to anticipateobjects that randomly enter the scene from the right and rewards associated to these objects.
Figure 4: Effect of reward weight on training loss in Ms Pacman. Each of the four panels depictsone experiment with a different reward weight λ. Each panel shows how the training loss evolvesover minibatch iterations in terms of two subplots reporting video frame reconstruction and rewardloss respectively. Each experiment was conducted three times with different initial random seedsdepicted in blue, green and red. Graphs were smoothed with an exponential window of size 1000.
Figure 5:	Effect of gradient clipping on training loss in Seaquest. The three panels compare ex-periments with no reward clipping to those with reward clipping using the threshold values 5 and 1respectively. Subplots within each panel are similar to those in Figure 4 but display in the first rowthe evolution of the compound training loss in addition to the frame reconstruction and reward loss.
Figure 6:	Effect of curriculum learning on five different Atari games. Each panel corresponds to adifferent game, individual panels are structured in the same way as are those in Figure 5A.5 Effect of Random SeedsWe conducted three different experiments per game with different initial random seeds. The effectof different initial random seeds on the cumulative reward error is summarized in Figure 7 whichreports how the median and the 5 to 95 percentiles of the cumulative reward error evolve over lookahead steps in the different experiments per game. Note that the results of the first column in Figure 7are shown in Figure 2 from the main paper together with a more detailed analysis depicting empiricalcumulative reward error distributions for some look ahead steps. The random initial seed does notseem to have a significant impact on the cumulative reward prediction except for Freeway where thenetwork in the third experiment starts to considerably overestimate cumulative rewards at around 30to 40 look ahead steps.
Figure 8: Example predictions in Freeway over 20 steps. The figure is similar in nature to Figure 3from the main paper with the only difference that predictions are depicted from time step 31 onwards.
Figure 9: Loss on test set over look ahead steps. Each row reports the loss on the test set over 100look ahead steps for a different game. The first column illustrates the compound loss consisting ofthe video frame reconstruction loss (second column) and the reward prediction loss (third column).
