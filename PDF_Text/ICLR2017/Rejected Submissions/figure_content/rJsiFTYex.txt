Figure 1: A comparison of the performance of Monte Carlo averaging, over sample size, to regularsingle-sample inverted dropout at test-time.
Figure 2: An illustration of the embed average pooling extension to a standard RNN model. Theoutput of the multilayer perceptron is concatenated to the final hidden state output by the RNN.
Figure 3: An illustration of vertical (ResV) and lateral residual (ResL) connections added to a 3-layerrNn. A model with only vertical residuals is denoted Res-V1, whereas a model with vertical andlateral residuals is denoted “Res-V2”.
Figure 4: These box-plots show the performance of compounding model features on fine-grain SSTvalidation accuracy. The red points, red lines, blue boxes, whiskers and plus-shaped points indicatethe mean, median, quartiles, range, and outliers, respectively.
Figure 5: These box-plots show the performance of compounding model features on binary IMDBvalidation accuracy.
Figure 6: Comparing the effects of layer depth between Vanilla RNNs, Res-V1 and Res-V2 modelson fine-grained sentiment classification (SST). As we increase the layers, we decrease the hidden sizeto maintain equivalent model sizes. The points indicate average validation accuracy, while the shadedregions indicate 90% confidence intervals.
