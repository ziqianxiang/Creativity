Figure 1: Humans come up with complete and plausible imaginations based on their familiar mem-ories, they imagine, rather than merely labeling pixels.
Figure 2: Our model consists of an imagination generator, a graphics-like imagination renderer,a memory retrieval engine and discriminator networks for distribution matching between inferredimaginations and retrieved relevant memories. Here We show our model tailored to the task offigure-ground layer inference, where the imaginations are the segmented foreground object and thecompleted background scene.
Figure 3: Model architecture for (a) image in-painting, (b) intrinsic image decomposition, and (C)figure-ground layer extraction.
Figure 4: Results of image in-painting on MNIST dataset. Row 2: BmemL2 treats retrieved relevantmemories as ground-truth imagination and penalizes the l2 loss between them. Row 3: Bmemrand:our model without memory retrieval but rather uniform at random memory access. Imaginationsdo not respect the input image conditioning. Row 4: BmemrandHR: our model without memoryretrieval but increased weight on the reconstruction loss. Imaginations do not look like correctdigits. Row 5: Our model. It produces correct in shape and texture digit imaginations in contrast tothe baselines above. Row 6: Top closest relevant memories retrieved by our engine. Row 7: Randommemory retrieval.
Figure 5: MIT intrinsic decomposition with unpaired shading and albedo. I: Input Image, A: In-ferred albedo, S: Inferred shading, R: Reconstructed Image using A and S. Left: inferred albedo andshading using our weakly supervised method. Right: inferred albedo and shading using a fully su-pervised model that minimizes regression loss. The bottom part shows the results of decompositionson unseen objects.
Figure 6: L2 distance between inferred albedo imagination and the ground truth. Fully-convolutionaldiscriminators (purple and green lines) converge faster than fully connected ones, that employ onlyone fake/real classifier per image.
