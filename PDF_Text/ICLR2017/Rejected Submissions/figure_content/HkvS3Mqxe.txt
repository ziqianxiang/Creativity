Figure 1: (a-d) shows four possible pruning granularities. The proposed work is focussed on the (b)feature map and (c) kernel pruning for simple sparse represenation. It can be observed that for thedepicted architecture in Fig. (b), four convolution kernels are pruned.
Figure 2: This figure compares the iterative andone-shot pruning. tpr and cpr represents the tar-get and current pruning ratio respectively. Theiterative pruning Han et al. (2015b) graduallyachieves the target pruning ratio in M steps of ∆size each, while the ∆ = tpr for one-shot prun-ing. This work adopts the one-shot pruning ap-proach.
Figure 3: The network architectures are reported in Table 1. The networks are feature map pruned togenerate the pre-retraining plots. Figure (a, c) compares the best candidate selected out of N randomcombinations for various pruning ratios. The distribution of N random evaluations is shown in Fig.
Figure 4: (a) This figure explains the idea presented in Li et al. (2016) and shows three layers, L1,L2 and L3. All the filters/kernels from previous layer to a feature map constitute one group whichis shown with similar color. The S1,S2 and S3 is computed by summing the absolute value ofall the weights in this group. (b) The comparison of the proposed method with the absolute weightsum method is shown here for the CNNSV HN. It can be observed that our proposed method inflictslesser adversary on the network for different pruning ratios. (d) In this plot, we prune a CNN networkwith various masks and compare their pre and post retraining performance. It can be observed thaton the average, pre-retraining masks perform better after retraining.
Figure 5:	Figure (a) and (b) shows feature map and kernel pruning of two networks:C N NC I F AR-10.small and CNNMNIST2. The corresponding network architectures are reportedin Table 1. The network can be pruned by more than 50% with very small degradation in perfor-mance. Further, due to finer nature, the kernel pruning may inflict lesser adversary on the networkperformance.
Figure 6:	(a) This figure shows the profiling results for kernel pruning with a customized GPU im-plementation. It can be observed that the kernel pruning reduces the execution time. The experimentis conducted with the CIFAR-10 CNN. In (b), Fi and Fo shows the input and output feature maps,while pr represents the pruning ratio. The GPU function scheduler shows that the call is only fornon-masked kernels.
Figure 7: The combinations of feature map and kernel pruning is reported here. Figure (a) and (b)provides pruning results for the C N NC I F AR10.small and CN NCIF AR10.large networks. It can beobserved from both figure, that more sparsity can be induced in the network by indcuing sparsitywith two granularities.
Figure 8: The pruning plots for 100 class classification problem is reported in (a). It can be observedthat this network can be pruned by more than 60% with very small degradation in the networkperformance. Figure (b) shows the pruning results for the CNNSV HN. It can be observed thatmore than 70% sparsity can be induced in the network while the network accuracy still remainsabove 96%.
Figure 9: This figure shows the per class MCR for the original, feature map, and kernel prunednetworks. It can be observed that the per class error does not vary much in the pruned networks.
