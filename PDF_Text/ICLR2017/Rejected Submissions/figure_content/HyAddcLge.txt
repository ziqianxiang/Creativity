Figure 2: Degradation of test classi-fication error with increasing averagegradient staleness in MNIST CNNmodel.
Figure 1: Gradient staleness dependence on model layer. Gradientsare computed in a bottom-up forward propagation step followed by atop-down back propagation step. Parameters are read from servers inthe forward prop, but gradients are sent to servers during the back prop.
Figure 3: CDF of time taken to aggregate gradientsfrom N machines. For clarity, we only show times of≤ 6s; the maximum observed time is 310s.
Figure 4: Mean and median times, across all itera-tions, to collect k gradients on N = 100 workers andb = 0 backups. Most mean times fall between 1.4sand 1.8s, except of final few gradients.
Figure 5: Number of iterations to converge when ag-gregating gradient from N machines.
Figure 6: Estimated time to converge when aggregat-ing gradients from N machines on a N + b = 100machine configuration. Convergence is fastest whenchoosing N = 96, b = 4.
Figure 7: Convergence of Sync-Opt on Inception model using N =100 workers and b = 6 backups, with varying initial learning rates γ0 .
Figure 8: Convergence of Sync-Opt and Async-Opt on Inception model using varying number of machines.
Figure 9: Convergence of synchronous and asynchronous training on PixelCNN model. Sync-Opt achieveslower negative log likelihood in less time than Async-Opt.
