Figure 1: Gating mechanism applied to the shortcut connection of a layer. The key difference withHighway Networks is that only a scalar k is used to regulate the gates instead ofa tensor.
Figure 2: Proposed network design applied to Residual Networks. Note that the joint network designresults in a shortcut path where the input remains unchanged. In this case, g(k) can be interpretedas an amplifier or suppressor for the residual fr(x, W).
Figure 3: A network can have layers added to it without losing performance. Initially, a network hasm ReLU layers with parameters {W1, . . . , Wm}. A new, (m+ 1)-th layer is added with Wm+1 = I.
Figure 4: Train loss for plain and residual networks, along with their augmented counterparts, withd = {2, 10, 20, 50, 100}. As the models get deeper, the error reduction due to the augmentationincreases.
Figure 5: Left: Values for k according to ascending order of residual blocks. The first block,consisted of the first two layers of the network, has index 1, while the last block - right before theSoftmax layer - has index 50. Right: Test accuracy (%) according to the number of removed layers.
Figure 6: Values for k according to ascending order of residual blocks. The first block, consisted ofthe first two layers of the network, has index 1, while the last block - right before the Softmax layer-has index 12.
Figure 7: Training and test curves for the Wide ResNet (4,10) with 0.3 dropout, showing error (%)on training and test sets. Dashed lines represent training error, whereas solid lines represent testerror.
