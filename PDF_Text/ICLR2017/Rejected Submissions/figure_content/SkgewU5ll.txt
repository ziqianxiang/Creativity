Figure 1: The illustration of GRAM. Leaf nodes (solid circles) represents a medical concept in the EHR,while the non-leaf nodes (dotted circles) represent more general concepts. The final representationgi of the leaf concept ci is computed by combining the basic embeddings ei of ci and eg , ec andea of its ancestors cg , cc and ca via an attention mechanism. The final representations form theembedding matrix G for all leaf concepts. After that, we use G to embed patient visit vector xt to avisit representation vt, which is then fed to a neural network model to make the final prediction yt.
Figure 2: t-SNE scatterplots of medical concepts trained by GRAM+, RNN+ and GloVerandomly chosen diseases learned by GRAM+ for sequential diagnoses prediction on Sutter data4(Figure 2a). The colors represent the highest disease categories and the text annotations represent thedetailed disease categories in CCS multi-level hierarchy. For comparison, we also show the t-SNEplots on the strongest results from RNN+ (Figure 2b), and GloVe (Figure 2c), the same embeddingtechnique in initializing the basic embeddings ei . Figures 2b and 2c confirm that interpretablerepresentations cannot simply be learned only by co-occurrence or supervised prediction withoutmedical knowledge. GRAM+ learns disease representations that are significantly more consistent withthe given knowledge DAG G . Therefore the neural network predictive model that accepts gi is usingaccurate representations that lead to higher predictive performance. Additional scatterplots of othermodels are provided in Appendix E for comparison. An interactive visualization tool can be accessedat http://www.sunlab.org/research/gram-graph-based-attention-model/.
Figure 3: GRAMâ€™s attention behavior during HF prediction for four representative diseases (eachcolumn). In each figure, the leaf node represents the disease and upper nodes are its ancestors. Thesize of the node shows the amount of attention it receives, which is also shown by the bar charts. Thenumber in the parenthesis next to the disease is its frequency in the training data. We exclude the rootof the knowledge DAG G from all figures as it did not play a significant role.
Figure 4: Creating the co-occurrence matrix to-gether with the ancestors. Here we exclude theroot node, which will be just a single row (column).
Figure 5: Scatterplot of medical concepts trained by various models. We used t-SNE to reduce thedimension to 2-D.
