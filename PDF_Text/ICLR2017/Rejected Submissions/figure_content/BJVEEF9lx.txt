Figure 1: Validation of stack trained on MNIST digits, and introspection of internal representation.
Figure 2: Generalization of the stack. Top left to top right, 10 images stacked in sequence usingpush. Bottom right to bottom left: result from calling pop on stack 10 times. This stack was trainedto stack three digits. It appears to generalize partially to four digits but quickly degrades after that.
Figure 3: Left: Stack versus queue encoding. Three MNIST images (top row) were enqueued ontothe empty queue (middle row left), and pushed onto the empty stack (bottom row left). Middle rowshows the internal queue representation after each enqueue operation, while bottom is the internalstack representation after each push. In this case, the learned stack representation compresses pixelintensities into different striated sections of real line, putting data about the first stacked items atlower values and then shifting these to higher values as more items are stacked. This strategy appearsdifferent from that in figure 1, which notably was trained to a lower error value. The internal queuerepresentation is less clear; the hexagonal dot pattern may be an artifact of optimization or criticalto its encoding. Both enqueue and push had the same convolutional architecture. Right: Internalrepresentations of natural numbers from 0 (top) to 19 (bottom). Natural numbers are internallyrepresented as a vector of 10 elements. Number representations on the left are found by repeatedingthe succesor function, e.g. (succ(zero), succ(succ(zero)), ...). Numbers on the right are found byencoding machine integers into this internal representation.
