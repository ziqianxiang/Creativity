Figure 1: One-dimensional CNN architecture where W ∈ RKn×MD is the matrix instantiation of convolutionover M channels with a filter bank consisting of K different filters. Note that a filter bank has K filters of sizel × M, such that there are lMK parameters in this architecture.
Figure 2: For 1D scaled Gaussian random filters W, we plot the histogram of ratios (a) kW T zk2/kzk2 (model-RIP condition in Equation (1); supposed to be concentrated at 1), (b) kWWTzk2/kzk2 (model-RIP corollaryfrom LemmaB.1 in the supplementary materials; supposed to be concentrated at 1), and (C) ∣∣x — x∣∣2/∣∣x∣∣2(reconstruction bound in Theorem 3.3, supposed to be small), where z is a Mk sparse signal that generatesthe vector X and X = WTMfiXed(Wx, k) is the reconstruction of x, where We use the naive unsampling torecover the reduced dimension due to pooling (see Section 2.3).
Figure 3: For VGGNet's conv(5, 2) filters W, we plot the histogram of ratios ∣∣ WTzk2∕kz∣∣2 (the model-RIPvalue derived from Equation (1); supposed to be concentrated at 1) where z is a Mk sparse signal. (a) z israndomly generated with the same sparsity as the conv(5, 2) activations and from a uniform distribution for thenon-zero magnitude. (b) z is recovered by Algorithm 2 from the conv(5,1) activations before applying ReLU.
Figure 4: Visualization of images reconstructed by a pretrained decoding network with VGGNet’s pool(4)activation reconstructed using different methods: (a) original image, (b) output of the 5-layer decoding networkwith original activation, (c) output of the decoding net with reconstructed activation by IHT with learned filters,(d) output of the decoding net with reconstructed activation by IHT with Gaussian random filters, (e) output ofthe decoding net with Gaussian random activation.
Figure 5: Visualization of images reconstructed by a pretrained decoding network with VGGNet’s pool(4)activation reconstructed using different methods: (a) original image, (b) output of the 5-layer decoding networkwith original activation, (c) output of the decoding net with reconstructed activation by IHT with learned filters,(d) output of the decoding net with reconstructed activation by IHT with Gaussian random filters, (e) output ofthe decoding net with Gaussian random activation.
