Figure 1: Read-Again Summarization Modelpropose Pointer Networks, which calculate a probability distribution over the input sequence insteadof predicting a token from a pre-defined dictionary. Cheng & Lapata (2016) develop a neural-basedextractive summarization model, which predicts the targets from the input sequences. Gulcehre et al.
Figure 2: Read-Again Model(b) LSTM Read-Again Encoderthis idea to two popular RNN architectures, i.e. GRU and LSTM, resulting in better encodings of theinput text. Note that although other alternatives, such as bidirectional RNN exist, the hidden statesfrom the forward RNN lack direct interactions with the backward RNN, and thus forward/backwardhidden states still cannot utilize the whole sequence. Besides, although we only use our model in auni-directional manner, it can also be easily adapted to the bidirectional case. We now describe thetwo variants of our model.
Figure 3: Hierachical Read-AgainCombining this with the standard GRU update ruleGRU2(xi,hi2-1) = (1 -zi)	hi2-1 +ziehi2,we can simplify the updating rule Eq. (4) to gethi2 = (1 - αi	zi)	hi2-1 + (αi	zi) ehi2	(6)This equations shows that our ‘read-again’ model on GRU is equivalent to replace the GRU cellwith a more general gating mechanism that also depends on the feature representation of the wholesentence computed from the first reading pass. We argue that adding this global information couldhelp direct the information flow for the forward pass resulting in a better encoder.
Figure 4: Weight Visualization. Black indicates high weight4.2	Evaluation of COPY MechanismTable 3 shows the effect on our model of decreasing the decoder vocabulary size. We can see thatwhen using the copy mechanism, we are able to reduce the decoder vocabulary size from 69K to2K, with only 2-3 points drop on RoUGE score. This contrasts the models that do not use the copymechanism. Equipped with a copy mechanism, our model is able to generate ooVs as summary8Under review as a conference paper at ICLR 2017	Rouge-1		Rouge-2		Rouge-L	Size	Ours-LSTM	Ours-LSTM (C)	Ours-LSTM	Ours-LSTM (C)	Ours-LSTM	Ours-LSTM (C)2K	14:39	24.21	6.46	11:27	13.74	23.095K	20.61	26.83	9.67	12.66	19.58	25.3115K	25.30	27.37	11.76	12.64	23.74	25.6930K	26.86	27.49	11.93	12.75	25.16	25.7769K	27.82	27.89	12.73	12.69	26.01	26.03Table 3: ROUGE Evaluation for Models with Different Decoder Size and 110k Encoder Size. Oursdenotes Read-Again. C denotes copy mechanism.
