Figure 1: Instantiation of our Dynamic Programming Net. An input instance is recurrently splitinto smaller instances until we reach a critical scale. The merge procedure works in the oppositedirection. We highlight the fact that split and merge sizes can vary and are data dependent, andtherefore the binary tree is sample-dependent. All split blocks share the same parameters θ andsimilarly for the merge blocks with φ.
Figure 2: (a) and (b): The loss at every scale is computed averaging over all the losses of the nodesfor all elements in the batch. We only show the first 6 scales. The input distribution for everynode of the generated tree is different at every block and scale because the inputs at a given scaleare the result of a finer partition of the inputs at the previous one. (a): Losses during training fordata set 〜N(0.5,0.25). We observe that the values of the losses are very similar. However, thelosses at first scales are a little bit larger than the others because the input distribution becomes moresymmetric as the partition gets finer. (b): Losses during training for data set 〜 Exp(μ = 0.1).
Figure 3: After every epoch, we test the model in a dataset of size 128. (a): Test accuracies. Weobserve that the accuracy is lower for 3 scales. This is due to the tree structure and how the errorspropagate through it. In the case of SS, the merge block is not able to correct mistakes comingfrom previous layers because the model is always trained with correct inputs. For WS, we alsoobserve a lower accuracy for 3 scales, however, the decrease is smaller than using strong supervision.
