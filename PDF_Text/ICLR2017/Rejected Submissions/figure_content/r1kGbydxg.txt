Figure 1: Simulated articulated figures and their state representation. Revolute joints connect alllinks. From left to right: 7-link biped; 19-link raptor; 21-link dog; State features: root height,relative position (red) of each link with respect to the root and their respective linear velocity (green).
Figure 2: Learning curves for each policy during 1 million iterations.
Figure 3: Performance when subjected to random perturbation forces of different magnitudes.
Figure 4: Performance of policies with different query rates for the biped (left) and dog (right).
Figure 5: Performance of intermediate MTU policies and actuator parameters per pass of actuatoroptimization following Algorithm 2.
Figure 6: Learning curves comparing initial and optimized MTU parameters.
Figure 7: Left: fixed initial state biases agent to regions of the state space near the initial state,particular during early iterations of training. Right: initial states sampled from reference trajectoryallows agent to explore state space more uniformly around reference trajectory.
Figure 8: Neural Network Architecture. Each policy is represented by a three layered network, with512 and 256 fully-connected hidden units, followed by a linear output layer.
Figure 9: Learning curves from different random network initializations. Four policies are trainedfor each actuation model.
Figure 10: Learning curves comparing the effects of scaling the standard deviation of the actiondistribution by 1x, 2x, and 1/2x.
Figure 11: Learning curves for different network architectures. The network structures include,doubling the number of units in each hidden layer, halving the number of units, and inserting anadditional hidden layer with 512 units between the two existing hidden layers.
Figure 12: Performance of different action parameterizations when traveling across randomly gen-erated irregular terrain. (left) Dog running across bumpy terrain, where the height of each bumpvaries uniformly between 0 and a specified maximum height. (middle) and (right) biped and dogtraveling across randomly generated slopes with bounded maximum steepness.
Figure 13: Simulated Motions Using the PD Action Representation. The top row uses an MTUaction space while the remainder are driven by a PD action space.
Figure 14: Policy actions over time and the resulting torques for the four action types. Data is fromone biped walk cycle (1s). Left: Actions (60 Hz), for the right hip for PD, Vel, and Tor, and the rightgluteal muscle for MTU. Right: Torques applied to the right hip joint, sampled at 600 Hz.
Figure 15: Learning curves for different state representations including state + target state, state +phase, and only state.
