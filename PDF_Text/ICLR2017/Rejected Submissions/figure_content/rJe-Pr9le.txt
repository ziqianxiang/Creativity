Figure 1: We chose i = 0 and k = 1. We assume a0 to be the pixels in the current image (the leftone) and c1 to be the jump action. Then, given that input, we want to predict r1 - r0, which is 1,because we earn a reward from time 0 to time 1.
Figure 2: Diagram of our predictive model.
Figure 3: The equations of the RRNN and a diagram of the network.
Figure 4: Each layer is followed by aBatch Normalization (Ioffe & Szegedy,2015) and a Rectifier Linear Unit.
Figure 5: Comparison between an agent that learns the three games simultaneously (continuousblue), one that learns each game individually (dashed red) and the score of human testers (horizontalgreen) as reported by Mnih et al. (2015).
