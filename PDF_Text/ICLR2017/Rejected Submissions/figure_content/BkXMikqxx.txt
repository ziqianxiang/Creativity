Figure 1: The cognitive process of reading, a seven steps procedure that includes an open-bigramsrepresentation layer. Additional information helps the organization of levels 4 and 5, when using aphonics method, but not a whole language method (today banned from reading teaching for lack ofefficiency, adapted from (Dehaene et al., 2005) and (Touzet, 2015).
Figure 2: Word representation as an explicit sequence of letters (a), and as a set of bigrams (b). Greyedges show the potential impact of misrecognitions.
Figure 3: Visualization of the bigram representation of the English vocabulary, for d = 1..3 (Touzetet al., 2014) (left), vs after t-SNE (Van der Maaten & Hinton, 2008) (right). Our complete bigramicmap of English: https://youtu.be/OR2vjj8MNeM?t=197.
Figure 4: Hypothetical context needed in the input image to make two consecutive (yellow and blue)bigram predictions, for d = 0 (left, to predict c, then t) to 3 (right, to predict ai, then cc). As dincreases, the contexts become more complex to model: they involve long range dependencies andare highly intertwined.
Figure 5: Distribution of lengths of filtered-out wordsA.2 Baseline SystemsWe built several models and used the same vocabulary as for the bigram decoder, and no languagemodel (all words have the same prior probability). The first one is a Hidden Markov Model, with 5(Rimes) or 6 (IAM) states per characters, and an emission model based on Gaussiam Mixture Mod-els. This system is trained with the Maximum Likelihood criterion, following the usual Expectation-Maximization procedure. At each iteration, the number of Gaussians is increased, until no improve-ment is observed on the validation set. The forced alignments with the GMM/HMM system areused to build a labeled dataset for training a Multi-Layer Perceptron (MLP) with 4 hidden layersof 1,024 sigmoid units. We optimized the cross-entropy criterion to train the network to predict theHMM states from the concatenation of 11 input frames, with a learning rate of 0.008. The learningrate was halved when the relative improvement was smaller than 1% from one epoch to the next.
Figure 6: Neural network architecture. A sequence of feature vectors is extracted with a slidingwindow, and fed to a multi-layer BLSTM network, with subsampling after the first BLSTM layer.
Figure 7: Relation between cosine similarity in bigram space and normalized edit distance, for theEnglish (a) and French (b) vocabularies.
Figure 8: Distribution of word errors vs. word length for the proposed (left) and sequential (right)approaches. We observe the same behaviour: short words tend to be more difficult to recognize.
