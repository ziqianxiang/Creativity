Figure 1: Building blocks for coding networks explored in this paper. Our coding model usesnon-linearities that are closely related to the standard ReLU activation function. (a) Keeping bothpositive and negative activations provides a baseline feed-forward model termed concatenated ReLU(CReLU). (b) Our spherical sparse coding layer has a similar structure but with an extra bias andnormalization step. Our proposed energy-based model uses (c) energy-based spherical sparse coding(EB-SSC) blocks that produces sparse activations which are not only positive and negative, but areclass-specific. These blocks can be stacked to build deeper architectures.
Figure 2: Comparing the behavior of asymmetric shrinkage for different settings of β+ and β-.
Figure 3: Comparing the effects of unrolling a 2-block version of our energy-based model. (Bestviewed in color.)In Fig. 3, we compare the performance between models that were unrolled zero to four times. Wesee that there is a difference in performance based on how many sweeps of the variables are made.
Figure 4: The reconstruction of an airplane image from different levels of the network (rows) acrossdifferent hypothesized class labels (columns). The first column is pure reconstruction, i.e., unbiasedby a hypothesized class label, the remaining columns show reconstructions of the learned class biasat each layer for one of ten possible CIFAR-10 class labels. (Best viewed in color.)The first column in Fig. 4 visualizes reconstructions of a given input image based on activationsfrom different layers of the model by convolution transpose. In this case we put in zeros for classbiases (i.e., no top-down) and are able to recover high fidelity reconstructions of the input. In theremaining columns, we use the same deconvolution pass to construct input space representations ofthe learned classifier biases. At low levels of the feature hierarchy, these biases are spatially smoothsince the receptive fields are small and there is little spatial invariance capture in the activations. Athigher levels these class-conditional bias fields become more tightly localized.
Figure 5: Visualizing the reconstruction of different input images (rows) for each of 10 differentclass hypotheses (cols) from the 2nd and 5th block activations for a model trained on MNIST digitclassification.
