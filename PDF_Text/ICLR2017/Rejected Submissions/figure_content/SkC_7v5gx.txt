Figure 1: Connection tensors of depth multiplier (left) and sparse random (right) approaches fornl-1 = 5 and nl = 10. Yellow denotes active connections. For both approaches, the connectionpattern is the same across spatial dimension and fixed before training. However, in the sparse randomapproach, each output channel is connected to a (possibly) different subset of input channels, andvice versa.
Figure 2: Comparison of accuracy (averaged over 5 rounds) vs. Number of Parameters/Number ofmultiply-adds between dense and sparse convolutions on MNIST dataset. Note that though sparseconvolution result in better parameter trade-off curve, the multiply-add curve shows the oppositepattern.
Figure 3: Comparison of accuracy (averaged over 2 rounds) vs. Number of parameters/Number ofmultiply-adds between dense and sparse convolutions on CIFAR-10 dataset.
Figure 4: Inception V3: Comparison of Precision@1 vs. Number of Parameters/Number ofmultiply-adds between dense and sparse convolutions on ImageNet/Inception-V3. The full networkcorresponds to the right-most point of the curve.
Figure 5: VGG 16: Preliminary Quantitative Results. Comparison OfPrecision@1 vs. Number ofparameters/Number of multiply-adds between dense and sparse convolutions on ImageNet/VGG-16n. The full network corresponds to the right-most point of the curve. Original VGG-16 as de-scribed in Simonyan & Zisserman (2015) (blue star) and the same model trained by us from scratch(red cross) are also shown.
Figure 6: Incremental Training Of Inception V3: We show Precision@1 during the training process,where the networks densify over time. The saturation points show where the networks actually reachtheir full density.
