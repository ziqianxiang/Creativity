Figure 1: Pretrained sequence to sequence model. The red parameters are the encoder and the blueparameters are the decoder. All parameters in a shaded box are pretrained, either from the sourceside (light red) or target side (light blue) language model. Otherwise, they are randomly initialized.
Figure 2: Two improvements to the baseline model: (a) residual connection, and (b) attention overmultiple layers.
Figure 3: English→German ablation study measuring the difference in validation BLEU betweenvarious ablations and the full model. More negative is worse. The full model uses LMs trained withmonolingual data to initialize the encoder and decoder, plus the language modeling objective.
Figure 4: Validation performance of pretraining vs. no pretraining when trained on a subset of theentire labeled dataset for English→German translation.
Figure 5: Summarization ablation study measuring the difference in validation ROUGE betweenvarious ablations and the full model. More negative is worse. The full model uses LMs trained withunlabeled data to initialize the encoder and decoder, plus the language modeling objective.
