Figure 1: Does initialization affect sample’s importance? Sample importance is preserved be-tween initializations of the network. For each epoch, and a pair of initializations, we computedSpearman Correlation of samples’ importance. Across all epochs, the correlation is of greater than0.9 in both MNIST and CIFAR-10. Early epochs show higher consistency between ranks of sampleimportance across different initializations.
Figure 2: Which classes and at which stage shape the network’s layer’s parameters? Parametersof different layers are learned at different times. Parameters in Output layers are learned mostlyduring the early training stage. In the lower layers, parameters are learned predominantly during themiddle and late training stage. All classes do not contribute equally to training of the model.
Figure 3: Are all data samples equally important for all layers? The top 20% most importantsamples contributes to the majority of parameter learning, especially in lower layers. “L1” to “L3”stands for Layer 1 to Layer 3. “Out” stands for output layer.
Figure 4: Is Sample Importance correlated with Negative log-likelihood of a sample? Sampleimportance is positively correlated with negative log-likelihood. As training goes on, their corre-lation becomes higher. However, there remain many samples with high NLL and low SI, and viceversa. Left column: correlation between sample importance and negative log likelihood for all sam-ples across epochs. Right column: scatter plot for NLL in the last epoch and all epoch sampleimportance for each sample.
Figure 5: When and where does an MNIST sample make the biggest impact? For “easy”samples, their biggest impact is on output layers and during the early training stage. As sample’sdifficulty increases (medium and hard), the biggest impact moves to lower layers and in the latetraining stage. Each row is a sample cluster. In each row, from left to right: example images inthe cluster; average sample importance and layer-wise decomposition across epochs; A boxplot ofaverage training negative log likelihood across epochs.
Figure 6: When and where does a CIFAR-10 sample make the biggest impact? For “easy” sam-ples, their biggest impact is on the first layer during the early training stage. As samples’s difficultyincreases (medium and hard), the biggest impact moves to lower layers and to late training stage.
Figure 8: Do parameters converge differently under different batch construction? In MNIST,the converging path for all batch constructions are very similar. In CIFAR-10, batch constructionwith mixed easiness (Rand, NLM, SIM) has a very different converging path with all other methods.
Figure 7: Does organizing batches by “easiness” affect training? When batches are constructedwith homogeneous easiness, the training performance become worse. Batches with mixed easinesshave lower test error. The solid color line represents the mean over 5 runs. The error bar indicatesthe standard error over 5 runs.
Figure 9: The training and test error on MNIST (first row) and CIFAR-10 (second row). The leftcolumn showed the average class-specific negative log likelihood.
Figure 10: Histogram of total sample importance.
