Figure 1: The NKLM model. The inputconsisting ofa word (either wto-1 or wtv-1)and a fact (at-1) goes into LSTM. TheLSTM’s output ht together with the knowl-edge context e generates the fact key kt .
Figure 2: This is a heatmap of an example sentence generated by the NKLM having a warmup “RoryCalhoun ( august 8 , 1922 april 28”. The first row shows the probability of knowledge-copy switch(Equation 5 in Section 3.1). The bottom heat map shows the state of the topic-memory at each timestep (Equation 2 in Section 3.1). In particular, this topic has 8 facts and an additional <NaF> fact.
Figure 3: This is an example sentence generated by the NKLM having a warmup “Louise Allbritton (3july <unk>february 1979 ) was”. We see that the model correctly retrieves and outputs the profession(“Actor”), place of birth (“Oklahoma”), and spouse (“Charles Collingwood”) facts. However, themodel makes a mistake by retrieving the place of birth fact in a place where the place of death fact issupposed to be used. This is probably because the place of death fact is missing in this topic memoryand then the model searches for a fact about location, which is somewhat encoded in the place ofbirth fact. In addition, Louise Allbritton was a woman, but the model generates a male profession“Actor” and male pronoun “he”. The “Actor” is generated because there is no “Actress” representationin Freebase.
