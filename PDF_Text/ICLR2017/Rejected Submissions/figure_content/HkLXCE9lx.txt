Figure 1: Procedure of agent-environment interactionThis process of interaction between an agent and the environment is illustrated in Figure 1. Here,each trial happens to consist of two episodes, hence n = 2. For each trial, a separate MDP isdrawn from ρM , and for each episode, a fresh so is drawn from the initial state distribution specificto the corresponding MDP. Upon receiving an action at produced by the agent, the environmentcomputes reward rt, steps forward, and computes the next state st+1. If the episode has terminated,it sets termination flag dt to 1, which otherwise defaults to 0. Together, the next state st+1, action2Under review as a conference paper at ICLR 2017at, reward rt, and termination flag dt, are concatenated to form the input to the policy1, which,conditioned on the hidden state ht+1, generates the next hidden state ht+2 and action at+1. At theend of an episode, the hidden state of the policy is preserved to the next episode, but not preservedbetween trials.
Figure 2: RL2 learning curves for multi-armed bandits. Performance is normalized such that Gittinsindex scores 1, and random policy scores 0.
Figure 3: RL2 learning curves for tabular MDPs. Performance is normalized such that OPSRLscores 1, and random policy scores 0.
Figure 4: Visual navigation. The target block is shown in red, and occupies an entire grid in themaze layout.
Figure 5: RL2 learning curves for visual navigation. Each curve shows a different random initial-ization of the RNN weights (by using a different random seed). Performance varies greatly acrossdifferent initializations.
Figure 6: Visualization of the agent’s behavior. In each scenario, the agent starts at the center of theblue block, and the goal is to reach anywhere in the red block.
