Figure 1: Depiction of the attack scenario. The VAE is used as a compression scheme to transmita latent representation of the image from the sender (left) to the receiver (right). The attacker con-vinces the sender to compress a particular image into its latent vector, which is sent to the receiver,where the decoder reconstructs the latent vector into some other image chosen by the attacker.
Figure 2: Results for the L2 optimization latent attack (see Section 4.3) on the VAE-GAN, targetinga specific image from the class 0. Shown are the first 12 non-zero images from the test SVHN dataset. The columns are, in order: the original image, the reconstruction of the original image, theadversarial example, the predicted class of the adversarial example, the reconstruction of the adver-sarial example, the predicted class of the reconstructed adversarial example, the reconstruction of thereconstructed adversarial example (see Section 4.5), and the predicted class of that reconstruction.
Figure 3: The VAE-GAN classifier architecture used to generate classifier-based adversarial exam-ples on the VAE-GAN. The VAE-GAN in the dashed box is the target network and is frozen whiletraining the classifier. The path X → fenc → Z → 九屋$ → y is used to generate adversarialexamples in z, which can then be reconstructed by fdec .
Figure 4: Results for the L2 optimization latent attack on the VAE-GAN, targeting the mean latentvector for 0. Shown are the first 12 non-zero images from the test MNIST data set. The columnsare, in order: the original image, the reconstruction of the original image, the adversarial example,the predicted class of the adversarial example, the reconstruction of the adversarial example, thepredicted class of the reconstructed adversarial example, the reconstruction of the reconstructedadversarial example (see Section 4.5), and the predicted class of that reconstruction.
Figure 5: Left: representative adversarial examples with a target class of 0 on the first 100 non-zero images from the MNIST validation set. These were produced using the L2 optimization latentattack (Section 4.3). Middle: VAE-GAN reconstructions from adversarial examples produced usingthe L2 optimization classifier attack on the same set of 100 validation images (those adversariesare not shown, but are qualitatively similiar, see Section 4.1). Right: VAE-GAN reconstructionsfrom the adversarial examples in the left column. Many of the classifier adversarial examples fail toreconstruct as zeros, whereas almost every adversarial example from the latent attack reconstructsas zero.
Figure 6: Left: VAE-GAN reconstructions of adversarial examples generated using the L2 optimiza-tion LVAE attack (single image target). Right: VAE-GAN reconstructions of adversarial examplesgenerated using the L2 optimization latent attack (single image target). Approximately 85 out of100 images are convincing zeros for the L2 latent attack, whereas only about 5 out of 100 could bemistaken for zeros with the LVAE attack.
Figure 7: Summary of different attacks on CelebA dataset: reconstructions of original images (top),reconstructions of adversarial examples generated using the latent attack (middle) and LVAE attack(bottom). Target reconstruction is shown on the right. Full results are in the Appendix.
Figure 8: Variational autoencoder architecture.
Figure 9: Original Inputs and Reconstructions: The first 100 images from the validation setreconstructed by the VAE (left) and the VAE-GAN (right).
Figure 10: Untargeted FGS LVAE Attack: VAE reconstructions (left) and VAE-GAN reconstruc-tions (right). Note the difference in reconstructions compared to Figure 9. Careful visual inspectionreveals that none of the VAE reconstructions change class, and only two of the VAE-GAN recon-structions change class (a 6 to a 0 in the next-to-last row, and a 9 to a 4 in the last row). CombiningFGS with LVAE does not seem to give an effective attack.
Figure 11: L2 Optimization Classifier Attack: Reconstructions of the first 100 adversarial exam-ples targeting 4, demonstrating why the AStarget metric is 0 for all source digits.
Figure 12: Untargeted FGS Classifer Attack: Adversarial examples (left) and their reconstruc-tions by the generative model (right) for the first 100 images from the MNIST validation set. Topresults are for VAE, while bottom results are for VAE-GAN. Note the difference in quality of thereconstructed adversarial examples.
Figure 13: Original images with random noise added (top) and their reconstructions by VAE (bottomleft) and VAE-GAN (bottom right). The magnitude of the random noise is the same as for thegenerated adversarial noise shown in Figure 12. Random noise does not cause the reconstructedimages to change in a significant way.
Figure 14:	L2 Optimization Latent Attack (mean latent vector targets): VAE-GAN reconstruc-tions of adversarial examples with target classes from 1 through 9. Original examples which alreadybelong to the target class are excluded.
Figure 15:	L2 Optimization Latent Attack (single latent vector target): VAE-GAN reconstruc-tions of adversarial examples generated using the latent attack with target classes 0 and 7 using tworandom targets in latent space per target class. Original examples which already belong to the targetclass are excluded. The stylistic differences in the reconstructions are clearly visible.
Figure 16: L2 Optimization Latent Attack (single latent vector target): t-SNE plot of the latentspace, with the addition of green circles representing the adversarial examples for target class 0. Inthis plot, it appears that the adversarial examples cluster around 6 (yellow) and 0 (red).
Figure 17: Original Inputs and Reconstructions: The first 100 images from the SVHN validationset (left) reconstructed by VAE-GAN (right).
Figure 18: L2 Optimization Latent Attack (single latent vector target): Nearest neighbors inlatent space for generated adversarial examples (target class 0) on the first 100 images from theMNIST (left) and SVHN (right) validation sets.
Figure 19: Original images in the CelebA dataset (left) and their VAE-GAN reconstructions (right).
Figure 20: L2 Optimization Latent Attack on CelebA Dataset (single latent vector target):Adversarial examples generated for 100 images from the CelebA dataset (left) and their VAE-GANreconstructions (right).
Figure 21: L2 Optimization LVAE Attack on CelebA Dataset (single image target): Adversarialexamples generated for 100 images from the CelebA dataset (left) and their VAE-GAN reconstruc-tions (right).
Figure 22: Effect of sampling on adversarial reconstructions. Columns in order: original image,reconstruction of the original image (no sampling, just the mean), reconstruction of the originalimage (1 sample), reconstruction of the original image (12 samples), reconstruction of the originalimage (50 samples), adversarial example (latent attack), reconstruction of the adversarial example(no sampling, just the mean), reconstruction of the adversarial example (1 sample), reconstructionof the adversarial example (12 samples), reconstruction of the adversarial example (50 samples).
