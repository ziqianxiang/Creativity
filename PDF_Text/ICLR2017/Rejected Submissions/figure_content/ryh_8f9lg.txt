Figure 1: Difference between the supervised and classless association tasks. The classless associationis more challenging that the supervised association because the model requires to learn to discriminatethe semantic concept without labels. In addition, both classifiers need to agree on the same codingscheme for each semantic concept. In contrast, the mentioned information is already known in thesupervised association scenario.
Figure 2: The proposed training rule applied to a single MLP. E-steps generates a set of pseudo-classescι,...,cm for each output in the mini-batch of size m, and a probability approximation Z of theoutput vectors in the mini-batch. M-step updates the MLP weights given the pseudo-classes and theweighting vector γ giving the target statistical distribution φ.
Figure 3: Overview of the presented model for classless association of two input samples thatrepresent the same unknown classes. The association relies on matching the network output and astatistical distribution. Also, it can be observed that our model uses the pseudo-classes obtained byMLP(1) as targets of MLP (2), and vice versa.
Figure 4: Example of the presented model during classless training. In this example, there are tenpseudo-classes represented by each row of MLP (1) and MLP(2). Note that the output classificationare randomly selected (not cherry picking). Initially, the pseudo-classes are assigned randomly to allinput pair samples, which holds a uniform distribution (first row). Then, the classless associationmodel slowly start learning the features and grouping similar input samples. Afterwards, the outputclassification of both MLPs slowly agrees during training, and the association matrix shows therelation between the occurrences of the pseudo-classes.
Figure 5: Example of the best and worst results among all folds and datasets. It can be observedour model is able to learn to discriminate each digit (first row). However, the presented model has alimitation that two or more digits are assigned to the same pseudo-class (last row of MLP (1) andMLP(2)).
Figure 1: Example of the classless training using Inverted MNIST dataset.
Figure 2: Example of the classless training using Random Rotated MNIST dataset.
