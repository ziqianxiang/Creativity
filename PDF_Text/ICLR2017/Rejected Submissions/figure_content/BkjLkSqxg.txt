Figure 1: LipNet architecture. A sequence ofT frames is used as input, and is processed by 3 layersof STCNN, each followed by a spatial max-pooling layer. The features extracted are processed by2 Bi-GRUs; each time-step of the GRU output is processed by a linear layer and a softmax. Thisend-to-end model is trained with CTC.
Figure 2: Saliency maps for the words (a) please and (b) lay, produced by backpropagation to theinput, showing the places where LipNet has learned to attend. The pictured transcription is given bygreedy CTC decoding. CTC blanks are denoted by '1’.
Figure 3: Intra-viseme and inter-viseme confusion matrices, dePicting the three categories with themost confusions, as well as the confusions between viseme clusters. Colours are row-normalised toemPhasise the errors.
Figure 4: LipNet’s full phoneme confusion matrix.
