Figure 1: Slot car racing - the agenthas learn how to driVe any of the carsas far as Possible (left), based on its rawobserVations (right).
Figure 2: Overview of the gated network for state representation learning for multiple tasks.
Figure 3: Reinforcement learning curves (mean and standard error) for different state representationsfor the two-slot car scenarios. Left: static visual cue. Right: dynamic visual cue.
Figure 4: State representation learned pertask (different markers) and per gate unit(different colors)will explain this phenomenon below. To conclude, MT-LRP allows to learn as good or better poli-cies than the baselines in all slot-car scenarios.
Figure 5: Reinforcement learning performance inthe three-slot car scenario with static visual cue.
Figure 6: Ï† learned by LRP (M = 2) for the two-car dynamic visual cue tasks. Row corresponds tostate dimension, column to RGB color channel.
Figure 7: Task coherence: Average re-ward per episode (8000 samples).
