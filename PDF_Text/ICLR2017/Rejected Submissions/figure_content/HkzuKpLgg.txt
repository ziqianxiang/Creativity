Figure 1:	Illustrations of various methods to accelerate the training. Black blocks stands for computa-tions, and white blocks stands for communications. CUDNN reduces the computation cost, while wereduce the communication cost.
Figure 2:	The data flow of broadcast, reduce and allreduce on 3 GPUs.
Figure 3: The performance of different collective algorithms at different message sizes on 4 K40m.
Figure 4: The scalability experiment: it measures performance variations with increasing GPUs.
Figure 5: The training losses in fixed iterations on 4 K40m. We set GoogLeNet lr = 0.01. AlexNetstarts at lr = 0.015, and set to 0.0015 after the average loss < 2. The solver is SGD + momentum, andthe dataset is ImageNet.
