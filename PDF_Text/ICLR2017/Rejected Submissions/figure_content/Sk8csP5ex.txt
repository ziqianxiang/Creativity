Figure 1: (a) A histogram of r(β), r = 1..p, for β = 0.1 and p = 100 . (b) Same for β = 0.5(c) Same for β = 2. (d) Values (y-axis) of the batch normalization parameters λl (x-axis) for10 layers ResNet trained to discriminate between 50 multivariate Gaussians (see Appendix C formore details). Higher plot lines indicate later stages of training. (e) The norm of the weights of aresidual network, which does not employ batch normalization, as a function of the iteration. (f) Theasymptotic of the mean number of critical points of a finite index as a function of β .
Figure 2: (a,c) The Norm of the convolutional layers once the factors of the subsequent BatchNormalization layers are absorbed, shown for CIFAR-10 and CIFAR-100 respectively. Each graphis a different epoch, see legend. Waving is due to the interleaving architecture of the convolutionallayers. (b,d) Respectively for CIFAR-10 and CIFAR-100, the mean of the norm of the convolutionallayers’ weights per epoch.
Figure 3: The norms of the multiplicative Batch Normalization coefficient vectors. (a,c) The Normof the coefficients, shown for CIFAR-10 and CIFAR-100 respectively. Each graph is a differentepoch (see legend). Since there is no monotonic increase between the epochs in this graph, it isharder to interpret. (b,d) Respectively for CIFAR-10 and CIFAR-100, the mean of the norm of themultiplicative factors per epoch.
