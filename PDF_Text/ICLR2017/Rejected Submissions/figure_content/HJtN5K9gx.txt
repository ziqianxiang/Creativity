Figure 1: Variation along (top) light-ing and (bottom) identity axes.
Figure 2: Example graphical model and its ex-pression in our framework. Further details inthe Appendix.
Figure 3: (left) Generative and(right) recognition model withdigit d and style n.
Figure 4: (a) Visual analogies for the MNIST data, with inferred style latent variable fixed andthe label varied. (b) Exploration in “style” space for a 2D latent gaussian random variable. Visualanalogies for the SVHN data when (c) fully supervised, and (d) supervised with just 100 labels/digit.
Figure 5: (Top) Classification error graphs over different labelled set (per class) sizes and supervisionrates for MNIST (left) and SVHN (right). Note the steep drop in error rate with just a handful oflabels per class (l), seen just a few times (r). (Bottom) Classification error rates for different (per-class) labelled-set sizes l over different runs.
Figure 7: (Top) Exploring the generative capacity of the model. Column 1: input image. Col-umn 2: reconstruction. Columns 3-7: reconstructions with fixed (inferred) lighting and varyingidentities. (Bottom) Classification and regression error rates for the identity and lighting latent vari-ables, fully-supervised, and semi-supervised with 20 distinct labelled example per variation axis (60total). Classification is a direct 1-out-of-38 choice, whereas for the comparison, error is a nearest-neighbour loss based on the inferred reflectance. Regression loss for lighting is measured as cosineangle distance. Results for Jampani et al. (2015) are estimated from plot asymptotes.
Figure 6: (Top) Generative and (Bottom)recognition model with identity i, light-ing l, reflectance r, and shading s.
Figure 8: Generative (l) and recognition (m) model with digit d, style n, canvas c, and count K.
