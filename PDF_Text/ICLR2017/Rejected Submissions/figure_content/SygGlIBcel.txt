Figure 1: CWE Model architecturehiWoutWhiddensCiCi = (wi : wi-1 : wi-2)â€¢	A classic NLM using word-level embeddings only, that We will note WE, which uses |V|*de parameters.
Figure 4: Model perplexity measured on the development set during training. The context size is3 words. Figure 3 shows models based on character-level word representations, with and withoutcomplete padding. Models are trained on the same data than Figure 2 but on smaller epochs (250Kn-grams).
