Figure 1: The loss surface when only the frequency ν = 10 of a sine needs to be learned. One of thethree sincs is centered in 0, the other two w = ±10.
Figure 2: The loss surface as a function of the network parameters when trying to learn g(x) =1 sin(ν x + 0). Cold colors are smaller values. The local minima in the ripples generated by the sincsare clearly visible.
Figure 3: The loss surface when only the frequency of the target sinusoid needs to be learned, onlya set of non-uniformly distributed samples is available at training time, and for a low frequency ν ofthe target function. Note that the central local minimum has disappeared.
Figure 4: Sin(X) and tanh(x) are very similar for —π∕2 < X < π∕2. The network might endup using only this part of the sine, therefore treating it as a monotonic function and ignoring itsperiodicity.
Figure 5: Accuracy curves of the ENC-DEC LSTM and RNN using sine or tanh. The number ofdigits for each sequence is sampled uniformly in {1, ..., D}. For uniform sampling of the addendsin {0, ..., 10D - 1}——which prevents small addends to appear very often——the experiments are in theappendix.
Figure 6: Accuracy and loss curves of the ENC-DEC RNN using sine or tanh, for the task sum with8 or 16 digits per addend. The digits are sampled uniformly.
Figure 7: Accuracy and loss curves of the ENC-DEC RNN using sine or tanh, for the task dif with8 or 16 digits per addend. The digits are sampled uniformly.
Figure 8: Accuracy curves of the ENC-DEC RNN using sine or tanh, for the tasks sum and dif with8 digits per addend. The digits are sampled uniformly.
Figure 9: Accuracy and loss curves of the ENC-DEC RNN using sine or tanh, for the tasks sum anddif. D starts from 8 and increases by 2 every 1000 iterations until it reaches 16 digits per addend atiteration 4000.
