Figure 1: The modular approach to neural networks involves feeding data through one or more pre-existing neural networks as well as a new network, the module. The existing networks have theirweights locked, so they will not be altered by the training process. Only the module weights aretrained. The end result is a representation that adds a new representation to an existing representationwithout losing any information from the original network.
Figure 2:	Modular networks do not simply need to be two models in parallel. Here, we presentthe stitched module approach. We insert a small neural network between each layer of the originalnetwork. This way, the modules explicitly receive information about the representations at eachlayer of the pre-trained network.
Figure 3:	After training a vanilla CNN on MNIST, images that maximally stimulate each filter areshown on the bottom rows. Images that maximally stimulate the auxiliary module network, trainedon the same data to supplement the original network, are shown on the top.
Figure 4:	By explicitly preserving the original representation learned on pre-trained net, the moduleis able to learn more robust features using fewer examples than naive fine-tuning.
Figure 5: Comparison of fine tuning vs the stitched module approach. TFT stands for ‘TraditionalFine Tuning.’ and the number of layers fine-tuned is indicated. Notice that our modular approachoutperforms fine-tuning for all amounts of training data. The modular approach’s benefit over fine-tuning increases as the amount of available training data decreases.
Figure 6: Network architecture used for Stanford Cars fine-tuned model.
Figure 7:	Network architecture used for Stanford Cars module model. Note, the ResNet used isidentical to the one describe in He et al. (2015).
Figure 8:	Diagram of architecture for our modular recurrent text classification network. Dimension-ality for embedding layers and number of units for all other layers are given in boxes denoting thoselayers.
