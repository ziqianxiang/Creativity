Figure 1: A visualization of the different variational distributions on a simple neural network.
Figure 2: An illustration of sampling network weights using the different variational distributions.
Figure 3: Examples of MNIST images with added Gaussian noise with varying standard deviations.
Figure 4: The MNIST test classification error, entropy, and calibration of the predictions of the fullyconnected networks: NN, NN+L2, Bernoulli DropConnect (BDC) with and without Monte-Carlo(MC) sampling, Gaussian DropConnect (GDC) with and without MC sampling, Bernoulli Dropout(BDO) with and without MC sampling, Gaussian Dropout with and without MC sampling, andspike-and-slab Dropout (SSD) with and without MC sampling.
Figure 5: The calibration curves for the MNIST test set with and without Gaussian noise of thesoftmax outputs of the fully connected networks: NN, NN+L2, Bernoulli DropConnect (BDC) withand without Monte-Carlo (MC) sampling, Gaussian DropConnect (GDC) with and without MCsampling, Bernoulli Dropout (BDO) with and without MC sampling, Gaussian Dropout with andwithout MC sampling, and spike-and-slab Dropout (SSD) with and without MC sampling.
Figure 6: The MNIST test classification error, entropy, and calibration of the predictions of theconvolutional networks: CNN, CNN+L2, Bernoulli DropConnect (BDC) with and without Monte-Carlo (MC) sampling, Gaussian DropConnect (GDC) with and without MC sampling, BernoulliDropout (BDO) with and without MC sampling, Gaussian Dropout with and without MC sampling,and spike-and-slab Dropout (SSD) with and without MC sampling.
Figure 7: The calibration curves for the MNIST test set with and without Gaussian noise of thesoftmax outputs of the convolutional networks: CNN, CNN+L2, Bernoulli DropConnect (BDC)with and without Monte-Carlo (MC) sampling, Gaussian DropConnect (GDC) with and withoutMC sampling, Bernoulli Dropout (BDO) with and without MC sampling, Gaussian Dropout withand without MC sampling, and spike-and-slab Dropout (SSD) with and without MC sampling.
Figure 8: The CIFAR-10 test classification error, entropy, and calibration of the predictions of theconvolutional neural networks: CNN, CNN+L2, Bernoulli DropConnect (BDC) with and with-out Monte-Carlo (MC) sampling, Gaussian DropConnect (GDC) with and without MC sampling,Bernoulli Dropout (BDO) with and without MC sampling, Gaussian Dropout with and without MCsampling, and spike-and-slab Dropout (SSD) with and without MC sampling.
Figure 9: The calibration curves for the CIFAR-10 test set with and without Gaussian noise ofthe softmax outputs of the convolutional neural networks: CNN, CNN+L2, Bernoulli DropConnect(BDC) with and without Monte-Carlo (MC) sampling, Gaussian DropConnect (GDC) with andwithout MC sampling, Bernoulli Dropout (BDO) with and without MC sampling, Gaussian Dropoutwith and without MC sampling, and spike-and-slab Dropout (SSD) with and without MC sampling.
