Figure 1: A visualization of RASOR, where the question is “What are the stators attached to?” and the passageis “. . . fixed to the turbine . . . ”. The model constructs question-focused passage word embeddings by concate-nating (1) the original passage word embedding, (2) a passage-aligned representation of the question, and (3)a passage-independent representation of the question shared across all passage words. We use a BiLSTM overthese concatenated embeddings to efficiently recover embedding representations of all possible spans, whichare then scored by the final layer of the model.
Figure 2:	F1 and Exact Match accuracy of RaSoR and the endpoint predictor over different predictions lengths,along with the distribution of both models’ prediction lengths and the gold answer lengths.
Figure 3:	Attention masks from RaSoR. Top predictions are ‘Egyptians’, ‘Egyptians against the British’, and‘British’ in the first example and ‘unjust laws’, ‘what they deem to be unjust laws’, and ‘laws’ in the second.
