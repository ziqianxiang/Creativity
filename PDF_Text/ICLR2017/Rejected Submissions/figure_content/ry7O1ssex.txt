Figure 1: Illustration of VGAN with variational contrastive divergence. On the left panel, energiesof real data X and generated data X are computed, with the generator shown on the right. For thegenerator on the right panel, each X ã€œPdata (x) is passed through an encoder to obtain h, which isthen passed through a decoder to achieve a reconstruction X. h is then mixed with a noise vector Zof the same dimensionality by a randomly generated binary mask vector m to obtain h followingh = m * z + (1 - m) * h. h is then passed through the same decoder to obtain the generated sampleXX.
Figure 2: Samples from the GAN (left), and generations of VGAN (right), with the same architec-ture. The first row corresponds to updating the generator one step at each iteration, and the secondrow corresponds to updating the generator three steps at each iteration.
Figure 3: Visualization of x, X, and X for P = 0,0.01,1 on MNIST. The first to third row corre-sponds to rho = 0, 0.0N, N, respectively. The first to third column corresponds to samples from thevalidation set x, reconstructions of the samples X, and the generated samples X.
Figure 4: Visualization of x, X, and X for P = 0.01 on SVHN and CIFAR10. The first to thirdcolumn corresponds to samples from the validation set x, reconstructions of the samples X, and thegenerated samples X.
Figure 5: Simulating a Markov Chain with pz (X |x). We show 30 and 28 images form the validationset for MNIST and SVHN in the first row of each panel, respectively, followed by 9 Gibbs samplingsteps. Note the smooth transition of digit types, shapes, and/or colors.
