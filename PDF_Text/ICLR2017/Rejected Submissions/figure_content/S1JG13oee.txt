Figure 2: Comparative results: estimated density ratio values rθD (x) from the training data (red), theestimated density-ratio values rθD (x) from the generated distribution (green), generator losses takenin the D-step and G-step (blue). The top, second, and bottom rows show rθD (x) and the losses ofb-GAN with the Pearson divergence, KL divergence, modified KL divergence (relative density ratioestimation version, α = 0.2), and reversed KL divergence, respectively.
Figure 3: Density ratio value rθD (x) and generator losses of b-GAN when the last output function isa sigmoid function multiplied by 5.
Figure 4: Divergence differences between D-step and G-step: b-GAN with Pearson divergence (left),b-GAN with KL divergence (right) .
Figure 5: (Left) original images set, (middle) a set of images generated based on Pearson divergence,and (right) a set of images based on the KL divergence.
Figure 6: Pearson divergenceFigure 7: KL divergenceC Proof of propositions in Section 3C.1 Proof of Prop 3.1From Eq. 4, the following equation holds,Ex~p [f 0 (rθ (X))] - Ex~q [(f 0(rθ (X))rθ (X)- f (rθ (X)))] = -BDf (rllrθ ) + Eq hf (P(X)) i . (9)Using BDf (r∣∣rθ) ≥ 0 yields Eq. 7. We have BDf (r∣∣rθ) = 0 When r is equal to re. Thus, theequality holds if and only if r is equal to rθ .
Figure 7: KL divergenceC Proof of propositions in Section 3C.1 Proof of Prop 3.1From Eq. 4, the following equation holds,Ex~p [f 0 (rθ (X))] - Ex~q [(f 0(rθ (X))rθ (X)- f (rθ (X)))] = -BDf (rllrθ ) + Eq hf (P(X)) i . (9)Using BDf (r∣∣rθ) ≥ 0 yields Eq. 7. We have BDf (r∣∣rθ) = 0 When r is equal to re. Thus, theequality holds if and only if r is equal to rθ .
