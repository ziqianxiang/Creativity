Figure 1: Comparison of the conventional content-based attention model using inner product andthe proposed Gaussian attention model with the same mean but two different covariances.
Figure 2: The input to the system is a question in natural language. TWo entities Forward andBorussia-Dortmund are identified in the question and associated with point mass distributionscentered at the corresponding entity vectors. An LSTM encodes the input into a sequence of outputvectors of the same length. Then we take average of the output vectors weighted by attention pt,e foreach recognized entity e to predict the weight αr,e for relation r associated with entity e. We forma Gaussian attention over the entities for each entity e by convolving the corresponding point masswith the (pre-trained) Gaussian embeddings of the relations weighted by αr,e according to Eq. (6).
Figure 3:	Variance of each relation. Each row shows the diagonal values in the variance matrixassociated with a relation. Columns are permuted to reveal the block structure.
Figure 4:	TransGaussian entity embeddings. Crosses are the subjects and circles are the objects of arelation. Specifically, crosses are players in (a)-(e) and professional football clubs in (f).
