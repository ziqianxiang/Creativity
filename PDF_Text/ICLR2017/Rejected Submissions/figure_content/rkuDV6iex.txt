Figure 1: An example of learning curve of neural networkOne of the interesting empirical observation is that we often observe is that the incremental improve-ment of optimization methods decreases rapidly even in non-convex problems. This behavior hasbeen discussed as a “transient” phase followed by a “minimization” phase (Sutskever et al., 2013)2Under review as a conference paper at ICLR 2017where the former finds the neighborhood of a decent local minima and the latter finds the localminima within that neighborhood. The existence of these phases implies that if certain methods arebetter at different phases one could create novel methods that schedule when to apply each method.
Figure 2: Visualization of the loss surface atweights interpolated between two initial configu-rations and the final weight vectors learned usingSGD from these initializations.
Figure 3: Visualization of the loss surface atweights interpolated between the weights learnedby four different algorithms from the same ini-tialization.
Figure 4:	(a) Training accuracy and (b) test accuracy for each of the optimization methods. Colorscorrespond to different initializations.
Figure 5:	Loss function value near local minima found by multiple restarts of each algorithm.
Figure 6:	Observing the absolute size of basin for different local minimas found by different opti-mization methods.
Figure 7: Switching methods from one method to another method at epoch 50 and 100 and 150.
Figure 8: (Without batch normalization) Loss function with parameter interporlated between Initialto Final. (Bottom) Loss function with parameters interporlated between Final to Final. Each columnof the plots are from different initializations.
Figure 9: Training accuracy curveFigure 10: Interpolation between initial points to final points upto α = 2 in Equation 2.
Figure 10: Interpolation between initial points to final points upto α = 2 in Equation 2.
Figure 11:	NIN trained from different initializaitons.
Figure 12:	NIN - Learning curve when switching methods from SGD to ADAM and visa versa atepoch 50 and 100. Learning rate switched from SGD (ADAM) to ADAM (SGD) at (left) 0.001 (0.1)to 0.1 (0.001), (middle) 0.001 (0.1) to 0.05, (0.001), and (right) 0.001 (0.1) to 0.01 (0.001).
Figure 13: VGG - Switching methods from SGD to ADAM and ADAM to SGD at epoch 50 and100. Zoomed in version (Left). Distance between initial weights to weights at each epoch (Middle).
Figure 14: VGG - Switching methods from SGD to Adadelta and Adadelta to SGD at epoch 50 and100. Zoomed in version (Left). Distance between initial weights to weights at each epoch (Middle).
Figure 15: VGG - Switching methods from ADAM to Adadelta and Adadelta to ADAM at epoch50 and 100. Zoomed in version (Left). Distance between initial weights to weights at each epoch(Middle). The interpolation between different convergence parameters (Right). Each figure showsthe results of switching methods at different learning rate. We label the switch of methods in termsof ratio. For instance, S50-A50 as trained with SGD in the first 100 epoch and swithced to Adadeltafor the rest of the epoch.
