Figure 1: Comparison of the empirical distribution p(z) of the post-noise activations with ourproposed prior when using: (a) ReLU activations, for which we propose a log-uniform prior, and (b)Softplus activations, for which we propose a log-normal prior. In both cases, the empirical distributionapproximately follows the proposed prior. Both histograms where obtained from the last dropoutlayer of the All-CNN-32 network described in Table 2, trained on CIFAR-10.
Figure 2: For four different input samples, we show the plot of the total KL-divergence at each spatiallocation in the first three Information Dropout layers of All-CNN-96 (see Table 2) trained on ClutteredMNIST with different values of β. This measures how much information from each part of the imagethe Information Dropout layer is letting flow to the next layer. While for low value β informationabout the nuisances is still transmitted to the next layers, for higher value of β the InformationDropout layers drop the information as soon as the receptive field is big enough to recognize it asa nuisance. The resulting representation is therefore more robust to nuisances, and provides bettergeneralization performances. Unlike in classical dropout or Variational Dropout, the noise addedby Information Dropout is tailored to the specific sample, to the point that the KL-divergence aloneprovides enough information to localize the digit.
Figure 3: (a) Average classification error on MNIST over 3 runs of several dropout methods appliedto a fully connected network with three hidden layers and ReLU activations. Information dropoutoutperforms binary dropout, especially on smaller networks, possibly because dropout severelyreduces the already limited capacity of the network, while Information Dropout can adapt the amountof noise to the data and the size of the network. Information dropout also outperforms a dropout layerthat uses constant log-normal noise with the same variance, confirming the benefits of adaptive noise.
Figure 4: (a) A few samples from our Occluded CIFAR dataset. (b) Plot of the testing error on themain task (classifying the CIFAR image) and on the nuisance task (classifying the occluding MNISTdigit) as β varies. For both tasks, we use the same representation of the data trained for the main taskusing Information Dropout. For larger values of β the representation is increasingly more invariant tonuisances, making the nuisance classification task harder, but improving the performance on the maintask by preventing overfitting. For the nuisance task, we test using the learned noisy representation ofthe data, since we are interested specifically in the effects of the noise. For the main task, we showthe result both using the noisy representation (N), and the deterministic representation (D) obtainedby disabling the noise at testing time.
Figure 5: Classification error on CIFAR-10 for several dropout methods applied to the All-CNN-32network (Table 2) using ReLU activations and varying the number of filters used. While in Figure 3bwe used controlled setting to provide a fairer comparison, here we use the same settings suggestedby Springenberg et al. (2014). In particular we add a weight decay factor of 0.001, reduce the batchsize to 128, and drop the inputs with probability 0.2. The main effect of Information Dropout is todynamically reduce the information flow in the network. Since the same can be achieved by carefullytuning the dropout rate and/or the number of filters used, we expect binary dropout to performssimilarly on a finely tuned standard architecture, as is the case here when using all the filters.
Figure 6: Plots of (a) the total information transmitted through the two dropout layers of a All-CNN-32 network with Softplus activations trained on CIFAR and (b) the average quantity of informationtransmitted through each unit in the two layers. From (a) we see that the total quantity of informationtransmitted does not vary much with the number of filters and that, as expected, the second layertransmits less information than the first layer, since prior to it more nuisances have been disentangledand discarded. In (b) we see that when we decrease the number of filters, we force each single unit tolet more information flow (i.e. we apply less noise), and that the units in the top dropout layer containon average more information relevant to the task than the units in the bottom dropout layer.
