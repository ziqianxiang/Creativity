Figure 1: Illustration of the proposed method. For each convolutional or fully connected layer, weuse different bias/variance terms to perform batch normalization for the training domain and the testdomain. The domain specific normalization mitigates the domain shift issue.
Figure 2: t-SNE (Van der Maaten & Hinton, 2008) visualization of the mini-batch BN feature vectordistributions in both shallow and deep layers, across different datasets. Each point represents theBN statistics in one mini-batch. Red dots come from Bing domain, while the blue ones are fromCaltech-256 domain. The size of each mini-batch is 64.
Figure 3: Accuracy when varying the number of mini-batches used for calculating the statistics ofBN layers in A → W and B → C, respectively. For B → C, we only show the results of using lessthan 100 batches, since the results are very stable when adding more examples. The batch size is 64in this experiment. For even smaller number of examples, the performance may be not consistentand drop behind the baseline (e.g. 0.652 with 16 samples, 0.661 with 32 samples).
Figure 4: Accuracy when adapting with different BN blocks in B → C. x = 0 corresponds to theresult with non-adapt method, and 1, 2, 3a, 3b, 4a, 4b, 4c, 5a, 5b correspond to the nine differentblocks in Inception-BN network..
Figure 5: Remote sensing images in different domains.
Figure 6: Visual cloud detection results on GF1 dataset. White pixels in (b) and (c) represent thedetected cloud regions.
