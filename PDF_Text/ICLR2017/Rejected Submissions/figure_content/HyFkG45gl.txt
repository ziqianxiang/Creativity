Figure 1: Physics domain (a): We consider a two-dimensional space with a free falling particle.
Figure 2: The first step from word problem to dynamical system is accomplished via neural net-works. The second step from dynamical system to solution is achieved with a numerical integrator.
Figure 3: The word problem passes through two RNNs to be transformed into the dynamical systemform.
Figure 4: Example of input to labeler with expected output. A label is associated with each word,where O indicates other, or a word not needed for the dynamical system translation. Input text isshortened for the example.
Figure 5: Example of input to classifier with expected output. Symbol x1 refers to horizontal dis-placement and symbol x2 refers to vertical displacement.
Figure 6: Outputs from the labeler and the classifier feed into the numerical integrator, where thelabeler outputs form the dynamical system to integrate and the classifier outputs control the focusand output of the integrator.
Figure 7: Examples of generated problems that adhere to the grammarFigure 8: The generative model allows us to generate the input and output for the neural networkswithout requiring any manual annotation.
Figure 8: The generative model allows us to generate the input and output for the neural networkswithout requiring any manual annotation.
Figure 9: Training accuracy of labeler (left) and classifier (right)The training accuracy for the label, question, and overall reach 100% for all by the end of the firstepoch. The classifier also reaches 100% accuracy on the training set by the end of the first epoch.
Figure 10: Top left: labeler LSTM weight distributions. Top right: classifier LSTM weight distri-butions. Bottom left: labeler output weight distributions. Bottom right: classifier output weightdistributions.
Figure 11: Heat map for labeler weights from LSTM hidden layer to output layer.
Figure 12: Examples of incorrectly extracted questions from the labeler and the classifierâ€™s responseto them. In all three cases, the question is cut short. The classifier still makes the correct theclassification for the first case, but fails for the second and third cases.
Figure 13: Top: The embeddings from the labeler network for the top 100 most frequent wordsin the word problems. Bottom: The embeddings from the classifier network for all words in thequestions.
