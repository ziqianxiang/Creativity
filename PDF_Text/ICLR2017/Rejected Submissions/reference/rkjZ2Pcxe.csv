title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 The effects of adding noise during backpropagation training on a generalizationperformance,1996, Neural Computation
 Normalization propagation:A parametric technique for removing internal covariate shift in deep networks,2016, ICML
 Layer normalization,2016, arXiv preprintarXiv:1607
 Neural machine translation by jointlylearning to align and translate,2014, ICLR
 Weight uncertainty inneural networks,2015, ICML
 The tradeoffs of large scale learning,2008, In NIPS
 On the propertiesof neural machine translation: Encoder-decoder approaches,2014, arXiv preprint arXiv:1409
 Learning phrase representations using RNN encoder-decoderfor statistical machine translation,2014, In EMNLP
 Theloss surfaces of multilayer networks,2015, In AISTATS
 Large scale distributed deep networks,2012, In NIPS
 Natural neuralnetworks,2015, In NIPS
 Adaptive subgradient methods for online learning andstochastic optimization,2011, JMLR
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proc
 Practical variational inference for neural networks,2011, In NIPS
 Generating sequences with recurrent neural networks,2013, arXiv preprint arxiv:1308
 Neural Turing Machines,2014, arXiv preprintarXiv:1410
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, ICCV
 Stochastic neighbor embedding,2002, In NIPS
 Deepneural networks for acoustic modeling in speech recognition,2012, Signal Processing Magazine
 Long short-term memory,1997, Neural Computation
 Batch normalization: accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Neural GPUs learn algorithms,2016, In ICLR
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Optimization by simulated annealing,1983, Science
 ImageNet classification with deep convo-lutional neural networks,2012, In NIPS
 Neural random-access machines,2016, InICLR
 Batch normal-ized recurrent neural networks,2015, arXiv preprint arXiv:1510
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deeply-supervised nets,2015, AISTATS
 Rectifier nonlinearities improve neural net-work acoustic models,2013, ICML
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, arXiv preprint arXiv:1503
 Rectified linear units improve Restricted Boltzmann Machines,2010, InICML
 MCMC using Hamiltonian dynamics,2011, Handbook of Markov Chain Monte Carlo
 Neural Programmer: Inducing latent programswith gradient descent,2016, In ICLR
 On the difficulty of training recurrent neuralnetworks,2013, Proc
 Stochastic gradient riemannian langevin dynamics on the proba-bility simplex,2013, In NIPS
 Towards neural network-based reason-ing,2015, arXiv preprint arxiv:1508
 Weight normalization: A simple reparameterization to ac-celerate training of deep neural networks,2016, 2016
 A recurrent network that performs a context-sensitive prediction task,1996, In CogSci
 Random walks: Training very deep nonlinear feed-forward networks with smartinitialization,2014, CoRR
 On the importance of initializa-tion and momentum in deep learning,2013, In ICML
 Bayesian learning via stochastic gradient Langevin dynamics,2011, InICML
 Memory networks,2014, arXiv preprintarXiv:1410
 Depth-gated recurrentneural networks,2015, arXiv preprint arXiv:1508
 Adadelta: An adaptive learning rate method,2012, arXiv preprint arXiv:1212
