title,year,conference
 Multi-residual networks,2016, arXiv preprint arXiv:1609
 Understanding intermediate layers using linear classifierprobes,2016, arXiv preprint arXiv:1610
 Layer normalization,2016, arXiv preprintarXiv:1607
 XcePtion: Deep learning with depthwise separable convolution,2016, arXiv preprintarXiv:1610
 Design patterns: elements ofreusable object-oriented software,1995, Pearson Education India
 Deep pyramidal residual networks,2016, arXiv preprintarXiv:1610
 Deep residual learning for image recog-nition,2015, arXiv preprint arXiv:1512
 Identity mappings in deep residualnetworks,2016, arXiv preprint arXiv:1603
 Franken-stein: Learning deep face representations using small data,2016, arXiv preprint arXiv:1603
 Deep networks with stochas-tic depth,2016, arXiv preprint arXiv:1603
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 The unreasonable effectiveness of noisy data for fine-grainedrecognition,2015, arXiv preprint arXiv:1511
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Zoneout:Regularizing rnns by randomly preserving hidden activations,2016, arXiv preprint arXiv:1606
 Fractalnet: Ultra-deep neural net-works without residuals,2016, arXiv preprint arXiv:1605
 Streaming normalization: Towards simplerand more biologically-plausible normalizations for online and recurrent learning,2016, arXiv preprintarXiv:1610
 Competitive multi-scale convolution,2015, arXiv preprintarXiv:1511
 Network in network,2013, arXiv preprint arXiv:1312
 On the computational efficiency of trainingneural networks,2014, In Advances in Neural Information Processing Systems
 Convolutional residual memory networks,2016, arXiv preprintarXiv:1606
 Neural networks: tricks of the trade,2003, Springer
 Decon-structing the ladder network architecture,2015, arXiv preprint arXiv:1511
 Deep learning made easier by linear transformationsin perceptrons,2012, In AISTATS
 Semi-supervised learning with ladder networks,2015, In Advances in Neural Information Processing Systems
 Weight normalization: A simple reparameterization to ac-celerate training of deep neural networks,2016, arXiv preprint arXiv:1602
 Convolutional neural fabrics,2016, arXiv preprint arXiv:1606
 Boosting neural networks,2000, Neural Computation
 Weighted residuals for very deep networks,2016, arXiv preprintarXiv:1605
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Swapout: Learning an ensemble of deep archi-tectures,2016, arXiv preprint arXiv:1605
 Gradual dropin of layers to train very deepneural networks,2015, arXiv preprint arXiv:1511
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Training very deep networks,2015, InAdvances in neural information processing systems
 Understand-ing locally competitive networks,2014, arXiv preprint arXiv:1410
 Re-thinking the inception architecture for computer vision,2015, arXiv preprint arXiv:1512
 Residual networks are exponential ensemblesof relatively shallow networks,2016, arXiv preprint arXiv:1605
 Deeply-fused nets,2016, arXiv preprintarXiv:1605
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Residual networks ofresidual networks: Multilevel residual networks,2016, arXiv preprint arXiv:1608
