title,year,conference
 Efficient approaches for escaping higher order saddle points innon-convex optimization,2016, arXiv preprint arXiv:1602
 Theloss surfaces of multilayer networks,2015, In AISTATS
 Open problem: The landscape of theloss surfaces of multilayer networks,2015, In Proceedings of The 28th Conference on Learning Theory
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in neural information processing systems
 Escaping from saddle pointsonline stochasticgradient for tensor decomposition,2015, In Proceedings of The 28th Conference on Learning Theory
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Aistats
 Toeplitz and circulant matrices: A review,2006, now publishers inc
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Deep residual learning for image recog-nition,2015, arXiv preprint arXiv:1512
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE InternationalConference on Computer Vision
 Identity mappings in deep residualnetworks,2016, arXiv preprint arXiv:1603
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Deep learning without poor local minima,2016, arXiv preprint arXiv:1605
 Efficient methods in convex programming,2005, 2005
 On the saddle point problemfor non-convex optimization,2014, arXiv preprint arXiv:1405
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Training very deep networks,2015, InAdvances in neural information processing systems
 Highway networks,2015, arXivpreprintarXiv:1505
 Residual networks are exponential ensemblesof relatively shallow networks,2016, arXiv preprint arXiv:1605
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Residual networks ofresidual networks: Multilevel residual networks,2016, arXiv preprint arXiv:1608
 The dataset outputs are represented using one-hot encoding,1992, The networkwas trained using gradient descent
