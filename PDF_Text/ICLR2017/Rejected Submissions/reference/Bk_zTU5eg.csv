title,year,conference
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT
 Accelerating stochasticgradient descent via online learning to sample,2015, arXiv preprint arXiv:1506
 Sample size selection in opti-mization methods for machine learning,2012, Mathematical programming
 Distributed asynchronous optimization of convolutional neural net-works,2014, In INTERSPEECH
 Project adam:Building an efficient and scalable deep learning training system,2014, In 11th USENIX Symposiumon Operating Systems Design and Implementation (OSDI 14)
 Big batch sgd: Automated inferenceusing adaptive batch sizes,2016, arXiv preprint arXiv:1610
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Fast distributed coordinate descentfor non-strongly convex losses,2014, In 2014 IEEE International Workshop on Machine Learning forSignal Processing (MLSP)
 A dual coordinate descent method for large-scale linear svm,2008, In Proceedings of the 25thinternational conference on Machine learning
 Communication-efficient distributed dual coordinate ascent,2014, In Advancesin Neural Information Processing Systems
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 Semi-stochastic coordinate descent,2014, arXiv preprintarXiv:1412
 Scaling distributed machine learning with theparameter server,2014, In 11th USENIX Symposium on Operating Systems Design and Implementation(OSDI 14)
 Efficient mini-batch training forstochastic optimization,2014, In Proceedings of the 20th ACM SIGKDD international conference onKnowledge discovery and data mining
 Asynchronous parallel stochastic gradient fornonconvex optimization,2015, In Advances in Neural Information Processing Systems
 Asynchronousaccelerated stochastic gradient descent,2016, In Proceedings of the 25th international joint conferenceon Artificial Intelligence
 Coordinate descent with arbitrary sampling i: Algorithms and Com-plexity,2014, arXiv preprint arXiv:1412
 Iteration complexity of randomized block-coordinate descentmethods for minimizing a composite function,2014, Mathematical Programming
 On parallelizability of stochasticgradient descent for speech dnns,2014, In 2014 IEEE International Conference on Acoustics
 The general inefficiency of batch training for gradientdescent learning,2003, Neural Networks
