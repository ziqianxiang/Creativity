Figure 2: The raw RGB frame from the environment is the observation that is given as input to theagent, along with the last action and reward. This observation is shown for a sample of a maze from thenav_maze_all_random_02 level in Labyrinth. The agent must navigate this unseen maze and PiCk UP applesgiving +1 reward and reach the goal giving +10 reward, after which it will respawn. Top down views of samplesfrom this maze generator show the variety of mazes proCedurally Created. A video showing the agent playingLabyrinth levels Can be viewed at https://youtu.be/Uz-zGYrYEjAthat reCognise states that lead to high reward and value. An agent with a good representation ofrewarding states, will allow the learning of good value funCtions, and in turn should allow the easylearning of a poliCy.
Figure 3: An overview of performance averaged across all levels on Labyrinth (Top) and Atari (Bottom). Inthe ablated versions RP is reward prediction, VR is value function replay, and PC is pixel control, with theUNREAL agent being the combination of all. Left: The mean human-normalised performance over last 100episodes of the top-3 jobs at every point in training. In Labyrinth, we achieve an average of 87% human-normalised score, with every element of the agent improving upon the 54% human-normalised score of vanillaA3C. Prior. Duel Clip and Duel Clip are Dueling Networks with gradient clipped to 10 as reported in Wang et al.
Figure 4: A breakdown of the improvement over A3C due to our auxiliary tasks for each level on Labyrinth.
Figure 5: Comparison of various forms of self-supervised learning on random maze navigation. Adding aninput reconstruction loss to the objective leads to faster learning compared to an A3C baseline. Predictingchanges in the inputs works better than simple image reconstruction. Learning to control changes leads to thebest results.
Figure 6: Learning curves for three example Atari games. Semi-transparent lines are agents withdifferent seeds and hyperparameters, the bold line is a mean over population and dotted line is thebest agent (in terms of final performance).
Figure 7: Learning curves averaged across all hyperparameters (left), and the same curves for threetypes of agent plotted with standard error (right).
Figure 8: Human normalised performance for each hyperparameter setting with respect to the mainhyperparameters of A3C - learning rate and entropy cost.
Figure 9: Top-down renderings of each Labyrinth level. The nav_maze * _0{1, 2, 3} levels showone example maze layout. In the alLrandom case, a new maze was randomly generated at the startof each episode.
Figure 10: Example images from the agentâ€™s egocentric viewpoint for each Labyrinth level.
