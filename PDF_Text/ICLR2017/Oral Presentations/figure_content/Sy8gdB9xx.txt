Figure 1: Fitting random labels and random pixels on CIFAR10. (a) shows the training loss ofvarious experiment settings decaying with the training steps. (b) shows the relative convergencetime with different label corruption ratio. (c) shows the test error (also the generalization error sincetraining error is 0) under different label corruptions.
Figure 2: Effects of implicit regularizers on generalization performance. aug is data augmentation,wd is weight decay, BN is batch normalization. The shaded areas are the cumulative best test ac-curacy, as an indicator of potential performance gain of early stopping. (a) early stopping couldpotentially improve generalization when other regularizers are absent. (b) early stopping is not nec-essarily helpful on CIFAR10, but batch normalization stablize the training process and improvesgeneralization.
Figure 3: The small Inception model adapted for the CIFAR10 dataset. On the left we show theConv module, the Inception module and the Downsample module, which are used to construct theInception architecture on the right.
