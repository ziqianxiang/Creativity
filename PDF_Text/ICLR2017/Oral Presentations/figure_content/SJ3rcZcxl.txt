Figure 1: Illustrations of OpenAI Gym MuJoCo domains (Brockman et al., 2016; Duan et al., 2016):(a) Ant, (b) HalfCheetah, (c) Hopper, (d) Humanoid, (e) Reacher, (f) Swimmer, (g) Walker.
Figure 2: Average return over episodes in HalfCheetah-v1 during learning, exploring adaptive Q-Prop methods and different batch sizes. All variants of Q-Prop substantially outperform TRPO interms of sample efficiency. TR-c-QP, conservative Q-Prop with trust-region update performs moststably across different batch sizes.
Figure 3: Average return over episodes in HalfCheetah-v1 and Humanoid-v1 during learning, com-paring Q-Prop against other model-free algorithms. Q-Prop with vanilla policy gradient outperformsTRPO on HalfCheetah. Q-Prop significantly outperforms TRPO in convergence time on Humanoid.
