Figure 1: Illustration of the SR problem via a toy example. Two-dimensional ues. The AffGAN and AffDGHR data y = [y1 , y2] is drawn from a Swiss-roll distribution (in gray). achieves cross-entropy valuesDownsampling is modelled as X = y1 + y2. a) Given observation X = 0.5, close to the MAP solution con-valid SR solutions lie along the line y2 = 1 - yι (-). The red shading firming that they minimize theH[qo, Py]	'mse (X, Ay)MAP	3.15	-MSE	9.10	1.25 ∙ 10-2MAE	6.30	4.04 ∙ 10-2AffGAN	4.10	0.0SoftGAN	4.25	8.87 ∙ 10-2AffDG	3.81	0.0SoftDG	4.19	1.01 ∙ 10-1Table 1: Directly estimatedillustrates the magnitude of the posterior pY |X=0.5. Bayes-optimal estimates desired quantity. The MSE andunder MSE and MAE as well as the MAP estimate given X = 0.5 are marked MAE models performs worsewith labels. The MAP estimates for different values of X ∈ [-8, 8] are also since they do not minimizeshown (- •一). b) Trained model outputs for X ∈ [-8,8] and estimated gradi- the cross-entropy. Further theents from a denoising function trained on PY. Note the AffGAN(- ∙- ) and models using affine projectionsAffDG(- ♦- ) models fit the posterior mode well whereas the MSE (- ♦- ) (Aff) performs better than theand MAE (- ♦- ) model outputs generally fall in low probability regions. soft constrained models.
Figure 2: CelebA performance for MSE models during training. The distance between HR model output y andtrue HR image y using MSE in a) and SSIM in b). MSE in LR space between input x and down-sampled modeloutput Ay in c). The tuple in the legend indicate: ((F)ixed / (T)rainable affine projection, (T)rained / (R)andominitialised affine projections). The models using pre-trained affine projections (fixed:---, trainable:----)always performs better in all metrics compared to models using either random initialized affine projections(----)or no projection (------). Further, a fixed pre-trained affine projection ensures the best consistencybetween input and down-sampled output as seen in figure c). A (top) and A+ (bottom) kernels of the affineprojection are seen in d).
Figure 3: 4× SR of grass textures. Top row shows LR model input x, true HR image y and model outputsaccording to figure legend. Bottom row shows zoom in on except from the images in the top row. The AffGANimage is much sharper than the somewhat blurry AffMSE image. Note that both the AffDG and AffLL producesvery blurry results. The Affinit shows the output from an untrained affine projected model, i. e. the baselinesolution, illustrating the effect of the upsampling using A+ .
Figure 4: 4× SR of CelebA faces. Model input x, target y and model outputs using the affine projections (Aff)SSIM PSNR	'mse (x,Ay)MSE	0.90	26.30	8.0 ∙	10-	5AffMSE	0.91	26.53	1.6 ∙	10-	10SoftGAN	0.76	21.11	2.3 ∙	10-	3AffGAN	0.81	23.02	9.1 ∙	10-	10Table 2: PSNR, SSIM and MSEscores for the CelebA dataset. Interms of PSNR and SSIM in HRspace the MSE trained modelsachieves the best scores as ex-pected and the AffGAN performsbetter than the SoftGAN. Consid-ering 'MSE (x, Ay) the modelsaccording to figure legend. Both the AffGAN and SoftGAN produces clearly clearly show better consistencyshaper images than the blurry MSE outputs. We found that AffGAN outputs between input x and down sam-slightly sharper images compared to SoftGAN, however also with slightly pled model output Ay than mod-more high-frequency noise.	els not using the projection.
Figure 5: 4× SR from 32 × 32 to 128 × 128 using AffGAN on the ImageNET. AffGAN outputs (top row), trueHR images y (middle row), model input x (bottom row). Generally the AffGAN produces plausible outputswhich are however still easily distinguishable from true images. Interestingly the snake depicted in the thirdcolumn is super resolved into water which is obviously wrong but still a very plausible image considering theLR input image.
Figure 6: Illustration of samples from two non-overlapping perfectly distributions distributions in a,) withone-sided label smoothing in b) and instance noise in c). One side-label smoothing shifts the optimal deci-sion boundary but pY still covers areas with no support in qθ . Instance Noise broadens the support of bothdistributions without biasing the optimal discriminator.
Figure 7: PSNR and SSIM results for the AffDG and AffLL models. Note that the step-like behaviour of theAffDG model is due to change of the DAE model with continuously lower noise levelswith 64 filter map with kernel sizes of 5, except for the first layers which used 7. The original PixelCNNuses a non-differentiable categorical distribution as the likelihood model why it can not be used for gradientbased optimization. Instead we used a MCGSM as the likelihood model (Theis & Bethge, 2015), which havebeen shown to be a good density model for images (Theis et al., 2012), using 32 mixture components and 32quadratic features to approximate the covariance matrices.
Figure 8: 4× SR from 32 × 32 to 128 × 128 using AffGAN on the ImageNET. AffGAN outputs (top row),true HR images y (middle row), model input x (bottom row).
