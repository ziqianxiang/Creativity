Table 1: Directly estimatedillustrates the magnitude of the posterior pY |X=0.5. Bayes-optimal estimates desired quantity. The MSE andunder MSE and MAE as well as the MAP estimate given X = 0.5 are marked MAE models performs worsewith labels. The MAP estimates for different values of X ∈ [-8, 8] are also since they do not minimizeshown (- •一). b) Trained model outputs for X ∈ [-8,8] and estimated gradi- the cross-entropy. Further theents from a denoising function trained on PY. Note the AffGAN(- ∙- ) and models using affine projectionsAffDG(- ♦- ) models fit the posterior mode well whereas the MSE (- ♦- ) (Aff) performs better than theand MAE (- ♦- ) model outputs generally fall in low probability regions. soft constrained models.
Table 2: PSNR, SSIM and MSEscores for the CelebA dataset. Interms of PSNR and SSIM in HRspace the MSE trained modelsachieves the best scores as ex-pected and the AffGAN performsbetter than the SoftGAN. Consid-ering 'MSE (x, Ay) the modelsaccording to figure legend. Both the AffGAN and SoftGAN produces clearly clearly show better consistencyshaper images than the blurry MSE outputs. We found that AffGAN outputs between input x and down sam-slightly sharper images compared to SoftGAN, however also with slightly pled model output Ay than mod-more high-frequency noise.	els not using the projection.
