Figure 1: (a) Illustration of nodes and edges "seen" at edge 4,13 in layer 1 and (b) Receptive field atlayer 1. All entries in grey show the oi0,j in covariance matrix used to compute o14,13 . (c) shows thedilation process and receptive field (red) at higher layersgrowing number of hidden nodes to approximate it. However, this problem has a great deal ofstructure that can allow efficient approximation. Firstly, higher order monomials will go to zeroquickly with a uniform prior on ρi,j, which takes values between 0 and 1, suggesting that in manycases a concentration bound exists that guarantees non-exponential growth. Furthermore, theexistence result is shown already for a shallow network, and we expect a logarithmic decrease in thenumber of parameters to peform function estimation with a deep network (Cohen et al., 2016).
Figure 2: Diagram of the DeepGraph structure discovery architecture used in this work. The input is firststandardized and then the sample covariance matrix is estimated. A neural network consisting of multiple dilatedconvolutions and a final 1 × 1 convolution layer is used to predict edges corresponding to non-zero entries in theprecision matrix.
Figure 4: Average test likelihood for COAD and BRCA subject groups in gene data and neuroimaging datausing different number of selected edges. Each experiment is repeated 50 times for genetics data. It is repeatedapproximately 1500 times in the fMRI to obtain significant results due high variance in the data. DeepGraphwith averaged permutation dominates in all cases for genetics data, while DeepGraph+Permutation is superior orequal to competing methods in the fMRI data.
Figure 5: Example solution from DeepGraph and Graph Lasso in the small sample regime on the same 35samples, along with a larger sample solution of Graph Lasso for reference. DeepGraph is able to extract similarkey edges as graphical lassoWe show the edges returned by Graph Lasso and DeepGraph for a sample from 35 subjects (Fig. 5)in the control group. We also show the result of a large-sample result based on 368 subjects fromgraphical lasso. In visual evaluation of the edges returned by DeepGraph we find that they closelyalign with results from a large-sample estimation procedure. Furthermore we can see several edges inthe subsample which were particularly strongly activated in both methods.
Figure 6: Average test likelihood over 50 trials of applying a network trained for 500 nodes, used on a 175 nodeproblemcovariance matrix from 40 samples and padded to the appropriate size. We observe that DeepGraph has similarperformance to graph lasso while permuting the input and ensembling the result gives substantial improvement.
