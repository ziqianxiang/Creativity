Figure 1: Demonstration of a black box attack (in which the attack is constructed without access tothe model) on a phone app for image classification using physical adversarial examples. We tooka clean image from the dataset (a) and used it to generate adversarial images with various sizes ofadversarial perturbation . Then we printed clean and adversarial images and used the TensorFlowCamera Demo app to classify them. A clean image (b) is recognized correctly as a “washer” whenperceived through the camera, while adversarial images (c) and (d) are misclassified. See video offull demo at https://youtu.be/zQ_uMenoBCk.
Figure 2: Top-1 and top-5 accuracy of Inception v3 under attack by different adversarial methodsand different compared to “clean images” — unmodified images from the dataset. The accuracywas computed on all 50, 000 validation images from the ImageNet dataset. In these experimentsvaries from 2 to 128.
Figure 3: Experimental setup: (a) generated printout which contains pairs of clean and adversar-ial images, as well as QR codes to help automatic cropping; (b) photo of the printout made by acellphone camera; (c) automatically cropped image from the photo.
Figure 4: Comparison of different adversarial methods with = 32. Perturbations generated byiterative methods are finer compared to the fast method. Also iterative methods do not always selecta point on the border of -neighbourhood as an adversarial image.
Figure 5: Comparison of images resulting from an adversarial pertubation using the “fast” methodwith different size of perturbation . The top image is a “washer” while the bottom one is a “ham-ster”. In both cases clean images are classified correctly and adversarial images are misclassified forall considered .
Figure 6: Comparison of adversarial destruction rates for various adversarial methods and types oftransformations. All experiments were done with = 16.
