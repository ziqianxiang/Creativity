Figure 1: Composing policies from subpolicies. HereWe have simplified versions of two tasks (make planksand make sticks, each associated with its own policy(∏ι and ∏2 respectively). These policies share an ini-tial high-level action bi: both require the agent to getwood before taking it to an appropriate crafting station.
Figure 2: Model overview. Each subpol-icy π is uniquely associated with a symbol bimplemented as a neural network that mapsfrom a state Si to distributions over A+, andchooses an action a% by sampling from thisdistribution. Whenever the stop action issampled, control advances to the next sub-policy in the sketch.
Figure 3: Example tasks from the environments used in this paper. (a) In the maze environment, the agent mustreach a goal position by traversing right (1), down (2) and down again (3) through a sequence of rooms, someof which may have locked doors. (b) In the crafting environment, an agent seeking to pick up the gold nuggetin the top corner must first collect wood (1) and iron (2), use a workbench to turn them into a bridge (3), anduse the bridge to cross the water (4).
Figure 4:	Comparing modular learning from sketches with standard RL baselines. Modular is the approachdescribed in this paper, while Independent learns a separate policy for each task, Joint learns a shared policythat conditions on the task identity, and Q reader learns a single network to map from states and action symbolsto Q values. Performance for the best iteration of the (off-policy) Q reader is plotted. (a) Performance ofthe three models in the maze environment. (b) Performance in the crafting environment. (c) Individual taskperformance for the modular model in the crafting domain. Colors correspond to task length. It can be seen thatthe sharp steps in the learning curve correspond to increases of 'max in the curriculum. The modular approachis eventually able to achieve high reward on all tasks, while the baseline models perform considerably worseon average.
Figure 5:	Ablation experiments. (a) The critic: lines labeled “task” include a baseline that varies with the taskidentity, while lines labeled “state” include a baseline that varies with the state identity. Estimating a baselinethat depends on both the representation of the current state and the identity of the current task is better thaneither alone or a constant baseline. (b) The curriculum: lines labeled “length” use a curriculum with iterativelyincreasing lengths, while lines labeled “weight” sample tasks in inverse proportion to their current reward.
