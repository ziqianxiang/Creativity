Figure 1: (a) The tag represents a unique binary code (cell 0 to 11) and encodes the orientation withthe semicircles 12 and 13. The red arrow points towards the head of the bee. This tag encodes theid 100110100010. (b) Cutout from a high-resolution image.
Figure 2: First row: Images from the 3D model without augmentation. Below: Correspondingimages from the RenderGAN. Last row: Real images of bee’s tags.
Figure 3: Topology of a GAN. The discriminator network D is trained to distinguish between ”fake”and real data. The generator network G receives a random vector as input. G is optimized tomaximize the chance of the discriminator making a mistake.
Figure 4: The generator G cannot directly produce samples. Instead, G has to predict parametersGl for the 3D model M . The image generated by M is then modified through the augmentationfunctions φi parameterized by Gi to match the real data.
Figure 5: Augmentation functions of the RenderGAN applied to the BeesBook project. The arrowsfrom G to the augmentation functions φ depict the inputs to the augmentation functions. The gener-ator provides the position and orientations to the 3D model, whereas the bits are sampled uniformly.
Figure 6: (a) Histogram of the discriminator scores on fake and real samples. (b) Losses of thegenerator (G) and discriminator (D).
Figure 7: Random points in the z-space given the tag parametersgenerator creates unrealistic high-frequencies artifacts. The discriminator unfailingly assigns a lowscore to theses images. We can therefore discard them for the training of the supervised algorithm.
Figure 8: Training and validation losses of DCNNs trained on different data sets. As some data setsare missing the orientation of the tags, only the loss of the bits are plotted. Binary crossentropy isused as loss for the bits. The train and validation loss of each dataset have the same color.
Figure 9: Continuum visualization on the basis of the discriminator score: Most realistc scored sam-ples top left corner to least realistc bottom right corner. Images with artifacts are scored unrealisticand are not used for training.
Figure 10: Images generated with the generator given a fixed bit configurationFigure 11: Correspondence of generated images and 3D model13Under review as a conference paper at ICLR 2017B	Handmade augmentationsWe constructed augmentations for blur, lighting, background, noise and spotlights manually. Forsynthesizing lighting, background and noise, we use image pyramids, i.e. a set of images L0, . . . , L6of size (2i × 2i) for 0 ≤ i ≤ 6. Each level Li in the pyramid is weighted by a scalar ωi. Each pixelof the different level Li is drawn from N (0, 1). The generated image I6 is given by:I0 = ω0L0Ii = ωiLi + upscale(Ii-1)(10)(11), where upscale doubles the image dimensions. The pyramid enables us to generate random imageswhile controlling their frequency domain by weighting the pyramid levels appropriately.
Figure 11: Correspondence of generated images and 3D model13Under review as a conference paper at ICLR 2017B	Handmade augmentationsWe constructed augmentations for blur, lighting, background, noise and spotlights manually. Forsynthesizing lighting, background and noise, we use image pyramids, i.e. a set of images L0, . . . , L6of size (2i × 2i) for 0 ≤ i ≤ 6. Each level Li in the pyramid is weighted by a scalar ωi. Each pixelof the different level Li is drawn from N (0, 1). The generated image I6 is given by:I0 = ω0L0Ii = ωiLi + upscale(Ii-1)(10)(11), where upscale doubles the image dimensions. The pyramid enables us to generate random imageswhile controlling their frequency domain by weighting the pyramid levels appropriately.
Figure 12: Training samples from the different datasets.
