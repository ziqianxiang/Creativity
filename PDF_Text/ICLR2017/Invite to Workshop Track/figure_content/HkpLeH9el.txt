Figure 1: Success rate of our models on straight-line programs of increasing lengthWe performed a cursory exploration of hyperparameter choices. We varied the choice of optimizationalgorithm (Momentum, Adam, RMSProp), the learning rate (from 0.001 to 5), gradient noise (testingthe recommended choices from Neelakantan et al. (2016b)), a decaying entropy bonus (starting from0.001 to 20), and gradient clipping (to values between 0.1 and 10). We sampled 100 hyperparametersettings from this space and tested their effect on two simple tasks. We ran the remaining experimentswith the best configuration obtained by this process: the RMSProp optimization algorithm, a learningrate of 0.1, clipped gradients at 1, and no gradient noise.
Figure 2: Our example tasks for loop based programs. “Simple” tasks are above the line.
Figure 3: Semantics of foldli, mapi, zipwithi in a Python-like language.
Figure 5:	Solutions to exGtK in the C+T+I and A+L models.
Figure 6:	A solution to findLastIdx in the C+T+I model.
Figure 4: A solution to allGtK in the C model. Code in gray is dead.
Figure 7:	A solution to getIdx in the C+T+I model.
Figure 8:	Solutions to last2 in the C+T and A+L models.
Figure 9:	A solution to len in the C+T+I model.
Figure 10:	A solution to mapAddK in the C+T+I model.
Figure 11:	A solution to mapInc in the C+T+I model.
Figure 12: Solutions to max in the C+T+I and A+L models.
Figure 13: A solution to pairwiseSum in the C+T+I model.
Figure 14: Solutions to rev in the C+T+I and A+L models.
Figure 15: Solutions to revMapInc in the C+T and A+L models.
Figure 16: Solutions to sum in the C+T+I and A+L models.
