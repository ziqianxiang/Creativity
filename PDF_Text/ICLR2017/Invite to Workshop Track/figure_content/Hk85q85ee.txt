Figure 1: (a) We consider the student and teacher network as nonlinear neural networks with ReLUnonlinearity. The student network updates its weight W from the output of the teacher, whoseweights W are fixed. (b)-(c) The network structure we consider in K = 1 and K ≥ 2 cases.
Figure 2: (a) Sampling strategy to maximize the probability of convergence. (b) Relationship be-tween sampling range r and desired probability of success (1 - e)/2. (C) Geometry of K = 1 2Dcase. There is a singularity at the origin. Initialization with random weights around the origin hasdecent probability to converge to w*.
Figure 3: (a) Distribution of relative RMS error with respect to θ = ∠(w, e). (b) Relative RMSerror decreases with sample size, showing the asympototic behavior of the close form expressionEqn. 10. (c) Eqn. 10 also works well when the input data X are generated by other zero-meandistribution X, e.g., uniform distribution in [-1/2, 1/2].
Figure 4: (a)-(b) Vector field in (x, y) plane following 2D dynamics (Eqn. 16) for K = 2 andK = 5. Saddle points are visible. The parameters of teacher’s network are at (1, 0). (c) Trajectoryin (x, y) plane for K = 2, K = 5, and K = 10. All trajectories start from (10-3, 0). Even thestarting point are aligned with w*, gradient descent dynamics takes detours. (d) Training curve.
Figure 5: Top row: Convergence when the initial weights deviates from symmetric initialization:W(I) = 10-3w* + e. Here e 〜N(0,10-3 * noise). The 2-layered network converges to w* untilvery large noise is present. Both teacher and student networks use g(x) = PK=I σ(w∣x). Eachexperiment has 8 runs. Bottom row: Convergence when We use g2(x) = PK=I ajσ(w∣x). Herethe top weights aj is fixed at different numbers (rather than 1). Large positive aj correponds to fastconvergence. When aj has positive/negative components, the network does not converge to w*.
Figure 6: (a)-(b) TWo cases in Lemma 7.2. (C) Convergence analysis in the symmetric two-layeredcase.
