Figure 1: Example of a machine-learning classifier (predictor) and a human annotator (oracle) forclassifying images of hand-written “0”. Both include two steps: feature extraction and classification.
Figure 2: An example showing that f1 with one unnecessary feature (according to f2) is prone toadversarial examples. The red circle denotes an adversarial example (e.g. generated by some attacksimilar as JSMA (Papernot et al., 2015a) (details in Section 8.2)). Each adversarial example is veryclose to its seed sample in the oracle feature space (according to d2), but it is comparatively far fromits seed sample in the feature space (according to d1 ) of the trained classifier and is at the differentside of the decision boundary of f1. Essentially “adversarial examples” can be easily found for allseed samples in this Figure. We only draw cases for two seeds. Besides, for each seed sample, wecan generate a series of “adversarial examples” (by varying attacking power) after the attacking linecrosses the decision boundary of f1. We only show one case of such an adversarial example for eachseed sample.
Figure 3: This figure shows one situation that (X, d01) is not a finer topology than (X, d02) (therefore,(X1, d1) and (X2, d2) are not topologically equivalent). According to Theorem (3.4), in this case, theDNN is vulnerable to adversarial examples. The two sample points a and b are close with regards to(w.r.t.) a norm || ∙ || in X. They are also close w.r.t. d2 in (X2 ,d2) space and close w.r.t. d2 in (X, d2)space. But they are far from each other in the space of (X, d01) and in the space of (X1, d1). In otherwords, while d2(a, b), d02(a, b) and ||a - b|| are small, d1(a, b) and d01(a, b) are large. Clearly, DNNcan be easily evaded by adding a small perturbation k a - b k on sample a or sample b. NOTE: it isnormally difficult to get the analytic form of (X2, d2) for most applications. Most previous studies(reviewed in Section 2.2) assume (X2, d2) equals to (X, || ∙ ||), where || ∙ || is a norm function.
Figure 4: An example figure illustrating Table 3 Case (III) when f1 is strong-robust. We assumec1 and c2 as linear classification functions. We show one case of X1 = X2 = R2 and f1, f2 arecontinuous a.e.. In terms of classification, f1 (green boundary line) is not accurate according to f2(red boundary line). All pairs of test samples (x, x0) can be categorized into the three cases shown inthis figure. Test-case (a): f1 and f2 assign the same classification label (yellow circle) on x and x0 . xand x0 are predicted as the same class by both. Test-case (b): f1 assigns the class of “blue square” onboth x and x0 . f2 assigns the class of “yellow circle” on both x and x0 . Test-case (c): f2 assigns theclass of “yellow circle” on both x and x0 . However, f1 assigns the class of “blue square” on x andassigns a different class of “yellow circle” on x0. This case has been explained in Section 11.
Figure 5: An example figure illustrating Table 3 Case (IV) when f1 is strong-robust. We assume c1and c2 as linear classification functions. We show one case of 1 = n1 < n2 = 2, X1 ⊂ X2 and f1,f2 are continuous a.e.. In terms of classification, f1 (green boundary line) is not accurate according tof2 (red boundary line). All pairs of test samples (x, x0) can be categorized into the three cases shownin this figure. Test-case (a): f1 and f2 assign the same classification label (yellow circle) on x and x0 .
Figure 6: An example showing boundary points of f1 and boundary points of f2 . We assume f1and f2 are continuous a.e.. We assume c1 and c2 as linear classification functions. The first twocolumns showing boundary points of f2 that are not considered in this paper. The third columndescribes “Boundary based adversarial attacks” that can only attack seed samples whose distance tothe boundary of f1 is smaller than . Essentially this attack is about those boundary points of f1 thatare treated as similar and belong to the same class by f2 .
Figure 7: When f1 is not continuous a.e., the strong-robustness of f1 is determined by both g1 and c1.
Figure 8: Sketch of Siamese training. Inputs are pairs of seed sample and their randomly perturbedversion, while we suppose the d2 distance between the pair is small. By forwarding a pair into theSiamese network and penalizing the outputs of the pair, this training intuitively limit the d1 distancebetween two similar samples to be small. Backpropagation is used to update the weights of thenetwork.
Figure 9: Result of CIFAR-10: (a) Test accuracy under adversarial example attacks: three differentcolors for three different training strategies. (Details in Section 12.2) We don’t include the resultof adversarial training because previous adversarial training can’t be used on networks with batch-normalization. Some tricks of training such networks are released in a recent paper (Kurakin et al.,2016) (b) ARC and ARCA for three different training strategies under adversarial example attacks.
Figure 10: (a) Test accuracy under adversarial example attacks on MNIST dataset: four differentcolors for four different training strategies. (Details in Section 12.2) (b) ARC and ARCA for fourdifferent training strategies under adversarial example attacks.
