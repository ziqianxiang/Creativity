Figure 1:	GPU timings for multi-plying two matrices in the dominantstep of the RNN model. We con-sider matrices of size 2560 × 2048and 2048 × k representing hiddenstates and word representations. Wereport the timings as a function ofk (number of word representations),and we compute the averages (cir-cles) over 1000 measures, and theminima and maxima for the K40.
Figure 2:	Our hierarchical model is organizedas (i) a first level that includes both the mostfrequent words and vectors representing clus-ters, and (ii) clusters on the second level thatare associated with rare words, the largestones being associated with the less frequentwords. The sizes are determined so as to min-imize our cost model on GPU.
Figure 3:	Training on Europarl(Finnish): perplexity (on validation) asthe function of time for our methodand approaches from the state of theart. We represent the result after eachepoch by a point. Our method fa-vorably compares with all other ap-proaches w.r.t. the tradeoff perplexityand training time, and of training datavs perplexity. Similar conclusions aredrawn for the other languages.
