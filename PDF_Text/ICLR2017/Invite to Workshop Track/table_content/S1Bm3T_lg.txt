Table 1:	Dataset propertiesName	#Training Exs. - #Testing Exs. Dimensions ClassesSmall NORB	24300-24300	96 × 96	5NORB Compositions	100-1000	256 × 256	2NORB Symmetries	{50, 100, . . . , 12800}-2916	108 × 108	6computer, assuming the size of the training set elements greatly exceeds the dimensionality of theleaf kernel, this sum node will require O(log(∣E∣)) time (the depth of a parallel ㊉ reduction circuit)and O(|E|) space. Duda et al. (2000) describe a constant time nearest neighbor circuit that relies onprecomputed Voronoi partitions, but this has impractical space requirements in high dimensions. Aswith SVMs, optimization of sparse element weights can greatly reduce model size.
Table 2:	Accuracy on Small NORBMethod	AccuracyConvnet (14 epochs) (Bengio & LeCun, 2007)	94.0%DBM with aug. training (Salakhutdinov & Hinton, 2009)	92.8%CKMW	89.8%Convnet (2 epochs) (Bengio & LeCun, 2007)	89.6%DBM (Salakhutdinov & Hinton, 2009)	89.2%SVM (Gaussian kernel) (Bengio & LeCun, 2007)	88.4%CKM	88.3%k-NN (LeCun et al., 2004)	81.6%Logistic regression (LeCun et al., 2004)	77.5%Table 3: Accuracy on NORB CompositionsMethod	Accuracy	Train+Test (min)CKM	82.4%	1.5 [CPU]SVM with convnet features	75.0%	1 [GPU+CPU]Convnet	50.6%	9 [GPU]k-NN on image pixels	51.2%	0.2 [CPU]architecture. We demonstrate the advantage of CKMs for representing composition and symmetryin the following experiments.
Table 3: Accuracy on NORB CompositionsMethod	Accuracy	Train+Test (min)CKM	82.4%	1.5 [CPU]SVM with convnet features	75.0%	1 [GPU+CPU]Convnet	50.6%	9 [GPU]k-NN on image pixels	51.2%	0.2 [CPU]architecture. We demonstrate the advantage of CKMs for representing composition and symmetryin the following experiments.
