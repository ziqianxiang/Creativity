title,year,conference
 When and why are log-linear models self-normalizing,2014, In ACL
 A maximum likelihood approach to continuous speechrecognition,1983, PAMI
 Adaptive importance sampling to accelerate training of a neuralprobabilistic language model,2008, Neural Networks
 Class-based n-grammodels of natural language,1992, Computational linguistics
 Empirical evaluation of gatedrecurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Fastand robust neural network joint models for statistical machine translation,2014, In ACL
 Adaptive subgradient methods for online learning and stochasticoptimization,2011, JMLR
 Finding structure in time,1990, Cognitive science
 Classes for fast maximum entropy training,2001, In ICASSP
 Speech recognition with deep recurrent neuralnetworks,2013, In ICASSP
 Noise-contrastive estimation: A new estimation principle for Unnormal-ized statistical models,2010, In International Conference on Artificial Intelligence and Statistics
 Deep neUral networks for acoUstic modeling inspeech recognition: The shared views of foUr research groUps,2012, Signal Processing Magazine
 Long short-term memory,1997, Neural computation
 On Using very large target vocabUlaryfor neUral machine translation,2015, 2015
 BlackoUt: SpeedingUp recUrrent neUral network langUage models with very large vocabUlaries,2015, arXiv preprint arXiv:1511
 Estimation of probabilities from sparse data for the langUage model component of a speechrecognizer,1987, ICASSP
 Improved backing-off for m-gram langUage modeling,1995, In ICASSP
 EUroparl: A parallel corpUs for statistical machine translation,2005, In MT summit
 A cache-based natUral langUage model for speech recognition,1990, PAMI
 Structured output layerneUral network langUage model,2011, In ICASSP
 A simple way to initialize recurrent networks of rectifiedlinear units,2015, arXiv preprint arXiv:1504
 Context dependent recurrent neural network language model,2012, In SLT
 Recurrent neuralnetwork based language model,2010, In INTERSPEECH
 Empirical evaluation andcombination of advanced language modeling techniques,2011, In INTERSPEECH
 Strategies for training largescale neural network language models,2011, In ASRU
 Extensions ofrecurrent neural network language model,2011, In ICASSP
 Efficient estimation of word representations invector space,2013, arXiv preprint arXiv:1301
 Learning longermemory in recurrent neural networks,2014, arXiv preprint arXiv:1412
 A scalable hierarchical distributed language model,2009, In NIPS
 Hierarchical probabilistic neural network language model,2005, In Aistats
 Sparse non-negative matrix language modeling forskip-grams,2015, In Proceedings of Interspeech
 Sequence to sequence learning with neural networks,2014, In Advancesin neural information processing systems
 Decoding with large-scale neurallanguage models improves translation,2013, In EMNLP
 Efficient exact gradient update for training deepnetworks with very large sparse targets,2015, In NIPS
 Backpropagation through time: what it does and how to do it,1990, 1990
 An efficient gradient-based algorithm for on-line training of recurrent networktrajectories,1990, Neural computation
 Human behavior and the principle of least effort,1949, 1949
 Speed regularization and optimality in word classing,2013, In ICASSP
