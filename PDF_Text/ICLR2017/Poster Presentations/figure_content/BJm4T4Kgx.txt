Figure 1: Influence of size of the model on top 1 classification accuracy of various adversarialexamples. Left column — base model without adversarial training, right column — model withadversarial training using “step l.l.” method. Top row — results on “step l.l.” adversarial images,middle row — results on “iter. l.l.” adversarial images, bottom row — results on “basic iter.”adversarial images. See text of Section 4.3 for explanation of meaning of horizontal and verticalaxes.
Figure 2: Influence of the size of adversarial perturbation on transfer rate of adversarial examples.
Figure 3: Comparison of different one-step adversarial methods during eval. Adversarial trainingwas done using “step l.l.” method. Some evaluation methods show increasing accuracy with in-creasing over part of the curve, due to the label leaking effect.
Figure 4: Influence of size of the model on top 5 classification accuracy of various adversarialexamples. For a detailed explanation see Section 4.3 and Figure 1.
Figure 5: Influence of the size of adversarial perturbation on the error rate on adversarial examplesgenerated for one model and classified using another model. Both source and target models wereInception v3 networks with different random intializations.
