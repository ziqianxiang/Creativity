Figure 1: An overview of our two models. Both models consist of an LSTM preprocessing layer,a match-LSTM layer and an Answer Pointer layer. For each match-LSTM in a particular direction,hq, which is defined as Hqα∣, is computed using the α in the corresponding direction, as describedin Eqn. (2)preprocess-ing Layerpreprocess-ing Layer2 MethodIn this section, we first briefly review match-LSTM and Pointer Net. These two pieces of existingwork lay the foundation of our method. We then present our end-to-end neural architecture formachine comprehension.
Figure 2:	Performance breakdown by answer lengths and question types on SQuAD developmentdataset. Top: Plot (1) shows the performance of our two models (where s refers to the sequencemodel , b refers to the boundary model, and e refers to the ensemble boundary model) over answerswith different lengths. Plot (2) shows the numbers of answers with different lengths. Bottom:Plot (3) shows the performance our the two models on different types of questions. Plot (4) showsthe numbers of different types of questions.
Figure 3:	Visualization of the attention weights α for four questions. The first three questions sharethe same paragraph. The title is the answer predicted by our model.
