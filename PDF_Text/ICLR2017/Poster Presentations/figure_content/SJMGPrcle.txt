Figure 1: Views from a small 5 × 10 maze, a large 9 × 15 maze and an I-maze, with corresponding maze layoutsand sample agent trajectories. The mazes, which will be made public, have different textures and visual cues aswell as exploration rewards and goals (shown right).
Figure 2: Different architectures: (a) is a convolutional encoder followed by a feedforward layer and policy (π)and value function outputs; (b) has an LSTM layer; (c) uses additional inputs (agent-relative velocity, reward,and action), as well as a stacked LSTM; and (d) has additional outputs to predict depth and loop closures.
Figure 3: Rewards achieved by the agents on 5 different tasks: two static mazes (small and large) with fixedgoals, two static mazes with comparable layout but with dynamic goals and the I-maze. Results are averagedover the top 5 random hyperparameters for each agent-task configuration. Star in the label indicates the use ofreward clipping. Please see text for more details.
Figure 4: left: Example of depth predictions (pairs of ground truth and predicted depths), sampled every 40 steps.
Figure 5: Trajectories of the Nav A3C*+D1L agent in the I-maze (left) and of the Nav A3C+D2 random goalmaze 1 (right) over the course of one episode. At the beginning of the episode (gray curve on the map), theagent explores the environment until it finds the goal at some unknown location (red box). During subsequentrespawns (blue path), the agent consistently returns to the goal. The value function, plotted for each episode,rises as the agent approaches the goal. Goals are plotted as vertical red lines.
Figure 6: Trajectory of the Nav A3C+D2 agent in the random goal maze 1, overlaid with the position probabilitypredictions predicted by a decoder trained on LSTM hidden activations, taken at 4 steps during an episode.
Figure 7: LSTM cell activations of LSTM A3C and Nav A3C*+D1L agents from the I-Maze collected overmultiple episodes and reduced to 2 dimensions using tSNE, then coloured to represent the goal location.
Figure 8: Details of the architecture of the Nav A3C+D+L+Dr agent, taking in RGB visual inputs xt, pastreward rt-1, previous action at-1 as well as agent-relative velocity vt, and producing policy π, value functionV , depth predictions gd(ft) and gd0 (ht) as well as loop closure detection gl (ht).
Figure 9: Results are averaged over the top 5 random hyperparameters for each agent-task configuration. Star inthe label indicates the use of reward clipping. Please see text for more details.
Figure 10: Comparison of agent architectures over non-navigation maze configurations, Seek-Avoid Arena andStairway to Melon, described in details in (Beattie et al., 2016). Image credits for (c) and (d): (Jaderberg et al.,2017).
Figure 11: Plot of the Area Under the Curve (AUC) of the rewards achieved by the agents, across differentexperiments and on 3 different tasks: large static maze with fixed goals, large static maze with comparable layoutbut with dynamic goals, and the I-maze. The reward AUC values are computed for each replica; 64 replicaswere run per experiment and the reward AUC values are sorted by decreasing value.
