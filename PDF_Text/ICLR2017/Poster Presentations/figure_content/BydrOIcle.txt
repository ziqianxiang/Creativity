Figure 1: An illustration of the computation graph for an unrolled GAN with 3 unrolling steps.
Figure 2: Unrolling the discriminator stabilizes GAN training on a toy 2D mixture of Gaussiansdataset. Columns show a heatmap of the generator distribution after increasing numbers of trainingsteps. The final column shows the data distribution. The top row shows training for a GAN with10 unrolling steps. Its generator quickly spreads out and converges to the target distribution. Thebottom row shows standard GAN training. The generator rotates through the modes of the datadistribution. It never converges to a fixed distribution, and only ever assigns significant probabilitymass to a single data mode at once.
Figure 3: Unrolled GAN training increases stability for an RNN generator and convolutional dis-criminator trained on MNIST. The top row was run with 20 unrolling steps. The bottom row is astandard GAN, with 0 unrolling steps. Images are samples from the generator after the indicatednumber of training steps.
Figure 4: Visual perception of sample quality and diversity is very similar for models trained withdifferent numbers of unrolling steps. Actual sample diversity is higher with more unrolling steps.
Figure 5: Training set images are more accurately reconstructed using GANs trained with unrollingthan by a standard (0 step) GAN, likely due to mode dropping by the standard GAN. Raw data ison the left, and the optimized images to reach this target follow for 0, 1, 5, and 10 unrolling steps.
Figure 6: As the number of unrolling steps in GAN training is increased, the distribution of pairwisedistances between model samples more closely resembles the same distribution for the data. Herewe plot histograms of pairwise distances between randomly selected samples. The red line givespairwise distances in the data, while each of the five blue lines in each plot represents a modeltrained with a different random seed. The vertical lines are the medians of each distribution.
