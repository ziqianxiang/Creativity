Figure 1: The diagram illustrates the distributed computation of K-FAC. Gradient workers (blue)compute the gradient w.r.t. the loss function. Stats workers (grey) compute the sampled SeCond-order statistics. Additional workers (red) compute inverse Fisher blocks. The parameter server(orange) uses gradients and their inverse Fisher blocks to compute parameter updates.
Figure 2: The results from our CIFAR-10 experiment looking at the effectiveness of asynchronouslycomputing the approximate Fisher inverses. gpu indicates the number of gradient workers. Dashedlines denote training curves and solid lines denote test curves. Top row: cross entropy loss andclassification error vs the number of updates. Bottom row: cross entropy loss and classificationerror vs wallclock time.
Figure 3: Optimization performance of distributed K-FAC and SGD training GoogLeNet on Ima-geNet. Dashed lines denote training curves and solid lines denote validation curves. bz indicates thesize of mini-batches. rbz indicates the size of chunks used to assemble the BN updates. Top row:cross entropy loss and classification error v.s. the number of updates. Bottom row: cross entropyloss and classification error vs wallclock time (in hours). All methods used 4 GPUs, with distributedK-FAC using the 4-th GPU as a dedicated asynchronous stats worker.
Figure 4: Optimization performance of distributed K-FAC and SGD training AlexNet on ImageNet.
Figure 5: Optimization performance of distributed K-FAC and SGD training ResNet50 on Ima-geNet. The dashed lines are the training curves and solid lines are the validation curves. bz indicatesthe size of mini-batches. rbz indicates the size of chunks used to assemble the BN updates. Toprow: cross entropy loss and classification error v.s. the number of updates. Bottom row: cross en-tropy loss and classification error v.s. wallclock time (in hours). All methods used 8 GPUs, withdistributed K-FAC using the 8-th GPU as a dedicated asynchronous stats worker.
Figure 6: The comparison of distributed K-FAC and SGD on per training case progress on trainingloss and errors. The experiments were conducted using GoogLeNet with various mini-batch sizes.
Figure 7: Empirical evaluation of the proposed cheaper Kronecker approximation on GoogLeNet.
Figure 8: Results from the experiment described in Appendix B. decayKL indicates the proposedstep-size selection method and decayLR indicates standard exponential learning rate decay.
