Figure 1:	Simulator sample dialogues for the bAbI task (left) and WikiMovies (right). Weconsider 10 different tasks following Weston (2016) but here describe only Task 6; other tasks aredetailed in the appendix. The teacher’s dialogue is in black and the bot is in red. (+) indicatesreceiving positive reward, given only 50% of the time even When correct.
Figure 2:	Human Dialogue from Mechanical Turk (based on WikiMovies) The human teacher’sdialogue is in black and the bot is in red. We show examples where the bot answers correctly (left)and incorrectly (right). Real humans provide more variability of language in both questions andtextual feedback than in the simulator setup (cf. Figure 1).
Figure 3: Training epoch vs. test accuracy for bAbI (Task 6) varying exploration and batchsize. Random exploration is important for both reward-based (RBI) and forward prediction (FP).
Figure 4: WikiMovies: Training epoch vs. test accuracy on Task 6 varying (top left panel) explo-ration rate while setting batch size to 32 for RBI, (top right panel) for FP, (bottom left) batch sizefor RBI, and (bottom right) comparing RBI, REINFORCE and FP with = 0.5. The model is robustto the choice of batch size. RBI and REINFORCE perform comparably. Note that supervised, ratherthan reinforcement learning, with gold standard labels achieves 80% accuracy on this task (Weston,2016).
Figure 5: The ten tasks our simulator implements, which evaluate different forms of teacher responseand binary feedback. In each case the same example from WikiMovies is given for simplicity, wherethe student answered correctly for all tasks (left) or incorrectly (right). Red text denotes responsesby the bot with S denoting the bot. Blue text is spoken by the teacher with T denoting the teacher’sresponse. For imitation learning the teacher provides the response the student should say denotedwith S in Tasks 1 and 8. A (+) denotes a positive reward.
Figure 6: Training epoch vs. test accuracy for bAbI (Task 2) varying exploration and batchsize.
Figure 7: Training epoch vs. test accuracy for bAbI (Task 3) varying exploration and batchsize. Random exploration is important for both reward-based (RBI) and forward prediction (FP).
Figure 8: Training epoch vs. test accuracy for bAbI (Task 4) varying exploration and batchsize. Random exploration is important for both reward-based (RBI) and forward prediction (FP).
Figure 9:	WikiMovies: Training epoch vs. test accuracy on Task 2 varying (top left panel) explo-ration rate while setting batch size to 32 for RBI, (top right panel) for FP, (bottom left) batch sizefor RBI, and (bottom right) comparing RBI, REINFORCE and FP setting = 0.5. The model isrobust to the choice of batch size. RBI and REINFORCE perform comparably.
Figure 10:	WikiMovies: Training epoch vs. test accuracy on Task 3 varying (top left panel) explo-ration rate while setting batch size to 32 for RBI, (top right panel) for FP, (bottom left) batch sizefor RBI, and (bottom right) comparing RBI, REINFORCE and FP setting = 0.5. The model isrobust to the choice of batch size. RBI and REINFORCE perform comparably.
Figure 11: WikiMovies: Training epoch vs. test accuracy on Task 4 varying (top left panel) explo-ration rate while setting batch size to 32 for RBI, (top right panel) for FP, (bottom left) batch sizefor RBI, and (bottom right) comparing RBI, REINFORCE and FP setting = 0.5. The model isrobust to the choice of batch size. RBI and REINFORCE perform comparably.
Figure 12: WikiMovies: Training epoch vs. test accuracy with varying batch size for FP on Task 2(top left panel), 3 (top right panel), 4 (bottom left panel) and 6 (top right panel) setting = 0.5. Themodel is robust to the choice of batch size.
