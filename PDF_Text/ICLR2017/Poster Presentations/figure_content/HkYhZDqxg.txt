Figure 1: Left: A cell of the doubly-recurrent neural network corresponding to node i with parent pand sibling s. Right: Structure-unrolled DRNN network in an encoder-decoder setting. The nodesare labeled in the order in which they are generated. Solid (dashed) lines indicate ancestral (fraternal)connections. Crossed arrows indicate production halted by the topology modules.
Figure 2: Trees generated by the DRNN decoder trained on subset of size N of the synthetic dataset,for a test example with description “ROOT B W F J V”.
Figure 4: Node and edge precision as a function of tree depth (left figure) and width (right).
Figure 3: Left: F1-Score for models trained on randomly sampled subsets of varying size, averagedover 5 repetitions. Right: Node (first column) and edge (second) precision as a function of tree size.
Figure 5: Example recipe from the IFTTT dataset. The description (above) is a user-generatednatural language explanation of the if-this-then-that program (below).
Figure 6: Likelihood change un-der target structural perturbation.
Figure 7: A single unit in each of the three alternative versions of the doubly-recurrent neural net-work, for node i with parent p and sibling s. Left: No explicit topology prediction, Middle: single(ancestral) topology prediction, Right: double (ancestral and fraternal) topology prediction. The top(left) incoming arrows represent the input and state received from the parent node (previous node,respectively).
Figure 8: Tree size distribution in the IFTTT dataset.
Figure 9: Selected trees generated by the Drnn decoder from vector-encoded descriptions for testexamples of the synthetic tree dataset. Trees in the same row correspond to predictions by modelstrained on randomly sampled subsets of size N of the training split. We present cases for which theprediction is accurate (a,c) and cases for which it is not (b,d). Note how in (d) the model predictsmany of the labels correctly, but confuses some of the dependencies (edges) in the tree.
