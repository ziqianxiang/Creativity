Figure 1: Left: Diagram of the Which is Heavier environment. Blocks are always arranged in a line,but mass of the different blocks changes from episode to episode. Right: Mass gap distributions fordifferent settings of β used in the experiments.
Figure 2: Learning curves for a typical agent trained on the Which is Heavier environment at varyingdifficulty settings. The y-axes show the probability of the agent producing the correct answer beforethe episode times out. Each plot shows the top 50% of agents started from 10 random seeds withidentical hyperparameter settings. The light lines show learning curves from individual agents, andthe dark lines show the median performance across the displayed runs for each difficulty. Left:Agents trained from features. Right: Agents trained from pixels.
Figure 3: Left: Histograms of episode lengths for different task difficulty (β) settings. There is atransition from β = 10 where the agents answer eagerly as soon as they find a heavy block to β = 3where the agents are more conservative about answering before they have acted enough to poke allthe blocks at least once. Right: Episode lengths as a function of the normalized mass gap. Unitson the x-axes are scaled to the range of possible masses, and the y-axis shows the number of stepsbefore the agent takes a labeling action. The black dots show individual episodes, and the red lineshows a linear trend fit by OLS and error bars show a histogram estimate of standard deviations.
Figure 4: Comparison between agents in the Which is Heavier environment following their learnedinteraction policies vs the randomized interaction policy baseline. The x-axes show Difficulty-Observation combinations (e.g. 10-F is difficulty 10 with feature observations and 3-P is difficulty 3with pixel observations) Left: Episode lengths when gathering information using the different inter-action policies. Right: Probability of choosing the correct label under different conditions (episodesterminating in timeout have been excluded). The dashed line shows chance performance.
Figure 5: Top: Example trajectory of a block tower being knocked down using the fist actuator. Left:Diagram of the hidden structure of the Towers environment. The tower on the left is composed offive blocks, but could decompose into rigid objects in any several ways that can only be distinguishedby interacting with the tower. Right: Behavior of a single trained agent using fist actuators whenvarying the control time step. The x-axis shows different control time step lengths (the trainingcondition 0.1). The blue line shows probability of the agent correctly identifying the number ofblocks. The red line shows the median episode length (in seconds) with error bars showing 95%confidence intervals computed over 50 episodes. The shaded region shows +/-1 control time steparound the median.
Figure 6:	Learning curves for agents trained on the Towers environment under different conditions.
Figure 7: Comparison between agents in the Towers environment following their learned interactionpolicies vs the randomized interaction policy baseline. The x-axes show different Observation-Actuator combinations (e.g. D-F is Direct-Features and F-P is Fist-Pixels). Left: Episode lengthswhen gathering information using the different interaction policies. Right: Probability of choosingthe correct label under different conditions (episodes terminating in timeout have been excluded).
