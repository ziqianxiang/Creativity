Figure 1: The model sampling chain. Each row shows a sample from p(z(0), . . . , z(T)) for a modelthat has been trained on MNIST digits. We see how the learned Markov transition operator progres-sively denoises an initial unstructured noise sample. We can also see that there remains ambiguity inthe early steps as to what digit this could become. This ambiguity gets resolved only in later steps.
Figure 2: Training infusion chains, infused with target X =W. This figure shows the evolutionof chain q(z(0), . . . , z(30) |x) as training on MNIST progresses. Top row is after network randomweight initialization. Second row is after 1 training epochs, third after 2 training epochs, and so on.
Figure 3: Training curves: lower bounds on aver-age log-likelihood on MNIST as infusion trainingprogresses. We also show the lower bounds esti-mated with the Parzen estimation method.
Figure 4: Mean predictions by our models on 4 different datasets. The rightmost column shows thenearest training example to the samples in the next-to last column.
Figure 5: Inpainting on CelebA dataset. In each row, from left to right: an image form the testset; the same image with bottom half randomly sampled from our factorial prior. Then several endsamples from our sampling chain in which the top part is clamped. The generated samples showthat our model is able to generate a varied distribution of coherent face completions.
Figure 6: Training curves on MNIST showing the log likelihood lower bound (nats) for differentinfusion rate schedules and different number of steps. We use an increasing schedule α(t) = α(t-1) +ω . In each sub-figure for a fixed number of steps, we show the lower bound for different infusionrates.
Figure 7: Comparing samples of constant infusion rate versus an increasing infusion rate on infusedand generated chains. The models are trained on MNIST in 15 steps. Note that having an increasinginfusion rate with a small value for ω allows a slow convergence to the target distribution. In contrasthaving a constant infusion rate leads to a fast convergence to a specific point. Increasing infusionrate leads to more visually appealing samples. We observe that having an increasing infusion rateover many steps ensures a slow blending of the model distribution into the target distribution.
Figure 8: Infusion chains (Sub-Figure 8a) and model sampling chains (Sub-Figure 8b) on CIFAR-10.
Figure 9: Infusion chains (Sub-Figure 9a) and model sampling chains (Sub-Figure 9b) on CelebA.
