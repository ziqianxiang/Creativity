Figure 1: Metacontroller architecture and task. A: All components are part of the metacontrolleragent (box) except the scene and the world, which are part of the agent’s environment. The managertakes the scene and history and determines which action to take (i.e., whether to execute or ponder,and with what expert to ponder with), denoted by the orange lines. The controller takes the sceneand history and computes a control (e.g., the force to apply to a spaceship), denoted by the bluelines. The orange line ending with a circle at the switch reflects the fact that the manager’s actionaffects the behavior of the switch, which routes the controller’s control to either an expert (e.g., asimulation model of the spaceship’s trajectory, an action-value function, etc.) or the world. Theoutcome and reward from the expert, along with the history, action, and control, are fed into thememory, which produces the next history. The history is fed back to the controller on the nextiteration in order to allow it to propose controls based on what it has already tried. B-C: Scenesconsisted of a number of planets (depicted here by colored circles) of different masses as well as aspaceship (also with a variable mass). The task was to apply a force to the spaceship for one timestep of simulation (depicted here as a solid red arrow) such that the resulting trajectory (dotted redarrow) would put the spaceship at a target (bullseye) after 11 steps of simulation. The white ring ofthe bullseye corresponds to a performance loss of 0.12-0.15, the black ring to a loss of 0.09-0.12,the blue ring to a loss of 0.06-0.09, the red ring to a loss of 0.03-0.06, and the yellow center to a lossof 0.03 or less. B depicts an easy, 1-planet scene, while C depicts a very difficult 5-planet scene.
Figure 2: Test performance of the reactive and iterative agents. Each line corresponds to theperformance of an iterative agent (either the true simulation expert, the MLP expert, or the interac-tion net expert) trained for a fixed number of ponder steps on one of the five datasets; the line colorindicates which dataset the controller was trained on. In all cases, performance refers to the perfor-mance loss, LP . Left: the MLP expert struggles with the task due to its limited expressivity, butstill benefits from pondering. Middle: the IN expert performs almost as well as the true simulationexpert, even though it is not a perfect model. Right: The true simulation expert does quite well onthe task, especially with multiple ponder steps.
Figure 3: Test performance of the metacontroller with a single expert on the five planetsdataset. Each column corresponds to a different experts. The lines indicate the performance ofthe iterative agents for different numbers of ponder steps. The points indicate the performance ofthe metacontroller, with each point corresponding to a different value of τ. The x-coordinate of eachpoint is an average across the number of ponder steps, and the y-coordinate is the average loss. Toprow: Here we show total cost rather than just performance on the task (i.e., including computationcost). Different colors show the result for different τ, with the different lines showing the cost for thesame iterative controller under different values of τ . The error bars (for the metacontroller) indicate2.5% and 97.5% confidence intervals. When the point is below its corresponding curve, it meansthat the metacontroller was able to achieve a better speed-accuracy trade-off than that achievable bythe iterative agent. Line colors of increasing brightness correspond to increasing τ , with τ valuestaken from [0, 0.0134, 0.0354, 0.0576, 0.0934, 0.152, 0.246]. Bottom row: Here we show just theperformance loss (i.e., without computational cost). Each point corresponds to a different value ofτ . The fact that the points are below the curve means the metacontroller agent learns to performbetter than the iterative agent with the equivalent number of ponder steps.
Figure 4:	Relationship between the number of ponder steps and per-episode difficulty for theIN metacontroller. Each subplot’s x-axis represents the episode difficulty, as measured by thereactive controller’s loss. Each y-axis represents the number of ponder steps the metacontroller took.
Figure 5:	Test performance of the metacontroller with multiple experts on the five planetsdataset. Left: The average number of total ponder steps, for different values of τ . As with thesingle-expert metacontrollers, fewer ponder steps are taken when the cost is very high, and more aretaken when the cost is low. Right: The fraction of ponder steps taken by the MLP expert relative tothe IN expert. In the majority of cases, the metacontroller favors using the IN expert as it is muchmore reliable. The few exceptions (red squares) are cases when the cost of the IN expert is muchhigher relative to the cost of the MLP expert.
Figure 6: Training each part of the network. In each subplot, red arrows depict gradients. Dottedarrows indicate backward connections that are not part of the forward pass. Colored nodes indicateweights that are being updated. All backpropagation occurs at the very end of a full forward pass(i.e., after the control has been executed in the world). A: Training the controller and memorywith backpropagation-through-time (BPTT), beginning with the critic, and flowing to the controller,through the memory, through the relevant expert, through the controller again, and so on. B: Trainingthe manager using REINFORCE (Williams, 1992). C: Training the experts (note that each expert mayhave a different loss with respect to the outcome from the world). D: Training the critic.
Figure 7: Cost of the best iterative controller compared to the managed controller. Each pointrepresents the total cost of the best iterative agent under a particular value of τ (x-axis) versus thetotal cost achieved by the metacontroller trained with the same value of τ (y-axis). The best iterativeagent was chosen by computing the cost for all the different number of ponder steps, and thenchoosing the whichever number of ponder stpes yielded the lowest cost (i.e., finding the minimumof the curves in Figure 3, top row). In almost all cases, the managed controller achieves a lower lossthan the iterative controller: for the metacontroller with the IN expert, the cost is 11% lower than theiterative controller on average, and for the metacontroller with the true simulation expert, it is 15%lower on average.
