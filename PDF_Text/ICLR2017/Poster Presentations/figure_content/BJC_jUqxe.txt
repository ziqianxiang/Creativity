Figure 1: A sample model structure showing the sentence embedding model combined with a fullyconnected and softmax layer for sentiment analysis (a). The sentence embedding M is computed asmultiple weighted sums of hidden states from a bidirectional LSTM (h1, ..., hn), where the summa-tion weights (Ai1, ..., Ain) are computed in a way illustrated in (b). Blue colored shapes stand forhidden representations, and red colored shapes stand for weights, annotations, or input/output.
Figure 2: Heatmap of Yelp reviews with the two extreme score.
Figure 3:	Heat maps for 2 models trained on Age dataset. The left column is trained without thepenalization term, and the right column is trained with 1.0 penalization. (a) and (b) shows detailedattentions taken by 6 out of 30 rows of the matrix embedding, while (c) and (d) shows the overallattention by summing up all 30 attention weight vectors.
Figure 4:	Attention of sentence embedding on 3 different Yelp reviews. The left one is trainedwithout penalization, and the right one is trained with 1.0 penalization.
Figure 5: Effect of the number of rows (r) in matrix sentence embedding. The vertical axes indicatestest set accuracy and the horizontal axes indicates training epoches. Numbers in the legends standfor the corresponding values of r. (a) is conducted in Age dataset and (b) is conducted in SNLIdataset.
Figure 6: Hidden layer with pruned weight connections. M is the matrix sentence embedding, Mvand M h are the structured hidden representation computed by pruned weights.
Figure 7: Model structure used for textual entailment task.
