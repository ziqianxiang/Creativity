Figure 1: Block diagrams showing the computation structure of the QRNN compared with typicalLSTM and CNN architectures. Red signifies convolutions or matrix multiplications; a continuousblock means that those computations can proceed in parallel. Blue signifies parameterless functionsthat operate in parallel along the channel/feature dimension. LSTMs can be factored into (red) linearblocks and (blue) elementwise blocks, but computation at each timestep still depends on the resultsfrom the previous timestep.
Figure 2: The QRNN encoder-decoder architecture used for machine translation experiments.
Figure 3: Visualization of the final QRNN layer's hidden state vectors CL in the IMDb task, Withtimesteps along the vertical axis. Colors denote neuron activations. After an initial positive statement“This movie is simply gorgeous” (off graph at timestep 9), timestep 117 triggers a reset of mosthidden states due to the phrase “not exactly a bad story” (soon after “main weakness is its story”).
Figure 4: Left: Training speed for two-layer 640-unit PTB LM on a batch of 20 examples of 105timesteps. “RNN” and “softmax” include the forward and backward times, while “optimizationoverhead” includes gradient clipping, L2 regularization, and SGD computations.
