Figure 1: Conceptual view of the direct sparse con-volution algorithm. Computation of output value at(y, x)th position of nth output channel is highlighted.
Figure 2: Sparse convolution pseudo code. Ma-trix W has compressed sparse row (CSR) for-mat, where rowptr[n] points to the first non-zeroweight of nth output channel. For the jth non-zero weight at (n,c,r,s), W.colidx[j] containsthe offset to (c,r, s)th element of tensor in, whichis pre-computed by layout function as f (c,r,s). Ifin has CHW format, f (c,r,s) = (CHin + r)Win + S.
Figure 3: Projected performance of sparse convolution and its speedup over dense convolution for a Xeon pro-cessor and an Atom processor. conv5 direct: direct spares convolution, conv5 lowered: sparse convolutionon tensors lowered to matrices. We use the processorsâ€™ achievable FLOP/S and memory bandwidth shown inTable 1 and the compute overhead of sparse convolution measured in Section 3.2.
Figure 4: Layer-by-layer sparsity from element-wise sparsity learning (ESL), guided ESL, dynamic networksurgery (DNS), guided DNS, and structured sparsity learning (SSL). The accuracies shown in percentage aretop-1 accuracy measured with the ImageNet test set. The original AlexNet and GoogLeNet top-1 accuraciesare 57.4% and 68.9%. DNS and SSL AlexNet results are from Guo et al. (2016) and Wen et al. (2016). GDNSAlxeNet and SSL GoogLeNet results are our own but with the same code used in their papers. The shaded areamarks the useful sparsity range predicted by our model for BDW. No shaded area means sparse convolution is notuseful for the layer regardless of sparsity. In GoogLeNet, we organize layers by their types, and, within eachlayer type, layers are ordered from the earlier layers in forward propagation.
Figure 5: Performance of conv2-5 layers of AlexNet with varying sparsity on Atom C2750 (a), Xeon E5-2697v4 (b), and Xeon Phi 7250 (c). SGEMM performance of each platform serves as a proxy to the performance ofdense convolutions.
