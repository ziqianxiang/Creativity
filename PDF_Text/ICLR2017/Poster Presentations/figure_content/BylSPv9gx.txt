Figure 1: Training and dev curves for baseline (dense) and sparse training. Figure 1a includestraining and dev curves for models with larger recurrent layers with 2560 and 3072 hidden unitscompared to the 1760 dense baseline. Figure 1b plots the training and dev curves for GRU models(sparse and dense) with 2560 parameters.
Figure 2: Pruning characteristics. Figure 2a plots sparsity of recurrent layers in the network withthe same hyper-parameters used for pruning . Figure 2b plots the pruning schedule of a single layerduring a training run.
