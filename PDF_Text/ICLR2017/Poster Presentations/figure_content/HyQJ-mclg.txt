Figure 1: An overview of our incremental network quantization method. (a) Pre-trained full-precision model used as a reference. (b) Model update with three proposed operations: weightpartition, group-wise quantization (green connections) and re-training (blue connections). (c) Finallow-precision model with all the weights constrained to be either powers of two or zero. In the fig-ure, operation (1) represents a single run of (b), and operation (2) denotes the procedure of repeatingoperation (1) on the latest re-trained weight group until all the non-zero weights are quantized. Ourmethod does not lead to accuracy loss when using 5-bit, 4-bit and even 3-bit approximations in net-work quantization. For better visualization, here we just use a 3-layer fully connected network asan illustrative example, and the newly re-trained weights are divided into two disjoint groups of thesame size at each run of operation (1) except the last run which only performs quantization on there-trained floating-point weights occupying 12.5% of the model weights.
Figure 2: Result illustrations. First row: results from the 1st iteration of the proposed three oper-ations. The top left cube illustrates weight partition operation generating two disjoint groups, themiddle image illustrates the quantization operation on the first weight group (green cells), and thetop right cube illustrates the re-training operation on the second weight group (light blue cells). Sec-ond row: results from the 2nd, 3rd and 4th iterations of the INQ. In the figure, the accumulatedportion of the weights which have been quantized undergoes from 50%→75%→87.5%→100%.
