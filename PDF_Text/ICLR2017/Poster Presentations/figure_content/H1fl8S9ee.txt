Figure 1: Solution for the minimization of the α-divergence between the posterior p (in blue) and theGaussian approximation q (in red and unnormalized). Figure source Minka (2005).
Figure 3: Visualization of three policies in state space. Waterfall is indicated by top black bar. Left:policy πV B obtained with a BNN trained with VB. Avg. reward is -2.53. Middle: policy πα=0.5obtained with a BNN trained with α = 0.5. Avg. reward is -2.31. Right: policy πGP obtainedby using a Gaussian process model. Avg. reward is -2.94. Color and arrow indicate direction ofpaddling of policy when in state st , arrow length indicates action magnitude. Best viewed in color.
Figure 4: Roll-outs of algorithm 1 for two starting states s0 (top/bottom) using different types ofBNNs (left to right) with K = 75 samples for T = 75 steps. Action sequence Ao, •一，AT=75 givenby dataset for each s0. From left to right: model trained using VB,α = 0.5 and α = 1.0 respectively.
Figure 5: Ground truth and predictive distributions for two toy problems introduced in main text. Top:bi-modal prediction problem, Bottom: heteroskedastic prediction problem. Left column: Trainingdata (blue points) and ground truth functions (red). Columns 2-4: predictions generated with VB,α = 0.5 and α = 1.0, respectively.
