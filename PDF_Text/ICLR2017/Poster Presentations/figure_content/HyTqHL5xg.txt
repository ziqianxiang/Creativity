Figure 1: Left: Graphical model for one transition under state-space model assumptions. The updatedlatent state zt+1 depends on the previous state zt , control input ut , and transition parameters βt . zt+1contains all information for generating observation xt+1. Diamond nodes indicate a deterministicdependency on parent nodes. Right: Inference performed during training (or while filtering). Pastobservations are indirectly used for inference as zt contains all information about them.
Figure 2: Left: General architecture for DVBF. Stochastic transition parameters βt are inferredvia the recognition model, e.g., a neural network. Based on a sampled βt, the state transition iscomputed deterministically. The updated latent state zt+1 is used for predicting xt+1. For details, seesection 3.1. Right: Zoom into latent space transition (red box in left figure). One exemplary transitionis shown, the locally linear transition from section 3.3.
Figure 3: (a) Our DVBF-LL model trained on pendulum image sequences. The upper plots show thelatent space with coloring according to the ground truth with angles on the left and angular velocitieson the right. The lower plots show regression results for predicting ground truth from the latentrepresentation. The latent space plots show clearly that all information for representing the fullstate of a pendulum is encoded in each latent state. (b) DKF from Krishnan et al. (2015) trainedon the same pendulum dataset. The latent space plot shows that DKF fails to learn velocities of thependulum. It is therefore not able to capture all information for representing the full pendulum state.
Figure 4: (a) Latent space walk in generative mode. (b) Latent space walk in filtering mode.
Figure 5: (a) Two dimensions of 4D bouncing ball latent space. Ground truth x and y coordinates arecombined into a regular 3×3 checkerboard coloring. This checkerboard is correctly extracted by theembedding. (b) Remaining two latent dimensions. Same latent samples, colored with ball velocitiesin x and y direction (left and right image, respectively). The smooth, perpendicular coloring indicatesthat the ground truth value is stored in the latent dimension.
Figure 6: Ground truth (top), reconstructions (middle), generative samples (bottom) from identicalinitial latent state for the two bouncing balls experiment. Red bar indicates length of trainingsequences.
Figure 7: Ground truth and samples from recognition and generative model. Complete version offig. 4 with all missing samples present.
