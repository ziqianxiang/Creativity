Figure 1: Top row: randomly chosen test images from Caltech-101 silhouettes. Middle and bottomrows: corresponding reconstructions of PC-AE and AE with H = 32 binary hidden units.
Figure 2: As Fig. 2, with H = 100 on Omniglot. Difference in quality is particularly noticeable inthe 1st, 5th, 8th, and 11th columns.
Figure 3: Top three rows: the reconstructions of random test images from MNIST (H = 12), as inFig. 2. PC-AE achieves loss 105.1 here, and AE 111.2. Fourth and fifth rows: visualizations of allthe hidden units of PC-AE and AE, respectively. It is not possible to visualize the PC-AE encodingunits by the image that maximally activates them, as commonly done, because of the form of theENC function which depends on W and lacks explicit encoding weights. So each hidden unit h isdepicted by the visible decoding of the encoded representation which has bit h "on" and all other bits"off." (If this were PCA with a linear decoding layer, this would simply represent hidden unit h by itscorresponding principal component vector, the decoding of the hth canonical basis vector in RH.)Akshay Balsubramani and Yoav Freund. Optimal binary classifier aggregation for general losses. InAdvances in Neural Information Processing Systems (NIPS), 2016. arXiv:1510.00452.
Figure 4: Actual reconstruction loss to real data (red) and slack function [objective function] value(dotted green), during Adagrad optimization to learn W using the optimal E, B. Monotonicity isexpected since this is a convex optimization. The objective function value theoretically upper-boundsthe actual loss, and practically tracks it nearly perfectly.
Figure 5: Visualizations of all the hidden units of PC-AE (left) and AE (right) from Omniglot forH = 100, as in Fig. 3.
Figure 6: AE (left) and PC-AE (right) visualizations of a random subset of MNIST test data, withH = 2 real-valued hidden units, and colors corresponding to class labels (legend at left). PC-AE’sloss is 〜189 here, and that of AE is 〜179.
Figure 7: As Fig. 2, with H = 100 on Caltech-101 silhouettes.
Figure 8: As Fig. 2, with H = 100 on MNIST.
Figure 9: As Fig. 2, with H = 32 on notMNIST.
