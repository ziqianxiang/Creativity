Figure 1: Mean field criticality. (a) The mean field phase diagram showing the boundary betweenordered and chaotic phases as a function of σ/ and σ2. (b) The residual |q* - qɑa∖ as a functionof depth on a log-scale with σ2 = 0.05 and σW from 0.01 (red) to 1.7 (purple). Clear exponentialbehavior is observed. (c) The residual |c* - dab∣ as a function of depth on a log-scale. Again, theexponential behavior is clear. The same color scheme is used here as in (b).
Figure 2: Depth scales. (a) The iterative correlation map showing Cia+1 as a function of Cab for threedifferent values of σW. Green inset lines show the linearization of the iterative map about the criticalpoint, e-1∕ξc. The three curves show networks far in the ordered regime (red), at the edge of chaos(purple), and deep in the chaotic regime (blue). (b) The depth scale for information propagated in asingle input, ξq as a function of σW for σb = 0.01 (black) to σb = 0.3 (green). Dashed lines showtheoretical predictions while solid lines show measurements. (c) The depth scale for correlationsbetween inputs, ξc for the same values of σb2. Again dashed lines are the theoretical predictionswhile solid lines show measurements. Here a clear divergence is observed at the order-to-chaostransition.
Figure 3: Dropout destroys the critical point, and limits the depth to which information can propagatein a deep network. (a) The iterative correlation map showing Cla+1 as a function of clab for threedifferent values of the dropout rate P for networks tuned close to their critical point. Green insetlines show the linearization of the iterative map about the critical point, e-1/&. (b) The asymptoticvalue of the correlation map, c*, as a function of σW for different values of dropout from P = 1(black) to ρ = 0.8 (blue). We see that for all values of dropout except for P = 1, C does not show asharp transition between an ordered phase and a chaotic phase. (c) The correlation depth scale ξc asa function of σW for the same values of dropout as in (b). We see here that for all values of P exceptfor ρ = 1 there is no divergence in ξc.
Figure 4: Gradient backpropagation behaves similarly to signal forward propagation. (a) The 2-norm, ∣Vwι E||2 as a function of layer, l, for a 240 layer random network with a cross-entropyabloss on MNIST. Different values of σW from 1.0 (blue) to 4.0 (red) are shown. Clear exponentialvanishing / explosion is observed over many orders of magnitude. (b) The depth scale for gradientspredicted by theory (dashed line) compared with measurements from experiment (red dots). Simi-larity between theory and experiment is clear. Deviations near the critical point are primarily due tofinite size effects.
Figure 5: Mean field depth scales control trainable hyperparameters. The training accuracy for neu-ral networks as a function of their depth and initial weight variance, σw2 from a high accuracy (red) tolow accuracy (black). In (a) we plot the training accuracy after 200 training steps on MNIST usingSGD. Here overlayed in grey dashed lines are different multiples of the depth scale for correlatedsignal propagation, nξc. We plot the accuracy in (b) after 2000 training steps on CIFAR10 usingSGD, in (c) after 14000 training steps on MNIST using SGD, and in (d) after 300 training steps onMNIST using RMSPROP. Here we overlay in white dashed lines 6ξc .
Figure 6: The effect of dropout on trainability. The same scheme as in fig. 5 but with dropout rates of(a) P = 0.99, (b) P = 0.98, and (c) P = 0.94. Even for modest amounts of dropout We see an upperbound on the maximum trainable depth for neural networks. We continue to see good agreementbetween the prediction of our theory and our experimental training accuracy.
Figure 7: Training accuracy on MNIST after (a) 45 (b) 304 (C) 2048 and (d) 13780 steps of SGDwith learning rate 10-3.
Figure 8: Training accuracy on MNIST after(a)45 (b) 304 (c) 2048 and (d) 13780 steps ofRMSPropwith learning rate 10-5.
