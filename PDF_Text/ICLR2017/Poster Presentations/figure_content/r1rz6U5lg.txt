Figure 1: Stochastic computation graph of the Metropolis algorithm used for program super-optimization. Round nodes are stochastic nodes and square ones are deterministic. Red arrowscorresponds to computation done in the forward pass that needs to be learned while green arrowscorrespond to the backward pass. Full arrows represent deterministic computation and dashed ar-rows represent stochastic ones. The different steps of the forward pass are:(a) Based on features of the reference program, the proposal distribution q is computed.
Figure 2:	Proposal distribution training. All models learn to improve the performance of the stochas-tic optimization. Because the tasks are different between the training and testing dataset, the valuesbetween datasets can’t directly be compared as some tasks have more opportunity for optimization.
Figure 3:	Distribution of the improvement achieved when optimising a training sample from theHacker’s Delight dataset. The first column represent the evolution of the score during the optimiza-tion. The other columns represent the distribution of scores after a given number of iterations.
Figure 4:	Training of the proposal distribution on the automatically generated benchmark.
Figure 5:	Generative Model of a Transformation.
Figure 6:	Program at different stage of the optimization.
