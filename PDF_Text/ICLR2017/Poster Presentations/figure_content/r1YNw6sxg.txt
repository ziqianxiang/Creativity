Figure 1: Multiscale bilinear model. The function h maps imagesx to feature maps y(0) , the operator d downsamples the featuremaps y(l-1) to y(l), and the bilinear function f(l) predicts the nextfeature y(l). The number of channels for each feature map is n©,regardless of the scale l.
Figure 2: Dilated VGG-16 network.
Figure 3: Cars used to learn the dynamics and thefeature weights. They were also used in some of thetest experiments.
Figure 4: Novel cars used only in the test experi-ments. They were never seen during training or vali-dation.
Figure 5: Costs of test executions using various feature dynamics models, where the feature weights are op-timized with FQI. We test on cars that were used during learning (left plot) and on novel cars that were onlyused at test time (right plot). The reported values are the mean and standard error across 100 trajectories, of upto 100 time steps each. The policies based on pixel intensities use either fully connected or locally connecteddynamics, whereas all the policies based on VGG features use locally connected dynamics. The policies basedon deeper VGG features generally achieve better performance, except for the deepest feature representation,VGG conv5_3, which is not as suitable for approximating Q-ValUes. The policies based on pixel intensities andVGG conv5_3 features perform worse on the novel cars. However, VGG features ConvL2 through conv4_3achieve some degree of generalization on the novel cars.
Figure 6: Comparison of costs on test executions of prior methods against our method based on VGG conv4_3feature dynamics. These costs are from executions with the training cars; the costs are comparable whentesting with the novel cars (Table 2). The first two methods use classical image-based visual servoing (IBVS)with feature points from an off-the-shelf keypoint detector and descriptor extractor (ORB features), and withfeature points extracted from bounding boxes predicted by a state-of-the-art visual tracker (C-COT tracker),respectively. The third method trains a convolutional neural network (CNN) policy end-to-end with TrustRegion Policy Optimization (TRPO). The other methods use the servoing policy based on VGG conv4_3 featuredynamics, either with unweighted features or weights trained with TRPO for either 2 or 50 iterations. In thecase of unweighted features, we learned the weights λ and a single weight w with the cross entropy method(CEM). We report the number of training trajectories in parenthesis for the methods that require learning. ForTRPO, we use a fixed number of training samples per iteration, whereas for CEM and FQI, we use a fixednumber of training trajectories per iteration. We use a batch size of 4000 samples for TRPO, which means thatat least 40 trajectories were used per iteration (since trajectories can terminate early, i.e. in less than 100 timesteps).
Figure 7: Costs of validation executions using various feature dynamics models, where the feature weights areoptimized with FQI (left plot) or TRPO (right plot). The reported values are the mean and standard error across10 validation trajectories, of up to 100 time steps each.
