Figure 1: Subfigures (a) and (b) show the plate diagrams for the relevant latent variable models.
Figure 2: Subfigure (a) shows test (expected) reconstruction error vs training epoch for the SB-VAEand Gauss VAE on the Frey Faces dataset, subfigure (b) shows the same quantities for the samemodels on the MNIST dataset, and subfigure (c) shows the same quantities for the same models onthe MNIST+rot dataset.
Figure 3: Subfigure (a) depicts samples from the SB-VAE trained on MNIST. We show the ordered,factored nature of the latent variables by sampling from Dirichlet’s of increasing dimensionality.
Figure 4: Subfigure (a) shows results ofakNN classifier trained on the latent representations producedby each model. Subfigures (b) and (c) show t-SNE projections of the latent representations learnedby the SB-VAE and Gauss VAE respectively.
Figure 5: Sparsity in the latent representation vs sparsity in the decoder network. The Gaussian VAE‘turns off’ unused latent dimensions by setting the outgoing weights to zero (in order to dispel thesampled noise). The SB VAE, on the other hand, also has sparse representations but without decay ofthe associated decoder weights.
