Figure 1: A two-layer fully-connected neural networkcombinations of given data. Fig. 1 shows a two-layer fully-connected network. The main compu-tation kernel performs numerous vector-matrix multiplications followed by non-linear functions ineach layer. In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown thatthe power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers,which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-artneural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler &Fergus (2013); Szegedy et al. (2015); Lecun et al. (1998)), independently or as a part of convolu-tional neural networks, contain most of the weights of a DNN. For instance, the first fully-connectedlayer of VGGNet (Simonyan & Zisserman (2014)), which is composed of 13 convolution layersand three fully-connected layers, contains 100M weights out of a total of 140M. Such large storagerequirements in fully-connected networks result in copious power/energy consumption.
Figure 2: (a) shows the formation of a Mask matrix M using a 3-bit LFSR for p = 0.57. (b) showsa fully-connected layer. (c) shows a sparsely-connected layer formed based on M .
Figure 3: (a) shows the conventional architecture of a single neuron of a fully-connected network.
