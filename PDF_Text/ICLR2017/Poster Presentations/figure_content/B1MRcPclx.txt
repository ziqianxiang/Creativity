Figure 1: (1a) QRN unit, (1b) 2-layer QRN on 5-sentence story, and (1c) entire QA system (QRN andinput / output modules). x, q, y are the story, question and predicted answer in natural language, respectively.
Figure 2: The schematics of QRN and the two state-of-the-art models, End-to-End Memory Networks (N2N)and Improved Dynamic Memory Networks (DMN+), simplified to emphasize the differences among the models.
Figure 3: (top) bAbI QA dataset (Weston et al., 2016) visualization of update and reset gates in QRN ‘2r’ model(bottom two) bAbI dialog and DSTC2 dialog dataset (Bordes and Weston, 2016) visualization of update andreset gates in QRN ‘2r’ model. Note that the stories can have as many as 800+ sentences; we only show part ofthem here. More visualizations are shown in Figure 4 (bAbI QA) and Figure 5 (dialog datasets).
Figure 4:	Visualization of update and reset gates in QRN ‘2r’ model for on several tasks of bAbI QA(Table 2). We do not put reset gate in the last layer. Note that we only show some of recent sentenceshere, though the stories can have as many as 200+ sentences.
Figure 5:	Visualization of update and reset gates in QRN ‘2r’ model for on several tasks of bAbI dialog andDSTC2 dialog (Table 3). We do not put reset gate in the last layer. Note that we only show some of recentsentences here, even the dialog has more sentences.
