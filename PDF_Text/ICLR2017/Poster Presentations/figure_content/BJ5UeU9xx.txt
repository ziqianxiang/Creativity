Figure 1: Example of our visualization method: explains why the DCNN (GoogLeNet) predicts "cockatoo".
Figure 2: Simple illustration of the sampling procedure in algorithm 1. Given the input image x, we selectevery possible patch Xw (in a sliding window fashion) of size k × k and place a larger patch Xw of size l × laround it. We can then conditionally sample Xw by conditioning on the surrounding patch Xw.
Figure 3: Visualization of the effects of marginal versus conditional sampling using the GoogLeNetclassifier. The classifier makes correct predictions (ostrich and saxophone), and we show the evidence for (red)and against (blue) this decision at the output layer. We can see that conditional sampling gives more targetedexplanations compared to marginal sampling. Also, marginal sampling assigns too much importance on pixelsthat are easily predictable conditioned on their neighboring pixels.
Figure 4: Visualization of how different window sizes influence the visualization result. We used theconditional sampling method and the AlexNet classifier with l = k + 4 and varying k. We can see that evenwhen removing single pixels (k = 1), this has a noticeable effect on the classifier and more important pixels geta higher score. By increasing the window size we can get a more easily interpretable, smooth result until theimage gets blurry for very large window sizes.
Figure 5: Visualization of feature maps from thee different layers of the GoogLeNet (l.t.r.: ”conv1/7x7_s2”,”inception_3a/output”, ”inception_5b/output”), using conditional sampling and patch sizes k = 10 and l = 14(see alg. 1). For each feature map in the convolutional layer, we first evaluate the relevance for every single unit,and then average the results over all the units in one feature map to get a sense of what the unit is doing as awhole. Red pixels activate a unit, blue pixels decreased the activation.
Figure 6: Visualization of three different feature maps, taken from the ”inception_3a/output” layer of theGoogLeNet (from the middle of the network). Shown is the average relevance of the input features over allactivations of the feature map. We used patch sizes k = 10 and l = 14 (see alg. 1). Red pixels activate a unit,blue pixels decreased the activation.
Figure 7: Visualization of the support for the top-three scoring classes in the penultimate- and outputlayer. Next to the input image, the first row shows the results with respect to the penultimate layer; the secondrow with respect to the output layer. For each image, we additionally report the values of the units. We used theAlexNet with conditional sampling and patch sizes k = 10 and l = 14 (see alg. 1). Red pixels are evidence fora class, and blue against it.
Figure 8: Comparison of the prediction visualization of different DCNN architectures. For two inputimages, we show the results of the prediction difference analysis when using different neural networks - theAlexNet, GoogLeNet and VGG network.
Figure 9: Visualization of the support for the correct classification ”HIV”, using the Prediction Differ-ence method and Logistic Regression Weights. For an HIV sample, we show the results with the predictiondifference (first row), and using the weights of the logistic regression classifier (second row), for slices 29 and 40(along the first axis). Red are positive values, and blue negative. For each slice, the left image shows the originalimage, overlaid with the relevance values. The right image shows the original image with reversed colors andthe relevance values. Relevance values are shown only for voxels with (absolute) relevance value above 15% ofthe (absolute) maximum value.
Figure 10: Prediction difference visualization for different samples. The first four samples are of the class”healthy”; the last four of the class ”HIV”. All images show slice 39 (along the first axis). All samples arecorrectly classified, and the results show evidence for (red) and against (blue) this decision. Prediction differencesare shown only for voxels with (absolute) relevance value above 15% of the (absolute) maximum value.
Figure 11: Visualization results across different slices of the MRI image, using the same input image asshown in 9. Prediction differences are shown only for voxels with (absolute) relevance value above 15% of the(absolute) maximum value.
Figure 12: How the patch size influences the visualization. For the input image (HIV sample, slice 39 alongthe first axis) we show the visualization with different patch sizes (k in alg. 1). Prediction differences are shownonly for voxels with (absolute) relevance value above 15% of the (absolute) maximum (for k = 2 it is 10%).
Figure 13: Results on 34 randomly chosen ImageNet images. Middle columns: original image; left columns:sensitivity maps (Simonyan et al., 2013) where the red pixels indicate high sensitivity, and white pixels mean nosensitivity (note that we show the absolute values of the partial derivatives, since the sign cannot be interpretedlike in our method); right columns: results from our method. For both methods, we visualize the results withrespect to the correct class which is given above the image. In brackets we see how the classifier ranks this class,i.e., a (1) means it was correctly classified, whereas a (4) means that it was misclassified, and the correct classwas ranked fourth. For our method, red areas show evidence for the correct class, and blue areas show evidenceagainst the class (e.g., the scuba diver looks more like a tea pot to the classifier).
