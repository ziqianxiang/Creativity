Figure 1: Illustration of the encoderand decoder RNNs used in our exper-iments. In this example, the input tothe encoder is x = (..., A, B, <EOS>)and the output of the decoder is y =(U, V, W, . . . ). We use four differentLSTMs for the bottom and top layersof the encoder and decoder networks.
Figure 2: Results for synthetic experiments. (a): Comparison of BPG with and without maximum likelihood(ML) initialisation and BPG without importance sampling (BPG-NIS). The dotted line indicates performanceof ML alone. (b): Comparison of BPG with its online counterparts OPG. We compare both methods usinga constant estimator (CONST) for the value function and GTD(λ). (c): Comparison of BPG with differentvalues of λ. All curves were averaged over 10 experiments where the training set was picked randomly from apool. The test set was the same in all 10 experiments. The error bars indicate one standard error.
