Figure 1: Both the actor and the critic are encoder-decoder networks. The actor receives an inputsequence X and produces samples Y which are evaluated by the critic. The critic takes in theground-truth sequence Y as input to the encoder, and takes the input summary (calculated usingan attention mechanism) and the actor's prediction yt as input at time step t of the decoder. Thevalues Q1,Q2,…，QT computed by the critic are used to approximate the gradient of the expectedreturns with respect to the parameters of the actor. This gradient is used to train the actor to optimizethese expected task specific returns (e.g., BLEU score). The critic may also receive the hidden stateactivations of the actor as input.
Figure 2: Progress of log-likelihood (LL), RE-INFORCE (RF) and actor-critic (AC) training interms of BLEU score on the training (train) and val-idation (valid) datasets. LL* stands for the anneal-ing phase of log-likelihood training. The curvesstart from the epoch of log-likelihood pretrainingfrom which the parameters were initialized.
Figure 3: The best 3 words according to the critic at intermediate steps of generating a translation.
