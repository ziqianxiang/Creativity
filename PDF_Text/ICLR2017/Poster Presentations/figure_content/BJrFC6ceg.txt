Figure 1: Marginal distribution of all sub-pixel values in CIFAR-10. The edge value of 255 ismuch more frequent than its neighbouring values: This is easy to model using our rounding basedapproach, but harder using continuous or truncated distributions.
Figure 2: Like van den Oord et al. (2016c), our model follows a two-stream (downward, anddownward+rightward) convolutional architecture with residual connections; however, there are twosignificant differences in connectivity. First, our architecture incorporates downsampling and up-sampling, such that the inner parts of the network operate over larger spatial scale, increasing com-putational efficiency. Second, we employ long-range skip-connections, such that each k-th layerprovides a direct input to the (K - k)-th layer, where K is the total number of layers in the net-work. The network is grouped into sequences of six layers, where most sequences are separated bydownsampling or upsampling.
Figure 3: Samples from our PixelCNN model trained on CIFAR-10.
Figure 4: Class-conditional samples from our PixelCNN for CIFAR-10 (left) and real CIFAR-10images for comparison (right).
Figure 6: Training curves for our model with logistic mixture likelihood versus our model withsoftmax likelihood.
Figure 7: Training curves for our model with and without short-cut connections.
Figure 8: Samples from intentionally overfitted PixelCNN model trained on CIFAR-10, with trainlog-likelihood of 2.0 bits per dimension: Overfitting does not result in great perceptual quality.
