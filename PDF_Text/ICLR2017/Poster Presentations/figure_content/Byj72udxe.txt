Figure 1: Illustration of the pointer sentinel-RNN mixture model. g is the mixture gate which usesthe sentinel to dictate how much probability mass to give to the vocabulary.
Figure 2: Visualization of the pointer sentinel-RNN mixture model. The query, produced fromapplying an MLP to the last output of the RNN, is used by the pointer network to identify likelymatching words from the past. The nodes are inner products between the query and the RNNhidden states. If the pointer component is not confident, probability mass can be directed to theRNN by increasing the value of the mixture gate g via the sentinel, seen in grey. If g = 1 then onlythe RNN is used. If g = 0 then only the pointer is used.
Figure A1: Mean difference in log perplexity on PTB when using the pointer sentinel-LSTMcompared to the LSTM model. Words were sorted by frequency and split into equal sized buckets.
Figure A2: In predicting the fall season has been a good one especially for those retailers, thepointer component suggests many words from the historical window that would fit - retailers, in-vestments, chains, and institutions. The gate is still primarily weighted towards the RNN componenthowever.
Figure A3: In predicting the national cancer institute also projected that overall u.s. mortality, thepointer component is focused on mortality and rates, both of which would fit. The gate is stillprimarily weighted towards the RNN component.
Figure A4: In predicting people do n’t seem to be unhappy with it he said, the pointer componentcorrectly selects said and is almost equally weighted with the RNN component. This is surprisinggiven how frequent the word said is used within the Penn Treebank.
Figure A5: For predicting the federal government has had to pump in $ N billion, the pointercomponent focuses on the recent usage of billion with highly similar context. The pointer componentis also relied upon more heavily than the RNN component - surprising given the frequency of billionwithin the Penn Treebank and that the usage was quite recent.
Figure A6: For predicting hunki ’s ghost sometimes runs through the e ring dressed like gen. nor-iega, the pointer component reaches 97 timesteps back to retrieve gen. douglas. Unfortunately thisprediction is incorrect but without additional context a human would have guessed the same word.
Figure A7: For predicting mr. iverson, the pointer component has learned the ability to point to thelast name of the most recent named entity. The named entity also occurs 45 timesteps ago, which islonger than the 35 steps that most language models truncate their backpropagation to.
Figure A8: For predicting mr. rosenthal, the pointer is almost exclusively used and reaches back 65timesteps to identify bruce rosenthal as the person speaking, correctly only selecting the last name.
Figure A9: For predicting in composite trading on the new york stock exchange yesterday inte-grated, the company Integrated and the hunki token are primarily attended to by the pointer com-ponent, with nearly the full prediction being determined by the pointer component.
