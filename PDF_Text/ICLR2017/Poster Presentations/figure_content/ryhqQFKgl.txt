Figure 1: MUS-ROVER's self-learning loop (the kth iteration). The teacher (discriminator) takesas inputs the student,s latest style Psk-Ii and the input style p, and identifies a feature φ throughwhich the two styles manifest the largest gap D(∙∣∣∙). The identified feature is then made into a rule(a constraint set Γk), and augments the ruleset {Γi}k=ι. The student (generator) takes as input thehkiaugmented ruleset to update its writing style into Pstu and favors creativity, i.e. more possibilities,by maximizing the Tsallis entropy Sq subject to the rule constraints. In short, the teacher extractsrules while the student applies rules; both perform their tasks by solving optimization problems.
Figure 2: MUS-ROVER Irs two-dimensional memory (left): the length axis enumerates n-gramorders; the depth axis enumerates features; and every cell is a feature distribution. Memory mask(right): 0 marks the removal of the corresponding cell from feature selection, which is caused by ahierarchical filter or the regularity condition or (contradictory) duplication.
Figure 3: Gap trajectories for two features. The dashed black lines show two different satisfactorygaps (Y = 0.5 and 0.1). The bottom charts show the informationally implied hierarchies.
Figure 4: The relative performance of the selected rule (pointed) among the pool of all cells in the2D memory. A desired rule has: higher confidence (measured by the number of examples, brighterregions in the first row), more regularity (measured by Shannon entropy, darker regions in the secondrow), and larger style gap (measured by KL divergence, brighter regions in the bottom two rows).
Figure 5: Visualization of Bach,s music mind for writing chorales. The underlying DAG representsthe conceptual hierarchy (note: edges always point downwards). Colors are used to differentiaterule activations from different n-gram settings. We have enlarged N = {1, 2,..., 10} to allow evenlonger-term dependencies.
