Figure 1: Comparison between the two loss functions represented by eq.2 and 7. (a) is the originalloss, (b) is the new loss that discards negative correlations given for different λ values. It can be seenλ 二 10 reaches a plateau when approximating to ∏. (C) and (d) shows the directions of the gradientsfor the two loss functions above. For instance, a red arrow coming from a green ball represents thegradient of the loss between the red and green balls with respect to the green one. In (d) most of thearrows disappear since the loss in (b) only applies to angles smaller than ∏.
Figure 3: Effects of local and global regularization on the Alexnet, VGG-16 and 50-layer-ResNetweights. The regularized versions reach higher decorrelation bounds (in terms of minimum angle)than the unregularized counterparts.
Figure 2: 2D toy dataset regularized with global loss (eq. 2) and local loss (eq. 7). (a) shows theinitial 2D randomly generated dataset. (b) the dataset after 300 regularization steps using the globalloss and (c) using the local loss. (d) the evolution of the mean nearest neighbor angle for the globalloss (b) and the local loss (c). (e) and (f) correspond to (b) and (c) but using gradient ascent insteadof gradient descent as a sanity-check.
Figure 4: (a) The evolution of the error rate on the MNIST validation set for different regularizationmagnitudes. It can be seen that for γ = 1 it reaches the best error rate (1.45%) while the unreg-ularized counterpart (γ = 0) is 1.74%. (b) Measures the overfitting of the model for different γ,confirming that higher regularization rates decrease overfitting.
Figure 5: (a) Shows the minimum error rate for different λ values. (b) Classification error on MNISTfor different loss functions. Not regularizing negative correlated feature weights (eq. 7) results inbetter test error than regularizing them (eq.2).
Figure 6: Wide ResNet error rate on Cifar10 and Cifar100. OrthoReg shows better performancethan the base regularizer (all feature maps), and the unregularized counterparts.
