Figure 1: The agent's RNN architecture that represents a policy. The environment is initialized witha latent vector h. At time step t, the environment produces a state st, and the agent takes as input Stand the previously sampled action a— and produces a distribution over the next action ∏θ(at | st).
Figure 2: Average reward during training for MENT (green) and UREX (blue). We find the besthyper-parameters for each method, and run each algorithm 5 times with random restarts. The curvespresent the average reward as well as the single standard deviation region clipped at the min andmax.
Figure 3: A graphical representation of a trained addition agent. The agent begins at the top leftcorner ofa 2 X n grid of ternary digits. At each time step, it may move to the left, right, up, or down(observing one digit at a time) and optionally write to output.
Figure 4: This plot shows the variance of the importance weights in the UREX updates as well asthe average reward for two successful runs. We see that the variance starts off high and reaches nearzero towards the end when the optimal policy is found. In the first plot, we see a dip and rise in thevariance which corresponds to a plateau and then increase in the average reward.
Figure 5: In this plot we present the average performance of UREX (blue) and MENT (green)over 100 repeats of a bandit-like task after choosing optimal hyperparameters for each method. Inthe task, the agent chooses one of 10,000 actions at each step and receives a payoff correspondingto the entry in a reward vector r = (r1, ..., r10,000) such that ri = uiβ, where ui ∈ [0, 1) hasbeen sampled randomly and independently from a uniform distribution. We parameterize the policywith a weight vector θ ∈ R30 such that ∏θ(a) 8 exp(φ(a) ∙ θ), where the basis vectors φ(a) ∈R30 for each action are sampled from a standard normal distribution. The plot shows the averagerewards obtained by setting β = 8 over 100 experiments, consisting of 10 repeats (where r andΦ = (φ(1), . . . , φ(10,000)) are redrawn at the start of each repeat), with 10 random restarts withineach repeat (keeping r and Φ fixed but reinitializing θ). Thus, this task presents a relatively simpleproblem with a large action space, and we again see that UREX outperforms MENT.
