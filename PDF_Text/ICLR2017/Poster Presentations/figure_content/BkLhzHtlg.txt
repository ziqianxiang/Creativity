Figure 1: Left: A 3D depiction of our network unrolled for 3 timesteps. The highlighted cells show the pathfrom an input through a classification cell to a motion prediction output. During training, motion predictionloss Lx is computed at every timestep, and classification loss Ly is computed only at frames for which labelsare provided. The diagonal connections between discriminative and generative cells enable higher levels ofthe network to represent high level information. Vector V represents agent,s sensory input, X its motion, hits internal state, and y labeled actions. Right: A zoom in on the blue and green cells showing the recurrentstate (horizontal arrows) and inputs to the recurrent cell function f. Merging of arrows represents vectorconcatenation, and branching vector duplication.
Figure 2: Snapshots from the three labeled datsets used for our evaluation and a list of control laws used togenerate synthetic fly trajectories. The table summarizes the statistics of each experimental dataset, where total# frames sums over all trials (videos / text documents) within an experiment and agents within a trial, total #instances sums over all action classes, and % frames is the percent of frames in labeled sequences containingactions of interest. IAM-OnDB* is a subset OfIAM-OnDB with additional annotations for 10 of its trials.
Figure 3: Left: Sensory input V for fruit flies represents how a fly sees other flies and chamber walls, their motorcontrol X lets them move their body along 8 dimensions (incl. right_wing_ang/len). Right: Motor control Xfor handwriting is represented as vector (dx, dy) along with binary stroke visibility Z (pen on/off whiteboard).
Figure 4: a) Performance of model trained With (solid, BESNet) and without (dashed, BENet) motion pre-diction, showing that BESNet requires significantly fewer labels to match the performance of BENet. b) Ourmodel reaches performance competitive with Eyjolfsdottir et al. (2014), without handcrafting or context fromfuture frames. c) Input x, label y, and classification score y, colored according to character label, showing highconfusion at the beginning of characters, partly explaining the lower F1-frame performance on IAM-OnDB.
Figure 5: a) Network variants used in experiments (compare BESNet to highlighted cells in its unrolled visual-ization in Figure 1). b) 1-step motion prediction performance on FlyBowl testset, see text above for explanation.
Figure 6: a) 10 x 20-frame lookaheads (simulations) for each test fly from its current location, demonstratingthe non-deterministic nature of the motion prediction. The ground truth 20-frame future trajectory is outlinedin black for comparison. b) shows trajectories of 20 flies simulated for 1000 frames, and C) shows 1000-frametrajectories for 20 real flies interacting. The simulation shows that the model has learnt a preference for stayingnear the boundary and to avoid walking through the boundary.
Figure 8: Comparison between synthetic fly (ground truth) and simulation by our model. The wing angles,distance to object, and left/right turn show the agentâ€™s motion over time, and the two hidden units indicate thatthe model has learnt to represent control laws 4 and 5 used to generate the synthetic trajectories.
Figure 7: Left: Text generated by our model, one vector at a time (approximately 20 vectors per character).
Figure 9: Left: Hidden state values of a 3 level model trained without any labels on IAM-OnDB, reducedto 2 dimensions using tSNE mapping. The network discovers writer identity at the highest level, while lowerlevel phenomena such as stroke length are represented at lower levels. Right: tSNE mapped input, output, andhidden state values of FlyBowl model (trained without any labels), colored by gender and male wing extension.
