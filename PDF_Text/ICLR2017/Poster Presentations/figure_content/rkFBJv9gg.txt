Figure 1: (Left) Bottom-level weights learned by a two-layer ReLU network trained on 16,384-samples windows (≈ 1/3 seconds) of raw audio with `2 regularized (λ = 1) square loss for multi-label note classification on raw audio recordings. (Middle) Magnified view of the center of each setof weights. (Right) The truncated frequency spectrum of each set of weights.
Figure 2: (Left) Heatmap visualization of local alignment costs between the synthesized andrecorded spectrograms, with the optimal alignment path in red. The block from x = 0 to x = 100frames corresponds to silence at the beginning of the recorded performance. The slope of the align-ment can be interpreted as an instantaneous tempo ratio between the recorded and synthesized per-formances. The curvature in the alignment between x = 100 and x = 175 corresponds to anextension of the first notes by the performer. (Right) Annotation of note onsets on the spectrogramof the recorded performance, determined by the alignment shown on the left.
Figure 3:	(Left) Features learned by a 2-layer ReLU network trained on small monophonic subsetof MusicNet. (Right) Features learned by the same network, trained on the full MusicNet dataset.
Figure 4:	Precision-recall curves for the convolutional network on the test set. Curves are eval-uated on subsets of the test set consisting of all data points (blue); points with exactly one label(monophonic; green); and points with exactly three labels (red).
Figure 5: (Left) The frequency distribution of notes in MusicNet. (Right) The frequency distributionof learned nodes in a 500-node, two-layer ReLU network.
Figure 6: The linear spectrogrammodel.
Figure 7: The 500 node, 2048raw sample MLP.
Figure 8: The 2500 node, 2048raw sample MLP.
Figure 9: The average poolingmodel.
Figure 10: The 500 node, 16384raw sample MLP.
Figure 11: The convolutionalmodel.
