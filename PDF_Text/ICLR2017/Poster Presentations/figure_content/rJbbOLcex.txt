Figure 1: (a) The unrolled TopicRNN architecture: x1, ..., x6 are words in the document, ht is thestate of the RNN at time step t, xi ≡ yi-1, l1, ..., l6 are stop word indicators, and θ is the latentrepresentation of the input document and is unshaded by convention. (b) The TopicRNN modelarchitecture in its compact form: l is a binary vector that indicates whether each word in the inputdocument is a stop word or not. Here red indicates stop words and blue indicates content words.
Figure 2: Inferred distributions using TopicGRU on three different documents. The content ofthese documents is added on the appendix. This shows that some of the topics are being picked updepending on the input document.
Figure 3: Clusters of a sample of 10000 movie reviews from the IMDB 100K dataset using Top-icRNN as feature extractor. We used K-Means to cluster the feature vectors. We then used PCAto reduce the dimension to two for visualization purposes. red is a negative review and green is apositive review.
