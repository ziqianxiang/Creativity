Figure 1: On top we show the distribution of a pretrained network. On the right the same distributionafter retraining. The change in value of each weight is illustrated by a scatter plot.
Figure 2: We show the results of optimizing hyper-parameters with spearmint. Specifically, we plotthe accuracy loss of a re-trained network against the compression rate. Each point represents onehyper-parameter setting. The guesses of the optimizer improve over time. We also present the resultsof other methods for comparison. Left: LeNet-300-100 Right: LeNet-5-Caffe.
Figure 3: Illustration of our mixture model compression procedure on LeNet-5-Caffe. Left: Dy-namics of Gaussian mixture components during the learning procedure. Initially there are 17 com-ponents, including the zero component. During learning components are absorbed into other com-ponents, resulting in roughly 6 significant components. Right: A scatter plot of initial versus fi-nal weights, along with the Gaussian components’ uncertainties. The initial weight distribution isroughly one broad Gaussian, whereas the final weight distribution matches closely the final, learnedprior which has become very peaked, resulting in good quantization properties.
Figure 4: Illustration of the process described in A.2. IC is represented byrelative indexes(diff). If the a relative index is larger than 8(= 23), A will befilled with an additional zero. Figure from Han et al. (2015a).
Figure 5: Gamma distribution with λ* = 100. α and β correspond to different choices for thevariance of the distribution.
Figure 6: Beta distribution with π*=o = 0.9. a and β correspond to different choices for the PSeUdo-count.
Figure 7: Convolution filters from LeNet-5-Caffe. Left: Pre-trained filters. Right: Compressedfilters. The top filters are the 20 first layer convolution weights; the bottom filters are the 20 by 50convolution weights of the second layer.
Figure 8: Feature filters for LeNet-300-100. Left: Pre-trained filters. Right: Compressed filters.
