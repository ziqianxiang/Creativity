Figure 1: Overall architecture of the proposed network. (a) illustrates MCnet without the Motion-Content Residual skip connections, and (b) illustrates MCnet with such connections. Our networkobserves a history of image differences through the motion encoder and last observed image throughthe content encoder. Subsequently, our network proceeds to compute motion-content features andcommunicates them to the decoder for the prediction of the next frame.
Figure 2: Quantitative comparison between MCnet and ConvLSTM baseline with and without multi-scale residual connections (indicated by "+ RES"). Given 10 input frames, the models predict 20frames recursively, one by one. Left column: evaluation on KTH dataset (Schuldt et al., 2004). Rightcolum: evaluation on Weizmann (Gorelick et al., 2007) dataset.
Figure 3:	Qualitative comparison between our MCNet model and ConvLSTM. We display predictionsstarting from the 12th frame, in every 3 timesteps. The first 3 rows correspond to KTH dataset for theaction of jogging and the last 3 rows correspond to Weizmann dataset for the action of walking.
Figure 4:	Quantitative comparison between our model, convolutional LSTM Shi et al. (2015), andMathieu et al. (2015). Given 4 input frames, the models predict 8 frames recursively, one by one.
Figure 5: Qualitative comparisons among MCnet and ConvLSTM and Mathieu et al. (2015). Wedisplay predicted frames (in every other frame) starting from the 5th frame. The green arrows denotethe top-30 closest optical flow vectors within image patches between MCnet and ground-truth. Moreclear motion prediction can be seen in the project website.
Figure 6: Qualitative comparisons on KTH testset. We display predictions starting from the 12thframe, for every 3 timesteps. More clear motion prediction can be seen in the project website.
Figure 7: Qualitative comparisons on KTH testset. We display predictions starting from the 12thframe, for every 3 timesteps. More clear motion prediction can be seen in the project website.
Figure 8: Qualitative comparisons on UCF-101. We display predictions (in every other frame) startingfrom the 5th frame. The green arrows denote the top-30 closest optical flow vectors within imagepatches between MCnet and ground-truth. More clear motion prediction can be seen in the projectwebsite.
Figure 9: Qualitative comparisons on UCF-101. We display predictions (in every other frame) startingfrom the 5th frame. The green arrows denote the top-30 closest optical flow vectors within imagepatches between MCnet and ground-truth. More clear motion prediction can be seen in the projectwebsite.
Figure 10: Qualitative comparisons on UCF-101. We display predictions (in every other frame)starting from the 5th frame. The green arrows denote the top-30 closest optical flow vectors withinimage patches between MCnet and ground-truth. More clear motion prediction can be seen in theproject website.
Figure 11:	Qualitative comparisons on KTH testset. We display predictions starting from the 12thframe, in every 3 timesteps. More clear motion prediction can be seen in the project website.
Figure 12:	Extended quantitative comparison including a baseline based on copying the last observedframe through time.
Figure 13:	Extended quantitative comparison on UCF101 including a baseline based on copying thelast observed frame through time using motion based pixel mask.
Figure 14: Quantitative comparison on UCF101 using motion based pixel mask, and separatingdataset by average '2-norm of time difference between target frames.
Figure 15: Quantitative comparison on UCF101 using motion based pixel mask, and separatingdataset by average '2-norm of time difference between target frames.
