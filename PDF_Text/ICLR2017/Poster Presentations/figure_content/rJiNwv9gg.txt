Figure 1: Effects of rounding and differentiable alternatives when used as replacements in JPEGcompression. A: A crop of an image before compression (GoToVan, 2014). B: Blocking artefactsin JPEG are caused by rounding of DCT coefficients to the nearest integer. Since rounding is usedat test time, a good approximation should produce similar artefacts. C: Stochastic rounding to thenearest integer similar to the binarization ofToderici et al.(2016a). D: Uniform additive noise (Balleet al., 2016).
Figure 2: Illustration of the compressive autoencoder architecture used in this paper. Inspired bythe work of Shi et al. (2016), most convolutions are performed in a downsampled space to speed upcomputation, and upsampling is performed using sub-pixel convolutions (convolutions followed byreshaping/reshuffling of the coefficients). To reduce clutter, only two residual blocks of the encoderand the decoder are shown. Convolutions followed by leaky rectifications are indicated by solidarrows, while transparent arrows indicate absence of additional nonlinearities. As a model for thedistributions of quantized coefficients we use Gaussian scale mixtures. The notation C X K X Krefers to K X K convolutions with C filters. The number following the slash indicates stride in thecase of convolutions, and upsampling factors in the case of sub-pixel convolutions.
Figure 3: A: Scale parameters obtained by finetuning a compressive autoencoder (blue). More fine-grained control over bit rates can be achieved by interpolating scales (gray). Each dot correspondsto the scale parameter of one coefficient for a particular rate-distortion trade-off. The coefficientsare ordered due to the incremental training procedure. B: Comparison of incremental training ver-sus non-incremental training. The learning rate was decreased after 116,000 iterations (bottom twolines). Non-incremental training is initially less stable and shows worse performance at later iter-ations. Using a small learning rate from the beginning stabilizes non-incremental training but isconsiderably slower (top line).
Figure 4: Comparison of different compression algorithms with respect to PSNR, SSIM, and MS-SSIM on the Kodak PhotoCD image dataset. We note that the blue line refers to the results ofToderici et al. (2016b) achieved without entropy encoding.
Figure 5: Closeups of images produced by different compression algorithms at relatively low bitrates. The second row shows an example where our method performs well, producing sharper linesthan and fewer artefacts than other methods. The fourth row shows an example where our methodstruggles, producing noticeable artefacts in the hair and discolouring the skin. At higher bit rates,these problems disappear and CAE reconstructions appear sharper than those of JPEG 2000 (fifthrow). Complete images are provided in Appendix A.6.
Figure 6: Results of a mean opinion score test.
Figure 7: A comparison of different JPEG modes on the Kodak PhotoCD image dataset. OptimizedHuffman tables perform better than default Huffman tables.
Figure 8: To disentangle the effects of quantization and dimensionality reduction, we reconstructedimages with quantization disabled. A: The original uncompressed image. B: A reconstruction gen-erated by a compressive autoencoder, but with the rounding operation removed. The dimensionalityof the encoder's output is 3Ã— smaller than the input. C: A reconstruction generated by the samecompressive autoencoder. While the effects of dimensionality reduction are almost imperceptible,quantization introduces visible artefacts.
Figure 9: Comparison of CAEs optimized for low, medium, or high bit rates.
Figure 10: An alternative to our approach is to replace the rounding function with additive uni-form noise during training (Bane et al., 2016). Using mean-squared error for measuring distortion,optimizing rate-distortion this way is equivalent to training a variational autoencoder (Kingma &Welling, 2014) with a Gaussian likelihood and uniform encoder (Section 2.4). Using the same train-ing proecedure autoencoder architecture for both approaches (here trained for high bit-rates), we findthat additive noise performs worse than redefining derivatives as in our approach. Rounding-basedquantization is used at test time in both approaches.
