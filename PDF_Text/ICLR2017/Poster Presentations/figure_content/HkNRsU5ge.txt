Figure 1: A: An ordinary deep network, which consists of an alternating sequence of linear operationswi(x), and nonlinear transforms hi(x). B: The Temporal-Difference Network, described in Section 3.1,computes the exact same function as network A, but communicates differences in activation betweenlayers. C: An approximation of network A where activations are rounded before being sent to thenext layer. D: The Sigma-Delta Network combines the modifications of B and C. Functionally, it isidentical to the Rounding Network, but it can compute forward passes more cheaply when input data istemporally redundant.
Figure 2: The Results of the “Random Network” experiment de-scribed in Section 5.1. Left: A plot of the layerwise scales. Greylines show randomly sampled scales, and coloured lines show op-timal scales for different values of λ. Right: Gray dots show theerror-scale tradeoffs of network instantiations using the (gray) ran-domly sampled rescalings on the left. Coloured lines show the op-timization trajectory under different values of λ, starting with theinitial state (•), and ending with ×.
Figure 3: Some Sam-ples from the Temporal-MNIST dataset. Eachcolumn shows a snippetof adjacent frames.
Figure 4: A visualization of our error-computation tradeoff curve for MNIST and our Temporal-mnistdataset. Plot 1: Each point on the line for the Rounding (blue) and Sigma-Delta (green) networkcorrespond to the performance of the network for a different value of the error-computation tradeoffparameter λ, ranging from λ = 10-10 (in the high-computation, low-error regime) to λ = 10-5 (inthe low-computation, high-error regime). The red line indicates the performance of the original, non-discretized network. The red dot on the right indicates the number of flops required for a full forwardpass when doing dense multiplication, and the dot on the left indicates the number of flops whenfactoring in layer sparsity. Note that for the Rounding and Sigma-Delta networks, we count the numberof additions, and for the original network we count the numbers of multiplications and additions (asper Section 3.3). Plot 2: The same, but on the Temporal-MNIST dataset. We see that the Sigma-Deltanetwork uses less computation thanks to the temporal redundancy in the data. Plots 3 and 4: Half of theoriginal network’s Ops were multiplies, which are more computational costly than the additions of theRounding and Sigma-Delta networks. In these plots the x-axis is rescaled according to the energy usecalculations of Horowitz (2014), assuming the weights and parameters of the network are implementedwith 32-bit integer arithmetic. Numbers are in Appendix E.
Figure 5: A comparison of the Original VGG Net with the Rounding and Sigma-Delta Networksusing the same parameters, after scale-optimization. Top: Frames taken from two videos from theILSVRC2015 dataset. The two videos, with 201 frames in total, were spliced together. The first has astatic background, and the second has more motion. Below every second image is the label generatedfor that image by VGGnet and the Sigma-Delta network (which is functionally equivalent to the Round-ing Network, though numerical errors can lead to small changes, not shown here). Scale parameterswere trained on separate videos. Second Plot: A comparison of the computational cost per-frame. Theoriginal VGG network has a fixed cost. The Sigma-Delta network has a cost that varies with the amountof action in the video. The spike in computation occurs at the point where the videos are spliced to-gether. We can see that the Sigma-Delta network does more computation for the second video, in whichthere is more movement. During the first video it performs about 11 times less computation than theOriginal Network, during the second, about 4 times less. The difference would be more pronounced ifwe were to count energy use, as we did in Figure 4. Third Plot: A plot of the cumulative mean error(over frames) of the Sigma-Delta/Rounding networks, as compared to the Original VGGnet. Most ofthe time, it gets the same result (Top-1) out of 1000 possible categories. On almost every frame, theguess of the Sigma-Delta network is one of the top-5 guesses of the original VGGNet. Fourth Plot:A breakdown of how much of the computational cost of each network comes from each layer. FifthPlot: The layer-wise ratio of the computational cost of the Sigma-Delta net to the rounding net. We hadexpected (and hoped) this ratio to become very low in the upper layers, as the high-level features shouldnot change much between frames. However this was not the case (as the ratio remains between 0.2 and
Figure 6: Top-Left: A heatmap showing the L1-distances between the feature representations (post-nonlinearity) of adjacent frames from the video in Figure 5 at different layers (rows) and frames(columns). The input is considered to be layer 0. Feature representations have been L1-normalizedper-layer Bottom Left: The L1-Norms (which are 1 due to the normalization) and inter-frame L1-Distances for each layer, averaged over frames. Top and Bottom Right: The same measurements,with the cosine-similarity metric instead of L1. We note from these plots that the inter-frame differenceis not much smaller in higher layers than it is at the pixel level, and that in the lower layers, featurerepresentations of neighbouring frames are significantly more dissimilar than they are at the pixel level.
