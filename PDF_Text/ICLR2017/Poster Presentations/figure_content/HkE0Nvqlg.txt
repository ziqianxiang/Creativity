Figure 1: Three versions of a latent variable attention model: (a) A standard soft-selection attention network,(b) A Bernoulli (sigmoid) attention network, (c) A linear-chain structured attention model for segmentation.
Figure 2: Algorithms for linear-chain CRF: (left) computation of forward-backward tables α, β, and marginalprobabilities p from potentials θ (forward-backward algorithm); (right) backpropagation of loss gradients withrespect to the marginals VpL . C denotes the state space and 〈t〉 is the special start/stop state. Backpropagationuses the identity VlLog p = pVpL to calculate VθL = VlLog pVlθog p, where is the element-wise multiplication.
Figure 3: Visualization of the source self-attention distribution for the simple (left) and structured (right)attention models on the tree transduction task. $ is the special root symbol. Each row delineates the distributionover the parents (i.e. each row sums to one). The attention distribution obtained from the parsing marginals aremore able to capture the tree structure—e.g. the attention weights of closing parentheses are generally placedon the opening parentheses (though not necessarily on a single parenthesis).
Figure 4: Visualization of the source attention distribution for the simple (top left), sigmoid (top right), andstructured (bottom left) attention models over the ground truth sentence on the character-to-word translationtask. Manually-annotated alignments are shown in bottom right. Each row delineates the attention weightsover the source sentence at each step of decoding. The sigmoid/structured attention models are able learn animplicit segmentation model and focus on multiple characters at each time step.
Figure 5: Visualization of the attention distribution over supporting fact sequences for an example questionin task 16 for the Binary CRF model. The actual question is displayed at the bottom along with the correctanswer and the ground truth supporting facts (5 → 6 → 8). The edges represent the marginal probabilitiesp(zk , zk+1 | x, q), and the nodes represent the n supporting facts (here we have n = 9). The text for thesupporting facts are shown on the left. The top three most likely sequences are: p(z1 = 5, z2 = 6, z3 =8 | x, q) = 0.0564, p(z1 = 5, z2 = 6, z3 = 3 | x, q) = 0.0364, p(z1 = 5, z2 = 2, z3 = 3 | x, q) = 0.0356.
Figure 6: Forward step of the syntatic attention layer to compute the marginals, using the inside-outsidealgorithm (Baker, 1979) on the data structures of Eisner (1996). We assume the special root symbol is the firstelement of the sequence, and that the sentence length is n. Calculations are performed in log-space semifieldWith ㊉ =logadd and Z = + for numerical precision. a, b — C means a — C and b — c. a —㊉ b meansa — a ㊉ b.
Figure 7: Backpropagation through the inside-outside algorithm to calculate the gradient with respect to theinput potentials. Vba denotes the Jacobian of a with respect to b (so VθL is the gradient with respect to θ).
