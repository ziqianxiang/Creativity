Figure 1: Influence of pre-activation variance on gradient propagation.
Figure 2: Accuracy on the validation set for the pixel by pixel MNIST classification tasks. Thebatch-normalized LSTM is able to converge faster relatively to a baseline LSTM. Batch-normalizedLSTM also shows some improve generalization on the permuted sequential MNIST that require topreserve long-term memory information.
Figure 3:	Penn Treebank evaluation7Published as a conference paper at ICLR 20170'00 IOO 200	300	400	500	600	700	800training steps (thousands)training steps (thousands)(a) Error rate on the validation set for the Atten- (b) Error rate on the validation set on the full CNNtive Reader models on a variant of the CNN QA QA task from Hermann et al. (2015).
Figure 4:	Training curves on the CNN question-answering tasks.
Figure 5: Convergence of population statistics to stationary distributions on the Penn Treebank task.
Figure 6: Training curves on pMNIST and Penn Treebank for various initializations of Î³.
