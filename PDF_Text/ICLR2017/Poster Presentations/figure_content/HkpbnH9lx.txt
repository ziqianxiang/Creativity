Figure 1: Real NVP learns an invertible, stable, mapping between a data distribution PX and a latentdistribution PZ (typically a Gaussian). Here we show a mapping that has been learned on a toy2-d dataset. The function f (x) maps samples x from the data distribution in the upper left intoapproximate samples z from the latent distribution, in the upper right. This corresponds to exactinference of the latent state given the data. The inverse function, f -1 (z), maps samples z from thelatent distribution in the lower right into approximate samples x from the data distribution in thelower left. This corresponds to exact generation of samples from the model. The transformation ofgrid lines in X and Z space is additionally illustrated for both f (x) and f -1 (z).
Figure 2: Computational graphs for forward and inverse propagation. A coupling layer applies asimple invertible transformation consisting of scaling followed by addition of a constant offset toone part x2 of the input vector conditioned on the remaining part of the input vector x1 . Because ofits simple nature, this transformation is both easily invertible and possesses a tractable determinant.
Figure 3: Masking schemes for affine coupling layers. On the left, a spatial checkerboard patternmask. On the right, a channel-wise masking. The squeezing operation reduces the 4 × 4 × 1 tensor(on the left) into a 2 × 2 × 4 tensor (on the right). Before the squeezing operation, a checkerboardpattern is used for coupling layers while a channel-wise masking pattern is used afterward.
Figure 4: Composition schemes for affine coupling layers.
Figure 5: On the left column, examples from the dataset. On the right column, samples from themodel trained on the dataset. The datasets shown in this figure are in order: CIFAR-10, Imagenet(32 × 32), Imagenet (64 × 64), CelebA, LSUN (bedroom).
Figure 6: Manifold generated from four examples in the dataset. Clockwise from top left: CelebA,Imagenet (64 × 64), LSUN (tower), LSUN (bedroom).
Figure 7: Samples from a model trained on Imagenet (64 × 64).
Figure 8: Samples from a model trained on CelebA.
Figure 9: Samples from a model trained on LSUN (bedroom category).
Figure 10: Samples from a model trained on LSUN (church outdoor category).
Figure 11: Samples from a model trained on LSUN (tower category).
Figure 12: Manifold from a model trained on Imagenet (64 × 64). Images with red borders are takenfrom the validation set, and define the manifold. The manifold was computed as described in Equation19, where the x-axis corresponds to φ, and the y-axis to φ0, and where φ,φ0 ∈ {0, ∏,…，7∏}.
Figure 13: Manifold from a model trained on CelebA. Images with red borders are taken from thetraining set, and define the manifold. The manifold was computed as described in Equation 19, wherethe x-axis corresponds to φ, and the y-axis to φ0, and where φ,φ0 ∈ {0, 4,…，7∏}.
Figure 14: Manifold from a model trained on LSUN (bedroom category). Images with red bor-ders are taken from the validation set, and define the manifold. The manifold was computed asdescribed in Equation 19, where the x-axis corresponds to φ, and the y-axis to φ0, and whereφ, φ0 ∈ {0,4,…，7π}.
Figure 15: Manifold from a model trained on LSUN (church outdoor category). Images with redborders are taken from the validation set, and define the manifold. The manifold was computedas described in Equation 19, where the x-axis corresponds to φ, and the y-axis to φ0 , and whereφ, φ0 ∈ {0,4,…，7π}.
Figure 16: Manifold from a model trained on LSUN (tower category). Images with red bor-ders are taken from the validation set, and define the manifold. The manifold was computedas described in Equation 19, where the x-axis corresponds to φ, and the y-axis to φ0 , and whereφ, φ0 ∈ {0,4,…，7π}.
Figure 17: We generate samples a factor bigger than the training set image size on Imagenet (64 × 64).
Figure 18: We generate samples a factor bigger than the training set image size on CelebA.
Figure 19: We generate samples a factor bigger than the training set image size on LSUN (bedroomcategory).
Figure 20: We generate samples a factor bigger than the training set image size on LSUN (churchoutdoor category).
Figure 21: We generate samples a factor bigger than the training set image size on LSUN (towercategory).
Figure 22: Conceptual compression from a model trained on Imagenet (64 × 64). The leftmostcolumn represent the original image, the subsequent columns were obtained by storing higher levellatent variables and resampling the others, storing less and less as we go right. From left to right:100%, 50%, 25%, 12.5% and 6.25% of the latent variables are kept.
Figure 23: Conceptual compression from a model trained on CelebA. The leftmost column representthe original image, the subsequent columns were obtained by storing higher level latent variablesand resampling the others, storing less and less as we go right. From left to right: 100%, 50%, 25%,12.5% and 6.25% of the latent variables are kept.
Figure 24: Conceptual compression from a model trained on LSUN (bedroom category). The leftmostcolumn represent the original image, the subsequent columns were obtained by storing higher levellatent variables and resampling the others, storing less and less as we go right. From left to right:100%, 50%, 25%, 12.5% and 6.25% of the latent variables are kept.
Figure 25: Conceptual compression from a model trained on LSUN (church outdoor category). Theleftmost column represent the original image, the subsequent columns were obtained by storinghigher level latent variables and resampling the others, storing less and less as we go right. From leftto right: 100%, 50%, 25%, 12.5% and 6.25% of the latent variables are kept.
Figure 26: Conceptual compression from a model trained on LSUN (tower category). The leftmostcolumn represent the original image, the subsequent columns were obtained by storing higher levellatent variables and resampling the others, storing less and less as we go right. From left to right:100%, 50%, 25%, 12.5% and 6.25% of the latent variables are kept.
Figure 27: Examples x from the CelebA dataset.
Figure 28: From a model trained on pairs of images and attributes from the CelebA dataset, we encodea batch of images with their original attributes before decoding them with a new set of attributes. Wenotice that the new images often share similar characteristics with those in Fig 27, including positionand background.
