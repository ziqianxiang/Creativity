Figure 1: The two embedding functionS fand g are trained with a contraStive loSS be-tween the domainS, along with decoderS thatoPtimize autoencoder loSSeS.
Figure 2: The 3 and 4 link robotS per-forming the button preSSing taSk, whichwe uSe to evaluate the performanceof our tranSfer method. Each taSk iStrained on multiple conditionS wherethe objectS Start in different locationS.
Figure 3: The 4-link robot pushing the button. Note that the reward function only tells the agent how far thebutton has been depressed, and provides no information to indicate that the arm should reach for the button.
Figure 4:	The 3 and 4 link robots performing each of the three proxy tasks we consider: target reaching, peginsertion, and block moving. Our results indicate that using all three proxy tasks to learn the common featurespace improves performance over any single proxy task.
Figure 5:	Performance of 4-link arm on the sparse reward button pressing task described in Section 5.2. Onthe left and middle, we compare our method with the methods described in Section 5.1. On the right, the “peg,”“push,” and “reach” proxy ablations indicate the performance when using embedding functions learned fromthose proxy tasks. The embedding improves significantly when learned from all three proxy tasks, indicatingthat our method benefits from additional prior experience.
Figure 6: The top images show the source andtarget domain robots: the robot on the left istorque driven at the joints and the one on theright is tendon driven. The tendons are high-lighted in the image; the green tendon has avariable-length lever arm, while the yellow ten-dons have fixed-length lever arms. Note that thefirst tendon couples two joints. The bottom im-ages show two variations of the test task.
Figure 7: The tendon-driven robot pulling the block. Note that the reward function only tells the agent how farthe block is from the red goal and provides no information to indicate that the arm should reach around theblock in order to pull it. The block is restricted to move only towards the red goal, but the agent needs to moveunder and around the block to pull it.
Figure 8: Performance of tendon-controlled arm on block pulling task. While the environment’s reward istoo sparse to succeed in a reasonable time without transfer, using our method to match feature space statedistributions enables faster learning. Using a linear embedding or mapping directly from source states totarget states allows for some transfer. Optimizing over P instead of assuming time-based alignment does nothurt performance. KCCA with quadratic kernel performs very well in this experiment, but not in experiment 1.
