title,year,conference
 Tensorflow: Large-scale machine learning on heteroge-neous distributed systems,2016, arXiv preprint arXiv:1603
 Natural gradient works efficiently in learning,1998, Neural computation
 Hessian-free optimization for learning deep multidimen-sional recurrent neural networks,2015, In Advances in Neural Information Processing Systems
 A self-correcting variable-metric algorithm for stochastic optimization,2016, In Proceedings of The33rd International Conference on Machine Learning
 Large scale distributed deep networks,2012, In Advances in neural informationprocessing systems
 Scaling up natural gradient by factorizing fisher information,2015, InProceedings of the 32nd International Conference on Machine Learning (ICML)
 Deep residual learning for image recognition,2015, arXivpreprint arXiv:1512
 Large scale distributed hessian-freeoptimization for deep neural network,2016, arXiv preprint arXiv:1606
 On “natural” learning and pruning in multilayered perceptrons,2000, Neural Computation
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In Proceedings of The 32nd International Conference on Machine Learning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Training neural networks with stochastic hessian-free optimization,2013, arXiv preprintarXiv:1301
 Learning multiple layers of features from tiny images,2009, 
 Imagenet classification with deep convolutional neuralnetworks,2012, In Advances in neural information processing systems
 Topmoumoute online natural gradient algo-rithm,2008, In Advances in neural information processing systems
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 New insights and perspectives on the natural gradient method,2014, arXiv preprint arXiv:1412
 Training deep and recurrent networks with Hessian-free optimization,2012, InNeural Networks: Tricks of the Trade
 A linearly-convergent stochastic L-BFGS algorithm,2016, InProceedings of the 19th International Conference on Artificial Intelligence and Statistics
 Riemannian metrics for neural networks i: feedforward networks,2013, arXiv preprintarXiv:1303
 Parallel training of DNNs with natural gradient andparameter averaging,2015, In International Conference on Learning Representations: Workshop track
 Centering neural network gradient factors,1998, In Genevieve B
 A stochastic quasi-newton method for online convex opti-mization,2007, In AISTATS
 Going deeper with convolutions,2014, arXiv preprintarXiv:1409
 Rethinking theinception architecture for computer vision,2015, arXiv preprint arXiv:1512
 Krylov subspace descent for deep learning,2012, In AISTATS
 The gradient of the parameters shared across the instantiationsare the sum of the individual gradients from each instantiation,2016, Given such computation graph
