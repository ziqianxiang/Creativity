title,year,conference
 Efficient approaches for escaping higher order saddle points in non-convex opti-mization,2016, arXiv:1602
 Bayesian dark knowledge,2015, In NIPS
 Unreasonableeffectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmicschemes,2016, PNAS
 Learning may need only a few bits ofsynaptic precision,2016, Physical Review E
 Subdominant dense clusters allow forsimple learning and high computational performance in neural networks with discrete synapses,2015, Physicalreview letters
 Neural networks and principal component analysis: Learning from examples withoutlocal minima,1989, Neural Networks
 Variational inference: A review for statisticians,2016, arXiv:1601
 Stochastic gradient descent tricks,2012, In Neural Networks: Tricks of the Trade
 Stability and generalization,2002, JMLR
 Metastability: A potential theoretic approach,2006, In International Congress ofMathematicians
 The statistics of critical points of Gaussian fields on large-dimensional spaces,2007, PhysicsReview Letter
 On the energy landscape of deep networks,2015, arXiv:1511
 Stochastic Gradient Hamiltonian Monte Carlo,2014, In ICML
 The loss surfaces of multilayernetworks,2015, In AISTATS
 Open problem: The landscape of the loss surfaces of multilayernetworks,2015, In COLT
 Recurrent batch normalization,2016, arXiv:1603
 Identifying and attacking the saddlepoint problem in high-dimensional non-convex optimization,2014, In NIPS
 Bayesian sampling using stochasticgradient thermostats,2014, In NIPS
 Adaptive subgradient methods for online learning and stochastic optimiza-tion,2011, JMLR
 Scalable Bayesian Learning of Recurrent Neural Networksfor Language Modeling,2016, arXiv:1611
 Escaping from saddle points — online stochastic gradient for tensordecomposition,2015, In COLT
 Qualitatively characterizing neural network optimization problems,2015, In ICLR
 Maxout networks,2013, ICML
 Mollifying networks,2016, arXiv:1608
 On graduated optimization for stochastic non-convex problems,2016, InICML
 Long short-term memory,1997, Neural cOmputatiOn
 Flat minima,1997, Neural COmputatiOn
 Batch normalization: Accelerating deep network training by reducing internal covariateshift,2015, arXiv:1502
 Beating the Perils of Non-Convexity: Guaranteed Training ofNeural Networks using Tensor Methods,2015, arXiv:1506
 Visualizing and understanding recurrent networks,2015, arXiv:1506
 Deep learning without poor local minima,2016, In NIPS
 Adam: A method for stochastic optimization,2014, arXiv:1412
 Learning multiple layers of features from tiny images,2009, Master’s thesis
 A complete recipe for stochastic gradient MCMC,2015, In NIPS
 A variational analysis of stochastic gradient algorithms,2016, arXiv:1602
 Building a large annotated corpus of English: The PennTreebank,1993, COmputatiOnal linguistics
 Training Recurrent Neural Networks by Diffusion,2016, arXiv:1601
 Weight space structure and internal representations: A direct approach tolearning and generalization in multilayer neural networks,1995, Physical review letters
 MCMC using Hamiltonian dynamics,2011, HandbOOk Of MarkOv Chain MOnte CarlO
 Fast exact multiplication by the hessian,1994, Neural cOmputatiOn
 Acceleration of stochastic approximation by averaging,1992, SIAM JOurnal On COntrOland OptimizatiOn
 Singularity of the Hessian in Deep Learning,2016, arXiv:1611:07476
 Weight normalization: A simple reparameterization to accelerate training of deepneural networks,2016, arXiv:1602
 Exact solutions to the nonlinear dynamics of learning in deep linearneural networks,2014, In ICLR
 No bad local minima: Data independent training error guarantees for multilayerneural networks,2016, arXiv:1605
 Dropout: a simple way to preventneural networks from overfitting,2014, JMLR
 On the importance of initialization and momentum in deeplearning,2013, In ICML
 Lecture 6,2012,5: RmsProp
 Bayesian learning via stochastic gradient Langevin dynamics,2011, In ICML
 Recurrent neural network regularization,2014, arXiv:1409
 Understanding deep learning requires rethinkinggeneralization,2016, arXiv:1611
 Deep learning with elastic averaging SGD,2015, In NIPS
