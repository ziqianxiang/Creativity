title,year,conference
 Modeling biological processes for readingcomprehension,2014, In EMNLP
 Attention-over-attention neural networks for reading comprehension,2016, arXiv preprint arXiv:1607
 Max-out networks,2013, ICML (3)
 Teaching machines to read and comprehend,2015, In Advances inNeural Information Processing Systems
 The goldilocks principle: Readingchildrenâ€™s books with explicit memory representations,2016, In ICLR
 Long short-term memory,1997, Neural computation
 Text understanding with theattention sum reader network,2016, arXiv preprint arXiv:1603
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 The stanford corenlp natural language processing toolkit,2014, In ACL (SystemDemonstrations)
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Mctest: A challenge dataset forthe open-domain machine comprehension of text,2013, In EMNLP
 Iterative alternating neural attention formachine reading,2016, arXiv preprint arXiv:1606
 Training very deep networks,2015, InAdvances in Neural Information Processing Systems 28
 Pointer networks,2015, In Advances in NeuralInformation Processing Systems
 Learning natural language inference with LSTM,2016, In Proceedingsof the 2016 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies
