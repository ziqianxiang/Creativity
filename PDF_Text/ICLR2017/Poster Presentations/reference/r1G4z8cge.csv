title,year,conference
 Numerical Continuation Methods,1980, An Introduction
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Online algorithms and stochastic approximations,1998, In David Saad (ed
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In NIPSâ€™2014
 Partial differential equations,1998, Graduate Studies in Mathematics
 Deep sparse rectifier neural networks,2011, InAISTATS
 Practical variational inference for neural networks,2011, In Advances in Neural InformationProcessing Systems
 Adaptive computation time for recurrent neural networks,2016, arXiv preprintarXiv:1603
 Knowledge matters: Importance of prior information foroptimization,2013, arXiv preprint arXiv:1301
 Learned-norm pooling fordeep feedforward and recurrent neural networks,2014, In Machine Learning and Knowledge Discoveryin Databases
 On graduated optimization for stochasticnon-convex problems,2015, arXiv preprint arXiv:1503
 Deep residual learning for imagerecognition,2015, arXiv preprint arXiv:1512
 Deep neuralnetworks for acoustic modeling in speech recognition,2012, Signal Processing Magazine
 Long short-term memory,1997, Neural Computation
 Flat minima,1997, Neural Computation
 Deep networks withstochastic depth,2016, arXiv preprint arXiv:1603
 Deepnetworks with stochastic depth,2016,	CoRR
 Batch normalization: Accelerating deep networktraining by reducing internal covariate shift,2015,	CoRR
 Neural gpus learn algorithms,2015, arXiv preprint arXiv:1511
 Grid long short-term memory,2015, arXiv preprintarXiv:1507
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Playing atari with deep reinforcement learning,2013, Technical report
 Training recurrent neural networks by diffusion,2016, arXiv preprint arXiv:1601
 Adding gradient noise improves learning for very deep networks,2015, CoRR
 Adding gradient noise improves learning for very deep networks,2015, arXiv preprintarXiv:1511
 Shaping and policy search in reinforcement learning,2003, PhD thesis
 Policy invariance under reward transformations:Theory and application to reward shaping,1999, In ICML
 Masteringthe game of go with deep neural networks and tree search,2016, Nature
 Gradual dropin of layers to train very deepneural networks,2016, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Training very deep networks,2015, InAdvances in Neural Information Processing Systems
 Reseg: A recurrent neural network for object segmentation,2015, arXiv preprint arXiv:1511
 Learning to execute,2014, arXiv preprint arXiv:1410
