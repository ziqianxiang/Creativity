Published as a conference paper at ICLR 2017
β-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A
Constrained Variational Framework
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,
Matthew Botvinick, Shakir Mohamed, Alexander Lerchner
Google DeepMind
{irinah,lmatthey,arkap,cpburgess,glorotx,
botvinick,shakir,lerchner}@google.com
Ab stract
Learning an interpretable factorised representation of the independent data gen-
erative factors of the world without supervision is an important precursor for the
development of artificial intelligence that is able to learn and reason in the same
way that humans do. We introduce β-VAE, a new state-of-the-art framework for
automated discovery of interpretable factorised latent representations from raw
image data in a completely unsupervised manner. Our approach is a modification
of the variational autoencoder (VAE) framework. We introduce an adjustable hy-
perparameter β that balances latent channel capacity and independence constraints
with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned
β > 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsu-
pervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled
factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we
devise a protocol to quantitatively compare the degree of disentanglement learnt
by different models, and show that our approach also significantly outperforms
all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few
assumptions about the data and relies on tuning a single hyperparameter β, which
can be directly optimised through a hyperparameter search using weakly labelled
data or through heuristic visual inspection for purely unsupervised data.
1 Introduction
The difficulty of learning a task for a given machine learning approach can vary significantly
depending on the choice of the data representation. Having a representation that is well suited to the
particular task and data domain can significantly improve the learning success and robustness of the
chosen model (Bengio et al., 2013). It has been suggested that learning a disentangled representation
of the generative factors in the data can be useful for a large variety of tasks and domains (Bengio
et al., 2013; Ridgeway, 2016). A disentangled representation can be defined as one where single
latent units are sensitive to changes in single generative factors, while being relatively invariant to
changes in other factors (Bengio et al., 2013). For example, a model trained on a dataset of 3D objects
might learn independent latent units sensitive to single independent data generative factors, such as
object identity, position, scale, lighting or colour, thus acting as an inverse graphics model (Kulkarni
et al., 2015). In a disentangled representation, knowledge about one factor can generalise to novel
configurations of other factors. According to Lake et al. (2016), disentangled representations could
boost the performance of state-of-the-art AI approaches in situations where they still struggle but
where humans excel. Such scenarios include those which require knowledge transfer, where faster
learning is achieved by reusing learnt representations for numerous tasks; zero-shot inference, where
reasoning about new data is enabled by recombining previously learnt factors; or novelty detection.
Unsupervised learning of a disentangled posterior distribution over the underlying generative factors
of sensory data is a major challenge in AI research (Bengio et al., 2013; Lake et al., 2016). Most
previous attempts required a priori knowledge of the number and/or nature of the data generative
factors (Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Zhu et al., 2014; Yang
et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016;
Karaletsos et al., 2016). This is not always feasible in the real world, where the newly initialised
learner may be exposed to complex data where no a priori knowledge of the generative factors exists,
and little to no supervision for discovering the factors is available. Until recently purely unsupervised
1
Published as a conference paper at ICLR 2017
Figure 1: Manipulating latent variables on celebA: Qualitative results comparing disentangling
performance of β-VAE (β = 250), VAE (Kingma & Welling, 2014) (β = 1) and InfoGAN (Chen
et al., 2016). In all figures of latent code traversal each block corresponds to the traversal of a single
latent variable while keeping others fixed to either their inferred (β-VAE, VAE and DC-IGN where
applicable) or sampled (InfoGAN) values. Each row represents a different seed image used to infer
the latent values in the VAE-based models, or a random sample of the noise variables in InfoGAN.
β-VAE and VAE traversal is over the [-3, 3] range. InfoGAN traversal is over ten dimensional
categorical latent variables. Only β-VAE and InfoGAN learnt to disentangle factors like azimuth
(a), emotion (b) and hair style (c), whereas VAE learnt an entangled representation (e.g. azimuth is
entangled with emotion, presence of glasses and gender). InfoGAN images adapted from Chen et al.
(2016). Reprinted with permission.
approaches to disentangled factor learning have not scaled well (Schmidhuber, 1992; Desjardins
et al., 2012; Tang et al., 2013; Cohen & Welling, 2014; 2015).
Recently a scalable unsupervised approach for disentangled factor learning has been developed,
called InfoGAN (Chen et al., 2016). InfoGAN extends the generative adversarial network (GAN)
(Goodfellow et al., 2014) framework to additionally maximise the mutual information between a
subset of the generating noise variables and the output of a recognition network. It has been reported
to be capable of discovering at least a subset of data generative factors and of learning a disentangled
representation of these factors. The reliance of InfoGAN on the GAN framework, however, comes
at the cost of training instability and reduced sample diversity. Furthermore, InfoGAN requires
some a priori knowledge of the data, since its performance is sensitive to the choice of the prior
distribution and the number of the regularised noise variables. InfoGAN also lacks a principled
inference network (although the recognition network can be used as one). The ability to infer the
posterior latent distribution from sensory input is important when using the unsupervised model in
transfer learning or zero-shot inference scenarios. Hence, while InfoGAN is an important step in the
right direction, we believe that further improvements are necessary to achieve a principled way of
using unsupervised learning for developing more human-like learning and reasoning in algorithms as
described by Lake et al. (2016).
Finally, there is currently no general method for quantifying the degree of learnt disentanglement.
Therefore there is no way to quantitatively compare the degree of disentanglement achieved by
different models or when optimising the hyperparameters of a single model.
2
Published as a conference paper at ICLR 2017
lμn∈Nes£p 一 M (q)<υ-41s 6<υ二 D)
DC-IGN InfoGAN
*p-≡ff** 00000年
上艮聚更察8局得同吊停章
Factor not learnt
B-VAE	VAE
，y J
■・“触	雨而涌
■j 〃打
一■—一一.	∙∙∙,90
R∙∙m ^tt⅜⅜⅜
2JT，，3J ]，■•♦・
Figure 2: Manipulating latent variables on 3D chairs: Qualitative results comparing disentangling
performance of β-VAE (β = 5), VAE (Kingma & Welling, 2014) (β = 1), InfoGAN (Chen et al.,
2016) and DC-IGN (Kulkarni et al., 2015). InfoGAN traversal is over the [-1, 1] range. VAE always
learns an entangled representation (e.g. chair width is entangled with azimuth and leg style (b)).
All models apart from VAE learnt to disentangle the labelled data generative factor, azimuth (a).
InfoGAN and β-VAE were also able to discover unlabelled factors in the dataset, such as chair width
(b). Only β-VAE, however, learnt about the unlabelled factor of chair leg style (c). InfoGAN and
DC-IGN images adapted from Chen et al. (2016) and Kulkarni et al. (2015), respectively. Reprinted
with permission.
In this paper we attempt to address these issues. We propose β-VAE, a deep unsupervised generative
approach for disentangled factor learning that can automatically discover the independent latent
factors of variation in unsupervised data. Our approach is based on the variational autoencoder (VAE)
framework (Kingma & Welling, 2014; Rezende et al., 2014), which brings scalability and training
stability. While the original VAE work has been shown to achieve limited disentangling performance
on simple datasets, such as FreyFaces or MNIST (Kingma & Welling, 2014), disentangling perfor-
mance does not scale to more complex datasets (e.g. Aubry et al., 2014; Paysan et al., 2009; Liu et al.,
2015), prompting the development of more elaborate semi-supervised VAE-based approaches for
learning disentangled factors (e.g. Kulkarni et al., 2015; Karaletsos et al., 2016).
We propose augmenting the original VAE framework with a single hyperparameter β that modulates
the learning constraints applied to the model. These constraints impose a limit on the capacity of
the latent information channel and control the emphasis on learning statistically independent latent
factors. β-VAE with β = 1 corresponds to the original VAE framework (Kingma & Welling, 2014;
Rezende et al., 2014). With β > 1 the model is pushed to learn a more efficient latent representation
of the data, which is disentangled if the data contains at least some underlying factors of variation
that are independent. We show that this simple modification allows β-VAE to significantly improve
the degree of disentanglement in learnt latent representations compared to the unmodified VAE
framework (Kingma & Welling, 2014; Rezende et al., 2014). Furthermore, we show that β-VAE
achieves state of the art disentangling performance against both the best unsupervised (InfoGAN:
Chen et al., 2016) and semi-supervised (DC-IGN: Kulkarni et al., 2015) approaches for disentangled
factor learning on a number of benchmark datasets, such as CelebA (Liu et al., 2015), chairs (Aubry
et al., 2014) and faces (Paysan et al., 2009) using qualitative evaluation. Finally, to help quantify
the differences, we develop a new measure of disentanglement and show that β-VAE significantly
outperforms all our baselines on this measure (ICA, PCA, VAE Kingma & Ba (2014), DC-IGN
Kulkarni et al. (2015), and InfoGAN Chen et al. (2016)).
Our main contributions are the following: 1) we propose β-VAE, a new unsupervised approach for
learning disentangled representations of independent visual data generative factors; 2) we devise a
protocol to quantitatively compare the degree of disentanglement learnt by different models; 3) we
demonstrate both qualitatively and quantitatively that our β-VAE approach achieves state-of-the-art
disentanglement performance compared to various baselines on a variety of complex datasets.
3
Published as a conference paper at ICLR 2017
DC-IGN	InfoGAN
VAE
号廿。・6也曾肾06© ⅛⅜∙f< t>Q∙0<l
引广，勺飞逸Tt 33',∙V . '”［（七
I / 1 7, G飞r m >，/力■■，方才呼3
^9∙∙fW皆曾曾行号 >・©勺*次⑦J∙V
ιt>e∙*t 矽恸钞磔掣 ⅛⅜t>Φ ⅜⅜<tt
r*情境力寸寸•KK・，«*♦ '3。。停,
号窗■■■■曾智智智* ■■勺拿∙ V¢¢¢,
§•••••，步9吩∙∙∙tftt ,，中曾号■
苍■■&*♦ *号曰恸电 t*^∙t ξ>^trt
一・・・♦	∙∙a∙∙ Va*中，
。。号q十 ttt>> W。。。QSOOG
∣t*⅜⅜⅜悔4也修意・・•・■ Baa¢4
I C e «UmY t t '♦，零。1G 3 ? # 号 g
曾。的好© "皆9。・⑵沙3*3
ttttt	=，9。6 也龟。售・
Figure 3: Manipulating latent variables on 3D faces: Qualitative results comparing disentangling
performance of β-VAE (β = 20), VAE (Kingma & Welling, 2014) (β = 1), InfoGAN (Chen et al.,
2016) and DC-IGN (Kulkarni et al., 2015). InfoGAN traversal is over the [-1, 1] range. All models
learnt to disentangle lighting (b) and elevation (c). DC-IGN and VAE struggled to continuously
interpolate between different azimuth angles (a), unlike β-VAE, which additionally learnt to encode a
wider range of azimuth angles than other models. InfoGAN and DC-IGN images adapted from Chen
et al. (2016) and Kulkarni et al. (2015), respectively. Reprinted with permission.
Figure 4: Latent factors learnt by β-VAE on celebA: traversal of individual latents demonstrates
that β-VAE discovered in an unsupervised manner factors that encode skin colour, transition from an
elderly male to younger female, and image saturation.
2 β-VAE FRAMEWORK DERIVATION
Let D = {X, V, W} be the set that consists of images x ∈ RN and two sets of ground truth data
generative factors: conditionally independent factors v ∈ RK, where log p(v|x) = Pk logp(vk|x);
and conditionally dependent factors w ∈ RH . We assume that the images x are generated by the
true world simulator using the corresponding ground truth data generative factors: p(x|v, w) =
Sim(v, w).
4
Published as a conference paper at ICLR 2017
We want to develop an unsupervised deep generative model that, using samples from X only, can
learn the joint distribution of the data x and a set of generative latent factors z (z ∈ RM, where
M ≥ K) such that z can generate the observed data x; that is, p(x|z) ≈ p(x|v, w) = Sim(v, w).
Thus a suitable objective is to maximise the marginal (log-)likelihood of the observed data x in
expectation over the whole distribution of latent factors z:
max Epθ(z)[pθ (x|z)]	(1)
θ
For a given observation x, we describe the inferred posterior configurations of the latent factors z by
a probability distribution qφ(z∣x). Our aim is to ensure that the inferred latent factors qΦ(z|x) capture
the generative factors v in a disentangled manner. The conditionally dependent data generative
factors w can remain entangled in a separate subset of z that is not used for representing v. In order
to encourage this disentangling property in the inferred qφ(z∣x), we introduce a constraint over it by
trying to match it to a prior p(z) that can both control the capacity of the latent information bottleneck,
and embodies the desiderata of statistical independence mentioned above. This can be achieved if
we set the prior to be an isotropic unit Gaussian (p(z) = N(0, I)), hence arriving at the constrained
optimisation problem in Eq. 2, where specifies the strength of the applied constraint.
maxEx〜D [Eq (z∣χ)[logp(x|z)]]	subject to DκL(qφ(z∣x)∣∣p(z)) < E	(2)
φ,θ	φ
Re-writing Eq. 2 as a Lagrangian under the KKT conditions (Kuhn & Tucker, 1951; Karush, 1939),
we obtain:
F(θ,φ,β;x,z) = Eqφ(z∣x)[logPθ(x∣z)] - β (Dkl®(z∣x)∣∣p(z)) - e)	(3)
where the KKT multiplier β is the regularisation coefficient that constrains the capacity of the latent
information channel z and puts implicit independence pressure on the learnt posterior due to the
isotropic nature of the Gaussian prior p(z). Since β, E ≥ 0 according to the complementary slackness
KKT condition, Eq. 3 can be re-written to arrive at the β-VAE formulation - as the familiar variational
free energy objective function as described by Jordan et al. (1999), but with the addition of the β
coefficient:
F(θ,Φ,β; x,z) ≥ L(θ,Φ; x,z,β) = Eqφ(z∣x)[logPθ(x∣z)] — β DklSφ(z∣x)IIp(z))	(4)
Varying β changes the degree of applied learning pressure during training, thus encouraging different
learnt representations. β-VAE where β = 1 corresponds to the original VAE formulation of (Kingma
& Welling, 2014). We postulate that in order to learn disentangled representations of the conditionally
independent data generative factors v, it is important to set β > 1, thus putting a stronger constraint
on the latent bottleneck than in the original VAE formulation of Kingma & Welling (2014). These
constraints limit the capacity of z, which, combined with the pressure to maximise the log likelihood
of the training data x under the model, should encourage the model to learn the most efficient
representation of the data. Since the data x is generated using at least some conditionally independent
ground truth factors v, and the DKL term of the β-VAE objective function encourages conditional
independence in qφ(z∣x), we hypothesise that higher values of β should encourage learning a
disentangled representation of v. The extra pressures coming from high β values, however, may
create a trade-off between reconstruction fidelity and the quality of disentanglement within the learnt
latent representations. Disentangled representations emerge when the right balance is found between
information preservation (reconstruction cost as regularisation) and latent channel capacity restriction
(β > 1). The latter can lead to poorer reconstructions due to the loss of high frequency details when
passing through a constrained latent bottleneck. Hence, the log likelihood of the data under the learnt
model is a poor metric for evaluating disentangling in β-VAEs. Instead we propose a quantitative
metric that directly measures the degree of learnt disentanglement in the latent representation.
Since our proposed hyperparameter β directly affects the degree of learnt disentanglement, we would
like to estimate the optimal β for learning a disentangled latent representation directly. However, it is
not possible to do so. This is because the optimal β will depend on the value of E in Eq.2. Different
datasets and different model architectures will require different optimal values of E. However, when
optimising β in Eq. 4, we are indirectly also optimising E for the best disentanglement (see Sec.A.7
for details), and while we can not learn the optimal value of β directly, we can instead estimate it
using either our proposed disentanglement metric (see Sec. 3) or through visual inspection heuristics.
5
Published as a conference paper at ICLR 2017
Uo-b=e
W≡≡5
A uosod
X uo8od
¾/1 QOOQ∣
，⑷Zdiff)
____ Linear
Zdiff = L
L
L
zldiff
l=1
Figure 5: Schematic of the proposed disen-
tanglement metric: over a batch of L samples,
each pair of images has a fixed value for one
target generative factor y (here y = scale)
and differs on all others. A linear classifier
is then trained to identify the target factor us-
ing the average pairwise difference zdbiff in the
latent space over L samples.
3	Disentanglement metric
It is important to be able to quantify the level of disentanglement achieved by different models.
Designing a metric for this, however, is not straightforward. We begin by defining the properties
that we expect a disentangled representation to have. Then we describe our proposed solution for
quantifying the presence of such properties in a learnt representation.
As stated above, we assume that the data is generated by a ground truth simulation process which
uses a number of data generative factors, some of which are conditionally independent, and we also
assume that they are interpretable. For example, the simulator might sample independent factors
corresponding to object shape, colour and size to generate an image of a small green apple. Because
of the independence property, the simulator can also generate small red apples or big green apples.
A representation of the data that is disentangled with respect to these generative factors, i.e. which
encodes them in separate latents, would enable robust classification even using very simple linear
classifiers (hence providing interpretability). For example, a classifier that learns a decision boundary
that relies on object shape would perform as well when other data generative factors, such as size or
colour, are varied.
Note that a representation consisting of independent latents is not necessarily disentangled, according
to our desiderata. Independence can readily be achieved by a variety of approaches (such as PCA or
ICA) that learn to project the data onto independent bases. Representations learnt by such approaches
do not in general align with the data generative factors and hence may lack interpretability. For this
reason, a simple cross-correlation calculation between the inferred latents would not suffice as a
disentanglement metric.
Our proposed disentangling metric, therefore, measures both the independence and interpretability
(due to the use of a simple classifier) of the inferred latents. To apply our metric, we run inference
on a number of images that are generated by fixing the value of one data generative factor while
randomly sampling all others. If the independence and interpretability properties hold for the inferred
representations, there will be less variance in the inferred latents that correspond to the fixed generative
factor. We use a low capacity linear classifier to identify this factor and report the accuracy value as
the final disentanglement metric score. Smaller variance in the latents corresponding to the target
factor will make the job of this classifier easier, resulting in a higher score under the metric. See
Fig. 5 for a representation of the full process.
More formally, we start from a dataset D = {X, V, W} as described in Sec. 2, assumed to contain a
balanced distribution of ground truth factors (v, w), where images data points are obtained using a
ground truth simulator process X 〜Sim(v, w). We also assume We are given labels identifying a
subset of the independent data generative factors v ∈ V for at least some instances.
We then construct a batch of B vectors zbdiff, to be fed as inputs to a linear classifier as folloWs:
1.	Choose a factor y 〜Unif [1...K] (e.g. y = scale in Fig. 5).
6
Published as a conference paper at ICLR 2017
2.	For a batch of L samples:
(a)	Sample two sets of latent representations, v1,l and v2,l, enforcing [v1,l]k =
[v2,l]k if k = y (so that the value of factor k = y is kept fixed).
(b)	Simulate image xι,ι 〜 Sim(vι,ι), then infer zι,ι = μ(x1,1), using the encoder
q(z∣x)〜N (μ(x),σ(x)).
Repeat the process for v2,l .
(c)	Compute the difference zldiff = |z1,l - z2,l |, the absolute linear difference between the
inferred latent representations.
3.	Use the average Zbiff = L PL=I Zdiff to predictp(y∣zdiiff) (again, y = scale in Fig. 5) and
report the accuracy of this predictor as disentangement metric score.
The classifier’s goal is to predict the index y of the generative factor that was kept fixed for a given
zdbiff. The accuracy of this classifier over multiple batches is used as our disentanglement metric score.
We choose a linear classifier with low VC-dimension in order to ensure it has no capacity to perform
nonlinear disentangling by itself. We take differences of two inferred latent vectors to reduce the
variance in the inputs to the classifier, and to reduce the conditional dependence on the inputs x. This
ensures that on average zdbiff y < zdbiff {\y}. See Equations 5 in Appendix A.4 for more details of
the process.
4	Experiments
In this section we first qualitatively demonstrate that our proposed β-VAE framework consistently
discovers more latent factors and disentangles them in a cleaner fashion that either unmodified VAE
(Kingma & Welling, 2014) or state of the art unsupervised (InfoGAN: Chen et al., 2016) and semi-
supervised (DC-IGN: Kulkarni et al., 2015) solutions for disentangled factor learning on a variety
of benchmarks. We then quantify and characterise the differences in disentangled factor learning
between our β-VAE framework and a variety of benchmarks using our proposed new disentangling
metric.
4.1	Qualitative benchmarks
We trained β-VAE (see Tbl. 1 for architecture details) on a variety of datasets commonly used to
evaluate disentangling performance of models: celebA (Liu et al., 2015), chairs (Aubry et al., 2014)
and faces (Paysan et al., 2009). Figures 1-3 provide a qualitative comparison of the disentangling
performance of β-VAE, VAE (β = 1) (Kingma & Welling, 2014), InfoGAN (Chen et al., 2016) and
DC-IGN (Kulkarni et al., 2015) as appropriate.
It can be seen that across all datasets β-VAE is able to automatically discover and learn to disentangle
all of the factors learnt by the semi-supervised DC-IGN (Kulkarni et al., 2015): azimuth (Fig. 3a,
Fig. 2a), lighting and elevation (Fig. 3b,c)). Often it acts as a more convincing inverse graphics
network than DC-IGN (e.g. Fig. 3a) or InfoGAN (e.g. Fig. 2a, Fig. 1a-c or Fig. 3a). Furthermore,
unlike DC-IGN, β-VAE requires no supervision and hence can learn about extra unlabelled data
generative factors that DC-IGN can not learn by design, such as chair width or leg style (Fig. 2b,c).
The unsupervised InfoGAN (Chen et al., 2016) approach shares this quality with β-VAE, and the two
frameworks tend to discover overlapping, but not necessarily identical sets of data generative factors.
For example, both β-VAE and InfoGAN (but not DC-IGN) learn about the width of chairs (Fig. 2b).
Only β-VAE, however, learns about the chair leg style (Fig. 2c). It is interesting to note how β-VAE
is able to generate an armchair with a round office chair base, even though such armchairs do not exist
in the dataset (or, perhaps, reality). Furthermore, only β-VAE is able to discover all three factors of
variation (chair azimuth, width and leg style) within a single model, while InfoGAN learns to allocate
its continuous latent variable to either azimuth or width. InfoGAN sometimes discovers factors that
β-VAE does not precisely disentangle, such as the presence of sunglasses in celebA. β-VAE does,
however, discover numerous extra factors such as skin colour, image saturation, and age/gender that
are not reported in the InfoGAN paper (Chen et al., 2016) (Fig. 4). Furthermore, β-VAE latents tend
to learn a smooth continuous transformation over a wider range of factor values than InfoGAN (e.g.
rotation over a wider range of angles as shown in Figs. 1-3a).
7
Published as a conference paper at ICLR 2017
Overall β-VAE tends to consistently and robustly discover more latent factors and learn cleaner
disentangled representations of them than either InfoGAN or DC-IGN. This holds even on such
challenging datasets as celebA. Furthermore, unlike InfoGAN and DC-IGN, β-VAE requires no
design decisions or assumptions about the data, and is very stable to train.
When compared to the unmodified VAE baseline (β = 1) β-VAE consistently learns significantly
more disentangled latent representations. For example, when learning about chairs, VAE entangles
chair width with leg style (Fig. 2b). When learning about celebA, VAE entangles azimuth with
emotion and gender (Fig. 1a); emotion with hair style, skin colour and identity (Fig. 1b); while the
VAE fringe latent also codes for baldness and head size (Fig. 1c). Although VAE performs relatively
well on the faces dataset, it still struggles to learn a clean representation of azimuth (Fig. 3a). This,
however, suggests that a continuum of disentanglement quality exists, and it can be traversed by
varying β within the β-VAE framework. While increasing β often leads to better disentanglement,
it may come at the cost of blurrier reconstructions and losing representations for some factors,
particularly those that correspond to only minor changes in pixel space.
4.2	Quantitative benchmarks
In order to quantitatively compare the disentangling performance of β-VAE against various baselines,
we created a synthetic dataset of 737,280 binary 2D shapes (heart, oval and square) generated from
the Cartesian product of the shape and four independent generative factors vk defined in vector
graphics: position X (32 values), position Y (32 values), scale (6 values) and rotation (40 values over
the 2π range). To ensure smooth affine object transforms, each two subsequent values for each factor
vk were chosen to ensure minimal differences in pixel space given 64x64 pixel image resolution.
This dataset was chosen because it contains no confounding factors apart from its five independent
data generative factors (identity, position X, position Y, scale and rotation). This gives us knowledge
of the ground truth for comparing the disentangling performance of different models in an objective
manner.
We used our proposed disentanglement metric (see Sec. 3) to quantitatively compare the ability of
β-VAE to automatically discover and learn a disentangled representation of the data generative factors
of the synthetic dataset of 2D shapes described above with that of a number of benchmarks (see
Tbl. 1 in Appendix for model architecture details). The table in Fig. 6 (left) reports the classification
accuracy of the disentanglement metric for 5,000 test samples. It can be seen that β-VAE (β = 4)
significantly outperforms all baselines, such as an untrained VAE and the original VAE formulation
of Kingma & Welling (2014) (β = 1) with the same architecture as β-VAE, the top ten PCA or ICA
components of the data (see Sec. A.3 for details), or when using the raw pixels directly. β-VAE also
does better than InfoGAN. Remarkably, β-VAE performs on the same level as DC-IGN despite the
latter being semi-supervised and the former wholly unsupervised. Furthermore, β-VAE achieved
similar classification accuracy as the ground truth vectors used for data generation, thus suggesting
that it was able to learn a very good disentangled representation of the data generative factors.
We also examined qualitatively the representations learnt by β-VAE, VAE, InfoGAN and DC-IGN
on the synthetic dataset of 2D shapes. Fig. 7A demonstrates that after training, β-VAE with β = 4
learnt a good (while not perfect) disentangled representation of the data generative factors, and
its decoder learnt to act as a rendering engine. Its performance was comparative to that of DC-
IGN (Fig. 7C), with the difference that DC-IGN required a priori knowledge about the quantity
of the data generative factors, while β-VAE was able to discover them in an unsupervised manner.
The most informative latent units zm of β-VAE have the highest KL divergence from the unit
Gaussian prior (p(z) = N (0, I)), while the uninformative latents have KL divergence close to
zero. Fig. 7A demonstrates the selectivity of each latent zm to the independent data generating
factors: Zm = f(Vk) ∀vk ∈ {VpositionX , IvpositionY, VsCale, Vrotation} (top three rows), where Zm is
the learnt Gaussian mean of latent unit zm . The effect of traversing each latent zm on the resulting
reconstructions is shown in the bottom five rows of Fig. 7A. The latents Z6 and Z2 learnt to encode X
and Y coordinates of the objects respectively; unit Z1 learnt to encode scale; and units Z5 and Z7 learnt
to encode rotation. The frequency of oscillations in each rotational latent corresponds to the rotational
symmetry of the corresponding object (2π for heart, π for oval and π∕2 for square). Furthermore,
the two rotational latents seem to encode cos and sin rotational coordinates, while the positional
latents align with the Cartesian axes. While such alignment with intuitive factors for humans is not
guaranteed, empirically we found it to be very common. Fig. 7B demonstrates that the unmodified
8
Published as a conference paper at ICLR 2017
Model	Disentanglement metric score
Ground truth Raw pixels PCA ICA	100% 45.75 ± 0.8% 84.9 ± 0.4% 42.03 ± 10.6%
DC-IGN InfOGAN	99.3 ± 0.1% 73.5 ± 0.9%
VAE untrained VAE β-VAE	44.14 ± 2.5% 61.58 ± 0.5% 99.23 ± 0.1%
Disentanglement Metric Score
Figure 6: Disentanglement metric classification accuracy for 2D shapes dataset. Left: Accuracy for
different models and training regimes Right: Positive correlation is present between the size of z and
the optimal normalised values of β for disentangled factor learning for a fixed β-VAE architecture. β
values are normalised by latent z size m and input x size n. Note that β values are not uniformly
sampled. Orange approximately corresponds to unnormalised β = 1. Good reconstructions are asso-
ciated with entangled representations (lower disentanglement scores). Disentangled representations
(high disentanglement scores) often result in blurry reconstructions.
VAE baseline (β = 1) is not able to disentangle generative factors in the data as well as β-VAE with
appropriate learning pressures. Instead each latent z (apart from z9, which learnt rotation) encodes at
least two data generative factors. InfoGAN also achieved a degree of disentangling (see Fig. 7D),
particularly for positional factors. However, despite our best efforts to train InfoGAN, we were not
able to achieve the same degree of disentangling in other factors, such as rotation, scale and shape.
We also found its ability to generate the different shapes in the dataset to be inaccurate and unstable
during training, possibly due to reported limitations of the GAN framework, which can struggle to
learn the full data distribution and instead will often learn a small subset of its modes (Salimans et al.,
2016; Zhao et al., 2016).
Understanding the effects of β We hypothesised that constrained optimisation is important for
enabling deep unsupervised models to learn disentangled representations of the independent data
generative factors (Sec. 2). In the β-VAE framework this corresponds to tuning the β coefficient. One
way to view β is as a mixing coefficient (see Sec. A.6 for a derivation) for balancing the magnitudes
of gradients from the reconstruction and the prior-matching components of the VAE lower bound
formulation in Eq. 4 during training. In this context it makes sense to normalise β by latent z size
m and input x size n in order to compare its different values across different latent layer sizes
and different datasets (βnorm = βM). We found that larger latent Z layer sizes m require higher
constraint pressures (higher β values), see Fig. 6 (Right). Furthermore, the relationship of β for a
given m is characterised by an inverted U curve. When β is too low or too high the model learns an
entangled latent representation due to either too much or too little capacity in the latent z bottleneck.
We find that in general β > 1 is necessary to achieve good disentanglement. However ifβ is too
high and the resulting capacity of the latent channel is lower than the number of data generative
factors, then the learnt representation necessarily has to be entangled (as a low-rank projection of
the true data generative factors will compress them in a non-factorial way to still capture the full
data distribution well). We also note that VAE reconstruction quality is a poor indicator of learnt
disentanglement. Good disentangled representations often lead to blurry reconstructions due to the
restricted capacity of the latent information channel z, while entangled representations often result in
the sharpest reconstructions. We therefore suggest that one should not necessarily strive for perfect
reconstructions when using β-VAEs as unsupervised feature learners - though it is often possible
to find the right β-VAE architecture and the right value ofβ to have both well disentangled latent
representations and good reconstructions.
We proposed a principled way of choosing β for datasets with at least weak label information. If
label information exists for at least a small subset of the independent data generative factors of
variation, one can apply the disentanglement metric described in Sec. 3 to approximate the level of
learnt disentanglement for various β choices during a hyperparameter sweep. When such labelled
information is not available, the optimal value ofβ can be found through visual inspection of what
9
effec二he traversal Of each SingIe Iarenr Unir Zm has On rhe generafed images (X 一 Z) in PiXeI SPaCe
(as ShOWnin Fig∙ 7 rows 4I8)∙ FOrrhe 2D ShaPeS dafaseLWe have found Cha二he OPrimaI VaIUeS
Of I3 as defermined by ViSUaIinSPeeriOn mash CIOSeIy Che OPrimaI VaIUeS as defermined by Che
disenfanglemenf mefric∙
5 CoNCLUSIoN
In fhis PaPer We have reformulafed fhe Sfandard VAE framework (Kingma 济 we≡ng∙ 2014; ReZende
er al: BoI4) as a ConSrrained OPrimiSarion PrObIem Wirh Srrong Iarenr CaPaCiry ConSrrainr and in—
dependence Prior PreSSUres∙ By augmenting rhe IOWer bound formularo∙n Wirhrhe I3 COefnCienr
Char regulates rhe Srrengrh Of SUCh PreSSUreS and" as a ConSeqUenCe 二 he qua≡arive nature Of rhe
represenfafions Iearnf by fhe modeLWe have achieved Sfafe OffhearfreSUHS for learning disenfangled
represenraaons Of dafa generative factors ∙ We have ShOWn ChaC OUr PrOPOSedvvAE framework
SignifiCamly OUfPerfOrmS bofh qua≡afively and quanH∙afively fhe OriginaI VAE (Kingma <& welling。
2014)" as well as Srare—OMhe—arr UnSUPerViSed (InfOGAN: Chen ef al: 2016) and Semi—supervised
(DC—IGN: KUIkami ef al: 2015) approaches fo disenfangled facf or learning, FUrthermOrPWe have
ShOWn rhaf 6—VAE COnSiSrenrIy and robuSrIy discovers more factors OfVariariOn in rhe darpand if
IeamS a represenraro∙n ChaC COVerS a Widerrange Of facsr VaIUeS and is disenfangled more CleanIy
Chan Orher benchmarks" all in a COmPIereIy UnSUPerViSed manner, Unlike InfOGAN and DC—IGZ"
OUr approach does nor depend On any P PriOri knowledge abou 二 he number Orrhe naare Of dafa
generative facf or s ∙ OUr Preliminary invesfigafions SUggeS 二 ha 二 he PerfOrmanCe OffhevvAE frame—
Work may depend On rhe SamPling density Of Che dafa generaHVe factors Wirhin a training da8ser
(See APPendiX A∙8 for more derails)，H appears ChaC having more denseIy SamPled dafa generaHVe
factors results in better disenfangling performance OfVVAE。however We IeaVe a more PrinCiPled
invesfigafion OffhiS effec二O fufure work∙
1。
PUbliShed as a COnferenCe PaPer af ICLR 2017
Ieamt single latent mean traversal
variance
∙51
rotation
rotation scale
DC—IGN
Z4 Zl n2
pllo∙32lo∙45
Objea
FIgUre T. A: RePreSemarIOnSleamr by
a B—VAE (6 = 4)∙ EaCh ColUmn rePre—
SenrS a Iarenr 2: Ordered according S
Che Iearnr GaUSSian VarianCe (IaSrrOW).
ROWI (POSiron) ShOWS rhe mean acr
VaHOn (red represents high VaIUeS) Of
each Iafenf Zi as a function Of all 32x32
IOCaHOnS averaged across ObjeCFrora—
Hons and scales，ROW 2 and 3 ShOW rhe
mean acfivafion Of each Unif 型 as a func—
Hon Of SCaIe (respectiveIy rorarion)" av—
eraged across rofafions and POSifiOnS (re—
SPeCfiVely SCaleS and POSifiOnS)∙ SqIlare
is red" OVaZiS green and heart is blue.
ROWS 4—8 (SeCOnd group) ShOW recon—
SrrUCHOnS resuming from rhe traversal
Of each IaCenC 型 OVerrhree Srandard de—
ViariOnS around Che Unir GaUSSian PriOr
mean WhiIe keeping fhe remaining 9/10
Iarenr UnirS Hxed Co Che VaIUeS Obrained
by running inference On an image from
fhe dafaseL B: SimiIar anaIySiS for VAE
(6 = I)∙ C: Similar analysis for DC—
IGN∙ CIamPing a SingIe Iarenr each for
SCappos≡on∞Orienrarion and 5 for
ShaPe∙ D: SimiIar anaIySiS for InfOGAN.
USing 5 COnrinUOUSlarenrS regularized
USing rhe muaal informaro∙n COSL and
5 additional UnCOnSfrained noise IafenfS
(nof ShOWn) ∙
Published as a conference paper at ICLR 2017
β-VAE is robust with respect to different architectures, optimisation parameters and datasets, hence
requiring few design decisions. Our approach relies on the optimisation of a single hyperparameter
β, which can be found directly through a hyperparameter search if weakly labelled data is available
to calculate our new proposed disentangling metric. Alternatively the optimal β can be estimated
heuristically in purely unsupervised scenarios. Learning an interpretable factorised representation
of the independent data generative factors in a completely unsupervised manner is an important
precursor for the development of artificial intelligence that understands the world in the same way
that humans do (Lake et al., 2016). We believe that using our approach as an unsupervised pretraining
stage for supervised or reinforcement learning will produce significant improvements for scenarios
such as transfer or fast learning.
6 Acknowledgements
We would like to thank Charles Blundell, Danilo Rezende, Tejas Kulkarni and David Pfau for helpful
comments that improved the manuscript.
References
M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based
2d-3d alignment using a large dataset of cad models. In CVPR, 2014.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. In
IEEE Transactions on Pattern Analysis & Machine Intelligence, 2013.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. arXiv,
2016.
Brian Cheung, Jesse A. Levezey, Arjun K. Bansal, and Bruno A. Olshausen. Discovering hidden
factors of variation in deep networks. In Proceedings of the International Conference on Learning
Representations, Workshop Track, 2015.
T. Cohen and M. Welling. Transformation properties of learned visual representations. In ICLR,
2015.
Taco Cohen and Max Welling. Learning the irreducible representations of commutative lie groups.
arXiv, 2014.
G. Desjardins, A. Courville, and Y. Bengio. Disentangling factors of variation via generative
entangling. arXiv, 2012.
Carl Doersch. Tutorial on variational autoencoders. arxiv, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 2011.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. NIPS,pp. 2672-2680, 20l4.
Ross Goroshin, Michael Mathieu, and Yann LeCun. Learning to linearize under uncertainty. NIPS,
2015.
G. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. International Conference
on Artificial Neural Networks, 2011.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Theofanis Karaletsos, Serge Belongie, and Gunnar Ratsch. Bayesian representation learning with
oracle constraints. ICLR, 2016.
W. Karush. Minima of Functions of Several Variables with Inequalities as Side Constraints. Master’s
thesis, Univ. of Chicago, Chicago, Illinois, 1939.
D. P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.
H. W. Kuhn and A. W. Tucker. Nonlinear programming. In Proceedings of 2nd Berkeley Symposium,
pp. 481-492, 1951.
Tejas Kulkarni, William Whitney, Pushmeet Kohli, and Joshua Tenenbaum. Deep convolutional
inverse graphics network. NIPS, 2015.
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people. arXiv, 2016.
11
Published as a conference paper at ICLR 2017
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. ICCV, 2015.
P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter. A 3d face model for pose and
illumination invariant face recognition. AVSS, 2009.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,
Alexandre Passos, and David Cournapeau. Scikit-learn: Machine learning in python. Journal of
Machine Learning Research, 2011.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv, 2015.
Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of
variation with manifold interaction. ICML, 2014.
Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-
mate inference in deep generative models. arXiv, 2014.
Karl Ridgeway. A survey of inductive biases for factorial Representation-Learning. arXiv, 2016.
URL http://arxiv.org/abs/1612.05299.
Oren Rippel and Ryan Prescott Adams. High-dimensional probability estimation with deep density
models. arXiv, 2013.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. arXiv, 2016. URL http://arxiv.org/abs/1606.
03498.
Jurgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation,
4(6):863-869,1992.
Wenzhe Shi, Jose Caballero, Ferenc Huszdr, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel
Rueckert, and Zehan Wang. Real-Time single image and video Super-Resolution using an efficient
Sub-Pixel convolutional neural network. arXiv, 2016.
Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton. Tensor analyzers. In Proceedings of the
30th International Conference on Machine Learning, 2013, Atlanta, USA, 2013.
William F. Whitney, Michael Chang, Tejas Kulkarni, and Joshua B. Tenenbaum. Understanding
visual concepts with continuation learning. arXiv, 2016. URL http://arxiv.org/pdf/
1602.06822.pdf.
Jimei Yang, Scott Reed, Ming-Hsuan Yang, and Honglak Lee. Weakly-supervised disentangling with
recurrent transformations for 3d view synthesis. NIPS, 2015.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv,
2016. URL http://arxiv.org/abs/1609.03126.
Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view perceptron: a deep model for learning face identity
and view representations. In Advances in Neural Information Processing Systems 27. 2014.
A Appendix
A. 1 Model architecture details
A summary of all model architectures used in this paper can be seen in Tbl 1.
A.2 InfoGAN training
To train the InfoGAN network described in Tbl. 1 on the 2D shapes dataset (Fig. 7), we followed
the training paradigm described in Chen et al. (2016) with the following modifications. For the
mutual information regularised latent code, we used 5 continuous variables ci sampled uniformly
from (-1, 1). We used 5 noise variables zi, as we found that using a reduced number of noise
variables improved the quality of generated samples for this dataset. To help stabilise training, we
used the instance noise trick described in Shi et al. (2016), adding Gaussian noise to the discriminator
inputs (0.2 standard deviation on images scaled to [-1, 1]). We followed Radford et al. (2015) for the
architecture of the convolutional layers, and used batch normalisation in all layers except the last in
the generator and the first in the discriminator.
12
Published as a conference paper at ICLR 2017
Dataset	Optimiser		Architecture
2D shapes	Adagrad	Input	4096 (flattened 64x64x1).
(VAE)	1e-2	Encoder	FC 1200, 1200. ReLU activation.
		Latents	10
		Decoder	FC 1200, 1200, 1200, 4096. Tanh activation. Bernoulli.
2D shapes	rmsprop	Input	64x64x1.
(DC-IGN)	(as in Kulkarni et al., 2015)	Encoder	Conv 96x3x3, 48x3x3, 48x3x3 (padding 1).
			ReLU activation and Max pooling 2x2.
		Latents	10
		Decoder	Unpooling, Conv 48x3x3, 96x3x3, 1x3x3.
			ReLU activation, Sigmoid.
2D shapes	Adam	Generator	FC 256, 256, Deconv 128x4x4, 64x4x4 (stride 2). Tanh.
(InfoGAN)	1e-3 (gen)	Discriminator	Conv and FC reverse of generator. Leaky ReLU activation.
	2e-4 (dis)		FC 1. Sigmoid activation.
		Recognition	Conv and FC shared with discriminator. FC 128, 5. Gaussian
		Latents	10: Zi…5 〜Unif (-1,1), cι...5 〜Unif (-1,1)
Chairs	Adam	Input	64x64x1.
(VAE)	1e-4	Encoder	Conv 32x4x4 (stride 2), 32x4x4 (stride 2), 64x4x4 (stride 2),
			64x4x4 (stride 2), FC 256. ReLU activation.
		Latents	32
		Decoder	Deconv reverse of encoder. ReLU activation. Bernoulli.
CelebA	Adam	Input	64x64x3.
(VAE)	1e-4	Encoder	Conv 32x4x4 (stride 2), 32x4x4 (stride 2), 64x4x4 (stride 2),
			64x4x4 (stride 2), FC 256. ReLU activation.
		Latents	32
		Decoder	Deconv reverse of encoder. ReLU activation. Gaussian.
3DFaces	Adam	Input	64x64x1.
(VAE)	1e-4	Encoder	Conv 32x4x4 (stride 2), 32x4x4 (stride 2), 64x4x4 (stride 2),
			64x4x4 (stride 2), FC 256. ReLU activation.
		Latents	32
		Decoder	Deconv reverse of encoder. ReLU activation. Bernoulli.
Table 1: Details of model architectures used in the paper. The models were trained using either
adagrad (Duchi et al., 2011) or adam (Kingma & Ba, 2014) optimisers.
A.3 ICA and PCA baselines
In order to calculate the ICA benchmark, we applied fastICA (Pedregosa et al., 2011) algorithm
to the whitened pixel data. Due to memory limitations we had to apply the algorithm to pairwise
combinations of the subsets of the dataset corresponding to the transforms of each of the three 2D
object identities. We calculated the disentangling metric for all three ICA models trained on each of
the three pairwise combinations of 2D objects, before presenting the average of these scores in Fig. 6.
We performed PCA on the raw and whitened pixel data. Both approaches resulted in similar
disentangling metric scores. Fig. 6 reports the PCA results calculated using whitened pixel data for
more direct comparison with the ICA score.
A.4 Disentanglement metric details
We used a linear classifier to learn the identity of the generative factor that produced zdbiff (see
Equations (5) for the process used to obtain samples of zbdiff). We used a fully connected linear
13
Published as a conference paper at ICLR 2017
classifier to predict p(y|zbdiff), where y is one of four generative factors (position X, position Y, scale
and rotation). We used softmax output nonlinearity and a negative log likelihood loss function. The
classifier was trained using the Adagrad (Duchi et al., 2011) optimisation algorithm with learning
rate of 1e-2 until convergence.
D = {V ∈ RK,W ∈ RH,X ∈ RN}, y 〜Unif [1...K]
Repeat for b = 1 . . . B :
VLP(V WLP(W W2，l ~p(W)，[V2，l]k = {MV：), OftkerwySe
xι,ι 〜Sim(vι,ι, w1,1), x2,1 〜Sim(v2,1, W2,1),	(5)
q(z∣x)〜N(μ(x), σ(x)), Zu = μ(xι,ι), Z2,1 = μ(x2,1)
l	b 1Ll
zdiff = |z1,l - z2,l|，	zdiff = L Ezdiff
l=1
All disentanglement metric score results reported in the paper were calculated in the following manner.
Ten replicas of each model with the same hyperparameters were trained using different random seeds
to obtain disentangled representations. Each of the ten trained model replicas was evaluated three
times using the disentanglement metric score algorithm, each time using a different random seed
to initialise the linear classifier. We then discarded the bottom 50% of the thirty resulting scores
and reported the remaining results. This was done to control for the outlier results from the few
experiments that diverged during training.
The results reported in table in Fig. 6 (left) were calculated using the following data. Ground truth
uses independent data generating factors v (our dataset did not contain any correlated data generating
factors w). PCA and ICA decompositions keep the first ten components (PCA components explain
60.8% of variance). β-VAE (β = 4), VAE (β = 1) and VAE untrained have the same fully connected
architecture with ten latent units z. InfoGAN uses “inferred” values of the five continuous latents that
were regularised with the mutual information objective during training.
A.5 Classifying the ground truth data generative factors values
In order to further verify the validity of our proposed disentanglement metric we ran an extra
quantitative test: we trained a linear classifier to predict the ground truth value of each of the five
data generative factors used to generate the 2D shapes dataset. While this test does not measure
disentangling directly (since it does not measure independence of the latent representation), a
disentangled representation should make such a classification trivial. It can be seen in Table 2 that the
representation learnt by β-VAE is on average the best representation for factor classification across
all five factors. It is closely followed by DC-IGN. It is interesting to note that ICA does well only at
encoding object identity, while PCA manages to learn a very good representation of object position.
Model	id	Classification accuracy				average
		scale	rotation	position X	position Y	
PCA	43.38	36.08	-^596^^	60.66	60.15	41.25
ICA	59.6	34.4	7.61	25.96	25.12	30.54
DC-IGN	44.82	45.92	15.89	47.64	45.88	40.03
InfOGAN	44.47	40.91	6.39	27.51	23.73	28.60
VAE untrained	39.44	25.33	^^6.09^^	16.69	14.39	20.39
VAE	41.55	24.07	8	16.5	18.72	21.77
β-VAE	50.08	43.03	20.36	52.25	49.5	43.04
Table 2: Linear classifier classification accuracy for predicting the ground truth values for each data
generative factor from different latent representations. Each factor could take a variable number of
possible values: 3 for id, 6 for scale, 40 for rotation and 32 for position X or Y. Best performing
model results in each column are printed in bold.
14
Published as a conference paper at ICLR 2017
A.6 INTERPRETING NORMALISED β
We start with the β-VAE constrained optimisation formulation that we have derived in Sec. 2.
L(θ,φ; x, z,β) = Eqφ(z∣χ)[logPθ(x|z)] - β DκL(qφ(z∣x)∣∣p(z))	(6)
We make the assumption that every pixel n in x ∈ RN is conditionally independent given z (Doersch,
2016). The first term of Eq. 6 then becomes:
Eqφ(z∣x) [log Pθ (x|z)] = Eqφ(z∣x)[lθg∏ Pθ(Xn|z)] = Eqφ(z∣x) [£ log Pθ(Xn|z)]	(7)
nn
Dividing both sides of Eq. 6 by N produces:
β
L(θ,φ;x, z,β) (X Eqφ(z∣χ)En[logpθ(Xn|z)] - N Dkl(qφ(z|x)∣∣p(z))	(8)
We design β-VAE to learn conditionally independent factors of variation in the data. Hence we
assume conditional independence of every latent zm given x (where m ∈ 1...M, and M is the
dimensionality of z). Since our prior p(z) is an isotropic unit Gaussian, we can re-write the second
term of Eq. 6 as:
DκL(qφ(z∣x)l∣p(z)) = / qφ(z∣x)logqp(zx) = χ/ qφ(zm∣x)logqp(TIX)
Multiplying the second term in Eq. 8 by a factor M produces:
L(θ, φ; x, z,β) X Eqφ(z∣χ)En[lθgpθ(Xn|z)]一
βM Em/mE (ZmIX)log⅛^
βM
Eqφ(z∣x)En [log pθ (XnlZ)]	N- Em[DKL (qφ (zm ∣X)Ilp(Zm))]
(9)
(10)
Hence using
βnorm
βM
NT
in Eq. 10 is equivalent to optimising the original β-VAE formulation from Sec. 2, but with the
additional independence assumptions that let us calculate data log likelihood and KL divergence
terms in expectation over the individual pixels xn and individual latents Zm .
A.7 RELATIONSHIP BETWEEN β AND
For a given E we can solve the constrained optimisation problem in Eq. 3 (find the optimal (θ*, φ*, β*),
such that ∆F(θ*, φ*, β*) = 0). We can then re-write our optimal solution to the original optimisation
problem in Eq. 2 as a function of E:
G (θ*(e),φ*(e)) = Eqφ* (C) (z|x)[log pθ*(e)(xlz)]
(11)
Now β can be interpreted as the rate of change of the optimal solution (θ*, φ*) to G when varying the
constraint E:

(12)
A.8 Data continuity
We hypothesise that data continuity plays a role in guiding unsupervised models towards learning the
correct data manifolds. To test this idea we measure how the degree of learnt disentangling changes
with reduced continuity in the 2D shapes dataset. We trained a β-VAE with β = 4 (Figure 7A) on
subsamples of the original 2D shapes dataset, where we progressively decreased the generative factor
sampling density. Reduction in data continuity negatively correlates with the average pixel wise
(Hamming) distance between two consecutive transforms of each object (normalised by the average
number of pixels occupied by each of the two adjacent transforms of an object to account for object
15
Published as a conference paper at ICLR 2017
scale). Figure 8 demonstrates that as the continuity in the data reduces, the degree of disentanglement
in the learnt representations also drops. This effect holds after additional hyperparameter tuning and
can not solely be explained by the decrease in dataset size, since the same VAE can learn disentangled
representations from a data subset that preserves data continuity but is approximately 55% of the
original size (results not shown).
Normalised Average Hamming distance [pixels]
Figure 8: Negative correlation between data transform continuity and the degree of disentangling
achieved by β-VAE. Abscissa is the average normalized Hamming distance between each of the
two consecutive transforms of each object. Ordinate is disentanglement metric score. Disentangling
performance is robust to Bernoulli noise added to the data at test time, as shown by slowly degrading
classification accuracy up to 10% noise level, considering that the 2D objects occupy on average
between 2-7% of the image depending on scale. Fluctuations in classification accuracy for similar
Hamming distances are due the different nature of subsampled generative factors (i.e. symmetries are
present in rotation but are lacking in position).
A.9 β-VAE SAMPLES
Samples from β-VAE that learnt disentangled (β = 4) and entangled (β = 1) representations can be
seen in Figure 9.
A.10 EXTRA β-VAE TRAVERSAL PLOTS
We present extra latent traversal plots from β-VAE that learnt disentangled representations of 3D
chairs (Figures 10-11) and CelebA (Figures 12-14) datasets. Here we show traversals from all
informative latents from a large number of seed images.
16
Published as a conference paper at ICLR 2017
Figure 9: Samples from β-VAE trained on the dataset of 2D shapes that learnt either a disentangled
(left, β = 4) or an entangled (right, β = 1) representation of the data generative factors. It can be
seen that sampling from an entangled representation results in some unrealistic looking samples. A
disentangled representation that inverts the original data generation process does not suffer from such
errors.
17
Published as a conference paper at ICLR 2017
Figure 10: Latent traversal plots from β-VAE that learnt disentangled representations on the 3D
chairs dataset.
18
Published as a conference paper at ICLR 2017
Figure 11: Latent traversal plots from β-VAE that learnt disentangled representations on the 3D
chairs dataset.
19
Published as a conference paper at ICLR 2017
z2 - skin colour
Figure 12: Latent traversal plots from β-VAE that learnt disentangled representations on
dataset.
*∙ ∙ a ∙xl
Ib壬仔，1多J
the CelebA
20
Published as a conference paper at ICLR 2017
Figure 13: Latent traversal plots from β-VAE that learnt disentangled representations on the CelebA
dataset.
-J *J 4 * I *
B鬣仔
∙⅛β∙a⅛
ttsβs*9
E*4ra∙鼻
ft*<2∙e
制3%
•翁•畲•
■・多庵,∙懵公
H:公
BBCEE^
:tuu 皂
∙64f 唐
武善 CB •.?-
∙a<Bt号
∙⅜,<∙ ?
al>>κ
21
Published as a conference paper at ICLR 2017
Figure 14: Latent traversal plots from β-VAE that learnt disentangled representations on the CelebA
dataset.
r ɪ. * ɪ ɪ ɪ ɪ 1
fc∙1 h∙ tι* tι∙朴卜.fɪ*
己皂WA`rr
IrirI一1二.上二
hft*dΓ Y 丫7口
≡≡s
z7- sunglasses/smile z8 - saturation
L⅜⅞⅞⅞⅞⅞l!EU I ⅞ I ⅞ 11
HtTTTTTlirTTrTTTTl

22