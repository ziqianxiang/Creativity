Published as a conference paper at ICLR 2017
Support Regularized Sparse Coding and Its
Fast Encoder
Yingzhen Yang1,2, Jiahui Yu2, Pushmeet Kohli3, Jianchao Yang1, Thomas S. Huang2
1	Snap Research
superyyzg@gmail.com,jianchao.yang@snapchat.com
2	Beckman Institute, University of Illinois at Urbana-Champaign
{jyu79,t-huang1}@illinois.edu
3	Microsoft Research
pkohli@microsoft.com
Ab stract
Sparse coding represents a signal by a linear combination of only a few atoms
of a learned over-complete dictionary. While sparse coding exhibits compelling
performance for various machine learning tasks, the process of obtaining sparse
code with fixed dictionary is independent for each data point without considering
the geometric information and manifold structure of the entire data. We propose
Support Regularized Sparse Coding (SRSC) which produces sparse codes that
account for the manifold structure of the data by encouraging nearby data in the
manifold to choose similar dictionary atoms. In this way, the obtained support
regularized sparse codes capture the locally linear structure of the data manifold
and enjoy robustness to data noise. We present the optimization algorithm of SRSC
with theoretical guarantee for the optimization over the sparse codes. We also
propose a feed-forward neural network termed Deep Support Regularized Sparse
Coding (Deep-SRSC) as a fast encoder to approximate the sparse codes generated
by SRSC. Extensive experimental results demonstrate the effectiveness of SRSC
and Deep-SRSC.
1 Introduction
The aim of sparse coding is to represent an input vector by a linear combination of a few atoms of a
learned dictionary which is usually over-complete, and the coefficients for the atoms are called sparse
code. Sparse coding is widely applied in machine learning and signal processing, and sparse code is
extensively used as a discriminative and robust feature representation with convincing performance
for classification and clustering (Yang et al., 2009; Cheng et al., 2013; Zhang et al., 2013). Suppose
the data X = [x1, x2, . . . , xn] ∈ IRd×n lie in the d-dimensional Euclidean space IRd, and the
dictionary matrix is D = [D1, D2, . . . , Dp] ∈ IRd×p with each Dk ∈ IRd (k = 1, . . . ,p) being an
atom of the dictionary, sparse coding method seeks for the linear sparse representation with respect
to the dictionary D for each vector x ∈ X by solving the following convex optimization problem:
minX 11lxi - DZill2 + λ∣∣Zi∣∣ι s.t. ∣∣Dk∣∣2 ≤ co,k = 1,...,p
, i=1
where λ is a weighting parameter for the `1 -norm of z, and c0 is a positive constant that bounds
the '2-norm of each dictionary atom. In (Gregor & LeCun, 2010), a feed-forward neural network
named Learned Iterative Shrinkage and Thresholding Algorithm (LISTA) is proposed to produce the
approximation for sparse coding (1). The architecture of LISTA is illustrated in Figure 1. The LISTA
network involves an finite number of stages wherein each stage performs the following operation on
the intermediate sparse code:
z(k+1) = hθ (Wx + Sz(k)), z(0) = 0	(1)
This material is based upon work supported by the National Science Foundation under Grant No. 1318971. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
1
Published as a conference paper at ICLR 2017
Figure 1: Illustration of LISTA network for approximate sparse coding.
where hθ is an element-wise shrinkage function defined as
[hθ(u)]k = sign(uk)(|uk| - θ)+, k = 1, . . . ,p	(2)
and (•)+ = max{∙, 0} is the positive part of a number. Let f indicate the LISTA network and it
generates the approximate sparse code z = f(x, Θ), where Θ = (W, S, θ) collectively denotes
the parameters of the LISTA network. Suppose the optimal sparse codes for the training data
xι,..., Xm are Z*1,..., Z*m, then the parameters Θ are learned by minimizing the cost function
which measures the distance between the predicted approximate sparse codes and the optimal sparse
m
codes: L(Θ) = m P ∣∣Z*i - f (xi, Θ)∣H. The optimization is performed by stochastic gradient
i=1
descent and back-propagation. Inspired by LISTA, a series of previous works have designed neural
networks to simulate different forms of linear coding and achieve end-to-end training for different
tasks such as image super-resolution (Liu et al., 2016) and hashing (Wang et al., 2016).
Sparse coding is widely used to model high-dimensional data. Based on the formulation of sparse
coding (1), it can be observed that the sparse code of each data point is obtained independently
when the dictionary is fixed, which ignores the geometric information and manifold structure of the
high-dimensional data. In order to obtain the sparse codes that account for the geometric information
and manifold structure of the data, many regularized sparse coding methods, such as (Liu et al.,
2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013), employ manifold assumption (Belkin
et al., 2006). Manifold assumption in these methods imposes local smoothness on the sparse codes
of nearby data, namely nearby data are encouraged to have similar sparse codes in the sense of
`2 -distance, and they are termed `2 -Regularized Sparse Coding (`2 -RSC). In this paper, we propose
Support Regularized Sparse Coding (SRSC). Compared to '2-RSC, SRSC captures the locally linear
structure of the data manifold by encouraging nearby data to share dictionary atoms. In addition,
SRSC enjoys robustness to data noise and preserves freedom in the spare representation of data
without constraints on the magnitude of the sparse codes.
The remaining parts of the paper are organized as follows. SRSC and its optimization algorithm,
together with '2-RSC are introduced in the next section. The theoretical properties of the optimization
of SRSC are shown in Section 3, with theoretical guarantee on the obtained sub-optimal solution for
each step of the coordinate descent for obtaining the support regularized sparse codes: convergence
to the critical point of the objective function and being close to the globally optimal solution. We then
show the performance of the SRSC on data clustering, and conclude the paper. We use bold letters
for matrices and vectors, and regular lower letter for scalars throughout this paper. The bold letter
with superscript indicates the corresponding column of a matrix, and the bold letter with subscript
indicates the corresponding element of a matrix or vector. ∣ ∙∣f and ∣∣∙∣∣p denote the Frobenius
norm and the `p -norm.
2 Support Regularized Sparse Coding
2.1 Capturing Locally Linear Structure: S upport Regularized Sparse Coding
In this section, we introduce Support Regularized Sparse Coding (SRSC) which is designed to
capture the locally linear structure of the data manifold for sparse coding. One of the most important
properties of manifold is that it is locally Euclidean, and each data point in the manifold has a
neighbourhood that is homeomorphic to a Euclidean space. The success of several manifold learning
methods, including LLE (Roweis & Saul, 2000), SMCE (Elhamifar & Vidal, 2011) and Locally
Linear Hashing (Irie et al., 2014), is built on exploiting the locally linear structure of manifold. In
these methods, the locally linear structure associated with each data point is a linear representation of
2
Published as a conference paper at ICLR 2017
Figure 2: Illustration of capturing the locally linear structure of the data manifold by Support
Regularized Sparse Coding. Nearby data are encouraged to share dictionary atoms. In this example,
xi and xj choose three common dictionary atoms so they lie on or close to the local subspace S1
spanned by the common atoms, and it is the similar case for xt and xk with local subspace S2 . Due
to the smoothness of the support of the sparse codes, neighboring local subspaces, such as S1 and
S2, can share dictionary atoms. In this example, the two local subspaces share two dictionary atoms
marked in red.
that point by a set of its nearest neighbors in a nonparametric manner, from which the low-dimensional
embedding complying to the manifold structure of the original data is obtained and used for various
learning tasks. In the context of sparse coding, the data lie on or close to the subspaces spanned by
the dictionary atoms specified by the nonzero elements of the corresponding sparse codes. Inspired
by this observation, we propose to capture the locally linear structure of the data manifold for sparse
coding by encouraging nearby data to share the atoms of the dictionary, so that nearby data are on or
close to the local subspace spanned by the common dictionary atoms (see Figure 2).
In order to obtain the sparse codes with locally similar support so as to capture the locally linear
structure of the data manifold, we propose Support Regularized Sparse Coding (SRSC), which uses
support distance to measure the distance between the sparse codes of nearby data. Given a proper
symmetric similarity matrix A, the sparse codes Z that capture the locally linear structure of the
manifold minimizes the following support regularization term:
nn
RA(Z) = 2 XX Aijd(Zi, Zj)	(3)
i=1 j=1
A is usually the adjacency matrix of K-Nearest-Neighbor (KNN) graph, i.e. Aij = 1 if and only
if xi is among the K nearest neighbors of xj or xj is among the K nearest neighbors of xi . Note
that KNN is extensively used in the manifold learning literature, such as Locally Linear Embedding
(LLE) (Roweis & Saul, 2000), Laplacian Eigenmaps (Belkin & Niyogi, 2003) and Sparse Manifold
Clustering and Embedding (SMCE) (Elhamifar & Vidal, 2011), to establish the local neighborhood
in the manifold. d indicates the support distance. For two vectors u, v of the same size, their support
distance is defined below:
|u|
d(u, v) =	(1Iut=0,vt 6=0 + 1Iut 6=0,vt=0)	(4)
t=1
where 1I is the indicator function. When the support distance between Zi and Zj is small for
nonzero Aij , xi and xj choose similar atoms of the dictionary for sparse representation. Therefore,
SRSC captures the locally linear structure of the data manifold by encouraging nearby data to share
dictionary atoms, wherein the common atoms shared by nearby data serve as the basis of the local
subspace.
The optimization problem of SRSC is presented below:
min L(D, Z) = X [ ∣∣xi - DZik2 + λ∣∣Zi∣∣ι + Y RA(Z) s.t. ∣∣Dk ∣∣2 ≤ 1,k = 1,...,p	(5)
D,Z	i=1 2
where γ > 0 is the weighting parameter for the support regularization term. Similar to (Lee et al.,
2006), problem (5) is optimized alternatingly with respect to the dictionary D and the sparse codes Z
respectively with the other variable fixed.
3
Published as a conference paper at ICLR 2017
2.1.1	OPTIMIZING WITH RESPECT TO D WITH FIXED Z
The optimization with respect to D with fixed Z is a quadratic programming problem:
min 1 ∣∣X - DZIIF s.t. ∣∣Dk∣∣2 ≤ 1, k = 1,...,p	(6)
which can be solved using Lagrangian dual (Lee et al., 2006).
2.1.2	OPTIMIZING WITH RESPECT TO Z WITH FIXED D
We use coordinate descent to optimize (5) with respect to Z with fixed D:
minXX 2∣xi - DZiIl2 + λ∣Zi∣ι + YRA(Z)	⑺
i=1
In each step of coordinate descent, the optimization is performed over the i-th column of Z, while
fixing all the other sparse codes {Zj }j 6=i . For each 1 ≤ i ≤ n, the optimization problem for Zi is
below:
min F (Zi) = 1 IlXi- DZill2 + λ∣Zi∣ι + YRA (Zi)	(8)
Zi	2
n
where RA(Zi) = P Aijd(Zi, Zj).
j=1
Inspired by recent advances in solving non-convex optimization problems by proximal linearized
method (Bolte et al., 2014), proximal gradient descent method (PGD) is used to optimize the
nonconvex problem (8). Although the proximal mapping is typically associated with a lower
semicontinuous function (Bolte et al., 2014) and it can be verified that RA is not always lower
semicontinuous, we can still derive a PGD-styple iterative method to optimize (8).
nn
Define GA ∈ IRp×n as GkAi = P Aij 1IZkj =0 - P Aij 1IZkj 6=0 where 1I is the indicator function,
j=1	j=1
then GkAi indicates the degree to which Zki is discouraged to be nonzero and it can be verified that 1
p
RA (Zi) =	GkAi1IZki 6=0	(9)
k=1
Since each indicator function 1IZki 6=0 is lower semicontinuous, RA is lower semicontinuous if
GAi ≥ 0 for k = 1,...,p. In the following text, We let Q(Zi) = 1 ∣∣Xi - DZi∣∣2∙ The superscript
with bracket indicates the iteration number of PGD or the iteration number of the coordinate descent
without confusion. The PGD-style iterative method for optimizing (8) is as follows:
Zi(t) = Zi(t-1) - -1(D>DZi(t-1) - D>Xi)	(10)
τs
arg min Hk (v) : uk 6= 0 or uk = 0 and GkAi ≥ 0
Zki(t) =	v∈{uk,0}	(11)
ε : uk = 0 and GkAi < 0
for k = 1, . . . , p and ε is any real number such that ε 6= 0 and Hk (ε) ≤ Hk(Z(kti-1)). Hk and u are
defined below:
Hk(V) = -2-(v - Zkti))2 + λ∣v∣ + YGki1Iv=O	(12)
for v ∈ IR and each 1 ≤ k ≤ p, and
U = max{∣么i(t)∣ ———,0} ◦ Sign(Zi(t))	(13)
τs
where ◦ means element-wise multiplication.
Proposition 1 shows that the PGD-style iterative method decreases the value of the objective function
in each iteration.
1 RA (Zi ) is equal to the right hand side of (9) up to a constant.
4
Published as a conference paper at ICLR 2017
Proposition 1. Let the sequence {Zi(t)}t be generated by the PGD-style iterative method with (10)
and (11), then the sequence of the objective {F (Zi (t))}t decreases, and the following inequality
holds for t ≥ 1:
F(Zi(t)) ≤ F(Zi(t-1)) - (T - 1)s ∣∣Zi(t) - Zi(tτ)∣∣2	(14)
And it follows that the sequence {F (Zi(t))}t converges.
Remark 1. (10) and (11) in each iteration of the proposed PGD-style iterative method re-
semble that of the ordinary PGD. (10) performs gradient descent on the differential part, and
(11) can be viewed as an approximate solution to the proximal mapping minv∈IRp H(v) =
(t)
Ts∣∣v 一 Zi ∣H + λ∣M∣ι + YRA(V). Since RA(Zi) is not always lower Semicontinuous,
arg minv∈IRp H(v) is not guaranteed to exist. One can see a simple example where this happens when
Uk = 0 and GAi < 0 for some k = 1,...,p, and in this case infv∈r Hk (V) = Ts(Zkti))2 + YGAi
but this infinitum can not be achieved.
In (10), τ > 1 is a constant and S is the LiPschitz constant for the gradient of function Q(∙), namely
l∣VQ(y) - VQ(z)∣2 ≤ Sky - z∣2, ∀y,z ∈ IRp	(15)
The PGD-style iterative method starts from t = 1 and continues until the sequence {F (Zi (t))}
converges or maximum iteration number is achieved. When the iterative method converges or
terminates for each Zi, the steP of coordinate descent for Zi is finished and the oPtimization algorithm
Proceeds to oPtimize other sParse codes.
Algorithm 1 SuPPort Regularized SParse Coding
Input:
The data set X = {xi}in=1, the Parameter λ, γ, maximum iteration number M for the alternating method
over D and Z, and maximum iteration number Mz for coordinate descent on Z, maximum iteration number
Mp for the PGD-style iterative method on each Zi (i = 1,. . . ,n).
and stoPPing threshold ε.
1:	m= 1
2:	while m ≤ M do
3:	Perform coordinate descent to oPtimize (7) and obtain Z(m) with fixed D(m-1). In i-th (1 ≤ i ≤ n) steP
of each iteration of coordinate descent, solve (8) using the PGD-style iterative method (10) and (11) to
uPdate Zi in each iteration of the PGD-style iterative method.
4:	OPtimize (6) using Lagrangian dual and obtain D(m) with fixed Z(m).
5:	if |L(D(m), Z(m)) - L(D(m-1), Z(m-1))| < ε then
6:	break
7:	else
8:	m = m + 1.
9:	end if
10:	end while
Output: the support regularized sparse codes Z when the above iterations converge or maximum iteration
number is achieved.
Time Complexity
Algorithm 1 describes the algorithm of SRSC. We solve the ordinary sparse coding problem (1) by
the online dictionary learning method (Mairal et al., 2009) and use the dictionary and the sparse codes
as the initialization D(0) and Z(0) for the alternating method in Algorithm 1. In Algorithm 1, the
time complexity of optimization over the sparse codes is O(MMzMpndp2), and time complexity
of optimization over the dictionary using Newton’s method to solve the Lagrangian dual problem is
O (M(np2 + Tnewton(3p2.807 + 2dp2 + dnp))), where Tnewton is the maximum iteration number
for Newton,s method. Therefore, the overall time complexity of Algorithm 1 is O (M (Mz Mpndp2 +
np2 + Tnewton(3p2.807 + 2dp2 + dnp)	. It should be emphasized that the optimization over the
dictionary for SRSC has the same efficiency as the efficient sparse coding method (Lee et al., 2006),
5
Published as a conference paper at ICLR 2017
and the optimization over the sparse code of each data point by the PGD-style iterative method (10)
and (11) is almost as efficient as the widely used Iterative Shrinkage and Thresholding Algorithm
(ISTA) (Daubechies et al., 2004; Beck & Teboulle, 2009). Note that step (10) and (13) are required
by both our method and ISTA; compared to ISTA, the extra operations incurred by our PGD-style
iterative method (10) and (11) are only the arithmetic operations with time complexity 20p for
evaluating the function Hk(∙) defined in (12) for k = 1,...,p. More specifically, evaluating the
value of function Hk(v) takes 10 arithmetic operations and two evaluations at v = uk and v = 0
are needed. Since a compact dictionary is preferred by the extensive study of the sparse coding and
dictionary learning literature and the dictionary size p ≤ 500 is adopted throughout our experiments,
our PGD-style iterative method only incurs extra operations of constant time complexity compared
to ISTA while learning supported regularized sparse codes. In Section 4, we propose Deep-SRSC
as a fast approximation of SRSC with considerable speedup for obtaining the sparse codes of the
new data or the test data (see more details in Section 4.1). Furthermore, we conduct the empirical
study and show that the parallel coordinate descent method, which updates the codes of a group of
P data points in parallel and provides P times speedup over the coordinate descent method used
in Section 2.1.2 and Algorithm 1, exhibits almost the same performance as the coordinate descent
method for the clustering task on the test set of the CIFAR-10 data. Please refer to the details in
the subsection “Deep-SRSC with the Second Test Setting (Referring to the Training Data)” in the
Appendix.
2.2 RELATED WORK: '2 REGULARIZED SPARSE CODING ('2-RSC)
The manifold assumption (Belkin et al., 2006) is usually employed by existing regularized sparse
coding methods (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013) to obtain
the sparse code according to the manifold structure of the data. Interpreting the sparse code of a
data point as its embedding, the manifold assumption in the case of sparse coding for most existing
methods requires that if two points xi and xj are close in the intrinsic geometry of the submanifold,
their corresponding sparse codes Zi and Zj are also expected to be similar to each other in the sense
of '2-distance (Zheng et al., 2011; Gao et al., 2013). In other words, Z varies smoothly along the
geodesics in the intrinsic geometry. Based on the spectral graph theory (Chung, 1997), extensive
literature uses graph Laplacian to impose local smoothness of the embedding and preserve the local
manifold structure (Belkin et al., 2006; Zheng et al., 2011; Gao et al., 2013).
The sparse code Z that captures the local geometric structure of the data in accordance with the
manifold assumption by graph Laplacian minimizes the following `2 regularization term, or the
Laplacian regularization term:
nn
RA2)(Z) = 1 XX AijkZi-Zjk2
(16)
i=1 j=1
where the `2 -norm is used to measure the distance between sparse codes, and A is the same as
that in Section 2.1. LA = DA - A is the graph Laplacian associated with the similarity matrix
A, the degree matrix DA is a diagonal matrix with each diagonal element being the sum of the
n
elements in the corresponding row of A, namely (DA)ii =	Aij. To the best of our knowledge,
j=1
such `2 regularization is employed by most methods that use graph regularization for sparse coding.
Incorporating the `2 regularization term into the optimization problem of sparse coding (1), the
formulation of '2 Regularized Sparse Coding ('2-RSC) is presented below
n
min L('2)(Z) = X ∣∣xi - DZik2 + λ∣∣Zi∣∣ι + Y e Rf2 (Z) s.t. ∣∣Dk∣∣2 ≤ 1,k = 1,...,p	(17)
D,Z
i=1
Advantage of SRSC Over '2-RSC
Although '2-RSC imposes the local smoothness on the sparse codes, it does not capture the locally
linear structure of the data manifold. By promoting the smoothness on the support of the sparse codes
rather than their '2-distance, SRSC encodes the locally linear structure of the manifold in the sparse
codes while reserving freedom in the sparse representation of the data with no constraints on the
6
Published as a conference paper at ICLR 2017
magnitude of the sparse codes. Moreover, as pointed out by (Wang et al., 2015), support regularization
offers robustness to noise for sparse coding. In SRSC, all the data consult their neighbors for choosing
the dictionary atoms rather than choosing the atoms on their own, and the sparse codes of the noisy
data are suppressed since they are forced to choose similar or the same atoms as the nearby clean
data instead of choosing the atoms in the interests of representing themselves.
3 Theoretical Analysis
It can be observed that optimization by coordinate descent over the sparse code in Section 2.1.2
is important for the overall optimization of SRSC, and each step of the coordinate descent (8) is a
difficult nonconvex problem and crucial for obtaining the support regularized sparse code, where the
nonconvexity comes from the support regularization term RA (Zi) (9). Therefore, the optimization
of (8) plays an important role in the overall optimization of SRSC. In the previous section, a PGD-
style iterative method is proposed to decrease the value of the objective in each iteration. In this
section, we provide further theoretical analysis on the optimization of problem (8) when GkAi ≥ 0 for
k = 1, . . . ,p. This condition is equivalent to the condition that the support regularization function
p
Rc(v) , Xck1Ivk6=0	(18)
k=1
is lower semicontinuous, where c ∈ IRp is the coefficients and ck = GkAi . Under this condition, we
prove that the sequence {Zi(t)}t produced by the PGD-style iterative method converges to the sub-
optimal solution which is a critical point of the objective (8). By connecting the support regularized
function to the capped-'1 norm and the nonconvexity analysis of the support regularization term, We
present the bound for '2 -distance between the sub-optimal solution and the globally optimal solution
to (8) in Theorem 1. Note that our analysis is valid for all 1 ≤ i ≤ n.
We first have the following result that the support regularization function (18) is lower semicontinuous
if and only if all the coefficients c are nonnegative.
Proposition 2. The support regularization function (18) is lower semicontinuous if and only if all
the coefficients c are nonnegative.
Therefore, if GkAi ≥ 0 for k = 1, . . . ,p, the support regularization term RA(Zi) is lower semicontin-
uous with respect to Zi in (9). In this case, the PGD-style iterative method proposed in Section 2.1.2
for each iteration t ≥ 1 becomes
Z5i(t) = Zi(t-1) - -1(D>DZi(t-1) - D>xi)	(19)
τs
Zki(t) = arg min Hk (v), k =1, . . . ,p	(20)
v∈{uk,0}
which is equivalent to the updates rules in the ordinary proximal gradient descent method. It is
worthwhile to mention the meaning of the condition that GkAi ≥ 0 for k = 1, . . . , p. For a data point
xi , if the number of its neighbors with zero k-th element of the sparse codes is larger than that with
nonzero k-th element of the sparse codes, which indicates that the neighbors of xi suggest that a zero
k-th element of the sparse code of xi is preferable, then GkAi ≥ 0 and GkAi quantitatively represents
the penalty if the sparse code element Zik is nonzero while the neighbors of xi suggest that Zik = 0 is
preferable. Intuitively, this situation happens when there is conflict between choosing the support of
the code solely by the data point itself and the suggestion of its neighbors; if the point is an outlier or
suffering from noise, the optimization can help that point make a sensible choice by considering the
suggestion of its neighbors. We observe that GkAi ≥ 0 for k = 1, . . . ,p happens in all the data sets
used in this paper.
In the following lemma, we show that the sequence {Zi(t)}t generated by (19) and (20) converges to
a critical point of F(Zi), denoted by Zi. Denote by Zi* the globally optimal solution to the original
* *	*4	* *r4 *
optimization problem (8). The following lemma also shows that both Zi and Zi are local solutions
to the capped-'1 regularized problem (21). Before stating the lemma, the following definitions are
introduced which are essential for our analysis.
7
Published as a conference paper at ICLR 2017
Definition 1. (Critical points) Given the non-convex function f : IRn → R ∪ {+∞} which is a
proper and lower semi-continuous function.
•	for a given x ∈ domf, its Frechet subdifferential of f at x, denoted by ∂f (x), is the set of
all vectors u ∈ IRn which satisfy
limsup f(y) -f(x) -hu，y -Xi ≥ 0
y6=x,y→x	ky- xk
•	The limiting-subdifferential of f at x ∈ IRn, denoted by written ∂f (x), is defined by
∂f(x) = {u ∈ IRn : ∃xk → X,f(xk) → f(x),uk ∈ ∂f(xk) → u}
The point x is a critical point of f if 0 ∈ ∂f (x).
Also, We are considering the following capped-'1 regularized problem, which replaces the indicator
function in the support regularization term RA(Zi) with the continuous capped-'1 regularization
term T:
1
βmRnp Lcapped-'1 (β) = 2 Ilxi - Dek2 + λkβk1 + T(β; b)	QI)
p
where T(β; b) = P Tk(βk; b), Tk(t; b) = YGAi mm{btl,b} for some b > 0. It can be seen that the
k=1
objective function of the capped-'1 problem approaches that of (8) when mm{bt|,b} approaches the
indicator function 1It=o as b → 0+. Define P(∙; b) = λ∣∣∙k 1 + T(∙; b), the location solution to the
capped-'1 problem is defined as follows.
Definition 2. (Local solution) A vector β is a local solution to the problem (21) if
∣D>(Dβ - Xi) + P(β; b)∣2 =0	(22)
where	P(β; b)	=	[P1(β1;	b),P2(β2; b),...,Pp(βp;	b)]>,	Pk(t； b)	=	λ∣t∣	+	Tk(t; b)	for k =
1,	. . . , p.
Note that in the above definition and the following text, Pk(t; b) can be chosen as any value between
the right differential d∂pk(t+; b) (or Pk(t+; b)) and left differential ∂pk(t-; b) (or Pk(t-; b)) for
k = 1, . . . , p.
Definition 3. (Degree of Nonconvexity of a Regularizer) For κ ≥ 0 and t ∈ IR, define
θ(t, κ) := sup{-sgn(s — t)(P(s; b) — P(t; b)) — κ∣s — t|}
as the degree of nonconvexity for function P. If u = (u1, . . . , up)> ∈ IRp, θ(u, κ)
[θ(u1, κ), . . . , θ(up, κ)]. sgn is the sign function.
Note that θ(t, κ) = 0 for convex function P .
Let Si = SuPP(Zi) where supp(∙) indicates the support of a vector, i.e. the indices of its nonzero
elements. Denote by Zi* the globally optimal solution to (8), and S* = SuPP(Zi*), then we have
Lemma 1. For any 1 ≤ i ≤ n, if GkAi ≥ 0for k = 1, . . . , p, then the sequence {Zi(t) }t generated
by (10) and (11) converges to a critical point of F (Zi), which is denoted by Zi. Moreover, if
0 <b< min{min |Z j |,	max	—罚一Y Gki--------, min ∣Zj*∣,	max	-ɪʒ——Y Gki-------}
j∈S	kESiCAi=0 (∂Z^|zi=zi - λ)+ j∈S	k∈si ,GAi=0 (∂zi|zi=zi* - λ) +
(23)
*
(ιfthe denominator is 0,小 is defined to be +∞ in the above inequality), then both Zi and Zi are
local solutions to the capped-'1 regularized problem (21).
8
Published as a conference paper at ICLR 2017
Using the degree of nonconvexity of the regularizer P, we have the following theorem showing that
the sub-optimal solution Zi obtained by our PGD-style iterative method can be close to the globally
optimal solution to the original problem (8), i.e. Zi*. In the following text, BI indicates a submatrix
of B whose columns correspond to the nonzero elements of I, and σmin(∙) indicates the smallest
singular value of a matrix.
Theorem 1. (Sub-optimal solution is close to the globally optimal solution) For any 1 ≤ i ≤ n, let
Ei = S i ∪S*. Suppose GA ≥ 0 for k = 1,..., p, DEi is not singular with κ0 , σmin (DEi ) > 0,
κ02 > κ > 0, and b is chosen according to (23) as in Lemma 1. Let Si = (Si \ Si*) ∪ (Si* \ Si) be the
symmetric difference between ^i and S*, then
kZi*- Zik2 ≤	-1— ((	X	(max{0,γGAi	-κ∣Zfci - b|})2	+ X	(maχ{0,噜一κb})2)2 + 间2
κ0 - κ	b	b
0	k∈Si∩;3i	k∈Si∖Si
(24)
where t ∈ IRp, tk = 2λ1Izi *2i <0 + 01IZi *2i >0 for k ∈ S% ∩ S*, and tk = 0 forall other k.
Remark 2. Note that the bound for distance between the sub-optimal solution and the globally
optimal solution presented in Theorem 1 does not require typical Restricted Isometry Property (RIP)
γGA	γGA
conditions, e.g. Cands (2008). Also, when ɪɪki — κ∣Zk% — b| and ɪɪki — Kb are no greater than
0 and Zi * and Zi has the same sign in the intersection of their support, the sub-optimal solution
γGA	γGA
Zi IS equal to the globally optimal SOlUtion. When ɪ-bki — κ∣Zk% — b| and 1^— - Kb are small
positive numbers and Zi * and Zi has similar sign in the intersection oftheir support, Zi is close to
the globally optimal solution.
4 Deep Support Regularized Sparse Coding: Deep-SRSC
x—►[w
>^m^^^∣^ hd∑}*+*0*[ min pooling
min pooling A
z
Figure 3:	Illustration of Deep-SRSC for approximate Support Regularized Sparse Coding.
Inspired by the PGD-style iterative method (10) and (11) for SRSC and the LISTA network, we
propose Deep Support Regularized Sparse Coding (Deep-SRSC) illustrated in Figure 3, which is
a neural network that produces the approximate support regularized sparse codes for SRSC. The
goal of Deep-SRSC is to approximate the sparse codes of the input data in a fast way by feeding the
data through the Deep-SRSC network, instead of running the iterative optimization algorithm for
SRSC in Section 2.1. To achieve this goal, the Deep-SRSC network is trained on the training data
by minimizing the squared distance between the predicted codes of the training data by the network
and their ground truth codes. The network design of Deep-SRSC is in accordance with the proposed
PGD-style iterative method. When W = LD>, S = I — LD>D where L = τ s, then each stage in
the recurrent structure of Deep-SRSC implements one iteration of PGD-style iterative method, i.e.
(10) and (11). In Deep-SRSC, W, S and L are to be learned by the network rather than computed
from a pre-computed dictionary D, and S is shared over different layers. The min-pooling neuron
in Deep-SRSC outputs the result of arg minHk(v) or ε, according to the update rule (11). Figure 3
v∈{uk ,0}
illustrates Deep-SRSC with 2 layers.
Denote the training data by x1, . . . , xm, and let Zsr be the ground truth support regularized sparse
codes of the training data which are obtained by the optimization method introduced in Section 2.1.
Let fsr be the Deep-SRSC encoder which produces the approximate support regularized sparse
code z = fsr(x, Θsr), where Θsr = (W, S, L) denotes the parameters of Deep-SRSC. Then the
parameters of Deep-SRSC are learned by minimizing the cost function which measures the distance
between the predicted approximate support regularized sparse codes and the ground truth ones:
m
ml P ∣∣ZSr — fsr (xi, Θsr)k2. Similar to the LISTA network, the optimization is performed by
i=1
9
Published as a conference paper at ICLR 2017
stochastic gradient descent and back-propagation. The batch size is set to 1 so as to simulate the
coordinate descent method for optimization over the sparse codes in Section 2.1.2. The adjacency
matrix of the KNN graph over the training data is required as input for training the network.
The approximate codes of the new data, or the test data, are obtained by feeding the new data through
the Deep-SRSC network learned on the training data. We provide two test settings below, depending
on whether training data are referred to in the test process.
1)	In the first setting where the training data are not referred to, the test data are a group of data
points. The test data and the KNN graph over them are fed into the Deep-SRSC network to obtain the
approximate codes of the test data. The locally linear manifold structure of the test data is encoded
in the KNN graph over the test data. This setting is potentially more suitable for the situation of
limited storage where the training data and their codes do not need to be stored in the test process.
This setting may not be suitable for the test data that do not reliably reflect the locally linear manifold
structure (e.g. in the case of a very small amount of test data), and in this case the second setting
below is a better choice.
2)	In the second setting where training data are referred to, the approximate code of each data point
is obtained by feeding that point and the KNN graph over that point and the training data into the
Deep-SRSC network. The code of each test point is reliably obtained by referring to its nearest
neighbors in the training data and this process is independent of the factor that whether the test data
reflect the locally linear manifold structure.
4.	1 Deep-SRSC as Fast Encoder
It should be emphasized that Deep-SRSC is a fast encoder for SRSC when obtaining the codes of the
new data (or test data). Each layer of Deep-SRSC resembles one iteration of the PGD-style iterative
method (10) and (11), and the computational cost of feeding forward a data point through one layer is
the same as that of executing one iteration of the PGD-style iterative method for that point. Therefore,
the feed-forward process of obtaining the sparse codes of the new data using '-layer DeeP-SRSC
is around MMp times faster than the PGD-Style iterative method used in Algorithm 1, where Mp
is the maximum iteration number for the PGD-style iterative method. In the experimental results
shown in the next section, Deep-SRSC with different number of layers are employed to produce the
approximate support regularized sparse codes, and 6-layer Deep-SRSC achieves minimum prediction
error. With Mp = 50 throughout our experiments, DeeP-SRSC is around 560 ≈ 8.3 times faster than
the PGD-style iterative method. Our analysis in this subsection holds for both test settings.
Table 1: Clustering results on USPS handwritten digits database. c in the left column is the cluster number, i.e.
the first C clusters of the entire data are used for clustering.
USPS # Clusters	Measure	KM	SC	Sparse Coding	'2-RSC	SRSC
c = 4	-AC	0.9243	0.4514	0.9869	0.9869	0.9880
	-NMI-	0.7782	0.4160	0.9429	0.9429	0.9467
c = 6	-AC	0.7130	0.4325	07781	0.7781	0.9723
	-NMI-	0.6845	0.4865	0.8507	0.8507	0.9135
c = 8	-AC	0.7294	0.4227	0.8163	0.8163	0.9645
	-NMI-	0.6851	0.4811	0.8669	0.8669	0.9027
c = 10	-AC	0.6878	0.4041	0.8178	0.8287	0.8293
	NMI	0.6312	0.4765	0.8321 —	0.8398	0.8471
Table 2: Clustering results on various data sets
Data Set	Measure	KM	SC	Sparse Coding	`2 -RSC	SRSC
COIL-20	-AC	0.6274	0.3347	0.9903	0.9903	0.9944
	-NMI-	0.7533	0.5667	0.9879	0.9879	0.9933
COIL-100	-AC	0.5221	0.2372	0.6979	0.6979	0.7267
	-NMI-	0.7633	0.5410	0.8837	0.8837	0.8876
UCI Gesture Phase Segmentation	-AC	0.3868	0.3375	0.4003	0.4023	0.4123
	NMI	0.1191	0.1300	0.1164	0.1164	0.1187
10
Published as a conference paper at ICLR 2017
Table 3: Prediction error (average squared error between the predicted codes and the ground truth codes) of
Deep-SRSC with different depth and different dictionary size on the test set of USPS data, using the first test
setting
Dictionary Size	1-layer	2-layer	6-layer
P = 100	-0.06-	-004-	-004-
p = 300	-014-	-009-	-007-
P = 500	0.24	0.12	0.11
Training Error versus Epoch Number for Deep-SRSC with p=100
Training Error versus Epoch Number for Deep-SRSC with p=300
Figure 4:	Training error of Deep-SRSC with dictionary size p
test error of Deep-SRSC is shown in Figure 5 in the appendix.
100, p = 300, andp = 500. The
Table 4: Clustering results on the test set of USPS data with different dictionary size p
Dictionary Size	Measure	KM	SC	Sparse Coding	'2-RSC	SRSC	6-Layer Deep-SRSC
p = 100	-AC	0.6020	0.3279	0.6363	0.6363	0.7105	07155
	-NMI-	0.5522	0.4372	OOn	0.7011	0.7068	0.6778
p = 300	-AC	-	-	0.6408	0.6462	0.7225	07000
	-NMI-	-	-	OOn	0.7011	0.7045	06817
p = 500	-AC	-	-	0.6263	0.6268	0.6248	06836
	NMI	-	-	0.6872	0.6898	0.7221	0.6537
5	Experimental Results
5.1	Clustering Performance
In this subsection, the superiority of SRSC is demonstrated by its performance in data clustering on
various data sets, e.g. USPS handwritten digits data set, COIL-20, COIL-100 and UCI Gesture Phase
Segmentation data set. Two measures are used to evaluate the performance of the clustering methods,
i.e. the Accuracy (AC) and the Normalized Mutual Information (NMI) (Zheng et al., 2004). SRSC is
compared to K-means (KM), Spectral Clustering (SC), Sparse Coding and '2-RSC in Section 2.2.
Throughout all the experiments, we set K = 3 for building the adjacency matrix A of KNN graph,
dictionary size P = 300 and λ = 0.1 for both '2-RSC and SRSC. We also set γ('2) = 1 which is the
suggested default value in (Zheng et al., 2011), and M = Mz = 5 and Mp = 50 in Algorithm 1. The
default value of the weight for support regularization term of SRSC is γ = 0.5. SRSC is implemented
by both MATLAB and CUDA C++ with extreme efficiency, and the code is published on GitHub:
https://github.com/yingzhenyang/SRSC.
The USPS handwritten digits data set is comprised of n = 9298 handwritten images of ten digits
from 0 to 9, and each image is of size 16 × 16 and represented by a 256-dimensional vector. The
whole data set is divided into training set of 7291 images and test set of 2007 images. We run
Algorithm 1 to obtain the support regularized sparse code Z, then build a n × n similarity matrix Y
over all the data. Two similarity measure are employed: the first similarity is the positive part of the
>
inner product of their corresponding sparse codes, namely Yij = max{0, Zi Zj }, the second one is
Yj = Ajq>i Qz^j where qv is a binary vector of the same size as V with element 1 at the indices of
nonzero elements of v. The second similarity measure is name the support similarity and it considers
the number of common dictionary atoms chosen by the sparse codes. Spectral clustering is performed
on the similarity matrix Y to obtain the clustering result of SRSC, and the best performance among
the two similarity measures is reported. The same procedure is performed by all the other sparse
coding based methods to obtain clustering results. The clustering results of various methods are
shown in Table 1.
11
Published as a conference paper at ICLR 2017
COIL-20 Database has 1440 images of resolution 32 × 32 for 20 objects, and the background is
removed in all images. The dimension of this data is 1024. Its enlarged version, COIL-100 Database,
contains 100 objects with 72 images of resolution 32 × 32 for each object. The images of each
object were taken 5 degrees apart when each object was rotated on a turntable. The UCI Gesture
Phase Segmentation data set contains the gesture information of three users when they told stories
of some comic strips in front of the Microsoft Kinect sensor. We use the processed file provided
by the original data consisting of 9873 frames, and the gesture information in each frame is the
vectorial velocity and acceleration of left hand, right hand, left wrist, and right wrist, represented by a
32-dimensional vector. The clustering results on these three data sets are shown in Table 2. It can be
observed from Table 1 and Table 2 that SRSC always produces better clustering accuracy than other
competing methods, due to its capability of capturing the locally linear manifold structure of the data
and robustness to noies. In the appendix, we further show the performance of different sparse coding
based methods with different dictionary size on COIL-100 data set in Table 5, and investigate the
parameter sensitive of SRSC by demonstrating its performance with varying γ and K in Table 6.
5.2	Approximation by Deep-SRSC
In this subsection, Deep-SRSC is employed as a fast encoder to approximate the support regularized
sparse codes of SRSC on the USPS data set. Throughout this subsection, we show results using the
first test setting introduced in Section 4, i.e. test without referring to the training data. Additional
experimental results on the performance of Deep-SRSC with the second test setting, including the
application to semi-supervised learning by label propagation (Zhu et al., 2003), are shown in the
appendix.
The Deep-SRSC network is trained on the training set of the USPS data comprising 7291 images.
We adopt three depth settings wherein Deep-SRSC has 1 layer, 2 layers, and 6 layers respectively.
We first run SRSC on the training set of USPS data to obtain the dictionary Dsr and the support
regularized sparse codes Zsr of the training data. Then the optimization problem (7) is solved by the
PGD-style iterative method in Section 2.1.2, where X is the test data, A is the adjacency matrix of
the KNN graph over the test data, to obtain the support regularized sparse codes Zsr,test of the test
data with dictionary Dsr . Zsr is used as the ground truth support regularized sparse codes to train
Deep-SRSC, and Zsr,test serves as the ground truth codes of the test data. The approximate codes of
the test data of the USPS data are obtained by feeding forward them into the Deep-SRSC network
together with the KNN graph over the test data, and the prediction error of Deep-SRSC is the average
of the squared error between the predicted codes and Zsr,test. Figure 4 illustrates the training error of
Deep-SRSC w.r.t. the epoch number for 1 layer, 2 layers, and 6 layers respectively, and Figure 5 in
the appendix illustrates the test error of Deep-SRSC. For each depth setting, Deep-SRSC is trained
with 300 epoches, and testing is performed for every 5 epoches during training. It can be observed
that deeper Deep-SRSC leads to smaller training and test error. Deep-SRSC is implemented with
TensorFlow (Abadi et al., 2016). The initial learning rate is set to 10-4, and divided by 10 at 100-th
epoch and 200-th epoch, so the final learning rate is 10-6 upon the termination of the training.
Table 3 shows the prediction error of Deep-SRSC for different dictionary size p and different number
of layers. It can be observed that Deep-SRSC with more layers demonstrates smaller prediction
error for the same dictionary size due to its better representation capability, and smaller dictionary
size leads to less prediction error for the same number of layers due to the reduced difficulty of
representation. Moreover, the codes predicted by 6-layer Deep-SRSC are used to perform clustering
on the test data because of its minimum prediction error, with comparison to the performance of
sparse coding and '2-RSC shown in Table 4 with respect to different dictionary size. For either sparse
coding or '2-RSC, the dictionary is firstly learned on the training data, then the sparse codes of the
test data obtained with respect to that dictionary are used to perform clustering on the test set of
USPS data. We can see that SRSC together with its approximation, 6-layer Deep-SRSC, achieve the
highest accuracy and NMI. In addition, a reasonably large dictionary benefits SRSC, e.g. increasing p
from 100 to 300 boosts its accuracy, since the dictionary atoms serve as the basis for the locally linear
structures (local subspaces) of the data manifold and a sufficiently large dictionary size is favorable
for modeling all such locally linear structures. On the other hand, a too large dictionary (such as
p = 500) imposes much difficulty on the optimization which can even hurt the performance of SRSC,
`2 -RSC and regular sparse coding.
12
Published as a conference paper at ICLR 2017
6	Conclusion
We propose Support Regularized Sparse Coding (SRSC) which captures the locally linear manifold
structure of the high-dimensional data for sparse coding and enjoys robustness to noise. SRSC
achieves this goal by encouraging nearby data in the manifold to share dictionary atoms. The
optimization algorithm of SRSC is presented with theoretical guarantee for the optimization over
the sparse codes. In addition, we propose Deep-SRSC, a feed-forward neural network, as a fast
encoder to approximate the support regularized sparse codes produce by SRSC. Experimental results
demonstrate the effectiveness of SRSC by its application to data clustering, and show that Deep-SRSC
renders approximate codes for SRSC with low prediction error. The approximate codes generated by
6-layer Deep-SRSC also deliver compelling empirical performance for data clustering.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gregory S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Gordon Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay
Vasudevan, Fernanda B. Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR,
abs/1603.04467, 2016.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAMJ. Img. Sci,2(1):183-202, MarCh 2009. ISSN 1936-4954. doi: 10.1137/080716542. URL http:
//dx.doi.org/10.1137/080716542.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.
Neural Computation, 15(6):1373-1396, 2003.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for
learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399-2434, 2006.
Jerome Bolte, Shoham Sabach, and Marc Teboulle. Proximal alternating linearized minimization for nonconvex
and nonsmooth problems. Math. Program., 146(1-2):459-494, August 2014. ISSN 0025-5610. doi:
10.1007/s10107-013-0701-9.
Joseph K. Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin. Parallel coordinate descent for l1-
regularized loss minimization. In Proceedings of the 28th International Conference on Machine Learning,
ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 321-328, 2011.
Emmanuel J. Cands. The restricted isometry property and its implications for compressed sensing. Comptes
Rendus Mathematique, 346(910):589 - 592, 2008. ISSN 1631-073X.
K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into
convolutional nets. In British Machine Vision Conference, 2014.
Hong Cheng, Zicheng Liu, Lu Yang, and Xuewen Chen. Sparse representation and learning in visual recognition:
Theory and applications. Signal Process., 93(6):1408-1425, June 2013. ISSN 0165-1684. doi: 10.1016/j.
sigpro.2012.09.011.
F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems
with a sparsity constraint. Comm. Pure Appl. Math., 57(11):1413-1457, 2004. ISSN 1097-0312. doi:
10.1002/cpa.20042. URL http://dx.doi.org/10.1002/cpa.20042.
Ehsan Elhamifar and Rene Vidal. Sparse manifold clustering and embedding. In NIPS, pp. 55-63, 2011.
Shenghua Gao, Ivor Wai-Hung Tsang, and Liang-Tien Chia. Laplacian sparse coding, hypergraph laplacian
sparse coding, and applications. IEEE Trans. Pattern Anal. Mach. Intell., 35(1):92-104, 2013.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th
International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 399-406,
2010.
13
Published as a conference paper at ICLR 2017
Xiaofei He, Deng Cai, Yuanlong Shao, Hujun Bao, and Jiawei Han. Laplacian regularized gaussian mixture
model for data clustering. Knowledge and Data Engineering, IEEE Transactions on, 23(9):1406-1418, Sept
2011. ISSN 1041-4347.
Go Irie, Zhenguo Li, Xiao-Ming Wu, and Shih-Fu Chang. Locally linear hashing for extracting non-linear
manifolds. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus,
OH, USA, June 23-28, 2014, pp. 2123-2130, 2014. doi: 10.1109/CVPR.2014.272.
Masayuki Karasuyama and Hiroshi Mamitsuka. Manifold-based similarity adaptation for label propaga-
tion. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neu-
ral Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake
Tahoe, Nevada, United States., pp. 1547-1555, 2013. URL http://papers.nips.cc/paper/
5001-manifold-based-similarity-adaptation-for-label-propagation.
Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. Efficient sparse coding algorithms. In NIPS, pp.
801-808, 2006.
Ding Liu, Zhaowen Wang, Bihan Wen, Jianchao Yang, Wei Han, and Thomas S. Huang. Robust single image
super-resolution via deep networks with sparse prior. IEEE Trans. Image Processing, 25(7):3194-3207, 2016.
doi: 10.1109/TIP.2016.2564643. URL http://dx.doi.org/10.1109/TIP.2016.2564643.
Jialu Liu, Deng Cai, and Xiaofei He. Gaussian mixture model with local consistency. In AAAI, 2010.
Julien Mairal, Francis R. Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding.
In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal,
Quebec, Canada, June 14-18, 2009, pp. 689-696, 2009. doi: 10.1145/1553374.1553463.
Peter RiChtarik and Martin TakaC. Parallel coordinate descent methods for big data optimization. Mathematical
Programming, 156(1):433-484, 2016. ISSN 1436-4646. doi: 10.1007/s10107-015-0901-6. URL http:
//dx.doi.org/10.1007/s10107-015-0901-6.
Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding.
SCIENCE, 290:2323-2326, 2000.
Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, and Thomas S. Huang. Learning A deep L∞ en-
coder for hashing. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,
IJCAI 2016, New York, NY, USA, 9-15 July 2016, pp. 2174-2180, 2016.
Zilei Wang, Jiashi Feng, and Shuicheng Yan. Collaborative linear coding for robust image classification. Int. J.
Comput. Vis., 114:322333, 2015.
Jianchao Yang, Kai Yu, Yihong Gong, and Thomas S. Huang. Linear spatial pyramid matching using sparse
coding for image classification. In CVPR, pp. 1794-1801, 2009.
Tianzhu Zhang, Bernard Ghanem, Si Liu, Changsheng Xu, and Narendra Ahuja. Low-rank sparse coding for
image classification. In IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia,
December 1-8, 2013, pp. 281-288, 2013. doi: 10.1109/ICCV.2013.42.
Miao Zheng, Jiajun Bu, Chun Chen, Can Wang, Lijun Zhang, Guang Qiu, and Deng Cai. Graph regularized
sparse coding for image representation. IEEE Transactions on Image Processing, 20(5):1327-1336, 2011.
Xin Zheng, Deng Cai, Xiaofei He, Wei-Ying Ma, and Xueyin Lin. Locality preserving clustering for image
database. In Proceedings of the 12th Annual ACM International Conference on Multimedia, MULTIMEDIA
’04, pp. 885-891, New York, NY, USA, 2004. ACM.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian fields and
harmonic functions. In Machine Learning, Proceedings of the Twentieth International Conference (ICML
2003), August 21-24, 2003, Washington, DC, USA, pp. 912-919, 2003.
Appendix
Proofs
Proof of Proposition 1. Note that U is the optimal solution to the lasso problem argminy∈Rp τ2s ∣∣v — Z kt) ∣∣2 +
λ∣∣vkι. Define Tk(V) = Ts(V — Zkt))2 + λ∣v∣ for V ∈ IR, then Uk = argminv∈RTk(v). Since the
14
Published as a conference paper at ICLR 2017
two functions Hk (v) and Tk (v) only differ at v = 0, arg minv∈{u ,0} Hk(v) is the optimal solution to
minv∈R Hk (v) when uk 6= 0 or uk = 0 and GkAi ≥ 0.
When Uk = 0 and GAi < 0, when ε → 0 and ε = 0, Hk(V) → GAi and infv∈r Hk(V) = τs(Zkt))2 + YGAi.
Note that the infimum can never be achieved. Since inf v∈IR Hk (v) < Hk (Z(kti-1)), we can always find ε 6= 0
such that Hk (ε) ≤ Hk (Z(kti-1)).
Define H(V) = τ2s∣∣v — Zii(t ∣∣2 + λ∣∣vkι + YRA(v). Based on the above argument, H(Zi(t)) ≤
H (Zi (t-1)) which indicates that
τs||Zi(t) — Zi(t-1)k2 + hZi(t) — Zi(t-1), VQ(Zi(t-1))i	(25)
+ λ∣Zi(t) ∣1 + YRA(Zi(t)) ≤ λ∣Zi(t-1)∣1+YRA(Zi(t-1))	(26)
Also, since S is the LiPsChitz constant for the gradient of function Q(∙), we have
Q(Zi(t)) ≤ Q(Zi(t-1)) +〈Zi(t) — Zi(t-1), VQ(Zi(t-1))i + S∣∣Zi(t) — Zi(t-1)k2	(27)
Combining (25) and (27),
F(Zi(t)) ≤ F(Zi(t-1)) — (T - I)S ∣Zi(t) — Zi(t-1) ∣2
□
Proof of Lemma 1. We first Prove that the sequences {Zi(t)}t is bounded for any 1 ≤ i ≤ n. By ProPosition 1,
i(t)
the sequence {F (Zi )}t decreases, so we have
F (Zi(t)) = 2 ∣Xi — DZi(t)k2 + λ∣Zi(t)kι + YRA(Zi(t))
≤ ∣∣Xi — DZi(O)∣2 + λ∣Zi(0)ko ≤ 1 + RA(Zi(O))
for t ≥ 1. Therefore,
∣∣Zi(t)kι ≤ 1 ∣∣Xi — DZi(O)k2 + λ∣Zi(0)∣0 ≤ 1 + RA(Zi(O))
λ
It follows that ∣Zi(t) ∣1 is bounded, and ∣Zi(t) ∣2 is also bounded. Since GkAi ≥ 0 for k = 1, . . . , p and
the indicator function 1I∙=o is semi-algebraic function, RA(∙) is also a semi-algebraic function and lower
semicontinuous. Therefore, according to Theorem 1 by Bolte et al. (2014), {Zi(t) }t converges to a critical Point
of F(Zi), denoted by Zi.
_	_ ^Γ ,一 ^ .	、	」,」.一、 一 一	-	-	一 Λ	_	^ . .	. .	_	_ _ ，_八^	_
Let v = D>(DZi — Xi) + P(Zi; b). For k such that G卷=0, since ZiiS a critical point of F(Zi), Vk = 0.
Now we consider the case that GkAi 6= 0.
For for k ∈ Si, since Zi is a critical point of F(Zi) = 2∣∣Xi — DZi∣2 + λ∣Zi∣ι + YRA(Zi). then
d(Q+∂ZZikI) ∣zi-5^i = 0 because dRAiZi) ∣zi_^i = 0 . Note that mink∈^. |Zk| >b,so ∂Z-∣zi= 0, and
∂Zk	Z	=Z	∂Zk	Z	=Z	∈ i	∂Zk	Z	=Z
it follows that Vk = 0.
For k ∈ Si, since 舞(Zk+; b)=萃 + λ and 舞(Zk —; b) = — γ≠ — λ,萃 + λ ≥ | 盍|不=分 |, We
kk	k
can choose the k-th element of P(Zi; b) such that Vk = 0. Therefore, ∣v∣∣2 = 0, and Zi is a local solution to
the problem (21).
Now we prove that Zi* is also a local solution to (21). Let v* = D>(DZi* — Xi) + P(Zi*; b), and Q is
defined as before. For k such that G卷=0, since Zi* is the globally optimal solution of F(Zi), v* = 0.
Again we consider the case that GkAi 6= 0.
For k ∈ Si* , since Zi * is the globally optimal solution to problem (8), we also have
If it is not the case and "Q+ZF" %=Zi*
k
∂(Q+λkZikι) |	,
% IZi=Zi
0.
6= 0, then we can change Zik by a small amount in the direction
of the gradient d(Q+λkiZikI)
∂Zk
objective F(Zi).
at the point Zi = Zi * while Zik is still nonzero, leading to a smaller value of the
15
Published as a conference paper at ICLR 2017
Note that mi∏k∈s* |Z『| > b, so -Zr k=^ = 0, and it follows that Vk = 0.
k
For k ∈ S*, since 7Gfei + λ ≥ maxfc∈^. |-∂QrIZi=Za* |, We can choose the k-th element of 户(Zi*; b) such
k
that Vk = 0. It follows that ∣∣v* 如=0, and Zi* is also a local solution to the problem (21).
□
ProofofTheorem 1. According to Lemma 1, both Zi and Zik are local solutions to problem (21). In the
following text, let βι indicates a vector whose elements are those of β with indices in I. Let ∆ = Zi* — Zi,
；*、
∆ = P(Zi ) — P(Zi). ByLemma1,wehave
∣∣DτD∆ + ∆ ∣∣2 =0
It follows that
∆τDτD∆ + ∆τ∆ ≤ ∣∣∆∣∣2∣∣DτD∆ + ∆∣∣2 =0
Also, by the proof of Lemma 1, for k ∈ Si ∩ S*, since (DTD∆)k = 2入彩工*z<<0 +。亚Zi*Z>>0 We have
∆k = —(DτD∆)k. We now present another property on any nonconvex function P using the degree of
nonconvexity in Definition 3: θ(t,κ) := sups{-sg∏(s — t)(P (s; b) — P (t; b)) — κ∖s — t|} on the regularizer
P. For any s,t ∈ IR, we have
—sg∏(s — t)(P(s; b) — P(t; b)) — κ∖s — t| ≤ θ(t, K)
by the definition of θ. It follows that
θ(t, k)|s — t| ≥ —(s — t) (P(s; b) — P(t; b)) — K(S — t)2
—(s — t) (P(s; b) — P(t; b)) ≤ θ(t, k)|s — t| + K(S — t)2	(28)
Applying (28) with P = Pk for k = 1,..., p, we have
∆τDτD∆ ≤ —∆τ∆ = —∆T ∆L ∆Tns:∆^g
≤ |ZSi* — Z SJθ(Z Si ,κ) + κ∣ZS「一 Z Si∣2 + ∣∆^ans* M ∣∆ ^i∩s* ∣2
≤ ∣θ(Z晟,κ)∣2IlZSi — ZSi∣2 + κ∣ZS * —Z自∣2 + k∆∣∣2∣∣∆^ins*M
i	i	i	i i
≤ ∣θ(Z ∣i ,κ)∣2 ∣∆∣2 +κ∣∆∣2 + ∣∆∣2 ∣∆ ^i∩s* ∣2	(29)
i	i i
On the other hand, ∆τDτD∆ ≥ κ2∣∆∣2. Itfollowsfrom(29)that
κ2∣∆∣2 ≤ ∣θ(ZSi,κ)∣21∆∣2 + κ∣∆∣2 + 忸但也§力电M
When ∣∣∆∣2 = 0, we have
K2k∆∣∣2 ≤∣θ(Zsi,κ)∣2 +κ∣∆∣2 + ∣∆^i∩s: ∣2
⇒ ∣∆∣2 ≤
...,^ - ... .. ' 一
∣θ(Zsi,κ)∣2 + ∣∆^i∩s* ∣2
K0 — K
(30)
According to the definition of θ, it can be verified that θ(t, κ) = max{0, 7Gki — K|t — b|} for |t| > b, and
θ(0+, κ) = max{0, 7Gki — κb}. Therefore,
∣θ(Z Si ,κ)∣2 = ( X (max{0,呼—k|Z ki —b|})2 +
. - "
k∈Si∩Si
X (max{0,苧— Kb})2)1
k∈Si∖Si
(31)
Therefore,
∣∆∣2 ≤
((X (max{0,γGi
' k∈si∩Si
—k|Z ki — b|})2+
X (max{0,? — Kb})2) 2 + ∣∆^i∩s: ∣2
k∈Si∖Si
(32)
1	τ	∕T-'1 T 1-'1 A∖	C∖rT	CTr	A--.C* EI ∙	，1	、，「，，♦
where ∆k	= —(D 1 D∆)k	= -2入彩5	*zi <0	— 01IZi	*Zi >0 for	k ∈ Si ∩ Si. This	proves the	result of this
Zk Zk<	zk zk >
theorem.	□
16
Published as a conference paper at ICLR 2017
Test Error versus Epoch Number for Deep-SRSC with p=100
Test Error versus Epoch Number for Deep-SRSC with p=300
Test Error versus Epoch Number for Deep-SRSC with p=500
Figure 5: Test error of Deep-SRSC with dictionary size p
100,p= 300,andp= 500
Table 5: Clustering Results on COIL-100 data with different dictionary size p
Dictionary Size		Measure	KM	SC	Sparse Coding	'2-RSC	SRSC
P	100	AC	0.5221	0.2372	07010	0.7010	0.7344
		-NMI-	0.7633	0.5410	08834	0.8834	0.8950
P	300	AC	-	-	06979	0.6979	0.7267
		-NMI-	-	-	08837	0.8837	0.8876
P =	500	AC	-	-	06979	0.6979	0.7117
		NMI	-	-	0.8839	0.8839	0.8856
Table 6: Parameter sensitivity with respect to γ and K on USPS data set
Varying Y with default K = 3	Measure	KM	SC	Sparse Coding	'2-RSC	SRSC
Y = 0.1	AC	0.6878	0.4041	0.8178	0.8287	0.8229
	-NMI-	0.6312	0.4765	0.8321	0.8398	0.8370
γ = 0.2	AC	-	-	0.8178	0.8287	0.8261
	-NMI-	-	-	0.8321	0.8398	0.8439
γ = 0.3	AC	-	-	0.8178	0.8287	0.8251
	-NMI-	-	-	0.8321	0.8398	0.8441
γ = 0.4	AC	-	-	0.8178	0.8287	0.8258
	-NMI-	-	-	0.8321	0.8398	0.8455
γ = 0.5	AC	-	-	0.8178	0.8287	0.8293
	-NMI-	-	-	0.8321	0.8398	0.8471
γ = 0.6	AC	-	-	0.8178	0.8287	0.8273
	-NMI-	-	-	0.8321	0.8398	0.8481
γ = 0.7	AC	-	-	0.8178	0.8287	0.8279
	-NMI-	-	-	0.8321	0.8398	0.8489
γ = 0.8	AC	-	-	0.8178	0.8287	0.8282
	NMI	-	-	0.8321 —	0.8398	0.8479
Varying K with default Y = 0.5	Measure	KM	SC	Sparse Coding	'2-RSC	SRSC
K = 3	AC	-	-	0.8178	0.8287	0.8293
	-NMI-	-	-	0.8321	0.8398	0.8471
K = 4	AC	-	-	0.8178	0.8287	0.8216
	-NMI-	-	-	0.8321	0.8398	0.8487
K = 5	AC	-	-	0.8178	0.8287	0.8243
	-NMI-	-	-	0.8321	0.8398	0.8535
K = 6	AC	-	-	0.8178	0.8287	0.8462
	NMI	-	-	0.8321	0.8398	0.7995
Table 7: Clustering results on the test set of MNIST Data
Measure	KM	SC	Sparse Coding	'2-RSC	SRSC	6-Layer Deep-SRSC
AC	0.5606	0.3452	0.6039	0.6312	0.7513	07621
NMI	0.5382	0.4129	0.6210	0.6749	0.7378	0.7537
Table 8: Clustering results on the test set of CIFAR-10 Data
Measure	KM	SC	Sparse Coding	'2-RSC	SRSC	6-Layer Deep-SRSC
AC	0.4548	0.4220	0.4113	0.4539	0.4625	0.4574
NMI	0.3655	0.3133	0.3335	0.3670	0.3869	0.3516
More Experimental Results
The test error of Deep-SRSC with different dictionary size, corresponding to Figure 4 showing the training error,
is illustrated in Figure 5. We vary the dictionary size and show the clustering results on COIL-100 data set in
Table 5, and we can see that SRSC always achieves highest accuracy and NMI with different dictionary size.
17
Published as a conference paper at ICLR 2017
In addition, we investigate the parameter sensitivity of SRSC, and show in Table 6 the performance change
while varying γ, the weight for the support regularization term, and K, the number of nearest neighbors when
building the KNN graph for the support regularization term, on the USPS data set. It can be observed that the
performance of SRSC is stable over a relatively large range of λ and K. SRSC often has the highest NMI while
maintaining a very competitive accuracy.
Deep-SRSC with the Second Test Setting (Referring to the Training Data)
We demonstrate the performance of SRSC and Deep-SRSC with the second test setting (referring to the training
data) on clustering and semi-supervised learning. The ground truth code of the each test data point is computed
by performing the PGD-style iterative method to solve the problem (8) where xi is the test point, D is Dsr
obtained from the training data as in Section 5.2, A is the adjacency matrix of the KNN graph over the test
point and the training data. Table 9 shows the prediction error of Deep-SRSC for different dictionary size p and
different number of layers on the USPS data, which is comparable to the case of the first test setting in Table 3.
Two more data sets are used in this subsection, i.e. MNIST for hand-written digit recognition and CIFAR-10 for
image recognition. MNIST is comprised of 60000 training images and 10000 test images of ten digits from
0 to 9, and each image is of size 28 × 28 and represented as a 784-dimensional vector. CIFAR-10 consists of
50000 training images and 10000 testing images in 10 classes, and each image is a color one of size 32 × 32.
Using the second test setting, Deep-SRSC is trained on the training set, and the codes of the test set predicted
by 6-layer Deep-SRSC are used to perform clustering on the test set for MNIST and CIFAR-10 data, with
comparison to other sparse coding based methods. The clustering results are shown in Table 7 and 8 respectively
with dictionary size p = 300. We observe that SRSC and Deep-SRSC always achieve the best performance
compared to other competing methods. We employ the fast deep neural network named CNN-F (Chatfield
et al., 2014) trained on the ILSVRC 2012 data to extract the 4096-dimensional feature vector for each image in
the CIFAR-10 data, and all the clustering methods are performed on the extracted features. In addition to the
coordinate descent method employed in Section 2.1.2 and Algorithm 1 for the optimization of the sparse codes
in SRSC, we further conduct the empirical study showing that the parallel coordinate descent method, which
updates the coordinates in parallel for improved efficiency and fits the needs of large-scale data optimization,
leads to almost the same results as the coordinate descent method on the CIFAR-10 data. Instead of optimization
with respect to the sparse code of a single data point in the coordinate descent method, the parallel coordinate
descent method updates the sparse codes of P data points in parallel using the same rule as that in the coordinate
descent method in Section 2.1.2 and Algorithm 1. While the parallel coordinate descent method is originally
designed for convex problems (Bradley et al., 2011; RiChtarik & TakaC, 2016), it demonstrates almost the same
empirical performance as the coordinate descent method for the clustering task on the test set of the CIFAR-10
data, with the aCCuraCy of 0.4622 and NMI of 0.3864. P -parallel Coordinate desCent leads to P times speedup
Compared to the Coordinate desCent method. We Choose P = 10 and the Codes of the training data of CIFAR-10
are learned by the parallel Coordinate desCent method, and note that the optimization of the Codes of the test data
are inherently parallelizable due to the nature of the seCond test setting studied in this subseCtion.
Moreover, Table 10 shows the prediCtion error of Deep-SRSC on the MNIST data and the CIFAR-10 data. It Can
be observed again that deeper Deep-SRSC network leads to smaller prediCtion error.
We also show the appliCation to semi-supervised learning via label propagation (Zhu et al., 2003), a widely
used semi-supervised learning method. Given the data {x1, x2, . . . , xl, xl+1, . . . , xn} ⊂ IRd, the first l points
{x1, x2, . . . , xl} are labeled and named the training data, and the remaining n - l points form the test data
for semi-supervised learning. Semi-supervised learning by label propagation aims to prediCt the labels of the
test data by enCouraging loCal smoothness of the labels in aCCordanCe with the similarity matrix over the entire
data. The performanCe of label propagation depends on the similarity matrix. For eaCh sparse Coding based
method, the similarity matrix Y over the entire data is built by the support similarity introduCed in SeCtion 5.1:
Yij = Aij q>i q^j, and Zi is the code of data point Xi for different sparse coding methods including the
6-layer Deep-SRSC with the seCond test setting. Label propagation is performed on the similarity matrix Y to
obtain the labels of the test data, and the error rate is reported. Note that in the experiment of semi-supervised
learning by label propagation, the codes of the test data of each data set are obtained first (e.g. the 10000 test
images in the MNIST data). If xi belongs to the test data of a data set, its code is obtained by performing the
the corresponding sparse coding optimization with the dictionary learned on the training data of that data set;
for SRSC and Deep-SRSC, such optimization also has the KNN graph over the test point xi and the training
data as input. With the codes of all the data, the similarity matrix Y over the entire data is constructed. Then, a
randomly sampled subset of each class is labeled as the training data, with the other data serving as the test data
for semi-supervised learning.
The semi-supervised learning results of our methods are compared to that of the Gaussian kernel graph (Gaussian),
i.e. the KNN graph with the edge weight set by the Gaussian kernel; Sparse Coding (SC) and '2-RSC; and
manifold based similarity adaptation (MBS) by Karasuyama & Mamitsuka (2013), one of the state-of-the-art
semi-supervised learning methods based on label propagation. MBS learns the manifold aligned edge similarity
by local reconstruction for label propagation.
18
Published as a conference paper at ICLR 2017
The comparison results of semi-supervised learning by label propagation on the USPS data and the MNIST
data are shown in Figure 6 and 7, which illustrate the error rate of label propagation with respect to different
number of labeled data points in each class. We can observe from Figure 6 that SRSC and Deep-SRSC with the
second test setting lead to superior results on the application to semi-supervised learning, and the performance of
SRSC and Deep-SRSC is always the best with respect to different dictionary size. It can also be observed from
Figure 6 and 7 that SRSC and Deep-SRSC have very similar performance, revealing the good quality of the
fast approximation by Deep-SRSC on the semi-supervised learning task. Furthermore, SRSC and Deep-SRSC
significantly outperform other baseline methods with the small number of labeled data points in each class, due
to the captured locally linear manifold structure.
Table 9: Prediction error (average squared error between the predicted codes and the ground truth codes) of
Deep-SRSC with different depth and different dictionary size on the test set of the USPS data, using the second
test setting
Dictionary Size	1-layer	2-layer	6-layer
p = 100	-005-	-003-	-003-
P = 300	-012-	-008-	-006-
P = 500	0.27	0.11	0.09
Table 10: Prediction error (average squared error between the predicted codes and the ground truth codes) of
Deep-SRSC with different depth on the test set of the MNIST data and CIFAR-10 data, using the second test
setting
Data Set	1-layer	2-layer	6-layer
-MnIst-	-0:15-	-012-	-010-
CIFAR-10 ∙	0.16	0.13	0.13
Error w.r.t Number of Labeled Samples by Label Propgation on USPS Data
Error w.r.t. Number of Labeled Samples by Label Propgation on USPS Data
Error w.r.t. Number of Labeled Samples by Label Propgation on USPS Data
Figure 6: Error rate of semi-supervised learning by label propagation on the USPS data, with
dictionary size p = 100, p = 300, andp = 500
Error w.r.t. Number of Labeled Samples by Label Propgation on MNIST Data
Number of Labeled Samples for Each Class
Figure 7: Error rate of semi-supervised learning by label propagation on the MNIST data with
dictionary size p = 300
19