Published as a conference paper at ICLR 2017
Online Bayesian Transfer Learning for
Sequential Data Modeling
Priyank Jaini1, Zhitang Chen4, Pablo Carbajal1, Edith Law1, Laura Middleton2,
Kayla Regan2, Mike Schaekermann1, George Trimponias4, James Tung3, Pascal Poupart1
pjaini@uwaterloo.ca, chenzhitang2@huawei.com, pablo@veedata.io,
{edith.law,lmiddlet,kregan}@uwaterloo.ca,
g.trimponias@huawei.com,
{mschaekermann,james.tung,ppoupart}@uwaterloo.ca
1	David R. Cheriton School of Computer Science, University of Waterloo, Ontario, Canada
2	Department of Kinesiology, University of Waterloo, Ontario, Canada
3	Dept. of Mechanical and Mechatronics Engineering, University of Waterloo, Ontario, Canada
4 Noah’s Ark Laboratory, Huawei Technologies, Hong Kong, China
Ab stract
We consider the problem of inferring a sequence of hidden states associated with
a sequence of observations produced by an individual within a population. Instead
of learning a single sequence model for the population (which does not account for
variations within the population), we learn a set of basis sequence models based
on different individuals. The sequence of hidden states for a new individual is in-
ferred in an online fashion by estimating a distribution over the basis models that
best explain the sequence of observations of this new individual. We explain how
to do this in the context of hidden Markov models with Gaussian mixture models
that are learned based on streaming data by online Bayesian moment matching.
The resulting transfer learning technique is demonstrated with three real-word ap-
plications: activity recognition based on smartphone sensors, sleep classification
based on electroencephalography data and the prediction of the direction of future
packet flows between a pair of servers in telecommunication networks.
1	Introduction
In several application domains, data instances are produced by a population of individuals that ex-
hibit a variety of different characteristics. For instance, in activity recognition, different individuals
might walk or run with different gait patterns. Similarly, in sleep studies, different individuals might
exhibit different patterns for the same sleep stages. In telecommunication networks, software ap-
plications might generate packet flows between two servers according to different patterns. In such
scenarios, it is tempting to treat the population as a homogeneous source of data and to learn a single
average model for the population. However, this average model will perform poorly in recognition
tasks for individuals that differ significantly from the average. Hence, there is a need for transfer
learning techniques that take into account the variations between individuals within a population.
We consider the problem of inferring a sequence of hidden states based on a sequence of observa-
tions produced by an individual within a population. Our first contribution is an online Bayesian
moment matching technique to estimate the parameters of a hidden Markov model (HMM) with
observation distributions represented by Gaussian mixture models (GMMs). This approach allows
us to learn separate basis models for different individuals based on streaming data. The second
contribution is an unsupervised online technique that infers a probability distribution over the basis
models that best explain the sequence of observations of a new individual. The classification of
hidden states can then be refined in an online fashion based on the individuals that most resemble
the new individual. Furthermore, since the basis models are fixed at classification time and we only
learn the weight of each model, good classification accuracy can be obtained more quickly as the
stream of observations of the new individual are processed. The third contribution of this work is
the demonstration of this approach across different real-world applications, which include activity
1
Published as a conference paper at ICLR 2017
recognition, sleep classification and the prediction of packet flow direction in telecommunication
networks.
The paper is organized as follows. Section 2 reviews some related work on transfer learning. Sec-
tion 3 provides some background regarding hidden Markov models Bayesian Moment Matching
algorithm Gaussian mixture models. Section 4 describes the proposed online transfer learning tech-
nique. Section 5 illustrates the transfer learning technique in three real-world tasks: activity recogni-
tion, sleep stage classification and flow direction prediction. Finally, Section 6 concludes the paper
and discusses directions for future work.
2	Related Work
There is a large literature on transfer learning (Pan & Yang, 2010; Taylor & Stone, 2009; Shao et al.,
2015; Cook et al., 2013). Depending on the problem, the input features, the output labels or the
distribution over the features and the labels may be different for the source and target domains. In
this work, we assume that the same input features are measured and the same output labels are in-
ferred in the source and target domains. The main problem that we consider is subject variability
within a population of individuals, which means that different individuals exhibit different distribu-
tions over the features and the labels. The problem of subject variability has been studied in several
papers. Chieu et al. (2006) describe how to augment conditional random fields with a subject hidden
variable to obtain a mixture of conditional random fields that can naturally infer a distribution over
the closest subjects in a training population when inferring the activities of a new individual based
on physiological data. Rashidi & Cook (2009) proposed a data mining technique with a similar-
ity measure to facilitate the transfer of activity recognition across different people. Chattopadhyay
et al. (2011) describe a similarity measure with an intrinsic manifold that preserve the topology of
surface electromyography (SEMG) while mitigating distributional differences among individuals.
Zhao et al. (2011) proposed a transfer learning technique that starts by training a decision tree to
recognize the activities of a user based on smartphone accelerometry. The decision tree is gradu-
ally adjusted to a new user by a clustering technique that successively re-weights the training data
based on the unlabeled data of the new individual. These approaches mitigate subject variability
by various offline transfer learning techniques. In contrast, we propose an online transfer learning
technique since the applications that we consider exhibit sequences of observations that arrive in a
streaming fashion and therefore require an online technique that can infer the hidden state of each
observation as it arrives.
In the next section, we describe an online transfer learning technique for hidden Markov models with
Gaussian mixture models. The approach learns different transition and emission models for each
individual in the training population. Those models are then treated as basis models to speed up the
online learning process for new individuals. More specifically, a weighted combination of the basis
models is learned for each new individual. This idea is related to boosting techniques for transfer
learning (Dai et al., 2007; Yao & Doretto, 2010; Al-Stouhi & Reddy, 2011) that estimate a weighted
combination of base classifiers. However, note that we focus on sequence modeling problems where
the classes of consecutive data points are correlated while transfer learning by boosting assumes that
the data points are identically and independently distributed.
3	Background
In this section, we give a brief overview of hidden Markov models (HMMs) and review the Bayesian
moment matching (BMM) algorithm in detail with an example. We will use both HMMs and BMM
subsequently in our transfer learning algorithm described in Section 4.
3.1	Hidden Markov Models
In a hidden Markov model (HMM), each observation Xt is associated with a hidden state Yt . The
Markov property states that the current state depends only on the previous state. HMMs have been
widely used in domains involving sequential data like speech recognition, activity recognition, nat-
ural language processing etc. An HMM is represented by two distributions
2
Published as a conference paper at ICLR 2017
•	Transition distribution: The transition distribution models the change in the value of the
hidden state over time. The distribution over the current state Yt given that the previous
state is Yt-1 = j is denoted by θj = Pr(Yt|Yt-1 = j) where θj = {θ1j, ..., θNj}, N is the
total number of states and θij = Pr(Yt = i|Yt-1 = j).
•	Emission distribution: The emission distribution models the effect of the hidden state on
the observation Xt at any given time t and is given by Pr(Xt|Yt). In this work, we model
the emission distribution as a mixture of Gaussians with M components, i.e., Pr(Xt|Yt =
j) = PM=1 WiN(Xt; μj, Σj)
In this paper, we will first estimate the parameters of the transition and emission distributions by
Bayesian learning from a set of source domains (individuals). Subsequently, we will use these
distributions as basis functions when estimating the transition and emission distributions of a target
domain in which we wish to predict the hidden state for each observation. Parameter learning of an
HMM using Bayesian learning is done by calculating the posterior over the parameters given a prior
distribution.
Pr Θ,Φ,Yt =j|Xt,Yt-1 =i
H
Emission distribution Transition Probability	Prior for t - 1
z-------A----------{z-----------A-------------{z----------------A----------------{
Pr(Xt|Yt =j)Pr(Yt =j|Yt-1 = i) Pr(Θ, Φ, Yt-1 = i|X1:t-1)
∀j ∈ {1, 2, ..., N} where Θ and Φ parametrize the transition and emission distributions respectively.
3.2	Bayesian Moment Matching Algorithm
The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed
by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based
on streaming data is intractable because the number of terms in the posterior after observing each
observation increases exponentially. BMM circumvents this issue by projecting the distribution of
the exact posterior P on a tractable family of distributions P by matching a set of sufficient moments.
In this section, we give a brief overview of the BMM algorithm with an example.
Note that Variational Bayes (VB) and Markov Chain Monte Carlo (MCMC) techniques can also be
used for approximate Bayesian learning as an alternative to BMM. However, MCMC is difficult to
run in an online fashion. A recent comparison by Omar Omar (2016) showed that BMM achieves
better results than online Vational Bayes (oVB) Sato (2001) and Stochastic Variational Inference
(SVI) Wang et al. (2011) in the context of topic modeling. BMM was also shown to work better
than other online techniques in several papers Rashwan et al. (2016); Hsu & Poupart (2016); Jaini
et al. (2016). This is due to the fact that BMM is naturally online and therefore does not require mini-
batches. In contrast, in order to run in an online fashion Variational Bayes requires mini-batches and
a decreasing learning rate, however the size of the mini-batches and the decay procedure for the
learning rate require some fine tuning. In general, the use of mini-batches always leads to some
information loss since data in previous mini-batches is not accessible. BMM does not suffer from
this type of information loss and there is no batch size nor learning rate to fine tune. Hence, we will
adapt BMM to transfer learning in this work.
Let X1:n be a set of d-dimensional i.i.d observations following Pr(X∣Θ) = PM=I WiN(x; μ%, Λ-1)
where Θ = {(wι, μι, Λ-1), (w2, μ?, Λ-1),...(wM,μM, ΛM1)} andM is known.
We choose the prior as a product of a Dirichlet distribution over the weights w and M Normal-
WiShart distributions corresponding to the parameters (μ, Λ-1) of each Gaussian component.
Such a prior forms a conjugate probability pair of the likelihood and is hence desirable. Con-
cretely, P o(⑼=Dir(WIa) QM1NW (〃，，Λi 匹出，Wi,%) where W = (wι,w?,…,wm ), α =
(α1, α2, ..., αM ), W is a symmetric positive definite matrix, κ > 0 is real, δ ∈ Rd and ν > d - 1 is
real. The posterior P 1(Θ∣X1) after observing the first data point Xi is given by
P 1(Θ∣X1) H P0(Θ)Pr(X1∣Θ)
MM
H Dir(w∣a) YNW(μi, Λi∣δi, Ki, Wi, Vi) X WjN(Xi； μj, Λ-i)
i=1	j=1
Since a Normal-Wishart distribution is a conjugate prior for a Normal distribution
with unknown mean and precision matrix, NW仙,Λ∕δi,Ki, Wi,%)N(Xi； μ^ Λ-i)	=
3
Published as a conference paper at ICLR 2017
CNW(μ%, Λi∣δi,Ki, Wi, Vi) where Cis some constant. Similarly, WjDir(W∣ɑι, α2,.., αj,.., aM)
kDir(w1,w2,..., WM∣α1,α2, ..αj.., ɑM) where k is some constant. Therefore, P 1(Θ∣X1) is
1M	M
P 1(Θ∣X1) = Z X ( CjDir(w∣<aj)NW(μj, Λj ∣δj,^j, Wj,Vj) YNW〃, Λ∕δi,Ki, Wi,%)
j=1	i6=j
where &j- = (α1,α2,.., αj-,.., ɑM) and Z is the normalization constant. The equation above SUg-
gests that the posterior is a mixture of product of distributions where each product component in
the summation has the same form as that of the family of distributions of the prior P0(Θ). It is
evident that the terms in the posterior grow by a factor ofM for each iteration, which is problematic.
The Bayesian moment matching algorithm approximates this mixture P1(Θ) with a single product
of Dirichlet and Normal-Wishart distributions P1(Θ) by matching all the sufficient moments of P1
with PI which belongs to the same family of distributions as the prior:
M
Pι(θ) = Dir(WIaI) YNW(〃i，Ai|61 ,κ1, w1,νi1)
i=1
We evaluate the parameters α1,	δi1, κi1, Wi1, νi1 ∀i ∈	{1, 2, ..,	M} by matching a set of suf-
ficient moments of P1(Θ) with	P1(Θ). The	set	of	sufficient	moments in this case is S =
{μj∙, μj∙μT, Λj∙, Λjkm,Wj, wj} ∀j ∈ 1,2,..., M where Λjkm is the (k, m)th element of the matrix
Λj. The expressions for sufficient moments are given by E[g] = Θ gP1(Θ)d(Θ). The parameters
of P1 can be computed from the following set of equations
E[wi] = P^— ;
jαj
E[Λ] = νW;
E[μ] = δ;
πτr 21	(αi)(αi + 1)
E[wi] = τ------VT---------V
Pjαj	1+Pjαj
V ar(Λij) = ν(Wi2j + WiiWjj)
E[(μ — δ)(μ - δ)T] = K +1	WT
κ(ν - d - 1)
Using this set of equations, the exact posterior P1 (Θ) can be approximated with P1(Θ). This pos-
terior will then be the prior for the next iteration and we keep following the steps above itera-
tively to finally have a distribution Pzn(Θ) after observing a stream of data X1:n. The estimate is
Θ = E[Pn(Θ)]. The exact calculations for the Bayesian Moment Matching algorithm are given in
appendix A.
4	Transfer Learning using BMM
In this section, we first motivate the need for an online transfer learning algorithm for sequential data
modeling and then explain in detail the different steps of the algorithm. The complete algorithm is
given in Alg. (1).
4.1	Motivation
Several applications produce data instances from a population of individuals that exhibit a variety of
different traits. For example, for the task of activity recognition, different individuals will have dif-
ferent gait patterns despite the fact that they are performing the same activity (e.g., walking, running,
standing, etc.). Therefore, it is problematic to make predictions in such domains by considering the
population to be homogeneous; however, every population will have individuals resembling each
other in some characteristics. This suggests that we can use individuals in a population to make pre-
dictions about similar individuals by identifying those individuals who closely resemble each other.
However, identifying individuals with similar traits is not straightforward. Alternatively, weights
can be assigned to each individual in a population based on a target individual (individual on whom
predictions are to be made). All those individuals who resemble closely the target individual will re-
ceive higher weights than those with dissimilar traits. Subsequently, predictions about the behavior
of the target individual will be based mostly on the observed behavior of the similar individuals.
4
Published as a conference paper at ICLR 2017
Our transfer learning algorithm addresses precisely these issues. It has three main steps - first, it
learns a model (transition and emission distributions) for each source domain (or individual in a
population) that best explains the observations of that source domain. Next, given a target domain
(or target individual), it identifies those individuals that closely resemble the target individual by
estimating a basis weight associated to each source domain. A higher weight for a source domain
implies that the corresponding individual resembles more closely the target individual. Finally, it
predicts the hidden states for each observation in the target domain by using the models learned in
the source domain and the basis weights that are given to each transition and emission distribution
of the source domains. We now explain each step of the algorithm in detail below.
4.2	Source Domain - Training
The first step is to learn a model for each source domain in the training data. Suppose that we have
labeled sequence data for K different source domains. Let
Ytk = hidden state label at time step t for source domain k
Xtk = feature vector at time step t for source domain k
Let the sequence of observations be given by X1k:T = {X1k, X2k, ..., XTk } and the hidden states be
{Y1k, Y2k, ..., YTk} where Ytk ∈ {1, 2, .., N} ∀t. Furthermore, let us define
Θikj = Pr(Ytk = i|Ytk-1 = j) i.e. the transition probability from state i to state j
We denote the transition matrix for the kth source domain with Θk. Let the emission distribution be
modeled by a mixture of Gaussian with M components. This implies
M
Pr(Xk∣Ytk = j) = X WkmN(Xk ∣μkm, ∑km) ∀j ∈ {1, 2,.., N}
m=1
Our aim is to learn the parameters characterizing the transition and the emission distribution for each
source domain. More precisely, if
Φk = {φk,Φ2,…,ΦN} where Φk = {3,μk, ∑"∙…,(WkM，μ%, ∑%)}
then we want to learn the parameters Θk for the transition distribution and Φk for the emission
distribution for each source domain k ∈ {1, 2, ..., K}. Since, we use a hidden Markov model, the
update equation at each time step for a source domain k is
Pr (θ, Φ,Ytk = j|Xk ,Yt-ι = i)
Emission distribution Transition Probability	Prior for t - 1
z--------^--------{z-----------^----------{，----------------A----------------{
(X Pr(Xk|Ytk = j) Pr(Ytk = j∣Yt-ι = i) Pr(Θk, Φk,匕3 = iX：1)
∀j ∈ {1, 2, ...,N}	(1)
The prior over (Θk , Φk ) is given by
N	NM
Pr(θk, φ )= [Y Dir(θk ∣αk)] [Y Dir(Wk ； βj) Y NW 忌,Aku ； j ,j, Wku ,j)]
i=1	j=1	u=1
(2)
After substituting the relevant terms in Eq (1), we get
MN
Pr (θ, φ,γtk=j∣χk ,匕-1 = i) X X Wkm NIXMm, j )θki[Y Dir(θk∣αk)]
m=1	i=1
NM
[Y Dir(Wk ； βk) Y NW (μku, Aku ； δku ,κku, Wku ,j)] ∀j ∈{1, 2,...,n }⑶
j=1	u=1
Further, Ajk = (Σjk )-1. The prior in Eq (2) can be understood as having the following components
5
Published as a conference paper at ICLR 2017
•	Transition Distribution : Each column of the N × N transition matrix specifies the prob-
ability of making a transition from that column index to another state given by the row
index. We define a Dirichlet distribution as a prior over each column of the transition
matrix. Hence, QN=I Dir(θf ∣α?) is the prior over Θk.
•	Emission Distribution : Dir(Wj; βj) QM=ι NW (μju, Λku; δju, Kku, Wku, Vku) defines a
prior over a mixture of Gaussians for hidden state j with M components where the Dirichlet
distribution is the prior over the mixture weights and the Normal-Wishart distribution is the
prior over the mean and precision matrix of the mixture components. We take a product
over j to obtain a prior over all emission distributions.
The posterior distribution (Eq (3)) after each observation is a mixture of products of distributions
where each component has the same form as the prior distribution since Pr(Xtk|Ytk = j) is a
mixture of Gaussians. Therefore, the number of terms in the posterior increases exponentially if we
perform exact Bayesian learning. To circumvent this, we use BMM for Gaussian Mixture Models
as described in (Jaini et al., 2016; Jaini & Poupart, 2016)3. The complete calculations for learning
in the source domain are given in appendix B.
The main computation in the learning and updating routine is the calculation of the sufficient set
of moments using the Bayesian posterior given in Eq. (9) in appendix B. Let M be the number of
components in the mixture model for emission distributions, N the number of hidden states and d
the number of features in the data. The computational complexity for updating the parameters in
the source domain learning step for each iteration is O(M 2N 2) for each scalar parameters and is
O(M 2N 2d3) for the parameters of the distribution over the precision matrix because that involves
a matrix multiplication step.
4.3 Target Domain - Prediction
The goal is to predict the hidden states for a target individual (or domain) as we observe a sequence
of observations. In the previous step, we learned the transition and emission distributions individ-
ually for K different sources. These sources can be thought of as individuals in a population. The
transition and emission distributions learned from the individual sources form a basis for the transi-
tion and emission distributions of the target domain. Specifically, let the transition distribution for
the kth source be denoted by g(Θk) and emission distribution be denoted by f (Φjk) for a certain
hidden state j . Then, the transition and emission distributions for the target domain is a weighted
combination given by
KK
Pr(Yt =j|Yt-1 =i) = X λm Pr(Ytm = j|Ytm-1 =i) = X λmg(Θjmi)	(4)
m=1	m=1
KK
Pr(Xt国=j) = X ∏j Pr(Xk |中=j) = X ∏ f (Φj)	⑸
k=1	k=1
We first need to compute the basis weights λ = (λ1, λ2, ., λK) and π = (π1, π2, ., πK). We
estimate (λ, π) in an unsupervised manner using BMM. We define a Dirichlet prior over λ and π,
i.e. Pr(λ, π) = Dir(λ; γ)Dir(π; ν). The posterior after observing a new data point is
N
Pr (λ,π,Yt	= j|X，(X	Pr(Xt 国=j) X Pr(Yt	= j |Yt-i =	i)	Pr(λ, π,	Yt-i =	i)	(6)
i=1
K	NK
X X πkf(Φjk) X X λmg(Θjmi )Dir(λ; γ)Dir(π; ν)	(7)
k=1	i=1 m=1
KN
X XXC(i,j,k,m) Dir(π; V)Dir(λ; Y)	(8)
k,m i=1
where f (Φj)g(Θm) are known from the source domains, ∏kDir(π; V) = akDir(π; V),
λmDir(λ; Y) = bmDir(λ; Y) and C(i,j,k,m) = a%bmf (Φj)g(Θm). The exact calculations are
given in Appendix C. We approximate the posterior in Eq (8) by projecting it onto a tractable family
6
Published as a conference paper at ICLR 2017
of distributions with the same set of sufficient moments as the posterior using the Bayesian Moment
Matching approach. Finally, the estimate of (λ, π) is the expected value of the final posterior. This
completes the learning stage.
The transition and emission distributions for the target domain are the weighted combination of
transition and emission distributions learned in the source domain respectively. The advantage of
this linear combination is to account for heterogeneity in the data. The learning step in the target
domain will ensure that only those source domains that resemble closely the target domain are given
higher weights. This helps to bias the predictions according to the closest basis models when the
population is not homogeneous.
Predictions can be made in two different manners
•	Online - initialize the prior over λ and π to be uniform. As each new data point is observed
in a sequence, a prediction is made based on the mean of the current posterior over λ and
π and subsequently the posterior is updated based on Eq (8).
•	Offline - compute the posterior of λ and π based on Eq (8) by using the entire sequence of
observations of the target individual. Once, the posterior is computed, predict the hidden
states for each observation in the sequence based on the mean estimates of the posterior.
In Fig. 1, we show the schematic for the proposed online transfer learning algorithm. The figure
shows the learning phase for each source domain where the emission and transition distributions are
learned using Bayesian Moment Matching technique. After learning in the source domain, we learn
the weights of the basis models in the target domain for each new observation and make predictions
in an online manner.
Figure 1: Transfer Learning architecture
Algorithm (1) gives the complete algorithm for transfer learning by Bayesian Moment Matching.
7
Published as a conference paper at ICLR 2017
The target domain step involves two routines :
•	Update step - In this step, the hyper-parameters (γ, ν) over the weights (λ, π) are updated.
The main computation in this step is the calculation of the set of sufficient moments from
the updated Bayesian posterior given in Eq. (8). Hence, the computational complexity of
the update step in the target domain for each observation is O(K2N2) where K is the
number of source domains and N is the number of hidden states.
•	Prediction step - In the prediction step, a hidden label is assigned to the observation based
on the model obtained from the update step. The main computation is calculation of the
likelihood of each hidden state for the observation. The computational complexity of the
prediction step is hence O(MKN) where M is the number of components in the mixture
model, K is the total number of source domains and N is the number of hidden states.
Algorithm 1 Online Transfer Learning by Bayesian Moment Matching
1:	Input (Learning): labeled sequence data from multiple domains (individuals)
2:	Input (Prediction): unlabeled sequence data from individuals
3:	Output:山bels for hidden States______________________________________________________
Source Domain - learning transition and emission distribution__________________________
4:	Input: labeled sequence data from K domains
5:	specify # of hidden states : nClass
6:	specify # of components in GMM : nComponents
7:	procedure LEARNSOURCEHMM(data, nClass, nComponents)
8:	for k = 1 : K do
9:	Let f(Θ, Φ) be a family of probability distributions with parameters γ
10:	Initialize a prior P0k(Θ, Φ) from f over transition and emission parameters respectively
11:	for n = 1 : Dk do	. Dk : size of data for kth source domain
12:	Compute Pn(Θ, Φ) from Pn-1 (Θ, Φ) using Eq. 3
13:	Using BMM approximate Pn With Pn(Θ, Φ) = f(Θ, Φ∣γ)
14:	Return : Θ = Eθ[Pn(Θ, Φ)]
15:	Return : Φ = Eφ[Pn(Θ, Φ)]
16:	RetUrn : emission and transition distributions for each source_____________________
Target Domain - learning basis WeightS for each SoUrce domain & Prediction_____________
17:	Input: unlabeled sequence data
18:	procedure PREDICTTARGETDOMAIN(data, sourceDistributions)
19:	Let g(λ, π) = Dir(λ; γ)Dir(π; ν) be a family of probability distributions
20:	Initialize a prior P0(λ, π) from g With equal Weights to each source distribution
21:	for n = 1 : D do	. D : size of data for target domain
22:	Compute Pn(Θ, Φ) from Pn-1(Θ, Φ) using Eq. 8
23:	Using BMM approximate Pn With Pn(λ, π) = g(λ, π)
24:	Predict: Yn = argmaxjPr (λ, π, Yrb = j|Xn) UsingEq (8)
25:	Return : λ = Eλ[Pn(λ, ∏)]
26:	Return : π = En [Pn (λ, π)]
27:	Return : prediction Yn
5 Experiments and Results
This section describes experiments on three tasks from different domains - activity recognition,
sleep cycle prediction among healthy individuals and patients suffering from Parkinson’s disease
and packet floW prediction in telecommunication netWorks.
8
Published as a conference paper at ICLR 2017
Experimental Setup
For each task, we compare our online transfer learning algorithm to EM (trained by maximum likeli-
hood) and a baseline algorithm (that uses Bayesian moment matching) that both learn a single HMM
with mixtures of Gaussians as emissions by treating the population as homogeneous. Furthermore,
we conduct experiments using recurrent neural networks (RNNs) due to their popularity in sequence
learning.
The baseline algorithm uses Bayesian Moment Matching to learn the parameters of the HMM. Con-
cretely, we have data collected from several individuals (or sources) in a population for each task.
For transfer learning, we train an HMM with mixture of Gaussian emission distributions for each
source (or individual) except the target individual. For the target individual, we estimate a posterior
over the basis weights in an online and unsupervised fashion and make online predictions about the
hidden states. We compare the performance of our transfer learning algorithm against the EM and
baseline algorithms that treat the population as homogeneous, i.e., we train an HMM by combining
the data from all the sources except the target individual. Then, using this model, we make online
predictions about the hidden states of the target individual.
We report the results based on leave-one-out cross validation where the data of a different individual
is left out in each round. For each task, we treat every individual as a target individual once. For
a fair comparison, the HMM model learned for both the baseline algorithm and the EM algorithm
has the same number of components as the HMM model learned by the online transfer learning
algorithm.
Regarding RNNs, we used architectures with as many input nodes as the number of attributes, one
hidden layer consisting of long short term memory (LSTM) units (Hochreiter & Schmidhuber, 1997)
and one softmax output layer with as many nodes as the number of classes. We use the categorical
cross-entropy loss as the cost function. We select LSTM units instead of sigmoid or hyperbolic
tangent units due to their popularity and success in sequence learning (Sutskever et al., 2014).
We perform grid search to select the best hyper-parameters for each setting. For the training method,
we either use Nesterov’s accelerated gradient descent (Nesterov, 1983; Sutskever et al., 2013) with
learning rates [0.001,0.01,0.1,0.2] and momentum values [0,0.2,0.4,0.6,0.8,0.9], or rmsprop (Tiele-
man & Hinton, 2012) having ε = 10-4 and decay factor 0.9 (standard values) with learning rates
[0.00005,0.0001,0.0002,0.001] and momentum values [0,0.2,0.4,0.6,0.8,0.9]. The weight decay
takes values from [0.001,0.01,0.1], whereas the number of LSTM units in the hidden layer takes
the possible values [2,4,6,9,12].
We experimented with various architectures before we ended up with the aforementioned values;
in particular, architectures with a single hidden layer consistently performed better than multiple
layers, possibly because our datasets are not very complex. We train the network by backpropagation
through time (bptt) truncated to 20 time steps (Williams & Peng, 1990). The RNNs are trained for a
maximum number of 150 epochs, or until convergence is reached. Our implementation is based on
the Theano library (Theano Development Team, 2016) in Python.
For each task, we run experiments 10 times with each individual taken as target and the rest acting
as source domains for training. We report the average percentage accuracy and use the Wilcoxon
signed rank test (Wilcoxon, 1950) to compute a p-value and report statistical significance when the
p-value is less than 0.05. In the following sections, we discuss the results for each task in detail.
Activity Recognition
As part of an on-going study to promote physical activity, we collected smartphone data with 19
participants and tested our transfer learning algorithm to recognize 5 different kinds of activities:
sitting, standing, walking, running and in-a-moving-vehicle. While APIs already exist to automat-
ically recognize walking, running and in-a-moving-vehicle by Android and Apple smartphones,
sitting and standing are not available in the standard APIs. Furthermore, our long term goal is to
obtain robust recognition algorithms for older adults and individuals with perturbed gait (e.g., due
to a stroke). Labeled data was obtained by instructing the 19 participants to walk at varying speeds
for 4 min, run for 2 min, stand for 2 min, sit for 2 min and ride a moving vehicle to a destination
of their choice. The data collected was segmented in epocs of 1 second where 48 features (means
and standard deviations of the 3D accelerometry in each epoch) were computed by the smartphone.
9
Published as a conference paper at ICLR 2017
The online transfer learning algorithm learned an HMM over 18 individuals which acted as basis
models for prediction on the 19th individual. In this manner, we ran experiments for each individual
10 times to get a statistical measure of the results.
Table 1: Average percentage accuracy of prediction for activity recognition on 19 different individ-
uals. The best results among the Baseline, the EM algorithm, RNN and Transfer Learning algorithm
are highlighted in bold font. ↑(or J) indicates that Transfer Learning has significantly better (or
worse) accuracy than the the best algorithm among the baseline, EM and RNN under the Wilcoxon
signed rank test with p-value < 0.05.
Target Domain	Baseline	EM	RNN	Transfer Learning
Person 1	91.29	83.57	71.15	88.36 (
Person 2	81.37	79.87	79.58	87.65↑
Person 3	74.68	75.91	69.56	93.15↑
Person 4	73.39	68.29	74.25	84.70↑
Person 5	95.94	89.59	95.36	99.75↑
Person 6	73.98	69.77	61.71	96.43↑
Person 7	57.62	55.15	69.22	70.75↑
Person 8	91.72	86.05	74.49	97.80↑
Person 9	81.19	78.88	78.72	88.75↑
Person 10	99.12	93.60	92.00	97.35(
Person 11	76.59	74.67	84.75	88.75↑
Person 12	55.36	59.71	53.63	95.05↑
Person 13	79.66	73.46	65.54	97.60↑
Person 14	92.06	89.11	63.59	93.12↑
Person 15	79.25	72.24	91.08	94.20↑
Person 16	84.08	79.23	74.74	83.5 1(
Person 17	93.95	91.03	81.25	97.60↑
Person 18	82.84	74.88	79.45	87.20↑
Person 19	95.97	89.06	95.88	95.06(
Table (1) compares the average percentage accuracy of prediction for activity recognition with 19
different individuals. It demonstrates that the transfer learning algorithm performed better than the
baseline on 15 individuals and in other cases its accuracy was close to the baseline. Furthermore,
it is also worth noting that in most cases, the confusion in the algorithm’s prediction was between
the following pairs of classes: In a Moving Vehicle—Standing and In a Moving Vehicle—Sitting.
This is expected because in most cases the person was either standing/sitting in a bus or sitting in
a car. Table (1) also demonstrates the superior performance of online transfer learning algorithm
as compared to the EM algorithm. Finally, note the poor performance of RNNs despite the fact
that we fine-tuned the architecture to get the best results. RNNs are in theory very expressive.
However, they are also notoriously difficult to train and fine-tune due to their non-convexity and
vanishing/exploding gradient issues that arise in backpropagation through time. Indeed, in several
cases they even underperform all other methods.
Sleep Stage Classification
Sleep disruption can lead to various health issues. Understanding and analyzing sleep patterns,
therefore, has great potential to significantly improve the quality of life for both patients and healthy
individuals. In both clinical and research settings, the standard tool for quantifying sleep architecture
and physiology is polysomnography (PSG), which is the measurement of electroencephalography
(EEG), electrooculography (EOG), electromyography (EMG), electrocardiography (ECG), and res-
piratory function of an individual during sleep. The analysis of sleep architecture is of relevance
for the diagnosis of several neurological disorders, e.g., Parkinson’s disease (Peeraully et al., 2012),
because neurological anomalies often also reflect in variations of a patient’s sleep patterns.
Typically, PSG data is divided into 30-second epochs and classified into 5 stages of sleep — wake
(W), rapid eye movement sleep (REM) or one of 3 non-REM sleep stages (N1, N2, and N3) —
based on the visual identification of specific signal features on the EEG, EOG, and EMG channels.
Epochs that cannot be distinctly sorted into one of the 5 stages are labeled as Unknown. While it
is a valuable clinical and research tool, visual classification of EEG data remains time consuming,
10
Published as a conference paper at ICLR 2017
requiring up to 2 hours for a highly trained technologist to classify all the epochs within a typical 7-
hour PSG recording. Beyond that, inter-scorer agreement rates remain low around 80 (Rosenberg &
Van Hout, 2013). High annotation costs and low inter-scorer agreement rates have motivated efforts
to develop fully automated approaches for sleep stage classification (Anderer et al., 2005; Jensen
et al., 2010; Mal, 2013; Punjabi et al., 2015). However, many of these methods result in generic
cross-patient classifiers that fail to reach levels of accuracy and reliability high enough to be adopted
in real-world medical settings.
The polysomnograms (PSGs) we used for our evaluation were obtained at a clinical neurophys-
iology laboratory in Toronto (name anonymized) according to the American Academy of Sleep
Medicine guidelines using a Grael HD PSG amplifier (Compumedics, Victoria, Australia). We se-
lected recordings from 142 patients obtained between 2009 and 2015. Out of these 142 recordings,
91 were from healthy subjects and 51 were from patients with Parkinson’s disease.
Each recording was manually scored by a single registered PSG technologist. Recordings were first
segmented into fixed-sized windows of 30 second epochs. To reduce complexity and processing
time in the feature extraction and manual labeling step, we only retained EEG channel C4-A1,
which is deemed especially important for sleep stage classification (Sil, 2007). Channel selection
and segmentation resulted in a ground truth data set where each instance was represented by a single-
channel time series of 7680 floating point numbers corresponding to 30 seconds of C4-A1, sampled
at 256 Hz. A vector of 26 scalar features was extracted from each epoch. Bao et al. (2011) and
Motamedi-Fakhr et al. (2014) give a detailed listing and explanation of all 26 features.
The online transfer learning algorithm learned an HMM over 50 individuals chosen at random which
acted as basis models for prediction on the target individual. We did not use all 140 individuals
for the basis models because it resulted in sources getting sparse weights diluting the effect of
heterogeneity. We completed the experiments for each individual 10 times in this manner to get
a statistical measure of the results.
Fig. (2) shows the scatter plots of accuracy for our online transfer learning technique and the three
baseline algorithms - BMM, EM (maximum likelihood) and RNNs - which treat the data as homoge-
neous for the sleep stage classification dataset. For each plot, a point above the dotted line indicates
higher accuracy of online transfer learning technique as compared to the corresponding baseline al-
gorithm for the target patient. The plots show consistent superior performance of our online transfer
learning technique as compared to both baseline algorithms - BMM and EM for all target patients.
The online transfer learning technique also performs better on a majority of patients (102 out of 142)
as compared to an optimized RNN.
6fEEΘ, ,lə-sue.1
100
80
60
40
20
Scatter Plot of accuracy for Sleep Stage Classification
40
20
Ooooo
0 8 6 4 2
6u.!eθ, .lə-sue-ll
0	20	40	60	80	100
BMM
0	20	40	60	80	100
EM (max. likelihood)
0	20	40	60	80	100
RNN
Figure 2: Performance comparison of online transfer learning algorithm with three different baseline
algorithms - BMM, EM (max. likelihood) and RNNs on Sleep Stage Classification data using scatter
plots of accuracy.
All the results are statistically significant under the Wilcoxon signed rank test with p-value < 0.05.
More detailed results for comparison of the online transfer learning technique with the three baseline
algorithms is given in appendix (D).
11
Published as a conference paper at ICLR 2017
Flow Direction Prediction
Accurate prediction of future traffic plays an important role in proactive network control. Proactive
network control means that if we know the future traffic (including directions and traffic volume),
then we have more time to find a better policy for the network routing, priority scheduling as well as
rate control in order to maximize network throughput while minimizing transmission delay, packet
loss rate, etc.
Better understanding the behavior of TCP connections in certain applications can provide important
input to automatic application type detection, especially in those scenarios where network traffic
is encrypted and DPI (Deep Packet Inspection) is nearly impossible. Different applications can be
distinguished by the distinct behavior of their TCP connections, which are well described by the
corresponding HMMs.
We performed our experiments with a publicly available dataset of real traffic from academic build-
ings. The dataset consists of packet traces with TCP flows. For our experiments, we only consider
three packet sizes and flow size as the features. The hidden labels are the source of generation of the
packet, i.e., Server or Client. We divided the dataset into 9 domains with each domain consisting of
a number of observation sequences. For the online transfer learning algorithm, we learned an HMM
for each of 8 sources that acted as basis models for prediction on the 9th source. We compared the
performance of the online transfer learning algorithm with EM and the baseline algorithm which
treat the data as homogeneous. Table 2 reports the average (of 10 experimental runs) percentage
accuracy for each source. The online transfer learning algorithm performs better than both the base-
line and the EM algorithm. The results are statistically significant under the Wilcoxon signed rank
test with p-value < 0.05. Furthermore, we compare our method to RNNs. It turns out that for the
task of traffic direction prediction, RNNs can actually perform well, unlike for instance the activity
recognition dataset. The better performance this time may be due to the simpler structure of the
data that consists of a single attribute and a binary class. This is in sharp contrast to the activity
recognition dataset whose instances contain 48 attributes and can belong to 5 classes, and is thus
harder to train.
Table 2: Average percentage accuracy of prediction for flow direction prediction for 9 different
domains. The best results among the Baseline, the EM algorithm, RNN and the Transfer Learning
algorithm are highlighted in bold font. ↑(or J) indicates that transfer learning has significantly better
(or worse) accuracy than the best technique among the baseline algorithm, EM and RNN under
Wilcoxon signed rank test with pvalue < 0.05.
Target Domain	Baseline	EM	RNN	Transfer Learning
Source 1	72.00	54.90	80.00	71.02 ；
Source 2	85.33	89.10	65.30	86.50；
Source 3	80.33	81.90	86.50	83.33↑
Source 4	86.50	75.80	86.60	87.17↑
Source 5	87.33	82.80	81.70	86.00；
Source 6	93.33	78.20	88.90	93.50↑
Source 7	95.17	90.70	93.50	95.33↑
Source 8	89.83	91.14	91.00	91.63↑
Source 9	76.67	75.68	81.98	78.83↑
6 Conclusion
In many applications, data is produced by a population of individuals that exhibit a certain degree of
variability. Traditionally, machine learning techniques ignore this variability and train a single model
under the assumption that the population is homogeneous. While several offline transfer learning
techniques have already been proposed to account for population heterogeneity, this work describes
the first online transfer learning technique (to our knowledge) that incrementally determines which
source models best explain a streaming sequence of observations while predicting the correspond-
ing hidden states. We achieved this by adapting the online Bayesian moment matching algorithm
originally developed for mixture models to hidden Markov models. Experimental results confirm
12
Published as a conference paper at ICLR 2017
the effectiveness of the approach in three real-world applications: activity recognition, sleep stage
recognition and flow direction prediction.
In the future, this work could be extended in several directions. Since it is not always clear how
many basis models should be used and that the observation sequences of target individuals can nec-
essarily be explained by a weighted combination of basis models, it would be interesting to explore
techniques that can automatically determine a good number of basis models and that can generate
new basis models on the fly when existing ones are insufficient. Furthermore, since recurrent neural
networks (RNNs) have been shown to outperform HMMs with GMM emission distributions in some
applications such as speech recognition (Graves et al., 2013), it would be interesting to generalize
our online transfer learning technique to RNNs.
Acknowledgments
This work was funded by grants from the Network for Aging Research at the University of Waterloo,
the PROPEL Centre for Population Health Impact at the University of Waterloo, Huawei Noah’s Ark
Laboratory in Hong Kong, CIHR (CPG-140200) and NSERC (CHRP 478468-15).
References
The Visual Scoring of Sleep in Adults. Journal ofClinical Sleep Medicine, 3(2):121-131, mar 2007.
ISSN 1550-9389.
Performance ofan Automated Polysomnography Scoring System Versus Computer-assisted Manual
Scoring. Sleep, 36(4):573-582, apr 2013. ISSN 1550-9109. doi: 10.5665/sleep.2548.
Samir Al-Stouhi and Chandan K Reddy. Adaptive boosting for transfer learning using dynamic
updates. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pp. 60-75. Springer, 2011.
Peter Anderer, Georg Gruber, Silvia Parapatics, Michael Woertz, Tatiana Miazhynskaia, Gerhard
Klosch, Bernd Saletu, Josef Zeitlhofer, Manuel J Barbanoj, Heidi Danker-Hopfe, Sari-Leena
Himanen, Bob Kemp, Thomas Penzel, Michael Grozinger, Dieter Kunz, Peter Rappelsberger,
Alois Schlogl, and Georg Dorffner. An E-health Solution for Automatic Sleep Classification
According to Rechtschaffen and Kales: Validation Study of the Somnolyzer 24 x 7 Utiliz-
ing the Siesta Database. Neuropsychobiology, 51(3):115-133, 2005. ISSN 0302-282X. doi:
10.1159/000085205.
Forrest S Bao, Xin Liu, and Christina Zhang. PyEEG: An Open Source Python Module for
EEG/MEG Feature Extraction. Computational Intelligence and Neuroscience, 2011:1-7, 2011.
ISSN 1687-5265. doi: 10.1155/2011/406391.
Rita Chattopadhyay, Narayanan Chatapuram Krishnan, and Sethuraman Panchanathan. Topology
preserving domain adaptation for addressing subject based variability in semg signal. In AAAI
Spring Symposium: Computational Physiology, pp. 4-9, 2011.
Hai Leong Chieu, Wee Sun Lee, and Leslie P Kaelbling. Activity recognition from physiological
data using conditional random fields. 2006.
Diane Cook, Kyle D Feuz, and Narayanan C Krishnan. Transfer learning for activity recognition: A
survey. Knowledge and information systems, 36(3):537-556, 2013.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu. Boosting for transfer learning. In Pro-
ceedings of the 24th international conference on Machine learning, pp. 193-200. ACM, 2007.
Morris H. Degroot. Optimal statistical dcisions. McGraw-Hill Book Company, New York, St Louis,
San Francisco, 1970. ISBN 0-07-016242-5. URL http://opac.inria.fr/record=
b1080767.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE international conference on acoustics, speech and signal
processing, pp. 6645-6649. IEEE, 2013.
13
Published as a conference paper at ICLR 2017
S Hochreiter and J Schmidhuber. Long short-term memory. Neural Comp, 9(8):1735-1780,1997.
Wei-Shou Hsu and Pascal Poupart. Online bayesian moment matching for topic modeling with
unknown number of topics. In Advances In Neural Information Processing Systems, 2016, 2016.
Priyank Jaini and Pascal Poupart. Online and distributed learning of gaussian mixture models by
bayesian moment matching. arXiv preprint arXiv:1609.05881, 2016.
Priyank Jaini, Abdullah Rashwan, Han Zhao, Yue Liu, Ershad Banijamali, Zhitang Chen, and Pascal
Poupart. Online algorithms for sum-product networks with continuous variables. In Proceedings
of the Eighth International Conference on Probabilistic Graphical Models, pp. 228-239, 2016.
Peter S Jensen, Helge B D Sorensen, Helle L Leonthin, and Poul Jennum. Automatic Sleep Scoring
in Normals and in Individuals with Neurodegenerative Disorders According to New International
Sleep Scoring Criteria. Journal of Clinical Neurophysiology: Official Publication of the American
Electroencephalographic Society, 27(4):296-302, aug 2010. ISSN 1537-1603. doi: 10.1097/
WNP.0b013e3181eaad4b.
Shayan Motamedi-Fakhr, Mohamed Moshrefi-Torbati, Martyn Hill, Catherine M Hill, and Paul R
White. Signal Processing Techniques Applied to Human Sleep EEG Signals - A Review.
Biomedical Signal Processing and Control, 10:21-33, mar 2014. ISSN 17468094. doi:
10.1016/j.bspc.2013.12.003.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate
O(1/sqr(k)). Soviet Mathematics Doklady, 27:372-376, 1983.
Farheen Omar. Online bayesian learning in probabilistic graphical models using moment matching
with applications. 2016.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2010.
Tasneem Peeraully, Ming-Hui Yong, Sudhansu Chokroverty, and Eng-King Tan. Sleep and Parkin-
son’s disease: A review of case-control polysomnography studies. Movement Disorders, 27(14):
1729-1737, dec 2012. ISSN 08853185. doi: 10.1002/mds.25197.
Naresh M Punjabi, Naima Shifa, Georg Dorffner, Susheel Patil, Grace Pien, and Rashmi N Au-
rora. Computer-Assisted Automated Scoring of Polysomnograms Using the Somnolyzer System.
Sleep, 38(10):1555-1566, 2015. ISSN 1550-9109. doi: 10.5665/sleep.5046.
Parisa Rashidi and Diane J Cook. Transferring learned activities in smart environments. In Intelli-
gent Environments, pp. 185-192, 2009.
Abdullah Rashwan, Han Zhao, and Pascal Poupart. Online and distributed bayesian moment match-
ing for sum-product networks. In International Conference on Artificial Intelligence and Statistics
(AISTATS), pp. 1727-1735, 2016.
Richard S. Rosenberg and Steven Van Hout. The American Academy of Sleep Medicine Inter-scorer
Reliability Program: Sleep Stage Scoring. Journal of Clinical Sleep Medicine, jan 2013. ISSN
1550-9389. doi: 10.5664/jcsm.2350.
Masa-Aki Sato. Online model selection based on the variational bayes. Neural Computation, 13(7):
1649-1681, 2001.
Ling Shao, Fan Zhu, and Xuelong Li. Transfer learning for visual categorization: A survey. IEEE
transactions on neural networks and learning systems, 26(5):1019-1034, 2015.
I. Sutskever, O. Vinyals, and Q.V. Le. Sequence to sequence learning with neural networks. In NIPS,
pp. 3104-3112, 2014.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of
initialization and momentum in deep learning. In Proceedings of International Conference on
Machine Learning (ICML), pp. 1139-1147, 2013.
14
Published as a conference paper at ICLR 2017
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research ,10(JUl):1633-1685, 2009.
Theano Development Team. Theano: A Python framework for fast computation of mathematical
expressions. arXiv e-prints, abs/1605.02688, 2016.
T. Tieleman and G. Hinton. LectUre 6.5 - rmsprop, coUrsera: NeUral networks for machine learning.
Technical report, 2012.
Chong Wang, John William Paisley, and David M Blei. Online variational inference for the hierar-
chical dirichlet process. In AISTATS, volUme 2, pp. 4, 2011.
Frank Wilcoxon. Some rapid approximate statistical procedUres. Annals of the New York Academy
of Sciences, pp. 808-814, 1950.
R.J. Williams and J. Peng. An efficient gradient-based algorithm for online training of recUrrent
network trajectories. Neural Computation, 2(4):490-501, 1990.
Yi Yao and Gianfranco Doretto. Boosting for transfer learning with mUltiple soUrces. In Computer
Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 1855-1862. IEEE, 2010.
Zhongtang Zhao, Yiqiang Chen, JUnfa LiU, Zhiqi Shen, and Mingjie LiU. Cross-people mobile-
phone based activity recognition. In Twenty-Second International Joint Conference on Artificial
Intelligence, 2011.
15
Published as a conference paper at ICLR 2017
A	Normal-Wishart and Dirichlet Distribution
Dirichlet Distribution
The Dirichlet distribution is a family of multivariate continuous probability distributions over the
interval [0,1]. It is the conjugate prior probability distribution for the multinomial distribution. We
next show how the combining happens for a Dirichlet as has been highlighted in (3).
WmDir (w, α) = Q W αi) Wm Y Wai
Γ(αi)
where
WmDir(w, α) =
γ(Ei αi) wam+1
∏i Rai) m
WmDir(w, α)
αi
ai= ɪai + 1
Wiαi
6=m
Jm Dir(w; α)
α
if i 6= m
if i = m
Normal Wishart Prior
The Normal-Wishart distribution is a conjugate prior of a multivariate Gaussian distribution with
unknown mean and precision matrix (Degroot, 1970). It is the combination of a Wishart distribution
over the precision matrix and Gaussian distribution over the mean given the precision matrix.
Let μ bea d-dimensional vector and Λ bea symmetric positive definite d X d matrix of random vari-
ables respectively. Then, a Normal-Wishart distribution over (μ, Λ) given parameters (μ0, κ, W, V)
is such that μ ~Nd(μ; μ0, (κΛ)-1) where K > 0 is real, μ° ∈ Rd and Λ has a Wishart distribution
given as Λ ~ W(Λ; W, V) where W ∈ Rd×d is a positive definite matrix and ν > d 一
The marginal distribution of μ is a multivariate t-distribution i.e μ∣Λ 〜tν-d+ι (μ; μ0,
1 is real.
W
K(V—d+1) 7 .
A Normal-Wishart distribution multiplies with a Gaussian with the same mean and precision matrix
to give a new Normal-Wishart distribution.
NdE ”, (κΛ)-1)NW(μ, Λ; μo, κ, W, V) = CNW(μ, Λ; μ0, κ*, W*,ν*)
where
*
μ*
*
K
*
ν
W*
κμo + y
κ + 1
1+K
V+1
:W+-KT (μo 一 y)(μo 一 y)T
K+1
Moment Matching
In this section we show the system of equations using which the parameters of a product of Dirichlet
and Normal-Wishart distribution can be estimated once the set of sufficient moments are known.
The set of sufficient moments in this case is S = {μj, μj μj, Λj, Kjk m,Wj ,wj} | ∀j ∈ 1,2,…，M}
where Λj2 , is the (k, m)th element of the matrix Λj.The expressions for the sufficient moments
are :
E[wi] = P^-；
jαj
E[Λ] = VW;
E[μ] = δ;
lσr 21	(αi)(αi + 1)
E[wi] = T------VV---------V
Pjαj	1+Pjαj
V ar(Λij) = V(Wi2j + WiiWjj)
E[(μ 一 δ)(μ 一 δ)T] = K +1 WT
K(V 一 d 一 1)
16
Published as a conference paper at ICLR 2017
EI	.	Γ∙ . t	•	,♦六	F	.	1	∙	.1	. ∙	F	∙	. )
The parameters of the approximate posterior P can be computed using the equations above in the
following manner
αi = E[wi]整 - E[w2]
L	E[w2]- E[wi]2
δ = E[μ]
Wii
Wij
_ Var(Λii)
=E[Λii]
_ Var(Λij)
=E[Λj]
E[Λ]
WT
K = 1 - ((V - d - 1)E[(μ - δ)(μ - δ)T])w)-1
B Source Domain Learning using BMM
The update equation at each time step for a source domain k is
Pr (θ, Φ,Ytk=j∣Xk,Yt-ι = i)
Emission distribution Transition Probability
-----------A-------------{z---------------A------------------
Prior for t - 1
人
(X Pr(Xk∣Ytk = j) Pr(Ytk = j∣Yt-ι = i) Pr(Θk, Φk, Y-γ = iM：1)
∀j ∈ {1,2,...,N}
The posterior after inserting all the relevant terms can be written as -
MN
Pr (θ, Φ,Ytk = j∖Xt ,Yt-ι = i) X X Wjm N(Xk ∣μkm, ∑km )θji [Y Dir癌\芯)]
m=1	i=1
NM
[Y Dir(Wk； βj)Y NW(μju, Aku ； δku, Kju, Wku, Vku)]	∀j ∈{1, 2,..∙, N}
j=1	u=1
Using (A), we can re-write this as
MNMN
Pr (θ, Φ,Ytk = j∖Xj ,Yt-1 = i) = Z XYY Y C (i,j,k,m) [Dir(θj ∖α j)Dir(θj ∖ɑU)]
m=1 u6=i u6=m i6=j
hDir(wj ；β j )Dir(wj ； βk )ihNW (μjm, Akm ； £,嗫,W km ,jm )nw (μju, Aku ； δju ,κju, Wku ,v ju )i
(9)
where Z = Pi,j,k,m C(i, j, k, m) is the normalization constant. Eq (9) is a mixture of product of
distributions where each component belongs to the same family as the prior distribution. The set of
sufficient moments in this case would be
S = {θk , (θk )2, Wk ,(wk )2,μkm ,μjm (μkm )T , Akm , Akm (Akm	S {1, 2,…,M}}
The exact moments can be calculated by
E[z]
/
Θ,Φ
zPr Θ, Φ, Ytk =j∖Xtk,Ytk-1
= i d(Θ)d(Φ)
∀z ∈ S
ν
z
{
Once we know the moments, we can use these moments to estimate the parameters of the approxi-
mate distribution using ideas discussed in (3).
17
Published as a conference paper at ICLR 2017
C Target Domain Learning using BMM
The prior over the weights is
Pr(λ, π) = Dir(λ; γ)Dir(π; ν)
where γ and ν are the hyper-parameters for the Dirichlet distribution. The posterior after each
observation is
N
Pr (λ,∏,Yt = j|Xt) (X Pr(Xt∣Yt = j) X Pr(Yt = j∣Yt-i = i) Pr(λ, π,Yt-i)
i=1
K M	NK
X X ∏k X NWu, ∑ku)χχλmθimj Dir(λ; γ)Dir(π; ν)
k=1	u=1	i=1 m=1
KN
ΣΣ∏kDir(π; ν) λmDir(λ; γ) E N(μju, Σku )θm
{^^^^^^^^^^^^^- *^^^^^^{∙^^^^^^^
k，m i=1 combines
combines	|=1
known
KN
Z XXC(j, k, m) Dir(∏; V)Dir(λ; Y)
k,m i=1
(10)
(11)
(12)
(13)
M
X
z
}
where Z = i,j,k,m C(i, j, k, m) is the normalization constant, K is the number of source domains
and N is the number of hidden classes.
Now, we can use the Bayesian Moment Matching algorithm to approximate Eq (8) as a product of
two Dirichlets, in the same form as the prior. This posterior will then act as the prior for the next
time step. Finally, the values of the weights will be the expected value of each Dirichlet. Let us next
see how the combining happens for a Dirichlet.
where
Therefore C(i,j,k,m) in Eq (8) is
λmDir(λ, γ)=二m Dir(λ; Y)
iγi
^i = C + 1
if i 6= m
if i = m
C(i, j, k, m)
iπi
M
X N(μku, Σu )θ
u=1
m
ij
Next, we outline the moment matching step. The set of sufficient moments is given by
S = {λi,冠,∏i,∏ ∣∀i ∈{1,2,..,K}}
1KN
E[λn] = Z ΣΣ	λnC(i,j, k, m)Dir(∏; V)Dir(λ; Y)d(λ)d(π)
k,m i=1
1KN
=⅛ΣΣ	λnC (i,j,k,m)Dir(λ; Y)d(λ)
k,m i=1
=Z X X (Pnr )c(i,j,k,m)
k,m i=1	u λu
(14)
(15)
(16)
(17)
(18)
Similarly, the second moment can be evaluated as
18
Published as a conference paper at ICLR 2017
1KN
Z XX∕λnc(
k,m i=1
i,j, k, m)Dir(π; V)Dir(λ; Y)d(λ)d(π)
KN
Z xx
k,m i=1
C / C 一、
λn (λn + I)
(PU λ )(1+ P λu)
C(i, j, k, m)
(19)
(20)
We evaluate the moments using the equations above ∀z ∈ S Once we have the two moments, we
can project the posterior into a family of Dirichlet distributions having the same moments. In this
way we can perform the learning of the parameters for the target domain.
Sleep Stage Classification for 142 patients
D Experiment Results : Sleep S tage Classification
Fig. 3, 4 and 5 compare the performance of the online transfer learning algorithm with the baseline
algorithm, the EM algorithm and recurrent neural networks (RNNs) respectively.
8 ∞∙,,°"∙,~l∙,
-3n8”
50	100
Patient ID
(a) Percentage accuracy
∞∞m2"
(Aoe-Inooa) eoue-le=fi%
∞∞4°2°
(Aoelnoow eoue-l-=ɑ %
(b) Accuracy difference
Figure 3:	Performance comparison of online transfer learning algorithm and baseline for the task of
sleep stage classification.
A0e-ln84%
(a) Percentage accuracy
Oooooo
0 8 6 4 2
(Acn8s ece4。-。
Oooooo
0 8 6 4 2
(Acn8s ece4。-。
(b) Accuracy difference
Figure 4:	Performance comparison of online transfer learning algorithm and EM algorithm for the
task of sleep stage classification.
Fig. 3a compares the average percentage accuracy for our online transfer learning technique and the
baseline algorithm and Fig. 4a compares EM and online transfer learning. The blue + signs represent
the accuracy of the baseline algorithm and the red o represent the accuracy of the online transfer
learning algorithm. The black line is a reference line that passes through the points plotting the
accuracy of the online transfer Learning algorithm. The accuracy is plotted against each individual
patient. The blue + signs are always below the black line indicating superior performance of the
transfer learning algorithm. Fig. 3b and 4b plot the difference between the accuracy of the baseline
algorithm and the transfer learning algorithm. In the top plot, the difference in accuracy is for each
19
Published as a conference paper at ICLR 2017
patient corresponding to those shown in Fig. 3a and 4a. In the bottom plot, the difference in accuracy
is plotted after sorting. A reference line of 0 is also plotted for the case when there is no difference
in performance. The plots suggest that for a majority of patients the transfer learning technique
outperforms both the baseline algorithm and EM.
δ-Enoo<%,
(a) Percentage accuracy
Difference in accuracy for each patient ID
60
ɪ 40
I 20
o 0
3-20
⅛ -40
0	50	100	150
(b) Accuracy difference
Figure 5:	Performance comparison of online transfer learning algorithm and Recurrent Neural Net-
works for the task of sleep stage classification.
In Fig. 5a we compare the performance of the online transfer learning algorithm with RNNs. Fig. 5b
plots the difference between the accuracy of RNN and the online transfer learning algorithm. In the
top plot, the difference in accuracy is for each patient corresponding to those shown in Fig. 5a. In
the bottom plot, the difference in accuracy is plotted after sorting. The figures show that the online
transfer learning algorithm outperformed RNNs for a majority of patients (102 out of 142). All the
results are statistically significant under the Wilcoxon signed rank test with p-value < 0.05.
20