Table 1: The accuracies of our experimental model, Multimodal Attention Residual Networks(MARN), with respect to the number of learning blocks (L#), the number of glimpse (G#), the po-sition of activation functions (tanh), answer sampling, shortcut connections, and data augmentationusing Visual Genome dataset, for VQA test-dev split and Open-Ended task. Note that our proposedmodel, Multimodal Low-rank Bilinear Attention Networks (MLB) have no shortcut connections,compared with MARN. MODEL: model name, SIZE: number of parameters, ALL: overall accu-racy in percentage, Y/N: yes/no, NUM: numbers, and ETC: others. Since Fukui et al. (2016) onlyreport the accuracy of the ensemble model on the test-standard, the test-dev results of their singlemodels are included in the last sector. Some figures have different precisions which are rounded. *indicates the selected model for each experiment.
Table 2: The VQA test-standard results to compare with state-of-the-art. Notice that these resultsare trained by provided VQA train and validation splits, without any data augmentation.
Table 3: The VQA test-standard results for ensemble models to compare with state-of-the-art. Forunpublished entries, their team names are used instead of their model names. Some of their figuresare updated after the challenge.
Table 4: Hyperparameters used in MLB (single model in Table 2).
Table 5: The effect of joint embedding size d.
Table 6: The individual models used in our ensemble model in Table 3.
