Table 1: Initial source domain distributionHopper	μ	σ	low	highmass	6.0	1.5	3.0	9.0ground friction	2.0	0.25	1.5	2.5joint damping	2.5	1.0	1.0	4.0armature	1.0	0.25	0.5	1.5Half-Cheetah	μ	σ	low	highmass	6.0	1.5	3.0	9.0ground friction	0.5	0.1	0.3	0.7joint damping	1.5	0.5	0.5	2.5armature	0.125	0.04	0.05	0.2OooooooOoooooo0 5 0 5 0 5 04 3 3 2 2 1 18 UeLLUOtSd5	6	7Torso Mass8	9Figure 3: Comparison between policies trained
Table 2: Performance statistics for different settings for the hopper taskPerformance (Return)C	mean	Std	Percentiles			5	10	25	50	75	90	 0.05	2889	502	1662 2633 2841	2939 2966 30830.1	3063	579	1618 2848	3223	3286 3336 33960.2	3097	665	1527	1833	3259	3362 3423	34830.3	3121	706	1461	1635 3251	3395	3477	35130.4	3126	869	1013	1241	3114 3412 3504 35460.5	3122	1009	984	1196	1969	3430 3481	35670.75	3133	952	1005	1516 2187	3363	3486 35481.0	3224	1060	1198	1354	1928	3461	3557	3604Max-Lik	1710	1140	352	414	646	1323	3088	3272A.6 Importance of baseline for BatchPolOptAs described in Section 3.1, it is important to use a good baseline estimate for the value function forthe batch policy optimization step. When optimizing for the expected return, we can interpret thebaseline as a variance reduction technique. Intuitively, policy gradient methods adjust parametersof the policy to improve probability of trajectories in proportion to their performance. By using abaseline for the value function, we make updates that increase probability of trajectories that performbetter than average and vice versa. In practice, this variance reduction is essential for getting policy
