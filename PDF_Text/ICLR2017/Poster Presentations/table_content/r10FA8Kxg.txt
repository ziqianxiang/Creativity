Table 1: Accuracy on CIFAR-10 of shallow and deep models trained on the original 0/1 hard classlabels using Bayesian optimization with dropout and weight decay. Key: c = convolution layer; mp= max-pooling layer; fc = fully-connected layer; lfc = linear bottleneck layer; exponents indicaterepetitions of a layer. The last two models (*) are numbers reported by Ba and Caruana (2014). Themodels with 1-4 convolutional layers at the top of the table are included for comparison with studentmodels of similar architecture in Table 2 . All of the student models in Table 2 with 1, 2, 3, and 4convolutional layers are more accurate than their counterparts in this table that are trained on theoriginal 0/1 hard targets — as expected distillation yields shallow models of higher accuracy thanshallow models trained on the original training data.
Table 2: Comparison of student models with varying number of convolutional layers trained to mimicthe ensemble of 16 deep convolutional CIFAR-10 models in Table 1 . The best performing studentmodels have 3-4 convolutional layers and 10M-31.6M parameters. The student models in thistable are more accurate than the models of the same architecture in Table 1 that were trained on theoriginal 0/1 hard targets — shallow models trained with distillation are more accurate than shallowmodels trained on 0/1 hard targets. The student model trained by Ba and Caruana (2014) is shown inthe last line for comparison; it is less accurate and much larger than the student models trained herethat also have 1 convolutional layer.
Table 3: Optimization bounds for student models. (Models trained on 0/1 hard targets were describedin Sections 6.1 and 6.2.) Abbreviations: fc (fully-connected layer, ReLu), c (convolutional, ReLu),linear (fully-connected bottleneck layer, linear activation function), dependent (dependent variable,chosen s.t. parameter budget is met).
