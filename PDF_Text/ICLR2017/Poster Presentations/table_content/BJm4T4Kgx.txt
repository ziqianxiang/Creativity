Table 1: Top 1 and top 5 accuracies of an adversarially trained network on clean images and ad-versarial images with various test-time . Both training and evaluation were done using “step l.l.”method. Adversarially training caused the baseline model to become robust to adversarial exam-ples but lost some accuracy on clean examples. We therefore also trained a deeper model with twoadditional Inception blocks. The deeper model benefits more from adversarial training in terms ofrobustness to adversarial perturbation, and loses less accuracy on clean examples than the smallermodel does.
Table 2: Accuracy of adversarially trained network on iterative adversarial examples. Adversarialtraining was done using “step l.l.” method. Results were computed after 140k iterations of training.
Table 3: Effect of label leaking on adversarial examples. When training and evaluation was doneusing FGSM accuracy on adversarial examples was higher than on clean examples. This effect wasnot happening when training and evaluation was done using “step l.l.” method. In both experimentstraining was done for 150k iterations with initial learning rate 0.0225.
Table 4: Transfer rate of adversarial examples generated using different adversarial methods andperturbation size = 16. This is equivalent to the error rate in an attack scenario where the attackerprefilters their adversarial examples by ensuring that they are misclassified by the source modelbefore deploying them against the target. Transfer rates are rounded to the nearest percent in order tofit the table on the page. The following models were used for comparison: A and B are Inception v3models with different random initializations, C is Inception v3 model with ELU activations insteadof Relu, D is Inception v4 model. See also Table 6 for the absolute error rate when the attack is notprefiltered, rather than the transfer rate of adversarial examples.
Table 5: Comparison of different one-step adversarial methods for adversarial training. The evalua-tion was run after 90k training steps.
Table 6: Error rates on adversarial examples transferred between models, rounded to the nearestpercent. Results are provided for adversarial images generated using different adversarial methodsand fixed perturbation size = 16. The following models were used for comparison: A and B areInception v3 models with different random initializations, C is Inception v3 model with ELU acti-vations instead of Relu, D is Inception v4 model. See also Table 4 for the transfer rate of adversarialexamples, rather than the absolute error rate.
Table 7: Activation functions and robustness to adversarial examples. For each activation functionwe adversarially trained the network on “step l.l.” adversarial images and then run classification ofclean images and adversarial images generated using various adversarial methods and .
Table 8: Results of adversarial training depending on k — number of adversarial examples in theminibatch. Adversarial examples for training and evaluation were generated using step l.l. method.
