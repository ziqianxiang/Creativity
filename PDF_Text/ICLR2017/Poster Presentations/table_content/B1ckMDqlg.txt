Table 1: Summary of high-capacity MoE-augmented models with varying computational budgets,vs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C.
Table 2: Results on WMT'14 En→ Fr newstest2014 (bold values represent best results).
Table 3: Results on WMT'14 En → De newstest2014 (bold values represent best results).
Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
Table 5: Multilingual Machine Translation (bold values represent best results).
Table 6: Experiments with different combinations of losses.
Table 7: Model comparison on 1 Billion Word Language Modeling Benchmark. Models markedwith * are from (Jozefowicz et al., 2016).
Table 8: Model comparison on 100 Billion Word Google News DatasetModel	Test Perplexity .1 epochs	Test Perplexity 1 epoch	ops/timestep (millions)	#ParamS excluding embed. & softmax (millions)	Total #ParamS (billions)	TFLOPS per GPU (observed)Kneser-Ney 5-gram	-67∏-	-453-	0.00001-		76.0-	4xLSTM-512	54.5	47.0	8.4	8.4	0.1	1.23MoE-32	48.5	40.4	8.4	37.8	0.1	0.83MoE-256-h	42.8	35.3	8.4	272.9	0.4	1.11MoE-1024-h	40.3	32.7	8.5	1079.0	1.2	1.14MoE-4096-h	38.9	30.9	8.6	4303.4	4.4	1.07MoE-16384-h	38.2	29.7	8.8	17201.0	17.3	0.96MoE-65536-h	38.2	28.9	9.2	68791.0	68.9	0.72MoE-131072-h	39.8	29.2	9.7	137577.6	137.7	0.30Results: We evaluate our model using perplexity on a holdout dataset. Results are reported inTable 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE16Published as a conference paper at ICLR 2017model than for the baseline model. It is notable that the measured computational efficiency ofthe largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likelya result of the fact that, for purposes of comparison to the other models, we did not increase thetraining batch size proportionally to the number of GPUs. For comparison, we include results fora computationally matched baseline model consisting of 4 LSTMs, and for an unpruned 5-gram
Table 9: Contexts corresponding to afew of the 2048 experts in the MoE layer in the encoder portionof the WMT’14 En→ Fr translation model. For each expert i, we sort the inputs in a training batchin decreasing order of G(x)i, and show the words surrounding the corresponding positions in theinput sentences.
