Table 1: Statistics of the Penn Treebank, WikiText-2, and WikiText-103. The out of vocabulary(OoV) rate notes what percentage of tokens have been replaced by an hunki token. The token countincludes newlines which add to the structure of the WikiText datasets.
Table 2: Single model perplexity on validation and test sets for the Penn Treebank language model-ing task. For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and largerefer to a 650 and 1500 unit two layer LSTM respectively. Parameter numbers with âˆ£ are estimatesbased upon our understanding of the model and with reference to Kim et al. (2016).
Table 3: Single model perplexity on validation and test sets for the WikiText-2 language modelingtask. All compared models use a two layer LSTM with a hidden size of 650.
