Table 1: Test set log-likelihood of various models on the permutation-invariant MNIST, Omniglot,and Caltech-101 Silhouettes datasets. For the discrete VAE, the reported log-likelihood is estimatedwith 104 importance-weighted samples (Burda et al., 2016). For comparison, we also report perfor-mance of some recent state-of-the-art techniques. Full names and references are listed in Appendix I.
Table 2: Architectural hyperparameters used for each dataset. Successive columns list the numberof layers of continuous latent variables, the number of such continuous latent variables per layer,the number of deterministic hidden units per layer in the neural network defining each hierarchicallayer of the prior, and the use of parameter sharing in the prior. Smaller datasets require moreregularization, and achieve optimal performance with a smaller prior.
