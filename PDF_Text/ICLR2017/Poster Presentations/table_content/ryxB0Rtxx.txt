Table 1: Architecture for CIFAR10/100 (55 convolutions, 13.5M parameters)variable dimensions	initial stride	description3 X 3 X 3 X 16	1	1 standard ConV3 x 3 x 16 x 64	1	9 residual blocks3 x 3 x 64 x 128	2	9 residual blocks3 X 3 X 128 X 256	2	9 residual blocks—	—	8 x 8 global average pool256 x num_classes	—	random projection (not trained)Steps (xlOOO)Figure 1: Convergence plots of best model for CIFAR10 (left) and CIFAR (100) right. One step is agradient update with batch size 128.
Table 2: Comparison of top-1 classification error on different benchmarksMethod	CIFAR10	CIFAR100	ImageNet	remarksAll-CNN	-7:25-	-3239-	-41.2-	all-convolutional, dropout, extra data processingOUrS	-638-	-24.64-	-35.29-	all-convolutionalReSNet	-643-	-25.16-	-19.38-	DenseNet	3.74	19.25	N/A	4.2	ImageNetThe ImageNet ILSVRC 2012 data set has 1, 281, 167 data points with 1000 classes. Each imageis resized to 224 × 224 pixels with 3 channels. We experimented with an all-convolutional variantof the 34-layer network in He et al. (2015). The original model achieved 25.03% classificationerror. Our derived model has 35.7M trainable parameters. We trained the model with a momentumoptimizer (with momentum 0.9) and a learning rate schedule that decays by a factor of 0.94 everytwo epochs, starting from the initial learning rate 0.1. Training was distributed across 6 machines8Under review as a conference paper at ICLR 2017updating asynchronously. Each machine was equipped with 8 GPUs (NVIDIA Tesla K40) and usedbatch size 256 split across the 8 GPUs so that each GPU updated with batches of size 32.
