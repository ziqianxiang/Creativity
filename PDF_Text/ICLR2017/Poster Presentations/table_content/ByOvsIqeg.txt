Table 1: Count of the Flops for the models used in this paper: the 3-hidden-layer MLP and the 110-layer ResNet we use later in the experiments section when not regularized, using DeCov (Cogswellet al. (2016)) and using OrthoReg. Batch size is set to 128, the same we use to train the ResNet.
Table 2: Error rates for a small CNN trained with the MNIST dataset. OrthoReg leads to muchbetter results when no other improvements such as Dropout and LSUV are present but it can stillmake small accuracy increments when these two techniques are present.
Table 3: Comparison with other CNNs on CIFAR-10 and CIFAR-100 (Test error %). Orthogonallyregularized residual networks achieve the best results to the best of our knowldege. Only single-cropresults are reported for fairness of comparison. *Median over 5 runs as reported by Zagoruyko &Komodakis (November 2016).
Table 4: Comparison with other CNNs on SVHN. Wide Resnets regularized with OrthoReg showbetter performance.
