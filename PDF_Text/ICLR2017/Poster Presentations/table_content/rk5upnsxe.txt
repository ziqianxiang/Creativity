Table 1: Different choices of the summation and suppression fields A and B, as well as the constant σ inthe normalizer lead to known normalization schemes in neural networks. d(i, j) denotes an arbitrary distancebetween two hidden units i and j , and R denotes the neighbourhood radius.
Table 2: CIFAR CNN specificationType	Size	Kernel	Strideinput	32 X 32 X 3	-	-conv +relu	32 X 32 X 32	5 X 5 X 3 X 32	1max pool	16 X 16 X 32	3 X 3	2conv +relu	16 X 16 X 32	5 X 5 X 32 X 32	1avg pool	8 X 8 X 32	3 X 3	2conv +relu	8 X 8 X 64	5 X 5 X 32 X 64	1avg pool	4 X 4 X 64	3 X 3	2fully conn. linear	64	-	-fully conn. linear	10 or 100	-	-				Table 3: CIFAR-10/100 experimentsModel	CIFAR-10 Acc.	CIFAR-100 Acc.
Table 3: CIFAR-10/100 experimentsModel	CIFAR-10 Acc.	CIFAR-100 Acc.
Table 4: PTB Word-level language modeling experimentsModel	LSTM	TanH RNN	ReLU RNNBaseline	115.720	149.357 二	147.630BN	123.245	148.052	164.977LN	119.247	154.324	149.128BN*	116.920	129.155	138.947LN*	101.725	129.823	116.609DN*	102.238	123.652	117.868of the neighborhood. Although the hidden states are randomly initialized, this structure will imposelocal competition among the neighbors.
Table 5: Average test results of PSNR and SSIM on Set14 Dataset.
Table 6: Average test results of PSNR and SSIM on BSD200 Dataset.
Table 7: Comparison of standard batch and layer normalation (BN and LN) models, to those with only L1regularizer (+L1), only the σ smoothing term (-s), and with both (*). We also compare divisive normalizationwith both (DN*), versus with only the smoothing term (DN).
Table 8: Average test results of PSNR and SSIM on Set5 Dataset.
