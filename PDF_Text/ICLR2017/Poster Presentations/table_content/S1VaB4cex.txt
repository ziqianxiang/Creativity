Table 1: CIFAR-100/CIFAR-10/SVHN. We compare test error (%) with other leading methods,trained with either no data augmentation, translation/mirroring (+), or more substantial augmentation(++). Our main point of comparison is ResNet. We closely match its benchmark results usingdata augmentation, and outperform it by large margins without data augmentation. Training withdrop-path, we can extract from FractalNet single-column (plain) networks that are highly competitive.
Table 2: ImageNet (validation set, 10-crop).
Table 3: Ultra-deep fractal networks(CIFAR-100++). Increasing depth greatly im-proves accuracy until eventual diminishingreturns. Contrast with plain networks, whichare not trainable if made too deep (Table 4).
Table 4: Fractal structure as a training appara-tus (CIFAR-100++). Plain networks perform well ifmoderately deep, but exhibit worse convergence dur-ing training if instantiated with great depth. How-ever, as a column trained within, and then extractedfrom, a fractal network with mixed drop-path, werecover a plain network that overcomes such depthlimitation (possibly due to a student-teacher effect).
