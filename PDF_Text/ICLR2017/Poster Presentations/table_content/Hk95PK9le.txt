Table 1: Model hyperparametersAside from architectural differences between ours and the other graph-based parsers, we make anumber of hyperparameter choices that allow us to outperform theirs, laid out in Table 1. We use100-dimensional uncased word vectors2 and POS tag vectors; three BiLSTM layers (400 dimensionsin each direction); and 500- and 100-dimensional ReLU MLP layers. We also apply dropout at everystage of the model: we drop words and tags (independently); we drop nodes in the LSTM layers(input and recurrent connections), applying the same dropout mask at every recurrent timestep (cf.
Table 2: Test accuracy and speed on PTB-SD 3.5.0. Statistically significant differences are markedwith an asterisk.
Table 3: Test Accuracy on PTB-SD 3.5.0. Statistically significant differences are marked with anasterisk.
Table 4: Results on the English PTB and Chinese PTB parsing datasetsModel	Catalan		Chinese		Czech		UAS	LAS	UAS	LAS	UAS	LASAndor et al.	92.67	89.83	84.72	80.85	88.94	84.56Deep Biaffine	94.69	92.02	88.90	85.38	92.08	87.38	English		German		Spanish	Model	UAS	LAS	UAS	LAS	UAS	LASAndor et al.	93.22	91.23	90.91	89.15	92.62	89.95Deep Biaffine	95.21	93.20	93.46	91.44	94.34	91.65Table 5: Results on the CoNLL ’09 shared task datasets4.2.4	Embedding DropoutBecause we increase the parser’s power, we also have to increase its regularization. In addition tousing relatively extreme dropout in the recurrent and MLP layers mentioned in Table 1, we alsoregularize the input layer. We drop 33% of words and 33% of tags during training: when one isdropped the other is scaled by a factor of two to compensate, and when both are dropped together,the model simply gets an input of zeros. Models trained with only word or tag dropout but notboth wind up signficantly overfitting, hindering label accuracy and—in the latter case—attachmentaccuracy. Interestingly, not using any tags at all actually results in better performance than usingtags without dropout.
Table 5: Results on the CoNLL ’09 shared task datasets4.2.4	Embedding DropoutBecause we increase the parser’s power, we also have to increase its regularization. In addition tousing relatively extreme dropout in the recurrent and MLP layers mentioned in Table 1, we alsoregularize the input layer. We drop 33% of words and 33% of tags during training: when one isdropped the other is scaled by a factor of two to compensate, and when both are dropped together,the model simply gets an input of zeros. Models trained with only word or tag dropout but notboth wind up signficantly overfitting, hindering label accuracy and—in the latter case—attachmentaccuracy. Interestingly, not using any tags at all actually results in better performance than usingtags without dropout.
