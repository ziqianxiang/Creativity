Table 1: Comparison of the final word level perplexities on the validation and test set for the 4 different models.
Table 2: Performance of the four different small models trained on the equally sized two partitions of Wikitext-2 training set. These results are consistent with those on PTB (see Table 1), which has a similar training set sizewith each of these partitions, although its word embedding dimension is three times smaller.
Table 3: Comparison of our work to previous state of the art on word-level validation and test perplexities onthe Penn Treebank corpus. Models using our framework significantly outperform other models.
Table 4: Prediction for the next word by the baseline (VD-LSTM) and proposed (VD-LSTM +REAL) networksfor a few example phrases in the PTB test set. Top 10 word predictions are sorted in descending probability,and are arranged in column-major format.
