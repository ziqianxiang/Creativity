Table 1: Activation-based attention transfer (AT) with various architectures on CIFAR-10. Error iscomputed as median of5 runs with different seed. F-ActT means full-activation transfer (see §4.1.2).
Table 2: Test error of WRN-16-2/WRN-16-1 teacher/studentpair for various attention map-ping functions. Median of 5 runstest errors are reported.
Table 3: Performance of various gradient-based attention methodson CIFAR-10. Baseline is a thin NIN network with 0.2M parame-ters (trained only on horizontally flipped augmented data and with-out batch normalization), min-l2 refers to using l2 norm of gradientw.r.t. input as regularizer, symmetry norm - to using flip invarianceon gradient attention maps (see eq. 6), AT - to attention transfer,and KD - to Knowledge Distillation (both AT and KD use a wideNIN of 1M parameters as teacher).
Table 4: Finetuning with attention transfer error on Scenes and CUB datasets4.2.1	Transfer learningTo see how attention transfer works in finetuning we choose two datasets: Caltech-UCSD Birds-200-2011 fine-grained classification (CUB) by Wah et al. (2011), and MIT indoor scene classification(Scenes) by Quattoni & Torralba (2009), both containing around 5K images training images. Wetook ResNet-18 and ResNet-34 pretrained on ImageNet and finetuned on both datasets. On CUBwe crop bounding boxes, rescale to 256 in one dimension and then take a random crop. Batch nor-malization layers are fixed for finetuning, and first group of residual blocks is frozen. We then tookfinetuned ResNet-34 networks and used them as teachers for ResNet-18 pretrained on ImageNet,with Fs2um attention losses on 2 last groups. In both cases attention transfer provides significant im-provements, closing the gap between ResNet-18 and ResNet-34 in accuracy. On Scenes AT worksas well as KD, and on CUB AT works much better, which we speculate is due to importance ofintermediate attention for fine-grained recognition. Moreover, after finetuning, student’s attentionmaps indeed look more similar to teacher’s (Fig. 6, Appendix).
Table 5: Attention transfer validation error (single crop) on ImageNet. Transfer losses are added onepoch 60/100.
