Table 1: ROUGE F1 scores on the sentence summarisation test set. The ‘uni’ and ‘bi’ in the paren-theses denote the encoder for the model proposing candidates is a unidirectional LSTM or bidirec-tional LSTM. Those rows marked with an * denote models that process their input online.
Table 2: Overview of results on the abstractive sentence summarisation task. ABS+ (Rush et al.,2015) is the attentive model with bag-of-words as the encoder. RAS-LSTM and RAS-Elman(Chopra et al., 2016) are the sequence to sequence models with attention with the RNN cell im-plemented as LSTMs and an Elman architecture (Elman, 1990), respectively. Pointing the unknownwords (Gulcehre et al., 2016) uses pointer networks (VinyaIs et al., 2015) to select the output to-ken from the input sequence in order to avoid generating unknown tokens. ASC + FSC (Miao &Blunsom, 2016) is the semi-supervised model based on a variational autoencoder.
Table 3: Preference ratings for 641 segments from the test set (each segment had ratings from atleast 2 raters with ≥ 50% agreement on the label and where one label had a plurality of the votes).
Table 4: BLEU scores from different models for the Chinese to English machine translation task.
Table 5: Example outputs on the test set from the direct model and noisy channel model for thesummarisation task and machine translation.
