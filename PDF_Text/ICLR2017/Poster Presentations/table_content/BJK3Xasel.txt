Table 1: Computational cost of efficient implementations of various algorithms, per mini-batch andweight. Operations that do not scale with the number of weights are not included. Operationsassociated with the computation of the gradient of the loss term (e.g. lines 7 and 8 in algorithm 1)as well as unit addition and removal (e.g. lines 18 to 24 in algorithm 1) are not included as they donot vary between algorithms. Algorithm	Network types	Cost per mini-batch and weightSGD, no `2 shrinkage	param., nonparam. SGD with `2 shrinkage	param., nonparam. AdaRad, no `2 shrinkage	param., nonparam. AdaRad with `2 shrinkage	param., nonparam. RMSprop, no `2 shrinkage param. RMSprop with `2 shrinkage param.	1 multiplication 3	multiplications 4	multiplications 4	multiplications 4 multiplications, 1 division, 1 square root 6 multiplications, 1 division, 1 square root3.3	ADAPTIVE RADIAL-ANGULAR GRADIENT DESCENT (AdaRad)The staple method for training neural networks is stochastic gradient descent. Further, there areseveral popular variants: momentum and Nesterov momentum (Sutskever et al., 2013), AdaGrad(Duchi et al., 2011) and AdaDelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012) and Adam(Kingma & Ba, 2015). All of these methods center around two key principles: (1) averaging thegradient obtained over consecutive iterations to smooth out oscillations and (2) normalizing eachcomponent of the gradient so that each weight learns at roughly the same speed. Principle (2) turnsout to be especially important for nonparametric neural networks. When a new unit is added, itdoes not initially contribute to the quality of the output of the network and so does not receive muchgradient from the loss term. If the gradient is not normalized, that unit may take a very long time tolearn anything useful. However, if we use a fan-in regularizer, we cannot normalize the componentsof the gradient outright as in e.g. RMSprop, as we would also have to scale the amount of shrinkageinduced by the regularizer accordingly. This, in turn, would cause the fan-in of new units to becomezero before they can learn anything useful.
Table 2: Test classification error of various models trained on the poker dataset.
Table 3: Hyperparameters and related choices.
