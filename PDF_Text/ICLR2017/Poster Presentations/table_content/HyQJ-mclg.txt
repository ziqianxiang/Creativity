Table 1: Our INQ well converts diverse full-precision deep CNN models (including AlexNet, VGG-16, GoogleNet, ResNet-18 and ResNet-50) to 5-bit low-precision versions with consistently im-proved model accuracy.
Table 2: Comparison of two different strategies for weight partition on ResNet-18.
Table 3: Our INQ generates extremely low-precision (4-bit and 3-bit) models with improved or verysimilar accuracy compared with the full-precision ResNet-18 model.
Table 4: Comparison of our 2-bit ternary model and some other binary or ternary models, includingthe BWN and the TWN approximations of ResNet-18.
Table 5: Comparison of the combination of our INQ and DNS, and deep compression method onAlexNet. Conv: Convolutional layer, FC: Fully connected layer, P: Pruning, Q: Quantization, H:Huffman coding.
Table 6: A statistical distribution of the quantized weights in our 5-bit AlexNet model.
Table 7: Comparison of our VGG-16 model with 5-bit weights and 4-bit activations, and the pre-trained reference with 32-bit floating-point weights and 32-bit float-point activations.
