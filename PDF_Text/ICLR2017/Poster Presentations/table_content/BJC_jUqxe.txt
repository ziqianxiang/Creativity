Table 1: Performance Comparision of Different Models on Yelp and Age DatasetModels	Yelp	AgeBiLSTM + Max Pooling + MLP	61.99%	77.40%CNN + Max Pooling + MLP	62.05%	78.15%Our Model	64.21%	80.45%We train all the three models until convergence and select the corresponding test set performanceaccording to the best development set performance. Our results show that the model outperformsboth of the biLSTM and CNN baselines by a significant margin.
Table 2: Test Set Performance Compared to other Sentence Encoding Based Methods in SNLI DatsetModel	Test Accuracy300D LSlM encoders (BoWman et al., 2016)	806%600D (300+300) BiLSTM encoders (Liu et al., 2016b)	83.3%300D lree-based CNN encoders (Mou et al., 2015a)	82.1%300D SPINN-PI encoders (BoWman et al., 2016)	83.2%300D NlI-SLSlM-LSlM encoders (Munkhdalai & Yu, 2016a)	83.4%1024D GRU encoders With Skiplhoughts pre-training (Vendrov et al., 2015) 81.4%300D NSE encoders (Munkhdalai & Yu, 2016b)	84.6%Ourmethod	844%7Published as a conference paper at ICLR 2017We find that compared to other published approaches, our method shows a significant gain (≥ 1%)to them, except for the 300D NSE encoders, which is the state-of-the-art in this category. However,the 0.2% different is relatively small compared to the differences between other methods.
Table 3: Performance comparision regarding the penalization termPenalization coefficient	Yelp AgeT.0	64.21%^^80.45%0.0	61.74% 79.27%From the figure we can tell that the model trained without the penalization term have lots of redun-dancies between different hops of attention (Figure 3a), resulting in putting lot of focus on the word”it” (Figure 3c), which is not so relevant to the age of the author. However in the right column, themodel shows more variations between different hops, and as a result, the overall embedding focuseson ”mail-replies spam” instead. (Figure 3d)For the Yelp dataset, we also observe a similar phenomenon. To make the experiments more ex-plorative, we choose to plot heat maps of overall attention heat maps for more samples, instead ofplotting detailed heat maps for a single sample again. Figure 4 shows overall focus of the sentenceembedding on three different reviews. We observe that with the penalization term, the model tendsto be more focused on important parts of the review. We think it is because that we are encouragingit to be focused, in the diagonals of matrix AAT (Equation 8).
Table 4: Model Size Comparison Before and After Pruning	Hidden layer	SoftmaX	Other Parts	Total	AccuracyYelp, Original, b=3000	^34M	-T5K	1.3M	55.3M	64.21%Yelp, Pruned, p=150, q=10	2.7M	52.5K	1.3M	4.1M	63.86%Age, Original, b=4000	~1MΛ	^^0K	1.3M	73.2M	80.45%Age, Pruned, p=25, q=20	822K	63.75K	1.3M	2.1M	77.32%SNLI, Original, b=4000	^^2M	^Γ2K	22.9M	95.0M	84.43%SNLI, Pruned, p=300, q=10	5.6M	45K	22.9M	28.6M	83.16%13Published as a conference paper at ICLR 2017r × p = b), this process prunes away (r - 1)/r of weight values, which is a fairly large portion whenr is large.
