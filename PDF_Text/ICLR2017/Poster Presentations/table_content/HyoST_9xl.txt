Table 1: Overview of the neural networks, data sets and performance improvements from DSD.
Table 2: DSD results on GoogLeNetGoogLeNet	Top-1 Err	Top-5 Err	Sparsity	Epochs	LRBaseline	31.14%	10.96%	-0%-	-250-	1e-2Sparse	30.58%	10.58%	30%	11	1e-3DSD	30.02%	10.34%	0%	22	1e-4LLR 一	30.20%	10.41%	0%	33	1e-5Improve (abs)	1.12%	-^062%-	-	-	-Improve (rel)	3.6%	5.7%	-	-	-4.2	VGGNETWe explored DSD training on VGG-16 (Simonyan & Zisserman (2014)), which is widely used indetection, segmentation and transfer learning. The baseline model is obtained from the Caffe ModelZoo (Jia (2013)). Similar to GoogLeNet, each layer is pruned to 30% sparsity. DSD training greatlyreduced the error by 4.31% (Top-1) and 2.65% (Top-5), detailed in Table 3. DSD also wins over theLLR result by a large margin.
Table 3: DSD results on VGG-16VGG-16	Top-1 Err	Top-5 Err	Sparsity	Epochs	LRBaseline	31.50%	11.32%	-0%-	-74-	1e-2Sparse	28.19%	9.23%	30%	1.25	1e-4DSD	27.19%	8.67%	0%	18	1e-5LLR 一	29.33%	10.00%	0%	20	1e-7Improve (abs)	4.31%	-^2.65%-	-	-	-Improve (rel)	13.7%	23.4%	-	-	-4.3	ResNetDeep Residual Networks (ResNets, He et al. (2015)) were the top performer in the 2015 ImageNetchallenge. The baseline ResNet-18 and ResNet-50 models are provided by Facebook (2016). Weprune to 30% sparsity uniformly, and a single DSD pass for these networks reduced top-1 error by1.26% (ResNet-18) and 1.12% (ResNet-50), shown in Table 4. A second DSD iteration can furtherimprove the accuracy. As a fair comparison, we continue train the original model by lowering thelearning rate by another decade, but can’t reach the same accuracy as DSD, as shown in the LLR row.
Table 4: DSD results on ResNet-18 and ResNet-50	ResNet-18		ResNet-50		Sparsity	Epochs	LR	Top-1 Err	Top-5 Err	Top-1 Err	Top-5 Err			Baseline	30.43%	10.76%	24.01%	7.02%	-0%-	90	1e-1Sparse	30.15%	10.56%	23.55%	6.88%	30%	45	1e-2DSD	29.17%	10.13%	22.89%	6.47%	0%	45	1e-3LLR	30.04%	10.49%	23.58%	6.84%	0%	90	1e-5Improve (abs)	-1.26%^^	0.63%	-∏2%^^	0.55%	-	-	-Improve (rel)	4.14%	5.86%	4.66%	7.83%	-	-	-5Published as a conference paper at ICLR 2017O Baseline:	a《Baseline: two X Baseline: a man and XBaseline: a person inbasketball player in dogs are playing a woman are sitting a red jacket is riding aa red uniform is together in a field. on a bench.	bike through theplaying with a ball.	woods.
Table 5: DSD results on NeuralTalkNeuralTalk	BLEU-I	BLEU-2	BLEU-3	BLEU-4	Sparsity	Epochs	LRBaseline	572^^	386^^	254^^	168	0	19	1e-2Sparse	58.4	39.7	26.3	17.5	80%	10	1e-3DSD	59.2	40.7	27.4	18.5	0	6	1e-4Improve(abs)	20	2∏	-~20	17	-	-	-Improve(rel)	3.5%	5.4%	7.9%	10.1%	-	-	-4.4 NeuralTalkWe evaluated DSD training on RNN and LSTM beyond CNN. We applied DSD to NeuralTalk(Karpathy & Fei-Fei (2015)), an LSTM for generating image descriptions. It uses a CNN as an imagefeature extractor and an LSTM to generate captions. To verify DSD training on LSTMs, We fixedthe CNN weights and only train the LSTM weights. The baseline NeuralTalk model We used is theflickr8k_cnn」stm_v1.p downloaded from NeuralTalk Model Zoo.
Table 6: Deep Speech 1 ArchitectureLayer ID	0	1	2		3		4	5Type	Conv	-FC-	-FC	Bidirectional Recurrent	-FC-	CTCCost#Params	1814528	1049600	1049600	3146752	1049600	29725Table 7: DSD results on Deep Speech 1: Word Error Rate (WER)DeepSpeech 1	WSJ ’92	WSJ ’93	Sparsity	Epochs	LRDense Iter 0	29.82	^^34.57	0%	-50-	-8e-4"Sparse Iter 1	27.90	32.99	50%	50	5e-4Dense Iter 1	27.90	32.20	0%	50	3e-4Sparse Iter 2	27.45^^	^^32.99	25%	-50-	1e-4Dense Iter 2	27.45	31.59	0%	50	3e-5Baseline	28.03^^	-^33.55	0%	150	~8e-4-Improve(abs)	-0.58^^	-^196-	-	-	-Improve(rel)	2.07%	5.84%	-	-	-speech. The validation set consists of 1 hour of speech. The test sets are from WSJ’92 and WSJ’93and contain 1 hour of speech combined. The Word Error Rate (WER) reported on the test sets for thebaseline models is different from Amodei et al. (2015) due to two factors. First, in DeepSpeech2,the models were trained using much larger data sets containing approximately 12,000 hours ofmulti-speaker speech data. Secondly, WER was evaluated with beam search and a language model inDeepSpeech2; here the network output is obtained using only max decoding to show improvement in
Table 7: DSD results on Deep Speech 1: Word Error Rate (WER)DeepSpeech 1	WSJ ’92	WSJ ’93	Sparsity	Epochs	LRDense Iter 0	29.82	^^34.57	0%	-50-	-8e-4"Sparse Iter 1	27.90	32.99	50%	50	5e-4Dense Iter 1	27.90	32.20	0%	50	3e-4Sparse Iter 2	27.45^^	^^32.99	25%	-50-	1e-4Dense Iter 2	27.45	31.59	0%	50	3e-5Baseline	28.03^^	-^33.55	0%	150	~8e-4-Improve(abs)	-0.58^^	-^196-	-	-	-Improve(rel)	2.07%	5.84%	-	-	-speech. The validation set consists of 1 hour of speech. The test sets are from WSJ’92 and WSJ’93and contain 1 hour of speech combined. The Word Error Rate (WER) reported on the test sets for thebaseline models is different from Amodei et al. (2015) due to two factors. First, in DeepSpeech2,the models were trained using much larger data sets containing approximately 12,000 hours ofmulti-speaker speech data. Secondly, WER was evaluated with beam search and a language model inDeepSpeech2; here the network output is obtained using only max decoding to show improvement inthe neural network accuracy, and filtering out the other parts.
Table 8: Deep Speech 2 ArchitectureLayer ID	0	1	2	3-8	9	10Type	2DConv	2DConv	-BR-	-BR-	-FC-	CTCCost#Params	19616	239168	8507840	9296320	3101120	95054Table 9: DSD results on Deep Speech 2 (WER)DeepSpeech 2	WSJ ,92	WSJ ,93	Sparsity	Epochs	LRDense Iter 0	-1183^^	^^17.42	0%	-20-	3e-4Sparse Iter 1	10.65	14.84	50%	20	3e-4Dense Iter 1	9.11	13.96	0%	20	3e-5Sparse Iter 2	-894^^	-^14.02	25%	-20-	3e-5Dense Iter 2	9.02	13.44	0%	20	6e-6Baseline	-955^^	-^14.52	0%	60	3e-4Improve(abs)	-053^^	-^108-	-	-	-Improve(rel)	5.55%	7.44%	-	-	-Layers are pruned. The Baseline model is trained for 60 epochs to provide a fair comparison withDSD training. The baseline model shows no improvement after 40 epochs. With one iteration ofDSD training, WER improves by 0.44 (WSJ ’92) and 0.56 (WSJ ’93) compared to the fully trainedbaseline.
Table 9: DSD results on Deep Speech 2 (WER)DeepSpeech 2	WSJ ,92	WSJ ,93	Sparsity	Epochs	LRDense Iter 0	-1183^^	^^17.42	0%	-20-	3e-4Sparse Iter 1	10.65	14.84	50%	20	3e-4Dense Iter 1	9.11	13.96	0%	20	3e-5Sparse Iter 2	-894^^	-^14.02	25%	-20-	3e-5Dense Iter 2	9.02	13.44	0%	20	6e-6Baseline	-955^^	-^14.52	0%	60	3e-4Improve(abs)	-053^^	-^108-	-	-	-Improve(rel)	5.55%	7.44%	-	-	-Layers are pruned. The Baseline model is trained for 60 epochs to provide a fair comparison withDSD training. The baseline model shows no improvement after 40 epochs. With one iteration ofDSD training, WER improves by 0.44 (WSJ ’92) and 0.56 (WSJ ’93) compared to the fully trainedbaseline.
Table 10: Validation of DSD on Cifar10 data using ResNet-20ResNet-20	Avg. Top-1 Err	SD. Top-1 Err	Sparsity	Epochs	LRBaseline	8.26% =	-	0%	164	1e-1Direct Finetune (First half)	8Λ6%	0.08%	-0%-	-45-	1e-3Direct Finetune (Second half)	7.97%	0.04%	0%	45	1e-4DSD (Fist half, Sparse)	8TΓ2%	0.05%	-50%-	-45-	1e-3DSD (Second half, Dense)	7.89%	0.03%	0%	45	1e-4Improve from baseline(abs)	0.37% =	-	-	-	-Improve from baseline(rel)	4.5%	-	-	-	-We used t-test (unpaired) to compare the top-1 testing error rate of the models trained using DSD and conventionalmethods. The results demonstrate the DSD training achieves significant improvements from both the baselinemodel (p<0.001) and conventional fine tuning (p<0.001).
