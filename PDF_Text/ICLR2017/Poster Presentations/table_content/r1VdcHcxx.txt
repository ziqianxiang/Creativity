Table 1: Accuracy obtained on the test set for the pixel by pixel MNIST classification tasksIn Figure 2 we show the validation accuracy while training for both LSTM and batch-normalizedLSTM (BN-LSTM). BN-LSTM converges faster than LSTM on both tasks. Additionally, we ob-serve that BN-LSTM generalizes significantly better on pMNIST. It has been highlighted in Ar-jovsky et al. (2015) that pMNIST contains many longer term dependencies across pixels than inthe original pixel ordering, where a lot of structure is local. A recurrent network therefore needs to5Published as a conference paper at ICLR 2017Model	Penn TreebankLSTM (Graves, 2013)	1.262HF-MRNN (Mikolov et al., 2012)	1.41Norm-stabilized LSTM (Krueger & Memisevic, 2016)	1.39ME n-gram (Mikolov et al., 2012)	1.37LSTM (ours)	1.38BN-LSTM (ours)	1.32Zoneout (Krueger et al., 2016)	1.27HM-LSTM (Chung et al., 2016)	1.24HyperNetworks (Ha et al., 2016)	1.22Table 2: Bits-per-character on the Penn Treebank test sequence.
Table 2: Bits-per-character on the Penn Treebank test sequence.
Table 3: Bits-per-character on the text8 test sequence.
Table 4: Error rates on the CNN question-answering task Hermann et al. (2015).
Table 5: Hyperparameter values that have been explored in the experiments.
