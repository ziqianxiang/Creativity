Table 1: Statically Binarized MNISTModel	NLL TestNormalizing flows (Rezende & Mohamed, 2015)	85.10DRAW (Gregor et al., 2015)	< 80.97Discrete VAE (Rolfe, 2016)	81.01PixelRNN (van den Oord et al., 2016a)	79.20IAF VAE (Kingma et al., 2016)	79.88AF VAE	79.30VLAE	79.03(2016) on statically binarized MNIST and make the single modification of replacing the originalIAF posterior with an equivalent AF prior, removing the context. As seen in Table 1, VAE with AFprior is outperforming VAE with an equivalent IAF posterior, indicating that the deeper generativemodel from AF prior is beneficial. A similar gain carries over when an autoregressive decoder isused: on statically binarized MNIST, using AF prior instead of IAF posterior reduces train NLL by0.8 nat and test NLL by 0.6 nat.
Table 2: Dynamically binarized MNISTModel	NLL TestConvolUtional VAE + HVI (Salimans et al., 2014)	81.94DLGM 2hl + IWAE (Burda et al., 2015a)	82.90Discrete VAE (Rolfe, 2016)	80.04LVAE (Kaae S0nderby et al., 2016)	81.74DRAW + VGP (Tran et al., 2015)	< 79.88IAF VAE (Kingma et al., 2016)	79.10Unconditional Decoder	87.55VLAE	78.53Table 3: OMNIGLOT. [1] (Burda et al., 2015a), Table 4: Caltech-101 Silhouettes. [1] (Born-[2] (Burda et al., 2015b), [3] (Gregor et al., schein & Bengio, 2014), [2] (Cho et al., 2011),2015), [4] (Gregor et al., 2016),	[3] (Du et al., 2015), [4] (Rolfe, 2016), [5](Goessling & Amit, 2015),Model	NLL Test				VAE [1]	106.31	Model	NLL TestIWAE [1]	103.38	RWS SBN [1]	113.3RBM (500 hidden) [2]	100.46	RBM [2]	107.8DRAW [3]	< 96.50	NAIS NADE [3]	100.0Conv DRAW [4]	< 91.00	Discrete VAE [4]	97.6
Table 3: OMNIGLOT. [1] (Burda et al., 2015a), Table 4: Caltech-101 Silhouettes. [1] (Born-[2] (Burda et al., 2015b), [3] (Gregor et al., schein & Bengio, 2014), [2] (Cho et al., 2011),2015), [4] (Gregor et al., 2016),	[3] (Du et al., 2015), [4] (Rolfe, 2016), [5](Goessling & Amit, 2015),Model	NLL Test				VAE [1]	106.31	Model	NLL TestIWAE [1]	103.38	RWS SBN [1]	113.3RBM (500 hidden) [2]	100.46	RBM [2]	107.8DRAW [3]	< 96.50	NAIS NADE [3]	100.0Conv DRAW [4]	< 91.00	Discrete VAE [4]	97.6Unconditional Decoder	95.02	SPARN [5]	88.48VLAE	90.98	Unconditional Decoder	89.26VLAE (fine-tuned)	89.83	VLAE	77.368Published as a conference paper at ICLR 2017As seen in Table 2,3,4, with the same set of hyperparameters tuned on statically binarized MNIST,VLAE is able to perform well on the rest of datasets, significantly exceeding previous state-of-the-art results on dynamically binarized MNIST and Caltech-101 Silhouettes and tying statisticallywith best previous result on OMNIGLOT. In order to isolate the effect of expressive PixelCNN asdecoder, we also report performance of the same PixelCNN trained without VAE part under the
Table 5: CIFAR10. Likelihood for VLAE is approximated with 512 importance samples. [1](van den Oord et al., 2016a), [2] (Dinh et al., 2014), [3] (van den Oord & Schrauwen, 2014), [4](Dinh et al., 2016), [5] (van den Oord et al., 2016b), [6] (Salimans et al., 2017), [7] (Sohl-Dicksteinet al., 2015), [8] (Gregor et al., 2016), [9] (Kingma et al., 2016)Method	bits/dim ≤Results with tractable likelihood models: Uniform distribution [1] Multivariate Gaussian [1] NICE [2] Deep GMMs [3] Real NVP [4] PixelCNN [1] Gated PixelCNN [5] PixelRNN [1] PixelCNN++ [6]	8.00 4.70 4.48 4.00 3.49 3.14 3.03 3.00 2.92Results with VariationaUy trained latent-variable models: Deep Diffusion [7] Convolutional DRAW [8] ResNet VAE with IAF [9] ResNet VLAE DenseNet VLAE	5.40 3.58 3.11 3.04 2.95We also investigate learning lossy codes on CIFAR10 images. To illustrate how does the receptivefield size of PixelCNN decoder influence properties of learned latent codes, we show visualizationsof similar VLAE models with receptive fields of different sizes. Specifically we say a receptive field,xWindowAround(i), has size AxB when a pixel xi can depend on the rectangle block of size AxBimmediately on top of Xi as well as the ∣"a2l1"∣ pixels immediately to the left of xi. We use thisnotation to refer to different types of PixelCNN decoders in Figure 3.
Table 1: Ablation on Dynamically binarized MNISTModel	NLL Test KLUnconditional PixelCNN	87.55	0^^PixelCNN Decoder + Gaussian Prior	79.48	10.60PixelCNN Decoder + AF Prior	78.94	11.73In Table 1, we can observe that each step of modification improves density estimation performance.
