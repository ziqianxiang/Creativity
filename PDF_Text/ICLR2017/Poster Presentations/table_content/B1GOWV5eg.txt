Table 1: Evaluation of FiGAR on MujocoDomainFiGAR-TRPO TRPOAntHopperInverted PendulumInverted Double PendulumSwimmer947.06 (28.35)3038.63 (1.00)1000.00 (1.00)8712.46 (1.01)337.48 (10.51)-161.93 (1.00)3397.58 (1.00)971.66 (1.00)8327.75 (1.00)364.55 (1.00)The full policy (fθa, fθx) is trained jointly. The policies learnt after each TRPO optimization step(details in Appendix C) are compared to current best known policy to arrive at the overall best
Table 2: Comparison of FiGAR-A3C variants to the A3C baseline for 3 games: Sea Quest, SpaceInvaders and Asterix. See Appendix E (Figure 7) for a bar graph visualization of this table.
Table 3: Description of FiGAR-A3C variants in terms of action repetition set W .
Table 4: Game Playing Experiments on Atari 2600NameFiGAR-A3CA3CAlien	3138.50 (2864.91, 3412.08)	2709.20 (2499.41, 2918.98)Amidar	1465.70 (1406.18, 1525.21)	1028.34 (1003.11, 1053.56)Assault	1936.37 (1855.85, 2016.88)	1857.61 (1787.19, 1928.02)Asterix	11949.00 (11095.62, 12802.37)	2364.00 (2188.12, 2539.87)Atlantis	6330600.00 (6330600.00, 6330600.00)	163660.00 (-46665.38, 373985.38)Bank Heist	3364.60 (3342.10, 3387.09)	1731.40 (1727.94, 1734.85)Beam Rider	2348.78 (2152.19, 2545.36)	2189.96 (2062.89, 2317.02)Bowling	30.09 (29.74, 30.43)	16.88 (15.23, 18.52)Breakout	814.50 (789.97, 839.02)	555.05 (474.89, 635.20)Centipede	3340.35 (3071.70, 3608.99)	3293.33 (2973.14, 3613.51)Chopper command	3147.00 (2851.02, 3442.97)	4969.00 (4513.12, 5424.87)Crazy Climber	154177.00 (148042.35, 160311.64)	166875.00 (161560.18, 172189.81)Demon Attack	7499.30 (7127.85, 7870.74)	26742.75 (22665.02, 30820.47)Enduro	707.80 (599.16, 816.43)	0.77 (0.45, 1.09)Freeway	33.14 (33.01, 33.26)	17.68 (17.41, 17.94)Frostbite	309.60 (308.81, 310.38)	306.80 (304.67, 308.92)
Table 5: Game Playing Experiments on Atari 2600 by STRAW [Vezhnevets et al. (2016)]Name	STRAW	STRAW-eAlien	2626	3230Amidar	2223	2022Breakout	344	386Crazy Climber	143803	153327Frostbite	4394	8108Q-bert	20933	23892Figure 4 demonstrates the evolution of the performance of FiGAR-A3C versus training progress.
Table 6: Distribution of Action Repetitions chosen when the policy (both πθa and πθx) is completelystochasticName	1-3	4-6	7-9	10-12	13-15	16-18	19-21	22-24	25-27	28-30Alien	0.33	0.15	0.13	0.11	0.13	0.07	0.03	0.02	0.014	0.01Amidar	0.19	0.14	0.10	0.08	0.08	0.07	0.06	0.08	0.09	0.12Assault	0.29	0.26	0.21	0.11	0.04	0.03	0.02	0.01	0.01	0.01Asterix	0.40	0.25	0.15	0.08	0.04	0.04	0.02	0.02	0.01	0.01Atlantis	0.25	0.16	0.11	0.09	0.08	0.06	0.05	0.07	0.06	0.08Bank Heist	0.950	0.04	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00Beam Rider	0.17	0.16	0.14	0.11	0.09	0.07	0.06	0.05	0.06	0.09Bowling	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.91Breakout	0.28	0.20	0.13	0.09	0.06	0.05	0.04	0.05	0.04	0.07Centipede	0.19	0.27	0.34	0.17	0.03	0.00	0.00	0.00	0.00	0.00Chpr Cmd	0.12	0.14	0.11	0.08	0.11	0.12	0.10	0.08	0.08	0.06Crzy Clmbr	0.34	0.06	0.03	0.51	0.02	0.01	0.01	0.01	0.01	0.01Dmn Attk	0.18	0.21	0.16	0.13	0.10	0.08	0.06	0.04	0.03	0.02Enduro	0.66	0.34	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00Pong	0.16	0.15	0.13	0.10	0.10	0.08	0.08	0.07	0.08	0.07Freeway	0.14	0.12	0.11	0.10	0.09	0.09	0.08	0.08	0.08	0.12Frostbite	0.33	0.16	0.08	0.07	0.05	0.03	0.03	0.02	0.07	0.14
Table 7: Distribution of Action Repetitions chosen when the policy (both πθa and πθx) is 0.1-greedyName	1-3	4-6	7-9	10-12	13-15	16-18	19-21	22-24	25-27	28-30Alien	0.50	0.08	0.11	0.07	0.12	0.07	0.02	0.02	0.01	0.01Amidar	0.49	0.08	0.06	0.04	0.04	0.04	0.04	0.07	0.03	0.11Assault	0.45	0.26	0.15	0.06	0.02	0.02	0.02	0.01	0.01	0.01Asterix	0.50	0.33	0.09	0.04	0.01	0.01	0.01	0.00	0.00	0.00Atlantis	0.51	0.07	0.18	0.08	0.02	0.02	0.01	0.02	0.03	0.07Bank Heist	0.96	0.04	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00Beam Rider	0.34	0.31	0.13	0.04	0.05	0.03	0.01	0.02	0.02	0.06Bowling	0.01	0.91	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01Breakout	0.29	0.23	0.12	0.09	0.05	0.04	0.02	0.03	0.03	0.11Centipede	0.02	0.03	0.94	0.02	0.00	0.00	0.00	0.00	0.00	0.00Chpr Cmd	0.29	0.23	0.12	0.03	0.06	0.09	0.06	0.04	0.06	0.03Crzy Clmbr	0.55	0.04	0.01	0.38	0.01	0.01	0.01	0.00	0.00	0.00Dmn Attk	0.16	0.35	0.14	0.12	0.08	0.05	0.05	0.03	0.01	0.02Enduro	0.91	0.09	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00Freeway	0.15	0.18	0.09	0.09	0.06	0.07	0.05	0.06	0.07	0.18Frostbite	0.47	0.20	0.13	0.01	0.03	0.01	0.01	0.00	0.03	0.11Gopher	0.47	0.19	0.21	0.05	0.04	0.01	0.02	0.01	0.00	0.00James Bond	0.28	0.11	0.22	0.08	0.06	0.06	0.05	0.05	0.03	0.06
Table 8: Average Action Repetition comparison between stochastic and greedy policiesName	Stochastic 0.1 -GreedyAlien	8.43	6.87Amidar	13.77	9.61Assault	7.14	5.86Asterix	6.53	4.22Atlantis	11.68	7.20Bank Heist	1.65	1.62Beam Rider	12.47	7.68Bowling	28.64	5.13Breakout	10.14	9.93Centipede	6.84	7.88Chopper Command	13.76	9.58Crazy Climber	8.00	5.74Enduro	2.91	2.69Demon Attack	10.23	8.59Freeway	14.62	14.25Frostbite	11.33	7.69Gopher	6.68	5.33James Bond	14.98	10.37
Table 9: Gameplay performance of FiGAR compared with FiGAR-wo-πθxName	FiGAR	FiGAR-Wo-∏θxAlien	3138.50	582.17Amidar	1465.70	497.90Assault	1936.37	1551.40Asterix	11949.00	780.00Atlantis	6330600.00	680890.00Bank Heist	3364.60	223.00Beam Rider	2348.78	3732.00Bowling	30.09	0.90Breakout	814.50	321.90Centipede	3340.35	3934.90Chopper Command	3147.00	2730.00Crazy Climber	154177.00	210.00EndUro	707.80	941.10Demon Attack	7499.30	6661.00Freeway	33.14	30.60Frostbite	309.60	308.00Gopher	12845.40	10738.00James Bond	478.0	320.00
Table 10: Evaluation of FiGAR with shared representations for fθa and fθx on MujocoDomain	FiGAR-TRPO FiGAR-shared-TRPO TRPOAnt	947.06 (28.35)	1779.72 (7.99)	-161.93 (1.00)Hopper	3038.63 (1.00)	2649.09 (2.07)	3397.58 (1.00)Inverted Pendulum	1000.00 (1.00)	986.35(1.00)	971.66 (1.00)Inverted Double Pendulum	8712.46(1.01)	9138.85 (1.00)	8327.75 (1.00)Swimmer	337.48 (10.51)	340.74 (8.02)	364.55 (1.00)FiGAR-shared-TRPO on the whole does not perform much better than FiGAR-TRPO. In theseTRPO experiments, the neural networks we used were rather shallow at only two hidden layersdeep. Hence, we believe that sharing of layers thus leads to only small gains in terms of optimalityof policy learnt.
