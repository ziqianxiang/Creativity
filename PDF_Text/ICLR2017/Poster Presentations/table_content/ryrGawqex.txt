Table 1: Inference timing benchmark; times are wall-clock averages in secondsbatch-size	manual		dynamic		full dynamic		cost ratio	speedup ratio	batch	tree	batch	tree	batch	tree		(CPU) 1024	14.62	0.014	18.68	0.018	18.37	0.017	1.27	-28.86-512	7.54	0.014	9.84	0.019	9.57	0.018	1.30	27.68256	4.14	0.016	5.22	0.020	5.25	0.020	1.26	25.23128	2.48	0.019	2.95	0.023	3.08	0.024	1.18	21.4764	1.64	0.025	1.76	0.027	1.78	0.027	1.06	18.5532	1.27	0.039	1.05	0.032	1.10	0.034	0.82	14.941	0.52	0.517	0.26	0.258	0.26	0.262	0.49	1.97(GPU) 1024	0.978	0.0009	1.590	0.0015	1.617	0.0015	1.62	101.79512	0.530	0.0010	0.715	0.0013	0.721	0.0014	1.34	114.15256	0.312	0.0012	0.323	0.0012	0.340	0.0013	1.03	120.86128	0.236	0.0018	0.164	0.0012	0.178	0.0013	0.69	115.0564	0.193	0.0030	0.093	0.0014	0.106	0.0016	0.48	96.4032	0.153	0.0047	0.061	0.0019	0.074	0.0023	0.40	68.79	1	0.161	0.1608	0.038	0.0376	0.036	0.0359	0.23	4.47However, the extra concat and gather ops that dynamic batching inserts do have a cost. The “costratio” column above shows the ratio between dynamic and manual batching, in the case where alltrees in the batch have the same shape. The cost is only 20% for inference on GPUs with batch-size
Table 2: Test set accuracies on the Stanford Sentiment Treebankmodel	fine-grained	binaryTaietal. (2015) Munkhdalai & Yu (2016a) MUnkhdaIai & YU (2016b)	51.0 (0.5)- 52.8 53.1	88.0 (0.3) 89.7 89.3Ours (Single Model) Ours (Ensemble)	52.3(0.7)- 53.6	89.4 (0.4) 90.2Table 3: Lines of code comparisonmodel	ours	original	ratioFeed-Forward Attention	26	^71	0.37Tree-LSTM	119	219	0.54Graph Convolutions	32	44	0.73recurrent dropout and L2 weight regularization. We eliminated weight regularization in favor of therecurrent dropout scheme introduced by Semeniuta et al. (2016) and increased the LSTM state sizefrom 150 to 300, leaving all other hyperparameters unchanged.
Table 3: Lines of code comparisonmodel	ours	original	ratioFeed-Forward Attention	26	^71	0.37Tree-LSTM	119	219	0.54Graph Convolutions	32	44	0.73recurrent dropout and L2 weight regularization. We eliminated weight regularization in favor of therecurrent dropout scheme introduced by Semeniuta et al. (2016) and increased the LSTM state sizefrom 150 to 300, leaving all other hyperparameters unchanged.
