Table 1: One Nearest Neighbors (1NN) classification accuracy (%) on the permutation-invariantMNIST (LeCun et al., 1998) test set in the feature space learned by BiGAN, Latent Regressor (LR),Joint Latent Regressor (JLR), and an autoencoder (AE) using an '1 or '2 distance.
Table 2: Classification accuracy (%) for the ImageNet LSVRC (Russakovsky et al., 2015) validationset with various portions of the network frozen, or reinitialized and trained from scratch, followingthe evaluation from Noroozi & Favaro (2016). In, e.g., the conv3 column, the first three layers-conv1 through conv3 - are transferred and frozen, and the last layers - conv4, conv5, and fullyconnected layers - are reinitialized and trained fully supervised for ImageNet classification. BiGAN iscompetitive with these contemporary visual feature learning methods, despite its generality. (*Resultsfrom Noroozi & Favaro (2016) are not directly comparable to those of the other methods as a differentbase convnet architecture with larger intermediate feature maps is used.)the BiGAN encoder E and generator G learn approximate inverse mappings, as shown theoreticallyin Theorem 2. In Appendix C.2, we present nearest neighbors in the BiGAN learned feature space.
Table 3: Classification and Fast R-CNN (Girshick, 2015) detection results for the PASCAL VOC2007 (Everingham et al., 2014) test set, and FCN (Long et al., 2015) segmentation results on thePASCAL VOC 2012 validation set, under the standard mean average precision (mAP) or meanintersection over union (mIU) metrics for each task. Classification models are trained with variousportions of the AlexNet (Krizhevsky et al., 2012) model frozen. In the fc8 column, only the linearclassifier (a multinomial logistic regression) is learned - in the case of BiGAN, on top of randomlyinitialized fully connected (FC) layers fc6 and fc7. In the fc6-8 column, all three FC layers are trainedfully supervised with all convolution layers frozen. Finally, in the all column, the entire network is“fine-tuned”. BiGAN outperforms other unsupervised (unsup.) feature learning approaches, includingthe GAN-based baselines described in Section 4.1, and despite its generality, is competitive withcontemporary self-supervised (self-sup.) feature learning approaches specific to the visual domain.
