Table 1: Experimental results (Pearson’s r × 100) on textual similarity tasks. The highest score ineach row is in boldface. The methods can be supervised (denoted as Su.), semi-supervised (Se.),or unsupervised (Un.). “GloVe+WR” stands for the sentence embeddings obtained by applying ourmethod to the GloVe word vectors; “PSL+WR” is for PSL word vectors. See the main text for thedescription of the methods.
Table 2: Results on similarity, entailment, and sentiment tasks. The sentence embeddings are com-puted unsupervisedly, and then used as features in downstream supervised tasks. The row for sim-ilarity (SICK) shows Pearson’s r × 100 and the last two rows show accuracy. The highest score ineach row is in boldface. Results in Column 2 to 6 are collected from (Wieting et al., 2016), andthose in Column 7 for skip-thought are from (Lei Ba et al., 2016).
Table 3: Comparison of results on the original datasets and the ones with words randomly shuffledin sentences. The rows with label “original” are the results on the original datasets, and those withlabel “random” are the results on the randomly shuffled datasets. The row for similarity (SICK)shows Pearson’s r × 100 and the other rows show accuracy.
Table 4: The STS tasks by year. Note that tasks with the same name in different years are actuallydifferent tasks.
Table 5: Experimental results (Pearson’s r × 100) on textual similarity tasks. The highest score ineach row is in boldface. The methods can be supervised (denoted as Su.), semi-supervised (Se.),or unsupervised (Un.). “GloVe+WR” stands for the sentence embeddings obtained by applying ourmethod to the GloVe word vectors; “PSL+WR” is for PSL word vectors. See the main text for thedescription of the methods.
Table 6: Experimental results (Pearson’s r × 100) on textual similarity tasks using only smoothinverse frequency, using only common component removal, or using both.
Table 7: Accuracy in 3-class classification on the SNLI dataset for each model. The results inthe first three rows are collected from (Bowman et al., 2015). All methods used 100 dimensionalsentence embeddings.
Table 8: Accuracy in sentiment analysis on the IMDB dataset for NB-SVM (Wang & Manning,2012) and our method.
