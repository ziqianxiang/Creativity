Table 1: Signed log-space semifield (from Li & Eis-ner (2009)). Each real number a is represented as a pair(la , sa) where la = log |a| and sa = sign(a). Thereforea = sa exp(la). For the above we let d = exp(lb - la) andassume |a| > |b|.
Table 2: Performance (average length to fail-ure %) of models on the tree-transduction task.
Table 3: Translation performance as mea-sured by BLEU (higher is better) on character-to-word and word-to-word Japanese-Englishtranslation for the three different models.
Table 4: Answer accuracy (Ans %) and supporting fact selection accuracy (Fact %) of the three QA modelson the 1K bAbI dataset. K indicates the number of hops/inference steps used for each task. Task 7 and 8 bothcontain variable number of facts and hence they are excluded from the fact accuracy measurement. Supportingfact selection accuracy is calculated by taking the average of 10 best runs (out of 20) for each task.
Table 5: Results of our models (bottom) and others (top) on the Stanford NLI test set. Our baseline model hasthe same architecture as Parikh et al. (2016) but the performance is slightly different due to different settings(e.g. we train for 100 epochs with a batch size of 32 while Parikh et al. (2016) train for 400 epochs with a batchsize of 4 using asynchronous SGD.)Despite being trained without ever being exposed to an explicit parse tree, the syntactic attentionlayer learns an almost plausible dependency structure. In the above example it is able to correctlyidentify the main verb fighting, but makes mistakes on determiners (e.g. head of The should bemen). We generally observed this pattern across sentences, possibly because the verb structure ismore important for the inference task.
