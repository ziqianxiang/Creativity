Table 1: Validation and test results of different models on the three language modelling tasks. Resultsare reported for the best-performing settings. Performance on Char-PTB and Text8 is measured in bits-per-character (BPC); Word-PTB is measured in perplexity. For Char-PTB and Text8 all models are1-layer unless otherwise noted; for Word-PTB all models are 2-layer. Results above the line are fromour own implementation and experiments. Models below the line are: NR-dropout (non-recurrentdropout), V-Dropout (variational dropout), RBN (recurrent batchnorm), H-LSTM+LN (HyperLSTM+ LayerNorm), 3-HM-LSTM+LN (3-layer Hierarchical Multiscale LSTM + LayerNorm).
Table 2: Error rates on the pMNIST digit classification task. Zoneout outperforms recurrent dropout,and sets state of the art when combined with recurrent batch normalization.
