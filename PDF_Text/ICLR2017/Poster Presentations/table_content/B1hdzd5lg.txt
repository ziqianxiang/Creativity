Table 1: Performance on the Twitter dataset. “word” and “char” means using word-level and character-level representations respectively.			Model	Precision@1	Recall@10	Mean Rankword (Dhingra et al., 2016b)	0.241	0.428	133char (Dhingra et al., 2016b)	0.284	0.485	104word char concat	0.2961	0.4959	105.8word char feat concat	0.2951	0.4974	106.2scalar gate	0.2974	0.4982	104.2fine-grained gate	0.3069	0.5119	101.5Table 2: Performance on the CBT dataset. The “GA word char concat” results are extracted from Dhingraet al. (2016a). Our results on fine-grained gating are based on a single model. “CN” and “NE” are two widelyused question categories. “dev” means development set, and “test” means test set.
Table 2: Performance on the CBT dataset. The “GA word char concat” results are extracted from Dhingraet al. (2016a). Our results on fine-grained gating are based on a single model. “CN” and “NE” are two widelyused question categories. “dev” means development set, and “test” means test set.
Table 3: Performance on the Who Did What dataset. “dev” means development set, and “test” means test set.
Table 4: Performance on the SQuAD dev set. Test set results are included in the brackets.
Table 5: Word tokens with highest and lowest gate values. High gate values favor character-level representa-tions, and low gate values favor word-level representations.
