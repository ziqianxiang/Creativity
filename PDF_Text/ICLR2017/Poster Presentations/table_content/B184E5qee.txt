Table 1: Test perplexity on the Penn Tree Bank.
Table 2: Test perplexity on the wikitext datasets. The two datasets share the same validation andtest sets, making all the results comparable.
Table 3: Perplexity on the text8 and lambada datasets. WB5 stands for 5-gram language modelwith Witten-Bell smoothing.
