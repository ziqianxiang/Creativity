title,year,conference
 Generating natural language adversarial examples,2018, In Proceedings of the 2018Conference on Empirical Methods in Natural Language Processing
 Synthetic and natural noise both break neural machine trans-lation,2017, CoRR
 A large anno-tated corpus for learning natural language inference,2015, In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing
 Piecewiselinear neural network verification: a comparative study,2017, arXiv preprint arXiv:1711
 Maximum resilience of artificial neuralnetworks,2017, In International Symposium on Automated Technology for Verification and Analysis
 Training verified learners with learned ver-ifiers,2018, arXiv preprint arXiv:1805
 Adual approach to scalable verification of deep networks,2018, arXiv preprint arXiv:1803
 Hotflip: White-box adversarial exam-ples for text classification,2018, In Proceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers)
 On the effectiveness of intervalbound propagation for training verifiably robust models,2018, CoRR
 Achieving verified robustness to symbol substitutionsvia interval bound propagation,2019, arXiv preprint arXiv:1909
 Excessive invariancecauses adversarial vulnerability,2019, In International Conference on Learning Representations
 Certified robustness to adversarialword substitutions,2019, arXiv preprint arXiv:1909
 Reluplex: Anefficient smt solver for verifying deep neural networks,2017, In International Conference on ComputerAided Verification
 Adversarially regularising neural nli models to integratelogical background knowledge,2018, In Proceedings of the 22nd Conference on Computational NaturalLanguage Learning
 Differentiable abstract interpretation for prov-ably robust neural networks,2018, In Proceedings of the 35th International Conference on MachineLearning
 Adversarial over-sensitivity and over-stability strategies for dialoguemodels,1047, In Proceedings of the 22nd Conference on Computational Natural Language Learning
 Verification of non-linear specifications for neural networks,2019, CoRR
 Certified defenses against adversarial exam-ples,2018, arXiv preprint arXiv:1801
 ”Why Should I Trust You?”: Explainingthe predictions of any classifier,2016, In Knowledge Discovery and Data Mining (KDD)
 Anchors: High-precision model-agnosticexplanations,2018, In AAAI Conference on Artificial Intelligence (AAAI)
 Semantically equivalent adversarial rulesfor debugging nlp models,2018, In Proceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Knowing when to stop: Evaluation and verification of conformity to output-size specifications,2019, arXiv preprint arXiv:1904
 Formal security analysisof neural networks using symbolic intervals,2018, arXiv preprint arXiv:1804
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In International Conference on Machine Learning
