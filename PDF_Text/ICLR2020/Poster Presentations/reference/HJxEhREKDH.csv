title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Learning polynomials withneural networks,2014, In International Conference on Machine Learning
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In 35th International Conference on Machine Learning
 A convergence analysis of gradient de-scent for deep linear neural networks,2019, In International Conference on Learning Representations
 On ex-act computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Gradient descent with identity initializa-tion efficiently learns positive-definite linear transformations by deep residual networks,2019, Neuralcomputation
 SGD learns over-parameterized networks that provably generalize on linearly separable data,2018, In InternationalConference on Learning Representations
 Width provably matters in optimization for deep linear neural networks,2019, InInternational Conference on Machine Learning
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Algorithm-dependent generalization bounds for over-parameterized deep residual networks,2019, In Advances in Neural Information Processing Systems
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Provable benefit of orthogonal initialization in op-timizing deep linear networks,2020, In International Conference on Learning Representations
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Gradient descent aligns the layers of deep linear networks,2019, In ICLR
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Gradient descent finds global minima for generalizable deepneural networks of practical sizes,2019, arXiv preprint arXiv:1908
 Learning overparameterized neural networks via stochastic gradi-ent descent on structured data,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Convergence analysis of two-layer neural networks with ReLU acti-vation,2017, In Proceedings of the 31st International Conference on Neural Information ProcessingSystems
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 Towards moderate overparameterization: global conver-gence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
 Exponential convergence time of gradient descent for one-dimensional deep linearneural networks,2018, arXiv preprint arXiv:1809
 On learning over-parameterized neural networks: A functional approxi-mation prospective,2019, arXiv preprint arXiv:1905
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
 Global convergence of gradient descent for deep linearresidual networks,2019, arXiv preprint arXiv:1911
 Training over-parameterized deep resnet isalmost as easy as training a two-layer network,2019, arXiv preprint arXiv:1903
 Learning one-hidden-layer ReLUnetworks via gradient descent,2018, arXiv preprint arXiv:1806
 Critical points of linear neural networks: Analytical forms and land-scape properties,2018, 2018
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In Advances in Neural Information Processing Systems
