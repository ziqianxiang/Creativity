title,year,conference
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Weight normalization based quantization for deep neural networkcompression,2019, arXiv preprint arXiv:1907
 Deep learning with low precisionby half-wave gaussian quantization,2017, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Pact: Parameterized clipping activation for quantized neuralnetworks,2018, arXiv preprint arXiv:1805
 Learned step size quantization,2020, In International Conference on LearningRepresentations
 What every computer scientist should know about floating-point arithmetic,1991, ACMComputing Surveys (CSUR)
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, In Advances in Neural Information Processing Systems
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Learning to quantize deep networks by optimizing quantizationintervals with task loss,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Towards accurate binary convolutional neural network,2017, InAdvances in Neural Information Processing Systems
 Convolutional neural networks usinglogarithmic data representation,2016, arXiv preprint arXiv:1603
 Model compression via distillation and quanti-zation,2018, arXiv preprint arXiv:1802
 Weight standardization,2019, arXivpreprint arXiv:1903
 Lq-nets: Learned quantization forhighly accurate and compact deep neural networks,2018, In Proceedings of the European Conferenceon Computer Vision (ECCV)
 Improving neural networkquantization without retraining using outlier channel splitting,2019, In International Conference onMachine Learning
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
