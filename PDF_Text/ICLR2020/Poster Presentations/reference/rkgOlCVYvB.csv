title,year,conference
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Neural networks and principal component analysis: Learning fromexamples without local minima,0893, Neural Networks
 On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport,2018, arXiv:1805
 TheLoss Surfaces of Multilayer Networks,2015, In Proceedings of the Eighteenth International Conferenceon Artificial Intelligence and Statistics
 The Eu-clidean distance degree of an algebraic variety,2013, arXiv:1309
 Identity Matters in Deep Learning,2016, arXiv:1611
 Learning algebraic models of quantum entanglement,2019, arXivpreprint arXiv:1908
 Deep Learning without Poor Local Minima,2016, CoRR
 Deep linear neural networks with arbitrary loss: All localminima are global,2017, arXiv:1712
 Depth Creates No Bad Local Minima,2017, arXiv:1702
 A Mean Field View of the Landscape ofTwo-Layers Neural Networks,2018, arXiv:1804
 Exact Solutions in StructuredLow-Rank Approximation,2013, arXiv:1311
 Spurious valleys in two-layer neural networkoptimization landscapes,2018, arXiv preprint arXiv:1802
 Small nonlinearities in activation functions create badlocal minima in neural networks,2018, arXiv preprint arXiv:1802
 Depth creates no more spurious local minima,2019, arXiv preprint arXiv:1901
 Critical Points of Neural Networks: Analytical Forms and LandscapeProperties,2017, arXiv:1710
 A detailed presentation can be found in Draisma et al,2020, (2013)
 Let Vi+ ⊆ V2+ ⊆ ,2020, 
