title,year,conference
 Synthesizing robust adversarialexamples,2017, arXiv preprint arXiv:1707
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Thermometer encoding: One hotway to resist adversarial examples,2018, 2018
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 On evaluating adversarial robustness,2019, arXivpreprint arXiv:1902
 Certified adversarial robustness via randomizedsmoothing,2019, arXiv preprint arXiv:1902
 Dawnbench: An end-to-end deep learning bench-mark and competition,2017, Training
 Provable robustness of relu net-works via maximization of linear regions,2018, arXiv preprint arXiv:1810
 Boost-ing adversarial attacks with momentum,2018, In Proceedings of the IEEE conference on computervision and pattern recognition
 Evaluating and understanding the robustnessof adversarial logit pairing,2018, arXiv preprint arXiv:1807
 Detecting adversarialsamples from artifacts,2017, arXiv preprint arXiv:1703
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 On the effectiveness of interval bound propagation fortraining verifiably robust models,2018, arXiv preprint arXiv:1810
 Countering adversarialimages using input transformations,2017, arXiv preprint arXiv:1711
 Adversarial logit pairing,2018, arXiv preprintarXiv:1803
 Reluplex: Anefficient smt solver for verifying deep neural networks,2017, In International Conference on ComputerAided Verification
 No need to worry about adversarialexamples in object detection in autonomous vehicles,2017, arXiv preprint arXiv:1707
 Adversarial robustness against the union of multipleperturbation models,2019, arXiv preprint arXiv:1909
 On detecting adversarialperturbations,2017, arXiv preprint arXiv:1702
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Differentiable abstract interpretation for prov-ably robust neural networks,2018, In International Conference on Machine Learning
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, arXivpreprint arXiv:1906
 Certifying some distributional robustness withprincipled adversarial training,2017, arXiv preprint arXiv:1710
 Super-convergence: Very fast training of residual networksusing large learning rates,2018, 2018
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Attacks meet interpretability:Attribute-steered detection of adversarial samples,2018, In Advances in Neural Information ProcessingSystems
 Evaluating robustness of neural networks with mixedinteger programming,2017, arXiv preprint arXiv:1711
 Ensemble adversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 Bilateral adversarial training: Towards fast training of more robust models againstadversarial attacks,2018, arXiv preprint arXiv:1811
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2017, arXiv preprint arXiv:1711
 Scaling provable adversarialdefenses,2018, In Advances in Neural Information Processing Systems
 Training for fasteradversarial robustness verification via inducing relu stability,2018, arXiv preprint arXiv:1809
 Me-net: Towards effective adversarial robustnesswith matrix estimation,2019, arXiv preprint arXiv:1905
 You only propagateonce: Painless adversarial training using maximal principle,2019, arXiv preprint arXiv:1905
