title,year,conference
 Adabatch: Adaptive batch sizes fortraining deep neural networks,2017, CoRR
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Stochastic gradient methods with layer-wise adaptive moments for training of deep networks,2019, arXiv preprint arXiv:1905
 On the computational inefficiency of large batchsizes for stochastic gradient descent,2018, arXiv preprint arXiv:1811
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Visualizing the loss land-scape of neural nets,2018, In Advances in Neural Information Processing Systems
 Asimple baseline for bayesian uncertainty in deep learning,2019, arXiv preprint arXiv:1902
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 Horovod: fast and easy distributed deep learning in tensor-flow,2018, arXiv preprint arXiv:1802
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Local sgd converges fast and communicates little,2018, arXiv preprintarXiv:1805
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
