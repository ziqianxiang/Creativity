title,year,conference
 Natural gradient works efficiently in learning,1998, Neural Computation
 Measuring and regularizing networks infunction space,2019, In International Conference on Learning Representations
 Understanding batch normal-ization,2018, In Advances in Neural Information Processing Systems
 Unidimensional and Evolution Methods for Optimal Transportation,2013, PhD thesis
 Improving sequence-to-sequence learning viaoptimal transport,2019, In International Conference on Learning Representations
 Reducing overfit-ting in deep networks by decorrelating representations,2016, In International Conference on LearningRepresentations
 Recurrentbatch normalization,2017, In International Conference on Learning Representations
 Mode normalization,2019, In International Conference onLearning Representations
 Learn-ing with a Wasserstein loss,2015, In Advances in Neural Information Processing Systems
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Advances in Neural Information Processing Systems
 Convolutionalsequence to sequence learning,2017, In International Conference on Machine Learning
 Dropblock: A regularization method for convolu-tional networks,2018, In Advances in Neural Information Processing Systems
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Artificial Intelligence and Statistics
 Im-proved training of Wasserstein gans,2017, In Advances in Neural Information Processing Systems
 Delving deep into rectifiers: Surpass-ing human-level performance on imagenet classification,2015, In IEEE International Conference onComputer Vision
 Deep residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Norm matters: Efficient and accuratenormalization schemes in deep networks,2018, In Advances in Neural Information Processing Systems
 Decorrelated batch normalization,2018, In IEEEConference on Computer Vision and Pattern Recognition
 Robust estimation of a location parameter,1964, The Annals of Mathematical Statistics
 Tying word vectors and word classifiers: Aloss framework for language modeling,2017, In International Conference on Learning Representations
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Training faster by separating modes of variation in batch-normalized models,2019, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Towards a theoretical understanding of batch normalization,2018, arXiv preprintarXiv:1805
 Sliced Wassersteinauto-encoders,2019, In International Conference on Learning Representations
 Efficient backprop,1998, InNeural Networks: Tricks of the Trade
 Layer normalization,2016, arXiv preprintarXiv:1607
 Streaming normalization: Towards simplerand more biologically-plausible normalizations for online and recurrent learning,2016, arXiv preprintarXiv:1610
 Regularizing by the variance of the activationsâ€™ sample-variances,2018, InAdvances in Neural Information Processing Systems
 Revisiting activation regularization for lan-guage rnns,2017, In International Conference on Machine Learning
 Pointer sentinel mixturemodels,2017, In International Conference on Learning Representations
 RecUr-rent neural network based language model,2010, In Annual Conference of the International SpeechCommunication Association
 All yoU need is a good init,2016, In International Conference on LearningRepresentations
 Using the oUtpUt embedding to improve langUage models,2016, arXiv preprintarXiv:1608
 Wasserstein barycenter and its applica-tion to textUre mixing,2011, In International Conference on Scale Space and Variational Methods inComputer Vision
 Deep learning made easier by linear transformationsin perceptrons,2012, In Artificial Intelligence and Statistics
 TopmoUmoUte online natUral gra-dient algorithm,2008, In Advances in Neural Information Processing Systems
 Optimal transport for applied mathematicians,2015, Birkauser
 Exact solUtions to the nonlinear dynam-ics of learning in deep linear neUral networks,2013, arXiv preprint arXiv:1312
 Accelerated gradient descent by factor-centering decomposition,1998, Technicalreport
 RegUlarization of neUralnetworks Using dropconnect,2013, In International Conference on Machine Learning
 Mean-normalized stochasticgradient for large-scale deep learning,2014, In IEEE International Conference on Acoustics
 Group normalization,2018, In European Conference on Computer Vision
 Regularizing deep convolutionalneural networks with a structured decorrelation constraint,2016, In IEEE International Conference onData Mining
 Fixup initialization: Residual learning withoutnormalization,2019, In International Conference on Learning Representations
 Policy optimization as Wasser-stein gradient flows,2018, In International Conference on Machine Learning
