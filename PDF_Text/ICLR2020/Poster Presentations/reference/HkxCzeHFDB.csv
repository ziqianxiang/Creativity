title,year,conference
 Kernels for vector-valued functions:A review,1935, Found
 Streaming Sparse Gaussian Process Approx-imations,2017, In I
 Gaussian processes: iterative sparse approximations,2002, PhD thesis
 Sparse online Gaussian processes,2002, Neural Computation
 Scalable Inference for Gaussian Process Models with Black-Box Likelihoods,2015, In C
 Laplace propagation,2004, In Advances in neuralinformation processing systems
 Towards Robust Evaluations of Continual Learning,2018, arXiv preprintarXiv:1805
 An empirical investi-gation of catastrophic forgetting in gradient-based neural networks,2013, arXiv preprint arXiv:1312
 Gaussian processes for Big Data,2013, In Conferenceon Uncertainty in Artificial Intellegence
 Scalable VariationalGaussian Process Classification,2015, In Proceedings of the Eighteenth International Conference onArtificial Intelligence and Statistics
 Bayesian surprise attracts human attention,2006, In Advances in neuralinformation processing systems
 One shot learning ofsimple visual concepts,2011, In Proceedings of the Annual Meeting of the Cognitive Science Society
 Learning without forgetting,2017, IEEE Transactions on Pattern Analysisand Machine Intelligence
 Gradient episodic memory for continual learning,2017, In Advances in NeuralInformation Processing Systems
 icarl:Incremental classifier and representation learning,2017, In 2017 IEEE Conference on Computer Visionand Pattern Recognition (CVPR)
 Continual learning in reinforcement environments,1994, PhD thesis
 Catastrophic forgetting and the pseudorehearsal solution inhopfield-type networks,1998, Connection Science
 Experiencereplay for continual learning,2018, arXiv preprint arXiv:1811
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Sparse variational inference for generalized gpmodels,2015, In Francis Bach and David Blei (eds
 Variational learning of inducing variables in sparse Gaussian processes,2009, InInternational Conference on Artificial Intelligence and Statistics
 Matching networks for one shotlearning,2016, In Advances in Neural Information Processing Systems
 Using the nystrom method to speed up kernelmachines,2001, In T
 filters Conv,2000, Kernel size Conv
001 for VCL and 0,2000,0001 for FRCL)
