title,year,conference
 Learning activation functions to improvedeep neural networks,2015, In Workshop Track Proceedings of the International Conference on LearningRepresentations
 Understanding deep neural networks with rectifiedlinear units,2018, In International Conference on Learning Representations
 Continuously differentiable exponential linear units,2017, arXiv preprint arXiv:1704
 Pad6 approximations,1994, Handbook ofnumerical analysis
 Rational neural networks for approximating graphconvolution operator on jump discontinuities,2018, In 2018 IEEE International Conference on DataMining (ICDM)
 Provable robustness of relu networks via maximizationof linear regions,2019, In The 22nd International Conference on Artificial Intelligence and Statistics(AISTATS)
 Maxout networks,2013, InProceedings ofthe 30th International Conference on Machine Learning (ICML)
 Learning activation functions: A new paradigm of understandingneural networks,2019, arXiv preprint arXiv:1906
 Hardware implementation of hyperbolic tangent and sigmoid activation functions,2018, Bulletinof the Polish Academy of Sciences
 Delving deep into rectifiers: Surpassing human-level per-formance on imagenet classification,2015, In Proceedings of the IEEE international conference oncomputer vision
 Deep residual learning for image recognition,2016, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Multilayer feedforward networks are universalapproximators,1989, Neural Networks
 Densely connected convolutionalnetworks,2017, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Deep learning with s-shaped rectified linearactivation units,2016, In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence
 Universal approximation with deep narrow networks,2019, arXiv preprintarXiv:1905
 Adam: A method for stochastic optimization,2015, In 3rd International Conferenceon Learning Representations
 Convolutional deep belief networks on cifar-10,2010, 2010
 Learning multiple layers of features from tiny images,2009, Technicalreport
 Gradient-based learning applied to documentrecognition,1998, ProceedingsofIEEE
 Multilayer feedforward networks with anonpolynomial activation function can approximate any function,1993, Neural Networks
 Rectifier nonlinearities improve neural network acousticmodels,2013, In in ICML Workshop on Deep Learning for Audio
 Learning combinations of activation functions,2018, In 2018 24th InternationalConference on Pattern Recognition (ICPR)
 Scalable parallel programming,2008, In 2008 IEEE Hot Chips 20Symposium (HCS)
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 Searching for activation functions,2018, In Proceedings of theWorkshop Track of the 6th International Conference on Learning Representations (ICLR)
 Mobilenetv2: Inverted residualsand linear bottlenecks,2018, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Rate-coded restricted boltzmann machines for face recognition,2001, InPrcoeedings of Neural Information Processing Systems (NIPS)
 Neural networks and rational functions,2017, In Proceedings of the 34th InternationalConference on Machine Learning (ICML)
 Approximation Theory and Approximation Practice,2012, SIAM
 Hyperactivations for activation function exploration,2017, In 31stConference on Neural Information Processing Systems (NIPS 2017)
 Fashion-mnist: a novel image dataset for benchmarking machinelearning algorithms,2017, CoRR
 Empirical evaluation of rectified activations in convolutionalnetwork,2015, CoRR
 Improving deep convolutional neural networks with mixedmaxout units,2017, PloS one
