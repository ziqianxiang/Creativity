title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Open problem: The landscape of the losssurfaces of multilayer networks,2015, In Conference on Learning Theory
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Escaping from saddle pointsâ€”online stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 Gradient descent onlyconverges to minimizers,2016, In Conference on learning theory
 Over-parameterized deep neural networks have no strict localminima for any continuous activations,2018, arXiv preprint arXiv:1812
 On the quality of the initial basin in overspecified neural networks,2016, InInternational Conference on Machine Learning
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2019, IEEE Transactions on InformationTheory
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks,2016, arXiv preprint arXiv:1605
 Exponentially vanishing sub-optimal local minima in multilayerneural networks,2017, arXiv preprint arXiv:1702
 Spurious valleys in two-layer neural networkoptimization landscapes,2018, arXiv preprint arXiv:1802
 Diverse neural network learns true target functions,2016, arXivpreprint arXiv:1611
