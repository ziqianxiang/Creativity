title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Regularizationtechniques for fine-tuning in neural machine translation,2017, arXiv preprint arXiv:1707
 Emnist: an extension ofmnist to handwritten letters,2017, arXiv preprint arXiv:1702
 The PASCAL recognising textual entailmentchallenge,2006, In Machine learning challenges
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Variational dropout and the local reparameteri-zation trick,2015, In Advances in Neural Information Processing Systems
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Sentence encoders on stilts: Supplementarytraining on intermediate labeled-data tasks,2018, arXiv preprint arXiv:1811
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of EMNLP
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Regularization of neuralnetworks using dropconnect,2013, In International conference on machine learning
 Fast dropout training,2013, In international conference on machinelearning
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
