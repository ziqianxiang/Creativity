title,year,conference
 Deep learning with differential privacy,2016, In Proceedings of the 2016 ACM SIGSACConference on Computer and Communications Security
 Robust bi-tempered logistic lossbased on Bregman divergences,2019, CoRR
 Two-temperature logistic regressionbased on the Tsallis divergence,2019, In The 22nd International Conference on Artificial Intelligenceand Statistics
 RobustEstimates of Location,1972, Princeton University Press
 Learning long-term dependencies with gradient descent isdifficult,1045, Trans
 Empirical risk minimization for heavy-tailedlosses,2015, Ann
 Loss functions for binary class probability estimationand classification: Structure and applications,2005, Technical report
 On symmetric losses for learningfrom corrupted labels,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Statistical machine learning in the t-exponential family of distributions,2013, PhD thesis
 t-logistic regression,2010, In J
 Calibrating noise to sensitivity inprivate data analysis,2006, In Proceedings of the Third Conference on Theory of Cryptography
 A model for a binary response with misclassifications,1982, In RobertGilchrist (ed
 The estimators of the Princeton robustness study,1973, Technical report
 Robust Statistics: the ApproachBased on Influence Functions,1986, Wiley New York
 Deep residual learning for image recognition,2016, In IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 Better generalization with less data using robust gradientdescent,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 RobUst mislabel logistic regression withoUt modelingmislabel probabilities,2018, Biometrics
 Learning mUltiple layers of featUres from tiny images,2009, Masterâ€™s thesis
 Gradient-based learning applied to docUmentrecognition,1998, Proceedings of the IEEE
 Random classification noise defeats all convex potentialboosters,0885, Machine Learning
" Decoupling ""when to update"" from ""how to update""",2017, InProceedings of the 31st International Conference on Neural Information Processing Systems
 Learning fromcorrupted binary labels via class-probability estimation,2015, In International Conference on MachineLearning (ICML)
 Learning from binary labelswith instance-dependent noise,2018, Machine Learning
 Regularizing and optimizing LSTMlanguage models,2018, In International Conference on Learning Representations
 Statistical Language Models Based on Neural Networks,2012, PhD thesis
 Geometric median and robust estimation in banach spaces,2015, Bernoulli
 Variance regularization with convex objectives,2017, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Stochastic gradient methods for distributionally robustoptimization with f-divergences,2016, In D
 Learning withnoisy labels,2013, In Advances in Neural Information Processing Systems (NIPS)
 Makingdeep neural networks robust to label noise: a loss correction approach,2017, In Computer Vision andPattern Recognition (CVPR)
 Piecewise linear regularized solution paths,2007, Ann
 Robust Regression and Outlier Detection,1987, John Wiley andSons
 A general method for comparing probability assessors,1989, Ann
 Classification with asymmetric label noise:consistency and maximal denoising,2013, In Conference on Learning Theory (COLT)
 Learning with bad training data via iterative trimmed lossminimization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Certifying some distributional robustnesswith principled adversarial training,2018, In 6th International Conference on Learning Representations
 SELFIE: Refurbishing unclean samples for robustdeep learning,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 A tunable loss function for binary classification,2019, In2019 IEEE International Symposium on Information Theory (ISIT)
 RobUstifying AdaBoost by adding the naive error rate,2004, NeuralComputation
 A sUrvey of sampling from contaminated distribUtions,1960, Contributions to probability andstatistics
 Learning with symmetriclabel noise: the importance of being Unhinged,2015, In Advances in Neural Information ProcessingSystems (NIPS)
 A general family of trimmed estimators forrobUst high-dimensional data analysis,2018, Electron
 Relaxed clipping: Aglobal training method for robust regression and classification,2010, In J
 Statistical behavior and consistency of classification methods based on convex riskminimization,2004, Annals of Statistics
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Proceedings of the 32nd International Conference on Neural InformationProcessing Systems
