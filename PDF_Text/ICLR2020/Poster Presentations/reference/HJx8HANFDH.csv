title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Riemannian approach to batch normalization,2017, In Advances inNeural Information Processing Systems
 Mode normalization,2019, In International Conference onLearning Representations
 Speech recognition with deep recur-rent neural networks,2013, In 2013 IEEE international conference on acoustics
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Mask r-cnn,2017, In Proceedings oftheIEEE international conference on computer vision
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, In Advances in Neural Information Processing Systems
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Decorrelated batch normalization,2018, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Iterative normalization: Beyond standardiza-tion towards efficient whitening,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Batch renormalization: Towards reducing minibatch dependence in batch-normalizedmodels,2017, In Advances in Neural Information Processing Systems
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In In Proceedings of The 32nd International Conference onMachine Learning
 Self-normalizingneural networks,2017, In Advances in neural information processing Systems
 Regularizing by the variance of the activationsâ€™ sample-variances,2018, InAdvances in Neural Information Processing Systems
 Differentiable learning-to-normalize via switchablenormalization,2019, In International Conference on Learning Representations
 Nofuss distance metric learning using proxies,2017, In Proceedings of the IEEE International Conferenceon Computer Vision
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Deep metric learning via liftedstructured feature embedding,2016, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Training region-based object detectorswith online hard example mining,2016, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 Evalnorm: Estimating batch normalization statistics forevaluation,2019, In Proceedings ofthe IEEE International Conference on Computer Vision
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Instance normalization: The missing in-gredient for fast stylization,2016, arXiv preprint arXiv:1607
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Group normalization,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Fixup initialization: Residual learning withoutnormalization,2019, In International Conference on Learning Representations
 Learning transferable architecturesfor scalable image recognition,2018, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Oneof the hypotheses for why Group Normalization generally performs slightly worse than Batch Nor-malization is the regularization effect of Batch Normalization due to random minibatches producingvariability in the normalization statistics,2020, Therefore
