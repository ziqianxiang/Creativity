title,year,conference
 Practical coreset constructions for machinelearning,2017, arXiv preprint arXiv:1703
 End to end learning for self-driving cars,2016, CoRR
 Online algorithms and stochastic approximations,2012, In David Saad (ed
 Optimization methods for large-scale machinelearning,2018, SIAMReview
 Efficient architecture search bynetwork transformation,2018, In AAAI
 Learnable embedding space for efficient neuralarchitecture compression,2019, ICLR
 The cityscapes dataset for semanticurban scene understanding,2016, In CVPR
 Essentially no bar-riers in neural network energy landscape,2018, In ICML
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descentlearns one-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2018, In ICML
 Deep residual learning for image recog-nition,2016, CVPR
 Mask r-cnn,2017, ICCV
 Stochastic gradient descent with hyperbolic-tangent decayon classification,2019, WACV
 Densely connectedconvolutional networks,2017, CVPR
 Caffe: Convolutional architecture for fast feature embed-ding,2014, In ACM Multimedia
 The kinetics hUman action video dataset,2017, CoRR
 Adam: A method for stochastic optimization,2015, ICLR
 Imagenet classification with deep convo-lUtional neUral networks,2012, NIPS
 Darts: Differentiable architectUre search,2019, ICLR
 SGDR: Stochastic gradient descent with warm restarts,2017, In ICLR
 Adaptive gradient methods with dynamicboUnd of learning rate,2019, ICLR
 Evidence for invariants in local search,1997, In AAAI
 Systematic evaluation of convolution neuralnetwork advances on the imagenet,2017, Computer Vision and Image Understanding
 Efficient neural architecturesearch via parameter sharing,2018, ICML
 ImageNet training in PyTorch v1,2019,0
 Streamed learning: one-pass svms,2009, InIJCAI
 Regularized evolution for imageclassifier architecture search,2019, AAAI
 On the convergence of adam and beyond,2018, InICLR
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Active learning for convolutional neural networks: A core-setapproach,2018, ICLR
 Cyclical learning rates for training neural networks,2017, WACV
 Going deeper with convolutions,2015, InCVPR
 Rethinkingthe inception architecture for computer vision,2016, In CVPR
 Medical robotics andcomputer-integrated surgery,2008, Springer handbook of robotics
 RMSProp: Divide the gradient by a running average of its recentmagnitude,2012, COURSERA: Neural Networks for Machine Learning
 Growing a brain: Fine-tuning by increasingmodel capacity,2017, In CVPR
 The marginalvalue of adaptive gradient methods in machine learning,2017, In NIPS
 Holistically-nested edge detection,2015, In ICCV
 Hierarchical discrete distribution decomposition formatch density estimation,2019, CVPR
 Pyramid scene parsingnetwork,2017, CVPR
 Online convex programming and generalized infinitesimal gradient ascent,2003, InICML
 Neural architecture search with reinforcement learning,2017, ICLR
 Our definition ofbudget is the number of examples seen during training,1024, So when the batch size increases
