title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 On the learning dynamics of deep neural networks,2018, arXiv preprint arXiv:1809
 The implicit bias of gradient descent on nonseparable data,2019, InConference on Learning Theory
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2017, arXiv preprint arXiv:1712
 Convergence of gradient descent on separable data,2018, arXiv preprint arXiv:1803
 Sgd on neural networks learns functions of increasing complexity,2019, arXivpreprint arXiv:1905
 The conver-gence rate of neural networks for learned functions of different frequencies,2019, InAdvances in Neural Information Processing Systems 32
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 Kernel anddeep regimes in overparametrized models,2019, arXiv preprint arXiv:1906
