title,year,conference
 Tensorflow: Large-scale machine learning on heterogeneousdistributed systems,2016, arXiv preprint arXiv:1603
 The convergence of stochastic gradient descent inasynchronous shared memory,2018, In Proceedings of the 2018 ACM Symposium on Principles of DistributedComputing
 Scalable training of deep learning machines by incremental block training withintra-block parallel optimization and blockwise model-update filtering,2016, In Acoustics
 ImageNet: A large-scale hierarchical imagedatabase,2009, In CVPR09
 On the computational inefficiency of large batch sizes for stochastic gradientdescent,2018, arXiv preprint arXiv:1811
 Delving deep into rectifiers: Surpassing human-levelperformance on imagenet classification,2015, In Proceedings of the IEEE international conference on computervision 
 Deep residual learning for image recognition,2016, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Identity mappings in deep residual networks,2016, InEuropean Conference on Computer Vision
 Flat minima,1997, Neural Computation
 On the diffusion approximation of nonconvex stochasticgradient descent,2017, arXiv preprint arXiv:1705
 Three factors influencing minima in SGD,2018, In International Conference on Artificial Neural Networks2018
 Learning multiple layers of features from tiny images,2009, 2009
 Scaling distributed machine learning with system and algorithm co-design,2017, PhD thesis
 Efficient mini-batch training for stochasticoptimization,2014, In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discoveryand data mining
 Training deep and recurrent networks with hessian-free optimization,2012, InNeural networks: Tricks of the trade
 Revisiting small batch training for deep neural networks,2018, arXiv preprintarXiv:1804
 An empirical model of large-batchtraining,2018, arXiv preprint arXiv:1812
 Automatic differentiation in PyTorch,2017, 2017
 Communication trade-offs for synchronized distributed SGD withlarge step size,2019, arXiv preprint arXiv:1904
 A stochastic approximation method,1985, In Herbert Robbins Selected Papers
 Adaptive communication strategies to achieve the best error-runtime trade-off inlocal-update SGD,2019, In SysML
 A walk with SGD,2018, arXiv preprintarXiv:1802
 Scaling SGD Batch Size to 32k for ImageNet Training,2017, arXivpreprint arXiv:1708
 Imagenet training in minutes,2018, InProceedings of the 47th International Conference on Parallel Processing
 Wide residual networks,2016, In BMVC
