title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, 2018
 Fine-grained analysisof optimization and generalization for overparameterized two-layer neural networks,2019, 2019
 Universal approximation bounds for superpositions of a sigmoidal function,1993, IEEETransactions on Information Theory
 The convergence rate of neuralnetworks for learned functions of different frequencies,2019, In NeurIPS
 On the inductive bias of neural tangent kernels,2019, In NeurIPS
 Generalization Error Bounds of Gradient Descent for LearningOver-parameterized Deep ReLU Networks,2019, 2019
 Kernel methods for deep learning,2009, In NIPS
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Gradient descent finds globalminima of deep neural networks,2018, 2018
 Gradient descent provably optimizesover-parameterized neural networks,2018, 2018
 Approximation and learning of convex superpositions,1995, In PaulVitanyi (ed
 Multilayer feedforward networks are universal approxi-mators,1989, Neural Networks
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 A Mean Field View of the Landscape ofTwo-Layers Neural Networks,2018, art
 Approximation by superposition of sigmoidal andradial basis functions,1992, Advances in Applied mathematics
 A function space view of boundednorm infinite width relu nets: The multivariate case,2019, 2019
 Towards moderate overparameterization: global conver-gence guarantees for training shallow neural networks,2019, 2019
 Remarques sur un resultat non publie de b,1980, maurey
 Understanding Machine Learning: From Theory toAlgorithms,2014, Cambridge University Press
 On the approximation properties of random relufeatures,2018, arXiv preprint arXiv:1810
 Uber die analytische darstellbarkeit sogenannter WillkUrliCher functionen eÎ¹nerreellen Veranderlichen,1885, Sitzungsberichte derAkademie ZU Berlin
 Error bounds for approximations With deep relu netWorks,2016, 2016
 Understandingdeep learning requires rethinking generalization,2016, arXiv:1611
 By Lemma 3,2020,1 and the choice of b1
2 and Lemma B,2020,1
