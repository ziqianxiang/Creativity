title,year,conference
 A latent variable modelapproach to pmi-based word embeddings,2016, TACL
 A latent variable modelapproach to pmi-based word embeddings,2016, TACL
 A simple but tough-to-beat baseline for sentenceembeddings,2016, 2016c
 Neural machine translation by jointlylearning to align and translate,2015, In ICML
 Rademacher and gaussian complexities: Risk bounds andstructural results,2002, JMLR
 Groupreduce: Block-wise low-rank approximation for neural language model shrinking,2018, In Advances in Neural InformationProcessing Systems 
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, ACL
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Breaking the Softmaxbottleneck via learnable monotonic pointwise non-linearities,2019, ICML
 Neural approaches to conversational ai,2019, Foundationsand TrendsR in Information Retrieval
 Representation degenerationproblem in training natural language generation models,2019, In ICLR
 Convolutionalsequence to sequence learning,2017, In ICML
 Frage: frequency-agnosticword representation,2018, In NIPS
 Long short-term memory,1997, Neural computation
 Tying word vectors and word classifiers: Aloss framework for language modeling,2017, In ICLR
 On com-putation and generalization of generative adversarial networks under spectrum control,2019, In ICLR
 Exploring thelimits of language modeling,2016, arXiv preprint arXiv:1602
 Sigsoftmax: Reanalysisof the softmax bottleneck,2018, In NIPS
 Ctrl:A conditional transformer language model for controllable generation,2019, arXiv:1909
 Stochastic gradient descent as approximatebayesian inference,2017, JMLR
 Regularizing and optimizing LSTMlanguage models,2018, In ICLR
 An analysis of neural language modelingat multiple scales,2018, arXiv preprint arXiv:1803
 Efficient estimation of word represen-tations in vector space,2013, arXiv preprint arXiv:1301
 All-but-the-top: Simple and effective postprocessing for wordrepresentations,2018, In ICLR
 Online learning with kernel losses,2019, ICML
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Deep contextualized word representations,2018, In NAACL
 Using the output embedding to improve language models,2017, In EACL
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Fast parametric learning with activationmemorization,2018, In ICML
 A comparison of techniques for language model integration in encoder-decoder speechrecognition,2018, In IEEE Workshop on Spoken Language Technology
 Attention is all you need,2017, In NIPS
 Improving neural language modeling via adversarialtraining,2019, In ICML
 Multi-agent dual learning,2019, In ICLR
 Early stopping for kernel boosting algorithms:A general analysis with localized complexities,2017, In NIPS
 Breaking the softmaxbottleneck: A high-rank RNN language model,2018, In ICLR
 Stabilizing gradients for deep neural networks via efficientSvd parameterization,2018, In International Conference on Machine Learning
