title,year,conference
 Tensorflow: Large-scale MachineLearning on Heterogeneous Distributed Systems,2016, arXiv:1603
 Information Dropout: Learning Optimal RepresentationsThrough Noisy Computation,2018, In PAMI
 Adaptive dropout for training deep neural networks,2013, In NIPS
 Metareg: Towards domain gener-alization using meta-regularization,2018, In NeurIPS
 Multitask Learning,1997, Machine Learning
 Entropy-SGD: Biasing Gradient Descent Into Wide Valleys,2017, In ICLR
 Imagenet: A Large-Scale HierarchicalImage Database,2009, In CVPR
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In ICML
 Probabilistic model-agnostic meta-learning,2018, InNeurIPS
 Dropout as a Bayesian Approximation: Representing Model Uncertaintyin Deep Learning,2016, In ICML
 Concrete Dropout,2017, In NIPS
 Dropblock: A regularization method for convolu-tional networks,2018, In NeurIPS
 Explaining and harnessing adversarialexamples,2015, In ICLR
 Uncertainty-Aware Attentionfor Reliable Interpretation and Prediction,2018, In NeurIPS
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In JMLR
 Variational Dropout and the Local ReparameterizationTrick,2015, In NIPS
 Adam: A method for stochastic optimization,2014, In ICLR
 Auto encoding variational bayes,2014, In ICLR
 Siamese neural networks for one-shotimage recognition,2015, In ICML
 Dropmax: Adaptivevariationial softmax,2018, In NeurIPS
 Meta-learning withdifferentiable convex optimization,2019, In CVPR
 Gradient-based meta-learning with learned layerwise metric andsubspace,2018, In ICML
 Meta-sgd: Learning to learn quickly for fewshot learning,2017, arXiv preprint arXiv:1707
 A simple neural attentive meta-learner,2018, In ICLR
 Exploring Generalization in DeepLearning,2017, In NIPS
 Tadam: Task dependent adaptivemetric for improved few-shot learning,2018, In NeurIPS
 Amortized bayesian meta-learning,2019, In ICLR
 Optimization as a model for feW-shot learning,2017, In ICLR
 Meta-learning With latent embedding optimization,2019, In ICLR
 How does batch normal-ization help optimization? In NeurIPS,2018, 2018
 On Large-BatchTraining for Deep Learning: Generalization Gap and Sharp Minima,2017, In ICLR
 Prototypical networks for few-shot learning,2017, InNIPS
 Learning structured output representation using deepconditional generative models,2015, In NIPS
 Disentangling adversarial robustness and general-ization,2019, In CVPR
 Deep Learning and the Information Bottleneck Principle,2015, In InIEEE Information Theory Workshop
 The information bottleneck method,1999, InAnnual Allerton Conference on Communication
 Manifold Mixup: Learning Better Representations by Interpolating HiddenStates,2019, In ICML
 Fast dropout training,2013, In ICML
 mixup: Beyond empiri-cal risk minimization,2017, In ICLR
 Fast contextadaptation via meta-learning,2019, In ICML
