title,year,conference
 A doWnsampled variant of imagenet as analternative to the cifar datasets,2017, arXiv preprint arXiv:1707
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In international conference on machine learning
 Training deep neural-netWorks using a noise adaptationlayer,2016, 2016
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InAdvances in neural information processing Systems
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In Advances in neural information processingsystems
 Using pre-training can improve model robustnessand uncertainty,2019, arXiv preprint arXiv:1901
 Mentornet: Learningdata-driven curriculum for very deep neural networks on corrupted labels,2017, arXiv preprintarXiv:1712
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Deep nets donâ€™t learn via mem-orization,2017, 2017
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In Advances in Neural Information ProcessingSystems
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 Dimensionality-driven learning with noisy labels,2018, arXivpreprint arXiv:1806
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Realis-tic evaluation of deep semi-supervised learning algorithms,2018, In Advances in Neural InformationProcessing Systems
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition 
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Learning with bad training data via iterative trimmed loss mini-mization,2019, In International Conference on Machine Learning
 Trainingconvolutional networks with noisy labels,2014, arXiv preprint arXiv:1406
 An empirical study of example forgetting during deep neural networklearning,2018, arXiv preprint arXiv:1812
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
