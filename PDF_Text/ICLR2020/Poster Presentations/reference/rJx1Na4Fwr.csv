title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, CoRR
 Unlabeled dataimproves adversarial robustness,2019, arXiv preprint arXiv:1905
 Zoo: Zeroth order opti-mization based black-box attacks to deep neural networks without training substitute models,2017, InProceedings of the 10th ACM Workshop on Artificial Intelligence and Security
 Query-efficient hard-label black-box attack: An optimization-based approach,2019, 2019
 Certified adversarial robustness via randomizedsmoothing,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Convergenceof adversarial training in overparametrized neural networks,2019, In Advances in Neural InformationProcessing Systems 32
 Ai2: Safety and robustness certification of neural networks with abstract interpreta-tion,2018, In 2018 IEEE Symposium on Security and Privacy (SP)
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 On the effectiveness of interval bound propagation fortraining verifiably robust models,2018, arXiv preprint arXiv:1810
 Learning with a strong adver-sary,2015, arXiv preprint arXiv:1511
 Adversarial logit pairing,2018, CoRR
 Certifiedrobustness to adversarial examples with differential privacy,2018, arXiv preprint arXiv:1802
 Second-order adversarial attackand certifiable robustness,2018, CoRR
 Towards robust neural networks viarandom self-ensemble,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Differentiable abstract interpretation for prov-ably robust neural networks,2018, In International Conference on Machine Learning
 Deepfool: a simple andaccurate method to fool deep neural networks,2015, CoRR
 Towards the science ofsecurity and privacy in machine learning,2016, arXiv preprint arXiv:1611
 Adversarial robustness through local linearization,2019, arXivpreprint arXiv:1907
 Foolbox: A python toolbox to benchmark therobustness of machine learning models,2017, arXiv preprint arXiv:1707
 Intriguing properties of neural networks,2013, CoRR
 Mixtrain: Scalable training of formallyrobust neural networks,2018, arXiv preprint arXiv:1811
 Towards fast computation of certified robustness for ReLU networks,2018, InJennifer Dy and Andreas Krause (eds
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In Jennifer Dy and Andreas Krause (eds
 Scaling provable adversarialdefenses,2018, In S
 Mitigating adversarialeffects through randomization,2017, arXiv preprint arXiv:1711
 Adversar-ially robust generalization just requires more unlabeled data,2019, CoRR
 You only propagateonce: Accelerating adversarial training via maximal principle,2019, arXiv preprint arXiv:1905
 Efficient neural net-work robustness certification with general activation functions,2018, In Advances in neural informationprocessing systems
 Towards sta-ble and efficient training of verifiably robust neural networks,2019, arXiv preprint arXiv:1906
