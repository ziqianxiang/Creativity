title,year,conference
 Identifiability of discrete-time neural networks,1993, InEuropean Control Conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learing (ICML)
 Symmetry-invariant optimization indeep networks,2016, In International Conference on Learning Representations (ICLR)
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, In Proceedings of the NationalAcademy of Sciences (PNAS)
 On the geometry of feedforWard neuralnetWork error surfaces,1993, Neural computation
 Approximation by superpositions ofa sigmoidal function,1989, Mathematics of control
 Gradient descent finds globalminima of deep neural netWorks,2019, In International Conference on Machine Learing (ICML)
 Recovering a feed-forWard net from its output,1994, In Conferenceon Neural Information Processing Systems (NIPS)
 Complexity of linear regions in deep netWorks,2019, In InternationalConference on Machine Learing (ICML)
 Multilayer feedforWard netWorks are uni-versal approximators,1989, Neural networks
 Functionally equivalent feedforward neural networks,1994, NeuralComputation
 Path-SGD: Path-normalized op-timization in deep neural networks,2015, In Conference on Neural Information Processing Systems(NIPS)
 Topological properties of the set of func-tions generated by neural networks of fixed size,2018, In arXiv:1806
 Bounding and counting linearregions of deep neural networks,2018, In International Conference on Machine Learing (ICML)
 Equi-normalization of neuralnetworks,2019, In International Conference on Learning Representations (ICLR)
 This proves the theorem,2020,	â–¡20Published as a conference paper at ICLR 2020Lemma A
