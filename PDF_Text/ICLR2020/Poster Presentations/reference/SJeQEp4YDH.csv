title,year,conference
 Deep speech 2: End-to-end speech recognition in english and mandarin,2016, In International conference on machine learning
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Natural and adversarial error detectionusing invariance to image transformations,2019, arXiv preprint arXiv:1902
 Dimensionality reduction as a defenseagainst evasion attacks on machine learning classifiers,2017, arXiv preprint arXiv:1704
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Adversarial robustness as a prior for learned representations,2019, arXiv preprintarXiv:1906
 Detecting adversarialsamples from artifacts,2017, arXiv preprint arXiv:1703
 Adversarial and clean data are not twins,2017, arXivpreprint arXiv:1704
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Towards deep neural network architectures robust to adversarialexamples,2014, arXiv preprint arXiv:1412
 Early methods for detecting adversarial images,2017, In ICLR
 Robust convolutional neural networksunder adversarial noise,2015, arXiv preprint arXiv:1511
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 A tutorial on energy-basedlearning,2006, Predicting structured data
 Adversarial examples detection in deep networks with convolutional filterstatistics,2017, 2017 IEEE International Conference on Computer Vision (ICCV)
 Neural trojans,2017, In 2017 IEEE International Conferenceon Computer Design (ICCD)
 Characterizing adversarial subspaces usinglocal intrinsic dimensionality,2018, arXiv preprint arXiv:1801
 On detecting adversar-ial perturbations,2017, CoRR
 Deep neural networks are easily fooled: High confi-dence predictions for unrecognizable images,2015, In Proceedings of the IEEE conference on computervision and pattern recognition
 Towards robust detection of adversarial ex-amples,2018, In Advances in Neural Information Processing Systems
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 The odds are odd: A statistical test for detectingadversarial examples,2019, arXiv preprint arXiv:1902
 Deep learning in medical image analysis,2017, Annualreview of biomedical engineering
 Certifiable distributional robustness withprincipled adversarial training,2018, In International Conference on Learning Representations
 One pixel attack for fooling deepneural networks,2019, IEEE Transactions on Evolutionary Computation
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 On adaptive attacks toadversarial example defenses,2020, arXiv preprint arXiv:2002
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Generating ad-versarial examples with adversarial networks,2018, In Proceedings of the 27th International JointConference on Artificial Intelligence
 Feature squeezing: Detecting adversarial examples in deepneural networks,2017, arXiv preprint arXiv:1704
 In Madry et al,2017, (2017)
