title,year,conference
 Convergence guarantees forrmsprop and adam in non-convex optimization and their comparison to nesterov acceleration onautoencoders,2018, arXiv preprint arXiv:1807
 An improvement of the convergence proof of theadam-optimizer,2019, arXiv preprint arXiv:1804
 Convex optimization,2004, Cambridge university press
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, In Proceedings of the 7th International Conference onLearning Representations
 Sadagrad: Strongly adaptive stochastic gradientmethods,2018, In Proceedings of 35th International Conference on Machine Learning
 Stronger baselines for trustable results in neural machinetranslation,2017, In Proceedings of the 1st Workshop on Neural Machine Translation
 Incorporating nesterov momentum into adam,2016, In Proceedings of 4th InternationalConference on Learning Representations
 Draw: arecurrent neural network for image generation,2015, In Proceedings of the 32nd International Conferenceon Machine Learning
 Logarithmic regret algorithms for online convexoptimization,2007, Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the 29th IEEE Conference on Computer Vision and PatternRecognition
 On the generalization ability of online strongly convexprogramming algorithms,2009, In Advances in Neural Information Processing Systems 21
 Adam: A method for stochastic optimization,2015, In Proceedings of3rd International Conference on Learning Representations
 Skip-thought vectors,2015, In Advances in Neural Information Processing Systems 27
 Learning multiple layers of features from tiny images,2009, Technical report
 Adaptive bound optimization for online convex op-timization,2010, In Proceedings of the 23rd Annual Conference on Learning Theory
 On the convergence of adam and beyond,2018, InProceedings of 6th International Conference on Learning Representations
 Online learning and online convex optimization,2012, Foundations andTrendsR in Machine Learning
 On the convergence proof of amsgrad and a new version,2019, IEEE Access
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Gadam: Genetic-evolutionary adam for deep neuralnetwork optimization,2018, arXiv preprint arXiv:1805
 Online convex programming and generalized infinitesimal gradient ascent,2003, InProceedings of the 20th International Conference on Machine Learning
