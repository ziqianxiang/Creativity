title,year,conference
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In International Conference on MachineLearning
 Towards evaluating the robustness of neural networks,2019, InSecurity and Privacy (SP)
 Parsevalnetworks: Improving robustness to adversarial examples,2018, In International Conference on MachineLearning
 Formal guarantees on the robustness of a classifieragainst adversarial manipulation,2019, arXiv preprint arXiv:1705
 Learning with a strong adver-sary,2015, arXiv preprint arXiv:1511
 On the effectiveness of low frequencyperturbations,2019, In Proceedings of the 28th International Joint Conference on Artificial Intelligence
 Robust large margin deepneural networks,2019, IEEE Transactions on Signal Processing
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Lipschitz-margin training: Scalable certifi-cation of perturbation invariance for deep neural networks,2016, In Advances in Neural InformationProcessing Systems
 This again is likely due to thatMNIST is an “easier” dataset,2019, It also indicates that the TransferGap is not purely due to the MMAtraining algorithm
5	under dif 1,2020,0	ferent 
5	cc under 1,2020,0	different 1
