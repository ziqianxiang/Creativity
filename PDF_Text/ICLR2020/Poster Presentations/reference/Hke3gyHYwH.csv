title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, In Advances in Neural Information Processing Systems31
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Graph neural tangent kernel: Fusing graph neural networks with graph kernels,2019, arXiv preprintarXiv:1905
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Spectral algorithmsfor supervised learning,2008, Neural Computation
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 A tail inequality for quadratic forms of subgaussianrandom vectors,2012, Electronic Communications in Probability
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Mentornet: Learningdata-driven curriculum for very deep neural networks on corrupted labels,2017, arXiv preprintarXiv:1712
 Gradient descent with early stopping is prov-ably robust to label noise for overparameterized neural networks,2019, arXiv preprint arXiv:1903
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
" Decoupling"" when to update"" from"" how to update""",2017, InAdvances in Neural Information Processing Systems
 Foundations of machine learning,2012, MITPress
 Generalization in deep networks: The role of distance frominitialization,2019, arXiv preprint arXiv:1901
 The roleof over-parametrization in generalization of neural networks,2019, In International Conference onLearning Representations
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Trainingconvolutional networks with noisy labels,2014, arXiv preprint arXiv:1406
 Early stopping for kernel boosting algorithms:A general analysis with localized complexities,2017, In Advances in Neural Information ProcessingSystems
 Understandingdeep learning requires rethinking generalization,2017, In Proceedings of the International Conferenceon Learning Representations (ICLR)
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
