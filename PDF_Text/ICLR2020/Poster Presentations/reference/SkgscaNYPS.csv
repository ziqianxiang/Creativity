title,year,conference
 A Convergence Theory for Deep Learningvia Over-Parameterization,2018, CoRR
 Entropy-sgd: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 On the Global Convergence of Gradient Descentfor Over-parameterized Models using Optimal Transport,2018, In Advances in NeuralInformation Processing Systems 31
 A note on lazy training in supervised differentiableprogramming,2018, arXiv preprint arXiv:1812
 Kernel Methods for Deep Learning,2009, In Advances inNeural Information Processing Systems 22
 Gradient Descent ProvablyOptimizes Over-parameterized Neural Networks,2019, 2019
 The jamming transition as a paradigm to understand theloss landscape of deep neural networks,2018, arXiv preprint arXiv:1809
 An investigation into neural net opti-mization via hessian eigenvalue density,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov(eds
 Gradient descent happens in a tinysubspace,2018, CoRR
 Flat minima,1997, Neural Computation
 Dynamics of deep neural networks and neural tangenthierarchy,2019, arXiv preprint arXiv:1909
 Neural Tangent Ker-nel: Convergence and Generalization in Neural Networks,2018, In Advances inNeural Information Processing Systems 31
 Universal Statistics of Fisher In-formation in Deep Neural Networks: Mean Field Approach,2018, jun 2018
 Deep Neural Networks as Gaussian Processes,2018, ICLR
 A mean field view of the landscapeof two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Mean-field theory of two-layersneural networks: dimension-free bounds and kernel limit,2019, arXiv preprint arXiv:1902
 Measurements of three-level hierarchical structure in the outliers in thespectrum of deepnet hessians,2019, CoRR
 On the saddle pointproblem for non-convex optimization,2014, arXiv preprint
 Geometry of Neural Network Loss Surfaces viaRandom Matrix Theory,2017, In Proceedings of the 34th International Conference on MachineLearning
 Exponential expressivity in deep neural networks through transient chaos,2016, InD
 Parameters as interacting particles: longtime convergence and asymptotic error scaling of neural networks,2018, In Advances inNeural Information Processing Systems 31
 EmpiricalAnalysis of the Hessian of Over-Parametrized Neural Networks,2017, CoRR
 How does batchnormalization help optimization? In S,2018, Bengio
 Information geometry of neural networks,0302, 1998
