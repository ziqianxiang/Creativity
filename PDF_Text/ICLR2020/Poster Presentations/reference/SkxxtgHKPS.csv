title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, In International Conference on Machine Learning (ICML)
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems (NeurIPS)
 Entropy-SGD: Biasing gradi-ent descent into wide valleys,2017, In International Conference on Learning Representations (ICLR)
 Stability and convergence trade-off of iterative optimizationalgorithms,2018, arXiv preprint arXiv:1804
 Derivations for linear algebra and optimization,2007, Berkeley
 Entropy-SGD optimizes the prior of a PAC-Bayesbound: Generalization properties of entropy-SGD and data-dependent priors,2018, In InternationalConference on Machine Learning (ICML)
 Leave-one-out error and stability of learning algorithmswith applications,2003, NATO science series sub series iii computer and systems sciences
 High probability generalization bounds for uniformly stable al-gorithms with nearly optimal rate,2019, In Conference on Learning Theory (COLT)
 Data-dependent stability of stochastic gradient descent,2018, InInternational Conference on Machine Learning (ICML)
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 Tighter pac-bayes bounds throughdistribution-dependent priors,2013, Theoretical Computer Science
 Generalization bounds for randomized learning with application to stochastic gradientdescent,2016, In NIPS Workshop on Optimizing the Optimizers
 A pac-bayesian analysis of randomized learning with application to stochastic gradientdescent,2017, In Advances in Neural Information Processing Systems (NeurIPS)
 Poincare and logarithmic sobolev inequalities by decompo-sition of the energy landscape,2014, The Annals of Probability
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory (COLT)
 Non-convex learning via stochastic gra-dient langevin dynamics: a nonasymptotic analysis,2017, In Conference on Learning Theory (COLT)
 Pac-bayes bounds for stable algorithms with instance-dependent priors,2018, In Advances inNeural Information Processing Systems (NeurIPS)
 On the importance of initializa-tion and momentum in deep learning,2013, In International Conference on Machine Learning (ICML)
 Some inequalities for information divergence and related measures of discrimi-nation,2000, IEEE Transactions on Information Theory
 Local optimality and generalization guaran-tees for the langevin algorithm via empirical metastability,2018, In Conference On Learning Theory(COLT)
 Data-dependent sample complexity of deep neural networks via lipschitzaugmentation,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Bayesian learning via stochastic gradient langevin dynamics,2011, InInternational Conference on Machine Learning (ICML)
 Information-theoretic analysis of generalization capability of learn-ing algorithms,2017, In Advances in Neural Information Processing Systems (NeurIPS)
 Global convergence of langevin dynamicsbased algorithms for nonconvex optimization,2018, In Advances in Neural Information ProcessingSystems (NeurIPS)
 A hitting time analysis of stochastic gradientlangevin dynamics,2017, In Conference on Learning Theory (COLT)
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
 The rest of the proof is the same as the proof of Theorem 11,2020,	■21Published as a conference paper at ICLR 2020B Proofs in Section 4B
 Applying Lemma 4 gives the generalization bound ofGLD,2013,	■Lemma 37 (Exponential decay in entropy)
