title,year,conference
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the NationalAcademy of Sciences
 Convex optimization: Algorithms and complexity,2015, Foundations andTrendsR in Machine Learning
 An analysis of deep neUral networkmodels for practical applications,2016, arXiv preprint arXiv:1605
 Ac-celerating stochastic gradient descent,2017, arXiv preprint arXiv:1704
 On the insuffi-ciency of existing momentum schemes for stochastic optimization,2018, International Conferenceon Learning Representations (ICLR)
 Adam: A method for stochastic optimization,2014, arXivpreprint arXiv:1412
 Toward deeper understanding of noncon-vex stochastic optimization with momentum using diffusion approximations,2018, arXiv preprintarXiv:1802
 Non-asymptotic analysis of stochastic approximation al-gorithms for machine learning,2011, In Advances in Neural Information Processing Systems
 In search of the real inductive bias:On the role of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Empirical analysisof the hessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Fast convergence of stochastic gradient descent under astrong growth condition,2013, arXiv preprint arXiv:1308
 Fast and faster convergence of sgd forover-parameterized models and an accelerated perceptron,2018, arXiv preprint arXiv:1810
 Understand-ing deep learning requires rethinking generalization,2017, International Conference on LearningRepresentations (ICLR)
