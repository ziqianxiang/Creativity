title,year,conference
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Nest: A neural network synthesis tool based on agrow-and-prune paradigm,2019, IEEE Transactions on Computers
 ImageNet: A large-scale hierarchicalimage database,2009, In CVPR09
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, In International Joint Conference on Artificial Intelligence(IJCAI)
 Filter pruning via geometric medianfor deep convolutional neural networks acceleration,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Deep networks withstochastic depth,2016, In European Conference on Computer Vision
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Error feedbackfixes SignSGD and other gradient compression schemes,2019, In International Conference on MachineLearning
 Learning multiple layers of features from tiny images,2009, 2009
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature communications
 Automatic differentiation inPyTorch,2017, 2017
 The error-feedback framework: Better rates for SGD withdelayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 Wide residual networks,2016, In BMVC
