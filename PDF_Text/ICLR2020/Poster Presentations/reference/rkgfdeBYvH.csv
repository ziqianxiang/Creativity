title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 On exactcompUtation with an infinitely wide neUral net,2019, In NeurIPS
 SGD learns over-parameterized networks that provably generalize on linearly separable data,2018, In 6th InternationalConference on Learning Representations
 Distributional and Lq norm inequalities for polynomials overconvex bodies in Rn,1073, Math
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In S
 Gradient descent provably optimizesover-parameterized neural networks,2019, In ICLR
 Gradient descent findsglobal minima of deep neural networks,2019, In ICML
 Is it time to swish? comparing deep learningactivation functions across nlp tasks,2018, In EMNLP
 Understanding the difficulty of training deep feedforwardneural networks,2010, In Yee Whye Teh and Mike Titterington (eds
 Deep Learning,2016, MIT Press
 On the impact of the activation functionon deep neural networks training,2019, CoRR
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In S
 Self-normalizingneural networks,2017, In Advances in neural information processing Systems
 Special functions and their applications,1972, Dover Publications
 Deep neural networks as gaussian processes,2017, arXiv preprint arXiv:1711
 A mean field view of the landscape of two-layer neural networks,0027, Proceedings of the National Academy of Sciences
 In search of the real inductive bias: Onthe role of implicit regularization in deep learning,2015, In 3rd International Conference on LearningRepresentations
 Activation func-tions: Comparison of trends in practice and research for deep learning,2018, CoRR
 Analysis of boolean functions,2014, Cambridge University Press
 Towards moderate overparameterization: global con-vergence guarantees for training shallow neural networks,2019, arXiv preprint arXiv:1902
 Nonlinear random matrix theory for deep learning,2017, In I
 The emergence of spectral universalityin deep networks,2018, In International Conference on Artificial Intelligence and Statistics
 Approximation theory of the MLP model in neural networks,1999, Acta Numerica
 Searching for activation functions,2018, In ICLRWorkshop
 Learning kernel-based halfspaces withthe 0-1 loss,0097, SIAM J
 Neural network with unbounded activation functions is universalapproximator,1063, Applied and Computational Harmonic Analysis
 Smoothed analysis of algorithms: Why the simplexalgorithm usually takes polynomial time,2004, J
 Diverse neural network learns true target functions,2017, InProceedings of the 20th International Conference on Artificial Intelligence and Statistics
 Revise saturated activation functions,2016, In ICLR Workshop
 The proof follows from the proofs of Theorem F,2020,8 and Theorem F
2using Fact C,2020,1
