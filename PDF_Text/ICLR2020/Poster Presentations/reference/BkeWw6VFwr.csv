title,year,conference
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In International Conference on MachineLearning
 Thermometer encoding: One hotway to resist adversarial examples,2018, In ICLR
 Mitigating evasion attacks to deep neural networks viaregion-based classification,2017, In Proceedings of the 33rd Annual Computer Security ApplicationsConference
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Provably minimally-distorted adver-sarial examples,2017, arXiv preprint arXiv:1709
 Maximum resilience of artificial neuralnetworks,2017, In International Symposium on Automated Technology for Verification and Analysis
 Certified adversarial robustness via randomizedsmoothing,2019, arXiv preprint arXiv:1902
 Provable robustness of relu net-works via maximization of linear regions,2018, In Proceedings of the 22nd International Conferenceon Artificial Intelligence and Statistics
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Stochastic activation pruning for robust adversarial de-fense,2018, In ICLR
 Output range analysisfor deep neural networks,2017, arXiv preprint arXiv:1709
 Training verified learners with learned ver-ifiers,2018, arXiv preprint arXiv:1805
 Ai2: Safety and robustness certification of neural networks with abstract interpreta-tion,2018, In 2018 IEEE Symposium on Security and Privacy (SP)
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 On simultaneous confidence intervals for multinomial proportions,1965, Technometrics
 Regularisation of neural networksby enforcing lipschitz continuity,2018, arXiv preprint arXiv:1804
 On the effectiveness of interval bound propagation fortraining verifiably robust models,2018, arXiv preprint arXiv:1810
 Countering adversarialimages using input transformations,2018, In ICLR
 Safety verification of deep neuralnetworks,2017, In International Conference on Computer Aided Verification
 Rank verification for exponential families,2019, The Annals ofStatistics
 AttriGuard: A practical defense against attribute inferenceattacks via adversarial machine learning,2018, In USENIX Security Symposium
 Reluplex: Anefficient smt solver for verifying deep neural networks,2017, In International Conference on ComputerAided Verification
 Adversarial machine learning at scale,2017, InInternational Conference on Learning Representations
 Certifiedrobustness to adversarial examples with differential privacy,2019, In IEEE Symposium on Security andPrivacy (SP)
 Tight certificates of adversarialrobustness for randomly smoothed classifiers,2019, In Advances in Neural Information ProcessingSystems
 Second-order adversarial attack andcertifiable robustness,2018, arXiv preprint arXiv:1809
 Towards robust neural networks viarandom self-ensemble,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Adv-bnn: Improved adversarial defensethrough robust bayesian neural network,2019, In ICLR
 An approach to reachability analysis for feed-forward reluneural networks,2017, arXiv preprint arXiv:1706
 Characterizing adversarial subspaces usinglocal intrinsic dimensionality,2018, In ICLR
 Magnet: a two-pronged defense against adversarial examples,2017, InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
 On detecting adversarialperturbations,2017, In ICLR
 Differentiable abstract interpretation for prov-ably robust neural networks,2018, In International Conference on Machine Learning
 Cascade adversarial machine learning regu-larized with a unified embedding,2018, In ICLR
 On the problem of the most efficient tests of statisticalhypotheses,1933, Philosophical Transactions of the Royal Society of London
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Certified defenses against adversarial exam-ples,2018, In International Conference on Learning Representations
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, InAdvances in Neural Information Processing Systems
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, In ICLR
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examPles,2017, In ICLR
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examPles,2018, In ICLR
 Peer-nets: ExPloiting Peer wisdom against adversarial attacks,2019, In ICLR
 Intriguing ProPerties of neural networks,2014, In ICLR
 Evaluating robustness of neural networks with mixedinteger Programming,2018, In ICML
 Ensemble adversarial training: Attacks and defenses,2018, In ICLR
 Lipschitz-margin training: Scalable certifi-cation of perturbation invariance for deep neural networks,2018, In Advances in Neural InformationProcessing Systems
 Mixtrain: Scalable training of formallyrobust neural networks,2018, arXiv preprint arXiv:1811
 Efficient formal safetyanalysis of neural networks,2018, In Advances in Neural Information Processing Systems
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In International Conference on Machine Learning
 Scaling provable adversarialdefenses,2018, In Advances in Neural Information Processing Systems
 Mitigating adversarialeffects through randomization,2018, In ICLR
 Feature squeezing: Detecting adversarial examples in deepneural networks,2018, In NDSS
 Efficient neural net-work robustness certification with general activation functions,2018, In Advances in Neural InformationProcessing Systems
