title,year,conference
 Fisher Information and Natural Gradi-ent Learning in Random Deep Networks,2019, In The 22nd International Conference on ArtificialIntelligence and Statistics
 Practical Gauss-Newton Optimisation for DeepLearning,2017, In International Conference on Machine Learning
 Optimization Methods for Large-Scale MachineLearning,2018, SIAM Review
 Convex Optimization,2004, Cambridge University Press
 Riemannian approach to batch normalization,2017, In Isabelle Guyon
 Deep Residual Learning for ImageRecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift,2015, In Francis R
 Universal Statistics of Fisher Information inDeep Neural Networks: Mean Field Approach,2019, In The 22nd International Conference on ArtificialIntelligence and Statistics
 Adam: A Method for Stochastic Optimization,2015, In Yoshua Bengioand Yann LeCun (eds
 Limitations of the empirical Fisher approxi-mation for natural gradient descent,2019, In Hanna M
 Deep Neural Networks as Gaussian Processes,2018, In International Conference onLearning Representations
 Wide Neural Networks of Any Depth Evolve as Linear ModelsUnder Gradient Descent,2019, In Hanna M
 Optimizing Neural Networks with Kronecker-factored Approxi-mate Curvature,2015, In International Conference on Machine Learning
 Bayesian Learning for Neural Networks,1996, Lecture Notes in Statistics
 Geometry of Neural Network Loss Surfaces via RandomMatrix Theory,2017, In International Conference on Machine Learning
 Resurrecting the sigmoid in deeplearning through dynamical isometry: Theory and practice,2017, In I
 The emergence of spectral universality indeep networks,2018, In International Conference on Artificial Intelligence and Statistics
 Weight Normalization: A Simple Reparameterization to Acceler-ate Training of Deep Neural Networks,2016, In D
 Exact solutions to the nonlineardynamics of learning in deep linear neural networks,2014, In Yoshua Bengio and Yann LeCun (eds
 Deep InformationPropagation,2017, In 5th International Conference on Learning Representations
 Spectral Theory of Block Operator Matrices and Applications,2008, Imperial CollegePr
 On orthogonality and learningrecurrent networks with long term dependencies,2017, In International Conference on Machine Learning
 Full-CapacityUnitary Recurrent Neural Networks,2016, In D
 In Jennifer G,2018, Dy and Andreas Krause (eds
