title,year,conference
 Xception: Deep learning with depthwise separable convolutions,2017, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 What does BERT lookat? an analysis of BERT’s attention,2019, arXiv preprint arXiv:1906
 Visualizing and measuring the geometry of BERT,2019, arXiv preprint arXiv:1906
 Approximation by superpositions ofa sigmoidal function,1989, Mathematics of control
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Learning binary codesfor high-dimensional data using bilinear projections,2013, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Depthwise separable convolutions for neuralmachine translation,2017, arXiv preprint arXiv:1706
 ResNet with one-neuron hidden layers is a universal approxi-mator,2018, In Advances in Neural Information Processing Systems
 RoBERTa: A robustly optimized BERT pre-training approach,2019, arXiv preprint arXiv:1907
 Effective approaches to attention-based neural machine translation,2015, In Empirical Methods in Natural Language Processing(EMNLP)
 On the Turing completeness of modern neuralnetwork architectures,2019, arXiv preprint arXiv:1901
 Improving language under-standing by generative pre-training,2018, Technical Report
 Languagemodels are unsupervised multitask learners,2019, Technical Report
 Rigid-motion scattering for image classification,2014, Ph
 Analyzing the structure of attention in a transformer languagemodel,2019, arXiv preprint arXiv:1906
 XLNet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
 This proves the lemma,2020,	□13Published as a conference paper at ICLR 2020B
