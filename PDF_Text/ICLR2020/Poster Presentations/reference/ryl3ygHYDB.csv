title,year,conference
 On the optimization of deep networks: Implicit acceleration byoverparametrization,2018, In Proceedings of the International Conference on Machine Learning
 Deep rewiring: training very sparse deepnetworks,2018, In International Conference on Learning Representations
 Compressing neural networks using the variational informationbottleneck,2018, In Proceedings of the International Conference on Machine Learning
 BackPACK: Packing more into backprop,2020, In InternationalConference on Learning Representations
 Learning to prune deep neural networks via layer-wiseoptimal brain surgeon,2017, In Advances in Neural Information Processing Systems
 Algorithmic regularization in learning deep homogeneous models:Layers are automatically balanced,2018, In Advances in Neural Information Processing Systems
 The state of sparsity in deep neural networks,2019, arXiv preprint1902
 Deep Learning,2016, MIT Press
 Deep residual learning for image recognition,2016, In IEEEConference on Computer Vision and Pattern Recognition
 AMC: AutoML for model compression andacceleration on mobile devices,2018, In European Conference on Computer Vision
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In Proceedings of the International Conference on Machine Learning
 Adam: A method for stochastic optimization,2015, In International Conferenceon Learning Representations
 Learning multiple layers of features from tiny images,2009, Technicalreport
 Optimal brain damage,1989, In Advances in Neural InformationProcessing Systems
 Gradient-based learning applied to document recog-nition,1998, In Proceedings of the IEEE
 Runtime neural pruning,2017, In Advances in Neural InformationProcessing Systems
 Learning efficient convolutional networksthrough network slimming,2017, In IEEE International Conference on Computer Vision
 Rethinking the value of network pruning,2019, InInternational Conference on Learning Representations
 Bayesian compression for deep learning,2017, In Advances inNeural Information Processing Systems
 Learning sparse neural networks through l0 regulariza-tion,2018, In International Conference on Learning Representations
 Variational dropout sparsified deep neural networks,2017, InProceedings of the International Conference on Machine Learning
 Some NP-complete problems in quadratic and nonlinear program-ming,1987, Mathematical programming
 Bayesian learning for neural networks,1996, Springer
 MobileNetV2: Inverted residuals andlinear bottlenecks,2018, In IEEE Conference on Computer Vision and Pattern Recognition
 The singular values of convolutional layers,2019, In InternationalConference on Learning Representations
 Very deep convolutional networks for large-scale image recogni-tion,2015, In International Conference on Learning Representations
 Learning structured sparsity in deep networks,2016, InAdvances in Neural Information Processing Systems
 Rethinking smaller-norm-less-informative assumption in chan-nel pruning of convolutional layers,2018, In International Conference on Learning Representations
 Wide residual networks,2016, In Proceedings of British Machine VisionConference
 Learning to share: simultaneous parameter ty-ing and sparsification in deep learning,2018, In International Conference on Learning Representations
