title,year,conference
 Tree-structured decoding with doubly-recurrent neuralnetworks,2017, In Proceedings of the International Conference on Learning Representations (ICLR)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 ConvolutionalSequence to Sequence Learning,2017, In Proc
 Top-down tree structured decoding with syntacticconnections for neural machine translation and parsing,2018, In Proceedings ofthe 2018 Conference on9Published as a conference paper at ICLR 2020Empirical Methods in Natural Language Processing
 Multi-granularity self-attention for neural machine translation,2019, In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing
 Tree-transformer: A transformer-based method forcorrection of tree-structured data,2019, arXiv preprint arXiv:1908
 A structural probe for finding syntax in wordrepresentations,2019, In Proceedings of the 2019 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 The Stanford CoreNLP natural language processing toolkit,2014, In Associationfor Computational Linguistics (ACL) System Demonstrations
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Self-attention with relative positionrepresentations,2018, In Proceedings of the 2018 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies
 Straight to the tree: Constituency parsing with neural syntactic distance,2018, In Proceedingsof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers)
 Ordered neurons: Integratingtree structures into recurrent neural networks,2019, In International Conference on LearningRepresentations
 On tree-based neural sentence modeling,2018, InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing
 Linguistically-informed self-attention for semantic role labeling,2018, In Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Processing
 Improved semantic representationsfrom tree-structured long short-term memory networks,2015, In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguistics and the 7th International JointConference on Natural Language Processing (Volume 1: Long Papers)
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Towards bidirectionalhierarchical representations for attention-based neural machine translation,2017, In Proceedings ofthe 2017 Conference on Empirical Methods in Natural Language Processing
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Multitree transformer: Integrating tree structuresinto self-attention,2019, In Proceedings of the 2019 Conference on Empirical Methods in NaturalLanguage Processing
