title,year,conference
 Strong mixed-integer pro-gramming formulations for trained neural networks,2019, arXiv preprint arXiv:1811
1: Useras manual for CPLEX,2019, 2019
 Benchmarking deep reinforcementlearning for continuous control,2016, In International Conference on Machine Learning
 Deep reinforcement learning in large discrete action spaces,2015, arXivpreprint arXiv:1512
 Output range analysis for deep feedforwardneural networks,2018, In NASA Formal Methods Symposium
 Global convergence of policy gradient methods forthe linear quadratic regulator,2018, In Proceedings of the 35th International Conference on MachineLearning
 Deep neural networks and mixed integer linear optimization,2018, Constraints
 Addressing function approximation error in actor-criticmethods,2018, In Proceedings of the 35th International Conference on Machine Learning
 The SCIP Optimization Suite 6,2018,0
 Continuous deep Q-learning with model-based accel-eration,2016, In Proceedings of The 33rd International Conference on Machine Learning
 Soft actor-critic: Off-policy maximum entropydeep reinforcement learning with a stochastic actor,2018, In Proceedings of the 35th InternationalConference on Machine Learning
 Deep reinforcement learning with double Q-learning,2016, InProceedings of the Thirtieth AAAI Conference on Artificial Intelligence
 Reluplex: An efficient SMT solverfor verifying deep neural networks,2017, In International Conference on Computer Aided Verification
 Equivalent and approximate transformations of deep neuralnetworks,2019, arXiv preprint arXiv:1905
 Actor-Expert: A framework for using action-valuemethods in continuous action spaces,2018, arXiv preprint arXiv:1810
 An approach to reachability analysis for feed-forward ReLU neuralnetworks,2017, arXiv preprint arXiv:1706
 Q-learning with linear function approximation,2007, In International Conferenceon Computational Learning Theory
 Online facility location,2001, In Proceedings 42nd IEEE Symposium on Foundations ofComputer Science
 Envelope theorems for arbitrary choice sets,2002, Econometrica
 Continuous-action Q-learning,2002, Machine Learning
 A unified view of entropy-regularized Markov decision pro-cesses,2017, arXiv preprint arXiv:1705
 Combining trust region and line search techniques,1998, In Advances in nonlinearprogramming
 Deep reinforcement learning forvision-based robotic grasping: A simulated comparative evaluation of off-policy methods,2018, CoRR
 Bounding and counting linear regions of deepneural networks,2018, In International Conference on Machine Learning
 Practical reinforcement learning in continuous spaces,2000, In Proceedingsof the Seventeenth International Conference on Machine Learning
 Policy gradient methods for reinforcementlearning with function approximation,2000, In Advances in Neural Information Processing Systems
 Evaluating robustness of neural networks with mixed integerprogramming,2019, In International Conference on Learning Representations
 Towards fastcomputation of certified robustness for ReLU networks,2018, arXiv preprint arXiv:1804
 Provable defenses against adversarial examples via the convex outer adver-sarial polytope,2017, arXiv preprint arXiv:1711
 Data points are average over a sliding window ofsize 6,1000, The length of an episode is 1000 steps
