title,year,conference
 Finding approximatelocal minima faster than gradient descent,2017, STOC
 Deep speech 2 : End-to-endspeech recognition in english and mandarin,2016, ICML
 Efficient approaches for escaping higher order saddle points innon-convex optimization,2016, COLT
 Accelerated linear convergence of stochasticmomentum methods in wasserstein distances,2019, ICML
 Gradient descent efficiently finds the cubic-regularized non-convexnewton step,2018, NeurIPS
 Gradient descent with randominitialization: Fast global convergence for nonconvex phase retrieval,2018, Mathematical Programming
 Theloss surfaces of multilayer networks,2015, AISTAT
 Autoaugment:Learning augmentation policies from data,2018, arXiv:1805
 Escaping saddles withstochastic gradients,2018, ICML
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, NIPS
 Gradientdescent can take exponential time to escape saddle points,2017, NIPS
 Spider: Near-optimal non-convexoptimization via stochastic path-integrated differential estimator,2018, NeurIPS
 Sharp analysis for nonconvex sgd escaping from saddlepoints,2019, COLT
 Stochastic heavy ball,2016, arXiv:1609
 Shake-shake regularization,2017, arXiv:1705
 Escaping from saddle points â€” online stochasticgradient for tensor decomposition,2015, COLT
 Global convergence of theheavy-ball method for convex optimization,2015, ECC
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Mathematical Programming
 Why momentum really works,2017, Distill
 Deep residual learning for imagerecognition,2016, Conference on Computer Vision and Pattern Recognition (CVPR)
 How to escapesaddle points efficiently,2017, ICML
 Accelerated gradient descent escapes saddlepoints faster than gradient descent,2018, COLT
 Stochastic gradientdescent escapes saddle points efficiently,2019, arXiv:1902
 On the insufficiency ofexisting momentum schemes for stochastic optimization,2018, ICLR
 Adam: A method for stochastic optimization,2015, ICLR
 Sub-sampled cubic regularization for non-convex opti-mization,2017, ICML
 Imagenet classification with deep convolu-tional neural networks,2012, NIPS
 First-order methods almost always avoid strict saddle-points,2019, MathematicalProgramming
 Nonconvex finite-sum optimization viascsg methods,2017, NIPS
 Accelerated gossip via stochastic heavy ball method,2018, Allerton
 Escaping saddle points in constrainedoptimization,2018, NeurIPS
 Some np-complete problems in quadratic and nonlinearprogramming,1987, Mathematical programming
 A generic approach for escaping saddle points,2018, AISTATS
 On the convergence of adam and beyond,2018, ICLR
 Mastering the game of go withouthuman knowledge,2017, Nature
 Escaping saddle pointswith adaptive gradient methods,2019, ICML
 A geometrical analysis of phase retrieval,2016, InternationalSymposium on Information Theory
 Non-ergodicconvergence analysis of heavy-ball algorithms,2019, AAAI
 On the importance of initializationand momentum in deep learning,2013, ICML
 Rmsprop: Divide the gradient by a running average of its recentmagnitude,2012, COURSERA: Neural Networks for Machine Learning
 Stochastic cubicregularization for fast nonconvex optimization,2018, NeurIPS
 Attention is all you need,2017, NIPS
 Themarginal value of adaptive gradient methods in machine learning,2017, NIPS
 First-order stochastic algorithms for escaping from saddlepoints in almost linear time,2018, NeurIPS
 Unified convergence analysis of stochastic momentummethods for convex and non-convex optimization,2018, IJCAI
