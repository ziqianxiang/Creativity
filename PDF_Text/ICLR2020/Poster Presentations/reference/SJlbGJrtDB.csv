title,year,conference
 Designing neural network architec-tures using reinforcement learning,2016, arXiv preprint arXiv:1611
 Deep rewiring: Trainingvery sparse deep networks,2017, arXiv preprint arXiv:1711
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Dynamic network surgery for efficient dnns,2016, InAdvances In Neural Information Processing Systems
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, arXiv preprint arXiv:1808
 Long short-term memory,1997, Neural computation
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Binarizedneural networks,2016, In Advances in neural information processing systems
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing Systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Hier-archical representations for efficient architecture search,2017, arXiv preprint arXiv:1711
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature communications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, arXiv preprint arXiv:1902
 Exploring sparsity in recurrentneural networks,2017, arXiv preprint arXiv:1704
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European Conference on ComputerVision (ECCV)
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Accurate and compact convolutional neural networks with trainedbinarization,2019, In British Machine Vision Conference (BMVC)
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 ResNet-50 models are trained using SGD with momentum 0,1024,9 and batch size of 1024with 90 training epochs
