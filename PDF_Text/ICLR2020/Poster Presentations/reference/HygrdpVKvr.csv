title,year,conference
 MANAS: Multi-Agent Neural Architecture Search,2019, arxiv:1909
 Autoaugment:Learning augmentation policies from data,2018, arXiv:1805
 Improved regularization of convolutional neural netWorksWith cutout,2017, arXiv:1708
 Hark side of deep learning-from grad studentdescent to automated machine learning,2019, arXiv:1904
 Deep residual learning for image recog-nition,2016, In Computer Vision and Pattern Recognition (CVPR)
 Delving deep into rectifiers: Surpassinghuman level performance on imagenet classification,2015, In Computer Vision and Pattern Recognition(CVPR)
 Deep networks withstochastic depth,2016, In European conference on computer vision
 sharpDARTS: Faster and more accurate differ-entiable architecture search,2019, arXiv:1903
 Learning multiple layers of features from tiny images,2009, Technical report
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Fractalnet: Ultra-deep neural networkswithout residuals,2017, In International Conference on Learning Representations (ICLR)
 StacNAS: Towards stable andconsistent optimization for differentiable Neural Architecture Search,2019, arXiv:1909
 Best practices for scientific research on neural architecturesearch,2019, arXiv:1909
 DARTS: Differentiable architecture search,2019, InInternational Conference on Learning Representations (ICLR)
 Xnas:Neural architecture search with expert advice,2019, arXiv:1906
 Recognizing indoor scenes,2009, In Computer Vision and PatternRecognition (CVPR)
 Large-scale evolution of image classifiers,2017, In InternationalConference on Machine Learning (ICML)
 Evaluatingthe search phase of neural architecture search,2019, arXiv:1902
 Automatic convolutional neural architecturesearch for image classification under different scenes,2019, IEEE Access
 Exploring randomly wired neuralnetworks for image recognition,2019, arXiv preprint arXiv:1904
 NAS-Bench-101: Towards reproducible neural architecture search,2019, In International Conference onMachine Learning (ICML 2019)
 Neural architecture search with reinforcement learning,2017, In InternationalConference on Learning Representations (ICLR)
 The LSTM model used to decode architecture has a hidden state size of 96,2014, The encoderand decoder are trained using Adam for 1000 epochs with a learning rate of 0
