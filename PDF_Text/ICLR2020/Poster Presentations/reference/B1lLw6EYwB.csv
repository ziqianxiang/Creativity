title,year,conference
 Making asynchronous stochastic gradient descent work fortransformers,2019, arXiv preprint arXiv:1906
 Revisiting distributedsynchronous sgd,2016, arXiv preprint arXiv:1604
 Geeps: Scalabledeep learning on distributed gpus with a gpu-specialized parameter server,2016, In Proceedings of theEleventh European Conference on Computer Systems
 Slowand stale gradients can win the race: Error-runtime trade-offs in distributed sgd,2018, arXiv preprintarXiv:1803
 Model accuracy and runtime tradeoff in distributed deePlearning: A systematic study,2016, In 2016 IEEE 16th International Conference on Data Mining(ICDM)
 Taming momentum in a distributedasynchronous environment,2019, arXiv preprint arXiv:1907
 Distributed deep learning on edge-devices: Feasibility viaadaptive compression,2017, In 2017 IEEE 16th International Symposium on Network Computing andAPPlications(NCA)
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Heterogeneity-aware distributed parameter servers,2017, InProceedings of the 2017 ACM International Conference on Management of Data
 Asynchronous parallel stochastic gradient fornonconvex optimization,2015, In Advances in Neural Information Processing Systems
 Pointer sentinel mixturemodels,2016, CoRR
 Measuring the effects of data parallelism on neural network training,2018, CoRR
 On the importance ofinitialization and momentum in deep learning,2013, In Proceedings of the 30th International Conferenceon Machine Learning
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation Processing systems
 Yet another acceleratedSGD: resnet-50 training on imagenet in 74,2019,7 seconds
 Image classification atsupercomputer scale,2018, CoRR
 Deep learning with elastic averaging SGD,2015, InAdvances in Neural Information Processing Systems 28: Annual Conference on Neural InformationProcessing Systems
 Staleness-aware async-sgd for distributed deeplearning,2015, CoRR
1•	Momentum Coefficient γ : 0,2017,9 with NAG•	Dampening: 0 (no dampening)•	Batch Size B: 128•	Weight Decay: 0
 In Section 5,2015,3
