title,year,conference
 Unsupervisedlabel noise modeling and loss correction,2019, In ICML
 A closer look at memorization in deep networks,2017, In ICML
 Mixmatch: A holistic approach to semi-supervised learning,2019, NeurIPS
 Understanding and utilizingdeep neural networks trained with noisy labels,2019, In ICML
 A semi-supervised two-stage approachto learning from noisy labels,2018, In WACV
 Training deep neural-networks using a noise adaptationlayer,2017, In ICLR
 Semi-supervised learning by entropy minimization,2004, In NIPS
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Identity mappings in deep residualnetworks,2016, In ECCV
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In NeurIPS
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In ICML
 Recycling: Semi-supervised learning with noisy labels in deep neural networks,2019, IEEEAccess
 Learning multiple layers of features from tiny images,2009, Mater’sthesis
 Temporal ensembling for semi-supervised learning,2017, In ICLR
 Pseudo-label: The simple and efficient semi-supervised learning method for deepneural networks,2013, In ICML Workshop on Challenges in Representation Learning
 Cleannet: Transfer learning for scalableimage classifier training with label noise,2018, In CVPR
 Learning to learn from noisylabeled data,2019, In CVPR
 Learning fromnoisy labels with distillation,2017, In ICCV
 Dimensionality-driven learning with noisy labels,2018, InICML
 Decoupling “when to update” from “how to update”,2017, InNIPS
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Regularizingneural networks by penalizing confident output distributions,2017, In ICLR Workshop
 A study of gaussian mixture models ofcolor and texture features for image classification and segmentation,2006, Pattern Recognition
 Training deep neural networks on noisy labels with bootstrapping,2015, In ICLR
 Learning with bad training data via iterative trimmed lossminimization,2019, In ICML
 Joint optimization frameworkfor learning with noisy labels,2018, In CVPR
 Learning from noisy labels by regularized estimation of annotator confusion,2019, In CVPR
 Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results,2017, In NIPS
 Combating label noise in deep learning using abstention,2019, In ICML
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 Probabilistic end-to-end noise correction for learning with noisy labels,2019, InCVPR
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 mixup: Beyond empiricalrisk minimization,2018, In ICLR
 Metacleaner: Learning to hallucinate clean representationsfor noisy-labeled visual recognition,2019, In CVPR
