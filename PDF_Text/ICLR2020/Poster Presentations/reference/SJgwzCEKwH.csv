title,year,conference
 Adversarial examples from computationalconstraints,2019, International Coference on International Conference on Machine Learning
 Towards evaluating the robustness of neural networks,2017, In IEEESymposium on Security and Privacy
 Detecting backdoor attacks on deep neural networks byactivation clustering,2018, arXiv preprint arXiv:1811
 Limitations of adversarial robustness: strong no free lunch theorem,2018, InternationalConference on International Conference on Machine Learning
 Essentially no barriers inneural network energy landscape,2018, In International Conference on Machine Learning
 The bernstein polynomial basis: A centennial retrospective,0167, Computer AidedGeometric Design
 The robustness of deepnetworks: A geometrical perspective,2017, IEEE Signal Processing Magazine
 Empiricalstudy of the topology and geometry of deep networks,2018, In IEEE Conference on Computer Visionand Pattern Recognition
 Explaining and harnessing adversarialexamples,2015, International Conference on Learning Representations
 Using mode connec-tivity for loss landscape analysis,2018, arXiv preprint arXiv:1806
 BadNets: Evaluating backdooring attacks on deepneuralnetworks,2019, IEEEAccess
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Black-box adversarial attacks withlimited queries and information,2018, International Coference on International Conference on MachineLearning
 Pruning filters forefficient convnets,2017, International Conference on Learning Representations
 Trojaning attack on neural networks,2018, In Network and Distributed System SecuritySymposium (NDSS)
 Training deep and recurrent networks with hessian-free optimiza-tion,2012, In Neural networks: Tricks of the trade
 Transferability in machine learning: fromphenomena to black-box attacks using adversarial samples,2016, arXiv preprint arXiv:1605
 Practical black-box attacks against machine learning,2017, In ACM Asia Conference onComputer and Communications Security
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Spectral signatures in backdoor attacks,2018, In Advancesin Neural Information Processing Systems
 Ro-bustness may be at odds with accuracy,2019, In International Conference on Learning Representations
 Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,2019, In IEEESymposium on Security and Privacy
 Identifying generalizationproperties in neural networks,2018, arXiv preprint arXiv:1809
 Towards robustdeep neural networks,2018, arXiv preprint arXiv:1810
 Towards understanding generalization of deep learning: Perspective ofloss landscapes,2017, In International Conference on Machine Learning
 Interpreting adversarial robustness: Aview from decision surface in input space,2018, arXiv preprint arXiv:1810
 An admm-based universal framework for adversarialattacks on deep neural networks,2018, In ACM Multimedia 2018
 On thedesign of black-box adversarial examples by leveraging gradient-free optimization and operatorsplitting method,2019, In ICCV 2019
 Fault sneaking attack:a stealthy framework for misleadingdeep neural networks,2019, DAC
 Towards query-efficient black-box adversary withzeroth-order natural gradient descent,2020, In AAAI 2020
 The target labels of the 4 selectedimages are different from their original correct labels,2020, The goal of the attacker is to change theclassification of the 4 images to the target labels while keeping the classification of the remaining 996images unchanged through modifying the model parameters
 The dataset is CIFAR-10 and the model architecture is ResNet,2020, For each bonafide26Published as a conference paper at ICLR 2020data size
