title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In 3rd International Conference on Learning Representations
 On structural identifiability,1970, Mathematical Biosciences
 What does BERT lookat? an analysis of bert’s attention,2019, CoRR
 Visualizing and measuring the geometry of BERT,2019, CoRR
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the Thirteenth International Conference on Artificial Intelligence andStatistics
 Bridging nonlinearities and stochastic regularizers with gaussianerror linear units,2016, CoRR
 Attention is not explanation,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Albert: A lite bert for self-supervised learning of language representations,2020, In InternationalConference on Learning Representations
 Open sesame: Getting inside bert’s linguistic knowl-edge,2019, arXiv preprint arXiv:1906
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Extracting syntactic trees from transformer encoder self-attentions,2018, In Proceedings of the Workshop: Analyzing and Interpreting Neural Networks forNLP
 Investigating the successes and failures ofBERT for passage re-ranking,2019, CoRR
 Dissecting contextualword embeddings: Architecture and representation,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing
 Evaluating neural network explanation methodsusing hybrid documents and morphosyntactic agreement,2018, In Proceedings of the 56th AnnualMeeting of the Association for Computational Linguistics
 Learningto deceive with attention-based explanations,2019, arXiv preprint arXiv:1909
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 An analysis of encoder representations in transformer-based machine translation,2018, In Proceedings of the Workshop: Analyzing and Interpreting NeuralNetworks for NLP
 Deep inside convolutional networks:Visualising image classification models and saliency maps,2014, In 2nd International Conference onLearning Representations
 BERT rediscovers the classical NLP pipeline,2019, InProceedings of the 57th Conference of the Association for Computational Linguistics
 Feature-rich part-of-speech tagging with a cyclic dependency network,2003, In Human Language Technology Conferenceof the North American Chapter of the Association for Computational Linguistics
 Visualizing attention in transformer-based language representation models,2019, CoRR
 Attending to mathematical language with transformers,2018,	CoRR
 Attention is not not explanation,2019, CoRR
 Adding interpretable attention to neural transla-tion models improves word alignment,2019, CoRR
1 to calculate the contribution of input tokens to agiven embedding does not look at the output of the model but at the intermediate hidden representa-tions and therefore is task independent,2019, Since the contribution values do not depend on the task thatis evaluated
