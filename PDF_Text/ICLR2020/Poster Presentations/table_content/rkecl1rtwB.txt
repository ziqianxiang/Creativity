Table 1: Comparison of ‘vanilla’ vs. PAIRNORM-enhanced SGC performance in Cora, Citeseer,Pubmed, and CoauthorCS for SSNC-MV problem, with missing rate ranging from 0% to 100%.
Table 2: Comparison of ‘vanilla’ and (PairNorm-si/ residual)-enhanced GCN performance onCora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% featuremissing rate. t represents the skip-step of residual connection. (See A.5 Fig. 8 for more settings.)Dataset	Cora		Citeseer		Pubmed		CoauthorCS	Missing(%)	0%	100%	0%	100%	0%	100%	0%	100%Method	ACC #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #LGCN	0.821	2	0.582 2	0.695	2	0.313	2	0.779 2	0.449 2	0.877 2	0.452 4GCN-PN	0.790 2	0.731 10	0.660 2	0.498 8	0.780 30	0.745 25	0.910 2	0.846 12GCN-t1	0.822 2	0.721 15	0.696 2	0.441 12	0.780 2	0.656 25	0.898 2	0.727 12GCN-t1-PN	0.780 2	0.724 30	0.648 2	0.465 10	0.756 15	0.690 12	0.898 2	0.830 20GCN-t2	0.820 2	0.722 10	0.691	2	0.432 20	0.779 2	0.645 20	0.882 4	0.630 20GCN-t2-PN	0.785 4	0.740 30	0.650 2	0.508 12	0.770 15	0.725 30	0.911	2	0.839 20Table 3: Comparison of ‘vanilla’ and (PairNorm-si/ residual)-enhanced GAT performance onCora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% featuremissing rate. t represents the skip-step of residual connection. (See A.5 Fig. 9 for more settings.)Dataset	Cora		Citeseer		Pubmed		CoauthorCS	Missing(%)	0%	100%	0%	100%	0%	100%	0%	100%Method	Acc #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #LGAT	0.823 2	0.653 4	0.693	2	0.428 4	0.774 6	0.631	4	0.892 4	0.737 4GAT-PN	0.787 2	0.718 6	0.670 2	0.483 4	0.774 12	0.714 10	0.916 2	0.843	8
Table 3: Comparison of ‘vanilla’ and (PairNorm-si/ residual)-enhanced GAT performance onCora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% featuremissing rate. t represents the skip-step of residual connection. (See A.5 Fig. 9 for more settings.)Dataset	Cora		Citeseer		Pubmed		CoauthorCS	Missing(%)	0%	100%	0%	100%	0%	100%	0%	100%Method	Acc #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #L	Acc #LGAT	0.823 2	0.653 4	0.693	2	0.428 4	0.774 6	0.631	4	0.892 4	0.737 4GAT-PN	0.787 2	0.718 6	0.670 2	0.483 4	0.774 12	0.714 10	0.916 2	0.843	8GAT-t1	0.822 2	0.706 8	0.693	2	0.461	6	0.769 4	0.698 8	0.899 4	0.842 10GAT-t1-PN	0.787 2	0.710 10	0.658 6	0.500 10	0.757 4	0.684 12	0.911	2	0.844 20GAT-t2	0.820 2	0.691	8	s0.692 2	0.461	6	0.774 8	0.702 8	0.895 4	0.803	6GAT-t2-PN	0.788 4	0.738 12	0.672 4	0.517 10	0.776 15	0.704 12	0.917 2	0.855 305	Related WorkOversmoothing in GNNs: Li et al. (2018) was the first to call attention to the oversmoothing prob-lem. Xu et al. (2018) introduced Jumping Knowledge Networks, which employ skip connectionsfor multi-hop message passing and also enable different neighborhood ranges. Klicpera et al. (2019)proposed a propagation scheme based on personalized Pagerank that ensures locality (via teleports)which in turn prevents oversmoothing. Li et al. (2019) built on ideas from ResNet to use residual aswell as dense connections to train deep GCNs. DropEdge Rong et al. (2019) proposed to alleviateoversmoothing through message passing reduction via removing a certain fraction of edges at ran-
Table 4: DataSet statistics.
