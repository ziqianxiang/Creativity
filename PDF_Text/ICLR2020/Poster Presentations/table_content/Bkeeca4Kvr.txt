Table 1: Results for various few-shot scenarios on Letter-High and TRIANGLES datasets. The bestresults are highlighted in bold while the second best results are underlined.
Table 2: Results for various few-shot scenarios on Reddit-12K and ENZYMES datasets. The bestresults are highlighted in bold while the second best results are underlined.
Table 3: Ablation Study: “No-SC” represents our classifier C(.) without Csup and “With-SC”represents C(.) with both Csup and CGAT present.
Table 4: Model analysis over number of super-classes in 20-shot scenario. There is no evaluation for5 super-classes on ENZYMES since the number of training classes is 4. Default value of parameterk is fixed at 2.
Table 5: Model analysis over number of neighbors (k) in super-graph for 20-shot scenario. Defaultvalue for the number of super-classes is fixed at 3.
Table 6: Dataset StatisticsDataset Name # Classes # Graphs Avg # Nodes Avg # EdgesReddit-12K	11	11929	391.41	456.89ENZYMES	6	600	32.63	62.14Letter-High	15	2250	4.67	4.50TRIANGLES	10	45000	20.85	35.50Dataset Description: Reddit-12K datasets contains 11929 graphs where each graph corresponds toa thread in which each node represents a user and each edge represents that one user has respondedto a comment from some other user. There are 11 different types of discussion forums correspondingto each of the 11 classes.
Table 7: Dataset SplitsDataset Name	# Train Classes	# Test Classes	# Training Graphs	# Validation Graphs	# Test GraphsReddit-12K	7	4	566	141	404ENZYMES	4	2	320	80	200Letter-High	11	4	1330	320	600TRIANGLES	7	3	1126	271	603The validation graphs are used to assess model performance on training classes itself to check over-fitting as well as for grid-search over hyperparameters. The actual train-testing class splits used forthis paper are provided with the code. Since the TRIANGLES dataset has a large number of sam-ples, this makes it infeasible to run many baselines including DL and non-DL methods. Hence, wesample 200 graphs from each class, making the total sample size 2000. Similarly we downsamplethe number of graphs from 11929 to 1111 (nearly 101 graphs per class). Downsampling is per-formed for Reddit-12K given extremely large graph sizes which makes the graph kernels as well assome deep learning baselines extremely slow.
Table 8: Silhouette coefficients of the test classes for the three dominant models - GAT variant ofOur Method, GIN and WL. The best scores are highlighted in bold.
Table 9: Semi-supervised fine-tuning results for various p values on 10-shot and 20-shot scenarios,where “No Semi-Sup” represents the fine-tuning stage without additional labeled samples.
Table 10: Active Learning Results. The value below each shot represents the number samples l,added to GN for second fine-tuning step, where “No AL” represents the model evaluation withoutadditional labeled samples.
Table 11: Silhouette coefficients of the test classes for three models - GAT variant of Our Methodfor 1 super-class which is equivalent to not using any super-classes vs the best performing number ofsuper-classes as well as GIN and WL on 20-shot scenario. For GIN and WL both the sub-columnscontain the same values as they don’t have any concept of super-classes.
