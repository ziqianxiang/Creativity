Table 1: Absolute Pearson correlations with system-level human judgments on WMT18. For eachlanguage pair, the left number is the to-English correlation, and the right is the from-English. Webold correlations of metrics not significantly outperformed by any other metric under Williams Testfor that language pair and direction. The numbers in parenthesis are the number of systems used foreach language pair and direction.
Table 2: Absolute Pearson correlations with system-level human judgments on WMT18. We use10K hybrid super-sampled systems for each language pair and direction. For each language pair, theleft number is the to-English correlation, and the right is the from-English. Bolding criteria is thesame as in Table 1.
Table 3: Model selection accuracies (Hits@1) on WMT18 hybrid systems. We report the average of100K samples and the 0.95 confidence intervals are below 10-3. We bold the highest numbers foreach language pair and direction.
Table 4: Kendall correlations with segment-level human judgments on WMT18. For each languagepair, the left number is the to-English correlation, and the right is the from-English. We bold corre-lations of metrics not significantly outperformed by any other metric under bootstrap sampling forthat language pair and direction. The numbers in parenthesis are the number of candidate-referencesentence pairs for each language pair and direction.
Table 5: Pearson	correlation on the			Fbert (idf)	0.777	0.6932015 COCO Captioning Challenge.
Table 6: Area under ROC curve (AUC) on QQPand PAWSQQP datasets. The scores of trained De-cATT (Parikh et al., 2016), DIIN (Gong et al., 2018),and fine-tuned BERT are reported by Zhang et al.
Table 7: Examples sentences where similarity ranks assigned by Human, FBERT, and BLEU differsignificantly on WMT16 German-to-English evaluation task. x: gold reference, X: candidate outputsof MT systems. Rankings assigned by Human, FBERT, and BLEU are shown in the right threecolumns. The sentences are ranked by the similarity, i.e. rank 1 is the most similar pair assigned bya score. An ideal metric should rank similar to humans.
Table 8: Recommended layer of representation to use for BERTScore. The layers are chosenbased on a held-out validation set (WMT16).
Table 9: Ablation Study of MoverScore and BERTScore using Pearson correlations on theWMT17 to-English segment-level data. Correlations that are not outperformed by others for thatlanguage pair under Williams Test are bolded. We observe that using WMD does not consistentlyimprove BERTSCORE.
Table 10: Pearson correlations with human judgments on the MSR Abstractive Text CompressionDataset.
Table 11: Bleu scores and BERTScores of publicly available pre-trained MT models infairSeq (Ott et al., 2019). We show both rescaled scores marked with ^ and raw BERTSCOREscores. *: trained on unconfirmed WMT data version, **: trained on WMT16 + ParaCrawl, ***:trained on WMT16, + : trained by us using fairseq.
Table 12: Pearson correlations with segment-level human judgments on WMT16 to-English trans-lations. Correlations of metrics not significantly outperformed by any other for that language pairare highlighted in bold. For each language pair, we specify the number of examples.
Table 13: Absolute Pearson correlations with segment-level human judgments on WMT17 to-English translations. Correlations of metrics not significantly outperformed by any other for thatlanguage pair are highlighted in bold. For each language pair, we specify the number of examples.
Table 14: Absolute Pearson correlation (|r|) and Kendall correlation (τ) with segment-level humanjudgments on WMT17 from-English translations. Correlations of metrics not significantly outper-formed by any other for that language pair are highlighted in bold. For each language pair, wespecify the number of examples.
Table 15: Absolute Pearson correlations with system-level human judgments on WMT17 to-Englishtranslations. Correlations of metrics not significantly outperformed by any other for that languagepair are highlighted in bold. For each language pair, we specify the number of systems.
Table 16: Absolute Pearson correlations with system-level human judgments on WMT17 from-English translations. Correlations of metrics not significantly outperformed by any other for thatlanguage pair are highlighted in bold. For each language pair, we specify the number of systems.
Table 17: Kendall correlations with segment-level human judgments on WMT18 to-English transla-tions. Correlations of metrics not significantly outperformed by any other for that language pair arehighlighted in bold. For each language pair, we specify the number of examples.
Table 18: Kendall correlations with segment-level human judgments on WMT18 from-Englishtranslations. Correlations of metrics not significantly outperformed by any other for that languagepair are highlighted in bold. For each language pair, we specify the number of examples.
Table 19: Absolute Pearson correlations with system-level human judgments on WMT18 to-Englishtranslations. Correlations of metrics not significantly outperformed by any other for that languagepair are highlighted in bold. For each language pair, we specify the number of systems.
Table 20: Absolute Pearson correlations with system-level human judgments on WMT18 from-English translations. Correlations of metrics not significantly outperformed by any other for thatlanguage pair are highlighted in bold. For each language pair, we specify the number of systems.
Table 21: Absolute Pearson correlations with human judgments on WMT18 to-English languagepairs for 10K hybrid systems. Correlations of metrics not significantly outperformed by any otherfor that language pair are highlighted in bold. For each language pair, we specify the number ofsystems.
Table 22: Absolute Pearson correlations with human judgments on WMT18 from-English languagepairs for 10K hybrid systems. Correlations of metrics not significantly outperformed by any otherfor that language pair are highlighted in bold. For each language pair, we specify the number ofsystems.
Table 23: Model selection accuracies (Hits@1) on to-English WMT18 hybrid systems. We reportthe average of 100K samples and the 0.95 confidence intervals are below 10-3 . We bold the highestnumbers for each language pair and direction.
Table 24: Mean Reciprocal Rank (MRR) of the top metric-rated system on to-English WMT18hybrid systems. We report the average of 100K samples and the 0.95 confidence intervals are below10-3 . We bold the highest numbers for each language pair and direction.
Table 25: Absolute Difference (×100) of the top metric-rated and the top human-rated system on to-English WMT18 hybrid systems. Smaller difference signify higher agreement with human scores.
Table 26: Model selection accuracies (Hits@1) on to-English WMT18 hybrid systems. We reportthe average of 100K samples and the 0.95 confidence intervals are below 10-3 . We bold the highestnumbers for each language pair and direction.
Table 27: Mean Reciprocal Rank (MRR) of the top metric-rated system on to-English WMT18hybrid systems. We report the average of 100K samples and the 0.95 confidence intervals are below10-3 . We bold the highest numbers for each language pair and direction.
Table 28: Absolute Difference (×100) of the top metric-rated and the top human-rated system on to-English WMT18 hybrid systems. Smaller difference indicate higher agreement with human scores.
Table 29: Pearson correlation on the 2015 COCO Captioning Challenge. The M1 and M2 measuresare described in Section 4. We bold the best correlating task-specific and task-agnostic metrics ineach setting LEIC uses images as additional inputs. Numbers with * are cited from CUi et al. (2018).
Table 30: Area under ROC curve (AUC) on QQP and PAWSQQP datasets. The scores of trainedDecATT (Parikh et al., 2016), DIIN (Gong et al., 2018), and fine-tuned BERT are reported by Zhanget al. (2019). We bold the best task-specific and task-agnostic metrics. Numbers with * are scoreson the held-out test set of QQP.
