Table 1: Comparison of classification accuracy when learning under uniform label noise on CIFAR-10 and CIFAR-100. Following previous works, we compare two evaluation scenarios: with a noisyvalidation set (top) and with 1000 clean validation samples (bottom). The best model is marked inbold. Having a small clean validation set improves the model but is not necessary.
Table 2: Asymmetric noise on CIFAR-10, CIFAR-100. All methods use Resnet34. CIFAR-10: flipTRUCK → AUTOMOBILE, BIRD → AIRPLANE, DEER → HORSE, CAT什DOG with prob. p.
Table 3: Effect of the choice of network architecture on classification accuracy on CIFAR-10 &-100 with uniform label noise. SELF is compatible with all tested architectures. Here * representsbaseline accuracy of the architectures that are trained on fully supervised setting at 0% label noise.
Table 4: Classification accuracy on cleanImageNet validation dataset. The mod-els are trained at 40% label noise and thebest model is picked based on the evalu-ation on noisy validation data. Mentornetshows the best previously reported results.
Table 5: Ablation study on CIFAR-10 and CIFAR-100. The Resnet baseline was trained on the fullnoisy label set. Adding progressive filtering im-proves over this baseline. The Mean Teacher main-tains an ensemble of model snapshots, which helpscounteract noise. Having progressive filtering andmodel ensembles (-MVA-pred.) makes the modelmore robust but still fails at 80% noise. The fullSELF framework additionally uses the predictionensemble for detection of correct labels.
Table 6: Analysis of semi-supervised learning (SSL) strategies: entropy learning, mean-teachercombined with recent works. Our progressive filtering strategy is shown to be effective and per-forms well regardless of the choice of the semi-supervised learning backbone. Overall, the proposedmethod SELF outperforms all these combinations. Best model in each SSL-category is markedin bold. Running mean-teacher+ co-teaching using the same configuration is not possible due tomemory constraints.
Table 7: Dataset description. Classification tasks on CIFAR-10 and CIFAR-100 with uniform noise.
Table 8: Accuracy of the complete removal of samples during iterative filtering on CIFAR-10 andCIFAR-100. The underlying model is the MeanTeacher based on Resnet26. When samples are com-pletely removed from the training set, they are no longer used for either supervised-or-unsupervisedlearning. This common strategy from previous works leads to rapid performance breakdown.
