Table 1: Test error rates of FCN on MNIST. Subscripts denote standard deviations, and bracketednumbers denote relative gains with respect to MP. Unpruned models have 1.98% error rate.
Table 2: Test error rates of Conv-6 on CIFAR-10. Subscripts denote standard deviations, and brack-eted numbers denote relative gains with respect to MP. Unpruned models have 11.97% error rate.
Table 3: Test error rates of VGG-19 on CIFAR-10. Subscripts denote standard deviations, andbracketed numbers denote relative gains with respect to MP. Unpruned models have 9.02% error rate.
Table 4: Test error rates of ResNet-18 on CIFAR-10. Subscripts denote standard deviations, andbracketed numbers denote relative gains with respect to MP. Unpruned models have 8.68% error rate.
Table 5: Top-5 test error rates of VGG-19 on Tiny-ImageNet. Subscripts denote standard deviations,and bracketed numbers denote relative gains with respect to MP. Unpruned models have 36.89%error rate. Top-1 test error rates are presented in Table 10.
Table 6: Top-5 test error rates of ResNet-50 on Tiny-ImageNet. Subscripts denote standard de-viations, and bracketed numbers denote relative gains with respect to MP. Unpruned models have23.19% error rate. Top-1 test error rates are presented in Table 11.
Table 7: Top-5 test error rates of WRN-16-8 on Tiny-ImageNet. Subscripts denote standard de-viations, and bracketed numbers denote relative gains with respect to MP. Unpruned models have25.77% error rate. Top-1 test error rates are presented in Table 12.
Table 8: Test error rates of VGG-11 on CIFAR-10. Subscripts denote standard deviations, and brack-eted numbers denote relative gains with respect to MP. Unpruned models have 11.51% error rate.
Table 9: Test error rates of VGG-16 on CIFAR-10. Subscripts denote standard deviations, andbracketed numbers denote relative gains with respect to MP. Unpruned models have 9.33% error rate.
Table 10: Top-1 test error rates of VGG-19 on Tiny-ImageNet. Subscripts denote standard devi-ations, and bracketed numbers denote relative gains with respect to MP. Unpruned models have64.55% error rate.
Table 11: Top-1 test error rates of ResNet-50 on Tiny-ImageNet. Subscripts denote standard de-viations, and bracketed numbers denote relative gains with respect to MP. Unpruned models have47.50% error rate.
Table 12: Top-1 test error rates of WRN-16-8 on Tiny-ImageNet. Subscripts denote standard de-viations, and bracketed numbers denote relative gains with respect to MP. Unpruned models have51.85% error rate.
Table 13: Test error rates of FCN on MNIST. Subscripts denote standard deviations, and bracketednumbers denote relative gains with respect to OBD. Unpruned models achieve 1.98% error rate.
Table 14: Test error rates of Conv-6 on CIFAR-10. Subscripts denote standard deviations, andbracketed numbers denote relative gains with respect to OBD. Unpruned models achieve 11.97%error rate.
Table 15: Computation time of OBD, OBD+LAP and LAP-act (averaged over 100 trials).
Table 16: Computation time ofMP and LAP on FCN, Conv-6, VGG-{11,16,19}, ResNet-18. Allfigures are averaged over 100 independent trials. Bracketed numbers denote relative increments.
Table 17: Test error rates of Conv-6 on CIFAR-10 for channel pruning. Subscripts denote standarddeviations, and bracketed numbers denote relative gains with respect to the best of MP-'1 and MP-'2 . Unpruned models achieve 11.97% error rate.
Table 18: Test error rates of VGG-19 on CIFAR-10 for channel pruning. Subscripts denote standarddeviations, and bracketed numbers denote relative gains with respect to the best of MP-'1 and MP-'2 . Unpruned models achieve 9.02% error rate.
Table 19: Test error rates of FCN on MNIST for global pruning. Subscripts denote standard devia-tions, and bracketed numbers denote relative gains with respect to MP-normalize (for data-agnosticalgorithms) and OBD-normalize (for data-dependent algorithms), respectively. Unpruned modelsachieve 1.98% error rate.
Table 20: Test error rates of Conv-6 on CIFAR-10 for global pruning. Subscripts denote standarddeviations, and bracketed numbers denote relative gains with respect to MP-normalize (for data-agnostic algorithms) and OBD-normalize (for data-dependent algorithms), respectively. Unprunedmodels achieve 11.97% error rate.
Table 21: Test error rates of FCN on MNIST, with LAP-all variant. Subscripts denote standarddeviations. Unpruned models achieve 1.98% error rate.
Table 22: Test error rates of various networks on CIFAR-10. Subscripts denote standard deviations,and bracketed numbers denote relative gains with respect to the unpruned MobileNetV2.
