Table 1: The prune ratio (PR) and the cor-responding test error (Err.) of the sparsestnetwork - With commensurate accuracy -generated by each algorithm.
Table 2: Overview of the pruning performance of each algorithm for various CNN architectures. For eachalgorithm and network architecture, the table reports the prune ratio (PR, %) and pruned Flops ratio (FR, %) ofpruned models when achieving test accuracy within 0.5% of the original networkâ€™s test accuracy (or the closestresult when the desired test accuracy was not achieved for the range of tested PRs). Our results indicate that ourpruning algorithm generates smaller and more efficient networks with minimal loss in accuracy, when comparedto competing approaches.
Table 3: We report the hyperparameters used during MNIST training,pruning, and fine-tuning for the LeNet architectures. LR hereby denotesthe learning rate and LR decay denotes the learning rate decay that wedeploy after a certain number of epochs. During fine-tuning we used thesame hyperparameters except for the ones indicated in the lower part ofthe table.
Table 4: We report the hyperparameters used during training, pruning, and fine-tuning for various convolutionalarchitectures on CIFAR-10. LR hereby denotes the learning rate and LR decay denotes the learning rate decaythat we deploy after a certain number of epochs. During fine-tuning we used the same hyperparameters exceptfor the ones indicated in the lower part of the table. {30, . . .} denotes that the learning rate is decayed every 30epochs.
Table 6: Comparisons of the performance of various pruning algorithms on ResNets trained on ImageNet (Rus-sakovsky et al., 2015). The reported results for the competing algorithms were taken directly from the corre-sponding papers. For each network architecture, the best performing algorithm for each evaluation metric, i.e.,Pruned Err., Err. Diff, PR, and FR, is shown in bold.
Table 7: We report the hyperparameters used for trainingand pruning the driving network of Amini et al. (2018)together with the provided data set. No fine-tuning wasconducted for this architecture. LR hereby denotes thelearning rate, LR decay denotes the learning rate decaythat we deploy after a certain number of epochs, andMSE denotes the mean-squared error.
Table 8: The performance of our algorithm and that of state-of-the-art filter pruning algorithms on modern CNNarchitectures trained on CIFAR-10. The reported results for the competing algorithms were taken directly fromthe corresponding papers. For each network architecture, the best performing algorithm for each evaluationmetric, i.e., Pruned Err., Err. Diff, PR, and FR, is shown in bold. The results show that our algorithm consistentlyoutperforms state-of-the-art pruning approaches in nearly all of the relevant pruning metrics.
