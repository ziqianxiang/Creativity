Table 1: Freezing successive layers (preventing inner loop adaptation) does not affect accuracy, support-ing feature reuse. To test the amount of feature reuse happening in the inner loop adaptation, we test theaccuracy of the model when we freeze (prevent inner loop adaptation) a contiguous block of layers at test time.
Table 2: ANIL matches the performance of MAML on few-shot image classification and RL. On bench-mark few-shot classification tasks MAML and ANIL have comparable accuracy, and also comparable averagereturn (the higher the better) on standard RL tasks (Finn et al., 2017).
Table 3: MAML and ANIL models learn comparable representations. Comparing CCA/CKA similarityscores of the of MAML-ANIL representations (averaged over network body), and MAML-MAML and ANIL-ANIL similarity scores (across different random seeds) shows algorithmic differences between MAML/ANILdoes not result in vastly different types of features learned.
Table 4: NIL algorithm performs as well as MAML and ANIL on few-shot image classification. Perfor-mance of MAML, ANIL, and NIL on few-shot image classification benchmarks. We see that with no test-timeinner loop, and just learned features, NIL performs comparably to MAML and ANIL, indicating the strength ofthe learned features, and the relative lack of importance of the head at test time.
Table 5: MAML/ANIL training leads to superior features learned, supporting importance of head attraining. Training with MAML/ANIL leads to superior performance over other methods which do not have taskspecific heads, supporting the importance of the head at training.
Table 6: ANIL offers significant computational speedup over MAML, during both training and inference.
Table 7: Test time performance is dominated by features learned, with no difference betweenNIL/MAML heads. We see identical performances of MAML/NIL heads at test time, indicating thatMAML/ANIL training leads to better learned features.
Table 8: MAML training most closely resembles multiclass pretraining, as illustrated by CCA and CKAsimilarities. On analyzing the CCA and CKA similarities between different baseline models and MAML(comparing across different tasks and seeds), we see that multiclass pretraining results in features most similar toMAML training. Multitask pretraining differs quite significantly from MAML-learned features, potentially dueto the alignment problem.
