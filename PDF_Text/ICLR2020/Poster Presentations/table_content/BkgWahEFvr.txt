Table 1: Hyperparameter settings for the four adversarial attack methods on the MNIST dataset.
Table 2: Hyperparameter settings for the four adversarial attack methods on the CIFAR10 dataset.
Table 3: Hyperparameter settings for the four adversarial attack methods on the CIFAR100 dataset.
Table 4: Details of the image transformation parameters for MNIST. The three transformation-basedmethods tested are random pixel noise (RPN), pixel deflection (PD) and random resize and padding(RRP). For RPN, the noise magnitude is unnormalized (out of 255). For PD, d is the number ofdeflections, w is the window size and Ïƒ is the denoising parameter.
Table 5: Details of the image transformation parameters for CIFAR10.
Table 6: Details of the image transformation parameters for CIFAR100.
Table 7: Performance for pixel deflection on the CIFAR10 dataset with and without class activationmaps (CAM). Numbers shown are the test accuracies (%).
Table 8: DRN network architecture for the distribution classifier, where 2x10 represents 2 hiddenlayers of 10 nodes.
Table 9: MLP network architecture for the distribution classifier, where 2x10 represents 2 hiddenlayers of 10 nodes.
Table 10: Random forest hyperparameters for the distribution classifier, where n represents thenumber of trees and d represents the maximum depth of the trees.
Table 11: MNIST results: For each attack, we compare the clean and adversarial (adv.) test accura-cies with majority voting (Vote) and the three distribution classifier methods: distribution regressionnetwork (DRN), random forest (RF) and multilayer perceptron (MLP). The three transformation-based defenses are random pixel noise (RPN), pixel deflection (PD) and random resize and padding(RRP). With no defense, the clean accuracy is 100% and the adversarial accuracy is 0%.
Table 12: CIFAR10 results: For each attack, we compare the clean and adversarial (adv.) test accura-cies with majority voting (Vote) and the three distribution classifier methods: distribution regressionnetwork (DRN), random forest (RF) and multilayer perceptron (MLP). The three transformation-based defenses are random pixel noise (RPN), pixel deflection (PD) and random resize and padding(RRP). With no defense, the clean accuracy is 100% and the adversarial accuracy is 0%.
Table 13: CIFAR100 results: For each attack, we compare the clean and adversarial (adv.) testaccuracies with majority voting (Vote) and the three distribution classifier methods: distribu-tion regression network (DRN), random forest (RF) and multilayer perceptron (MLP). The threetransformation-based defenses are random pixel noise (RPN), pixel deflection (PD) and random re-size and padding (RRP). With no defense, the clean accuracy is 100% and the adversarial accuracyis 0%.
Table 14: Distribution classifier outperforms majority voting for clean images that are misclassifiedby CNN and images where the attack has failed. The results are for CIFAR100 with FGSM attack,random resize and padding and random forest classifier.
