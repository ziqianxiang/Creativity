Table 1: Top-1 test error (%) on CIFAR-10. We replicate the results of Baseline, Cutout and Au-toAugment methods from Cubuk et al. (2019), and the results of PBA from Ho et al. (2019) in all ofour experiments.
Table 2: Top-1 test error (%) on CIFAR-100.
Table 3: Top-1 / Top-5 test error (%) on ImageNet. Note that the result of ResNet-50-D is achievedonly through substituting the architecture.
Table 4: Top-1 test error (%) of ResNet-50 with different augmentation methods on ImageNet.
Table 5: The comparison of computing cost (GPU hours) and time overhead (days) in trainingResNet-50 on ImageNet between AutoAugment and our method. The computing cost and timeoverhead are estimated on 64 NVIDIA Tesla V100s.
Table 6: Top-1 test error (%) of the transfer of the augmentation policies learned with ResNet-50 onImageNet.
Table 7: Model hyperparameters on CIFAR-10/CIFAR-100 and ImageNet. LR represents learningrate, and WD represents weight decay. We do not specifically tune these hyperparameters, and allof these are consistent with previous works, expect for the number of epochs.
