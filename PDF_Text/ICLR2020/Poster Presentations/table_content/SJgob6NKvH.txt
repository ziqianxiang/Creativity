Table 1: Final win rate on simplest variantof RTFM. The models are trained on one setof dynamics (e.g. training set) and evaluated onanother set of dynamics (e.g. evaluation set).
Table 2: Curriculum training results. We keep 5 randomly initialised models through the entirecurriculum. A cell in row i and column j shows transfer from the best-performing setting in theprevious stage (bold in row i - 1) to the new setting in column j. Each cell shows final mean andstandard deviation of win rate on the training environments. Each experiment trains for 50 millionframes, except for the initial stage (first row, 100 million instead). For the last stage (row 4), we alsotransfer to a 10 × 10 + dyna + group + nl variant and obtain 61 ± 18 win rate.
Table 3: Win rate when evaluating onnew dynamics and world configurationsfor txt2π on the full RTFM problem.
Table 4: Variable dimensions12Published as a conference paper at ICLR 2020C Model detailsC.1 TXT2πHyperparameters. The txt2π used in our experiments consists of 5 consecutive FiLM2 layers,each with 3x3 convolutions and padding and stride sizes of 1. The txt2π layers have channels of16, 32, 64, 64, and 64, with residual connections from the 3rd layer to the 5th layer. The Goal-docLSTM (see Figure 3) shares weight with the Goal LSTM. The Inventory and Goal LSTMs have ahidden dimension of size 10, whereas the Vis-doc LSTM has a dimension of 100. We use a wordembedding dimension of 30.
Table 5: Statistics of the three variations of the Rock-paper-scissors taskFigure 10: Performance on the Rock-paper-scissors task across models. Left shows final perfor-mance on environments whose goals and dynamics were seen during training. Right shows perfor-mance on the environments whose goals and dynamics were not seen during training.
