Table 1: Instability of learning DNNs from different initializations and instability of learning DNNsusing different training data. Without a huge training set, networks with more layers (e.g. ResNet-34) usually suffered more from the over-fitting problem.
Table 2: Magnitudes of consistent features of different orders, when we train DNNs from differentinitializations. Features in different layers have significantly different variance magnitudes. Mostneural activations of the feature belong to low-order consistent components.
Table 3: Classification accuracy by using the original and the refined features. Features of DNN Awere used to reconstruct features of DNN B. For residual networks, we selected the last feature mapwith a size of 14 × 14 as the target for refinement. All DNNs were learned without data augmentationor pre-training. The removal of effects of additional parameters in gθ is discussed in Section 4.3-fairness of comparisons and Appendix A. Consistent components slightly boosted the performance.
Table 4: Top-1 classification accuracy before and after removing redundant features from DNNs.
