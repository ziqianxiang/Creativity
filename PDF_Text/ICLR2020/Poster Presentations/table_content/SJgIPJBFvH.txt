Table 1: Numerical Results for Baselines and Oracular Complexity MeasuresWe next look at one of the most well-known complexity measures in machine learning; the VC-Dimension. Bartlett et al. (2019) proves bounds on the VC dimension of piece-wise linear networkswith potential weight sharing. In Appendix D.1, we extend their result to include pooling layersand multi-class classification. We report two complexity measures based on VC-dimension boundsand parameter counting. These measures could be predictive merely when the architecture changes,which happens only in depth and width hyperparameter types. We observe that, with both types,6Published as a conference paper at ICLR 2020VC-dimension as well as the number of parameters are negatively correlated with generalization gapwhich confirms the widely known empirical observation that overparametrization improves gener-alization in deep learning.
Table 2: Numerical Results for Selected (Norm & Margin)-Based Complexity MeasuresSpectral bound: The most surprising observation here is that the spectral complexity is stronglynegatively correlated with generalization, and negatively correlated with changes within every hy-perparameter type. Most notably, it has strong negative correlation with the depth of the network,which may suggest that the largest singular values are not sufficient to capture the capacity of themodel. To better understand the reason behind this observation, we investigate using different com-ponents of the spectral complexity as the measure. An interesting observation is that the Frobe-nius distance to initialization is negatively correlated, but the Frobenius norm of the parameters isslightly positively correlated with generalization, which contradicts some theories suggesting solu-tions closer to initialization should generalize better. A tempting hypothesis is that weight decayfavors solution closer to the origin, but we did an ablation study on only models with 0 weight decayand found that the distance from initialization still correlates negatively with generalization.
Table 3: Numerical results for selected Sharpness-Based Measures; all the measure use the origin asthe reference and mag refers to magnitude-aware version of the measure.
Table 4: Optimization-Based MeasuresNumber of Iterations: The number of iterations roughly characterizes the speed of optimization,which has been argued to correlate with generalization. For the models considered here, we ob-served that the initial phase (to reach cross-entropy value of 0.1) of the optimization is negativelycorrelated with the speed of optimization for both τ and Ψ. This would suggest that the difficultyof optimization during the initial phase of the optimization benefits the final generalization. Onthe other hand, the speed of optimization going from cross-entropy 0.1 to cross-entropy 0.01 doesnot seem to be correlated with the generalization of the final solution. Importantly, the speed ofoptimization is not an explicit capacity measure so either positive or negative correlation couldpotentially be informative.
Table 5: Complexity measures (rows), hyperparameters (columns) and the rank-correlation coeffi-cients with models trained on CIFAR-10.
Table 6: Complexity measures (rows), hyperparameters (columns) and the mutual informationwith models trained on CIFAR-10.
Table 7: Complexity measures (rows), hyperparameters (columns) and the rank-correlation coeffi-cients with models trained on SVHN dataset.
Table 8: Complexity measures (rows), hyperparameters (columns) and the rank-correlation coeffi-cients with models trained on CIFAR-10 when converged to Loss = 0.1.
Table 9: Complexity measures (rows), hyperparameters (columns) and the average rank-correlation coefficients over 5 runs with models trained on CIFAR-10. The numerical valuesare consistent of that of Table 5.
Table 10: Complexity measures (rows), hyperparameters (columns) and the standard deviation ofeach entry measured over 5 runs with models trained on CIFAR-10. The standard deviation forΨ is computed assuming that each hyperparamters are independent from each other. We see that allstandard deviation are quite small, suggesting the results in of Table 5 are statistically significant.
