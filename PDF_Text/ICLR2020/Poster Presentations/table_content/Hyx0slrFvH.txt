Table 1: Comparison of different DQ parametrizations for ResNet-20 on CIFAR-10.
Table 2: Number of multiplications Clmul, additions Cladd as well as required memory to store theweights Slw and activations Slx of fully connected and convolutional layers.
Table 3: Homogeneous vs. heterogeneous quantization of ResNet-20 on CIFAR-10.						Bitwidth Weight/Activ.	qmax Weight/Activ.	Size Weight/Activ.(max)/Activ.(sum)	Uniform quant. Validation error	Power-of-two quant. Validation errorBaseline	32bit∕32bit	-	1048KB/64KB/736KB		7.29%Fixed	2bit/32bit	fixed/—	65.5KB/64KB/736KB	10.81%	8.99%TQT (Jain et al., 2019)	2bit/32bit	learned/ 一	65.5KB/64KB/736KB	9.47%	8.79%Ours (w/ constr. (8a))	learned/32bit	learned/-	70KB/64KB/736KB	8.59%	8.53%Fixed	2bit/4bit	fixed/fixed	65.5KB/8KB/92KB	11.30%	11.62%TQT (Jain et al., 2019)	2bit/4bit	learned/learned	65.5KB/8KB/92KB	9.62%	11.29%Ours (w/ constr. (8a) and (8b))	learned/learned	learned/learned	70KB/ - /92KB	9.38%	11.29%Ours (w/ constr. (8a) and (8c))	learned/learned	learned/learned	70KB/8KB/ -	8.58%	11.23%ImageNet, we train the quantized DNNs for 50 epochs, using SGD with momentum 0.9 and alearning rate schedule starting with 0.01 and reducing it by a factor of 10 after 16 and 32 epochs,respectively. Please note that we quantize all layers opposed to other papers which use a higherprecision for the first and/or last layer.
Table 4: Homogeneous vs. heterogeneous quantization of MobileNetV2 and ResNet-18 on ImageNet.
Table 5: Error rate of ResNet-20 on CIFAR-10 using different quantization parametrizations. Trainingis done either by SGD with momentum or ADAM.
