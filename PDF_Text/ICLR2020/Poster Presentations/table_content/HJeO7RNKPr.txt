Table 1: Results on the NYU dataset. Our approach outperforms existing single-view and multi-view depth estimation methods. Ours (self-init) uses a constant depth map for initialization whileours(fcrn-init) uses a single-image depth network for initialization.
Table 2: ScanNet experiments evaluating depth and pose accuracy and cross-dataset generalization.
Table 3: Results on SUN3D dataset and comparison to DeepTAM. DeepTAM only evaluates depth inisolation and uses the poses from the dataset during inference, while our approach jointly estimatescamera poses during inference. We outperform DeepTAM and DeMoN on SUN3D even when wedo not use SUN3D data for training.
Table 4: Results on the KITTI dataset. We compare to state-of-the-art single-image depth networkDORN (Fu et al., 2018) and multiview BA-Net (Tang & Tan, 2018). BA-Net reports results using adifferent form of the Sq-Rel metric which We denote by f.
Table 5: Tracking results in the RGB-D benchmark (translational rmse [m/s]).
Table 6: Timing and memory details for different versions of our approach.
Table 7: Per-Sequence tracking results on the RGB-D benchmark evaluated using translationalRMSE [m/s]. We outperform DeepTAM and DVO on 12 of the 16 sequences and achieve a lowertranslational RMSE averaged over all sequences. While DeepTAM requires optical flow supervi-sion to achieve good performance, we do not require supervision on optical flow since the relationbetween camera motion and optical flow is embedded into our network architecture.
Table 8: Impact of pose estimation method on depth accuracy. Replacing our motion module withSfM degrades performance for both MVSNet and our approach.
