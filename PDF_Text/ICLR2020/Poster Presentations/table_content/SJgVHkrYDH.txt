Table 1: HotpotQA development set results: QA and SP (supporting fact prediction) results onHotpotQA's full Wiki and distractor settings. “-” denotes no results are available.
Table 2: HotpotQA full wiki test set results: of-ficial leaderboard results (on November 6, 2019)on the hidden test set of the HotpotQA full wikisetting. Work marked with * appeared afterSeptember 25.
Table 3: SQuAD Open results: we report F1 andEM scores on the test set of SQuAD Open, fol-lowing previous work.
Table 4: Natural Questions Open results: wereport EM scores on the test and developmentsets of Natural Questions Open, following pre-vious work.
Table 5: Retrieval evaluation: Comparing ourretrieval method with other methods across An-swer Recall, Paragraph Recall, Paragraph EM,and QA EM metrics.
Table 6: Ablation study: evaluating differentvariants of our model on HotpotQA full wiki.
Table 7: Performance with different linkstructures: comparing our results on theHotpot QA full wiki development set whenwe use an off-the-shelf entity linking systeminstead of the Wikipedia hyperlinks.
Table 9: Statistics of the reasoning paths: theaverage length and the distribution of length ofthe reasoning paths selected by our retriever andreader for HotpotQA full wiki. Avg. EM repre-sents QA EM performance.
Table 8: Performance with different reason-ing path length: comparing the performancewith different path length on HotpotQA fullwiki. L-step retrieval sets the number of thereasoning steps to a fixed number.
Table 10: Retrieval evaluation: Comparing our retrieval method with other methods across AnswerRecall, Paragraph Recall, Paragraph EM, and QA EM metrics.
Table 11: Effects of the question-dependent paragraph encoding: Comparing our retriever modelwith and without the query-dependent encoding. For our question-dependent approach, the fullwiki results correspond to “retriever, no link-based negatives” in Table 6, and the distractor resultscorrespond to “Ours (Reader: BERT wwm)” Table 1, to make the results comparable.
Table 12: Two examples of the questions that our model retrieves a reasoning path with only oneparagraph. We partly remove sentences irrelevant to the questions. Words in red correspond to theanswer strings.
Table 13: An example question where our model predicts reasoning paths of the length of three. Ourmodel expects that the question is answerable based on the last paragraph of the annotated path.
Table 14: Two examples from the HotpotQA distractor development set. Highlighted text shows thebridge entities for multi-hop reasoning, and also the words in red denote the predicted answer.
Table 15: Statistics of the reasoning paths for SQuAD Open and Natural Questions Open: theaverage length and the distribution of length of the reasoning paths selected by our retriever andreader for SQuAD Open and Natural Questions Open.
Table 16: An example from Natural Questions Open. The bold text represents titles and paragraphindices (e.g., (I) denotes that the paragraph is an introductory paragraph). The highlighted phraserepresents a bridge entity and the text in red represents an answer span.
