Table 1: Selected KGE models and training strategies from the literature. Entries marked in boldwere introduced (or first used) in the context of KGE in the corresponding publication.
Table 2: Model performance in prior studies and our study (as percentages, on test data). First:first reported performance on the respective datasets (oldest models first); Ours: performance in ourstudy; Recent: best performance results obtained in prior studies of selected recent models; Large:best performance achieved in prior studies using very large models (not part of our search space).
Table 3: Hyperparameters of best performing models after quasi-random hyperparameter searchand Bayesian optimization w.r.t. filtered MRR (on validation data). For each hyperparameter, wealso give the reduction in filtered MRR for the best configuration that does not use this value of thehyperparameter (in parenthesis).
Table 4: Dataset statistics		Hyperparameter	Quasi-random search	Bayesian optimizationEmbedding size	{128, 256, 512}	Value from Tab. 6Training type	{NegSamp, 1vsAll, KvsAll}	Value from Tab. 6Reciprocal	{True, False}	Value from Tab. 6No. subject samples (NegSamp)	[1, 1000], log scale	[1, 1000], log scaleNo. object samples (NegSamp)	[1, 1000], log scale	[1, 1000], log scaleLabel smoothing (KvsAll)	[0.0, 0.3]	[0.0, 0.3]Loss	{BCE, MR, CE}	Value from Tab. 6Margin (MR)	[0, 10]	[0, 10]Lp -norm (TransE)	{1, 2}	Value from Tab. 6Optimizer	{Adam, Adagrad}	Value from Tab. 6Batch size	{128, 256, 512, 1024}	Value from Tab. 6Learning rate	[10-4, 1], log scale	[10-4, 1], log scaleLR scheduler patience	[0, 10]	[0, 10]Lp regularization	{L1, L2, L3, None}	Value from Tab. 6Entity emb. weight	[10-20, 10-5]	[10-20,10-5]Relation emb. weight	[10-20, 10-5]	[10-20,10-5]Frequency weighting	{True, False}	Value from Tab. 6Embedding normalization (TransE)		
Table 5: Hyperparameter search space used in our study. Settings that apply only to certain config-urations are indicated in parenthesis.
Table 6: Hyperparameters of best performing models found with quasi-random hyperparameter optimization on FB15K-237. Settings that apply only to certainconfigurations are indicated in parenthesis. For each hyperparameter, we also give the reduction in filtered MRR for the best configuration that does not use thisvalue of the hyperparameter (in parenthesis).
Table 7: Hyperparameters of best performing models found with quasi-random hyperparameter optimization on WNRR. Settings that apply only to certain config-urations are indicated in parenthesis. For each hyperparameter, we also give the reduction in filtered MRR for the best configuration that does not use this value ofthe hyperparameter (in parenthesis).
Table 8: Hyperparameters of best performing models found after quasi-random hyperparameteroptimization and Bayesian optimization. All other hyperparameters are the same as in Tables 6(FB15K-237) and 7 (WNRR).
Table 9: Mean and standard deviation of the validation data performance of each modelâ€™s besthyperparameter configuration when trained from scratch five times.
