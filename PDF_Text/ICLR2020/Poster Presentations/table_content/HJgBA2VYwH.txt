Table 1: MNIST classification accuracy over 6 runs (different pre-trained networks between runs):mean ± stdev for σ = 0.05. Frozen: training with frozen pre-trained auto-encoder weights. Unfrozen:unfrozen auto-encoder weights (fine-tuning). Random init: auto-encoder weights not used.
Table 2: CLEVR results over 10 runs: mean ± stdev of accuracy after 350 epochs, epochs to reachan accuracy milestone, and wall time required with a 1080 Ti GPU. * averages over only 8 runsbecause 2 runs did not reach 99%. MAC (Hudson & Manning, 2018) is a model specifically designedfor CLEVR and the state-of-the-art for image inputs and without program supervision.
Table 3: Direct mean squared error (in hundredths) on Polygon dataset with different number ofpoints in the set. Lower is better.
Table 4: Chamfer loss (in hundredths) on Polygon dataset with different number of points in the set.
Table 5: Linear assignment loss (in hundredths) on Polygon dataset with different number of points in the set. Lower is better.						Set size	2	4	8	16	32	64FSPOOL	0.000	0.001	0.000	0.000	0.000	0.001MLP + Chamfer	0.595	0.885	0.137	0.641	0.160	0.285MLP + Hungarian	0.758	0.200	0.126	0.634	0.163	0.040Random	36.424	9.933	2.556	0.635	0.161	0.041C MNIST ReconstructionResults We show the results for the default MNIST setting in Table 6. Interestingly, the sumpooling baseline has a lower Chamfer reconstruction error than our model, despite the exampleoutputs in Figure 3 looking clearly worse. This demonstrates a weakness of the Chamfer loss. Ourmodel avoids this weakness by being trained with a normal MSE loss (with the cost of a potentiallyhigher Chamfer loss), which is not possible with the baselines. The sum pooling baseline has a bettertest Chamfer loss because it is trained to minimise it, but it is also solving an easier task, since it doesnot need to distinguish padding from non-padding elements.
Table 6: Test Chamfer loss (in 10 000ths) for MNIST for different input noise levels σ over 6 runs.
Table 7: Test Chamfer loss (in 10 000ths) for MNIST with additional mask features (see descriptionin Appendix C) on every element for different input noise levels σ over 6 runs. Lower is better.
Table 8: Classification accuracy (mean ± stdev) on MNIST σ = 0.00 over 6 runs.
Table 9: Classification accuracy (mean ± stdev) on MNIST for 100 epochs over 6 runs.
Table 10: Cross-validation classification results (%) on various commonly-used graph classificationdatasets, with the mean cross-validation accuracy averaged over 10 repeats and sample standarddeviations (±). Hyperparameters of entries marked with * are known to be selected based ontest accuracy instead of validation accuracy, so results are likely not comparable to other existingapproaches that were (hopefully) selected based on validation accuracy. Our results were selectedbased on validation accuracy.
Table 11: Average Precision (AP, mean ± stdev) for different intersection-over-union thresholds ofthe predicted bounding boxes over 6 runs. DSPN-RN-FSPool results are taken from Zhang et al.
Table 12: Average Precision (AP, mean ± stdev) for different distance thresholds of the predictedstate descriptions over 6 runs. DSPN-RN-FSPool results are taken from Zhang et al. (2019a).
Table 13: Average of best hyperparameters over 10 repeats.
