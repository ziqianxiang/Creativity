Table 1: Accuracy (%) on “Digit-Five” dataset with UFDA protocol. FADA achieves 73.6%, outperformingother baselines. We incrementally add each component t our model, aiming to study their effectiveness on thefinal results. (model I: with dynamic attention; model II: I+adversarial alignment; model III: II+representationdisentanglement. mt, up, sv, sy, mm are abbreviations for MNIST, USPS, SVHN, Synthetic Digits, MNIST-M.)(a) Source Features	(b) f-DANN Features	(c) f-DAN Features(d) FADA FeaturesFigure 3: Feature visualization: t-SNE plot of source-only features, f-DANN (Ganin & Lempitsky, 2015)features, f-DAN (Long et al., 2015) features and FADA features in sv,mm,mt,sy→up setting. We use differentmarkers and colors to denote different domains. The data points from target domain have been denoted by redfor better visual effect. (Best viewed in color.)5.1	Experiments on Digit RecognitionDigit-Five This dataset is a collection of five benchmarks for digit recognition, namely MNIST (Le-Cun et al., 1998), Synthetic Digits (Ganin & Lempitsky, 2015), MNIST-M (Ganin & Lempitsky,2015), SVHN, and USPS. In our experiments, we take turns setting one domain as the target domainand the rest as the distributed source domains, leading to five transfer tasks. The detailed architectureof our model can be found in Table 7 (see supplementary material).
Table 2: Accuracy on Office-Caltech10 dataset with unsupervised federated domain adaptation protocol. Theupper table shows the results for AlexNet backbone and the table below shows the results for ResNet backbone.
Table 3: Accuracy (%) on the DomainNet dataset (Peng et al., 2018) dataset under UFDA protocol. The uppertable shows the results based on AlexNet (Krizhevsky et al., 2012) backbone and the table below are the resultsbased on ResNet (He et al., 2016) backbone.
Table 4: Accuracy (%) on “Amazon Review” dataset with unsupervised federated domain adaptation protocol.
Table 5: The ablation study results show that the dynamic attention module is essential for our model.
Table 6: Notations occurred in the paper.
Table 7: Model architecture for digit recognition task (“Digit-Five” dataset). For each convolutionlayer, we list the input dimension, output dimension, kernel size, stride, and padding. For the fully-connected layer, we provide the input and output dimensions. For drop-out layers, we provide theprobability of an element to be zeroed.
Table 8: Model architecture for cross-doman sentimental analysis task (“Amazon Review”dataset (Blitzer et al., 2007a)). For the fully-connected layers (FC), we provide the input andoutput dimensions. For drop-out layers (Dropout), we provide the probability of an element to bezeroed.
Table 9: Model architecture for image recognition task (Office-Caltech10 (Gong et al., 2012) andDomainNet (Peng et al., 2018)). For each convolution layer, we list the inPut dimension, outPutdimension, kernel size, stride, and Padding. For the fully-connected layer, we Provide the inPut andoutPut dimensions. For droP-out layers, we Provide the Probability of an element to be zeroed.
Table 10: Detailed number of samples we used in our experiments.
