Table 1: Top-1 test accuracy of SOTA DNNs on CIFAR-10 for unstructured weight pruning. We consideredunstructured pruning and the ? indicates the method cannot converge. The results are averaged for three runs. Theresults we presented for each model consider some reasonable pruning ratios (we prune more aggressively fordeeper and wider neural networks), and readers can refer to Table 4 (in Appendix A.3.1) for a complete overview.
Table 2: Top-1 test accuracy of SOTA DNNs on CIFAR-10 for unstructured weight pruning via some simplepruning techniques. This table complements Table 1 and evaluates the performance of model compression underOne-shot P+FT and Incremental, as well as how extra fine-tuning (FT) impact the performance of Incrementaland our DPF. Note that One-shot P+FT prunes the dense model and uses extra fine-tuning itself. The Dense, In-cremental and DPF train with the same number of epochs from scratch. The extra fine-tuning procedure considersthe model checkpoint at the end of the normal training, uses the same number of training epochs (60 epochs in ourcase) with tuned optimizer and learning rate. Detailed hyperparameters tuning procedure refers to Appendix A.2.
Table 3: Top-1 test accuracy of ResNet-50 on ImageNet for unstructured weight pruning. The # of parametersfor the full model is 25.56 M. We used the results of DSR from Mostafa & Wang (2019) as we use the same(standard) training/data augmentation scheme for the same neural architecture. Note that different methodsprune different types of layers and result in the different # of parameters for the same target pruning ratio.
Table 4: Top-1 test accuracy for training (compressed) SOTA DNNs on CIFAR-10 from scratch. We consideredunstructured pruning and the ? indicates the method cannot converge. The results are averaged for three runs.
Table 5: Investigate how the reparameterization scheme (layer-wise or not) impact the MACs (for the samenumber of compressed parameters), for using DPF on ResNet-20 with CIFAR-10.
Table 6: Performance evaluation of DPF (and other baseline methods) for training (Wide)ResNet variants onCIFAR-10. We use the norm-based criteria for filter selection (as in SFP (He et al., 2018)) to estimate the outputchannel-wise pruning threshold. We follow the gradual pruning warmup scheme (as in Zhu & Gupta (2017))from 0 epoch to the epoch when performing the second learning rate decay. Note that SFP prunes filters withinthe layer by a given ratio while our DPF prunes filters across layers. Due to the difference between filters fordifferent layers, the # of parameters pruned by DPF might slight different from the one pruned by SFP. Thepruning ratio refers to either prune filters within the layer or across the layers.
