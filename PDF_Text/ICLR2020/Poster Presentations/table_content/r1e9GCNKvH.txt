Table 1: Validation Error % of Various 400 Unit RNN Architectures after 50 Epochs of Training onSeq. MNIST; our method works well across all common recurrent architectures. Sparsity of 95 %was used on all experiments.
Table 2: Benchmarking of Various Pruning Algorithms on 95% Sparse GRUs on seq. MNIST. SNIP,GraSP and Random pruning are competitive for smaller models, but the results tend to diminishas the network size increases. Our method obtains strong results even as the network size is large.
Table 3: Sparsity Level vs Validation Error % on 400 Unit GRUs, for seq. MNIST. Our methodconsistently beats random pruning.
Table 4: Training Perplexities of Training Sparse Models on Large Language Benchmarks. Ourmethod successfully reduces the perplexity score across all benchmarks, often significantly, howeverthere is still a large gap to the dense performance. Parameters are reported only for the recurrent layeras other layers were not pruned during training.
Table 5: Validation Perplexities of Pruned 800-unit GRU Models on Penn Treebank. For a simplecomparison we do not finetune these models, or apply any regularization tricks besides early stopping.
Table 6: Benchmarking of validation Error % on Different Initializations, for Sequential MNISTTask with 400 Unit GRU. Our algorithm successfully beats random on well-conditioned normaldistributions, but fails on high variance and the uniform distribution.
Table 7: Benchmarking of Pruning Algorithm Runtimes; our method is faster than GraSP as theHessian is larger than the Jacobian, but slower than SNiP for a single time instance. It should benoted that our algorithm works best when iterated across several time steps, while GraSP requiresiteration across the entire training set, and SNiP requires only a single computation.
