Table 1: Classification errors(%) on CIFAR10.						Method	VGG-13	VGG-16	VGG-19	WRN 52-1	WRN 16-4	WRN 28-10SGD	5.88	6.32	6.49	6.23	4.96	3.89ADAM	6.43	6.61	6.92	6.77	5.32	3.86Cayley SGD	5.90	5.77	5.85	6.35	5.15	3.66Cayley ADAM	5.93	5.88	6.03	6.44	5.22	3.57Table 2: Classification errors(%) on CIFAR100.						Method	VGG-13	VGG-16	VGG-19	WRN 52-1	WRN 16-4	WRN 28-10SGD	26.17	26.84	27.62	27.44	23.41	18.66ADAM	26.58	27.10	27.88	27.89	24.45	18.45Cayley SGD	24.86	25.48	25.68	27.64	23.71	18.26Cayley ADAM	25.10	25.61	25.70	27.91	24.18	18.10Performance: Table 1 and Table 2 show classification errors on CIFAR10 and CIFAR100 respec-tively using different optimization algorithms. As shown in the tables, the proposed two algorithmsachieve competitive performance, and for certain deep architectures, the best performance. Specif-ically, the network WRN-28-10 trained with Cayley ADAM achieves the best error rate of 3.57%and 18.10% on CIFAR10 and CIFAR100 respectively. Fig. 1 compares training curves of our al-gorithms and baselines in terms of epochs, and shows that both Cayley SGD and Cayley ADAMconverge faster than the baselines. In particular, the training curves of the baselines tend to getstuck in a plateau before the learning rate drops, which is not the case with our algorithms. This
Table 2: Classification errors(%) on CIFAR100.						Method	VGG-13	VGG-16	VGG-19	WRN 52-1	WRN 16-4	WRN 28-10SGD	26.17	26.84	27.62	27.44	23.41	18.66ADAM	26.58	27.10	27.88	27.89	24.45	18.45Cayley SGD	24.86	25.48	25.68	27.64	23.71	18.26Cayley ADAM	25.10	25.61	25.70	27.91	24.18	18.10Performance: Table 1 and Table 2 show classification errors on CIFAR10 and CIFAR100 respec-tively using different optimization algorithms. As shown in the tables, the proposed two algorithmsachieve competitive performance, and for certain deep architectures, the best performance. Specif-ically, the network WRN-28-10 trained with Cayley ADAM achieves the best error rate of 3.57%and 18.10% on CIFAR10 and CIFAR100 respectively. Fig. 1 compares training curves of our al-gorithms and baselines in terms of epochs, and shows that both Cayley SGD and Cayley ADAMconverge faster than the baselines. In particular, the training curves of the baselines tend to getstuck in a plateau before the learning rate drops, which is not the case with our algorithms. Thismight be because the baselines do not enforce orthonormality of network parameters. In training,the backpropagation of orthonormal weight vectors, in general, does not affect each other, and thushas greater chances to explore new parameter regions. Fig. 2 also compares the training loss curvein terms of time. Our Cayley SGD and Cayley ADAM converge the fastest among methods that alsoaddress orthonormality. Although the baselines SGD and ADAM converge faster at the beginningdue to their training efficiency, our Cayley SGD and Cayley ADAM can catch up with the baseline
Table 3: Error rate and training time per epoch comparison to baselines with WRN-28-10 on CI-FAR10 and CIFAR100. All experiments are performed on one TITAN XP GPU.
Table 4: Pixel-by-pixel MNIST accuracy and training time per iteration of the closed-form CayleyTransform, Cayley SGD, and Cayley ADAM for Full-uRNNs (Wisdom et al., 2016). All experi-ments are performed on one TITAN Xp GPU.
Table 5: Checking unitariness by computing the error ||KHK - I||F for varying numbers of itera- tions in the iterative Cayley transform and the closed-form Cayley transform.			Hidden Size	s=0	s=1	s=2	s=3	s=4	Closed-formn=116	3.231e-3	2.852e-4	7.384e-6	7.353e-6	7.338e-6	8.273e-5n=512	6.787e-3	5.557e-4	2.562e-5	2.547e-5	2.544e-5	3.845e-57	ConclusionWe proposed an efficient way to enforce the exact orthonormal constraints on parameters by opti-mization on the Stiefel manifold. The iterative Cayley transform was applied to the conventionalSGD and ADAM for specifying two new algorithms: Cayley SGD with momentum and CayleyADAM, and the theoretical analysis of convergence of the former. Experiments show that both algo-rithms achieve comparable performance and faster convergence over the baseline SGD and ADAMin training of the standard VGG and ResNet on CIFAR10 and CIFAR100, as well as RNNs on thepixel-by-pixel MNIST task. Both Cayley SGD with momentum and Cayley ADAM take less run-time per iteration than all existing hard orthonormal methods and soft orthonormal methods, and canbe applied to non-square parameter matrices.
