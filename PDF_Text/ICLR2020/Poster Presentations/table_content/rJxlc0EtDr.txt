Table 1: Inference queriesLength	EMN	DNC	UT	MEMO3 items (set: A-B-C - accuracy on A-C)	61.01	96.85	85.60	98.26(0.67)4 items (set: A-B-C-D - accuracy on A-D)	48.66	51.56	44.16	97.22(0.13)5 items (set: A-B-C-D-E - accuracy on A-E)	45.13	62.61	47.93	84.54(5.72)Test results for the best 5 hyper-parameters (chosen by validation loss) for MEMO. For EMN andDNC and UT results on the best run (chosen by validation loss)Figure 2: Weights analysis of an inference query in the length 3 PAI task. An example of memorycontent and related inference query is reported in the first column on the left. For clarity we reportimage class ID. Cue and Match are images from the same sequence e.g. A10 - C10, where 10 is theslot ID. The lure is an images presented in the same memory store, but associated with a differentsequence, e.g. C13 . The 3 most right columns report the weights associated with the 3 hops used bythe network, for each probability mass we report the associated retrieved slot.
Table 2: Undirected graph - shortest pathGraph Structure	Prediction of First Node				Prediction of Second Node			Nodes I Out-degree ∣ Path length	EMN	UT	DNC I	MEMO	EMN	UT	DNC I	MEMO10	2	2	94.99	100.00	100.00	100.00(0.00)	n/a	n/a	n/a	n/a20	3	3	31.99	39.00	97.00	94.40(0.02)	66.00	84.80	98.00	93.00(0.03)20	5	3	23.99	28.00	30.00	69.20(0.07)	43.00	61.51	40.99	68.80(0.09)Test results for the best 5 hyper-parameters (chosen by training loss) for MEMO, mean andcorresponding standard deviation are reported. For EMN, UT, and DNC we report results from thebest run.
Table 4: Paired Associative - length 3: A-B-CTrial Type	EMN	DNC	UT	MEMOA-B	98.19	98.58	97.43	99.82(0.30)B-C	97.93	99.34	98.28	99.76(0.38)A-C	61.01	96.85	85.60	98.26(0.67)Table 5: Paired Associative - length 4: A-B-C-DTrial Type	EMN	DNC	UT	MEMOA-B	96.31	94.26	99.32	99.57(0.20)B-C	97.57	84.94	88.31	99.33(0.13)C-D	96.59	95.68	93.37	99.58(0.13)A-C	48.71	49.38	54.87	98.93(0.15)B-D	47.42	49.89	51.92	99.14(0.19)A-D	48.63	58.63	44.16	97.22(0.13)14Published as a conference paper at ICLR 2020Table 6: Paired Associative - length 4: A-B-C-D-ETrial Type	EMN	DNC	UT	MEMOA-B	95.68	98.88	96.94	99.20(0.43)B-C	95.82	94.60	92.63	98.93(0.17)C-D	95.43	95.20	89.99	97.27(0.21)
Table 5: Paired Associative - length 4: A-B-C-DTrial Type	EMN	DNC	UT	MEMOA-B	96.31	94.26	99.32	99.57(0.20)B-C	97.57	84.94	88.31	99.33(0.13)C-D	96.59	95.68	93.37	99.58(0.13)A-C	48.71	49.38	54.87	98.93(0.15)B-D	47.42	49.89	51.92	99.14(0.19)A-D	48.63	58.63	44.16	97.22(0.13)14Published as a conference paper at ICLR 2020Table 6: Paired Associative - length 4: A-B-C-D-ETrial Type	EMN	DNC	UT	MEMOA-B	95.68	98.88	96.94	99.20(0.43)B-C	95.82	94.60	92.63	98.93(0.17)C-D	95.43	95.20	89.99	97.27(0.21)D-E	95.16	95.98	97.27	95.06(0.12)A-C	48.68	48.66	41.85	87.33(0.12)B-D	45.75	46.87	39.62	86.65(1.27)C-E	49.46	49.51	35.87	87.08(0.92)A-D	52.08	50.32	52.38	86.12(0.57)
Table 6: Paired Associative - length 4: A-B-C-D-ETrial Type	EMN	DNC	UT	MEMOA-B	95.68	98.88	96.94	99.20(0.43)B-C	95.82	94.60	92.63	98.93(0.17)C-D	95.43	95.20	89.99	97.27(0.21)D-E	95.16	95.98	97.27	95.06(0.12)A-C	48.68	48.66	41.85	87.33(0.12)B-D	45.75	46.87	39.62	86.65(1.27)C-E	49.46	49.51	35.87	87.08(0.92)A-D	52.08	50.32	52.38	86.12(0.57)B-E	46.69	52.27	43.27	86.37(0.77)A-E	48.31	48.79	47.93	84.54(5.72)For EMN and DNC the results shown are from the hyper-parameter with the lower loss on thevalidation set. For MEMO the results are the average and relative standard deviation (reported inparenthesis), obtained by averaging the 5 hyper-parameters with the lower loss on the validation set.
Table 7: PAI - Ablations - sequence of length 3: A-B-CMEMO Network Architecture			A-C inference trialPositional encoding as in (Vaswani et al., 2017)	Memories kept separated	Recurrent attention w/ Layernorm	Accuracy✓	X	X	57.59(10.11)✓	X	✓	52.79(3.12)X	✓	X	73.26(15.86)X	✓	✓	97.59(1.85)Results for the best run (chosen by validation Set) on the PAI task. X= not present; ✓ = presentA.3.2 Attention weights analysisFigure 4: Attention weights analysis of length 3 PAI task, in the case where the network converged to7 hops. In this case the network uses the first two hops to retrieve the slot where the cue is presentand the the hops number 3, 4 and 5 to retrieve the slot with the match. The weights are sharp and theyfocus only on 1 single slot.
Table 8: Undirected graph - shortest path - Comparing results on the second node based on using ground truth or predicted answer of the first node.								Graph Structure			Prediction of First Node		Prediction of Second Node			Nodes	Out-degree	Path length	EMN	MEMO	EMN ground truth	EMN predicted answer	MEMO ground truth	EMN predicted annswer20	3	3	31.99	94.40(0.02)	657Γ3	26.00	96.80(0.02)	93.76(0.08)20	5	3	23.99	69.20(0.07)	43.00	22.30	85.38(0.05)	68.80(0.09)Test results for the best 5 hyper-parameters (chosen by training loss) for MEMO, mean andcorresponding standard deviation are reported. For EMN we report results from the best run.
Table 9: bAbI - AbEionSMEMO Network Architecture				bAbI 10K	Positional encoding as in (Vaswani et al., 2017)	Memories keept separated	Recurrent attention	Layernorm	Number of solved tasks	Average error✓	X	X	X	11/20	14.81X	✓	X	X	14/20	10.43✓	X	✓	✓	17/20	4.80X	✓	✓	X	18/20	5.00X	✓	✓	✓	20/20	0.21Results for the best run (chosen by validation set) on the bAbI task. The model was trained andtested jointly on all tasks. All tasks received approximately equal training resources. X= not present;✓= preSent20Published as a conference paper at ICLR 2020C.3 Task-wise resultsTable 10: bAbI Results - average over 5 hyper-parameters with lower loss on the Valiation setTrial 	Type		MEMO	MEMO top 5 seeds1 - Single Supporting Fact	100.00	100.00(0.00)2 - TWo Supporting Facts	100.00	99.13(1.78)3 - Three Supporting Facts	97.05	94.15(6.35)4 - Two Arg. Relations	100.00	100.00(0.00)
Table 10: bAbI Results - average over 5 hyper-parameters with lower loss on the Valiation setTrial 	Type		MEMO	MEMO top 5 seeds1 - Single Supporting Fact	100.00	100.00(0.00)2 - TWo Supporting Facts	100.00	99.13(1.78)3 - Three Supporting Facts	97.05	94.15(6.35)4 - Two Arg. Relations	100.00	100.00(0.00)5 - Three Arg. Relations	100.00	100.00(0.00)6 - Yes/No Questions	100.00	100.00(0.00)7 - Counting	100.00	96.69(3.57)8 - Lists/Sets	100.00	99.13(1.94)9 - Simple Negation	100.00	100.00(0.00)10 - Indefinite Knowledge	100.00	99.35(1.44)11 - Basic Coreference	100.00	100.00(0.00)12 - Conjunction	100.00	100.00(0.00)13 - Compound Coref	100.00	100.00(0.00)14 - Time Reasoning	100.00	100.00(0.00)15 - Basic Deduction	100.00	100.00(0.00)16 - Basic Induction	98.75	95.05(5.12)17 - Positional Reasoning	100.00	100.00(0.00)18 - Size Reasoning	100.00	99.13(1.94)
Table 11: Fixed hyper-parameters used across tasksTasksParameternameISOdcddDropOutaDropOutoGRURhidden sizeMLPRnumber of layersMLPRhidden sizePAI length 3	PAI length 4	PAI length 5	Shortest path 10-2-2	Shortest path 20-3-3	Shortest path 20-5-3	bAbI32	48	64	20"^	60	100	3203	3	3	2	2	2	111000	1000	1000	1000	1000	1000	177128	128	128	128	128	128	128256	256	256	512	512	512	512
Table 12: Range of hyper-parameters used in sweepsTasks(maxParameter name	PAI	PAI	PAI length 3 length 4 length 5	Shortest path Shortest path Shortest path 10-2-2	20-3-3	20-5-3	bAbIN number of hops) γ α β biasinit H	[3,5,20] [0.85,0.9] [1e-4, 1e-2, 0.1] [1e-3, 1e-2, 0.1] [2, 5] 1	[5, 20] [0.9] [1e-2, 0.1] [1e-2, 0.1] [2, 10] [4, 8]	[5, 20] [0.85,0.9] [1e-4, 1e-2, 0.1] [1e-3, 1e-2, 0.1] [2, 5] [4, 8][7e-4, 5e-4, 1e-4][1e-4, 5e-5]lstartmemolhalt22Published as a conference paper at ICLR 2020E MEMO complexity analysisIn terms of temporal complexity, MEMO has a complexity of O(n ∙ A ∙ N ∙ H ∙ I ∙ S ∙ d), where nis the number of samples we process with our network, A is the number of answers, N is the upperbound of the number of hops we can take, H is the number of heads used, I is the number of stories,and S is the number of words in each sentence. This is due to the fact that, for each sample, we do thehopping procedure for every answer, taking a number of hops. For each hop we query our memoryby interacting with all its slots I, for all its size S × d. For all our experiments, all parameters A, N,H, I, S, d are fixed to constants.
Table 13: Hyperparameters used on all tasks trained with DNC.
Table 14: Hyperparameters ranges used to search over with DNC.
Table 15: Hyperparameters used for all experiments for UT.
Table 16: Hyperparameters ranges used to search over with UT.
