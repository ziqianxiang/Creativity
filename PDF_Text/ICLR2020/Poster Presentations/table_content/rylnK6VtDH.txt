Table 1: Word-level perplexity on WikiText-103Model	Valid	Test	No. ParamsLSTM Raeet al. (2018)	34.1	34.3	88MGated CNN Dauphin et al. (2017)	-	37.2	-RMC Santoro et al. (2018)	30.8	31.6	-Trellis Networks Bai et al. (2019)	-	30.35	180MTransformerXL Dai et al. (2018)	17.7	18.3	257MLSTM (ours)	34.7	36.7	88MLSTM + MultDec	31.7	33.7	105MLSTM + MultEncDec	28.9	30.3	110MWe note that we have competitive results using only a single-layer LSTM as our base model and farfewer parameters overall. Our intuition is that using such embeddings is orthognal to most of theother recent advances proposed and can thus be stacked on top of them. We leave as future work theintegration of these ideas with Transformer based models.
