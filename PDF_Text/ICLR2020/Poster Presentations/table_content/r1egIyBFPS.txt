Table 1: Example simplification rules learned by Hiss (left) and their corresponding rules listed inHalide (right). If the learned rule has no corresponding preset rule in Halide, the row is left blank.
Table 2: Simplification process of Hiss (upper) and Halide (lower) on Eq. (10). The subtreesselected for simplification in the next iteration are marked with box unless the entire tree is selected.
Table 3: The vocabulary of the symbols and operators that are used to construct the traverse equiva-lence dataset.
Table 4: Involved simplification rules discovered by Hiss.
Table 5: Mean and standard deviation of the performance metrics among random initialization. Theexperiment setting is the same as in section 5.1.
Table 6:	Simplification traces of Hiss with and without the subtree selector on Eq. (11). Thesubexpressions selected by the subtree selector are boxed, unless the entire expression is chosen.
Table 7:	Output sequences of Linear-LSTM trained on Traverse Equivalence datasetLinear-LSTM with only equivalence rewardInput	Outputmax(v1 +v0, v2 + v0)	[',’，’C8’，’2’，’2’，’1’，’v9‘，‘,‘，’C8’，’2‘，’2']min(min(v2, v0 + v1), v0 + v3)	[’,’, ’c9’, ’c9’, ’c9’, ’c9’, ’2’, ’2’, ’1’, ’,’, ’c9’](v0 &&(v2||v1))||v1	[‘,’，’c8’，’2’，’2’，’1‘，‘,‘，’C9‘，’C9‘，’C9‘，’2']Linear-LSTM trained with equivalence and valid expression rewardInput	Outputmin(v2, v0 + v1)-v1	['vɪ7]v1||(v0||v1)	[’C0’]v0 - min(v2, v0 + v1)	['2','≥','v0']is finally able to esCape from the plateau and reaCh a higher reward level， whereas the one withoutthe embedding similarity loss gets trapped in the plateau. This result suggests that the embeddingsimilarity loss provides an extra training signal to address the ConvergenCe issue of REINFORCE.
