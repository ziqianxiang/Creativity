Table 1: Test performance with heavy traffic on difficult initial and goal lanes configurationsConfig	Initial lanes	Goal lanes	CM3	IAC	COMAC1	[1, 2]	[3,o]	16.17	11.40	10.00C2	Unif. random	Unif. random	14.93	12.20	12.93C3	[1, 2]	[2, 1]	15.85	14.32	15.00C4	[0, 1]	[3, 2]	16.35	9.73	8.1We investigated whether policies trained with few agent vehicles (N = 2) on an empty road cangeneralize to situations with heavy SUMO-controlled traffic. We also tested on initial and goal laneconfigurations (C3 and C4) which occur with low probability when training with configurationsC1 and C2. Table 1 shows the sum of agents’ reward, averaged over 100 test episodes, on theseconfigurations that require cooperation with each other and with minimally-interactive SUMO-controlled vehicles for success. CM3’s higher performance than IAC and COMA in training isreflected by better generalization performance on these test configurations. There is almost negligibledecrase in performance from train Figure 5d to test, giving evidence to our hypothesis that centralizedtraining with few agents is feasible even for deployment in situations with many agents, for certainapplications where local interactions are dominant.
Table 2: Absolute training runtime of all algorithms in secondsEnvironment	CM3	IAC	COMA	QMIXAntipodal	1.1e4±348	0.9e4±20	1.9e4±238	1.0e4±19Cross	1.9e4±256	1.5e4±26	1.3e4±12	1.1e4±34Merge	8.5e3±21	6.8e3±105	9.6e3±294	1.2e4±61SUMO	9.6e3±278	7.0e3±1.5e3	8.7e3±1.3e3	6.3e3±21Checkers	9.2e3±880	8.5e3±568	7.7e3±2.2e3	11e3±1.4e3G Environment detailsThe full Markov game for each experimental domain, along with the single-agent MDP inducedfrom the Markov game, are defined in this section. In all domains, each agent’s observation in theMarkov game consists of two components, oself and oothers. CM3 leverages this decomposition forfaster training, while IAC, COMA and QMIX do not.
Table 3: Parameters used for CM3, ablations, and baselines in cooperative navigationParameter	CM3				IAC	COMA	QMIX	Stage 1	Stage 2	QV	Direct			Episodes	1e3	8e4	8e4	8e4	8e4	8e4	8e4start	1.0	0.5	0.5	1.0	1.0	1.0	1.0end	0.01	0.05	0.05	0.05	0.05	0.05	0.05div	1e3	2e4	2e4	8e4	8e4	2e4	8e4Replay buffer	1e4	1e4	1e4	1e4	1e4	1e4	1e4Minibatch size	256	128	128	128	128	128	128Episodes per train	10	10	10	10	10	10	N/ALearning rate π	1e-4	1e-4	1e-4	1e-4	1e-4	1e-5	N/ALearning rate Q	1e-3	1e-3	1e-3	1e-3	N/A	1e-4	1e-3Learning rate V	N/A	N/A	1e-3	N/A	1e-3	N/A	N/AEpochs	24	24	24	24	24	24	NASteps per train	N/A	N/A	N/A	N/A	N/A	N/A	10Max env steps	25	50	50	50	50	50	50Table 4: Parameters used for CM3 and baselines in SUMOCM3Parameter	Stage 1	Stage 2	QV	Direct	IAC	COMA	QMIXEpisodes	2.5e3	5e4	5e4	5e4	5e4	5e4	5e4
Table 4: Parameters used for CM3 and baselines in SUMOCM3Parameter	Stage 1	Stage 2	QV	Direct	IAC	COMA	QMIXEpisodes	2.5e3	5e4	5e4	5e4	5e4	5e4	5e4start	0.5	0.5	0.5	0.5	0.5	0.5	0.5end	0.05	0.05	0.05	0.05	0.05	0.05	0.05step	2e3	1e3	4e4	4e4	1e3	1e4	4e4Replay buffer	1e4	2e4	2e4	2e4	2e4	2e4	2e4Minibatch size	128	128	128	128	128	128	128Steps per train	10	10	10	10	N/A	N/A	10Episodes per train	N/A	N/A	N/A	N/A	10	10	N/ALearning rate π	1e-4	1e-4	1e-4	1e-4	1e-4	1e-4	N/ALearning rate Q	1e-3	1e-3	1e-3	1e-3	N/A	1e-3	1e-3Learning rate V	N/A	N/A	1e-3	N/A	1e-3	N/A	N/AEpochs	N/A	N/A	N/A	N/A	33	33	N/AMax env steps	33	33	33	33	33	33	33Table 5: Parameters used for CM3 and baselines in CheckersCM3Parameter	Stage 1	Stage 2	QV	Direct	IAC	COMA	QMIXEpisodes	5e3	5e4	5e4	5e4	5e4	5e4	5e4
Table 5: Parameters used for CM3 and baselines in CheckersCM3Parameter	Stage 1	Stage 2	QV	Direct	IAC	COMA	QMIXEpisodes	5e3	5e4	5e4	5e4	5e4	5e4	5e4start	1.0	0.5	0.5	1.0	1.0	1.0	1.0end	0.1	0.1	0.1	0.1	0.1	0.1	0.1step	5e2	1e3	1e3	1e4	2e4	1e4	1e4Replay buffer	1e4	1e4	1e4	1e4	1e4	1e4	1e4Minibatch size	128	128	128	128	128	128	128Steps per train	N/A	10	10	10	N/A	N/A	10Episodes per train	10	N/A	N/A	N/A	10	10	N/ALearning rate π	1e-4	1e-4	1e-4	1e-4	1e-4	1e-4	N/ALearning rate Q	1e-3	1e-3	1e-3	1e-3	N/A	1e-3	1e-5Learning rate V	N/A	N/A	1e-3	N/A	1e-3	N/A	N/AEpochs	10	N/A	N/A	N/A	33	33	N/AMax env steps	75	75	75	75	75	75	7523Published as a conference paper at ICLR 2020J Stage 1The Stage 1 functions Q1 and π1 for a single agent are trained with the N = 1 equivalents of (4) and
