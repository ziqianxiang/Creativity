Table 1: IBP trained models have low IBP verified errors but when verified with a typically muchtighter bound, including convex adversarial polytope (CAP) (Wong et al., 2018) and CROWN (Zhanget al., 2018), the verified errors increase significantly. CROWN is generally tighter than convexadversarial polytope however the gap between CROWN and IBP is still large, especially at large .
Table 2: The verified, standard (clean) and PGD attack errors for models trained using IBP andCROWN-IBP on MNIST and CIFAR-10. We only present performance on model DM-large heredue to limited space (see Table C for a full comparison). CROWN-IBP outperforms IBP under all κsettings, and achieves state-of-the-art performance on both MNIST and CIFAR datasets for all .
Table A: Model structures from GoWal et al. (2018). “CONV k wxh+s"represents a 2D convolutionallayer with k filters of size w×h using a stride of s in both dimensions. “FC n” = fully connected layerWith n outputs. Last fully connected layer is omitted. All netWorks use ReLU activation functions.
Table B: Model structures used in our training stability experiments. We use ReLU activations forall models. We omit the last fully connected layer as its output dimension is always 10. In the table,“Conv k w × w + s” represents to a 2D convolutional layer with k filters of size w × w and a strideof s. Model A - J are referred to as “small models” and model K to T are referred to as “mediummodels”.
Table C: The verified, standard (clean) and PGD attack errors for 3 models (DM-small, DM-medium,DM-large) trained on MNIST and CIFAR test sets. We evaluate IBP and CROWN-IBP underdifferent κ schedules. CROWN-IBP outperforms IBP under the same κ setting, and also achievesstate-of-the-art results for '∞ robustness on both MNIST and CIFAR datasets for all e.
Table D: Verified and standard (clean) test errors for a large number of models trained on MNISTand CIFAR-10 datasets using IBP and CROWN-IBP. The purpose of this experiment is to comparemodel performance statistics (min, median and max) on a wide range of models, rather than a fewhand selected models. For each setting we report 3 representative models: the models with smallest,median, and largest verified error. We also report the standard error of these three selected models.
Table E: Means and standard deviations of verified and standard errors of 10 MNIST models trainedusing CROWN-IBP. The architectures of these models are presented in Table B. We run each model5 times to compute its mean and standard deviation.
Table F: `1 regularized and unregularized models’ standard and verified errors on training and test set.
Table G: IBP and CROWN-IBP’s training time on different models in seconds. For IBP and CROWN-IBP, we use a batchsize of 256 for MNIST and 128 for CIFAR-10. For convex adversarial polytope,we use 50 random Cauchy projections, and reduce batch size if necessary to fit into GPU memory.
Table H: Comparison of verified and standard errors for CROWN-IBP models trained on TPUs andGPUs (CIFAR-10, DM-Large model).
