Table 1: Comparison of our experiment results with those reported in the literature	N = 20 Obj.	N = 50 Obj.	N = 100 Obj.
Table 2: Motivating examples: traveling cost under different policies for random problem instances	Problem instances										1	2	3	4	5	6	7	8	9	10Policy 1	18.65	16.67	15.41	15.11	16.69	14.97	16.88	16.37	13.72	15.56Policy 2	18.86	16.61	15.39	15.10	16.55	14.75	16.73	16.39	13.77	15.56Policy 3	18.81	16.63	15.43	15.06	16.82	14.72	16.70	16.37	13.73	15.79Policy 4	18.90	16.60	15.37	15.04	16.66	14.93	16.83	16.32	13.70	15.62Policy 5	18.73	16.56	15.43	15.12	16.66	15.09	16.80	16.53	13.72	15.56Policy 6	18.92	16.60	15.43	15.09	16.73	14.65	16.59	16.55	13.73	15.56isoɔωu=-θ>ejlISoɔMU=-OAeJl(a) CVRP-20lsoɔωu=-①16.415.615.515.4 ——5008500	16500	24500	32500	40000Rollout steps
Table 3: Improvement operators mostly usedClass	Name	DetailsIntra-route	2-Opt	Remove two edges and reconnect their endpoints	Relocate(1)	Move a customer in the route to a new locationInter-route	Cross(2)	Exchange the tails of two routes	Symmetric-exchange(2)	Exchange segments of length m (m = 1) between two routes	Relocate(2)	Move a segment of length m ( m = 1) from a route to another3.3	Analysis of operator usagesAs we mentioned before, the RL model is able to differentiate more useful improvement operatorsfrom less useful ones for CVRP. In our experiments, we count the usage of different operators fordifferent policies as training epochs grow. When the myopic reward function RF1 in Section 2.1.4 isused, our experimental results show that the policy converges to use a fixed subset of improvementoperators (for detailed operators, see Table 3). This subset of operators are also preferred by allpolicies when we use RF2. However, the pattern of operator usages varies among the policies. Forexample, Figure 4(a) and (b) illustrate different patterns of operator usages for Policy 1 and Policy2, respectively.
Table 4: State featuresType	Name	DetailsProblem- and solution-specific	C	Demand of customer i	Ci	Free capacity of the route containing customer i	(xi,yi)	Location of customer i	(Xi- , yi- )	Location of node visited before i	(Xi+ , yi+ )	Location of node visited after i	di-,i	Distance from i- to i	di,i+	Distance from i to i+	di-,i+	Distance from i- to i+History-related	at —h	Action taken h steps before	et-h	Effect of at-hB Details of States and OperatorsWe list the details of our state features in Table 4, and of operators in Table 5 and 6.
Table 5: Improvement operatorsClass	Name	DetailsIntra-route	2-Opt	Remove two edges and reconnect their endpoints	Symmetric-exchange(1)	Exchange two customers in the route	Relocate(I)	Move a customer in the route to a new locationInter-route	Cross(2)	Exchange the tails of two routes	Reverse-cross(2)	-Reverse one of two routes and then exchange their tails	Symmetric-exchange(2)	Exchange segments of length m (m = 1,2,3) between two routes	Asymmetric-exchange(2)	Exchange segments of length m and n (m = 1, 2, 3, n = 1, 2, 3, m 6= n) between two routes	Relocate(2)	Move a segment of length m (m = 1,2,3) from a route to another	Cyclic-exchange(3)	Exchange cyclically one customer between three routesTable 6: Perturbation operatorsClass	Name	DetailsInter-route perturbation	Random-permute	Randomly destroy m routes and re-construct routes by visiting affected customers in a random order	Random-exchange(2)	Randomly exchange m pairs of nearby customers between two routes	Cyclic-exchange	Exchange cyclically customers between multiple routesC Policy NetworkFigure 6	shows the structure of our policy network.
Table 6: Perturbation operatorsClass	Name	DetailsInter-route perturbation	Random-permute	Randomly destroy m routes and re-construct routes by visiting affected customers in a random order	Random-exchange(2)	Randomly exchange m pairs of nearby customers between two routes	Cyclic-exchange	Exchange cyclically customers between multiple routesC Policy NetworkFigure 6	shows the structure of our policy network.
