Table 1: Per category IoU and mean IoU [%] on Cityscapes validation set, for a budget of 12k regions. Forclarity, only the mean of 5 runs is reported. Results with standard deviations in Table C.1.
Table C.1: Per category IoU and mean IoU [%], on Cityscapes validation set, for a budget of 12k regions. Boththe mean and standard deviation of 5 runs is reported.
Table E.1: Contribution to the validation mean IoU performance [%] of Cityscapes dataset, for a budget of4K and for each of the components of our state representation, compared to the baselines. Mean and standarddeviation of 5 runs is reported.
Table E.2: Comparison between labeling a full image and 24 non-overlapping square regions (pixel-wise,equivalent to a full image), for different methods. Performance is measured in terms of validation mean IoUperformance [%] in CamVid dataset, for a budget of 0.5k. In the first row, results for “full im.”, one entire imageis labeled at each step (region size equal to the size of the image). In the second row,“24 R” results for labeling24 regions at each step. Pool size selected as the one that performed better, out of 10, 20, 50 and 100. Resultsare reported with the mean and standard deviation of 5 runs.
Table E.3: Results of varying the number of regions to be labeled at each step by our method. Performance ismeasured in terms of validation mean IoU performance [%] in CamVid dataset, for a budget of 0.5k Results arereported with the mean and standard deviation of 5 runs.
