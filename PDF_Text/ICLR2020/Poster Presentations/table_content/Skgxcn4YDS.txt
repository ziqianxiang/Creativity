Table 1: Summary of tasks, datasets, dataset sizes, and their corresponding metrics. As this workuses no development set, only the training and test datasets are shown. nF1 is the normalized versionof the F1 score; EM represents an exact match between texts: for text classification, this amounts toaccuracy; for WOZ, it is equivalent to dfEM (turn-based dialogue state exact match); for WikiSQL,it is equivalent to lfEM (exact match of logical forms).
Table 2: Comparison of GPT-2 and other methods on single task scores. Other scores are retrievedfrom Bryan McCann & Socher (2018) or d’Autume et al. (2019). Better performance in boldface.
Table 3: Summary of averaged metric scores for different methods under permuted task ordersusing models at last epoch of last task. The Average and Std columns respectively are the averageand standard deviation of the averaged scores for each row of the methods. Multitasked learning asan upper bound is shown at the bottom.
Table 4: Summary of averaged score on five tasks. The scores are reported as the averaged scoreover all tasks of the models after training on every task. The rightmost three columns - LAMOLwith Y = 0.05 and Y = 0.2 of real samples from previous tasks and Multitasked - are upper boundsfor comparison. Best performance in boldface.
Table 5: Summary of results on text classification tasks using averaged EM score (equivalent toaveraged accuracy in d’Autume et al. (2019)) of models at last epoch of last task. The four ordersmirror those in d’Autume et al. (2019). For MBPA++ (out impl.) and LAMOL0TA.2SK, the results areaveraged over two runs. The p-value of pairted t-test between eight numbers of MBPA++ (our impl.)and LAMOL0TA.2SK is smaller than 1%, which shows that there is significant difference. Our imple-mentation of MBPA++ is available at https://github.com/Daikon-Sun/EM-in-LLL.
Table 6:	Summary of averaged score on reversed five tasks. The scores are reported as the averagedscore over all tasks of the models after training on every task. The rightmost three columns 一LAMOL with γ = 0.05 and γ = 0.2 of real samples from previous tasks. Best performance inboldface.
Table 7:	Examples generated by LAMOL with task-specific tokens. Annotations __squad1 __,_wkisql__, __sst__, =rl__ correspond to each task-specific token of SQuAD, WikiSQL, SST, andQA-SRL, respectively. __ans__ is the ANS token that separates the question from the answer. Theupper frame shows the normal situation whereas the lower frame shows generated contents that areinconsistent with their task-specific token.
