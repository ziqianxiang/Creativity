Table 1: The values chosen for Ep on the different datasets and the expected 12-robustnesslevel (last column) given E1 and E∞, computed according to (6).
Table 2: We report, for the different datasets and training schemes, the test error (TE) andlower (LB) and upper (UB) bounds on the robust test error (in percentage) wrt the union oflp -norms for p ∈ {1, 2, ∞} denoted as l1 + l2 + l∞ (that is the largest test error possible ifany perturbation in the union l1 + l2 + l∞ is allowed). The training schemes compared areplain training, adversarial trainings of Madry et al. (2018); Tramer & Boneh (2019) (AT),robust training of Wong & Kolter (2018); Wong et al. (2018) (KW), MMR regularizationof Croce et al. (2019a), MMR combined with AT (MMR+AT) and our MMR-Universalregularization. The models of our MMR-Universal are the only ones which have non trivialupper bounds on the robust test error for all datasets.
Table 3: Percentage of l1-adversarial examples contained in the l∞-ball and vice versa.
Table 4: We report, for the different datasets and training schemes, the test error (TE) andlower (LB) and upper (UB) bounds on the robust test error (in percentage) wrt the lp-normsat thresholds 与,with P = 1, 2, ∞ (that is the largest test error possible if any perturbationof lp-norm equal to tp is allowed). Moreover We show the 11 + 12 + l∞-UB, that is the upperbound on the robust error when the attacker is allowed to use the union of the three lp-balls.
Table 5: Robustness of other combinations of MMR and AT.
Table 6: MMR-Universal models trained on the "Large" architecture from Wong et al. (2018)with the same tp as in the experiments in Section 5. * For the l∞-robustness in this case themethod of Tjeng et al. (2019) is not used.
