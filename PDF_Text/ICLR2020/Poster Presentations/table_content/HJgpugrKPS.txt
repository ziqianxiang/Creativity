Table 1: Comparing SESN to SiCNN Xu et al. (2014), SI-ConvNet Kanazawa et al. (2014), SEVFMarcos et al. (2018), DSS Worrall & Welling (2019) and SS-CNN Ghosh & Gupta (2019). “In-terscale” refers to the ability of capturing interscale interactions with kernels of non-unitary scaleextent. “Grid” stands for the scales which generate images which lie exactly on the initial pixel grid.
Table 2: Classification error of different methods on MNIST-scale dataset, lower is better. In ex-periment we use image resolution of 28 × 28 and 56 × 56. We test both the regime without dataaugmentation, and the regime with scaling data augmentation, denoted with “+”. All results arereported as mean ± std over 6 different fixed realizations of the dataset. The best results are bold.
Table 3: Classification error onSTL-10. The best results are bold.
Table 4: Average time per epoch during training on input data with resolution 28 × 28 and 56 × 56.
Table 5: Number of channels in convolutional layers, number of units in fully-connected layers andnumber of scales used by different models in Section 6.2.
Table 6: Number of channels in convolutional blocks and number of scales used by different modelsin Section 6.2. We report the number of channels up to the widening factor.
