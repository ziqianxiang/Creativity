Table 1: Performance and memory comparisonbetween adaptive optimizers for GBW languagemodeling with a Transformer network.
Table 2: Comparison of memory-efficient op-timizers on a double-sized model, so that to-tal memory consumption is lower than that ofAdaGrad/Adam on the smaller model. Finalperplexities are given with the same runningtime allowance as the corresponding main ex-periment (middle column), as well as the sameiteration count (500K steps; right column).
Table 3: Wall clock comparisons for optimizing the base Transformer model.
Table 4: Tensor indices used for different levels of extreme tensoring for the ResNet-18 model onCifar-10.
Table 5: Performance and memory comparison between adaptive optimizers for Cifar-10 classifica-tion with a ResNet-18 network.
Table 6: Results of Transformer experiments with SM3 preconditioner estimates. The same mono-tonic behavior is exhibited as when the extreme tensoring granularity is varied.
