Table 1: Comparison of different methods in terms of perplexity on WikiText-2 dataset for the taskof language modeling.
Table 2: Comparison of different methods in terms of perplexity on WikiText-103 dataset for the taskof language modeling.
Table 3: Comparison of different methods in terms of isotropy for the tasks of language modelingand machine translation. (For perfect isotropy, I1 (W) = 1, I2(W) = 0.)Language Modeling	Machine TranslationMethod	Ii(W)	I2(W)	Method	I1 (W)	I2 (W)Standard Transformer-XL	0.24	0.037 Ours	0.63	0.022	Transformer-Base	0.31	0.031 Ours	0.88	0.005Table 4: Comparison of different methods in terms of BLEU scores on the task of De→En machinetranslation, trained on IWSLT 2014 dataset.
Table 4: Comparison of different methods in terms of BLEU scores on the task of De→En machinetranslation, trained on IWSLT 2014 dataset.
Table 5: Comparison of different methods in terms of BLEU scores on the task of En→De machinetranslation, trained on WMT 2014 dataset.
Table 6: Comparison of different penalty functions in terms of perplexity on WikiText-2 dataset.
Table 7: Comparisons of our method and baseline method in terms of the average training time perepoch and the memory cost.
Table 8: Comparisons of different priors on different datasets.
