Table 1: Scaling behavior of local SGD inclock-time for increasing number of work-ers K, for different number of local steps H ,for training ResNet-20 on CIFAR-10 withBloc = 128. The reported speedup (averagedover three runs) is over single GPU trainingtime for reaching the baseline top-1 test accu-racy (91.2% as in (He et al., 2016a)). We usea 8 × 2-GPU cluster with 10 Gbps network.
Table 2: Test performance (generalization) for local SGD variants and mini-batch SGD (highlighting selecteddata from Figure 2, Table 3 and Figure 3). Mini-batch SGD suffers from the generalization gap (and sometimeseven from optimization issues due to insufficient training) when scaling to larger H and K. Same experimentalsetup as in Figure 2 (fine-tuned mini-batch SGD baselines). The ς→, denotes the transition at the first learningrate decay. The reported results are averaged over three runs.
Table 3: Top-1 test accuracy of training different CNN models via post-local SGD on K = 16 GPUs witha large global batch size (KBIOC = 2048). The reported results are the average of three runs and all settingsaccess to the same total number of training samples. We compare to small and large mini-batch baselines. The ,indicates a fine-tuned learning rate, where the tuning procedure refers to Appendix A.4.
Table 4: Top-1 test accuracy of training ResNet-20 on CIFAR via sign-based compression scheme and post-local SGD (KBloc = 2048 and K = 16). H = 1 in the table for the simplicity corresponds to the originalsign-based algorithm and we fine-tune their hyper-parameters. The reported results are the average of threeruns with different seeds. The learning rate schedule is fixed for different local update steps (for H > 1). Forthe detailed tuning procedure, training scheme, as well as the pseudo code of the used variants, we refer toAppendix C.5.5.
Table 5: The Top-1 test accuracy of training ResNet50 on ImageNet. We report the performance (w/ or w/opost-local SGD) achieved by using SGD with Nesterov Momentum, learning schemes in (Goyal et al., 2017),and LARS. We follow the general experimental setup described in Section A.4.2, and use K 32 and H = 4.
Table 6: Scaling ratio for different models.
Table 7: The system performance of running forward and backward pass on a single GPU, for training ResNet20Tim CTΠΔR-1∩ Thp ''Rc"c'' indimfpe tbp Time of evW^Eg 4096 SamPleS With SPeCfiedmini-batch SiZe Thp ''T彳mA' K i∏ QprCnrkon c^IFI^AR 10. Ihe ^Ratιo indicates tιιe T. C 1 ,- ，ccz . ,∙∣ . . 1 ,匚,，ccz . Ihe iiɪme is in seconds.
Table 8: Evaluate local momentum and global momentum for ResNet-20 on CIFAR-10 data via local SGDtraining (H = 1 case) on 5 × 2-GPU Kubernetes cluster. The local mini-batch size is 128 and base batch sizeis 64 (used for learning rate linear scale). Each local model will access to a disjoint data partition, using thestandard learning rate scheme as He et al. (2016a).
Table 9: The Speedup of mini-batch SGD and post-local SGD, over different CNN models and datasets. Thespeedup is evaluated by TTa, where Ta is the training time of the algorithm a on 1 GPU and TK corresponds toTathe training on K GPUs. We use 16 GPUs in total (with Bloc = 128) on an 8 义 2-GPU cluster with 10 Gbpsnetwork bandwidth. The experimental setup is the same as Table 3.
Table 10: The Speedup of mini-batch SGD and post-local SGD (only consider the phase of performingpost-local SGD) over different CNN models and datasets. The speedup is evaluated by Ta, where Ta is theTatraining time of the algorithm a (corresponding to the second phase) on 1 GPU and TaK corresponds to thetraining on K GPUs. We use 16 GPUs in total (with Bloc = 128) on an 8 × 2-GPU cluster with 10 Gbps networkbandwidth. The experimental setup is the same as Table 3.
Table 11: Top-1 test accuracy of training different CNN models via post-local SGD on K = 32 GPUs witha large batch size (BlocK = 4096). The reported results are the average of three runs. We include the smalland large batch baseline, where the models are trained by mini-batch SGD with mini-batch size 256 and 4096respectively. The ★ indicates the fine-tuned learning rate.
Table 12: ToP-1 teSt accuracy of large-batch SGD and Post-local SGD, for training ResNet-20 on CIFAR-100with BlocK = 4096. The reported results are the average of three runs. We include the small and large batchbaseline, where the models are trained by mini-batch SGD with 256 and 4096 respectively. The ★ indicates thefine-tuned learning rate. The learning rate will be decayed by 10 when the distributed algorithm has accesses50% and 75% of the total number of training samples.
Table 13: The perplexity (lower is better) of language modeling task on WikiText-2. We use K = 16 andKB = KBloC = 1024. The reported results are evaluated on the validation dataset (average of three runs). Wefine-tune the learning rate for mini-batch SGD baselines.
Table 14: Top-1 test accuracy of adding isotropic noise for large-batch training. We revisit the case ofResNet-20 in Table 3 (2K for CIFAR-10) and Table 11 (4K for CIFAR-100), where adding isotropic noiseas in Neelakantan et al. (2015) cannot address the large-batch training difficulty. The ★ indicates a fine-tunedlearning rate.
Table 15: Top-1 test accuracy of training ReSNet-20 on CIFAR (KBlOC = 2048 and K = 16).
Table 16: Training CIFAR-10 with ReSNet-20 via local SGD on a 8 × 2-GPU cluster. The local batch size Blocis fixed to 128 with Hb = 1, and We scale the number of local steps H from 1 to 1024. The reported trainingtimeS are the average of three runs and all the experiments are under the same training configurations for theequivalent of 300 epochs, without specific tuning.
Table 17: The performance of training CIFAR-10 with ResNet-20 via hierarchical local SGD on a 16-GPUKubernetes cluster. We simulate three different types of cluster topology, namely 8 nodes with 2 GPUs/node, 4nodes with 4 GPUs/node, and 2 nodes with 8 GPUs/node. The configuration of hierarchical local SGD satisfiesH-Hb = 16. All variants either synchronize within each node or over all GPUs, and the communication cost isestimated by only considering H-Hb = 16 model updates during the training (the update could come from adifferent level of the synchronizations). The reported results are the average of three runs and all the experimentsare under the same training configurations, training for the equivalent of 300 epochs, without specific tuning.
