Table 1: Few-shot classification performance on conventional 4-layer convolutional neural networks. Allreported results are average performances over 1000 randomly selected episodes with standard errors for 95%confidence interval over tasks.
Table 2: Comparison against existing perturbation-based regularization techniques. The performancesare obtained by applying the regularizers to the inner gradient steps of MAML.
Table 3: Ablation study on the noise types applied to the inner-gradient steps. (X) means that the baseline isa core component of Meta-dropout.)Models (MAML+)	Rand. samp.	Learn. mult.	Input dep.	Omniglot 20-way		miniImageNet 5-way					1-shot	5-shot	1-shot	5-shotNone	X	X	X	95.23±0.17	98.38±0.07	49.58±0.65	64.55±0.52Fixed Gaussian (X)	O	X	-^X^^	95.44±0.17	98.99±0.06	49.39±0.63	66.84±0.54Weight Gaussian	O	X	X	94.32±0.18	98.35±0.07	49.37±0.64	64.78±0.54Independent Gaussian	O	O	X	94.36±0.18	98.26±0.08	50.31±0.64	66.97±0.54MAML + More param	X	O	O^^	95.83±0.15	97.85±0.09	50.63±0.64	65.20±0.51Determ. Meta-drop. (X)	X	O	O	95.99±0.14	97.78±0.09	50.75±0.63	65.62±0.53Meta-drop. w/ learned var.	O	O	O	95.98±0.15	98.87±0.06	50.93±0.68	66.15±0.56Meta-dropout	O	O	O^^	96.63±0.13	98.73±0.06	51.93±0.67	67.42±0.52(a) Omniglot 1-shot (`1 )80m40mAU2n8eepsilon(b)Omniglot 1-shot ('2) (C)Omniglot 1-shot ('∞) (d)Omniglot 5-shot ('∞)Figure 5:	Adversarial robustness against PGD attack with varying size of radius . The region of cleanaCCuraCies are magnified for better visualization.
