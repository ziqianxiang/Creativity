Table 1: An example: generating sentences based on different templates.
Table 2: Dataset statistics in our experiments.
Table 3: Result for SpNlg data set. Under the 0.05 significance level, VTM gets significantly higherresults in all the fluency metrics than all the baselines except Table2seq-beam.
Table 4: Human evaluation results on different models.
Table 5: Results for Wiki dataset. All the metrics Figure 5: Quality-diversity trade-off curveare significant under 0.05 significance level.	compared with NER+Table2seq.
Table 6: Computational cost for each model.
Table 7: An example of the generated text by our model and the baselines on Wiki dataset.
Table 8: An example of the generated text by our model and the baselines on SpNlg dataset.
