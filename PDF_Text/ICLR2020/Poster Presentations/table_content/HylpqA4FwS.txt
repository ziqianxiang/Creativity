Table 1: Results for Pixel-by-Pixel MNIST and Permuted MNIST datasets. K denotes pre-defined recursionsembedded in graph to reach equilibrium.
Table 2: Results for Noise Padded CIFAR-10 and MNIST datasets. Since the equilibrium surface is smooth andresilient to small perturbations, iRNN achieves better performance than the baselines with faster convergence.
Table 3: Results for Activity Recognition Datasets. iRNN outperforms the baselines on all metrics even withK = 1. Its worth noticing that although K = 5 increases test time, it’s well within LSTM’s numbers, the overalltrain time and resulting performance are better than K = 1.
Table 4: Dataset Statistics & Long Term DependenceDataset	Avg. Activity Time	InpUt Time	Sequence Ratio	#Train	#Fts	#Steps	#TestGoogle-30	25ms	1000ms	3/99	51,088	32	99	6,835HAR-2	256ms	2560ms	13/128	7,352	9	128	2,947Noisy-MNIST	28	1000	7/250	60,000	28	1000	10,000Noisy-CIFAR	32	1000	4/125	60,000	96	1000	10,000Pixel-MNIST				60,000	1	784	10,000PermUted-MNIST				60,000	1	784	10,000A.3 Convergence Guarantees for General Learning Rates.
Table 5: Various hyper-parameters to reproduce resultsDataset Hidden UnitsGoogle-30HAR-2Pixel-MNISTPermuted-MNISTNoisy-MNISTNoisy-CIFARAddition TaskCopying TaskPTB8888886808012121212121225A.6 Hyper-parameters for reproducibilityWe report various hyper-parameters we use in our experiments for reproduciblity. As mentionedearlier we mainly use ’ReLU’ as the non-linearity and Adam as the optimizer. Apart from this, otherhyper-parameters are mentioned in table 5.
Table 6: Other Dataset Statistics & Long Term DependenceDataset	Avg. Acitivity Time	Input Time	Sequence Ratio	#Train	#Fts	#Steps	#TestGoogle-12	25ms	1000ms	3/99	22,246	32	99	3,081DSA-19	500ms	5000ms	13/125	4,560	45	125	4,560Yelp-5	20	300	1/15	500,000	128	300	500,000PTB				929,589	300	300	82,430(a)Figure 5: Mean Squared Error shown for the Add Task (Sequence Length) : (c) 100 (d) 400(b)A.7 Additional ExperimentsA.7.1 Copying and Addition TasksFigure 5 shows the results for remaining experiments for the addition task for length 100, 400.
Table 7: Results for Pixel-by-Pixel MNIST and Permuted MNIST datasets. K denotes pre-definedrecursions embedded in graph to reach equillibrium.
Table 8: Results for Yelp Dataset.
Table 9: Results for Activity Recoginition Datasets.
Table 10: PTB Language Modeling: 1 Layer. To be consistent with our other experiments we useda low-dim U; For this size our results did not significantly improve with K. This is the dataset ofKusupati et al. (2018) which uses sequence length 300 as opposed to 30 in the conventional PTB.
Table 11: HAR-2 dataset (Sigmoid, ReLU activations): K denotes pre-defined recursions embeddedin graph to reach equillibrium.
