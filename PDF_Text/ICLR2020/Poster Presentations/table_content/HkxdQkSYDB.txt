Table 1: Battle	DGN	DGN-R	DGN-M	MFQ	CommNet	DQNmean reward	0.91	0.84	0.50	0.70	0.03	-0.03# kills	220	208	121	193	7	2# deaths	97	101	84	92	27	74kill-death ratio	2.27	2.06	1.44	2.09	0.26	0.036Published as a conference paper at ICLR 2020emies, DGN agents learn to move against and attack one of the enemy’s open flanks, as depictedin Figure 5a. CommNet agents adopt an active defense strategy. They seldom launch attacks butrather run away or gather together to avoid being attacked. DQN agents driven by self-interest failto learn a rational policy. They are usually forced into a corner and passively react to the enemy’sattack, as shown in Figure 5b. MFQ agents do not effectively cooperate with each other because themean action incurs the loss of important information that could help cooperation. In DGN, relationkernels can extract high order relations between agents through graph convolution, which can beeasily exploited to yield cooperation. Therefore, DGN outperforms other baselines.
Table 2: Jungle	DGN	MFQ	CommNet	DQNmean reward	0.66	0.62	0.30	0.24# attacks	1.14	2.74	5.44	7.357Published as a conference paper at ICLR 2020Table 3: Routing(N, L)		Floyd FlOydW/BL DGN			MFQ	CommNet	DQN	mean reward			1.23	1.02	0.49	0.18(20, 20)	delay	6.3	8.7	8.0	9.4	18.6	46.7	throughput	3.17	2.30	2.50	2.13	1.08	0.43	mean reward			0.86	0.78	0.39	0.12(40, 20)	delay	6.3	13.7	9.8	11.8	23.5	83.6	throughput	6.34	2.91	4.08	3.39	1.70	0.49	mean reward			0.73	0.59	0.31	0.06(60, 20)	delay	6.3	14.7	12.6	15.5	27.0	132.0	throughput	9.52	4.08	4.76	3.87	2.22	0.45shown in Figure 5c. Moreover, attacks between DGN agents are much less than others, e.g., 2×less than MFQ. Sneak attack, fierce conflict, and hesitation are the characteristics of CommNet andDQN agents, as illustrated in Figure 5d, verifying their failure of learning cooperation.
Table 3: Routing(N, L)		Floyd FlOydW/BL DGN			MFQ	CommNet	DQN	mean reward			1.23	1.02	0.49	0.18(20, 20)	delay	6.3	8.7	8.0	9.4	18.6	46.7	throughput	3.17	2.30	2.50	2.13	1.08	0.43	mean reward			0.86	0.78	0.39	0.12(40, 20)	delay	6.3	13.7	9.8	11.8	23.5	83.6	throughput	6.34	2.91	4.08	3.39	1.70	0.49	mean reward			0.73	0.59	0.31	0.06(60, 20)	delay	6.3	14.7	12.6	15.5	27.0	132.0	throughput	9.52	4.08	4.76	3.87	2.22	0.45shown in Figure 5c. Moreover, attacks between DGN agents are much less than others, e.g., 2×less than MFQ. Sneak attack, fierce conflict, and hesitation are the characteristics of CommNet andDQN agents, as illustrated in Figure 5d, verifying their failure of learning cooperation.
Table 4: HyperparametersHyperparameter	DGN	CommNet	MFQ	DQNdiscount (γ)		0.96, 0.96, 0.98		batch size		10		buffer capacity		2 × 105		β		0.01		and decay		0.6/0.996		optimizer		Adam		learning rate		10-4		# neighbors	3	-	3	-# convolutional layers	2		-	# attention heads	8		-	τ	0.25		-	λ	0.03		-	κ	2		-	# encoder MLP layers	2	2	-	-# encoder MLP units	(512, 128)	(512, 128)	-	-Q network	affine transformation affine transformation		(1024, 256) (1024, 256)	MLP activation		ReLU		initializer		random normal		
