Table 1: Ablation study in the unsupervised setting. We train a BiGAN on CIFAR-10 and useits encoder as the base network for classification of the same dataset. All results are reported usingclassification accuracy. We obtain the pre-trained & and a from the pre-trained base network, andω by first training the activation baseline for the target task. The specifications of θι, θ2 and ω onlyaffects gradient evaluation. Activation features are always output from the pre-trained base network.
Table 2: Ablation study in the supervised setting. We use the PyTorch distribution of ImageNetpre-trained ResNet18 as the base network for VOC07 object classification. All results are reportedusing mean average precision (mAP). Predictions are based on a single center crop at test time.
Table 3: Unsupervised Learning Results. We consider two base networks, namely the encoder ofa BiGAN and that of a VAE, trained on three datasets. Our target task is image classification on thesame datasets. All results are reported using classification accuracy.
Table 4: Self-supervised and Transfer Learning Results. We consider a ResNet50 pre-trained onthe jigsaw pretext task, and a ResNet18 pre-trained on the ImageNet classification task. Our targettask is VOC07 and COCO2014 object classification. All results are reported using mean averageprecision (mAP). Predictions are averaged over ten random crops at test time.
