Table 1: Performance of baselines and finetuned-LMapproaches on the test set of ART. Test accuracy is re-ported as the mean of five models trained with randomseeds, with the standard deviation in parenthesis.
Table 2: Performance of generative models on the test set of ART. All models except GPT2-Fixedare finetuned on ART.
Table 3: BERTâ€™s performance and human evalua-tion on categories for 1,000 instances from the testset, based on commonsense reasoning domains(Numerical, Spatial, Emotional). The number inparenthesis indicates the size of the category.
Table 4: Fraction of dataset for which a particulartransition in the story is broken for the negativehypothesis, for 1,000 random instances from thetest set.
Table 5: Transfer Learning from ART7	TRANSFER LEARNING FROM ARTART contains a large number of questions for the novel abductive reasoning task. In addition to serv-ing as a benchmark, we investigate if ART can be used as a resource to boost performance on othercommonsense tasks. We apply transfer learning by first training a model on ART, and subsequentlytraining on four target datasets - WinoGrande Sakaguchi et al. (2020), WSC LeVesqUe et al. (2011),DPR Rahman & Ng (2012) and HellaSwag Zellers et al. (2019). We show that compared to a modelthat is only trained on the target dataset, a model that is sequentially trained on ART first and thenon the target dataset can perform better. In particular, pre-training on ART consistently improVesperformance on related datasets when they haVe relatiVely few training examples.
Table 6: Some statistics summarizing the ART dataset. The train set includes all plausible and im-plausible hypotheses collected via crowdsourcing, while the dev and test sets include the hypothesesselected through the Adversarial Filtering algorithm.
Table 7: Input formats for GPT and BERT fine-tuning.
Table 8: Input format used to training and generated text from various GPT2 based models. cijrefers to the COMeTembeddings obtained using a separate transformer model for relation i andobservation j . Similarly, Tij is the textual phrase for relation i, observation j. Where appropriate,field specific start and end-tags are added to the sequence of inputs.
