Table 1: Element-wise pruning results on LeNet-300-100 model @ accuracy 98.4%Nonzero wights left after pruningMethod	Total	FC1	FC2	FC3Orig	266.2k	235.2k	30k	1k(Han et al., 2015b)	21.8k (8%)	18.8k (8%)	2.7k (9%)	260 (26%)(Zhang et al., 2018)	11.6k (4.37%)	9.4k (4%)	2.1k (7%)	120 (12%)(Lee et al., 2019)	13.3k (5.0%)	Not reported in (Lee et al., 2019)		(Ma et al., 2019)1	6.4k (2.40%)	5.0k(2.11%)	1.2k (4.09%)	209 (20.90%)Hoyer	6.0k (2.27%)	5.3k (2.25%)	672 (2.24%)	82 (8.20%)Hoyer-Square	4.6k (1.74%)	3.7k (1.57%)	768 (2.56%)	159 (15.90%)Table 2: Element-wise pruning results on LeNet-5 model @ accuracy 99.2%Method	Nonzero wights left after pruning					Total	CONVI	CONV2	FC1	FC2Orig	430.5k	500	25k	400k	5k(Han et al., 2015b)	36k (8%)	330 (66%)	3k (12%)	32k (8%)	950 (19%)(Zhang et al., 2018)	6.1k (1.4%)	100 (20%)	2k (8%)	3.6k (0.9%)	350 (7%)(Lee et al., 2019)	8.6k (2.0%)	Not reported in (Lee et al., 2019)			(Ma et al., 2019)1	5.4k (1.3%)	100 (20%)	690 (2.8%)	4.4k (1.1%)	203 (4.1%)Hoyer	4.0k (0.9%)	53(10.6%)	613 (2.5%)	3.2k (0.8%)	136 (2.7%)Hoyer-Square	3.5k (0.8%)	67 (13.4%)	848 (3.4%)	2.4k (0.6%)	234 (4.7%)
Table 2: Element-wise pruning results on LeNet-5 model @ accuracy 99.2%Method	Nonzero wights left after pruning					Total	CONVI	CONV2	FC1	FC2Orig	430.5k	500	25k	400k	5k(Han et al., 2015b)	36k (8%)	330 (66%)	3k (12%)	32k (8%)	950 (19%)(Zhang et al., 2018)	6.1k (1.4%)	100 (20%)	2k (8%)	3.6k (0.9%)	350 (7%)(Lee et al., 2019)	8.6k (2.0%)	Not reported in (Lee et al., 2019)			(Ma et al., 2019)1	5.4k (1.3%)	100 (20%)	690 (2.8%)	4.4k (1.1%)	203 (4.1%)Hoyer	4.0k (0.9%)	53(10.6%)	613 (2.5%)	3.2k (0.8%)	136 (2.7%)Hoyer-Square	3.5k (0.8%)	67 (13.4%)	848 (3.4%)	2.4k (0.6%)	234 (4.7%)5	Experiment resultThe proposed DeepHoyer regularizers are first tested on the MNIST benchmark using the LeNet-300-100 fully connected model and the LeNet-5 CNN model (LeCun et al., 1998). We also conducttests on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009) with ResNet models (He et al., 2016)in various depths, and on ImageNet ILSVRC-2012 benchmark (Russakovsky et al., 2015) with theAlexNet model (Krizhevsky et al., 2012) and the ResNet-50 model (He et al., 2016). All the modelsare implemented and trained in the PyTorch deep learning framework (Paszke et al., 2017), wherewe match the model structure and the benchmark performance with those of previous works for thefairness of comparison. The experiment results presented in the rest of this section show that theproposed DeepHoyer regularizers consistently outperform previous works in both element-wise and
Table 3: Element-wise pruning results on AlexNet model.
Table 4: Structural pruning results on LeNet-300-100 model			Method	AccUracy	#FLOPs	PrUned strUctUreOrig	98.4%	266.2k	784-300-100Sparse VD (Molchanov et al., 2017)	98.2%		67.3k (25.28%)	512-114-72BC-GNJ (Louizos et al., 2017a)	98.2%	28.6k (10.76%)	278-98-13BC-GHS (Louizos et al., 2017a)	98.2%	28.1k (10.55%)	311-86-14'θhc (LoUizos et al., 2017b)	98.2%	26.6k (10.01%)	266-88-33Bayes 'ιtrim (YUn et al., 2019)	98.3%	20.5k (7.70%)	245-75-25GroUp-HS	98.2%	16.5k (6.19%)	353-45-11the compression rate by 21.3×. This result is the highest among all methods, even better than theADMM method (Zhang et al., 2018) which requires two additional Lagrange multipliers and involvesthe optimization of two objectives. Considering that the optimization of the Hoyer-Square regularizercan be directly realized on a single objective without additional variables, we conclude that theHoyer-Square regularizer can achieve a sparse DNN model with a much lower cost. A more detailedlayer-by-layer sparsity comparison of the compressed model can be found in Appendix C.2.
Table 5: Structural pruning result on LeNet-5 modelMethod	AccUracy	#FLOPs	PrUned strUctUreOrig	99.2%	2293k	20-50-800-500Sparse VD (Molchanov et al., 2017)	99.0%	660.2k (28.79%)	14-19-242-131GL (Wen et al., 2016)	99.0%	201.8k (8.80%)	3-12-192-500SBP (Neklyudov et al., 2017)	99.1%	212.8k (9.28%)	3-18-284-283BC-GNJ (Louizos et al., 2017a)	99.0%	282.9k (12.34%)	8-13-88-13BC-GHS (Louizos et al., 2017a)	99.0%	153.4k (6.69%)	5-10-76-16'0hc (LoUiZos et al., 2017b)	99.0%	390.7k (17.04%)	9-18-26-25Bayes 七…(YUn et al., 2019)	99.0%	334.0k (14.57%)	8-17-53-19GroUp-HS	99.0%	169.9k (7.41%)	5-12-139-13ResNet-HOon CIFAR-10ResNet-56 on CIFAR-10ResNet-50 on ImageNet5.5ɔ Previous worksDeepHoyerPrevious works* DeepHoyerFigure 3: Comparisons of accuracy-#FLOPs tradeoff on ImageNet and CIFAR-10, black dash lines
Table 6: Hyper parameter used for MNIST benchmarksModel	LeNet-300-100	LeNet-5Regularizer Decay	Threshold/std ∣ Decay	Threshold/stdHoyer	0.02	0.05	0.01	0.08Hoyer-Square	0.0002	0.03	0.0001	0.03Group-HS	0.002	0.8	0.1	0.008Transformed `1	2e-5	0.3	2e-5	0.6is set to zero and is fixed during the final finetuning. The pruned model is finetuned for another 100steps without DeepHoyer regularizers and the best testing accuracy achieved is reported. Detailedparameter choices used in achieving the reported results are listed in Table 6.
Table 7: Element-wise pruning results on AlexNet without accuracy loss. Refer to Table 3for the full reference of the mentined methods.
Table 8: Structural pruning result of the ResNet-50 model on imageNet.
Table 9: Structural pruning result of the ResNet-56 model on CIFAR-10.
Table 10: Structural pruning result of the ResNet-110 model on CIFAR-10.
