Table 1: The pruning results on MNIST for various architectureswith other sparse learning algorithms on CIFAR-10 as presented in Table 2. The state-of-the-artalgorithms, DSR (Mostafa & Wang, 2019) and Sparse momentum (Dettmers & Zettlemoyer, 2019),are selected for comparison. Dynamic Sparse Training (DST) outperforms them in almost all thesettings as present in Table 2.
Table 2: Comparison with other sparse training methods on CIFAR-10.
Table 3: Comparison with other sparse training methods for ResNet-50 on ImageNet.
