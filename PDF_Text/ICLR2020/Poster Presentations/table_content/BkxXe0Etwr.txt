Table 1: The mean ± standard deviation of (95-percentile) final returns with the best hyper-parameter configuration. CAQL significantly outperforms NAF on most benchmarks, as well asDDPG, TD3, and SAC on 11/14 benchmarks.
Table 2: The mean ± standard deviation of (95-percentile) final returns over all 320 configurations(32 hyper parameter combinations × 10 random seeds). CAQL policies are less sensitive to hyperparameters on 11/14 benchmarks.
Table 3: Ablation analysis on CAQL-GA with dual filtering and clustering, where both the mean ±standard deviation of (95-percentile) final returns and the average %-max-Q-reduction (in parenthe-sis) are based on the best configuration. See Figure 7 in Appendix E for training curves.
Table 4: Ablation analysis on CAQL-GA with dynamic tolerance, where both the mean ± standarddeviation of (95-percentile) final returns and the average number of GA iterations (in parenthesis)are based on the best configuration. See Figure 9 in Appendix E for training curves. NOTE: In (*)the performance significantly drops after hitting the peak, and learning curve does not converge.
Table 5: Ablation analysis on CAQL-MIP with dynamic tolerance, where both the mean ± standarddeviation of (95-percentile) final returns and the (median, standard deviation) of the elapsed time κ(in msec) are based on the best configuration. See Figure 11 in Appendix E for training curves.
Table 6: Benchmark Environments. Various action bounds are tested from the default one to smallerones. The action range in bold is the default one. For high-dimensional environments such asWalker2D, HalfCheetah, Ant, and Humanoid, we only test on action ranges that are smaller than thedefault (in bold) due to the long computation time for MIP. A smaller action bound results in a MIPthat solves faster.
Table 7: Hyper parameters settings for CAQL(+ MIP, GA, CEM) and NAF. We sweep over theQ-function learning rates, action function learning rates, and exploration noise decays.
Table 8: Hyper parameters settings for DDPG, TD3, and SAC. We sweep over the critic learningrates, actor learning rates, temperature,and exploration noise decays.
Table 9: The (median, standard deviation) for the average elapsed time κ (in msec) of various solverscomputing max-Q problem.
Table 10: The mean ± standard deviation of (95-percentile) final returns over all 320 configura-tions (32 hyper parameter combinations × 10 random seeds). The full training curves are given inFigure 12.
