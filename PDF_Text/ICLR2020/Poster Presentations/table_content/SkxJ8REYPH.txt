Table 1: Comparisons to the original distributed optimization algorithms on various training tasks.
Table 2: Average time per iteration with and without SlowMo. Recall that τ = 48 for the SGP andOSGP base optimizer and τ = 12 for Local SGD/Local Adam. In some cases, with SlowMo wasfaster than without; we hypothesize that this is due to statistical variations in timing and backgroundnetwork traffic.
Table B.1: Validation NLL (lower is better) with and without SlowMo on WMT’16 En-De. Weobserve that SlowMo improves the validation NLL of SGP and Local Adam.
Table B.2: Effect of different buffer strategies: ImageNet, batch size:8k, 32 nodes.
Table B.3: Effect of different buffer strategies: WMT’16 En-De, batch size:200k, 8 nodes.
Table B.4: Validation Accuracy with and without SlowMo on CIFAR-10. Using SlowMo con-sistently improves the performance of the base algorithms.
Table C.1: Examples of update directions used by the base optimizer in SlowMo, where h(i)and v(i) denote the first-order and second-order momentum buffers respectively and βlocal, β1 , β2are the corresponding local momentum factors. When the local momentum buffers are reset at thebeginning of each inner loop, then ht(,i0) = 0, vt(,i0) = 0 and l = k; when the local momentum buffersare maintained, then ht(,i	0) = ht(-)1,τ , vt(,0) = vt(-)1,τ and l = tτ + k.
Table D.1: List of notations.
