Table 1: Accuracy of predicting reachability at different test-set sizes, trained on graphs of 20 nodes.
Table 2: Accuracy of predicting the shortest-path predecessor node at different test-set sizes. (cur-riculum) corresponds to a curriculum wherein reachability is learnt first. (no-reach) corresponds totraining without the reachability task. (no-algo) corresponds to the classical setup of directly trainingon the predecessor, without predicting any intermediate outputs or distances.
Table 3: Mean squared error for predicting the intermediate distance information from Bellman-Ford,and accuracy of the termination network compared to the ground-truth algorithm, averaged across alltimesteps. (curriculum) corresponds to a curriculum wherein reachability is learnt first. (no-reach)corresponds to training without the reachability task.
Table 4: Shortest-path predecessor accuracy of the MPNN-max model trained jointly with thereachability objective on 20-node graphs, at different test graph sizes (up to 75× larger).
Table 5: The predictive performance of MPNN-max on 100-node graphs, after training on 20-nodegraphs of a particular type (Erdos-Renyi, or trees).
Table 6: Accuracy of selecting the next node to add to the minimum spanning tree, and predicting theminimum spanning tree predecessor node—at different test-set sizes. (no-algo) corresponds to theclassical setup of directly training on the predecessor, without adding nodes sequentially.
Table 7: Summary of inputs and supervision signals of the three algorithms considered.		Algorithm	Inputs	Supervision signalsBreadth-first search	x(it) : is i reachable from s in ≤ t hops?	xi(t+1), τ(t) : has the algorithm terminated?Bellman-Ford	x(it): shortest distance from s to i (using ≤ t hops)	xi(t+1), τ(t), pi(t): predecessor of i in the shortest path tree (in ≤ t hops)Prim’s algorithm	x(it) : is node i in the (partial) MST (built from s after t steps)?	xi(t+1), τ(t), pi(t): predecessor of i in the partial MSTA	Summary of algorithm inputs and supervision signalsTo aid clarity, within Table 7, we provide an overview of all the inputs and outputs (supervisionsignals) for the three algorithms considered here (breadth-first search, Bellman-Ford and Prim).
Table 8: Results of scaling to large graphs with 1000 nodes, while training on graphs with 100 nodes.
Table 9: The predictive performance of MPNN-max on 100-node graphs—after training on 20-nodegraphs—partitioned by graph type.
