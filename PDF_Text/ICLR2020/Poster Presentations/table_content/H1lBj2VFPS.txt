Table 1: Comparison between different quantizers: All-Layer (AL) denotes quantizing all the pa-rameters of all the operators in networks, including weights, bias, activations, and the scaling factorfor low-precision networks; BN donates that the BN operation is only invoked in training but mergedinto weights and induces no overhead in integer inference; Linear-Symmetric (LS) denotes linearsymmetric quantization; Activation Functions (AF) donates the support of Leaky ReLU and activa-tion functions besides ReLU. Structure-Intact (SI) indicates the network structure is unmodified.
Table 2: Comparison of SG and EMA.
Table 3: Comparison with the state-of-the-art low-bit quantization methods on CIFAR-10. Thebitwidth for weights(w), activations(a), bias(b) and scaling factor(α) are given.
Table 4: LLSQ on YOLOv2 detector.
Table 5: Comparison of our low-precision integer Neural Network Processors.
Table 6: Train Time of ResNet18#Training Process	training timeTrain the fp32 network from scratch	10XQuantize w/a to 4/4 according to Step1 of Alg. 1	069XQuantize w/a to 3/3 according to Step1 of Alg. 1	069XQuantize b/a to 8/8 according to Step2 of Alg. 1	0.01x	—and it consists of five Conv layers, three FC layers, three MaxPool layers, and two Dropout layers.
Table 7: Comparison with state-of-the-art quantization methods on ImageNet. Top1, Top5 accu-racy(%) and degradation of Top1 are given.
