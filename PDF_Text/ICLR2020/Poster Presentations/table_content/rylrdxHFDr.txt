Table 1: Performance on Hopper-v2 and HalfCheetah-v2	HoPPer-v2			HalfCheetah-v2		# Demo	10		50		10		50	Expert		3566 ± 1.24				12294.22 ± 273.59		BC	1318.76 ± 804.36	3525.87 ± 160.74	971.42 ± 249.62	4813.20 ± 1949.26-GAIL-	3372.66 ± 130.75	3363.97 ± 262.77	474.42 ± 389.30	--175.83 ± 26.76-BC-GAIL	3132.11 ± 520.65	3130.82 ± 554.54	578.85 ± 934.34	1597.51 ± 1173.93-AIRL-	3.07 ± 0.02	3.31 ± 0.02	-146.46 ± 23.57	-755.46 ± 10.92-Our init	3412.58 ± 450.97	3601.16 ± 300.14	1064.44 ± 227.32	7102.29 ± 910.54Our final	3539.56 ± 130.36	3614.19 ± 150.74	1616.34 ± 180.76	8817.32 ± 860.555.3	Ablation study5.3.1	COEFFICIENT β IN β-VAEβ-VAE introduces an additional parameter to the original VAE. It controls the variance of the ran-domly sampled latent variable sampling, which subsequently affects the reconstruction quality androbustness. Theoretically, a smaller β leads to better state prediction quality, with the cost of losingthe deviation correction ability (Dai et al., 2018).
Table 2: Analyze the role of VAE coefficient. The “None” item means replacing VAE with anordinary network with linear layers.
Table 3: Compare behavior cloning to variational behavior cloningβ	Environments		HalfCheetah-50	Hopper-50-0.1-	-230.52 ± 13.26-	203.87 ± 14.390.01	1320.04 ± 15.43	438.10 ± 20.430.001	3306.91 ± 12.51	3303.72 ± 10.46None	4813.20 ± 1949.26	3525.87 ± 6T74-(a) HalfCheetah-v2(b) Humanoid-v2Figure 5: (a), (b) show the effects of Wasserstein distance and KL regularization on HalfCheetah-v2and Humanoid-v2 given 20 demonstration trajectories. And (c) presents the result on Antmaze.
Table 4: Performance on modifeid Swimmer	DiSabledSWimmer	LightSwimmer	HeavySwimmerBC	-249.09 ± 1.53-	277.99 ± 3.41	-255.95 ± 2.5-GAIL	-228.46 ± 2.02-	-4.11 ± 0.51	254.91 ± 1.35AIRL	-283.42 ± 3.69-	67.58 ± 25.09	301.27 ± 5.21SAIL(Ours)^^	287.71 ± 2.31	342.61 ± 6.14-	286.4 ± 3：2~~Table 5: Performance on modified Ant	DisabledAnt	HeavyAnt	LightAntBC	1042.45 ± 75.13	-550.6 ± 77.62-	4936.59 ± 53.42GAIL	-1033.54 ± 254.36	-1089.34 ± 174.13	-971.74 ± 123.14AIRL	-3252.69 ± 153.47	--62.02 ± 5.33-	-626.44 ± 104.31SAIL(Ours)^^	3305.71 ± 67.21	5608.47 ± 57.67	4335.46 ± 82.34B	Imitation B enchmark Experiments settings and resultsWe use six MuJoCo (Todorov et al., 2012) control tasks. The name and version of the environmentsare listed in Table 6, which also list the state and action dimension of the tasks with expert perfor-mance and reward threshold to indicate the minimum score to solve the task. All the experts aretrained by using SAC (Haarnoja et al., 2018) except Swimmer-v2 where TRPO (Schulman et al.,2015) get higher performance.
Table 5: Performance on modified Ant	DisabledAnt	HeavyAnt	LightAntBC	1042.45 ± 75.13	-550.6 ± 77.62-	4936.59 ± 53.42GAIL	-1033.54 ± 254.36	-1089.34 ± 174.13	-971.74 ± 123.14AIRL	-3252.69 ± 153.47	--62.02 ± 5.33-	-626.44 ± 104.31SAIL(Ours)^^	3305.71 ± 67.21	5608.47 ± 57.67	4335.46 ± 82.34B	Imitation B enchmark Experiments settings and resultsWe use six MuJoCo (Todorov et al., 2012) control tasks. The name and version of the environmentsare listed in Table 6, which also list the state and action dimension of the tasks with expert perfor-mance and reward threshold to indicate the minimum score to solve the task. All the experts aretrained by using SAC (Haarnoja et al., 2018) except Swimmer-v2 where TRPO (Schulman et al.,2015) get higher performance.
Table 6: Performance on benchmark Control tasksEnvironment	State Dim	Action Dim	Reward threshold	Expert PerformanceSwimmer-v2	8	2	360	332Hopper-v2	∏	3	3800	3566Walker2d-v2	17	6	-	4924Ant-v2	rn	8	6000	6157HalfCheetah-v2	17	6	4800	12294Humanoid-v2	376	17	—	1000	5187The exact performance of all methods are list in Table 7, 8, 9, 10, 11, 12. We compare GAIL(Ho &Ermon, 2016), behavior cloning, GAIL with behavior cloning initilization and AIRL to our methodcontaining. Means and standard deviations are calculated from 20 trajectories after the agents con-verge and the number total interactions with environments is less than one million environmentsteps.
Table 7: Performance on SWimmer-v2 with different trajectoriesSWimmer-v2#Demo	5	10	20	50Expert		332.88 ± 1.24				BC	328.85 土 2.26	331.17 土 2.4	332.17 ± 2.4	330.65 土 2.42GAIL	304.64 ± 3.16	271.59 ± 11.77	56.16 ± 5.99	246.73 ± 5.76BC-GAIL	313.80 ± 3.42	326.58 ± 7.87	294.93 ± 12.21	315.68 ± 9.99AIRL	332.11 ± 2.57	338.43 ± 3.65	335.67 ± 2.72	340.08 ± 2.70Our init	332.36 ± 3.62	335.78 ± 0.34	336.23 ± 2.53	334.03 ± 2.11Our final	332.22 ± 3.23	339.67 ± 3.21	336.18 ± 1.87	336.31 ± 3.20Table 8: Performance on Hopper-v2 with different trajectoriesHOPPer-v2				#Demo	5	10	20		50	Expert		3566 ± 1.24				BC	1471.40 ± 637.25	1318.76 土 804.36	1282.46 ± 772.24	3525.87 土 160.74-GAIL-	3300.32 ± 331.61	3372.66 ± 130.75	3201.97 ± 295.27	3363.97 ± 262.77BC-GAIL	3122.23 ± 358.65	3132.11 ± 520.65	3111.42 ± 414.28	3130.82 ± 554.54-AIRL-	4.12 ± 0.01	3.07 ± 0.02	4.11 ± 0.01	3.31 ± 0.02Our init	2322.49 ± 300.93	3412.58 ± 450.97	3314.03 ± 310.32	3601.16 ± 300.14Our final	3092.26 ± 670.72	3539.56 ± 130.36	3516.81 ± 280.98	3610.19 ± 150.74
Table 8: Performance on Hopper-v2 with different trajectoriesHOPPer-v2				#Demo	5	10	20		50	Expert		3566 ± 1.24				BC	1471.40 ± 637.25	1318.76 土 804.36	1282.46 ± 772.24	3525.87 土 160.74-GAIL-	3300.32 ± 331.61	3372.66 ± 130.75	3201.97 ± 295.27	3363.97 ± 262.77BC-GAIL	3122.23 ± 358.65	3132.11 ± 520.65	3111.42 ± 414.28	3130.82 ± 554.54-AIRL-	4.12 ± 0.01	3.07 ± 0.02	4.11 ± 0.01	3.31 ± 0.02Our init	2322.49 ± 300.93	3412.58 ± 450.97	3314.03 ± 310.32	3601.16 ± 300.14Our final	3092.26 ± 670.72	3539.56 ± 130.36	3516.81 ± 280.98	3610.19 ± 150.74Table 9: Performance on Walker2d-v2 With different trajectoriesWaIker2d-v2				#Demo	5	10	20		50	Expert-		5070.97 ± 209.19				BC	1617.34 土 693.63	4425.50 土 930.62	4689.30 ± 372.33	4796.24 土 490.05-GAIL-	1307.21 ± 388.55	692.16 ± 145.34	1991.58 ± 446.66	751.21 ± 150.18BC-GAIL	3454.91 ± 792.40	2094.68 ± 1425.05	3482.31 ± 828.21	2896.50 ± 828.18-AIRL-	--7.13 ± 0.11	-7.39 ± 0.09	-3.74 ± 0.13	--4.64 ± 0.09Ourinit -	1859.10 ± 720.44	2038.90 ± 260.78	4509.82 ± 1470.65	4757.58 ± 880.45Our final	2681.20 ± 530.67	3764.14 ± 470.01	4778.82 ± 760.34	4780.73 ± 360.66
Table 9: Performance on Walker2d-v2 With different trajectoriesWaIker2d-v2				#Demo	5	10	20		50	Expert-		5070.97 ± 209.19				BC	1617.34 土 693.63	4425.50 土 930.62	4689.30 ± 372.33	4796.24 土 490.05-GAIL-	1307.21 ± 388.55	692.16 ± 145.34	1991.58 ± 446.66	751.21 ± 150.18BC-GAIL	3454.91 ± 792.40	2094.68 ± 1425.05	3482.31 ± 828.21	2896.50 ± 828.18-AIRL-	--7.13 ± 0.11	-7.39 ± 0.09	-3.74 ± 0.13	--4.64 ± 0.09Ourinit -	1859.10 ± 720.44	2038.90 ± 260.78	4509.82 ± 1470.65	4757.58 ± 880.45Our final	2681.20 ± 530.67	3764.14 ± 470.01	4778.82 ± 760.34	4780.73 ± 360.66Table 10: Performance on Ant-v2 With different trajectories	Ant-v2					#Demo	5	10	20		50	Expert		6190.90 ± 254.18				BC	3958.20 土 661.28	3948.88 土 753.41	5424.01 ± 473.05	5852.79 土 572.97-GAIL-	-340.02 ± 59.02-	-335.25 ± 89.19-	314.35 ± 52.13	-284.18 ± 32.40-BC-GAIL	-1081.30 ± 673.65	-1177.27 ± 618.67	-13618.45 ± 4237.79	-1166.16 ± 1246.79-AIRL-	-839.32 ± -301.54	-386.43 ± 156.98	-586.07 ± 145.43	-393.90 ± 145.13OUr init	1150.82 ± 200.87	3015.43 ± 300.70	5200.58 ± 870.74	5849.88 ± 890.56OUr final	1693.59 ± 350.74	3983.34 ± 250.99	5980.37 ± 420.16	5988.65 ± 470.03
Table 10: Performance on Ant-v2 With different trajectories	Ant-v2					#Demo	5	10	20		50	Expert		6190.90 ± 254.18				BC	3958.20 土 661.28	3948.88 土 753.41	5424.01 ± 473.05	5852.79 土 572.97-GAIL-	-340.02 ± 59.02-	-335.25 ± 89.19-	314.35 ± 52.13	-284.18 ± 32.40-BC-GAIL	-1081.30 ± 673.65	-1177.27 ± 618.67	-13618.45 ± 4237.79	-1166.16 ± 1246.79-AIRL-	-839.32 ± -301.54	-386.43 ± 156.98	-586.07 ± 145.43	-393.90 ± 145.13OUr init	1150.82 ± 200.87	3015.43 ± 300.70	5200.58 ± 870.74	5849.88 ± 890.56OUr final	1693.59 ± 350.74	3983.34 ± 250.99	5980.37 ± 420.16	5988.65 ± 470.03Table 11: Performance on HalfCheetah-v2 With different trajectoriesHaIfCheetah-v2				#Demo	5	10	20		50	Expert		12294.22 ± 208.41				BC	225.42 ± 147.16	971.42 土 249.62	2782.76 土 959.67	4813.20 土 1949.26-GAIL-	-84.92 ± 43.29	474.42 ± 389.30	--116.70 ± 34.14	-175.83 ± 26.76-BC-GAIL	1362.59 ± 1255.57	578.85 ± 934.34	3744.32 ± 1471.90	1597.51 ± 1173.93-AIRL-	782.36 ± 48.98	-146.46 ± 23.57	1437.25 ± 25.45	-755.46 ± 10.92-OUr init	267.71 ± 90.38	1064.44 ± 227.32	3200.80 ± 520.04	7102.74 ± 910.54OUr final	513.66 ± 15.31	1616.34 ± 180.76	6059.27 ± 344.41	8817.32 ± 860.55
Table 11: Performance on HalfCheetah-v2 With different trajectoriesHaIfCheetah-v2				#Demo	5	10	20		50	Expert		12294.22 ± 208.41				BC	225.42 ± 147.16	971.42 土 249.62	2782.76 土 959.67	4813.20 土 1949.26-GAIL-	-84.92 ± 43.29	474.42 ± 389.30	--116.70 ± 34.14	-175.83 ± 26.76-BC-GAIL	1362.59 ± 1255.57	578.85 ± 934.34	3744.32 ± 1471.90	1597.51 ± 1173.93-AIRL-	782.36 ± 48.98	-146.46 ± 23.57	1437.25 ± 25.45	-755.46 ± 10.92-OUr init	267.71 ± 90.38	1064.44 ± 227.32	3200.80 ± 520.04	7102.74 ± 910.54OUr final	513.66 ± 15.31	1616.34 ± 180.76	6059.27 ± 344.41	8817.32 ± 860.5514Published as a conference paper at ICLR 2020Table 12: Performance on Humanoid-v2 with different trajectoriesHumanoid-v2				#Demo	5	10	20		50	Expert		5286.21 ± 145.98				BC	1521.55± 272.14	3491.07± 518.64	4686.05 ±355.74	4746.88 ±605.61-GaIL-	-485.92± 27.59-	-486.44 ±27.18-	-477.15± 22.07-	-481.14± 24.37-BC-GAIL	-363.68 ±44.44	-410.03 ±33.07-	-487.99± 30.77-	-464.91 ±33.21--AIrL-	-79.72 ± 4.27-	-87.15 ± 5.01-	-1293.86 ± 10.70	84.84 ± 6.46
Table 12: Performance on Humanoid-v2 with different trajectoriesHumanoid-v2				#Demo	5	10	20		50	Expert		5286.21 ± 145.98				BC	1521.55± 272.14	3491.07± 518.64	4686.05 ±355.74	4746.88 ±605.61-GaIL-	-485.92± 27.59-	-486.44 ±27.18-	-477.15± 22.07-	-481.14± 24.37-BC-GAIL	-363.68 ±44.44	-410.03 ±33.07-	-487.99± 30.77-	-464.91 ±33.21--AIrL-	-79.72 ± 4.27-	-87.15 ± 5.01-	-1293.86 ± 10.70	84.84 ± 6.46Our init	452.31 ± 190.12	1517.63 ± 11045	461025 ± 2750.86	4776.83 ± 1320.46OUr final	1225.58 ± 21088	2190.43 ± 280.18	4716.91 ±680.29	4780.07 ± 700.01C Hyper-parameter and network architectureWhen we pretrain the policy network with our methods, we choose β = 0.05 in β-VAE. We useAdam with learning rate 3e-4 as the basic optimization algorithms for all the experiments. Thepolicy network and value network used in the algorithms all use a three-layer relu network withhidden size 256. We choose σ = 0.1 in the policy prior for all the environments.
