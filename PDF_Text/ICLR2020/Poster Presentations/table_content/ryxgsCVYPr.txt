Table 1: Results reported on SQuAD 2.0. All the results are from their own publications, except forthose with a dagger。)，which are reproduced. The symbols on the left indicate the correspondingcomparison group which are explained in Section 4.
Table 2: Results of applying NeurQuRI to BERT reader on the NewsQA test set & MS MARCO devset. The scores with a dagger。)are reproduced. ACC indicates answerability classification accuracy.
Table 3: Ablation studies on NeurQuRI. The results are obtained from the development set inSQuAD 2.0 using BERT (Large), DocQA (ELMo), and QANet as readers.
Table 4: Comparison on the EM, F1 scores of models between training with the ground-truth ofanswerability label (φ) and training with the modified answerability label (φd) which is explained inSection 2.2. The results are obtained from the development set in SQuAD 2.0.
Table 5: Comparison on the EM, F1 and the answerable classification accuracy between bi-LSTMand NeurQuRI based on the BERT (Large) reader. The results are obtained from the dev set inSQUAD 2.0. The size of parameters are kept same on both answerable classification module (33M).
