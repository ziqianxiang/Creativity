Table 1: Kernel Inception Distance×100±std.×100 for ablation our model. Lower is better. Thereare some notations; GN: Group Normalization, G-CAM: CAM of generator, D-CAM: CAM ofdiscriminatorModel	selfie2anime	anime2selfieU-GAT-IT	二	11.61 ± 0.57=	11.52 ± 0.57=U-GAT-IT w/ IN	13.64 ± 0.76	13.58 ± 0.8U-GAT-IT w/ LN	12.39 ± 0.61	13.17 ± 0.8U-GAT-IT w/ AdaIN-	12.29 ± 0.78	11.81 ± 0.77U-GAT-IT w/ GN	12.76 ± 0.64	12.30 ± 0.77U-GAT-IT w/o CAM"	12.85 ± 0.82	14.06 ± 0.75U-GAT-ITW/o G_CAM	12.33 ± 0.68	13.86 ± 0.75U-GAT-IT w/o D-CAM	12.49 ± 0.74"	13.33 ± 0.89一Also, as shown in Table 1, we demonstrate the performance of the attention module and AdaLINin the selfie2anime dataset through an ablation study using Kernel Inception Distance (KID)(Binkowski et al. (2018)). OUr model achieves the lowest KID values. Even if the attention mod-ule and AdaLIN are used separately, we can see that our models perform better than the others.
Table 2: Preference score on translated images by user study.
Table 3: Kernel Inception Distance×100±std.×100 for difference image translation mode. Loweris better.
Table 4: The detail of generator architecture.
Table 5: The detail of local discriminator.
Table 6: The detail of global discriminator.
