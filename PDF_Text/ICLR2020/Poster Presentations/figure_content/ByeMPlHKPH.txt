Figure 1: Left: the size of recent NLP models grows rapidly and exceeds the mobile constraints to alarge extent. Right: the search cost of AutoML-based NLP model is prohibitive, which emits carbondioxide nearly 5× the average lifetime emissions of the car.
Figure 2: Flattening the bottleneck of transformer blocks increases the proportion of the attentionversus the FFN, which is good for further optimization for attention in our LSRA.
Figure 3: Lite Transformer architecture (a) and the visualization of attention weights. Conventionalattention (b) puts too much emphasis on local relationship modeling (see the diagonal structure). Wespecialize the local feature extraction by a convolutional branch which efficiently models the localityso that the attention branch can specialize in global feature extraction (c). More visualizations areavailable in Figure A1.
Figure 4: Trade-off curve for machine learning on WMT En-Fr and language modeling onWIKITEXT-103 dataset. Both curves illustrate that our Lite Transformer outperform the basictransformer under the mobile settings (blue region).
Figure 5: The model size and BLEU score on WMT En-Fr dataset with model compression. Our LiteTransformer can be combined with general compression techniques and achieves 18.2× model sizecompression. * ‘Quant, indicates ‘Quantization’.
Figure A1: Conventional attention puts too much emphasis on local relationship modeling (seethe diagonal structure). We specialize the local feature extraction by a convolutional branch whichefficiently models locality so that the attention branch can specialize in global feature extraction (c).
