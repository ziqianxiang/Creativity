Figure 1: Illustration of a Multi-Head Self-Attention layer applied to a tensor image X. Each head hattends pixel values around shift ∆(h) and learn a filter matrix Wv(ahl ) . We show attention mapscomputed for a query pixel at position q .
Figure 2: Test accuracy on CIFAR-10.
Figure 3: Centers of attention of each attention head (different colors) at layer 4 during the trainingwith quadratic relative positional encoding. The central black square is the query pixel, whereassolid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.
Figure 4: Centers of attention of each attention head (different colors) for the 6 self-attention layersusing quadratic positional encoding. The central black square is the query pixel, whereas solid anddotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.
Figure 5: Attention probabilities of each head (column) at each layer (row) using learned relativepositional encoding without content-based attention. The central black square is the query pixel. Wereordered the heads for visualization and zoomed on the 7x7 pixels around the query pixel.
Figure 6: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) usinglearned relative positional encoding and content-content based attention. Attention maps are aver-aged over 100 test images to display head behavior and remove the dependence on the input content.
Figure 7: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) usinglearned relative positional encoding and content-content attention. We present the average of 100test images. The black square is the query pixel.
Figure 8: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) usinglearned relative positional encoding and content-content based attention. The query pixel (blacksquare) is on the frog head.
Figure 9: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) usinglearned relative positional encoding and content-content based attention. The query pixel (blacksquare) is on the horse head.
Figure 10: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) usinglearned relative positional encoding and content-content based attention. The query pixel (blacksquare) is on the building in the background.
Figure 11: Factorization of the vectorized weight matrices Vqconv and VqSA used to compute theoutput at position q for an input image of dimension H × W . On the left: a convolution of kernel2 × 2, on the right: a self-attention with Nh = 5 heads. Din = 2, Dout = 3 in both cases.
Figure 13: Evolution of test accuracy on CIFAR-10. Pruned model (yellow) is continued trainingof the non-isotropic model (orange).
