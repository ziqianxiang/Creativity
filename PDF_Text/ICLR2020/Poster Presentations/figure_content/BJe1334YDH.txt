Figure 1: An illustration of CVRP. Here we provide a prob-lem instance. The red one is a sample route, and three routesconsist of a solution for this problem instance. After apply-ing an operator, current solution changes to a new solutionwith dashed lines replaced by blue lines.
Figure 2: Our hierarchy framework. Given a problem in-stance, our algorithm first generates a feasible solution.
Figure 3: Average traveling cost of different policies with different rollout stepsIn Figure 3(a), (b) and (c), we plot the average traveling cost over 2000 problem instances for CVRPwith N = 20, 50, 100, respectively. It is worthwhile to point out that the same hyper-parameters areused for different N values. The top blue line is for a random policy, the bottom red line is for ourensemble method, and lines in between are for Policy 1, 2, . . . , 6. The plots show that, regardlessof the number of rollout steps, our trained RL policies consistently outperform the random policy.
Figure 4: Pattern of operator usages as training epoch grows10000-Unoɔ əwejəw(b) Policy 20 .
Figure 5: Impact of perturbation magnitude8Published as a conference paper at ICLR 20204	ConclusionIn this paper we propose “Learn to Improve” for solving VRP, which starts with an initial solutionand iteratively updates the solution with an improvement operator selected by an RL-based controlleror with a perturbation operator chosen by a rule-based controller. We also propose an ensemblemethod that trains several RL policies and chooses the best solution produced by the policies. Ourmethod achieved new state-of-the-art results for CVRP instances.
Figure 6: Policy network. The dash-line box is the state embedding part of policy network, whichcontains problem- and solution-specific input features, an attention network, and a sequence ofhistorical actions and effects. The concatenated values are fed into a network of two fully connectedlayers, producing a vector of action probabilities.
Figure 7:	Results for TSP-100given in Figure 8, which shows that our ensemble method scales well as the number of customersincreases. In particular, the running time of our method increases less dramatically than LKH3.
Figure 8: Traveling cost and average computation time as the number of customers increasesF Sensitivity analysisFigure 9: CVRP-100 results under different data distributions by Policy 3(b) Customer positioning6.36.256.2∞0□ WU三①----Using trained CVRP-20 modelUsing trained CVRP-50 modelUsing trained CVRP-100 model6.05550010500Rollout steps15500	20000ISO。0oU 三① >e」lRollout steps(b) Tested on CVRP-50 with different models(a) Tested on CVRP-20 with different models
Figure 9: CVRP-100 results under different data distributions by Policy 3(b) Customer positioning6.36.256.2∞0□ WU三①----Using trained CVRP-20 modelUsing trained CVRP-50 modelUsing trained CVRP-100 model6.05550010500Rollout steps15500	20000ISO。0oU 三① >e」lRollout steps(b) Tested on CVRP-50 with different models(a) Tested on CVRP-20 with different models6Figure 10: Generalization results using Policy 3
Figure 10: Generalization results using Policy 3Following the same protocol of data generation as in Uchoa et al. (2017) (the authors of the paperwere the creators and owners of the CVRPLib website4), we tested our method for four additionalscenarios, namely, central depot positioning, eccentric depot positioning, clustered customer po-sitioning, and random-clustered customer positioning. It is worthwhile to point out that the datadistribution used in our initial paper corresponds to random depot and random customer position-ing. Figure 9 (a) shows the impact of depot positioning (while using random customer positioning),and (b) shows the effect of customer positioning (while using random depot positioning). To makeresults from different data distributions comparable, we normalized the distance by the minimal dis-tance achieved for each data distribution (denoted by “Ratio” as in the Figure), respectively. Theplots show that our method works across different data distributions, and the decreasing trends ofdistance look similar.
