Figure 1: A typical RNN with unbounded Markov order is shown in (a). The factor graphs of ourzero-order and first-order Markov approximations are illustrated in (b) and (c), respectively. Theblue and red factors correspond to likelihood terms and the constraint violations, respectively, fromequations (3), for order k = 0, and (4), for k = 1.
Figure 2: Some test examples from (a) Daily Dialogue and (b) SWAG datasets.
Figure 3: The figure shows the penaliseddecoding objective for zero, first and sec-ond order Markov modelsimations. The results confirm the increase in the BLEU score and decrease in the perplexity, asthe Markov order increases. Furthermore, it shows the trade-off in the solution quality vs. decodingtime for these different approximations. In Table 3, we report results based on two different methodsfor updating the state variables in our method, i.e. gradient-based and forced decoding. As expected,gradient-based updates produced more accurate results compared to forced decoding, but at the costof a longer run-time.
Figure 4: Improvement of the sentence in different iterations7 ConclusionThis work presented a method for improving decoding in discrete autoregressive models usingdynamic programming. The core idea is to introduce auxiliary variables to decouple the non-Markovian aspects of the model, permitting an approximate solution. This solution is used to createthe next model approximation, and the process iterates. Our results show that our decoding frame-work is effective, leading to substantial improvements over greedy and beam search baselines. Ourapproach does have limitations, most notably the computational complexity which is polynomial inthe vocabulary size, thus limiting its application to open text generation problems. Improving thecomplexity of decoding is an important direction for future research, as is applying the method toother autoregressive models, such as the Transformer, which includes self attention, as well as otherstructured prediction problems.
