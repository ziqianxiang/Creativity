Figure 1: GraphSAINT training algorithmFull GCN on GsGraphSAINT follows the design philosophy of directly sampling the training graph G, rather thanthe corresponding GCN. Our goals are to 1. extract appropriately connected subgraphs so that littleinformation is lost when propagating within the subgraphs, and 2. combine information of manysubgraphs together so that the training process overall learns good representation of the full graph.
Figure 2: Convergence curves of 2-layer models on GraphSAINT and baselinesClearly, with appropriate graph samplers, GraphSAINT achieves significantly higher accuracy on alldatasets. For GraphSAINT-Node, we use the same node Probability as FastGCN. Thus, the accuracyimprovement is mainly due to the switching from layer sampling to graph sampling (see “Remark” inSection 3.3). Compared with AS-GCN, GraphSAINT is significantly faster. The sampler of AS-GCNis expensive to execute, making its overall training time even longer than vanilla GCN. We providedetailed computation complexity analysis on the sampler in Appendix D.2. For S-GCN on Reddit, itachieves similar accuracy as GraphSAINT, at the cost of over 9× longer training time. The releasedcode of FastGCN only supports CPU execution, so its convergence curve is dashed.
Figure 3: Sensitivity analysis---GraPhSAINT 2-layer -GraPhSAINT 4-layer——GraPhSAGE 2-layer ——GraPhSAGE 4-layerTraining time (second)Figure 4: GraphSAINT with JK-net and GAT (Reddit)normalization techniques and samPling algorithms to imProve training quality. We have conductedextensive exPeriments to demonstrate the advantage of GraphSAINT in accuracy and training time.
Figure 4: GraphSAINT with JK-net and GAT (Reddit)normalization techniques and samPling algorithms to imProve training quality. We have conductedextensive exPeriments to demonstrate the advantage of GraphSAINT in accuracy and training time.
Figure 5: Degree DistributionC.2 Additional Dataset DetailsHere we Present the detailed Procedures to PrePare the Flickr, YelP and Amazon datasets.
Figure 6: ComParison of training efficiency Figure 7: Fraction of training time on samPlingD Additional ExperimentsD. 1 Training Efficiency on Deep ModelsWe evaluate the training efficiency for deePer GCNs. We only comPare with S-GCN, since imPlemen-tations for other layer samPling based methods have not yet suPPorted arbitrary model dePth. Thebatch size and hidden dimension are the same as Table 2. On the two large graPhs (Reddit and YelP),we increase the number of layers and measure the average time Per minibatch execution. In Figure6, training cost of GraphSAINT is aPProximately linear with GCN dePth. Training cost of S-GCNgrows dramatically when increasing the dePth. This reflects the “neighbor exPlosion” Phenomenon(even though the exPansion factor of S-GCN is just 2). On YelP, S-GCN gives “out-of-memory” errorfor models beyond 5 layers.
