Figure 1: Fitting an over-parameterized Deep Decoder (DD-O) and the deep image prior (DIP) toa (a) noisy image, (b) clean image, and (c) pure noise. Here, MSE denotes Mean Square Error ofthe network output with respect to the clean image in (a) and fitted images in (b) and (c). While thenetwork can fit the noise due to over-parameterization, it fits natural images in significantly feweriterations than noise. Hence, when fitting a noisy image, the image component is fitted faster thanthe noise component which enables denoising via early stopping.
Figure 2:	Denoising with BM3D and various convolutional generators. The relative ranking ofalgorithms in this picture is representative and maintained on a larger set of test images (see Ap-pendix A). DD-U is an under-parameterized deep decoder. DIP and DD-O are over-parameterizedconvolutional generators and with early stopping outperform the BM3D algorithm, the next bestmethod that does not require training data.
Figure 3:	The 1st, 2nt, 6th, and 21th trigonometric basis functions in dimension n = 300.
Figure 5: Gradient descent on the least squares problem of minimizing ky - Jcτ k2 , whereJ ∈ R100×100 has decaying singular values (left panel) and the observation is the sum of a signalcomponent, equal to the leading singular vector wι of J, and a noisy component Z 〜N(0, (1∕n)I),i.e., y = w1 + z. The signal component w1 is fitted significantly faster than the other components(right panel), thus early stopping enables denoising.
Figure 6: Triangular and Gaussian kernels and the weights associated to low-frequency trigono-metric functions they induce, for a generator network of output dimension n = 300. The wider thekernels are, the more the weights are concentrated towards the low-frequency components of thesignal.
Figure 7: The Singular value distribution of the Jacobian of a four-layer deep decoder after t = 50and t = 3500 iterations of gradient descent (panel (a)), along with the corresponding singularvectors/function (b-i). The singular functions corresponding to the large singular vectors are closeto the low-frequency Fourier modes and do not change significantly through training.
Figure 8:	Fitting the phantom MRI and noise with different architectures of depth d = 5, fordifferent number of over-parameterization factors (1,4, and 16). Gradient descent on convolutionalgenerators involving fixed convolutional matrixes fit an image significantly faster than noise.
Figure 9:	The relative distances of the weights in each layer from its random initialization. Theweights need to change significantly more to fit the noise, compared to an image, thus a naturalimage lies closer to a random initialization than noise.
Figure 10:	The loss curves for architecture i), a convolutional generator with linear upsamplingoperations, averaged over 100 3 × 512 × 512 (color) images from the Imagenet dataset. The errorband is one standard deviation. Convolutional generators fit natural images significantly faster thannoise.
Figure 11: Fitting a step function and noise with a two-layer deep decoder: Even for a two-layernetwork, the simple image (step function) is fitted significantly faster than the noise.
Figure 12: The distribution of the '2 -norm of the inner product of the Jacobian of a deep decoder (a)and a deep image prior (b) at a random initialization, with an image y = x* and noise y = z, bothof equal norm. For both deep decoder and deep image prior, this quantity is significantly smaller fornoise than fora natural image. Thus, a natural image is better aligned with the leading singular vec-tors of the Jacobian than noise. This demonstrates that the Jacobian of the networks is approximatelylow-rank, with natural images lying in the space spanned by the leading singularvectors.
