Figure 1: Examples from the TabFact dataset. The top table contains the semi-structured knowl-edge facts with caption ”United...”. The left and right boxes below provide several entailed andrefuted statements. The error parts are highlighted with red font.
Figure 2: Proportion of different higher-order operations from the simple/complex channels.
Figure 3: The program synthesis procedure for the table in Figure 1. We link the entity (e.g. demo-cratic, republican), and then composite functions on the fly to return the values from the table.
Figure 4: The diagram of Table-BERT with horizontal scan, two different linearizations are depicted.
Figure 5: The two uniqueness of Table-based fact verification against standard QA problems.
Figure 6:	The function definition used in TabFact.
Figure 7:	The visualization of different functions.
Figure 8:	The trigger words used to shrink the search space.
Figure 9:	The error case of symbolic reasoning modelBERT In contrast, Table-BERT model seems to have no coverage problem as long as it can feedthe whole table content. However, due to the template linearization, the table is unfolded into a longsequence as depicted in Figure 10. The useful information, ”clay” are separated in a very long spanof unrelated words. How to grasp such a long dependency and memorize the history informationposes a great challenge to the Table-BERT model.
Figure 10: The error case of BERT NLI modelStatistics Here we pick 200 samples from the validation set which only involve single semanticand divide them into different categories. We denote the above-mentioned cases as ”linguistic in-ference”, and the sentences which only describe information from one row as ”Trivial”, the rest arebased on their logic operation like Aggregation, Superlative, Count, etc. We visualize the accuracyof LPA and Table-BERT in Figure 11. From which we can observe that the statements with linguis-tic inference are much better handled with the BERT model, while LPA achieves an accuracy barelyhigher than a random guess. The BERT model can deal with trivial cases well as it uses a horizontalscan order. In contrast, the LPA model outperforms BERT on higher-order logic cases, especiallywhen the statement involves operations like Count and Superlative.
Figure 11: The error analysis of two different modelsD	Reasoning DepthGiven that our LPA has the breadth to cover a large semantic space. Here we also show the reasoningdepth in terms of how many logic inference steps are required to tackle verify the given claims. Wevisualize the histogram in Figure 12 and observe that the reasoning steps are concentrated between4 to 7. Such statistics indicate the difficulty of fact verification in our TabFact dataset.
Figure 12: The histogram of reasoning steps required to verify the claimsE	Whether to keep Wikipedia contextBefore crowd-sourcing the annotation for the tables, we observed that the previous WikiTableQues-tion Pasupat & Liang (2015) provides context (Wikipedia title) during annotation while the Wik-iSQL Zhong et al. (2017) does not. Therefore, we particularly design ablation annotation tasks tocompare the annotation quality between w/ and w/o Wikipedia title as context. We demonstrate atypical example in Figure 13, where a Wiki table10 aims to describe the achievements of a tennisplayer named Dennis, but itself does not provide any explicit hint about “Tennis Player Dennis”.
Figure 13: Comparison of worker annotation w/ and w/o Wikipedia title as contextF Entity LinkingHere we propose to use the longest string match to find all the candidate entities in the table, whenmultiple candidates coexist, we select the one with the minimum edit distances. The visualization isdemonstrated in Figure 14.
Figure 14: Entity Linking System.
Figure 15: We demonstrate the top program candidates and use the discriminator to rank them.
