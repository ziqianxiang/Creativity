Figure 1: Effects of the QCNN on a 28x28 input image. From left to right: original image, imageafter applying a capReLu activation function with a cap C at 2.0, introduction of a strong noiseduring amplitude estimation with = 0.5, quantum sampling with ratio σ = 0.4 that samples thehighest values in priority. The useful information tends to be conserved in this example. The sidegray scale indicates the value of each pixel. Note that during the QCNN layer, a convolution issupposed to happen before the last image but we chose not to perform it for visualisation matter.
Figure 2: Training curves comparison between the classical CNN and the Quantum CNN (QCNN)for = 0.01, C = 10, δ = 0.01 and the sampling ratio σ from 0.1 to 0.5. We can observe a learningphase similar to the classical one, even for a weak sampling of 20% or 30% of each convolutionoutput, which tends to show that the meaningful information is distributed only at certain locationof the images, coherently with the purpose of the convolution layer. Even for a very low samplingratio of 10%, we observe a convergence despite a late start.
Figure 3: RGB decomposition, a colored image is a 3-tensor.
Figure 4: Convolution ofa 3-tensor input (Left) by one 3-tensor kernel (Center). The ouput (Right)is a matrix for Which each entry is a inner product betWeen the kernel and the corresponding over-lapping region of the input.
Figure 5: Convolutions of the 3-tensor input X' (Left) by one 4-tensor kernel K' (Center). Eachchannel of the output X'+1 (Right) corresponds to the output matrix of the convolution with one ofthe 3-tensor kernel.
Figure 6: A convolution product is equivalent to a matrix-matrix multiplication.
Figure 7: Activation functions: ReLu (Left) and capReLu (Right) with a cap C at 5.
Figure 8: A 2×2 tensor pooling. A point in f (X '+1) (left) is given by its position (i'+1 ,j'+1, d'+1).
Figure 9: Numerical simulations of the training of the QCNN. These training curves represent theevolution of the Loss L as we iterate through the MNIST dataset. For each graph, the amplitudeestimation error (0.1, 0.01), the non linearity cap C (2, 10), and the backpropagation error δ(0.1, 0.01) are fixed whereas the quantum sampling ratio σ varies from 0.1 to 0.5. We can compareeach training curve to the classical learning (CNN). Note that these training curves are smoothed,over windows of 12 steps, for readability.
