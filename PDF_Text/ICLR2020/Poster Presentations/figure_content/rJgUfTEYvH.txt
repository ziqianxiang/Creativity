Figure 2: We condition the VideoFlow model withthe frame at t = 1 and display generated trajectoriesat t = 2 and t = 3 for three different shapes.
Figure 3: We measure realism using a 2AFC testand diversity using mean pairwise cosine distancebetween generated samples in VGG perceptualspace.
Figure 4: For a given set of conditioning frames on the BAIR action-free we sample 100 videos from each ofthe stochastic video generation models. We choose the video closest to the ground-truth on the basis of PSNR,SSIM and VGG perceptual metrics and report the best possible value for each of these metrics. All the modelswere trained using ten target frames but are tested to generate 27 frames. For all the reported metrics, higher isbetter.
Figure 5: We display three different futures for two sets of conditioning frames (left and right) at T = 0.6showcasing diversity in outcomesDiversity and quality in generated samples: For each set of conditioning frames in the test set, wegenerate 10 videos and compute the mean distance in VGG perceptual space across these 45 differentpairs. We average this across the test-set for T = 1.0 and T = 0.6 and report these numbers in Figure3. We also assess the quality of the generated videos at T = 1.0 and T = 0.6, using a real vs fakeAmazon Mechanical Turk test and report fooling rates. We observe that VideoFlow outperformsdiversity values reported in prior work (Lee et al., 2018) while being competitive in the realism axis.
Figure 6: Left: We display interpolations between a) a small blue rectangle and a large yellow rectangle b) asmall blue circle and a large yellow circle. Right: We display interpolations between the first input frame andthe last target frame of two test videos in the BAIR robot pushing dataset.
Figure 7: Left: We generate 100 frames into the future with a temperature of 0.5. The top and bottom rowcorrespond to generated videos in the absence and presence of occlusions respectively. Right: We use VideoFlowto detect the plausibility of a temporally inconsistent frame to occur in the immediate future.
Figure 8: We display ten frame rollouts conditioned on a single frame on the Moving MNIST dataset.
Figure 10: Left: We predict a gaussian distribution over z(tl) via a 3-D Residual network conditioned on z(<l)tand z(t>l). Right: Our 3-D residual network architecture is augmented with dilations and gated activation unitsimproving performance.
Figure 11: B: baseline, A: Temporal Skip Connection, C: Dilated Convolutions + GATU, D: DilationConvolutions + Temporal Skip Connection, E: Dilation Convolutions + Temporal Skip Connection +GATU. We plot the holdout bits-per-pixel on the BAIR action-free dataset for different ablations of ourVideoFlow model.
Figure 12: We repeat our evaluations described on the SV2P and SAVP-VAE model in Figure 4 using tempera-tures from 0.0 to 1.0 while sampling from the latent gaussian prior.
Figure 13: We provide a comparison between training progression (measured in the mean bits-per-pixel objectiveon the test-set) and the quality of generated videos.
Figure 14: We compare P(X4 = Xt|X<4) and VGG cosine similarity between X4 and Xt for t = 4 . . . 13K VideoFlow: Low parameter regimeWe repeated our evaluations described in Figure 4, with a smaller version of our VideoFlow modelwith 4x parameter reduction. Our model remains competetive with SVG-LP on the VGG perceptualmetrics.
Figure 15: We repeat our evaluations described in Figure 4 with a smaller version of our VideoFlow model.
