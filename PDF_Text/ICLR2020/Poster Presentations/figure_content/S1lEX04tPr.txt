Figure 1: In Stage 1, Q1 and π1 learn to achieve multiple goals in a single-agent environment.
Figure 4: Agent sedans must perform double lane merge toreach goal lanes. SUMO controls yellow sedans and trucks.
Figure 3: Cooperative navigationaugmented from a copy of Q1 , such that when Q1 inputs are (senv, sn , am , gn), the new module’sinputs are (sm, s-n).5 We train the policy using (5), train the credit function with loss (4), and trainthe global Q-function with the joint-action analogue of (4).
Figure 5: a-e: Comparison against baselines in cooperative navigation (a-c), SUMO (d), Checkers (e).
Figure 6: Stage 1 reward curves for CM3 in cooperative navigation, SUMO and Checkers.
