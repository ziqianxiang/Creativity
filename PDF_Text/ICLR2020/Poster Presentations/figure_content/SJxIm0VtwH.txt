Figure 1: OAdagrad, OSG and Alternating Adam for WGAN-GP on CIFAR10 dataWGAN-GP on CIFAR10 With Alternating Adam"O 0.5	1	1.5	2Number of Iterations	×ιo5x106 WGAN-GP (CIFAR10)一uθ-pe∙l°pθs-nujujno0	0.5	1	1.5	2Number of Iterations	×ιo5x106	wgan (BeDroom)x106 WGAN-GP (CIFAR10)(sθu) -u-pe∙l°pθs-nujujnox106 WGAN-GP (CIFAR10)⑨əu) -u-pe∙l°pθs-nujujno"0	0.5	1	1.5	2Number of Iterations ×ιo5x106	wgan (BeDroom)Number of Iterations ×ιo5"0	0.5	1	1.5	2Number of Iterations ×ιo5- x106	wgan (BeDroom)
Figure 2: Cumulative Stochastic Gradient as a function of number of iterations, where netD andnetG stand for the discriminator and generator respectively. The blue curve and red curve stand forthe growth rate of the cummulative stochastic gradient for OAdagrad and its corresponding tightestpolynomial growth upper bound, respectively.
Figure 3: Self-Attention GAN on ImageNet, with evaluation using Official TensorFlow InceptionScore and Official TensorFlow FID. We see that OAdagard indeed outperforms Simultaneous Adamin terms of the (TensorFloW) Inception score (higher is better), and in terms of (TensorFloW) FrechetInception Distance (lower is better). We don’t report here Alternating Adam since in our run it hascollapsed.
Figure 4:	WGAN-GP: Generated CIFAR10 images using different optimization methods at iteration8000.
Figure 5:	Self-Attention GAN (SA-GAN): Generated ImageNet images using different optimizationmethods at iteration 135000. OAdagrad produces better quality images than simultaneous Adam.
Figure 6:	Self-Attention GAN on ImageNet, with evaluation using Unoffical PyTorch InceptionScore and Unoffical Pytorch FID. We see that OAdagard indeed outperforms Simultaneous Adam interms of the (PyTorch) Inception score (higher is better), and in terms of (PyTorch) Frechet InceptionDistance (lower is better). We don’t report here Alternating Adam since in our run it has collapsed.
Figure 7: OAdagrad, OSG and Alternating Adam for WGAN-GP on CIFAR10 data with differentbatch sizes22Published as a conference paper at ICLR 2020F The equivalence between OSG in unconstrained case and thealgorithm in Daskalakis et al. (2017)Define gk = m^ PmI T(Zk; ξk), then the update rule of Algorithm 1 becomesZk = Xk-I - ngk-1	(33)andXk = Xk-1 - η^k.	(34)These two equalities together imply thatZk+ι = Xk - η^k = χk-ι - 2ηgk = Zk + ηgk-ι - 2ηgk,	(35)where the first equality comes from (33) by replacing k to k + 1, the second equality holds by (34),and the third equality holds by using (33) again. (35) is the algorithm in (Daskalakis et al. 2017).
