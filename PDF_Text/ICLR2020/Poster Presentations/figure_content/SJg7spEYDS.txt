Figure 1: Training results with projected dimension fixed to 2.
Figure 2: Training after 2,000 epochs by varying noise dimension h and the hidden layer size ofcritic model. For each model, each row is a different layer size in [20, 100, 200] and each column is adifferent h in [2, 4, 8, 16]. Half of the GAN training diverges while all GRAM training converges.
Figure 3: Computation graphs of GAN, MMD-net, MMD-GAN and GRAM-net. K is the kernelGram matrix. Solid arrows represents the flow of the computation and dashed lines representsmin-max relationship between the losses, i.e. saddle-point optimization in which minimizing one lossmaximizes the other. Therefore in the zero-sum game case (GAN, MMD-GAN) the two objectives(LY and LÎ¸) cannot be optimized simultaneously (Mescheder et al., 2017).
Figure 4: Nearest training images to samples from a GRAM-net trained on Cifar10. In each column,the top image is a sample from the generator, and the images below it are the nearest neighbors.
Figure 5: Hyper-parameter sensitivity of MMD-GAN, GAN and GRAM-net on Cifar10 dataset.
Figure 6: Training results of GRAM-nets with projected dimension fixed to 2 on the 3D ring dataset.
Figure 7: Corresponding plots to Figure 2 for MMD-nets and MMD-GANs.
Figure 2a: generated samples tend to be too concentrated around the mode of the individual clusters.
Figure 8: Training results of GRAM-nets on the MNIST dataset.
Figure 9: Training of MMD-GAN with projected dimension fixed to 2 before diverging. Data andsamples in the original (top) and projected space (bottom) during training; four plots are at iteration100, 500 and 1,000 respectively. Notice how the projected space separates P and q.
Figure 10: Random Samples from a randomly selected epoch (>100).
