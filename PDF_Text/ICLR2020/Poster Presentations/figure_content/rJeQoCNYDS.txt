Figure 1: πr learns to generate an optimal dataset forthe VAE, whose performance is the reward for πr . En-coding Z by the VAE is given to control policy ∏.
Figure 2: (a-e): Comparison against baselines. (a-b): Number of steps to solve 2D navigation andAcrobot in a single test episode; failure to solve is assigned a count of 50 in 2D nav and 200 inAcrobot. (c-e): Cumulative reward versus test episode step. BNN requires long computation time andshowed low rewards on HIV, hence we report 3 seeds in Figure 4b. (f-j): Ablation results. DynaSEPTis out of range in (g), see Figure 4a. Error bars show standard error of mean over all test instancesover 20 training runs per method.
Figure 3: Cumulative reward on test episode for different Tp (a-c) and different dim(z) (d-f). 8Mindependent test instances for each hyperparameter setting.
Figure 4:	(a) Ablations on ACrobot, inCluding DynaSEPT with 3 seeds; (b) BNN and MAML attainedorders of magnitude lower rewards than other baselines (3 seeds); (C) DynaSEPT performs well onnonstationary dynamiCs in 2D navigation.
Figure 5:	Percent of solved test instances as a function of time steps during the test episode. Percentageis computed among 200 test instances for (a) 2D navigation and (b) 100 test instances for Acrobot.
Figure 6:	Average episodic return over training episodes. Only SEPT and Oracle converged in 2Dnavigation. All methods converged in Acrobot. All methods except MAML converged in HIV. BNNis not shown as the implementation (Killian et al., 2017) does not record training progress.
Figure 7: Two-dimensional encodings generated for four instances of Acrobot (represented by fourground-truth colors), for different values of β . We chose β = 1 for Acrobot.
Figure 8: Probe policy reward curve in one training run in 2D navigationE Experimental detailsFor 2D navigation, Acrobot, and HIV, total number of training episodes allowed for all methods are10k, 4k, and 2.5k, respectively. We switch instances once every 10, 8 and 5 episodes, respectively.
