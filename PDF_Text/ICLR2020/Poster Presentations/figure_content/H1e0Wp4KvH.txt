Figure 1: Training schematic (see also appendix B.2). The setter and judge only receive condi-tioning observations of the environment (blue) when the environment varies across episodes. Thedesirability loss (red) can be used when the distribution of desired tasks is known a priori.
Figure 2: Our environments. For the 2D grid-world task, the solver (white square) can pick-upobjects (bi-colored squares). The object it is currently carrying is displayed in the upper left.
Figure 3: All three of our setter losses (V: validity, F: feasibility, C: coverage) are necessary (pink)in complex environments, but various subsets (green) suffice in simpler environments. (Curvesplotted are rewards on random evaluation tasks averaged across three runs per condition ± standarddeviation across runs; performance is averaged over the last 1000 trials for (a-b), and 5000 for (c).
Figure 4: In varying environments, setters that condition on environment observations outperformunconditioned setters, but unconditioned setters are still much better than random goals. (Curvesplotted are evaluation rewards averaged across three runs per condition ± standard deviation acrossruns; performance is averaged over the last 15000 trials for (a), and 1000 for (b).)4.1	Complex environments require all three lossesFirst, we demonstrate that it is necessary to consider all of goal validity, feasibility, and coveragein complex environments (fig. 3). In the alchemy environment the validity and coverage lossesare necessary, while the feasibility loss is not necessary, but does improve consistency (fig. 3a).
Figure 5: Targeting a desired goal distribution, when one is known. (Averages across three runs percondition ± std. dev. across runs. Performance is averaged over the last 5000 and 1000 trials for (b)and (c), respectively.)model still outperform those trained with randomly generated goals. However, the version of ourmodel which conditions on an observation results in better solver performance. To the best ofour knowledge, these are the first results demonstrating the success of any automated curriculumapproach for goal-conditioned RL in a varying environment.
Figure 6: Comparison to the Goal GAN (Florensa et al., 2017), the closest approach to our own.
Figure 7: Supplemental plots for figure 5b. (a) Random evaluation is not impaired by the desirabil-ity loss, suggesting that in this setting it is actually helping the setter to expand goals in a usefuldirection. (b) Partial continuation of figures 5b, showing the advantages persist. However, some ofthe experiment versions were killed by a server issue around 2 billion steps, which is why the figurein the main text terminates there. This panel plots runs with slightly worse hyperparameter settings(weight βdes. of 1 instead of 5), but that ran for longer.
Figure 8: Comparing scores agents obtain on the training levels proposed by our method and GoalGAN. In the location task (a) both methods generate levels of intermediate feasibility/difficulty.
Figure 9: Visualizing the generated curriculum for the location-finding task which we used in section4.4. We chose to visualize the curriculum for this task because itis in a lower-dimensional and easier-to-understand task space than the main tasks we considered. In this plot we show a heat map for thegoal locations selected by the setter over the course of learning. The agent is placed in an L-shapedroom. Goals are chosen by specifying two coordinates where the agent needs to go. We assume wehave a rough idea of a bounding box for the coordinates of reachable locations in the room, and usethose bounds to limit the goals sampled by the setter. The L-shaped room is actually placed in thebottom left part of this bounding box, and the setter needs to discover this. The first picture aboveshows how at random initialisation, the setter chooses locations which are not inside the room. Ourvalidity loss will then help move these locations inside the room, as shown in the second picture.
Figure 10: Learning a good latent representation from a conditioning observation can be difficult.
Figure 11: Examining the indivudal losses. All curves are for the Color pair finding task. Note howcurves (a) to (c) are only up to 100 million steps. This is still early in training, but the losses arevery stable after that point, as the setter and judge continually adapt to the solver’s improvements.
Figure 12: The setter is learning to appropriately condition on the feasibility input to produce tasksof varying difficulty. This plot shows the average score (where the maximum score is 1) of the solveron 1-color finding tasks generated by a setter for feasibility inputs of 0.25, 0.5 and 0.75. Each graphis averaged over 3 setter/solver pairs. We only show the behaviour early on in training, as it is stablethroughout. The setter is producing tasks close to the desired input feasibility, so that uniformlysampling feasibility causes it to produce tasks of varying difficulty.
