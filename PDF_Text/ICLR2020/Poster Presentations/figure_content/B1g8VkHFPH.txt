Figure 1: (a) Searching for the optimal momentum on Birds dataset with fixed learning rate 0.01 and differentweight decays. Detailed learning curves and results of other hyperparameters can be found in Appendix A. (b)Comparison of momentum 0.9 and 0.0 with different learning rates on the Birds dataset, λ is fixed at 0.0001.
Figure 2: An illustration of the effect of momentum on different fine-tuning scenarios from the loss-landscapeperspeCtive. The red point is the pre-trained model and the blue point is the fine-tuned solution. The dashedlines are loss Contours. Assuming the step size is fixed, large momentum aCCelerates the ConvergenCe whenthe initialization is far from the minimum ((a) and (b)). On the Contrary, large momentum may impede theConvergenCe as shown in (C) and (d) when the initialization is Close to the minimum.
Figure 3: The effect of momentumw/ and w/o fixing ELR η0 . When η0is the same, momentum 0 and 0.9 arealmost equivalent. If η0 is allowed tochange, there is almost no differencebetween optimal performance obtainedby different m.
Figure 4:	The best validation errors obtained by different ELRs for different source-target domains. Note thatthe optimal ELR for each target dataset falls in the interior of search space. Each point in (a-e) is the lowestvalidation error obtained with different weight decay values while ELR is fixed. The first row suggests that theconnection between optimal ELR and domain similarity is architecture agnostic. The second row verifies thatoptimal ELR depends on the similarity between source domain and target domain.
Figure 5:	The relationship between optimal effective weight decay and source datasets. The optimal effectiveweight deCay is larger when the sourCe domain is similar with the target domain.
Figure 6: The normalized L2 norm and L2-SP norm during training. The y-axis is the relative change ofthe regularization term in comparison to the initial value, i.e., ∣∣θt ||2/|仇||2 for L2 norm and (λι∣∣θ0 — θo ∣∣2 +λ2∣∣θ00k2)∕(λ2kθ00k2) for L2-SP norm. Optimal hyperparameters are also given in the legend. Note thatexperiment uses batch size 64 instead of 256, which results in smaller optimal learning rate comparing toprevious result.
Figure 7: SearChing for the optimal momentum on Birds dataset with fixed learning rate and weight deCays.
Figure 8: The effect of momentum when learning rate is allowed to change. The learning rate for thebest performance increases 10x after changing m from 0.9 to 0.0, which is coherent with the rule ofeffective learning rate. Note that weight decay λ is fixed at 0.0001.
Figure 9: The effect of momentum when learning rate is allowed to change (Figure 8 continued).
Figure 10: Performance of different BN momentum for each dataset with existing optimal hyperparameters.
Figure 11: Fine-tuning with different data augmentation methods and hyperparameters. Dashed curvesare the validation errors. Strong data augmentation is harder to train as it converge slowly and needsmore number of epochs to observe the advanced performance on datasets such as Aircrafts. Simpledata augmentation (red curves) converges much faster in training error. Strong data augmentation(blue curves) overfits the Dogs dataset with default hyperparameter but performs well with m = 0.
Figure 12: Comparison of data augmentation methods with different momentum values (Figure 11continued). The other hyperparameters are: n = 256, η = 0.01 and λ = 0.0001.
Figure 13: Training from scratch with various learning rate and weight decay. The batch size is 256and the momentum is 0.9. The solid curves are training error and the dashed lines are valdiation error.
