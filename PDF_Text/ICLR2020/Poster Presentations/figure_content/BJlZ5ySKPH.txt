Figure 1: The model architecture of U-GAT-IT. The detailed notations are described in SectionModelacquire the desired results for both image translation preserving the shape (e.g., horse2zebra) andimage translation changing the shape (e.g., cat2dog) with the fixed network architecture and hyper-parameters. The network structure or hyper-parameter setting needs to be adjusted for the specificdataset.
Figure 2: Visualization of the attention maps and their effects shown in the ablation experiments:(a) Source images, (b) Attention map of the generator, (c-d) Local and global attention maps of thediscriminator, respectively. (e) Our results with CAM, (f) Results without CAM.
Figure 3: Comparison of the results using each normalization function: (a) Source images, (b) Ourresults, (c) Results only using IN in decoder with CAM, (d) Results only using LN in decoder withCAM, (e) Results only using AdaIN in decoder with CAM, (f) Results only using GN in decoderwith CAM.
Figure 4: Visual comparisons on the five datasets. From top to bottom: selfie2anime, horse2zebra,cat2dog, photo2portrait, and photo2vangogh. (a)Source images, (b)U-GAT-IT, (c)CycleGAN,(d)UNIT, (e)MUNIT, (f)DRIT, (g)AGGANTable 1: Kernel Inception Distance×100±std.×100 for ablation our model. Lower is better. Thereare some notations; GN: Group Normalization, G-CAM: CAM of generator, D-CAM: CAM ofdiscriminatorModel	selfie2anime	anime2selfieU-GAT-IT	二	11.61 ± 0.57=	11.52 ± 0.57=U-GAT-IT w/ IN	13.64 ± 0.76	13.58 ± 0.8U-GAT-IT w/ LN	12.39 ± 0.61	13.17 ± 0.8U-GAT-IT w/ AdaIN-	12.29 ± 0.78	11.81 ± 0.77U-GAT-IT w/ GN	12.76 ± 0.64	12.30 ± 0.77U-GAT-IT w/o CAM"	12.85 ± 0.82	14.06 ± 0.75U-GAT-ITW/o G_CAM	12.33 ± 0.68	13.86 ± 0.75U-GAT-IT w/o D-CAM	12.49 ± 0.74"	13.33 ± 0.89一Also, as shown in Table 1, we demonstrate the performance of the attention module and AdaLINin the selfie2anime dataset through an ablation study using Kernel Inception Distance (KID)(Binkowski et al. (2018)). OUr model achieves the lowest KID values. Even if the attention mod-ule and AdaLIN are used separately, we can see that our models perform better than the others.
Figure 5: Visual comparisons of the selfie2anime with attention features maps. (a) Source images,(b) Attention map of the generator, (c-d) Local and global attention maps of the discriminators, (e)Our results, (f) CycleGAN (Zhu et al. (2017)), (g) UNIT (Liu et al. (2017)), (h) MUNIT (Huanget al. (2018)), (i) DRIT (Lee et al. (2018)), (j) AGGAN (Mejjati et al. (2018)), (k) CartoonGAN(Chen et al. (2018)).
Figure 6: Visual comparisons of the anime2selfie with attention features maps. (a) Source images,(b) Attention map of the generator, (c-d) Local and global attention maps of the discriminators, (e)Our results, (f) CycleGAN (Zhu et al. (2017)), (g) UNIT (Liu et al. (2017)), (h) MUNIT (Huanget al. (2018)), (i) DRIT (Lee et al. (2018)), (j) AGGAN (Mejjati et al. (2018)).
Figure 7: Visual comparisons of the horse2zebra with attention features maps. (a) Source images,(b) Attention map of the generator, (c-d) Local and global attention maps of the discriminators, (e)Our results, (f) CycleGAN (Zhu et al. (2017)), (g) UNIT (Liu et al. (2017)), (h) MUNIT (Huanget al. (2018)), (i) DRIT (Lee et al. (2018)), (j) AGGAN (Mejjati et al. (2018)).
Figure 8: Visual comparisons of the zebra2horse with attention features maps. (a) Source images,(b) Attention map of the generator, (c-d) Local and global attention maps of the discriminators, (e)Our results, (f) CycleGAN (Zhu et al. (2017)), (g) UNIT (Liu et al. (2017)), (h) MUNIT (Huanget al. (2018)), (i) DRIT (Lee et al. (2018)), (j) AGGAN (Mejjati et al. (2018)).
Figure 9: Visual comparisons of the cat2dog with attention features maps. (a) Source images, (b)Attention map of the generation, (c-d) Local and global attention maps of the discriminators, (e) Ourresults, (f) CycleGAN (Zhu et al. (2017)), (g) UNIT (Liu et al. (2017)), (h) MUNIT (Huang et al.
Figure 10: Visual comparisons of the dog2cat with attention features maps. (a) Source images, (b)Attention map of the generation, (c-d) Local and global attention maps of the discriminators, (e) Ourresults, (f) CycleGAN (Zhu et al. (2017)), (g) UNIT (Liu et al. (2017)), (h) MUNIT (Huang et al.
Figure 11: Visual comparisons of the photo2vangogh with attention features maps. (a) Sourceimages, (b) Attention map of the generation, (c-d) Local and global attention maps of the discrimi-nators, respectively, (e) Our results, (f) CycleGAN (Zhu et al. (2017)), (g) UNIT (Liu et al. (2017)),(h) MUNIT (Huang et al. (2018)), (i) DRIT (Lee et al. (2018)), (j) AGGAN (Mejjati et al. (2018)).
Figure 12: Visual comparisons of the photo2portrait with attention features maps. (a) Source images,(b) Attention map of the generator, (c-d) Local and global attention maps of the discriminators,respectively, (e) Our results,(f) CycleGAN (Zhu et al. (2017)), (g) UNIT (Liu et al. (2017)), (h)MUNIT (Huang et al. (2018)), (i) DRIT (Lee et al. (2018)), (j) AGGAN (Mejjati et al. (2018)).
