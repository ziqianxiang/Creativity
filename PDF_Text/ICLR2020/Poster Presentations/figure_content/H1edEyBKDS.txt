Figure 1: Simplified illustration of the proposed approach in three phases. In Step 1, a forward passis performed through the language model to compute the likelihood of a desired attribute using anattribute model that predicts p(a|x). In Step 2, a backward pass updates the internal latent represen-tations of the LM, using gradients from the attribute model, to increase the likelihood of the passagehaving the desired attribute. In Step 3, a new distribution over the vocabulary (pet+1) is generatedfrom the updated latents (Ht) and the current token xt . The next token is then sampled from theupdated distribution. This process of updating the latents is repeated at each time-step, leading toa gradual transition towards the desired attribute. For computational efficiency, one may choose tomodify only the latents within some window of the recent past, depicted as the dotted-red region.
Figure 2: An oversimplified view into why stepsthat maximize both log p(a|x) and log p(x) areneeded. The sentence under consideration isshown as a black dot, which is first pushed in thedirection of maximizing log p(a|x) and then in thedirection of maximizing log p(x). In practice weuse a single step and simply add the log proba-bilities; we take steps in continuous space of hid-den representations H rather than in the discrete x(byte pair) space, and rather than resampling theentire sentence each step, we take one step in Hspace per byte-pair sample.
Figure S3:	Topic relevance by human evaluation. We can see that taking a PPLM gradient step(B→BC) makes a big difference. Reranking is mostly helpful (B→BR; BC→BCR). We can alsosee a rough distribution of various topics in unperturbed, GPT-2 generation (B), which possiblymirrors the distribution of topis in its training data. Some topics, like science, naturally appearrather frequently.
Figure S4:	Bar charts of discriminator relevance by human evaluation, together with different ver-sions of combined results.
Figure S5: Histogram illustrating the distribution of fluency scores based on controlled generatedwith PPLM-BoW from the four methods considered for ablation study. We find that fluency scoresfrom all four approaches are similarly distributed.
Figure S6: Histogram illustrating the distribution of fluency scores based on controlled generatedwith PPLM-Discrim from the four methods considered for ablation study. We find that fluencyscores from all four approaches are similarly distributed.
