Figure 1: Schematics of our approach. The message encoder generates an embedding distributionthat is sampled and concatenated with the current local history to serve as an input to the localaction-value function. Local action values are fed into a mixing network to to get an estimation ofthe global action value.
Figure 2:	(a) Task sensor; (b) Performance comparison on sensor; (c) Performance comparisonwhen different percentages of messages are dropped. We measure the drop rate of our method intwo ways: count by the number of messages (NDQ) or count by the number of bits (NDQ (bits)).
Figure 3:	Message distributions learned by our method on sensor under different values of β. (Mes-sages are cut by bit, if μ < 2.0). A mean of 0 means that the corresponding bit is below the cuttingthreshold and is not sent. When β = 10-3, NDQ learns the minimized communication strategy thatis effective.
Figure 4: Results on hallway. (a, b) Task hallway and performance comparison. (c) Similar toFig. 2(c), we show performance comparison when different percentages of messages are dropped.
Figure 5: Message embedding representations learned by our method on hallway. A mean of 0means that the corresponding bit is below the cutting threshold (μ=3) and is not sent.
Figure 6: Snapshots of the StarCraft II sCenarios that we Consider.
Figure 7: Learning curves ofQMIX+TarMAC.
Figure 8: Performance of our method and QMIX+TarMAC when 80% of messages are cut off. Wealso plot the learning curves of QMIX for comparison.
Figure 9: Performance of our method and QMIX+TarMAC when 100% messages are cut off. Wealso plot the learning curves of QMIX for comparison.
Figure 10: Task Independent-search. Two agents are both reward- and transition-independent.
