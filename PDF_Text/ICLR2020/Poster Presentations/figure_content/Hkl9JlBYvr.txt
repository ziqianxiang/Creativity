Figure 1: Illustration of different exploration strategies. (a) Environment: The agent starts at thebottom left and has to navigate to an unknown goal, located in the grey area. (b) A Bayes-optimalexploration strategy that systematically searches possible grid cells to find the goal, shown in solid(past actions) and dashed (future actions) blue lines. A simplified posterior is shown in the back-ground in grey (p = 1/(number of possible goal positions left) of containing the goal) and white(p = 0). (c) Posterior sampling, which repeatedly samples a possible goal position (red squares)from the current posterior, takes the shortest route there, and updates its posterior. (d) Explorationstrategy learned by variBAD. The grey background represents the approximate posterior the agenthas learned. (e) Average return over all possible environments, over six episodes with 15 steps each(after which the agent is reset to the starting position). VariBAD results are averaged across 20random seeds. The performance of any exploration strategy is bounded above by the optimal be-haviour (of a policy with access to the true goal position). The Bayes-optimal agent matches thisbehaviour from the second episode, whereas posterior sampling needs six rollouts. VariBAD closelyapproximates Bayes-optimal behaviour in this environment.
Figure 2: VariBAD architecture: A trajectory of states, actions and rewards is processed online usingan RNN to produce the posterior over task embeddings, qφ (m|T:t). The posterior is trained using adecoder which attempts to predict past and future states and rewards from current states and actions.
Figure 3: Behaviour of variBAD in the gridworld environment. (a) Hand-picked but representativeexample test rollout. The blue background indicates the posterior probability of receiving a rewardat that cell. (b) Probability of receiving a reward for each cell, as predicted by the decoder, over thecourse of interacting with the environment (average in black, goal state in green). (c) Visualisationof the latent space; each line is one latent dimension, the black line is the average.
Figure 4: Average test performance for the first 5 rollouts of MuJoCo environments (using 5 seeds).
Figure 5: Results for the gridworld toy environment. Results are averages over 20 seeds (with 95%confidence intervals for the learning curve).
Figure 6: Learning curves for the MuJoCo results presented in Section 5.2. The top row showsperformance evaluated at the first rollout, and the second row shows the performance at the N-throllout. For variBAD and RL2, N = 2. For ProMP and E-MAML, N = 20. For PEARL, N = 10.
Figure 7: Behaviour at test time for the for the task “walk left” in HalfCheetahDir. The x-axisreflects the position of the agent; the y-axis the steps in the environment (to be read from bottom totop). Rows are separate examples, columns the number of rollouts.
Figure 8: Visualisation of the latent space at meta-test time, for the HalfCheetahDir environmentand the tasks ”go right” (top) and the task ”go left” (bottom). Left: value of the posterior meanduring a single rollout (200 environment steps). The black line is the average value. Middle: valueof the posterior log-variance during a single rollout. Right: Behaviour of the policy during a singlerollout. The x-axis show the position of the Cheetah, and the y-axis the step (should be read frombottom to top).
