Figure 1: Images generated with our approach and a BigGAN model (Brock et al., 2018), showingthat the position of the object can be controlled within the image.
Figure 2: Quantitative results on the ten categories of the ILSVRC dataset used for training (Top) andfor ten other categories used for validation (Bottom) for three geometric transformations: horizontaland vertical translations and scaling. In blue, the distribution of the measured transformationparameter and in red the standard deviation of the distribution with respect to t. Note that forlarge scales the algorithm seems to fail. However, this phenomenon is very likely due to the poorperformance of the saliency model when the object of interest covers almost the entire image (scale≈ 1.0). (best seen with zoom)which are injected at different level of the generator. We wanted to see by which part of the latentcode these directions are encoded. The squared norm of each part of the latent code is reported inFigure 4 for horizontal position, vertical position and scale. This figure shows that the directionscorresponding to spatial factors of variations are mainly encoded in the first part of the latent code.
Figure 3: Qualitative results for some categories of ILSVRC dataset for three geometric transforma-tions: horizontal and vertical translations and scaling.
Figure 4: Squared norm of each part of the latent code for horizontal position, vertical position andscale.
Figure 5: Results of our evaluation procedure with four β-VAE for β = 1, 5, 10, 20. Note the erfshape of the results which indicates that the distribution of the shape positions has been correctlylearned by the VAE. See Figure 2 for additional information on how to read this figure.
Figure 6: Reconstruction results with different σ values. We typically used a standard deviation of 3pixels for the kernel.
Figure 7: Reconstruction results obtained with different reconstruction errors: MSE, DSSIM (ZhouWang et al., 2004) and our loss. With or without the constraint on ||z||. Note the artifacts when usingour loss without constraining z (best seen with zoom).
Figure 8: Two trajectories are shown in the pixel space, between an image and its transformed version,for three types of transformations: translation, scale and orientation. Red: shortest path (interpolation)between the two extremes of the trajectory. Blue: trajectory of the actual transformation. At eachposition along the trajectories, we report the corresponding image (best seen with zoom).
Figure 9: Qualitative results for 10 categories of ILSVRC dataset for three geometric transformations(horizontal and vertical translations and scaling) and for brightness.
Figure 10: Comparison of the speed of convergence on a single example for our method (top) givenby equation 5 and a naive approach (bottom) given by equation 4. The numbers indicate the step ofoptimization. Both experiences have been conducted with Adam optimizer with a learning rate of1e-1.
