Figure 1: A single update by FPP. It randomly samples i, and performs a gradient descent update to si-T , si , Wand β, where the loss on the targets affects si-T , W, β and the loss producing the next state variable si affectssi-T, si, W. The state variables are stored in the buffer, but are explicit variables we learn, just like W and β.
Figure 2: The ratio error of each of the algorithms with respect to the baseline of predicting 0 at every time stepis our measure of performance. For all the values of p, FPP seems to be more robust to T, especially with largerp. The numbers are average over 30 runs with standard error bars.
Figure 3: Average online performance for FPP (red), T-BPTT (orange) and No-Overlap T-BPTT (blue). Acrossall the domains, FPP seems to be more robust to T, and it does much better than T-BPTT especially for small T.
Figure 4: The performanCe for inCrease number of updates (with mini-batCh of B = 1) and inCreasing mini-batChsize (with number of updates M = 1). The numbers are average over 30 runs with 10000 training steps. Thesolid line is FPP and the dashed line is FPP without state updating.
Figure 5: Sensitivity to buffer length and trajectory length in FPP, for buffer sizes 100, 1000 and 10000 andtruncations of 1, 5,10,15 and 50.
Figure 6: Sensitivity of Lambda for various values of T. For small T, higher lambda works better suggesting theimpact of propagation of state values across the buffer.
Figure 7: The performance of FPP and FPP without state updating with T = 1, B = 16 after 50000 trainingsteps, for varying M . This result highlights that FPP can better take advantage of more updates and largermini-batches, with its sound updating strategy on a buffer.
