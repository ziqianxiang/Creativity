Figure 1: Visualization our Multi-View Information Bottleneck model for both multi-view andsingle-view settings, where Iξ(z1 ; z2) refers to the sample-based parametric mutual informationestimation. Whenever p(v1) andp(v2) have the same distribution, the two encoders can share theirparameters.
Figure 2: Information Plane determined byI(x; z) (x-axis) and I(y; z) (y-axis). Different ob-jectives are compared based on their target.
Figure 3: Left: mean average precision (mAP) of the classifier trained on different multi-viewrepresentations for the MIR-Flickr task. Right: comparing the performance for different values ofβ and percentages of given labeled examples (from 1% up to 100%). Each model uses encoders ofcomparable size, producing a 1024 dimensional representation. * results from Wang et al. (2016).
Figure 4: Comparing the representations obtained with different objectives on MNIST dataset. Theempirical estimation of the coordinates on the Information Plane (in nats on the left) is followedby the respective classification accuracy for different number of randomly sampled labels (from 1example per label up to 6000 examples per label). Representations that discard more observationalinformation tend to perform better in scarce label regimes. The measurements used to produce thetwo graphs are reported in Appedix G.4.1.
Figure 5: Visualization of the graphical model G that relates the observations x, label y, functionsused for augmentation t1 , t2 and the representation z1 .
Figure 6: Examples of pictures v1, tags v2 and category labels y for the MIR-Flickr dataset (Srivas-tava & Salakhutdinov, 2014). As visualized is the second row, the tags are not always predictive ofthe label. For this reason, the mutual redundancy assumption holds only approximately.
Figure 7: Linear projection of the embedding obtained by applying the MIB encoder to the MNISTtest set. The 64 dimensional representation is projected onto the two principal components. Differentcolors are used to represent the 10 digit classes.
Figure 8:	Visualization of the coordinates on the Information Plane (plot on the left) and predic-tion accuracy (center and right) for the MV-InfoMax and MIB objectives with different amount oftraining labels and corruption percentage used for data-augmentation.
Figure 9:	Visualization of the coordinates on the Information Plane (plot on the left) and predic-tion accuracy (center and right) for the β-VAE, Multi-View InfoMax and Multi-View InformationBottleneck objectives with different amount of training labels and different values of the respectivehyperparameter β .
