Figure 1: Predictions of three architectures trained on the identity mapping task with 60k MNISTexamples. The red dashed line separates 3 training examples from the test examples.
Figure 2: Visualization of the outputs of fully connected networks trained on a single exam-ple. The first row shows the single training example (7) and a set of evaluation images consistingof a linear combination of two digits, random digits from MNIST test set, random images fromFashion MNIST, and some algorithmically generated image patterns. Each row below indicates anarchitecture and the output from that architecture for a given input.
Figure 3: Visualization of predictions from CNNs trained on a single example. The first rowshows the single training example (7) and a set of test inputs. Each row below shows the output of aCNN whose depth is indicated to the left of the row. The hidden layers of the CNN consist of 5 × 5convolution filters organized as 128 channels.
Figure 4: Predictions of CNNs on test examples at different angles to the training image. Thehorizontal axis shows the train-test correlation, while the vertical axis indicate the number of hiddenlayers for the CNNs being evaluated. The heatmap shows the similarity (measured in correlation)between the model prediction and the reference function (the constant or the identity function).
Figure 5:	Illustration of the collapse of predic-tive power as function of layer’s depth. Errorrate is measured by using the representations com-puted at each layer to a simple averaging basedclassifier on the MNIST test set. The error rate ateach layer is plotted for a number of trained CNNsof different depth. The thick red line shows thecurve ofan untrained 20-layer CNN for reference.
Figure 6:	Visualization of a 5-layer CNN on test images of different sizes. The two subfiguresshow the results on 7 × 7 inputs and 112 × 112 inputs, respectively.
Figure 7:	Visualization of the predictions of a 20-layer CNN on test images of different sizes(indicated by the number on each row). The input patterns are the same as in Figure 6 (constructedin different resolutions), which are not shown for brevity.
Figure 8:	Comparing bias towardsconstant and identity when trainedwith different image sizes. The x-axis is the depth of the CNNs, whilethe y-axis is the mean correlation (av-erage of each row from the heatmapslike in Figure 4). Each curve cor-responds to training with a differentimage size.
Figure 9: Visualizing the predictions from 5-layer CNNs with various filter sizes. The first rowshows the single training example (7) and a set of test inputs. Each row below shows the output ofa CNN whose filter size is indicated by the number on the left. See Appendix I for more results.
Figure 10: Visualization of the predictions from CNNs for various number of hidden channels.
Figure 12: Visualization of predictions from two-layer ReLU networks. The first row shows theinput images for evaluation, including the single training image “7” at the beginning of the row.
Figure 11: Visualization of predictions from trained multi-layer linear networks. The first rowshows the input images for evaluation, including the single training image “7” at the beginning ofthe row. The remaining rows shows the prediction from a trained linear network with 1, 3, and 5hidden layers, respectively.
Figure 13: Visualization of predictions from multi-layer ReLU networks. The first row showsthe input images for evaluation, including the single training image “7” at the beginning of the row.
Figure 14: Quantitative evaluation of the learned model on randomly generated test samplesat various angles (correlation) to the training image. The horizontal axis shows the train-test cor-relation, while the vertical axis indicate the number of hidden layers for the FCNs being evaluated.
Figure 15: Visualization the intermediate layers of CNNs with different number of layers. Thefirst column shows a randomly initialized 20-layer CNN (random shallower CNNs look similar tothe truncation of this). The rest of the columns show the trained CNNs with various number oflayers.
Figure 16:	Visualizing the intermediate layers of a trained 7-layer CNN. The three subfiguresshow for each layer: 1) the top singular vector across the channels; 2) the channel that maximallycorrelate with the input image; 2) a random channel, respectively.
Figure 17:	Visualizing the intermediate layers of a trained 14-layer CNN. The three subfiguresshow for each layer: 1) the top singular vector across the channels; 2) the channel that maximallycorrelate with the input image; 2) a random channel, respectively.
Figure 18:	Visualizing the intermediate layers of a trained 20-layer CNN. The three subfiguresshow for each layer: 1) the top singular vector across the channels; 2) the channel that maximallycorrelate with the input image; 2) a random channel, respectively.
Figure 19:	The relative `2 distance of the weight tensors before and after training at eachlayer. The curves compare models at different depth. Most of the networks have significantly largerdistances on the top-most layer. To see a better resolution at the bottom layers, we cut off the toplayer in the figures by manually restricting the y axis.
Figure 20:	Visualization of a 5-layer CNN on test images of different sizes. Every two rows showthe inputs and model predictions. The numbers on the left indicate the input image size (both widthand height).
Figure 21: Visualization of intermediate representations when testing on images of differentsizes from training images for a 20-layer trained CNN. The CNN is trained on a 28 × 28 imageof the digit “7”.
Figure 22: Visualizing only the final layer in trained networks. The first row are the input images,which are directly fed into the final layer of trained networks (skipping the bottom layers). Theremaining rows shows the predictions from the top layers of CNNs, with the numbers on the leftindicating their (original) depth.
Figure 23:	Visualizing only the top two layers in trained networks. The first row are the inputimages, which are directly fed into the top two layer of trained networks (skipping the bottom layers).
Figure 24:	Visualzing the top 3 layers, 6 layers and 10 layers of a 20-layer CNN. Visualizationsformatted in the same way as Figure 23.
Figure 25:	Inductive bias of a 5-layer CNN with varying convolutional filter size. The heatmapis arranged similarly as Figure 4, except that the rows correspond to CNNs filter sizes.
Figure 26:	Visualizing the predictions from CNNs with various filter sizes. The first row isthe inputs, including the single training image “7”. The remaining rows are predictions, with thenumbers on the left showing the corresponding filter sizes.
Figure 27: Correlation to the constant and the identity function for different convolution channels in a 5-layer CNN.
Figure 28: Visualization predictions from CNNs with 3 convolution channels and with variousnumber of layers (numbers on the left). The first row is the inputs, and the remaining rowsillustrate the network predictions.
Figure 29:	Visualizing the randomly initialized models to compare two 5-layer CNNs with 3convolution channels per layer and 128 convolution channels per layer, respectively. The sub-figures visualize the predictions of intermediate layers of the two network at random initialization.
Figure 30:	Comparing two 5-layer CNNs with 3 convolution channels per layer and 128 convo-lution channels per layer, respectively. Layout is similar to Figure 29.
Figure 31:	Visualization of the predictions from CNNs trained with the D) default, Xn) Xaviernormal, Xu) Xavier uniform, Kn) Kaiming normal, Ku) Kaiming uniform, and Or) orthogonalinitialization schemes. The first row shows the inputs, and the remaining rows shows the predictionsfrom each trained networks.
Figure 32:	Visualization of the predictions from CNNs trained with different optimizers. Thefirst row shows the inputs, and the remaining rows shows the predictions from SGD (lr 0.01), SGD(lr 0.001), SGD (lr 0.0001), Adagrad, RMSprop, Aam, and Adamax respectively.
Figure 33:	Quantitative evaluation of linear FCNs. The same as Figure 14(a), except MSE isplotted here instead of correlation.
Figure 34: Quantitative evaluation of ReLU FCNs. The same as Figure 14(b), except MSE isplotted here instead of correlation.
Figure 35: Quantitative evaluation of CNNs. The same as Figure 4, except MSE is plotted hereinstead of correlation.
Figure 36:	Visualization of predictions from CNNs trained on a single example. The sametraining example is used as in the main text, but two extra runs of training and evaluation are listedto show the robustness of our main observations to the randomness in the experiments.
Figure 37:	Visualization of predictions from CNNs trained on a single example. Two different(randomly chosen) training images (a) digit 3, (b) digit 0, are shown to compare robustness of ourmain observations to different training images.
Figure 38: Visualization of the predictions from networks trained with 2 examples. The first twocolumns are training examples, and the remaining columns are unseen test cases. H0—H9 showsfully connected networks with the corresponding number of hidden layers. C3—C24 shows CNNswith the corresponding number of layers.
Figure 39: Visualization of the predictions from networks trained with 3 examples. The firstthree columns are training examples, and the remaining columns are unseen test cases. H0—H9shows fully connected networks with the corresponding number of hidden layers. C3—C24 showsCNNs with the corresponding number of layers.
Figure 40: Visualization of the predictions from networks trained with 60k examples. Thefirst three columns are (3 out of 60k) training examples, and the remaining columns are unseen testcases. H0—H9 show fully connected networks (ReLU activation) with the corresponding numberof hidden layers. LinH1—LinH9 show linear fully connected networks (without activation) with thecorresponding number of hidden layers. And C3—C24 show CNNs with the corresponding numberof layers.
Figure 41: Visualization of predictions from multi-layer ReLU networks with residual con-nections. The first row shows the input images for evaluation, including the single training image“7” at the beginning of the row. The remaining rows show the predictions from trained multi-layerReLU FCNs with residual connections, with the numbers on the left indicating the number of hiddenlayers.
Figure 42: Visualization of predictions from various networks trained on a single CIFAR-10image (dog). The first row shows the input images, where the first one is the training image and therest are images for evaluation. The evaluation images consist of unseen images from the CIFAR-10test set and artificially generated grayscale patterns. The grayscale patterns are duplicated into threechannels before feeding into the networks. The remaining rows show the predictions from trainedFCNs and CNNs. For FCNs, the numbers indicate the number of hidden layers. For example, H3means a 3-hidden-layer FCN. For CNNs, the numbers indicate the number of (convolutional) layers.
Figure 43: Visualization of predictions from various networks trained on a single CIFAR-10image (air plane). The first row shows the input images, where the first one is the training imageand the rest are images for evaluation. The evaluation images consist of unseen images from theCIFAR-10 test set and artificially generated grayscale patterns. The grayscale patterns are duplicatedinto three channels before feeding into the networks. The remaining rows show the predictions fromtrained FCNs and CNNs. For FCNs, the numbers indicate the number of hidden layers. For example,H3 means a 3-hidden-layer FCN. For CNNs, the numbers indicate the number of (convolutional)layers. For example, C16 means a 16-layer CNN.
