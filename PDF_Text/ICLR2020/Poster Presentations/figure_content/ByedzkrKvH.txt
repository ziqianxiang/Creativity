Figure 1: Extensive-Form IIG and Information Setof any other sequences. A(h) ={a: ha∈H} is theset of available actions after non-terminal history h∈ H\Z. A player function P assigns a member ofN ∪{c} to each non-terminal history, where c is the chance ( we set c=-1). P(h) is the player who takesan action after history h. For each player i, imperfect information is denoted by information set (infoset)Ii. All states h∈Ii are indistinguishable to i. Ii refers to the set of infosets ofi. The utility function ui(z)defines the payoff of i at state z. See appendix B.1 for more details.
Figure 2: (a) tabular CFR and (b) our double neural CFR framework. r∙σt ((a∖Ii)∖Qj) is the estimated regret inMCCFR, Rit-1(a|Ii) is the cumulative regret, sit(a|Ii) is the weighted additional strategy and Sit-1(a|Ii) is thecumulative behavior strategy. In tabular CFR, cumulative regret and strategy are stored in the tabular memory, whichlimits it to solve large games. In DNCFR, we use double deep neural networks to approximate these two values.
Figure 3: (a) recurrent neural network architecture with attention for extensive games. Both RSN and ASN arebased on this architecture but with different parameters (θR and θS respectively). (b) an overview of the proposedrobust sampling and mini-batch techniques. The trajectories marked by red arrows are the samples produced by robustsampling (k = 2 here).
Figure 4: Log-log performance on Leduc(5). (a) different sampling methods, k refers to the number of samplingaction for the proposed robust sampling method in each infoset. (b) neural architectures. (c) number of parameters. (d)proportion of observed infosets. Higher proportion indicates more working memory.
Figure 5: Log-log performance. (a) Individual effect of RSN and ASN. RS-MCCFR+ refers to the tabular mini-batchMCCFR+ method with the proposed robust sampling. RS-MCCFR+-RSN only uses one neural network RSN to learncumulative regret while uses a table to save cumulative strategy. RS-MCCFR+-ASN only use one neural networkASN. RS-MCCFR+-RSN-ASN refers to DNCFR with both RSN and ASN. (b) Warm start from tabular CFR andRS-MCCFR+. (c) DNCFR vs XFP vs NFSP. (d) Large Leduc(10) and Leduc(15).
Figure 6: time space trade-off.
Figure 7: Performance of DNCFR on heads-up no-limit Texas Hold’em. (a) Log-log performance of DNCFR onHUNL(1) under different embedding size. (b) Log-Log performance of DNCFR on HUNL(1) under different numbersof gradient descent updates on each iteration. (c) DNCFR beats ABS-CFR by 9.8±4.1 chips per hand and achievessimilar performance with its tabular version but using much less memory.
Figure 8: Comparison of different CFR-family methods on Leduc Hold’em. (a) Performance of robustsampling with different batch size. (b) Robust sampling vs strategy sampling.
Figure 9: Huber loss of three counterfactual value network in our implemented DeepStack. (a) Huber lossof auxiliary network on preflop subgame, the training loss is 0.0000789 and the validation loss is 0.0000776while they are 0.000053 and 0.000055 in original DeepStack. (b) Huber loss of deep counterfactual valuenetwork on flop subgame, the training sample is 0.008 and the validation sample is 0.019 while theyare 0.008 and 0.034 in original DeepStack. (c) Huber loss of deep counterfactual value network on turnsubgame (contains last two rounds of HUNL), the training sample is 0.016 and the validation sample is0.035 while they are 0.016 and 0.026 in original DeepStack. Specifically, the learning rate is decayedin 200th iteration(iteration is equal to epoch here), therefore the huber loss in (b) and (c) decreased. Tobalance the performance of both training and validation samples, we finally select the checkpoints thathave the lowest validation loss.
Figure 10: Comparison of action probability between Alberta,s DeePStack (Moravcik et al., 2017) (the leftcolumn) and our reimplemented DeepStack (the right column).
