Figure 1: Our method extracts a character from an uncontrolled video, and enables us to control itsmotion. The pose of the character, shown in the first row, is created by our Pose2Pose network inan autoregressive way, so that the motion matches the control signal illustrated by the joystick. Thesecond row depicts the character’s appearance, as generated by the Pose2Frame network, which alsogenerates the masks shown in the third row. The final frame (last row) blends a given backgroundand the generated frames, in accordance with these masks.
Figure 2: Comparison with Esser et al. (2018b).
Figure 3: Comparison with Esser et al. (2018a).
Figure 4: The architecture of the Pose2Pose generator. During training, the middle nr - 2 residualblocks are conditioned by a linear projection (FC layer) of the center-mass differences between con-secutive frames (in the x and y axes). For each concatenation of input pose and object [pi-1 , obji-1],the network generates the next consecutive pose and object [pi , obji]. At inference time, the networkgenerates the next pose-object pair in an autoregressive manner, conditioned on input directions.
Figure 5: The Pose2Frame network. (a) For each two combined input pose and object (p =[pi-1 + obji-1, pi + obji]), the network generates an RGB image (zi) and a mask (mi). The RGBand background images are then linearly blended by the generated mask to create the output frame fi .
Figure 6: Samples of masks that model both thecharacter, and places in the scene in which appear-ance is changed by the character. (a) The shadowand the tennis racket of the character are capturedby the mask, (b) the dancer’s shadow appears aspart of the mask.
Figure 7: Generated frames for the controllable tennis character, blended into different backgrounds.
Figure 8: The occlusion-based augmentation technique used to increase robustness during trainingthe P2P network. Each row is a single sample. (a) pi-1 with part of it occluded by a random ellipse,(b) the predicted Pose pi, (C) the ground truth Pose pi. The generated output seems to "fill in" themissing limbs, as well as predict the next frame. In this figure and elsewhere, the colors represent the3D UV mapping.
Figure 9: Mask losses applied during the P2F network training. An inverse binary-thresholded maskis used to penalize pixel intensity for the generated mask, in the regions excluding the character ofinterest. For the generated mask, we apply regularization over the derivatives in the x and y axes aswell, to encourage smooth mask generation, and discourage high-frequency pattern generation.
Figure 10: Training the P2F network. (a) A sample pose, (b) the target frame, (c) the generated rawframe, the mask, and the output frame at different epochs: 10, 30, 50, and 200 (final).
Figure 11: Synthesizing a walking character, emphasizing the control between the frames. Shown arethe sequence of poses generated in an autoregressive manner, as well as the generated frames.
Figure 12: Generated frames for the controllable fencing character. Each column is a different pose.
Figure 13: Synthesizing an additional walking character. Shown are the sequence of poses generatedin an autoregressive manner, as well as the generated frames.
Figure 14: A comparison of the P2F network with the pix2pixHD method of Wang et al. (2018b). (a)Ground truth image used as the pose source, (b) our result, (c) The results of pix2pixHD. The baselinemethod results in many background artifacts, as it generates the entire frame. The degradation inimage quality is apparent as well, and that of the character in particular.
Figure 15: A comparison of the P2F network with the vid2vid method of Wang et al. (2018a). (a)The target-pose image, (b) the pose extracted from this image, (c) the result of vid2vid, (d) our result,(e) a frame from the reference video. Many artifacts are apparent in the background produced byvid2vid. vid2vid also distorts the character’s appearance and dimensions to better match the pose.
Figure 16: P2F ablation. (a) Ours, (b) no VGG FM on the full-frame (no shadows generated), (c) nomask regularization (background artifacts), (d) 1 input pose (no racket generation due to a semanticsegmentation mis-detection), (e) no discriminator FM (character/racket heavily distorted), (f) nomask, i.e. background fully-generated (excessive distortion in background / character).
Figure 17: P2P vs. baseline method comparison. Temporal consistency of P2P generated motion(row 1) is apparent, as opposed to the baseline method (row 2), that results in temporal inconsistency.
