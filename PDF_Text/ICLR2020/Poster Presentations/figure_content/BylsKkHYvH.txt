Figure 1: First column: Sedhain et al. (2015) on Movielens 100K (collaborative filtering) dataset.
Figure 2: The change of output values according to the sparsity level of inputs (darker color indicatesgreater absolute value and dotted circles indicate missing nodes with zero imputation). SN makes thepossible output range of a network be more stable with respect to the sparsity level.
Figure 3:	Debiasing variable sparsity using SN according to test set ratio on six imputation tasks ofsingle cell RNA sequence dataset. Test RMSE with 95% confidence interval of 10-runs is provided.
Figure 4:	Debiasing variable sparsity using SN with respect to drop rates on three popular UCIregression datasets. Test RMSE with 95% confidence interval of 5-runs is provided.
Figure 5: Negative log likelihood of MADE on binarized MNIST with and without SN.
Figure 6: Test RMSE of AutoRec with SN on Movielens 100K (left) and Movielens 1M (right) withrespect to the number of hidden units.
Figure 7: Debiasing variable sparsity using SN according to test set ratio on imputation task ofPreimplantation dataset. Test RMSE with 95% confidence interval of 10-runs is provided.
Figure 8: Negative log-likelihood of MADE on binarized MNIST with and without SN in the case ofsmall learning rate (0.001).
Figure 9: Predicted values of AutoRec (Sedhain et al., 2015) (user vector encoding) on Movielens100K (collaborative filtering) dataset with zero imputation (w/ or w/o a normalization) according tothe number of known entries for a randomly selected test point. Input masks are randomly sampled(to artificially control its sparsity level). For each target sparsity level through x-axis, 100 samples aredrawn, scattering the predicted values and plotting the average in solid line. Predicted values of themodel is also plotted according to the strength of the weight decay controlling Î» of the hyper-parameterfor weight decay. First row: The vanilla zero imputation. Second row (ours.): Zero imputation withSparsity Normalization. Third row: Zero imputation with Layer Normalization (Lei Ba et al., 2016).
