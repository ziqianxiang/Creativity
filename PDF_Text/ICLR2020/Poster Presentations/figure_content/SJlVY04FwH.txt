Figure 1: Left: linear convergence of optimal EG, Jacobi OGD, GaUss-Seidel OGD in a bilineargame with the log distance; Middle: comparison among Adam, SGD and EG in learning the mean ofa Gaussian with WGAN with the squared distance; Right: Comparison between EG with (α = 0.02,γ = 2.0) and without scaling (α = γ = 0.2). We use the squared distance.
Figure 2: Heat maps of the spectral radii of different algorithms. We take σ = 1 for convenience. Thehorizontal axis is α and the vertical axis is β. Top row: Jacobi updates; Bottom row: Gauss-Seidelupdates. Columns (left to right): EG; OGD; momentum. If the spectral radius is strictly less than one,it means that our algorithm converges. In each column, the Jacobi convergence region is contained inthe GS convergence region (for EG we need an additional assumption, see Theorem 3.2).
Figure 3: Jacobi vs. GS updates. y-axis: Squared distance ∣∣φ - v||2. x-axis: Number of epochs.
Figure 4: Test samples from the generator network trained with stochastic GD (step size α = 0.01).
Figure 5: Test samples from the generator network trained with stochastic OGD (α = 2β = 0.02).
Figure 6: Test samples generated from the generator network trained with stochastic Adam. Top row:Jacobi updates; Bottom row: GaUss-Seidel updates. Columns (left to right): epoch 0, 5, 10, 20.
Figure 7: Contour plot of spectral radius equal to 0.8. The red curve is for the Jacobi polynomial andthe blue curve is for the GS polynomial. The GS region is larger but for some parameter settings,Jacobi OGD achieves a faster convergence rate.
