Figure 2: The conceptualsearch coverage in the policyspace by parallel learnersWe applied P3S to TD3 as the base algorithm. The constructed algorithm is named P3S-TD3.
Figure 3: Performance for PPO (red), ACKTR (purple), SQL (brown), (clipped double Q) SAC(orange), TD3 (green), and P3S-TD3 (proposed method, blue) on MuJoCo tasks.
Figure 4: Performance of different parallel learning methods on MuJoCo environments (up), ondelayed MuJoCo environments (down)1.	Original Algorithm The original algorithm (TD3) with one learner2.	Distributed RL (DRL) N actors obtain samples from N environment copies. The com-mon policy and the experience replay buffer are shared by all N actors.
Figure 5: Ablation study of P3S-TD3 on Delayed Ant-v1: (a) Performance and Î² (1 seed) withdmin = 0.05, (b) Distance measures with dmin = 0.05, and (c) Comparison with different dmin =0.02, 0.05Performance on Delayed MuJoCo environments Sparse reward environments especially requiremore search to obtain a good policy. To see the performance of P3S in sparse reward environments,we performed experiments on Delayed MuJoCo environments. Delayed MuJoCo environments arereward-sparsified versions of MuJoCo environments and used in Zheng et al. (2018). Delayed Mu-JoCo environments give non-zero rewards periodically with frequency freward or only at the end ofepisodes. That is, in a delayed MuJoCo environment, the environment accumulates rewards givenby the corresponding MuJoCo environment while providing zero reward to the agent, and gives theaccumulated reward to the agent. We evaluated the performance on the four delayed environmentswith freward = 20: Delayed Hopper-v1, Delayed Walker2d-v1, Delayed HalfCheetah-v1 and De-layed Ant-v1.
Figure 6: Performance for PPO (brown), ACKTR (purple), (clipped double Q) SAC (orange), TD3(green), and P3S-TD3 (proposed method, blue) on the four delayed MuJoCo tasks with freward =20.
Figure 7: Performance ofP3S-TD3, CEM-TD3, and TD3(f) Delayed HalfCheetah-v1-400-6000.0	0.2	0.4	0.6	0.8	1.0Total Time Steps (Ie6)(h) Delayed Ant-V121Published as a conference paper at ICLR 2020Appendix E.	C omparis on to Method using Center PolicyIn this section, we consider a variant of the proposed P3S-TD3 algorithm, named Center-TD3. Thisvariant uses a center policy like in Distral (Teh et al. (2017)) and Divide-and-Conquer (Ghosh et al.
Figure 8: Performance ofP3S-TD3, Center-TD3, and TD323Published as a conference paper at ICLR 2020Appendix F.	Res ult on Swimmer-v 1Khadka & Tumer (2018); Pourchot & Sigaud (2019) noticed that most deep RL methods sufferfrom a deceptive gradient problem on the Swimmer-v1 task, and most RL methods could not learneffectively on the Swimmer-v1 task. Unfortunately, we observed that the proposed P3S-TD3 algo-rithm could not solve the deceptive gradient problem in the Swimmer-v1 task either. Fig. 9 showsthe learning curves of P3S-TD3 and TD3 algorithm. In Khadka & Tumer (2018), the authors pro-posed an effective evolutionary algorithm named ERL to solve the deceptive gradient problem onthe Swimmer-v1 task, yielding the good performance on Swimmer-v1, as shown in Fig. 9. P3S-TD3falls short of the performance of ERL on Swimmer-v1. However, it is known that CEM-TD3 dis-cussed in Appendix D outperforms ERL on other tasks (Pourchot & Sigaud (2019)). Furthermore,we observed that P3S-TD3 outperforms CEM-TD3 on most environments in Appendix D.
Figure 9: Performance on Swimmer-v1 of P3S-TD3 (blue), TD3 (orange), and the final performanceof evolutionary RL (Khadka & Tumer (2018), green dashed line).
