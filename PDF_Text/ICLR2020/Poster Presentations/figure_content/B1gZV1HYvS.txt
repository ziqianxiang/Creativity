Figure 1: The density and marginal distribution of agent positions, in 100 repeated episodes with differentinitialized states, generated from different learned poliCies upon Keep-away. The top row of eaCh sub-figure isdrawn from state-aCtion pairs of all agents. Meanwhile, the bottom row explains for eaCh individual (KL meansthe KL divergenCe between generated interaCtions shown in the top row and the demonstrators).
Figure 2: Results of different entropy coefficient λ.
Figure 3: The density and marginal distribution of agents’ positions, (x, y), in 100 repeated episodes with differ-ent initialized states, generated from different learned policies upon Cooperative-communication. Experimentsare done under the same random seed, and we only consider one movable agent. KL is the KL divergencebetween generated interactions (top figure) with the demonstrators.
Figure 4: The density and marginal distribution of agents’ positions, (x, y), in 100 repeated episodes withdifferent initialized states, generated from different learned poliCies upon Cooperative-navigation. Experimentsare done under the same random seed. The top of eaCh sub-figure is drawn from state-aCtion pairs of all agentswhile the below explain for eaCh one. KL is the KL divergenCe between generated interaCtions (top figure) withthe demonstrators.
Figure 5: The density and marginal distributions of agents’ positions, (x, y), in 100 repeated episodes withdifferent initialized states, generated from different learned policies upon Predator-prey. Experiments are con-ducted under the same random seed. The top of each sub-figure is drawn from state-action pairs of all agentswhile the below explains for each one. The KL term means the KL divergence between generated interactions(top figure) with the demonstrators.
