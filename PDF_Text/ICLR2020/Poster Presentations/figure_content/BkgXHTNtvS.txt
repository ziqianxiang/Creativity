Figure 1: Architecture of the shallow network considered in this paper. The network has a singlehidden layer of m neurons with ReLU activation functions, and a neuron with linear activationfunction in its output layer.
Figure 2: A (2, t, 0)-configuration witht = 8. The blue dots show the points inA and red crosses are the points in B.
Figure 3: Illustration of data pointsX1,..., Xn for d = 3 and m = 12.
Figure 4: Different types of cupped minima in terms of differentiability, discussed in Appendix A.
Figure 5: An illustration of the points in equation 24-equation 27 for d = 3 and t = 4. The red♦ f .	.1	∙	.	∙ Λ I I	1,1	11	1	.	1 .	, 1	∙	,	∙	7~l I I 7~lcrosses indicate the points in AUA and the blue dots correspond to the points in BIJ B.
