Figure 1: Left: High-level view of our architecture. The encoder (top-right) estimates the positionof N objects in each input frame. These are passed to the velocity estimator which estimates objects’velocities at the last input frame. The positions and velocities of the last input frame are passedas initial conditions to the physics engine. At every time-step, the physics engine outputs a set ofpositions, which are used by the decoder (bottom-right) to output a predicted image. If the system isactuated, an input action is passed to the physics engine at every time-step. See Section 3 for detaileddescriptions of the encoder and decoder architectures.
Figure 2: Future frame predictions for 3-ball gravitational system (top) and 2-digit spring system(bottom). IN: Interaction Network. Only the combination of Physics and Inverse-Graphics maintainsobject integrity and correct dynamics many steps into the future.
Figure 3: Frame prediction accuracy (SSI, higher is better) for the balls datasets. Left of thegreen dashed line corresponds to the training range, Tpred, right corresponds to extrapolation, Text .
Figure 4: Contents and masks learned bythe decoder. Object masks: σ(m). Ob-jects for rendering: σ(m) c. Contentsand masks correctly capture each part of thescene: colored balls, MNIST digits and CI-FAR background. We omit the black back-ground learned on the balls dataset.
Figure 5: Top: Comparison between our model and PlaNet Hafner et al. (2019) in terms of learningsample efficiency (left). Explicit physics allows reasoning for zero-shot adaptation to domain-shiftin gravity (center) and goal-driven control to balance the pendulum in any position (right). DDPG(VAE) corresponds to a DDPG agent trained on the latent space of an autoencoder (trained with 320kimages) after 80k steps. DDPG (proprio) corresponds to an agent trained from proprioception after30k steps. Bottom: The first 3 rows show a zero-shot counterfactual episode with a gravity multiplierof 1.4 for an oracle, our model and planet, with vertical as the target position (as trained). The lastrow shows an episode using a goal image to infer the non-vertical goal state.
Figure 6: Comparison between graphics decoder and two black-box decoders, trained on data whereobjects only appear in the top half of the scene. Only the graphics decoder is able to correctly renderthe objects in the bottom half of the scene at test time. Broadcast: spatial broadcast decoder (Watterset al., 2019b); Deconv: standard deconvolutional network.
Figure 7: Results for incorrect number of object slots in the physics engien for the 3-body gravitationalsystem Left: Contents and masks learned for 2 object slots. Right: Contents and objects learned for4 object slots.
