Figure 1: A conceptual visualization ofthe proposed adversarial example detec-tion mechanism.
Figure 2: (a) Performances of integrated detection and generative detection under L∞ = 0.3 con-strained attack. (b) Performances of the integrated classifier (discussed in Section 4.3) and generativeclassifier under L∞ = 0.3 constrained and L∞ = 0.4 constrained attacks. The performances ofthe robust classifier (Madry et al., 2017) (accuracy 0.984, error 0.08 at = 0.3, and accuracy 0.984,error 0.941 at = 0.4) are indicated with red cross marks. PGD attack steps 100, step size 0.01.
Figure 3:	Natural samples and corresponding perturbed samples produced by performing a targetedattack against the generative classifier and robust classifier (Madry et al., 2017). Targets from toprow to bottom row are digit class from 0 to 9. We perform the targeted attack by maximizing thelogit output of the targeted class, using L∞ = 0.4 constrained PGD attack of steps 100 and stepsize 0.01. Both classifiers are trained with L∞ = 0.3 constraint.
Figure 4:	(a) Performances of generative detection and integrated detection under L∞ = 8 attack.
Figure 5: Natural samples and corresponding perturbed samples by performing targeted attackagainst the generative classifier and robust classifier (Madry et al., 2017). The targeted attack isperformed by maximizing the logit output of the targeted class. We use L∞	= 12 constrainedPGD attack of steps 30 and step size 2.0 to produce these samples.
Figure 6: Images generated from class conditional Gaussian noise by performing targeted attackagainst the generative classifier and robust classifier. We use PGD attack of steps 60 and step size0.5 × 255 to perform L2	= 30 × 255 constrained attack (same as Santurkar et al. (2019). TheGaussian noise inputs from which these two plots are generated are the same. Samples not selected.
Figure 7: Performance of generative detection (a) and generative classification (b) on MNIST datasetunder attacks with different loss functions. Please refer to MadryLab (b) for the implementations ofcross-entropy loss and CW loss based attacks.
Figure 8: Distributions of class 1’s logit outputs of natural samples from class 1 and perturbedsamples from the first row of Figure 3 (MNIST dataset).
Figure 9: Performance of generative detection (a) and generative classification (b) on CIFAR10dataset under attacks with different loss functions. Cross-entropy and CW loss is only able to out-performs loss 9 when detection threshold is low (over 0.9 TPR). Please refer to MadryLab (a) forthe implementations of cross-entropy loss and CW loss based attacks.
Figure 10: Distributions of class 1’s logit outputs of natural samples of class 1 and generated samplesfrom the first row of Figure 6 (CIFAR10 dataset).
Figure 11: Training and testing AUC histories of two base detectors. Adv AUC is the AUC scorecomputed on {(x, 0) : x ∈ D0f\k} ∪ {(x, 1) : x ∈ Dkf}, and nat AUC is the score computed on{(x, 0) : x ∈ D\fk} ∪ {(x, 1) :x ∈Dkf}.
Figure 12: Perturbed samples produced by attacking the k = 0 (airplane) detectors and the naturaltrained classifier’s 1st logit output. All samples reached the same L2 perturbation of 1200 (producedusing PGD attacks of step size 10.0).
Figure 13: Images generated from class conditional Gaussian noise by attacking L∞ = 16 con-strained CIFAR10 detectors. we use L2 = 100 × 255 constrained PGD attack of steps 200 andstep size 0.5 × 255. Samples not selected.
Figure 14: ImageNet 224 × 224 × 3 random samples generated from class conditional Gaussiannoise by attacking robust classifier and detector models trained with different constrains. Note thanlarge perturbation models didn’t reach robustness. Please refer to Santurkar et al. (2019) for thedetail about how the class conditional Gaussian is estimated.
Figure 15: Perturbed samples produced by attacking the L∞	= 0.3 trained dog detector usingL2 = 30 constrained PGD attack of steps 100 and step size 5. Top rows are original images, andsecond rows are attacked images.
Figure 17: More 224 × 224 × 3 random samples generated by attacking the L∞ = 0.3 traineddetector with L2 = 100 constrained PGD attack of steps 100 and step size 10.0.
Figure 18: Image generated by attacking the generative classifier (based on L∞	= 16 traineddetectors) and discriminative robust classifier (Madry et al., 2017) using (the same) Gaussian noiseimage. We used unconstrained L2 PGD attack of step size 0.5*255. The five columns correspondingto the perturbed images at step 0, 50, 100, 150, and 200.
Figure 19: Ordinary discriminative training and generative adversarial training on real 1D data. Thepositive class data (blue points) are sampled from a mixture of Gaussians (mean 0.4 with std 0.01,and mean 0.6 with std 0.005, each with 250 samples). Both the blue and red data has 500 samples.
Figure 20: 2D datasets (top row, blue points are class 1 data, and red points are class 0 data, bothhave 1000 data points) and sigmoid outputs of GAT trained models (bottom row). The architectureof the MLP model for solving these tasks is 2-500-500-500-500-500-1. PGD attack steps 10, stepsize 0.05, and perturbation limit L∞ = 0.5.
