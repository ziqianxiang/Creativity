Figure 1: 'ι- and '2-norms of the gradients forCIFAR-10 test-set mini-batches. Note the differ-ence between the scales on the horizontal and ver-tical axis. We observe that our regularization termdecreases the 'ι-norm significantly, compared to itsunregularized counterpart.
Figure 2: KL-divergence of the floating pointpredictive distribution to the predictive distribu-tion of the quantized model for CIFAR-10 test-set mini-batches. We observe that the regulariza-tion leads to a smaller gap, especially for smaller bit-widths.
Figure 3: Predicting induced loss using first-order terms. We added '∞-bounded noise With δ CorresPond-ing to 4-bit quantization to all weights of ResNet-18 and compared the induced loss on the CIFAR-10 test-setWith the PrediCtions using gradients. While not PerfeCt, the first-order term is not insignifiCant.
Figure 4: Accuracy of regularized VGG-like after post-training quantization. We trained 5 models withdifferent initializations and show the mean accuracy for each quantization configuration. The error bars indicatemin/max observed accuracies. (a) Weight-only quantization (b) Activation quantization fixed to 4-bitsis a fixed constant (Baydin et al., 2018). This can be seen from the fact that kVwLkι is a functionR|w| → R, where |w| denotes the number of weights and the computation of the gradient w.r.t. theloss contains E elementary operations, as many as the forward pass. In practice, enabling regular-ization increased time-per-epoch time on CIFAR10 from 14 seconds to 1:19 minutes for VGG, andfrom 24 seconds to 3:29 minutes for ResNet-18. On ImageNet epoch-time increased from 33:20minutes to 4:45 hours for ResNet-18. The training was performed on a single NVIDIA RTX 2080Ti GPU.
Figure 5: Random cross sections of decision boundaries in the input space. To generate these cross-sections, we draw a random example from the CIFAR-10 test set (represented by the black dot in the center)and pass a random two-dimensional hyper-plane ⊂ R1024 through it. We then evaluate the network’s output foreach point on the hyper-plane. Various colors indicate different classes. Softmax’s maximum values determinethe contours. The top row illustrates the difference between the baseline and the regularized VGG-like networks(and their quantized variants) when they all classify an example correctly. The bottom row depicts a case wherethe quantized baseline misclassifies an example while the regularized network predicts the correct class. Wecan see that our regularization pushes the decision boundaries outwards and enlarges the decision cells.
Figure 6: Quantization noise is uniformly distributed. In this plot we show the quantization noise on eachindividual weight in an ImageNet trained ResNet18 model. The noise is scaled by the width of the quantizationbin for each weight quantizer. This plot shows that quantization noise is uniformly distributed between —δ∕2and δ∕2.
Figure 7:	The gradients in unregularized networks tend to become smaller as training progresses. This meansfor large parts of the training there is no need to apply the regularization. The plots on the left show theregularization penalty in unregularized networks. The plots on the right show how turning on the regularizationin the last 15 epochs of the training can push the regularization loss even further down.
Figure 8: '∞-bounded vectors include other bounded- norm vectors. In this plot We show that the pertur-bations With bounded 'p-norm are a subset of '∞-bounded perturbations. For P = 1, 2, ∞, we plot the vectorswith kxkp = 1.
