Figure 1: Intuitive illusion of how training data moves and how sample den-sity varies in a two-dimensional feature space during the training procedure.
Figure 2: Intuitive illustration on the inherent limitations of the g-SCE loss. Reasonably learned features for a classification task shoulddistribute in clusters, so it is counter-intuitive that the feature points tend to move to infinity to pursue lower loss values when applying theg-SCE loss. In contrast, MMC induces models to learn more structured and orderly features.
Figure 3: (a) Test error rates on clean images w.r.t training time on CIFAR-10. Here AT refers to 10-steps targeted PGD adversarial training,i.e., ATt1a0r. (b) Two-dimensional visualization of the attacks on trained MMC networks in the feature space of MNIST. For each attack there is= 0.3 with step size of 0.01. The total number of iteration steps is 50, where Iter-n indicates the perturbed features at n-th iteration step.
Figure 4: Classification accuracy under the black-box transfer-based attacks on the test set of CIFAR-10. The substitute model refers to theone used to craft adversarial examples, and the target model is the one that an adversary actually intends to fool. Here AT refers to ATt1a0r .
Figure 5: Intuitive illustration of the Max-Mahalanobis centers in the cases of L = 2, 3, 4.
Figure 6: Intuitive demonstration of the attacking mechanisms under different adaptive objectives. Here y is the original label, yarg maxl6=y hl is the label of the nearest other decision region w.r.t. the feature z, and yt is the target label of targeted attacks.
