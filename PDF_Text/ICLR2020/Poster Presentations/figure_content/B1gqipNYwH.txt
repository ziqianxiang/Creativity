Figure 1: (a) Learning curves comparing deep skill chaining (DSC), a flat agent (DDPG) and Option-Critic. (b) Comparison with Hierarchical Actor Critic (HAC). (c) the continuous control tasks cor-responding to the learning curves in (a) and (b). Solid lines represent median reward per episode,with error bands denoting one standard deviation. Our algorithm remains the same between (a) and(b). All curves are averaged over 20 runs, except for Ant Maze which was averaged over 5 runs.
Figure 2: Initiation sets of options learned in the Lock and Key task. Blue sphere in top-right roomrepresents the key, red sphere in top-left room represents the lock. Red regions represent statesinside the initiation classifier of learned skills, whereas blue/gray regions represent states outside ofit. Each column represents an option - the top row corresponding to the initiation set when has_keyis false and the bottom row corresponding to the initiation set when has_key is true.
Figure 3:	Solution trajectories found by deep skill chaining. Sub-figure (d) shows two trajectoriescorresponding to the two possible initial locations in this task. Black points denote states in whichπO chose primitive actions, other colors denote temporally extended option executions.
Figure 4: An illustration of the deep skill chaining algorithm. ? represents the goal state, × repre-sents the two start states. (a) Before the agent has discovered its first skill/option, it acts accordingto its global DDPG policy. Having encountered the goal state N times, the agent creates an optionto trigger the goal from its local neighborhood. (b) Now, when the agent enters the initiation set ofthe first option, it begins to learn another option to trigger the first option. (c) Because the agent hastwo different start states, it learns two qualitatively different options to trigger the option learned in(b). (d) Finally, the agent has learned a skill tree which it can follow to consistently reach the goal.
Figure 5: Analysis of performance (as measured by mean cumulative reward) of DSC agent as itis allowed to learn more skills in (a) Point-Maze, (b) Four Rooms with LoCk and Key, (C) E-Mazeand (d) Ant-Maze. Note that in general, DSC disCovers as many skills as it needs to solve the givenproblem. For this experiment alone, we restriCt the number of skills that the DSC agent Can learn.
Figure 6: Initiation set Classifiers learned in the Point E-Maze domain. DisCovered skills organizein the form of a tree with a branChing faCtor of 2. The option on the extreme left initiates in theproximity of the goal. Options learned after the goal option branCh off into two separate skill Chains.
Figure 7:	(a) Initially, the policy over options πO can only choose the global option oG as a proxyfor selecting primitive actions. (b) Over time, the agent learns temporally extended skills and addsoutput nodes to the final layer of the DQN parameterizing πO . This continues until the start state s0lies inside the initiation set of a learned option. (c) Empirical evaluation of how the number of skillsin the agent’s option repertoire changes over time in Point-Maze and Four-Rooms with a Lock andKey.
Figure 8:	Variation in DSC performance (as measured by mean cumulative reward) as a function oftwo hyperparameters: (left) the buffer length K and (right) the gestation period N of the option. Fora qualitative description of both hyperparameters, refer to Section 3.3. This experiment shows thatDSC is fairly robust to most reasonable choices of these parameters. All experiments averaged over5 runs. Error bars denote 1 standard deviation. Higher is better.
