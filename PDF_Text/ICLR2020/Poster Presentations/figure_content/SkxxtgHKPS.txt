Figure 1: Training MLP with GLD (σt = 0.2√2γt) on a smaller version of MNIST with differentrandom label portion p. (a) shows the training accuracy. (b) shows the generalization error, i.e.,the gap between the 0/1 loss L01 on the training data and on the test data. (c) plots our bound inTheorem 9. (d) shows that for p = 0, the gradient norms become much smaller at later stages oftraining.
Figure 2: Training MLP with GLD (σt = 0.2γt) on the full MNIST dataset without label corruption.
Figure 3: SGLD fitting random labels. The meaning of these plots are the same as those in Figure 1.
Figure 4: Training SGLD with AlexNet on a subset of MNIST (250 data points) with differentrandom label portion p. The meanings of the y-labels are the same as that in Figure 1. We usethe gradient clipping trick to force the gradient norms of every data points are within CL = 1.
