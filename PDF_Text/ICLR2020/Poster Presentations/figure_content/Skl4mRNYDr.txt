Figure 1: Our method: deep imitative models. Top Center. We use demonstrations to learn aprobability density function q of future behavior and deploy it to accomplish various tasks. Left: Aregion in the ground plane is input to a planning procedure that reasons about how the expert wouldachieve that task. It coarsely specifies a destination, and guides the vehicle to turn left. Right: Goalpositions and potholes yield a plan that avoids potholes and achieves one of the goals on the right.
Figure 2: Imitative planning with the Figure 3: Costs can be assigned to “pot-Gaussian State Sequence enables fine- holes” only seen at test-time. The plan-grained control of the plans.	ner prefers routes avoiding potholes.
Figure 4: Goalregions can becoarsely specifiedto give directions.
Figure 5: Illustration of our method applied to autonomous driving. Our method trains an imitativemodel from a dataset of expert examples. After training, the model is repurposed as an imitativeplanner. At test-time, a route planner provides waypoints to the imitative planner, which computesexpert-like paths to each goal. The best plan is chosen according to the planning objective andprovided to a low-level PID-controller in order to produce steering and throttle actions. Thisprocedure is also described with pseudocode in Appendix A.
Figure 6: Planning with the Region Final StateIndicator yields plans that end inside the region.
Figure 7: Even with a wider goal region thanFig. 6, the vehicle remains in its lane. Despitetheir coarseness, these wide goal regions stillprovide useful guidance to the vehicle.
Figure 8: Planning with the Final State Indicatoryields plans that end at one of the provided loca-tions. Orange diamonds indicate the locations inthe goal set. Red circles indicate the chosen plan.
Figure 9: Planning with the Line Segment FinalState Indicator yields plans that end along a seg-ment. Orange diamonds indicate line segmentendpoints. Red circles indicate the chosen plan.
Figure 10: Tolerating bad goals. The planner prefers goals in the distribution ofexpert behavior (on the road at a reasonable distance). Left: Planning with 1/2decoy goals. Right: Planning with all goals on the wrong side of the road.
Figure 11: Test-time plans steeringaround potholes.
Figure 12: Architecture of mθ and σθ, which parameterize qθ(S∣φ = {χ, s-τ:0, λ}). Inputs: LIDARχ, past-states s-τ:0, light-state λ, and latent noise ZLT. Output: trajectory SLT. Details inAppendix C.
Figure 13: Left: Samples from the prior, q(S∣φ), go left or right. Right: Samples go forward or right.
Figure 14: Tolerating bad waypoints. The planner prefers waypoints in the distribution of expertbehavior (on the road at a reasonable distance). Columns 1,2: Planning with 1/2 decoy waypoints.
Figure 15: Baseline methods we compare against. The red crosses indicate the past 10 positions ofthe agent. Left: Imitation Learning baseline: the green cross indicates the provided goal, and theyellow plus indicates the predicted setpoint for the controller. Right: Model-based RL baseline: thegreen regions indicate the model’s predicted reachability, the red regions are post-processed LIDARused to create its obstacle map.
