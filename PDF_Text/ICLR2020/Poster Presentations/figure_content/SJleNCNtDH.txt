Figure 1: An overview of our approach to incentivizing synergistic behavior via intrinsic motivation. A heavyred bar (requiring two arms to lift) rests on a table, and the policy πθ suggests for arms A and B to lift thebar from opposite ends. A composition of pretrained single-agent forward models, fA and fB, predicts theresulting state to be one where the bar is only partially lifted, since neither fA nor fB has ever encounteredstates where the bar is lifted during training. A forward model trained on the complete two-agent environment,fjoint, correctly predicts that the bar is fully lifted, very different from the compositional prediction. We trainπθ to prefer actions such as these, as a way to bias toward synergistic behavior. Note that differentiating thisintrinsic reward with respect to the action taken does not require differentiating through the environment.
Figure 2: Screenshots of two of our manipulation tasks and our locomotion tasks. From left to right: corkscrewrotating, bar pickup, ant push, soccer. These tasks are all designed to require two agents. We learn policies forthese tasks given only sparse binary rewards, by encouraging synergistic behavior via intrinsic motivation.
Figure 3: Learning curves for each of our environments. Each curve depicts an average across 5 random seeds,with standard deviations shaded. We see that it is much more sample-efficient to shape the reward via r1intrinsic orr2intrinsic than to rely only on the extrinsic, sparse reward signal. Also, typical formulations of intrinsic motivationas surprise do not work well for synergistic tasks because they encourage the system to affect the environmentin ways it cannot currently predict, while our approach encourages the system to affect the environment in waysneither agent would if acting on its own, which is a useful bias for learning synergistic behavior.
Figure 4: Top: Non-synergistic surprise baseline with varying amounts of pretraining for the joint model fjoint.
Figure 5: Left: Screenshots of three-agent versions of ant push and soccer environments. Right: Learningcurves for these environments. Each curve depicts an average across 5 random seeds, with standard deviationsshaded. In these three-agent environments, taking random actions almost never leads to success due to theexponentially lower likelihood of finding a valid sequence of joint actions leading to the goal, and so using onlyextrinsic reward does not perform well. It is apparent that our proposed bias toward synergistic behavior is auseful form of intrinsic motivation for guiding exploration in these environments as well.
Figure 6: The policy πθ maps a state to 1) a categorical distribution over skills for A, 2) a categorical distri-bution over skills for B, 3) means and variances of independent Gaussian distributions for every continuousparameter of skills for A, and 4) means and variances of independent Gaussian distributions for every contin-uous parameter of skills for B . To sample from the policy, we first sample skills for A and B, then sampleall necessary continuous parameters for the chosen skills from the Gaussian distributions. Altogether, the twoskills and two sets of parameters form an action, which can be fed into the forward models for prediction.
Figure 7: Task success rate of learned policy at convergence for the bar pickup task, using our best-performingreward function r2intrinsic. Each result is averaged across 5 random seeds, with standard deviations shown. Wecan infer that once λ reaches a high enough value for the extrinsic rewards to outscale the intrinsic rewardswhen encountered, the agents will be driven toward behavior that yields extrinsic rewards. These extrinsic,sparse rewards are only provided when the task is successfully completed.
