Figure 1:	Effect of the example-weighing hyperparameter α on ImageNet for ResNet-152, Mo-bileNetV2, and NASNet-A, measuring top-1 and top-5 accuracies and the cross-entropy loss.
Figure 2:	Effect of the example-weighing hyperparameter α for models trained with Group Nor-malization on CIFAR-100, SVHN, and Caltech-256.
Figure 3: Accuracy vs. Ghost Batch Normalization size for CIFAR-100, SVHN, and Caltech-256.
Figure 4: The complementary effects of Inference Example Weighing (Sec. 3.1) and Ghost BatchNormalization (Sec. 3.2) on CIFAR-100, SVHN, and Caltech-256.
Figure 5: Total performance changes across batch sizes for CIFAR-100 and Caltech-256 (a) whentraining from scratch, incorporating all proposed improvements to Batch Normalization. On the bot-tom (b) is the same on Flowers-102 and CUB-2011, which employs transfer learning via fine-tuningfrom ImageNet. Also shown within each plot is the performance of Group Normalization, an ide-alized Batch Normalization that scales perfectly across batch sizes, and Ghost Batch Normalization(Sec. 3.2) by itself, for which the x-axis represents the Ghost Batch Size B0.
Figure 6: Range of output values obtained during inference on the CIFAR-10 test set, comparedwith the range observed during training and the bound of Eq. 2. See text for details.
Figure 7:	Effect of the example-weighing hyperparameter α on ImageNet; supplemental version ofFig. 1 with a larger range of α.
Figure 8:	Effect of the example-weighing hyperparameter α for models trained with Group Nor-malization on CIFAR-100, SVHN, and Caltech-256; supplemental version of Fig. 2 with a largerrange of α.
Figure 9: The complementary effects of Inference Example Weighing and Ghost Batch Normaliza-tion on CIFAR-100, SVHN, and Caltech-256; supplemental version of Fig. 4 with a larger range ofα.
