Figure 1: Left: a single once-for-all network is trained to support versatile architectural configurationsincluding depth, width, kernel size, and resolution. Given a deployment scenario, a specialized sub-network is directly selected from the once-for-all network without training. Middle: this approachreduces the cost of specialized deep learning deployment from O(N) to O(1). Right: once-for-allnetwork followed by model selection can derive many accuracy-latency trade-offs by training onlyonce, compared to conventional methods that require repeated training.
Figure 2: Comparison between OFA and state-of-the-art CNN models on ImageNet.OFA provides80.0% ImageNet top1 accuracy under the mobile setting (< 600M MACs).
Figure 3: Illustration of the progressive shrinking process to support different depth D, width W,kernel size K and resolution R. It leads to a large space comprising diverse sub-networks (> 1019).
Figure 5: Left: Kernel transformation matrix for elastic kernel size. Right: Progressive shrinking forelastic depth. Instead of skipping each layer independently, we keep the first D layers and skip thelast (4 - D) layers. The weights of the early layers are shared.
Figure 4: Progressive shrinking can be viewed as a generalized network pruning technique withmuch higher flexibility. Compared to network pruning, it shrinks more dimensions (not only width)and provides a much more powerful once-for-all network that can fit different deployment scenariosrather than a single pruned network.
Figure 6: Progressive shrinking for elastic width. In this example, We progressively support 4, 3, and2 channel settings. We perform channel sorting and pick the most important channels (with large L1norm) to initialize the smaller channel settings. The important channels, weights are shared.
Figure 7: ImageNet top1 accuracy (%) performances of sub-networks under resolution 224 X 224.
Figure 8: OFA saves orders of magnitude design cost compared to NAS methods.
Figure 9:OFA achieves 80.0% topi accuracy with 595M MACS and 80.1% topi accuracy with143ms PiXelI latency, setting a new SOTA ImageNet top1 accuracy on the mobile setting.
Figure 10:OFA consistently outperforms MobileNetV3 on mobile platforms.
Figure 11: Specialized OFA models consistently achieve significantly higher ImageNet accuracywith similar latency than non-specialized neural networks on CPU, GPU, mGPU, and FPGA. Moreremarkably, specializing for a new hardware platform does not add training cost using OFA.
Figure 13: Quantative study of OFA's roofline model on Xilinx ZU9EG and ZU3EG FPGAs (logscale). OFA model increased the arithmetic intensity by 33%/43% and GOPS/s by 72%/92% on thesetwo FPGAs compared with MnasNet.
Figure 14: OFA can design specialized models for different hardware and different latency constraint.
Figure 15: Performances of selected sub-networks using different accuracy prediction model.
