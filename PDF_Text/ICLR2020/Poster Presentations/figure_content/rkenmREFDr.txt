Figure 1: Stages of our framework(c) Learned partitionalgorithmic efficiency. In the high accuracy regime, our framework yields partitions that lead to processing up to 2.3×fewer candidates than the strongest baseline.
Figure 2: Hierarchical partition into 9 bins with m1 = m2 = 3. Ri ’s are partitions, Pj ’s are the bins of the dataset.
Figure 3: Largest ratio between the number of candidates for Neural LSH and k-means over the settings where bothattain the same target 10-NN accuracy, over accuracies of at least 0.85. See details in Section 4.2.
Figure 4: Comparison of Neural LSH with baselines; x-axis is the number of candidates, y-axis is the 10-NN accuracyNeural LSH (Q.95-□uantιlek-ɪneans averagek*means (0.95-quantileLSH 0.95-quantileITQ average.
Figure 5: Comparison of decision trees built from hyperplanes: x-axis - number of candidates, y-axis - 10-NN accuracy4.3	Comparison with tree-based methodsNext we compare binary decision trees, where in each tree node a hyperplane is used to determine which of the twosubtrees to descend into. We generate hyperplanes with the following methods: Regression LSH, the Learned KD-treeof Cayton & Dasgupta (2007), the Boosted Search Forest of Li et al. (2011), cutting the dataset into two equal halvesalong the top PCA direction (Sproull, 1991; Kumar et al., 2008), 2-means clustering, and random projections of thecentered dataset (Dasgupta & Sinha, 2013; Keivani & Sinha, 2018). We build trees of depth up to 10, which correspondto hierarchical partitions with the up to 210 = 1024 bins. Results for GloVe and SIFT are summarized in Figure 5 (seeappendix). For random projections, we run each configuration 30 times and average the results.
Figure 6: Effect of various hyperparameters5 Conclusions and future directionsWe presented a new technique for finding partitions of Rd which support high-performance indexing for sublinear-timeNNS. It proceeds in two major steps: (1) We perform a combinatorial balanced partitioning of the k-NN graph of thedataset; (2) We extend the resulting partition to the whole ambient space Rd by using supervised classification (suchas logistic regression, neural networks, etc.). Our experiments show that the new approach consistently outperformsquantization-based and tree-based partitions. There is a number of exciting open problems we would like to highlight:•	Can we use our approach for NNS over non-Euclidean geometries, such as the edit distance (Zhang & Zhang, 2017)or the optimal transport distance (Kusner et al., 2015)? The graph partitioning step directly carries through, but thelearning step may need to be adjusted.
Figure 7: MNIST, comparison of Neural LSH with k-means; x-axis - number of candidates, y-axis - 10-NN accuracyFigure 8: MNIST, comparison of trees built from hyperplanes; x-axis - number of candidates, y-axis - 10-NN accuracy13Published as a conference paper at ICLR 2020B Effect of Neural Catalyzer on Space PartitionsIn this section we compare vanilla k-means with k-means run after applying a Neural Catalyzer map (Sablayrolles et al.,2019). The goal is to check whether the Neural Catalyzer - which is designed to boost UP the performance of sketchingmethods for NNS by adjusting the input geometry - could also improve the quality of space partitions for NNS. SeeFigure 9 for the comparison on GloVe and SIFT with 16 bins. On both datasets (especially SIFT), Neural Catalyzerin fact degrades the quality of the partitions. We observed a similar trend for other numbers of bins than the settingreported here. These findings support our observation that while both indexing and sketching for NNS can benefit fromlearning-based enhancements, they are fundamentally different approaches and require different specialized techniques.
Figure 8: MNIST, comparison of trees built from hyperplanes; x-axis - number of candidates, y-axis - 10-NN accuracy13Published as a conference paper at ICLR 2020B Effect of Neural Catalyzer on Space PartitionsIn this section we compare vanilla k-means with k-means run after applying a Neural Catalyzer map (Sablayrolles et al.,2019). The goal is to check whether the Neural Catalyzer - which is designed to boost UP the performance of sketchingmethods for NNS by adjusting the input geometry - could also improve the quality of space partitions for NNS. SeeFigure 9 for the comparison on GloVe and SIFT with 16 bins. On both datasets (especially SIFT), Neural Catalyzerin fact degrades the quality of the partitions. We observed a similar trend for other numbers of bins than the settingreported here. These findings support our observation that while both indexing and sketching for NNS can benefit fromlearning-based enhancements, they are fundamentally different approaches and require different specialized techniques.
Figure 9: Comparison of k-means and Catalyzer + k-meansC Proof of Theorem 3.1Proof. Consider an undirected graph G = (V, E), where the set of vertices V is P, and the (multi-)set of edges containsan edge (p,p0) for every p0 ∈ Nk(p). The graph contains n vertices and kn edges, and some of the edges might bedouble (if p0 ∈ Nk(p) andp ∈ Nk(p0) at the same time). Let AG be the symmetric adjacency matrix of G normalizedby 2kn (so that the sum of all the entries equals to 1, thus giving a probability distribution over P × P, which canbe seen to be equal to Dclose). The rows and columns of AG can naturally be indexed by the points of P . DenoteρG(p) = Pp0 (AG)p,p0. It is immediate to check that ρG yields a distribution over P, which can be seen to be equal toD. Denote DG = diag(ρG). Denote LG = DG - AG the Laplacian of AG. Due to the equivalence of ρG and D andAG and Dclose , we have:a_	Ppi(AG)p,p0 ∙kp-P0k2一— ------------：_：--：-：-T-----TTTT .
Figure 10: Results from Figure 4 with broader candidate and accuracy regimes. The “Learned RCS-LSH” baseline isthe learned rectilinear cell structure locality sensitive hashing method of Cayton & Dasgupta (2007).
