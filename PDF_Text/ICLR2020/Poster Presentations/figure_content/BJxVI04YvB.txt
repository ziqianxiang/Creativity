Figure 1: Results on ResNet for ImageNet with n = 20000. Default parameters are = 0.01and δ = 10-5 . We plot the median and min/max confidence set sizes. (a) Ablation study; C is“calibrated predictor” (i.e., use fφ T instead of fφ), and D is “direct bound” (i.e., use Theorem 1instead of the VC generalization bound). (b) Restricted to correctly vs. incorrectly labeled images.
Figure 2: Confidence set sizes for an object tracking benchmark (Wu et al., 2013); we use n =5, 000, = 0.01, and δ = 10-5. (a) Ablation study similar to Figure 3. In (b) and (c), we show howthe confidence set sizes produced using our algorithm vary with respect to and δ, respectively.
Figure 3: Results on the dynamics model for the half-cheetah with n = 5000. Default parametersare = 0.01 and δ = 10-5. (a) Ablation study; A is “accumulated variance” (i.e., for each t ∈{1,…，20}, use ∑t instead of ∑t = Σ(xt-ι)), and C and D are as for ResNet. We plot the medianand min/max confidence set sizes (see Section 3.6), averaged across t ∈ {1, ..., 20}. (b) Sameablations, but with per time step size. We plot the average size of the confidence set for the predictedstate xt on step t, as a function oft ∈ {1, ..., 20}. (c) Varying , and (d) varying δ.
Figure 4: Sample complexity of different bounds; we fix δ = 10-5. Left: Sample complexity ofVCbound and direct bound when k = 0. Right: Sample complexity of direct bound for varying k.
Figure 5: Comparison to baselines that do not have theoretical guarantees. In (a) and (b), we showresults for ImageNet, and in (c) and (d), we show results for the half-cheetah. In (a) and (c), weshow the empirical error in the confidence set sizes; the dotted line denotes = 0.01, our targetconfidence set error. In (b) and (d), we show the sizes of the constructed confidence sets.
Figure 6: Confidence set sizes for two neural network architectures trained on ImageNet; for both,we use n = 20, 000, = 0.01 and δ = 10-5. Left: AlexNet (Krizhevsky, 2014); here, the empiricalconfidence set error of our approach C + D is 0.0066. Right: GoogLeNet (Szegedy et al., 2015);here, the empirical confidence set error of our approach is 0.0061.
Figure 7: Confidence set sizes for three additional classification benchmarks: (a) the arrhythmiadetection dataset (Guvenir et al., 1997); here, n = 90,	= 0.1, δ = 0.05, and the empiricalconfidence set error of our approach C + D is 0.0435, (b) the car evaluation dataset (Bohanec &Rajkovic, 1988); here, n = 345, = 0.05, δ = 10-5, and the empirical confidence set error of ourapproach C + D is 0.0172, and (c) the CHOP alarm dataset (Bonafide et al., 2017); here, n = 1000,= 0.02, δ = 10-5, and the empirical confidence set error of our approach C+D is 0.0159. (d) Thefractions of actionable and false alarms with a confidence set {0} (i.e., only contains false alarm).
Figure 8: Confidence set sizes for two benchmarks focused on regression; for both, we use = 0.1and δ = 0.05. Left: the Auto MPG dataset (Quinlan, 1993); here, n = 70, and the empiricalconfidence set error of our approach C + D is 0.1250. Right: The student grade dataset (Cortez &Silva, 2008); here, n = 100, and the empirical confidence set error of our approach is 0.0597.
