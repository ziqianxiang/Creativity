Figure 1: Testing error as a function of number of mini-batch updates. pb = 60% means 60% of thetraining data are from the high frequency region [-2, 0) and is labeled as Biased-high. We includeunbiased training dataset as a reference (Unbiased). The total numbers of training data are the sameacross all experiments. The testing set is unbiased and the results are averaged over 50 random seedswith the shade showing standard error.
Figure 2: We show the learning curve of the l2 regression on three training datasets in Figure (a) andshow 1k points uniformly sampled from the two biased training data sets in (b)(c) respectively. Thetotal number of training data points are the same across all experiments. The yellow area includesall the spikes, and is defined by restricting ||y| - 1.0| < 0.1. The testing set is unbiased and theresults are averaged over 50 random seeds with the shade indicating standard error.
Figure 3:	Evaluation curves (sum of episodic reward v.s. environment time steps) of Dyna-Value,PrioritizedER, Dyna-Frequency, ER on MountainCar with different number of planning updateswith different reward noise variance. Notice that the dashed line denotes the evaluation curve ofour algorithm with an online learned model. σ = 0 indicates the original deterministic reward. Allresults are averaged over 30 random seeds.
Figure 4:	(a) is a visualization of the MazeGridWorld domain. (b) shows evaluation curves ofDyna-Value and Dyna-Frequency. Dashed line indicates using an online learned model of ouralgorithm. All results are averaged over 30 random seeds.
Figure 5: The state distribution in the search-control queue of our algorithm Dyna-Frequency (a)and Dyna-Value (b) at 50k environment time step. Each blue shadow area is a 0.1 × 0.1 squareindicating the hole where the agent can go through the wall. Our search-control queue has a statedistribution with a high density around those squares. In (a), there are 25.3% points fall inside a 0.1radius ball centered at each square in total; in (b), there are 11.7% such points. The black box onthe top right is the goal area.
Figure 6: Evaluation curves (sum of episodic reward v.s. environment time steps) of hill climbingon gradient norm (Dyna-GradNorm) and Hessian norm (Dyna-HessNorm) on MountainCar andGridWorld with 10 planning updates. All results are averaged over 30 random seeds.
Figure 7: The learning curves showing sum of rewards per episode as a function of environmenttime steps. We use 5 planning steps for both algorithm. The results are averaged over 10 randomseeds.
Figure 8: The state distribution in the search-control queue and ER buffer at 50, 000 environmenttime step. The blue shadow indicates the hole area where the agent can go through the wall. Theblack box on the right top is the goal area.
