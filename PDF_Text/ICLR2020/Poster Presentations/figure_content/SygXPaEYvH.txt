Figure 1: Architecture for pre-training VL-BERT. All the parameters in this architecture includingVL-BERT and Fast R-CNN are jointly trained in both pre-training and fine-tuning phases.
Figure 2: Input and output formats for fine-tuning different visual-linguistic downstream tasks.
Figure 3: Visualization of attention maps in pre-trained VL-BERTbase. Line intensity indicatesthe magnitude of attention probability with the text token as query and the image RoI as key. Theintensity is affinely rescaled to set the maximum value as 1 and the minimum as 0, across differentheads in each layer. The index of network layer and attention head is counted from 0.
