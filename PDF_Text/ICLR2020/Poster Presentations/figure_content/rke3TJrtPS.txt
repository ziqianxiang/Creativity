Figure 1: Update procedures for PCPO. In stepFig. 1).
Figure 2: Update procedures for CPO (Achiamet al., 2017). CPO computes the update by si-multaneously considering the trust region (lightgreen) and the constraint set (light orange). CPObecomes infeasible when these two sets do not in-tersect.
Figure 3: The gather, circle, grid and bottleneck tasks. (a) Gather task: the agent is rewarded forgathering green apples but is constrained to collect a limited number of red fruit (Achiam et al.,2017). (b) Circle task: the agent is rewarded for moving in a specified wide circle, but is constrainedto stay within a safe region smaller than the radius of the circle (Achiam et al., 2017). (c) Gridtask: the agent controls the traffic lights in a grid road network and is rewarded for high throughputbut constrained to let lights stay red for at most 7 consecutive seconds (Vinitsky et al., 2018). (d)Bottleneck task: the agent controls a set of autonomous vehicles (shown in red) in a traffic mergesituation and is rewarded for achieving high throughput but constrained to ensure that human-drivenvehicles (shown in white) have low speed for no more than 10 seconds (Vinitsky et al., 2018).
Figure 4:	The values of the discounted reward and the undiscounted constraint value (the totalnumber of constraint violation) along policy updates for the tested algorithms and task pairs. Thesolid line is the mean and the shaded area is the standard deviation, over five runs. The dashed linein the cost constraint plot is the cost constraint threshold h. The curves for baseline oracle, TRPO,indicate the reward and constraint violation values when the constraint is ignored. (Best viewed incolor, and the legend is shared across all the figures.)Gaussian policies. For the simulations in the grid and bottleneck tasks, we use a neural networkwith two hidden layers of size (16, 16) and (50,25) to represent Gaussian policies, respectively. Inthe experiments, since the step size is small, we reuse the Fisher information matrix of the rewardimprovement step in the KL projection step to reduce the computational cost. The step size δ is setto 10-4 for all tasks and all tested algorithms. For each task, we conduct 5 runs to get the meanand standard deviation for both the reward and the constraint value over the policy updates. Theexperiments are implemented in rllab (Duan et al., 2016), a tool for developing and evaluating RLalgorithms. See the supplemental material for the details of the experiments.
Figure 5:	The value of the discounted reward versus the cumulative constraint value for the testedalgorithms and task pairs. See the supplemental material for learning curves in the other tasks.
Figure 6: Update procedures for PCPO when the current policy πk is infeasible. πlk+1 is the projec-tion of ∏k+ 2 onto the sublevel set of the constraint set. We find the KL divergence between ∏k andπk+1.
Figure 7: The projection onto the convex set with θ0 ∈ C and θ* = ProjL (θ).
Figure 8: The values of the cumulative constraint value over policy update, and the reward versusthe cumulative constraint value for the tested algorithms and task pairs. The solid line is the meanand the shaded area is the standard deviation, over five runs. The curves for baseline oracle, TRPO,indicate the performance when the constraint is ignored. (Best viewed in color, and the legend isshared across all the figures.)suggest that line search is more conservative in optimizing the policies since it usually take smallersteps. However, we conjecture that if using smaller δ, the effect of line search is not significant.
Figure 9: The values of the reward and the constraint value for the tested algorithms and task pairs.
Figure 10: The values of the reward and the constraint value for the tested algorithms and task pairs.
Figure 11: The values of the reward and the constraint value for the tested algorithms and task pairs.
Figure 12: (1) The values of the reward and the constraint, (2) the condition number of the Fisherinformation matrix, and (3) the approximation error of the constraint update direction over trainingepochs with the conjugate gradient method’s iteration of 10 and 20, respectively. The one with largernumber of iteration has more constraint satisfaction since it has more accurate approximation. (Bestviewed in color)# of policy updatesapproximation of H-1a cause PCPO with KL divergence projection have more constraint violationthan TRPO.
Figure 13: The semantic overview of stationery points of PCPO. The red dashed lines are negativedirections of normal cones, and the green dashed lines are objective update directions. The objectiveupdate direction in an stationary point is belong to the negative normal cone.
Figure 14: The policy update direction that combines the objective and the constraint update di-rections of each point (top), and the optimization path of PCPO with KL divergence and L2 normprojections with the initial point [0.5, -2.0]T (below). The red star is the initial point, the red arrowsare the optimization paths, and the region that is below to the black line is the constraint set. We seethat both projections converge to different solutions.
