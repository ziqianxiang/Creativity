Figure 1: Content transfer example. Given an image of a face with glasses (left), and another imageof a face without ones (top), the proposed method successfully identifies and translates the specifiedglasses from the former domain to the latter one.
Figure 2: An illustration of the inference procedure. The discriminator C and decoder DA are notused during inference but are included for illustrative purposes.
Figure 3: Glasses from guide images in domain B (left) augment the glasses-less source images fromdomain A (top). The content transfer of Press et al. (2019) (middle) is compared to our results (right).
Figure 4: Interpolation between Es(b1) (bottom left) and Es(b2) (bottom right) for b1, b2 ∈ B, whilefixing the source image a ∈ A (top ends). The generated images (top) and masks (bottom) are shown.
Figure 5: Adding a handle to a handbag.
Figure 6: Attr removal.
Figure 7: Removal of smile and addition of glasses according to the guided image on the left. In themiddle, the translation of Press et al. (2019) and on the right, our result.
Figure 8: Segmentation of women’s hair. (a) orig-inal image, (b) ground truth segmentation, (c) ourresults, (d) the results of Press et al. (2019), (e)the results of Ahn & Kwak (2018), (f) the resultsof CAM.
Figure 9: (a) Guide images in domain B (faces with facial hair). (b) Results by the method of Presset al. (2019): the top row is the source images in domain A. The others incorporate the facial hairfrom the corresponding row of (a). (c) Same mapping for our method.
Figure 10: Facial Hair InterpolationF	Additional sequential content transfer and attribute removalRESULTSWe provide additional images produced by our method as well as by the baseline method. As canbe seen, in order to perform the guided content transfer of two attributes from two different domain,Press et al. (2019) passes the source input image into the network twice which wastefully reconstructsstatic facial features twice.
Figure 14: Additional content transfer example. Given an image with glasses (left), and anotherimage of a face with no glasses (top), the proposed method identifies and translates the specifiedglasses from the former domain to the latter.
Figure 15: Masks generated for the guided transfer of glasses experiment. Masks generated are forthe translated images in Fig 14.
Figure 16: (a) Guide images in domain B (glasses). (b) Press et al. (2019) method: the top row is thesource images in domain A. The others incorporate the glasses from the corresponding row of (a). (c)Same mapping for our method.
Figure 17: Additional content transfer example. Given an image of smiling face (left), and anotherimage of a non-smiling face (top), the proposed method identifies and translates the specified smilesfrom the former domain to the latter.
Figure 18: Masks generated for the guided transfer of smile experiment. Masks generated are for thetranslated images in Fig 17.
Figure 19: Smile Interpolation23Published as a conference paper at ICLR 2020Figure 20: (a) Guide images in domain B (faces with smile). (b) Press et al. (2019): the top row isthe source images in domain A. The others incorporate the smile from the corresponding row of (a).
Figure 20: (a) Guide images in domain B (faces with smile). (b) Press et al. (2019): the top row isthe source images in domain A. The others incorporate the smile from the corresponding row of (a).
Figure 21: Our method (left) compared to Mejjati et al. (2018) (right) on the task of adding glasses tothe original image (top). We show the generated masks and the final result for both methods. For ourmethod we also show the guidance images.
Figure 22: Out of domain translation. (a) Results on extremely out of domain images. (b) Resultsobtained by manipulating LFW images.
Figure 23: Out of distribution translation. The mapping between faces without and with glasses istrained only on women and applied to men. (a) Guide images in domain B0 (men with glasses) andtop row is the source images in domain A0 (men without glasses), both not given during training. (b)The remaining rows are translated images by the method of Press et al. (2019). (c) Same mapping forour results.
Figure 25: (a) Guide images in domain B (handbags with handles). (b) Press et al. (2019): the toprow is the source images in domain A. The others incorporate the handles from the correspondingrow of (a). (c) Same mapping for our method.
Figure 26: Additional content transfer example. Given an image of bag with a handle (left), andanother image of a handbag (top), the proposed method identifies and translates the specified handbagfrom the former domain to the latter.
Figure 27: Attribute removal for the task of mustache (top left), smile (top right) and glasses (bottomleft). The result of our method is shown alongside the baseline methods, Press et al. (2019), Fader(Lample et al. (2017)), AttGAN (He et al. (2019)) and STGAN (Liu et al. (2019)).
Figure 28: First two images on the left are the content donors applied sequentially, either with facialhair or glasses. The top row is the input source images. The results on the bottom are the translationof Press et al. (2019) while on the top are our results.
Figure 29: Additional removal and content transfer results. Given an image with glasses (left), andanother image of a face with no glasses and a smile (top), the proposed method removes the smileand identifies and translates the specified glasses from the former domain to the latter. In the middleare the translated examples of Press et al. (2019) while on the right are our translated results.
Figure 30: Additional removal and content transfer results for smile removal and glasses addition.
Figure 31: Facial hair swap results. Our method first removes the facial hair and then adds the facialhair of the guided image on the left. In the middle are the translated examples of Press et al. (2019)while on the right are our translated results.
Figure 32: Additional facial hair swap results. Our method first removes the facial hair and then addsthe facial hair of the guided image on the left35Published as a conference paper at ICLR 2020Figure 33: Segmentation of men’s hair. (a) original image, (b) ground truth segmentation, (c) ourresults, (d) the results of Press et al. (2019), (e) the results of Ahn & Kwak (2018), (f) results ofCAM.
Figure 33: Segmentation of men’s hair. (a) original image, (b) ground truth segmentation, (c) ourresults, (d) the results of Press et al. (2019), (e) the results of Ahn & Kwak (2018), (f) results ofCAM.
Figure 34: Additional Segmentation results for of women’s hair. (a) original image, (b) ground truthsegmentation, (c) our results, (d) the results of Press et al. (2019), (e) the results of Ahn & Kwak(2018), (f) results of CAM.
Figure 35: Additional segmentation results for the domain of glasses and facial hair.
Figure 36: Ablation analysis. The first row is images without facial hair on which we want to transferthe facial hair of the image in the first column. Second row: All losses L, third row: without LRAecon2,fourth row: without LBRecon2, fifth row: without LCycle, sixth row: without LBRecon1, seventh row:without LRAecon1, eighth row: without LDC. The ninth row shows the translation where LRAecon2 andLBRecon2 are replaced by L2 regularization of the mask. The tenth row: L1 norm is replaced with L2norm for LRAecon2 and LBRecon2. The last row: L1 norm is replaced with L2 norm for LRAecon1 andLBRecon1 .
Figure 37: The effect of the threshold used to binarized the mask. The left column is theoriginal image and rest of the column are the segmentation mask created using the thresholds:0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 (from left to right).
Figure 39: Sensitivity to changes in λ1. Given an image with glasses (left), and another image of aface with no glasses (top), the proposed method translates the specified glasses using different valuesof λ1: 3.0, 4.0, 5.0, 6.0, 7.0 (from left to right). All other hyperparameters remain fixed.
