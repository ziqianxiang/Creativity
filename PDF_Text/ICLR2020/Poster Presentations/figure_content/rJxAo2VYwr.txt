Figure 1: (top) Given a pre-trained whiteboxmodel f, we capture the layer-wise and class-wisefeature distributions with binary neural networksgl,c, aiming to model the probability that the layerl features extracted from input x are from the classc feature distribution (i.e. p(y = c|fl(x))). (bot-tom) Forward pass for FDA targeted attack.
Figure 2: Targeted adversarial attack transfer results. The x-axis of each plot is the relative layerdepth at which the adversarial example was generated from. Each row is a different whitebox model.
Figure 3: Disruption versus layer depth for all transfer scenarios. Each row uses a different whiteboxmodel. Each line is a different attack, where all FDAs are FDA+fd.
Figure 4: Correlation of a layer'sauxiliary models with the white-box model output.
Figure 5: Saliency maps of auxiliary models on several interesting inputs across model depth.
Figure 6: Class separability versuslayer depth for each whitebox.
Figure 7: Full targeted adversarial attack transfer results. Each row is a unique transfer scenarioand each column is a different attack success metric. The x-axis of each plot is the layer depth atwhich the adversarial example was generated from. Note, top two rows are transfers from DN121whitebox model, middle two rows are from VGG19 whitebox model, and bottow two rows are fromRN50 whitebox.
Figure 8: Error versus layer depth plots caused by untargeted adversarial attacks for DN121 →VGG19 and RN50 → VGG19 transfer scenarios at tWo different attack strengths e = 4, 8.
Figure 9: SmoothGrad saliency maps for RN50 auxiliary models.
