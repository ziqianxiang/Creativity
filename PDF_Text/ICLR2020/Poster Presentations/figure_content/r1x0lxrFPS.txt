Figure 1: Training results of VGG-7 on CIFAR-10 dataset. (a) Test accuracy with various activationprecision. (b) Test accuracy of various activation precision and various network width.
Figure 2: Activation functions and their derivatives used at forward and backward passes (left). Therelationship between the best test accuracy and the cumulative difference when using various STEs(right). Steep4 is extended version of steep2 with slope of 4.
Figure 3: Cosine similarity between true/coarse gradient and CDG in a feed-forward network with3 hidden layers. (a) Cosine similarity value of each layer when different precision is used for activa-tion. The label total refers to the cosine similarity when gradients for each layer are concatenated.
Figure 4: Activation functions of baseline binary (left), ternary (middle) and decoupled binary (right)models. Circle, square and rectangle denote full-precision, binary and ternary data, respectively.
Figure 5: Example model architectures of baseline, coupled ternary model and decoupled binarymodel with specific width numbers. Two weights highlighted in the decoupled model are half thevalue of the weight highlighted in the coupled model.
Figure 6: Training results of VGG-7 on CIFAR-10 dataset in the order of coupled ternary model,decoupled binary model and decoupled binary model trained from scratch (left). 5 circles in eachplot represent the top 5 test accuracy points and the red circle is for the best result. The best accuracyresult is shown at the top left corner of each contour plot. Test accuracy of various models withdifferent activation precision and training schemes (right).
Figure 7: Training results of VGG-7 on CIFAR-10 dataset. 5 circles represent top 5 test accuracypoints and the red circle is for the best result. The best test accuracy in each contour plot is shownon top of the contour plot.
Figure 8: Graphical explanation of CDG using bivariate loss function.
Figure 9: The examples of loss curve and coordinate discrete gradient in a single dimension withdifferent scale of ε and the different number of samples used to calculate the loss curve. The numberof samples used is small (a and b) and large (c and d). The size of ε is large (a and c) and small (band d).
Figure 10: Effect of step size (ε) on cosine similarity between CDG and coarse gradient with dif-ferent precision (a,c,e and g) and with different sophisticated STEs (b,d,f and h). Cosine similaritytrend of each layer (fc1, fc2, fc3 and total) is shown in each row from top to bottom. It can beobserved that there is a large difference in cosine similarity between ternary activation and binaryactivation regardless of the step size. In addition, all BNNs have similar cosine similarity valuesregardless of STE choices and step size.
Figure 11: Training curves for AlexNet and ResNet-18 with BinaryDuo training scheme.
Figure 12: Training result of coupled 2-bit model and the corresponding decoupled model.
Figure 13: Example model architectures of coupled ternary and decoupled binary model when usingBinaryDuo-Q. Coupled ternary model with 2x2 configuration is decoupled to binary model with 4x4configuration.
Figure 14: Training results of coupled ternary model and decoupled binary model when usingBinaryDuo-Q.
Figure 15: Effect ofσ on cosine similarity between ESG and coarse gradient with different precision(a,c,e and g) and with different sophisticated STEs (b,d,f and h). Cosine similarity trend of each layer(fc1, fc2, fc3 and total) is shown in each row from top to bottom.
