Figure 1: Policy ensemble composition model that takes the state information St and a set of primi-tive policies' output {6i}N=o to compute a composite action at.
Figure 2: Benchmark control and manipulation tasks requiring an agent to reach or move the objectto the given targets (shown in red for pusher and green for rest).
Figure 3: Comparison results of our method against several standard RL methods averaged overten trials in a set of difficult tasks. The vertical and horizontal axis represents the distance of theagent/object from the target and environment steps in millions, respectively. Note that our compo-sition framework learns to solve the task with high samples efficiency, whereas other benchmarkmethods either fail or perform poorly.
Figure 4: Performance comparison of our composition model trained with HIRO on a standard Ant(150 units torque limit) against three variants of standard HIRO formulation in three challengingenvironments. The pretrained HIRO model undergoes 4 million steps of extra training to counter forthe training time utilised by premitive skills of our composition model. The low-torque-Ant has 30units toruqe limit. We report mean and standard error, over ten trials, of agent’s final distances fromthe given goals, normalized by their initial distance, over 10 million steps.
Figure 5: Ablative Study: Performance comparison, averaged over ten trials, of our composite modelagainst its ablated variations that lack attention model, bidirectional-RNN (BRNN) or both attentionand BRNN (AttBRNN) in three different environments.
Figure 6: Each path corresponds to its adja-cent attention weight mapping. The weighting“strength” of each primitive policy is depicted foreach step (i.e. up (U), down (D), left (L), and right(R)). Each path begins at the origin and ends whenthe point-mass is within one unit of a goal. Theplot contours represent the position cost.
Figure 7: New skill acquisition task. The composite model has access to a primitive policy formoving right and a trainable policy function. Our method trained the new function to move in theupward direction to reach the given goal.
Figure 8: The attention weights: The blue and yellow colors show low and high values, respectively15Published as a conference paper at ICLR 2020Figure 9: The depiction of attention weights for the halfcheetah-hurdle environment. The weighting“strength” of each primitive policy (i.e., run and jump) for each step is shown. There are three hur-dles in front of the agent. The red-markers on the attention plot shows the position of those hurdles.
Figure 9: The depiction of attention weights for the halfcheetah-hurdle environment. The weighting“strength” of each primitive policy (i.e., run and jump) for each step is shown. There are three hur-dles in front of the agent. The red-markers on the attention plot shows the position of those hurdles.
Figure 10: The depiction of attention weights for the pusher environment. The weighting “strengthof each primitive policy (i.e., Bottom and Left) for each step is shown. It can be seen that the agentconcurrently as well sequentially compose the push-to-the-bottom and push-to-the-left primitives toreach the target.
Figure 11: The depiction of attention weights for the cross-maze environment. The weighting“strength” of each primitive policy (i.e., up (U), down (D), left (L), and right (R)) for each stepis shown. The top and bottom rows show the agent reaching the goals on the top and right, re-spectively. In the top row, the agent emphasizes on the upward primitive and learns to ignore theremaining primitives. In the bottom row, the agent first focuses on the upward primitive to reach thecross-point and then switches to right-direction motion primitive to reach the given goal.
