Figure 1: Pseudo-code for inference from representation and adversarial re-purposingAlgorithm 1 De-censoring representations	1	: Input: Auxiliary dataset Daux, black-box oracle E, observed representation z?2	: Train auxiliary model Maux = Eaux ◦ Caux on Daux3	: Initialize transform model T , inference attack model Mattack4	: for each training iteration do5	:	Sample a batch of data (x, S) from Daux and compute z = E(x), zaux = Eaux (x)6	:	Update T on the batch of (z, zaux) with loss ||T(z) - zaux||227	:	Update Mattack on the batch of (T (z), S) with cross-entropy loss8	: end for9	return prediction S = Mattack(T(z?))3.1	Inferring sensitive attributes from representationWe measure the leakage of sensitive properties from the representations of overlearned models viathe following attack. Suppose an adversary can observe the representation z? of a trained model Mon input x? at inference time but cannot observe x? directly. This scenario arises in practice whenmodel evaluation is partitioned in order to protect privacy of inputs—see Section 2. The adversarywants to infer some property s of x? that is not part of the task label y .
Figure 2: Reduction in accuracy due to censoring. Blue lines are the main task, red lines are theinference of sensitive attributes. First row is adversarial training with different γ values; second andthird row is information-theoretical censoring with different β and λ values respectively.
Figure 3: Pairwise similarities of layer representations between models for the original task (A) andfor predicting a sensitive attribute (B). Numbers 0 through 4 denote layers conv1, conv2, conv3, fc4and fc5.
Figure 4: Similarity of layer representations of a partially trained gender classifier to a randomlyinitialized model before training. Models are trained on FaceScrub using 50 IDs (blue line) and 500IDs (red line).
