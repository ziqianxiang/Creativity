Figure 1: Each column displays the first and last frame of an episode top-to-bottom. After watching onedemonstration (left), the scene is re-arranged. With one trial episode (middle), our method can learn to solvethe task (right) by leveraging both the demo and trial-and-error experience.
Figure 2: Meta-training Overview: First, we meta-train πθIaccording to Eq. 2. Next, we collect L trial trajectories permeta-training task Ti in the environment using our trained πθI .
Figure 3: Our vision-based Phase II architecture: Upper left: we pass the RGB observation for each timestepthrough a 4-layer CNN with ReLU activations and layer normalization, followed by a spatial softmax layerthat extracts 2D keypoints (Levine et al., 2016). We flatten the output keypoints and concatenate them withthe current gripper pose, gripper velocity, and context embedding. Upper Right: We pass the resulting vectorthrough the actor network, which predicts the parameters of a Gaussian mixture over the commanded end-effector position, axis-angle orientation, and finger angle. Lower left: To produce the context embedding, theembedding network applies a vision network to 40 ordered observations sampled randomly from the demo andtrial trajectories. We concatenate the demo and trial outputs with the trial episode rewards along the embeddingfeature dimension, then apply a 10x1 convolution across the time dimension, flatten, and apply a MLP toproduce the final context embedding. The Phase I policy architecture (Appendix Fig. 8) is the same as shownhere, but omits the concatenation of trial embeddings and trial rewards.
Figure 5: Illustration of example episodes in four distinct task families: button pressing, grasping, sliding,and pick-and-place. Each column shows the first and last frames of an episode top-to-bottom. We meta-traineach model on hundreds of tasks from each of these task families. For each task within a task family, we use aunique pair of kitchenware objects sampled from a set of nearly one hundred different objects.
Figure 4: Average return of each method on heldout meta-test tasks in the reaching environment, afterone demonstration and one trial. Our Watch-Try-Learn(WTL) method is quickly able to learn to imitate thedemonstrator. Each line shows the average over 5 sepa-rate training runs with identical hyperparameters, eval-uated on 50 randomly sampled meta-test tasks. Shadedregions indicate 95% confidence intervals.
Figure 6: The average success rate of different methods in the gripper control environment, for both statespace (non-vision) and vision based policies. The leftmost column displays aggregate results across all taskfamilies. Our Watch-Try-Learn (WTL) method significantly outperforms the meta-imitation (MIL) baseline,which in turn outperforms the behavior cloning (BC) baseline. We conducted 5 training runs of each methodwith identical hyperparameters and evaluated each run on 40 held out meta-test tasks. Error bars indicate 95%confidence intervals.
Figure 7: Our reaching toy environment with 2 target objects and 2 degrees of freedom (DOFs),with per-task randomized dynamics.
Figure 8: Our vision-based Phase I architecture. This is identical to the architecture in Figure 3, except thatthere are no trial embeddings or trial rewards to concatenate in the conditioning network.
Figure 9: For “BC+SAC”, we pre-train agents with behavior cloning and use RL to fine-tune on each meta-test task. By comparison, WTL uses a demo and a single trial episode per task (< 50 environment steps).
