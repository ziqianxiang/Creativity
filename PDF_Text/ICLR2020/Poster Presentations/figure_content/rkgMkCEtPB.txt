Figure 1: Rapid learning and feature reuse paradigms. In Rapid Learning, outer loop training leads to aparameter setting that is well-conditioned for fast learning, and inner loop updates result in significant taskspecialization. In Feature Reuse, the outer loop leads to parameter values corresponding to reusable features,from which the parameters do not move significantly in the inner loop.
Figure 2: High CCA/CKA similarity between representations before and after adaptation for all layersexcept the head. We compute CCA/CKA similarity between the representation of a layer before the inner loopadaptation and after adaptation. We observe that for all layers except the head, the CCA/CKA similarity isalmost 1, indicating perfect similarity. This suggests that these layers do not change much during adaptation, butmostly perform feature reuse. Note that there is a slight dip in similarity in the higher conv layers (e.g. conv3,conv4); this is likely because the slight representational differences in conv1, conv2 have a compounding effecton the representations of conv3, conv4. The head of the network must change significantly during adaptation,and this is reflected in the much lower CCA/CKA similarity.
Figure 3: Inner loop updates have little effect on learned representations from early on in learning. Leftpane: we freeze contiguous blocks of layers (no adaptation at test time), on MiniImageNet-5way-5shot andsee almost identical performance. Right pane: representations of all layers except the head are highly similarpre/post adaptation - i.e. features are being reused. This is true from early (iteration 10000) in training.
Figure 4: Schematic of MAML and ANIL algorithms. The difference between the MAML and ANILalgorithms: in MAML (left), the inner loop (task-specific) gradient updates are applied to all parameters θ,which are initialized with the meta-initialization from the outer loop. In ANIL (right), only the parameterscorresponding to the network head θhead are updated by the inner loop, during training and testing.
Figure 5: MAML and ANIL learn very similarly. Loss and accuracy curves for MAML and ANILon MiniImageNet-5way-5shot, illustrating how MAML and ANIL behave similarly through thetraining process.
Figure 6: Euclidean distance before and after finetuning for MiniImageNet. We compute the average(across tasks) Euclidean distance between the weights before and after inner loop adaptation, separately fordifferent layers. We observe that all layers except for the final layer show very little difference before and afterinner loop adaptation, suggesting significant feature reuse.
Figure 7: Computing CCA similarity pre/post adaptation across different random seeds further demon-strates that the inner loop doesn’t change representations significantly. We compute CCA similarity of L1from seed 1 and L2 from seed 2, varying whether we take the representation pre (before) adaptation or post(after) adaptation. To isolate the effect of adaptation from inherent variation in the network representation acrossseeds, we plot CCA similarity of of the representations before adaptation against representations after adaptationin three different combinations: (i) (L1 pre, L2 pre) against (L1 pre, L1 post), (ii) (L1 pre, L2 pre) against (L1pre, L1 post) (iii) (L1 pre, L2 pre) against (L1 post, L2 post). We do this separately across different randomseeds and different layers. Then, we compute a line of best fit, finding that in all three plots, it is almost identicalto y = x, demonstrating that the representation does not change significantly pre/post adaptation. Furthermore acomputation of the coefficient of determination R2 gives R2 ≈ 1, illustrating that the data is well explained bythis relation. In Figure 8, we perform this comparison with CKA, observing the same high level conclusions.
Figure 8: We perform the same comparison as in Figure 7, but with CKA instead. There is more variation in thesimilarity scores, but we still see a strong correlation between (Pre, Pre) and (Post, Post) comparisons, showingthat representations do not change significantly over the inner loop.
Figure 9: Inner loop updates have little effect on learned representations from early on in learning.
Figure 10: ANIL and MAML on MiniImageNet and Omniglot. Loss and accuracy curves for ANIL andMAML on (i) MiniImageNet-5way-1shot (ii) MiniImageNet-5way-5shot (iii) Omniglot-20way-1shot. Theseillustrate how both algorithms learn very similarly over training.
Figure 11: Computing CCA similarity across different seeds of MAML and ANIL networks suggeststhese representations are similar. We plot the CCA similarity between an ANIL seed and a MAML seed,plotted against (i) the MAML seed compared to a different MAML seed (ii) the ANIL seed compared to adifferent ANIL seed. We observe a strong correlation of similarity scores in both (i) and (ii). This tells usthat (i) two MAML representations vary about as much as MAML and ANIL representations (ii) two ANILrepresentations vary about as much as MAML and ANIL representations. In particular, this suggests that MAMLand ANIL learn similar features, despite having significant algorithmic differences.
