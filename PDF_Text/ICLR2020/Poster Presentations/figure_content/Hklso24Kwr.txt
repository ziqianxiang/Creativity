Figure 1: Average test classification accuracy vs. the number of observed tasks in 6 experiments.
Figure 2: Evaluating catastrophic forgetting by measuring performance retention. Classificationaccuracy of the initial task is monitored along with the progression of tasks. Results are displayed forfive datasets. CLAW is the least forgetful algorithm since performance levels achieved on the initialtask do not degrade as much as in the other methods after facing new tasks. The legend and Î» valuesfor EWC are the same as in Figure 1. Best viewed in colour.
Figure 3: Evaluating Forward transfer, or to what extent a continual learning framework can avoidnegative transfer. The impact of learning previous tasks on a specific task (the last task) is inspectedand used as a proxy for evaluating forward transfer. This is performed by evaluating the relativeperformance achieved on a unique task after learning a varying number of previous tasks. This meansthat the value at x-axis = 1 refers to the learning accuracy of the last task after having learnt solelyone task (only itself), the value at 2 refers to the learning accuracy of the last task after having learnttwo tasks (an additional previous task), etc. Overall, CLAW achieves state-of-the-art results in 4 out ofthe 5 experiments (at par in the fifth) in terms of avoiding negative transfer. Best viewed in colour.
Figure 4: Ablation studies on different datasets.
