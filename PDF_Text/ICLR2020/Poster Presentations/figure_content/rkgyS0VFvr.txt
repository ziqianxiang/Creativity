Figure 1: Overview of centralized and distributed backdoor attacks (DBA) on FL. The aggregator atround t + 1 combines information from local parties (benign and adversarial) in the previous round t,and update the shared model Gt+1. When implementing backdoor attacks, centralized attacker uses aglobal trigger while distributed attacker uses a local trigger which is part of the global one.
Figure 2: Trigger factors (size, gap and location) in back-doored images.
Figure 3: Trigger factor (feature im-portance ranking) in tabular data.
Figure 4: Attack A-M and A-S. DBA is more effective and persistent than centralized attack.
Figure 5: Attack effectiveness comparison on two robust RL methods: RFA and FoolsGoldDistributed Attack against Mitigating Sybils Defence. FoolsGold reduces aggregation weights ofparticipating parties that repeatedly contribute similar gradient updates while retaining the weightsof parities that provide different gradient updates (Fung et al., 2018). Fig.5 shows that DBA alsooutperforms centralized attack under FoolsGold. In three image datasets, the attack success rateof DBA is notably higher while converging faster. DBA in MNIST reaches 91.55% in round 30when centralized attack fails with only 2.91% attack success rate. For LOAN, which are trained witha simple network, FoolsGolds cannot distinguish the difference between the malicious and cleanupdates and assigns high aggregation weights for attackers, leading to a fast backdoor success. Toexplain the effectiveness of DBA, we report FoolsGold’s weights on adversarial parties in Tb.2 inAppendix. Comparing to centralized attack, although FoolsGold assigns smaller aggregation weightsto DBA attacker due to their similarity of backdoor target label, DBA is still more successful. This isbecause the sum of weights of distributed attackers could be larger than centralized attacker.
Figure 6: Decision visualization of poisoned	Figure 7: Feature importance of LOANdigit 4 with target 2 on a DBA poisoned model learned from its soft decision treeUsing the soft decision tree of MNIST as another example, we find that the trigger area after poisoningindeed becomes much more significant for decision making in the corresponding soft decision tree,as shown in Fig.22 in Appendix.A.7. Similar conclusion is found in LOAN. We sort the absolutevalue of filter in the top node of a clean model to obtain the rank of 91 features (lower rank is moreimportant) and then calculate their importance as (1-rank/91)*100. Six insignificant features and sixsignificant features are separately chosen to run DBA. The results in Fig.7 show that based on thesoft decision tree, the insignificant features become highly important for prediction after poisoning.
Figure 8: Effects of Scale on Attack Success Rate and Model AccuracyS(a) LOAN	(b) MNIST	(c) CIFAR	(d) Tiny-imagenetFigure 9:	Effects of Trigger Location on Attack Success Rate and Model Accuracy4.2	Effects of Trigger LocationFor three images datasets, we move the global trigger pattern from the left upper corner to the center,then to the right lower corner. The dotted line in Fig.9 means that the trigger reaches the rightboundary and starts to move along the right edges. The implementation details are in Appendix.A.9.
Figure 9:	Effects of Trigger Location on Attack Success Rate and Model Accuracy4.2	Effects of Trigger LocationFor three images datasets, we move the global trigger pattern from the left upper corner to the center,then to the right lower corner. The dotted line in Fig.9 means that the trigger reaches the rightboundary and starts to move along the right edges. The implementation details are in Appendix.A.9.
Figure 10:	Effects of Trigger Gap on Attack Success Rate and Model Accuracydu<φsαSSgUnS w-us<(a) MNISTOooooo0 8 6 4 2du< /SSgUnS w-us<(b) CIFAROooooo0 8 6 4 2du< /SSgUnS w-us<(c) Tiny-imagenetFigure 11:	Effects of Local Trigger Size on Attack Success Rate and Model Accuracy4.5 Effects of Poison Interval•	The attack performance is poor when all distributed attackers submit the scaled updates at the sameround (I = 0) in all datasets because the scaling effect is too strong, vastly changing the parameter inthe global model and causes it to fail in main accuracy. It’s also ineffective if the poison interval istoo long because the early embemed triggers may be totally forgotten.
Figure 11:	Effects of Local Trigger Size on Attack Success Rate and Model Accuracy4.5 Effects of Poison Interval•	The attack performance is poor when all distributed attackers submit the scaled updates at the sameround (I = 0) in all datasets because the scaling effect is too strong, vastly changing the parameter inthe global model and causes it to fail in main accuracy. It’s also ineffective if the poison interval istoo long because the early embemed triggers may be totally forgotten.
Figure 12: Effects of Poison Round Interval on Attack Success Rate and Model AccuracyDBA-ASRDBA^ASRrllMaln-AccMaIrMcc-Il16 24 32 40 48Poison Per Batch(a) LOANOooooo0 8 6 4 2d⅛ 一seκSsguns Jpeκ‹(b) MNISTd⅛ 一seκSsguns Jpeκ‹(c) CIFAR————————————W 8 6 4 2d⅛ 一seκSsguns Jpeκ‹(d) Tiny-imagenetFigure 13: Effects of Poison Ratio on Attack Success Rate and Model Accuracy9
Figure 13: Effects of Poison Ratio on Attack Success Rate and Model Accuracy9Published as a conference paper at ICLR 20204.6	Effects of Poison RatioIn our experiments, the training batch size is 64. As the X-axis variable (# of poisoned samples)in Fig.13 increases from 1, DBA-ASR and DBA-ASR-t first increase and then drop. It’s intuitivethat more poisoned data can lead to a better backdoor performance. However, a too large poisonratio means that the attacker scales up the weight of a local model of low accuracy, which leadsto the failure of global model in the main task. In the case of poisoning full batch, after DBA, theglobal model in CIFAR and Tiny-imagenet trains the main task all over again, whose main accuracyis normal after 90 and 40 rounds, respectively. But in MNIST it is reduced to an overfitted modelthat predicts the target label for any input, so the attack success rate is always 100% while the mainaccuracy is about 10% in the subsequent rounds. Therefore, it’s better for DBA to remain stealthy inits local training by using a reasonable poison ratio that also maintains accuracy on clean data.
Figure 14: Examples of irregular shape triggers in image datasets225 250 275 300 325 350 375 400Rounds(b) CIFAR20 30 40 50 60 70Rounds(a) MNISTTesting poison typeEP"w ------ Local Trigger 1Local Trigger 2Local Trigger 3Local Trigger 4----Global TriggerAttaCktyPe30 40 50 60 70 80 90 100 . LRounds	…centrahzed(c) Tiny-imagenetFigure 15: Attack A-M for irregular pixel logo ‘ICLR’as DBA, but each update includes 1/f number of poisoning samples, so that the total number ofpoisoning samples included to compute the gradient update still stay the same. There are two ways to
Figure 15: Attack A-M for irregular pixel logo ‘ICLR’as DBA, but each update includes 1/f number of poisoning samples, so that the total number ofpoisoning samples included to compute the gradient update still stay the same. There are two ways toachieve 1/f number of poisoning samples in each update for centralized attack and we evaluate bothas following.
Figure 16: Attack A-M for white glasses pattern Figure 17: Attack A-M for white, black, purpleon Tiny-imagenet	glasses patterns on Tiny-imagenet14Published as a conference paper at ICLR 2020I-…….∙Ir1008060402003⅛& SSgUnS w-orat<se≈SSaUUnS xuet<1∞80s40MOcs≈SSaUUnS xuet<225 250 275 300 325 350 375Rounds(C)CIFAR罂≈IOT80504020-0-
Figure 18:	Scale f times with 1/f poison ratio each time(in Tb.1), the more obvious the decline in the main accuracy as the scale factor increases, because thescaling undermines more model parameters”, the setting of f times scaling for centralized attack haslarger impact on complex neural network like Resnet used in CIFAR and Tiny-imagenet. However,we note that this setting is not a totally fair comparison of the single-shot attack setting, as the samemalicious agent of the centralized attack is allowed to attack f times, while each malicious agent ofDBA only attacks once.
Figure 19:	Scale f times with 1/f data size each timeA.5 More Details about robust aggregationWe report RFA distance and FoolsGold weights on adversarial parties in Tb. 2.
Figure 20:	Multi-Krum	Figure 21: BulyanIn summary, Multi-Krum and Bulyan have stricter assumptions on the proportion of attackers thanRFA and FoolsGold. In addition, while RFA and FoolsGold still assign potential outliers withextreme low weights, Krum (Multi-Krum, Krum-based Bulyan) directly removes them, making itimpossible to inject backdoors if the malicious updates are obviously far from the benign updates.
Figure 22: Soft decision tree for MNIST(a) based on W(b) based on W*X for clean data(c) based on W*X for poisoned dataFigure 23: Feature importance in soft decision tree for LOAN(a) predict 2 (target label) for poisoned digit 1	(b) predict 1 for clean digit 1Figure 24: Examples for the poisoned MNIST soft decision tree(a) predict 1 for poisoned digit 1(b) predict 1 for clean digit 1Figure 25: Examples for the clean MNIST soft decision tree17Published as a conference paper at ICLR 2020A.8 More Grad-cam results on MNISTWe test the global model of MNIST poisoned by DBA under Attack A-M in round 16 of Fig.4 withlocal backdoored images and global backdoored images. More Grad-cam results are provided inFig.26 and Fig.27.
Figure 23: Feature importance in soft decision tree for LOAN(a) predict 2 (target label) for poisoned digit 1	(b) predict 1 for clean digit 1Figure 24: Examples for the poisoned MNIST soft decision tree(a) predict 1 for poisoned digit 1(b) predict 1 for clean digit 1Figure 25: Examples for the clean MNIST soft decision tree17Published as a conference paper at ICLR 2020A.8 More Grad-cam results on MNISTWe test the global model of MNIST poisoned by DBA under Attack A-M in round 16 of Fig.4 withlocal backdoored images and global backdoored images. More Grad-cam results are provided inFig.26 and Fig.27.
Figure 24: Examples for the poisoned MNIST soft decision tree(a) predict 1 for poisoned digit 1(b) predict 1 for clean digit 1Figure 25: Examples for the clean MNIST soft decision tree17Published as a conference paper at ICLR 2020A.8 More Grad-cam results on MNISTWe test the global model of MNIST poisoned by DBA under Attack A-M in round 16 of Fig.4 withlocal backdoored images and global backdoored images. More Grad-cam results are provided inFig.26 and Fig.27.
Figure 25: Examples for the clean MNIST soft decision tree17Published as a conference paper at ICLR 2020A.8 More Grad-cam results on MNISTWe test the global model of MNIST poisoned by DBA under Attack A-M in round 16 of Fig.4 withlocal backdoored images and global backdoored images. More Grad-cam results are provided inFig.26 and Fig.27.
Figure 28: Effects of α in Dirichlet data distribution on Attack Success RateA.11 More details about Loan DatasetsThe lable distribution is uneven in LOAN, which is shown in Tb.3. The five most important featuresamong the 91 features in LOAN under various classification methods are shown in Tb.4 and the resultis consistent.
