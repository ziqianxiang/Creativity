Figure 1: Non-acceleration of NesterovSGD and fast convergence of MaSS.
Figure 2: Convergence speed per it-eration s(m). Larger s(m) indicatesfaster convergence per iteration.
Figure 3: Convergence speed per itera-tion s(m). Larger s(m) indicates fasterconvergence. Red solid curve: experi-mental results. Blue dash curve: theo-retical lower bound Kmmmm- Criticalmini-batch sizes: m1* ≈ 10, m2* ≈ 50.
Figure 4: Comparison on com-ponent decoupled data. Hyper-parameters: We use optimal param-eters for SGD+Nesterov, SGD+HBand SGD; the setting in (8) forASGD; Eq. 13 for MaSS.
Figure 5: Comparison of SGD, SGD+Nesterov and MaSS on (left) fully-connected neural netWork,(middle) convolutional neural netWork, and (right) kernel regression.
Figure 7: Fast convergence of MaSS and non-acceleration of SGD+Nesterov on 3-d Gaussian data.
Figure 6: Fast convergence of MaSS and non-acceleration of SGD+Nesterov on component decou-pled data. (left) σ12 = 1 and σ22 = 1/212; (right) σ12 = 1 and σ22 = 1/215.
Figure 8: Divergence of SGD+Nesterov With large step size. Step size: η* = 1/L1 = 1/6, andmomentum parameter: γ is 0.9 or 0.99.
Figure 9: Convergence speed per iteration as afunction of mini-batch size m. Red solid curve:experimental results. Larger s(m) indicates fasterconvergence. Blue dash curve: theoretical lowerbound √KmKm. Critical mini-batch size values:mi； ≈ 10, m；2 ≈ 50.
