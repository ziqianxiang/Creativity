Figure 1: Diagrams of the three model architectures we consider. (a) The Bi-encoder encodesthe context and candidate separately, allowing for the caching of candidate representations duringinference. (b) The Cross-encoder jointly encodes the context and candidate in a single transformer,yielding richer interactions between context and candidate at the cost of slower computation. (c)The Poly-encoder combines the strengths of the Bi-encoder and Cross-encoder by both allowing forcaching of candidate representations and adding a final attention mechanism between global featuresof the input and a given candidate to give richer interactions before computing a final score.
Figure 2: (a) The Bi-encoder (b) The Cross-encoder (c) The Poly-encoder with first m vectors. (d)The Poly-encoder with m learnt codes.
