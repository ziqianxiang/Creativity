Figure 1: Example of observational overfitting in Sonic from Gym Retro (Nichol et al., 2018).
Figure 2: (a) Visual Analogy of the Observation Function. (b) Our combinations for 1-D (top) and2-D (bottom) images for synthetic tasks.
Figure 3: (Left) We show that the generalization gap vs noise dimension is tight as the noise dimensionincreases, showing that this bound is accurate. (Middle and Right) LQR Generalization Gap vsNumber of Intermediate Layers. We plotted different Φ = Pj=0 kAkkL terms without exponents, aspowers of those terms are monotonic transforms since kkAk^ ≥ 1 ∀A and ∣∣A∣∣* = IlAkF , ∣∣A∣∣1∙ Wesee that the naive spectral bound diverges at 2 layers, and the weight-counting sums are too loose.
Figure 4: Each Mujoco task is given 10 training levels (randomly sampling gθ parameters). We useda 2-layer ReLU policy, with 128 hidden units each. Dimensions of outputs of (f, g) were (30, 100)respectively.
Figure 5: Effects of Depth.
Figure 6: Effects of Width.
Figure 7: Performance of architectures in the synthetic Gym-Deconv dataset. To cleanly depict testperformance, training curves are replaced with horizontal (max env. reward) and vertical black lines(avg. timestep when all networks reach max reward).
Figure 8: We only show the observation from gθ (s), which tests memorization capacity onSwimmer-v2.
Figure 9: Overparametrization improves generalization for CoinRun.
Figure A1: (a,b): Singular Values for varying depths and widths. (c,d): Train and Test Loss forvarying widths and depths. (e): Train and Test Loss for varying Noise Dimensions.
Figure A2: Explicit Regularization on layer norms.
Figure A3: Deconvolution memorization test using LQR as underlying MDP.
Figure A4: Large Architecture Training/Testing Curves (Smoothed).
Figure A5: Margin Distributions across training.
Figure A6: Margin Distributions at the end of training.
Figure A7: Plot of E[C(K∞)] as a function of m,p, n with elsewhere fixed default values n = 10,p = 1000, m = 10, and ψ = 1.
