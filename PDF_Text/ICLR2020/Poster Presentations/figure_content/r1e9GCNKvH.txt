Figure 1: Plot of Remaining Connections by Gate and Type. SNiP and GraSP consistently prunerecurrent connections at a much higher ratio than input connections. The ratio of remaining input torecurrent (I/R) connections is given for each method; the dense ratio is 0.07 for comparison. SNiPand GraSP also exhibit severe imbalance between gates, while our imbalance is far milder.
Figure 2: Singular Value Magnitude Histograms after 50 epochs of Training, for 400 Unit GRUon seq. MNIST. Compared to SNiP, our method prevents spectral concentration at 0, with a meansingular value magnitude of 0.31 to SNiPâ€™s 0.18 This helps to explain our relative performance gain.
Figure 3: Map of remaining connections, with the x-axis indicating the output size (flattened acrossgates) and the y-axis indicated the input size.Our method is significantly more spread out acrossneurons and gates than the others.
Figure 4: Plot of Log Train Loss for a 400 Unit GRU, trained on Sequential MNIST. GraSP is theworst performing, followed by SNiP and then Random, which is on par with our method. L2 is shownas a lower bound. It is surprising that random is competitive, but it is free from the gate imbalanceexhibited by SNiP and GraSP.
Figure 5: Plot of Log Train Perplexity on the 1b dataset, with 2k LSTM network. Our model clearlyoutperforms random pruning by a significant margin, however more work is needed before we achievenear-dense performance.
