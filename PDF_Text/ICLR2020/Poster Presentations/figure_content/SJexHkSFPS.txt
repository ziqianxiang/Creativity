Figure 1: Shaded nodes represent observed variables and unshaded nodes represent unobservedrandom variables. (a): In “blocking” MDPs, the environment state does not change while the agentrecords the current state and selects an action. (b): In “concurrent” MDPs, state and action dynamicsare continuous-time stochastic processes s(t) and ai (t). At time t, the agent observes the state ofthe world s(t), but by the time it selects an action ai(t + tAS), the previous continuous-time actionfunction ai-1 (t - H + tAS00) has “rolled over” to an unobserved state s(t + tAS). An agent thatconcurrently selects actions from old states while in motion may need to interrupt a previous actionbefore it has finished executing its current trajectory.
Figure 2: In concurrent versions of Cartpole and Pendulum, we observe that providing the criticwith VTG leads to more robust performance across all hyperparameters. (a) Environment rewardsachieved by DQN with different network architectures [either a feedforward network (FNN) or aLong Short-Term Memory (LSTM) network] and different concurrent knowledge features [Uncon-ditioned, Vector-to-go (VTG), or previous action and tAS] on the concurrent Cartpole task for ev-ery hyperparameter in a sweep, sorted in decreasing order. (b) Environment rewards achieved byDQN with a FNN and different frame-stacking and concurrent knowledge parameters on the con-current Pendulum task for every hyperparameter in a sweep, sorted in decreasing order. Largerarea-under-curve implies more robustness to hyperparameter choices. Enlarged figures provided inAppendix A.5.
Figure 3: An overview of the robotic grasping task. A static manipulator arm attempts to graspobjects placed in bins front of it. In simulation, the objects are procedurally generated.
Figure 4: The execution order of different stages are shown relative to the sampling period H aswell as the latency tAS . (a): In “blocking” environments, state capture and policy inference areassumed to be instantaneous. (b): In “concurrent” environments, state capture and policy inferenceare assumed to proceed concurrently to action execution.
Figure 5: Concurrent knowledge representations can be visualized through an example of a 2-Dpointmass discrete-time toy task. Vector-to-go represents the remaining action that may be executedwhen the current state st is observed. Previous action represents the full commanded action fromthe previous timestep.
Figure 6: Environment rewards achieved by DQN with different network architectures [either afeedforward network (FNN) or a Long Short-Term Memory (LSTM) network] and different con-current knowledge features [Unconditioned, vector-to-go (VTG), or previous action and tAS] on theconcurrent Cartpole task for every hyperparameter in a sweep, sorted in decreasing order. Providingthe critic with VTG information leads to more robust performance across all hyperparameters. Thisfigure is a larger version of 2a.
Figure 7: Environment rewards achieved by DQN with a FNN and different frame-stacking andconcurrent knowledge parameters on the concurrent Pendulum task for every hyperparameter in asweep, sorted in decreasing order.
