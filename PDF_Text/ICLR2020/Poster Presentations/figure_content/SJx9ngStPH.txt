Figure 1: Overview of the NAS-Bench-1Shot1 analysis strategy. The one-shot model we constructonly contains discrete architectures that are elements of NAS-Bench-101 (Ying et al., 2019). Thecell architecture chosen is similar to that of Bender et al. (2018), with each choice block containingan operation decision. Note that NAS-Bench-101 does not contain a separate reduction cell type.
Figure 2:	Comparison of different one-shot NAS optimizers on the three different search spacesdefined on NASBench. The solid lines show the anytime test regret (mean ± std), while the dashedblurred lines the one-shot validation error (Best viewed in color).
Figure 3:	Correlation between the one-shot validation error and the corresponding NAS-Bench-101test error for each search space. (Best viewed in color).
Figure 4: Illustration of the impact that Cutout has on the test regret on NAS-Bench-101 and thevalidation error of the one-shot model using DARTS, GDAS and PC-DARTS on search space 3(Best viewed in color).
Figure 5: Optimizing the hyperparameters of one-shotoptimizers with BOHB on search space 3. (best viewedin color). Results for search space 1 and 2 are shownin Figure 16.
Figure 6: Distribution of test error in the search spaces with loose ends.
Figure 7: Comparison of different One-Shot Neural Architecture optimizers on the three differentsearch spaces defined on NAS-Bench-101 over 100 epochs.
Figure 8:	Comparison of DARTS first and second order on the three different search spaces definedon NAS-Bench-101 for 25 epochs.
Figure 9:	Comparison of the effect of using Cutout during architecture search on GDAS for searchspace 1 and 2.
Figure 10:	Comparison of the effect of using cutout during architecture search on PC-DARTS forsearch space 1 and 2.
Figure 11:	Illustration of the impact that weight decay has on the test regret on NAS-Bench-101 andthe validation error of the one-shot model using DARTS, GDAS and PC-DARTS on search space 3(Best viewed in color).
Figure 12:	DARTS first order w/o cutout trained with different levels of L2 regularization for searchspace 1 and 2.
Figure 13:	Comparison of the effect of using different values of weight decay during architecturesearch on GDAS for search space 1 and 2.
Figure 14:	Comparison of the effect of using different values of weight decay during architecturesearch on PC-DARTS for search space 1 and 2.
Figure 15: Test regret of architectures found with DARTS (1st order) configurations sampled byBOHB on CS1. All the lines except the BOHB-DARTS one show the mean±std of the best archi-tecture from 500 search repetitions (Best Viewed in color).
Figure 16: Test regret of architectures found with DARTS, GDAS and PC-DARTS (1st order) con-figurations sampled by BOHB on CS2. All the lines except BOHB-DARTS, BOHB-GDAS andBOHB-PC-DARTS show the mean±std of the best architecture from 500 search repetitions (Bestviewed in color).
Figure 17: Analogous to Figure 15, with the only difference being that here we optimize on CS3.
Figure 18: Test regret of architectures found with DARTS, GDAS and PC-DARTS (2nd order)configurations sampled by BOHB on CS2.
Figure 19:	Parameter importance for two hyperparameters, Cutout (CO) and L2 regularization (CS2)across different training epochs and search spaces (SS).
Figure 20:	In this experiment we varied the total number of cells within [3, 6, 9] and the number ofinitial channels of the proxy model within [2, 4, 8, 16, 36].
