Figure 1: Final test and train error for different numbers of asynchronous workers N . The figureshows the average (bold line) and standard deviation (band) of 5 runs on the CIFAR10 dataset usingthe WideResNet model. The black dashed line is the SGD error using a single worker.
Figure 2: Final test error for different numbers of asynchronous workers N . Each line in the figurerepresents the average (bold line) and standard deviation (band) of 5 runs on a specific framework.
Figure 3: Gamma-distribution in homogeneous and heterogeneous environments. The x-axis is thesimulated time units the iteration takes while the y-axis is the probability. Both environments havethe same mean (128 time units). The red area represents the probability to have an iteration whichtakes more than 1.25x longer than the mean iteration time.
Figure 4: Delay and Gap throughout the training process for different number of workers using GAand DANA-GA. The figure shows the average (bold line) and standard deviation (band) of 5 runsfor N ∈ [4, 8, 16, 24, 32]. All sub-figures are equally scaled to easily compare between them. GApenalizes the stale gradient much less than SA. DANA-GA penalizes the stale gradients even lessthan GA thanks to its approximation.
Figure 5: Final test and train error for different numbers of workers N . The figure shows the average(bold line) and standard deviation (band) of 5 runs. The black dashed line is the SGD error using asingle worker.
Figure 6: Final train error for different numbers of workers N . The figure shows the average (boldline) and standard deviation (band) of 5 runs on different frameworks.
Figure 7: Test error throughout the training using 16 workers. The figure shows the average (boldline) and standard deviation (band) of 5 runs on different frameworks.
Figure 8: Test error throughout the training using 64 workers on ImageNet. DANA-GA remains closeto the baseline. GA surpasses SA and DANA-SA.
Figure 9: Test Accuracy of ASGD using 32 asynchronous workers on CIFAR10 ResNet-20 usingdifferent learning rate and momentum coefficients. The best accuracy achieved is 88% (η = 0.03, Y =0.5). The tuning includes negative values of momentumEnlUeEOH0.95-	86.8	89.93	140.84 10.0 10.0	10.0	-900.9-	84.0	88.54	90.92 88.88	10.0	L50.8-	80.43	86.3	89,5 91.42	10.0	0.5-	71.69	82.04	87.64 90.03 90.861	10.0	L-60 1-450.25^ 0.0-	66.53 62.23	78.99 76.23	86.02 89.92 90.77 86.47 84.63 89.12 91.07 89.67	10.0 10.0	0.25 J	59.39	73.81	83.55 88.12 91.04 90.28	37.46	I-0.5- -0.8-	56.74 54.25	71.76 69.14	82.62 87.47 91.02 91.55 80.96 87.08 90.0 91.57	50.47 57.0	.30-0.9-	53.45	68.41	81.23 86.94 90.16 90.69	10.0	1115	0.001	0.003	0.01 0.03 OJ θ'ɜ Learning Rate	1.0	Figure 10: Test Accuracy of ASGD using 32 asynchronous workers on CIFAR10 WideResNet 16-4using different learning rate and momentum coefficients. The best accuracy achieved is 91.57%(η = 0.3, Y = -0.8). The tuning includes negative values of momentumC.13 Gradient Staleness NoiseWe notice that in the ImageNet experiments (Table 3) NAG-ASGD remains relatively close to thebaseline even when the number of workers is large, as opposed to the CIFAR experiments, in whichNAG-ASGD severely deteriorates as N scales up. This phenomenon suggests that there is some
Figure 10: Test Accuracy of ASGD using 32 asynchronous workers on CIFAR10 WideResNet 16-4using different learning rate and momentum coefficients. The best accuracy achieved is 91.57%(η = 0.3, Y = -0.8). The tuning includes negative values of momentumC.13 Gradient Staleness NoiseWe notice that in the ImageNet experiments (Table 3) NAG-ASGD remains relatively close to thebaseline even when the number of workers is large, as opposed to the CIFAR experiments, in whichNAG-ASGD severely deteriorates as N scales up. This phenomenon suggests that there is some28Published as a conference paper at ICLR 2020EnlUeEOH0.95-0.9-0.8-0.5-0.25-0.0--0.25--0.5--0.8--0.9-
Figure 11: Test Accuracy of ASGD using 32 asynchronous workers on CIFAR10 WideResNet 16-4using different learning rate and momentum coefficients. The best accuracy achieved is 71.07%(η = 0.3, Y = -0.5). The tuning includes negative values of momentumgradient StaIeneSS noise that a framework can “tolerate” and still perform well. Following thisintuition, it is reasonable that some gradient staleness should be allowed to go ”un-penalized” toavoid limiting the step-size needlessly. This idea explains why SA and GA demonstrated relativelypoor results in ImageNet, especially when the number of workers was relatively small. Though weconsider this analysis beyond the scope of this work, it is relevant for this paper to note that wethink that the tolerable gradient staleness noise depends on the size of the model and dataset, whichsuggests that GA can be further improved by correctly analysing the tolerable gradient stalenessnoise and starting the penalization accordingly. We plan to continue our research in this path as well.
Figure 12: Theoretical speedups for any ASGD (such as GA, SA or DANA variants) and SSGDalgorithms when batch execution times are drawn from a gamma distribution. Each line is an averageof 20 runs with 100000 iterations per run. Communication overheads are not modeled; however,asynchronous algorithms are more communication efficient. Accounting for the communicationoverheads should expand the gap between the asynchronous and synchronous training.
Figure 13: Final test error for different numbers of heterogeneous workers N . The figure showsthe average (bold line) and standard deviation (band) of 5 runs on different frameworks. The blackdashed line represents the average result of SGD using a single worker.
