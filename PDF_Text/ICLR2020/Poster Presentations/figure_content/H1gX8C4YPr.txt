Figure 1: Left: In PointGoal Navigation, an agent must navigate from a random starting location(blue) to a target location (red) specified relative to the agent (“Go 5m north, 10m east of you”) in apreviously unseen environment without access to a map. Right: Performance (SPL; higher is better)of an agent equipped with RGB-D and GPS+Compass sensors on the Habitat Challenge 2019 (Savvaet al., 2019) train & val sets. Using DD-PPO, we train agents for over 180 days of GPU-time inunder 3 days of wall-clock time with 64 GPUs, achieving state-of-art results and ‘solving’ the task.
Figure 2: Comparison of asynchronous distribution (left) and synchronous distribution via dis-tributed data parallelism (right) for RL. Left: rollout workers collect experience and asynchronouslysend it to the parameter-server. Right: a worker alternates between collecting experience, synchro-nizing gradients, and optimization. We find this highly effective in resource-intensive environments.
Figure 3: Our agent for PointGoalNav. At very time-step, the agent receives an egocentric Depthor RGB (shown here) observation, utilizes its GPS+Compass sensor to update the target position tobe relative to its current position, and outputs the next action and an estimate of the value function.
Figure 4: Scaling performance (in steps of experience per second relative to 1 GPU) of DD-PPO forvarious preemption threshold, p%, values. Shading represents a 95% confidence interval.
Figure 5: Performance (higher is better) on Flee (left) and Exploration (right) under five settings.
Figure 6: Example episodes broken down by geodesic distance between agent’s spawn location andtarget (on rows) vs SPL achieved by the agent (on cols). Gray represents navigable regions on themap while white is non-navigable. The agent begins at the blue square and navigates to the redsquare. The green line shows the shortest path on the map (or oracle navigation). The blue lineshows the agent’s trajectory. The color of the agent’s trajectory changes changes from dark to lightover time. Navigation dataset from the longer validation episodes proposed in Chaplot et al. (2019).
Figure 7: Histogram of SPL fornon-perfect (SPL<0.99) episodes.
Figure 8: Scaling of DD-PPO under homogeneous and heterogeneous workloads for various differ-ent values of the percentage of rollouts that are fully completed by optimizing the model. Shadingrepresents a bootstrapped 95% confidence interval.
Figure 9: Implementation of DD-PPO using PyTorch (Paszke et al., 2017) v1.1 and the NCCLbackend. We use SLURM to populate the world_rank, world_size, and local_rank fields.
Figure 10: Illustration of DD-PPO. Processes collecting experience in environments that are morecostly to simulate (stragglers) have their experience collection stage preempted such that other pro-cesses do not have to wait for them. Note that we implement the monitor with a simple key-valuestorage and have processes preempt themselves. Note that the order of processes is irrelevant anddone solely for aesthetic purposes.
Figure 11: Examples of Gibson meshes for a given quality rating from Savva et al. (2019)19Published as a conference paper at ICLR 2020Figure 12: Training and validation performance (in SPL; higher is better) of different architecturesfor Depth agents with GPS+Compass on the Habitat Challenge 2019 (Savva et al., 2019). Gib-son (Xia et al., 2018)-4+ refers to the subset of Gibson train scenes with a quality rating of 4 orbetter. Gibson-4+ and MP3D refers to training on both Gibson-4+ and all of Matterport3D. Gibson-2+ refers to training on the subset of Gibson train scenes with a quality rating of 2 or better.
Figure 12: Training and validation performance (in SPL; higher is better) of different architecturesfor Depth agents with GPS+Compass on the Habitat Challenge 2019 (Savva et al., 2019). Gib-son (Xia et al., 2018)-4+ refers to the subset of Gibson train scenes with a quality rating of 4 orbetter. Gibson-4+ and MP3D refers to training on both Gibson-4+ and all of Matterport3D. Gibson-2+ refers to training on the subset of Gibson train scenes with a quality rating of 2 or better.
Figure 13: Performance vs. Geodesic Distance from start to goal for Blind, RGB, and RGB-D (us-ing Depth only) models trained with DD-PPO on the Habitat Challenge 2019 (Savva et al., 2019)validation split. Bars at the bottom represent the fraction of episodes within each geodesic distancebin.
Figure 14: Performance vs. Geodesic Distance from start to goal for Blind, RGB, and RGB-D (usingDepth only) models trained with DD-PPO on the longer and harder validation episodes proposedin Chaplot et al. (2019). Bars at the bottom represent the fraction of episodes within each geodesicdistance bin.
