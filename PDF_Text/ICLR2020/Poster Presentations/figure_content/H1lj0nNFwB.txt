Figure 1: Incremental learning dynamics in deep models. Each panel shows the evolution of the fivelargest values of σ, the parameters of the induced model. All models were trained using gradientdescent with a small initialization and learning rate, on a small training set such that there are mul-tiple possible solutions. In all cases, the deep parameterization of the models lead to “incrementallearning”, where the values are learned at different rates (larger values are learned first), leadingto sparse solutions. (a) Depth 4 matrix sensing, σ denotes singular values (see section 4.1). (b)Quadratic networks, σ denotes singular values (see section 4.2). (c) Depth 3 diagonal networks, σdenotes feature weights (see section 4.3). (d) Depth 3 circular-convolutional networks, σ denotesamplitudes in the frequency domain of the feature weights (see appendix G).
Figure 2: Incremental learning dynamics in the toy model. Each panel shows the evolution of σσ(t)for σ* ∈ {12, 6,4,3} according to the analytical solutions in theorem 1, under different depthsand initializations. The first column has all values converging at the same rate. Notice how thedeep parameterization with small initialization leads to distinct phases of learning, where values arelearned incrementally (bottom-right). The shallow model’s much weaker incremental learning, evenat small initialization scales (second column), is explained in theorem 2.
Figure 3: Empirical comparison of the dynamics of the toy model to OMP. The toy model has adepth of 5 and was initialized with a scale of 1e-4 and a learning rate of 3e-3. We compare thefraction of agreement between the sets of first s features selected of the two algorithms for everygiven sparsity level s, averaged over 100 experiments (the shaded regions are empirical standarddeviations). For example, for sparsity level 3, we look at the sets of first 3 features selected by eachalgorithm and calculate the fraction of them that appear in both sets.
Figure 4: Evolution of the top-5 singular values of the deep matrix sensing model, with Gaussianinitialization with variance such that the initial singular values are in expectation 1e-4. The model’ssize and data are in R50×50. The columns correspond to different parameterization depths, while therows correspond to different dataset sizes. In both cases the problem is over-determined, since thenumber of examples is smaller than the number of parameters. Since the original matrix is rank-4,we can recognize an unsuccessful recovery when all five singular values are nonzero, as seen clearlyfor both depth-1 plots.
Figure 5: Quadratic model’s evolution of top-5 singular values for a rank-4 labeling function. Therows correspond to whether or not a global bias is introduced to the model. The first two columnsare for a large dataset (one optimal solution) and the last two columns are for a small dataset (over-determined problem). When a bias is introduced, it is initialized to it’s optimal value at initialization.
Figure 6: Incremental learning in binary classification. A model as in section 4.3 is trained over 200i.i.d. random Gaussian examples, where d = 100. The data is labeled by a weight vector with 4nonzero values, making the problem realizable with a sparse solution while the max-margin solutionisn’t sparse. The left panel describes the obtained solution’s correlation with the sparse labelingvector for different depths and initializations. The results are averaged over 100 experiments, withshaded regions denoting empirical standard deviations. We see that depth-1 models reach resultssimilar to the max-margin SVM solution as predicted by Gunasekar et al. (2018), while deepermodels are highly correlated with the sparse solution, with this correlation increasing when theinitialization scale is small. The other panels show the evolution of the absolute values of the top-5weights of σ for the smallest initialization scale. Note that as we increase the depth, incrementallearning is clearly presented.
Figure 7: Incremental learning in convolutional networks. A model as in appendix G is trained over200 i.i.d. random Gaussian examples, where d = 100. The weights are initialized randomly and thedata is labeled by a weight vector with 4 nonzero frequencies, making the problem realizable with asparse solution in the frequency domain. The left panel describes the obtained solution’s correlationin the frequency domain with the sparse labeling vector for different depths and initializations. Theresults are averaged over 9 experiments, with shaded regions denoting empirical standard deviations.
