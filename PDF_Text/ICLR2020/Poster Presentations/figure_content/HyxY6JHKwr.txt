Figure 1: Illustration of the proposed method. At training time (left), for each training data pointloss parameters λ are sampled from a distribution Pλ, and the model is conditioned on these. Then,at test time (right), the model can be conditioned on any desired loss parameters to achieve behaviorcorresponding to the loss function with these parameters.
Figure 2: Quantitative β-VAE results on CIFAR-10 (a,b), and Shapes3D (c,d) for models of varyingcapacity (width). In most cases, the proposed method performs close to models trained indepen-dently for each loss weight value. This is especially the case for high capacity models.
Figure 3: Qualitative β-VAE results on the Shapes3D dataset. For each loss weight β, the recon-struction and sampling results of YOTO are very similar to those of the separately trained models.
Figure 4: Loss averaged over all loss weight values as a function of the model capacity. We comparethe proposed method to a set of fixed-weight models on two datasets: CIFAR-10 (a) and Shapes3D(b). We vary the capacity of the network by multiplying the width of the network by a factor. Theperformance of the proposed method overall matches that of fixed-based models with a roughly1.5x wider model. Moreover, the performance of the proposed method gets closer to that of thefixed-weight models when the capacity of the model is increased.
Figure 5: Quantitative compression results on the Tecnick and Kodak datasets. A basic YOTO modelunder-performs compared to a set of fixed-weight models, but a wider network nearly matches theperformance of the fixed-weight models, especially in the high-quality regime.
Figure 7: Qualitative comparison of image stylization models on an image from the validation set ofImageNet. The first row shows the results of YOTO trained on all parameters, and the second rowshows the results of fixed-weight models trained independently for each loss parameters. In bothcases the parameter values increase from left to right. YOTO generates results very similar to theseparate models, while training just a single model.
Figure 6: Quantitative results on style transfer. In both cases YOTO performance approaches ormatches that of separate models trained per loss parameter vector.
Figure 8: Qualitative VAE results on the CIFAR-10 dataset. Both for reconstruction and sampling,the YOTO results are qualitatively very similar to those of separate models trained for each value ofthe loss weight.
Figure 9: Quantitative comparison of the proposed method with baselines for beta-VAE on CIFAR-10 (a,b), and Shapes3D (c,d). The proposed method outperforms the ”single model” baseline by alarge model. The ”interpolation” baseline is reported for completeness, but it is not a fair baseline,since it does not correspond to a VAE.
Figure 10: Quantitative results of compression with the MS-SSIM metric on Tecnick (a) and Kodak(b)	.
Figure 11: Ablation study of the CIFAR-10 β-VAE. The distribution of the loss weight used at train-ing time has the largest impact on the performance. Moreover, conditioning the model by providingthe loss weights as inputs to the model underperforms compared to more advanced conditioningmethods. Other variants all perform close to the full method.
Figure 12: Additional qualitative comparison of image stylization models on images from the val-idation set of ImageNet. The first row shows the results of YOTO trained on all parameters, thesecond row - of models trained independently per configuration. In both cases parameter valueincreases from left ot right. YOTO results are very similar to those of independently trained models.
