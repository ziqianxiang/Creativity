Figure 1: The hierarchical accumulation process of tree structures (best seen in colors). Given aparse tree, it is interpolated into a tensor S, which is then accumulated vertically from bottom totop to produce S. Next, the (branch-level) component representations of the nonterminal nodes arecombined into one representation as N by weighted aggregation. Multi-colored blocks indicateaccumulation of nodes of respective colors. The rows of S in Eq. 5 are counted from the bottom.
Figure 2: Hierarchical Embeddings.
Figure 3: Subtree masking.
Figure 4: Illustration of the proposed Tree-based Attentions: (a) Encoder self-attention, (b) Decodercross-attention. Circle-ended arrows indicate where hierarchical accumulations take place. Theoverall Transformer architecture is provided in Figure 6 (Appendix 7.3).
Figure 5: Training data size and training/inference time analysis.
Figure 6: Overall architecture of Tree Transformer. (Dashed lines: sharing parameters)Model	Base	BigTransformer	61,747,200	209,813,504Ours	61,810,944 (+0.1%)	209,967,104(+0.07%)Table 5: Exact number of parameters for Transformer and our model, both used for WMTâ€™14English-German task.
Figure 7: Process to break standard tree (fig. 7a) into BPE tree (fig. 7b).
