Figure 1: Overview of our method in the context of prepare breakfast task. This task can be broken down intosubtasks (e.g., pickup mug) that composes the underlying subtask graph G. (Left) To learn about the unknowntask, the agent collects trajectories over K episodes through a parameterized adaptation policy πθadapt that learnsto explore the environment. (Center) With each new trajectory, the agent attempts to infer the task’s underlyingground-truth subtask graph G with Gb. (Right) A separate test policy πGtbest uses the inferred subtask graph Gb toproduce a trajectory that attempts to maximize the agent’s reward P rt (e.g., the green trajectory that achievesthe boil egg subtask). The more precise Gb, the more reward the agent would receive, which implicitly improvesthe adaptation policy πθadapt to better explore the environment and therefore better infer Gb in return.
Figure 2: Our inductive logic programming module infers the precondition Gc from adaptation trajectory. Forexample, the decision tree of subtask E (bottom row) estimates the latent precondition function fGE : x 7→ eEby fitting its input-output data (i.e., agent’s trajectory {xt, etE}tH=1). The decision tree is constructedcby choosinga variable (i.e., a component of x) at each node that best splits the data. The learned decision trees of all thesubtasks are represented as logic expressions, and then transformed and merged to form a subtask graph.
Figure 3: Left: A visual illustration of Playground domain and an example of underlying subtask graph. Thegoal is to execute subtasks in the optimal order to maximize the reward within time budget. The subtask graphdescribes subtasks with the corresponding rewards (e.g., transforming a chest gives 0.1 reward) and dependenciesbetween subtasks through AND and OR nodes. For instance, the agent must first transform chest AND transformdiamond before executing pick up duck. Right: A warfare scenario in SC2LE domain (Vinyals et al., 2017).
Figure 4: Learning curves on thePlayground domain. We mea-sure the normalized reward (y-axis) in a test phase, after a certainnumber of training trials (x-axis).
Figure 5: Generalization performance on unseen tasks (D1-Eval, D2, D3, D4, and Mining-Eval) with varyingadaptation horizon. We trained agent with the fixed adaptation budget (K = 10 for Playground and K = 25for Mining) denoted by the vertical dashed line, and tested with varying unseen adaptation budgets. We reportthe average normalized return during test phase, where GRProp+Oracle is the upper bound (i.e., Rb = 1) andRandom is the lower bound (i.e., R = 0). The shaded area in the plot indicates the range between R + σ andRb - σ where σ is the standard error of normalized return.
Figure 6: Adaptation performance with different adaptation horizon on SC2LE domain.
Figure 7: Dependencybetween subtask graphand MDP(14)The observation is updated such that agent moves on to the target object, and perform correspondingprimitive action. The eligibility vector et+1 is computed from the completion vector xt+1 andprecondition Gc as follows:eit+1 = j∈COhRildi yAj ND(15)yAND = AND	xbt,+1 ,	(16)j∈Childixbit,+j1 =xtj+1wi,j+(1-xtj+1)(1-wi,j),	(17)where wi,j = 0 if there is a NOT connection between i-th node andj-th node, otherwise wi,j = 1, andwi,j’s are defined by the Gc. Intuitively, xbit,j = 1 when j-th node does not violate the precondition ofi-th node. Executing each subtask costs different amount of time depending on the map configuration.
Figure 8: An example observation and subtask graph of the Mining domain (Sohn et al., 2018). The preconditionof each subtask has semantic meaning based on the Minecraft game, which closely imitates the real-world tasks.
Figure 9: (Top) The agent starts the game initially with limited resources of 50 minerals, 0 gases, 3 foods, 11SCVs collecting resources, 1 idle SCV and pre-built Refinery. (Middle) From the initial state, the agent needs tostrategically collect resources and build structures in order to be well prepared for the upcoming battle. (Bottom)After 2,400 environment steps, the war breaks; all the buildings in the map are removed, and the enemy unitsappear. The agent’s units should eliminate the enemy units within 240 environment steps during the war.
Figure 10: Precision and recall of binary assignments on the inferred subtask graph’s precondition.
Figure 11: The actual tech-tree of Terran race in StarCraft II. There exists a hierarchy in the task, which can beautonomously discovered by our MSGI agent.
Figure 12: A simplified version of subtask graph inferred by our MSGI-GRProp agent after 10 episodes ofadaptation.
Figure 13: The full subtask graph inferred by our MSGI agent.
Figure 14:	Adaptation performance of MSGI-GRProp, Random, and HRL agents with different adaptationhorizon on AI2-THOR domain. The episode terminates after the agent executes 20 subtasks or when there isno subtask available to execute. Our MSGI-GRProp achieves around 2.5 total reward within an episode byexecuting roughly two serve subtasks, while the baseline methods almost never get any reward.
Figure 15:	(a) - (d) demonstrates an example task of preparing fried potato in the AI2-THOR domain. Theserve PotatoSliced on Plate subtask requires slicing the potato (e.g., (b)) and frying the sliced potato on thepan (e.g., (c)) before serving the dish. The agent receives a reward after finishing the final subtask (e.g., (d)) ofserving the dish on the plate.
Figure 16:	The subtask graph inferred by our MSGI-GRProp agent in the AI2-THOR environment after 20episodes.
Figure 17: Comparison of meta-training MSGI-Meta agent that was trained with UCB bonus and extrinsicreward, and MSGI-Meta without UCB agent that was trained with extrinsic reward only in the Playgroundand Mining domain. In both domains, adding UCB bonus improves the meta-training performance of ourMSGI-Meta agent.
Figure 18: A qualitative example of subtask graph inference, in the Mining domain.
Figure 19: (Left) Our MSGI model and (Right) the architecture of adaptation policy πθadapt.
