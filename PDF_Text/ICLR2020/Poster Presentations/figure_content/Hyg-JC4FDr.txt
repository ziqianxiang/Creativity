Figure 1: Results of ValueDICE on a simple Ring MDP. Left: The expert data is sparse and onlycovers states 0, 1, and 2. Nevertheless, ValueDICE is able to learn a policy on all states to best matchthe observed expert state-action occupancies (the policy learns to always go to states 1 and 2). Right:The expert is stochastic. ValueDICE is able to learn a policy which successfully minimizes the trueKL computed between dÏ€ and dexp.
Figure 2:	Comparison of algorithms given 1 expert trajectory. We use the original implementationof GAIL (Ho & Ermon, 2016) to produce GAIL and BC results.
Figure 3:	Comparison of algorithms given 10 expert trajectories. ValueDICE outperforms othermethods. However, given this amount of data, BC can recover the expert policy as well.
Figure 4: ValueDICE outperforms behavioral cloning given 1 trajectory even without replay regu-larization.
