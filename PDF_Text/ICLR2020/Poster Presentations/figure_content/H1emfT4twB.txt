Figure 1:	Different classes exhibit different worddistributions in HuffPost headlines. We computethe local mutual information (LMI) (Evert, 2005)between words and classes. For each class, weinclude its top 2 LMI-ranked words. Darker colorsindicate higher LMI.
Figure 2:	Visualization of word importance onexample from class fifty in HuffPost headlines.
Figure 3:	Episode generation. a) Meta-training: First, sample N classes from Ytrain. Then, samplethe support set and the query set from the N classes. We use examples from the remaining classesto form the source pool. b) Meta-testing: Select N new classes from Ytest and sample the supportset and the query set from these N classes. We use all examples from Y train to form the source pool.
Figure 4:	Single episode with N = 3, K = 1, L = 2. Rectangles denote input examples. The textinside corresponds to their labels. An episode contains a support set, a query set, and a source pool.
Figure 5: Illustration of our model for an episode with N = 3, K = 1, L = 2. The attentiongenerator translates the distributional signatures from the source pool and the support set into anattention α for each input example x (5a). The ridge regressor utilizes the generated attention toweight the lexical representations (5b). It then learns from the support set (5c) and makes predictionsover the query set (5d).
Figure 6:	Learning curve of cnn+proto (left) v.s. our (right) on the Reuters dataset. We plotaverage 5-way 1-shot accuracy over 50 episodes sampled from seen classes (blue) and unseen classes(red). While our has weaker representational power, it generalizes better to unseen classes.
Figure 7:	PCA visualization of the input representation for the query set of a testing episode (N = 5,K = 5, L = 500) sampled from 20 Newsgroups. Weighted averages of word embeddings given by(a) s(∙),(b) t(∙), and (c) the attention generator meta-trained on a disjoint set of training classes.
Figure 8:	Attention weights generated by our model are specific to task. We visualize our model’sinputs s(x) (top), t(x) (middle), and output (bottom) for one query example from class jobs inReuters dataset. Word “statistical” is downweighed for jobs when compared to other economicsclasses (left), but it becomes important when considering dissimilar classes (right). Fine-grainedclasses: jobs, retail, industrial production index, wholesale production index, consumer productionindex. Coarse-grained classes: jobs, cocoa, aluminum, copper, reserves.
Figure 9: Learning curves of induction net on Huffpost and Reuters. We plot average 5-way5-shot accuracy over 50 episodes sampled from seen classes (blue) and unseen classes (red).
Figure 10: Perplexity during BERT language model finetuning in p-maml. The lexical distributionsmismatch significantly between meta-train and meta-test classes.
Figure 11: PCA visualization of the input representation for a testing episode in 20 Newsgroupswith N = 5, K = 5, L = 500 (the query set has 500 examples per class). AVG: average wordembeddings. s(∙): weighted average of word embeddings with weights given by s(∙). t(∙): weightedaverage of word embeddings with weights given by t(∙). OUR: weighted average of word embed-dings with weights given by the attention generator meta-trained on a disjoint set of training classes.
Figure 12:	PCA visualization of the input representation for a testing episode in HuffPost Headlineswith N = 5, K = 5, L = 500. AVG: average word embeddings. s(∙): weighted average of wordembeddings with weights given by s(∙). t(∙): weighted average of word embeddings with weightsgiven by t(∙). OUR: weighted average of word embeddings with weights given by the attentiongenerator meta-trained on a disjoint set of training Classes.
Figure 13:	Average cosine similarity to the oracle word importance over the query set of a testingepisode with N = 5, K = 5, L = 500. This oracle is estimated using all labeled examples from theN classes. Since examples in HuffPost Headlines are 30 times shorter, the cosine similarities arehigher in this corpora. AVG: uniform distribution over the words. s(∙): word importance estimateddirectly by s(∙). t(∙): word importance estimated directly by t(∙). OUR: word importance estimatedby the meta-learned attention generator.
Figure 14:	Visualization of the attention generated by our model on 10 query examples from a 5-way5-shot testing episode in Huffpost Headlines.
Figure 15: Learning Curve for fully-supervised CNN Classifier vs our. Blue indiCates CNN aCCu-raCy, With standard deviation shaded. Solid horizontal line is our’s 5-shot aCCuraCy; dashed lineis 1-shot aCCuraCy. our’s 5-shot performanCe is Competitive, espeCially When the total number oflabeled examples is small.
