Figure 1: Visualization of representers in scalar quantization vs. reparameterized quantization. Theaxes represent two different model parameters (e.g., linear filter coefficients). Dots are samples ofthe model parameters, discs are the representers. Left: in scalar quantization, the representers mustbe given by a Kronecker product of scalar representers along the cardinal axes, even though the dis-tribution of samples may be skewed. Right: in reparameterized scalar quantization, the representersare still given by a Kronecker product, but in a transformed (here, rotated) space. This allows abetter adaptation of the representers to the parameter distribution.
Figure 2: Classifier architecture. The Φ tensors (annotated with a tilde) are stored in their com-pressed form. During inference, they are read from storage, uncompressed, and transformed via finto Θ, the usual parameters of a convolutional or dense layer (denoted without a tilde).
Figure 3: The internals of fconv and fdense in our experiments for layer k, annotated with the di-mensionalities. In fconv, H, W, I, O refer to the convolutional height, width, input channel, outputchannel, respectively. For fdense, I and O refer to the number of input and output activations. Forfconv, we use an affine transform, while for fdense we use a scalar shift and scale, whose parametersare captured in Ψ. Note that in both cases, the number of parameters of f itself (labeled as ψ) issignificantly smaller than the size of the model parameters it decodes.
Figure 4: Error vs. rate plot for ResNet-18 on ImageNet and ResNet-20-4 on CIFAR-10 using SQ,DFT transform, and random but fixed orthogonal matrices. The DFT is clearly beneficial in com-parison to the other two transforms. All experiments were trained with the same hyper-parameters(including the set of entropy penalties), only differing in the transformation matrix.
