Figure 1: (a) DPFRL tracks a learned latent belief with a differentiable particle filter and learns a policyconditioned on the particle belief. (b) Generative models learn p(o | ht), the distribution of observations o giventhe latent state ht . The particle filter then evaluates the probability of an observed ot to get a compatibilitymeasure of ot and ht. (c) The compatibility function directly predicts a compatibility measure of ot and ht usinga learned neural network fobs (ot , ht).
Figure 2: DPFRL Network. In DPFRL, latent particles {(ht,wi)}K=ι are maintained by a differentiableparticle filter algorithm, which includes observation-conditioned transition function ftrans, discriminativecompatibility function fθbs, and soft-resampling. The policy and value function is conditioned on the belief,which is summarized by the mean particle ht, and m moment generating function features, M1:m.
Figure 3: Mountain Hike Task.
Figure 4: Results for Mountain Hike where useful observations are concatenated with a noise vector of length l.
Figure 5: Partially Observable Atari Games. In Flickering Atari Games frames are randomly dropped andreplaced with a blank frame. In Natural Flickering Atari Games the background is replaced with a random videostream and the Atari components of the image are randomly dropped.
Figure 6: RGB-D Habitat ObservationsTable 2: Visual Navigation Results	SPL	Success Rate	RewardDPFRL	0.79	0.88	12.82±5.82DVRL	0.09	0.11	5.22±2.24GRU	0.63	0.74	10.14±2.82PPO(Savva et al., 2019)	0.70	0.80	—Visual navigation poses a great challenge for deep RL (Mirowski et al., 2016; Zhu et al., 2017; Lample& Chaplot, 2017). We evaluate DPFRL for visual navigation in the Habitat Environment (Savvaet al., 2019), using the real-world Gibson dataset (Xia et al., 2018). In this domain, a robot needsto navigate to goals in previously unseen environments. In each time step, it receives a first-personRGB-D camera image and its distance and relative orientation to the goal. The main challenge lies inthe partial and complex observations: first-person view images only provide partial information aboutthe unknown environment; and the relevant information for navigation, traversability, is encoded inrich RGB-D observations along with many irrelevant features, e.g., the texture of the wall. We usethe Gibson dataset with the training and validation split provided by the Habitat challenge.
Figure 7: Habitat Visual Navigation RewardE.3 Particle Visualization with PCAWe further visualize the latent particles by principal component analysis (PCA) and choose the first 2components. We choose a trajectory in the Habitat Visual Navigation experiment, where 15 particlesare used. We observe that particles initially spread across the space (t = 0). As the robot only receivepartial information in the visual navigation task, particles gradually form a distribution with twoclusters (t = 56), which represent two major hypotheses of its current state. After more informationis incorporated into the belief, they begin to converge and finally become a single cluster (t = 81).
