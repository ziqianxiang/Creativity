Figure 1: Our population-invariant Q function: (a) utilizes the attention mechanism to combineembeddings from different observation-action encoder fi ; (b) is a detailed description for fi , whichalso utilizes an attention module to combine M different entities in one observation.
Figure 2: Environment Visualizationsand the (eaten) sheep will obtain a negative reward and becomes inactive (dead). A sheep will berewarded when it comes across a grass pellet and the grass will be collected and respawned in anotherrandom position. Note that in this survival game, each individual agent has its own reward and doesnot share rewards with others.
Figure 3: Example matches between EPC and MADDPG trained agents in Grassland(a) EPC	(b) MADDPGFigure 4: Adversarial Battle: dark particles are dead agents.
Figure 4: Adversarial Battle: dark particles are dead agents.
Figure 5: Food Collectionthe other hand, in Fig. 3b, we can see that the EPC sheep learns to eat the grass and avoid the wolvesat the same time.
Figure 6: Results in Grassland. In part (a), we show the normalized scores of wolves and sheeptrained by different methods when competing with EPC sheep and EPC wolves respectively. In part(b), we measure the sheep statistics over different scales (x-axis), including the average number oftotal grass pellets eaten per episode (left) and the average percentage of sheep that survive until theend of episode (right). EPC trained agents (yellow) are consistently better than any baseline method.
Figure 9: Ablation analysis on the second curriculum stage in all the games over 3 different trainingseeds. Stability comparison (top) in (a), (b) and (c): We observe EPC has much less variancecomparing to vanilla-PC. Normalized scores during fine-tuning (bottom) in (d), (e) and (f): Thisillustrates that EPC can successfully transfer the agents trained with a smaller population to a largerpopulation by fine-tuning.
Figure 10: Environment Generalization: We take the agents trained on the largest scale and test on anenvironment with twice the population. We perform experiments on all the games and show that EPCalso advances the agentsâ€™ generalizability.
Figure 11: Full learning curves on the second curriculum stage in all the games. EPC fine-tunesthe policies obtained from the previous stage while MADDPG and Att-MADDPG are trained fromscratch for a much longer time.
Figure 13: Normalized scores in the original Predator-prey gameBesides, we also report the raw reward numbers when competing against EPC. Since the Predator-prey game isa zero-sum game, we simply report the predator rewards (the prey reward is exactly the negative value).
