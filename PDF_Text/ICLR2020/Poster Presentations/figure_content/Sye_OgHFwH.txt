Figure 1: An overview of proposed attacks. Top: Colorization attack (CAdv); Bottom: Texturetransfer attack (t Adv). Our attacks achieve high attack success rate via semantic manipulation withoutany constraints on the Lp norm of the perturbation. Our methods are general and can be used forattacking both classifiers and captioners.
Figure 2: Class color affinity. Samples from unconstrained CAdv attacking network weights withzero hints provided. For (a) the ground truth (GT) class is pretzel; and for (b) the GT is car.
Figure 3: Controlling cAdv. We show a comparison of sampling 50 color hints from k clusterswith low-entropy. All images are attacked to misclassify as golf-cart. Second and fourth rowvisualize our cluster segments, with darker colors representing higher mean entropy and red dotsrepresenting the sampled hints location. Sampling hints across more clusters gives less color variety.
Figure 4: Number of color hints required for cAdv. All images are attacked to Merganser withk = 4. When the number of hints increases (from left to right), the output colors are more similar togroundtruth. However, when the number of hints is too high (500), cAdv often generates unrealisticperturbations. This is due to a harder optimization for cAdv to both add adversarial colors and matchGT color hints. CAdV is effective and realistic with a balanced number of hints.
Figure 5: tAdv strategies. Texture transferred from random “texture source” (Ts) in row 1, randomtarget class Ts (row 2) and from the nearest target class Ts (row 3). All examples are misclassifiedfrom Beacon to Nautilus. Images in the last row look photo realistic, while those in the first tworows contain more artifacts as the texture weight α increases (left to right).
Figure 6: Captioning attack. Top: cAdv; Bottom: tAdv. We attack the second word to dog andshow the corresponding change in attention mask of that word. More examples in Appendix.
Figure 7: Density Plot. Our methods achieve large L∞ norm perturbations without notable reductionin user preference. Each plot is a density plot between perturbation (L∞ norm) on X axis andP r(user prefers adversarial image) on Y axis. For ideal systems, the density would be a concentratedhorizontal line at 0.5. All plots are on the same set of axes. On the left, plots for three baselinemethods (Fig a - Fig c). Note the very strong concentration on small norm perturbations, which userslike. Right 4 plots shows our methods (Fig d - Fig g). Note strong push into large norm regions,without loss of user preference.
Figure 8: Perturbation comparisons. Images are attacked from tarantula to beacon, golfcart, nautilus, photocopier, pretzel from left to right. Our perturbations (cAdvand tAdv) are large, structured and have spatial patterns when compared with other attacks. Perturba-tions from cAdv are low-frequency and locally smooth while perturbations from tAdv are primarilyhigh-frequency and structured. Note gray color indicates no perturbations.
Figure 9: Additional qualitative examples for controlling cAdv. We show a Comparison ofsampling 50 Color hints from k Clusters with low-entropy. All images are attaCked to golf-cart.
Figure 10: Images attacked with the method of (Hosseini & Poovendran, 2018) are not realistic, asthese examples show. Compare Fig 12, showing results of our color attack. Similar qualitative resultsfor CIFAR-10 are visible in Hosseini & Poovendran (2018).
Figure 11: Additional qualitative examples for tAdv. Texture transferred from random images(row 1), random images from adversarial target class (row 2) and from the nearest neighbor ofthe victim image from the adversarial target class (row 3). All examples are misclassified fromMerganser to Umbrella. Images in the last row look photo realistic, while those in the first tworows contain more artifacts as the texture weight α increases (left to right).
Figure 12: Captioning attack. We attack the second word of each caption to {dog, bird} and showthe corresponding change in attention mask of that word. For tAdv we use the nearest neighborselection method and for cAdv we initialize with all groundtruth color hints.
Figure 13: Randomly sampled, semantically manipulated, unrestricted adversarial examples and theirperturbations. (a) Adversarial examples generated by cAdv attacking Hints and mask with 50 hints.
