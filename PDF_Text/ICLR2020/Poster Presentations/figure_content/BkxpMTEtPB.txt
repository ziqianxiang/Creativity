Figure 1: Arecurrent unitGLADcell.
Figure 2: Convergence of ADMM in terms of NMSEand optimization objective. (Refer to Appendix C.2).
Figure 3: Minimalist neural network architectures designedfor GLAD experiments in sections(5.2, 5.3, 5.4, 5.5). Refer Ap-pendix C.5 for further details about the architecture parameters.
Figure 4: GLAD vs traditional methods. Left 3 plots: Sparsity level is fixed as s = 0.1. Right 3 plots: Sparsitylevel of each graph is randomly sampled as S 〜U(0.05, 0.15). Results are averaged over 100 test graphs whereeach graph is estimated 10 times using 10 different sample batches of size M . Standard error is plotted but notvisible. Intermediate steps of BCD are not evaluated because we use sklearn packagePedregosa et al. (2011) andcan only access the final output. Appendix C.4, C.5 explains the experiment setup and GLAD architecture.
Figure 5: Sample complexity for model selection consistency.
Figure 6:	Performanceon the SynTReN gener-ated gene expression datawith graph as Erdos-renyihaving sparsity p = 0.05.
Figure 7:	Recovered graphstructures for a sub-networkof the E. coli consisting of43 genes and 30 interactionswith increasing samples. In-creasing the samples reducesthe fdr by discovering moretrue edges.
Figure 8: Varying the number of unrolled iterations. The results are averaged over 1000 test graphs.
Figure 9: We attempt to illustrate how the traditional methods are very sensitive to the hyperparametersand it is a tedious exercise to finetune them. The problem setting is same as described in section(5.3).
Figure 10: Convergence on Erdos-random graphs with fixed sparsity (p = 0.1). All the settings aresame as the fixed sparsity case described in Figure 4. We see that the AM based parameterization‘GLAD’ consistently performs better than the ADMM based unrolled architecture ‘ADMMu’.
