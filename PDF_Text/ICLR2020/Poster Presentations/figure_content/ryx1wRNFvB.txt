Figure 1: a Schematic diagrams of different recurrent networks and the corresponding recurrentconnectivity matrices (upper panel). b Memory curves, J (k) (Equation 3), for the four recurrentnetworks shown in a. The non-normal networks, chain and chain with feedback, have extensivememory capacity: Jtot 〜 O(N), whereas the normal networks, identity and random orthogonal, haveJtot = 1. c Extensive memory is made possible in non-normal networks by transient amplification:the signal is amplified for a time of length O(N) before it dies out, abruptly in the case of the chainnetwork and more gradually in the case of the chain network with feedback. In b and c, the networksize is N = 100 for all four networks.
Figure 2: Linear decoding experiments. a In a linear network with no noise, the past signal s1 can beperfectly reconstructed from the current activity vector h100 using a linear decoder. b When noiseis added, the chain network outperforms the orthogonal network as predicted from the theory inGanguli et al. (2008). c In a completely deterministic system, introducing a non-linearity has a similareffect to that of noise. The chain network again outperforms the orthogonal one when the signalis reconstructed with a linear decoder. As discussed further in the text, this is because the signal issubject to more interference in the orthogonal network than in the chain network. All simulations inthis figure used networks with N = 100 recurrent units. In c, We used the elu non-linearity for f (∙)(Clevert et al., 2016). For the chain network, we assume that the signal is fed at the source neuron.
Figure 3: Results on copy, addition, and psMNIST bench-marks. a-c Validation losses with the best hyper-parametersettings. Solid lines are the means and shaded regions arestandard errors over different runs using different randomseeeds. For the copy and addition tasks, we also show theloss values for random baseline models (dashed lines). Forthe psMNIST task, the mean cross-entropy loss for a randomclassifier is log(10) ≈ 2.3, thus all four models comfortablyoutperform this random baseline right from the end of thefirst training epoch. d-f Number of “successful” runs (or hy-perparameter configurations) that converged to a validationloss below 50% of the loss for the random baseline model.
Figure 4: Results on language modeling benchmarks. Solid lines are the means and shaded regionsare standard errors over 3 different runs using different random seeeds.
Figure 5: Training induces hidden chain-like feedforward structures in vanilla RNNs. The unitsare first ordered by the time of their peak activity. Then, the mean recurrent connection weight isplotted as a function of the order difference between two units, i - j . Results are shown for RNNstrained on the addition (a) and the permuted sequential MNIST (b) tasks. The left column showsthe results for RNNs initialized with a scaled identity matrix, the right column shows the results forRNNs initialized with random orthogonal matrices. In each case, training induces hidden chain-likefeedforward structures in the networks, as indicated by an asymmetric bump peaked at a positivei - j value in the weight profile. This kind of structure is either non-existent (identity) or muchless prominent (orthogonal) in the initial untrained networks. For the results shown here, we onlyconsidered sufficiently well-trained networks that achieved a validation loss below 50% of the lossfor a baseline random model at the end of training. The solid lines and shaded regions representmeans and standard errors of the mean weight profiles over these networks.
Figure 6: The recurrent weight matrices inside the input, forget, and output LSTM gates do notdisplay the characteristic signature of a prominent chain-like feedforward structure. The weightprofiles are instead an approximately monotonic function of i - j. The recurrent weight matrix insidethe update (tanh) gate, however, does display an asymmetric chain-like structure similar to thatobserved in vanilla RNNs. The examples shown in this figure are from the input (a), forget (b), output(c), and update gates (d) of the second layer LSTM in a 3-layer LSTM architecture trained on theword-level PTB task. The weight matrices shown here were initialized with orthogonal initializers.
Figure 7: Additional linear decoding experiments: a linear networks with low noise (σ = 0.01) and blinear networks with high noise (σ = 1). In both cases, the chain network outperforms the orthogonalnetwork suggesting that the results are not sensitive to the noise scale.
Figure 8:	Additional linear decoding experiments: a tanh networks with no noise and b relunetworks with no noise. In both cases, the chain network outperforms the orthogonal networksuggesting that the results are not sensitive to the choice of the non-linearity.
Figure 9:	Validation loss at the end of training as a function of the feedback parameter β in thepsMNIST task. All networks with a better-than-random loss at the end of training are included in thisfigure. The solid line shows the mean and the shaded region represents the standard errors.
Figure 10:	Validation accuracy in the psMNIST task. The corresponding validation losses areshown in Figure 3c in the main text. Note that we used RNNs with n = 25 recurrent units in thesesimulations, so these numbers are not directly comparable to those reported in some previous works(e.g. Arjovsky et al. (2016); Kerg et al. (2019)).
