Figure 1: FGSM classification accuracy as a function of noise added (Epsilon) for the three datasetstested: A) Patches B) MNIST C) CUB-200.
Figure 2: Sleep increases robustness to general distortions. A) Generalization classification accuracyfor 5 networks for noise and blur on the MNIST dataset. B) Generalization classification accuracyfor 4 networks for the noisy CUB-200 task. Note that there are only 4 networks because there is noblur task here.
Figure 3: A) Patches dataset example - four binary images with 25 pixels turned on in each imageand 15 pixel overlap. B)Types of images tested on for generalization for the Patches dataset. Top -Images with Gaussian noise added with increasing variannce (from 0 to 1.0 in steps of 0.2). Bottom- Gaussian blurred images with increasing sigma (from 0 to 2.5 in steps of 0.5). C) Generalizationaccuracy for noise and blur of five different networks tested (Control, Defensively distilled, Fine-tuned-blur, Fine-tuned-noise, and Sleep) for the Patches dataset. D) Weight spread for each of the 5networks tested in C.
Figure 4: Weights for each of the 5 networks (Control, defensively distilled, finetuning on noise,finetuning on blur, and sleep) trained on the Patches dataset. Each column shows the weights value(y-axis) connecting from each of the 100 input neurons (x-axis) to the corresponding output neuron(i.e. the first column of graphs if the weights connecting from all input neurons to the first outputneuron. Points are color-coded based on which of these pixels in the input layer correspond to ON-pixels, OFF-pixels, or overlapping pixels based on the input that should trigger that output neuron.
Figure 5: A) Example function learned in a 3-layer neural network illustrates that sleep alters de-cision boundaries in favor of making one class (corresponding to black points) more robust whileimpinging on another class (blue points). B) Average noise (epsilon) needed for FGSM attack forspecific digits. C) Output layer scores for each digit (rows) before and after sleep. Columns repre-sent average activation of each of the 10 output neurons.
Figure 6: A) DeepFool adversarial examples for each defense. The networkâ€™s prediction is shownabove each image. B) JSMA. C) Boundary Attack.
Figure 7: Types of images tested on for generalization for the MNIST dataset. Top - Images withGaussian noise added with increasing variannce (0, 0.1, 0.3, 0.5, 0.7, 0.9, left to right). Bottom -Gaussian blurred images with increasing sigma (from 0 to 2.5 in steps of 0.5).
Figure 8:	Correlation differences between defense network and control network for 4 different de-fenses. Correlations are computed based on the activations in each layer for each pair of digits (meancorrelation). The difference between the correlation of the defense method (column) and the controlnetwork is plotted. Activations are computed based on the undistorted test images.
Figure 9:	same as Figure 8 but activations are computed when presented with noisy images.
Figure 10: same as Figure 8 but activations are computed When presented With blurred images.
Figure 11: Normalized firing rates of neurons specific to individual digits when presented with noisyimages is greater after applying sleep than before sleep.
Figure 12: Normalized activations of neurons specific to individual digits when presented with noisyimages is greater after applying sleep than before sleep.
