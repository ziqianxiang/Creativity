Figure 1: (a) illustrates the conventional convolution process; (b) shows the convolution with NES method.
Figure 2: Our proposed compression process along the spatial dimension. We only show the transformationalong spatial dimensions for easy understanding and the transformation along the channel dimension can befound in Figure 3. The indexing learner learns the position mapping function, M : (i, j, m) -→ (p, q), betweenthe convolution kernel elements and the sub-tensor in the epitome Es . The learned starting indices and theepitome are fed into Eqn. (7) to sample the weight tensor. The outputs of Eqn. (7) are concatenated togetherto form the weight tensor. Note that we use a moving average way to update M so that the indexing learnercan be removed during inference. This is shown in details in section 3.2 in the paragraph ‘Routing map’. Thewhole training process is end-to-end trainable and hence can be used for any specified network architecture.
Figure 3: Transformation along the input channel dimension of NES. In the figure, we only show three dimensionsof the epitome with E ∈ RWE ×HE ×CiEn ×1. To simplify the illustration, we set WE = w and HE = h wherew and h are the size of the convolution kernel. Thus, the starting indices along the spatial dimension,(p,q), are notshown in the figure. The generated weight tensor has input channel number equal to 8. Rcin = dCin∕βι[ = 2denotes the number of samplings applied along the input channel dimension. During each transformation,sub-tensor with shape w × h × β1 is selected each time based on Eqn. (8) by replacing the starting index (p,q)in Eqn. (7) with (cin) and enumerating over the input channel dimension. In this example, β1 is set to 4.
Figure 4: NES transformation along the input channel dimension with channel wrapping. To simplify theillustration, we choose an epitome with shape Rw×h×3×1 and CiEn = 3. In this example, the transformation isapplied twice and the two learned starting indices are 0 . The input feature map F is first added based on thelearned interpolation position of the kernels. Input feature map F1 and F4 are both multiplied with the firstchannel in the epitome since W1 and W4 are both generated with E1 . To reuse the multiplication, feature mapF1 and F4 are first added together before multiplying with the weights kernel E1 . The figure uses integer indexto simplify the illustration. When the learned indices are fractions, the feature maps are the weighted summationof the two nearest integer indexed sub-tensors in the epitome as shown in Eqn. (8). For example, if the twostarting indices in this figure are 0.6 and 0.3, the calculation becomes (0.6F1 + 0.3F4 + 0.4F3 + 0.7F6)0E 1 +(0.4F1 + 0.7F4 + 0.6F2 + 0.3F5) 0E2 + (0.4F2 + 0.7F5 + 0.6F3 + 0.3F6)区E3. Since We group the featuremap first before the convolution, the computation cost is reduced.
Figure 5: ImageNet classification accuracy of our method, EfficientNet, MobileNetV2 baselines and otherNAS based methods including AMC (He et al., 2018), IGCV3 (Sun et al., 2018), MNasNet (Tan et al., 2018),ChamNet (Dai et al., 2018) and ChannelNet (Gao et al., 2018). Our method outperforms all the methods withinthe same level of MAdd. Here, MObileNetV2* is our implementation of baseline models and MobileNetV2is the original model with width multiplier of 0.35,0,5,0,75 and 1. The backbone model for our results areEfficientNet_b1 and b0 (Tan & Le, 2019) with multiplier 0.5 and MobileNetV2 with multiplier of 0.75, 0.5,0.35 and 0.2, respectively (from top to bottom). Note that we do not use additional training tricks including thesqueeze-and-excitation module (Hu et al., 2018) and the Swish activation function (Ramachandran et al., 2017).
Figure 6: NES transformation along the input channel dimension with channel wrapping. To simplify theillustration, we choose an epitome with shape Rw×h×3×1 and β1 = 3. The transformation is applied twiceand the two starting indices are 0. The input feature map F is first grouped based on the learned interpolationposition of the kernels. Input feature map F1 and F4 are both multiplied with the first channel in the epitomesince W1 and W4 are both generated with E1. To reuse the multiplication, feature map F1 and F4 are firstadded together before multiplying with the weights kernel E1 . The figure uses integer index to simplify theillustration. When the learned indices are fractions, the feature maps are the weighted summation of the twonearest integer indexed sub-tensors in the epitome as shown in Eqn. (7). For example, if the two starting indicesin this figure are 0.6 and 0.3, the calculation becomes (0.6F1 + 0.3f4 + 0.4F3 + 0.7f6)0E1 + (0.4F1 + 0.7F4 +0.6F2 + 0.3F5) 0E2 + (0.4F2 + 0.7F5 + 0.6F3 + 0.3F6)区E3. Since We group the feature map first before theconvolution, the cost is reduced.
