Figure 1: (a) shows an example of RPM tasks containing XOR relations across diagrams in rows and theoverview of MXGNet architecture. Here Fρ is object representation module, Eγ is edge embeddings module,Gφ is graph summarization module and Rθ is reasoning network. (b) shows an example of a multilayer graphformed from objects in the first row of diagrams in the example. (c) An example of syllogism represented inEuler diagrams.
Figure 2: Illustration of multiplex edge embeddings and cross-gating function. Each edge contains a set ofdifferent sub-connections (colored differently). Multiplex edges connecting to each node in the last layer areaggregated according to its originating layer. Aggregated embeddings are then passed to a gating function G,which outputs gating variables from each aggregated embeddings.
Figure 3: Architecture of a single Residual Convolution Block.
Figure 4: CNN feature object-level representation module. ’Conv’ is convolution layers, ’Max-Pooling’ ismax-pooling layer and ’ResConv Block’ is Residual Convolutional Block.
Figure 5: Spatial attention based feature object-level representation module. ’Conv’ is convolution layers,’Max-Pooling’ is max-pooling layer and ’ResConv Block’ is Residual Convolutional Block. z is the spatialattention variable (zpres, zwhere). Sampler is a grid sampler which samples grid of points from given featuremaps.
Figure 6: Architecture overview of reasoning module. ’RelEmbed’ is relation embeddings, ’Concat’ isconcatenation layer. ’ResBlock’ is Residual Convolutional Block. ’FC’ is fully connected layer.
Figure 7: Two examples in PGM dataset. (a) task contains a ’Progression’ relation of the number of objectsacross diagrams in columns while (b) contains a ’XOR’ relation of position of objects across diagrams inrows.
Figure 8: Two examples in PGM dataset containing background line objects.
Figure 9: Illustration of diagram ordering in the matrix and numbered representation of subsets.
