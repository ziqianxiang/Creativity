Figure 1: (a) The histogram of `2 distances between the input and the reconstruction using thecorrect capsule or other capsules in CapsNet on the real MNIST images. Notice the stark differencebetween the distributions of reconstructions of the capsule corresponding to the correct class andother capsules. (b) The histograms of `2 distances between the reconstruction and the input for realand adversarial images for the three models explored in this paper on the MNIST dataset. We usePGD (Madry et al., 2017) with the '∞ bound e = 0.3 to create the attacks.
Figure 2: The undetected rate of the white-box targeted defense-aware R-PGD attack versus the FalsePositive Rate on the MNIST, Fashion-MNIST and SVHN datasets.
Figure 3: The defense-aware R-PGD attack is tested on the CIFAR-10 dataset with ∞ = 8/255.
Figure 4: This diagram visualizes the adversarial success rates for each source/target pair for targetedR-PGD attacks on Fashion-MNIST with ∞ = 25/255. The size of the box at position x, y representsthe success rate of adversarially perturbing inputs of class x to be classified as class y. We can seethat there is significantly higher variance for the CapsNet model than for the two CNN models.
Figure 5: These are randomly sampled (not cherry picked) successful and undetected adversarialattacks created by R-PGD with a target class of0 for each model on the SVHN dataset(∞ = 25/255).
Figure 6: The architecture for the CapsNet, CNN+R and CNN+CR model used for our experimentson MNIST (LeCun et al., 1998), FashionMNIST (Xiao et al., 2017), and SVHN (Netzer et al., 2011).
Figure 7: Examples of Corrupted MNIST and the reconstructed image for each model. A red boxrepresent that this input is flagged as an adversarial example while a green box represent this inputhas been misclassified and not been detected.
Figure 8: Examples of Corrupted MNIST and the reconstructed image for each model. A red boxrepresents that this input is flagged as an adversarial example while a green box represents that thisinput has been misclassified and not been detected.
Figure 9: An example shows the plot of the success rate in (a) and undetected rate in (b) of targetedreconstructive PGD attack vesus the hyperparameter beta β for each model on the MNIST test set.
Figure 10: The source clean image is presented in the first row with its reconstruction in the secondrow. For each model, the top row are the targeted adversarial examples and the bottom are thecorresponding reconstruction image when the input are the PGD on the MNIST (left), R-PGD on theFashion-MNIST (middle), CW on the SVHN (right).
Figure 11: These are randomly sampled (not cherry picked) inputs (top row) and the result ofadversarially perturbing them with targeted R-PGD against the CapsNet model (other rows). Manyof these attacks are not successful. Note the visual similarity between many of the attacks and thetarget class.
