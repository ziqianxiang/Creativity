Figure 1: The test time cost (blue) and memorycost of BatchEnsemble (orange) w.r.t the ensem-ble size. The result is relative to single modelcost. Testing time cost and memory cost of naiveensemble are plotted in green.
Figure 2: An illustration on how to generate theensemble weights for two ensemble members.
Figure 3: Performance for lifelong learning. (a): Validation accuracy for each Split-ImageNettask. Standard deviation is computed over 5 random seeds. (b): BatchEnsemble and several othermethods on SPlit-CIFAR100. BatchEnsemble achieves the best trade-off among Accuracy (â†‘),Forget (1), and Time & Memory (1) costs. VAN: Vanilla neural network. EWC: Elastic weightconsolidation (Kirkpatrick et al., 2016). PNN: Progressive neural network (Rusu et al., 2016). BN-Tuned: Fine tuning Batch Norm layer per subsequent tasks. BatchE: BatchEnsemble. Upperbound:Individual ResNet-50 per task.
Figure 4: Comparison between BatchEnsemble and single model on WMT English-German andEnglish-French. Training stops after the model reaches targeted validation perplexity. BatchEnsemblegives a faster convergence by taking the advantage of multiple models. (a): Validation loss of WMT16English-German task. (b): Validation loss of WMT14 English-French task. Big: Tranformer bigmodel. Base: Transformer base model. BE: BatchEnsemble. Single: Single model.
Figure 5: Calibration on CIFAR-10 corruptions: boxplots showing a comparison of ECE underall types of corruptions on CIFAR-10. Each box shows the quartiles summarizing the resultsacross all types of skew while the error bars indicate the min and max across different skew types.
Figure 6: BLEU on English-German task.
Figure 8: Comparison among BatchEnsemble, naive ensemble and dropout ensemble over diversitymetric. Each point in the plot represents a trained model where x-axis represents its accuracy onvalidation set and y-axis represents its diversity against the base model. The base model triviallyhas 0 diversity. We plot the diversity of models trained on different proportions of training data,respectively 100%, 50%, 20% and 10%.
Figure 9: Visualizing prediction diversity among BatchEnsemble (top row) and naive ensemble(bottom row) members on selected test examples on CIFAR-10. The y-axis label denotes meanprediction of ensemble (Mean), individual ensemble member prediction (from E1 to E4) and singlemodel prediction (Single). Correct class is labelled as red. BatchEnsemble preserves the modeldiversity as naive ensemble.
