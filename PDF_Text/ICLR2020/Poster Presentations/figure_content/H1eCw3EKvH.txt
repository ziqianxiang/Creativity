Figure 1: A histogram of the up-date size (x-axis) to the total pre-dicted probability of the 10 mostprobable tokens (left) or the mostprobable token (right) in the Con-stant Reward setting. An updateis overwhelmingly more probableto increase this probability than todecrease it.
Figure 2: Token probabilities through Reinforce training, in the controlled simulations in the SimulatedReward setting. The left/center/right figures correspond to simulations where the target token (ybest) wasinitially the second/third/fourth most probable token. The green line corresponds to the target token, yellowlines to medium-reward tokens and red lines to no-reward tokens.
Figure 3: The cumulative distri-bution of the probability of themost likely token in the NMT ex-periments. The green distribu-tion corresponds to the pretrainedmodel, and the blue correspondsto the reinforced model. The y-axis is the proportion of condi-tional probabilities with a modeof value â‰¤ x (the x-axis). Notethat a lower cumulative percent-age means a more peaked outputdistribution. A lower cumulativepercentage means a more peakedoutput distribution.
Figure 4: Cumulative percentageof contexts where the pretrainedmodel ranks ybest in rank x orbelow and where it does not rankybest first (x = 0). In about halfthe cases it is ranked fourth or be-low.
Figure 5: The probability of dif-ferent tokens following CMRT, inthe controlled simulations in theSimulated Reward setting. Theleft/right figures correspond tosimulations where the target to-ken (ybest) was initially the sec-ond/third most probable token.
Figure 6: Difference betweenthe ranks of ybest in the rein-forced and the pretrained model.
Figure 7: The probability of different tokens following Reinforce, in the controlled simulations in the Con-stant Reward setting. The left/center/right figures correspond to simulations where the target token (ybest) wasinitially the second/third/fourth most probable token. The green line corresponds to the target token, yellowlines to medium-reward tokens and red lines to tokens with r(y) = 0.
Figure 8: Difference between the ranks of ybest in the reinforced with constant reward and the pretrained model.
