Figure 1: Model performance (e.g.,test accuracy) in federated netWorks canvary Widely across devices. Our objec-tive, q-FFL, aims to increase the fair-ness/uniformity of model performanceWhile maintaining average performance.
Figure 2: q-FFL leads to fairer test accuracy distributions. While the average accuracy remainsalmost identical (see Table 1), by setting q > 0, the distributions shift towards the center as lowaccuracies increase at the cost of potentially decreasing high accuracies on some devices. Settingq = 0 corresponds to the original objective (1). The selected q values for q > 0 on the four datasets,as well as distribution statistics, are also shown in Table 1.
Figure 3: q-FFL (q > 0) compared with uniform sampling. In terms of testing accuracy, our objectiveproduces more fair solutions than uniform sampling. Distribution statistics are provided in Table 9 inthe appendix. q-FFL achieves similar average accuracies and more fair solutions.
Figure 4: For a fixed objective (i.e., q-FFL with the same q), the convergence of q-FedAvg (Alg.2),q-FedSGD (Alg.1), and FedSGD. For q-FedAvg and q-FedSGD, we tune a best step-size on q = 0and apply that step-size to solve q-FFL with q > 0. For q-FedSGD, we tune the step-size directly.
Figure 5: q-FFL results infairer (more centered) ini-tializations for meta-learningtasks.
Figure 6:	q-FFL (q > 0) results in more centered (i.e., fair) training accuracy distributions acrossdevices without sacrificing the average accuracy.
Figure 7:	q-FFL (q > 0) compared with uniform sampling in training accuracy. We see that on somedatasets uniform sampling has higher (and more fair) training accuracies due to the fact that it isoverfitting to devices with few samples.
Figure 8: The convergence speed of q-FFL compared with FedAvg. We plot the distance to thehighest accuracy achieved versus communication rounds. Although q-FFL with q>0 is a moredifficult optimization problem, for the q values we choose that could lead to more fair results, theconvergence speed is comparable to that of q = 0.
Figure 9:	q-FFL is more efficient than AFL. With the worst device achieving the same final testingaccuracy, q-FFL converges faster than AFL. For Vehicle (with 23 devices) as opposed to FashionMNIST (with 3 devices), we see that the performance gap is larger. We run full gradient descent ateach round for both methods.
Figure 10:	Convergence of q-FedAvg compared with q-FedSGD under different data heterogeneity.
