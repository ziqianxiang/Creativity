Figure 1: The relationship between model robustness and the portion of clean images used fortraining. We observe that the strongest robustness can be obtained by training completely withoutclean images, surpassing the baseline model by 18.3% accuracy against PGD-2000 attacker.
Figure 2: Comprehensive robust evaluation on ImageNet. For models trained with differentstrategies, we show their accuracy against PGD attackers with 10 to 2000 iterations. Only thecurve of 100% adv + 0% clean becomes asymptotic when evaluating against attackers with moreiterations.
Figure 3: Disentangling the mixture distribution for normalization secures model robustness.
Figure 4:	Standard BN (left) estimates normaliza-tion statistics on the mixture distribution. MBN(right) disentangles the distribution by constructingdifferent mini-batch for clean and adversarial im-ages to estimate normalization statistics.
Figure 5:	Statistics of running mean and running variance of MBN on randomly sampled 20 channelsin a ResNet-152’s res3 block. This suggests that clean and adversarial images induce significantlydifferent normalization statistics.
Figure 6: Comparison of batch statistics and running statistics of BN on randomly sampled 20channels in a ResNet-152’s res3 block. We observe that batch mean can converge to running mean,while batch variance still differs from running variance.
Figure 7: Compared to traditional image classification tasks, adversarial training exhibits a strongerdemand on deeper networks. The performance gain of traditional image classification becomesmarginal after ResNet-200 while the adversarial robustness continues to grow even for ResNet-638.
