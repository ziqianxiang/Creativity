Figure 1: The training ofDBLE. Taking support setSe and a query image xi as input, fθ learns a repre-sentation space through the training of classificationmodel. In this space, hi is the encoded query inputsand {c1, ..., cN} are the class centers. For classifi-cation, DBLE employs proto-loss to update fθ . Forcalibration, DBLE employs the centers and the sam-pled representation zi for the query to update gφ . Ifthe query xi is correctly classified, the componentsin the dotted rectangular are not be computed. Solidarrows represent the forward pass and dotted arrowsrepresent the backward pass to update the network.
Figure 2: Average test accuracy as dt or d0t increases. dt is the distance of a test sample xt to itsground-truth class center and d0t is its distance to the predicted class center. It shows that dt in thespace achieved by prototypical learning in DBLE can better estimate the model’s performance on xt,since DBLE’s accuracy curve is more monotonic and less oscillating as the distance increases.
Figure 3: The Gaussian distribution within one standard deviation σ away from mean, shown beforeand after updating φ. The dotted circles represent the σ of the sample inside it. Red and blue dotsrepresent training samples belonging two different classes in the representation space. The dotted lineis the decision boundary in the space. Let’s take the two mis-classified training samples ha , hb andthe correctly classified hc as examples. (a), before updating φ, the σs of both correctly and wronglylocated samples are initialized with a small value. (b), after updating φ, σ of mis-classified samplesare much larger. Because the proto-loss for calibration will move z, sampled from N (h, diag(σ σ)),to be as close to the correct class center as possible.
Figure 4: Test ECE and NLL of the last 45 epochs during training on CIFAR-100. The learning rateof both vanilla training and DBLE is annealed by 10x at epoch 60, 70, 80.
Figure 5: The histogram of σ of models trained with all training samples vs.
