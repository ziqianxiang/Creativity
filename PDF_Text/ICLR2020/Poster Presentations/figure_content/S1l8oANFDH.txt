Figure 1: Running example: retrieving an autonomous car from tight parking spots. The goal isto learn a state-machine policy (e) that is trained on scenarios (a), (b), and (c), that generalizes toscenario (d).
Figure 2: Flowchart connecting the different components of the algorithm.
Figure 3: Visualization showing the student-teacher interaction for two iterations. (a) The loop-freepolicies (with their corresponding rewards) learned by the teacher for two different initial states. Here,the boxes signify the different segments in the loop-free policies, the colors signify different actions,and the lengths of the boxes signify the durations of the segments. (b) The mapping between thesegments and the modes in the state-machine—i.e., p(μ = mj). Each box shows the compositionof modes vertically distributed according their probabilities. For example, the third segment in theloop-free policy for x1 has p(μ = Green) = 0.65 and p(μ = Brown) = 0.35. (c) The most probablerollouts from the state-machine policy learned by the student. Finally, (d), (e) and (f) are similar to(a), (b) and (c), but for the second iteration.
Figure 4: Comparison of performances on train (left) and test (middle) distributions. Our approachoutperforms the baselines on all benchmarks in terms of test performance. An empty bar indicatesthat the policy learned for that experiment failed on all runs. We also plot test performance fordifferent choices of training distribution for the Car benchmark (right).
Figure 5: (a-c) The RL policy generates unstructured trajectories, and therefore does not generalizefrom (a) the training distribution to (b) the test distribution. In contrast, our state machine policy in(c) generates a highly structured trajectory that generalizes well. (c-e) A user can modify our statemachine policy to improve performance. In (d), the user sets the steering angle to the maximum value0.5, and in (e), the user sets the thresholds in the switching conditions Gmm2 , Gmm1 to 0.1.
Figure 6: Left: Trajectories for the Quad (leftmost) and QuadPO (second from the left) benchmarksusing our state machine policy. Right: Graph of vertical acceleration over time for both our policy(red) and the neural network policy (blue), for Quad (second from the right) and QuadPO (rightmost).
Figure 7:	Switching conditions represented as decision trees.
Figure 8:	Summary of our benchmarks. #A is the action dimension, #O is the observation dimension,X0train is the set of initial states used for training, X0test is the set of initial states used to test inductivegeneralization, # modes is the number of modes in the state machine policy, and A_G and C_G arethe grammars for action functions and switching conditions, respectively. Depth of C_G indicates thenumber of levels in the Boolean tree.
Figure 9: Experiment results for additional benchmarks. G is the average goal error (closer to 0 isbetter). T_G is the average number of timesteps to reach the goal (lower the better). ⊥ indicatestimeout. We can see that both our approach and RL generalizes for these benchmarks.
Figure 10: Trajectories taken by our state machine policy (left) and the RL policy (middle) onPendulum for a test environment (i.e., heavier pendulum). Green (resp., red) indicates positive (resp.,negative) torque. Our policy performs optimally by using positive torque when angular velocity ≥ 0and negative torque otherwise. In contrast, the RL policy performs sub-optimally (especially in thebeginning of the trajectory).
Figure 11: Trajectories taken by our state machine policy on Swimmer for (a) a train environmentwith segments of length 1, and (b) a test environment with segments of length 0.75. The colorsindicate different modes. The axes are the x and y coordinates of the center of mass of the swimmer.
Figure 12: Action vs time graphs for the car benchmark for both our policy (red) and the neuralnetwork policy (blue). Left shows the velocity of the agent and Right shows the steering angle.
Figure 13:	Action vs time graphs for the pendulum benchmark (left) and the cartpole benchmark(right) for both our policy (red) and the neural network policy (blue).
Figure 14:	Action vs time graphs for the swimmer benchmark for the three torques at the threedifferent joints of the swimmer. Blue line is for the neural network policy and red line is for the statemachine policy.
Figure 15: Synthesis times (in seconds, wall clock time) for learning state machines policies for thedifferent benchmarks. The plot breaks down the total synthesis time into time taken by the teacher,the student and other miscellaneous parts of the algorithm. Misc. mostly includes the time spent forchecking convergence at every iteration. The plot also shows the number of teacher-student iterationstaken for each benchmark.
Figure 16: Synthesized state-machine policy for the Quad benchmark.
Figure 17: Synthesized state-machine policy for the QuadPO benchmark.
Figure 18: Synthesized state-machine policy for Pendulum.
Figure 19: Synthesized state-machine policy for Cartpole.
Figure 20: Synthesized state-machine policy for Acrobot.
Figure 21: Synthesized state-machine policy for Mountain car.
Figure 22: Synthesized state-machine policy for Swimmer.
