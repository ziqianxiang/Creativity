Figure 1: Illustration of our neuron coreset construction on a toy example: (a) a full network, (b) thecompressed network. Both neurons in the second layer in (b) choose the same coreset comprisingneurons {1, 2, 3, 7} from layer 1, but with different weights. The compressed network has prunedneurons {2, 5, 6} from layer 1.
Figure 2: Approximation error of a single neuron on MNIST dataset across different coreset sizes.
Figure 3: Average accuracy of various algorithms on LeNet-200-105 on MNIST dataset acrossdifferent sparsity rates. Plot (a) shows the results of all tested methods. Plot (b) focuses on thethree top methods. Our method, which constructs neural coresets by applying the Algorithm 2 andCorollary 8 in a layer-by-layer fashion, outperforms other coreset-based algorithms.
Figure 4: Average accuracy over 5 runs of the proposed framework and the uniform baseline onLeNet-300-100 on MNIST dataset across different compression rates. Plot (a) shows the resultsbefore fine-tuning, plot (b)-after fine-tuning. The fine-tuning was done until convergence. Fine-tuning of the uniform sampling almost doubles in time compared to the fine-tuning of the coreset.
Figure 5: (left) Any point on a circle can be separated from the other points via a line. (right) Thesame holds for a circle which is the intersection of a d-dimensional sphere and a hyperplane; seeTheorem 6.
