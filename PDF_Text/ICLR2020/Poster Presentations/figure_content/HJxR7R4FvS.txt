Figure 1: Difference between MLE-based training lossand ranking-based evaluation. For A, -1 × log 0.8 - 1 ×log 0.1 = - log 0.08; For B, -1×log0.3 - 1 ×log0.3 =- log 0.09. NLL assigns a better value to the misrankedexample than to the properly-ranked one. NDCG alwaysassigns maximum value to properly-ranked scorings.
Figure 2: Illustration of learning parameters {φ, θ} in the two different paradigms. (a) Learning withMLE, as in VAEs; (b) Learning with a learned ranking-critic. The actor can be viewed as the functioncomposition of encoder fφ(∙) and gθ(∙) in VAEs. The critic mimics the ranking-based evaluationscores, so that it can provide ranking-sensitive feedback in the actor learning.
Figure 3: Performance improvement (NDCG@100) with RaCT over the VAE baseline.
Figure 4: Correlation between the learning objectives(MLE or RaCT) and evaluation metrics on training.
Figure 5: Ablation study on features.
Figure 6: Correlation between the learning objectives (NLL or RaCT) and evaluation metrics NDCG.
Figure 7: The improvement at various cut-off value R in evaluation. Given a specific R, the dashedline shows the VAE, and square dot shows the RaCT.
Figure 8: Improvement breakdown over different user interactions. (a) Scatter plot betweenNDCG@100 and activity levels. Note only # interactions ≤ 1000 is visualized, there is a long tail(>1000) in the distribution. (b) Comparison of the mean NDCG@100 values for four user groups.
