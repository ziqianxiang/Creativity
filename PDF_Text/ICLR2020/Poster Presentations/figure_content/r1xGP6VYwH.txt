Figure 2: A simple regression task to illustrate the effect of an optimistic initialisation in neuralnetworks. Left: 10 different networks whose final layer biases are initialised at 3 (shown in green),and the same networks after training on the blue data points (shown in red). Right: One of the trainednetworks whose output has been augmented with an optimistic bias as in equation 1. The counts wereobtained by computing a histogram over the input space [-2, 2] with 50 bins.
Figure 3: Results for the randomised chain environment. Median across 20 seeds is plotted and the25%-75% quartile is shown shaded. Left: OPIQ outperforms the baselines. Right: OPIQ is morestable than its ablations.
Figure 4: Results for the maze environment comparing OPIQ and baselines. Median across 8 seedsis plotted and the 25%-75% quartile is shown shaded. Left: The episode reward. Right: Numberof distinct states visited over training. The total number of states in the environment is shown as adotted line.
Figure 6: Values used during action selection for each of the 4 actions. The region in blue indicatesstates that have already been visited. Other colours denote Q-values between 0 (black) and 10(white). Left: The Q-values used by DQN with pseudocounts. Right: Q+-values used by OPIQwith Caction = 100.
Figure 5: Results for the maze environment comparing OPIQ and ablations. Median across 8 seedsis plotted and the 25%-75% quartile is shown shaded. Left: The episode reward. Right: Numberof distinct states visited over training. The total number of states in the environment is shown as adotted line.
Figure 7: Results for Montezuma’s Revenge. Median across 4 seeds is plotted and the 25%-75%quartile is shown shaded. Left: The episode reward. Right: The maximum reward achieved duringan episode.
Figure 8: Further Results for Montezuma’s Revenge showing the number of rooms visited overtraining comparing OPIQ and baselines. Median across 4 seeds is plotted and the 25%-75% quartileis shown shaded.
Figure 9: We consider the counting scheme from Figure 2, but vary the number of bins used. Left: 6bins are used. Only the data points far from the training data are given an optimistic bias. Right: 50bins are used. An optimistic bias is given to all data points that are not very close to the training data.
Figure 10: 100 Chain environment.
Figure 11: Maze environment.
Figure 12: The number of distinct states visited over training for the chain environment. The medianacross 20 seeds is plotted and the 25%-75% quartile is shown shaded.
Figure 13: Comparing the performance of M ∈ {0.1, 0.5, 2, 10} on the chain environment. The besthyperparameter combination for the differing values of M is shown. The median across 20 seeds isplotted and the 25%-75% quartile is shown shaded.
Figure 14: The Q+-values OPIQ used during bootstrapping with Cbootstrap = 0.01.
Figure 15: Results for Montezuma’s Revenge comparing OPIQ and ablation. Medianis plotted and the 25%-75% quartile is shown shaded.
Figure 16:(x+11)M for various values of M, and the indicator function l{x < 1} shown in black.
Figure 17:	A simple failure case for pessimistically initialised greedy Q-learning. There is 1 statewith 2 actions and H = 1. The agent receives 0.1 reward for the left action and 1 for the right action.
Figure 18: The parametrised MDP.
