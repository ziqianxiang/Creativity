Figure 1: Architecture schemes for distributed PPO, IMPALA, and IMPACT. PPO aggregates worker batchesinto a large training batch and the learner performs minibatch SGD. IMPALA workers asynchronously generatedata. IMPACT consists of a batch buffer that takes in worker experience and a target’s evaluation on theexperience. The learner samples from the buffer.
Figure 2: In asynchronous PPO, there are multiple candidate policies from which the trust regioncan be defined: (1) πworkeri , the policy of the worker process that produced the batch of experiences,(2) πlearner, the current policy of the learner process, and (3) πtarget, the policy of a target network.
Figure 3: Training curves of the ablation study on control benchmarks. In (a), the IMPACT objective outperformsother possible ratio choices for the surrogate loss: Ri = πθs-, R2 = -θ^—, R3 =---------------7——π⅛--------V. In (b),1 πtarget,	2 πworkeri,	3	max(πtarget,βπworkeri )	,we show the target network update frequency is robust to a range of choices. We try target network updatefrequency ttarget equal to the multiple (ranging from 1/16 and 16) of n = N ∙ K, the product of the size ofcircular buffer and the replay times for each batch in the buffer.
Figure 4: (a): The Circular Buffer in a nutshell: N and K correspond to buffer size and max times abatch can be traversed. Old batches are replaced by worker-generated batches. (b): The performanceof IMPACT with different K in terms of time. (c): The performance of IMPACT with different K interms of timesteps. IMPACT can achieve greater timestep as well as time efficiency by manipulatingK. K = 2 outperforms all other settings in time and is more sample efficient than K = 1, 4, 16, 32.
Figure 5:	IMPACT outperforms baselines in both sample and time efficiency for Continuous ControlDomains: Hopper, Humanoid, HalfCheetah.
Figure 6:	IMPACT outperforms PPO and IMPALA in both real-time and sample efficiency forDiscrete Control Domains: Breakout, SpaceInvaders, and Pong.
Figure 7:	Performance of IMPACT with respect to the number of workers in both continuous anddiscrete control tasksOff-policy methods, including DDPG and QProp, utilize target networks to stabilize learning the Qfunction (Lillicrap et al., 2015; Gu et al., 2016). This use of a target network is related but differentfrom IMPACT, which uses the network to define a stable trust region for the PPO surrogate objective.
Figure 8:	IMPACT, PPO and IMPALA wallclock time and sample efficiency for Discrete ControlDomains: Qbert, BeamRider, and Gravitar.
Figure 9:	IMPALA to IMPACT: Incrementally Adding PPO Objective, Replay, and Target-WorkerClipping to IMPALA. The experiments are done on the HalfCheetah-v2 gym environment.
Figure 10: Likelihood ratio rt (θ) for different objective functions, including PPO’s. We assume a diagonalGaussian policy for our policy. Left: Corresponding one dimensional action distributions for Worker i, Target,and Master Learner; Right: Ratio values graphed as a function of possible action values. IMPACT with PPOclipping is a lower bound of PPO.
