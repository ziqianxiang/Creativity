Figure 1: Are we making progress? The box-plot illustrates the performance of state-of-the-art few-shotalgorithms on the Mini-ImageNet (Vinyals et al., 2016) dataset for the 1-shot 5-way protocol. The boxesshow the ± 25% quantiles of the accuracy while the notches indicate the median and its 95% confidenceinterval. Whiskers denote the 1.5× interquartile range which captures 99.3% of the probability mass for anormal distribution. The spread of the box-plots are large, indicating that the standard deviations of the few-shotaccuracies is large too. This suggests that progress may be illusory, especially considering that none outperformthe simple transductive fine-tuning baseline discussed in this paper (rightmost).
Figure 2: Mean accuracy of transductive fine-tuning for different query shot, way and support shot.
Figure 3: Comparing the accuracy of transductive fine-tuning (solid lines) vs. support-based initialization(dotted lines) for different datasets, ways (5, 10, 20, 40, 80 and 160) and support shots (1 and 5). Abscissaeare computed using (9) and a Resnet-152 (He et al., 2016b) network trained for standard image classification onthe ImageNet-1k dataset. Each marker indicates the accuracy of transductive fine-tuning on a few-shot episode;markers for support-based initialization are hidden to avoid clutter. Shape of the markers denotes differentways; ways increase from left to right (5, 10, 20, 40, 80 and 160). Size of the markers denotes different supportshot (1 and 5); it increases from the bottom to the top. E.g., the ellipse contains accuracies of different 5-shot10-way episodes for ImageNet-21k. Regression lines are drawn for each algorithm and dataset by combiningthe episodes of all few-shot protocols. This plot is akin to a precision-recall curve and allows comparing twoalgorithms for different few-shot scenarios. The areas in the first quadrant under the fitted regression lines are295 vs. 284 (CIFAR-FS), 167 vs. 149 (FC-100), 208 vs. 194 (Mini-ImageNet), 280 vs. 270 (Tiered-ImageNet)and 475 vs. 484 (ImageNet-21k) for transductive fine-tuning and support-based initialization.
Figure 4: ImageNet-21k is a highly imbalanced dataset. The most frequent class has about 3K images whilethe rarest class has a single image.
Figure 5: t-SNE (Maaten & Hinton, 2008) embedding of the logits for 1-shot 5-way few-shot episode ofMini-ImageNet. Colors denote the ground-truth labels; crosses denote the support samples; circles denote thequery samples; translucent markers and opaque markers denote the embeddings before and after transductivefine-tuning respectively. Even though query samples are far away from their respective supports in the beginning,they move towards the supports by the end of transductive fine-tuning. Logits of support samples are relativelyunchanged which suggests that the support-based initialization is effective.
