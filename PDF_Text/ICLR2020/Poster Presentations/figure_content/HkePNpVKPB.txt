Figure 1: Referential communication game and architectures of the agents.
Figure 2: Illustration of the learning speed of Alice and performance improving speed of Bob whenpre-training is done with various languages of different topological similarities.
Figure 3: Game performance and average topological similarity for the possible resetting strategiesof our proposed iterated learning procedure of 80 generations. In these experiments, I=80 andIg =4000, with all other hyper-parameters following Table 3.
Figure 4: Validation performance and topological similarity with validation size equals eight. NILleads to the evolution of languages which allow agents to perform well on unseen items.
Figure 5: A simple representation of two languages corresponding to topological similarities ofρ = 1 (top) and ρ = 0.5 (bottom).
Figure 6: Probabilitic explanation of different phases in NIL.
Figure 7: Illustration of learning a high-P language and loW-P language.
Figure 8: Numbers of message types from different settings.
Figure 9: Two corner case test. NIL with degenerate initialized means Alice is initialized with adegenerate language at the beginning of each generation. NIL with degenerate mixed means Aliceis initialized with a degenerate language, AND the Di is mixed with Is degenerate language pairs.
Figure 10: The change of EL〜P(l∣NIL) [P(L)] in generation 3 and 6.
Figure 11: Distribution of P(P(L) ∣Di, Ri) through 80 generations. Values of P are divided into tengroups. The distribution ofρ in each generation is smoothed using linear interpolation.
Figure 12: Language evolution of none-reset0.620.53 ∏j0.000.27 q∏Jθ-18 ⅞0.09 æ0.44 Q0.36 3φ6ran3case in a 3D illustration.
Figure 13: Language evolution of resetting-both case in a 3D illustration.
Figure 14: Distribution of ρ(L) at different generations.
Figure 15: Evolution of language with different values of ρ.
