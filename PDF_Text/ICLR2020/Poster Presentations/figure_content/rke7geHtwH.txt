Figure 1: Learning control suite tasks from fixed data. Standard off-policy RL fails on Cheetah,Hopper, Quadruped. Methods that regularize to the data distribution succeed; for hopper the behaviourdistribution is multimodal and a standard behavior modlling prior is broad (see BM[prior]) here onlyABM succeeds. Best trajectory in data marked with a star.
Figure 2: Control suite when using only the first 2k episodes of low-quality data from each run;return for best behavior episode in the data was 718/159/435/728 (left to right). While plain RL isable to learn on walker, learned priors improve performance and stabilize learning for cheetah andquadruped (with overall lower performance than when learning from good data).
Figure 3: Learning curves for MPO with and without behavior extraction priors. We show 4 of the 7tasks here. Five training runs are shown for each parameter setting, evaluation is average reward over50 episodes. Behavioral modelling priors significantly improve performance across all tasks, andABM further improves learning speed and final performance.
Figure 4: (Left) Learning curves for the tasks "bring to corner" and "bring to center". These taskswere learned using only data from the seven intial stacking tasks. The stacking dataset was richenough to learn these new tasks fully offline with ABM. (Right) Simulated Sawyer environment.
Figure 5: (Left) Original and offline learning on the real robot. The original training run takes over200 hours to learn to stack and leave (blue bar, right y-axis) - bottle-necked by slow real-robotepisode generation. Using the complete dataset of interactions in a batch-RL setting with the ABMprior, we are able to re-learn the final task in 12 hours purely from logged data (orange bar, righty-axis), while obtaining a policy that achieves similar performance on all tasks (box plots, distributionof returns over 50 test episodes, orange vs blue, left y-axis). (Right) Real Sawyer environment.
Figure 6: Learning curves for five seeds of training with MPO on the control suite tasks. Datagenerated during these training runs is later used for offline learning.
Figure 7: Detailed plots for the control suite showing the performance of the respective learned prior(left column) and RL policy (right column) for all tasks (one per row).
Figure 8:	All intentions for blocks stacking in simulation. Performance of executing the prior on theleft, performance of the algorithm learned using this prior on the right.
Figure 9:	Learning the value function from the ABM prior policy on block stacking in simulation.
