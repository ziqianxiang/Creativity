Figure 1: Overview of our framework. The details of the leader’s module and the follower’s modulecan be found in Section 4.2 and Section 4.3, respectively. The implement details of each module canbe found in Appendix D.2.1.
Figure 2: An example and a probabilistic graphical model to illustrate our method.
Figure 3: Leader’s reward curves for different tasks (rule-based followers).
Figure 4: The final reward for RL-based followers. No ab-straction means the vanilla RL-based followers.
Figure 5:	Illustration of different tasks.
Figure 6:	Total incentives for predator-prey task and multi-bonus resource collections task.
Figure 7:	Total incentives for resource collections task and navigation task.
Figure 9: The ablation study of sparse EBPG in the predator-prey task.
Figure 8: The ablation study of sparse EBPG in the multi-bonus resource collections task.
Figure 10: Visualization of the attention. The x-axis indicates the commit time for different agentsand the y-axis indicates the corresponding weights w k when the leader commits to that agent.
Figure 11:	The ablation study of reward curves for two time-scale update method in resource col-lections task.
Figure 12:	The ablation study of reward curves for fixed commitment time in resource collectionstask.
Figure 13: Reward curves for RL-based followers in different tasks.
Figure 14:	Leader’s reward curves for different tasks (RL-based followers).
Figure 15:	The final reward of different number of follower agents in different tasks.
