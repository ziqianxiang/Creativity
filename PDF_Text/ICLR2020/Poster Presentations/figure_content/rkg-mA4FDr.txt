Figure 1: Difference between two-tower models and cross-attention models. Following previousworks, we consider [CLS] embedding and average pooling as the aggregatorâ€™s output for the two-tower Transformer model and the two-tower MLP model, respectively.
Figure 2: An illustrative example of the three pre-training tasks where each query q is highlightedin different colors. All queries are paired with the same text block d. Concretely, (q1,d) of ICT isdefined locally within a paragraph; (q2,d) of BFS is defined globally within an article; (q3,d) ofWLP is defined distantly across two related articles hyper-linked by the Wikipedia entity.
