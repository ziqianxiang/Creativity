Figure 1: Paired associative inference. The panel on the left illustrates a memory store filled withrandom pairs of images. The panels to the right illustrate (from left to right) two ’direct’ queries (ABand BC) where no inference is require, and an ’indirect’ query (AC) where inference is requiredOne contribution of this paper is to introduce a task, derived from neuroscience, to carefully probe thereasoning capacity of neural networks. This task is thought to capture the essence of reasoning - theappreciation of distant relationships among elements distributed across multiple facts or memories.
Figure 2: Weights analysis of an inference query in the length 3 PAI task. An example of memorycontent and related inference query is reported in the first column on the left. For clarity we reportimage class ID. Cue and Match are images from the same sequence e.g. A10 - C10, where 10 is theslot ID. The lure is an images presented in the same memory store, but associated with a differentsequence, e.g. C13 . The 3 most right columns report the weights associated with the 3 hops used bythe network, for each probability mass we report the associated retrieved slot.
Figure 3: Analysis of length 3 PAI task. a. Evaluation accuracy on the inference trial A-C; b. Numberof hops taken during training; c. Distribution of evaluation accuracy obtained by averaging directqueries (A-B and B-C). This was obtained over 100 different hyper-parameters and seeds; d. same asc, but on the inference queries (A-C)15Published as a conference paper at ICLR 2020A.3.1 AblationsTable 7: PAI - Ablations - sequence of length 3: A-B-CMEMO Network Architecture			A-C inference trialPositional encoding as in (Vaswani et al., 2017)	Memories kept separated	Recurrent attention w/ Layernorm	Accuracy✓	X	X	57.59(10.11)✓	X	✓	52.79(3.12)X	✓	X	73.26(15.86)X	✓	✓	97.59(1.85)Results for the best run (chosen by validation Set) on the PAI task. X= not present; ✓ = presentA.3.2 Attention weights analysisFigure 4: Attention weights analysis of length 3 PAI task, in the case where the network converged to7 hops. In this case the network uses the first two hops to retrieve the slot where the cue is presentand the the hops number 3, 4 and 5 to retrieve the slot with the match. The weights are sharp and theyfocus only on 1 single slot.
Figure 4: Attention weights analysis of length 3 PAI task, in the case where the network converged to7 hops. In this case the network uses the first two hops to retrieve the slot where the cue is presentand the the hops number 3, 4 and 5 to retrieve the slot with the match. The weights are sharp and theyfocus only on 1 single slot.
Figure 5: Comparison between MEMO + REINFORCE and MEMO + ACT on length 3 PAI task.
