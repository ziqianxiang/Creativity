Figure 1: This figure shows N-LAMB and NN-LAMB can achieve a comparable accuracy comparedto LAMB optimizer. Their performances are much better than momentum solver. The result ofmomentum optimizer was reported by Goyal et al. (2017). For Nadam, we use the learning rate recipeof (Goyal et al., 2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learningrate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).
Figure 2: The figure shows that adam-correction has the same effect as learning rate warmup. Weremoved adam-correction from the LAMB optimizer. We did not observe any drop in the test orvalidation accuracy for BERT and ImageNet training.
Figure 3: We tried different norms in LAMB optimizer. However, we did not observe a significantdifference in the validation accuracy of ImageNet training with ResNet-50. We use L2 norm as thedefault.
Figure 4: Lamb is better than the existing solvers (batch size = 512). We make sure all the solvers arecarefully tuned. The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001,0.0002, 0.0004, 0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1,0.2, 0.4, 0.6, 0.8, 1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}. The momentum optimizer was tunedby the baseline implementer. The weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01,0.1, 1.0}.
Figure 5: Our experiments show that even the validation loss is not reliable in the large-scale training.
Figure 6: This figure shoWs the training loss curve of Lamb optimizer. We just Want to use this figureto shoW that Lamb can make the training converge smoothly. Even if We scale the batch size to theextremely large cases, the loss curves are almost identical to each other.
Figure 7: This figure shows the training loss curve of LAMB optimizer. This figure shows that LAMBcan make the training converge smoothly at the extremely large batch size (e.g. 64K).
Figure 8: We achieve 76.8% scaling efficiency (49 times speedup by 64 times computational resources)and 101.8% scaling efficiency with a mixed, scaled batch size (65.2 times speedup by 64 timescomputational resources). 1024-mixed means the mixed-batch training on 1024 TPUs.
Figure 9: The LAMB trust ratio.
Figure 10: The LAMB trust ratio.
Figure 11: The LAMB trust ratio.
Figure 12: The LAMB trust ratio.
Figure 13: The LAMB trust ratio.
Figure 14: The LAMB trust ratio.
