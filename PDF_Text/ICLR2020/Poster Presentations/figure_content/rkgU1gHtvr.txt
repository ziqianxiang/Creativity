Figure 1: (a) shoWs that scatter plot of pairs (dπtrue , dπ) and pairs (dπesti , dπ). The diagonal lineindicates exact estimation. The default values of the number of trajectories is 200, and the length ofhorizon is 200. (b) shows the weighted total variation distance (TV distance) between d∏true and d∏,d∏esti and d∏ respectively, along different number of trajectories and the length of horizons.
Figure 2: Single-behavior-policy results of BCH, EMP, DualDice and WIS across continuousand discrete environments with average reward. Each node indicates the mean value and the barsrepresents the standard error of the mean.
Figure 3: Multiple-behavior-policy results of BCH (pooled), EMP, DualDice and MIS across continu-ous and discrete environments with average reward.
Figure 4: Multiple-behavior-policy results of EMP (single), EMP, KL-EMP across continuous anddiscrete environments with average reward.
Figure 5: Results of policy-aware OPPE methods (BCH, BCH (pooled) and BCH (KL-pooled))and their corresponding partially policy-agnostic version (EMP (single), EMP and KL-EMP ) acrosscontinuous and discrete environments with average reward.
