Figure 1: Training loss v.s. # ofiterations of Transformers on theDe-En IWSLT’14 dataset.
Figure 2:	The absolute gradient histogram of the Transformers on the De-En IWSLT’ 14 datasetduring the training (stacked along the y-axis). X-axis is absolute value in the log scale and theheight is the frequency. Without warmup, the gradient distribution is distorted in the first 10 steps.
Figure 3:	The histogram of the absolute value of gradients (on a log scale) during the training ofTransformers on the De-En IWSLT’ 14 dataset. using Adam-2k, RAdam and Adam-eps.
Figure 4: Language modeling (LSTMs) on the One Billion Word.
Figure 5: Training of ResNet-18 on the ImageNet and ResNet-20 on the CIFAR10 dataset.
Figure 6: Performance of RAdam, Adam and SGD with different learning rates on CIFAR10.
Figure 7: Performance of RAdam, Adam with warmup on CIFAR10 with different learning rates.
Figure 8: The value of Equation 2,Equation 5 and their difference (abso-lute difference). The x-axis is ρ andthe y-axis is the variance (log scale).
Figure 9: The simulation of Var[*] and Var[Vt]. The x-axisis iteration # (from 5), the y-axis is the variance (log scale).
