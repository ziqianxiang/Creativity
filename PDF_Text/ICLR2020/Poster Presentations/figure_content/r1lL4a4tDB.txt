Figure 1: Diagrams of the proposed algorithm. (a) Overview. (b, c) The generative model and theinference model of a VRM.
Figure 2: Computation graph of the proposed algorithm. (a) The RL controller. (b) The executionphase. (c) The learning phase of a VRM. a: action; z : latent variable; d: RNN state variable; x: rawobservation (including reward); Q: state-action value function; V : state value function. A bar on avariable means that it is the actual value from the replay buffer or the environment. Each stochasticvariable follows a parameterized diagonal Gaussian distribution.
Figure 3:	Learning curves of the classic control tasks. Shaded areas indicate S.E.M..
Figure 4:	Learning curves of the robotic control tasks, plotted in the same way as in Fig. 3.
Figure 5: Learning curves of the sequential target reaching task.
Figure 6: Example tasks showing relationship between average return of the agent and negativeELBO (loss function, dashed) of the keep-learning VRM.
Figure 7: Learning curves of our algorithms and the modified ones.
Figure 8: Robots learned to hop or walk in PO environments using our algorithm. Each panel showstrajectory of a trained agent (randomly selected) within one episode.
Figure 9: Examples of observation predictions by keep-learning VRMs of trained agents.
Figure 10: The learning curves of our algorithm using the hyperparameters for the VRMs used inthe paper (Table 2), and using a range of random hyperparameters (Appendix F). Data are Mean Â±S.E.M., obtained from 20 repeats using different random seeds.
