Figure 1: Generated face images from PGGAN and car images from StyleGAN. Images in evenrows are generated depth images with colormaps. Though the models are trained on unlabeledRGB image datasets, they achieve RGBD image generation as well as explicit control over thecamera poses.
Figure 2: Proposed pipeline. We train the RGBD image generator with the self-supervised 3Dconsistency loss and adversarial loss for RGB channels. The model generates two RGBD imageswith different camera parameters and learns them to be consistent with the 3D world.
Figure 3: Generator architectures tested. PGGAN-based model (left), StyleGAN-based model (mid-dle), and DeepVoxels-based model (right).
Figure 4: Visualization of comparison for the generated images from each model on FFHQ dataset.
Figure 5: Visualization of comparison for the generated images from each model on ShapeNet carimages. Images in each row are generated from the same latent vector z but different azimuth orelevation angles. The images with colormaps are the generated depth images.
Figure 6: Normal map and point cloud visualization for FFHQ and ShapeNet car datasets. Pointclouds in occluded region are not visualized in the figure.
Figure 7: Generated car and bedroom images changing the azimuth angle range.
Figure 8: Comparison between the softmax weighting (left) and our occlusion reasoning algorithm(right). The values denote the voxel weight, and the orange regions are the visible voxels in thecamera. For softmax weighting, the occlusion network needs to change the voxel weight, accordingto the camera location. Our method accumulates the weight along the camera ray and ignored thevoxels where the accumulative values exceed one, thus the occlusion network does not need tochange the weights according to the camera location.
Figure 9: Comparison of the occlusion reasoning algorithms. The proposed method can acquiremore consistent rotation and generate consistent depth maps than the softmax weighting.
Figure 10: Additional results on FFHQ dataset. Images in each row are generated from the same la-tent vector z but different azimuth or elevation angles. The images with colormaps are the generateddepth images.
Figure 11: Additional results on ShapeNet car dataset. Images in each row are generated from thesame latent vector z but different azimuth or elevation angles. The images with colormaps are thegenerated depth images.
Figure 12: Randomly generated RGB images on the FFHQ dataset from PGGAN, with and withoutproposed loss.
Figure 13: Randomly generated RGB images on the ShapeNet car dataset from PGGAN, with andwithout proposed loss.
Figure 14: Randomly generated RGB images on the FFHQ dataset from StyleGAN, with and with-out proposed loss.
Figure 15: Randomly generated RGB images on the ShapeNet car dataset from StyleGAN, with andwithout proposed loss.
Figure 16: Randomly generated RGBwithout proposed loss.
Figure 17: Randomly generated RGB images on the ShapeNet car dataset from DeepVoxels, withand without proposed loss.
