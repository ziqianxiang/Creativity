Figure 1: Our NADST has 3 key components: encoders (“red"), fertility decoder (“blue"), andstate decoder (“green"). (i) Encoders encode sequences of dialogue history, delexicalized dialoguehistory, and domain and slot tokens into continuous representations; (ii) Fertility Decoder has 3attention mechanisms to learn potential dependencies across (domain, slot) pairs in combinationwith dialogue history. The output is used to generate fertilities and slot gates; and (iii) State Decoderreceives the input sequence including sub-sequences of (domain, slot)×fertility to decode a completedialogue state sequence as concatenation of component slot values. For simplicity, we do not showfeedforward, residual connection, and layer-normalization layers in the figure. Best viewed in color.
Figure 2: Comparison of model latency as wall-clock time (in ms) per prediction of complete dia-logue state (not by individual slot). The latency is plotted against the length of the dialogue history.
Figure 3: Comparison of model latency as wall-clock time (in ms) per prediction of complete dia-logue state. The latency is plotted against the length of the dialogue history. For a fair comparison,we compare our models with TRADE (Wu et al., 2019) in 2 cases: original TRADE which decodesdialogue state slot by slot and TRADE* which decodes dialogue state in parallel at slot-level. Herewe plot the base-10 logarithm of latency to show the difference between the 2 cases of TRADE. Wevary our models with different values of number of attention layers T = Tfert = Tstate = 1, 2, 3.
Figure 4: Heatmap visualization of self-attention scores of 5 heads between Zds×fert representa-tions in the state decoder. The corresponding prediction output for each representation is presentedon the right side. The examples are for the 6th turn in dialogue ID MUL0536 (upper row) andPMUL3759 (lower row) in MultiWOZ2.1.
Figure 5: Visualization of all attention heads in the last attention step Tstate in the stateThe DST prediction is done for the 6th turn in dialogue ID MUL0536 in MultiWOZ2.1.
Figure 6: Visualization of all attention heads in the last attention step Tstate in the stateThe DST prediction is done for the 6th turn in dialogue ID PMUL3759 in MultiWOZ2.1.
