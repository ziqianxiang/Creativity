Figure 1: (left) Training architecture for the embedding network (right) NGU’s reward generator.
Figure 2: (Left and Center) Sample screens of Random Disco Maze. The agent is in green, andpathways in black. The colors of the wall change at every time step. (Right) Learning curves forRandom projections vs. learned controllable states and a baseline RND implementation.
Figure 3: Human Normalized Scores on dense reward and hard exploration games.
Figure 4: Human Normalized Scores on the 6 hard exploration games.
Figure 5: Mean episodic return for agents trained (left) Pitfall! and (right) Montezuma’s Revenge.
Figure 6: NGU(N=32) behavior for β0 (blue) and β31 (orange).
Figure 7: Values taken by the {βi}iN=-01 and the {γi}iN=-01 for N = 32 and β = 0.3.
Figure 8: Mean episodic return for agentstrained Pitfall!.
Figure 9: Mean episodic return for agentstrained Montezuma’s Revenge.
Figure 10: Mean episodic return for agents	Figure 11: Mean episodic return for agentstrained Pitfall!.	trained Montezuma’s Revenge.
Figure 12: Mean episodic return for agentstrained Pitfall!.
Figure 13: Mean episodic return for agentstrained Montezuma’s Revenge.
Figure 14: R2D2(Retrace) (green), NGU(N=32) with eval β0.0 (blue) and eval β = 0.3 (orange).
Figure 15: Embedding Network Architecture.
Figure 16: RND Network Architecture.
Figure 17: R2D2 Agent Architecture.
Figure 18: Prediction loss from learned controllable states of the position (top) and room (bottom) onMontezuma’s Revenge against minibatch updates.
Figure 19: Mean episodic return (left) and mean number of visited rooms per episode (right) vsenvironment frames for agents trained Montezuma’s Revenge with hand-crafted controllable states.
