Figure 1: DGN consists of three modules: en-coder, convolutional layer, and Q network. Allagents share weights and gradients are accumu-lated to update the weights.
Figure 2: Illustration of computation of the convolutionallayer with relation kernel of multi-head attention.
Figure 3: Illustration of experimental scenarios: battle (left), jungle (mid), and routing (right).
Figure 4: Learning curves in battle.
Figure 5: Illustration of representative behaviors of DGN and DQN agents in battle and jungle.
Figure 6: Learning curves in jungle.
Figure 7: Learning curves in routing.
Figure 10: Learning curves ofDGN on fixed graph and unfixedgraph in battle.
Figure 11: Learning curves ofDGN, ATOC and TarMAC in bat-tle.
Figure 12: DGN versus Floyd WithBL under increasingly heavier traf-fic in routing.
Figure 13: Learning curves ofDGN, DGN-R, and DGN-M injungle.
Figure 14: Learning curves ofDGN, DGN-R, and DGN-M inrouting.
