Figure 1: Stochastic computation graph of the proposedobjective JÏ€ . The stochastic nodes are represented bycircles and the deterministic ones by squares.
Figure 2: Comparison against state-of-the-art model-free and model-based baselines in four different MuJoCoenvironments. Our method, MAAC, attains better sample efficiency and asymptotic performance than previousapproaches. The gap in performance between MAAC and previous work increases as the environments increasein complexity.
Figure 3: L1 error on the policy gradient when usingthe proposed objective for different values of the horizonH as well as the error obtained when using the truedynamics. The results correlate with the assumptionthat the error in the learned dynamics is lower than theerror in the Q-function, as well as they correlate with thederived bounds.
Figure 4: Ablation test of our method. We test the importance of several components of our method: not usingthe model to train the policy (H = 0), not using the STEVE targets for training the Q-function (-STEVE), andusing a single sample estimate of the pathwise derivative. Using the model is the component that affects themost the performance, highlighting the importance of our derived estimator.
Figure 5: We further test the significance of some components of our method: not use the dynamics to generatedata, and only use real data sampled from environments to train policy and Q-functions (real_data), removeentropy from optimization objects (no_entropy), and using a single sample estimate of the pathwise derivativebut increase the batch size accordingly (5x batch size). Considering entropy and using dynamic models toaugment data set are both very important.
