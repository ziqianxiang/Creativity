Figure 1: We transform each planned candidate action trajectory with PCA into a 2D blue scatter.
Figure 2:	Performance curves on different bench-marking environments. 4 random seeds are run foreach environment. The full figures of all 12 MuJoCo environments are summarized in appendix 8.
Figure 3:	The MPC control and policy control performance of the proposed POPLIN-A, and POPLIN-P with its three training schemes, which are namely behavior cloning (BC), generative adversarialnetwork training (GAN) and setting parameter average (Avg).
Figure 4:	The performance of PETS, POPLIN-A, POPLIN-P using different population size ofcandidates on Cheetah. The variance of the candidates trajectory σ in POPLIN-P is set to 0.1.
Figure 5: The reward optimization surface in the solution space. The expected reward is higher fromcolor blue to color red. We visualize candidates using different colors as defined in the legend. Thefull results can be seen in appendix A.7.
Figure 7: The ablation study of of POPLIN-A, POPLIN-P-BC, POPLIN-P-Avg, POPLIN-P-GAN.
Figure 9:	The planning performance and the testing performance of the proposed POPLIN-A, andPOPLIN-P with its three training schemes, which are namely behavior cloning (BC), generativeadversarial network training (GAN) and setting parameter average (Avg).
Figure 10:	The performance of POPLIN-A, POPLIN-P-BC, POPLIN-P-Avg, POPLIN-P-GAN usingdifferent hyper-parameters. The tested environment is Cheetah.
Figure 11: The performance of POPLIN-A, POPLIN-P, and PETS of different random seeds onCheetah environment.
Figure 12:	The performance of PETS, POPLIN-A, POPLIN-P-Avg, POPLIN-P-BC and POPLIN-P whose network has fixed parameters of zeros. The variance of the candidates trajectory σ inPOPLIN-P is set to 0.1. The tested environment is Cheetah.
Figure 13:	Reward surface in solution space (action space) for PETS algorithm.
Figure 14: Reward surface in solution space (action space) for POPLIN-A-Replan.
Figure 15: Reward surface in solution space (action space) for POPLIN-A-Init.
Figure 16: Reward surface in solution space (parameter space) for POPLIN-P with 0 hidden layer.
Figure 17: Reward surface in solution space (parameter space) for POPLIN-P using 1 hidden layer.
Figure 18: The color indicates the expected cost (negative of expected reward). We emphasis that allthese figures are visualized in the action space. And all of them are very unsmooth. For the figuresvisualized in solution space, We refer to Figure 13.
Figure 19: The figures are the planned trajectories of PETS.
Figure 20: The figures are the planned trajectories of POPLIN-P using 1 hidden layer MLP.
Figure 21: The figures are the planned trajectories of POPLIN-P using 0 hidden layer MLP.
