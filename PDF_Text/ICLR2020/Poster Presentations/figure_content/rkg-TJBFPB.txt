Figure 1: RIDE rewards the agent for actions that have an impact on the state representation (RI DE),which is learned using both a forward (Lfw) and an inverse dynamics (Linv) model.
Figure 2: Rendering of aprocedurally-generated en-vironment from MiniGrid'sMultiRoomN12S10 task.
Figure 3: Performance of RIDE, Count, RND, ICM and IMPALA on a variety of hard explorationproblems in MiniGrid. Note RIDE is the only one that can solve the hardest tasks.
Figure 4: Intrinsic reward heatmaps for RND, ICM, and RIDE (from left to right) for opening doors(green), moving forward (blue), or turning left or right (red) on a random environment from theMultiRoomN7S4 task. A is the agent's starting position, G is the goal position and D are doors thathave to be opened on the way.
Figure 5: Mean intrinsic re-ward for models trained on Multi-RoomN12S10.
Figure 6: Training on a singletoninstance of ObstructedMaze2Dlh.
Figure 7: State visitation heatmaps for Count, RND, ICM, Random, and RIDE models (from leftto right) trained for 50m frames without any extrinsic reward on a singleton maze (top row) and onprocedurally-generated mazes (bottom row) in MultiRoomN10S6.
Figure 8: Performance on Mario with intrinsic reward only (a), with intrinsic and extrinsic reward(b), and VizDoom (c). Note that IMPALA is trained with extrinsic reward only in all cases.
Figure 9: Comparison between the performance of RIDE and three ablations: OnlyEpisodicCounts,NoEpisodicCounts, and NoEntropyNoEpisodicCounts.
Figure 10: Average number of states visited during an episode of MultiRoomN12S10, measuredat different training stages for our full RIDE model (blue) and the NoEpisodicCounts ablation (or-ange).
Figure 11: State visitation heatmaps for Count, RND, ICM, and RIDE (from left to right) trainedfor 100m frames on MultiRoomN12S10.
Figure 12: Intrinsic reward heatmaps for RND, ICM, and RIDE (from left to right) on Multi-RoomN12S10.
Figure 13: Intrinsic reward heatmaps for RND (left) and RIDE (right) for interacting with objects(i.e. open doors, pick up / drop keys or balls) (green), moving forward (blue), or turning left or right(red) on a random map from ObstructedMaze2Dlh. A is the agent's starting position, K are the keyshidden inside boxes (that need to be opened in order to see their colors), D are colored doors thatcan only be opened by keys with the same color, and B is the ball that the agent needs to pick up inorder to win the game. After passing through the door the agent also needs to drop the key in orderto be able to pick up the ball since it can only hold one object at a time.
Figure 14: Performance on DynamicObstacles with varying degrees of difficulty.
Figure 15: Performance on a version of the MiniGridRoomN7S4 in which the colors of the wallsand goals are randomly picked from a set of 4 colors at the beginning of each episode.
