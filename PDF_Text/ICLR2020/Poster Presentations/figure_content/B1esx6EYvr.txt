Figure 1: Single-image self-supervision. Weshow that several self-supervision methods can beused to train the first few layers of a deep neuralnetworks using a single training image, such asthis Image A, B or even C (above), provided thatsufficient data augmentation is used.
Figure 2: conv1 filters trained using a single image. The 96 learned (3 × 11 × 11) filters for thefirst layer of AlexNet are shown for each single training image and method along with their linearclassifier performance. For visualization, each filter is normalized to be in the range of (-1, 1).
Figure 3: Style transfer with single-image pre-training. We show two style transfer results us-ing the Image A trained BiGAN and the ImageNetpretrained AlexNet.
Figure 4: ImageNet images for the N = 10 experiments.
Figure 5: Filter visualization. We show activation maximization (left) and retrieval of top 9 ac-tivated images from the training set of ImageNet (right) for four random non-cherrypicked targetfilters. From top to bottom: conv1-5 of the BiGAN trained on a single image A. The filter visual-ization is obtained by learning a (regularized) input image that maximizes the response to the targetfilter using the library Lucid (Olah et al., 2018).
Figure 6: Linear Classifiers on ImageNet. Classification accuracies of linear classifiers trained onthe representations from Table 2 are shown in absolute scale.
Figure 7: Example crops of Image A (N = 1) dataset.
Figure 8: Example crops of Image B (N = 1) dataset. 50 samples were selected randomly.
Figure 10: Example crops of kilo (N = 1000) dataset. 50 samples were selected randomly.
