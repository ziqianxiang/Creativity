Figure 1: (a) Diagram of the supervised self-play (S2P) procedure (phases 1-3) and the testingprocedure considered in this work (phase 4). (b) The environments considered in this paper (Sec. 4).
Figure 2: A visual representa-tion of the different S2P meth-ods.
Figure 3: Results from training 50 S2P agents onthe IBR game with |D| = 10000. (a) The agentshave a range of predictions on many images. (b)When playing with each other, the agents exhibituneven performance (color is mean reward, yellowis higher), indicating policy variability.
Figure 4: (a) Left: In the OR game, best performance (number of total samples required to achieve95% test accuracy, lower is better) for S2P is achieved when all of the samples are in the seed. 0 on thex-axis corresponds to sp2sup and Optimal is the actual (minimum) number of samples required tosolve this optimization problem (see Appenix B). Right: This is also the case in the IBR game, whereperformance is measured by the generalization accuracy using 10k total training samples (higher isbetter). (b) Adding more samples to initial supervised learning in the IBR game improves agents’generalization to L*. (C) EVen When We learn the perfect distribution with emergent communicationin the OR game, it still performs worse than Pop-S2P (using sup2sp S2P).
Figure 5: Results from the OR game with 1 property and 10 types. When the supervised updatesare performed first (supervised data available for words 0 - 3), then the self-play updates makesensible predictions for the unknown words 4 - 7. When the self-play updates are performed first,the subsequent supervised updates merely correct the predictions for words 1 - 4, without enforcingthe constraint that each word should result in a separate type to solve the task.
Figure 6: S2P (sched) out-performs the supervised base-line in the IBR game, and isin turn outperformed by Pop-S2P.
Figure 7: (a) Comparing test performances of different S2P methods on the IBR game. For eachmethod, we picked the model that gave the best performance on Dval. (b) 2D visualization of S2P(Sched) performance over the course of training, in terms of performance on L (vertical axis) andperformance in self-play (horizontal axis). The zig-zag patterns indicates that most self-play updatesresult in a short-term decrease in target language performance. (c) Visualization of the role of thesupervised and self-play updates in sched S2P.
Figure 8: Training curves for various S2P methods in the IBR game described in §4.
