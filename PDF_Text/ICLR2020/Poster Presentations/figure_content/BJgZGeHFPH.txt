Figure 1: A 1D environment.
Figure 2: Computational architecture for training the DynE encoders ea and es . The encodersare trained to minimize the information content of the learned embeddings while still allowing thepredictor f to make accurate predictions.
Figure 3: The distribution of state distances reached by uniform random exploration using DynEactions (k = 4) or raw actions in Reacher Vertical. Left: Randomly selecting a 4-step DynE actionreaches a state uniformly sampled from those reachable in 4 environment timesteps. Right: Overthe length of an episode (100 steps), random exploration with DynE actions reaches faraway statesvery much more often than exploration with raw actions. The visit ratio shows how frequently DynEexploration reaches a certain distance compared to raw exploration.
Figure 4: The relationship between state representations and task value. Each plot shows the t-SNEdimensionality reduction of a state representation, where each point is colored by its value under anear-optimal policy. (a) The DynE embedding from pixels places states with similar values closetogether. (b) The low-dimensional states, which consist of joint angles, relative positions, andvelocities, have some neighborhoods of similar value, but also many regions of mixed value. (c) Therelationship between the pixel representation and the task value is very complex.
Figure 5: Performance of DynE-TD3 and baselines on two families of environments with low-dimensional observations. Dark lines are mean reward over 8 seeds and shaded areas are bootstrapped95% confidence intervals. Across all the environments, TD3 learns faster with the DynE action spacethan with the raw actions. Within each family of environments, the DynE action space was trainedonly on the simplest task (left).
Figure 6: Performance of TD3 trained with various representations. Learned representations for statewhich incorporate the dynamics make a dramatic difference. SA-DynE converges stably and rapidlyand achieves performance from pixels that nearly equals TD3’s performance from states. Dark linesare mean reward over 8 seeds and shaded areas are bootstrapped 95% confidence intervals.
Figure 7: The Reacher family of environments. ReacherVertical requires the agent to movethe tip of the arm to the red dot. ReacherTurn requires the agent to turn a rotating spinner (darkred) so that the tip of the spinner (gray) is close to the target point (red). ReacherPush requiresthe agent to push the brown box onto the red target point. The initial state of the simulator and thetarget point are randomized for each episode. In each environment the rewards are dense and there isa penalty on the norm of the actions. The robot’s kinematics are the same in each environment butthe state spaces are different.
Figure 8: The 7DoF family of environments. Pusher-v2 requires the agent to use a C-shaped endeffector to push a puck across the table onto a red circle. Striker-v2 requires the agent to usea flat end effector to hit a ball so that it rolls across the table and reaches the goal. Thrower-v2requires the agent to throw a ball to a target using a small scoop. As with the Reacher family, thedynamics of the robot are the same within the 7DoF family of tasks. However, the morphology of therobot, as well as the object it interacts with, is different.
Figure 9: DynE-TD3 results on Reacher Push with varying k. We find that increased temporalabstraction improves performance up to a point, beyond which the action space is no longer able torepresent the optimal policy and performace degrades. Solid points are the mean reward obtainedafter training for 1M environment steps. Shaded bars represent the min and max performance over 4seeds.
Figure 10: The mapping between the outcomes and embeddings of action sequences. We sample10K random sequences of four actions and evaluate their outcomes in the environment dynamics,measured by (∆x, ∆y) = st+4 - st. (a) We plot the outcome (∆x, ∆y) of each action sequenceand color each point according to its location in the plot. (b) We use DynE to embed each actionsequence into two dimensions; each point in this plot corresponds to a point in (a) and takes its colorfrom that corresponding point. The similarity of the two plots and the smooth color gradient in (b)indicate that DynE is embedding action sequences according to their outcomes.
Figure 11: These plots allow for direct comparison between the methods from pixels (PiXel-TD3,VAE-TD3, S-DynE-TD3, and SA-DynE-TD3) and our baselines from low-dimensional states (PPOand SAC). The DynE methods from pixels perform competitively with some baselines from states.
Figure 12: These figures illustrate the way the DynE action space enables more efficient exploration.
