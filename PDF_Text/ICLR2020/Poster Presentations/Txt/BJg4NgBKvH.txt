Published as a conference paper at ICLR 2020
Training B inary Neural Networks with Real-
to-Binary Convolutions
Brais Martinez1, Jing Yang1,2,*, Adrian Bulat1,* & Georgios Tzimiropoulos1,2
1	Samsung AI Research Center, Cambridge, UK
2	Computer Vision Laboratory, The University of Nottingham, UK
{brais.a,adrian.bulat,georgios.t}@samsung.com
Ab stract
This paper shows how to train binary networks to within a few percent points
(~ 3 - 5%) of the full precision counterpart. We first show how to build a strong
baseline, which already achieves state-of-the-art accuracy, by combining recently
proposed advances and carefully adjusting the optimization procedure. Secondly,
we show that by attempting to minimize the discrepancy between the output of
the binary and the corresponding real-valued convolution, additional significant
accuracy gains can be obtained. We materialize this idea in two complemen-
tary ways: (1) with a loss function, during training, by matching the spatial at-
tention maps computed at the output of the binary and real-valued convolutions,
and (2) in a data-driven manner, by using the real-valued activations, available
during inference prior to the binarization process, for re-scaling the activations
right after the binary convolution. Finally, we show that, when putting all of
our improvements together, the proposed model beats the current state of the art
by more than 5% top-1 accuracy on ImageNet and reduces the gap to its real-
valued counterpart to less than 3% and 5% top-1 accuracy on CIFAR-100 and
ImageNet respectively when using a ResNet-18 architecture. Code available at
https://github.com/brais-martinez/real2binary.
1	Introduction
Following the introduction of the BinaryNeuralNet (BNN) algorithm (Courbariaux et al., 2016),
binary neural networks emerged as one of the most promising approaches for obtaining highly effi-
cient neural networks that can be deployed on devices with limited computational resources. Binary
convolutions are appealing mainly for two reasons: (a) Model compression: if the weights of the
network are stored as bits in a 32-bit float, this implies a reduction of 32× in memory usage. (b)
Computational speed-up: computationally intensive floating-point multiply and add operations are
replaced by efficient xnor and pop-count operations, which have been shown to provide practi-
cal speed-ups ofup to 58× on CPU (Rastegari et al., 2016) and, as opposed to general low bit-width
operations, are amenable to standard hardware. Despite these appealing properties, binary neural
networks have been criticized as binarization typically results in large accuracy drops. Thus, their
deployment in practical scenarios is uncommon. For example, on ImageNet classification, there is a
〜18% gap in top-1 accuracy between a ReSNet-18 and its binary counterpart when binarized with
XNOR-Net (Rastegari et al., 2016), which is the method of choice for neural network binarization.
But how far are we from training binary neural networks that are powerful enough to become a
viable alternative to real-valued networks? Our first contribution in this work is to take stock of
recent advances on binary neural networks and train a very strong baseline which already results in
state-of-the-art performance. Our second contribution is a method for bridging most of the remain-
ing gap, which boils down to minimizing the discrepancy between the output of the binary and the
corresponding real-valued convolution. This idea is materialized in our work in two complemen-
tary ways: Firstly, we use an attention matching strategy so that the real-valued network can more
* Denotes equal contribution
1
Published as a conference paper at ICLR 2020
Real block (teacher)
1 × 1 × C
1 × 1 × 亨
1 × 1 × ɑ
1 × 1 × C
1 × 1 × C
Figure 1: Left: The proposed real-to-binary block. The diagram shows how spatial attention maps
computed from a teacher real-valued network are matched with the ones computed from the binary
network. Supervision is injected at the end of each binary block. See also section 4.2. Right: The
proposed data-driven channel re-scaling approach. The left-hand side branch corresponds to the
standard binary convolution module. The right-hand side branch corresponds to the proposed gating
function that computes the channel-scaling factors from the output of the batch normalization. The
factor r controls the compression ratio on the gating function, and H, W and C indicate the two
spatial and the channel dimensions of the activation tensors. See also section 4.3.
closely guide the binary network during optimization. However, we show that due to the architec-
tural discrepancies between the real and the binary networks, a direct application of teacher-student
produces sub-optimal performance. Instead, we propose to use a sequence of teacher-student pairs
that progressively bridges the architectural gap. Secondly, we further propose to use the real-valued
activations of the binary network, available prior to the binarization preceding convolution, to com-
pute scale factors that are used to re-scale the activations right after the application of the binary
convolution. This is in line with recent works which have shown that re-scaling the binary convo-
lution output can result in large performance gains (Rastegari et al., 2016; Bulat & Tzimiropoulos,
2019). However, unlike prior work, we compute the scaling factors in a data-driven manner based on
the real-valued activations of each layer prior to binarization, which results in superior performance.
Overall, we make the following contributions:
•	We construct a very strong baseline by combining some recent insights on training binary
networks and by performing a thorough experimentation to find the most well-suited opti-
mization techniques. We show that this baseline already achieves state-of-the-art accuracy
on ImageNet, surpassing all previously published works on binary networks.
•	We propose a real-to-binary attention matching: this entails that matching spatial attention
maps computed at the output of the binary and real-valued convolutions is particularly
suited for training binary neural networks (see Fig. 1 left and section 4.2). We also devise an
approach in which the architectural gap between real and binary networks is progressively
bridged through a sequence of teacher-student pairs.
•	We propose a data-driven channel re-scaling: this entails using the real-valued activations
of the binary network prior to their binarization to compute the scale factors used to re-
scale the activations produced right after the application of the binary convolution. See
Fig. 1, right, and section 4.3.
•	We show that our combined contributions provide, for the first time, competitive results on
two standard datasets, achieving 76.2% top-1 performance on CIFAR-100 and 65.4% top-1
performance on ImageNet when using a ResNet-18 -a gap bellow 3% and 5% respectively
compared to their full precision counterparts.
2
Published as a conference paper at ICLR 2020
2	Related work
While being pre-dated by other works on binary networks (Soudry et al., 2014), the BNN algo-
rithm (Courbariaux et al., 2016) established how to train networks with binary weights within the
familiar back-propagation paradigm. The training method relies on a real-valued copy of the net-
work weights which is binarized during the forward pass, but is updated during back-propagation
ignoring the binarization step. Unfortunately, BNN resulted in a staggering 〜 28% gap in top-1
accuracy compared to the full precision ResNet-18 on ImageNet.
It is worth noting that binary networks do have a number of floating point operations. In fact, the
output of a binary convolution is not binary (values are integers resulting from the count). Also, in
accordance to other low bit-width quantization methodologies, the first convolution (a costly 7 × 7
kernel in ResNet), the fully connected layer and the batch normalization layers are all real-valued.
In consequence, a line of research has focused on developing methodologies that add a fractional
amount of real-valued operations in exchange for significant accuracy gains. For example, the sem-
inal work of XNOR-Net (Rastegari et al., 2016) proposed to add a real-valued scaling factor to each
output channel of a binary convolution, a technique that has become standard for binary networks.
Similarly, Bi-Real Net (Liu et al., 2018) argued that skip connections are fundamental for binary
networks and observed that the flow of full precision activations provided by the skip connections is
interrupted by the binary downsample convolutions. This degrades the signal and make subsequent
skip connections less effective. To alleviate this, they proposed making the downsample layers real
valued, obtaining around 3% accuracy increase in exchange for a small increase in computational
complexity.
Improving the optimization algorithm for binary networks has been another fundamental line of
research. Examples include the use of smooth approximations of the gradient, the use of PReLU
(Bulat et al., 2019), a two-stage training which binarizes the weights first and then the activations
(Bulat et al., 2019) and progressive quantization (Gong et al., 2019; Bulat et al., 2019). The work in
(Wang et al., 2019) proposed to learn channel correlations through reinforcement learning to better
preserve the sign of a convolution output. A set of regularizers are added to the loss term in (Ding
et al., 2019) so as to control the range of values of the activations, and guarantee good gradient
flow. Other optimization aspects, such the effect of gradient clipping or batch-norm momentum,
were empirically tested in (Alizadeh et al., 2019). In section 4.1, we show how to combine many of
the insights provided in these works with standard optimization techniques to obtain a very strong
baseline that already achieves state-of-the-art accuracy.
While the aforementioned works either maintain the same computational cost, or increase it by a
fractional amount, other research has focused instead on relaxing the problem constraints by in-
creasing the number of binary operations by a large amount, typically a factor of 2 to 8 times.
Examples include ABC-Net (Lin et al., 2017), the structure approximation of (Zhuang et al., 2019),
the circulant CNN of (Liu et al., 2019), and the binary ensemble of (Zhu et al., 2019). Note that
the large increase of binary operations diminishes the efficiency claim that justifies the use of binary
networks in first place. Furthermore, we will show that there is still a lot of margin in order to bridge
the accuracy gap prior to resorting to scaling up the network capacity1.
The methodology proposed in this paper has some relations with prior work: our use of atten-
tion matching as described in section 4.2 is somewhat related to the feature distillation approach
of (Zhuang et al., 2018). However, (Zhuang et al., 2018) tries to match whole feature maps of the
to-be-quantized network with the quantized feature maps of a real-valued network that is trained
in parallel with the to-be-quantized network. Such an approach is shown to improve training of
low-bitwidth quantized models but not binary networks. Notably, our approach based on matching
attention maps is much simpler and shown to be effective for the case of binary networks.
Our data-driven channel re-scaling approach, described in section 4.3, is related to the channel re-
scaling approach of XNOR-Net, and also that of (Xu & Cheung, 2019; Bulat & Tzimiropoulos,
2019), which propose to learn the scale factors discriminatively through backpropagation. Contrary
to (Xu & Cheung, 2019; Bulat & Tzimiropoulos, 2019), our method is data-driven and avoids using
1There is also a large body of work focusing on other low-bit quantization strategies, but a review of these
techniques goes beyond the scope of this section.
3
Published as a conference paper at ICLR 2020
fixed scale factors learnt during training. Contrary to XNOR-Net, our method discriminatively learns
how to produce the data-driven scale factors so that they are optimal for the task in hand.
3	Background
This section reviews the binarization process proposed in (Courbariaux et al., 2016) and its improved
version from (Rastegari et al., 2016), which is the method of choice for neural network binarization.
We denote by W ∈ Ro×c×k×k and A ∈ Rc×win×hin the weights and input features ofaCNN layer,
where o and c represent the number of output and input channels, k the width and height of the
kernel, and win and hin represent the spatial dimension of the input features A. In (Courbariaux
et al., 2016), both weights and activations are binarized using the sign function and then convolution
is performed as A * W ≈ Sign(A) ∈) Sign(W) where ∈) denotes the binary convolution, which can
be implemented using bit-wise operations.
However, this direct binarization approach introduces a high quantization error that leads to low
accuracy. To alleviate this, XNOR-Net (Rastegari et al., 2016) proposes to use real-valued scaling
factors to re-scale the output of the binary convolution as
A * W ≈ (sign(A) * sign(W))	Kα,	(1)
where denotes the element-wise multiplication, α and K are the weight and activation scaling
factors, respectively, calculated in Rastegari et al. (2016) in an analytic manner. More recently,
Bulat & Tzimiropoulos (2019) proposed to fuse α and K into a single factor Γ that is learned via
backpropagation, resulting in further accuracy gains.
4	Method
This section firstly introduces our strong baseline. Then, we present two ways to improve the ap-
proximation of Eq. 1: Firstly, we use a loss based on matching attention maps computed from the
binary and a real-valued network (see section 4.2). Secondly, we make the scaling factor a function
of the real-valued input activations A (see section 4.3).
4.1	Building a strong baseline
Currently, almost all works on binary networks use XNOR-Net and BNN as baselines. In this sec-
tion, we show how to construct a strong baseline by incorporating insights and techniques described
in recent works as well as standard optimization techniques. We show that our baseline already
achieves state-of-the-art accuracy. We believe this is an important contribution towards understand-
ing the true impact of proposed methodologies and towards assessing the true gap with real-valued
networks. Following prior work in binary networks, we focus on the ResNet-18 architecture and
apply the improvements listed below:
Block structure: It is well-known that a modified ResNet block must be used to obtain optimal
results for binary networks. We found the widely-used setting where the operations are ordered as
BatchNorm → Binarization → BinaryConv → Activation to be the best. The skip connection is the
last operation of the block (Rastegari et al., 2016). Note that we use the sign function to binarize the
activations. However, the BatchNorm layer includes an affine transformation and this ordering of
the blocks allows its bias term act as a learnable binarization threshold.
Residual learning: We used double skip connections, as proposed in (Liu et al., 2018).
Activation: We used PReLU (He et al., 2015) as it is known to facilitate the training of binary
networks (Bulat et al., 2019).
Scaling factors: We used discriminatively learnt scaling factors via backpropagation as in (Bulat &
Tzimiropoulos, 2019).
Downsample layers: We used real-valued downsample layers (Liu et al., 2018). We found the
large accuracy boost to be consistent across our experiments (around 3 - 4% top-1 improvement on
ImageNet).
4
Published as a conference paper at ICLR 2020
We used the following training strategies to train our strong baseline:
Initialization: When training binary networks, it is crucial to use a 2-stage optimization strat-
egy (Bulat et al., 2019). In particular, we first train a network using binary activations and real-valued
weights, and then use the resulting model as initialization to train a network where both weights and
activations are binarized.
Weight decay: Setting up weight decay carefully is surprisingly important. We use 1e - 5 when
training stage 1 (binary activation and real weights network), and set it to 0 on stage 2 (Bethge et al.,
2019). Note that weights at stage 2 are either 1 or -1, so applying an L2 regularization term to them
does not make sense.
Data augmentation: For CIFAR-100 we use the standard random crop, horizontal flip and rotation
(±15°). For ImageNet, We found that random cropping, flipping and colour jitter augmentation
worked best. However, colour jitter is disabled for stage 2.
Mix-up: We found that mix-up (Zhang et al., 2017) is crucial for CIFAR-100, While it slightly hurts
performance for ImageNet - this is due to the higher risk of overfitting on CIFAR-100.
Warm-up: We used Warm-up for 5 epochs during stage 1 and no Warm-up for stage 2.
Optimizer: We used Adam (Kingma & Ba, 2014) With a stepWise scheduler. The learning rate is
set to 1e - 3 for stage 1, and 2e - 4 for stage 2. For CIFAR-100, We trained for 350 epochs, With
steps at epochs 150, 250 and 320. For ImageNet, We train for 75 epochs, With steps at epochs 40, 60
and 70. Batch sizes are 256 for ImageNet and 128 for CIFAR-100.
4.2	Real-to-Binary Attention Matching
We make the reasonable assumption that if a binary netWork is trained so that the output of each
binary convolution more closely matches the output ofa real convolution in the corresponding layer
of a real-valued netWork, then significant accuracy gains can be obtained. Notably, a similar as-
sumption Was made in (Rastegari et al., 2016) Where analytic scale factors Were calculated so that
the error betWeen binary and real convolutions is minimized. Instead, and inspired by the attention
transfer method of (Zagoruyko & Komodakis, 2017), We propose to enforce such a constraint via a
loss term at the end of each convolutional block by comparing attention maps calculated from the
binary and real-valued activations. Such supervisory signals provide the binary netWork With much-
needed extra guidance. It is also Well-knoWn that backpropagation for binary netWorks is not as
effective as for real-valued ones. By introducing such loss terms at the end of each block, gradients
do not have to traverse the Whole netWork and suffer a degraded signal.
Assuming that attention matching is applied at a set of J transfer points Within the netWork, the
total loss can be expressed as:
Latt=Xk岛-⅛k,
(2)
Where Qj = Pic=1 |Ai|2 and Ai is the i-th channel of activation map A. Moreover, at the end of
the netWork, We apply a standard logit matching loss (Hinton et al., 2015).
Progressive teacher-student: We observed that teacher and student having as similar architecture
as possible is very important in our case. We thus train a sequence of teacher-student pairs that
progressively bridges the differences betWeen the real netWork and the binary netWork in small
increments:
Step 1: the teacher is the real-valued netWork With the standard ResNet architecture. The student is
another real-valued netWork, but With the same architecture as the binary ResNet-18 (e.g. double
skip connection, layer ordering, PReLU activations, etc). Furthermore, a soft binarization (a Tanh
function) is applied to the activations instead of the binarization (sign) function. In this Way the
netWork is still real-valued, but it behaves more closely to a netWork With binary activations.
Step 2: The netWork resulting from the previous step is used as the teacher. A netWork With binary
activations and real-valued Weights is used as the student.
Step 3: The netWork resulting from step 2 is used as the teacher and the netWork With binary Weights
and binary activations is the student. In this stage, only logit matching is used.
5
Published as a conference paper at ICLR 2020
4.3	Data-driven channel re-scaling
While the approach of the previous section provides better guidance for the training of binary net-
works, the representation power of binary convolutions is still limited, hindering its capacity to
approximate the real-valued network. Here we describe how to boost the representation capability
of a binary neural network and yet incur in only a negligible increment on the number of operations.
Previous works have shown the effectiveness of re-scaling binary convolutions with the goal of
better approximating real convolutions. XNOR-Net (Rastegari et al., 2016) proposed to compute
these scale factors analytically while (Bulat & Tzimiropoulos, 2019; Xu & Cheung, 2019) proposed
to learn them discriminatively in an end-to-end manner, showing additional accuracy gains. For the
latter case, during training, the optimization aims to find a set of fixed scaling factors that minimize
the average expected loss for the training set. We propose instead to go beyond this and obtain
discriminatively-trained input-dependent scaling factors - thus, at test time, these scaling factors
will not be fixed but rather inferred from data.
Let us first recall what the signal flow is when going through a binary block. The activations entering
a binary block are actually real-valued. Batch normalization centers the activations, which are then
binarized, losing a large amount of information. Binary convolution, re-scaling and PReLU follow.
We propose to use the full-precision activation signal, available prior to the large information loss
incurred by the binarization operation, to predict the scaling factors used to re-scale the output of
the binary convolution channel-wise. Specifically, we propose to approximate the real convolution
as follows:
A * W ≈ (Sign(A) (J) sign(W)) Θ α Θ G(A; Wg),	(3)
where WG are the parameters of the gating function G. Such function computes the scale factors
used to re-scale the output of the binary convolution, and uses the pre-convolution real-valued acti-
vations as input. Fig. 1 shows our implementation of function G. The design is inspired by Hu et al.
(2018), but we use the gating function to predict ahead rather than as a self-attention mechanism.
An optimal mechanism to modulate the output of the binary convolution clearly should not be the
same for all examples as in Bulat & Tzimiropoulos (2019) or Xu & Cheung (2019). Note that
in Rastegari et al. (2016) the computation of the scale factors depends on the input activations.
However the analytic calculation is sub-optimal with respect to the task at hand. To circumvent the
aforementioned problems, our method learns, via backpropagation for the task at hand, to predict
the modulating factors using the real-valued input activations. By doing so, more than 1/3 of the
remaining gap with the real-valued network is bridged.
4.4	Computational Cost Analysis
Table 1 details the computational cost of the different binary network methodologies. We differen-
tiate between the number of binary and floating point operations, including operations such as skip
connections, pooling layers, etc. It shows that our method leaves the number of binary operations
constant, and that the number of FLOPs increases by only 1% of the total floating point operation
count. This is assuming a factor r of 8, which is the one used in all of our experiments. To put
this into perspective, the magnitude is similar to the operation increase incurred by the XNOR-Net
with respect to its predecessor, BNN. Similarly, the double skip connections proposed in (Liu et al.,
2018) adds again a comparable amount of operations. Note however that in order to fully exploit
the computational efficiency of binary convolutions during inference, a specialized engine such as
(Zhang et al., 2019; Yang et al., 2017) is required.
5	Results
We present two main sets of experiments. We used ImageNet (Russakovsky et al., 2015) as a bench-
mark to compare our method against other state-of-the-art approaches in Sec. 5.1. ImageNet is the
most widely used dataset to report results on binary networks and, at the same time, allows us to
show for the first time that binary networks can perform competitively on a large-scale dataset. We
further used CIFAR-100 (Krizhevsky & Hinton, 2009) to conduct ablation studies (Sec. 5.2).
6
Published as a conference paper at ICLR 2020
MethOd		BOPS	FLOPS
BNN (Courbariaux et al., 2016)	1.695×109	1.314×108 ：
XNOR-Net (Rastegari et al., 2016)	1.695 ×109	1.333×108
Double Skip ((LiU et al., 2018)	1.695 ×109	1.351×108
Bi-Real (Liu etal., 2018)	1.676 ×109	1.544 ×108
Ours	1.676 ×109	1.564 ×108
Full Precision	0	1.826×109 ■
Table 1: Breakdown of floating point and binary operations for variants of binary ResNet-18.
5.1	Comparison with the State-of-the-Art
Table 2 shows a comparison between our method and relevant state-of-the-art methods, including
low-bit quantization methods other than binary.
Vs. other binary networks: Our strong baseline already comfortably achieves state-of-the art
results, surpassing the previously best-reported result by about 1% (Wang et al., 2019). Our full
method further improves over the state-of-the-art by 5.5% top-1 accuracy. When comparing to
binary models that scale the capacity of the network (second set of results on Tab. 2), only (Zhuang
et al., 2019) outperforms our method, surpassing it by 0.9% top-1 accuracy - yet, this is achieved
using 4 times the number of binary blocks.
Vs. real-valued networks: Our method reduces the performance gap with its real-valued counter-
part to 〜 4% top-1 accuracy, or 〜 5% if We compare against a real-valued network trained with
attention transfer.
Vs. other low-bit quantization: Table 2 also shows a comparison to the state-of-the-art for low-bit
quantization methods (first set of results). It can be seen that our method surpasses the performance
of all methods, except for TTQ (Zhu et al., 2017), which uses 2-bit weights, full-precision activations
and 1.5 the channel width at each layer.
5.2	Ablation Studies
In order to conduct a more detailed ablation study we provide results on CIFAR-100. We thoroughly
optimized a ResNet-18 full precision network to serve as the real-valued baseline.
Teacher-Student effectiveness: We trained a real-valued ResNet-18 using ResNet-34 as its teacher,
yielding 〜1% top-1 accuracy increase. Instead, our progressive teacher-student strategy yields
〜5% top-1 accuracy gain, showing that it is a fundamental tool when training binary networks,
and that its impact is much larger than for real-valued networks, where the baseline optimization is
already healthier.
Performance gap to real-valued: We observe that, for CIFAR-100, we close the gap with real-
valued networks to about 2% when comparing with the full-precision ResNet-18, and to about 3%
when optimized using teacher supervision. The gap is consistent to that on ImageNet in relative
terms: 13% and 10% relative degradation on ImageNet and CIFAR-100 respectively.
Binary vs real downsample: Our proposed method achieves similar performance increase irrespec-
tive of whether binary or real-valued downsample layers are used, the improvement being 5.5% and
6.6% top-1 accuracy gain respectively. It is also interesting to note that the results on the ablation
study are consistent for all entries on both cases.
Scaling factors and attention matching: It is also noteworthy that the gating module is not effec-
tive in the absence of attention matching (see SB+G entries). It seems clear from this result that both
are interconnected: the extra supervisory signal is necessary to properly guide the training, while the
extra flexibility added through the gating mechanism boosts the capacity of the network to mimic
the attention map.
7
Published as a conference paper at ICLR 2020
Method	ImageNet		
	Bitwidth (W/A)	Top-1	Top-5
BWN (Rastegari et al., 2016)	1/32 =	60.8	83.0
TTQ (ZhU et al., 2017)	2/32	66.6	87.2
HWGQ (Cai et al., 2017)	1/2	59.6	82.2
LQ-Net (Zhang etal.,2018)	1/2	62.6	84.3
SYQ (Faraone etal.,2018)	1/2	55.4	78.6
DOREFA-Net (Zhou et al., 2016)	2/2	62.6	84.4
ABC-Net (Lin et al., 2017)	(1/1)x5	65.0	85.9
Circulant CNN (LiU et al., 2019)	(1∕1)×4	61.4	82.8
Struct APPr (Zhuang et al., 2019)	(1∕1)×4	64.2	85.6
Struct Appr** (Zhuang et al., 2019)	(1∕1)×4	66.3	86.6
Ensemble (ZhU et al., 2019)	(1∕1)×6	61.0	—
BNN (Courbariaux et al., 2016)	171	42.2	69.2
XNOR-Net (Rastegari et al., 2016)	1/1	51.2	73.2
Trained Bin (XU & Cheung, 2019)	1/1	54.2	77.9
Bi-Real Net (Liu et al., 2018)**	1/1	56.4	79.5
CI-Net (Wang et al., 2019)	1/1	56.7	80.1
XNOR-Net++ (Bulat & Tzimiropoulos, 2019)	1/1	57.1	79.9
CI-Net (Wang et al., 2019)**		1/1	59.9	84.2
Strong Baseline (ours)**	171	60.9	83.0
Real-to-Bin (ours)**	1/1	65.4	86.2
Real valued	32/32	69.3	89.2
Real valued T-S	32/32	70.7	90.0
Table 2: Comparison with state-of-the-art methods on ImageNet. ** indicates real-valued down-
sample. The second column indicates the number of bits used to represent weights and activations.
Methods include low-bit quantization (upper section), and methods multiplying the capacity of the
network (second section). For the latter case, the second column includes the multiplicative factor
of the network capacity used.
MethOd		Stage 1	Stage 2
	Top-1 / Top-5	Top-1 /Top-5
Strong Baseline	69.3/88.7 =	68.0/88.3 =
SB + Att Trans	72.2/90.3	71.1/90.1
SB + Att Trans + HKD	73.1/91.2	71.9/90.9
SB + G	67.2/87.0	66.2 / 86.8
SB + Progressive TS	73.8/91.5	72.3/89.8
Real-to-Bin	75.0 / 92.2	73.5 / 91.6
Strong Baseline**	72.1/89.9	69.6/89.2
SB + AttTrans**	74.3/91.3	72.6/91.4
SB + AttTrans + HKD**	75.4 / 92.2	73.9/91.2
SB + G**	72.0/89.8	70.9 / 89.3
SB + Progressive TS**	75.7/92.1	74.6/91.8
Real-to-Bin**	76.5 / 92.8	76.2 / 92.7
Full Prec (our impl.)	78.3/93.6	
Full Prec + TS (our impl.)		79.3/94.4		
Table 3: Top-1 and Top-5 classification accuracy using ResNet-18 on CIFAR-100. ** indicates
real-valued downsample layers. G indicates that the gating function of Sec. 4.3 is used.
8
Published as a conference paper at ICLR 2020
6	Conclusion
In this work we showed how to train binary networks to within a few percent points of their real-
valued counterpart, turning binary networks from hopeful research into a compelling alternative to
real-valued networks. We did so by training a binary network to not only predict training labels, but
also mimic the behaviour of real-valued networks. To this end, we devised a progressive attention
matching strategy to drive optimization, and combined it with a gating strategy for scaling the output
of binary convolutions, increasing the representation power of the convolutional block. The two
strategies combine perfectly to boost the state-of-the-art of binary networks by 5.5 top-1 accuracy
on ImageNet, the standard benchmark for binary networks.
References
Milad Alizadeh,Javier Fernandez-Marques, Nicholas D. Lane, and Yarin GaL An empirical study of
binary neural networks’ optimisation. In International Conference on Learning Representations,
2019.
Joseph Bethge, Haojin Yang, Marvin Bornstein, and Christoph Meinel. Back to simplicity: How to
train accurate BNNs from scratch? arXiv preprint arXiv:1906.08637, 2019.
Adrian Bulat and Georgios Tzimiropoulos. XNOR-Net++: Improved binary neural networks. In
British Machine Vision Conference, 2019.
Adrian Bulat, Georgios Tzimiropoulos, Jean Kossaifi, and Maja Pantic. Improved training of binary
networks for human pose estimation and image recognition. arXiv preprint arXiv:1904.05868,
2019.
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by
half-wave gaussian quantization. In IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2017.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to +1
or-1. arXiv, 2016.
Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. Regularizing activation distribu-
tion for training binarized deep networks. In IEEE Conference on Computer Vision and Pattern
Recognition, 2019.
Julian Faraone, Nicholas J. Fraser, Michaela Blott, and Philip H. W. Leong. SYQ: learning symmet-
ric quantization for efficient deep neural networks. In IEEE Conference on Computer Vision and
Pattern Recognition, 2018.
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and
Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks.
arXiv, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpass-
ing human-level performance on imagenet classification. In IEEE International Conference on
Computer Vision, pp. 1026-1034, 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In IEEE Conference on Computer
Vision and Pattern Recognition, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
9
Published as a conference paper at ICLR 2020
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In
Advances on Neural Information Processing Systems, 2017.
Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, and
David Doermann. Circulant binary convolutional networks: Enhancing the performance of 1-bit
dcnns with circulant back propagation. In IEEE Conference on Computer Vision and Pattern
Recognition, 2019.
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-Real
Net: Enhancing the performance of 1-bit CNNs with improved representational capability and
advanced training algorithm. In European Conference on Computer Vision, 2018.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-Net: Imagenet
classification using binary convolutional neural networks. In European Conference on Computer
Vision, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-
Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal on Computer
Vision,115(3):211-252, 2015.
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free train-
ing of multilayer neural networks with continuous or discrete weights. In Advances on Neural
Information Processing Systems, 2014.
Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. Learning channel-wise interactions
for binary convolutional neural networks. In IEEE Conference on Computer Vision and Pattern
Recognition, 2019.
Zhe Xu and Ray C.C. Cheung. Accurate and compact convolutional neural networks with trained
binarization. In British Machine Vision Conference, 2019.
Haojin Yang, Martin Fritzsche, Christian Bartz, and Christoph Meinel. BMXNet: An open-source
binary neural network implementation based on MXNet. In ACM International Conference on
Multimedia, 2017.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the per-
formance of convolutional neural networks via attention transfer. In International Conference on
Learning Representations, 2017.
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. LQ-Nets: Learned quantization
for highly accurate and compact deep neural networks. In European Conference on Computer
Vision, 2018.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. Mixup: Beyond empiri-
cal risk minimization. arXiv preprint arXiv:1710.09412, 2017.
Jianhao Zhang, Yingwei Pan, Ting Yao, He Zhao, and Tao Mei. dabnn: A super fast inference
framework for binary neural networks on ARM devices. In ACM International Conference on
Multimedia, 2019.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. DoReFa-Net:
Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. Interna-
tional Conference on Learning Representations, 2017.
Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or
more networks per bit? In IEEE Conference on Computer Vision and Pattern Recognition, 2019.
Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian D. Reid. Towards effective
low-bitwidth convolutional neural networks. In IEEE Conference on Computer Vision and Pattern
Recognition, 2018.
10
Published as a conference paper at ICLR 2020
Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Structured binary neural
networks for accurate image classification and semantic segmentation. In IEEE Conference on
Computer Vision and Pattern Recognition, 2019.
11