Published as a conference paper at ICLR 2020
Sign Bits Are All You Need for Black-Box At-
TACKS
Abdullah Al-Dujaili
CSAIL, MIT
Cambridge, MA 02139
aldujail@mit.edu
Una-May O’Reilly
CSAIL, MIT
Cambridge, MA 02139
unamay@csail.mit.edu
Ab stract
We present a novel black-box adversarial attack algorithm with state-of-the-art
model evasion rates for query efficiency under '∞ and '2 metrics. It exploits a
sign-based, rather than magnitude-based, gradient estimation approach that shifts
the gradient estimation from continuous to binary black-box optimization. It
adaptively constructs queries to estimate the gradient, one query relying upon
the previous, rather than re-estimating the gradient each step with random query
construction. Its reliance on sign bits yields a smaller memory footprint and it
requires neither hyperparameter tuning or dimensionality reduction. Further, its
theoretical performance is guaranteed and it can characterize adversarial subspaces
better than white-box gradient-aligned subspaces. On two public black-box attack
challenges and a model robustly trained against transfer attacks, the algorithm’s
evasion rates surpass all submitted attacks. For a suite of published models, the
algorithm is 3.8× less failure-prone while spending 2.5× fewer queries versus
the best combination of state of art algorithms. For example, it evades a standard
MNIST model using just 12 queries on average. Similar performance is observed
on a standard IMAGENET model with an average of 579 queries.
1	Introduction
Problem. Deep Neural Networks (DNNs) are vulnerable to adversarial examples, which are malicious
inputs designed to fool the model’s prediction—see (Biggio and Roli, 2018) for a comprehensive,
recent overview of adversarial examples. Research on generating these malicious inputs started in the
white-box setting, where access to the gradients of the models is assumed. Since the gradient points to
the direction of steepest ascent, an input can be perturbed along the gradient’s direction to maximize
the network’s loss, thereby potentially causing misclassification under class prediction, e.g. with
images, or evasion under detection, e.g. with malware. The assumption of access to the underlying
gradient does not however reflect real world scenarios. Attack algorithms under a more realistic,
restrictive black-box threat model, which assumes access to predictions in lieu of gradients, are
therefore studied. Central to their approaches is estimating the gradient. To estimate the magnitudes
and signs of the gradient, the community at large has formulated a continuous optimization problem
of O(n) complexity where n is the input dimensionality. Most recently work has sought to reduce
this complexity by means of data-/time-dependent priors Ilyas et al. (2019). In this paper, we take a
different tact and reduce the central problem to just estimating the signs of the gradients. Our intuition
arises from observing that estimating the sign of the top 30% gradient coordinates by magnitude is
enough to achieve a rough misclassification rate of 70%. Figure 1 reproducing Ilyas et al. (2019)
illustrates this observation for the MNIST dataset-see Appendix A for other datasets. Therefore our
goal is to recover the sign of the gradient with high query efficiency so we can use it to generate
adversarial examples as effective as those generated by full gradient estimation approaches.
Related Work. We organize the related work in two themes, namely Adversarial Example Generation
and Sign-Based Optimization. The literature of the first theme primarily divides into white-box and
black-box settings. The white-box setting, while not the focus of this work, follows from the works
of Biggio et al. (2013) and Goodfellow et al. (2015) who introduced the Fast Gradient Sign Method
(FGSM), including several methods to produce adversarial examples for various learning tasks and
threat perturbation constraints (Carlini and Wagner, 2017; Moosavi-Dezfooli et al., 2016; Hayes and
1
Published as a conference paper at ICLR 2020
Danezis, 2017; Al-Dujaili et al., 2018; Kurakin et al., 2017; Shamir et al., 2019). Turning to the black-
box setting and iterative optimization schemes, Narodytska and Kasiviswanathan (2017), without
using any gradient information, use a naive policy of perturbing random segments of an image to
generate adversarial examples. Bhagoji et al. (2017) reduce the dimensions of the feature space using
Principal Component Analysis (PCA) and random feature grouping, before estimating gradients.
Chen et al. (2017) introduce a principled approach by using gradient based optimization. They
employ finite differences, a zeroth-order optimization means, to estimate the gradient and then use it
to design a gradient-based attack. While this approach successfully generates adversarial examples, it
is expensive in how many times the model is queried. Ilyas et al. (2018) substitute traditional finite
differences methods with Natural Evolutionary Strategies (NES) to obtain an estimate of the gradient.
Tu et al. (2018) provide an adaptive random gradient estimation algorithm that balances query counts
and distortion, and introduces a trained auto-encoder to achieve attack acceleration. Ilyas et al. (2019)
extend this line of work by proposing the idea of gradient priors and bandits: BanditsT D. Our
work contrasts with the general approach of these works in two ways: a) We focus on estimating the
sign of the gradient and investigate whether this estimation suffices to efficiently generate adversarial
examples. b) The above methods employ random sampling in constructing queries to the model
while our construction is adaptive.1 Another approach involves learning adversarial examples for
one model (with access to its gradient information) to transfer them against another (Liu et al., 2016;
Papernot et al., 2017). Alternately, Xiao et al. (2018) use a Generative Adversarial Network (GAN) to
generate adversarial examples which are based on small norm-bounded perturbations. These methods
involve learning on a different model, which is expensive, and not amenable to comparison with
setups—including ours—that directly query the model of interest.
Sign-Based Optimization. In the context of general-
purpose continuous optimization methods, sign-
based stochastic gradient descent was studied in both
zeroth- and first-order setups. In the latter, Bern-
stein et al. (2018) analyzed signSGD, a sign-based
Stochastic Gradient Descent, and showed that it en-
joys a faster empirical convergence than SGD in ad-
dition to the cost reduction of communicating gra-
dients across multiple workers. Liu et al. (2019)
extended signSGD to zeroth-order setup with the
ZO-SignSGD algorithm. ZO-SignSGD (Liu et al.,
2019) was shown to outperform NES against a black-
box model on MNIST. These approaches use the sign
of the gradient (or its zero-order estimate) to achieve
better convergence, whereas our approach both esti-
mates and uses the sign of the gradient.
Contributions. We present the following contribu-
tions at the intersection of adversarial machine learn-
ing and black-box (zeroth-order) optimization: 1) We
0%	20%	40%	60%	80%	100%
k percent of MNIST coordinates
Figure 1: Misclassification rate of an MNIST
model on the noisy FGSM’s adversarial ex-
amples as a function of correctly estimated
coordinates of sign(Vχf (x, y)) on 1000 ran-
dom MNIST images. Estimating the sign of
the top 30% gradient coordinates (in terms
of their magnitudes) is enough to achieve a
rough misclassification rate of 70%. More
details can be found in Appendix A.
exploit the separability property of the directional derivative of the loss function of the model under
attack in the direction of {±1}n vectors, to propose a divide-and-conquer, adaptive, memory-efficient
algorithm, we name SignHunter, to estimate the gradient sign bits. 2) We provide a worst-case
theoretical guarantee on the number of queries required by SignHunter to perform at least as well
as FGSM (Goodfellow et al., 2015), which has access to the model’s gradient. To our knowledge,
no black-box attack from the literature offers a similar performance guarantee. 3) We evaluate our
approach on a rigorous set of experiments on both, standard and adversarially hardened models. All
other previous works on this topic have published their results on a subset of the datasets and threat
models we experimentally validate in this work. Through these experiments, we demonstrate that
SignHunter’s adaptive search for the gradient sign allows it to craft adversarial examples within
a mere fraction of the theoretical number of queries thus outperforming FGSM and state-of-the-art
black-box attacks. 4) We release a software framework to systematically benchmark adversarial
1We use the term adaptive here to characterize how the algorithm constructs its perturbation vector qt
deterministically based on the previous perturbations. Mathematically, the perturbation vector at time t can
be expressed as deterministic function qt = g(qt-1). In contrast, other algorithms construct I.I.D randomly
perturbationvectors: qt 〜N(0, I).
2
Published as a conference paper at ICLR 2020
black-box attacks, including SignHunter’s, on MNIST, CIFAR10, and IMAGENET models in
terms of success rate, query count, and other metrics. 5) We demonstrate how SignHunter can be
used to characterize adversarial cones in a black-box setup and in doing so, highlight the gradient
masking effect.
Notation. Let n denote the dimension of datapoint x. Denote a hidden n-dimensional binary code by
q*. That is, q* ∈ H ≡ {-1, +1}n. Further, denote the directional derivative of some function f at a
point X in the direction of a vector V by Dv f (x) ≡ VT Nxf (x) which often can be approximated by
the finite difference method. That is, for δ > 0, we have
Dvf(x)= VTVxf(X) ≈ f(x + δV)- f(x) .	⑴
δ
Let ∏s(∙) be the projection operator onto the set S, Bp(x, e) be the 'p ball of radius e around x.
2	Gradient Estimation
At the heart of black-box adversarial attacks is generating a perturbation vector to slightly modify the
original input X so as to fool the network prediction of its true label y . Put differently, an adversarial
example X0 maximizes the network’s loss L(X0, y) but still remains e-close to the original input X.
Although the loss function L can be non-concave, gradient-based techniques are often very successful
in crafting an adversarial example Madry et al. (2017). That is, setting the perturbation vector as
a step in the direction of VxL(X, y). Consequently, the bulk of black-box attack methods try to
estimate the gradient by querying an oracle that returns, for a given input/label pair (X, y), the value
of the network’s loss L(X, y), consulting prediction or classification accuracy. Using only such
value queries, the basic approach relies on the finite difference method to approximate the directional
derivative (Eq. 1) of the function L at the input/label pair (X, y) in the direction of a vector V, which
corresponds to VTVxL(X, y). With n linearly independent vectors {ViTVxL(X, y) = di}1≤i≤n,
one can construct a linear system of equations to recover the full gradient. Clearly, this approach’s
query complexity is O(n), which can be prohibitively expensive for large n (e.g., n = 268, 203
for the IMAGENET dataset). Recent works try to mitigate this issue by exploiting data- and/or
time-dependent priors (Tu et al., 2018; Ilyas et al., 2018; 2019). However, the queries are not adaptive,
they are constructed based on i.i.d. random vectors {Vi}. They fail to make use of the past queries’
responses to construct the new query and recover the full gradient more efficiently. As stated in the
introduction, we solve the smaller problem of gradient sign estimation with adaptive queries based on
the observation that simply leveraging (noisy) sign bits of the gradient yields successful attacks-see
Figure 1.
Definition 1. (Gradient Sign Estimation Problem) For an input/label pair (X, y) and a loss function
L, let g* = VxL(X, y) be the gradient of L at (X, y) and q* = sign(g*) ∈ H be the sign bit vector
of g*.2 Then the goal of the gradient sign estimation problem is to find a binary vector q ∈ H
maximizing the directional derivative3
max
q∈H
DqL(X, y) ,
(2)
from a limited number of (possibly adaptive) function value queries L(X0, y).
3	A Method for Estimating Sign of the Gradient from Adaptive
Queries
Our goal is to estimate the gradient sign bits of the loss function L of the model under attack at an
input/label pair (X, y) from a limited number of loss value adaptive queries L(X0, y). To this end,
we examine the basic concept of directional derivatives that has been employed in recent black-box
2Without loss of generality, we encode the sign bit vector in H ≡ {-1, +1}n rather than {0, 1}n. This
is a common representation in sign-related literature. Note that the standard sign function has the range of
{-1, 0, +1}. Here, we use the non-standard definition (Zhao, 2018) whose range is {-1, +1}. This follows
from the observation that DNNs’ gradients with respect to their inputs are not sparse (Ilyas et al., 2019, Appendix
B.1).
3The maximization follows from DqL(x, y) = qτg*, which is maximized when q = q* = sign(g*).
3
Published as a conference paper at ICLR 2020
adversarial attacks. Based on the definition of the directional derivative (Eq. 1), the following can be
stated.
Property 1 (Separability of Dq L(x, y)). The directional derivative Dq L(x, y) of the loss function
L at an input/label pair (x, y) in the direction of a binary code q is separable. That is,
n
max max、qigi.	(3)
qi ∈{-1,+1}
i=1
Algorithm 1 SignHunter
g : H → R : the black-box function to be maxi-
mized over the binary hypercube H ≡ {-1, +1}n
def init(g):
i ― 0, h — 0
g — g
S 〜U (H)	〃e.g., [+1,...,+1]
done — false
gbest 4--∞
def is_done() :
return done
def step() :
c_len 4 dn/2he
s[i*c_len:(i+1)*c_len] *= -1
if g(s) ≥ gbest:
gbest 4 g(s)
else:
s[i*c_len:(i+1)*c_len] *= -1
increment i
if i == 2h:
i 4 0, increment h
ifh == dlog2 (n)e + 1:
done 4 true
def get_current_sign_estimate() :
return s
max Dq L(x,y) = max qτ g*
q∈H	q∈H
This reformulates the gradient sign estimation prob-
lem from single n-dimensional to n 1-dimensional
binary black-box optimization problems, reducing
the search space of sign bits from 2n to 2n. Sub-
sequently, one could recover the gradient sign bits
with n + 2 queries as follows: i. Start with an ar-
bitrary sign vector q and compute the directional
derivative DqL(x, y). Using Eq. 1, this requires two
queries: L(x + δq, y) and L(x, y). ii. For the remain-
ing n queries, flip q’s bits (coordinates) one by one
and compute the corresponding directional derivative-
one query each L(x + δq, y). iii. Retain bit flips that
maximize the directional derivative DqL(x, y) and
revert those otherwise. This, however, still suffers
from the O(n) complexity of full gradient estimation
methods. Further, each query recovers at most one
sign bit and the natural question to ask is: can we
recover more sign bits per query?
Consider the case where all the gradient coordinates
have the same magnitude, i.e., |{|g^ ∣}ι≤i≤n| = 1, and
let the initial guess q1 have r correct bits and n - r
wrong ones. Instead of flipping its bits sequentially,
we can flip them all at once to get q2 = -q1 . If
Dq2 L(x, y) ≥ Dq1 L(x, y), then we retain q2 as
our best guess with n - r correct bits, otherwise
q1 remains. In either cases, with three queries, we
will recover max(r, n - r) sign bits. One can think
of this flip/revert procedure as one of majority voting
by the guess’s coordinates on whether they agree with their gradient sign’s counterparts. To see this,
let |g* ∣= 1 for all i, then the condition Dq? L(x, y) ≥ Dqi L(x, y) can be written as n - r - r ≥
r - n + r =⇒ n ≥ 2r. If the agree votes r are less than half of the total votes n, then q2 is retained.
Besides flipping all the coordinates, one can employ the same procedure iteratively on a subset
(chunk) of the coordinates [qj, . . . , qj+ni] of the guess vector q, recovering max(ri, ni - ri) sign
bits, where ni and ri is the length of the ith chunk and the number of its correct signs, respectively.
While the magnitudes of gradient coordinates may not have the same value as assumed in the
previous example; through empirical evaluation (see Appendix F), we found them to be concentrated.
Consequently and with high probability, their votes on retaining or reverting chunks of sign flips are
weighted (by their corresponding gradient magnitude) similarly. That said, if we are at a chunk where
the distribution of the gradient coordinate magnitudes is uniform, then the flip/revert procedure could
favor recovering few sign coordinates with large magnitude counterparts over many sign coordinates
with small magnitude counterparts. From our experiments on the noisy FGSM, this still suffices
to generate adversarial examples: an attack with 30% correct sign bits (that correspond to the top
gradient coordinates magnitudes) is more effective than an attack with 50% correct arbitrary sign
bits as shown in Figure 1. Put differently, we would like to recover as many sign bits as possible with
as few queries as possible. However, if we can only recover few, they should be those that correspond
to coordinates with large gradient magnitude. This notion is in line with the flip/revert procedure.
We employ the above observation in a divide-and-conquer search which we refer to as SignHunter.
As outlined in Algorithm 1, the technique starts with an initial guess of the sign vector q1 (s in
Algorithm 1). It then proceeds to flip the sign of all the coordinates to get a new sign vector q2,
4
Published as a conference paper at ICLR 2020
and revert the flips if the loss oracle returned a value L(x + δq2, y) (or equivalently the directional
derivative ) less than the best obtained so far L(x + δq1, y). SignHunter applies the same rule to
the first half of the coordinates, the second half, the first quadrant, the second quadrant, and so on.
For a search space of dimension n, SignHunter needs 2dlog(n)+1e - 1 sign flips to complete its
search. If the query budget is not exhausted by then, one can update x with the recovered signs and
restart the procedure at the updated point with a new starting code s. If we start with a sign vector
whose Hamming distance to the optimal sign vector q* is n/2: agreeing with q* in the first half
of coordinates. In this case, SignHunter needs just four queries to recover the entire sign vector
independent of n, whereas the sequential bit flipping still require n + 2 queries. In the next theorem,
we show that SignHunter is guaranteed to perform at least as well as FGSM with O(n) oracle
queries. Up to our knowledge, no such guarantees exist for any black-box attack from the literature.
Theorem 1. (Optimality of SignHunter) Given 2dlog(n)+1e queries and that the directional deriva-
tive is well approximated by the finite-difference (Eq. 1), SignHunter is at least as effective as
FGSM (Goodfellow et al., 2015) in crafting adversarial examples.
The proof can be found in Appendix B. Theorem 1 provides an upper bound on the number of
queries required for SignHunter to recover the gradient sign bits, and perform as well as FGSM.
In practice (as will be shown in our experiments), SignHunter crafts adversarial examples with a
small fraction of this upper bound. The rationale here is that we do not need to recover the sign bits
exactly; we rather need a fast convergence to an adversarially helpful sign vector s. In our setup,
we use the best sign estimation obtained s so far in a similar fashion to FGSM, whereas full-gradient
estimation approaches often employ an iterative scheme of T steps within the perturbation ball
Bp(x, ), calling the gradient estimation routine in every step leading to a search complexity of
nT . Instead, our gradient sign estimation routine runs at the top level of our adversarial example
generation procedure. Further, SignHunter is amenable to parallel hardware architecture and has
a smaller memory footprint (just sign bits) and thus can carry out attacks in batches more efficiently.
Crafting black-box adversarial attacks with SignHunter is outlined in Algorithm 2.
4	Experiments
We evaluate SignHunter and
compare it with established al-
gorithms from the literature:
ZO-SignSGD Liu et al. (2019),
NES Ilyas et al. (2018), and
BanditsT D Ilyas et al. (2019) in
terms of effectiveness in crafting
(without loss of generality) untargeted
black-box adversarial examples. To
highlight SignHunter’s adaptive
query construction, we introduce a
variant of Algorithm 2, named Rand.
At every iteration, Rand’s sign vector
is sampled uniformly from H.4.
Both '∞ and '2 threat models are
considered on the MNIST, CIFAR10,
and IMAGENET datasets. Code and
data for the experiments can be found
at https://bit.ly/3acIHoQ.
Algorithm 2 Black-Box Adversarial Example Generation
with SignHunter
xinit: input to be perturbed, yinit : xinit’s true label,
Bp(., ) : `p perturbation ball of radius
L : loss function of the model Under attack___________
1: δ — e // set finite-difference probe to perturbation bound
2: xo《—Xinit
3:	Define the function g as
L(ΠBp (xinit,) (xo + δq), yinit) - L(xo, yinit)
g(q) =------p----------------ʌ--------------------
δ
4:	SignHunter.init(g)
5:	//C(∙) returns top class
6:	while C(x) = yinit do
7:	SignHunter.step()
8:	S J SignHunter.get_current_sign_estimate()
9:	x J πBp(χinitQ (Xo + δS)
10:	if SignHunter.is_done() then
11:	xo J x
12:	Define the function g as in Line 3 (with xo update)
13:	SignHunter.init(g)
14:	return x
Experiments Setup. Our experiment
setup is similar to (Ilyas et al., 2019). Each attacker is given a budget of 10, 000 oracle queries per
attack attempt and is evaluated on 1000 images from the test sets of MNIST, CIFAR10, and the
validation set of IMAGENET. We did not find a standard practice for setting the perturbation bound
e. For the '∞ threat model, We use (Madry et al., 2017)’S bound for MNIST and (Ilyas et al., 2019)’S
bounds for both CIFAR10 and IMAGENET. For the `2 threat model, (Ilyas et al., 2019)’s bound is
used for IMAGENET. MNIST’s bound is set based on the sufficient distortions observed in (Liu et al.,
4That is, replace Line 8 in Algorithm 2 by S 〜U (H).
5
Published as a conference paper at ICLR 2020
____ < k J
səɪjənb*ə^e,ɪəʌ,e
BanditsTD
---NES
Rand
SignHUnter
---ZOSignSGD
二
0.0	0.2	0.4	0.6	0.8	1.0
success rate
(a) MNIST '∞
səɪjənb*ə^e,ɪəʌ,e
0-
0.0	0.2	0.4	0.6	0.8	1.0
success rate
(C)IMAGENET '∞
--
sə-ənbftə^e,ɪəʌ,e
0.0	0.2	0.4	0.6	0.8	1.0
success rate
(d) MNIST '2
--
sə-ənbftəmejəʌB
0.0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8
success rate
(e) CIFAR10 '2
-- --
sə-ənbftəmejəʌB
0.0	0.2	0.4	0.6	0.8
success rate
(f) IMAGENET '2
Figure 2: Performance of black-box attacks in the '∞ and '2 perturbation constraint. The plots show
the average number of queries used per successful image for each attack when reaching a specified
success rate.
2019), which are smaller than the one used in (Madry et al., 2017). We use the observed bound in
(Cohen et al., 2019) for CIFAR10. We show results based on standard models-i.e., models that are not
adversarially hardened. For MNIST and CIFAR10, the naturally trained models from (Madry et al.,
2017)’s MNIST and CIFAR10 challenges are used. For IMAGENET, TensorFlow’s Inception (v3)
model is used. The loss oracle returns the cross-entropy loss of the respective model. See Appendix
C for other general experimental setup details.
Hyperparameters Setup. While SignHunter does not have any hyperparameters, to fairly compare
it with the other algorithms, we tuned their hyperparameters starting with the default values reported
by the corresponding authors. The finite difference probe δ for SignHunter is set to the perturbation
bound as it is used for both computing the finite difference and crafting the adversarial examples—
see Line 1 in Algorithm 2. This tuning-free aspect of SignHunter offers a robustness advantage
over algorithms which require expert hypertuning. Details on the hyperparameter setup are available
in Appendix C.
Table 1: Summary of attacks effectiveness on CIFAR10 under '∞ and '2 perturbation constraints,
and with a query limit of 10, 000 queries. The Failure Rate ∈ [0, 1] column lists the fraction of failed
attacks over 1000 images. The Avg. # Queries column reports the average number of queries made to
the loss oracle only over successful attacks.
Failure Rate Avg. # Queries
Attack	'∞	'2	'∞			`2
BanditsT D	0.95	0.39	432.24	1201.85
NES	0.37	0.67	312.57	496.99
Rand	0.20	0.89	422.16	1018.17
SignHunter	0.07	0.21	121.00	692.39
ZOSignSGD	0.37	0.80	161.28	528.35
Results. Figure 2 shows the trade-off between the success (evasion) rate and the mean number of
queries (of the successful attacks, per convention) needed to generate an adversarial example for the
MNIST, CIFAR10, and IMAGENET classifiers under the '∞ and '2 perturbation constraints. These
plots indicate the average number of queries required for a desired success rate. Table 1 represents
a tabulated summary of plots (b) and (e) of Figure 2.5 We observe the following: For any given
success rate, SignHunter dominates the previous state of the art approaches in all settings except
the IMAGENET '2 setup, where BanditsT D shows a better query efficiency when the desired
success rate is roughly greater than 0.35. This is all the more remarkable because BanditsT D
5More tabulated summaries of these plots can be found in Appendix D-Tables 7-9. We also plot the
classifier’s loss and the gradient estimation quality (in terms of Hamming distance and cosine similarity)
averaged over all the images as a function of the number of queries in Figures 4-6 of Appendix D.
6
Published as a conference paper at ICLR 2020
exploits tiles, a data-dependent prior, searching over 50 × 50 × 3 dimensions for IMAGENET, while
SignHunter searches over the explicit data 299 × 299 × 3 dimensions: 36× more dimensions.
'∞ vs. '2 Perturbation Threat. In view of BanditsTD,s advantage, SignHunter is remarkably
efficient in the '∞ setup, achieving a 100% evasion using-on average■—just 12 queries per image
against the MNIST classifier! In the `2 setup, SignHunter’s performance degrades—yet it still
outperforms the other algorithms. This is expected, since SignHunter perturbs all the coordinates
with the same magnitude and the '2 perturbation bound €2 for all the datasets in our experiments
is set such that e2∕√n is significantly less than the '∞ perturbation bound e∞. Take the case of
MNIST (n = 28 X 28), where e∞ = 0.3 and e2 = 3. For SignHunter, the '2 setup is equivalent
to an '∞ perturbation bound of 3/28 ≈ 0.1. The employed '2 perturbation bounds give the state of
the art—continuous optimization based—approaches more perturbation options. For instance, it is
possible for NES to perturbjust one pixel in an MNIST image by a magnitude of 3; two pixels by a
magnitude of 3/√2 ≈ 2.1 each; ten pixels by a magnitude of 3/√Iθ ≈ 0.9 each, etc. On the other
hand, the binary optimization view of SignHunter limits it to always perturb all 28 × 28 pixels
by a magnitude of 3/28 ≈ 0.1. Despite its fewer degrees of freedom, SignHunter maintains
its effectiveness in the '2 setup. The plots can also be interpreted as a sensitivity assessment of
SignHunter as e gets smaller going from '∞ to the '2 perturbation threat.
SignHunter vs FGSM. The performance of SignHunter is in line with Theorem 1 when com-
pared with the performance of FGSM (the noisy FGSM at k = 100% in Figures 1 and 2 of Appendix
A) in both '∞ and '2 setups across all datasets. For instance, FGSM has a failure rate of 0.32 for
CIFAR10 '2 (Appendix A, Figure 2 (b)), while SignHunter achieves a failure rate of 0.21 with
692.39 < 2n = 2 × 3 × 32 × 32 = 6144 queries (Appendix D, Table 8). Note that for IMAGENET,
SignHunter outperforms FGSM with a query budget of 10, 000 queries, a fraction of the theoretical
number of queries required 2n = 536, 406 to perform at least as well. Incorporating SignHunter
in an iterative framework of perturbing the data point x till the query budget is exhausted (Lines 10
to 14 in Algorithm 2) supports the observation in white-box settings that iterative FGSM—or Projected
Gradient Descent (PGD)—is stronger than FGSM (Madry et al., 2017; Al-Dujaili et al., 2018). This
is evident by the upticks in SignHunter’s performance on the MNIST '2 case (Appendix D,
Figure 4), which happens after every iteration (after every other 2 × 28 × 28 queries).
Gradient Estimation. Plots of the Hamming similarity capture the number of recovered sign bits,
while plots of the average cosine similarity capture the value of Eq. 2. Both SignHunter and
BanditsT D consistently optimize both metrics. In general, SignHunter (BanditsT D) con-
verges faster especially on the Hamming(cosine) metric as it is estimating the signs(signs and
magnitudes) compared to BanditsT D’s full gradient (SignHunter’s gradient sign) estimation.
This is most obvious in the IMAGENET '2 setup (Appendix D, Figure 6). Note that once an attack is
successful, the estimated gradient sign at that point is used for the rest of the plot. This explains why,
in the '∞ settings, SignHunter’s plot does not improve compared to its '2 counterpart, as most of
the attacks are successful in the very first few queries made to the loss oracle and no further refined
estimation is required. Another possible reason is that the gradient direction can be very local and
does not capture the global loss landscape compared to SignHunter’s estimation. More on this is
discussed in Section 6.
SignHunter vs. Rand. Given these results, one could argue that SignHunter is effective,
because it maximally perturbs datapoints to the vertices of their perturbation balls.6 However, Rand’s
poor performance does not support this argument and highlights the effectiveness of SignHunter’s
adaptive query construction. Except for MNIST and CIFAR10 '∞ settings, Rand performs worse
than the full-gradient estimation approaches, although it perturbs datapoints similar to SignHunter.
Overall, SignHunter is 3.8× less failure-prone than the state-of-the-art approaches combined, and
spends over all the images (successful and unsuccessful attacks) 2.5× less queries.7
5	SIGNHUNTER vs. Defenses
To complement Section 4, we evaluate SignHunter against adversarial training, a way to improve
the robustness of DNNs (Madry et al., 2017). Specifically, we attacked the secret models used
6We define perturbation vertices as extreme points of the ball Bp(x, e). That is, X ± e∞, where e∞ = e
when P = ∞ and e∞ = e∕√n when P = 2.
7 The number of queries spent is computed based on Tables 7-9 of Appendix D as (1 - fail_rate) *
avg_#_queries + fail_rate * 10,000.
7
Published as a conference paper at ICLR 2020
in public challenges for MNIST and CIFAR10. For IMAGENET, we used ensemble adversarial
training, a method that argues security against black-box attacks based on transferability Tramer et al.
(2017a). Appendix E reports the same metrics used in Section 4 as well as a tabulated summary for
the results discussed below.
Public MNIST Black-Box Attack Challenge. In line with the challenge setup, 10, 000 test images
were used with an '∞ perturbation bound of e = 0.3. Although the secret model is released, We
treated it as a black box similar to our experiments in Section 4. No maximum query budget was
specified, so we set it to 5, 000 queries. This is equal to the number of iterations given to a PGD
attack in the white-box setup of the challenge: 100-steps with 50 random restarts. SignHunter’s
attacks resulted in the lowest model accuracy of 91.47%, outperforming all the submitted attacks to
the challenge, with an average number of queries of 233 per successful attack. Note that the attacks
submitted to the challenge are based on transferability and do not query the model of interest. On the
other hand, the most powerful white-box attack by Wang et al. (2018)—as of May 15, 2019—resulted
in a model accuracy of 88.42%. Further, a PGD attack with 5, 000 back-propagations achieves
89.62% in contrast to SignHunter’s 91.47% with just 5, 000 forward-propagations.
Public CIFAR10 Black-Box Attack Challenge. This challenge setup is similar to the above, but
with an '∞ perturbation bound of e = 8. SignHunter's attacks resulted in the lowest model
accuracy of 47.16%, outperforming all the submitted attacks to the challenge, with an average
number of queries of 569 per successful attack. Similar to the MNIST challenge, all the submitted
attacks are based on transferability. On the other hand, the most powerful white-box attack by Zheng
et al. (2018)—as of May 15, 2019—resulted in a model accuracy of 44.71%. Further, a PGD attack
with 200 back-propagations achieves 45.21% in contrast to SignHunter’s 47.16% with 5, 000
forward-propagations.
Ensemble Adversarial Training on IMAGENET. In line with Tramer et al. (2017a), we set e =
0.0625 and report the v3adv-ens4 model’s misclassification over 10,000 random images from IMA-
GENET’s validation set. After 20 queries, SignHunter achieves a top-1 error of 40.61% greater
than the 33.4% rate of a series of black-box attacks (including PGD with 20 iterations) transferred
from a substitute model. With 1000 queries, SignHunter breaks the model’s robustness with a
top-1 error of 90.75%!
6	Characterizing Adversarial Cones with SIGNHUNTER
Estimating the size of adversarial cones, the space of adversarial examples in the vicinity of a point,
for a model has been a topic of interest by the machine learning community Tramer et al. (2017a); Ma
et al. (2018); Lu et al. (2018). The Gradient-Aligned Adversarial Subspace (GAAS) method Tramer
et al. (2017b) provides an approximation of the adversarial cone dimensionality by finding a set of
orthogonal perturbations of norm e that are all adversarial with respect to the model. By linearizing
the model’s loss function, this is reduced to finding orthogonal vectors that are maximally aligned
with its gradient g*——or its gradient sign q* in the '∞ setup Tramer et al. (2017a). In Figure 3,
we reproduce (Tramer et al., 2017a, Fig. 2) and show that aligning the orthogonal vectors with
SignHunter’s estimation (we refer to this approach as SAAS) instead of aligning them with the
gradient (GAAS) results in a better approximation of the adversarial cone for the two IMAGENET
models considered earlier, even when the number of queries given to SignHunter is just a fraction
of the dimensionality n. Through its query-efficient finite-difference sign estimation, SignHunter
is able to quickly capture the larger-scale variation of the loss landscape in the point’s neighborhood,
rather than the infinitesimal point-wise variation that the gradient provides, which can be very local.
This is important in adversarial settings, where the loss landscape is analyzed in the vicinity of the
point Moosavi-Dezfooli et al. (2018); Tramer et al. (2017a). One interesting observation at k = 1
(note here, r1 = q* ) across all e is that GAAS finds adversarial directions for fewer points against
the v3adv-ens4 model than the naturally trained model v3, whereas SAAS reports similar probability of
adversarial directions for both. This contrast suggests that ensemble adversarial training Tramer et al.
(2017a) still exhibits the gradient masking effect, where the gradient poorly approximates the global
loss landscape.
7	Conclusion
Assuming a black-box threat model, we studied the problem of generating adversarial examples
for neural nets and proposed the gradient sign estimation problem as the core challenge in crafting
8
Published as a conference paper at ICLR 2020
Figure 3: TWo estimations of the '∞ adversarial cones for two IMAGENET models: v3 and
v3adv-ens4. The first estimation (GAAS: Gradient-Aligned Adversarial Subspace) finds k orthogonal
vectors maximally aligned with the gradient sign q* Tramer et al. (2017a). The second (SAAS:
SignHunter-Aligned Adversarial Subspace) finds k orthogonal vectors that are maximally aligned
with SignHunter’s s (Algorithm 2, Line 8) after 1, 000 queries. Similar to (Tramer et al., 2017a,
Figure 2), for 500 correctly classified points x and ∈ {4, 10, 16}, we plot the probability that we
find at least k orthogonal vectors ri—computed based on (Tramer et al., 2017a, Lemma 7)—such
that ∣∣ri∣∣∞= e and X + r is misclassified. For both models and for the same points x, SAAS finds
more orthogonal adversarial vectors ri than GAAS, thereby providing a better characterization of
the space of adversarial examples in the vicinity of a point, albeit without a white-box access to the
models.
these examples. We formulate the problem as a binary black-box optimization one: maximizing
the directional derivative in the direction of {±1}n vectors, approximated by the finite difference
of the queries’ loss values. The separability property of the directional derivative helped us devise
SignHunter, a query-efficient, tuning-free divide-and-conquer algorithm with a small memory
footprint that is guaranteed to perform at least as well as FGSM after O(n) queries. No similar
guarantee is found in the literature. In practice, SignHunter needs a mere fraction of this number
of queries to craft adversarial examples. The algorithm is one of its kind to construct adaptive queries
instead of queries that are based on i.i.d. random vectors. Robust to gradient masking, SignHunter
can also be used to estimate the dimensionality of adversarial cones. Moreover, SignHunter
achieves the highest evasion rate on two public black-box attack challenges and breaks a model that
argues robustness against substitute-model attacks.
Acknowledgments
This work was supported by the MIT-IBM Watson AI Lab. We would like to thank Shashank Srikant
for his timely help. We are grateful for feedback from Nicholas Carlini and Zico Kolter.
References
Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and Una-May O’Reilly. Adversarial deep learning for robust
detection of binary encoded malware. In 2018 IEEE Security and Privacy Workshops (SPW), pages 76-82.
IEEE, 2018.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signSGD: Com-
pressed optimisation for non-convex problems. In Jennifer Dy and Andreas Krause, editors, Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learn-
ing Research, pages 560-569, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/bernstein18a.html.
Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Exploring the space of black-box attacks on deep
neural networks. arXiv preprint arXiv:1712.09491, 2017.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern
Recognition, 84:317-331, 2018.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto,
and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference on
machine learning and knowledge discovery in databases, pages 387-402. Springer, 2013.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pages 39-57. IEEE, 2017.
9
Published as a conference paper at ICLR 2020
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based
black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th
ACM Workshop on Artificial Intelligence and Security, pages 15-26. ACM, 2017.
Jeremy Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via randomized smoothing.
arXiv:1902.02918v1, 2019.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations, 2015. URL http://arxiv.org/abs/1412.
6572.
Chuan Guo, Jacob R Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q Weinberger. Simple black-box
adversarial attacks. arXiv preprint arXiv:1905.07121, 2019.
Jamie Hayes and George Danezis. Machine learning as an adversarial service: Learning black-box adversarial
examples. CoRR, abs/1708.05207, 2017.
Elad Hazan, Adam Klivans, and Yang Yuan. Hyperparameter optimization: A spectral approach. arXiv preprint
arXiv:1706.00764, 2017.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited
queries and information. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2137-
2146, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://Proceedings.
mlr.press/v80/ilyas18a.html.
Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial attacks
with bandits and priors. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=BkMiWhR5K7.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. 2017. URL
https://arxiv.org/abs/1611.01236.
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signSGD via zeroth-order oracle. In Interna-
tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=
BJe-DsC5Fm.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and
black-box attacks. arXiv preprint arXiv:1611.02770, 2016.
Pei-Hsuan Lu, Pin-Yu Chen, and Chia-Mu Yu. On the limitation of local intrinsic dimensionality for characteriz-
ing the subspaces of adversarial examples. arXiv preprint arXiv:1803.09638, 2018.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song,
Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local intrinsic dimensionality.
arXiv preprint arXiv:1801.02613, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Seungyong Moon, Gaon An, and Hyun Oh Song. Parsimonious black-box adversarial attacks via efficient
combinatorial optimization. arXiv preprint arXiv:1905.06635, 2019.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2574-2582, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via
curvature regularization, and vice versa. arXiv preprint arXiv:1811.09716, 2018.
Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial attacks on deep neural
networks. In CVPR Workshops, volume 2, 2017.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference
on Computer and Communications Security, pages 506-519. ACM, 2017.
Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkelman. A simple explanation for the existence of adversarial
examples with small hamming distance. arXiv preprint arXiv:1901.10861, 2019.
10
Published as a conference paper at ICLR 2020
Florian Tram3r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble
adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017a.
Florian Tram3r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of transferable
adversarial examples. arXiv preprint arXiv:1704.03453, 2017b.
Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming
Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural
networks. arXiv preprint arXiv:1805.11770, 2018.
Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Mixtrain: Scalable training of formally robust
neural networks. arXiv preprint arXiv:1811.02625, 2018.
Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial examples
with adversarial networks, 2018. URL https://openreview.net/forum?id=HknbyQbC-.
Yun-Bin Zhao. Sparse optimization theory and methods. CRC Press, an imprint of Taylor and Francis, Boca
Raton, FL, 2018. ISBN 978-1138080942.
Tianhang Zheng, Changyou Chen, and Kui Ren. Distributionally adversarial attack. arXiv preprint
arXiv:1808.05537, 2018.
Appendix A.	Noisy FGSM
This section shows the performance of the noisy FGSM on standard models (described in Section 1 of the main
paper) on the MNIST, CIFAR10 and IMAGENET datasets. In Figure 4, we consider the '∞ threat perturbation
constraint. Figure 5 reports the performance for the 2 setup. Similar to Ilyas et al. (2019), for each k in the
experiment, the top k percent of the signs of the coordinates—chosen either randomly (random-k) or by
the corresponding magnitude ∣∂L(x,y)∕∂xi∣ (top-k)—are set correctly, and the rest are set to —1 or +1
at random. The misclassification rate shown considers only images that were correctly classified (with no
adversarial perturbation). In accordance with the models’ accuracy, there were 987, 962, and 792 such images
for MNIST, CIFAR10, and IMAGENET out of the sampled 1000 images, respectively. These figures also serve
as a validation for Theorem 1 of the main paper when compared to SignHunter’s performance shown in
Appendix D.
(a)
0%	20%	40%	60%	80%	100%
k percent of CIFAR10 coordinates
(b)
0%	20%	40%	60%	80%
k percent of IMAGENET coordinates
(c)
Figure 4: Misclassification rate of three neural nets (for (a) MNIST, (b) CIFAR10, and (c) IMAGENET,
respectively) on the noisy FGSM’s adversarial examples as a function of correctly estimated coordinates of
sign(Vχf (x,y)) on random 1000 images from the corresponding evaluation dataset, with the maximum
allowed '∞ perturbation e being set to 0.3, 12, and 0.05, respectively. Across all the models, estimating the sign
of the top 30% gradient coordinates (in terms of their magnitudes) is enough to achieve a misclassification rate
of 〜70%. Note that Plot (c) is similar to Ilyas et al. (2019)’S Figure 1, but it is produced with TensorFloW
rather than PyTorch.
11
Published as a conference paper at ICLR 2020
m30M201∙10Mm
ΓΓΓΓΓΓΓΓ
Oooooooo
θj UOΓ0yΓSSIDSriU
0%	20%	40%	60%	80%	100%
k percent of MNIST coordinates
(a)
0%	20%	40%	60%	80%	100%
k percent of CIFAR10 coordinates
(b)
0%	20%	40%	60%	80%	100%
k percent of IMAGENET coordinates
(c)
Figure 5: Misclassification rate of three neural nets (for (a) MNIST, (b) CIFAR10, and (c) IMAGENET,
respectively) on the noisy FGSM’s adversarial examples as a function of correctly estimated coordinates of
sign(Vχf (x,y)) on random 1000 images from the corresponding evaluation dataset, With the maximum
allowed `2 perturbation being set to 3, 127, and 5, respectively. Compared to Figure 4, the performance on
MNIST and CIFAR10 drops significantly.
12
Published as a conference paper at ICLR 2020
Appendix B.	Proofs for Theorems in the Main Paper
In this section, we present a proof of Theorem 1 of Section 3. Note that the theorem makes the assumption that
the finite difference is a good approximation of the directional derivative. This assumption has been the core
concept behind most of the black-box adversarial attack algorithms and we state it here for completeness.
Theorem 1. (Optimality of SignHunter) Given 2dlog(n)+1e queries and that the directional derivative is
well approximated by the finite-difference (Eq. 1 in the main paper), SignHunter is at least as effective as
FGSM (Goodfellow et al., 2015) in crafting adversarial examples.
Proof. Based on the separability property of the directional derivative, the ith coordinate of the gradient sign
vector can be recovered as follows: construct two binary codes u and v such that only their ith bit is different.
Therefore, we have
qi = sign(gi)
ui if DuL(x, y) > DvL(x, y) ,
vi otherwise .
(4)
From the definition of SignHunter, this is carried out for all the n coordinates after 2dlog(n)+1e queries.
Put it differently, after 2dlog(n)+1e queries, SignHunter has flipped every coordinate alone recovering its
sign exactly as shown in Eq. 4 above. Therefore, the gradient sign vector is fully recovered, and one can
employ the FGSM attack to craft an adversarial example. Note that this is under the assumption that our finite
difference approximation of the directional derivative (Eq. 1 in the main paper) is good enough (or at least
rank-preserving).	□
13
Published as a conference paper at ICLR 2020
Appendix C.	Experiments Setup
This section outlines the experiments setup. To ensure a fair comparison among the considered algorithms,
we did our best in tuning their hyperparameters. Initially, the hyperparameters were set to the values reported
by the corresponding authors, for which we observed suboptimal performance. We made use of a synthetic
concave loss function to efficiently tune the algorithms for each dataset × perturbation constraint combination.
The performance curves on the synthetic loss function using the tuned values of the hyperparameters did show
consistency with the reported results from the literature. For instance, we noted that ZO-SignSGD converges
faster than NES, and that BanditsT D outperformed the rest of the algorithms towards the end of query budget.
Further, in our adversarial examples generation experiments, we observed failure rate and query efficiency in
line with the algorithms’ corresponding papers—e.g., compare the performance of BanditsT D and NES in
Table 9 of Appendix D with (Ilyas et al., 2019, Table 1). That said, we invite the community to provide their best
tuned attacks.
Note that SignHunter does not have any hyperparameters to tune. The finite difference probe δ for
SignHunter is set to the perturbation bound as it is used for for both computing the finite difference
and crafting the adversarial examples—see Line 1 in Algorithm 2 of the main paper. This tuning-free setup
of SignHunter offers a robust edge over the state-of-the-art black-box attacks, which often require expert
knowledge to carefully tune their parameters.
Table 3 describes the general setup for the experiments. Table 2 lists the sources of the models we attacked in this
work, while Tables 4, 5, 6, and 7 outline the algorithms’ hyperparameters. Figure 6 shows the performance of the
considered algorithms on a synthetic concave loss function after tuning their hyperparameters. All experiments
were run on a CUDA-enabled NVIDIA Tesla V100 16GB.
A possible explanation of SignHunter’s superb performance is that the synthetic loss function is well-behaved
in terms of its gradient given an image. That is, most of gradient coordinates share the same sign, since
pixels tend to have the same values and the optimal value for all the pixels is the same Xmin+xmax . Thus,
SignHunter will recover the true gradient sign with as few queries as possible (recall the example in Section 3
of the main paper). Moreover, given the structure of the synthetic loss function, the optimal loss value is always
at the boundary of the perturbation region; the boundary is where SignHunter samples its perturbations.
Table 2: Source of attacked models.
Model	Source
MNIST models	https://github.com/MadryLab/mnist_challenge
CIFAR10 models	https://github.com/MadryLab/cifar10_challenge
IMAGENET- v3 model	https://bit.ly/2VYDc4X
IMAGENET- v3adv-ens4 model https://bit.ly/2XWTdKx
Table 3: General setup for all the attacks
Parameter	Value MNIST '∞	'2	CIFAR10 '∞	'2	IMAGENET '∞	'2
(allowed perturbation)	0.3	3	12	127	0.05	5
Max allowed queries		10000	
Evaluation/Test set size		1000	
Data (pixel value) Range	[0,1]	[0,255]	[0,1]
Table 4: Hyperparameters setup for NES
Hyperparameter	Value MNIST		CIFAR10		IMAGENET	
	'∞	'2	'∞	'2	'∞	'2
δ (finite difference probe)	0.1	0.1	2.55	2.55	0.1	0.1
η (image 'p learning rate)	0.1	1	2	127	0.02	2
q (number of finite difference estimations per step)	10	20	20	4	100	50
14
Published as a conference paper at ICLR 2020
Table 5: Hyperparameters setup for ZO-SignSGD
Hyperparameter	Value MNIST		CIFAR10		IMAGENET	
	'∞	`2	'∞	`2	'∞	`2
δ (finite difference probe)	0.1	0.1	2.55	2.55	0.1	0.1
η (image `p learning rate)	0.1	0.1	2	2	0.02	0.004
q (number of finite difference estimations per step)	10	20	20	4	100	50
Table 6: Hyperparameters setup for BanditsT D
Hyperparameter	Value MNIST '∞	`2	CIFAR10		IMAGENET	
			'∞	`2	'∞	`2
η (image `p learning rate)	0.03	0.01	5	12	0.01	0.1
δ (finite difference probe)	0.1	0.1	2.55	2.55	0.1	0.1
τ (online convex optimization learning rate)	0.001	0.0001	0.0001	1e-05	0.0001	0.1
Tile size (data-dependent prior)	8	10	20	20	50	50
ζ (bandit exploration)	0.01	0.1	0.1	0.1	0.01	0.1
Table 7: Hyperparameters setup for SignHunter
Hyperparameter	Value MNIST '∞	'2	CIFAR10 '∞	'2	IMAGENET '∞	'2
δ (finite difference probe)	0.3	3	12	127	0.05 5
SSOI 0o∙eJ0>e
SSOI 0weJ0>e
(a) MNIST '∞
×107
0 1 1 2 2 3 3
. . . . . .
1111111
-------
SSOI ©we」。Ae
0	200	400	600	800	1000
# queries
(c) CIFAR10 '∞
Bandi BanditsTD
NES
-22000 SignHunter
ZOSignSGD
(e) Imagenet '∞
200	400	600	800	1000
# queries
-24000」0
(b) MNIST `2
×107
SSOI 0o∙eJ0>e
-23200
-23400
-23600-
-23800-
(d) CIFAR10 `2
(f) IMAGENET `2
0	200	400	600	800	1000
# queries
Figure 6: Tuning testbed for the attacks. A synthetic loss function was used to tune the performance
of the attacks over a random sample of 25 images for each dataset and `p perturbation constraint.
The plots above show the average performance of the tuned attacks on the synthetic loss function
L(x, y) = -(X - x*)T(X - x*), where x* = Xmin + Xmax using a query limit of 1000 queries for
each image. Note that in all, BanditsT D outperforms both NES and ZO-SignSGD. Also, we
observe the same behavior reported by Liu et al. (2019) on the fast convergence of ZO-SignSGD
compared to NES. We did not tune SignHunter; it does not have any tunable parameters.
15
Published as a conference paper at ICLR 2020
Appendix D.	Results of Adversarial Black-Box Examples
Generation
This section shows results of our experiments in crafting adversarial black-box examples on standard models in
the form of tables and performance traces, namely Figures 7, 8, and 9; and Tables 8, 9, and 10.
Table 8: Summary of attacks effectiveness on MNIST under '∞ and '2 perturbation constraints, and
with a query limit of 10, 000 queries. The Failure Rate ∈ [0, 1] column lists the fraction of failed
attacks over 1000 images. The Avg. # Queries column reports the average number of queries made to
the loss oracle only over successful attacks.
Attack	Failure Rate		Avg. # Queries	
	'∞	`2	'∞	`2
BanditsT D	0.68	0.59	328.00	673.16
NES	0.63	0.63	235.07	361.42
Rand	0.33	0.96	847.77	1144.74
SignHunter	0.00	0.04	11.06	1064.22
ZOSignSGD	0.63	0.75	157.00	881.08
Table 9: Summary of attacks effectiveness on CIFAR10 under '∞ and '2 perturbation constraints,
and with a query limit of 10, 000 queries. The Failure Rate ∈ [0, 1] column lists the fraction of failed
attacks over 1000 images. The Avg. # Queries column reports the average number of queries made to
the loss oracle only over successful attacks.
Failure Rate Avg. # Queries
Attack	'∞	'2	'∞			`2
BanditsT D	0.95	0.39	432.24	1201.85
NES	0.37	0.67	312.57	496.99
Rand	0.20	0.89	422.16	1018.17
SignHunter	0.07	0.21	121.00	692.39
ZOSignSGD	0.37	0.80	161.28	528.35
Table 10: Summary of attacks effectiveness on IMAGENET under '∞ and '2 perturbation constraints,
and with a query limit of 10, 000 queries. The Failure Rate ∈ [0, 1] column lists the fraction of failed
attacks over 1000 images. The Avg. # Queries column reports the average number of queries made to
the loss oracle only over successful attacks.
Failure Rate Avg. # Queries
'∞ Attack		'2	'∞		'2
BanditsT D	0.07	0.11	1010.05	1635.55
NES	0.26	0.42	1536.19	1393.86
Rand	0.72	0.93	688.77	418.02
SignHunter	0.02	0.23	578.56	1985.55
ZOSignSGD	0.23	0.52	1054.98	931.15
16
Published as a conference paper at ICLR 2020
2
`
3 2 1
SSol 3saJ3Aa
0
2000	4000	6000	8000	10000
# queries
» » »
SSOI 3ulaJ3Aa
O
I.
0.515-
0.510
0.505-
&LiBInmSltluππlπBH 3ulaJ3Aa
2000	4000	6000	8000	10000
# queries
0	2000	4000	6000	8000	10000
# queries
» » » » »
&LiBInmSuluπmπBH 3ulaJ3Aa
0.500-
0
&E-nms amsoɔ 3ulaJ3Aa
moo
O O
2 1
O O
Inms
2000	4000	6000	8000	10000
# queries
-64-
- _ ♦
- O O -
Φ3⅛J ssəuuns
saJ ssaɔɔns
0.0∙∣ M
0	2000	4000	6000	8000	10000
# queries
0.0-∣_____________________________________________________
0	2000	4000	6000	8000	10000
# queries
BanditsTD
----NES
Rand
SignHunter
ZOSignSGD
0.0	0.2	0.4	0.6	0.8	1.0
success rate
Ooooo
Ooooo
0 8 6 4 2
SQIJQnb # 3J3A
Figure 7: Performance curves of attacks on MNIST for '∞ (first column) and '2 (second column)
perturbation constraints. Plots of Avg. Loss row reports the loss as a function of the number
of queries averaged over all images. The Avg. Hamming Similarity row shows the Hamming
similarity of the sign of the attack's estimated gradient g with true gradient's sign q*, computed as
1 - ∣∣sign(g) - q*∣∣H/n and averaged over all images. Likewise, plots of the Avg. Cosine Similarity
row show the normalized dot product of g and g* averaged over all images. The Success Rate row
reports the attacks' cumulative distribution functions for the number of queries required to carry out a
successful attack up to the query limit of 10, 000 queries. The Avg. # Queries row reports the average
number of queries used per successful image for each attack when reaching a specified success rate:
the more effective the attack, the closer its curve is to the bottom right of the plot.
17
Published as a conference paper at ICLR 2020
0.0-
0
SSol 3ulaJ3Aa
2000	4000	6000	8000	10000
# queries
0.5 0.5125
≡
员 0.5100
Z
■g 0.5075
身 0.5050
出
E 0.5025
区 0.5000
Φ
0	2000	4000	6000	8000	10000
# queries
0	2000	4000	6000	8000	10000
# queries
02™
— — — — — — I I I I I Illloo
SSoI 3J3ALiBInmSuπmπBH 3J3Λ∙LiBInmS amsoɔ 3J3A
`2
0	2000	4000	6000	8000	10000
# queries
53-
52-
51-
50-
0	2000	4000	6000
# queries
8000	10000
10
08-
06
04
2000	4000	6000	8000	10000
# queries
8642°
♦ ♦ ♦ ♦ ♦
Ooooo
J ssaɔɔns
2000	4000	6000	8000	10000
# queries
BanditSTD
NES
r^- Rand
SignHUnter
----ZOSignSGD
4000	6000	8000	10000
# queries
Oooo
4 3 2 1
SQIJQnb # 3J3A
0-
0.0	0.2	0.4	0.6	0.8
success rate
Oooooo
Oooooo
2 0 8 6 4 2
SQIJQnb # 3J3A
0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8
success rate

Figure 8: Performance curves of attacks on CIFAR10 for '∞ (first column) and '2 (second column)
perturbation constraints. Plots of Avg. Loss row reports the loss as a function of the number
of queries averaged over all images. The Avg. Hamming Similarity row shows the Hamming
similarity of the sign of the attack,s estimated gradient g with true gradient,s sign q*, computed as
1 - ∣∣sign(g) - q*∣∣H/n and averaged over all images. Likewise, plots of the Avg. Cosine Similarity
row show the normalized dot product of g and g* averaged over all images. The Success Rate row
reports the attacks, cumulative distribution functions for the number of queries required to carry out a
successful attack up to the query limit of 10, 000 queries. The Avg. # Queries row reports the average
number of queries used per successful image for each attack when reaching a specified success rate:
the more effective the attack, the closer its curve is to the bottom right of the plot.
18
Published as a conference paper at ICLR 2020
4000	6000
# queries
8000	10000
0	2000	4000	6000	8000	10000
# queries
510
508 ∙
506-
504-
Oooooo
&LiBlnmSluππlπBH 3uaJ3Aa
A0.020∙
,C
a
W 0.015
S
φ
U 0.010
,in
O
U
8 0.005
bθ
cβ
J
ω
点 0.000
0
4000	6000
# queries
8000	10000
2000
4000	6000
# queries
8000	10000
2000	4000	6000	8000	10000
# queries
4 3 2 1 0
Ooooo
♦ ♦ ♦ ♦ ♦
Ooooo
nms amsoɔ 3J3A
2000	4000	6000	8000	10000
# queries
____ ___
L----- ssaɔɔns
0	2000	4000	6000	8000	10000
# queries
___ ___ ,
saJ ssaɔɔns
0	2000	4000	6000	8000	10000
# queries
SQIJQnb # 3ulaJ3Aa
0-
0.0
0.2	0.4	0.6	0.8
success rate
-- --
SQIJQnb # 3ulaJ3Aa
0
0.0	0.2	0.4	0.6	0.8
success rate
Figure 9: Performance curves of attacks on IMAGENET for '∞ (first column) and '2 (second
column) perturbation constraints. Plots of Avg. Loss row reports the loss as a function of the
number of queries averaged over all images. The Avg. Hamming Similarity row shows the Hamming
similarity of the sign of the attack's estimated gradient g with true gradient,s sign q*, computed as
1 - ∣∣sign(g) - q*∣∣H/n and averaged over all images. Likewise, plots of the Avg. Cosine Similarity
row show the normalized dot product of g and g* averaged over all images. The Success Rate row
reports the attacks’ cumulative distribution functions for the number of queries required to carry out a
successful attack up to the query limit of 10, 000 queries. The Avg. # Queries row reports the average
number of queries used per successful image for each attack when reaching a specified success rate:
the more effective the attack, the closer its curve is to the bottom right of the plot.
19
Published as a conference paper at ICLR 2020
Appendix E.	Public Black-Box Challenge Results
This section shows results of our experiments in crafting adversarial black-box examples on adversarially trained
models in the form of tables and performance traces, namely Tables 11, 12, 13, and Figure 10.
Table 11: Leaderboard of the MNIST black-box challenge. Adapted from the challenge’s website—as
of May 15, 2019.
Black-Box Attack	Model Accuracy
SignHunter (Algorithm 2 in the main paper)	91.47%
Xiao et al. (2018)	92.76%
PGD against 3 independently & adversarially trained copies of the net- work	93.54%
FGSM on the CW loss for model B from (Tramer et al., 2017a)	94.36%
FGSM on the CW loss for the naturally trained public network	96.08%
PGD on the cross-entropy loss for the naturally trained public network	96.81%
Attack using Gaussian Filter for selected pixels on the adversarially trained public network	97.33%
FGSM on the cross-entropy loss for the adversarially trained public network	97.66%
PGD on the cross-entropy loss for the adversarially trained public net- work	97.79%
Table 12: Leaderboard of the CIFAR10 black-box challenge. Adapted from the challenge’s website—
as of May 15, 2019.
Black-Box Attack	Model Accuracy
SignHunter (Algorithm 2 in the main paper)	47.16%
PGD on the cross-entropy loss for the adversarially trained public net-	63.39%
work	
PGD on the CW loss for the adversarially trained public network	64.38%
FGSM on the CW loss for the adversarially trained public network	67.25%
FGSM on the CW loss for the naturally trained public network	85.23%
Table 13: Top 1 Error Percentage. The numbers between brackets are computed on 10,000 images
from the validation set. The rest are from (Tramer et al., 2017a, Table 4).
Model	clean	Max. Black-box	Sign after 20 queries	Hunter after 1000 queries
v3adv-ens4	24.2 (26.73)	334	(40.61)	(90.75)
20
Published as a conference paper at ICLR 2020
Madry's Lab (MNIST)
	 	
1000	2000	3000	4000	5000
# queries
_ ___ ____ ___
WU-IUlUeH 0weJ0>e
1000	2000	3000	4000	5000
# queries
Madry's Lab (CIFAR10)
-， ，-， - - U 5 O 5 O
321 76655
・ -一 . . . . .
Ooo Ooooo
SSOI 0weJ0>eJ-lu-s WU-IUlUeH 0weJ0>e
Ensemble Adv. Training (IMAGENET)
1 C
_ _________ __________ _________
^aJj-lu-s WU-IUlUeH 0weJ0>e
^=Jjt-w-s QU-SOOQWeJQAe
1000	2000	3000	4000	5000
# queries
0	1000	2000	3000	4000	5000	0	200	400	600	800	1000
# queries	# queries
0.0008-
0.0006-
0.0004-
0.0002-
0	200	400	600	800	1000
# queries
2 2 110
. . . . .
Ooooo
J-luQU-SOOQWeJQAe
0.00-
0	1000	2000	3000	4000	5000
# queries
^=Jj,-lu-s QU-SOOQWeJQAe
0	1000	2000	3000	4000	5000
# queries
_ ______
Ss08ns
140
120
100
80
60
40
20
0
_ _______
Ss08ns
0	200	400	600	800	1000
# queries
Oooo
5 0 5
SQ-JQnb # 0weJ0>e
0	1000	2000	3000	4000	5000
# queries
SQ-JQnb # 0weJ0>e
0.00	0.01	0.02	0.03	0.04	0.05	0.06	0.07	0.0	0.1	0.2	0.3	0.4	0.5	0.0	0.2	0.4	0.6	0.8
success rate	success rate	success rate
Figure 10: Performance curves of attacks on the public black-box challenges for MNIST (first
column), CIFAR10 (second column) and IMAGENET (third column). Plots of Avg. Loss row
reports the loss as a function of the number of queries averaged over all images. The Avg. Hamming
Similarity row shows the Hamming similarity of the sign of the attack’s estimated gradient g with true
gradient,s sign q*, computed as 1 - ∣∣sign(g) - q」H/n and averaged over all images. Likewise,
plots of the Avg. Cosine Similarity row show the normalized dot product of g and g* averaged over all
images. The Success Rate row reports the attacks' cumulative distribution functions for the number
of queries required to carry out a successful attack up to the query limit of 5, 000 queries for MNIST
and CIFAR10 (1, 000 queries for IMAGENET). The Avg. # Queries row reports the average number
of queries used per successful image for each attack when reaching a specified success rate: the more
effective the attack, the closer its curve is to the bottom right of the plot.
21
Published as a conference paper at ICLR 2020
Appendix F.	Histogram of Gradient Coordinates’ Magnitudes
This section illustrates our experiment on the distribution of the magnitudes of gradient coordinates as sum-
marized in Figure 11. How to read the plots: Consider the first histogram in Plot (a) from below; it corre-
sponds to the 1000th image from the sampled MNIST evaluation set, plotting the histogram of the values
{∣∂L(x, y)∕∂xi∣}ι≤i≤n, where the MNIST dataset has dimensionality n = 784. These values are in the range
[0, 0.002]. Overall, the values are fairly concentrated—with exceptions, in Plot (e) for instance, the magnitudes
of the 〜400th image,s gradient coordinates are spread from 0 to 〜0.055.
Il
MNIST	I	CIFAR10	I	IMAGENET
səjEinPJooj JU9IPE3
x segamI lanigir
gradient coordinate magnitude
(a)
gradient coordinate magnitude
(b)
gradient coordinate magnitude
(c)
),x(∞B ∈ segamI debrutreP
gradient coordinate magnitude
(e)
gradient coordinate magnitude
(f)
Figure 11: Magnitudes of gradient coordinates are concentrated: Plots (a), (b), and (c) show
histograms of the magnitudes of gradient coordinates of the loss function L(x, y) with respect to the
input point (image) x for MNIST, CIFAR10, and IMAGENET neural net models over 1000 images
from the corresponding evaluation set, respectively. Plots (d), (e), (f) show the same but at input
points (images) sampled randomly within B∞(x, ∈): the '∞-ball of radius e = 0.3, 12, and 0.05
around the images in Plots (a), (b), and (c), respectively.
22
Published as a conference paper at ICLR 2020
Appendix G.	On Schemes for Sign Flips
In this section, we show the performance of different sign flip schemes in comparison to SignHunter. Results
are summarized in Figure 12. SignHunter’s adaptive flips shows a clear advantage over other schemes despite
having a worse upper-bound on the query complexity—e.g., Naive can retrieve the signs in n + 2 queries, as
discussed in Section 3.
8 6 4 2
sərɪənb # əmpjəa"
0.0	0.2	0.4	0.6	0.8	1.0
success rate
sərɪənb # əmpjəa"
0.0	0.2	0.4	0.6	0.8	1.0
success rate
sərɪənb # əmpjəa"
0-
0.0	0.2	0.4	0.6	0.8	1.0
success rate
MNIST '∞	CIFAR10 '∞	IMAGENET '∞
-- --
sərɪənb # əsbjəa"
0.0	0.2	0.4	0.6	0.8	1.0
success rate
sərɪənb # əsbjəa"
0.0	0.2	0.4	0.6	0.8
success rate
sərɪənb # əsbjəa"
0.0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8
success rate
MNIST `2
CIFAR10 `2
IMAGENET `2
Figure 12: Performance of different sign flips patterns for Algorithm 2, Line 8 in the '∞ and '2
perturbation constraints: our proposition (SignHunter), random sign flips (Rand), sequential
single sign flips (Naive), stochastic hill climbing (SHC), which is similar to Rand but retain the flip
only if it is better in terms of the observed model loss. With higher dimensions, SHC is comparable
to SignHunter but does not enjoy a deterministic upper-bound on the query complexity.
23
Published as a conference paper at ICLR 2020
Appendix H.	SignHunter and Recent Related Work
In this section, we discuss recent work related to our proposition.
Parsimonious Black-Box Adversarial Attacks (Moon et al., 2019). Our experiment on the public CIFAR10
black-box attack challenge corresponds to [1, Table 1]. The authors report a 48% success rate (52% model
accuracy) with an average number of queries of 1261. On the other hand, our proposed algorithm achieves a
52.84% success rate (47.16% model accuracy) with an average number of queries of 569. Further, (Moon et al.,
2019, Table 2) corresponds to our results in Appendix D, Table 9; the paper reports a 98.5% success rate with an
average number of queries of 722. Our proposed algorithm achieves a 98% success rate with 578.56 average
number of queries. Based on these numbers, SignHunter demonstrates better performance than (Moon et al.,
2019)’s attack.
Simple Black-Box Attack (SIMBA) (Guo et al., 2019). The main distinction is that SIMBA performs a
ternary flip over {-δ, 0, +δ} for one random single coordinate at an iteration with δ ≤ . On the other
hand, SignHunter performs a binary flip {-, } for a group of coordinates at an iteration. Most of Guo et al.
(2019)’s experiments were performed for the `2 perturbation constraint and against models different from those
considered in this paper—except for the IMAGENET v3 model, which the authors find much more difficult to
attack. The v3 curves at 10, 000 queries in (Guo et al., 2019, Figure 4) for SIMBA (and its variant SIMBA-DCT)
look comparable to SignHunter’s of Figure 9. For completeness, we implemented SIMBA and evaluated it
against the CIFAR10 model in Section 4. The results are shown in Figure 13. In line with Guo et al. (2019),
SIMBA is a strong baseline in the'2 setup. However, its performance drops significantly in the '∞ setup.
(a) CIFAR10 '∞
Figure 13: Performance of SIMBA and SignHunter in the '∞ and '2 perturbation settings of
Section 4 on CIFAR10. The plots show the average number of queries used per successful image for
each attack when reaching a specified success rate. In line with Guo et al. (2019), we used a step size
of δ = 50 for `2 (the authors used δ = 0.2 for [0, 1]-valued pixels, our setup takes images in [0, 255]
so δ = 0.2 * 255 〜50). For '∞, We used δ = 2, following NES,s setup in Table 4.
Harmonica (Hazan et al., 2017). Both SignHunter and Harmonica seek to optimize a black-box function
over the binary hypercube {±1}n, albeit with different assumptions on the objective function. Harmonica
assumes that the objective function can be approximated by a sparse and low degree polynomial in the Fourier
basis. Our assumption with SignHunter is that the objective function is separable (Property 1, Section 3), this
lets us optimize the black-box function with O(n) queries given an initial guess instead of searching over the 2n
vertices. If this assumption is not met, we can restart SignHunter with another guess with a search complexity
of O(mn) where m is the number of restarts. With this difference in assumptions of the two algorithms, we
conducted an empirical comparison using the two sample problems provided along with Harmonica’s authors
implementation. As shown in Table 14 , the results show that SignHunter optimizes the two problems with
8× less number of queries than Harmonica, not to mention the significant computational advantage.
Table 14: Performance comparison of SignHunter and Harmonica on two sample problems
from https://github.com/callowbird/Harmonica. The lower solution quality, the
better.
	Algorithm	Solution Quality	# queries	Time per Query
Problem 1	Harmonica	-50.0	4223	67.47 ms
	SignHunter	-50.0	20	36.30μs
Problem 2	Harmonica	-916.22	4223	60.22 ms
	SignHunter	-916.21	500	584.28μs
24
Published as a conference paper at ICLR 2020
Appendix I.O	N THE '2-'∞ Performance GAP
As discussed in Section 4, in an - `2 threat setup, black-box attacks that are based on continuous optimization
(e.g., NES and BanditsTD) can vary each pixel X within [x - e, x + e]. On the other hand, SignHunter
is restricted to [x — e∕√n, X + e∕√n]. In other words, SignHunter in e — '2 perturbation setup behaves
exactly the same when used in e∕√n — '∞ perturbation setup. This is illustrated in Figure 14
To highlight the additional perturbation space that other algorithms have over SignHunter in the `2 setup,
We ran NES and BanditsTD as representative examples of standard and dimensionality-reduction-based
algorithms against the CIFAR10 model used in Section 4 with an '∞ perturbation setup of e = 127∕√n. In this
and and the `2 setup used in Section 4, SignHunter behaves the same, while the performance of NES and
BanditsT D drops significantly from their `2 performance due to the reduction in the perturbation space.
A possible fix to allow SignHunter to access the additional search space introduced in the `2 setup is to
extend the notion of binary sign flips over {+1, —1} to ternary sign flips over {+1, 0, —1} and we intend to
explore this thoroughly in a future work.
Figure 14: Illustration of adversarial examples crafted by SignHunter in comparison to attacks
that are based on the continuous optimization (e.g., NES and BanditsTD) in both (a) '∞ and (b)
'2 settings. For both e-'2 and ∈∕√n-'∞ perturbation balls, SignHunter behaves the same, while
continuous attacks such as NES have access to more possible perturbations in the `2 setup compared
to their perturbations in the '∞ setup. This is demonstrated on CIFAR10 in Figure 15.
Figure 15: Performance of black-box attacks in the '∞ and '2 perturbation constraints. The plots
show the average number of queries used per successful image for each attack when reaching a
specified success rate. Note that (b) is similar to the `2 setup examined in Section 4.
25