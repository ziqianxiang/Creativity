Published as a conference paper at ICLR 2020
Infinite-horizon Off-Policy Policy Evaluation
with Multiple Behavior Policies
Xinyun ChenIj Lu Wang2 j Yizhe Hang3, Heng Ge4 & Hongyuan Zha5*
1	Insitute for Data and Decision Analytics, The Chinese University of Hong Kong, Shenzhen &
Shenzhen Institute of Artificial Intelligence and Robotics for Society
2	Department of Computer Science, East China Normal University
3	Department of Computer Science, University of Science and Technology of China
4	School of Mathematics and Statistics, Shandong University
5	Insitute for Data and Decision Analytics, The Chinese University of Hong Kong, Shenzhen &
Shenzhen Institute of Artificial Intelligence and Robotics for Society & Georgia Institute of Technology
1 chenxinyun@cuhk.edu.cn, 2luwang@stu.ecnu.edu.cn,
3	hangyhan@mail.ustc.edu.cn, 4 hengge@mail.sdu.edu.cn,
5	zhahy@cuhk.edu.cn
Ab stract
We consider off-policy policy evaluation when the trajectory data are generated by
multiple behavior policies. Recent work has shown the key role played by the state
or state-action stationary distribution corrections in the infinite horizon context
for off-policy policy evaluation. We propose estimated mixture policy (EMP), a
partially policy-agnostic methods to accurately estimate those quantities. With
careful analysis, we show that EMP gives rise to estimates with reduced variance for
estimating the state stationary distribution correction while it also offers a useful
induction bias for estimating the state-action stationary distribution correction.
In extensive experiments with both continuous and discrete environments, we
demonstrate that our algorithm offers significantly improved accuracy compared to
the state-of-the-art methods.
1	Introduction
In many real-world decision-making scenarios, evaluating a novel policy by directly executing it in
the environment is generally costly and can even be downright risky. Examples include evaluating a
recommendation policy (Swaminathan et al., 2017; Zheng et al., 2018), a treatment policy (Hirano
et al., 2003; Murphy et al., 2001), and a traffic light control policy (Van der Pol & Oliehoek, 2016).
Off-policy policy evaluation methods (OPPE) utilize a set of previously-collected trajectories (for
example, website interaction logs, patient trajectories, or robot trajectories) to estimate the value of a
novel decision-making policy without interacting with the environment (PrecUP et al., 2001; DUd´k
et al., 2011). For many reinforcement learning applications, the value of the decision is defined in a
long- or infinite-horizon, which makes OPPE more challenging.
The state-of-the-art methods for infinite-horizon off-policy policy evalUation rely on learning (dis-
counted) state stationary distribution corrections or ratios. In particUlar, for each state in the
environments, these methods estimate the likelihood ratio of the long-term probability measUre for
the state to be visited in a trajectory generated by the target policy, normalized by the probability
measUre generated by the behavior policy. This approach can effectively avoid the exponentially high
variance compared to the more classic importance sampling (IS) estimation methods (pre; Dudlk et al.,
2011; Hirano et al., 2003; Wang et al., 2017; MUrphy et al., 2001), especially for infinite-horizon
policy evaluation (Liu et al., 2018; Nachum et al., 2019; Hallak & Mannor, 2017). However, learning
state stationary distribution requires detailed information on distributions of the behavior policy, and
we call them policy-aware methods. As a consequence, policy-aware methods are difficult to apply
* Corresponding author. On leave from College of Computing, Georgia Institute of Technology.
2Equal Contribution.
1
Published as a conference paper at ICLR 2020
when off-policy data are pre-generated by multiple behavior policies or when the behavior policy’s
form is unknown. To address this issue, Nachum et al. (2019) proposes a policy-agnostic method,
DualDice, which learns the joint state-action stationary distribution correction that is much higher
dimension, and therefore needs more model parameters than the state stationary distribution. Besides,
there is no theoretic comparison between policy-aware and policy-agnostic methods.
In this paper, we propose a OPPE method with behavior policy learning, EMP (estimated mixture
policy) for infinite-horizon off-policy policy evaluation with multiple known or unknown behavior
policies. We call EMP a partially policy-agnostic method in the sense that, EMP does not require any
information on each“physical” behavior policy, instead, it utilizes some aggregated information of
the behavior policies learned from data. In detail, EMP includes a pre-estimation step using certain
parametric model to learn a “virtual” policy (we call it the mixture policy and formally define it in
Section 4). Hence, its performance depends on the accuracy of mixture policy estimation. Like the
method in Liu et al. (2018), EMP obtain OPPE also via learning the state stationary distribution
correction, so it remains computationally cheap and is scalable in terms of the number of behavior
policies. Besides, inspired by Hanna et al. (2019), we provide a theoretic guarantee that EMP
yields smaller mean square error (MSE) than the policy-aware methods in stationary distribution
corrections learning, even in the single-behavior policy setting. On the other hand, compared
to DualDice, EMP learns the state stationary distribution correction of smaller dimension, more
importantly the estimation of the mixture policy can be considered as an inductive bias as far as
the stationary distribution correction is concerned, and hence could achieve better performance
when the pre-estimation is not expensive. In addition, we propose an ad-hoc improvement of EMP,
whose theoretical analysis is left for future studies. EMP is compared with both policy-aware and
policy-agnostic methods in a set of continuous and discrete control tasks and shows significant
improvement.
2	Background and Related Work
We first introduce the general setting of OPPE in infinite horizon. Then we review two families of
OPPE methods, based on importance sampling (IS) and stationary distribution correction learning,
respectively.
2.1	Infinite-horizon Off-policy Policy Evaluation
We consider a Markov Decision Process (MDP) and our goal is to estimate the infinite-horizon
average reward. The environment is specified by a tuple M = hS, A, R, Ti, consisting of a state
space, an action space, a reward function, and a transition probability function. A policy π interacts
with the environment iteratively, starting with an initial state s0. At step n = 0, 1, ... , the policy
produces a distribution ∏(∙∣Sn) over the actions A, from which an action an is sampled and applied
to the environment. The environment stochastically produces a scalar reward r(sn, an) and a next
state Sn+ι 〜T(∙∣Sn, an). The infinite-horizon average reward under policy π is
1N
Rn = lim -TT^^I^^~ E E En [r(sn, an)] .
N→∞ N + 1
n=0
Without gathering new data, off-policy policy evaluation (OPPE) considers the problem of estimating
the expected reward of a target policy π via pre-collected state-action-reward tuples from policies
that are different from π, which are called behavior policies. In our paper, we consider the general
setting that the data are generated by multiple behavior policies πj (j = 1, .., m). Most OPPE
literature has focused on the single-behavior-policy case where m = 1. In this case, we denote the
behavior policy by π0 to distinguish from the multiple-behavior-policy case. Roughly speaking,
most OPPE methods can be grouped into two categories: importance-sampling(IS) based OPPE and
stationary-distribution-correction based OPPE.
2.2	Importance Sampling Policy Evaluation
As for short-horizon off-policy policy evaluation, importance sampling policy evaluation (IS) meth-
ods (Precup et al., 2001; DUd´k et al., 2011; Swaminathan et al., 2017; Precup et al., 2000; Horvitz &
2
Published as a conference paper at ICLR 2020
Thompson, 1952) have shown promising empirical results. The main idea of importance sampling
based OPPE is using importance weighting ∏∕∏j to correct the mismatch between the target policy ∏
and the behavior policy πj that generates the trajectory.
One key element in our EMP method are inspired by importance sampling literature. Li et al. (2015)
and Hanna et al. (2019) show that using estimated behavior policy in the importance weighting can
reduce the mean square error (MSE). EMP also uses estimated policy, but there are two key difference
between EMP and the previous works: (1) EMP is not an IS-based method, it involves a min-max
problem; (2) EMP focuses on multiple-behavior-policy setting while these papers have focused on
single-behavior setting.
2.3	Policy Evaluation via Learning Stationary Distribution Correction
The state-of-the-art methods for long-horizon off-policy policy evaluation are stationary-distribution-
correction based (Liu et al., 2018; Nachum et al., 2019; Hallak & Mannor, 2017). Let dπ0 (s) and dπ(s)
be the stationary distribution of state s under the behavior policy π0 and target policy π respectively.
The main idea of such methods is directly applying importance weighting by ω = d∏/d∏ on the
stationary state-visitation distributions to avoid the exploding variance suffered from IS, and estimate
the average reward as
π(a∣s)
Rπ = E(s,a)〜dn[rGa)] = E(s,a)〜d∏0 ω(S) ∙ πθ(a∣s) r(s,a) .	(I)
For example, Liu et al. (2018) uses min-max approach to estimate ω directly from the data. This
class of methods require exact knowledge of behavior policy π0 and are not straightforward to
apply in multiple-behavior-policy setting. Recently, Nachum et al. (2019) proposed DualDice to
overcome such limitation by learning the state-action stationary distribution correction ω(S, a) =
d∏(s)∏(a∖s)∕d∏o (s)∏o(a∣s).
3	Single B ehavior Policy Estimation
When the behavior policy π0 in (1) is unknown, a natural idea is to estimate it from data. In this
section, we focus on the standard case where the data are generated by a single behavior policy so that
estimation of behavior policy is more straightforward. We first breifly review the method introduced
by Liu et al. (2018) in Section 3.1, which we shall refer as the BCH method in the rest of the paper,
to explain the min-max problem formulation of the stationary distribution correction learning task. In
Section 3.2, we show that behavior policy estimation is beneficial in two aspects. First, it extends
the stationary distribution correction method to settings where behavior policy is unknown. Second,
even when the behavior policy is known with exact values, we prove that the stationary distribution
correction learned using behavior policy estimation has smaller MSE than that using exact values.
Later, we will extend this behavior policy estimation idea to more general multiple-behavior-policy
cases in Section 4.
3.1	Learning Stationary Distribution Correction with Exact Behavior Policy
Assume the data, consisting of state-action-next-state tuples, are generated by a single behavior
policy π0, i.e. D = {(Sn, an, S0n) : n = 1, 2, ..., N}. Recall that dπ0 and dπ are the stationary state
distribution under the behavior and target policy respectively, and ω = dπ /dπ0 is the stationary
distribution correction. In the rest of Section 3, by slight notation abuse, we also denote dπ (S, a) =
dπ (S)π(a∖S), dπ0 (S, a) = dπ0 (S)π0(a∖S) and dπ0 (S, a, S0) = dπ0 (S)π0(a∖S)T(S0∖a, S).
We briefly review the BCH method proposed by Liu et al. (2018). As dπ(S) is the stationary
distribution of Sn as n → ∞ under policy π, it follows that:
d∏(s0) = X d∏(s)∏(a∖s)T(s0∖s, a) = X ω(s) "") d∏ (s)∏o(a∖s)T(s0∖a, s), ∀s0.
π0(a∖S)	0
s,a	s,a
Therefore, for any function f : S → R,
X ω(s')d∏0 (s0)f(s0) = X ω(s)∏(a∖sS)d∏0 (s)πθ(a∖s)T(S0∖a, s)f (s').
s0	s,a,s0	0
3
Published as a conference paper at ICLR 2020
Recall that dπ0 (s, a, s0) = dπ0 (s)π0 (a|s)T (s0 |a, s), so ω and the data sample satisfy the following
equation
E(s,a,s0)~d∏0 (ω(SO) - ω(S) Π(as) ) f (SO) =0, ∀f.
BCH solves the above equation via the following min-max problem:
mωnmax E(s,a,s0)~d∏0
ω(s0)-ω(s)"
n0(a|s)
f(SO)2,
(2)
and use kernel method to solve ω . The derivation of kernel method is put in Appendix A.
3.2	Learning Stationary Distribution Correction with Estimated Behavior
Policy
The objective function in the min-max problem (2), evaluated by data sample, can be viewed as a
one-step importance sampling estimation. As shown in Hanna et al. (2019), importance sampling
with estimated behavior policy has smaller MSE. Motivated by this fact and the heuristic that better
objective function evaluation will lead to more accurate solution, we show that the BCH method can
also be improved by using estimated behavior policy to obtain smaller asymptotic MSE. We will use
this result to build theoretic guarantee for the performance of EMP method in Section 4.
To formally state the theoretic result, we need to introduce more notation. Assume that we are given a
class of stationary distribution correction Ω = {ω(η; s) : η ∈ En}, and there exists no ∈ En such that
the true distribution correction ω(s) = ω(n0; s). Let ω(n; s) be the stationary distribution correction
learned by the min-max problem (2) and ω(n; s) be that learned by a min-max problem similar to (2)
with ∏o replaced with its estimation ∏o. Intuitively, ∏o is estimated from the data sample and appears
in the denominator, as a result, it could cancel out a certain amount of random error in data sample.
Following this intuition and applying the proof techniques in Henmi et al. (2007), we establish the
following theoretic guarantee that using estimated behavior policy 1 yields better estimates of the
stationary distribution correction.
Theorem 1. Under Assumptions 1 and 2, we have, asymptotically
E[(n - no)2] ≤ E[(n - no)2].
As a direct consequence, we derive the finite-sample error bound for ^
Corollary 1. Let N be the number of (s, a, sO) tuples in the data. Under Assumptions 1 and 2,
E[(n - no)2]
Due to the space limit, we put the precise descriptions of Assumptions 1 and 2 (which involves details
of the kernel method that solves the min-max problem (2)) for Theorem 1 and Corollary 1 to hold
and their proofs are in Appendix B.
4 EMP for Multiple B ehavior Policies
In this section, we apply the behavior policy estimation idea and develop our EMP method for OPPE
in settings of multiple behavior policies and establish theoretic variance reduction results for EMP. In
Section 4.1, we first clarify what is the policy to estimate from data when there are multiple behavior
policies. Then, we introduce EMP in Section 4.2 and establish variance reduction result in 4.3.
4.1	Mixture Policy and Mixture State Distribution
Let’s first take a closer look at the distribution of data generated by multiple behavior policies. In
particular, we show that the data from different behavior policies can be pooled together as if they are
generated by a virtual“mixture policy” πM, which plays a key role in derivation of EMP method.
1We use MLE to obtain ∏o.
4
Published as a conference paper at ICLR 2020
In the multiple-behavior setting, we assume the state-action-next-state tuples are generated by m
different unknown behavior policies πj,j = 1, 2, ..., m. For each j, there are Nj state-action-next-
state tuples generated by πj and follows the corresponding stationary state distribution dπj . Let
N = Pj Nj and denote by wj = Nj /N the proportion of data generated by policy πj . We use DM
to denote the data set, then DM = {(sj,nj , aj,nj , s0j,n ) : j = 1, 2, .., m, nj = 1, 2, ..., Nj}. Note
that the policy label j in the subscript is only for notation clarity and it is not revealed in the data.
Then, if we randomly draw a single (s, a, s0) tuple from DM, its distribution function is the mixture
of state-action-next-state tuple distributions generated by each behavior policy:
dM (s, a, s0) :=	wj dπj (s)πj (a|s)T (s0 |a, s).
j
With slight notation abuse, we write dM (s) = Pj wj dπj as the mixture state distribution. For
each state-action pair (a, s), define the mixture policy πM (a|s) as the weighted average of the
behavior policies:
πM (a|s) :=
j
dM (s)
πj (a|s), ∀ (s, a).
(3)
The following result shows that, the multiple-behavior-policy data DM and the corresponding state
distribution dM (s) can be viewed as if they were generated by the mixture policy πM
Proposition 1. The state-action-next-station tuples generated by πM follows the distribution
dM (s, a, s0). As a consequence, dM (s) is the stationary state distribution generated by πM.
Let ωM (s) = dπ (s)/dM (s) be the likelihood ratio of the state distribution generated by the target
policy over the mixture state distribution. We can estimate the average reward by ωM :
Proposition 2. The average award satisfies
Rn = E(s,a)〜dM ωM (S)
π(a∣s)
--------r
πM (a|s)
(s, a) .
(4)
4.2	EMP Method
Proposition 1 shows that dM (s), the state distribution generated by multiple behavior policies, is
equal to the state stationary distribution generated by πM . If πM is known, then, following the
same argument of BCH, we can learn ωM (s) = ω(η0; s) via a similar min-max problem as (2), by
replacing dπ0 with dM and π0 with πM, and then, estimate Rπ using (4), by replacing dπ0 with dM,
π0 with πM and ω with ωM. But πM is usually unknown. Indeed, it involves not only the behavior
polices πj, but also their stationary distributions dπj , which are unknown and hard to compute. In
our EMP method, we will estimate πM directly from data, without learning πj and dπj . In particular,
we assume the mixture policy πM belongs to some parametric family, i.e. πM (a|s) = π(θM; a, s).
For instance, π(θ; a, s) could be a regression model or a neural network. We then estimate θM by
MLE, i.e.
Nj
Θm = arg maxΣΣlog(π(θ; sj,n, aj,n)).	(5)
θ j n=1
After this pre-estimation step, we replace the exact mixture policy πM with the estimated mixture
policy π(Θm; ∙) and finally formulate the following min-max problem for EMP to learn ωM:
minmax E(s,a,s')〜DM (ω(η; s0) - ω(η; S) ：(小)	f f (s0) .	(6)
η f	π(θM; a, s)
Applying Theorem 1, we build the following MSE bound for EMP method.
Theorem 2. Under the same conditions of Theorem 1, if ω(η; S) and ω(η; S) are the stationary
distribution correction learned from (6) and from the same min-max problem but with exact value of
πM, then, asymptotically
E[(n - ηo)2] ≤ E[(η -ηo)2]∙
As a result, E[(η^ — ηo)2] = O (N).
5
Published as a conference paper at ICLR 2020
4.3 Why Pooling is B eneficial for EMP
One important feature of EMP is that it pools the data from different policy behaviors together and
treat them as if they are from a single mixture policy. Of course, pooling makes EMP applicable to
settings with minimal information on the behavior policies, for instance, EMP does not even require
the knowledge on the number of behavior policies. In this part, we show that, the pooling feature of
EMP is not just a compromise to the lack of behavior policy information, it also leads to variance
reduction in an intrinsic manner.
If instead, the data are treated separately according to the behavior policies, we can still use EMP,
or any OPPE method for single behavior policy, to obtain the stationary distribution correction
ωj = dπ /dπj for each behavior policy. Given ωj , a common approach for variance reduction is to
apply multiple importance sampling (MIS) (Tirinzoni et al., 2019; Veach & Guibas, 1995) technique
and the average reward estimator is of the form
m 1 Nj
RMIS = £NZhj(sj,n)ωj(sj,n)r∏(sj,n), With r∏(S) = £ π(a∣s)r(s,a).	(7)
j=1	j n=1	a
Where the function h is often referred to as heuristics and must be a partition of unity, i.e., Pj hj(s) =
1 for all s ∈ S. It has been proved by Veach & Guibas (1995) that MIS is unbiased, and, for given
Wj = Nj/N, there is an optimal heuristic function to minimize the variance of RMIS.
Proposition 3. (Veach & Guibas, 1995) For MIS with fixed values of wj, j = 1, 2, ..., m, among all
possible values of heuristics h, the balanced heuristic
hj(s) = L)Wjdnj(SI 、, ∀j = 1, 2,…，m and S ∈ S,
j=1 wj dπj (s)
reaches the minimal variance.
Plug the optimal heuristic hj (S) into MIS estimator (7), and We Will obtain that the optimal MIS
estimator coincides With the EMP estimator (4). In this light, by pooling the data together and directly
learning ωM, EMP also learns the optimal MIS Weight inexplicitly.
(a) State Distribution Comparison
(b) TV Distance
L ♦	t Z ∖ 1	,1	, ,	1 , i~ ∙ Z 1	7 ∖	F	∙ Z 1	7 ∖ EI	F ♦	1 1 ∙
Figure 1: (a) shoWs that scatter plot of pairs (dπtrue , dπ) and pairs (dπesti , dπ). The diagonal line
indicates exact estimation. The default values of the number of trajectories is 200, and the length of
horizon is 200. (b) shows the weighted total variation distance (TV distance) between d∏true and d∏,
d∏esti and d∏ respectively, along different number of trajectories and the length of horizons.
5	Experiment
In this section, we evaluate EMP on OPPE problems in three discrete-control tasks Taxi, Singlepath,
Gridworld and one continuous-control task Pendulum (see Appendix D.1 for the details), in both
single-behavior-policy (Section 5.1)and multiple-behavior-policy settings (Section 5.2), with follow-
ing purposes: (i) to compare the performance of EMP with existing OPPE methods; (ii) to validate
the theoretical properties for EMP; (iii) to explore potential improvement of EMP methods for future
study. We will release the codes with the publication of this paper for relevant study.
6
Published as a conference paper at ICLR 2020
-→- BCH - EMP -→- DuaIDice -→- WIS
GridWorId
-1.0
Number of Trajectories
Tuncated length
Figure 2: Single-behavior-policy results of BCH, EMP, DualDice and WIS across continuous
and discrete environments with average reward. Each node indicates the mean value and the bars
represents the standard error of the mean.
Number OfTrajectories
Pendulum
5.1	Results for Single Behavior Policy
In this section, we compare the EMP method with BCH, DualDice2 and step-wise importance
sampling (IS) in the setting of single-behavior policy, i.e. the data is generated from a single behavior
policy.
Experiment Set-up. A single behavior policy is learned by a certain reinforcement learning algo-
rithm 3 for evaluating BCH and IS. This single behavior policy then generates a set of trajectories
consisting of s-a-s-r tuples. These tuples are used to estimate the behaviour policy in EMP method 4
as well as estimating the stationary distribution corrections for estimating the average step reward of
the target policy.
Stationary Distribution Learning Performance. To validate Theorem 1, we use the Taxi domain
as an example to compare the stationary distribution dπtrue and dπesti learned by BCH (using exact
behavior policy) and EMP (using estimated behavior policy). Figure 1(a) plots the scatter pairs
(dπtrue, dπ) and (dπesti, dπ) estimated by 200 trajectories of 200 steps. It shows that dπesti approximate
7	1	.1	1	τr-<∙	⅛ /1 ∖ F τr-<∙	⅛ /1 ∖	.1 Ey τ T ,	C	1	1	1	.	7
dπ better than dπtrue. Figure 1(b) and Figure 1(b) compare the TV distance from dπtrue and dπesti to dπ.
The results indicate that both dπtrue and dπesti converge, while dπesti converges faster and is significantly
closer to dπ when the data size is small. These observations are well consistent with Theorem 1.
Policy Evaluation Performance. Figure 2 reports the MSE of policy evaluation by EMP, BCH,
DualDice and IS methods for the 4 different environments. We observe that, (i) EMP consistently
obtains smaller MSE than the other three methods for different sample scales and different environ-
ments. (ii) The performance of EMP, BCH and DualDice improves as the number of trajectories and
length of horizons increase, while the IS method suffers from growing variance.
5.2	Results for Multiple Unknown Behavior Policies
In this section, we compare the performance of EMP with a multiple-behavior version of BCH method,
DualDice and MIS (Precup et al., 2000), and explore potential improvement of EMP methods.
2DualDice was actually designed for discounted problems, not for the average problem as considered in this
paper. However, it is the only policy-agnostic algorithm for off-policy evaluation in literature, to the best of our
knowledge.
3We use Q-learning in discrete control tasks and Actor Critic in continuous control tasks.
4For discrete state-action space, we estimate the behavior policy by count-frequency. For continuous state-
action space, we fit the behavior policy by a neural network model and use MLE to estimate the model parameters.
The details are given in Appendix D.2.
7
Published as a conference paper at ICLR 2020
BCH (pooled) -EMP
DuaIDice MIS
GridWorId
Taxi
SingIePath
Pendulum
200	400	600 BOO
Number OfTrajectories
200	400	600 BOO
Tuncated length
200	400	600 BOO
Number of Trajectories
200	400	600 BOO 1000
Tuncated length
Figure 3: Multiple-behavior-policy results of BCH (pooled), EMP, DualDice and MIS across continu-
ous and discrete environments with average reward.
→- EMP(SingIe) -EMP -→- KL-EMP
Taxi	SingIePath	Pendulum
GridWorId
200	400	600 BOO
Number OfTrajectories
200	400	600 BOO	200	400	600 BOO
Tuncated length	Number of Trajectories
200	400	600 BOO 1000
Tuncated length
Figure 4: Multiple-behavior-policy results of EMP (single), EMP, KL-EMP across continuous and
discrete environments with average reward.
8
Published as a conference paper at ICLR 2020
Experiment Setup. For each environment, we use 5 behavior policies to generate the data. They are
obtained by reinforcement learning after different number of training steps, i.e. the first 20%, 40%,
50%, 70%, 90% of training steps by which we learn the target policy. To form the multiple-behavior-
policy samples, we generate same number of trajectories of equal length from each policy.
Policy Evaluation Performance. We implement 4 methods: EMP, BCH, DualDice and WIS
introduced in pre using balanced heuristics. To compare with policy-aware BCH in the multiple-
behavior-policy setting, we develop an extension BCH method called BCH (pooled), which utilize
the exact values of all the behavior policies5. Figure 3 shows that, in all 4 environments, we found
EMP consistently obtain smaller MSE than the other three methods.
Pooling is Beneficial. To validate the variance reduction result Proposition 3, we compare EMP with
its variation EMP (single) in which trajectories from different behaviors are not pooled. In detail,
EMP (single) applies EMP to each group of data generated by same behavior policy and return the
mean average reward estimation. Figure 4 shows that EMP outperforms EMP (single), which is
consistent with the theoretic variance reduction result. On the other hand, the optimality of EMP
in Proposition 3 holds for fixed wj , we now explore the possibility of further variance reduction
via optimizing wj , i.e. , by re-weighting the proportion of samples generated by different behavior
policies. In detail, we implement a variation of EMP in which the data samples are re-weighted
according to the KL-divergence between its behavior policy and the target policy6. Figure 4 also
shows that the performance of KL-EMP has greater improvement with the increase of sample size
and could outperform EMP in cases of large sample size where the KL-divergence is better estimated.
6 Conclusion
In this paper, we advocate the viewpoint of partial policy-awareness and the benefits of estimating a
“virtual” mixture policy for off-policy policy evaluation. The theoretical results of reduced variance
coupled with experimental results illustrate the power of this class of methods. One key question
that still remains is the following: if we are willing to estimate the individual behavior policies, can
we further improve EMP by developing an efficient algorithm to compute the optimal weights? The
preliminary experiment results suggest that the answer would be yes, and we will leave this for future
study.
Acknowledgement: We thank Zhaoyuan Li for her helpful technical comments on the proofs. Xinyun
Chen is grateful to the financial support from NSFC Grant No. 91646206 and 11901493. Part of
the work done by Hongyuan Zha is supported by Shenzhen Institute of Artificial Intelligence and
Robotics for Society, and Shenzhen Research Institute of Big Data.
Acknowledgments
We thank Zhaoyuan Li for her helpful technical comments on the proofs. Xinyun Chen is grateful
to the financial support from NSFC Grant No. 91646206 and 11901493. Part of the work done by
Hongyuan Zha is supported by Shenzhen Institute of Artificial Intelligence and Robotics for Society,
and Shenzhen Research Institute of Big Data.
5The details of BCH (pooled) are given in Appendix D.4
6The details of KL-EMP are given in Appendix D.3
9
Published as a conference paper at ICLR 2020
References
Dietterich and Thomas G. Hierarchical reinforcement learning with the maxq value function decom-
position. Journal of artificial intelligence research, 13:227-303, 2000.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In
ICML,pp.1097-1104, 2011.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. In ICML, pp. 1372-1383,
2017.
Josiah Hanna, Scott Niekum, and Peter Stone. Importance sampling policy evaluation with an
estimated behavior policy. In ICML, pp. 2605-2613, 2019.
Masayuki Henmi, Ryo Yoshida, and Shinto Eguchi. Importance sampling via the estiamted sampler.
Biometrika, (4):985-991, 2007.
Keisuke Hirano, Guido W Imbens, and Geert Ridder. Efficient estimation of average treatment effects
using the estimated propensity score. Econometrica, pp. 1161-1189, 2003.
Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement from
a finite universe. Journal ofthe American statistical Association, 47(260):663-685, 1952.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. JMLR,
2015.
Qaing Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In NeurIPS, 2018.
Susan A Murphy, Mark J van der Laan, James M Robins, and Conduct Problems Prevention Re-
search Group. Marginal mean models for dynamic regimes. Journal of the American Statistical
Association, pp. 1410-1423, 2001.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. In NeurIPS, 2019.
Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In ICML, pp. 759-766, 2000.
Doina Precup, Richard S. Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning with
function approximation. In ICML, pp. 417T24, 2001.
Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudik, John Langford,
Damien Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation. In NeurIPS, pp.
3632-3642, 2017.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In ICML,pp. 2139-2148, 2016.
Andrea Tirinzoni, Mattia Salvini, and Marcello Restelli. Transfer of samples in policy search via
multiple importance sampling. In ICML, pp. 6264-6274, 2019.
Elise Van der Pol and Frans A Oliehoek. Coordinated deep reinforcement learners for traffic light
control. In NeurIPS, 2016.
Eric Veach and Leonidas J Guibas. Optiamlly combining sampling techniques for Monte Carlo
rendering. In SIGGRAPH,pp. 419-428, 1995.
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudik. Optimal and adaptive off-policy evaluation in
contextual bandits. In ICML, pp. 3589-3597, 2017.
Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and
Zhenhui Li. Drn: A deep reinforcement learning framework for news recommendation. In WWW,
pp. 167-176, 2018.
10
Published as a conference paper at ICLR 2020
A Kernel Method
We use the reproducing kernel Hilbert space to solve the mini-max problem of BCH (Liu et al.
(2018)). The key property of RKHS we leveraged is called reproducing property. The reproducing
property claims, for any function f ∈ H (H is a RKHS), the evaluation of f at point x equals its
inner product with another function in RKHS: f (S) = hf, k(s, •))”
Given the objective function of BCH L(w,f) = E(s,0,s，)〜d∏0 [(ω(s)π0∏(a)-ω )f (s0)]. We use the
reproducing property to obtain the closed form representation of maxf ∈F L(w, f)2, which is shown
as follows:
max L(W, f) = E(s,a,s0)〜d∏o ,(M,a,q0)-dno[A (ω; s,a,s ) δ (w； s,a,s ) k (S , s )]
f∈F
This equation has been proved in BCH Liu et al. (2018).
B Proof of Theorem 1
B.1	Assumptions
In this appendix, we provide the mathematical details and proof of Theorem 1. We first introduce
some notations and assumptions.
We assume the behavior policy ∏o(a∣s) belongs to a class of policies Π = {∏(θ; a,s) : θ ∈ E§},
where Eθ is the parameter space, i.e. there exists θo ∈ Eθ such that ∏o(a∣s) = ∏(θ0; a, s). The
estimated behavior policy ∏o(a∣s) = π(θ; a, s) is obtained Via maximum likelihood method, i.e.
N
θ = argmaXElog(∏(θ; Sn, an)).
n=1
I-V T	. 1 1 ∙ ∙ . . 1	IllC 久
We assume central limit theorem holds for θ :
Assumption 1. (CLT of MLE)
ED[(θ - θo)2] = OQ/N).
Recall that we have assumed in Section 3.2 that the true stationary distribution correction ω(s) =
ω(ηo; s). Using the kernel method introduced in Appendix A, we estimate ω(s) = ω(η; s) by
η = arg min ɪ2 G(η,θ; xi,xj),
η 1≤i,j≤N
with xi = (si , ai , s0i ) and
G(η,θ"χi,xj)) =	(ω(η;Si)	π,ilsi)	-	ω(η,Si))	(ω(η;	Sj)	：&Sj)	- Mη,sj)[	k(Si,sj)∙
π(θ, ai, si)	π(θ, aj, sj)
The BCH method estimate θ by
η = argmin	G(η,θ0; x%,xj).
η
1≤i,j≤N
Assumption 2. We assume the following regularity conditions on G:
1.	G is second order differentiable.
2.	E[∂η ∂θ G(η0, θ0; xi, xj)] is finite.
3.	E[∂η2G(η0, θ0; xi, xj)] is finite and non-zero.
4.	E[∂η G(η0, θ0; xi, xj)2] is finite.
Here we simply write Exi 〜dπ° % 〜dπ° as E for the simplicity of notation.
11
Published as a conference paper at ICLR 2020
B.2	Proof of Theorem 1
Theorem 1.	Under Assumptions 1 and 2, we have, asymptotically
E[(η - ηo)2] ≤ E[(η - ηo)2].
Proof. Following the kernel method,
η = arg min = arg min	G(η, θ; (xi, Xj)) with Xi = (Si, a%, Si) and Si，si+ι.
ηη
1≤i,j≤N
EI	、~~>	C∕^∙V / 人 %/	∖ ∖	C	1
Then, ∑ι≤i,j≤N dnG(η,θ; (xi,xj)) = 0, we have
0 = 龈 X	dηG(η0,	θ0;	(xi, xj )) +	√N(η	-	ηO) N2	X	dηG(η0, θ0; (xi,	xj))
N N 1≤i,j≤N	N 1≤i,j≤N
+ √N(θ - θ) n2	^X dθdηG(ηo, θo; (χi, xj))
1≤i,j≤N
=√--	X	dηG(η0,θ0；(χi,χj)) +	√N(η	-	ηo)E	[dηG(ηo,θo;	(χι,χ2))]
N N 1≤i,j≤N
+ √N(θ - θ)E [∂θ∂ηG(ηo,θo"xι, X2))] + Op(1).
By definition, η is the solution to the optimization Problemmaxn Pι≤i j≤N G(η, θo; (xi, Xj)). There-
fore, 0 = N√N Pι≤i,j∙≤N d〃G(η0,θ0; (xi, Xj))+√N(η - ηo)E [∂jG(ηo, θo; (X1,X2))] + Op(1).
Define S(θ; (Xi, Xj)) = log(π(θ; Si, ai)) + log(π(θ; Sj, aj)). Similarly, θ is the optimal solution to
argmaxθ Pι≤i,j≤N S(θ; (xi, Xj)). Therefore, 0 = N√n Pι≤i,j≤N ∂θS(θo; (xi, Xj)) + √N(θ -
θ0)E [∂θ2S(θ0; (X1, X2))] + op(1). Following the proof of Theorem 1 of (Henmin et al. 2007), it
suffices to prove that
E [∂θ∂nG(ηo,θo"Xι, X2))] = E [-∂nG(ηo,θ°"Xι, X2))∂θS(Θ°"xi, X2))] .	(8)
One can check
E [∂nG(ηo,θo"Xι,X2))]
=E
k(s1，s2)叵 ω(η0;SI) π⅛⅛⅛j
-dnω(η0; SI)) (ω(η0;S2)π⅛⅛⅛
- ω(η0; S02)
+ (dnω(η; S2)∏θ⅛⅛ - dnω(η0; S2)) (ω(η0; SI)∏θ⅛⅛ - ω(η0; SI))
=E
(k(S01, S02) + k(S02, S01))
π( s .	, n(a1|S1)
Sω(η0; SI) π(θo; a1,S1)
-dn ω(η0; SI)) (ω(η0;S2) π⅛⅛⅛
- ω(η0; S02)
The last equality holds because (xi, X2)〜d∏0 (si)∏(xi; no) 0 d∏0 (S2)π(X2; no). For the simplicity
of derivation, in the rest of proof, we denote
g1 = ∂ηω(no; S1)
π(a1∣S1)
π(θo; a1, S1)
- ∂ηω(no; S01), g2 = ω(no; S2)
π(a2∣S2)
π(θo; a2, S2 )
- ω(no; S02 ).
On the other hand, note that
∂θS(θ; (X1, X2))
Then, we derive
∂θπ(θ; a1,S1)
π(θ; a1,S1)2
+ ∂θπ(θ; a2,S2)
π(θ; a2, S2)2 .
E [∂θ∂ηG(no, θ; (X1, X2))]
E (k(S01, S02) + k(S02, S01))
-∂ηω(no; S1)
∂θπ(θo; a1, S1)
-Tg--------五∙ 92 - ω(no; S2)
π(θo; a1, S1)2
∂θπ(θo; a2,S2)
π(θo; a2,S2)2
E (k(S01, S02) + k(S02, S01))
- E (k(S01, S02) + k(S02, S01))
∂θ0π(θo; a1, S1)
∂θπ(θo; a2, S2 )
π(θo; a1,S1)2 91g2	π(θo; a2,S2)2 91 92
dn ω(no; SI) +
π(θo; a1,S1) 92
ω(no; s2)
π(θo; a2,S2)
—
E [-∂ηG(no, θo; (X1, X2))∂θS(θo; (X1, X2))] + E [(k(S01, S02) + k(S02, S01)) (H1 + H2)] .
12
Published as a conference paper at ICLR 2020
Uara ∖τuα rlαnctα ∂ —— dηω5θ;SI)C H 一 s(n0;s2) C NTcfafkaf
Here，We denote HI — ∏(θοwι,sι) g2, H2 一 ∏(θο；a2,S2)g1 Note that
E[g2|a1, s1, s01, s02] = E	ω(η0; s2)
π(a2 |s2)
π(θ0, a2, s2)
- ω(η0, s02)	|a1, s1, s01,
0,
E[g1|a2, s2, s01, s02] = E	∂ηω(η0; s1)
∏(aι∣sι)
π(θ0; a1, s1)
- ∂ηω(η0; s01)	|a2, s2, s01,
0.
Therefore, E[(k(s01, s02) + k(s02, s01)) (H1 + H2)] = 0. So we obtain (8).
B.3 Proof of Corollary 1
Corollary 1. Let N be the number of (s, a, s0) tuples in the data. Under Assumptions 1 and 2,
E[(n -ηο)2]=O(N
Proof. In the prove of Theorem 1, we see that
1
N2 ɪs	dηG(ηο, θο; (xi,Xj)) + KI(n - ηο) + K2(θ - θ) = op(1)∙
1≤i,j≤N
with K1 = E ∂η2G(η0, θ0; (x1,x2)) and K2 = E [∂θ∂ηG(η0, θ0; (x1, x2))]. Therefore,
E[(η - ηo)2] ≤ 2K-2	k2e[(Θ - θo)2] + E
N12	X	dη G(η0,θ0；(χi,χj))
1≤i,j≤N
We assume that CLT holds for the maximum likelihood estimator θ, i.e. E[(θ - θ0)2] = O(1/N).
Besides, as E[∂ηG(η0, θ0; (xi, xj))] = 0, under Condition 4 of Assumption 2, , We can apply the
central limit theorem (for stationary Markov chain) and have
E I I N √Ν	X	dηG(η0,θ0; (χi,xj))
N N	1≤i,j≤N
C Proofs of Theoretic Results for EMP
O(1).
Propostion 1. The state-action-next-station tuples generated by πM follows the distribution
dM (s, a, s0). As a consequence, dM (s) is the stationary state distribution generated by πM.
Proof. It suffices to check that for any s0,
dM(s0) =	dM (s)πM (a|s)T (s0 |a, s).
(9)
s,a
For each behavior policy πj , we have
s,a
Therefore,
wjdπj(s0) =	wj dπj (s)πj (a|s)T (s0 |a, s).
s,a j
□
j
/
∖
2
2
□
Note that the left hand side is simple dM(s0). In the right hand side,
j
Xwkdπk(s)
k wkdπj (s)
πj (a|s) = dM (s)πM (a|s),
and then we immediately obtain (9).
□
13
Published as a conference paper at ICLR 2020
Propostion 2. The average award satisfies
Rn = E(s,a)~dM
ωM (s)
∏M⅞⅛ N"
Proof.
(s, a)I= X ω(S) ∏M¾ …X wjdπj (S)πj(a∣S)
π(als)
E(S，a)~dM 卜(S) ∏m(α∣s) r
X ω(s) π(alsL r(s,a)dM(s)∏M(S) = X “M(s)ω(s)π(a∣s)r(s,a) = R∏.
s,a	πM (a|s)	s,a
□
Theorem 2.	Under the same conditions of Theorem 1, if ω(η; S) and ω(η; S) are the stationary
distribution correction learned from (6) and from the same min-max problem but with exact value of
πM, then, asymptotically
E[(η -ηo)2] ≤ E[(η -ηo)2]∙
As a result, E[(η^ - η0)2] = O (N).
Proof. The proof follows immediately from that of Theorem 1. In particular, assume πM ∈
{π(θ; a, s) : θ ∈ Eθ} and the estimated ∏m = ∏(θ; ∙) is obtained Via
Nj
θ = arg mθaxΣΣlog(π(θ; Sj,n, aj,n)).
j n=1
The rest part of the proof follows the same argument in the proof of Theorem 1.	口
D Experimental Details
D. 1 Environment Description
Taxi (Dietterich & G, 2000) is a 5 × 5 grid world simulating a taxi movement. Six actions are
contained in Taxi: moves North, East, South, West, pick up and drop off a passenger. A reward of 20
is received when it picks up a passenger or drops her/he off at the right place, and a reward of -1 for
each time step. The passengers are allow to randomly appear and disappear at every corner of the map
at each time step. The 5 × 5 grid size yields 2000 states in total (25 × 24 × 5, corresponding to 25
taxi locations, 24 passenger appearance status and 5 taxi status (empty or with one of 4 destinations)).
Gridworld (Thomas & Brunskill, 2016) is a 4 × 4 grid world which including one reward state,
one terminate state and one fire state and thirteen normal state. Four action can be taken in this
environment: up, down, left and right. A reward of -1 will be received while the agent in normal
states, 1 reward is obtained in reward state, 100 reward is got in terminate state and -11 reward will
got in fire state.
SinglePath has 5 states, 2 actions. The agent begins in state 0 and both actions either take the agent
from state n to state n + 1 or cause the agent to remain in state n. If the agent arrives at a new state, it
will receive a +1 reward, otherwise it will get a 1 reward.
Pendulum has a continuous state space of R3 which describes the triangle of and a action space of
[-2, 2].
D.2 Mixture Policy Estimation
For the discrete environments Taxi, Gridworld and SinglePath, the MLE estimate (5) coincide with
count-frequency, and therefore, we directly estimate
∏M (α∣S)
Pj Pn== 1 1(aj,n = a, Sj,n = S)
Pj Pn=II(Sj,n = S)
14
Published as a conference paper at ICLR 2020
For the continuous environment Pendulum, we use a neural network to model the policy. In detail,
we train a two-layer MLP neural network to estimate the policy. The size of the two hidden layers
are both 32 with the learning rate 0.001 and tanh activation function. We use MEL (5) and Adam
optimizer to train the neural network with batch size 128.
D.3 KL-EMP
In EMP algorithm, the proportion of samples from policy πj in the data buffer, wj , is fixed. In an
ad-hoc way, we optimize the weights wjKL according to the KL-divergence between the behavior
policy πj and the target policy. Then, we will generate a new data buffer as follows. First, we sample
j ∈ {1, 2, ..., m} with probability wjKL. Then, given j, we sample uniformly from the subgroup of
data generated by j . Note that, to implement KL-EMP, one do not need to know the exact value of
πj , but one need to know which data are generated from which policy.
In the numerical experiment, we use the following formula to compute wKL
for finite state space:
KL =	Es∈s 1(j = argminι≤k≤m DKL(∏(∙∣s)∣∣∏k(∙∣s)))
Wj	Pm=ι Ps∈s 1(i = argminι≤k≤m DκL(∏(∙∣s)∣∣∏k(∙∣s)))
_ Ps∈S 1(j = argminι≤k≤m DκL(∏(∙∣s)∣∣∏k(∙∣s)))
(10)
|S|
To implement this method for infinite or continuous state space, we replace the set of all possible
states S in (10) with the set of all states that has been visited in the data buffer. Besides, the behavior
policy ∏k is unknown, to estimate the KL-divergence DκL(∏(∙∖s)∖∏k(∙∣s)), We use a neural network
to learn πk from the subgroup of data that are generated from policy πk.
The numerical results show that using the KL weights {wjKL } could achieve smaller MSE compared
to using {wj } as given by the data sample. We believe this approach deserves more careful analysis
in future research studies.
D.4	Additional Experiment Results
Note that EMP, EMP (single) and KL-EMP all have their policy-aware analogues. In order to test
the variance reduction effect of policy estimation, we implement the policy-aware version for each
EMP-type algorithm and compare their performances.
The policy-aware version of EMP (single) is naive BCH, in which we first apply BCH to each
behavior policy and then return the estimation average.
The policy-aware version of EMP is named as BCH (pooled). In BCH (pooled), the corresponding
min-max problem formation is
min max
ωf
E(j,s,a,s0)~D
0	π(a∖s)
ω(Si(S) ∏-(0M
f(s0)2.
(11)
They both pool the data from different behavior policies together and the main difference is that BCH
(pooled) uses the exact behavior policies.
The policy-aware version of KL-EMP is called BCH (KL-polled). The main difference between
BCH (KL-polled) and BCH (polled) is that BCH (KL-polled) utilizes KL-divergence to optimize the
weights.
Comparison results are shown in Figure 5. We observe that the partially policy-agnostic methods
consistently outperform their policy-aware analogues.
15
Published as a conference paper at ICLR 2020
-EMP (Single)	-→- BCH	BCH (KL-pooled)
一^ EMP	— BCH (pooled) -→- KL-EMP
山 SW6O-
山 SW6O_
Taxi
200	400	600 BOO
Number OfTrajectories
Figure 5: Results of policy-aware OPPE methods (BCH, BCH (pooled) and BCH (KL-pooled))
and their corresponding partially policy-agnostic version (EMP (single), EMP and KL-EMP ) across
continuous and discrete environments with average reward.
16