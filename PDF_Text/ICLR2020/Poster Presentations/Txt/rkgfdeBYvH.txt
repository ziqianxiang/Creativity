Published as a conference paper at ICLR 2020
Effect of Activation Functions on the Train-
ing of Overparametrized Neural Nets
Abhishek Panigrahi	Abhishek Shetty *	Navin Goyal
Microsoft Research India	Cornell University	Microsoft Research India
t-abpani@microsoft.com shetty@cs.cornell.edu navingo@microsoft.com
Ab stract
It is well-known that overparametrized neural networks trained using gradient-
based methods quickly achieve small training error with appropriate hyperparam-
eter settings. Recent papers have proved this statement theoretically for highly
overparametrized networks under reasonable assumptions. These results either
assume that the activation function is ReLU or they depend on the minimum eigen-
value of a certain Gram matrix. In the latter case, existing works only prove that
this minimum eigenvalue is non-zero and do not provide quantitative bounds which
require that this eigenvalue be large. Empirically, a number of alternative activation
functions have been proposed which tend to perform better than ReLU at least
in some settings but no clear understanding has emerged. This state of affairs
underscores the importance of theoretically understanding the impact of activation
functions on training.
In the present paper, we provide theoretical results about the effect of activation
function on the training of highly overparametrized 2-layer neural networks. A
crucial property that governs the performance of an activation is whether or not it
is smooth:
•	For non-smooth activations such as ReLU, SELU, ELU, which are not smooth
because there is a point where either the first order or second order derivative
is discontinuous, all eigenvalues of the associated Gram matrix are large under
minimal assumptions on the data.
•	For smooth activations such as tanh, swish, polynomial, which have derivatives
of all orders at all points, the situation is more complex: if the subspace spanned
by the data has small dimension then the minimum eigenvalue of the Gram
matrix can be small leading to slow training. But if the dimension is large and
the data satisfies another mild condition, then the eigenvalues are large. If we
allow deep networks, then the small data dimension is not a limitation provided
that the depth is sufficient.
We discuss a number of extensions and applications of these results.
1	Introduction
It is now well-known that overparametrized feedforward neural networks trained using gradient-based
algorithms with appropriate hyperparameter choices reliably achieve near-zero training error, e.g.,
Neyshabur et al. (2015). Importantly, overparametrization also often helps with generalization; but
our central concern here is the training error which is an important component in understanding
generalization. We study the effect of the choice of activation function (we often just say activation)
on the training of overparametrized neural networks. By overparametrized setting we roughly mean
that the number of parameters or weights in the networks is much larger than the number of data
samples.
The well-known universal approximation theorem for feedforward neural networks states that any
continuous function on a bounded domain can be approximated arbitrarily well by a finite neural
network with one hidden layer. This theorem is generally stated for specific activation functions
such as sigmoid or ReLU. A more general form of the theorem shows this for essentially all non-
polynomial activations (Leshno et al., 1993; Pinkus, 1999; Sonoda & Murata, 2017). This theorem
* Work done when the author WaS at Microsoft Research India.
1
Published as a conference paper at ICLR 2020
concerns only the expressive power and does not address how the training and generalization of
neural networks are affected by the choice of activation, nor does it provide quantitative information
about the size of the network needed for the task.
Traditionally, sigmoid and tanh had been the popular activations but a number of other activations
have also been considered including linear and polynomial activations (Arora et al., 2019a; Du &
Lee, 2018; Kileel et al., 2019). One of the many innovations in the resurgence of neural networks
in the last decade or so has been the realization that ReLU activation generally performs better than
the traditional choices in terms of training and generalization. ReLU is now the de facto standard
for activation functions for neural networks but many other activations are also used which may be
advantageous depending on the situation (e.g. (Goodfellow et al., 2016, Chapter 6)). In practice,
most activation functions often achieve reasonable performance. To quote Goodfellow et al. (2016):
In general, a wide variety of differentiable functions perform perfectly well. Many unpublished
activation functions perform just as well as the popular ones. Concretely, Ramachandran et al.
(2018) provides a list of ten non-standard functions which all perform close to the state of the art at
some tasks. This hints at the possibility of a universality phenomenon for training neural networks
similar to the one for expressive power mentioned above.
Search for activation functions. A number of recent papers have proposed new activations—such
as ELU, SELU, penalized tanh, SiLU/swish—based on either theoretical considerations or automated
search using reinforcement learning and other methods; e.g. Clevert et al. (2016); Klambauer et al.
(2017); Xu et al. (2016); Elfwing et al. (2017); Ramachandran et al. (2018). For definitions, see
Section 2 and Appendix B. These activation functions have been found to be superior to ReLU in
many settings. See e.g. Eger et al. (2018); Nwankpa et al. (2018) for overview and evaluation. We
quote once more from Goodfellow et al. (2016): The design of hidden units is an extremely active
area of research and does not yet have many definitive guiding theoretical principles.
Theoretical analysis of training of highly overparametrized neural networks. Theoretical anal-
ysis of neural network training has seen vigorous activity of late and significant progress was made
for the case of highly overparametrized networks. At a high level, the main insight in these works
is that when the network is large, small changes in weights can already allow the network to fit the
data. And yet, since the weights change very little, the training dynamics approximately behaves as
in kernel methods and hence can be analyzed (e.g. Jacot et al. (2018); Li & Liang (2018); Du et al.
(2019a); Allen-Zhu et al. (2019); Du et al. (2019b); Allen-Zhu et al. (2018); Arora et al. (2019c);
Oymak & Soltanolkotabi (2019)). There are also many other approaches for theoretical analysis, e.g.
Brutzkus et al. (2018); Mei et al. (2018); Chizat & Bach (2018). Because of the large number of
papers on this topic, we have chosen to list only the most closely related ones.
Analyses in many of these papers involve a matrix G, either explicitly (Jacot et al., 2018; Du et al.,
2019a;b) or implicitly (Allen-Zhu et al., 2019). (This matrix also occurs in earlier works (Xie et al.,
2017; Tsuchida et al., 2018).) λmin(G), the minimum eigenvalue of G, is an important parameter
that directly controls the rate of convergence of gradient descent training: the higher the minimum
eigenvalue the faster the convergence. Jacot et al. (2018); Du et al. (2019b) show that λmin(G) > 0
for certain activations assuming that no two data points are parallel. Unfortunately, these results do
not provide any quantitative information. The result of Allen-Zhu et al. (2019), where the matrix
G does not occur explicitly, can be interpreted as showing that the minimum eigenvalue is large
under the reasonable assumption that the data is δ-separated, meaning roughly that no two data points
are very close, and the activation used is ReLU. This quantitative lower bound on the minimum
eigenvalue implies fast convergence of training. So far, ReLU was the only activation for which such
a proof was known.
Our results in brief. A general result one could hope for is that based on general characteristics of
the activations, such as smoothness, convexity etc., one can determine whether the smallest eigenvalue
of G is small or large. We prove results of this type. A crucial distinction turns out to be whether
the activation is (a) not smooth (informally, has a “kink”) or (b) is smooth (i.e. derivatives of all
orders exist over R). The two classes of functions above seem to cover all “reasonable” activations;
in particular, to our knowledge, all functions used in practice seem to fall in one of the two classes
above.
•	Activations with a kink, i.e., those with a a jump discontinuity in the derivative of some constant
order, have all eigenvalues large under minimal assumptions on the data. E.g., the first derivatives
of ReLU and SELU, the second derivative of ELU have jump discontinuities at 0. These results
2
Published as a conference paper at ICLR 2020
imply that for such activations, training will be rapid. We also provide a new proof for ReLU with
the best known lower bound on the minimum eigenvalue.
•	For smooth activations such as polynomials, tanh and swish the situation is more complex: the
minimum eigenvalue can be small depending on the dimension of the span of the dataset. We give
a few examples: Let n be size of the dataset which is a collection of points in Rd, and let d0 be the
dimension of the span of the dataset. For quadratic activation, if d0 = O(√n), then the minimum
singular value is 0. For tanh and swish, if d0 = O(log0.75 n), then the minimum eigenvalue is
inverse superpolynomially small (exp(-Ω(n"2d0))). In fact, a significant fraction of eigenvalues
can be small. This implies that for such datasets, training using smooth activations will be slow if
the dimension of the dataset is small. A trade off is possible: assuming stronger bounds on the
dimension of the span gives stronger bounds on the eigenvalues. We also show that these results are
tight in a precise sense. We further show that the above limitation of smooth activations disappears
if one allows sufficiently deep networks.
Unless otherwise stated, we work with one hidden layer neural nets where only the input layer is
trained. This choice is made to simplify exposition; extensions of various types including the ones
that drop the above restriction are possible and discussed in Section 5.
2	Preliminaries
Denote the unit sphere in Rn by SnT = {u ∈ Rn : ∣∣u∣∣2 = 1} where||u『:=∣∣uk2 := P3 *.
For u, v ∈ Rn, define the standard inner product by hu, vi := Pi uivi. Given a set S, denote by
U (S) the uniform distribution over S. N(μ, σ2) denotes the univariate normal distribution with
mean μ and variance σ2. Let ZS denote the indicator of the set S. For a matrix A containing elements
aij , ai denotes its i-th column. We define some of the popular non-traditional activation functions
here: Swish(X) := 1+X-x (Ramachandran et al., 2018) (called SiLU in Elfwing et al. (2017));
ELU(x) := max (x, 0) + min (x, 0) (ex - 1) (Clevert et al., 2016); SELU(x) := α1 max (x, 0) +
α2 min (x, 0) (ex - 1), where α1 and α2 are two different constants (Klambauer et al., 2017). See
Appendix B for more definitions.
We consider 2-layer neural networks
F (x; a, W)
m
(1)
where x ∈ Rd is the input and W = [w1, . . . , wm] ∈ Rm×d is the hidden layer weight matrix
and a ∈ Rm is the output layer weight vector. Activation function φ : R → R acts entrywise
on vectors and c2φ := Ez∈N (0,1)φ(z)2. In the case of one hidden layer nets, we set cφ = 1 to
simplify expressions; this is without loss of generality. For deeper networks we do not do this as
this assumption would result in loss of generality. Elements of W and a have been initialized i.i.d.
from the standard Gaussian distribution. This initialization and the parametrization in Equation 1 are
from Du et al. (2019b). Together, these will be referred to as the DZPS setting. Parametrization in
practice does not have √cm in Equation 1; standard initializations in practice include He (He et al.,
2015) and Glorot initializations (Glorot & Bengio, 2010) and variants. In the DZPS setting, the
argument of the activation can be larger compared to the standard initializations, making the analysis
harder. Our theorems apply to the DZPS setting as well as to standard initializations and for the
latter easily follow as corollaries to the analysis in the DZPS setting. We defer this discussion to the
appendix. Unless otherwise stated, we work in the DZPS setting with one hidden layer neural nets
with initialization as above with only the input layer trained.
Given labeled input data {(xi, yi)}in=1, where xi ∈ Rd and yi ∈ R, we want to find the best fit weights
W so that the quadratic loss L({(xi, yi)}n=ι; a, W) := 2 Pn=ι(yi — F(xi； a, W))2 is minimized.
We train the neural network by fixing the output weight vector a at random initialization and training
the hidden layer weight matrix W (output layer being trained can be easily handled as in Du et al.
(2019a;b); See Appendix M for details). In this paper, we focus on the gradient descent algorithm for
this purpose. The gradient descent update rule for W is given by W(t+1) := W⑶-NWL(W⑶)，
where W(t) denotes the weight matrix after t steps of gradient descent and η > 0 is the learning rate.
The output vector u(t) ∈ Rn is defined by ui(t) := F(xi; a, W(t)).
3
Published as a conference paper at ICLR 2020
Next, we define the matrix alluded to earlier, the Gradient Gram matrix G ∈ Rn×n associated with
the neural network defined in (1), referred to as the G-matrix in the sequel:
gi,j := m1 X akφ0(wkXi)φ0(WTXj)hxi,χji.	⑵
k∈[m]
We will often work with the related matrix M ∈ Rmd×n, whose i-th column is obtained by vectorizing
VwF(xi,yi), i.e., Md(k-i)+i：dk,i ：= akφ0 (WTXi) Xi for k ∈ [m]. G is a Gram matrix： G =
mlMTM. Denote by λi(G) the i-th eigenvalue of G with λι ≥ λ? ≥ ..., and similarly by σ%(M)
the i-th singular value of M. These quantities are related by λi(G) = mmσ2 (M).
Following Allen-Zhu et al. (2019), we make the following mild assumptions on data.
Assumption 1. kXi k2 = 1 ∀i ∈ [n].
Assumption 2. k(In - XiXiT)Xj k2 ≥ δ ∀i, j ∈ [n], i 6= j i.e., the distance between the subspaces
spanned by Xi and Xj is at least δ > 0.
Assumption 1 can be easily satisfied by the following preprocessing: renormalize each Xi so that
IlXiIl ≤ 1/√2, add another coordinate to each Xi so that k Xik = 1/√2 and then add another
coordinate with value 1/√2 to each Xi. This ensures that kXi — Xj∣∣2 ≥ δ implies Assumption 2 for
Xi, Xj, which we later verify empirically for CIFAR10.
3	Review of Relevant Prior Work
To motivate the importance of the G-matrix, let us first consider the continuous time gradient flow
dynamics W⑴=-VWL(W(t)), where L(W) denotes the loss function (in the notation we
suppressed dependence on data and the weights of the output layer) and W denotes the derivative
with respect to t. Let y ∈ Rn be the vector of outputs. It follows from an application of the chain
rule that IU(t) = G(t)(y — u(t)). Here gi(tj ：= mm 卜，Xj) Pm=I Φ0(wkt)TXj)φ0(Wkt)TXi). It can be
shown that as m → ∞, the matrix G(t) remains close to its initial value G(0) which is exactly the
G-matrix (see e.g. Jacot et al. (2018); Arora et al. (2019b) for closely related results). This leads us
to the approximate solution, which upon diagonalizing the PSD matrix G(0) is given by
y - u(t) = X(e-λitviviT)(y -u(0)).	(3)
i∈[n]
Thus, it can be seen that the eigenvalues of the G-matrix, in particular λmin(G(0)), control the rate
of change of the output of the neural network towards the true labels. The following result plays a
central role in the present paper.
Theorem 3.1 (Theorem 4.1 of Du et al. (2019a)). Let φ be ReLU. Define matrix G∞ as
G∞ ：= EW〜N(o,id) IwTxi≥o,wTXj≥o〈Xi, Xj). Assume λ = λmin (G∞) > 0. If we set
m ≥ Ω (n6λ-4κ-3) and η ≤ O (λn-3) and initialize Wk 〜N (0,Id) and ak 〜U{ -1,+1}
for k ∈ [m], then with probability at least 1 -κ over the random initialization, for t ≥ 1 we have
ky - u(t)k22 ≤ (1 - 0.5ηλ)t ky - u(0)k22.
In the theorem above it can be seen that the time required to reach a desired amount of error is inversely
proportional to λ. Du et al. (2019b) extended the above result to general real-analytic functions.
While the definition of the G-matrix shows that it is positive semidefinite, it is not immediately clear
that the matrix is non-singular. But the following theorem, from Du et al. (2019b) says that the matrix
is indeed non-singular under very weak assumptions on the data and activation function. A similar
result for the limit m → ∞ but for more general non-polynomial Lipschitz activations was shown in
Jacot et al. (2018) using techniques from Daniely et al. (2016).
Lemma 3.2 (Lemma F.1 in Du et al. (2019b)). If φ is a non-polynomial analytic function and Xi and
Xj are not parallel for distinct i, j ∈ [n], then λmin (G∞) > 0.
In these papers the number of neurons required and the rate of convergence depend on λmin (G∞)
(e.g. Theorem 3.1 above) and thus it is necessary for the matrix to have large minimum singular
value for their analysis to give useful quantitative bounds. Unfortunately, these papers do not provide
quantitative lower bound for λmin (G∞ ).
4
Published as a conference paper at ICLR 2020
Allen-Zhu et al. (2019) considered L-layer networks using ReLU (see Appendix D for details on their
parametrization). A major step in their analysis is a lower bound on the gradient norm at each step.
Theorem 3.3 (Theorem 3 in Allen-Zhu et al. (2019)). With probability at least 1 -
exp(-Ω(m∕poly(n, 1))) with respect to the initialization, for every W such that k W 一 W(0)∣∣F ≤
1∕poiy(n,δ-1), we have ∣∣VwL (W) ∣∣F ≥ Ω(L(W) mδd-1n-2).
We show that λmin (G) is directly related to the lower bound on the gradient in the case of ReLU. It
is not clear if the same method can be extended to other activation functions.
With this in mind, we aim to characterize the minimum eigenvalue of Gram matrix G∞ . Since, G∞
is the same matrix as G(0) in the limit m → ∞, we will focus on proving high probability bounds
for eigenvalues of G(0).
4	Main Results
4.1	Activations with a kink
For any positive integer constant r, presence of a jump discontinuity in the r-th derivative of the
activation leads to a large lower bound on the minimum eigenvalue. The activation function has the
form φ(x) = φ1(x) Ix<α + φ2(x) Ix≥α, where -1 ≤ α ≤ 1. Recall that Cr+1 denotes the set of
r + 1 times continuously differentiable functions. We need φ1 and φ2 to satisfy the following set of
conditions parametrized by r and denoted Jr :
•	φ1, φ2 ∈ Cr+1 in the domains (-∞, α] and [α, ∞), respectively.
•	The first (r + 1) derivatives of φ1 and φ2 are upper bounded in magnitude by 1 in (-∞, α]
and [α, ∞) respectively.
•	For 0 ≤ i < r, we have φ(1i) (α) = φ(2i) (α).
•	∣φIr)(α) - φ2r)(α)| = 1, i.e., the r-th derivative has ajump discontinuity at α.
Remark. We fix the constants in Jr to 1 for simplicity. We could easily parameterize these constants
and make explicit the dependence of our bounds on these parameters. The requirement on the
boundedness Ofderivatives is also not essential and can be relaxed as “all the action happens” in the
interval [-O(√logm), O(√logm)].
J1 covers activations such as ReLU, SELU and LReLU, while J2 covers activations such as ELU.
Below we state the bound explicitly for J2. Similar results hold for Jr for r ≥ 1. See Section K for
details.
Theorem 4.1 (J2 activations). : If the activation φ satisfies J2 then we have
λmin(G⑼)≥ Ω(δ3n-7(logn)-1),
with probability at least 1 — e-Q(3m/n2) With respect to {wk°)}m=1 and {ak0)}m=1, given that
m > max(Ω(n3δ-1 log(nδ-1)), Ω(n2δ-1 log d)).
The theorem above shows that the presence of a jump discontinuity in the derivative of activation
function (or one of its higher derivatives) leads to fast training of the neural network. For the special
case of ReLU we give a new proof. To our knowledge, lower bound on the minimum eigenvalue
of the G-matrix below is the best known. The proof technique uses Hermite polynomials and is
motivated by our results for smooth activations in the next section. See Section L for details.
Theorem 4.2. If the activation is ReLU and m ≥ Ω(n4δ-3 log4 n), then λma( G(O)) ≥
Ω((δ1∙5 log-15 n), with probability at least 1 - e-Mmδ3n 2 Iog 3 n).
The dependence on n in the above bound is inverse-polylogarithmic as opposed to inverse-polynomial
that seems to result from using the technique of Allen-Zhu et al. (2019). It implies that with
m = Ω(n6∕δ6) in poly(log(n∕e), 1∕δ) steps gradient descent training achieves error less than e.
4.2	Smooth Activations
In contrast to activations with a kink, the situation is more complex for smooth activations and we
can divide the results into positive and negative.
5
Published as a conference paper at ICLR 2020
Negative results for smooth activations. The G-matrix of constant degree polynomial activations,
such as quadratic activation, must have many zero eigenvalues; and of sufficiently smooth activations,
such as tanh or swish, must have many small eigenvalues, if the dimension of the span of data is
sufficiently small:
Theorem 4.3 (restatement of Theorem F.2). Let the activation be a degree-p polynomial such that
Φ0(x) = Pp=(I C'x' and let d0 = dim(span{xι ... Xn}) = O(n1/p). Then we have λmin(G(0))=
0. Furthermore, λk = 0, for k ≥ dn/d0e.
Theorem 4.4 (restatement of Theorem F.10). Let the activation function be tanh and let d0 =
dim(span{x1... Xn}) = O(log0.75 n). Then we have λmin(G(0)) ≤ exp(-Ω(n1∕2d0)), Withprob-
ability at least 1 - 1/n3.5 over the random choice of weight vectors {wk(0)}km=1 and {a(k0)}km=1.
Furthermore, the same upper bound is satisfied by λk, for k ≥ dn/d0e.
See Appendix F for proofs. Note that the bounds above do not make any assumption on the data other
than the dimension of its span. The proof technique generalizes to give similar results for general
classes of smooth activation such as swish (Section 6 and Appendix H). In contrast to the above
result, the average eigenvalue of the G-matrix for all reasonable activation functions is large:
Theorem 4.5 (informal version of Theorem E.1). Let φ be a non-constant activation function, with
Lipschitz constant α and let G be its G-matrix. Then, tr(G) = Ω(n) with high probability.
The previous two theorems together imply that the G-matrix is poorly conditioned when d =
O(log0.75 n) and the activation function is smooth, e.g., tanh. The effect on training of G-matrix
being poorly conditioned can be easily seen in Equation 3 for the m → ∞ case with gradient flow
discussed earlier. For the finite m setting, we show that the technique of Arora et al. (2019c) can be
extended to the setting of smooth functions (see Appendix M.4) to prove the following.
Theorem 4.6. Denote by vi the eigenvectors of G(0) and with λi the corresponding eigenvalue.
With probability at least 1 - κ over the random initialization, the following holds for t ≥ 0,
ky-u(t)I∣2 ≤ (pi=ι(1-ηλi)2t(vT(y-u(0)))2)1/2+e,providedm ≥ Ω(n5κ-1λmin(G(0))-4e-2)
and η ≤ O(n-2λmin(G(0))).
This result can be interpreted to mean that in the small perturbative regime of Du et al. (2019a); Arora
et al. (2019c), smooth functions like tanh do not train fast. The learning rate in the above result is
small as λmin(G(0)) is small. Analyzing the training for higher learning rates remains open.
Positive results for smooth activations. We show that in a certain sense the results of Theorem 4.3
and Theorem 4.4 are tight. Let us illustrate this for Theorem 4.4. Suppose that the activation function
is tanh and that the dimension of the span V of the data x1,..., Xn is Ω(nγ), for a constant Y.
Furthermore, we assume that the data is smoothed in the following sense. We start with a preliminary
dataset X01 , . . . , X0n with the same span V, then we perturb each data point by adding i.i.d. Gaussian
noise, i.e. Xi = X0i + ni, and normalize to have unit Euclidean norm (see Assumption 3 for a precise
statement). This Gaussian noise is obtained by taking the standard Gaussian variable on V multiplied
by a small factor. Thus the new data points have span V. For such datasets we show the following
theorem. For more general theorems for polynomial activations and tanh, see Theorem I.3 and
Theorem I.4 respectively.
Theorem 4.7 (Informal version of Corollary I.4.2). Let the activation be tanh, let the perturbation
noise be of the order (δ∕√n) and d0 = dimspan{xι,..., Xn} ≥ Ω(nγ), for a COnStant γ. Then
we have λma(G(0)) ' Q(6(2/Y)n-(3/Y)) with probability at least 0.99 w.r.t. the noise matrix N,
{wk°)}m=ι and {ak0)}m=ι, provided m ' C(n6/Y6-4/y).
We now say a few words about our assumption that the data is smoothed. Smoothed analysis,
originating from Spielman & Teng (2004), is a general methodology for analyzing efficiency of
algorithms (often those that work well in practice) and can be thought of as a hybrid between worst-
case and average-case analysis. Since in nature, problem instances are often subject to numerical and
observational noise, one can model them by the process of smoothing. Smoothed analysis involves
analyzing the performance of the algorithm on smoothed instances, which can be substantially better
than the worst-case instances. Smoothed analysis has also been used in learning theory and our proof
is inspired by Anderson et al. (2014) addressing a different problem, namely rank-1 decomposition
6
Published as a conference paper at ICLR 2020
of tensors. In the present case, smoothness of the data rules out situations where the data has span d0
in a non-robust sense: most of the data points lie in small dimensional subspace and the dimension of
the span is high because of a few points. Some such condition seems essential.
In another direction, we show that if the network is sufficiently deep for tanh, only the separability
assumption on the data suffices. For deep networks, Du et al. (2019b) generalized the notion of
G-matrix to be the G-matrix for the penultimate layer (see Section J). This matrix and its eigenvalues
play similar role in the dynamics of training as for the one-hidden layer case discussed before; we
continue to denote this matrix by G(0). This result can be generalized to other smooth activations.
Theorem 4.8 (informal version of Theorem J.6). Let the activation be tanh and let the data satisfy
Assumption 1 and Assumption 2. Let the depth L satisfy L = Θ(log1∕δ). Then λmin(G(0)) ≥
e-n(vlog n)》1∕poly(n) With high probability, provided m ≥ Ω (Poly (n, 1∕δ)).
5	Extensions
For a large part of the paper we confine ourselves to the case of one hidden layer where only the
input layer is trained. This is in order to focus on the core technical issues of the spectrum of the
G-matrix. Indeed, our results can be extended along several axes, often by combining our results
for the G-matrix with existing techniques from the literature. We now briefly discuss some of these
extensions. Some of these are worked out in the appendix for completeness.
We can easily generalize to the case when the output layer is also trained (Section M.2). Also, we
have focused on training with gradient descent, but training with stochastic gradient descent can also
be analyzed for activations in Jr (Section M.5).
Generalization bounds from Arora et al. (2019c) can easily be extended to the set of functions
satisfying Jr using techniques from Du et al. (2019b). Similarly, techniques from Allen-Zhu et al.
(2019) for higher depth generalize to functions such as SELU, LReLU and ELU. We believe this also
generalizes to Jr . Other loss functions such as cross-entropy can be handled as well as activations
with more than one kink. The case of multi-class classification can also be handled (Sec. M.3). We
do not pursue these directions in this paper choosing to focus on the core issues about activations.
We briefly discuss extension to more general classes of activations in Appendix H.
6	Proof S ketch
In this section, we provide a high level sketch of the proofs of our results.
Activations with a kink. We first sketch the proof of Theorem K.1, which shows that the minimum
eigenvalue of the G-matrix is large for activations satisfying J1. As an illustrative example, consider
ReLU. Its derivative, the step function, is discontinuous at 0. In their convergence proof for ReLU
networks, Allen-Zhu et al. (2019) prove that the norm of the gradient for a W is large if the loss at
W is large. We observe that their technique also shows a lower bound on the lowest singular value
of the M-matrix by considering the norm of all possible linear combinations of the columns. For
ζ ∈ Sn-1, define the linear combination fζ (w) := Pin=1 ζi φ0(wTxi) xi.
Theorem 6.1 (Informal statement of Claim K.2). Let φ ∈ J1. For any ζ ∈ Sn-1, fζ (w) has large
norm with high probability for a randomly chosen standard Gaussian vector w.
Using an -net argument on ζ , the above result implies a lower bound on the minimum singular
value of M. Allen-Zhu et al. (2019) write w as two independent Gaussian vectors w0 and w00,
with large and small variances respectively. They isolate an event E involving w0 . This event
happens if all but one of the summands in fζ (w) = Pin=1 ζi φ0((w0 + w00)T xi) xi are fixed to
constant values with good probability over the choice of w00 . For the exceptional summand, say
ζj φ0((w0 + w00)T xj)xj, the choice of w0 is such that the argument (w0 + w00)Txj can be on either
side of the jump discontinuity with large probability over the random choice of w00 . The random
choice of w00 now shows that the whole sum is not concentrated and so with significant probability
has large norm. They show that E has substantial probability over the choice of w0 , which implies
that with significant probability kfζ (w)k is large. The property of all but one of the summands being
fixed relies crucially on the fact that the derivative of ReLU is constant on both sides of the origin.
When generalizing this proof to activations in J1 we run into the difficulty that the derivative need
not be a constant function on the two sides of the jump discontinuity. We are able to resolve this
difficulty with additional technical ideas, in particular, using the assumption that ∣φ0(∙)∣ is bounded.
7
Published as a conference paper at ICLR 2020
We work with event E0 involving w0 : in the sum defining fζ (w) there is one exceptional summand
ζj φ0((w0 + w00)T xj) xj such that the sum involving the rest of the summands—while not fixed to a
constant value unlike for ReLU—does not change much over the random choice of w00. Whereas the
exceptional summand varies a lot with the random choice of w00 because the argument moves around
the jump discontinuity. We show that E0 has significant probability, which proves the theorem.
Now, we look at the proof of Theorem 4.1 which handles activations in J2. The goal again is to show
that for any ζ ∈ Sn-1, the function fζ (w) has large norm with good probability for the random choice
of w. To this end, we consider the Taylor approximation of gζ (w) := Pin=1 ζi φ0(wTxi) around w0,
that is, gζ(w0 + w00) = gζ(w0) + Uwg4(WO))Tw00 + H(w0, w00 ) where H is the error term. We
show that ^Vwgζ (w0)∣∣ is likely to be large over the random choice of w0, and so WwgZ (WO))Tw00
is likely to be not concentrated on any single value if the error term H(w0, w00) is sufficiently small,
which we show. To prove that kVwgζ(w0)k is large, note that Vwgζ (w0) = Pin=1 ζi φ00(wT xi) xi,
which allows us to use the argument above for fζ (w) being large in the case of J1. This implies
that gζ(w) is large with good probability, which implies, with further argument, that fζ (w) is large.
Full proofs of these results can be found in Appendix K. As mentioned earlier, the argument can be
generalized to condition Jr for any constant r; we omit the details.
Smooth activations. First we look at the proof sketch for Theorem 4.4. To understand the behavior
of smooth activations under gradient descent, we first look at the behavior of a natural subclass of
smooth functions: polynomials. The proof actually works with the M -matrix introduced earlier
whose spectrum is closely related to that of G = MT M/m. In this case, the problem of computing
the smallest eigenvalue reduces to finding a non-trivial linear combination of the columns of M
resulting in 0. We show that if d0, the dimension of the span of the data, is sufficiently small, then
this can be done implying that the smallest eigenvalue is 0. By a simple extension of the argument,
we can show that in fact the G-matrix has low rank. This gives
Theorem 6.2	(Informal version of Theorem F.2). The G-matrix for polynomial activation functions
has low rank if the data spans a low-dimensional subspace.
Given that polynomials have singular M -matrices, a natural idea is to approximate the smooth
function tanh0 by a suitable family of polynomials, and then use the above theorem to “kill” the
polynomial part using an appropriate linear combination and get an upper bound on the eigenvalue
comparable to the error in the approximation. An immediate choice is Taylor’s approximation. The
Taylor series for tanh0 around 0 has a radius of convergence n/2. Depending on the initialization and
m, the argument of the function can take values outside [-n/2, n/2]. To circumvent this difficulty,
we consider a different notion of approximation. Consider a series of Chebyshev polynomials
P anTn(x) that approximates tanh0(x) in the L∞ norm in some finite interval. The fact that tanh0
can be extended analytically to the complex plane can be used to show that the coefficients of the
above series decay rapidly. The approximation is captured by the following theorem.
Theorem 6.3	(Informal version of Theorem F.4). tanh0 is approximable on the interval [-k, k] in the
L∞-norm by (Chebyshev) polynomials to error > 0 using a polynomial of degree O(k log(k/)).
When applying the lemma above, the degree required for approximation increases with the number
of neurons m. This is because the interval [-k, k], in which the approximation is required to hold,
grows with m (the maximum of mn Gaussians grows as O(√log mn)). This leads the degree of
polynomial to become too large to be “killed” as m becomes larger. Thus, for large m, this fails
to give the required bound. To remedy this, we relax the approximation requirement. Since we
are working with Gaussian initialization of weights a natural relaxation is the L2-approximation
under the Gaussian measure. This leads us to consider the Hermite expansion (see also Daniely et al.
(2016)) of tanh0. The p-th coefficient in Hermite expansion is an integral involving the p-th Hermite
polynomial. For large p, these polynomials are highly oscillatory which makes evaluation of the
coefficients difficult. Fortunately, a theorem of Hille (1940) comes to rescue. Again, the fact that
tanh0 can be analytically extended to a certain region of the complex plane can be used to bound the
decay of the Hermite coefficients, which in turn bounds the error in polynomial approximation:
Theorem 6.4 (Informal version of Theorem G.2). tanh0 is approximable on R in the L2-norm with
respect to the Gaussian measure by (Hermite) polynomials of degree P with error exp(-Ω(√p)).
In contrast, the p-th Hermite coefficients of the step function (also called threshold or sgn) which is
the derivative of ReLU (whose G-matrix has large minimum eigenvalue), decays as p-0.75 (this fact
8
Published as a conference paper at ICLR 2020
underlies Theorem L.2). The L2 -approximation gives us a bound on the expected loss. To argue about
high probability bounds, we need to resort to concentration of measure arguments. This requires the
number of neurons m to be large. Thus, these two notions of approximation complement each other.
Now, using these theorems for the small and large m regimes, we can show that the eigenvalues of
the G-matrix are indeed small as stated in Theorem 4.4. These results can be easily extended to swish.
In fact, the above theorems hold for general functions satisfying certain regularity conditions such as
having an analytic continuation onto a strip of complex plane that contains the domain of interest, e.g.
an interval of R or all of R (Appendix F).
For smoothed data not restricted to small dimension, tanh works well. We sketch the proof of
Theorem 4.7 showing that our results about the limitations of smooth activations are essentially tight
when the data is smoothed. It is well-known (see Fact C.9) that the minimum singular value of a (tall)
matrix M is lower-bounded as follows: take a column of M and consider its distance from the span
of the rest of the columns. The minimum of this quantity over all columns gives a lower bound on the
minimum singular value (up to polynomial factors in the dimensions of M). The problem of lower
bounding λmin (G(0)) then reduces to the problem of lower bounding a product involving (a) the
minimum singular value of X*p, the p-th Khatri-Rao power of the data matrix X = [xι,..., Xn], (b)
the p-th Hermite coefficient of tanh0 (see Lemma I.1 and Lemma I.2). We usep to be approximately
equal to 1∕γ. For (a) we use the above strategy to lower bound the minimum singular value of X*p.
It turns out that for any given column, its distance from the span of the rest of the columns can be
written as a polynomial in the noise variables. We can then use the anticoncentration inequality of
Carbery-Wright (see Fact C.8) to show that this distance is unlikely to be small, and then use the
union bound to show that this is unlikely to be small for every column. For (b), we invoke a result
of Boyd (1984) implying that the upper bound exp(-Ω (√p)) on the p-th Hermite coefficient of
tanh0 used in Theorem 6.4 above is essentially tight. The choice ofp that gives the best lower bound
depends on the activation function.
Depth helps for tanh. We now sketch the proof of Theorem 4.8 (for a formal statement, see
Theorem J.6). For each i ∈ [n] and l ∈ {0, ..., L - 1}, let Xi(l) be the output of layer l on input Xi.
We track the behavior of the Xi(l) as l increases:
Lemma 6.5 (informal version of Lemma J.1 and Lemma J.2). As l increases, the Euclidean norm of
each Xi(l) is approximately preserved. On the other hand, for every i 6= j, |(Xi(l))TX(jl) | shrinks.
This implies that for sufficiently large L, the Gram matrix of the output of the penultimate layer, i.e.
(X(L-1) )TX(L-1), where X(L-1) = [X(1L-1) , . . . , X(nL-1)] is diagonally-dominant and has large
minimum eigenvalue. The rest of the proof has some overlap with the proof for smoothed data
above. For each p ≥ 0, λmin(G(0)) can be lower bounded by a product involving (a) the minimum
eigenvalue of the p-th Hadamard power of the Gram matrix of Xi(L-1), and (b) the p-th Hermite
coefficient of tanh0 (see Equation 47). For (a) we use the diagonal-dominance of the Gram matrix.
For (b) we proceed as in the case of smoothed data. The choice p = Θ(log n) turns out to give the
best lower bound on λmin(G(0) ).
7 Experiments
Synthetic data. We consider n equally spaced data points on S1, randomly lifted to S9. We
randomly label the data-points from U {-1, 1}. We train a 2-layer neural network in the DZPS
setting with mean squared loss, containing 106 neurons in the first layer with activations tanh, ReLU,
swish and ELU at learning rate 10-3. The output layer is not trained during gradient descent. In
Figure 1(a) and Figure 1(b) we plot the squared loss against the number of epochs trained. Results
are averaged over 5 different runs. We observed that the eigenvalues and the eigenvectors stayed
essentially constant throughout training, indicating overparametrized regime. ReLU converges to
zero training error much faster than other activation functions, ELU is faster than tanh and swish. In
Figure 1(c) and Figure 1(d) we plot the eigenvalues at initialization. Eigenvalues of ReLU and ELU
are larger compared to those of tanh and swish. This is consistent with the theory.
Real data. We consider a random subset of 104 images from CIFAR10 dataset (Krizhevsky &
Hinton, 2009). We train a 2-layer network containing 105 neurons in the first layer. First, we verify
Assumption 2 regarding δ-separation of data samples. We plot the L2-distances between all pairs
9
Published as a conference paper at ICLR 2020
(a)	(b)	(C)
ReLU ------ ELU -------- Swish Quadratic -------------Tanh
Eiaen value comparison: n= 50
Eigenvalue indices
(d)
Figure 1: Experiments on synthetiC dataset (From left to right) (a)Rate of ConvergenCe of 2-layer
network for different aCtivations when n = 10 (b) Rate of ConvergenCe of 2-layer network for different
aCtivations when n = 50 (C) Eigenvalues of the G-matrix at initialization for different aCtivations
when n = 10 (d) Eigenvalues of the G-matrix at initialization for different aCtivations when n = 50
of preproCessed images (desCribed in SeCtion 2) in Figure 2(a). It shows that the assumptions hold
for CIFAR10, with δ at least 0.1. Figure 2(b) has the plot of the Cumulative sums of eigenvalues,
normalized to the range [0, 1], of the data CovarianCe matrix. This figure shows that the intrinsiC
dimension of data is muCh larger than O (log n), where n denotes the number of samples.
Eigenvalues of the G-matrix for different aCtivations at initialization are plotted in Figure 2(C). This
shows that ReLU has higher eigenvalues Compared to other aCtivations. However there isn’t muCh
differenCe between the speCtrum of ELU and tanh. This is likely due to the faCt that we are in the
regime of Theorem I.4.
We observed a differenCe in the rate of ConvergenCe while training a 2-layer network, with both
layers trainable, using 256 batCh sized stoChastiC gradient desCent (SGD) with Cross entropy loss
on the random subset of CIFAR10 dataset at l.r. 10-3 (Figure 2(d)). Here we are not in the
overparametrized regime as the eigenvalues and eigenveCtors Change Considerably during training.
Therefore, observations in Figure 2(d) Can be attributed to the eigenvalue plots in Figure 2(C) only in
the first few iterations of SGD.
(a)
0工+JB ①Ξe>u ① 6i∑iJo 6θZ ① Sea
(b)
(c)	(d)
ReLU - Tanh — Swish ELU
Figure 2: Experiments on a random subset of 104 images from CIFAR10 dataset: (a) L2-distances
between all pairs of preprocessed images (b) Semilog plot of sum of squares of top k singular values
of data matrix (c) Eigenvalue distribution of G-matrix at initialization (d) Convergence speed of 2
layer networks using different activation functions.
8 Conclusion
In this paper we characterized the effect of activation function on the training of neural networks in
the overparametrized regime. Our results hold for very general classes of activations and cover all
the activations in use that we are aware of. Many avenues for further investigation remain: there are
gaps between theory and practice because of the differences in the sizes, learning rates, optimization
procedures and architectures used in practice and those analyzed in theory: compared to practice,
most theoretical results in the recent literature (including the present paper) require the size of the
networks to be very large and the learning rate to be very small. Bridging this gap is an exciting
challenge. Fine-grained distinction between the performance of activations is also of interest. For
example, Figure 2(d) shows that ReLU converges much faster compared to the other activations. But
the roles can be reversed based on the architecture and the dataset etc., e.g., Ramachandran et al.
(2018). In a given situation, what makes one activation more suitable than another?
10
Published as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. CoRR, abs/1811.04918, 2018. URL http://arxiv.
org/abs/1811.04918.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research,pp. 242-252, Long Beach, California, USA, 09-15 JUn 2019. PMLR. URL
http://proceedings.mlr.press/v97/allen-zhu19a.html.
Joseph Anderson, Mikhail Belkin, Navin Goyal, LUis Rademacher, and James R. Voss. The more,
the merrier: the blessing of dimensionality for learning large gaUssian mixtUres. In Proceedings of
The 27th Conference on Learning Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, pp.
1135-1164, 2014. URL http://proceedings.mlr.press/v35/anderson14.html.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei HU. A convergence analysis of gra-
dient descent for deep linear neUral networks. In 7th International Conference on Learn-
ing Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019a. URL
https://openreview.net/forum?id=SkMQg3C5K7.
Sanjeev Arora, Simon DU, Wei HU, ZhiyUan Li, RUss SalakhUtdinov, and RUosong Wang. On exact
compUtation with an infinitely wide neUral net. In NeurIPS, 2019b.
Sanjeev Arora, Simon S. DU, Wei HU, Zhi yUan Li, and RUosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neUral networks. volUme
abs/1901.08584, 2019c.
Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities. Oxford Univer-
sity Press, Oxford, 2013. ISBN 978-0-19-953525-5. doi: 10.1093/acprof:oso/9780199535255.
001.0001. URL https://doi.org/10.1093/acprof:oso/9780199535255.001.
0001. A nonasymptotic theory of independence, With a foreword by Michel Ledoux.
John P Boyd. Asymptotic coefficients of hermite function series. Journal of Computational Physics,
54(3):382-410, 1984.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum?
id=rJ33wwxRb.
Anthony Carbery and James Wright. Distributional and Lq norm inequalities for polynomials over
convex bodies in Rn. Math. Res. Lett., 8(3):233-248, 2001. ISSN 1073-2780. doi: 10.4310/MRL.
2001.v8.n3.a1. URL https://doi.org/10.4310/MRL.2001.v8.n3.a1.
LenaiC Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In S. Bengio, H. Wallach, H. Larochelle, K. Grau-
man, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems 31, pp. 3036-3046. Curran Associates, Inc., 2018. URL http :/ /papers .nips .
cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-
over-parameterized-models-using-optimal-transport.pdf.
Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). In ICLR, 2016.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
11
Published as a conference paper at ICLR 2020
Simon S. Du and Jason D. Lee. On the power of over-parametrization in neural networks with
quadratic activation. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmdssan, Stockholm, Sweden,July 10-15, 2018, pp. 1328-1337, 2018. URL
http://proceedings.mlr.press/v80/du18a.html.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In ICLR, 2019a.
Simon Shaolei Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. In ICML, 2019b.
Steffen Eger, Paul Youssef, and Iryna Gurevych. Is it time to swish? comparing deep learning
activation functions across nlp tasks. In EMNLP, 2018.
Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning. CoRR, abs/1702.03118, 2017. URL http:
//arxiv.org/abs/1702.03118.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
neural networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of
Machine Learning Research, pp. 249-256, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010.
PMLR. URL http://proceedings.mlr.press/v9/glorot10a.html.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 31, pp. 582-591. Curran Associates, Inc., 2018. URL
http://papers.nips.cc/paper/7339- which- neural- net- architectures-
give- rise- to- exploding- and- vanishing- gradients.pdf.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31, pp. 571-581. Curran Associates, Inc.,
2018. URL http://papers.nips.cc/paper/7338- how- to- start- training-
the-effect-of-initialization-and-architecture.pdf.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function
on deep neural networks training. CoRR, abs/1902.06853, 2019. URL http://arxiv.org/
abs/1902.06853.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Einar Hille. Contributions to the theory of Hermitian series. II. The representation problem. Trans.
Amer. Math. Soc., 47:80-94, 1940. ISSN 0002-9947. doi: 10.2307/1990002. URL https:
//doi.org/10.2307/1990002.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 8571-
8580. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8076-
neural - tangent - kernel - convergence - and - generalization - in - neural -
networks.pdf.
CG Khatri and C Radhakrishna Rao. Solutions to some functional equations and their applications to
characterization of probability distributions. Sankhya： The Indian Journal of Statistics, SeriesA,
pp. 167-180, 1968.
Joe Kileel, Matthew Trager, and Joan Bruna. On the expressive power of deep polynomial neural
networks. In NeurIPS, 2019.
12
Published as a conference paper at ICLR 2020
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and SePP Hochreiter. Self-normalizing
neural networks. In Advances in neural information processing Systems, pp. 971-980, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiPle layers of features from tiny images.
Technical report, Citeseer, 2009.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection.
Annals of Statistics, pp. 1302-1338, 2000.
N. N. Lebedev. Special functions and their applications. Dover Publications, Inc., New York, 1972.
Revised edition, translated from the Russian and edited by Richard A. Silverman, Unabridged and
corrected republication.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function. Neural
Networks, 6(6):861 - 867, 1993. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-
6080(05) 80131 - 5. URL http :/ /www. sciencedirect. com/ science /article /
pii/S0893608005801315.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
8157-8166. Curran Associates, Inc., 2018. URL http:/ /papers .nips. cc /paper/
8038 - learning - overparameterized - neural - networks -via- stochastic-
gradient-descent-on-structured-data.pdf.
Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural networks.
Ann. Appl. Probab., 28(2):1190-1248, 04 2018. doi: 10.1214/17-AAP1328. URL https:
//doi.org/10.1214/17-AAP1328.
J.C. Mason and D.C. Handscomb. Chebyshev Polynomials. CRC Press, 2002. ISBN 9781420036114.
URL https://books.google.co.in/books?id=8FHf0P3to0UC.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018. ISSN 0027-8424. doi: 10.1073/pnas.1806579115. URL https://www.pnas.org/
content/115/33/E7665.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On
the role of implicit regularization in deep learning. In 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings,
2015. URL http://arxiv.org/abs/1412.6614.
Chigozie Nwankpa, Winifred Ijomah, Anthony Gachagan, and Stephen Marshall. Activation func-
tions: Comparison of trends in practice and research for deep learning. CoRR, abs/1811.03378,
2018.
Ryan O’Donnell. Analysis of boolean functions. Cambridge University Press, 2014.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674,
2019.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 30, pp. 2637-2646. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/6857- nonlinear- random- matrix- theory-
for-deep-learning.pdf.
13
Published as a conference paper at ICLR 2020
Jeffrey Pennington and Pratik Worah. The spectrum of the fisher information matrix of a single-
hidden-layer neural network. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
5410-5419. Curran Associates, Inc., 2018. URL http : / /papers . nips . cc /paper /
7786- the- spectrum- of-the- fisher- information- matrix- of- a- single-
hidden-layer-neural-network.pdf.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In I. Guyon, U. V. Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Infor-
mation Processing Systems 30, pp. 4785-4795. Curran Associates, Inc., 2017. URL http:
/ / papers . nips . cc / paper / 7064 - resurrecting - the - sigmoid - in - deep -
learning- through- dynamical- isometry- theory- and- practice.pdf.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The emergence of spectral universality
in deep networks. In International Conference on Artificial Intelligence and Statistics, AISTATS
2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain, pp. 1924-1932, 2018.
URL http://proceedings.mlr.press/v84/pennington18a.html.
Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica, 8:
143-195, 1999. doi: 10.1017/S0962492900002919.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. In ICLR
Workshop, 2018.
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences, 62(12):1707-1739, 2009.
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with
the 0-1 loss. SIAM J. Comput., 40(6):1623-1646, December 2011. ISSN 0097-5397. doi:
10.1137/100806126. URL http://dx.doi.org/10.1137/100806126.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal
approximator. Applied and Computational Harmonic Analysis, 43(2):233 - 268, 2017. ISSN 1063-
5203. doi: https://doi.org/10.1016/j.acha.2015.12.005. URL http://www.sciencedirect.
com/science/article/pii/S1063520315001748.
Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. J. ACM, 51(3):385-463, 2004. doi: 10.1145/990308.
990310. URL https://doi.org/10.1145/990308.990310.
Ggbor SzegO. Orthogonal polynomials. American Mathematical Society, Providence, R.I., fourth
edition, 1975. American Mathematical Society, Colloquium Publications, Vol. XXIII.
Sundaram Thangavelu. Lectures on Hermite and Laguerre expansions, volume 42 of Mathematical
Notes. Princeton University Press, Princeton, NJ, 1993. ISBN 0-691-00048-4. With a preface by
Robert S. Strichartz.
Russell Tsuchida, Farbod Roosta-Khorasani, and Marcus Gallagher. Invariance of weight distributions
in rectified mlps. In Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsmdssan, Stockholm, Sweden, July 10-15, 2018, pp. 5002-5011, 2018. URL
http://proceedings.mlr.press/v80/tsuchida18a.html.
Richard S Varga. Gersgorin and his circles, volume 36. Springer Science & Business Media, 2010.
Hermann Weyl. Das asymptotische Verteilungsgesetz der Eigenwerte linearer partieller Dif-
ferentialgleichungen (mit einer Anwendung auf die Theorie der Hohlraumstrahlung). Math.
Ann., 71(4):441-479, 1912. ISSN 0025-5831. doi: 10.1007/BF01456804. URL https:
//doi.org/10.1007/BF01456804.
14
Published as a conference paper at ICLR 2020
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In
Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS
2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pp. 1216-1224, 2017. URL http: //
proceedings.mlr.press/v54/xie17a.html.
Bing Xu, Ruitong Huang, and Mu Li. Revise saturated activation functions. In ICLR Workshop,
2016.
15
Published as a conference paper at ICLR 2020
Contents
1	Introduction	1
2	Preliminaries	3
3	Review of Relevant Prior Work	4
4	Main Results	5
4.1	Activations with a kink .................................................... 5
4.2	Smooth Activations ......................................................... 5
5	Extensions	7
6	Proof Sketch	7
7	Experiments	9
8	Conclusion	10
A	Additional Related Work	18
B	Activations	18
C	Preliminary facts and definitions	18
C.1 General Approach for Bounding Eigenvalue of G-Matrix ....................... 20
D	Standard Parametrization and initializations	20
D.1 Note on G-Matrix for Standard Initializations .............................. 21
E	Lower Bound on the Trace of G-Matrix	21
F	Upper Bound on Lowest Eigenvalue for tanh	22
F.1 Polynomial Activation Functions ............................................ 22
F.2 L∞ Approximation using the Chebyshev Polynomials ........................... 24
F.3 L2 Approximation using Hermite Polynomials ................................. 25
F.3.1 DZPS Setting ........................................................ 29
F.3.2 Standard Setting .................................................... 33
G	Upper Bound on Lowest Eigenvalue for Swish	35
H	A discussion on Upper Bound of Lowest Eigenvalue for General Activation Functions 35
I	Lower bound on eigenvalues for tanh when the dimension of the data is not too small 37
J Depth helps for tanh
41
16
Published as a conference paper at ICLR 2020
K Lower Bound on Lowest Eigenvalue for Non-Smooth Functions	49
K.1 J1: The first derivative is discontinuous at a point ........................... 49
K.1.1 DZPS Setting ............................................................ 49
K.1.2 J1 for Standard Setting ................................................. 53
K.2 J2 : The second derivative has jump discontinuity at a point ................... 53
K.2.1 DZPS setting ............................................................ 53
K.2.2 J2 for Standard Setting ................................................. 59
L A new proof for the minimum eigenvalue for ReLU	60
M Extensions and Additional Discussion	62
M.1 Polynomial minimum eigenvalue of G-matrix at time t ............................ 62
M.2 Trainable output layer ......................................................... 62
M.3 Multi Class Output with Cross Entropy Loss ..................................... 63
M.4 Fine-grained analysis for smooth activation functions .......................... 65
M.5 Proof for SGD .................................................................. 69
N Some basic facts about Hermite polynomials	74
17
Published as a conference paper at ICLR 2020
A	Additional Related Work
The literature is extensive; we only mention some of the related work Pennington et al. (2017); Louart
et al. (2018); Hanin (2018); Hanin & Rolnick (2018); Pennington & Worah (2017; 2018); Pennington
et al. (2018). Of these, perhaps the closest to our work is Louart et al. (2018): they keep the random
weights in the input layer fixed and only train the output layer. The main result is determination of the
spectrum of a matrix associated with the input layer with the activation function being Lipschitz. This
matrix is different from one considered here. Also the scaling used is different: both the dimension of
the data, number of samples, and the number of neurons grow at the same rate to infinity. The effect
of the activation function on the training via the spectrum is considered but no results are provided
for popular activations. Hayou et al. (2019) aim to find the best possible initialization of weights, that
lead to better propagation of information in an untrained neural network, i.e. the rate of convergence
of correlation among hidden layer outputs of datapoints to 1 should be of the order Poly(L), where L
is the number of layers. They show smooth activation functions have a slower rate and hence, are
better to use in deep neural networks. However, this set of parameterizations, called “edge of chaos”,
were proven to be essential for training by Lee et al. (2017) under the framework of equivalence of
infinite width deep neural networks to Gaussian processes over the weight parameters. The extent to
which the approximation of stochastic gradient descent by Bayesian inference holds is still an open
problem.
B Activations
We introduce some of the most popular activation functions. These activation functions are unary, i.e.
of type φ : R → R, and act on vectors entrywise: φ((t1, t2, ...)) := (φ(t1), φ(t2), ...). In this paper
we do not study activation functions with learnable parameters such as PReLU, or activation functions
such as maxout which are not unary. Activations functions are also referred to as nonlinearities,
although the case of linear activation functions has also received much attention.
•	ReLU(X) := max (x, 0)
•	LReLU(x) := max (x, 0) + α min (x, 0), where α is a constant less than 1
•	Linear(x) := x
2x
•	tanh(x) := e2χ+ι
•	Sigmoid(x)：= ι+∣-x
•	Swish(x) := ι+χ-χ Ramachandran et al. (2018) (called SiLU in Elfwing et al. (2017))
•	ELU(x) := max (x, 0) + min (x, 0) (ex - 1) Clevert et al. (2016)
•	SELU(x) := α1 max (x, 0) + α2 min (x, 0) (ex - 1), where α1 and α2 are two different
constants Klambauer et al. (2017).
C Preliminary facts and definitions
We note some well known facts about concentration of Gaussian random variables to be used in the
proofs.
Fact C.1. For a Gaussian random variable V 〜N(0, σ2), Vt ∈ (0, σ), we have
Pv{|v| ≥ t} ∈ (1 - ξ-, 1 - I Z).
5σ 3σ
Infact,thefollowing holds true. Vt ∈ (0, σ∕2), we have
Pv{|v - a| ≥ t} ∈ (1 - I-, 1 - J-).
5σ 4σ
where 0 ≤ a ≤ σ.
Fact C.2. For a Gaussian random variable V 〜N(μ, σ2), Vt ∈ (0, σ), we have Vt ≥ 0
t2
Pv {∣v — μ∣≤ t} ≤ 1 — 2e-2σ2.
18
Published as a conference paper at ICLR 2020
Fact C.3 (Hoeffding’s inequality, see Boucheron et al. (2013)). Let X1, X2, .., Xn be n independent
random variables, where Xi lies in the interval [电，bi], and let X be the empirical mean, i.e., X =
Pzn Xi. Then,
2n2t2
Pr (IX - E (X)∣ ≥ t) ≤ 2e-P=IMf)
Fact C.4 (Multiplicative Chernoff bound, see Boucheron et al. (2013)). Let X1, X2, .., Xnbe indepen-
dent random variables taking values in {0,1}, and let X be the empirical mean, i.e., X = EinI Xi.
Then
/	∖	t2 E(五)
Pr(X < (1 - t) E (x)) ≤ e--L,
/	、	t2E(X)
Pr(X > (1 + t) E (X)) ≤ e--2+^
for all t ∈ (0, 1).
Fact C.5 (Maximum of Gaussians, see Boucheron et al. (2013)). Let X1, X2, ..., Xnbe n Gaussians
following N(0, σ2). Then,
Pr
max|Xi | ≤ k
i∈[n]
≥1-
2
k2 _ ι
n ɪ-1
where k > 0 is a constant.
Fact C.6 (Chi square concentration bound, see lemma 1 in Laurent & Massart (2000)). For a variable
X that follows chi square distribution with k degrees of freedom, the following concentration bounds
hold true.
P(x — k ≥ 2√kt + 2t) ≤ exp(-1),
P(k — x ≥ 2√kt) ≤ exp(-1).
Fact C.7 (Mini-max formulation of singular values). Given a matrix M ∈ Rm×n, assuming n ≤ m,
the singular values of M can be defined as follows.
σk (M)
min
U
max
ζ
Mζ
ζ∈U,ζ 6=0 ∣dim(U)=k
∀k ∈ [n].
Fact C.8 (adaptation of Carbery & Wright (2001); see Anderson et al. (2014)). Let Q (X1 , . . . , Xn)
be a multilinear polynomial of degree d. If Var(Q) = 1, when Xi 〜N (0,1) ∀i ∈ [n]. Then,
∀t ∈ R and > 0, there exits C > 0 s.t.
Pr(XI,…,Xn)~N (0,in) (∣Q (X1,..., Xn) - t∣ ≤ e) ≤ Cde1/d.
Fact C.9 (Rudelson & Vershynin (2009)). Given a matrix A ∈ Rn×m, the following holds true for
all i ∈ [m].
-j= min dist(ai, A-i) ≤ σmin(A)
m i∈[m]
where A-i = span aj : j 6= i
Fact C.10 (Gershgorin circle theorem, see e.g. Varga (2010)). Every eigenvalue ofa matrix A ∈
Cn×n, with entries aij, lies within at least one of the discs D(aii, ri) in=1, where ri = Pj:j6=i ∣aij ∣
and D(aii, ri) ⊂ C denotes a disc centered at aii and with radius ri.
Fact C.11 (Weyl’s inequalities (Weyl, 1912)). Let A and B be two hermitian matrices. Then, the
following hold true ∀i, j ∈ [n].
λi+j-1 (A + B) ≤λi(A)+λj(B)
λi+j-n (A + B) ≥ λi(A)+λj(B)
19
Published as a conference paper at ICLR 2020
Definition C.1 (Khatri Rao product and Hadamard product, see Khatri & Rao (1968)). Given a
matrix A ∈ Rm×n and a matrix B ∈ Rp×q, the Kronecker (or tensor) product A 0 B is defined as
the mp × nq matrix given by
A0B=	aiiB a21B . .	a12B ∙ a22B ∙ ..	..	.	ainB •	a2nB . .
	. amiB	. am2B ∙	.. •	amnB
Given a matrix A ∈ Rm×n and a matrix B ∈ Rp×n , the Khatri Rao product A * B is defined as the
mp × n matrix given by
A * B = [ aι 0 bi a2 0 b2	…an 0 bn ].
Given a matrix A ∈ Rm×n and a matrix B ∈ Rm×n , the Hadamard product A B ∈ Rm×n is
defined as
(A *B)ij = aijbij,	∀i ∈ [m], j ∈ [n].
Notation: Given a matrix X ∈ Rm×n, we denote order-r Khatri-Rao product of X as X*r ∈ Rmr×n,
which represents X * X... * X. We denote order-r Hadamard product of X as Xθr ∈ Rn×n, which
`-----------------------{--------}
r times
represents X Θ X... Θ X and can be shown to be equal to (X*r )T X*r.
^^^≡^^^{^^^^^^^^}
^^{^^≡
r times
C.1 GENERAL APPROACH FOR BOUNDING EIGENVALUE OF G-MATRIX
The Gram matrix G can be written as
G = -1 MT M.
m
where M ∈ Rmd×n is defined by
Md(k-1)+1: dk,i = akφ0 wkT xi xi for k ∈ [m].
Denoting by λk(G) and σk(M) as kth eigenvalue and kth singular value of G and M respectively,
we can write λk (G) as *σk (M)2. The minimum singular value of M can be defined (through the
minmax theorem) as
σmin (M) = min kMζk .
ζ∈Sn-1
Hence, the general approach to show a lower bound of σmin (M) is to show that the following
quantity
m
n
2
n
ζimi
i=1
u	akζiφ0(wkTxi)
k=1
i=1
xi
(4)
is lower-bounded for all ζ ∈ Sn-1.
To show an upper bound, we pick a ζ0
∈ Sn-1 and use
n
σmin (M)
min kMζ k ≤	ζi0mi .
n-1
ζ∈S
n-1
i=1
(5)
D Standard Parametrization and initializations
A 2-layer (i.e. 1-hidden layer) neural network is given by
F(x;a,b,W
m
X akφ wkTx +
k=i
20
Published as a conference paper at ICLR 2020
where a ∈ Rm is the output layer weight vector, W ∈ Rd×m is the hidden layer weight matrix and
b ∈ Rm denotes the hidden layer bias vector. This parametrization differs slightly from the choice
made in Equation 1. By standard initialization techniques He et al. (2015) and Glorot & Bengio
(2010), there are two ways in which the initial values of the weights W(0) and a(0) are chosen,
depending on whether the number of neurons in the previous layer is taken into account (fanin) or the
number of neurons in the current layer is taken into account (fanout).
•	Init(fanin): WkO)〜N(0, dId) and akO)〜N(0, mm) Vk ∈ [m],
•	Init(fanout): WkO)〜N(0, .Id) and a，0) 〜N (0,1) Vk ∈ [m].
Note that Allen-Zhu et al. (2019) use Init (fanout) initialization. The elements of b are initialized
from N 0, β2 , where β is a small constant. We set β = 0.1 in Init (fanin) initialization and
β = √= in Init (fanout) initialization.
D.1 NOTE ON G-MATRIX FOR S TANDARD INITIALIZATIONS
We follow the argument in Section 3 to get the G-matrix (G) as
m
gi(jt) = ηXa2kφ0 WkT xi +bk φ0 WkT xj + bk	xi,xj ,
k=1
where η is the gradient flow rate needed to control the gradient during descent algorithm to stop
gradient explosion, i.e. we need to control the maximum eigenvalue of the Gram matrix.
For Init (fanin) and Init (fanout), We set η as 1 and . respectively to get the same Gradient Gram
matrix as in Equation 2.
E LOWER BOUND ON THE TRACE OF G-MATRIX
In this section, We loWer bound the trace of the gradient matrix for a general activation function as a
point of comparison for our results regarding the loWest eigenvalue.
Theorem E.1. Let φ be an activation function, with Lipschitz constant α and let G(O) be the G-
matrix at initialization. Let EW 〜N(o,i)(φ0 (WT x,2) = 2c for a positive COnStant c and for all i.
Then, tr(G(0)) ≥ cn with probability greater than 1 一 e-Wm/m) 一 m-3∙5.
Remark. The constant c depends on the choice of the activation function and is bounded aWay from 0
for most activation functions such as ReLU or tanh.
Proof. The trace of the G-matrix is given by
.n	2
tr(G(0)) = J XX (a" (WjO)Txi))
m j=1 i=1
can be shown to be in the range (-3√logm, 3√logm) with probability
j=1
Using Fact C.5, a(jO)
at-least 1 一 m-3.5. Assuming this is true, We claim the folloWing tWo statements. For any j We have
(expectation is over W(O) and
n
EWjO) ,aj0) : IajO) ∣≤3√log m Σ√
i=1
EL0 Eaj0) ： ∣aj0)∣≤3√l0g≡ (a。)) EWjO) φ^,
2
n
≥ X 2c
i=1
≥ cn.
21
Published as a conference paper at ICLR 2020
where we use the independence of a(j0) and wj(0) and the variance of a(j0) is at-least
taking the bound on a(j0) into account, in the intermediate steps.
(1- qlomm
Also, we get from the Hoeffding bounds,
Pr
mn
xx	a(0)φ0S
j=1 i=1
xi
2
≥ — cn
-2
≤1-
e-Ω(mn2∕α4 log2 m)
as required.
□
This shows that the trace is large with high probability. From it follows that average of eigenvalue is
Ω(1). It also follows that maximum eigenvalue is Ω(1).
F UPPER BOUND ON LOWEST EIGENVALUE FOR tanh
For activation functions represented by polynomials of low degree, we show that the G-matrix
is singular. In fact, the rank of this matrix is small if the degree of the polynomial is small. To
upper bound the lowest eigenvalue of the G-matrix for the tanh activation function, we proceed by
approximating tanh0 with polynomials of low degree. It turns out that tanh0 can be well-approximated
by polynomials in senses to be described below. This allows us to use the result about polynomial
activation functions to show that the minimum singular value of the G-matrix of tanh is small. The
approximation of tanh0 by polynomials turns out to be non-trivial. It does have Taylor expansion
centered at 0 but the radius of convergence is n/2 and thus cannot be used directly if the approximation
is required for bigger intervals.
We consider two different notions of approximations by polynomials, depending on the initialization
and the regime of the parameters. It is instructive to first consider polynomial activations before going
to tanh. Next section is devoted to polynomial activations.
F.1 Polynomial Activation Functions
Let’s begin with the linear activation function. In this case, the G-matrix turns out to be the Gram
matrix of the data. Since each datapoint is low dimensional, we get that it is singular:
Lemma F.1. (Linear activation function) If φ(x) = x and d < n, then the G-matrix is singular, that
is
λmin G(0) = 0.
In fact, at least (n - d) eigenvalues of the G-matrix are 0.
Proof. Since φ (x) = x, we have φ0 (x) = 1 and the G-matrix is given by
G=XX a2) XT X
where X = [x1, x2 , ..., xn] is the matrix containing the xi’s as its columns. Since the xi’s are
d-dimensional vectors, rank(X) ≤ d and so, X can have at most d non-zero singular values, which
leads to at most d non-zero eigenvalues for G.	□
We next show that activation functions represented by low degree polynomial must have singular
Gradient Gram matrices:
Theorem F.2. Let φ0(x) = Ep=O cxX and d0 = dim (Span {xι... xn}). Then, the G-matrix is
singular, that is
λmin G(0) =0,
assuming that the following condition holds,
d0+p <n+1.
p
(6)
22
Published as a conference paper at ICLR 2020
Proof. Referring to Equation 5, it suffices to find one ζ0 ∈ Sn-1 such that
2
n
X ζi0 mi
i=1
m
utuX
k=1
n
ζi0akφ0(wkTxi)xi
i=1
= 0.
2
2
For any k ∈ [m] and ζ ∈ Rn consider
n	n	p	`-1
ζiakφ0 (wkT xi)xi =	ζiak	C'-1 (WT Xi)	Xi
i=1	i=1	'=1
which can be written as
pn
ak X c'-i	X	X Zixβ Xi	We
l = 1	β∈Z+,kβkι='-1 ∖si=1_J
^^^^^^{^^^^^}
t
(7)
Here Z+ denotes the set of non-negative integers, and the notation Xβ is shorthand for Qjd=1 xjβj .
Note that the term denoted by f is a d-dimensional vector, since Xi ∈ Rd. The actual number of
unique equations in f depends on d0, since the x/s can have only UPto d0 order unique moments.
Hence, if we want to make the above zero, it suffices to make the term denoted by f zero for each
of the summands. This is a system of linear equations in variables {ζi}in=1. Counting the number
of constraints for each summand and summing over all the indices gives us that the number of
constraints is given by
p
X
'=1
d0 + ` -
`
1
which is equal to (d jp) - 1. Note that this can also be seen by counting the number of non-trivial
monomials of degree at most p in d0 variables. Hence, making the number of constraints less then
number of variables, leads to existence of at least one non-zero vector ζ00 satisfying the set of
constraints. Since, the set of linear equations is independent of the choice of k, the claim holds true
for all k. Thus, using a unit normalized Z00 in Equation 5, We get σmin (M) =0.	□
Corollary F.2.1. If d0 = dim (span {xi... Xn}) ≤ O (log0.75 n), φ0(x) = PP=0c'x' and P ≤
O(nd0) then the minimum eigenvalue ofthe G-matrix satisfies
λminG(0) =0.
Proof. If d0 = O log0.75 n , Condition 6 can be simplified to get
p = O(n 方)
Applying Theorem F.2 with the above condition, we get the desired solution.
□
By slightly modifying the proof of the above theorem, we can actually show not only that the matrix
is singular, but also that the kernel must be high dimensional.
Theorem F.3. If d0 =dim (span {xi ... Xn}) ≤ O (log0.75 n^, φ0(x) = PP=0 c'x' and P ≤
O(n d0), then(1 一 d) n low-order eigenvalues ofthe G-matrix satisfy
λk	G(0) = 0,
∀k ≥
n
d0
23
Published as a conference paper at ICLR 2020
Proof. We follow the proof of Theorem F.2. Referring to Fact C.7, it suffices to find one n (1 - d10)
dimensional subspace U such that
2
n
ζimi
i=1
m
utuX
k=1
n
ζiakφ0(wkTxi)xi
i=1
= 0.
2
2
holds true ∀ζ ∈ U.
For a weight row vector wk ∈ {w1, ... , wm}, its corresponding output weight ak and an arbitrary
ζ ∈ Rn, we can simplify the following quantity
(∖ '-1
d
Σ wk,j xi,j	xi
j=1
to get the same set of constraints on variable ζ, as in Equation 7. Making the number of constraints
less than「pleads to the existence of the desired subspace U, whose dimension is n(1 - d),
satisfying the constraints. This can be restated as
d0+P) -1 ≤ n
PJ ~ d0
which can be further simplified using the fact that d0 ≤ O log0.75 n to get
In the above inequality, We use the fact that 1 ≤ d01/d ≤ √2. Since, the set of linear equation is
independent of the choice of w, the claim holds true for all w ∈ {w1, ..., wm}, from which the result
follows.	口
Now, if a function is well approximated by a low degree polynomial then we can use the idea from
the above theorem to kill all but the small error term leaving us with a small eigenvalue. But, for
different regimes of the parameters, we need to consider different polynomial approximations.
F.2 L∞ APPROXIMATION USING THE CHEBYSHEV POLYNOMIALS
Let f : [-k, k] → R be a function for some k > 0. We would like to approximate f with polynomials
in the L∞ norm. That is, we would like a polynomial gp of degree P such that
sup f (x) - gp(x) ≤ .
x∈[-k,k]
First, we reduce the above problem to that of approximating functions on [-1, 1]. The idea is to
consider the scaled function fk(x) = f (kx). Note that fk : [-1, 1] → R. Let h be a polynomial
approximating fk i.e. supx fk(x) - h(x) ≤ . Then, consider the function gp(x) = h x/k . Then,
∣f (x) - gp(x)∣ = ∣fk (x/k) - h(x∕k)∣ ≤ e. Thus, we can consider approximation of functions on
[-1, 1].
We are interested in approximating the derivative of tanh on (-τ, τ). Denote by σ the sigmoid
function given by
σ(X) = £-x.
It follows from the definition that tanh (x) = 2σ (2x) - 1.
The approach is to consider a series in the Chebyshev polynomials Tn . That is, we consider
N
an Tn
i=0
24
Published as a conference paper at ICLR 2020
The coefficients an corresponding to σ can be computed using the orthogonality relations for the
Chebyshev polynomials.
an =2/1 σ(X)Tn(X) dx.
∏ J-1 √1-xI 2
Using this, one can show the following theorem about the polynomial approximations of the sigmoid
function.
Theorem F.4 (Equation B.7 in Shalev-Shwartz et al. (2011)). For each k ≥ 0 and ∈ (0, 1], there is
a polynomial gp of degree p with
_ log (4π∏+2k)
P log (1 + πk-1)
such that
sup gp(X) -σ(kX) ≤ .
x∈[-1,1]
The proof of the claim follows by bounding an using contour integration. From the above discussion,
in order to approximate tanh in the interval (-τ, τ), we need to approximate 2σ (2τ X) -1. From
Theorem F.4, we require a polynomial of degree
一 log (4π+2τ) 一
C 一	O ∖ π2 e )	∕Q∖
log (1 + πτ-1).
Recall that we actually need to approximate the derivative of tanh. But this can be achieved easily
from the fact that tanh0(X) = 1 + tanh(X) 1 - tanh(X) and the following lemma.
Lemma F.5. Let I be an interval and let fi, gi : I → R for i ∈ {1, 2} be functions such that
supx∈I fi(X) -gi(X) ≤ for all i and fi (X) ≤ 1 for X ∈ I and all i. Then,
supf1(X)f2(X) -g1 (X)g2(X) ≤ 3.
x∈I
Proof. For any X ∈ I we have
f1 (X)f2 (X) - g1(X)g2 (X) ≤ f1 (X)f2 (X) - f1(X)g2 (X) + f1 (X)g2 (X) - g1(X)g2 (X)
≤ f1 (X)f2 (X) - f1(X)g2 (X) +f1(X)g2(X) - g1(X)g2 (X)
≤ f1 (X)f2 (X) - g2 (X) +g2(X)f1 (X) -g1 (X)
≤ + (1 + )
≤ 2 + 2
≤ 3e.	□
F.3 L2 APPROXIMATION USING HERMITE POLYNOMIALS
Next we consider approximating f in the 2-norm i.e. we would like to find a polynomial hp of degree
p such that
I If(X)- hp(x)∣2 dμ(x)
-∞
is minimized. μ denotes the Gaussian measure on the real line. Note that this problem can be solved
using the technique of orthogonal polynomials since L2 (R, μ) is a Hilbert space. The study of
orthogonal polynomial is a rich and well-developed area in mathematics (see Szego (1975); Lebedev
(1972)). Our main focus in this section will be the case where f is the derivative of tanh and in later
sections we extend this analysis to other activation functions.
25
Published as a conference paper at ICLR 2020
Let Hk denote the k-th normalized (physicists’) Hermite polynomial given by
Hk(X)=[62”12(-1)® ex2 与e-x2
(9)
and the corresponding normalized (probabilists’) Hermite polynomial is given by
Hek(X) = [√∏k!]-1/2 (-1)k ex2/2e-x2/2.	(10)
The Hermite polynomials are usually written without the normalization. The normalization terms
ensure that the polynomials form orthonormal systems with respect to their measures. Recall that
μ (x; σ) denotes the density function of a Gaussian variable with mean 0 and standard deviation σ.
The series of polynomials Hk are orthonormal with respect to Gaussian measure μ (x;	and the
series of polynomials Hek are orthonormal with respect to the standard Gaussian measure μ (x; 1)
i.e.
I	Hm(X)Hn(X) dμ
-∞
δm,n ,
Z∞
Hem(X)Hen(X) dμ(χ; I) = δm,n.
∞
(11)
(12)
The two versions of the Hermite polynomials are related by
Hm(X)= Hem (√2x) .	(13)
For any function f ∈ L2 (μ), we can define the Hermite expansion of the function as
∞
f = X fiHei.
i=0
From the orthogonality of the Hermite polynomials, we can compute the coefficients as
fi = Jf(X) Hei (x) dμ (x; 1).
Since L2 is a Hilbert space, we can use the Pythagoras theorem to bound the error of the projection
onto the space of degree k polynomials as
∞
X |fi|2.
i=k+1
This leads us to consider the Hermite coefficients of the functions we would like to study.
We defined the Hermite expansion in terms of probabilists’ Hermite polynomials. For physicists’
version we can define an expansion similarly. In this paper, we will use probabilists’ version in our
proofs. Since the literature we draw on comes from both conventions, we will need to talk about
physicists’ version also.
In the following we will be using complex numbers. For z ∈ C, the imaginary part of z is denoted by
=(z).
The following theorem provides conditions under which the Hermite coefficients decay rapidly. It
says that if a function extends analytically to a strip around the real axis and the function decays
sufficiently rapidly as the real part goes to infinity, the Hermite series converges uniformly over
compact sets in the strip and consequently has rapidly decaying Hermite coefficients. The extension
to the complex plane provide the function with strong regularity conditions.
Theorem F.6. (Theorem 1 in Hille (1940)) Let f(z) be an analytic function. A necessary and
sufficient condition in order that the Fourier-Hermite series
∞	z2	∞	t2
fckHk(z)e-ɪ,	Ck = f (t)Hk (t)e-ɪ dt
k=0	-∞
(14)
26
Published as a conference paper at ICLR 2020
shall exist and converge to the sum f(z) in the strip Sτ = z ∈ C : = (z) < τ , is that f(z) is
holomorphic in Sτ and that to every β, 0 ≤ β < τ, there exits a finite positive B(β) such that
If(X + iy)∣ ≤ B(β)eTχ02-y2y/2
where x ∈ (-∞, ∞), y ∈ (-β, β). Moreover, whenever the condition is satisfied, we have
∣ck | ≤ M(e)e-(τ-e)√2k+1
for all positive . Here, M denotes a function that depends only on .
(15)
The function tanh can be naturally extended to the complex plane using its definition in terms of
the exponential function. From this definition, it follows that tanh has a simple pole at every point
such that e2z + 1 = 0. The set of solutions to this are given by 2z = (2n + 1) π. Thus, tanh is
holomorphic in any region not containing these singularities. In particular, tanh is holomorphic in
the strip S∏∕2 = {z ∈ C : ∣=(z)∣ < π∕2}
The same holds for tanh0.
Using the above theorem, we bound the size of the Hermite coefficients of the derivative of tanh and
thus bound the error of approximation by low degree polynomials.
Theorem F.7.
{Hk}k∞=0, as
Let φι(z) = tanh0 (z∕√2) e--Z. Consider the Hermite expansion of φι in terms of
∞	z2
Φι(z) = £ck Hk (z)e-ɪ.
k=0
(16)
Then,
|ck| ≤ O
Proof. As before consider the strip Sτ = z ∈ C : ∣∣=(z)∣∣ < τ . Note that φ1(z) is holomorphic in
Sτ for T < √2π∕2. For every β ∈ [0, √2π∕4], consider Z = X + iy ∈ Se and set √2x0 = X and
√2y0 = y. Also note that tanh0(z) = 1/cosh2(z). Then
1
cosh2 (X0 + iy0)
1	∣∣
cosh (X0 + iy0) ∣
1
e-(x0+iy0)2
∣∣e-x2+y2-2ixy
cos y0 cosh X0 + i sin y0 sinh X
1
∣∣∣ ∣∣e-x2+y2 -2ixy
cos2 y0 cosh2 X0 + sin2 y0 sinh2 X0
e-x2 ey2 ∣∣e2ixy∣∣
∣∣e-x2+y2 -2ixy
≤	o^^∖	Γ2~二
cos2 y0 cosh2 X0
e-x2 eβ2
≤ 7	Γ2 ~二
(cos2 β) cosh2 X0
≤ e-x2 eβ2 (sec2 β) sech2X0
≤ 4e-x2 eβ2 (sec2 β) ex0 + e-x0
≤ 4e-x2eβ2 (sec2 β) e-√2lxl
≤ 40 eβ2 (sec2 β) e-√β2-y2∖χ∖.
27
Published as a conference paper at ICLR 2020
The last inequality follows by noting that，/2 - y2 ≤ β ≤ √2π∕4 ≤ √2. This satisfies the required
condition with B (β) =40 eβ2 (sec2 β). Hence, from Theorem F.6, we have that Equation 16 is
convergent in the strip Sτ , for T ≤字. Thus, from Equation 15, using e = 2πw we have
∣Ck∣ ≤ Ce-4√2√2k+1
for some constant C independent of k.	□
Corollary F.7.1. Let φ2 (x) = tanh0(x). Consider the Hermite expansion of φ2:
∞
Φ2 (x) = X Ck Hek (x).	(17)
k=0
Then,
∣Ck| ≤O (e-π√k).
Proof. Using orthonormality of Hek with respect to the standard Gaussian measure, we have
Ck =	Φ2(x) Hek(x) dμ(x; 1)
-∞
/	/	、Tr /	、-x2	7
φ2 (x) Hek(x)e 2 dx
∞
φ2 (√2x) Hek (√2x) e-x dx
Using φ2 (√2x) = φι (x) ex2, defined in Theorem F.7 and Hek (√2x) = Hk (x), as given by
Equation 13, we have
cCk
Applying Theorem F.7, we get the required bound.
-^= I	φι (x) Hk (x) e--X dx
π -∞
Corollary F.7.2. Let φ2 (x) = tanh0 (x) and let φ2 be approximated by Hermite polynomials
{H ek}k∞=0 of degree up to p in Equation 17, denoted by
p
hp (x) :=	cCk Hek (x).
k=1
Let
Then,
Ep(x) := φ2(x) - hp(x).
J Ep(X)2 dμ(x; 1) ≤ O (√pe-4√2√p)
Proof.
∞
Ep(x) = φ2 (x) - hp(x) =	cCkHek (x).
k=p+1
Using orthonormality of normalized Hermite polynomials with respect standard Gaussian measure,
we have
Z∞	∞∞
Ep(X)2 dμ(x; 1) =I	CkHek(X)Hek(X) dx
∞	k=p+1 -∞
cC2k.
k=p+1
Substituting the bounds for {cCk}k∞=p+1 from Corollary F.7.1, we have
∞	∞
X Ck ≤ X e-4√2 …
k=p+1	k=p+1
≤ L∞ e-4√2 …dx
32
π2
√2P+I + 1 ) e-4√ √2p+1
□
∞
as required.
□
28
Published as a conference paper at ICLR 2020
For comparison, consider the Hermite expansion of the derivative of ReLU, the threshold function. It
can be shown (see (Lebedev, 1972, page 75)) that
1 X (-1)k p√∏22k+1(2k + 1)!	1
ReLU(X) = 2√π A -22k (2k +1) k!— H2k+1(x) + 2
1 X (-1)k P(2k + 1)!	1
=√4π⅛	2®(2k + 1)k H2k+1(χ)+ 2
白(-1)k √22kk2k√2ke-2k ττ	,、	1
≈	H^2k+1 (X) +
总 2k p(2k + 1)kk√ke-k	+	2
∞	( 1)k	1
≈ Xp=-==√:H2k+l(x) +
k=0	(2k + 1) k	2
∞ (-1)k	1
≈	工0寿H2k+1(x) + 2.
(18)
k=0
We can also expand the threshold function, in terms of probabilist’s Hermite polynomials in the
following manner. If the expansion of threshold function is written as Pk∞=0 cCkHek (x), then
Ck
√= / ReLU0(x)Hek (x)e-* dx
√π ∞ ReLU0(√2y)Hek (√2y)e-y2dy
ɪ ∞ ReLU0(y)Hk(y)e-y2dy
π -∞
(-1)k
=Ck ≈ M^7Γ
where We use Equation 13 and the fact that ReLU0(y) = ReLU0(√2y) in Equation 19.
It can now be seen that the Hermite coefficients do not decay rapidly for this function.
(19)
(20)
F.3.1 DZPS SETTING
For this choice of initialization, defined in section 2, we need to consider two different regimes
depending on the number of neurons m. When m is small, we use Chebyshev approximation in
the L∞ norm, while we use the L2 approximation by Hermite polynomials when m is large. First
consider the Chebyshev approximation.
Theorem F.8. Assuming φ(x) = tanh(x) and weights w®0)〜N(0, Id), a®0)〜N(0,1) ∀k ∈ [m],
the minimum eigenvalue of the G-matrix is
λmin (G(O)) ≤ O n
I
/	_____∖2∖
4π + 6√lognm ∣
(1+	∏/3 )p J
√log nm
with probability at least 1 一 他；)3.5 with respect to
largest integer that satisfies Condition 6.
nwk(0)om and na(k0)om ,wherepisthe
k=1	k=1
Proof. In the following, for typographical reasons we will write wk instead of wk(0) and ak instead
of a(k0) . Referring to Equation 5, it suffices to find a vector ζg ∈ Sn-1 s.t. Pin=1 ζig mi is small.
For each k ∈ [m] and i ∈ [n], wkT xi is a Gaussian random variable following N(0, 1). Thus,
there are mn Gaussian random variables and with probability at least (1 一(一；产5)with re-
spect to {wk}m=ι, maxi∈[n],k∈[m] ∖wTXil ≤ 3√log nm. Hence, we restrict ourselves to the
29
Published as a conference paper at ICLR 2020
range (-3√lognm, 3√log nm), when We analyze φ(x). Now, from Equation 8 and Lemma F.5,
we have that there exists a polynomial g (x) of degree p that can approximate φ0 in the interval
(一3√log nm, 3√log nm) with the error of approximation in L∞ norm e given by
≤3
/
∖
4π + 6√lognm
ι+√⅛ y
2
From Theorem F.2, there exists ζg ∈ Sn-1 s.t.
m
X
k=1
n
X ζigak g wkT xi xi
i=1
=0
2
provided Condition 6 holds true. For any weight vector wk, it’s corresponding output weight ak and
any ζ ∈ Sn-1, we have
nn
ζi akφ0(wkTxi)xi -	ζi akg(wkT xi)xi
i=1	i=1
≤t
n
ak X ZHwWTxi)- g(wk Xi))2、
i=1
n
Xkxik2
i=1
2
(21)
≤ √ne∣ak∣
(22)
where we use triangle inequality and the Cauchy-Schwartz inequality in Inequality 21, kζk = 1,
kxi k = 1 and that the maximum error of approximation is e in Inequality 22. Hence, for ζ = ζg , we
have
n
ζigakφ0(wkTxi) xi
i=1
n	n
≤	ζigakφ0(wkTxi) xi -	ζigakg(wkT xi) xi	+
i=1	i=1	2
≤ √n∣ak| e
n
ζigakg(wkTxi) xi
i=1
(23)
Thus,
n
X ζig mi
i=1
um
utuX
k=1
n
X Zgakφ0 (WTXi) Xi
i=1
2
≤
≤ 5m√nhe
where in the final step, we use chi-square concentration bounds (Fact C.6) to show that vz∑m=ι ak
is at-most √5m with probability at-least 1 - e-m. Using Equation 5, σmin (M) ≤ √5m√ne. That
implies, λmin(G)=2λmin (M)2 ≤ 5ne2. Since, e decreases with increasing p, we substitute the
value of e at maximum value of P possible in order to get the desired result.	□
Note that since the upper bound on the eigenvalue depends on m, the bound becomes increasingly
worse as we increase m. This is because as we increase m, the interval (-3√log nm, 3√log nm) in
which we need the polynomial approximation to hold increases in length and thus the degree needed
to approximate grows with m. To remedy this, we relax the approximation guarantee required from
the L∞ norm to the L2 norm under the Gaussian measure. This naturally leads to approximation by
Hermite polynomials as discussed in subsection F.3.
Theorem F.9. Assuming φ(x) = tanh(x) and weights WkO)〜N(0, Id) , a"〜N(0,1) ∀k ∈ [m],
with probability at least 1 — e-口 (n2log m)— m-3.5 over the choice of {wk°)} and {ak°)}
the minimum eigenvalue of the G-matrix is bounded by c, i.e.
k=1
c = max
λmin(G(0)) ≤ c, where
O (nlognlogm) ο
and p is the largest integer that satisfies Condition 6.
m
2
30
Published as a conference paper at ICLR 2020
Proof. In the following, for typographical reasons we will write wk instead of wk(0) . Referring to
Equation 5, it suffices to find a vector ζh ∈ Sn-1 s.t. Pin=1 ζihmi is small.
Theorem G.2 gives the error of approximating φ0 by a polynomial h, consisting of Hermite polyno-
mials of degree ≤ p, in the L2 norm. Let Ep denote the error function of approximation, given by
Ep(x) = φ0 (x) - h(x).
From Theorem F.2, there exists Zh ∈ Sn-I s.t. for all W and a We have
n
X Zhah(WTXi) Xi= 0,
i=1
provided p satisfies Condition 6.
We can use Fact C.5 to confine the maximum magnitude of ak to 3√log m. Thus, assuming that this
condition holds true, We claim the folloWing. Using Zh, We get
Ew~N (0,id),a~N (0,1)
n
X Zihaaφ0(WTXi) Xi
i=1
2
n
n
2
EZhahp(WTXi) Xi + EaZIhEP (wtXi) Xi
(24)
i=1
i=1
Ew~N (0,id),a~N (0,1)
(25)
(26)
(27)
(28)
We approximate φ0 by h and use the definition of Zh in Equation 24, apply Cauchy-SchWartz in
Equation 25, kXik = 1 ∀i ∈ [n], Zh = 1, the linearity of expectation in Equation 26 and maximum
variance of aa, given the constraint on it’s magnitude, as upperbounded by 1 in Equation 27. WTX1
folloWs a Gaussian distribution N (0, 1). Hence, denoting h as the error of approximation from
Corollary F.7.2, We get
Ew~N (o,id),a~N (0,1)
2
n
X	Zihaaφ0(WTXi) Xi ≤ n2h
i=1
Where
eh ≤ O(√pe-4√2√p).
Applying Hoeffding,s inequality for m weight vectors Wk ~ N(0, Id) and ak ~ N(0,1), we get
2
Pr
{wk}km=1,{ak}km=1
n
Zihakφ0(WkTXi)Xi
i=1
≤
(n2eh + t)} ≥ 1 — e- 9n2 log2 m — m-3.5.
In the above inequality, we use the restriction of the maximum magnitude of ak to 3√lng m and
hence, ∀k,∣∣pn=ι Zhakφ0 (WT Xi) Xi ∣∣ ∈ (0, 3√n√log m). Using t = max (n2 eh, n logʌn^g m ), C
31
Published as a conference paper at ICLR 2020
being a constant and substituting the value of , we get the final upper bound. Thus,
m
X
k=1
n
X ζihφ0(wkTxi)xi
i=1
Using Equation 5 and the fact λmin(G) = mlσmin(M)2, We get the final bound.
(29)
□
The upper bound on minimum eigenvalue from Theorem F.8 can be reWritten as
λmin(G⑼)≤ O fnlog(nm)e-plog (1+ 3√log(nm))
for a small constant C and p denotes the largest integer that satisfies Condition 6. Let us assume
that d0 ≤ O (log"75 n), where d0 = dim (SPan {x1... Xn}). Then, We use the value of P from
Corollary F.2.1 for the next set of arguments. Substituting the value ofp, We get
λmin(G⑼)≤ O (nlog(nm)e-n1"log (1+3√log(nm))
O(n1/d0
Assuming m < e , we have
λmin(G⑼)≤ O (n2eT(id0)) = e—Mn1/").
Ω(n'∕2d
By Theorem F.9, when m > e
λmin(G(0)) ≤ max n1'5 log(n)e—Q(n, ) , O (n2e-4√2n /
Lw/2
with high probability with respect to {wk}km=1and {ak}km=1. Hence, the final bounds of minimum
singular value of Gram matrix in case of tanh activation has been summarized below.
Theorem F.10. Let d0 = dim (span {xι... xn}) ≤ O (log0*5 n). Assuming φ(x) = tanh(x) and
weights WkO)〜N(0, Id) ,akO)〜N(0,1) ∀k ∈ [m], the minimum eigenvalue of the G-matrix
satisfies
λmin (G⑼)≤ eT(n1/2dO)
with probability at least 1 -
n15 with respect to the weight vectors { wk°)}
km=1 and na(kO)okm=1
In fact, as in Theorem F.3, we can show that the smallest(1 - d7) n eigenvalues of the matrix are
small. This is captured in the following theorem.
Corollary F.10.1. Let d = dim (span {xι ... xn}) = O (log0.75 n). Assuming φ(x) = tanh(x)
and weights WkO)〜N(0, Id) , akO)〜N(0,1) ∀k ∈ [m], then eigenvalues of the G-matrix satisfy
λk (G(O)) ≤ e-*/")	Vk ≥
n
d0
with probability at least 1 -
1
n25
with respect to the weight vectors
nwk(O)okm=1andna(kO)okm=1.
Proof. In the following, for typographical reasons we will write wk instead of wk(O) and ak instead of
a(kO) . We give a proof outline. We will approximate φ0 by a p-degree polynomial h as in Theorem F.8
32
Published as a conference paper at ICLR 2020
and Theorem F.9, where p ≤ O n1/d0 . From Theorem F.3, we get that for polynomial h, there
exits a n(1 - d) dimensional subspace U for which the following quantity
2
m n
∖
E EZiakh (WTXi) Xi
k=1 i=1
0
is 0, ∀Z ∈ U. Now, we can take an orthonormal basis Z(U) = Z(I),Z⑵，…,Znn1-~d0)) of U and
for each ζ(j), we can follow the same proof structure in Theorem F.8, Theorem F.9 and Theorem F.10
to get
m n
m X X Zyjakφ (WT Xi) χi
k=1 i=1
2
≤ e"d0)
with probability at-least 1 - n15 with respect to {wk}k=1 and {ak}km=1. Now, for bounding singular
value σk(M), for k ≥「耨,we use the following argument. We choose a subset S(n-k) of size
n - k from Z(U). This subset is a n - k dimensional subspace U0 ofRn. Each Z ∈ U0 can be written
in the form
Z = X	αjZ(j)
j∈[n]: ζ(j) ∈U0
with Pj∈[n]: ζ(j)∈U0 αj2 = 1. Then, for each Z ∈ U0,
2
m
n
m
-X
m
k=1
n
Xi
i=1
mn
IXX χ
k=1
m
-X
m
k=1
i=1 j∈[n]: ζ(j) ∈U
∑
j∈[n]: ζ(j) ∈U0
2
i=1
(30)
=f2d
/
Σ
j∈[n]: ζ(j) ∈U
Thus, it follows from the definition ofσk (M) from Fact C.7 that
σk (M) ≤ Vme-W")
Using λk (G(O)) = mmσk (M)2, we get the final upper bound.
□
F.3.2 S tandard Setting
Now, we consider upper bounding the eigenvalue of the G-matrix for the standard initialization,
defined in Appendix D.
Theorem F.11 (Init(fanout) setting). Assuming φ(x) = tanh(x) and weights wk°j 〜
N(0, m Id)
G-matrix is
ak°)〜N(0,1) and bk°j 〜N (0,.)∀k ∈ [m], the minimum eigenvalue of the
(
O n
∖
/
∖
4π + 6
log nm
1+√
π∕3
log nm
m
m
∖
p
/
2
/
33
Published as a conference paper at ICLR 2020
with probability at least 1 一(mn2)3.5 with respect to {wk0)}	, {ak°)}	and {bk°)}	, where
p is the largest integer that satisfies Condition 6.
Proof. For each k ∈ [m] and i ∈ [n], WTXi is a Gaussian random variable following N(0,*).
Thus, there are mn Gaussian random variables and it follows from Fact C.5 that with probability
at least(1 一 武厂,with respect to {wk}m=1, maXi∈[n],k∈[m]∖wTXil
≤ 3 ∕0gnm. Now, We
follow the same proof as F.8 but restricted to the range
result.
(W Iomnm, 3j Iognm) to get the desired
□
Corollary F.11.1 (Init(fanout) setting). If d0 = dim (SPan {xι... Xn}) ≤ O (log0*5 nj. Assum-
ing φ(x) = tanh(x) and weights wk0) 〜N(0, mId) , akO)〜N(0,1) and b(0 〜N(0, m) ∀k ∈
[m], the minimum eigenvalue of the G-matrix satisfies
λmin (G(O)) ≤ e-Q(n1/2d0)
with probability at least 1 — n3 With respect to the weight vectors {wk°)}	, {ak°)}	and
m
b(k0)
k=1
Proof. It follows from the same proof as Theorem F.3.
□
TheoremE12 (Init(fanin) setting). Assuming φ(x) = tanh(x) and weights WkO)〜 N (0,dId),
ak°)~ N (0, m) and b(0 〜N (0,0.01) ∀k ∈ [m], the minimum eigenvalue of the G-matrix is
(
λmin (G(O)) ≤ O n2
∖
/	____∖2∖
4π + 6 J Iognm
，+ ；/ Y
U	√1ogdnm7 /
with probability at least 1 一(mn2)3.5 with respect to {wk°)}	, {ak°)}	and {bk°)}	, where
p is the largest integer that satisfies Condition 6.
Proof. The proof follows from the proofs of Theorem F.8 and Theorem F.9, with the region of
approximation reduced to
CorollaryE12.1 (Init(fanin) setting). If d0 = dim (Span {xι ... Xn}) ≤ O logO.75 n . Assuming
φ(x) = tanh(x) and weights WkO) ~ N (0, dId), a，0) ~ N (0, 2) and b(0 ~ N (0, 0.01) ∀k ∈
[m], the minimum eigenvalue of the G-matrix satisfies
λmin (G(O)) ≤ e-…i
with probability at least 1 — n3 With respect to the weight vectors {wk°)}	, {ak°)}	and
b(kO)
k=1
□
m
34
Published as a conference paper at ICLR 2020
G Upper B ound on Lowest Eigenvalue for Swish
In this section, we show upper bounds on the eigenvalues for the G-matrix for the Swish activation
function using techniques largely similar to the techniques uses for the tanh activation function. This
is not too surprising since they satisfy the following functional identity
swish(x)
tanh
x
2
Hence
swish0 (x) = ɪ
1+2 tanh0(2
+ tanh
Theorem G.1. swish0(t) is approximated by a degree p polynomial gp(t) within error in the interval
[-k, k] in the L∞ norm:
sup swish0(t) - gp(t) ≤
t∈[-k,k]
where
p
log
4πk+2k2λ
∏2 e
log (1 + πk-1)
Similarly, for the L2 approximation for swish, we proceed using the same technique as for tanh.
Theorem G.2. Let φ2 (x) = swish0(x) and let φ2 be approximated by Hermite polynomials
{H ek}k∞=0 of degree up to p in Equation 17, denoted by
p
hp(x) = EekHek (x).
k=1
Let
Then,
Ep(x) = φ2(x) - hp(x).
Using the above theorems and the techniques from the previous sections, we can upper bound the
eigenvalues of the G-matrix with the swish activation f0unction. We summarize this in the following
theorems.
Theorem G.3. Consider the setting of Du et al. (2019a). If d0 = dim Span {x1 . . . xn} ≤
O (log"75 n). Assuming φ(x) = Swish(X) and weights w，0) 〜N(0,Id)∀k ∈ [m], then eigenvalues
of the G-matrix satisfy
λk (G(O)) ≤ e-.id0)	Vk ≥
n
d0
with probability at least 1 -
n15 with respect to the weight vectors { w，0)}
m and na(k0)om
k=1	k=1
H A discussion on Upper B ound of Lowest Eigenvalue for General
Activation Functions
In this section, we generalize the results of the previous sections upper bounding the eigenvalues of
the G-matrix to a more general class of activation functions. To this end we note that the only property
of the tanh and swish we used was that these functions are well-approximated by polynomials of low
degree. The approximation theorems used in the previous sections can be stated under fairly general
conditions on the activation functions.
35
Published as a conference paper at ICLR 2020
For the Chebyshev approximation, it can be shown that a function with k derivatives with bounded
norms can be approximated by Chebyshev polynomials of degree N with error that decays like N-k.
This shows that for smooth functions the error decays faster than any inverse polynomial. Under
the assumption of analyticity, this can be further improved to get exponential decay of error. We
summarize this in the following theorem.
Theorem H.1 (see Section 5.7 in Mason & Handscomb (2002)). Let f : [-1, 1] → R be a function
with k + 1 continuous derivatives. Let SNf be the Chebyshev approximation of f to degree N. Then,
we have
sup	f(x)-(SNf)(x) ≤ O N-k .
x∈[-1,1]
Furthermore, if f can be extended analytically to the ellipse
Er = (Z ∈ C ： Z = (w+2w-1)	∣w∣≤J ,
then
sup f (x) - (SN f) (x) ≤ O r-N .
x∈[-1,1]
Similarly, for Hermite approximation one can state the decay of the Hermite coefficients in terms of
the regularity of the derivatives, expressed in terms of inclusion of the function in certain Sobolev
spaces. Also, Theorem F.6 indicates that extending the function on to the complex plane gives better
convergence properties. See Thangavelu (1993) for further details.
With these general approximation theorems and techniques from the previous sections, we can extend
the upper bound on the eigenvalues on activation functions satisfying sufficient regularity conditions.
36
Published as a conference paper at ICLR 2020
I	LOWER B OUND ON EIGENVALUES FOR tanh WHEN THE DIMENSION OF THE
DATA IS NOT TOO SMALL
For the following proof, we assume the data generation process as follows.
Assumption 3. The data is mildly generic as in smoothed analysis: e.g., xi is obtained by adding
small multiple (σ = O (2√n) ) ofIID Standard Gaussian noise within the subspace V0 Ofarbitrary
initial samples x0i, with V0 := Span{x01, . . . , x0n} and renormalizing to 1. Denoting the initial set
of samples as X0 = x01 , . . . x0n and the noise matrix N, that has each entry coming iid from
N 0, σ2 , we have the data matrix X defined by {x1, . . . xn}, where
_ ni + Xi
Xi	= F+^
Remark. Assuming that the initial samples X0i are one-normalized, w.h.p. the norm of the noisy
vectors X0i + ni are in the range (1 - δ, 1 + δ) and thus, renormalization involves division by a
constant in the range (ι++δ, ι-⅛)∙ Assuming that the initial samples Xi are 2δ separated, w.h.p. the
separation between Xi can be shown to be at least δ, thus satisfying Assumption 2.
Assumption 4. Let d0 = span{X1, . . . , Xn}. For simplicity, we assume d = d0 i.e. X1, ..., Xn lie in
d0-dimensional space (otherwise we project them to d0 dimensional space using SVD) and d0 ≥ 2.
While our result here builds upon the smoothed analysis of Anderson et al. (2014), the following
lemma provides a more modular approach though no new essential technical ingredient.
Lemma I.1 (cf. Lemma H.1 in Oymak & Soltanolkotabi (2019)). For an activation function φ and
a data matrix X ∈ Rd×n with unit Euclidean norm columns, the minimum eigenvalue of the Gram
matrix G∞, satisfies the following inequality
λmin (G∞) ≥ Cr (φ0) λmin ((XTx1-1)) , VT ≥ 0
where (XTX)0(r+1) is given by (X*r )T X*r, X*r ∈ Rn×dr denotes the Khatri-Rao product of
matrix X and cCr (φ0) denotes the r-th coefficient in the probabilists’ Hermite expansion of φ0.
Proof. Each element of G∞ can be expressed in the following manner.
∞	a+1
g∞ = Ew〜N(0,ι),a〜N(0,1)52φ (W Xi) φ (WTXj) xTXj = X Ca (φ0) (xTXj)
a=0
where we use a) unit variance of a and independence of a and W and b) the fact that wTXi and wTXj
are XiT Xj correlated for a normally distributed vector W and hence, use Lemma N.4. Thus,
g∞=X Ca(°，)(XT XLa+1)
a=0
Using Weyl,s inequality (Fact C.11) for the sum of PSD matrices {(XTX) θ(a)}	, we get
G∞ 占 Cr (φ0)(XTX)0(r+1), Vr ≥ 0
from which, the assertion follows.
Lemma I.2. Let d0 = dim span{X1, . . . , Xn}. Denote by p be an integer that satisfies
dp0	≥n.
Then for any κ ∈ (0, 1), with probability at least 1 - κ with respect to the noise matrix N,
□
(31)
σn (X*p) ≥ Ω
37
Published as a conference paper at ICLR 2020
Proof. X*p has d0p rows and n columns. However, the number of distinct rows is given by (d +p-1).
This follows by counting the number of distinct terms in the polynomial Pkd0=1 vk , where {vk}dk0=1
d0
is a set of d0 variables. Since, we assume that dp , which is lesser than this quantity, is greater than
n, the number of distinct rows is greater than the number of columns for the matrix X*p. Let X*p
denotes a n X n sized square block of X*p, that contains any random subset of size n from the set of
distinct rows of X*p as rows, such that each row represents elements of Khatri-Rao product of the
form Qj∈[do] Xjj, 0 ≤ bj ≤ 1 ∀j ∈ [d0], for a vector X ∈ Rd0. We can see that λn(X*p) ≤ σn(X*p).
Hence, we will focus on the minimum eigenvalue λn(X*p).
Fix k ∈ [n] and let U be the vector orthogonal to the subspace spanned by the columns of X*p, except
the kth column. Vector U is well-defined with probability 1. Then the distance between X；p and the
span of the rest of the columns, denoted dist(X；p, X-Pk), is given by
UTXkP= X usgs ({cxkj + Cnkj }∕ = j	1
=: P ({cnkj }d= ι),	1
where gs denotes a degree-p polynomial and is given by
gs	nCx0kj +	Cnkjojd=1	= Y	Cx0kj +	Cnkj	j ,	0 ≤	bjs	≤ 1	∀j	∈	[d0],	X	bjs	= p.
j=1	j∈[d0]	j∈[d0]
(32)
(33)
c denotes a constant in the range (ɪ+^, ι⅛), which is the normalization factor used in Assumption 3.
Hence, Equation 32 is a degree p polynomial in variables nkj . We will apply the anticoncentration
inequality of Carbery-Wright to show that the distance between any column and the span of the rest
of the columns is large with high probability. The variance of the polynomial is given by
Var
P({nkj }d= ι)	≥ X Iusl2 Y E Cx0kj +	Cnkj	j
s∈[n]	j∈[d0]
(ι⅛ r ≥
≥
where we use the fact that kuk = 1 and nkj∙ are Gaussian variables of variance 4δ2. Using a minor
adjustment of Fact C.8, which takes into consideration the fact that our gaussian variables are of
variance δ- and the variance of the polynomial is not 1, we have
1/P
≤ Cpk,
2
C > 0 is a constant.
Using a union bound over the choice of k, we get
Pr {dist(Xkp, X-k) ≤ e, Vk ∈ [n]} ≤ Cpn
USinge=(⅛y,we get
σ- (X *p) = √n m[n]dist(X 冷 X -k) ≥ e∕√n.
with probability at least 1 - κ. We use Fact C.9 in the above inequality.
□
Theorem I.3.	Let φ(x) be a constant degree p polynomial, with leading coefficient 1, and d0
dimspan{xι,..., x-} ≥ Ω(n1/P). Thenfor any K ∈ (0,1) we have
λmin
38
Published as a conference paper at ICLR 2020
with probability at least 1 - κ w.r.t. the noise matrix N and {wk(0)}km=1, provided
p4pn4p+4 log (n∕κ) log2p+3 m∖
m ≥ Ω	.
σ4pκ4p
Proof. For a degree-p polynomial with leading coefficient 1, the (p - 1)-th coefficient in Hermite
expansion of φ0 is given by 1. Also, note that Equation 31 is satisfied by p, given that d0 ≥ Ω (nP)
for a constant p. Thus, using Lemma I.1, Lemma I.2 to find minimum eigenvalue of G∞ and then
applying Hoeffding’s inequality (Fact C.3) to bound the deviation of minimum eigenvalue of G(0)
from G∞, we get the desired bound.	□
Theorem I.4.	Let the activation function φ be tanh and d0 = dimspan{xι,..., Xn} ≥ Ω (log n).
Then for any κ ∈ (0, 1) we have
λmin (Cl。) ≥ Ω (I-1等！
with probability at least 1 - κ w.r.t. the noise matrix N and {wk(0)}km=1, provided
Pp4pn4p+4 log (n/K) log2 m∖
m ≥ ω (	σ4ρκ4ρ	J
where p denotes the smallest odd integer satisfying
dp0	≥n,
and Cp (tanh0) denotes the P-th coefficient in theprobabilists' Hermite expansion of tanh0.
Proof. p is chosen such that Equation 31 is satisfied. We use Lemma I.1, Lemma I.2 to find minimum
eigenvalue of G∞ and then applying Hoeffding’s inequality (Fact C.3) to bound the deviation of
minimum eigenvalue of G(O) from G∞, we get the desired bound.	□
Now, we specify the behavior of the probabilists’ hermite expansion coefficients cp-1 φ0 for
φ = tanh. Let β, the exponent of real axis convergence of tanh0, be the least upper bound on γ for
which
tanh0(x) = O (e-νlxl") , X ∈ R,
for some constant ν > 0 as |x| → ∞. We have β = 1, as tanh0(x)〜e-4|x| for large|x|.
Hence, using Eq. 5.15 in Boyd (1984) for the coefficients cCk in the probabilists’ Hermite expansion
of tanh0 we have
Ck =------2一胃Θ (e~4(2k+1)2 ) , as k →∞.
k (2k + 1)1/4
(34)
We remark that Boyd (1984) uses physicists’ Hermite expansion. Following similar technique as
in Corollary F.7.1 and Theorem F.7, we can get the exact similar form of probabilists’ Hermite
expansion coefficients cCk tanh0 .

Thus for p = O (log n), we have Cp = Ω (e-c
Corollary I.4.1. Let φ(x) be the activation function tanh and d0
Then,
span{x1 ,
.,xn} = Θ (log n).
λmin
(G(O)) ≥ (W)
O(log n)
with probability at least 1 一 1∕poly(n) — e-。(log2 m (n)
{wk(O)}km=1.
w.r.t. the noise matrix N and
39
Published as a conference paper at ICLR 2020
Corollary I.4.2. Let the activation be tanh and d0 = dim span{x1, ..., xn} = Θ (nγ), fora constant
Y ≥ Ω (logθgonn). Thenfor any K ∈ (0,1) we have
λmin (G(O)) ≥ Ω e
「c0√ogn Kσσ∖ 2p
np
n
with probability at least 1 - K w.r.t. the noise matrix N and {wk(0) }km=1, provided
m ≥ Ω
p4pn4p+4 log (n/K) log2 m
σ4p K4p
where p is the smallest ”odd” integer satisfying
dp0	≥n,
and c0 is a constant. p can be shown to lie in the range,
1 ≤ P ≤ —
Y	Y - ιogn
40
Published as a conference paper at ICLR 2020
J DEPTH HELPS FOR tanh
Let the neural network under consideration be
Fk a，{w(ι)}1)=√m X akφ ((WkLy X(LT
where x(l) ∈ Rm ∀l ≥ 1 and x(l) ∈ Rd for l = 0, is defined recursively by its components as
follows.
Xkl) = -√φ= φ ((Wkl)) X(IT))	Vk ∈ [m],∀l ≥ 1
x(k0) = xk ∀k ∈ [d].
cφ = (Ez〜N(0,i)φ(z)2) 2, with φ following the following three properties.
•	φ(0) = 0.
•	φ is α-Lipschitz.
•	Ez〜N(0,1) φ(Z) = 0
The weight matrices and the output weight vector are given by
a ∈ Rm, W(l) ∈ Rm×m for l ≥ 2 and W(l) ∈ Rm×d for l =
and a respectively, where
Now, we define the Gram matrix G(0) as follows (cf. Eq. 13 in Du et al. (2019b)).
婿=J X αkΦ0 ((wkL))TX(LT) Φ0 ((WkLy XjLT))
k∈[m]
with its counterpart G∞, when m → ∞, given by
g∞ = Ew〜N(0,i),a〜N(0,i)ɑ2φo (WTXiLT)) φ0 (WTXjLT))
Lemma J.1. For a small constant e > 0, if m ≥ Ω (max ( 2 Iogm log nL, (nL)2/7) ) ,then with
probability at least 1 - e-Q(m' ∕2 log m) - -nL we have
Xi	∈ (1 - , 1 + )	∀i ∈ [n], l ∈ {0, . . . , L - 1} .
(35)
Proof. We will use induction on l to show that for any given i with appropraite probability we have
Xi(l)	∈ 1 - (4c2φα2) -Le, 1 + (4c2φα2) -Le
∀l ∈ [L].
We will apply union bound over the choice ofi to derive the result for all i ∈ [n]. The result holds true
for l = 0 by Assumption 1. Let’s assume that the result holds true for l = t. For a randomly picked
vector W 〜N (0, I), wtXit) follows a normal distribution with mean 0 and standard deviation
Xi(t)	∈ (1 - et, 1 + et), where et = 4c2φα2 e. Denote unit normalized form of Xi(t) as
X(t). Since, there are m random Gaussian vectors in the matrix W(t+1), leading to formation of m
Gaussians along the dimension of W(t+1)TXi(t), we can apply Fact C.5 to confine each dimension of
W(t+I)T Xit) to the range (-3√log m ∣∣X(t)∣∣, 3√log m ∣∣X(t)∣∣^ with probability at least 1 - m‰.
Assuming that this holds true, we can claim the following: First,

1
Pr
W(t+1)
m
∖
m
X cΦφ (Wkt+I)TXit))	- Ew〜N(0,i)cΦφ (WTXit))
k=1
≥ α2cφet I ≤ 2e-o(lm⅛
(36)
41
Published as a conference paper at ICLR 2020
where we use Hoeffding’s inequality (Fact C.3) and bound on wk(t)T xi(t) and the α-Lipschitzness of
the activation φ to put a bound on φ(wk(t+1)Txi(t)) to be used in Hoeffding’s inequality.
Second, using Taylor expansion of φ, the deviation of EW〜N(o,i)cφφ(wτXit))2 from
Ez〜N(0,1) cφφ(z)2 can be bounded in the following manner.
EW 〜N (0,I) cφ φ(wτ Xft )2 = EW 〜N (0,I) cφ φ (WT X(t)) + e0 = Ez 〜N(o,i)cφ φ(z)2 + e' (37)
where 0 ∈ -2c2φα2t, 2c2φα2t . This follows from the following set of equations.
φ(wTXit))2 = φ (WTx(t))
+ 2 Z φ ((1 — S)WTX(t) + SWTXit)) φ' ((1 — S)WTXit) + SWTXit)) (WTXit) — WTx(t)) ds
≤ Φ (wtXit)) + 2et(1 + et)α2 (wtXit)) .	(38)
In the inequality above we used the facts that φ is α-Lipschitz, and since φ(0) = 0 by assumption,
φ(z) ≤ α∣z∣.
This gives
EW〜Nio；() cφ φ(WTXit))2 ≤ EW〜Nio；() cφ φ (wTXit)) + 2et(1 + et)α2cφ EW〜Nio；!) (wTXit))
≤ EW〜N(o,i) cφφ (wtXit)) + 2et(1 + et)α2cφ.
Third, we use the definition of cφ and the α-Lipschitzness of φ, to bound the error due to restricting
the maximum magnitude of wt Xit) to 3√log m l∣Xit)ll, to get
EW〜N(0,i)：|WTxit)∣≤3√ιogmcΦφ (wTXit))2 = EW〜N(0,i)cΦφ (WTXit))2 + O (romm)
=ι+O Irm)
This can be shown by the following equation.
EW〜N(0,I):|wtx(t)∣≤3√lθgmcφ φ (WTx()) - EW〜N(0,I)Cφ φ (WTXi))
Combining Equation 36, Equation 37 and Equation 39, we get that with probability at least 1 —
2e-θ( log⅛ ) — m-7/2,
∕1 m	∖ 1/2
卜(t+1) ∣∣ = (m X cφ φ(Wkt)T Xi)2)	∈ (1 - 4α%φ et, 1+4α2cφet).
We use the union bound for all the induction steps and examples to get the final desired bounds. □
42
Published as a conference paper at ICLR 2020
Lemma J.2. Iffor a pair i,j ∈ [n], XyT)TXjlT) = P s.t. ∣ρ∣ ≤ 1 一 δ and if Equation 35 holds,
then for all small > 0,
Wl)TXjll ≤ 3(1 + 2e) eα2cφ+(1+ 2e)max [+c^ (/) + 1--c (∖)，1+Γ|p| + 1--c |p|2
nn
with probability at least 1 一 m-7/2 一 e-Wm*4"n4 log2m) w^r.t. initialization, where C denotes the
rαtio P∞1%φ).
Proof. From Equation 35, for a small constant , Xi(l-1) ∈ (1 一 , 1 + ) , ∀i ∈ [n] with high
probability. For a vector X set X := x∕∣∣x∣∣; thus We will use XyT) for x(l-1)∕∣∣x(l-1) ∣∣. We
use Fact C.5 to restrict the maximum magnitude of the 2m Gaussians wkT Xi(l-1) and wkT X(jl-1) for
k ∈ [m] to 3√logm∣∣x(l-1)∣∣ and 3√logm∣∣Xjl-I)∣∣ respectively. Assuming that this condition
holds, we have
χ(l)Tχ(l)—浮 1 Xeb (W(l)Tχ(l-1)ʌ 小(W(l)Tχ(l-1)A
Xi	Xj = @ m 匚 φ (Wk Xi φ Wk Xj
k∈[m]
=cφ m X φ (Wkl)T xij)) φ (Wkl)T XjlT)) + e0	(41)
k∈[m]
=cφEw~N(0,i)φ (Wkl)Tx(l-1)) φ (Wkl)Txjl-1)) + e0 + J	(42)
∞
=cφ X Ca (φ) Pa + e0 + e00 + e000 ∙	(43)
a=0
We get Equation 41 along the lines of Equation 38: we use 1-st order Taylor expansion of
φ (WTx(l-1)) around WTxil-1) and φ (WTXjl-I)) around WTxjl-1), α-Lipschitzness of φ and
upper and lower bounds of∣∣Xi(l-1) ∣∣ and ∣∣X(jl-1) ∣∣ from Lemma J.1 to get
φ(wTx(l-1))φ(WTXjl-I)) = φ (WTxil-1)) φ(wTxjl-1))
1
+ / ɔ φ(wTxjlτ))φ0 ((1 — S)WTx(l-1) + SWTx(l-1)) (WTx(l-1) 一 WTx(l-1)) ds
1
+	/_(/(WTxil-1))φ0 ((I- t)WTXjlT) + tWTXjlT))(WTx(lT)-WTXjlT)) dt
+	ZS J ° φ0 ((I 一 S)WTx(l-1) + SWTXilT)) (WTx(l-1) 一 WTx(l-1))
φ ((1 一 t)wTx(l-1) + tWTXjlT)) (WTXjl-I) 一 WTxjl-1)) dsdt
≤ φ (WTMlT)) φ (WTxjl-1)) + α2
1 + e) + e2(l + e)2)∣WTxil-1)，WTxjl-1)∣ . (44)
In the inequality above we used the facts that φ is α-Lipschitz, and since φ(0) = 0 by assumption,
φ(z) ≤ α∣z∣. This gives
mm
m X cφφ(wTx( - ))φ(wT Xj-1)) ≤ m X cφφ(wT x( - ))φ(wT x( -))
k=1	k=1
m
+ α2 (2e(1+e) + e2(1 + e)2) m X cφ∣wT χ(l-1)∣∣wT xjl-1)∣
m k=1
m
≤ m X cΦφ(wTx(l-1))φ(WTx(lT)) + 4α2cφ (2e(1 + e) + e2(l + e)2).
k=1
43
Published as a conference paper at ICLR 2020
In the last step, we use Hoeffding’s inequality to
m1 Pm=ιcΦ∣wTMlT)IWTxMlT)I from Ew~N©I)cΦwTMlT)
dard gaussian moments, We can show that that Ew~n(0,1) WTx(l-1)
combining everything,we get
0 ∈ -20α2 c2φ, 20α2c2φ
bound the deviation of
wtXjlT) and using stan-
wtXjl-I) is at-most 4. Thus,
with probability at least 1 - e-Q(m/ log2 m). In Equation 42, We use HOeffding's inequality (Fact C.3),
α-Lipschitzness of φ and bound for the magnitude of the 2m gaussians WTXy-I) and WTXjl-I) to
get 00 ∈ (-τ, τ) where
j 1-c|p| (1 -|p|),	if|p| ≥ n⅛.
T = j 1-c 今(1 - n⅛) , otherwise.
with probability at least 1 - e-(o(mδ /α n log m)).
Note that, the minimum value of τ is
1-c nδ2(1 - n2). The reason of using this form of T will be discussed below. We use Lemma N.4 in
Equation 43, with ρoo =卜(IT)I ,ριι =WT)I and ρoι = P卜(IT)IwXjlT)『This follows
from the fact that for a random normal vector W 〜N(0, I), wtX follows a normal distribution with
mean 0 and variance kXk2 and the covariance of WTX and WTy is XTy for two vectors X and y. We
have an additional error term e"0 ∈ --'⅛m,，⅛m ) owing to the condition that ∣wtX(I-I) ∣
must be at most 3√log m (proof will follow exactly along the lines of Equation 40).
Let us now focus on the quantity R (P) for the activation function cφφ(.), defined in Fact N.2. From
the definition of cφ, it follows that cφ = P∞ %⑷.Thus, we have ∣R(ρ) ∣ ≤ R(∣ρ∣) ≤ R(1) = 1,
which comes from the properties of R, as giva=en ian Fact N.2. Again, we have
∣R(P)∣ ≤ R(|P|) ≤
∞
cφ X Ca ⑷ |p|2+cφc2 (φ) (|p| - |p|2)
a=1
11 +	- (φ)
|P| + pa=1 碌(φ)
1 -|P|	|P|
Thus,
∣ (l)T (l) ∣	1 + c δ 1 - c δ 1 + c 1 - c 2
忖	Xj	∣≤∣e	∣ + ∣e∣ + ∣e ∣ +∣R(P)I≤6eαcφ + max1%—(荔)+	—2―(康),—2-|p| +	—2-|p|
(45)
where C denotes the ratio P∞(φ2(φ∙. In the equation above, we can see that the form of T = max∣e0∣
has been chosen so that R(∣ρ∣) +∣e0∣ ≤ 1 (R(∣ρ∣) +∣ρ∣) Thus,
Wl)TXjl)I ≤ 6(1 + 2e) eα2cφ + (1 + 2e)max (1+C (ɪ) + 1--C (白),ɪɪ^|p| + 1-Γ |P|
nn
(46)
where we use the bound on∣∣X(l)∣∣ from Equation 35.	口
LemmaJ.3. ∀i, j ∈ [n],i = j, if XT Xj ≤ 1 - δ ,then X(LT)T XjLT) ≤ e, where
ω (-2∩----V) ≤ e ≤ 1 - ω (^^2∩--V),
n2 (1 - c)	n2 (1 - c)
44
Published as a conference paper at ICLR 2020
and
2
L ≥ ----max I Ω
1	2Ln2α4c4φ log (n2L) log2 m
with probability at least 1 - Ω (加,,provided m ≥ Ω I --φ—δ2------------ I,, where C denotes
C1(φ YCc1( _c1 (φφ_
MerrmO P∞=ι W
Proof. Applying Lemma J.2 with e =②。2 22, We have for each layer l ∈ [L] and i,j ∈ [n]; i = j,
20n αcφ
Wl)TXjl) I ≤ f (WTTX(TD
holds with probability 1 - Ω (亲)，provided m ≥ Ω (2 n α cφ log(n L)Iog m). Here function
f : R → R is s.t. for ρ ∈ R,
f	If(P),	2	if∣ρ∣≥ 条.
P ( 1+c (今)+ /(今 j+总 otherwise.
1	C . ∙	£ ♦ F C F
where function f is defined as
1+c 1-c 2 δ
f (P)=二 ρ+ ^P + n
Thus,
WL)T XjL)I ≤ f ◦ fz ” (IX(O)T χj0) °
L times
Let,s now focus on the function f. It can be seen that for the function f, ^(彳―了 and 1 - ^^―丁
are two fixed points, and starting from any positive P(O) strictly less than 1 - 吟：—and following
fixed point algorithm leads to convergence to the point 必2-④.Since, f equals the function f till
the convergence point, the rate of convergence of fixed point algorithm for the function f is well
approximated by the rate of convergence for the function f . Also, the rate of convergence of f is
equal to the rate of convergence of the function f : R → R defined for each P ∈ R as
1+c 1-c 2
f (P) = —2— P + —2- P .
Lemma J.4 shows that starting at P(O) = 1 - δ, the number of fixed point iteration steps to reach e for
function f is given by 瓷 max (ω log (1), Ω log (ɪ)). Hence, from this argument, if ∣X(O)TXjO)I ≤
1 - δ and L ≥ 击 max (ω (log 1), Ω (log ɪ)), the quantity ∣X(L)TXjL)I becomes less than e . □
Tr	τr /	_ -∏-h t 。	,，八	，♦	3 Trh	Trh ι
Lemma J.4. For P ∈ R, define the function f : R → R by
f (P) = aP +(I - a)P2
where a is a constant in (0, 2). Starting at P(O) = 1 — δ, the number of fixed point iteration steps to
reach e is given by ɪ-a max (Ω (log 1) , Ω (log ɪ)).
T->	八 EI l'	. ∙	C F ∙ , zʌ	1-1 1 . ∙ .1	.	,1	,	∙ , 1 1
Proof. The function f has fixed points 0 and 1, but it’s easy to see that starting at any point below
1, fixed point iteration converges to 0; we want to understand the speed of convergence. We will
divide the fixed point iterate’s path into two sub-paths (a) movement from 1 - δ to 1 - b (b is a small
constant) and (b) movement from 1 - b to e.
45
Published as a conference paper at ICLR 2020
•	Movement from 1 - δ to 1 - b :. We will divide the path into subpaths (1 - 2tδ, 1 - 2t-1δ),
where t is an integer in (0, log b). We show that ∀t ≤ O (log b) ,the number of iterations
to reach from 1 - 2t-1 δ to 1 - 2tδ can be upper bounded by 匚3. The number of iterations
of function f to go from (1 - 2t-1δ, 1 - 2tδ) is at most the number of iterations of the linear
function f : R → R defined by
〃P)=(I-(I- a)2t-1
P.
The number of fixed point iterations of f to go from(1 - 2t-1δ) to(1 - 2tδ) is given by
log 1-2t_1δ
Iog(I_(11-；；— B), which can be shown to be less than 2(2--2：—得 = ɪ-a using Taylor
expansion of log function. Thus, the total number of iterations involved in the entire path is
upper bounded by ɪ-a log ∣.
•	Movement from 1 - b to : The number of iterations of f is t most that of a linear function
f in the domain (0, 1 - b) defined by
f(P) = (I-(I- a)b) ρ.
log 1 - b
The number of iterations of f to go from (1 - b) to e is given by 1	, which upper
lθg 1-b(1-a)
bounded by ®-m log 1-b, for small enough constant b.
Thus, summing the number of steps needed in the two subpaths leads to the desired quantity. □
Theorem J.5. If
2
L ≥ ----max I Ω
then
λmin (G(O)) ≥ ω (l⅛(log n) (φ0))
1	2Ln4α4c4φ log (n2L) log2 m	0
with probability at least 1 一 Ω (正)，provided m ≥ Ω I -------φ-g2---------- I, where Ck φo )
denotes the kth order coefficient in the probabilists’ Hermite expansion of φ0 and c denotes the ratio
吊(φ)
P∞=1 或(Φ) .
Proof. Assuming Equation 35 with e = 2 J 2, using Lemma I.1 we get
n cφ α
λmin (G∞) ≥ cΘ(log n) (φ0) λmin
+ e0 + e
00
(47)
where X(L) denotes a m X n matrix, with its ith column containing x(L), which is the unit normalized
form of XLL, (X(L)) denotes its order rth Khatri-Rao power. There are two error terms e0 and e00
because of two reasons (a) norm of xi(L) is e away from 1 (b) magnitude of wT xi(l) is restricted to
3√log m. Following the line of proof of Equation 38, magnitude of e0 can be bounded to O (ncφa2e).
Also, following the line of proof of Equation 40, magnitude of e00 can be bounded to n Jlogm. Now,
we make the following claims.
First, xiTxj, for any i, j ∈ [n] with i 6= j, can be shown to be at most to 1 - δ, using Assumption 2.
46
Published as a conference paper at ICLR 2020
Second, if XTXj = P s.t. |p| ≤ 1 一 δ, applying Lemma J.3 shows that (X(L)) XjL) ≤ 己 for a
small constant 七 provided L ≥ 高 max (ω (log 1), Ω (log 4)), with high probability.
Third, if for any i,j ∈ [n], (X(L)) XjL) = ρ0 < 1, then we can see that ((X(L)) ) (XjL))	=
ρ0r, where X*r denotes the order-r Khatri-Rao product of a vector x.
Combining these claims, we get the following.
First, The diagonal elements of the matrix
* log n
* log n
are equal to 1. Second,
* log n T	* log n
the non diagonal element at row i and column j of the matrix	X(	ι	( X( ) is
given by ( (XiL))	' ) (XiL))	' , whose magnitude is bounded by 髀g n. Hence, if
2
L ≥ ----max I Ω
T
each non diagonal element,s absolute value becomes less than * and so the absolute sum of non
diagonal elements is at least 2 away from the absolute value of the diagonal element, for each row of
the matrix (X(L)) g (X(L)) g . Using Fact C.10, we get for r = log n,
λmin (X(L)*rTX(L)*r) ≥ j	(48)
Combining the value of 0, value of 00 and Equation 48 gives us the minimum eigenvalue of G∞ .
Since,
λmin (G(O)) ≥ λmin (G∞) 一 ](g∞ 一 G(O)) || ,
we use Hoeffding’s inequality to bound the magnitude of each element of G∞ 一 G(0) to
λmin (G∞) /2n and hence, get a bound on λmin (G(O)).	□
Theorem J.6. If φ = tanh and
then
λmin G(O)
L ≥ ---max
-Ω
≥e
1/poly(n)
n
with probability at least 1 — Ω (^^), provided m ≥ Ω (2 n log),L)Iog
m
, for an arbitrary
constant > 0. The constant c denotes the ratio
至 2(tanh)
P∞=1 或(tanh)'
Proof. The proof follows from Theorem J.5, with the bound on Hermite coefficients for tanh0 from
Eqn. (34).
We re-state the rate of convergence theorem from Du et al. (2019b) and use our bounds on minimum
eigenvalue of the gram matrix to give a more finer version of the theorem.
47
Published as a conference paper at ICLR 2020
Theorem J.7 (Thm 5.1 in Du et al. (2019b)). Assume that Assumption 1 and Assumption 2 hold
true, |yi| = O(1) ∀i ∈ [n] and the number of neurons per layer satisfy
J O(L)	I	n4	n n2 log (Ln)[
m ≥	2 max] λmm (G(O)) , K, λmm (G ⑼)I
Then, if we follow a Gradient Descent algorithm with step size
η = O( λmin (G ⑼)]
η [ n22O(L) ) ,
with probability at least 1 - κ over the random initialization, the following holds true ∀t ≥ 1.
y-
≤
(
1 -
∖
Thus, refining the above theorem with our computed bounds for λmin (G(O)) > O( 1), we get the
following.
Theorem J.8. If φ = tanh,
L ≥ ---max
and the number of neurons per layer satisfy
m ≥ Ω
2O(L) max
卜, K,n4 log
then, if we follow a Gradient Descent algorithm with step size
η ≤ O (n32o(L)) ,
with probability at least 1 - κ over the random initialization, the following holds true ∀t ≥ 1.
The constant c denotes the ratio
y-
t
1 - 2n)卜-
c2 (tanh)
P∞=或(tanh) ∙
u(O)
2
48
Published as a conference paper at ICLR 2020
K	Lower B ound on Lowest Eigenvalue for Non-Smooth Functions
For α ∈ (-1, 1) define the activation function φ by
φ(x) = φ1 (x)Ix<α + φ2(x)Ix≥α.
We show that the minimum singular value of the G-matrix is at least inverse polynomially large in n
and δ, provided φ1 and φ2 satisfy the following properties for some positive integer r. We denote this
condition by Jr .
•	φ1 , φ2 ∈ Cr+1 in the domains (-∞, α] and [α, ∞), respectively.
•	The first (r + 1) derivatives of φ1 and φ2 are upper bounded in magnitude by 1 in (-∞, α]
and [α, ∞) respectively.
•	For 0 ≤ i < r, we have φ(1i) (α) = φ(2i) (α).
•	φ(1r) (α) - φ(2r)(α) = 1, i.e. the r-th derivative has a jump discontinuity at α.
In the following we consider J1 and J2. The results can be easily generalized to higher r but with
lower bound degrading as n-2r.
K. 1 J1 : THE FIRST DERIVATIVE IS DISCONTINUOUS AT A POINT
K.1.1 DZPS SETTING
Recall that this setting was defined in section 2. ReLU, SELU and LReLU satisfy the conditions for
the following theorem. The data set {(xi, yi)}in=1, for xi ∈ Rd and yi ∈ R is implicitly understood
in the theorem statements below.
Theorem K.1. Let the condition on φ be satisfied for r = 1. Assume that WkO) 〜
N(0, Id) and a(k0) 〜 N (0,1) ∀k ∈ [m]. Then, the minimum singular value of the G-matrix
satisfies
λmin (G ⑼)≥ ω( n),
with probability at least 1 一 e-WδmVn ) With respect to {wk0)}m=ι and {ak0)}m=ι, given that m
satisfies
n3 n
T log δ1∕4	.
Proof. In the following, we will write wk instead of w(k0) and ak instead of a(k0) . Consider the
following sum for an arbitrary unit vector ζ and a random standard normal vector w:
n
X ζi φ0 wTxi xi.
i=1
To lower bound the lowest eigenvalue of the G-matrix, we will give a lower bound on the norm of
this vector. In order to do this, we use the following claim whose proof is deferred to later in the
section.
Claim K.2. Forζ ∈ Sn-1, let
n
f(w) = Xζiφ0 wTxi xi.
i=1
Then we have
WjPr0,id)nf(w)L ≥√⅛)≥ ω( ⅛).
49
Published as a conference paper at ICLR 2020
From this claim, we have
Pr
W 〜N (0,Id)
xi
≥ Ω
Hence,
Pr
W 〜N (0,id),a 〜N (0,1)
owing to the fact that for a standard normal variate α, ∣a∣ is at least 1 with probability at least 0.2
using Fact C.1. Applying the Chernoff bounds, we have
{Wfc}mPK}m	m XX Ziakφ0 (wTXi) Xi	≥ ⅞Tδ
{Wk }k=1 ,{ak }k=1	m k=1 i=1	n
To get the bound for all Z ∈ Sn-1, We use an e-net argument with e = Θ (W) and e-net size (ɪ)n.
This gives that
m
ɪ X
m
k=1
holds for all Z ∈ Sn-1 with probability at least
1-
with respect to {wk}k=ι and {ak}m=ι, assuming that m ≥ Ω (先 log nδrj ∙ Thus, Using the fact that
λmin (G(O)) = mσmin (M)2, we get the final bound.	□
Corollary K.2.1. Let the activation be ReLU, then
with probability at least 1-
satisfies
-Ω
e
k=1
and a(k0)
,
k=1
given that m
)
m
m
We now move on to showing the main claim required in the Theorem K.1. Claim K.2 is an adaptation
of (Allen-Zhu et al., 2019, Claim 6.4) with a slightly different choice of parameters and exposition.
ProofofClaim K.2 . Let i* denote arg maxi∈[n] Zi. We split vector W into two independent weight
vectors, as follows
w = w0 + w00,
w0 = (Id - Xi*XT) w - pl - θgιXi*,
W0 = θg2Xi*,
(49)
where g1 and g2 are two independent Gaussian random variables following N 0, l - θ2 and
N(0, θ2) respectively and we set θ =皋.
50
Published as a conference paper at ICLR 2020
Let E denote the following event.
{l .E	I δ . I .E	I δ	δ δ
Iw xi* - α l< 际 andl w xi - α l> * VI ∈ [n]∖{i }and 的 21e(而，嬴
Assuming event E occurs, for i = i* We have
w'τx› -α I ≤ ι0⅛, I WT%l=lθg2χTχ*l ≥ 乙,
and for Vi = i* we have
I w'τXi- α∣ > 白,I w''τχ∕="g2XTχi i ≤ /.
Hence, conditioned on E, for i = i* we have ZWTxi≥α = Iwzτxi≥ɑ always and ZWTx^* ≥α =
∑w∕τχ,* ≥ɑ with probability 1/2.
Conditioned on E and using triangle and Cauchy-Schwartz inequalities we get
k Σ	ζi φ'(wτXi)Xi - E ζi φ'(w'τXi)Xik2
i∈[n] ,i=i*	[n],i=i*
≤ X kZiXik2∣ φ'(wτXi)- φ'(w'τXi)I
i∈[n] ,i=i*
≤( X	kMik2)	( X (φ'(wτXi)-φ'(w'τXi))21
i∈[n],i=i*	i∈[n],i=i*
=(X	(φ'(WTXi)- φ'(W'τXi))2)
i∈[n],i=i*
≤ ( X	I wτXi - w'τXi I 2)	(50)
i∈[n],i=i*
≤ O (Vn) ξ^^2 = O f A 1 5 ),
5n2	∖5n1∙5 J
where we use our assumption that ∣ φ''(x) ∣ ≤ 1 for x ∈ R ∖ {α} in Inequality 50. Conditioning
on E was used in concluding that either φ'(wτXi) - φ'(w'τXi) = φ1(wτXi) - φ1(w'τXi) or
φ'(wτXi) - φ'(w'τXi) = Φ2(wτXi) - Φ2(w'τXi). In other words, φ1 and φ2 “don'tmix”. Since
∣limz→α- φ'(Z) - limz→α+ φ'(Z)I = 1 and∣Zi*∣ ≥ =, we have
To see this, note that given conditioned on E, with probability 0.5 with respect to g2, wτXi is going
to cross the jump discontinuity at α and thus, φ' is going to change by at least 1, minus the maximum
movement on either side of α, which is bounded. Thus,
≥ 0.5.
We now need to show that E occurs with high probability. To do this, we state the following claim
that we prove later.
Claim K.3. Let all the variables be as in Claim K.3. Then,
Pr f∣ W'τXi* 一 ɑ ∣ ≤ —ʒ and ∣ W'τXi — α ∣ ≥ -ɪ- Vi = i*) ≥ Ω (-，
W，l∣	i I - 10n2	I i I - 4n2	十 )一	1n2√ι - ff2
51
Published as a conference paper at ICLR 2020
From Claim K.3, We have
Pr (IWOTXi* — Ql ‹ —ʒ and IWOTXi — Ql > -ɪ- Vi = i*\ > Ω (	§ ).
w，l∣ i I - 10n2	l i I - 4n2	十 √ √	ι√1 —. n2)
> Pr (Ilf (w)II >
一W 〜N(0,id)	2 一 √n
Pr	IIf(W)L >
W 〜N (0,id)	2 一
E Pr	[E ]
W 〜N (0,Id)L
> ω (nɪ
Proofof K.3. By the definition of w0, WOTXi* is equal to √1 — θ2gι, which is distributed according
to N (0,1 — θ2). Hence, applying concentration bounds from Fact C.1, we get that
Pr (IWZTXi* — Q∣ < —ɜ^ >-----------.	.	(51)
w，ll	L 10n27 - 40n2√1 — θ2
We can divide wz Vi ∈ [n] into two parts:
•	Component orthogonal to Xi* given by WZT (Id — Xi* XT) Xi
•	Component parallel to Xi* given by WZT (x* XT) Xi.
This gives us
WOTXi = WOT (Id — Xi*XT) Xi +WOT (xi*XT) Xi.
'------------------------'
'∙^^^^^^^^^^^{^^^^^^^^^^^^
◊
Conditioning on WOTXi* such that Equation 51 is satisfied, we get that WOTXi is distributed according
to
WZT Xi 〜N I W0T
(xi*XT) Xi, (1 — θ2) (Id — Xi*XT) Xi
By our assumption, 1(Id — Xi* XT) x^ > δ. Also,
0 < WOT Xi* XiT*	Xi
δ 1
< Q +------ < 1
一	10n2 一
(52)
Hence, again applying concentration bounds from Fact C.1, we get for a fixed i = i*
Pr(IWOT Xi — Q i >
—> 1----,	.
4n2√ —	5n2 VT—θ2 δ
Taking a union bound, we get that Vi ∈ [n] and i = i*
Wr(IWOT χi- q i>
4n2 J > 1 - 5n√T1> > 5,
□
2
2
δ
δ
δ
1
4
as required.
□
52
Published as a conference paper at ICLR 2020
K. 1.2 J1 FOR STANDARD SETTING
The above theorems can be easily adapted to the standard settings, defined in Appendix D. We
capture this with the following corollaries.
Corollary K.3.1 (Adapting Theorem K.1 for Init (fanin) setting). Let the condition on φ be satisfied
for r =	1.	Assume, w，0)	~ N(0, dId)	,	b(0	~ N (0,0.01) and a^0	~ N(0,	m)	Vk	∈	[m].
Then, the minimum singular value of the G-matrix satisfies
λmin
≥ Ω
with probability at least 1 一 e-Wδm∕n ) With respect to {wk°)}k=ι, {bk°)}m=ι and {ak°)}k=ι, given
that m satisfies
m ≥ Ω (Q log ʌ
— δ g δ1/4
Corollary K.3.2 (Adapting Theorem K.1 for Init (fanout) setting). Let the condition on φ be satisfied
for r = 1. Assume, w，0) ~ N(0,mId) , a，0) ~ N (0,1) and b，0) ~ N(0, .) Vk ∈ [m]. Then,
the minimum singular value of the G-matrix satisfies
λmin
≥ ω (/
with probability at least 1 一
that m satisfies
e-Wδm∕n) with respect to {w，0)}，^, {b，0)}，^ and {0，°)}烂「given
m ≥ Ω
K.2 J2 : THE SECOND DERIVATIVE HAS JUMP DISCONTINUITY AT A POINT
K.2. 1 DZPS setting
Recall that this setting was defined in Section 2.
Theorem K.4. Let φ satisfy the condition for r = 2. Assume that WkO) 〜N(0, Id) and a，0)〜
N(0, 1) Vk ∈ [m]. Then, the minimum singular value of the G-matrix satisfies
λmin (G(O)) ≥ ω (n⅛
With probability at least 1 — e-Wδm∕n^ With respect to {w，0)}m=i and {a，0)}m=i, given that m
satisfies
n3
m > max Ω I ɪ log
Ω ( n2 log(d)
Proof. In the following, we will use w， instead of w，(0) and a， instead of a(，0). Referring to
Equation 4, it suffices to show that
n
ζimi
i=1
2
m
=X
2	，=1
n
w
i=1
T
，
xi
2
2
is lower bounded for all vectors ζ ∈ Sn-1 with high probability. Fix a particular ζ ∈ Sn-1. For each
k ∈ [m] and each i ∈ [n], We have WTXi ~ N (0,1). First, We analyze the sum for a fixed k, i.e. We
consider Pin=1 ζi φ0 w，T xi xi. We split vector w， as
Wk = W，+ W，,
53
Published as a conference paper at ICLR 2020
where w^k and Wk are two independent Gaussian vectors in Rd distributed according to
N(0,(1 - θ2) Id) and N (0,θ2L) respectively. We set
θ
δ
2000 n2 √logn
Define the event Cw on a weight vector w as
Cw =	wT xi - α ≥
盛:i ∈ [n]}.
Using Fact C.1 and the fact that xi are unit vectors,
δ
w〜n (oPr-θ2)id) [Cw]
Define the event Dw on a weight vector w as
1--------,
400n√1 - θ2
Fact C.2 shows that,
Dw =	wT xi ≤
二：i ∈ [n]∖ .
500n2	j)
Pr	[Dw] ≥ 1 - 2n exp
w~N (0,θ2 Id)
≥ 1 - M
n7
≥ 0.5,
(53)
(54)
(55)
(56)
≥
where we set t = 500δn2θ to get Inequality 55.
We want W^k to satisfy condition Cw卜 and W k to satisfy condition Dw卜.Since, W^k and Wk are
independent of each other, we use Equation 53 and Equation 54 to get
Pwk ,wk (Cwk and Dwk) ≥ 0.25.
Assuming both the conditions hold, it follows that Zwt乂i=IwTχi≥ɑ∙ We will work conditioned
on both the events.
Define a function f as follows,
n
f(W) = Xζi φ0(WTxi).
i=1
Note that
n
Vwf(w) = XZi φ"(WTXi)xi.
i=1
In the sequel, we will use f0 for Vwf(W) and f00 for V2wf(W). It is easy to see that the only
discontinuities of the derivative of function f are when WTxi = α, since it is the only point of
discontinuity for φ00. Thus, assuming that Cw卜 and Dw卜 hold, we can apply Taylor expansion to
f (Wk) for a perturbation of Wk, ensuring that all the derivatives exist in the neighborhood of interest.
Hence,
f (Wk) = f (Wk) + hWk,f(Wk)i + R2 (Wk),
where R2 denotes the second order remainder term in the Taylor expansion given by
R2 (Wk)
2 Zt OD f" (Wk + tWk), Wm2E
dt.
Using V2wf(W) = Pin=1 ζi xim2 φ(3) (z)
, we have
z=hw,xi i
1	1n
R2 (Wk) = - / EZi (〈Wk, Xii)2 φ⑶(hWk + tWk,Xii) dt.
2 t=0 i=1
54
Published as a conference paper at ICLR 2020
The magnitude of this term can be bounded as follows.
∖
X Z2 (hwk, Xii)4	(57)
i=1
dt
(58)
where Inequality 57 uses Cauchy-Schwartz inequality, Inequality 58 uses the fact that all the deriva-
tives of φ of order UPto r +1 are bounded for x = 0 and wk satisfies condition DW卜 and wk satisfies
CWk. Thus we have
f (Wk) = f(Wk) + hwk,f(Wk)i + O (n35
(59)
Consider the following two cases.
Case 1： f(wk)∣ < 1 ^00θ
First, we condition on the event that wk is picked so that∣∣f0 (Wk)∣∣2 ≥ 0n1 and CWk holds true. We
shall refer to this condition as BWk. By Claim K.5, we get that
w r [bw J=W r (∣∣f0(W k)∣2 ≥ 詈and Cw J ≥ ω ((T-⅛).
(Wk, f(Wk)i is a random variable following N(0, θ2 ∣∣f 0(Wk) ∣∣2). Thus applying Fact C.1,
(60)
wr(∣hwk,f0(wk)i∣ ≥ ∖∣01θ'θ Bw
%) ≥ 5.
Now letting event D W k denote the complement of the event DW k We have
= Pr
W k
≥ 1 - W
5 n7
≥ 1/10.
0≡ θ ^ D
001 θ BWk,DW
Hence, from Equation 59, we get
Pr ∣f(wk)∣ ≥
2J0nF θ and DW k
BWk	≥ 0.1.
n
W k
W "D W k]
Thus,
Pr(If(Wk)∣ ≥ 1∖∕叫θ ) ≥ Pr(If(Wk)∣ ≥ 1∖∕四θ and BWk and DWk
Wk	2 n	Wk	2 n	k	k
≥Wr[bwk]pr If(Wk)∣ ≥ 1∖∣0n1 θandDwk
BW k
≥ ω (/
55
Published as a conference paper at ICLR 2020
Case2: If(WQ∣ ≥ 氯冷θ.
We can upper-bound the magnitude of f (Wk) by O(√n) as follows.
n
Ilf0(W k)ll = X Ziφ"(w!T Xi)Xi
i=1
≤t
n
Xζ2φ00(W T Xi)2 ∖
i=1
n
XkXi『≤o(√n).
i=1
Here we use the fact that kζ k = 1, φ00 is bounded by a constant at all x 6= α andkXik = 1 for i ∈ [n].
Note that, this bound always holds true, irrespective of the value of Wk. Again, using the fact that
〈Wk, f(Wk)i is a Gaussian variable following N(0, θ2 ∣∣f0(Wk)∣∣2), Fact C.1 shows that,
0.1
≥ 一
2/3
√nllf0(W k)1 ≥
0.1 2/3
4n
k
0n1 θ Cw
—60n	n7
≥ω(n).
---θ	CW k, DD W
w r [D W %]
4
n

W k
Hence, from Equation 59, we get
Wr lf(wk)l≥ 1v0nιθ ^ DWk
CWk	≥ω(1
Thus,
Pr(If(Wk)l	≥ 1∖∕001 θ	) ≥ Pr(If(Wk)l	≥ 1∖∕001 θ and	CWk	^ DWk
Wk	4 n	Wk	4 n
≥Wr[Cwk] pr If(Wk)∣ ≥ 1∖∣0n1 θandDW%
≥ ω (1
Thus, combining the two cases, we have
Wruf(Wk )∣ ≥ ω
n2-5 √logn
≥ Ω
(61)
δ
Hence,
I
n
δ
≥ ω g
Pr
wk,ak
EZiakΦ0(WTXi) ≥ Ω
i=1
n2-5 ʌ/log n
(62)
∖
owing to the fact that for a standard normal variate a, |a| is at-least 1, with probability at-least 0.2
using Fact C.1. Applying a Chernoff bound over all k ∈ [m], we get
2
mn
X X ZiΦ0 (WTXi)	≥ Ω
k=1	i=1
δ3m
n7 log n
56
Published as a conference paper at ICLR 2020
with probability at least 1 - exp(-Ω(δm)) With respect to {wk}m=ι and {ak}m=ι∙. Applying an
e-net argument over Z ∈ Sn-1, with e = 2n4√0gn and e-net size (ɪ)n, we get that
X(X Ziφ0(wT Xi))2 ≥ ω (三!	(63)
holds for all ζ ∈ Sn-1 with probability at least
1 - (2n4√ogn!n e-Ω(黎)
with respect to {wk}m=ι and {ak}「「，where we assume that m > Ω (n3 l°g (δn3^) ). Now
consider the following function,
mn
f(w) = X X ζiakφ0 wkT xi xi
k=1 i=1
where ζ ∈ Sn-1. Note that f(w) is a d-dimensional vector. Also, the above can be written as
f(w) = Qv
where Q = [qij ] ∈ Rd×n is defined by
qi,j = ζj xj,i
and v ∈ Rn, defined by
vi
akφ0(wkTxi).
Also, since kζk = 1 and kxik = 1 ∀i ∈ [n], we have kQkF = 1. Consider the following quantity
Pjn=1 qi,j vj . This quantity denotes the dot product of a row vector of Q and v. We can apply
Equation 63 to get
2
n
X qij v
j=ιkqikj
δ3m
≥ ω n7i°gn
holds true with probability at-least 1 - e- (n2)with respect to {wk}m=ι and {ak}m=「Note
that, the coefficients have been normalized to unit norm to satisfy the condition based on which
Equation 63 was derived. We can take a union bound over all the rows of Q to get
f(w)2
i=1
j=1
qi,jvj
2
d
≥ Xkqik2 Ω
i=1
δ3m
n7 l°g n
d
n
=kQkF ω( ɪ )=ω( ɪ
F	n7 l°g n	n7 l°g n
with probability at least
with probability at least 1 - e
1 - de-Q(翳)≥ 1-e-Q(翳)
「ω(n2) with respect to {wk}m=I and {ak}k=1, assuming that m ≥
Ω (n2 Iog d). Thus, we can use Equation 4 to show that,
σmin (M) ≥ ω[S nτ3mnJ
Using the fact that λmin (G(O)) = mσmm (M)2, we get that λmm (M(O)) ≥ Ω(n：：gn) with
-ω( δm
probability at least 1 - e	t n2 人	□
57
Published as a conference paper at ICLR 2020
ELU satisfies the conditions required for Theorem K.4. We state this explicitly in the following
theorem.
Corollary K.4.1. Assume	WkO)	~	N (0, Id)	and a^0 ~ N (0,1) ∀k	∈	[m],	if	φ(x)=
Zχ<0 (ex — 1) + Iχ≥ox, we have thatfor the G-matrix,
λmin (G(O)) ≥ Ω (-73^∖
n7 log n )
with probability at least 1 — e-Q(U) with respect to {w(O)}	, given that m satisfies
n∖
m > max Ω I ɪ log
ω n nl log(d)
Claim K.5. Let the variables have the same meaning as in the theorem Theorem K.4. Then,
Pr	( kf0(W)Ill > ɪ and CW) > ^(ɪ-)
W~N (0,c2∑d)	l — 4n	cnl'
for a variable c that depends on n and δ.
Proof. Let i* denote arg maxi∈[n] ζi. We can split w as W = W + w”, where
w0 = (Id — Xi* XT )w + √1 — θlg∖Xi*
and
w" = θglXi*
where θ = / and gι,gl are two independent gaussians 〜N(0, cl). Let E denote the following
event.
{l ,τ	I δ 一 I ,τ	I δ	δ δ
∣w xi* —α i < 10nland i w Xi- α i > * VI ∈ [n],i = i and ∣θgl∣ ∈ (荻,获
Event E satisfies condition CW because for i*,
wτx* — Q | = | WOTXi* + w"τXi* — ɑ∣ > ∣ WOTXi* — Q | — | w,,τXi* | > go i
and for all i = i*,
∣ wτXi — α 1 =∣ WOTXi + w"τXi — α∣ > ∣ WOTXi — α — — ∣ w//TXi ∣ > ^^
Hence, we can write that
Pr	(IIf0(w)∣∣∣ ≥ ɪ and CW)
w~N (o,c2id)	4n
≥ Pr	(Ilf0(W)IlI ≥ ɪ andCW E) Pr E
—w~N(o,(i-θ2)id) ∖	— 4n	W w~N(o,(i-θ2)id)
= Pr I If0(w)∣∣ ≥ ɪ E J Pr E
w~N(o,(i-θ2)id) ∖	- 4n	W w~N(o,(i-θ2)id)
10.2δ _ δ
—2 cn2 cnl '
where we use Claim K.6 in the final step.
□
58
Published as a conference paper at ICLR 2020
Claim K.6.
Pr
w~N (0,c2Id)
kf0(w)∣∣2 ≥ 0n1E
and
Pr	(E) ≥
w~N(0,c2Id)
0.08δ
c2 n2
1
≥ —
一2
where E is defined and w has been split into w0 and w00 as in proof of Claim K.5.
Proof. This follows from the proof of Claim K.2, with a slight change in distribution of w from
N (0, Id) to N 0, c2Id and the function under consideration is changed to
n
f0(w) = Xζiφ00	wTxi	xi.
i=1
□
K.2.2 J2 FOR STANDARD SETTING
As before, we state the main theorem for standard initializations, defined in Appendix D, as corollaries.
Corollary K.6.1 (Adapting Theorem K.4 for Init(fanin) setting). Let the condition on φ be satisfied
for r = 1. Assume,	w，0)	〜N(0, dI√) , b(0	〜N (0,0.01) and	ak°)	〜N(0, J)	Vk	∈	[m].
Then, the minimum singular value of the G-matrix satisfies
λmin (G(O)) ≥ ω (n7dlogn)
(”)with respect to {wk°)}	, given that m satisfies
with probability at least 1 一 e ω
m> max (ω (9 log (δV⅛79)) , ω (n⅞(d)
Corollary K.6.2 (Adapting Theorem K.4 for Init(fanout) setting). Let the condition on φ be satisfied
for r = 1. Assume, w，0) 〜N(0, JId) , b,)〜N(0, J) and a，0) 〜N (0,1) Vk ∈ [m]. Then,
the minimum singular value of the G-matrix satisfies
λmin(G(0)) ≥ Ω
δ2
mn4 log n ,
with probability at least 1 — e-Q(δmVnO With respect to {wk0)}m=ι, {bk0)}m=ι and {ak0)}m=ι, given
that m satisfies
n2	mn5 log n
m ≥ ω 了 log
59
Published as a conference paper at ICLR 2020
L A NEW PROOF FOR THE MINIMUM EIGENVALUE FOR ReLU
Theorem L.1 (Thm 3.3 in Du et al. (2019a)). Consider the 2-layer feed forward network in Equa-
tion 1. Assume that WkO)〜N (0,1) and ak0) 〜U{ —1,+1} ∀k ∈ [m], Assumption 1 and
Assumption 2 hold true and |yi | ≤ C, for some constant C. Then, if we use gradient flow optimiza-
tion and set the number of hidden nodes m = Ω
over the initialization we have
n6 log m
ʌ——(	K4 3 , with probability at least 1 — K
λmin (G ) κ
kut — yk2 ≤e-λmin(G∞)tku0 —yk
Remark. The above proof can be adapted for the gradient descent algorithm with a learning rate less
than O (Am® (G∞) /n2), following the proof of Theorem 5.1 in Du et al. (2019b) and theorem 4.1
in Du et al. (2019a) without substantial changes in the bounds.
Theorem L.2. Assume that we are in the setting of Theorem L.1. If the activation is ReLU and
m ≥ Ω (n4δ-3 log4 n ,then with probability at least 1 — exp ( — Ω Qm^ n))
λmin (G(O)) ≥ Ω
1.5
Remark. Compare the previous theorem with Cor. K.2.1.
Proof. Using Lemma I.1, we have
λmin (G∞) ≥ Cr (φ0) λmin ((X*r)T X*r) , VT ∈ Z+
where X denotes a d X n matrix, with its i-th column containing Xi and X*r ∈ Rdr ×n denotes its
order-r Khatri-Rao product. Note that, by Assumption 1, the columns of X*r are unit normalized
euclidean vectors. If for any i, j ∈ [n], XTXj = ρ < 1, then we can see that (X厂)T x*r = ρr, where
x*r denotes the order-r Khatri-Rao product of a vector x. Also, |p| can be shown to be at most 1 — δ,
using Assumption 2. Thus, for the magnitude of (x*r)T xjr, for any i, j ∈ [n],i = j, to be less than
2n, we must have r ≥ to = log52n. Hence, for any r ≥ to the diagonal elements of (X*r )T X*r are
equal to 1 and magnitude of the non diagonal elements are less than *. Thus, applying Fact C.10,
we get that
λmin ((X*r )T X*r) ≥ j .
Using Equation 18, we see that for r = Θ (log52n),
Thus,
For computing λmin G(o) , we bound the absolute difference in each element of G∞ and G(o) by
2nλmin (G∞) using Hoeffding,s inequality (Fact C.3) and 1-Lipschitzness of φ and then apply a
union bound over all the indices. The bound stated in the theorem follows from the fact that
λmin G(o)
≥ λmin (G∞)
F
≥ λmin (G∞) — λmin (G∞) /2.
□
60
Published as a conference paper at ICLR 2020
Using the bound of λmin (G∞) from Theorem L.2 in Theorem L.1, we get the following explicit rate
of convergence for 2 layer feedforward networks.
Theorem L.3 (Thm 3.3 in Du et al. (2019a)). Assume that the assumptions in Du et al. (2019a)
hold true. Then, if we use gradient flow optimization and set the number of hidden nodes m =
Ω (n logδ6n3log m ), with probability at least 1 - K over the initialization we have
kut -yk2 ≤
∀t ≥ Ω (logι.5 n log n), for an arbitrarily small constant e > 0.
61
Published as a conference paper at ICLR 2020
M Extensions and Additional Discussion
In this section, we discuss proofs and extensions of our theorems, using adaptations from related
work.
M.1 POLYNOMIAL MINIMUM EIGENVALUE OF G-MATRIX AT TIME t
The upcoming lemma shows that, if we restrict the change in weight matrices, the minimum eigenvalue
of G(t) stays close to the minimum eigenvalue of G(0) .
Lemma M.1. IfaCtivationfunCtion φ is α-lipsChitz andβ-smooth and∣∣wrt) 一 wr0) ∣∣ ≤，：＜,),
∀r ∈ [m], then
λmin (G㈤)≥ 2λmin (G⑼)
Proof. The claim follows a similar proof as the proof of lemma B.4 in Du et al. (2019b) and has been
repeated in Lemma M.11.	□
The restriction is ensured by the large number of neurons we can choose for our neural network, as
we mention in the next lemma.
Lemma M.2. Let St ⊆ [n] denote a randomly picked batch of size b. Denote V(t) as
▽w(t)L ({(xi, yi)}i∈[n]; a, W(t)). Let the activation function φ used be α-lipschitz and β-smooth.
The GD iterate at time t + 1 is given by,
W(t+1) = W(t) 一 ηV(t)
Let η ≤ O
. If
m ≥ Ω
n4α4β2 log m
λmin (G(O))4
then,
∣∣y 一 u(t) ∣∣	≤
for t ≥ Ω
Moreover,
(log(n)
ηλmin(G(O))
, with probability at-least 1 一
m-3.5
w.r.t.
nw(kO)okm=1 and na(kO)o
k=1
(t)
wk
(0)
一 wk
λmin (G(O))
4αβn
m
holds true ∀k ∈ [m] and ∀t ≥ 0.
Proof. The claim follows a similar proof as the proof of lemma A.1 in Du et al. (2019b), where we
keep the output vector a non trainable in GD update.	□
Remark. The above lemmas are applicable for activation functions in Jr for r ≥ 2. Similar lemmas
can be proved for J1, along the lines of the proof of Theorem 4.1 in Du et al. (2019a).
M.2 Trainable output layer
Similar to the proof of Theorem 3.3 in Du et al. (2019b), we can show that the GD dynamics depends
on sum of two matrices, i.e.
du(t)
dt
(G(t) + H(t))(y 一 u(t)),
62
Published as a conference paper at ICLR 2020
where the definition of G stays the same and H is given by
hij = m1 X σ(wTXi)σ(wTXj),
r∈[m]
implying H is p.s.d. For the positive results, e.g. Theorem 4.1, observe that λmin(G + H) ≥
λmin(G), hence a bound on λmin (G) suffices in this case. For the negative results, Theorem 4.3 and
Theorem 4.4 can be restated as follows.
Theorem M.3. Let the activation function φ be a degree-P polynomial such that φ0(x) = Pp- c'x'
and let d0 = dim (SPan {xι...xn} = O n 1). Then we have
λk G(0) + H(0) = 0, ∀k ≥ 2n/d0
Theorem M.4. Let the activation function be tanh and let d0 = dim sPan {x1 . . . xn}
O log0.75 n . Then we have
λk(G(0) + H(0)) ≤ exp(-.(n1/2''))《1∕poly(n), ∀k ≥ ∣^2n∕d[
with probability at least 1 - 1/n3.5 over the random choice of weight vectors wk(0)
na(k0)okm=1.
m
and
k=1
Proof. (Proof sketch for Theorem M.3 and Theorem M.4) Following the proof of Theorem 4.3 and
Theorem 4.4 gives Us similar lower bounds for H(0) i.e. n(1 一 1)lower order eigenvalues are 0,
if φ is a degree-p polynomial and exponentially small in n1/d0 with high probability, if φ is tanh.
Thus, We can use Weyl's inequality (Fact C.11) to show that n(1 一 %) lower order eigenvalues
of G(0) + H(0) are 0, if φ is a degree-p polynomial and exponentially small in n1/d0 with high
probability, if φ is tanh.	□
Remark. The lemmas in subsection M.1, that were proved for a network with non trainable output
vector a, can also be proved for a network with trainable output vector a. And so, a polynomial lower
bound on minimum eigenvalue of G(0) implies polynomial lower bound on minimum eigenvalue of
G(t), under appropriate number of neurons and GD training.
M.3 Multi Class Output with Cross Entropy Loss
Let’s say, we have a classification task, where the number of classes is C and we use the following
neural network for prediction.
m
fq (x; A, W):= √L X ak,qφ (WT x) , Vq ∈ [C ]
m k=1
where x ∈ Rd is the input and W = [w1, . . . , wm] ∈ Rm×d is the hidden layer weight matrix and
A ∈ Rm×C is the output layer weight matrix. We define u (x) ∈ RC as
u (x) = softmax f(x; A, W)
where softmax on a vector v ∈ RC denotes the following operation
evi
Softmax(v)i =------------.
j∈[C] evj
Given a set of examples {xi, yi}in=1, where xi ∈ Rd and yi ∈ [C] ∀i ∈ [n], we use the following
cross entropy loss to train the neural network.
n
L (a, W; {xi,yi}n=l) = 一 X log (fy"x; a, W))
i=1
63
Published as a conference paper at ICLR 2020
yC(i-1)+j = {
Let's denote the vector y as an nC dimensional vector, whose elements are defined as follows.
0, ifj 6= yi
1, otherwise
Also, let's define another vector U ∈ RnC as follows.
UC(i-i)+j =Softmax (f(xi； A, W)) j
All the network dependent variables have a superscript t, depending on the time step at which they
are calculated.
Using chain rule and derivative of cross entropy loss w.r.t. output of softmax layer, we can show the
following differential equation for gradient flow.
dU = G ㈤(y - U)
where G ∈ RnC×nC is a gram matrix defined by its elements as follows.
gpr = m1 X ak,qak,q0Φ (WTXi) φ0 (WTXj),
k∈[m]
where i = [CC + 1,j = [CC + 1,q = P mod C and q0 = r mod C. Thus,
:I = - (y - u(t))TG㈤(y - u(t)) ≤ -λmin (G㈤)∣∣y - U⑴∣∣2
Again following the argument discussed in section 3, if there hasn't been much movement in the
weights of the network due to large number of neurons, (G(t)) stays close to (G(O)) and hence, the
rate of convergence depends on the gram matrix (G(0)). We show that the gram matrix possesses a
unique structure and is related to the gram matrix defined for a single output network.
G contains C disjoint principal n X n blocks, denoted by {Bq}C=ι, where Bq is defined as follows:
bqj = m X ak,qφ (wTxi) φ0 (wTxj).
k∈[m]
As can be seen, each Bq is structurally identical to the gram matrix defined for a single output neural
network with input weight matrix W and output weight vector aq (Equation 2).
0
Let's denote the set of C(C - 1) remaining disjoint non diagonal blocks of G as {Bq,q }q,q0∈[C],q6=q0,
where each block is defined as follows.
bqjq0 = m1 X ak,qak,q0φ (wTχi) φ (wTχj).
k∈[m]
Assuming that we have sufficient number of neurons, G(0) can be shown to be close to the matrix
G∞ using Hoeffding's inequality, where G∞ has the following diagonal {(B∞)q}C=ι and non
diagonal blocks {(B∞)q,q0}qC=1 defined as follows.
(B∞)qj = Ew〜N(o,i),a〜N(0,1) a2φ0 (wTXi) Φ0 (wTXj)，
(B∞)qjq0 = Ew〜N(0,i),a,a〜N(0,1) aaφ0 (WTXi) φ0 (WTxj).
Using independence of random gaussian variables a and a and random gaussian vector w, we
can show that (B∞)q,q are identically zero matrices. Also, the diagonal blocks (B∞)q are identi-
cally equal to the G∞ matrix defined for single output layer neural networks (Equation 2). Thus,
λmin(G∞) = λmin(G∞) and hence the bounds for eigenvalues of λmin(G(0)) can be derived
from the bounds for eigenvalues of λmin(G(0)), defined for a single output layer neural network
(Equation 2).
64
Published as a conference paper at ICLR 2020
M.4 Fine-grained analysis for smooth activation functions
In this section, we show the behavior of the loss function under gradient descent, in the low learning
rate setting considered by Du et al. (2019a), Du et al. (2019b) and Arora et al. (2019c). We consider
the neural network given by Equation 1.
We assume that the activation function φ satisfies the following properties.
•	φ∈C3,
•	φ is β-lipschitz and γ-smooth.
Now, we state some important theorems from Du et al. (2019b), that we will use for the future
analysis. There are some differences in our setting and the setting of Du et al. (2019b). a) Du et al.
(2019b) work with a general L layer neural network. Hence, we state their theorems for L = 1. b)
For simplicity of presentation, we have assumed that ak has been kept fixed during gradient descent,
which can be easily removed as in subsection M.2 and Du et al. (2019a).
Theorem M.5 (Lemma B.4 in Du et al. (2019b)). Assume that ∀i ∈ [n] ,|yi| = O (1) and
If we set step size as
η=O
，n (G ⑼)
n2
∖
then with probability at least 1 -
holds ∀t ∈ Z+.
κ over
wk(0)	, the following
Note that Du et al. (2019b) consider λmin (G∞) in their arguments. However, in the overparametrized
regime, with high probability with respect to {wk}km=1, λmin (G∞) and λmin G(0) differ only by
a constant factor, as given by lemma B.4 in Du et al. (2019b). Thus, we show their theorems using
λmin
Theorem M.6 (Lemma B.6 in Du et al. (2019b)). Assuming the setting in Theorem M.5, the following
holds ∀t ∈ Z+ and ∀k ∈ [m].
wk(t) - wk(0)	≤ O
n
√mλmin (G(O))
Theorem M.7. Assuming the setting in Theorem M.5, the following holds ∀t ∈ Z+.
≤O
Proof. We follow the proof of lemma B.5 of Du et al. (2019b). We will bound the change in
each element of the G-matrix and then, take a sum over all the elements to get a bound over the
65
Published as a conference paper at ICLR 2020
perturbation.
—
(Xi, Xj
m
m
m
m
k = 1
k = 1
m
1
+---
m
k=1
m
k=1
m
m
≤ 2γO ⑴ ||wk0)- wk" ≤O
Yn
√mλmin G(0))
where, We use Theorem M.6 and the fact that φ0 is Y-smooth and is bounded by O(1) in the final step.
Thus, we get
—
G⑼IIf ≤ t
2
Yn
√mλmin G(0))
as required.
□
Lemma M.8 (Claim 3.4 in Du et al. (2019a)). In the setting of Theorem M.5,
y - u(0)|| ≤ O
m
holds with probability at least 1 — K with respect to {wk0)}
k=1
Now, we state the following theorem, which is a simple adaptation of theorem 4.1 in Arora
et al. (2019c). Let v1, v2,…,Vn denote the eigenvectors of G(0), corresponding to its eigenval-
ues λ1,λ2, ∙∙∙,λn.
Theorem M.9. With probability at least 1 — K over the random initialization, Vt ∈ Z+, thefollowing
holds
y -叫=t
n	2t /	、 2
X (1-ηcφ%)	(vT (y -U(O))) ± e,
i=1
provided
n5
m ≥ Ω ------------------7i——
γκλmin (G(O))4 e2
and
η ≤O
^ʌmin (G(O))
cφ n2
∖
66
Published as a conference paper at ICLR 2020
Proof. For each i ∈ [n], we get
u(t+1) - u(t) =黑
2m n
-~mφXak	X (φ0 (wk)	Xi)φ	(Wk) χj))	(Uj)-yj)〈~,Xj +	ei	⑴
k=1	j=1
n
-ηc2φ X Gi(jt) (uj(t) - yj) + ei (t)
j=1
(66)
where we use Taylor expansion of φ in Equation 65. ei (t) denotes the error term due to truncated
Taylor expansion, whose norm can be bounded by
mn	2	2
≤ 2⅛η2o(i) XIX akφ0 (wkt)TXi)(Xi，Xj〉2 )∣∣y - u(t) Il
k=1	j=1
≤O (	…	!∣∣y -u(t)『
y pmb/ log m J ∣∣	∣∣
(67)
(68)
(69)
where we use the fact that φ00(z) ≤ O(1)∀z ∈ R in Equation 67, use the Cauchy-Schwartz
inequality in Equation 68 and use the fact that φ0(z) ≤ O(1)∀z ∈ R, Xi, Xj ≤ 1 ∀i, j ∈ [n]
and|ak| ≤ √log m with high probability in Equation 69. Thus, this gives
u(t+1) - u(t) = -ηc2φG(t) u(t) -y + e(t)
where
∣e(t)∣ = t
n
Xei(t)2≤O
i=1
p½) Iill2
Now, since G(t) is close to G(0), we can write
u(t+1) - u(t) = -ηc2φG(0) (u(t) - y) + τ(t)
(70)
where τ(t) = -ηc2φ G(t) - G(0)	u(t) - y + e(t). The norm of τ(t) can be bounded as follows.
∣τ(t)∣ ≤ ηc2φ
≤ ηc2φ
≤ O ηγ
≤O
√mλmin G(O))
ηγn2 c2φ
√mλmin G(O))
n2 c2φ
u(t)∣∣ + O( η"2
m/logm
67
Published as a conference paper at ICLR 2020
Thus, applying Equation 70 recursively, We get
t	t-1	t，
U⑴一y = (I- ηcφG(O)) (U(O)- y) + X (I- ηcφG⑼)T(t - 1 - t0).	(71)
t0 = 0
We bound the norm of each term in the above equation. The norm of the first term can be given as
folloWs.
—
i - ηcφ
(O) - y
i=1
-ηcφλi)	(vT (U(O)-y))
/	、t	∖ /	-
X (1 - ηcφλi) ViVi) (u() - y
i=1
(72)
NoW, the norm of the second term can be bounded as
t — 1	t，
Il X (i - ηcφG(O)) T(t -1 -1Z)II2
t0 = O
S
S
(73)
In Equation 73, We use the folloWing.
/]	ηcφλmin (G(O))
2
u(0) - y∣∣ ≤
']”Φλmin (G(O))
4
4
∖ ,
Thus, combining the tWo terms, We have
∖
≤ t
2t
i=1
4
ηcφλmin
1-----------
O
t-1
γηn5∕2cφ
√rnκλmi∏ G(O))
(74)
2
±O
γn5/2
√κmλmin (G(O))
(75)
n
2
± t
∖
I
,
{z
.
Where in Equation 74, We use the fact that
t
∖
ηcφλmin
1----------
4
t-1
≤ ηcφλmin (G(O)).
4
68
Published as a conference paper at ICLR 2020
Thus, for the term denoted by C to be less than e, We need
m ≥ Ω
γn5
Kλmin (G⑼)4 e2
□
M.5 PROOF FOR SGD
The folloWing theorem is an adaptation of Theorem 2 in Allen-Zhu et al. (2019), Which asserts fast
convergence of SGD for ReLU. The theorem beloW applies to activation in Jr for r ≥ 2; the case of
J1 can be handled by another adaptation of Theorem 2 in Allen-Zhu et al. (2019) Which We do not
discuss. Du et al. (2019b) analyzed gradient descent for this setting and mentioned the analysis of
SGD as a future Work.
Theorem M.10. Let St ⊆ [n] denote a randomly picked batch of size b. Let V(t) denote
nb Vw(t)L ^{(xi, yi)}i∈st ； a, W(t)). Let the activation function φ be a-Lipschitz and β-smooth.
The SGD iterate at time t + 1 is given by,
W(t+1) = W(t) - ηV(t)
Let η ≤ O (λmin (G(O)) β⅛Οτ) . If
m ≥ Ω
n6α4β2 log m
b2λmm (G(O))4
then,
y - u(t)2 ≤ e
fort ≥ Ω (b2入	))2βlog (nn))，WithProbabiIityat Ieast 1 一 e-Q(n2)
choice of St for t ≥ 0.
一 m-3.5 w.r.t. random
Proof. Note that
EStV(t) = VW(t)L ({(xi,yi)}i∈[n] ; a, W(t))
Using taylor expansion for each coordinate i, We have
ui(t+1) 一 ui(t)
=Ui (W(t) 一 ηV(t)) 一 Ui(W(t))
—
=一	DV⑴,ui(W㈤)E ds + /:
,I1i(t)+I2i(t)
V(t), Ui(W㈤)一 Ui (W㈤ 一 sV(t)),ds
(76)
Writing the decrease of loss at time t, We have
y 一 u(t+1)	=y 一 u(t) 一 (u(t+1) 一 u(t))
=y 一 u(t)	一 2(y 一 u(t))>(u(t+1) 一 u(t)) + u(t+1) 一 u(t)
=y 一 u(t)2 一 2(y 一 u(t))>I1 一 2(y 一 u(t))>I2 +u(t+1) 一 u(t)2	(77)
(78)
69
Published as a conference paper at ICLR 2020
where I1 ∈ Rn and its ith coordinate is given by I1i . Similarly, we define I2.
I1i is given as,
Ii = -η Dv(t),ui(w(t))E
=-ηb X (Ujt)-y) Duj(W⑴),ui(w⑴)〉	(79)
j∈St
,-ηnn X (u(t) - %) gijt	(80)
j∈St
where we use the definition of v(t) in Equation 79.
That implies,
klιk = ηnn G(t)D㈤(u(t) - y)|
≤ ηb忖1加-U叫
≤ ηnbnα2∣∣y - u(t)∣∣	(81)
where D(t) ∈ Rn×n denotes a diagonal matrix that has 1 in ith diagonal element, iff i ∈ St and 0
otherwise and kGk2 ≤ kGkF ≤ nL2, since φ is α-lipschitz.
Note that,
ESt(y - u(t))>Iι = ESt XX ηn (uu(tt) - yi) gj (uujt) - yj)
i∈[n] j∈St
=η(y-u(t))>G(t) (y - u(t))
≥ ηλmin (G(t))∣∣∣y-u(t)∣∣∣2	(82)
Also, we can bound I2 in the following manner.
I2i(t) ≤ η∣∣v(t)∣∣F 0m≤sa≤xη
u0i (W(t)) - ui0 W(t) - sv(t)
F
Since,
∣∣v(t) ∣∣F=( √m )2XX(yi-u(t)) ar φ0 (WT Xi) Xi
r=1∣i∈St
2
≤
and
u0i(W(t)) -u0i
2
2
1
2
2
m∣
X∣∣arβ sv(rt)T Xi Xi
r=1
≤ √m t
(83)
70
Published as a conference paper at ICLR 2020
where we use β-smoothness of the activation function φ in the first step, Pr ar2 ≤ 5m with high
probability andmaxr∈m] (▽¥)∣∣ 二(▽¥)( in 3rd step.
That implies,
∣I2(t)∣ ≤ βη2n3αb2∣∣y-U(Il ≤ βη2n4-U(Il	(84)
Also,
2
ui(W(t)) - ui
WTXi - S▽!)TXi
m∣
X∣∣arα	sV(rt)T Xi
r=1
2
1
≤ —
m
≤ α2η2 (mm X ar) mm』Vrt)∣∣2
r∈[m]
≤ α2η2 rm∈[amx]∣∣∣V(rt)∣∣∣22
≤ α2η2 (n1.5，|y-u(t)||)2
Hence, using Equation 77, Equation 81, Equation 84 and Equation 85, we get
∣∣∣y - U(t+1)∣∣∣2 =∣∣∣y - U(t)∣∣∣2 - 2(y -u(t))>I1 - 2(y - U(t))>I2 +∣∣∣U(t+1) - U(t)∣∣∣2
=(1 + 2ηn-L- + 2βη2n4-5 α + α η2n4! ∣∣y - u(t) f
= o((ι + 2η⅛2 )∣∣y"∣∣)
(85)
(86)
(87)
(88)
Taking log both the sides, we get
log (∣∣y - u(t+1)∣∣2) ≤ O (ηn~0~! + log (∣∣y - u(t)∣∣2
By azuma-hoeffding inequality, we have
≤ √tO
(89)
With probability at-least 1 — IWn2.
Also,
ESt∣∣∣y - U(t+1)∣∣∣- = ∣∣∣y - U(t)∣∣∣- - ESt2(y - U(t))>I1 - ESt2(y - U(t))>I-+ESt∣∣∣U(t+1) - U(t)∣∣∣-
≤ (l - ηλmin(G(t)) +2βη2n4.5αb- + αb-η2n4)∣∣y - u(t)∣∣	(90)
≤ (l - 1 ηλmin (G(O)) + 2βη2n4.51- + I-η2n4! ∣∣y - u(t)∣∣
≤(1- 4ηλmin (G(O)))∣∣y - u(t)∣∣2	(91)
71
Published as a conference paper at ICLR 2020
Where We use Equation 82, Equation 84 and Equation 85 in Equation 90.
Taking log both the sides, We get
y - u(t+1) ∣∣2) ≤ log (∣∣y - u(t) ∣∣2) - 1 ηλmin (G(O))
log
Using Jensen,s inequality, we get
Est log (∣∣ y - u(t+1) ∣∩ ≤ log (∣∣y - u(t) ∣∣ ) - ∣ηλmin (G(O))
(92)
Thus, for t ≥ 0, using Equation 89 and Equation 92, We get
log (∣ ∣ y- u(t)∣∣2) ≤ √tbO 0中)n+log (∣ ∣y-U(O) ∣∣2)- ω G `in
t
Hence, if t ≥ Ω
≤ log (∣ ∣ y - U(O) ∣∣2) - (jηλmin (G ⑼)Ω (√t) - O (r
+ θ (Jλmin ηG(0)) n-f)
≤ log (∣ ∣ y - U(O) ∣∣2) - (jηλmin (G ⑼)Ω (√t) - O (r
≤ log( ∣∣y -U(O) ∣∣2) -Z
≤ log(∣∣y -U(O) ∣∣2) -Z
t≥
t≥
79))2β log( ? )),wehave
2
飞	n3α2∖ ∖
λmin G(°) b
2
飞	n3α2∖ ∖
λmin G(°) b
n6α4
b2λmin (G(O))2
n6α4
b2λmin (G(O))2
b2λmin
βn6α4
Ω ηλmin
t+1
t+1
(93)
+ 1
log (∣∣y - u(t) ∣∩ ≤ log (O (n)) - Ω (log
≤ log (e)
implying ∣∣y - u(t) ∣∣ ≤ e. Also, let T0 =
steps T0, we get
∞
X ∣ ∣ y -U(t)|| ≤ 2T0O (√n)
t=0
which implies
6 4
n-a	2. Then, applying Equation 93 in chunks of
b2λmin(G(O))
+ 2T0
O(√n) + O(√n)
2	+
+ ... = O (√nT0)
(94)
∞
XWr
t=0
Xi
αn2ηʌ/log m
-b√m- 0
(95)
4
(96)
72
Published as a conference paper at ICLR 2020
where in Equation 95 We use the fact that maximum magnitude of a『is √log m with high probability.
Also, for J] G⑴-G(O)Il to be less than 1 λmin (G(O)), we need to have (Lemma M.11)
λmin (G(O))
4αβn
Thus, for both the conditions to hold true, we must have
m ≥ Ω
n6α4β2 log m
b2λmin (G(O))4
□
Lemma M.11. If activation function φ is α-lipschitz and β -smooth and
λmin (G(O)
4αβn
, ∀r ∈ [m], then
λmin(G(t)) ≥ 2λmin (G(O))
(97)
(t)	(O)
wr - wr
≤
Proof.
m
Igj)- gi(O)I = m X arφ0 (Wrt)T Xi) φ0 (Wrt)T Xj)- ar- (WrO)T Xi)φ(wrO)T Xj)
r=1
≤ — (X a) max φ0 (Wrt)TXi) φ0 (Wrt)TXj)- φ0 (WrO)TXi) φ0 (WrO)TXj)
≤ max IIIφ0 (W(rt)TXi) φ0 (Wr(t)T Xj ) - φ0 (Wr(O)T Xj )III
+IIIφ0 (Wr(O)T Xj ) φ0 (W(rt)TXi) - φ0 (Wr(O)TXi)III
≤ max 2αβIIWr(t) - Wr(O) II .	(98)
Hence,
IIIG(t)-G(O)IIIF≤rm∈[amx]2LβnIIIW(rt)-Wr(O)III
Since
λmin (G(t)) ≥ λmin (G(O)) -IIIG(t) -G(O)III ,
we have
λmin G(t) ≥ λmin G(O) - max 2αβnIIWr(t) - Wr(O) II .
ThgfOr λmin (G(t)) ≥ 1 λmin (G(O)) ,wehave
mHWrt)-WrO) b 4⅛λmin (G(O)).
□
73
Published as a conference paper at ICLR 2020
N Some basic facts ab out Hermite polynomials
For ρ ∈ [-1, 1] we say that the Gaussian random variable (v0, v1) is ρ-correlated if
(v0,v1) 〜N (0, ( P P )).
Fact N.1 (Proposition 11.31 in O’Donnell (2014)).
E(v0,v1) ρ-correlated Hen (v0) Hem (v1 )
Pn
0
if n = m,
otherwise.
where recall that Hen denotes the degree-n probabilists’ Hermite polynomial given by (10).
The following fact follows immediately from the previous one.
Fact N.2. For an activation function, define function R : R → R by
R(P) := E(v
0,v1)~ P - correlated φ(VO)φ(VI) .
Then,
∞
R(P) = X C (φ) ρa,
a=0
where Ca (φ) is the a -th coefficient in the Probabilists' Hermite expansion of φ. Thefunction satisfies
the following two properties.
•	R(P) ≤ R(|P|),
•	R(P) is increasing in (0, 1).
In the following we let Σ :=	PP00 PP01	.
Lemma N.3.
E(v0,v1)〜N (0,∑) Hen √ ʌ ! Hem ( ɪ) =( (√‰ Y if n = m，
ρ00 I ∖ √ρ11	0	otherwise.
Proof. The proof follows from the proof of Fact N.1, by using the r.v. (V1,V2), defined by
V0
V1
Vi
√ρ11
so that vector (V1,V2)〜N 0, Σ0 where Σ0 :
1
P P01
√P00P11
( P01
√P00P11
1
□
Lemma N.4.
Ev〜N(0,∑) φ
φ
∞a
X ca(φ) (√!⅛)
Proof.
Ev 〜N (0,∑)φ
∞∞
Ev 〜N (0,∑)ΣΣcCa(φ)cCa0 (φ)Hea
a=0a0=0
=X ca(φ) √ √^‰ )a=R (√^‰
M	∖√P00P11 /	∖√P0OP11
where we use Lemma N.3 and Fact N.2 in the final step.
□
74