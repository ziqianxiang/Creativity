Published as a conference paper at ICLR 2020
RENYI Fair Inference
Sina Baharlouei
Industrial and Systems Engineering, USC
baharlou@usc.edu
Maher Nouiehed
Industrial Engineering and Management, AUB
mn102@aub.edu.lb
Ahmad Beirami
EECS, MIT
beirami@mit.edu
Meisam Razaviyayn
Industrial and Systems Engineering, USC
razaviya@usc.edu
Ab stract
Machine learning algorithms have been increasingly deployed in critical auto-
mated decision-making systems that directly affect human lives. When these algo-
rithms are solely trained to minimize the training/test error, they could suffer from
systematic discrimination against individuals based on their sensitive attributes,
such as gender or race. Recently, there has been a surge in machine learning
society to develop algorithms for fair machine learning. In particular, several
adversarial learning procedures have been proposed to impose fairness. Unfortu-
nately, these algorithms either can only impose fairness up to linear dependence
between the variables, or they lack computational convergence guarantees. In
this paper, We use Renyi correlation as a measure of fairness of machine learning
models and develop a general training framework to impose fairness. In particu-
lar, We propose a min-max formulation Which balances the accuracy and fairness
When solved to optimality. For the case of discrete sensitive attributes, We sug-
gest an iterative algorithm With theoretical convergence guarantee for solving the
proposed min-max problem. Our algorithm and analysis are then specialized to
fair classification and fair clustering problems. To demonstrate the performance of
the proposed Renyi fair inference framework in practice, we compare it with well-
knoWn existing methods on several benchmark datasets. Experiments indicate that
the proposed method has favorable empirical performance against state-of-the-art
approaches.
1 Introduction
As we experience the widespread adoption of machine learning models in automated decision-
making, we have witnessed increased reports of instances in which the employed model results
in discrimination against certain groups of individuals - see Datta et al. (2015); Sweeney (2013);
Bolukbasi et al. (2016); Angwin et al. (2016). In this context, discrimination is defined as the un-
wanted distinction against individuals based on their membership to a specific group. For instance,
Angwin et al. (2016) present an example ofa computer-based risk assessment model for recidivism,
which is biased against certain ethnicities. In another example, Datta et al. (2015) demonstrate
gender discrimination in online advertisements for web pages associated with employment. These
observations motivated researchers to pay special attention to fairness in machine learning in recent
years; see Calmon et al. (2017); Feldman et al. (2015); Hardt et al. (2016); Zhang et al. (2018); Xu
et al. (2018); Dwork et al. (2018); Fish et al. (2016); Woodworth et al. (2017); Zafar et al. (2017;
2015); Perez-SUay et al. (2017); Bechavod & Ligett (2017); Liao et al. (2019).
In addition to its ethical standpoint, equal treatment of different groups is legally required by many
countries Act. (1964). Anti-discrimination laws imposed by many countries evaluate fairness by
notions such as disparate treatment and disparate impact. We say a decision-making process suf-
fers from disparate treatment if its decisions discriminate against individuals of a certain protected
group based on their sensitive/protected attribute information. On the other hand, we say it suffers
from disparate impact if the decisions adversely affect a protected group of individuals with cer-
tain sensitive attribute - see Zafar et al. (2015). In simpler words, disparate treatment is intentional
1
Published as a conference paper at ICLR 2020
discrimination against a protected group, while the disparate impact is an unintentional dispropor-
tionate outcome that hurts a protected group. To quantify fairness, several notions of fairness have
been proposed in the recent decade Calders et al. (2009); Hardt et al. (2016). Examples of these
notions include demographic parity, equalized odds, and equalized opportunity.
Demographic parity condition requires that the model output (e.g., assigned label) be independent of
sensitive attributes. This definition might not be desirable when the base ground-truth outcome of the
two groups are completely different. This shortcoming motivated the use of equalized odds notion
Hardt et al. (2016) which requires that the model output is conditionally independent of sensitive
attributes given the ground-truth label. Finally, equalized opportunity requires having equal false
positive or false negative rates across protected groups.
Machine learning approaches for imposing fairness can be broadly classified into three main
categories: pre-processing methods, post-processing methods, and in-processing methods. Pre-
processing methods modify the training data to remove discriminatory information before passing
data to the decision-making process Calders et al. (2009); Feldman et al. (2015); Kamiran & Calders
(2010; 2009; 2012); Dwork et al. (2012); Calmon et al. (2017); Ruggieri (2014). These methods map
the training data to a transformed space in which the dependencies between the class label and the
sensitive attributes are removed Edwards & Storkey (2015); Hardt et al. (2016); Xu et al. (2018);
Sattigeri et al. (2018); Raff & Sylvester (2018); Madras et al. (2018); Zemel et al. (2013); Louizos
et al. (2015). On the other hand, post-processing methods adjust the output of a trained classifier
to remove discrimination while maintaining high classification accuracy Fish et al. (2016); Dwork
et al. (2018); Woodworth et al. (2017). The third category is the in-process approach that enforces
fairness by either introducing constraints or adding a regularization term to the training procedure
Zafar et al. (2017; 2015); Perez-SUay et al. (2017); Bechavod & Ligett (2017); Berk et al. (2017);
Agarwal et al. (2018); Celis et al. (2019); Donini et al. (2018); Rezaei et al. (2019); Kamishima et al.
(2011); Zhang et al. (2018); Bechavod & Ligett (2017); Kearns et al. (2017); Menon & Williamson
(2018); Alabi et al. (2018). The Renyi fair inference framework proposed in this paper also belongs
to this in-process category.
Among in-processing methods, many add a regUlarization term or constraints to promote statistical
independence between the classifier oUtpUt and the sensitive attribUtes. To do that, varioUs inde-
pendence proxies sUch as mUtUal information Kamishima et al. (2011), false positive/negative rates
Bechavod & Ligett (2017), eqUalized odds Donini et al. (2018), Pearson correlation coefficient Zafar
et al. (2015; 2017), Hilbert Schmidt independence criterion (HSIC) Perez-SUay et al. (2017) were
Used. As will be discUssed in Section 2, many of these methods cannot captUre nonlinear depen-
dence between random variables and/or lead to compUtationally expensive algorithms. Motivated
by these limitations, We propose to use Renyi correlation to impose several known group fairness
measures. Renyi correlation captures nonlinear dependence between random variables. Moreover,
Renyi correlation is a normalized measure and can be computed efficiently in certain instances.
Using Renyi correlation coefficient as a regularization term, we propose a min-max optimization
framework for fair statistical inference. In particular, we specialize our framework to both classi-
fication and clustering tasks. We show that when the sensitive attribute(s) is discrete (e.g., gender
and/or race), the learning task can be efficiently solved to optimality, using a simple gradient ascent-
descent approach. We summarize our contributions next:
•	We introduce Renyi correlation as a tool to impose several notions of group fairness. Unlike
Pearson correlation and HSIC, which only capture linear dependence, Renyi correlation captures
any statistical dependence between random variables as zero Renyi correlation implies indepen-
dence. Moreover, it is more computationally efficient than the mutual information regularizers
approximated by neural networks.
•	Using Renyi correlation as a regularization term in training, we propose a min-max formulation
for fair statistical inference. Unlike methods that use an adversarial neural network to impose
fairness, we show that in particular instances such as binary classification, or discrete sensitive
variable(s), it suffices to use a simple quadratic function as the adversarial objective. This obser-
vation helped us to develop a simple multi-step gradient ascent descent algorithm for fair inference
and guarantee its theoretical convergence to first-order stationarity.
•	Our Renyi correlation framework leads to a natural fair classification method and a novel fair
K-means clustering algorithm. For K-means clustering problem, we show that sufficiently
2
Published as a conference paper at ICLR 2020
large regularization coefficient yields perfect fairness under disparate impact doctrine. Unlike
the two-phase methods proposed in Chierichetti et al. (2017); BackUrs et al. (2019); Rosner &
Schmidt (2018); Bercea et al. (2018); Schmidt et al. (2018), our method does not require any pre-
processing step, is scalable, and allows for regUlating the trade-off between the clUstering qUality
and fairness.
2 RENYI Correlation
The most widely Used notions for groUp fairness in machine learning are demographic parity, eqUal-
ized odds, and eqUalized opportUnities. These notions reqUire (conditional) independence between a
certain model oUtpUt and a sensitive attribUte. This independence is typically imposed by adding fair-
ness constraints or regUlarization terms to the training objective fUnction. For instance, Kamishima
et al. (2011) added a regUlarization term based on mUtUal information. Since estimating mUtUal in-
formation between the model oUtpUt and sensitive variables dUring training is not compUtationally
tractable, Kamishima et al. (2011) approximates the probability density fUnctions Using a logistic
regression model. To have a tighter estimation, Song et al. (2019) Used an adversarial approach
that estimates the joint probability density fUnction Using a parameterized neUral network. AlthoUgh
these works start from a well-jUstified objective fUnction, they end Up solving approximations of the
objective fUnction dUe to compUtational barriers. ThUs, no fairness gUarantee is provided even when
the resUlting optimization problems are solved to global optimality in the large sample size limit.
A more tractable measUre of dependence between two random variables is the Pearson correla-
tion. The Pearson correlation coefficient between the two random variables A and B is defined
as ρP (A, B)
Cov(A,B)
,where Cov(∙, ∙) denotes the covariance and Var(∙) denotes the
variance. The Pearson correlation coefficient is Used in Zafar et al. (2015) to decorrelate the binary
sensitive attribUte and the decision boUndary of the classifier. A major drawback of Pearson corre-
lation is that it only captUres linear dependencies between random variables. In fact, two random
variables A and B may have strong dependence bUt have zero Pearson correlation. This property
raises concerns aboUt the Use of the Pearson correlation for imposing fairness. Similar to the Pear-
son correlation, the HSIC measure proposed in Perez-SUay et al. (2017) may be zero even if the two
variables have strong dependencies. While Universal Kernels can be Used to alleviate this issUe, they
could arrive at the expense of computational intractability. In addition, HSIC is not a normalized
dependence measure Gretton et al. (2005b;a) which raises concerns about the appropriateness of
using it as a measure of dependence.
In this paper, We suggest to use HirSchfeId-GebeIein-Renyi correlation Renyi (1959); Hirschfeld
(1935); Gebelein (1941) as a dependence measure between random variables to impose fairness.
Renyi correlation, which is also known as maximal correlation, between two random variables A
and B is defined as
ρR(A,B)=supE[f(A)g(B)]	s.t. E[f(A)] = E[g(B)] =0,	E[f2(A)] = E[g2(B)] = 1,
f,g
(1)
where the supremum is over the set of measurable functions f (∙) and g(∙) satisfying the constraints.
Unlike HSIC and Pearson correlation, Renyi correlation captures higher-order dependencies be-
tween random variables. Renyi correlation between two random variables is zero if and only if the
random variables are independent, and it is one if there is a strict dependence between the variables
Renyi (1959). These favorable statistical properties of PR do not come at the price of computational
intractability as opposed to other measures such as mutual information. In fact, as we will discuss in
Section 3, ρR can be used in a computationally tractable framework to impose several group fairness
notions.
3 A General MIN-MAX Framework for Renyi FAIR Inference
Consider a learning task over a given random variable Z. our goal is to minimize the average
inference loss L(∙) where our loss function is parameterized with parameter θ. To find the optimal
3
Published as a conference paper at ICLR 2020
value of parameter θ with the smallest average loss, we solve the following optimization problem
min EL(θ, Z),
θ
where the expectation is taken over Z and possible regularization terms are absorbed in the loss
function L(∙). Notice that this formulation is quite general and can include regression, classification,
clustering, or dimensionality reduction tasks as special cases. As an example, in the case of linear
regression Z = (X, Y ) and L(θ , Z) = (Y - θTX)2 where X is a random vector and Y is the
random target variable.
Assume that, in addition to minimizing the average loss, we are interested in bringing fairness to
our learning task. Let S be a sensitive attribute (such as age or gender) and Yθ(Z) be a certain
output of our inference task using parameter θ. Assume we are interested in removing/reducing
the dependence between the random variable Yθ (Z) and the sensitive attribute S. To balance the
goodness-of-fit and fairness, one can solve the following optimization problem
min E[L(θ, Z)] + λρR(Ybθ (Z),S),	(2)
θ
where λ is a positive scalar balancing fairness and goodness-of-fit. Notice that the above framework
is quite general. For example, Yθ may be the assigned label in a classification task, the assigned
cluster in a clustering task, or the output of a regressor in a regression task.
Using the definition of Renyi correlation, We can rewrite optimization problem in equation 2 as
min SUp E[L(θ, Z)] + λ(E[f (Ybθ(Z)) g(S)])2,
θ f,g	(3)
s.t. E[f(Ybθ(Z))] =E[g(S)] =0, E[f2(Ybθ(Z))] =E[g2(S)] = 1,
where the supremum is taken over the set of measurable functions. The next natural question to ask
is whether this optimization problem can be efficiently solved in practice. This question motivates
the discussions of the following subsection.
3.1 Computing RENYI Correlation
The objective function in equation 3 may be non-convex in θ in general. Several algorithms have
been recently proposed for solving such non-convex min-max optimization problems Sanjabi et al.
(2018); Nouiehed et al. (2019); Jin et al. (2019). Most of these methods require solving the inner
maximization problem to (approximate) global optimality. More precisely, we need to be able to
solve the optimization problem described in equation 1. While popular heuristic approaches such
as parameterizing the functions f and g with neural networks can be used to solve equation 1, we
focus on solving this problem in a more rigorous manner. In particular, we narrow down our focus
to the discrete random variable case. This case holds for many practical sensitive attributes among
which are the gender and race. In what follows, we show that in this case, equation 1 can be solved
“efficiently” to global optimality.
Theorem 3.1	(Witsenhausen (1975)). Let a ∈ {a1, . . . , ac} and b ∈ {b1, . . . , bd} be two discrete
random variables. Then the Renyi coefficient ρn(a, b) is equal to the Second largest singular value
of the matrix Q = [qij]i,j ∈ Rc×d, where qij
P(a=ai,b=bj)
√P(a=ai)P(b=bj )
The above theorem provides a computationally tractable approach for computing the Renyi coeffi-
cient. This computation could be further simplified when one of the random variables is binary.
Theorem 3.2.	Suppose that a ∈ {1, . . . , c} is a discrete random variable and b ∈ {0, 1} is a
binary random variable. Let a be a one-hot encoding of a, i.e., a = ei if a = i, where ei 二
(0,..., 0,1, 0 ..., 0) is the i-th standard unit vector. Let b = b 一 1/2. Then,
PR(a,b) = 1- - P(b = ιγp(b = 0),
where γ , minw∈Rc
. Equivalently,
cc
Y , min	^X w2P(a	=	i)	一	^X Wi(P(a	= i, b = 1) 一 P(a	= i, b = 0)) +	1/4.
w∈Rc	i=1	i=1
4
Published as a conference paper at ICLR 2020
Proof. The proof is relegated to the appendix.
□
Let us specialize our framework to classification and clustering problems in the next two sections.
4 RENYI Fair Classification
In a typical (multi-class) classification problem, we are given samples from a random variable Z ,
(X, Y ) and the goal is to predict Y from X. Here X ∈ Rd is the input feature vector, and Y ∈
Y , {1, . . . , c} is the class label. Let Ybθ be the output of our classifier taking different values in the
set {1, . . . , c}. Assume further that
P(Ybθ = i | X) = Fi(θ,X), ∀i = 1,...,c.
Here θ is that parameter of the classifier that needs to be tuned. For example, F(θ, X) =
(F1 (θ, X), . . . , Fc(θ, X)) could represent the output of a neural network after softmax layer; the
soft probability label assigned by a logistic regression model; or the 0-1 probability values obtained
by a deterministic classifier. in order to find the optimal parameter θ, we need to solve the optimiza-
tion problem
min E L(F(θ, X),Y) ,	(4)
θ
where L is the loss function and the expectation is taken over the random variable Z = (X, Y). Let
S be the sensitive attribute. We say a model satisfies demographic parity if the assigned label Yb is
independent of the sensitive attribute S, see Dwork et al. (2012). Using our regularization frame-
work, to find the optimal parameter θ balancing classification accuracy and fairness objective, we
need to solve
min E L(F(θ, X),Y) + λρR(Yθ,S).
θ
(5)
4.1 General Discrete Case
When S ∈ {s1, . . . , sd} is discrete, Theorem 3.1 implies that equation 5 can be rewritten as
min max
θ v⊥v1, kvk2≤1
fD(θ, V) , E L(F(θ, X), Y) + λvτQTQθV .
(6)
Here V1
IpP(S = si),..., pp(s=sd)] ∈ Rd is the right singular vector corresponding to
the largest singular value of	Qθ	=	[qj]ij ∈	Rc×d,	with	qj，P(Yθ	= i | S =	Sj)P(S = Sj).
P(Ybθ =i)P(S= sj)
Given training data (xn, yn)nN=1 sampled from the random variable Z = (X, Y), we can estimate
the entries of the matrix Qθ using P(Yθ = i) = E[P(Yθ = i | X)] ≈ EPnN=I Fi(θ, Xn),
and P(Yθ = i |S = Sj) ≈i Px∈ Px∈χ Fi(θ, x), where Xj is the set of samples with sensitive
attribute Sj. Motivated by the algorithm proposed in Jin et al. (2019), we present Algorithm 1 for
solving equation 6.
Algorithm 1 Renyi Fair Classifier for Discrete Sensitive Attributes
1
2
3
4
5
Input: θ0 ∈ Θ, step-size η.
for t = 0, 1, . . . , T do
Set vt+1 一 maxv∈⊥v1,kvk≤1 f0(θt, V) by finding the second singular vector of Qθt
Set θt+1 - θt - ηVθfD(θt, vt+1)
end for
To understand the convergence behavior of Algorithm 1 for the nonconvex optimization prob-
lem equation 6, we need to first define an approximate stationary solution. Let us define g(θ) =
5
Published as a conference paper at ICLR 2020
maxv∈⊥v1,kvk≤1 f (θ, v). Assume further that f (∙, V) has LI-LiPschitz gradient, then g(∙) is L∖-
weakly convex; for more details check Rafique et al. (2018). For such weakly convex function, we
Say θ* is a E-Stationary solution if the gradient of its MoreaU envelop is smaller than epsilon, i.e.,
∣∣Vgβ(∙)k ≤ ε with gβ(θ)，mine，g(θ0) + ɪkθ - θ0k and β < is a given constant. The fol-
lowing theorem demonstrates the convergence of Algorithm 1. This theorem is a direct consequence
of Theorem 27 in Jin et al. (2019).
Theorem 4.1. Suppose that f is L0-Lipschitz and L1-gradient Lipschitz. Then Algorithm 1 com-
putes an ε-stationary solution of the objective function in equation 6 in O(ε-4) iterations.
4.2 Binary Case
When S is binary, we can obtain a more efficient algorithm compared to Algorithm 1 by exploiting
Theorem 3.2. Particularly, by a simple scaling ofλ and ignoring the constant terms, the optimization
problem in equation 5 can be written as
minmax f(θ, V)，E[L(F(θ, X),Y)] - λ[ PC=1 w2p(Yθ = i)
-PC=ι Wi (P(YJ = i,S = 1) — P(YJ = i,S = O))].
1	ʌ i'	K rʌ r> 1 .t	f	FI	f	∙..
Defining S = 2S - 1, the above problem can be rewritten as
(7)
cc
min max E[L(F(θ, X), Y) - λ X wi2Fi(θ, X) + λ X wiSeFi(θ, X)].
J w	i=1	i=1
Thus, given training data (xn, yn)nN=1 sampled from the random variable Z = (X, Y), we solve
1N	c	c
min max fB (θ, W)，N X[L(F (θ, Xn),yn) — λ X Wi2Fi(θ, Xn) 十 λ Xwi sen Fi (θ , xn )] .
n=1	i=1	i=1	(8)
Notice that the maximization problem in equation 8 is concave, separable, and has a closed-form
solution. We propose Algorithm 2 for solving equation 8.
Algorithm 2 Renyi Fair Classifier for Binary Sensitive Attributes
1:	Input: θ0 ∈ Θ, step-size η.
2:	for t = 0, 1, . . . , T do
3:	Set wt+1 - argmaxw fb (θt, w), i.e., set wt+1 - PP=N SnFi(θ XCn)), ∀i = 1,..., c
4:	Set θt+1 J θt — ηVθfB (θt, wt+1)	n=1	…
5:	end for
While the result in Theorem 4.1 can be applied to Algorithm 2, under the following assumption, we
can show a superior convergence rate.
Assumption 4.1. We assume that there exists a constant scalar μ > 0 such that PN=I Fi(θ, Xn) ≥
μ, ∀i = 1,...,C.	一
This assumption is reasonable when soft-max is used. This is because we can always assume θ lies
in a compact set in practice, and hence the output of the softmax layer cannot be arbitrarily small.
Theorem 4.2. Suppose that f is L1 -gradient Lipschitz. Then Algorithm 2 computes an ε-stationary
solution of the objective function in equation 8 in O(ε-2 ) iterations.
Proof. The proof is relegated to the appendix.
□
Notice that this convergence rate is clearly a faster rate than the one obtained in Theorem 4.1. More-
over, this rate of convergence matches the oracle lower bound for general non-convex optimization;
6
Published as a conference paper at ICLR 2020
see Carmon et al. (2019). This observation shows that the computational overhead of imposing fair-
ness is negligible as compared to solving the original non-convex training problem without imposing
fairness.
Remark 4.3 (Extension to multiple sensitive attributes). Our discrete Renyi classification frame-
work can naturally be extended to the case of multiple discrete sensitivity attributes by concatenat-
ing all attributes into one. For instance, when we have two sensitivity attribute S1 ∈ {0, 1} and
S2 ∈ {0, 1}, we can consider them as a single attribute S ∈ {0, 1, 2, 3} corresponding to the four
combinations of {(S1 =0,S2 =0),(S1 =0,S2 = 1),(S1 =0,S2 =0),(S1 = 1,S2 = 1)}.
Remark 4.4 (Extension to other notions of fairness). Our proposed framework imposes the demo-
graphic parity notion of group fairness. However, other notions of group fairness may be represented
by (conditional) independence conditions. For such cases, we can again apply our framework. For
example, we say a predictor Yθ satisfies equalized odds condition if the predictor Yθ is conditionally
independent of the sensitive attribute S given the true label Y. Similar to formulation equation 5,
the equalized odds fairness notion can be achieved by the following min-max problem
min E L(F (θ, X),Y) + λ X PR 伍,S∣ Y = y).
y∈Y
(9)
5 RENYI Fair Clustering
In this section, We apply the proposed fair Renyi framework to the widespread K-means clustering
problem. Given a set of data points x1, . . . , xN ∈ RN×d, in the K-means problem, we seek to
partition them into K clusters such that the following objective function is minimized:
NK	K
min	aknkxn	- ckk2	s.t.	akn	=	1,	∀n,	akn	∈	{0,	1},	∀k,	n (10)
, n=1 k=1	k=1
where ck is the centroid of cluster k; the variable akn = 1 if data point xn belongs to cluster k
and it is zero otherwise; A = [akn]k,n and C = [c1, . . . , cK] represent the association matrix and
the cluster centroids respectively. Now, suppose we have an additional sensitive attribute S for each
one of the given data points. In order to have a fair clustering under disparate impact doctrine, we
need to make the random variable an = [a1n, . . . , aKn] independent of S. In other words, we need
to make the clustering assignment independent of the sensitive attribute S. Using our framework
in equation 2, we can easily add a regularizer to this problem to impose fairness under disparate
impact doctrine. In particular, for binary sensitive attribute S, using Theorem 3.2, and absorbing the
constants into the hyper-parameter λ, we need to solve
NK
min max	aknkxn - ckk2
A,C w∈RK	n n
n=1 k=1
s.t.	Pk=1 akn = 1, ∀n,
N
λ X(aTn w - sn)2
—
(11)
n=1
akn ∈
∀k, n.
where an = (a1n, . . . , aKn)T encodes the clustering information of data point xn and sn is the
sensitive attribute for data point n.
Fixing the assignment matrix A, and cluster centers C, the vector w can be updated in closed-form.
More specifically, wk at each iteration equals to the current proportion of the privileged group in
the k-th cluster. Combining this idea with the update rules of assignments and cluster centers in the
standard K-means algorithm, we propose Algorithm 3, which is a fair K-means algorithm under
disparate impact doctrine. To illustrate the behavior of the algorithm, a toy example is presented in
Appendix C.
The main difference between this algorithm and the popular K-means algorithm is in Step 6 of
Algorithm 3. This step is a result of optimizing equation 11 over A when both C and w are fixed.
When λ = 0, this step would be identical to the update of cluster assignment variables in K-
means. However, when λ > 0, Step 6 considers fairness when computing the distance considered in
updating the cluster assignments.
7
Published as a conference paper at ICLR 2020
Algorithm 3 Renyi Fair K-means
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
Input: X = {x1 , . . . , xN} and S = {s1 , . . . , sN}
Initialize: Random assignment A s.t. PkK=1 akn
while Aprev 6= A do
Set Aprev = A
for n = 1, . . . , N do
k = argmi□k IlXn - Ck∣∣2 - λ(wk - Sn)2
Set ak*n = 1 and akn = 0 for all k = k*
PN
Set Wk = TnN Snakn, ∀k = 1,...,K.
n=1 akn
end for
PN
Set Ck = JN aknxn, ∀k = 1,...,K.
n=1 akn
1 ∀n; and akn ∈ {0, 1}. Set Aprev = 0.
. Update A
. Update w
. Update C
end while
Remark 5.1. Note that in Algorithm 3, the parameter w is being updated after each assignment of
a point to a cluster. More specifically, for every iteration of the algorithm, w is updated N times. If
we otherwise update w after completely updating the matrix A, then with a simple counterexample
we can show that the algorithm can get stuck; see more details in Appendix C.1.
6	Numerical Experiments
In this section, We evaluate the performance of the proposed Renyi fair classifier and Renyi fair k-
means algorithm on three standard datasets: Bank, German Credit, and Adult datasets. The detailed
description of these datasets is available in the supplementary material.
We evaluate the performance of our proposed Renyi classifier under both demographic parity, and
equality of opportunity notions. We have implemented a logistic regression classifier regular-
ized by Renyi correlation on Adult dataset considering gender as the sensitive feature. To mea-
sure the equality of opportunity We use the Equality of Opportunity (EO) violation, defined as
EO Violation = ∣P(Y = 1|S = 1,Y = 1) - P(Y = 1|S = 0,Y = 1)∣, where Y and Y
represent the predicted, and true labels respectively. Smaller EO violation corresponds to a more
fair solution. Figure 1, parts (a) and (b) demonstrate that by increasing λ, the Renyi regularizer co-
efficient decreases implying a more fair classifier at the price ofa higher training and testing errors.
Figure 1, part (c) compares the fair Renyi logistic regression model to several existing methods in
the literature Hardt et al. (2016); Zafar et al. (2015); Rezaei et al. (2019); Donini et al. (2018). As we
can see in plot (c), the Renyi classifier outperforms other methods in terms of accuracy for a given
level of EO violation.
The better performance of the Renyi fair classifier compared to the baselines could be attributed
to the following. Hardt et al. (2016) is a post-processing approach where the output of the classi-
fication is modified to promote a fair prediction. This modification is done without changing the
classification process. Clearly, this approach limits the design space and cannot explore the possibil-
ities that can reach with “in-processing” methods where both fairness and classification objectives
are optimized jointly. Zafar et al. (2015) imposes fairness by using linear covariance and thus can
only capture linear dependence between the predictor and the sensitive attribute. Consequently,
there might exist nonlinear dependencies which are revealed in fairness measures such as DP or EO
violation. Rezaei et al. (2019); Donini et al. (2018), on the other hand, propose to use nonlinear
measures of dependence as regularizers. However, due to computational barriers, they approximate
the regularizer and solve the approximate problem. The approximation step could potentially have
an adverse effect on the performance of the resulting classifier. Notice that while these methods are
different in terms of the way they impose fairness, they are all implemented for logistic regression
model (with the exception of SVM model used in Donini et al. (2018)). Thus, the difference in the
performance is not due to the classification model used in the experiments.
To show the practical benefits of Renyi correlation over Pearson correlation and HSIC regulariz-
ers under the demographic parity notion, we evaluate the logistic regression classifier regularized
8
Published as a conference paper at ICLR 2020
by these three measures on Adult, Bank, and German Credit datasets. For the first two plots,
^	^
we use p% = mm(P；0_/s_o；，P；/—Js-i；)as a measure of fairness. Since p% is defined only
for binary sensitive variables, for the last two plots in Figure 2 (German dataset with gender and
marital status, and Adult dataset with gender and race as the sensitive features), we use the in-
verse of demographic parity (DP) violation as the fairness measure. We define DP violation as
DP Violation = maxa,b∣P(Y = 1|S = a) - P(Y = 1|S = b)∣. As it is evident from the figure,
Renyi classifier outperforms both HSIC and Pearson classifiers, especially when targeting high lev-
els of fairness. For the last two experiments in Figure 2, we could not further increase fairness by
increasing the regularization coefficient for Pearson and HSIC regularizers (see green and red curves
cannot go beyond a certain point on the fairness axis). This can be explained by the nonlinear corre-
lation between the predictor and the sensitive variables in these two scenarios which cannot be fully
captured using linear or quadratic independence measures. Interestingly, our experiments indicate
that minimizing Renyi correlation eventually minimizes the Normalized Mutual Information (NMI)
between the variables (See Supplementary Figure 6). Recall that similar to Renyi correlation, NMI
can capture any dependence between two given random variables.
Finally, to evaluate the performance of our fair k-means algorithm, we implement Algorithm 3 to
find clusters of Adult and Bank datasets. We use the deviation of the elements of the vector w as
a measure of fairness. The element wk of w represents the ratio of the number of data points that
belong to the privileged group (S = 1) in cluster k over the number of data points in that cluster. This
notion of fairness is closely related to minimum balance introduced by Chierichetti et al. (2017). The
deviation of these elements is a measure for the deviation of these ratios across different clusters. A
clustering solution is exactly fair if all entries of w are the same. For K = 14, we plot in Figure 3
the minimum, maximum, average, and average ± standard deviation of the entries of w vector for
different values of λ. For an exactly fair clustering solution, these values should be the same. As we
can see in Figure 3, increasing λ yields exact fair clustering at the price of a higher clustering loss.
• Hardt
X Donihl
Figure 1: Trade-off between the accuracy of classifier and fairness on the adult dataset under the
equality of opportunity notion. (a, b) By increasing λ from 0 to 1000, EO violation (the blue curve
on the left axis) approaches to 0. The fairer solution comes at the price of a slight increase of the
training/test error (Red curve, right axis). (C) Comparison of the existing approaches With Renyi
classifier, under the equality of opportunity notion. Renyi classifier demonstrates a better accuracy
for a given level of fairness measured by EO violation.
7	Conclusion
In this paper, we proposed Renyi fair inference as an in-process method to impose fairness in em-
pirical risk minimization. Fairness is defined as (conditional) independence betWeen a sensitive
attribute and the inference output from the learning machine. As statistical independence is only
measurable when the data distributions are fully known, we can only hope to promote independence
through empirical surrogates in this framework. Our method imposes a regularizer in the form of
the Renyi correlation (maximal correlation) between a sensitive attribute(s) and the inference output.
Renyi correlation between two random variables is zero if and only if they are independent, which
is a desirable property for an independence surrogate. We pose Renyi fair correlation as a minimax
optimization problem. In the case where the sensitive attributes are discrete (e.g., race), we present
an algorithm that finds a first-order optimal solution to the problem with convergence guarantees. In
the special case where the sensitive attribute is binary (e.g., gender), we show an algorithm with op-
timal convergence guarantees. Our numerical experiments show that Renyi fair inference captures
9
Published as a conference paper at ICLR 2020
nonlinear correlations better than Pearson correlation or HSIC. We also show that increasing the
regularization hyperparameter results in near statistical independence between the sensitive attribute
and the inference output. Future work would naturally consider extension to continuous sensitive
attributes and problems with missing or non-explicit sensitive labels such as fair word embedding
problem.
(％) AU2rDU‹
(％) Au)rm<

1/(DPVioIation)	1/(DP Violation)
Figure 2: Trade-off between accuracy and fairness for logistic regression classifier regularized with
Renyi, HSIC, and Pearson measures, on German Credit, Adult, and Bank datasets. (Top) The drop
in the accuracy of the model regularized by Renyi, is less than the same model regularized by
HSIC, and Pearson correlation. Moreover, as can be observed for both Bank and Adult datasets,
Pearson and HSIC regularizers usually cannot increase p% beyond a certain limit, due to the fact
that removing all linear correlations does not guarantee independence between the predictor and
the sensitive attribute. (Down) When the sensitive attribute is not binary (or we have more than
one sensitive attribute), obtaining a fair model for HSIC and Pearson regularizers is even harder.
The model regularized by HSIC or Pearson, cannot minimize the DP violation (or maximize its
reciprocal) beyond a threshold.
(％) dno.10Pθ6θ->μd əfoUo=-IOd0」d
LOSS
0 5 0 5 0 5
6 5 5 4 4 3
80706050
(％) dno-CDPφ6θ=>,ldφf Jo Uo=-Iodo.Id
Figure 3: Performance and fairness of K-means algorithm in terms of Renyi regularizer hyper-
parameter λ. By increasing λ, the standard deviation of the w vector components (each component
represents the relative proportion of the privileged group in the corresponding cluster) is reduced
accordingly. Both plots demonstrate that the standard deviation of w is reduced fast with respect to
λ, and the increase in loss is small when λ ≤ 0.005. However, to reach a completely fair clustering,
a λ ≥ 1 must be chosen that can increase the loss (the right axis, red curve) drastically.
10
Published as a conference paper at ICLR 2020
References
Civil Rights Act. Civil rights act of 1964, title vii,equal employment opportunities. 1964.
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reduc-
tions approach to fair classification. arXiv preprint arXiv:1803.02453, 2018.
Daniel Alabi, Nicole Immorlica, and Adam Kalai. Unleashing linear optimizers for group-fair learn-
ing and optimization. In Conference On Learning Theory, pp. 2043-2066, 2018.
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. ProPublica, 2016.
Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scal-
able fair clustering. arXiv preprint arXiv:1902.03519, 2019.
Yahav Bechavod and Katrina Ligett. Penalizing unfairness in binary classification. arXiv preprint
arXiv:1707.00044, 2017.
Ioana O Bercea, Martin Groβ, Samir Khuller, Aounon Kumar, Clemens Rosner, Daniel R
Schmidt, and Melanie Schmidt. On the cost of essentially fair clusterings. arXiv preprint
arXiv:1811.10319, 2018.
Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgen-
stern, Seth Neel, and Aaron Roth. A convex framework for fair regression. arXiv preprint
arXiv:1706.02409, 2017.
Dimitri P Bertsekas. Control of uncertain systems with a set-membership description of the uncer-
tainty. PhD thesis, Massachusetts Institute of Technology, 1971.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is
to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances
in neural information processing systems, pp. 4349-4357, 2016.
Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency
constraints. In 2009 IEEE International Conference on Data Mining Workshops, pp. 13-18.
IEEE, 2009.
Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R
Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Infor-
mation Processing Systems, pp. 3992-4001, 2017.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points i. Mathematical Programming, pp. 1-50, 2019.
L Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K Vishnoi. Classification with fairness
constraints: A meta-algorithm with provable guarantees. In Proceedings of the Conference on
Fairness, Accountability, and Transparency, pp. 319-328. ACM, 2019.
Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through
fairlets. In Advances in Neural Information Processing Systems, pp. 5029-5037, 2017.
John M Danskin. The theory of max-min and its application to weapons allocation problems, vol-
ume 5. Springer Science & Business Media, 1967.
Amit Datta, Michael Carl Tschantz, and Anupam Datta. Automated experiments on ad privacy
settings. Proceedings on privacy enhancing technologies, 2015(1):92-112, 2015.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil. Em-
pirical risk minimization under fairness constraints. In Advances in Neural Information Process-
ing Systems, pp. 2791-2801, 2018.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214-226. ACM, 2012.
11
Published as a conference paper at ICLR 2020
Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Max Leiserson. Decoupled classifiers
for group-fair and efficient machine learning. In Conference on Fairness, Accountability and
Transparency,pp. 119-133, 2018.
Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint
arXiv:1511.05897, 2015.
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 259-268. ACM, 2015.
Benjamin Fish, Jeremy Kun, and Adam D Lelkes. A confidence-based approach for balancing fair-
ness and accuracy. In Proceedings of the 2016 SIAM International Conference on Data Mining,
pp. 144-152. SIAM, 2016.
Hans Gebelein. Das statistische problem der korrelation als variations-und eigenwertproblem und
sein zusammenhang mit der ausgleichsrechnung. ZAMM-Journal of Applied Mathematics and
Mechanics/Zeitschrift fUr Angewandte Mathematik UndMechanik, 21(6):364-379, 1941.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical de-
pendence with hilbert-schmidt norms. In International conference on algorithmic learning theory,
pp. 63-77. Springer, 2005a.
Arthur Gretton, RalfHerbrich, Alexander Smola, Olivier Bousquet, and Bernhard Scholkopf. Kernel
methods for measuring independence. Journal of Machine Learning Research, 6(Dec):2075-
2129, 2005b.
Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In
Advances in neural information processing systems, pp. 3315-3323, 2016.
Hermann O Hirschfeld. A connection between correlation and contingency. In Mathematical Pro-
ceedings of the Cambridge Philosophical Society, volume 31, pp. 520-524. Cambridge University
Press, 1935.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Minmax optimization: Stable limit points of
gradient descent ascent are locally optimal. arXiv preprint arXiv:1902.00618, 2019.
Faisal Kamiran and Toon Calders. Classifying without discriminating. In 2009 2nd International
Conference on Computer, Control and Communication, pp. 1-6. IEEE, 2009.
Faisal Kamiran and Toon Calders. Classification with no discrimination by preferential sampling.
In Proc. 19th Machine Learning Conf. Belgium and The Netherlands, pp. 1-6. Citeseer, 2010.
Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrim-
ination. Knowledge and Information Systems, 33(1):1-33, 2012.
Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regular-
ization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp.
643-650. IEEE, 2011.
Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymander-
ing: Auditing and learning for subgroup fairness. arXiv preprint arXiv:1711.05144, 2017.
Jiachun Liao, Oliver Kosut, Lalitha Sankar, and Flavio du Pin Calmon. Tunable measures for infor-
mation leakage and applications to privacy-utility tradeoffs. IEEE Transactions on Information
Theory, 2019.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair
autoencoder. arXiv preprint arXiv:1511.00830, 2015.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. arXiv preprint arXiv:1802.06309, 2018.
12
Published as a conference paper at ICLR 2020
Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classification. In
Conference on Fairness, Accountability and Transparency, pp. 107-118, 2018.
Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
Maher Nouiehed, Maziar Sanjabi, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-
convex min-max games using iterative first order methods. arXiv preprint arXiv:1902.08297,
2019.
Adrian Perez-Suay, Valero Laparra, Gonzalo Mateo-Garcla, Jordi Mufioz-Marl, LUis Gomez-Chova,
and Gustau Camps-Valls. Fair kernel learning. In Joint European Conference on Machine Learn-
ing and Knowledge Discovery in Databases, pp. 339-355. Springer, 2017.
Edward Raff and Jared Sylvester. Gradient reversal against discrimination: A fair neural network
learning approach. In 2018 IEEE 5th International Conference on Data Science and Advanced
Analytics (DSAA), pp. 189-198. IEEE, 2018.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060,
2018.
Alfred Renyi. On measures of dependence. ACta mathematica hungarica, 10(3-4):441451, 1959.
Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian Ziebart. Fair logistic regression: An
adversarial perspective. arXiv preprint arXiv:1903.03910, 2019.
Clemens Rosner and Melanie Schmidt. Privacy preserving clustering with constraints. arXiv
preprint arXiv:1802.02497, 2018.
Salvatore Ruggieri. Using t-closeness anonymity to control for non-discrimination. Trans. Data
Privacy, 7(2):99-129, 2014.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and ro-
bustness of training gans with regularized optimal transport. In Advances in Neural Information
Processing Systems, pp. 7091-7101, 2018.
Prasanna Sattigeri, Samuel C Hoffman, Vijil Chenthamarakshan, and Kush R Varshney. Fairness
gan. arXiv preprint arXiv:1805.09910, 2018.
Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algo-
rithms for fair k-means clustering. arXiv preprint arXiv:1812.10854, 2018.
Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning
controllable fair representations. arXiv preprint arXiv:1812.04218, 2019.
Latanya Sweeney. Discrimination in online ad delivery. arXiv preprint arXiv:1301.6822, 2013.
Hans S Witsenhausen. On sequences of pairs of dependent random variables. SIAM Journal on
Applied Mathematics, 28(1):100-113, 1975.
Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-
discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.
Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. Fairgan: Fairness-aware generative adver-
sarial networks. In 2018 IEEE International Conference on Big Data (Big Data), pp. 570-575.
IEEE, 2018.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness constraints: Mechanisms for fair classification. arXiv preprint arXiv:1507.05259, 2015.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th International Conference on World Wide Web, pp.
1171-1180. International World Wide Web Conferences Steering Committee, 2017.
13
Published as a conference paper at ICLR 2020
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp.
335-340. ACM, 2018.
14
Published as a conference paper at ICLR 2020
A Appendix: Proof of Theorem 3.2
Proof. First, notice that since a is a one-hot encoding of a, any function f : {1,...,c}→ R can be
equivalently represented as f (a) = UTa for some U ∈ Rc. Therefore, following the definition of
Renyi correlation, We can write
ρR(a, b) = max
u,g
s.t.
E (UT ae)g(b)
E [(uta)2] ≤ 1,	E[uta] = 0
E[g2(b)] ≤ 1,	E[g(b)] =0
Notice that since b is binary, there is a unique function g(b) = √ b-q)satisfying the constraints
where q , P(b = 1). Therefore, the above optimization problem can be written as
ρR (a, b) = max UTE [aeg(b)]
u
s.t. UTE [aaτ] u ≤ ι
UT E[a] = o
The last constraint simply implies that U should be orthogonal to P，E[a], which is a stochastic
vector capturing the distribution of a. Equivalently, we can write U =(I - PPT2) V for some
v ∈ Rc . Thus, we can simplify the above optimization problem as
ρR(a, b) = max
v
s.t.
VT (I-PpT2) E[eg(b)]
PP∣2) diag(p) (I-
PpT )
V≤1,
where in the constraint, we used the equality E [aaT] = diag(p), which follows the definition.
T
Let us do the change of variable V = diag(√p)(I - Ppɪ)v. Then the above optimization can be
simplified to
PR(a, b) = max VTdiag(1∕√p)E [ag(b)]
V
s.t. ∣∣vIl ≤ 1.
Clearly, this leads to
2
ρ2R(a, b) = diag
E [eag(b)]
E p⅛)(p(a=i,b=ι)rzF - p(a，…SI)
(12)
where in the last equality we use the fact that g(1) = J 1-q and g(0) = - jɪ-q. Define pio，
(a = i, b = 0) and pi1 ,P(a = i, b = 0), pi ,P(a = i) = pi0 + pi1. Then, using simple
15
Published as a conference paper at ICLR 2020
algebraic manipulations, we have that
2
c
X
i=1
(2pii(1 - q - 2pi0q)2
4piq(i - q
- pi0	1
^X (PiO - PiI)2
=4piq(i - q)
c
+X
i=1
(PiO - Pi1)
4piq(i - q
Σ
i=1
((3 - 2q)pi1 - (I + 2q)PiO)((I - 2q)PiI + (I - 2q)PiO)
4piq(i - q
c
+X
i=1
So - PiIy2
4Piq(i - q
c
4q(1 -qq) ((3 - 2q)q - (1 + 2q)(1 - q)) + X
(PiO - Pil)2
4Piq(I - q)
1-
1 - Pc=I(PiO - Pil)2∕Pi = 1 - Y
4q(1 - q)	q(1 - q)
where in the last equality We used the definition of Y and the optimal value of equation 3.2.	□
B Proof of Theorem 4.2
Proof. Define gB(θ) = maxw fB(θ, w). Since the optimization problem maxw fB(θ, w) is
strongly concave in w, using Danskin’s theorem (see Danskin (1967) and Bertsekas (1971)), we
conclude that the function gB (∙) is differentiable. Moreover,
VθgB (θ) = VθfB (θ, W)
where W = argmaxw fp (θ, w). Thus Algorithm 2 is in fact equivalent to the gradient descent
algorithm applied to gB (θ). Thus according to (Nesterov, 2018, Chapter 1), the algorithm finds a
point with IlVgB(θ)k ≤ E in O(e-2) iterations.	□
C RENYI Fair K-MEANS
To illustrate the behavior of Algorithm 3, we deployed a simple two-dimensional toy example. In
this example we generated data by randomly selecting 5 center points and then randomly generating
500 data points around each center according to a normal distribution with small enough variance.
The data is shown in Figure 4 with different colors corresponding to different clusters. Moreover, we
assigned for each data point xi a binary value si ∈ {0, 1} that corresponds to its sensitive attribute.
This assignment was also performed randomly except for points generated around center 2 (green
points in Figure 4) which were assigned a value of 1 and points generated around center 4 (blue
points in Figure 4) which were assigned a value of 0. Without imposing fairness, traditional K-
means algorithm would group points generated around center 2 in one cluster regardless of the fact
that they all belong to the same protected group. Similarly, points generated around center 4 will
belong to the same cluster. Hence, according to traditional K-means clustering shown in Figure 4,
the proportion of the protected group in clusters 2 and 4 are 1 and 0 respectively. However, when
imposing our fairness scheme, we expect these points to be distributed among various clusters to
achieve balanced clustering. This is illustrated in Figure 5. It is evident from Figure 5 that increasing
lambda, data points corresponding to centers 2 and 4 are now distributed among different clusters.
16
Published as a conference paper at ICLR 2020
20
15
10
-10
-15
■20
-25
-30
-10	-5	0
5	10	15	20
λ = 0
Figure 4: Applying K-means algorithm without fairness on the synthetic dataset.
Figure 5: Applying fair K-means algorithm with different values of λ on the synthetic dataset.
C. 1 UPDATING w AFTER UPDATING THE ASSIGNMENT OF EACH DATA POINT IN ALGORITHM
3
To understand the reasoning behind updating the vector of proportions w after updating each ai
which is the assignment of data point i, we discuss a simple one-dimensional counterexample.
Consider the following four data points X1 = -5, X2 = -4, X3 = 4, and X4 = 5 with their
corresponding sensitive attributes S1 = S2 = 1 and S3 = S4 = 0. Moreover, assume the following
initial A0 and C0
一1 0
A0 =01，C0 = [-4.5,4.5].
01
Hence, X1 and X2 which both have a sensitive attribute of 1 are assigned to cluster 1 with center
C10 = -4.5 and X3 and X4 which both have a sensitive attribute of 0 are assigned to cluster 2 with
center C02 = 4.5. Then w which is the current proportion of the privileged group in the clusters will
be w0 = [1, 0]. Now, for sufficiently large λ if we update A according to Step 6 of Algorithm 3, we
get the following new assignment
-0 1
A1 =00，C1 = [4.5,-4.5], w1 = [0,1].
10
Hence, the points just switch their clusters. Then, performing another iteration will get us back to
the initial setup and the algorithm will get stuck between these two states that are both not fair. To
overcome this issue we update the proportions w after updating the assignment of each data point.
17
Published as a conference paper at ICLR 2020
D	Datasets Description
In this section we introduce the datasets used in numerical experiment discussed in Section 6. All
of these datasets are publicly available at UCI repository.
•	German Credit Dataset:1 German Credit dataset consists of 20 features (13 categorical
and 7 numerical) regarding to social, and economic status of 1000 customers. The assigned
task is to classify customers as good or bad credit risks. Without imposing fairness, the
DP violation of the trained model is larger than 20%. We chose first 800 customers as the
training data, and last 200 customers as the test data. The sensitive attributes are gender,
and marital-status.
•	Bank Dataset:2 Bank dataset contains the information of individuals contacted by a Por-
tuguese bank institution. The assigned classification task is to predict whether the client
will subscribe a term deposit. For the classification task we consider all 17 attributes (ex-
cept martial status as the sensitive attribute). Removing the sensitive attribute, and train a
logistic regression model on the dataset, yields to a solution that is biased under the de-
mographic parity notion (p% = 70.75%). To evaluate the performance of the classifier,
we split data into the training (32000 data points), and test set (13211 data points). For
the clustering task, we sampled 3 continuous features: Age, balance, and duration. The
sensitive attribute is the marital status of the individuals.
•	Adult Dataset:3 Adult dataset contains the census information of individuals including
education, gender, and capital gain. The assigned classification task is to predict whether a
person earns over 50K annually. The train and test sets are two separated files consisting
of 32000 and 16000 samples respectively. We consider gender and race as the sensitive
attributes (For the experiments involving one sensitive attribute, we have chosen gender).
Learning a logistic regression model on the training dataset (without imposing fairness)
shows that only 3 features out of 14 have larger weights than the gender attribute. Note
that removing the sensitive attribute (gender), and retraining the model does not eliminate
the bias of the classifier. the optimal logistic regression classifier in this case is still highly
biased (p% = 31.49% ). For the clustering task, we have chosen 5 continuous features
(Capital-gain, age, fnlwgt, capital-loss, hours-per-week), and 10000 samples to cluster.
The sensitive attribute of each individual is gender.
E Supplementary Figures
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0	200000 400000 600000 800000 1000000 1200000
LambdaVaIues
PearSon CoITe-ation
.10.08.06.04
Uo 4pwou一 AU 发
Figure 6: The relationship between Renyi correlation, Pearson correlation, and normalized mutual
information. Direct optimization of normalized mutual information is intractable due to its non-
convexity. However, as We can observe on the right-hand-side, by minimizing Renyi correlation to
0, the normalized mutual information is converging to 0 accordingly.
1https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
2 https://archive.ics.uci.edu/ml/datasets/Bank%20Marketing.
3 https://archive.ics.uci.edu/ml/datasets/adult.
18
Published as a conference paper at ICLR 2020
F Fair Neural Network
In this section, We train a 2-layers neural network on the adult dataset regularized by the Renyi
correlation. In this experiment, the sensitive attribute is gender. We set the number of nodes in
the hidden layer, the batch-size, and the number of epochs to 12, 128, and 50, respectively. The
following table depicts the performance of the trained model.
p%	Test Accuracy	Time (Seconds)
31.49%	85:33	731
80.42%	83.34	915
Table 1: Performance and training time of a neural network trained on the Adult dataset. The first
and second rows correspond to the networks not regularized, and regularized by Renyi correlation
respectively. As can be seen in the table, while adding Renyi regularizer makes the classifier more
fair, it does so by bringing a minimum amount of additional computational overhead.
19