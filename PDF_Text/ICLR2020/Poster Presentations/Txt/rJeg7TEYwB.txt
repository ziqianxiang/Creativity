Published as a conference paper at ICLR 2020
Pruned Graph S cattering Transforms
Vassilis N. Ioannidis *
Dpt. of Electrical and Computer Engineering
Univ. of Minnesota
Minneapolis, MN, USA
ioann006@umn.edu
Siheng Chen
Mitsubishi Electric Research Laboratories
Cambridge, MA, USA
schen@merl.com
Georgios B. Giannakis
Dpt. of Electrical and Computer Engineering
Univ. of Minnesota
Minneapolis, MN, USA
georgios@umn.edu
Ab stract
Graph convolutional networks (GCNs) have achieved remarkable performance
in a variety of network science learning tasks. However, theoretical analysis of
such approaches is still at its infancy. Graph scattering transforms (GSTs) are
non-trainable deep GCN models that are amenable to generalization and stability
analyses. The present work addresses some limitations of GSTs by introducing
a novel so-termed pruned (p)GST approach. The resultant pruning algorithm is
guided by a graph-spectrum-inspired criterion, and retains informative scattering
features on-the-fly while bypassing the exponential complexity associated with
GSTs. It is further established that pGSTs are stable to perturbations of the input
graph signals with bounded energy. Experiments showcase that i) pGST performs
comparably to the baseline GST that uses all scattering features, while achieving
significant computational savings; ii) pGST achieves comparable performance to
state-of-the-art GCNs; and iii) Graph data from various domains lead to different
scattering patterns, suggesting domain-adaptive pGST network architectures.
1 Introduction
The abundance of graph-structured data calls for advanced learning techniques, and complements
nicely standard machine learning tools that cannot be directly applied to irregular data domains.
Permeating the benefits of deep learning to the graph domain, graph convolutional networks (GCNs)
provide a versatile and powerful framework to learn from complex graph data (Bronstein et al., 2017).
GCNs and variants thereof have attained remarkable success in social network analysis, 3D point
cloud processing, recommender systems and action recognition. However, researchers have recently
reported inconsistent perspectives on the appropriate designs for GCN architectures. For example,
experiments in social network analysis have argued that deeper GCNs marginally increase the learning
performance (Wu et al., 2019), whereas a method for 3D point cloud segmentation achieves state-of-
the-art performance with a 56-layer GCN network (Li et al., 2019). These ‘controversial’ empirical
findings motivate theoretical analysis to understand the fundamental performance factors and the
architecture design choices for GCNs.
Aiming to bestow GCNs with theoretical guarantees, one promising research direction is to study
graph scattering transforms (GSTs). GSTs are non-trainable GCNs comprising a cascade of graph
filter banks followed by nonlinear activation functions. The graph filter banks are mathematically
designed and are adopted to scatter an input graph signal into multiple channels. GSTs extract
scattering features that can be utilized towards graph learning tasks (Gao et al., 2019), with competitive
performance especially when the number of training examples is small. Under certain conditions
on the graph filter banks, GSTs are endowed with energy conservation properties (Zou & Lerman,
*This work was mainly done while V. N. Ioanndis was working at Mitsubishi Electric Research Laboratories.
1
Published as a conference paper at ICLR 2020
(c) 3D point cloud
(a)	Academic collaboration
(b)	Protein-protein network
Figure 1: Illustration of the same pGST applied to different graph datasets. Notice that for the social
network (a) most of GST branches are pruned, suggesting that most information is captured by local
interactions.
2019), as well as stability meaning robustness to graph topology deformations (Gama et al., 2019a).
However, GSTs are associated with exponential complexity in space and time that increases with
the number of layers. This discourages deployment of GSTs when a deep architecture is needed.
Furthermore, stability should not come at odds with sensitivity. A filter’s output should be sensitive
to and “detect” perturbations of large magnitude. Lastly, graph data in different domains (social
networks, 3D point clouds) have distinct properties, which encourages GSTs with domain-adaptive
architectures.
The present paper develops a data-adaptive pruning framework for the GST to systematically retain
important features. Specifically, the contribution of this work is threefold.
C1. We put forth a pruning approach to select informative GST features that we naturally
term pruned graph scattering transform (pGST). The pruning decisions are guided by a
criterion promoting alignment (matching) of the input graph spectrum with that of the graph
filters. The optimal pruning decisions are provided on-the-fly, and alleviate the exponential
complexity of GSTs.
C2. We prove that the pGST is stable to perturbations of the input graph signals. Under certain
conditions on the energy of the perturbations, the resulting pruning patterns before and after
the perturbations are identical and the overall pGST is stable.
C3. We showcase with extensive experiments that: i) the proposed pGSTs perform similarly and
in certain cases better than the baseline GSTs that use all scattering features, while achieving
significant computational savings; ii) The extracted features from pGSTs can be utilized
towards graph classification and 3D point cloud recognition. Even without any training on
the feature extraction step, the performance is comparable to state-of-the-art deep supervised
learning approaches, particularly when training data are scarce; and iii) By analyzing the
pruning patterns of the pGST, we deduce that graph signals in different domains call for
different network architectures; see Fig. 1.
2	Related work
GCNs rely on a layered processing architecture comprising trainable graph convolution operations to
linearly combine features per graph neighborhood, followed by pointwise nonlinear functions applied
to the linearly transformed features (Bronstein et al., 2017). Complex GCNs and their variants have
shown remarkable success in graph semi-supervised learning (KiPf & Welling, 2017; VelickoVic et al.,
2018) and graph classification (Ying et al., 2018). To simplify GCNs, (Wu et al., 2019) has shown
that by employing a single-layer linear GCN the performance in certain social network learning
tasks degrades only slightly. On the other hand, (Li et al., 2019) has developed a 56-layer GCN
that achieves state-of-the-art performance in 3D point cloud segmentation. Hence, designing GCN
architectures guided by properties of the graph data is a highly motivated research question.
Towards theoretically explaining the success of GCNs, recent works study the stability properties
of GSTs with respect to metric deformations of the domain (Gama et al., 2019b;a; Zou & Lerman,
2019). GSTs generalize scattering transforms (Bruna & Mallat, 2013; Mallat, 2012) to non-Euclidean
2
Published as a conference paper at ICLR 2020
domains. GSTs are a cascade of graph filter banks and nonlinear operations that is organized in a
tree-structured architecture. The number of extracted scattering features ofa GST grows exponentially
with the number of layers. Theoretical guarantees for GSTs are obtained after fixing the graph filter
banks to implement a set of graph wavelets. The work in (Zou & Lerman, 2019) establishes energy
conservation properties for GSTs given that certain energy-preserving graph wavelets are employed,
and also prove that GSTs are stable to graph structure perturbations; see also (Gama et al., 2019b)
that focuses on diffusion wavelets. On the other hand, (Gama et al., 2019a) proves stability to relative
metric deformations for a wide class of graph wavelet families. These contemporary works shed
light into the stability and generalization capabilities of GCNs. However, stable transforms are not
necessarily informative, and albeit highly desirable, a principled approach to selecting informative
GST features remains still an uncharted venue.
3	Background
Consider a graph G := {V, E} with node set V := {vi}iN=1, and edge set E := {ei}iE=1. Its
connectivity is described by the graph shift matrix S ∈ RN×N, whose (n, n0)th entry Snn0 is nonzero
if (n, n0) ∈ E orifn = n0. A typical choice for S is the adjacency or the Laplacian matrix. Further,
each node can be also associated with a few attributes. Collect attributes across all nodes in the matrix
X := [x1, . . . , xF] ∈ RN×F, where each column xf ∈ RN is a ‘graph signal.’
Graph Fourier transform. A Fourier transform corresponds to the expansion of a signal over
bases that are invariant to filtering; here, this graph frequency basis is the eigenbasis of the graph
shift matrix S. Henceforth, S is assumed normal with S = VΛV>, where V ∈ RN×N forms
the graph Fourier basis, and Λ ∈ RN ×N is the diagonal matrix of corresponding eigenvalues
λ0, . . . , λN-1. These eigenvalues represent graph frequencies. The graph Fourier transform (GFT)
of x ∈ RN is xb = V>x ∈ RN, while the inverse transform is x = Vxb. The vector xb represents the
signal’s expansion in the eigenvector basis and describes the graph spectrum of x. The inverse GFT
reconstructs the graph signal from its graph spectrum by combining graph frequency components
weighted by the coefficients of the signal’s graph Fourier transform. GFT is a tool that has been
popular for analyzing graph signals in the graph spectral domain.
Graph convolution neural networks. GCNs permeate the benefits of CNNs from processing
Euclidean data to modeling graph structured data. GCNs model graph data through a succession of
layers, each of which consists of a graph convolution operation (graph filter), a pointwise nonlinear
function σ(∙), and oftentimes also a pooling operation. Given a graph signal X ∈ RN, the graph
convolution operation diffuses each node’s information to its neighbors according to the graph shift
matrix S, as SX. The nth entry [SX]n = Pn0 ∈N Snn0 xn0 is a weighted average of the one-hop
neighboring features. Successive application of S will increase the reception field, spreading the
information across the network. Hence, a Kth order graph convolution operation (graph filtering) is
K
h(S)X := X wkSkX = Vbh(Λ)Xb	(1)
k=0
where the graph filter h(∙) is parameterized by the learnable weights {wk}3o, and the graph filter in
the graph spectral domain is bh(Λ) = PkK=0 wkΛk. In the graph vertex domain, the learnable weights
reflect the influences from various orders of neighbors; and in the graph spectral domain, those
weights adaptively adjust the focus and emphasize certain graph frequency bands. GCNs employ
various graph filter banks per layer, and learn the parameters that minimize a predefined learning
objective, such as classification, or regression.
Graph scattering transforms. GSTs are the nontrainable counterparts of GCNs, where the parame-
ters of the graph convolutions are selected based on mathematical designs. GSTs process the input at
each layer by a sequential application of graph filter banks {hj(S)}jJ=1, an elementwise nonlinear
function σ(∙), and a pooling operator U. At the first layer, the input graph signal X ∈ RN constitutes
the first scattering feature vector z(0) := X. Next, z(0) is processed by the graph filter banks and
σ(∙) to generate {zj)}J=ι with z(j):= σ(h7-(S)z(o)). At the second layer, the same operation is
repeated per j . The resulting computation structure is a tree with J branches at each non-leaf node;
see also Fig. 2. The 'th layer of the tree includes J' nodes. Each tree node at layer ' in the scattering
transform is indexed by the path p(`) of the sequence of ` graph convolutions applied to the input
3
Published as a conference paper at ICLR 2020
z(3,1)	z(3,2)	z(3,3)
z(1,1)	z(1,2)	z(1,3)
z(2,1) z(2,2)	z(2,3)
Figure 2: Scattering pattern associated with a pGST with J = 3 and L = 3. The dashed lines
represent the pruned branches. The example of a graph signal and the GFTs of x and the filter banks
are included as well. Note that the third filter j = 3 at ` = 1 generates no output, i.e. z(3) = 0, and
hence is pruned.
graph signal x, i.e. p(') := (j(I),j⑵,…,产).1 The scattering feature vector at the tree node
indexed by (p(`), j) at layer ` + 1 is
z(p('),j) = σ(hj (S)Z(P⑷力	⑵
where the variable p(`) holds the list of indices of the parent nodes ordered by ancestry, and all
path p(') in the tree with length ' are included in the path set P(') with |P(') | = 2'. The nonlinear
transformation function σ(∙) disperses the graph frequency representation through the spectrum, and
endows the GST with increased discriminating power (Gama et al., 2019a). By exploiting the sparsity
of the graph, the computational complexity of (2) is O(KE), where E = |E| is the number of edges
in G.1 2 Each scattering feature vector z(p('))is summarized by an aggregation operator U(∙) to obtain
a scalar scattering coefficient as。他⑷):= U(z(p('))), where U(∙) is typically an average or sum
operator that effects dimensionality reduction of the extracted features. The scattering coefficient at
each tree node reflects the activation level at a certain graph frequency band.
These scattering coefficients are collected across all tree nodes to form a scattering feature map
φ (X) := {{φ(p('))}p(')∈P(') }L=0	⑶
where ∣Φ(x)∣ = PL=0 J`. The GST operation resembles a forward pass of a trained GCN. This is
why several works study GST stability under perturbations of S in order to understand the working
mechanism of GCNs (Zou & Lerman, 2019; Gama et al., 2019a;b).
4	Pruned Graph Scattering Transforms
While the representation power of GST increases with the number of layers, the computational and
space complexity of the transform also increase exponentially with the number of layers due to
its scattering nature. Hence, even if informative features are available at deeper GST layers, the
associated exponential complexity of extracting such features is prohibitive with the existing GST
architectures. On the other hand, various input data (social networks, 3D point clouds) may have
distinct properties, leading to different GST feature maps. In some cases, only a few tree nodes
in deep layers are informative; and in other cases, tree nodes in shallow layers carry significant
information; see Fig. 1. This requires a customized GST to adaptively choose significant tree nodes.
Alleviating GST limitations, we introduce a pruned graph scattering transform (pGST) to system-
atically retain informative tree nodes without additional complexity. Our novel pGST alleviates
the exponential complexity and adapts GST to different input data. Furthermore, pGST offer a
practical mechanism to understand the architecture of GCNs. Based on the pruning patterns, the
proposed pGST suggests when a deeper GCN is desirable, and when a shallow one will suffice.
Pruning the wavelet packets has been traditionally employed for compression in image processing
1A tree node is fully specified by its corresponding path.
2Any analytical function h(S) can be written as a polynomial of S with maximum degree N - 1 (Horn &
Johnson, 2012).
4
Published as a conference paper at ICLR 2020
applications (Xiong et al., 2002), where the pruning is guided by a rate-distortion optimallity criterion.
In this work, we consider a graph spectrum inspired criterion. Intuitively, each tree node in the GSTs
reflects a unique subband in the graph spectrum. When the subband of a tree node does not have a
sufficient overlap with the the graph spectrum of a graph signal, this tree node cannot capture the
property of this graph signal, and should be pruned.
For example, consider a smooth graph signal x, where connected nodes have similar signal values,
that has a sparse (low-rank) representation in the graph spectral domain that is, xb := V>x ∈ RN
and [xb]n = 0 for n ≥ b. The graph spectrum of the jth graph filtered output is then
V>hj (S)x = diag (bhj (λ))xb = [bhj (λ1)xb1 , bhj (λ2)xb2, . . . , bhj (λN)xbN]>
where λn is the nth eigenvalue of S and each frequency xbn is weighted by the corresponding
transformed eigenvalue hj (λn). Hence, if the support of the graph spectrum {hj (λn)}n is not
included in the support of [xb]n then the jth graph filter output will not capture any information; that
is, hj(S)x = 0N; see Fig. 2. Thus, identifying such graph filters and pruning the corresponding tree
nodes will result to a parsimonius and thus computationally efficient GST.
Pruning criterion. Motivated by the aforementioned observation, we introduce a pruning criterion to
select the scattering branches per tree node by maximizing the alignment between the graph spectrum
of the graph filters and the scattering feature. At the tree node p, the optimization problem is
maxJ	X Xbhj(λn)2 -τ [bz(p)]2n f(p,j)	(4)
{f(p,j)}j=1	j=1 n=1
s. t.	f(p,j) ∈ {0, 1}, j= 1,...,J
where bz(p) := Vz(p) is the graph spectrum of the scattering feature vector z(p); τ is a user-specific
threshold; and, f(p,j) stands for the pruning assignment variable indicating whether node (p, j) is
active (f(p,j) = 1) or it should be pruned (f(p,j) = 0). The objective in (4) promotes retaining tree
nodes that maximize the alignment of the graph spectrum of bz(p) with that of hj(λ). The threshold τ
introduces a minimum spectral value to locate those tree nodes whose corresponding graph spectral
response is small, i.e. bhj(λn)2 τ. Note that criterion (4) is evaluated per tree node p, thus allowing
for a flexible and scalable design.
The optimization problem in (4) is nonconvex since f(p,j) is a discrete variable. Furthermore,
recovering bz(p) requires an eigendecomposition of the Laplacian matrix that incurs O(N3) complexity.
Nevertheless, by exploiting the structure in (4), we develop an efficient pruning algorithm that achieves
the maximum of (4), as summarized in the following theorem.
Theorem 1. The optimal pruning assignment variables
of (4) is given as follows
(1	if kz(p,j) k2 > T
f* J 1 矿 kz(p)k2 >τ,	J
f(p,j) =	kz k2	, j = 1, . . ., J
I o	if kz(P，j)k	< τ
I0	IJ kz(p)k2 <τ∙
(5)
The optimal variables f(p 力 are given by comparing the energy of the input z(p)to that of the output
z(p,j) per graph filter j that can be evaluated at a low complexity of O(N). Our pruning criterion
leads to a principled and scalable algorithm to selecting the GST tree nodes to be pruned. The pruning
objective is evaluated at each tree node p, and pruning decisions are made on-the-fly. Hence, when
f(p) = 1, tree node P is active and the graph filter bank will be applied to z(p), expanding the tree
to the next layer; otherwise, the GST will not be expanded further at tree node p, which can result
to exponential savings in computations. An example of such a pruned tree is depicted in Fig. 2.
Evidently, the hyperparameter τ controls the input-to-output energy ratio. A large τ corresponds to
an aggressively pruned scattering tree, while a small τ amounts to a minimally pruned scattering tree.
The pGST is then defined as
ψ(X) := {φ(p)}p∈τ
where T is the set of active tree nodes T := {p ∈ P| f(p)= 1}.
5
Published as a conference paper at ICLR 2020
Our pruning approach provides a concise version of GSTs and effects savings in computations as well
as memory. Although the worst-case complexity of pGST is still exponential, a desirable complexity
can be effected by properly selecting τ . As a byproduct, the scattering patterns of pGSTs reveal
the appropriate depths and widths of the GSTs for different input data; see also Fig. 1. The pruning
approach so far is an unsupervised one, since no input data labels are assumed available.
5 Stability and sensitivity of pGST
In this section, we prove the stability of pGST to perturbations of the input graph signal. To
establish the ensuing results, we consider graph wavelets that form a frame with frame bounds
A and B (Hammond et al., 2011). Specifically, for any graph signal x ∈ RN, it holds that,
A2kxk2 ≤ PjJ=1 khj (S)xk2 ≤ B2 kxk2. In the graph vertex domain, the scalar frame bounds
A and B characterize the numerical stability of recovering a graph signal x from {hj (S)x}j . In the
graph spectral domain, they reflect the ability of the graph filter bank to amplify x along each graph
frequency. Tight frame bounds, satisfying A2 = B2, are of particular interest because such wavelets
lead to enhanced numerical stability and faster computations (Shuman et al., 2015). The frame
property of the graph wavelet plays an instrumental role in proving GST stability to perturbations of
the underlying graph structure (Gama et al., 2019a;b; Zou & Lerman, 2019).
Consider a perturbed graph signal X given by
X := X + δ ∈ RN	(6)
where δ ∈ RN is the perturbation vector. Such an additive model (6) may represent noise in the
observed feature or adversarial perturbations. We are interested in studying how and under which
conditions our pGST is affected by such perturbations. A stable transformation should have a similar
output under small input perturbations.
Before establishing that our pGST is stable, we first show that GST is stable to small perturbations of
the input graph signal3.
Lemma 1. Consider the GST Φ(∙) with L layers and J graph filters; and suppose that the graph
filter bank forms aframe with bound B, while X and X are related via (6). It then holds that
kΦ(X)- Φ(X)k2
Hφ(X)I
≤t
PL=o(B2J)`
PL=o Je
kδk2.
(7)
The squared difference of the GSTs is normalized by the number of scattering features in Φ(∙), that
is ∣Φ(x) ∣ = PL=I J'. The bound in (7) relates to the frame bound of the wavelet filter bank. Notice
that for tight frames with B = 1, then the normalized stability bound (7) is tight. Let T be the
structure of the pruned tree for Ψ(X). The following lemma asserts that the pGST offers the same
pruned tree for the original and the perturbed inputs.
Lemma 2. Let Zp denote the perturbed scattering feature at the tree node P and δp := Zp — Zp. If
for all p ∈ P and j = 1, . . . , J, it holds that
Ikhj(S)Zpk2一τkzpk2∣> khj(S)δpk2 + TIkZpk2-kZpk2∣.	(8)
Then, we obtain
1. The pruning transform will output the same tree for Ψ(x) and Ψ(x) ; that is, T = T; and,
2. With g(X) := khj (S)Xk2 — τkXk2, a necessary condition for (8) is
|g(Zp)| > g(δp) .	(9)
According to (9), Lemma 2 can be interpreted as a signal-to-noise ratio (SNR) condition because
under g(δp) > 0, it is possible to write (9) as |g(Zp) ∣∕g(δp) > 1. Lemma 2 provides a per-layer and
branch condition for pGST to output the same scattering tree for the original or the perturbed signal.
By combining Lemmas 1 and 2, we arrive at the following stability result for the pGST network.
3Prior art deals with GST stability to structure perturbations (Gama et al., 2019a;b; Zou & Lerman, 2019).
6
Published as a conference paper at ICLR 2020
-∙- DS MCS ・•・ THS . PDS T- PMCS T- PTHS
300	600	1,000	1,400
Number of samples
(a) Authorship
1	30	60	100
SNR
(b) Facebook
(c) Runtime (d) Feature overlap
Figure 3:	Classification accuracy against number of samples in the authorship attribution (a) and SNR
in dB for source localization (b). Runtime comparison in seconds of the scattering transforms (c).
Theorem 2. Consider the PGST transform Ψ(∙) with L layers and J graph filters; and suppose that
the graph filter bank forms a frame with bound B, while X and X are related via (6). The pGST is
stable to bounded perturbations δ, in the sense that
kΨ(x)-Ψ(X)∣∣2
P∣Ψ(X)T
≤t
Pi b2'
PLF`
kδk2
where f` := |P(') ∪ T| is the number ofactive scattering features at layer ', and ∣Ψ(x) | = PL=° f`
the number of retained scattering features.
6	Experiments
This section evaluates the performance of our pGST in various graph classification tasks. Graph
classification amounts to predicting a label yi given xi and Si for the ith graph. Our pGST extracts
Ψ(xi), which is utilized as a feature vector for predicting yi. During training, the structure of the
pGST T is determined, which is kept fixed during validation and testing. The parameter τ is selected
via cross-validation. Our goal is to provide tangible answers to the following research questions.
RQ1 How does the proposed pGST compare to GST?
RQ2 How does pGST compare to state-of-the-art GCN approaches for graph classification?
RQ3 What are the appropriate scattering patterns for various graph data?
Appendix A includes additional experiments on ablation studies over the effect of the parameters
J, L, τ .
pGST vs. GST. To address RQ1, we reproduce the experiments of two tasks in (Gama et al., 2019a):
authorship attribution and source localization. For the scattering transforms, we consider three
implementations of graph filter banks: the diffusion wavelets (DS) in (Gama et al., 2019b), the monic
cubic wavelets (MCS) in (Hammond et al., 2011) and the tight Hann wavelets (THS) in (Shuman
et al., 2015).4 The scattering transforms use J = 5 filters, L = 5 layers, and τ = 0.01. The extracted
features from GSTs are subsequently utilized by a linear support vector machine (SVM) classifier.
Authorship attribution amounts to determining if a certain text was written by a specific author. Each
text is represented by a graph with N = 244, where words (nodes) are connected based on their
relative positions in the text, and x is a bag-of-words representation of the text; see also (Gama et al.,
2019b). Fig. 3 (a) reports the classification accuracy for the authorship attribution task as the number
of training samples (texts) increases. GSTs utilize P5=1 5' = 781 scattering coefficients, while
pGSTs rely only on |T| = 61 for PDS, |T| = 30 for PMCS, and |T| = 80 for PTHS. Evidently, the
proposed pGST achieves comparable performance as the baseline GST, whereas pGST uses only a
subset of features (12.8%, 3.8% and 10.2% respectively). The SVM classifier provides a coefficient
that weighs each scattering scalar. The magnitude of each coefficient shows the importance of the
corresponding scattering feature in the classification. Fig. 3 (d) depicts the percentage of features
4PDS, pMCS, pTHS denote the pruned versions of these transforms.
7
Published as a conference paper at ICLR 2020
Method
Data Set
Enzymes	D&D
Collab	Proteins
…MCS — PMCS - POINTNET — POINTNET++ - 3DSHAPENETS - VOXNET
Classification accuracy
0.75
ycarucca noitacfiissal
0.7
246
Network depth L
(a)	Tr./test : 9843/2468
0.6
2345
Network depth L
(b)	Tr./test : 615/11703
gnirettac
Shortest-path
WL-OA
42.32
60.13
78.86
79.04
59.10
80.74
76.43
75.26
PatchySan
GraphSage
ECC
Set2set
SortPool
DiffPool-Det
DiffPool-NoLP
DiffPool
54.25
53.50
60.15
57.12
58.33
62.67
64.23
76.27
75.42
74.10
78.12
79.37
75.47
79.98
81.15
72.60
68.25
67.79
71.75
73.76
82.13
75.63
75.50
75.00
70.48
72.65
74.29
75.54
75.62
77.42
78.10
GSC
GST
53.88
59.84
76.57
79.28
76.88
77.32
74.03
76.23
PGST (Ours)
60.25
81.27
78.40	78.57
Figure 4:	3D point cloud classification.	Table 1: Graph classification accuracy.
after prunning retained in the top-2|T | most important GST features given by the SVM classifier. It
is observed, that although pGST does not take into account the labels, the retained features are indeed
informative for classification.
Source localization amounts to recovering the source of a rumor given a diffused signal over a
Facebook subnetwork with N = 234; see the detailed settings in (Gama et al., 2019b). Fig. 3 (b)
shows the classification accuracy of the scattering transforms for increasing SNR in dB. In accordance
to Lemma 1 and Theorem 2, both pGST and GST are stable for a wide range of SNR. Furthermore,
the performance of pGST matches the corresponding GST, while the pGST uses only a subset of
features. Fig. 3 (c) depicts the runtime of the different scattering approaches, where the computational
advantage of the pruned methods is evident.
Graph classification. Towards answering RQ2, the proposed pGST is compared with the following
state-of-the-art approaches.5 The kernel methods shortest-path (Borgwardt & Kriegel, 2005), and
Weisfeiler-Lehman optimal assignment (WL-OA) (Kriege et al., 2016); the deep learning approaches
PatchySan (Niepert et al., 2016), GraphSage (Hamilton et al., 2017), edge-conditioned filters in CCNs
(ECC) (Simonovsky & Komodakis, 2017), Set2Set (Vinyals et al., 2015), SortPool (Zhang et al.,
2018), and DiffPool (Ying et al., 2018); and the geometric scattering classifier (GSC) (Gao et al.,
2019). Results are presented with protein data sets D&D, Enzymes and Proteins, and the scientific
collaboration data set Collab. Detailed description of the datasets is included in the Appendix. We
perform 10-fold cross validation and report the classification accuracy averaged over the 10 folds.
The gradient boosting classifier is employed for pGST and GST with parameters chosen based on
the performance on the validation set. The graph scattering transforms use the MC wavelet with
L = 5, J = 5 and τ = 0.01. Table 1 lists the classification accuracy of the proposed and competing
approaches. Even without any training on the feature extraction step, the performance of pGST is
comparable to the state-of-the-art deep supervised learning approaches across all datasets. GST and
pGST outperform also GSC, since the latter uses a linear SVM to classify the scattering features.
Point cloud classification. We further test pGST in classifying 3D point clouds. Given a point
cloud, a graph can be created by connecting points (nodes) to their nearest neighbors based on
their Euclidian distance. Each node is also associated with 6 scalars denoting its x-y-z coordinates
and RGB colors. For this experiment, GSTs are compared against PointNet++ (Qi et al., 2017a;b),
3dShapeNets (Wu et al., 2015) and VoxNet (Maturana & Scherer, 2015), that are state-of-the-art deep
learning approaches. Fig. 4 reports the classification accuracy for the ModelNet40 dataset (Wu et al.,
2015) for increasing L. In Fig. 4 (a) 9,843 clouds are used for training and 2,468 for testing using the
gradient boosting classifier; whereas, in Fig. 4 (b) only 615 clouds are used for training and the rest
for testing using a fully connected neural network classifier with 3 layers. The scattering transforms
use an MC wavelet with J = 5 for Fig. 4 (a) and J = 9 for Fig. 4 (b). Fig. 4 showcases that scattering
transforms are competitive to state-of-the-art approaches, while pGST outperforms GST. This may be
attributed to overfitting effects, since a large number of GST features is not informative. Furthermore,
the exponential complexity associated with GSTs prevents their application for L = 6. Fig. 4 (b)
shows that when the training data are scarce, GST and pGST outperform the PointNet++, which
requires a large number of training data to optimize over the network parameters.
5For the competing approaches we report the 10-fold cross-validation numbers reported by the original
authors; see also (Ying et al., 2018).
8
Published as a conference paper at ICLR 2020
Scattering patterns. Towards answering RQ3, we depict the scattering structures of pGSTs, with a
MC wavelet, J = 3 and L = 5, for the Collab, Proteins, and ModelNet40 datasets in Fig. 1. Evidently,
graph data from various domains require an adaptive scattering architecture. Specifically, most tree
nodes for the academic collaboration dataset are pruned, and hence most informative features are
in the shallow layers. This is consistent with the study in (Wu et al., 2019), which experimentally
shows that deeper GCNs do not contribute as much for social network data. These findings are further
supported by the small-world phenomenon in social networks, which suggests the diameter of social
networks is small (Watts & Strogatz, 1998). On the other hand, the tree nodes for a 3D point cloud
are minimally pruned, which is in line with the work in (Li et al., 2019) that showcases the advantage
of deep GCNs in 3D point clouds classification. For the protein datasets, additional experiments are
performed in Appendix A.4 that corroborate the pGST insights regarding the required number of
GCN layers.
7 Conclusions
This paper developed a novel approach to pruning the graph scattering transform. The proposed pGST
relies on a graph-spectrum-based data-adaptive criterion to prune non-informative features on-the-fly,
and effectively reduce the computational complexity of GSTs. Furthermore, when the input signal
is perturbed, the stability of pGST is established. Experiments demonstrate that i) the performance
gains of pGSTs relative to GSTs; ii) pGST is competitive in a variety of graph classification tasks;
and (iii) graph data from different domains exhibit unique pruned scattering patterns, which calls for
adaptive network architectures.
Acknowlegdments. This work was supported by Mitsubishi Electric Research Laboratories, the
Doctoral Dissertation Fellowship of the University of Minnesota, and the NSF grants 171141, and
1500713.
References
Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In IEEE Intl. Conf.
on Data Mining (ICDM), 2005.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: going beyond euclidean data. IEEE Sig. Process. Mag., 34(4):18-42, 2017.
Joan Bruna and StePhane Mallat. Invariant scattering convolution networks. IEEE Trans. Pattern
Anal. Mach. Intel., 35(8):1872-1886, 2013.
Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability of graph scattering transforms. In
Proc. Advances Neural Inf. Process. Syst. (NeurIPS), 2019a.
Fernando Gama, Alejandro Ribeiro, and Joan Bruna. Diffusion scattering transforms on graphs. In
Proc. Intl. Conf. on Learn. Representations (ICLR), 2019b.
Feng Gao, Guy Wolf, and Matthew Hirn. Geometric scattering for graph data analysis. In Proc. Intl.
Conf. Mach. Learn. (ICML), pp. 2122-2131, 2019.
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and
applications. IEEE Data Engineering Bulletin, 2017.
David K Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral
graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011.
Roger A Horn and Charles R Johnson. Matrix Analysis. Cambridge university press, 2012.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In Proc. Intl. Conf. on Learn. Representations (ICLR), Toulon, France, Apr. 2017.
Nils M Kriege, Pierre-Louis Giscard, and Richard Wilson. On valid optimal assignment kernels and
applications to graph classification. In Proc. Advances Neural Inf. Process. Syst. (NeurIPS), 2016.
Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Can gcns go as deep as cnns? In
Proc. Int. Conf. Comput. Vis., 2019.
9
Published as a conference paper at ICLR 2020
StePhane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65
(10):1331-1398,2012.
Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time
object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), 2015.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks
for graphs. In Proc. Intl. Conf. Mach. Learn. (ICML), 2016.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In IEEE Conf. on Comp. Vis. and Pat. Rec., 2017a.
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature
learning on point sets in a metric space. In Proc. Advances Neural Inf. Process. Syst. (NeurIPS),
2017b.
David I Shuman, Christoph Wiesmeyr, Nicki Holighaus, and Pierre Vandergheynst. Spectrum-adapted
tight graph wavelet and vertex-frequency frames. IEEE Trans. Sig. Process., 63(16):4223-4235,
2015.
Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional neural
networks on graphs. In IEEE Conf. on Comp. Vis. and Pat. Rec., 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In Proc. Intl. Conf. on Learn. Representations (ICLR), 2018.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
Proc. Intl. Conf. on Learn. Representations (ICLR), 2015.
Duncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’ networks. Nature, 393
(6684):440, 1998.
Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q
Weinberger. Simplifying graph convolutional networks. In Proc. Intl. Conf. Mach. Learn. (ICML),
2019.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In IEEE Conf. on Comp. Vis.
and Pat. Rec., 2015.
Zixiang Xiong, Kannan Ramchandran, and Michael T Orchard. Space-frequency quantization for
wavelet image coding. 2002.
Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In Proc. Advances Neural
Inf. Process. Syst. (NeurIPS), 2018.
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classification. In AAAI Conference on Artificial Intelligence, 2018.
Dongmian Zou and Gilad Lerman. Graph convolutional neural networks via scattering. Applied and
Computational Harmonic Analysis, 2019.
10
Published as a conference paper at ICLR 2020
Dataset	Graphs	Features F	Max number of nodes per graph
Collab	5000	1	二	492
D&D	1178	89	5748
Enzymes	600	3	126
Proteins	1113	3	620
Table 2: Dataset characteristics
PDs PDS ■ PMCS HI- PTHS
86
..
00
ycarucca noitacfiissalC
10-5	10-4	10-3	10-2
τ
10-1	100
104
20
0
1
|T|
10-5	10-4	10-3	10-2	10-1	100
τ
(a)	Classification accuracy
10-1
(b)	Number of active features |T|
(c) Runtime in seconds
Figure 5:	Performance of pGSTs for varying τ .
A	Additional experiments
Dataset characteristics. The characteristics of the Datasets used in Table 1 are shown in Table 2.
Notice that the nodes in the Collab dataset did not have any features, and hence x was selected as the
vector that holds the node degrees.
ycarucca noitacfiissalC
pDs PDS ■ PMCS + PTHS
24
ycarucca noitacfiissalC
0.9
0.6
6
0.8
0.7
4	6	8	10
J
L
Figure 6:	Classification accuracy over
different parameters.
11
Published as a conference paper at ICLR 2020
A. 1 Ablation study
Fig. 5 reports how the pGST is affected by varying the threshold τ in the task of source localization,
with J = 6 and L = 5. Fig. 5 (a) shows the classification accuracy that generally decreases at τ
increases since the number of active features |T| decreases; cf. Fig. 5 (b). Fig. 5 (c) reports the
runtime in seconds of the approaches. Fig. 6 showcases the classification performance of the pGST
with τ = 0.01 for varying L with J = 3 on the left and for varying J with L = 3 on the right. It is
observed, that the classification performance generally increases with L and J.
B Proof of Theorem 1
First, the objective in (4) is rewritten as
JN	J
X X	bhj(λn)2	-τ	[bz(p)]2n	fj	= X bz(>p)	diag(bhj(λ))	-τI	bz(p)fj	(10)
j=1 n=1	j=1
that follows by definition. By introducing the scalars αj := bz(>p) (diag (bhj (λ))2 - τ I)bz(p) for
j = 1, . . . , J, (4) can be rewritten as
J
max	αjfj	(11)
fj	j=1
s.	t.	fj ∈ {0, 1}, j = 1, . . . , J.
The optimization problem in (11) is nonconvex since fj is a discrete variable. However, maximizing
the sum in (11) amounts to setting fj = 1 for the positive αj over j. Such an approach leads to the
optimal pruning assignment variables as follows
1	if αj > 0,
K = V	, j = 1,...,J	(12)
j 0 if αj < 0.
The rest of the proof focuses on rewriting αj as follows
αj =bz(>p)(diag(bhj(λ))2 - τI)bz(p)	(13)
=kdiag(bhj(λ))bz(p)k2-τkbz(p)k2	(14)
Furthermore, since V is orthogonal matrix it holds that kbz(p) k2 = kV>z(p) k2 = kz(p) k2 and it
follows
kdiag (bhj(λ))bz(p)k2 =khj(S)z(p)k2	(15)
=kσ(hj(S)z(p))k2
=kz(p,j)k2	(16)
where the second line follows since σ(∙) is applied elementwise and does not change the norm.
C Proof of Lemma 1
By definition (3) it holds
L
kφ(X)- φ(X)k2 = X X	lφ(p('))- φ(p('))|2	(O)
'=0 p(')∈P(')
Hence, it is well motivated to bound each term of the sum in (17) as follows
lφ(p(')) - φ(p('))1 =IU(Z(P(')) - U(z(p(')))1	(18)
≤kUkkz(p(')) - z(p('))k	(19)
12
Published as a conference paper at ICLR 2020
where (19) follows since the norm is a sub-multiplicative operator. Next we will show the following
recursive bound
kz(p(')) - Z(p('))k =kσ(hj(') (S)z(p('-i))) - σ(hj(') (S)Z(P('-i)))k	(20)
≤kσ()kkhj(')(S)z(p('-i))- hj(`)(S)Z(p('-i))k	(21)
≤khj(')(S)z(p('-i))- hj(`)(S)Z(p('—i))k	(22)
≤khj(') (S)kkz(p('-i))- Z(p('-i))k	(23)
where (21), (23) follow since the norm is a sub-multiplicative operator and (22) follows since the
nonlinearity is nonexpansive, i.e. kσ()k < 1. Hence, by applying (23) ` - 1 times the following
condition holds
kz(P(')) - Z(p('))k ≤ khj(`) (S)kkhj('-i) (S)k …khj(i) (S)kkx - Xk	(24)
and by further applying the frame bound and (6) it follows that
kz(p(')) - Z(p('))k ≤ B'kδk	(25)
Combining (19), (25) and the average operator property kU k = 1 it holds that
IΦ(p(')) - Φ(p('))l ≤ B'kδk	(26)
By applying the bound (26) for all entries in the right hand side of (17) it follows that
L
kΦ(x) - Φ(X)k2 ≤ X X B2lkδk2	(27)
'=0 p(')∈P(')
By factoring out kδ k and observing that the sum in the right side of (27) does not depend on the path
index p it follows that
kΦ(x) - Φ(X)k2 ≤ (X 仍(')∣B2') kδk2	(28)
Finally, since the cardinality of the paths at ' is ∣P(')∣	= J' and PL=g(B2J)'
((BJ )l)∕(B2J - 1) it holds
kφ(X)- φ(X)k≤ BJδMk
(29)
D Proof of Lemma 2
We will prove the case for ` = 0, where zp(0) = X, since the same proof holds for any `. First, we
adapt (8) to the following
Ikhj(S)Xk2-TkXk2∣> khj(S)δk2 + TlkXk2 -kXk2∣.	(30)
The proof will examine two cases and will follow by contradiction. For the first case, consider that
branch j is pruned in Ψ(x) and not pruned in Ψ(X), i.e. (j) ∈ T and (j) / T. By applying (5) for
z(j) = σ(hj (S)X) there exists C ≥ 0 such that
，≤τ-C
khj(S)Xk2≤TkXk2-CkXk2
Furthermore, from (5) it holds for Zj) = σ(hj(S)X) that
khj(S)Xk2 >
kXk2
(31)
(32)
(33)
13
Published as a conference paper at ICLR 2020
By applying (6) to (33), and using the triangular inequality it follows that
khj(S)Xk2 + khj(S)δk2 ≥ Tkxk2	(34)
Next, by applying (32) it holds that
τkxk2 - CkXk2 + khj(S)δk2 ≥τkXk2	(35)
τ(kxk2 -kXk2) + khj(S)δk2 ≥Ckxk2.	(36)
Next, by utilizing (30) and the absolute value property |a| ≥ a to upper-bound the left side of (36) it
follows that
khj(S)Xk2-τkXk2>CkXk2.	(37)
Finally, by applying (32) the following is obtained
0> 2CkXk2	(38)
which implies that C < 0. However, this contradicts (31) since C ≥ 0. Following a symmetric
argument we can complete the proof for the other case.
14