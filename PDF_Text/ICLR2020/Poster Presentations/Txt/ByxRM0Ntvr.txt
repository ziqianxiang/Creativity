Published as a conference paper at ICLR 2020
Are Transformers universal approximators
of sequence-to-sequence functions ?
Chulhee Yun*
MIT
chulheey@mit.edu
Srinadh Bhojanapalli
Google Research NY
bsrinadh@google.com
Ankit Singh Rawat
Google Research NY
ankitsrawat@google.com
Sashank J. Reddi
Google Research NY
sashank@google.com
Sanjiv Kumar
Google Research NY
sanjivk@google.com
Ab stract
Despite the widespread adoption of Transformer models for NLP tasks, the ex-
pressive power of these models is not well-understood. In this paper, we establish
that Transformer models are universal approximators of continuous permutation
equivariant sequence-to-sequence functions with compact support, which is quite
surprising given the amount of shared parameters in these models. Furthermore,
using positional encodings, we circumvent the restriction of permutation equiv-
ariance, and show that Transformer models can universally approximate arbitrary
continuous sequence-to-sequence functions on a compact domain. Interestingly,
our proof techniques clearly highlight the different roles of the self-attention and
the feed-forward layers in Transformers. In particular, we prove that fixed width
self-attention layers can compute contextual mappings of the input sequences,
playing a key role in the universal approximation property of Transformers. Based
on this insight from our analysis, we consider other simpler alternatives to self-
attention layers and empirically evaluate them.
1	Introduction
Self-attention based Transformer networks (Vaswani et al., 2017) have been at the center of the
recent progress on various natural language processing (NLP) tasks, including machine translation
(Vaswani et al., 2017), language modeling (Radford et al., 2018; 2019), and question answering
(Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019). All these tasks involve learning models that
map an input sequence of tokens to an output sequence of tokens. Transformers make it feasible
to train large models to approximate these sequence-to-sequence functions due to their ability to
process the input tokens in a parallel way, as opposed to the sequential nature of RNNs and LSTMs.
A Transformer block consists of two kinds of layers: a self-attention layer and a token-wise feed-
forward layer, with skip connections present in both layers. The self-attention layer transforms each
input token embedding using a weighted combination of the embeddings of all tokens in the input
sequence, where weights are generated by pairwise dot-products among the input token embeddings.
The token-wise feed-forward layer then independently processes each of these modified input token
embeddings without any interaction among them. Notably, Transformers employ parameter reuse
across tokens, as both layers use the same parameters to process each token. Moreover, Transformers
have to rely solely on the pairwise dot-products to capture interaction between the input tokens.
Given the parameter sharing and limited interactions between tokens, it is natural to wonder: what
class of sequence-to-sequence functions can the Transformer networks represent? Also, what is the
role of the two different kinds of layers? Are both layers needed to obtain the representation power
of Transformers? In the existing literature, the advantage of Transformers has often been attributed
to their capability of computing contextual embeddings/mappings of the input, as opposed to fixed
word embeddings as in word2vec (Mikolov et al., 2013). Is it possible to formalize the notion of
* Based on work performed at Google Research New York
1
Published as a conference paper at ICLR 2020
contextual mappings? If yes, can Transformers actually compute such mappings? Such questions
still remain elusive.
In this paper, we provide a mathematical definition of contextual mappings and show that multi-head
self-attention layers can indeed compute contextual mappings of the input sequences. We further
show that this ability to compute contextual mappings coupled with the value mapping ability of the
feed-forward layers makes Transformers universal approximators of any permutation equivariant
sequence-to-sequence function. We also improve this result using positional encodings, and show
that Transformers can represent any sequence-to-sequence function; i.e., the restriction of permuta-
tion equivariance can be removed by positional encodings.
These results on universal approximation of sequence-to-sequence functions raise a natural question:
is it possible to have a more efficient architecture to compute contextual mappings, consequently,
preserving the ability to universally approximate sequence-to-sequence functions? Towards this, we
explore other architectures that can implement contextual mappings (to some extent), and experi-
mentally evaluate their performance. In our experiments, we notice that the models that combine
these simpler architectures with Transformers have better performance, compared to the standalone
Transformers. We conclude the paper by presenting more discussion and interesting future research
directions along these lines.
1.1	Summary of our contributions
•	We prove that Transformers are universal approximators of continuous and permutation equiv-
ariant sequence-to-sequence functions with compact support (Theorem 2). We also show that,
if Transformers have trainable positional encodings added to the input, then they are universal
approximators of continuous sequence-to-sequence functions on a compact domain (Theorem 3).
•	We formalize the notion of contextual mappings and show that the attention layers can compute
contextual mappings, where each unique context is mapped to a unique vector (Lemma 6).
•	We experimentally evaluate other simpler layers that can compute contextual mappings to some
extent, such as bi-linear projections and separable convolutions, and show that substituting some
of the self-attention layers with these layers can result in better performance (Section 5).
1.2	Related works & notation
Analysis of attention-based models. Given the popularity of Transformers, there have been numer-
ous works trying to understand the role of attention layers in natural language processing models.
One such line of work focuses on probing the output of attention layers to understand the attention
mechanism and internal language representation (Hewitt & Manning, 2019; Clark et al., 2019; Co-
enen et al., 2019; Vig & Belinkov, 2019). Although these results give valuable insights, a consistent
theoretical analysis corroborating these findings is missing.
Universal approximation theorems. Universal approximation theorems are classical results in neu-
ral network theory, dating back many decades (Cybenko, 1989; Hornik, 1991). These results show
that given unbounded width, a one-hidden-layer neural network can approximate arbitrary contin-
uous function with compact support, up to any accuracy. Other results focusing on depth appeared
more recently (Lu et al., 2017; Hanin & Sellke, 2017; Lin & Jegelka, 2018). In particular, Lu et al.
(2017); Hanin & Sellke (2017) consider fully-connected ReLU networks whose input dimension is
d, and show that networks with width d + 1 and unbounded depth are universal approximators of
scalar-valued continuous functions. Lin & Jegelka (2018) show that a residual network with one
hidden neuron per residual block is a universal approximator of scalar-valued functions, given un-
bounded depth. Although Transformer networks do have residual connections, due to their heavy
parameter sharing, the existing analyses for residual networks do not extend to Transformers. Sannai
et al. (2019) consider universally approximating permutation invariant/equivariant functions using
fully-connected ReLU networks.
Turing completeness results on Transformers. Recently, Perez et al. (2019) have shown that
Transformers with infinite precision are Turing complete, which is not the case in finite precision
setting (Dehghani et al., 2018). We note that Turing completeness deals with computation on formal
languages (thus discrete objects), while universal approximation focuses on functions on a contin-
uum. In other words, these are two different concepts; and one does not imply another.
2
Published as a conference paper at ICLR 2020
Notation. We use the following notation in the paper. Given a matrix A, let Ai,j, Ai,:, and A:,j
denote its (i,j)-th entry, i-th row, and j-th column, respectively. We use kAkp to denote the entry-
wise 'p norm of A. Let σ[∙] be the Softmax operator, which takes a matrix as input and applies
softmax operation to each column of the matrix, which results in a column stochastic matrix, i.e.,
a matrix that has non-negative entries with each column summing to 1. We similarly define σH[∙]
to be the hardmax operator, which outputs the one-hot representation of the arg max entry for each
column of the input matrix. If there are k arg max entries, then the output is 1/k for such entries. We
use 1n to denote a vector of length n whose entries are all 1. We denote the 0-1 indicator function
by 1 {∙}. We use d and n to denote the embedding dimension and the sequence length, respectively.
We assume throughout that n ≥ 2, as the Transformers reduce to residual networks when n = 1.
2	Transformer networks
A Transformer block is a sequence-to-sequence function mapping Rd×n to Rd×n . It consists of
two layers: a self-attention layer and a token-wise feed-forward layer, with both layers having a
skip connection. More concretely, for an input X ∈ Rd×n consisting of d-dimensional embeddings
of n tokens, a Transformer block with multiplicative or dot-product attention (Luong et al., 2015)
consists of the following two layers1:
Attn(X) = X + χi=1 WOWVX ∙ σ[(WKX)TWQX],	(1)
FF(X) = Attn(X) + W2 ∙ ReLU(W1 ∙ Attn(X) + bi IT) + b2lT,	(2)
where WOi ∈ Rd×m , WVi , WKi , WQi ∈ Rm×d, W2 ∈ Rd×r , W1 ∈ Rr×d , b2 ∈ Rd , b1 ∈ Rr, and
FF(X) is the output of the Transformer block. The number of heads h and the head size m are two
main parameters of the attention layer; and r denotes the hidden layer size of the feed-forward layer.
Here, we would like to point out that our definition of the self-attention layer (1) is an equivalent re-
formulation of (Vaswani et al., 2017), where they concatenate attention heads and multiply a matrix
WO ∈ Rd×mh to the concatenation. One difference in our setup is the absence of layer normaliza-
tion, which simplies our analysis while preserving the basic architecture of the Transformer.
We define the Transformer networks as the composition of Transformer blocks. The family of the
sequence-to-sequence functions corresponding to the Transformers can be defined as:
T h,m,r := {g : Rd×n → Rd×n | g is a composition of Transformer blocks th,m,r’s}.
where th,m,r : Rd×n → Rd×n denotes a Transformer block defined by an attention layer with h
heads of size m each, and a feed-forward layer with r hidden nodes.
We say that a function f : Rd×n → Rd×n is permutation equivariant if for any permutation matrix
P, we have f(XP) = f(X)P; i.e., if we permute the columns of X, then the columns of f(X)
are permuted in the same way. A Transformer block is permutation equivariant, which we formally
prove in Section A. This consequently establishes the permutation equivariance of the class Th,m,r.
Claim 1. A Transformer block th,m,r defines a permutation equivariant map from Rd×n to Rd×n.
As seen in above, both layers (cf. (1) and (2)) of a Transformer block employ parameter
reuse/sharing, because each token/column undergoes the same transformations (e.g., WQi , WKi , or
W1 ) regardless of its position. Moreover, interactions between tokens can only be captured through
pairwise dot-products in the softmax operator σ[∙] (cf.(1)). Given such limitations in a single Trans-
former block’s representation power, it is not obvious what kinds of sequence-to-sequence functions
T h,m,r can approximate; we provide the answer to this question in the next section.
3	Transformers are universal approximators of
sequence-to-sequence functions
In this section, we present our theorems showing that the Transformer networks are universal ap-
proximators of sequence-to-sequence functions. Let us start by defining the target function class
FPE, which consists of all continuous permutation equivariant functions with compact support that
1In our proof we use bias vectors biQ for query projections in attention layers. We omit them here for brevity.
3
Published as a conference paper at ICLR 2020
map Rd×n to Rd×n . Here, continuity is defined with respect to any entry-wise `p norm, 1 ≤ p < ∞.
Given two functions f1 , f2 : Rd×n → Rd×n, for 1 ≤ p < ∞, we define a distance between them as
dp(f1,f2) := Z kf1(X)-f2(X)kppdX1/p.
The following result shows that a Transformer network with a constant number of heads h, head size
m, and hidden layer of size r can approximate any function in FPE .
Theorem 2.	Let 1 ≤ p < ∞ and > 0, then for any given f ∈ FPE, there exists a Transformer
network g ∈ T 2,1,4, such that dp (f, g) ≤ .
Next, we present our theorem on Transformers with positional encodings. In order to endow the
Transformer networks with the ability to capture the information about the position of tokens in the
input sequence, itis a common practice to add positional encodings E ∈ Rd×n to the input sequence
before feeding it to the Transformer network (Vaswani et al., 2017; Devlin et al., 2018). Consider
the functions represented by Transformers with positional encodings:
TPh,m,r := {gP(X) =g(X+E) | g ∈ T h,m,r andE ∈ Rd×n}.
Here we show that if E is trainable, these positional encodings are sufficient to remove the permu-
tation equivariance restriction of the Transformers. Towards this, we define FCD to be the set of all
continuous functions that map a compact domain in Rd×n to Rd×n . Note that FCD does not have
the restriction of permutation equivariance as in FPE, but any f ∈ FCD is defined on a compact
domain instead of the whole Rd×n . The following result states that, equipped with the trainable
positional encodings, Transformers can approximate any sequence-to-sequence function in FCD .
Theorem 3.	Let 1 ≤ p < ∞ and > 0, then for any given f ∈ FCD, there exists a Transformer
network g ∈ TP2,1,4 such that we have dp(f, g) ≤ .
Theorems 2 and 3 provide an interesting characterization of the representation power of fixed-width
Transformer networks. Since the function classes T h,m,r and TPh,m,r become richer as we increase
the values of (h, m, r), our results establish that general Transformer networks are also universal
approximators of sequence-to-sequence functions. Remarkably, none of the parameters (h, m, r)
depend on the input sequence length n or embedding dimension d.
Here, we would like to again point out that Theorems 2 and 3 appear quite surprising at a first glance,
given the parameter sharing across all the tokens in a sequence, e.g., feed-forward layers are applied
token-wise and the projection matrices in the self-attention layers are the same across different
tokens. Furthermore, attention layers can only capture pairwise interaction between different tokens
in the sequence. In the next subsection, we briefly describe one of our key steps in overcoming the
aforementioned restrictions and proving universal approximation power of Transformers.
3.1	A key step: self-attention layers can implement contextual mappings
Let us consider a setting where we are interested in embedding two sentences: 1) I am happy; and
2) I am Bob. These sentences are fed to a sequence-to-sequence model as
X = [X:,1, X:,2, X:,3] = [vI, vam, vhappy] and X = [X:,1, X:,2, X:,3] = [vI, vam, vBob],
where vI, vam, vhappy, and vBob denote d-dimensional embedding for the tokens ‘I’, ‘am’, ‘happy’,
and ‘Bob’, respectively. Since the word ‘I’ occurs in different contexts in these sentences, in order to
implement arbitrary sequence-to-sequence functions, the sequence-to-sequence model should map
the two occurrences of ‘I’ to different values. We formally define this requirement below.
Definition 3.1 (Contextual mapping). Consider a finite set L ⊂ Rd×n. A map q : L → R1×n
defines a contextual mapping if the map satisfies the following:
1.	For any L ∈ L, the n entries in q(L) are all distinct.
2.	For any L, L0 ∈ L, with L 6= L0, all entries of q(L) and q(L0) are distinct.
In other words, a contextual mapping maps each token (column) of L ∈ L to a unique value which
depends on the entire L; as a result, capturing the precise context of L. This allows the subsequent
4
Published as a conference paper at ICLR 2020
token-wise function (e.g., defined by the feed-forward layers in case of Transformer networks) to
realize the outputs of any arbitrary sequence-to-sequence functions.
At the first thought, we can consider getting a contextual mapping by simply averaging all the tokens,
because this can capture the one-word difference (e.g., “happy” vs. “Bob”) in two different contexts.
However, if there are multiple words that are different, it is not guaranteed that the average will be
different. Indeed, requiring unique mappings for all the tokens for any change in any number of
tokens, is a steep requirement.
While the self-attention layer does consider pair-wise interactions among different input tokens, it
is not clear if this weak form of pair-wise interaction with shared projection weights is sufficient to
extract the underlying context. The following result, which we sketch here, shows that self-attention
layers can implement a permutation equivariant contextual mapping over almost all elements of a
grid in [0, 1]d×n. We defer the full statement to Section 4.2.
Lemma 6 (informal). Consider the grid Gδ := {0, δ, . . . , 1 - δ}d×n. Then, there exist a function
gc : Rd×n → Rd×n composed of δ-d + 1 self-attention layers (h = 2, m = 1) anda vector u ∈ Rd
such that q(L) := uT gc (L) satisfies the following properties, for a subset Gδ ⊂ Gδ that contains
almost all elements of Gδ:
1.	For any L ∈ Geδ, the entries of q(L) are all distinct.
2.	For any L, L0 ∈ Gδ such that L is not a permutation of L0, all entries ofq(L), q(L0) are distinct.
Lemma 6 shows that a series of self-attention layers can implement contextual mappings, despite
the apparent restriction that each of them can only capture pair-wise interaction. However, the
restriction of permutation equivarance still exists because attention layers are inherently permutation
equivariant. Coupled with the ability of token-wise feed-forward layers to map different values in
q(L) to arbitrary output values, we can prove universal approximation capability of Transformers.
3.2 Proof of the universal approximation theorem (Theorem 2)
Next, we outline the proof of Theorem 2 in greater detail. We refer the reader to Section C for the
proof of Theorem 3, since it is a modification of Theorem 2. Even though Theorems 2 and 3 do not
specifically mention the required depth for approximation, our proof techniques do characterize it,
and we show that our construction is tight in the number of parameters. We defer the discussion of
depth to Section 4.4.
Recall that we want to show that given a function f ∈ FPE , we can find a Transformer network
g ∈ T 2,1,4 such that dp(f, g) ≤ . Without loss of generality, we can assume that the compact
support of f is contained in [0, 1]d×n. We achieve our desired objective in three key steps:
Step 1. Approximate FPE with piece-wise constant functions. We first use (a variant of) the
classical result that any continuous function can be approximated up to arbitrary accuracy by piece-
wise constant functions. For δ > 0, we define the following class of piece-wise constant functions.
FPE(δ) ：= f ： X → XLeG
L∈Gδ
ALI {X ∈ Sl} | f is permutation equivariant, AL ∈ Rd×n},
where Gδ ：= {0, δ,..., 1 - δ}d×n and, for a grid point L ∈ Gδ, SL := Qd=ι Qn=ι[Lj,k,Lj,k +
δ) ⊂ [0,1]d×n denotes the associated cube of width δ. Let f ∈ F PE(δ) be such that dp(f,f) ≤ e/3.
Step 2. Approximate Fpe(6) with modified Transformers. We then consider a slightly modified
architecture for Transformer networks, where the softmax operator σ[∙] and ReLU(∙) are replaced by
the hardmax operator σH [∙] and an activation function φ ∈ Φ, respectively. Here, the set of allowed
activations Φ consists of all piece-wise linear functions with at most three pieces, where at least one
piece is constant. Let Th,m,r denote the function class corresponding to the sequence-to-sequence
functions defined by the modified Transformer networks. The following result establishes that the
modified Transformer networks in T2,1,1
can closely approximate functions in Fpe(6).
Proposition 4. For each f ∈ FpE(δ) and 1 ≤ p < ∞, ∃ g ∈ T2,1,1 such that dp(f ,g) = O(6d/P).
Step 3. Approximate modified Transformers with (original) Transformers. Finally, we show
that g ∈ T , , can be approximated by T2,1,4. Let g ∈ T2,1,4 be such that dp(g,g) ≤ e/3.
5
Published as a conference paper at ICLR 2020
Theorem 2 now follows from these three steps, because we have
dp(f,g) ≤ dp(f,f)+ dp(f,g) + dp(g,g) ≤ 2e∕3 + O(δdp).
Choosing δ small enough ensures that dp(f, g) ≤ e.	□
We refer the reader to Sections B.1 and B.2 in the supplementary material for the formal statements
and proofs of Steps 1 and 3, respectively. As for Step 2, which is the most critical step in estab-
lishing the universal approximation property of Transformers, we provide a sketch of the proof of
Proposition 4 in the next section, and refer the reader to Section B.3 for the complete proof.
4 Proof sketch of Proposition 4: different roles of two layers
As mentioned earlier, the heavy parameter sharing in Transformers makes the goal of universally
approximating sequence-to-sequence functions seemingly difficult. Both the self-attention and the
feed-forward layer weights inside a Transformer block are fixed across n tokens. In this section, we
show that Transformers are able to overcome this architectural constraint, and compute contextual
mappings of the entire input sequence just based on the pair-wise interactions. The token-wise
feedforward layers then transform these contextual mappings to the desired output sequence.
We highlight these inner workings of Transformers en route to proving Proposition 4. We want to
show that given a piece-wise constant function f ∈ FPE (δ), there exists a modified Transformer
network g ∈ T2,1,1 that closely approximates f. We achieve this goal by establishing the following
three claims, which correspond to Lemmas 5, 6, and 7.
1.	Given an input X ∈ Rd×n, a series of feed-forward layers in the modified Transformer network
can quantize X to an element L on the extended grid Gδ+ := {-δ-nd, 0, δ, . . . , 1 - δ}d×n.
2.	Next, a series of self-attention layers in the modified Transformer network can take the input L
and implement a contextual mapping q such that, for L and L0 that are not permutation of each
other, all the elements in q(L) and q(L0) are distinct.
3.	Finally, a series of feed-forward layers in the modified Transformer network can map elements
of the contextual embedding q(L) to the desired output value of f ∈ FPE at the input X.
Before discussing these three claims in detail, we note that even though a Transformer network
stacks self-attention and feed-forward layers in an alternate manner, the skip connections enable
these networks to employ a composition of multiple self-attention or feed-forward layers. Further-
more, as alluded earlier, these three steps clearly highlight the different roles that self-attention and
feed-forward layers play in realizing the ability to universally approximate sequence-to-sequence
functions: 1) self-attention layers compute precise contextual maps; and 2) feed-forward layers then
assign the results of these contextual maps to the desired output values.
4.1 Quantization by feed-forward layers
Since our objective in Proposition 4 is to approximate the function f ∈ Fpe(6), which takes a con-
stant value on the cubes Sl's, the (modified) Transformer network approximating f first quantizes
the input X according to these cubes. In particular, we want each input X ∈ SL to be mapped to
the point L. The following result shows that a modified Transformer network can indeed implement
this quantization map with a composition of multiple feed-forward layers.
Lemma 5. Consider a scalar quantization map gqent : R → {-δ-nd, 0, δ, . . . , 1 - δ}:
ent	kδ	if kδ ≤ t < (k	+ 1)δ,	k = 0, . .	. , 1∕δ - 1,
gq	-δ-nd	otherwise.
There	exists a function	gq :	Rd×n	→	G+ composed	of J +	d token-wise feed-forward layers	with
r =	1 and activations in Φ,	which	employs the scalar	quantization gqent	to each entry of its input.
As desired, the function gq maps any X ∈ SL to L. Furthermore, if any element of X is not in [0, 1],
the element is mapped to -δ-nd, indicating that X is outside the compact support of f ∈ Fpe(6).
6
Published as a conference paper at ICLR 2020
4.2	Contextual mapping by self-attention layers
In this subsection, we show that the (modified) Transformer network can compute contextual map-
pings (cf. Definition 3.1) from the output L ∈ Gδ+ of the map gq (cf. Section 4.1) by using a com-
position of self-attention layers. The following lemma, sketched earlier in Section 3.1, shows that
the (modified) Transformer networks can implement a permutation equivariant contextual mapping
over almost all elements of Gδ , while mapping the rest of elements in Gδ+ to a disjoint set.
Lemma 6. Consider the following subset of Gδ = {0, δ, . . . , 1 - δ}d×n:
Geδ := {L ∈ Gδ | L:,i 6= L:,j for all i 6= j}.
Assume that n ≥ 2 and δ-1 ≥ 2. Then, there exist a function gc : Rd×n → Rd×n composed
of δ-d + 1 self-attention layers (h = 2, m = 1) that employ the σH operator, a vector u ∈ Rd,
constants tl, tr ∈ R (0 < tl < tr), such that q(L) := uT gc (L) satisfies the following properties:
1.	For any L ∈ Geδ, the entries of q(L) are all distinct.
2.	For any L, L0 ∈ Gδ such that L is not a permutation of L0, all entries ofq(L), q(L0) are distinct.
3.	For any L ∈ Geδ, all the entries of q(L) are in [tl, tr].
4.	For any L ∈ Gδ+ \ Geδ, all the entries of q(L) are outside [tl, tr].
At this point, a few remarks about the result in Lemma 6 are in order. First, since the Transformer
networks are bound to implement permutation invariant maps, we require the Property 6.2 to hold
for the pair of sequences that cannot be mapped to each other via permutation of columns. Further-
more, the self-attention layers implement the desirable contextual map for only Gδ ⊆ Gδ, where
all columns of L are distinct. Note that for small δ, Gδ \ Ge δ constitutes a negligible fraction of Gδ
because ∣Gδ \ Gδ | = O(δd∣Gδ |). The function q in Lemma 6 maps the elements of Gj \ Gδ outside
[tl , tr]—the interval where the outputs of the contextual mapping for Gδ reside.
4.	2.1 Proof sketch of Lemma 6
Since Lemma 6 is one of the major technical contributions of this paper, we provide a short sketch
of its proof. The complete proof is presented in Section B.5. For simplicity, we consider the case
d = 1, so the input L ∈ Gδ+ is a row vector of length n.
The key idea of the proof is that, using two attention heads of size 1, one can implement a self-
attention layer that shifts up input entries that are in a specific interval, while leaving all other entries
intact. We call this the selective shift operation. Since the entries in L are quantized, we apply the
selective shift operation to 0,δ,..., 1 - δ using 1∕δ attention layers. Interestingly, the value of the
largest output entry after these operations is unique for each L ∈ Gδ up to permutations. Using the
largest entry, one can add one last layer that shifts up the entire matrix and outputs q(L) that satisfies
Properties 6.1 and 6.2 of the lemma.
More concretely, the following function Ψ : R1×n → R1×n, parametrized by b, b0 ∈ R satisfying
b < b0, can be implemented with two attention heads of size 1 with the hardmax (σH) operator:
Ψ(Z; b,b0)ι,j
maxk Z1,k - mink Z1,k
0
if b < Z1,j < b0,
if Z1,j < b or Z1,j > b0.
If we define an attention layer of the form Z 7→ Z + Ψ(Z; b, b0), then any entry Z1,j in (b, b0) is
shifted up by maxk Z1,k - mink Z1,k, while all the other entries stay untouched. We can choose b
and b0 to selectively shift certain entries, hence the name selective shift operation.
We stack 1∕δ self-attention layers, with attention parts δ-1Ψ(∙; l - δ∕2,l + δ∕2) for each l ∈
{0, δ, . . . , 1 - δ}, in increasing order of l. With these layers, we can apply the selective shift
operations to input entries of values 0, δ, . . . , 1 - δ. To see how the shift operations modify the
input, now consider n = 2 for simplicity, and let L = [l1 l2] ∈ Ge δ . Without loss of gener-
ality, we can assume l1 < l2 . The selective shift operation is applied to l1 first, shifting it by
δ-1(max L - min L) = δ-1(l2 - l1), resulting in l1 = l1 + δ-1(l2 - l1) > l2. After that, the
operation on l2 shifts it up by δ-1(el1 - l2). Thus, the first 1∕δ layers map L = [l1 l2] (l1 < l2) to
Le =	el1	el2	:= l1 + δ-1 (l2	-	l1)	l2	+ (δ-2 -	δ-1)(l2	-	l1)	.
7
Published as a conference paper at ICLR 2020
We can show that the map from [l1 l2] ∈ {L ∈ Gδ | l1 < l2 } to l2 is one-to-one, and that
23
0 < l1 < l2 < δ-2. We then add one last layer that shifts all positive entries of L by δ-3 maxL =
δ-3el2, whose output we denote by q(L) = δ-3el2 + el1 δ-3el2 + el2 . All entries of q(L) are in
[δ-3el2 , δ-3el2 + δ-2), and this interval is disjoint for different L’s because L 7→ el2 is one-to-one.
Thus, q(L) satisfies Properties 6.1 and 6.2 of the lemma. The remaining details are in Section B.5.
4.3	Function value mapping by feed-forward layers
This brings us to the final step, which demonstrates the key utility of the feed-forward layers. After
the contextual mapping by self-attention layers, each token captures the entire context available in
the input sequence. The following result shows that token-wise application of a composition of
feed-forward layers can map these tokens to the desired output values required by the function f.
Lemma 7. Let gc : Rd×n → Rd×n be the function from Lemma 6. Then, there exists a function
gv : Rd×n → Rd×n composed of O(n(δ)dn∕n!) token-wise feed-forward layers (r = 1) with
activations in Φ such that gv is defined by a token-wise function gvtkn : Rd → Rd on each column,
gv (Z)= [gVkn (Z：,1)…gVkn(Z,n)],
where for all j ∈ {1, . . . , n},
tkn( (L) )= ((AL):,j	ifL∈Geδ,
gv gc	:,j	0d	ifL ∈ Gδ+ \ Geδ.
4.4	Tightness of constructions
We showed in this section that Theorem 2 requires O(n(1∕δ)dn∕n!) Transformer blocks for ap-
proximation, where δ is the width of the cubes. Each transformer block is of constant width, so
it has O(d) parameters; this means that the total number of parameters is O(dn(1∕δ)dn/n!). We
note that this exponential dependence cannot be avoided in the worse case. If we assume continuity
without any additional smoothness, quantizing the domain to cubes and approximating the function
with constants require memorizing (output dim) × (num cubes)/n! real numbers, where the factor
of 1/n! is due to permutation equivariance. Thus, Theorem 2 is optimal in the order of parameters.
If we compare with the residual network result (Lin & Jegelka, 2018), we can consider “flattening”
X into a dn-dimensional vector and fitting the function. The proof technique in (Lin & Jegelka,
2018) requires O((1∕δ)dn) layers, where each layer has O(dn) parameters: the total parameter re-
quirement is O(dn(1∕δ)dn). This shows that Transformers can approximate permutation equivariant
functions in a more efficient way than residual networks.
In Section C, our proof of Theorem 3 shows that we require O(n(1∕δ)dn) layers to approximate con-
tinuous (not permutation equivariant) sequence-to-sequence functions. As seen from the argument
above, this construction is also optimal in the order of parameters.
5 Discussion and Experiments
As detailed in Section 4, the ability of the self-attention layers to compute contextual mappings
plays a crucial role in the universal approximation property. Interestingly, our analysis shows that
replacing the dot-product attention in Transformers with any other component capable of computing
contextual mappings should preserve this universal approximation property. This leads naturally to
questions about the alternative architectures that realize certain kinds of contextual mappings at dif-
ferent computational and memory costs. We explore and discuss some examples of such alternatives
in this section. Our preliminary empirical study demonstrates their practical utility.
5.1	Bi-linear projection
Given token embeddings X as input, the bi-linear projection layer computes the following update.
BProj(X) = X + WO ∙ X ∙ Wp.
The bi-linear projection layer (Gong et al., 2013) is motivated from the ability of random (Gaussian)
matrices to map sparse differences to dense vectors (Ailon & Chazelle, 2009). If there are two input
8
Published as a conference paper at ICLR 2020
(a) SQuAD
Figure 1: Performance of hybrid models constructed by first taking BERTBASE, a 12 layer Trans-
former model, and replacing the self-attention layers with depth-wise separable convolution layers,
in a varying number of the Transformer blocks closer to the input. Surprisingly, replacing 1 or 2
self-attention layers with convolutions improves the performance, while replacing more hurts the
performance. This suggests both that Transformers have functionality beyond just computing con-
textual mappings, and having simpler layers to realize contextual mapping can aid Transformers.
(b) MNLI
contexts X1 and X2 that differ in one token, their difference X1 - X2 is sparse; however, after
random projection, the difference (X1 - X2)WP will be dense, and the numbers are distinct with
high probability, implementing a form “pair-wise contextual mapping,”2 although different from the
contextual mapping in Definition 3.1.
This layer advantageously incurs smaller number of matrix multiplications as compared to the dot-
product attention. That said, the number of parameters in this layer depend on the sequence length,
making it harder to reuse the model across tasks with different input sequence lengths. Moreover,
the weights used to compute the contextual embeddings (WP ) are independent of the inputs (X),
whereas in self-attention the weights (σ[(WKi X)T WQi X]) depend on X. The first drawback can
be addressed by replacing the linear projection with a depth-wise separable convolution layer, which
is discussed in the next subsection.
5.2	Depth-wise separable convolutions
A depth-wise convolution layer (Sifre & Mallat, 2014; Chollet, 2017; Kaiser et al., 2017) involves
convolving each dimension of X with a corresponding convolution filter of size k:
SepConv(X) = X + WO (X * WC),
where WC ∈ Rd×k and (X * WC)i: := Xi,： * (WC)i,：. Unlike bi-linear projection, this layer
can be used across tasks with different input sequence lengths as the number of parameters are
independent of the sequence length. While a single layer is unable to compute contextual mappings
when the filter size is small, stacking multiple such layers can potentially provide a cheaper way
to compute contextual mappings. In fact, based on depth-wise separable convolutions, Wu et al.
(2019) proposed a light-weight dynamic convolution architecture that performs competitively with
Transformers on machine translation.
5.3	Experiments
We now present our experiments with these other architectures, with the goal of understanding the
extent to which computing contextual mappings can capture the performance of Transformers. As
discussed earlier, BProj and SepConv do not implement contextual mappings (cf. Definition 3.1),
so we do not expect that either BProj or SepConv based models to have the same performance as
the expensive Transformers. These models do not use input dependent weights to compute attention,
and hence have weaker representation power. Instead, our goal is to see if we can use these cheaper
layers to replace (some of) the expensive self-attention layers.
2This guarantee only holds for a finite set (can be exponential in n) of fixed vectors in Rn .
9
Published as a conference paper at ICLR 2020
We follow the experimental setting from Devlin et al. (2018) to train the Transformers, with the
masked language model pre-training followed by a task specific fine-tuning, and work with a 12 layer
architecture based on BERTBASE. We present our results on a question answering task (SQuAD)
(Rajpurkar et al., 2016) and a sentence entailment task (MNLI) (Williams et al., 2018). In our
first set of experiments we train models that employ BProj and SepConv layers, instead of the self-
attention layer in eq.(1). We notice that, as expected, these simpler models have weaker performance
than the self-attention layer. See Table 1 in Section D for a comparison of these models on MNLI.
Next, we swap a varying number of the first few self-attention layers in BERTBASE with SepConv,
implemented with filter reuse across dimensions (Wu et al., 2019)3. Fig. 1 illustrates the perfor-
mance of these hybrid models. Interestingly, models with 1 or 2 convolution layers and rest the
self-attention layers, perform better than models with only the self-attention layers. Note that, re-
placing self-attention layer with SepConv also reduces the computational cost and the number of
parameters. One explanation we have is that the first few attention layers tend to attend broadly to
the whole sequence (as empirically observed in (Clark et al., 2019)), and the cheaper convolution
layers can perform this job more efficiently. A detailed evaluation of such hybrid architectures will
be interesting future research.
Our experiments also call for a deeper understanding of the exact nature of the embeddings com-
puted by practical attention models. Since Transformers in practice have fixed depth, we believe
that they might not be able to exactly implement contextual mappings as we defined in Defini-
tion 3.1. However, there is some preliminary empirical evidence that Transformers do implement
some sort of “contextual mappings.” For example, Fig. 4 of Coenen et al. (2019) presents visualiza-
tions of embeddings of a single word in different contexts (sentences). They experimentally notice
that Transformers, in addition to computing contextual mappings, also map a word into semantic
clusters. Formalizing and evaluating this property of Transformers is an interesting direction for
future work. We again note that Wu et al. (2019) have proposed an alternative way to compute such
embeddings based on dynamic convolution layers. Evaluating the mappings computed by these
models should shed more light on the workings of attention models and inspire efficient and better
performing architectures.
References
Nir Ailon and Bernard Chazelle. The fast Johnson-LindenstraUss transform and approximate nearest
neighbors. SIAM Journal on Computing, 39(1):302-322, 2009.
Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 1251-1258, 2017.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does BERT look
at? an analysis of BERT’s attention. arXiv preprint arXiv:1906.04341, 2019.
Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viegas, and Martin Wat-
tenberg. Visualizing and measuring the geometry of BERT. arXiv preprint arXiv:1906.02715,
2019.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Eukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Yunchao Gong, Sanjiv Kumar, Henry A Rowley, and Svetlana Lazebnik. Learning binary codes
for high-dimensional data using bilinear projections. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 484-491, 2013.
Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
3We refer to Section D for a complete description of the setup.
10
Published as a conference paper at ICLR 2020
John Hewitt and Christopher D Manning. A structural probe for finding syntax in word representa-
tions. In Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-
pers), pp.4129-4138,2019.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Lukasz Kaiser, Aidan N Gomez, and Francois Chollet. Depthwise separable convolutions for neural
machine translation. arXiv preprint arXiv:1706.03059, 2017.
Hongzhou Lin and Stefanie Jegelka. ResNet with one-neuron hidden layers is a universal approxi-
mator. In Advances in Neural Information Processing Systems, pp. 6169-6178, 2018.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-
training approach. arXiv preprint arXiv:1907.11692, 2019.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Advances in neural information processing systems,
pp. 6231-6239, 2017.
Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-
based neural machine translation. In Empirical Methods in Natural Language Processing
(EMNLP), pp. 1412-1421, Lisbon, Portugal, September 2015. Association for Computational
Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111-3119, 2013.
Jorge Perez, Javier Marinkovic, and Pablo Barcelo. On the Turing completeness of modern neural
network architectures. arXiv preprint arXiv:1901.03429, 2019.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. Technical Report, OpenAI, 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. Technical Report, OpenAI, 2019.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2383-2392, 2016.
Akiyoshi Sannai, Yuuki Takai, and Matthieu Cordonnier. Universal approximations of permutation
invariant/equivariant functions by deep neural networks. arXiv preprint arXiv:1903.01939, 2019.
Laurent Sifre and StePhane Mallat. Rigid-motion scattering for image classification. Ph. D. disser-
tation, 2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
LUkaSz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998-6008, 2017.
Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language
model. arXiv preprint arXiv:1906.04284, 2019.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pp. 1112-1122. Association for Computational Linguistics, 2018. URL
http://aclweb.org/anthology/N18-1101.
11
Published as a conference paper at ICLR 2020
Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with
lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V.
Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019.
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and
Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching
movies and reading books. In Proceedings of the IEEE international conference on computer
vision,pp.19-27, 2015.
12
Published as a conference paper at ICLR 2020
A Proof of Claim 1
Suppose XP was given as input, where P is a permutation matrix. First note that
(WKiXP)T(WQiXP)=PT(WKiX)T(WQiX)P
After the softmax operation, we get
σ[PT(WKiX)T(WQiX)P] =PTσ[(WKiX)T(WQiX)]P.
Then,
h
Attn(XP) = XP + X WO(WVXP) ∙ PTσ[(W^X)T(WQQX)]P = Attn(X)P,
i=1
where we used PPT = I . Permutation equivariance of the token-wise feed-forward layer can be
shown similarly:
FF(XP) = Attn(X)P +	W2	∙	ReLU(W1	∙ Attn(X)P + bι1TP) + b2ITP
=Attn(X)P +	W	∙	ReLU(W1	∙ Attn(X) + .1T)P + b2ITP =	FF(X)P,
where ReLU(XP) = ReLU(X)P Was used. This analysis shows thatthefUnctionclass Th，m，r(∙)
is restricted to permutation equivariant functions.
B Proof details of Theorem 2
We first define some additional notation. For a, b ∈ N where a ≤ b, let [a] = {1, . . . , a} and
[a : b] = {a, a + 1, . . . , b - 1, b}. For a, b, c ∈ R where b - a > 0 is an integer multiple of c > 0,
we write [a : c : b] := {a, a + c, a + 2c, . . . , b - c, b}.
B.1 Approximating FPE WITH Fpe(6)
Lemma 8. For any given f ∈ FPE and 1 ≤ p < ∞, one can find a δ* > 0 such that ∃ f ∈ Fpe(6*)
which satisfies dp(f, f) ≤ e/3.
Proof Since f : Rd×n → Rd×n is a continuous function with compact support, the function
is uniformly continuous. Since continuity is defined using entry-wise `p norm, and entry-wise `p
norm is equivalent to entry-wise '∞ norm when the number of entries are finite, uniform continuity
implies that
∀e > 0,∃δ > 0suchthat∀X,Y, kX -Y k∞ < δ =⇒ kf(X) -f(Y)kp < e.
This means that given any e/3 > 0, we have such a δ > 0. Using this δ, we can create a grid Gδ
and corresponding cubes SL, as described in the main text. For any L ∈ Gδ, we define CL ∈ SL
to be the center point of the cube Sl. Then, we can define a piece-wise constant approximation
f(X) = PL∈Gδ f (Cl)I {X ∈ Sl}. Notethat,for any X ∈ Sl, we have IlX - Cl∣∣∞ < δ, so
by uniform continuity, we have ∣∣f (X) 一 f (X)}=∣∣f (X) 一 f (。工)|，< e/3. This proves that
dp(f,f) <e/3.
As for permutation equivariance, since f is permutation equivariant, we have f(CLP) = f(CL)P
for any permutation matrix P. For any X ∈ SL, we have XP ∈ SLP, so
f (XP) = f (Clp ) = f (ClP ) = f (Cl)P = f (X )P∙
Thus, the approximation f is also permutation equivariant. This proves the lemma.	□
13
Published as a conference paper at ICLR 2020
B.2 Approximating T2,1,1 WITH T 2，1,4
Lemma 9. For each g ∈ T2,1,1 and 1 ≤ p < ∞, ∃ g ∈ T2,1,4 such that dp(g, g) ≤ e/3.
Proof Recall that T h,m,r refers to the class of functions representable with composition of Trans-
former blocks with h heads of size m in self-attention layers and r hidden nodes in feed-forward
layers. The same notation holds for the modified Transformers Th,m,r.
Note that the softmax operator on a matrix A can be made arbitrarily close to hardmax by scaling
up A. That is,
σ [λA] → σH [A] as λ → ∞.
This means that by scaling up parameters inside σ, we can approximate σH arbitrarily closely. Thus,
the modified self-attention layers can be approximated with the original self-attention layers of the
same number of heads h and head size m.
Also, any arbitrary (possibly discontinuous) piecewise linear function φ ∈ Φ can be approximated
arbitrarily closely by four ReLU’s. Note that φ ∈ Φ as at most three pieces, and at least one of the
pieces is constant. For example, consider the following function φ ∈ Φ:
Γbι	if t < ci,
φ(t) =	a2t + b2	ifc1 ≤ t < c2,
la3t + b3	if c2 ≤ t.
This function can be approximated by four ReLU’s, as claimed by the lemma:
e(t) = bi + a2c1 +eb2 - bi ReLU(t - ci + e)+ 卜2 - a2c1 +：2 - b1) ReLU(t - ci)
+ ( a3c2 + b3- a：(C2-e)-b2 - a2) ReLU(t - e2 + e)
+ (a3 - a3c2 + b3 - a：(C2 -e)-b2 ) ReLU(t - c2)
b1	if t < C1 — e,
a2c1+b2-^bι(t — c1) + a2c1 + b2	if C1 — e ≤ t < C1,
= a2t + b2	ifc1 ≤ t < c2 - e,
a3。2+.— ©-12 (t - C2)+ a3C2 + b3	if C2 - e ≤ t<C2,
、a3t + b3	if C2 ≤ t.
Also, as we make e → 0, we can approximate φ as closely as possible using φ. The cases where the
second or third piece is constant can be shown similarly. This means that the modified feed-forward
layers (whose activation is φ ∈ Φ) with single hidden node can be approximated with the original
feed-forward layers (ReLU) with four hidden nodes.
Thus, given any g ∈ T2,1,1, there exists a function g ∈ T2,1,4 arbitrarily close to g, by appropriately
choosing the parameters to be large enough. This finishes the proof.	□
B.3	Finishing proof of Proposition 4
As we have already discussed in Section 4, we establish proposition 4 in three steps:
1.	Given an input X, a group of feed-forward layers in the modified Transformer network can
quantize X to an element L on the extended grid Gδ+ := {-δ-nd, 0, δ, . . . , 1 - δ}d×n.
2.	Next, a group of self-attention layers in the modified Transformer network can take the
input L and produce desirable contextual mappings q(L) such that, for L and L, that are
not permutation of each other, all the elements in q(L) and q(L) are distinct.
14
Published as a conference paper at ICLR 2020
3.	Finally, a group of feed-forward layers in the modified Transformer network can map ele-
ments of the contextual embedding q(L) to the desirable values, i.e., the output of f ∈ FPE
on the input X .
These steps are formally stated in Lemmas 5, 6, and 7 in the main text. We present the proofs of
these lemmas in the subsequent sections.
With the results established in these lemmas, we are now equipped with all the tools necessary to
complete the proof of Proposition 4. Let us recall the functions gq , gc, and gv from Lemma 5, 6,
and 7, respectively. We now show that the (modified) Transformer network g = gv ◦ gc ◦ gq
approximates the underlying peicewise constant function f ∈ FPE over all points in its support
except for a set of of measure O(δd).
Consider a point X ∈ SL ⊂ [0, 1]d×n, where L ∈ Geδ. By Lemma 5, we have that gq(X) = L.
Thus, it follows from Lemmas 6 and 7 that
gv ◦ gc ◦ gq(X ) = gv ◦ gc (L)= [gVkn(gC(L)∙,I) gVkn(gc (L)∙,2) …gVkn(gc(L)In)] = AL ∙
On the other hand, any point X ∈ SL∈G \Ge SL ∪ (Rd×n \ [0, 1]d×n) is mapped by gq to L ∈
Gδ+ \ Geδ; as a result, we get gv ◦gc ◦ gq(X) = gv ◦gc(L) = 0.
Therefore, we have g(X) = gv◦ gc ◦ gq(X) = AL = f(X) for X ∈ Ul∈gi5 Sl, and 0 everywhere
else. Recall that f has its compact support in [0,1]d, thus bounded; i.e., there exists B ≥ 0 such
that kf(X)kp ≤ B. The modified Transformer network g takes the same value as f on all points
in [0,1]d except for a set Ul∈g15∖Gδ SL that has measure O(δd). This implies that dp(f,g) ≤
(Bpδd)1∕p = O(δ"p).
B.4	Proof of Lemma 5
The proof strategy is simple; using ∣ + 1 token-wise feed-forward layers, we implement the quan-
tization function gent that works on the first row of the input. Then stack another δ + 1 layers that
quantizes the second row, and so on.
Given input X, we first start by clipping X1,: in the set (-∞, 0)∪[1, +∞) and mapping the intervals
to -δ-nd. This can be done by the following layer:
Z 7→ Z + e(1)φ((e(1))TZ), φ(t) = 0-t - δ-nd
if t < 0 or t ≥ 1,
otherwise.
Next, add 1∕δ layers of the following form, for k = 0,δ,..., 1 - δ.
Z7→Z+e(1)φ((e(1))TZ-kδ1Tn), φ(t) = 0 t<0ort≥δ
-t 0 ≤ t < δ.
Each layer quantizes X1,: in [kδ, kδ + δ) to kδ, without modifying other intervals.
Note that both φ's used in this construction are piecewise linear functions with three pieces, and at
least one of them are constant. Thus, both φ's are in Φ. We can repeat the same thing for the other
rows, and at the end we will get a map from Rd×n to Gδ+ .
B.5	Proof of Lemma 6
Selective shift operation. Before starting the proof, we first describe the key component of our
proof, which we refer to the selective shift operation. Consider the following function, which can
be expressed with a multiplicative attention head, with head size m = 1 and hardmax σH :
ψ(Z; bQ) = e(1)uT ZσH [(uT Z)T (uT Z - bQ 1Tn)]
where u ∈ Rd is a vector that we will choose later, and e(1) = (1, 0, 0, . . . , 0) ∈ Rd is the standard
basis vector.
15
Published as a conference paper at ICLR 2020
To see what this function computes, first consider the j-th column of the attention score matrix:
(uT Z)T (uT Z:,j - bQ). Note that, if uT Z:,j > bQ, σH will calculate arg max of uTZ, whereas if
uT Z:,j < bQ, it will calculate arg min. Therefore, the (1,j)-th entry of ψ(Z; bQ) ∈ Rd×n can be
written as
T	T T T	maxk uT Z:,k	ifuTZ:,j > bQ
ψ(Z; bQ)Ij=U Z σH[(u Z) (Uzj- bQ)] Tmink UT Zk	if UT Z：j < bQ
for j ∈ [n]. Note that due to e(1), all rows of ψ(Z; bQ) except the first row are zero. From this
observation, one can define a function parametrized by bQ and b0Q, where bQ < b0Q , which consists
of two attention heads:
Ψ(Z; bQ,bQ) := ψ(Z; bQ) - ψ(Z; bQ),
Ψ(Z; bQ,bQ)lj
maxk UT Z:,k - mink UT Z:,k
0
ifbQ < UT Z:,j < b0Q,
if UT Z:,j < bQ or UT Z:,j > b0Q .
What this means is that, if we define an attention layer of the form Z 7→ Z + Ψ(Z; bQ,bQ),
then any column Z:,j satisfying UT Z:,j ∈ (bQ, b0Q) is shifted up in its first coordinate Z1,j by
maxk UT Z:,k - mink UTZ:,k, while all the other coordinates stay untouched. We call this the
selective shift operation, because we can choose bQ and b0Q to selectively shift certain entries of the
input.
Bijective column id mapping. Recall that the input to this step is from the range of gq
(Lemma 5), which is Gδ+ = {-δ-nd, 0, δ, . . . , 1 - δ}d×n. Now consider L ∈ Gδ+ and U =
(1, δ-1, δ-2,..., δ-d+1).
For any j ∈ [n], it is easy to check two following facts:
1.	If Li,j 6= -δ-nd for all i ∈ [d], i.e., L:,j ∈ {0, δ, . . . , 1 - δ}d, then UT L:,j ∈ [0 : δ :
δ-d+1 - δ], and the map L:,j 7→ UT L:,j from {0, δ, . . . , 1 - δ}d to [0 : δ : δ-d+1 - δ] is a
bijection.
2.	If there exists i ∈ [d] such that Li,j = -δ-nd, then UT L:,j ≤ -δ-nd + δ-d+1 - 1 < 0.
Therefore, one can say that UT L:,j gives the “column id” for each possible value of L:,j ∈
{0, δ, . . . , 1 - δ}d.
The rough idea of the construction is to apply the selective shift operation to each column id, by
setting u in the definition of Ψ(∙) to be (1, δ-1,δ-2,..., δ-d+1) and choosing bQ = l - δ∕2 and
bQ = l + δ∕2 for each l ∈ [0 : δ : δ-d+1 — δ]. More concretely, We stack (1∕δ)d attention layers,
with attention parts δ-dΨ(∙; l — δ∕2,l + δ∕2) for each l ∈ [0 : δ : δ-d+1 — δ], in increasing order of
l. After that, we add an extra single-head attention layer with attention part δ-(n+1)dψ(∙; 0).
We now divide possible input values L ∈ Gδ+ into three disjoint categories, and show how these
layers change the input values at the end of all the layers. Recall the hierarchy Geδ ⊂ Gδ ⊂ Gδ+ .
The categories are defined as follows:
1.	L ∈ Ge δ . All entries are between 0 and 1 - δ, and all columns are unique.
2.	L ∈ Gδ \ Ge δ . All entries are between 0 and 1 - δ, but there are duplicate columns.
3.	L ∈ Gδ+ \ Gδ. The point has at least one entry that equals to -δ-nd.
B.5. 1 Category 1
In Category 1, we have L ∈ Geδ. Let lj := UT L:,j. Due to permutation equivariance, we can assume
without loss of generality that lj's are in increasing order: lι < l2 < •一 < In. The first (1∕δ)d
layers sweep the set [0 :δ: δ-d+1
- δ] and apply selective shift operation on each element in the
set. This means that selective shift operation will be applied to l1 first, then l2, and then l3, and so
on, regardless of the specific values of lj’s.
16
Published as a conference paper at ICLR 2020
First shift operation. In the first selective shift operation, the (1, 1)-th entry of L (L1,1) is shifted
by the operation, while the other entries are left untouched. The updated value L1,1 is
Le1,1 = L1,1 + δ-d(maxk uT L:,k - mink uTL:,k) = L1,1 + δ-d(ln - l1).
Therefore, after the operation, the output of the layer is Le:,1 L:,2 . . . L:,n , and the new value
of the first column L:,1 results in
dd
uTLe:,1 = Le1,1 + X δ-i+1Li,1 = L1,1 + δ-d (ln - l1) + X δ-i+1Li,1 = l1 + δ-d(ln - l1).
i=2	i=2
T
Let us denote the updated “column id” uT L:,1 as l1. We can show that ln < l1, because
e1 := l1 + δ-d(ln - l1) ≥ 0 + δ-d ∙ δ = δ-d+1 > ln.
Therefore, after updating,
max uT Le :,1 L:,2 . . . L:,n = max{el1, l2, . . . , ln} = el1,
and the new minimum is l2.
Second shift operation. The second selective shift operation is applied to l2, by which only one
entry L1,2 will be shifted. The updated value L1,2 is
Le1,2 = L1,2 + δ-d(el1 - l2) = L1,2 + δ-d(l1 - l2) + δ-2d(ln - l1).
After updating, the new inner product of u and L:,2 results in
el2 := uTLe :,2 = l2 + δ-d(l1 - l2) + δ -2d (ln - l1).
We can show that el1 < el2, because
l1 + δ-d(ln - l1) < l2 + δ-d(l1 - l2) + δ-2d(ln - l1)
⇔ (δ-d - 1)(l2 - l1) < δ-d(δ-d - 1)(ln - l1),
and the last inequality is true because δ-d > 1 and ln > l2 . Since we have el1 < el2 , and the new
maximum in uT Le :,1 Le :,2 L:,3 . . . L:,n is now el2, and the new minimum is l3.
Repeating the process. More generally, we can repeat this process, and show that the j -th shift
d
operation shifts L1,j by δ-d(lj-1 - lj), and results in the new column id
j-1
elj := uTLe :,j = lj + X δ-kd(lj-k - lj-k+1) + δ-jd(ln - l1).
k=1
In the general case, lj-1 < lj holds j = [2 : n], because
j-1
elj-1 = lj-1 + X δ-kd+d(lj-k - lj-k+1) + δ-(j-1)d(ln - l1)
k=2
j-1
< elj = lj + X δ-kd(lj-k - lj-k+1) + δ-jd(ln - l1)
k=1
j-1
⇔ X δ-kd+d(δ-d - 1)(lj-k+1 - lj-k) < δ-(j-1)d(δ-d - 1)(ln - l1),
k=1
and the last inequality holds because
j-1	j-1
δ-(j-1)d(ln - l1) > δ-(j-1)d X(lj-k+1 - lj-k) > X δ-kd+d(lj-k+1 - lj-k).
k=1	k=1
Therefore, after the j-th selective shift operation, lj is the new maximum among
{l1, . . . , lj, lj+1, . . . , ln} and lj+1 is the new minimum, which makes us possible to continue the
process until the n-th operation.
17
Published as a conference paper at ICLR 2020
After n shift operations. As a result, after the whole sweep from 0 to δ-d+1 - δ by the first
(1∕δ)d layers, a total of n shift operations are applied, and the input L is mapped to a new point L,
where UTL = [e1 e ... en] and e <e < …< e.
We can now prove the following technical lemma, whose proof is deferred to Appendix B.5.4:
Lemma 10. After n shift operations, ln = uT L:,n satisfies the following bounds:
δ-(n-1)d+1(δ-d - 1) ≤ eln ≤ δ-nd+1 (δ-d - 1) - δ(δ-d - 1)2.
Also, the map from	[lι	l2	…	ln]	∈ [0 : δ : δ-d+1	一	δ]	(where lι <	l2	< …<	ln)	to	ln	is
one-to-one.
Global shifting by the last layer. As mentioned earlier, after this sweep, there is another attention
( +1)d
layer with attention part δ (n+1)dψ(∙; 0). Since 0 < lι < •一< ln, what it does to L is that it adds
δ-(n+1)d maxk uTLe:,k = δ-(n+1)deln to each entry in the first row of Le. The output of this layer is
defined to be the function gc(L).
Now, in summary, for any L ∈ Geδ, i ∈ [d], and j ∈ [n], we have
L1,j + Pjk-=11 δ-kd(lj-k 一 lj-k+1) + δ-jd (ln 一 l1) + δ-(n+1)deln	if i = 1,
gc(L)i,j = Li,j	ifi∈ [2, d],
and for any L ∈ Gδ and j ∈ [n],
uTgc(L):,j = elj + δ-(n+1)deln.
Checking Properties 6.1 and 6.2. Given this result so far, it is now left to check if the constructed
network is really a permutation equivariant contextual mapping, i.e., ifit satisfies Properties 6.1 and
6.2 in Lemma 6.
First, for any L ∈ Gδ, Property 6.1 holds because we already know lι < l2 < •一< ln, so they
are all distinct. As for Property 6.2, note that the upper bound on eln from Lemma 10 also holds for
other lj ’s, so
uTgc(L):,j ∈ [δ-(n+1)deln, δ-(n+1)deln + δ-(n+1)d+1),
for all j ∈ [n]. Now, from Lemma 10, two L, L0 ∈ Geδ (that are not permutations of each
other) map to different ln and ln0 , and they differ at least by δ. This means that two intervals
[δ-(n+1)deln, δ-(n+1)deln+δ-(n+1)d+1) and [δ-(n+1)deln0 , δ-(n+1)del0n + δ-(n+1)d+1) are guaranteed
to be disjoint, so the entries ofuTgc(L) and uTgc(L0) are all distinct. This proves Property 6.2.
Therefore, we finished showing that the map gc(∙) we constructed using (1∕δ)d + 1 attention layers
implements a permutation equivariant contextual mapping on Ge δ .
Checking Property 6.3. It is now left to check if the map gc satisfies the other properties. At this
point, we can check Property 6.3. From uTgc(L):,j ∈ [δ-(n+1)deln, δ-(n+1)deln + δ-(n+1)d+1) and
Lemma 10, we can show that for any L ∈ Ge δ, we have
δ-2nd+1 (δ-d 一 1) ≤ UTgc(L):,j < 6-(n+1)d(δ-nd+1(δ-d — 1) 一 δ(δ-d - 1)2) + δ-(n+1)d+1
≤ δ-(2n+1)d+1(δ-d 一 1),
where we used δ-1 ≥ 2. This proves that all UTgc (L):,j are between tl = δ-2nd+1 (δ-d 一 1) and
tr = δ-(2n+1)d+1(δ-d 一 1). For the remaining input points L ∈ Gδ+ \ Geδ, we will check that
UTgc(L):,j is outside the interval [tl, tr] (Property 6.4).
B.5.2 Category 2
In Category 2, we have L ∈ Gδ \ Gδ. Here, all entries are between 0 and 1 -δ, but there are duplicate
columns. Again, let j := UTL：,j, and assume without loss of generality that l1 ≤ l2 ≤ ∙∙∙ ≤ ln.
18
Published as a conference paper at ICLR 2020
For the input L in Category 2, there exist some j, j0 ∈ [n], j 6= j0, such that lj = lj0 . This means
that when the input passes through the attention layer δ-dΨ(∙; j - δ∕2,j + δ∕2),the selective shift
operation for lj is applied to both j-th and j0-th columns; the two columns are coupled together.
More generally, suppose we have n0 < n distinct columns.
If n0 = 1. In the extreme case of n0 = 1, we have maxj lj = minj lj, so the selective shift
operation applied at j does not shift the entry at all; therefore, at the end of the first (1∕δ)d attention
layers, L = L.
If 1 < n0 ≤ n - 1. When 1 < n0 ≤ n - 1, let the n0 distinct values of lj’s be l01, . . . , ln0 0. The shift
operation is applied n0 times, to l10 , . . . , l0n0, and shifts one or more entries at a time. After the first
dT
(1∕δ)d layers, the output L has n0 distinct j = UTL：,j, 0 ≤ lι ≤ l2 ≤ ∙∙∙ ≤ ln, whose distinct
values are the same as the numbers we get when we apply shift operations to a length-n0 sequence
[l10 . . . ln0 0 ]. Then, applying the same calculations from Category 1 shows that
n0-1
eln = uT Le:,n = ln0 0 + X δ-kd(ln0 0-k - ln0 0-k+1) + δ-n d(ln0 0 - l10 ),
k=1
and it follows from the upper bound in Lemma 10 that
eln ≤ δ-n0d+1(δ-d - 1) - δ(δ-d - 1)2 < δ-(n-1)d+1 (δ-d - 1).
Note that the RHS matches the lower bound in Lemma 10. This implies that the value of ln cal-
culated from the input L ∈ Gδ \ Gδ (Category 2) is always strictly less (by at least δ) than that
1 一…	,一 K /C,	Y、
calculated from L ∈ Gδ (Category 1).
Checking Property 6.4. After the global shifting by the last layer with attention part
δ-(n+1)dψ(∙; 0), We get the output gc(L) which satisfies
uTgc(L):,j =elj + δ-(n+1)deln ≤ (δ-(n+1)d + 1)(δ-(n-1)d+1(δ-d - 1) - δ(δ-d - 1)2)
< δ-2nd+1(δ-d - 1) =: tl.
where the RHS is a lower bound on possible values of uTgc(L):,j for L ∈ Geδ (Category 1). This
means that the entries of uT gc(L) for Category 2 are outside [tl, tr], which satisfies Property 6.4.
B.5.3	Category 3
In Category 3, we have L ∈ Gδ+ \ Gδ; the point L has at least one entry that equals to -δ-nd. Let
lj := uT L:,j, and recall that whenever a column L:,j has an entry that equals to -δ-nd, we have
lj = UT L：,j ≤ -δ-nd + δ-d+1 -1 < 0. Assume without loss of generality that lι ≤ l2 ≤ ∙∙∙ ≤ ln.
Recall that the selective shift operation is applied to each element of [0 : δ : δ-d+1 - δ], not to
negative values. In case of Category 3, we have mink UT L:,k = l1 < 0, and l1 never gets shifted
upwards, so it remains as the minimum for the whole time.
If all lj ’s are negative. In case where all lj ’s are negative, selective shift operation never changes
the input L, so we get L = L. Since we have UTL < 0Tn (entry-wise), the last layer with attention
part δ-(n+1)dψ(∙; 0) adds δ-(n+1)dmink UT L:,k < 0 to each entry in the first row of L, further
pushing it to the negative side. Therefore, the final output gc(L) satisfies UTgc(L) < 0Tn < tl1Tn .
If not all lj ’s are negative. Now consider the case where at least one lj is positive. Let i be the
index that satisfies li-1 < 0 ≤ li. Then, selective shift operation does not affect l1, . . . , li-1, and
then it shifts li by
δ-d(max UT L:,k -minUTL:,k) = δ-d(ln -l1) ≥ δ-d(0 + δ-nd - δ-d+1 +1) ≥ δ-(n+1)d+1,
where we used δ-1 ≥ 2 at the last inequality. The next shift operations shift li+1, . . . , ln by even
larger amount, so at the end of the first (1∕δ)d layers, we have δ-(n+1)d+1 ≤ ee ≤ ∙∙∙ ≤ en, while
lj = lj < 0 for j ∈ [i - 1].
19
Published as a conference paper at ICLR 2020
Shifts by the last layer. Here, the last layer with attention part δ-(n+1)dψ(∙; 0) acts differently
( +1)d	( +1)d
for negative and positive lj’s. For negative lj’s, it adds δ-(n+1)d mink lk = δ-(n+1)dl1 < 0
to l1, . . . , li-1, pushing them further to the negative side. For positive lj’s, the layer adds
δ-(n+1)d maxk elk = δ-(n+1)deln ≥ δ-(2n+2)d+1 to eli, . . . ,eln,
so that they are all greater than or
equal to δ-(2n+2)d+1. Note that δ-(2n+2)d+1 > tr.
Checking Property 6.4. Therefore, in both cases, we can see that the final output gc (L) satisfies
uTgc(L):,j ∈/ [tl, tr], for all j ∈ [n]. This completes the verification of Property 6.4.
B.5.4	Proof of Lemma 10
Proof of lower and upper bounds on eln are straightforward:
n-1
eln := ln + X δ -kd (ln-k - ln-k+1) + δ-nd(ln - l1)
k=1
n-1
≥ δ-(n-1)d X(ln-k - ln-k+1) + δ-nd(ln - l1) = (δ-nd - δ-(n-1)d)(ln - l1)
k=1
≥ δ-(n-1)d+1 (δ-d - 1),
eln ≤ ln + δ-d(l1 - ln) + δ-nd(ln - l1) ≤ δ-d+1 - δ + (δ-nd - δ-d)(δ-d+1 -δ)
= δ-nd+1(δ-d - 1) - δ(δ-2d - 2δ-d + 1) = δ-nd+1(δ-d - 1) - δ(δ-d - 1)2.
For one-to-one property of the map, consider [lι l2 •一	ln] and [l'1 l'2	•一	l,n] with increas-
ing entries, which are mapped to ln and ln0 , respectively. Suppose ln = ln0 . By definition,
eln - el0n =(ln - ln0 ) + δ-d(ln-1 - ln - ln0 -1 + l0n) + δ-2d(ln-2 - ln-1 - ln0 -2 + ln0 -1) + . . .
+ δ-(n-1)d(l1 -l2 -l10 +l20)+δ-nd(ln-l1 -ln0 +l10) =0.
Now assume for contradiction that ln 6= ln0 . Then, we have -δ-d+1 + δ ≤ ln - ln0 ≤ δ-d+1 - δ.
However, the remaining terms have “coarse resolution”, and they can never cancel ln - ln0 and
make the sum zero, because for example, δ-d(ln-1 - ln - ln0 -1 + ln0 ) can only have values
0, δ-d+1, -δ-d+1, 2δ-d+1, -2δ-d+1,   Thus, ln = ln0 must hold and the first term must be
zero.
Similarly, assume that ln-1 6= ln0 -1. Then, the second term is in the interval [-δ-2d+1 +
δ-d+1, δ-2d+1 - δ-d+1]. Again, the remaining terms cannot cancel the second term, hence
ln-1 = ln0 -1 must hold. We can proceed this way, and show that lj = lj0 must hold for all j ∈ [n],
hence proving that the map is one-to-one.
B.6 Proof of Lemma 7
Note that ∣G+∣ = ( 1 + 1)dn, so the image of gc(G,) (from Lemma 6) has finite number of distinct
real numbers. Let M be the maximum over all these numbers. By construction of gc, we know that
M >0.
To construct a function gvtkn that satisfies the statement of the lemma, we first implement the second
part: gvtkn(gc(L):,j) = 0d if L ∈ Gδ+ \ Geδ. Note from Lemma 6 that, for any L ∈ Geδ, we have
UTgc(L)：,j ∈ [tι,tr] forall j, and for any L ∈ G+ \ Gδ, UTgc(L)：,j ∈ [tι,tr] forall j. Using this,
we add the following feed-forward layer:
Z 7→ Z - (M + 1)1nφ(UTZ), φ(t)
01
ift ∈ [tl, tr]
if t ∈/ [tl , tr].
Input to this layer is gc(L). If L ∈ Geδ, then φ(UTgc (L)) = 0Tn, so the output stays the same as
the input. If L ∈ Gδ+ \ Geδ, then φ(UTgc (L)) = 1Tn, so all the entries of the input are shifted by
-M - 1, and become strictly negative.
20
Published as a conference paper at ICLR 2020
Recall that by definition of Gδ, all the entries of gc (L) for L ∈ Gδ are nonnegative. So the next
thing to do is mapping all strictly negative entries to zero. This can be done in a similar way as
Lemma 5. For i ∈ [d], add the following layer:
Z 7→ Z + e(i)φ((e(i))TZ), φ(t) = -t ift<0
0 if t ≥ 0.
After these d layers, the output for L ∈ Gδ+ \ Ge δ is a zero matrix, while the output for L ∈ Ge δ is
gc(L).
Now, it is left to map gc(L) to AL, for L ∈ Geδ. Up to permutation equivariance, each different
context L maps to n unique numbers uTgc(L), which are at least δ apart from each other. The idea
of value mapping is to map each unique number to the corresponding output column.
More precisely, choose any L ∈ Gδ. For each value of UTgc(L):,j, j ∈ [n], We add one feed-
forward layer
Z→ Z +((AL)：,j - gc(L)：,j )φ(uτ Z -UT gc(L)：,jlT), φ(t) = [0 t <-δ/2 or t ≥δ/2,
1 一δ / 2 ≤ t < δ / 2.
If the input Z is a zero matrix, Which is the case for L ∈ Gδ+ \ Ge δ, UTZ = 0Tn. Since tl is much
larger than 0, activation is all zero. Thus, zero input matrix remains the same at the output.
If the input Z is gc(L), where L ∈ Gδ is not a permutation of L, then
φ(UTgc (L)- UTgc (L) :,j1T) = OT,
so gc(L) is left untouched.
If some other L is a permutation of L, and L：,i = L：,j, then
Φ(utgc(L)-UTgc(L)：jlT) = (e⑴)T,
so i-th column ofgc(L) will turn to
gc(L)：,i → gc(L)：,i + ((AL)：,j - gC(L)：,j) = gc(L)：,i + ((AL)：,i - gc(L)：,i) = (AL)：,i,
which is the desired output. In conclusion, this layer maps the column gc(L)：,j to (Al)：,1, without
affecting any other columns.
As seen above, we need one layer per each unique value of UTgc (L)：,j for each L ∈ Ge δ. Note
that there are O(n(1∕δ)dn∕n!) such numbers, so we can use O(n(1∕δ)dn∕n!) layers to finish our
construction.
C Proof of Theorem 3
Proof of Theorem 3 can be done in a similar way as Theorem 2. As in the proof of Theorem 2, there
are three parts: Lemma 8, Proposition 4, and Lemma 9. The statement and proof of Lemmas 8 and
9 can be done in almost the same way, this time without permutation equivariance.
For the proof of the second part, which corresponds to Proposition 4, we construct the network in a
similar way. Recall that we can assume without loss of generality that X ∈ [0, 1]d×n. Choose
■0 1 2 …n - 1
0 1 2 …n — 1
E =...	..
...	.
...	.
_0 1 2 …n — L
Then, the first column of X + E is in [0, 1]d, second is in [1, 2]d, and so on; this means that for
all rows, the coordinates are monotonically increasing. So we can use the same technique as the
proof of Proposition 4 to divide the input values into cubes, quantize them to L, apply contextual
mapping, and then value mapping. We describe each step in the following.
21
Published as a conference paper at ICLR 2020
C.1	Quantization by feed-forward layers
In a similar way as Lemma 5, the goal of this step is to quantize the input in [0,1]d X [1,2]d X …X
[n - 1, n]d to its discrete version:
[0 : δ :1 — δ]d X [1: δ : 2 — δ]d x∙∙∙x [n — 1: δ : n — δ]d.
This can be done by dn∕δ feed-forward layers. We add dn∕δ layers of the following form, for
k = 0,δ, .. .,n -δandi = 1, .. .,d:
Z7→Z+e(i)φ((e(i))TZ-kδ1Tn), φ(t) = 0 t<0ort≥δ
-t 0 ≤ t < δ.
After dn∕δ layers, any input entry of X + E in [kδ, kδ + δ) is quantized to kδ.
C.2 Contextual mapping by attention layers
By Step 1, we quantized any input X + E to its quantized version. We call this quantized version
L:
L ∈ [0 : δ : 1 — δ]d X [1 : δ : 2 — δ]d X …X [n — 1 : δ : n — δ]d.
As done in Lemma 6, we define u := (1, δ-1, . . . , δ-d+1) and lj := uTL:,j, for all j ∈ [n]. Note
that, because L:,j ∈ [j - 1 : δ : j - δ]d, we have
(j -1)(1 + δ-1 + …+ δ-d+1) ≤ lj ≤ (j - 1)(1 + δ-1 + …+ δ-d+1) + δ-d+1 - δ,
and lι < l2 < …< lnr. Notice that this corresponds to the Category 1 in the proof of Lemma 6.
For simplicity of notation, let Sj = (j - 1) Pd=0 δ-k. We stack n(1∕δ)d attention layers, with
attention parts δ-dΨ(∙; l — δ∕2, l + δ∕2) for each l ∈ U；=i [sj : δ : Sj + δ-d+1 一 δ], in increasing
order of l.
These n(1∕δ)d attention layers perform selective shift operations on Ij,s, in increasing order of j.
As seen in Appendix B.5.1, shift operations result in lι < l2 < •一< ln. Also, the map from L to
ln is one-to-one, which can be shown in the same way as Appendix B.5.4. Since the range of lj ’s
are a bit different, we have a different upper bound on ln :
T
ln :
n-1
ln + X δ-kd(ln-k - ln-k+1) + δ-nd (ln - l1)
k=1
≤ ln + δ-d(l1 - ln) + δ-nd(ln - l1) ≤ sn + δ-d+1 -δ+(δ-nd-δ-d)(sn+δ-d+1-δ)
=(δ-nd - δ-d + 1) ((n - 1) δ-d--1 + δ-d+1 - δ)
≤ (δ-nd - δ-d + 1)(δ-d - 1)(n - 1+δ) <nδ-(n+1)d.
Finally, We add an extra single-head attention layer with attention part nδ-(n+1)dτψ(ι0). We
define the output of this layer as gc(L). In a similar way as Appendix B.5.1, this layer shifts all the
layers by nδ-(n+1)d-1eln, thus making the intervals corresponding to different values of eln disjoint
from each other. This ensures that different contexts L are mapped to distinct numbers in uTgc(L),
thus implementing a contextual mapping.
C.3 Function value mapping by feed -forward layers
Now, it is left to map gc(L) to the desired output. As seen in the last step, each different context L
maps to n unique numbers uTgc(L), which are at least δ apart from each other. The value mapping
step can be done in a similar way as Lemma 7. The construction now requires O(n(1∕δ)dn) layers
because there is no permutation equivariance.
22
Published as a conference paper at ICLR 2020
Architecture	Average Attention	BProj	SePConv	Transformer
#Params	88.3M	90M	102.5M	H0M
Masked LM accuracy (%)	28	59	60	63
MNLI accuracy (%)	66	72.3	73	78.2	—
Table 1: Performance of bi-linear projection and separable convolution layers on masked LM pre-
training task and MNLI. Note that we expect these computationally cheaper models to have lower
performance than the expensive Transformers as they do not compute input dependent attention
weights and have weaker representation power. Our goal in studying them is to see if they can
substitute some of the expensive attention layers for computing the contextual mappings. These
models are trained in a large batch setting, with a batch size of 8192 for 60k steps, unlike the other
set of experiments reported in Fig. 1. Note that average attention has clearly worse performance,
showing that theses tasks indeed require an advanced architecture.
D	Experimental setup
For our experiments we follow the same setting as in BERT (Devlin et al., 2018). We first pre-train
the models on the masked language modeling task and the next sentence prediction task. We use
English Wikipedia corpus and BooksCorpus dataset (Zhu et al., 2015) for this pre-training. We use
BERTBASE, a 12 layer Transformer model as the baseline. This model uses an embedding size of 768
and has 12 head self-attention layers and 3072 wide feed forward layers. We train it with the Adam
optimizer, with .01 dropout and weight decay. We do pre-training for 250k steps with a batch size
of 1024 and a max sequence length of 512. Pre-training takes around 2 days on 16 TPUv3 chips.
We take the pre-train models and finetune them on the MNLI and SQuAD datasets separately using
the same hyper-parameters as in Devlin et al. (2018). MNLI is a sentence entailment task in which,
given a premise sentence, requires us to classify a hypothesis sentence into neutral, contradiction
or entailment classes. We report the classification accuracy on this task. SQuAD is a question
answering task, in which given a paragraph and a question, requires us to identify the answer as a
span of the words in the paragraph. For this task we report both the F1 score and the Exact Match
(EM) percentage. The metrics are reported on the dev sets of these datasets.
For our experiments with the depth-wise separable convolution layers, we follow the implementation
in (Wu et al., 2019). We first use a GLU layer followed by the convolution layer. We use 16 separable
convolution filters, of filter length 128, and reuse them, with each filter operating on 48 of the 768
dimensions of the input. This layer also has a skip connection and the output is normalized using
layer normalization, similar to the self-attention layer. In our experiments, we replace the self-
attention layers of the Transformers, in the lower layers, with this convolution layer. We keep the
feed forward layer of the Transformer block the same.
For the experiments performed in this paper, one might consider an alternate explanation that the
tasks considered maybe are easy, and do not require any advanced architecture to solve them, and
even a simple architecture (bi-linear projection or separable convolution) might solve these tasks. To
rule out this case we consider an even simpler architecture, namely average attention, as a baseline
for our experiments.
Average attention. An average attention layer replaces the self-attention layer, and just computes
the average of projections of all the other tokens. That is, we replace σ[(WKi X)TWQi X] in (1)
with a matrix full of 1/n. The model still has the skip connections and the feed-forward layers like
Transformer.
23