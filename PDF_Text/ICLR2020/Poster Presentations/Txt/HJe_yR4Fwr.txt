Published as a conference paper at ICLR 2020
Improved Sample Complexities for Deep Net-
works and Robust Classification via an All-
Layer Margin
Colin Wei	Tengyu Ma
Stanford University	Stanford University
colinwei@stanford.edu	tengyuma@stanford.edu
Ab stract
For linear classifiers, the relationship between (normalized) output margin and
generalization is captured in a clear and simple bound - a large output margin
implies good generalization. Unfortunately, for deep models, this relationship is
less clear: existing analyses of the output margin give complicated bounds which
sometimes depend exponentially on depth. In this work, we propose to instead an-
alyze a new notion of margin, which we call the “all-layer margin.” Our analysis
reveals that the all-layer margin has a clear and direct relationship with general-
ization for deep models. This enables the following concrete applications of the
all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter general-
ization bounds for neural nets which depend on Jacobian and hidden layer norms
and remove the exponential dependency on depth 2) our neural net results easily
translate to the adversarially robust setting, giving the first direct analysis of robust
test error for deep networks, and 3) we present a theoretically inspired training al-
gorithm for increasing the all-layer margin. Our algorithm improves both clean
and adversarially robust test performance over strong baselines in practice.
1 Introduction
The most popular classification objectives for deep learning, such as cross entropy loss, encourage a
larger output margin - the gap between predictions on the true label and and next most confident la-
bel. These objectives have been popular long before deep learning was prevalent, and there is a long
line of work showing they enjoy strong statistical guarantees for linear and kernel methods (Bartlett
and Mendelson, 2002; Koltchinskii et al., 2002; Hofmann et al., 2008; Kakade et al., 2009). These
guarantees have been used to explain the successes of popular algorithms such as SVM (Boser et al.,
1992; Cortes and Vapnik, 1995).
For linear classifiers, the relationship between output margin and generalization is simple and direct
- generalization error is controlled by the output margins normalized by the classifier norm. Con-
cretely, suppose we have n training data points each with norm 1, and let γi be the output margin on
the i-th example. With high probability, if the classifier perfectly fits the training data, we obtain1
1 uu n classifier norm 2
Test classification error . — t (-----------------；-----J + low order terms
(1.1)
For deeper models, the relationship between output margin and generalization is unfortunately less
clear and interpretable. Known bounds for deep nets normalize the output margin by a quantity that
either scales exponentially in depth or depends on complex properties of the network (Neyshabur
et al., 2015; Bartlett et al., 2017; Neyshabur et al., 2017b; Golowich et al., 2017; Nagarajan and
Kolter, 2019; Wei and Ma, 2019). This is evidently more complicated than the linear case in (1.1).
These complications arise because for deep nets, it is unclear how to properly normalize the output
1This is a stronger version of the classical textbook bound which involves the min margin on the training
examples. We present this stronger version because it motivates our work better. It can be derived from the
results of Srebro et al. (2010).
1
Published as a conference paper at ICLR 2020
margin. In this work, we remedy this issue by proposing a new notion of margin, called "all-layer
margin", which we use to obtain simple guarantees like (1.1) for deep models. Let mi be the all-
layer margin for the i-th example. We can simply normalize it by the sum of the complexities of the
weights (often measured by the norms or the covering number) and obtain a bound of the following
form:
1	uu n sum of the complexities of each layer 2
Test error . — t (------------------------；-----------------J + low order terms (1.2)
As the name suggests, the all-layer margin considers all layers of the network simultaneously, unlike
the output margin which only considers the last layer. We note that the definition of the all-layer
margin is the key insight for deriving (1.2) - given the definition, the rest of the proof follows
naturally with standard tools. (Please see equation (2.2) for a formal definition of the margin, and
Theorem 2.1 for a formal version of bound (1.2).) To further highlight the good statistical properties
of the all-layer margin, we present three of its concrete applications in this paper.
1.	By relating all-layer margin to output margin and other quantities, we obtain improved gener-
alization bounds for neural nets. In Section 3, we derive an analytic lower bound on the all-layer
margin for neural nets with smooth activations which depends on the output margin normalized
by other data-dependent quantities. By substituting this lower bound into (1.2), we obtain a gen-
eralization bound in Theorem 3.1 which avoids the exponential depth dependency and has tighter
data-dependent guarantees than (Nagarajan and Kolter, 2019; Wei and Ma, 2019) in several ways.
First, their bounds use the same normalizing quantity for each example, whereas our bounds are
tighter and more natural because we use a different normalizer for each training example - the local
Lipschitzness for that particular example. Second, our bound depends on the empirical distribution
of some complexity measure computed for each training example. When these complexities are
small for each training example, we can obtain convergence rates faster than 1 /√n. We provide a
more thorough comparison to prior work in Section 3.
Furthermore, for relu networks, we give a tighter generalization bound which removes the depen-
dency on inverse pre-activations suffered by (Nagarajan and Kolter, 2019), which they showed to be
large empirically (see Section B.1). The techniques of (Wei and Ma, 2019) could not remove this
dependency because they relied on smooth activations.
2.	We extend our tools to give generalization bounds for adversarially robust classification error
which are analogous to our bounds in the standard setting. In Section 4, we provide a natural ex-
tension of our all-layer margin to adversarially robust classification. This allows us to translate our
neural net generalization bound, Theorem 3.1, directly to adversarially robust classification (see
Theorem 4.1). The resulting bound takes a very similar form as our generalization bound for clean
accuracy - it simply replaces the data-dependent quantities in Theorem 3.1 with their worst-case
values in the adversarial neighborhood of the training example. As a result, it also avoids explicit
exponential dependencies on depth. As our bound is the first direct analysis of the robust test error,
it presents a marked improvement over existing work which analyze loose relaxations of the adver-
sarial error (Khim and Loh, 2018; Yin et al., 2018). Finally, our analysis of generalization for the
clean setting translates directly to the adversarial setting with almost no additional steps. This is an
additional advantage of our all-layer margin definition.
3.	We design a training algorithm that encourages a larger all-layer margin and demonstrate that
it improves empirical performance over strong baselines. In Section 5, we apply our regularizer to
WideResNet models (Zagoruyko and Komodakis, 2016) trained on the CIFAR datasets and demon-
strate improved generalization performance for both clean and adversarially robust classification.
We hope that these promising empirical results can inspire researchers to develop new methods for
optimizing the all-layer margin and related quantities.
1.1	Additional Related Work
Zhang et al. (2016); Neyshabur et al. (2017a) note that deep learning often exhibits statistical prop-
erties that are counterintuitive to conventional wisdom. This has prompted a variety of new perspec-
tives for studying generalization in deep learning, such as implicit or algorithmic regularization (Gu-
nasekar et al., 2017; Li et al., 2017; Soudry et al., 2018; Gunasekar et al., 2018), new analyses of
interpolating classifiers (Belkin et al., 2018; Liang and Rakhlin, 2018; Hastie et al., 2019; Bartlett
2
Published as a conference paper at ICLR 2020
et al., 2019), and the noise and stability of SGD (Hardt et al., 2015; Keskar et al., 2016; Chaudhari
et al., 2016). In this work, we adopt a different perspective for analyzing generalization by studying
a novel definition of margin for deep models which differs from the well-studied notion of output
margin. We hope that our generalization bounds can inspire the design of new regularizers tailored
towards deep learning. Classical results have bounded generalization error in terms of the model’s
output margin and the complexity of its prediction (Bartlett and Mendelson, 2002; Koltchinskii et al.,
2002), but for deep models this complexity grows exponentially in depth (Neyshabur et al., 2015;
Bartlett et al., 2017; Neyshabur et al., 2017b; Golowich et al., 2017). Long and Sedghi (2019) prove
bounds for convolutional networks which scale with the number of parameters and their distance
from initialization. Recently, Nagarajan and Kolter (2019); Wei and Ma (2019) derived complexity
measures in terms of hidden layer and Jacobian norms which avoid the exponential dependence on
depth, but their proofs require complicated techniques for controlling the complexity of the output
margin. Neyshabur et al. (2017a); Arora et al. (2018) also provide complexity measures related to
the data-dependent stability of the network, but the resulting bounds only apply to a randomized or
compressed version of the original classifier. We provide a simple framework which derives such
bounds for the original classifier. Novak et al. (2018); Javadi et al. (2019) study stability-related
complexity measures empirically.
A recent line of work establishes rigorous equivalences between logistic loss and output margin
maximization. Soudry et al. (2018); Ji and Telgarsky (2018) show that gradient descent implicitly
maximizes the margin for linearly separable data, and Lyu and Li (2019) prove gradient descent con-
verges to a stationary point of the max-margin formulation for deep homogeneous networks. Other
works show global minimizers of regularized logistic loss are equivalent to margin maximizers, in
linear cases (Rosset et al., 2004) and for deep networks (Wei et al., 2018; Nacson et al., 2019). A
number of empirical works also suggest alternatives to the logistic loss which optimize variants of
the output margin (Sun et al., 2014; Wen et al., 2016; Liu; Liang et al., 2017; Cao et al., 2019). The
neural net margin at intermediate and input layers has also been studied. Elsayed et al. (2018) de-
sign an algorithm to maximize a notion of margin at intermediate layers of the network, and Jiang
et al. (2018) demonstrate that the generalization gap of popular architectures can empirically be
predicted using statistics of intermediate margin distributions. Verma et al. (2018) propose a regu-
larization technique which they empirically show improves the structure of the decision boundary at
intermediate layers. Sokolic et al.(2017) provide generalization bounds based on the input margin
of the neural net, but these bounds depend exponentially on the dimension of the data manifold.
These papers study margins defined for individual network layers, whereas our all-layer margin
simultaneously considers all layers. This distinction is crucial for deriving our statistical guarantees.
A number of recent works provide negative results for adversarially robust generalization (Tsipras
et al., 2018; Montasser et al., 2019; Yin et al., 2018; Raghunathan et al., 2019). We provide pos-
itive results stating that adversarial test accuracy can be good if the adversarial all-layer margin is
large on the training data. Schmidt et al. (2018) demonstrate that more data may be required for
generalization on adversarial inputs than on clean data. Montasser et al. (2019) provide impossiblity
results for robust PAC learning with proper learning rules, even for finite VC dimension hypothesis
classes. Zhang et al. (2019) consider the trade-off between the robust error and clean error. Farnia
et al. (2018) analyze generalization for specific adversarial attacks and obtain bounds depending
exponentially on depth. Yin et al. (2018); Khim and Loh (2018) give adversarially robust general-
ization bounds by upper bounding the robust loss via a transformed/relaxed loss function, and the
bounds depend on the product of weight matrix norms. Yin et al. (2018) also show that the product
of norms is inevitable if we go through the standard tools of Rademacher complexity and the out-
put margin. Our adversarial all-layer margin circumvents this lower bound because it considers all
layers of the network rather than just the output.
1.2	Notation
We use the notation {ai }ik=1 to refer to a sequence of k elements ai indexed by i. We will use
◦ to denote function composition: f ◦ g(x) = f (g(x)). Now for function classes F, G, define
F ◦ G , {f ◦ g : f ∈ F, g ∈ G}. We use Dh to denote the partial derivative operator with
respect to variable h, and thus for a function f(h1, h2), we use Dhif(h1, h2) to denote the partial
derivative of f with respect to h evaluated at (hi, h2). We will use ∣∣ ∙ ∣∣ to denote some norm.
For a function f mapping between normed spaces DI, DO with norms ∣∣ ∙ ∣∣i, ∣ ∙ ∣∣o, respectively,
3
Published as a conference paper at ICLR 2020
define IIfkop，supχ∈Dι kf：)kO, which generalizes the operator norm for linear operators. Let
kM kfro, kM k1,1 denote the Frobenius norms and the sum of the absolute values of the entries of M,
respectively. For some set S (often a class of functions), We let M∙k (e, S) be the covering number of
S in the metric induced by norm k ∙ k with resolution e. For a function class F, let N∞ (e, F) denote
the covering number of F in the metric d∞(f, f) = supx kf (x) - f (x)k. For a function f and
distribution P, we use the notation kf |匕(P)，(Ex〜P[|f (x)|q])1/q. We bound generalization for
a test distribution P given a set of n training samples, Pn , {(xi, yi)}in=1 where x ∈ D0 denotes
inputs and y ∈ [l] is an integer label. We will also use Pn to denote the uniform distribution on
these training samples. For a classifier F : D0 → Rl, we use the convention that maxy0∈[l] F (x)y0
is its predicted label on input x. Define the 0-1 prediction loss `0-1 (F (x), y) to output 1 when F
incorrectly classifies x and 0 otherwise.
2	Warmup: Simplified All-Layer Margin and its Guarantees
Popular loss functions for classification, such as logistic and hinge loss, attempt to increase the
output margin of a classifier by penalizing predictions that are too close to the decision boundary.
Formally, consider the multi-class classification setting with a classifier F : D0 → Rl, where l
denotes the number of labels. We define the output margin on example (x, y) by γ(F (x), y) ,
max {0, F (x)y - maxy0 6=y F (x)y0 }.
For shallow models such as linear and kernel methods, the output margin maximization objective
enjoys good statistical guarantees (Kakade et al., 2009; Hofmann et al., 2008). For deep networks,
the statistical properties of this objective are less clear: until recently, statistical guarantees depend-
ing on the output margin also suffered an exponential dependency on depth (Bartlett et al., 2017;
Neyshabur et al., 2017b). Recent work removed these dependencies but require technically involved
proofs and result in complicated bounds depending on numerous properties of the training data (Na-
garajan and Kolter, 2019; Wei and Ma, 2019).
In this section, we introduce anew objective with better statistical guarantees for deep models (The-
orem 2.1) and outline the steps for proving these guarantees. Our objective is based on maximizing
a notion of margin which measures the stability of a classifier to simultaneous perturbations at all
layers. Suppose that the classifier F(x) = fk ◦…。fi(x) is computed by composing k functions
fk , . . . , f1 , and let δk , . . . , δ1 denote perturbations intended to be applied at each hidden layer. We
recursively define the perturbed network output F(x, δ1, . . . , δk) by
h1(x, δ) = f1(x) + δ1kxk2
hi(x,δ) = fi(hi-1(x, δ)) + δikhi-1(x, δ)k2	(2.1)
F(x, δ) = hk(x, δ)
The all-layer margin will now be defined as the minimum norm of δ required to make the classifier
misclassify the input. Formally, for classifier F, input x, and label y, we define
k
mF(x,y),	δ1m,..i.n,δktuXi=1kδik22	(2.2)
subject to arg max F(x, δ1, . . . , δk)y0 6= y
y0
Note that the constraint that F(x, δ) misclassifies x is equivalent to enforcing γ (F (x), y) ≤ 0.
Furthermore, mF is strictly positive if and only if the unperturbed prediction F(x) is correct. Here
multiplying δi by the previous layer norm khi-1(x, δ)k2 is important and intuitively balances the
relative scale of the perturbations at each layer. We note that the definition above is simplified to
convey the main intuition behind our results - to obtain the tightest possible bounds, in Sections 3
and 4, we use the slightly more general mF defined in Section A.
Prior works have studied, both empirically and theoretically, the margin of a network with respect
to single perturbations at an intermediate or input layer (Sokolic et al., 2017; Novak et al., 2018;
Elsayed et al., 2018; Jiang et al., 2018). Our all-layer margin is better tailored towards handling
the compositionality of deep networks because it considers simultaneous perturbations to all layers,
which is crucial for achieving its statistical guarantees.
4
Published as a conference paper at ICLR 2020
Formally, let F，{fk ◦…O f : f ∈ Fi} be the class of compositions of functions from
function classes F1, . . . , Fk. We bound the population classification error for F ∈ F based on
the distribution of mF on the training data and the sum of the complexities of each layer, mea-
sured via covering numbers. For simplicity, we assume the covering number of each layer scales as
logN∣∣∙kop(e, Fi) ≤ ∖C2/e2C for some complexity Ci, which is common for many function classes.
Theorem 2.1 (Simplified version of Theorem A.1). In the above setting, with probability 1 - δ over
the draw of the training data, all classifiers F ∈ F which achieve training error 0 satisfy
EP['0-1 (F(x),y)] . ⅞Ci"xy)~Pn mF(X,y)2_ log2 n + Z
where Z，O (Iog(Iq+log n) is a low-order term.
In other words, generalization is controlled by the sum of the complexities of the layers and the
quadratic mean of 1/mF on the training set. Theorem A.1 generalizes this statement to provide
bounds which depend on the q-th moment of 1/mF for any integer q > 0 and converge at rates
faster than 1∕√n. For neural nets, Ci scales with weight matrix norms and 1/mF can be upper
bounded by a polynomial in the Jacobian and hidden layer norms and output margin, allowing us to
avoid an exponential dependency on depth when these quantities are well-behaved on the training
data.
We will break down the proof of Theorem 2.1 into two simple parts. The first part hinges on showing
that mF has low complexity which scales with the sum of the complexities at each layer. The second
part relates mF to the 0-1 loss using the simple fact that mF (x, y) is nonzero if and only if F
correctly classifies x.
Lemma 2.1 (Complexity Decomposition Lemma). Let m ◦ F = {mF : F ∈ F} denote the family
of all-layer margins of function compositions in F. Then
log n∞ (SX e2,m ◦ F j ≤ X log NkHIop (ei, Fi)	(2.3)
The covering number of an individual layer commonly scales as log N∣∙∣op (ei, Fi) ≤ ∖C2∕e2C. In
this case, for all e > 0, we obtain log N∞ (e, m ◦ F) ≤ ({Ci) .
Lemma 2.1 shows that the complexity of mF scales linearly in depth for any choice of layers Fi .
In sharp contrast, lower bounds show that the complexity of the output margin scales exponentially
in depth via a product of Lipschitz constants of all the layers (Bartlett et al., 2017; Golowich et al.,
2017). Our proof only relies on basic properties of mF, indicating that mF is naturally better-
equipped to handle the compositionality of F. In particular, we prove Lemma 2.1 by leveraging a
uniform Lipschitz property of mF . This uniform Lipschitz property does not hold for prior defi-
nitions of margin and reflects the key insight in our definition - it arises only because our margin
depends on simultaneous perturbations to all layers.
Claim 2.1. For any two compositions F = fk ◦…。fι and F = fk ◦•••◦ fι and any (x,y), we
have |mF (x, y) - mFb(x,y)| ≤ Pik=1 kfi - fbiko2p.
τ-> 八 T . T τ ,	2 K ,1	,♦	1	1	<' C ∙	,1 F C ♦ ,	。	/	∖ x-r r ∙11	,	, G
Proof sketch. Let δ? be the optimal choice of δ in the definition of mF (x, y). We will construct δ
suchthat kb∣∣2 ≤ ∣∣δ*∣∣2 + JPiIIfi- fi∣∣2P and γ(F(x, δ),y) = 0 as follows: define δi，δ? + ∆i
for ∆i ，fi(hiτ(X,δ ))-¾hi-1(x,δ )), where h is defined as in (2.1) with respect to the classi-
i	khi-ι(x,δ*)∣2	,
fier F. Note that by our definition of ∣∣ ∙ ∣∣op, We have ∣∣∆i∣∣2 ≤ IIfi 一 fi∣∣op. Now it is possi-
ble to check inductively that F(x, δ) = F(x, δ?). In particular, δ is satisfies the misclassifica-
tion constraint in the all-layer margin objective for F. Thus, it follows that mFb(x, y) ≤ ∣δ∣2 ≤
∣∣δ*∣∣2 + ∣∣∆∣∣2 ≤ mF(x, y) + JPiIlfi- fi∣∣2p, where the last inequality followed from ∣∣∆i∣2 ≤
5
Published as a conference paper at ICLR 2020
ɪɪ (- Gn x-rτ∙,ι ,ι	∙	1 , •	/	/	∖	/vɔ H c QnO
kfi - fikop. With the same reasoning,we obtain mF (x, y) ≤ mFb(x, y) + i kfi - fiko2p
|mF (x, y) - mFb(x,y)| ≤	Pi kfi - fbiko2p.
so
□
Given Claim 2.1, Lemma 2.1 follows simply by composing i-covers of Fi. We prove a more
general version in Section A (see Lemmas A.1 and A.3.)
The second part of the proof of Theorem 2.1 is to upper bound the 0-1 test error by the test error
of some smooth surrogate loss ` ◦ mF. A result by Srebro et al. (2010) shows that generic smooth
losses ` enjoy faster O(n-1) covergence rates if the empirical loss is low. We straightforwardly
combine Lemma 2.1 with their results to obtain the following generalization bound for ` ◦ mF:
Lemma 2.2. Suppose that ` is a β-smooth loss function taking values in [0, 1]. Then in the setting
of Theorem 2.1, we have with probability 1 - δ for all F ∈ F:
Ep['(mF(x,y))] ≤ 3EPn ['(mF(x,y))]+ C (eP ɑʃlog2 n + log(1∕δ) + loglog n)(2.4)
2n	n	n
for some universal constant c > 0.
To complete the proof of Theorem 2.1, we will choose ` ◦ mF which upper bounds the 0-1 loss
such that the right hand side of (2.4) gives the desired bound. In Section A, we formalize the proof
plan presented here and also define a slightly more general version of mF used to derive the bounds
presented in the following Sections 3 and 4.
Connection to (normalized) output margin Finally, we check that when F is a linear classifier,
mF recovers the standard output margin. Thus, we can view the all-layer margin as an extension of
the output margin to deeper classifiers.
Example 2.1. In the binary classification setting with a linear classifier F(x) = w>x where the
data x has norm 1, we have mF (x, y) = max{0, yw> x} = γ(F (x), y).
For deeper models, the all-layer margin can be roughly bounded by a quantity which normalizes the
output margin by Jacobian and hidden layer norms. We formalize this in Lemma 3.1 and use this to
prove our main generalization bound for neural nets, Theorem 3.1.
3	Generalization Guarantees for Neural Networks
Although the all-layer margin is likely difficult to compute exactly, we can analytically lower bound
it for neural nets with smooth activations. In this section, we obtain a generalization bound that
depends on computable quantities by substituting this lower bound into Theorem 2.1. Our bound
considers the Jacobian norms, hidden layer norms, and output margin on the training data, and
avoids the exponential depth dependency when these quantities are well-behaved, as is the case in
practice (Arora et al., 2018; Nagarajan and Kolter, 2019; Wei and Ma, 2019). Prior work (Nagarajan
and Kolter, 2019; Wei and Ma, 2019) avoided the exponential depth dependency by considering
these same quantities but required complicated proof frameworks. We obtain a simpler proof with
tighter dependencies on these quantities by analyzing the all-layer margin.
The neural net classifier F will be parameterized by r weight matrices {W(i)} and compute F(x) =
W(r)φ(…Φ(W(i)χ)…)for smooth activation φ. Let d be the largest layer dimension. We model
this neural net by a composition of k = 2r - 1 layers alternating between matrix multiplications
and applications of φ and use the subscript in parenthesis (i) to emphasize the different indexing
system between weight matrices and all the layers. We will let s(i)(x) denote the k ∙ k2 norm of
the layer preceding the i-th matrix multiplication evaluated on input x, and κj-(x) will denote the
k ∙ kop norm of the Jacobian of the j-th layer with respect to the i - 1-th layer evaluated on x. The
following theorem bounds the generalization error of the network and is derived by lower bounding
the all-layer margin in terms the quantities s(i)(x), KjI(x), Y(F(x), y).
Theorem 3.1. Assume that the activation φ has a κ0φ -Lipschitz derivative. Fix reference matrices
{A(i), B(i)}ik=1 and any integer q > 0. With probability 1 - δ over the draw of the training sample
6
Published as a conference paper at ICLR 2020
Pn, all neural nets F which achieve training error 0 satisfy
EP [`0-1 ◦ F ] ≤ O
((Pik 嘴 kL∕3pn)
nq/(q+2)
3q/(q+2)	2
q log2 n
------------------1 + ζ
(3.1)
∖
where κ(NiN) captures a local Lipschitz constant of perturbations at layer i and is defined by
NNz	、ʌ s(i-1) (x)κ2r-1^2i(x)
K⑴ (x,y) ,	Y(F(X),y)
for a secondary term ψ(i)(x, y) given by
+ ψ(i)(x, y)
(3.2)
r-1
ψ(i)(x, y) , X
j=i
S(i-1)(X)K2j—2i(x)
+
1≤j≤2i-1≤j0≤2r-1
Kj0 ―2i(x)K2i-2—j(X)
κj0 —j (X)
0
j
X
+
1≤j≤j0 ≤2r-1 j00 =max{2i,j},j00even
κ'φκ j0―j00 +I(X)Kj00-1 —2i(x)Kj00-1 — j (X)S(i-1) (X)
Kj，—j (x)
Here the second and third summations above are over the choices of j,j 0 satisfying the con-
straints specified in the summation. We define a(i)by。⑴ ，min{√d∣∣W(i) — A(i)kfro, k W(i) 一
B(i) k1,1}√log d + poly(n-1) and Z .
r log n+log(I/δ)+Pilog(a(i) + 1) is a low-order term.
n
For example, when q = 2, from (3.1) We obtain the following bound which depends on the second
moment of KNN and features the familiar 1 / √n convergence rate in the training set size.
EP [`0-1 ◦F]
(PiEPn [(嘴)2]1/3(a(i))2/3)3/2log2 n
√n	、
For larger q, we obtain a faster convergence rate in n, but the dependency on K(NiN) gets larger. We
will outline a proof sketch which obtains a variant of Theorem 3.1 with a slightly worse polynomial
dependency on K(NiN) and a(i). For simplicity we defer the proof of the full Theorem 3.1 to Sections B
and C. First, we need to slightly redefine mF so that perturbations are only applied at linear layers
(formally, fix δ2i = 0 for the even-indexed activation layers, and let δ(i) , δ2i-1 index perturbations
to the i-th linear layer). It is possible to check that Lemma 2.1 still holds since activation layers
correspond to a singleton function class {φ} with log covering number 0. Thus, the conclusion of
Theorem 2.1 also applies for this definition of mF . Now the following lemma relates this all-layer
margin to the output margin and Jacobian and hidden layer norms, showing that mF (x, y) can be
lower bounded in terms of {K(NiN) (x, y)}.
Lemma 3.1. In the setting above, we have the lower bound mF (x, y) ≥
1________
ιι{κNN(χ,y)}r=ιk2.
Directly plugging the above lower bound into Theorem 2.1 and choosing C2i = 0, C2i-1 = a(i)
would give a variant of Theorem 3.1 that obtains a different polynomial in K(NiN) , a(i).
Heuristic derivation of Lemma 3.1 We compute the derivative of F(x, δ) with respect to δ(i) :
Dδ(i) F (x, δ) = Dh2i-1(x,δ)F (x, δ)kh2i-2(x, δ)k2. We abused notation to let Dh2i-1(x,δ)F (x, δ)
denote the derivative of F with respect to the 2i — 1-th perturbed layer evaluated on input
(x, δ). By definitions of Kj^i, s,) and the fact that the output margin is I-LiPSChitz, we obtain
kDδ(i)Y(F(x,δ),y)lδ=0k2 ≤ kDh2i-ι(x,δ)F(X, 0)||op||h2i-2(x, 0)k2 = κ2r-1-2i(X)S(i-l)(x).
With the first order approximation Y(F(x,δ),y) ≈ Y(F(x),y) + Pi Dδ(i)γ(F(x,δ),y)∣δ=oδ(i)
around δ = 0, we obtain
Y(F(x, δ),y) ≥ Y(F(X),y) - Eκ2r-1-2i(X)S(i-1)(X)Ilδ(i)k2
i
The right hand side is nonnegative whenever ∣∣δ∣2 ≤ 收也	(F)(X),y)(χ)}r~, which would
imply that mp(x, y) ≥ 收地 一一〕(F)(X),y)(χ)}r~. However, this conclusion is imprecise and
7
Published as a conference paper at ICLR 2020
non-rigorous because of the first order approximation - to make the argument rigorous, We also
control the smoothness of the network around x in terms of the interlayer Jacobians, ultimately
resulting in the bound of Lemma 3.1. We remark that the quantities κ(NiN) are not the only expressions
With Which We could loWer bound mF (x, y). Rather, the role of κ(NiN) is to emphasize the key term
s(i-1)YχF(2r-y;2i(x), which measures the first order stability of the network to perturbation 6,)and
relates the all-layer margin to the output margin. As highlighted above, if this term is small, mF
will be large so long as the network is sufficiently smooth around (x, y), as captured by ψ(i)(x, y).
Comparison to existing bounds We can informally compare Theorem 3.1 to the existing
bounds of (Nagarajan and Kolter, 2019; Wei and Ma, 2019) as follows. First, the leading term
S(i-1)YXF(；—y；2i(x) of KNiN depends on three quantities all evaluated on the same training example,
whereas the analogous quantity in the bounds of (Nagarajan and Kolter, 2019; Wei and Ma, 2019)
appears as maxpn Y(F(；)期)∙ maxpn s(i—1)(x) ∙ maxpn K2r-ι.2i(x), where each maximum is taken
over the entire training set. As we havekκ(NiN)kLq(Pn) ≤ maxPn κ(NiN) (x, y), the term kκ(NiN)kLq(Pn) in
our bound can be much smaller than its counterpart in the bounds of (Nagarajan and Kolter, 2019;
Wei and Ma, 2019). An interpretation of the parameter q is that we obtain fast (close to n-1) con-
vergence rates if the model fits every training example perfectly with large all-layer margin, or we
could have slower convergence rates with better dependence on the all-layer margin distribution. It is
unclear whether the techniques in other papers can achieve convergence rates faster than O(1/√n)
because their proofs require the simultaneous convergence of multiple data-dependent quantities,
whereas we bound everything using the single quantity mF.
Additionally, we compare the dependence on the weight matrix norms relative to n (as the degree
of n in our bound can vary). For simplicitly, assume that the reference matrices A(i) are set to
0. Our dependence on the weight matrix norms relative to the training set size is, up to logarithmic
min{√dkw(i) ∣∣fro,kW(i) ∣∣1,1}
I	~√n
factors,
2q/(q+2)
, which always matches or improves on the dependency
obtained by PAC-Bayes methods such as (Neyshabur et al., 2017b; Nagarajan and Kolter, 2019).
Wei and Ma (2019) obtain the dependency kW(%k2,1, where ∣∣W(i)>∣∣2,1 is the sum of the ∣∣ ∙ ∣∣2
norms of the rows of W(i). This dependency on W(i) is always smaller than ours. Finally, we
note that Theorem 2.1 already gives tighter (but harder to compute) generalization guarantees for
relu networks directly in terms of mF. Existing work contains a term which depends on inverse
pre-activations shown to be large in practice (Nagarajan and Kolter, 2019), whereas mF avoids this
dependency and is potentially much smaller. We explicitly state the bound in Section B.1.
4 Generalization Guarantees for Robust Classification
In this section, we apply our tools to obtain generalization bounds for adversarially robust clas-
sification. Prior works rely on relaxations of the adversarial loss to bound adversarially robust
generalization for neural nets (Khim and Loh, 2018; Yin et al., 2018). These relaxations are not
tight and in the case of (Yin et al., 2018), only hold for neural nets with one hidden layer. To the
best of our knowledge, our work is the first to directly bound generalization of the robust classi-
fication error for any network. Our bounds are formulated in terms of data-dependent properties
in the adversarial neighborhood of the training data and avoid explicit exponential dependencies in
depth. Let Badv(x) denote the set of possible perturbations to the point x (typically some norm ball
around x). We would like to bound generalization of the adversarial classification loss '0dv defined
by '0dv(F(x), y)，maXχ,∈Badv(χ) 'ɔ-ɪ (F(x0), y). We prove the following bound which essentially
replaces all data-dependent quantities in Theorem 3.1 with their adversarial counterparts.
Theorem 4.1. Assume that the activation φ has a κ0φ -Lipschitz derivative. Fix reference matrices
{A(i), B(i)}ik=1 and any integer q > 0. With probability 1 - δ over the draw of the training sample
Pn, all neural nets F which achieve robust training error 0 satisfy
EP ['0dv ◦ F] ≤ O
(q log2 n(Pikκadv 喘 PnE3)3""+2)
nq∕ (q+2)
+ζ
∖
8
Published as a conference paper at ICLR 2020
where κa(id)v is defined by κa(id)v(x, y) , maxx0∈Badv(x) κ(NiN) (x0, y) for κ(NiN) in (3.2), and a(i) , ζ are
defined the same as in Theorem 3.1.
Designing regularizers for robust classification based on the bound in Theorem 4.1 is a promising
direction for future work. To prove Theorem 4.1, we simply define a natural extension to our all-
layer margin, and the remaining steps follow in direct analogy to the clean classification setting.
We define the adversarial all-layer margin as the smallest all-layer margin on the perturbed inputs:
maFdv(x, y) , minx0∈Badv(x) mF (x, y). We note that maFdv(x, y) is nonzero if and only if F correctly
classifies all adversarial perturbations of x. Furthermore, the adversarial all-layer margin also satis-
fies the uniform Lipschitz property in Claim 2.1. Thus, the remainder of the proof of Theorem 4.1
follows the same steps laid out in Section 2. As before, we note that Theorem 4.1 requires mF to be
the more general all-layer margin defined in Section A. We provide the full proofs in Section E.
5	Empirical Application of the All-Layer Margin
Inspired by the good statistical properties of the all-layer margin, we design an algorithm which
encourages a larger all-layer margin during training. Letting ` denote the standard cross entropy
loss used in training and Θ the parameters of the network, consider the following objective:
G(δ, Θ; x, y)，'(Fθ(x, δ), y) - λ∣∣δk2	(5.1)
This objective can be interpreted as applying the Lagrange multiplier method to a softmax re-
laxation of the constraint maxy0 F(x, δ1, . . . , δk)y0 6= y in the objective for all-layer margin. If
G(δ, Θ; x, y) is large, this signifies the existence of some δ with small norm for which FΘ (x, δ)
suffers large loss, indicating that mFΘ is likely small. This motivates the following training ob-
jective over Θ: L(Θ) , EPn [maxδ G(δ, Θ; x, y)]. Define δΘ? ,x,y ∈ arg maxδ G(δ, Θ; x, y). From
Danskin’s Theorem, if G(δ, Θ; x, y) is continuously differentiable2, then we have that the quantity
-EPn [▽㊀G(δΘ X y, Θ; x, y)] will be a descent direction in Θ for the objective L(Θ) (see Corollary
A.2 of (Madry et al., 2017) for the derivation of a similar statement). Although the exact value
δΘ?	is hard to obtain, we can use a substitute δΘ xy found via several gradient ascent steps in δ.
,x,y	,,y
This inspires the following all-layer margin optimization (AMO) algorithm: we find perturbations
δ for each example in the batch via gradient ascent steps on G(δ, Θ; x, y). For each example in the
batch, We then compute the perturbed loss '(Fθ(χ, δθ,χy)) and update Θ with its negative gradient
with respect to these perturbed losses. This method is formally outlined in the PerturbedUpdate
procedure of Algorithm 1.3
We use Algorithm 1 to train a WideResNet architecture (Zagoruyko and Komodakis, 2016) on
CIFAR10 and CIFAR100 in a variety of settings. For all of our experiments we use t = 1,
ηperturb = 0.01, and we apply perturbations following conv layers in the WideResNet basic blocks.
In Table 1 we report the best validation error achieved during a single run of training, demonstrating
that our algorithm indeed leads to improved generalization over the strong WideResNet baseline for
a variety of settings. Additional parameter settings and experiments for the feedforward VGG (Si-
monyan and Zisserman, 2014) architecture are in Section F.1. In addition, in Section F.1, we show
that dropout, another regularization method which perturbs each hidden layer, does not offer the
same improvement as AMO.
Inspired by our robust generalization bound, we also apply AMO to robust classification by extend-
ing the robust training algorithm of (Madry et al., 2017). Madry et al. (2017) adversarially perturb
the training input via several steps of projected gradient descent (PGD) and train using the loss com-
puted on this perturbed input. At each update, our robust AMO algorithm initializes perturbations δ
to 0 for every training example in the batch. The updates to these δ are performed simultaneously
with PGD updates for the adversarial perturbations with the same update rule as Algorithm 1.
2If we use a relu network, G(δ, Θ; x, y) is technically not continuously differentiable, but the algorithm that
we derive still works empirically for relu nets.
3We note a slight difference with our theory: in the ForwardPerturb function, we perform the update
fj—i (x, δ) = fj (f-i—ι(x, δ)) + kfj (fj-i—ι(x, δ))kδj, rather than scaling δ by the previous layer norm 一
this allows the perturbation to also account for the scaling of layer j .
9
Published as a conference paper at ICLR 2020
Algorithm 1 All-layer Margin Optimization (AMO)
procedure PERTURBEDUPDATE(minibatch B = {(xi, yi)}ib=1, current parameters Θ)
Initialize δi = 0 for i = 1, . . . , b.
for s = 1, . . . , t do
for all (xi , yi) ∈ B: do
UPdate δi《-(I ― ηperturbλ)δi + ηperturb^δ '(FORWARDPERTURB(Xi, δi, θ), yi)
Set update g = Vθ [ b Pi '( FORWARDPERTURB(Xi, δi, Θ), yi)].
Update Θ J Θ 一 η(g + VθR(Θ)).	. R is a regularizer, i.e. weight decay.
function FORWARDPERTURB(x, δ, Θ)	. The net has layers fι(∙,Θ),..., f (∙; Θ),
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaawith intended perturbations δ(1) , . . . , δ(r).
Initialize h J x.
for j = 1, . . . , r do
Update h J fj (h; Θ).
Update h J h + khkδ(j) .
return h
Table 1: Validation error on CIFAR for standard training vs. AMO (Algorithm 1).
Dataset	Arch.	Setting	Standard SGD	AMO
		Baseline	435%	3.42%
	WRN16-10	No data augmentation	9.59%	6.74%
CIFAR-10		20% random labels	9.43%	6.72%
		Baseline	3.82%	3.00%
	WRN28-10	No data augmentation	828%	6.47%
		20% random labels	8.17%	6.01%
	WRN16-10	Baseline	20.12%	19.14%
CIFAR-100		No data augmentation	31.94%	26.09%
	WRN28-10	Baseline	18.85%	17.78%
		No data augmentation	30.04%	24.67%
In Table 2, we report best validation performance for robust AMO and the baseline method
of (Madry et al., 2017) on CIFAR10. Our results demonstrate that our robust AMO algorithm can
also offer improvements in the robust accuracy. We provide parameter settings in Section F.2.
6	Conclusion
Many popular objectives in deep learning are based on maximizing a notion of output margin, but
unfortunately it is difficult to obtain good statistical guarantees by analyzing this output margin.
In this paper, we design a new all-layer margin which attains strong statistical guarantees for deep
models. Our proofs for these guarantees follow very naturally from our definition of the margin.
We apply the all-layer margin in several ways: 1) we obtain tighter data-dependent generalization
bounds for neural nets 2) for adversarially robust classification, we directly bound the robust gen-
eralization error in terms of local Lipschitzness around the perturbed training examples, and 3) we
design a new algorithm to encourage larger all-layer margins and demonstrate improved perfor-
mance on real data in both clean and adversarially robust classification settings. We hope that our
results prompt further study on maximizing all-layer margin as a new objective for deep learning.
Table 2: Robust validation error on CIFAR-10 for standard robust training (Madry et al., 2017) vs.
robust AMO. The attack model is 50 steps of PGD with 10 random restarts using '∞ perturbations
with radius = 8.
Arch.	(Madry et al., 2017) Robust AMO
WideReSNet16-10	48.75%	45.94%
WideReSNet28-10	45.47%	42.38%
10
Published as a conference paper at ICLR 2020
7	Acknowledgements
CW acknowledges support from an NSF Graduate Research Fellowship. The work is also partially
supported by SDSI and SAIL.
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pages 6240-6249, 2017.
Peter L Bartlett, Philip M Long, Ggbor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. arXiv preprint arXiv:1906.11300, 2019.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. arXiv preprint arXiv:1802.01396, 2018.
Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal
margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory,
pages 144-152. ACM, 1992.
Olivier Bousquet. Concentration inequalities and empirical processes theory applied to the analysis
of learning algorithms. 2002.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning Imbalanced
Datasets with Label-Distribution-Aware Margin Loss. arXiv e-prints, art. arXiv:1906.07413, Jun
2019.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient
descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273-297,
1995.
Richard M Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian pro-
cesses. Journal of Functional Analysis, 1(3):290-330, 1967.
Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. In Advances in neural information processing systems,
pages 842-852, 2018.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, and Dimitris Tsipras. Robustness (python li-
brary), 2019. URL https://github.com/MadryLab/robustness.
Farzan Farnia, Jesse M Zhang, and David Tse. Generalizable adversarial training via spectral nor-
malization. arXiv preprint arXiv:1811.07457, 2018.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pages 6151-6159, 2017.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pages
9461-9471, 2018.
11
Published as a conference paper at ICLR 2020
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Thomas Hofmann, Bernhard Scholkopf, and Alexander J Smola. Kernel methods in machine learn-
ing. The annals ofstatiStics, pages 1171-1220, 2008.
Hamid Javadi, Randall Balestriero, and Richard Baraniuk. A hessian based complexity measure for
deep networks. arXiv preprint arXiv:1905.11639, 2019.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300, 2018.
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap
in deep networks with margin distributions. arXiv preprint arXiv:1810.00113, 2018.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds, margin bounds, and regularization. In Advances in neural information processing
systems, pages 793-800, 2009.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Justin Khim and Po-Ling Loh. Adversarial risk bounds for binary classification via function trans-
formation. arXiv preprint arXiv:1810.09519, 2018.
Vladimir Koltchinskii, Dmitry Panchenko, et al. Empirical margin distributions and bounding the
generalization error of combined classifiers. The Annals of Statistics, 30(1):1-50, 2002.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. arXiv preprint arXiv:1712.09203,
2017.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel" ridgeless" regression can gener-
alize. arXiv preprint arXiv:1808.00387, 2018.
Xuezhi Liang, Xiaobo Wang, Zhen Lei, Shengcai Liao, and Stan Z Li. Soft-margin softmax for
deep classification. In International Conference on Neural Information Processing, pages 413-
421. Springer, 2017.
Weiyang Liu. Large-margin softmax loss for convolutional neural networks.
Philip M Long and Hanie Sedghi. Size-free generalization bounds for convolutional neural networks.
arXiv preprint arXiv:1905.12600, 2019.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Omar Montasser, Steve Hanneke, and Nathan Srebro. Vc classes are adversarially robustly learnable,
but only improperly. arXiv preprint arXiv:1902.04217, 2019.
Mor Shpigel Nacson, Suriya Gunasekar, Jason D Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. arXiv
preprint arXiv:1905.07325, 2019.
Vaishnavh Nagarajan and J Zico Kolter. Deterministic pac-bayesian generalization bounds for deep
networks via generalizing noise-resilience. arXiv preprint arXiv:1905.13344, 2019.
12
Published as a conference paper at ICLR 2020
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pages 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pages 5947-
5956, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017b.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint
arXiv:1802.08760, 2018.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. Adversarial
training can hurt generalization. arXiv preprint arXiv:1906.06032, 2019.
Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin
classifier. Journal of Machine Learning Research, 5(Aug):941-973, 2004.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances in Neural Information Processing
Systems, pages 5014-5026, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and MigUel RD Rodrigues. Robust large margin deep
neural networks. IEEE Transactions on Signal Processing, 65(16):4265-4280, 2017.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth
loss. arXiv preprint arXiv:1009.3896, 2010.
Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation by
joint identification-verification. In Advances in neural information processing systems, pages
1988-1996, 2014.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.
Vikas Verma, Alex Lamb, Christopher Beckham, Aaron Courville, Ioannis Mitliagkis, and Yoshua
Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer.
stat, 1050:13, 2018.
Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz
augmentation. arXiv preprint arXiv:1905.03684, 2019.
Colin Wei, Jason D. Lee, Qiang Liu, and Tengyu Ma. Regularization Matters: Generalization and
Optimization of Neural Nets v.s. their Induced Kernel. arXiv e-prints, art. arXiv:1810.05369, Oct
2018.
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach
for deep face recognition. In European conference on computer vision, pages 499-515. Springer,
2016.
Dong Yin, Kannan Ramchandran, and Peter Bartlett. Rademacher complexity for adversarially
robust generalization. arXiv preprint arXiv:1810.11914, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
13
Published as a conference paper at ICLR 2020
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pages 7472-7482, 2019.
14
Published as a conference paper at ICLR 2020
A Generalized All-Layer Margin and Missing Proofs for
Section 2
In this section, we provide proofs for Section 2 in a more general and rigorous setting. We first
formally introduce the setting, which considers functions composed of layers which map between
arbitrary normed spaces.
Recall that F denotes our classifier from Section 2 computed via the composition fk ◦•••◦ fι, For
convenience, we overload notation and also let it refer to the sequence of functions {f1, . . . , fk}.
Recall that F denotes the class of all compositions of layers Fk, . . . , F1, where we let functions in
Fi map domains Di-1 to Di. We will fix Dk , Rl, the space of predictions for l classes. Each space
is equipped with norm k ∙ k (our theory allows the norm to be different for every i, but for simplicity
we use the same symbol k ∙ k for all layers).
As in Section 2, we will use F(x, δ) to denote the classifier output perturbed by δ. It will be useful to
define additional notation for the perturbed function between layers i and j, denoted by fj-(h, δ),
recursively as follows:
fii(h,δ)，fi(h)+ δikhk, and J(h,δ)，J(—h, δ)) + δjIIfj-n(h, δ)k
where we choose fi-n(h,δ)，h. Note that F (χ,δ)，fk^ι(x, δ), and the notation hi(x,δ) from
Section 2 is equivalent to fn(x, δ). We will use the simplified notation fj—(x)，fjι(x, 0)
when the perturbation δ is 0 at all layers.
For a given F, we now define the general all-layer margin mF : D0 × [l] → R as follows:
(,,	minlllδl11
mF(x,y),subjecttoγ(F(x,δ),y)≤0
(A.1)
The norm ||| ∙ ||| will have the following form:
|| 训=k(αι∣διk,...,αk |弧 |此
where ɑi ≥ 0 will be parameters chosen later to optimize the resulting bound, and k ∙ kp de-
notes the standard `p-norm. For F = {f1, . . . , fk}, we overload notation and write |||F ||| =
k(α1kf1kop, . . . , αkkfkkop)kp.
This more general definition ofmF will be useful for obtaining Theorems 3.1 and 4.1. Note that by
setting αi = 1 for all i and p = 2, we recover the simpler mF defined in Section 2.
As before, it will be convenient for the analysis to assume that the -covering number of Fi in
operator norm scales with -2 . We formally state this condition for general function classes and
norms below:
Condition A.1 (-2 covering condition). We say that a function class G satisfies the -2 covering
Condition with respect to norm ∣∣∙∣∣ With complexity C∣∣∙k (G) if for all e > 0,
logNH (e, G) ≤ C2⅛G)
Now we provide the analogue of Theorem 2.1 for the generalized all-layer margin:
Theorem A.1. Fix any integer q > 0. Suppose that all layer functions Fi satisfy Condition A.1 with
operator norm ∣∙ kop and complexity function。心八叩(Fi). Let the all layer margin mF be defined as
in (A.1). Then with probability 1 - δ over the draw of the training data, all classifiers F ∈ F which
achieve training error 0 satisfy
2q/(q+2)
。mF∣IL	(P )C||H||(f
Lq (Pn)	I
√n	J
q log2 n + ζ
∖
where CH(F)，(Pi α2p"p+2)C∣H∣op(Fi)2p/(p+2))(p+2"”
ditionA.1)for covering F in ||| ∙ ||| and Z，O (Iog(I/^+log n)
is a complexity (in the sense of Con-
is a low-order term.
15
Published as a conference paper at ICLR 2020
Note that this recovers Theorem 2.1 when αi = 1 for all i and p = 2. The proof of Theorem A.1
mirrors the plan laid out in Section 2. As before, the first step of the proof is showing that mF has
low complexity as measured by covering numbers.
Lemma A.1. Define m ◦ F , {(x, y) 7→ mF (x, y) : F ∈ F}. Then
N∞(e,m ◦F) ≤N∣∣HI∣(e, F)
As in Section 2, we prove Lemma A.1 by bounding the error between mF and mFb in terms of the
||| ∙ ∣∣∣-norm of the difference between F and F.
Claim A.1. For any x, y ∈ D0 × [l], and function sequences F = {fi}ik=1, F = {fi}ik=1, we have
|mF(x,y) - mFb(x,y)| ≤ |||F - Fb|||.
Proof. Suppose that δ? optimizes equation (A.1) used to define mF (x, y). Now we use the notation
,.ʌ ， . ^ ...
h? , fi-ι(x, δ?). Define δ as follows:
δbi , δi? +
^. .
fi(h?-i)- fi(h?-i)
kh?-ik
We first argue Via induction that f»ι(x, δ) = h?. As the base case, We trivially have fo.ι(x, δ)=
x = h0?.
^ ^ ^, ^ ^, ^^ ^ ..
fi-l(x,δ) = fi(fi-i-ι(x, δ)) + δikfi-i-ι(x,δ)k
??
=fi(h?-i) + δi∣∣h:-ιk	(by the inductive hypothesis)
=fb(hLI) + ( δ? + fi(h?-1)- fi(h?T)! kh?-ik	(definition of b)
i-1	i	khi?-1k	i-1
=fi(h?-i)+ 6?kh?-ik
= hi?	(definition of gi?)
Thus, we must have F(x, δ) = F(x, δ?), so it follows that γ(F(x, δ), y) ≤ 0 as well. Furthermore,
by triangle inequality
^ ^
I∣∣δ∣∣∣≤∣∣∣δ?∣∣∣ + ∣∣∣δ- δ*∣∣∣	(A.2)
Now we note that as fi(hi-jh-f jhi-1，≤ ∣∣fi - fi kop, it follows that
...^ .... ....... ^.. .. ^..................... ... ^...
IM - δ III ≤ IKaIIIfI- fιkop,...,akIIfk- fkkoP)Ilp = IllF - Flll
Thus, using (A.2) and the definition of mFb, we have
, .	...^.. . . ... 一	^...
mFb(x,y) ≤ IIIδbIII ≤ mF (x, y) + IIIF - FbIII
where we relied on the fact that IM?III = mp(χ,y). Using the same reasoning, we also obtain
the inequality mF (x, y) ≤ mFb(x, y) + IIIF - FbIII. Combining gives ImF (x, y) - mFb(x, y)I ≤
IllF - Flll.	口
Lemma A.1 now directly follows.
Proof of Lemma A.1. As Claim A.1 holds for any choice of (x, y) ∈ D0 × [l], it follows that if Fb
covers F in norm lll∙ Ill, then m ◦Fb will be a cover for m ◦ F in the functional ∞ norm. 口
We now state the generalized version of Lemma 2.2. The statement below is a straightforward
application of our covering number bound in Lemma A.1 with theory in (Srebro et al., 2010); for
minor technical reasons we translate their result to covering numbers and reprove it in Section A.1.
16
Published as a conference paper at ICLR 2020
C I I I ∙ I I I (F) , (X a2P/(P+2)CHOP(Fi)2P/(P+2)
Lemma A.2 (Straightforward adaptation from (Srebro et al., 2010)). Suppose that' is a β-smooth
loss function taking values in [0,1]. Furthermore suppose that F satisfies ConditionA.1 with respect
to norm ||| ∙ ||| and complexity 0∣∣∣.∣∣∣ (F). Then with probability 1 — δ,forall F ∈ F;
EP ['(mF(x,y))] ≤ 3 Epn['(mp(x,y))] + C β β C∣2 ` ∣ ∣ I(F)	" + ♦( - n
2	n	n
for some universal constant c > 0.
The final ingredient is showing that when each individual layer Fi satisfies Condition A.1 in operator
norm, the class of compositions F satisfies Condition A.1 with respect to norm ||| ∙ |||.
Lemma A.3. Suppose that each Fi satisfies Condition A.1 with norm ∣∣ ∙ ∣∣op and complexity
CIHbP(Fi). Define the complexity measure C∣ ∣ ∣,∣ ∣ ∣ (F) by
(p+2)∕2p
(A.3)
Then we have
C2 11∣(F)
logN∣∣ ∙ ∣∣(e, F) ≤
which by definition implies that F satisfies Condition A.1 with norm ||| ∙ ||| and COmPIexity C∣ ∣ ∣,∣ ∣ ∣ (F).
Proof. Let Fi be an ∈i-cover of Fi in the operator norm ∣∙ ∣ op. We will first show that F，{F1k ◦• ∙ •◦
F : Fi ∈ Fi} is a ∣∣{ai∈i}k=1kp-cover of F in ||| ∙ |||. To see this, for any F = (fk,..., ∕1) ∈ F, let
fi ∈ Fi be the cover element for fi, and define F , (f,..., fι). Then we have
∖∖∖F — F | | | = k{αikfi- fi∣∣op}3kp
≤k-
ʌ 一 C2 k (Fi)
as desired. Furthermore, we note that log |F| ≤Eil "'呵("'by Condition A.1. Now we will
Ji
choose
q = eC∣∣H∣∣(F)-2/(P+2)CHoP (Fi)2/(P+2)a-p/(p+2)
We first verify that this gives an e-cover of F in ||| ∙ |||:
∣∣{αiei}IIkP = e 卜HII(F)-2p/(P+2) XCHoP(Fi)2p/(P+"中/("2))
1/P
CHOP (Fi)2P/(P+2)a2P/(P+2))
=eC∣ HI(F)-2/(P+2)CHI(F)2/(P+2)= e
Next, we check that the covering number is bounded by C| J12(F):
Pi C2,∣∣op(Fi )2ClHI(F)4/(P+2) CHOP (Fi)-4/(P+2)a2P/(P+2)
e2
Pi CIIHI∣(F)4/(P+2)CHOP (Fi)2P/(P+2) a2P/(P+2)
e2
C∣I∙ III (F)4/(P+2)CIII, III (F)2P/(P+2)	= c∣∣∣∙ II (F)2
e2	=	e2
eC HI(F)-2/(P+2) (X
C2∙kop (Fi)
Σ
i
≤
e
□
17
Published as a conference paper at ICLR 2020
Finally, we prove Theorem A.1 (and as a result, Theorem 2.1). This will hinge on applying
Lemma A.2 with the correct choice of smooth loss.
Proof ofTheoremsA.1 and2.1. Define 'β(m)，1 + 2min{0,Prz〜N(o,i)(Z∕√β ≥ m) 一 0.5}.
By Claim A.2, this loss is c1β smooth and for mF (x, y) > 0 satisfies
'β(mF(X,y)) ≤ (而c2√q-c)
βmF (x, y)
for universal constants c1,c2. We additionally have '0-1(F(χ),y) ≤ 'β(mF(χ,y)). Because of
Lemma A.3, the conditions of Lemma A.2 are satisfied, and applying Lemma A.2 with smooth loss
'β gives with probability 1 一 δ, for all F ∈ F with training error 0
Ep['0-1(F(x),y)] . Ef(β) + log(1∕δ) + loglogn
n
where EF (β) is defined by
EF ⑺,n X (
(x,y)∈Pn
c2	q
√βmF (x,y)
q + βC2∙∣∣∣ (F)Iog2 n
n
and Cj,∣∣∣ (F) is defined as in Lemma A.3. Choosing β to minimize the above expression would give
the desired bound - however, such a post-hoc analysis cannot be performed because the optimal β
depends on the training data, and the loss class has to be fixed before the training data is drawn.
Instead, we utilize the standard technique of union bounding over a grid of β in log-scale. Let
ξ，C∣2∙∣∣∣(F)poly(n-1) denote the minimum choice of β in this grid, and select in this grid all
^
^
choices of β in the form ξ2j for j ≥ 0. For a given choice of β, we assign it failure probability
δ = -bδ-, such that by design £6 = δ. Thus, applying Lemma A.2 for each choice of β with
2β/ξ
1∙	1	1 ∙ι∙, G	,	1	1 ∙ι∙, r c
corresponding failure probability δ, we note with probability 1 一 δ,
EP['0-1(F(χ),y)] . EF(β) + log(1∕δ) + log(β∕ξ) + loglogn
n
, -. ..^ 一一 一
holds for all βb and F ∈ F .
Now for fixed F ∈ F, let βF? denote the optimizer of EF(β). We claim either there is some choice
.^ ..
of βb with
1	/-1 / c-∖ , ι	'/八 ，ι ι	/ i	, i / -1 / c-∖
EfG) + log(1∕δ)+ log(β∕ξ)+ loglogn . Ef(b? ) + O llogn + log(1∕δ)
F	n	FF	n
(A.4)
or EF (βF) & 1, in which case the generalization guarantees of Theorem A.1 for this F anyways
^
^
trivially hold. To see this, we note that there is β in the grid such that β ∈ [βF? , 2βF? + ξ]. Then
1
EF ⑶ ≤ n X
(x,y)∈Pn
c2 √q
βFFm mF (χ,y)
q + 4βF%(F)lθg2 n + poly(n-i)
n
≤ 4EEf (βF) + poly(n-1)
Furthermore, We note that if βF > poly(n)ξ, then EF(βF) & 1. This allows to only consider
β < poly(n)ξ, giving (A.4).
Thus, with probability 1 - δ, for all F ∈ F , we have
EP['0-1 (F(x),y)] . EF(βF) + O (logn +n0g(1∕δ))
(∕n∣/IlL (P)\2/(『
It remains to observe that setting βF = Θ I q I C“武产产迄”：	gives us the theorem
statement.
□
18
Published as a conference paper at ICLR 2020
Claim A.2. For β > 0, define the loss function 'β (m) ，1 + 2min{0, PrZ 〜N (o,i)(Z∕√β ≥
m) 一 0.5}. Then 'β satisfies the following properties:
1.	For all m ∈ R, 'β (m) ∈ [0,1], andfor m ≤ 0, 'β(m) = 1.
2.	Thefunction 'β is cιβ-smoothfor some constant ci independent of β.
3.	For any integer q > 0 and m > 0, 'β(m) ≤ 埼；]^^ for some COnStant c2 independent of
q.
Proof. The first property follows directly from the construction of 'β. For the second property, We
first note that
d	L	mβ3∕2
Pr	(Z∕√β ≥ m) = -^p exp(-m2β∕2)
dm2 Z〜N(0,i)' /	‘	√2∏
Now first note that at m = 0, the above quantity evaluates to 0, and thus 'β has a second
derivative everywhere (as m = 0 is the only point where the function switches). Furthermore,
maxm m√βexp(-m2β∕2) = max# y exp(-y2∕2) ≤ c0 for some constant c0 independent of β.
Thus, the above expression is upper bounded by √βπ c0, giving the second property.
For the third property, we note that for m > 0, 'β(m) = 2Prz〜N(o,i)(Z∕√β ≥ m). As the q-th
moment of a Gaussian random variable with variance 1 is upper bounded by qq∕2 cq2 for all q and
some c2 independent of q, Markov,s inequality gives the desired result.	□
A.1 Proof of Lemma A.2
The proof is a straightforward application of Lemma A.1 and conversion of (Srebro et al., 2010) from
the language of Rademacher complexity to covering numbers.
Proof of Lemma A.2. We can follow the proof of Theorem 1 in (Srebro et al., 2010), with the only
difference that we replace their Rademacher complexity term with our complexity function C∣∣∣∙∣∣∣(F).
For completeness, we outline the steps here.
Define H(μ) = {h ∈ ' ◦ m ◦F : EPn [h] ≤ μ} to be the class of functions in ' ◦ m ◦F with empirical
loss at most μ. Define ψ(μ)，CIIH"(F√√48βμ log n. By Claim A.3, the following holds for all μ:
sup	σih(xi, yi)
h∈H(μ) i
≤ ψ(μ)
Now using the same steps as (Srebro et al., 2010) (which relies on applying Theorem 6.1 of (Bous-
quet, 2002)), we obtain for all F ∈ F, with probability 1 - δ
EP [` ◦ mF] ≤ EPn [` ◦ mF] + 106rn?
48	/	4	(A.5)
+---(log1∕δ + log log n) + VEPn[' ◦ mF ](8魔 + —(log1∕δ + log log n))
n	n	nn
where 嚷 is the largest solution of ψ(μ) = μ. We now plug in 琮. βlog nC*111 (F) and use the fact
that √c1c2 ≤ (ci + c2 )/2 for any ci ,c2 > 0 to simplify the square root term in A.5.
EP [' ◦ mF] ≤ 3EP [' ◦ mF] + c ββ%∣∣ (F)Iog2 n + log(10 + loglog n)
2n	n	n
for some universal constant c.	□
Claim A.3. In the setting above, for all μ > 0, we have
Eσ	sup X σih(xi,yi) ≤C∣∣∣∙∣∣∣ (Fy48βμ 麻n
h∈H(μ) V	√n
where {σi}in=i are i.i.d. Rademacher random variables.
19
Published as a conference paper at ICLR 2020
Proof. First, by Dudley’s Theorem (Dudley, 1967), we have
sup	σih(xi, yi)
h∈H(μ) V
Now by Claim A.4, we obtain
≤ On>f0 (α + √n / JlogNLz(Pn) (E, H(M))de
≤ inf α +一
α>0	n
≤ 慰卜+√1n ∕∞
(A.6)
sup	σih(xi, yi)
h∈H(μ) i
We obtained the last line via change of variables to e0 = e/√48βμ. Now We substitute ɑ
CIIHI (F√√48βμ and note that the integrand is 0 for e0 > C∣∣∣∙∣∣∣(F) to get
sup	σih(xi, yi)
h∈H(μ) V
Cw(F )√⅜
CHHII(F)
√n
Cw(F )√⅝
c∣∣∣∙∣ι∣(F )∕√n
log n
≤
≤
□
The following claim applies Lemma A.1 in order to bound the covering number of H(μ) in terms of
cII∣∙∣(f).
Claim A.4. In the setting of Lemma A.2, we have the covering number bound
logNL2(Pn)(e, H(μ)) ≤ 48βμCIH∣∣1F)
Proof. As ` ◦ m ◦ F is the composition of a β-smooth loss ` with the function class m ◦ F, by
equation (22) of (Srebro et al., 2010) we have
log NL2(Pn)(e, H(μ)) ≤ log N∞(e∕P48βμ,m OF)
≤ logN∣∣∣∙∣∣∣(e//48βμ, F)	(by Lemma A.1)
≤ 48βμ °"∙∣y I	(as F satisfies Condition A.1)
□
B Proofs for Neural Net Generalization
This section will derive the generalization bounds for neural nets in Theorem 3.1 by invoking the
more general results in Section C. Theorem 3.1 applies to all neural nets, but to obtain it, we first need
to bound generalization for neural nets with fixed norm bounds on their weights (this is a standard
step in deriving generalization bounds). The lemma below states the analogue of Theorem 3.1, for
all neural nets satisfying fixed norm bounds on their weights.
Lemma B.1. In the neural network setting, suppose that the activation φ has a κ0φ-Lipschitz deriva-
tive. For parameters {a(i)}ir=1 meant to be norm constraints for the weights, define the class of
neural nets with bounded weight norms with respect to reference matrices {A(i), B(i)} as follows:
F，{x → F(x)： min{√d∣∣W(i) - A(i)kfro, kW(i) — B(i)k1,1}plogd ≤。⑷ ∀i}
20
Published as a conference paper at ICLR 2020
Then with probability 1 - δ, for any q > 0 and for all F ∈ F, we have
EP [`0-1 (F (x), y)]
3	r log n + log(1∕δ)∖
≤ 2 EPn ['θ-i(F (X),y)] + O (------n--------J
+ (1 - EPn['o-ι(F(x),y)])2/(q+2) O (q (B) (q+2)(£(1畸kLq(Sn)a(i))2/3! (q+J
where Sn denotes the subset of examples classified correctly by F and κ(NiN) is defined as in (3.2).
Proof. We will identify the class of neural nets with matrix norm bounds {a(i)}ir=1 with a sequence
of function families
F2i-1 , {h→ Wh ： W ∈ Rd×d, min{√dkW - A(i)∣∣F JW - B(i)k1,1 }Plogd ≤。⑴}
F2i , {h 7→ φ(h)}
and let F，F2r-1 ◦•••◦Fi denote all possible Parameterizations of neural nets with norm bounds
{a(i)}r=ι∙Let∣H∣op be defined with respect to Euclidean norm k • k2 on the input and output spaces,
which coincides with matrix operator norm for linear operators. We first claim that
logN∣∣∙kop(e, F2i-1)
This is because We can construct two covers: one for {h → Wh : √d∣∣W∣∣f/√log d ≤ a(i)}, and
onefor {h→ Wh : ∣ W ∣∣1,1∕√log d ≤。⑴}, each of which has log size bounded by O(ba(i)2∕e2C)
by Lemma B.2 and Claim B.2. Now we offset the first cover by the linear operator A(i) and the
second by B(i) and take the union of the two, obtaining an e-cover for F2i-1 in operator norm.
Furthermore, logN∣∙∣cp(e, F2i) = 0 simply because F2i is the singleton function.
Thus, F2i-1, F2i satisfy Condition A.1 with norm ∣ • ∣op and complexity functions
C∣∙kop(F2i-1) . a(i) and。卜|叩(F2i) = 0, so we can apply Theorem C.1. It remains to ar-
gue that K?i-i(x,y) as defined for Theorem C.1 using standard Euclidean norm ∣∣ • ∣∣2 is equivalent
to κ(NiN) (x, y) defined in (3.2). To see this, we note that functions in F2j-1 have 0-Lipschitz
derivative, leading those terms with a coefficient of a_、to cancel in the definition of k?(x, y).
There is a 1-1 correspondence between the remaining terms of K?i-i(x,y) and KNN(χ,y), so
we can substitute κ(NiN) (x, y) into Theorem C.1 in place of κ2i-ι(x,y). Furthermore, as we have
C∣∙kop(F2i) = 0, the corresponding terms disappear in the bound of Theorem C.1, finally giving the
desired result.
□
Now we obtain Theorem 3.1 by union bounding Lemma B.1 over choices of {a(i)}ir=1.
Proof of Theorem 3.1. We will use the standard technique of applying Lemma B.1 over many
choices of {a(i)}, and union bounding over the failure probability. Choose ξ = poly(n-1) and
consider a grid of {αb(i)} with ab(i) = ξ2ji for ji ≥ 1. We apply Lemma B.1 with for all possible
norm bounds {αb(i)} in the grid, using failure probability δ = δ∕( i ba(i)∕ξ) for a given choice of
{αb(i)}. By union bound, with probability 1 - δ = 1 - δ, the bound of Lemma B.1 holds simul-
taneously for all choices of {αb(i)}. In particular, for the neural net F with parameters {W(i)}, there
is a choice of {αb(i)} satisfying
min{√dkW(i)- A(i)|辰，kW(i) - B(i) ∣∣1,1}plogd
≤ b(i) ≤ 2min{√dkw(i) - A(i) kfro, k W(i) - B(i) k 1,1 } PlOg d + ξ
for all i. The application of Lemma B.1 for this choice of αb(i) gives us the desired generalization
bound.
□
21
Published as a conference paper at ICLR 2020
B.1	Generalization Bound for Relu Networks
In the case where φ is the relu activation, we can no longer lower bound the all-layer margin
mF (x, y) using the techniques in Section C, which rely on smoothness. However, we can still ob-
tain a generalization bound in terms of the distribution of 1/mF (x, y) on the training data. We can
expect 1/mF (x, y) to be small in practice because relu networks typically exhibit stability to per-
turbations. Prior bounds for relu nets suffer from some source of looseness: the bounds of (Bartlett
et al., 2017; Neyshabur et al., 2017b) depended on the product of weight norms divided by margin,
and the bounds of (Nagarajan and Kolter, 2019) depended on the inverse of the pre-activations, ob-
served to be large in practice. Our bound avoids these dependencies, and in fact, it is possible to
upper bound our dependency on 1/mF (x, y) in terms of both these quantities.
For this setting, We choose a fixed ||| ∙ ||| defined as follows: if i corresponds to a linear layer in the
network, set αi = 1, and for i corresponding to activation layers, set αi = ∞ (in other words, we
only allow perturbations after linear layers). We remark that we could use alternative definitions of
||| ∙ |||, but because we do not have a closed-form lower bound on mF, the tradeoff between these
formulations is unclear.
Theorem B.1. In the neural network setting, suppose that φ is any activation (such as the relu
function) and mF is defined using ||| ∙ ||| as described above. Fix any integer q > 0. Then with
probability 1 - δ, for all relu networks F parameterized by weight matrices {W(i)}ir=1 that achieve
training error 0, we have
(HmlFIlL(P『Pia*"1
Ep ['0-1 ◦ F] ≤ O log2 n	-一乜岑----------- I	+ Z
√n
∖ ∖ )
where a(i)is defined as in Theorem 3.1, and Z，O (ι°g(1∕δ)+r log；+Pi(a(i) +1)) is a low-order
term.
The proof follows via direct application of Theorem A.1 and the same arguments as Lemma B.1
relating matrix norms to covering numbers. We remark that in the case of relu networks, we can
upper bound 二七 y) Via a quantity depending on the inverse pre-activations that mirrors the bound
of Nagarajan and Kolter (2019). However, as mentioned earlier, this is a pessimistic upper bound
as Nagarajan and Kolter (2019) show that the inverse preactivations can be quite large in practice.
B.2	Matrix Covering Lemmas
In this section we present our spectral norm cover for the weight matrices, which is used in Section B
to prove our neural net generalization bounds.
Lemma B.2. Let Mfro(B) denote the set ofd1 × d2 matrices with Frobenius norm bounded by B,
i.e.
Mfro(B) , {M ∈ Rd1×d2 : kM kfro ≤ B}
Then letting d , max{d1, d2} denote the larger dimension, for all > 0, we have
36dB2 log(9d)
log NikOP (e, MfrO(B)) ≤ -----^2------
Proof. The idea for this proof is that since the cover is in spectral norm, we only need to cover the
top d0 , bB2/2c singular vectors of matrices M ∈ M.
First, it suffices to work with square matrices, as a spectral norm cover of max{d1, d2} ×
max{d1, d2} matrices will also yield a cover of d1 × d2 matrices in spectral norm (as we can extend
a d1 × d2 matrices to a larger square matrix by adding rows or columns with all 0). Thus, letting
d , max{d1, d2}, we will cover Mfro(B) defined with respect to d × d matrices.
Let d0 , b9B2/2c. We first work in the case whend0 ≤ d. Let Ub be a U Frobenius norm cover of
d × d0 matrices with Frobenius norm bound d0. Let V be the cover of d0 × d matrices with Frobenius
22
Published as a conference paper at ICLR 2020
norm bound B in Frobenius norm with resolution V . We construct a cover M for Mfro (B) as
follows: take all possible combinations of matrices U , V from U, V, and add UV to M. First note
that by Claim B.1, we have
log |Mc| ≤ dd0(log(3d0/U) + log(3B/V))
Now we analyze the cover resolution of M: for M ∈ M, first let truncd0 (M) be the truncation of
M to its d0 largest singular values. Note that as M has at most d0 singular values with absolute value
greater than /3, kM - truncd0 (M)kop ≤ /3. Furthermore, let USV = truncd0 (M) be the SVD
decomposition of this truncation, where U ∈ Rd×d0 , kU kfro ≤ d0 and SV ∈ Rd0×d, kSV kfro ≤ B.
Let Ub ∈ Ub satisfy kUb - U kfro ≤ U, and Vb ∈ Vb satisfy kVb - SV kfro ≤ V . Let Mc = UbVb. Then
we obtain
-	-r^..	_____ ..	___ -r^..
kM - Mckop ≤ kM - truncd0 (M)kop + ktruncd0 (M) - Mckop
≤ /3+ kUSV -UbSVkop+ kUbSV -UbVbkop
≤ + UB + V d
Thus, setting U =	/3B, V = /3d0 , then we get a -cover of M with log cover size
b9dB2/2c(log 81d02B2/2). As d0 ≤ d, this simplifies to b36dB2 log(9d)/2c.
Now when d0 ≥ d, we simply take a Frobenius norm cover of d × d matrices with Frobenius norm
bound B, which by Claim B.1 has log size at most d2 log(3B/) ≤ b36dB2 log(9d)/2c, where the
inequality followed because 9B2/2 ≥ d.
Combining both cases, we get for all > 0,
36dB2 log(9d)
log NikOP (e, MfrO(B)) ≤	-----2------
□
The following claims are straightforward and follow from standard covering number bounds for
k ∙ ∣∣2 and IHII balls.
Claim B.1. Let MfrO (B) denote the class of d1 × d2 matrices with Frobenius norm bounded by B.
Thenfor 0 < e < B, log Nk∙kfro (e, MfrO(B)) ≤ d1d2 log(3B∕e).
Claim B.2. Let M∣∣∙k1,1 (B) denote the class of di × d2 matrices with the '1 norm of its entries
bounded by B. Then log2用限0 (e, M∣∣∙kι 1 (B)) ≤ 5[B2∕e2C log 10d.
C Generalization B ound for Smooth Function Compositions
In this sectiOn, we Present the bOund fOr general smOOth functiOn cOmPOsitiOns used tO PrOve TheO-
rem 3.1.
We will work in the same general setting as Section A. Let JjJi (x, δ) denote the i-to-j Jacobian
evaluated at fi-iji(x, δ), i.e. Jjji(x, δ)，Dhfji(h, δ)∣h=f-(x,δ). We will additionally
define general notation for hidden layer and Jacobian norms which coincides with our notation for
neural nets. Let si(x) , IfiJ1(x)I and s0(x) , IxI. As the function DfjJi outPuts oPera-
tors maPPing Di-1 to Dj, we can additionally define κjJi(x) , IDfjJi ◦ fi-1J1(x)IoP, with
κjJj+1 (x) , 1.
Let κ0i be an uPPer bound on the LiPschitz constant of DfiJi measured in oPerator norm:
IDfiJi(h) - DfiJi(h+ ν)IoP ≤ κ0iIνI
23
Published as a conference paper at ICLR 2020
Now We define the value Ki (x, y), which can be thought of as a LiPschitz constant for perturbation
δi in the definition of mF , as follows:
κi(χ,y)，Si-ι(χ)
8κki+1(χ) + χ 8κj.i+ι(χ)
Y(F (X),y)	J	Sj(X)
j=i
+ si-1(X)
j1
XX
1≤j2≤j1≤k j0=max{i+1,j2}
16
Kjo Kjθ-H+ι(x)Kjι jj0 + 1(x)Kj0-ijj2(x)
κj1Jj2(X)
+8
j2≤i≤j1
Kji-i+1 (X)Ki-1T2(x)
Kj112(X)
(C.1)
For this general setting, the following theorem implies that for any integer q > 0, if F classifies
all training examples correctly, then its error converges at a rate that scales with n-q/(q+2) and the
PrOdUCts kK?kLq(Pn)CHoP(Fi).
Theorem C.1. Let F = {fk ◦•••◦ fι : f ∈ Fi} denote a class of compositions of functions from
k families {Fi}k=ι, each Ofwhich satisfies Condition A.1 with operator norm ∣∣ ∙ ∣∣op and complexity
ClHloP (Fi). For any ChOiCe of integer q > 0, with probability 1 一 δ for all F ∈ F the following
bound holds:
Ep ['0-1(F (X),y)]
2
+ (1- EPn ['0-1(F(X),y)])中 O
n
≤ 2(EPn['0-1(F(X),y)]) + o(klogn + M10)
q/(q+2)
(X kκikL*n)Ck∙kop (Fi)2/3
where Sn denotes the subset of training examples correctly classified by F and Ki? is defined in (C.1).
In particular, if F classifies all training samples correctly, i.e. |Sn | = n, with probability 1 一 δ we
have
l 2	q/(q+2)	3q/(q+2)
Ep['0-1(F(X),y)]. q (十)	(X kKikLhcop (Fi)2/31	+ Z
where Z，O (k log n[og(1/8)) is a low order term.
To Prove this theorem, we will Plug the following lower bound on mF into Lemma A.2 with the
aPProPriate choice of smooth loss `, and Pick the oPtimal choice of {αi}ik=1 for the resulting bound.
We remark that we could also use Theorem A.1 as our starting Point, but this would still require
oPtimizing over {αi}ik=1.
Lemma C.1 (General version of Lemma 3.1). In the setting of Theorem C.1, where each layer Fi
is a class of smooth functions, ifγ(F(X), y) > 0, we have
mF(X,y) ≥ k{K?(X,y)/ai}k=ik-/1(p-i)
We Prove Lemma C.1 in Section D by formalizing the intuition outlined in Section 3. With
Lemma C.1 in hand, we can Prove Theorem C.1. This Proof will follow the same outline as the
Proof of Theorem A.1. The Primary difference is that we oPtimize over k values of αi, whereas
Theorem A.1 only oPtimized over the smoothness β.
Proofof Theorem C.1. We use 'β with β = 1 defined in Claim A.2 as a surrogate loss for the 0-1
loss. Since Claim A.2 gives '0-1(F(x), y) ≤ 'β=ι(mF(x, y)), by Lemma A.2 it follows that
Ep['0-1(F(X),y)] ≤ Ep['β=ι(mF(X,y))]
≤ 2EPn ['β=ι(mF(X,y))] + ci (%(Fw n + log≡+ loglogn
(C.2)
24
Published as a conference paper at ICLR 2020
Now We first note that for a misclassified pair, 'β=ι(mF(χ,y)) = '0-1(F(χ),y) = 1. For cor-
rectly classified examples, we also have the bound 'β=ι(mF(χ,y)) ≤』；篙* for constant c2
independent of q. Thus, it follows that
EPn ['e=l(mF (X，y))] ≤ EPn ['0-1(F (x),y)] + n X
(x,y)∈Sn
(c2q)q/2
mF (x,y)q
Plugging this into (C.2), we get with probability 1 - δ for all F ∈ F,
EPn['β=ι(mF(x,y))] ≤ 2Epn['0-1 (F(x),y)] + O(E + log(1∕δ) ；loglogn)	(C.3)
where E is defined by
E = 1 X	(c2q)q/2 + c2∙ιι(F )log2 n
n , JW mF(x,y)q	n
(x,y)∈Sn
(C.4)
Thus, it suffices to upper bound E. By Lemma C.1, we have mF (x, y)	≥
k{κ?(x, y)∕αi}k=ιk-1(p-i) for the choice of a,p used to define mF. We will setP = q/(q - 1) and
union bound (C.3) over choices of α.
First, for a particular choice of α and p = q/(q - 1), we apply our lower bound on mF (x, y) to
simplify (C.4) as follows:
E ≤ 1 X (c2q)q/2k(K?(x,y)/ai)k=ikq + C2∙ιι(F)lθg n
n	qn
(x,y)∈Sn
∣-	-I	3q-2
≤ X α-q T X K?(x,y)q + (X a”"%.% (Fi)2q/(3q-2)) q 呼
i=1	n	(x,y)∈Sn	i	n
(C.5)
For convenience, we use EF (α) to denote (C.5) as a function of α. Note that κi? depends on F.
Now let aF denote the mmimizer of EF(a). As we do not know the exact value of αF before the
training data is drawn, we cannot simply plug the exact value of aF into (C.5). Instead, we will
apply a similar union bound as the proof of Theorem A.1, although this union bound is slightly
more complicated because we optimize over k quantities simultaneously.
We use ξi to denote the lower limit on ai in our search over a, setting ξi =
CIHloP (Fi)TPoly(k-1n-1).4 * Now we consider a grid of {abi}ik=1, where ab has entries of the form
abi = ξi2j for any j ≥ 0. For a given choice of ab, we assign it failure probability
b = δ
=Qi 2bi∕ξ
where δ is the target failure probability after union bounding. First, note that
Xb=δX…X .，一 ≤δ
^1	^1	^1 2jι+-+jk + k -
j1 ≥0	jk ≥0
(C.6)
Therefore, with probability 1 - δ, we get that (C.2) holds for mF defined with respect to every ab.
In particular, with probability 1 - δ, for all F ∈ F and ab in the grid,
EP ['o-i(F (x),y)]
≤ EPn ['β=i(mF (x,y))]
/ 3	万	Pi log3"ξi)+log(1/6) + loglog n∖
≤ 2EPn['o-i(F(X), y)] + O [EF(a) +----------------n-----------------1
(C.7)
4If Ck∙kop (Fi) = 0, then we simply set αi = ∞, which is equivalent to restricting the perturbations used in
computing mF to layers where C∣∣∙kop(Fi) > 0.
25
Published as a conference paper at ICLR 2020
where the last term was Obtained by subsituting (C.6) fOr the failure PrObability.
NOw we claim that there is sOme chOice Of αb in the grid such that either
于「、工 EiIOg(2&”&)+ log(10+logbg nv Q 币? ?	k log(n)+log(10
EF (α) H------------------------------≤ 9Ef (Qf ) + O (--------------
n
n
(C.8)
〜
〜
or EF(aF) & 1 (in which case it is trivial to obtain generalization error bounded by EF(aF)).
To see this, We first consider b in our grid such that ai ∈ [ɑ不 分，2αF 2 + ξi]. By construction of our
grid of αb, such a choice always exists. Then we have
〜...
EF (a)
k
= X αbi-q
i=1
(c2q)q/2
n
κi? (x, y)q
(x,y)∈Sn
3q-2
+ (X b2q∕"Ck∙kop (Fif))	q 修
k
≤ X αF,i-q
i=1
(c2q)q/2
E	κi(x,y)q
(x,y)∈Sn
n
(3q-2)/q	2
+ 9 (XaF,i2q/(3q-2)Cklop(Fi)2"(3q-2))	lognn + poly(n-1)
The first term we obtained because ai ≥ a"” and the second via the upper bound ai ≤ 2αF i + ξi.
〜
〜
Thus, for some choice of αb in the grid, we have EF (αb) ≤ 9EF (αF? ) + Poly(n-1). Furthermore,
if αF > c ∙ Ck∙kop(Fi) 1n for some constant c, we note that EF(αF) & 1 - thus, it suffices to only
consider αF ≤ C ∙。/八叩(Fi)-1n. In particular, we only need to consider ai where log(2bi∕ξi).
lOg kn. Finally, we note that we can assume WLOG that k . n otherwise (C.8) would give a trivial
bound. Combining these facts gives (C.8).
Thus, it follows that for all F ∈ F,
3
EP['0-1(F(X),y)] ≤ 2EPn['o-i(F(X),y)] + O
klOgn + lOg(1∕δ)
Qf )+-----------------
n
(C.9)
Finally, we can apply Lemma C.2 using zi
(c2q)q/2
n
?	q 1/q
E(x,y)∈Sn Ki (χ,y)q)
to get
ClHIOP (Fi)IOg n
√n
Ef (qF).(乎 V q
q/(q+2)
(X MkL*n)Ck∙kop (Fi)2/3
3q/(q+2)
Substituting intO (C.9) gives the desired bOund.
□
Lemma C.2. For coefficients {zi}ik=1, {bi}ik=1 > 0 and integer q > 0, define
(3q-2)/q
E(α) , X zq∕αq+ (X a2q/(3q-2)b2q/(3q-2))
with minimizer α? and minimum value E?. Then
3q/(q+2)
26
Published as a conference paper at ICLR 2020
Proof. Choose {ɑi}k=ι as follows (We obtained this by solving for α for which PaE(α) = 0):
αi
2
3q
Xzi2/3bi2/3
i
2-2q
q(q+2)
q）入产⅛
For this particular choice of α, we can compute
Ezi7αq
i
Xzi2/3bi2/3
i
Xzi2/3bi2/3
i
3q
q + 2
2q-2
^+2^
2 q — 2
^q÷2
Likewise, we can also compute
(3q-2)/q
α2q∕(3q-2)b2q∕(3q-2) ∖
3q2+4q-4+4-4q
q(q+2)
3q
q + 2
Finally, we note that 居)2/(q+2) + (22)-q+2 ≤ 2, so we obtain
E? ≤ E(α) ≤ 2
32∕(2+2)
□
D LOWER B OUNDING mF FOR SMOOTH LAYERS
In this section, we prove Lemma C.1, which states that when the function F is a composition of
functions with Lipschitz derivative, we will be able to lower bound mF (x, y) in terms of the inter-
mediate Jacobians and layer norms evaluated at x. To prove Lemma C.1, we rely on tools developed
by (Wei and Ma, 2019) which control the change in the output of a composition of functions if all
the intermediate Jacobians are bounded.
First, we define the soft indicator l≤t as follows:
1	if z ≤ t
l≤t(z) =	2 — z/t if t ≤ z ≤ 2t
0	if 2t ≤ z
We also define the ramp loss Tρ as follows:
1	if z ≥ ρ
Tρ(z) =	z/p if 0 ≤ z<ρ
0	if z < 0
Using the techniques of (Wei and Ma, 2019), we work with an “augmented” indicator which lower
bounds the indicator that the prediction is correct, l[γ(F(x, δ), y) ≥ 0]. We define this augmented
27
Published as a conference paper at ICLR 2020
indicator by
I(δ;x,y) , Tρ(γ(fk-ι(x,δ),y))	Y 1&(||力一也6)||) Y	l≤Tj一(k Jj(x,δ)k°p)
1≤i≤k-1	1≤i≤j≤k
(D.1)
for nonnegative parameters ρ,ti,τj^i which We will later choose to be the margin, hidden layer
norm, and Jacobian norms at the unperturbed input. Because the augmented indicator I(δ; x, y)
conditions on small Jacobian and hidden layer norms, it will turn out to be κ?(x, y)-Lipschitz in the
perturbation δi. Furthermore, by construction, the value of the augmented indicator I(δ; x, y) will
equal 1 when δ = 0, and we will also have
l[γ(F(χ,δ),y) ≥ 0] ≥ I(δ; x,y) ≥ 1 - £K?(X,y)∣δik
i
This immediately gives a lower bound on the perturbation level required to create a negative margin.
The lemma below formally bounds the Lipschitz constant of I(δ; x, y) in δi.
Lemma D.1. For nonnegative parameters α,τj-, ρ,with τ7∙-j+ι = 1 for any j and Tjf = 0 for
j ≤ j0 + 2, define the function I(δ; x, y) as in (D.1). Then in the setting of Lemma C.1, for a given
i ∈ [k], for all choices ofδi and ν, ifδj = 0forj > i, we have
|I(δi + ν,δ-i; x,y) -I(δi,δ-i; x,y)∣ ≤ 一|同|
for Ki defined asfollows:
κi ti-1
8Tk—i+ι
P
+
j1
XX
1≤j2≤j1≤k j0=max{i+1,j2}
16κ0j0
Tj0-1—i + 1TjI―j0 + 1τj0-1-j2
TjIT2
+8
Tj1—i + 1 Ti-IT2
Tj1 —j2
(D.2)
We prove Lemma D.1 in Section D.1. With Lemma D.1, we can formalize the proof of Lemma C.1.
ProofofLemma C.1. We will apply Lemma D.1, using t = si(x), P = γ(F(x),y), TjI =
KjI (x). First, note that for this choice of parameters, the Lipschitz constant Ki of Lemma D.1
evaluates to κi(x, y). Thus, it follows that for all δ,
|I(0; x, y) - I(δ; x, y)|
≤ E ∣I(δι,...,δi-i,δi = 0,δj>i = 0; x,y) -I(δι,...,δi ,δj> = 0; x,y)∣
i
≤ X K?(x,y)|Mik	(D.3)
i
Furthermore, by the definition ofI(δ; x, y), we have
l[γ(F(x, δ), y) ≥ 0] ≥ I(δ; x, y)
Finally, by our choice of the parameters used to define I(δ; x, y), we also have I(0; x, y) ≥ 1.
Combining everything with (D.3), we get
l[γ(F(χ,δ),y) ≥ 0] ≥ I(δ;χ,y) ≥ I(0;x,y) - Xκ?(X,y)kδik
i
≥ 1 -k{∙(x,y)∕αi}k=ιkp∕(p-i)k{αi∣Mik}k=ιkp
(since ∣∣ ∙ kp∕(p-i) and ∣∣ ∙ ∣∣p are dual norms)
= 1 - k{Ki?(X, y)∕αi}ik=1kp∕(p-1)∣∣∣δ∣∣∣
Thus, for any δ, if ∣∣∣δ∣∣∣ < |{k?(x, y)∕αi}k=ιk-/1(P-1), then l[γ(F(x, δ),y) ≥ 0] > 0, which
in turn implies γ(F(X, δ), y) ≥ 0. It follows by definition of mF (X, y) that mF (X, y) ≥
k{κ? (x,y)∕αi}k=1 k-∕1(p-1).	□
28
Published as a conference paper at ICLR 2020
D.1 Proof of Lemma D.1
To see the core idea of the proof, consider differentiating I(δ; x, y) with respect to δi (ignoring for
the moment that the soft indicators are technically not differentiable). Let the terms A1 , . . . , Aq
represent the different indicators which the product I(δ; x, y) is comprised of. Then by the product
rule for differentiation, we would have
DδiI(δ; x, y) =	Aj0(δ; x, y)DδiAj(δ; x, y)
j j0 6=j
Now the idea is that for every j, the product Qj0 6=j Aj0 (δ; x, y) contains an indicator that
DδiAj (δ; x,y) is bounded - this is stated formally by Lemmas D.3, D.4, and D.5. Informally,
this allows us to bound ∣∣DδiI(δ; x, y)k by the desired LiPSchitz constant Ki.
To formally prove this statement for the case of non-differentiable functions (as the soft-indicators
l≤t are non-differentiable), it will be convenient to introduce the following notion of product-
LiPschitzness: for functions A1 : DI → R+ and A2 : DI → R+, where DI is some normed
space, We say that function Ai is τ-product-Lipschitz w.r.t. A2 if there exists some c,C > 0 such
that for any ∣ν ∣ ≤ c and x ∈ DI, we have
|Ai(x + V) - Aι(ν)∣A2(x) ≤ TkV∣ + CIlVk2
We use the following fact that the product of functions which are product-Lipschitz with respect to
one another is in fact Lipschitz. We provide the proof in Section D.2.
Lemma D.2. Let Aι,...,Aq : DI → [0,1] bea set OfLipschitzfunctions such that Ai is Ti-product-
Lipschitz w.r.t Hj= Aj for all i. Then the product Y[i Ai is 2 Ei Ti -Lipschitz.
Now we proceed to formalize the intuition of product-rule differentiation presented above, by show-
ing that the individual terms in I(δ; x, y) are product-Lipschitz with respect to the other terms. For
the following three lemmas, we require the technical assumption that for any fixed choice of x, δ-i,
the functions 力.i(x, δ), Jj,-j∙00(x, δ) are worst-case Lipschitz in δi as measured in ∣∣ • k, k ∙ kop,
respectively, with Lipschitz constant C0. Our proof of Lemma D.1, however, can easily circumvent
this assumption. The proofs of the following three lemmas are given in Section D.2.
Lemma D.3. Choose i, j with k - 1 ≥ j ≥ i. Then after we fix any choice of
x,δ-i, the function l≤j (∣f∙.ι(x,δ)∣) is 4τj~i+1tiτ -ProdUCt-LiPSChitz in δi with respect to
l≤Tji (kJj-i+l(x,δ)kop)l≤ti-1 (kfi-i-i(x,δ)k).
Lemma D.4. Choose i ≤ k.	Then after we fix any choice of x, δ-i, the func-
tion Tρ(γ (fk-ι(x,δ),y)) is "kijti-1 -product-Lipschitz in δi with respect to
l≤Tji (kJl+l(x,δ)kop) I≤ti-1 (kfi-1-1 (x,δ)k).
Lemma D.5. Choose i,j1,j2 with ji ≥ j2, ji > i. Set product-Lipschitz constant τ asfollows:
_	8 (TjiJi+1 τi-1-j2 + ti-1 EjLmax{j2,i+I}≤j0≤j1 κj0 τj0 -1-i+1τjι-j0 + 1τj0 -1-j2
T =---------------------------------------------------------------------------
Tj1 Jj2
Then for any fixed choice of	x, δ-i	satisfying δj =	0 for j	>	i, the func-
tion l≤τj1 一2 (kJ'1Jj2(x, δ)kop) is	T-product-Lipschitz	in δi	with respect to
I≤ti-1 (kfi-iJi(x, δ)k) Qi≤j00≤j0≤k 1」,-(kJ∙0jj00(x,δ)kop). Here note that we have
Ti-1Jj2 = 0 ifi - 1 ≤ j2 + 1.
Given the described steps, we will now complete the proof of Lemma D.1.
Proof of Lemma D.1. We first assume that the conditions of Lemmas D.3, D.4, D.5 regarding C0-
worst-case Lipschitzness hold. We note that I(δ; x, y) is a product which contains all the functions
appearing in Lemmas D.3, D.4, and D.5. Thus, Claim D.1 allows us to conclude that each term in
I(δ; x, y) is product-Lipschitz with respect to the product of the remaining terms. As these lemmas
also account for all the terms inI(δ; x, y), we can thus apply Lemma D.2, where each Ai is set to be
a term in the product forI(δ; x, y). Therefore, to bound the Lipschitz constant in δi ofI(δ; x, y), we
sum the product-Lipschitz constants given by Lemmas D.3, D.4, and D.5. This gives that I(δ; x, y)
is κKi -Lipschitz in δi for κKi defined in (D.2).
29
Published as a conference paper at ICLR 2020
Now to remove the C0 worst-case Lipschitzness assumption, we can follow the reasoning of Claim
D.6 of (Wei and Ma, 2019) to note that such Lipschitz constants exist if we restrict δi to some
compact set, and thus conclude the lemma statement for δi restricted to this compact set. Now we
simply choose this compact set sufficiently large to include both δi and δi + ν.	口
D.2 Proofs for Product-Lipschitz Lemmas
Proof of Lemma D.2. As each Ai is Lipschitz and there are a finite number of functions, there exists
C0 such that any possible product A” Ai? •一Aij is C0-Lipschitz. Furthermore, by the definition of
product-Lipschitz, there are c, C > 0 such that for any kν k ≤ c, x ∈ DI, and 1 ≤ i ≤ q, we have
∣(Ai(x + ν) - Ai(X))I Y Aj(x) ≤ TikVk + CkVk2
j6=i
Now we note that
Y Ai(X + V) - Y Ai (X) = X(Y Aj (x + V) Y Aj (XI(Ai(X + V)- Ai(X))	(D.4)
i	i	i j=1	j=i+1
Now for any i, we have
i-1	q
|(Ai(X+V) -Ai(X))YAj(X+V) Y Aj(X)|
j=1	j=i+1
i-1	q
≤ |(Ai(X+V)-Ai(X))|(YAj(X)+C0kVk) Y Aj (X)	(as Qij-=1 ii1 Aj is C0-Lipschitz)
j=1	j=i+1
≤ |Ai(X+V)-Ai(X)|(YAj(X)+C0kVk)
j6=i
We used the fact that Qjq=i+1 Aj (X) ≤ 1. Now we have |Ai(X + V) - Ai(X)| Qj6=i Aj(X) ≤
TikVk + CkVk2, and ∣Ai(X + V) - Ai(∕)∣C0|同| ≤ C02|同|2 as Ai is C0-Lipschitz, so
i-1	q
I(Ai(X + V) - Ai(x)) Y Aj(x + v) Y Aj(x)| ≤ TikVk + (C + C02)用2
j=1	j=i+1
Plugging this back into (D.4) and applying triangle inequality, we get
I Y Ai(X + ν) - Y Ai(X)∣ ≤ kνk (χTi + q(C + C02)kνk)
ii	i
Define the constant C00，min{c,勺(*；02)}. For any X and all V satisfying ∣∣νk ≤ C00, We have
IYAi(X + v) - YAi(X)I ≤ 2∣vk XTi	(D.5)
ii	i
Now for any x, y ∈ DI, we wish to show ∣ Qi Ai(y) - Qi Ai(X)I ≤ 2∣x - yk Pi Ti. To this end,
We divide X - y into segments of length at most C00 and apply (D.5) on each segment.
Define x(j) = X + jkC-yj(y - χ) for j = 1,..., [∣∣χ - yk∕C00C∙ Then as ∣∣χ(j) - χ(j-1)k ≤ C00,
we have ∣ Qi Ai(Xj)) - Qi Ai(xjT))I ≤ ∣∣x(j) - x(j-1)k Pi Ti. Furthermore, we note that
the sum of all the segment lengths equals ky - Xk. Thus, we can sum this inequality over pairs
(X, X(1)), . . . , (X(bkx-yk/C00c), y) and apply triangle inequality to get
i Y Ai(y) - Y Ai(X)I ≤ 2kχ-ykXTi
as desired.
□
30
Published as a conference paper at ICLR 2020
Proof of Lemma D.3. For convenience, define
A(x,δ) , l≤Tjj+ι(kJji+ι(x,δ)kop)I≤ti-1 (k-)k)
We first note that Dδif∙一ι(χ, δ), the partial derivative of f∙-1 with respect to δi, is given by
J∙ji+ι(χ,δ)kfi-ι.ι(x,δ)k by ClaimD.2. As J∙ι+ι(χ,δ), kfi-ι.ι(χ,δ)k are both worst-case
Lipschitz in δi with some Lipschitz constant C0, we can apply Claim H.4 of (Wei and Ma, 2019) to
obtain:
IIfjJι(χ,δ-i,δi + V)- fjι(x,δ-i,δi)k ≤ (∣∣Dδifjτ(χ,δ)kop + C 00/2IlV II)IlVk
for any V and some Lipschitz constant C00. Thus, by the t-1 LiPSchitz-ness of the indicator l≤j,
we have
|l≤tj(kfjJi(x,δ-i,δi + V)k) - l≤tj(IIfjJι(x,δ-i,δik)∣A(x,δ)
≤ A(x, δ)
∣∣fjjl(x, δ-i, δi + V) — fjji(x, δ-i, δi)I
tj
W A( δ (IDδifjJi(x,δ)Iop + C00∕2∣∣VIl)IlVk
≤ A(X,δ)	-
tj
W A δ (151+1(/，训后||力-1.1(/,训|+ C00/2IlVIl)IIVll
≤ A(X,δ)	-
tj
Now by definition of A(X, δ), we get that the right hand side equals 0 if IJjJi+1(X, δ)Iop ≥
2τjJi+1 or Ifi-1J1(X, δ)I ≥ 2ti-1. Thus, the right hand side must be bounded by
4τjji+1ti-1 ∣∣v∣∣ + c 00/2312
tj
which gives PrOdUCt-LiPSChitzneSS with constant 4Tj~；+lti-1.	□
Proof of Lemma D.4. This proof follows in an identical manner to that of Lemma D.3. The only
additional step is using the fact that γ(h, y) is 1-Lipschitz in h, so the composition Tρ(γ(h, y)) is
PT-LiPSChitz in h.	□
Proof of Lemma D.5. Let C0 be an upper bound on the Lipschitz constant in δi of all the functions
Jj0Jj00 (X, δ). As we assumed that each fj has κ0j-Lipschitz Jacobian, such an upper bound exists.
We first argue that
IJj1 Jj2 (X, δ-i , δi + V) - Jj1 Jj2 (X, δ-i , δi )Iop
≤ IVI	X	κ0j0(IJj1Jj0+1(X, δ)Iop + C0/2IVI)
j0nax{j2,i+l}≤j0≤j1	(D.6)
•(|山-11+156)临||力-1-1也谢 + C 00/23|)|[ 4，-1—2也6)临)
+IVI(IJj1Ji+1Iop+C0/2IVI)IJi-1Jj2Iop
for some Lipschitz constant C00. The proof of this statement is nearly identical to the proof of Claim
D.3 in (Wei and Ma, 2019), so we only sketch it here and point out the differences. We rely on the
expansion
JjI Jj2 (X, δ) = JjIJjI (X，δ) Jj1-1Jjl-1(x, δ) ∙∙∙ Jj2 Jj2 (X, δ)
which follows from the chain rule. Now we note that we can compute the change in a single term
Jj0Jj0 (X, δ) from perturbing δi as follows:
IJj 0Jj0 (X, δ-i , δi + V) - Jj0Jj 0 (X, δ-i , δi )Iop
= IIDhfj0Jj0 (h, δ)lh=fjθ-ι^ι(x,δ-i ,δi+ν) — Dhfj0Jj0 (h, 6 |h=fj0-n (x,δ-i 也)∣∣op
31
Published as a conference paper at ICLR 2020
Note that when j0 > i, by assumption δjo = 0, so Dh j f(h, δ) = DhfjO (h). Thus, as j has
κ0j0 -Lipschitz derivative, we get
k JjOJjO(X, δ-i, δi + V) ― Jj0 JjO(X, δ-i, δi ) k op
≤ κjo kfj0-1j1 (x, δ-i, δi + V) - fjθ-ljl(x, δ-i, δi)k
(since the derivative of fjO is κ0jO-Lipschitz)
≤ κ0jO (kDδi fjO-1J1(X, δ-i, δi)kop +C00/2kVk)kVk	(by Claim H.4 of (Wei and Ma, 2019))
≤ κ0jO (kJjO-1Ji+1(X, δ)kopkfi-1J1(X, δ)k + C00/2kVk)kVk
We obtained the last line via Claim D.2. We note that the cases when j0 > i contribute to the terms
under the summation in (D.6).
When j0 = i, we have DhfiJi(h, δ) = Dhfi(h) + Dh[δikhk] for any h, so kDhfiJi(h,δi,δ-i) -
DhfiJi(h, δi + V, δ-i)kop = kDh[Vkhk]kop ≤ kVk. As this holds for any h, it follows that
kJiJi(X, δ-i, δi + V) - JiJi(X, δ-i, δi)kop ≤ kVk. This term results in the last quantity in (D.6).
Finally, whenj0 < i, we have JjOJjO (X, δ-i, δi +V) = JjOJjO (X, δ-i, δi) as JjOJjO does not depend
on δi .
To see how (D.6) follows, we would apply the above bounds in a telescoping sum over indices j0
ranging from max{j2, i} toj1. For a more detailed derivation, refer to the steps in Claim D.3 of (Wei
and Ma, 2019).
Now for convenience define
A(x,δ) , I≤ti-1 (kfi-1jl(x,δ)k)	Y	l≤Tj,^j,0(kJjojjoo(x,δ)kop)
1≤jOO≤jO≤k
Note that if any of the bounds set by the indicators in A(X, δ) are violated, then A(X, δ) = 0, and
thus
ll≤Tjιf (kJjlJj2 (x, δ-i, δi + V )kop ) - l≤Tj∙ιf (kJjιJj2 (x, δ-i, δi)kop)∣A(x, δ) = 0
In the other case, we have kfi-1J1(X, δ)k ≤ 2ti-1, and kJjOJjOO (X, δ)kop ≤ 2τjOJjOO, in which
case (D.6) can be bounded by
kJj1Jj2(X,δ-i,δi+V)-Jj1Jj2(X,δ-i,δi)kopA(X,δ)
≤ 8ti-1 I	E	κj0τj0-1Ji+1τjιJj0 + 1τj0-1Jj2 I kνk
j0nax{j2,i+l}≤j0≤j1
+ τj1Ji+1τi-1Jj2 kVk + C000kVk2
for some C000 that is independent of x, δ, V. Thus, by LiPSchitz-ness of 1二7力 ^j2 (∙) and the triangle
inequality, we have
I Hτj1 一 2 (kJj—2 (x,δ-i,δi + V )kop) - l≤τj1^j2 (kJj—2 (x,δ-i,δi)kop )∣A(x,δ) ≤
(D.7)
8 (Tjιi+ιτi-ι-j2 + ti-ι PjOmax{j2,i+1}≤j0≤ji KjO TjO-1^i+1Tj1^jO + 1TjO-1^j2^ k νk
Tj1—j2
+C000kνk2
This gives the desired result.
□
Claim D.1. Let A1,A2,A3 : DI → [0,1] be functions where Ai is T-product-Lipschitz w.r.t. A2.
Then Ai is also T-product Lipschitz w.r.t. A2A3.
Proof. This statement follows from the definition of product-Lipschitzness and the fact that
|Ai(X+ V) - Ai(V)|A2(X)A3(X) ≤ |Ai(X+V) -Ai(V)|A2(X)
since A3(x) ≤ 1.	□
Claim D.2. The partial derivative of fjJi with respect to variable δi evaluated at X, δ can be
computed as
Dδi fjJi(X, δ) = JjJi+i (X, δ)kfi-iJi(X, δ)k
32
Published as a conference paper at ICLR 2020
Proof. By definition, fj.ι(x,δ) = fj1+ι(fi.ι(x,δ),δ). Thus, we note that fj.ι(x,δ) only
depends on δi through 力.ι(x, δ), so by chain rule We have
Dδi fj-1(x,δ) = Dhfj—i+1 (h, δ)∖h= fiι^ι(x,δ)D δi fi^1(x, δ)
=Jjι+ι(x, δ)Dδi[fi(fi-ιτ(x, δ)) + δikfi-ι一ι(x, δ)k]
=Jji+l(x, δ)kfi-i.ι(x,δ)k
In the second line, we invoked the definition of Jj1+i (x, δ).	□
Claim D.3. We have the expansion
Jj―i (x, δ) = Jj—j (x, δ) ∙ ∙ ∙ JiJi (x, δ)
Proof. This is a result of the chain rule, but for completeness we state the proof here. We have
Dhfj-i(h, δ) = Dhfj(fj-i-i(h, δ),δ)
=Dh fj-j(h0, δχh0=fj-11(h,δ)Dhfj-1-i(h, δ)	(by chain rule)
(D.8)
Thus, plugging in h = fi-1-1(x, δ), we get
JjI(X,δ = Dhfj-Kh, 6∖h=fi-1 一ι(χ,δ)
=DhO fj-j (h，δ) ∖h0 = fj-11(fi-1 ji(x,δ),δ)Dhfj-H(h, δ)∖h=fi-ι^ι(x,δ)
=DhO fj-j (h , δ) ∖h0 = fj-ι^ι(x,δ)Jj-1-i(x, δ)
= Jj-j (x, δ)Jj -1-i (x, δ)
Now we can apply identical steps to expand Jj-ι-i(x, δ), giving the desired result.	□
E Proofs for Adversarially Robust Classification
In this section, we derive the generalization bounds for adversarial classification presented in Sec-
tion 4. Recall the adversarial all-layer margin maFdv(x, y) , minxO ∈Badv (x) mF (x0, y) defined in
Section 4. In this section, we will use the general definition ofmF in (A.1).
We will sketch the proof of Theorem E.1 using the same steps as those laid out in Sections 2 and A.
We will rely on the following general analogue of Theorem C.1 with the exact same proof, but with
κ? (defined in Section C) replaced by KadV(x, y)，maXχθ∈Badv(χ) k?(x0, y) everywhere:
Theorem E.1. Let F = {fk ◦•••◦ fι : f ∈ Fi} denote a class of compositions of functions from
k families {Fi}k=ι, each Ofwhich satisfies Condition A.1 with operator norm ∣∣ ∙ ∣∣op and complexity
ClHloP (Fi). For any choice of integer q > 0, with probability 1 一 δ for all F ∈ F the following
bound holds:
EP ['0dv(F (x),y)]
≤ 3 (EPn['0dv(F(X),y)]) + o(klogn + M10 )
q	_q_	/	∖ -3⅛
+ (1 -EPn['0dv(F(X),y)])q+2O q(学广(X YkL%"HoP(W3)"2
where Snadv denotes the subset of training examples correctly classified by F with respect to adver-
sarial perturbations. In particular, if F classifies all training samples with adversarial error 0, i.e.
∖Snadv ∖ = n, with probability 1 一 δ we have
2	q/(q+2)	3q/(q+2)
q (^^^l-^)	(X kκadvkL/;SadV)Ck∙kop(Fi)2/3)	+ Z
where ζ , O
k log n+log(1∕δ)
is a low order term.
n
33
Published as a conference paper at ICLR 2020
Given Theorem E.1, Theorem 4.1 follows with the same proof as the proof of Theorem 3.1 given in
Section B. To prove Theorem E.1, we first have the following analogue of Lemma A.1 bounding the
covering number of madv ◦ F.
Lemma E.1. Define madv ◦ F , {(x, y) 7→ maFdv (x, y) : F ∈ F}. Then
N∞(3madv ◦F) ≤∙¼H∣∣(gF)
This lemma allows us to invoke Lemma A.2 on a smooth loss composed with madv ◦ F, as we did
for the clean classification setting. The lemma is proven the exact same way as Lemma A.1, given
the Lipschitz-ness of maFdv below:
人 !- ♦--GY L	一八 ..「71	Ir ，•	T-I C c ~∖ k π	ι
Claim E.1. For any x, y ∈ D0 × [l], and function sequences F = {fi}ik=1, F = {fi}ik=1, we have
|maFdv(x, y) - maFbdv(x,y)| ≤ |||F - Fb|||.
Proof. Let x? ∈ Badv(x) be such that maFdv (x, y) = mF (x?, y). By Claim A.1, we have
mFdv(χ, y) ≤ mF(χ? v) ≤ mF(χ? y) + M - F||| = mFdv(χ, y) + M - F|||
We can apply the reverse reasoning to also obtain maFdv(x, y) ≤ mabdv(x, y) + |||F - F |||. Combining
the two gives Us the desired result.	口
Next, we lower bound maFdv when each function in F is smooth.
Lemma E.2. In the setting of Lemma C.1, let KadV(x,y) ， maXχo∈Badv(χ) κ*(x0, y). Then if
'0dv(F(x),y) = 0, we have
mFd(χ,y ≥k{κadv(χ,v)∕θi}∖=ιk-1(p-i)
Proof. By definition and Lemma C.1, we have
mFdv(χ,y)= "/乳)mF(X0,y) ≥ χo∈⅛1(χ) M螳HLlik/p-i)
_	1
maxx0∈Badv(x) ∣∣{κi (X, y) /ai}i=1kp/ (p-1)
1
≥	TTT	Tk
∣ {maxX，∈Badv(x)K?(x0,y)/ai}i=i kp/(p-i)
_	1
=k{κadv(x0,y)∕αi}k=ιkp∕(p-i)
□
To finish the proof of Theorem E.1, we use 'β=ι(mθdv(χ, y)) as an upper bound for '0d1(F(χ),y)
and follow the steps of Theorem C.1 to optimize over the choice of {αi}ik=1. As these steps are iden-
tical to Theorem C.1, we omit them here. With Theorem E.1 in hand, we can conclude Theorem 4.1
using the same proof as Theorem 3.1.
F Additional Experimental Details
F.1 Additional Details for Clean Classification Setting
For the experiments presented in Table 1, the other hyperparameters besides t and
ηperturb are set to their defaults for WideResNet architectures. Our code is inspired by
the following PyTorch WideResNet implementation: https://github.com/xternalz/
WideResNet-pytorch, and we use the default hyperparameters in this implementation. Al-
though we tried a larger choice of t, the number of updates to the perturbations δ, our results did not
depend much on t.
34
Published as a conference paper at ICLR 2020
Table 3:	Validation error on CIFAR-10 for VGG-19 architecture trained with standard SGD and
AMO.
Arch. Standard SGD AMO
VGG-19 5.66%	5.06%
Table 4:	Validation error on CIFAR for models trained with dropout vs. AMO. We tuned the dropout
probability p and display the best-performing value.
Dataset Arch.
CIFAR-10
WRN16-10
WRN28-10
CIFAR-100
WRN16-10
WRN28-10
Tuned Dropout	AMO
4.04%(p = 0.1)	3.42%
3.52%(p = 0.1)	3.00%
19.57%(p = 0.1)^^19.14%
18.77%(p = 0.2)^^17.78%
In Table 3, we demonstrate that our AMO algorithm can also improve performance for conventional
feedforward architectures such as VGG (Simonyan and Zisserman, 2014). We report the best vali-
dation error on the clean classification setting. For both methods, we train VGG-19 for 350 epochs
using weight decay 0.0005 and an initial learning rate of 0.05 annealing by a factor of 0.1 at the
150-th and 250-th epochs. For the AMO algorithm, we optimize perturbations δ for t = 1 steps and
use learning rate ηperturb = 0.01.
In Table 4, we demonstrate that our AMO algorithm can offer improvements over dropout, which
is also a regularization method based on perturbing the hidden layers. This demonstrates the value
of regularizing stability with respect to worst-case perturbations rather than random perturbations.
For the combinations of CIFAR dataset and WideResNet architecture, we tuned the level of dropout
and display the best tuned results with the dropout probability p in Table 4. We use the same
hyperparameter settings as described above.
F.2 Additional Details for Robust AMO
Our implementation is based on the robustness library by (Engstrom et al., 2019)5. For both meth-
ods, We produce the perturbations during training using 10 steps of PGD with '∞ perturbations with
radius = 8. We update the adversarially perturbed training inputs with a step size of 2 in pixel
space. During evaluation, we use 50 steps of PGD with 10 random restarts and perturbation radius
= 8. For all models, we train for 150 epochs using SGD with a learning rate decay of 0.1 at
the 50th, 100th, and 150th epochs. For the baseline models, we use a weight decay of 5e-4, initial
learning rate of 0.1, and batch size of 128.
For both models trained with robust AMO, the perturbations δ have a step size of 6.4e-3, and we
shrink the norm of δ by a factor of 0.92 every iteration. For the WideResNet16 model trained with
robust AMO, we use a batch size of 128 with initial learning rate of 0.1 and weight decay 5e-4. For
the WideResNet28 model trained using robust AMO, we use a batch size of64 (chosen so that the δ
vectors fit in GPU memory) with initial learning rate of 0.707 (chosen to keep the covariance of the
gradient update the same with the decreased batch size). We use weight decay 2e-4. We chose these
parameters by tuning.
5https://github.com/MadryLab/robustness
35