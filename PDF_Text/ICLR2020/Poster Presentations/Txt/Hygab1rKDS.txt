Published as a conference paper at ICLR 2020
Quantum Algorithms For Deep Convolutional
Neural Network
Iordanis Kerenidis, Jonas Landman & Anupam Prakash
Institut de Recherche en Informatique Fondamentale (IRIF)
Universite de Paris, CNRS
Paris, France
landman@irif.fr
Ab stract
Quantum computing is a powerful computational paradigm with applications in
several fields, including machine learning. In the last decade, deep learning, and
in particular Convolutional Neural Networks (CNN), have become essential for
applications in signal processing and image recognition. Quantum deep learning,
however, remains a challenging problem, as it is difficult to implement non lin-
earities with quantum unitaries Schuld et al. (2014). In this paper we propose a
quantum algorithm for evaluating and training deep convolutional neural networks
with potential speedups over classical CNNs for both the forward and backward
passes. The quantum CNN (QCNN) reproduces completely the outputs of the
classical CNN and allows for non linearities and pooling operations. The QCNN
is in particular interesting for deep networks and could allow new frontiers in the
image recognition domain, by allowing for many more convolution kernels, larger
kernels, high dimensional inputs and high depth input channels. We also present
numerical simulations for the classification of the MNIST dataset to provide prac-
tical evidence for the efficiency of the QCNN.
1	Introduction
The growing importance of deep learning in research, in industry and in our society will require
extreme computational power as the dataset sizes and the complexity of these algorithms is expected
to increase. Quantum computers are a good candidate to answer this challenge. The recent progress
in the physical realization of quantum processors and the advances in quantum algorithms increases
the importance of understanding their capabilities and limitations. In particular, the field of quantum
machine learning has witnessed many innovative algorithms that offer speedups over their classical
counterparts Kerenidis et al. (2019); Lloyd et al. (2013; 2014); Kerenidis & Prakash (2017b); Wiebe
et al. (2014a).
Quantum deep learning refers to the problem of creating quantum circuits that mimic and enhance
the operations of neural networks. It has been studied in several works Allcock et al. (2018); Reben-
trost et al. (2018); Wiebe et al. (2014b) but remains challenging as it is difficult to implement non
linearities with quantum unitaries Schuld et al. (2014). In this work we propose a quantum algorithm
for convolutional neural networks (CNN), a type of deep learning designed for visual recognition,
signal processing and time series. We also provide results of numerical simulations to evaluate the
running time and accuracy of the quantum convolutional neural network (QCNN). Note that our
algorithm is theoretical and could be compiled on any type of quantum computers (trapped ions,
superconducting qubits, cold atoms, photons, etc.)
The CNN was originally developed by LeCun et al. (1998) in the 1980’s. They have achieved great
practical success over the last decade Krizhevsky et al. (2012) and have been used in cutting-edge
domains like autonomous cars Bojarski et al. (2016) and gravitational wave detection George &
Huerta (2018). Despite these successes, CNNs suffer from computational bottlenecks due to the
size of the optimization space and the complexity of the inner operations, these bottlenecks make
deep CNNs resource expensive.
1
Published as a conference paper at ICLR 2020
The growing interest in quantum machine learning has led researchers to develop different variants
of Quantum Neural Networks (QNN). The quest for designing quantum analogs of neural networks
is challenging due to the modular layer architecture of the neural networks and the presence of non
linearities, pooling, and other non unitary operations, as explained in Schuld et al. (2014). Several
strategies have been tried in order to to implement some features of neural networks Allcock et al.
(2018); Wiebe et al. (2014b); Beer et al. (2019) in the quantum setting.
Variational quantum circuits provide another path to the design of QNNs, this approach has been
developed in Farhi & Neven (2018); Henderson et al. (2019); Killoran et al. (2018). A quantum con-
volutional neural network architecture using variational circuits was recently proposed Cong et al.
(2018). However further work is required to provide evidence that such techniques can outperform
classical neural networks in machine learning settings.
2	Preliminaries
2.1	Convolution Product as Matrix Multiplication
We briefly introduce the formalism and notation concerning classical convolution product and its
equivalence with matrix multiplication. More details can be found in Appendix (Section C). A
single layer ' of the classical CNN does the following operations: from an input image X' ∈
RH'×W'×D' seen as a 3D tensor, and a kernel K' ∈ RH×W×D'×D'+1 seen as a 4D tensor, it
performs a convolution product and outputs X'+1 = X' * K', with X'+1 ∈ RH'+1×W'+1×D'+1.
This convolution operation is equivalent to the matrix multiplication A' F' = Y'+1 where A', F'
and Y'+1 are suitably vectorized versions of X', K' and X'+1 respectively. The output of the layer
' of the CNN is f (X'+1) where f is a non linear function.
2.2	Quantum Computing
For a detailed introduction to quantum computing and its applications to machine learning in the
context of this work, we invite the reader to look at Appendix F. We also refer to Nielsen & Chuang
(2002b) for a more complete overview of quantum computing.
In this part we will discuss only briefly the core notions of quantum computing. Like a classical
bit, a quantum bit (qubit) can be |0i, |1i, but can also be in a superposition state α |0i + β |1i with
amplitudes (α,β) ∈ C such that ∣α∣2 + ∣β∣2 = 1. With n qubits it is then possible to construct
a superposition of the 2n binary combinations possible, each with a specific amplitude. We will
note the ith combination (e.g. |01 … 110))as |i). A vector V ∈ Rd can be encoded in a quantum
state made of dlog(d)e qubits. This encoding is a quantum superposition, where the components
(vι, ∙∙∙ , Vd) of v are used as the amplitudes of the d binary combinations. We note this state
|v):=6 Pi∈[d] Vi |i), where |i) is a register representing the ith vector in the standard basis.
Quantum computation proceeds by applying quantum gates which are defined to be unitary matrices
acting on 1 or 2 qubits, for example the Hadamard gate that maps |0) → √^(|0) + |1)) and |1) →
√12(|0) 一 |1)). The output of the computation is a quantum state that can be measured to obtain
classical information. The measurement ofa qubit α |0i +β |1i yields either 0 or 1, with probability
equal to the square of the respective amplitude. A detailed discussion of the results from quantum
machine learning and linear algebra used in this work can be found in Appendix (Section F).
3	Main results
In this paper, we design a quantum convolutional neural network (QCNN) algorithm with a modular
architecture that allows for any number of layers, any number and size of kernels, and that can
support a large variety of non linearity and pooling methods. Our main technical contributions
include a new notion of a quantum convolution product, the development of a quantum sampling
technique well suited for information recovery in the context of CNNs and a proposal for a quantum
backpropagation algorithm for efficient training of the QCNN.
2
Published as a conference paper at ICLR 2020
The QCNN can be directly compared to the classical CNN as it has the same inputs and outputs.
We show that it offers a speedup compared to certain cases of classical CNN for both the forward
pass and for training using backpropagation. For each layer, on the forward pass (Algorithm 1),
the speedup is exponential in the size of the layer (number of kernels) and almost quadratic on the
spatial dimension of the input. We next state informally the speedup for the forward pass, the formal
version appears as Theorem D.1.
Result 1 (Quantum Convolution Layer)
Let X' be the input and K' be the kernel for layer ' of a convolutional neural network, and f :
R → [0, C ] with C > 0 be a non linear function so that f (X '+1) := f (X' * K') is the output for
layer '. Given X' and K' stored in QRAM (Quantum Random Access Memory), there is a quantum
algorithm that, for precision parameters e > 0 and η > 0, CreateS quantum state |f (X'+1)i such
that If(X '+1) 一 f (X '+1)∣∣ ≤ 2e and retrieves classical tensor X '+1 such that for each pixel j,
(∣x'+1 - f (Xj+v)∖ ≤ 2e if f (Xj+1) ≥ η
lxj+1 = 0	if f (Xj+1) <η
(1)
The running time of the algorithm is O ∙ √==='+= w where E(∙) represents the aver-
age value, O hides factors poly-logarithmic in the size of X' and K' and the parameter M =
maxp,q ∣∣Ap k kFq ∣∣ is the maximum product ofnormsfrom subregions of X' and K'.
We see that the number of elements in the input and the kernels appear only with a poly-logarithmic
contribution in the running time. This is one of the main advantages of our algorithm and it allows
us to use for larger and even exponentially deeper kernels. For the number of elements in the input,
their number is hidden in the precision parameter η in the running time. Indeed, a sufficiently
large fraction of pixels must be sampled from the output of the quantum convolution to retrieve the
meaningful information. In the Numerical Simulations (Section 6) we provide empirical estimates
for η. For details about the QRAM, see Appendix F.2.
Following the forward pass, a loss function L is computed for the output of a classical CNN. The
backpropagation algorithm is then used to calculate, layer by layer, the gradient of this loss with
respect to the elements of the kernels K', in order to update them through gradient descent. We state
our quantum backpropagation algorithm next, the formal version of this result appears as Theorem
E.1
Result 2 (Quantum Backpropagation for Quantum CNN)
Given the forward pass quantum algorithm in Result 1, and given the kernel matrix F', input
matrices A' and Y', StOred in the QRAMfor each layer ', and a loss function L, there is a quantum
backpropagation algorithm that estimates each element of the gradient tensor ∂L within additive
error δ ∣∣ dF ∣∣, and updates F' according to a gradient descent update rule. The running time of a
single layer ` for quantum backpropagation is given by
o (((〃(a')+μ( ∂Y+r)) κ( ∂∂F)+
(μ(∂Y+1)+ μ(F')) K(焉)) l⅞")
(2)
wherefor a matrix V ∈ Rn×n, K(V) is the condition number and μ( V) ≤ √n is a matrix dependent
parameter defined in Equation (5).
For the quantum back-propagation algorithm, we introduce a quantum tomography algorithm with
'∞ norm guarantees, that could be of independent interest. It is exponentially faster than tomog-
raphy with `2 norm guarantees and is given as Theorem G.1 in Section G. Numerical simulations
on classifying the MNIST dataset show that our quantum CNN achieves a similar classification
accuracy as the classical CNN.
4	Forward pass for QCNN
The forward pass algorithm for the QCNN implements the quantum analog of a single quantum
convolutional layer. It includes a convolution product between an input and a kernel, followed by
3
Published as a conference paper at ICLR 2020
the application of a non linear function and pooling operations to prepare the next layer’s input. We
provide an overview of the main ideas of the algorithm here, the complete technical details are given
in the Appendix (Section D).
Algorithm 1 QCNN Layer
Require: Matrix A' representing input to layer ' and kernel matrix F' stored in QRAM. Precision
parameters and η, a boolean circuit for a non linear function f : R 7→ [0, C].
Ensure: Outputs the data matrix A'+1 for the next layer which is the result of the convolution
between the input and the kernel, followed by a non linearity and pooling.
1:	Step 1: Quantum Convolution
1.1: Inner product estimation
Perform the following mapping, using QRAM queries on rows Ap and columns Fq, fol-
lowed by quantum inner product estimation (Theorems F.2 and F.4) to implement the mapping
春 Pp,q ∣pi ∣qi → Kk Pp,q ∣pi ∣qi∣Ppq 啊i _________________
Where Ppq is e-close to Ppq = 1+吃凡i, K = √H'+1W'+1D'+1 is a normalisation factor
and |gpqi is some garbage quantum state that can be ignored.
1.2:	Non linearity
Use an arithmetic circuit and two QRAM queries to obtain Y'+1, an e-apprOXimatiOn of the
convolution output Yp',+q 1 = (A'p , Fq' ) and apply the non linear function f as a boolean circuit to
obtain k1 Pp,q |p)|q)|f(Yp+q1)i |gpq).
2:	Step 2: Quantum Sampling
TTC r. f	…	「 F .	.	. 1	1	/	f (Y'+1)
Use Conditional Rotation and Amplitude Amplification to encode the values apq := C—
1	"V'+"
into the amplitudes to obtain K Ep q apq |pi |qi |f (Ypq )i∣gpq). Perform '∞ tomography from
` ,γy'+1
Theorem G.1 with precision η, and obtain classically all positions and values (p, q, f(Ypq ))
such that, with high probability, values above η are known exactly, while others are set to 0.
3:	Step 3: QRAM Update and Pooling
Update the QRAM for the next layer A'+1 while sampling. The implementation of pooling
(Max, Average, etc.) can be done by a specific update to the QRAM data structure described in
Section D.2.2.
In this algorithm, we propose the first quantum algorithm for performing the convolution product.
Our algorithm is based on the observation that the convolution product can be regarded as a ma-
trix product between reshaped matrices. The reshaped input’s rows A'p and the reshaped kernel’s
columns Fq' are loaded as quantum states, in superposition. Then the entries of the convolution
hA'p |Fq'i are estimated using a simple quantum circuit for inner product estimation and stored in an
auxiliary register as in Step 1.1 of Algorithm 1.
One of the difficulties in the design of quantum neural networks is that non linear functions are hard
to implement as unitary operations. We get around this difficulty by applying the non-linear function
f as a boolean circuit to the output of the quantum inner product estimation circuit in Step 1.2 of
Algorithm 1. Most of the non linear functions in the machine learning literature can be implemented
using small sized boolean circuits, our algorithm thus allows for many possible choices of the non-
linear function f (see Appendix F.1 for details on non linear boolean circuits in quantum circuits).
Step 2 of Algorithm 1 develops a quantum importance sampling procedure wherein the pixels with
'+1
high values of f(Ypq ) are read out with higher probability. This is done by encoding these values
into the amplitudes of the quantum state using the well known Amplitude Amplification algorithm
Brassard et al. (2002). This kind of importance sampling is a task that can be performed easily
in the quantum setting and has no direct classical analog. Although it does not lead to asymptotic
improvements for the algorithms running time, it could lead to improvements that are significant in
practice.
More precisely, during the measurement of a quantum register in superposition, only one of its
values appears, with a probability corresponding the the square of its amplitude. It implies that the
4
Published as a conference paper at ICLR 2020
output,s pixels measured with more probability are the ones with the highest value f (Yp'+ 1). Once
measured, we read directly from the registers the position p, q and the value itself. Thus we claim
that we measure only a fraction of the quantum convolution product output, and that the set of pixels
measured collect most of the meaningful information for the CNN, the other pixels being set to 0.
After being measured, each pixel’s value and position are stored in a QRAM to be used as quantum
state for next layer’s input. During this phase, it is possible to discard or aggregate some values to
perform pooling operations as described in Step 3 of Algorithm 1. The forward pass for the QCNN
thus includes the the convolution product, the non linearity f and pooling operation, in time poly-
logarithmic in the kernel’s dimensions. In comparison, the classical CNN layer in linear in both
kernel and input dimensions.
Note finally that quantum importance sampling in Step 2 implies that the non linear function f be
bounded by a parameter C > 1. In our experiments we use the capReLu function, which is a
modified ReLu function that becomes constant above C .
5	Quantum backpropagation algorithm
The second algorithm required for the QCNN is the quantum backpropagation algorithm given as
Algorithm 2. Like the classical backpropagation algorithm, it updates all kernels weights according
to the derivatives of a given loss function L. In this sectiion, we explain the main ideas and compare
it to the classical backpropagation algorithm, the complete details are given in Appendix (Section
E).
Algorithm 2 Quantum Backpropagation
Require: Precision parameter δ. Data matrices A' and kernel matrices F' stored in QRAM for
each layer `.
Ensure: Outputs gradient matrices ∂L and ∂∂L for each layer '.
1:	Calculate the gradient for the last layer L using the outputs and the true labels: ∂∂LL
2:	for ' = L 一 1,…,0 do
3:	Step 1 : Modify the gradient
With ∂YL+ι stored in QRAM, set to 0 some of its values to take into account pooling, tomog-
raphy and non linearity that occurred in the forward pass of layer `. These values correspond
to positions that haven’t been sampled nor pooled, since they have no impact on the final loss.
4:	Step 2 : Matrix-matrix multiplications
With the modified values of ∂γL, use quantum linear algebra algorithm (Theorem F.7) to
perform the matrix-matrix multiplications (A')T ∙ ∂YL⅛ and ∂YL+τ ∙ (F')T, allowing to
obtain quantum states corresponding to ∂∂L and ∂∂Ll.
5:	Step 3 : '∞ tomography
Measure the previous outputs, as in Algorithm 3. This allows to estimate each entry of ∂L
and ddYL' with errors δ ∣∣ ∂∂L ∣∣ and δ ∣∣ ∂L ∣∣ respectively, using '∞ tomography from Theorem
G.1. Store all elements of ∂L in QRAM.
6:	Step 4 : Gradient descent
From the previous tomography, perform the gradient descent to update the values of F' in
QRAM： / — FS,q - λ (∂FLq ± 2δ Il 第 II2)
7:	end for
We describe briefly detail the implementation of quantum backpropagation at layer `. The algorithm
assumes that ^篙1is known. First, the backpropagation of the quantum convolution product is
equivalent to the classical one, and we use the matrix-matrix multiplication formulation to obtain
the derivatives ∂L and ∂∂L. The first one is the result wanted and the second one is needed for layer
` 一 1. This matrix-matrix multiplication can be implemented as a quantum circuit, by decomposing
into several matrix-vector multiplications, known to be efficient, with a running time depending
on the ranks and Frobenius norm of the matrices. We obtain a quantum state corresponding to a
5
Published as a conference paper at ICLR 2020
superposition of all derivatives. We use again the '∞ tomography to retrieve each derivative with
precision δ > 0 such that, for all kernel,s weight F% We have approximated it,s loss derivative
with ∂∂L-, with an error bounded by I ∂FL--∂fl~ I ≤ 2δ ∣∣ ∂∂L J]?. This implies that the gradient
s,q	s,q	s,q
descent rule is perturbed by 2δ ∣∣ ∂L ∣∣2 at most, see Appendix (Section E.4).
We also take into account the effects of quantum non linearity, quantum measurement and pooling.
The quantum pooling operation is equivalent to the classical one, where pixels that were not selected
during pooling see their derivative set to 0. Quantum measurement is similar, since pixels that
haven,t been measured don,t contribute to the gradient. For the non linearity, as in the classical
case, pixels with negative values were set to zero, hence should have no contribution to the gradient.
Additionally, because we used the capReLu function, pixels bigger than the threshold C must also
have null derivatives. This two rules can be implemented by combining them with measurement
rules compared to classical backpropagation, see Appendix (Section E.2.2) for details.
6	Numerical Simulations
As described above, the adaptation of the CNNs to the quantum setting implies some modifications
that could alter the efficiency of the learning or classifying phases. We now present some experi-
ments to show that such modified CNNs can converge correctly, as the original ones.
The experiment, using the PyTorch library developed by Paszke et al. (2017), consists of training
classically a small convolutional neural network for which we have added a “quantum” sampling
after each convolution. Instead of parametrising it with the precision η, we have choosed to use the
sampling ratio σ that represents the fraction of pixels drawn during tomography. This two definitions
are equivalent, as shown in Appendix (Section D.1.5), but the second one is more intuitive regarding
the running time and the simulations.
We also add a noise simulating the amplitude estimation (parameter ), followed by a capReLu
instead of the usual ReLu (parameter C), and a noise during the backpropagation (parameter δ). In
the following results, we observe that our quantum CNN is able to learn and classify visual data
from the widely used MNIST dataset. This dataset is made of 60.000 training images and 10.000
testing images of handwritten digits. Each image is a 28x28 grayscale pixels between 0 and 255 (8
bits encoding), before normalization.
Let,s first observe the “quantum” effects on an image of the dataset. In particular, the effect of the
capped non linearity, the introduction of noise and the quantum sampling.
We now present the full simulation of our quantum CNN. In the following, we use a simple network
made of 2 convolution layers, and compare our quantum CNN to the classical one. The first and
second layers are respectively made of 5 and 10 kernels, both of size 7x7. A three-layer fully
connected network is applied at the end and a softmax activation function is applied on the last
layer to detect the predicted outcome over 10 classes (the ten possible digits). Note that we didn,t
introduce pooling, being equivalent between quantum and classical algorithms and not improving
the results on our CNN. The objective of the learning phase is to minimize the loss function, defined
by the negative log likelihood of the classification on the training set. The optimizer used was a
built-in Stochastic Gradient Descent.
Using PyTorch, we have been able to implement the following quantum effects (the first three points
are shown in Figure 1):
-	The addition of a noise, to simulate the approximation of amplitude estimation during the forward
quantum convolution layer, by adding gaussian noise centered on 0 and with standard deviation
2M, with M = maxp,q kApk kFqk.
-	A modification of the non linearity: a ReLu function which is constant above the value T (the cap).
-	A sampling procedure to apply on a tensor with a probability distribution proportional to the tensor
itself, reproducing the quantum sampling with ratio σ .
-	The addition of a noise during the gradient descent, to simulate the quantum backpropagation,
by adding a gaussian noise centered on 0 with standard deviation δ, multiplied by the norm of the
gradient, as given by Equation (28).
6
Published as a conference paper at ICLR 2020
Figure 1: Effects of the QCNN on a 28x28 input image. From left to right: original image, image
after applying a capReLu activation function with a cap C at 2.0, introduction of a strong noise
during amplitude estimation with = 0.5, quantum sampling with ratio σ = 0.4 that samples the
highest values in priority. The useful information tends to be conserved in this example. The side
gray scale indicates the value of each pixel. Note that during the QCNN layer, a convolution is
supposed to happen before the last image but we chose not to perform it for visualisation matter.
The CNN used for this simulation may seem “small” compared to the standards AlexNet developed
by Krizhevsky et al. (2012) or VGG-16 by Simonyan & Zisserman (2014), or those used in industry.
However simulating this small QCNN on a classical computer was already very computationally
intensive and time consuming, due to the“quantum” sampling task, apparently not optimized for
a classical implementation in PyTorch. Every single training curve showed in Figure 9 could last
for 4 to 8 hours. Hence adding more convolutional layers wasn’t convenient. Similarly, we didn’t
compute the loss on the whole testing set (10.000 images) during the training to plot the testing
curve. However we have computed the test losses and accuracies once the model trained (see Table
4), in order to detect potential overfitting cases.
We now present the result of the training phase for a quantum version of this CNN, where partial
quantum sampling is applied, for different sampling ratio (number of samples taken from the result-
ing convolution). Since the quantum sampling gives more probability to observe high value pixels,
we expect to be able to learn correctly even with small ratio (σ ≤ 0.5). We compare these training
curve to the classical one. The learning has been done on two epochs, meaning that the whole dataset
is used twice. The following plots show the evolution of the loss L during the iterations on batches.
This is the standard indicator of the good convergence of a neural network learning phase. We can
compare the evolution of the loss between a classical CNN and our QCNN for different parameters.
Most results are presented in Appendix (Section H).
Our simulations show that the QCNN is able to learn despite the introduction of noise, tensor sam-
pling and other modifications. In particular it shows that only a fraction of the information is mean-
ingful for the neural network, and that the quantum algorithm captures this information in priority.
This learning can be more or less efficient depending on the choice of the key parameters. For de-
cent values of these parameters, the QCNN is able to converge during the training phase. It can then
classify correctly on both training and testing set, indicating neither overfitting nor underfitting.
We notice that the learning curves sometimes present a late start before the convergence initializes,
in particular for small sampling ratio. This late start can be due to the random initialization of the
kernel weights, that performs a meaningless convolution, a case where the quantum sampling of the
output is of no interest. However it is very interesting to see that despite this late start, the kernel
start converging once they have found a good combination.
Overall, it is possible that the QCNN presents some behaviors that have no classical equivalence.
Understanding their potential effects, positive or negative, is an open question, all the more so as
the effects of the classical CNN’s hyperparameters are already a topic of active research, see the
work of Samek et al. (2017) for details. Note also that the neural network used in this simulation is
7
Published as a conference paper at ICLR 2020
Figure 2: Training curves comparison between the classical CNN and the Quantum CNN (QCNN)
for = 0.01, C = 10, δ = 0.01 and the sampling ratio σ from 0.1 to 0.5. We can observe a learning
phase similar to the classical one, even for a weak sampling of 20% or 30% of each convolution
output, which tends to show that the meaningful information is distributed only at certain location
of the images, coherently with the purpose of the convolution layer. Even for a very low sampling
ratio of 10%, we observe a convergence despite a late start.
rather small. A following experiment would be to simulate a quantum version of a standard deeper
CNN (AlexNet or VGG-16), eventually on more complex dataset, such as CIFAR-10 developed by
Krizhevsky & Hinton (2009) or Fashion MNIST by Xiao et al. (2017).
7	Conclusions
We have presented a quantum algorithm for evaluating and training convolutional neural networks
(CNN). At the core of this algorithm, we have developed a novel quantum algorithm for computing
a convolution product between two tensors, with a substantial speed up. This technique could be
reused in other signal processing tasks that could benefit from an enhancement by a quantum com-
puter. Layer by layer, convolutional neural networks process and extract meaningful information.
Following this idea of learning foremost important features, we have proposed a new approach of
quantum tomography where the most meaningful information is sampled with higher probability,
hence reducing the complexity of our algorithm.
Our QCNN is complete in the sense that almost all classical architectures can be implemented in a
quantum fashion: any (non negative and upper bounded) non linearity, pooling, number of layers
and size of kernels are available. Our circuit is shallow and could be run on relatively small quantum
computers. One could repeat the main loop many times on the same shallow circuit, since perform-
ing the convolution product is simple, and is similar for all layer. The pooling and non linearity are
included in the loop. Our building block approach, layer by layer, allows high modularity, and can
be combined with work on quantum feedforward neural network developed by Allcock et al. (2018).
The running time presents a speedup compared to the classical algorithm, due to fast linear alge-
bra when computing the convolution product, and by only sampling the important values from the
resulting quantum state. This speedup can be highly significant in cases where the number of chan-
nels d' in the input tensor is high (high dimensional time series, videos sequences, games play) or
When the number of kernels D'+1 is big, allowing deep architectures for CNN, which was the case
in the recent breakthrough of DeepMind AlphaGo algorithm of Silver et al. (2016). The Quantum
CNN also allows larger kernels, that could be used for larger input images, since the size the kernels
must be a contant fraction of the input in order to recognize patterns. However, despite our new
techniques to reduce the complexity, applying a non linearity and reusing the result ofa layer for the
next layer make register encoding and state tomography mandatory, hence preventing from having
an exponential speedup on the number of input parameters.
Finally we have presented a backpropagation algorithm that can also be implemented as a quantum
circuit. The numerical simulations on a small CNN show that despite the introduction of noise and
sampling, the QCNN can efficiently learn to classify visual data from the MNIST dataset, perform-
ing a similar accuracy than the classical CNN.
8
Published as a conference paper at ICLR 2020
References
Jonathan Allcock, Chang-Yu Hsieh, Iordanis Kerenidis, and Shengyu Zhang. Quantum algorithms
for feedforward neural networks. arXiv preprint arXiv:1812.03089, 2018.
Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J Osborne, Robert Salzmann, and Ramona
Wolf. Efficient learning for deep quantum neural networks. arXiv preprint arXiv:1902.10445,
2019.
Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Larry Jackel,
Urs Muller, and Karol Zieba. Visualbackprop: efficient visualization of cnns. arXiv preprint
arXiv:1611.05418, 2016.
Gilles Brassard, Peter Hoyer, Michele Mosca, and Alain Tapp. Quantum amplitude amplification
and estimation. Contemporary Mathematics, 305:53-74, 2002.
Shantanav Chakraborty, Andras Gilyen, and Stacey Jeffery. The power of block-encoded ma-
trix powers: improved regression techniques via faster Hamiltonian simulation. arXiv preprint
arXiv:1804.01973, 2018.
Iris Cong, Soonwon Choi, and Mikhail D Lukin. Quantum convolutional neural networks. arXiv
preprint arXiv:1810.03787, 2018.
Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term pro-
cessors. arXiv preprint arXiv:1802.06002, 2018.
Daniel George and EA Huerta. Deep learning for real-time gravitational wave detection and param-
eter estimation: Results with advanced ligo data. Physics Letters B, 778:64-70, 2018.
Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, and Tristan Cook. Quanvolu-
tional neural networks: Powering image recognition with quantum circuits. arXiv preprint
arXiv:1904.04767, 2019.
Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. Proceedings of the
8th Innovations in Theoretical Computer Science Conference, 2017a.
Iordanis Kerenidis and Anupam Prakash. Quantum gradient descent for linear systems and least
squares. arXiv:1704.04992, 2017b.
Iordanis Kerenidis and Anupam Prakash. A quantum interior point method for LPs and SDPs.
arXiv:1808.09266, 2018.
Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, and Anupam Prakash. q-means: A
quantum algorithm for unsupervised machine learning. Neural Information Processing systems
(NeurIPS), 2019.
Nathan Killoran, Thomas R Bromley, Juan MigUel Arrazola, Maria Schuld, Nicolas Quesada, and
Seth Lloyd. Continuous-variable quantum neural networks. arXiv preprint arXiv:1806.06871,
2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, L Bottou, Yoshua Bengio, andP Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 1998.
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum algorithms for supervised and
unsupervised machine learning. arXiv, 1307.0411:1-11, 7 2013. URL http://arxiv.org/
abs/1307.0411.
9
Published as a conference paper at ICLR 2020
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis.
Nature Physics, 10(9):631, 2014.
Michael A Nielsen and Isaac Chuang. Quantum computation and quantum information, 2002a.
Michael A Nielsen and Isaac Chuang. Quantum computation and quantum information, 2002b.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Patrick Rebentrost, Thomas R Bromley, Christian Weedbrook, and Seth Lloyd. Quantum hopfield
neural network. Physical Review A, 98(4):042308, 2018.
Wojciech Samek, Thomas Wiegand, and Klaus-Robert Muller. Explainable artificial intelli-
gence: Understanding, visualizing and interpreting deep learning models. arXiv preprint
arXiv:1708.08296, 2017.
Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. The quest for a quantum neural network.
Quantum Information Processing,13(11):2567-2586, 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Nathan Wiebe, Ashish Kapoor, and Krysta M Svore. Quantum Algorithms for Nearest-Neighbor
Methods for Supervised and Unsupervised Learning. arXiv:1401.2142v2, 2014a. URL https:
//arxiv.org/pdf/1401.2142.pdf.
Nathan Wiebe, Ashish Kapoor, and Krysta M Svore. Quantum deep learning. arXiv preprint
arXiv:1412.3489, 2014b.
J Wu. Introduction to convolutional neural networks. https://pdfs.semanticscholar.
org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
10
Published as a conference paper at ICLR 2020
Appendix A	Variable Summary
We recall the most important variables for layer `. They represent tensors, their approximations, and
their reshaped versions.
Data	Variable	Dimensions	Indices
Input	-X'-	H' X W' X D'	(i', j',d')
	-Y'-	(H'W') X D'	-
	-A'-	(H'+1W'+1) X(HWDD-	(p,r)
Kernel	K'-	-H X W X D' X D'+1	(i,j, d,d'0)
	F'	(HWD') X D'+1	—	Gq)
Table 1: Summary of input variables for the `th layer, along with their meaning, dimensions and
corresponding notations. These variables are common for both quantum and classical algorithms.
We have omitted indices for Y' which don't appear in our work.
Data	Variable	Dimensions	Indices
Output of Quantum Convolution	〃y'+1)	(H'+1 W'+1) X D'+1	(P, q)
	f(X'+1)	H'+1 X W'+1 X D'+1	(i'+1,j'+1,d'+1)
Output of Quantum Tomography	X '+1	H'+1 X W'+1 X D'+1	(i'+1,j'+1,d'+Γ
Output of Quantum Pooling	X'+1	H+ X W+1 X ~D+1~	(i'+1, j'+1, d'+1)
Table 2: Summary of variables describing outputs of the layer `, with the quantum algorithm.
Data	Variable	Dimensions	Indices
Output of Classical Convolution	f(Y'+1)	-(H+TW+ryXD+^	(p, q)
	f(X'+1)	H'+1 X W'+1 X D'+1	(i'+1,j'+1,d'+τΓ
Output of Classical Pooling	Xj '+1	H+ X 中 X ^+r~	(i'+1, j'+1, d'+1)
Table 3: Summary of variables describing outputs of the layer `, with the classical algorithm.
Classical and quantum algorithms can be compared with these two diagrams:
(Quantum convolution layer : X' → |X'+1i → |f (X'+1)i → X'+1 → X'+1	(3)
[Classical convolution layer : X' → X'+1 → f (X'+1) → X'+1
We finally provide some remarks that could clarify some notations ambiguity:
-	Formally, the output of the quantum algorithm is X'+1. It is used as input for the next layer ' + 1.
But We consider that all variables, names are reset when starting a new layer: X'+1 J X'+1.
-	For simplicity, we have sometimes replaced the indices (i'+1,j'+1, d'+1) by n to index the ele-
ments of the output.
-	In Section D.2.2, the input for layer ' + 1 is stored as A'+1, for which the elements are indexed by
(p0, r0).
Appendix B	Preliminaries in Quantum Information
We introduce a basic and broad-audience quantum information background necessary for this work.
For a more detailed introduction we recommend Nielsen & Chuang (2002a).
11
Published as a conference paper at ICLR 2020
B.1	Quantum Information
Quantum Bits and Quantum Registers: The bit is the most basic unit of classical information.
It can be either in state 0 or 1. Similarly a quantum bit or qubit, is a quantum system that can be is
state |0)，11)(the braket notation 卜〉is a reminder that the bit considered is a quantum system) or in
superposition of both states a |0) + β |1) with coefficients a,β ∈ C such that ∣α∣2 + ∣β∣2 = 1. The
amplitudes α and β are linked to the probabilities of observing either 0 or 1 when measuring the
qubit, since P(0) = ∣α∣2 and P⑴=∣β|2.
Before the measurement, any superposition is possible, which gives quantum information special
abilities in terms of computation. With n qubits, the 2n possible binary combinations can exist
simultaneously, each with a specific amplitude. For instance we can consider an uniform distribution
2n-1
√1n Ei=0 |i) where |i) represents the ith binary combination (e.g. |01 …1001)). Multiple qubits
together are often called a quantum register.
In its most general formulation, a quantum state with n qubits can be seen as vector in a complex
Hilbert space of dimension 2n. This vector must be normalized under `2 -norm, to guarantee that the
squared amplitudes sum to 1.
Quantum Computation: To process qubits and therefore quantum registers, we use quantum
gates. These gates are unitary operators in the Hilbert space as they should map unit-norm vectors
to unit-norm vectors. Formally, we can see a quantum gate acting on n qubits as a matrix U ∈ C2n
such that UU * = U *U = I, where U * is the conjugate transpose of U. Some basic single qubit
gates includes the NOT gate (； 0)that inverts |0) and |1), or the Hadamard gate -√2 (1 ʌ
that maps |0) → √∣(|0) + |1)) and |1) → -√^(∣0)-∣1)), creating the quantum superposition.
Finally, multiple qubits gates exist, such as the Controlled-NOT that applies a NOT gate on a target
qubit conditioned on the state of a control qubit.
The main advantage of quantum gates is their ability to be applied to a superposition of inputs.
Indeed, given a gate U such that U |x) 7→ |f (x)), we can apply it to all possible combinations of x
at once U (C Px Ixi) → * PxIf(X)i.
We now state some primitive quantum circuits, which we will use in our algorithm: For two integers
i and j, we can check their equality with the mapping Iii Iji I0i 7→ Iii Iji I[i = j]i. For two real
value numbers a > 0 and δ > 0, we can compare them using Iai Iδi I0i 7→ Iai Iδi I[a ≤ δ]i. Finally,
for a real value numbers a > 0, we can obtain its square Iai I0i 7→ Iai Ia2i. Note that these circuits
are basically a reversible version of the classical ones and are linear in the number of qubits used to
encode the input values.
Any classical boolean function can be implemented in a quantum unitary, even though this seems
at first contradictory with the requirements of unitaries (reversibility, linearity). Let σ : R 7→ R
be a classical function, we define Uσ the unitary that acts as Uσ Ixi I0i 7→ Ixi Iσ(x)i. Using a
second quantum register to encode the result of the function, the properties of quantum unitaries are
respected.
B.2	Quantum Subroutines for Data Encoding
Knowing some basic principles of quantum information, the next step is to understand how data can
be efficiently encoded using quantum states. While several approaches could exist, we present the
most common one called amplitude encoding, which leads to interesting and efficient applications.
Let x ∈ Rd be a vector with components (xι, ∙∙∙ , xd). Using onlydlog(d)] qubits, We can form
∣x), the quantum state encoding x, given by |x)= 看 Pd-I xj |j). We see that the jth component
xj becomes the amplitude of Iji, the jth binary combination (or equivalently the jth vector in the
standard basis). Each amplitude must be divided by IlxktO preserve the unit '2-norm of ∣x).
Similarly, for a matrix A ∈ Rn×d or equivalently for n vectors Ai for i ∈ [n], we can express each
row of A as IAii =号k Pd-O Ajji.
12
Published as a conference paper at ICLR 2020
We can now explain an important definition, the ability to have quantum access to a matrix. This
will be a requirements for many algorithms.
Definition 1 [Quantum Access to Data]
We say that we have quantum access to a matrix A ∈ Rn×d if there exist a procedure to perform
the following mapping, for i ∈ [n], in time T :
•	|ii |0i 7→ |ii |Aii
•	∣0i→ k⅛PikAikIii
By using appropriate data structures the first mapping can be reduced to the ability to perform a
mapping of the form |ii |ji |0i 7→ |ii |ji |Aiji. The second requirement can be replaced by the
ability of performing |ii |0i 7→ |ii |kAi ki or to just have the knowledge of each norm. Therefore,
using matrices such that all rows Ai have the same norm makes it simpler to obtain the quantum
access.
The time or complexity T necessary for the quantum access can be reduced to polylogarithmic
dependence in n and d if we consider the access to a Quantum Memory or QRAM. The QRAM
Kerenidis & Prakash (2017a) is a specific data structure from which a quantum circuit can allow
quantum access to data in time O(log (nd)).
Theorem B.1 (QRAM data structure, see Kerenidis & Prakash (2017a)) Let A ∈ Rn×d, there
is a data structure to store the rows of A such that,
1.	The time to insert, update or delete a single entry Aij is O(log2 (n)).
2.	A quantum algorithm with access to the data structure can perform the following unitaries
in time T = O(log2 n).
(a)	|ii |0i → |ii |Aii for i ∈ [n].
(b)	|0i → Pi∈[n] kAik |ii.
We now state important methods for processing the quantum information. Their goal is to store some
information alternatively in the quantum state’s amplitude or in the quantum register as a bitstring.
TheoremB.2 [Amplitude Amplification and Estimation Brassard et al. (2002)] Given a unitary
operator U such that U : |0)一 √p |y)|0〉+ √1 一 P ∣y⊥i∣1> in time T, where p > 0 is the
probability of measuring “0”, it is possible to obtain the state
to estimate p with relative error δ
using O( δ√p) queries to U.
|y)|0)using O(√) queries to U, or
Theorem B.3 [Conditional Rotation] Given the quantum state |ai, with a ∈ [一1, 1], it is possible
.	r	I ∖ I r>∖	. I ∖ / lr>∖ .	∏	I 1 ∖ ∖	∙.ι	ι ■. K∕r∖
to perform |a)|0)→ |a〉(a |0〉+ √1 — a |1))with complexity O⑴.
Using Theorem F.3 followed by Theorem F.2, it then possible to transform the state % Pd-I |xji
into 击 Pd-I xj |xj i.
In addition to amplitude estimation, we will make use of a tool developed in Wiebe et al. (2014a)
to boost the probability of getting a good estimate for the inner product required for the quantum
convolution algorithm. In high level, we take multiple copies of the estimator from the amplitude
estimation procedure, compute the median, and reverse the circuit to get rid of the garbage. Here we
provide a theorem with respect to time and not query complexity.
Theorem B.4 (Median Evaluation, see Wiebe et al. (2014a)) Let U be a unitary operation that
maps
u ： ∣o0ni → √α ∣x, ii + √r-a ∣G, 0i
for some 1/2 < a ≤ 1 in time T. Then there exists a quantum algorithm that, for any ∆ > 0 and
for any 1/2 < ao ≤ a, produces a state ∣Ψ> such that k ∣Ψi — |0MLiIxik ≤ √2∆ for some integer
13
Published as a conference paper at ICLR 2020
L, in time
2T
ln(1∕∆)
2 (1a。1 - 2 )2
B.3 Quantum subroutines for Linear Algebra
In the recent years, as the field of quantum machine learning grew, its “toolkit” for linear alge-
bra algorithms has become important enough to allow the development of many quantum machine
learning algorithms. We introduce here the important subroutines for this work, without detailing
the circuits or the algorithms.
Definition 2 For a matrix A,	the parameter μ(A)	is defined by μ(A)	=
minp∈[o,i] (kA∣∣F ,，s2p(A)s2(i-p)(AT)) where Sp(A) = maxi(kAikP)∙
The next theorems allow to compute the distance between vectors encoded as quantum states, and
use this idea to perform the k-means algorithm.
Theorem B.5 [Quantum Distance Estimation Wiebe et al∙ (2014b); Kerenidis et al∙ (2019)]
Given quantum access in time T to two matrices U and V with rows ui and vj of dimen-
sion d, there is a quantum algorithm that, for any pair (i, j), performs the following mapping
∣ii∣ji∣0i → |i〉|j)|d2(ui, Vj )i, estimating the euclidean distance between Ui and Vj with precision
|d2(ui, Vj) - d2(ui, Vj)| ≤ for any > 0∙ The algorithm has a running time given by O(Tη∕),
where η = maxij(kuik kVj k), assuming that mini(kuik) = mini(kVik) = 1∙
Theorem B.6 [Quantum k-means clustering Kerenidis et al∙ (2019)]
Given quantum access in time T to a dataset V ∈ Rn×d, there is a quantum algorithm that outputs
with high probability k centroids ci, ∙∙∙ , Ck that are consistent with the output ofthe k-means algo-
rithm with noise δ > 0, in time O(T X (kdηδvV)K(V)(μ(V) + kη(δv)) + k2 η(VV2— K(V)μ(V))) per
iteration∙
Definition 3 For a matrix V ∈ Rn×d, its parameter η(V) is defined as as ：：：,|：4),or as
maxi(kVik2) assuming mini(kVik) = 1∙
In theorem F.6, the other parameters in the running time can be interpreted as follows : δ is the
precision in the estimation of the distances, but also in the estimation of the position of the centroids.
κ(V) is the condition number of V and μ(V) is defined above (Definition 5). Finally, in the case
of well clusterable datasets, which should be the case when we will apply k-means during spectral
clustering, the running simplifies to O(T × (k2dη(VV3 + k25 η(V))).
Note that the dependence in n is hidden in the time T to load the data. This dependence becomes
polylogarithmic in n if we assume access to a QRAM.
Theorem B.7 (Quantum Matrix Operations, Chakraborty et al. (2018) ) Let M ∈ Rd×d and
x ∈ Rd∙ Let δ1, δ2 > 0∙ If M is stored in appropriate QRAM data structures and the time to
prepare |xi is Tx, then there exist quantum algorithms that with probability at least 1 - 1∕poly(d)
return
1∙ A state |z〉such that |||z)— ∣Mxik2 ≤ δι In time O((K(M)μ(M) + TxK(M)) log(1∕δι))∙
Note that this also implies k|zi - |M xik∞ ≤ δ1
2∙ Norm estimate z ∈	(1 ± δ2) kMxk2, with relative error δ2,	in time
O(TxK(Mδμ(M) log(1∕δi))∙
The linear algebra procedures above can also be applied to any rectangular matrix V ∈ Rn×d
by
considering instead the symmetric matrix V
V0 .
14
Published as a conference paper at ICLR 2020
Appendix C	Classical Convolutional Neural Network (CNN)
CNN is a specific type of neural network, designed in particular for image processing or time series.
It uses the Convolution Product as a main procedure for each layer. We will focus on image pro-
cessing with a tensor framework for all elements of the network. Our goal is to explicitly describe
the CNN procedures in a form that can be translated in the context of quantum algorithms.
As a regular neural network, a CNN should learn how to classify any input, in our case images. The
training consists of optimizing a series of parameters, learned on the inputs and their corresponding
labels.
C.1 Tensor representation
Images, or more generally layers of the network, can be seen as tensors. A tensor is a generalization
of a matrix to higher dimensions. For instance an image of height H and width W can be seen as
a matrix in RH ×W , where every pixel is a greyscale value between 0 ans 255 (8 bit). However the
three channels of color (RGB: Red Green Blue) must be taken into account, by stacking three times
the matrix for each color. The whole image is then seen as a 3 dimensional tensor in RH×W×D
where D is the number of channels. We will see that the Convolution Product in the CNN can be
expressed between 3-tensors (input) and 4-tensors (convolution filters or kernels), the output being
a 3-tensor of different dimensions (spatial size and number of channels).
Figure 3: RGB decomposition, a colored image is a 3-tensor.
C.2 Architecture
A CNN is composed of4 main procedures, compiled and repeated in any order : Convolution layers,
most often followed by an Activation Function, Pooling Layers and some Fully Connected layers at
the end. We will note ` the current layer.
Convolution Layer : The `th layer is convolved by a set of filters called kernels. The output of this
operation is the (` + 1)th layer. A convolution by a single kernel can be seen as a feature detector,
that will screen over all regions of the input. If the feature represented by the kernel, for instance
a vertical edge, is present in some part of the input, there will be a high value at the corresponding
position of the output. The output is commonly called the feature map of this convolution.
Activation Function : As in regular neural network, we insert some non linearities also called
activation functions. These are mandatory for a neural network to be able to learn any function. In
the case of a CNN, each convolution is often followed by a Rectified Linear Unit function, or ReLu.
This is a simple function that puts all negative values of the output to zero, and lets the positive
values as they are.
Pooling Layer : This downsampling technique reduces the dimensionality of the layer, in order
to improve the computation. Moreover, it gives to the CNN the ability to learn a representation
invariant to small translations. Most of the time, we apply a Maximum Pooling or an Average
Pooling. The first one consists of replacing a subregion of P × P elements only by the one with the
15
Published as a conference paper at ICLR 2020
maximum value. The second does the same by averaging all values. Recall that the value of a pixel
corresponds to how much a particular feature was present in the previous convolution layer.
Fully Connected Layer : After a certain number of convolution layers, the input has been suf-
ficiently processed so that we can apply a fully connected network. Weights connect each input
to each output, where inputs are all element of the previous layer. The last layer should have one
node per possible label. Each node value can be interpreted as the probability of the initial image to
belong to the corresponding class.
C.3 Convolution Product as a Tensor Operation
Most of the following mathematical formulations have been very well detailed by Wu (2017).
At layer ', We consider the convolution of a multiple channels image, seen as a 3-tensor X' ∈
Rh'×w'×d'. Let's consider a single kernel in Rh×w×d'. Note that its third dimension must
match the number of channels of the input, as in Figure 4. The kernel passes over all possible re-
gions of the input and outputs a value for each region, stored in the corresponding element of the
output. Therefore the output is 2 dimensional, in RH'+1×W'+1
Figure 4: Convolution ofa 3-tensor input (Left) by one 3-tensor kernel (Center). The ouput (Right)
is a matrix for Which each entry is a inner product betWeen the kernel and the corresponding over-
lapping region of the input.
In a CNN, the most general case is to apply several convolution products to the input, each one With
a different 3-tensor kernel. Let's consider an input convolved by D'+1 kernels. We can globally
see this process as a whole, represented by one 4-tensor kernel K' ∈ Rh×w×D'×D'+1. As D'+1
convolutions are applied, there are D'+1 outputs of 2 dimensions, equivalent to a 3-tensor X'+1 ∈
rH'+1×W '+1×d'+1
We can see on Figure 5 that the output's dimensions are modified given the following rule:
(H'+1 = H' - H +1
[W'+1 = W' - W + 1
(4)
We omit to detail the use of Padding and Stride, two parameters that control how the kernel moves
through the input, but these can easily be incorporated in the algorithms.
An element of X' is determined by 3 indices (i',j', d'), while an element of the kernel K' is
determined by 4 indices (i,j, d, d0). For an element of X'+1 we use 3 indices (i'+1,j'+1, d'+1).
We can express the value of each element of the output X'+1 with the relation
H W D'
Xi'+1,j'+1,d'+1 = XXX Ki,j,d,d'+1 Xi'+ι+i,j'+1+j,d
i=0 j=0 d=0
(5)
16
Published as a conference paper at ICLR 2020
Figure 5: Convolutions of the 3-tensor input X' (Left) by one 4-tensor kernel K' (Center). Each
channel of the output X'+1 (Right) corresponds to the output matrix of the convolution with one of
the 3-tensor kernel.
C.4 Matrix Expression
Figure 6: A convolution product is equivalent to a matrix-matrix multiplication.
Itis possible to reformulate Equation (5) as a matrix product. For this we have to reshape our objects.
We expand the input X' into a matrix A' ∈ R(H'+1 W'+1 )×(HWD'). EaCh row of A' is a vectorized
version of a subregion of X'. This subregion is a volume of the same size as a single kernel volume
H×W X D'. Hence each of the H'+1 X W'+1 rows of A' is used for creating one value in X'+1.
Given such a subregion of X', the rule for creating the row of A' is to stack, channel by channel,
a column first vectorized form of each matrix. Then, we reshape the kernel tensor K' into a matrix
F' ∈ R(HWD')×D'+1, such that each column of F' is a column first vectorized version of one of
the D'+1 kernels.
The convolution operation X' * K' = X'+1 is equivalent to the following matrix multiplication
A'F' = Y'+1,	(6)
17
Published as a conference paper at ICLR 2020
where each column of Y'+1 ∈ R(H'+1W'+1)×D'+1 is a column first vectorized form of one of the
D'+1 channels of X'+1. Note that an element Yp',+1 is the inner product between the Pth row of A'
and the qth column of F'. It is then simple to convert Y'+1 into X'+1 The indices relation between
the elements Y'+1 and X'+1,j'+ι,d'+ι is given by:
d'+1 = q
j'+1 = b H+ι C	⑺
i'+1 = P — H '+1b h⅛γ C
A summary of all variables along with their meaning and dimensions is given in Section A.
Appendix D	Quantum Convolutional neural network
In this section we will design quantum procedures for the usual operations in a CNN layer. We start
by describing the main ideas before providing the details. Steps are gathered in Algorithm 1.
First, to perform a convolution product between an input and a kernel, we will use the mapping be-
tween convolution of tensors and matrix multiplication from Section C.3, that can further be reduced
to inner product estimation between vectors, in order to use quantum linear algebra procedures to
perform these computations faster. The output will be a quantum state representing the result of the
convolution product, from which we can sample to retrieve classical information to feed the next
layer. This is stated by the following Theorem:
Theorem D.1 (Quantum Convolution Layer)
Given 3D tensor input X' ∈ Rh'×w'×d' and 4D tensor kernel K' ∈ Rh×w ×d'×d'+1 Stored in
QRAM, there is a quantum algorithm that computes a quantum states ∆-close to |f (X'+1)i with
arbitrary small parameter ∆ > 0. |f (X'+1)i is close to the result of the convolution product
X'+1 = X' * K' followed by any non linear function f : R → R+, with an error bounded by
∣∣f (X '+1) — f (X '+1)∣∣ ≤ 2Me for any precision e > 0, where M is the maximum norm of a
product between one of the D'+1 kernels, and one of the regions of X' of size HWD'. The time
complexity of this procedure is given by O (1/), where O hides factors poly-logarithmic in ∆ and
in the size of X' and K'.
In a second step, we efficiently retrieve classical information from the output. Recall that a convo-
lution can be seen as a pattern detection on the input image, where the pattern is the kernel. The
output values correspond to “how much” the pattern was present in the corresponding region of the
input. Low value pixels in the output indicate the absence of the pattern in the input at the corre-
sponding regions. Therefore, by sampling according to these output values, where the high value
pixels are sampled with more probability, we could retrieve less but only meaningful information
for the neural network to learn. While sampling, we update the QRAM data structure with the new
information (see Section D.2). We also perform the Pooling operation during this phase (see Section
D.2.2). It is an interesting use case where amplitudes of a quantum state are proportional to the
importance of the information they carry, giving a new utility to the probabilistic nature of quantum
sampling. Numerical simulations are presented in Section 6 to have an empirical estimation of how
many samples from the output state are necessary.
D. 1 Single Quantum Convolution Layer
In order to develop a quantum algorithm to perform the convolution as described above, we will
make use of quantum linear algebra procedures. We will use quantum states proportional to the rows
ofA', noted |Ap i, and the columns ofF', noted |Fqi (we omit the ` exponent in the quantum states
to simplify the notation). These states are given by |Api = k^^ PHWD -1 Apr |ri and |Fq)=
D'+ι -1
kFk Es=0	Fsq |s). We suppose We can load these vectors in quantum states by performing the
following queries:
|Pi |0i 7→ |Pi|Api
|qi |0i 7→ |qi|Fqi
(8)
18
Published as a conference paper at ICLR 2020
Such queries, in time poly-logarithmic in the dimension of the vector, can be implemented with
a Quantum Random Access Memory (QRAM). See Section D.2 for more details on the QRAM
update rules and its integration layer by layer.
D.1.1 Inner Product Estimation
The following method to estimates inner product is derived from previous work by Kerenidis et al.
(2019). With the initial state |pi |qi √12(|0i + |1i) |0i we apply the queries detailed above in a con-
trolled fashion, followed simply by a Hadamard gate to extract the inner product hAp|Fqi in an
amplitude∙ √2 (|pi |qi |0i |0i + |pi |qi |1i l0i) → √2 (IpiIqil0ilApi + |pi |qi |1i |Fqi). By apply-
ing a Hadamard gate on the third register we obtain the following state, 2 |pi ∣qi ( ∣0i (∣Api + ∣Fq i) +
|1i (|Api -|Fqi) . The probability of measuring 0 on the third register is given by Ppq
1+ hAp | Fq i
Thus we can rewrite the previous state as
|ypqi and |yp0 qi are some garbage states.
|pi |qi (PPpq l0,ypqi + p1 - PPq l1,y'pqi),
2 .
where
We can perform the previous circuit in superposi-
tion. Since A' has H'+1W'+1 rows, and F' has D'+1 columns, We obtain the state: |ui
√H '+1W '+1 D'+1
Pp Pq Ipi Iqi (PPPq ∣0, ypqi + p1"-ppq ∣1, y'pqi ) Therefore the probability of
measuring the triplet (p, q, 0) in the first three registers is given by P0 (p, q)
Ppq
1 + h Ap | Fq i
H'+1 W '+1 D'+1
2H'+1W '+1 D'+1
Now we can relate to the Convolution product. Indeed, the triplets (p, q, 0) that are
the most probable to be measured are the ones for which the value hAp|Fqi is the highest. Recall that
each element of Y'+1 is given by Y'+1 = (Ap, Fq), where "(.,.)” denotes the inner product. We
see here that we will sample most probably the positions (p, q) for the highest values of Y'+1, that
corresponds to the most important points of X'+1, by the Equation (7). Note that the the values of
Y '+1 can be either positive of negative, which is not an issue thanks to the positiveness of P0(p, q).
1
A first approach could be to measure indices (p, q) and rely on the fact that pixels with high values,
hence a high amplitude, would have a higher probability to be measured. However we have not
exactly the final result, since hApIFqi 6= (Ap, Fq) = kApk kFqk hApIFqi. Most importantly we then
want to apply a non linearity f (Ypq+1) to each pixel, for instance the ReLu function, which seems
not possible with unitary quantum gates if the data is encoded in the amplitudes only. Morever, due
to normalization of the quantum amplitudes and the high dimension of the Hilbert space of the input,
the probability of measuring each pixel is roughly the same, making the sampling inefficient. Given
these facts, we have added steps to the circuit, in order to measure (p, q, f (Ypq+1)), therefore know
the value of a pixel when measuring it, while still measuring the most important points in priority.
D.1.2 Encoding the amplitude ina register
Let U be the Unitary that map |0i to |ui: 1Ui = √H'+1W'+1D'百 Pp,q |pi |qi (PPPq |0, ypqi +
pi - PPq ∣1, ypqi ). The amplitude PPPq can be encoded in an ancillary register by using Ampli-
tude Estimation (Theorem F.2) followed by a Median Evaluation (Theorem F.4). For any ∆ > 0
and v〉°, We Can tmvc a StiItC △ close to √√〉 √H'+1 w'+1 D'+1'Pp,q IpiIqiIOilPpqiIgpqi With
probability at least 1 - 2∆, where ∣Ppq - PPq ∣ ≤ E and |gpqi is a garbage state. This requires
O(In(I^)) queries of U. In the following we discard the third register ∣0i for simplicity.
The benefit of having PPq in a register is to be able to perform operations on it (arithmetic
or even non linear). Therefore we can simply obtain a state corresponding to the exact value
of the the convolution product. Since we,ve built a circuit such that PPq = 1+hAplFqi, with
two QRAM calls, we can retrieveJhe norm of the vectors by applying the following unitary
IpiIqiIPpqi Igpqi |0i ∣0i → IpiIqiIPpqi∣gpqi IkApki IkFqki. On the fourth register, we can then
write Ypq+1 = k Apkk Fq k hApIFq i using some arithmetic circuits (addition, multiplication by a
scalar, multiplication between registers). We then apply a boolean circuit that implements the ReLu
function on the same register, in order to obtain an estimate of f (Ypq+1) in the fourth register. We
finish by inverting the previous computations and obtain the final state:
19
Published as a conference paper at ICLR 2020
If (Y'+1)i = √H'+ιW'+ιD'+ι X ∣pi∣qif (Yp+ 1)i∣gpq i	⑼
Because of the precision E on ∣PPq)，our estimation Y'+ 1 = (2PPq - 1) ∣∣Apk kFq∣∣, is obtained
with error such that ∣Yp+ 1 - Y'+1∣ ≤ 2e ∣Ap∣ ∣Fq∣∣.
In superposition, we can bound this error by ∣Υp+ 1 - Ypq+11 ≤ 2Me where we define
M = max ∣AP ∣ ∣Fq∣	(10)
P,q
M is the maximum product between norms of one of the D'+1 kernels, and one of the regions of X'
of size HWD'. Finally, since the previous error estimation is valid for all pairs (p, q), the overall
error committed on the convolution product can be bounded by ∣∣Y'+1 - Y'+1∣∣ ≤ 2Me, where
∣∣.∣∞ denotes the '∞ norm. Recall that Y'+1 is just a reshaped version of X'+1. Since the non
linearity adds no approximation, we can conclude on the final error committed for a layer of our
QCNN
If(X'+1) - f (X'+1) L ≤ 2Me	(11)
At this point, we have established Theorem D.1 as we have created the quantum state (9), with given
precision guarantees, in time poly-logarithmic in ∆ and in the size of X' and K'.
We know aim to retrieve classical information from this quantum state. Note that ∣Ypq+1i is rep-
resenting a scalar encoded in as many qubits as needed for the precision, whereas |AP i was repre-
senting a vector as a quantum state in superposition, where each element AP,r is encoded in one
amplitude (See Section F). The next step can be seen as a way to retrieve both encoding at the same
time, that will allow an efficient tomography focus on the values of high magnitude.
D.1.3 Conditional rotation
In the following sections, we omit the ` + 1 exponent for simplicity. Garbage states are removed
as they will not perturb the final measurement. We now aim to modify the amplitudes, such that
the highest values of ∣f(Y)〉are measured with higher probability. A way to do so consists in
applying a conditional rotation on an ancillary qubit, proportionally to f (YPq). We will detail
the calculation since in the general case f (YPq) can be greater than 1. To simplify the notation,
we note X = f (YPq). This step consists in applying the following rotation on a ancillary qubit:
lxil0i → |xi (pmxX |0i + β l1i), where max X = maxP,qf (YPq) and β = p1 - (mXXx F.
Note that in practice it is not possible to have access to |max xi from the state (9), but we will
present a method to know a priori this value or an upper bound in section D.1.6. Let’s note

αPq
____f (YPq )_
maxp,q(f(γ Pq ))
The ouput of this conditional rotation in superposition on state (9) is
then √hWd P^ |p)|q)|f (YPq )i 5q |0i+ J - OPq |1i).
D.1.4 Amplitude Amplification
In order to measure (p, q, f (YPq)) with higher probability where f (YPq) has high value, we
could post select on the measurement of |0i on the last register. Otherwise, we can per-
form an amplitude amplification on this ancillary qubit. Let’s rewrite the previous state as
√HWD Pr,q αPq |pi |qi |f (Y Pq )i|0i + ʌ/1 - αpq|gP0 q i |1i, where |gP0 qi is another garbage state.
The overall probability of measuring |0)on the last register is P(0) = HWD PPq ∣α^q∣2. The
number of queries required to amplify the state |0)is O(√=y), as shown by Brassard et al.
(2002).
Since f(YPq) ∈ R+, we have *q
____f (YPq )
maxp,q(f (Y Pq ))
Therefore the number of
20
Published as a conference paper at ICLR 2020
queries is O	(f(YPq)) / 1	,	f (Y)) = O ('√x'f Pq)))
∖	H HWD P^p,q J(Y Pq) I	∖ VEp,qf(Y Pq))
where the nota-
tion Ep,q(f (YPq)) represents the average value of the matrix f (Y). It can also be written E(f (X))
as in Result 1: Ep,q(f (YPq)) = HWD PP q f (YPq). At the end of these iterations, We have modi-
fied with high probability the state to the following:
If (Y)i = √⅛ X αPq IPiIqiIf (YPq)i
HWD
P,q
(12)
Where, to respect the normalization of the quantum state, α0Pq =
probability of measuring (p, q,f (YPq)) is given by p(p, q, f (YPq))
αpq
PP,q
apq
HWD
. Eventually, the
S2
HWD
. Note
that we have used the same type of name ∣f (Y)〉for both state (9) and state (12). For now on, this
state name will refer only to the latter (12).
D.1.5 '∞ TOMOGRAPHY AND PROBABILISTIC SAMPLING
We can rewrite the final quantum state obtained in (12) as
1
qPP,qf(YP1)
X √f (YP+1) ∣pi∣qiIf(YP+ 1)i
P,q
(13)
We see here that f (Y ), the values of each pixel, are encoded in both the last register and in
the amplitude. We will use this property to extract efficiently the exact values of high magnitude
“/b'+1、
pixels. For simplicity, we will use instead the notation f(Xn ) to denote a pixel’s value, with
n ∈ [H'+1W'+1D'+1 ]. Recall that Y'+1 and X'+1 are reshaped version of the same object.
The pixels with high values will have more probability of being sampled. Specifically, we perform
a tomography with '∞ guarantee and precision parameter η > 0. See Theorem G.1 and Section G
for details. The '∞ guarantee allows to obtain each pixel with error at most η, and require O(1∕η2)
`	p7-'+1
samples from the state (13). Pixels with low values f(Xn ) < η will probably not be sampled due
to their low amplitude. Therefore the error committed will be significative and we adopt the rule of
“/*+1、
setting them to 0. Pixels with higher values f(Xn ) ≥ η, will be sample with high probability,
,	,	,	,	,	'+1	.............
and only one appearance is enough to get the exact register value f(Xn ) of the pixel, as is it also
written in the last register.
To conclude, let’s note Xn'+1 the resulting pixel values after the tomography, and compare it to the
real classical outputs f (Xnt+1) Recall that the measured values f (Xn+1) are approximated with
error at most 2M with M = maxP,q kAP k kFq k. The algorithm described above implements the
following rules:
。麓+1- f(Xn+1)∣≤ 2Me if f(Xn+1) ≥ η
l%n+1 = 0	if f (Xn+1) <η
(14)
Concerning the running time, one could ask what values of η are sufficient to obtain enough mean-
ingful pixels. Obviously this highly depends on the output’s size H'+1W'+1D'+1 and on the out-
put’s content itself. But we can view this question from an other perspective, by considering that we
sample a constant fraction of pixels given by σ ∙ (H'+1W'+1D'+1) where σ ∈ [0,1] is a sampling
ratio. Because of the particular amplitudes of state (13), the high value pixels will be measured and
known with higher probability. The points that are not sampled are being set to 0. We see that this
approach is equivalent to the '∞ tomography, therefore we have η⅛ = σ ∙ H'+1 W'+1D'+1.
21
Published as a conference paper at ICLR 2020
We will use this analogy in the numerical simulations (Section 6) to estimate, for a particular QCNN
architecture and a particular dataset of images, which values of σ are enough to allow the neural
network to learn.
D.1.6 Regularization of the Non Linearity
'+1、、
In the previous steps, we see several appearances of the parameter maxp,q(f(Ypq )). First for the
conditional rotation preprocessing, we need to know this value or an upper bound. Then for the
running time, we would like to bound this parameter. Both problems can be solved by replacing the
usual ReLu non linearity by a particular activation function, that we note capReLu. This function
is simply a parametrized ReLu function with an upper threshold, the cap C, after which the function
remain constant. The choice of C will be tuned for each particular QCNN, as a tradeoff between
accuracy and speed. Otherwise, the only other requirement of the QCNN activation function would
be not to allow negative values. This is already often the case for most of the classical CNN. In
practice, we expect the capReLu to be as good as a usual ReLu, for convenient values of the cap C
(≤ 10). We performed numerical simulations to compare the learning curve of the same CNN with
several values of C . See the numerical experiments presented in Section 6 for more details.
Figure 7: Activation functions: ReLu (Left) and capReLu (Right) with a cap C at 5.
D.2 QRAM update
We wish to detail the use of the QRAM between each quantum convolution layer, and present how
the pooling operation can happen during this phase. General results about the QRAM is given as
Theorem F.1. Implementation details can be found in the work of Kerenidis & Prakash (2017a). In
this section, we will show how to store samples from the output of the layer `, to create the input of
layer ` + 1.
D.2. 1 Storing the output values during the sampling
At the beginning of layer ' +1, the QRAM must store A'+1, a matrix where each elements is indexed
by (p0, r0), and perform ∣p0) |0) → ∣p0) | Ap+1). The data is stored in the QRAM as a tree structure
described by Kerenidis & Prakash (2017b). Each row Ap+1 is stored in such a tree T'+1. Each leaf
Ap+r1 correspond to a value sampled from the previous quantum state |f (Y'+1)), output of the layer
'.The question is to know where to store a sample from |f (Y'+1)) in the tree T'+1.
When a point is sampled from the final state of the quantum convolution, at layer `, as described in
Section D.1.4, we obtain a triplet corresponding to the two positions and the value of a point in the
matrix f (Y'+1). We can know where this point belong in the input of layer ' + 1, the tensor X'+1,
by Equation (7), since Y' is a reshaped version of X'.
The position in X'+1, noted (i'+1, j'+1, d'+1), is then matched to several positions (p0, r0) in A'+1.
For each p0, we write in the tree T'+1 the sampled value at leaf r0 and update its parent nodes, as
required in the work of Kerenidis & Prakash (2017b). Note that leaves that weren’t updated will be
considered as zeros, corresponding to pixels with too low values, or not selected during pooling (see
next section).
22
Published as a conference paper at ICLR 2020
Having stored pixels in this way, We can then query |p0)|0)→ ∣p0i∣Apo), using the quan-
tum circuit developed by KerenidiS & Prakash (2017b), where we correctly have ∣Ap+ 1) =
U 1 U P o A'+r 1 ∣r0). Note that each tree has a logarithmic depth in the number ofleaves, hence the
"P+1L P
running time of writing the output of the quantum convolution layer in the QRAM gives a marginal
7'+l、、
multiplicative increase, poly-logarithmic in the number of points sampled from |f(Y	)i, namely
o(iog(i∕η2)).
D.2.2 Quantum Pooling
As for the classical CNN, a QCNN should be able to perform pooling operations. We first detail
the notations for classical pooling. At the end of layer `, we wish to apply a pooling operation of
size P on the output f (X'+1). We note X'+1 the tensor after the pooling operation. For a point in
f (X'+1) at position (i'+1,j'+1,d'+1), we know to which pooling region it belongs, corresponding
to aposition (i'+1, j'+1,(d'+1) in X'+1:
(15)
Figure 8: A 2×2 tensor pooling. A point in f (X '+1) (left) is given by its position (i'+1 ,j'+1, d'+1).
A point in X'+1 (right) is given by its position (i'+1, j'+1,<j'+1). Different pooling regions in
f (X'+1) have separate colours, and each one corresponds to a unique point in X'+1.
We now show how any kind of pooling can be efficiently integrated to our QCNN structure. In-
deed the pooling operation will occur during the QRAM update described above, at the end of a
convolution layer. At this moment we will store sampled values according to the pooling rules.
In the quantum setting, the output of layer ' after tomography is noted X'+1. After pooling, we will
describe it by X'+1, which has dimensions HP- × wp+1 X D'+1. X'+1 will be effectively used as
input for layer ' + 1 and its values should be stored in the QRAM to form the trees rT'+1, related to
the matrix expansion A'+1.
However X'+1 is not known before the tomography is over. Therefore we have to modify the
update rule of the QRAM to implement the pooling in an online fashion, each time a sample from
|f (X'+1)) is drawn. Since several sampled values of |f (X'+1)) can correspond to the same leaf
Ap+1 (points in the same pooling region), we need an overwrite rule, that will depend on the type
of pooling. In the case of Maximum Pooling, we simply update the leaf and the parent nodes if the
new sampled value is higher that the one already written. In the case of Average Polling, we replace
the actual value by the new averaged value.
In the end, any pooling can be included in the already existing QRAM update. In the worst case, the
running time is increased by O(P∕η2), an overhead corresponding to the number of times we need
to overwrite existing leaves, with P being a small constant in most cases.
23
Published as a conference paper at ICLR 2020
As We will see in Section E,the final positions (p, q) that were sampled from |f (X'+1)i and selected
after pooling must be stored for further use during the backpropagation phase.
D.3 Running Time
We will now summarise the running time for one forward pass of convolution layer '. With O we
hide the polylogaryhtmic factors. We first write the running time of the classical CNN layer, which
is given by O (H'+1W'+1D'+1 ∙ HWD'). For the QCNN, the previous steps prove Result 1 and
can be implemented in time O (系∙ √=√'+=). Note that, as explain in Section D.1.5, the
quantum running time can also be written O ∣ σH'+1W'+1D'+1 ∙ —/ M√C4：∣, with σ ∈ [0,1]
∖	e √E(f(X'+1))/
being the fraction of sampled elements among H'+1W'+1D'+1 of them.
Itis interesting to notice that the one quantum convolution layer can also include the ReLu operation
and the Pooling operation in the same circuit, for no significant increase in the running time, whereas
in the classical CNN each operation must be done on the whole data again.
Appendix E Quantum Backprogation
The entire QCNN is made of multiple layers. For the last layer’s output, we expect only one possible
outcome, or afew in the case of a classification task, which means that the dimension of the quantum
output is very small. A full tomography can be performed on the last layer’s output in order to
calculate the outcome. The loss L is then calculated, as a measure of correctness of the predictions
compared to the ground truth. As the classical CNN, our QCNN should be able to perform the
optimization of its weights (elements of the kernels) to minimize the loss by an iterative method.
Theorem E.1 (Quantum Backpropagation for Quantum CNN)
Given the forward pass quantum algorithm in Algorithm 1, the input matrix A' and the kernel matrix
F' stored in the QRAM for each layer `, and a loss function L, there is a quantum backpropagation
algorithm that estimates, for any precision δ > 0, the gradient tensor -∂∂L and update each element
to perform gradient descent such that ∀(s, q), ∣ ∂FL-∂∂L | ≤ 2δ ∣∣ ∂FL ∣∣2∙ Let ∂L be the gradi-
ent with respect to the `th layer. The running time of a single layer ` for quantum backpropagation
is given by
O (((μ(A`) + μ( ∂Y+τ O κ( W) + (μ( ∂Y⅛)+3卜(华))吟)(⑹
wherefor a matrix V, K(V) is the condition number and μ(V) is defined in Equation (5).
E.1 Classical Backpropagation
After each forward pass, the outcome is compared to the true labels and define a loss. We can
update our weights by gradient descent to minimize this loss, and iterate. The main idea behind the
backpropagation is to compute the derivatives of the loss L, layer by layer, starting from the last
one.
At layer ', the derivatives needed to perform the gradient descent are ∂L and ∂Y⅛. The first one
represents the gradient of the final loss L with respect to each kernel element, a matrix of values that
we will use to update the kernel weights Fs',q . The second one is the gradient of L with respect to
the layer itself and is only needed to calculate the gradient ∂FLι at layer ' - 1.
E.1.1 Convolution Product
We first consider a classical convolution layer without non linearity or pooling. Thus the output of
layer ` is the same tensor as the input of layer ` + 1, namely X'+1 or equivalently Y'+1. Assuming
we know ∂X∂L+ι or equivalently ∂YL+ι, both corresponding to the derivatives of the (' + 1)th layer's
24
Published as a conference paper at ICLR 2020
input, We will show how to calculate ∂FL, the matrix of derivatives with respect to the elements of
the previous kernel matrix F'. This is the main goal in order to optimize the kernel,s weights.
The details of the following calculations can be found in the work of Wu (2017). We will use the
notation vec(X) to represents the vectorized form of any tensor X.
Recall that A' is the matrix expansion of the tensor X', whereas Y' is a matrix reshaping of X'.
By applying the chain rule ∂ve∂F')τ
∂L	∂vec(X'+1)
∂vec(X'+1)T ∂vec(F')T
∂ L
∂F'
(A')T ∂YL1
we can obtain:
(17)
See calculations details in the work of Wu (2017). Equation (17) shows that, to obtain the desired
gradient, we canjust perform a matrix-matrix multiplication between the transposed layer itself (A')
and the gradient with respect to the previous layer (辞等).
Equation (17) explains also why we will need to calculate ∂L in order to backpropagate through
layer ' - 1. To calculate it, we use the chain rule again for ∂ve∂X')τ
∂L	∂vec(X '+1)
∂vec(X'+1)t ∂vec(X')T .
Recall that a point in A', indexed by the pair (p, r), can correspond to several triplets (i',j', d') in
X`. We will use the notation (p, r)什(i',j', d') to express formally this relation. One can show
that ∂YL+ι (F')T is a matrix of same shape as A', and that the chain rule leads to a simple relation
to calculate ∂L :
[∂⅛]
i`,j`,d`
Σ
(p,r)Ti',j',d')
(18)
p,r
We have shown how to obtain the gradients with respect to the kernels F' and to the layer itself Y'
(or equivalently X').
E.1.2 Non Linearity
The activation function has also an impact on the gradient. In the case of the ReLu, we should only
cancel gradient for points with negative values. For points with positive value, the derivatives remain
the same since the function is the identity. A formal relation can be given by
∂L
∂X'+1
=([ðf (XL+1)] i'+1,j'+1,d'+1 if X''+1,j'+1,d'+1 ≥ 0
i`+1 ,j'+1 ,d`+1	10 otherwise
(19)
E.1.3 Pooling
If we take into account the pooling operation, we must change some of the gradients. Indeed, a
pixel that hasn’t been selected during pooling has no impact on the final loss, thus should have a
gradient equal to 0. We will focus on the case of Max Pooling (Average Pooling relies on similar
idea). To state a formal relation, we will use the notations of Section D.2.2: an element in the output
of the layer, the tensor f(X'+1), is located by the triplet (i'+1,j'+1, d'+1). The tensor after pooling
is noted X'+1 and its points are located by the triplet (i'+1, j'+1, d'+1). During backpropagation,
after the calculation of ∂XL+τ, some of the derivatives of f (X'+1) should be set to zero with the
following rule:
一 ∂L 一
∂f(X '+1) .
=([dXL⅛] -e+1 m+1 j'+1 if (i'+1,j'+1, d'+1) was selected during pooling
i'+1,j'+1,d'+1	10 otherwise
(20)
25
Published as a conference paper at ICLR 2020
E.2 Quantum Algorithm for Backpropagation
In this section, we want to give a quantum algorithm to perform backrpopagation on a layer `, and
detail the impact on the derivatives, given by the following diagram:
∂L	∂L	∂L	∂L	∂L
∂X'+1 J ∂f(X'+1) J ∂X'+1 J ∂X'+1 = ∂X+1
(21)
We assume that backpropagation has been done on layer ' +1. This means in particular that ∂X⅛τ is
stored in QRAM. However, as shown on Diagram (21), ∂X∂L+ι corresponds formally to ∂X+r, and
not -=L+τ. Therefore, We will have to modify the values stored in QRAM to take into account non
linearity, tomography and pooling. We will first consider how to implement ∂X⅛ and ∂∂L through
backpropagation, considering only convolution product, as if -=L+τ and ∂XL+ι where the same.
Then we will detail how to simply modify ∂X∂L+ι a priori, by setting some of its values to 0.
E.2. 1 Quantum Convolution Product
In this section we consider only the quantum convolution product without non linearity, tomography
nor pooling, hence writing its output directly as X'+1. Regarding derivatives, the quantum convo-
lution product is equivalent to the classical one. Gradient relations (17) and (18) remain the same.
Note that the -approximation from Section D.1.2 doesn’t participate in gradient considerations.
The gradient relations being the same, we still have to specify the quantum algorithm that imple-
ments the backpropagation and outputs classical description of ∂X⅛ and ∂FL. We have seen that the
two main calculations (17) and (18) are in fact matrix-matrix multiplications both involving ∂YL+r,
the reshaped form of ∂X∂L+γ ∙ For each, the classical running time is O(H'+1W'+1D'+1HWD').
We know from Theorem F.7 and Theorem G.1 a quantum algorithm to perform efficiently a
matrix-vector multiplication and return a classical state with '∞ norm guarantees. For a matrix
V and a vector b, both accessible from the QRAM, the running time to perform this operation
is O (μ(V)K(V)Iog 1∕δ), where K(V) is the condition number of the matrix and μ(V) is a matrix
parameter defined in Equation (5). Precision parameter δ > 0 is the error committed in the approxi-
mation for both Theorems F.7 and G.1.
We can therefore apply theses theorems to perform matrix-matrix multiplications, by simply de-
composing them in several matrix-vector multiplications. For instance, in Equation (17), the matrix
could be (A')t and the different vectors would be each column of ∂YL+ι. The global running time
to perform quantumly Equation (17) is obtained by replacing μ(V) by μ(∂γdL+τ) + μ(A') and K(V)
by k((A`)t ∙ ∂YL+τ). Likewise, for Equation (18), we have μ( ∂YdL+τ )+μ(F') and K(∂YLτ ∙ (FDT).
Note that the dimension of the matrix doesn,tappear in the running time since we tolerate a '∞ norm
guarantee for the error, instead of a '2 guarantee (see Section G for details). The reason why '∞
tomography is the right approximation here is because the result of these linear algebra operations
are rows of the gradient matrices, that are not vectors in an euclidean space, but a series of numbers
for which we want to be δ-close to the exact values. See next section for more details.
It is a open question to see if one can apply the same sub-sampling technique as in the forward pass
(Section D.1) and sample only the highest derivatives of ∂L, to reduce the computation cost while
maintaining a good optimization. We then have to understand which elements of ∂∂∂L+τ must be set
to zero to take into account the effects the non linearity, tomography and pooling.
E.2.2 Quantum Non Linearity and Tomography
To include the impact of the non linearity, one could apply the same rule as in (19), and simply
replace ReLu by capReLu. After the non linearity, we obtain f (X'+1), and the gradient relation
would be given by
26
Published as a conference paper at ICLR 2020
∂L
一 ∂X'+1 一
i'+1,j'+1,d'+1
∂f ∂f (X'+1)] i'+1 ,j'+1 ,d'+1
0 otherwise
if 0 ≤
^γr'+1
X i'+1,j'+1,d'+1
≤C
(22)
If an element of X + was negative or bigger than the cap C, its derivative should be zero during
the backpropagation. However, this operation was performed in quantum superposition. In the
quantum algorithm, one cannot record at which positions (i'+1,j'+1,d'+1) the activation function
was selective or not. The gradient relation (22) cannot be implemented a posteriori. We provide
a partial solution to this problem, using the fact that quantum tomography must also be taken into
account for some derivatives. Indeed, only the points (i'+1,j'+1, d'+1) that have been sampled
should have an impact on the gradient of the loss. Therefore we replace the previous relation by
一 ∂L 一
一 ∂X'+1 一
i'+1 ,j'+1 ,d'+1
(∖ ∂L ]
J LdX'+1 Ji'+1,j'+1,d'+1
0 otherwise
if (i'+1,j'+1,d'+1) was sampled
(23)
Nonetheless, we can argue that this approximation will be tolerable. In the first case where
X'+I ,j'+ι ,d'+ι < 0, the derivatives can not be set to zero as they should. But in practice, their values
will be zero after the activation function and such points would not have a chance to be sampled. In
-Y7"'+1
conclusion their derivatives would be zero as required. In the other case, where Xi'+1,j'+1,d'+1 > C,
the derivatives can not be set to zero as well but the points have a high probability of being sampled.
Therefore their derivative will remain unchanged, as if we were using a ReLu instead of a capReLu.
However in cases where the cap C is high enough, this shouldn’t be a source of disadvantage in
practice.
E.2.3 Quantum Pooling
From relation (23), we can take into account the impact of quantum pooling (see Section D.2.2) on
the derivatives. This case is easier since one can record the selected positions during the QRAM
update. Therefore, applying the backpropagation is similar to the classical setting with Equation
(20).
∂ L
∂X'+1
=([aχ'+J i'+1,j'+1,d'+1
i'+1,j'+1,d'+1	1° otherwise
if (i'+1,j'+1,d'+1) was selected during pooling
(24)
Note that we know dXXjL+ι as it is equal to aJXL+ι, the gradient with respect to the input of layer ' +1,
known by assumption and stored in the QRAM.
E.3 Conclusion and Running Time
In conclusion, given aYdL+ι in the QRAM, the quantum backpropagation first consists in applying
the relations (24) followed by (23). The effective gradient now take into account non linearity,
tomography and pooling that occurred during layer `. We can know use apply the quantum algorithm
for matrix-matrix multiplication that implements relations (18) and (17).
Note that the steps in Algorithm 2 could also be reversed: during backpropagation of layer ` + 1,
when storing values for each elements of aYdL+ι in the QRAM, one can already take into account
(24) and (23) of layer '. In this case we directly store ^lLr, at no supplementary cost.
Therefore, the running time of the
`, given as Algorithm 2, corresponds
the circuits for implementing relations
quantum backpropagation for one layer
to the sum of the running times of
(17) and (18). We finally obtain
27
Published as a conference paper at ICLR 2020
O (((μ(A') + μ(∂YdL+ι)) κ((A')T ∙") + (μ(R) + μ(F')) K" ∙ (f')t)) Y),
which can be rewritten as
O (((〃(A') 十 μ(∂YYL+ι)) κ(∂L) + (〃(∂YL+τ) + μ(F')) κ(焉))log2")	(25)
Besides storing ∂XL⅛, the main output is a classical description of ∂∂L, necessary to perform gradient
descent of the parameters of F'. In the Appendix (Section E.4), which details the impact of the
quantum backpropagation compared to the classical case, which can be reduced to a simple noise
addition during the gradient descent.
E.4 Quantum Gradient Descent and Classical equivalence
In this part we will see the impact of the quantum backpropagation compared to the classical case,
which can be reduced to a simple noise addition during the gradient descent. Recall that gradient
descent, in our case, would consist in applying the following update rule F` J F' - λ ∂∂∂L With the
learning rate λ.
Let,s note X = ∂∂L and its elements Xs,q = ∂fl~. From the first result of Theorem F.7 with
s,q
error δ < 0, and the tomography procedure from Theorem G.1, with same error δ, We can obtain a
classical description of ^^ with '∞ norm guarantee, such that:
Ilxk2	∣∣xk2
≤δ
∞
in time O(K(VMV)log(6), where we note V is the matrix stored in the QRAM that allows to obtain
x, as explained in Section E.2. The '∞ norm tomography is used so that the error δ is at most the
same for each component
∀(s,q),哥 - 什 ≤ δ
kxk2 kxk2
From the second result of the Theorem F.7 we can also obtain an estimate ∣∣X∣2 of the norm, for the
same error δ, such that
~	∣kx∣2 -kx∣2 l≤ δ kx∣2
in time O(KV)μ(v) log(δ)) (which does not affect the overall asymptotic running time). Using both
results we can obtain an unnormalized state close to x such that, by the triangular inequality
Ilx - x∣∞
∣⅛ kx∣2-向 kx∣2
∞
≤ ∣⅛ kx∣2 -1⅛ kx∣2∞+ ∣⅛^Xk2 -向 kx∣2∞
≤ ITkxk2-同2 |+ ∣x∣2 ∙∣ ∣⅛ -向L
≤δ llχ∣2 + ∣x∣2δ ≤2δ llχ∣2
in time O(KV)μ(V)log(δ)). In conclusion, with '∞ norm guarantee, having also access to the norm
of the result is costless.
Finally, the noisy gradient descent update rule, expressed as F%
J FS',q - λ ∂∂fl- can written in
the worst case with
∂FL- ±2δ
s,q
∂L
∂F'
(26)
∂ L
2
To summarize, using the quantum linear algebra from Theroem F.7 with '∞ norm tomography from
Theroem G.1, both with error δ, along with norm estimation with relative error δ too, we can obtain
classically the unnormalized values ∂∂L such that Il ∂∂L - ∂F-II ≤ 2δ ∣∣ ∂FF-b or equivalently
∀(s, q),
I ^^∂^	∂L I
1西-∂F'q∣≤2
∂L
∂F'
(27)
28
Published as a conference paper at ICLR 2020
Therefore the gradient descent update rule in the quantum case becomes Fsq J Fsq - λ ∂FL-,
s,q
which in the worst case becomes
Fs,q J Fs,q - λ (∂Fs- ±2δ 而 2)	(28)
This proves the Theorem E.1. This update rule can be simulated by the addition ofa random relative
noise given as a gaussian centered on 0, with standard deviation equal to δ. This is how we will
simulate quantum backpropagation in the Numerical Simulations.
Compared to the classical update rule, this corresponds to the addition of noise during the optimiza-
tion step. This noise decreases as ∣∣ ∂∂L ∣∣2, which is expected to happen while converging. Recall
that the gradient descent is already a stochastic process. Therefore, we expect that such noise, with
acceptable values of δ, will not disturb the convergence of the gradient, as the following numerical
simulations tend to confirm.
Appendix F	Preliminaries in Quantum Information
We introduce a basic and broad-audience quantum information background necessary for this work.
For a more detailed introduction we recommend Nielsen & Chuang (2002a).
F.1 Quantum Information
Quantum Bits and Quantum Registers: The bit is the most basic unit of classical information.
It can be either in state 0 or 1. Similarly a quantum bit or qubit, is a quantum system that can be is
state |0〉，11)(the braket notation |) is a reminder that the bit considered is a quantum system) or in
superposition of both states a |0) + β |1) with coefficients a,β ∈ C such that ∣α∣2 + ∣β∣2 = 1. The
amplitudes α and β are linked to the probabilities of observing either 0 or 1 when measuring the
qubit, since P(0) = ∣α∣2 and P⑴=∣β|2.
Before the measurement, any superposition is possible, which gives quantum information special
abilities in terms of computation. With n qubits, the 2n possible binary combinations can exist
simultaneously, each with a specific amplitude. For instance we can consider an uniform distribution
2n-1
√n Ei=O ∣i) where |i) represents the ith binary combination (e.g. |01 …1001)). Multiple qubits
together are often called a quantum register.
In its most general formulation, a quantum state with n qubits can be seen as vector in a complex
Hilbert space of dimension 2n. This vector must be normalized under `2 -norm, to guarantee that the
squared amplitudes sum to 1.
Quantum Computation: To process qubits and therefore quantum registers, we use quantum
gates. These gates are unitary operators in the Hilbert space as they should map unit-norm vectors
to unit-norm vectors. Formally, we can see a quantum gate acting on n qubits as a matrix U ∈ C2n
such that UU * = U TU = I, where U * is the conjugate transpose of U. Some basic single qubit
gates includes the NOT gate (； j) that inverts |0) and |1), or the Hadamard gate = (1 —1
that maps |0) → 土(|0) + |1)) and |1) → √12(∣0)-∣1)), creating the quantum superposition.
Finally, multiple qubits gates exist, such as the Controlled-NOT that applies a NOT gate on a target
qubit conditioned on the state of a control qubit.
The main advantage of quantum gates is their ability to be applied to a superposition of inputs.
Indeed, given a gate U such that U |x) 7→ |f (x)), we can apply it to all possible combinations of x
at once U(C Px Ixi) → C1 Px lf(x)i.
We now state some primitive quantum circuits, which we will use in our algorithm:
For two integers i and j, we can check their equality with the mapping |ii |ji |0i 7→ |ii |ji |[i = j]i.
For two real value numbers a > 0 and δ > 0, we can compare them using |a) ∣δ)∣0) →
|a) ∣δ)∣[a ≤ δ]). Finally, for a real value numbers a > 0, we can obtain its square |a) |0) → |a) |a2).
29
Published as a conference paper at ICLR 2020
Note that these circuits are basically a reversible version of the classical ones and are linear in the
number of qubits used to encode the input values.
F.2 Quantum Subroutines for Data Encoding
Knowing some basic principles of quantum information, the next step is to understand how data can
be efficiently encoded using quantum states. While several approaches could exist, we present the
most common one called amplitude encoding, which leads to interesting and efficient applications.
Let X ∈ Rd be a vector with components (xι, ∙∙∙ , xd). Using onlydlog(d)] qubits, We can form
|xi，the quantum state encoding x, given by |x)= 看 Pd-I Xjji∙ We see that the jth component
xj becomes the amplitude of |ji, the jth binary combination (or equivalently the jth vector in the
standard basis). Each amplitude must be divided by IlXkto preserve the unit '2-norm of |x).
Similarly, for a matrix A ∈ Rn×d or equivalently for n vectors Ai for i ∈ [n], we can express each
row of A as IAii = kA1ik Pd-(I Aij |ji.
We can now explain an important definition, the ability to have quantum access to a matrix. This
will be a requirements for many algorithms.
Definition 4 [Quantum Access to Data]
We say that we have quantum access to a matrix A ∈ Rn×d if there exist a procedure to perform
the following mapping, for i ∈ [n], in time T :
•	|ii |0i 7→ |ii |Aii
•	l0i → 曷7 PikAikIii
By using appropriate data structures the first mapping can be reduced to the ability to perform a
mapping of the form Iii Iji I0i 7→ Iii Iji IAiji. The second requirement can be replaced by the
ability of performing Iii I0i 7→ Iii IkAi ki or to just have the knowledge of each norm. Therefore,
using matrices such that all rows Ai have the same norm makes it simpler to obtain the quantum
access.
The time or complexity T necessary for the quantum access can be reduced to polylogarithmic
dependence in n and d if we consider the access to a Quantum Memory or QRAM. The QRAM
Kerenidis & Prakash (2017a) is a specific data structure from which a quantum circuit can allow
quantum access to data in time O(log (nd)).
Theorem F.1 (QRAM data structure, see Kerenidis & Prakash (2017a)) Let A ∈ Rn×d, there
is a data structure to store the rows of A such that,
1.	The time to insert, update or delete a single entry Aij is O(log2(n)).
2.	A quantum algorithm with access to the data structure can perform the following unitaries
in time T = O(log2 n).
(a)	Iii I0i → Iii IAii for i ∈ [n].
(b)	I0i → Pi∈[n] kAik Iii.
We now state important methods for processing the quantum information. Their goal is to store some
information alternatively in the quantum state’s amplitude or in the quantum register as a bitstring.
TheoremF.2 [Amplitude Amplification and Estimation Brassard et al. (2002)] Given a unitary
operator U such that U : |0)一 √p |y)|0〉+ √1 一 P ∣y⊥i∣1> in time T, where p > 0 is the
probability of measuring “0”, it is possible to obtain the state
to estimate p with relative error δ
using O(δ√p) queries to U.
|y)|0)using O(√T) queries to U, or
Theorem F.3 [Conditional Rotation] Given the quantum state Iai, with a ∈ [一1, 1], it is possible
to perform |a〉|0)→ |a〉(a |0〉+ √1 — a |1〉)with complexity O⑴.
30
Published as a conference paper at ICLR 2020
Using Theorem F.3 followed by Theorem F.2, it then possible to transform the state % P：- |xj)
into k⅛ Pd- Xj∣Xji∙
In addition to amplitude estimation, we will make use of a tool developed in Wiebe et al. (2014a)
to boost the probability of getting a good estimate for the inner product required for the quantum
convolution algorithm. In high level, we take multiple copies of the estimator from the amplitude
estimation procedure, compute the median, and reverse the circuit to get rid of the garbage. Here we
provide a theorem with respect to time and not query complexity.
Theorem F.4 (Median Evaluation, see Wiebe et al. (2014a)) Let U be a unitary operation that
maps
U : ∣00ni → √a |x, 1i + √Γ-Σ |G, 0i
for some 1/2 < a ≤ 1 in time T. Then there exists a quantum algorithm that, for any ∆ > 0 and
for any 1/2 < α0 ≤ a, produces a state ∣Ψ> such that k ∣Ψi — |0MLiIxik ≤ √2∆ for some integer
L, in time
2T & In(I/A 2'.
2 (|。0| - 2)
F.3 Quantum subroutines for Linear Algebra
In the recent years, as the field of quantum machine learning grew, its “toolkit” for linear alge-
bra algorithms has become important enough to allow the development of many quantum machine
learning algorithms. We introduce here the important subroutines for this work, without detailing
the circuits or the algorithms.
Definition 5 For a matrix A,	the parameter μ(A)	is defined by μ(A)
minp∈[0,1]
(kAkF , ↑∕s2p(A)s2(i-p)(AT/) where Sp(A) = maxi(kAikP)∙
The next theorems allow to compute the distance between vectors encoded as quantum states, and
use this idea to perform the k-means algorithm.
Theorem F.5 [Quantum Distance Estimation Wiebe et al∙ (2014b); Kerenidis et al∙ (2019)] Given
quantum access in time T to two matrices U and V with rows ui and vj of dimension d, there
is a quantum algorithm that, for any pair (i, j), performs the following mapping |ii |ji |0i 7→
∣ii∣ji∣d2(ui, Vj )i, estimating the euclidean distance between Ui and Vj with precision ∣d2(ui, Vj)—
d2 (ui, Vj)| ≤ for any > 0∙ The algorithm has a running time given by O(T η/), where
η = maxij (kui k kVj k), assuming that mini(kuik) = mini(kVik) = 1∙
Theorem F.6 [Quantum k-means clustering Kerenidis et al∙ (2019)]
Given quantum access in time T to a dataset V ∈ Rn×d, there is a quantum algorithm that outputs
with high probability k centroids ci, ∙∙∙ , Ck that are consistent with the output ofthe k-means algo-
rithm with noise δ > 0, in time O(T X (kdηδvV)K(V)(μ(V) + kη(δv)) + k2 η(VV2— K(V)μ(V))) per
iteration∙
Definition 6 For a matrix V ∈ Rn×d, its parameter η(V) is defined as as ：：：,[：：“), or as
maxi(kVik2) assuming mini(kVik) = 1∙
In theorem F.6, the other parameters in the running time can be interpreted as follows : δ is the
precision in the estimation of the distances, but also in the estimation of the position of the centroids.
κ(V) is the condition number of V and μ(V) is defined above (Definition 5). Finally, in the case
of well clusterable datasets, which should be the case when we will apply k-means during spectral
clustering, the running simplifies to O(T × (k2dη(VV3 + k25 η(V3) )).
Note that the dependence in n is hidden in the time T to load the data. This dependence becomes
polylogarithmic in n ifwe assume access to a QRAM.
31
Published as a conference paper at ICLR 2020
Theorem F.7 (Quantum Matrix Operations, Chakraborty et al. (2018) ) Let M ∈ Rd×d and
x ∈ Rd. Let δ1, δ2 > 0. If M is stored in appropriate QRAM data structures and the time to
prepare |xi is Tx, then there exist quantum algorithms that with probability at least 1 - 1/poly(d)
return
1.	A state |z〉such that |||z)— ∣Mxik2 ≤ δι in time O((K(M)μ(M) + TxK(M))log(1∕δι)).
Note that this also implies k|zi - |M xik∞ ≤ δ1
2.	Norm estimate z ∈	(1 ± δ2) kMxk2,	with relative error δ2,	in time
O(TxKMμM iog(i∕δι)).
The linear algebra procedures above can also be applied to any rectangular matrix V ∈ Rn×d by
considering instead the symmetric matrix V
Appendix G Algorithm and Proof for '∞ norm tomography
Finally, we present a logarithmic time algorithm for vector state tomography that will be used to re-
cover classical information from the quantum states with '∞ norm guarantee. Given a unitary U that
produces a quantum state | x)= jX1^ Pd- Xjji, by calling O (log d∕δ2) times U, the tomography
algorithm is able to reconstruct a vector X that approximates |x) with '∞ norm guarantee, such that
|Xe i —	|xi	≤ δ, or equivalently that ∀i	∈	[d], |xi	—	Xei|	≤	δ.	Such a tomography is of interest
when the com∞ponents xi ofa quantum state are not the coordinates ofan meaningful vector in some
linear space, but just a series of values, such that we don’t want an overall guarantee on the vector
(which is the case with usual `2 tomography) but a similar error guarantee for each component in
the estimation.
Theorem G.1 ('∞ Vector state tomography) Given access to unitary U such that U |0) = |x)
and its controlled version in time T(U), there is a tomography algorithm with time complexity
O(T(U) lOg2d) that produces unit vector X ∈
(1 — 1∕poly(d)).
Rd such that Xe — x
≤ δ with probability at least
The proof of this theorem is similar to the proof of the '2-norm tomography by Kerenidis & Prakash
(2018). However the '∞ norm tomography introduced in this paper depends only logarithmically
and not linearly in the dimension d. Note that in our case, T(U) will be logarithmic in the dimension.
Theorem G.2 [`2 Vector state tomography Kerenidis & Prakash (2018)] Given access to unitary
U such that U |0i = |xi and its controlled version in time T(U), there is an algorithm that allows
to output a classical vector X ∈ Rd with '2 -norm guarantee
≤ δ for any δ > 0, in time
O(T(U) X do2d).
Xe — x2
in the following we consider a quantum state |xi = Pi∈[d] xi |ii, with x ∈ Rd and kxk2 = 1.
The following version of the Chernoff Bound will be used for analysis of algorithm 3.
Theorem G.3 (Chernoff Bound) Let Xj , for j ∈ [N], be independent random variables such that
Xj ∈ [0, 1] and let X = j ∈[N] Xj . We have the three following inqualities:
1.	For 0 <β< 1,P[X < (1 — β)E[X]] ≤ e-β2E[X]/2
2.	For β > 0, P[X > (1+ β)E[X]] ≤ e-2+βE[X]
3.	For 0 < β < 1, P[|X - E[X ]| ≥ βE[X ]] ≤ e-β2E[x]/3 ,by composing 1. and 2.
32
Published as a conference paper at ICLR 2020
Algorithm 3 '∞ norm tomography
Require: Error δ > 0, access to unitary U : |0i 7→ |xi =	i∈[d] xi |ii, the controlled version of U,
QRAM access.
Ensure: Classical vector Xe ∈ Rd, such that
Xe=
1 and
Xe - x∞
< δ.
1: Measure N = 36δ⅛^d copies of |x〉in the standard basis and count ni, the number of times the
outcome i is observed. Store √pi = Yni/N in QRAM data structure.
2： Create N = 36δnd COPieS of theState √. |0i Pi∈[d] Xi |ii + √2 |1i Pi∈[d] √pi |ii.
3:	Apply an Hadamard gate on the first qubit to obtain
IΦi = 2 X ((Xi + √pi) |0, ii + (Xi- √pi) |1, ii)
i∈[d]
4:	Measure both registers of each copy in the standard basis, and count n(0, i) the number of time
the outcome (0, i) is observed.
5:	Set σ(i) = +1 if n(0, i) > 0.4N pi and σ(i) = -1 otherwise.
6:	Output the unit vector X such that ∀i ∈ [N],Xi = σi√pi
Theorem G.4 Algorithm 3 produces an estimate X ∈
probability at least 1 一 ^0183.
Rd such that Xe - X
< (1 + √2)δ with
Proving	X - Xe	≤ O(δ)	is equivalent to show that for all i ∈	[d],	we have	|Xi	-	Xei|	=
|xi — σ(i)√pi∣ ≤ O(δ). Let S be the set of indices defined by S = {i ∈ [d]; ∣x∕ > δ}. We will
separate the proof for the two cases where i ∈ S and i ∈/ S.
Case 1	: i ∈ S.
We will show that if i ∈ S, we correctly have σ(i) = sgn(Xi) with high probability. Therefore
we will need to bound |xi 一 σ(i)√pi∣ = ||xi| 一 √pi∣.
We suppose that Xi > 0. The value of σ(i) correctly determines sgn(Xi) if the number of times
We have measured (0, i) at Step 4. is more than half of the outcomes, i.e. n(0, i) > 1 E[n(0, i)]. If
Xi < 0, the same arguments holds for n(1, i). We consider the random variable that represents the
outcome of a measurement on state | φi. The Chernoff Bound, part 1 with β = 1/2 gives
P[n(0, i) ≤ 1 E[n(0, i)]] ≤ e-E[n(O㈤]/8	(29)
From the definition of ∣φi we have E[n(0,i)] = N (Xi + √pi)2. We will lower bound this value with
the following argument.
For the kth measurement of |Xi, with k ∈ [N], let Xk be a random variable such that Xk = 1 if
the outcome is i, and 0 otherwise. We define X = Pk∈[N] Xk. Note that X = ni = Npi and
E[X] = NXi2. We can apply the Chernoff Bound, part 3 on X for β = 1/2 to obtain,
P[|X - E[X]| ≥ EX]/2] ≤ e-E[X]/12
(30)
P[∣x2 一 Pil ≥ x2/2] ≤ e-Nx2/12
We have N = 36δn d and by assumption Xti > δ2 (since i ∈ S). Therefore,
P[∣x2 - Pil ≥ Xi/2] ≤ e-36ln d/12 = 1/d3
33
Published as a conference paper at ICLR 2020
This proves that the event |x2 - pi| ≤ x2/2 occurs with probability at least 1 - ʤ if i ∈ S. This
previous inequality is equivalent to vz2pi∕3 ≤ ∣x∕ ≤ √2pi. Thus, with high probability We have
E[n(0,i)] = N(Xi + √pi)2 ≥ 0.82Npi, since p2pi∕3 ≤ |xi|. Moreover, since ∣p∕ ≤ x2∕2,
E[n(0, i)] ≥ 0.82N xi2 /2 ≥ 14.7 ln d. Therefore, equation equation 29 becomes
P[n(0, i) ≤ 0.41Npi] ≤ e-1.83 ln d = 1∕d1.83
We conclude that for i ∈ S, if n(0, i) > 0.41N pi, the sign of xi is determined correctly by σ(i)
with high probability 1 - d⅛, as indicated in Step 5.
We finally show |xi - σ(i)√pi∣ = ∣∣χ∕- √pi∣ is bounded. Again by the Chernoff Bound (3.) we
have, for 0 < β < 1:
P[∣x2 - Pi| ≥ βx2] ≤ eβ2N*2/
By the identity |x2 - pi| = (∣x∕- √pi)(∣Xi| + √pi) we have
P
≤ eβ2Nχ2∕3
Since √pi	>	0,	we have β∣xi∣+√κ	≤	β若 =β∣Xi∣,	therefore P ["i∣	-	√pi∣	≥ β∣χ∕]	≤
eβ2Nx2/3. Finally, by chosing β = δ∕∣x∕ < 1 we have
P h∣∣Xi∣-√pi∣ ≥ δ] ≤ e36lnd/3 = 1∕d12
We conclude that, if i ∈ S, we have |xi - Xi | ≤ δ with high probability.
Since |S| ≤ d, the probability for this result to be true for all i ∈ S is 1 - d0^. This can be proved
by using the Union Bound on the correctness of σ(i).
Case 2	: i ∈∕ S.
If i ∈∕ S, we need to separate again in two cases. When the estimated sign is wrong, i.e. σ(i) =
-Sgn(xi), we have to bound ∣Xi - σ(i)√pi∣ = ||x/ + √pi∣. On the contrary, if it is correct, i.e.
σ(i) = sgn(xi), we have to bound |xi - σ(i)√pi| = ∣∣x∕- √pi∣ ≤ ∣∣x∕ + √pi∣. Therefore only
one bound is necessary.
We use Chernoff Bound (2.) on the random variable X with β > 0 to obtain
P[pi > (1 + β)x2] ≤ e2+βNx
δ4
We chose β = δ2∕x2 and obtain Pp > x2 + δ2] ≤ e3δ2 = 1∕d12. Therefore, if i ∈ S, with very
high probability 1 - 总 we have Pi ≤ x2 + δ2 ≤ 2δ2. We can conclude and bound the error:
∣Xi - Xi | ≤ ∣∣Xi∣ + √pi | ≤ δ + √2δ =(1 + √2)δ
Since |S| ≤ d, the probability for this result to be true for all i ∈ S is 1 -击.This follows from
applying the Union Bound on the event pi > xi2 + δ2 .
34
Published as a conference paper at ICLR 2020
Appendix H Additional Numerical Simulations
Training cur∖^es for e=0.1, C=Io.0, 3=0.1
Training curves for e=0.01, C=2.0, ∂=0.01
Figure 9: Numerical simulations of the training of the QCNN. These training curves represent the
evolution of the Loss L as we iterate through the MNIST dataset. For each graph, the amplitude
estimation error (0.1, 0.01), the non linearity cap C (2, 10), and the backpropagation error δ
(0.1, 0.01) are fixed whereas the quantum sampling ratio σ varies from 0.1 to 0.5. We can compare
each training curve to the classical learning (CNN). Note that these training curves are smoothed,
over windows of 12 steps, for readability.
In the following we report the classification results of the QCNN when applied on the test set
(10.000 images). We distinguish to use cases: in Table 4 the QCNN has been trained quantumly as
described in this paper, whereas in Table 5 we first have trained the classical CNN, then transferred
the weights to the QCNN only for the classification. This second use case has a global running time
worst than the first one, but we see it as another concrete application: quantum machine learning
could be used only for faster classification from a classically generated model, which could be
the case for high rate classification task (e.g. for autonomous systems, classification over many
simultaneous inputs). We report the test loss and accuracy for different values of the sampling ratio
σ, the amplitude estimation error , and for the backpropagation noise δ in the first case. The cap C
is fixed at 10. These values must be compared to the classical CNN classification metrics, for which
the loss is 0.129 and the accuracy is 96.1%. Note that we used a relatively small CNN and hence
35
Published as a conference paper at ICLR 2020
the accuracy is just over 96%, lower than the best possible accuracy with larger CNN.
	QCNN Test - C山Ssfication						
σ	€	0.01		0.1	
	δ	0.01	0.1	0.01	0.1
0.1	Loss	0.519	0.773	2.30	2.30
	Accuracy	82.8%	74.8%	11.5%	11.7%
0.2	Loss	0.334	0.348	0.439	1.367
	Accuracy	89.5%	89.0%	86.2%	54.1%
0.3	Loss	0.213	0.314	0.381	0.762
	Accuracy	93.4%	90.3%	87.9%	76.8%
0.4	Loss	0.177	0.215	0.263	1.798
	Accuracy	94.7%	93.3%	91.8%	34.9%
0.5	Loss	0.142	0.211	0.337	1.457
	Accuracy	95.4%	93.5%	89.2%	52.8%
Table 4: QCNN trained with quantum backpropagation on MNIST dataset. With C = 10 fixed.
QCNN Test - αassfication			
Q	€	0.01	0.1
0.1	Loss Accuracy	1.07 86.1%	1.33 78.6%
0.2	Loss Accuracy	0.552 92.8%	0.840 86.5%
0.3	Loss Accuracy	0.391 94,3%	0.706 85.8%
0.4	Loss Accuracy	0.327 94.4%	0.670 84.0%
0.5	Loss Accuracy	0.163 95.9%	0.292 93.5%
Table 5: QCNN created from a classical CNN trained on MNIST dataset. With δ = 0.01 and
C = 10 fixed.
36