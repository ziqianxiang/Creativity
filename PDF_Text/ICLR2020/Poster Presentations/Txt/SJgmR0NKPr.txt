Published as a conference paper at ICLR 2020
Training Recurrent Neural Networks Online
by Learning Explicit State Variables
Somjit Nath, Vincent Liu, Alan Chan, Xin Li, Adam White and Martha White
Department of Computing Science
University of Alberta
{somjit,vliu1,achan4,xzli,amw8,whitem}@ualberta.ca
Ab stract
Recurrent neural networks (RNNs) allow an agent to construct a state-
representation from a stream of experience, which is essential in partially ob-
servable problems. However, there are two primary issues one must overcome
when training an RNN: the sensitivity of the learning algorithm’s performance to
truncation length and and long training times. There are variety of strategies to
improve training in RNNs, the mostly notably Backprop Through Time (BPTT)
and by Real-Time Recurrent Learning. These strategies, however, are typically
computationally expensive and focus computation on computing gradients back in
time. In this work, we reformulate the RNN training objective to explicitly learn
state vectors; this breaks the dependence across time and so avoids the need to
estimate gradients far back in time. We show that for a fixed buffer of data, our
algorithm—called Fixed Point Propagation (FPP)—is sound: it converges to a
stationary point of the new objective. We investigate the empirical performance
of our online FPP algorithm, particularly in terms of computation compared to
truncated BPTT with varying truncation levels.
1	Introduction
Many online prediction problems are partially observable: the most recent observation is typically
insufficient to make accurate predictions about the future. Augmenting the inputs with a history can
improve accuracy, but can require a long history when there are long-term dependencies back in time.
Recurrent Neural Networks (RNNs) (Elman, 1990; Hopfield, 1982) learn a state which summarizes
this history. Specifically, RNNs contain recurrent connections to their hidden layers which allow past
information to propagate through time. This state need not correspond to a true underlying state;
rather, the state is subjective and constructed to facilitate prediction. RNNs have been widely used, in
speech recognition (Hinton et al., 2012; Graves et al., 2013; Miao et al., 2015; Chan et al., 2016),
image captioning (Mao et al., 2014; Lu et al., 2016; Vinyals et al., 2014), speech synthesis (Mehri et
al., 2016) and reinforcement learning (Hochreiter and Schmidhuber, 1997; Dull et al., 2012).
Despite these success, there are significant stability and computational issues in training RNNs online
(Pascanu et al., 2013; Tallec and Ollivier, 2017). In the online setting, the agent faces an unending
stream of data and on each step the agent must update its parameters to make a new prediction.
RNNs are typically trained either using Backpropagation-through-time (BPTT) (Werbos, 1990) or
approximations to an algorithm called Real-Time Recurrent Learning (RTRL) (Williams and Zipser,
1989a; Pearlmutter, 1995), although there are methods that appeal to other principles (see Murray
(2019) for instance). The update for BPTT is a variant of standard backpropagation, computing
gradients all the way back in time. This approach is problematic because the computational cost scales
linearly with the number of time-steps. A more common alternative is truncated BPTT (T-BPTT)
(Williams and Peng, 1990) which only computes the gradient up to some maximum number of steps:
we truncate how far back in time we unroll the network to update the parameters. This approximation,
though, is not robust to long-term dependencies (Tallec and Ollivier, 2017). Approximate gradients
can also be computed online by RTRL (Williams and Zipser, 1989b). This online algorithm, however,
has high computational complexity per step and therefore is not commonly used in practice.
1
Published as a conference paper at ICLR 2020
Recently, there have been some efforts towards approximating gradients for back-propagation, both
for feedforward NNs and RNNs. Synthetic gradients and BP(λ) (Jaderberg et al., 2017) use an
idea similar to returns from reinforcement learning: they approximate gradients by bootstrapping
off estimated gradients in later layers (Jaderberg et al., 2017; Czarnecki et al., 2017). There are
several methods that approximate RTRL—which is itself an approximation of the true gradient
back in time—including NoBackTrack (Ollivier and Charpiat, 2015), Unbiased Online Recurrent
Optimization (UORO) (Tallec and Ollivier, 2017) which uses an unbiased rank-1 approximation to
the full matrix gradient, and Kronecker Factored RTRL (Mujika et al., 2018) which uses a Kronecker
product decomposition to approximate the RTRL update for a class of RNNs. Finally, there are some
methods that use selective memory back in time to compute gradients for the most pertinent samples,
using skip connections (Ke et al., 2017). All of these methods, however, attempt to approximate the
gradient back in time, for the current observation and state, and so suffer to some extent from the
same issues as BPTT and RTRL.
In this paper, we investigate an alternative optimization strategy that does not attempt to approximate
the gradient back in time. Instead we learn the state variables in the RNN explicitly. These new
variables are optimized to both improve prediction accuracy, and to maintain consistency in producing
the next learned state variables. This second constraint is a fixed-point formula for the states under
the given RNN dynamics.1 We develop a provably sound stochastic update for the new fixed-point
objective, which we then use to develop an online algorithm for training RNNs. The algorithm
explicitly optimizes state vectors and RNN parameters with many efficient one-step—or short term
multi-step updates—across a buffer. Instead of focusing computation to get a more accurate gradient
estimates for this time-step, our algorithm, called Fixed Point Propagation (FPP), can more effectively
use computation to update prediction accuracy across states. We demonstrate that the algorithm is
effective on several problems with long-term dependencies, and improves over T-BPTT, particularly
in terms of stability and computation.
2	Problem Setting and Background
We consider a partially observable online setting, where an immediate observation is not sufficient
for prediction. More formally, assume there is a sequence of n observations, o1 , . . . , on, which
provide only partial information about an unknown underlying sequence of states. After obtaining
an observation Oi, the agent makes a prediction y and sees the actual outcome y The goal is to
minimize this prediction error. Given only oi , the agent is unlikely to make accurate predictions about
yi, because oi is not a sufficient statistic to predict yi: p(y|oi, oi-1, oi-2, . . .) 6= p(y|oi). The agent
could obtain a lower prediction error using a history of observations. Unfortunately, the agent may
require a prohibitively long history, even if this history could have been summarized compactly.
An alternative is to construct state using a Recurrent Neural Network (RNN), by learning a state-
update function. Given the current (constructed) state st-1 ∈ Rk, and a new observation ot ∈ Rd,
the parameterized state-update function fW : Rk × Rd → Rk , with parameters W, produces the next
(constructed) state st = fW(st-1, ot). For example, fW could be a linear weighting of st-1 and
ot, with a ReLu activation: fW(st-1, ot) = max([st-1, ot]W, 0). More complex state-updates are
possible, like the gating in Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).
The objective is to adapt these parameters W for the state-update to minimize prediction error. For
the current state st , a prediction is made by parameterized function gβ : Rk → Rm with learned
parameters β. For example, the prediction could be a linear weighting of the state, gβ (s) = β>s.
We denote the prediction error as 'β : Rk X Rm → R fora given β. For example, this loss could be
'β (st； yt) = kgβ (st) — ytk2.
1Recurrent Backpropagation and related variants (Almeida, 1987; Pineda, 1987; Scellier and Bengio, 2017;
Liao et al., 2018) also use fixed points for their optimization, but in a different way. These algorithms only
address a restricted class of RNNs, that assume a fixed input and converge to a single low-energy state—a fixed
point of the dynamics for that given input. These RNNs are actually highly related to Graph NNs (Scarselli et al.,
2008), because the temporal nature only arises from cyclic connections, rather than from temporal data. Their
problem setting is fundamentally different from our online prediction setting, and is usually used for associative
memory with Hopfield networks or semi-supervised problems. Recurrent Backpropagation cannot be used for
our setting and so we do not further discuss this class of RNN algorithms.
2
Published as a conference paper at ICLR 2020
The goal in RNNs training is to minimize, for some start state s0,
n
min∑2 'β (fw(…fw (fw(so, oι), 02),…,ɑɔ; yj	⑴
吟 M	`一{z一}
s1
Computing gradients for this objective, however, can be prohibitively expensive. A large literature on
optimizing RNNs focuses on approximating this gradient, either through approximations to RTRL or
improvements to BPTT. RTRL (Williams and Zipser, 1989b) uses a recursive gradient form, which
can take advantage of gradients computed up until the last observation to compute the gradient
for the next observation. This estimate, however, is only exact in the offline case and thus RTRL
is an approximation of the true gradient in our online setting. Further, in either online or offline,
RTRL requires O(k4) computation per observation (recall k is the dimension of the state). In BPTT,
gradients are computed back in time, by unrolling the network. In the online setting, it is infeasible to
compute gradients all the way back to the beginning of time. Instead, this procedure is truncated T
steps back in time. T-BPTT is suitable for the online setting, and requires O(T k2 ) computation per
step, i.e., for each observation.
Arguably the most widely-used strategy is T-BPTT, because of its simplicity. Unfortunately, T-BPTT
has been shown to fail in settings where dependencies back in time are further than T (Tallec and
Ollivier, 2017), as we affirm in our experiments. Yet, the need for simple algorithms remains. In this
work, we investigate an alternative direction for optimizing RNNs that does not attempt to estimate
the gradients of (1).
Note that in addition to a variety of optimization strategies, different architectures have also been
proposed to facilitate learning long-term dependencies with RNNs. The most commonly used are
LSTMs (Hochreiter and Schmidhuber, 1997), which use gates to remember and forget components of
the state. Other architectures include clockwork RNNs (KoUtnik et al., 2014), phased LSTMs (Neil
et al., 2016), hierarchical multi-scale RNNs (Chung et al., 2016), dilated RNNs (Chang et al., 2017),
and skip RNNs (Campos et al., 2017). In this work, we focus on a general purpose RNN algorithm,
that could be combined with each of these architectures for further improvement.
3	A New Fixed-Point Objective for RNNs
In this section we introduce our new formulation for training RNNs. We begin with an idealized
setting to introduce and explain the ideas. Later we will generalize our approach to partially observable
online training tasks.
First, assume the observations are produced by an underly Markov Chain with a discrete set of states,
and the agent has access to a set of observations that are deterministic function of the state. We denote
the set of states H = {1, . . . , n}, and the observations from each state as 01, . . . , 0n . The goal is to
find state vectors s1 , . . . , sn ∈ Rk that satisfy two goals. One is to enable the state to be updated
fW(si,0j) = sj ∀j where P(i, j) > 0	(2)
for P : H × H → [0, 1] the transition dynamics. Another criterion is for these state vectors to
facilitate accurate predictions. In particular, the learned state should minimize 'β (sj; y) for all
h, where yj ∈ R is the expected target for a true state j . Together, this results in the following
optimization, with the relationship between states encoded as a constraint
MΣP(i,j )'β (fW M o )，yj)
i,j∈H
s.t. fW(si, 0j) = sj ∀ i,j where P(i,j) > 0
The satisfiability of this will depend on fW and if si and 0j can uniquely determine sj .
More generally, we will not know the underlying state, nor is it necessarily discrete. But, we
can consider a similar objective for observed data. Assume n observations have been observed,
01, . . . , 0n, with corresponding targets y1, . . . , yn. Let the state variables be stacked in a matrix
S ∈ Rk×n and observations as a matrix O ∈ Rd×n, with S = [s0, . . . , sn] and O = [01, . . . , 0n].
The constraint on the states becomes S = FW(S, O) for operator
FW(S, O) d=ef [S:,0,fW(S:,0, O:,1),...,fW(S:,n-1, O:,n)].	(3)
3
Published as a conference paper at ICLR 2020
We call this the fixed-point constraint, since a solution S to the constraint is a fixed point of the
system defined by FW(∙, O). The resulting optimization, for this batch, is
n
min E'β(fw(si-1, Oi)； yi)	s.t. S = FW(S, O).
β,W,S
i=1
(4)
The solution to this new optimization corresponds to the solution for the original RNN problem in
(1)—when also optimizing over s0 in (1)—because the fixed-point constraint forces variables si to
be representable by fW . Therefore, the reformulation as a fixed point problem has not changed the
solution; rather, it has only made explicit that the goal is to learn these states and facilitates the use of
alternative optimization strategies.
Reformulations like the one in (4) have been widely considered in optimization, because (4) is
actually an auxiliary variable reformulation of (1). In this case, the auxiliary variables are the states
S. Using auxiliary variables is a standard strategy in optimization—under the general term method of
multipliers—to decouple terms in an optimization and so facilitate decentralized optimization.
Such auxiliary variable methods have even been previously considered for optimizing neural networks.
Carreira-Perpindn and Wang (2014) introduced the Method of Auxiliary Coordinates (MAC), which
explicitly optimize hidden vectors in the neural network. Taylor et al. (2016) proposed a similar
strategy, but introduced an additional set of auxiliary variables to obtain further decoupling and
a particularly efficient algorithm for the batch setting. Scellier and Bengio (2017) introduced
Equilibrium Propagation for symmetric neural networks, where the state of the network is explicitly
optimized to obtain a stationary point in terms of the energy function. Gotmare et al. (2018) built on
these previous ideas to obtain a stochastic gradient descent update for distributed updates to blocks of
weights in a neural network. Our proposed optimization can be seen as a variation of the objective
considered for MAC (Carreira-Perpindn and Wang, 2014, Equation 1), though we arrived at it from a
different perspective: with the goal to learn explicit state vectors.
The objective in (4) still has two issues. First, it is not amenable to online updating: it is a batch
optimization with a constraint. Second, it does not allow for any training back in time. But, this
stringent computational restriction is unnecessary. We could have instead asked: learn states so that
when iterated twice through the RNN, the resulting state enables accurate predictions on the target
two steps in the future. We develop a more general objective below to address both issues.
We can rewrite the objective so that it is clear how to stochastically sample it, and so enable online
updating. As in MAC-QP (Carreira-Perpindn and Wang, 2014), we reformulate this constrained
objective into an unconstrained objective with a quadratic penalty, with λ > 0
nn
L(β, W, S) =f — X'β(fw(si-ι, Oi)； yi) + 2- X ksi - fw(Si-1, Oi)k2
n i=1	2n i=1
(5)
Once in this unconstrained form, we can perform stochastic gradient descent on this objective in
terms of β, W and S to reach a stationary point. To use stochastic gradient descent, the objective
needs to break up into a sum of losses, L(β, W, S) = n Pinn=1 Li(β, W, S), where we define
Li(β, W, S)=f 'β (fW (si-1, oi)； yi) + 2 Ilsi- fW(Si-L Oi)Il2.
We can stochastically sample i from our buffer of n samples and update our variables with VLi.
Fortunately, because the state variables break connections across time, this gradient is zero for most
variables, except β, W, si-1 and si . Therefore, each stochastic update can be computed efficiently.
Second, we can generalize this objective to incorporate more than one step of propagation back in
time, simply by generalizing the fixed-point operator. Consider the more general T -step fixed point
problem S = FT,W(S, O) where
FT,W(S,O)d=ef S:,0, S:,1
. , S:,T-1, fW(. . . fW(fW(S:,0, O:,1), O:,2), . . .), O:,T), . . .
X----------------------------V-------------------}
S：,T
, O:n-T), O:,n-T+1),...), O:,n) .
fW (. . . fW (fW (S:,n-T -1
4
Published as a conference paper at ICLR 2020
(aftebuffdate) {st-n, ot-n+1, yt-n+1,…Si-T, oi-t+1, yi—1+1,…,si, oi + 1, yi+1 …,st}
Figure 1: A single update by FPP. It randomly samples i, and performs a gradient descent update to si-T , si , W
and β, where the loss on the targets affects si-T , W, β and the loss producing the next state variable si affects
si-T, si, W. The state variables are stored in the buffer, but are explicit variables we learn, just like W and β.
For T = 1, we recover the operator provided in (3). This generalization mimics the use of T -step
methods for learning value functions in reinforcement learning. This generalization provides more
flexibility in using the allocated computation per step. For example, for a budget of T updates per
step, we could use T 1-step updates, T/2 2-step updates, all the way up to one T -step update.
The loss for general T similarly decomposes into a sum .-T+ι Pn=T Li(β, W, S) for
Li(β, W, S) = 'β(Si(Si-T, W); %) + 2kχ - Si(Si-T W)k2	(6)
Where Si(Si-T, W) d=f fW (…fW(fW (Si-T-1, oi-T ), oi-T +1),..J, oi).
For each stochastic sample i, RLi is only non-zero for β, W, Si-T and Si. Though these updates
can simply be computed using automatic differentiation on Li , the explicit updates are simple so We
include them here, using shorthand Si for Si(Si-T, W):
Rsi-TLi = [Vsi'β(Si; yi) - λ(Si - Si)]>Vsi-τSi
▽*Li = λ(Si - Si)
VWLi = [Vsi'β(Si; yi) - λ(Si - Si)]>VwSi	(7)
Ve Li = Ve 'β (Si； yi)
The online algorithm uses these updates on a sliding WindoW buffer, instead of a fixed buffer. This
algorithm—called Fixed Point Propagation (FPP)— is summarized in Figure 1 and Algorithm 1.
As alluded to, the advantage of FPP over T-BPTT is that We are not restricted to focusing all
computation to estimate the gradient T -steps back in time for one state-observation pair. Rather,
instead of sWeeping all the Way back, We spread value by using updates on random transitions in the
buffer. This has three advantages. First, it updates more states per step, including updates toWards
their targets. Second, this ensures that targets for older transitions are constantly being reinforced,
and spends gradient computation resources toWards this goal, rather than spending all computation
on computing a more exact gradient for the recent time step. This distributes updates better across
time, and should likely also result in a more stable state. Third, the formulation as stochastic gradient
descent on the fixed point objective makes it a sound strategy—as opposed to truncation Which is not
sound. FPP, therefore, maintains the simplicity of T-BPTT, but provides a more viable direction to
obtain sound algorithms for training RNNs.
4 Convergence Results
In this section We shoW tWo theoretical results. First, We shoW that the FPP algorithm converges to a
stationary point, for a fixed buffer. This result is a relatively straightforWard application of recent
theory for nonconvex optimization (Ghadimi et al., 2016), mainly requiring us to shoW that our
5
Published as a conference paper at ICLR 2020
Algorithm 1 Fixed Point Propagation (FPP)
Input: a truncation parameter T, mini-batch size B, and number of updates per step M
Initialize weights W and β randomly
Initialize state so J 0 ∈ Rd
Initialize an empty buffer B of size N
for t J 1, 2, ... do
if B is full then
Remove the oldest transition
end if
Observe ot, yt, and compute st = fW(st-1,ot)
Add (st,ot,yt) to buffer B
if t ≥ T then
for j J 1,…，M do	. Multiple updates
Sample a mini-batch of size B, of trajectories of length T , from the buffer:
{(sil-T, oil-T, . . . , sil , oil , yil)}lB=1 where il is the index of the l-th mini-batch
Compute the average mini-batch loss and update {sil -T, sil }lB=1, W and β
Update {sil -T, sil }lB=1 in the buffer
end for
end if
end for
algorithm can be written as an instance of that framework and to show that each stochastic gradient
update is unbiased. This convergence result, nonetheless, is key, as it suggests that FPP is a sound
strategy for using replay with RNNs. Previous attempts to use replay for RNNs, in reinforcement
learning, were not able to show convergence (Kapturowski et al., 2019), which is to be expected as
truncated BPTT updates on a buffer may not be sound.
Additionally, we show that as λ approaches infinity, the set of stationary points of the FPP objective
approaches the set of stationary points for the RNN objective. In our experiments, we use λ = 1,
as obtaining precisely the same solutions as the RNN objective is not our goal. We include this
theoretical result nonetheless for completeness to characterize the relationship between the stationary
points of FPP objective and the RNN objective. The proof is similar to that for MAC-QP (Carreira-
Perpindn and Wang, 2014), with the main novelty in checking the KKT conditions for our objective
and for linear independence in the Jacobian. Full proofs for both results are in Appendix A.
4.1 Convergence of FPP to a Stationary Point for a Fixed Buffer
Recent work uses the idea of randomized gradient descent to show convergence to a stationary point
for nonconvex objectives (Ghadimi et al., 2016), as opposed to typical restrictions such as convexity
or the PL condition (Karimi et al., 2016). The randomized approach uses a random stopping time
R, and characterizes the norm of the expected gradient for the variables at this random time. The
variables we learn are (W, β, S) ∈ Rz, where z is the appropriate dimension.
For the proof we also require the variables to remain in a closed, convex set, to ensure that our
objective is Lipschitz. To do so, we will analyze our update with the addition of a projection operator
onto a closed ball C in Rz of radius r > 0 about the origin. r can be very large, and we emphasize
that C is only a convenience used for theoretical analysis. In practice, we do not project our iterates.
Since Rd is a Hilbert space and C is closed and convex, we have the existence of a unique projection
operator Γ
Γ(W0, β0, S0) d=ef arg min k(W0, β0, S0) - (W, β, S)k2.	(8)
(W,β,S)∈C
Our objective is L(W, β, S) =ef 冗_；+1 Pn=T Li (W, β, S), for Li defined in Equation (6), for
n > T samples. Each time we perform an update, we randomly sample k 〜uniform-(T, n),
inclusive of both endpoints. The update to parameters at time t, for stepsize αt, is
(Wt+1, βt+ι, St+ι) =f Γ((Wt, βt, St) - aVLkt (Wt, βt, St)).	(9)
6
Published as a conference paper at ICLR 2020
Theorem 1.	Let D be a Lipschitz constant of VL(W, β, S). Define probability mass functions
PN (k) :
ak 一 Dak
PN=1 aj - Daj
for each N ∈ N. Let R be distributed according to PN. Assume at =力 for all t and that we
perform N stochastic updates. Write xR = (WR, βR, SR). Then
12
E ∖Rk kr(aRVL(XR川2
4.2 Recovering RNN Solutions
Consider the standard RNN problem,
n
min E(W, β, SO)	for E(W, β, SO)=f X 'β(fW(…fW(ZW(S0, OI), o2), ∙∙∙ , Oi); yi) (IO)
β,W,s0
i=1
where we also optimize over sO. Our goal is to show that for increasing λ, the set of stationary points
of the FPP objective in Equation (5) approach stationary points of the RNN objective in Equation
(10). We assume T = 1 in our analysis of FPP.
Theorem 2.	Assume we have a positive, increasing sequence {λk} → ∞, a non-negative sequence
{k} → 0, and a sequence of points {(Wk, βk, Sk)} such thatkVL(Wk, βk, Sk); λk)k ≤ k for
nλ
L(Wk, βk, Sk ); λk ) = n J2'β (ZW(SiT, Oi)； yi) + ɪ Ilsi - fW(Si-1, Oi)Il 2	(II)
Assume further that {(Wk, βk, Sk)} has a convergent subsequence {(Wki, βki, Ski )} with limit
(W*, β*, S*). Then (W*, β*, S*) is a KKTpoint ofthe constrained FPP objective (see (12)) and
(w*, β*, SO) is a KKT point ofthe RNN objective (10). Further, if (W*, β*, S*) is a local min of
the constrained FPP objective, then (W*, β*, Sq) is a local min of (10).
5	Experimental Results
We designed a sequence of experiments in real and synthetic problems to evaluate our new method
compared with several common baselines and to highlight the robustness of our method to different
truncation lengths, buffer sizes and number of updates. In particular we compare (1) against T-BPTT
with a variety of truncation lengths greater and lesser than the temporal delay required to solve each
problem; (2) No-Overlap T-BPTT, a common a variant of T-BPTT that updates on disjoint partitions
of the data; and (3) FPP without the state update, which is similar to the Stored State T-BPTT
algorithm (Kapturowski et al., 2019). We begin by describing the problems we used to evaluate our
methods, and why they were chosen. Unless otherwise stated, we report average performance over
all training steps (online performance), averaged over 30 independent runs.
Simulation Problems We used two small simulation problems to highlight the robustness of each
method to increasing temporal delay in online training. The first tasks is a simple ring of states, called
Cycle World. On each timestep the agent deterministically transitions to the next state in the chain.
The agent’s observation is zero in every state, except the last. The agent’s objective is to predict
the next observation, which is difficult without a memory of length equal to the length of the cycle.
With a shorter memory, the agent cannot tell when the last non-zero observation, which is essential
for predicting the next observation. This task has been used exclusively in benchmarking k-Markov
methods, POMDPs, and predictive state representations (Tanner and Sutton, 2005). The complexity
of the task can be easily varied, and yet the determinism ensures the variance does not introduce
confounding factors. At each time step, we measure the prediction accuracy for the next observation.
We also experimented with a stochastic prediction task, where correct prediction requires remem-
bering two independent observation’s from the past. In particular, the target on the next timestep is
probabilistically dependent on the one-dimensional observation 15 timesteps ago and 30 timesteps
ago. The dynamics are summarised in Table 1, in Appendix C. This task is called Stochastic World.
7
Published as a conference paper at ICLR 2020
For this problem, a cross-entropy loss of 0.66 or higher indicates that the learned state did not capture
the observation from either 15 or 30 steps in the past. If the state captures the observation from 15
time-steps ago the cross entropy loss is about 0.51. Optimal performance in this problem results in a
cross-entropy loss is about 0.46. Like Cycle World, Stochastic World requires a long and detailed
memory of past observations, but the stochastic nature of the target pose an additional challenge.
Real DataSets We also performed experiments on two fixed datasets, to gain insights into how
each method performed on better known benchmark tasks. In both cases the data was processed and
performance evaluated in an online fashion. The first problem is Sequential MNIST. The objective is
to classify numerical digits based on a stream of pixel inputs. On each timestep the input is one row
(1x28) of the image, and the target is the label of the image. We used an RNN architecture with 512
hidden units as in previous work (Arjovsky et al., 2015). It is not possible to predict the target image
base on a few samples, so we wait until 15 steps (corresponding to 15x28 pixels) to begin measuring
the error. Here, we report these incorrect predictions for the last 15 time-steps for every image. We
ran this on 1000 images, which correspond to 28000 steps.
Finally, we also include results on a a character prediction problem called Penn Tree Bank dataset.
This problem is relevant because language modelling remains an important application of recurrent
learning systems, and robust performance on this dataset can provide insight into the utility of our
new method in application. We used a vocabulary size of 10000. The Target Loss function used here
is a weighted cross-entropy loss for a sequence of logits. We used an LSTM with 200 hidden nodes
as this architecture was found to perform well in previous work (Zaremba et al., 2014).
Comparison to T-BPTT We compare FPP to T-BPTT for varying truncation levels. For all the
algorithms, we used a constant buffer size of 100 and the trajectory length T for both T-BPTT(overlap
and no overlap versions) and FPP. All algorithms use O(T ) computation per step. For overlap
T-BPTT, we employ T-BPTT online by taking each observation and updating with respect to the loss
at that time-step, using a T-step truncated gradient. The no-overlap version of T-BPTT performs a
batch update for every T observations, such that these observations do not overlap.
Additionally, we include UORO (Tallec and Ollivier, 2017) as another baseline. UORO uses an
unbiased rank-1 approximation to approximate RTRL. It is a relatively new method for training RNN
online, and has only been tested on small scale datasets. In our experiment, we include memory-1
and rank-1 UORO in Cycleworld and StochasticWorld.
We first compare the performances of FPP and T-BPTT on Cycleworld with varying p. We expect
T-BPTT to degrade with T less than the dependence back in time (the length of the cycle p); we
therefore test both T = p and T = p/2 for increasing p. To make the results comparable across p, we
report performance as the ratio to a simple baseline of predicting 0 at every time step. From Figure 2,
we can see that FPP is more robust to T , whereas T-BPTT with T = p/2 performs poorly even when
given more data (Figure 2(b)). In early learning, with fewer samples, FPP has an even more clear
advantage. Even though T-BPTT can eventually learn optimal predictions for T = p, it takes longer
than FPP which learns near optimal predictions in early learning (Figure 2(a)).
We additionally compare FPP and the two variants of T-BPTT across all four problems, under different
settings of T, shown in Figure 3. Across all problems, FPP outperforms the other two for every T,
except T = 1 in CycleWorld where all three methods perform similarly. The performance of FPP is
notably better for smaller T , as compared to T-BPTT. For example, in Figure 3(b) 20-BPTT has a
high loss and is unable to learn both the dependencies, whereas FPP with T=20, performs almost as
well as 40-BPTT. Similar conclusions can be made for T ∈ {3, 5} in (a), T ∈ {10, 15, 20, 30} in (b),
T ∈ {7,14,21,28}in(c)andT ∈ {1, 5, 10, 20} in (d).
Benefits of mini-batch updates and multiple updates per step One of the advantages of using
a buffer is the ability to perform mini-batch updates and multiple updates per step. We evaluate
the performance of FPP with and without state updates using M updates per step and a mini-batch
of size B. We show the performance with varying T . To show the effect of multiple update, we
fix B and vary M ∈ {1, 2, 4, 8, 16}. To show the effect of mini-batch update, we fix M and vary
B ∈ {1, 2, 4, 8, 16}. We use a buffer size of 1000 and 10000 training steps.
We also include FPP without state updating, to determine if the benefits of FPP are mainly due to
using a buffer rather than due to the new objective to learn explicit state variables. We particularly
expect FPP to outperform FPP without state updating under more updates per step, because we
showed converge for FPP on a fixed buffer whereas no such result exists for FPP without state
8
Published as a conference paper at ICLR 2020
P-CycleWorld
1.0-
0.8-
cRati°	0.6-
Error With
respect
to 0.4-
Baseline
0.2-
0.0-
p=4	p=8	p=12	p=16 p=2O
Cycle Length
(a)	Early Learning (2500 steps)
(b)	Learning with More Data (15000 steps)
Figure 2: The ratio error of each of the algorithms with respect to the baseline of predicting 0 at every time step
is our measure of performance. For all the values of p, FPP seems to be more robust to T, especially with larger
p. The numbers are average over 30 runs with standard error bars.
(a) CycleWorld
(b) StochasticWorld
(c) Sequential MNIST
(d) Penn-Tree Bank
Figure 3: Average online performance for FPP (red), T-BPTT (orange) and No-Overlap T-BPTT (blue). Across
all the domains, FPP seems to be more robust to T, and it does much better than T-BPTT especially for small T.
The numbers are average over 30 runs with one standard error with (a) being run for 5000 steps, (b) for 10000
steps, (c) for 1000 images (28000 steps) and (d) for 5000 steps (5000 points in dataset, processed in order). FPP
at T = 20, 30, 40 reaches a final solution with optimal performance; it is only above the second line because the
plot shows average performance across all steps, rather than final performance.
updating. Here, the buffer is not fixed, but performing more updates per step should move the FPP
solution closer to a stationary point of the current buffer.
Figure 4 (a) and (b) shows the effect of multiple updates and (c) and (d) the effect of mini-batch
updates. For both, increasing the number of updates and the size of the mini-batch improves
performance, except for a bit of overfitting we observed in Stochastic World for increasing updates
(B = 1, M = 16). However, in general, FPP can better take advantage of both multiple updates
and mini-batch updating. The most noticeable gaps are for T = 16 and T = 32 in StochasticWorld
and T = 1 and T = 2 in CycleWorld. The theory suggests that more updates, even with T = 1,
should allow FPP to converge to a reasonable solution. We test this on CycleWorld (with Figure 7 in
Appendix C), and find that for both larger mini-batch and number of updates FPP can get the error
down to zero, whereas FPP without state updating cannot.
9
Published as a conference paper at ICLR 2020
M
B
(a) Multiple Updates in CyCleWorld (B = 1)
Figure 4: The performanCe for inCrease number of updates (with mini-batCh of B = 1) and inCreasing mini-batCh
size (with number of updates M = 1). The numbers are average over 30 runs with 10000 training steps. The
solid line is FPP and the dashed line is FPP without state updating.
(c) Mini-batch Updates in CycleWorld (M = 1)
(d) Mini-batCh Updates in StoChastiCWorld (M = 1)
6	Conclusion
The main objeCtive of this paper is to reformulate RNN training to expliCitly learn state variables. In
partiCular, the goal is to investigate methods that Can better distribute Computation, and improve state
updating without having to Compute expensive—and potentially unstable—gradients baCk in time for
eaCh state. We introduCe a new objeCtive to expliCitly learn state variables for RNNs, whiCh breaks
gradient dependenCe baCk in time. The ChoiCe of T to Compute gradients baCk in time is used only to
improve training speed, rather than to effeCtively approximate gradients. We found that our algorithm,
Called FPP, was indeed more robust to T , than trunCated BPTT was to its trunCation level. We proved
that our algorithm Converges to a stationary point, under a fixed buffer, and so is a sound approaCh
to using a buffer to train RNNs. Further, we Chose simple optimization ChoiCes in this work; there
are Clear next steps for benefiting more from the deCoupled update, suCh as by parallelizing updates
aCross state variables. Overall, this work provides evidenCe that FPP Could be a promising direCtion
for robustly training RNNs, without the need to Compute or approximate long gradients baCk in time.
References
Luis B Almeida. A learning rule for asynChronous perCeptrons with feedbaCk in a Combinatorial
environment. In International Conference on Neural Networks, 1987.
MarHn Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.
CoRR, abs/1511.06464, 2015.
Dimitri P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Academic Press,
1982.
Victor Campos, Brendan Jou, Xavier Gir6 i Nieto, Jordi Torres, and Shih-Fu Chang. Skip RNN:
learning to skip state updates in recurrent neural networks. CoRR, abs/1708.06834, 2017.
Miguel A Carreira-Perpindn and Weiran Wang. Distributed optimization of deeply nested systems.
In International Conference on Artificial Intelligence and Statistics, 2014.
10
Published as a conference paper at ICLR 2020
W. Chan, N. Jaitly, Q. Le, and O. Vinyals. Listen, attend and spell: A neural network for large
vocabulary conversational speech recognition. In IEEE International Conference on Acoustics,
Speech and Signal Processing, 2016.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael J.
Witbrock, Mark Hasegawa-Johnson, and Thomas S. Huang. Dilated recurrent neural networks.
CoRR, abs/1710.02224, 2017.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.
CoRR, abs/1609.01704, 2016.
Wojciech Marian Czarnecki, Max Jaderberg, Simon Osindero, Oriol Vinyals, and Koray Kavukcuoglu.
Understanding Synthetic Gradients and Decoupled Neural Interfaces. arXiv:1411.4000v2, 2017.
SiegmUnd Dull, Steffen UdlUfL and Volkmar Sterzing. Solving partially observable reinforcement
learning problems with recurrent neural networks. In Neural Networks: Tricks of the Trade, 2012.
Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179-211, 1990.
Saeed Ghadimi, GUanghUi Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods
for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):267-305,
2016.
Akhilesh Gotmare, Valentin Thomas, Johanni Brea, and Martin Jaggi. Decoupling Backpropagation
using Constrained Optimization Methods. 2018.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep
recurrent neural networks. CoRR, abs/1303.5778, 2013.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82-97, Nov
2012.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997.
J J Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the National Academy of Sciences, 79(8):2554-2558, 1982.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David
Silver, and Koray Kavukcuoglu. Decoupled Neural Interfaces using Synthetic Gradients. In
International Conference on Machine Learning, 2017.
Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent
experience replay in distributed reinforcement learning. In International Conference on Learning
Representations, 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojaSieWiCz condition. In European Conference on Machine
Learning and Knowledge Discovery in Databases. Springer, 2016.
Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Laurent Charlin, Chris Pal, and
Yoshua Bengio. Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent
NetWorks. arXiv:1509.01240v2, 2017.
Jan Koutnk, Klaus Greff, Faustino J. Gomez, and Jurgen Schmidhuber. A clockwork RNN. CoRR,
abs/1402.3511, 2014.
Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urtasun,
and Richard S Zemel. Reviving and Improving Recurrent Back-Propagation. In International
Conference on Machine Learning, 2018.
Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive
attention via A visual sentinel for image captioning. CoRR, abs/1612.01887, 2016.
11
Published as a conference paper at ICLR 2020
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille. Explain images with multimodal
recurrent neural networks. CoRR, abs/1410.1090, 2014.
Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo,
Aaron C. Courville, and Yoshua Bengio. Samplernn: An unconditional end-to-end neural audio
generation model. CoRR, abs/1612.07837, 2016.
Yajie Miao, Mohammad Gowayyed, and Florian Metze. Eesen: End-to-end speech recognition using
deep rnn models and wfst-based decoding. CoRR, abs/1507.08240, 2015.
Asier Mujika, Florian Meier, and Angelika Steger. Approximating real-time recurrent learning
with random kronecker factors. In Advances in Neural Information Processing Systems, pages
6594-6603, 2018.
James M Murray. Local online learning in recurrent networks with random feedback. eLife, 8:e43299,
2019.
Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased LSTM: accelerating recurrent network
training for long or event-based sequences. CoRR, abs/1610.09513, 2016.
Yann Ollivier and Guillaume Charpiat. Training recurrent networks online without backtracking.
arXiv, 2015.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International Conference on Machine Learning, 2013.
B. A. Pearlmutter. Gradient calculations for dynamic recurrent neural networks: a survey. IEEE
Transactions on Neural Networks, 1995.
Fernando J Pineda. Generalization of back-propagation to recurrent neural networks. Physical review
letters, 1987.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Benjamin Scellier and Yoshua Bengio. Equilibrium Propagation: Bridging the Gap between Energy-
Based Models and Backpropagation. Frontiers in Computational Neuroscience, 2017.
Corentin Tallec and Yann Ollivier. Unbiased Online Recurrent Optimization. arXiv:1411.4000v2
[cs.LG], 2017.
Brian Tanner and Richard S. Sutton. Td(lambda) networks: Temporal-difference networks with
eligibility traces. 2005.
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
neural networks without gradients: A scalable admm approach. In International Conference on
Machine Learning, 2016.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. CoRR, abs/1411.4555, 2014.
P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE,
78(10):1550-1560, Oct 1990.
Ronald J. Williams and Jing Peng. An efficient gradient-based algorithm for on-line training of
recurrent network trajectories. Neural Computation, 1990.
R. J. Williams and D. Zipser. A learning algorithm for continually running fully recurrent neural
networks. Neural Computation, 1989.
Ronald J Williams and David Zipser. A Learning Algorithm for Continually Running Fully Recurrent
Neural Networks. Neural Computation, 1989.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
CoRR, abs/1409.2329, 2014.
12
Published as a conference paper at ICLR 2020
A	Full Proofs
A. 1 Convergence on a Fixed Buffer
At first glance, the update (9) is different than the update in Ghadimi et al. (2016, p. 276). Nevertheless,
the following lemma guarantees that they are indeed the same.
Lemma 1. Let f be L or a stochastic sample of L and let α > 0. Write x = (W, β, S). Then
arg min (〈▽/(x), Ui +	∣∣x — u∣∣2 1 = arg min {∣∣u — (X — aVf (x))k2 } =: Γ(x — aVf (x)).
u∈C	2α	u∈C
Proof. The proof is a straightforward calculation.
arg min {∣u — (X — aVf (x))∣2 } = arg min {∣u — x∣2 + α2∣Vf (x))∣2 + 2(u — x, αVf (x))}
u∈C	u∈C
= arg min ∣u — x∣2 + 2αhu, Vf (x)i
u∈C
=arg min {<Vf(x),u) + ； l∣x - UI2}
□
Our goal is to apply Corollary 3 of Ghadimi et al. (2016, p. 282). We must show that Vg is Lipschitz
on C and demonstrate that Assumption A1 in Ghadimi et al. (2016, p. 268) holds.
Lemma 2. VL is Lipschitz on C.
Proof. A function is Lipschitz if it gradient is bounded. Since L is smooth and C is compact
(continuous functions on compact sets are bounded), this lemma follows.	口
Lemma 3. VLk is an unbiased estimate of VL, where k 〜uniform-(T, n).
Proof. The terms in VLk corresponding to the gradients of W and β are exactly VWL and Vβ L in
expectation, given that k 〜uniform-(T, n).
Let us consider the gradient elements corresponding to the parameters s0:n. For shorthand, define
[a : b] := {a, a + 1,…，b — 1,b}. Define P0 := [0 : n — T],Pι := [T : n]. If j ∈ Pq, then Sj
predicts future states. If sj ∈ P1 , then sj is predicted by other states in the regularizer terms of L.
Note that P0 and P1 are not disjoint. First, we calculate.
n-T +1 (Vsj+t'β(Sj+T; yj+T) — λ(Sj+T — sj+T))>VSjSj if j ∈ P0 ∩ P{
fɪ λ(Sj - Sj) if j ∈ PO ∩ Pi
Vsj L(W, β, S) := < n-T+1 [λ(sj — sj ) + (Vsj+t 'β (sj+T; yj+T ) — λ(sj+T - sj+T ))>Vsj sj]
if j ∈ Po ∩ Pi
、0 if j ∈ P{ ∩ P{
If j ∈ Pq ∩ P{, then sj does not show up as the target (i.e., the term that is not sk) in any regularizer
term of L. Hence, VsjLk is zero with probability 1 —九二十],and is (Vsj∙+τ'β(sj+T; yj+T) —
λ(sj+T — sj+T))>Vsjsj with probability n-T+τ.
If j ∈ Pq{ ∩ Pi, then sj only shows up as a target in a regularizer term, so Vsj Lk is zero with
probability 1 — n-T+ι and is otherwise 冗-；+] λ(sj — sj).
If j ∈ Pq ∩ Pi, then Vsj Lk is zero with probability 1 —冗-τ+], λ(sj — sj) with probability 冗-；+],
and (Vsj+τ'β(sj+T; yj+T) — λ(sj+T — sj+T))>Vsj∙sj with probability n-T+τ.
The case for j ∈ P(O ∩ P{ is trivial. Consequently, E[Vsj∙ Lk] = Vsj L for all j ∈ {0, •…，n}.
□
13
Published as a conference paper at ICLR 2020
Lemma 4. The variance of VLk is bounded on C.
Proof. This follows because VLk and VL are both smooth functions on a compact set C, and thus
bounded.	□
Theorem 1. Let D be a Lipschitz constant of VL(W, β, S). Define probability mass functions
PN (k) :
ak 一 Dak
PN=1 j Daj
for each N ∈ N. Let R be distributed according to PN. Assume at =力 for all t and that we
perform N stochastic updates. Write xR = (WR, βR, SR). Then
1
E ∖Rk EaRVL(XR川2
Proof. The gX,R (defined in Ghadimi et al. (2016, p. 271, 274)) in Corollary 3 of Ghadimi et al.
(2016, p. 282) corresponds in our case to the following.
gx,R ：= Or 卜R — arg min {〈Vf(x),u)+ 2^ l∣x
= (xR 一 γ(XR — aRVL(XR)))∙
aR
In the last line, we use Lemma 1. Since we project based on squared norm distance in (8) (corre-
spending to ω(x) = ɪ ∣∣x∣2 in Ghadimi et al. (2016)), the α in Ghadimi et al. (2016, p. 271) (not our
step-size at) can be set to 1.
After applying our Lemma 2, Lemma 3, and Lemma 4, we have from Corollary 3 of Ghadimi et al.
(2016, p. 282) that
12
E [or kr(XR -αRVL(XR))-XRk
The only thing left to check is that Γ(XR 一 aRVL(XR)) 一 XR = Γ(VL(XR)).
Γ(xr - OrVL(Xr)) — Xr = arg min {∣u — (xr — orVL(xr))∣∣2} — XR
u∈C
= arg min {lu — VL(XR))l2}
u∈C
= Γ(aRVL(XR)).
□
A.2 Recovery of RNN Solutions
Recall our goal is to compare to the RNN solutions of (10).
n
Ε(W, β, so) := X'β(fw(…fw(fw(so, oι), 02),…，Oi)； yi)	(10 revisited)
i=1
min E(W, β, so),
β,w,s0
Let us also write a constrained version of the above problem, which we will use in the analysis of
FPP.
n
Efpp(W, β, so,…，Sn) ：= f'β (fw(si-ι, Oi )； yi)	(12)
i=1
s.t. ∀1 ≤ i ≤ n,fw(si-1,Oi) = si
min Efpp(W,β, so,…，Sn)
W,βS0-.n …
14
Published as a conference paper at ICLR 2020
The idea is that FPP can be viewed as a way to solve the problem (12) and thus (10) through quadratic
regularization.
We will use so：n as shorthand for {so,…，Sn}, which in the main paper We labeled as S, but for this
proof it will be convenient to use explicit variables. Define the feasible set of (12) as
Ω := {(W, β, so:n) : W ∈ Rw; Si ∈ Rk; β ∈ Rb; ∀ 1 ≤ i ≤ n, Si = fw(s-, 5)}.
Proposition 1. Let (W*, β*, s0) be a local min of (10). For 1 ≤ i ≤ n, define recursively
s* := fw*(si-ι, Oi). Then (W*, β*, s£) is a local min of (12).
Let (W*, β*, s0：n) be a local min of (12). Then (W*, β*, s0) is a local min of (10).
Proof. First, let N ⊂ Rw+b+k be a neighbourhood of (W*, β*, s0) such that ∀(W, β, so) ∈ N, we
have
E(W*, β*, so*) ≤E(W,β,so).
Without loss of generality, we may take N to be open. Otherwise, by definition of a neighbourhood,
we may take a smaller open set around (W*, β*, so*) by definition of a neighbourhood and call that
set N.
Let si* be defined as above. Define M := N × Rnk, which is an open neighbourhood of (W*, β*, so*)
since N is open. Let (W, β, so：n) ∈ M ∩ Ω. Note that (W, β, so) ∈ N. By definition of Ω, we
have that fW(si-1,oi) = si. Hence, Efpp(W, β, so:n) = E(W, β, so).
By definition of (12), (10), we have E(W*, β*, s*o) = Efpp(W*, β*, so*:n). Finally,
Efpp(W*, β*, so*:n) =E(W*,β*,so*) ≤ E(W, β, so) = Efpp(W, β, so:n).
For the second part of the proof, assume (W*, β*, so*:n) is a local min of (12), meaning there is a
neigbourhood M ⊂ Rw+b+(n+1)k of (W*, β*, s*：n) such that for every (W, β, so：n) ∈ M ∩ Ω,
Efpp(W*, β*, sθ:n) ≤ Efpp(W, β, so：n).
Similarly, without loss of generality, we can assume that M is an open ball, so we may write for
some > 0, M = B(W*, β*, s*o:n).
We will construct an open set N ⊂ Rw+b+k such that (W*, β*, so*) is a local min with respect to N.
Define the projection π onto the first W + b + k indices. Define N := π(M ∩ Ω). Let US show that
N is open.
We will write fw(so：n-i, oi：n) to mean {fw(so, oι),…，fw(fw(…(so, oι), 02),…，On))}.
We can write N as
N = {(W, β, so) : (W, β, so:n) ∈ Ω ∩ Be(W*, β*, s*：n)}
={(W,β,so) : (W, β, so, fw(so:n-1, o1:n)) ∈ B(W*, β*, s*o:n)}
={(W,β,so) : k(W, β, so, fw(so:n-1, o1:n)) - (W*, β*, so*:n)k <}
On the second line, we used the fact that Si = fw(si-ι, Oi) in Ω. Since the norm and f are
continuous and (-∞, ) is open, we have that N, a continuous preimage of an open set, is open.
Now, let (W, β, so) ∈ N such that ∃ si：n with (W, β, so：n) ∈ M ∩ Ω.
E(W*,β*,s*) = Efpp(W*, β*,so：n) ≤ Efpp(W,β,so:n) = E(W, β,so)
The claim follows.	□
Proposition 2. The first order KKT equations for (10) and for (12) are the same.
Proof. Given (W*, β*, s*), for 1 ≤ i ≤ n define Si := fw (每―1, Oi), where so := s*. If we write
dfw*dS；Ol+1)for instance, this is taken to mean the gradient of fw*(Sι, 01+1) with respect to the
function arguments corresponding to si. Furthermore, when writing dfw*∂Woj+1), We only mean
the gradient with respect to the parameters of the outer fw* , and not with respect to any of the
parameters of ssj .
15
Published as a conference paper at ICLR 2020
Using the chain rule for the first and third equations below, the first order KKT conditions for (10)
are given by
∂E(W*, β∖ Sd) _ ∂Q(fw*(s0, oι); yι) ∂fw*(sgb oι)
∂W
dfw
∂W
(13)
+
n-1
X
j=1
d'β*(fw*(Sj, oj+ι); yj+ι)
dfw
∂E(W*,用 s0)
fw*® oj + 1) +
∂W	+
jj
Xπ
i=1 l=i
dfW* (SI, ol + 1)∖ dfw*(si-1, Oi)
∂sι
∂W
0
n
X
i=1
d'β* (fw* (Si-L 5); yi)
(14)
∂E(W*, β*, s0) _ ( ∂%* (fw*(s0,。1)；町)
∂S0
dfw
+(X
d'β* (fw* (Si, 5+1)； Ji+1)
dfw
(15)
i
π
l = 1
0
dfw*(sι, ol+1)
∂ Sι
))
dfw* (s0, o1)
∂ S0
∂ β
∂ β
0
The Lagrangian for (12) is
n
LfPP(W, β, S0：n) = E'β (fw(Si-1, oi); yi) - λτ (fw (Si-1, Oi)-Si),
i=1
(16)
where λi ∈ Rk for 1 ≤ i ≤ n are Lagrange multipliers. We define λ0 := 0 for convenience. The
KKT equations for (16) are
∂L⅛(W*, β*, Sdn)
∂W
n
X
i=1
∂'β* (fw* (si-1, oi); yi) ∂fw* (si-1, oi)
∂fw
fw* (Si-1, Oi)
∂W	=
∂W
(17)
∂L"3W,β,. S0,J
∂β
n
X
i=1
∂β
oi); yi) = O
∂L⅛(W*, β, S0n)
λT	if j = n
d'β* (fW* (s* ,oj + 1 )；yj+1 )
∂fw
∖T ʌ dfw* (s*,oj+1)
λj+1J	西
(18)
	
^^(^^^-1,
	
I if 0 ≤ j<n
=0
Si = fw* (si-1, oi),	V1 ≤ i ≤ n
First, let us find a closed-form expression for λi.
Lemma 1. Let 0 ≤ j ≤ n. Then
	
n-1
X
i=j
%β* (fw* (Si, oi+ι); yi+ι)
i
π
l=j
∂fw* (s； , oι + ι)
∂Sι
,fw
Proof. We proceed by induction. The base case and the case j = n - 1 are trivial. Assume the claim
is true for m + 1 > 0. We will show the claim for j = m. Using the KKT equations (17) and the
induction hypothesis,
16
Published as a conference paper at ICLR 2020
d'β* (fW*(sm, θm+1); Um+。	λT 、d∕w*(8m, om+1)
∂fW	m+ιj	∂sm
f∖w"sm ∙ om「I： Um
dfW
n—1
+ X
i=m+1
d'β* (fw* (s^, Oi+1)； Ui+1)
dfw
i
π
l=m+1
dfw* (Si , ol+1) ʌ d fW* (Sm) Om+1)
∂Sl
∂Sm
d'β* (fW* (Sm) om+1); Um+1) dfW* (Sm) om+1)
∂fW	∂Sm
n—1
X
i=m+1
d'β*(fW* (S, Oi+1)； Ui+1)
dfW
i
π
l=m+1
dfW* (S , Ol + 1) dfW* (Sm) om+1)
∂Sι	∂ Sm
	
gd'β* (fW* 闾,Oi+1)； Ui+1)
∂fW
i
π
l=m
dfW* (S, ol+1)
∂Sι
□
Now, We will show that the sets of equations are the same. First, it is clear that the two equations
involving gradients of β in (13) and (17) are the same given that the constraint must be satisfied in
(17). Now consider the equations involving gradients with respect to W.
∂Lfpp _ X ∂'β*(fW*(S二 1)oi); yi) dfW*®—1)oi)
∂W - = f	∂W	+
n n—1
X(X
i=1 ∖j=i
d'β*(fW* (Sj) oj+1); yj+1)
dfW
j
π
l=i
∂fW* (Sj) Oι + 1)
∂Sι
dfW* (Si-I, Oi)
∂W
n
X
j=1
d'β*(fW*(sj-1
dfW
jj dfW*(SjT) 0j)+
∂W
n—1
X
j=1
XX d'β*(fW*(sj) oj+1); yj+1)
3	fW
j
π
ι=i
∂fW* (s∣ ) oι+1)
∂ Sι
dfW*(si-1)Oi)
∂W
∂'β* (fW*(s0)。1)； U1) ∂fW*(s0)。1)	X d'β*(fW*(sj, Oj+1)； yj+1)
∂fW	∂W — + ⅛	∂fW
dfW* (Sj) oj + 1)
∂W	+
jj
X π
∂fW* (sj) oι + 1)
∂Sι
∂fW*(si-1) oi)
∂W
0.
By substituting in the constraint equations Si = fW* 闾—「oj, this recovers exactly the gradient
with respect to W in (13).
17
Published as a conference paper at ICLR 2020
Finally, consider the gradient with respect to s0 .
∂Lfpp(W*, β, S0：n)二 ∂S0	λT . d d'β* (fW* (S0, o1); yI)	λτλ dfW*(s0, OI) =λ0 + (	∂fW	λi) 一∂S0一 _ dd'β* (Zw*(S0, oi);yi) + nnX d'β*(fw*(S0,oi+i); yi+ι) 一[	∂fW	+ S	∂fW YY dfW* (S0, ol+i) ! ! dfW* (S0, oi) 出一国—D -菽一 0.
This matches the corresponding equation in (13).	□
Proposition 3. Let (W*, β*, s£) be a local min of (12). Write the constraints of (12) as a vector:
h(W, S0:n) := [[(fw(S0, Ol) - Si )T …(fw (Sn-1, On) — Sn)T]	(19)
Index each element of h by hi. Then the vectors Vhi(W*, s6：n) are linearly independent.
Proof. In the following, We will write ∣[gil to mean the derivative of the l-th component of g with
respect to the i-th component of Xj. For compactness, write g(i) := fw* (s*-ι, Oi) - s*. We can
write the Jacobian Vh(W*, s£) as
	∣"∂[g(1)]ι 	:	 . ∂W1 .	∂[g(1)]ι • —		 ∂Ww ..	∂[g(1)]ι	. ∂s1 .	∂[g(1)]ι •	~~∂^ ..	∂[g(i)]ι^∣ dsn .
	. . ∂[g(1)]k	.. .. ∂[g(1)]k	. . ∂[g(1)]k	.	.. .. .∂[g(1)]k	. . ∂[g(1)]k
Vh(W0,S00:n)=	∂W1	• a[g(2)]ι	•	∂Ww d[g(2)]i	∂s1 ∂[g(21)]ι	^	∂ Sk •	ME)]] ••	dsn d[g(2)]1
				--	 . ∂W1 .	• —		 ∂Ww ..	∂s1 .	∂s'k ..	dsn .
	. . d[g(n)]k	.. .. d[g(n)]k	. . d[g(n)]k	.	.. .. .d[g(n)]k..	. . a[g(n)]k
	—-—~~—	. ∂W1	•		T	 ∂Ww	∂s1	∂s1	dsn _
We will show that the rows of Vh(W*, s0：n are linearly independent. To this end, let λj ∈ R for
i ∈ {1, ∙∙∙ ,n}, j ∈ {1, ∙∙∙ ,k} be such that:
kn
XX λij V[fw*(s0-1, Oi)-S0]j=0.
j=1 i=1
In particular, for 1 ≤ a ≤ n, 1 ≤ b ≤ k,
kn
XXλij
j=1 i=1
dfW* (S0-1, Oi) - s0]j
dsa
1dfW* (S0-1, Oi)]j
∂Sba
- δai δbj
1a<n
k
λa+1,j
j=1
dfW* (Sa, θa+1)]j
dsa+ι
- λab
(20)
(21)
(22)
0
By setting a = n, we have that λnb = 0 for all 1 ≤ b ≤ k. Setting a = n - 1, we similarly have
that λn-1,b = 0. Proceeding in this fashion, we have that λab = 0 for all 1 ≤ a ≤ n, 1 ≤ b ≤ k.
Actually, we did not at any point use the fact that (W0, β0, S00:n) is a local min, so that the constraint
gradients are linearly independent everywhere, and in particular at (W0, β*, s0：n).	□
18
Published as a conference paper at ICLR 2020
Theorem 2. Assume we have a positive, increasing sequence {λk } → ∞, a non-negative sequence
{ek } → 0, and a sequence of points {(Wk, βk, Sk)} such that ∣∣VL(Wk, βk, Sk); λk)k ≤ e《for
nλ
L(Wk, βk, Sk ); λk ) = n ɪ^'e (ZW(Si-1, Oi); yi) + ɪ Ilsi- fW(si-1, Oi)Il 2	(II)
i=1
Assume further that {(Wk, βk, Sk)} has a convergent subsequence {(Wki, βki, Ski )} with limit
(W*, β*, S*). Then (W*, β*, S*) is a KKTpoint ofthe constrained FPP objective (see (12)) and
(w*, β*, s0) is a KKT point ofthe RNN objective (10). Further, if (W*, β*, S*) is a local min of
the constrained FPP objective, then (W*, β*, Ss) is a local min of (10).
Proof. By Proposition 3 and Proposition 2.3 from Bertsekas (1982), We have the existence of a
Lagrange multiplier vector λ such that
VEfpp (W*, β*, sS：n) - Vh(W*, sS/λ = 0,
h(Ws, S0s:n) = 0,
where h(Ws, S0s:n) is as in Proposition 3. Hence, (Ws, βs, S0s:n) is a KKT point of (12).
By Proposition 2, (Ws, β*, s0) is a KKT Pointfor (10). Finally, if (Ws, β*, s0：n) is a local min of
(12), then by Proposition 1 We have that (Ws, β*, sɔ) is a local min of (10).	□
B Parameter Study
We investigate the sensitivity of FPP to its tWo key parameters: the length of the trajectory T , and
the buffer size N . Overall, the losses on y-axis of Figure 5 shoW that FPP is robust to buffer size
and truncation length. As expected, for very small T, performance degrades, but otherWise the move
from T= 10 to T= 50 does not result in a large difference. The algorithm Was quite invariant to
buffer size, starting from a reasonable size of 100. For too large a buffer With a small number of
updates, performance did degrade someWhat. Overall, though, across this Wide range of settings, FPP
performed consistently Well.
Figure 5: Sensitivity to buffer length and trajectory length in FPP, for buffer sizes 100, 1000 and 10000 and
truncations of 1, 5,10,15 and 50.
(b) StochasticWorld
We also investigated hoW performance changes When changing λ. Throughout all previous exper-
iments, We simply set λ = 1, to avoid unfairly tuning our method to each problem. Interestingly,
tuning λ does enable further performance improvements, though the algorithm Worked Well for quite
a large range of λ.
C Experimental Details
The dynamics for the Stochastic World environment are in Table 1.
For all experiments, We use RMSprop optimizer and the learning rate is chosen over the set
{0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03} based on the average accuracy/loss. For real datasets,
We use multiple trajectories to speed up training. The details of each task are provided beloW:
19
Published as a conference paper at ICLR 2020
(a) 10-CycleWorld
Lambda
(b) StochasticWorld
Figure 6: Sensitivity of Lambda for various values of T. For small T, higher lambda works better suggesting the
impact of propagation of state values across the buffer.
P(Yt = 1∣θt-T1,θt-T2)	Ot-TI	Ot-T2
50%	-0-	0
100%	1	0
25%	0	1
75%	1	1
Table 1: The conditional probability of the target output given the past observations.
Figure 7: The performance of FPP and FPP without state updating with T = 1, B = 16 after 50000 training
steps, for varying M . This result highlights that FPP can better take advantage of more updates and larger
mini-batches, with its sound updating strategy on a buffer.
C.1 CycleWorld
Network Type = simple RNN
Hidden Units = 4
C.2 Stochastic World
Network Type = simple RNN
Hidden Units = 32
C.3 SEQUENTIAL MNIST
Network Type = simple RNN
Hidden Units = 512
Image Size = 784 pixels
Input Dimension = 28 pixels
Number of Steps = 28000 (1000 images of 28 steps)
20
Published as a conference paper at ICLR 2020
Number of Trajectories = 20
C.4 PTB
Network Type = LSTM
Hidden Units = 200
Vocabulary Size = 10000
Embedding Size = 200
Number of Steps = 5000 (5000 samples in the dataset)
Number of Trajectories = 20
21