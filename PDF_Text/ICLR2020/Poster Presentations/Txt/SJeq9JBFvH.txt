Published as a conference paper at ICLR 2020
Deep Probabilistic subsampling for
Task-adaptive Compressed Sensing
Iris A.M. Huijben*
Department of Electrical Engineering
Eindhoven University of Technology
Eindhoven, The Netherlands
i.a.m.huijben@tue.nl
Bastiaan S. Veeling*
Department of Computer Science,
University of Amsterdam
Amsterdam, The Netherlands
basveeling@gmail.com
Ruud J.G. van Sloun
Department of Electrical Engineering
Eindhoven University of Technology
Eindhoven, The Netherlands
r.j.g.v.sloun@tue.nl
Ab stract
The field of deep learning is commonly concerned with optimizing predictive
models using large pre-acquired datasets of densely sampled datapoints or signals.
In this work, we demonstrate that the deep learning paradigm can be extended to
incorporate a subsampling scheme that is jointly optimized under a desired sam-
pling rate. We present Deep Probabilistic Subsampling (DPS), a widely appli-
cable framework for task-adaptive compressed sensing that enables end-to-end
optimization of an optimal subset of signal samples with a subsequent model that
performs a required task. We demonstrate strong performance on reconstruction
and classification tasks of a toy dataset, MNIST, and CIFAR10 under stringent
subsampling rates in both the pixel and the spatial frequency domain. Thanks to
the data-driven nature of the framework, DPS is directly applicable to all real-
world domains that benefit from sample rate reduction. The code used for this
paper is made publicly available* 1.
1	Introduction
In many real-world prediction problems, acquiring data is expensive and often bandwidth-
constrained. Such is the case in regimes as medical imaging (Lustig et al., 2007; Choi et al., 2010;
Chernyakova & Eldar, 2014), radar (Baraniuk, 2007), and seismic surveying (Herrmann et al., 2012).
By carefully reducing the number of samples acquired over time, in pixel-coordinate space or in k-
space, efficient subsampling schemes lead to meaningful reductions in acquisition time, radiation
exposure, battery drain, and data transfer.
Subsampling is traditionally approached by exploiting expert knowledge on the signal of interest.
Famously, the Nyquist theorem states that when the maximum frequency of a continuous signal is
known, perfect reconstruction is possible when sampled at twice this frequency. More recently, it
has been shown that if the signal is sparse in a certain domain, sub-Nyquist rate sampling can be
achieved through compressive measurements and subsequent optimization of a linear system under
said sparsity prior; a framework known as compressed sensing (CS) (Donoho et al., 2006; Eldar &
Kutyniok, 2012; Baraniuk, 2007).
CS methods however lack in the sense that they do not (under a given data distribution) focus solely
on the information required to solve the downstream task of interest, such as disease prediction
or semantic segmentation. Formalizing such knowledge is challenging in its own right and would
require careful analysis for each modality and downstream task. In this work, we propose to explore
* Equal contribution.
1https://github.com/IamHuijben/Deep-Probabilistic-Subsampling.git
1
Published as a conference paper at ICLR 2020
the deep learning hypothesis as a promising alternative: reducing the need for expert knowledge in
lieu of large datasets and end-to-end optimization of neural networks.
As subsampling is non-differentiable, its integration into an end-to-end optimized deep learning
model is non-trivial. Here we take a probabilistic approach: rather than learning a subsampling
scheme directly, we pose a probability distribution that expresses belief over effective subsampling
patterns and optimize the distribution’s parameters instead. To enable differentiable sampling from
this distribution, we leverage recent advancements in a continuous relaxation of this sampling pro-
cess, known as Gumbel-softmax sampling or sampling from a concrete distribution (Jang et al.,
2017; Maddison et al., 2016). This enables end-to-end training of both the subsampling scheme and
the downstream model.
Naively, the number of parameters of a distribution over an n-choose-k problem scales factorially,
which is intractable for all practical purposes. We propose a novel, expressive yet tractable, parame-
terization for the subsampling distribution that conditions on the output sample index. We hypothe-
size that such conditioning prevents redundant sampling: it enables modeling the scenario in which
multiple candidate samples can be equally good, yet redundant in combination. Furthermore, we
investigate a parameter-restricted approach with a single parameter per candidate sample, balancing
tractability and exploration (Kool et al., 2019). In this case, we adopt a continuous relaxation of
top-K sampling to guarantee differentiability (Plotz & Roth, 2018).
Our main contributions are as follows:
•	DPS: A new regime for task-adaptive subsampling using a novel probabilistic deep learning
framework for incorporating a sub-Nyquist sampling scheme into an end-to-end network.
•	DPS enables joint optimization of a subsampling pattern with a predictive downstream
model, without the need for explicit knowledge on a sparsifying basis.
•	We demonstrate improved performance over strong subsampling baselines in image classi-
fication and reconstruction, while sampling both in Fourier and pixel space.
2	Related work
Recent works have proposed deep-learning-based subsampling methods for fast MRI, historically
being one of the most prominent applications of CS. Weiss et al. (2019) exploit gradient back-
propagation to a fixed set of real-valued coordinates, enabled by their subsequent (limited-support)
interpolation on the discrete k-space grid, and Bahadir et al. (2019) formulate the sampling problem
by learning pixel-based thresholding of i.i.d. samples drawn from a uniform distribution, referred to
as LOUPE. Where the former suffers from limited exploratory capabilities (likely due to its compact
support), with learned sampling schemes typically not deviating far from their initialization, the lat-
ter controls the sample rate only indirectly, through the addition of a sparsity-promoting `1 penalty
on the sampling mask.
Related to our methodology, yet different in purpose, Plotz & Roth (2018); Xie & Ermon (2019);
Kool et al. (2019) leverage an extension to the Gumbel-max trick (Gumbel, 1954) for (ordered)
subset selection using a categorical distribution with N - 1 free parameters. They rely on top-K
sampling, as first proposed by Vieira (2014). Based upon the continuous relaxation of the cate-
gorical distribution, known in the deep learning community as the Gumbel-softmax trick or the
concrete distribution (Jang et al., 2017; Maddison et al., 2016), Plotz & Roth (2018) show that such
a relaxation also exists for top-K sampling. In this work, we investigate the use of Gumbel-based
relaxations for task-adaptive compressive sampling, and further propose a novel parametrization
where we condition the sampling distribution on the output sample index.
We differentiate our contribution from deep encoder-decoder methods for data compression (Baldi
& Hornik, 1989; Hinton & Zemel, 1993; Blier & Ollivier, 2018; Habibian et al., 2019), which
do not aim at reducing data rates already at the sensing and digitization stage. Related work by
Mousavi et al. (2019) and Wu et al. (2019), focusses on the problem of learning compressive linear
encoders/filters, rather than discrete subsampling as addressed here. The authors of Balm et al.
(2019) use a Gumbel-max-based auto-encoder for discrete feature selection, which is however not
task-adaptive.
2
Published as a conference paper at ICLR 2020
Desired task S
Backpropagation
出* digit
Figure 1: (a) System-level overview of the proposed framework, in which a probabilistic generative
sampling model (DPS) and a subsequent task model are jointly trained to fulfill a desired system
task. (b,c) Two illustrative task-based sampling paradigms: image classification from a partial set of
pixels (b), and image reconstruction from partial Fourier measurements (c), respectively.
Task
Task
2
Through the lens of contemporary deep learning, subsampling can be interpreted as a form of at-
tention (Bahdanau et al., 2014; Kim et al., 2017; Parikh et al., 2016; Vaswani et al., 2017). Rather
than attending on intermediate representations, our model “attends” directly on the input signal. For
subsampling to be effective, sparse weights are essential. In the space of attention, this is known as
hard attention (Xu et al., 2015), and is typically optimized using the REINFORCE gradient estima-
tor (Williams, 1992). In contrast to the method of attention as applied in these works, our method
aims for a fixed, reduced subsampling rate.
3	Method
3.1	Task-adaptive system model
We consider the problem of predicting some downstream task s, through a learned subsampling
scheme A on a fully-sampled signal x ∈ RN, resulting in measurements y ∈ RM, with M << N:
y = Ax,	(1)
where A ∈ {0, 1}M×N is the subsampling measurement matrix. We here concern ourselves
specifically with scenarios in which the rows of A are constrained to having cardinality one, i.e.
||am||0 = 1, ∀m ∈ {1, ..., M}. In such cases, A serves as a subset selector, sampling M out of N
elements in x. From the resulting low-rate measurements y, we aim at predicting task s through:
S = fθ (y),	(2)
With fθ(∙) being a function that is differentiable with respect to its input and parameters θ, e.g. a
neural network.
Given a downstream task and dataset, we are interested in learning both the optimal processing pa-
rameters θ, and the sampling scheme as described in eq. (1). To circumvent the non-differential
nature of discrete sampling, we will introduce a novel fully probabilistic sampling strategy that al-
lows for gradient-based learning through error backpropagation, on which we detail in the following
section.
3
Published as a conference paper at ICLR 2020
3.2	DPS: Deep Probabilistic Subsampling
Since direct optimization of the elements in A is intractable due to its combinatorial nature, we
here instead propose to leverage a tractable generative sampling model that is governed by a learned
subsampling distribution, parameterized by Φ:
A(Φ)〜P(A∣Φ).	(3)
Thus, rather than optimizing A(Φ), we optimize the distribution parameters Φ. To warrant sufficient
expressiveness while maintaining tractability, we learn the parameters φm ∈ RN of M independent
categorical distributions (rather than the joint distribution, which scales factorially), being the rows
ofΦ ∈ RM×N.
Formally, we define each mth measurement row am ∈ {0, 1}N in A(Φ) as a one-hot encoding
of an independent categorical random variable rm 〜 Cat(N, πm). We define πm ∈ RN =
{πm,1 , . . . , πm,N}, being a vector containing N class probabilities, and parameterize it in terms
of its unnormalized logits φm such that:
exp φm,n
πm,n = ^N	"	.	(4)
i=1 exp φm,i
We sample from Cat(N, πm) by leveraging the Gumbel-max trick (Gumbel, 1954), a reparameteri-
zation of the sampling procedure. It perturbs the unnormalized logits φm,n with i.i.d. Gumbel noise2
samples em,n 〜 Gumbel(0, 1). The argmax of these perturbed logits, results in a sample from the
original categorical distribution Cat(N, ∏m). A realization rm is then defined as:
rm = argmax{wm-1,n + φm,n + em,n },	m ∈ {1,..∙, M},	(5)
n
in which wm-ι,n ∈ {-∞, 0} for n ∈ {1... N} mask previous realizations ri,…,rm-ι, by
adding -∞ to the logit of the previously selected category, enforcing sampling without replacement
among the M distributions. Introducing the function one_hotn(∙) as the operator that returns a
one-hot vector of length N, we finally obtain:
am = one-hotN {rm} = one-hotN {argmax{wm-1,n + φm,n + em,n}}∙
(6)
To permit error backpropagation for efficient optimization of Φ, We require Vφmam to exist
∀m ∈ {1,...,M}. Since argmax(∙) is a non-differentiable operator, we adopt the Straight-Through
Gumbel Estimator (Jang et al., 2017; Maddison et al., 2016) as a surrogate for Vφmam:
Vφm am := Vφm Eem softmaxτ (wm-1 + φm + em)
V E	exp{(wm-i + φm + em)∕τ}
φm em _PN=I exp{(wm-1,i + φm,i + em,i)∕τ上，
(7)
with (row operator) SoftmaxT(∙) as a continuous differentiable approximation of the one-hot en-
coded argmax(∙) operation. Appendix A provides the full derivation of Vφmam, and fig. 1 shows a
schematic overview of the proposed framework.
We refer to sampling using the Softmaxτ(∙) function as soft sampling. Its temperature parameter
τ serves as a gradient distributor over multiple entries (i.e. logits) in φm . Using a relatively high
value enables updating of multiple logits during training, even though a hard sample was taken in
the forward pass. In the limit of T → 0, soft sampling approaches the one-hot encoded argmax(∙)
operator in eq. (6) (Jang et al., 2017; Maddison et al., 2016). While lowering τ reduces the gradient
estimator’s bias, it comes at the cost ofa higher variance. We find however that using a fixed (tuned)
value of T worked well for our experiments, and do not explore methods to reduce the introduced
gradient bias (Grathwohl et al., 2017; Tucker et al., 2017).
2The Gumbel distribution is typically used to model the maximum of a set of independent samples. Its
probability density function is of the form: f (x) = 1 e-z-e z, with Z = x-μ, where μ and β are the mean
and standard deviation, respectively. The standard Gumbel distribution, with μ = 0 and β = 1 is adopted in
the Gumbel-max trick.
4
Published as a conference paper at ICLR 2020
To study the trade-off between parameter efficiency and convergence, we further study a more
parameter-restricted version of DPS that shares the weights φm across all M distributions. This
allows for a more memory-efficient implementation, by leveraging the Gumbel top-K trick (Kool
et al., 2019) (rather than Gumbel-max), and its corresponding relaxation (Plotz & Roth, 2018). We
refer to this particular model as DPS-topK, and adopt DPS-top1 to indicate the sampling model pre-
sented in equations 6 and 7, where weights are not shared across distributions, and one sample is
drawn from each distribution. Algorithm 1 in appendix B describes the DPS algorithm.
4	Experiments
We test the applicability of the proposed task-adaptive DPS framework for three datasets and two
distinct tasks: image classification and image reconstruction. We explore subsampling in pixel-
coordinate space as well as in k-space. The latter is relevant for scenarios in which data is acquired
in the frequency domain, such as (but not limited to) magnetic resonance imaging (MRI).
4.1	MNIST classification
Experiment setup Classification performance was tested on the MNIST database (LeCun et al.,
1998), comprising 70,000 28 × 28 grayscale images of handwritten digits 0 to 9. We split the
dataset into 50,000 training images, 5,000 validation, and 5,000 test images. We train DPS-top1 and
DPS-topK to take partial measurements in either the image or Fourier domain, and process them
through the task model to yield a classification outcome. Results are compared to those obtained
using uniformly distributed pixel/Fourier samples, a sampled disk/low pass filter, and the data-driven
LOUPE method (Bahadir et al., 2019).
Besides, to show the benefit of task-adaptive sampling, i.e. jointly training sampling with the task,
we add a case in which we disjointly learn the classification task from the sampling pattern. As such,
we train DPS-topK for MNIST reconstruction, after which the learned sampling pattern is frozen,
and the classifier is trained.
Task model After sampling M elements, all N zero-masked samples (or 2N in the case of
complex Fourier samples) are passed through a series of 5 fully-connected layers, having N, 256,
128, 128 and 10 output nodes, respectively. The activations for all but the last layer were leaky Re-
LUs, and 20% dropout was applied after the first three layers. The 10 outputs were normalized by a
softmax function to yield the respective classification probabilities. Table 1 in appendix C shows a
detailed overview of the complete architecture of the task model. Zero-filling and connecting all pos-
sible samples, rather than only connecting the M selected samples, facilitated faster co-adaptation
of the network to different sampling patterns during training.
Training details We train the network to maximize the log-likelihood of the observations
D = {(xi, si) | i ∈ 0, . . . , L} through minimization of the categorical cross-entropy between the
predictions and the labels, denoted by Ls . We moreover promote training towards one-hot distribu-
tions Cat(N, πm) by penalizing high entropy, adopting:
MN
Le = -	πm,n log πm,n ,
m=1 n=1
with πm,n defined as in eq. (4). The total optimization problem is thus:
ɪ ∕Λ	∙ I TO
Φ,θ = argmin I E(x,s)~p。
(8)
(9)
Ls + μLe
where PD is the data generating distribution. Penalty multiplier μ was set to linearly increase 1e-5
per epoch, starting from 0.0. Increasing this entropy penalty enforces one-hot distributions, and
therefore less variations in the realizations, i.e. sampling patterns, when training evolves. The tem-
perature parameter τ in eq. (7) was set to 2.0, and the sampling distribution parameters Φ were
initialized randomly, following a zero-mean Gaussian distribution with standard deviation 0.25.
Equation 9 was optimized using stochastic gradient descent on batches of 32 examples, approxi-
mating the expectation by a mean across the train dataset. To that end, we used the ADAM solver
5
Published as a conference paper at ICLR 2020
(a)
Image-domain sampling
(b)	FoUrier-domain sampling
qdluexw Nd。'Sda Td。'Sda
Perc. error:	Perc. error:
DPS-topK	2.1%	6.6%
DPS-top1	2.6%	9.4%
Perc. error:
1.3%
1.2%
Perc. error:
1.3%
1.8%
LOUPE	2.1%	19.9%	6.2%	32.0%
Uniform	4.2%	28.6%	9.1%	54.5%
Disk	6.3%	27.3%	1.3%	1.7%
Figure 2: MNIST classification for (a) image-domain, and (b) FoUrier-domain subsampling. (top)
Several example images in the respective sampling domains, (2nd and 3rd row) learned task-adaptive
(DPS-topK and DPS-top1, respectively) subsampling patterns, with their relative sample incidence
across a 1000 such realizations (inset), and (bottom) hold-out classification results of the pro-
posed DPS methods, compared to the LOUPE baseline, and two non-learned baseline sampling
approaches.
(β1 = 0.9, β2 = 0.999, and = 1e-7 ) (Kingma & Ba, 2014), and we trained until the vali-
dation loss plateaued. We adopted different learning rates for the sampling parameters Φ and the
parameters of the task model θ, being 2e - 3 and 2e - 4, respectively.
Results The results presented in fig. 2a show that image-domain sampling using DPS sig-
nificantly outperforms the fixed sampling baselines (uniform and disk). The data-driven LOUPE
method is outperformed in case of a strong subsampling factor. The resulting patterns qualita-
tively demonstrate how, for this task, a sensible selection of pixels that are most informative was
made (slightly slanted, and capturing discriminative areas). Notably, partial Fourier measurements
(fig. 2b) allowed for a much greater reduction of the number samples, with DPS sampling out-
performing uniform sampling and LOUPE, and showing similar performance as fixed disk sam-
pling. Interestingly, the DC and very-low frequency components were consistently not selected.
Figure 3 shows the learned sampling pattern in pixel
space (96.8% pixels removed), when training DPS-topK
for the reconstruction task. The adopted reconstruction
network architecture is the same as the one used for
CIFAR10 reconstruction (see section 4.3). The subse-
quently trained classifier resulted in a classification er-
ror of 9.3%, compared to 6.6% for task-adaptive learning
with DPS-topK. This case clearly illustrates the benefit of
task-adaptive learning of a sampling pattern.
Figure 3: A realization of the learned
distribution (inset) for reconstruction.
4.2	‘LINES AND CIRCLES’ IMAGE RECONSTRUCTION
Experiment setup To evaluate reconstruction of structured images from highly undersampled
partial Fourier (k-space) measurements (keeping 3.1% of the coefficients), we generated synthetic
toy data comprising images that each contain up to 5 horizontal lines and randomly-sized circles.
Lines and circles were placed at random positions and their pixel intensity was drawn from a uniform
distribution between 1 and 10. Examples were generated in an on-line fashion during training. A
6
Published as a conference paper at ICLR 2020
Proposed
Uniform
Random
Low pass
LOUPE
DPS-top1
DPS-topK
partial k-space
sampling
3.1%
Full k-space 40dB
Tal
(a)
(c)
PSNR: 37.6 dB
(f)
PSNR: 39.6 dB
(g)
PSNR: 16.7dB
(b)
PSNR: 28.7 dB
(d)
PSNR: 35.5 dB
(e)
Figure 4: Image reconstruction performance from partial k-space (Fourier) measurements on a CUs-
tom toy dataset consisting of lines and circles with random locations and sizes. Illustrative examples
of the k-space and target images are given in (a). The sampling patterns, reconstructed images and
PSNR value (across the entire test set) for the different sampling strategies are displayed in: (b)
uniform, (c) random, (d) low pass, (e) LOUPE, (f) DPS-top1, and (g) DPS-topK, respectively. In all
cases, only 3.1% of the Fourier coefficients have been selected.
2D Fourier ι≡
transform
PSNR: 27.4 dB

pre-generated hold-out test set of 1000 randomly generated examples was used for all cases. Two
illustrative test examples, along with their Fourier-domain representations, are given in Figure 4(a,b).
We compare the results to those obtained using three fixed partial Fourier measurement baselines,
following uniform, random, and low pass subsampling patterns, respectively, and the data-driven
LOUPE method.
Task model Image reconstruction from partial Fourier measurements was performed by fol-
lowing the methodology in Zhu et al. (2018). We use a deep neural network consisting of two
subsequent fully-connected layers with tanh activations that map the 2N (zero-filled) Fourier coef-
ficients (stacked real and imaginary values) to a vector of length N. This vector was subsequently
reshaped into a √N X √N image, and processed by 3 convolutional layers. Table 2 in appendix C
provides the layer parameters of this task network.
Training details The optimization problem is the same as in eq. (9), however with the loss Ls
now defined as the negative log-likelihood:
Ls = E(χ,s)〜PD kfθ(A(φ)x)-sk2.	(10)
The learning rates for Φ and θ were 1e 一 3 and 1e 一 4, respectively, and μ and T were respectively
set to 2e - 4 and 5.0. The ADAM optimizer (with settings as provided in section 4.1) was used to
train in mini-batches of 128 examples. Training was stopped when the validation loss plateaued.
Results An overview of the results is given in fig. 4. As expected, uniform subsampling leads to
strong spatial aliasing that can not be recovered by the task model due to violation of the Nyquist cri-
terion. In turn, random subsampling introduces an incoherent aliasing pattern, that can only partly be
recovered. Although not suffering from aliasing, low pass sampling deteriorates resolution, of which
the effect is particularly evident for the broadband/sharp horizontal lines. In contrast, data-driven
sampling methods (LOUPE, DPS-top1, and DPS-topK) prevent aliasing by learning an appropriate
sampling pattern. DPS sampling specifically, yields high-resolution accurate reconstructions with
an improved PSNR evaluated on the entire test set compared to the baselines. Note how the learned
7
Published as a conference paper at ICLR 2020
2D Fourier is
transform 2。
Low pass
LOUPE
DPS-top1
DPS-topK
Proposed
Uniform
Random
partial k-space
sampling	'
12.5%	!
(a)
PSNR: 25.3 dB
SSIM: 0.82
(d)
PSNR: 27.1 dB
SSIM: 0.88
(f)
PSNR: 25.3 dB
SSIM: 0.86
(e)
PSNR: 12.7 dB
SSIM: 0.11
(c)
PSNR: 28.3 dB
SSIM: 0.90
(g)
Figure 5: Image reconstruction performance from partial k-space (Fourier) measurements on the
CIFAR10 database. Illustrative examples of the k-space and target images are given in (a). The
sampling patterns, images and statistical quality metrics for uniform, random, low pass, LOUPE,
DPS-top1, and DPS-topK are given in (b-g), respectively. In all cases, 12.5% of the Fourier coeffi-
cients have been selected.
PSNR: 14.8 dB
SSIM: 0.22
(b)
sampling patterns (top row) have evolved from a random initialization, similar to that of (c), to the
ultimate task-adaptive scheme (e-g).
4.3	CIFAR 1 0 image reconstruction
Experiment setup The CIFAR10 database (Krizhevsky et al., 2009) contains 60,000 images
of 32 × 32 pixels in 10 different classes. We converted all images to grayscale, and subsequently
split them into 50,000 training images, 5,000 validation and 5,000 test images. We again learn
partial Fourier sampling and image reconstruction, keeping only 12.5% of the Fourier coefficients,
and compare ourselves to the three fixed partial Fourier measurement baselines and LOUPE, as
described in section 4.2.
Task model The challenging reconstruction task for CIFAR10 motivates the adoption of a struc-
tured model to enable strong reconstruction. We draw inspiration from iterative proximal-gradient
schemes (Parikh et al., 2014), which are dedicated to solving the ill-posed linear measurement prob-
lem in eq. (1). To that end, we unfold K = 5 such iterations, learning an adequate image-domain
proximal mapping Pθ(k) and stepsize α(k) at each fold:
s(k+1) = P(k) W) - a(k)FHATφ) (A(Φ)FS⑹-A(φ)x)) ,	(11)
where F ∈ CN×n is a discrete Fourier transform (DFT) matrix, and (∙)H denotes the Hermitian
(conjugate transpose). In the above formulation, at each fold a step is taken towards the sampling-
consistent subspace that adequately represents the physical measurement ofs by A(Φ)F. The trained
proximal operator Pθ(k), a 7-layer convolutional network, then projects this onto the manifold of
visually plausible images (Mardani et al., 2018), removing noise, aliasing, or blurring artifacts.
Table 3 in appendix C provides provides details on the layers in the task model.
Training details Optimization settings were similar to those in section 4.2, leveraging a mean-
squared-error (negative log-likelihood) reconstruction cost and a distribution entropy penalty on πm.
To promote visually plausible reconstructions, we added an adversarial (Ledig et al., 2017) cost by
8
Published as a conference paper at ICLR 2020
adopting a discriminator network Dψ(∙) that aims to discriminate between images reconstructed
from partial Fourier measurements (S) and actual images s. The discriminator comprised 3 ConvolU-
tional layers with leaky ReLU activations, followed by global average pooling, 40% dropout, and a
logistic binary classification model (parameters are provided in table 3, appendix C). The sampling-
and task model parameters were then trained to both minimize the negative log-likelihood and dis-
criminator loss LDψ, in addition to the entropy penalty Le:
Ldψ = Es,s [log(Dψ(S)) +log(1 - Dψ(S))],
(12)
Φ ,θ,ψ = argmin argmax{E(x,s)〜PD IIfθ (A(Φ)X)-Sll2 + μLe + λLDψ f ,	(13)
Φ,θ	ψ	D
where λ weighs adherence to the data-driven MSE loss and the discriminator loss. It was empirically
set to 0.004. The learning rates for {Φ, ψ} and θ were 1e 一 3 and 2e — 4, respectively, and μ was
empirically set to 1e - 6, balancing sharpness and high frequency hallucinations. Temperature
parameter τ was set at a constant value of 2.0. Training was performed using the ADAM optimizer
(with settings as given in section 4.1) in batches of 8 images, until the validation loss plateaued.
Results Figure 5 shows how task-adaptive sampling significantly outperforms all baselines.
Both the uniform and random subsampling schemes suffer from severe aliasing which the task model
is not able to adequately restore. While low pass sampling does not lead to aliasing, the absence
of real high-frequency information causes the task model to ‘hallucinate’ such (super-resolution)
content. Interestingly, LOUPE (e) learned a rather compact, i.e. around DC frequencies, sampling
scheme, resulting in blurry images, but DPS learned a more wide-spread pattern, also capturing
higher frequencies. The result of this is particularly notable for the example displayed in the bottom
row of fig. 5, being less structured (or predictable) than that in the row above it. DPS thus enabled
high-quality, high-resolution reconstructions that go beyond those obtained with the other methods,
reaching a PSNR of 27.1 dB and 28.3 dB, and a structural similarity index (SSIM) of 0.88 and 0.90
across the 5000 test examples, for topK and top1 respectively. For the most competitive methods,
i.e. fixed sampling low pass and data-driven LOUPE, these statistics were 25.3 dB and 0.82, and
25.3 dB and 0.86, respectively.
5 Conclusions
We have introduced Deep Probabilistic Subsampling (DPS), a framework that enables jointly learn-
ing a data- and task-driven sampling pattern, with a subsequent task-performing model. The frame-
work is generic and can be combined with any network architecture that performs the required task
using the subsampled set of signal elements. Empirically we find the method to perform strongly
on toy datasets and canonical deep learning problems. Further work can explore the effectiveness
of DPS in real-world problems such as MRI scanning. Even though LOUPE (Bahadir et al., 2019)
learned similar subsampling patterns in all experiments, compared to DPS, its performance on the
downstreak task showed to be lower. We argue that LOUPE probably suffers from the fact that
samples in the training phase are relaxed. As such, the task network is optimized on a relaxed sub-
sampled pattern, whereas a hard thresholded pattern is used for inference on the test set to make fair
comparisons to DPS.
In comparing the parameter-restricted DPS-topK versus the more expressive DPS-top1, we find the
former to perform better in our experiments. Studying the sampling distributions after training, we
find that DPS-top1 exhibits more uncertainty. The insets in fig. 2 show this by the higher variance
present among the 1000 realizations for DPS-top1 compared to DPS-topK. We tentatively attribute
this due to difficulties with convergence. Further work will explore if the DPS-top1’s more ex-
pressive parametrization can improve over DPS-topK in more complex problems that might benefit
from a broader hypothesis space during training, as well as under more carefully tuned softmax
temperature annealing schemes.
Our data-driven method does not explicitly need knowledge regarding the, often unknown, spar-
sifying basis, whereas conventional CS algorithms do require this. Like all data-driven optimized
methods, a learned sampling scheme is at risk of overfitting to a small training dataset. Although we
9
Published as a conference paper at ICLR 2020
did not observe this issue in our experiments, careful regularization might be required to ensure that
this effect is minimal in such high-risk tasks. The fully-differentiable DPS framework allows for
flexibility, and interesting extensions can be explored in future work. Rather than learning a fixed
set of parameters for the subsampling distribution, a neural network can be used to predict the pa-
rameters instead, conditioned on contextual data or the samples acquired so far. Finally, our method
currently requires the desired sampling rate to be predetermined as a hyperparameter. Future work
can explore if this rate can be jointly optimized to incorporate optimization of the subsampling rate.
Acknowledgments
This research was supported in part by Philips Research. It is also part of a research program Ru-
bicon ENW 2018-3 with project number 019.183.EN.014, which is financed by the Dutch Research
Council (NWO). We thank Wouter Kool and the anonymous reviewers for their valuable feedback.
References
Cagla Deniz Bahadir, Adrian V Dalca, and Mert R Sabuncu. Learning-based optimization of the
under-sampling pattern in mri. In International Conference on Information Processing in Medical
Imaging, pp. 780-792. Springer, 2019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. September 2014.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural Netw., 2(1):53-58, January 1989.
Muhammed Fatih Balin, Abubakar Abid, and James Zou. Concrete autoencoders: Differentiable
feature selection and reconstruction. In International Conference on Machine Learning, pp. 444-
453, 2019.
Richard G Baraniuk. Compressive sensing. IEEE signal processing magazine, 24(4), 2007.
Leonard Blier and Yann Ollivier. The description length of deep learning models. In S Bengio,
H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, and R Garnett (eds.), Advances in Neural
Information Processing Systems 31, pp. 2216-2226. Curran Associates, Inc., 2018.
Tanya Chernyakova and Yonina C Eldar. Fourier-domain beamforming: the path to compressed
ultrasound imaging. IEEE transactions on ultrasonics, ferroelectrics, and frequency control, 61
(8):1252-1267, 2014.
Kihwan Choi, Jing Wang, Lei Zhu, Tae-Suk Suh, Stephen Boyd, and Lei Xing. Compressed sensing
based cone-beam computed tomography reconstruction with a first-order method a. Medical
physics, 37(9):5113-5125, 2010.
David L Donoho et al. Compressed sensing. IEEE Transactions on information theory, 52(4):
1289-1306, 2006.
Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge
university press, 2012.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint
arXiv:1711.00123, 2017.
Emil Julius Gumbel. Statistical theory of extreme values and some practical applications. NBS
Applied Mathematics Series, 33, 1954.
Amirhossein Habibian, Ties van Rozendaal, Jakub M Tomczak, and Taco S Cohen. Video compres-
sion with Rate-Distortion autoencoders. August 2019.
10
Published as a conference paper at ICLR 2020
Felix J Herrmann, Michael P Friedlander, and Ozgur Yilmaz. Fighting the curse of dimensionality:
ComPressive sensing in exploration seismology. IEEE Signal Processing Magazine, 29(3):88-
100, 2012.
Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length and helmholtz
free energy. NIPS, 1993.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumbel-softmax. stat,
1050:17, 2017.
Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. Structured attention networks.
February 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2014.
Wouter Kool, Herke Van Hoof, and Max Welling. Stochastic beams and where to find
them: The gumbel-top-k trick for sampling sequences without replacement. arXiv preprint
arXiv:1903.06059, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-
gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 4681-4690, 2017.
Michael Lustig, David Donoho, and John M Pauly. Sparse mri: The application of compressed
sensing for rapid mr imaging. Magnetic Resonance in Medicine: An Official Journal of the
International Society for Magnetic Resonance in Medicine, 58(6):1182-1195, 2007.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Morteza Mardani, Qingyun Sun, David Donoho, Vardan Papyan, Hatef Monajemi, Shreyas
Vasanawala, and John Pauly. Neural proximal gradient descent for compressive imaging. In
Advances in Neural Information Processing Systems, pp. 9573-9583, 2018.
Ali Mousavi, Gautam Dasarathy, and Richard G. Baraniuk. A data-driven and distributed ap-
proach to sparse signal representation and recovery. In ICLR 2019, 2019. URL https:
//openreview.net/pdf?id=B1xVTjCqKQ.
Ankur P Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model for natural language inference. June 2016.
Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and TrendsR in Optimization,
1(3):127-239, 2014.
Tobias Plotz and Stefan Roth. Neural nearest neighbors networks. October 2018.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar:
Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in
Neural Information Processing Systems, pp. 2627-2636, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
E Ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I Guyon, U V Luxburg,
S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett (eds.), Advances in Neural Infor-
mation Processing Systems 30, pp. 5998-6008. Curran Associates, Inc., 2017.
11
Published as a conference paper at ICLR 2020
Tim Vieira. Gumbel-max trick and weighted reservoir sampling — graduate de-
scent.	https://timvieira.github.io/blog/post/2014/08/01/
gumbel-max-trick-and-weighted-reservoir-sampling/, August 2014.
Accessed: 2019-9-24.
Tomer Weiss, Ortal Senouf, Sanketh Vedula, Oleg Michailovich, Michael Zibulevsky, and Alex
Bronstein. Pilot: Physics-informed learned optimal trajectories for accelerated mri. arXiv preprint
arXiv:1909.05773, 2019.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Mach. Learn., 8(3):229-256, May 1992.
Shanshan Wu, Alexandros G Dimakis, Sujay Sanghavi, Felix X Yu, Daniel Holtmann-Rice, Dmitry
Storcheus, Afshin Rostamizadeh, and Sanjiv Kumar. Learning a compressed sensing measure-
ment matrix via gradient unrolling. International Conference on Machine Learning (ICML),
2019.
Sang Michael Xie and Stefano Ermon. Reparameterizable subset sampling via continuous relax-
ations. In International Joint Conference on Artificial Intelligence, 2019.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov,
Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. February 2015.
Bo Zhu, Jeremiah Z Liu, Stephen F Cauley, Bruce R Rosen, and Matthew S Rosen. Image recon-
struction by domain-transform manifold learning. Nature, 555(7697):487, 2018.
12
Published as a conference paper at ICLR 2020
A Gradient of Gumbel-Softmax sampling
For any mth pair of rows (am, φm), any nth element am,n in am can be differentiated towards all
elements in φm through:
% Φm a
m,n
%φm Eem softmaxτ (wm-1 + φm + em)
Eem %φm softmaxτ (wm-1 + φm + em)
exp{(wm-1 + φm + em)∕τ}
PN=I exp{(wm-i,i + φm,i + £m,i)/T} In
(14)
GUmbel noise vector emr can be reparametrized as a function of a uniform noise vector Wm 〜U(0,1)
i.i.d., through:
em = — log(— log(Wm)).
This allows rewriting eq. (14) into:
(15)
%φm am,n = Em %φm
exp{(wm-1 + φm — log(— log(m)))∕τ}
PiN=1 exp{(wm-1,i + φm,i — log(— log(m,i )))∕τ} n
Z∞
∞
.[∞ P [Wm =[kι,…，kN ]] Vφm	Nexp{ (Wm-I + φ - Iog(Tog㈣)"T}1
J-∞	PiX1 exp{(wm-1,i + φm,i — log(- log(ki)))∕τ} In
…dkι
Z1.
0
P P [∈m,1 = kl] P [∈m,2 =k2〕…。P [∈m,N = kN]
0
%φm
exp{(wm-1 + φm — log(— log(k)))∕τ}
PN=I exp{(wm-i,i + φm,i — log(- log(ki)))∕τ}
dk_N …dkι
n
Z1.
0
∕11∙Vφm	Nexp{ (Wm-I + φ - Iog(Tog㈣))/T}	I dkN -dkι.
Jo	PiXI exp{(wm-1,i + φm,i — log(— log(ki)))∕τ} In
(16)
]
]
13
Published as a conference paper at ICLR 2020
B	Algorithm description
Algorithm 1 Deep Probabilistic Subsampling (DPS)
Require: Training dataset D, Number of iterations niter, temperature parameter τ , initialized train-
able parameters Φ and θ.
Ensure: Trained logits matrix Φ and task network parameters θ.
for i = 1 to niter do
-	Draw mini-batches (xi ,si): a random subset of D
-	Compute AΦ = [a1; . . . ; aM] using:
if DPS-top1 then
-	Initialize mask: w0 = 0
for m = 1 to M do
-	Draw i.i.d. Gumbel noise samples em ∈ RN
-	Sample from the distribution : rm, = argmax{wm,-ι,n + φm,n + em,n}
n
-	Create one-hot vector: am, = one_hotn(rm,)
-	Take current mask: wm = wm-1
-	Update mask: wm,rm = -∞
end for
else if DPS-topK then
-	Draw i.i.d. Gumbel noise samples e1 ∈ RN
-	Sample M samples from the distribution and create an M-hot vector:
M-hot = TopK{φι,n + eι,n}
n
-	Expand the M-hot vector in M one-hot vectors am, with m ∈ {1, . . . , M}
-	Create mask per sample: Wm = -∞ ∙ am, ∀m ∈ {1,..., M}
-	Create the cumulative masks wm = Pim=1 wi, ∀m ∈ {1, . . . , M}.
end if
-	Subsample the signal: yi = AΦxi
-	Compute the task: Si = fθ (yi)
-	Compute loss using : Li = ∣∣Si - Sik2 + LPen
-	∀m ∈{1,..., M}, redefine Vφmam := VφmEem Isoftmaxτ(wm-ι + φm + em)],
where φm = φ1 for DPS-topK
-	Use Adam optimizer to update Φ and θ
end for
14
Published as a conference paper at ICLR 2020
C Hyperparameters of adopted task models
This appendix contains tables that provide details on the adopted task model architectures for the
different experiments. We use the abbreviation FC, which stands for ‘fully-connected layer’.
Table 1: The layer parameters corresponding to the task model in MNIST classification.
Layer	Output size	Init	Activation
FC 1	N	Glorot uniform3	LeakyReLU (0.2)
Dropout 1 (20%)	N	-	-
FC 2	256	Glorot uniform	LeakyReLU (0.2)
Dropout 2 (20%)	256	-	-
FC 3	128	Glorot uniform	LeakyReLU (0.2)
Dropout 3 (20%)	128	-	-
FC 4	128	Glorot uniform	LeakyReLU (0.2)
FC 5	10	Glorot uniform	Softmax
Table 2: The layer parameters corresponding to the task model in ‘lines and circles’ reconstruction.
Layer	Output size	Init	Activation	Kernel size	Strides
FC 1	N	Glorot uniform	tanh	-	-
FC 2	N	Glorot uniform	tanh	-	-
Reshape	√N × √N	-	-	-	-
Conv2D 1	√N X √N X 64	Glorot uniform	ReLU	5x5	1x1
Conv2D 2	√N X √N x 64	Glorot uniform	ReLU	5x5	1x1
Conv2D 3	√N x √N x 1	Glorot uniform	None	7x7	1x1
3Glorot & Bengio (2010)
15
Published as a conference paper at ICLR 2020
Table 3: The layer parameters corresponding to the task model in CIFAR10 reconstruction.
Network type	Layer	Output size			Init	Activation	Kernel size	Strides
Generator	Proximal operator Pθ(k) 6× Conv2D 1× Conv2D	√N X √N X 64 √N x √N x 1			Glorot uniform Glorot uniform	ReLU ReLU	3X3 3X3	1X1 1X1
	Stepsize a(k) 1× Conv2D	√N X √N		X1	Glorot uniform	ReLU	3X3	1X1
Discriminator Dψ	Conv2D 1	√N χ 2 X	√N X 2 X	128	Glorot uniform	LeakyReLU (0.2)	3X3	2X2
	Conv2D 2	√N χ 4 X	√N X 4 X	128	Glorot uniform	LeakyReLU (0.2)	3X3	2X2
	Conv2D 3 GlobalAveragePooling2D Dropout 40% FC 1	√N X 8 X	√N X 8 X 128 128 1	128	Glorot uniform - - -	LeakyReLU (0.2) - - Sigmoid	3X3 - - -	2X2 - - -
D Example training graph for training of ‘lines and circles
reconstruction using DPS-topK
(a)
Training and validation loss
IOOO 1250 1500 1750 2000
EDOCh
Figure 6: Training graphs for the ‘lines and circles' reconstruction problem from section 4.2. (a)
The graph shows that even though neural network loss functions are typically non-convex, the op-
timization trajectory of the loss value still followed a smooth path. The train and validation loss
exactly behave the same way, suggesting no overfitting. (b) The corresponding PSNR value during
training.
16