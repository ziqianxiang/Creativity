Published as a conference paper at ICLR 2020
Identity Crisis: Memorization and General-
ization under Extreme Overparameterization
Chiyuan Zhang & Samy Bengio
Google Research, Brain Team
Mountain View, CA 94043, USA
{chiyuan,bengio}@google.com
Michael C. Mozer
Google Research, Brain Team
Mountain View, CA 94043, USA
mcmozer@google.com
Moritz Hardt
University of California, Berkeley
Berkeley, CA 94720, USA
hardt@berkeley.edu
Yoram Singer
Princeton University
Princeton, NJ 08544, USA
y.s@princeton.edu
Ab stract
We study the interplay between memorization and generalization of overparame-
terized networks in the extreme case of a single training example and an identity-
mapping task. We examine fully-connected and convolutional networks (FCN and
CNN), both linear and nonlinear, initialized randomly and then trained to mini-
mize the reconstruction error. The trained networks stereotypically take one of
two forms: the constant function (memorization) and the identity function (gen-
eralization). We formally characterize generalization in single-layer FCNs and
CNNs. We show empirically that different architectures exhibit strikingly differ-
ent inductive biases. For example, CNNs of up to 10 layers are able to generalize
from a single example, whereas FCNs cannot learn the identity function reliably
from 60k examples. Deeper CNNs often fail, but nonetheless do astonishing work
to memorize the training output: because CNN biases are location invariant, the
model must progressively grow an output pattern from the image boundaries via
the coordination of many layers. Our work helps to quantify and visualize the
sensitivity of inductive biases to architectural choices such as depth, kernel width,
and number of channels.
1 Introduction
The remarkable empirical success of deep neural networks is often attributed to the availability of
large data sets for training. However, sample size does not provide a comprehensive rationale since
complex models often outperform simple ones on a given data set, even when the model size exceeds
the number of training examples.
What form of inductive bias leads to better generalization performance from highly overparameter-
ized models? Numerous theoretical and empirical studies of inductive bias in deep learning have
been conducted in recent years (Dziugaite & Roy, 2016; Kawaguchi et al., 2017; Bartlett et al.,
2017; Neyshabur et al., 2017; Liang et al., 2017; Neyshabur et al., 2018; Arora et al., 2018; Zhou
et al., 2019) but these postmortem analyses do not identify the root source of the bias.
One cult belief among researchers is that gradient-based optimization methods provide an implicit
bias toward simple solutions (Neyshabur et al., 2014; Soudry et al., 2018; Shah et al., 2018; Arora
et al., 2019). However, when a network is sufficiently large (e.g., the number of hidden units in each
layer is polynomial in the input dimension and the number of training examples), then under some
mild assumptions, gradient methods are guaranteed to fit the training set perfectly (Allen-Zhu et al.,
2018b; Du et al., 2018a;b; Zou et al., 2018). These results do not distinguish a model trained on
a data distribution with strong statistical regularities from one trained on the same inputs but with
randomly shuffled labels. Although the former model might achieve good generalization, the latter
can only memorize the training labels. Consequently, these analyses do not tell the whole story on
the question of inductive bias.
1
Published as a conference paper at ICLR 2020
Input
20-layer
CNN
10-layer
ReLU FCN
1-layer
linear model
7
■
7
7
7



■
7
"
□ □□□
©

Figure 1: Predictions of three architectures trained on the identity mapping task with 60k MNIST
examples. The red dashed line separates 3 training examples from the test examples.
Another line of research characterizes sufficient conditions on the input and label distribution that
guarantee generalization from a trained network. These conditions range from linear separabil-
ity (Brutzkus et al., 2018) to compact structures (Li & Liang, 2018). While very promising, this
direction has thus far identified only structures that can be solved by linear or nearest neighbor
classifiers over the original input space. The fact that in many applications deep neural networks
significantly outperform these simpler models reveals a gap in our understanding of deep neural
networks.
As a formal understanding of inductive bias in deep networks has been elusive, we conduct a
novel exploration in a highly restrictive setting that admits visualization and quantification of in-
ductive bias, allowing us to compare variations in architecture, optimization procedure, initialization
scheme, and hyperparameters. The particular task we investigate is learning an identity mapping in a
regression setting. The identity mapping is interesting for four reasons. First, it imposes a structural
regularity between the input and output, the type of regularity that could in principle lead to system-
atic generalization (He et al., 2016; Hardt & Ma, 2017). Second, it requires that every input feature
is transmitted to the output and thus provides a sensitive indicator of whether a model succeeds in
passing activations (and gradients) between inputs and outputs. Third, conditional image generation
is a popular task in the literature (e.g., Mirza & Osindero, 2014; Ledig et al., 2017); an identity
mapping is the simplest form of such a generative process. Fourth, and perhaps most importantly, it
admits detailed analysis and visualization of model behaviors and hidden representations.
Consider networks trained on the identity task with 60k MNIST digits. Although only digit images
are presented during the training, one might expect the strong regularity of the task to lead to good
generalization to images other than digits. Figure 1 compares three different architectures. The top
row shows various input patterns, and the next three rows are outputs from a 20-layer convolutional
net (CNN), a 10-layer fully connected net (FCN) with rectified-linear unit (ReLU) activation func-
tions, and a 1-layer FCN. The 1-layer FCN amounts to a convex optimization problem with infinitely
many solutions, however gradient decent converges to a unique closed-form solution. All nets per-
form well on the training set (first three columns) and transfer well to novel digits and digit blends
(columns 4-6). Yet, outside of the hull of hand-printed digits, only the CNN discovers a reasonably
good approximation to the identity function.
Figure 1 reflects architecture-specific inductive bias that persists even with 60k training examples.
Despite this persistence, a model’s intrinsic bias is more likely to be revealed with a smaller training
set. In this paper, we push this argument to the limit by studying learning with a single training
example. Although models are free to reveal their natural proclivities in this maximally overparam-
eterized regime, our initial intuition was that a single example would be uninteresting as models
would be algebraically equivalent to the constant function (e.g., via biases on output units). Further,
it seemed inconceivable that inductive biases would be sufficiently strong to learn a mapping close
to the identity. Unexpectedly, our experiments show that model behavior is subtle and architecture
dependent. In a broad set of experiments, we highlight model characteristics—including depth, ini-
tialization, and hyperparameters—that determine where a model lands on the continuum between
memorization (learning a constant function) and generalization (learning the identity function). The
simplicity of the training scenario permits rich characterization of inductive biases.
2
Published as a conference paper at ICLR 2020
Figure 2: Visualization of the outputs of fully connected networks trained on a single exam-
ple. The first row shows the single training example (7) and a set of evaluation images consisting
of a linear combination of two digits, random digits from MNIST test set, random images from
Fashion MNIST, and some algorithmically generated image patterns. Each row below indicates an
architecture and the output from that architecture for a given input.
2	Related work
The consequences of overparameterized models in deep learning have been extensively studied in
recently years, on the optimization landscape and convergence of SGD (Allen-Zhu et al., 2018b;
Du et al., 2018a;b; Bassily et al., 2018; Zou et al., 2018; Oymak & Soltanolkotabi, 2018), as well
as the generalization guarantees under stronger structural assumptions of the data (Li & Liang,
2018; Brutzkus et al., 2018; Allen-Zhu et al., 2018a). Another line of related work is the study of
the implicit regularization effects of SGD on training overparameterized models (Neyshabur et al.,
2014; Zhang et al., 2017; Soudry et al., 2018; Shah et al., 2018; Arora et al., 2019).
The traits of memorization in learning are also explicitly studied from various perspectives such as
prioritizing learning of simple patterns (Arpit et al., 2017) or perfect interpolation of the training set
(Belkin et al., 2018; Feldman, 2019). More recently, coincidentally with the writing of this paper,
Radhakrishnan et al. (2018) reported on the effects of the downsampling operator in convolutional
auto-encoders on image memorization. Their empirical framework is similar to ours, fitting CNNs
to the autoregression problem with few training examples. We focus on investigating the general
inductive bias in the extreme overparameterization case, and study a broader range of network types
without enforcing a bottleneck in the architectures.
3	Experiments
We explore a progression of models: linear convex models, linear non-convex models (with multiple
linear layers), fully-connected multilayered architectures with nonlinearities, and finally the case of
greatest practical importance, fully convolutional networks. In all architectures we study, we ensure
that there is a simple realization of the identity function (see Appendix B). We train networks by
minimizing the mean squared error using standard gradient descent.
3.1	Fully connected networks
Figure 2 shows examples of predictions from multi-layer fully connected networks. Infinitely many
solutions exist for all models under this extreme over-parameterization, and the figure shows that all
the models fit the training example perfectly. However, on new test examples, contrasting behaviors
are observed between shallow and deep networks. In particular, deeper models bias toward predict-
ing a constant output, whereas shallower networks tend to predict random white noises on unseen
inputs. The random predictions can be characterized as follows (proof in Appendix C).
Theorem 1. A one-layer fully connected network, when trained with gradient descent on a single
training example X, converges to a solution that makes the following prediction on a test example X:
f(x) = Πk(x) + RΠ⊥(x),	(1)
3
Published as a conference paper at ICLR 2020
Input
1-layer
3-layer
5-layer
7-layer
9-layer
11-layer
12-layer
14-layer
15-layer
16-layer
18-layer
19-layer
20-layer
Figure 3: Visualization of predictions from CNNs trained on a single example. The first row
shows the single training example (7) and a set of test inputs. Each row below shows the output of a
CNN whose depth is indicated to the left of the row. The hidden layers of the CNN consist of 5 × 5
convolution filters organized as 128 channels.
where x = Πk (x) +Π⊥(x) decomposes x into orthogonal components that are parallel and perpen-
dicular to the training example X, respective. R is a random matrix from the network initialization,
independent of the training data.
For test examples similar to the training example—i.e., where Πk (x) dominates Π⊥ (x)—the out-
puts resemble the training output; on the other hand, for test examples that are not highly correlated
to the training example, Π⊥ (x) dominates and the outputs looks like white noise due to the ran-
dom projection by R. The behavior can be empirically verified from the second row in Figure 2.
Specifically, the first test example is a mixture of the training and an unseen test example, and the
corresponding output is a mixture of white noise and the training output. For the remaining test
examples, the outputs appear random.
Although Theorem 1 characterizes only the 1-layer linear case, the empirical results in Figure 2
suggest that shallow (2 layer) networks tend to have this inductive bias. However, this inductive bias
does not miraculously obtain good generalization: the trained model fails to learn either the identity
or the constant function. Specifically, it predicts well in the vicinity (measured by correlations) of
the training example X, but further away its predictions are random. In particular, when the test
example X is orthogonal to X, the prediction is completely random. In deeper (6 layer) nets, the
deviations from the identity function take on a quite different characteristic form.
Interestingly, deeper linear networks behave more like deeper ReLU networks, with a strong bias
towards a constant function that maps any input to the single training output. A multilayer linear
network with no hidden-layer bottleneck has essentially the same representational power as a 1-layer
linear network, but gradient descent produces different learning dynamics that alter the inductive
biases. See Appendix E for more results and analysis on FCNs.
4
Published as a conference paper at ICLR 2020
rho: train-test correlation
correlation to identity function
5-
7-□i
9-	□□
11- 1■■■■■■!
12-■■■■■■■■!
16-■■■■■■■■■■1
18-■■■■■■■■■■■!
19-■■■■■■■■■■■!
20.........∖
o'o 0'2 0'3 0'5 o'β o'β 0'9
1.0
0.8
0.6
o.4
0.2
0.0
rho: train-test correlation
Figure 4: Predictions of CNNs on test examples at different angles to the training image. The
horizontal axis shows the train-test correlation, while the vertical axis indicate the number of hidden
layers for the CNNs being evaluated. The heatmap shows the similarity (measured in correlation)
between the model prediction and the reference function (the constant or the identity function).
3.2 Convolutional networks
We next study the inductive bias of convolutional neural networks with ReLU activation functions.
Figure 3 shows predictions on various test patterns obtained by training CNNs of varying depths.
Compared to FCNs, CNNs have strong structural constraints that limit the receptive field of each
neuron to a spatially local neighborhood and the weights are tied and being used across the spatial
array. These two constraints match the structure of the identity target function. (See Appendix B.3
for an example of constructing the identity function with CNNs.) Similar to the fully connected
case, for one-layer CNN, we can bound the error as follows (proof in Appendix D).
Theorem 2. A one-layer convolutional neural network can learn the identity map from a single
training example with the mean squared error over all output pixels bounded as
MSE ≤O (m"/C Tr
(2)
where mis the number of network parameters, C is the number of channels in the image, and
r≤ m/C is the rank of the subspace formed by the span of the local input patches.
The error grows with m, the number of parameters in the network. For example, learning CNNs
with larger receptive field sizes will be harder. Even though the bound seems to decrease with more
(input and output) channels in the image, note that the number of channels C also contributes to
the number of parameters (m= KHKWC2), so there is a trade-off. Unlike typical generalization
bound that decays with number of i.i.d. training examples, we have only one training example here,
and the key quantity that reduces the bound is the rank rof the subspace formed by the local image
patches. The size of the training image implicitly affects bounds as larger image generates more
image patches. Note the rank r also heavily depends on the contents of the training image. For
example, simply padding the image with zeros on all boundaries will not reduce the error bound.
With enough linearly independent image patches, the subspace becomes full rank r = m/C, and
learning of the global identity map is guaranteed.
The theorem guarantees only the one-layer case. Empirically—as shown in Figure 3—CNNs with
depth up-to-5 layers learn a fairly accurate approximation to the identity function, with the exception
of a few artifacts at the boundaries. For a quantitative evaluation, we measure the performance by
calculating the correlation (See Appendix J for the results in MSE.) to two reference functions: the
identity function and the constant function that maps every input to the training point x. To examine
how a model’s response varies with similarity to the training image, we generate test images having
correlation ρ ∈ [0, 1] to the training image by: (1) sampling an image with random pixels, (2) adding
ax to the image, picking α such that the correlation with X is ρ; (3) renormalizing the image to be
of the same norm as X. For P = 0, the test images are orthogonal to X, whereas for P = 1, the test
images equal X. The results for CNNs of different depths are shown in Figure 4. The quantitative
findings are consistent with the visualizations: shallow CNNs are able to learn the identity function
from only one training example; very deep CNNs bias towards the constant function; and CNNs
of intermediate depth correlate well with neither the identity nor the constant function. However,
5
Published as a conference paper at ICLR 2020
Error rate
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0 1 2	3 4 5 6 7 8 9 10 11 12 13 14 15 U 17 lβ 19 20
Uyer Index
Figure 5:	Illustration of the collapse of predic-
tive power as function of layer’s depth. Error
rate is measured by using the representations com-
puted at each layer to a simple averaging based
classifier on the MNIST test set. The error rate at
each layer is plotted for a number of trained CNNs
of different depth. The thick red line shows the
curve ofan untrained 20-layer CNN for reference.
unlike FCNs that produce white-noise-like predictions, from Figure 3 CNNs of intermediate depth
behave like edge detectors.
To evaluate how much information is lost in the intermediate layers, we use the following simple
criterion to assess the representation in each layer. We feed each image in the MNIST dataset
through a network trained on our single example. We collect the representations at a given layer and
perform a simple similarity-weighted classification. For each example xi from the MNIST test set,
we predict its class as a weighted average of the (one hot) label vector of each example xj from the
MNIST training set, where the weight is the inner-product of xi and xj .
This metric does not quantify how much information is preserved as the image representation prop-
agates through the layers, because the representation could still be maintained yet not captured by
the simple correlation-weighted classifier. It nonetheless provides a simple metric for exploring the
(identity) mapping: using the input image as the baseline, if a layer represents the identity function,
then the representation at that layer would obtain a similar error rate when using the input represen-
tation; on the other hand, if a layer degenerates into the constant function, then the corresponding
representation would have error rate close to random guessing of the label. The results are plotted
in Figure 5. The error curve for a randomly initialized 20-layer CNN is shown as reference: at
random initialization, the smoothing effect renders the representations beyond the sixth layer un-
useful for the averaging-based classifier. After training, the concave nonmonotonicity in the curves
indicates loss and then recovery of the information present in the input. Trained networks try to
recover the washed out intermediate layer representations as means to link the input and the output
layer. However, if the depth is too large, the network tries to infer input-output relations using partial
information, resulting in models that behave like edge detectors. Finally, for the case of 20 layers,
the curve shows that the bottom few layers do get small improvements in error rate comparing to
random initialization, but the big gap between the input and output layer drives the network to learn
the constant function instead. On a first sight, this deems to underscore a vanishing gradient prob-
lem, but Figure 1 reveals that given a sufficient number of training examples, a 20-layer CNN can
still learn the identity map. See also Appendix G for further discussions on vanishing gradients.
Since CNNs preserve spatial structure, we can also visualize information loss in the intermediate
layers. The visualization results, described in Appendix F, are consistent with the aforementioned
observations.
3.3 Robustness to changes in input scale
Whether a relatively shallow CNN learns the identity function from a single training example or a
relatively deep CNN learns the constant function, both outcomes reflect an inductive bias because the
training objective never explicitly mandates the model to learn one structure or another. The spatial
structure of CNNs enables additional analyses of the encoding induced by the learned function. In
particular, we examined how changing the size of the input by scaling the dimensions of the spatial
map affects the model predictions. Figure 6 depicts the predictions of a 5-layer CNN trained on
a 28 × 28 image and tested on 7 × 7 and 112 × 112 images. Although the learned identity map
generally holds up against a larger-than-trained input, the identity map is disturbed on smaller-than-
trained inputs. See Appendix H.1 for a comprehensive description of the results. Note that CNNs
are capable of encoding the identity function for arbitrary input and filter sizes (Appendix B.3).
Figure 7 shows the predictions on the rescaled input patterns of Figure 6 by a 20-layer CNN that has
learned the constant function on a 28 × 28 image. The learned constant map holds up over a smaller
range of input sizes than the learned identity map in a 5-layer CNN. It is nonetheless interesting to
see smooth changes as the input size increases to reveal the network’s own notion of “7”, clearly
6
Published as a conference paper at ICLR 2020
aim U ∙ιιιι∙∙e0
ill
αD∣ι□□ q∙ιιι∙∙e□
Figure 6:	Visualization of a 5-layer CNN on test images of different sizes. The two subfigures
show the results on 7 × 7 inputs and 112 × 112 inputs, respectively.
36
21212
2IZI2
24
28
7∣7∣7"∣7∣7∣7
7 P
~~
212
32
□□


回
212
Figure 7:	Visualization of the predictions of a 20-layer CNN on test images of different sizes
(indicated by the number on each row). The input patterns are the same as in Figure 6 (constructed
in different resolutions), which are not shown for brevity.
defined with respect to the corners of the input map (Figure 21 in Appendix H reveals interesting
details on how the patterns progressively grow from image boundaries in intermediate layers). We
performed additional experiments to directly feed test images to the upper subnet, which reveals
more about the generative process by which the net synthesizes a constant output (Appendix H).
3.4 Varying other factors during training
We studied a variety of common hyperparameters, including image dimensions, convolutional filter
dimensions, number of kernels, weight initialization scheme, and the choice of gradient-based opti-
mizer. With highly overparameterized networks, training converges to zero error for a wide range of
hyperparameters. Within the models that all perform optimally on the training set, we now address
how the particular hyperparameter settings affect the inductive bias. We briefly present some of the
most interesting observations here; please refer to Appendix I for the full results and analyses.
Size of training image. Figure 8 shows, for varying training-image size, the mean correlation to
the constant and the identity function at different depths within the network. The training examples
are resized versions of the same image. The bias toward the constant function at a given depth
increases with smaller training images. This finding makes sense considering that smaller images
provide fewer pixel-to-pixel mapping constraints, which is aligned with Theorem 2.
Size of convolution filters. Figure 9 illustrates the inductive bias with convolution filter size vary-
ing from 5 × 5 to 57 × 57. Predictions become blurrier as the filter size grows. With extremely large
filters that cover the entire input array, CNNs exhibit a strong bias towards the constant function.
Avg correlation to constant
Avg correlation to identity
Figure 8:	Comparing bias towards
constant and identity when trained
with different image sizes. The x-
axis is the depth of the CNNs, while
the y-axis is the mean correlation (av-
erage of each row from the heatmaps
like in Figure 4). Each curve cor-
responds to training with a different
image size.
7
Published as a conference paper at ICLR 2020
Input
3
128
1024
Input
5x5
13x13
25x25
57x57
Figure 9: Visualizing the predictions from 5-layer CNNs with various filter sizes. The first row
shows the single training example (7) and a set of test inputs. Each row below shows the output of
a CNN whose filter size is indicated by the number on the left. See Appendix I for more results.
N■日叨□E1∙III Illl BK□
HiHSBffiHHHEBnaEE
HiElBHΠ∏aH≡KBΞ□
H!ElBHΠ□aH≡nHΞ□
(a) 5-layer
InputElgIIEltIlnIilll ■。::
(b) 20-layer
Figure 10: Visualization of the predictions from CNNs for various number of hidden channels.
The first row shows a single training example (7) and a set of test inputs. Each row below shows the
output of CNNs differing in the number of channels. This number is indicated on the left of each
row; for intermediate layers, the number specifies both the input and output channel count.
Because the training inputs are of size 28 × 28, a 29 × 29 filter size allows each neuron to see at
least half of the spatial domain of the previous layer, assuming large boundary padding of the inputs.
With 57 × 57 filters centered at any location within the image, each neuron sees the entire previous
layer. This is also consistent with Theorem 2, in which the error bound deteriorates as the filter sizes
increases. Note even with a large filter, CNNs do not perform the same elementary computation as
FCNs because the (shared) convolution filter is repeatedly applied throughout the spatial domain.
Number of channels. Appendix B.3 shows that in principle two channels suffice to encode the
identity function for gray-scale inputs via a straightforward construction. However, in practice the
outcome of training may be quite different depending on the channel count, as shown in Figure 10(a).
On the one hand, the aggressively overparameterized network With 1024 channels (~25M Parame-
ters per middle convolution layer) does not seem to suffer from overfitting. On the other hand, the
results with three channels often lose content in the image center. The problem is not underfitting as
the network reconstructs the training image (first column) correctly.
A potential reason why 3-channel CNNs do not learn the identity map well might be poor initial-
ization when there are very few channels (Frankle & Carbin, 2019). This issue is demonstrated
in Figure 29 of Appendix I. Our study of training with different initialization schemes indeed con-
firms that the random initial conditions have a big impact on the inductive bias (Appendix I).
Figure 10(b) shows the case for 20-layer CNNs. Surprisingly, having an order of magnitude more
feature channels in every layer does not seem to help much at making the information flow through
layers, as the network still learns to ignore the inputs and construct a constant output.
4 Conclusions
We presented an systematic study of the extreme case of overparameterization when learning from
a single example. We investigated the interplay between memorization and generalization in deep
8
Published as a conference paper at ICLR 2020
neural networks. By restricting the learning task to the identity function, we sidestepped issues
such as the underlying optimal Bayes error of the problem and the approximation error of the hy-
pothesis classes. This choice also facilitated rich visualization and intuitive interpretation of the
trained models. Under this setup, we investigated gradient-based learning procedures with explicit
memorization-generalization characterization. Our results indicate that different architectures ex-
hibit vastly different inductive bias towards memorization and generalization.
For future work, we plan to extend the study to other domains and neural network architectures, like
natural language processing and recurrent neural networks, and aim for more qualitative relationship
between the inductive bias and various architecture configurations.
Acknowledgments
We would like to thank Kunal Talwar, Hanie Sedghi, and Rong Ge for helpful discussions.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. CoRR, 1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via Over-
Parameterization. CoRR, arXiv:1811.03962, 2018b.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. CoRR, arXiv:1802.05296, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019.
Devansh Arpit, StanisIaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look
at memorization in deep netWorks. In International Conference on Machine Learning, 2017.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Raef Bassily, Mikhail Belkin, and Siyuan Ma. On exponential convergence of SGD in non-convex
over-parametrized learning. CoRR, arXiv:1811.02564, 2018.
Mikhail Belkin, Daniel Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for clas-
sification and regression rules that interpolate. In Advances in Neural Information Processing
Systems, 2018.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In ICLR, 2018.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. CoRR, arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. CoRR, arXiv:1810.02054, 2018b.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In UAI, 2016.
Vitaly Feldman. Does learning require memorization? a short tale about a long tail. arXiv preprint
arXiv:1906.05271, 2019.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In ICLR, 2019.
9
Published as a conference paper at ICLR 2020
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850, 2013.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
CoRR, arXiv:1710.05468, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-
gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 4681-4690, 2017.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In NIPS, 2018.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geome-
try, and complexity of neural networks. CoRR, arXiv:1711.01530, 2017.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. CoRR, arXiv:1412.6614, 2014.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to
Spectrally-Normalized margin bounds for neural networks. In ICLR, 2018.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? CoRR, arXiv:1812.10004, 2018.
Adityanarayanan Radhakrishnan, Mikhail Belkin, and Caroline Uhler. Downsampling leads to im-
age memorization in convolutional autoencoders. CoRR, arXiv:1810.10333, 2018.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In International Conference on Learning
Representations, 2014.
Vatsal Shah, Anastasios Kyrillidis, and Sujay Sanghavi. Minimum norm solutions do not always
generalize well for over-parameterized problems. CoRR, arXiv:1811.07055, 2018.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70),
2018.
10
Published as a conference paper at ICLR 2020
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Non-vacuous
generalization bounds at the ImageNet scale: a PAC-Bayesian compression approach. In ICLR,
2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. CoRR, arXiv:1811.08888, 2018.
11
Published as a conference paper at ICLR 2020
A	Experiment details and hyper-parameters
We specify the experiment setups and the hyper-parameters here. Unless otherwise specified in each
study (e.g. when we explicitly vary the number of convolution channels), all the hyper-parameters
are set according to the default values here.
The main study is done with the MNIST dataset. It consists of grayscale images of hand written
digits of size 28 × 28. For training, we randomly sample one digit from the training set (a digit
‘7’) with a fixed random seed. For testing, we use random images from the test set of MNIST and
Fashion-MNIST, as well as algorithmically generated structured patterns and random images. The
training and test images are all normalized by mapping the pixel values in {0, 1, . . . , 255} to [0, 1],
and then standardize with the mean 0.1307 and standard deviation 0.3081 originally calculated on
the (full) MNIST training set.
The models are trained by minimizing the mean squared error (MSE) loss with a vanilla SGD (base
learning rate 0.01 and momentum 0.9). The learning rate is scheduled as stagewise constant that
decays with a factor of 0.2 at the 30%, 60% and 80% of the total training steps (2,000,000). No
weight decay is applied during training.
For neural network architectures, the rectified linear unit (ReLU) activation is used for both fully
connected networks (FCNs) and convolutional networks (CNNs). The input and output dimensions
are decided by the data. The hidden dimensions for the FCNs are 2,048 by default. The CNNs
use 5 × 5 kernels with stride 1 and padding 2, so that the geometry does not change after each
convolution layer.
B	Representation of the identity function using deep networks
In this section, we provide explicit constructions on how common types of neural networks can
represent the identity function. Those constructions are only proof for that the models in our study
have the capacity to represent the target function. There are many different ways to construct the
identity map for each network architecture, but we try to provide the most straightforward and
explicit constructions. However, during our experiments, even when the SGD learns (approximately)
the identity function, there is no evidence suggesting that it is encoding the functions in similar ways
as described here. We put some mild constraints (e.g. no “bottleneck” in the hidden dimensions)
to allow more straightforward realization of the identity function, but this by no means asserts that
networks violating those constraints cannot encode the identity function.
B.1	Linear models
For a one-layer linear network f(x) = Wx, where W ∈ Rd×d, setting W to the identity matrix
will realize the identity function. For a multi-layer linear network f (x) = (Q' W')x, We need to
require that all the hidden dimensions are not smaller than the input dimension. In this case, a simple
concrete construction is to set each w` to an identity matrix.
B.2	Multi-layer ReLU networks
The ReLU activation function σ(∙) = max(0, ∙) discards all the negative values. There are many
ways one can encode the negative values and recover it after ReLU. We provide a simple approach
that uses hidden dimensions twice the input dimension. Consider a ReLU network with one hidden
layer f(x) = W2σ(W1x), where W2 ∈ Rd×2d, W1 ∈ R2d×d. The idea is to store the positive and
negative part of x separately, and then re-construct. This can be achieved by setting
W1 = -IId	,	W2 = (Id -Id)
where Id is the d-dimensional identity matrix. For the case of more than two layers, we can use the
bottom layer to split the positive and negative part, and the top layer to merge them back. All the
intermediate layers can be set to 2d-dimensional identity matrix. Since the bottom layer encode all
the responsives in non-negative values, the ReLU in the middle layers will pass through.
12
Published as a conference paper at ICLR 2020
B.3	Convolutional networks
In particular, we consider 2D convolutional networks for data with the structure of multi-channel
images. A mini-batch of data is usually formatted as a four-dimensional tensor of the shape
B × C × H × W , where B is the batch size, C the number of channels (e.g. RGB or feature
channels for intermediate layer representations), H and W are image height and width, respectively.
A convolutional layer (ignoring the bias term) is parameterized with another four-dimensional ten-
Sor of the shape C X C X KH X KW, where C is the number of output feature channels, KH and
KW are convolutional kernel height and width, respectively. The convolutional kernel is applied at
local KH X KW patches of the input tensor, with optional padding and striding.
For one convolution layer to represent the identity function, we can use only the center slice of the
kernel tensor and set all the other values to zero. Note it is very rare to use even numbers as kernel
size, in which case the “center” of the kernel tensor is not well defined. When the kernel size is odd,
we can set
W _ (1 C = c, h = bKH/2c, W = bKW/2c
W cchw = ∖c	♦一
0 otherwise
By using only the center of the kernel, we essentially simulate a 1 X 1 convolution, and encode a
local identity function for each (multi-channel) pixel.
For multi-layer convolutional networks with ReLU activation functions, the same idea as in multi-
layer fully-connected networks can be applied. Specifically, we ask for twice as many channels as
the input channels for the hidden layers. At the bottom layer, separately the positive and negative
part of the inputs, and reconstruct them at the top layer.
C Proof of Theorem 1
Consider the 1-layer linear model fw (x) = Wx, where X ∈ Rd and W ∈ Rd×d. Let X be the single
training example. The training objective is to minimize the empirical risk R = 1/2kfW (X) - Xk22.
The optimization problem is convex and well understood. Due to overparameterization, the solution
of the empirical risk minimization is not unique. However, given randomly initialized weights W0,
gradient descent obtains a unique global minimizer.
The gradient of the empirical risk is
∂R^
∂W
(W — I )XX>
(3)
Gradient descent with step sizes ηt and initialization weights W0 updates weights as
T
W T = W 0 — X ηt(W t-1 — I )xx>
t=1
:=W 0 + UT X>
where uT ∈ Rd is a vector decided via the accumulation in the optimization trajectory. Because of
the form of the gradient, it is easy to see the solution found by gradient descent will always have
such parameterization structure. Moreover, under this parameterization, a unique minimizer exists
that solves the equation
X = fw (X) = W 0X + ux>X
via
(I — W 0)X
(4)
U
商2
Therefore, the global minimizer can be written as
fw (x) = (W 0 + UX>)x
σ∙ σ∙	c / rr rr
=kwι∙x+W (x-kwι∙X
13
Published as a conference paper at ICLR 2020
For the one-layer network case, the optimization problem is convex. Under standard conditions in
convex optimization, gradient descent will converge to the global minimizer shown above.
We can easily verify that the red term is exactly the projection of X onto the training example x,
while the blue term is the residual projection onto the orthogonal subspace.
D	Proof of Theorem 2
Lemma 1. Consider the linear model f(x) = W>x, where x ∈ RD, W ∈ RD×d. Let the training
set be {(x1, y1), . . . (xN, yN)}. Assume the model is overparameterized (N ≤ d) and there is no
sample redundancy (rank of the data matrix is N). Fitting f (∙) by optimizing the square loss with
gradient descent on the training set converges to f(x) = W>x, where
W = W0 + X>(XX>)-1(Y - XW0).	(5)
W0 ∈ RD×d is the random initialization of weights, X = (x1, . . . , xN)> ∈ RN×D and Y =
(y1, . . . , yN)> ∈ RN×d are the matrices formed by the inputs and outputs, respectively.
Lemma 2. With the same notation of Lemma 1, assume the model is underparameterized (N > d)
and there is no feature redundancy (rank of the data matrix is d). Fitting f (∙) by optimizing the
square loss with gradient descent on the training set converges to f(x) = W>x where
W= (X >X )-1 X >Y,	(6)
Proof of Theorem 2. Let X ∈ RH ×w ×c be the single training image of size H X W, and C color
channels. Let KH X KW be the convolution receptive field size, so the weights of convolution filters
can be parameterized via a 4-dimensional tensor Θ ∈ RKH×KW×C×C. Let Ξ be the collection of
2D coordinates of the local patches from the input image that the convolutional filter is applied to,
and Pij (X) ∈ RKH ×KW ×c be the local patch of X centered at the coordinate (i,j) ∈ Ξ. Note for our
case the input and output are of the same shape, so the convolution stride is one, and ∣Ξ∣ = H X W.
The empirical risk of fitting X with a one-layer convolution model can be written as
1C
R = 2 E £(he：：：k, Pij(Xyi-Xijk),
(i,j)∈Ξ k=1
(7)
where ㊀二加 ∈ RKH×KW×C is the subset of convolution weights corresponding to the k-th output
channel, and Xijk is the pixel value at coordinate (i,j) and channel k.
Case 1 — overparameterization: ∣Ξ∣ ≤ KH X KW X C X C. Assumme the patches are linearly
independent1 , with slight abuse of notatons, we represent the empirical risk in matrix form with the
1When two patches are identical, or more generally, when there is a patch (i, j) ∈ Ξ, which can be written
as a linear combination of other patches:
∃αij ,s.t.	Pij(X)= E	αijpj(X)
(i,j) = (i,j)
Note because each local patch contains the target pixel: Xijk ∈ Pij (X), ∀k ∈ [C], so for all k ∈ [C], the same
coefficients {αj } can be used to written Xijk as the same linear combination of the other (corresponding)
pixels. Therefore, we can find a linearly independent basis for the patches and rewrite the fitting objectives
with the basis.
14
Published as a conference paper at ICLR 2020
following matrices2:
W := (θ:::i,..., Θ:::C) ∈ R(KH KW C)×C,
X := (...,Pij (X),...)> ∈ Riξi×(KH KW C),
/ .	.	. ∖
..	.
..	.
Y := Xiji ...	XijC ∈ Riξi×c,
..	.
..	..	..
and apply Lemma 1 to obtain W. For a test example x, the prediction of the (i, j)-th (multi-channel)
pixel is
W>Pij(x) = (W0 + X>(XXT)T (Y - XW0))> Pij(χ)
=W0T (i - XT (XXT)T X) Pij(x) + Yt(XXt)-1XPij(x)
Note we are learning the identity map, the (i, j )-th row of the learning target matrix Y is the
(multi-channel) pixel at the (i,j)-th coordinate of X. This is exactly the center of the (i,j)-th
patch Pij (X), i.e. the (i,j)-th row of the matrix X. As a result, there is a linear projection ma-
trix Λ ∈ R(KH KW C)×C that maps X to Y: XΛ = Y. So
W TPij (x) = W0TnX Pij (x) + Λτ∏X Pij (x)
where ΠkX = XT(XXT)-1X is the linear operator that projects a vector into the subspace spanned
by the rows of X, and Π⊥X = I - ΠkX is the operator projecting into the orthogonal subspace.
To compute the prediction errors, it is suffice to look at the errors at each (i, j)-th (multi-channel)
pixel separately:
Error2j = k∕(x)j - ΛτPj∙(x)k2
= W0TΠ⊥XPij(x)+ΛTΠkXPij(x)-ΛTPij(x)2
= (W 0 -Λ)TΠ⊥XPij(x)2
≤ ((IlW0TnXlI+ ∣∣ΛTnXlI) kPij(x)k)2.
Assume the absolute value of pixel values are bounded by B, then
kPij(x)k ≤ BpKHKWC.	(8)
The first two terms can be bounded according to the rank r of the subspace projection nkX . Let the
nullity of the projection be n = KHKWC - r. Note the projection matrix can be decomposed as
n
n⊥X =	vkvkT
k=1
where {vk}k is a orthonormal basis for the projection subspace.
ln	l
kΛTn⊥Xk = llΛT XvkvkT ll
l	k=1	l
= tuXn kΛTvk k2	(Orthonormality of {vk }k)
k=1
≤，nMk2	(SUb-multiplicativity of the Frobenius norm)
=√nC.	(9)
2In particular, W is used for both the “width” of the image and the parameters of the linear system. But
the meaning should be clear from the context. The multi-dimensional tensors ®：：：k and Pij (X) are used inter-
changeably with their flatten column vector counterparts.
15
Published as a conference paper at ICLR 2020
Similarly,
W0>Π⊥X	=
n
XkW0>vkk2
k=1
∖
Assume the entries of W0 are initialized as i.i.d. Gaussians N(0, σ2). Since {vk}k are orthonormal
vectors, for each k, let Pk = ∣∣ W0>vk ∣∣2∕σ2, then Pk is distributed according to χ2 distribution with
C degree of freedom. For any 1 < ζ ≤ M, where M is a constant chosen a priori, and for each
k = 1, . . . , n,
P (Pk ≥ ζC) ≤ e-ζtCE etρk ,	(Markov’s inequality)
= e-ζtC(1 - 2t)-C/2, ∀t < 1/2,	(MGF of χ2)
= exp(-C(ζt+ 1/2 log(1 - 2t))) ,
≤ exp(-c∕2(Z — 1 + log(1∕ζ))),	(optimal at t? = 1∕2(l — 1∕ζ))
≤ exp (一c∕2(Z — 1 + log(1/M))).
By union bound,
P (∃k ∈{1,..., n} ： Pk ≥ ZC) ≤ nexp(—c∕2(Z — 1 + log(1∕M))).	(10)
Let δ equals the right hand side, and solve for ζ, we get for any δ ≥ δ0 > 0, with probability at least
1 — δ
∀k : Pk < 2log (n) + C (1 + log M)	(11)
where δ0 ≥ 2n∕(C(M — log M — 1)) is chosen to satisfy ζ ≤ M. We can also choose δ0 first and
set M accordingly.
Putting everyting together, with probability at least 1 — δ, the mean squared error (averaged over all
output pixels)
Error2
(12)
Note KHKWC2 is the number of parameters in the convolution net.
Case 2 — underparameterization: ∣Ξ∣ > KH X KW X C X C. Using the same notation above,
assuming no redundant features, we apply Lemma 2 to get the prediction of the (i, j)-th (multi-
channel) pixel of a test example x as
W >Pij (x) = Y >X (X > X )-1 Pij (x)
= Λ>X>X(X>X)-1Pij(x)
= Λ>Pij(x)
Recall the definition of Λ, which maps a patch to the corresponding (multi-channel) pixel. In this
case, the prediction is exact, and the error is zero. Since in this case n, this case can be merged with
equation 12.	口
Proof of Lemma 1. The proof is an extension of Theorem 1 to the case of more than one training
examples. Using the notation in the Lemma, the training objective can be written as the matrix form
RR= 2 ∣XW — Y∣2,
16
Published as a conference paper at ICLR 2020
and the gradient as
∂R
∂W
X>(XW -Y).
Since the model is overparameterized, and there is no unique minimizer to the empirical risk. How-
ever, gradient descent converges to a unique solution. Note the gradient descent step is:
Wt
Wt-1 - ηtX>(XWt-1 - Y)
t-1
W0 - X ητX>(XWτ - Y )
τ=1
W0 + X>Ut,
Wt-1
∂R
-ηt ∂W
W =Wt-1
where Ut = - Ptτ-=11 ητ(XWτ -Y ) ∈ RN×d parameterizes the solution at iteration t. Since XX>
is invertible in this case. A unique solution exists under this parameterization, which can be obtained
by solving
X (W 0 + X >U) = Y
⇒ U = (XX>)-1 (Y - XW0).
Plug this into the parameterization, we get
W = W0 + X> (XX>)-1 (Y - XW0).	(13)
□
Proof of Lemma 2. Using the same notation as in the proof of Lemma 1, since the model is under-
parameterized, a unique minimizer of the empirical risk exists. Directly solving for the optimality
condition ∂R∕∂W = 0, We get
W= (X >X )-1 X >Y.	(14)
□
E	Full results of fully connected multi-layer networks
In this section, We present the detailed results on fully connected netWorks that are omitted from
Section 3.1 due to space limit.
E.1 Fully connected linear networks
Figure 11 shoWs the results on multi-layer linear netWorks With various number of hidden layers
and hidden units. The depth of the architecture has a stronger effect on the inductive bias than the
Width. For example, the netWork With one hidden layer of dimension 2048 has 3.2M parameters,
more than the 2.5M parameters of the netWork With three hidden layers of dimension 784. But the
latter behaves less like the convex case.
E.2 Two-layer fully connected ReLU networks
Li & Liang (2018) offer a theoretical characterization of learning in a tWo-layer ReLU neural net-
Work. They shoW that When the data consists of Well separated clusters (i.e., the cluster diameters
are much smaller than the distances betWeen each cluster pair), training an overparameterized tWo-
layer ReLU netWork Will generalize Well. To simplify the analysis, they study a special case Where
the Weights in the top layer are randomly initialized and fixed; only the bottom layer Weights are
learned.
17
Published as a conference paper at ICLR 2020
(a) Hidden dimension 784 (= input dimension)
(b) Hidden dimension 2048
Figure 12: Visualization of predictions from two-layer ReLU networks. The first row shows the
input images for evaluation, including the single training image “7” at the beginning of the row.
The remaining rows shows the predictions from trained models with hidden dimension 2,048 and
16,384, repectively.
Figure 11: Visualization of predictions from trained multi-layer linear networks. The first row
shows the input images for evaluation, including the single training image “7” at the beginning of
the row. The remaining rows shows the prediction from a trained linear network with 1, 3, and 5
hidden layers, respectively.
We study the problem of learning a two-layer ReLU network under our identity-mapping task. Fig-
ure 12 compares the cases of learning the bottom layer only and learning both layers (left and right
panels, respectively). The two cases demonstrate different inductive biases for predictions on unseen
test images. When only the first layer is trained, the tendency is toward speckled noise, but when
both layers are trained, the tendency is toward a constant output (the image used for training). Our
observation does not contradict the theoretical results of Li & Liang (2018), which assume a well
separated and clustered data distribution.
For bottom-layer only training, we can analyze it in a similar way to the one-layer networks. Let us
denote
fW (x) = hα, ReLU(z)i, z = Wx	(15)
where W ∈ Rm×d is the learnable weight matrix, and α ∈ Rm is randomly initialized and fixed.
Although the trained weights no longer have a closed-form solution, the solution found by gradient
descent is always parameterized as
W T = W 0 + UT x>
(16)
where X is the training example, and UT ∈ Rm summarizes the efforts of gradient descent up to time
T. In particular, the gradient of the empirical risk R with respect to each row W:r of the learnable
weight is
„ ʌ	_ ʌ	„ ʌ
∂R	∂R ∂zr	∂R 人 >
-	=-------=-——x
dW：r----------∂Zr dW：r-∂Zr
Putting it together, the full gradient is
, ʌ _ ʌ
∂R _ ∂R
∂W = ∂z
(17)
(18)
Since the gradient lives in the span of the training example X, the solution found by gradient descent
is always parameterized as equation 16.
18
Published as a conference paper at ICLR 2020
Figure 13: Visualization of predictions from multi-layer ReLU networks. The first row shows
the input images for evaluation, including the single training image “7” at the beginning of the row.
The remaining rows show the predictions from trained multi-layer ReLU FCNs with 1, 3, and 9
hidden layers.
correlation to constant function
correlation to constant function
0.0	0.2	0.3	0.5	0.6	0.8	0.9
AqUoPPɪ#
(a) ReLU FCNs	(b) Linear FCNs
Figure 14: Quantitative evaluation of the learned model on randomly generated test samples
at various angles (correlation) to the training image. The horizontal axis shows the train-test cor-
relation, while the vertical axis indicate the number of hidden layers for the FCNs being evaluated.
The heatmap shows the similarity (measured in correlation) between the model prediction and the
reference function (the constant function or the identity function). (a) shows the results for FCNs
with the ReLU activation function; (b) shows the results for linear FCNs.
The same arguments applies to multi-layer neural networks. The prediction on any test example that
is orthogonal to X will depend only on randomly initialized W0 and upper layer weights. When only
the bottom layer is trained, the upper layer weights will also be independent from the data, therefore
the prediction is completely random. However, when all the layers are jointly trained, the arguments
no longer apply. The empirical results presented in the main text that multi-layer networks bias
towards the constant function verify this.
In particular, if the test example is orthogonal to X (i.e., X> X = 0), the prediction depends solely
on the randomly initialized values in W0 and therefore can be characterized by the distribution used
for parameter initialization.
However, when both layers are trained, the upper layer weights are also tuned to make the prediction
fit the training output. In particular, the learned weights in the upper layer depend on W0 . Therefore,
the randomness arguments shown above no longer apply even for test examples orthogonal to X. As
the empirical results show, the behavior is indeed different.
E.3 Nonlinear multi-layer fully connected networks
In this section, we consider the general case of multilayer fully connected networks (FCNs) with
ReLU activation functions. Figure 13 visualizes predictions from trained ReLU FCNs with 1, 3, or 9
19
Published as a conference paper at ICLR 2020
Figure 15: Visualization the intermediate layers of CNNs with different number of layers. The
first column shows a randomly initialized 20-layer CNN (random shallower CNNs look similar to
the truncation of this). The rest of the columns show the trained CNNs with various number of
layers.
hidden layers. The deepest network encodes the constant map with high confidence; the shallowest
network shows behavior similar to that of a one-layer linear net. Quantitative evaluations of the
behaviors are shown in Figure 14, computed in the same way as Figure 4 for CNNs (see Section 3.2).
The results for linear FCNs are also shown for comparison. The linear and ReLU FCNs behave
similarly when measuring the correlation to the identity function: neither of them performs well for
test images that are nearly orthogonal to X. For the correlation to the constant function, ReLU FCNS
overfit sooner than linear FCNs when the depth increases. This is consistent with our previous visual
inspections: for shallow models, the networks learn neither the constant nor the identity function, as
the predictions on nearly orthogonal examples are random.
F	Visualization of the intermediate layer representations for
CNNS
Unlike the FCNs, CNNs preserve the spatial relation between neurons in the hidden layers, so we
can easily visualize the intermediate layers as images in comparison to the inputs and outputs, to
gain more insights on how the networks are computing the functions layer-by-layer. In Figure 15,
we visualize the intermediate layer representations on some test patterns for CNNs with different
depths. In particular, for each example, the outputs from a convolutional layer in an intermediate
layer is a three dimensional tensor of shape (#channel, height, width). To get a compact visualization
for multiple channels in each layer, we compute flatten the 3D tensor to a matrix of shape (#channel,
height × width), compute SVD and visualize the top singular vector as a one-channel image.
In the first column, we visualize a 20-layer CNN at random initialization3. As expected, the ran-
domly initialized convolutional layers gradually smooth out the input images. The shape of the
input images are (visually) wiped out after around 8 layers of (random) convolution. On the right of
the figure, we show several trained CNNs with increasing depths. For a 7-layer CNN, the holistic
structure of inputs are still visible all the way to the top at random initialization. After training, the
network approximately renders an identity function at the output, and the intermediate activations
also become less blurry. Next we show a 14-layer CNN, which fails to learn the identity function.
3Shallower CNNs at random initialization can be well represented by looking at a (top) subset of the visu-
alization.
20
Published as a conference paper at ICLR 2020

7 1	W
7 ^^T1Λ∙■网■■■■
，《"・，・ BIIIIHB∙∙
，9<riJa ∙ι川■■••
7 W<FI^∙ Hllll ■■••
■■■■■■■■■■■■■
HBSQBEl∣aH≡EHΞ□
QDBBHKiaHIIII IiSn
□αsEa□naHim[B^ :□
DBSEa□nιι EnnEa□□
□DSQU□auιπιnH□□
BDSQUUHUOUB□□
O□SB□GII□θnHΞ□
□□SQ□□B□≡Γ1B□□
HBSQHnaH≡nHΞ□
Figure 16:	Visualizing the intermediate layers of a trained 7-layer CNN. The three subfigures
show for each layer: 1) the top singular vector across the channels; 2) the channel that maximally
correlate with the input image; 2) a random channel, respectively.
QBSBHKiaHIIII ■：：□
□QSQΠEJDB≡EH^□
□DSQUuαuEuazj□
DDSQUUaUIUK□□□
DDS□UQa□ODU□□
nDB□UEJHUL]UJU□
□□H□UL]aUUUH□□
□□Q□UUH□□UH□□
□□S□U□α□□EM□□
□□s□uuαu∣Linfl^□
Q□S□B□a□[EK]HΞ□
HQSQQEaaiS]∣llKJH≡□
HaSOBHBHHHHHH
HDSBHHIIHiHIGHHB
QDSHHKiaHIIII ■：：□
HaSSHaaBIIIIHHHB
■«■■■■■■■■■■■
ααsQ□□α□ιπi[]≡r]2
■■■■■■■■■■■■a
■■■■■■■■■■■■■
nQQQ□aa□πιiEJ□na
■■■■■■■■■■■■■
■■■■■■■■■■■■■
DQSHBUilUl^nUCQ
W■■■■■■■■■
hhbqbbbħħbss□
El”■■■■■■■■■■
HBSHHHIIHiHIGHHB
Figure 17:	Visualizing the intermediate layers of a trained 14-layer CNN. The three subfigures
show for each layer: 1) the top singular vector across the channels; 2) the channel that maximally
correlate with the input image; 2) a random channel, respectively.
However, it manages to recover meaningful information in the higher layer activations that were
(visually) lost in the random initialization. On the other hand, in the last column, the network is so
deep that it fails to make connection from the input to the output. Instead, the network start from
scratch and constructs the digit ‘7’ from empty and predict everything as ‘7’. However, note that
around layer-8, we see the activations depict slightly more clear structures than the randomly ini-
tialized network. This suggests that some efforts have been made during the learning, as opposed to
the case that the bottom layers not being learned due to complete gradient vanishing. Please refer to
Appendix G for further details related to potential gradient vanishing problems.
Two alternative visualizations to the intermediate multi-channel representations are provided that
show the channel that is maximally correlated with the input image, and a random channel (channel
0). Figure 16, Figure 17 and Figure 18 illustrate a 7-layer CNN, a 14-layer CNN and a 20-layer
CNN, respectively.
G Measuring the change in weights of layers post training
In this section, we study the connection between the inductive bias of learning the constant function
and the potential gradient vanishing problem. Instead of measuring the norm of gradient during
training, we use a simple proxy that directly compute the distance of the weight tensor before and
after training. In particular, for each weight tensor W0 at initialization and W? after training, we
compute the relative `2 distance as
d(W0, W?) :
∣∣W0 - W?k
kW0 k
The results for CNNs with various depths are plotted in Figure 19(a). As a general pattern, we do
see that as the network architecture gets deeper, the distances at lower layers do become smaller.
But they are still non-zero, which is consistent with the visualization in Figure 15 showing that even
21
Published as a conference paper at ICLR 2020
QBBBOUaHIIII
□QSQΠE]aB≡[H^Ξ
DDBB□nDB≡CH□□
Q□SQ□nαBHI]E≡□□
口口日 口口 口。口匚 s□□
□□S□□DO□OE≡□□
□□s□□□α□nc∣i□□
■■■■■■■■■■■■■
Ilw■■■■■■■■
■■■■■■■■■■■■■
■■LJULLHLLLULL
IJLJ匚LJLJ匚■匚匚匚・匚LJ
QDESBEHEEEHEa
HHBHHEiaillllllIIIIII IlSn
■■■■■■■■■■■■■
HBHBHBHHiniHHSH
■■■■■■■■■■■■■
“”■■■■■■■■■
ΠDSQ□∏α∏01E]i]∏□
IlLIafl 0∣ιEI∙∙nι UHI■■
■■■■■■■■■■■■■
□□□□□□□□□□□□□
UUUUUUUUUUUUU
」」_u」」」_u_u_u
□□□□□□□□□□□□□
■■■■■■■■■■■■■
□□□□□□□□□□□□□
■■■■■■■■■■■■■
■■■■■■■■■■■■■
■■■■■■■■■■■■■
■■■■■■■■■■■■■
Figure 18:	Visualizing the intermediate layers of a trained 20-layer CNN. The three subfigures
show for each layer: 1) the top singular vector across the channels; 2) the channel that maximally
correlate with the input image; 2) a random channel, respectively.
5	10	15	20
layer index
(b) Linear FCNs
(c) ReLU FCNs
(a) CNNs
Figure 19:	The relative `2 distance of the weight tensors before and after training at each
layer. The curves compare models at different depth. Most of the networks have significantly larger
distances on the top-most layer. To see a better resolution at the bottom layers, we cut off the top
layer in the figures by manually restricting the y axis.
22
Published as a conference paper at ICLR 2020
7
α ιιιιιιι hi
III
π□□π
sξħħ

m
Figure 20:	Visualization of a 5-layer CNN on test images of different sizes. Every two rows show
the inputs and model predictions. The numbers on the left indicate the input image size (both width
and height).
for the 20-layer CNN, where the output layer fits to the constant function, the lower layers does get
enough updates to allow them to be visually distinguished from the random initialization.
In Figure 19(b) and (c), we show the same plots for linear FCNs and FCNs with ReLU activation,
respectively. We see that especially for ReLU FCN with 11 hidden layers, the distances for the
weight tensors at the lower 5 layers are near zero. However, recall from Figure 13 in Section E.3,
the ReLU FCNs start to bias towards the constant function with only three hidden layers, which are
by no means suffering from vanishing gradients as the plots here demonstrate.
H	Full results on robustness of inductive biases
H.1 Testing on different input image sizes
Figure 6 in Section 3.3 visualize the predictions of a 5-layer CNN on inputs of the extreme sizes of
7 × 7 and 112 × 112. Here in Figure 20 we provide more results on some intermediate image sizes.
We also test a trained 20-layer CNN on various input sizes (Figure 7). It is interesting that the
constant map also holds robustly across a wide range of input sizes. Especially from the prediction
on large input images, we found hints on how the CNNs actually compute the constant function.
It seems the artifacts from the outside boundary due to convolution padding are used as cues to
“grow” a pattern inwards to generate the digit “7”. The visualizations of the intermediate layer
representatiosn in Figure 21 is consistent with our hypothesis: the CNN take the last several layers
to realize this construction. This is very clever, because in CNNs, the same filter and bias is applied
to all the spatial locations. Without relying on the artifacts on the boundaries, it would be very
challenging to get a sense of the spatial location in order to construct an image with a holistic
structure (e.g. the digit “7” in our case).
H.2 The upper subnetworks
In the visualization of intermediate layers (Figure 15), the intermediate layers actually represent the
“lower” subnetwork from the inputs. Here we investigate the “upper” subnetwork. Thanks again
to the spatial structure of CNNs, we can skip the lower layers and feed the test patterns directly to
the intermediate layers and still get interpretable visualizations4 . Figure 22 shows the results for the
top-one layer from CNNs with various depths. A clear distinction can be found at 15-layer CNN,
4Specifically, the intermediate layers expect inputs with multiple channels, so we repeat the grayscale inputs
across channels to match the expected input shape.
23
Published as a conference paper at ICLR 2020
(a) 16 × 16
(b) 28 × 28
(c) 36 × 36
Figure 21: Visualization of intermediate representations when testing on images of different
sizes from training images for a 20-layer trained CNN. The CNN is trained on a 28 × 28 image
of the digit “7”.
(d) 48 × 48
24
Published as a conference paper at ICLR 2020
IlEI 国电 RnIMlIIU
11
15
16
18
20
SΠHF≡I
dr
Eg 灯 oil
因以HrJIHI川LU匕
BΠK JFJQIlll1,[.[[ ■：,：
B∏i IFJElI III I ■:]
后
物厦町nι
nmc□Ξ
Figure 22: Visualizing only the final layer in trained networks. The first row are the input images,
which are directly fed into the final layer of trained networks (skipping the bottom layers). The
remaining rows shows the predictions from the top layers of CNNs, with the numbers on the left
indicating their (original) depth.
tHHBHOEJaill Illl IlEn
Bllll ■■0•
57无B四口PJHiMl "工
? ≤rrlΛ∙
7 El/囱冏1口喝11111川口舞「二
7 0<-rVJ" ■删■■■,
9河幻⑸En_JiiJ;
ZHicrtJ- ■删■■■•
11巨豆亘五厂JEIIJnilL趟|「二
HElBHVJaHIIII
14： , .∏. IIrnI工港■■
■■■■■■■■■■■■■
157琳,康f’价房Iliil⑥以
ΠOSQIJIDHMI≡∙β
Mi博时卧债第憧IMIU脚◎/
Hoshbjiqbiiii
18刃画可引智用WiilIIIIl ・«•营
Em日研 MElillIIl ■■■□
2。厅偏修回藤场Fill I皿I」删翻豆
Figure 23:	Visualizing only the top two layers in trained networks. The first row are the input
images, which are directly fed into the top two layer of trained networks (skipping the bottom layers).
The remaining rows shows the predictions from the top two layers of CNNs, with the numbers on
the left indicating their (original) depth. More specifically, each of the two top layers occupies one
row. The colorful rows are the visualizations (as the top singular vector across channels) of the
outputs of the second to the last layer from each network. The grayscale rows are the outputs of the
final layer from each network.
which according to Figure 3 is where the networks start to bias away from edge detector and towards
the constant function.
The predictions from the final two layers of each network are visualized in Figure 23. Figure 21
focuses on the 20-layer CNN that learns the constant map, and visualize the upper 3 layers, 6 layers
and 10 layers, respectively. In particular, the last visualization shows that the 20-layer CNN is
already starting to construct the digit “7” from nowhere when using only the upper half of the
model.
25
Published as a conference paper at ICLR 2020
IHOBHEiIJaiII III I BLZ□
■■■■■■■■■■■■■
，∙ < ^u∙ ■■■■■•
■■■■■■•■■■■■■
■■■■■■■■■■■■■
s■■■■■■■■■■■■■
^ESEHBUHQBHEaQ
Figure 24:	Visualzing the top 3 layers, 6 layers and 10 layers of a 20-layer CNN. Visualizations
formatted in the same way as Figure 23.
0.0	0.2	0.3	0.5	0.6	0.8	0.9
rho: train-test correlation
correlation to identity function
1-° flj 5-
0.8/ 9 HI
0.6fel3-"∣
≤ 17-■■■■■■■■■
0 4 > 25 H∣
0.2 O 29-B∣
nn 57-■■■■1
u.u	ɪ ɪ ɪ ɪ ɪ ɪ
0.0	0.2	0.3	0.5	0.6	0.8
rho: train-test correlation
0.9
1.0
0.8
0.6
0.4
0.2
0.0
Figure 25:	Inductive bias of a 5-layer CNN with varying convolutional filter size. The heatmap
is arranged similarly as Figure 4, except that the rows correspond to CNNs filter sizes.
I	Full results on varying different factors when training CNNs
The study on the effects of various hyperparameters on the inductive bias of trained CNNs is briefly
presented in Section 3.4. The full results are shown here.
Convolution filter sizes Figure 25 and Figure 26 illustrate the inductive bias with varying convo-
lution filter size from 5 × 5 to 57 × 57. The visualization shows that the predictions become more
and more blurry as the filter sizes grow. The heatmaps, especially the correlation to the identity
function, are not as helpful in this case as the correlation metric is not very good at distinguishing
images with different levels of blurry. With extremely large filter sizes that cover the whole inputs,
the CNNs start to bias towards the constant function. Note our training inputs are of size 28 × 28,
so 29 × 29 filter size allows all the neurons to see no less than half of the spatial domain from the
previous layer. 57 × 57 filters centered at any location within the image will be able to see the whole
previous layer. On the other hand, the repeated application of the same convolution filter through
out the spatial domain is still used (with very large boundary paddings in the inputs). So the CNNs
are not trivially doing the same computation as FCNs.
Convolution channel depths Figure 10 in Section 3.4 shows that the learned identity function
is severely corrupted when only 3 channels are used in the 5-layer CNNs. Figure 27 presents the
correlation to the constant and the identity function when different numbers of convolution channels
are used. The heatmap is consistent with the visualizations, showing that the 5-layer CNN fails to
approximate the identity function when only three channels are used in each convolution layer.
Furthermore, Figure 28 visualize the predictions of trained 3-channel CNNs with various depths.
The 3-channel CNNs beyond 8 layers fail to converge during training. The 5-layer and the 7-layer
CNNs implement functions biased towards edge-detecting or countour-finding. But the 6-layer and
the 8-layer CNNs demonstrate very different biases. The potential reason is that with only a few
26
Published as a conference paper at ICLR 2020
57
7∣7E7∣7∣7∣ ∣7∣7∣7E7∣7
Figure 26:	Visualizing the predictions from CNNs with various filter sizes. The first row is
the inputs, including the single training image “7”. The remaining rows are predictions, with the
numbers on the left showing the corresponding filter sizes.
—r 1.0
-0.8
-0.6
-0.4
-0.2
l0.0
UUCP#
correlation to identity function
correlation to constant function
3-■■■■■■■■
128-■■■■■■■
1024-■■■■■■■■■■■■■■■■■■■
0.0	0.2	0.3	0.5	0.6	0.8	0.9
3 8
2
1
0.0	0.2	0.3	0.5	0.6	0.8	0.9
rho: train-test correlation
—r 1.0
-0.8
-0.6
-0.4
-0.2
l0.0
Figure 27: Correlation to the constant and the identity function for different convolution chan
nels in a 5-layer CNN.
Figure 28: Visualization predictions from CNNs with 3 convolution channels and with various
number of layers (numbers on the left). The first row is the inputs, and the remaining rows
illustrate the network predictions.
27
Published as a conference paper at ICLR 2020
□■■■■■
κ∙∙∙∙∙
1■■■■
IJBBHH
≡ 唧■!!■■
■■■■■■
0，工至■ ■
因■■■■■
■■■■■■
ES 用■■■■
日修■■■■
o⅛∙∙∙∙
H∙∙∙∙0
◎ 0日因HIEI011IIU *n
? tiλ mill nil
■■■■■■■■■■■■■
■■■■■■
■ ■■■■■♦ Ii ■■■■■
■■■■■■■■■■■■■
(a)	3 channels, random init
(b)	128 channels, random init
Figure 29:	Visualizing the randomly initialized models to compare two 5-layer CNNs with 3
convolution channels per layer and 128 convolution channels per layer, respectively. The sub-
figures visualize the predictions of intermediate layers of the two network at random initialization.
The multi-channel intermediate layers are visualized as the top singular vectors.
◎ 0日因HIEI011IIU BK□
ɪift tj` Iiiiiiiiiiiii■■0«
>]∣? <≡VJ- IllllIIIIIBSae
y > < f IJ ∙ HIIllIflBe
■■■■■■■■■■■■■
HHSBBHaHIUlBHiGQ
◎ 0日因HIEI011IIU *n
7 =/ ∙ Illlll
7 > <ru∙ ∙ιιιι∙∙∙∙
7 > < rv J - BIiii
，.”■」∙ .∣∣∣∣.∙∙
(a)	3 channels, after training
(b)	128 channels, after training
Figure 30:	Comparing two 5-layer CNNs with 3 convolution channels per layer and 128 convo-
lution channels per layer, respectively. Layout is similar to Figure 29.
channels, the random initialization does not have enough randomness to smooth out “unlucky” bad
cases. Therefore, the networks have higher chance to converge to various corner cases. Figure 29
and Figure 30 compare the random initialization with the converged network for a 3-channel CNN
and a 128-channel CNN. From the visualizations of the intermediate layers, the 128-channel CNN
already behave more smoothly than the 3-channel CNN at initialization.
Initialization schemes In transfer learning, it is well known that initializing with pre-trained net-
work weights could affect the inductive bias of trained models. On the other hand, different random
initialization schemes are mainly proposed to help with optimization by maintaining information
flow or norms of representations at different depths. It turns out that they also strongly affect the
inductive bias of the trained models. Figure 31 visualizes the different inductive biases. Let fi , fo be
the fan in and fan out of the layer being initialized. We tested the following commonly used initial-
ization schemes: 1) default: N (0, σ2 =1(fifo)); 2) Xavier (a.k.a. Glorot) init (Glorot & Bengio,
2010): N(0, σ2 = 2(fi + fo)); 3) Kaiming init (He et al., 2015): N(0, σ2 = 2fi); 4) Orthog-
onal init (Saxe et al., 2014). Variations with uniform distributions instead of Gaussian distributions
are also evaluated for Xavier and Kaiming inits. All initialization schemes bias toward the identity
function for shallow networks. But Kaiming init produces heavy artifacts on test predictions. For
the bias towards the constant function in deep networks, Xavier init behaves similarly to the default
init scheme, though more layers are needed to learn a visually good identity function. On the other
hand, the corresponding results from the Kaiming init is less interpretable.
Optimizers First order stochastic optimizers are dorminately used in deep learning due to the
huge model sizes and dataset sizes. To improve convergence speed, various adaptive methods are
introduced (Duchi et al., 2011; Kingma & Ba, 2014; Graves, 2013). It is known that those methods
lead to worse generalization performances in some applications (e.g. Wu et al. (2016)). But they
are extremely popular in practice due to the superior convergence speed and easier hyper-parameter
tuning than the vanilla SGD. We compare several popular optimizers in our framework, and confirm
that different optimizers find different global minimizers, and those minimizers show drastically
28
Published as a conference paper at ICLR 2020
HOBΞU∏α∣lllllll
DHHBHrjnaillIIIIII
XnHHQHrjnaillIIIIII
XuHHBHnnaiIIII
KnnEl日归□πnil I
KuNEI曰H□nnil I
OrHBBHnnaiIIIII
1-layer CNNs
I IlTn=El曰崂□ιInIllIU Ilen
I HQBHΠ∏aH≡KaΞ□
I HOBΞ□OEIH≡∣]BΞ□
I HHBHΠθaH≡HBΞ□
I Ir in胴解昌园口网。删m1鬻庭o
3-layer CNNs
0■日/间IlnIIlIU IIKn N■曰NtiiiniIIiu IIen
DHHBHΠ□HΓIΠE!iΞ□ HQΞHEHE]E1H^EH^Ξ
XnNEI 曰H□□0IZH[LI1U□ HQΞH□□α∏^Γ]HΞΞ
XuHDHHU∏α∣ III「■)【〕□ 口口曰万口口。口皿口，回仃
KnN园翳髓园鼬■■血㈤舞彩藁H≡^!3□EJ^≡≡□≡□□
K同酷羽魅mi图网心匚嬲尸巴口用髓蠡诵避回晨聊国■则需
orHQΞHΓJElOI 皿L Iiiln HHSBΠΠΠΠ□E1SΠΠ
5-layer CNNs
14-layer CNNs
■■囱陶国IInIIlIU IlTnNEI□□□ιιnιιιιι∙ a^□
dHHQHE1HE3□□HE1E1H hhhhhhhhhhhhh
X
nHQHQΠ□BΠ□ElFlE]H 以初同立■同■切切国内归园
X
同，阳归国”0同通直]血因G HBHHEaElEa□El□□ΞH
KnHΠOOΠΠ!JOO!^ΠΠΠ HE3Si2OOEi□ΞEO0Ξ
KuHO≡OBOE≡□BKIiEl□ HEiOOΠΠElθ^EliΠEl
o同Ea时归口□■□□，，回口口口篇归附初・而明血*描胃
20-layer CNNs
24-layer CNNs
Figure 31:	Visualization of the predictions from CNNs trained with the D) default, Xn) Xavier
normal, Xu) Xavier uniform, Kn) Kaiming normal, Ku) Kaiming uniform, and Or) orthogonal
initialization schemes. The first row shows the inputs, and the remaining rows shows the predictions
from each trained networks.
29
Published as a conference paper at ICLR 2020
-BJp3 P3 B3 ■ PΠ BJ
-QQHHHHh
N —— 0 Ig —— O T —— OIppp dsirn 日PPP XlUPPP
ESESESES . ES BS
ΞΞΞΞHaa
Oaaaaaaa
□π□π□Hn□
Ξħħħξħħħ
3 Bl CQ Pl B3 每 InBJ
HHOŋHHπO
□□□□Ξ≡□S
CΞCC•回・
脚■■■■能 一E∙m
IBEnHBHU
NOOOOBmm
3-layer CNNs
5-layer CNNs
H
□□□□□■□□
14-layer CNNs
20-layer CNNs
17-layer CNNs
24-layer CNNs
Figure 32:	Visualization of the predictions from CNNs trained with different optimizers. The
first row shows the inputs, and the remaining rows shows the predictions from SGD (lr 0.01), SGD
(lr 0.001), SGD (lr 0.0001), Adagrad, RMSprop, Aam, and Adamax respectively.
different inductive biases on test examples, as shown in Figure 32. Some optimizer requires a
smaller base learning rate to avoid parameter exploding during training. In particular, we use base
learning rate 0.001 for Adagrad and Adamax, 0.0001 for Adam and RMSprop. For comparison, we
also include results from SGD with those corresponding base learning rates.
J Correlation vs MSE
Figure 33, Figure 34 and Figure 35 can be compared to their corresponding figures in the main text.
The figures here are plotted with the MSE metric between the prediction and the groundtruth, while
the figures in the main text uses the correlation metric. Each corresponding pair of plots are overall
consistent. But the correlation plots show the patterns more clearly and has a fixed value range of
[0, 1] that is easier to interpret.
30
Published as a conference paper at ICLR 2020
∙lc,A-uc,ppz#
mse to constant function
I-口■■■■■■■■■■■■■■■■■■■
3■■■■■■■■■■■■■■■■■■■■
5-∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙
?-■■■■■■■■■■■■■■■■■■■■
9■■■■■■■■■■■■■■■■■■■■
!!-■■■■■■■■■■■■■■■■■■■■
θ*θ 0：2	03	0^5	0"6 θ'β 0"9
rħo: train-test correlation
I—∣-0.16
-0.14 ⅛5
-0-12/
-0.10-
-0.08 ω
-0.06 ŋ
- -0.04 W
■-0.02*
≡-0.00
mse to identity function
■■■■■■■■■■■■■■■■■
5-■■■■■■■■■■■■■■■■■■■■
7-■■■■■■■■■■■■■■■■■■■■
II-■■■■■■■■■■■■■■■■■■■■
0.0	0.2	0.3	0.5	0.6	0.8	0.9
rho: train-test correlation
-0.10
-0.08
-0.06
-0.04
-0.02
0.00
Figure 33:	Quantitative evaluation of linear FCNs. The same as Figure 14(a), except MSE is
plotted here instead of correlation.
∙lc,A-uc,ppz#
-0.14
-0.12
-0.10
-0.08
0.06
-0.04
-0.02
≡-0.00
Figure 34: Quantitative evaluation of ReLU FCNs. The same as Figure 14(b), except MSE is
plotted here instead of correlation.
mse to identity function
mse to constant function
0.0	0.2	0.3	0.5	0.6	0.8	0.9
0.14
0.12
0.10
0.06
0.04
rho: train-test correlation
I-■■■■■■■■■■■■■■■■■■■■
3■■■■■■■■■■■■■■■■■■■■
5-■■■■■■■■■■■■■■■■■■■■
7-■■■■■■■■■■■■■■■■■■■■
9-■■■■■■■■■■■■■■■■■■■■
0.0811-■■■■■■■■■■■■■■■■■■■■
12-■■■■■■■■■■■■■■■■■■■■
14-■■■■■■■■■■■■■■■■■■■■
16-■■■■■■■■■■■■■■■■■■■■
18-■■■■■■■■■■■■■■■■■■■■
0.02 19-■■■■■■■■■■■■■■■■■■■■
0.0020-, ■■■■■■■■■■■■■■■■■■
0.0	0.2	0.3	0.5	0.6	0.8	0.9
0.12
0.10
0.08
0.06
0.04
0.02
0.00
rho: train-test correlation
Figure 35: Quantitative evaluation of CNNs. The same as Figure 4, except MSE is plotted here
instead of correlation.
31
Published as a conference paper at ICLR 2020
Input
1-layer
3-layer
5-layer
7-layer
9-layer
11-layer
12-layer
14-layer ¾¾
15-layer
16-layer
18-layer jJ
19-layer N
EI咯 ESU □ 011 III ■ HK□
HQQE□a∣IIIIIIHΞ□
ħqqqueib≡kbξ□
BQQ□□I1B≡KHΞ□
HQ□BΠI1H≡HHΞ□
BΠΠBS∣αHΠIHHS]Ξ
■E30。国0圆册■■画以
□BDEI≡aH≡nH^]Ξ
OnOBHBHiBIEHHS
OHDIHHaHHHHSa
HBSnDHaHHHHEB
HHaHBBBBHBBH
HHHHHaaHHHBH
Input
1-layer
3-layer ⅝¾
5-layer ¾¾
7-layer ¾¾
9-layer
11-layer ¾¾
12-layer ⅝¾
14-layer
15-layer
16-layer ⅝¾
18-layer
19-layer N
20-layer R
EI 咯 E∣□□nιι ιιι∙ιιcn
E1∙E3B□∙III III ■ Hlgn
HQQSUEIB≡nHΞ□
HQ□Q□aB≡EHΞ□
DQDBΠBH≡HHΞ□
DDQSΞOSIEHH^]Ξ
■IS■。国01ΠO∙1■画目
Hn∏HHBHΠIIΠHI≡Ξ
O∏nBHBH^ElHI≡Ξ
Wn■■■■■■■■
OnnQHKIHHHHKB
HHDQaBBBHBBa
HHHHHaHaHHHH
HHHHHcollHHHElHH
(b) 3rd run
Figure 36:	Visualization of predictions from CNNs trained on a single example. The same
training example is used as in the main text, but two extra runs of training and evaluation are listed
to show the robustness of our main observations to the randomness in the experiments.
Input ∣¾jj	目咯ES□□n∣ Illl HK□ InPU0四用ESU□n∣ Illl nnκ□
1-layer	目咯Es=□nιιιu nκ□ 3E因用Es=口nιιιι∙ IIea
3-layer U	^Q□Q□αH≡nHΞ□ 3-layer 5国用 QuniHimiIIIEO
5-layer U	OQDQ□α∣HinHΞ□ 5-layer ΞDQ□□□OMIin∣IΞ□
7-layer	ΞQQD□αHHn∣IΞ□ 7-layer Ξ!□□□SΞOHθnHΞ□
9-layer	HBBS!SllHΠDHH[g]Ξ ❷-即圆的EaaO回。胸皿■■©日
11-layer ∣jj	IaIaR。回InEI■©日 U-IayerR；500 口西 El∙W∣∣∙∙m
12-layer	EIEaESa国IlBiIID■■画目旧CaR=EIEl圆
14-layer ∣¾	SHBSEUBHEIHHQ 14-iayer 0[。•口臼0画画 E国国日
15-layer ∣ξ¾	HSHSEBEIHHHHIHES ι5-iayerΞ^ΞHΞElQBE3ΞElS^lQ
16-layer ∣⅛J	eiħd≡esbħshħes 16-1那「日50口。回0国国11・团曰
18-layer ∣S¾	EIElEI昌园超目ElElEl目目回加口团国5曰门田口口门口口由
19-layer ∣¾	目图画目目目目目目目目目19-layer目囱物KIBI局KI合■已■肛也
20-layer 目	目目目目目目目目目目目目20-layerq'qqQΞE由QQQΞQΞ
(a) training with digit 3
(b) training with digit 0
Figure 37:	Visualization of predictions from CNNs trained on a single example. Two different
(randomly chosen) training images (a) digit 3, (b) digit 0, are shown to compare robustness of our
main observations to different training images.
K	Robustness of observations to random seeds
We evaluate the robustness of our main observations to randomness from the experiments in this
section. In Figure 36, two different runs of training and evaluation are compared side by side, and
the results are very consistent. In Figure 37 we further perturb the random seeds for data loading, so
that different (single) training images are loaded for training. We can see that the main observations
32
Published as a conference paper at ICLR 2020
Figure 38: Visualization of the predictions from networks trained with 2 examples. The first two
columns are training examples, and the remaining columns are unseen test cases. H0—H9 shows
fully connected networks with the corresponding number of hidden layers. C3—C24 shows CNNs
with the corresponding number of layers.
hold for both cases: CNNs learn the identity map, edge detector and the constant map as the depth
increases.
L Learning with multiple examples
We focus on learning from a single example in this paper because the two extreme inductive bi-
ases of the identity map and the constant map can be precisely defined and tested against. When
training with multiple examples, the constant map is no longer a viable solution to the optimization
problem. Nevertheless, we can still qualitatively evaluate the inductive bias via visualization of pre-
dictions. Figure 38 shows the results on various networks trained with two examples. In particular,
for networks that are known to overfit to the constant map when trained with one example (e.g. fully
connected networks with 9 hidden layers, and 20-layer convolutional networks), similar overfitting
behaviors are observed. In this case, a proper definition of a constant map no longer holds, as the
network perfectly reconstruct each individual digit from the training set. On the test set, a notion
of memorization can be recognized as always predicting a mixture of the training samples. Note
sometimes the prediction is biased more towards one of the training samples depending on the input
patterns.
Figure 39 shows the results with three examples, with similar observations. Figure 40 shows the
situation when training with the full MNIST training set (60k examples). In this case, even the
deepest convolutional networks we tried successfully learn the identity function. Fully connected
networks learn the identity function on the manifold of digit inputs, but still cannot reconstruct
meaningful results on other test patterns.
33
Published as a conference paper at ICLR 2020
5-5-5-5-5-5-5
aaaαααs
□ □□ΞΞΞS
©©回画画曰■
■■■・BIWI 目
7
ΞΞΞΞΞΞΞ
ΞΞΞΞΞQ
Hq—一
□ □□Q
2
口
Q∙
二E

Figure 39: Visualization of the predictions from networks trained with 3 examples. The first
three columns are training examples, and the remaining columns are unseen test cases. H0—H9
shows fully connected networks with the corresponding number of hidden layers. C3—C24 shows
CNNs with the corresponding number of layers.
34
Published as a conference paper at ICLR 2020
input
HΞ
0 15 9
HHHH
51
CTM
ΞD□□α
LinH5∣
LinH9
51
5∣2
1
1
3 7 4 0 4
士〕 1 2 2
CCCCC
7・7・7
00000
aaaaa
n□□□n
门 口 口 口 口
□ □□□□
e©©E:©
川™™™m

5
¾

海一圆一 百一，一
Figure 40: Visualization of the predictions from networks trained with 60k examples. The
first three columns are (3 out of 60k) training examples, and the remaining columns are unseen test
cases. H0—H9 show fully connected networks (ReLU activation) with the corresponding number
of hidden layers. LinH1—LinH9 show linear fully connected networks (without activation) with the
corresponding number of hidden layers. And C3—C24 show CNNs with the corresponding number
of layers.
35
Published as a conference paper at ICLR 2020
FCN num layers	1	2	4	6	8	10
hidden dim = 784	614,656	1,229,312	2,458,624	3,687,936	4,917,248	6,146,560
hidden dim = 2048		3,211,264	11,599,872	19,988,480	28,377,088	36,765,696
CNN num layers	1	3	5	6	7	8
5 × 5 filter, 3 channels	25	375	825	1,050	1,275	1,500
5 × 5 filter, 128 channels 5 × 5 filter, 1024 channels	25	416,000	1,235,200 78,694,400	1,644,800	2,054,400 131,123,200	2,464,000
7 × 7 filter, 3 channels	49	735	1,617	2,058	2,499	2,940
7 × 7 filter, 128 channels 7 × 7 filter, 1024 channels	49	815,360	2,420,992 154,241,024	3,223,808	4,026,624 257,001,472	4,829,440
CNN num layers	9	10	11	12	13	14
5 × 5 filter, 3 channels	1,725	1,950	2,175	2,400	2,625	2,850
5 × 5 filter, 128 channels 5 × 5 filter, 1024 channels	2,873,600	3,283,200	3,692,800	4,102,400 262,195,200	4,512,000	4,921,600 314,624,000
7 × 7 filter, 3 channels	3,381	3,822	4,263	4,704	5,145	5,586
7 × 7 filter, 128 channels 7 × 7 filter, 1024 channels	5,632,256	6,435,072	7,237,888	8,040,704 513,902,592	8,843,520	9,646,336 616,663,040
CNN num layers	15	16	17	19	20	24
5 × 5 filter, 3 channels	3,075	3,300	3,525	3,975	4,200	5,100
5 × 5 filter, 128 channels 5 × 5 filter, 1024 channels	5,331,200	5,740,800	6,150,400	6,969,600	7,379,200 471,910,400	9,017,600
7 × 7 filter, 3 channels	6,027	6,468	6,909	7,791	8,232	9,996
7 × 7 filter, 128 channels 7 × 7 filter, 1024 channels	10,449,152	11,251,968	12,054,784	13,660,416	14,463,232 924,944,384	17,674,496
CNN filter size	9×9	13× 13	17 × 17	25 × 25	29 × 29	57 × 57
5 layers, 128 channels	4,002,048	8,349,952	14,278,912	30,880,000	41,552,128	160,526,592
Table 1: Number of parameters of different neural network architectures used in this paper.
The number of parameters are calculated for 28 × 28 grayscale input / output sizes.
M	Parameter counts for neural networks used in this paper
For convenience of cross comparing the results of different architectures with similar number of
parameters, we list in Table 1 the architectures used in this paper and their associated number of
parameters.
N Residual Networks
In this section, we evaluate training FCNs with residual connections. In particular, an identity skip
connection is added for every two fully connected layers. In other words, the networks are built with
two-layer blocks that computes x 7→ x + ReLU(W2ReLU(W1x)), except that no ReLU is applied
at the output layer. Adding skip connection after ReLU ensures the input gets passed directly to the
output layer. Because the residual structure requires the same input-output shape for every block,
we use 784 hidden dimensions.
Comparing Figure 41 with the vanilla FCNs in Figure 2, the identity skip connection strongly bi-
ased the FCNs towards learning the identity map. On the other hand, we can still observe that the
prediction noises become stronger with larger number of hidden layers, suggesting that learning the
identity function is non-trivial even with the explicit identity skip connections built into the archi-
tectures.
O	Experiments on CIFAR- 1 0 Images
In this section, we show experiments on some colored images from the CIFAR-10 dataset (32 × 32
RGB images). Figure 42 and Figure 43 show the results from two different randomly sampled
training images, respectively. The network architectures used here are nearly identical to the ones
used to train on MNIST digits in the main text of the paper: the CNNs are with 128 channels and 5×5
36
Published as a conference paper at ICLR 2020
Figure 41: Visualization of predictions from multi-layer ReLU networks with residual con-
nections. The first row shows the input images for evaluation, including the single training image
“7” at the beginning of the row. The remaining rows show the predictions from trained multi-layer
ReLU FCNs with residual connections, with the numbers on the left indicating the number of hidden
layers.
filters. The FCNs are slightly modified to have larger hidden dimensions 3072 = 32×32×3, to avoid
explicitly enforcing bottlenecks in the hidden representations. The training loss and optimization
algorithms are the same as in the MNIST case.
The main observations are consistent the results on the MNIST digits. In particular, shallow FCNs
produce random noisy predictions on unseen evaluation inputs, but deep FCNs bias towards mem-
orization and hallucinate the training image on all test outputs. For CNNs, shallow networks are
capable of learning the identity function, but deep networks learn the constant function instead.
37
Published as a conference paper at ICLR 2020
SNUT
SNNU
Figure 42: Visualization of predictions from various networks trained on a single CIFAR-10
image (dog). The first row shows the input images, where the first one is the training image and the
rest are images for evaluation. The evaluation images consist of unseen images from the CIFAR-10
test set and artificially generated grayscale patterns. The grayscale patterns are duplicated into three
channels before feeding into the networks. The remaining rows show the predictions from trained
FCNs and CNNs. For FCNs, the numbers indicate the number of hidden layers. For example, H3
means a 3-hidden-layer FCN. For CNNs, the numbers indicate the number of (convolutional) layers.
For example, C16 means a 16-layer CNN.
38
Published as a conference paper at ICLR 2020
ItOI 357 1 5926048
]puHHHHH CCCC1C1C2C2C2
⅛ - - - -
Figure 43: Visualization of predictions from various networks trained on a single CIFAR-10
image (air plane). The first row shows the input images, where the first one is the training image
and the rest are images for evaluation. The evaluation images consist of unseen images from the
CIFAR-10 test set and artificially generated grayscale patterns. The grayscale patterns are duplicated
into three channels before feeding into the networks. The remaining rows show the predictions from
trained FCNs and CNNs. For FCNs, the numbers indicate the number of hidden layers. For example,
H3 means a 3-hidden-layer FCN. For CNNs, the numbers indicate the number of (convolutional)
layers. For example, C16 means a 16-layer CNN.
SNUT
SNNU
39