Published as a conference paper at ICLR 2020
Critical initialisation in continuous approxi-
MATIONS OF B INARY NEURAL NETWORKS
George Stamatescu, Ian Fuss and Langford B. White
School of Electrical and Electronic Engineering
University of Adelaide
Adelaide, Australia
{george.stamatescu}@gmail.com
{lang.white,ian.fuss}@adelaide.edu.au
Federica Gerace
Institut de Physique TheoriqUe
CNRS & CEA & Universite Paris-Saclay
Saclay, France
federicagerace91@gmail.com
Carlo Lucibello
Bocconi Institute for DataScience and Analytics
Bocconi University
Milan, Italy
carlo.lucibello@unibocconi.it
Ab stract
The training of stochastic neural network models with binary (±1) weights and
activations via continuous surrogate networks is investigated. We derive new sur-
rogates using a novel derivation based on writing the stochastic neural network as
a Markov chain. This derivation also encompasses existing variants of the surro-
gates presented in the literature. Following this, we theoretically study the surro-
gates at initialisation. We derive, using mean field theory, a set of scalar equations
describing how input signals propagate through the randomly initialised networks.
The equations reveal whether so-called critical initialisations exist for each surro-
gate network, where the network can be trained to arbitrary depth. Moreover,
we predict theoretically and confirm numerically, that common weight initialisa-
tion schemes used in standard continuous networks, when applied to the mean
values of the stochastic binary weights, yield poor training performance. This
study shows that, contrary to common intuition, the means of the stochastic binary
weights should be initialised close to ±1, for deeper networks to be trainable.
1	Introduction
The problem of learning with low-precision neural networks has seen renewed interest in recent
years, in part due to the deployment of neural networks on low-power devices. Currently, deep
neural networks are trained and deployed on GPUs, without the memory or power constraints of
such devices. Binary neural networks are a promising solution to these problems. If one is interested
in addressing memory usage, the precision of the weights of the network should be reduced, with
the binary case being the most extreme. In order to address power consumption, networks with both
binary weights and neurons can deliver significant gains in processing speed, even making it feasible
to run the neural networks on CPUs Rastegari et al. (2016). Of course, introducing discrete variables
creates challenges for optimisation, since the networks are not continuous and differentiable.
Recent work has opted to train binary neural networks directly via backpropagation on a differen-
tiable surrogate network, thus leveraging automatic differentiation libraries and GPUs. A key to
this approach is in defining an appropriate differentiable surrogate network as an approximation to
the discrete model. A principled approach is to consider binary stochastic variables and use this
stochasticity to “smooth out” the non-differentiable network. This includes the cases when (i) only
weights, and (ii) both weights and neurons are stochastic and binary.
In this work we study two classes of surrogates, both of which make use of the Gaussian central
limit theorem (CLT) at the receptive fields of each neuron. In either case, the surrogates are written
1
Published as a conference paper at ICLR 2020
as differentiable functions of the continuous means of stochastic binary weights, but with more
complicated expressions than for standard continuous networks.
One approximation, based on analytic integration, yields a class of deterministic surrogates Soudry
et al. (2014). The other approximation is based on the local reparameterisation trick (LRT) Kingma
& Welling (2013), which yields a class of stochastic surrogates Shayer et al. (2017). Previous works
have relied on heuristics to deal with binary neurons Peters & Welling (2018), or not backpropa-
gated gradients correctly. Moreover, none of these works considered the question of initialisation,
potentially limiting performance.
The seminal papers of Saxe et al. (2013), Poole et al. (2016), Schoenholz et al. (2016) used a mean
field formalism to explain the empirically well known impact of initialization on the dynamics of
learning in standard networks. From one perspective the formalism studies how signals propagate
forward and backward in wide, random neural networks, by measuring how the variance and corre-
lation of input signals evolve from layer to layer, knowing the distributions of the weights and biases
of the network. By studying these moments the authors in Schoenholz et al. (2016) were able to
explain how heuristic initialization schemes avoid the “vanishing and exploding gradients problem”
Glorot & Bengio (2010), establishing that for neural networks of arbirary depth to be trainable they
must be initialised at “criticality”, which corresponds to initial correlation being preserved to any
depth.
The paper makes three contributions. The first contribution is the presentation of new algorithms,
with a new derivation able to encompass both surrogates, and all choices of stochastic binary
weights, or neurons. The derivation is based on representing the stochastic neural network as a
Markov chain, a simplifying and useful development. As an example, using this representation we
are easily able to extend the LRT to the case of stochastic binary neurons, which is new. This was
not possible in Shayer et al. (2017), who only considered stochastic binary weights. As a second
example, the deterministic surrogate of Soudry et al. (2014) is easily derived, without the need for
Bayesian message passing arguments. Moreover, unlike Soudry et al. (2014) we correctly backprop-
agate through variance terms, as we discuss.
The second contribution is the theoretical analysis of both classes of surrogate at initialisation,
through the prism of signal propagation theory Poole et al. (2016), Schoenholz et al. (2016). This
analysis is achieved through novel derivations of the dynamic mean field equations, which hinges on
the use of self-averaging arguments Mezard et al. (1987). The results of the theoretical study, which
are supported by numerical simulations and experiment, establish that for a surrogate of arbitrary
depth to be trainable, it must be randomly initialised at “criticality”. In practical terms, critical-
ity corresponds to using initialisations that avoid the “vanishing and exploding gradients problem”
Glorot & Bengio (2010). We establish the following key results:
•	For networks with stochastic binary weights and neurons, the deterministic surrogate can
achieve criticality, while the LRT cannot.
•	For networks with stochastic binary weights and continuous neurons, the LRT surrogate
can achieve criticality (no deterministic surrogate exists for this case)
In both cases, the critical initialisation corresponds to randomly initialising the means of the binary
weights close to ±1, a counter intuitive result.
A third contribution is the consideration of the signal propagation properties of random binary net-
works, in the context of training a differentiable surrogate network. We derive these results, which
are partially known, and in order to inform our discussion of the experiments.
This paper provides insights into the dynamics and training of the class of binary neural network
models. To date, the initialisation of any binary neural network algorithm has not been studied,
although the effect of quantization levels has been explored through this perspective Blumenfeld
et al. (2019). Currently, the most popular surrogates are based on the so-called “Straight-Through”
estimator Bengio et al. (2013), which relies on heuristic definitions of derivatives in order to define
a gradient. However, this surrogate typically requires the use of batch normalization, and other
heuristics. The contributions in this paper may help shed light on what is holding back the more
principled algorithms, by suggesting practical advice on how to initialise, and what to expect during
training.
2
Published as a conference paper at ICLR 2020
Paper outline: In section 2 we present the binary neural network algorithms considered. In sub-
section 2.1 we define binary neural networks and subsection 2.2 their stochastic counterparts. In
subsection 2.3 we use these definitions to present new and existing surrogates in a coherent frame-
work, using the Markov chain representation of a neural network to derive variants of both the
deterministic surrogate, and the LRT-based surrogates. We derive the LRT for the case of stochastic
binary weights, and both LRT and deterministic surrogates for the case of stochastic binary weights
and neurons. In section 3 we derive the signal propagation equations for both the deterministic and
stochastic LRT surrogates. This includes deriving the explicit depth scales for trainability, and solv-
ing the equations to find the critical initialisations for each surrogate, if they exist. In section 4 we
present the numerical simulations of wide random networks, to validate the mean field description,
and experimental results to test the trainability claims. In section 5 we summarize the key results,
and provide a discussion of the insights they provide.
2	B inary neural network algorithms
2.1	Continuous neural networks and binary neural networks
A neural network model is typically defined as a deterministic non-linear function. We consider a
fully connected feedforward model, which is composed of N' X N'-1 weight matrices W' and bias
vectors b' in each layer ' ∈ {1,..., L}, with elements Wj ∈ R and b' ∈ R. Given an input vector
x0 ∈ RN0 , the network is defined in terms of the following recursion,
x' = φ'(h'),	h' = √	W 'x'-1 + b'	(1)
where the pointwise non-linearity is, for example, φ`(∙) = max(0, ∙). We refer to the input to a
neuron, such as h` , as the pre-activation field.
A deterministic binary neural network simply has weights Wj ∈ {±1} and φ`(∙) = sign(∙), and
otherwise the same propagation equations. Of course, this is not differentiable, thus we instead
consider stochastic binary variables in order to smooth out the non-differentiable network. Ideally,
the product of training a surrogate of a stochastic binary network is a deterministic (or stochastic)
binary network that is able to generalise from its training set.
2.2	Stochastic binary neural networks
In stochastic binary neural networks we denote the matrices as S' with all weights1 Sj ∈ {±1} be-
ing independently sampled binary variables with probability is controlled by the mean Mj = ESj.
Neuron activation in this model are also binary random variables, due to pre-activation stochasticity
and to inherent noise. We consider parameterised neurons such that the mean activation conditioned
on the pre-activation is given by some function taking values in [-1, 1], i.e. E[xi' | hi'] = φ(hi'), for
example φ(∙) = tanh(∙). We write the propagation rules for the stochastic network as follows:
S' 〜p( •	；	M');	h'	=	√^'-τ	S'x'-1 + b';	x' 〜p(	• ；	φ(h'))	(2)
Notice that the distribution of x' factorizes when conditioning on x'-1. The form of the neuron’s
mean function φ(∙) depends on the underlying noise model. We can express a binary random variable
X ∈ {±1} with X 〜p(x; θ) via its latent variable formulation X = sign(θ + αL). In this form θ
is referred to as a “natural” parameter, and the term L is a latent random noise, whose cumulative
distribution function σ(∙) determines the form of the non-linearity since φ(∙) = 2σ(∙) — 1. In general
the form of φ(∙) will impact on the surrogates, performance, including within and beyond the mean
field description presented here. However, a result from the analysis in Section 3 is that choosing
a deterministic binary neuron, ie. the sign(∙) function, or a stochastic binary neuron, produces the
same signal propagation equations, up to a scaling constant.
1We denote random variables with bold font. Also, following physics’ jargon, we refer to binary ±1 vari-
ables as Ising spins or just spins.
3
Published as a conference paper at ICLR 2020
2.3	Derivations of new and existing surrogate networks
The idea behind several recent papers Soudry et al. (2014), Baldassi et al. (2018), Shayer et al.
(2017), Peters & Welling (2018) is to adapt the mean of the binary stochastic weights, with the
stochastic model essentially used to “smooth out” the discrete variables and arrive at a differentiable
function, open to the application of continuous optimisation techniques. We now derive both the de-
terministic surrogate and LRT-based surrogates, in a common framework. We consider a supervised
classification task, with training set D = {χμ, y*}p=ι, With y* the label. We define a loss function
for our surrogate model via
1P
L(M,b) = — p ElOg Es,χP(y“ | xμ, S, x,b),	⑶
μ=1
For a given input Xμ and a realization of weights, neuron activations and biases in all layers,
denoted by (S, x, b), the stochastic neural netWork produces a probability distribution over the
classes. Expectations over weights and activations are given by the mean values, ES' = M' and
E[x`∣h'] = φ(h'). This objective can be recognised as a (minus) marginal likelihood, thus this
method could be described as Type II maximum likelihood, or empirical Bayes.
The starting point for our derivations comes from rewriting the expectation equation 3 as the
marginalization of a Markov chain, with layers ` indexes corresponding to time indices.
Markov chain representation of stochastic neural network:
L
Es,χp(y“ | χμ, S, b, χ) = X	p(y“ | xL) Yp(χ' | χ'-1, S') p(s'; M')
S,x : X0 = Xμ	'=1
=X p(yμ∣SL, XLT)P(SL) Xp(XLT|xL-2, SLT)P(SLT)…Xp(x1∣Xμ, S1)p(S1) (4)
SL,xL-1	SL-1,xL-2	S1
where in the second line we dropped from the notation p(S'; M') the dependence on M' for brevity.
Therefore, for a stochastic network the forward pass consists in the propagation of the joint distribu-
tion of layer activations, p(x' ∣Xμ), according to the Markov chain. We drop the explicit dependence
on the initial input x* from now on.
In what follows we will denote with φ(h') the average value of x' according to p(x'). The first
step to obtaining a differentiable surrogate is to introduce continuous random variables. We take the
limit of large layer width and appeal to the central limit theorem to model the field h' as Gaussian,
with mean h' and covariance matrix ∑'.
Assumption 1: (CLT for stochastic binary networks) In the large N limit, under the Lyapunov
central limit theorem, the field h' = √"- S'x'-1 + b' converges to a Gaussian random variable
with mean	h'	=	√；'-	Pj	Mjφ(hj-1)	+	b'	and Covariance matrix	∑'	with diagonal	∑'	=
NI-T PjI-(Mijφ(h'-1))2∙
While this assumption holds true for large enough networks, due to S' and x'-1 independency, the
Assumption 2 below, is stronger and tipically holds only at initialization.
Assumption 2: (correlations are zero) We assume the independence of the pre-activation field h'
between any two dimensions. Specifically, we assume the covariance Σ = Cov(h', h') to be well
approximated by Σ'MF (φ(h'-1)), with MF denoting the mean field (factorized) assumption, where
(ςMF(XAiiO = δii0 N'_1 X I-(Mij0(h'T))2	(5)
j
This assumption approximately holds assuming the neurons in each layer are not strongly corre-
lated. In the first layer this is certainly true, since the input neurons are not random variables2.
In subsequent layers, since the fields hi' and h'j share stochastic neurons from the previous layer,
this cannot be assumed to be true. We expect this correlation to not play a significant role, since
2In this case the variance is actually N⅛ Pj(I - (MI)2)(xμ,j)2.
4
Published as a conference paper at ICLR 2020
the weights act to decorrelate the fields, and the neurons are independently sampled. However, the
choice of surrogate influences the level of dependence. The sampling procedure used within the lo-
cal reparametrization trick reduces correlations since variables are sampled, while the deterministic
surrogate entirely discards them.
We obtain either surrogate model by successively approximating the marginal distributions, p(x')=
J dh' p(x'∣h') ≈ p(x'), starting from the first layer. We can do this by either (i) marginalising over
the Gaussian field using analytic integration, or (ii) sampling from the Gaussian. After this, we use
the approximation p(x') to form the Gaussian approximation for the next layer, and so on.
Deterministic surrogate: We perform the analytic integration based on the analytic form of
p(x'+1∣h`) = σ(x'h'), with σ(∙) a sigmoidal function. In the case that σ(∙) is the Gaussian CDF,
We obtain p(x') exactly3 by the Gaussian integral of the Gaussian cumulative distribution function,
P(Xf) = /
dh σ(x'h) N(h; h`, ςMf,h) = φ(^	τi~-1/2Xf)
,	(1 + ςMf )1/2
(6)
Since we start from the first layer, all random variables are marginalised out, and thus h' has no
dependence on random hjf-1 via the neuron means φ(hf) as in Assumption 1. Instead, we have
dependence on means x' = Eh'E [x` | h'] = Eh' φ(h'). Thus it is convenient to define the mean
under p(x') as ,(h,σ2) = Jidh φ'(h)N(h; h,σ2). In the case that σ(∙) is the Gaussian CDF,
then φe(∙) is the error function. Finally, the forward pass can be expressed as
x' = d(h')	h' = (1 + ∑MF 厂 2 h'	h' = -=L= M'x'-1 + b',	⑺
Nf-1
This is a more general formulation than that in Soudry et al. (2014), which considered sign activa-
tions, which we obtain in the appendices as a special case. Furthermore, in all implementations we
-1
backpropagate through the variance terms ∑mF , which were ignored in the previous work of Soudry
et al. (2014). Note that the derivation here is simpler as well, not requiring complicated Bayesian
message passing arguments, and approximations therein.
LRT surrogate: The basic idea here is to rewrite the incoming Gaussian field h 〜N(μ, Σ) as
h = μ + √Σ E where E 〜N(0, I). Thus expectations over h can be written as expectations over E
and approximated by sampling. The resulting network is thus differentiable, albeit not deterministic.
The forward propagation equations for this surrogate are
h' = √=^≡ M'x'-1+ b' + √∑Mf(X'-1) e',	X' = φ'(h').	(8)
N'-1
The local reparameterisation trick (LRT) Kingma & Welling (2013) has been previously used to
obtain differentiable surrogates for binary networks. The authors of Shayer et al. (2017) consid-
ered only the case of stochastic binary weights, since they did not write the network as a Markov
chain. Peters & Welling (2018) considered stochastic binary weights and neurons, but relied on
other approximations to deal with the neurons, having not used the Markov chain representation.
The result of each approximation, applied successively from layer to layer by either propagating
means and variances or by, produces a differentiable function of the parameters Mi'j . It is then
possible to perform gradient descent with respect to the M and b. Ideally, at the end of training
we obtain a binary network that attains good performance. This network could be a stochastic net-
work, where we sample all weights and neurons, or a deterministic binary network. A deterministic
network might be chosen taking the most likely weights, therefore setting Wi'j = sign(Mi'j ), and
replacing the stochastic neurons with sign(∙) activations.
3	Signal propagation theory for continuous surrogates
Since all the surrogates still retain the basic neural network structure of layerwise processing, cru-
cially applying backpropagation for optimisation, it is reasonable to expect that surrogates are likely
3In the Appendices we show that other sigmoidal σ(∙) can be approximated by a Gaussian CDF.
5
Published as a conference paper at ICLR 2020
to inherit similar “training problems” as standard neural networks. In this section we apply this
formalism to the surrogates considered, given random initialisation of the means Mj and biases b'.
We are able to solve for the conditions of critical initialisation for each surrogate, which essentially
allow signal to propagate forwards, and gradients to propagate backwards, without the effects such
as neuron saturation. The critical initialisation for the surrogates, the key results of the paper, are
provided in Claims 1 and 3.
3.1	Forward signal propagation for standard continuous networks
We first recount the formalism developed in Poole et al. (2016). Assume the weights of a standard
continuous network are initialised with Wj 〜N(0, σW), biases b` 〜N(0, σ2), and input signal
Xa has zero mean Ex0 = 0 and variance E[xg ∙ xg] = q0a, and with a denoting a particular input
pattern. As before, the signal propagates via Equation 1 from layer to layer.
We are interested in computing, from layer to layer, the variance q'0 = N^ Pi(h'；a)2 from a par-
ticular input x0a, and also the covariance between the pre-activations qab = N Pi h,ah,b, aris-
ing from two different inputs x0a and xb0 with given covariance qa0b . The mean field approxima-
tion used here replaces each element in the pre-activation field hi` by a Gaussian random variable
whose moments are matched. Assuming also independence within a layer; Ehi；ahj；a = q'aδj and
Eh'；ahj巧 =qfabδij, one can derive recurrence relations from layer to layer,
q'a = σW / Dzφ[qq-Z) + σ2 = σWEφ2(h%1) + 彘	(9)
2
with Dz = √dz∏e-^W the standard Gaussian measure. The recursion for the covariance is given by
qab = σW / DzIDz2φ(ua)φ(ub) + σ2 = σWE[φ(h'-1 )φ(hj-1)] + σ2	(IO)
where Ua = Pqa-1 zι, Ub = Jq'-1(cf- 1zι + Jl - (Ca-1)2z2), and we identify Cab as the corre-
lation in layer `. The other important quantity is the slope of the correlation recursion equation or
mapping from layer to layer, denoted as X, which is given by:
∂C' L C f —	—	.....
X =	'-1 = σW DzI Dz2 φ (ua)φ (Ub)
∂cab
(11)
We denote X at the fixed point C = 1 as χι. As discussed Poole et al. (2016), when χ1 = 1,
correlations can propagate to arbitrary depth.
Definition 1: Critical initialisations are the points (σb2, σw2 ) corresponding to χ1 = 1.
Furthermore, χ1 is equivalent to the mean square singular value of the Jacobian matrix for a sin-
∂h'
gle layer Jij = 诩二,as explained in Poole et al. (2016). Therefore controlling χ1 will prevent
j
the gradients from either vanishing or growing exponentially with depth. We thus define critical
initialisations as follows. This definition also holds for the surrogates which we now study.
3.2	Signal propagation theory for deterministic surrogates
For the deterministic surrogate model we assume at initialization that the binary weight means Mj
are drawn independently and identically from a distribution P(M), with mean zero and variance of
the means given by σm2 . For instance, a valid distribution could be a clipped Gaussian4, or another
stochastic binary variable, for example P(M) = 11 δ(M + σm/) + 1 δ(M — σm,), whose variance is
σ21. The biases at initialization are distributed as b` 〜N(0, σb).
We show in Appendix B that the stochastic and deterministic binary neuron cases reduce to the same
signal propagation equations, up to scaling constants. In light of this, we consider the deterministic
4That is, sample from a Gaussian then pass the sample through a function bounded on the interval [-1, 1].
6
Published as a conference paper at ICLR 2020
sign(∙) neuron case, since equation for the field is slightly simpler:
Pj Mij 以 hj-1)+ √N-τ b
qPj[1-(Mj)%2 (h'-1)]
(12)
which we can be read from the Eq. 7. As in the continuous case we are interested in computing the
variance q^a = N Pi(h'J and Covariance Ehl.ahj.b = q^δij, via recursive formulae. The key
to the derivation is recognising that the denominator
.∑MFii is a self-averaging quantity Mezard
et al. (1987). This means it concentrates in probability to its expected value for large N. Therefore
we can safely replace it with its expectation. Following this self-averaging argument, we can take
expectations more readily as shown in the appendices. We find the variance recursion to be
' =σm 即 2(hj-aI) 十 σ
qaa= 1 - σmEd(hj-a1)
Based on this expression, and assuming qaa = qbb, the correlation recursion can be written as
(13)
` _ ι+q'a σmM(Jhja)中时/+σ
cab=ιaa	ι+σ	()
The slope of the correlation mapping from layer to layer, when the normalized length of each input
is at its fixed point q^1a = qbb = q* (σm, σb), denoted as χ, is given by:
∂c',	1 + q*
X = ∂C`-1 = 1 + σ σm J DzIDzW (UaW (Ub)	(15)
where ua and ub are defined exactly as in the continuous case. Refer to the appendices for full
details of the derivation.
3.2.1	Critical initialisation: deterministic surrogate
The condition for critical initialisation is χ1 = 1, since this determines the stability of the correlation
map fixed point c* = 1. Note that for the deterministic surrogate this is always a fixed point. We can
solve for the hyper-parameters (σb2 , σm2 ) that satisfy this condition, using the dynamical equations
of the network.
Claim 1: The points (σb2 , σm2 ) corresponding to critical initialisation are given by σm2 =
1∕E[(夕0 (√q*z))2] + E[夕2 (√q*z)] and finding σ2 that satisfies
q'a = σb + (σ2 + 1)	j[(M聘
E2(√q⅞))2]
This can be established by rearranging Equations 13 and 15. We solve for σb2 numerically, as shown
in Figure 3, for different neuron noise models and hence non-linearities 夕(∙). We find that the critical
initialisation for any of these design choices is close to the point (σm2 , σb2) = (1, 0). However, it is
not just the singleton point, as for example in Hayou et al. (2019) for the ReLu case for standard
networks. We plot the solutions in the Appendix.
3.2.2	Asymptotic expansions and depth scales
The depth scales, as derived in Schoenholz et al. (2016) provide a quantitative indicator to the
number of layers correlations will survive for, and thus how trainable a network is. Similar depth
scales can be derived for these deterministic surrogates. Asymptotically in network depth `, we
expect that ∣q'a - q* | 〜exp(-ξ-) and * - c* | 〜exp(-ξ-), where the terms ξq and ξc define
the depth scales over which the variance and correlations of signals may propagate. We are most
interested in the correlation depth scale, since it relates to χ. The derivation is identical to that of
Schoenholz et al. (2016). One can expand the correlation Cab = c* + e', and assuming q'0 = q*, it
is possible to write
e'+1 = e'[ 1 + σ2 σm / Dzd(U1)”(U2)] + O((C')2)
(16)
7
Published as a conference paper at ICLR 2020
The depth scale ξc-1 are given by the log ratio log
e'+1
~7'~.
- log
1 + q*
σ^m / Dzd(UIWO(U2»
- log χ
(17)
We plot this depth scale in Figure 2. We derive the variance depth scale in the appendices, since it
is different to the standard continuous case, but not of prime practical importance.
3.3	Signal propagation theory for local reparameterization trick surrogates
From Equation 8, the pre-activation field for the perturbed surrogate with both stochastic binary
weights and neurons is given by,
hi,a = F
+ bi + e',a √√N jχ 1-(Mj)2Φ2 j
(18)
j
where We recall that E ~ N(0,1). The non-linearity φ(∙) can of course be derived from any valid bi-
nary stochastic neuron model. Appealing to the same self-averaging arguments used in the previous
section, we find the variance map to be
q'a = E [(hi,a)2] = σmEφ2( ja1)+ σb + (1-点Eφ2( j；1)) = 1+ 式 (19)
Interestingly, we see that the variance map does not depend on the variance of the means of the binary
weights. This is not immediately obvious from the pre-activation field definition. In the covariance
map we do not have such a simplification since the perturbation Ei,a is uncorrelated between inputs
a and b. Thus the correlation map is given by
(20)
3.4	Critical initialisation: LRT surrogates
Claim 2: There is no critical initialisation for the local reparameterisation trick based surrogate,
for a network with binary weights and neurons.
Proof: The conditions for a critical initialisation are that c* = 1 to be a fixed point and χ1 = 1.
No such fixed point exists. We have a fixed point c* = 1 if and only if σm = 1∕E[φ2(hj^1)].
Note that σm2 ≤ 1. For any φ(z) which is the mean of the stochastic binary neuron, the expectation
E[φ2(z)] ≤ 1. For example, consider φ(z) = tanh(κz) for any finite kappa.
We also considered the LRT surrogate with continuous (tanh(∙)) neurons and stochastic binary
weights. The derivations are very similar to the previous case, as we show in the appendix. The
variance and correlation maps are given by
(21)
This leads to the following result,
Claim 3: The critical initialisationfor the LRTSurrogate,for the case ofcontinuous tanh(∙) neurons
and stochastic binary weights is the singleton (σb2, σm2 ) = (0, 1).
Proof: From the correlation map we have a fixed point c* = 1 if and only if σm2 = 1, by inspection.
In turn, the critical initialisation condition χ1 = 1 holds if E[(φ0(h-1))2] = σ⅛- = 1. Thus, to find
the critical initialisation, we need to find a value of qaa = Eφ2(hlj-,a1) + σb2 that satisfies this final
condition. In the case that φ(∙) = tanh(∙), then the function (φ0(hj^1))2 ≤ 1, taking the value 1 at
the origin only, this requires qaa → 0. Thus we have the singleton (σb2, σm2 ) = (0, 1) as the solution.
8
Published as a conference paper at ICLR 2020
Figure 1: Dynamics of the variance and correlation maps, with simulations of a network of width
N = 1000, 50 realisations, for various hyperparameter settings: σm2 ∈ {0.2, 0.5, 0.99} (blue, green
and red respectively). (a) variance evolution, (b) correlation evolution. (c) correlation mapping (cin
to cout), with σb2 = 0.001
4	Numerical and experimental results
4.1	Simulations
We first verify that the theory accurately predicts the average behaviour of randomly initialised
networks. We present simulations for the deterministic surrogate in Figure 1. We see that the
average behaviour of random networks are well predicted by the mean field theory. Estimates of
the variance and correlation are plotted, with dotted lines corresponding to empirical means and
the shaded area corresponding to one standard deviation. Theoretical predictions are given by solid
lines, with strong agreement for even finite networks. Similar plots can be produced for the LRT
surrogate. In Appendix D we plot the depth scales as functions of σm and σb .
4.2	TRAINING PERFORMANCE FOR DIFFERENT MEAN INITIALISATION σm2
Here we experimentally test the predictions of the mean field theory by training networks to overfit
a dataset in the supervised learning setting, having arbitrary depth and different initialisations. We
consider first the performance of the deterministic and LRT surrogates, not their corresponding
binary networks.
We use the MNIST dataset with reduced training set size (50%) and record the training performance
(percentage of the training set correctly labeled) after 10 epochs of gradient descent over the training
set, for various network depths L < 70 and different mean variances σm2 ∈ [0, 1). The optimizer
used was SGD with Adam Kingma & Ba (2014) with a learning rate of 2 × 10-4 chosen after simple
grid search, and a batch size of 64. We see that the experimental results match the correlation depth
scale derived, which are overlaid as dotted curves. A proportion of 3ξc was found to indicate the
maximum attenuation in signal strength before trainability becomes difficult, similarly to previous
works Schoenholz et al. (2016).
A reason we see the trainability not diverging in Figure 2 is that training time increases with depth,
on top of requiring smaller learning rates for deeper networks, as described in Saxe et al. (2013). The
experiment here used the same number of epochs regardless of depth, meaning shallower networks
actually had an advantage over deeper networks. Note that the theory does not specify for how many
steps of training the effects of critical initialisation will persist. Therefore, the number of steps we
trained the network for is an arbitrary choice, and thus the experiments validate the theory in a more
qualitative way. Results were similar for other optimizers, including SGD, SGD with momentum,
and RMSprop. Note that these networks were trained without dropout, batchnorm or any other
heuristics.
In Figure 2 we present the training performance for the deterministic surrogate and its stochastic bi-
nary counterpart. The results for a deterministic binary network were similar to a single Monte Carlo
sample. Once again, we test our algorithms on the MNIST dataset and plot results after 5 epochs.
We see that the performance of the stochastic network matches more closely the performance of the
continuous surrogate as the number of samples increases, from N = 5 to N = 100 samples. We can
report that the number of samples necessary to achieve better classification, at least for more shal-
low networks, appears to depends on the number of training epochs. This is a sensible relationship,
since during the course of training we expect the means of the weights to polarise, moving closer to
9
Published as a conference paper at ICLR 2020
the bounds ±1. Likewise, we expect that neurons, which initially have zero mean pre-activations,
will also “saturate” during training, becoming either always “on” (+1) or “off” (-1). A stochastic
Figure 2: Top: Training performance of the deterministic surrogate (left) and the LRT surrogate for
stochastic binary weights and continuous neurons (right). The vertical axis represents network depth
against the variance of the means σm2 . Both surrogates were trained with σb2 = 0. Thus, as σm2 → 1
we approach criticality in both cases. Overlaid are curves proportional to the correlation depth scale
ξc . Bottom: Training performance of the deterministic surrogate and its binary counterparts after
training on the MNIST dataset for 5 epochs. Left: performance of the continuous surrogate. Centre:
the performance of the stochastic binary network, averaged over 5 Monte Carlo samples. Right:
100 Monte Carlo samples. The deterministic binary evaluation is similar to a single Monte Carlo
sample, resembling the central figure.
5	Discussion
This study of two classes of surrogate networks, and the derivation of their initialisation theories has
yielded results of practical significance. Based on the results of Section 3, in particular Claims 1-3,
we can offer the following advice. If a practitioner is interested in training networks with binary
weights and neurons, one should use the deterministic surrogate, not the LRT surrogate, since the
latter has no critical initialisation. If a practitioner is interested in binary weights only,the LRT in
this case does have a critical initialisation (and is the only choice from amongst these two classes
of surrogate). Furthermore, both networks are critically initialised when σb2 → 0 and by setting the
means of the weights to ±1.
It was seen that during training, when evaluating the stochastic binary counterparts concurrently with
the surrogate, the performance of binary networks was worse than the continuous model, especially
as depth increases. We reported that the stochastic binary network, with more samples, outperformed
the deterministic binary network, a reasonable result since the objective optimised is the expectation
over an ensemble of stochastic binary networks.
A study of random deterministic binary networks, included in the Appendices, and published re-
cently Blumenfeld et al. (2019) for a different problem, reveals unsurprisingly that binary networks
are always in a chaotic phase. However a binary network which is trained via some algorithm will
of course have different signal propagation behaviour. It makes sense that the closer one is to the
early stages of the training process, the closer the signal propagation behaviour is to the randomly
initialised case. We might expect that as training progresses the behaviour of the binary counter-
parts approaches that of the trained surrogate. Any such difference would not be observed for a
heuristic surrogate as used in Courbariaux & Bengio (2016) or Rastegari et al. (2016), which has no
continuous forward propagation equations.
10
Published as a conference paper at ICLR 2020
References
Carlo Baldassi, Federica Gerace, Hilbert J. Kappen, Carlo Lucibello, Luca Saglietti, Enzo
Tartaglione, and Riccardo Zecchina. Role of synaptic stochasticity in training low-precision neu-
ral networks. Phys. Rev. Lett., 120:268103, Jun 2018. doi: 10.1103/PhysRevLett.120.268103.
URL https://link.aps.org/doi/10.1103/PhysRevLett.120.268103.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation, 2013.
Yaniv Blumenfeld, Dar Gilboa, and Daniel Soudry. A mean field theory of quantized deep networks:
The quantization-depth trade-off, 2019.
Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights
and activations constrained to +1 or -1. CoRR, abs/1602.02830, 2016. URL http://arxiv.
org/abs/1602.02830.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth Interna-
tional Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine
Learning Research, pp. 249-256, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010. PMLR.
URL http://proceedings.mlr.press/v9/glorot10a.html.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function
on deep neural networks training. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pp. 2672-2680, Long Beach, California, USA, 09-15 Jun 2019.
PMLR. URL http://proceedings.mlr.press/v97/hayou19a.html.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114,
2013.
Marc Mezard, Giorgio Parisi, and Miguel Virasoro. Spin Glass Theory and Beyond, volume 9. 01
1987. doi: 10.1063/1.2811676.
Jorn W. T. Peters and Max Welling. Probabilistic binary neural networks. CoRR, abs/1809.03368,
2018. URL http://arxiv.org/abs/1809.03368.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems 29, pp. 3360-3368. Curran Associates, Inc., 2016.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In Bastian Leibe, Jiri Matas, Nicu Sebe,
and Max Welling (eds.), Computer Vision - ECCV 2016, pp. 525-542, Cham, 2016. Springer
International Publishing. ISBN 978-3-319-46493-0.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlin-
ear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013. URL
http://arxiv.org/abs/1312.6120.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. CoRR, abs/1611.01232, 2016. URL http://arxiv.org/abs/1611.01232.
Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameteri-
zation trick. CoRR, abs/1710.07739, 2017. URL http://arxiv.org/abs/1710.07739.
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of
multilayer neural networks with continuous or discrete weights. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems 27, pp. 963-971. Curran Associates, Inc., 2014.
11
Published as a conference paper at ICLR 2020
A Derivation of deterministic surrogate networks
A.1 Integrating over stochastic or deterministic binary neurons
The form of each neuron’s probability distribution depends on the underlying noise model. We can
express a stochastic binary random variable S ∈ {±1} with S 〜 p(S; θ) via its latent variable
formulation,
S = sign(θ + αL)	(22)
In this form θ is referred to as a “natural” parameter, from the statistics literature on exponential
families. The term L is a latent random noise, which determines the form of the probability distribu-
tion. We also introduce a scaling α to control the variance of the noise, so that as α → 0 the neuron
becomes a deterministic sign function. Letting α = 1 for simplicity, we see that the probability of
the binary variable taking a positive value is
p(S = +1)
Z-θ
-∞
p(L)dL
(23)
where p(L) is the known probability density function for the noise L. The two common choices of
noise models are Gaussian or logistic noise. The Gaussian of course has shifted and scaled erf(∙)
function as its cumulative distribution. The logistic random variable has the classic “sigmoid” or
logistic function as its CDF, σ(z) = 1+；一.
Thus, the probability of a the variable being positive is a function of the CDF. In the Gaussian case,
this is Φ(θ). By symmetry, the probability of p(S = -1) = Φ(-θ). Thus, we see the probability
distribution for the binary random variable in general is the CDF of the noise L, and we write
p(S) = Φ(Sθ). In the logistic noise case, we have p(S) = σ(Sθ)
For the stochastic neurons, the natural parameter is the incoming field h' = Pj S'/'-1 + b'. AS-
suming this is approximately Gaussian in the large layer width limit, we can successively marginalise
over the stochastic inputs to each neuron, calculating an approximation of each neuron’s probability
distribution, p(x'). This approximation is then used in the central limit theorem for the next layer,
and so on.
For the case of neurons with latent Gaussian noise as part of the binary random variable model, the
integration over the pre-activation field (assumed to be Gaussian) is exact. Explicitly,
P(Xi) = E Ep(X'lx'-1, Se)P(S'T)p(χ')
x'—1 S'
≈ Z Φ(x'h')N(h'∣h', (∑Mf)ii)
=φ ( /	hi	= Xi) = P(Xi)
1 + 2(ΣMF)ii
(24)
where Φ(∙) is the CDF of the Gaussian distribution. We have again ∑mf denoting the mean field
approximation to the covariance between the stochastic binary pre-activations. The Gaussian ex-
pectation of the Gaussian CDF is a known identity, which we state in more generality in the next
section, where we also consider neurons with logistic noise.
This new approximate probability distribution p(x') can then used as part of the Gaussian CLT
applied at the next layer, since it determines the means of the neurons in the next layer,
Ex' = 2φ(	h'	- - - 1	(25)
1 + (ΣiMF)ii
If we follow these setps from layer to layer, we see that we are actually propagating approximate
means for the neurons, combined non-linearly with the means of the weights. Given the approxi-
mately analytically integrated loss function, it is possible to perform gradient descent with respect
to the means and biases, Mi'j and bi' .
12
Published as a conference paper at ICLR 2020
In the case of deterministic sign() neurons we obtain particularly simple expressions. In this case
the “probability” of a neuron taking, for instance, positive is just Heaviside step function of the
incoming field. Denoting the Heaviside with Θ(∙), we have
P(Xf) = X XP(X'lx'-1, Se)P(S'T)P(X'T)
x'-1 S'
≈ Z Θ(χ'h')N(h'∣h', (∑Mf)ii)
≈ Φ ( —hi-∑τXi) = P(x')	(26)
ΛςMf)ii 2	)
We can write out the network forward equations for the case of deterministic binary neurons, since
it is a particularly elegant result. In general we have
χ' = φ(ηhi),	hi = √∑mfhi, hi = Miχi-1 + bi	(27)
where φ(∙) = erf(∙) is the mean of the next layer of neurons, being a scaled and shifted version of
the neuron,s noise model CDF. The constant is η = =, standard for the Gaussian CDF to error
functin conversion.
A.2 Exact and approximate Gaussian integration of sigmoidal functions
We now present the integration of stochastic neurons with logistic as well as Gaussian noise as
part of their latent variable models. The logistic case is an approximation built on the Gaussian
case, motivated by approximating the logistic CDF with the Gaussian CDF. The reason we may be
interested in using logistic CDFs, rather than just considering latent Gaussian noise models which
integrate exactly, is not justified in any rigorous or experimental way. Any such analysis would likely
consider the effect of the tails of the logistic versus the Gaussian distributions, where the logistic
tails are much heavier than those of the Gaussian. One historic reason for considering the logistic
function, we note, is the prevalence oflogistic-type functions (such as tanh(∙)) in the neural network
literature. The computational cost of evaluating either logistic or error functions is similar, so there
is no motivation from the efficiency side. Instead it seems a historic preference to have logistic type
functions used with neural networks.
As we saw in the previous subsection, the integration over the analytic probability distribution for
each neuron gave a function which allows us to calculate the means of the neurons in the next layer.
Therefore, we directly calculate the expression for the means.
The Gaussian integral of the Gaussian CDF was used in the previous section to derive the exact
probability distribution for the stochastic binary neuron in the next layer. The result is well known,
and can be stated in generality as follows,
ZC∞	----o 
∞	e 2σ2	x
∞ φ(ay) √2∏σ^dy=φ( tt+op )
(28)
We can integrate a logistic noise binary neuron using this result as well. The idea is to approximate
the logistic noise with a suitably scaled Gaussian noise. However, since the overall network approx-
imation results in propagating means from layer to layer, we can equivalently need to approximate
the tanh(∙) with the with the erf. Specifically, if we have f (x; α) = tanh(X), an approximation
is g(x; α) = erf(娈x), by requiring equality of derivatives at the origin. In order to establish this,
consider
f0(0; α) = (1 — tanh2(0∕α)L =—
αα
(29)
and
d erf (x; σ)
-dX- lx=0
(30)
13
Published as a conference paper at ICLR 2020
Equating these, gives σ2 = 4∏2, thus σ = √∏.
The approximate integral over the stochastic binary neuron mean is then
Z∞
∞
(y-χ)2
e 2σ2
f(y；α) 不~2 dy ≈
2πσ2
Z	erf( Py) e √^2~2 dy
-∞	2α	2πσ 2
= erf(
五x)
2αγ )
with γ
(31)
(32)
(33)
If We so desire, We can approximate this again with a tanh(∙) using the tanh(∙) to erf(∙) approxi-
mation in reverse. The scale parameter of this tanh(∙) will be α2 = 4απγ. If α = 1 as is standard,
then
πx
≈ tanh(4γ)
(34)
B Equivalence of deterministic and stochastic neurons for
DETERMINISTIC S URRO GATE
Assume a stochastic neuron with some latent noise, as per the previous appendix, with mean x' =
Ep(Xi)x' = Φ(h'-1). The field is given by
「 1	Pj MijΦ(h'-1) + b'
hi = -ʒ=	/
v2√1 + 2 Pj[1-(Mj)2Φ2 (h'-1)]
(35)
We see that the expression for the variance of the field simplifies as follows,
E(hi)2 =ι	Pj M Φ(h'-1) +b'
21 + 2Pj[1 - (Mj)2Φ2(h'-1)]
=1	N(% Eφ2(hj-a1)+。2)
―21 + 2(N - NσmEφ2(hj-a1))
1	σm2 Eφ2(hlj-,a1) +σb2
=	;—: 
2	2(1 - σmEφ2(hjr1))
(36)
(37)
(38)
By similar steps, we find that in the deterministic binary neuron case, we would obtain the same
expression, albeit with a different scaling constant. This is easily seen by inspection of the field term
in the deterministic neuron case,
1	Pj Mij φ(hi-1) + b'
√2 qPj[1-(Mj)2Φ2(h'τ)]
(39)
which again was derived in the previous appendix.
C Derivation of signal propagation equations in deterministic
SURROGATE NETWORKS
Here we present the derivations for the signal propagation in the continuous network models studied
in the paper.
14
Published as a conference paper at ICLR 2020
C .1 Variance propagation
We first calculate the variance given a signal:
九=N XMa)2= E [MM
i
Where for us:
hl	Pj mij φ (%-1) + bi
hi,a = -/	=
，Pj (l-(mj)2 Φ2 (⅛α1))
and
mij 〜N (0, σim) bi 〜N(0, Ni-iσ2)
(40)
(41)
(42)
1-喘 Eφ2 (ja1)
(43)
Where, Eφ2 (hj-a1) can be written explicitly, taking into account that h1— 〜 N (0, q0α):
E [φ2 (jH = Z Dhjama=."后E ["a)2i exp (-2EjJ2]) φ2 (ja)
=M p⅛ exp 卜 j 卜(j)	(44)
We can now perform the following change of variable:
(45)
Then:
σ2n R Dzφ2 (Pqa-IZ) + σ2
1 -喙 R Dzφ2 (PaaTz)
(46)
(47)
In the first layer, input neurons are not stochastic: they are samples drawn from the Gaussian distri-
bution x0 〜N(0, q0):
15
Published as a conference paper at ICLR 2020
C.1.1 Correlation propagation
To determine the correlation recursion we start from its definition:
clab
qla,b
VZ qaaqb
(48)
where qal b represents the covariance of the pre-activations hli,a and hli,b, related to two distinct input
signals and therefore defined as:
qab = N ^X hi,a hi,b = E [hi,a hi,b] .
(49)
Replacing the pre-activations with their expressions provided in eq. (41) and taking advantage of
the self-averaging argument, we can then write:
I	σm E [φ (hj-a1) φ(hj-b1) ] + σ22
cab = —	- ~/	.
Jqaa (ι-σm E [φ2 (hj,;1)]) ʌ/ 弧(ι - σm E 卜2 (居51)])
(50)
At this point, given that qal a and qblb quite quickly approach the fixed point, we can conveniently as-
sume qala = qblb. Moreover, exploiting eq.(47), we can finally write the expression for the correlation
recursion:
clab
+ σb2
(51)
C.2 Derivation of the slope of the correlations at the fixed point
To check the stability at the fixed point, we need to compute the slope of the correlations mapping
from layer to layer at the fixed point:
Xl =5
χlq*	d J-1
∂cab
1 + q* σm2
∂
E
|q*
/ DzaDzbφ (Ua) φ (Ub) |q*
(52)
q*	1 + σ2 dca-1
1 + q*	σm2	∂
q*
where we get rid of σb because independent from cla-b 1. Replacing the definition of Ua and Ub
provided in the continuous model, we can explicitly compute the derivative with respect to cla-b 1:
X=* ⅛(A - B),
(53)
where we have defined A and B as:
16
Published as a conference paper at ICLR 2020
A
B
√q /
√q /
DzaDzbφ
DzaDzbφ
cla-b1
cla-b1
za +
za +
cl-1
cab_______
cιτ)2
cab
zb.
(54)
We can focus on B first. Integrating by parts over zb we get:
B
√*. /
DzaDzbφ
cla-b1
za +
(55)
Then, integrating by parts over za, we the get:
√*∙ /
DzaDzbφ
za +
za +
cla-b1
-q* / DzaDzbφ0
cla-b1
za +
(56)
Replacing A and B in eq. (53), we then obtain the closest expression for the stability at the variance
fixed point, namely:
X|q* = 1 + ；2 σm / DzaDzbφ0 (Ua) φ (Ub)	(57)
C.3 Variance depth scale
As pointed out in the main text, it should hold asymptotically that:
ιqa+1 - q* | ~ eχp (	ξ-
(58)
with ξq defining the variance depth scale. To compute it we can expand over small perturbations
around the fixed point, namely:
qal+a1 = q* + l
σm R Dzφ2 (Pq* + elz) + σ2
1 - σm R Dzφ2 (pq* + elz)
(59)
Expanding the square root for small el, we can then write:
σm R Dzφ2 (√q*z + 2√q*z) + σ2
1 - σm R Dzφ2 (√q*z + 2√q*z.)
(60)
We can now expand the activation function φ around small perturbations and then computing the
square getting rid of higher order terms in el, thus finally obtaining:
B
17
Published as a conference paper at ICLR 2020
qla+1 ` q* +
1 + q* σm / Dzφ (√q*z) φ (√q*z) Z I
√q*	1 - σ2n R Dzφ2 (√q*z)	e
(61)
Comparing this expression with the one in eq. (59), we can then write:
j+1 ` 1 + q* σm / Dzφ (√q*Z) φ (√q*z) Z I
√ √q*	1- σ2n R Dzφ2 (√q*z)	e
(62)
Integrating by parts over Z, we then obtain:
l+1 '
(1 + q*)
σm R Dzφ0 (√q*z)φ (√q*z)+R Dzφ0 (√q* Z)φ (√q*z)
1—σm JDZφ2 (√q*z)
l.
(63)
Given that it holds eq. (47), and noticing that χ evaluated at the correlation fixed point c* = 1 is
given by:
χ∣c*=ι = τ^m⅛ (1 + q*) / Dz [Φ0 (√q*Z)]2 ,	(64)
1 + σb
we can finally get:
e'+1' X∣c* = ι + %二四) Z DZφ00 (√q*Z) φ (√q*Z) ɪlɪ.	(65)
Given that we expect (58) to hold asymptotically, that is:
el+1 〜exp (-1+1) ,	(66)
we can finally obtain the variance depth scale:
ξ-1 = log (1 + q*) — log (χ∣c* = ι + *1(++q*) /DzΦ" (√q*Z) Φ(√7z)) .	(67)
D Supplementary Figures
D.1 Critical initialisation simulations: deterministic surrogate case
We see in Figure 3 that the set of critical initialisations exist in the plane, for but σb2 > 10-20 all the
corresponding mean variances σ+ m2 > 1 which is not possible.
D.2 Depth scales
We see in Figure 4 the depth scales for the deterministic surrogate. Note the divergence as one
expects following the simulations in Figure 3.
18
Published as a conference paper at ICLR 2020
Figure 3: Plots of the valid critical initialisations for the deterministic surrogate model, for stochastic
binary weights and stochastic or deterministic binary neurons. Presented are the critical initialisa-
tions in the (0也,σ2), for both the a) stochastic neuron case with φ(z) = erf( 4 z), b) the determinis-
tic sign neuron case with φ(z) = erf(2 ∙), and (C) the logistic based stochastic neuron, with tanh()
approximation. We see all lines are above σ2 = 1 for all but small σb2 << 1.
(a) The depth scale controlling the variance propagation
of a signal (b) The depth scale controlling correlation propagation of two signals. Notice that the
correlation depth scale ξc only diverges as σm2 → 1, whereas for standard continuous networks,
there are an infinite number of such points, corresponding to various combinations of the weight and
bias variances.
19
Published as a conference paper at ICLR 2020
D.3 Jacob ian mean s quared singular value and Mean Field Gradient
Backpropagation
An alternative perspective on critical initialisation, to be contrasted with the forward signal prop-
agation theory, is that we are simply attempting to control the mean squared singular value of the
input-output Jacobain matrix of the entire network, which we can decompose into the product of
single layer Jacobian matrices. In standard networks, the single layer Jacobian mean squared singu-
lar value is equal to the derivative of the correlation mapping χ as established in Poole et al. (2016).
For the Gaussian model studied here this is not true, and corrections must be made to calculate the
true mean squared singular value. This can be seen by observing the terms arising from denominator
of the pre-activation field,
J' = dh',a
ij — ∂hj-
j,a
(68)
Since ∑n is a quantity that scales with the layer width n`, it is clear that when We consider squared
quantities, such as the mean squared singular value, the second term, from the derivative of the
denominator, will vanish in the large layer width limit. Thus the mean squared singular value of the
single layer Jacobian approaches χ. We will proceed as if χ is the exact quantity we are interested
in controlling.The analysis involved in determining whether the mean squared singular value is well
approximated by χ essentially takes us through the mean field gradient backpropagation theory as
described in Schoenholz et al. (2016). This idea provides complementary depth scales for gradient
signals travelling backwards.
E Reparameterisation trick surrogate
E.1 Signal propagation equations
We present, in slightly more detail, the signal propagation equations for the case of continuous
neurons and stochastic binary weights yields the variance map,
qaa
(69)
Thus, once again, the variance map does not depend on the variance of the means of the binary
weights. The covariance map however does retain a dependence on σm2 ,
qalb=σm2Eφ(hlj-,a1)φ(hlj-,a1)+σb2
with the same expression as before. The correlation map is given by
l = σm Eo(h-a1欣hj-I)+σ
cab = -Eφ2 (hj-1) + σ一
and we have the derivative of the correlation map given by
χ=σm2Eφ0(hlj-,a1)φ0(hlj-,b1)
(70)
(71)
(72)
E.2 Determining the critical initialisation conditions
We recount the argument from the paper here. Since the mean variance σm2 does not appear in the
variance map, we must once again consider different conditions for critical initialisation. Specifi-
cally, from the correlation map We have a fixed point C = 1 if and only if
σm2 = 1	(73)
In turn, the condition χ1 = 1 holds if
(74)
Thus, to find the critical initialisation, we need to finda value of qaa = Eφ2(hlj-,a1) +σb2 that satisfies
this final condition. In the case that φ(∙) = tanh(∙), then the function (φ0(hj-^1))2 ≤ 1, taking the
20
Published as a conference paper at ICLR 2020
value 1 at the origin only, this requires qaa → 0. Thus the critical initialisation is the singleton point
(σb2 , σm2 ) = (0, 1). This is confirmed by experiment, as we reported in the paper.
It is of course possible to investigate this perturbed surrogate for different noise models. For exam-
ple, given different noise scaling κ, as in the previous chapter, there will be a corresponding σb2 that
satisfy the critical initialisation condition. We leave such an investigation to future work, given the
case of binary weights and continuous neurons does not appear to be of a particular interest over the
binary neuron case.
F Signal propagation of binary networks
F.1 Forward signal propagation
In this neural network, it should be understood that all neurons are simply sign(∙) functions of their
input, and all weights Wj ∈ {±1} are randomly distributed according to
P(Wj = +1) = 0.5
(75)
(76)
thus maintaining a zero mean.
The pre-activation field is given by
h' = PN=I XX Wj sign(hjτ) + b'	(77)
So, the length map is:
qfaa = / Dz(Sign( yqa=1 Z)2 ) + σ2	(78)
= 1+σb2	(79)
Interestingly, this is the same value as for the perturbed Gaussian with stochastic binary weights and
neurons.
The covariance evolves as
q'b = / DzιDz2 Sign(Ua) Sign(Ub) + σ2
(80)
we again have a correlation map:
Cab
Dz1Dz2 Sign(Ua) Sign(Ub) + σb2
Jqa=Iq'=1
(81)
where as in the paper, Ua =，q'=1 zι,
Ub= ≠bb⅜a=1zι+J1-d=1)2 z2).
We can find this correlation in closed form. First we rewrite our integral with h, for a joint density
p(ha, hb), and then rescale the ha such that the variance is 1, so that dha = √qa∑dva
dhadhbSign(ha) Sign(hb)p(ha, hb)
dvadvbSign(va) Sign(vb)p(va,vb)
2P (v1 > 0,v2 > 0) - 2P (v1 > 0,v2 < 0)
(82)
(83)
where p(va ,vb) is a joint with the same correlation cab (which is now equal to its covariance), and
the capital P(v1,v2) corresponds to the (cumulative) distribution function. A standard result for
standard bivariate normal distributions with correlation ρ,
P(vι	>	0,v2	> 0) = 1 +sin/(P),	P(vι	>	0,v2	< 0) =	coj0	(84)
4	2π	2π
21
Published as a conference paper at ICLR 2020
So we then have that
1	_____11	sin-1(c'-1)
dhadhbφ(ha)φ(hb)p(ha ,hb) = √qOOqbb( 2 +-----∏-----
COsT(Ca-I))
π
Thus the correlation map is:
Cab
(1 + sin-1(Ca-I)
Jqa-Iq'-1
2 SinT(Ca-I)十σ
∕q'-1qb-1
Since, from before we have qaa = 1 + σb2, we then obtain
(85)
(86)
(87)
(88)
Recall that sin-1(1) = 2, so we have that c* = 1 is a fixed point always.
....................................... ∂C∖	..	•八
We will now derive its slope, denoted as X = 嬴-b1, but by first integrating over the φ() = sign()
cab
non-linearities, and then taking the derivative.
Now we are in a place to take the derivative :
∂c'b	2	1	1	2	1	1
X = ---=-T =----,	-,	二=---------Fj--,	二
F π ”万，1-虑1)2	π (1+σb)，1-虑1)2
(89)
We can see that the derivative X diverges at Cab = 1, meaning that there is no critical initialisation
for this system. This of course means that correlations will not propagate to arbitrary depth in
deterministic binary networks, as one might have expected.
F.2 Stochastic weights and neurons
We begin again with the variance map,
qala=E(hli,a)2
where in this the field is given by
(90)
(91)
where xhl-1 denotes a stochastic binary neuron whose natural parameter is the pre-activation from
j,a
the previous layer.
The expectation for the length map is defined in terms of nested conditional expectations, since we
wish to average over all random elements in the forward pass,
qia = EhEχ∣hXh∣-ι + σ	(92)
= 1+σb2	(93)
Once again, this is the same value as for the perturbed Gaussian with stochastic binary weights and
neurons.
Similarly, the covariance map gives us,
qalb=Ehli,ahli,b	(94)
=Eha,hbExb|haExb|hbxhlj-,a1xhlj-,b1 +σb2	= Eφ(hlj-,a1)φ(hlj-,a1) + σb2	(95)
with phi(∙) being the mean function, or a shifted and scaled version of the cumulative distribution
function for the stochastic binary neurons, just as in previous Chapters. This expression is equivalent
to the perturbed surrogate for stochastic binary weights and neurons, with a mean variance of σm2 =
1. Following the arguments for that surrogate, no critical initialisation exists.
22
Published as a conference paper at ICLR 2020
F.3 Stochastic binary weights and continuous neurons
In this case, as we show in the appendix, the resulting equations are
q'a = Eφ2(hj-a1) + σb	(96)
qal b = Eφ(hlj-,a1)φ(hlj-,a1) + σb2	(97)
which are, once again, the same as for the perturbed surrogate in this case, with σm2 = 1. This means
that this model does have a critical initialisation, at the point (σm2 , σb2 ) = (1, 0).
F.4 Continuous weights and stochastic binary neurons
Similar arguments to the above show that the equations for this case are exactly equivalent to the
perturbed surrogate model. This means that no critical initialisation exists in this case either.
G	Miscellaneous comments
G.1 Remark: Valdity of the CLT for the first level of mean field
A legitimate immediate concern with initialisations that send σm2 → 1 may be that the binary
stochastic weights Sj are no longer stochastic, and that the variance of the Gaussian under the
central limit theorem would no longer be correct. First recall the CLT’s variance is given by
Var(h`) = Pj(1-mjxj'). Ifthemeans mj → ±1 then variance is equal in value to Pj mj(1-xj),
which is the central limit variance in the case of only stochastic binary neurons at initialisation.
Therefore, the applicability of the CLT is invariant to the stochasticity of the weights. This is not
so of course if both neurons and weights are deterministic, for example if neurons are just tanh()
functions.
23