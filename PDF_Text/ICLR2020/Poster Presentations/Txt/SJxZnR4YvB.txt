Published as a conference paper at ICLR 2020
Distributed Bandit Learning:	Near-Optimal
Regret with Efficient Communication
Yuanhao Wang*	Jiachen Hu*
Institute for Interdisciplinary Information Sciences, School of Electronics Engineering and Computer Science,
Tsinghua University	Peking University
yuanhao-16@mails.tsinghua.edu.cn	NickH@pku.edu.cn
Xiaoyu Chen
Key Laboratory of Machine Perception, MOE, School of EECS,
Peking University
cxy30@pku.edu.cn
Liwei Wang
Key Laboratory of Machine Perception, MOE, School of EECS
Center for Data Science, Peking University
wanglw@cis.pku.edu.cn
Ab stract
We study the problem of regret minimization for distributed bandits learning, in
which M agents work collaboratively to minimize their total regret under the
coordination of a central server. Our goal is to design communication protocols
with near-optimal regret and little communication cost, which is measured by the
total amount of transmitted data. For distributed multi-armed bandits, we propose
a protocol with near-optimal regret and only O(M log(M K)) communication
cost, where K is the number of arms. The communication cost is independent
of the time horizon T, has only logarithmic dependence on the number of arms,
and matches the lower bound except for a logarithmic factor. For distributed
d-dimensional linear bandits, we propose a protocol that achieves near-optimal
regret and has communication cost of order O ((Md + dlog log d) logT), which
has only logarithmic dependence on T .
1	Introduction
Bandit learning is a central topic in online learning, and has various real-world applications, including
clinical trials (Wang, 1991), model selection (Maron & Moore, 1994) and recommendation systems
(Agarwal et al., 2009; Li et al., 2010; Abe et al., 2003). In many tasks using bandit algorithms, it
is appealing to employ more agents to learn collaboratively and concurrently in order to speed up
the learning process. In many other tasks, the sequential decision making is distributed by nature.
For instance, multiple spatially separated labs may be working on the same clinical trial. In such
distributed applications, communication between agents is critical, but may also be expensive or
time-consuming. Another example is a recommendation system deployed on multiple servers to
handle high demand. Since the communication between servers may cause service latency, it would
be desirable to design communication strategies without communicating too much. This motivates us
to consider efficient protocols for distributed learning in bandit problems.
A straightforward communication protocol for bandit learning is immediate sharing: each agent shares
every new sample immediately with others. Under this scheme, agents can have good collaborative
behaviors close to that in a centralized setting. However, the amount of communicated data is directly
proportional to the total size of collected samples. When the bandit is played for a long timescale,
the cost of communication would render this scheme impractical. A natural question to ask is: How
* These two authors contributed equally.
1
Published as a conference paper at ICLR 2020
much communication is actually needed for near-optimal performance? In this work, we show that
the answer is somewhat surprising: The required communication cost has almost no dependence on
the time horizon.
In this paper, we consider the distributed learning of stochastic multi-armed bandits (MAB) and
stochastic linear bandits. There are M agents interacting with the same bandit instance in a syn-
Chronized fashion. In time steps t = 1,…，T, each agent pulls an arm and observes the associated
reward. Between time steps, agents can communicate via a server-agent network. Following the
typical formulation of single-agent bandit learning, we consider the task of regret minimization (Lai
et al., 1987; Dani et al., 2008; Bubeck et al., 2012). The total regret of all agents is used as the
performance criterion of a communication protocol. The communication cost is measured by the
total amount of data communicated in the network. Our goal is to minimize communication cost
while maintaining near-optimal performance, that is, regret comparable to the optimal regret of a
single agent in MT interactions with the bandit instance.
For multi-armed bandits, we propose the DEMAB protocol, which achieves near-optimal regret. The
amount of transmitted data per agent in DEMAB is independent of T, and is logarithmic with respect
to other parameters. For linear bandits, we propose the DELB protocol, which achieves near-optimal
regret, and has communication cost with at most logarithmic dependence on T.
1.1	Problem Setting
Communication Model The communication model we consider consists of a server and several
agents. Agents can communicate with the server by sending or receiving packets. Each data packet
contains integers or real numbers. We define the communication cost of a protocol as the number of
integers or real numbers communicated between server and agents1. We assume that communication
between server and agents has zero latency. Note that protocols in our model can be easily adapted to
a network without a server, by designating an agent as the server.
Distributed Multi-armed Bandits In distributed multi-armed bandits, there are M agents, labeled
1,...,M . Each agent is given access to the same stochastic K-armed bandit instance. Each arm k in
the instance is associated with a reward distribution Pk. Pk is supported on [0,1] with mean μ(k).
Without loss of generality, We assume that arm 1 is the best arm (i.e. μ(1) ≥ μ(k), ∀k ∈ [K]). At
each time step t = 1, 2, ..., T, each agent i chooses an arm at,i, and receives reward rt,i independently
sampled from Pat,i. The goal of the agents is to minimize their total regret, which is defined as
TM
REG(T) = XX
(μ(1) - μ(at,i)).
Distributed Linear Bandits In distributed linear bandits, the agents are given access to the same
d-dimensional stochastic linear bandits instance. In particular, we assume that at time step t, agents
are given an action set D ⊆ x ∈ Rd : kxk2 ≤ 1 . Agent i chooses action xt,i ∈ D and observes
reward yt,i. We assume that the mean of the reward is decided by an unknown parameter θ* ∈ Rd:
yt,i = XTiθ* + ηt,i, where ηt,i ∈ [-1,1] are independent and have zero mean. For simplicity, we
assume ∣∣θ* ∣∣2 ≤ 1. For distributed linear bandits, the cumulative regret is defined as the sum of
individual agent’s regrets:
TM
REG(T) =ΣΣ(^max xτθ* — XTiθ*).
Here, We assume that the action set is fixed. A more general setting considers a time-varying action
set Dt. In both cases, algorithms with O(d√Tlog T) regret have been proposed (Abbasi-Yadkori
et al., 2011), while a regret lower bound of Ω (d√T) is shown in Dani et al. (2008).
For both distributed multi-armed bandits and distributed linear bandits, our goal is to use as little
communication as possible to achieve near-optimal regret. Since any M -agent protocol running for
1In our protocols, the number of bits each integer or real number uses is only logarithmic w.r.t. instance scale.
Using the number of bits as the definition of communication complexity instead will only result in an additional
logarithmic factor. The number of communicated bits is analyzed at the end of Appendix C G and H
2
Published as a conference paper at ICLR 2020
Setting	Algorithm	Regret	Communication
Multi-armed	Immediate Sharing	O ZMKT)	O(M2T)
bandits	DEMAB (Sec. 3.2)	O (PMKT Iog(MK))	O(M log(MK))
	Lower bound (Sec. 3.4)	o (M √KT)	Ω(M)
	Immediate Sharing	O (d√MT log T)	O(M 2dT)
Linear bandits	DCB (Korda et al., 2016)	O (d√MT log T)	O (Md2T)
	DELB (Sec. 4.2)	O (d√MT log T)	O ((Md + d log log d) log T)
	DisLinUCB (Sec. 4.4)	O (d√MT log2 T)	O (M 1.5d3)
Table 1: Summary of baseline approaches and our results
T steps can be simulated by a single-agent bandit algorithm running for MT time steps, the regret of
any protocol is lower bounded by the optimal regret of a single-agent algorithm running for MT time
steps. Therefore, we consider O( MKT) regret for multi-armed bandits and O(d MT) regret for
linear bandits to be near-optimal.
We are mainly interested in the case where the time horizon T is the dominant factor (compared to
M or K). Unless otherwise stated, We assume that T > max{ M 嘴 M, M, K} in the multi-armed
bandits case and T > M in the linear bandits case.
1.2	Our Contribution
Now we give an overview of our results. In both settings, we present communication-efficient
protocols that achieve near-optimal regret. Our results are summarized in Table 1.
Our results are compared with a naive baseline solution called immediate sharing in Table 1: each
agent sends the index of the arm he pulled and the corresponding reward he received to every other
agent via the server immediately. ThiS protocol can achieve near-optimal regret for both MAB and
linear bandits (O(√MKT) and O(d√Mτ)), but comes with high communication cost (O(M2T)
and O(M2dT)).
Distributed MAB For distributed multi-armed bandits, we propose DEMAB (Distributed Elim-
ination for MAB) protocol, which achieves near-optimal regret (O (PMKT log(MK))) with
O(M log(M K)) communication cost. The communication cost is independent of the number of
time steps T and grows only logarithmically w.r.t. the number of arms. We also prove the following
lower bound: For any protocol with regret bound o(M√KT), the expected communication cost is at
least Ω(M). That is, in order to achieve near-optimal regret, the communication cost of DEMAB
matches our lower bound except for logarithmic factors.
Distributed Linear Bandits We propose DELB (Distributed Elimination for Linear Bandits), an
elimination based protocol for distributed linear bandits which achieves near-optimal regret bound
(O (d√MT log T)) with communication cost O ((Md + d log log d) log T). The communication
cost of DELB enjoys nearly linear dependence on both M and d, and has at most logarithmic
dependence on T. For the more general case where the action set is time-varying, we propose
DisLinUCB (Distributed LinUCB) protocol, which achieves near-optimal regret with O (M 1∙5d3)
communication cost.
2	Related Work
There has been growing interest in bandits problems with multiple players. One line of research
considers the challenging problem of multi-armed bandits with collisions (Rosenski et al., 2016;
3
Published as a conference paper at ICLR 2020
Bistritz & Leshem, 2018; Kalathil et al., 2014), in which the reward for an arm is 0 if it is chosen
by more than one player. The task is to minimize regret without communication. Their setting is
motivated by problems in cognitive radio networks, and is fairly different from ours.
SzOrenyi et al. (2013) and Korda et al. (2016) consider the distributed learning of MAB and linear
bandits with restriction on the communication network. Motivated by fully decentralized applications,
SzOrenyi et al. (2013) consider P2P communication networks, where an agent can communicate with
only two other agents at each time step. A gossip-based -greedy algorithm is proposed for distributed
MAB. Their algorithm achieves a speedup linear in M in terms of error rate, but the communication
cost is linear in T. The work of Korda et al. (2016) uses a gossip protocol for regret minimization
in distributed linear bandits. The main difference between their setting and ours is that each agent
is only allowed to communicate with one agent at each time step in Korda et al. (2016) 2. Their
algorithm achieves near-optimal(O (d√MT log T)) total regret using O(Md2T) communication
cost.
Another setting in literature concerns about distributed pure exploration in multi-armed bandits (Hillel
et al., 2013; Tao et al., 2019), where the communication model is the most similar one to ours. These
works use elimination based protocols for collaborative exploration, and establish tight bounds for
communication-speedup tradeoff. However, their task (speedup in pure exploration) is not directly
comparable to ours (i.e. are not reducible to each other). Moreover, in Hillel et al. (2013); Tao et al.
(2019), the number of communication rounds is used as the measure of communication, while we use
the amount of transmitted data.
3	Main Results for Multi-armed Bandits
In this section, we first summarize the single-agent elimination algorithm (Auer & Ortner, 2010), and
then present our Distributed Elimination for MAB (DEMAB) protocol. The regret and communication
efficiency of the protocol is then analyzed in Sec. 3.3. A communication lower bound is presented in
Sec. 3.4.
3.1	Elimination Algorithm for Single-agent MAB
The elimination algorithm (Auer & Ortner, 2010) is a near-optimal algorithm for single-agent MAB.
The agent acts in phases l = 1,2,…，and maintains a set of active arms Ai. Initially, Ai = [K]
(all arms). In phase l, each arm in Al is pulled for Θ(4l log T) times; arms with average reward 2-l
lower than the maximum are then eliminated from Al .
For each arm k ∈ [K], define its suboptimality gap to be ∆k := μ(1) - μ(k). In the elimination algo-
rithm, with high probability arm k will be eliminated after approximately lk = log2 ∆k-1 phases, in
which it is pulled for at most O (∆-2 log T) times. It follows that regret is O (Pk∙∆^>0 ∆k-1 log T ,
which is almost instance-optimal. The regret is O (√KT log T) in the worst case.
In order to apply elimination algorithm to our setting and remove the log T dependence in the
regret bound, we slightly modify the elimination algorithm: In phase l, each arm in Al is pulled for
Θ(4l lοg(4-lT)) instead of Θ(4l lοg(T)) times. By doing this, the regret bound can be improved
from KKTo log T to KKTo log K. Our DEMAB protocol is based on this modified elimination
algorithm. See Section 3.2 and Appendix C for detailed explanation.
3.2	The DEMAB Protocol
The DEMAB protocol executes in two stages. In the first stage, each agent directly runs the single-
agent modified elimination algorithm for D = dT/MKe time steps. The remaining arms of agent
i are denoted as A(i). In D time steps, an agent completes at least lο = blog41。。。长 Dg(MK) C
phases. The purpose of this separate burn-in period is to eliminate the worst arms quickly without
communication, so that in the second stage, elimination can begin with a small threshold of O(2-l0 ).
D and lo is chosen so that the total regret within the first stage is O (√MT).
2Our algorithms can be modified to meet this restriction with almost no change in performance.
4
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Protocol 1: Distributed Elimination for Multi-Armed Bandits (DEMAB)
D = dT/MKe, lo = blog4 (1000KDg(MK))c, mi = [4l+3log(MKT ∙ 4-l)C
/* Stage 1: Separate Burn-in	*/
for Agent i = 1,…，M do
|_ Agent i runs single-agent elimination for D time steps, denote remaining arms as A(i)
/* Switching: Random Allocation	*/
Generate public random numbers r1,...,rK uniformly distributed in [M]
Bl0+1 = {a ∈ A(i)|ra = i}, Blo + 1 = Ui∈[M] Blo + l
/* Stage 2: Distributed Elimination	*/
for l = lo + 1,…do
if Nl = |Bl | > M then
Agent i sends nl(i) = Bl(i) to server; server broadcasts nmax = maxi nl(i)
if (nl(1), ..., nl(M)) is not balanced then
|_ Reallocate // Adjust B(i)
so that their sizes are balanced
Agent i pulls each a ∈ B(i) for mi times, denotes average reward as ^ι (a), and then pulls
arms in round-robin for (nmax - nl(i))ml times before the next communication round
Communication round: Agent i sends max°∈Β(i) Ui to server and waits to receive
u； = maXα∈Bι Ui from server
Agent i eliminates bad arms: B(+)ι = {a ∈ B(i) : Ui (a) + 2-i ≥ u；}
else
For each arm in Bi, the server asks M/Ni agents to pull it miNi/M times
Server computes Ui (a), the average reward for mi pulls of arm a
Server eliminates bad arms: Bi+1 = {a ∈ Bi : Ui(a) + 2-i ≥ maxb∈Bι Ui(b)}
Between the two stages, the remaining arms are randomly allocated to agents. Public randomness is
used to allocate the remaining arms to save communication. Agents first generate (ri,…,rκ), K
uniformly random numbers in [M], from a public random number generator. Agent i then computes
B(i) = a ∈ A(i) |ra = i . By doing so, agent i keeps each arm in A(i) with probability 1/M,
and the resulting sets B⑴,…,B(M) are disjoint. Meanwhile, every arm in T\iEM] A(i) is kept in
Bi0+1 = i∈[M] B(i), so that the best arm remains in Bi0+1 with high probability3.
In the second stage, agents start to simulate a single-agent elimination algorithm starting from
phase l0 + 1. Initially, the arm set is Bi0+1. In phase l, each arm in Bi will be pulled for at least
mi = d4i+3 log(MKT ∙ 4i)[ times. Denote the average reward of arm a in phase l by Ui(a). If
Ui(a) < maxao∈b1 Ui(a0) — 2-i, it will be eliminated; the arm set after the elimination is Bi+i.
This elimination in the second stage is performed over M agents in two ways: In distributed mode
or in centralized mode. Let Ni = |Bi | be the number of remaining arms at the start of phase l. If
Ni is larger than M , the elimination is performed in distributed mode. That is, agent i keeps a set
of arms Bi(i), and pulls each arm in Bi(i) for mi times in phase l. Each agent only needs to send the
highest average reward to the server, who then computes and broadcasts maXα∈Bι Ui (a). Agent i
then eliminates low-rewarding arms from Bi(i) on its local copy.
When Ni ≤ M, the elimination is performed in centralized mode. That is, Bi will be kept and
updated by the server4. In phase l, the server assigns an arm in Bi to M/Ni agents, and asks each of
3Si∈[M] A(i) may not be a subset of Bl0+1, which is not a problem in the regret analysis.
4In the conversion from distributed mode to centralized mode, agents send their local copy to the server.
Since the remaining action sets of agents are disjoint, the communication cost is O(M).
5
Published as a conference paper at ICLR 2020
them to pull it mlNl/M times5. The server waits for the average rewards to be reported, and then
performs elimination on Bl .
One critical issue here is load balancing, especially in distributed mode. Suppose that nl(i) = |Bl(i) |,
(i)
nmax = maxi∈[M] nl . Then the length of phase l is determined by nmaxml. Agent i would need to
keep pulling arms for (nmax - nl(i))ml times until the start of the next communication round. This
will cause an arm to be pulled for much more than ml times in phase l, and can hurt the performance.
Therefore, it is vital that at the start of phase l, ~nl := nl(1) , ..., nl(M) is balanced6.
The subroutine Reallocate is designed to ensure this by reallocating arms when ~nl is not balanced.
First, the server announces the total number of arms; then, agents with more-than-average number of
arms donate surplus arms to the server; the server then distributes the donated arms to the other agents,
so that every agent has the same number of arms. However, calling Reallocate is communication-
expensive: it takes O (min{Nl , Nl0 - Nl }) communication cost, where l is the current phase and
l0 is the last phase where Reallocate is called. Fortunately, since Bl(i)+1	are generated
randomly, it is unlikely that one of them contain too many good arms or too many bad arms. By
exploiting shared randomness, we greatly reduce the expected communication cost needed for load
balancing.
Detailed descriptions of the single-agent elimination algorithm, the Reallocate subroutine, and
the DEMAB protocol are provided in Appendix B.
Access to a public random number generator, which is capable of generating and sharing random
numbers with all agents with negligible communication cost, is assumed in DEMAB. This is not a
strong assumption, since it is well known that a public random number generator can be replaced
by private random numbers with a little additional communication (Newman, 1991). In our case,
only O (M logT) additional bits of communication, or O(M) additional communication cost, are
required for all of our theoretical guarantees to hold. See Appendix C for detailed discussion.
3.3	Regret and Communication Efficiency of DEMAB
In this subsection, we show that the DEMAB protocol achieves near-optimal regret with efficient
communication, as captured by the following theorem.
Theorem 1.	The DEMAB protocol incurs O ( PMTK log(MK)) regret, O (M log MK) Commu-
nication cost with probability 1 - δ, and O (M log(M K)) communication cost in expectation.
The worst-case regret bound above can be improved to an instance-dependent near-optimal regret
bound by changing the choice of D and l0 to 0. In that case the communication cost is O(M log T),
which is a small increase. See Theorem 5 in appendix for detailed discussion.
We now give a sketch of the proof of Theorem 1.
Regret In the first stage, each agent runs a separate elimination algorithm for D timesteps, which
has regret O(PKD log(MK)). Total regret for all agents in this stage is O(M√KD log D)=
O(，MT Iog(MK)). After the first stage, each agent must have completed at least lo phases. Hence,
with high probability, before the second stage, Bl0+1 = Si∈[M] Bl(i)+1 contains the optimal arm and
only arms with suboptimality gap less than 2-l0+1.
In the second stage, if a ∈ Bl, it will be pulled at most 2ml times in phase l because of our load
balancing effort. Therefore, if arm k has suboptimality gap 0 < ∆k < 2-l0+1, it will be pulled for
O(∆-2) times. It follows that regret in the second stage is O (PMKT Iog(MK)), and that total
regret is O (PMKT log (MK)).
5The indivisible case is handled in Appendix B.
6By saying a vector of numbers is balanced, we mean the maximum is at most twice the minimum.
6
Published as a conference paper at ICLR 2020
Communication In the first stage and the random allocation of arms, no communication is needed.
The focus is therefore on the second stage.
During a phase, apart from the potential cost of calling Reallocate, communication cost is O (M).
The communication cost of calling Reallocate in phase l is at most O (min {Nl, Nl0 - Nl}),
where l0 is the last phase where Reallocate is called. Therefore, total cost for calling
Reallocate in one execution is at most O (Nl1 ), where l1 is the first phase in which Reallocate
is called. From the definition of ml and l0, we can see that there are at most L = O (log(M K))
phases in the second stage. Therefore in the worst case, communication cost is O (ML + K) since
Nl1 ≤ K.
However, in expectation, Nl1 is much smaller than K. That is to say, Reallocate is called for the
first time when Nl is much smaller that K with high probability. Since arms are randomly allocated
to agents during switching step, when the total number of remaining arms is large enough, ~nl would
be balanced with high probability. In fact, with probability 1 - δ, Nh = O (M log MK )∙ Setting
δ = 1/K, we can show that the expected communication complexity is O(M log MK).
3.4 Lower Bound
Intuitively, in order to avoid a Ω (M√Kt) scaling of regret, Θ(M) amount of communication cost
is necessary; otherwise, most of the agents can hardly do better than a single-agent algorithm. We
prove this intuition in the following theorem.
Theorem 2.	For any protocol with regret bound o(M√KT), the expected communication cost is at
least Ω(M).
The theorem is proved using a reduction from single-agent bandits to multi-agent bandits, i.e. a
mapping from protocols to single-agent algorithms. This theorem shows that, in order to achieve
near-optimal regret(O (√MKt) ), the communication cost is at least Ω(M). That is to say, the
communication cost of DEMAB protocol matches the lower bound except for a logarithmic factor.
4 Main Results for Linear Bandits
In this section, we first summarize the single-agent elimination algorithm for linear bandits (Lattimore
& Szepesvdri, 2019, chap. 22), and then present the Distributed Elimination for Linear Bandits
(DELB) protocol. DELB is designed for the case where the action set D is fixed. Our results for
linear bandits with time-varying action set is presented in Sec. 4.4. For convenience, we assume D is
a finite set, which is without loss of generality from a regret point of view7.
4.1	Elimination Algorithm for Single-agent Linear Bandit
The elimination algorithm for linear bandits (Lattimore & Szepesvdri, 2019) also iteratively eliminates
arms from the initial action set. In phase l, the algorithm maintains an active action set Al. It computes
a distribution ∏ι(∙) over Al and pulls arms according to ∏ι(∙). A total of n pulls is made in this phase
according to ∏l(∙). Then, linear regression is used to estimate the expected reward of each arm based
on these pulls. Arms with estimated rewards Θ(2-l) lower than the maximum are eliminated at the
end of the phase.
To ensure that arms with suboptimality gap Θ(2-l) are eliminated in phase l with high probability,
the estimation error in phase l needs to be smaller than 2-l. On the other hand, to minimize regret, the
number of pulls make in phase l needs to be as small as possible, especially when l is small. Suppose
that Vl(π) = Px∈A π(x)xx> and gl(π) = maxx∈Al x>Vl(π)-1x. By analyzing linear regression
(see, e.g., Lattimore & Szepesvdri (2019, chap. 21)), one can show that if each arm x ∈ Supp(πl) is
pulled ∣"π(x)gl (π)4l log (1)] times, the estimation error for any arm X ∈ Al is at most 2-l with high
probability. Thus, to minimize ml, one should find a distribution π(∙) that minimizes gl(∏), a task
known as G-optimal design (Pukelsheim, 2006). It is known that the optimal value for gl (π) is d, and
7When D is infinite, we can replace D with an -net of D, and only take actions in the -net. If < 1/T,
this will not affect the scaling of the regret. This is a feasible approach, but computationally inefficient.
7
Published as a conference paper at ICLR 2020
that there exists a minimizer ∏* such that the support set of ∏*, also called the core set, has cardinality
at most d(d +1)/2. Consequently, only Px∈supp(π.) d∏*(x)gι(∏*)4l log( δ)] ≤ O(4ld log(1)+ d2)
pulls are needed in phase l.
4.2	The DELB Protocol
In this protocol, we parallelize the data collection part of each phase by sending instructions to agents
in a communication-efficient way. In phase l, the server and all agents first locally solve the same
G-optimal design problem on Al, the remaining set of actions. In our case, only a 2-approximation
to the optimal design is needed. That is, We only need to find ∏ι(∙) with g(∏) ≤ 2d. By using the
Frank-Wolfe algorithm under appropriate initialization (Todd, 2016, Proposition 3.17), we can find
such a 2-approximate solution with a support smaller than ξ = 48d log log d.
After that, the server assigns arms in Al to agents. Since the server and the agents obtain the same
core set when solving the approximate G-optimal design8, the server only needs to send the index
among ξ arms to identify and allocate each arm. After pulling arms, agents send the average reward
for each arm to the server, who computes the least squares estimator. The agents and the server then
eliminate low rewarding arms from their local copies of Al .
For notational convenience, we define V (π) = Px∈D π(x)xx>, g(π) = maxx∈D x>V (π)-1x.
1
2
3
4
5
6
7
8
9
Protocol 2: Distributed Elimination for Linear Bandits (DELB)
Ai = D, Ci = 600
for l = 1, 2, 3, ... do
/* All agents and server: Solve a G-optimal design problem	*/
Find distribution ∏ι(∙) over Al such that: 1. its support has size at most ξ = 48d log log d; 2.
g(π) ≤ 2d
/* Server: Assign pulls and summarize results	*/
Assign ml(x) = dCi4ld2πl(x) ln MTe pulls for each arm x ∈ Supp(πl) and wait for results9 10.
Receive rewards for each arm x ∈ Al reported by agents
For each arm in the support of ∏l (∙), calculate the average reward μ(x)
COmPUte10χ = pχ∈Supp(∏ι) ml(X)μ(X)x, Vl = pχ∈Supp(∏ι) ml(X)Xx>, θ = XTX
Send θ to all agents
/* All agents and server: Eliminate low-rewarding arms	*/
Eliminate low rewarding arms: Al+i = X x ∈ Al : maxb∈4 hθ, b 一 x〉≤ 2-l+1 〉
4.3	Regret and Communication Efficiency of DELB
We state our results for the elimination-based protocol for distributed linear bandits. The full proof is
given in Appendix G.
Theorem 3.	The DELB protocol achieves expected regret O (d√TM log T) with communication
cost O ((Md + dloglogd) logT).
Proof sketch: In round l, the number of pulls is at most 48d log log d + Ci4ld2 log MT. Based on
the analysis for elimination-based algorithm, we can show that the suboptimality gap (θ*,χ* 一 Xi is
at most 2-l+2 with probability 1 一 1/MT for any arm X pulled in phase l. Suppose there are at most
L phases, we can prove that E(REG(T)) ≤ PL=I O(d,4ld2 log2 TM) ≤ O(d√TM logTM).
In each phase, communication cost comes from three parts: assigning arms to agents, receiving
average rewards of each arm and sending θ to agents. In the first and second part, each arm
X ∈ Supp(πl) is designated to as few agents as possible. We can show that the communication cost
8The Frank-Wolfe algorithm and its initialization are deterministic.
9We assign the pulls of each arm to as few agents as possible. See Appendix F for detailed description.
10Vl is always invertible when Al spans Rd. When Al doesn’t span Rd, we can always consider span(Al) in
phase l and reduce the number of dimensions.
8
Published as a conference paper at ICLR 2020
of these parts is O(M + dlog log d). In the third part, the cost of sending θ is Md. Since l is at most
O (log T), the total communication is O ((Md + d log log d) log T).	□
4.4 Protocol for Linear Bandits with Time-varying Action Set
In some previous work on linear bandits (Chu et al., 2011; Abbasi-Yadkori et al., 2011), the action set
available at timestep t may depend on t. That is, players can only choose actions from Dt at time t,
while regret is defined against the optimal action in Dt . The DELB protocol does not apply in this
scenario. To handle this setting, we propose a different protocol DisLinUCB (Distributed LinUCB)
based on LinUCB (Abbasi-Yadkori et al., 2011).
In our distributed algorithm, agent i uses all samples available for him to maintain a confi-
dence set Ct,i ⊆ Rd for the parameter θ*. In each step, he chooses an optimistic estimate
θet,i = argmaxθ∈Ct-1,i (maxx∈Dhx, θi) and then chooses action Xt,i = argmaxx∈D x, θet,i ,
which maximizes the reward according to the estimate θt,i . We denote τ xτ xτT and τ xτyτ as
W and U in our algorithm respectively. We use Wt,i and Ut,i to denote the sum calculated using
available samples for agent i at time step t. We construct the confidence set Ct,i using Wt,i and Ut,i :
I , ʌ	∕det (V ti)1∕2 det(λI )-1/2\	1,9	I
Ct,i =	∣	θ ∈ Rd	: kθt,i	- θkvt,i	≤ t 2log (—〈 M δ — +	+ λ1/2	I	,	(1)
where Vt,i = λI + Wt,i,3=(λI + Wt,i)-1 Ut,i, kxkV := xTVx. Essentially, without synchro-
nization, agents in our protocol execute LinUCB separately. In a synchronization round, agents share
all newly acquired samples with each other. The remaining question is when to synchronize.
Our key observation, whichcomes from the analysis in Abbasi-Yadkori et al. (2011), is that the change
in the log-determinant of Vt is a good indicator of learning progress. Based on this observation, we
only synchronize when agent i finds that the log-determinant of Vt,i has changed significantly since
the last synchronization. We designed this synchronization criterion carefully such that, on the one
hand, regret is close to the lower bound and on the other hand, the number of synchronization rounds
is small. The full protocol is described below.
1
2
3
4
5
6
7
8
9
10
Protocol 3: Distributed Linear UCB (DisLinUCB)
for t = 1,	∙ ,T do
for Agent i = 1,…，M do
Compute Wt,i = Pτ xτ xτT and Vt,i = I + Pτ xτyτ using all available samples
Construct the confidence ellipsoid Ct,i using Wt,i and θt,i = Vt-IUt,i.
(xt,i, θt,i) = arg max(x,θ)∈Dt ×Ct,i hx, θi
Play xt,i and observe reward yt,i .
Update Wt,i and Vt,i
if log (det Vt,i∕det Mast) ∙ (t - tiast) > T TOg(MT)∕(dM) then
Start a synchronization round
[tiast = t, Viast = I + PT XτJτ using all available examples
We state the following regret and communication bound for this protocol. The proof is deferred to
Appendix H.
Theorem 4.	The DisLinUCB protocol can achieve a regret of O (d√MT log2(T)) with
O M1.5d3 communication cost.
Although the regret bound is still near-optimal, the communication cost has worse dependencies on
M and d compared to that of DELB.
9
Published as a conference paper at ICLR 2020
5	Conclusions and future work
In this work, we propose communication-efficient protocols with near-optimal regret for both multi-
armed bandits and linear bandits problem. For multi-armed bandits, We propose DEMAB protocol,
which achieves JMKT log(MK) regret with M log(MK) communication cost. The communica-
tion cost of DEMAB protocol matches the loWer bound We proposed except for a logarithmic factor.
For distributed linear bandits, our DELB protocol and DisLinUCB protocol achieve near-optimal
regret with O ((Md + d log log d) log T) and O M1.5d3 communication cost respectively.
An interesting open question is proving lower bounds of communication cost in order to achieve near-
optimal regret in linear bandits. From a reduction from multi-armed bandits to linear bandits, Thm. 2
implies Ω(M) communication lower bound for linear bandits. However, there is still a gap between
this bound and the communication cost of DELB protocol. We conjecture that close-to-M d amount
of communication is necessary, which is the case in offline distributed linear regression (Zhang et al.,
2013; Braverman et al., 2016) in the context of achieving an optimal risk rate.
6	acknowledgements
The authors thank Chi Jin, Chicheng Zhang and Nan Jiang for helpful discussions. This work is
supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502),
NSFC (61573026), BJNSF (L172037) and Beijing Acedemy of Artificial Intelligence.
References
Yasin Abbasi-Yadkori, Dgvid Pdl, and Csaba SzepesvM. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.
Naoki Abe, Alan W Biermann, and Philip M Long. Reinforcement learning with immediate rewards
and linear hypotheses. Algorithmica, 37(4):263-293, 2003.
Deepak Agarwal, Bee-Chung Chen, Pradheep Elango, Nitin Motgi, Seung-Taek Park, Raghu Ra-
makrishnan, Scott Roy, and Joe Zachariah. Online models for content optimization. In Advances
in Neural Information Processing Systems, pp. 17-24, 2009.
Peter Auer and Ronald Ortner. Ucb revisited: Improved regret bounds for the stochastic multi-armed
bandit problem. Periodica Mathematica Hungarica, 61(1-2):55-65, 2010.
Ilai Bistritz and Amir Leshem. Distributed multi-player bandits-a game of thrones approach. In
Advances in Neural Information Processing Systems, pp. 7222-7232, 2018.
Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodruff. Communication
lower bounds for statistical estimation problems via a distributed data processing inequality. In
Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pp. 1011-1020.
ACM, 2016.
SebaStien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and TrendsR in Machine Learning, 5(1):1-122, 2012.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics, pp. 208-214, 2011.
Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit
feedback. In 21st Annual Conference on Learning Theory - COLT 2008, Helsinki, Finland, July
9-12, 2008, pp. 355-366, 2008. URL http://colt2008.cs.helsinki.fi/papers/
80-Dani.pdf.
Eshcar Hillel, Zohar S Karnin, Tomer Koren, Ronny Lempel, and Oren Somekh. Distributed
exploration in multi-armed bandits. In Advances in Neural Information Processing Systems, pp.
854-862, 2013.
10
Published as a conference paper at ICLR 2020
Dileep Kalathil, Naumaan Nayyar, and Rahul Jain. Decentralized learning for multiplayer multiarmed
bandits. IEEE Transactions on Information Theory, 60(4):2331-2345, 2014.
Nathan Korda, BalgzS SzOrCnyi, and Li Shuai. Distributed clustering of linear bandits in peer to
peer networks. In Journal of machine learning research workshop and conference proceedings,
volume 48, pp. 1301-1309. International Machine Learning Societ, 2016.
Tze Leung Lai et al. Adaptive treatment allocation and the multi-armed bandit problem. The Annals
of Statistics, 15(3):1091-1114, 1987.
Tor Lattimore and Csaba Szepesvgri. Bandit algorithms. preprint, 2019.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on
World wide web, pp. 661-670. ACM, 2010.
Oded Maron and Andrew W Moore. Hoeffding races: Accelerating model selection search for
classification and function approximation. In Advances in neural information processing systems,
pp. 59-66, 1994.
Ilan Newman. Private vs. common random bits in communication complexity. Information processing
letters, 39(2):67-71, 1991.
Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.
Jonathan Rosenski, Ohad Shamir, and Liran Szlak. Multi-player bandits-a musical chairs approach.
In International Conference on Machine Learning, pp. 155-163, 2016.
BalgzS SzorCnyi, R6bert BuSa-Fekete, Istvdn HegedUs, R6bert Ormgndi, Mdrk Jelasity, and BalgzS
KCgl. Gossip-based distributed stochastic bandit algorithms. In Journal of Machine Learning
Research Workshop and Conference Proceedings, volume 2, pp. 1056-1064. International Machine
Learning Societ, 2013.
Chao Tao, Qin Zhang, and Yuan Zhou. Collaborative learning with limited interaction: Tight bounds
for distributed exploration in multi-armed bandits. arXiv preprint:1904.03293, 2019.
Michael J Todd. Minimum-volume ellipsoids: Theory and algorithms, volume 23. SIAM, 2016.
You-Gan Wang. Sequential allocation in clinical trials. Communications in Statistics-Theory and
Methods, 20(3):791-805, 1991.
Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright. Information-theoretic
lower bounds for distributed statistical estimation with communication constraints. In Advances in
Neural Information Processing Systems, pp. 2328-2336, 2013.
11
Published as a conference paper at ICLR 2020
A Organization
The appendix is organized as follows.
Sections B-E are mainly about distributed multi-armed bandits.
In Section B, we present omitted details for the DEMAB protocol. In Section C, we analyze the
performance of DEMAB and prove Theorem 1. We also analyze the number of communicated bits in
DEMAB in this section. In Section D, we discuss how to remove the usage of public randomness in
the DEMAB protocol with little increase in communication cost. In Section E, we give a proof of the
communication lower bound for multi-armed bandit.
Sections F-H are mainly about distributed linear bandits.
In Section F, we present the omitted details of DELB protocol. In Section G, we prove the regret and
communication cost bounds of DELB protocol, and also analyze the number of communicated bits in
DELB. In Section H, we analyze the performance of the DisLinUCB protocol.
Finally, in Section I, we demonstrate that our protocols can be adapted to the P2P communication
networks considered in Korda et al. (2016), which proves our claim in footnote 2 of the main paper.
12
Published as a conference paper at ICLR 2020
B Detailed Description of DEMAB
In this section, we give a detailed description of the DEMAB protocol and some subroutines used in
the protocol.
Protocol 4: Distributed Elimination for Multi-armed Bandits (DEMAB)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
1
2
3
4
5
6
D = dT/MKe, C2 = 1000,10 = blog4 (c2k 1D(MK))C, mi =d41+3 log(MKT ∙ 4-l)]
/* Stage 1: Separate Burn-in
for agent i = 1, ..., M do
|_ A(i) =Eliminate([K],D)
/* Switching: Random Allocation
Generate public random numbers r1, ..., rK in [M]
B(0+ι = {a ∈A(i)∣ra =i}
/* Stage 2: Distributed Elimination
for l = l0 + 1, ... do
if Centralize has not been called then
/* distributed mode
Agents send nl(i) = Bl(i) to server, Nl = Pi nl(i), Nmax = maxi nl(i)
if Nl ≤ M then
L Centralize, go to line 22
if ~nl is not balanced then
L Reallocate
Server sends Nmax to all agents
for agent i = 1, ..., M do
PUll each a ∈ B(i) for mi times, denote average as %(•)
Pull other arms in round-robin for (Nmax - |Bl |)ml times
Send (argmaxaθ Ul(a0), max。，U1(a0)) to server
Server receives (a*1,u*1) from agent j, and sends u； = maxj Ujl
for agent i = 1, ..., M do
[Elimination: B(+] = {i ∈ B(i) : Ul (a) + 2-l ≥ uj}
to every agent
else
/* centralized mode
Server assigns arms in Bl to agents evenly and schedUles ml pUlls for each arm
Agents pUll arms as reqUired by the server, and report the average for the pUlled arm
Server calculates Ul(a), average reward for ml pulls in this phase, for each arm a ∈ Bl
Elimination: Bl+1 = {a ∈ Bl : Ul(a) + 2-l ≥ maXj∈Bι Ul(j)}
*/
*/
*/
*/
*/
Eliminate: Eliminate executes the single-agent elimination algorithm. In this function, each
agent runs the single-agent elimination algorithm for D time steps, then return the remaining arms.
Protocol 5: Eliminate_________________________________________________________________
Input: A set of arms Ai, time step D.
for l = 1, ... do
for a ∈ Al do
Pull arm a for ml = d4l+3 log(4-lMKT)] times, denote average reward as μl(a)
If time step D is reached, go to line 6
Elimination: Al+i = {a ∈ Al : μl (a) > maXk∈Aι μl (k) 一 2-l}
Return Al
Reallocate: In Reallocate, the server announces the average number of arms; agents with more-
than-average arms then donate surplus arms to the server; the server then distributes the donated
arms to the other agents, so that every agent has nearly the same number of arms. After calling
13
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
1
2
3
4
5
6
1
2
3
Reallocate, ~nl becomes balanced again. The function contains the following two parts: One
running on the server, and the other running on each agent.
Protocol 6: Reallocate for Server_________________________________________________________
Input: n(1),...,n(M)
n= bPMι n(i)∕MC
Send “reallocation"，n to every player
Receive a set of n(i) - n arms, Ai, from player i if n(i) > n; Atemp = Ui Ai
for i = 1, ..., M do
|_ If n(i) < n, send n(i) - n arms in Atemp to player i, and remove them from Atemp
Send the remaining arms in Atemp to players 1,...,|Atemp| (one each)
Protocol 7: Reallocate for Agents
if Receive “reallocation”, n then
if n(i) > n then
Pick a subset of n(i) - n arms, Ai, from B(i), and send them to server
B(i) = B(i)∖Ai
else
|_ Wait until receiving Ai from server, B(i) = Ba) ∪ Ai
Centralize: When the number of arms drops below M, the subroutine Centralize is called,
in which agents send their local copy of remaining arms, Bl(i), to the server, and server receives
Bl = Ui∈[M] Bl .
Protocol8: Centralize___________________________________________________________________
for agent i = 1, ..., M do
Send B(i) to server
Server receives Bl(i) from agent i, and calculates Bl = Ui Bl(i)
Assignment Strategy: In centralized mode, server assigns arms to agents in the following way.
Let Nl = |Bl |. If M is exactly divisible by Nl, for each arm in Bl, server asks M∕Nl separate
agents to play it for dmlNl∕Me times. If not, we allocate pulls to agents in the following way: Let
pl = dmlNl∕Me denote the average pulls each agent needs to perform. Our assignment starts from
the arm with the smallest index ak1 and agent 1. For arm akj and agent i, if agent i has been assigned
pl pulls, we turn to agent i + 1. If we have finished allocating ml pulls for arm akj, we continue
designating arm akj+1 . The assignment is finished until all pulls are scheduled to agents.
C Proof of Theorem 1
In this section, we give a full proof of Theorem 1, which bounds the total regret and communication
cost of the DEMAB protocol. In the analysis below, We will use Bi to represent B(I) ∪∙∙∙∪ B(M)
(in the distributed mode) or Bl (in the centralized mode). It refers to the set of remaining arms at the
start of the l-th phase at stage 2, either held separately by the agents or held by the server.
Suppose that the protocol terminates when l = l0. We also let NM (a, t) = Ptj =1 PiM=1 I [ai,j = a]
be the number of times arm a is pulled before time step t. Without loss of generality, we assume that
arm 1 is the best arm, and define ∆k := μ(1) - μ(k).
We first state a few facts and lemmas.
Fact 1. We state some facts regarding the execution of the algorithm.
1.	At line 6 of the server’s part in Reallocate, |Atemp| < M;
14
Published as a conference paper at ICLR 2020
2.	After Reallocate is called, ^∣B(1) ∣ , ∙∙∙ , ∣B(M )|)is balanced;
3.	For any player i, the number of completed phases in stage 1 is at least l0 =
[log4 (c2k⅛k)∖,nddαmoosl ID = 口叫"DMK)].
4.	The number of phases at stage 2 is less than L = 4 + 5 log(MK) = O (log(M K)).
Proof. 1. Let S+ = {i : n(i) ≥ n} and S- = {i : n(i) < n}. At line 3 of server's reallocate
code, server receives Pi∈s+ (n(i) - n) arms. At line 5, Pi∈s- (n - n(i)) arms are removed. So at
line 6,
IAtempI = X (n(i)- n) - X (n-n(I(八=X n(i) - Mb X n(i)/Mc < M.
i∈S+	i∈S-	i∈[M]	i∈[M]
2.	Let nl(i) = Bl(i) . If maxi nl(i) ≤ 2 mini nl(i), the reallocation procedure will do nothing,
and hnl(1), ..., nl(M)i is by definition balanced. If maxi nl(i) > 2 mini nl(i), then at the end of the
reallocation procedure, every player has a new set of arms B(i) such that n ≤ ∣B(i)∣ ≤ n + 1. This
implies that the number of arms is balanced, since when reallocation is called, Pi nl(i) ≥ M.
3.	The length of the l-th phase at stage 1 is at most Kd4l+3 log(4-l M K T)e. After l phases, the
number of timesteps is at most
l
K X 4p+3 log(4-pMKT)
p=1
MKT) + 1
,+ 3
Setting C2 = 1000 and lo = blog4 (CKDMKJC, one can verify that
4l0+4	1	256D	1
-Ir (Iog(L0MKT)	+ 3)≤ 3C2 log(MK)	(Iog(C2MMK3log(MK)	+ 3))	≤ D.
Since the total number of timesteps in stage 1 is D, we know that the number of completed phases in
stage 1 is at least l0.
On the other hand, in l phases, the number of timesteps is at least
l
X -p+3 log(--pMKT) ≥ -l+3 log(--lMKT).
p=1
By setting C20 = 512 and lD
log4
D
C2 log MK
one can show that lD +3 log(-lDMKT) >
D. Therefore, the number of of completed phases in stage 1 is at most lD.
4.	Suppose that stage 2 has L = -+ 5 log(M K) phases, then the total number of phases during
stage 1 and stage 2 is at least L + lo. In phase L + lo, at least 4l0+L+3 log (4-(l0 +L)MKT) pulls
are made. With some elementary calculation, one can show that the number of arms pulled in phase
L + lo is MT ∙ ioMMκ)log log(MK), which is even greater than the total number of timesteps MT.
That is to say, the number of phases in stage 2 is less than L.	□
Via a direct application of Hoeffding’s inequality and union bound, we have the following lemma.
Lemma C.1. Denote the average rewards computed by agent i in phase l of stage 1 be μi,ι(∙). At
stage 1, we denote the following even as Λl: in phase l at stage 1, for any agent i, for any arm
a ∈ Al(i) ,
I^i,ι(a) - μ(a)∣≤ 2-l-1.
Then Pr[Λι] ≥ 1 - 2 ∙-/(MKT).
15
Published as a conference paper at ICLR 2020
Proof. For any agent i, any phase l ≤ Id, denote the empirical mean for arm a in phase l by Ui,ι (a).
By a direct application of Hoeffding’s bound and union bound, we can observe that for any fixed
i, l, a, by Hoeffding’s bound,
Pr [lμi,ι(a) -μ(O)I > 2-l-1] ≤ 2eχp-- 1 mi ∙4τ-1] ≤ ∕MK42τ.
2	(MK ) T
A union bound for all arms a ∈ A(i) and all agents i ∈ [M] proves the lemma.	□
Lemma C.2. For phase l in stage 1, Λι ∧∙∙∙∧ Λι implies the following:
1.	1 ∈ Al(+i)1;
2.	∀i ∈ [M], ∀a ∈ A(+ι, μ(a) ≥ μ(1) - 2-l+1;
3.	Recall that NM (k, t) = Ptj=1 PiM=1 I [ai,j = k] is the number of times arm k is pulled
for all agents before time step t. If li = dlog2 ∆i-1e + 1 ≤ l, then NM (i, D) ≤
C5M log(MKT ∆2)∕∆2;
In particular, let Id be the maximum number ofphasesfor all agents at stage 1. Λι ∧∙∙∙∧ Λ5 also
implies the following:
4.	∃i∈ [M],1∈Bl(0i)+1;
5.	∀i ∈ [M], ∀a ∈ B(i+1, μ(a) ≥ μ(1) - 2-lo + 1;
Proof. These results are direct implications of lemma C.1.
1.	Notice that if Λl happens,
μi,i(1) ≥ μ(1) - 2-l-1 ≥ μ(a) - 2-l-1 ≥ μi,i(a) — 27.
Thus, arm 1 will never be eliminated throughout the first l phases.
2.	Λl implies that for any a ∈ Al(+i)1,
μi,i(a) ≥ max μi,i(k) - 2-l ≥ μi,i(1) - 2-l,
k∈Al(i)
which means
μ(a) ≥ μi,i(a) - 2-l-1 ≥ μi,i(1) - 2-l - 2-l-1 ≥ μ(1) - 2-l+1.
3.	From statement 2 We know that Λι ∧ …∧ Λι implies that arm i will be eliminated after Ii phases,
which is less than l. Therefore, before eliminated, the total number of times arm i is pulled at stage 1
by any agent is at most
X mi ≤ 4l34 (log(4-iiMKT) + 1 )≤ C5 Iog(M2KT冷
for some absolute constant C5 . Multiplying by M proves the assertion.
4.	Let A(i) denote the remaining arms at the end of stage 1 for agent i. From statement 1 we know
that Λι ∧ …∧ Λ1d implies 1 ∈ A(i) for any i ∈ [M]. So at line 5 in protocol 1, arm 1 will be assign
to agent ra .
5.	Let A(i) denote the remaining arms at the end of stage 1 for agent i. From statement 2 we know
that Λι ∧∙∙∙∧ Λ1d implies μ(a) ≥ μ(1) - 2-l(i)+1 for any arm in Ui∈[M] A(i). Here l(i) denotes
the number of phases for agent i at stage 1. Since Bi0+1 = Si∈[M] Bi(i)+1 is a subset of Si∈[M] A(i)
and li ≥ lo, we conclude that ∀a ∈ Bιo+ι, μ(a) ≥ μ(1) - 2-l(i) + 1 ≥ μ(1) - 2-lo+1.
□
16
Published as a conference paper at ICLR 2020
Lemma C.3. For l > lo, denote thefollowing event by Ωι: at stage 2,for any a ∈ Bl,
∣ul(a) - μ(a)∣ ≤ 2-l-1.
Then Pr[Ωl] ≥ 1 - 2 ∙ 4l∕(M2KT).
Proof. Recall that for any l > l0, Ul (a) is the average of ml independent samples of the reward from
arm a. Therefore, for fixed l and a, by Hoeffding’s inequality,
_ ................. -	1 1	, ,1	2 ∙ 4l
Prljul⑷-μ(O)I > 2 l 1] ≤ 2eχp--2ml ∙ 4 j ≤ (MK)2T.
A union bound for all a ∈ Bl proves this lemma.
□
Lemma C.4. Recall that ∆% = μ(1) — μ(i). If ∆% > 0, let Ii =dlog2 ∆-1] + 1. Let Λ denote
Λι ∧∙∙∙∧ ΛlD. Then at stage 2, Λ ∧ Ωl0 ∧ …∧ Ωli implies thefollowing:
NM (i, T) - NM (i, D) ≤
C4 log(MKT∆2)
∆	+ η(a)
where C4 is a universal constant, and	a∈[K] η(a) ≤ M log M.
Proof. Suppose that |Bl| > M. By Fact 1, at phase l ≥ l0 + 1 in stage 2, the sequence
^∣B(1)∣, ∙∙∙ , ∣B(MI〉is balanced. Therefore, during phase l at stage 2, the number of times
an arm in Bl is pulled is at most 2ml.
If |Bl ∣ ≤ M, an arm in Bl is also pulled at most 2ml times, unless |Bl ∣ ∙ ml < M. In that case, a phase
only lasts for 1 timestep. This is possible only when ml < M, which requires l < blog4 Mc < log M.
We denote the number of times a is pulled in such phases by η(a).
On the other hand, let li =「log2 ±] + 1. Ifli ≤ lo, Λ? implies that i / Bl0+ι. In that case, the
number of times arm i is pulled after timestep D is 0. Assume Ωl0+ι ∧…∧ Ωli holds and i / Bli.
Then
uli ≥ μ(1)- 2-li-1 ≥ μ(i)+∆i- 2-li-1 ≥ Gli (i)+2-li.
Therefore, Ωl0+ι ∧ … ∧ Ωli implies that i / Bli+ι. In this case, number of times arm i is pulled is
li+1
NM(i,T) -NM(i,D) ≤ X 2ml + η(a)
l=l0+1
81
≤ 34li +4 log(MKT ∙ 4-li-l) + 3 + L + η(a)
215 log(MKT∆i2)
≤ ---3∆2----- +4 + 5log(MK) + η(a)
(215 +27)Iog(MKT∆2)
≤-------E-------+ n(a).
It is not hard to see that	a∈[K] η(a) ≤ Mlog M.
□
Lemma C.5. Let l ≥ lo + 1, and nl = ∣Bl ∣. With probability 1 - 2LKδ, either nl < 21M log 1 or
no reallocation is performed before the start of the l-th phase at stage 2.
Proof. Let Yi,l = I [i / Bl], Xi,j = I [ri = j], i / [K],j / [M]. Observe that Xi,j and Yio,l
are independent. This is because the elimination process between Bl and Bl+1 uses exactly ml
17
Published as a conference paper at ICLR 2020
independent samples for each arm; therefore, the conditional probability for remaining has no
dependence on which player an arm is assigned to. Let Y~ denote {Yi,l , ∀i, l}. Since
IB(I= XYi,ιXi,j,同=nι = XYi,ι,
i=1	i=1
by Chernoff’s inequality,
Pr〕X K,ιXi,j > SM? X YM M ≤ exΡ 卜 Ρ2⅛4
i=1	i=1
pr"Xκ,ιXi,j< 3⅛ X% Y# ≤ exp (-PKMri).
i=1	i=1
Consequently,
Pr
4网
3M
nι > 21M log ɪ
δ
≥1 - EY 卜XP (-2nM)+ exp (-焉)I nl > 21m log δ
≥1 - 2δ.
Note that 瘾网 < ∣B(j)∣ < 熹 ∣Bι∣ for all j ∈ [K] implies that (∣B(1)∣,…，∣B(M)IE is
balanced. Therefore, if n > 21M log 1, with probability 1 - 2(l - lo)Kδ, no reallocation will be
performed before the l-th phase at stage 2.	□
Now we are ready to prove Theorem 1.
Theorem 1. The DEMAB protocol incurs O (PMTK log(MK))) regret, O (M log MδK) Com-
munication cost with probability 1 - δ, and O (M log(M K)) communication cost in expectation.
Proof. Regret: Let li =dlog2 ∆-1] + 1. By Lemma C.2, when Λι ∧ …∧ Λ^ holds,
NM (i, D) ≤
C5M Iog(MKT ∆2)
∆
Therefore, the probability that this does not hold is at most
li	li
X Pr [ΛP] ≤ X
p=1	p=1
2 ∙ 4p /	128
MKT ≤ 3MKT∆
Thus for any arm i,
C5Mlog(MKT∆i2)	128
e [Nm(i, D)] ≤	∆2	+ IMKTZ2 ∙ md
C5M Iog(MKT ∆2)	64
≤	Hi	+3∆2.
Similarly, if l ≥ lo, when Λ ∧ Ωι0+ι ∧∙∙∙∧ Ωii holds,
NM (i, T) - NM (i, D) ≤
C4 log(MKT ∆2)
-----∆-------+η(i)
The probability that this does not hold is at most
li+1	lD
Pr 四 + X PrM ≤ X
p=l0	p=1
2 ∙ 4p
MKT
li
+X
p=l0
2 ∙ 4p
M2KT
256 C2 + 128
≤ -MT ∆~
,	600
≤ MT∆ .
18
Published as a conference paper at ICLR 2020
Therefore
E [Nm(i, T) — NM(i, D)] ≤ C4 '°g(MKT^ + η(i) + M^ ∙ MT
C4 log(MKT∆i2) 600
=------T-------+ ∆2 + η(i).
It follows that
K
X ∆aE [NM (a, D)] = X ∆aE[NM(a,D)]+ X ∆aE[NM (a, D)]
a=1	a = ∆a>eι	a : ∆a≤eι
≤ KCSM 1。以〃衣叫)+ 256/3 + 1.MD
1
≤ CgPmtlog(MK),
where in the last line We choose 印=KK log(κ), and Cq is some absolute constant. Similarly,
K
X ∆aE [Nm(a, T) - Nm(a, D)] ≤ X TaE [Nm(a, T) - Nm(a, D)] + ◎ ∙ MT
a=1	a = ∆a>e2
≤ Mlog M + KC4 l°g(MKTei)+C9 + e2 ∙ mt
∈2
≤ M log M + (3C4 + 601) PMKT log(K),
where in the last line we chose eι = KK MTK), By our assumption, M log M ≤ PMTK log(K).
Therefore by the definition of total regret,
KK
E [REG(T)] =X∆aE[NM(a,D)]+X∆a(E[NM(a,T)] -E[NM(a,D)])
a=1	a=1
≤ C6PMTlog(MK) + (3C4 + 601 + 1)PMKTlog(K)
≤ √2 ∙ max{C6, 3C4 + 602} PMT(K log(K) + log(MK)).
Communication: Total communication in stage 1 is 0. We first consider the worst case communica-
tion cost in stage 2. Note that during a phase (either in distributed mode or in centralized mode), the
communication cost is O(M) if reallocation is not performed. The cost for reallocation at the start of
phase l is O (min{nl, nl0 - nl}), where l0 is the last phase where reallocation is performed. Summing
this over all phases (at most L = O(log(M K))) in stage 2, we conclude that total communication
cost for reallocation is O(ML + K).
Define l1 to be the first phase such that reallocation is performed. Then, our argument above shows
that communication cost is O(ML + nιj. If niι > 21Mlog 1, the event in lemma C.5 will be
violated for some l. The probability for that is at most 2L2Kδ. By resetting δ, we can show that with
probability 1 一 δ, n,、< 21M log (2L2κ), Therefore, with probability 1 一 δ, total communication
cost is
O (ML + 21Mlog (2LK) = = O (Mlog
In particular, by choosing δ = 1/K, we can show that expected communication cost is
O (Mlog(MK)).	口
Theorem 5.	When D = l0 = 0 in DEMAB, the protocol incurs near-optimal instance-dependent
regret O(Pk∆>0 ∆-1 log T + M log M). With probability 1 — δ the communication cost is
O (M log(T∕δ)). The expected communication cost is O (M log T).
19
Published as a conference paper at ICLR 2020
Proof. Regret: In the case D = 0, it can be seen from the proof of Theorem 1 that
雨 WLErC4 log(MKT∆2L 「—600
E [NM(i,T)] ≤	∆	+ η(i) + ∆2,,
where Pi∈[K] η(i) ≤ M log(M). It follows that total expected regret is
C4 log(MKT∆2) + 600
E E NM(i,T)∆i ≤ E ∆iη(i) + E -ɪ-g(_X i)+—
-i=∆i>0	」	i=∆i>0	i=∆i>0	i
=O Ml-g(M) + X ∆k-1 l-g T .
∖	k = ∆k>0	)
Communication: By the proof of Theorem 1, the worst-case communication cost of this protocol
is O (M l-gT + K). This is because we need O(M) communication at the end of each phase to
perform elimination, and at most a total number of O(K) additional communication among all phases
to perform reallocation.
Let l* be the last complete phase such that nι* + ι > 21M l-g(KL0∕δ). By lemma C.5, no reallocation
is needed before phase l* + 1 with probability 1 - δ, so the total communication before phase l + 1
is O(M L0).
From the beginning of phase l* + 1, the total communication in the following phases is at most
O(ML + M l-g(KL0∕δ)).
Therefore, with probability 1 一 δ, the communication cost is O(MlogT + Ml-g(KL0∕δ)) =
O (M l-g(τ∕δ)). Let δ = 1/K, the expected communication is O (M log T + (ML0 + K)/K) =
O (M logT).
□
Finite precision: We now show that only O(logT) bits are needed for each number sent in
DEMAB. The integers sent in DEMAB are numbers in {0,…,K}. Therefore O(log T) bits are
sufficient for each integer11. In the DEMAB protocol, the only real numbers that are transmitted are
the average reward in a phase. In our proof, it is only required that the average of ml samples is
41+2 ιog(MKT4-i)-SUbgaUSSian.In fact, the average of mi samples is 4+ ]og(MKT4-1)-subgaussian.
Therefore, we can use randomized rounding for the average reward with precision E = 4TM. Then,
the rounding error for each real number has zero mean, and is M41+3 iog11Mκτ4-1)-subgaussian. When
computing the average of mi samples at phase l, at most M rounding error terms will contribute to
it, whose sum is 41+3 i0g(MKT4-1)-subgaussian. Therefore, the concentration inequality in lemma
C.3 and consequently our main theorem still holds. Apparently log2 ɪ = O QOg(MT)) = O(log T).
Therefore, expected number of communicated bits is O (M log(MK) log T).
D	Removing Public Randomness
The DEMAB protocol makes use of a public random number generator. It can be viewed as a
sequence of uniformly random bits written on a public blackboard that every agent can read, and
reading from this sequence does not require communication. In practice, this can be approximated
by using a pseudorandom number generator with a truly random seed. In this case, regardless of
the amount of public random number used, the communication cost is O(M), which is the cost of
broadcasting a short random seed.
However, we can also totally remove the usage of public random numbers. The role of shared
randomness in communication complexity has already been investigated. It is known that shared
randomness can be efficiently replaced by private randomness and additional communication, as
stated by the Newman’s Theorem Newman (1991). In our case, the argument is slightly different:
we are considering an online learning task instead of function evaluation. Also, in DEMAB, the
communication cost itself depends on the public random bits. In particular, we show the following
theorem.
11Recall that T > K.
20
Published as a conference paper at ICLR 2020
Theorem 6.	There exists a Protocolfor distributed MAB that does not use public randomness with
expected regret O( VMKTlog T). It has communication cost bounded by O(M log(MK) + K)
(worst case), and expected communication cost O(M log(M K)).
Proof. We make the following modifications to the original DEMAB protocol. Instead of using
a public random bit string s 12, we predetermine B strings s1,...,sB (which can be hardcoded in
advance), and randomly choose from them. That is, the server will generate a random number
uniformly distributed in [B], and broadcast it to everyone. The communication cost of doing so will
be M dlog2 Be. We now analyze how the choice of s1, ..., sB affects the performance of the protocol.
In terms of regret bound, observe that for any random string s, the expected regret of any bandits
instance X is always O(√MKTlog T). Therefore, regret bound will not be affected when public
randomness is removed.
Now define f(X, s) to be the expected communication cost of the DEMAB protocol using the public
random string s and interacting with the multi-armed bandit instance X. Our analysis for DEMAB
tells us that ∃c1, ∀X,
Er [f(X, s)] ≤c1Mlog(MK).
Therefore, if we draw i.i.d. uniform bit strings s1, ..., sB,
1B
Esi,...,sb B ∑f(X,si) ≤ cιMlog(MK).
We say that a set of bit strings {s1, ..., sB} is bad for a bandit X if
1B
B Ef (X, Si) > 2cιM log(MK).
B i=1
We know that there exists c2 > 0 such that 0 ≤ f(X, Si) ≤ c2(M log(M K) + K). Therefore, by
Hoeffding’s inequality,
Pr
s1,...,sB
1B
B Ef (X, Si) > 2cιMlog(MK)
B i=1
≤ eχTx	2Bc2Mlog(MK)]
≤ PI C2(1 + k/(mlog(MK)))广
In other words, for fixed X, the probability that we will draw a bad set {S1, ..., SB} is exponentially
small. Therefore, for a family of bandits with size Q, the probability for drawing a set of S1, ..., SB
that is bad for some bandit is at most Q ∙ exp
-
2Bc2M log(MK)
c2(l+K∕(M log(MK)))
.
If we can show that this
quantity is smaller than 1, it would follow that there exists { s 1,…，s b } such that it is not bad for
any bandit in the family.
Now, we consider the following family X of bandits. For each arm, the expected reward could be
a∕∆, where a ∈ {0,1,…，[△-" }. The reward distribution is Bernoulli. The size of this family is
Q ≤ (∆-1 + I)K. Now, consider any other bandit Xi. Without loss of generality, We can assume
that X1 is a Bernoulli bandit, and that the expectation of each arm is in [1/4, 3/4]13. Apparently we
can find a bandit X2 ∈ X such that their expected rewards are ∆-close in ∣∣ ∙ k∞. As a result, the KL-
divergence of each arm,s reward in Xi and X2 is O(∆2). Let H(X) = {a1,1, r1,1,…，aτ,M, IrTM}
be the random history of the DEMAB interacting with the bandit instance X . Since communication
cost is determined given H(X), ∀S,
|f(Xi, S) -f(X2,S)| ≤ c2 (M log(M K) + K) dT V (H(Xi), H(X2))
=O ((M log(MK) + K) ∙ √TM△).
With △ = K-i (MT)-0.5, the right-hand-side is O (M log(M K)). Therefore, it suffices to consider
the bandit family X .
12which has K dlog2 Me bits
13For a general bandit instance X1 , when reward r is received, we can generate a Bernoulli reward with
expectation r/2 + 1/4 to replace it. The regret bound will increase by only a constant factor.
21
Published as a conference paper at ICLR 2020
Therefore, We only need to guarantee that i+KMMgOMMK))> C0K log MKT, where C0 is a
universal constant. This can be met by setting B = d2C0K2 log(M K T)e. In this case, we can
guarantee that there exists a set of bit strings {s1, ..., sB}, such that for any bandit instance X, when
choosing s randomly from this set, the expectation of f(X, s) is O(M log(M K)).
The additional communication overhead for generating the random string (in bits) is
O(MlogB) = O(MlogK+Mloglog(MKT)).
Therefore, under our usual assumption that T > max{M, K}, the number of total communicated
bits is bounded by O (M log K + M log log T). In our formulation, we may view logT bits as
one packet. Therefore additional communication cost is O (M). It follows that total expected
communication cost is O(M log(MK)).	□
E Proof for Theorem 2
Proof. In order to prove the theorem, we show that for any protocol with communication cost less
that M/c (C is a constant), the regret is at least Ω(MKKT).
First, we list two lemmas that will be used in our proof.
Lemma E.1. (Lattimore & Szepesvdri, 2019, Theorem 9.1) For K-armed bandits, there is an
algorithm with expected regret
REG(T) ≤ 38√KT.
Lemma E.2. (Lattimore & Szepesvdri, 2019, Theorem 15.2) For K-armed bandits, for any algorithm,
there exists an instance such that
REG(T) ≥ ɪ P(K - 1)T.
75
The original lower bound is proved for Gaussian bandits, which doesn’t fit exactly in our setting. we
modified the proof to work for Bernoulli bandits, which results in a different constant.
We now prove the theorem’s statement via a reduction from single agent bandit to multi-agent bandit.
That is, we map communication protocols to single-agent algorithms in the following way. For
simplicity, we consider protocols as M blocks of code. In agent i’s block, each line could be a local
computation, sending a message, or waiting for a message to receive.
Consider a communication protocol with communication cost B(M). We denote Xi (i ∈ [M]) to
be the number of integers or real numbers that agent i sends or receives throughout a run. Xi is a
random variable. Since expected communication cost is less than M/c,
M
X EXi ≤ M/c.
i=1
Denote S as the set of M/2 agents with smaller EXi . The expected communication cost of any
i ∈ S is at most 2/c. For any i ∈ S, P(Xi ≥ 1) ≤ EXi ≤ 2/c. That is, for any of these agents,
the probability of either speaking to or hearing from someone is less than 2/c. Suppose that agent
j is such an agent. Then, we can map the communication protocol to a single-agent algorithm by
simulating agent j .
The simulation is as follows. Interacting with single agent bandit with time T, we run the code for
agent i in the protocol. When no communication is needed, we may proceed to the next line of agent
i’s code. When this line of code sends a message or waits for a message, we terminate the code. In
the rest of the timesteps, we run a single-agent optimal algorithm (the one used to realize lemma E.1).
Then, if agent j’s code has δ probability of involving in communication, and if agent j’s regret
REGj (T) ≤ A, via this reduction, we can obtain an algorithm for single-agent MAB with expected
regret
REG(T) ≤ A + δ ∙ 38√KT.
22
Published as a conference paper at ICLR 2020
By lemma 2, REG(T) cannot have a regret upper bound better than，T(K - 1)/75. Therefore
A + δ ∙ 38√KT ≥ P(K - 1)T/75.
If 38δ ‹ 1/75, We can show that A = Ω (√Kt) . In our case, C = 3000 will suffice. Since We can
show this for any agent in S, we can show that total regret is Ω (M √Kt) .	□
F Omitted Details of DELB
Assignment Strategy: At line 4, we assign pulls to agents in the following way. Let pl =
dPx ml (x)/M e denote the average pulls each agent needs to perform. Our assignment starts
from the arm with the largest ml(x) and agent 1. For arm xk and agent i, if agent i has been assigned
with pl pulls, we turn to agent i + 1. If we have finished designating ml(xk) pulls for arm xk, we
continue designating arm xk+1. The assignment is finished until all pulls are scheduled to agents.
Observe that at the start of each phase, each agent has the same Al as the server. Therefore, at line 3
they obtain the same πl, with the support size at most dlog log d. In that case, the server only needs
to send a index (O⑴ communication cost) over ξ = 48d log log d arms, instead of a vector (Ω(d)
communication cost), to identify an arm x ∈ Supp(πl).
G	Proof of Theorem 3
First, we consider some properties of the elimination based protocol for linear bandits.
Fact 2. Let Tl denote the total number of pulls in the l-th phase, then
C14ld2logMT ≤ Tl ≤ ξ+ C14ld2logMT,
where ξ = 48d log log d.
Proof. For an arm x in the core set Supp(πl), the DELB protocol pulls it dC14ld2πl(x) log MTe
times. Thus the total number of pulls in phase l satisfies
X	ml(a) ≥ X	C14ld2πl(x) ≥ C14ld2log(MT),
a∈Supp(πl)	a∈Supp(πl)
and
^X	mi (a) ≤	^X	(Cι4ld2∏ι (x) + l) ≤ 48d log log d + Cι4ld2 log MT.
a∈Supp(πl )	a∈Supp(πl)
□
Lemma G.1. In phase l, with probability 1 - 1/TM, for any x ∈ D,
∣hθ - θ*,xi∣ ≤ 2-i.
Proof. First, we construct an l-covering of D with l = 2-l-2. Denote the center of the covering as
X = {χι,..., XQ}. Here Q satisfies Q ≤ 3d2d(l+2).
Assume that θ is calculated from linear regression on x1, ... , xt0. For fixed x ∈ D, it is known that
hθ - θ*,xi is SUbgaUSSian with variance proxy
t0
Xhx,Vl-1xsi2 = kxk2V -1 ≤2kxk2V-1.	(2)
s=1
Therefore with probability 1 - 2δ,
∣hθ-θ*,xi∣≤ 2JkxkV-1 log；.
23
Published as a conference paper at ICLR 2020
Suppose nl pulls are made in phase l. In our case,
2
kxk2V -1
≤ g(π) ≤_____________
— nι — 4lCιd log Mr
Therefore with probability 1 - 2δ,
M - θ*,xil≤ 2-l+1r cidio2gMτ logδ.
Choose δ= 1/(2T M Q). It can be shown that with C1 = 600,
2log(2MTQ) / log2 + 1 + dlog3 + 2dlog2 + d/2 / 1
Cidlog MT ≤	300d	≤ 64.
Therefore with probability 1 - 1/(TM), for all x ∈ X
∣hθ - θ*,xi∣ ≤ 2-l-2.
Now, consider an arbitrary X ∈D. There exists X ∈ X such that k X - Xk ≤ 2-l-2. Therefore with
probability 1 - 1/TM, for any x ∈ D,
∣hθ - θ*,χ>∣ ≤ ∣hθ - θ* ,χi∣ + ∣hθ - θ* ,x - xi ∣
≤ 2-l-i + kθ - θ*∣H∣x - Xk
≤ 2-l .
□
Lemma G.2. Let x* = arg maxχ∈d(3* ,x) be the optimal arm. Then with probability 1 一
Iog(MT)∕(TM), x* will not be eliminated until the protocol terminates.
Proof. If X* is eliminated at the end of round l, one of the following must happen: either (1)
∣(θ - θ*,X*i∣ > 2-l;or (2) there exists X = x*, ∣(θ - 3*,x)∣ > 2^l. Therefore the probability for
X* to be eliminated at a particular round is less than 1 - 1/(TM). The total number of phases is at
most log MT. Hence a union bound proves the proposition.	□
Lemma G.3. Suppose δ = 2 log(TM)/TM, and ∆x denotes the suboptimality gap of X, i.e.
∆χ = (3*,x* — Xi. For suboptimal X ∈ D, define Ix = inf {l : 8 ∙ 2- ≤ ∆χ}. Then with probability
1 - δ, for any suboptimal X, X 6∈ Alx.
Proof. First, let us only consider the case where X* is not eliminated. That is,
Pr [∃X ∈ D : X ∈ Alx] ≤ Pr [X*is eliminated] + Pr [∃X : X ∈ Alx-1, X ∈ Alx |X* ∈ Ala] .
Note that conditioned on X* ∈ Alx, {X ∈ Alx-1 ∧ X ∈ Alx } implies that at phase lx - 1, either
∣(θ - 3*,x)∣ > 2-lx+1 or ∣(θ - θ*,X*i ∣ > 2-lχ+1. Therefore the probability that there exists such
X is less than Iog(TM)∕TM. Hence, with probability 1 — 2log(TM)∕TM, X will be eliminated
before phase lx.	□
We are now ready to prove our main result for DELB.
Theorem 3.	DELB protocol has expected regret O (d√TM log T), and has communication cost
O((Md+dloglogd)logT).
Proof. Regret: We note that at the start of round l, the remaining arms have suboptimality gap at
most 8 ∙ 2-l∙ Suppose that the last finished phase is L. Therefore total regret is
L
REG(T) ≤ X Cι4ld2 log MT ∙ 8 ∙ 2^l + δ ∙ 2MT
l=1
≤ C32Ld2 log TM.
24
Published as a conference paper at ICLR 2020
Apparently C14Ld2 logTM ≤ TM. Therefore
REG(T) ≤ JC24ld4 log2 TM ≤ C7dpTM log TM.
Under our usual assumption that T > M, this can be simplified to O(d，TM log T). Here C3 and
C7 are some universal constants.
Communication Cost: Let pl = Px ml(x)/M denote the average pulls each agent needs to perform.
Observe that for each arm, the number of agents that it is assigned to is at most 1 + dml(x)/ple
agents. Therefore, total communication for scheduling is at most
X (dml(x)/ple + 1) ≤ 2ξ + M = O(M + d log log d).
x
Similarly, total communication for reporting averages is the same. The cost for sending θ is Md.
Hence, communication cost per phase is O(Md + d log log d). On the other hand, total number of
phases is apparently O(log TM). Hence total communication is
O((Md+dloglogd)logTM)
Under the assumption that T > M, this can be simplified to O ((Md + dlog log d) log T).	□
Finite precision: We now discuss the number of bits needed in DELB. The integers sent in DELB
are less than max{T, ξ} (recall that ξ = 48d log log d). Therefore, every integer can be encoded with
O(log(dT)) bits. It remains to be proven that transmitting each real number with logarithmic bits is
sufficient. In the DELB protocol, two types of real numbers are transmitted: average of rewards, and
entries of θ. To transmit real numbers with finite number of bits, we make the following modifications
to the original protocol: 1. when transmitting average rewards at line 5, use randomized rounding
1
with precision €1 = Mt ; 2. after computing θ =匕 X at line 7, let θ be the entry-wise rounded
1
vector of θ with €2 = MTd.
It can be seen that we only need to prove that after the modifications, lemma G.1 still holds. In each
phase, originally μ(χ) is m匕(方)-subgaussian, but is only required to be m:方)-subgaussian for (2) to
hold. After the modification, the contribution of rounding error to a μ(x) comes from at most M
independent terms, and is therefore subgaussian with variance proxy MT ≤ 仃^(方).Therefore, after
the modifications, the computed μ(χ) is m九)-subgaussian; hence, (2) holds. It follows that with
probability 1 一 1∕(TM), for all X ∈ X, ∣hθ 一 θ*, xi ∣ ≤ 2-l-2. Therefore, for any a ∈D,
∣hθ - θ*,xi∣ ≤ 2-l-1.
Combined with the fact that for any x ∈ D ,
∣h石 一 θ,χi∣ ≤ kθ 一 θk ≤ vd€2 ≤ MT ≤ 2-l-1,
we can prove that with probability 1 一 1/(T M), for all x ∈ X,
∣hθ 一 θ*,xi∣ ≤ 2-l-2.
Therefore, after the modifications, the regret of the protocol is still O (d√TM log T). The amount
of communicated bits is thus
O ((Md + dlog log d) ∙ log T ∙ log(dT)).
25
Published as a conference paper at ICLR 2020
H Proof of Theorem 4
First, let us restate the DisLinUCB protocol in full detail.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Protocol 3: Distributed Linear UCB (DisLinUCB)
D = T log(MT)∕(dM), λ = 1
for Agent i = 1, ..., M do
I Set W~syn,,i = 0, Usyn,i = 0, Wnew,i = 0, Unew,i = 0, tla,st = 0, Vlast = λI
for t = 1, ..., T do
for Agent i = 1, ..., M do
Vt,i = λI + Wsyni + Wnew,i, θt,i = V-1 (Usyn,i + °new,i)∙
Construct the confidence ellipsoid Ct,i using Vt,i and θt,i .
(xt,i, θt,i) = arg max(x,θ)∈Dt ×Ct,i hx, θi
Play xt,i and get the reward yt,i .
Update Wnew,i
Wnew,i + xt,ixt,i, Unew,i =
Unew,i + xt,iyt,i.
Vt,i = λI + Wsyn,i + Wnew,i
if log (det Vt,i∕det V∕ɑst,i) ∙ (t - tiast) > D then
L Send a synchronization signal to server to start a communication round.
if A communication round is started then
Send Wnew,i and Unew,i to server
Server computes Wsyn = Wsyn + PjM=1 Wnew,j , Usyn = Usyn + PjM=1 Unew,j
Receive Wsyn , Usyn from server
_ Set Wnew,i = 0, Unew,i = 0, tlαst = t, VIast= λI + Wsyn
Let us then state several lemmas that will be useful in our proof.
Lemma H.1. For any δ > 0, with probability 1 — Mδ, θ* always lies in the constructed Ct,i for all t
and all i.
Proof. Using Theorem 2 in Abbasi-Yadkori et al. (2011) and union bound over all agents, we can
prove the lemma.	□
For any positive definite matrix V0 ∈ Rd×d, any vector x ∈ Rd, define the norm of x w.r.t. V0 as
∣∣xkvo := VzxTVOx.
Lemma H.2. (Lemma 11 in Abbasi-Yadkori et al. (2011)) Let {Xt }t∞=1 be a sequence in Rd, V is a
d X dpositive definite matrix and define Vt = V + Ps=1 XsX>. Then we have that
log (» ≤ X kXtkV-Λ .
Further, if ∣Xt ∣2 ≤ L for all t, then
det (Vn) — log det V) ≤ 2 Idlog ((trace(V) + nL2) /d) — log det V).
Using Lemma H.1, we can bound single step pseudo-regret rt,i .
Lemma H.3. With probability 1 — Mδ, single step PSeUdo-regret rt,i = <θ*,x* — xQ is bounded
by
rt,i ≤ 2 (j2log (deK⅞i)V2δiet(λl) 1ΞJ + λ1/2! kxt,ikvt-iι = O (Jdlog IT! ∣xt,ikvt-iι .
(3)
26
Published as a conference paper at ICLR 2020
Proof. Assuming θ* ∈ Ct,i,
rt,i = hθ*,x*i — hθ*,xt,ii
.-T	.	. _ .	,
≤ hθt,i, xt,ii - hθ , xt,ii
.~ _ , .
=hθt,i - θ , xt,ii
= hθt,i - θt,i, xt,ii + hθt,i - θ , xt,ii
≤ at,i—θt,i∣ιVt.i Ei"%—21+iiθt,i—θ*ιιVt,i kχt,ikvt-iι
≤ 2 (S2^≡2δd≡J+a"忆
T ^^T∖ U U
d log J	kxt,i kVt-iι .
□
Now we are ready to prove Theorem 4.
Theorem 4.	DisLinUCB protocol achieves a regret of O (d√MT log2 (T)) with O (M 1∙5d3) com-
munication cost.
Proof. Regret: Set δ = 1/(M2T), the expected regret caused by the failure of Eq. (3) is at most
MT ∙ 1∕(MT) = O(1), thus We mainly consider the case where Eq. (3) holds.
In our protocol, there will be a number of epochs divided by communication rounds. We denote
Vlast in epoch p as Vp . Suppose that there are P epochs, then VP will be the matrix with all samples
included.
Observe that det Vo = det(λI) = λd. det(Vp) ≤ (tr(VP)) ≤ (λ + MT∕d)d. Therefore
det(VP)
og det(Vo)
≤ d log
1+
MT、
F )
Let R := dd log(1 + MdT)]. It follows that for all but R epochs, we have
1 ≤ det Vj
一 det Vj-ι
≤ 2.
(4)
We call those satisfying Eq. 4 good epochs. In these epochs, we can use the argument for theorem
4 in Abbasi-Yadkori et al. (2011). First, we imagine the MT pulls are all made by one agent
in a round-robin fashion (i.e. he takes χι,ι, χ1,2,…，xi,m, x2,1,…，xt,m). We use Vt,i = λI +
P{(p,q)∙∙(p<t)∨(p=t∧q<i)} Xp,qxT,q to denote the Vt,i this imaginary agent calculates when he gets to
xt,i. If xt,i is in one of those good epochs(say the j-th epoch), then we can see that
Therefore
det Vt,i
≤ 2.
1 ≤ -t,i ≤ det Vj
一 det Vt,i - det Vj—i
27
Published as a conference paper at ICLR 2020
We can then use the argument for the single agent regret bound and prove regret in these good epochs.
We denote regret in all good epochs as REGgood. Suppose Bp means the set of (t, i) pairs that belong
to epoch p, and Pgood means the set of good epochs, using lemma H.2, we have
REGgood =	rt,i
≤, M X X 啥
p∈Pgood (t,i)∈Bp
/
≤O

dMTlog(T) X X min (kxt,ikV-ι, 1)
p∈Pgood (t,i)∈Bp	,
≤ O (VMT;gTJ∏lV≡)
p∈ good
dMT log( T )logfdet-(yγ
δ	det (V0)
≤ O (d√MTlog(MT)).
Now we focus on epochs that are not good. For each bad epoch, suppose at the start of the epoch we
have Viast. Supposethat the epoch starts from time step to, and the length of the epoch is n. Then
agent i proceeds as Vto,i,..., Vt°+n,i∙ OUr argument above tells us that regret in this epoch satisfies
Mn
REG ≤ 2 (Pdlog T∕δ) X X min (kxt,ik匕-i , 1)
i=1 t=t0
det Vto+n,i
det Viast
Now, for all but 1 agent, n log ddtVtV+n,i < D. Therefore We can show that
det Vlast
REG(n) ≤ O (√d log T∕δ) ∙ M√D.
Since det(VP ) ≤ (λ + MT∕d)d, we know that the number of such epochs are rare. (Less than
R = O(d log MT)). Therefore the second part of the regret is
REGbad ≤ O (Md1∙5 log1∙5 MT) ∙ D1/2.
Ifwe choose D = (T IdMMr) , then REG(T) = O (d√MT log2 (MT)) . Since T > M, we have
REG(T) = O (d√MT log2(T)).
Communication: Let α = (DR)0.5 . Apparently there could be at mostdT/ae such epochs that
contains more than α time steps. If the j -th epoch contains less than α time steps, log (ddlVj+1) > D.
Since
P 1
X log
j=0
det Vj+1
det Vj )
det VP
og det VI
≤ R,
There could be at most dDRa] = dRa] epochs with less than α time steps. Therefore, the total
number of epochs is at most
T Rα
d ae + d 万e
With our choice of D, the right-hand-side is O M0.5d . Communication is only required at the
end of each epoch, when each agent sends O(d2) numbers to the server, and then downloads O(d2)
numbers. Therefore, in each epoch, communication cost is O(M d2). Hence, total communication
cost is O(M 1∙5d3) .	□
28
Published as a conference paper at ICLR 2020
Finite precision: we now consider the number of bits transmitted in the DisLinUCB protocol.
To that end, we make the following minor modification to DisLinUCB. First, when reward yt,i is
observed, we replace it with a random integer in {±1} with expectation yt,i . Second, after line
8, after xt,i is played, we round each entry of xt,i with precision , and use the rounded vector
in the calculation in line 9. In this case, each entry of Wnew,i and Unew,i is a multiple of 2.
Therefore, transmitting them requires O(d2 log -1) bits. The total communication complexity is
then O (M 1.5d3 log ET) bits.
We now discuss how to choose such that regret is not effected. Define
MT
REG (H) := ^X ^X maχhx — xt,i,θ*),
i=1 t=1x∈Dt
MT
REG (H) := ^X^X maxhx — xt,i,θ*).
i=1 t=1 x∈Dt
Here H is a shorthand for a history (χι,ι, y1,1,…，xt,m, yτ,M). D refers to set of rounded actions.
For every X ∈D, there exists X ∈ D such that k X — Xk ≤ √de. Therefore
IREG (H) — REG (H)I ≤ MT√de.
On the other hand, let H be the (random) history of the DisLinUCB with rounding on action sets
Dt, while H is the (random) history of the DisLinUCB with rounding running on action sets Dt.
Then at each time step, the mapping from past history to the next action is the same. Therefore
KL(H, H) = O(MTdc2)Jtfollows that
1E [REG (H)] — E [REG (H)] i ≤ O (√M3T3de).
When the action set is Dt, no rounding is needed, so the regret analysis for the DisLinUCB protocol
without rounding directly follows. Therefore,
E [REG (H)] = O (d√MT log2 T).
By choosing e = (MT)-1, we can guarantee E [REG (H)] = O (d√MTlog2 T) for any action
set. In this case, the total number of communicated bits is O M1.5d3 logT .
I DEMAB and DELB in P2P Communication Networks
In this section, we will briefly discuss how to implement our protocols (i.e. DEMAB and DELB) in
the P2P communication network considered in Korda et al. (2016). We show that our protocols can
be adopted to P2P networks after little modification. The communication cost will remain the same,
while regret bounds would only increase marginally.
In P2P communication networks, an agent can receive information from at most one other agent at a
time, which leads to an information delay for each agent. In order to cope with such delay, we need
to extend the length of each communication stage from 1 time step to M time steps, so that agents
can complete the communication in turn. Since there are at most O(log(M K)) communication
stages in DEMAB and O(log T) communication stages in DELB, the extension of communication
stages incurs at most O(M2 log(M K)) regret in DEMAB and O(M2 logT) regret in DELB for M
agents. When the time horizon T is large (i.e. T > M3 log M), the additional term is dominated by
O (√MKT log T) and O (d√MT log T). Another issue for P2P networks is that there is no longer
a physical server in the networks. To solve this problem, we can designate agent 1 as the server:
agent 1 will execute both the codes for the server and the codes for an agent.
Specifically, by saying “extending the length of the communication stage”, we mean that we can use
Procedure 10 and 11 to realize communication subroutines used in our protocols in P2P networks:
sending message to the server and receiving messages from the server.
29
Published as a conference paper at ICLR 2020
1
2
3
4
5
1
2
3
4
5
Procedure 4: Server2Agent: Agent 1 sends message mi to agent i in a P2P network.
For the next M - 1 time steps:
/* For agent 1:	*/
Send mi to agent i + 1 at the i-th step
Pull an arbitrary arm at each time step
/* For agent i(i > 1):	*/
Receive mi from agent 1 at the (i - 1)-st step
Pull an arbitrary arm at each time step
Procedure 5: Agent2Server: Agents i(i > 1) sends message mi to agent 1 in a P2P network.
For the next M - 1 time steps:
/* For agent 1:	*/
Receive mi+1 from agent i + 1 at the i-th step
Pulls an arbitrary arm at each time step
/* For agent i(i > 1):	*/
Send mi to agent 1 at the (i - 1)-st step
Pull an arbitrary arm at each time step
I.1 DEMAB IN P2P NETWORKS
For distributed DEMAB in a P2P network, we can replace the communication stage in DEMAB (i.e.
line 8, 12, 15 of Protocol 1) by Procedure Server2Agent and Agent2Server. In this way, it costs M
time steps instead of a single time step to collect, aggregate, and boardcast information. We have the
following theorem showing the efficacy of distributing DEMAB in a P2P network.
Theorem 7. The DEMAB protocol in P2P networks incurs regret
O (√MKTlogT + M2 log(MK)), with expected communication cost O (MTog(MK)).
When T > M3 log M, the regret bound ofthisprotocol is near-optimal O	MKT log T).
Proof. Regret: We compare DEMAB protocol in P2P net with the original one (i.e. Protocol 1).
The burn-in stage (i.e. Stage 1) of both protocols are the same. For distributed elimination stage
(i.e. Stage 2), the length of each phase in the new protocol is no shorter than that in Protocol 1.
Therefore, the number of phases after phase l0 + 1 (included) in new protocol is no more than that in
Protocol 1, which is O(log(M K)). In each phase starting from phase l0 + 1, new protocol needs
O(M) additional steps to complete the communication in this phase, incurring O(M) additional
regret per agent. Therefore, this protocol incurs O(M2) additional regret per phase starting from
phase lo + 1. The total regret of this protocol is thereby O (√MKT log T + M2 log(MK)).
Communication: We still consider only distributed elimination stage. There are three communica-
tion stages per phase in Protocol 1: Line 8, 12, and 15.
In line 8 of DEMAB protocol, the total communication is O(M) since nmax is boardcast from the
(i)
server, and each agent sends nl to the server. We can observe that the communication cost at
corresponding place is also O(M) by replacing the boardcast with Server2Agent. In line 12, the
communication cost of both protocols is still the same due to the same reason. In line 15, the new
protocol calls Agent2Server which runs for M steps, while the agents report the rewards in the
original protocol in a single step. The communication cost is O(M) for both protocols.
In summary, the communication cost of the new protocol is the same as that of Protocol 1, which is
O (M log(MK)).
□
30
Published as a conference paper at ICLR 2020
I.2 DELB IN P2P NETWORKS
Very similar to the P2P version of DEMAB, we can also distribute DELB to P2P networks by
replacing the communication stage of DELB by Server2Agent and Agent2Server. We have the
following theorem for the P2P version of DELB.
Theorem 8. The DELB protocol in a P2P network has regret O (d√MTlogT + M2 log T) with
expected communication cost O ((Md + dlog log d) logT). When T > M3 logM, the regret of
DELB is a near-optimal regret O (d√MTlogT).
Proof. The proof is very similar to the proof of theorem 7. Note that in the P2P version of DELB,
there are O(log T) communication stages in total, which incurs O(M2 log T) additional regret. The
communication cost of the new protocol is the same as Protocol 2 for the same reason mentioned in
the proof of theorem 7.	□
31