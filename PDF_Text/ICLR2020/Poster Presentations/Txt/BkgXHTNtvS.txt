Published as a conference paper at ICLR 2020
Bounds on Over-Parameterization
for Guaranteed Existence of Descent Paths
in Shallow ReLU Networks
Arsalan Sharifnassab*
Department of Electrical Engineering
Sharif University of Technology
Tehran, Iran
a.sharifnassab@gmail.com
Saber Salehkaleybart
Department of Electrical Engineering
Sharif University of Technology
Tehran, Iran
saleh@sharif.edu
S. Jamaloddin Golestani
Department of Electrical Engineering
Sharif University of Technology
Tehran, Iran
golestani@sharif.edu
Ab stract
We study the landscape of squared loss in neural networks with one-hidden layer
and ReLU activation functions. Let m and d be the widths of hidden and input
layers, respectively. We show that there exist poor local minima with positive
curvature for some training sets of size n ≥ m + 2d - 2. By positive curvature
of a local minimum, we mean that within a small neighborhood the loss function
is strictly increasing in all directions. Consequently, for such training sets, there
are initialization of weights from which there is no descent path to global optima.
It is known that for n ≤ m, there always exist descent paths to global optima
from all initial weights. In this perspective, our results provide a somewhat sharp
characterization of the over-parameterization required for “existence of descent
paths” in the loss landscape.
1	Introduction
We consider shallow neural networks of the form shown in Fig. 1. The network comprises a hidden
layer and an input layer of widths m and d, respectively; and is to be trained over a training set of size
n. Our results concern the slightly over-parameterized regime where n ≈ m. We study the existence
of poor local minima that have positive curvature in the empirical squared loss landscape.
It is well-known that poor local minima exist in the loss landscape of shallow networks of arbitrary
width. In fact, in a shallow network with ReLU activation functions, it is easy to construct training
sets whose empirical loss landscape has high plateaus.* 1 It is however not fully understood that under
what conditions poor local minima may have positive curvature. This paper presents results that
improve this understanding.
Non-existence of spurious local minima is closely connected to the so called descent path property: a
loss landscape is said to have the descent path property if starting from any initial point there is a
path of descent loss to a global minimum. From optimization perspective, the descent path property
favors descent optimization algorithms like the pure gradient descent (GD) method. For SGD as well,
non-existence of poor local minima is known to be a favorable property for guaranteed convergence
(Ge et al., 2015; Jin et al., 2017; Lee et al., 2016). The descent path property is shown to be satisfied
in over-parameterized shallow networks with sufficiently large widths (Venturi et al., 2018). The
* Webpage: http://ee.sharif.ir/~sharifnassab/
* Webpage: http://sina.sharif.ir/~saleh/
1e.g., for a training set and a set of weights at which all neurons are inactive.
1
Published as a conference paper at ICLR 2020
results we present in this work, tighten the existing bounds on the over-parameterization required to
guarantee this property.
1.1	Background
Over the past few years, deep neural networks have achieved tremendous performance in various
artificial intelligence applications such as computer vision, reinforcement learning, and natural
language processing, etc. Despite their remarkable success in practice, theoretical aspects of this
success remain a mystery. It has long been an open problem why simple local search algorithms for
training deep neural networks, like stochastic gradient descent (SGD), typically converge to local
minima with low training error despite the highly non-convex behavior of empirical loss. It has been
observed, e.g., in (Choromanska et al., 2015), that these methods may get stuck in poor local minima
(i.e., local minima with empirical loss much larger than the global optimum) for small networks,
while the problem fades away as the number of parameters grows larger. Such observations are
often explained by studying the loss landscape in over-parameterized regime where the number of
parameters in the network exceeds the training sample size.
Recently, several attempts have been made to characterize properties of squared loss landscape by
conditioning on the layers’ dimensions and sample size. Soudry and Hoffer (2017) showed that
weights of a neural network can be adjusted such that the empirical loss is zero almost surely if
m > 4dn/(2d - 2)e ≈ (2n)/d. This result is consistent with experimental observations that neural
networks can fit training data if the number of parameters (here approximately 2n) is greater than
the sample size. They also proved for normally distributed input that as n goes to infinity, the ratio
between the volume of poor flat local minima regions to the volume of flat global minima fades
exponentially if d = Ω(√n) and m = Ω(n∕d). Safran and Shamir (2016) showed that if the number
of neurons in the hidden layer is Ω(nrank(X)) (where X is the matrix containing all input), then with
high probability, random initialization of weights will put them in a region of parameter space at
which the loss surface has a basin-like structure, i.e., every local minimum in that region is global. In
another work (Safran and Shamir, 2017), the same authors provide a computer-assisted proof to show
that spurious local minima are common in the expected loss landscape of shallow under-parameterized
(small-width) networks. Xie et al. (2016) showed that if the input data is drawn uniformly at random
from the unit sphere, and if m = Ω(nβ) and d = Ω(nβ) with β ∈ (0,1) being the decay exponent of
the smallest eigenvalue of a kernel matrix, then every critical point is a global minimum. Li et al.
(2018) proved that for any continuous activation function and under the assumption that data samples
are distinct, there exist no poor local minima with positive curvature if m ≥ n . In the same spirit,
Venturi et al. (2018) showed that for any continuous activation function, there is always a descent
path to an optimal solution with zero loss in the empirical loss landscape if m ≥ n.
Several works have proposed similar results in other settings and under different assumptions. Soudry
and Carmon (2016) showed that in a network of leaky ReLU activation functions with randomized
perturbation of slopes, all differentiable local minima are global minima if m ≥ n/d. Kawaguchi
(2016) proved that in shallow networks with linear activation functions, every local minimum is
a global minimum and all the saddle points are strict in the sense that they have a direction of
strictly negative curvature. Soltanolkotabi et al. (2019) showed that the same result carries over to
quadratic activation functions under the assumption that the last layer comprises at east d positive and
d negative weights. Du and Lee (2018) established similar results for quadratic activation functions,
assuming m ≥ √2n. For deep neural networks with linear activation functions, Freeman and Bruna
(2016) showed that all local minima are global minima if there is a hidden layer whose number of
neurons exceeds the minimum of the widths of input and output layers. For deep neural networks
with analytical activation functions, Nguyen and Hein (2017) proved a similar property under the
assumptions that the number of neurons in some hidden layer is greater than sample size and the
network has a pyramidal structure.
Such studies on the properties of loss landscape do not only provide insights into the complication of
training, but are also beneficial for proving performance guarantees for some local search algorithms.
For the class of loss functions whose landscape satisfy the properties of: a) all local minima are
global, b) all saddle points are strict, it has been shown in several works (Ge et al., 2015; Jin et al.,
2017; Lee et al., 2016) that perturbed gradient descent converges to global optima in polynomial time.
Another direction of research concerns the convergence of specific optimization algorithm such as
2
Published as a conference paper at ICLR 2020
Figure 1: Architecture of the shallow network considered in this paper. The network has a single
hidden layer of m neurons with ReLU activation functions, and a neuron with linear activation
function in its output layer.
pure gradient descent and SGD without assuming such properties for the loss landscape (DU et al.,
2018; Allen-Zhu et al., 2018; Du et al., 2018).
1.2	Our contributions
We study the amount of over-parameterization required for guaranteed existence of descent paths to
zero loss in the empirical loss landscape. Previous works suggest that zero loss is always possible for
m ≥ 2n/d (Soudry and Hoffer, 2017). on the other hand, the best existing bound for guaranteed
existence of descent paths to this zero loss requires m ≥ n neurons in the hidden layer (Venturi
et al., 2018). Prior to the present work, it was not known whether the “descent path property” holds
for m < n. Even for m ∈ (2n/d, n), where zero empirical risk is known to be achievable (Soudry
and Hoffer, 2017), the existence of descent paths was in question. in this work, we tighten this gap
and prove that there are training sets, under which in any network of width m ≤ n - 2d + 2, there
exist initial weights that have no descent path to global minima. We do this by showing that the
loss landscape, in this regime, admits poor local minima with positive curvature. We also provide
evidences and make conjectures that these results carry over to networks of width m = n - 4,
which, if true, provides a sharp characterization of the over-parameterization required for guaranteed
existence of descent paths. We also wish to point that unlike most previous works, we do not restrict
to differentiable local minima; for a simple argument shows that local minima with positive curvature
cannot be differentiable if m > n/d (cf. Appendix A).
1.3	Outline
We continue by discussing details of the system model and introducing our key definitions in Section 2.
We then present, in Section 3, the main results of the paper. Proof of the main results are then given
in Section 4. We finally discuss implications and possible extensions of our results in Section 5 along
with a number of open problems and directions for future research.
2	Preliminaries
2.1	Model
We consider shallow networks of the form shown in Fig. 1. The network takes d-dimensional inputs
denoted by X. There is a single hidden layer comprising m neurons with ReLu activation function.
For simplicity of our proofs, we only consider even values of m. We denote the input weights of
r-th neuron by a d-dimensional vector wr, for r = 1, . . . , m. We then let w ∈ Rmd be the vector
representation of all weights in the first layer.
The output layer has a single neuron, whose activation function is linear with an m-dimensional
weight vector denoted by v. The network outputs a scalar y(w, V) = Pm=I VrWTX 1(wTX ≥ 0).
We fix a training set (X1, y1), . . . , (Xn, yn) of size n, and consider the landscape of empirical squared
loss function:
F(w,v) , X (yi(w,v) - y，2.	(1)
i=1
3
Published as a conference paper at ICLR 2020
2.2	Properties of the landscape
We first provide a formal definition for the descent path property, which is a necessary condition for
guaranteed performance of descent optimization algorithms.
Definition 1 (Descent path property). Consider a Continuousfunction f : Rd → R and let f * =
inf x∈Rd f(x) be its infimum. We say that f has the descent path property iffor any x ∈ Rd, there
exists a continuous curve with γ : [0, 1] → Rd such that γ(0) = x, f γ(1) = f*, and f γ(t) is a
non-increasing function of t.
The descent path property is a necessary condition for any descent optimization algorithm to provably
find a global minimum from all initial conditions. It was shown in Venturi et al. (2018) that the
empirical loss landscape of a shallow neural network with ReLU activation and squared loss has the
descent path property if the size of training data is no larger than the width of the hidden layer, i.e.,
n ≤ m. We now characterize a class of local minima of specific form in the following definition.
Definition 2 (Cupped minima). Given a function f : Rd → R, we call x ∈ Rd a cupped minimum of
f if there are , δ > 0 such that for any y in the δ-neighborhood of x, we have f(y) ≥ f(x)+ky-xk2.
By a sub-optimal cupped minimum we mean a cupped minimum that is not a global minimum.
Note that every cupped minimum is a local minimum, but not every local minimum is cupped (e.g.,
flat local minima are not curved downwards, and hence are not cupped). Also note that a function is
not necessarily differentiable at its cupped minima. We study cupped minima of the loss function
in equation 1. Note however that for any α > 0, F(αw,v∕ɑ) = F(w, v). Therefore, F(∙, ∙) has
no cupped minima if both arguments are taken as variables. For that matter, when talking about
cupped minima of F, we fix a V and consider F(∙, V) as a function of its first argument. Interestingly,
existence of cupped minima for F(∙,v) leads to violation of descent path property for F(∙, ∙) over
both arguments, as shown in the following lemma. The proof is given in Appendix B.
Lemma 1. Consider a shallow network with loss function F in equation 1, and a pair of weights
(w, v). Suppose that W is a sub-optimal cupped minimum of F(∙, V), and that Wr = 0, for r 二
1,..., m. Then, F (∙, ∙) has no descent path (w(t), v(t)), initiated at (w, v), to its global minima.
3	Main results
The following theorem and corollary state the main results of the paper.
Theorem 1. For any d ≥ 4, m ≥ 8 + 4 3/(d - 3) , and n ≥ m + 2d - 2, there exists a training
set of size n such that the empirical loss function F of a shallow neural network of width m has the
following property. For any m-dimensional vector v, with m/2 number of positive and m/2 number
of negative entries, F (∙, v) has exponentially many SUb-OPtimal cupped minima.
The proof is constructive and is given in Section 4. In particular, We devise a training sequence
(Xi, yι),..., (Xn, yn) such that for weights (w, v) at the cupped minima, we have ∣∣vr Wrk = 1∕√m,
for r = 1, . . . , m. Moreover, kXik ≤ 1, |yi| ≤ 2, and |ei| = 1 for i = 1, . . . , n (cf. Remark 2).
According to Theorem 1, there are training sequences tailored to give rise to sub-optimal cupped
minima. However, we wish to point that the existence of such cupped minima does not stem from
measure-zero incidents like placement of several data points on a low dimensional plane. In fact, in
view of Lemma 1, any path that starts from a cupped minimum and end up in a global minimum
would have an uphill climb of at least , for some > 0. Since the loss surface is a continuous
function of (Xi, yi), a small perturbation of (Xi, yi)’s leads continuously to a small change in F.
Therefore, for small enough perturbations of (Xi, yi), any path to the set of global minima would still
witness a positive uphill-climb. Hence, the descent path property remains out of order, even when
the training data is slightly perturbed. Based on the above intuition, we can establish the following
corollary2,
Corollary 1. For any d ≥ 4, m ≥ 8 + 4 3/(d - 3) , and n ≥ m + 2d - 2; and when the inputs
X and labels y are randomly drawn from independent normal distributions, there is a non-zero
probability that F (∙, ∙) does not have the descent path property.
2A formal proof is pretty tedious, and is not presented here.
4
Published as a conference paper at ICLR 2020
Note that Corollary 1 does not imply Theorem 1, because a landscape could be cupped-min-free even
if it the descent path property is not satisfied.
It was shown in Venturi et al. (2018) that n ≤ m is sufficient for the descent path property to hold. In
contrast, Corollary 1 show that if n ≥ m + 2d - 2, then the descent path property is not necessarily
in effect. This leave a gap of size 2d - 2 for the edge of over-parameterization required to guarantee
the descent path property. We believe that this edge lies sharp at n = m. We conjecture a stronger
version of Theorem 1, that cupped minima can emerge for training data sizes as small as m = n - 4.
Conjecture 1. Statement of Theorem 1 holds for all d ≥ 4, m ≥ 2d + 4, and n ≥ m + 4.
See Remark 1 for insights into the possibility of this conjecture.
4	Proof of the main res ult
In this section, we present the proof of Theorem 1 organized in a sequence of four subsections. We
first present some preliminaries in Subsection 4.1. In Subsection 4.2, we introduce a geometric
structure called “(d, t, k)-configuration”, based on which we construct, in Subsection 4.3, the training
set that gives rise to cupped minima in the loss landscape. Finally, in Subsection 4.4, we prove
the existence of cupped mimima in the devised setting. In order to provide intuitions on the loss
landscape at cupped minima and motivate our construction of the training set in Subsection 4.3, we
make a short note on different types of cupped minima in Appendix A. We also defer the proofs of
some lemmas from this section to appendices for improved readability.
4.1	Preliminaries
Consider weights (w, v) and let (w0, v0) be another set of weights such that for r = 1, . . . , m, vr and
vr0 have the same sign and wr0 = (vr/vr0 )wr. Then, F(w, v) = F (w0, v0). Moreover, it is no difficult
to see that W is a cupped minimum of F(∙, V) if and only if w0 is a cupped minimum of F(∙, v0). For
this reason, it suffices to prove existence of cupped minima for a fixed vector v. Note also that where
wr ’s are distinct, any permutation of the order of neurons would give rise to a new cupped minimum.
Hence, existence of a cupped minimum for F(∙,v) implies existence of exponentially many cupped
minima for F(∙, v).
We denote by ei = y^i(w, V) - yi the estimation error for input Xi. We let Ud = [0,..., 0,1]T be
a d-dimensional vector with the last entry equal to one and all other entries equal to zero. For a
region P ⊆ Rd, we denote its interior and and its convex-hull by int(P) and Conv(P), respectively.
Assuming differentiability of F at w, the partial derivatives of the loss function with respect to wr,
r = 1, . . . , m, is as follows
n
Rwr F(w,v) = Vr X eiXi 1(WTXi ≥ 0).	(2)
i=1
4.2	A geometric configuration
We introduce a geometric structure for sets of points in Rd . This configuration will be used in
Subsection 4.3 to construct a landscape with cupped minima.
Definition 3 ((d, t, k)-Configuration). Given integers d, t, k ≥ 0 and disjoint sets A, A, B, and B of
points in Rd, we say that (A, A, B, B) forms a (d, t, k)-configuration ifthefollowingproperties are
satisfied:
(p1) Each ofA and B consists of t points, and each ofA and B consists of k points.
(p2) (A) The convex hull ofA A forms a polytope PA that has exactly t + k vertices. Equiva-
lently, no point in AUA is a convex combination ofother points in AUA∙
(B)	Similarly, the convex hull of B	B forms a polytope PB that has exactly t + k vertices.
(p3) We have 0 ∈ int(PA) and 0 ∈ int(PB).
(p4) There exists a constant β ∈ (0, 1) such that
(A)	For any a ∈A, βa lies on a facet ofPB. We denote this facet by SB (a).
5
Published as a conference paper at ICLR 2020
(B)	For any b ∈ B, βb lies on a facet of PA. We denote this facet by SA(b).
(p5) (A) For any a ∈ A, SB (a) is a (d - 1)-dimensional simplex.
(B)	For any b ∈ B, SA(b) is a (d - 1)-dimensional simplex.
(p6) (A)	For any a ∈	A, there exist scalars α1, . . . , αd ∈ (0, 1) such	that	a	=	Pid=1	αisi(a),
where s1(a),	. . . , sd (a) are the vertices of simplex SB (a).
(B)	For any b ∈	B, there exist scalars α1 , . . . , αd ∈ (0, 1) such	that	b	=	Pid=1	αisi(b),
where s1(b),	. . . , sd(b) are the vertices of simplex SA(b).
(p7) (A) For any pair a and a0 of distinct points in A, we have SB (a) 6= SB (a0). Moreover,
letting Ha be the hyperplane that contains SB (a), a and a00 lie on opposite sides of
Ha ,for all a" ∈ AUA With a00 = a ∙
(B)	For any pair b and b0of distinct points in B, we have SA(b) 6= SA (b0). Moreover,
letting Hb be the hyperplane that contains SA(b), b and b00 lie on opposite sides of Hb,
for all b ∈ B U B with b = b.
(p8) Consider a 2t × 2t matrix M whose rows and columns are associated to points p ∈ A B
and q ∈ A B, and whose entries are as follows
Mpq
d p, Hq , if p = q, OR p ∈ A and q ∈ B, OR p ∈ B and q ∈ A,
0,
otherwise,
(3)
where Hq is the hyperplane define in Property (p7), and d(∙, ∙) is the euclidean distance.
The property requires M to be full-rank.
Among the above properties, the most difficult of all is Property (p4), and the requirement that it
involves the same β for all points in A U B . In fact, elimination of Property (p4) gives rise to trivial
configurations. 3
In the two dimensional space, for any t ≥ 4 there exists a (2, t, 0)-configuration of the form illustrated
in Fig. 2. In the following proposition, we generalize this observation to higher dimensions.
Proposition 1. For any d ≥ 2 and t ≥ 4, there exists a (d, t, d - 2)-configuration.
The proof is constructive, and is given in Appendix C. We conjecture that there also exist (d, t, 0)-
configurations.
Conjecture 2. For any d ≥ 2 and t ≥ 2d, there exists a (d, t, 0)-configuration.
Remark 1. Using a configuration given by Conjecture 2 instead of the configuration from Proposi-
tion 1 in the construction and the proof that follow, we obtain a proof for Conjecture 1. In this view,
establishing Conjecture 2 would also resolve Conjecture 1.
4.3	Constructing a landscape with cupped minima
Here we present a set of training data (X1, y1), . . . , (Xn, yn) and a set of wights
(w1, v1), . . . , (wm, vm) such that the empirical loss surface corresponding to (X1, y1), . . . , (Xn, yn)
has a sub-optimal cupped minimum at (w1, v1), . . . , (wm, vm). Without loss of generality, we assume
n = m + 2d - 2. Extension to larger values of n is straightforward via replication.
Let (A, A, B, B) be a (d - 1, m/2, d - 3)-configuration in the (d - 1)-dimensional space, as
in Proposition 1. In view of Property (p3), let 0 > 0 be such that PA and PB contain the 0-
neighborhood of 0. Let
ξ , W。 X all + U X "D+ n + 1.	(4)
0	β
a∈AS A	b∈B S B
We proceed by introducing the data points X1, . . . , Xn. An illustration of these data points in the
three dimensional space is shown in Fig. 3.
3e.g., for A and A chosen uniformly at random over the unit sphere, and letting (B, B) be a small rotation of
(A, A) around an arbitrary axis, it is not difficult to see that all properties except (p4) would be satisfied with
high probability.
6
Published as a conference paper at ICLR 2020
Figure 2: A (2, t, 0)-configuration with
t = 8. The blue dots show the points in
A and red crosses are the points in B.
¾^‹⅞+
X
Xb+~-	XB-
Figure 3: Illustration of data points
X1,..., Xn for d = 3 and m = 12.
Data points X: We consider a total number of n = m + 2d - 2 data points as follows.
•	For each a ∈ A U A, we consider a new data point Xa as follows. Let [z1,..., zd-1 ] ∈ Rd-1
be the representation of a in the Cartesian coordinates. We let Xa = z1, . . . , zd-1, 1 .
•	For each b ∈ B U B, we consider a new data point Xb as follows. Let[z1,...,zd-1] ∈ Rd-1
be the representation of b in the Cartesian coordinates. We let Xb = - z1 , . . . , zd-1, 1 .
•	We consider two extra points XA+, XA- , XB+, and XB- as follows. We let XA- , ξud,
XB- , -ξud, and
Xa+	,	XA-	-	X	Xa	+ (l∕β - 1)ud	(5)
a∈A U A
XB+	,	XB-	+	X	Xb	+ (1/e -I)Ud	⑹
b∈B U B
where ud = [0, . . . , 0, 1]T , and ξ and β are defined in equation 4 and Property (p4),
respectively.
Weights at cupped minimum: We associate each of m neurons to a point p in A B , in a one-one
manner; and denote the vector of input weights and the output weight of that neuron by wp and vp ,
respectively. These weights are chosen as follows:
•	For each a ∈ A, we let Va = -1/√m.
•	For each b ∈ B, we let Vb = 1/ʌ/m.
•	For each a ∈ A, consider the facet SB (a) defined in Property (p4), and let
s1(a), . . . , sd-1(a) be the vertices of SB (a) (as in Property (p6)). We let wa be the unique
vector such that kwa k = 1 and
waTXsi(a) =0,	i = 1,...,d-1,	(7)
waT ud < 0.	(8)
•	For each b ∈ B, consider the facet SA(b) and let s1(b), . . . , sd-1(b) be the vertices of SA(b).
We let wb be the unique vector such that kwb k = 1 and
wbTXsi(b) = 0,	i = 1, . . . , d - 1,	(9)
wbT ud > 0.	(10)
Labels y: Having determined the data points X and the weights (w, V), the output y(w, V) of the
network is determined for all input X . In the following, we choose the true labels y to obtain a
desired error e = y — y for each input data. In particular:
7
Published as a conference paper at ICLR 2020
T-,	1	_ Λ I I Λ	∙	PT-	111	.1	Δ ʌ /	∖	T
•	For each a ∈ AIJ A,we associate to Xa a label y& so that e&，^^a(w, V) - y& = 1.
•	For each b ∈ BIJ B,we associate to Xb a label yb so that eb，^^b(w, V) - yb = -1.
•	We choose the labels associated to XA+, XA- , XB+, and XB- such that eA+ = eB+ = 1
and eA- = eB- = -1.
This completes the description of the training set. As shown in in Soudry and Carmon (2016), there
exist weights that achieve zero loss if m > 4dn/(2d - 2)e. In our case, n = m + 2d - 2, and its easy
to check that m > 4(m + 2d - 2)/(2d - 2) for all d ≥ 4 and m ≥ 8 + 43/(d - 3). It follows
that in our setting the global optimum has zero loss, showing that the above (w, V) is sub-optimal.
Remark 2. In the above construction, the norms of inputs vectors may be very large. Ifwe scale
down the inputs such that ∣∣Xik ≤ 1/√m for i = 1,... ,n, and modify the corresponding labels
yi such that ei remains unchanged, then a same set of weights will still be a cupped minimum for
the landscape defined in terms of new (Xi, yi)’s. Moreover, for this setting, it is easy to check that
∣wr ∣ = 1, ∣ei ∣ = 1, and |yi | ≤ 2.
4.4 Proving the cupped minima property
Let
δ，min ʃ IwrZ^lj I WT Xi = 0, r = 1,..., m, i = 1,...,n].	(11)
∣Xi ∣
Then, δ > 0. For any θ ∈ Rmd with ∣θ∣ = 1 and for any t ∈ [0, δ], let
Fθ (t) = F(w + θt, V).	(12)
We show that there is an > 0 such that for any unit-norm θ and any t ∈ [0, δ],
Fθ (t) ≥ Fθ (0) + t2.	(13)
Lemma 2. For any θ ∈ Rmd with ∣∣θ∣ = 1, Fθ(∙) is a quadratic and COnvexfunCtiOn over [0, δ].
The proof is give in Appendix D and relies on the fact that neuron activations do not alter at w + θt
for t ∈ [0, δ]. Consider now the following m-dimensional subspace of Rmd
(Γ αιwι	]
I .	∣ αι,.. ., am ∈ R } .	(14)
αmwm	
For any θ ∈ Rmd, let θk and θ⊥ be the orthogonal projections of θ on Hw and Hw⊥ , respectively.
Then, θ = θk + θ⊥. In order to establish equation 13, we need lower bounds on Fθ0 (0) and Fθ00, which
we derive in the next two lemmas.
Lemma 3. There exists μ> 0 such that for any θ ∈ Rmd with ∣θ∣ = 1,we have
dF+Γ ∣t=0 ≥ 川叫.	(15)
The proof is given in Appendix E, and relies in a subtle way on the choice of data points in
subsection 4.3. We now bound the curvature of Fθ .
Lemma 4. There exist constants η1, η2 > 0 such that for any θ ∈ Rmd with ∣θ∣ = 1, and for any
t ∈ (0, δ),
d Ft2(t ≥ max (2η1∣∣θk∣∣2 - 2η2∣∣θ⊥∣∣, 0).	(16)
The proof is given in Appendix F, and relies on Property (p8) of the underlying configuration.
Consider now the second order polynomial p(x) = ηιδ(1 - x2) - η2δx - μx, where μ, m, and η
are the constants in Lemmas 3 and 4. Since p(0) > 0 and p(1) < 0, the polynomial p has exactly one
root in the interval (0, 1), which we denote by x0. Let
E , μxo∕δ.	(17)
8
Published as a conference paper at ICLR 2020
Lemma 5. For any θ ∈ Rmd with kθk = 1, and any t ∈ [0, δ], we have Fθ (t) ≥ Fθ (0) + t2.
This lemma is a simple consequence of Lemmas 3 and 4, and its proof is given in Appendix G. It
follows from Lemma 5 that for any w0 in the δ-neighborhood of w, we have F (w0, v) ≥ F (w, v) +
d∣w0 - w∣∣2. This shows that W is a cupped minimum for F(∙, v), and completes the proof of
Theorem 1.
5 Discussion
The guaranteed existence of descent paths in shallow networks of ReLU neurons was previously
established (Venturi et al., 2018), given an over-parameterization of m ≥ n (where m and n are
the number of neurons and the size of training data, respectively). This left an uncertainty gap of
2n/d < m < n, where zero empirical risk is known to be achievable (for m ≥ 2n/d) (Soudry and
Hoffer, 2017), but the existence of descent paths was in question. In this work, we have tightened this
uncertainty gap to n - 2d+ 2 < m < n, by proving that for any m ≤ n - 2d+ 2, there are input data
and initial weights for which a descent path does not exist. This conclusion we reach by establishing
the existence of cupped minima for m ≤ n - 2d + 2, and for the right choice of input data.
Compared to similar existing results for other activation functions, our results suggest that the edge
m ≈ n of over-parameterization required for elimination of sub-optimal cupped minima in ReLU
networks is much higher than that of networks with quadratic activation functions, m ≈ √2n (DU
and Lee, 2018), and linear activation functions, m ≈ n/d (Kawaguchi, 2016), and is almost as high
as general continuous activation functions of any form, m ≤ n (Venturi et al., 2018).
Non-existence of spurious local minima and the decent path property favor the convergence of decent
optimization methods like GD. However, for different variants of noisy GD, like SGD and Langevin
dynamics, it is quite common for the empirical loss to fluctuate during the training. Nevertheless, for
theoretical analysis purposes it usually helps to take the noise away, for example by tending the step
size to zero. The resulting GD, which always follows a descent path, is usually easier to analyze and
can also help to study the SGD dynamics. On the other hand, from a practical view, convergence of
SGD in local-min-free landscapes is well-studied.
Aside from addressing Conjecture 2, there remain several open problems, which we review next.
As an important direction for future research, it would be interesting if one could obtain bounds
on the probability of existence of cupped minima over random data sets, underneath the edge of
over-parameterization. In particular, we showed in Corollary 1 that this probability is non-zero,
however we gave no clue on either the size or scaling of this probability. Another class of problems
concerns basins of local minima, and how they affect dynamics of first order optimization algorithms.
As a step toward this goal, one might characterize the true over-parameterization regime in which the
basins of poor local minima have considerable volume.
Among other directions are extensions of our results to deep ReLU networks, shallow non-ReLU
networks, and shallow ReLU networks under loss functions more general than the squared loss.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Anna Choromanska, Yann LeCun, and Gerard Ben Arous. Open problem: The landscape of the loss
surfaces of multilayer networks. In Conference on Learning Theory, pages 1756-1760, 2015.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic
activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540, 2016.
9
Published as a conference paper at ICLR 2020
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 1724-1732. JMLR. org, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information
processing systems, pages 586-594, 2016.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on learning theory, pages 1246-1257, 2016.
Dawei Li, Tian Ding, and Ruoyu Sun. Over-parameterized deep neural networks have no strict local
minima for any continuous activations. arXiv preprint arXiv:1812.11039, 2018.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pages 2603-2612. JMLR.
org, 2017.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In
International Conference on Machine Learning, pages 774-782, 2016.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
arXiv preprint arXiv:1712.08968, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2019.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer
neural networks. arXiv preprint arXiv:1702.05777, 2017.
Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network
optimization landscapes. arXiv preprint arXiv:1802.06384, 2018.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. arXiv
preprint arXiv:1611.03131, 2016.
10
Published as a conference paper at ICLR 2020
Appendices
A	Different types of cupped minima in terms of differentiability
Here we discuss different types of cupped minima and provide elementary intuitions on the loss
landscape at cupped minima.
We first characterize curvature of the loss function at differentiable points. For r = 1, . . . , m, let Jr
be an n × n diagonal matrix whose (i, i) entry equals 1 wrT Xi ≥ 0 . Let G be an n × md matrix of
the form:
G = h VlJlXT I ∙∙∙ I VmJmXT i，	(18)
where X is a d × n matrix that has Xi in its i-th column. If F is differentiable at (w, v), its gradient
is VwF(w, V) = GT [eι,..., em,]T, where ei = y^i - yi is the output error for input X%. Moreover,
if F is differentiable at (w, v), its Hessian with respect to w equals
V2w F (w, V) = GTG.	(19)
We classify cupped minima into three categories in terms of differentiability. Specifically, for a
cupped minimum W of F(∙, v), we consider three cases:
Type 1) F is differentiable at w.
Type 2) F(w + θt, V) as a function oft is non-differentiable at t = 0, for all θ ∈ Rmd.
Type 3) There are θ1, θ2 ∈ Rmd such that F (w+θ1t, V) is differentiable att = 0, while F (w+θ2t, V)
is non-differentiable at t = 0.
Fig. 4 illustrates examples of loss surface at the above three types of cupped minima. We now argue
that the first two types are not possible in the loss landscape of shallow networks.
If F(∙, v) is differentiable at w, its Hessian given in equation 19 equals GTG. Since G is an n X md
matrix, assuming md > n, GT G would have zero eigenvalues. Therefore, w cannot be a cupped
minimum of F(∙, v). It follows that there exists no differentiable cupped minimum (nor saddle point)
if md > n.
For non-differentiable points, note that F(w + wt, v) as a function of t is a differentiable quadratic
function. This is because the output scales proportionally with w. Therefore, cupped minima of
the second type are not possible, as well. In the same spirit, it can be shown that F(w + θt, v) is
differentiable as a function of t, if θ ∈ Rmd belongs to the m dimensional subspace Hw defined in
equation 14.
For the above reasons, all cupped minima, if any, are of the third type. Therefore in the construction
of Subsection 4.3, we introduce a training set and a pair of weights (w, v) such that w is a cupped
minimum of F(∙,v), and F(W + θt,v) is differentiable in t only for θ ∈ Hw
B	Proof of Lemma 1
If v has a zero entry, Vr = 0 for some r, then F(∙,v) is a constant function with respect to Wr, and
thereby has no cupped minima. Therefore, we assume that v has no zero entries. Let
to = inf {t | ∃r, Vr (t) = 0}.
For t > 0, we define
∣vι(t)∕vι∣ × wι(t)
W(t) =	.	.	(20)
∣vm (t)/vm ∣ × Wm (t)
Let
tι = inf {t | W(t) = w}.
11
Published as a conference paper at ICLR 2020
Figure 4: Different types of cupped minima in terms of differentiability, discussed in Appendix A.
(C) Type 3
We show that tι < to. If to < ∞, then continuity of v(t) implies that Vr (to) = 0, for some r ≤ m.
Therefore, Wr (to) = 0 = Wr. It then follows from the continuity of Wr (∙) that there is an e > 0 such
that Wr (to - e) = wr. Consequently, tι < to.
The inequality tι < to implies that there is an e > 0 such that Wr (tι + e) = w, and for any t ∈
[0, t1 + e], we have sgn vr(t) = sgn(vr), r = 1, . . . , m. Therefore, for any t ∈ [0, t1 + e], we have
F(W(t), v) = F(W(t), v(t)). Since W is a cupped minimum of F(∙, v), there is an S ∈ [tι, tι + e]
such that F(W(t),v) > F(w, v). This shows that(W(t),v(t)) cannot be a descent path for F(∙, ∙),
and completes the proof of Lemma 1.
C Proof of Proposition 1
For i = 3, . . . , d, let
1
i — 1 + 2 cos(π∕t)
Fix a constant
dd
,Y(1 + Ci) = Y i
i=3	i=3
i + 2 cos(π∕t)
—1 + 2 cos(π∕t)
d + 2cos(π∕t)
2 + 2 cos(π∕t)
Let γ be a uniform random variable
C cos(π∕t) — cos(2π∕t)
Y 〜unif ---------------------
4c
cos(π∕t) — cos(2π∕t)
2C
(21)
(22)
(23)
c
We take a sample from the above distribution and fix a γ for the rest of the proof.
We now introduce the points in the configuration. For i = 0, . . . , t — 1, we consider points ai ∈ A
and bi ∈ B as follows
ai
bi
2π(i — 1∕2)	2π(i — 1∕2)
cos I ----1----- I , SinI-----1----- I , YC3, YC4, • • •, Ydcd
sin
, —Yc3, —Yc4, . . . , —Ydcd
τr^< ∙ c	7	• 1	∙ ~d 一 彳 F Td _ T^> i' 11
For i = 3,..., d,we consider points a ∈ A and bi ∈ B as follows
Gi
[0,..., 0, -1, ci+ι,..., cd],
i-1
~.
abi
[0,..., 0, 1, -ci+ι,..., -cd].
i-1
(24)
(25)
(26)
(27)
Fig. 5 shows an illustration of these points for d = 3.
We proceed by verifying Properties (p1)-(p8).
Property (p1): Property (p1) is straightforward from the above construction.
Property (p2): Considering only the first two coordinates, it is easy to see that no point of A is a
i ∙	i' ,ι	∙	Λ I I	Ai	, ,ι , i` ■ C	7 ~d ∙ ,ι	ι	∙
convex combination of other points in A A. Also note that for i = 3, . . . , d, aai is the only point
12
Published as a conference paper at ICLR 2020
I Y I
B	A
Figure 5: An illustration of the points in equation 24-equation 27 for d = 3 and t = 4. The red
♦ f .	.1	∙	.	∙ Λ I I	1,1	11	1	.	1 .	, 1	∙	,	∙	7~l I I 7~l
crosses indicate the points in AUA and the blue dots correspond to the points in BIJ B.
in A U A whose i-th entry is negative. Therefore, no point in A can be represented as a convex
combination of other points in A∣J A. A similar argument works for Part (B) of Property (p2).
Property (p3): Let
a ==，	i = 1,...,t,	(28)
γt
αi = (C3 + I)…(Ci-I + I)Ci,	i = 3,...,d.	(29)
A simple induction on j shows that
j-1
X ai = -j - 1.	(30)
i=3	Cj
for j = 4, . . ., d. We show that
gaiai + X &ia^i	= 0.	(31)
i	i=3 i
For the first two coordinates, equation 31 is easy. Let aj and Gj be the j-th entries of ai and ai,
respectively. For the j -th coordinate, j = 3, . . ., d, we then have
t-1	d
αi aij +	αGi aGij
αj
—
where the second equality is due to equation 30. This establishes equation 31. It then follows from
equation 31 that 0 is a convex combination of points in A A. Consequently, 0 ∈ int(PA). A similar
argument shows that 0 ∈ int(PB).
Properties (p4)-(p7): We only prove Part (B) for each of these properties; as similar proofs work also
for Part (A)’s. Moreover, because of the rotational symmetry of A and B in the first two coordinates,
it suffices to prove of the Properties (p4)-(p7) only for b0.
13
Published as a conference paper at ICLR 2020
Properties (p4) and (p5): For the j-th coordinate, j = 3, . . . , d, we have
d	j-1
aj + a1 + Ifa = Ycj + Ycj + YEcj - Y
i=3	i=3
=γ(Cj - I)Cj - 1)
=Y (j - 1)j - 1 + 2cos(π∕t) - 1)	(32)
-2γ cos(π∕t)
j - 1 + 2cos(π∕t)
= -(2cos(π∕t)) γcj
=2cos(π∕t) bj,
where bj0 is the j -th entry of b0 defined in equation 25. Similarly, for the first two coordinates, we
have:
d
al + al + γ ɪ2 ɑi = cos(∏∕t) +cos(∏∕t) = 2cos(π∕t) b0,
i=3
d
a2 + a1 + γ^X & = sin(π∕t) - sin(π∕t) = 0 = 2cos(π∕t) bj.
i=3
It then follows from equation 32 and equation 33 that
b0 = ʒ~~1Γ~U∖ I a0 + a1 + Y X ai ).
2 cosCπ∕t)	i=3
Let
B , 2cos(π∕t)
"(d - 2)y + 2.
Then, from equation 34,
d
a0 + a，+ Y): ai I .
i=3
Therefore, βb0 is a convex combination of a0, a1,a3, ...,ad, and therefore lies on the simplex S that
has a0,a1, a3,..., ad as its vertices. Next, We show that S is a facet of Pa.
Consider a vector z ∈ Rd with entries
=(C - 1)y + 1
21	Cθs(∏∕t) ,
z2 = 0,
d	(37)
zi = -	(cj + 1), i = 3, . . . , d - 1,
j=i+1
zd = -1.
Then, a simple backward induction, with base case i = d, shows that for i = 3, . . . , d, we have
Pjd=i+1 cjzj = zi + 1. In the same vein,
d
X Cjzj = -C+1.	(38)
j=3
It follows that for i = 3, . . . , d,
d
ZTai = X zj Cj - zi = 1.	(39)
j=i+1
(33)
(34)
(35)
(36)
βb0 = (d - 2)Y + 2 (
14
Published as a conference paper at ICLR 2020
Moreover, for i = 0, 1,
d
zTai =
ZjY ZjYcj + zι cos(π∕t)
j=3
=γ(-c+1) + Zi cos(π∕t)	(40)
=-Y(C- I) + (cc-s1Π/+ 1 COm)
= 1,
where the second equality is due to equation 38. Let H be the hyperplane that passes through
a0, a1,α3,..., ad. It follows from equation 39 and equation 40 that for any P ∈ {a0,a1, a3,..., ad},
we have ZTp = 1. Therefore, Z is orthogonal to H. Consequently,
H = {x ∈ Rd | ztX = 1}.	(41)
For i = 2, . . . , t - 1, we have
ZTai = zt (ai- a0) + ZTa(O
=ZTia- a0) + 1
=zi I cos(2π(i — 1∕2)∕t) — cos(π∕t)) + 1
< 1,
(42)
where the second equality is due to equation 40, and the inequality is because Z1 > 0 and cOs(2π(i -
1∕2)∕t) - cOs(π∕t) < 0. It follows from equation 41 and equation 42 that all points of A\{a0, a1}
lie on one side of H, while all points of AaS{a0, a1} lie on H. Then, H is a tangent hyperplane to
PA . Thus, the simplex S is a facet of PA . This completes the proofs of Properties (p4) and (p5).
Moreover, from the definition Hb in Property (p7), we have
Hb0 = H.	(43)
Property (p6): Since t ≥ 4, we have 2 cOs(π∕2) > 1. Moreover, recall from equation 23 that Y < 1.
Property (p6) then follows from equation 34 and the fact that S is a facet of PA .
Property (p7): For i = 0, . . . , t - 1,
ZTbi = ZT(bi-a0) + ZTa0
d
=zi I cos(2πi∕t) — cos(π∕t)) — 2 ɪ2 Zj Ycj + 1	(44)
j=3
=Zi (cos(2πi∕t) — cos(π∕t)) + 2γ(c — 1) + 1.
where the second equality is due to equation 40 and definitions of a0 and bi , and the third equality is
from equation 38. It follows that
ZT b0 = Zi(1 — cos(π∕t)) + 2γ(c — 1) + 1 > 1,	(45)
where the inequality is because the first two terms on the left hand side of the inequality are positive.
In the same vein, for i = 1, . . . , t — 1
zt bi = Zi (cos(2πi∕t) — cos(π∕t)) + 2γ(c — 1) + 1
< zi I cos(2π∕t) — cos(π∕t)) + 2γc + 1
< I cos(2π∕t) — cos(π∕t)) + 2γc + 1	(46)
≤ (cos(2π∕t) — cos(π∕t)) + (cos(π∕t) — cos(2π∕t)) + 1
= 1,
where the second inequality follows from the definition of Zi in equation 37 and the fact that
Zi > 1, and the third inequality is from the definition of Y in equation 23 and the fact that 2Yca ≤
15
Published as a conference paper at ICLR 2020
cos(π∕t) - cos(2π∕t). It then follows from equation 41, equation 45, and equation 46 that b0 and bi
lie on opposite sides of H, for i = 1, . . . , t - 1.
On the other hand, since bi = —ai, for i = 3,...,d, it follows from equation 39 that ZTbi =
-ZTai = -1 < 1. Therefore, for i = 1,..., 3, bi and b0 lie on opposite sides of H. This completes
the proof of Property (p7).
Property (p8): We have
d(b0, HbO) = ɪ(ZTb0 - 1)
kZk
zι(1 — eos(n/t)) + 2γ(c — 1)
(47)
kZk
k⅛ (a-Cos(S))+ Z12(C-1)),
where the first equality is due to equation 43 and equation 41, and the second equality follows from
the equality in equation 45. Similarly, from equation 42, we have for i = 0, . . . , t - 1
d(αi, Hb0 ) = η-ʊ- (zTai - 1)
kZk
=∣z1∣ ( cos (2π(i - 1/2)/t) - Cos(n/t)).
kZk
For i, j ∈ {0,...,t —1}, let rhai,b，cos (2π(i — j- 1/2)/t) — cos(π∕t) and rh心田
(48)
cos 2π(i -
j + 1/2)/t) — cos(π∕t). Then, it follows from equation 48 and rotational symmetry of A and B in
the first two coordinates that for i, j ∈ {0, . . . , t - 1},
d(ai, Hbj) = kZkmai,bj ,
d(bi, Haj) = z17m m bi,aj .
kZk ,
(49)
Note that for any p,q ∈ A U B, τ^p,q is a constant independent of the value of Y. Let M be a 2t X 2t
matrix, with entries
1 1 — eos(n/t), if P = q,
Mpq =〈 mp,q,
if p ∈ A and q ∈ B, OR p ∈ B and q ∈ A,
otherwise,
(50)
for p, q ∈ A U B. Then, all entries of M are constants independent of γ. Let λ1,..., λ2t be the
eigenvalues of m. It follows that λ1,..., λ2t are also constants independent of Y.
Consider the matrix M defined in equation 3. It follows from equation 47 and equation 49 that for
any p, q ∈ A U B,
/ k⅜ ((1 - cos(πλt)) + Z2(c - 1)) , if p = q,
Mpq = < ∕⅛mpq,	if p ∈ A and q ∈ B, OR P ∈ B and q ∈ A,
I kzk F,"
(0,	otherwise.
(51)
Consider the order a0, . . . , at-1, b0, . . . , bt-1 on the elements of A S B. Then,
M =黄(M + 工2(2 - 1)I)=黄(M + " I)CosT) I),
IlZIl ∖	zι	k IlZIl ∖ C- 1 + 1/Y	)
(52)
where I is the 2t × 2t identity matrix and the second equality is from the definition ofZ1 in equation 37.
Denote the eigenvalues of M by λ1 , . . . , λ2t . Then, from elementary linear algebra,
zι 八 2(2 - 1)cos(n/t)
λi =画 Ci + C - 1 + 1/Y
(53)
for i = 1, . . . , 2t. Therefore, there is at most one value of γ for which λi = 0. Then, in view of
equation 23, we have Pr(λi = 0) = 0, over the random choice of γ. Thus, with probability one, M
has no zero eigenvalues and is thereby full-rank. As an immediate consequence, M is full-rank for
suitable choice of γ. This establishes Property (p8) and completes the proof of Proposition 1.
16
Published as a conference paper at ICLR 2020
D Proof of Lemma 2
Consider a block representation of θ as follows
(54)
where each θr is a d-dimensional vector.
It follows from the definition of δ that for any t ∈ (0, δ), if wrT Xi > 0, then (wr + θrt)TXi > 0;
and if wrT Xi < 0, then (wr + θr t)T Xi < 0. Therefore, for r = 1, . . . , m and i = 1, . . . , n, and for
any t ∈ [0, δ) ,
l((Wr + θrt)TXi ≥ 0)= 1(WTXi > 0)+ 1(WTXi = 0, θTXi ≥ 0)
Consequently, for any t ∈ [0, δ] and i = 1, . . . , n, we have
m
(55)
yi (w + θt,v) = Evr (WT Xi + tθT Xi)l((wr + θr t)TXi ≥ 0)
r=1	(56)
m
=Xvr (WT Xi + tθT X,(1(WT Xi > 0)+ 1(WT Xi = 0, θT Xi ≥ 0)).
r=1
It follows that yi (W + θt,v) is a linear function of t over the interval t ∈ [0,δ]. Therefore, Fθ (t)=
pn=ι (yi(W + θt, v) - yi) is a quadratic and convex function of t, for t ∈ [0, δ].
E Proof of Lemma 3
We first characterize active neurons for different inputs. For two subsets S1, S2 ⊂ Rd we let
S1\S2 = S1 T S2c. Recall the definition of si(a) and si(b) from Property (p6).
Claim 1. For any a ∈ A, we have
WaT Xsi (a) = 0,	i	1,. ..,d- 1,		(57)
WaT Xa > 0,				(58)
WaT Xb > 0,	b	∈ (B U B) \ {sι (a),..	. , sd-1(a) ,	(59)
WaT XB+ > 0,				(60)
WaT XB- > 0,				(61)
WaT XA+ < 0,				(62)
WaT XA- < 0,				(63)
WaT Xa0 < 0,	a0	∈ (a U A) ∖{a}.		(64)
Similarly, for any b ∈ B, we have				
WbT Xsi (b) = 0,	i	1, . . . , d - 1,		(65)
WbT Xb > 0,				(66)
WbT Xa > 0,	a	∈ (A U A) \ {s1(b), ..	. , sd-1(b) ,	(67)
WbT XA+ > 0,				(68)
WbT XA- > 0,				(69)
WbT XB+ < 0,				(70)
WbT XB- < 0,				(71)
WbT Xb0 < 0,	b0	∈ (B U B)∖{b}.		(72)
17
Published as a conference paper at ICLR 2020
Proof of Claim 1. Fix a b ∈ B. We begin by introducing some notations. Let H be the (d - 2)-
dimensional hyperplane in the (d - 1)-dimensional space that passes through s1(b), . . . , sd-1(b). In
the same spirit, let H be the (d - 1)-dimensional subspace in the d-dimensional space that passes
through Xs1 (b) , . . . , Xsd-1(b), equivalently H is the subspace orthogonal to wb. We denote the
convex hull of X$i(b),..., Xsd-(b) by CB，Conv({Xsi(b),..., Xsd-i(b)}). Similarly, We let
Qa，conv({Xa | a ∈ AU A}) and QB，Conv({Xb | b ∈ B U B}).
Before presenting the proofs of properties equation 57-equation 72, we review make some easy
observations. Recall the definition of 0 from the paragraph proceeding equation 4. Let Bd-1 be the
intersection of the 0 -ball centered at 0 with the orthogonal space of ud . Then, from the definition of
, we have
Bd0-1 + ud ⊂ QA, Bd0-1 - ud ⊂ QB .	(73)
For x ∈ Rd, let π(x) be the projection of x on the span of first d - 1 coordinates, i.e., the orthogonal
space of ud. Then,
Iln ( Xa+ )∣∣	= UPa∈A S A(Xa - 1升
ξ +1∕β — n — 1 ξ +1∕β — n — 1
= Il Pa∈A U A aII
ξ +1∕β — n — 1
<	ll Pa∈AU A aII
ξ — n
<	U Pa∈A U A aII
II Pa∈A U A aII/e0
= 0,
(74)
where the first equality is from the definition of XA+ and the second inequality follows from the
definition of ξ in equation 4. Therefore, ∏(Xa+ )∕(ξ + 1∕β — n — 1)∈ Bd-1. Consequently,
1	X =	π(XA+)	+ (U Xa+) Ud
ξ + 1∕β — n — 1 A +	ξ + 1∕β — n — 1 ξ +1∕β — n — 1
∏(Xa+ )	(ξ + 1/β — n — 1)ud
ξ + 1∕β — n — 1 + ξ + 1∕β — n — 1
(75)
∏(Xa+)
ξ + 1∕β — n — 1
+ ud
∈ Bd0-1 + ud
∈ QA,
where the first equality is orthogonal decomposition of XA+, and last inclusion follows from equa-
tion 73. In the same vein, we can show that
—
1
ξ + n +1 — 1∕β
XB+ ∈ QA.
(76)
We proceed to verify equation 65-equation 72. Eq. equation 65 follows from equation 9. Recall the
definitions XA- = ξud and XB- = —ξu√. Then, equation 10 implies equation 69 and equation71.
Since X§i(b),..., Xsd_(b)define a boundary of the (d — 1)-dimensional convex set Qa, and HH
passes through Xs1(b), . . . , Xsd-1(b), then all points in QA lie on a same side of H. In other words,
either we have wbTx ≥ 0, for all x ∈ QA; or we have wbTx ≤ 0, for all x ∈ QA. In view of
Property (p3), ud ∈ int(QA). It then follows from equation 10 that for any x ∈ QA, we have
18
Published as a conference paper at ICLR 2020
WTX ≥ 0. Consequently, for any X ∈ Qa\H, We have WTX > 0. In particular,
WTXa > 0,	a ∈ (A [ A) \ {sι(b),..., Sd-i(b)},
WbT XA+ > 0,
-Wb XB + > 0,
where the first inequality is because Property (p5) implies that Xa ∈ HH for a ∈
(AU A) \ {sι(b),..., sd-ι(b)}, and the last two inequalities are due to equation 75 and equa-
tion 76, respectively. This establishes equation 67, equation 68, and equation 70.
For equation 66, it follows from Property (p4) that b and the origin, 0, lie on opposite sides of
hyperplane H. Consequently, -Xb and ud also lie on opposite sides of hyperplane H. Therefore,
WbT Xb and WbT ud have a same sing. It then follows from equation 10 that WbTXb> 0, and equation 66
follows.
For equation 72, it follows from Property (p7) that for any b0 ∈ B B with b0 6= b, Xb and Xb0 lie
on opposite sides of H. Eq. equation 66 then implies that WbT Xb0 < 0. This establishes equation 72,
and completes the proof of Claim 1.	□
In light of Claim 1, it is easy to see for r = 1, . . . , m and i = 1, . . . , n that if WrT Xi = 0, then
Vr ei = ;——	(77)
m
In our next claim, we examine a linear combination of data points for which a particular neuron is
active.
Claim 2. For r = 1, . . . , m, there exist constants γ1r , . . . , γmr such that
nn
X YreiXi 1(WTXi = 0)+ X eiXi 1(WTXi > 0)= 0.	(78)
i=1	i=1
Proof of Claim 2. Fix a b ∈ B . We prove the claim for the neuron associated to b. It follows from
Claim 1 that
n
eiXi1(WTXi > 0) = ebXb +	ɪ2	eaXa + eA+XA+ + eA-XA-
i=1	a∈(A S A)∖{sι(b),…,Sd-ι(b)}
=-Xb	+	X	Xa + (XA+ -	XA-)
a∈(A S A)∖{sι(b),…,Sd-ι (b)}
=-Xb	+	X	Xa — (X	Xa —	(1∕β — 1) Ud
a∈(A S A)∖{sι(b),…,Sd-ι (b)}	∖a∈A S A	,
d-1
= —Xb — X Xsi (b) + (1∕β — 1) ud,
i=1
(79)
where the second equality is due to the definitions of ea, eb, eA+, and eA- , and the third equality is
from the definitions of XA+ in equation 5.
On the other hand, it follows from Property (p6) that there exist scalars α1, . . . , αd ∈ (0, 1) such that
b = Pid=-11 αisi(b). Therefore, from the definition of Xb,
d-1
-(Xb + Ud) = ɪ2 ai (Xsi(b) — Ud)	(80)
i=1
19
Published as a conference paper at ICLR 2020
Moreover, Property (p4) implies that Pd-I a = 1∕β. Then, from equation 80,
d-1	d-1	d-1
-Xb	=	αiXsi(b)	-	αi	- 1 ud =	αiXsi(b)	- (1∕β - 1)ud.	(81)
i=1	i=1	i=1
For i = 1, . . . , d - 1, let γi = 1 - αi . Then, Claim 1 implies that
n Yi eiXi 1 WbT Xi i=1	d-1 0) =	Yi eiXsi (b) i=1 d-1 =	(1 - αi )Xsi (b) i=1	(82) d-1	d-1 =	Xsi(b) -	αiXsi(b) i=1	i=1 =X χsi(b) + Xb - (β - 1) ud,
where last equality in due to equation 81. Combing equation 79 and equation 82, we obtain equation 78
for wr = wb. A similar argument implies equation 78 for wr = wa, a ∈ A. This completes the proof
OfClaim2.	□
Er , min
i=1,...,d-1
Back to the proof of Lemma 3, for r = 1, . . . , m, let
min (Yr ,(1-Yr))),	(83)
for the constant γir defined in Claim 2. It follows that r > 0, for r = 1, . . . , m.
For any r ≤ m, Xs1(r), . . . , Xsd-1 (r) are linearly independent and, by definition, are all orthogonal
to wr. Therefore, there exists a constant 0r > 0 such that for any ζr ∈ Rd with kζr k = 1, we
have maxi=1,...,d-1 |ZTXsQ)∣ ≥ ErkZ⊥k, where Z⊥ is the projection of Zr on the null-space of Wr.
Consequently, for any ζr ∈ Rd with kζr k = 1,
max IZTXi| × 1(wTXi = 0) ≥ ErIlZ⊥k∙
i=1,...,n
In particular, considering the block-vector representation of θ in equation 54, we obtain for r =
1, . . . , m,
max ∣ΘTXi∣ × 1(WTXi = 0)≥ max er, ∣θ⊥∣.	(84)
i=1,...,n	i=1,...,n
20
Published as a conference paper at ICLR 2020
Let μ，minr Cr Er / √m. Then, μ > 0. It then follows from Claim 2 that, for r = 1,... ,m,
nn
Vr XeiθτXi 1 (wTXi = 0, θTXi ≥ 0)+ Vr X eiθTXi l(wTXi > 0)
i=1	i=1
nn
Vr X eiθTXil(wTXi = 0, θTXi ≥ 0) - Vr X YreiθTXil(wTXi = 0)
i=1	i=1
n
Vr X eiθTXi (l(θTXi ≥ 0) - Yr) 1(wTXi = 0)
i=1
n
X θTXi (l(θTXi ≥ 0) - Yr) 1(wTXi = 0)
i=1
1n
√m]ζ ∣θTXi I × ∣1(θTXi > 0) - Yr I × l(wTXi = 0)
(85)
1n
≥ √mer X ∣θrXi ∣ ι(wrXi = 0)
i=1
≥ ~√= max ∣θTXi ∣ l(wTXi = 0)
≥ √rr kθ⊥k
≥ μkθ⊥k,
where the third equality is due to equation 77, the fourth equality is because θrT Xi and 1(θrT Xi >
0) - Yir have always the same sign, the first inequality is by definition of Cr in equation 83, the third
inequality follows from equation 84, and the last inequality is from the definition of μ.
On the other hand, equation 2 implies that
dFθ⑴
d+t
t=0
mn
ltim X Vr θT X eiXil((Wr + θrt)TXi ≥ 0)
mn
= XVrXθTeiXi (l(wTXi = 0, θTXi ≥ 0) + l(wTXi > 0))
r=1	i=1
mn	n
=X	Vr X eiθTXil(wTXi	= 0, θTXi	≥	0)	+	Vr X	eiθTXil(wTXi	> 0)
r=1	i=1	i=1
m
≥ Xμ kθ⊥k
r=1
≥ μkθ⊥k
(86)
where the second equality is due to equation 55 and the first inequality follows from equation 85.
This completes the proof of Lemma 3.
F Proof of Lemma 4
We begin by a claim. Given a q ∈ A S B , recall the definition of hyperplane Hq from Property (p7).
Claim 3. For any pair of points p, q ∈ A B, we have
∣wT XpI = p1+d：0, Hq )2 d(P, Hq ).
(87)
21
Published as a conference paper at ICLR 2020
Proof of Claim 3. In the (d - 1)-dimensional space, let ω be the unit normal vector of Hq. Recall
from Property (p6) that s1(q), . . . , sd-1(q) are located on Hq. Let,
Y , ωτsι(q) = d(0, Hq).	(88)
Then,
d(p,Hq) = ∣ωτP - γ∣.	(89)
Without loss of generality suppose that q ∈ A. Let ω be the lifting of ω from the (d - 1)-dimensional
space to the d-dimensional space by appending ω by a new coordinate with zero coefficient, i.e., ω is
a d-dimensional vector with ω%
let
ωi, for i = 1,...,d - 1, and ωd = 0. For Y defined in equation 88,
(90)
Then, we have kzk = 1 and zTud < 0. Moreover, for i = 1, . . . , d - 1,
ZTXSi(q) =	/, ,	2 3TXSi(q) - YUTXSi(q))
1 + Y2
=p⅛彳 I-"'si® + YuTUd)	(91)
1
=pτ=^ (-Y + Y)
1 + Y2
0,
where the second equality is because si (q) ∈ B B for q ∈ A. It follows from equation 91 and the
definition of wq in equation 7 and equation 8 that wq = z . Therefore,
∣wqT Xp ∣
1
P1 + Y2
∣ωTP
- Y ∣
(92)
1
P1 + y2
√1+d(θ, Hq )2 "P, Hq ),
where the second equality is from the definition of z in equation 90, and the last equality is due to
equation 88 and equation 89. This completes the proof of Claim 3.	□
We now proceed to the proof of Lemma 4. Fix an arbitrary θ ∈ Rd with kθk = 1. Without loss
of generality4 assume that F(∙, ∙) is differentiable at (W + δθ∕2, V). For r = 1,...,m, let Jr be a
diagonal matrix whose (i, i) entry, for i = 1, . . . , n, equals
l((wr + θrt)TXi	≥	0)= I(WTXi	>	0)+	I(WTXi	= 0,	θTXi	≥	0),	(93)
where the equality is due to equation 55. Recall the definition of matrix G in equation 18:
G = h ViJiXt I ∙∙∙ I VmJmXT ].	(94)
Then, for any t ∈ (0, δ),
dɪ) = d2F(W+ tθ,v) = θτ VW+tθF(W + tθ,v) θ = θτGTGθ = kGθk2,	(95)
4This is because otherwise, (W + δθ∕2,v) is the limit point of a sequence of weights at which F is
differentiable. Since our arguments carry over to the weights in this sequence, we can conclude that all bounds
also apply to their limit point, (W + δθ∕2,v).
22
Published as a conference paper at ICLR 2020
where the third equality is due to equation 19. On the other hand, since θ = θk + θ⊥, we have
Gθ = Gθk + Gθ⊥.	(96)
In the following claim, we elaborate on kGθk k.
Claim 4. There exists a constant η1 > 0 such that kGθk k2 ≥ 2η1 kθk k2, for all θ ∈ Rd.
Proof of Claim 4. Recall that θk is the projection of θ on subspace Hw defined in equation 14. Then,
there exist constants α1 . . . , αm such that
α1w1
θk =	.	.
.
αm wm
Let α be the vector representation of α1, . . . , αm. Then,
mm
kθkk2=Xαi2kwik2=Xαi2 = kαk2.
i=1	i=1
Consider the n × m matrix
G = V V1J1XτWi ∙ ∙∙∙ I VmJmXTWm ].
Then, from the definition of matrix G in equation 18,
α1W1
Gθk = h ViJiXT I ∙∙∙ I VmJmXT i .
αmWm
=V ViJiXTWi I ∙∙∙ I VmJmXTWm ] α
=Gg a.
(97)
(98)
(99)
(100)
Each column of G corresponds to a neuron, and thereby is associated to a point in A B . In the
same vein, every row of G is associated to an input vector. By removing some rows of G, we devise
a matrix M so that each row of M is associated to an input Xp for p ∈ A B . Therefore, M is an
m X m matrix, whose rows and columns are associated to the points in A U B. It follows that MMα is
a vector obtained by removing some entries from vector Gα. As a result,
kMαk ≤ IlGai∣.	(101)
In the following, we capitalize on Property (p8) to show that MM is full-rank.
For q ∈ A U B , let
Yei,	, Vq .
√1 + d(0, Hq)
For p,q ∈ A U B, the entry in row P and column q of MM equals
=VqXTWq (I(WTXp > θ) + I(WTXP = 0, θTXP ≥ θ))
=VqXPWq 1(Wq^Xp > 0)	(102)
=	/] Jqn Z q d(p, Hq)1(WTXP > 0)
1 + d(0, Hq)
= γq d p, Hq 1 WqT XP > 0 .
where the first equality is from the definition of MM, the second equality follows from the definitions
of G and Jq in equation 99 and equation 93, the third equality is because XTWq 1 (WTXP = 0)= 0,
and the fourth equality is due to Claim 3. Then, Claim 1 implies that for any p, q ∈ A U B,
Mf	— ∫Yq d(P, Hq),
MPq =	0,
if p = q, OR p ∈ A and q ∈ B, OR p ∈ B and q ∈ A,
otherwise.
(103)
23
Published as a conference paper at ICLR 2020
f-Λ	1 .	. ∙ TI ʃ 1 r∙ λ ∙	. ∙	~>	1	1	Γ∙ τι~r	ι .ι	ι	/` τι r
Compared to matrix M defined in equation 3, each column q of M equals the column q of M
multiplied by a non-zero constant γq . In view of Property (p8), M is full-rank. It follows that M is
full-rank, as well.
Let σ be the smallest singular value of M . Since M is full-rank, we have σ > 0. Moreover,
.... .. ..
IIMak ≥ σ∣∣αk∙	(104)
Then,
kGθkk2 = kGαk2 ≥ ∣MMɑ∣2 ≥ σ2∣ɑ∣2 = σ2∣θkk2,	(105)
where the equations are due to equation 100, equation 101, equation 104, and equation 98, respectively.
Claim 4 then follows for ηι = σ2∕2.	□
Back to the proof of Lemma 4, we denote by σmax the largest singular value of G. Let η2 , σm2 ax .
Then,
IGθI2 = Gθk + Gθ⊥2
=∣∣Gθk∣∣2 + ∣∣Gθ⊥∣∣2 + 2(Gθk)T (Gθ⊥)
≥ Gθk 2 - 2Gθk × Gθ⊥
≥ ∣∣Gθk∣∣2 - 2σmax IθkI × σmax Iθ⊥I	(106)
≥ ∣∣Gθk∣∣2 -2σm2axIθ⊥I
= ∣∣Gθk ∣∣2 - 2η2 Iθ⊥ I
≥ 2η1∣∣θk∣∣2 - 2η2Iθ⊥I,
where the second inequality is from the definition of σmax , the third inequality is because Iθk I ≤
IθI = 1, the last equality is by the definition of η2, and the last inequality follows from Claim 4.
This completes the proof of Lemma 4.
G Proof of Lemma 5
Fix a θ ∈ Rmd with IθI = 1. Recall the definition of x0 from the paragraph proceeding equation 17.
If Iθ⊥ I ≥ x0, then for any t ∈ [0, δ],
Fθ (t) - Fθ (0) ≥ Fθ0 (0)t
≥ μkθ⊥kt
≥ μxot
= δt
≥ t2,
where the first inequality is from convexity of Fθ in Lemma 2, the second inequality is due to
Lemma 3, the equality is by the definition of in equation 17, and the last inequality is because t ≤ δ.
On the other hand, if Iθ⊥ I < x0, then for any t ∈ [0, δ],
Fθ (t)- Fθ (0) = Fθ(0)t + 1 Fθ012
≥ 1 Fθ012
≥ η1 Iθk I2 - η2 ∣∣θ⊥ ∣∣t2
=(ηι(1 - kθ⊥k2) - η2∣∣θ⊥∣∣)t2
< (ηι(i — χ2) — η2χ0)t2
μχ0 2
=丁t
= t2,
24
Published as a conference paper at ICLR 2020
where the first equality is because Fθ is quadratic (c.f. Lemma 2), the first inequality follows from
Lemma 3, the second inequality is due to Lemma 4, the third inequality is because kθ⊥ k < x0,
and the last two equalities are due to p(x0) = 0 and the definition of in equation 17, respectively.
Combining the above two cases, we obtain Lemma 5.
25