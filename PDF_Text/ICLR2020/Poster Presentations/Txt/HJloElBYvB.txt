Published as a conference paper at ICLR 2020
Phase Transitions for the Information B ottle-
NECK IN REPRESENTATION LEARNING
Tailin Wu
Stanford
tailin@cs.stanford.edu
Ian Fischer
Google Research
iansf@google.com
Ab stract
In the Information Bottleneck (IB), when tuning the relative strength between
compression and prediction terms, how do the two terms behave, and what’s their
relationship with the dataset and the learned representation? In this paper, we
set out to answer these questions by studying multiple phase transitions in the IB
objective: IBe [p(z∣χ)] = I (X; Z )-βI (Y; Z) defined on the encoding distribution
p(z|x) for input X, target Y and representation Z, where sudden jumps of dI(Y;Z)
and prediction accuracy are observed with increasing β . We introduce a definition
for IB phase transitions as a qualitative change of the IB loss landscape, and show
that the transitions correspond to the onset of learning new classes. Using second-
order calculus of variations, we derive a formula that provides a practical condition
for IB phase transitions, and draw its connection with the Fisher information
matrix for parameterized models. We provide two perspectives to understand
the formula, revealing that each IB phase transition is finding a component of
maximum (nonlinear) correlation between X and Y orthogonal to the learned
representation, in close analogy with canonical-correlation analysis (CCA) in linear
settings. Based on the theory, we present an algorithm for discovering phase
transition points. Finally, we verify that our theory and algorithm accurately predict
phase transitions in categorical datasets, predict the onset of learning new classes
and class difficulty in MNIST, and predict prominent phase transitions in CIFAR10.
1 Introduction
The Information Bottleneck (IB) objective (Tishby et al., 2000):
IBe[p(z∣x)] := I(X; Z) - βI(Y; Z)
(1)
explicitly trades off model compression (I(X; Z), I(∙; ∙) denoting mutual information) with pre-
dictive performance (I(Y; Z)) using the Lagrange multiplier β, where X, Y are observed random
variables, and Z is a learned representation of X . The IB method has proved effective in a variety of
scenarios, including improving the robustness against adversarial attacks (Alemi et al., 2016; Fischer,
2018), learning invariant and disentangled representations (Achille & Soatto, 2018a;b), underlying
information-based geometric clustering (Strouse & Schwab, 2017b), improving the training and
performance in adversarial learning (Peng et al., 2018), and facilitating skill discovery (Sharma et al.,
2019) and learning goal-conditioned policy (Goyal et al., 2019) in reinforcement learning.
From Eq. (1) we see that when β → 0 it will encourage I(X; Z) = 0 which leads to a trivial
representation Z that is independent of X, while when β → +∞, it reduces to a maximum likelihood
objective1 that does not constrain the information flow. Between these two extremes, how will the IB
objective behave? Will prediction and compression performance change smoothly, or do there exist
interesting transitions in between? In Wu et al. (2019), the authors observe and study the learnability
transition, i.e. the β value such that the IB objective transitions from a trivial global minimum to
learning a nontrivial representation. They also show how this first phase transition relates to the
structure of the dataset. However, to answer the full question, we need to consider the full range of β .
1For example, in classification, it reduces to cross-entropy loss.
1
Published as a conference paper at ICLR 2020
Figure 1: CIFAR10 plots (a) showing the information plane, as well as β vs (b) I(X; Z) and I(Y ; Z),
and (c) accuracy, all on the training set with 20% label noise. The arrows point to empirically-observed
phase transitions. The vertical lines correspond to phase transitions found with Alg. 1.
Motivation. To get a sense of how I(Y ; Z) and I(X; Z) vary with β, we train Variational Infor-
mation Bottleneck (VIB) models (Alemi et al., 2016) on the CIFAR10 dataset (Krizhevsky & Hinton,
2009), where each experiment is at a different β and random initialization of the model. Fig. 1 shows
the I(X; Z), I(Y ; Z) and accuracy vs. β, as well as I(Y ; Z) vs. I(X; Z) for CIFAR10 with 20%
label noise (see Appendix I for details).
From Fig. 1(b)(c), we see that as we increase β, instead of going up smoothly, both I(X; Z) and
I(Y; Z) show multiple phase transitions, where the slopes d(X* and dlI(Y* are discontinuous
and the accuracy has discrete jumps. The observation lets us refine our question: When do the
phase transitions occur, and how do they depend on the structure of the dataset? These questions are
important, since answering them will help us gain a better understanding of the IB objective and its
close interplay with the dataset and the learned representation.
Moreover, the IB objective belongs to a general form of two-term trade-offs in many machine learning
objectives: L = Prediction-loss + β ∙ Complexity, where the complexity term generally takes the
form of regularization. Usually, learning is set at a specific β . Many more insights can be gained if
we understand the behavior of the prediction loss and model complexity with varying β, and how
they depend on the dataset. The techniques developed to address the question in the IB setting may
also help us understand the two-term tradeoff in other learning objectives.
Contributions. In this work, we begin to address the above question in IB settings. Specifically:
•	We identify a qualitative change of the IB loss landscape w.r.t. p(z|x) for varying β as IB
phase transitions (Section 3).
•	Based on the definition, we introduce a quantity G[p(z|x)] and use it to prove a theorem
giving a practical condition for IB phase transitions. We further reveal the connection
between G[p(z|x)] and the Fisher information matrix when p(z|x) is parameterized by θ
(Section 3).
•	We reveal the close interplay between the IB objective, the dataset and the learned represen-
tation, by showing that in IB, each phase transition corresponds to learning a new nonlinear
component of maximum correlation between X and Y, orthogonal to the previously-learned
Z, and each with decreasing strength (Section 4).
To the best of our knowledge, our work provides the first theoretical formula to address IB phase
transitions in the most general setting. In addition, we present an algorithm for iteratively finding the
IB phase transition points (Section 5). We show that our theory and algorithm give tight matches with
the observed phase transitions in categorical datasets, predict the onset of learning new classes and
class difficulty in MNIST, and predict prominent transitions in CIFAR10 experiments (Section 6).
2	Related Work
The Information Bottleneck Method (Tishby et al., 2000) provides a tabular method based on the
Blahut-Arimoto (BA) Algorithm (Blahut, 1972) to numerically solve the IB functional for the optimal
encoder distribution P (Z|X), given the trade-off parameter β and the cardinality of the representation
2
Published as a conference paper at ICLR 2020
variable Z. This work has been extended in a variety of directions, including to the case where all
three variables X, Y, Z are multivariate Gaussians (Chechik et al., 2005), cases of variational bounds
on the IB and related functionals for amortized learning (Alemi et al., 2016; Achille & Soatto, 2018a;
Fischer, 2018), and a more generalized interpretation of the constraint on model complexity as a
Kolmogorov Structure Function (Achille et al., 2018). Previous theoretical analyses ofIB include Rey
& Roth (2012), which looks at IB through the lens of copula functions, and Shamir et al. (2010),
which starts to tackle the question of how to bound generalization with IB. We will make practical
use of the original IB algorithm, as well as the amortized bounds of the Variational Informormation
Bottleneck (Alemi et al., 2016) and the Conditional Entropy Bottleneck (Fischer, 2018).
Phase transitions, where key quantities change discontinuously with varying relative strength in the
two-term trade-off, have been observed in many different learning domains, for multiple learning ob-
jectives. In Rezende & Viola (2018), the authors observe phase transitions in the latent representation
of β-VAE for varying β. Strouse & Schwab (2017b) utilize the kink angle of the phase transitions
in the Deterministic Information Bottleneck (DIB) (Strouse & Schwab, 2017a) to determine the
optimal number of clusters for geometric clustering. Tegmark & Wu (2019) explicitly considers
critical points in binary classification tasks using a discrete information bottleneck with a non-convex
Pareto-optimal frontier. In Achille & Soatto (2018a), the authors observe a transition on the tradeoff
of I(θ; X, Y ) vs. H(Y |X, θ) in InfoDropout. Under IB settings, Chechik et al. (2005) study the
Gaussian Information Bottleneck, and analytically solve the critical values βi = ι-λ, Where λi are
eigenvalues of the matrix Σχ∣y Σ-1, and Σχ is the covariance matrix. This work provides valuable
insights for IB, but is limited to the special case that X, Y and Z are jointly Gaussian. Phase
transitions in the general IB setting have also been observed, which Tishby (2018) describes as
“information bifurcation”. In Wu et al. (2019), the authors study the first phase transition, i.e. the
learnability phase transition, and provide insights on how the learnability depends on the dataset.
Our work is the first work that addresses all the IB phase transitions in the most general setting,
and provides theoretical insights on the interplay between the IB objective, its phase transitions, the
dataset, and the learned representation.
3	Formula for IB phase transitions
3.1	Definitions
Let X ∈ X , Y ∈ Y , Z ∈ Z be random variables denoting the input, target and representation,
respectively, having a joint probability distribution p(X, Y, Z), with X × Y × Z its support. X, Y
and Z satisfy the Markov chain Z - X - Y , i.e. Y and Z are conditionally independent given X. We
assume that the integral (or summing if X , Y or Z are discrete random variables) is on X × Y × Z .
We use x, y and z to denote the instances of the respective random variables. The above settings are
used throughout the paper. We can view the IB objective IBe[p(z∣χ)] (Eq. 1) as a functional of the
encoding distributionp(z|x). To prepare for the introduction of IB phase transitions, we first define
relative perturbation function and second variation, as follows.
Definition 1. Relative perturbation function: For p(z|x), its relative perturbation function r(z|x)
is a boundedfUnction that maps X XZ to R and satisfies Ez〜p(z∣χ) [r(z∣x)] = 0. Formally, define
Qz∣x := {r(z∣x) : X × Z → Rl Ez〜p(z∣χ) [r(z∣x)] = 0, and ∃M > 0 s.t. ∀X ∈ X, Z ∈
Z, |r(z |x)| ≤ M}. We have that r(z|x) ∈ QZ|X iff r(z|x) is a relative perturbation function of
p(z∣x). The perturbed probability (density) is p0(z∣x) = p(z∣x) (1 + E ∙ r(z∣x)) for some E > 0.
Definition 2. Second variation: Let functional F [f (x)] be defined on some normed linear space R.
Let US add a PertUrbativefUnction E ∙ h(x) to f (x), and now thefunctional F[f (x) + E ∙ h(x)] can be
expanded as
∆F[f (x)] = F[f (x) + E ∙ h(x)] - F[f (x)]
=夕i[e ∙ h(x)] + 22[e ∙ h(x)] + 夕r[E ∙ h(x)] ∣∣E ∙ h(x)∣∣2
such that lim 夕/e ∙ h(x)] = 0, where ∣∣∙k denotes the norm,夕i[e ∙ h(x)] = EdF[f(X)] is a linear
functional of E∙h(x), and is called the first variation, denoted as δF [f (x)].夕 2 [E∙h(x)] = 1 e2 d Ff2(x)]
is a quadratic functional of e ∙ h(x), and is called the second variation, denoted as δ2F[f (x)].
3
Published as a conference paper at ICLR 2020
We can think of the perturbation function E ∙ h(χ) as an infinite-dimensional “vector” (X being the
indices), with being its amplitude and h(x) its direction. With the above preparations, we define the
IB phase transition as a change in the local curvature on the global minimum of IBe[p(z∣x)].
Definition 3. IB phase transitions: Let r(z|x) ∈ QZ|X be a perturbation function of p(z|x),
Pβ (z|x) denote the optimal solution of IB β [p(z∣x)] at β, where the IB functional IB [∙] is defined in
Eq. (1). The IB phase transitions βic are the β values satisfying the following two conditions:
(1)	∀r (ZIx) ∈ QZIX, δ2IBβ [p(ZIx)]∣然(ZIx) ≥ 0;
(2)
lim inf	δ2IBβo[p(z∣x)]∣ *, ∣、
β→β+ r(z∣x)∈Qz∣x	β L L 川pβ(ZIx)
0-.
Here β+ and 0- denote one-sided limits.
We can understand the δ2IBβ[p(ZIx)] as a local “curvature” of the IB objective IBβ (Eq. 1) w.r.t.
p(ZIx), along some relative perturbation r(ZIx). A phase transition occurs when the convexity
of IBβ[p(ZIx)] w.r.t. p(ZIx) changes from a minimum to a saddle point in the neighborhood of
its optimal solution pe(z∣x) as β increases from βc to βc + 0+. This means that there exists a
perturbation to go downhill and find a better minimum. We validate this definition empirically below.
3.2	Condition for IB phase transitions
The definition for IB phase transition (Definition 3) indicates the important role δ2IBβ[p(ZIx)] plays
on the optimal solution in providing the condition for phase transitions. To concretize it and prepare
for a more practical condition for IB phase transitions, We expand IBe[p(z∣x)(1 + 〜r(z∣x))] to the
second order of E, giving:
Lemma 0.1. For IBe [p(Z Ix)], the condition of ∀r(ZIx) ∈ QZIX, δ2IBe [p(ZIx)] ≥ 0 is equivalent
to β ≤ G[p(ZIx)]. The threshold function G[p(ZIx)] is given by:
G[p(ZIx)] :
G[r(ZIx);p(ZIx)] :
inf	G[r(ZIx); p(ZIx)]
r(z∣x)∈Qz∣χ
Ex,z〜p(x,z) Ir? (ZIx)] - EZ〜p(z) [(Ex〜p(x∣z)[r(ZIx)])]
(2)
Ey,z〜p(y,z) [(Ex〜p(x|y,z)[r(ZIx)]) - - EZ〜p(z) [(Ex〜p(x∣z)[r(ZIx)])]
The proof is given in Appendix B, in which we also give Eq. (20) for empirical estimation. Note that
Lemma 0.1 is very general and can be applied to any p(z∣x), not only at the optimal solution pe(z∣x).
The Fisher Information matrix. In practice, the encoderpθ(ZIx) is usually parameterized by some
parameter vector θ = (θ1, θ2, ...θk)T ∈ Θ, e.g. Weights and biases in a neural net, Where Θ is the
parameter field. An infinitesimal change of θ0 - θ + ∆θ induces a relative perturbation L r(Z〔x) `
∆θτ dlogpθ(zlx) onpθ (ZIx), from Which We can compute the threshold function GΘ [pθ (ZIx)]:
Lemma 0.2. For IBe [pθ (ZIx)] objective, the condition of ∀∆θ ∈ Θ, δ2IBe[pθ (ZIx)] ≥ 0 is equiva-
lent to β ≤ GΘ [pθ (Z Ix)], where i * * * * * * * *
GΘ[pθ (ZIx)]
i f ∆θτ (Zz∣χ⑻-TZ(θ))∆θ = λ-ι
∆θ∈ΘΔθτ(lz∣γ(θ)-Iz(θ)) ∆θ = max
(3)
where IZ(θ) := R dZpθ(Z) (dlo¾θ(Z))(。"霏(Z)) is the Fisher information matrix
of θ for Z, Izix(θ) := RdxdZp(x)pθ(ZIx)(辿g>)(FZIx)[ Iz∣γ(θ)：=
R dydZp(y)pθ(Z1y) (dlogp^Z|y)) (dogpθ(Z^)) are the conditional Fisher information matrix
(Zegers, 2015) ofθ for Z conditioned on X and Y, respectively. λmax is the largest eigenvalue
ofC-1 IZIY (θ) - IZ (θ) (CT)-1 with vmax the corresponding eigenvector, where CCT is the
Cholesky decomposition of the matrixIZIX(θ) - IZ (θ), and vmax is the eigenvector for λmax. The
infimum is attained at ∆θ = (CT)-1vmax.
4
Published as a conference paper at ICLR 2020
The proof is in appendix C. We see that for parameterized encoders pθ(z|x), each term of G[p(z∣χ)]
in Eq. (2) can be replaced by a bilinear form with the Fisher information matrix of the respective
variables. Although this lemma is not required to understand the more general setting of Lemma 0.1,
where the model is described in a functional space, Lemma 0.2 helps understand G[p(z|x)] for
parameterized models, which permits directly linking the phase transitions to the model’s parameters.
Phase Transitions. Now we introduce Theorem 1 that gives a concrete and practical condition for
IB phase transitions, which is the core result of the paper:
Theorem 1. The IB phase transition points {βic} as defined in Definition 3 are given by the roots of
the following equation:
G[pβ (ZIx)] = β	(4)
where G[p(z∣x)] is given by Eq. (2) andpe(z|x) is the optimal solution ofIBβ [p(z∣x)] at β.
We can understand Eq. (4) as the condition when δ2IBβ [p(z|x)] is about to be able to be negative
at the optimal solution pe(z∣x) for a given β. The proof for Theorem 1 is given in Appendix D. In
Section 4, we will analyze Theorem 1 in detail.
4	Understanding the formula for IB phase transitions
In this section we set out to understand G[p(z|x)] as given by Eq. (2) and the phase transition
condition as given by Theorem 1, from the perspectives of Jensen’s inequality and representational
maximum correlation.
4.1	Jensen’ s Inequality
The condition for IB phase transitions given by Theorem 1 involves G[p(z|x)]	=
inf	G[r(z|x);p(z|x)] which is in itself an optimization problem. We can understand
r(ZIx)∈QZ∣X
G[p(z∣x)] =	inf	AB--C in Eq. (2) using Jensen,s inequality:
r(ZIx)∈QZ∣X —
Ex,Z〜p(x,z) [r (ZIx)]
'--------{z---------}
≥ Ey,z~p(y,z)
[(Ex~p(x∣y,z) [r(ZIx)D ]
______ - /
≥ EZ〜p(z) [ (Ex〜p(x| z) [r(ZIx)])]
X-------------------------------
C
(5)
A
B
z
The equality between A and B holds when the perturbation r(ZIx) is constant w.r.t. x for any Z; the
equality between B and C holds when Ex〜p(x∣y,z) [r(z∣x)] is constant w.r.t. y for any z. Therefore,
the minimization of BA--C encourages the relative perturbation function r(z∣x) to be as constant
w.r.t. x as possible (minimizing intra-class difference), but as different w.r.t. different y as possible
(maximizing inter-class difference), resulting in a clustering of the values of r(ZIx) for different
examples x according to their class y. Because of this clustering property in classification problems,
we conjecture that there are at most IY I - 1 phase transitions, where IY I is the number of classes,
with each phase transition differentiating one or more classes.
4.2 Representational Maximum Correlation
Under certain conditions we can further simplify G[p(ZIx)] and gain a deeper understanding of it.
Firstly, inspired by maximum correlation (Anantharam et al., 2013), we introduce two new concepts,
representational maximum correlation and conditional maximum correlation, as follows.
Definition 4. Given a joint distribution p(X, Y ), anda representation Z satisfying the Markov chain
Z - X - Y, the representational maximum correlation ρr (X, Y ; Z) is defined as
Pr(X, Y； Z)= SUp	Ex,y,z^p(x,y,z)[f (x, z)g(y, z)]	⑹
(f(x,Z),g(y,Z))∈S1
where Si = {(f : X × Z → R,g : Y × Z → R) ∣ f,g bounded, and Ex〜p(x∣z)[f (x,z)]=
Ey 〜p(y∣z)[g3,Z)] = 0, Ex,z 〜p(x,z)[f2 (X,z)] = Ey,z^p(y,z)[g2 (y,z)] = 1} ∙
5
Published as a conference paper at ICLR 2020
The conditional maximum correlation ρm (X, Y |Z) is defined as:
Pm(XiY IZ)=	sup	Ex,y 〜ρ(x,y∣z)[f (X)g(y)]	⑺
(f (x),g(y))∈S2
where S2 = {(f : X → R,g : Y → R) ∣ f,g bounded, and∀z ∈ Z : Ex〜p(χ∣z)[f (x)]=
Ey 〜p(y∣z)[g(y)] = 0，Ex 〜p(x∣z)[f2 (X)] = Ey 〜p(y∣z)[g2 (y)] = 1} -
We prove the following Theorem 2, which expresses G[p(z|x)] in terms of representational maximum
correlation and related quantities, with proof given in Appendix F.
Theorem 2. Define Q(Z0|)X := {r(z |X) : X × Z → R ∣∣ r bounded}. If Q(Z0|)X and QZ|X satisfy:
∀r(z∣x) ∈ QZOI)X ,there exists2 rι(z∣x) ∈ Qz∣χ, s(z) ∈ {s(z) : Z → R | S bounded} s.t. r(z∣x)=
r1 (z |X) + s(z)， then we have:
(i)	The representation maximum correlation and G:
G[p(z|x)] = PI(XYTZ)	⑻
(ii)	The representational maximum correlation and conditional maximum correlation:
Pr(X,Y;Z)= sup [Pm(X, Y |Z)]	(9)
Z∈Z
(iii)	When Z is continuous, an optimal relative perturbation function r(z|X) for G[p(z|X)] is
given by
*/ I、7 *Z ʌ ∕δ(z — z*)
r (z|x)=h (XN	P(Z)
(10)
where z* = arg max Pm(X, Y|Z = z), and h*(X) is the optimal solution for the learn-
z∈Z
ability threshold function h* (X) = arg min	β0[h(X)] with P(X, Y|Z = z*)
h(x)∈{h:X→R ∣ h bounded}
(β0[h(X)] is given in Theorem 4 of Wu et al. (2019)).
(iv) For discrete X, Y and Z, we have
Pr (X, Y; Z) = maxσ2(Z)
Z∈Z
(11)
where σ2(Z) is the second largest singular value of the matrix QX,Y |Z
( Lp(X,y|Z) ʌ =( pχ,y)	/p(ZIx) ʌ
卜 √p(XIZ)P(y|Z)) x y 卜 √p(X)P(y) V P(ZIy) Jxy'
Theorem 2 furthers our understanding of G[P(z|X)] and the phase transition condition (Theorem 1),
which we elaborate as follows.
Discovering maximum correlation in the orthogonal space of a learned representation: Intu-
itively, the representational maximum correlation measures the maximum linear correlation between
f(X, Z) and g(Y, Z) among all real-valued functions f, g, under the constraint that f(X, Z) is
“orthogonal” to P(X |Z) and g(Y, Z) is “orthogonal” toP(Y|Z). Theorem 2 (i) reveals that G[P(z|X)]
is the inverse square of this representational maximum correlation. Theorem 2 (ii) further shows
that G[P(z|X)] is finding a specific z* on which maximum (nonlinear) correlation between X and Y
2For discrete X, Z such that the cardinality |Z| ≥ |X |, this is generally true since in this scenario, h(x, z)
and s(z) have |X ||Z| + |Z| unknown variables, but the condition has only |X ||Z| + |X | linear equations.
The difference between QZ|X and Q(Z0|)X is that Q(Z0|)X does not have the requirement of Ep(z|x) [r(z|x)] = 0.
Combined with Lemma 2.2, this condition allows us to replace r(z|x) ∈ QZ|X by r(z|x) ∈ Q(Z0|)X in Eq. (2).
6
Published as a conference paper at ICLR 2020
conditioned on Z can be found. Combined with Theorem 1, we have that when we continuously in-
crease β, for the optimal representation Ze given by pe (z|x) at β, Pr (X,Y; Ze) shall monotonically
decrease due to that X and Y has to find their maximum correlation on the orthogonal space of an
increasingly better representation Zβe that captures more information about X . A phase transition
occurs when ρr(X,Y; Ze) reduces to √1β, after which as β continues to increase, ρr(X,Y; Ze)
will try to find maximum correlation between X and Y orthogonal to the full previously learned
representation. This is reminiscent of canonical-correlation analysis (CCA) (Hotelling, 1992) in
linear settings, where components with decreasing linear maximum correlation that are orthogonal to
previous components are found one by one. In comparison, we show that in IB, each phase transition
corresponds to learning a new nonlinear component of maximum correlation between X and Y in Z,
orthogonal to the previously-learned Z. In the case of classification where different classes may have
different difficulty (e.g. due to label noise or support overlap), we should expect that classes that are
less difficult as measured by a larger maximum correlation between X and Y are learned earlier.
Conspicuous subset conditioned on a single z: Furthermore, we show in (iii) that an optimal
relative perturbation function r(z|x) can be decomposed into a product of two
factors, a 产-F
factor that only focus on perturbing a specific point ze in the representation space, and an he(x)
factor that is finding the “conspicuous subset” (Wu et al., 2019), i.e. the most confident, large, typical,
and imbalanced subset in the X space for the distribution (X, Y)〜p(X, Y∣ze).
Singular values In categorical settings, (iv) reveals a connection between G[p(z|x)] and the
singular value of the QX,Y |Z matrix. Due to the property of SVD, we know that the square of the
singular values of QX,Y |Z equals the non-negative eigenvalue of the matrix QTX,Y |Z QX,Y |Z. Then
the phase transition condition in Theorem 1 is equivalent to a (nonlinear) eigenvalue problem. This
is resonant with previous analogy with CCA in linear settings, and is also reminiscent of the linear
eigenvalue problem in Gaussian IB (Chechik et al., 2005).
5	Algorithm for phase transitions discovery in classification
As a consequence of the theoretical analysis above, we are able to derive an algorithm to efficiently
estimate the phase transitions for a given model architecture and dataset. This algorithm also permits
us to empirically confirm some of our theoretical results in Section 6.
Typically, classification involves high-dimensional inputs X. Without sweeping the full range of β
where at each β it is a full learning problem, it is in general a difficult task to estimate the phase
transitions. In Algorithm 1, we present a two-stage approach.
In the first stage, we train a single maximum likelihood neural network fθ with the same encoder
architecture as in the (variational) IB to estimate p(y|x), and obtain an N × C matrix p(y|x), where
N is the number of examples in the dataset and C is the number of classes. In the second stage, we
perform an iterative algorithm w.r.t. G and β, alternatively, to converge to a phase transition point.
Specifically, for a given β, we use a Blahut-Arimoto type IB algorithm (Tishby et al., 2000) to
efficiently reach IB optimalpee(z|x) at β, then use SVD (with the formula given in Theorem 2 (iv))
to efficiently estimate G[pee(z|x)] at β (step 8). We then use the G[pee(z|x)] value as the new β and
do it again (step 7 in the next iteration). At convergence, we will reach the phase transition point
given by G[pee(z|x)] = β (Theorem 1). After convergence as measured by patience parameter K, we
slightly increase β by δ (step 13), so that the algorithm can discover the subsequent phase transitions.
6	Empirical study
We quantitatively and qualitatively test the ability of our theory and Algorithm 1 to provide good
predictions for IB phase transitions. We first verify them in fully categorical settings, where X, Y, Z
are all discrete, and we show that the phase transitions can correspond to learning new classes as we
increase β. We then test our algorithm on versions of the MNIST and CIFAR10 datasets with added
label noise.
7
Published as a conference paper at ICLR 2020
Algorithm 1 Phase transitions discovery for IB
Require (X, Y ): the dataset
Require fθ: a neural net with the same encoder architecture as the (variational) IB
Require K : patience
Require δ: precision floor
Require R: maximum ratio between β(th) and β.
// First stage: fitp(y|x) using neural net fθ:
1： p(y∣x) J fitting (X, Y) using fθ via maximum likelihood.
2： P(X) J N
// Second stage: coordinate descent using G[p(z|x)] and IB algorithm:
3： β0c J β(th) (1)
4： B J {β0c} //B is a set collecting the phase transition points
5： (β(new), β, count) J (β0c, 1,0)
(new)
6: while β-g- < R do:
7： β J β(new)
8:	β(new) J β(th) (β)
9:	if β(new) - β < δ do:
10:	count J count + 1
11:	if count > K do:
12:	B J B∪ {β(new)}
13:	β(new) J β(new) + δ
14:	end if
15:	else: count J 0
16:	end if
17:	end while
18:	return B
subroutine β(th) (β):
s1:	ComputePe(z∣x) using the IB algorithm (Tishby et al., 2000).
s2:	β(new) J G[pβ(z|x)] using SVD (Eq. 8 and 11).
s3:	return β(new)
6.1	Categorical dataset
For categorical datasets, X and Y are discrete, andP(X) and P(Y|X) are given. To test Theorem
1, we use the Blahut-Arimoto IB algorithm to compute the optimal Pe (z|x) for each β. I (Y; Z *)
vs. β is plotted in Fig. 2 (a). There are two phase transitions at β0c and β1c. For each β and the
corresponding Pe(z|x), we use the SVD formula (Theorem 2) to compute G[pβ(z|x)], shown in
Fig. 2 (b). We see that G[pβ(z|x)] = β at exactly the observed phase transition points βC and βf.
Moreover, starting at β = 1, Alg. 1 converges to each phase transition points within few iterations.
Our other experiments with random categorical datasets show similarly tight matches.
Furthermore, in Appendix G we show that the phase transitions correspond to the onset of separation
of P(z|x) for subsets of X that correspond to different classes. This supports our conjecture from
Section 4.1 that there are at most |Y| - 1 phase transitions in classification problems.
6.2	MNIST dataset
For continuous X , how does our algorithm perform, and will it reveal aspects of the dataset? We first
test our algorithm in a 4-class MNIST with noisy labels3, whose confusion matrix and experimental
settings are given in Appendix H. Fig. 3 (a) shows the path Alg. 1 takes. We see again that in each
3We use 4 classes since it is simpler than the full 10 classes, but still potentially possesses phase transitions.
We use noisy label to mimic realistic settings where the data may be noisy and also to have controllable difficulty
for different classes.
8
Published as a conference paper at ICLR 2020
(a)	(b)
Figure 2: (a) I(Y; Z*) vs. β for a categorical dataset with |X| = |Y| = |Z| = 3, where Z* is given
by p*β(z|x), and the vertical lines are the experimentally discovered phase transition points β0 and βf.
(b) G[pβ(z∖x)] vs. β for the same dataset, and the path for Alg. 1, with β0 and βf in (a) also plotted.
The dataset is given in Fig. 5.
Figure 3: (a) Path of Alg. 1 starting with β = 1, where the maximum likelihood model fθ is using
the same encoder architecture as in the CEB model. This stairstep path shows that Alg. 1 is able to
ignore very large regions of β, while quickly and precisely finding the phase transition points. Also
plotted is an accumulation of G[p*β(z∖x)] vs. β by running Alg. 1 with varying starting β (blue dots).
(b) Per-class accuracy vs. β, where the accuracy at each β is from training an independent CEB
model on the dataset. The per-class accuracy denotes the fraction of correctly predicted labels by the
CEB model for the observed label y.
phase Alg. 1 converges to the phase transition points within a few iterations, and it discovers in
total 3 phase transition points. Similar to the categorical case, we expect that each phase transition
corresponds to the onset of learning a new class, and that the last class is much harder to learn due to
a larger separation of β. Therefore, this class should have a much larger label noise so that it is hard
to capture this component of maximum correlation between X and Y, as analyzed in representational
maximum correlation (Section 4.2). Fig. 3 (b) plots the per-class accuracy with increasing β for
running the Conditional Entropy Bottleneck (Fischer, 2018) (another variational bound on IB). We
see that the first two predicted phase transition points β0c , β1c closely match the observed onset of
learning class 3 and class 0. Class 1 is observed to learn earlier than expected, possibly due to the gap
between the variational IB objective and the true IB objective in continuous settings. By looking at
the confusion matrix for the label noise (Fig. 7), we see that the ordering of onset of learning: class 2,
3, 0, 1, corresponds exactly to the decreasing diagonal element p(y = 1∖y = 1) (increasing noise) of
the classes, and as predicted, class 1 has a much smaller diagonal element p(y = 1∖y = 1) than the
other three classes, which makes it much more difficult to learn. This ordering of classes by difficulty
is what our representational maximum correlation predicts.
9
Published as a conference paper at ICLR 2020
(a)	(b)	(c)
Figure 4: (a) Accumulated G[pβ(z|x)] vs. β by running Alg. 1 with varying starting β (blue dots).
Also plotted are predicted phase transition points. (b) I(X; Z) and I(Y ; Z) vs. β. The manually-
identified phase transition points are labelled with arrows. The vertical black lines are the phase
transitions identified by Alg. 1, denoted as β0c, β1c,... β8c, from left to right. (c) Accuracy vs. β with
the same sets of points identified. The most interesting region is right before β = 2, where accuracy
decreases with β . Alg. 1 identifies both sides of that region, as well as points at or near all of the early
obvious phase transitions. It also seems to miss later transitions, possibly due to the gap between the
variational IB objective and the true IB objective in continuous settings.
6.3	CIFAR 1 0 DATASET
Finally, we investigate the CIFAR10 experiment from Section 1. The details of the experimental setup
are described in Appendix I. This experiment stretches the current limits of our discrete approximation
to the underlying continuous representation being learned by the models. Nevertheless, we can see in
Fig. 4 that many of the visible empirical phase transitions are tightly identified by Alg. 1. Particularly,
the onset of learning is predicted quite accurately; the large interval between the predicted β3 = 1.21
and β4 = 1.61 corresponds well to the continuous increase of I(X; Z) and I(Y ; Z) at the same
interval. And Alg. 1 is able to identify many dense transitions not obviously seen by just looking at
I(Y ; Z) vs. β curve alone. Alg. 1 predicts 9 phase transitions, exactly equal to |Y| - 1 for CIFAR10.
7	Conclusion
In this work, we observe and study the phase transitions in IB as we vary β . We introduce the
definition for IB phase transitions, and based on it derive a formula that gives a practical condition for
IB phase transitions. We further understand the formula via Jensen’s inequality and representational
maximum correlation. We reveal the close interplay between the IB objective, the dataset and
the learned representation, as each phase transition is learning a nonlinear maximum correlation
component in the orthogonal space of the learned representation. We present an algorithm for
finding the phase transitions, and show that it gives tight matches with observed phase transitions
in categorical datasets, predicts onset of learning new classes and class difficulty in MNIST, and
predicts prominent transitions in CIFAR10 experiments. This work is a first theoretical step towards
a deeper understanding of the phenomenon of phase transitions in the Information Bottleneck. We
believe our approach will be applicable to other “trade-off” objectives, like β-VAE (Higgins et al.,
2017) and InfoDropout (Achille & Soatto, 2018a), where the model’s ability to predict is balanced
against a measure of complexity.
8	Acknowledgements
The authors would like to thank Alex Alemi, Kevin Murphy, Sergey Ioffe, Isaac Chuang and Max
Tegmark for helpful discussions.
10
Published as a conference paper at ICLR 2020
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep
representations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018a.
Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations
through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence,
2018b.
Alessandro Achille, Glen Mbeng, and Stefano Soatto. The dynamics of differential learning i:
Information-dynamics and task reachability. arXiv preprint arXiv:1810.02440, 2018.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Venkat Anantharam, Amin Gohari, Sudeep Kamath, and Chandra Nair. On maximal correlation,
hypercontractivity, and the data processing inequality studied by erkip and cover. arXiv preprint
arXiv:1304.6133, 2013.
Richard Blahut. Computation of channel capacity and rate-distortion functions. IEEE transactions
on Information Theory, 18(4):460-473, 1972.
Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for gaussian
variables. Journal of machine learning research, 6(Jan):165-188, 2005.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.
Ian Fischer. The conditional entropy bottleneck, 2018. URL openreview.net/forum?id=
rkVOXhAqY7.
Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew Botvinick, Hugo Larochelle,
Sergey Levine, and Yoshua Bengio. Infobot: Transfer and exploration via the information
bottleneck. arXiv preprint arXiv:1901.10902, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017. URL https://openreview.net/forum?id=Sy2fzU9gl.
Harold Hotelling. Relations between two sets of variates. In Breakthroughs in statistics, pp. 162-190.
Springer, 1992.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015. URL https://arxiv.org/abs/1412.
6980.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, CIFAR, 2009.
Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational
discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining
information flow. arXiv preprint arXiv:1810.00821, 2018.
Melanie Rey and Volker Roth. Meta-gaussian information bottleneck. In Advances in Neural
Information Processing Systems, pp. 1916-1924, 2012.
Danilo Jimenez Rezende and Fabio Viola. Taming VAEs. arXiv preprint arXiv:1810.00597, 2018.
11
Published as a conference paper at ICLR 2020
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. Theoretical Computer Science, 411(29-30):2696-2711, 2010.
Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019.
DJ Strouse and David J Schwab. The deterministic information bottleneck. Neural computation, 29
(6):1611-1630, 2017a.
DJ Strouse and David J Schwab. The information bottleneck and geometric clustering. arXiv preprint
arXiv:1712.09657, 2017b.
Max Tegmark and Tailin Wu. Pareto-optimal data compression for binary classification tasks. arXiv
preprint arXiv:1908.08961, 2019.
Naftali Tishby. Lecture: the information theory of deep neural networks: the sta-
tistical physics aspects. https://www.perimeterinstitute.ca/videos/
information-theory-deep-neural-networks-statistical-physics-aspects/,
2018.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Tailin Wu, Ian Fischer, Isaac Chuang, and Max Tegmark. Learnability for the information bottleneck.
arXiv preprint arXiv:1907.07331, 2019.
S. Zagoruyko and N. Komodakis. Wide Residual Networks. arXiv: 1605.07146, 2016.
Pablo Zegers. Fisher information properties. Entropy, 17(7):4918-4939, 2015.
12
Published as a conference paper at ICLR 2020
Appendix
A CALCULUS OF VARIATIONS AT ANY ORDER OF IBβ [p(z|x)]
Here we prove the Lemma 2.1, which will be crucial in the lemmas and theorems in this paper that
follows.
Lemma 2.1. Fora relative perturbation function r(z|x) ∈ QZ|X for a p(z|x), where r(z|x) satisfies
Ez〜p(z∣x) [r(z∣x)] = 0, we have that the IB objective can be expanded as
IBe[p(z∣x)(1 + 〜r(z∣x))]
=IBe [p(ZIx)] + E ∙ I Ex,z〜p(χ,z)
r(ZIx)log
-β ∙ Ey,z~p(y,z)
r(ZIy)log *一
+ XX ⅛1nEn {(E[rn(Z∣x)] - E[rn(Z)]) - β ∙ (E[rn(Z∣y)] - EL(z)])}
n=2n(n-1)
=IBe [p(ZIx)] + E ∙ I Ex,z〜p(χ,z)
r(ZIx)log *一
-β ∙ Ey,z~p(y,z)
r(ZIy)log *一
E2
+ L {(E[r2(z∣x)] - E[r2(z)]) - β ∙ (E[r2(z∣y)] - E[r2(z)])}
1	∙ 2
E3
—『{(E[r3(z∣x)] - E[r3(z)]) - β ∙ (E[r3(z∣y)] - E[r3(z)])}
2 ∙ 3
E4
+ τ-7 {(E[r4(z∣x)] - E[r4(z)]) - β ∙ (E[r4(z∣y)] - E[r4(z)])}
3 ∙ 4
—...
(12)
where r(z∣y) = Ex〜p(x∣y,z) [r(z∣x)] andr(z) = Ex〜p(x∣z) [r(z∣x)]. The expectations in the equations
are all w.r.t. all variables. For example E[r2(z∣x)] = Ex,z〜p(x,z)[r2(z∣x)].
Proof. Suppose that we perform a relative perturbation r(ZIx) on P(ZIx) such that the perturbed
conditional probability is p0(z∣x) = p(z∣x) (1 + E ∙ r(Z∣x)), then We have
PO(Z)=Z p(x)p0(Z1x)dx=/ dxp(x)p(Z|x)(i+e ∙(附=P(Z)+E ∙/ dxp(X)P(Z1x)r(ZIx)
Therefore, we can denote the corresponding relative perturbation r(z) on p(z) as
p(z)
r(z) ≡ IpO(Z)- P(Z)
dxp(x)p(z∣x)r(z∣x) = Ex 〜p(χ∣z)[r(z∣x)]
Similarly, we have
0	p0(y, Z)
P(ZIy)=W
dxp(x, y)p(z∣x) (1 + E ∙ r(z∣x)) = p(z∣y) + E ∙
dxp(x, y )p(z |x)r(z |x)

And we can denote the corresponding relative perturbation r(z|y) on p(z|y) as
r
1 PO(ZIy) - P(ZIy)
(ZIy) ≡ -
P(ZIy)
P(ZIy1)P(y) / dxP(x，y)P(ZIx)r(ZIx) = Ex〜p(x|y,z)[r(ZIx)]
E
Since
IBe [p(zIx)] = I (X ； Z) — β ∙ I (Y ； Z) = / dxdZP(x, Z)lθg PF	— β ∙ / dydZP(y, Z)log PFy)
P(Z)	P(Z)
13
Published as a conference paper at ICLR 2020
We have
IBe[p0(ζ∣x)] = IBe[p(z∣x)(1 + 〜r(ζ∣x))]
=Z dxdζp(x)p0(ζ∣x)logP (ZIX) — β ∙ Z dydzp(y)p,(z∣y)logP 尸?
J	P0(z)	J	P0(z)
=Z dxdζp(x)p(ζ∣x)(1 + E ∙ r(ζ∣x))logP(Z∣x)(1 + 〜r(z∣X))
J	P(Z)(I + e ∙ r(Z))
―β ∙ [ dydzn(y)n(ζ∣")(1 + e ∙ r(z∣y))loaP(ZIy)(I + E . r(ZIy))
β	ayU‹Zp(y)P(ZIy	+ E / (ZIy))!og
J	p(z)(1 + E∙r(Z))
/
P(Z ∣x)
dxdZp(x)p(Z∣x)(1 + e ∙ r(Z∣x)) log————+ log (1 + e ∙ r(Z∣x)) — log (1 + e ∙ r(Z))
P(Z)
-β •/
.................. ..一	P(ZIy)	,	..一	,	‘一
dydZp(y)p(Z∣y)(1 + e ∙ r(Z∣y)) log + log (1 + e ∙ r(Z∣y)) — log (1 + e ∙ r(Z))
P(Z)
/
dxdZp(x)p(Z ∣ x)(1 + e ∙ r(Z∣ x)) logP(Z ∣ X) +	( —1)n-1 — (r(Z∣ x) — r(Z))
p(Z )	n
n=1
-β •/
dydZp(y)p(Z∣y)(1 + e ∙ r(Z∣y)) logPPZZy) + X (T)n-1 n EWy'	r''"
_	、 / n=1	.
The 0th-order term is simply IBe [p(z∣x)]. The first order term is
δIBβ [p(z∣x)] =E ∙ ( Ex,z〜p(x,z)
r(ZIx)log *
—β . Ey,z~p(y,z)
r(ZIy)log ⅛f一
The nth-order term for n ≥ 2 is
δnIBe [p(z∣x)]
=(—1)nEn / dxdZp(x)p(Z∣x) (—, [rn(Z∣x) — rn(Z)] + r(Z∣x)—^―- Γrn-1(Z∣x) — rn
n	n—1
—β ∙ ( —1)nEn d dydZp(y)p(Z∣y) (—1 [rn(Z∣y) — rn(Z)] + r(Z∣y)^- IrnT(Z∣y) — rn
n	n—1
(—1)nen
n(n — 1)
(Ex,z〜p(x,z)[r (z∣x)] - nEx,z〜p(x,z) [r(z∣x)r	(z)] - (n - I)Ez〜p(z)[r (z)])
—β ∙ ::—1) (Ey,z〜P(y,z)[rn(ZI y)] — nEy,z〜p(y,z)[r(ZI y)rn-1(Z)] — (n — I)Ez〜p(z)[rn(Z)D
(—1)nEn
(--⅛ ((E[rn(Z Ix)] — E[rn(Z)]) — β ∙ (E[rn(Z∣ y)] — E[rn(Z)])}
n(n — 1)
In the last equality we have used
Eχ,z〜p(χ,z)[r(z∣x)rnT(z)] = Ez^p(z) [rn-1(z)Ex^p(x∣z) [r(z∣x)]] = Ez〜「⑶[rn-1(z)r(z)] = Ez〜「⑶[rn(z)]
Combining the terms with all orders, we have
IBe[p(z∣x)(1 + J r(z∣x))]
=IBe[P(ZIx)] + e ∙	Ex,z〜p(x,z)
r(ZIx)log ⅛f 一
—β . Ey,z~p(y,z)
r(ZIy)log 展一
∞
+ X
n=2
(—1)nen
n(n — 1)
{(E[rn(z∣x)] — E[rn(z)]) — β ∙ (E[rn(z∣y)] — E[rn(z)])}
□
14
Published as a conference paper at ICLR 2020
As a side note, the KL-divergence betweenp0(z∖x) = p(z∣x)(1 + e ∙ r(z∣x)) andp(z∖x) is
KL (pz(z∣χ)∣∣p(z∣χ))
d dzp(z∣x)(1 + 〜r(z∣x))log P(HX)(I+：「(HX))
J	p(z∣x)
J dzp(z∣x)(1 + e ∙ r(z∣x)) (e ∙ r(z∣x) — ɪ ∙ r2(z∣x)) + O(e3)
e ∙ J dzp(z∣x)r(z∣x) + ɪ J dzp(z∣x)r2(z∣x) + O(e3)
e2
ɪ Ez 〜p(z∣x)r (Z∣x)]+ O(E3 )
Therefore, to the second order, we have
e2
Ex〜P(X) [KL(PO(WX)IIP(Z∣x))] = 2E因r2(Z⑶]
(13)
Similarly, we have Ex〜p(x)[KL (P(ZIX)∣∣p0(ZIX))]= 彳E[r2(z∣x)] up to the second order. Using
similar procedure, we have up to the second-order,
Ey〜p(y)[KL(PO(ZIy)∣∣P(ZIy))] = Ey〜p(y)[KL(P(ZIy)∣∣p0(ZIy))] = %E[r2(Z∣y)]
f2
KL(PO(Z)∣∣P(Z))=KL(P(Z)∣∣p0(Z)) = - E[M(z)]
B Proof of Lemma 0.1
Proof. From Lemma 2.1, we have
62
δ2IBβ[p(Z∣x)] = — {(E[r2(z∣x)] — E[M(z)]) — β ∙ (E[r2(Z∣y)] — E[r2(Z)])}	(14)
The condition of
∀r(z∣x) ∈ QZIX, δ2IBβ[p(z∣x)] ≥ 0	(15)
is equivalent to
∀r(z∣x) ∈ Qz∣χ,β ∙ (E[r2(z∣y)] — E[r2(z)]) ≤ E[r2(z∣x)] — E[r2(z)]	(16)
Using Jensen,s inequality and the convexity of the square function, we have
E[r2 (z∣y)] = Ey/〜p(y,z) [(Ex〜p(x|y/)[r(ZIx)])2]
Ez 〜p(z)
[Ey ~p(y∣z)
[(Ex~p(x|y,z) Ir(ZIX)])]]
≥ Ez〜p(z) [(Ey〜p(y∣z) [Ex〜p(x∣y,z) [r(z∣x)][)]
=Ez〜p(z) [(Ex〜p(x∣z)[r(z∣x)])2]
= E[r2 (Z)]
The equality holds iff r(z∣y) = Ex〜p(x∣y,z) [r(z∣x)] is constant w.r.t. y, for any z.
Using Jensen,s inequality on E[r2(z)], we have E[r2(z)]
Ez〜p(z) [(Ex〜p(x∣z) [r(z∣x)]) ] ≤
Ez〜p(z) [Ex〜p(x∣z)[r2(ζ∣x)]] = E[r2(z∣x)], where the equality holds iff r(z∣x) is constant w.r.t. X
for any ζ.
15
Published as a conference paper at ICLR 2020
When E[r2(z∣y)] - E[r2(z)] > 0, We have that the condition Eq. (16) is equivalent to ∀r(z∣x) ∈
Q β ≤ E[r2(ZIx)]-E[r2(Z)] ie
QZIX, β ≤ E[r2(z∣y)]-E[r2(z)] ,i.e.
≤ G[p(z|x)]
E[r2 (z|x)] - E[r2 (z)]
≡ inf --------------------------------
r(z∣x)∈Qz∣x E[r2(z∣y)]- E[r2(z)]
(17)
where r(z∣y) = Ex〜p(x∣y,z)[r(z∣x)] and r(z) = Ex〜p(x∣z) [r(z∣x)].
If E[r2 (z|y)] - E[r2 (z)] = 0, substituting into Eq. (16), We have
β ∙ 0 ≤ E[r2(z∣x)] — E[r2(z)]	(18)
which is always true due to that E[r2(z|x)] ≥ E[r2(z)], and will be a looser condition than Eq. (17)
above. Above all, we have Eq. (17).
□
Empirical estimate of G[p(z|x)] To empirically estimate G[p(z|x)] from a minibatch of
{(xi, yi)}, i = 1, 2, ...N and the encoder p(z|x), we can make the following Monte Carlo im-
portance sampling estimation, where we use the samples {xj}〜p(χ) and also get samples of
{zi}〜p(z) = p(x)p(z∣x), and have:
Ex,z〜p(x,z) [r2(z∣x)] =J dxdzp(x)p(z)
p(x, z)
p(x)p(z )
r2 (z |x)
〜ɪXXXX P(Xj,Zi) M(z∙Ix∙)
'N2 = j= P(Xj)p(Zi)	(i| j)
EZ〜P(Z)Ir (Z)] =EZ〜p(z) [(Ex〜p(x∣z) Ir(ZIx)])]
2
2
1 I ɪ	P(ZiIxj)	/ I、
N s IN jδ NPNKQ r"ixj
2
16
Published as a conference paper at ICLR 2020
Ey,z〜p(y,z)[r (ZIy)] =Ey,z〜p(y,z) [(Ex〜p(x∣y,z)[r(ZIx)])]
1
N
dxp(xIyi, Zi)r(Zi Ix)
1N
N X
i=1
1N
N x
i=1
1N
N x
i=1
1N
N x
i=1
二 ∕dxp(yi)p(XIyi)P(ZiIx)r(ZiIx))
R dxp(yi)p(χ∣yi)p(zi∣χ)r(zi∣χ) )2
R dχp(yi)p(χ∣yi)p(zi∣χ)	J
Pxj ∈Ωχ(yi) P(Zi |xj )r(ZiIxj ) !2
PXj∈Ωχ(yi) P(Zi lxj )	)
PN=I P(ZiIxj )r(zi|xj )1 [yi = yj ] A
PN=IP(ZiIxj )1 [yi = yj ]	J
Here Ωχ(y%) denotes the set of x examples that has label of yi, and 1[∙] is an indicator function that
takes value 1 if its argument is true, 0 otherwise.
The requirement of Ez〜p(z∣χ) [r(Z〔x)] = 0 yields
0=Ez 〜p(z∣χ)卜(Z⑶]=Z 由(Z) PPZZx) r(ZIx) ` N X PpiZxj) r(ZiIxj)	(19)
for any xj .
Z-X 1 ∙	∙	11 .	1	. 1	..1	♦	♦	1 A「 / I ∖ 1 ∙	♦	F
Combining all terms, we have that the empirical G[P(ZIx)] is given by
人- , . .,
G[p(zIx)]
inf
r(z∣x)∈Qz∣χ
1 PN PN	P(Xj,Zi) 22( .I .) _ PN
N 2=1=1 2j=ι p(xj)p(zi)r (ZiIXj) - 2=1=1
P Pj=I P(zi|xj)r(zi|xj)
1-PN=ι p(zi∣χj)-
2
P^N ( PN=I p(zi | xj )r(zi |xj )l[yi = yj])
乙i=1 I-PN=I p(zi|Xj)B[yi=yj]-)
(20)
where {zi}〜P(Z) and {xi}〜p(x). It is also possible to use different distributions for importance
sampling, which will results in different formulas for empirical estimation of G[P(ZIx)].
C	GΘ[pθ(zIx)] FOR PARAMETERIZED DISTRIBUTION pθ(zIx)
Proof. For the parameterized4 pθ(zIx) with θ ∈ Θ, after θ0 J θ + ∆θ, where5 ∆θ ∈ Θ is an
infinitesimal perturbation on θ, we have that the distribution changes fromPθ(ZIx) to Pθ+∆θ(ZIx),
4In this paper, θ =	(θ1,θ2,...θk)τ and	d叼zlx)	=	(dpθ(z|x),	dpθ(z|x),...	dpθ(z|x)	)T are all column
,	1 , 2 , ... k	∂θ	∂θ1	,	∂θ2	, ...	∂θk
vectors. "lpθZ|x) is a k × k matrix with (i, j) element of djpθ(ZIx).
.	∂θ2	,	∂θi ∂θj .
5Note that since Θ is a field, it is closed under subtraction, we have ∆θ ∈ Θ.
17
Published as a conference paper at ICLR 2020
and thus the relative perturbation on pθ (z|x) is
E∙r(z∣x)
Pθ+∆θ(z∣x) - Pθ(z∣x)
Pθ (z∣x)
=^^ (pθ(z∣χ) + ∆θτdpθ^x- + 1∆θτ呼2P)(ZIX)∆θ + O(k∆θk3) -Pθ(HX))
Pθ(z∣x) ∖	∂θ 2	∂θ2	J
'∆θτ ɪ logP) (z∣x) + 1∆θτ -ɪ d⅞⅛ ∆θ + O(∣∣∆θ∣∣3)
∂θ	2	pθ (z∣x)	∂θ2
where ∣∣∆θ∣∣ is the norm of ∆θ in the parameter field Θ.
Similarly, we have
〜r(z∣y) = ∆θT∂θlogPθ(z∣y) + ∣∆θτ京讨d⅛匝∆θ + O(∣∣∆θ∣∣3)
〜T(Z) = ∆θT∂θlogPe(Z) + WT京冬舁海 + O(k△即3)
Substituting the above expressions into the expansion of IBe [p(z∣x)] in Eq. (12), and preserving to
the second order ∣∣∆θ∣∣2, We have
IBe[P)(z∣x)(1 + 〜r(z∣x))]
=IBe [P) (z∣x)] + 〜Ex,z~Pθ (x,z)
r(ZIX)IOgPP(ZZX) - β ∙ Ey/〜Pθ(y,z) r(z∣y)logPP：；Zy))
e2
+ E { (Ex,z~pθ(x,z)[r (Z∣x)] - EZ~pθ(z)[r (z)]) - β ∙ (Ey,z~pθ(y,z)[r (Wy)] - EZ~p® (z)[r (z)])}
1 ∙ 2
=IBe[pθ(Z⑶]+ Ex,z〜pθ(x,z) (^θr焉logpθ(z∣χ) + 14严	(∖)a p)(ZIX)^θ) logpθ(ZIX)
v 7 ∖	∂ θ	2	pe (z∣x)	∂θ2	P	pe (z)
-β	Ey,z~pθ(y,z)	∂1 (aθ ∂θlogpθ(ZIy)+ 2	∆θT 1 Pe (Z∣y)	—∆θ)log * ]	
1 + 2	(Ex,z~pe (x,z)	](m ∂θ log pθ (ZIX))	-EZ〜Pθ (z)	](∆θT ∂θ log Pθ (Z))2	)
β —— 2	(Ey,z~pθ (y,z)	]"T ∂θ log pe (ZIy))	-EZ〜Pθ(z)	](∆θT 亲 log Pθ (Z)[	)
一「	/ , ×-,	ACT/L	L Pθ(z∣x) ∂ “	/ I、ICL	L	Pθ(z∣x) ∂ “	/ I、
=IBe[pθ(ZIX)] +	4铲1Ex,z〜pθ(x,z)卜g	Pe(Z)	∂θlogPθ(Z∣x)]	- β ∙	Ex,z〜pθ(x,z)	^log	Pe(Z)	而logPθ(ZIX)
+ ^aθ, {(iz∣x (θ) - ZZ(θ)) - β (Zz∣x (θ) - ZZ(θ))} ∆θ
In the last equality we have used Ex,z〜pθ(x,z)[pθ二)"馨Ix) ] = J dxp(x)蔡 J "zpθ(z∣x)=
J dxP(X)蔡 1 = 0, and SimiIarIy Ey,z 〜Pθ(y,z)[ pθ⅛) 喘ZIy)] = 0. In other words, the
∣∣∆θ∣∣2 terms in the first-order variation δIBe[pθ(z∣x)] vanish, and the remaining ∣∣∆θ∣∣2 are all
in δ2IBe [pθ(z∣x)]. Also in the last expression, ZZ(θ) ≡ J dzpe(z) (dlogpθ(Z))(切0霏(Z)) is the
Fisher information matrix of θ for Z, Zz∣x(θ) ≡ J dxdzp(x)p)(z∣x) (dlog驾ZIx)) (dlog驾ZIx)),
Zz∣y(θ) ≡ J dydzp(y)pe(z∣y) (dlog喝ZIy)) (dlog驾ZIy)) are the conditional Fisher information
matrix (Zegers, 2015) of θ for Z conditioned on X and Y, respectively.
18
Published as a conference paper at ICLR 2020
Let us look at
δ2IBβ[pθ(z|x)] = 2∆θT {(lz∣χ(θ) - IZ(θ)) - β (Iz∣χ(θ) - IZ(θ))} ∆θ	(21)
Firstly, note that δ2IBβ[pθ(z|x)] is a quadratic function of ∆θ, and the scale of ∆θ does not change
the sign of δ2IBβ[pθ(z|x)], so the condition of ∀∆θ ∈ Θ, δ2IBβ[pθ(z|x)] ≥ 0 is invariant to
the scale of ∆θ, and is describing the “curvature” in the infinitesimal neighborhood of θ. There-
fore, ∆θ can explore any value in Θ. Secondly, we see that Eq. (21) is a special case of Eq.
(14) with E ∙ r(z∣x) = ∆θT∂θlogpθ(z|x). Therefore, The inequalities due to Jensen still hold:
e2 (E[r2(z∣x)] - E[r2(z)])	=	∆θτ	(Iz∣χ (θ)-Iz (θ)) ∆θ	≥	0,	E	(E[r2(z∣y)] - E[r2(z)])=
∆θτ (Iz∣γ(θ) -Iz(θ)) ∆θ ≥ 0. If ∆θτ (Iz∣γ(θ) -IZ(θ)) ∆θ > 0, then the condition of
∀∆θ ∈ Θ, δ2IBβ [pθ(z|x)] ≥ 0 is equivalent to ∀∆θ ∈ Θ,
∆θT (IZX (θ)-IZ (θ)) ∆θ
β ≤ ∆θτ (IZY(θ)-IZ(θ)) ∆θ
i.e.
β ≤ Gθ[pθ(z|x)] ≡
inf ∆θτ(IZ∣χ(θ)-IZ(θ))∆θ
∆θ∈Θ∆θτ (IZ|Y(θ)-IZ(θ)) ∆θ
(22)
If ∆θτ (IZ∣ γ(θ) - IZ(θ)) ∆θ = 0, We have that Eq. (21) always holds, which is a looser condition
than Eq. (22). Above all, we have that the condition of ∀∆θ ∈ Θ, δ2IBβ[pθ(z|x)] is equivalent to
β ≤ Gθ[pθ(z|x)].
Moreover, (Gθ[pθ(z∣x)])-1 given by Eq. (22) has the format of a generalized Rayleigh quotient
R(A, B; x) ≡ ∆θTA∆θ where A = IZ∣γ (θ)-IZ(θ) and B = IZ∣χ (θ)-IZ (θ) are both Hermitian
matrices6, which can be reduced to Rayleigh quotient R(D, CT∆θ) = ((CT迄)彳(CT∆θ)), with the
transformation D = C-1A(CT)-1 where CCT is the Cholesky decomposition of B = IZ|X (θ) -
IZ(θ). Moreover, we have that when Gθ [pθ(z|x)] attains its minimum value, the Reyleigh quotient
R(D, CT∆θ) attains its maximum value of λmax with CT∆θ = vmax, i.e. ∆θ = (CT)-1vmax,
where λmax is the largest eigenvalue of D and vmax the corresponding eigenvector.
□
D Proof of Theorem 1
Proof. Define
Te(β0) :=	inf	[(Eβ[r2(z|x)] - Ee卜2(z)]) - β ∙ (Ee[r2(z|y)] - Ee[r2(z)])]	(23)
r(zlX)∈QZ∣X
where Ee[∙] denotes taking expectation w.r.t. the optimal solution Pe(x, y, Z) = p(x, y)pβ(z|x) at
β. Using Lemma 2.1, we have that the IB phase transition as defined in Definition 3 corresponds to
satisfying the following two equations:
Te(β0)leo=e ≥ 0
lim Te(β 0) = 0-
e0→e+
(24)
(25)
Now we prove that Te(β 0) is continuous at β 0= β, i.e. ∀ε > 0, ∃δ > 0 s.t. ∀β ∈ (β - δ, β + δ), we
have |Te(β0) - Te(β)∣ < e.
6Here all the Fisher information matrices are real symmetric, thus Hermitian.
19
Published as a conference paper at ICLR 2020
From Eq. (23), We have Te(β0) - Te(β) = -(β0 - β) ∙ (Ee[r2(z∣y)] - Ee[r2(z)]). Since r(z∖x) is
bounded, i.e. ∃M > 0 s.t. ∀z ∈ Z, x ∈ X, ∣r(z∣x)∣ ≤ M, we have
I Ee [r2(z∣y)] I = Feh(Ex〜p(χ∣y,z) [r(zlx)])2] ∣ ≤ 回[(Ex〜p(χ∣y,z) [M])2] I = M2
Similarly, we have
I Ee [r2(Z)] ∣ = 1Eeh(Ex〜p(x∣z) [r(z∣X)])2] ∣ ≤ 归e [(Ex〜p(x∣z)[M])2]] = M2
HenCe, ∣Te(β0) - Te(β)∣ = ∣β0 - β∣ ∣ Ee[r2(z∣y)] - Ee[r2(z)] ∣ ≤ 2∣β0 - β∣M2.
To prove that Te(β0) is continuous at β0 = β, we have ∀ε > 0, ∃δ = 2^ > 0, s.t. ∀β0 ∈
(β - δ, β + δ), we have
∣Te(β0) - Te(β)∣ ≤ 2∣β0 - β∣M2 < 2δM2 = 22MM2 = ε
Hence Te (β0) is continuous at β0 = β.
Combining the continuity of Te (β0) at β0 = β, and Eq. (24) and (25), we have Te(β) = 0, which is
equivalent to G[^e(z∣x)] = β after simple manipulation.
□
E Invariance of G[r(z∣x);p(z∣χ)] to addition of A global
REPRESENTATION
Here we prove the following lemma:
Lemma 2.2. G[r(z∣x); p(z∣x)] defined in Lemma 0.1 is invariant to the transformation r0(z∣x)一
r(z∣x) + S(Z).
Proof. When we r(z∣x) is shifted by a global transformation r0(z∣x) - r(z∣x) + s(z), we have
r0(Z) - Ex〜p(x∣z)[r(z∣x) + s(z)] = Ex〜双方⑸[r(z∣x)] + S(Z)Ex〜似方⑸[1] = r(z) + s(z), and
similarly r0(z∣y) - r(z∣y) + s(z).
The numerator of G[r(z∣x);p(z∣x)] is then
Ex∕~p(x∕) [(r0(ZIX))2] - Ez〜p(z) [(r0(Z))2]
=Ex,z〜p(x,z) [(r(ZIX) + S(Z))2] - Ez〜p(z) [(r(Z) + S(Z))2]
=(Ex,z〜p(x,z) [r (Z∣x)] + 2Ex,z〜p(x,z) Ir(ZIX)S(Z)] + Ex,z〜p(x,z) [s (Z)])
-	(Ez 〜p(z) [r (Z)] + 2Ez 〜p(z) [r(Z)S(Z)] + Ez 〜p(z) [S (z)])
= (Ex,z〜p(x,z) [r (ZIX)] + 2Ez〜p(z) [s(Z)Ex〜p(x∣z) [r(ζ∣x)]] + Ez〜p(z) [s (z)])
-	(Ez〜p(z) [r2(ζ)] +2Ez〜p(z) [r(ζ)S(ζ)]+ Ez〜仪幻[s2(z)])
=(Ex,z〜p(x,z) [r (Z∣x)] + 2Ez〜p(z) [r(Z)S(Z)] + Ez〜p(z) [s (z)])
-	(Ez〜p(z) [r2(ζ)] +2Ez〜p(z) [r(ζ)S(ζ)]+ Ez〜p(z) [s2(z)])
=Ex,z〜p(x,z) [r (Z∣x)] - Ez〜p(z) [r (z)]
Symmetrically, we have
Ey,z~p(y,z) [(r (ZIy)) ] - Ez~p(z) [(r (Z)) ] = Ey,z~p(y,z) [r (ZIy)] - Ez~p(z) [r (z)]
Therefore,GMax)； P(Z∣x)] =E:二p(""z∣∣y;匕二[；;匕*] isinvariantto r0 (ZIX) J r(ζ∣x)+
s(z).	□
20
Published as a conference paper at ICLR 2020
F Proof of Theorem 2
Proof. Using the condition of the theorem, We have that ∀r(z∣x) ∈ QZ∣χ, there exists rι(z∣x) ∈
QZ|X and s(z) ∈ {s : Z → R|s bounded} s.t. r(z|x) = r1(z|x) + s(z). Note that the only
difference betWeen QZ|X and Q(Z0|)X is that QZ|X requires Ep(z|x) [r1(z|x)] = 0. Using Lemma 2.2,
We have
inf	G[r(z|x); p(z|x)]
r(ZIx) ∈QZ)X
inf	G [r1 (z |x); p(z|x)] = G[p(z|x)]
rι(z∣x)∈Qz∣x
where r(z∣x) doesn,t have the constraint of Ep(z∣x) [∙] = 0.
After dropping the constraint of Ez〜p(z∣x) [r(z∣χ)] = 0, again using Lemma 2.2, we can let r(z)=
Ex〜p(x∣z)[r(z∣x)] = 0 (since we can perform the transformation r0(z∣x) J r(z∣x) - r(z), so that
the new r0(z) ≡ 0). Now we get a simpler formula for G[p(z|x)], as follows:
G[p(z|x)]
inf	Ex,z〜p(x,z) [r (ZIx)]
r(z|x)eQZ)X Ey,z〜p(y,z) [(Ex〜p(x|y,z)[r(z|x)])[
(26)
where QZII)X := {r : X ×Z → R IEx〜p(x∣z)[r(z∣x)] = 0,r bounded}.
From Eq. (26), we can further require that Ex,z〜p(x,z) [r2 (ζ∣x)] = 1. Define
ρs2(X, Y; Z) := sup	E[(E[f(X, Z)|Y, Z])2] = SUP	Ey,z~p(y,z) h(Ex~p(x|y,z) [f(x, z)])]
f (X,Z)∈QZ2)χ	f (x,z)∈QZ)χ
(27)
where7 QZ2)χ := {r : X ×Z → R ∣Ex〜p(x∣z)[r(z∣x)] = 0, Ex,z〜p(x,z)[r2(z∣x)] = 1,r bounded}.
Comparing with Eq. (26), it immediately follows that
G[p(z|x)] = ρ2(x1Y; Z)
(i) We only have to prove that ρs (X, Y; Z) = ρr (X, Y; Z), where ρr (X, Y; Z) is defined in Defini-
tion 4.
We have
E[f (X, Z)g(Y, Z)]
dxdydzp(x, y, z)f(x, z)g(y, z)
dydzp(y, z)g(y, z)	dxp(x|y, z)f(x, z)
dydzp(y, z)g(y, z)F (y, z)
ZJ dydzp(y,z)g2(y, z)
ZJ dydzp(y,z)F2(y,z)

≤
where F(y, z) := dxp(x|y, z)f(x, z). We have used Cauchy-Schwarz inequality, where the equal-
ity holds when g(y, z) = αF (y, z) for some α. Since E[g2(y, z)] = 1, we have α2E[F 2(y, z)] = 1.
7In the definition of ρr (X, Y ; Z), we have used an equivalent format f(x, z) instead of r(z|x).
21
Published as a conference paper at ICLR 2020
Taking the supremum of (E[f (X, Z)g(Y, Z)])2 w.r.t. f and g, we have
ρr (X,Y; Z) =	sup	(E[f (X, Z)g(Y, Z)])2
(f(X,Z),g(Y,Z))∈S1
= sup / dydzp(y,z)g2 (y,z) ∙	dydzp(y,z')F2 (y,z)
(f(x,z),g(y,z))∈S1
sup
f(x, Z) ∈QZ)X
dydzp(y, z)F2 (y, z)
= sup	dydzp(y, z)	dxp(x|y, z)f(x, z)
f(x,z)∈QZ)χ J	'J	)
= sup	E[(E[f (X, Z)|Y, Z])2]
f (X,Z)∈QZ)χ
≡ρs2(X, Y; Z)
Here S1 is defined in Definition 4. By definition both ρr(X, Y ; Z) and ρs (X, Y ; Z) take non-negative
values. Therefore,
Ps(X,Y; Z)= Pr (X,Y; Z)
(ii) Using the definition of ρr (X, Y; Z), we have
(28)
ρr2(X, Y; Z)
≡ sup dydzp(y, z) dxp(x|y, z)f(x, z)
f (x,z)∈QZ)χ J	'J	)
=	sup	dzp(z) dyp(y|z)	dxp(x|y, z)f(x, z)
f (x,z)∈Q∞χ J	J	'J	)
≡	sup	dzp(z)W [f (x, z)]
f(x, Z) ∈QZ)X J
where W[f(x,z)]:= R dyp(y∣z) (R dxp(x∣y,z)f (x,z))2.
Denote c(z) := P(Z)Ex〜p(x∣z)[f2(x,z)],wehave R c(z)dz = Ex,z〜p(x,z)[f2(x,z)] = L Thenthe
supremum ρr2 (X, Y; Z) = sup	dzp(z)W [f (x, z)] is equivalent to the following two-stage
f(x,z)∈QZ)χ
supremum:
ρr2(X, Y; Z)
sup	dzp(z)	sup W [f (x, z)]
C(Z):R C(Z)dz = 1J	f (x,z)∈QZ)χ
(29)
where Q(3∣X ：= {X × Z → RlEx〜p(χ∣z)[f2(x, z)]=需,Ex〜p(χ∣z)[f(x, z)] = 0,f bounded}
We can think of the inner supremum supf(x,z)∈Q(3) W [f (x, z)] as only w.r.t. x, for some given z.
Now let’s consider another supremum:
sup	dyp(y|z)	dxp(x|y, z
h(x)∈Q(Xh)
(30)
where Q(Xh) := {h : X → R Ep(x|z) [h(x)] = 0, Ep(x|z) [h2(x)] = 1, h bounded}. Using similar
technique in (ii), it is easy to prove that it equals ρ2m(X, Y|Z) as defined in Definition 4.
2
22
Published as a conference paper at ICLR 2020
Comparing Eq. (30) and the supremum:
sup	W[f(x, z)]
f(x, Z) ∈QZ)X
We see that the only difference is that in the latter Ex〜p(χ∣z) [f2 (x, z)] equals P(I) instead of 1. Since
W[f(x, z)] is a quadratic functional of f(x, z), we have
sup
f(x, Z) ∈QZ)χ
W [f(χ,z)] = M ρm(x,Y |Z)
p(z )
Therefore,
Pr(x,Y; Z)
sup
c(Z): c(Z)dZ=1
sup
c(Z): c(Z)dZ=1
dz p(z)	sup	W[f(x, z)]
J	f(x,z)∈QZX
Zdzp ⑶ M ρm(x,Y |Z)
p(z )
sup	dz c(z)ρ2m(X, Y|Z = z)
c(Z): c(Z)dZ=1
sup ρ2m(X, Y |Z)
Z∈Z
Where in the last equality We have letc(z) have “mass” only on the place Where ρ2m (X, Y|Z = z)
attains supremum W.r.t. z.
(iii) When Z is a continuous variable, let f (x, Z) = fχ(x) Jδ(P-ZZO), where δ(∙) is the Dirac-delta
function, z0 is a parameter, fX (x) ∈ Q(Xf|)Z, With Q(Xf|)Z := {fX : X → R fX bounded; ∀Z ∈ Z :
EX〜P(X∣z)[fx(x)] = 0,EX〜p(x∣z)[fX(x)] = 1}∙ WehaVe
Ex 〜p(x∣z)f (X,Z)] = / P(XIz)f (x,z)dx
=j(；-z：o) ZP(XIz)fX(X)dx
_ Sδ(z - zo) o
P p(z)
=0
And
E[f2 (X, Z)] = P(X, z)f2 (X, z)dXdz
=/ p(x,z)fX (x) '(；(z：o) dxdz
=
=Z
=1
dzδ(z -zo)	dXP(XIz)fX2 (X)dX
dzδ(z — zo) ∙ 1
Therefore, such constructed f (x, z) = ∕x(x) Jδ(P-ZO) ∈ QZ2)χ, satisfying the requirement for
ρs (X, Y; Z) (which equals ρr (X, Y; Z) by Eq. 28).
23
Published as a conference paper at ICLR 2020
Substituting in the special form of f(x, z) into the expression of ρs (X, Y ; Z) in Eq. (27), we have
sup_______	dzp(z)	dyp(y∣z) (	dxp(x∣y,z)f (x,z) j
f(x,Z)：f(X,Z)=fx(X)qδ(P-ZOɪ ,fχ(X)∈Qχf∣z
SUP	/ dzp(z) [ dyp(n|z) Z / dxp(x∖y z)fx(χ)Sδ(z-z0 !
SuP	dzp(z)	dyp(ylz)	dxp(xιy, z)fχ(x)
fχ (χ)∈QXf∣Z ,zo∈Zj	JVJ	V P(Z) )
sup d dzp(z) δ(z -Z0)	sup	d dyp(y∣z)
Z0∈Z	p(z)
fx(χ)∈Qf∣Z J
2
dxp(x∣y, z
sup	dzδ(z - z0)	sup	E[(E[fX (X)∣Y, Z = z])2 ∣Z = z]
Z(OEZJ	fx (X)∈Qf)z
sup
Z0 ∈Z
dzδ(z
- z0)ρ2m(X, Y ∣Z = z)
sup ρ2m(X, Y ∣Z = z0)
Z0∈Z
sup ρ2m(X, Y ∣Z)
Z∈Z
We can identify supf (X)∈Q(f) E[(E[fX(X)∣Y, Z
z])2∣Z = z] with ρ2m(X, Y ∣Z = z) because
fX (x) satisfies the requirement for conditional maximum correlation that Ep(X|Z) [fX (x)] = 0 and
Ep(X|Z) [fX2 (x)] = 1, for any z, and using the same technique in (i), it is straightforward to prove that
supfx(X)∈Q(f) E[(E[fX(X)∣Y, Z = z])2∣Z = z]
defined in Definition 4.
equals the conditional maximum correlation as
Since the conditional maximum correlation can be viewed as the maximum correlation between X
and Y, where X,Y 〜p(X,Y∣Z), using the equality of (β0[h(x)])-1 = Pm(X; Y) (Eq. 7 in WU
et al. (2019)), We can identify the h(χ) in β0[h(x)] with the fχ(X) here, and an optimal fX(X) that
maximizes Pm(X,Y∣Z) is also an optimal h*(x) that minimizes βo[h(x)].
(iv) For discrete X, Y and Z and a given Z = z, let QX,Y |Z
P(X,y|Z)	ʌ
ʌ/p(XIZ)P(y|Z) J
X,y
P(X,y)	/p(ZIX) ∖
√p(X)p(y) V P(ZIy))
X,y
we first prove that its second largest singular value is P2m(X, Y∣Z)
sup EX,y~p(X,y∣Z)[f(x)g(y)](S2 is defined in Definition 4).
(f,g)∈S2
Let column vectors uι = /p(x∣z) and vι = /p(y∣z) (note that Z is given and fixed). Also let
u2 = f (x) ,p(x∣z) and V = g(y) ,p(y∣z). Denote inner producthu, Vi ≡ Pi Uivi, and the length
of a vector as ∣∣u∣∣ =，hu, u〉. We have ∣∣uι∣∣ = ∣∣vι∣∣ = 1 due to the normalization of probability,
∣∣u2∣∣ = ∣∣v2∣∣ = 1 due to EX 〜p(X∣Z)f2 (X)] = Ey 〜p(y|Z)[g2(y)] = 1, and hu1,u2i = hv1,v2i = 0
due to EX〜p(X∣Z)[f(x)] = Ey〜p(y∣Z)[g(y)] = 0. Furthermore, we have
sup EX,y〜p(X,y∣Z)[f (x)g(y)] = max U QX,Y|Zv
(f,g)∈S2	u,v
which is exactly the second largest singular value σ2(Z) of the matrix QX,YIZ. Using the result in
(ii), we have that Pr (X, Y; Z)
max σ2 (Z).
Z∈Z
□
24
Published as a conference paper at ICLR 2020
G	Subset separation at phase transitions
In this section we study the behavior of p(z|x) on the phase transitions. We use the same categorical
dataset (where |X| = |Y | = |Z| = 3 and p(x) is uniform, and p(y|x) is given in Fig. 5). In Fig. 6
we show the p(z |x) on the simplex before and after each phase transition. We see that the first phase
transition corresponds to the separation of x = 2 (belonging to y = 2) w.r.t. x ∈ {0, 1} (belonging to
classes y ∈ {0, 1}), on the p(z|x) simplex. The second phase transition corresponds to the separation
of x = 0 with x = 1. Therefore, each phase transition corresponds to the ability to distinguish subset
of examples, and learning of new classes.
H MNIST Experiment Details
We use the MNIST training examples with class 0, 1, 2, 3, with a hidden label-noise matrix as given
in Fig. 7, based on which at each minibatch we dynamically sample the observed label. We use
conditional entropy bottleneck (CEB) (Fischer, 2018) as the variational IB objective, and run multiple
independent instances with different the target β . We jump start learning by started training at
β = 100 for 100 epochs, annealing β from 100 down to the target β over 600 epochs, and continue to
train at the target epoch for another 800 epochs. The encoder is a three-layer neural net, where each
hidden layer has 512 neurons and leakyReLU activation, and the last layer has linear activation. The
classifier p(y|z) is a 2-layer neural net with a 128-neuron ReLU hidden layer. The backward encoder
p(z |y) is also a 2-layer neural net with a 128-neuron ReLU hidden layer. We trained with Adam
(Kingma & Welling, 2013) at learning rate of 10-3, and anneal down With factor 1/(1 + 0.01 ∙ epoch).
For Alg. 1, for the fθ we use the same architecture as the encoder of CEB, and use |Z | = 50 in Alg.
1.
I CIFAR10 Experiment Details
We use the same CIFAR10 class confusion matrix provided in Wu et al. (2019) to generate noisy
labels with about 20% label noise on average (reproduced in Table 1). We trained 28 × 1 Wide
ResNet (He et al., 2016; Zagoruyko & Komodakis, 2016) models using the open source implementa-
tion from Cubuk et al. (2018) as encoders for the Variational Information Bottleneck (VIB) (Alemi
et al., 2016). The 10 dimensional output of the encoder parameterized a mean-field Gaussian with
unit covariance. Samples from the encoder were passed to the classifier, a 2 layer MLP. The marginal
distributions were mixtures of 500 fully covariate 10-dimensional Gaussians, all parameters of which
are trained.
With this standard model, we trained 251 different models at β from 1.0 to 6.0 with step size of 0.02.
As in Wu et al. (2019), we jump-start learning by annealing β from 100 down to the target β. We do
this over the first 4000 steps of training. The models continued to train for another 56,000 gradient
steps after that, a total of 600 epochs. We trained with Adam (Kingma & Ba, 2015) at a base learning
rate of 10-3, and reduced the learning rate by a factor of 0.5 at 300, 400, and 500 epochs. The models
converged to essentially their final accuracy within 40,000 gradient steps, and then remained stable.
Figure 5:	p(y|x) for the categorical dataset in Fig. 2 and Fig. 6. The value in ith row and jth column
denotes p(y = j |x = i). p(x) is uniform.
25
Published as a conference paper at ICLR 2020
0 5 0 5 0
∙2∙∙∙0∙0
s
O 5
3 2
b
(d)	(e)
Figure 6:	(a) I(Y ; Z) vs. β for the dataset given in Fig. 5. The phase transitions are marked with
vertical dashed line, with β0 = 2.065571 and βC = 5.623333. (b)-(e) Optimal pβ(z∣x) for four
values of β, i.e. (b) β = 2.060, (c) β = 2.070, (d) β = 5.620 (e) β = 5.625 (their β values are also
marked in (a)), where each marker denotes p(z|x = i) for a given i ∈ {0, 1, 2}.
26
Published as a conference paper at ICLR 2020
rθ.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Figure 7:	Confusion matrix for MNIST experiment. The value in ith row and jth column denotes
0® = j∖y = i) for the label noise.
The accuracies reported in Figure 4 are averaged across five passes over the training set. We use
∖Z ∖ = 50 in Alg. 1.
Table 1: Class confusion matrix used in CIFAR10 experiments, reproduced from (Wu et al., 2019).
The value in row i, column j means for class i, the probability of labeling it as class j . The mean
confusion across the classes is 20%.
	Plane	Auto.	Bird	Cat	Deer	Dog	Frog	Horse	Ship	Truck
Plane	0.82232	0.00238	0.021	0.00069	0.00108	0	0.00017	0.00019	0.1473	0.00489
Auto.	0.00233	0.83419	0.00009	0.00011	0	0.00001	0.00002	0	0.00946	0.15379
Bird	0.03139	0.00026	0.76082	0.0095	0.07764	0.01389	0.1031	0.00309	0.00031	0
Cat	0.00096	0.0001	0.00273	0.69325	0.00557	0.28067	0.01471	0.00191	0.00002	0.0001
Deer	0.00199	0	0.03866	0.00542	0.83435	0.01273	0.02567	0.08066	0.00052	0.00001
Dog	0	0.00004	0.00391	0.2498	0.00531	0.73191	0.00477	0.00423	0.00001	0
Frog	0.00067	0.00008	0.06303	0.05025	0.0337	0.00842	0.8433	0	0.00054	0
Horse	0.00157	0.00006	0.00649	0.00295	0.13058	0.02287	0	0.83328	0.00023	0.00196
Ship	0.1288	0.01668	0.00029	0.00002	0.00164	0.00006	0.00027	0.00017	0.83385	0.01822
Truck	0.01007	0.15107	0	0.00015	0.00001	0.00001	0	0.00048	0.02549	0.81273
27