Published as a conference paper at ICLR 2020
Generalization bounds
FOR DEEP CONVOLUTIONAL NEURAL NETWORKS
Philip M. Long* and Hanie Sedghi*
Google Brain
{plong,hsedghi}@google.com
Ab stract
We prove bounds on the generalization error of convolutional networks. The
bounds are in terms of the training loss, the number of parameters, the Lipschitz
constant of the loss and the distance from the weights to the initial weights. They
are independent of the number of pixels in the input, and the height and width
of hidden feature maps. We present experiments using CIFAR-10 with varying
hyperparameters of a deep convolutional network, comparing our bounds with
practical generalization gaps.
1	Introduction
Recently, substantial progress has been made regarding theoretical analysis of the generalization
of deep learning models (see Neyshabur et al., 2015; Zhang et al., 2016; Dziugaite and Roy, 2017;
Bartlett et al., 2017; Neyshabur et al., 2017; 2018; Arora et al., 2018; Golowich et al., 2018; Neyshabur
et al., 2019; Wei and Ma, 2019a; Cao and Gu, 2019; Daniely and Granot, 2019). One interesting
point that has been explored, with roots in (Bartlett, 1998), is that even if there are many parameters,
the set of models that can be represented using weights with small magnitude is limited enough
to provide leverage for induction (Neyshabur et al., 2015; Bartlett et al., 2017; Neyshabur et al.,
2018). Intuitively, if the weights start small, since the most popular training algorithms make small,
incremental updates that get smaller as the training accuracy improves, there is a tendency for these
algorithms to produce small weights. (For some deeper theoretical exploration of implicit bias in
deep learning and related settings, see (Gunasekar et al., 2017; 2018a;b; Ma et al., 2018).) Even more
recently, authors have proved generalization bounds in terms of the distance from the initial setting
of the weights instead of the size of the weights (Dziugaite and Roy, 2017; Bartlett et al., 2017;
Neyshabur et al., 2019; Nagarajan and Kolter, 2019). This is important because small initial weights
may promote vanishing gradients; it is advisable instead to choose initial weights that maintain
a strong but non-exploding signal as computation flows through the network (see LeCun et al.,
2012; Glorot and Bengio, 2010; Saxe et al., 2013; He et al., 2015). A number of recent theoretical
analyses have shown that, for a large network initialized in this way, accurate models can be found by
traveling a short distance in parameter space (see Du et al., 2019b;a; Allen-Zhu et al., 2019; Zou et al.,
2018; Lee et al., 2019). Thus, the distance from initialization may be expected to be significantly
smaller than the magnitude of the weights. Furthermore, there is theoretical reason to expect that,
as the number of parameters increases, the distance from initialization decreases. This motivates
generalization bounds in terms of distance from initialization (Dziugaite and Roy, 2017; Bartlett
et al., 2017).
Convolutional layers are used in all competitive deep neural network architectures applied to image
processing tasks. The most influential generalization analyses in terms of distance from initialization
have thus far concentrated on networks with fully connected layers. Since a convolutional layer has an
alternative representation as a fully connected layer, these analyses apply in the case of convolutional
networks, but, intuitively, the weight-tying employed in the convolutional layer constrains the set of
functions computed by the layer. This additional restriction should be expected to aid generalization.
* Authors are ordered alphabetically.
1
Published as a conference paper at ICLR 2020
In this paper, we prove new generalization bounds for convolutional networks that take account of
this effect. As in earlier analyses for the fully connected case, our bounds are in terms of the distance
from the initial weights, and the number of parameters. Additionally, our bounds independent of the
number of pixels in the input, or the height and width of the hidden feature maps.
Our most general bounds apply to networks including both convolutional and fully connected layers,
and, as such, they also apply for purely fully connected networks. In contrast with earlier bounds for
settings like the one considered here, our bounds are in terms of a sum over layers of the distance
from initialization of the layer. Earlier bounds were in terms of product of these distances which led
to an exponential dependency on depth. Our bounds have linear dependency on depth which is more
aligned with practical observations.
As is often the case for generalization analyses, the central technical lemmas are bounds on covering
numbers. Borrowing a technique due to Barron et al. (1999), these are proved by bounding the
Lipschitz constant of the mapping from the parameters to the loss of the functions computed by the
networks. (Our proof also borrows ideas from the analysis of the fully connected case, especially
(Bartlett et al., 2017; Neyshabur et al., 2018).) Covering bounds may be applied to obtain a huge
variety of generalization bounds. We present two examples for each covering bound. One is a
standard bound on the difference between training and test error. Perhaps the more relevant bound
has the flavor of “relative error”; it is especially strong when the training loss is small, as is often the
case in modern practice. Our covering bounds are polynomial in the inverse of the granularity of the
cover. Such bounds seem to be especially useful for bounding the relative error.
In particular, our covering bounds are of the form (B/)W, where is the granularity of the cover,
B is proportional to the Lipschitz constant of a mapping from parameters to functions, and W is
the number of parameters in the model. We apply a bound from the empirical process literature in
terms of covering bounds of this form due to Gine and Guillou (2001), Who paid particular attention
to the dependence of estimation error on B. This bound may be helpful for other analyses of the
generalization of deep learning in terms of different notions of distance from initialization. (Applying
bounds in terms of Dudley’s entropy integral in the standard Way leads to an exponentially Worse
dependence on B.)
Related previous work. Du et al. (2018) proved bounds for CNNs in terms of the number of
parameters, for tWo-layer netWorks. Arora et al. (2018) analyzed the generalization of netWorks
output by a compression scheme applied to CNNs. Zhou and Feng (2018) provided a generalization
guarantee for CNNs satisfying a constraint on the rank of matrices formed from their kernels. Li
et al. (2018) analyzed the generalization of CNNs under other constraints on the parameters. Lee and
Raginsky (2018) provided a size-free bound for CNNs in a general unsupervised learning frameWork
that includes PCA and codebook learning.
Related independent work. Ledent et al. (2019) proved bounds for CNNs that also took account
of the effect of Weight-tying. (Their bounds retain the exponential dependence on the depth of the
netWork from earlier Work, and are otherWise qualitatively dissimilar to ours.) Wei and Ma (2019b)
obtained bounds for fully connected netWorks With an improved dependence on the depth of the
netWork. Daniely and Granot (2019) obtained improved bounds for constant-depth fully-connected
netWorks. Jiang et al. (2019) conducted a Wide-ranging empirical study of the dependence of the
generalization gap on a variety of quantities, including distance from initialization.
Notation. If K(i) is the kernel of convolutional layer number i, then op(K(i) ) refers to its operator
matrix 1 and vec(K (i)) denotes the vectorization of the kernel tensor K(i). For matrix M, kMk2
denotes the operator norm of M. For vectors, || ∙ || represents the Euclidian norm, and || ∙ ∣∣ι
is the L1 norm. For a multiset S of elements of some set Z , and a function g from Z to R, let
ES[g] = ml Pm=I g(zt). We will denote the function parameterized by Θ by fθ.
2	Bounds for a basic setting
In this section, we provide a bound for a clean and simple setting.
1Convolution is a linear operator and can thus be written as a matrix-vector product. The operator matrix of
kernel K, refers to the matrix that describes convolving the input with kernel K. For details, see (Sedghi et al.,
2018).
2
Published as a conference paper at ICLR 2020
2.1	The setting and the bounds
In the basic setting, the input and all hidden layers have the same number c of channels. Each input
x ∈ Rd×d×c satisfies k vec(x)k ≤ 1.
We consider a deep convolutional network, whose convolutional layers use zero-padding (see Good-
fellow et al., 2016). Each layer but the last consists ofa convolution followed by an activation function
that is applied componentwise. The activations are 1-Lipschitz and nonexpansive (examples include
ReLU and tanh). The kernels of the convolutional layers are K(i) ∈ Rk×k×c×c for i ∈ {1, . . . , L}.
Let K = (K(1), ..., K(L)) be the L × k × k × c × c-tensor obtained by concatening the kernels for
the various layers. Vector w represents the last layer; the weights in the last layer are fixed with
||w|| = 1. Let W = Lk2c2 be the total number of trainable parameters in the network.
We let K0(1) , . . . , K0(L) take arbitrary fixed values (interpreted as the initial values of the kernels)
subject to the constraint that, for all layers i, || op(K0(i))||2 = 1. (This is often the goal of initialization
schemes.) Let K0 be the corresponding L × k × k × c × c tensor. We provide a generalization bound
in terms of distance from initialization, along with other natural parameters of the problem. The
distance is measured with ||K - Ko∣∣σ def PL=I || op(K(i)) - op(KOi))∣∣2.
For β > 0, define Ke to be the set of kernel tensors within ∣∣∙∣∣σ distance β of Ko, and define Fe to
be set of functions computed by CNNS with kernels in Ke. That is, Fe = {fκ : ||K - Ko ∣∣σ ≤ β}.
Let' : R X R → [0,1] bea loss function such that '(∙,y) is λ-Lipschitz for all y. An example is the
1 /λ-margin loss.
For a function f from Rd×d×c to R, let `f (x, y) = `(f (x), y).
We will use S to denote a set {(x1, y1), . . . , (xm, ym)} = {z1, . . . zm} of random training examples
where each zt = (xt, yt).
Theorem 2.1 (Basic bounds). For any η > 0, there is a C > 0 such that for any β, δ > 0, λ ≥ 1,
for any joint probability distribution P over Rd×d×c × R, if a training set S of n examples is drawn
independently at random from P, then, with probability at least 1 - δ, for all f ∈ Fe,
C(W (β + log(λn)) + log(1∕δ))
Ez〜P['f(Z)] ≤ (I + η)Es['f]+-------------n--------------
and, if β ≥ 5, then
JP['f (Z)] ≤ ES['f] + CrW≡0g≡÷jθg≡
and otherwise
Ez〜P ['f (z)] ≤ ES['f ] + C
log(1∕δ)
n
If Theorem 2.1 is applied with the margin loss, then Ez〜P ['f (z)] is in turn an upper bound on
the probability of misclassification on test data. Using the algorithm from (Sedghi et al., 2018),
∣∣∙∣∣σ may be efficiently computed. Since ||K - Ko∣∣σ ≤ || Vec(K) - vec(K0)∣∣1 (Sedghi et al.,
2018), Theorem 2.1 yields the same bounds as a corollary if the definition of Fe is replaced with the
analogous definition using || vec(K) - vec(Ko)||1.
2.2	TOOLS
Definition 2.2. For d ∈ N, a set G of real-valued functions with a common domain Z, we say that
G is (B, d)-Lipschitz parameterized if there is a norm || ∙ || on Rd and a mapping φ from the unit
ball w.r.t. ∣∣∙∣∣ in Rd to G such that, for all θ and θ0 such that ∣∣θ∣∣ ≤ 1 and ∣∣θ0∣∣ ≤ 1, and all Z ∈ Z,
∣(φ(θ))(z)-(φ(θ0))(z)∣≤ B∣∣θ-θ0∣∣.
The following lemma is essentially known. Its proof, which uses standard techniques (see Pollard,
1984; TaIagrand,1994; 1996; Barron et al., 1999; Van de Geer, 2000; Gine and Guillou, 2001; Mohri
et al., 2018), is in Appendix A.
3
Published as a conference paper at ICLR 2020
Lemma 2.3. Suppose a set G of functions from a common domain Z to [0, M] is (B, d)-Lipschitz
parameterized for B > 0 and d ∈ N.
Then, for any η > 0, there is a C such that, for all large enough n ∈ N, for any δ > 0, for any
probability distribution P over Z, if S is obtained by sampling n times independently from P, then,
with probability at least 1 - δ, for all g ∈ G,
ŋ r , .η / E「]	CM(dIog(Bn) + log(1∕δ))
Ez〜P[g(z)] ≤ (1 + η)Es[g] + —(-g-)+ g( / ))
n
and if B ≥ 5,
Ez〜P [g(z)] ≤ ES[g] + CM∖^oB+oI,
n
and, for all B,
Ez〜p[g(z)] ≤ ES[g] + C
2.3	Proof of Theorem 2.1
Definition 2.4. Let 'f = {f : f ∈ F}.
We will prove Theorem 2.1 by showing that 'f^ is [βλeβ, W)-Lipschitz parameterized. This will be
achieved through a series of lemmas.
Lemma 2.5. Choose K ∈ Ke and a layer j. Suppose K ∈ K satisfies K(i) = K(i) for all i = j.
Then, for all examples (x,y), ∣'(fκ (x),y) - '(fK (x), y)| ≤ λeβ ∣∣op(K(j)) 一 op(K (j))∣∣ .
Proof. For each layer i, let βi = || op(K(i)) - op(K0(i))||2.
Since ' is λ-Lipschitz w.r.t. its first argument, we have that ∣'(fκ(χ),y) - '(fKK(χ),y)∣ ≤ λ∣fκ(x)-
fκ(x)\, so it suffices to bound ∣fκ(x) - fKK(x)|. Let gup be the function from the inputs to the
whole network with parameters K to the inputs to the convolution in layer j , and let gdown be the
function from the output of this convolution to the output of the whole network, so that fK
gdown ◦ fop(K(j)) ◦ gup. Choose an input x to the network, and let u = gup(x). Recalling that \\x\\ ≤ 1,
and the non-linearities are nonexpansive, we have
kuk ≤ Y ∣∣op(K(i))∣∣ . Since the non-linearities
i<j	2
are 1-Lipschitz, and, recalling that K(i) = K(i) for i = j, we have
∖fκ (x) - fK (x)\ = \gdown(op(K S))U)- gdown(θp(K (j))u)∖
≤ (Y∣∣op(K (i))j ∣∣op(K⑺)U - op(K(j))u∣∣
≤ (Y∣∣op(K (i))j ∣∣oP(Kj))- Op(K ⑶)∣L H
≤ (Y∣∣op(K (i))j ∣∣op(K(j)) — op(K j))∣∣
≤ (Y(1+ βi)) ∣∣op(Kj))- Op(K(j))∣∣2
4
Published as a conference paper at ICLR 2020
where the last inequality uses the fact that || op(K(i))-op(K0(i))||2 ≤ βi for all i and || op(K0(i))||2 =
1 for all i.
Now Qi6=j (1 + βi) ≤ QL=ι(1 + βi), and the latter is maximized over the nonnegative βi's subject
to Pi=j βi ≤ β when each of them is β∕L. Since (1 + β∕L)L ≤ eβ, this completes the proof. 口
Now we prove a bound when all the layers can change between K and K .
Lemma 2.6. For any K, K ∈ Ke, for any input X to the network, ∣'(fκ(X), y) 一 '(fκ(x), y) | ≤
λeβ 卜-K L.
Proof. Consider transforming K to K by replacing one layer of K at a time with the corresponding
layer in K. Applying Lemma 2.5 to bound the distance traversed with each replacement and
combining this with the triangle inequality gives
L
l'(fκ(x),y) — '(fκ(x),y)l ≤ λeβ χ∣∣op(KS)- op(K(j))∣∣2 = λeβ∣∣K 一 K,.
□
Now we are ready to prove our basic bound.
Proof(of Theorem 2.1). Consider the mapping φ from the ball w.r.t. |卜||。of radius 1 in RLk2c2
centered at vec(K0) to . defined by φ(θ) = f +)，where VecT (θ) is the reshaping of θ
into a L × k × k × c × c-tensor. Lemma 2.6 implies that this mapping is β λeβ -Lipschitz. Applying
Lemma 2.3 completes the proof.	□
2.4	Comparisons
Since a convolutional network has an alternative parameterization as a fully connected network, the
bounds of (Bartlett et al., 2017) have consequences for convolutional networks. To compare our
bound with this, first, note that Theorem 2.1, together with standard model selection techniques,
yields a
W (||K - Ko∣∣σ +log(λ))+log(1∕δ)
(1)
n
bound on Ez~p ['f (z)] - ES ['f (z)] (For more details, please see Appendix B.) Translating the bound
of (Bartlett et al., 2017) to our setting and notation directly yields a bound on Ez~p ['f (z)] -ES ['f (z)]
whose main terms are proportional to
λ (nL=ι ||op(K⑴)1。(PL=I ||oP(KCop)KP>|l2/13)3/2iog(d4c2L) + piog(w
√n
(2)
where, for a p × q matrix A, ||A||2,1 = ||(||A:,1 ||2, ..., ||A:,q||2)||1. One can get an idea of how
this bound relates to (1) by comparing the bounds in a simple concrete case. Suppose that each
of the convolutional layers of the network parameterized by K0 computes the identity function,
and that K is obtained from K0 by adding to each entry. In this case, disregarding edge effects,
for all i, || op(K(i))∣∣2 = 1 + ek2c and ||K - Ko∣∣σ = ek2cL (as proved in Appendix C). Also,
|| op(K (i))> - op(K0i))>∣∣2,ι = (cd2)(e√ck2) = ec3∕2d2k. We get additional simplification if we
set e =吉.In this case, (2) gives a constant times
(c + 1)L√Cd(d∕k)L3∕2λ log(dcL) + ,log(1∕δ)
5
Published as a conference paper at ICLR 2020
where (1) gives a constant times
c3/2kL + Ckplog(λ) + piog(1∕δ)
√n	.
In this scenario, the new bound is independent of d, and grows more slowly with λ, c and L. Note
that k ≤ d (and, typically, it is much less).
This specific case illustrates a more general phenomenon that holds when the initialization is close to
the identity, and changes to the parameters are on a similar scale.
Golowich et al. (2017) established bounds that improve on the bounds of (Bartlett et al., 2017) in
some cases. Their bound requires a restriction on the activation function (albeit one that is satisfied by
the ReLU). For large n, the main term of the natural consequence of Corollary 1 of their paper in the
setting of this section, with the required additional assumption on the activation function, grows like
λ√LQL=I ||op(κ(i))l∣F ≈ λ(cd)L√L
√n	√n	,
when = 1∕k2. (We note, however, that, in addition to not trying to take account of the convolutional
structure, Golowich et al. (2017) also did not make an effort to obtain stronger bounds in the case
that the distance from initialization is small. On the other hand, we suspect that modifying their proof
analogously to (Bartlett et al., 2017) to do so would not remove the exponential dependence on L.)
3	A more general b ound
In this section, we generalize Theorem 2.1.
3.1	The setting
The more general setting concerns a neural network where the input is a d × d × c tensor whose
flattening has Euclidian norm at most χ, and network’s output is a m-dimensional vector, which may
be logits for predicting a one-hot encoding of an m-class classification problem.
The network is comprised of Lc convolutional layers followed by Lf fully connected layers. The
ith convolutional layer includes a convolution, with kernel K(i) ∈ Rki ×ki ×ci-1 ×ci, followed by a
componentwise non-linearity and an optional pooling operation. We assume that the non-linearity
and any pooling operations are 1-Lipschitz and nonexpansive. Let V (i) be the matrix of weights for
the ith fully connected layer. Let Θ = (K(1), ..., K(Lc), V (1), ..., V (Lf)) be all of the parameters of
the network. Let L = Lc + Lf.
We assume that, for all y, '(∙, y) is λ-Lipschitz for all y and that '(y, y) ∈ [0, M] for all y and y.
An example (x, y) includes a d× d× c-tensor x and y ∈ Rm .
We let K0(1), . . . , K0(Lc), V0(1), ..., V0(Lf) take arbitrary fixed values subject to the constraint that, for
all convolutional layers i, || op(K0(i))||2 ≤ 1+ν, and for all fully connected layers i, ||V0(i) ||2 ≤ 1+ν.
LetΘ0= (K0(1),...,K0(Lc),V0(1),...,V0(Lf)).
For Θ = (K⑴，…，K(Lc), V⑴，…，V(Lf)) and Θ = (K⑴，…，K(Lc), V⑴，…，V(Lf)). define
I∣Θ - Θ ||n = (X || op(K⑴)-op(K ⑺)Q + Xf ||V ⑴-V ⑴∣∣2.
i=1	i=1
For β, ν ≥ 0, define Fβ,ν to be set of functions computed by CNNs as described in this subsection
With parameters within || ∙ ||n-distance β of Θ0. Let Oβ,ν be the set of their Parameterizations.
Theorem 3.1 (General Bound). For any η > 0, there is a constant C such that the following holds.
For any β , ν, χ > 0, for any δ > 0, for any joint probability distribution P over Rd×d×c × Rm such
that, with probability 1, (x, y)〜P satisfies || Vec(X) ∣∣2 ≤ X, under the assumptions of this section,
6
Published as a conference paper at ICLR 2020
if a training set S of n examples is drawn independently at random from P, then, with probability at
least 1 - δ, for all f ∈ Fβ,ν,
W	巾…乙一而 a - CM (W (β + VL + log(χλβn)) + log(1∕δ))
Ez〜p['f(z)] ≤ (1 + η)Es['f] +-----------n-----------------
and, if χλβ(1 + ν + β ∕L)L ≥ 5,
JP['f (z)] ≤ ES['f ] + CMr W (e + νL + Mχλβ))+log(10
and a bound of
Ez 〜P [`f(Z)] ≤ ES [g]+C (λβ(ι+V+e/L)L r ~+M
holds for all χ, λ, β > 0.
3.2	Proof of Theorem 3.1
We will prove Theorem 3.1 by using ∣∣∙∣∣n to witness the fact that 'Fβ,ν is (χλβ(1 + V + β∕L)L,W)-
Lipschitz parameterized.
The first two lemmas concern the effect of changing a single layer. Their proofs are very similar to
the proof of Lemma 2.5, and are in the Appendices D and E.
Lemma 3.2. Choose Θ =	(K⑴,…,K(Lc),V⑴,...,V(Lf)), Θ	=
(K ⑴，...，K (Lc),V ⑴，…，V(Lf)) ∈ Oβ,ν and a convolutional layer j. Suppose that K(i) = K(i)
for all convolutional layers i = j and V(i) = V(i) for all fully connected layers i. Then, for all
examples (x, y),
l'(fθ(χ),y) - '(fθ(χ),y)l ≤ χλ(i + V + β∕L)LIop(Kj))- op(K(j))∣∣2.
Lemma 3.3. Choose Θ =	(K⑴，…，K(Lc),V(1),...,V(Lf)), Θ	=
(K(I),…，K(Lc), V(I),…，V(Lf)) ∈ Oβ,ν and afully connected layer j. Suppose that K(i) = K(i)
for all convolutional layers i and V(i) = V(i) for all fully connected layers i = j. Then, for all
examples (x, y),
l'(fθ(χ), y) - '(fθ(χ),y)l ≤ χλ (1 + V + β∕L)L ∣∣V(j) - V⑶∣∣2.
TL T	F	1	1	11.1	1	1	1	L、	1 人
Now we prove a bound when all the layers can change between Θ and Θ.
Lemma 3.4. For any Θ =	(K(I),…,K (Lc),V(I),...,V(Lf)), Θ	=
(K(I),..., K(Lc), V(I),..., V(Lf)) ∈ Oβ,ν, for any input X,
l'(fθ(χ),y) - '(fθ(X),y)| ≤ χλ (1 + V + e/L)L ∣∣θ - θ∣∣7v .
ɪʌ z- x-ι ∙ i	r∙	♦ rʌ, KF	i	ι	.	/` /-ʌ	∙ . 1 .t	∙>♦
Proof. Consider transforming Θ to Θ by replacing one layer at a time of Θ with the corresponding
layer in Θ. Applying Lemma 3.2 to bound the distance traversed with each replacement of a
convolutional layer, and Lemma 3.3 to bound the distance traversed with each replacement of a fully
connected layer, and combining this with the triangle inequality gives the lemma.	□
Now we are ready to prove our more general bound.
Proof(of Theorem 3.1). Consider the mapping φ from the ball of || ∙ ||n-radius 1 centered
at Θ0 to 'Fβ,ν defined by φ(Θ) = 'fΘ +βΘ . Lemma 2.6 implies that this mapping is
(χλβ (1 + V + β/L)L , W) -Lipschitz. Applying Lemma 2.3 completes the proof.	□
7
Published as a conference paper at ICLR 2020
de。Uo-4-»ez=e」① U ①。
Figure 1: Generalization gaps for a 10-layer all-conv model on CIFAR10 dataset.
0.040」
deω
Figure 2: Generalization gap as a function of W
3.3	More comparisons
Theorem 3.1 applies in the case that there are no convolutional layers, i.e. for a fully connected
network. In this subsection, we compare its bound in this case with the bound of (Bartlett et al.,
2017). Because the bounds are in terms of different quantities, we compare them in a simple
concrete case. In this case, for D = cd2 , each hidden layer has D components, and there are
D classes. For all i, V0(i) = I and V(i) = I + H∕√D, where H is a Hadamard matrix (using
the Sylvester construction), and χ = M = 1. Then, dropping the superscripts, each layer V has
||V ||2 =2, ||V-V0||2 =1, ||V-V0||2,1 =D.
Further, in the notation of Theorem 3.1, W = D2L, and β = L, and ν = 0. Plugging into to
Theorem 3.1 yields a bound on the generalization gap proportional to
DL + DpL log(λ) + Plog(1∕δ)
√n
where, in this case, the bound of (Bartlett et al., 2017) is proportional to
X2LL3/2D Iog(DL) + Plog(1∕δ)
√n
and, when D is large relative to L, Corollary 1 of (Golowich et al., 2017) (approximately) gives
X2l/5Dl/2 + Plog(1∕δ)
√n	.
8
Published as a conference paper at ICLR 2020
(a) mean and error bar
Figure 3: ||K - Ko ∣∣σ as a function of W.
4	Experiments
We trained a 10-layer all-convolutional model on the CIFAR-10 dataset. The architecture was similar
to VGG (Simonyan and Zisserman, 2014). The network was trained with dropout regularization and
an exponential learning rate schedule. We define the generalization gap as the difference between
train error and test error. In order to analyze the effect of the number of network parameters on the
generalization gap, we scaled up the number of channels in each layer, while keeping other elements
of the architecture, including the depth, fixed. Each network was trained repeatedly, sweeping over
different values of the initial learning rate and batch sizes 32, 64, 128. For each setting the results
were averaged over five different random initializations. Figure 1 shows the generalization gap for
different values of W ||K - Ko∣∣σ. As in the bound of Theorem 3.1, the generalization gap increases
with W||K - Ko∣∣σ. Figure 2 shows that as the network becomes more over-parametrized, the
generalization gap remains almost flat with increasing W . This is expected due to role of over-
parametrization on generalization (Neyshabur et al., 2019). An explanation of this phenomenon that
is consistent with the bound presented here is that, ultimately, increasing W leads to a decrease in
value of ||K - Ko∣∣σ; see Figure 3a. The fluctuations in Figure 3a are partly due to the fact that
training neural networks is not an stable process. We provide the medians ||K - Ko∣∣σ for different
values of W in Figure 3b.
Acknowledgments
We thank Peter Bartlett, Jaeho Lee and Sam Schoenholz for valuable conversations, and anonymous
reviewers for helpful comments.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. ICML, 2019.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Andrew Barron, Lucien Birg6, and Pascal Massart. Risk bounds for model selection via penalization.
Probability theory and related fields, 113(3):301-413, 1999.
P. L. Bartlett. Dudley,s entropy integral, 2013. https://www.stat.berkeley.edu/-bartlett/courses/2013spring-
stat210b/notes/14notes.pdf; downloaded 4/17/19.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of the
weights is more important than the size of the network. IEEE transactions on Information Theory,
44(2):525-536, 1998.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pages 6240-6249, 2017.
9
Published as a conference paper at ICLR 2020
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. arXiv preprint arXiv:1905.13210, 2019.
Amit Daniely and Elad Granot. Generalization bounds for neural networks via approximate de-
scription length. In Advances in Neural Information Processing Systems, pages 12988-12996,
2019.
Simon S Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Ruslan R Salakhutdinov, and Aarti
Singh. How many samples are needed to estimate a convolutional neural network? In Advances in
Neural Information Processing Systems, pages 373-383, 2018.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. ICML, 2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. ICLR, 2019b.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. UAI, 2017.
Evarist Gine and Armelle Guillou. On consistency of kernel density estimators for randomly
censored data: rates holding uniformly over adaptive intervals. In AnnaIeS de l'IHP Probabilites et
statistiques, volume 37, pages 503-522, 2001.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pages 249-256, 2010.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pages 297-299, 2018.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pages 6151-6159, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pages
9461-9471, 2018b.
D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning
applications. Information and Computation, 100(1):78-150, 1992.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pages 1026-1034, 2015.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. ICLR, 2019.
A. N. Kolmogorov and V. M. Tikhomirov. ε-entropy and ε-capacity of sets in function spaces.
Uspekhi Matematicheskikh Nauk, 14(2):3-86, 1959.
Yann A LeCun, L6on Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pages 9-48. Springer, 2012.
Antoine Ledent, Yunwen Lei, and Marius Kloft. Norm-based generalisation bounds for multi-class
convolutional neural networks. arXiv 1905.12430, 2019.
10
Published as a conference paper at ICLR 2020
Jaeho Lee and Maxim Raginsky. Learning finite-dimensional coding schemes with nonlinear recon-
struction maps. arXiv preprint arXiv:1812.09658, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing Systems, pages 8570-8581,
2019.
Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao. On tighter generalization bound
for deep neural networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159, 2018.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex
statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion.
In ICML, pages 3351-3360, 2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance from
initialization. arXiv preprint arXiv:1901.01672, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pages 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems, pages 5947-5956,
2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. ICLR, 2018.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks. ICLR, 2019.
D. Pollard. Convergence of Stochastic Processes. Springer Verlag, Berlin, 1984.
D. Pollard. Empirical Processes : Theory and Applications, volume 2 of NSF-CBMS Regional
Conference Series in Probability and Statistics. Institute of Math. Stat. and Am. Stat. Assoc., 1990.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. arXiv
preprint arXiv:1805.10408, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
M. Talagrand. Sharper bounds for Gaussian and empirical processes. Annals of Probability, 22:
28-76, 1994.
Michel Talagrand. New concentration inequalities in product spaces. Inventiones mathematicae, 126
(3):505-563, 1996.
Sara A Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press,
2000.
V. N. Vapnik. Estimation of Dependencies based on Empirical Data. Springer Verlag, 1982.
V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events
to their probabilities. Theory of Probability and its Applications, 16(2):264-280, 1971.
Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz
augmentation. arXiv preprint arXiv:1905.03684, 2019a.
11
Published as a conference paper at ICLR 2020
Colin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classification
via an all-layer margin. ICLR, 2019b.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Pan Zhou and Jiashi Feng. Understanding generalization and optimization performance of deep cnns.
In ICML, pages 5955-5964, 2018.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
A Proof of Lemma 2.3
Definition A.1. If (X, ρ) is a metric space and H ⊆ X, we say that G is an -cover of H with
respect to ρ if every h ∈ H has a g ∈ G such that ρ(g, h) ≤ . Then Nρ (H, ) denotes the size of the
smallest -cover of H w.r.t. ρ.
Definition A.2. For a domain Z, define a metric ρmax on pairs of functions from Z to R by
ρmax(f, g) = supz∈Z |f(z) -g(z)|.
We need two lemmas in terms of these covering numbers. The first is by now a standard bound from
Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971; Vapnik, 1982; Pollard, 1984). For
example, it is a direct consequence of (Haussler, 1992, Theorem 3).
Lemma A.3. For any η > 0, there is a constant C depending only on η such that the following holds.
Let G be an arbitrary set of functions from a common domain Z to [0, M]. If there are constants B
and d SUCh that, NPmax (G, e) ≤ (B∙) for all e > 0, then, for all large enough n ∈ N, for any δ > 0,
for any probability distribution P over Z, if S is obtained by sampling n times independently from P,
then, with probability at leaSt 1 - δ, for all g ∈ G,
Ez〜P [g(z)] ≤ (1 + η)Es [g] + CMdIog(Bn≡g≡.
n
We will also use the following, which is the combination of (2.5) and (2.7) of (Gine and Guillou,
2001).
Lemma A.4. Let G be an arbitrary Set of funCtionS from a Common domain Z to [0, M]. If there
are constants B ≥ 5 and d such that NPmax (G, e) ≤ (B∙) for all e > 0, for all large enough
n ∈ N, for any δ > 0, for any probability diStribution P over Z, if S iS obtained by Sampling n timeS
independently from P, then, with probability at least 1 - δ, for all g ∈ G,
Ez〜P [g(z)] ≤ ES [g] + CM S dlθg Bn+lθg 1,
where Cis an absolute constant.
The above bound only holds for B ≥ 5. The following, which can be obtained by combining
Talagrand’s Lemma with the standard bound on Rademacher complexity in terms of the Dudley
entropy integral (see (Van de Geer, 2000; Bartlett, 2013)), yields a bound for all B .
Lemma A.5. Let G be an arbitrary set of functions from a common domain Z to [0, M]. If there
are constants B > 0 and d such that NPmax (G, e) ≤ (B∙) for all e > 0, then,for all large enough
n ∈ N, for any δ > 0, for any probability distribution P over Z, if S is obtained by sampling n times
independently from P, then, with probability at least 1 - δ, for all g ∈ G,
Ez〜p[g(z)] ≤ ES[g] + C
J+ + M
n
where C is an absolute constant.
12
Published as a conference paper at ICLR 2020
So now we want a bound on Nρmax (G, ) for Lipschitz-parameterized classes. For this, we need the
notion of a packing which we now define.
Definition A.6. For any metric space (X, ρ) and any H ⊆ S, let Mρ (H, ) be the size of the largest
subset of H whose members are pairwise at a distance greater than w.r.t. ρ.
Lemma A.7 ((Kolmogorov and Tikhomirov, 1959)). For any metric space (X, ρ), any H ⊆ X, and
any > 0, we have
Nρ(H,) ≤Mρ(H,).
We will also need a lemma about covering a ball by smaller balls. This is probably also already
known, and uses a standard proof (see Pollard, 1990, Lemma 4.1), but we haven’t found a reference
for it.
Lemma A.8. Let
•	d be a positive integer,
•	∣∣∙∣∣ be a norm
•	P be the metric induced by ∣∣∙∣∣, and
•	κ, > 0.
A ball in Rd ofradius K w.r.t. P can be Covered by (3κ)d balls ofradius e.
Proof. We may assume without loss of generality that κ > . Let q > 0 be the volume of the unit
ball w.r.t. P in Rd. Then the volume of any α-ball with respect to P is αdq. Let B be the ball of
radius r in Rd. The e/2-balls centered at the members of any e-packing of B are disjoint. Since these
centers are contained in B, the balls are contained in a ball of radius κ + e/2. Thus
Mρ(B,e) (e) q ≤ (K + e) q ≤ (3κ) q.
Solving for Mρ(B, e) and applying Lemma A.7 completes the proof.
□
We now prove Lemma 2.3. Let || ∙ || be the norm witnessing the fact that G is (B, d)-Lipschitz
parameterized, and let B be the unit ball in Rd w.r.t. ∣∣∙∣∣ and let P be the metric induced by ∣∣∙∣∣.
Then, for any e, an e/B-cover of B w.r.t. P induces an e-cover of G w.r.t. Pmax, so
NPmax(G,e) ≤ Nρ(B,e∕B).
Applying Lemma A.8, this implies
Nρmax(G,e)≤
Then applying Lemma A.3, Lemma A.4 and Lemma A.5 completes the proof.
B Proof of (1)
For δ > 0, and for each j ∈ N, let βj = 5 X 2j let δj =奈.Taking a union bound over an
application of Theorem 2.1 for each value ofj, with probability at least 1 - Pj δj ≥ 1 - δ, for all j,
and all f ∈ Fβj
Ez〜P [`f(z)] ≤ (1+ η)Es[`f(z)] + C(W∙ + logy + logC≡
and
r W Cej+Iog(X))+τog(j∕δ)
Ez〜P ['f (z)] ≤ ES ['f (z)] + CV-------n-----------.
13
Published as a conference paper at ICLR 2020
For any K, if We apply these bounds in the case of the least j such that || K - Ko || σ ≤ βj, we get
C(W⑵|K - Ko∣∣σ + log(λn)) + log(log(∣∣K - Ko∣∣σ)∕δ))
Ez〜P ['f (z)] ≤ (1 + η)Es ['f (z)] +----------------------n---------------------------
and
JP['f (z)] ≤ Es['f (z)]+ CrW⑵IK - K01」+log叫+log(log(||K - K011，西,
and simplifying completes the proof.
C	THE OPERATOR NORM OF op(K(i))
Let J = K(i) - K0(i). Since || op(K(i))||2 = 1 + || op(J)||2, it suffices to find || op(J)||2.
For the rest of this section, we number indices from 0, let [d] = {0, ..., d - 1}, and define ω =
exp(2πi∕d). To facilitate the application of matrix notation, pad the k × k × c × c tensor J out with
zeros to make a d × d × c × c tensor J.
The following lemma is an immediate consequence of Theorem 6 of Sedghi et al. (2018).
Lemma C.1 (Sedghi et al. (2018)). Let F be the complex d × d matrix defined by Fij = ωij .
For each u,v ∈ [d] × [d] ,let P (u,v) be the C × C matriXgiVen by PkUVv = (F T J：,：,k,'F )uv. Then
||op(J)||2=max||P(u,v)||2.
u,V
First, note that, by symmetry, for each u and v, all components of P (u,V) are the same. Thus,
||P(u,V)||2=C|P0(0u,V)|.	(3)
For any u, v ,
闿U,v)| = X ωupωvq Jp,q,0,0 ≤ ek2
p,q
and P0(00,0) = k2. Combining this with (3) and Lemma C.1, || op(J)||2 = Ck2, which implies
||op(K)||2 = 1 + Ck2.
D Proof of Lemma 3.2
For each convolutional layer i, let βi = || op(K(i)) - op(K0(i))||2, and, for each fully connected
layer i, let γi = ||V (i) - V0(i)||2.
Since ' is λ-Lipschitz w.r.t. its first argument, we have that ∣'(fθ (x), y) - '(fθ (x), y)| ≤ λ∣fθ(χ)-
fθ (x) I ∙ Let gup be the function from the inputs to the whole network with parameters Θ to the inputs
to the convolution in layer j , and let gdown be the function from the output of this convolution to the
output of the whole network, so that fΘ = gdown ◦ fop(K(j)) ◦ gup. Choose an input x to the network,
and let u = gup(x). Recalling that IIxII ≤ χ, and that the non-linearities and pooling operations
are non-expansive, we have kuk ≤ χY op(K(i)) . Using the fact that the non-linearities are
14
Published as a conference paper at ICLR 2020
1-Lipschitz, we have
∣fθ(x) - fθ (x)| = |gdown(op(K (j))u) - gdown(θp(K (j))u)∣
≤(∏∣∣op(K(i))∣D (∏∣∣v (叱) ∣∣op(K (j))u — op(Kj))u∣∣
≤(∏∣∣op(K(i))∣D(∏∣∣v (RHop(K(j)) — op(K(j))∣∣ ∣∣uk
≤ X (YM(K 川(YIIV Ci 川op(K (j)) — op(K (j))∣∣
≤ X (∏(1 + V + βi) (∏(1 + V + Yi)! ∣∣op(K(j)) - op(K(j))∣∣2
where the last inequality uses the fact that || op(K(i)) - op(K0(i))||2 ≤ βi for all i, ||V (i) -V0(i)||2 ≤
γi for all i, || op(K0(i))||2 ≤ 1 + V for all i and ||V0(i) ||2 ≤ 1 + V for all i.
Since Qi6=j(1 + V+ βi) (Qi(1 + V+ γi)) ≤ (Qi(1 + V+βi)) (Qi(1 + V+ γi)), and the latter
is maximized subject to (Pi βi) + Pi Yi ≤ β when each summand is β∕L, this completes the proof.
E Proof of Lemma 3.3
For each convolutional layer i, let βi = || op(K(i)) - op(K0(i))||2, and, for each fully connected
layer i, let Yi = ||V (i) - V0(i)||2.
Since ' is λ-Lipschitz w.r.t. its first argument, we have that ∣'(fθ(x), y) 一 '(fθ(x), y)| ≤ λ∣fθ(χ) 一
fθ (x) |. Let gup be the function from the inputs to the whole network with parameters Θ to the inputs
to fully connected layer layer j , and let gdown be the function from the output of this layer to the
output of the whole network, so that fΘ = gdown ◦ fV (j) ◦ gup. Choose an input x to the network,
and let u = gup(x). Recalling that ||x|| ≤ X, and that the non-linearities and pooling operations are
non-expansive, we have ∣u∣ ≤ X
∏∣∣∣op(K(i))∣∣∣2!(∏∣∣∣V(i)∣∣∣2
i	i<j
. Thus
∣fθ (x) - fθ (x)l = ∣gdown(V (JjU)- gdown(V (j)u)∣
≤ (∏∣∣v (i)∣∣) ∣∣v (j) u -V (j)u∣∣
≤(∏∣∣v Cl) ∣∣v (j) - V (j)∣∣2kuk
≤ x (∏∣"i))∣∣2) (∏∣∣v Clj ∣∣v (j)-V (j)∣∣2
≤ X∏(1+ V + βi )! (∏(i+ν+Yi)) ∣∣v(j) - V(j)∣∣2.
Since (Qi(1 + V +βi)) Qi6=j (1 + V +Yi) ≤ (Qi(1 + V+βi)) (Qi(1 + V+ Yi)), and the latter
is maximized subject to (Pi βi) + Pi Yi ≤ β when each summand is β∕L, this completes the proof.
15