Published as a conference paper at ICLR 2020
CERTIFIED ROBUSTNESS FOR TOP-k PREDICTIONS
against Adversarial Perturbations via Ran-
domized Smoothing
Jinyuan Jia, Xiaoyu Cao, Binghui Wang, Neil Zhenqiang Gong
Duke University
{jinyuan.jia,xiaoyu.cao,binghui.wang,neil.gong}@duke.edu
Ab stract
It is well-known that classifiers are vulnerable to adversarial perturbations. To
defend against adversarial perturbations, various certified robustness results have
been derived. However, existing certified robustnesses are limited to top-1 pre-
dictions. In many real-world applications, top-k predictions are more relevant.
In this work, we aim to derive certified robustness for top-k predictions. In par-
ticular, our certified robustness is based on randomized smoothing, which turns
any classifier to a new classifier via adding noise to an input example. We adopt
randomized smoothing because it is scalable to large-scale neural networks and
applicable to any classifier. We derive a tight robustness in `2 norm for top-
k predictions when using randomized smoothing with Gaussian noise. We find
that generalizing the certified robustness from top-1 to top-k predictions faces
significant technical challenges. We also empirically evaluate our method on CI-
FAR10 and ImageNet. For example, our method can obtain an ImageNet classi-
fier with a certified top-5 accuracy of 62.8% when the '2-norms of the adversar-
ial perturbations are less than 0.5 (=127/255). Our code is publicly available at:
https://github.com/jjy1994/Certify_Topk.
1	Introduction
Classifiers are vulnerable to adversarial perturbations (Szegedy et al., 2014; Goodfellow et al., 2015;
Carlini & Wagner, 2017b; Jia & Gong, 2018). Specifically, given an example x and a classifier f, an
attacker can carefully craft a perturbation δ such that f makes predictions for x + δ as the attacker
desires. Various empirical defenses (e.g., Goodfellow et al. (2015); Svoboda et al. (2019); Buckman
et al. (2018); Ma et al. (2018); Guo et al. (2018); Dhillon et al. (2018); Xie et al. (2018); Song et al.
(2018)) have been proposed to defend against adversarial perturbations. However, these empirical
defenses were often soon broken by adaptive adversaries (Carlini & Wagner, 2017a; Athalye et al.,
2018). As a response, certified robustness (e.g., Wong & Kolter (2018); Raghunathan et al. (2018a);
Liu et al. (2018); Lecuyer et al. (2019); Cohen et al. (2019)) against adversarial perturbations has
been developed. In particular, a robust classifier verifiably predicts the same top-1 label for data
points in a certain region around any example x.
In many applications such as recommender systems, web search, and image classification cloud ser-
vice (Clarifai; Google Cloud Vision), top-k predictions are more relevant. In particular, given an
example, a set of k most likely labels are predicted for the example. However, existing certified
robustness results are limited to top-1 predictions, leaving top-k robustness unexplored. To bridge
this gap, we study certified robustness for top-k predictions in this work. Our certified top-k ro-
bustness leverages randomized smoothing (Cao & Gong, 2017; Cohen et al., 2019), which turns any
base classifier f to be a robust classifier via adding random noise to an example. For instance, Cao
& Gong (2017) is the first to propose randomized smoothing with uniform noise as an empirical
defense. We consider random Gaussian noise because of its certified robustness guarantee (Cohen
et al., 2019). Specifically, we denote by pi the probability that the base classifier f predicts label i
for the Gaussian random variable N(x, σ2I). The smoothed classifier gk (x) predicts the k labels
with the largest probabilities pi’s for the example x. We adopt randomized smoothing because it is
scalable to large-scale neural networks and applicable to any base classifier.
1
Published as a conference paper at ICLR 2020
Our major theoretical result is a tight certified robustness bound for top-k predictions when using
randomized smoothing with Gaussian noise. Specifically, given an example x, a label l is verifiably
among the top-k labels predicted by the smoothed classifier gk (X + δ) when the '2-norm of the ad-
versarial perturbation δ is less than a threshold (called certified radius). The certified radius for top-1
predictions derived by Cohen et al. (2019) is a special case of our certified radius when k = 1. As
our results and proofs show, generalizing certified robustness from top-1 to top-k predictions faces
significant new challenges and requires new techniques. Our certified radius is the unique solution
to an equation, which depends on σ, pl, and the k largest probabilities pi’s (excluding pl). However,
computing our certified radius in practice faces two challenges: 1) it is hard to exactly compute the
probability pl and the k largest probabilities pi ’s, and 2) the equation about the certified radius does
not have an analytical solution. To address the first challenge, we estimate simultaneous confidence
intervals of the label probabilities via the Clopper-Pearson method and Bonferroni correction in
statistics. To address the second challenge, we propose an algorithm to solve the equation to obtain
a lower bound of the certified radius, where the lower bound can be tuned to be arbitrarily close
to the true certified radius. We evaluate our method on CIFAR10 (Krizhevsky & Hinton, 2009)
and ImageNet (Deng et al., 2009) datasets. For instance, on ImageNet, our method respectively
achieves approximate certified top-1, top-3, and top-5 accuracies as 46.6%, 57.8%, and 62.8% when
the '2-norms of the adversarial perturbations are less than 0.5 (127/255) and σ = 0.5.
Our contributions are summarized as follows:
•	Theory. We derive the first certified radius for top-k predictions. Moreover, we prove our
certified radius is tight for randomized smoothing with Gaussian noise.
•	Algorithm. We develop algorithms to estimate our certified radius in practice.
•	Evaluation. We empirically evaluate our method on CIFAR10 and ImageNet.
2	CERTIFIED RADIUS FOR TOP-k PREDICTIONS
Suppose we have a base classifier f, which maps an example X ∈ Rd to one of c candidate labels
{1,2,…，c}. f can be any classifier. Randomized smoothing (Cohen et al., 2019) adds an isotropic
Gaussian noise N (0, σ2I) to an example X. We denote pi as the probability that the base classifier
f predicts label i when adding a random isotropic Gaussian noise to the example X, i.e., pi =
Pr(f (x + E) = i), where E 〜N(0,σ2I). The smoothed classifier gk(x) returns the set of k
labels with the largest probabilities pi’s when taking an example X as input. Our goal is to derive a
certified radius Rl such that We have l ∈ gk(x + δ) for all ∣∣δ∣∣2 < Ri. Our main theoretical results
are summarized in the following two theorems.
Theorem 1 (Certified Radius for Top-k Predictions). Suppose we are given an example x, an arbi-
trary base classifier f, E 〜N(0, σ2I), a Smoothed classifier g, an arbitrary label l ∈ {1, 2, ∙∙∙ , c},
andpl,Pι, ∙∙∙ ,Pι-ι,Pl+ι,…，p ∈ [0,1] that satisfy thefollowing conditions:
Pr(f (x + E) = l) ≥ P andPr(f(x + E) = i) ≤ pi. ∀i = l,	(1)
where P and P indicate lower and upper bounds of P, respectively. Let Pbk ≥ PbkT ≥ ∙∙∙ ≥ Pbl be
the k largest ones among {Pι, ∙∙∙ ,Pl-1,Pl+1, ∙∙∙ ,Pj, where ties are broken uniformly at random.
Moreover we denote by St = {bι, b2, ∙…，bt} the set of t labels with the smallest probability upper
bounds in the k largest ones and by PSt = Pj=ι Pbj the sum of the t probability upper bounds,
where t = 1, 2,…，k. Then, we have:
l ∈ gk(x + δ),∀∣∣δ∣∣2 <Rl,	(2)
where Rl is the unique solution to the following equation:
ɪ/ɪ-i/ x Rlxx k 以①T(PSj+Rσl)) ∩	zɔʌ
Φ(Φ 1(Pι)-------)) - min----------S----σ— = O,	(3)
一 σ	t=ι	t
where Φ and Φ-1 are the cumulative distribution function and its inverse of the standard Gaussian
distribution, respectively.
Proof. See Appendix A.
□
2
Published as a conference paper at ICLR 2020
Algorithm 1: PREDICT
Input: f, k, σ, x, n, and α.
Output: ABSTAIN or predicted top-k labels.
1	T = 0
2	counts = SAMPLEUNDERNOISE(f, σ, x, n)
3	ci, c2, •…，ck+ι = top-{k + 1} indices in counts (ties are broken uniformly at random)
4	ncι ,nc2, ∙ ∙ ∙ ,nck+ι = CoUnts[ci], counts®],…,counts [ck+i]
5	for t - 1 to k do
6	if BINOMPVALUE(nct , nct + nct+1 , 0.5) ≤ α then
7	I T = T ∪ Ct	―
8	else
9	I return ABSTAIN
10	return T
Theorem 2 (Tightness of the Certified Radius). Assuming we have Pl + Pk=IPbj ≤ 1 andPl +
Ei=I …l-ι l+i …C Pi ≥ L Then，for any perturbation ∣∣δ∣∣2 > Rl, there exists a base classifier f *
consistent with (1) but we have l ∈/ gk(x + δ).
Proof. We show a proof sketch here. Our detailed proof is in Appendix B. In our proof, we first
show that, via mathematical induction and the intermediate value theorem, we can construct k + 1
disjoint regions Ci, i ∈ {l} ∪ {bi, b2,…，bk} that satisfy Pr(X + E ∈ Cl) = pl, Pr(X + E ∈ Ci)= pi,
and Pr(x + δ + E ∈ Ci) is no smaller than some critical value for i ∈ {bi, b2,…,bk}. Moreover,
We divide the remaining region Rd \ (∪i=l,b1,b2,… 及Ci) into C 一 k 一 1 regions, which We denote as
Cbk+1, Cbk+2,…，Cbc-ι and satisfy Pr(x + E ∈ Ci) ≤ Pi for i = bk+i,bk+2,…，bc-i. Then, we
construct a base classifier f * that predicts label i for an example if and only if the example is in the
region Ci, where i ∈ {1, 2,…，c}. As 丝 + Pk=I讥j ≤ 1 and 丝 + Pi=i,…,l-i,l+i,…,°Pi ≥ 1,
f * is well-defined. Moreover, f * satisfies the conditions in (1). Finally, we show that if ∣∣δ∣∣2 > Rl,
then we have Pr(f *(x + δ + E) = l) < min；=] Pr(f *(x + δ + E)= b7-), i.e., l ∈ gk(x + δ).	□
We have several observations about our theorems.
•	Our certified radius is applicable to any base classifier f .
•	According to Equation 3, our certified radius Rl depends on σ, Pl, and the k largest prob-
ability upper bounds {Pbfc ,Pbk_ɪ, ∙∙∙ ,Pb1} excluding Pl. When the lower bound Pl and
the upper bounds {Pbk,Pbk-ι,…，讥】} are tighter, the certified radius Rl is larger. When
Rl < 0, the label l is not among the top-k labels predicted by the smoothed classifier even
if no perturbation is added, i.e., l ∈/ gk (X).
•	When using randomized smoothing with Gaussian noise and no further assumptions are
made on the base classifier, it is impossible to certify a `2 radius for top-k predictions that
is larger than Rl .
•	When k = 1, we have Rl = 2(Φ-i(Pl) 一 Φ-i(PbJ), wherePbl is an upper bound of the
largest label probability excluding Pl. The certified radius derived by Cohen et al. (2019)
for top-1 predictions (i.e., their Equation 3) is a special case of our certified radius with
k = 1, l = A, and bi = B .
3 Prediction and Certification in practice
3.1	Prediction
It is challenging to compute the top-k labels gk (X) predicted by the smoothed classifier, because
it is challenging to compute the probabilities Pi’s exactly. To address the challenge, we resort to
a Monte Carlo method that predicts the top-k labels with a probabilistic guarantee. In particular,
we leverage the hypothesis testing result from a recent work (Hung et al., 2019). Algorithm 1
3
Published as a conference paper at ICLR 2020
shows our PREDICT function to estimate the top-k labels predicted by the smoothed classifier. The
function SAMPLEUNDERNOISE(f, σ, x, n) first randomly samples n noise ∈ι, 3 ∙∙∙ ,5 from the
Gaussian distribution N(0, σ2I), uses the base classifier f to predict the label of x + j for each
j ∈ {1,2,…,n}, and returns the frequency of each label, i.e., counts[i] = P；=i I(f (x + e7∙) = i)
for i ∈ {1, 2, ∙∙∙ , c}. The function BINOMPVALUE performs the hypothesis testing to calibrate the
abstention threshold such that we can bound with probability α of returning an incorrect set of top-k
labels. Formally, we have the following proposition:
Proposition 1. With probability at least 1 -α over the randomness in PREDICT, if PREDICT returns
a set T (i.e., does not ABSTAIN), then we have gk (x) = T.
Proof. See Appendix C.	□
3.2	Certification
Given a base classifier f, an example x, a label l, and the standard deviation σ of the Gaussian
noise, we aim to compute the certified radius Rl . According to our Equation 3, our Rl relies on
a lower bound of Pl, i.e., Pl, and the upper bound of PSt, i.e., PS古,which are related to f, x, and
σ. We first discuss two Monte Carlo methods to estimate Pl and PSt with probabilistic guarantees.
However, given Pl and PSt, it is still challenging to exactly solve Rl as the Equation 3 does not have
an analytical solution. To address the challenge, We design an algorithm to obtain a lower bound
of Rl via solving Equation 3 through binary search. Our lower bound can be tuned to be arbitrarily
close to Rl .
3.2.1	ESTIMATING Pl AND PS=
Our approach has two steps. The first step is to estimate Pl and Pi for i = l. The second step is to
estimate PSt using Pi for i = l.
Estimating Pl and Pi for i = l: The probabilities P1,P2,…，Pc can be viewed as a multinomial
distribution over the labels {1,2,…，c}. If we sample a Gaussian noise E uniformly at random,
then the label f(x + ) can be viewed as a sample from the multinomial distribution. Therefore,
estimating Pl and Pi for i = l is essentially a one-sided simultaneous confidence interval estimation
problem. Inparticular, we aim to estimate these bounds with a confidence level at least 1 - α. In
statistics, Goodman (1965); Sison & Glaz (1995) are well-known methods for simultaneous confi-
dence interval estimations. However, these methods are insufficient for our problem. Specifically,
Goodman’s method is based on Chi-square test, which requires the expected count for each label to
be no less than 5. We found that this is usually not satisfied, e.g., ImageNet has 1,000 labels, some of
which have close-to-zero probabilities and do not have more than 5 counts even ifwe sample a large
number of Gaussian noise. Sison & Glaz’s method guarantees a confidence level of approximately
1 - α, which means that the confidence level could be (slightly) smaller than 1 - α. However, we
aim to achieve a confidence level of at least 1 - α. To address these challenges, we discuss two
confidence interval estimation methods as follows:
1)	BinoCR This method estimates Pl using the standard one-sided Clopper-Pearson method and
treats Pi as Pi = 1 - Pl for each i = l. Specifically, we sample n random noise from N(0, σI2),
i.e., ci,% ∙∙∙ ,en. We denote the count for the label l as nl = P；=i I(f (x + e7∙) = l). nl follows
a binomial distribution with parameters n and Pl, i.e., nl 〜Bin(n, Pl). Therefore, according to the
Clopper-Pearson method, we have:
Pl = B(α; nl, n - nl + 1),	(4)
where 1 - α is the confidence level and B(α; u, v) is the αth quantile of the Beta distribution with
shape parameters u and v. We note that the Clopper-Pearson method was also adopted by Cohen
et al. (2019) to estimate label probability for their certified radius of top-1 predictions.
2) SimuEM. The above method estimates Pi as 1 - Pl, which may be conservative. A conserva-
tive estimation makes the certified radius smaller than what it should be. Therefore, we introduce
SimuEM to directly estimate Pi together with Pl. We let n = P；=i I(f (x + Cj) = i) for each
i ∈ {1,2, ∙∙∙ , c}. Each n follows a binomial distribution with parameters n and Pi. We first use
4
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 2: CERTIFY
Input: f, k, σ, x, l, n, μ, and α.
Output: ABSTAIN or R.
counts = SAMPLEUNDERNOISE(f, σ, x, n, a)
[pι,Pι, ∙ ∙ ∙ ,Pι-ι,Pι+ι,…，Pc] = BINOCP(CoUnts, α) or SIMUEM(CoUnts, α)
Ri = 0
for t — 1 to k do
PSt = min(Pj=I 瓦j , * 1 - Pl)
Rlt =BINARYSEARCH( Pl, PSt ,t,σ, μ)
if Rιt > Rι then
I 2=~R
if Rι > 0 then
I return Rι
else
I return ABSTAIN
the Clopper-Pearson method to estimate a one-sided ConfidenCe interval for eaCh label i, and then
we obtain simUltaneoUs ConfidenCe intervals by leveraging the Bonferroni correction. SpeCifiCally,
if We can obtain a confidence interval with confidence level at least 1 -段 for each label i, then Bon-
ferroni correction tells Us that the overall ConfidenCe level for the simUltaneoUs ConfidenCe intervals
is at least 1 - α, i.e., we have confidence level at least 1 - α that all confidence intervals hold at the
same time. Formally, we have the following boUnds by applying the Clopper-Pearson method with
confidence level 1 - C to each label:
Pι = B (Oc； nι,n - n + 1)
Pi = B(1 - α； n + 1,n - n), ∀i = l.
c
(5)
(6)
Estimating PS=: One natural method is to estimate PS==Pj=ιP%. However, this bound may
be loose. For example, when using BinoCP to estimate the probability bounds, we have PSt =
t ∙ (1 - Pι), which may be bigger than 1. To address the challenge, we derive another bound for PSt
from another perspective. Specifically, we have PSt ≤ Pi= Pi ≤ 1 - Pι. Therefore, we can use
1 - Pι as an upper bound of PSt, i.e., PS==1 - Pι. Finally, we combine the above two estimations
by taking the minimal one, i.e., PSt = min(Pj=ιPbj, 1 - Pι).
3.2.2 ESTIMATING A LOWER BOUND OF THE CERTIFIED RADIUS Rι
It is challenging to compute the certified radius Rι exactly because Equation 3 does not have an
analytical solution. To address the challenge, we design a method to estimate a lower bound of
Rι that can be tuned to be arbitrarily close to Rι . Specifically, we first approximately solve the
following equation for each t ∈ {1,2,…，k}:
Φ(Φ-1(pι) - Rt))-虫&-1(PSt) + 阴) =0.	⑺
We note that it is still difficult to obtain an analytical solution to Equation 7 when t > 1. However,
we notice that the left-hand side has the following properties: 1) it decreases as Rιt increases; 2)
when Rιt → -∞, it is greater than 0; 3) when Rιt → ∞, it is smaller than 0. Therefore, there exists
a unique solution Rt to Equation 7. Moreover, we leverage binary search to find a lower bound Rlt
that can be arbitrarily close to the exact solution Rt. In particular, we run the binary search until
the left-hand side of Equation 7 is non-negative and the width of the search interval is less than a
parameter μ > 0. Formally, we have:
Rlt ≤ Rt ≤ Rιt + μ, ∀t ∈ {1, 2,…，k}.	(8)
5
Published as a conference paper at ICLR 2020
8 6 4
CiCiCi
AoeJnOOe P0g-E8
0.0
0	1	2	3	4	5
radius
(a) CIFAR10
8 6 4
CiCiCi
AoeJnOOe PagnJ8
0.0
0	1	2	3	4	5
radius
(b) ImageNet
Figure 1: Impact of k on the certified top-k accuracy.
After obtaining Rit, We let R = maxk=ι R be our lower bound of Rl. Based on Rl = maxk=ι Rt
and Equation 8, we have the following guarantee:
Rl ≤ Rl ≤ R + μ.	⑼
3.2.3 Complete Certification Algorithm
Algorithm 2 shows our algorithm to estimate the certified radius for a given example x and a la-
bel l. The function SAMPLEUNDERNOISE is the same as in Algorithm 1. Functions BINOCP and
S imuEM return the estimated probability bound for each label. Function BinarySearch per-
forms binary search to solve the Equation 7 and returns a solution satisfying Equation 8. Formally,
our algorithm has the following guarantee:
Proposition 2. With probability at least 1 - α over the randomness in CERTIFY, if CERTIFY returns
a radius Rl (i.e., does notABSTAIN),then we have l ∈ gk(x + δ), ∀∣∣δ∣∣2 < Rl.
Proof. See Appendix D.
□
4 Experiments
4.1 Experimental Setup
Datasets and models: We conduct experiments on the standard CIFAR10 (Krizhevsky & Hinton,
2009) and ImageNet (Deng et al., 2009) datasets to evaluate our method. We use the publicly
available pre-trained models from Cohen et al. (2019). Specifically, the architectures of the base
classifiers are ResNet-110 and ResNet-50 for CIFAR10 and ImageNet, respectively.
Parameter setting: We study the impact of k, the confidence level 1 - α, the noise level σ, the
number of samples n, and the confidence interval estimation methods on the certified radius. Unless
otherwise mentioned, we use the following default parameters: k = 3, α = 0.001, σ = 0.5,
n = 100,000, and μ = 10-5. Moreover, we use SimuEM to estimate bounds of label probabilities.
When studying the impact of one parameter on the certified radius, we fix the other parameters to
their default values.
Approximate certified top-k accuracy: For each testing example x whose true label is l, we
compute the certified radius Rl using the Certify algorithm. Then, we compute the certified top-
k accuracy at a radius r as the fraction of testing examples whose certified radius are at least r.
Note that our computed certified top-k accuracy is an approximate certified top-k accuracy instead
of the true certified top-k accuracy. However, we can obtain a lower bound of the true certified
top-k accuracy based on the approximate certified top-k accuracy. Appendix E shows the details.
Moreover, the gap between the lower bound of the true certified top-k accuracy and the approximate
top-k accuracy is negligible when α is small. For convenience, we simply use the term certified
top-k accuracy in the paper.
6
Published as a conference paper at ICLR 2020
8 6 4
CiCiCi
Aoeinooe Pθy-E8
8 6 4
CiCiCi
&e」n8e P0g-Eg
Figure 2: Impact of the confidence level 1 一
2	3
radius
(a) CIFAR10
4	5
8 6 4
CiCiCi
&e」n8e PagnJg
2	3
radius
(b) ImageNet
4	5
a on the certified top-3 accuracy.
—σ = 0.25
σ=0.50
σ=1.00
radius
(a) CIFAR10
Figure 3: Impact of σ on the certified top-3 accuracy.
8 6 4
CiCiCi
Aoeinooe PaynJ8
2	3
radius
(b) ImageNet
4	5
8 6 4
CiCiCi
&e」n8e Pag-E90
BinoCP
SimuEM
8 6 4
CiCiCi
&e」n8e P0≡七 əo
BinoCP
SimuEM
2
radius
2
radius
(a) CIFAR10
(b) ImageNet
Figure 4: BinoCP vs. SimuEM, where k = 3.
4.2 Experimental results
Figure 1 shows the certified top-k accuracy as the radius r increases for different k. Naturally, the
certified top-k accuracy increases as k increases. On CIFAR10, we respectively achieve certified
top-1, top-2, and top-3 accuracies as 45.2%, 58.8%, and 67.2% When the '2-norm of the adversarial
perturbation is less than 0.5 (127/255). On ImageNet, we respectively achieve certified top-1, top-3,
and top-5 accuracies as 46.6%, 57.8%, and 62.8% when the '2-norm of the adversarial perturbation
is less than 0.5. On CIFAR10, the gaps betWeen the certified top-k accuracy for different k are
smaller than those between the top-k accuracy under no attacks, and they become smaller as the
radius increases. On ImageNet, the gaps between the certified top-k accuracy for different k remain
similar to those between the top-k accuracy under no attacks as the radius increases. Figure 2 shows
the influence of the confidence level. We observe that confidence level has a small influence on
the certified top-k accuracy as the different curves almost overlap. The reason is that the estimated
confidence intervals of the probabilities shrink slowly as the confidence level increases. Figure 3
shows the influence of σ. We observe that σ controls a trade-off between normal accuracy under
no attacks and robustness. Specifically, when σ is smaller, the accuracy under no attacks (i.e., the
accuracy when radius is 0) is larger, but the certified top-k accuracy drops more quickly as the radius
7
Published as a conference paper at ICLR 2020
increases. Figure 4 compares BinoCP with SimuEM. The results show that SimuEM is better when
the certified radius is small, while BinoCP is better when the certified radius is large. We found the
reason is that when the certified radius is large, Pl is relatively large, and thus 1 - Pl already provides
a good estimation for pi, where i = l.
5	Related Work
Numerous defenses have been proposed against adversarial perturbations in the past several years.
These defenses either show robustness against existing attacks empirically, or prove the robustness
against arbitrary bounded-perturbations (known as certified defenses).
5.1	Empirical defenses
The community has proposed many empirical defenses. The most effective empirical defense is
adversarial training (Goodfellow et al., 2015; Kurakin et al., 2017; Tramer et al., 2018; Madry et al.,
2018). However, adversarial training does not have certified robustness guarantees. Other examples
of empirical defenses include defensive distillation (Papernot et al., 2016), MagNet (Meng & Chen,
2017), PixelDefend (Song et al., 2017), Feature squeezing (Xu et al., 2018), and many others (Liu
et al., 2019; Svoboda et al., 2019; Schott et al., 2019; Buckman et al., 2018; Ma et al., 2018; Guo
et al., 2018; Dhillon et al., 2018; Xie et al., 2018; Song et al., 2018; Samangouei et al., 2018; Na
et al., 2018; Metzen et al., 2017). However, many of these defenses were soon broken by adaptive
attacks (Carlini & Wagner, 2017a; Athalye et al., 2018; Uesato et al., 2018; Athalye & Carlini,
2018).
5.2	Certified defenses
To end the arms race between defenders and adversaries, researchers have developed certified de-
fenses against adversarial perturbations. Specifically, in a certifiably robust classifier, the predicted
top-1 label is verifiably constant within a certain region (e.g., '2-norm ball) around an input exam-
ple, which provides a lower bound of the adversarial perturbation. Such certified defenses include
satisfiability modulo theories based methods (Katz et al., 2017; Carlini et al., 2017; Ehlers, 2017;
Huang et al., 2017), mixed integer linear programming based methods (Cheng et al., 2017; Lomuscio
& Maganti, 2017; Dutta et al., 2017; Fischetti & Jo, 2018; Bunel et al., 2018), abstract interpreta-
tion based methods (Gehr et al., 2018; Tjeng et al., 2018), and global (or local) Lipschitz constant
based methods (Cisse et al., 2017; Gouk et al., 2018; Tsuzuku et al., 2018; Anil et al., 2019; Wong
& Kolter, 2018; Wang et al., 2018a;b; Raghunathan et al., 2018a;b; Wong et al., 2018; Dvijotham
et al., 2018a;b; Croce et al., 2018; Gehr et al., 2018; Mirman et al., 2018; Singh et al., 2018; Gowal
et al., 2018; Weng et al., 2018; Zhang et al., 2018). However, these methods are not scalable to large
neural networks and/or make assumptions on the architectures of the neural networks. For example,
these defenses are not scalable/applicable to the complex neural networks for ImageNet.
Randomized smoothing was first proposed as an empirical defense (Cao & Gong, 2017; Liu et al.,
2018) without deriving the certified robustness guarantees. For instance, Cao & Gong (2017) pro-
posed randomized smoothing with uniform noise from a hypercube centered at an example. Lecuyer
et al. (2019) was the first to prove the certified robustness guarantee of randomized smoothing for
top-1 predictions. Their results leverage differential privacy. Subsequently, Li et al. (2018) further
leverages information theory to improve the certified radius bound. Cohen et al. (2019) obtains
a tight certified radius bound for randomized smoothing with Gaussian noise by leveraging the
Neyman-Pearson Lemma. Pinot et al. (2019) theoretically demonstrated the robustness to adversar-
ial attacks of randomized smoothing when adding noise from Exponential family distributions and
devised an upper bound on the adversarial generalization gap of randomized neural networks. Lee
et al. (2019) generalized randomized smoothing to discrete data. Salman et al. (2019) employed ad-
versarial training to improve the performance of randomized smoothing. Unlike the other certified
defenses, randomized smoothing is scalable to large neural networks and applicable to arbitrary clas-
sifiers. Our work derives the first certified robustness guarantee of randomized smoothing for top-k
predictions. Moreover, we show that our robustness guarantee is tight for randomized smoothing
with Gaussian noise.
8
Published as a conference paper at ICLR 2020
6	Conclusion
Adversarial perturbation poses a fundamental security threat to classifiers. Existing certified de-
fenses focus on top-1 predictions, leaving top-k predictions untouched. In this work, we derive
the first certified radius under `2 -norm for top-k predictions. Our results are based on randomized
smoothing. Moreover, we prove that our certified radius is tight for randomized smoothing with
Gaussian noise. In order to compute the certified radius in practice, we further propose simultane-
ous confidence interval estimation methods as well as design an algorithm to estimate a lower bound
of the certified radius. Interesting directions for future work include 1) deriving a tight certified ra-
dius under other norms such as '1 and '∞, 2) studying which noise gives the tightest certified radius
for randomized smoothing, and 3) studying certified robustness for top-k ranking.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for insightful reviews. This work was supported by NSF grant
No. 1937786.
References
Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In Inter-
national Conference on Machine Learning, 2019.
Anish Athalye and Nicholas Carlini. On the robustness of the cvpr 2018 white-box adversarial
example defenses. The Bright and Dark Sides of Computer Vision: Challenges and Opportunities
for Privacy and Security (CV-COPS), 2018.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning,pp. 274-283, 2018.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. In ICLR, 2018.
Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda. A unified
view of piecewise linear neural network verification. In Advances in Neural Information Process-
ing Systems, pp. 4790-4799, 2018.
Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating evasion attacks to deep neural networks via
region-based classification. In Proceedings of the 33rd Annual Computer Security Applications
Conference, pp. 278-287. ACM, 2017.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, pp. 3-14. ACM, 2017a.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017b.
Nicholas Carlini, Guy Katz, Clark Barrett, and David L Dill. Provably minimally-distorted adver-
sarial examples. arXiv preprint arXiv:1709.10207, 2017.
Chih-Hong Cheng, Georg NUhrenberg, and Harald Ruess. Maximum resilience of artificial neural
networks. In International Symposium on Automated Technology for Verification and Analysis,
pp. 251-268. Springer, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 854-863. JMLR. org, 2017.
Clarifai. https://www.clarifai.com/demo. July 2019.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
9
Published as a conference paper at ICLR 2020
Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu net-
works via maximization of linear regions. In Proceedings of the 22nd International Conference
on Artificial Intelligence and Statistics, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi,
Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial de-
fense. In ICLR, 2018.
Souradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan, and Ashish Tiwari. Output range analysis
for deep neural networks. arXiv preprint arXiv:1709.09130, 2017.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned ver-
ifiers. arXiv preprint arXiv:1805.10265, 2018a.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and Pushmeet Kohli.
A dual approach to scalable verification of deep networks. In UAI, pp. 550-559, 2018b.
Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Verification and Analysis, pp. 269-286. Springer,
2017.
Matteo Fischetti and Jason Jo. Deep neural networks and mixed integer linear optimization. Con-
straints, 23:296-309, 2018.
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Mar-
tin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpreta-
tion. In 2018 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Leo A Goodman. On simultaneous confidence intervals for multinomial proportions. Technometrics,
7(2):247-254, 1965.
Google Cloud Vision. https://cloud.google.com/vision/. July 2019.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks
by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for
training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial
images using input transformations. In ICLR, 2018.
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural
networks. In International Conference on Computer Aided Verification, pp. 3-29. Springer, 2017.
Kenneth Hung, William Fithian, et al. Rank verification for exponential families. The Annals of
Statistics, 47(2):758-782, 2019.
Jinyuan Jia and Neil Zhenqiang Gong. AttriGuard: A practical defense against attribute inference
attacks via adversarial machine learning. In USENIX Security Symposium, 2018.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An
efficient smt solver for verifying deep neural networks. In International Conference on Computer
Aided Verification, pp. 97-117. Springer, 2017.
10
Published as a conference paper at ICLR 2020
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
International Conference on Learning Representations, 2017.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In IEEE Symposium on Security and
Privacy (SP), 2019.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. Tight certificates of adversarial
robustness for randomly smoothed classifiers. In Advances in Neural Information Processing
Systems,pp. 4911-4922, 2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack and
certifiable robustness. arXiv preprint arXiv:1809.03113, 2018.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via
random self-ensemble. In Proceedings of the European Conference on Computer Vision (ECCV),
pp. 369-385, 2018.
Xuanqing Liu, Yao Li, Chongruo Wu, and Cho-Jui Hsieh. Adv-bnn: Improved adversarial defense
through robust bayesian neural network. In ICLR, 2019.
Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu
neural networks. arXiv preprint arXiv:1706.07351, 2017.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. In ICLR, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
pp. 135-147. ACM, 2017.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial
perturbations. In ICLR, 2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning, pp. 3575-3583,
2018.
Taesik Na, Jong Hwan Ko, and Saibal Mukhopadhyay. Cascade adversarial machine learning regu-
larized with a unified embedding. In ICLR, 2018.
Jerzy Neyman and Egon Sharpe Pearson. On the problem of the most efficient tests of statistical
hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing
Papers of a Mathematical or Physical Character, 231(694-706):289-337, 1933.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016.
Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, Cedric Gouy-
Pailler, and Jamal Atif. Theoretical evidence for adversarial robustness through randomization.
In Advances in Neural Information Processing Systems, pp. 11838-11848, 2019.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018a.
11
Published as a conference paper at ICLR 2020
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp.
10877-10887, 2018b.
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and
Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
Advances in Neural Information Processing Systems, pp. 11289-11300, 2019.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. In ICLR, 2018.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially
robust neural network model on mnist. In ICLR, 2019.
GagandeeP Singh, Timon Gehr, Matthew Mirman, Markus PuscheL and Martin Vechev. Fast and
effective robustness certification. In Advances in Neural Information Processing Systems, pp.
10802-10813, 2018.
Cristina P Sison and JosePh Glaz. Simultaneous confidence intervals and samPle size determination
for multinomial ProPortions. Journal of the American Statistical Association, 90(429):366-369,
1995.
Yang Song, TaesuP Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examPles. In ICLR,
2017.
Yang Song, TaesuP Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examPles. In ICLR,
2018.
Jan Svoboda, Jonathan Masci, Federico Monti, Michael M Bronstein, and Leonidas Guibas. Peer-
nets: ExPloiting Peer wisdom against adversarial attacks. In ICLR, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing ProPerties of neural networks. In ICLR, 2014.
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer Programming. In ICML, 2018.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. In ICLR, 2018.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certifi-
cation of perturbation invariance for deep neural networks. In Advances in Neural Information
Processing Systems, pp. 6541-6550, 2018.
Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aaron Oord. Adversarial risk and the
dangers of evaluating against weak attacks. In International Conference on Machine Learning,
pp. 5032-5041, 2018.
Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Mixtrain: Scalable training of formally
robust neural networks. arXiv preprint arXiv:1811.02625, 2018a.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety
analysis of neural networks. In Advances in Neural Information Processing Systems, pp. 6367-
6377, 2018b.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S
Dhillon, and Luca Daniel. Towards fast computation of certified robustness for relu networks.
arXiv preprint arXiv:1804.09699, 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5283-5292, 2018.
12
Published as a conference paper at ICLR 2020
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems, pp. 8400-8409, 2018.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial
effects through randomization. In ICLR, 2018.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. In NDSS, 2018.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural net-
work robustness certification with general activation functions. In Advances in Neural Information
Processing Systems, pp. 4939-4948, 2018.
A Proof of Theorem 1
Given an example x, we define the following two random variables:
X = X + C 〜N(x, σ2I),	(10)
Y = x + δ + C 〜N(x + δ, σ2I),	(11)
where C 〜N(0, σ2I). The random variables X and Y represent random samples obtained by
adding isotropic Gaussian noise to the example x and its perturbed version x + δ, respectively.
Cohen et al. (2019) applied the standard Neyman-Pearson Lemma (Neyman & Pearson, 1933) to
the above two random variables, and obtained the following lemma:
Lemma 1 (Neyman-Pearson for Gaussians with different means). Let X 〜N(x, σ2I), Y 〜N(x+
δ, σ2I), and M : Rd -→ {0, 1} be a random or deterministic function. Then, we have the following:
(1)	IfZ = {z ∈ Rd : δTz ≤ β} for some β and Pr(M (X) = 1) ≥ Pr(X ∈ Z), then Pr(M (Y) =
1) ≥ Pr(Y ∈ Z)
(2)	IfZ = {z ∈ Rd : δTz ≥ β} for some β and Pr(M (X) = 1) ≤ Pr(X ∈ Z), then Pr(M (Y) =
1) ≤ Pr(Y ∈ Z)
Moreover, we have the following lemma from Cohen et al. (2019).
Lemma 2. Given an example x, a number q ∈ [0, 1], and regions A and B defined as follows:
A= {z : δT(z-x) ≤ σ kδk2 Φ-1(q)}	(12)
B= {z : δT(z -x) ≥ σ kδk2 Φ-1(1 - q)}	(13)
Then, we have the following equations:
Pr(X ∈ A) =	q	(14)
Pr(X ∈ B) =	q	(15)
Pr (Y ∈ A) =	Φ(Φ-1(q) -	kδk2)	(16)
σ
Pr (Y ∈ B) = Φ(Φ-1(q) + kδk2)	(17)
σ
Proof. Please refer to Cohen et al. (2019).	□
Based on Lemma 1 and 2, we derive the following lemma:
Lemma 3. Suppose we have an arbitrary base classifier f, an example x, a set of labels which are
denoted as S,two probabilities PS and PS that satisfy PS ≤ PS = Pr (f (X) ∈ S) ≤ PS, and regions
AS and BS defined asfollows:
AS = {z : δτ(Z - x) ≤ σ Mk2 Φ-1(ps)}	(18)
BS = {z ： δτ(z - x) ≥ σ Mb Φ-1(1 - PS)}	(19)
Then, we have:
Pr(X ∈ AS) ≤ Pr(f(X) ∈ S) ≤Pr(X∈BS)	(20)
Pr(Y ∈ AS) ≤ Pr(f(Y) ∈ S) ≤ Pr(Y ∈ BS)	(21)
13
Published as a conference paper at ICLR 2020
Proof. We know that Pr(X ∈ AS) = PS based on Lemma 2. Combined with the condition that
PS ≤ Pr(f (X) ∈ S), We obtain the first inequality in (20). Similarly, We can obtain the second
inequality in (20). We define M(Z) = I(f (Z) ∈ S). Based on the first inequality in (20) and
Lemma 1, we have the following:
Pr(Y ∈ AS) ≤ Pr(M(Y) = 1) = Pr(f(Y) ∈ S),	(22)
which is the first inequality in (21). The second inequality in (21) can be obtained similarly. □
Next, we restate Theorem 1 and show our proof.
Theorem 1 (Certified Radius for Top-k Predictions). Suppose we are given an example x, an arbi-
trary base classifier f, E 〜N(0, σ2I), a smoothed classifier g, an arbitrary label l ∈ {1, 2, ∙∙∙ , c},
andPι,7Pι, ∙∙∙ ,7Pι-ι,7Pι+ι, …，p ∈ [0,1] that satisfy thefollowing conditions:
Pr(f(x + E) = l) ≥ P and Pr (f (x + E) = i) ≤ Pi, ∀i = l,	(1)
where P and P indicate lower and upper bounds of P, respectively. Let Pbk ≥ PbkT ≥ ∙∙∙ ≥ Phγ be
the k largest ones among {P「∙∙∙，Pι-ι,Pι+ι, ∙∙∙ ,Pj, where ties are broken uniformly at random.
Moreover we denote by St = {bι, b2, ∙…，bt} the set of t labels with the smallest probability upper
bounds in the k largest ones and by PS== Pj=ι Pbj the sum of the t probability upper bounds,
where t = 1, 2,…，k. Then, we have:
l ∈ gk(X + δ),∀Uδll2 < Ri,	⑵
where Rl is the unique solution to the following equation:
而,而-1/ 、	RlN	k φ(φ-1(PSt) + Rl))	∩	小
Φ(Φ 1(pi)-------)) - min----------Tt----σ- = 0,	(3)
— σ	t=ι	t
where Φ and Φ-1 are the cumulative distribution function and its inverse of the standard Gaussian
distribution, respectively.
Proof. Roughly speaking, our idea is to make the probability that the base classifier f predicts l
when taking Y as input larger than the smallest one among the probabilities that f predicts for a set
of arbitrary k labels selected from all labels except l. For simplicity, we let Γ = {1, 2, ∙∙∙ , c}\ {l},
i.e., all labels except l. We denote by Γk a set of k labels in Γ. We aim to find a certified radius
Rl such that we have maxΓk⊆Γ mini∈Γk Pr(f (Y) = i) < Pr(f (Y) = l), which guarantees l ∈
gk(x + δ). We first upper bound the minimal probability mini∈Γk Pr(f (Y) = i) for a given Γk, and
then we upper bound the maximum value of the minimal probability among all possible Γk ⊆ Γ.
Finally, we obtain the certified radius Rl via letting the upper bound of the maximum value smaller
than Pr(f (Y) = l).
Bounding mini∈Γk Pr(f (Y) = i) for a given Γk: We use S to denote a non-empty subset of Γk
and use |S| to denote its size. We define PS = Pi∈s Pi, which is the sum of the upper bounds of the
probabilities for the labels in S. Moreover, we define the following region associated with the set S:
BS = {z ： δτ(z - x) ≥ σ kδk2 Φ-1(1 - Ps)}	(23)
We have Pr(f (Y) ∈ S) ≤ Pr(Y ∈ BS) by applying Lemma 3 to the set S. In addition, we have
Pi∈S Pr(f (Y) = i) = Pr(f (Y) ∈ S). Therefore, we have:
X Pr(f (Y) =i) = Pr(f(Y) ∈S) ≤Pr(Y∈BS)
i∈S
Moreover, we have:
min Pr(f (Y) = i) ≤ min Pr(f (Y) = i)
≤ Pi∈s Pr(f(Y) = i)
≤	∣s∣
Pr(Y ∈ BS)
≤ -S-,
(24)
(25)
(26)
(27)
14
Published as a conference paper at ICLR 2020
where we have the first inequality because S is a subset of Γk and we have the second inequality
because the smallest value in a set is no larger than the average value of the set. Equation 27 holds
for any S ⊆ Γk . Therefore, by taking all possible sets S into consideration, we have the following:
min Pr(f (Y) = i) ≤ min
i∈Γk	S⊆Γk
Pr(Y ∈ BS)
|S|
k
min min
t=1 S⊆Γk ,|S|=t
Pr(Y ∈ BS)
(28)
(29)
t
_ k Pr(Y eBsJ
=mmin t ,	(30)
where St is the set of t labels in Γk whose probability upper bounds are the smallest, where ties
are broken uniformly at random. We have Equation 30 from Equation 29 because Pr(Y ∈ BS)
decreases as PS decreases.
Bounding maxΓk⊆r mini∈Γk Pr(f (Y) = i): Since Pr(Y ∈ BSt) increases as IpSt increases,
Equation 30 reaches its maximum value when Γk = {b1,b2,…，bk}, i.e., when Γk is the set of k
labels in Γ with the largest probability upper bounds. Formally, we have:
k Pr(Y ∈ BSt )
max min Pr(f (Y) = i) ≤ min-----------,	(31)
Γk⊆Γ i∈Γk	t=1	t
where St = {b1,b2, ∙∙∙ ,bt}.
Obtaining Rl: According to Lemma 3, we have the following forS = {l}:
Pr(f(Y) =l) ≥ Pr(Y ∈ A{l})	(32)
Recall that our goal is to make Pr(f (Y) = l) > maxΓk⊆Γ mini∈Γk Pr(f(Y) = i). It suffices to let:
Pr(Y ∈ A{l})
> min Pr(Y ∈BSt)
t=1	t
(33)
According to Lemma 2, We have Pr(Y ∈ A{i}) = Φ(Φ-1(pι) - ɪ^tk)) and Pr(Y ∈ BSt)
Φ(Φ-1(pSt) + l^σLk)). Therefore, We have the following constraint on δ:
Φ(Φ-1(pι) -≡2))-
一 σ
mkmφ(φT(pSt) + 呼))> 0.
t=1
(34)
t
Since the left-hand side of the above inequality 1) decreases as ∣∣δ∣∣2 increases, 2) is larger than 0
when ∣∣δ∣∣2 → -∞, and 3) is smaller than 0 when ∣∣δ∣∣2 → ∞, we have the constraint ∣∣δ∣∣2 < Rι,
where Rl is the unique solution to the following equation:
φ(φ-1(Pi)- R))-
min©®-1(PSt) + 萼))=0.
t=1
(35)
□
t
B Proof of Theorem 2
Following the terminology we used in proving Theorem 1, we define a region A{ι} as follows:
A{i} = {z : δτ(z - x) ≤ σ ∣∣δk2 Φ-1(pi)}.	(36)
According to Lemma 2, we have Pr(X ∈ A{i}) = Pl We first show the following lemma, which is
the key to prove our Theorem 2.
Lemma 4. Assuming we have Pl + Pj=I Pbj ≤ L For any perturbation ∣∣δ∣2 > Rl, there exists k
disjoint regions Cbj ⊆ Rd \ A{l}, j ∈ {1,2, ∙∙∙ , k} that satisfy thefollowing:
Pr(X ∈Cbj )= Pbj, ∀j ∈{1, 2,…，k}	(37)
k Pr(Y ∈ BS )
Pr (Y ∈ Cbj) ≥ mi? ( ∈	St), ∀j ∈{1, 2,…，k},	(38)
j	t=1	t
where the random variables X and Y are defined in Equation 10 and 11, respectively; and
{bι, b2,…，bj} and St are defined in Theorem 1.
15
Published as a conference paper at ICLR 2020
Proof. Our proof is based on mathematical induction and the intermediate value theorem. For
convenience, We defer the proof to Appendix B.1.	□
Next, we restate Theorem 2 and show our proof.
Theorem 2 (Tightness of the Certified Radius). Assuming we have Pl + P：=i P分 ≤ 1 and Pl +
∑2i=ι …l-ι l+1 …C Pi ≥ L Then，for any perturbation ∣∣δ∣∣2 > Rl, there exists a base classifier f *
consistent with (1) but we have l ∈/ gk (x + δ).
Proof. Our idea is to construct a base classifier such that l is not among the top-k labels predicted
by the smoothed classifier for any perturbed example X + δ when ∣∣δ∣∣2 > Rl. First, according
to Lemma 4, we know there exists k disjoint regions Cbj ⊆ Rd \ A{l}, j ∈ {1, 2, ∙∙∙ ,k} that
satisfy Equation 37 and 38. Moreover, we divide the remaining region Rd \ (A{l} ∪jk=1 Cbj) into
C- k -1 regions, which we denote as Cbk+1, Cbk+2, ∙∙∙ , Cbc-ι and satisfy Pr(X ∈ Cbj) ≤ pbj for j ∈
{k +1,k + 2, ∙∙∙ ,c - 1}. Note that b1,b2,…,bc-ι is some permutation of {1,2, ∙ ∙ ∙ ,c}∖{l}. We
can divide the remaining region into such C — k — 1 regions because Pl + Pi=ι …l-ιl+ι …C Pi ≥ 1.
Then, based on these regions, we construct the following base classifier:
f* (X) = l,	ifX∈ A{l}
Ibj, if X ∈ Cbj, ∀j ∈ {1, 2, ∙∙∙ ,c- 1}
Based on the definition of f*, we have the following:
Pr(f *(X) = l) = Pr(X ∈A{l} )= P
Pr(f *(X) = bj )= Pr(X ∈ Cbj) = Pbj ,j = 1,2,…，k
Pr(f*(X) = bj )= Pr(X ∈Cbj) ≤ Pbj ,j = k + 1,k + 2,…，c - 1
(39)
(40)
(41)
(42)
Therefore, f * satisfies the conditions in (1). Next, we show that l is not among the top-k labels
predicted by the smoothed classifier for any perturbed example X + δ when ||δ||2 > Rl. Specifically,
we have:
Pr(f*(Y) = l∣kδ∣2 >Rl)				(43)
=Pr(Y ∈ A{l}| ∣δ∣2 >Rl)	(Definitionoff*)			(44)
<Pr(Y ∈ A{l}| ∣δ∣2 = Rl)	(Pr(Y ∈ A{l} ) increases as	∣δ∣2	decreases)	(45)
= mkn Pr(Y ∈BStlkδ∣2 = Rl) t=1	t	(Definition of Rl )			(46)
< mkn Pr(Y ∈BstM2 >Rl) t=1	t	(Pr(Y ∈ BSt ) increases as	∣δ∣2	increases)	(47)
≤Pr(Y ∈Cbj ∣kδ∣∣2 >Rl)	(Lemma 4)			(48)
=Pr(f*(Y) = bjlkδ∣∣2 >Rl)	(Definition of f*),			(49)
where j = 1, 2,…，k. Since we have found k labels whose probabilities are larger than the proba-
bility of the label l, we have l ∈ gk(x + δ) when ∣∣δk2 > Rl.	□
B.1 Proof of Lemma 4
We first define some key notations and lemmas that will be used in our proof.
Definition 1 (C(q1, q2), C0(q1, q2), rx(q1, q2), ry(q1, q2)). Given two values q1 and q2 that satisfy
0 ≤ q1 < q2 ≤ 1, we define the following region:
C(q1,q2) = {z : σ ∣δ∣2 Φ-1(1 - q2) < δT(z - X) ≤ σ ∣δ∣2 Φ-1(1 - q1)}	(50)
According to Lemma 2, we have:
Pr(X ∈ C(q1, q2)) = q2 - q1,	(51)
where the Gaussian random variable X is defined in Equation 10. Moreover, assuming we have
pairs of (qi, q2), i = 1, 2, 3, .…，where qii, q2 ∈ [q1,q2], ∀i. We define thefollowing region:
C0(q1, q2) = C(q1, q2) \ (∪iC(q1i , q2i )).
(52)
16
Published as a conference paper at ICLR 2020
C0(qι, q2) is the remaining region of C(qι, q2) excluding C(q1, q1), C(q2,q2), ∙∙∙ ∙ Given two values
q1λ and q2λ that satisfy q1 ≤ q1λ ≤ q2λ ≤ q2, we also define the following two functions:
rx(q1λ,q2λ) =Pr(X∈C0(q1,q2)∩C(q1λ,q2λ))	(53)
ry(q1λ,q2λ) =Pr(Y∈C0(q1,q2)∩C(q1λ,q2λ)),	(54)
where the random variables X and Y are defined in Equation 10 and 11, respectively.
Next, we show a key property of our defined functions rx(q1λ, q2λ) and ry(q1λ, q2λ).
Lemma 5. If rx(q1κ, q2κ) ≤ rx(q1λ, q2λ) and q2κ ≥ q2λ (or q1κ ≥ q1λ), then we have ry (q1κ, q2κ) ≤
ry(q1λ,q2λ).
Proof. We consider three scenarios.
Scenario I:	q2κ	≥	q1κ	≥	q2λ	≥	q1λ .	We denote hx and	hy	as the probability densities for the
random variables X and Y, respectively. Then, we have hx(z) =(√2∏σ )d eχp(-Pd=12σi-xi)2)
and hy (z) = (√1∏σ)d exp(- PiT(Z2-2xi-δi) ). Therefore, the ratio of the probability density of Y
and the probability density of X at a given point z is as follows:
，xp(
δτ(z-x)一处
2σ2 )
σ2
(55)
Next, we compare the ratio for the points in different regions and have the following:
hy(z|z ∈ C(q1,q2)) hx(z|z ∈ C0(qK,qK)) ≤ exp(σ 间2 φ-1(1-qκ) -kδk2) σκ	2σκ ≤ exp(σ 间2 φ-1(1-qλ) -kδkK) σκ	2σκ ≤ hy(ZIZ ∈ C'(q1,q2)) 一hx(z∣z ∈C0(q1 ,qλ))	(56) (57) (58) (59)
The Equation 57 from 56 is based on Equation 55 and the fact that δT(z -x) ≤ σ kδk2 Φ-1(1 - q1κ)
for any point z in the region C0(q1κ, q2κ) from Definition 1. Similarly, we can obtain Equation 59
from 58. We note that the Equation 58 from 57 is because q2λ ≤ q1κ . Based on Equation 57 and 58,
we know that there exists a real number u such that:
σ 间2 φ](1-qκ)
σ2
kδk2	σ
—2σ2) ≤U ≤ exp(—
∣∣δ∣∣2 Φ-1(1 - qλ)	∣∣δ∣∣2
2σ2 )
σ2
(60)
Combining the Equation 56, 57, and 60, we have the following:
hy(z|z ∈ C0(qK,qK)) ≤ U ∙ hχ(z∣z ∈ C0(qK, q2)	(61)
Taking an integral on both sides of the Equation 61 in the region C0(q1κ, q2κ) and recalling the defini-
tion ofrx(q1κ, q2κ) and ry(q1κ, q2κ), we have the following:
ry (q1, qκ ) ≤ U ∙ rχ(qκ, q2κ)	(62)
Similarly, we have:
u ∙ rχ(qλ,q2) ≤ Ty(q1, q2')	(63)
Based on Equation 62, 63, and the condition that rx(q1κ, qκκ) ≤ rx(q1λ, qκλ), we have the following:
ry (q1κ, qκκ) ≤ ry (q1λ, qκλ)
(64)
Scenario II: qκκ ≥ qκλ ≥ q1κ ≥ q1λ . We have:
rx(q1κ,qκκ)	= rx(q1κ, qκλ)	+rx(qκλ,qκκ)	≤ rx(q1λ,q1κ) + rx(q1κ, qκλ)	= rx(q1λ, qκλ)	(65)
17
Published as a conference paper at ICLR 2020
(66)
(67)
(68)
Therefore, we have the following equation:
rx (q2λ , q2κ) ≤ rx (q1λ , q1κ)
Similar to Scenario I, we know that there exists u such that:
hy(ZIZ ∈ C(q2,q2) ≤ U ≤ hy(ZIZ ∈ C0(qλ,qκ)
hχ(z∣z ∈ CIq, qξ)) — — hχ(z∣z ∈ C0(qλ, q1)
Similar to Scenario I, we have the following based on Equation 66:
ry (q2λ, q2κ) ≤ ry (q1λ, q1κ)
Therefore, we have the following:
ry(q1κ, q2κ) = ry(q1κ, q2λ) + ry(q2λ, q2κ) ≤ ry(q1λ, q1κ) + ry(q1κ, q2λ) = ry(q1λ, q2λ)	(69)
Scenario III: qK ≥ qλ ≥ qλ ≥ qK. As rχ(qK,qK) ≤ rχ(qλ,qλ), we have rχ(qK,qK) = rχ(qλ,qλ).
Therefore, we have ry(qK,q2) = ry(qλ,qλ).	□
Next, we list the well-known Intermediate Value Theorem and show several other properties of our
defined functions rx(q1λ, q2λ) and ry(q1λ, q2λ).
Lemma 6 (Intermediate Value Theorem). If a function F is continuous at every point in the interval
[a, b] and (F(a) 一 V) ∙ (F(b) 一 V) ≤ 0, then there exists X such that F(X) = V.
Roughly speaking, the Intermediate Value Theorem tells us that if a continuous function has values
no larger and no smaller (or no smaller and no larger) than V at the two end points of an interval,
respectively, then the function takes value V at some point in the interval.
Lemma 7. Given two probabilities qx, qy, if we have:
qx ≤ rx(q1, q2)	(70)
Then, there exists q10 , q20 ∈ [q1, q2] such that:
rx (q1, q2) = qx	(71)
rx (q1, q2) = qx	(72)
Furthermore, if we have:
Cry(qι, q2) 一 qy) Yry(q1, q2) — qy) ≤ 0,	(73)
then there exists q100, q200 ∈ [q1, q2] such that:
rx (q100, q200 ) = qx	(74)
ry (q100, q200) = qy	(75)
Proof. We define function F(x) = rχ(qι,χ). Then, we have (F(qι) — qχ) ∙ (F(q2) — qx) ≤ 0
since F(q1) = 0 ≤ qx and F(q2) = rx(q1, q2) > qx based on Equation 70. Therefore, according to
Lemma 6, there exists q10 ∈ [q1, q2] such that:
rx(q1, q20 ) = qx	(76)
Similarly, we can prove that there exists q20 ∈ [q1, q2] such that rx(q10 , q2) = qx.
For any	q2e	∈	[q20 ,	q2],	we define	H(X)	= rx (X, q2e).	Then, we know H(q1)	= rx (q1,	q2e)	≥
rx(q1, q20 ) = qx since q2e ≥ q20 . Moreover, we have H(q2e) = 0 ≤ qx. Therefore, we have
(H(qι) — qχ) ∙ (H(q2) — qx) ≤ 0. According to Lemma 6,we know there exists qf ∈ [qι, q2e] such
that rx(q1e, q2e) = qx for arbitrary q2e ∈ [q20 , q2]. We define G(X) = ry(q1e, X) where X ∈ [q20 , q2],
and q1e are a value such that rx(q1e, X) = qx for a given X. When X = q20 , we can let q1e = q1 since
rx(q1, q20 ) = qx, and when X = q1, we can let q1e = q20 since rx(q20 , q2) = qx. Based on Equation 73
and Lemma 6, we know that there exists X ∈ [q20 , q2] such that G(X) = qy. Therefore, there exists
q100 and q200 such that:
rx (q100, q200) = qx	(77)
ry (q100, q200) = qy	(78)
□
18
Published as a conference paper at ICLR 2020
Lemma 8. Assuming we have qλ = 0, qλ = PSt. If qλ = PSt ≤ mini q1, then we have the
following:
rχ(qλλ, q2) = q2 - q1 = PSt	(79)
ry(q1λ,q2λ) =Pr(Y∈BSt)	(80)
Proof. If q2λ ≤ mini q1i, then we haveC0(q1, q2)∩C(q1λ, q2λ) = C(q1λ, q2λ) since no region is excluded.
Therefore, wehavery(q1λ, q2λ) = Pr(Y ∈ C0(q1, q2)∩C(q1λ, q2λ)) = Pr(Y ∈ C(q1λ, q2λ)) = q2λ -q1λ =
pst based on Equation 51. We note that C(qλ, qλ) = BSt When qλ = 0 and qλ = PS=.Therefore,
We can obtain Equation 80 based on the definition of ry (qλ, qλ) from Definition 1.	口
Lemma 9. If we have q1 ≤ q2o ≤ q2, then we have the following:
Ty(qi,qθ) ≥「( ry(qi,q2)-oπ	(81)
2	drx(q1, q2)/rx(q1,q2o)e
Proof. By applying Lemma 5.	口
We further generalize Lemma 5 to tWo regions. Specifically, We have the folloWing lemma:
Lemma 10. Assuming we have a region Cw ⊆ C(qW, qW) and we have C0(q1,q2) ∩ C(qW, qW) = 0.
Ifq1 ≥ q1w, q2 ≥ q2w and rx(q10 , q20 ) ≤ Pr(X ∈ Cw), then we have:
ry (q1, q2 ) ≤ Pr(Y ∈ Cw )	(82)
Proof. We let q1 = max(q1, q2w). As C0(q1, q2) ∩ C(q1w, q2w) = 0 and q1 ≥ q1w, We can obtain the
conclusion by applying Lemma 5 on C0(q1, q2) ∪ Cw.	口
Next, We restate Lemma 4 and shoW our proof.
Lemma 4. Assuming we have Pl + Pk=I Pbj ≤ 1. For any perturbation ∣∣δk2 > Rl, there exists k
disjoint regions Cbj ⊆ Rd \ A{l}, j ∈ {1,2, ∙∙∙ , k} that satisfy thefollowing:
Pr (X ∈Cbj)= Pbj, ∀j ∈{1, 2,…，k}	(37)
k Pr(Y ∈ BS )
Pr (Y ∈ Cbj) ≥ min ( ∈ St), ∀j ∈{1, 2,…，k},	(38)
j	t=1	t
where the random variables X and Y are defined in Equation 10 and 11, respectively; and
{bι, b2,…，bk} and St are defined in Theorem 1.
Proof. Our proof leverages Mathematical Induction, Which contains tWo steps. In the first step, We
shoW that the statement holds initially. In the second step, We shoW that if the statement is true for
the mth iteration, then it also holds for the (m + 1)th iteration. Without loss of generality, We assume
T = argmink=ι Pr(Y∈BSt). Therefore, we have the following:
∀i = τ, Pr(Y ∈BSi) ≥ Pr(Y ∈BSτ)	(83)
iτ
Recall the definition of BSτ and we have the following:
BST = {z : δT(Z — x) ≥ σ kδ∣2 φ-1(1 - pSt )},	(84)
where Pst = Pj∈Ssτ Pj. We can split BSk into two parts: BST and BSk \ Bst . We will show that
∀j ∈ [1, τ], we can find disjoint Cbj ⊆ BSτ whose union is BSτ such that:
Pr(X ∈Cbj )= Pbj
Pr(Y ∈Cbj ) = Pr(Y^
jτ
(85)
(86)
19
Published as a conference paper at ICLR 2020
For the other part, we will show that ∀j ∈ [τ + 1, k], we can find disjoint Cbj ⊆ BSk \ BSτ whose
union is BSk \ BSτ such that:
Pr(X ∈ “I= Pbj	(87)
Pr(Y ∈ Cbj) ≥ Pr(Y ∈Bsτ)	(88)
jτ
We first show that ∀j ∈ [1, τ], we can find Cbj ⊆ BSτ that satisfy Equation 85 and 86. Since our
proof leverages Mathematical Induction, we iteratively construct each Cbj , ∀j ∈ [1, τ]. Specifically,
we first show that we can find Cbτ ⊆ BSτ that satisfies the requirements. Then, assuming we can
find Cbτ, ∙∙∙ , Cbτ-m+ι, We show that We can find 最…⊆ BST \ (∪j=τ-m+ι0bj). We will leverage
Lemma 7 to prove the existence for each Cbj . Next, we show the two steps.
Step I: We show that we can find Cbτ ⊆ BSτ that satisfies Equation 85 and 86. We let q1 = 0 and
q2 = PST, and we define the following region:
C0(q1,q2)= C (0,Psτ)= BST	(89)
We have:
rχ(qι, q2) = Pr(X ∈ C0(qι,q2)) = PST	(9O)
ry(q1,q2)=Pr(Y∈C0(q1,q2)) =Pr(Y∈BST),	(91)
which can be directly obtained as C0(q1,q2) = Bst. As We have PbT ≤ rχ(q1,q2) = PST =
PT=1 Pbj, there exist q'γ = PST - PbT, q2 = PbT SUCh that:
rχ(qι,q2) = PbT	(92)
rx(q1 ,q2) = PbT	(93)
Moreover, we have the following:
Ty(qι,q2) ≥	Ty(q1,Pb1) =	Pr(Y ∈ C(q1,Pb1)) = Pr(Y	∈ Bsi)	≥	Pr(Y	∈	BST)	(94)
The equality in the middle is from Lemma 8, the left inequality is because q2 = PbT ≥ P^, and the
right ineqUality is from EqUation 83. FUrthermore, we have the following:
Ty(q10,q2)	(95)
=Pr(Y∈C0(q1,q2)∩C(q10,q2))	(96)
=Pr(Y ∈C(Pst — Pbτ,Pst))	(97)
=Pr(Y ∈ C(0,Pst)) — Pr(Y ∈ C(0,PSt — PbT))	(98)
=Pr(Y ∈ C(0,Pst)) — Pr(Y ∈ C(0,PSt-i))	(99)
=Pr(Y ∈BST) — Pr(Y ∈BST-1)	(100)
≤Pr(Y ∈Bst ) — (T- I) ∙ PT(Y S	(101)
=Pr(Y ∈Bst) T	(102)
We obtain Equation 100 from Equation 99 based on Lemma 8, and Equation 101 from Equation 100
based on Equation 83. Therefore, we have the following:
(Ty (q1 ,q2) - Pr(Y ∈bst )) ∙ (Ty (qι, q2) — Pr(Y ; 8S)) ≤ 0	(103)
Thus, there exists (qT,屯)such that Tχ(qT,屯)=PbT, Ty (q[,屯)= Pr(YTBST) based on Lemma 7.
Then, we have the following based on the definition of Tx , Ty :
Pr(X ∈ C0(q1, q2) ∩ C(qT,q))=Pbτ	(104)
Pr(Y ∈C0(q1,q2) ∩c(qT,qT)) = Pr(Y ∈bst)	(105)
τ
20
Published as a conference paper at ICLR 2020
Finally, we let Cbτ = C0(q1, q2) ∩ C(q1τ, q2τ), which meets our goal.
Step II： Assuming We can find {(qT, qT), (q[-1, q2-1),…，(q[-m+1, qj-m+1)} (∀j ∈ [τ - m +	
1, τ], q1 ≤ q1j ≤ q2j ≤ q2) Where 1 ≤ m ≤ τ - 1 such that ∀j ∈ [τ - m + 1, τ], We have:	
Pr(X EC%)= Pbj	(106)
Pr(Y ∈Cbj ) = P^⅛, jτ	(107)
as Well as the folloWing:	
∀j,t ∈ [τ — m + 1,τ] and j = t,Cbj ∩Cbt = 0	(108)
We denote e = τ - m. We shoW We can find Cbe such that We have:	
Pr(X ∈ Cbe ) = Pbe	(109)
Pr(Y ∈Cbe ) = Pr(Y^	(110)
∀j ∈ [e + 1, τ], Cbe ∩ Cbj = 0	(111)
We let q1 = 0, q2 = PST and denote	
C0(q1,q2) = C(q1, q2) \ ∪jτ=e+1Cbj	(112)
We have the folloWing:	
rx(q1,q2)	(113)
=Pr(X ∈ C0(q1,q2) ∩ C(q1, q2))	(114)
=Pr(X ∈ C(q1, q2)) - Pr(X ∈ ∪jτ=e+1Cbj)	(115)
τ =Pr(X ∈ C(q1, q2)) - X Pr(X∈Cbj)	(116)
j=e+1 ττ	
=X Pbj- X Pbj	(117)
j=1	j=e+1	
e =X Pbj	(118)
j=1	
The Equation 116 from 115 is based on the Equation 108, and the Equation 117 from 116 is based	
the Equation 51 and 106. Furthermore, We have the folloWing:	
ry(q1,q2)	(119)
=Pr(Y ∈ C0(q1,q2) ∩ C(q1, q2))	(120)
=Pr(Y∈C(q1,q2))-Pr(Y∈∪jτ=e+1Cbj)	(121)
τ =Pr(Y ∈ C(q1, q2)) - X Pr(Y∈Cbj)	(122)
j=e+1	
=Pr(Y ∈Bst)- m . Pr(Y ∈BSτ) Tτ	(123)
=(T — m) ∙ Pr(Y ∈ BST)	(124)
τ
The Equation 123 from 122 is because C(q1,q2) = BST and the Equation 107. We have Pbe ≤
rx(q1, q2). Therefore, based on Lemma 7, there exist q10 , q20 such that:
rχ(qι,q2) = Pbe	(125)
rχ(q1 ,q2) = Pbe	(126)
We have:
ry(q1,q20)
(127)
21
Published as a conference paper at ICLR 2020
1
≥ry(qι,q2) ∙
drx(q1,q2)/rx(q1, q20)e
≥ry(qι,q2) ∙
1
d(pe=i pbj/pb」
≥ Pr(Y ∈Bsτ)
τ
(128)
(129)
(130)
Equation 128 from 127 is based on Lemma 9, Equation 129 from 128 is based on Equation 90 and
125, and Equation 130 from 129 is obtained from EqUation91 and the fact that d(Pe=ι P%)∕PbJ ≥
τ . Next, we show:
ry(q1 ,q2) ≤
(131)
In particular, we consider two scenarios.
Scenario 1). q10 ≥ minjτ=τ -m+1 q1j . We denote t = argminjτ=τ -m+1 q1j . We let Cw = Cbt ⊆
C(qt,qt). As q'ι ≥ q1 ,q2 ≥ qt and Tχ(q1, q2) = Pbe ≤ Pr(X ∈ Cw) = pttt, We have the following
based on Lemma 10:
ry(q1 ,q2) ≤ Pr(Y ∈ Cw) = Pr(Y ∈BSτ)	(132)
Scenario 2). q10 < minjτ=τ -m+1 q1j . We have the following:
C0(q1,q2) ∩ C(q1, q10) = C(q1,q10)	(133)
Furthermore, we have:
rx(q1,q10)	(134)
=rx(q1,q2) - rx(q10, q2)	(135)
e
=X Pbj- Pbe	(136)
j=1
e-1
=X Pbj	(137)
j=1
(138)
(139)
(140)
(141)
(142)
(143)
(144)
Moreover, we have rx(q1, q10 ) = q10 - q1 from Lemma 8. The above two should be equal. Thus, we
have q1 = Pe-I Pbj = Psτ-m_1 since e = T - m. we have:
ry(q10,q2)
=ry(q1, q2) - ry(q1,q10)
=ry(q1,q2) - Pr(Y ∈ BSτ-m-1)
≤ (T - m) ∙ Pr(Y ∈Bsτ) - (T - m - 1) ∙ Pr(Y ∈ BsJ
一	T	T
=Pr(Y ∈Bsτ)
T
We obtain Equation 140 from Equation 139 based on Lemma 8.
Therefore, we have the following in both scenarios:
0	Pr(Y eBsτ)
ry(qι,q2) ≥------T-------
Pr(Y ∈Bsτ)
ry(qi,q2) ≤------T-------
Based on Lemma 7, there exist qee, qee SUCh that rx (qee, q2) = RbT ,ry (qf, qee) = Pr(YTBST). Then, we
have the following based on the definition of rx , ry :
Pr(X ∈ C0(q1, q2) ∩ C(q1, q2))= Pbe	(145)
22
Published as a conference paper at ICLR 2020
Pr(Y ∈ C0(q1, q2) ∩C(q1e,q2e))
Pr(Y ∈Bsτ)
τ
(146)
We let Ce =	C0(q1, q2)	∩	C(q1e, q2e).	From the definition of C0(q1, q2),	we have	∀j	∈	[e	+
1,τ],C0(q1,q2) ∩ Cbj = 0. Thus, we have ∀j ∈ [e + 1,τ],Cbe ∩C% = 0 since Cbe ⊆ C0(q1,q2).
Therefore, we reach our goal by Mathematical Induction, i.e., for ∀j ∈ [1, τ], we have:
	Pr(X ∈Cbj )= Pbj	(147) Pr(Y ∈ Cbj) = Pr(Y ∈BSτ)	(148) jτ
We can also verify that ∪jτ=1 Cbj = BSτ.
Next, we show our proof based on Mathematical Induction for the other part, i.e., BSk \ BSτ . Our
construction process is similar to the above first part but has subtle differences.
Step I: Let q1 =	二 PT=I Pbj and q2 = Pk=I Pbj. We define: C0(q1, q2) = C(q1, q2) = BSk \ BST	(149)
Then, we have:	k rχ(qι, q2) = q2 一 qι = E Pbj	(15O) j=T +1 ry(q1, q2)	(151) =Pr(Y∈C0(q1,q2)∩C(q1,q2))	(152) =Pr(Y ∈ C(0, q2)) — Pr(Y ∈ C(q1,q2))	(153) =Pr(Y ∈Bsk) - Pr(Y ∈ Bs, )	(154) ≥ k ' Pr(Y ∈BSτ) 一 Pr(Y ∈ B S,)	(155) τ =(k - T) ∙ Pr(Y ∈BSτ)	(156) τ
The Equation 150 is based on the fact that C0(q1, q2) = C(q1, q2) and Definition 1, and we obtain
Equation 154 from 155 based on Equation 83. We have Pbk ≤ rχ(q1,q2). Therefore, based on
Lemma 7, We know that there exists q[ = q2 一 Pbk ,q2 = qι + Pbk such that:
rχ(qι,q2) = Pbk	(157)
rx(q1, q2) = Pbk	(158)
We consider two scenarios.
Scenario 1). In this scenario, we consider ry (qj, q2) > Pr(YTBST). We let qf = q1 ,q§ = q2, i.e., we
have Cbk = C(q10, q2) ∩ C0(q1, q2). Then, we have:
Pr(X ∈	Cbk )	=	rχ(q1 ,q2)	= Pbk	(159)
Pr(Y ∈	Cbk)	=	ry(q'ι, q2)	≥ Pr(Y	∈ BST)	(160)
Therefore, we have the following:
Pr(Y ∈C(q1,q2) \Cbk) =Pr(Y ∈ C(q1, q10)) =Pr(Y ∈C(0,q1)) - Pr(Y ∈C (0,qι)) =Pr(Y ∈Bsk-ι) — Pr(Y ∈Bs,) ≥ (k 一 T 一 1) ∙ Pr(Y ∈Bsτ)	(161) (162) (163) (164) (165)
τ
23
Published as a conference paper at ICLR 2020
Scenario 2). In this scenario, We consider ry (q1 ,q2) ≤ Pr(YTBST). We have the following:
ry(q1,q20)
≥ry(qι,q2) ∙
1
drx (qi,q2)/rx(qi,q2 升
≥ (k - T) ∙ Pr(Y ∈Bsτ) ∙ ɪ
τ	k-τ
Pr(Y CBS,)
τ
(166)
(167)
(168)
(169)
We obtain Equation 167 from 166 via Lemma 9, and we obtain Equation 168 from 167 based on
Pk= +1 Pb
Equation 150 to 156 and the fact rx(q1, q2)∕rx(q1, q2) = —ZfI-j ≥ k-τ. We have (ry (q01 ,q2)-
pbτ +1
Pr(YTBST)) ∙ (ry(qi, q2) - Pr(YTBST)) ≤ 0. Therefore, from Lemma 7, we know that there exist
(q1k , q2k ) such that:
Pr(X ∈Cbk )= rχ(qk ,qk )= Pbk	(170)
Pr(Y ∈ Cbk ) = Ty(qk, qk) = Pr(Y ∈ BSτ)	(171)
We also have the following:
Pr(Y ∈C(q1,q2) \Cbk)	(172)
=ry(q1,q2) - ry(q1k,q2k)
≥ (k - T - 1) ∙ Pr(Y ∈Bst )
τ
(173)
(174)
Similarly, we let Cbk = C0(q1, q2) ∩C(q1k, q2k).
Based on the conditions of our constructions in the two scenarios, we know that if Pr(Y ∈ Bbk ) >
Pr(YTBST), then we have qk = q2.
Step II: We show that if we can find (q1k, q2k), ∙ ∙ ∙ , (q1k-m+1, q2k-m+1) where m ∈ [1, k - T - 1]
and Cbj, ∀j ∈ [k, k - m + 1] such that:
Pr(X ∈Cbj= Pbj	(175)
Pr(Y ∈ Cbj) ≥ Pr(Y ∈BSτ)	(176)
jT
Pr(Y ∈ C(qι, q2) \ ∪k=k-m+1Cbt) ≥ (k - T - m) TPr(Y ∈BSτ)	(177)
Then, we can find (q1k-m, q2k-m) such that:
Pr(X ∈Cbk-m )= Pbk-m	(178)
Pr(Y ∈Cbk-m) ≥ Pr(Y ∈BSτ)	(179)
Pr(Y ∈C(q1,q2) ∖∪k=k-mCbt) ≥ (k-T-m - 1τ) ∙ Pr(Y ∈BSτ)	(180)
For simplicity, we denote e = k 一 m, we let qι = P；=i Pbj and q2 = Pk=ι P%, and we define:
	C0(q1, q2) = C(q1, q2) \ ∪jk	=e+1Cbj	(181)
Then, we have:			
rx(q1,q2)	Pr(X ∈ C(q1, q2) \ ∪jk=e+1Cbj) =	e X Pbj	(182)
		j=T+1	
ry(q1,q2)	= Pr(Y ∈C(q1,q2)∖∪k=e+1Cbj) ≥	(k — T — m) ∙ Pr(Y ∈ BST) T	(183)
24
Published as a conference paper at ICLR 2020
We havePbe ≤ rχ(qι, q2). Therefore, based on Lemma 7, We know that there exists q1, q2 such that:
rχ(q1 ,qw) = Pbe rχ(qι,q2) = Pbe	(184) (185)
Similarly, we consider two scenarios: Scenario 1). In this scenario, we consider that the following holds:	
Pr(Y ∈Bst ) ry(qi,qw) >	τ		(186)
We let q1e = q10, qwe = qw, i.e., Cbe = C(q10 , qw) ∩ C0(q1, qw). Then, we have:	
Pr(X ∈ Cbe) = rx(q1 ,qw) = Pbe	(187)
Pr(Y ∈ Cbe) = Ty (q1, qw ≥ Pr(Y ∈BSτ)	(188)
We note that we have q10 ≤ minjk=e+1 q1i in this scenario. Otherwise, Equation 186 will not hold
based on Lemma 10. We give a short proof.
Assuming q10	> minjk=k-m+1	q1j.	We denote w = argminjk=k-m+1	q1j.	We let	Cw	=	Cbw	⊆
C(q1w, q2w ). Note that in this case, we have q2w < q2 because q2w = q2 and q10 > q1w cannot hold
at the same time as long as ry (q1 ,q2) > 0. Thus, we have Pr(Y ∈ Cw) = Pr(YTBST) because if
Pr(Y ∈ Cw) > Pr(YTBST), we have qw = q2. As we have q1 > qw,q2 > qw and rχ(q1 ,q2)=
Pbe ≤ Pr(X ∈ Cw) = PbJ We have the following based on Lemma 10:
ry (q1 ,qw) ≤ Pr(Y ∈ Cw) = Pr(Y ∈BS)	(189)
Since Equation 186 and Equation 189 cannot hold at the same time, the assumption q10 >
minjk=k-m+1 q1j must be wrong. Therefore, we have q10 ≤ minjk=k-m+1 q1j.
Based on q10 ≤ minjk=k-m+1 q1j, we have the following:
C0(q1,qw) ∩ C(q1, q10) = C(q1,q10)	(190)
Therefore, we have rx(q1, q10 ) = q10 - q1 from Definition 1. Moreover, we have the following:
rx(q1,q10)	(191)
=rx(q1,qw) - rx(q10, qw)	(192)
e
=X Pbj- pe	(193)
j=T +1
e-1
=X Pbj	(194)
j=T +1
The above two should be equal. Therefore, we have q10 = Pje=-T1+1 Pbj + q1 = Pje-=11 Pbj . Recall
that we let Cbe = C0(q1, qw) ∩ C(q10 , qw). Thus, we have:
Pr(Y ∈C(qi,qw) ∖∪k=eCbj )
=Pr(Y ∈C0(q1,q2 )∖Cbe)
=Pr(Y ∈ C(q1, q10))
=Pr(Y∈C(0,q10))-Pr(Y∈C(0,q1))
=Pr(Y∈BSe-1)-Pr(Y∈BST)
≥ (k - T - m - 1) ∙ Pr(Y ∈ Bs,)
τ
Scenario 2). In this scenario, we consider that the following holds:
Pr(Y ∈Bst )
Ty3,qw) ≤-------τ------
(195)
(196)
(197)
(198)
(199)
(200)
(201)
25
Published as a conference paper at ICLR 2020
Note that we have:
ry(q1,q20)		(202)
≥ry(qι,q2) ∙	1 drx (qi,q2)/rx(qi,q2)]	(203)
≥ry(qι,q2) ∙	1 k-T-m	(204)
≥ Pr(Y ∈BSτ)		(205)
We obtain Equation 203 from 202 via Lemma 9, and we obtain Equation 204 from 203 based on
v^~' e	—
Equation 183 and the fact rχ(qι, q2)∕rχ(q1, q2) =	j=τ+1~~j ≥ k - T - m. We have (ry (q1, q2)-
pbτ +1
Pr(YTBST))∙ (ry m, q2) - Pr(YTBST)) ≤ 0. Based on Lemma 7, We can find (q1, qξ) such that We
have:
Pr(X ∈ C0(q1,q2) ∩C(qe,qe)) = rχ(qe,qe) = Pbe	(206)
Pr(Y ∈ C0(q1, q2) ∩ C(q1, q2 )) = Ty (qf, q2) = Pr(Y WBS)	(207)
We let Cbe = C0(q1, q2) ∩ C(q1e, q2e). We also have the folloWing:
Pr(Y ∈C (q1,q2) ∖∪k=eCbj)
=Pr(Y ∈C0(q1 ,q2)∖Cbe)
=ry(q1,q2) - ry(q1e,q2e)
≥ (k - T - m) ∙ Pr(Y ∈bs,) - Pr(Y ∈ BS,)
一	T	T
≥ (k - T - m - 1) ∙ Pr(Y ∈ BST)
T
(208)
(209)
(210)
(211)
(212)
Similar to Step I, we still hold the conclusion that if Pr(Y ∈ Cbe) > Pr(YTBST), We have q2 =
q2 . Then, We can apply Mathematical Induction to reach the conclusion. Also, We can verify
∪k=τ +1Cbj = BSk ∖BSτ.	口
C Proof of Proposition 1
The function SAMPLEUNDERNOISE(f, k, σ, x, n, α) Works as folloWs: We first draW n random
noise from N(0,σ2I), i.e., ei,g, ∙∙∙ ,tn. Then, we compute the values: ∀i ∈ [1,c],ci =
Pjn=1 I(f (x + j) = i). The function BINOMPVALUE(nct, nct + nct+1 ,p) returns the result of
p-value of the two-sided hypothesis test for n% 〜 Bin(n5 + n^+1,p).
Proposition 1: With probability at least 1 -α over the randomness in PREDICT, if PREDICT returns
a set T (i.e., does not ABSTAIN), then we have gk (x) = T .
Proof. We aim to compute the probability that PREDICT returns a set which not equals to gk(x),
which happens if and only ifgk(x) 6= T and PREDICT doesn’t abstain. Specifically, we have:
Pr(PREDICT returns a set 6= gk (x))	(213)
=Pr(gk(x) 6= T, PREDICT doesn’t abstain)	(214)
=Pr(gk (x) = T) ∙ Pr(PREDICT doesn'tabstain|gk (x) = T)	(215)
≤Pr(PREDICT doesn’t abstain|gk(x) 6= T)	(216)
Theorem 1 in Hung et al. (2019) shows the above conditional probability is as follows:
Pr(PREDICT doesn’t abstain|gk(x) 6= T) ≤ α	(217)
Therefore, we reach the conclusion.	□
26
Published as a conference paper at ICLR 2020
D Proof of Proposition 2
Proposition 2: With probability at least 1 - α over the randomness in CERTIFY, if CERTIFY returns
a radius R (i.e., does not ABSTAIN), then We have l ∈ gk(X + δ), ∀ ∣∣δk2 < R.
Proof. From the definition of BINOCP and S IMUEM, we know the probability that the following
inequalities simultaneously hold is at least 1 - α over the sampling of counts:
Pi ≤ PMf(X + E) = i) if i = l
Pi ≥ Pr(f(x + E) = i) if i = l
(218)
(219)
Then, With the returned bounds, We can invoke Theorem 1 to obtain the robustness guarantee if the
calculated radius is larger than 0. Note that otherwise Certify abstains.	□
E CERTIFIED TOP-k ACCURACY
We show how to derive a lower bound of the certified top-k accuracy based on the approximate
certified top-k accuracy. The process is similar to that Cohen et al. (2019) used to derive a lower
bound of the certified top-1 accuracy based on the approximate certified top-1 accuracy. Specifically,
we have the following lemma from Cohen et al. (2019).
Lemma 11. Let zi be a binary variable and Yi be a Bernoulli random variable. Suppose if zi = 1,
then Pr(Yi = 1) ≤ α. Then, for any ρ > 0, with probability at least 1 - ρ, we have the following:
m
_ X Zi ≥
m
i=1
1	(pm=ι Y
1 — α
-α
12α(1 — α) log
m
log( ρ)
3m
(220)
m
1
P
—
Proof. Please refer to Cohen et al. (2019).
□
Assuming we have a test dataset Dtest = {(x1,y1), (x2,y2), ∙∙∙ , (xm, ym)} as well as a radius r.
We define the following indicate value:
ai = I(yi ∈ gk(xi + δ)),∀∣∣δ∣∣2 < r	(221)
Then, the certified top-k accuracy of the smoothed classifier g at radius r can be computed as
* Pm=I ai. For each sample Xi, we run the Certify function with 1 — α confidence level and
we use a random variable Yi to denote that the function CERTIFY returns a radius bigger than r.
From Proposition 2, we know:
Pr(Yi = 1) ≤ α, if ai = 1	(222)
The approximate certified top-k accuracy of the smoothed classifier at radius r is m Pm=I 匕.Then,
we can use Lemma 11 to obtain a lower bound of * Pm=I a，i. Specifically, for any ρ > 0, with
probability at least 1 — ρ over the randomness of CERTiFY, we have:
-α-J
m
一 X ai ≥
m
i=1
1	(Pm=I Yi
1 — α(
2α(1 — α) log ɪ log(P)
-------------------------
3m
(223)
m
m
)
We can see that the difference between the certified top-k accuracy and the approximate certified
top-k accuracy is negligible when α is small.
27