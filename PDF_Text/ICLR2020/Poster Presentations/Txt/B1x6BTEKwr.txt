Published as a conference paper at ICLR 2020
Piecewise linear activations substantially
SHAPE THE LOSS SURFACES OF NEURAL NETWORKS
Fengxiang He*, Bohan Wang*t & Dacheng Tao
UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering
The University of Sydney
Darlington, NSW 2008, Australia
{fengxiang.he, dacheng.tao}@sydney.edu.au, bhwangfy@gmail.com
Ab stract
Understanding the loss surface of a neural network is fundamentally important
to the understanding of deep learning. This paper presents how piecewise linear
activation functions substantially shape the loss surfaces of neural networks. We
first prove that the loss surfaces of many neural networks have infinite spurious
local minima which are defined as the local minima with higher empirical risks
than the global minima. Our result demonstrates that the networks with piecewise
linear activations possess substantial differences to the well-studied linear neural
networks. This result holds for any neural network with arbitrary depth and arbi-
trary piecewise linear activation functions (excluding linear functions) under most
loss functions in practice. Essentially, the underlying assumptions are consistent
with most practical circumstances where the output layer is narrower than any hid-
den layer. In addition, the loss surface of a neural network with piecewise linear
activations is partitioned into multiple smooth and multilinear cells by nondiffer-
entiable boundaries. The constructed spurious local minima are concentrated in
one cell as a valley: they are connected by a continuous path, on which empirical
risk is invariant. Further for one-hidden-layer networks, we prove that all local
minima in a cell constitute an equivalence class; they are concentrated in a valley;
and they are all global minima in the cell.
1	Introduction
Neural networks have been successfully deployed in many real-world applications (LeCun et al.,
2015; Witten et al., 2016; Silver et al., 2016; He et al., 2016; Litjens et al., 2017). In spite of this,
the theoretical foundations of neural networks are somewhat premature. To the many deficiencies
in our knowledge of deep learning theory, the investigation into the loss surfaces of neural networks
is of fundamental importance. Understanding the loss surface would be helpful in several relevant
research areas, such as the ability to estimate data distributions, the optimization of neural networks,
and the generalization to unseen data.
This paper studies the role of the nonlinearities in activation functions in shaping the loss surfaces
of neural networks. Our results demonstrate that the impact of nonlinearities is profound.
First, we prove that the loss surfaces of nonlinear neural networks are substantially different to those
of linear neural networks, in which local minima are created equal, and also, they are all global
minima (Kawaguchi, 2016; Baldi & Hornik, 1989; Lu & Kawaguchi, 2017; Freeman & Bruna,
2017; Zhou & Liang, 2018; Laurent & von Brecht, 2018; Yun et al., 2018). By contrast,
Neural networks with arbitrary depth and arbitrary piecewise linear activa-
tions (excluding linear functions) have infinitely many spurious local minima un-
der arbitrary continuously differentiable loss functions.
*Both authors contributed equally.
tBohan Wang is also affiliated With University of Science and Technology of China. This work was Com-
pleted when he was a summer intern at UBTECH Sydney AI Centre, School of Computer Science, Faculty of
Engineering, the University of Sydney.
1
Published as a conference paper at ICLR 2020
This result only relies on four mild assumptions that cover most practical circumstances: (1) the
training sample set is linearly inseparable; (2) all training sample points are distinct; (3) the output
layer is narrower than the other hidden layers; and (4) there exists some turning point in the piece-
wise linear activations that the sum of the slops on the two sides does not equal to 0.
Our result significantly extends the existing study on the existence of spurious local minimum. For
example, Zhou & Liang (2018) prove that one-hidden-layer neural networks with two nodes in the
hidden layer and two-piece linear (ReLU-like) activations have spurious local minima; Swirszcz
et al. (2016) prove that ReLU networks have spurious local minima under the squared loss when
most of the neurons are not activated; Safran & Shamir (2018) present a computer-assisted proof
that two-layer ReLU networks have spurious local minima; a recent work (Yun et al., 2019b) have
proven that neural networks with two-piece linear activations have infinite spurious local minima,
but the results only apply to the networks with one hidden layer and one-dimensional outputs; and
a concurrent work (Goldblum et al., 2020) proves that for multi-layer perceptrons of any depth, the
performance of every local minimum on the training data equals to a linear model, which is also
verified by experiments.
The proposed theorem is proved in three stages: (1) we prove that neural networks with one hidden
layer and two-piece linear activations have spurious local minima; (2) we extend the conditions to
neural networks with arbitrary hidden layers and two-piece linear activations; and (3) we further ex-
tend the conditions to neural networks with arbitrary depth and arbitrary piecewise linear activations.
Since some parameters of the constructed spurious local minima are from continuous intervals, we
have obtained infinitely many spurious local minima. At each stage, the proof follows a two-step
strategy that: (a) constructs an infinite series of local minima; and (b) constructs a point in the pa-
rameter space whose empirical risk is lower than the constructed local minimum in Step (a). This
strategy is inspired by Yun et al. (2019b) but we have made significant and non-trivial development.
Second, we draw a “big picture” for the loss surfaces of nonlinear neural networks. Soudry & Hoffer
(2018) highlight a smooth and multilinear partition of the loss surfaces of neural networks. The
nonlinearities in the piecewise linear activations partition the loss surface of any nonlinear neural
network into multiple smooth and multilinear open cells. Specifically, every nonlinear point in the
activation functions creates a group of the non-differentiable boundaries between the cells, while the
linear parts of activations correspond to the smooth and multilinear interiors. Based on the partition,
we discover a degenerate nature of the large amounts of local minima from the following aspects:
•	Every local minimum is globally minimal within a cell. This property demonstrates that
the local geometry within every cell is similar to the global geometry of linear networks,
although technically, they are substantially different. It applies to any one-hidden-layer
neural network with two-piece linear activations for regression under convex loss. We rig-
orously prove this property in two stages: (1) we prove that within every cell, the empirical
risk R is convex with respect to a variable W mapped from the weights W by a mapping
Q. Therefore, the local minima with respect to the variable W are also the global minima in
the cell; and then (2) we prove that the local optimality is maintained under the constructed
mapping. Specifically, the local minima of the empirical risk TR with respect to the param-
eter W are also the local minima with respect to the variable W. We thereby prove this
property by combining the convexity and the correspondence of the minima. This proof is
technically novel and non-trivial, though the intuitions are natural.
•	Equivalence classes and quotient space of local minimum valleys. All local minima in
a cell are concentrated as a local minimum valley: on a local minimum valley, all local
minima are connected with each other by a continuous path, on which the empirical risk is
invariant. Further, all these local minima constitute an equivalence class. This local minima
valley may have several parallel valleys that are in the same equivalence class but do not
appear because of the restraints from cell boundaries. If such constraints are ignored, all
the equivalence classes constitute a quotient space. The constructed mapping Q is exactly
the quotient map. This result coincides with the property of mode connectivity that the
minima found by gradient-based methods are connected by a path in the parameter space
with almost invariant empirical risk (Garipov et al., 2018; Draxler et al., 2018; Kuditipudi
et al., 2019). Additionally, this property suggests that we would need to study every local
minimum valley as a whole.
2
Published as a conference paper at ICLR 2020
•	Linear collapse. Linear neural networks are covered by our theories as a simplified case.
When all activations are linear, the partitioned loss surface collapses to one single cell, in
which all local minima are globally optimal, as suggested by the existing works on linear
networks (Kawaguchi, 2016; Baldi & Hornik, 1989; Lu & Kawaguchi, 2017; Freeman &
Bruna, 2017; Zhou & Liang, 2018; Laurent & von Brecht, 2018; Yun et al., 2018).
Notations. If M is a matrix, Mi,j denotes the (i, j)-th component of M. If M is a vector, Mi
denotes the i-th component of M. Define Eij as a matrix in which the (i, j)-th component is 1
while all other components are 0. Also, denote ei as a vector such that the i-th component is 1 while
all others are 0. Additionally, we define 1k ∈ Rk×1 is a vector whose components are all 1, while
those of 0n×m ∈ Rn×m (or briefly, 0) are all 0. For the brevity, [i : j] denotes {i, ∙∙∙ ,j}.
2	Related work
Some works suggest that linear neural networks have no spurious local minima. Kawaguchi (2016)
proves that linear neural networks with squared loss do not have any spurious local minimum under
three assumptions about the data matrix X and the label matrix Y : (1) both matrices XXT and
XYT have full ranks; and (2) the input layer is wider than the output layer; and (3) the eigenvalues
of matrix YX> XXT -1 XYT are distinct with each other. Zhou & Liang (2018) give an analytic
formulation of the critical points for the loss function of deep linear networks, and thereby obtain
a group of equivalence conditions for that critical point is a global minimum. Lu & Kawaguchi
(2017) prove the argument under one assumption that both matrices X and Y have full ranks, which
is even more restrictive. However, in practice, the activations of most neural networks are not linear.
The nonlinearities would make the loss surface extremely non-convex and even non-smooth and
therefore far different from the linear case.
The loss surfaces of over-parameterized neural networks have some special properties. Choroman-
ska et al. (2015) empirically suggest that: (1) most local minima of over-parameterized networks are
equivalent; and (2) small-size networks have spurious local minima but the probability of finding
one decreases rapidly with the network size. Li et al. (2018) prove that over-parameterized fully-
connected deep neural networks with continuous activation functions and convex, differentiable loss
functions, have no bad strict local minimum. Nguyen et al. (2019) suggest that “sufficiently over-
parameterized” neural networks have no bad local valley under the cross-entropy loss. Nguyen
(2019) further suggests that the global minima of sufficiently over-parameterized neural networks
are connected within a unique valley. Many other works study the convergence, generalization, and
other properties of stochastic gradient descent on the loss surfaces of over-parameterized networks
(Chizat & Bach; Arora et al., 2018; Brutzkus et al., 2018; Du et al., 2019; Soltanolkotabi et al., 2018;
Allen-Zhu et al., 2019a;b; Oymak & Soltanolkotabi, 2019).
Many advances on the loss surfaces of neural networks are focused on other problems. Zhou &
Feng (2018) and Mei et al. (2018) prove that the empirical risk surface and expected risk surface
are linked. This correspondence highlights the value of investigating loss surfaces (empirical risk
surfaces) to the study of generalization (the gap between empirical risks to expected risks). Hanin &
Rolnick (2019) demonstrate that the input space of neural networks with piecewise linear activations
are partitioned by multiple regions, while our work focuses on the partition of the loss surface. Xie
et al. (2017) proves that the training error and test error are upper bounded by the magnitude of the
gradient, under the assumption that the geometry discrepancy of the parameter W is bounded. Sagun
et al. (2016; 2018) present empirical results that the eigenvalues of the Hessian of the loss surface
are two-fold: (1) a bulk centered closed to zero; and (2) outliers away from the bulk. Kawaguchi
& Kaelbling (2020) prove that we can eliminate the spurious local minima by adding one unit per
output unit for almost any neural network in practice. Tian (2017); Andrychowicz et al. (2016);
Soltanolkotabi (2017); Zhong et al. (2017); Brutzkus & Globerson (2017); Tian (2017); Li & Yuan
(2017); Zou et al. (2019); Li & Liang (2018); Du et al. (2018a; 2019); Zhang et al. (2019b); Zhou
et al. (2019); Wang et al. (2019) study the optimization methods for neural networks. Other relevant
works include Sagun et al. (2016; 2018); Nguyen & Hein (2018); Du et al. (2018b); Haeffele & Vidal
(2017); Liang et al. (2018); Wu et al. (2018); Yun et al. (2019a); Zhang et al. (2019a); Kuditipudi
et al. (2019); Garipov et al. (2018); Draxler et al. (2018); He et al. (2019); Kawaguchi & Kaelbling
(2020).
3
Published as a conference paper at ICLR 2020
3	Neural network has infinite spurious local minima
This section investigates the existence of spurious local minima on the loss surfaces of neural net-
works. We find that almost all practical neural networks have infinitely many spurious local minima.
This result stands for any neural network with arbitrary depth and arbitrary piecewise linear activa-
tions excluding linear functions under arbitrary continuously differentiable loss.
3.1	Preliminaries
Consider a training sample set {(X1 , Y1), (X2, Y2), . . . , (Xn, Yn)} of size n. Suppose the dimen-
sions of feature Xi and label Yi are dX and dY , respectively. By aggregating the training sample
set, we obtain the feature matrix X ∈ RdX ×n and label matrix Y ∈ RdY ×n.
Suppose a neural network has L layers. Denote the weight matrix, bias, and activation in the j-
th layer respectively by Wj ∈ Rdj ×dj-1, bj ∈ Rdj, and h : Rdj ×n → Rdj ×n, where dj is the
dimension of the output of the j -th layer. Also, for the input matrix X , the output of the j -th layer
is denoted as the Y(j) and the output of the j-th layer before the activation is denoted as the Y(j),
Y ⑶=Wj Y (j-1) + bilT,	(1)
Y(j) =hWjY(j-1)+bi1Tn .	(2)
The output of the network is defined as follows,
Y = hL (WLhL-1 (WL-IhL-2 Jh (WιX + bilT) ...) + bL-ιlT) + bL 1T) .	(3)
Also, We define Y(0) = X, Y(L) = Y, do = dχ, and dL = dγ. In some situations, We use
Y ([Wi]L=ι，[bi]L=i) to clarify the parameters, as well as Y(j), Y(j), etc.
This section discusses neural netWorks With pieceWise linear activations. A part of the proof uses
tWo-piece linear activations hs- ,s+ Which are defined as folloWs,
hs-,s+ (x) = I{x≤0}s-x + I{x>0}s+x,	(4)
where |s+| = ∣s-∣ and I{.} is the indicator function.
Remark. Piecewise linear functions are dense in the space of continuous functions. In other words,
for any continuous function, we can always find a piecewise linear function to estimate it with
arbitrary small distance.
This section uses continuously differentiable loss to evaluate the performance of neural networks.
Continuous differentiability is defined as follows.
Definition 1 (Continuously differentiable). We call a function f : Rn → R continuously differen-
tiable with respect to the variable x if: (1) the function f is differentiable with respect to x; and (2)
the gradient Vχf (x) ofthefUnction f is continuous with respect to the variable X.
3.2 Main result
The theorem in this section relies on the following assumptions.
Assumption 1. The training data cannot be fit by a linear model.
Assumption 2. All data points are distinct.
Assumption 3. All hidden layers are wider than the output layer.
Assumption 4. For the piece-wise linear activations, there exists some turning point that the sum
of the slops on the two sides does not equal to 0.
To our best knowledge, our assumptions are the least restrictive compared with the relevant works in
the literature. These assumptions are respectively justified as follows: (1) most real-world datasets
are extremely complex and cannot be simply fit using linear models; (2) it is easy to guarantee
that the data points are distinct by employing data cleansing methods; (3) for regression and many
classification tasks, the width of output layer is limited and narrower than the hidden layers; and (4)
this assumption is invalid only for activations like f(x) = a|x|.
Based on these four assumptions, we can prove the following theorem.
4
Published as a conference paper at ICLR 2020
Theorem 1. Neural networks with arbitrary depth and arbitrary piecewise linear activations (ex-
cluding linear functions) have infinitely many spurious local minima under arbitrary continuously
differentiable loss whose derivative can equal 0 only when the prediction and label are the same.
In practice, most loss functions are continuously differentiable and the derivative can equal 0 only
when the prediction and label are the same, such as squared loss and cross-entropy loss (see Ap-
pendix A.1, Lemmas 2 and 3). Squared loss is a standard loss for regression and is defined as the L2
norm of the difference between the ground-truth label and the prediction as follows.
l2 5=2旧-WF.	⑸
Meanwhile, cross-entropy loss is used as a standard loss in multiclass classification, which is defined
as follows. Here, we treat the softmax function as a part of the loss function.
Ice(Y ,Y) = - X ¾ log (p>jY ).	⑹
j=1	k=1 Yi,k
One can also remove Assumption 4, if Assumption 3 is replaced by the following assumption, which
is mildly more restrictive (See a detailed proof in pp. 34-37).
Assumption 5. The dimensions of the layers satisfy that:
d1 ≥ dY + 2,
di ≥ dY + 1, i = 2, . . . , L - 1.
Our result demonstrates that introducing nonlinearities into activations substantially reshapes the
loss surface: they bring infinitely many spurious local minima into the loss surface. This result
highlights the substantial difference from linear neural networks that all local minima of linear neural
networks are equally good, and therefore, they are all global minima (Kawaguchi, 2016; Baldi &
Hornik, 1989; Lu & Kawaguchi, 2017; Freeman & Bruna, 2017; Zhou & Liang, 2018; Laurent &
von Brecht, 2018; Yun et al., 2018).
Some works have noticed the existence of spurious local minima on the loss surfaces of nonlin-
ear neural networks, which however has a limited applicable domain (Choromanska et al., 2015;
Swirszcz et al., 2016; Safran & Shamir, 2018; Yun et al., 2019b). A notable work by Yun et al.
(2019b) proves that one-hidden-layer neural networks with two-piece linear (ReLU-like) activations
for one-dimensional regression have infinitely many spurious local minima under squared loss. This
work first constructs a series of local minima and then prove they are spurious. This idea inspires
some of this work. However, our work makes significant and non-trivial development that extends
the conditions to arbitrary depth, piecewise linear activations excluding linear functions, and contin-
uously differentiable loss.
3.3 Proof skeleton
This section presents the skeleton of the proof. Theorem 1 is proved in three stages. We first prove
a simplified version of Theorem 1 and then extend the conditions in the last two stages. The proof is
partially inspired by Yun et al. (2019b) but the proof in this paper has made nontrivial development
and the results are significantly extended.
Yun et al. (2019b) and our paper both employ the following strategy: (a) construct a series of local
minima based on a linear classifier; and (b) construct a new point with smaller empirical risk and
thereby we prove that the constructed local minima are spurious. However, due to the differences in
the loss function and the output dimensions, the exact constructions of local minima are substantially
different.
Our extensions from Yun et al. (2019b) are three-fold: (1) From one hidden layer to arbitrary depth:
To prove that networks with an arbitrary depth have infinite spurious local minima, we develop a
novel strategy that employs transformation operations to force data flow through the same linear
parts of the activations, in order to construct the spurious local minima; (2) From squared loss to
arbitrary differentiable loss: Yun et al. (2019b) calculate the analytic formations of derivatives of
5
Published as a conference paper at ICLR 2020
the loss to construct the local minima and then prove they are spurious. This technique cannot
be transplanted to the case of arbitrary differentiable loss functions, because we cannot assume
the analytic formation. To prove that the loss surface under an arbitrary differentiable loss has an
infinite number of spurious local minima, we employ a new proof technique based on Taylor series
and a new separation lemma; and (3) From one-dimensional output to arbitrary-dimensional output:
To prove the loss surface of a neural network with an arbitrary-dimensional output has an infinite
number of spurious local minima, we need to deal with the calculus of functions whose domain
and codomain are a matrix space and a vector space, respectively. By contrast, when the output
dimension is one, the codomain is only the space of real numbers. Therefore, the extension of the
output dimension significantly mounts the difficulty of the whole proof.
Stage (1): Neural networks with one hidden layer and two-piece linear activations.
We first prove that nonlinear neural networks with one hidden layer and two-piece linear activation
functions (ReLU-like activations) have spurious local minima. The proof in this stage further follows
a two-step strategy:
(a)	We first construct local minima of the empirical risk TR (See Appendix A.2, Lemma 4). These
local minimizers are constructed based on a linear neural network which has the same network size
(dimension of weight matrices) and evaluated under the same loss. The design of the hidden layer
guarantees that the components of the output Y⑴ in the hidden layer before the activation are all
positive. The activation is thus effectively reduced to a linear function. Therefore, the local geometry
around the local minima with respect to the weights W is similar to those of linear neural networks.
Further, the design of the output layer guarantees that its output Y is the same as the linear neural
network. This construction helps to utilize the results of linear neural networks to solve the problems
in nonlinear neural networks.
(b)	We then prove that all the constructed local minima in Step (a) are spurious (see Appendix A.2,
Theorem 4). Specifically, we assumed by Assumption 1 that the dataset cannot be fit by a linear
1 1 EI	Γ∙	. 1	1 ∙	X_7 <Γ-> Γ∙ . 1	♦ ♦ 1 ♦ 1 <A	∙ . 1	. . . 1	1 ∙ . ∙	W ♦
model. Therefore, the gradient NYR of the empirical risk R with respect to the prediction Y is
.	1~1	.1	■ .)	Γ∙ .∖	1 ∙	X—7 <A ♦	.	EI	El	♦	1
not zero. Suppose the i-th row of the gradient VY>R is not zero. Then, We use Taylor series and a
preparation lemma (see Appendix A.5, Lemma 7) to construct another point in the parameter space
that has smaller empirical risk. Therefore, we prove that the constructed local minima are spurious.
Furthermore, the constructions involve some parameters that are randomly picked from a continuous
interval. Thus, we constructed infinitely many spurious local minima.
Stage (2) - Neural networks with arbitrary hidden layers and two-piece linear activations.
We extend the condition in Stage (1) to any neural network with arbitrary depth and two-piece linear
activations. The proof in this stage follows the same two-step strategy but has different implemen-
tations:
(a)	We first construct a series of local minima of the empirical risk R (see Appendix A.3, Lemma
5). The construction guarantees that every component of the output Y(i) in each layer before the
activations is positive, which secure all the input examples flow through the same part of the acti-
vations. Thereby, the nonlinear activations are reduced to linear functions. Also, our construction
guarantees that the output Y of the network is the same as a linear network with the same weight
matrix dimensions.
(b)	We then prove that the constructed local minima are spurious (see Appendix A.3, Theorem
5). The idea is to find a point in the parameter space that has the same empirical risk R with the
constructed point in Stage (1), Step (b).
Stage (3) - Neural networks with arbitrary hidden layer and piecewise linear activations.
We further extend the conditions in Stage (2) to any neural network with arbitrary depth and arbitrary
piecewise linear activations. We continue to adapt the two-step strategy in this stage:
(a)	We first construct a local minimizer of the empirical risk R based on the results in Stages (1) and
(2) (see Appendix A.4, Lemma 6). This construction is based on Stage (2), Step (a). The difference
of the construction in this stage is that every linear part in activations can be a finite interval. The
constructed weight matrices use several uniform scaling and translation operations to the outputs
6
Published as a conference paper at ICLR 2020
of hidden layers in order to guarantee that all the input training sample points flow through the
same linear parts of the activations. We thereby reduce the nonlinear activations to linear functions,
effectively. Also, our construction guarantees that the output Y of the neural network equals to that
of the corresponding linear neural network.
(b)	We then prove that the constructed local minima are spurious (see Appendix A.4). We use the
same strategy in Stage (2), Step (b). Some adaptations are implemented for the new conditions.
4 A big picture of the loss s urfac e
This section draws a big picture for the loss surfaces of neural networks. Based on a recent result
by Soudry & Hoffer (2018), we present four profound properties of the loss surface that collectively
characterize how the nonlinearities in activations shape the loss surface.
4.1	Preliminaries
The discussions in this section use the following concepts.
Definition 2 (Open ball and open set). The open ball in H centered at x ∈ H and of radius r > 0
is defined by B(h, r) = {x : kx - hk < r}. A subset A ⊂ H of a space H is called a open set, if
for every point h ∈ A, there exists a positive real r > 0, such that the open ball B(h, r) with center
h and radius r is in the subset A: B(h, r) ⊂ A.
Definition 3 (Interior point and interior). For a subset A ⊂ H ofa space H, a point h ∈ A is called
an interior point of A, if there exists a positive real r > 0, such that the open ball B(h, r) with
center h and radius r is in the subset A: B(h, r) ⊂ A. The set of all the interior points of the set A
is called the interior of the set A.
Definition 4 (Limit point, closure, and boundary). For a subset A ⊂ H ofa space H, a point h ∈ A
is called a limit point, if for every r > 0, the open ball B(h, r) with center h and radius r contains
some point of A: B(h, r) ∩ A = 0. The closure A ofthe set A consists ofthe union ofthe Set A and
all its limit points. The boundary ∂A is defined as the set of points which are in the closure of set A
but not in the interior of set A.
Definition 5 (Multilinear). A function f: X1 ×X2 → Y is called multilinear if for arbitrary x11, x12 ∈
X, x2,x2 ∈ X2, and constants λι, λ2, μι, and μ2, we have
f (λιx1 +λ2X2,μιx2 +μ2X2) = λιμf (x1,X1) + λιμ2f (x；,x2) + λ2μ1f (x1, x2) + λ2μ2f (x2,x2).
Remark. The definition of “multilinear” implies that the domain of any multilinear function f is a
connective and convex set, such as the smooth and multilinear cells below.
Definition 6 (Equivalence class, and quotient space). Suppose X is a linear space. [x] =
{v ∈ X : V 〜x} is an equivalence class, if there is an equivalent relation 〜on [x], such that
for any a,b,c ∈ [x], we have: (1) reflexivity: a 〜a; (2) symmetry: if a 〜b, b 〜a; and (3)
transitivity: if a 〜b and b 〜c, a 〜c. The quotient space and quotient map are defined to be
X/ 〜={{v ∈ X : v 〜x} : X ∈ X} and X → [x], respectively.
4.2	Main results
In this section, the loss surface is defined under convex loss with respect to the prediction Y of the
neural network. Convex loss covers many popular loss functions in practice, such as the squared
loss for the regression tasks and many others based on norms. The triangle inequality of the norms
secures the convexity of the corresponding loss functions. The convexity of the squared loss is
checked in the appendix (see Appendix B, Lemma 8).
We now present four propositions to express the loss surfaces of nonlinear neural networks. These
propositions give four major properties of the loss surface that collectively draw abig picture for the
loss surface.
We first recall a lemma by Soudry & Hoffer (2018). It proves that the loss surfaces of neural
networks have smooth and multilinear partitions.
7
Published as a conference paper at ICLR 2020
Lemma 1 (Smooth and multilinear partition; cf. Soudry & Hoffer (2018)). The loss surfaces of
neural networks of arbitrary depth with piecewise linear functions excluding linear functions are
partitioned into multiple smooth and multilinear open cells, while the boundaries are nondifferen-
tiable.
Based on the smooth and multilinear partition, we prove four propositions as follows.
Theorem 2 (Analogous convexity). For one-hidden-layer neural networks with two-piece linear
activation for regression under convex loss, within every cell, all local minima are equally good,
and also, they are all global minima in the cell.
Theorem 3 (Equivalence classes of local minimum valleys). Suppose all conditions of Theorem 2
hold. Assume the loss function is strictly convex. Then, all local minima in a cell are concentrated
as a local minimum valley: they are connected with each other by a continuous path and have the
same empirical risk. Additionally, all local minima in a cell constitute an equivalence class.
Corollary 1 (Quotient space of local minimum valleys). Suppose all conditions of Theorem 3 hold.
There might exist some “parallel” local minimum valleys in the equivalence class of a local mini-
mum valley. They do not appear because of the constraints from the cell boundaries. If we ignore
such constraints, all equivalence classes of local minima valleys constitute a quotient space.
Corollary 2 (Linear collapse). The partitioned loss surface collapses to one single smooth and
multilinear cell, when all activations are linear.
4.3	Discussions and proof techniques
The four propositions collectively characterize how the nonlinearities in activations shape the loss
surfaces of neural networks. This section discusses the results and the structure of the proofs. A
detailed proof is omitted here and given in Appendix B.
Smooth and multilinear partition. Intuitively, the nonlinearities in the piecewise linear activation
functions partition the surface into multiple smooth and multilinear cells. Zhou & Liang (2018);
Soudry & Hoffer (2018) highlight the partition of the loss surface. We restate it here to make the
picture self-contained. A similar but also markedly different notions recently proposed by Hanin
& Rolnick (2019) demonstrate that the input data space is partitioned into multiple linear regions,
while our work focuses on the partition in the parameter space.
Every local minimum is globally minimal within a cell. In convex optimization, convexity guar-
antees that all the local minima are global minima. This theorem proves that the local minima within
a cell are equally good, and also, they are all global minima in the cell. This result is not surpris-
ing provided the excellent training performance of deep learning algorithms. However, the proof is
technically non-trivial.
Soudry & Hoffer (2018) proved that the local minima in a cell are the same. However, there would
be some point near the boundary has a smaller empirical risk and is not locally minimal. Unfortu-
nately, the proof by Soudry & Hoffer (2018) cannot exclude this possibility. By contrast, our proof
completely solves this problem. Furthermore, our proof holds for any convex loss, including squared
loss and cross-entropy loss, but Soudry & Hoffer (2018) only stands for squared loss.
It is challenging to prove, because the proof techniques for the case of linear networks cannot be
transplanted here. Technically, linear networks can be expressed by the product of a sequence of
weight matrices, which guarantees good geometrical properties. Specifically, the effect of every
linear activation function is just equivalently multiplying a real constant to the output. However, the
loss surface within a cell of a nonlinear neural network does not have this property. Below is the
skeleton of our proof.
We first prove that the empirical risk TR is a convex function within every cell with respect to a
variable W which is calculated from the weights W. Therefore, all local minima of the empirical
risk R with respect to W are also globally optimal in the cell. Every cell corresponds to a specific
series of linear parts of the activations. Therefore, in any fixed cell, the activation hs- ,s+ can be
expressed by the slopes of the corresponding linear parts as the following equations,
nn
R(W1,W2) = - X l (yi,W2h(W1Xi)) = - X l (yi,W2diag (A∙,i) Wιxi),	(7)
n i=1	n i=1
8
Published as a conference paper at ICLR 2020
where A∙,i is the i-th column of matrix
-hS-,s+ ((W1)1,∙X1)…hS-,s+ ((W1)1,∙Xn)'
A =	.	..	.	.
.	..
h0s-,s+ ((W1)d1 ,∙xι)…hS-,s+ ((Wl)dl,∙Xn).
Matrix A is constituted by collecting the slopes of the activation h at every point (Wι)i,∙Xj.
Different elements of the matrix A can be multiplied either one of {s-, s+}. Therefore, we cannot
use a single constant to express the effect of this activation, and thus, even within the cell, a nonlinear
network cannot be expressed as the product of a sequence of weight matrices. This difference
ensures that the proofs of deep linear neural networks cannot be transplanted here.
Then, we prove that (see p. 40)
W2diag (A∙,i) WiXi = ATidiag(W2)W1Xi.	(8)
Applying eq. (8) to eq. (7), the empirical risk TR equals to a formulation similar to the linear neural
networks,
1n
R= n El Vi- ATidiag(W2)WiXi) .	(9)
i=1
Afterwards, define W1 = diag(W2)W1 and then straighten the matrix W1 to a vector W,
W= ((W1)1,∙…(WI)dι,∙),
Define Q : (W1,W2) → W, and also define,
X = (A∙,i 0 xi … A∙,n 0 Xn) .
We can prove the following equations (see p. 41),
(ATlW1 Xi …	ATnW1 Xn) =W X.
Applying eq. (9), the empirical risk is transferred to a convex function as follows,
nn
R =1XI (yi, (A∙,i)T W1Xi) =1X l(yi, W X)∙
n i=1	n i=1
We then prove that the local optimality of the empirical risk RR is maintained when the weights W are
mapped to the variable W. Specifically, the local minima of the empirical risk R with respect to the
weight W are also the local minima with respect to the variable W. The maintenance of optimality
is not surprising but the proof is technically non-trivial (see a detailed proof in pp. 42-43).
Equivalence classes and quotient space of local minimum valleys. The constructed mapping Q is
a quotient map. Under the setting in the previous property, all local minima in a cell is an equivalence
class; they are concentrated as a local minimum valley. However, there might exist some “parallel”
local minimum valley in the equivalence class, which do not appear because of the constraints from
the cell boundaries. Further for neural networks of arbitrary depth, we also constructed a local
minimum valley (the spurious local minima constructed in Section 3). This result explains the
property of mode connectivity that the minima found by gradient-based methods are connected by a
path in the parameter space with almost constant empirical risk, which is proposed in two empirical
works (Garipov et al., 2018; Draxler et al., 2018). A recent theoretical work (Kuditipudi et al., 2019)
proves that dropout stability and noise stability guarantee the mode connectivity.
Linear collapse. Our theories also cover the case of linear neural networks. Linear neural networks
do not have any nonlinearity in their activations. Correspondingly, the loss surface does not have
any non-differentiable boundaries. In our theories, when there is no nonlinearity in the activations,
the partitioned loss surface collapses to a single smooth, multilinear cell. All local minima wherein
are equally good, and also, they are all global minima as follows. This result unites the existing
results on linear neural networks (Kawaguchi, 2016; Baldi & Hornik, 1989; Lu & Kawaguchi, 2017;
Freeman & Bruna, 2017; Zhou & Liang, 2018; Laurent & von Brecht, 2018; Yun et al., 2018).
9
Published as a conference paper at ICLR 2020
5 Conclusion and future directions
This paper reports that the nonlinearities in activations substantially shape the loss surfaces of neural
networks. First, we prove that neural networks have infinitely many spurious local minima which are
in contrast to the circumstance of linear neural networks. This result stands for any neural network
with arbitrary hidden layers and arbitrary piecewise linear activations (excluding linear functions)
under many popular loss functions in practice (e.g., squared loss and cross-entropy loss). This result
significantly extends the conditions of the relevant results and has the least restrictive assumptions
that cover most practical circumstances: (1) the training data is not linearly separable; (2) the training
sample points are distinct; (3) all hidden layers are wider than the output layer; and (4) there exists
some turning point in the piece-wise linear activation that the sum of the slops on the two sides does
not equal to 0. Second, based on a recent result that the loss surface has a smooth and multilinear
partition, we draw a big picture of the loss surface from the following aspects: (1) local minima in
any cell are equally good, and also, they are all global minima in the cell; (2) all local minima in
one cell constitute an equivalence class and are concentrated as a local minimum valley; and (3) the
loss surface collapses to one single cell when all activations are linear functions, which explains the
results of linear neural networks. The first and second properties are rigorously proved for any one-
hidden-layer nonlinear neural networks with two-piece linear (ReLU-like) activations for regression
tasks under convex/strictly convex loss without any other assumption.
Theoretically understanding deep learning is of vital importance to both academia and industry. A
major barrier recognized by the whole community is that deep neural networks’ loss surfaces are
extremely non-convex and even non-smooth. Such non-convexity and non-smoothness make the
analysis of the optimization and generalization properties prohibitively difficult. A natural idea is to
bypass the geometrical properties and then approach a theoretical explanation. We argue that such
“intimidating” geometrical properties are exactly the major factors that shape the properties of deep
neural networks, and also the key to explaining deep learning. We propose to explore the magic
of deep learning from the geometrical structures of its loss surface. Future directions towards fully
understanding deep learning are summarized as follows,
•	Investigate the (potential) equivalence classes and quotient space of local minimum
valleys for deep neural networks. This paper suggests a degenerate nature of the large
amounts of local minima: all the local minima within one cell constitute an equivalence
class. We construct a quotient map for one-hidden-layer neural networks with two-piece
activations for regression. Whether deep neural networks have similar properties remains
an open problem. Understanding the quotient space would be a major step of understanding
the approximation, optimization, and generalization of deep learning.
•	Explore the sophisticated geometry of local minimum valleys. The quotient space of
local minima suggests a strategy that treats every local minimum valley as a whole. How-
ever, the sophisticated local geometrical properties around the local minimum valleys are
still premature, such as the sharpness/flatness of the local minima, the potential categoriza-
tion of the local minimum valley according to their performance, and the volumes of the
local minima valleys from different categories.
•	Tackle the optimization and generalization problems of deep learning. Empirical re-
sults have overwhelmingly suggested that deep learning has excellent optimization and
generalization capabilities, which is, however, beyond the current theoretical understand-
ing: (1) one can employ stochastic convex optimization methods (such as SGD) to min-
imize the extremely non-convex and non-smooth loss function in deep learning, which
is expected to be NP-hard but practically solved by computationally cheap optimization
methods; and (2) heavily-parametrized neural networks can generalize well in many tasks,
which is beyond the expectation of most current theoretical frameworks based on hypoth-
esis complexity and the variants. The sophisticated geometrical expression, if fortunately,
we possess in the future, would be a compelling push to tackle the generalization and opti-
mization muses of deep learning.
Acknowledgments
This work was supported by Australian Research Council Project FL-170100117. The authors sin-
cerely appreciate Micah Goldblum and the anonymous reviewers for their constructive comments.
10
Published as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in Neural Information Processing
Systems, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, 2019b.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in Neural Information Processing Systems, 2016.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning, 2018.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural Networks, 2(1):53-58, l989.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In International Conference on Machine Learning, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations, 2018.
Lenalc Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in Neural Information Processing
Systems.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In International Conference on Artificial Intelligence and
Statistics, 2015.
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers
in neural network energy landscape. In International Conference on Machine Learning, 2018.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, 2018a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent
learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. In International Confer-
ence on Machine Learning, 2018b.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
In International Conference on Learning Representations, 2017.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss
surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information
Processing Systems, 2018.
Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, and Tom Goldstein. Truth or
backpropaganda? an empirical investigation of deep learning theory. In International Conference
on Learning Representations, 2020.
Benjamin D. Haeffele and Rene Vidal. Global optimality in neural network training. In IEEE
Conference on Computer Vision and Pattern Recognition, July 2017.
Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In International
Conference on Machine Learning, 2019.
11
Published as a conference paper at ICLR 2020
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
well: Theoretical and empirical evidence. In Advances in Neural Information Processing Systems,
2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, 2016.
Kenji Kawaguchi and Leslie Pack Kaelbling. Elimination of all bad local minima in deep learning.
In International Conference on Artificial Intelligence and Statistics, 2020.
Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, and
Rong Ge. Explaining landscape connectivity of low-cost solutions for multilayer nets. In Ad-
vances in Neural Information Processing Systems, 2019.
Thomas Laurent and James von Brecht. The multilinear structure of relu networks. In International
Conference on Machine Learning, 2018.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.
Dawei Li, Tian Ding, and Ruoyu Sun. Over-parameterized deep neural networks have no strict local
minima for any continuous activations. arXiv preprint arXiv:1812.11039, 2018.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems 31. 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems 30. 2017.
Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant. Understanding the loss surface
of neural networks for binary classification. In International Conference on Machine Learning,
2018.
Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco
Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I
Sanchez. A survey on deep learning in medical image analysis. Medical Image Analysis, 42:
60-88, 2017.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint
arXiv:1702.08580, 2017.
Song Mei, Yu Bai, Andrea Montanari, et al. The landscape of empirical risk for nonconvex losses.
The Annals of Statistics, 46(6A):2747-2774, 2018.
Quynh Nguyen. On connected sublevel sets in deep learning. In International Conference on
Machine Learning, 2019.
Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. In
International Conference on Machine Learning, 2018.
Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. On the loss landscape of a class
of deep neural networks with no bad local valleys. In International Conference on Learning
Representations, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? In International Conference on Machine Learning, 2019.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, 2018.
Levent Sagun, Leon Bottou, and Yann LeCun. Singularity of the hessian in deep learning. arXiv
preprint arXiv:1611.07476, 2016.
12
Published as a conference paper at ICLR 2020
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis
of the hessian of over-parametrized neural networks. In International Conference on Learning
Representations Workshop, 2018.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information
Processing Systems 30. 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer
neural networks. In International Conference on Learning Representations Workshop, 2018.
Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of
deep networks. arXiv preprint arXiv:1611:06310, 2016.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. In International Conference on Machine
Learning, 2017.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. IEEE Transactions on Signal Processing, 67(9):
2357-2370, 2019.
Ian H Witten, Eibe Frank, Mark A Hall, and Christopher J Pal. Data Mining: Practical machine
learning tools and techniques. Morgan Kaufmann, 2016.
Chenwei Wu, Jiajun Luo, and Jason D Lee. No spurious local minima in a two hidden unit relu
network. In International Conference on Learning Representation Workshop, 2018.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Inter-
national Conference on Artificial Intelligence and Statistics, 2017.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks.
In International Conference on Learning Representations, 2018.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Efficiently testing local optimality and escaping saddles
for reLU networks. In International Conference on Learning Representations, 2019a.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small nonlinearities in activation functions create
bad local minima in neural networks. In International Conference on Learning Representations,
2019b.
Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov. Deep neural networks with multi-branch
architectures are intrinsically less non-convex. In International Conference on Artificial Intelli-
gence and Statistics, 2019a.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu net-
works via gradient descent. In International Conference on Artificial Intelligence and Statistics,
2019b.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In International Conference on Machine Learning, 2017.
Pan Zhou and Jiashi Feng. Empirical risk landscape analysis for understanding deep neural net-
works. In International Conference on Learning Representations, 2018.
Yi Zhou and Yingbin Liang. Critical points of neural networks: Analytical forms and landscape
properties. In International Conference on Learning Representations, 2018.
13
Published as a conference paper at ICLR 2020
Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global
minimum in deep learning via star-convex path. In International Conference on Learning Repre-
sentations, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. Machine Learning, 2019.
14
Published as a conference paper at ICLR 2020
A Proof of Theorem 1
This appendix gives a detailed proof of Theorem 1 omitted from the main text. It follows the skeleton
presented in Section 3.3.
A.1 Squared loss and cross-entropy loss
We first check whether squared loss and cross-entropy loss are covered by the requirements of
Theorem 1.
Lemma 2. The squared loss (defined by eq. 5) is continuously differentiable with respect to the
prediction of the model, whose gradient of loss equal to zero when the prediction and the label are
different.
Proof. Apparently, the squared loss is differentiable with respect to Y. Specifically, the gradient
with respect to Y is as follows,
VYIy - Y『=2 (y -Y),
which is continuous with respect to Y.
Also, when the prediction Y does not equals to the label Y, We have
VYlIy -Y ∣∣2 = 0.
The proof is completed.	□
Lemma 3. The cross-entropy loss eq. (6) is continuously differentiable with respect to the prediction
of the model, whose gradient of loss equal to zero when the prediction and the label are different.
Also, we assume that the ground-truth label is a one-hot vector.
Proof. For any i ∈ [1 : n], the cross-entropy loss is differentiable with respect to Yi. The j-th
component of the gradient with respect to the prediction Yi is as follows,
d (- PI Yk,i log C•
_ ʌ
∂Y∙ ∙
∂Yj,i
kXd=Y1Yk,i
ʌ
eYj,i
dY
(10)
——	√v
P eYk,i
k=1
-
ʌ
which is continuous with respect to Yi. So, the cross-entropy loss is continuously differentiable with
respect to Yi.
Additionally, if the gradient (eq. (10)) is zero, we have the following equations,
dY
eYj,i - Yji X eYk,i = O, j = 1, 2,…，n
k=1
Rewrite it into the matrix form, we have
dY P Yk,i - Y1,i	-Y1,i	…	-Y1,i
k = 1		
-Y2,i .	dY P Yk,i - Y2,i	…	-Y2,i .	.
	k=1 .	
. .	. .	..
-Yd ,i	• ∙ ∙	dY -Yd,i	Yk,i - Yd,i i=1
, eYι,i
eγ2,i
.
.
.
"
eYd ,i
0.
15
Published as a conference paper at ICLR 2020
dY
Since P Yk,i = 1, we can easily check the rank of the left matrix is dY - 1. So the dimension of
k=1
the solution space is one. Meanwhile, we have
dY
P Yk,i - Y1,i	-Y1,i	…	-Y1,i
k = 1
dγ
-Y2,i	P Yk,i - Y2,i	…	-Y2,i
k = 1
-YdY,i
dY
-YdY,i	Yk,i -YdY,i
i=1
「"
Y2,i
.
.
.
YdY,i
Therefore, 0 = eYk,i = λYk,i, for some λ ∈ R, which contradicts to the assumption that some of
the components of Y is 0 (Yi,∙ is a one-hot vector).
The proof is completed.
A.2 S tage (1)
In Stage (1), we prove that deep neural networks with one hidden layer, two-piece linear activation
hs-,s+, and multi-dimensional outputs have infinite spurious local minima.
This stage is organized as follows: (a) we construct a local minimizer by Lemma 4; and (b) we prove
that the local minimizer is spurious in Theorem 4 by constructing a set of parameters with smaller
empirical risk.
Without loss of generality, we assume that s+ 6= 0. Otherwise, suppose that s+ = 0. From the
definition of ReLU-like activation (eq. (4)), we have s- 6= 0. Since
hs-,s+(x) = h
the output of the neural network with parameters
-s+,-s- (-x),
[Wi]iL=1 , [bi]iL=1 and activation hs-,s+ equals
to that of the neural network with parameters [Wi0]iL=1 , [b0i]iL=1 and activation h-s+ ,-s- where
W0 = -Wi, bi = -bi, i = 1,2,…，L - 1 and WL = Wl2L = g SinCe {[用]3 ,也]3 } →
[Wi0]iL=1, [b0i]iL=1 isan one-to-one map, it is equivalent to consider either the two networks, with
h-s+,-s- (x) has non-zero slope when x > 0.
Step (a). Construct local minima of the loss surface.
Lemma 4. Suppose that W is a local minimizer of
1n
f(W )4 n X l U"
xi
1
i=1
Under Assumption 3, any one-hidden-layer neural network has a local minimum at
Wig]
0(d1-dY)×dX
ʌ
,b1
"[叮dx+1 - η1dγ
-η1d1 -dY
and
W2 = [ S+ IdY	0dγ ×(dι-dγ) ] , b2 = η1dγ ,
(11)
(12)
(13)
□
where W1 and bi are respectively the weight matrix and the bias of the first layer, W2 and b2 are
respectively the weight matrix and the bias of the second layer, and η is a negative constant with
absolute value sufficiently large such that
__~	E
WX - η1dγ1n > 0,
(14)
where > is element-wise.
Also, the loss in this lemma is continuously differentiable loss whose gradient does not equals to 0
when the prediction is not the same as the ground-truth label.
16
Published as a conference paper at ICLR 2020
Proof. We show that the empirical risk is higher in the neiborhood of
, in order
to prove that
is a local minimizer.
The output of the first layer before the activation is
Y⑴=WIX + bι1T = [τ-χi:ι-1Υ⅛T].
Because η is a negative constant with absolute value sufficiently large such that eq. (27)) holds,
the output above is positive (element-wise), the output of the neural network with parameters
ʌ_	_	/  ʌ r∏ ∖	ʌ E
Y =W2hs-,s+ (W1X + bιlT) + b2lT
=s+W2 (W1X + bιlT) +b2lT
=s+ h s+ IdY	0dγ ×(dι-dγ) i	WXI-η1dY1Tτ + ηidγ IT
=W X,
where X is defined as
(15)
Therefore, the empirical risk R in terms of parameters {W1, W2, b1, b2} is
R (Wi,w2, bι, Q =1XX l (匕,(W X), ɔ = nXX l (k,w
i=1	,	i=1
f(W).
Then, we introduce a sufficiently small disturbance [δW i]i2=1 , [δbi]i2=1
into the parameters
{ [Wii	, [bj	}. When the disturbance is sufficiently small, all components of the output
of the first layer remain positive. Therefore, the output after the disturbance is
Y ([Wi + δwJ2= ], [bi +
(W2 + δW2) hs-,s+ ((W1
+ δW 1) X + (b1 + δb1) 1T) + (b2 + δb2) 1T
=) (W2 + δw2) s+ ((W1 + δw 1) X + (b1 + δb1) 1T) + (b2 + δb2) 1T
=s+δw2 ((W1 + δw 1) X + (b1 + δb1) 1T) + s+ W2δw 1X + s+ W2δb1IT + δb2IT
+ W2 s+ (W1X + b1IT )+b2IT
=(s+δw2 (W1 + δw 1) + s+W2δw 1) X + (s+W2δb1 + δb2 + s+δw2 (b1 + δb1)) 1T
+ W2hs-,s+ (W1X + b11n ) + b2 1n
=(W + δ)
X
1Tn ,
where eq. (*) is because all components of (W1 + δw 1) X + (b； + δb1) 1T are positive, and δ is
defined as the following matrix
δ =	[s+	(W2δw 1 +	δw2W1	+ δw2δw 1)	s+W2δbi	+	δb2	+ s+δw2	(bi	+
17
Published as a conference paper at ICLR 2020
,~ .
=f(W + δ).
δ approaches zero when the disturbances {δW1, δW2, δb1, δb2} approach zero (element-wise). Since
W is the local minimizer of f (W), we have
RR ([Wi『I, [bi]= f (W) ≤ f (W + δ) = RR ([Wi + δwi j], [bi + δbif= J .	(16)
Because the disturbances {δW1, δW2, δb1, δb2}
are arbitrary, eq.
is a local minimizer.
The proof is completed.
(16) demonstrates that
□
Step (b). Prove the constructed local minima are spurious.
Theorem 4. Under the same conditions of Lemma 4 and Assumptions 1, 2, and 4, the constructed
spurious local minima in Lemma 4 are spurious.
Proof. The minimizer W is the solution of the following equation
Vw f (W )=0.
Specifically, we have
∂f (W)
~WjΓ
0, i ∈ {1, ∙∙∙ ,dY}, j ∈ {1, ∙∙∙ ,dX},
Applying the definition of f(W) (eq. (11)),
∂f (W)
"∂Wk7
XX VYil (k,w M) Ekjk
i=1
n
X
i=1
(VYil (k,w
xi
1
xi
1
j
,V^l (匕，W Xi ) ∈ R1×dY. Since k, j are arbitrary in {1, •…，dγ} and
where 匕=W xi
{1,…，dχ }, respectively, We have
V XT 1n =0,	(17)
where
T
-»-vτ .ι	∙>r∙ Wr TTT- -∖τ^ Al ♦	a	. ∙ -t	ι
We then define Y = WX . Applying Assumption 1, we have
Thus, there exists some k-th row of Y - Y that does not equal to 0.
18
Published as a conference paper at ICLR 2020
We can rearrange the rows of W and Y simultaneously, while W is maintained as the local mιnι-
mizer of f (W) and f (W) invariant1. Without loss of generality, We assume k = 1 (k is the index of
the row). Set U = V^,. and Vi = Y1,i in Lemma 7. There exists a non-empty separation I = [1 : l0]
and J = [l0 + 1 : n] of S = {1,2, ∙∙∙ ,n} and a vector β ∈ RdX, such that
(1.1)	for any positive constant α small enough, and i ∈ I, j ∈ J, Yι,i - αβτXi < YIj - αβτXj;
(1.2)	PieI V1,i=0.
Define
ηι = Y1," - ɑβTXij + 2
min
∖i∈{l0 + 1,∙∙∙,n}
Applying (1.1), for any i ∈ I
Yι,i - αβτXi - ηι
i - αβτXi - Yι,lf + αβτXlf
1 (i∈{l,min..,n} (Y1"
-ɑβTXi) - (Y1,l0 - ɑβτxI0
<0,
while for any j ∈ J,
YIj — ɑβτXj — ηι > 0
j - αβTXi- Y1,1o+αβTXI)-1 (∈{l,min..,n} (Y1,i
—ɑβ
ITXi) - (YI,” 一 ɑβ
T Xlf
≥- f min
2 ∖i∈{l0+1,...,n}
>0.
i - aB，X) - (Y1,l0 - QβTχl0
Define Y ∈ R which satisfies
(ɪ min	ɑβτ (xi — Xi),
2 i∈{l0 + 1,...,st+ι}
ɑ,
l0 <
st+1
l0
st+1
where st+ι is defined in Lemma 7.
We argue that
1 (∈{l,min..,n} (Yι'- αβ TXi)-(Yι,lo - αβτ Xl0
IYl > 0.
(18)
—
When l = st+ι, eq. (58) stands. Also,
lim
min
ɑ→0十 ∖i∈{l0 + 1,...,n}
lim Y = 0,
ɑ→0+
,i - αβTXi) - (Yi," - αβτXlf
min
i∈{l0 + 1 ,...,n}
Y1,i - Y1,l0
> 0.
Therefore, we get eq. (18) when α is small enough.
When l0 < st+1, eq. (57) stands. Therefore,
1
1
∣γ∣
-- min
2 2 ∖i∈{l∕ + 1,...,n}
i - ɑβτXi) - (Y1,1' - αβTXi
which apparently leads to eq. (18).
1 f is also the function in term of Y.
19
Published as a conference paper at ICLR 2020
Therefore, for any i ∈ I, we have that
Y1,i - QeTxi - ηi + ∣γ∣
≤ — 1 Q川n..,n} (HL QeTxJ-M，- αβTw)) + lY।
<0,
while for any j ∈ J,
Y1,j - QeTXj - ηι - ∣γ∣
≥ 1 Q川n..,n} (ZLQeTXJ-(Z" - QeTXl)) -lγl
>0.
Furthermore, define ηi (2 ≤ i ≤ dγ) as negative reals with absolute value sufficiently large, such
that for any i ∈ [2 : dγ] and any j ∈ [1 : n],
Yi,j - ηi > 0.
Now we construct a point in the parameter space whose empirical risk is smaller than the proposed
local minimum in Lemma 4 as follows
W1,[1:dx] - QeT
-W1,[1：dx ] + QeT
W2,[1：dx ]
.
.
.
Wdy ,[1：dx ]
0(dι -dy — 1) × dχ
(19)
and
〜
bi
W1,[dχ + 1] - η1 + Y
-W1,[dχ + 1] + η1 + Y
W2,[dχ + 1] - η2
Wdy ,[dχ + 1] - ηdy
0(dι-dy —1)×1
-	1	1
s++s- s + + s-
0
0
1
s +
•	∙	0	0
..
0	..	..
•	•	0
0
0	0	0	0
0
(20)
(21)
η1
η2
b2 =	.
.
.
-ηdy -
(22)
1	TTT-	1 ^t	.1	•	1 .	.	•	1 . 1	1 •	/' . 1	■ .1	1	. •	1
where Wi and bi are the weight matrix and the bias of the i-th layer, respectively.
20
Published as a conference paper at ICLR 2020
After some calculations, the network output of the first layer before the activation in terms of
Ifc卜S
	W1,.X- αβτ X—η11n+YIT —W1 ∙X + αβT X + η11τ + YIT
Y ⑴=WIX + b11T =	W2,∙X — η21T . . . WdY ,X - ηdγ 1T -	0(dι-dγ-1)×n	-
Therefore, the output of the whole neural network is
/	___ ~	r~Γ∣∖	~ F
Y = W2hs-,s+(W1X + bι1T) + b21T
〜
W2hs-,s +
W1,∙X — αβτ X — η11T + γ1T
-Wι,.X + αβτ X + ηι 1T + YIT
W2,∙X — η21T
~	F
+ b2 1n .
ʊr	√>	TT
Wdγ ,∙x - ηdγ 1n
θ(dɪ -dγ — 1) ×n
Specifically, if j ≤ l0,
22
i=1, [bi]i=1
=W1,∙
1,j
Xj
1
—αβτ Xj — ηι + Y
〜
=γ1,j
—ɑβτXj — ηι + γ < 0,
2
i=1
i=1
=—W1,-
2,j
Xj
1
+ αβτ Xj + ηι + Y

2
EI	∕'	Z-I • \ .1	.	∕' W
Therefore, (1,j)-th component of Y
2=1∕C1) is
—Y,j + αβτXj + η1 + γ > 0.
∖,fC))1,j
W1,∙X — αβτ X — η11τ + γ IT
-W1,∙X + αβτ X + η11τ + YIT
ʃ—" ______________
s++s- , s + + s-
,0,…，0 hs.,s+
〜	E
W2,∙X — η21T
(
1
—
1
∖
Wdγ ,∙x — ηdγ 1T
0dι-dγ-11n
∙,j
+ η1
1
s+ + S-
τ
hs-,s+ (Y1,j — aβ Xj — η1 + 7) —
S+ + S-
hs-,s+ (—Y1,j + ɑβτ xj + η1 + 7)
+ η1
S-
S+ + S-
τ
(Y1,j — αβ Xj — η1 + γ) —
s+
S+ + S-
(—Y1,j + aβτ χj + η1 + γ)
1
+ η1
=Y1,j — αβT Xj +
S— - S +
S+ + S-
■7；
(23)
21
Published as a conference paper at ICLR 2020
Similarly, when j > l0, the (1,j)-th component is
s+
s+ + S-
(Y^1 j — αβT Xj
-ηι + Y)—
S-
S+ + S-
(—Y1,j + αβτ χj+ ηι+ Y) + ηι
O	CT	, s+ - s-
=γι,j - αβ xj + s+ + s Y，
(24)
and
[bi1 1) ) ∙ ∙ = ɪ+ (Yij- ηi) + ηi = Yi,j, i ≥ 2∙
(25)
Y (囱2=1
Thus, the empirical risk of the neural network with parameters
1
n
2
i=1
,[bi]2=1}is
n
Xι(κ,w2 (WiXi + bιiT) +b2iT
i=1	'
nX (l H w
i=1、
n
+ X o
i=1
Xi
1
W2 (WiXi + bιiT) +b2iT - W
+ VYi l∖Yi,W
1
1Xi + b11n ) + b21n - W
Xi
1
(26)
Applying eqs. (23), (24), and (25), We have
n
X
i=1
l0
τ ∖ τ ττ.
IXi + bi + b2 - W
Xi
1
Xi
1
(=)X Vι,i(-αβTXi + s+-^≡
W	s+ + s-
l0
=2γ X 任产 Vi,i,
匕 s++ s-
n
E V1,i(-αβTXi-
i=l0 + 1
s+-^ Y)
s+ + s-
T NY (κ,W
T
Y) +
where eq. (*) is because
—αβT Xj +
iXi + bi ) + b2 - W
Xi
1
—αβT X j —
s- - s+
----1---Y,
s+ + s-
s- - s+
----1---Y,
s+ + s-
1,i ≤ l0
1,i > l0 .
lo,
j≥2
j
j
j
Furthermore, note that α = O(Y) (from the definition of γ). We have
n	/	-
X o( W2 (WiXi + bi) +b2 - W Xi
i=1 ∖	L ∙
=O(Y).
22
Published as a conference paper at ICLR 2020
Let α be sufficiently small while sgn(γ)
-Sgn (P S+-s- V1,ij. We have
=2γ X s+≡V1,i + 0(Y)
(**)
< 0,
where inequality (**) comes from (1.2) (see p. 18).
From Lemma 4, there exists a local minimizer
bi i	w with empirical risk that equals
to f(W). Meanwhile, we just construct a point in the parameter space with empirical risk smaller
,~ .
than f (!W).
Therefore,
is a spurious local minimum.
The proof is completed.
□
A.3 S TAGE (2)
Stage (2) proves that neural networks with arbitrary hidden layers and two-piece linear activation
hs-,s+ have spurious local minima. Here, we still assume s+ 6= 0. We have justified this assumption
in Stage (1).
This stage is organized similarly with Stage (1): (a) Lemma 5 constructs a local minimum; and (b)
Theorem 5 proves the minimum is spurious.
Step (a). Construct local minima of the loss surface.
Lemma 5. Suppose that all the conditions of Lemma 4 hold, while the neural network has L - 1
hidden layers. Then, this network has a local minimum at
Wι=" Wil% #, bi = "hWLx+1- η1dγ #,
0(d1-dY)×dX	-η1d1-dY
1 dY	1 di
W^0 = -- X Ej,j + -- X Ej,(dγ + 1)，bi =0(i = 2, 3,…,L -I),
s+ j=1	s+ j=dY+1
and
WL = S S+ IdY 0dγ ×(dL-ι-dγ) ] , bL = η1dγ ,
where Wi0 and b0i are the weight matrix and the bias of the i-th layer, respectively, andη is a negative
constant with absolute value sufficiently large such that
WX - η1dγ 1T > O,	(27)
where > is element-wise.
Proof. Recall the discussion in Lemma 4 that all components of WιX + bi IT are positive. Specif-
ically,
where Y is defined in Lemma 4.
23
Published as a conference paper at ICLR 2020
Similar to the discussions in Lemma 4, when the parameters equal to
output of the first layer before the activation function is
{[WC"k
and
Y ⑴=W1X + blιT
Y - ηldγIT
~η1d1-dγ 1T
Y - η1dγ 1T > 0,
-η1dι-dv 1T > 0.
Here > is defined element-wise.
, the
(28)
(29)
After the activation function, the output of the first layer is
Y⑴=% 一(WI x + bi1T) = s+(Wix + bi1T ) = s+
We prove by induction that for all i ∈ [1 : L - 1] that
Y⑴ > 0 , element-wise,
Suppose that for 1 ≤ k ≤ L - 2, Y(k) is positive (element-wise) and
1~	E ∙
Y - η1dγ1T
-η1dι -dγ 1n.
(30)
(31)
Then the output of the (k + 1)-th layer before the activation is
Y(k+1)= Wk+iY(k)+bk+i1T
1	d dγ	dk十1	ʌ
=—I EEj j + £ Ej,(dγ + 1)卜+
+ ∖j=1	j=dγ + 1	)
Y - η1dγ1T
=.
_-η1dk 十ι-dγ 1n _
Applying eqs. (28) and (29), We have
Y -η1dγ1T
-η1dk-dγ 1T
Y (k+1)
〜	E
Y -η1dγ 1T
> 0,
where > is defined element-wise. Therefore,
Y(k+1) = hs-,s十(Y(k+1)) = S + Y(k+1) = s +
We thereby prove eqs. (30) and (31).
Therefore, Y(L) can be calculated as
Y = Y(L) = WL Y(LT) + %1T
0dγ ×(dL-i-dγ) ] s +
Y
Y -η1dγ1T
_-η1d⅛+ι-dγ 1n-
■ ~	Er
Y -η1dJT
+η1dv1T
-η1di-dγ 1T_
(32)
24
Published as a conference paper at ICLR 2020
Then, we show the empirical risk is higher around
in order to prove that
is a local minimizer.
Let
point
all components of Y(i)
be point in the parameter space which is close enough to the
. Since the disturbances δW0 i and δb0i are both close to 0 (element-wise),
ʌ . .
bi + δbi
remains positive. Therefore, the output
of the neural network in terms of parameters{旧十δwiLhbi+/I 卜
+ (bL + δb L) 1T
=(WL + δWL)s+ (∙∙∙s+ ((W0 + δWι) X + 化 + δb 1)1T)…)
+ (bL + δb l) 1T
=M1X +M21Tn,
where Mi and M2 can be obtained from { [Wi]	, [bR } and {[δW/3 , Mb/3} through
several multiplication and summation operations2.
Rewrite the output as
X
1Tn
M1 X + M2 1Tn = [M1	M2 ]
Therefore, the empirical risk R before and after the disturbance can be expressed as f(W) and
f ([M1 M2]), respectively.
When the disturbances {[δW/3，层/3 } approach 0 (element-wise), [Mi M2] approaches W.
Therefore, when [δW0 i]iL=1 , [δb0i]iL=1 are all small enough, we have
TR
bi+
=TR
=f([M1	M2])
,~ .
≥f (W)
(33)
Since
, eq. (33) yields that
i=ι,hbi +
are arbitrary within a sufficiently small neighbour of
bii	} isa local minimizer.
□
Step (b). Prove the constructed local minima are spurious.
2Since the exact form of M1 and M2 are not needed, we omit the exact formulations here.
25
Published as a conference paper at ICLR 2020
Theorem 5. Under the same conditions of Lemma 5 and Assumptions 1, 2, and 4, the constructed
spurious local minima in Lemma 5 are spurious.
Proof. We first construct the weight matrix and bias of the i-th layer as follows,
dY
WWi0 = 一 XEi,i, bi = 0(i =3,4,...,L -I),
s+ i=1
and
1 dY
WL = 一 ^X Ei,i, bL = -λ1dγ ,
s+ i=1
where WW1, W2, bi and b2 are defined by eqs. (19), (20), (21), and (22), respectively, and λ is a
sufficiently large positive real such that
Y (画；[可2=1)+也1T > 0,
(34)
where > is defined element-wise.
defined in Lemma 4.
We argue that
corresponds to a smaller empirical risk than f (W ) which is
τ-,∙	. El	λ 1	1.1	. . 1	∙	. I TTT- TTT- 7	7】
First, Theorem 4 has proved that the point W1, W2, b1, b2
,~ .
than f (IW).
corresponds to a smaller empirical risk
We prove by induction that for any i ∈ {3, 4, ..., L - 1},
(35)
(36)
Apparently the output of the first layer before the activation is
Y⑴(恒iL=i, [bii') = WIX + bi 1T = WIX + bi 1T = Y⑴(Wii2=1, hbii2=i).
Therefore, the output of the first layer after the activation is
26
Published as a conference paper at ICLR 2020
Thus, the output of the second layer before the activation is
Y⑵
Y
+ λ1dγ1T
r ~	-∣
b2
.θ(d2-dγ) × 1_
；fC
十 λ1d2 IT
Applying the definition of λ (eq. (34)),
Y⑵
λ1d,2-dγ 1n
,element-wise.
(37)
Therefore, the output of the second layer after the activation is
T
n
L
PL)Canbe
Meanwhile, the output of the third layer before the activation is Y(3)
i=1
+ λ1dγ IT
Jbii	) + λ1dγ1T
i=1 L J i=1/
Y
Applying eq. (37),
Y⑶
0(d3 -dγ )×n
, element-wise.
(38)
Therefore, the output of the third layer after the activation is
27
Published as a conference paper at ICLR 2020
Suppose eqs. (35) and (36) hold for k (3 ≤ k ≤ L — 2), when k + 1,
Y (k + 1)
+ λ1dγIT
Y (k+1)
Applying eq. (38),
element-wise.
(39)
Therefore, the output of the (k + 1)-th layer after the activation is
Therefore, eqs. (35) and (36) hold for any i ∈ {3,4,…，L — 1}.
Finally, the output of the network is
Y (WFH)=y(L) um
Applying Theorem 4, We have
fC)
,~ .
<f(W).
The proof is completed.
□
A.4 Stage (3)
Finally, we prove Theorem 1.
This stage also follows the two-step strategy.
Step (a). Construct local minima of the loss surface.
28
Published as a conference paper at ICLR 2020
Lemma 6. Suppose t is a non-differentiable point for the piece-wise linear activation function h and
σ is a constant such that the activation h is differentiable in the intervals (t - σ, t) and (t, t + σ).
Assume that M is a sufficiently large positive real such that
MfB1W1X + b11TI∣F <σ.	(40)
Let αi be any positive real such that
α1 = 1
0 <αi < 1, i = 2,…，L - 1.
(41)
Then, under Assumption 3, any neural network with piecewise linear activations and L - 1 hidden
layers has local minima at
W 00=MM WW1，b10=MMb1+t1dι，
W1 = αilW0, bi0 = -αiW0h(t)1di-ι +、。+ ^^X, ( = 2, 3,…，L - 1),
and
WL0 = πl^MWL, UL = -qLmγ-WLh(t)idL-ι + b0L
j=2 αj	j=2 αj
where
is the local minimizer constructed in Lemma 5. Also, the loss is con-
tinuoUsly differentiable, whose derivative with respect to the prediction Yi may equal to 0 only when
the prediction Y and label Yi are the same.
Proof. Define s- = lim h0(θ) and s+ = lim h0(θ).
We then prove by induction that for all i ∈ [1 : L - 1], all components of the i-th layer output before
the activation Y(i)
are in interval (t, t + σ), and
Y ⑻(WC, [bC)=h(t)1di 1T+π⅛F Y ⑻(Wk, W).
The first layer output before the activation is,
Y⑴([WliLi/bi0]；)= W10X + bl01T = MMW1X + Mbin + tid11T.	(42)
We proved in Lemma 5 that W0X + b0i 1T is positive (element-wise). Since the FrobeniUs norm of
a matrix is no smaller than any component’s absolute value, applying eq. (40), we have that for all
i ∈ [1, d1] and j ∈ [1 : n],
0 <MM (W1 x + b11T )j<σ.	(43)
Therefore,(吉(W：X + b；1T) +1) ∈ (t,t + σ). So,
Y ⑴(hWk,a)=h(Y ⑴(hWc,a))
(=)hs-,s+ ( MW1X + Mb11T) + h(t)1dι 1T
=MMY ⑴(hWC,hbC)+hmιT,
where eq.(*) is because for any X ∈ (t - σ,t + σ),
h(x) = h(t) + hs-,s+ (x - t).	(44)
29
Published as a conference paper at ICLR 2020
Suppose the above argument holds for k (1 ≤ k ≤ L — 2). Then
Y(k+1) (KCja)
=Wk+1Y(k)(［叼：h用：)+bk+ιlT
= (-αk+1WVk +Ih(t)lrffc+1
+ tldk+1 +πM^i bk+ι)1T + αk+ιWk+1Y(k)(WIjlJ 用 Li)
=%1M+1 (MdIT + C Mk) (WCIjbCI))
+ (—αk + 1Wk +ιh(t)1dfc + t1dfc+ι + iM i bk+i) IT
=* Wk +1Y(k)(［用『1, W『1) +* bk + 11T + t1dk + 1 1T
=t1,	IT + nk=1αi
t1dk+ι 1n + M
Lemma 5 has proved that all components of Y(k+1)
are contained in
(旧LwI)
. Combining
Y⑴
t1dι 1T <t1dι 1T + MY⑴(［闸［，网< (t+σ)1dι 1T,
we have
t1dk+ι1T <t1dk+ι1T + πM&Y(k+1)
+ σ)1dfc+1 1n .
Here < are all element-wise, and inequality (*) comes from the property of ai (eq. (41)).
Furthermore, the (k + 1)-th layer output after the activation is
ML)
= h(t)1dk + ι1T +
= h(t)1dk + ι1T +
∏"ι%i
M
∏31α,
M
where eq. (*) is because of eq. (44). The above argument is proved for any index k ∈{1,...,L-1}.
30
Published as a conference paper at ICLR 2020
Therefore, the output of the network is
RWLhbC)=R ([闸 LJiiL=)=f(W).
We then introduce some small disturbances
order to check the local optimality.
{[δW∕3,[δbi]3} into {[优0『1，[bi0iL11} in
Since all comonents ofY (i) are in interval (t, t+σ), the activations in every hidden layers is realized
at linear parts. Therefore, the output of network is
Y (M + 添iiL=ι，hbi0 + δbii∖)
=(WL + δWL) h (…h ((W00 + δW 1)X + 3 + δb0ι) 1T)…)+ 仅L + δbL) 1T
=(WL + δWl) s+ (…s+ ((W10 + δWl) X + (b10 + δb1) 1T) + f ⑴ 1dι1T …)
+ f ⑴ 1dL 1T + (bL + δbL) 1T
=M1X + M21Tn
= [M1	M2]
X
1Tn
Similar to Lemma 5, [M1
M2] approaches W as disturbances
(element-wise). Combining that W is a local minimizer of f (W), we have
[δWi ]iL=1 , [δbi ]iL=1
approach 0
The proof is completed.
一 一 ~
f([Mι	M2]) ≥ f(W)
□
Step (b). Prove the constructed local minima are spurious.
Proof of Theorem 1. Without loss of generality, we assume that all activations are the same.
Let t be a non-differentiable point of the piece-wise linear activation function h with
s- = lim h0(θ),
θ→0-
s+
lim h0(θ).
θ→0+
31
Published as a conference paper at ICLR 2020
Let σ be a constant such that h is linear in interval (t - σ, t) and interval (t, t + σ).
Then construct that
~,,	1 ~, ~,,	1~,
WI= MWI, b1 = M b1+ t1d1,
~ .. I ~ .	~ ..	I , . ~ .	I ~ .
W2 = MW2, b2 = t1d2 - M h(t)W21d2 + MMb2，
1
Wi'' = Wil bi0 = -Wih(t)1d一+ t1di + Ebi，(i = 3,4,...,L - 1)
and
WZ = MMWL, bL = b'L - MMWLh(t)1L-1,
where ｛叫「］,［可「］｝ are constructed in Theorem 5, M is a large enough positive real such
that
MIWK + b11T∣∣F <σ,	(45)
and M a large enough positive real such that
(46)
of
First,
Y⑴(μ匕/用')=W" + b 1T = M<W1X + bi1T) + t1TjT.	(47)
For any i ∈ ［1 : d1］ and j ∈ ［1 : n］, eq. (45) implies
1(M<WK + b'K))J≤ MIlWX + &1TL <σ.
Thus,
(M (WOX + b 11T) + t1Ti1T) . . ∈ (t -σ,t + σ).
(48)
Therefore, the output of the first layer after the activation is
Y ⑴(WL同］=h" (Wia))
=h(M^(Wlx + bi1T) + t1di 1fj
=h(t)1dι 1T + hs-,s+ (M∙ (W1 X + bl))
=h(t)1dι1T + Mhs-,s+ ((WIX + bl))
=h(t)1di 1T+MY ⑴(W 匕，［矶二),
32
Published as a conference paper at ICLR 2020
where eq. (*) is from eq. (44) for any x ∈ (t - δ,t + δ).
+ M⅛b21T + t1d21T
+ tid21T.
-ɪW2Y(I) ( WOiL
M M 2	」=
ɪY(2) ([W0↑l ,
MM V4=i'
Recall in Theorem 5 we prove all components of Y ⑵
ing the definition of M (eq. (46)), we have
t1d21T <Y(2)
1
〜
are positive. Combin-
Y⑵
+ t1d21T
Therefore,
MM
<(t + σ)1d21T
= h(t)1d2 1T +
ɪ h
MM s-,s+
1
M M
= h(t)1d2 1T +
Suppose the above argument holds for k-th layer.
The output of (k + 1)-th layer before the activation is
+ (-WWk +1h(t)1dk + t1dk+ι + MrMWk+ ) 1T
33
Published as a conference paper at ICLR 2020
Recall proved in Theorem 5 that all components of Y(k+1)
except those that
are 0 are contained in Y(k)
(W0L [be)Wehave
Therefore,
t1dk+ι1τ < Mmm Y (k+1) ([W0i i=1，[bi] i=1)+t1dk+ι 1T < (t+σ)ιdk+ι1τ.
Thus, the argument holds for any k ∈ {2, . . . , L - 1}.
So,
Y(L) ([WCι
=WLOY(LT ([WC/bO+bL
=MMWL (h⑴IdL-IIT + MMY(LT) (Wil [b『i))
+ bLiT - MM WL h(t)idL-ι 1T
Therefore,
(49)
From eq. (49) and Theorem 5, we have
which completes the proof of local minimizer.
Furthermore, the parameter M used in Lemma 6 (not those in this proof) is arbitrary in a continuous
interval (cf. eq. (40)), we have actually constructed infinite spurious local minima.
□
Theorem 1 relies on Assumption 4. We can further remove it by replacing Assumption 3 by a mildly
more restrictive variant Assumption 5.
Corollary 3. Suppose that Assumptions 1, 2, and5 hold. Neural networks with arbitrary depth and
arbitrary piecewise linear activations (excluding linear functions) have infinitely many spurious
local minima under arbitrary continuously differentiable loss whose derivative can equal 0 only
when the prediction and label are the same.
Proof. The proof is delivered by modifications of Theorem 4 in Stage 1 of Theorem 1’s proof. We
only need to prove the corollary under the assumption that s- + s+ = 0.
Let the local minimizer constructed in Lemma 4 be
Then, we construct a
point in the parameter space whose empirical risk is smaller as follows:
34
Published as a conference paper at ICLR 2020
〜
〜
bi
-1	1
2s+	s+
0	0
0	0
00
and
W 1,[1:dx] - αβτ
〜WW1,[1：dx]
-W1,[1:dx ] + αβτ
W2,[1：dx ]
Wdy ,[1：dx ]
_	θ(dɪ -dγ — 2) × dχ
WIJdX + 1] - η1 + Y
〜W1,[dχ + 1] - η
-W1,[dχ + 1] + η1 + Y
W2,[dχ + 1] - η
WdY ,[dχ + 1] - ηdγ
0(dι-dy — 2)×1
1
2s +
0
0
1
s+
0
0
0
1
s+
00
00
00
1
s+
0
0
0
.
.
.
0
0
0	0	0
0
〜
b2
η
η2
.
.
.
ndγ -
where α, β, and n are defined the same as those in Theorem 4, and η is defined by eq. (27).
Then, the output of the first layer is
=hs-,s+ (W1X + b1iT)
WW 1,∙x - αβτX - η1IT + YIT
~	W1,∙X - η1T
-W1,∙X + αβT X + η1 1t + YIT
W2,∙X -η21T
Wdγ,∙x - ηdy 1T
0dι-dy-21n
35
Published as a conference paper at ICLR 2020
Further, the output of the whole network is
Y
2	,W]2 )
i=1 L J i=1)
WI x - QBTX - ηι1T + YIT
〜 ------------
=W2hs-,s+
~	Wi,X - η1T
-Wi.X + QBTX + ηι1T + YIT
W2,.X - η21T
+ b21T
=W2hs-,s+
Wdy ,-X - ηdγ 1T
0dι-dy-21T
W1,∙x - QBTX - η11T + YIT
~	W1,∙X - η1T
-Wi,.X + QBT X + ηι1T + YIT
W2,∙X - η21T
η
η2
1T
〜
〜
+
Wdγ,-X - ηdy 1T
0dι-dy-21T
Lndy」
ʌ.
Therefore, if j ≤ l0, the (1,j)-th component of Y
2	, [bi]2 >s
i=1 L J i=1J
1
2s+
+ 2s+ (Yj - η) - s+ (-Yj + QBTXj + ηι + Y))
j - QeTXj - ηι + Y) + 2s+ (YIj - η) - s+ (-Yj + QeTXj + ηι + Y))
+ η
2S^ (2s+Y›ι,j - 2s+η - 2s+y) + η

=YIj- Y
Otherwise j > l0), the (1,j)-th component of Y
ʌ
2
i=1 L 」i=1
,bi I is
2	, [bii2)
i=1 L 」i=1，j
2
〜
2s+
+η
j - QeTXj - ηι + Y) + 2s+ (YIj - η) - S- (-Yj + QeTXj + ηι + Y))
2s+
2s+
j - QBTχj - ηι+ Y) + 2s+ (YIj - η) + s+ (-Yij + QeTXj + ηι + Y) ) + η
2s+Yι,j - 2s+η + 2s+γ ) + η


=γι,j + γ,
1 .1	/ ■	∙ ∖ .1	/	.	1 ∖	. r∙ W
and the (i,j )-th (i > 1) component of Y
2
i=1
is Y
ιs Iij ∙

36
Published as a conference paper at ICLR 2020
Therefore, we have
j = 1,i ≤ l;
j = 1,i > l;
j≥2.
Then, similar to Theorem 4, we have
2 l0
- n EVmy+O(Y),
i=1
where V and l0 are also defined the same as those in Theorem 4.
When γ is sufficiently small and sgn(γ) = sgn Pli0=1 V1,i , we have that
RR
,~ .
f(W).
This complete the proof of Corollary 3.
□
A.5 A preparation lemma
We now prove the preparation lemma used above.
Lemma 7. Suppose U = (uι ∙∙∙ Un) ∈ R1×n which satisfies U = 0 and
n
ui = 0,
i=1
(50)
while {xι,..., xn} is a set of vector ⊂ Rm×1. Suppose index set S = {1,2, ∙∙∙ , n}. Thenfor any
series of real number {vι, •…,vn}, there exists a non-empty separation I, J of S, which satisfies
I ∪ J = S, I ∩ J = 0 and both I and J are not empty, a vector β ∈ Rm×1 ,such that,
(1.1)	for any sufficiently small positive real α, i ∈ I, andj ∈ J, we have vi -αβTxi < vj -αβTxj;
(1.2)	Pi∈I ui 6=0.
Proof. If there exists a non-empty separation I and J of the index set S, such that when β = 0,
(1.1) and (1.2) hold, the lemma is apparently correct.
Otherwise, suppose that there is no non-empty separation I and J of the index set S such that (1.1)
and (1.2) hold simultaneously when β = 0.
Some number Vi in the sequence (v1,v2,…，vn) are probably equal to each other. We rerarrange
the sequence by the increasing order as follows,
Vl = V2 = .一 = Vsi < Vsι + 1 = .一 = Vs2 < .一 < Vsk-ι + 1 = .一 = Vsk = Vn,	(51)
where sk = n.
37
Published as a conference paper at ICLR 2020
Then, for any j ∈ {1, 2,…，k - 1}, We argue that
sj
ui = 0.
i=1
OtherWise, suppose there exists a sj , such that
sj
X ui 6= 0.
i=1
Let I = {1, 2, ..., sj} and J = {sj + 1, ..., n}. Then, When β = 0, We have
vi - αβT xi = vi < vj = vj - αβTxj,
and
sj
ui =	ui 6= 0,
i∈I	i=1
Which are exactly the arguments (1.1) and (1.2). Thereby We construct a contrary example. There-
fore, for any j ∈ {1,2,…，k - 1}, we have
sj
ui = 0.
i=1
Since we assume that u 6= 0, there exists an index t ∈ {1, . . . , k - 1}, such that there exists an index
i ∈ {st + 1, ..., st+1 } that ui 6= 0.
Let l ∈ {st + 1, ..., st+1} is the index such that xl has the largest norm while ul 6= 0:
l = arg max	kxj k .	(52)
j∈{st+1,...,st+1 }, uj 6=0
We further rearrange the sequence (vst+1, ..., vst+1) such that there is an index l0 ∈ {st +
1, . . . , st+1},
kxl0 k =	max	kxj k ,
j∈{st+1,...,st+1 }, uj 6=0
and
∀i ∈ {st + 1, ..., l0}, hxl0, xii ≥ kxl0 k2 ;	(53)
∀i ∈{l0 + 1,…，st+ι}, hx,xii < ∣∣xιok2 .	(54)
It is worth noting that it is probably l0 = st+1, but it is a trivial case that would not influence the
result of this lemma.
Let I = {1, ..., l0}, J = {l0 + 1, ..., n}, and β = xl0. We prove (1.1) and (1.2) as follows.
Proof of argument (1.1).
We argue that for any i ∈ I, vi - αβTxi ≤ vl0 - αβTxl0 and for any j ∈ J, vj - αβT xj >
vl0 - αβTxl0.
There are three situations:
(A)	i ∈ {1,..., St} and j ∈ {st+ι + 1, ∙∙∙ ,n}. Applying eq. (51), for any i ∈ {1,...,st} and
j ∈ {st+ι + 1, ∙∙∙ , n}, we have that Vi < v“ and Vj > v”. Therefore, when a is sufficiently small,
we have the following inequalities,
Vi - αβT xi < Vl0 - αβTxl0,
Vj - αβT xj > Vl0 - αβT xl0.
(B)	i ∈{st + 1, ∙∙∙ ,l0}. Applying eq. (53) and because of a > 0, we have
-αhβ,xii ≤ -α∣β∣2 = -αhβ, xl0i.
38
Published as a conference paper at ICLR 2020
Since vi = vl0, it further leads to
vi - αβT xi ≤ vl0 - αβT xl0.
(C)	j ∈ {l0 + 1,…,st+ι}. Similarly, applying eq. (54) and because of α > 0, We have
-αhβ, xji > -α kβk2 = -αhβ, xl0 i.
Since vj = vl0, it further leads to
vj - αβT xj > vl0 - αβT xl0,
Which is exactly the argument (1.1).
Proof of argument (1.2).
We argue that for any i ∈ {st + 1,…，l0 - 1}, Ui = 0. Otherwise, suppose there exists an i ∈
{st + 1,…，l0 - 1} such that Ui = 0. From eq. (52), we have ∣∣Xik ≤ ∣∣χ" ∣∣. Therefore,
hxl0,xii ≤ kxl0 kkxi k ≤ kxl0k2,
where the first inequality strictly holds if the vector xl0 and xi have the same direction, while the
second inequlity strictly holds when xi and xl0 have the same norm. Because xl0 6= xi , we have the
following inequality,
hxl0, xii < ∣xl0 ∣2,
which contradicts to eq. (53), i.e.,
hxio,Xii ≥ ∣∣X10k2, ∀i ∈{st + 1,…，l0}.
Therefore,
st	l0-1
Ui =	Ui +	Ui + Ul0 = Ul0 6= 0,
i∈I	i=1	i=st +1
which is exactly the argument (1.2).
The proof is completed.	□
Remark. For any i ∈ {l0 + 1, ..., st+1}, we have
vi - αβT xi - vl0 - αβTxl0 = αβT (xl0 - xi),	(55)
while for any j ∈ {st+1 + 1, ..., n}, we have
vj - αβT xj - vl0 - αβT xl0 = vj - vl0 + αβT (xl0 - xj).	(56)
Because vj > vl0, when the real number α is sufficiently small, we have
αβT (xl0 - xi) < vj - vl0 + αβT (xl0 - xj).
Applying eqs. (55) and (56), we have
vi - αβT xi	-	vl0	- αβTxl0	<	vj	- αβT xj	-	vl0	- αβT xl0	.
Therefore, if l0 < st+1, we have
min	vi	- αβT xi	-	vl0	- αβT xl0	= min	αβT (xl0	-	xi);	(57)
i∈{l0+1,...,n}	i∈{l0+1,...,st+1}
while if l0 = st+1,
min vi - αβT xi	-	vl0	- αβTxl0	= min	vi	- vl	+ αβT (xl0	-	xi).	(58)
i∈{l0+1,...,n}	i∈{l0 +1,...,n}
Eqs. (57) and (58) make senses because l0 < n. Otherwise, from Lemma 7 we have	in=1 Ui 6= 0,
which contradicts to the assumption.
39
Published as a conference paper at ICLR 2020
B Proofs of Theorem 2, Theorem 3, Corollary 1, and Corollary 2
This appendix gives the proofs of Theorem 2, Theorem 3, Corollary 1, and Corollary 2 omitted from
Section 4.
B.1 S quared loss
We first check that the squared loss is strictly convex, which is even restrictive than “convex”.
Lemma 8. The empirical risk R under squared loss (defined by eq. (5)) is strictly convex with
respect to the prediction Y.
Proof. The second derivative of the empirical risk R under squared loss with respect to the predic-
tion Y is
∂2lce(Y,Y) _ ∂2(y - Y)2 _ 2
八	—	八	—2 > 0.
∂Y2	∂Y2
Therefore, the empirical risk R under squared loss is strictly convex with respect to prediction
Y.	□
B.2	Smooth and multilinear partition.
If the activations are all linear functions, the neural networks is reduced to a multilinear model. The
loss surface is apparently smooth and multilinear. The nonlinearity in the activations largely reshape
the landscape of the loss surface. Specifically, if the input data flows through the linear parts of every
activation functions, the output falls in a smooth and multilinear region in the loss surface. When
some parameter changes by a sufficiently small swift, the data flow may not move out of the linear
parts of the activations. This fact guarantees that each smooth and multilinear regions expands to an
open cell. Meanwhile, every nonlinear point in the activations is non-differentiable. If the input data
flows through these nonlinear points, the corresponding empirical risk is not smooth with respect to
the parameters. Therefore, the nonlinear points in activations correspond to the non-differentiable
boundaries between cells on the loss surface.
B.3	Every local minimum is globally minimal within a cell.
Proof of Theorem 2. In every cell, the input sample points flows through the same linear parts of the
activations no matter what values the parameters are.
(1)	We first proves that the empirical risk R equals to a convex function with respect to a variable
W that is calculated from the parameters W.
Suppose (W1, W2) is a local minimum within a cell. We argue that
nn
X l (y,W2diag (A∙,i) WιXi) — X l (yi, ATidiag(Wz)Wix, ,	(59)
i=1	i=1
where A∙,i is the i-th column of the following matrix
-hS-,s+ ((Wl)l,∙xl)…hS-,s+ ((WI)I,∙xn)-
A —	.	...	.	.	(60)
h's-,s+ ((Wi)dι ,∙xi)…hS-,s+ ((Wι)dι,∙Xn1
The left-hand side (LHS) is as follows,
n
LHS — X l (yi, W2diag (A∙,i) WiXi)
i=1
n
—X l (yi, [(W2)1,1A1,i	…(W2)1,dι Adι,i] WIxi).
i=1
40
Published as a conference paper at ICLR 2020
Meanwhile, the right-hand side (RHS) is as follows,
n
RHS = Xl 3,ATidiag(W2)W1Xi),
i=1
n
=X l (yi, [(W2)1,1A1,i …(W2)1,dιAdι,i] WIxi).
i=1
Apparently, LHS = RHS. Thereby, we proved eq. (59).
Afterwards, we define
W1 = diag(W2 )Wι,	(61)
and then straighten the matrix W1 to a vector W ,
W= ((W1)1,…(Wι)dι,∙),	(62)
Also define
X = (A∙,1 0 x1	…	A∙,n @ xn).	(63)
Then, we can prove that the following equations,
(ATlW1x1 …ATnW1xn) = ((Wι)ι,∙…(Wi)心，)(A∙,1 氧 x1 …A∙,n 乳 xn)
ʌ ʌ
=W X.	(64)
Applying eq. (64), the empirical risk is transferred to a convex function as follows,
1
R(W1,W2)=—
n
1
n
nn
Xl (yi, (A∙,i)Tdiag(W2)W1x) = ~Xl (yi, (A∙,i)T W 1xi
i=1	n i=1
n
Xl (yi,WXi).
i=1
(65)
We Can See that the empirical risk is rearranged as a convex function in terms of W which unite the
two weight matrices W1 and W2 and the activation h are together as W.
Applying eqs. (61) and (62), we have
W = [(W2)1(W1)1,∙…(W2)d1 (W1)d1,∙].
(2)	We then prove that the local minima (including global minima) of the empirical risk TR with
respect to the parameter W is also local minima with respect to the corresponding variable W.
We first prove that for any i ∈ [1 : d1d2], we have
eiX V = 0,
,
where V is defined as follows,
V = hV(WX)ιl (Y1, (WXD …V(WX)nl (Yn，(WX)n)]T.
To see this, we divide i into two cases: (W2)i 6= 0 and (W2)i = 0.
Case 1:	(W2 )i 6= 0.
The local minimizer of the empirical risk TR with respect to the parameter W satisfies the following
equation,
∂ TR
∂(WKj =.
41
Published as a conference paper at ICLR 2020
Therefore,
∂R
=∂(WDij
∂ (Pj(Q,
=	∂(W1)i,j
n
=P [ 0	…0	(W2)i	0 …0 ] XkRwX) Jq,k,
(66)
'—{—}
dX d1 -dX (i-1)-j
'—{z—}
dX (i-1)+j-1
where (W2) is a vector and (W2)i is its i-th component.
Then, divid the both hand sides of eq. (66) with (W2)i, we can get the following equation,
(edχ (i-1)+j攵N = 0.
Case 2:	(W2)i = 0. Suppose u1 ∈ Rd0 is a unitary vector, u2 ∈ R is a real number, and ε is a
small enough positive constant. Then, define a disturbance of W1 and W2 as follows,
∆W1 = [ 0 …0	εuι	0 …0 ],
~{{z~/	~{{z~/
dX (i-1)	d1dX -dXi
△卬2 = [ 0 …0	ε2U2	0 …0 ].
~}}	}
i-1	d1 -i
When ε is sufficiently small, △W1 and △W2 are also sufficiently small. Since (W1, W2) is a local
minimum, we have
n
n Xl (Yk，((w+δ)x )k)
k=1
ʌ ,
=TR(W1 + ∆Wι, W2 + ∆W2)
≥R(W1,W2)
n
=n Xl (Yk，(W X A，	⑹7
k=1
where △ is defined as follows,
△ = [(W2 + ∆W2)ι(Wι + ∆Wl)l …	(W2 + ∆W2)dι (Wl + ∆Wl)dι]
-[(W2)1(W1)1 …(W2)d1 (Wl)dι]
= [ 0 …0 ε2u2 (εu1 + (WI)i) 0 …0 ] .	/X1QX
{}	{}	(68)
dX (i-1)	d1dX -dXi
Here, eq. (*) comes from (W2)i = 0. Rearrange eq. (67) and apply the Taylor's Theorem, We can
get that
△ ∙XV + oH∣∆∙XIi2) ≥0.
42
Published as a conference paper at ICLR 2020
Applying eq. (68), we have
∖ 0 …	0	ε2u2 (εuι + (Wι)i)	0 …	0 ] XV
~^}	S~}}
dX (i-1)	dX d1 -idX
+ε4O (k[0 …	0	U2 (εuι + (W))	0 …	0 ] X
、V~}}	、---V~}}
dX (i-1)	dX d1 -idX
(=)10 …0 ε3u2u1	0	…0 ] XV
、—V/	、—V—}
dX (i-1)	dX d1 -idX
+ε4O (k [ 0 …	0	U2 (εuι + (Wι)i)	0 …	0 ] X口2)
、~{{}	、---V/
dX (i-1)	dX d1 -idX
ε3[0 …0 U2Uι 0 …0 ] XV + o(ε3)
SV«_}}	V—}
dX (i-1)	dX d1 -idX
≥0.
(69)
(70)
Here, eq. (**) Can be obtained from follows. Because W2 is a local minimizer, for any component
(W2)i of W2,
d (Pn=Il (Yk, (WX)k))	0
∂(W2)i	=,
which leads to
[0 …0 (Wι)i 0 …0 ] XV = 0.
、~{V----}	、~{{z~/
dX (i-1)	dX d1 -idX
When ε approaches 0, eq. (69) leads to the following inequality,
-_	_	_	_	r	ʌ_
[0	…0	u2U1	0	…0	] XV	≥	0.
SV-----}	SV}	一
dX (i-1)	dX d1 -idX
Since u1 and u2 are arbitrarily picked (while the norms equal 1), the inequality above further leads
to
-	_	_	_	_	r	ʌ_
[0	.…0	ej	0	…0 ] XV =	0,
SV«_}}	SV-----}
dX (i-1)	dX d1 -idX
(71)
which finishes the proof of the argument.
Therefore, for any i and j , we have proven that
ed0 (i-1)+j X V = 0,
which demonstrates that
ʌ
X V = 0,
which means
W is also a local
minimizer of the empirical risk TR,
n
R(W ) = X l(Yi,WXi).	(72)
i=1
(3) Applying the property of convex
function, W is a global minimizer of the empirical risk R,
which leads to (W1 , W2 ) is a global minimum inside this cell.
The proof is completed.
□
43
Published as a conference paper at ICLR 2020
B.4 Equivalence classes of local minimum valleys in cells.
Proof of Theorem 3 and Corollary 1. In the proof of Theorem 2, we constructed a map Q:
(W1,W2) → W. Further, in any fixed cell, the represented hypothesis of a neural network is
uniquely determined by W .
We first prove that all local minima in a cell are concentrated as a local minimum valley.
Since the loss function l is strictly convex, the empirical risk has one unique local minimum (which
is also a global minimum) with respect to W in every cell, if there exists some local minimum in
the cell. Meanwhile, we have proved that all local minima with respect to (W1, W2) are also local
minima with respect to the corresponding W. Therefore, all local minima with respect to (Wι, W2)
correspond one unique W. Within a cell, when Wi expands by a positive real factor a to W0 and
W2 shrinks by the same positive real factor α to W20, we have Q(W1, W2) = Q(W10, W20), i.e., the
W remains invariant.
Further, we argue that all local minima in a cell are connected with each other by a continuous path,
on which the empirical risk is invariant. For every local minima pair (W1, W2) and (W10, W20), we
have
diag(W2)W1 = diag(W20)W10.	(73)
Since h0s ,s (W1X) = h0s ,s (W10X) (element-wise), for every i ∈ [1, d1],
sgn((W2)i) = sgn((W20)i) .
Therefore, a continuous path from (W1, W2) to (W10, W20) can be constructed by finite moves, each
of which expands a component of W2 by a real constant α and then shrinks the corresponding line
of W1 by the same constant α.
We then prove that all local minima in a cell constitute an equivalence class.
Define an operation 〜R as follows,
(Wi,w2)〜R (w2,w22),
if
Q(W11,W21) =Q(W12,W22).
We then argue that 〜R is an equivalence relation. The three properties of equivalence relations are
checked as follows.
(1)	Reflexivity:
For any (W1, W2), we have
Q(W1,W2) =Q(W1,W2).
Therefore,
(W1,W2)〜R (Wι,W2).
(2)	Symmetry:
For any pair (W11, W21) and (W12, W22), Suppose that
(Wi,w2)〜R (w2,w22).
Thus,
Q(W11,W21) =Q(W12,W22).
Apparently,
Q(W12,W22) =Q(W11,W21).
Therefore,
q(w2,w22)〜R q(w1,w2l).
(3)	Transitivity:
44
Published as a conference paper at ICLR 2020
For any (W11, W21), (W12, W22), and (W13, W23), suppose that
(w1,wJ)〜R (w2,w22),
(w2,w22)〜R (w3,w∣5).
Then,
Q(W11,W21) =Q(W12,W22),
Q(W12,W22) =Q(W13,W23).
Apparently,
Q(W11,W21) =Q(W12,W22) =Q(W13,W23).
Therefore,
(w1,wj)〜R (w3,w∣5).
We then prove the mapping Q is the quotient map.
Define a map as follows,
T : (W1,W2) → (diag(W2)W1, 11×d1).
We then define an operator ㊉ as,
(wl,W2l)㊉(w2,w22) = τ (wl,W2l) + τ (w2,w22),
the inverse of (W1, W2 ) is defined to be (-W1, W2 ) and the zero element is defined to be (0, 11×d1).
Obviously, the following is a linear mapping:
Q : ((Rd1×dX, R1×d1),㊉)→ (R1×dXd1,+).
For any pair (W11, W21) and (W12 , W22 ), we have
(wl,W2l)〜R (w2,w22),
if and only if
(W1，W» ㊉(-W2,W22) ∈ Ker(Q).
Therefore, the quotient space (Rd1 ×dX , R1×d1 )/Ker(Q) is a definition of the equivalence relation
〜R.
The proof is completed.	□
B.5 Linear collapse.
When there is no nonlinearities in the activations, there is apparently no non-differentiable regions
on the loss surface. In other words, the loss surface is a single smooth and multilinear cell.
45