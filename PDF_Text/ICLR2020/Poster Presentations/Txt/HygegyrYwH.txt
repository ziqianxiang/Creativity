Published as a conference paper at ICLR 2020
Polylogarithmic width suffices for gradient
descent to achieve arbitrarily small test er-
ROR WITH SHALLOW RELU NETWORKS
Ziwei Ji & Matus Telgarsky
University of Illinois, Urbana-Champaign
{ziweiji2,mjt}@illinois.edu
Ab stract
Recent theoretical work has guaranteed that overparameterized networks trained
by gradient descent achieve arbitrarily low training error, and sometimes even
low test error. The required width, however, is always polynomial in at least
one of the sample size n, the (inverse) target error 1/, and the (inverse) failure
probability 1∕δ. This work shows that Θ(1∕e) iterations of gradient descent With
Ω(1∕e2) training examples on two-layer ReLU networks of any width exceeding
Polylog(n, 1/e, 1∕δ) suffice to achieve a test misclassification error of e. We also
prove that stochastic gradient descent can achieve test error with polylogarithmic
width and Θ(1/) samples. The analysis relies upon the separation margin of the
limiting kernel, which is guaranteed positive, can distinguish between true labels
and random labels, and can give a tight sample-complexity analysis in the infinite-
width setting.
1 Introduction
Despite the extensive empirical success of deep networks, their optimization and generalization
properties are still not fully understood. Recently, the neural tangent kernel (NTK) has provided
the following insight into the problem. In the infinite-width limit, the NTK converges to a limiting
kernel which stays constant during training; on the other hand, when the width is large enough, the
function learned by gradient descent follows the NTK (Jacot et al., 2018). This motivates the study
of overparameterized networks trained by gradient descent, using properties of the NTK. In fact,
parameters related to the NTK, such as the minimum eigenvalue of the limiting kernel, appear to
affect optimization and generalization (Arora et al., 2019).
However, in addition to such NTK-dependent parameters, prior work also requires the width to
depend polynomially on n, 1∕δ or 1∕e, where n denotes the size of the training set, δ denotes
the failure probability, and denotes the target error. These large widths far exceed what is used
empirically, constituting a significant gap between theory and practice.
Our contributions. In this paper, we narrow this gap by showing that a two-layer ReLU network
with Ω(ln(n∕δ)+ln(1∕e)2) hidden units trained by gradient descent achieves classification error E on
test data, meaning both optimization and generalization occur. Unlike prior work, the width is fully
polylogarithmic in n, 1∕δ, and 1∕E; the width will additionally depend on the separation margin of
the limiting kernel, a quantity which is guaranteed positive (assuming no inputs are parallel), can
distinguish between true labels and random labels, and can give a tight sample-complexity analysis
in the infinite-width setting. The paper organization together with some details are described below.
Section 2 studies gradient descent on the training set. Using the `1 geometry inherent in classifi-
cation tasks, we prove that with any width at least polylogarithmic and any constant step
size no larger than 1, gradient descent achieves training error E in Θ(1∕E) iterations (cf.
Theorem 2.2). As is common in the NTK literature (Chizat & Bach, 2019), we also show
the parameters hardly change, which will be essential to our generalization analysis.
1
Published as a conference paper at ICLR 2020
Section 3 gives a test error bound. Concretely, using the preceding gradient descent analysis, and
standard Rademacher tools and exploiting how little the weights moved, we show that
2
With Ω(1∕e2) samples and Θ(1∕e) iterations, gradient descent finds a solution With E test
error (cf. Theorem 3.2 and Corollary 3.3). (AS discussed in Remark 3.4, Ω(1∕e) samples
also suffice via a smoothness-based generalization bound, at the expense of large constant
factors.)
Section 4 considers stochastic gradient descent (SGD) With access to a standard stochastic on-
line oracle. We prove that with width at least Polyloganthmic and Θ(1∕e) samples, SGD
achieves an arbitrarily small test error (cf. Theorem 4.1).
Section 5 discusses the separation margin, which is in general a positive number, but reflects the
difficulty of the classification problem in the infinite-width limit. While this margin can
degrade all the way down to O(1/√n) for random labels, it can be much larger when there
is a strong relationship between features and labels: for example, on the noisy 2-XOR data
introduced in (Wei et al., 2018), we show that the margin is Ω(1/ln(n)), and our SGD
sample complexity is tight in the infinite-width case.
Section 6 concludes with some open problems.
1.1	Related work
There has been a large literature studying gradient descent on overparameterized networks via the
NTK. The most closely related work is (Nitanda & Suzuki, 2019), which shows that a two-layer
network trained by gradient descent with the logistic loss can achieve a small test error, under the
same assumption that the NTK with respect to the first layer can separate the data distribution.
However, they analyze smooth activations, while we handle the ReLU. They require Ω(1∕e2) hidden
X / 1 / Zl ∖	1	.	1	1 八 ∕T∕9∖,	< ∙1	1 .	1	1	1	1	∙ .1	1 ∙ -I -I
units, Ω(1∕e4) data samples, and O(1∕e2) steps, while our result only needs polyloganthmιc hidden
2
units, Ω(1∕e2) data samples, and O(1∕e) steps.
Additionally on shallow networks, Du et al. (2018b) prove that on an overparameterized two-layer
network, gradient descent can globally minimize the empirical risk with the squared loss. Their
resultrequires Ω(n6∕δ3) hidden units. Oymak & Soltanolkotabi (2019); Song & Yang (2019) further
reduce the required overparameterization, but there is still a poly(n) dependency. Using the same
amount of overparameterization as (Du et al., 2018b), Arora et al. (2019) further show that the two-
layer network learned by gradient descent can achieve a small test error, assuming that on the data
distribution the smallest eigenvalue of the limiting kernel is at least some positive constant. They
also give a fine-grained characterization of the predictions made by gradient descent iterates; such
a characterization makes use of a special property of the squared loss and cannot be applied to the
logistic regression setting. Li & Liang (2018) show that stochastic gradient descent (SGD) with the
cross entropy loss can learn a two-layer network with small test error, using poly(', 1/e) hidden
units, where ` is at least the covering number of the support of the feature distribution using balls
whose radii are no larger than the smallest distance between two data points with different labels.
Allen-Zhu et al. (2018a) consider SGD on a two-layer network, anda variant of SGD on a three-layer
network. The three-layer analysis further exhibits some properties not captured by the NTK. They
assume a ground truth network with infinite-order smooth activations, and they require the width to
depend polynomially on 1/E and some constants related to the smoothness of the activations of the
ground truth network.
On deep networks, a variety of works have established low training error (Allen-Zhu et al., 2018b;
Du et al., 2018a; Zou et al., 2018; Zou & Gu, 2019). Allen-Zhu et al. (2018c) show that SGD can
minimize the regression loss for recurrent neural networks, and Allen-Zhu & Li (2019b) further
prove a low generalization error. Allen-Zhu & Li (2019a) show that using the same number of
training examples, a three-layer ResNet can learn a function class with a much lower test error than
any kernel method. Cao & Gu (2019a) assume that the NTK with respect to the second layer of
a two-layer network can separate the data distribution, and prove that gradient descent on a deep
network can achieve E test error with Ω(1∕e4) samples and Ω(1∕e14) hidden units. Cao & Gu
(2019b) consider SGD with an online oracle and give a general result. Under the same assumption as
in (Cao & Gu, 2019a), their result requires Ω(1∕e14) hidden units and sample complexity O(1∕e2).
2
Published as a conference paper at ICLR 2020
By contrast, with the same online oracle, our result only needs polylogarithmic hidden units and
sample complexity Oe(1/).
1.2	Notation
The dataset is denoted by {(xi, yi)}in=1 where xi ∈ Rd and yi ∈ {-1, +1}. For simplicity, we
assume that kxi k2 = 1 for any 1 ≤ i ≤ n, which is standard in the NTK literature.
The two-layer network has weight matrices W ∈ Rm×d and a ∈ Rm . We use the following
parameterization, which is also used in (Du et al., 2018b; Arora et al., 2019):
1m
f(x; W, a) := √m £as。(〈Ws, Xi),
s=1
with initialization
Ws,o -N(0,Id),	and as.〜unif ({ -1, +1}).
Note that in this paper, ws,t denotes the s-th row of W at step t. We fix a and only train W, as
in (Li & Liang, 2018; Du et al., 2018b; Arora et al., 2019; Nitanda & Suzuki, 2019). We consider
the ReLU activation σ(z) := max {0, z}, though our analysis can be extended easily to Lipschitz
continuous, positively homogeneous activations such as leaky ReLU.
We use the logistic (binary cross entropy) loss `(z) := ln 1 + exp(-z) and gradient descent. For
any 1 ≤ i ≤ n and any W, let fi(W) := f(xi; W, a). The empirical risk and its gradient are given
by
nn
RR(W):= — X' (yifi(W)), and VTR(W) = - X' (yifi(W)) y"fi(W).
n i=1	n i=1
For any t ≥ 0, the gradient descent step is given by Wt+1 := Wt - ηtVR(Wt). Also define
n
fi(t)(W):= (Vfi(Wt), W〉, and TR⑴(W) :=- X'3if")).
n i=1
Note that fi(t)(Wt) = fi(Wt). This property generally holds due to homogeneity: for any W and
any ≤ s ≤ m,
∂fi
∂Ws
as1 hws, xii > 0
xi,
and
and thus Vfi(W),W =fi(W).
2 Empirical risk minimization
In this section, we consider a fixed training set and empirical risk minimization. We first state our
assumption on the separability of the NTK, and then give our main result and a proof sketch.
The key idea of the NTK is to do the first-order Taylor approximation:
f (x; W, a) ≈ f (x; Wo, a) + (Vw f(x; Wo, a), W - W。〉.
In other words, we want to do learning using the features given by Vfi(W0) ∈ Rm×d. A natural
assumption is that there exists U ∈ Rm×d which can separate {(Vfi(W。)，y，n==i with a positive
margin:
U, Vfi(WO)E) = ɪminʃɪ } i √1= X as〈Us,Xi〉1 [〈Ws,。,Xi〉> 0]j > 0.	(2.1)
The infinite-width limit of eq. (2.1) is formalized as Assumption 2.1, with an additional bound on
the (2, ∞) norm of the separator. A concrete construction of U using Assumption 2.1 is given in
eq. (2.2).
1m≤ii≤nn	yi
3
Published as a conference paper at ICLR 2020
Let μN denote the Gaussian measure on Rd, given by the Gaussian density with respect to the
Lebesgue measure on Rd . We consider the following Hilbert space
H := {w : Rd → Rd / kw(z)k2 d〃N(Z) < ∞}.
For any x ∈ Rd, define φx ∈ H by
φx(z) := x1 hz, xi > 0 ,
and particularly define φi := φxi for the training input xi .
Assumption 2.1. There exists V ∈ H and γ > 0, such that„v(z)„2 ≤ 1 for any Z ∈ Rd, and for any
1 ≤ i ≤ n,
yi hv,φiiH := yi /(v(Z),φi(Z)〉d〃N(Z) ≥ γ.
♦
As discussed in Section 5, the space H is the reproducing kernel Hilbert space (RKHS) induced by
the infinite-width NTK with respect to W, and φx maps x into H. Assumption 2.1 supposes that
the induced training set {(φi, yi)}n=ι can be separated by some v ∈ H, with an additional bound
on IIv(Z) Il2 which is crucial in our analysis. It is also possible to give a dual characterization of the
separation margin (cf. eq. (5.2)), which also allows us to show that Assumption 2.1 always holds
when there are no parallel inputs (cf. Proposition 5.1). However, it is often more convenient to
construct v directly; see Section 5 for some examples.
With Assumption 2.1, we state our main empirical risk result.
Theorem 2.2. Under Assumption 2.1, given any risk target ∈ (0, 1) and any δ ∈ (0, 1/3), let
λ:
，2 ln(4n∕δ) + ln(4∕e)
γ/4
4096λ2
and M :=-------孱—
γ6
Then for any m ≥ M and any constant step size η ≤ 1, with probability 1 - 3δ over the random
initialization,
T X R(Wt) ≤ €, where T := d2λ2∕ηe].
t<T
Moreover for any 0 ≤ t < T and any 1 ≤ s ≤ m,
ιιws,t - ws,0ι∣2 ≤ γ√m.
While the number of hidden units required by prior work all have a polynomial dependency on
n, 1∕δ or 1∕e, Theorem 2.2 only requires m = Ω (ln(n∕δ) + ln(1∕e)2). The required width has a
polynomial dependency on 1∕γ, which is an adaptive quantity: while 1∕γ can be poly(n) for random
labels (cf. Proposition 5.2), it can be polylog(n) when there is a strong feature-label relationship, for
example on the noisy 2-XOR data introduced in (Wei et al., 2018) (cf. Proposition 5.3). Moreover,
We show in Proposition 5.4 that if We want {(Vfi(W0),y，}n=ι to be separable, which is the
starting point of an NTK-style analysis, the width has to depend polynomially on 1∕γ.
In the rest of Section 2, we give a proof sketch of Theorem 2.2. The full proof is given in Ap-
pendix A.
2.1	Properties at initialization
In this subsection, we give some nice properties of random initialization.
Given an initialization (W0, a), for any 1 ≤ s ≤ m, define
Us ：= -1=asv(ws,o),	(2.2)
where v is given by Assumption 2.1. Collect Us into a matrix U ∈ Rm×d. It holds that ∣∣Us |卜 ≤
1∕√m,andHUι∣F ≤ 1.
4
Published as a conference paper at ICLR 2020
Lemma 2.3 ensures that with high probability U has a positive margin at initialization.
Lemma 2.3. Under Assumption 2.1, given any δ ∈ (0, 1) and any 1 ∈ (0, γ), if m ≥
(2 ln(n∕δ)) /e2 ,then with probability 1 一 δ ,it holds Simultaneouslyfor all 1 ≤ i ≤ n that
yfi(O) (U) = y DVfi(W0),U〉≥ Y —《2ln(n∕δ) ≥ Y — J.
For anyW , any 2 > 0, and any 1 ≤ i ≤ n, define
Lemma 2.4 controls ai(W0,⑦).It will help us show that U has a good margin during the training
process.
Lemma 2.4. Under the condition of Lemma 2.3, for any 2 > 0, with probability 1 一 δ, it holds
simultaneously for all 1 ≤ i ≤ n that
% (Wo, £2)≤ rɪq+rInnδ) ≤ q+羡.
π	2m	2
Finally, Lemma 2.5 controls the output of the network at initialization.
Lemma 2.5. Given any δ ∈ (0, 1), if m ≥ 25 ln(2n∕δ), then with probability 1 一 δ, it holds
simultaneously for all 1 ≤ i ≤ n that
∣f (Xi； Wo, a) I ≤ q2ln (4n∕δ).
2.2	Convergence analysis of gradient descent
We analyze gradient descent in this subsection. First, define
n
1
Q(W) ：=- ∑-'0 (yifi(w)).
n i=1
We have the following observations.
•	For any W and any 1 ≤ s ≤ m, ∣∣∂fi∕∂ws∣∣2 ≤ 1∕√m, and thus ∣∣Vfi(W)卜 ≤ 1.
Therefore by the triangle inequality, ∣∣VRb (W )∣∣	≤ Qb(W ).
一 一 ..一 •一 一 ^ 一一 一 ^_________________________________
•	The logistic loss satisfies 0 ≤ 一'0 ≤ 1, and thus 0 ≤ Q(W) ≤ 1.
•	The logistic loss satisfies -' ≤ ', and thus Q(W) ≤ R(W).
The quantity Q first appeared in the perceptron analysis (Novikoff, 1962) for the ReLU loss, and
has also been analyzed in prior work (Ji & Telgarsky, 2018; Cao & Gu, 2019a; Nitanda & Suzuki,
2019). In this work, Q specifically helps us prove the following result, which plays an important
role in obtaining a width which only depends on Polylog(1 ∕e).
Lemma 2.6. For any t ≥ 0 and any W, if η ≤ 1, then
ηtR(Wt) ≤ ∣∣Wt - W∣∣2 -∣∣Wt+ι - W∣∣2 + 2ηtR⑴(W).
Consequently, if we use a constant step size η ≤ 1 for 0 ≤ τ < t, then
η (XR(WT) J +∣∣Wt - W∣∣2 ≤∣∣Wo - W∣∣2 +2η (XR(T) (W)
τ<t	F	F	τ<t
5
Published as a conference paper at ICLR 2020
The proof of Lemma 2.6 starts from the standard iteration guarantee:
∣∣wt+ι - w ∣∣2=IIWt- w ∣∣2 - 2ηt DVTR(Wj Wt- w E+褚卜元(WtU： .
We can then handle the inner product term using the convexity of ` and homogeneity of ReLU, and
control kVR(Wt)k2F by R(Wt) using the above properties of Q(Wt). Lemma 2.6 is similar to
(Allen-Zhu & Li, 2019a, Fact D.4 and Claim D.5), where the squared loss is considered.
Using Lemmas 2.3 to 2.6, we can prove Theorem 2.2. Below is a proof sketch; the full proof is
given in Appendix A.
1. We first ShoW that as long as ∣∣ws,t 一 Ws,0k2 ≤ 4λ∕(γ√m) for all 1 ≤ S ≤ m, it holds
that TR(t) (Wo + λU) ≤ e/4. To see this, let us consider R(0) first. For any 1 ≤ i ≤
n, Lemma 2.5 ensures that |hVfi(W0), W0i| is bounded, While Lemma 2.3 ensures that
(Vfi(Wo), U' is concentrated around Y with a large width. As a result, with the chosen
λ in Theorem 2.2, we can show that <Vfi(W°), Wo + λU) is large, and TR(0)(Wo + λU)
is small due to the exponential tail of the logistic loss. To further_handle R(t), we use a
standard NTK argument to control〈Vfi(Wt) — Vfi(Wo), Wo + λU) under the condition
that ∣ws,t - ws,o∣2 ≤ 4λ∕(γ√m).
2.
We then prove by contradiction that the above bound on ∣ws,t - ws,o∣2 holds for at least
the first T iterations. The key observation is that as long as R(t) (Wo + λU) ≤ e∕4, we
can use it and Lemma 2.6 to control τ<t Q(Wτ), and then just invoke ∣ws,t - ws,o∣2 ≤
____ ^ , . . _______
η Pτ<t Q(WT )∕√m∙
EI	~>	^i∕τττ ∖ <	if	∙ t t ∙	♦	ι /c C 八 CCTC
The quantity τ<t Q(Wτ) has also been considered in prior work (Cao & Gu, 2019a;
Nitanda & Suzuki, 2019), where it is bounded by
√t JPτ<t Q(WT)2 using the Cauchy-
Schwarz inequality, which introduces a √t factor. To make the required width depend only
on polylog(1∕), we also need an upper bound on T<t Q(WT) which depends only on
Polylog(1∕e). Since the above analysis results in a √t factor, and in our case Ω(1∕e) steps
are needed, it is unclear how to get a polylog(1∕) width using the analysis in (Cao &
Gu, 2019a; Nitanda & Suzuki, 2019). By contrast, using Lemma 2.6, we can show that
T<t Q(WT) ≤ 4λ∕γ, which only depends on ln(1∕).
3. The claims of Theorem 2.2 then follow directly from the above two steps and Lemma 2.6.
3 Generalization
To get a generalization bound, we naturally extend Assumption 2.1 to the following assumption.
Assumption 3.1. There exists v ∈ H and γ > 0, such that 11 v(z) 11? ≤ 1 for any Z ∈ Rd, and
y /〈v(z), x) 1 [hz, Xi > 0] dμN(Z) ≥ Y
for almost all (x, y) sampled from the data distribution D.	♦
The above assumption is also made in (Nitanda & Suzuki, 2019) for smooth activations. (Cao &
Gu, 2019a) make a similar separability assumption, but in the RKHS induced by the second layer a;
by contrast, Assumption 3.1 is on separability in the RKHS induced by the first layer W.
Here is our test error bound with Assumption 3.1.
Theorem 3.2. Under Assumption 3.1, given any ∈ (0, 1) and any δ ∈ (0, 1∕4), let λ and M be
given as in Theorem 2.2:
λ := p21n4n^ + ln(4∕e),	and M :=瞥2.
Y∕4	Y6
6
Published as a conference paper at ICLR 2020
Jln(2∕δ)
V	2n
Then for any m ≥ M and any constant step size η ≤ 1, with probability 1 - 4δ over the random
initialization and data sampling,
16 (p2 ln(4n∕δ) + ln(4∕c))
P(χ,y)〜D (yf (x; Wk, a) ≤ 0)≤ 2 +----------------γ2√n------------L + 6
where k denotes the SteP with the minimum empirical risk before d2λ2∕ηe].
Below is a direct corollary of Theorem 3.2.
Corollary 3.3. Under Assumption 3.1, given any , δ ∈ (0, 1), using a constant step size no larger
than 1 and let
1
n = Ω ( -ɪ-2 I , and m = Ω
ln(n∕δ) + ln(1∕)2
γ8
it holds with probability 1 一 δ that P(χ,y)〜D (yf (x; Wk, a) ≤ 0)≤ G where k denotes the SteP with
the minimum empirical risk in the first Θ(1∕γ2) steps.
The proof of Theorem 3.2 uses the sigmoid mapping -'0(z) = e-z /(1 + e-z) ,the empirical average
Q(Wk), and the corresponding population average Q(Wk) := E(χ,y)〜D [一'0 (yf (x; Wk, a))]. As
noted in (Cao & Gu, 2019a), because P(x,y)〜D (yf(x; Wk, a) ≤ 0)≤ 2Q(Wk), it is enough to
control Q(Wk). As Q(Wk) is controlled by Theorem 2.2, it is enough to control the generalization
error Q(Wk) — Q(Wk). Moreover, since —' is supported on [0,1] and 1-Lipschitz, it is enough to
bound the Rademacher complexity of the function space explored by gradient descent. Invoking the
bound on Wk> 一 W0> 2,∞ finishes the proof. The proof details are given in Appendix B.
Remark 3.4. To get Theorem 3.2, we use a Lipschitz-based Rademacher complexity bound. One
can also use a smoothness-based Rademacher complexity bound (Srebro et al., 2010, Theorem 1)
and get a sample complexity O(1∕γ4). However, the bound will become complicated and some
large constant will be introduced. It is an interesting open question to give a clean analysis based on
smoothness.
♦
4 Stochastic gradient descent
There are some different formulations of SGD. In this section, we consider SGD with an online
oracle. We randomly sample W0 and a, and fix a during training. At step i, a data example (xi, yi)
is sampled from the data distribution. We still let fi(W) := f(xi; W, a), and perform the following
update
Wi+ι ：= Wi-m' (yifi(Wi)) yiVfi(Wi).
Note that here i starts from 0.
Still with Assumption 3.1, we show the following result.
Theorem 4.1. Under Assumption 3.1, given any , δ ∈ (0, 1), using a constant step size and m =
Ω ((In(I∕6+lnα∕02)∕γ8), it holds with probability 1 — δ that
1n
一£P(x,y)〜D (yf(x; Wi, a) ≤ 0) ≤ 3 for n = Θ(1∕γ%).
n
i=1
Below is a proof sketch of Theorem 4.1; the complete proof is given in Appendix C. For any i and
W, define
Ri(W) ：= ' (yi〈Vfi(Wi), W〉), and Qi(W) := —'0 (统(Vfi(Wi), W〉).
Due to homogeneity, it holds that Ri(Wi) = ' (yifi(Wi)) and Qi(Wi) = —'0 (yifi(Wi)).
7
Published as a conference paper at ICLR 2020
The first step is an extension of Lemma 2.6 to the SGD setting, with a similar proof.
Lemma 4.2. With a constant SteP size η ≤ 1, for any W and any i ≥ 0,
η (XRt(Wtj +∣∣Wi - W∣∣2 ≤∣∣Wo - W∣∣2 + 2η (XRt (W)J
With Lemma 4.2, we can also extend Theorem 2.2 to the SGD setting and get a bound on
Pi<n Qi(Wi), using a similar proof. To further get a bound on the cumulative population risk
Pi<n Q(Wi), the key observation is that Pi<n (Q(Wi) — Qi(Wi)) is a martingale. Using amar-
tingale Bernstein bound, we prove the following lemma; applying it finishes the proof of Theo-
rem 4.1.
Lemma 4.3. Given any δ ∈ (0, 1), with Probability 1 - δ,
X Q(Wt) ≤ 4 X Qt(Wt)+4ln (；).
t<i
t<i
5 On separability
In this section we give some discussion on Assumption 2.1, the separability of the NTK. The proofs
are all given in Appendix D.
Given a training set (xi, yi) in=1, the linear kernel is defined as K0(xi, xj) := xi, xj . The
maximum margin achievable by a linear classifier is given by
γ0
min
q∈∆n
(qy)>K0 (qy).
(5.1)
where ∆n denotes the probability simplex and denotes the Hadamard product. In addition to the
dual definition eq. (5.1), when γo > 0 there also exists a maximum margin classifier U which gives
a primal characterization of γ0: it holds that1同以 =1 and yi〈U,xi)≥ γo for all i.
In this paper we consider another kernel, the infinite-width NTK with respect to the first layer:
KI (xi,Xj) := E Ff(Xi;W0,a)
∂f (Xj； W0,a)
∂W0
Ew~N(0,Id) Kxi1 [hxi,wi > 0] ,xj1 [hxj,wi > 0])] = hφi,φj,H∙
Here φ and H are defined at the beginning of Section 2. Similar to the dual definition of γ0, the
margin given by K1 is defined as
γ1
min
q∈∆n
(q	y)> K1 (q y).
(5.2)
We can also give a primal characterization of γ1 when it is positive.
Proposition 5.1. If γι > 0, then there exists V ∈ H such thatk^∣∣H = 1, and yi hv, φiiH ≥ γι for
any 1 ≤ i ≤ n. Additionally ∣∣V(z)∣∣2 ≤ 1∕γι for any Z ∈ Rd.
The proof is given in Appendix D, and uses the Fenchel duality theory. Using the upper bound
∣∣v(z)∣∣2 ≤ 1/Yi, We can see that γιV satisfies Assumption 2.1 with Y ≥ γ2. However, such an
upper bound ∣∣ V(z) ∣∣ 2 ≤ 1∕γι might be too loose, which leads to a bad rate. In fact, as shown later,
in some cases we can construct v directly which satisfies Assumption 2.1 with a large Y. For this
reason, we choose to make Assumption 2.1 instead of assuming a positive γ1.
However, we can use Y1 to show that Assumption 2.1 always holds when there are no parallel inputs.
Oymak & Soltanolkotabi (2019, Corollary I.2) prove that if for any two feature vectors Xi and Xj,
we have kXi - Xj k2 ≥ θ and kXi + Xj k2 ≥ θ for some θ > 0, then the minimum eigenvalue of K1
is at least θ∕(100n2). For arbitrary labels y ∈ { —1, +1}n, since ∣∣q Θ y∣∣2 ≥ 1 /√n, we have the
worst case bound Y12 ≥ θ∕100n3. A direct improvement of this bound is θ∕100n3S, where nS denotes
the number of support vectors, which could be much smaller than n with real world data.
On the other hand, given any training set (Xi, yi) in=1 which may have a large margin, replacing y
with random labels would destroy the margin, which is what should be expected.
8
Published as a conference paper at ICLR 2020
Proposition 5.2. Given any training set (xi, yi) in=1, if the true labels y are replaced with random
labels E 〜Unif ({ - 1, +1}n) ,then with probability 0.9 over the random labels, it holds that γι ≤
1∕√20n.
Although the above bounds all have a polynomial dependency on n, they hold for arbitrary or random
labels, and thus do not assume any relationship between the features and labels. Next we give some
examples where there is a strong feature-label relationship, and thus a much larger margin can be
proved.
5.1	The linearly separable case
Suppose the data distribution is linearly separable with margin γ0: there exists a unit vector U such
that y hu, Xi ≥ γo almost surely. Then We can define v(z) := U for any Z ∈ Rd. For almost all
(x, y), we have
y
/〈V(z), x)1 [hz, Xi > 0] dμN(Z)
/ y hU, Xi 1 [hz, Xi > 0] dμN(Z)
≥γ
1 [hZ, Xi > 0]
dμN(Z)
Y0
"2
and thus Assumption 2.1 holds with Y = γo∕2.
5.2	The noisy 2-XOR distribution
We consider the noisy 2-XOR distribution introduced in (Wei et al., 2018). It is the uniform distri-
bution over the following 2d points:
(X1, x2, y, x3,..., Xd) ∈ {(√d=-i, 0,1), (0, Td=-ZT, -1), (√--r, 0,1), (0, √--ι, -1)}
× ʃ _-2_______)
l√d^-ι ,√d-ι J	.
The factor 1∕√d-1 ensures that 1⑶卜=1, and X above denotes the Cartesian product. Here the label
y only depends on the first two coordinates of the input X.
To construct v, we first decompose R2 into four regions:
Ai ：=	{(z1,z2)	I	zι ≥ 0,	|zi|	≥	∣z2∣},
A2 :=	{(zi,Z2)	1	Z2 > 0,	|zi|	<	∣Z2∣},
A3 :={(Z1,Z2) IIZ1 ≤ 0, |Z1| ≥ |Z2|} \ {(0, 0)},
A4 := {(Z1, Z2) II Z2 < 0, |Z1| < |Z2|} .
Then v can de defined as follows. It only depends on the first two coordinates of z.
((1,0,0,∙∙∙, 0)	if (z1,z2) ∈ Ai,
= J(0, - 1 , 0, . . . , 0) if (z1, z2) ∈ A2,
I ( —1, 0, 0, . . . , 0) if (z1, z2) ∈ A3,
[(0,1,0,..., 0) if (z1,z2) ∈ A4.
(5.3)
The following result shows that Y = Ω(1∕d). Note that n could be as large as 2d, in which case Y is
basically O (1∕ln(n)).
Proposition 5.3. For any (X, y) sampled from the noisy 2-XOR distribution and any d ≥ 3, it holds
that
y
/(v(Z), x) 1 [hz, Xi > 0] d〃N(Z) ≥ 60d.
We can prove two other interesting results for the noisy 2-XOR data.
9
Published as a conference paper at ICLR 2020
The width needs a Poly(1∕γ) dependency for initial separability. The first step of an NTK
analysis is to show that {(Vfi(Wo), y，仁]is separable. Proposition 5.4 gives an example where
{(Vfi(W0), y, }n=ι is nonseparable when the network is narrow.
Proposition 5.4. Let D = {(xi, yi)}i4=1 denote an arbitrary subset of the noisy 2-XOR dataset
such that Xi's have the same last (d 一 2) coordinates. For any d ≥ 20, f m ≤ √d — 2/4, then
with probability 1/2 over the random initialization of W0, for any weights V ∈ Rm×d, it holds that
yi V, Vfi(W0) ≤ 0 for at least one i ∈ {1, 2, 3, 4}.
For the noisy 2-XOR data, the separator V given by eq. (5.3) has margin Y = Ω(1∕d), and 1∕γ =
O(d). As a result, if We want {(Vfi(W0), y, }n=] to be separable, the width has to be Ω(1∕√γ).
For a smaller width, gradient descent might still be able to solve the problem, but a beyond-NTK
analysis would be needed.
A tight sample complexity upper bound for the infinite-width NTK. (Wei et al., 2018) give a
d2 sample complexity lower bound for any NTK classifier on the noisy 2-XOR data. It turns out that
γ could give a matching sample complexity upper bound for the NTK and SGD.
(Wei et al., 2018) consider the infinite-width NTK with respect to both layers. For the first layer, the
infinite-width NTK K1 is defined in Section 5, and the corresponding RKHS H and RKHS mapping
φ is defined in Section 2. For the second layer, the infinite width NTK is defined by
∂fE; Wo,a) ∂f(xj; Wo,a)
∂a
∂a
Ew〜N(0,Id) [σ (hw,χii) σ (hw,xj〉)].
The corresponding RKHS K and inner product hw1, w2iK are given by
K := w : Rd → R
JW(Zy dμN(z) < ∞} , and hιw1,w2)κ = / w1(z)w2(z)dμN(z).
Given any x ∈ Rd, it is mapped into ψx ∈ K, where ψx (z) := σ hz, xi . It holds that K2 (xi, xj) =
hψxi, ψxjiK. The infinite-width NTK with respect to both layers is just K1+K2. The corresponding
RHKS is just H × K with the inner product
h(v1, w1), (v2, w2)iH×K = hv1,v2iH + hw1, w2iK.
The classifier V considered in eq. (5.3) has a unit norm (i.e., |历||制 =1) and margin Y on the space
H. On HX K, it is enough to consider (V, 0), which also has a unit norm and margin γ. Since the
infinite-width NTK model is a linear model in H × K, (Ji & Telgarsky, 2018, Lemma 2.5) can be
used to show that SGD on the RKHS H×K could obtain a test error of with a sample complexity of
O(1∕γ2e). (The analysis in (Ji & Telgarsky, 2018) is done in Rd, but it still works with a well-defined
inner product.) Since Y = Ω(1 / d) ,to achieve a constant test accuracy we need O(d2) samples. This
mathces (up to logarithmic factors) the sample complexity lower bound of d2 given by Wei et al.
(2018).
6	Open problems
In this paper, we analyze gradient descent on a two-layer network in the NTK regime, where the
weights stay close to the initialization. It is an interesting open question if gradient descent learns
something beyond the NTK, after the iterates move far enough from the initial weights. It is also
interesting to extend our analysis to other architectures, such as multi-layer networks, convolutional
networks, and residual networks. Finally, in this paper we only discuss binary classification; it is
interesting to see ifit is possible to get similar results for other tasks, such as regression.
Acknowledgements
The authors are grateful for support from the NSF under grant IIS-1750051, and from NVIDIA via
a GPU grant.
10
Published as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? arXiv
preprint arXiv:1905.10337, 2019a.
Zeyuan Allen-Zhu and Yuanzhi Li. Can sgd learn recurrent neural networks with provable general-
ization? arXiv preprint arXiv:1902.01028, 2019b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. arXiv preprint arXiv:1810.12065, 2018c.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. JMLR, 3:463-482, Nov 2002.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit
algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics, pp. 19-26, 2011.
Jonathan M. Borwein and Qiji J. Zhu. Techniques of Variational Analysis, volume 20 of. CMS
Books in Mathematics, 2005.
Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019a.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. arXiv preprint arXiv:1905.13210, 2019b.
Lenaic Chizat and Francis Bach. A Note on Lazy Training in Supervised Differentiable Program-
ming. arXiv:1812.07956v2 [math.OC], 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300v2, 2018.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Percy Liang. Stanford CS229T/STAT231: Statistical Learning Theory, Apr 2016. URL https:
//web.stanford.edu/class/cs229t/notes.pdf.
Atsushi Nitanda and Taiji Suzuki. Refined generalization analysis of gradient descent for over-
parameterized two-layer neural networks with smooth activations on classification problems.
arXiv preprint arXiv:1905.09870, 2019.
11
Published as a conference paper at ICLR 2020
Albert B.J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on
the Mathematical Theory ofAutomata, 12:615-622, 1962.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-
gence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674, 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.
arXiv preprint arXiv:1906.03593, 2019.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In
Advances in neural information processing systems, pp. 2199-2207, 2010.
Martin J. Wainwright. UC Berkeley Statistics 210B, Lecture Notes: Basic tail and concentration
bounds, Jan 2015. URL https://www.stat.berkeley.edu/~mjwain/stat210b/
Chap2_TailBounds_Jan22_2015.pdf.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. arXiv preprint arXiv:1810.05369, 2018.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. arXiv preprint arXiv:1906.04688, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
A Omitted proofs from Section 2
Proof of Lemma 2.3. By Assumption 2.1, given any 1 ≤ i ≤ n,
On the other hand,
> 0
μ := Ew〜N(0,id)
yi
≥ γ.
yi
is the empirical mean of i.i.d. r.v.’s supported on [-1, +1] with mean μ. Therefore by Hoeffding's
inequality, with probability 1 - δ∕n,
f (U) - γ ≥ yifi(0) (U) - μ ≥-rr2nnδ.
Applying a union bound finishes the proof.	□
Proof of Lemma 2.4. Given any fixed 2 and 1 ≤ i ≤ n,
Elai(W。,切]=P(Ihw,xiiI ≤ e2)≤ √√2∏ = W'⅛,
because hw, xii is a standard Gaussian r.v. and the density of standard Gaussian has maximum
1/√2π. Since ɑi(Wo,⑦)is the empirical mean of Bernoulli r.v.'s, by Hoeffding,s inequality, with
probability 1 - δ∕n,
αi(Wo, £2)≤E [αi(W0, £2)]+r lnnδ) ≤ r∏ £2+rɪnn/ɪ).
Applying a union bound finishes the proof.	□
To prove Lemma 2.5, we need the following technical result.
12
Published as a conference paper at ICLR 2020
Lemma A.1. Consider the random vector X = (X1, . . . , Xm), where Xi = σ(Zi) for some σ :
R → R that is 1-Lipschitz, and Zi are i.i.d. standard Gaussian r.v.’s. Then the r.v. kXk2 is 1-sub-
Gaussian, and thus with probability 1 - δ,
kXk2 - E[kXk2] ≤ P2ln(1∕δ).
Proof. Given a ∈ Rm , define
um
f(a) = uX σ(ai)2 =σ(a)2 ,
i=1
where σ(a) is obtained by applying σ coordinate-wisely to a. For any a, b ∈ Rm, by the triangle
inequality, we have
f (a) - f (b) = σ(a)2 -σ(b)2 ≤ σ(a) - σ(b)2
m
X(σ(ai) - σ(bi))2,
i=1
∖
and by further using the 1-Lipschitz continuity of σ, we have
f(a)-f(b)≤
m
X(σ(ai) — σ(bi))2 ≤
i=1
∖
m
X(ai -bi)2 =ka-bk2 .
i=1
As a result, f is a 1-Lipschitz continuous function w.r.t. the `2 norm, indeed f(X) is 1-sub-Gaussian
and the bound follows by Gaussian concentration (Wainwright, 2015, Theorem 2.4).	□
ProofofLemma 2.5. Given 1 ≤ i ≤ n, let hi = σ(W°Xi)∕√m. ByLemmaA.1, ∣∣hik2 is SUb-
Gaussian with variance proxy 1∕m, and with probability at least 1 - δ∕2n over W0,
khik2 - E [khik2] ≤ r2ln(2n∕δ) ≤ S 2ln(2n/f). ≤ 1 - √2.
11 il 12 l" i"2」—V m - 25 25ln(2n∕δ) 一 2
On the other hand, by Jensen’s inequality,
E [khik2] ≤ Je [khik2] = E.
As a result, with probability 1 - δ∕2n, it holds that khik2 ≤ 1. By a union bound, with probability
1 - δ∕2 over W0, for all 1 ≤ i ≤ n, we have khik2 ≤ 1.
For any W0 such that the above event holds, and for any 1 ≤ i ≤ n, the r.v. hhi , ai is sub-Gaussian
with variance proxy khik22 ≤ 1. By Hoeffding’s inequality, with probability 1 - δ∕2n over a,
∣hhi, ai∣ = ∣f (xi； Wo,a)∣ ≤ Jain (4n∕δ).
By a union bound, with probability 1 - δ∕2 over a, for all 1 ≤ i ≤ n, we have f(xi; W0, a) ≤
J2in (4n∕δ).
The probability that the above events all happen is at least (1 - δ∕2)(1 - δ∕2) ≥ 1 - δ, over W0 and
a.	□
Proof of Lemma 2.6. We have
M+1- W ∣2 = M- WIj - 2ηt DvtR (Wt ),Wt - W E + η2 卜R(Wt)Ij.	(a.i)
13
Published as a conference paper at ICLR 2020
The first order term of eq. (A.1) can be handled using the convexity of` and homogeneity of ReLU:
n
(vτR(Wt), Wt- W)= — X' (yifi(Wt) yi Rfi(Wt) Wt- W)
n i=1
n
=-X' Efi(Wt)) QHfi(Wt>-yifitt (W))
n i=1
≥ n X (' (yifi(Wt)) - ' Qiw (W))) = RR(Wt)- RR⑴(W).
i=1	(A.2)
The second-order term of eq. (A.1) can be bounded as follows
ηt2vRb (Wt)2 ≤ ηt2Qb(Wt)2 ≤ ηtQb(Wt) ≤ηt Rb (Wt),	(A.3)
becausevRb (Wt)	≤	Qb(Wt),	and ηt,	Qb(Wt)	≤	-,	and Qb(Wt)	≤ Rb (Wt).	Combining eqs. (A.1)
to (A.3) gives
ηtR(Wt) ≤ IlWt- W∣∣2 -∣∣Wt+ι - W∣∣2 + 2ntR(t) (W).
Telescoping gives the other claim.	□
ProofofTheorem 2.2. The required width ensures that with probability 1 一 3δ, Lemmas 2.3 to 2.5
hold with ∈ι = γ2∕8 and e? = 4λ∕(γ√m).
Let tι denote the first step such that there exists 1 ≤ S ≤ m with ∣ws,tι 一 ws,0∣∣2 > 4λ∕(γ√m).
Therefore for any 0 ≤ t <tι and any 1 ≤ S ≤ m, it holds that ∣ws,t 一 ws,01∣2 ≤ 4λ∕(γ√m). In
addition, We let W := Wo + λU.
We first prove that for any 0 ≤ t < tι, it holds that R(t) (W) ≤ e/4. Since ln(1 + r) ≤ r for any
r, the logistic satisfies `(z) = ln(1 + exp(-z)) ≤ exp(-z), and it is enough to prove that for any
1 ≤ i ≤ n,
yi DVfi(Wt),WE ≥ ln(g).
We will split the left hand side into three terms and control them individually:
y DVfi(Wt), WE = yi〈Vfi(Wo), Wo〉+ yi CVfi(Wt)- Vfi(Wo), Wo) + λy DVfi(Wt)QE ∙
(A.4)
•	The first term of eq. (A.4) can be controlled using Lemma 2.5:
Iyi〈Vfi(Wo), Wo) J ≤ P2ln(4n∕δ).	(A.5)
•	The second term of eq. (A.4) can be written as
yi Vfi(Wt) -Vfi(Wo),Wo =yi
1m
m Xa，s(1 [<Ws,t,Xi) > 0] - 1 [<Ws,o,Xi> > 0]) (Ws,o,Xi).
s=1
Let Sc := S	II	1	ws,t, xi)	>	0]	- 1	ws,o, xi)	>	0]	6=	0, 1 ≤ S ≤ m . Note that S ∈
Sc implies
J〈Ws,o,Xi)J ≤ |〈Ws,t - Ws,o,Xi)∣ ≤ ∣∣Ws,t - Ws,o∣∣2 IlxiIl2 = ∣∣Ws,t - Ws,o∣∣2 ≤ 4λ∕(γ√m) = 0
14
Published as a conference paper at ICLR 2020
Therefore Lemma 2.4 ensures that
|Sc| ≤ ns
| ws,0, xi |
and thus
∣yi (^fi(Wt) - Vfi(W0),	W0〉| ≤	√m	∙∣Sc∣∙ γ√m	≤	γ26√= +	λγ	≤	λ2γ,	(A.6)
where in the last step We use the condition that m ≥ 4096λ2∕γ6.
•	The third term of eq. (A.4) can be bounded as follows: by Lemma 2.3,
y Rfi(Wt)RE = y DVfi(Wo),u) + y Rfi(Wt)- VfWU
≥ Y - e1 + y DVfi(Wt)- Vfi(WO), UE .
In addition,
1 m /	∖
yi (Vfi(Wt)- Vfi(Wo), U = = yi— X(1 ［〈ws,t,xi〉> θ］ — 1 ［〈Ws,o,Xi〉> θj <v(ws,0), x0
m i=1
≥-
ɪ ∙ IScI ≥ -ɪ
Ym
2
1	Y2
-^2 ≥ - 16
1
2^,
m
—
where we use m ≥ 4096λ2∕γ6. Therefore,
y (Vfi(WjU) ≥ γ -eι
Y2
16
旦
2^
(A.7)
—
—
Y -亡≥也
4	4 _ 4
Putting eqs. (A.5) to (A.7) into eq. (A.4), we have
yi DVfi(Wt), W E ≥-S2ιn( 4n)-λγ+ɪ=λγ - s2ιn( 4n)=ιn e)，
for the λ given in the statement of Theorem 2.2. Consequently, for any 0 ≤ t < t1, it holds that
元⑶(W) ≤ e/4.
Let T := d2λ2∕η€e. The next claim is that tι ≥ T. To see this, note that Lemma 2.6 ensures
IWtI- W∣∣2 ≤ Il Wo - W∣∣2 + 2η (X TR⑴(W)j ≤ λ2 + ∣ηtι.
Suppose t1 < T, then we have tι ≤ 2λ2∕ne, and thus ∣∣Wtι — W∣∣	≤ 2λ2 . As a result, using
kU∣F ≤ 1 and the definition of W,
√2λ ≥∣∣Wtι - W∣∣尸 ≥(叫1 - W, uE = (Wtι - Wo,uE - DW - Wo, uE
≥ (Wtι - Wo, UE - λ.
Moreover, due to eq. (A.7),
n
(Wtι - Wo,U)= -η X (VTR(WT),u) = η X n x -`0 (yifi(Wτ)) yi(Vfi(WT ),u)
τ<t1
τ<t1 i=1
≥ η X Q(WT)34Y.
τ<t1
15
Published as a conference paper at ICLR 2020
As a result,
η	Qb(Wτ) ≤
τ<t1
4(√2+1)λ
3γ
4λ
Y
Furthermore, by the triangle inequality, for any 1 ≤ s ≤ m
ws,t -ws,02 ≤ η
1n
X n X'0 (y∙fi(Wτ))
τ<t
i=1
∂f Il
yi dws,τ L
n
≤η X n χκ0(yifi (WT 计 I ∂Wf7∣
τ<t n i=1	ws,τ 2
X1
Q(WT) -r
τ<t	√m
1	4λ
≤ η E Q(WT) √= ≤ —√=,
W	√m	γ√m
(A.8)
which contradicts the definition of t1 . Therefore t1 ≥ T .
Now we are ready to prove the claims of Theorem 2.2. The bound on Iws,t - ws,0 I2 follow by
repeating the steps in eq. (A.8). The risk guarantee follows from Lemma 2.6:
T X R(Wt) ≤ M -TWIIF + T X R ㈤(W) ≤ 1 + 1 = e.
η
t<T	t<T
□
n
suP	1ih(zi)
h∈H i=1
B Omitted proofs from Section 3
The proof of Theorem 3.2 is based on Rademacher complexity. Given a sample S = (z1, . . . , zn)
(where zi = (xi, yi)) and a function class H, the Rademacher complexity of H on S is defined as
Rad (H ◦ S) := 一Eg{-ι +1}
n,
We will use the following general result.
Lemma B.1. (Shalev-Shwartz & Ben-David, 2014, Theorem 26.5) If h(z) ∈ [a, b], then with prob-
ability 1 - δ,
Jln(2∕δ)
V	2n
SuP I Ez~d ∖h(z)] — X X h(zi) I ≤ 2Rad (H ◦ S) + 3(b - a)
h∈H	ni=1
We also need the following contraction lemma. Consider a feature sample X = (x1, . . . , xn) and
a function class F on X . For each 1 ≤ i ≤ n, let gi : R → R denote a K-Lipschitz function. Let
g ◦ F denote the class of functions which map xi to gi(f(xi)) for some f ∈ F.
Lemma B.2. (Shalev-Shwartz & Ben-David, 2014, Lemma 26.9) Rad (g ◦ F ◦ X)	≤
KRad(F ◦X).
To prove Theorem 3.2, we need one more Rademacher complexity bound. Given a fixed initializa-
tion (W0, a), consider the following classes:
Wρ := nW ∈ Rm×d IIws - ws,0II2 ≤ ρ for any 1 ≤ s ≤mo ,
16
Published as a conference paper at ICLR 2020
and
Fρ ：= {x → f (x; W, a) I W ∈ Wρ}.
Given a feature sample X, the following LemmaB.3 controls the Rademacher complexity of FP ◦X.
A similar version was given in (Liang, 2016, Theorem 43), and the proof is similar to the proof of
(Bartlett & Mendelson, 2002, Theorem 18) which also pushes the supremum through and handles
each hidden unit separately.
Lemma B.3. Rad (FP ◦ X)≤ PdmIn.
Proof of Lemma B.3. We have
n
sup	if(xi; W, a)
W∈Wρ i=1
n m1
SUP EGE 尸
W ∈Wρ i=1 W √m
1	mn
-sup ΣΣ iasσ hws, xii
Zm W ∈Wρ R i=i
1m	n
S= sUP sup	Eeiasσ (hws,Xii)
m s=1 kws -ws,0 k2 ≤P i=1
m
XE
i=1
1
n
sup	iasσ hws, xii
kws -ws,0 k2≤P i=1
Note that for any 1 ≤ s ≤ m, the mapping z 7→ asσ(z) is 1-Lipschitz, and thus Lemma B.2 gives
n
sup	if(xi; W, a)
W∈Wρ i=1
1m
≤√m X Ee
i=1
1m
≤√m X Ee
i=1
kw
n
sup	iasσ hws, xii
-ws,0 k2 ≤P i=1
kw
sup
-ws,0k2≤P
i=1
s
s
n
Invoking the Rademacher complexity of linear classifiers (Shalev-Shwartz & Ben-David, 2014,
Lemma 26.10) then gives
Rad (FP ◦ X) = 1 Ee
n
n
sup	if(xi; W, a)
W∈Wρ i=1
/ P√m
一√n .
□
Now we are ready to prove the main generalization result Theorem 3.2.
Proof. Fix an initialization (Wo,a), and let H := {(x,y) → -'0 (yf (x)) ∣ f ∈ Fρ}. Since for
any h ∈ H and any z, h(z) ∈ [0, 1], Lemma B.1 ensures that with probability 1 - δ over the data
sampling,
1n
Ez〜D [h(z)]——V"h(zi)
n i=1
SUP (Q(W)-Q(W)) ≤ 2Rad (H。S ) + 3/皿21"
W∈Wρ	2n
Since for each 1 ≤ i ≤ n, the mapping z 7→ -`0 (yi z) is (1/4)-Lipschitz, Lemma B.2 further
ensures that Rad (H ◦ S) ≤ Rad FP ◦ X /4, and thus
WUW P(Q(W)-Q(W ))≤ ρ√√m + √ln≡
(B.1)
17
Published as a conference paper at ICLR 2020
On the other hand, Theorem 2.2 ensures that under the conditions of Theorem 3.2, for any fixed
dataset, with probability 1 - 3δ over the random initialization, we have
Qb(Wk) ≤ Rb(Wk) ≤ ,
and
ws,k - ws,02
4λ
As a result, invoking eq. (B.1) with P = 4λ∕(γ√m), with probability 1 - 4δ over the random
initialization and data sampling,
Q(Wk) ≤ Q(Wk) + ɪ + 3 Jln2®
γ n	2n
≤+
8 (p2 ln(4n∕δ) + ln(4∕c))
γ2√n
+ 3∕n≡
2n
Invoking P(χ,y)〜D (yf(x; W, a) ≤ 0)≤ 2Q(W) finishes the proof.
□
C Omitted proofs from Section 4
ProofofLemma 4.2. RecanthatIl Vft(Wt)IIF ≤ 1, we have
∣∣Wt+ι - W∣∣2 ≤ IlWt - W∣∣2 - 2η'0 (ytft(Wt)) yt DVft(Wt), Wt- WE + η2 ('0 (ytft(Wt)))2.
(C.1)
Similar to the proof of Lemma 2.6, the first order term of eq. (C.1) can be handled using the convexity
of ` and homogeneity of ReLU as follows
'0 (ytft(Wt)) yt DVft(Wt), Wt- WE ≥ Rt(Wt) - Rt(W) ,	(C.2)
and the second-order term of eq. (C.1) can be bounded as follows
η2 ('0 (ytft(Wt)))2 ≤ -η'0 (ytft(Wt)) ≤ η' (ytft(Wt)) = ηRt(Wt),	(C.3)
since η, -`0 ≤ 1 and -`0 ≤ `. Combining eqs. (C.1) to (C.3) gives
ηRt(Wt) ≤∣∣Wt - W∣∣2 -∣∣Wt+ι - W∣∣2 + 2ηRt (W).
Telescoping gives the claim.	□
With Lemma 4.2, we give the following result, which is an extension of Theorem 2.2 to the SGD
setting.
Lemma C.1. Under Assumption 3.1, given any ∈ (0, 1), any δ ∈ (0, 1/3), and any positive
integer n0, let
λ	p2 ln(4no∕δ) + ln(4∕e) d M 4096λ2
λ :	,	, α<tvd^ /m :	.
γ∕4	γ6
For any m ≥ M and any constant step size η ≤ 1, ifn0 ≥ n := d2λ2∕ηe, then with probability
1 - 3δ,
1 X Qi(Wi) ≤ e.
n
i<n
Proof. We first sample n0 data examples (x0, y0), . . . , (xn0-1, yn0-1), and then feed (xi, yi) to
SGD at step i. We only consider the first n0 steps.
The proof is similar to the proof of Theorem 2.2. Let nι denote the first step before n° such that
there exists some 1 ≤ S ≤ m with ∣∣ws,m - ws,0∣∣2 > 4λ∕(γ√m). If such a step does not exist, let
n1 = n0.
Let W := Wo + λU, in exactly the same way as in Theorem 2.2, we can show that with probability
1 - 3δ, for any 0 ≤ i < n1 ,
yi DVfi(Wi),WE ≥ ln (J , andthus Ri (W) ≤ 〃4.
18
Published as a conference paper at ICLR 2020
Now consider n := d2λ2∕ηe]. Using Lemma 4.2, in the same way as the proof of Theorem 2.2
(replacing Q(Wτ) with Qi(Wi), etc.), we can show that n ≤ n1. Then invoking Lemma 4.2 again,
we get
1 X Qi(Wi)	≤ 1XRi(Wi)	≤ 'W0 -叫F	+ 2 XRi	(W) ≤ B + I	= e.
n	n	ηn	n	2	2
i<n	i<n	i<n
□
Next we prove Lemma 4.3. We need the following martingale Bernstein bound.
Lemma C.2. (Beygelzimer et al., 2011, Theorem 1) Let (Mt, Ft)t≥0 denote a martingale with M0 =
0 and F0 be the trivial σ-algebra. Let (∆t)t≥1 denote the corresponding martingale difference
sequence, and let
t
Vt :=XE ∆j2Fj-1
j=1
denote the sequence of conditional variance. If∆t ≤ R a.s., then for any δ ∈ (0, 1), with probability
at least 1 - δ,
Mt ≤ R (e — 2)+ R ln β
Proof of Lemma 4.3. For any i ≥ 0, let zi denote (xi, yi), and z0,i denote (z0, . . . , zi). Note that
the quantity Pt<i Q(Wt) - Qt(Wt) is a martingale w.r.t. the filtration σ(z0,i-1). The martingale
difference sequence is given by Q(Wt) - Qt(Wt), which satisfies
Q(Wt)- Qt(Wt) = E(x,y)~D [-'0 (yf(x; Wt, a))] + '0 (ytf (xt； Wt, a)) ≤ 1,
since -1 ≤ `0 ≤ 0. Moreover, we have
E h(Q(Wt) -Qt(Wt))2∣σ(zo,t-1)i
= Q(Wt)2 - 2Q(Wt)E Qt(Wt)∣∣σ(z0,t-1) +EhQt(Wt)2∣∣∣σ(z0,t-1)i
= - Q(Wt)2 +E hQt(Wt)2∣∣∣σ(z0,t-1)i
≤E hQt(Wt)2∣∣∣σ(z0,t-1)i
≤E Qt(Wt)∣∣σ(z0,t-1)
= Q(Wt).
Invoking Lemma C.2 with eqs. (C.4) and (C.5) gives that with probability 1 - δ,
X (Q(Wt)- Qt(Wt)) ≤ (e —2) X Q(Wt) + ln (δ).
Consequently,
X Q(Wt) ≤ 4 X Qt(Wt)+4ln (1).
(C.4)
(C.5)
□
Finally, we prove Theorem 4.1.
Proofof Theorem 4.1. Suppose the condition ofLemma C.1 holds. Then we have for n = d2λ2∕ηe],
with probability 1 - 3δ,
1 X Qi(Wi) ≤ I.
n
i<n
19
Published as a conference paper at ICLR 2020
Further invoking Lemma 4.3 gives that With probability 1 - 4δ,
n χQ(Wi) ≤ 4 χQi(Wi)+n ln( 1)≤ 5e.
i<n
i<n
Since P(χ,y)〜D (yf (x； W, a) ≤ 0) ≤ 2Q(W), We get
1n
-∑P(χ,y)-D (yf (x； Wi, a) ≤ 0) ≤ 10E∙
n i=1
For the condition of Lemma C.1 to hold, it is enough to let
Which gives
M = θ(ln(1"ln(1/e)2 !	and n = θ(皿1/2:1/^ ).
□
D Omitted proofs from Section 5
Proof of Proposition 5.1. Define f : H → R by
f (W) = I Z kw(z)k2 dμN (Z) = IkwkH.
It holds that f is continuous, and f * has the same form. Define g : Rn → R by
g(p) := max pi,
1≤i≤n
With conjugate
*	0, ifq∈∆n,
g* (q) =
+∞, o.W.
Finally, define the linear mapping A : H → Rn by (Aw)i =yi hw, φiiH.
Since f, f*, g and g* are loWer semi-continuous, and domg - Adom f = Rn, and dom f* -
A* dom g * = H, Fenchel duality may be applied in each direction (BorWein & Zhu, 2005, Theorem
4.4.3), and ensures that
inf (f(w) + g(Aw)) = SUp (-f*(A*q) — g*(-q)).
w∈H	q∈Rn
with optimal primal-dual solutions (w, q). Moreover
inf, (f (w) + g(Aw)) =	inf „ SUp (f (w) + g(Aw + u) + hq, Ui)
w∈H	w∈H,u∈Rn q∈Rn
≥ SUp inf	f(w) + g(Aw + u) + hq, ui
q∈Rn w∈H,u∈Rn
= SUp inf „ ((f (w) -(A* q,wi)H + (g(Aw + u) — h—q, Aw + Ui)
q∈Rn w∈H,u∈Rn	H
=SUp (-f*(A*q) — g*(-q)).
q∈Rn
By strong duality, the inequality holds with equality. It follows that
wq = A*qq,	and supp(—qq) ⊂ arg max (Awq)i.
1≤i≤n
20
Published as a conference paper at ICLR 2020
Now let us look at the dual optimization problem. It is clear that
SUp (-f*(A*q)- g*(-q)) = - inf f*(A*q).
q∈Rn	q∈∆n
In addition, we have
n
2
f *(A* q)
1∕X
'j √-1
i=1
qiyiφi(z)∣∣	dμN(z)
1n
2 Eqiqj yiyj 3(Z),φj (z))dμN(Z)
i,j=1
1n
2 x
i,j=1
1n
2 x
qiqj yiyj / (φi (z),φj(Z)〉dμN(Z)
qiqj myj Kι(i,j) = ∣(q © y)>Kι(q © y)
i,j=1
and thus f *(A*q) = γ12∕2. Since W = A*q, WehavethatkwkH = γ1. In addition,
g(Aw) = -f * (A* q) — f (W) = -γ2,
and thus -W has margin γ2. Moreover, We have
n
n
w(z) = EqiyiΦi(z) = EqiyixiI [hz,χ” > 0],
i=1	i=1
and thus Ilw(Z)b ≤ 1. Therefore, V = -w∕γι satisfies all requirements of Proposition 5.1. 口
Proofof Proposition 5.2. Let q denote the uniform probability vector (1∕n,..., 1∕n). Note that
n1
Ee~unif({-1, + 1}n) B © E) KI (q © e)] = Ee~unif ({-1,+ 1}n)	X / Eiej KI(Xi,xj )
i,j=1
1n
=η Σ Ee~unif({-1,+ 1}n) [eiejKI(Xi,xj)]
i,j=1
1n	1
=n ∑Kι(χi,xi) = 2n.
i=1
Since 0 ≤ (q © e)> K1 (q © E) ≤ 1 for any e, by Markov,s inequality with probability 0.9, it holds
that (q © e)> Ki (q © E) ≤ 1∕(20n), and thus γι ≤ 1∕√20n.	口
Proof of Proposition 5.3. By symmetry, We only need to consider an (X, y) Where (X1, X2, y)
(1∕√d-Γ, 0,1). Let Zp,q denote (Zp, Zp+i,..., Zq), and similarly define Xp,q. We have
y /〈V(z), x)1 [{z, Xi > 0] dμN(z)
y/ (/ <V(z),x) 1 [{z,x> > 0] dμN(Z3,d)) dμN(Z1,2)
y/ <V(z)1,2,X1,2) (/ 1 h〈Z1,2,xi,2)+ (Z3,d,X3,d) >。] dμN(Z3,d)) dμN(Z1,2)
(D.1)
(D.2)
Xy / <V(z)1,2,xi,2〉(/ l[〈Zi2Xi,2)+〈Z3,d,X3,d)>。] dμN(Z3,d)) 1 [zi,2 ∈ Ai] dμN(Z1,2),
i=1	(D.3)
21
Published as a conference paper at ICLR 2020
where eq. (D.1) is due to the independence between z1,2 and z3,d, and in eq. (D.2) we use the fact that
v(z)1,2 only depends on z1,2 and v(z)3,d are all zero. Since ^v(z)1,2,χ1,2) = 0 for z1,2 ∈ A2 ∪ A4,
we only need to consider A1 and A3 in eq. (D.3). For simplicity, we will denote z1,2 by p ∈ R2, and
v(z)1,2 by v(p), and z3,d by q ∈ Rd-2.
For any nonzero P ∈ A1,we have -P ∈ A3, and (v(p), x1,2) = 1∕√d - 1 .Therefore
[<p,χι,2) 十 <q,χ3,d> > 0 dμN(q)
十 y〈v(-P),xι,2> (/1 [<-p,χι,2) 十〈q,x3,d〉> 0i dμN(q)
q, x3,d > 0
≤ q, x3,d ≤
卜 〃n (q)
(D.4)
Let 夕 denote the density function of the standard Gaussian distribution, and for c > 0, let U(C)
denote the probability that a standard Gaussian random variable lies in the interval [-c, c]:
U(c) :
/° 以t)dt.
-c
Since〈q, χ3,d) is a Gaussian variable with standard deviation，(d-2)/(d-i), we have
(D.5)
Plugging eqs. (D.4) and (D.5) into eq. (D.3) gives:
y [〈v(z), x〉1 [hz, Xi > 0] dμN(Z) = 1—r U u (-7==) 1 [p ∈ Ai] dμN(P)
d-1	d-2
=√==j= Z U (√p=) ( Z	^(P2)dP2 J θ(Pl)dPi
d - 1 0	d - 2	-p1
T⅛ 0∞ U (√⅛) U(P")dP1
≥
U (pi)夕(P1)dP1.
For t ∈ [-1, +1], it holds that 夕(t) ≥ 1√2πe, and thus
U (a) = Z 夕⑴ dt ≥ √=.
-a	2πe
Therefore eq. (D.3) is lower bounded by
Γ1-Γ / U (√p= ) U(PiW(Pi)dPi ≥ -F= / -J= ∙ √f⅛ ∙ √2p= ∙ √=dPi
d - 1 0 d - 2	d - 1 0	2πe	d - 2	2πe	2πe
1	i2
≥ 20p(d - 1)(d-2) J0 Pi Pi
__	1
=60P(d - 1)(d - 2)
1
≥ ---;.
一 60d
□
22
Published as a conference paper at ICLR 2020
To prove Proposition 5.4, We need the following technical lemma.
Lemma D.1. Given zι 〜N(0,1) and z2〜N(0, b2) that are independent where b > 1, we have
P(∣zι∣ < ∣z2∣) > 1 - ；.
Proof. First note that for zɜ 〜N(0,1) which is independent of z1,
p (∣zι∣ < ∣z2∣) = p (∣zι∣ < b∣z3∣) = 1 - P (∣z3∣ < b∣zι∣).
Still let 夕 denote the density of N(0,1), and let U(C) denote the probability that z3 ∈ [-c, c]. We
have
1
∣z3∣ < b∣z1
1
P
∣Z3∣ <b∣Z1∣ 2(z3)夕(z1)dz3 dzι
/ U (b ∣zι∣)夕(ZI)dzι
∣z1∣^(z1)dz1
2	1
πb ‹ b,
where we use the facts that U(c) ≤ 2c/√2π and E[∣z1∣] = ,2∕π.
□
We now give the proof of Proposition 5.4 using Lemma D.1.
Proofof Proposition 5.4. By symmetry, we only need to consider the following training set:
xι = (1, 0,1,..., I), yι = 1,
χ2 = (0,1,1,..., 1), y2 = -1,
x3 = (-1, 0, 1, . . . , 1), y3 = 1,
χ4 = (0, -1,1,..., 1), y4 = -1.
The 1 /√d - 1 factor is omitted also because we only discuss the 0/1 loss.
For any s, let AS denote the event that
1 [<Ws,X1i > 0] = 1 [(Ws,X2)> 0] = 1 [〈Ws,X3)> 0] = 1 [(ws,X4)>。].
We will show that if m ≤ √d - 2/4, then AS is true for all 1 ≤ S ≤ m with probability 1/2, and
Proposition 5.4 follows from the fact that the XOR data is not linearly separable.
For any S and i,
d
hw,Xii = (ws)1(xi)1 + (ws)2 (xi)2 + E(WS)j .
j=3
Since ((Xi)1, (g)2) is (1,0) or (0,1) or (-1,0) or (0, -1), event AS will happen as long as
d
d
j=3
s=3
Note that (ws)ι, (ws)2 〜N (0,1) while Pd=3(ws)j 〜N (0, d-2). As a result, due to Lemma D.1,
/
d
/
d
P
∖
j=3
s=3
> 1-√⅛
P
∖
Using a union bound, P(AS) > 1 - 2/vd-2. If m ≤ √d - 2/4, then by a union bound again,
P ( U ASl > 1 - ɪm ≥ 1 - ɪ 工=1.
∖1≤‰	)	十 ≥ 十 4	2
□
23