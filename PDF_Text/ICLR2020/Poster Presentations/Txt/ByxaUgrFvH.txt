Published as a conference paper at ICLR 2020
Mutual Information Gradient Estimation for
Representation Learning
Liangjian Wen1,2, Yiji Zhou1 *, Lirong He1, Mingyuan Zhou3, Zenglin Xu4,2,1
1	SMILE Lab, School of Computer Science and Engineering
University of Electronic Science and Technology of China, Chengdu, China
2	Center for Artificial Intelligence
Peng Cheng Laboratory, Shenzhen, China
3	McCombs School of Business
University of Texas at Austin, Austin, United States
4	School of Computer Science and Technology
Harbin Institute of Technology, Shenzhen, China
wlj6816@gmail.com,zhouyiji@outlook.com,ronghe1217@gmail.com,
mingyuan.zhou@mccombs.utexas.edu,xuzenglin@hit.edu.cn
Ab stract
Mutual Information (MI) plays an important role in representation learning. How-
ever, MI is unfortunately intractable in continuous and high-dimensional settings.
Recent advances establish tractable and scalable MI estimators to discover useful
representation. However, most of the existing methods are not capable of pro-
viding an accurate estimation of MI with low-variance when the MI is large. We
argue that directly estimating the gradients of MI is more appealing for represen-
tation learning than estimating MI in itself. To this end, we propose the Mutual
Information Gradient Estimator (MIGE) for representation learning based on the
score estimation of implicit distributions. MIGE exhibits a tight and smooth gra-
dient estimation of MI in the high-dimensional and large-MI settings. We expand
the applications of MIGE in both unsupervised learning of deep representations
based on InfoMax and the Information Bottleneck method. Experimental results
have indicated significant performance improvement in learning useful represen-
tation.
1	Introduction
Mutual information (MI) is an appealing metric widely used in information theory and machine
learning to quantify the amount of shared information between a pair of random variables. Specifi-
cally, given a pair of random variables x, y, the MI, denoted by I(x; y), is defined as
I(x; y) = Ep(x,y)
log
p(x, y)
p(x)p(y)
(1)
where E is the expectation over the given distribution. Since MI is invariant to invertible and smooth
transformations, it can capture non-linear statistical dependencies between variables (Kinney & At-
wal, 2014). These appealing properties make it act as a fundamental measure of true dependence.
Therefore, MI has found applications in a wide range of machine learning tasks, including feature
selection (Kwak & Choi, 2002; Fleuret, 2004; Peng et al., 2005), clustering (Muller et al., 2012;
Ver Steeg & Galstyan, 2015), and causality (Butte & Kohane, 1999). It has also been pervasively
used in science, such as biomedical sciences (Maes et al., 1997), computational biology (Krish-
naswamy et al., 2014), and computational neuroscience (Palmer et al., 2015).
Recently, there has been a revival of methods in unsupervised representation learning based on MI.
A seminal work is the InfoMax principle (Linsker, 1988), where given an input instance x, the goal
of the InfoMax principle is to learn a representation Eψ (x) by maximizing the MI between the
input and its representation. A growing set of recent works have demonstrated promising empirical
performance in unsupervised representation learning via MI maximization (Krause et al., 2010; Hu
1
Published as a conference paper at ICLR 2020
et al., 2017; Alemi et al., 2018b; Oord et al., 2018; Hjelm et al., 2019). Another closely related work
is the Information Bottleneck method (Tishby et al., 2000; Alemi et al., 2017), where MI is used to
limit the contents of representations. Specifically, the representations are learned by extracting task-
related information from the original data while being constrained to discard parts that are irrelevant
to the task. Several recent works have also suggested that by controlling the amount of information
between learned representations and the original data, one can tune desired characteristics of trained
models such as generalization error (Tishby & Zaslavsky, 2015; Vera et al., 2018), robustness (Alemi
et al., 2017), and detection of out-of-distribution data (Alemi et al., 2018a).
Despite playing a pivotal role across a variety of domains, MI is notoriously intractable. Exact
computation is only tractable for discrete variables, or for a limited family of problems where the
probability distributions are known. For more general problems, MI is challenging to analytically
compute or estimate from samples. A variety of MI estimators have been developed over the years,
including likelihood-ratio estimators (Suzuki et al., 2008), binning (Fraser & Swinney, 1986; Dar-
bellay & Vajda, 1999; Shwartz-Ziv & Tishby, 2017), k-nearest neighbors (Kozachenko & Leonenko,
1987; Kraskov et al., 2004; Perez-Cruz, 2008; Singh & Poczos, 2016), and kernel density estima-
tors (Moon et al., 1995; Kwak & Choi, 2002; Kandasamy et al., 2015). However, few of these
mutual information estimators scale well with dimension and sample size in machine learning prob-
lems (Gao et al., 2015).
In order to overcome the intractability of MI in the continuous and high-dimensional settings, Alemi
et al. (2017) combines variational bounds of Barber & Agakov (2003) with neural networks for
the estimation. However, the tractable density for the approximate distribution is required due to
variational approximation. This limits its application to the general-purpose estimation, since the
underlying distributions are often unknown. Alternatively, the Mutual Information Neural Estima-
tion (MINE, Belghazi et al. (2018)) and the Jensen-Shannon MI estimator (JSD, Hjelm et al. (2019))
enable differentiable and tractable estimation of MI by training a discriminator to distinguish sam-
ples coming from the joint distribution or the product of the marginals. In detail, MINE employs
a lower-bound to the MI based on the Donsker-Varadhan representation of the KL-divergence, and
JSD follows the formulation of f-GAN KL-divergence. In general, these estimators are often noisy
and can lead to unstable training due to their dependence on the discriminator used to estimate the
bounds of mutual information. As pointed out by Poole et al. (2019), these unnormalized critic
estimators of MI exhibit high variance and are challenging to tune for estimation. An alternative
low-variance choice of MI estimator is Information Noise-Contrastive Estimation (InfoNCE, Oord
et al. (2018)), which introduces the Noise-Contrastive Estimation with flexible critics parameterized
by neural networks as a bound to approximate MI. Nonetheless, its estimation saturates at log of
the batch size and suffers from high bias. Despite their modeling power, none of the estimators
are capable of providing accurate estimation of MI with low variance when the MI is large and the
batch size is small (Poole et al., 2019). As supported by the theoretical findings in McAllester &
Statos (2018), any distribution-free high-confidence lower bound on entropy requires a sample size
exponential in the size of the bound. More discussions about the bounds of MI and their relationship
can be referred to Poole et al. (2019).
In summary, existing estimators first approximate MI and then use these approximations to optimize
the associated parameters. For estimating MI based on any finite number of samples, there exists
an infinite number of functions, with arbitrarily diverse gradients, that can perfectly approximate
the true MI at these samples. However, these approximate functions can lead to unstable training
and poor performance in optimization due to gradients discrepancy between approximate estimation
and true MI. Estimating gradients of MI rather than estimating MI may be a better approach for MI
optimization. To this end, to the best of our knowledge, we firstly propose the Mutual Information
Gradient Estimator (MIGE) in representation learning. In detail, we estimate the score function
of an implicit distribution, Vχ log q(x), to achieve a general-purpose MI gradient estimation for
representation learning. In particular, to deal with high-dimensional inputs, such as text, images and
videos, score function estimation via Spectral Stein Gradient Estimator (SSGE) (Shi et al., 2018)
is computationally expensive and complex. We thus propose an efficient high-dimensional score
function estimator to make SSGE scalable. To this end, we derive a new reparameterization trick
for the representation distribution based on the lower-variance reparameterization trick proposed
by Roeder et al. (2017).
We summarize the contributions of this paper as follows:
2
Published as a conference paper at ICLR 2020
d = 5	d = 10	d = 20
PPP
Figure 1: Estimation performance of MINE, MINE-f and MIGE. Each estimation approach has
been taken additional 20 times and plotted with light curves. Top: True MI and corresponding
estimation of MINE and MINE-f . Bottom: True gradient and corresponding estimation of MINE,
MINE-f and MIGE. Our approach MIGE only appears in bottom figures since it directly gives
gradient estimation. As we observe, MIGE gives more stable, smooth and accurate results.
•	We propose the Mutual Information Gradient Estimator (MIGE) for representation learning
based on the score function estimation of implicit distributions. Compared with MINE
and MINE-f, MIGE provides a tighter and smoother gradient estimation of MI in a high-
dimensional and large-MI setting, as shown in Figure 1 of Section 4.
•	We propose the Scalable SSGE to alleviate the exorbitant computational cost of SSGE in
high-dimensional settings.
•	To learn meaningful representations, we apply SSGE as gradient estimators for both In-
foMax and Information Bottlenck, and have achieved improved performance than their
corresponding competitors.
2	S calable S pectral S tein Gradient Estimator
Score estimation of implicit distributions has been widely explored in the past few years (Song et al.,
2019; Li & Turner, 2017; Shi et al., 2018). A promising method of score estimation is the Stein
gradient estimator (Li & Turner, 2017; Shi et al., 2018), which is proposed for implicit distributions.
It is inspired by generalized Steins identity (Gorham & Mackey, 2015; Liu & Wang, 2016) as
follows.
Steins identity. Let q(x) be a continuously differentiable (also called smooth) density supported
on X ⊆ Rd, and h(x) = [h1 (x), h2(x), . . . , hd0 (x)]T is a smooth vector function. Further, the
boundary conditions on h is
q(x)h(x) = 0, ∀x ∈ ∂X if X is compact, or lim q(x)h(x) = 0 if X = Rd.	(2)
x→∞
Under this condition, the following identity can be easily checked using integration by parts, assum-
ing mild zero boundary conditions on h,
Eq [h(x)Vχ log q(x)T + Vχh(x)] = 0.	(3)
3
Published as a conference paper at ICLR 2020
Here h is called the Stein class of q(x) if Steins identity Eq. (3) holds. Monte Carlo estimation of
the expectation in Eq. (3) builds the connection between Vχ log q(x) and the samples from q(x) in
Steins identity. For modeling implicit distributions, Motivated by Steins identity, Shi et al. (2018)
proposed Spectral Stein Gradient Estimator (SSGE) for implicit distributions based on Stein’s iden-
tity and a spectral decomposition of kernel operators where the eigenfunctions are approximated by
the Nystrom method. Below We briefly review SSGE. More details refer to Shi et al. (2018). SPecif-
ically, we denote the target gradient function to estimate by g : X → Rd : g(x) = Vx log q(x).
The ith component of the gradient is gi(x) = Vxi log q(x). We assume g1, . . . , gd ∈ L2(X, q).
{ψj}j≥1 denotes an orthonormal basis of L2(X, q). We can expand gi(x) into the spectral series,
i.e., gi(x) = Pj∞=1 βij ψj (x). The value of the jth eigenfunction ψj at x can be approximated by
the Nystrom method (Xu et al., 2015). Due to the orthonormality of eigenfunctions {ψj }j≥ι, thereis
a constraint under the probability measure q(.): / ψi(x)ψj(x)q(x)dx = δj, where δj = l[i = j].
Based on this constraint, we can obtain the following equation for {ψj}j≥1:
/ k(χ, y)ψ(y)q(y)dy = μψ(χ),
(4)
where k(.) is a kernel function. The left side of the above equation can be approximated by the
Monte Carlo estimate using i.i.d. samples x1,..., xM from q(.) : MKψ ≈ μψ, where K is the
Gram Matrix and ψ = ψ χ1 , . . . , ψ χM > . We can solve this eigenvalue problem by choose
the J largest eigenvalues λι ≥ ∙∙∙ ≥ λj for K. Uj denotes the eigenvector of the Gram matrix. The
approximation for {ψj}j≥1 can be obtained combined with Eq. (4) as following: ψj(x) ≈ ψj (x) =
√M PM=I Ujmk (x, xm).
Furthermore, based on the orthonormality of {ψj}j≥1, we can easily obtain βij = -Eq Vxi ψj (x).
By taking derivative both sides of Eq. (4), we can show that:
μj Vxi ψj(X) = Vxi
k(x, y)ψj (y)q(y)dy =
Vxi k(x, y)ψj(y)q(y)dy.
(5)
Then we can estimate as following:
Vxi ψj (x) ≈
1
μj M
M
XVxik(x,xm )ψj(xm ).
m=1
(6)
Finally, by truncating the expansion to the first J terms and plugging in the Nystrom approximations
of {ψj}j≥1, we can get the score estimator:
JM
X1
Biψj(x),	∣βij = - M E Vxiψj(x ).	⑺
j=1	m=1
In general, representation learning for large-scale datasets is usually costly in terms of storage and
computation. For instance, the dimension of images in the STL-10 dataset is 96 × 96 × 3 (i.e., the
vector length is 27648). This makes it almost impossible to directly estimate the gradient of MI
between the input and representation. To alleviate this problem, we introduce random projection
(RP) (Bingham & Mannila, 2001) to reduce the dimension of x.
We briefly review RP. More details refer to Bingham & Mannila (2001). RP projects the original
d-dimensional data into a k-dimensional (k << d) subspace. Concretely, let matrix Xd×N denotes
the original set ofN d-dimensional data, the projection of the original data XkR×PN is obtained by in-
troducing a random matrix Rk×d whose columns have unit length, as follows (Bingham & Mannila,
2001), XkR×PN = Rk×dXd×N . After RP, the Euclidean distance between two original data vectors
can be approximated by the Euclidean distance of the projective vectors in reduced spaces:
∣∣xι - x2k ≈ ppd/k ∣∣Rxι - Rx2k ,	(8)
where x1 and x2 denote the two data vectors in the original large dimensional space.
4
Published as a conference paper at ICLR 2020
Based on the principle of RP, we can derive a Salable Spectral Stein Gradient Estimator, which is
an efficient high-dimensional score function estimator. One can show that the RBF kernel satisfies
Steins identity (Liu & Wang, 2016). Shi et al. (2018) also shows that it is a promising choice for
SSGE with a lower error bound. To reduce the computation of the kernel similarities of SSGE in
high-dimensional settings, we replace the input of SSGE with a projections obtained by RP accord-
ing to the approximation of Eq. (8) for the computation of the RBF kernel.
3	Mutual Information Gradient Estimator
As gradient estimation is a straightforward and effective method in optimization, we propose a
gradient estimator for MI based on score estimation of implicit distributions, which is called Mutual
Information Gradient estimator (MIGE). In this section, we focus on three most general cases of MI
gradient estimation for representation learning, and derive the corresponding MI gradient estimator
for these circumstances.
We outline the general setting of training an encoder to learn a representation. Let X and Z be
the domain, and Eψ : X → Z with parameters ψ denotes a continuous and (almost everywhere)
differentiable parametric function, which is usually a neural network, namely an encoder. p(x)
denotes the empirical distribution given the input data x ∈ X. We can obtain the representation
of the input data through the encoder, z = Eψ (x). qψ (z) is defined as the marginal distribution
induced by pushing samples from p(x) through encoder Eψ(.) We also define qψ(x, z) as the joint
distribution with x and z, which is determined by encoder Eψ (.).
Circumstance I. Given that the encoder Eψ(.) is deterministic, our goal is to estimate the gradient
of MI between input x and encoder output z w.r.t. the encoder parameters ψ. There is a close
relationship between mutual information and entropy, which is as following:Iψ (x; Z) = H(x) +
Hψ (z) - Hψ(x, z). Here H(x) is data entropy and not relevant to ψ. The optimization of Iψ(x, z)
with parameters ψ can neglect the entry H (x). We decompose the gradient of the entropy of qψ(z)
and qψ (x, z) as (see Appendix A):
Vψ H (Z) = -Vψ Eqψ (z)[log q(z)],	Vψ H (x,z) = -Vψ Eqψ (χ,z)[log q(x,z)].	(9)
Hence, we can represent the gradient of MI between input x and encoder output z w.r.t. encoder
parameters ψ as following:
VψIψ(x; z) = -VψEqψ(z)[logq(z)] + VψEqψ(χ,z)[logq(x,z)].	(10)
However, this equation is intractable since an expectation w.r.t qψ (z) is directly not differentiable
w.r.t ψ. Roeder et al. (2017) proposed a general variant of the standard reparameterization trick for
the variational evidence lower bound, which demonstrates lower-variance. To address above prob-
lem, we adapt this trick for MI gradient estimator in representation learning. Specifically, we can
obtain the samples from the marginal distribution of z by pushing samples from the data empirical
distribution p(x) through Eψ (.) for representation learning. Hence we can reparameterize the repre-
sentations variable Z 〜qψ (z) using a differentiable transformation:z = Eψ (x) with X 〜 P(X),
where the data empirical distribution p(x) is independent of encoder parameters ψ. This reparame-
terization can rewrite an expectation w.r.t qψ (z) and qψ(x, z) such that the Monte Carlo estimate of
the expectation is differentiable w.r.t ψ.
Relying on this reparameterization trick, we can represent the gradient ofMI w.r.t. encoder param-
eters ψ in Eq. 10 as follows:
VψIψ(x; z) = -Eq(χ)[Vz log q(Eψ(x))VψEψ(x)]
+ Eq(x) [V(x,z) log q(x, Eψ (x))Vψ (x, Eψ (x))],	(11)
where the score function Vz log qψ(Eψ (x)) can be estimated based on i.i.d. samples from an im-
plicit density qψ(Eψ(x)) (Shi et al., 2018; Song et al., 2019). The samples form the joint dis-
tribution qψ (x, z) are produced as following: we sample observations from empirical distribution
p(x); then the corresponding samples of z is obtained through Eψ (.). Hence we can also estimate
V(x,z) log q(x, Eψ(x)) based on i.i.d. samples from qψ(x, Eψ(x)). Vψ Eψ (x) and Vψ(x, Eψ(x))
are directly computed with x.
5
Published as a conference paper at ICLR 2020
Circumstance II. Assume that we encode the input to latent data space h = Cψ (x) that reflects
useful structure in the data. Next, we summarize this latent variable mapping into final representa-
tions by the function fψ, z = Eψ(x) = fψ ◦ Cψ(x). The gradient estimator of MI between h and z
is represented by the data reparameterization trick as follows:
VψIψ (h; Z) = VψHψ (h) + VψHψ (Z)- VψHψ (h,z)	(12)
=-Eq(X)[Vz logq(Eψ(x))VψEψ(x)] - Eq(χ)[Vhlogq(Cψ(x))VψCψ(x)]
+ Eq(x)[V(h,z) logq(Cψ(x), Eψ (x))Vψ (Cψ x, Eψ (x))].	(13)
Circumstance III. Consider stochastic encoder function Eψ (., ) where is an auxiliary variable
with independent marginal p(). By utilizing data reparameterization trick. we can represent the
gradient of the conditional entropy Hψ (z|x) as follows (see Appendix A):
VψHψ(ZIX) = -Ep(X) [Ep(e) [V(z∣χ) log q(Eψ (x, e) ∣x)VψEψ(x, €)]],	(14)
where the term V(z|x) log q(Eψ(x, )|x) can be easily estimated by score estimation.
Based on the condition entropy gradient estimation in Eq. (14), the gradient estimator ofMI between
input and encoder output can be represented as following:
VψIψ(x; Z) = VψHψ(Z) - VψHψ(ZIx)	(15)
= -Ep(X)p() [Vz [log p(Eψ (x, ))]VψEψ(x, )]
+ Ep(X)[Ep()[V(z|X) logq(Eψ(x,)Ix)VψEψ(x, )]].	(16)
In practical MI optimization, we can construct MIGE of the full dataset based on mini-batch Monte
Carlo estimates. We have provided an algorithm description for MIGE in Appendix B.
4	Toy Experiment
Recently, MINE and MINE-f enable effective computation of MI in the continuous and high-
dimensional settings. To compare with MINE and MINE-f , we evaluate MIGE in the correlated
Gaussian problem taken from (Belghazi et al., 2018).
Experimental Settings. We consider two random variables x and y (x, y ∈ Rd), coming from
a 2d-dimension multivariate Gaussian distribution. The component-wise correlation of x and y
is defined as follows: corr(xi, yi) = δijρ, ρ ∈ (-1, 1), where δij is Kronecker’s delta and ρ
is the correlation coefficient. Since MI is invariant to smooth transformations of x, y, we only
consider standardized Gaussian for marginal distribution p(x) and p(y). The gradient of MI w.r.t
P has the analytical solution: VPI(x; y) = 1PPς . We apply MINE and MINE-f to estimate MI of
x, y by sampling from the correlated Gaussian distribution and its marginal distributions, and the
corresponding gradient ofMI w.r.t ρ can be computed by backpropagation implemented in Pytorch.
Results. Fig.1 presents our experimental results in different dimensions d = {5, 10, 20}. In the case
of low-dimensional (d = 5), all the estimators give promising estimation of MI and its gradient.
However, the MI estimation of MINE and MINE-f are unstable due to its relying on a discriminator
to produce estimation of the bound on MI. Hence, as showed in Fig.1, corresponding estimation of
MI and its gradient is not smooth. As the dimension d and the absolute value of correlation coeffi-
cient IρI increase, MINE and MINE-f are apparently hard to reach the True MI, and their gradient
estimation of MI is thus high biased. This phenomenon would be more significant in the case of
high-dimensional or large MI. Contrastively, MIGE demonstrates the significant improvement over
MINE and MINE-f when estimating MI gradient between twenty-dimensional random variables
x, y. In this experiment, we compare our method with two baselines on an analyzable problem
and find that the gradient curve estimated by our method is far superior to other methods in terms
of smoothness and tightness in a high-dimensional and large-MI setting compared with MINE and
MINE-f.
5	Applications
To demonstrate the performance in downstream tasks, we deploy MIGE to Deep InfoMax (Hjelm
et al., 2019) and Information Bottleneck (Tishby et al., 2000) respectively, namely replacing the
original MI estimators with MIGE. We find that MIGE achieves higher and more stable classification
accuracy, which indicating its good gradient estimation performance in practical applications.
6
Published as a conference paper at ICLR 2020
Table 1: CIFAR-10 and CIFAR-100 classification accuracy (top 1) of downstream tasks compared
with vanilla DIM. JSD and infoNCE are MI estimators, and PM denotes matching representations
to a prior distribution (Hjelm et al., 2019).
Model	conv	CIFAR-10 fc(1024)	Y(64)	conv	CIFAR-100 fc(1024)	Y(64)
DIM (JSD)	55.81%	45.73%	40.67%	28.41%	22.16%	16.50%
DIM (JSD + PM)	52.2%	52.84%	43.17%	24.40%	18.22%	15.22%
DIM (infoNCE)	51.82%	42.81%	37.79%	24.60%	16.54%	12.96%
DIM (infoNCE + PM)	56.77%	49.42%	42.68%	25.51%	20.15%	15.35%
MIGE	57.95%	57.09%	53.75%	29.86%	27.91%	25.84%
Table 2: STL-10 classification accuracy (top 1) of down- ›46-
Stream tasks compared with vanilla DIM. The dimension of ⅛ 45'
STL-10 images (27648) results in exorbitant computational M :
cost. Random Projection (RP) is applied to reduce the di- ʧ 42.
mension.	异 4i-
Model	conv	STL-10 fc(1024)	Y(64)	16	32	64 128 256 512 1024 RP dimension k
DIM (JSD) DIM (infoNCE)	42.03% 43.13%	30.28% 35.80%	28.09% 34.44%	Figure 2:	STL-10 Y(64) classification
MIGE MIGE + RP to 512d	unaffordable computational cost 52.00%	48.14%	44.89%			accuracy (top 1) with different RP di- mension.	
5.1	Deep InfoMax
Discovering useful representations from unlabeled data is one core problem for deep learning. Re-
cently, a growing set of methods is explored to train deep neural network encoders by maximizing
the mutual information between its input and output. A number of methods based on tractable vari-
ational lower bounds, such as JSD and infoNCE, have been proposed to improve the estimation of
MI between high dimensional input/output pairs of deep neural networks (Hjelm et al., 2019). To
compare with JSD and infoNCE, we expand the application of MIGE in unsupervised learning of
deep representations based on the InfoMax principle.
Experimental Settings. For consistent comparison, we follow the experiments of Deep Info-
Max(DIM)1 to set the experimental setup as in Hjelm et al. (2019). We test DIM on image datasets
CIFAR-10, CIFAR-100 and STL-10 to evaluate our MIGE. For the high-dimensional images in
STL-10, directly applying SSGE is almost impossible since it results in exorbitant computational
cost. Our proposed Scalable SSGE is applied, to reduce the dimension of images and achieve rea-
sonable computational cost. As mentioned in Hjelm et al. (2019), non-linear classifier is chosen to
evaluate our representation, After learning representation, we freeze the parameters of the encoder
and train a non-linear classifier using the representation as the input. The same classifiers are used
for all methods. Our baseline results are directly copied from Hjelm et al. (2019) or by running the
code of author.
Results. As shown in Table 1, MIGE outperforms all the competitive models in DIM experiments
on CIFAR-10 and CIFAR-100. Besides the numerical improvements, it is notable that our model
have the less accuracy decrease across layers than that of DIM(JSD) and DIM(infoNCE). The results
indicate that, compared to variational lower bound methods, MIGE gives more favorable gradient
direction, and demonstrates more power in controlling information flows without significant loss.
With the aid of Random Projection, we could evaluate on bigger datasets, e.g., STL-10. Table 2
shows the result of DIM experiments on STL-10. We can observe significant improvement over the
baselines when RP to 512d. Note that our proposed gradient estimator can also be extended to the
multi-view setting(i.e., with local and global features) of DIM, it is beyond the scope of this paper.
More discussions refer to Appendix C.
1Codes available at https://github.com/rdevon/DIM
7
Published as a conference paper at ICLR 2020
Ablation Study. To verify the effect of different dimensions of Random Projection on classification
accuracy in DIM experiments, we conduct an ablation study on STL-10 with the above experimental
settings. Varying RP dimension k ∈ {16, 32, 64, 128, 256, 512, 1024}, we measure the classification
accuracy of Y(64) which is shown in Fig.2. We find that the classification accuracy increases with
RP dimension from 16 to 128. After that, the approximation in Equ.(8) with the further increase of
the RP dimension reaches saturation, while bringing extra computational costs.
5.2	Information Bottleneck
Information Bottleneck (IB) has been widely applied to a variety of application domains, such as
classification (Tishby & Zaslavsky, 2015; Alemi et al., 2017; Chalk et al., 2016; Kolchinsky et al.,
2017), clustering (Slonim & Tishby, 2000), and coding theory and quantization (Zeitler et al., 2008;
Courtade & Wesel, 2011). In particular, given the input variable x and the target variable y, the
goal of the IB is to learn a representation of x (denoted by the variable z) that satisfies the following
characteristics:
1)	z is sufficient for the target y, that is, all information about target y contained in x should
also be contained in z. In optimization, it should be
2)	z is minimal. In order not to contain irrelevant information that is not related to y, z is
required to contain the smallest information among all sufficient representations.
The objective function of IB is written as follows:
max I(z; y), s.t. I(z; x) ≤ c.
(17)
Equivalently, by introducing a Lagrangian multiplier β , the IB method can maximize the following
objective function: GIB = I(z; y) - βI(z; x). Further, it is generally acknowledged that I(z; y) =
H(y) - H (y|z), and H(y) is constant. Hence we can also minimize the objective function of the
following form:
LIB = H (y|z) + βI(z; x),
(18)
where β ≥ 0 plays a role in trading off the sufficiency and minimality. Note that the above formulas
omit the parameters for simplicity.
To overcome the intractability of MI in the continuous and high-dimension setting, Alemi et al.
(2017) presents a variational approximation to IB, which adopts deep neural network encoder to
produce a conditional multivariate normal distribution, called Deep Variational Bottleneck (DVB).
Rencently, DVB is exploited to restrict the capacity of discriminators in GANs (Peng et al., 2019).
However, a tractable density is required for the approximate posterior in DVB due to their reliance
on a variational approximation while MIGE does not.
To evaluate our method, we compare MIGE-IB with DVB and MINE-IB in IB application. We
demonstrate an implementation of the IB objective on permutation invariant MNIST using MIGE.
Experiments. For consistent comparison, we adopt the same architecture and empirical settings
used in Alemi et al. (2017) except that the initial learning rate of 2e-4 is set for Adam optimizer,
and exponential decay with decaying rate by a factor of 0.96 was set for every 2 epochs. The
implementation of DVB is available from its authors2. Under these experimental settings, we use
our MI Gradient Estimator to replace the MI estimator in DVB experiment. The threshold of score
function’s Stein gradient estimator is set as 0.94. The threshold is the hyper-parameter of Spectral
Stein Gradient Estimator (SSGE), and it is used to set the kernel bandwidth of RBF kernel. Our
results can be seen in Table 3 and it manifests that our proposed MIGE-IB outperforms DVB and
MINE-IB.
2https://github.com/alexalemi/vib_demo
8
Published as a conference paper at ICLR 2020
Table 3: Permutation-invariant MNIST misclassification rate. Datas except our model are cited from
Belghazi et al. (2018)
Model	Misclass rate
Baseline	1.38%
Dropout	1.34%
Confidence penalty	1.36%
Label Smoothing	1.4%
DVB	1.13%
MINE-IB	1.11%
MIGE-IB (ours)	1.05%
6	Conclusion
In this paper, we present a gradient estimator, called Mutual Information Gradient Estimator
(MIGE), to avoid the various problems met in direct mutual information estimation. We manifest the
effectiveness of gradient estimation of MI over direct MI estimation by applying it in unsupervised
or supervised representation learning. Experimental results have indicated the remarkable improve-
ment over MI estimation in the Deep InfoMax method and the Information Bottleneck method.
Accknowledgement
This work was partially funded by the National Key R&D Program of China (No. 2018YFB1005100
& No. 2018YFB1005104).
9
Published as a conference paper at ICLR 2020
References
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. In ICLR, 2017.
Alexander A Alemi, Ian Fischer, and Joshua V Dillon. Uncertainty in the variational information
bottleneck. arXiv preprint arXiv:1807.00906, 2018a.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy.
Fixing a broken ELBO. In ICML, 2018b.
David Barber and Felix V Agakov. The im algorithm: a variational approach to information maxi-
mization. In Advances in neural information processing systems, pp. None, 2003.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: mutual information neural estimation. ICML, 2018.
Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to
image and text data. In Proceedings of the seventh ACM SIGKDD international conference on
Knowledge discovery and data mining, San Francisco, CA, USA, pp. 245-250, 2001.
Atul J Butte and Isaac S Kohane. Mutual information relevance networks: functional genomic
clustering using pairwise entropy measurements. In Biocomputing 2000, pp. 418-429. World
Scientific, 1999.
Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational informa-
tion bottleneck. In Advances in Neural Information Processing Systems, pp. 1957-1965, 2016.
Thomas A Courtade and Richard D Wesel. Multiterminal source coding with an entropy-based
distortion measure. In 2011 IEEE International Symposium on Information Theory Proceedings,
pp. 2040-2044. IEEE, 2011.
Georges A Darbellay and Igor Vajda. Estimation of the information by an adaptive partitioning of
the observation space. IEEE Transactions on Information Theory, 45(4):1315-1321, 1999.
Francois Fleuret. Fast binary feature selection with conditional mutual information. Journal of
Machine learning research, 5(Nov):1531-1555, 2004.
Andrew M Fraser and Harry L Swinney. Independent coordinates for strange attractors from mutual
information. Physical review A, 33(2):1134, 1986.
Shuyang Gao, Greg Ver Steeg, and Aram Galstyan. Efficient estimation of mutual information for
strongly dependent variables. In Artificial intelligence and statistics, pp. 277-286, 2015.
Jackson Gorham and Lester Mackey. Measuring sample quality with stein’s method. In Advances
in Neural Information Processing Systems, pp. 226-234, 2015.
R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In ICLR, 2019.
Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning
discrete representations via information maximizing self-augmented training. In ICML, 2017.
Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, et al. Non-
parametric von mises estimators for entropies, divergences and mutual informations. In Advances
in Neural Information Processing Systems, pp. 397-405, 2015.
Justin B Kinney and Gurinder S Atwal. Equitability, mutual information, and the maximal informa-
tion coefficient. Proceedings of the National Academy of Sciences, 111(9):3354-3359, 2014.
Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck.
arXiv preprint arXiv:1705.02436, 2017.
10
Published as a conference paper at ICLR 2020
LF Kozachenko and Nikolai N Leonenko. Sample estimate of the entropy of a random vector.
Problemy Peredachi Informatsii, 23(2):9-16,1987.
Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. Phys-
ical review E, 69(6):066138, 2004.
Andreas Krause, Pietro Perona, and Ryan G Gomes. Discriminative clustering by regularized infor-
mation maximization. In Advances in neural information processing systems, 2010.
Smita Krishnaswamy, Matthew H Spitzer, Michael Mingueneau, Sean C Bendall, Oren Litvin, Erica
Stone, Dana Peer, and Garry P Nolan. Conditional density-based analysis of t cell signaling in
single-cell data. Science, 346(6213):1250689, 2014.
Nojun Kwak and Chong-Ho Choi. Input feature selection by mutual information based on parzen
window. IEEE Transactions on Pattern Analysis & Machine Intelligence, (12):1667-1671, 2002.
Yingzhen Li and Richard E Turner. Gradient estimators for implicit models. arXiv preprint
arXiv:1705.07107, 2017.
Ralph Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances in neural information processing systems, pp. 2378-2386, 2016.
Frederik Maes, Andre Collignon, Dirk Vandermeulen, Guy Marchal, and Paul Suetens. Multimodal-
ity image registration by maximization of mutual information. IEEE transactions on Medical
Imaging, 16(2):187-198, 1997.
David McAllester and Karl Statos. Formal limitations on the measurement of mutual information.
arXiv preprint arXiv:1811.04251, 2018.
Young-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using
kernel density estimators. Physical Review E, 52(3):2318, 1995.
Andreas C Muller, Sebastian Nowozin, and ChriStoPh H Lampert. Information theoretic clustering
using minimum spanning trees. In Joint DAGM (German Association for Pattern Recognition)
and OAGM Symposium, pp. 205-215. Springer, 2012.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. NIPS, 2018.
Stephanie E Palmer, Olivier Marre, Michael J Berry, and William Bialek. Predictive information
in a sensory population. Proceedings of the National Academy of Sciences, 112(22):6908-6913,
2015.
Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information:
criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern
Analysis & Machine Intelligence, (8):1226-1238, 2005.
Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational dis-
criminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining infor-
mation flow. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019, 2019.
Fernando Perez-Cruz. Kullback-leibler divergence estimation of continuous distributions. In 2008
IEEE international symposium on information theory, pp. 1666-1670. IEEE, 2008.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 5171-5180, 2019.
Geoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-variance
gradient estimators for variational inference. In Advances in Neural Information Processing Sys-
tems, pp. 6925-6934, 2017.
11
Published as a conference paper at ICLR 2020
Jiaxin Shi, Shengyang Sun, and Jun Zhu. A spectral approach to gradient estimation for implicit
distributions. arXiv preprint arXiv:1806.02925, 2018.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Shashank Singh and Barnabas Poczos. Finite-sample analysis of fixed-k nearest neighbor density
functional estimators. In Advances in neural information processing systems, pp. 1217-1225,
2016.
Noam Slonim and Naftali Tishby. Document clustering using word clusters via the information
bottleneck method. In Proceedings of the 23rd annual international ACM SIGIR conference on
Research and development in information retrieval, pp. 208-215. ACM, 2000.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. arXiv preprint arXiv:1905.07088, 2019.
Taiji Suzuki, Masashi Sugiyama, Jun Sese, and Takafumi Kanamori. Approximating mutual infor-
mation by maximum likelihood density ratio estimation. In New challenges for feature selection
in data mining and knowledge discovery, pp. 5-20, 2008.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
2015 IEEE Information Theory Workshop (ITW), pp. 1-5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On
mutual information maximization for representation learning. ArXiv, abs/1907.13625, 2019.
Greg Ver Steeg and Aram Galstyan. Maximally informative hierarchical representations of high-
dimensional data. In Artificial Intelligence and Statistics, pp. 1004-1012, 2015.
Matias Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of the information bottleneck
in representation learning. In IEEE International Symposium on Information Theory (ISIT), pp.
1580-1584. IEEE, 2018.
Zenglin Xu, Rong Jin, Bin Shen, and Shenghuo Zhu. Nystrom approximation for sparse kernel meth-
ods: Theoretical analysis and empirical evaluation. In Blai Bonet and Sven Koenig (eds.), Pro-
ceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015,
Austin, Texas, USA, pp. 3115-3121. AAAI Press, 2015. URL http://www.aaai.org/ocs/
index.php/AAAI/AAAI15/paper/view/9860.
Georg Zeitler, Ralf Koetter, Gerhard Bauch, and Joerg Widmer. Design of network coding functions
in multihop relay networks. In 2008 5th International Symposium on Turbo Codes and Related
Topics, pp. 249-254. IEEE, 2008.
12
Published as a conference paper at ICLR 2020
A Derivation of Gradient Estimates for Entropy
Unconditional Entropy Given that the encoder Eψ(.) is deterministic, our goal is to optimize the
entropy H(q) = -Eq log q, where q is short for the distribution qψ (z) of the representation z w.r.t.
its parameters ψ. We can decompose the gradient of the entropy of qψ (z) as:
Vψ H (Z) = -VψEqψ (z)[log q(z)] - Eq(z)[Vψ log qψ (z)],	(19)
The second term on the right side of the equation can be calculated:
Eq(z) [Vψ log qψ (z)] = Eq(z) [Vψ qψ (z) ×
Vψqψ(z)dz = Vψ
qψ(z)dz
0.
(20)
Therefore, the gradient of the entropy of qψ (z) becomes
VψH(z) = -VψEqψ(z) [log q(z)].
(21)
Conditional Entropy Consider nondeterministic encoder function Eψ (., ) where is an auxil-
iary variable with independent marginal p(e). The distribution qψ (z|x) is determined by E and the
encoder parameters ψ. The auxiliary variable introduces randomness to the encoder. First, we
decompose the gradients of Conditional Entropy as following:
Vψ H (z|x)
-Vψ
∕pψ (z, x) logpψ (z∣x)dzdx
-Ep(x) [Vψ
/ pψ (z|x) log pψ (z| x)dz]
-Ep(X) [Vψ Epψ (z∣χ)[log p(z∣x)] +
/p(z∣x)Vψ logpψ(z∣x)dh]
-Ep(x) [Vψ Epψ (z|x) [log p(z|x)] +
-Ep(x) [Vψ Epψ (z|x) [log p(z|x)] -
-Ep(X) [Vψ Epψ (z∣χ)[log ρ(z∣x)]].
/ Vψ Pψ (z∣x)dh]
Vψ pψ (h, x)dhdx]
(22)
Note that z = Eψ(x, E), such that we can apply reparameterization trick to the gradient estimator of
conditional entropy in Eq. (22),
Hψ(z|x) = -Ep(χ) [Ep(e)[V(z∣χ) logq(Eψ(x, E)IX)VψEψ(x, e)]].	(23)
B MIGE Algorithm Description
The algorithm description of our proposed MIGE is stated in Algorithm 1.
Algorithm 1 MIGE (Circumstance I)
1. Sampling:
Draw n samples from the data distribution p(x), n denotes mini-batch size,
then compute the corresponding output of the encoder
(x(1) 2 3 4, Z(I)),…,(x(n), z(n))〜qψ(x, z)
Z(I),…，z(n)〜qψ (z)
2. Estimate the score function:
Vz log qψ (z(i)) - SSGE(Z(I), ∙∙∙ ,z(n))
V(χ,z) log qψ(x(i), z(i)) - SSGE((X⑴,z(1)),…，(x(n), z(n)))
3. Estimate the entropy gradient:
VψH(z)《——1 pi=ι [Vψz(i)Vzlogqψ(z(i))]
VψH(x, z){——§ pi=ι [Vψ(x(i), z(i))V(χ,z) logqψ(x(i), z(i))]
4. Estimate the MI gradient:
VψI(x; z) - VψH(z) - Vψ H(x; z)
13
Published as a conference paper at ICLR 2020
C Discussion on DIM(L)
DIM(L) (Hjelm et al., 2019) is the state-of-the-art unsupervised model for representaion learning,
which maximizes the average MI between the high-level representation and local patches of the
image, and achieve an even higher classification accuracy than supervised learning. As shown in
Table 4, we apply MIGE into DIM(L) and surprisingly find there is a significant performance gap to
DIM(L).
To our knowledge, the principle of DIM(L) is still unclear. Tschannen et al. (2019) argues that
maximizing tighter bounds in DIM(L) can lead to worse results, and the success of these methods
cannot be attributed to the properties of MI alone, and they strongly depend on the inductive bias
in both the choice of feature extractor architectures and the parameterization of the employed MI
estimators. For MIGE, we are investigating the behind reasons, e.g., to investigate the distributions
of the patches.
Table 4: CIFAR-10 and CIFAR-100 classification accuracy (top 1) of downstream tasks compared
with vanilla DIM(L).
Model	Conv	CIFAR-10 fc(1024)	Y(64)	conv	CIFAR-100 fc(1024)	Y(64)
DIM(L)(JSD)	72.16%	67.99%	66.35%	41.65%	39.60%	39.66%
DIM(L)(JSD + PM)	73.25%	73.62%	66.96%	48.13%	45.92%	39.6%
DIM(L)(infoNCE)	75.05%	70.68%	69.24%	44.11%	42.97%	42.74%
DIM(L)(infoNCE + PM)	75.21%	75.57%	69.13%	49.74%	47.72%	41.61%
MIGE	一	59.72%	56.14%	54.01%	30.0%	28.96%	27.65%
14