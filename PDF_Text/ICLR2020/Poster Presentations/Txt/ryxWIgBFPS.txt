Published as a conference paper at ICLR 2020
A Meta-Transfer Objective for Learning to
Disentangle Causal Mechanisms
Yoshua Bengio1, 2, 5 Tristan Deleu1 Nasim Rahaman4 Nan Rosemary Ke3
Sebastien Lachapelle1 Olexa Bilaniuk1 Anirudh Goyal1 Christopher Pal3,5
Mila - Montreal, Quebec, Canada
Ab stract
We propose to use a meta-learning objective that maximizes the speed of transfer
on a modified distribution to learn how to modularize acquired knowledge and
discover causal dependencies. In particular, we focus on how to factor a joint
distribution into appropriate conditionals, consistent with the causal directions. To
replace the assumption that the test cases are of the same distribution as the training
examples, this method exploits the assumption that the changes in distributions
are localized (e.g. to one of the marginals, for example due to an intervention
on a cause). We prove that under this assumption of localized changes in causal
mechanisms, the correct causal graph will tend to have only a few of its param-
eters with non-zero gradient, i.e. that need to be adapted (those of the modified
variables). We argue and observe experimentally that this leads to faster adaptation,
and use this property to define a meta-learning surrogate score which, in addition
to a continuous parametrization of graphs, would favour correct causal graphs,
making it possible to discover causal structure by gradient-based methods. Finally,
motivated by the AI agent point of view (e.g. of a robot discovering its environ-
ment autonomously), we consider how the same objective can discover the causal
variables themselves, as a transformation of observed low-level variables with no
causal meaning. Experiments in the two-variable case validate the proposed ideas
and theoretical results.
1	Introduction
The data used to train our models is often assumed to be independent and identically distributed (iid.),
according to some unknown distribution. Likewise, the performance of a machine learning model is
typically evaluated using test samples from the same distribution, assumed to be representative of
the learned system’s usage. While these assumptions are well analyzed from a statistical point of
view, they are rarely satisfied in many real-world applications. For example, an accident on a major
highway could completely perturb the trajectories of cars, and a driving policy trained in a static way
might not be robust to such changes. Ideally, we would like our models to generalize well and adapt
quickly to out-of-distribution data.
However, this comes at a price - in order to successfully transfer to a novel distribution, one
might need additional information about these distributions. In this paper, we are not considering
assumptions on the data distribution itself, but rather on how it changes (e.g., when going from a
training distribution to a transfer distribution, possibly resulting from some agent’s actions). We focus
on the assumption that the changes are sparse when the knowledge is represented in an appropriately
modularized way, with only one or a few of the modules having changed. This is especially relevant
when the distributional change is due to actions by one or more agents, because agents intervene
at a particular place and time, and this is reflected in the form of the interventions discussed in
the causality literature (Pearl, 2009; Peters et al., 2016), where a single causal variable is clamped
to a particular value or a random variable. In general, it is difficult for agents to influence many
underlying causal variables at a time, and although this paper is not about agent learning as such,
this is a property of the world that we propose to exploit here, to help discovering these variables
1 Universit6 de Montreal, 2 CIFAR Senior Fellow,3 Ecole Polytechnique Montreal, 4 Max-Planck Institute
for Intelligent Systems, Tubingen,5 Canada CIFAR AI Chair
1
Published as a conference paper at ICLR 2020
and how they are causally related to each other. In this context, the causal graph is a powerful tool
because it tells us how perturbations in the distribution of intervened variables will propagate to all
other variables and affect their distributions.
As expected, it is often the case that the causal structure is not known in advance. The problem of
causal discovery then entails obtaining the causal graph, a feat which is in general achievable only
with strong assumptions. One such assumption is that a learner that has learned to capture the correct
structure of the true underlying data-generating process should still generalize to the case where the
structure has been perturbed in a certain, restrictive way. This can be illustrated by considering the
example of temperature and altitude from Peters et al. (2017): a learner that has learned to capture the
mechanisms of atmospheric physics by learning that it makes more sense to predict temperature from
the altitude (rather than vice versa) given training data from (say) Switzerland, will still remain valid
when tested on out-of-distribution data from a less mountainous country like (say) the Netherlands. It
has therefore been suggested that the out-of-distribution robustness of predictive models can be used
to guide the inference of the true causal structure (Peters et al., 2016; 2017).
How can we exploit the assumption of localized change? As we explain theoretically and verify
experimentally here, if we have the right knowledge representation, then we should get fast adaptation
to the transfer distribution when starting from a model that is well trained on the training distribution.
This arises because of our assumption that the ground truth data generative process is obtained
as the composition of independent mechanisms, and that very few ground truth mechanisms and
parameters need to change when going from the training distribution to the transfer distribution. A
model capturing a corresponding factorization of knowledge would thus require just a few updates, a
few examples, for this adaptation to the transfer distribution. As shown below, the expected gradient
on the unchanged parameters would be near 0 (if the model was already well trained on the training
distribution), so the effective search space during adaptation to the transfer distribution would be
greatly reduced, which tends to produce faster adaptation, as found experimentally. Thus, based
on the assumption of small change in the right knowledge representation space, we can define a
meta-learning objective that measures the speed of online adaptation in order to optimize the way in
which knowledge should be represented, factorized and structured. This is the core idea presented in
this paper.
Returning to the example of temperature and altitude: when presented with out-of-distribution data
from the Netherlands, we expect the correct model to adapt faster given a few transfer samples of
actual weather data collected in the Netherlands. Analogous to the case of robustness, the adaptation
speed can then be used to guide the inference of the true causal structure of the problem at hand,
possibly along with other sources of signal about causal structure.
Contributions. We first verify on synthetic data that the model that correctly captures the underlying
causal structure adapts faster when presented with data sampled after a performing certain interven-
tions on the true two-variable causal graph (which is unknown to the learner). This suggests that the
adaptation speed can indeed function as a score to assess how well the learner fits the underlying
causal graph. We then use a smooth parameterization of the considered causal graph to directly
optimize this score in an end-to-end gradient-based manner. Finally, we show in a simple setting that
the score can be exploited to disentangle the correct causal variables given an unknown mixture of
the said variables.
2	Which is Cause and Which is Effect?
As an illustrative example of the proposed ideas, let us consider two discrete random variables A
and B , each taking N possible values. We assume that A and B are correlated, without any hidden
confounder. Our goal is to determine whether the underlying causal graph is A → B (A causes B),
or B → A. Note that this underlying causal graph cannot be identified from observational data from
a single (training) distribution p only, since both graphs are Markov equivalent for p (Verma & Pearl,
1991); see Appendix A. In order to disambiguate between these two hypotheses, we will use samples
from some transfer distribution P in addition to our original samples from the training distribution p.
2.1	The advantage of the correct causal model
Without loss of generality, we can fix the true causal graph to be A → B, which is unknown to
the learner. Moreover, to make the case stronger, we will consider a setting called covariate shift
(Rojas-Carulla et al., 2018; Quionero-Candela et al., 2009), where we assume that the change (again,
2
Published as a conference paper at ICLR 2020
whose nature is unknown to the learner) between the training and transfer distributions occurs after
an intervention on the cause A. In other words, the marginal of A changes, while the conditional
P(B | A) does not, i.e. P(B | A) = P(B | A). Changes on the cause will be most informative, since
they will have direct effects on B . This is sufficient to fully identify the causal graph (Hauser &
Buhlmann, 2012).
In order to demonstrate the advantage of choosing the causal model A → B over the anti-causal
B → A, we can compare how fast the two models can adapt to samples from the transfer distribution
p. We quantify the speed of adaptation as the log-likelihood after multiple steps of fine-tuning via
(stochastic) gradient ascent, starting with both models trained on a large amount of data from the
training distribution. In Figure 1 (see Section 3.3 for the experimental setup), we can see that the
model corresponding to the underlying causal model adapts faster. Moreover, the difference is more
significant when adapting on a small amount of data, of the order of 10 to 30 samples from the
transfer distribution. We will make use of this property as a noisy signal to infer the direction of
causality, which here is equivalent to choosing how to modularize the joint distribution.
Figure 1: Adaptation to the transfer distribution (average log-likelihood of the model during fine-
tuning adaptation to transfer examples, vertical axis), as more transfer examples are seen by the
learner (horizontal axis). The curves are the median over 20,000 runs, with their 25-75th quantiles
intervals. The dotted line is the asymptotic log-likelihood (here, that of the ground truth p). The
red region corresponds to the range where the effect is the most significant (10-30 samples from the
transfer distribution).
2.2	Parameter counting argument
A simple parameter counting argument can help us understand what we are observing in Figure 1.
Since we are using gradient ascent for the adaptation, let’s first inspect how the gradients of the
log-likelihood wrt. each module behave under the transfer distribution.
Proposition 1. Let G be a causal graph, andP a (training) distribution that factorizes according to
G, with parameters θ. Let P be a second (transfer) distribution that alsofactorizes according to G. If
the training and transfer distributions have the same conditional probability distributions for all Vi
but a subset C (e.g. the transfer distribution is the result of an intervention on the nodes in C):
P(V | PaG(K)) = P(V | PaG(Vi))	∀Vi ∈ C	(1)
then the expected gradient w.r.t. the parameters θi such that Vi ∈/ C of the log-likelihood under the
transfer distribution will be zero
Wi ∈ C, EV〜p [d logP(V) ] = 0.	(2)
Proposition 1 (see proof in Appendix B.1) suggests that if both distributions factorize according to the
correct causal graph, then only the parameters of the mechanisms that changed between the training
and transfer distributions need to be updated. This effectively reduces the number of parameters that
need to be adapted compared to any other factorization over a different graph. It also affects the
number of examples necessary for the adaptation, since the sample complexity of a model grows
approximately linearly with the VC-dimension (Ehrenfeucht et al., 1989; Vapnik & Chervonenkis,
1971), which itself also grows approximately linearly with the number of parameters (for linear
3
Published as a conference paper at ICLR 2020
models and neural networks; Shalev-Shwartz & Ben-David, 2014). Therefore we argue that the
performance on the transfer distribution (in terms of log-likelihood) will tend to improve faster if it
factorizes according to the correct causal graph, an assertion which may not be true for every graph
but that we can test by simulations.
Recall that in our example on two discrete random variables (each taking say N values), we assumed
that the underlying causal model is A → B, and the transfer distribution is the result ofan intervention
on the cause A. If the model we learn on the training distribution factorizes according to the correct
graph, then only N - 1 free parameters should be updated to adapt to the shifted distribution,
accounting for the change in the marginal distribution P(A), since the conditional P(B | A) = p(B |
A) stays invariant. On the other hand, if the model factorizes according to the anti-causal graph
B → A, then the parameters for both the marginal P(B) and the conditional P(A | B) must be
adapted. Assuming there is a linear relationship between sample complexity and the number of free
parameters, the sample complexity would be O(N 2) for the anti-causal graph, compared to only
O(N) for the true underlying causal graph A → B.
3	The Meta-Transfer Objective
Since the speed of adaptation to some transfer distribution is closely related to the right modularization
of knowledge, we propose to use it as a noisy signal to iteratively improve inference of the causal
structure from data. Moreover, we saw in Figure 1 that the gap between correct and incorrect models
is largest with a small amount of transfer data. In order to compare how fast some models adapt to a
change in distribution, we can quantify the speed of adaptation based on their accumulated online
performance after fine-tuning with gradient ascent on few examples from the transfer distribution.
More precisely, given a small “intervention” dataset Dint = {xt}T=ι fromp, We can define the online
likelihood as
L(D Y -T] rt( θ θ(t) G)	θG1) = GGL(Dobs)
LG(Dint) = ∏P(Xt; % ,G) θG+1)=暇 + αVθ logp(χt ；娉,G),⑶
Where θG(t) aggregates all the modules’ parameters in G after t steps of fine-tuning With gradient
ascent, with learning rate a, starting from the maximum-likelihood estimate GML(Dobs) on a large
amount of data Dobs from the training distribution P. Note that, in addition to its contribution to
the update of the parameters, each data point Xt is also used to evaluate the performance of our
model so far; this is called a prequential analysis (Dawid, 1984), also corresponding to sequential
cross-validation (Gingras et al., 1999). From a structure learning perspective, the online likelihood
(or, equivalently, its logarithm) can be interpreted as a score we would like to maximize, in order to
recover the correct causal graph.
3.1	Connection to the Bayesian score
We can draw an interesting connection between the online log-likelihood, and a widely used score in
structure learning called the Bayesian score (Heckerman et al., 1995; Geiger & Heckerman, 1994).
The idea behind this score is to treat the problem of learning the structure from a fully Bayesian
perspective. If we define a prior over graphs P(G) and a prior P(GG | G) over the parameters of each
graph G, the Bayesian score is defined as scoreB(G ; Dint) = log P(Dint | G) + log P(G), where
P(Dint | G) is the marginal likelihood
T
P(Di
nt | G) =	P(Xt | X1 , . .
t=1
. , Xt-1,
T
G)=Y
t=1	Θ
P(Xt | GG , G)P(GG | X1:t-1, G) dGG
(4)
In the online likelihood, the adapted parameters GG(t) act as a summary of past data X1:t-1. Eq. (3)
can be seen as an approximation of the marginal likelihood in Eq. (4), where the posteriors over
the parameters P(GG | X1:t-1, G) is approximated by the point estimate GG(t). Therefore, the online
log-likelihood provides a simple way to approximate the Bayesian score, which is often intractable.
3.2	A smooth parametrization of the causal structure
Due to the super-exponential number of possible Directed Acyclic Graphs (DAGs) over n nodes,
the problem of searching for a causal structure that maximizes some score is, in general, NP-hard
4
Published as a conference paper at ICLR 2020
(Chickering, 2002a). However, we can parametrize our belief about causal graphs by keeping track
of the probability for each directed edge to be present. This provides a smooth parametrization of
graphs, which hinges on gradually changing our belief in individual binary decisions associated with
each edge of the causal graph. This allows us to define a fully differentiable meta-learning objective,
with all the beliefs being updated at the same time by gradient descent.
In this section, we study the simplest version of this idea, applied to our example on two random
variables from Section 2. Recall that here, we only have two hypotheses to choose from: either
A → B or B → A. We represent our belief of having an edge connecting A to B with a structural
parameter γ such that p(A → B) = σ(γ), where σ(γ) = 1/(1 + exp(-γ)) is the sigmoid function.
We propose, as a meta-transfer objective, the negative log-likelihood R (a form of regret) over the
mixture of these two models, where the mixture parameter is given by σ(γ):
R(Dint) = - log [σ (γ)LA→B (Dint) + (1 - σ(γ))LB→A(Dint)]
(5)
This meta-learning mixture combines the online adaptation likelihoods of each model over one
meta-eXamPle or episode (specified by a Dint 〜P), rather than considering and linearly mixing the
per-example likelihoods as in ordinary mixtures.
In the experiments below, after each episode involving T examples Dint from the transfer distribution
p, We update Y by doing one step of gradient descent, to reduce the regret R. Therefore, in order to
update our belief about the edge A → B, the quantity of interest is the gradient of the objective R
with respect to the structural parameter, ∂R∕∂γ. This gradient is pushing σ(γ) towards the posterior
probability that the correct model is A → B , given the evidence from the transfer data:
Proposition 2. The gradient of the negative log-likelihood of the transfer data Dint in Equation (5)
wrt. the structural parameter γ is given by
∂R
=—=p(A → B) - p(A → B | Dint),
(6)
where p(A → B | Dint) is the posterior probability of the hypothesis A → B (when the alternative
is B → A). Furthermore, this can be equivalently written as
σ(γ) -σ(γ+∆),
(7)
where ∆ = log LA→B (Dint) - log LB→A(Dint) is the difference between the online log-likelihoods
of the two hypotheses on the transfer data Dint.
The proof is given in Appendix B.2. Note how the posterior probability is basically measuring which
hypothesis is better explaining the transfer data Dint overall, along the adaptation trajectory. This
posterior depends on the difference in online log-likelihoods ∆, showing the close relation between
minimizing the regret R and maximizing the online log-likelihood score. The sign and magnitude
of ∆ have a direct effect on the convergence of the meta-transfer objective. We can show that the
meta-transfer objective is guaranteed to converge to one of the two hypotheses.
Proposition 3. With stochastic gradient descent (and an appropriately decreasing learning rate)
on EDint [R(Dint)], where the gradient steps are given by Proposition 2, the structural parameter
converges towards
σ(γ) → 1 if EDint [LA→B(Dint)] > EDint [LB→A(Dint)]
or σ(γ) → 0 otherwise
(8)
This proposition (proved in Appendix B.3) shows that optimizing γ is equivalent to picking the
hypothesis that has the smallest regret (or fastest convergence), measured as the accumulated log-
likelihood of the transfer dataset Dint during adaptation. The distribution over datasets Dint is
similar to a distribution over tasks in meta-learning. This analogy with meta-learning also appears
in our gradient-based adaptation procedure, which is linked to existing methods like the first-order
approximation of MAML (Finn et al., 2017), and its related algorithms (Grant et al., 2018; Kim et al.,
2018; Finn et al., 2018). The pseudo-code for the proposed algorithm is given in Algorithm 1.
This smooth parametrization of the causal graph, along with the definition of the meta-transfer
objective in Equation (5), can be extended to graphs with more than 2 variables. This general
formulation builds on the bivariate case, where decisions are binary for each individual edge of the
graph. See Appendix E for details and a generalization of Proposition 2; the structure of Algorithm 1
remains unchanged. Experimentally, this generalization of the meta-transfer objective proved to be
effective on larger graphs (Ke et al., 2019), in work following the initial release of this paper.
5
Published as a conference paper at ICLR 2020
Algorithm 1 Meta-learning algorithm for learning the structural parameter
Require: TWo graph candidates G = A → B and G = B → A
Require: A training distribution p that factorizes over the correct causal graph
1:	Set the initial structural parameter γ = 0	. equal belief for both hypotheses
2:	Sample a large dataset Dobs from the training distribution p
3:	Pretrain the parameters of both models With maximum likelihood on Dobs
4:	for each episode do
5:	Draw a transfer distribution P (via an intervention)
6:	Sample a (small) transfer dataset Dint = {xt}T=ι fromP
7:	for t = 1, . . . , T do
8:	Accumulate the online log-likelihood for both models LA→B and LB→A as they adapt
9:	Do one step of gradient ascent for both models: θ(G+1) = θG) + αVθ logP(Xt; θ(G),G)
10:	Compute the regret R(Dint)
11:	Compute the gradient of the regret wrt. γ (see Proposition 2)
12:	Do one step of gradient descent on the regret w.r.t. γ
13:	Reset the models’ parameters to the maximum likelihood estimate on Dobs
3.3 Experimental results
To illustrate the convergence result from Proposition 3, we experiment with learning the structural
parameter γ in a bivariate model. Following the setting presented in Section 2.1, we assume in all
our experiments that A and B are two correlated random variables, and the underlying causal model
(unknown to the algorithm) is fixed to A → B . Recall that both variables are observed, and there is
no hidden confounding factor. Since the correct causal model is A → B , the structural parameter
should converge correctly, with σ(γ) → 1. The details of the experimental setups, as well as details
about the models, can be found in Appendix C.
We first experiment with the case where both A and B are discrete random variables, taking N
possible values. In this setting, we explored how two different parametrizations of the conditional
probability distributions (CPDs) might influence the convergence of the structural parameter. In the
first experiment, we parametrized the CPDs as multinomial logistic CPDs (Koller & Friedman, 2009),
maintaining a tabular representation of the conditional probabilities. For example, the conditional
distribution P(B | A) is represented as
P(B = j | A = i ; θ)
exp(θj)
Pk eχp(θik),
(9)
where the parameter θ is an N × N matrix. We used a similar representation for the other marginal
and conditional distributions P(A), P(B) and P(A | B). In a second experiment, we used structured
CPDs, parametrized with multi-layer perceptrons (MLPs) with a softmax nonlinearity at the output
layer. The advantage over a tabular representation is the ability to share parameters for similar
contexts, and reduces the overall number of parameters required for each module. This would be
crucial if either the number of categories N, or the number of variables, increased significantly.
Figure 2: Evolution of the belief that A → B is the correct causal model, as the number of episodes
increases, starting with an equal belief for both hypotheses. (Left) multinomial logistic CPDs, (right)
MLP parametrization.
6
Published as a conference paper at ICLR 2020
In Figure 2, we show the evolution of σ(γ), which is the model’s belief of A → B being the correct
causal model, as the number of episodes increases, for different values of N . As expected, the struc-
tural parameter converges correctly to σ(γ) → 1, within a few hundreds episodes. This observation
is consistent in both experiments, regardless of the parametrization of the CPDs. Interestingly, the
structural parameter tends to converge faster with a larger value of N and a tabular representation,
illustrating the effect of the parameter counting argument described in Section 2.2, which is stronger
as N increases. Precisely when generalization is more difficult (too many parameters and too few
examples), we get a stronger signal about the better modularization.
We also experimented with A and B being continuous random variables, where they follow either
multimodal distributions, or they are linear-Gaussian. Similar to Figure 2, we found that the structural
parameter σ(γ) consistently converges to the correct causal model as well. See Appendix C.3 and
Appendix C.4 for details about these experiments.
4 Representation Learning
So far, we have assumed that all the variables in the causal graph are fully observed. However, in many
realistic scenarios for learning agents, the learner might only have access to low-level observations
(e.g. sensory-level data, like pixels or acoustic samples), which are very unlikely to be individually
meaningful as causal variables. In that case, our assumption that the changes in distributions are
localized might not hold at this level of observed data. To tackle this, we propose to follow the deep
learning objective of disentangling the underlying causal variables (Bengio et al., 2013), and learn a
representation in which the variables can be meaningfully cause or effect of each other. Our approach
is to jointly learn this representation, as well as the causal graph over the latent variables.
We consider the simplest setting where the learner maps raw observations to a hidden representation
space with two causal variables, via an encoder E . The encoder is trained such that this latent space
helps to optimize the meta-transfer objective described in Section 3. We consider the parameters
of the encoder, as well as γ (see Section 3.2), as part of the set of structural meta-parameters to
be optimized. We assume that we have two raw observed variables (X, Y ), generated from the
true causal variables (A, B) via the action of a ground truth decoder D (or generator network), that
the learner is not aware of. This allows us to still have the ability to intervene on the underlying
causal variables (e.g. to shift from training to transfer distributions) for the purpose of conducting
experiments, while the learner only sees data from (X, Y ).
Data generation (unknown to the learner)
A B
O→<3
(A, B)	Decoder D	(X, Y)	Encoder E	(U, V)
UV
o→o
or
UV
Figure 3: The complete experimental setup. The ground-truth variables (A, B) are assumed to
originate from the true underlying causal model, but the observations available to the learner are
samples from (X, Y ). The observed variables (X, Y ) are derived from (A, B) via the action of a
decoder D. The encoder E must be learned to undo this action of the decoder, and thereby recover
the true causal variables up to symmetries. The components of the data generation on the left are
hidden to the model.
In this experiment, we only want to validate the proposed meta-objective as a way to recover a good
encoder, and we assume that both the decoder D and the encoder E are rotations, whose angles are
θD and θE respectively. The encoder maps the raw observed variables (X, Y ) to the latent variables
(U, V ), over which we want to infer the causal graph. Similar to our experiments in Section 3.3, we
assume that the underlying causal graph is A → B, and the transfer distribution P (now over (X, Y))
is the result of an intervention over A. Therefore, the encoder should ideally recover the structure
U → V in the learned latent space, along with the angle of the encoder θE = -θD . However,
since the encoder is not uniquely defined, V → U might also be a valid solution, if the encoder is
Θe = -n/2 - Θd. Details about the experimental setup are provided in Appendix D. In Figure 4,
7
Published as a conference paper at ICLR 2020
we consider that the learner succeeds, since both structural parameters converge to one of the two
options. This shows how minimizing the meta-transfer objective can disentangle (here in a very
simple setting) the ground-truth variables.
π
4
π
8
智 0
π
-8
π
-4
0	200	400	600	800	1000	0	200	400	600	800	1000
Number of episodes	Number of episodes
Figure 4: Evolution of structural parameters θE and γ, as number of episodes increases. Angle of the
rotation for the decoder is set to Θd = -n/4, so there are two valid solutions for the angle Θe of the
encoder: either Θe = n/4, or Θe = -n/4; the model converges to the former solution.
5	Related work
As stated already by Bengio et al. (2013), and clearly demonstrated by Locatello et al. (2019),
assumptions, priors, or inductive biases are necessary to identify the underlying explanatory variables.
The latter paper (Locatello et al., 2019) also reviews and evaluates recent work on disentangling,
and discusses different metrics that have been proposed. Chalupka et al. (2015; 2017) recognize
the potential and the challenges underlying causal representation learning. Closely related to our
efforts is (Chalupka et al., 2017), which places a strong focus on the coalescence of low (e.g. sensory)
level observations (microvariables) to higher level causal variables (macrovariables), albeit in a more
observational setting.
There also exists an extensive literature on learning the structure of Bayesian networks from (observa-
tional) data, via score-based methods (Koller & Friedman, 2009). Heckerman et al. (1995); Daly et al.
(2011) provide a comprehensive review of these methods. Many of these algorithms are based on
greedy-search with local changes to the graphs (Chickering, 2002b), whereas we propose a continuous
and fully-differentiable alternative. While most of these approaches only rely on observational data,
it is sometimes possible to extend the definition of these scores to interventional data (Hauser &
Buhlmann, 2012). The online-likelihood score presented here supports interventional data as its main
feature.
Some identifiability results exist for causal models with purely observational data though (Peters et al.,
2017), based on specific assumptions on the underlying causal graph. However, causal discovery is
more natural under local changes in distributions (Tian & Pearl, 2001), similar to the setting used
in this paper. Pearl’s seminal work on do-calculus (Pearl, 1995; 2009; Bareinboim & Pearl, 2016)
lays the foundation for expressing the impact of interventions on causal graphical models. Here we
are proposing a meta-learning objective function for learning the causal structure (without hidden
variables), requiring mild assumptions such as localized changes in distributions and faithfulness of
the causal graph, in contrast to the stronger assumptions necessary for these identifiability results.
Our work is also related to other recent advances in causation, domain adaptation, and transfer
learning. Magliacane et al. (2018) have sought to identify a subset of features that leads to the best
predictions for a variable of interest in a source domain, such that the conditional distribution of
that variable given these features is the same in the target domain. Zhang et al. (2017) also examine
non-stationarity and find that it makes causal discovery easier. Our adaptation procedure, using
gradient ascent, is also closely related to gradient-based methods in meta-learning (Finn et al., 2017;
Finn, 2018). Alet et al. (2018) proposed a meta-learning algorithm to recover a set of specialized
modules, but did not establish any connections to causal mechanisms. More recently, Dasgupta et al.
(2019) adopted a meta-learning approach to perform causal inference on purely observational data.
8
Published as a conference paper at ICLR 2020
6	Discussion & Future Work
We have established, in very simple bivariate settings, that the rate at which a learner adapts to
sparse changes in the distribution of observed data can be exploited to infer the causal structure, and
disentangle the causal variables. This relies on the assumption that with the correct causal structure,
those distributional changes are localized. We have demonstrated these ideas through some theoretical
results, as well as experimental validation. The source code for the experiments is available here:
https://bit.ly/2M6X1al.
This work is only a first step in the direction of causal structure learning based on the speed of
adaptation to modified distributions. On the experimental side, many settings other than those studied
here should be considered, with different kinds of parametrizations, richer and larger causal graphs
(see already Ke et al. (2019), based on a first version of this paper), or different kinds of optimization
procedures. On the theoretical side, much more needs to be done to formally link the locality
of interventions to faster adaptation, to clarify the conditions for this to work. Also, more work
needs to be done in exploring how the proposed ideas can be used to learn good representations in
which the causal variables are disentangled. Scaling up these ideas would permit their application
towards improving the way learning agents deal with non-stationarities, and thus improving sample
complexity and robustness of these agents.
An extreme view of disentangling is that the explanatory variables should be marginally independent,
and many deep generative models (Goodfellow et al., 2016), and Independent Component Analysis
models (Hyvarinen et al., 2001; Hyvarinen et al., 2018), are built on this assumption. However, the
kinds of high-level variables that we manipulate with natural language are not marginally independent:
they are related to each other through statements that are usually expressed in sentences (e.g. a
sentence in natural language, or a classical symbolic AI fact or rule), involving only a few concepts at
a time. This kind of assumption has been proposed to help discover relevant high-level representations
from raw observations, such as the consciousness prior (Bengio, 2017), with the idea that humans
focus at any particular time on just a few concepts that are present to our consciousness. The work
presented here could provide an interesting meta-learning approach to help learn such encoders
outputting causal variables, as well as figure out how the resulting variables are related to each other.
In that case, one should distinguish two important assumptions: the first one is that the causal graph
is sparse, which a common assumption in structure learning (Schmidt et al., 2007); the second is that
the changes in distributions are sparse, which is the focus of this work.
References
Ferran Alet, TomgS Lozano-PCrez, and Leslie P Kaelbling. Modular meta-learning. arXiv preprint
arXiv:1806.10166, 2018.
Elias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. Proceedings of the
National Academy of Sciences, 2016.
Yoshua Bengio. The Consciousness Prior. arXiv preprint arXiv:1709.08568, 2017.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Review and New
Perspectives. IEEE transactions on pattern analysis and machine intelligence, 2013.
Christopher M Bishop. Mixture Density Networks. Technical report, 1994.
David Blackwell. Conditional Expectation and Unbiased Sequential Estimation. The Annals of
Mathematical Statistics, 1947.
Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Visual causal feature learning. Confer-
ence on Uncertainty in Artificial Intelligence (UAI) 2015, 2015.
Krzysztof Chalupka, Frederick Eberhardt, and Pietro Perona. Causal feature learning: an overview.
Behaviormetrika, 2017.
David Maxwell Chickering. Learning equivalence classes of Bayesian-network structures. Journal of
machine learning research, 2002a.
9
Published as a conference paper at ICLR 2020
David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine
learning research, 2002b.
R6ndn Daly, Qiang Shen, and Stuart Aitken. Learning Bayesian networks: approaches and issues.
The knowledge engineering review, 2011.
Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward
Hughes, Peter Battaglia, Matthew Botvinick, and Zeb Kurth-Nelson. Causal Reasoning from
Meta-reinforcement Learning. arXiv preprint arXiv:1901.08162, 2019.
A Philip Dawid. Present position and potential developments: Some personal views statistical theory
the prequential approach. Journal of the Royal Statistical Society: Series A (General), 1984.
Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. A general lower bound on
the number of examples needed for learning. Information and Computation, 1989.
Chelsea Finn. Learning to Learn with Gradients. PhD thesis, UC Berkeley, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation
of Deep Networks. International Conference on Machine Learning (ICML), 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic Model-Agnostic Meta-Learning. In
Advances in Neural Information Processing Systems, 2018.
Dan Geiger and David Heckerman. Learning Gaussian Networks. In Proceedings of the Tenth
international conference on Uncertainty in artificial intelligence, 1994.
FrangoiS Gingras, YoShUa Bengio, and Claude Nadeau. On Out-of-Sample Statistics for Financial
Time-Series. Technical report, Departement d,informatique et recherche Qperationnelle, Universite
deMontr6al,1999.
Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. URL
http://deeplearningbook.org.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting Gradient-
Based Meta-Learning as Hierarchical Bayes. arXiv preprint arXiv:1801.08930, 2018.
Alain Hauser and Peter Buhlmann. Characterization and Greedy Learning of Interventional Markov
Equivalence Classes of Directed Acyclic Graphs. Journal of Machine Learning Research, 2012.
David Heckerman, Dan Geiger, and David M Chickering. Learning Bayesian networks: The
combination of knowledge and statistical data. Machine learning, 1995.
Aapo Hyvarinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. Wiley-
Interscience, 2001.
Aapo Hyvarinen, Hiroaki Sasaki, and Richard E. Turner. Nonlinear ICA Using Auxiliary Variables
and Generalized Contrastive Learning. International Conference on Artificial Intelligence and
Statistics (AISTATS) 2019, 2018.
Nan Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Christopher Pal, and Yoshua
Bengio. Learning Neural Causal Models from Unknown Interventions. 2019.
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian Model-Agnostic Meta-Learning. In Advances in Neural Information Processing Systems,
2018.
Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT
press, 2009.
Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Scholkopf, and Olivier
Bachem. Challenging Common Assumptions in the Unsupervised Learning of Disentangled
Representations. ICLR 2019 Workshop on Reproducibility in Machine Learning, 2019.
10
Published as a conference paper at ICLR 2020
Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M
Mooij. Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distribu-
tions. In Advances in Neural Information Processing Systems, 2018.
Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Scholkopf. Learning
Independent Causal Mechanisms. 2017.
Judea Pearl. Causal diagrams for empirical research. Biometrika, 1995.
Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant
prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series
B (StatisticalMethodology), 78(5):947-1012, 2016.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of Causal Inference: Foundations
and Learning algorithms. MIT press, 2017.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset
Shift in Machine Learning. MIT Press, 2009.
C. Radhakrishna Rao. Information and the Accuracy Attainable in the Estimation of Statistical
Parameters. 1992.
Mateo Rojas-Carulla, Bernhard Scholkopf, Richard Turner, and Jonas Peters. Invariant Models for
Causal Transfer Learning. Journal of Machine Learning Research, 2018.
Mark W. Schmidt, Alexandru Niculescu-Mizil, and Kevin P. Murphy. Learning Graphical Model
Structure Using L1-Regularization Paths. Association for the Advancement of Artificial Intelligence
(AAAI) 2007, 2007.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning - from Theory to
Algorithms. Cambridge University Press, 2014.
Jin Tian and Judea Pearl. Causal Discovery from Changes. In Proceedings of the Seventeenth
conference on Uncertainty in Artificial Intelligence, 2001.
V. N. Vapnik and A. Y. Chervonenkis. On the Uniform Convergence of Relative Frequencies of
Events to Their Probabilities. Theory of Probability and its Applications, 1971.
Thomas Verma and Judea Pearl. Equivalence and Synthesis of Causal Models. In Proceedings of the
Sixth Annual Conference on Uncertainty in Artificial Intelligence, UAI 90, 1991.
Kun Zhang, Biwei Huang, Jiji Zhang, Clark Glymour, and Bernhard Scholkopf. Causal discovery
from nonstationary/heterogeneous data: Skeleton estimation and orientation determination. In
IJCAI: proceedings of the conference, 2017.
Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. DAGs with NO TEARS:
Continuous Optimization for Structure Learning. In Advances in Neural Information Processing
Systems 31. 2018.
11
Published as a conference paper at ICLR 2020
A Results on Non-Identifiability of the Causal Structure
Suppose that A and B are two discrete random variables, each taking N possible values. We show
here that the maximum likelihood estimation of both models A → B and B → A yields the same
estimated distribution over A and B . The joint likelihood on the training distribution is not sufficient
to distinguish the causal model between the two hypotheses. If p is the training distribution, let
θi = P(A = i)	θj∣i = P(B = j | A = i)	(10)
ηj = P(B = j)	ηi∣j = P(A = i I B = j)	(11)
Let Dobs be a training dataset. If Ni(A) is the number of samples in Dobs where A = i, Nj(B) the
number of samples where B = j , and Nij the number of samples where A = i and B = j , then the
maximum likelihood estimator for each parameter is
θi = Ni(A)/N	θj∣i = Nij/N(A)	(12)
ηj = NB/N	ηi∣j = Nij/NjB.	(13)
The estimated distributions for each model A → B and B → A, under the maximum likelihood
estimator, will be equal:
ʌ ʌ
P(A = i,B = j ； A → B) = θiθj∣i = Nij/N	(14)
P(A = i,B = j ; B → A) = ηj^i∖j = Nij/N	(15)
To illustrate this result, we also experiment with maximizing the likelihood for each modules for both
models A → B and B → A with SGD. In Figure A.1, we show the difference in log-likelihoods
between these two models, evaluated on training and test data sampled from the same distribution,
during training. We can see that while the model A → B fits the data faster than the other model
(corresponding to a positive difference in the figure), both models achieve the same log-likelihoods
at convergence. This shows that the two models are indistinguishable, in the limit, based on data
sampled from the same distribution, even on test data.
Figure A.1: Difference in log-likelihoods between the two models A → B and B → A on training
and test data from the same distribution on discrete data, for different values of N, the number
of discrete values per variable. Once fully trained, both models become indistinguishable from
their log-likelihoods only, even on test data. The solid curves represent the median values over 100
different runs, and the shaded areas their 25-75 quantiles.
B	Proofs
B.1 Zero-gradient under mechanism change
Let us restate Proposition 1 here for convenience:
Proposition 1. Let G be a causal graph, andP a (training) distribution that factorizes according to
G, With parameters θ. Let P be a second (transfer) distribution that alsofactorizes according to G. If
the training and transfer distributions have the same conditional probability distributions for all Vi
but a subset C (e.g. the transfer distribution is the result of an intervention on the nodes in C):
P(V I PaG(K))= P(V | PaG(Vi))	∀vi ∈c	(16)
12
Published as a conference paper at ICLR 2020
then the expected gradient w.r.t. the parameters θi such that Vi ∈/ C of the log-likelihood under the
transfer distribution will be zero
∀Vi ∈C, EV〜P [d logp(V) ] =0.
∂θi
(17)
Proof. For Vi ∈/ C, we can simplify the expected gradient as follows:
EV〜P	∂ log p(V) 丽i	=EV〜P ]χ		X logP(Vj l PaG(Vj) ； θj) ∂θi			(18)
		=EV 〜P	∂ 西	log p(Vi I PaG (Vi) ; θi)			(19)
		=EV 〜P	∂ 西	log pP(Vi I PaG(Vi); θi)			(20)
		=EV 〜P	n √-1	∂θ" log p(Vj | PaG(V, )； θj)			(21)
		=EV 〜P	"∂ log P(V )■ -丽i一		=0		(22)
where Equation (20) arises from our assumption that the conditional distribution of Vi given its
parents in G does not change between the training distribution p and the transfer distribution P
Moreover, the last equality arises from the marginalization
pP(v) = 1
v
(23)
B.2 Gradient of the structural parameter
Let us restate Proposition 2 here for convenience:
Proposition 2. The gradient of the negative log-likelihood of the transfer data Dint in Equation (5)
wrt. the structural parameter γ is given by
∂R
= p(A → B ) - p(A → B | Dint ),
(24)
where p(A → B | Dint) is the posterior probability of the hypothesis A → B (when the alternative
is B → A). Furthermore, this can be equivalently written as
∂R
=r = σ(γ) - σ(γ + △),
(25)
where ∆ = log LA→B (Dint) - log LB→A(Dint) is the difference between the online log-likelihoods
of the two hypotheses on the transfer data Dint.
Proof. First note that, using Bayes rule,
(A → B |d ) = ___________________P(DintI A → B)P(A → B)____________________
p(	|	int) = p(Dint I a → B)p(A → B)+ p(Dint I B → A)p(B → A)
_	La→b (Dint)σ(γ)
=La→b(Dint)σ(γ) + LB→A(Dint)(1 - σ(γ))
LA→B(Dint)σ (γ)
(26)
(27)
(28)
M
13
Published as a conference paper at ICLR 2020
where M = LA→B (Dint)σ(γ) + LB→A (Dint)(1 - σ(γ)) is the online likelihood of the transfer
data under the mixture, so that the regret is R(Dint) = - log M. For Equation (27), note that if
Dint = {at, bt}tT=1,
T
p(Dint | A → B) = Y p(at, bt | A → B, {as, bs}ts-=11)	(29)
t=1
T
=	p(at,bt | A→B; θA(t→) B) =LA→B(Dint)	(30)
t=1
where θA(t→) B encapsulates the information about the previous datapoints {as , bs }ts-=11 in the graph
A → B, through some adaptation procedure. Since we only consider the two hypotheses A → B
and B → A, we also have
P(B → A | Dint) = 1 — P(A → B | Dint) = LB→A(DinM(I - (Js))	(31)
Therefore, the gradient of the regret wrt. the structural parameter γ is
~∂^ = - M [σ(Y)(I - σ(T))LA→B (Dint) - J(T)(I - J(T))LB→A(Dint)]	(32)
= σ(γ)P(B → A | Dint) - (1 - σ (γ))P(A → B | Dint)	(33)
= J(T) - J(T)P(B → A | Dint) - P(A → B | Dint) + J (T)P(A → B | Dint)	(34)
= J(T) -P(A→B | Dint)	(35)
= P(A → B) - P(A → B | Dint )	(36)
which concludes the first part of the proof. Moreover, given Equation (35), it is sufficient to show
that P(A → B | Dint) = J(T + ∆) to prove the equivalent formulation in Equation (25). Using the
logit function σ-1(z) = log ɪ-z, and the expression in Equation (28), We have
J-1(P(A → B | Dint))
J(T)LA→B (Dint)
M - j(t)La→B (Dint)
(37)
l	J(T)LA→B (Dint)
θg(1- σ(γ))LB→A(Dint)
log i-----L + log La→B (Dint) - log Lb→A (Dint)
1 - J(T)	|--------------{z-------------}
|-----V---}	=∆
=γ
(38)
(39)
T+∆.	(40)
B.3 Convergence point of Gradient Descent on the structural parameter
Let us restate Proposition 3 here for convenience:
Proposition 3. With stochastic gradient descent (and an appropriately decreasing learning rate)
on EDint [R(Dint)], where the gradient steps are given by Proposition 2, the structural parameter
converges towards
J(T) → 1 if EDint [LA→B(Dint)] > EDint [LB→A(Dint)]
or J(T) → 0 otherwise
(41)
Proof. We are going to consider the fixed point of gradient descent (a point Where the gradient is
zero), since We already knoW that SGD converges With an appropriately decreasing learning rate.
Let us introduce some notations to simplify the algebra: letP = J(T), M = PLA→B (Dint) + (1 -
P)LB→A(Dint), so that the regret is R(Dint) = - log M. We define P1 and P2 as (see also the
proof in Appendix B.2)
Pl = pLA→M("t = P(A → B | Dint)	P2 = (I- P)LM→a(Dint) = 1 - Pi (42)
14
Published as a conference paper at ICLR 2020
Framing the stationary point in terms of p rather than γ gives us a constrained optimization problem,
with inequality constraints -p ≤ 0 andp -
min
p
s.t.
1 ≤ 0, and no equality constraint.
EDint [R(Dint)]
(43)
-p≤0
p-1 ≤0
(44)
(45)
Applying the KKT conditions to this problem, with constraint functions -p and p - 1, gives us
EDint
-μι + μ2
(46)
μi ≥ 0 for i = 1, 2
μιp = 0
μ2(P - I)=O
(47)
(48)
(49)
We already see from equations (48) & (49) that ifp ∈ (0, 1) (i.e. excluding 0 and 1), we must have
μι = μ2 = 0, that is
EDint
0.
(50)
Let us study that case first, and show that it leads to an inconsistent set of equations (thus, forcing
the solution to be either p = 0 or p = 1). Let us rewrite the gradient to highlight p in it (using
Proposition 2):
∂R
∂p
1
p(1 - p)
1
(p - p(A → B | Dint))
(51)
pLA→B(Dint)
-------P---------------
p(1 - p)	M
1	P(PLA→B (Dint) + (1 - P)LB→A (Dint)) - PLA→B (Dint)
(52)
p(1 - p)
LB→A(Dint) - LA→B(Dint)
M
(53)
M
(54)
This derivation is valid since we assume that p ∈ (0, 1). Suppose that p 6= 0; multiplying both sides
of Equation (50) by p gives
0 = EDint
EDint
EDint
EDint
p(LB→A(Dint) - LA→B (Dint))
pLB→A (Dint)
M
Lb→A ①int)
M
LB→A(Dint)
M
For this equation to be satisfied, we need LB→A
M
- P1
- P2 - P1
-1
(55)
(56)
(57)
(58)
M almost surely, since LB→A (Dint) ≤ M by
construction. This would, however, correspond to p = 0, which contradicts our assumption. Similarly,
assuming that p 6= 1, we can also multiply both sides of Equation (50) by 1 - p and get
0 = ED
int
(1 - p)(LB→A (Dint) - LA→B (Dint)
EDint
ED
int
ED
int
P2 -
M
(1 - p)LA→B (Dint)
P2 + P1 -
M
LA→B (Dint)
M
LA→B(Dint)
1 M
(59)
(60)
(61)
(62)
15
Published as a conference paper at ICLR 2020
Again, this can only be true if LA→B = M almost surely, meaning that p = 1, contradicting our
assumption. We conclude that the solutions p ∈ (0, 1) are not possible because they would lead to
inconsistent conclusions, which leaves only p = 0 or p = 1.
C Results on Learning which is Cause and which is Effect
In order to assess the performance of our meta-learning algorithm, we applied it on generated data
from three different domains: discrete random variables, multimodal continuous random variables
and multivariate Gaussian-distributed variables. In this section, we describe the setups for all three
experiments, along with additional results to complement the results descrbed in Section 3.3. Note that
in all these experiments, we fix the ground-truth structure as A → B, and only perform interventions
on the cause A.
C.1 Discrete variables with tabular representation
We consider a bivariate model, where both random variables are sampled from a categorical distribu-
tion. The underlying ground-truth model can be described as
A 〜Categorical(∏A)	(63)
B | A = a 〜Categorical(∏B∣ɑ),	(64)
with ∏a a probability vector of size N, and ∏b∣° a probability vector of size N, which depends on
the value of the variable A. In our experiment, each random variable can take one of N = 10 or
N = 100 values. Since we are working with only two variables, the only two possible models are:
•	Model A → B: p(A, B) = p(A)p(B | A)
•	Model B → A: p(A, B) = p(B)p(A | B)
We build 4 different modules, corresponding to every possible marginal and conditional distributions.
Here, we use multinomial logistic Conditional Probability Distributions (Koller & Friedman, 2009).
The modules’ definition, and their corresponding parameters, are shown in Table C.1.
Table C.1: Description of the 2 models, with the parametrization of each module, for a bivariate
model with discrete random variables. Model A → B and Model B → A both have the same number
of parameters N2 + N.
Distribution	Module	Parameters	Dimension
Model P(A)	P(xA = i ; θA) = [softmax(θA)]i	θA	N
A → B P(B | A)	P(XB = j | XA = i ； Θb∣a) = [softmax(θB∣A(i))]j	θB∣A	N2
Model P(B)	P(xB = j ; θB) = [softmax(θB)]j	Θb	N
B → A p(A | B)	P(XA = i | XB = j ; Θa∣b) = [softmax(θA∣B (j))]i	θA∣B	N2
In order to get a set of initial parameters, we first train all 4 modules on a training distribution (p in
the main text). This distribution corresponds to a fixed choice of ∏(1) and ∏B∣a (for all N possible
values of a). The superscript in πA(1) emphasizes the fact that this defines the distribution prior to an
intervention, with the mechanism p(B | A) being unchanged by the intervention. These probability
vectors are sampled randomly from a uniform Dirichlet distribution:
∏A1) 〜Dirichlet(IN)	(65)
∏B∣a 〜Dirichlet(IN)	∀a ∈ [1, N].	(66)
Given this training distribution, we can sample a large dataset of samples Dobs = {ai , bi }im=1 for
the ground truth model, using ancestral sampling. Using Dobs , we can train all 4 modules using
gradient ascent on the log-likelihood (or any other advanced first-order optimizer, like RMSprop).
The parameters Θa, Θb∣a, Θb & Θa∣b of the maximum likelihood estimate will be used as the initial
parameters for the adaptation on the new transfer distribution.
16
Published as a conference paper at ICLR 2020
Similar to the way We defined the training distribution, We can define a transfer distribution (p in the
main text) as an intervention on the random variable A. In this experiment, this accounts for changing
the distribution of A, that is With a neW probability vector πA(2), also sampled from a uniform Dirichlet
distribution
π(2) 〜Dirichlet(IN)∙	(67)
To perform adaptation on the transfer distribution, We also sample a smaller transfer dataset Dint =
{at, bt}tT=1, With T m. In our experiment, We used T = 20 datapoints, folloWing the observation
from Section 2.1.
C.2 Discrete Variables with MLP parametrization
We consider a bivariate model, similar to the one defined in Appendix C.1, Where each random
variable is sampled from a categorical distribution. Instead of expressing the CPDs in tabular form,
We use structured CPDs, parametrized With multi-layer perceptrons (MLPs). In our experiment, all
the MLPs have only one hidden layer With H = 8 hidden units, With a ReLU non-linearity, and the
output layer has a softmax non-linearity. To avoid any modeling bias, We assume that the ground-truth
model is also parametrized by MLPs, such that
A 〜Categorical(MLP(0 ; WA))	(68)
B | A = a 〜Categorical(MLP(1[a]; WB))	(69)
Where 0 is a vector of size N Will all zeros, and 1[a] is a one-hot vector of size N. WA and WB
summarize the parameters of the ground truth model, With the Weights and biases for the 2 layers.
Similar to the tabular representation, We define 4 different modules, this time using MLPs. Their
definition, as Well as their corresponding parameters, are shoWn in Table C.2.
Table C.2: Description of the 2 models, With the parametrization of each module, for a bivariate
model With discrete random variables, and MLP parametrization. Model A → B and Model B → A
both have the same number of parameters 3NH + 2(N + H).
Distribution	Module	Parameters	Dimension
Model P(A) A → B P(B | A)	P(XA = i; Θa) = [MLP(0; θa)]i P(XB = j | XA ； θb∣a) = [MLP(I[xA] ； θB∣A)j	θA θB∣A	NH + H + N 2NH + H + N
Model P(B) B → A P(A | B )	P(XB = j ； θB ) = [MLP(0; Θb)j P(XA = i | XB ; Θa∣b) = [MLP(1[xb]; Θa∣b)]i	Θb θA∣B	NH + H + N 2NH + H + N
Again, to define the training distribution, We first fix the parameters WA(1) and WB . We use randomly
initialized netWorks for the training distribution, With the parameters sampled using the He initializa-
tion. We train all the modules using maximum likelihood on a large dataset of training samples Dobs,
to get the initial set of parameters for the adaptation on the transfer distribution.
We also define a transfer distribution as the result of an intervention on A. In this experiment, this
means sampling a neW set of parameters WA(2), still as a randomly initialized netWork. We sample a
transfer dataset Dint = {at, bt}tT=1, With T = 20 datapoints.
C.3 Continuous Multimodal Variables
Consider a family ofjoint distributions Pμ(A, B) over the causal variables A and B, defined by the
folloWing structural causal model (SCM):
A 〜Pμ (A) = N(μ, σ2 = 4)	(70)
B := f(A) + NB	NB 〜 N(0, 1),	(71)
Where f is a randomly generated spline, and the noise NB is sampled iid. from the unit Gaussian
distribution. To obtain the spline, We sample K points {xk }kK=1 uniformly spaced from the interval
[-8, 8], and another K points {yk}kK=1 uniformly randomly from the interval [-8, 8]. This yields K
17
Published as a conference paper at ICLR 2020
pairs {xk, yk}kK=1, which make the knots of a second-order spline. We choose K = 8 points in our
experiments.
The conditional distributions p(B | A) and p(A | B) are parametrized as 2-layer Mixture Den-
sity Networks (MDNs; Bishop, 1994), with 32 hidden units and 10 components. The marginal
distributions p(A) and p(B) are parametrized as Gaussian Mixture Models (GMMs), also with 10
components. The definition of the different modules, as well as their corresponding parameters, are
shown in Table C.3.
Table C.3: Description of the 2 models, with the parametrization of each module, for a bivariate
model with continuous multimodal variables. Model A → B and Model B → A both have the same
number of parameters 2,140.
Distribution	Module	Parameters	Dimension
Model p(A) A → B p(B | A)	p(xa ； Θa) = GMM(XA ； Θa) P(XB | XA ； Θb∣a) = MDN(XB ,XA ； Θb∣a)	θA θB∣A	30 2,110
Model p(B) B → A p(A | B )	p(xb ； Θb ) = GMM(XB ； Θb ) P(XA | XB ; Θa∣b) = MDN(XA, XB ; Θa∣b)	Θb θA∣B	30 2,110
We select p0 (A, B) as the training distribution, from which we sample a large dataset Dobs using
ancestral sampling. Similar to the earlier experiments, this dataset is used to get the initial set
of parameters for the adaptation on the transfer distribution. The MDNs are fitted with gradient
descent, while the GMMs are learned via Expectation Maximization. The transfer distribution is the
result of an intervention on A, where We shift the distribution Pμ(A) with μ sampled uniformly in
[-1,1]. In Figure C.1, we plot samples from the training distribution (μ = 0), as well as two transfer
distributions (μ = ±4).
Figure C.1: Samples from the training (blue) and transfer (red and green) distributions, from an SCM
generated with the procedure described above. The red datapoints are sampled from p-4(A, B), the
green datapoints from p4(A, B), and the blue datapoints from p0(A, B).
The structural regret R(γ) is now minimized with respect to γ for 500 iterations (updates of γ).
In the notation of Algorithm 1, these are the iterations over the number of episodes. Figure C.2
shows the evolution of σ(γ) as training progresses. This is expected, given that we expect the causal
model to perform better on the transfer distributions, i.e. we expect LA→B > LB→A in expectation.
Consequently, assigning a larger weight to LA→B optimizes the objective (see Proposition 3).
Finally as a sanity check, we test the experimental set-up described above on a linear SCM with
additive Gaussian noise. In this setting, it is well known that the causal structure cannot be discovered
from observations alone Peters et al. (2017) and one must rely on the transfer distribution tell cause
from effect.
To that end, we repeat the experiment in Figure C.2 with the following amendments: (a) we replace
the non-linear spline with a linear curve (Figure C.3), and (b) in addition to training the structural
18
Published as a conference paper at ICLR 2020
0.2-
Θ-4B)
0.0
0	100	200	300	400	500
Number of episodes
Figure C.2: Evolution of the sigmoid of the structural parameter σ(γ), with the number of episodes
(meta-training iterations). The belief of A → B being the correct causal model increases as the
number of episodes increases.
parameter by adapting the A → B and B → A models to multiple interventional distributions, we
train it by “adapting" the said model to the train distribution, where the latter serves as a baseline.
Figure C.4 shows that using multiple transfer (i.e. interventional) distributions (“With Interventions")
enables causal discovery, as opposed to the model trained with a single observational distribution.
This confirms that our method indeed relies on the interventional distributions to discover the causal
structure.
Figure C.3: Samples from a linear SCM, showing training (orange) and two transfer distributions
(blue and green).
C.4 Linear Gaussian Model
In this experiment, the two variables A and B are vector-valued, taking values in Rd . The ground-truth
causal model is given by
A 〜N(μ, Σ)	(72)
B ：= βιA + βo + NB	NB 〜N(0, ∑),	(73)
where μ ∈ Rd, βo ∈ Rd and βι ∈ Rd×d. Σ and Σ are two d X d covariance matrices. In our
experiment, d = 100. Once again, we want to identify the correct causal direction between A and B.
19
Published as a conference paper at ICLR 2020
Q.9.8.7.6.5,43
Iooooooo
>① 0ps+lUd① w】(Ab
100
0.2
0	20	40	60	80
Number of episodes
Figure C.4: Evolution of the sigmoid of the structural parameter σ(γ), with the number of episodes
(meta-training iterations) in case of a linear model with additive Gaussian noise. The blue curve
corresponds to the setting where we make use of interventions, whereas the orange curve corresponds
to one where we do not (i.e. use a single distribution). The shaded bands show the standard deviation
over 40 runs (of both pre- and meta-training). We find (as expected) that causal discovery fails
without interventions but succeeds when transfer distributions are available.
To do so, we consider two models A → B and B → A parametrized with Gaussian distributions.
The details of the modules’ definitions, as well as their parameters, is given in Table C.4. Note that
each covariance matrix is parametrized using the Cholesky decomposition.
Table C.4: Description of the 2 models, with the parametrization of each module, for a bivariate
model with linear Gaussian variables. Model A → B and Model B → A both have the same number
of parameters 2d2 + 3d.
Distribution	Module	Parameters	Dimension
Model p(A)	p(xA ； Θa) = N(XA | μA, ∑a)	μA, ςa	d(d + 1)/2 + d
A→ B p(B | A)	P(XB | XA ； Θb∣a) = N(XB | W1XA + Wθ, ∑B∣a)	W1,Wθ, ∑B∣A	3d(d + 1)/2
Model p(B)	P(XB ； Θb ) = N(XB | μB, ∑B )	μB, ∑b	d(d + 1)/2 + d
B→A p(A | B)	P(XA | XB ； Θa∣B) = N(XA | V1XB + %, ∑A∣B)	^IVl ∑A∣B	3d(d + 1)/2
To build the training distribution, We draw μ(1), βo and βι from a Gaussian distribution, and Σ(1)
and Σ from an inverse Wishart distribution. The transfer distribution is the result of an intervention
on A, meaning that the marginal P(A) changes. To do so, We sample new parameters μ(2) from a
Gaussian distribution, and Σ(2) from an inverse Wishart distribution as well.
Unlike the previous experiments, we are not conduction any pre-training on actual data from the
training distribution. Instead, we fix the parameters of both models to their exact values, according to
the ground truth distribution. For Model A → B, this can be done easily. For the Model B → A,
we compute the exact parameters analytically using Bayes rule. This can be seen as the maximum
likelihood estimate in the limit of infinite data. In Figure C.5, we show that, after 200 episodes, σ(γ)
converges to 1, indicating the success of the method on this particular task.
C.5 Experiments with Soft Intervention
In this section, we describe an experimental setting where the conditional p(B | A) is perturbed
while the distribution of the cause, p(A), is left unchanged. To that end, consider a set-up similar to
that in Section C.3:
20
Published as a conference paper at ICLR 2020
Figure C.5: Convergence of the causal belief (to the correct answer) as a function of the number of
meta-learning episodes, for the linear Gaussian experiments.
A 〜Pμ(A)= U(-8, 8)	(74)
B ：= fo(A) + NB	NB 〜N(0,1),	(75)
where f0 is a randomly generated spline and NB is sampled iid. from the unit Gaussian distribution
and the cause variable A is sampled from the uniform distribution supported on [-8, 8].
To induce soft-interventions, we modify the SCM as follows. Consider the knots {ai , bi}i5=1 of
the order 3 spline f0 ; we obtain a new spline fint by randomly perturbing the b-coordinate of the
knots, where the perturbations are sampled from another uniform distribution1. Using the perturbed
spline fint instead of f0 in Equation (74) results in a new SCM, from which we generate a single
transfer distribution (i.e. for a single episode). In Figure C.6 we plot samples from three such transfer
distributions.
-1	I	Γ
-5	0	5
A
Figure C.6: Samples from different training (blue) and transfer (orange) distributions, from SCMs
generated with the procedure described above, namely: all transfer SCMs (orange) are obtained by
soft-intervening of the underlying training SCM (blue).
The models used are identical to those detailed in Appendix C.3 and are trained on the training
SCM (corresponding base-spline f0) with a large amount of samples (≈ 3,000k). The meta-training
procedure differs in that (a) in every transfer episode, we create a new spline fint and sample a
transfer distribution Dint from the corresponding SCM, and (b) we use the following measure of
adaptation:
LG(Dint) =exp[logp(Dint | θG(T))-logp(Dint | θG(0))]	(76)
where G is one of A → B or B → A. The meta-transfer objective in Equation (5) remains the same.
Figure C.7 shows the evolution of the σ(γ) as the training progresses, and we find that the structural
parameter correctly converges to 1, representing the correct causal graph A → B .
1The scale of the perturbation is 0.5 times that of the original knots.
21
Published as a conference paper at ICLR 2020
O	20	40	60	80 IOO
Number of episodes
Figure C.7: Convergence of the causal belief (to the correct answer) as a function of the number of
meta-learning episodes, for the the experiments with soft interventions. The error band is over 5
different runs.
Failure case In addition to the result above, we also observed that using soft interventions on the
effect B instead on changes on the marginal p(A) was sometimes failing to recover the correct causal
graph. Instead, the anti-causal graph (here B → A) was found, with high confidence. We describe
here one such experiment where using the meta-transfer objective failed at recovering the correct
causal graph.
Our experimental setting is similar to the one described in Appendix C.1. However instead of
changing the marginal p(A), the conditional distribution p(B | A) changes and p(A) remains
unchanged. Following the notations in Appendix C.1, we have
∏a 〜Dirichlet(IN)	(77)
π(1)a 〜Dirichlet(IN) & ∏(2∖ 〜Dirichlet(IN)	∀a ∈ [1, N],	(78)
where ∏鼠 are the parameters of the conditional distribution before intervention, and ∏^a its
parameters after intervention. We again sample data from both the training and transfer distributions
to get datasets Dobs and Dint . The different modules and their corresponding parameters are defined
in Table C.1.
Figure C.8: Evolution of the belief that A → B is the correct causal model, as the number of episodes
increases, starting with an equal belief for both hypotheses, under soft interventions on the effect B .
In Figure C.8, we show the evolution of the structural parameter σ(γ), the model’s belief that A → B
is the correct causal model, as the number of episodes increases. Unlike our previous experiments in
Section 3.3, the structural parameter now converges to σ(γ) → 0, corresponding to a strong belief
that the model is B → A. We are therefore unable to recover the correct causal graph here under the
22
Published as a conference paper at ICLR 2020
assumption that p(B | A) changes. Note that here the parameter counting argument from Section 2.2
clearly does not hold anymore, since the modules all use a tabular representation, and both models
require the same order O(N 2) of updates to adapt to a transfer distribution.
D Results on Representation Learning
The true latent causal variables (A, B) are sampled from the distribution described in Appendix C.3
(Equations (74) & (75)). These variables are then mapped to observations (X, Y)〜pμ (X, Y) via
a hidden (and unknown to the learner) decoder D = RθD , where Rθ is a rotation of angle θ. The
observations are then mapped to the hidden state (U, V)〜Pμ(U, V) via the encoder E = Rθe ; in
this experiment, the angle θE is the only additional meta-parameter, besides the structural parameter
Y. The computational graph is depicted in Figure 3. In our experiment, Θd = -n/4 is fixed for
all our observation and intervention datasets. Interventional data is acquired by intervening on the
latent variables (A, B), following the process described in Appendix C.3, and then mapping the data
through the decoder D .
Since the underlying latent causal variables (A, B) are unobserved, we need to define the online
likelihood over the recovered variables (U, V) instead. Analogous to how we defined the online
likelihood in the fully observable case in Section 3, this is defined as
L (D ,	θ ∖	_ YY (R (	、.	θ(t) G) θ(1)	=	θML(RθE (Dobs))	(79、
LG(Dint;	θE)	= ∏PSXM	θG ,G)	θGt+1)	=	θG)+ αVθ logp(RθE (χt);	θ(G),G),	(79)
where RθE (Dobs) = {RθE (x) | x ∈ Dobs}. Note that here the online likelihood depends on the
parameters of the encoder E (here, θE). Using this definition of the online likelihood that takes into
account the encoder, the meta-transfer objective is also similar to the one defined in Equation (5):
R(Dint; γ,θE) = - log [σ(γ)LU →V (Dint; θE) + (1 - σ(γ))LV →U (Dint; θE)] .	(80)
On the one hand, the gradient of R(Dint; γ, θE) with respect to the structural parameter γ can be
computed using Proposition 2, similar to the fully observable case. On the other hand, the gradient of
the meta-transfer objective with respect to the meta-parameter θE is computed using backpropagation
through the T updates of the parameters θG of the modules in Equation (79); this process is similar
to backpropagation through time. In our experiment, we did not observe any degenerate behaviour
like vanishing gradients, due to the limited amount of interventional data (T = 5).
E More than Two Causal Hypotheses
In Section 3.2, we defined the meta-transfer objective only in the context of bivariate models. The
challenge with learning the structure of graphs on n variables is that there is a super-exponential
number of DAGs on n variables, making the problem of structure learning NP-hard (Chickering,
2002a). If we were to naively extend the meta-transfer objective to graphs on n > 2 variables, this
would require adaptation of 2OGn2 ) different models (hypotheses), which is intractable.
Instead, we can decouple the optimization of the graph from the acyclicity constraint, since causal
graphs can have cycles (Peters et al., 2017). This constraint can be enforced as an extra penalty to the
meta-transfer objective (Zheng et al., 2018). We consider the problem of optimization on the graph as
O(n2) independent binary decisions on whether Vj is a parent (or direct cause) of Vi. Motivated by
the mechanism independence assumption (Parascandolo et al., 2017), we propose a heuristic to learn
the causal graph, in which we independently parametrize the binary probability Pij that Vj is a parent
of Vi . We can then define a distribution over graphs (or more precisely, their adjacency matrix B) as:
Bij 〜Bernoulli(Pij)	(81)
P(B) = Y P(Bij ),	(82)
i,j
where Pij = σ(γij). We denote PaB(Vi) as the parent set of Vi in the graph defined by the adjacency
matrix B (that is the nodes Vj such that Bij = 1). We can slightly rewrite the definition of the
23
Published as a conference paper at ICLR 2020
online-likelihood as in Section 3 to show the dependence on B :
T	nT
LB(Dint) = ∏p(Xt ； θB),B) = ∏ ∏p(Xit) I XPtaB(Vi) ； θB)i),	(83)
t=1	i=1 t=1
where the second equality uses the factorization ofp in the graph defined by B. Note that since the
graph defined by B can contain cycles, the definition in Equation (83) involves the pseudolikelihood
instead of the joint likelihood (which is defined as the product of individual conditional distributions
only if the graph is a DAG). The pseudolikelihood was shown to be a reasonable approximation of the
true joint likelihood when maximizing the joint likelihood (which is performed here for adaptation;
Koller & Friedman (2009)). Similar to the bivariate case, we want to consider a mixture over all
possible graph structures, but where each component must explain the whole adaptation sequence.
We can generalize our definition of the regret as
R(Dint) = - log EB [LB (Dint)].	(84)
Note, however, that this expectation is over the O(2n2) possible values of B, which is intractable.
We can rewrite the regret in a more convenient form:
Proposition 4. The regret R(Dint) defined in Equation (84) can be decomposed as
n
R(Dint) = -	log EBi [LBi (Dint)],	(85)
i=1
where Bi is a row of the matrix B, and LBi (Dint) appears in the factorization of LB (Dint) in
Equation (83):
T
LBi = Y p(xi(t) |
XPaB(Vi) ； θB,i)	(86)
t=1
Proof. Recall that LB(Dint) = i LBi (Dint), so that we can rewrite the regret as follows:
R(Dint) = -logEB[LB(Dint)]	(87)
= - log Xp(B)LB (Dint)	(88)
B
= - log XX...X Yn
p(Bi)LBi (Dint)	(89)
= -logYn	Xp(Bi)LBi (Dint)	(90)
i=1 Bi
n
= - log	p(Bi)LBi(Dint)	(91)
i=1	Bi
n
= -	log EBi [LBi (Dint)]	(92)
i=1
The structural parameters, here, are the O(n2) scalars γij . Regardless of the intractability of the
regret, we can still derive its gradient wrt. each γij . The following proposition provides a direct
extension of Proposition 2 to the case of multiple variables:
Proposition 5. The gradient of the regret R(Dint) wrt. the structural parameter γij is given by
σ(γij ) - σ(γij + ∆ij ),	(93)
where ∆ij is the difference in log-likelihoods of two mixture candidates, conditioning on the variable
Vj
∆ij =log(EBi[LBi(Dint) | Vj ∈ PaB(Vi)])-log(EBi[LBi(Dint) | Vj ∈/ PaB(Vi)])	(94)
∂R
dYij
24
Published as a conference paper at ICLR 2020
Proof. To simplify the notation, we remove the explicit dependence on the transfer distribution Dint
in this proof. Recall from Proposition 4 that the regret can be written as
n
R=-XlogEBi[LBi].	(95)
i=1
Using a conditional expectation, it follows that for any i, j
EBi[LBi] =	p(Bi)LBi	(96)
Bi
=P(Bij = D ∙ X P(Bi | Bij = I)LBi + P(Bij=0 ∙ X P(Bi | Bij = O)LBi
Bi∣Vj ∈PaB (Vi)	Bi∖Vj∈PaB (Vi)
(97)
=σ(Yij) ∙ X P(Bi | Bij = I)LBi + (I- σ(Yij)) ∙ X P(Bi | Bij= O)LBi (98)
Bi∖Vj ∈PaB (Vi)	Bi∖Vj∈PaB (Vi)
To simplify the notation, let us define Ei(j1) and Ei(j0) the two conditional expectations of LBi ,
conditioned on whether or not Vj is a parent of Vi in B
EiJ)= E	P(Bi	I	Bij	= O)LBi=	EBi[LBi	I	Vj	∈ PaB(Vi)]	(99)
Bi∖Vj/PaB (Vi)
Ei(j1) = X	P(Bi	I	Bij	= 1)LBi	=	EBi[LBi	I	Vj	∈ PaB(Vi)],	(100)
Bi∖Vj∈PaB(Vi)
so that Equation (98) can be written as
EBi[LBi] =σ(Yij)Ei(j1)+(1-σ(Yij))Ei(j0).	(101)
Note that neither Ei(j0) nor Ei(j1) depend on the structural parameter Yij. Therefore we can now easily
compute the gradient of R wrt. Yij only
∂γR = - ∂γij log (σ(γij )ES) + (1- σ (Yij ))Ei?))	(i02)
=-EB⅛i (σ(Yij)(1-σ(Yij))hEijI)- Ei(j0)i)	(103)
IfWe substract σ(Yij) from this expression gives us
σ(Yij) - ∂Yj = EBiICBi] (σ(γij)2Ej) + σ(Yij)(1 - σ(YijXEj
+σ(Yij)(1-σ(Yij)) hEi(j1) -Ei(j0)i	(104)
=EB⅛ EijI) = X	(105)
Denoting the previous expression as x, we can also easily compute 1 - x:
I r ― 1 - σ(Yij)
1 X ----- r 八 -l
EBi[LBi]
(106)
Using the logit function σ-1(x) = log ɪ-X, We can conclude that
σ-1	IG 、 ∂R∖ -1	σ(YijMI (σ(Yij) - ∂h~- J - log X	(一而西	(I07) ∂Yij	(1 - σ(Yij ))Eij =log Fjʒ + log EijL)- log EijO)	(108) 1 - σ(Yij )	ij	ij = Yij + ∆ij	(109)
25
Published as a conference paper at ICLR 2020
While Proposition 5 gives an analytic form for the gradient of the regret wrt. the structural parameters,
computing it is still intractable, due to ∆ij . However, we can still get an effecient stochastic gradient
estimator from Proposition 4, which can be computed separately for each node of the graph (with
samples arising only out of Bi , the incoming edges of Vi ):
Proposition 6. Ifwe consider multiple samples of B in parallel, a biased but asymptotically unbiased
(as the number K of these samples B(k) increases to infinity) estimator of the gradient of the overall
regret with respect to the meta parameters can be defined as:
_ Pk(σ(Yij) - BaLB
gij = -PkLI-,
where the index (k) indicates the values obtained for the k-th draw of B.
Proof. The gradient of the regret with respect to the meta-parameters γi of node i is
dR_	PBiP(Bi)LBi d⅛萼
---——---------------:--:-------
∂γi	PBi P(BiILBi
EBi [LBi d⅛B1 ]
-------------:---:-----
EBi [LBi]
Note that with the sigmoidal parametrization ofp(Bi),
logp(Bi) = Bij logσ(γij) + (1 - Bij) log(1 - σ(γij))
as in the cross-entropy loss. Its gradient can similarly be simplified to
(110)
(111)
(112)
∂ log P(Bij)
dYij
σ⅛σ(γijMjej)) - (i1--σBj))σ(γijMjej)))
= Bij - σ(γij).	(113)
A biased, but asymptotically unbiased, estimator of ∂R∕∂γj is thus obtained by sampling K graphs
(over which the means below are run):
L(k)
gij = ΣS(σ(γj)-Bjp Bi(k0)	(114)
k	k0 LBi
where index (k) indicates the k-th draw of B, and we obtain a weighted sum of the individual binomial
gradients weighted by the relative regret of each draw Bi(k) of Bi, leading to Equation (110).
We can therefore adapt Algorithm 1 using the gradient estimate in Proposition 6 to update the
structural parameters γij, without having to explicitly compute the full regret R(Dint). In addition
to the gradient estimate provided by Proposition 6, we can also derive a Rao-Blackwellized (Rao,
1992; Blackwell, 1947) estimate of the gradient of the regret, based on the formulation derived in
Proposition 5.
Proposition 7. Let {B(k)}kK=1 be K binary matrices (corresponding to sample graphs), sampled
from independent Bernoulli distributions depending on the structural parameters γij
Bj 吧 BernouUi(σ(γj)),	(115)
and their corresponding likelihoods L(Bk). A Monte-Carlo estimate of the log-likelihood difference
∆ij in Equation (94) is given by
δ (K)=log 3 kXi)LBk) - log 3 kXo)LBk))	(116)
where Ki(j0) = {k ; Bi(jk) = 0} and Ki(j1) = {k ; Bi(jk) = i} are (disjoint) sets of indices k, depending
on the value of Bi(jk) .
Based on this Monte-Carlo estimate of ∆ij, we can define an estimate of the gradient of the regret R
wrt. the structural parameter γij by
ʃ-——-
∂R
=-=σ(γij) - σ(γij + △ ij )∙	(117)
∂γij
26
Published as a conference paper at ICLR 2020
F The effect of adaptation on the online likelihood
Since we are using the online likelihood defined in Equation (3) as a measure of adaptation in our
meta-transfer objective, it is reasonable to know if this measure is sound. To validate this assumption,
we are running an experiment similar to the one described in Section 2.1 and Figure 1, using the same
experimental setup on discrete variables described in Section 3.3. However, instead of measuring the
raw log likelihood on a validation set, we report the online likelihood LG(Dint) for both models in
Figure F.1. The online likelihoods are scaled by the number of transfer examples seen for visualization.
Similar to Figure 1, we can see that the difference in online likelihoods for both models is most
significant on a small amount of data.
(Q)t7 pooɪrnə官 əuɪɪuo
-3.75
-5.00
-5.25
-5.50
A → B
---B → A
100	101	102	103
Number of examples
104
Figure F.1: Adaptation to the transfer distribution (online likelihood on transfer examples, vertical
axis), as more transfer examples are seen by the learner (horizontal axis). The curves are the median
over 20,000 runs, with their 25-75th quantiles intervals. The dotted line is the asymptotic online
likelihood
27