Published as a conference paper at ICLR 2020
Learning-Augmented Data Stream Algo-
RITHMS
Yi Li
School of Physical and Mathematical Sciences
Nanyang Technological University
Singapore 637371
yili@ntu.edu.sg
Yisong Ruan
Department of Software engineering
Xiamen University
Xiamen, Fujian, China 361000
24320152202802@stu.edu.xmu.cn
Tanqiu Jiang
Department of Electrical and Computer Engineering
Lehigh University
Bethlehem, PA 18015, USA
taj320@lehigh.edu
Honghao Lin
Zhiyuan College
Shanghai Jiao Tong University
Shanghai, China 200240
honghao_lin@sjtu.edu.cn
David P. Woodruff
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dwoodruf@cs.cmu.edu
Ab stract
The data stream model is a fundamental model for processing massive data sets
with limited memory and fast processing time. Recently Hsu et al. (2019) incor-
porated machine learning techniques into the data stream model in order to learn
relevant patterns in the input data. Such techniques led to the training of an oracle
to predict item frequencies in the streaming model. In this paper we explore the
full power of such an oracle, showing that it can be applied to a wide array of
problems in data streams, sometimes resulting in the first optimal bounds for such
problems, and sometimes bypassing known lower bounds without such an oracle.
We apply the oracle to counting distinct elements on the difference of streams,
estimating frequency moments, estimating cascaded aggregates, and estimating
moments of geometric data streams. For the distinct elements problem, we bypass
the known lower bound and obtain a new space-optimal algorithm. For estimating
the p-th frequency moment for 0 < p < 2 we obtain the first algorithms with
optimal space and update time. For estimating the p-th frequency moment for
p > 2 we obtain a quadratic savings in memory, bypassing known lower bounds
without an oracle. We empirically validate our results, demonstrating also our
improvements in practice.
1 Introduction
Processing data streams has been an active research field in the past two decades. This is motivated
by the increasingly common scenario where the size of the data far exceeds the size of the available
storage, and the only feasible access to the data is to make a single or afew passes over the data. This
situation occurs, for example, in network applications with high-speed packets being sent through
routers with limited resources. Other examples include processing internet search logs, sensor net-
works, and scientific data streams. For an introduction to data streams, see, e.g., Muthukrishnan
(2005).
Formally, in the data stream model, we assume there is an underlying frequency vector x ∈ Zn ,
initialized to 0n, which evolves throughout the course of a stream. The stream consists of updates of
the form (i, w), meaning Xi J Xi + w. Suppose M is such that ∣∣x∣∣∞ = maxi∈{i,…,n} |xi| ≤ M
1
Published as a conference paper at ICLR 2020
throughout the stream. At the end of the stream, we are asked to approximate f(x) for a function
f : Zn → R. Because of the sheer size of the stream, most algorithms are approximate and
randomized. Typically, such algorithms output an estimate Z for which Pr{(1 - )f (x) ≤ Z ≤
(1 + )f (x)} ≥ 2/3, where the probability is over the randomness used by the algorithm, and not
of the input stream, which may be a worst-case stream. The success probability 2/3 can be replaced
with any constant greater than 1/2 by independently repeating the algorithm a constant number of
times and taking the median estimate. The space complexity of the algorithm is measured in bits
and the goal is to use much less than the trivial n bits of space required to store x.
A common difficulty in designing algorithms in the data stream model is that the coordinates i for
which |xi | is large, i.e., the heavy hitters, are not known in advance. Such coordinates usually
require very accurate estimates in order to achieve an accurate final estimate of the function value.
A number of techniques have emerged for identifying such coordinates, such as the Count-Min
and CountSketch. These techniques are often used at multiple scales by subsampling the input
coordinates and finding heavy items in the subsample, and have found applications to estimating
frequency moments, cascaded aggregates, earthmover distance, and empirical entropy, among other
statistics (Indyk & Woodruff, 2005; Chakrabarti et al., 2006; Jayram & Woodruff, 2009; Andoni
et al., 2009; McGregor et al., 2016).
In many applications, however, the worst-case input is rarely seen; in fact, there is usually a pattern
inside the underlying vector. For instance, in network traffic monitoring, the large or “elephant”
flows fluctuate on a daily basis, but the actual source and destination servers of those flows do not
vary much. In our formulation, this implies the value xi fluctuates while whether or not xi is a
heavy coordinate remains stable. Motivated by this as well as related work on learning oracles for
algorithm design (Gupta & Roughgarden, 2016; Dick et al., 2017; Balcan et al., 2018a;b), Hsu et al.
(2019) introduced the notion of a heavy hitter oracle into the data stream model. The heavy hitter
oracle, trained on past data, receives a coordinate index i and returns a prediction of whether xi will
be heavy or not. They showed that such an oracle simplifies classical heavy hitter algorithms such
as Count-Min and Count-Sketch. We note that similar oracles were also studied in previous
work, e.g., membership oracles for Bloom filters by Kraska et al. (2017). These works demonstrate
the feasibility of creating such an oracle using machine learning techniques. Recently, learned or-
acles for low rank approximation, which can be viewed as an analogue of heavy hitters for heavy
directions, was explored by Indyk et al. (2019).
The work of Hsu et al. (2019) is the starting point of our work. While Hsu et al. (2019) showed
applications of the heavy hitter oracle to frequency estimation, the full power of such an oracle
remained largely unexplored. Our main goal is to understand if such an oracle is more broadly
useful, and whether it can give memory and time improvements to the large number of data stream
problems mentioned above. Accurately estimating the heavy hitters in a vector is the bottleneck of
existing algorithms for numerous data stream problems, and one cannot help but ask:
Can a heavy hitter oracle be applied to a large number of problems in the data stream model to
obtain significantly improved bounds for these problems?
We consider estimating the following common functions f (x) in the data stream model:
•	Distinct Elements: f(x) = kxk0, where kxk0 is the number of nonzero coordinates ofx, defined
as kxk0 = |{i : xi 6= 0}|. This quantity is useful for detecing denial of service attacks and
database query optimization (Kane et al., 2010b).
•	the Fp-Moment: f(x) = kxkpp, where kxkp is the usual `p-norm of x, defined as kxkp =
(Pi|xi|p)1/p. For0 < p < 2, these are often more robust than the Euclidean norm (Indyk,
2000). For p > 2 these are used to estimate skewness and kurtosis (Indyk & Woodruff, 2005).
•	the (k, p)-Cascaded Norm: in this problem x is an n × d matrix that receives updates to its in-
dividual entries, and f(x) = kxkk,p := (Pi(Pj |xij |p)k/p)1/k. These are useful for executing
multiple databse queries (Cormode & Muthukrishnan, 2005; Jayram & Woodruff, 2009).
These problems have also been studied for non-numerical data, such as geometric data. For example,
the Fp -moment problem was studied on a stream of rectangles in Tirthapura & Woodruff (2012).
One can think of an underlying ∆d-dimensional vector x, identified with a d-dimensional rectangle
{1, . . . , ∆}d. Without loss of generality, we assume that ∆ is a power of 2. Each item in the stream
is a pair of the form (R, w), where R is a d-dimensional rectangle and w is an integer, meaning
2
Published as a conference paper at ICLR 2020
Problem	ReSUIt TyPe	Previous Result	This work
Distinct Elements	S	O(* log n(log 看 + log log n)) Kaneet al.(2010b)	O(表 log n log 看) Theorem 3
Fp Moment (p > 2)	S	<e(n1-2∕p) e.g., Andoni et al. (2011)	O(n1∕2T∕p) Theorem 7
Fp Moment Fast Update (p < 2)	Update T Reporting T	amortized O(log2 ɪ log log ɪ) O(表 log2 ɪ log log ɪ) Kane etal.(2011)	expected amortized O(1) O(表) Theorem 17
Cascaded Norm (k ≥ P ≥ 2)	S	O(n1-2/k d1-2∕p) Jayram & Woodruff (2009)	(e(n1-1/k-P/(2k)d1/2-1/P) Corollary 20
Rectangle Fp Moment (p > 2)	S	O(∆d(1-2∕p) poly(d)) Tirthapura & Woodruff (2012)	O0d(1/2-1/p) poly( d )) Theorem 23	'
Table 1: Summary of previous results and the results obtained in this work. We assume that m, M =
poly(n). In the column of result type, S denotes space complexity and T denotes time complexity.
We view as a constant for the listed results of the Fp Moment and Cascaded Norm problems.
Problem	Heavy Hitter Oracle
Distinct Elements	|xi| ≥ 2poly(1/e)
Fp Moment (p > 2)	∣χi∣p ≥ √nkχkp
Fp Moment Fast Update (p < 2)	|xi|p ≥ e2kxkp 2 |xi|p ≥ logz(l/e)logl,g(l/e) kxkp
Cascaded Norm (k ≥ P ≥ 2)	∣Xi∣p ≥ k'x'kp∕(d1 n1-2k)
Rectangle FP Moment (p > 2)	∣Xi∣p ≥ 向IP∕∆d∕2
Table 2: Summary of the threshold of the heavy hitter oracles used in each problem. Note that two
oracles are used for the Fp Moment Fast Update problem.
Xi J Xi + W for all points i ∈ R. The goal is to estimate f (x) = Ilxkp. This is called the Rectangle
Fp-Moment problem, which occurs in spatial data streams and constraint databases.
Our Results We show that a heavy hitter oracle can greatly improve the complexity of a wide
array of commonly studied problems in the data stream model, leading to the first optimal bounds
for several important problems, and shattering lower bounds that have stood in the way of making
further progress on important problems. We note that not only do we give new algorithms, we
also give several lower bounds for algorithms equipped with the oracle, that show optimality of
our algorithms even with an oracle. Our algorithms not only give practically better, theoretically-
grounded improvements to these problems with an oracle, they also shed light on what exactly the
difficulties are of making further progress on data stream problems without an oracle. We consider
both perfect oracles and oracles that may sometimes make mistakes.
We summarize in Table 1 our improvements to existing algorithms using appropriate heavy hit-
ter oracles, whose conditions are summarized in Table 2. Throughout, we make the conventional
assumption that m, M = poly(n).
Distinct Elements: For the problem of estimating kXk0 , we improve the prior memory bound of
O(表 log n(log ɪ + log log n)) to O(± log n log ɪ). For constant e, this gives an optimal O(log n)
bits of memory, breaking a recent Ω(log n log log n) bit lower bound shown in Woodruff & Yang
(2019). Examining the lower bound argument in Woodruff & Yang (2019), the hard instance is
precisely when there are many items of large frequency, namely, the product of the first small number
of primes. This prevents hashing-based algorithms, such as the one in Kane et al. (2010b), from
working with O(log n) bits of space, since hash values are typically taken modulo a small prime
and such heavy items necessitate the use ofa large enough prime. Surprisingly, we show that with a
heavy hitter oracle, we can separately handle such items and thus bypass this lower bound. We note
that our O(log n)-bit algorithm is optimal even given a heavy hitter oracle; see the Ω(log n) lower
bound in Alon et al. (1999), which holds even if all item frequencies are in the set {0, 1, 2}.
3
Published as a conference paper at ICLR 2020
Fp -Moment Estimation, 0 < p < 2: There is a long line of work on this problem with the best
known bounds given in Kane et al. (2011), which achieve an optimal O(-2 log n) bits of space,
and O(log2 (1/) log log 1/) time to process each element. The existing algorithm is based on
separately handling those elements i for which |xi|p ≥ 2kxkpp, i.e., the heavy hitters, from the
remaining “light items”. Although we can simply store the heavy hitters given a heavy hitter oracle,
unfortunately, the large O(log2 (1/) log log 1/) processing time in the algorithm of Kane et al.
(2011) also comes from running a fast multipoint evaluation on the light elements.
We observe that if we instead train two oracles, one for detecting heavy hitters, and one for de-
2
tecting items Xi with |x/p ≥ 啥⑴^)以iog(i/e)l∣xkp, then We can also separate out “medium
items”, i.e., items that are detected with this oracle but not our heavy hitter oracle. There are only
O(-2 log2(1/) log log(1/)) medium items and so assuming logn ≥ poly(log(1/)), one could
perfectly hash all such items and maintain their hashed identities in the optimal O(-2 log n) bit of
space. However, one cannot maintain their counts, since each count is O(log n) bits, and approxi-
mate counts do not work if the stream can increment or decrement the coordinate values xi . This
is too much memory, as it would lead to O(-2 (log n) log2(1/) log log(1/)) bits of space, even
worse than without the oracle. We could also maintain a so-called p-stable random variable for each
medium item, and use it to estimate the Fp-moment of the medium items, but this would again be
O(-2(log n) log2 (1/) log log(1/)) bits of space to store one random variable per medium item.
We instead observe that most of the p-stable random variables are small, and so can be stored with
many fewer than log n bits, and therefore globally we can get by with only O(-2 log n) bits in total
for estimating the medium items. Finally, for the light items, the critical observation is that we can
sub-sample a iog2(i/e)i0g iog(i/e)fraction of them, and still accurately estimate their contribution.
But this means we can run the multipoint evaluation algorithm of Kane et al. (2011) on such items,
since while that has update time O(log2 (1/) log log(1/)), on average it is only executed on one
out of every log2(1/) log log(1/) items.
In summary, with the aid of our oracles and by separately handling these three types of items, we
are able to achieve the optimal O(1) amount of update time per item, while retaining the optimal
O(-2 log n) bits of memory.
Fp-Moment Estimation for p > 2: It is well-known (Bar-Yossef et al., 2004) that a hard input
distribution for this problem is when a random coordinate ofx has absolute value n1/p, and all other
coordinates have value 0 or 1. In this case, one needs to certify that there is indeed a coordinate
of value n1/p, as it contributes a constant fraction of Ilxkp. There is a known Ω(n1-2∕p) bits of
space lower bound for this. However, a heavy hitter oracle exactly allows us to overcome this,
as it can be used to separately identify such items. Our idea is to separate out every item i for
which ∣xi∣p ≥ n11∕2 ∣∣xkp with the oracle, and run an existing Fp-estimation algorithm on such
items. Since there are at most n1/2 items, existing algorithms give (n1/2)1-2/p poly(log n) =
n1/2-1/p poly(log n) bits of memory for this problem. For the remaining light elements, one can
show that if one subsamples only a 1/n1/2 fraction of them and then runs an existing Fp-estimation
algorithm on them, one can use this to accurately estimate their contribution. This again takes
n1/2-1/p poly(log n) bits of memory, balancing the memory with the other part of our algorithm
and giving a quadratic improvement in memory in the case without a heavy hitter oracle. Overall
our algorithm uses O(n1/2-1/p poly(log n)) bits with an oracle and we can in fact show that this is
tight up to logarithmic factors as there exists a lower bound of Ω(n1∕2-1∕p) bits provided the same
oracle.
We describe our algorithms for cascaded norms and rectangle Fp -moments in the appendix.
We conduct experiments for the distinct elements and the Fp moment (p > 2) problems, on both
real-world and synthetic data, which demonstrate significant practical benefits.
2	Preliminaries
Notation We let [n] denote the set {1, . . . , n}. The function lsb(x) denotes the index of the least
significant bit of a non-negative integer x.
4
Published as a conference paper at ICLR 2020
Space and Time Complexity For streaming algorithms, the efficiency is characterized by the space
(memory) complexity and the time complexity. The time is further divided into update time and re-
porting time. A data stream algorithm maintains a data structure D while processing the data stream
and at the end of the stream, it outputs an approximation Z to f(x). The space complexity refers to
the size, in bits, of D. The update time refers to the time spent to update the data structure D upon
receiving a single stream update. The reporting time refers to the time to output the approximation
Z at the end of the stream.
Heavy Hitter Oracle We assume access to a heavy hitter oracle, which receives an input i ∈ [n],
and outputs whether xi will be a heavy hitter at the end of the stream. The definition of heavy hitter
varies for different data stream problems. There are two kinds of oracles. The first kind indicates
whether or not ∣x∕ ≥ T and the second kind indicates whether or not |x/p ≥ T Ilxkp, where T is
a pre-determined threshold associated with the problem. We shall use an oracle of the first kind for
the distinct elements problem and oracles of the second kind for all other problems.
In the experiments, the heavy hitter oracle receives an i ∈ [n] and outputs the predicted value of
xi . The implementation of the first kind of oracle is thus straightforward. For the second kind of
oracle, there are at most T coordinates χ satisfying ∣x∕p ≥ T1 ∣∣xkp. We shall regard the largest T
coordinates in the predicted vector x as the heavy hitters in our implementation (instead of calculat-
ing the kxkpp of the predicted vector x), which is in conformity with the implementation in the prior
experiments of Hsu et al. (2019).
3	Distinct Elements
The current best algorithm for estimating L0 = kxk0 , in the presence of additions and deletions to
coordinates of x, is due to Kane et al. (2010b). A key component of their algorithm is a constant-
factor approximation algorithm, called ROUGHL0ESTIMATOR, which returns a value L0 satisfying
IIO Lo ≤ Lo ≤ Lo with probability at least 2/3. The space is O (log n log log(mM)) bits and the UP-
date and reporting times are O(1). In this section, we improve the space of ROUGHL0ESTIMATOR
to O(log n) bits with the help of a trained oracle. All proofs are postponed to Section A.
The algorithm first picks a pairwise hash fUnction h : [n] → [n] and sUbsamples n coordinates at
log n scales, where the j-th scale consists of the coordinates i for which lsb(h(i)) = j. In each
scale, it Uses a small-space coUnting algorithm ExactCount to obtain, with probability at least
1 - η, the exact valUe of kxko at that scale. Then it finds the deepest scale, i.e., the largest j, with
kxko above some fixed constant threshold, and retUrns Lo = 2j as the estimate. If sUch aj does not
exist, it retUrns Leo = 1.
The EXACTCOUNT procedUre in Kane et al. (2010b) Uses a hash fUnction h0 : [n] → [B], where
B = Θ(c2) for some constant c for which h0 is a perfect hashing of a set of size at most c. Each
bucket' ∈ [B] is a counter which keeps track of (Ph(i)=' xi) mod P for some large prime p. The
estimate is the nUmber of nonzero bUckets, and ExactCount retUrns the maximUm estimate of
Θ(log(1∕η)) parallel repetitions.
Now, with the trained oracle, we can pick p tobe a smaller poly(1/) value in EXACTCOUNT. In the
higher-level RoughL0Estimator algorithm, whenever an update is made to a heavy coordinate
i identified by the oracle, the corresponding bucket inside EXACTCOUNT is marked as nonempty
regardless of the counter value, since we know the heavy item will never be entirely deleted, since
by definition it is heavy at the end of the stream (note this is not true of non-heavy elements). In the
end, RoughL0Estimator finds the deepest scale with at least one nonempty bucket.
The following is our new guarantee of ExactCount.
Lemma 1. Assume that Lo ≤ c and suppose that B = Θ(c2). The subroutine EXACTCOUNT
returns the exact value of Lo with probability at least 1 - η using O(B log(1/)) bits of space, in
addition to storing O(log(1∕η)) independently Chosen pairwise-independent hash functions from
[n] to [B]. The update and reporting times are O(1).
Now we specify some details of the algorithm ROUGHTL0ESTIMATOR. It chooses c = 141 and
η = 1/16 for EXACTCOUNT, and the EXACTCOUNT algorithms for different scales all share the
same O(log(1∕η)) hash functions. The following is our new guarantee.
5
Published as a conference paper at ICLR 2020
Theorem 2. The algorithm ROUGHL0ESTIMATOR outputs a value Lo, which satisfies 击Lo ≤
Lo ≤ Lo with probability at least 13/16. The space complexity is O(log(n) log(1/)) bits, and the
update and reporting times are O(1).
With a constant-factor estimate R to Lo, the high-level idea is to subsample each index in [n] with
probability K/R. Then the number of distinct surviving items is Θ(K). To estimate the number
of distinct surviving items efficiently, one hashes them into Θ(K) buckets and counts the number
T of non-empty buckets, which is simulated by updating a bit vector of length Θ(K). The final
estimate can be deduced from the number of distinct surviving items, which can be shown to be a
(1 + eps)-approximation to F0 when K = Θ(1∕e2). Since We do not know Lo in advance, We have
to guess its value using log n geometrically increasing values, and so our memory can be thought
of as a bucket matrix A of dimension (log n) × K. In parallel we run the ROUGHL0ESTIMATOR
above, so at the end of the stream, we will obtain a constant-factor estimate R, allowing us to look
into the appropriate row of A (the appropriate buckets). A problem is how to efficiently maintain a
bucket. In Kane et al. (2010b), each bucket stores the dot product of frequencies hashing to it with a
random vector over a large finite field of order poly(K log(nM)), using O(log K+log log(nM)) =
O(log(1/) + loglog(nM)) bits. Now, using a similar argument to the proof of Lemma 1, with the
help of our heavy hitter oracle, we can mark the buckets containing a heavy hitter as nonempty and
reduce the order of the finite field to O(poly(1/)). This improves the space complexity of each
bucket to O(log(1/)) bits. Combined with the complexity of ROUGHL0ESTIMATOR, we conclude
with the following theorem.
Theorem 3. There isan algorithm for (1±)-approximating Lo using space O(-2 (log n) log(1/))
with success probability at least 2/3, and with O(1) update and reporting times.
4	Fp ESTIMATION, p > 2
We start with a well-known algorithm for estimating kxkpp in a stream.
Lemma 4 (Precision Sampling, Andoni et al. (2011)). There is a randomized algorithm, called
the Precision Sampling algorithm, which returns an estimate X to kxkpp such that it holds with
probability at least 0.9 that (1 - )kxkpp ≤ X ≤ (1 + )kxkpp. The algorithm uses B(n) =
O(p2-2-4/pn1-2/p log(n) log(M)) bits of space.
In the rest of the section, we assume that there is an oracle which can tell Us if ∣Xi∣p ≥ S ∣∣χkp is a
heavy hitter. The main idea is that we separately estimate the Fp-moment of the heavy hitters, and
for the remaining light elements, we use sub-sampling to estimate their contribution with sampling
rate 1∕ρ.
Let IH , IL ⊆ [n] denote the subsets of indices of the heavy hitters and the light elements, respec-
tively. Since |IH| ≤ s, we can use an Fp estimator to (1 + )-approximate ∣xIH ∣pp with space
B(s). For the light elements, we use a sub-sampling algorithm: we set a sampling rate 1∕ρ, and
for each item i ∈ Il, with probability 1/p we choose it. For each item i we choose, we calculate
Fs = P |xi|p, and use ρFs to estimation ∣xIL ∣pp. By a Chernoff bound, with high probability, we
sample at most 3n∕ρ elements.
To analyze it, we let Y = ρFS, and we have the following lemmas. All proofs in this section are
postponed to Section B.
Lemma 5. E[Y] = IlxILIlp, Var[Y] ≤ PIlxIlpp.
Lemma 6. Ifwe repeat the sub-sampling kρ∕s times independently with sub-sampling rate ρe2, we
can get a (1 ± )-approximation ofFp with probability at least 1 - 1/k.
If we use the estimator in Lemma 4 in both parts (heavy part and light part), The total space com-
plexity is B(s) + €-2SB(ρn2). Letting P = S = √n, we obtain an Fp estimation algorithm with
space O(-4n1/2-1/p log(n) log(M)). We remark that this bound is tight in n up to logarithmic
factors as there is a lower bound of Ω(n1∕2-1∕p) bits even in the presence of the oracle.
Theorem 7.	Under the assumption of a heavy hitter oracle, we can estimate IxIpp within a factor
1 ± 2 in O(-4 n1/2-1/p log(n) log(M)) bits with success probability at least 3/5.
6
Published as a conference paper at ICLR 2020
The above analysis is based on the assumption that the oracle is perfect. In practice, sometimes the
oracle makes mistakes. Here we assume that for each item i, the probability the oracle gives us an
incorrect prediction is δ . We have the following theorem for noisy oracles.
Theorem 8.	With a noisy heavy hitter oracle with error probability δ, we can estimate kxkpp
within a factor 1 ± 2e in O(€-4n1/2-1/p log(n) log(M)) bits of space when δ = O(1∕√n), or
in O(E-4(nδ)1-2∕p log(n) log(M)) bits of space otherwise. Both guarantees hold with success
probability at least 3/5.
5	Experiments
5.1	Distinct Elements
5.1.1	Internet Traffic Data
For this experiment, the goal is to estimate the number of distinct packets for each network flow.
A flow is a sequence of packets between two machines on the Internet. It is identified by the IP
addresses of its source and destination and the application ports.
Dataset: The traffic data is collected at a backbone link of a Tier1 ISP between Chicago and Seattle
in 2016 (CAIDA). Each recording session is around one hour. Within each minute, there are around
30 million packets (meaning 30 million updates) and 1 million unique flows. The distribution of
packet counts over Internet flows is heavy tailed, see Hsu et al. (2019).
Model: We use the prediction results in Hsu et al. (2019), which predict the logarithm of the packet
counts for each flow. In Hsu et al. (2019), the authors use two RNNs to encode the source and
destination IP addresses separately and additionally use two-layer fully-connected networks to en-
code the source and destination ports. Then they concatenate the encoded IP vectors, encoded port
vectors, and the protocol type as the final features to predict the packet counts. They use the first 7
minutes for training, the following minute for validation, and estimate the packet counts in subse-
quent minutes.
Results: We plot the estimation error vs. total number of buckets for the 20th minute in the CAIDA
data. We run the estimation algorithm 5 times and calculate the average estimation error. The result
is shown in Figure 2. The total number of unique flows appearing in a single test minute is about
1.1 × 106 . For every light coordinate, we make an extra update that makes this coordinate 0 with
probability 0.5.
There are two parts to our algorithm. For ROUGHL0ESTIMATOR, we set c = 10 and η = 1/4. We
use the heavy hitter oracle to predict whether the coordinate will be larger than 210. We randomly
select a prime from [11, 31] for the hash buckets. The total number of buckets in this part is about
4 × 103 (each bucket can store a number in {0, 1, . . . , 31} and has an extra bit to indicate whether
it has been marked). For the other part of our algorithm, we create a bucket matrix A of dimension
(log n) × K with K ranging from 26 to 29 . Each entry in A can also store a number between 0
and 31 and with an additional bit to indicate whether it has been marked. From the plot we can see
that the algorithm achieves an approximation error of about 2.5% using at most 1.5 × 104 buckets,
approximately 1.5% of the space needed if we were to keep an individual counter for each flow.
5.1.2	Synthetic Data
To further demonstrate the advantage of having a heavy hitter oracle, we run the previous algorithm
due to Kane et al. (2010b) (where we randomly choose a prime p ∈ [11, 31] as the divisor for each
bucket) and our modified algorithm on the synthetic data designed as follows: first, we generate an
input vector x of dimension n = 106 with i.i.d entries uniform on {0, 1, . . . , 100}. Then for each
coordinate i and each prime p ∈ [11, 31], we multiply xi by p with probability 0.2. We set the
threshold to be a heavy hitter to be 210, that is, the oracle reports xi as a heavy hitter if |xi| > 210.
Results: We use the optimal parameters in the last experiment (c = 10, η = 1/4 and K = 29) and
vary the total number of buckets from 7000 to 15000. The results are shown in Figure 1. We see
that having an oracle significantly reduces the estimation error.
7
Published as a conference paper at ICLR 2020
Figure 1: The estimation error of the L0-
norm in the synthetic data
Figure 2: The estimation error of the L0 -
norm in the 20th minute of the CAIDA data
5.2	Fp ESTIMATION
5.2.1	Search Query Data
For this experiment, the goal is to estimate the frequency moments of the vector indicating the
number of occurrences each search query appears.
Dataset: We use the AOL query log dataset, which consists of 21 million search queries collected
from 650 thousand users over 90 days. There are 3.8 million unique queries. Each query is a search
phrase with multiple words. The distribution of the search query frequencies is Zipfian, see Hsu
et al. (2019).
Model: We use the prediction result in Hsu et al. (2019), which predicts the number of times each
search phrase appears. In Hsu et al. (2019), the authors train an RNN with LSTM cells that takes
characters of a search phrase as input. The final states encoded by the RNN are fed to a fully-
connected layer to predict the query frequency. They use the first 5 days for training, the following
day for validation, and estimate the number of times different search queries appear in subsequent
days. We use this prediction result as an indicator of which search phrases are the heavy hitters.
Results: We plot the estimation error versus the total number of buckets for the 50th and 80th day
(p = 3 or p = 4). For the same data, we run the estimator 10 times and calculate the average
estimation error. The results are shown in Figure 3.
For our algorithm, We notice that the total number of search phrases that appear in a single day is
about n = 1.5 X 105 and so √n < 400. We can store the heavy coordinates identified by the heavy
hitter oracle directly. Given the budget of the number of buckets, We divide them into tWo parts.
Half of them Were used to store the heavy items, and the other half are used by the sub-sampling
algorithm With the precision sampling estimators to estimate the frequency moment of the light
elements.
In comparison, We also run the classical precision sampling estimators. Our results shoW a larger
estimation error oWing to the constraint of relatively feW buckets. Note that for a fixed bucket
budget Btot, there is a trade-off betWeen the number k of repetitions and the number B = Btot/k of
buckets allotted for the hash function in each repetition. We plot the results for different values of
k= 10, 20, 30.
It is clear from the plots that the estimation error of the oracle-aided algorithm is about 1% even
When the total number of buckets is small, demonstrating a strong advantage over classical precision
sampling estimators.
5.2.2	Synthetic Data
A number of real-World datasets have the property that the heavy coordinates contribute a large
fraction of the Fp-moment, as p increases, and so one needs only to obtain a good approximation of
8
Published as a conference paper at ICLR 2020
Figure 3: The estimation error of the moment of search queries. The left figure corresponds to p = 3
(80th test day) and the right p = 4 (50th test day).
the Fp -moment of the heavy coordinates. This is indeed one of the reasons why we achieved low
error for search query estimation. In order to show that our subsampling algorithm also achieves a
good approximation for the light coordinates, we considered the following data set.
We generated an input vector X of dimension n = 108 with the first √n coordinates being heavy
and the remaining coordinates being light. Each coordinate is a uniform random variable in (0, 1),
later renormalized such that the heavy and the light parts contribute equally to the total Fp-moment,
i.e., kxIH kpp = kxIL kpp. Hence, a good approximation of kxkpp requires a good approximation of
both parts.
Results: We plot the estimation error vs. total number of buckets for p = 3 in Figure 4. For our
algorithm and classical precision sampling, we fixed the number k of repetitions to 100 and varied
the total number of buckets from 106 to 2 × 106 (1% to 2% of the length of x). We repeated the
sub-sampling algorithm 9 times. In each of the 9 repetitions, we also used the precision sampling
estimators, repeating 100 times.
From the plots we see that our algorithm achieves significantly smaller relative approximation error
than the classical precision sampling algorithm for every number of buckets. In particular, using
2 ∙ 106 buckets, i.e., 2% space of what is needed to store the entire vector x, our algorithm achieves
a 15% relative estimation error while the classical precision sampling obtains only a 27% relative
error. Another observation is that since subsampling reduces the space consumption, our algorithm
has a faster update time when n is large.
Figure 4: Estimation error for the F3-
moment on synthetic data
Acknowledgments
The authors would like to thank the anonymous
reviewers for helpful comments. Y. Li was sup-
ported in part by Singapore Ministry of Educa-
tion (AcRF) Tier 2 grant MOE2018-T2-1-013.
D. Woodruff would like to thank partial sup-
port from the National Science Foundation un-
der Grant No. CCF-1815840 and the Office of
Naval Research (ONR) under grant N00014-18-
1-2562.
9
Published as a conference paper at ICLR 2020
References
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. Journal ofComputer and system sciences, 58(1):137-147, 1999.
Alexandr Andoni, Khanh Do Ba, Piotr Indyk, and David P. Woodruff. Efficient sketches for earth-
mover distance, with applications. In 50th Annual IEEE Symposium on Foundations of Computer
Science, FOCS 2009, October 25-27, 2009, Atlanta, Georgia, USA, pp. 324-330, 2009.
Alexandr Andoni, Robert Krauthgamer, and Krzysztof Onak. Streaming algorithms via precision
sampling. In IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS 2011,
Palm Springs, CA, USA, October 22-25, 2011, pp. 363-372, 2011.
Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design,
online learning, and private optimization. In 59th IEEE Annual Symposium on Foundations of
Computer Science, FOCS 2018, Paris, France, October 7-9, 2018, pp. 603-614, 2018a.
Maria-Florina Balcan, Travis Dick, and Colin White. Data-driven clustering via parameterized
lloyd’s families. In Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal,
Canada., pp. 10664-10674, 2018b.
Ziv Bar-Yossef, T. S. Jayram, Ravi Kumar, and D. Sivakumar. An information statistics approach to
data stream and communication complexity. J. Comput. Syst. Sci., 68(4):702-732, 2004.
Vladimir Braverman and Rafail Ostrovsky. Recursive sketching for frequency moments. CoRR,
2010.
CAIDA. Caida internet traces 2016 chicago. http://www.caida.org/data/monitors/
passive-equinix-chicago.xml.
Amit Chakrabarti, Khanh Do Ba, and S. Muthukrishnan. Estimating entropy and entropy norm on
data streams. Internet Mathematics, 3(1):63-78, 2006.
Graham Cormode and S. Muthukrishnan. Space efficient mining of multigraph streams. In Proceed-
ings of the Twenty-fourth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database
Systems, June 13-15, 2005, Baltimore, Maryland, USA, pp. 271-282, 2005.
Travis Dick, Mu Li, Venkata Krishna Pillutla, Colin White, Nina Balcan, and Alexander J. Smola.
Data driven resource allocation for distributed learning. In Proceedings of the 20th International
Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Laud-
erdale, FL, USA, pp. 662-671, 2017.
Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm selection.
In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science,
Cambridge, MA, USA, January 14-16, 2016, pp. 123-134, 2016.
Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
algorithms. In 7th International Conference on Learning Representations, ICLR 2019, New Or-
leans, LA, USA, May 6-9, 2019.
Piotr Indyk. Stable distributions, pseudorandom generators, embeddings and data stream computa-
tion. In Proceedings 41st Annual Symposium on Foundations of Computer Science, pp. 189-197.
IEEE, 2000.
Piotr Indyk and David P. Woodruff. Optimal approximations of the frequency moments of data
streams. In Proceedings of the 37th Annual ACM Symposium on Theory of Computing, Baltimore,
MD, USA, May 22-24, 2005, pp. 202-208, 2005.
Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. CoRR,
abs/1910.13984, 2019.
T. S. Jayram and D. P. Woodruff. The data stream space complexity of cascaded norms. In 2009
50th Annual IEEE Symposium on Foundations of Computer Science, pp. 765-774, Oct 2009.
10
Published as a conference paper at ICLR 2020
Daniel M. Kane, Jelani Nelson, and David P. Woodruff. On the exact space complexity of sketching
and streaming small norms. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2010, Austin, Texas, USA, January 17-19, 2010, pp. 1161-1178,
2010a.
Daniel M. Kane, Jelani Nelson, and David P. Woodruff. An optimal algorithm for the distinct ele-
ments problem. In Proceedings of the Twenty-ninth ACM SIGMOD-SIGACT-SIGART Symposium
on Principles of Database Systems, PODS ’10, pp. 41-52, New York, NY, USA, 2010b. ACM.
Daniel M Kane, Jelani Nelson, Ely Porat, and David P Woodruff. Fast moment estimation in data
streams in optimal space. In Proceedings of the forty-third annual ACM symposium on Theory of
computing, pp. 745-754. ACM, 2011.
Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned
index structures. CoRR, abs/1712.01208, 2017.
Ping Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable ran-
dom projections. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 2008, San Francisco, California, USA, January 20-22, 2008, pp. 10-19, 2008.
Andrew McGregor, A Pavan, Srikanta Tirthapura, and David P Woodruff. Space-efficient estimation
of statistics over sub-sampled streams. Algorithmica, 74(2):787-811, 2016.
S.	Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in Theoret-
ical Computer Science, 1(2), 2005.
John Nolan. Stable distributions: models for heavy-tailed data. Birkhauser Boston, 2003.
Anna Ostlin and Rasmus Pagh. Uniform hashing in constant time and linear space. In Proceedings
of the thirty-fifth annual ACM symposium on Theory of computing, pp. 622-628. ACM, 2003.
Srikanta Tirthapura and David Woodruff. Rectangle-efficient aggregation in spatial data streams.
In Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database
Systems, pp. 283-294. ACM, 2012.
David P. Woodruff and Guang Yang. Separating k-player from t-player one-way communication,
with applications to data streams. In 46th International Colloquium on Automata, Languages,
and Programming, ICALP 2019, July 9-12, 2019, Patras, Greece., pp. 97:1-97:14, 2019.
Vladimir M Zolotarev. One-dimensional stable distributions, volume 65. American Mathematical
Soc., 1986.
11
Published as a conference paper at ICLR 2020
A Omitted Proofs in Section 3
Proof of Lemma 1. If a bucket contains a heavy hitter, it is marked nonempty and it is indeed
nonempty. It suffices to show that a bucket containing only light coordinates is nonempty mod-
ulo p. Since the light coordinates are at most 2poly(1/), there are at most c of them by assumption,
their sum S is thus at most 2poly(1/). This implies that S has at most poly(1/) prime factors. Ifwe
choose a random prime p of size poly(1/), then S mod p 6= 0 with probability at least 1 - poly()
if S 6= 0. Taking a union bound over Θ(c2) buckets proves the correctness. The space and time
complexity are immediate from the description of the algorithm.	□
Proof of Theorem 2. The proof is almost identical to that in Kane et al. (2010b). The space and time
complexities are follow from the description of the algorithm.
Next we show correctness. Let L0(j) denote the true number of distinct elements in the j-th scale.
Then EL°(j) = Lo∕2j. Let j* = max{j : E L°(j) ≥ 1} and j** = max{j < j* : EL0(j) ≥ 55}.
As shown in Kane et al. (2010b), if j** exists, it holds that 55 ≤ E Lo(j**) < 110 and Pr{32 <
L0(j**) < 142} ≥ 8/9. With our choices of c = 141 and η = 1/16, EXACTCOUNT returns a
nonzero value for the j**-th scale with probability 8/9 - 1/16 > 13/16 by Lemma 1. It follows
that the deepest scale j We find satisfies j** ≤ j ≤ j*. Hence, Lo = 2j** ELo(j**) ≤ 110 ∙ 2j
and Lo = 2j* ELo(j*) ≥ 2j as desired. If j** does not exist, then Lo < 55, and Lo = 1 is a
55-approximation in this case.	□
B	Omitted Proofs in Section 4
Proof of Lemma 5. We let Y = ρFs = ρ Pi fi|xi|p, where fi = 1 if i was chosen, and fi = 0
otherwise. So we have that E[fi] = 1.
It thus follows that E[Y] = kxIL kpp, that is, Y is an unbiased estimate. Then,
Var[Y ]= E[Y 2] - E[Y ]2 = ρ2 (EX 加 ∣2p + X fifj ∣Xi∣p∣Xj |p] ∣ -|喇
i	i6=j
≤ P2 (EXfi|Xil2p] + X=ki∣p∣χjlp) -kxkpp
i	i,j ρ
=ρX |xi|2p .
i∈IL
To bound this, we notice that we have the fact that Xi < kχSk^ and p > 2, so
2p	kxk2pp	ρ	2p
PTxp ≤	= Skxkpp∙
i∈IL
So finally we have that Var[Y] ≤ S Ilxkpp .	□
Proof of Lemma 6. By Chebyshev’s inequality, we have that
2
.	吟 Ixk2p	1
Pr {∣Y -kxkpl >ekxkp} ≤	p“ =L	□
I........p.........p' - P e2kxkpp	k
Proof of Theorem 7. For the heavy hitters, note that | Ih | ≤ n/s = √n. For this part, we can use
9B(√n) space to get an H for which |H 一 kx5 kpl ≤ EkxIH kp with probability at least 8 by
independently running 9 estimators and then taking the median.
For the light elements, for every sub-sample round i, we independently run O(1) estimators with
rate √ne2 and use their median Zi to be the estimate of this level of sub-sampling. Let 匕 be the
12
Published as a conference paper at ICLR 2020
true Fp-moment of this sub-sample. Then with probability at least 盖 it holds that | Yi - Zi | ≤ eYi.
Taking a union bound, We get that if We do the sub-sampling 10 times, with probability at least 磊,
|Yi - Zi | ≤ EkxILkp holds for every i. Using Lemma 6 we can get that with probability at least *,
|Z - IlxILkpI ≤ EllxILllp + Ekxkp. The overall success probability of this part is at least 5.
So we get that |H+Z - kxkpp| ≤ EkxIL kpp + Ekxkpp + EkxIH kpp = 2Ekxkpp with success probability at
least 3/5. The total space complexity is B(√n) + O(E-I2)B(√√n) = O(E-4n1 -P log(n) log(M)).
□
Proof of Theorem 8. For an element xi which the oracle indicates as light, we know that with prob-
ability 1 - δ, it is truly light. Combined with Theorem 5, we obtain that
VarY] ≤ ρ(1 - δ) X ∣xi∣2p + ρδ X ∣xi∣2p ≤ (1 - δ)P kxkpp + ρδkxkpp∙
i∈IL	i∈IH
Hence, when δ = O(表),Theorem 7 continues to hold. Otherwise we can let S = nδ and P =
E2δ-1, and the space complexity becomes O(E-4(nδ)1-2 log(n) log(M)) bits.	□
Lower Bound The upper bound in Theorem 7 is asymptotically tight in n up to logarithmic fac-
tors, since we have the following matching lower bound even in the presence of an oracle which
indicates whether |xi| > √ kxkp.
Theorem 9. Suppose that 1∕√n< E < 1/4 and that a heavy hitter oracle indicates whether |xi| >
√n kxkp for the whole underlying vector x. Any randomized streaming algorithm that estimates
kxkp with a factor (1 ± E) with probability ≥ 9/10 requires Ω(E-2∕pn1∕2τ∕p) bits ofspace.
ProofofTheorem 9. We assume √n to be an integer and define our hard instance as follows: for
every 1 ≤ i ≤ √n, we let xi = 1 with probability 1 -1 /√n, and xi = (4E)1/pn1/2p with probability
1 /√n. And for i > √n, xi = 4 m。壮 √n. In this case, kxkp ≥ n, so any coordinate will not be a
heavy hitter. We note that if our algorithm can outputs an (1 ± E) approximation of kxkpp , it can also
output an (1 ± E) approximation of kx[√n] ∣∣.
To prove our lower bound, we will use the '∞ communication problem in Bar-Yossef et al. (2004):
there are two parties, Alice and Bob, holding vectors a, b ∈ Zm respectively, and their goal is to
decide if ka - b∣∣∞ ≤ 1 or ∣∣a - b∣∣∞ ≥ k. From Bar-Yossef et al. (2004), we can get an C(m/k2)
bits lower bound.
Now we claim that: if we set m = √n, k = (4e)1/pn1/2p and make a - b = x[√n], then any
streaming algorithm outputs a (1 ± e) approximation Y of kx[√n]kp can be used to build a commu-
nication protocol for solving this '∞ problem with communication proportional to the algorithm's
space complexity. In this case, if ka 一 b∣∣∞ = 1, then we have Y ≤ (1 + E)kx[√n]kp ≤ (1 + E)√n.
Otherwise, Y ≥ (1 — E)(√n — 1 + 4E√n) > (1 + E)√n.
By this reduction, we can finally get an C(m/k2) = Ω(e-2∕pn1/2-1/p) bits space lower bound
under the oracle assumption.	□
C FAST MOMENT ESTIMATION, 0 < p < 2
When 0 < p < 2, the Fp estimation problem can be done in an optimal O(E-2 log n) bits of space.
Here we show how to improve the update time while maintaining this amount of space.
In the rest of the section, we assume there are two oracles: one can tell us whether |xi|p ≥ E2kxkpp,
2
and the other can tell us whether |xi|p ≥ 匕目？./0∖giog(i/e)kxkp. We call these 3 categories of
elements heavy elements, medium elements, and light elements. Let IH , IM , IL ⊆ [n] denote the
subset of indices of the heavy elements, medium elements, and the light elements, respectively.
For the heavy elements, we can use O(E-2 log(M)) space to store them. We now turn to the other
two parts. For the light elements, we will use the LightEstimator in Section 3 in Kane et al. (2011):
13
Published as a conference paper at ICLR 2020
Lemma 10 (Kane et al. (2011)). Suppose we are given 0 <	< 1, and given a list L ⊆ [n] such
that i ∈ L if and only if |xi|p ≥ 2kxkpp. There is an algorithm LightEstimator which returns
an estimate X to kxn\L kpp such that |X - kxn\Lkpp| ≤ kxkpp with probability at least 0.9. The
space usage is O(-2 log(nmM)), the amortized update time is O(log2 (1/) log log(1/)), and the
reporting time is O(1/2).
2
For the light elements Xi, note We have the condition ∣x∕p ≤ ]0目2(1左)^ ^⑴^)l∣xkp. Using
Lemma 5 Lemma 6 and the argument in Theorem 7, if we use the LightEstimator to do the sub-
sampling algorithm With rate ρ = log2(1/) log log(1/) for the light elements, We just need to
sub-sample O(1) times to get an approximation X of lxIL lpp such that |X - lxIL lpp| ≤ lxlpp With
probability at least 0.9. Note that for each update (i, v) in the sub-sampling process, if xi is not se-
lected by the sub-sampling algorithm, the update time is O(1) (We do not need to do anything), oth-
erWise, by Kane et al. (2011) We knoW that the amortized update time is O(log2 (1/) log log(1/)).
Notice that in Lemma 5 We just need a pairWise independent hash function for the sub-sampling. So
during the process, the expected amortized update time is O( log2(1∕')log log(1∕0) = O(1).
Theorem 11. Suppose we are given 0 < < 1, and given the lists IL. There is an algorithm which
returns an estimate X to lxIL lpp such that |X - lxIL lpp | ≤ lxlpp with probability at least 0.9. The
space usage is O(-2 log(nmM)), the expected amortized update time is O(1), and the reporting
time is O(1/2).
It remains to handle the medium elements. Here We Will still use the LightEstimator, but do some
adjustments. Before describing our LightEstimator data structure, We first define the p-stable dis-
tribution.
Definition 1 (Zolotarev Zolotarev (1986)). For 0 < p < 2, there exists a probability distribution
Dp called the p-stable distribution satisfying the folloWing property. For any positive integer n and
vector x ∈ Rn, if Zι,…，Zn 〜 Dp are independent, then P；=i ZjXj 〜IIxIlpZ for Z 〜 Dp.
Lemma 12 (Pagh and Pagh, Ostlin & Pagh (2003) Theorem 1.1). Let S ⊆ U = [u] be a set of
z > 1 elements, and let V = [v], with 1 < v ≤ U. Suppose the machine word size is Ω(log(u)).
For any constant c > 0 there is a word RAM algorithm that, using time log(z) logO(1) (v) and
O(log(z) + log log(u)) bits of space, selects a family H of functions from U to V (independent of
S) such that:
1.	With probability 1 - O(1/zc), H is z-wise independent when restricted to S.
2.	Any h ∈ H can be represented by a RAM data structure using O(z log(v)) bits of space,
and h can be evaluated in constant time after an initialization step taking O(z) time.
In Kane et al. (2011), LightEstimator Was defined by creating R = 4/2 independent instantiations
of the estimator D1, Which We label D11, ..., D1R, and picking a hash function h : [n] → [R] using
Lemma 12. Upon receiving an update to Xi in the stream, the update Was fed to D1h(i). The estimator
D1i is a slightly modified version of the geometric mean estimator ofLi (2008), Which takes a matrix
A ∈ Rt×n, where each row contains Ω(1/Ep)-WiSe p-stable entries and the rows being independent
from each other, and maintains y = AX in the stream. Furthermore, in parallel We run the algorithm
of Kane et al. (2010a) with constant error parameter to obtain a value Fp in [IXIpp/2, 3IXIpp/2].
The estimator Estp is min{Ct,p(Qtj=1 |yj |p/t), Fep/E}. Hence in Kane et al. (2011), the update time
is the time to evaluate a Θ(1/Ep)-WiSe independent hash function over a field of size PoIy(nmM),
which is O(log2(1/E) log log(1/E)). Therefore, ifwe can store the hash values of the p-stable entries
for the medium elements, then we can improve the update time to O(1) for this part (note that the
parameter t is a constant). We need the following lemma.
Lemma 13. Let k = O(* log2 ɪ log log ɪ) and Xi, X2 ,…，Xk be P-stable random variables. With
probability at least 0.9, the following holds. There exists T ⊆ [k] such that |T | = O(1/E2) for which
|Xi| ≤ poly log(1/E) for all i 6∈ T, and consequently, each such Xi can be approximated by some
Xi satisfying |Xi - Xi| ≤ poly(E) and each such Xi can be recovered from O(log(1/E)) bits.
To prove this, we need the following result.
14
Published as a conference paper at ICLR 2020
Lemma 14 (Nolan (2003), Theorem 1.12). For fixed 0 < p < 2, the probability density function
φp of the p-stable distribution satisfies φp (x) = O(1/(1 + |x|p+1)) and is an even function. The
cumulative distribution function satisfies Φp(x) = O(|x|-p).
Proof of Lemma 13. Let I = [- log3/p(1/), log3/p(1/)]. For a p-stable random variable Xi, by
Lemma 14, Pr{Xi ∈/ I} = O(1/ log3(1/)). So the expected number of Xi for which Xi ∈/ I
is O(k/ log3(1/)) = O(1/2). By Markov’s inequality, with probability at least 0.9, the number
of these Xi are O(1/2). For Xi contained in I, we can uniformly partition I into subintervals of
length poly(), and there will be O(poly(1/)) many subintervals. Let Xi be the nearest partition
point. It is clear that Xi can be recovered from O(log(1/)) bits to store the index of the partition
point.	口
The following lemma tells us that we can use the approximation value given by the p-stable random
variables in the Esti .
Lemma 15. Let k = O(表 log2 ɪ log log ɪ). Suppose we are g^ven a,b,x ∈ Rk and % — b/ ≤ eq
for all i. Then we have |ha, xi - hb, xi| ≤ q-2 kxkp for sufficiently small .
Proof. By the CaUchy-SchWarz inequality, |〈a — b, x)| ≤ ∣∣a — b∣∣2∣∣x∣∣2 ≤ vzke2q∣∣x∣∣2 ≤
eq-2∣∣x∣∣2 ≤ eq-2∣∣x∣p (recall thatP < 2).	口
Finally, We have the folloWing theorem.
Theorem 16. Suppose we are given 0 < e < 1 and the lists IM. There is an algorithm which
returns an estimate X to ∣xim ∣p such that ∖X — IlxIM kp| ≤ 2e∣x∣p with probability at least 0.8.
The space usage is O(e-2 max{log(nmM), log3(1/e) log log(1/e)}), the amortized update time is
O(1), and the reporting time is O(1/e2).
Proof. Let k = 表 log2 ɪ log log ɪ. Note that there are at most k medium elements. Using
Lemma 12, We pick tWo hash functions h1 : [n] → [k], h2 : [n] → [1/e2], and We can assume
With high probability, they perfectly hash our input set of items. Lemma 13 tells us With probability
at least 0.9, We can use these tWo hash functions and use O(1) time to read the p-stable random
variables for the medium elements (if Xi ∈ I, We store Xi in the cell Which h1 (i) refers to, using
O(log(1/e)) bits. OtherWise We store Xi in the cell Which h2(i) refers to, using O(log(M)) bits).
Suppose that φ(x) is the pdf of the p-stable distribution. For a p-stable Y , We have that Pr{|Y | <
eq} = R-qq φ(x)dx ≤ 2φ(0)eq = poly(e) When q is sufficiently large. Hence, in Lemma 13 and
Lemma 15, We can pick a sufficiently large q such that With probability at least 1 — poly(e), all of the
true values yi output by the Esti are at least eq ∣x∣p, and the yei output by the Esti using the stored
p-stable random variables satisfy |yi —词 ≤ eq+1 ∣x∣p. So the approximation from the p-stable
random variables will affect our estimate by at most e∣xkp. This completes the proof.	口
Using Theorem 11 and 16 We Will have our result:
Theorem 17. Let 0 < p < 2 and 0 < e < 1/2. There exists a randomized algorithm which outputs
(1 ± 3e)∣x∣pp with probability at least 0.7 using O(e-2 max{log(nmM), log3(1/e) log log(1/e)})
bits of space. The expected amortized update time is O(1). The reporting time is O(1/e2).
In the case of a noisy oracle, we assume that for a fixed s, our oracle will not identify more than O(s)
items xi as heavy hitters (meaning that |xi| > S Ilxkp). Then we can adapt the preceding theorem to
the noisy oracle version.
Theorem 18. Let 0 < p < 2 and 0 < e < 1/2. Suppose that the heavy hitter oracle errs with
2
probability δ = O(bg2(i∕e)\giog(i/e)). There exists an algorithm which outputs (1 ± 3e)kx∣p with
probability at least 0.7 using O(e-2 max{log(nmM), log3(1/e) log log(1/e)}) bits of space. The
expected amortized update time is O(1). The reporting time is O(1/e2).
15
Published as a conference paper at ICLR 2020
Proof. We only need to consider the difference in the estimation of kxIL k. When δ =
O( iog2(i/e)∖g iog(i∕e) )，note that our SUb-SamPling rate is log2(1/e) log log(1∕e). It follows from
Theorem 8 that Theorem 17 continues to hold.	□
D Cascaded Norms
For notational simPlicity, we consider aPProximating Fk(Fp(x)) = Pin=1(Pjd=1 |xij|p)k with k ≥
1 andp ≥ 2. The cascaded norm kxkk,p then satisfies kxkkk,p = Fk/p(Fp(x) for k ≥ p ≥ 2.
We follow the algorithm in Jayram & Woodruff (2009), which first downsamPles the rows in log n
levels, and then samPles in each level Q = O(n1-1/k) entries from the row-samPled submatrix
ProPortional to |xi|p, and at last estimates the Fk(Fp) cascaded norm from those samPles. The first
steP of `p -samPling is further decomPosed into two stePs: (i) dividing the entries into layers by
their magnitudes and uniformly samPling a sufficiently large set of entries in each layer, and (ii)
performing “'p-sampling” from those samples. We shall improve the space complexity of Step (i)
with a heavy hitter oracle.
We first elaborate how Step (i) was done. It splits the entries in the row-sampled matrix X into
layers, where the t-th layer St(X) consists of entries in [ζηt-1, ζηt]. A layer t is said to be con-
tributing if |St(X)∣(Zηt)p ≥ Fp(X)∕(Bθ). It needs to return βt = θQ∣St(XM(Znty/Fp(X)
samples from each contributing layer t. Suppose that |St(X)|/2j ≤ βt < |St(X)|/2j-1 for
some j, and the algorithm subsamples each entry with probability 1/2j, obtaining a subset Yj
of entries. It can be shown that with high probability, for a contributing layer t, it holds that
(Znty2 ≥ ∣∣γj- k2∕(Q2∕pθ4∣X∣1-2∕p log |X|), and therefore the standard Count-Sketch algorithm
can recover all those entries with Oe(Q2/p|X|1-2/p) space.
Consider the top level without subsampling of the rows. Suppose that the heavy hitter oracle de-
termines whether |xi|p ≥ ∣x∣pp /T for each entry xi of the matrix x with some threshold T. In
the algorithm of Step (i), without loss of generality, we can assume each layer contains either
heavy entries only or light entries only, and we call the layer a heavy layer or a light layer ac-
cordingly. If a light layer is contributing, since each entry is at most Fp/T , there must exist at
least Nt = T |St(X)|(Znt)p/Fp entries in that level. We would require that Nt ≥ βt, for which
it suffices to have T = Ω(θQ). We can downsample the layer at rate ΘQ∕T, and then apply the
preceding algorithm and thus the number of buckets can be reduced by a factor of T /(θQ). Note
that the heavy layers come from the at most T heavy hitters. This leads to a space complexity of
O(Q2∕pT 1-2∕p + Q2/p|X∣1-2∕p∕(T∕(θQ)). Let T = Θ((∣X|Q)1/2). In this case, T = Ω(θQ) is
satisfied and the space complexity is Oe(Q1/2+1/p|X|1/2-1/p).
For the j-th level of row sampling, note that the same (global) heavy hitter oracle, when applied to
this row-subsampled matrix, effectively has T replaced with T∕2j . However, |X | is also replaced
with |X∣∕2j, and thus We would require T = Ω(2jθQ) and We can downsample the layer at rate
2j θQ∕T. This means that the space complexity is a (2-j)1-2/p fraction of the space of the top level.
The constraint of T = Ω(2jθQ) allows for j ≤ ɪ log |Q|. For j > ɪ log |Q1, we can just run the
old algorithm withspace O(Q2/p(|X∣∕2j)1-2/p) = O(Q1∕2+1∕p∣X|1/2-1/p).
Plugging in Q = Θ(n1-1∕k) and |X| = nd and noting that the space of the subsequent steps are
dominated by this very first sampling step, we have:
Theorem 19. Let > 0 and k ≥ 1,p ≥ 2 be constants. There exists a randomized algorithm which
receives an n × d matrix x and outputs a (1 + )-approximation to Fk (Fp(x)) with probability at
least 2/3 using O(n^~ k⅛ - 2k d2 -1) space.
Replacing k with k∕p, we obtain our final result for cascaded norms.
Corollary 20. Let > 0 and k ≥ p ≥ 2 be constants. There exists a randomized algorithm which
receives an n × d matrix x in a stream of additions and deletions to its coordinates, and outputs a
(1 + e)-approximation to Ilxkk,p with probability at least 2/3 using O(n1- 1 -盘 d 1-P) space.
16
Published as a conference paper at ICLR 2020
E RECTANGULAR Fp
The rectangle-efficient algorithm in Tirthapura & Woodruff (2012) is based on the Fp moment es-
timation algorithm in Braverman & Ostrovsky (2010). In Braverman & Ostrovsky (2010), they
first subsample to obtain the substream Dj (j = 0, 1, 2, ... log n), where D0 is the full stream, and if
i ∈ Dj, then i ∈ Dj+1 with probability 1/2. For every substream Dj, they run the COUNT-SKETCH
algorithm to recover all (αP/n1-2)-heavy hitters, whence they can obtain a good final estimate.
The algorithm in Tirthapura & Woodruff (2012) is similar. Instead of updating the counter in
each coordinate inside a rectangle, they developed a rectangle-efficient data structure called Rect-
ANGLECOUNTSKetch. We follow their notation that O*(f) denotes a function of the form
f ∙ poly(1∕e, d, log(m△/δ)).
Lemma 21 (RECTANGLECOUNTSKETCH, Tirthapura & Woodruff (2012)). The data structure
RECTANGLECOUNTSKETCH(Y) Can be updated rectangle-efficiently in time Ο*(γ-2). The total
space is O* (γ-2) words. The data structure can be used to answer any query i ∈ GF(∆)d, return-
ing a number Φ(i) with ∣Φ(i) 一 xi| ≤ Ykxk2. The algorithm succeeds on all queries simultaneously
with probability ≥ 1 - δ.
Under an arbitrary mapping g : GF (△)d → {1, 2, ..., △}, they subsample the items into a logarith-
mic number φ = dlog(∆) of levels. In particular, for j ∈ [φ], i ∈ Dj if g(i) ≤ ∆d∕2j-1. In each
level, they give an algorithm which invokes RECTANGLECOUNTSKETCH(Y) to recover a list of the
Fp heavy hitters. Then they use the heavy hitters from each layer to calculate the final estimate.
Lemma 22 (Tirthapura & Woodruff (2012)). For p > 2, there is an algorithm which uses
RECTANGLECOUNTSKETCH(Y) with Y = Θ(e1+1 αp/△d/2-d/p) and with probability at least
1 — δ outputs a set P of O(1∕α) pairs (i, xi) such that £(分 χθ)∈p ∣∣xi∣p — |xi|p| ≤ Ekxkp and all
elements i with |xi|p ≥ αkxkpp appear as the first element of some pair in P. The algorithm uses
O* (Y-2) words of space.
Since the algorithm relies on a subroutine that finds the 'p-heavy hitters, the improvement with a
heavy hitter oracle is similar to that for the Fp-moment problem. Specifically, suppose the heavy
hitter oracle is able to indicate whether |xi|p > ∆1∕2 kxkp. We know from Theorem 7 that We can
use a rectangle-efficient estimator for the substream which contains all the heavy items, while for
the light items, we can use the subsampling algorithm with rate △d/2. Note that for each substream,
We have ∣∣xko ≤ ∆d/2 (for those i's that are not in the substream, We see xi = 0), so by Holder's
inequality, it holds that kxk2 ≤ △d/4-d/(2p) kxkp. Following the proof of Lemma 22, we can set
γ = Θ(e1+1 ɑP/△d/4-d/(2p)) to find those heavy hitters. We thus have the following theorem.
Theorem 23. Under the assumption of a heavy hitter oracle, there is a rectangle-efficient single-
pass streaming algorithm which outputs a (1 ± E)-approximation to kxkpp with probability 1 — δfor
p > 2. It uses Ο*(∆d(I/2-1/P)) bits of space and Ο*(∆d(I/2-1/P)) time to process each rectangle
in the stream.
17