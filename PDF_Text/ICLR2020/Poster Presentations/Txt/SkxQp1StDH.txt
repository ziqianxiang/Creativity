Published as a conference paper at ICLR 2020
Low-dimensional statistical manifold embed-
DING OF DIRECTED GRAPHS
Thorben Funke
L3S Research Center
Leibniz University Hannover
Hannover, Germany
Alen Lancic
Faculty of Science
Department of Mathematics
University of Zagreb, Croatia
Tian Guo
Computationl Social Science
ETH Zurich
Zurich, Switzerland
Nino Antulov-Fantulin
Computationl Social Science
ETH Zurich
Zurich, Switzerland
anino@ethz.ch
Ab stract
We propose a novel node embedding of directed graphs to statistical manifolds,
which is based on a global minimization of pairwise relative entropy and graph
geodesics in a non-linear way. Each node is encoded with a probability density
function over a measurable space. Furthermore, we analyze the connection between
the geometrical properties of such embedding and their efficient learning procedure.
Extensive experiments show that our proposed embedding is better in preserving
the global geodesic information of graphs, as well as outperforming existing
embedding models on directed graphs in a variety of evaluation metrics, in an
unsupervised setting.
1 Introduction
In this publication, we study the directed graph embedding problem in an unsupervised learning
setting. A graph embedding problem is usually defined as a problem of finding a vector representation
X ∈ RK for every node of a graph G = (V, E) through a mapping φ: V → X. On every graph
G = (V, E), defined with set of nodes V and set of edges E, the distance dG : V × V → R+ between
two vertices is defined as the number of edges connecting them in the shortest path, also called a graph
geodesic. In case that X is equipped with a distance metric function dX : X × X → R+, we can
quantify the embedding distortion by measuring the ratio dX / dG between pairs of corresponding
embedded points and nodes. Alternatively, one can measure the quality of the mapping by using a
similarity function between points. In contrast to distance, the similarity is usually a bounded value
[0, 1] and is in some ad-hoc way connected to a notion of distance e.g. inverse of the distance. Usually,
we are interested in a low-dimensional (K n) embedding of graphs with n nodes as it is always
possible to find an embedding [32] with L∞ norm with no distortion in an n-dimensional Euclidean
space. Bourgain theorem (1985) [6] proved that it is possible to construct an O(log(n))-dimensional
Euclidean embedding of an undirected graph with n nodes with finite distortion.
For the last couple of decades, different communities such as machine learning [16; 33; 10; 40; 14],
physics [29; 38; 35], and computer science [42; 43] independently from mathematics community [32;
25; 12; 6] developed novel graph representation techniques. In a nutshell, they can be characterized
by having one of the following properties (i-iv). (i) The Type of loss function that quantifies the
distortion is optimizing local neighbourhood [44; 21] or global graph structure properties [53; 40; 14].
(ii) The target property to be preserved is either geodesic (shortest paths) [42; 43; 12] or diffusion
distance (heat or random walk process) [46; 40; 14] or another similarity property [21; 55; 39].
(iii) The mapping φ is a linear [23; 39] or non-linear dimensionality reduction technique such as
SNE [21], t-SNE [55], ISOMAP [53], Laplacian eigenmaps [3] or deep learning work DeepWalk [40],
node2vec [14], HOPE [39], GraphSAGE [17], NetMF [33] and others [16]. (iv) The geometry of
1
Published as a conference paper at ICLR 2020
Figure 1: Visualization of synthetic example network together with our embedding. The graph is
embedded into the 2-variate normal distributions, which are represented by a σ ellipse (boundary
of 1 standard deviation around mean μ). The σ ellipses of the green nodes are contained in one of
the greys nodes, which represents that the distance (measured with divergence in embedded space)
between green and grey is small, but in the opposite direction very large.
-20	0	20
X has zero curvature (Euclidean) [32; 12], positive curvature [57] (spherical) or negative curvature
[29; 35; 10] (hyperbolic) or mixed curvatures [15].
However, for directed graphs the asymmetry of graph shortest path distances dG(vi , vj) 6= dG (vj , vi)
is violating the symmetry assumption (not a metric function). This is the reason why only recently
this problem was tackled by constructing two independent representations for each node (source and
target representations) [39; 59; 27]. In this paper, we propose a single-node embedding for directed
graphs to the space of probability distributions, described with the geometry of statistical manifold
theory. Up to this point, different probability distributions were used for different embedding purposes
such as: (i) applications to word embedding [45; 56], knowledge graphs [20], attributed graphs [4]
or (ii) generalized point embeddings with elliptical distributions [36], but not for low-dimensional
embedding of directed graphs, characterized by the theory of statistical manifolds.
The main contributions of this paper are: (i) We propose a node embedding for directed graphs to
the elements of the statistical manifolds [51], to jointly capture asymmetric and infinite distances
in a low-dimensional space. We determine the connection between geometry and gradient-based
optimization. (ii) Furthermore, we develop a sampling-based scalable learning algorithm for large-
scale networks. (iii) Extensive experiments demonstrate that the proposed embedding outperforms
several directed network embedding baselines, i.e. random walks based [59] and matrix factorization
based [39] methods, as well as the deep learning undirected representative [40] and deep Gaussian
embeddings for attributed graphs [4].
2	Statistical manifold divergence embedding for directed graphs
In this section, we analyze the limits of metric embedding. Then, we continue by providing an
intuition of our embedding with a synthetic example before presenting the learning procedure of the
embedding method. Bourgain theorem (1985) [6] states that it is possible to construct an O(log(n))-
dimensional embedding (φ: v 7→ xv) of an undirected graph with n nodes to Euclidean space with
O(log(n)) distortion by using random subset projection. But, on directed graph G = (V, E), the
directed shortest paths are not necessarily symmetric dv,u 6= du,v . This does not imply that it is not
possible to provide low dimensional metric embedding [6; 32; 25; 12] with a variant of Bourgain
theorem. In the appendix A.1, we show that the main problem is not asymmetry, but the existence of
infinite directed distances, which is quite common in real directed networks, see Table 1.
2.1	Learning statistical manifold embedding
Instead of a conventional network embedding in a metric space, we propose to use the Kullback-
Leibler divergence defined on probability distributions to capture asymmetric finite and infinite
distances in directed graphs. Each node is mapped to a corresponding distribution described by some
parameters, which we learn during training. In this paper, we choose the class of exponential power
distributions due to analytical and computational merits. With minor modifications, our method can
also be applied to other distribution classes.
2
Published as a conference paper at ICLR 2020
Intuition. In Fig. 1, we use a toy example to qualitatively illustrate that our embedding can represent
infinite distances without requiring a high dimensional embedding space. More precisely, the network
consists of five groups (color-coded), with five nodes each. The two top blocks have a connection
only to the center group and, similarly, the two bottom groups have only a link from the central group
to them. Our model learns this connectivity pattern by embedding members of these groups in a
similar fashion. In this example, each node u is embedded as a 2-variate normal distribution with
mean μu = (μU, μU) and variances ∑u = diag(σ1, σ2), which can be visualized by the σ-ellipse,
the curve which represent one-σ distance from their respective means. The nodes of graphs are
embedded by minimizing the difference between the pair-wise graph distances and corresponding
Kullback-Leibler divergences between embeddings. For two distribution p(x), q(x), if p(x) q(x)
on an open subset U, the Kullback-Leibler divergence KL(p(x), q(x)) is then relatively high. In
other words, if in Figure 1 the σ-ellipse of node u is contained in or very similar to the σ-ellipse of
node v, then the embedding represents d(u, v) < ∞. Using this, we see that the embedding retrieved
by our optimization and visualized in Fig. 1 includes most of the observed infinite distances.
Statistical manifold embedding. Our embedding space X is the space of k-variate exponential
power distributions [37] (also called generalized error distributions), described by the following
probability density function for λ > 0
ψλ (Xlς, μ)
λΓ(2)	[ [(x - μ)τ∑-1(x - μ)]2
21+ λπ2 det(Σ)2Γ(k)	[	2
(1)
with mean vector μ ∈ Rk and covariance matrix Σ ∈ Rk×k. Γ(.) denotes the Gamma function
and xT denotes the transposed vector of x. Note that λ = 2 results in the multivariate Gaussian
distribution and λ = 1 yields the multivariate Laplacian distribution. As we are interested in
non-degenerate distributions, we enforce positive definite co-variance matrices and further restrict
ourselves to diagonal matrices, i.e. Σu = diag(σu1, . . . , σuk) with σui ∈ R+. With the latter, we reduce
the degrees of freedom for each node to 2k and simplify our optimization by replacing a positive
definite constraint on Σu with the constraints σui > 0. A common asymmetric function operating on
continuous random variables is the Kullback-Leibler divergence. The asymmetric distance between
nodes u and v, denoted by KLu,v , is
KLu,
KL(Pu,pV) = ʃpU log P dx
v
with Pu = ψλ(x∣∑u, μu) and Pv = ψλ(x∣∑v, μv). We approximate it with importance sampling
Monte Carlo estimation. In particular, KLu,v can be expressed as the expectation (for λ* = 2 ≤ λ)
where pu* = ψu* (x∣Σu, μu) is easy to sample and a proposal function for pu(x∣Σu, μu). If a
closed expression for KLu,v is known, its evaluation replaces the importance sampling Monte Carlo
estimation, like in the special case of λ = 2. See Appendix A.9 for more details.
Now, in order to learn these statistical manifold embeddings, we can define the loss function over
asymmetric distances D = (du,v)u,v∈v of a directed graph and {(μu, Σu)}u as:
L({(μu, ∑u)}u ) = ∑ll(i + T KLu,v )-1 - d-β ∣∣2,	⑵
u6=v
where τ ∈ R+ is a free (trainable) parameter and β ∈ R+ a fixed value. This loss function given in the
Eq. (2) is minimized during the learning process so that the KLu,v based on learned node distribution
representations captures the distances du,v in the directed graph. Empirically, we transform the given
distances into finite numbers with du-,βv ∈ [0, 1], for all u 6= v and a β ∈ R+, which can be used to
increase the differentiation between large du,v values and between finite and infinite distances. We
modify the unbounded Kullback-Leibler divergence in a similar fashion (1 + τ KLu,v)-1 ∈ [0, 1],
where τ ∈ R+ is a free (trainable) parameter and the additional 1 ensures a value in the same interval
[0,1]. We start the optimization from random initial parameters {μu, Σu}u∈v, and iteratively
minimize the loss function with stochastic gradient descent optimizers, such as Adam [28]. For small
k, which we will consider in the next section, more enhanced initialization strategies, such as using
3
Published as a conference paper at ICLR 2020
graph plotting algorithms to determine the means and exploiting memberships of strongly connected
components for the covariance initialization, probably reduce the number of training epochs.
Scalable learning procedure. The full objective function Eq. (2) consists of |V |(|V | - 1) terms and
only graphs with up to the magnitude of around 104 nodes can be applied. To extend our method
beyond this limit, we propose an approximated solution, where the size of the training data scales
linearly in the number of nodes and thus can be applied to large graphs.
This approximation solution is based on a decomposition of the loss function Eq. (2) into a neighbor-
hood term and a singularity term as:
L({(μu, Σu)}u)=	£ 11(1+ T KLu,v)-1
、半，u,v-----------------
^^^^™^^^{^^^^^^^^—
neighborhood term
- du-,βv ||22 +	||(1 + τ KLu,v)-1 - du-,βv ||22.
u6=v,du,v =∞
、
{^^^^^^^≡
singularity term
}
}
We efficiently sample from each of these sums for each node a small number of B samples and
optimize our model based on the sampled information about closeness and infinite distances.
One straightforward approach to approximate the neighborhood term is to use all direct neighbors.
Yet, the number of samples available via the directed neighbors is limited by |E |, which usually
corresponds to a small B in sparse real-world examples. Therefore, we apply for each node a
breadth-first-search into both directions until we retrieve B new samples. In this way, we obtain
smaller du,v first, and the approximation tends to the original term in the limit B → |V |.
For the singularity term, we use the topological sorting [26] of the strongly connected components.
In a topological sorting of a directed acyclic graph, all edges are from lower indices to higher indices
(with respect to the topological sorting).
Two important restrictions are noteworthy. First, the reverse statement does not hold, i.e. not all
infinite distances are given via a single topological sorting [26]. As a consequence, we won’t sample
uniformly from all infinities, but from a subset of the infinities given by the topological sorting.
Second, the construction of topological sorting is only possible for acyclic graphs, and most directed
graphs have cycles.
Since we are only interested in sampling singularities and the graph defined by the strongly connected
components of a directed graph is acyclic, we use Tarjan’s algorithm [52] to retrieve the strongly
connected components and topological sorting of them. After this preprocessing, we can efficiently
generate samples of infinite distance for the nodes.
With these two approximations we can construct two sets Uclose and U∞ and the loss function reduces
to
L= X 11(1 + τ KLu,v )-1 - d-β ||2	+ X ∣∣(1 + T KLu,v )-1 - d-β ||2.
(u,v)∈Uclose	(u,v)∈U∞
Lemma 1. Let G = (V, E) be a graph. Then our embedding has 2k|V | + 1 degrees of freedom
and one hyperparameter (β). Generating the training samples for the full method, is equivalent to
the all pair shortest path problem, which can be solved within O(|V |2 log |V | + |V ||E|) for sparse
graphs and evaluating the loss function has time complexity O(|V |2). The scalable variant has, for
B |V |, a time complexity of O((B + 1)|V | + |E|) and the loss function has O(B|V |) terms.
See Appendix A.2 for proof using Johnson’s algorithm [24] and the pseudo-code of our methods.
3	Statistical manifolds and directed graph geometry
In this section, we show the properties of our representation space and the effects on our learning
procedure.
Theorem 1. Let λ be an even number. Then the distributions with density of Eq. (1) and parametriza-
tion (σ1,. .. σk, μ1,..., μk) → X 〜ψλ(x∣∑ = diag(σ1, ..., σk), μ = (μ1, ..., μk)) with σi > 0
are a statistical manifolds S with the following properties:
1. For univariate exponential power distribution, the curvature is constant and equal to — 1∕λ.
4
Published as a conference paper at ICLR 2020
2.	the Fisher information matrix, i.e. the Riemannian metric tensor, in this coordinate system
at point ψλ(x∣∑, μ) with ∑ = diag(σ1,..., σk) and μ = (μ^1,..., μk) is given by
where
(gij )ij = diag
γ(I - 1 )λ(λ - 1)
γG)
c2 = λ.
3.	the Riemannian distance between ψλ(x∣∑, μ) and ψλ(x∣∑, μ) Is
-,..___ . ，,二一、、
dF3i(x|£, μ),以但三,μ))=
t
k
λ	arcosh2
i=1
f
1 +
∖
with c3 = J (λ-1⅞- 1).
See the Appendix A.4 for more details and proofs.
The first observation of Theorem 1 is that the geometry of the statistical manifold has constant
negative curvature (see [22; 48; 58] for detailed derivation). Negative curvature also comes as a
natural model for power-law degree distributions in complex networks [29; 38]. In Fig. 3 of Appendix
A.5 we exemplify the negative curvature of our embedding for the graph representations of Fig. 1 by
making the isometric mapping (preserving distances) to the hyperbolic space. See the Appendix A.5
for the derivation of the used isometry. Since our representation space is not flat, the Riemannian
metric is different from the Euclidean. Therefore, We need to adjust the Euclidean gradient VL
of one of our objective functions L ∈ {L, L} with respect to the metric tensor [2]. The steepest
descent direction is given by VL = G-1VL, where G-1 is the inverse of the metric tensor G of Sn
evaluated at the specific point. In other words, starting from the same representations and performing
one step into the direction of VL will always result in lower values of the loss function L than going
into the direction of VL. In our experiments, we report the results using VL. Further discussions
and details can be found in Appendix A.8.
With the last property of our embedding space, the non-Euclidean Riemannian distance, is important
for embedding based applications, like clustering. Note, that the value of c3 is approximately close to
the value 1.0 for different values of λ. It implies that after the embedding is done with one family of
distributions, we can see that the pair-wise Riemannian distance of embedded points for different
representations (λ) is scaling only by the multiplicative factor √λ (see Appendix A.4 for more details).
Also note another useful connection, the Hessian of KL divergence is the Fisher information metric
gi,j. When the Fisher distance is small, we have KL (pjλ ∣∣ Pv) ≈ 2dp (pθ,p^)2 6 dp (pjλ,pU).
Furthermore, the Fisher information metric is unique (up to scaling) and is the only Riemannian
metric on the statistical manifolds [9; 8]. Due to this fact and its connection with KL divergence, this
is the reason why we have not used other divergence functions.
4	Experiments
4.1	Datasets
From the Koblenz Network Collection [30] we retrieved three datasets of different sizes and connec-
tivity. See Table 1 for an overview.
Political blogs. The small dataset is compiled during the 2004 US election [1]. In addition, we
evaluate two larger networks with different proportions of reachability between their nodes.
Cora. [50] consists of citations between computer science publications and was used as an example
in baseline APP [59] and HOPE [39] as well.
Publication network. With a higher density but lower reciprocity, our largest example is the
publication network given by arXiv’s High Energy Physics Theory (hep-th) section [31].
5
Published as a conference paper at ICLR 2020
Table 1: Properties of datasets
Name	|V|	|E|	∣{du,v : du,v = ∞}∣∕∣V|2	Reciprocity
Synthetic example	25	30	0.48	34.3%
Political blogs	1224	19,025	0.34	24.3%
Cora	23,166	91,500	0.83	5.1%
arXiv hep-th	27,770	352,807	0.71	0.3%
4.2	Baselines
APP is the asymmetric proximity preserving graph embedding method [59] based on the skip-gram
model, which is used by many other methods like Node2Vec and DeepWalk. In contrast to the
symmetric counterparts, APP explicitly splits the representation into a source vector and a target
vector, which are updated in a direction-aware manner during the training with random walks with
reset. Their method implicitly preserves the rooted PageRank score for any two vertices.
HOPE stands for High-Order Proximity preserved Embedding [39]. This method uses a generaliza-
tion of the singular value decomposition to efficiently retrieve low-rank approximation of proximity
measures like Katz Index or rooted PageRank.
DeepWalk is the deep learning representative of the undirected graph embedding methods. It does
not differentiate between source and target and retrieves a single representation su ∈ RK for each
node. Like APP, DeepWalk uses the skip-gram model, trains the representation with random walks,
and evaluates by cosine similarity between two node representations.
Graph2Gauss trains a neural network to output mean and variance of Gaussians in a way such that a
ranking loss is minimized [5]. Their loss function concentrates on preserving the 1-hop and 2-hop
neighborhood, but not the global graph structure (du,v > 2). The method was designed for graphs
with additional node feature data, and in the absence of such information, one-hot vector encoding
should be used instead.
4.3	Set-up
In this paper, we focus on directed network embedding, which preserves global properties of directed
graphs. Considering this, our emphasis is different from the conventional evaluation tasks of network
embedding, e.g. the link prediction task only evaluates the differentiation between d(u, v) = 1 and
d(u, v) > 1, which respectively correspond to the case of link existence and not.
Evaluation metrics. Three evaluation metrics are used on the pairs of inverse true distances
and approximated values derived by learned embedding from baselines and our methods. We
use the Pearson correlation coefficient ρ for checking a linear dependency [47] and Spear-
man’s rank correlation coefficient r for evaluating the monotonic relationship. In addition
to these statistical measures, we propose to use an information-theoretic non-linear evalua-
tion metric, i.e. mutual information (MI). We choose the non-parametric k-nearest-neighbor
based MI estimator, LNC [13]. It is recently developed to overcome local non-uniformity
and can capture relationships with limited data. We briefly describe the LNC MI here as:
I = NN Pn=IlOg 总:p(y) - N Pn=IlOg V(n)j , where N is the total number of data instances, the
second term is a correction term handling non-uniform region V(n) surrounding point n, and p^(.)
represents the kNN density estimator.
Hyper-parameters. With the nonlinear interactions in our embedding, our method needs only a
small number of dimensions, and for the presented results we used an embedding into a 2-variate
normal distribution, i.e. k = 2 and the number of free parameters for each node is 4. The initial
means μu and co-variances ∑u are randomly initialized.
For the random walk based methods APP and DeepWalk, we unified both default settings with 20
random walks for each node and length 100. The embedding dimension was set to K = 4, where we
allowed APP to use two 4 dimensional vectors. In the same fashion, we set the embedding dimension
for HOPE to 4, resulting in two |V | × 4 matrices. All other parameters we left at the default value. For
6
Published as a conference paper at ICLR 2020
our proposed embedding, we evaluate both the exact and approximate version. For the approximate
one, we report the results based on B = 10 and B = 100 samples for each node.
We executed the optimization with β ∈ {4, 1, 2, 1} and consistently retrieved the best results for
β = 2. For our method, We selected from the runs the point with the highest Pearson correlation
between du-,1v and (1 + τ KLu,v)-1, which usually coincided with the minimal observed objective
function value, like in Figure 1. In the experiments, we applied Adam optimizer with learning rates in
{.001, .01, .1} and retrieved the reported results with the .1 for political blogs and .001 for the others.
HOPE was executed with GNU Octave version 4.4.1, and the other methods were executed in Python
3.6.7 and Tensorflow on a server with 258 GB RAM and a NVIDIA Titan Xp GPU.
4.4	Results
First, we compared the performance of the embedding using different λ, i.e. different classes of distri-
butions. From the results in Appendix Table 3, we observe no significant performance improvement.
So we fixed λ = 2 for the other experiments to have the performance gains of the analytical known
KL divergence.
Table 2 shows the results of evaluation metrics on different datasets. KL(∙) and KL(full) refer to the
approximate and exact version of our proposed embedding. We report the mean and standard deviation
of MI by 40 bootstrap samples. For all metrics, the higher the value, the better the performance.
The results support this intuition that our method retrieves the highest values in all cases and
significantly outperforms baselines. In addition, our approximated variant using only 10 neighbors
and 10 infinities for each node already achieves higher values than baselines. Increasing the number
of samples to 100, further enhances the performance of the approximate version, which even embeds
the large network in a similar quality as our full method. A visualization of the results is given in
Figure 2. Further details are in the Appendix A.6. Additional experiments such as dimensionality
experiment, graph reconstruction, and evaluation on undirected graph can be found in Appendix A.7.
Table 2: Results including Pearson correlation coefficient ρ, Spearman’s rank correlation coefficient
r, and mutual information (MI). The p-value for ρ and r are in all cases below 10-8. For our method,
we include the results with 10 samples, 100 samples for each node, and using the full distance matrix.
Network	APP		HOPE	DeepWalk	Graph2Gauss	KL (10)	KL (100)	KL (full)
Political	ρ	.16	.45	.25	-.17	.73	.77	.88
Blogs	r	.29	.45	.24	-.33	.71	.72	.89
	avg. MI	.15	.65	.12	.09	.47	.59	.85
	std. MI	.006	.007	.005	.005	.005	.005	.006
Cora	ρ	.10	.17	-07	-.004	.53	.66	.77
	r	.01	.41	.02	.013	.56	.62	.65
	avg. MI	.018	.13	.013	.01	.23	.35	.43
	std. MI	002	.005	.004	.003	.005	.006	.007
arXiv	ρ	.01	.20	-08	-.05	.53	.62	.68
hep-th	r	.12	.28	.04	-.06	.59	.68	.72
	avg. MI	.033	.15	.014	.01	.25	.37	.36
	std. MI	.003	.004	.003	.003	.005	.005	.005
5	Conclusion and discussion
Although the techniques for graph embedding are quite mature [16; 33; 10; 40; 14], there still exist
obstacles for using them for directed graphs. Obstacle arises from the asymmetric property of
graph geodesics and a large ratio of pairs with infinite distances. Motivated by this, we propose a
mapping of nodes to the elements of the statistical manifolds [51] by minimizing the divergence
function between embedded points a graph geodesics. This allows a single representation that
allows elegant geometrical encoding of infinite and finite distances in low-dimensional statistical
manifolds by using the divergence function. One can encode an arbitrary number of infinities into the
low-dimensional space, as this embedding does not need to push point on infinite geodesic distance
7
Published as a conference paper at ICLR 2020
ωφ⊃-ro> P①IeE.xo」dde
1	1
∞	T
XIJV l-∞
O 1

T-
1
i
WdOH

1 1
le2
1-	.. I __ τ
ol⅛⅛⅛⅛
1 1
∞∙
-eMd ①①。SSneDZqde」D
Figure 2: Visualization of approximated values w.r.t. inverse true distance (d-,V )u,v with boxplots,
representing the means, first and third quartiles as well as 1.5 interquartile range. Violin plots indicate
the kernel densities. Columns respectively correspond to dataset political blogs (first column),
Cora (second column), and arXiv hep-th (last column). Rows from top to bottom represent APP
HOPE, DeepWalk, GraPh2Gauss, and our KL method. Ideally, as the ground-truth value increases,
approximated values are expected to follow this trend as well. The baseline methods show only a
weak separation between the closest distances and all other. For our embedding method (last row)
a monotonic increasing relation is visible, which was reflected in the highest mutual information,
Pearson and Spearman,s correlation coefficient in Table 2.
8
Published as a conference paper at ICLR 2020
on a manifold. Furthermore, this embedding allows the use of analytical tools from statistics and
differential geometry, e.g. connection of curvature and natural gradient [2; 11; 48; 7]. In contrast to
the previous work, we have characterized the geometrical structure of the underlying space to which
nodes are being embedded into, and its connection to the learning via curvature corrected gradient.
Furthermore, we have proposed an efficient divergence estimation method via importance sampling
method. Theoretical understanding of the geometrical structures for directed graph embeddings
opened many interesting theoretical and practical questions. This is the reason why we had to restrict
the scope of this work only to unsupervised setting and leave link prediction and node classification
task for future work.
References
[1]	Adamic, L. A., and Glance, N. The political blogosphere and the 2004 US election: divided they
blog. In Proceedings of the 3rd international workshop on Link discovery (2005), ACM, pp. 36-43.
[2]	Amari, S.-I. Natural gradient works efficiently in learning. Neural Comput. 10, 2 (Feb. 1998), 251-276.
[3]	Belkin, M., and Niyogi, P. Laplacian eigenmaps for dimensionality reduction and data representation.
Neural Computation 15, 6 (June 2003), 1373-1396.
[4]	BOJCHEvsκι, A., and GUNNEMANN, S. DeeP gaussian embedding of attributed graphs: Unsupervised
inductive learning via ranking. In ICLR (2018).
[5]	Bojchevski, A., and Günnemann, S. Deep Gaussian embedding of graphs: Unsupervised inductive
learning via ranking. In International Conference on Learning Representations (2018).
[6]	BOURGAIN, J. On Lipschitz embedding of finite metric spaces in Hilbert space. Israel Journal of
Mathematics 52, 1-2 (Mar. 1985), 46-52.
[7]	Burbea, J., and Rao, C. Entropy differential metric, distance and divergence measures in probability
spaces: A unified approach. Journal of Multivariate Analysis 12, 4 (Dec. 1982), 575-596.
[8]	CATICHA, A. The basics of information geometry. In AIP Conference Proceedings (2015), AIP, pp. 15-26.
[9]	CENCOV, N. N. Statistical Decision Rules and Optimal Inference (Translations of Mathematical Mono-
graphs). American Mathematical Society, 2000.
[10]	Chamberlain, B. P., Clough, J. R., and Deisenroth, M. P. Neural embeddings of graphs in
hyperbolic space. CoRR abs/1705.10359 (2017).
[11]	EFRON, B. Curvature and inference for maximum likelihood estimates. Ann. Statist. 46, 4 (08 2018),
1664-1692.
[12]	Frankl, P., and Maehara, H. The Johnson-Lindenstrauss lemma and the sphericity of some graphs.
Journal of Combinatorial Theory, Series B 44, 3 (June 1988), 355-362.
[13]	Gao, S., Ver Steeg, G., and Galstyan, A. Efficient estimation of mutual information for strongly
dependent variables. In Artificial Intelligence and Statistics (2015), pp. 277-286.
[14]	GROVER, A., AND LESKOVEC, J. Node2Vec: Scalable feature learning for networks. In Proceedings of
the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (New York,
NY, USA, 2016), KDD ’16, ACM, pp. 855-864.
[15]	Gu, A., Sala, F., Gunel, B., and Re, C. Learning mixed-curvature representations in product spaces.
In ICLR (2019).
[16]	Hamilton, W. L., Ying, R., and Leskovec, J. Representation learning on graphs: Methods and
applications, 2017. cite arxiv:1709.05584Comment: Published in the IEEE Data Engineering Bulletin,
September 2017; version with minor corrections.
[17]	Hamilton, W. L., Ying, Z., and Leskovec, J. Inductive representation learning on large graphs. In
NIPS (2017).
[18]	Hamsterster full network dataset - KONECT, Sept. 2016.
[19]	Hayter, R. The hyperbolic plane “a strange new universe”, 2008. "Technical report, accessed May
2019".
9
Published as a conference paper at ICLR 2020
[20]	He, S., Liu, K., Ji, G., and Zhao, J. Learning to represent knowledge graphs with Gaussian embedding.
In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management
(New York, NY, USA, 2015), CIKM ,15, ACM,pp. 623-632.
[21]	HINTON, G., AND ROWEIS, S. Stochastic neighbor embedding. Advances in neural information processing
systems 15 (2003), 833-840.
[22]	ICHI AMARI, S. Differential-Geometrical Methods in Statistics. Springer New York, 1985.
[23]	Jianchao Yang, Shuicheng Yang, Yun Fu, Xuelong Li, and Huang, T. Non-negative graph
embedding. In 2008 IEEE Conference on Computer Vision and Pattern Recognition (June 2008), pp. 1-8.
[24]	JOHNSON, D. B. Efficient algorithms for shortest paths in sparse networks. Journal of the ACM (JACM)
24, 1 (1977), 1-13.
[25]	Johnson, W. B., Lindenstrauss, J., and Schechtman, G. Extensions of Lipschitz maps into
Banach spaces. Israel Journal of Mathematics 54, 2 (June 1986), 129-138.
[26]	KAHN, A. B. Topological sorting of large networks. Communications of the ACM 5, 11 (Nov. 1962),
558-562.
[27]	Khosla, M., Leonhardt, J., Nejdl, W., and Anand, A. Node representation learning for directed
graphs. CoRR abs/1810.09176 (2018).
[28]	KINGMA, D. P., AND BA, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980
(2014).
[29]	Krioukov,	D.,	Papadopoulos,	F.,	Kitsak,	M.,	Vahdat,	A., and	Boguna,	M. Hyperbolic
geometry of complex networks. Phys. Rev. E 82 (Sep 2010), 036106.
[30]	KuNEGis, J. Konect: the Koblenz network collection. in Proceedings of the 22nd International Conference
on World Wide Web (2013), ACM, pp. 1343-1350.
[31]	Leskovec, J., Kleinberg, J., and Faloutsos, C. Graph evolution: Densification and shrinking
diameters. ACM Transactions on Knowledge Discovery from Data (TKDD) 1, 1 (2007), 2.
[32]	Linial, N., London, E., and Rabinovich, Y. The geometry of graphs and some of its algorithmic
applications. Combinatorica 15, 2 (June 1995), 215-245.
[33]	Liu, X., Murata, T., Kim, K.-s., Kotarasu, C., and Zhuang, C. A general view for network
embedding as matrix factorization. in Proceedings of the Twelfth ACM International Conference on Web
Search and Data Mining (New York, NY, usA, 2019), WsDM ’19, ACM, pp. 375-383.
[34]	Lovasz, L., and Vesztergombi, K. Geometric representations of graphs. In INPAUL ERDOS, PROC.
CONF (1999), springer-Verlag, pp. 17-3.
[35]	Muscoloni, A., Thomas, J. M., Ciucci, s., Bianconi, G., and Cannistraci, C. V. Machine
learning meets complex networks via coalescent embedding in the hyperbolic space. Nature Communica-
tions 8, 1 (Nov. 2017).
[36]	Muzellec, B., and Cuturi, M. Generalizing point embeddings using the Wasserstein space of
elliptical distributions. in Proceedings of the 32Nd International Conference on Neural Information
Processing Systems (usA, 2018), Nips’18, Curran Associates inc., pp. 10258-10269.
[37]	NADARAJAH, s. A generalized normal distribution. Journal of Applied Statistics 32, 7 (sept. 2005),
685-694.
[38]	NARAYAN, o., AND sANiEE, i. Large-scale curvature of networks. Phys. Rev. E 84 (Dec 2011), 066108.
[39]	Ou, M., Cui, p., Pei, J., Zhang, z., and Zhu, W. Asymmetric transitivity preserving graph embedding.
in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining - KDD 16 (2016), ACM press.
[40]	Perozzi, B., Al-Rfou, R., and s kiena, s. DeepWalk: online learning of social representations.
in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (New York, NY, usA, 2014), KDD ’14, ACM, pp. 701-710.
[41]	PEYRE, G., Cuturi, M., ET al. Computational optimal transport. Foundations and Trends® in Machine
Learning 11, 5-6 (2019), 355-607.
10
Published as a conference paper at ICLR 2020
[42]	Potamias, M., B onchi, F., Castillo, C., and Gionis, A. Fast shortest path distance estimation in
large networks. In Proceedings of the 18th ACM Conference on Information and Knowledge Management
(New York, NY, USA, 2009), CIKM ,09, ACM,pp. 867-876.
[43]	Rattigan, M. J., Maier, M., and Jensen, D. Using structure indices for efficient approximation of
network properties. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (New York, NY, USA, 2006), KDD ’06, ACM, pp. 357-366.
[44]	ROWEIS, S. T. Nonlinear dimensionality reduction by locally linear embedding. Science 290, 5500 (Dec.
2000), 2323-2326.
[45]	Rudolph, M., Ruiz, F. J. R., Mandt, S., and Blei, D. M. Exponential family embeddings. In
Proceedings of the 30th International Conference on Neural Information Processing Systems (USA, 2016),
NIPS’16, Curran Associates Inc., pp. 478-486.
[46]	S aerens, M., Fouss, F., Yen, L., and Dupont, P. The principal components analysis of a graph, and
its relationships to spectral clustering. In Machine Learning: ECML 2004 (Berlin, Heidelberg, 2004), J.-F.
Boulicaut, F. Esposito, F. Giannotti, and D. Pedreschi, Eds., Springer Berlin Heidelberg, pp. 371-383.
[47]	SHALEV-SHWARTZ, S., AND BEN-DAVID, S. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
[48]	SKOVGAARD, L. T. A Riemannian geometry of the multivariate normal model. Scandinavian Journal of
Statistics 11, 4 (1984), 211-223.
[49]	S taley, M. D. Models of hyperbolic geometry, 2019. "Technical report, accessed May 2019".
[50]	Subelj, L., and Bajec, M. Model of complex networks based on citation dynamics. In Proceedings of
the 22nd international conference on World Wide Web (2013), ACM, pp. 527-530.
[51]	Sun, K., and Marchand-Maillet, S. An information geometry of statistical manifold learning. In
Proceedings of the 31st International Conference on International Conference on Machine Learning -
Volume 32 (2014), ICML’14, jMLR.org, pp. II-1-II-9.
[52]	TARjAN, R. Depth-first search and linear graph algorithms. SIAM Journal on Computing 1, 2 (1972),
146-160.
[53]	Tenenbaum, j., Silva, V., and Langford, j. A global geometric framework for nonlinear dimen-
sionality reduction. Science 290, 5500 (2000), 2319-2323.
[54]	Tsitsulin, A., Mottin, D., Karras, P., and Muller, E. Verse: Versatile graph embeddings from
similarity measures. In Proceedings of the 2018 World Wide Web Conference (2018), International World
Wide Web Conferences steering Committee, pp. 539-548.
[55]	VAN DER MAATEN, L., AND HINTON, G. Visualizing data using t-sNE. Journal of Machine Learning
Research 9 (2008), 2579-2605.
[56]	VILNIs, L., AND MCCALLuM, A. Word representations via Gaussian embedding. In ICLR (2015).
[57]	Wilson, R. C., Hancock, E. R., Pekalska, E., and Duin, R. P. W. spherical and hyperbolic
embeddings of data. IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 11 (Nov 2014),
2255-2269.
[58]	YuAN, M. On the geometric structure of some statistical manifolds. arXiv preprint arXiv:1902.06144
(2019).
[59]	Zhou, C., Liu, Y., Liu, X., Liu, Z., and Gao, j. scalable graph embedding for asymmetric proximity.
In AAAI (2017), s. P. singh and s. Markovitch, Eds., AAAI Press, pp. 2942-2948.
11
Published as a conference paper at ICLR 2020
A	Appendix
A. 1 Limits of low-dimensional metric representations
Definition 1. We define the following metric function du,v = min(d%v, dv,u).
Definition 2. For α > 1, we say that the directed graph G = (V, E) is α-asymmetric if ∀u, v : du,v ≤ αdu,v
(control of maximal asymmetry).
The following Lemma can be viewed as the corollary of Bourgain theorem. It implies that distortion w.r.t.
directed graph is is not finite when α = ∞. Note that the ∞-asymmetry is quite common in real directed
networks, see Table 1.
Lemma 2. If a graph is α-asymmetric, then there exists a metric representations that uses random subsets
embedding (φ : v 7→ xv) in m = dlog(n)e dimensional space, such that for every pair (u, v) the following
bounds: ∣∣Xu — Xv ||i / du,v ≤ m and E[∣∣Xu — Xv ∣∣ι]/ du,v ≥ y6α hold, where ∣∣.∣∣ι denotes the L1 norm and
E[.] is the expectation operator over random subsets.
Proof. Bourgain theorem (1985) [6] constructs an O(log(n))-dimensional embedding (φ : v 7→ Xv) of an
undirected graph with n nodes to Euclidean space with O(log(n)) distortion by using random subset projection.
Distortion is defined as the ratio D = maxu,v∈V Fxu-Xv[h/du,v , where &“ denotes the shortest path in a graph
minu,v∈V ||xu -xv ||1 /du,v	,
and ||Xu — Xv ||1 the L1 distance between the embedded points. For directed graphs du,v is not symmetric and
we propose to use the metric ddu,v = min(du,v, dv,u) function, which fulfills the properties of non-negativity,
symmetry and triangle inequality (proof omitted).
It is possible to construct m = dlog(n)e random subsets Ai ⊆ V , where each node from V is put inside with
the probability 1/2i. The following embedding for node j can be constructed as Xj = (d(j, A1), ..., d(j, Am)),
where d(j, A1) denotes the distance from node j to set A1. Then, the following L1 bounds [34] for Bourgain
theorem [6] for every pair (u, v) hold
||Xu — Xv||1 ≤ mddu,v
and
E[||Xu — Xv||1] ≥ ddu,v/16,
where ||.||1 denotes the L1 norm and E[.] the expectation operator over random subset.
Together with definition of d and α-asymmetry gives ∣∣Xu - Xv∣∣ι∕du,v ≤ m and E[∣∣Xu — Xv∣∣ι]/du,v ≥ 16α∙
It implies that distortion w.r.t. directed graph is D ≥ 冲遂片旷 Uxu-：；k∕du,v ≥ maχu,v∈V Uxu-xv h/du.v is
EIlxu-xv Il 1 /du,v	i6α
not finite when α = ∞.	□
A.2 Proof of Lemma 1
Lemma. Let G = (V, E) be a graph. Then our embedding has 2k|V | + 1 degrees of freedom and one
hyperparameter (β). Generating the training samples for the full method, is equivalent to the all pair shortest
path problem, which can be solved within O(|V |2 log |V | + |V ||E|) for sparse graphs and evaluating the
loss function has time complexity O(|V |2). The scalable variant has for B	|V | a time complexity of
O((B + 1)|V | + |E|) and the loss function has O(B|V |) terms.
Proof. The embedding maps each node U to a k-variate normal distribution Nu with mean μu = (μU ,...,μU)
and covariance Σu = diag(σu1 , . . . σuk), which are in total 2k|V | parameters. With the trainable τ in our
objective function, we have in total 2k|V | + 1 degrees of freedom.
Johnson’s algorithm [24] solves the problem of all pair shortest path length in O(|V |2 log |V | + |V ||E|). The
result are the |V |2 distances, which are used in the full loss function.
For the scalable variant, we need to perform Tarjan’s algorithm [52] one time, which has time complexity
of O(|V | + |E|). This assigns every node its strongly connected component and at the same time returns
a topological sorting of the strongly connected component. Using this, the sampling of infinities reduces to
sampling from an array, which needs for B samples O(B). The neighborhood terms can be retrieved using a
breadth-first search, which stops after finding B new samples. Under the assumption of B |V |, the total time
complexity is O((B + 1)|V | + |E|). Finally, the approximated loss function uses two sums over each B|V |
elements.	□
12
Published as a conference paper at ICLR 2020
A.3	Algorithm
To clarify our two algorithms, we include the pseudo-code of the full algorithm (Algorithm 1) and the scalable
variant (Algorithm 2). The main differences between Algorithm 2 and Algorithm 1 is the reduced amount of
pre-processing needed and consequently the reduced amount of elements in the loss function L. Lemma 1 gives
the complexity for pre-processing and an optimization step.
Algorithm 1 Full algorithm
Input: Graph G = (V, E)
1:	Pre-processing: Calculate all graph distances du,v, for all u, v ∈ V
2:	Initialization: Uniformly sample random values for distribution parameters μu, ∑u for each node
u∈V
3:	Optimizing: Update μu, ∑u, T using Adam optimizer and loss function L
Algorithm 2 Scalable algorithm
Input: Graph G = (V, E)
1:	Pre-processing: Calculate the strongly connected components and their topological sorting using
Tarjans algorithm. Sample for each node u ∈ V up to B nodes v with du,v = ∞.
2:	Pre-processing: Perform for each node u a breadth-first-search for the successors and another for
the predecessors to sample up to B nodes with du,v < ∞
3:	Initialization: Uniformly sample random values for distribution parameters μu, ∑u for each node
u∈V
4:	Optimizing: Update μu, ∑u, T using Adam optimizer and loss function L
A.4 Proof of Theorem 1
Let us consider a surface S = {y (μ,σ) : (μ,σ) ∈ R × R+} of natural logarithms of PDFs of univariate
exponential power distributions parameterized by their expectation and deviation, y (μ, σ) = ln f (x| (μ, σ))=
ln Qσrλl∕λ) e-(--μ) ). For given μ, σ, λ let X have an exponential power distribution with those parameters
and define the inner product at (μ,σ) as hy1∣y2i (μ, σ) := E [ln (p1 (X∣μ, σ)) ∙ ln(p2 (X∣μ,σ))]. Let us
calculate the metric coefficients:
g11 (aμ, σ) = — E
a2λ (λ - 1)
σ2
=a2λ 1) E h(X — aμ)λ-2i
= α2X (λ - 1) 广一泞(λ-1) = a2X (λ - 1) γ (1 - 1)
=rτi^ = ʒ2	Γ7ir,
odd
a 、— a 、—百	aλ2 (X - μ)λ 1 — aλ2 何( γ xλ — { I _ ∩
g12 (aμ,σ) =g21 (aμ,σ) = -E-σλ+ι- = σλ+ι E (X - μ)	= 0,
g22 (aμ,σ) = - E σ -λ (λ+1) ɪ~0入+，,
- σ12+”+/)E h(X - aμ)λi
工+》(入+1)r(牛)=ʌ
σ2	σ2	Γ (1)	σ2
We see that that for a
γ( 1)
(λ-1)r(1-1)
the metric tensor at (aμ, σ) is given by
gF (aμ, σ) = λ
σ-2	0
0	σ-2
λgH2 (μ, σ) .
13
Published as a conference paper at ICLR 2020
Similarly, for a multivariate exponential power distribution With parameters μ = (μι,...,μk), Σ =
diag (σι,...,σk) ,λ, we get gF (aμ, Σ) = λdiag (σ-2,...,σ-2,σ-2,...,σ-2,) = λgR2k (μ, Σ) so we
get
∣∣x (aμ, ∑)∣∣f =pxτgF (aμ, ∑) x = PxTλgH (μ, Σ) X = √λ∣∣x (μ, ∑)∣∣r2 ⇒
dF (x (aμι, ∑ι) , X (aμ2, ∑2))=仄&且2卜((μι, ∑ι) , (μ2, ∑2))
=λ λ XX arcosh2 (1 + 仙1 - μi2)2+ 31 - σi2)2 )
N 占 卜 +	2σi1σi2	/
from which it follows that
dF (X (μι, ∑ι) , x (μ2, ∑2))

λXarcosh2 (1+(-)2 + 31-°扪2
i=1	2σi1σi2
The detailed derivation of the curvature for the univariate case is given in [58], where the closed form solution
for α-Gaussian curvature is given. In this paper, we are interested only in the case of α = 0 Gaussian curvature
of the Riemannian metric, which leads to the — 1∕λ curvature.
A.5	Derivation of mapping from Gaussian statistical manifold to hyperboloid
To see how the points on the manifold relate to each other in terms of distance, we will map them to the upper half
of a two-sheathed hyperboloid (see Figure 3) while preserving their distance up to a multiplicative constant factor.
To do this, we first map a point (μ, θ) to the POinCar6 half-plane through the mapping (μ, θ) → (μ/vz2, θ),
which can be shown to be a similarity with the similarity coefficient Vz2 [7]. Then we isometrically map the
Poincar6 half-plane to the Poincar6 disc by using the Cayley mapping [19]. We finish by mapping the Poincar6
disc to the above-mentioned hyperboloid by using the inverse of the stereographic projection, which can be
shown is also an isometry [49], where the distance of two points on the hyperboloid is the length of the curve
which is an intersection of the hyperboloid and the plane passing through the origin and those two points. Note
that the composition of these three maps is also a similarity with the similarity coefficient vz2.
We have combined existing [7; 19; 49], well-known results about similarities, isometries and geodesics being
mapped by those maps between the Gaussian stochastic manifold and various models for the hyperbolic geometry
(Poincar6 half-plane, Poincar6 disc and a two-sheathed hyperboloid), using minor adjustments to suit our needs
when necessary, to allow easy, direct visualisation of distances between the points on the stochastic manifold in
the natural framework in which they were defined (hyperbolic geometry).
It is a well-know fact that the Cayley map (x,y) → (χχ++y+-12,忆2/；： 1)2 ) is an isometry from the
Poincar6 half-plane to the Poincar6 disc. It is also known that the stereographic projection of the upper
half of the two-sheathed hyperboloid z2 - x2 - y2 = 1 through the point (0, 0, -1) to the plane z = 0
is an isometry from the hyperboloid onto the Poincar6 disc, so it’s inverse, which can be computed to be
(x,y) → ( I X =, / y =, / 1	= ), is also an isometry. To obtain the inverse, we must find t
1-x2-y2	1-x2 -y2	1-x2-y2
such that the point t ((x, y, 0) - (0, 0, -1)) = (tx, ty, t) lies on the hyperboloid, i.e. t2 - (tx)2 + (ty)2 = 1
⇒ t = / 1	= since we,re interested only in the upper sheath. From here, we read that the inverse of the
1-x2-y2
stereographic projection is as stated above.
Finally, it can be shown that a similarity f : S1 → S2 maps a geodesic α on S1 to a curve f ◦ α on S2 which
can be reparameterized by arc length and such reparameterization f ◦ α ◦夕 is a geodesic on S2. If f has the
similarity coefficient c, then it is easy to see that dS1 (x, y) = cdS2 (f (x) , f (y)) by using those two geodesics
and the fact that geodesics are shortest lines between points they pass through.
A.6 Further details of experiments
For baseline HOPE, as input HOPE operates on any proximity measure S in the form S = Mg-1 Ml, we use
HOPE with Mg = I the identity matrix and Ml = ((1 - δu,v)(du,v +ε)-1)u,v the matrix of element-wise
inverse distances shifted by a small ε = 10-6 to avoid division by zero. The output of HOPE are two matrices
US ∈ RlVl×κ, Ut ∈ Rκ×lVl and the approximated values are calculated via US ∙ Ut.
The initial means μU are drawn uniformly from [0,10] and the initial co-variances σU are drawn uniformly from
[4, 7]. As initial value of τ we selected 2.5.
14
Published as a conference paper at ICLR 2020
Figure 3: Visualization of the hyperboloid geometry of directed graphs for one-dimensional Gaussian
distributions. The coordinate frame is shown With the green (σ) and orange lines (μ). The nodes from
the synthetic directed network from Figure 1 are shown as points with different colors of groups,
depending on the position in the network. The embeddings for nodes (μ, σ) in the statistical manifold
are found by minimizing the objective function (2) and then making the mappings to the hyperboloid
model.
Table 3: Results of the exponential power distributions for λ ∈ {2, 4, 8} and 100 samples evaluated with Pearson correlation coefficient ρ, Spearman’s rank correlation coefficient r and mutual infor- mation (MI). ρ and r were evaluated on all edges (u, v) with u 6= v using the values given by each method and the ground truth distance. The p-value for ρ and r are in all cases below 10-8.				
Network	λ = 2		λ=4	λ=8
Political	ρ	.77	.78	.78
Blogs	r	.74	.74	.72
	MI	.57 ±.005	.61 ± .005	.61 ±.005
For our full method, we used no batching for the political blogs network, 410 batches for Cora and 2777 batches
for arXiv hep-th with and without shuffling between each epoch, where we saw an increased performance
with shuffling especially for the larger datasets. The approximated approach uses no batch for the variant with
B = 10 and 10 batches for B = 100.
The computation time of pre-processing and learning for (KL 10, KL 100, KL full) experiments were on blogs
(10sec/1.5min/10min), Cora (15sec/2min/5.5hours) and arXiv hep-th (30sec/3min/1.5day).
A.7 Further evaluation experiments
In this section, we report further results of experiments with different hyperparameters, another baseline for
political blogs as well as the down-stream task graph reconstruction and experiments on an undirected graph.
First, we want to verify different hyperparameters of our method. Table 3 shows the results for different
exponential power distributions, i.e. λ ∈ {2, 4, 8}. The results were calculated using the proposed importance
sampling, see Appendix A.9 for more details. Since for higher λ the evaluation measures do not increase
significantly, we have fixed λ = 2 for the other experiments, due to the analytical KL divergence in that case.
Another hyperparameter is the embedding dimension, which translates to selecting of k-variate exponential
power distributions. Table 4 reports the results for k ∈ {2, 5, 10, 50}. Up to the value k = 10 all values increase,
whereby in particular the mutual information registers the strongest increase. A further increase of the dimension
to k=50 does not seem to bring any further advantage. Since the focus of this publication is on low-dimensional
embeddings, we have set k = 2.
15
Published as a conference paper at ICLR 2020
Table 4: Results of the k-variate exponential power distributions for k ∈ {2, 5, 10, 50} of KL (full)
evaluated with Pearson correlation coefficient ρ, Spearman’s rank correlation coefficient r and mutual
information (MI). ρ and r were evaluated on all edges (u, v) with u 6= v using the values given by
each method and the ground truth distance. The P-ValUe for P and r are in all cases below 10-8.
Network	k=2	k = 5	k= 10	k = 50
Political ρ	.88	.90	.91	.88
Blogs	r	.89	.90	.92	.90
MI	.85 ± .006	.90 ±.007	.97 ±.006	.90 ±.006
Table 5: Comparision of results of elliptical embedding on political blogs and our method
Method	P	r	MI
Elliptical	-0.17	-0.14	.04± .004
KL (full)	0.88	0.89	.85± .006
APP	0.16	0.29	.15± .006
HOPE	0.45	0.45	.65± .007
DeepWalk	0.25	0.24	.12± .005
Graph2Gauss	-0.17	-0.33	.09± .005
For the Political blogs network, we have evaluated as an additional baseline the elliPtical embedding [36]. The
work [36] of Muzellec et al. studies the problem of embedding objects as elliptical probability distributions,
which are the generalization of Gaussian multivariate densities. Their work is rooted in the optimal transport
theory by using the Wasserstein distance. The physical interpretation of this distance in the case of optimal
transport is given by the cost of moving mass from one distribution to another one (see Monge-Kantorovich
transportation problem [41]). In particular, for univariate Gaussian distributions, the Wasserstein distance
between embedded points becomes the two-dimensional Euclidean distance of (μι,σι) and (μ2,σ2), i.e. flat
geometry. In our case, the geometry of univariate Gaussians has constant negative curvature, and distance is
measured with the Fisher distance. Our work is rooted in statistical manifold theory, where the Fisher distances
arise from measuring the distinguishability between different distributions. According to the results in Table 5,
we observe that this method is not outperforming our method.
Our method was created with the motivation to embed directed graphs. However, we also evaluated the perfor-
mance on an undirected example. We have selected Hamsterster [18] from the Koblenz Network Collection [30].
With 2 426 nodes and 16 631 edges this friendship network has a medium size. The results in Table 6 show
that even in this scenario our full method outperforms the others. Our intuition to these results is the following:
Although KL divergence is an asymmetric function, in special cases it can also become symmetric. E.g. in
the case of two Gaussian distributions with equal standard deviations, the KL divergence is symmetric. This
demonstrates the generality of representation. Note that we did not set the additional equality constraint on the
standard deviation parameters in the learning phase for this experiment. However, this only works, if all data is
available and thus our approximation method does not generalize in a similar quality as seen before from the
training samples to the full data set in the undirected case.
As last additional experiment, we have verified the performance of our method on the down-stream task graph
reconstruction. In particular, for every node u with outdegree θu , we retrieve the θu best candidate successors
Nθouut (u) from each embedding and compare them to the direct successors of node u. To assess the performance,
precision is computed
P ..out	Pu NθUt(U) ∩ SUcceSSorS(U)
Precision = ——U--------u----------------------.
|E|
Table 6: ReSUItS on UndireCted network HamSterSter
Network	APP		HOPE	DeepWalk	Graph2Gauss	KL (10)	KL (100)	KL (full)
Hamsterster	ρ	-.03	.23	.36	-.26	.17	.19	.91
	r	.45	.37	.37	-.76	.12	.12	.89
	avg. MI	.29	.43	.10	.45	.63	.64	.89
	std. MI	.006	.007	.006	.005	.005	.004	.005
16
Published as a conference paper at ICLR 2020
Table 7: Results of graph reconstruction for the political blogs network
Method	out-degree precision	in-degre precision
APP	0.1077	0.2624
HOPE	0.1010	0.1125
DeepWalk	0.2620	0.1669
Graph2Gauss	0.0258	0.0003
Elliptical	0.0383	0.0250
KL (full)	0.2861	0.2329
Figure 4: Relative effect of correction ▽ L in comparison to VL. Starting from the same representa-
tion, we optimized with gradient descent optimization with VL and VL with learning rate 1 * 10-6.
Then the relative improvement "“。黑；[LL was calculated and averaged over 1000 random initial-
ization, where ∆v∑L is the value of our loss function after applying gradient descent with gradient
VL. For this experimented we generated directed random networks with the ErdoS-Renyi model for
p = .15 andn ∈ {50, 100, 250, 500, 750, 1000}.
This tells us the ratio of actual neighboring links the embedding can extract. The same is done for incoming
links, using the in-degree of each node. This kind of task was previously used in several papers that study graph
embeddings e.g. [54] and [27].
Table 7 shows the resulting in- and out-precisions for the political blogs network. Although, on average, our
method (KL) is performing well, it was not designed to encode only the local neighborhood but rather the whole
spectrum of finite and infinite distances. Our representation is designed for preserving the global geodesic
information of directed graphs in an unsupervised setting. To excel at other supervised tasks, one would have to
modify the loss function to include additional terms more suitable for that down-streaming task.
A.8 Correction of Euclidean gradient
In section 3, we have deduced that the steepest direction is given by 章L = GT VL. In other words, in each step
of our optimization, we need to correct [2] the Euclidean gradient by the inverse of Fisher information matrix at
the respective point. To be more specific, to update the representation of a node with σ1,...σk,μ1,...,μk we
apply
V ɪ L = (σ-)- V ɪ L,
∂σi	Cl	∂σi
V ɪ L = (σ-)- V ɪ L,
∂μi	C-	∂μi
where C1 and C- are the constants from Theorem 1. As we can see in Figure 4, the relative improvement of the
corrected gradient decreases with the network size.
17
Published as a conference paper at ICLR 2020
A.9 Importance sampling
As there exists no closed form of KL divergence for the generalized exponential power distributions, we propose
an efficient importance sampling Monte Carlo estimation, which is applicable for an even more distributions. In
particular for this class of exponential power distribution, we perform
KLu,v
KL(pλu (x), qvλ (x))
E
Ex 〜Pu (x)
log
PUU (χ)
q*(x)
However, one can also re-write the integral in order to use the easy sampling function ψλ* (x):
Φpλ≡log 韬dx
Ex〜ψλ* (x)
∣^ Pλ (X) lc∕λ (X)I
fc砌log西]
For the proposal function, we will use the same class of distributions with parameter λ* = 2. When λ ≥ λ*
due to the properties of exponential power distributions, the proposal distribution is concentrated around the
mean from target distribution puλ(X) so the variance of estimator is relatively low. Finally, the KL divergence is
approximated with m samples X1, ...Xm from ψλ* (X):
Ex~ψλ* (x)
[ψλ⅛X)iog Pλ≡] ≈ ml X [IM
In the special case of λ = 2 the distributions are multivariate Gaussians and the KL divergence has an analytical
form. For two nodes u,v ∈ V represented with (Σn, μu) respectively (Σv, μv) we have
v
u
KLu,v = 1 t tr(Σ-1∑u) + (μv - 〃“)TΣ-1(μv - μU) - k + ln detf
2	det Σ
18