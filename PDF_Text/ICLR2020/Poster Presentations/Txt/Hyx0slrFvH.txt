Published as a conference paper at ICLR 2020
Mixed Precision DNNs:
All you need is a good parametrization
Stefan Uhlich； Lukas Mauch； Fabien Cardinaux； Kazuki Yoshiyama
Javier Alonso Garcia, Stephen Tiedemann, Thomas Kemp
Sony Europe B.V., Germany
firstname.lastname@sony.com
Akira Nakamura
Sony Corporate, Japan
akira.b.nakamura@sony.com
Ab stract
Efficient deep neural network (DNN) inference on mobile or embedded devices typically involves
quantization of the network parameters and activations. In particular, mixed precision networks
achieve better performance than networks with homogeneous bitwidth for the same size constraint.
Since choosing the optimal bitwidths is not straight forward, training methods, which can learn
them, are desirable. Differentiable quantization with straight-through gradients allows to learn the
quantizer’s parameters using gradient methods. We show that a suited parametrization of the quantizer
is the key to achieve a stable training and a good final performance. Specifically, we propose to
parametrize the quantizer with the step size and dynamic range. The bitwidth can then be inferred
from them. Other parametrizations, which explicitly use the bitwidth, consistently perform worse. We
confirm our findings with experiments on CIFAR-10 and ImageNet and we obtain mixed precision
DNNs with learned quantization parameters, achieving state-of-the-art performance.
1	Introduction
Quantized DNNs apply quantizers Q : R → {q1, ..., qI} to discretize the weights and/or activations
of a DNN (Han et al., 2015; Zhou et al., 2017; Li et al., 2016; Liu & Mattina, 2019; Cardinaux
et al., 2018; Jain et al., 2019; Bai et al., 2018). They require considerably less memory and have a
lower computational complexity, since discretized values {q1, ..., qI} can be stored, multiplied and
accumulated efficiently. This is particularly relevant for inference on mobile or embedded devices
with limited computational power.
However, gradient based training of quantized DNNs is difficult, as the gradient of a quantization
function vanishes almost everywhere, i.e., backpropagation through a quantized DNN almost always
returns a zero gradient. Different solutions to this problem have been proposed in the literature:
A first possibility is to use DNNs with stochastic weights from a categorical distribution and to
optimize the evidence lower bound (ELBO) to obtain an estimate of the posterior distribution of
the weights. As proposed in (Jang et al., 2016; Maddison et al., 2016; Louizos et al., 2019), the
categorical distribution can be relaxed to a concrete distribution - a smoothed approximation of the
categorical distribution - such that the ELBO becomes differentiable under reparametrization.
A second possibility is to use the straight through estimator (STE) (Bengio et al., 2013). STE allows
the gradients to be backpropagated through the quantizers and, thus, the network weights can be
adapted with standard gradient descent (Hubara et al., 2016). Compared to STE based methods,
stochastic methods suffer from large gradient variance, which makes training of large quantized
DNNs difficult. Therefore, STE based methods are more popular in practice.
More recent research (Jain et al., 2019; Esser et al., 2019; Wang et al., 2018; Elthakeb et al., 2018)
focuses on methods which can also learn the optimal quantization parameters, e.g., the stepsize,
dynamic range and bitwidth, in parallel to the network weights. This is a promising approach as
DNNs with learned quantization parameters almost always outperform DNNs with handcrafted ones.
* Equal contribution.
1
Published as a conference paper at ICLR 2020
Recently, and in parallel to our work, (Jain et al., 2019) explored the use of STE to define the gradient
with respect to the quantizers’s dynamic range. The authors applied a per-tensor quantization and
used the dynamic range as an additional trainable parameter also learned with gradient descent.
Similarly, (Esser et al., 2019) learned the stepsize using gradient descent. However, neither of them
learned the optimal bitwidth of the quantizers.
One approach was proposed in (Wang et al., 2018; Elthakeb et al., 2018). They learn the bitwidth
with reinforcement learning, i.e., they learn an optimal bitwidth assignment policy. Their experiments
show that a DNN with a learned and heterogeneous bitwidth assignment outperforms quantized
DNNs with a homogeneous bitwidth assignment. However, such methods have a high computational
complexity as the bitwidth policy must be learned, which involves training many quantized DNNs.
In this paper, we will use the STE approach and show that the quantizer’s parameters, including the
bitwidth, can be learned with gradient methods if a good parametrization is chosen. Specifically,
we show that directly learning the bitwidth is not optimal. Instead, we propose to learn the stepsize
and dynamic range. The bitwidth can then be inferred from them. Compared to (Wang et al., 2018;
Elthakeb et al., 2018), our method has the advantage that training quantized DNNs has nearly the
same computational complexity as standard float32 training.
The contributions of this paper are:
1.	We show that there are three different parametrizations for uniform and power-of-two quantiza-
tion and that, in both cases, one of them has gradients particularly well suited to train quantized DNNs.
The other parametrizations have the problem of yielding gradients with an unbounded gradient norm
and coupled components.
2.	Using this parametrization, we are able to learn all quantization parameters for DNNs with
per-tensor quantization and global memory constraints. We formulate the training as a constrained
optimization problem, where the quantized DNN is constrained not to exceed a given overall memory
budget, and show how to solve it in a penalty framework.
3.	We confirm our findings with experiments on CIFAR-10 and ImageNet. For example, we train
a heterogeneously quantized MobileNetV2 on ImageNet requiring a total of only 1.65MB to store the
weights and only 0.57MB to store its largest feature map. This is equivalent to a homogenous 4bit
quantization of both weights and activations. However, our network learns to allocate the bitwidth het-
erogeneously in an optimal way. Our MobileNetV2 achieves an error of 30.26% compared to 29.82%
for the floating point baseline. This is state-of-the-art for such a heavily quantized MobileNetV2.
We use the following notation throughout this paper: x, x, X and X denote a scalar, a (column)
vector, a matrix and a tensor with three or four dimensions, respectively; b.c and d.e are the floor and
ceiling operators. Finally, δ(.) denotes the Dirac delta function.
2 Choosing a quantization parametrization
Let Q(x; θ) be a quantizer with the parameters θ, which maps x ∈ R to discrete values {q1, ..., qI}.
In this section, we compare different parametrizations of Q(x; θ) for uniform quantization and power-
of-two quantization and analyze how well the corresponding straight-through gradient estimates
∂χQ(χ; θ) and ▽6Q(χ; θ) are suited to optimize the quantizer parameters θ. Our key result is, that
the training of quantized DNNs which learns both, the optimal quantized weights and the optimal
quantization parameters θ, is very sensitive to the choice of the parametrization of the quantizers.
From an optimization point of view, it is best to parametrize the quantizer Q(x; θ) with the stepsize
d and the dynamic range qmax as it leads to gradients with stable norms. Doing so, we can use
standard gradient descent to learn the quantization parameters and do not need to use stochastic or
reinforcement based algorithms, which are computationally expensive.
2.1	Parametrization and straight through gradient estimates
A symmetric uniform quantizer QU (x; θ) which maps a real value x ∈ R to one of I = 2k + 1
quantized values q ∈ {-kd, ..., 0, ..., kd} computes
d d d . d ∖ fd I lxl + 2 I |x| ≤ qmax	小
q = QU (x; θ) = sign(x)	d 2	,	(1)
qmax	|x| > qmax
2
Published as a conference paper at ICLR 2020
1,000
500
0
d 0 2
8
(a) Case U1: θ = [b, d]T
Figure 1:	Maximum gradient norm maxχ ∣∣Vθ QU (x; θ)∣∣. For “U1” and “U2” the maximum gradient
norm can grow exponentially with varying bitwidth b whereas it is bounded for “U3”.
where the parameter vector θ = [d, qmax, b]T consists of the step size d ∈ R, the maximum value
qmax ∈ R and the number of bits b ∈ N, b ≥ 2 used to encode the quantized values q .
When training quantized DNNs, we want to optimize QU (x; θ) with respect to the input x and the
quantization parameters θ, meaning that We need the gradients VxQU(x; θ) and ▽6Q(χ; θ). A
common problem is, that the exact gradients are not useful for training. For example, ∂xQU (x; θ) =
dP2kb=--1-2b2-1+ ι δ (X 一 d (k + 2)) vanishes almost everywhere. A solution is to define the derivative
using STE (Bengio et al., 2013), Which ignores the floor operation in (1). This leads to
∂x QU (x)
10
|x| ≤ qmax
|x| > qmax
(2)
which is non-zero in the interesting region |x| ≤ qmax and which turned out to be very useful to
train quantized DNNs in practice (Yin et al., 2019). In this work, we follow this idea and define the
gradients VxQ(x; θ) and VθQ(x; θ), using STE whenever we need to differentiate a floor operation.
We refer to this as differentiable quantization (DQ).
An important observation from (1) is that the parameters θ = [d, qmax, b]T of a quantizer depend
on each other, i.e., qmax = (2b-1 一 1)d. This means, that we can choose from three equivalent
parametrizations of QU (x; θ): Case “U1” with θ = [b, d]T, case “U2” with θ = [b, qmax]T and case
“U3” with θ = [d, qmax]T. Interestingly, they differ in their gradients:
Case U1: Parametrization with respect to θ = [b, d]T , using qmax = qmax(b, d) gives
VθQu (x; θ)
∂b QU (x; θ)
∂dQU (x; θ)
01 (QU (x; θ) 一 x)	|x| ≤ (2b-1 一 1)d
-dJ
2 2b-lo-(7d Sign(X) |x| >(2b-1 -I)d
(3a)
Case U2: Parametrization with respect to θ = [b, qmax]T , using d = d(b, qmax) gives
VθQu(x; θ)
∂bQU(x; θ)
∂qmxQu(x; θ)
2b-1 log 2
_ ,
2b-1-i
1
qmax
0
sign(x)
(QU (x; θ) 一 x)
|x| ≤ qmax
|x| > qmax
(3b)
Case U3: Parametrization with respect to θ = [d, qmax]T, using b = b(d, qmax) gives
Vθ Qu (x; θ)
∂d QU (x; θ)
∂qmax Qu (x; θ)
"(Qu(x; θ) - x)
0
sign(x)
|x| ≤ qmax
|x| > qmax
(3c)
Fig. 1 shows the maximum gradient norm maxx ∣Vθ Qu (x; θ)∣ for the three parametrizations “U1”
to “U3”. For the parametrizations “U1” and “U2”, maxx ∣Vθ Qu (x; θ)∣ can grow exponentially
with varying bitwidth b as ∂dQU(x; θ) ∈ [-2b-1 - 1, 2b-1 一 1] for “U1” and ∂bQu(x; θ) ∈
[一 2qmαX1 log2, 2qmit-1 log 2^∣ for “U2”. This is not desirable when training quantized DNNs,
because it will lead to large changes of the gradient norm and forces us to use small learning rates to
3
Published as a conference paper at ICLR 2020
2
∂ ∂, QU(X)八 ∂d QU(X)
-1 一	一
→j-----
X
qmax
db QU(X)个dqmaχ Q U(X)
1
X
qmax
ddQU(Ix) AdQmax QU(X)
1 ——	J----
X
ax
(a) Case U1	(b) Case U2
1 小 ∂xQu(x)
X
qmax
(c) Case U3	(d) Input derivative
枇W
Figure 2:	Partial derivatives of QU (x; θ) with respect to the input and the quantization parameters
d, qmax and b. Partial derivatives are coupled for “U1” and “U2” but are decoupled for “U3”.
avoid divergence. However, parametrization “U3” does not suffer from such an unbounded gradient
norm as both partial derivatives ∂dQu(x; θ) ∈ [-2, 2] and ∂qmaxQU(x; θ) ∈ {-1,1} are bounded.
Fig. 2 shows the gradients for the parametrization “U1” to “U3”. For parametrization “U3”, the
partial derivatives in VθQu(x; θ) are decoupled, i.e., ▽6QU(x; θ) is a unit vector, which either
points only in the direction of d if |x| ≤ qmax or only in the direction of qmax, if |x| > qmax. We will
show in Sec. 2.3 that this implies a diagonal Hessian, which results in a better convergence behavior
of gradient descent. In contrast, both parametrizations “U1” and “U2” have partial derivatives that
are coupled. In summary, this implies that parametrization “U3” is the best DQ parametrization.
Similar considerations can be made for the power-of-two quantization QP (x; θ), which maps a
real-valued number x ∈ R to a quantized value q ∈ {±2k : k ∈ Z} by
|x| ≤ qmin
qmin < |x| ≤ qmax ,	(4)
|x| > qmax
where qmin and qmax are the minimum and maximum absolute values of the quantizer for a bitwidth
of b bit. Power-of-two quantization is an especially interesting scheme for DNN quantization, since a
multiplication of quantized values can be implemented as an addition of the exponents. Using the
STE for the floor operation, the derivative ∂xQP (x; θ) is given by
{0	|x| ≤ qmin
2b0.5+log2 |x|c
∣χ∣	qmin < |x| ≤ qmax ∙	(5)
0	|x| > qmax
The power-of-two quantization has the three parameters [b, qmin, qmax] =: θ, which depend on
each other with the relationship qmax = 22b-1-1qmin. Therefore, we have again three different
parametrizations with θ = [b, qmin], θ = [b, qmax] or θ = [qmin, qmax], respectively. Similarly to the
uniform case, one parametrization (θ = [qmin, qmax]) leads to a gradient of a very simple form
[1, 0]T	|x| ≤ qmin
[0, 0]	qmin < |x| ≤ qmax ,	(6)
[0, 1]	|x| > qmax
which has again a bounded gradient magnitude and independent components and is, hence, best
suited for first order gradient based optimization.
(qmin
2b0.5+log2 ∣x∣c
qmax
∂qmin QU (x; θ)
∂qmax QU (x; θ)
Vθ Qp (x; θ)
2.2	CONSTRAINTS ON θ
In practice, for an efficient hardware implementation, we need to ensure that the quantization
parameters only take specific discrete values: for uniform quantization, only integer values are
allowed for the bitwidth b, and the stepsize d must be a power-of-two, see e.g. (Jain et al., 2019); for
power-of-two quantization, the bitwidth must be an integer, and the minimum and maximum absolute
values qmin and qmax must be powers-of-two.
We fulfill these constraints by rounding the parameters in the forward pass to the closest integer or
power-of-two value. In the backward pass we update the original float values, i.e., we used again the
STE to propagate the gradients.
2.3	Experimental comparison of DQ parametrizations
In the following we compare the parametrizations using two experiments.
4
Published as a conference paper at ICLR 2020
13
--
00
11
rorre derauqs nae
rorre derauqs nae
(a) Uniform quantization	(b) Power-of-two quantization
Figure 3:	MSE for quantizing Gaussian data X 〜N(0,1) with uniform and PoWer-of-two quantiza-
tion. Parametrizations “U3” and “P3” converge to the lowest MSE without any oscillations.
1)	Quantization of Gaussian data In our first exPeriment we use DQ to learn the oPtimal quantiza-
tion parameters θ* which minimize the mean squared error (MSE) E [2(Q(x; θ) - x)2] with
gradient descent and comPare the convergence sPeed for three Possible Parametrizations of a
uniform and power-of-two quantizer. We choose this example as the gradient ▽&Q(χ; θ) =
E [(Q(χ; θ) - x)Vθ Q(χ; θ)] is just a scaled version of P θ Q(χ; θ), i.e., the gradient direction de-
pends directly on the parametrization of Q(x; θ) and thus the effects of changing the parametrization
can be observed.
It is interesting to study the Hessian H = PθPθTE (Q(x; θ) - x)2 ∈ R2×2 of the MSE:
H=E PθQ(x;	θ)PθQ(x; θ)T	+ (Q(x;	θ) -	x)PθPθTQ(x;	θ)	≈E	PθQ(x; θ)PθQ(x;	θ)T	.
Note that we use the outer-product approximation (Bishop, 2006) in order to simplify our con-
siderations. From this equation it is apparent that the Hessian will be diagonal for the case
U3 as PθQ(x; θ)PθQ(x; θ)T only contains an element in either (1, 1) or (2, 2) and, therefore,
E PθQ(x; θ)PθQ(x; θ)T is a diagonal matrix. From this, we can see that gradient descent with an
individual learning rate for each parameter is equivalent to Newton’s method and, therefore, efficient.
In general this will not be the case for U1 and U2.
We conduct an experiment, using ADAM to optimize the mean squared quantization error on
artificially generated data, which is generated by drawing 104 samples from N(0, 1). Please note
that the same example was studied in (Jain et al., 2019). The results in Fig. 3 clearly show that
the parametrizations “U3” and “P3” are best suited to optimize the uniform and power-of-two
quantization parameters, respectively. Indeed, these quantizers converge without oscillation to the
lowest MSE. It is interesting to see, that even adaptive gradient methods like ADAM can not solve the
scaling issue described in Sec. 2.1. In the Appendix A.4 we give further empirical evidence to support
this claim and compare the different parametrizations for the training of a quantized ResNet-20 on
CIFAR-10 using ADAM. Note that all cases use the same learning rate. For the interested reader, a
more detailed visualization of the error surfaces over the quantization parameters can be found in
Appendix A.3.
2)	CIFAR-10 In our second experiment we train a ResNet-20 (He et al., 2016) with quantized
parameters and activations on CIFAR-10 (Krizhevsky & Hinton, 2009) using the same settings as
proposed by (He et al., 2016). Fig. 4 shows the evolution of the training and validation error during
training for the case of uniform quantization. The plots for power-of-two quantization can be found
in the appendix (Fig. 10). We initialize this network from random parameters or from a pre-trained
float network. The quantized DNNs are trained for 160 epochs, using SGD with momentum 0.9 and
a learning rate schedule starting with 0.01 and reducing it by a factor of 10 after 80 and 120 epochs,
respectively. We use random flips and crops for data augmentation. Each epoch takes about 2.5 min
ona single GTX 1080 Ti.
In case of randomly initialized weights, we use an initial stepsize dl = 2-3 for the quantization
of weights and activations. Otherwise, we initialize the weights using a pre-trained floating point
network and the initial stepsize for a layer is chosen to be dl = 2blog2(max |W l |/(2b-1 -1))c . The
remaining quantization parameters are chosen such that we start from an initial bitwidth of b = 4
bit. This is a reasonable upper limit for b, as in practice no performance degradation can be observed
for b > 4bit. Even simple offline algorithms like min/max quantization result in networks with good
accuracies. We define no memory constraints during training, i.e., the network can learn to use a large
number of bits to quantize weights and activations of each layer. From Fig. 4, we again observe that
the parametrization θ = [d, qmax]T is best suited to train a uniformly quantized DNN as it converges
5
Published as a conference paper at ICLR 2020
Table 1: Comparison of different DQ parametrizations for ResNet-20 on CIFAR-10.
(validation error with “random”/“float net” initialization)
Quantization
Float32
Uniform quantization	Power-of-two quantization
θ =	[b,	d]T	θ =	[b,	qmax]T	θ = [d,	qmax]T θ =	[b,	qmax]T	θ =	[b,	qmin]T	θ =	[qmin,	qmax]T
Weights
Weights+Activations
8.50%/7.29%
17.8%/8.18%
28.9%/9.03%
8.80%/7.44%
9.43%/7.74%
8.50%/7.32%	11.70%/7.90% 53.07%/23.01%	10.61%/7.56%
9.23%/7.40% 22.91%/11.68% diverging/35.68%	15.10%/9.86%
(a)	Random initialization
(b)	Pre-trained initialization
246
Iteration
rorre noitadilaV4
100
10-1
0
246
Iteration
rorre gniniarT
4
10
-
-
0
Iteration
rorre noitadilaV4
Iteration
...... b, d (UI) --------- b, qmax (U2)	------d, qmax (U3)
Figure 4:	ResNet-20 with uniformly quantized weights and activations.
to the best local optimum. Furthermore, we observe the smallest oscillation of the validation error for
this parametrization.
Table 1 compares the best validation error for all parametrizations of the uniform and power-of-two
quantizations. We trained networks either with quantized weights and full precision activations or
with both being quantized. In case of activation quantization with power-of-two, we use one bit
to explicitly represent the value x = 0. This is advantageous as the ReLU nonlinearity will map
many activations to this value. We can observe that training the quantized DNN with the optimal
parametrization of DQ, i.e., using either θ = [d, qmax]T or θ = [qmin, qmax]T results in a network with
the lowest validation error. This result again supports our theoretical considerations from Sec. 2.1.
3	Training quantized DNNs with memory constraints
We now discuss how to train quantized DNNs with memory constraints. Such constraints appear in
many applications when the network inference is performed on an embedded device with limited
computational power and memory resources.
A quantized DNN consists of layers which compute
X1 = fι(Q(W 1； θw)* Q(Xl-1； θχ-ι)+ Q(cι; θW)) With l=1,...,L,	⑺
where fl(∙) denotes the nonlinear activation function of layer l and Q(∙; θ) is a per-tensor quantization
With parameters θ applied separately to the input and output tensors Xl-1 ∈ Il and Xl ∈ Il , and
also to both the weight tensors Wl ∈ Pl and the bias vector cl ∈ RMl .1 For a fully connected
layer, IIl-I = RMl-1, Il = RMl are vectors, Pl = RMl ×Ml-1 are matrices and A * B is a matrix-
vector product. In case of a convolutional layer, Il-1 = RMl-1 ×Nl-1×Nl-1, Il = RMl×Nl×Nl,
Pl = RMl ×Ml-1 ×Kl ×Kl are tensors and A * B is a set of Ml-1Ml 2D convolutions, where the
convolution is performed on square-sized feature maps of size Nl-1 × Nl-1 using square-sized
kernels of size Kl × Kl .
DNNs with quantized weights and activations have a smaller memory footprint and are also com-
putationally cheaper to evaluate since Q(α; θ) ∙ Q(β; θ) for α,β ∈ R requires only an integer
multiplication for the case of uniform quantization or an integer addition of the exponents for power-
of-two quantization. Furthermore, Q(α; θ) + Q(β; θ) for α,β ∈ R only requires an integer addition.
Table 2 compares the computational complexity and the memory footprint of layers which apply
uniform or power-of-two quantization to weights and activations.
We consider the following memory characteristics of the DNN, constraining them during training:
1.	Total memory Sw(θ1w, ..., θLw) = PlL=1 Slw(θlw) to store all weights: We use the constraint
L
g1(θ1w,...,θLw)=Sw(θ1w,...,θLw)-S0w=	l=1Slw(θlw)-S0w	≤0,	(8a)
1In this paper, we use “weights” to refer to W and c.
6
Published as a conference paper at ICLR 2020
Table 2: Number of multiplications Clmul, additions Cladd as well as required memory to store the
weights Slw and activations Slx of fully connected and convolutional layers.
Layer	Quantization	Cmul	Cadd	Sw	Sx
Fully connected	uniform pow-2	MlMl-I 0	MlMl-I 2MlMl-ι	Ml(Ml-1 + 1)blw	Mlblx
Convolutional	uniform pow-2	MlMl-1Nl2Kl2 0	MlMl-INj2Kl 2MlMl-ιNl2K2	Ml(Ml-1Kl2 + 1)blw	MlNl2blx
to ensure that the total weight memory requirement Sw(θ1w, ..., θLw) is smaller than a certain maximum
weight memory size S0w. Table 2 gives Slw(θlw) for the case of fully connected and convolutional
layers. Each layer’s memory requirement Slw (θlw ) depends on the bitwidth blw : reducing Slw (θlw)
will reduce the bitwidth blw .
2.	Total activation memory Sx(θ1x, ..., θLx) = PlL=1 Slx(θlx) to store all feature maps: We use the
constraint
L
g2(θ1x,...,θLx)=Sx(θ1x,...,θLx)-S0x=	l=1 Slx(θlx) -	S0x	≤0,	(8b)
to ensure an upper limit on the total activation memory size S0x. Table 2 gives Slx(θlx) for the case
of fully connected and convolutional layers. Such a constraint is important if we use pipelining for
accelerated inference, i.e., if we evaluate multiple layers with several consecutive inputs in parallel.
This can, e.g., be the case for FPGA implementations (Guo et al., 2017).
3.	Maximum activation memory Sx(θ1x, ..., θLx ) = maxl=1,...,L Slx to store the largest feature
map: We use the constraint
, … ʌ , . ʌ , ʌ
g3(θ1,…,θL)	=	Sx(θX,…,θL)	- Sx	= max (SX)	- Sx	≤	0,	(8c)
l=1,...,L
.	.1 . . 1	♦	. ∙	♦ Am 1	.	1	1 ∙	Am EI ♦	. ∙ . ∙
to ensure that the maximum activation size Sx does not exceed a given limit S0x. This constraint is
relevant for DNN implementations where layers are processed sequentially.
To train the quantized DNN with memory constraints, we need to solve the optimization problem
min	Ep(X,Y)[J(XL,Y)]	s.t.	gj(θ1w,...,θLw,θ1x,...,θLx)	≤0 forallj=	1,..., 3	(9)
Wl ,cl ,θlw,θlx
where J(XL, Y) is the loss function for yielding the DNN output XL although the ground truth is
Y. Eq. (9) learns the weights Wl, cl as well as the quantization parameters θlx, θlw. In order to use
simple stochastic gradient descent solvers, we use the penalty method (Bertsekas, 2014) to convert
(9) into the unconstrained optimization problem
min	Ep(X,Y) [J(XL, Y)] +XJ λjmax(0,gj(θ1w,...,θLw,θ1x,...,θLx))2,	(10)
Wl ,cl ,θlw ,θlx	j=1
where λj ∈ R+ are individual weightings for the penalty terms. Hence, training with weight and
activation size constraints requires choosing two penalty weightings λj , one for (8a) and one for
either (8b) or (8c).
Note, that the optimization problem (10) does not necessarily give a quantized DNN which fulfills
the memory constraints. The probability to fulfill the constraint gj depends on the choice of λj . In
particular, this probability increases with larger λj . However, choosing a too large λj will yield
a penalty term that dominates over the network loss decreasing the network performance. In our
experiments, we choose λj such that the initial loss and the penalty term have approximately the same
magnitude. Using this simple heuristic, we optained quantized DNNs that reached a high accuracy
and at the same time fulfilled the constraints at the end of training.
4 Experiments
In the following, we will use the best parametrizations for uniform and power-of-two DQ, i.e.,
θU = [d, qmax]T and θP = [qmin, qmax]T , that we found in Sec. 2. Both parametrizations do not
directly depend on the bitwidth b. Therefore, We compute it by using b(θu) =「log? (qmax + 1)+ 1]
and b(θp) = [log? (log?(黑)+ 1)+ l].
All quantized networks use a pre-trained float32
network for initialization and all quantizers are initialized as described in Sec. 2.3. For our experiments
on CIFAR-10, we use the same training setup as described in Sec. 2.3. For the experiments on
7
Published as a conference paper at ICLR 2020
Table 3: Homogeneous vs. heterogeneous quantization of ResNet-20 on CIFAR-10.					
	Bitwidth Weight/Activ.	qmax Weight/Activ.	Size Weight/Activ.(max)/Activ.(sum)	Uniform quant. Validation error	Power-of-two quant. Validation error
Baseline	32bit∕32bit	-	1048KB/64KB/736KB		7.29%
Fixed	2bit/32bit	fixed/—	65.5KB/64KB/736KB	10.81%	8.99%
TQT (Jain et al., 2019)	2bit/32bit	learned/ 一	65.5KB/64KB/736KB	9.47%	8.79%
Ours (w/ constr. (8a))	learned/32bit	learned/-	70KB/64KB/736KB	8.59%	8.53%
Fixed	2bit/4bit	fixed/fixed	65.5KB/8KB/92KB	11.30%	11.62%
TQT (Jain et al., 2019)	2bit/4bit	learned/learned	65.5KB/8KB/92KB	9.62%	11.29%
Ours (w/ constr. (8a) and (8b))	learned/learned	learned/learned	70KB/ - /92KB	9.38%	11.29%
Ours (w/ constr. (8a) and (8c))	learned/learned	learned/learned	70KB/8KB/ -	8.58%	11.23%
ImageNet, we train the quantized DNNs for 50 epochs, using SGD with momentum 0.9 and a
learning rate schedule starting with 0.01 and reducing it by a factor of 10 after 16 and 32 epochs,
respectively. Please note that we quantize all layers opposed to other papers which use a higher
precision for the first and/or last layer.
In our experiments, we noticed that the performance of DQ is not sensitive to the choice of λj in
(10). For the CIFAR-10 experiments, we use λ = 0.1 for both constraints (for sizes in kB). For the
ImageNet experiments, we kept the same regularization level by scaling λj with the square of the
size ratio between the ImageNet model and the CIFAR-10 model. We scale with the square-ratio as
the constraints in (10) are squared penalty terms.
First, in Table 3/top, we train a ResNet-20 on CIFAR-10 with quantized weights and float32 activa-
tions. We start with the most restrictive quantization scheme with fixed qmax and b = 2bit (“Fixed”).
Then, we allow the model to learn qmax while b = 2bit remains fixed as was done in (Jain et al., 2019)
(“TQT”). Finally, we learn both qmax and b with the constraint that the weight size is at most 70KB
(“Ours”), which is just 4.5kB larger that the previous 2Bit networks. This allows the model to allocate
more than two bits to some layers. From Table 3/top, we observe that the error is smallest when we
learn all quantization parameters.
In Table 3/bottom, weights and activations are quantized. For activation quantization, we consider
two cases as discussed in Sec. 3. The first one constrains the total activation memory Sx while
the second constrains the maximum activation memory Sx such that both have the same size as a
homogeneously quantized model with 4bit activations. Again, we observe that the error is smallest
when we learn all quantization parameters.
We also use DQ to train quantized ResNet-18 (He et al., 2016) and MobileNetV2 (Sandler et al.,
2018) on ImageNet (Deng et al., 2009) with 4bit uniform weights and activations or equivalent-sized
networks with learned quantization parameters. This is quite aggressive and, thus, a fixed quantization
scheme loses more than 6% accuracy while our quantization scheme loses less than 0.5% compared
to a float32 precision network.
Our results compare favorably to other recent quantization approaches. To our knowledge, the best
result for a 4bit ResNet-18 was reported by (Esser et al., 2019) (29.91% error). This is very close to
our performance (29.92% error). Importantly, (Esser et al., 2019) did not quantize the first and last
layers, meaning that their network is much bigger. Specifically, compared to our quantized ResNet-18,
their model with high precision input and output layers requires 37% more memory to store the
weights. Moreover, (Esser et al., 2019) learns stepsizes which are not restricted to powers-of-two.
As explained in Sec. 2.2, uniform quantization with power-of-two stepsize leads to more efficient
inference, effectively allowing to efficiently compute any multiplication with an integer multiplication
and bit-shift. To our knowledge only (Wang et al., 2018) reported results of MobileNetV2 quantized
to 4bit. They keep the baseline performance constraining the network to the same size as the 4bit
network. However, they do not quantize the activations in this case. In addition, DQ training is
efficient since it is comparable to the training of unquantized network. Specifically, one epoch on
ImageNet takes 37min for MobileNetV2 and 18min for ResNet-18 on four Nvidia Tesla V100.
Fig. 5 shows the weight bitwidth assignment over layers. We observe that small bitwidths are used
for layers with many parameters, i.e., pointwise convolutions and fully connected layers. However,
the resulting bitwidth assignments are complex, meaning that there is no simple heuristic. Therefore,
it is important to learn the optimal bitwidth assignment.
8
Published as a conference paper at ICLR 2020
Table 4: Homogeneous vs. heterogeneous quantization of MobileNetV2 and ResNet-18 on ImageNet.
	Bitwidth Weight/Activ.	qmax Weight/Activ.	MobileNetV2		ResNet-18	
			Size Weight/Activ(max)	Validation Error	Size Weight/Activ(max)	Validation Error
Baseline	32bit∕32bit	-	13.23MB/4.59MB	29.82%	44.56MB/3.04MB	29.72%
Fixed	4bit/4bit	fixed/fixed	1.65MB/0.57MB	36.27%	5.57MB/0.38MB	34.15%
TQT (Jain et al., 2019)	4bit/4bit	learned/learned	1.65MB/0.57MB	32.21%	5.57MB/0.38MB	30.49%
Ours (w/ constr. (8a) and (8c))	learned/learned	learned/learned	1.55MB/0.57MB	30.26%	5.40MB/0.38MB	29.92%
Ours (w/o constr.)	learned/learned	learned/learned	3.14MB/1.58MB	29.41%	10.50MB/1.05MB	29.34%
Figure 5: Weight bitwidth assignment over layers for ResNet-18 and MobileNetV2 on ImageNet
with weights constrained to a maximum size of 5.57MB. Our method has learned a heterogeneous
bitwidth distribution, which gives a better performance than a homogeneous one (see Table 4).
5 Conclusions
In this paper we discussed differentiable quantization and its application to the training of compact
DNNs with memory constraints. In order to fulfill memory constraints, we introduced penalty
functions during training and used stochastic gradient descent to find the optimal weights as well
as the optimal quantization values in a joint fashion. We showed that there are several possible
parametrizations of the quantization function. In particular, learning the bitwidth directly is not
optimal; therefore, we proposed to parametrize the quantizer with the stepsize and dynamic range
instead. The bitwidth can then be inferred from them. This approach is competitive to other recent
quantization methods while it does not require to retrain the network multiple times in contrast to
reinforcement learning approaches (Wang et al., 2018; Elthakeb et al., 2018).
Acknowledgements
We would like to thank Masato Ishii for many helpful comments during the preparation of this
manuscript.
References
Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal
operators. CoRR, abs/1810.00861, 2018. URL http://arxiv.org/abs/1810.00861.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Dimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press,
2014.
Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
Fabien Cardinaux, Stefan Uhlich, Kazuki Yoshiyama, Javier Alonso GarCia, Stephen Tiedemann,
Thomas Kemp, and Akira Nakamura. Iteratively training look-up tables for network quantization.
arXiv preprint arXiv:1811.05355, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
9
Published as a conference paper at ICLR 2020
Ahmed T. Elthakeb, Prannoy Pilligundla, Amir Yazdanbakhsh, Sean Kinzer, and Hadi Esmaeilzadeh.
Releq: A reinforcement learning approach for deep quantization of neural networks. CoRR,
abs/1811.01704, 2018. URL http://arxiv.org/abs/1811.01704.
Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar-
mendra S. Modha. Learned step size quantization. CoRR, abs/1902.08153, 2019. URL
http://arxiv.org/abs/1902.08153.
Kaiyuan Guo, Shulin Zeng, Jincheng Yu, Yu Wang, and Huazhong Yang. A survey of fpga-based
neural network accelerator. arXiv preprint arXiv:1712.08934, 2017.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015. URL
http://arxiv.org/abs/1510.00149.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. CoRR,
abs/1609.07061, 2016. URL http://arxiv.org/abs/1609.07061.
Sambhav R. Jain, Albert Gural, Michael Wu, and Chris Dick. Trained uniform quantization for
accurate and efficient neural network inference on fixed-point hardware. CoRR, abs/1903.08066,
2019. URL http://arxiv.org/abs/1903.08066.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711,
2016.
Zhi-Gang Liu and Matthew Mattina. Learning low-precision neural networks without straight-through
estimator (ste). arXiv preprint arXiv:1903.01061, 2019.
Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling.
Relaxed quantization for discretized neural networks. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=HkxjYoCqKX.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. CoRR, abs/1611.00712, 2016. URL http://arxiv.
org/abs/1611.00712.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4510-4520, 2018.
Sony. Neural Network Libraries (NNabla). https://github.com/sony/nnabla.
Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: hardware-aware automated
quantization. CoRR, abs/1811.08886, 2018. URL http://arxiv.org/abs/1811.08886.
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Under-
standing straight-through estimator in training activation quantized neural nets. arXiv preprint
arXiv:1903.05662, 2019.
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization:
Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
10
Published as a conference paper at ICLR 2020
A Derivation of the gradients for differentiable quantization
(DQ)
In the following sections, We will give the derivatives 忌Q(x; θ) and gradients VθQ(χ; θ) for the
uniform and the power-of-two quantizers. The results are summarized in Sec. 2.
We use the straight-through gradient estimate whenever we need to differentiate a non-differentiable
floor function, i.e., we assume
d- bxc = 1.	(II)
A.1 Derivatives of the uniform quantizer
Fig. 6(a) shows a symmetric uniform quantizer QU (x; θ) which maps a real value x ∈ R to one of
I = 2k + 1 quantized values q ∈ {-kd, ..., 0, ..., kd} by computing
q = QU (x; θ) = sign(x)
(d j|x| + 2k |x| ≤ qmax
qmax	|x| > qmax
(12)
using the parameters θ = [d, qmax, b]T where d ∈ R is the stepsize, qmax ∈ R is the maximum value
and b ∈ N is the number of bits that we use to encode the quantized values q. The elements of θ are
dependent as there is the relationship qmax = (2b-1 - 1)d.
A.1.1 CASE U1: PARAMETRIZATION WITH RESPECT TO b AND d
For the parametrization with respect to the bitwidth b and steps size d, (12) is given by
q = Qu(x; b, d) = Sign(X)d [	+ 2I	|x| ≤ (2
2b-1-1	|x| > (2b
-1
-1
- 1)d
- 1)d
and the derivatives are given by
∂QU (x; b, d)	2b-1 log 2
—d—=Sign(X)Ibɪɪ
∂QU (x; b, d)	1
―w-=Sign(XE
d[⅛l
|x| ≤ (2b
1 - 1)d |x| > (2b
+ 1] - |x| |x| ≤ (2bτ
(2b-1 - 1)d
|x| > (2b-1
-	1 - 1)d
-1-1)d,
-	1)d
.
-	1)d
(13)
(14a)
(14b)
A.1.2 CASE U2: PARAMETRIZATION WITH RESPECT TO b AND qMAX
For the parametrization with respect to the bitwidth b and maximum value qmax, (12) is given by
q = Qu (x; b,qmax) = Sign(X)qmax( 2b-1-1	^ + 2J W qmx
1	|X| > qmax
(15)
and the derivatives are given by
dQU (x; b, qmax)
∂b
./ ∖2b-1log2 ʃ-2b⅛1 ||x|2b-1-1 + 2∣ + βι
sign(x) 2b7----— 2	2	1L	qmαx	2」
|X| ≤ qmax
|X| > qmax
(16a)
dQu(x； b,qmaχ)	= Sign(X)L ("1	]|x|2b-1-1	+ 2] + β |x|	≤	qmax	,	(16b)
qmax	qmax qmax	|X|	>	qmax
∂j |x| 2b-1-1 +1 k	R	∂j |x| 2b-1-1 +1 k
where βι = 2⅛⅛ L	霏—21 = |x| and β2 = 2b⅛ D ∣ ,黑	2」=-∣X∣,ifwe usethe
straight-through gradient estimate for the floor function.
11
Published as a conference paper at ICLR 2020
A
QU (x)
qmax
d
d qmax
(a) Uniform quantization
(b) Power-of-two quantization
Figure 6: Examples of uniform quantizer QU (x) and power-of-two quantizer QP (x) for b = 3 bits
A.1.3 CASE U3: PARAMETRIZATION WITH RESPECT TO d AND qMAX
Eq. (12) gives the quantization with respect to the step size d and maximum value qmax. The
derivatives are
	dQu(X：d,qmaX) =	Sign(X)1 Id	愣	+ 1]Txl	lxl≤ qmax ,	(17a) dd	d [0	|x|	>qmax dQU(X “aX) =	Sign(X)L ʃ0	|x|	≤	qmax	.	(17b) ∂qmax	qmax	qmax	|x|	>	qmax
A.2 Derivatives of the power-of-two quantizer
(qmin
2b0.5+log2 ∣x∣c
qmax
Power-of-two quantization QP (x; θ) maps a real-valued number x ∈ R to a quantized value q ∈
{±2k : k ∈ Z} by
|x| ≤ qmin
qmin < |x| ≤ qmax ,	(18)
|x| > qmax
where qmin and qmax are the minimum and maximum (absolute) values of the quantizer for a bitwidth
of b bits. Fig. 6b shows the quantization curve for this quantization scheme.
Using the STE for the floor operation, the derivative ∂xQP(x; θ) is given by
{0	|x| ≤ qmin
2b0.5+log2 |x|c
∣χ∣	qmin < |x| ≤ qmax ∙	(19)
0	|x| > qmax
The power-of-two quantization has the three parameters θ = [b, qmin, qmax], which are dependent on
each other, i.e., qmax = 22b-1-1qmin. Therefore, we have again three different parametrizations with
θ = [b, qmin], θ = [b, qmax] or θ = [qmin, qmax], respectively. The resulting partial derivatives for each
parametrization are shown in Fig. 7 and summarized in the following sections. Similar to the uniform
case, one parametrization (θ = [qmin, qmax]) leads to a gradient with the nice form
[1, 0]T |x| ≤ qmin
[0, 0]T qmin < |x| ≤ qmax ,	(20)
[0, 1]	|x| > qmax
which has a bounded gradient magnitude and independent components and is, hence, well suited for
first order gradient based optimization.
Vθ Qp (x; θ)
∂qmin QU (x; θ)
∂qmax QU (x; θ)
A.2. 1 CASE P1: PARAMETRIZATION WITH RESPECT TO b AND qMAX
For the parametrization with θ = [b, qmax], (18) is given by
2-2b-1+1
I 乙	qmax
QP (x; b, qmax) = sign(x)	2b0.5+log2 ∣x∣c
qmax
|x| ≤ 2-2b-1+1qmax
2-	+ qmax < |x| ≤ qmax
(21)
|x| > qmax
12
Published as a conference paper at ICLR 2020
∂Qp (x)个 ∂Qp (x)
db	d Qmax
1r
∂Qp (x)个 ∂Qp (x)
砒dQmin
∂Qp (x)八 ∂Qp (x)
∂qmin	∂qmax
TL____二
x	x
∂Qp (x)
∂X
(a) Case P1
(b) Case P2	(c) Case P3	(d) Input derivative
Figure 7:	Derivatives for the three different parametrizations of QP(x; θ)
and the partial derivatives are
∂Qp (x； AqmaX)
∂b
[-2-2b-1 + b(lθg2)2qmax
sign(x) < 0
[0
|x| ≤ -2-21
-2 -	+“max < |x| ≤ qmax
|x| > qmax
■-1+1
qmax
(22a)
∂Qp (x； b,qmax)
dqmax
2-2b-1+1
sign(x) 0
I 1
…	|x| ≤ -2-2b-1 + 1 qmax
-2 -	+“max < |x| ≤ qmax
|x| > qmax
(22b)
A.2.2 Case P2: Parametrization WITH respect to b and qM2
For the parametrization with θ = [b, qmin], (18) is given by
{qmin
2b0-5+log2∣x∣c
22b-1-1
qmin
|x| ≤ qmin
qmin < |x| ≤ 2
|x| > 22
,b- 1
,b-1
-1qmin
-1qmin
(23)
and the partial derivatives are
∂Qp(x； b,qmin)
∂b
「J0
sign(x)	0
I 22b-1+b-2(lθg2)2qmin
|x| ≤ qmin
qmin < |x| ≤ 2	Iqmin ,
|x| > 22b-1 -1 qmin
(24a)
∂Qp(x； b, qmin)
∂qmin
「J1
sign(x) 0
I 22b-1-1
|x| ≤ qmin
qmin < |x| ≤ 2	Iqmin .
|x| > 22b 1-1 qmin
(24b)
A.2.3 Case P3: Parametrization with respect to qMiN and 9max
Eq. (18) gives the parametrization of Q(x; θ) with respect to the minimum value qmin and maximum
value qmax. The derivatives are
dQP (x； qmin, qmax)
∂qmin
	1	|x|	≤ qmin
sign(x)	!0	qmin < |x|	≤ qmax
	I0	|x|	> qmax
dQP (x； qmin, qmax)
dqmax
	0	|x|	≤ qmin
sign(x)	jo	qmin < |x|	≤ qmax
	I1	|x|	> qmax
(25a)
(25b)
13
Published as a conference paper at ICLR 2020
(a) Case U1
1
0.5
0
■	θ*	U
5
5	-10
qmax	10 15 b
(b) Case U2
(c) Case U3
Figure 8:	MSE surfaces for uniform quantization. Only U3 reaches the optimum θ*.
1
0.5
0
6
qmax	10 8 b
(a) Case P1
(b) Case P2
(c) Case P3
Figure 9:	MSE surfaces for PoWer-of-two quantization. Only P3 reaches the optimum θ*.
A.3 Visualization of the error surface for the quantization of Gaussian data
In Sec. 2.3 of the paper, we compared the three different parametrizations of the uniform quantizer
at the example of optimal quantization of Gaussian data. To get a better understanding of Fig 3,
we show how the error surfaces look like for this example problem. The experimental setup is the
same as in Sec. 2.3, i.e., we use DQ to learn the optimal quantization parameters of a uniform and a
power-of-two quantizer, which minimize the expected quantization error minθ E (x - Q(x; θ))2 .
We use three different parametrizations, adapt the quantizer’s parameters with gradient descent and
compare the convergence speed as well as the final quantization error. As an input, we generate 104
samples from N(0, 1).
Fig. 8	shows the corresponding error surfaces for the three different parametrizations of the uniform
quantization. The red curve shows the path through the parameter space taken by gradient descent
in order to optimize the MSE, starting with the initial values b = 2, d = qmax = 1. The optimum
θ* is located at b = 16, d/2-13, qmax = 4, since we allow a maximal bitwidth of 16bit and the
largest sample magnitude in our dataset is max{x1, ..., xN} / 4. In each of the cases U1-U3, the
error surface is composed of steep ridges and large flat regions. The steep ridges force us to use small
learning rates to avoid divergence. For cases U1 and U2, the optimal θ* can not be reached. However,
for U3, θ* lies at the border of a flat region and can be easily reached. Furthermore, case U3 shows
a much faster and more stable convergence without oscillation, since the gradient magnitudes are
bounded and the error surface has fewer steep ridges where gradient descent starts oscillating.
Fig. 9	shows the corresponding error surfaces for the three different parametrizations of the power-of-
two quantization. Again, the optimum θ* is not attained for two parametrizations, namely P1 and
P2, as θ* is surrounded by a large, mostly flat region. For these two cases, gradient descent tends
to oscillate at steep ridges and tends to be unstable. However, gradient descent converges to a point
close to θ* for parametrization P3, where θ = [qmin, qmax].
Finally, we also did a comparison of the different power-of-two quantizations on CIFAR-10. Fig. 10
shows the evolution of the training and validation error if we start from a random or a pre-trained
float network initialization. We can observe that θ = [qmin, qmax] has the best convergence behavior
and thus also results in the smallest validation error (cf. Table 1). The unstable behavior of P2 is
expected as the derivative ∂Qqp can take very large (absolute) values.
A.4 Further experiments with ADAM
Finally, we did an experiment to verify that the parametrization is important, even if adaptive gradient
descent methods like ADAM are used for optimization. Table 5 gives the results for a ResNet-20
trained on CIFAR-10. We observe, that again U3 and P3 are the best parametrizations.
14
Published as a conference paper at ICLR 2020
(a) Random initialization
(b) Pre-trained initialization
rorre gniniarT
rorre noitadilaV40
100
10-1
Iteration	Iteration	Iteration
I	b, d (UI) ----------- b, qmax (U2)	-------d, qmax (U3)
Figure 10: ResNet-20 with power-of-two quantized weights and activations.
Table 5: Error rate of ResNet-20 on CIFAR-10 using different quantization parametrizations. Training
is done either by SGD with momentum or ADAM.
Parametrization	SGD momentum	ADAM
U1	11.74	7.61
U2	7.44	7.85
U3	7.32	7.36
P1	15:35	7.54
P2	7.74	7.79
P3	7.40	7.40
B	Implementation details for differentiable quantization
The following code gives our differentiable quantizer implementation in NNabla (Sony). The source
code for reproducing our results will be published after the review process has been finished.
15
Published as a conference paper at ICLR 2020
B.1	Uniform quantization
B.1.1	CASE U1: PARAMETRIZATION WITH RESPECT TO b AND d
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
def ParametriJfixed_point_quantize_d_b(x, sign,
n_init, n_min, n_max,
d_init, d_min, d_max,
fix_parameters=False):
"""Parametric version of ‘fixed_point_quantize ‘ where the
bitwidth ‘b‘ and stepsize ‘d‘ are learnable parameters .
Returns :
~nnabla.Variable : N-D array .
"""
def clip_scalar(v, min_value, max_value):
return F.minimum_SCalar(F.maximum_SCalar(v, min_value), max_value)
def broadcast_SCalar(v, shape):
return F.broadcast(F.reshape(v, (1,) * len (shape), inplaCe=False), shape=shape)
def quantize_pow2(v):
return 2 ** F.round(F.log(v) / np.log(2.))
n = get_parameter_or_create("n", (),
COnStantInitializer(n_init),
need_grad=True,
as_need_grad=not fix_parameters)
d = get_parameter_or_create("d", (),
COnStantInitializer(d_init),
need_grad=True,
as_need_grad=not fix_parameters)
#	ensure that bitwidth is in specified range and an integer
n = F.round(clip_scalar(n, n_min, n_max))
if sign:
n = n - 1
#	ensure that stepsize is in specified range and a power of two
d = quantize_pow2(clip_SCalar(d, d_min, d_max))
#	ensure that dynamic range is in specified range
xmax = d * (2 ** n - 1)
#	compute min/max value that We can represent
if sign:
xmin = -xmax
else:
xmin = nn.Variable((1,), need_grad=False)
xmin.d = 0.
#	broadcast variables to correct size
d = broadcast_scalar(d, shape=x.shape)
xmin = broadcast_SCalar(Xmin, shape=x.shape)
xmax = broadcast_SCalar(Xmax, shape=x.shape)
#	apply fixed-point quantization
return d * F.round(F.clip_by_value(x, xmin, xmax) / d)
16
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
B.1.2	CASE U2: PARAMETRIZATION WITH RESPECT TO b AND qMAX
def Parametri Jfixed_point_quantize_b_Xmax(x, sign,
n_init, n_min, n_max,
xmax_init, xmax_min, xmax_max,
fix_parameters=False):
"""Parametric version of ‘fixed_point_quantize ‘ where the
bitwidth ‘b‘ and dynamic range ‘xmax ‘ are learnable parameters .
Returns :
~nnabla.Variable : N-D array .
"""
def clip_scalar(v, min_value, max_value):
return F.minimum_SCalar(F.maximum_SCalar(v, min_value), max_value)
def broadcast_SCalar(v, shape):
return F.broadcast(F.reshape(v, (1,) * len (shape), inplace=False), shape=shape)
def quantize_pow2(v):
return 2 ** F.round(F.log(v) / np.log(2.))
n = get_parameter_or_create("n", (),
COnStantInitializer(n_init),
need_grad=True,
as_need_grad=not fix_parameters)
xmax = get_parameter_or_create("xmax", (),
COnStantInitializer(xmax_init),
need_grad=True,
as_need_grad=not fix_parameters)
#	ensure that bitwidth is in specified range and an integer
n = F.round(clip_scalar(n, n_min, n_max))
if sign:
n = n - 1
#	ensure that dynamic range is in specified range
xmax = clip_scalar(xmax, xmax_min, xmax_max)
#	compute step size from dynamic range and make sure that it is a pow2
d = quantize_pow2(xmax / (2 ** n - 1))
#	compute min/max value that We can represent
if sign:
xmin = -xmax
else:
xmin = nn.Variable((1,), need_grad=False)
xmin.d = 0.
#	broadcast variables to correct size
d = broadcast_SCalar(d, shape=x.shape)
xmin = broadcast_scalar(xmin, shape=x.shape)
xmax = broadcast_scalar(xmax, shape=x.shape)
#	apply fixed-point quantization
return d * F.round(F.clip_by_value(x, xmin, xmax) / d)
17
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
B.1.3	CASE U3: PARAMETRIZATION WITH RESPECT TO d AND qMAX
def Parametri Jfixed_point_quantize_d_Xmax(x, sign,
d_init, d_min, d_max,
xmax_init, xmax_min, xmax_max,
fix_parameters=False):
"""Parametric version of ‘fixed_point_quantize ‘ where the
stepsize ‘d‘ and dynamic range ‘xmax ‘ are learnable parameters .
Returns :
~nnabla.Variable : N-D array .
"""
def clip_scalar(v, min_value, max_value):
return F.minimum_SCalar(F.maximum_SCalar(v, min_value), max_value)
def broadcast_SCalar(v, shape):
return F.broadcast(F.reshape(v, (1,) * len (shape), inplace=False), shape=shape)
def quantize_pow2(v):
return 2 ** F.round(F.log(v) / np.log(2.))
d = get_parameter_or_create("d", (),
COnStantInitializer(d_init),
need_grad=True,
as_need_grad=not fix_parameters)
xmax = get_parameter_or_create("xmax", (),
COnStantInitializer(xmax_init),
need_grad=True,
as_need_grad=not fix_parameters)
#	ensure that stepsize is in specified range and a power of two
d = quantize_pow2(clip_SCalar(d, d_min, d_max))
#	ensure that dynamic range is in specified range
xmax = clip_scalar(xmax, xmax_min, xmax_max)
#	compute min/max value that We can represent
if sign:
xmin = -xmax
else:
xmin = nn.Variable((1,), need_grad=FalSe)
xmin.d = 0.
#	broadcast variables to correct size
d = broadcast_SCalar(d, shape=x.shape)
xmin = broadcast_scalar(xmin, shape=x.shape)
xmax = broadcast_scalar(xmax, shape=x.shape)
#	apply fixed-point quantization
return d * F.round(F.clip_by_value(x, xmin, xmax) / d)
18
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
B.2	Power-of-two quantization
B.2.1	CASE P1: PARAMETRIZATION WITH RESPECT TO b AND qMAX
def Parametric_pow2_quantize_b_Xmax(x, sign, with_zero,
n_init, n_min, n_max,
xmax_init, xmax_min, xmax_max,
fix_parameters=False):
"""Parametric version of ‘pow2_quantize ‘ where the
bitwidth ‘n‘ and range ‘xmax ‘ are learnable parameters .
Returns :
~nnabla.Variable : N-D array .
"""
def clip_scalar(v, min_value, max_value):
return F.minimum_Scalar(F.maximum_Scalar(v, min_value), max_value)
def broadcast_scalar(v, shape):
return F.broadcast(F.reshape(v, (1,) * len (shape), inplace=False), shape=shape)
def quantize_pow2(v):
return 2 ** F. round (F.log(F.abs (v)) / np.log(2.))
n = get_parameter_or_create("n", (),
COnStantInitializer(n_init),
need_grad=True,
as_need_grad=not fix_parameters)
xmax = get_parameter_or_create("xmax", (),
COnStantInitializer(XmaX_init),
need_grad=True,
as_need_grad=not fix_parameters)
#	ensure that bitwidth is in specified range and an integer
n = F.round(clip_scalar(n, n_min, n_max))
if sign:
n = n - 1
if with_zero:
n = n - 1
#	ensure that dynamic range is in specified range and an integer
xmax = quantize_pow2(clip_Scalar(Xmax, xmax_min, xmax_max))
#	compute min value that We can represent
xmin = (2 ** (-(2 ** n) + 1)) * xmax
#	broadcast variables to correct size
xmin = broadcast_Scalar(Xmin, shape=x.shape)
xmax = broadcast_Scalar(Xmax, shape=x.shape)
#	if unsigned, then quantize all negative values to zero
if not sign:
x = F.relu(x)
#	compute absolute value/sign of input
ax = F.abs(x)
sx = F.sign(x)
if with_zero:
#	prune smallest elements (in magnitude) to zero if they are smaller
#	than ′x_min / \sqrt(2)‘
x_threshold = xmin / np.sqrt(2)
idx1 = F.greater_equal(ax,
idx2 = F.greater_equal(ax,
idx3 = F.greater_equal(ax,
else:
idx1 = F.less(ax, xmin)
idx2 = F.greater_equal(ax,
idx3 = F.greater_equal(ax,
x_threshold) * F.less(ax, xmin)
xmin) * F.less(ax, xmax)
xmax)
xmin) * F.less(ax, xmax)
xmax)
#	do not backpropagate gradient through indices
idx1.need_grad = False
idx2.need_grad = False
idx3.need_grad = False
#	do not backpropagate gradient through sign
sx.need_grad = False
#	take care of values outside of dynamic range
return sx * (xmin * idx1 + quantize_pow2(ax) * idx2 + xmax * idx3)
19
Published as a conference paper at ICLR 2020
B.2.2 CASE P2: PARAMETRIZATION WITH RESPECT TO b AND qMIN
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
def
Parametric_pow2_quantize_b_Xmin(x, sign, with_zero,
n_init, n_min, n_max,
xmin_init, xmin_min, xmin_max,
fix_parameters=False):
"""Parametric version of ‘pow2_quantize ‘ where the
bitwidth ‘n‘ and the smallest value ‘xmin ‘ are learnable parameters .
Returns :
~nnabla.Variable : N-D array .
"""
def clip_scalar(v, min_value, max_value):
return F.minimum_Scalar(F.maximum_Scalar(v, min_value), max_value)
def broadcast_scalar(v, shape):
return F.broadcast(F.reshape(v, (1,) * len (shape), inplace=False), shape=shape)
def quantize_pow2(v):
return 2 ** F. round (F.log(F.abs (v)) / np.log(2.))
n = get_parameter_or_create("n", (),
COnStantInitializer(n_init),
need_grad=True,
as_need_grad=not fix_parameters)
xmin = get_parameter_or_create("xmin", (),
COnStantInitializer(xmin_init),
need_grad=True,
as_need_grad=not fix_parameters)
#	ensure that bitwidth is in specified range and an integer
n = F.round(clip_scalar(n, n_min, n_max))
if sign:
n = n - 1
if with_zero:
n = n - 1
#	ensure that minimum dynamic range is in specified range and a POwer-of-two
Xmin = quantize_pow2(clip_Scalar(xmin, xmin_min, xmin_max))
#	compute min/max value that We can represent
xmax = xmin * (2 ** ((2 ** n) - 1))
#	broadcast variables to correct size
xmin = broadcast_Scalar(Xmin, shape=x.shape)
xmax = broadcast_Scalar(Xmax, shape=x.shape)
#	if unsigned, then quantize all negative values to zero
if not sign:
x = F.relu(x)
#	compute absolute value/sign of input
ax = F.abs(x)
sx = F.sign(x)
if with_zero:
#	prune smallest elements (in magnitude) to zero if they are smaller
#	than ′x_min / \sqrt(2)‘
x_threshold = xmin / np.sqrt(2)
idx1 = F.greater_equal(ax,
idx2 = F.greater_equal(ax,
idx3 = F.greater_equal(ax,
else:
idx1 = F.less(ax, xmin)
idx2 = F.greater_equal(ax,
idx3 = F.greater_equal(ax,
x_threshold) * F.less(ax, xmin)
xmin) * F.less(ax, xmax)
xmax)
xmin) * F.less(ax, xmax)
xmax)
#	do not backpropagate gradient through indices
idx1.need_grad = False
idx2.need_grad = False
idx3.need_grad = False
#	do not backpropagate gradient through sign
sx.need_grad = False
#	take care of values outside of dynamic range
return sx * (xmin * idx1 + quantize_pow2(ax) * idx2 + xmax * idx3)
20
Published as a conference paper at ICLR 2020
B.2.3 CASE P3: PARAMETRIZATION WITH RESPECT TO qMIN AND qMAX
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
def
Parametric_pow2_quantize_xmin_xmax(x, sign, with_zero,
xmin_init, xmin_min, xmin_max,
xmax_init, xmax_min, xmax_max,
fix_parameters=False):
"""Parametric version of ‘pow2_quantize ‘ where the
min value ‘xmin ‘ and max value ‘xmax ‘ are learnable parameters .
Returns :
~nnabla.Variable : N-D array .
def clip_scalar(v, min_value, max_value):
return F.minimum_Scalar(F.maximum_Scalar(v, min_value), max_value)
def broadcast_scalar(v, shape):
return F.broadcast(F.reshape(v, (1,) * len(shape), inplace=False), shape=shape)
def quantize_pow2(v):
return 2. ** F.round(F.log(F.abs (V)) / np.log(2.))
xmin = get_parameter_or_create("xmin", (),
COnStantInitializer(xmin_init),
need_grad=True,
as_need_grad=not fix_parameters)
xmax = get_parameter_or_create("xmax", (),
COnStantInitializer(xmax_init),
need_grad=True,
as_need_grad=not fix_parameters)
#	ensure that minimum dynamic range is in specified range and a POwer-of-two
xmin = quantize_pow2(clip_Scalar(Xmin, xmin_min, xmin_max))
#	ensure that minimum dynamic range is in specified range and a power-of-two
xmax = quantize_pow2(clip_Scalar(Xmax, xmax_min, xmax_max))
#	broadcast variables to correct size
xmin = broadcast_Scalar(Xmin, shape=x.shape)
xmax = broadcast_scalar(xmax, shape=x.shape)
#	if unsigned, then quantize all negative values to zero
if not sign:
x = F.relu(x)
#	compute absolute value/sign of input
ax = F.abs(x)
sx = F.sign(x)
if with_zero:
#	prune smallest elements (in magnitude) to zero if they are smaller
#	than ′x_min / \sqrt(2)‘
x_threshold = xmin / np.sqrt(2)
idx1 = F.greater_equal(ax,
idx2 = F.greater_equal(ax,
idx3 = F.greater_equal(ax,
else:
idx1 = F.less(ax, xmin)
idx2 = F.greater_equal(ax,
idx3 = F.greater_equal(ax,
x_threshold) * F.less(ax, xmin)
xmin) * F.less(ax, xmax)
xmax)
xmin) * F.less(ax, xmax)
xmax)
#	do not backpropagate gradient through indices
idx1.need_grad = False
idx2.need_grad = False
idx3.need_grad = False
#	do not backpropagate gradient through sign
sx.need_grad = False
#	take care of values outside of dynamic range
return sx * (xmin * idx1 + quantize_pow2(ax) * idx2 + xmax * idx3)
21