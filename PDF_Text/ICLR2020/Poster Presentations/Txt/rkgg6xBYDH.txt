Published as a conference paper at ICLR 2020
Understanding Generalization in Recurrent
Neural Networks
Zhuozhuo Tu, Fengxiang He, Dacheng Tao
UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering
The University of Sydney
Darlington, NSW 2008, Australia
zhtu3055@uni.sydney.edu.au, {fengxiang.he,dacheng.tao}@sydney.edu.au
Ab stract
In this work, we develop the theory for analyzing the generalization performance
of recurrent neural networks. We first present a new generalization bound for
recurrent neural networks based on matrix 1-norm and Fisher-Rao norm. The
definition of Fisher-Rao norm relies on a structural lemma about the gradient of
RNNs. This new generalization bound assumes that the covariance matrix of the
input data is positive definite, which might limit its use in practice. To address this
issue, we propose to add random noise to the input data and prove a generalization
bound for training with random noise, which is an extension of the former one.
Compared with existing results, our generalization bounds have no explicit depen-
dency on the size of networks. We also discover that Fisher-Rao norm for RNNs
can be interpreted as a measure of gradient, and incorporating this gradient mea-
sure not only can tighten the bound, but allows us to build a relationship between
generalization and trainability. Based on the bound, we theoretically analyze the
effect of covariance of features on generalization of RNNs and discuss how weight
decay and gradient clipping in the training can help improve generalization.
1	Introduction
The Recurrent Neural network (RNN) is a neural sequence model that has achieved the state-of-
the-art performance on numerous tasks, including natural language processing (Yang et al., 2018;
Mikolov & Zweig, 2012), speech recognition (Chiu et al., 2018; Graves, 2013) and machine transla-
tion (Wu et al., 2016; Kalchbrenner & Blunsom, 2013). Unlike feedforward neural networks, RNNs
allow connections among hidden units associated with a time delay. Through these connections,
RNNs can maintain a ”memory” that summarizes the past sequence of inputs, enabling it to capture
correlations between temporally distant events in the data.
RNNs are very powerful, and empirical studies have shown that they have a very good generaliza-
tion property. For example, Graves (2013) showed that deep LSTM RNNs achieved a test error
of 17.7% on TIMT phoneme recognition benchmark after training with only 462 speech samples.
Despite of the popularity of RNNs in practice, their theory is still not well understood. A number
of recent works have sought to shed light on the effective representational properties of recurrent
networks trained in practice. For example, Oymak (2018) studied the state equation of recurrent
neural networks and showed that SGD can efficiently learn the unknown dynamics from few obser-
vations under proper assumptions. Miller & Hardt (2019) tried to explain why feed-forward neural
networks are competitive with recurrent networks in practice. They identified stability as a necessary
condition and proved that stable recurrent neural networks are well approximated by feed-forward
networks for the purpose of both inference and training by gradient descent. Despite of the impres-
sive progress in understanding the training behavior of RNNs, there is no generalization guarantee
in these works.
Understanding the generalization performance in machine learning has been a central problem for
many years and revived in recent years with the advent of deep learning. One classical approach
to proving generalization bound is via notions of complexity. For deep neural networks, numer-
ous complexity measures have been proposed to capture the generalization behavior such as VC
1
Published as a conference paper at ICLR 2020
dimension (Harvey et al., 2017) and norm-based capacity including spectral norm (Bartlett et al.,
2017; Neyshabur et al., 2019), Frobenius norm (Neyshabur et al., 2015b;a; 2018) and lp-path norm
(Neyshabur et al., 2015b; Bartlett & Mendelson, 2002; Golowich et al., 2018). These existing norm-
based complexity measures depend on the number of hidden units of the network explicitly and
thus can not explain why neural networks generalize so well in practice, despite that they operate
in an overparametrized setting (Zhang et al., 2017). Neyshabur et al. (2019) proved generalization
bounds for two layer ReLU feedforward networks, which decreased with the increasing number of
hidden unit in the network. However their results only applied to two layer ReLU networks and
some specific experiments. More recently, a new generalization bound based on Fisher-Rao norm
was proposed (Liang et al., 2017). This notion of Fisher-Rao norm is motivated by information
geometry and has good invariance properties. But they proved the bound only for linear deep neural
networks. There are also some works about the generalization of recurrent neural networks (Zhang
et al., 2018; Chen et al., 2019; Allen-Zhu & Li, 2019). However these bounds also depend on the
size of networks, which makes them vacuous for very large neural networks.
Our main contributions are summarized as follows.
•	We define the Fisher-Rao norm for RNNs based on its gradient structure and derive new
Rademacher complexity bound and generalization bound for recurrent neural networks
based on Fisher-Rao norm and matrix 1-norm. In contrast to existing results such as spec-
tral norm-based bounds, our bound has no explicit dependence on the size of networks.
•	We prove a generalization bound for RNNs when training with random noises. Our bound
applies to general types of noises and can potentially explain the effect of noise training on
generalization of recurrent neural networks as demonstrated by our empirical results.
•	We propose a new technique to decompose RNNs with ReLU activation into a sum of linear
network and difference terms. As a result, each term in the decomposition can be treated
independently and easily when estimating the Rademacher complexity. This decomposi-
tion technique can potentially be applied to other neural networks architectures such as
convolutional neural networks, which might be of independent interest.
The remainder of this paper is structured as follows. We define the problem and notations in Sec-
tion 2. The notion of Fisher-Rao norm for RNNs is introduced in Section 3.1. We prove the general-
ization bound for RNNs in Section 3.2, and the generalization bound for training with random noise
is derived in Section 3.3. Section 3.4 gives a detailed analysis of our generalization bound. Finally
we conclude and discuss future directions.
2	Preliminaries
We focus on the vanilla RNNs with ReLU activation. Let U ∈ Rm×d, V ∈ Rk×m and W ∈ Rm×m
be the weight matrices. Given the input sequence X = (xi, χ2, ∙∙∙ ,xl) ∈ RLd where each Xi ∈ Rd
and L is the input sequence length, the vanilla RNNs can be described as follows.
gt = UXt + Wht-1,
ht = ρ(gt),	(1)
yt = V ht,
where gt and ht ∈ Rm represents the input and output of hidden layer at step t, ρ(∙) is the ReLU
function and yt ∈ Rk denotes the output value at step t.
For simplicity, in this paper, we only consider the final output yL. We assume that data (X, y) is
drawn i.i.d. from some unknown distribution D over RLd × Y where Y represents the label space
{1,2,…，k}. The RNNs above define a mapping yL(x) from RLd → Rk, where k is the number
of classes. We convert yL (X) to a classifier by selecting the output coordinate with the largest
magnitude, meaning
X → argmaxi[yL(X)]i,
where Hi represents the i-th element of a vector. This naturally leads to the definition of margin
MyL (X, y) of the output yL at a labeled example (X, y):
MyL (X, y) = [yL(X)]y - max[yL (X)]y0.
y0 6=y
2
Published as a conference paper at ICLR 2020
Thus, yL misclassifies (x, y) if and only if MyL (x, y) ≤ 0. The quality of the prediction made by
yL is measured by the expected risk defined as
E(χ,y)~D [ɪMyL (χ,y)≤0].
Since the underlying distribution D is unknown to us, we instead consider the empirical error on
sample data given by
1n
n Σ(lMyL(xi,yi)≤α).
n i=1
The generalization error is then the difference between expected risk and empirical risk, defined as
1n
E(X,yhD [IMyL (χ,y)≤0] - n23(ɪMyL (xi,yi)≤α).
n i=1
Our goal in this paper is to study the generalization error for RNNs theoretically.
To establish the generalization bound, a little bit of notations are necessary. For a vector, we
denote the lp norm by ||v||p = (P ∣v∕p)1/p and the l∞ norm by ∣∣v∣∣∞ = max |vi|. For
a matrix, we denote the matrix p-norm as ||A||p = max||x||p=1 ||Ax||p, the matrix 1-norm by
||A||1 = maxj{Pi |aij|} and the Frobenius norm by ||A||2F = trace(AAT). The smallest eigen-
value of a matrix A is given by λmin(A). The activation function ρ and its derivative ρ0 are entry-
wise, i.e., ρ(A) = (ρ(aj ))j and ρ0(v) = (P(Viy)i. We denote C =(L + 1,L,…，2)T, η(θ)=
[Vdiag(ρ0(gL))...Wdiag(ρ0(g1))Ux1, Vdiag(ρ0(gL))…Wdiag(P0(g2))Ux2,…，Vdiag(ρ0(gL))
Uxl] ∈ Rk×L and T(θ) = (VWLTUx1, VWL-2Ux2,…，VUxL) where θ = (U, W, V) and
diag converts a vector into a diagonal matrix.
3	Main Result
In this section, we prove a generalization bound for RNNs with ReLU activation. Our new bound is
based on Fisher-Rao norm and matrix 1-norm. We first define the Fisher-Rao norm for RNNs.
3.1	Fisher-Rao Norm for RNNs
We adapt the notion of Fisher Rao norm to recurrent neural networks. To begin with, we establish
the following structural result for RNNs.
Lemma 1. Given an input x = (xι, x2, ∙∙∙ ,xl), consider the recurrent neural network in (1), we
have the identity
∂yL	∂yL	∂yL
Ib 西 Vab+工 西 wij+Iq ∂upq Uuq ="
The notion of Fisher-Rao norm is motivated by Fisher-Rao metric of information geometry and is
defined as follows.
Definition 1 ((Liang et al., 2017), Definition 2). The Fisher-Rao norm for a parameter θ is defined
as
∣∣θ∣∣f r =<θ,I(θ)θ>,
where I(θ) = E(Vl(yLθ(x), y) 0 Vl(yLθ(x), y)) and l(.,.) is the lossfunction.
The following lemma gives the explicit formula of Fisher-Rao norm in RNNs. We can see that the
notion of Fisher-Rao norm relies mainly on the gradient structure of RNNs.
Lemma 2. Assume that the loss function l(., .) is smooth in the first argument. Then the following
identity holds for the RNN in (1),
llθll2f r = E(S(θ)c,
dl(yLθ (x),y) ∖f
dyLθ
3
Published as a conference paper at ICLR 2020
Remark 1. We observe that each term V diag(ρ0(gL))...W diag(ρ0(gi))U xi in η(θ) is actually the
gradient component in Backpropagation through time (BPTT). Therefore, the Fisher-Rao norm can
be regarded as a measure of gradient. As will be shown later, we can build a relationship between
generalization and trainability in RNNs via Fisher-Rao norm.
For the linear activation function and margin loss l(yLθ (x), y) = Φα(MyL (x, y)) where α > 0 is
the margin parameter, one might upper bound the Fisher-Rao norm in Lemma 2 by
llθllfr ≤ α42E( max[(τ(θ)c)i]2),
since τ (θ)c,
Wjlθ (X),y) ∖2
Me
≤ 02 maxi[(τ(θ)c)i]2 by definition of MyL (x,y) and Iipschitz
property of Φα(∙). We define this upper bound as
llθllf S := E( max[(τ ⑻C)i]2),
(2)
and still call it ”Fisher-Rao norm” in the paper by slightly abusing the terminology as they are
equivalent for k = 1. In the rest of the paper, we will use this Fisher-Rao norm || ∙ |fs to derive
generalization bound for RNNs.
3.2	Generalization B ound for RNNs
We use matrix 1-norm and Fish-Rao norm together to derive a generalization bound for RNNs.
Since it is very challenging to bound the Radermacher complexity of ReLU networks directly in
terms of the Fisher-Rao norm, we consider decomposing the ReLU network into the sum of a linear
network and a difference term, i.e., yL = ψ(θ)x + (yL - ψ(θ)x). For the linear network part
ψ(θ)x, the Rademacher complexity can be bounded directly by Fisher-Rao norm. For the difference
term (yL - ψ(θ)x), we further decompose it into a sum of simpler terms and then upper bound the
Rademacher complexity of these simpler terms by matrix 1-norm. We first give the results for the
linear network part.
Lemma 3. Define Fr= {x → [ψ(θ)x]y ： ∣∣θ∣∣fs ≤ r,y ∈ Y} where X ∈ RLd and ψ(θ):=
(VWl-1U, VWl-2U, •…，VU). For any data x1,x2, ∙∙∙ ,Xn drawn i.i.dfrom the distribution D,
collect them as columns of a matrix X ∈ RLd×n. Then we have
3	, 一 .
Rn (Fr ) ≤
r∣X IIf
2n
λλmin(E(xXT)) ,
assuming that E(XXT) is positive definite.
Remark 2. If E(X) = 0, E(XXT) is the covariance matrix of random variable X.
Remark 3. We should mention that our assumption that E(XXT) is positive definite is not so restric-
tive and usually holds in practice. For example, for the case that X is continuous random variable,
we can prove that E (XXT ) is positive definite as follows. Suppose that X is a continuous random
variable in the n-dimensional subspace X ⊂ Rn. If there exists u ∈ Rn such that uTE(XXT)u = 0,
then for any X ∈ X we have uTX = 0, i.e., u ⊥ X. Since X is n-dimensional, the only u that
satisfies is that u = 0. Therefore, by definition, E(XXT) is positive definite. As we will show
in Section 3.3, this assumption can be removed, and a more general generalization bound will be
presented.
Now we bound the Rademacher complexity of the difference term yL - ψ(θ)X. With a slight abuse
of notations, given input data X1,X2,… ,Xn ∈ RLd the corresponding g1,g2,… ,gn ∈ RLm and
hl,h2,…，hn ∈ RLm are calculated by (1). We collect all input data as a matrix denoted by
X, all input data at time t as a matrix denoted by Xt , all input of the hidden layer at time t as a
matrix denoted by Gt and all output of hidden layer at time t denoted by Ht, where X ∈ RLd×n ,
Xt ∈ Rd×n, Gt ∈ Rm×n, Ht ∈ Rm×n and t = 1,…,L. The difference term can be decomposed
by the following lemma.
Lemma 4. Define Ht00 := Ht - Gt. Then the following equality holds
L
VHL - ψ(θ)X = XVWL-iHi00.
i=1
4
Published as a conference paper at ICLR 2020
To bound the Rademacher complexity of each term in the above decomposition, we need a technical
lemma given as follows.
Lemma 5. For any P ≥ 1, ∣∣H00∣∣p ≤ mP(I-P)n 1(I-P)∣∣Gt∣∣p.
As we will see, the operator norm in Lemma 5 will be instantiated for the case of p = 1. The use
of || ∙ || ι helps avoid the appearance of the dimension m when upper bounding the Rademacher
complexity. Also it guarantees that Rademacher complexity has a convergence rate O(1/n). The
upper bound for the Rademacher complexity of these individual term is given by the following
lemma.
Lemma 6. Let Ω := {θ = (U, W, V) : ||VT||i ≤ βv, ||Wτ∣∣ι ≤ βw, ||UT||i ≤ βu}. Thenfor
any i = 1,…，L, we have
1	1i
Eσ F MVWL-*" ≤ n βV βUX "XTh,
where σ = (σ1,σ2, ∙∙∙ , σn)T is Rademacher random variable and [∙]y, represents the y-th row of
the matrix.
We are now ready to put the ingredients together to prove our first theorem.
Theorem 1 (Rademacher complexity of RNNs). Let Ω := {θ = (U, W, V) : || VT∣∣ι ≤
βv, ||WT∣∣ι ≤ βw, ||UT∣∣ι ≤ βu, ∣∣θ∣∣fs ≤ r}. Then, the empirical Rademacher complexity of
RNNs with ReLU can be bounded as follows
Eσ( Sup 1 Pn=ι[yLθ (Xi )]y。，≤ rφF J、京 t、、+1 βv βu ||X T ∣∣lΛ ,
θ∈Ω,y∈Yn	2n λ λmin(E(xxτ )) n	,
1-βL	2
where Λ := ɪ-* (] - /：-LeW) ifβw = 1 and Λ := l+l- for βw = L
To establish the generalization bound for RNNs, we need the following classical results for multi-
class margin bounds.
Lemma 7 ((Kuznetsov et al., 2015), Theorem 2). Let H ⊆ RX×Y be a hypothesis set with Y =
{1, 2, ∙∙∙ ,k}. Fix a > 0. Then, for any δ > 0, with probability at least 1 一 δ, the following
multi-class classification generalization bound holds for all h ∈ H:
1 n	4k
R(h) ≤ — φα( φα(Mh(Xi, yi)) + ~^R^n(πi (H)) + 3 * * * * * * *
n i=1	α
where Π1 (H) = {X → h(X, y) : y ∈ Y, h ∈ H}.
The generalization bound for RNNs follows from combining Theorem 1 and Lemma 7.
Theorem 2. Fix margin parameter α, then for any δ > 0, with probability at least 1 - δ, the follow-
ing holds for every RNN whose weight matrices θ = (U, W, V) satisfy ||V T ||1 ≤ βV, ||W T ||1 ≤
βw, ∣∣uT1ll ≤ βu and ∣∣θ∣∣fs ≤ r:
E[1 MyL (χ,y)≤0] ≤ n PlM yL (χi,yi)≤a + 4k(普普 J λmin(E(xxT)) + n BV eU||X Tll1A) 十
3
(3)
Comparison with existing results. We compare our result with the existing generalization bounds
(Zhang et al., 2018; Chen et al., 2019). In comparison with the bound in Zhang et al. (2018), which
max{d, m, k}L2 ||U ||2 ||V ||2 max{1, ||W ||2L}
is of the order O(--ɪ------------  -----------ɪ--~~^ɪɪ), there is no explicit appearance of
nα
the network size parameters d and m in our bound. As we have mentioned before, the reason that
we can avoid these dimensional factors is that we use matrix 1-norm instead of spectral norm to
5
Published as a conference paper at ICLR 2020
upper bound the Rademacher complexity of the network. Moreover, there is always a L2 factor in
their bound, whereas the L2 term only occurs in our bound when ||WT ||1 = 1. For the case that
||WT || > 1, our bound has only a linear dependence on L, and for the case that ||WT ||1 < 1, by
simple calculation, We can show that Λ ≤([,叩严 and the dependence on L would vanish. Both
of our bounds have an exponential term ||W ||L, which would make the bounds become vacuous
for ||W || > 1. It should also be pointed out that our bound scales linearly with the number of
classes since we handle multiclass on each coordinate of a k-tuple of functions and pay a factor
of k. Chen et al. (2019) also derived generalization bound for RNNs in terms of spectral norm
and the total number of parameters of the network by using covering number analysis. Since their
work assumed that the activation function in the hidden layers was bounded rather than the ReLU
activation function considered in our paper, their bound is not directly comparable to ours, and we
do not make a comparison here due to the page limit. We should emphasis that our proof technique
is totally different from the PAC-Bayes approach (Zhang et al., 2018) and covering number analysis
(Chen et al., 2019). In particular, we work on the Rademacher complexity of RNNs directly with no
invocation of complicated tools such as covering number, which makes our analysis conceptually
much simpler. There is also an additional bonus of our proof technique. In the next section, we
will use this proof technique to derive a generalization bound for RNNs when training with random
noise.
3.3	Generalization Bound for Training with Random Noise
The generalization bound in Theorem 2 requires the input covariance matrix E(xxT ) to be positive
definite and would become very poor when the smallest eigenvalue is close to 0, which greatly
limits the power of our bound. To address this issue, we consider adding random noise to the input
data. We notice that after adding random noise with mean 0 and variance σ2, the term E(xxT )
in the bound becomes E((x + )(x + )T) and the smallest eigenvalue of E((x + )(x + )T) is
(λmin(E(xxT)) + σ2), which is greater than σ2. Therefore, our bound is still applicable even when
the covariance matrix of original input data is rank-deficient. Involving noise variables has been
widely used in recurrent neural networks as a regularization technique (Bayer et al., 2013; Zaremba
et al., 2014; Dieng et al., 2018; Gal & Ghahramani, 2016). For example, Bayer et al. (2013) claimed
that conventional dropout did not work well with RNNs because the recurrence amplified noise,
which in turn hurt learning. To fix this problem, Zaremba et al. (2014) proposed to inject noise only
to the input and output of RNNs. Although their method greatly reduced overfitting on a variety
of tasks, the generalization guarantee was not provided. In this section, we present a generalization
bound for RNNs with noise training. For simplicity, we assume that the noise is drawn i.i.d. from
a Gaussian distribution with zero mean and variance σ2 . Let i denotes the d-dimensional gaussian
noise generated at step i and E = (6ι, o, ∙∙∙ , EL) ∈ RLd We collect all noise data as a matrix
denoted by X . To prove the generalization bound, we need to use the Lipschitz property of RNNs
given by the following lemma.
Lemma 8. For every RNN in (1) with weight matrices θ = (U, W, V ), yL is Lipschitz with respect
to || ∙ || ∞, i.e.,
||yL(X)-yL(χ0)ll∞ ≤X I∣V T llιl∣u T llιl∣w τllL-il∣χi-χill∞
i
for any X = (xι, x2,…,xl), X = (x1, x2,…，XL) ∈ R^d.
The generalization bound for training with random noise is described as follows.
Theorem 3. Fix margin parameter α, then for any δ > 0, with probability at least 1 - δ over a
Sample ((xι, €ι, yι), (x2, €2, y2),… ,(xn, ∈n, yn)),thefollowingholdsforeVeryRNNwhoseweight
matrices θ = (U, W, V) satisfy ||VT||i ≤ βv, ||WT||i ≤ βw, ||UT||i ≤ βu and ∣∣θ∣∣fs ≤ r
E[IMyL (x,y)≤0] ≤ n P φα(MyL (Xi + Ei,yi)) + 工 PieVBuβL-iσe P2 log(2d) + 3↑J 2n T
竺(r||X + XJIF r	1 T ɪ + 1 βvβu||XT + XT||ιΛ)
α 2n	λmin(E(XXT)) + σ2 n
Remark 4. The above bound can be easily extended to other kinds of noises by replacing
Qep2log(2d) by Ee∣∣Ei∣∣∞.
6
Published as a conference paper at ICLR 2020
Remark 5. The bound in Theorem 3 is an extension of that in Theorem 2 and can be ap-
plied even when the smallest eigenvalue of E(xxT ) is very close to 0. For example, when
λmin(E(xxT)) = 1 × 10-6, applying Theorem 2 directly might lead to a vacuous bound. But
if using Theorem 3 by choosing a small noise with mean 0 and variance 0.01, we might obtain a
better bound since the term
1
λmin(E((xXT ))) + σ(
≤ 10. Notice that adding noise can not always
guarantee an improved generalization especially when λmin(E(xxT)) is not so small as it incurs
2
an additional linear term 一 Pi βvβu/印 σe，2 log(2d) and might also increase other parameters
in the bound such as ||X + X||F . Therefore we suggest adding noise only when the smallest
eigenvalue of E(xxT) is very small. For this case, a small noise such as σ = 0.1 not only can
1
greatly improve the term
λmin(E(xxT))
but also ensure that the extra cost σe vz2 log(2d) and
||X + X||F/n be small enough since ||X + X||F/n ≤ ||X ||F /n + ||X||F/n and ||X||F/n
would be small when n is large.
Remark 6. If We remove the constraint condition ∣∣θ∣∣fs ≤ r, which means that We
do not have any knowledge about the gradients, the generalization bound in Theorem 2
and Theorem 3 still holds by substituting r with βvβuB((1_：皿)2 + ι-βw) for βw <
1. But With this extra gradient measure, the bound can become much tighter, especially
when λmin(E(xxT ) is small. Please refer to the detailed analysis in the next section.
Experiments. We now study the effect of ran-
dom noise on generalization of RNNs empir-
ically. For simplicity, we consider the IMDB
dataset, a collection of 50K movie reviews for
binary sentiment classification. We use GloVe
word embedding to map each word to a 50-
dimensional vector. We train vanilla RNNs
with ReLU activation function for sequence
length L = 100. The corresponding smallest
eigenvalue of E(xxT ) is approximated by us-
ing the total training data, which is 4 × 10-4 .
We add Gaussian noise to the input data in the
training process with σ = 0.1, 0.2, 0.3 and
0.4. The generalization error which is the gap
between test error without noise and training
error with noise for L = 100 and different val-
ues ofσ is shown in Figure 1 (results for other Figure 1: Generalization error for training with
values of L in Appendix D). We observe that noise (mean ± standard error averaged on 5 runs).
as we start injecting noise, the generalization
error becomes better. But when the deviation
of noise keeps growing, the generalization error shows an increasing tendency. This behavior is
consistent with the prediction made by our bound.
3.4	Analysis of Generalization Bound
Our theoretical results have a number of implications for the generalization performance in RNNs,
and some of them have been observed in empirical studies. We summarize these implications as
follows.
3.4.1	GENERALIZATION AND SMALLEST EIGENVALUE OF E(xxT)
According to our results, the generalization performance in RNNs is influenced by the smallest
eigenvalue ofE(xxT). Since the smaller eigenvalues may contribute to high frequency components
of the input signal, our bound suggests that high frequency information is potentially more difficult
to generalize, which is consistent with intuition. There are many factors that impact on the smallest
eigenvalue and therefore the generalization performance in RNNs. In particular, we study the effect
of the correlation between features on the generalization in RNNs. The exact answer for this problem
may be complicated. Here we only make an initial attempt and claim that weaker correlation would
7
Published as a conference paper at ICLR 2020
help improve the generalization, and a non-rigorous proof is given as follows. Denote the covariance
matrix E(xxT) by Ξ where each element ξij in Ξ represents the covariance between feature i and j.
Suppose that ∣∣Ξ - I||i ≤ Z with Z < 1. By definition of || ∙ ||i matrix norm, We immediately get
∣ξii - 11 + Pj=i ∣ξij | ≤ Z for any i Then by simple derivation, we obtain ξu - Pj=i ∣ξij | ≥ 1 - Z
for any i. Applying Gershgorin circle theorem, we have that the smallest eigenvalue must be greater
or equal than 1 - Z. Since the element ξij with i 6= j represents the covariance between feature i
and j, a weaker correlation between feature i and j means a smaller value of ∣ξj | and we need a
smaller ξ to upper bound || Ξ -1 ||i, which gives us a bigger lower bound on the smallest eigenvalue.
Therefore the generalization bound becomes better.
3.4.2	Generalization and Trainability
The generalization of RNNs also depends on parameters βU, βV , βW and r, where βU, βV and βW
control the weight matrices and r represents the gradient measure. It has a natural relationship with
the training process. The normal procedure in training RNNs is to use weight decay for regulariza-
tion and gradient clipping to avoid the exploding gradients problems (Bengio et al., 1994; Pascanu
et al., 2013). From the perspective of generalization, these strategies can decrease the value of these
parameters βU , βV , βW and r and thus improves the generalization. For example, if βW ≤ 1, we
have Λ ≤([—匕印产,and the second term — βvβu||Xτ∣∣ιΛ in the generalization bound would be
small when βW is not so close to 1. Similarly, if λmin (E(xxT)) is very small, by setting the gra-
dient clipping value in the training procedure, we can achieve a smaller value of r and thus good
generalization. Therefore our bound partially explains why training RNNs in this way can achieve
good performance in practice.
3.4.3	Generalization and Gradient Measure
We are interested in how the gradient measure contributes to generalization. Suppose now that we
only have the weights, i.e., the parameters βU, βW and βV and the gradient measure parameterized
by r is unknown to us. To apply our bound, a natural idea is to infer the gradient measure parameter
r based on the known weight parameters. An upper bound for r in terms ofβU, βW and βV is given
as follows. Under the same conditions as Theorem 2, if we further assume that the data x be given
with ||xT || ι ≤ B, by the definition of ∣∣∙∣∣fs in (2), for any y ∈ Y, we have
((T (θ)c)y )2 = ((L + 1)[V ]y,W LTUxI + L[V ]y,W L-Ux + …+ 2[V ]y,UxL)2
≤ (I(L +1)[V]y,WLTUxil + |L[V]y,WL-2Ux2l + …+ |2[V]y,UxL∣)2
≤ ((L + 1)βVβUBβWL-1 + LβVβUBβWL-2 + 2βVβUB)2	,
=(βvβuB(βW--WW + 2-MWβW ))2 ≤ (βvβuB( (1⅛2 + 1-βw))2
for βW < 1, and ((T(θ)c)y)2 ≤ (βVβUB3L+L2)2 for βW = L The above inequality holds
for any x and y. So we can get ∣∣θ∣∣fs = E(maxi[(τ(θ)c)i]2)1/2 ≤ βvβuB( (1-βW产 + 1-^)
for βw < 1. By replacing r with βvβuB((1,叩产 + 二W),the generalization bound (3) also
holds. But notice that this bound is obtained without any knowledge about the gradients. If we
happen to know that the parameter r is much smaller than βvβuB((1,叩产 + 二叩),for example,
by setting gradient clipping value to be small in training process, this extra gradient measure can
provide us with a better generalization bound, especially when the smallest eigenvalue of E(xxT)
is small. Therefore the introduction of Fisher-Rao norm can help eliminate the negative effect of
λmin (E(xxT)) and thus improve the generalization bound. 4
4 Conclusion
In this paper, we propose a new generalization bound for RNNs in terms of matrix 1-norm and
Fisher-Rao norm, which has no explicit dependence on the size of networks. Based on the bound, we
analyze the influence of covariance of features on generalization of RNNs and discuss how weight
decay and gradient clipping in the training can help improve generalization. While our bound is
useful for analyzing generalization performance of RNNs, it would become vacuous because of an
exponential term when ||W T ||1 > 1. It is of interest to get a tighter bound which can avoid this
8
Published as a conference paper at ICLR 2020
exponential dependence. Moreover, our bound only applies to vanilla RNNs with ReLU activation,
and extending the results to tangent and sigmoid activation functions or other variants of RNNs like
LSTM and MGU might be an interesting topic for future research.
Acknowledgments
This work was supported by Australian Research Council Project FL-170100117. We would like to
thank Gemeng Zhang from Tulane University for helpful discussions and all the reviewers for their
constructive comments.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Zeyuan Allen-Zhu and Yuanzhi Li. Can sgd learn recurrent neural networks with provable general-
ization? arXiv preprint arXiv:1902.01028, 2019.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban, and Patrick
van der Smagt. On fast dropout and its applicability to recurrent networks. arXiv preprint
arXiv:1311.0701, 2013.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.
Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural
networks, 2019. URL https://openreview.net/forum?id=Skf-oo0qt7.
Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng
Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, et al. State-of-the-art
speech recognition with sequence-to-sequence models. In 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pp. 4774-4778. IEEE, 2018.
Adji Bousso Dieng, Rajesh Ranganath, Jaan Altosaar, and David Blei. Noisin: Unbiased regu-
larization for recurrent neural networks. In International Conference on Machine Learning, pp.
1251-1260, 2018.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in neural information processing systems, pp. 1019-1027, 2016.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Proceedings of the 31st Conference On Learning Theory, volume 75 of
Proceedings of Machine Learning Research, pp. 297-299. PMLR, 2018.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850, 2013.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piece-
wise linear neural networks. In Conference on Learning Theory, pp. 1064-1068, 2017.
Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1700-1709,
2013.
Vitaly Kuznetsov, Mehryar Mohri, and U Syed. Rademacher complexity margin bounds for learning
with a large number of classes. In ICML Workshop on Extreme Classification: Learning with a
Very Large Number of Labels, 2015.
9
Published as a conference paper at ICLR 2020
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geome-
try, and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.
Vladimir Alexandrovich Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues for
some sets of random matrices. Matematicheskii Sbornik, 114(4):507-536, 1967.
Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.
In 2012 IEEE Spoken Language Technology Workshop (SLT), pp. 234-239. IEEE, 2012.
John Miller and Moritz Hardt. Stable recurrent models. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=Hygxb2CqKm.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422-2430, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=Skz_WfbCZ.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role of
over-parametrization in generalization of neural networks. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=BygfghAcYX.
Samet Oymak. Stochastic gradient descent learns state equations with nonlinear activations. arXiv
preprint arXiv:1809.03019, 2018.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International Conference on Machine Learning, pp. 1310-1318, 2013.
J Saniuk and I Rhodes. A matrix inequality associated with bounds on solutions of algebraic riccati
and lyapunov equations. IEEE Transactions on Automatic Control, 32(8):739-740, 1987.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax
bottleneck: A high-rank RNN language model. In International Conference on Learning Repre-
sentations, 2018. URL https://openreview.net/forum?id=HkwZSG-CZ.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. 2017. URL https://arxiv.org/abs/
1611.03530.
Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient
svd parameterization. In International Conference on Machine Learning, pp. 5801-5809, 2018.
10
Published as a conference paper at ICLR 2020
A Proofs in Section 3.1
A.1 Proof of Lemma 1
Proof. To begin with, by equation (1), we have yL = V hL . Then the derivative of yL with respect
to vab can be calculated as
SL =(0, 0,…，[hL]b,…，O)T,
∂vab
i.e., a k-dimensional vector with a-th element [hL]b and all other elements zero. Multiplying both
sides by vab and summing them up, we get
X yLVaVab = VhL = VL.
∂vab
a,b
The derivative of yL with respect to W and U can be derived by using chain rule in the similar way
as follows.
and
∂VL =	VdhL	
∂Wij	∂Wij	
∂hL 		= ∂Wij	diag(p(gL))(0,…	∙ , [hL-1]j
∂VL 二	二V驾	
dUpq	dUpq	
∂hL 		= ∂Upq	二 diag(ρ0(gL))(O, •	∙ ∙ , [xL]q, ∙
,…，O)T+diag(p0(gL))W¾1
…，O)T + diag(p(gL))W dhLT
∂upq
where we use the property of ReLU activation function that ρ(z) = ρ0(z)z. Summing up these terms
immediately gives us the following equality.
L ∂hL	. ∂h∂hL
ij 西 Wj+Iq ∂upq UPq
=diag(ρ0(gL))(P(0, ∙ ∙ ∙ , [hL-1]j, …，O)T Wj + P(0, ∙ ∙ ∙ , [xL]q, …，O)T upq) + diag(ρ0(gL))
i,j
W (P T…P d∂hUp-1 UPq )
p,q
diag(ρ0(gL))(WhL-1 + UxL) + diag(ρ0(gL))W (
∂hL-1
i,j
hL + diag(ρ0(gL))W (P dhL-1 Wij + P dhL-1 Upq).
i,j ∂Wij	p,q ∂upq
∂wij
Wij+ P ¾-∙ Upq )
p,q ∂Upq
For ease of exposition, define % :=P jL Wij + P ,L UPq. Then the above equality can be
rewritten as
fL = hL + diag(ρ0(gL))W fL-1.
By induction, we have
fL = hL + diag(ρ0(gL))WhL-1 + diag(ρ0(gL))W diag(ρ0(gL-1))W hL-2+
diag(ρ0(gL))W diag(ρ0(gL-1))...W diag(ρ0(g2))W h1	.
Multiplying both sides by V and using some basic calculation, we can show that VfL = LyL -
η(θ)(0,1,…，L - I)T. Therefore,
PSbVab + P∂⅛Wj+P∂⅛Upq =VL + VfL = (L+1)yL- η(θ)(0,1,…，L - 1)T .
Substituting vl = η(θ)(1,1, •…，I)T into the above equality leads to the desired result
P 豫Vab+P ∂Wj Wj+P ∂UPq UPq=η(θ)c.
□
11
Published as a conference paper at ICLR 2020
A.2 Proof of Lemma 2
Proof. Using the definition of Fisher-Rao norm,
d(yLθ ,y)
dVLθ
dl(VLθ,y)
dV LL
llθllfr = E(< θ, Vl(yLθ,y) >2) = E(< θ, VyLθ(x)-
=E((θT vyLθ (x) F)2)=E(< NVyL (x)T θ,
>2)
>2)
By Lemma 1, we have VVLL (x)Tθ = η(θ)c. Substituting it into the above equality gives us
网 fr= E(S(θ)C, H*‘ R,")〉2
∂VLL
□
B Proofs in Section 3.2
B.1 Proof of Lemma 3
The proof of Lemma 3 relies on the following result in Saniuk & Rhodes (1987).
Proposition 1. Let X, Y∈ Rn×n with Y symmetric and nonnegative definite. Then,
trace(XY) ≤ ||X||2 ∙ trace(Y).
Now we are ready to prove Lemma 3.
Proof. Denote A := E(((L + 1)xT, LxT,…,2xTT )T((L + 1)xT, LxT ,…，2x1T)). By the defini-
tion of ∣∣θ∣∣fs, for any V ∈ Y, we have
l∣[Ψ(θ)]y,T IIA
=[ψ⑻]y,E(((L + 1)XT, LXT,…，2xT)T((L + 1)XT, LXT,…，2xT))[ψ(θ)]y,T
=E([ψ(θ)]y,((L + 1)XT, LXT,…，2xT)T((L + 1)XT, LXT,…，2xT)[ψ(θ)]y,T)，
= E([(τ(θ)c)y]2) ≤ IIθII2fs
where [ψ(θ)]y, represents the y-th row of ψ(θ). On the other hand, from the definition of
Rademacher complexities,
R n(Fr ) = Eσ( Sup 1 Pn=If(Xi)。# = Eσ(sup1 Pn=I [Ψ(θ)Xi]y σi) = Eσ(sup1 Pn=I [Ψ(θ)]y,Xi σi)
f∈Fr n	i=1	L,y n i=1	L,y n	i=1
=Eσ(Sup < LPn=I xiσi, [ψ(θ)]y,T >) ≤ Eσ(Sup(IIL Pn=ι XWi||A-i ll[ψ(θ)]y,T||A))
θ,y n	,	θ,y n_____________________
≤ rEσ (II1 Pn=I "i∣∣A-ι )= rEσ∖ ∕< (1 Pn=I "i)(1 Pn=I XT σi),A-1>
n	nn
≤ r∖/Eσ < (1 Pn=I xiσi)(1 Pn=I XTσi),A-1 > = rʌ占 < Pn=I XiXT,A-1 〉
nn	n
=r p< XXT,A-1 > = r p< XXT, (CE(XXT)C)-1 > = r Ptrace(XXTCT(E(XXT))TCT)
nn	n
where the first inequality uses Cauchy-Schwarz inequality and C := diαg((L+1)Id,LId, •…，2Id).
Since C-1 and E(XXT) are positive definite, we have
trace(XXT C-1(E(XXT))-1C-1) = trace(C-1(E(XXT))-1C-1XXT)
≤ IIC-1(E(XXT))-1C-1II2trace(XXT) ≤ IIC-1II2II(E(XXT))-1II2IIC-1II2trace(XXT)
=4"(E(XXT)) — 1H2IXIIF ≤ 1 XmjBT))
where the first inequality is by Proposition 1 and the last inequality uses trace(XXT) = IIX II2F.
Therefore,
R n (Fr ) ≤ rf> J
2n
λmin(E(xXT)) .
1
□
12
Published as a conference paper at ICLR 2020
B.2	PROOFOFLEMMA 4
Proof. Denote H[ := UXt + WH0_1 and H1 := UX1. By the definition of Hl, We have
Hl - HL = P(UXL + WHl-i) - UXL - WHL-ι
=p(UXl + WHl-i) - UXl - WHl-i + W (HL-I- HL-1 = HL + W (HL-I- HLj
Which by induction gives
Hl - HL = H'-L + WHL-1 + ∙∙∙ + W L-I(HI- H；) = PL=I WL-iH∕ .
So the difference term can be reWritten as
VHl - Ψ(θ)X = VHl - VHL = PL=1 VWLTH ,
where the second equality uses ψ(θ)X = VHL.	□
B.3	Proof OF Lemma 5
Proof. Using Riesz-Thorin Theorem, we have ||H''||p ≤ ∣∣H''∣∣"∣∣H''∣∣⅛τ"p. And since H''=
ρ(Gt) - Gt, by the definition of the induced L； and Lg matrix norm, we know ∣∣H''∣∣ι ≤ ∣∣Gt∣∣ι
and ∣∣H''∣∣∞ ≤ ∣∣Gt∣∣∞. Therefore,
∣∣H''∣∣p ≤ ||H''||1/p||H',||1-1/p ≤ ||Gt||1/p||Gt||1-1/p ≤ m 1 (1- 1)n 1 (1- 1)∣∣Gt∣∣p ,
where the last inequality uses some basic facts about matrix norm that ∣∣Gt ∣∣ 1 ≤ m1-1/p ∣∣Gt∣∣p and
∣∣Gth ≤ n1/p∣∣Gt∣∣p.	□
B.4	PROOF OF LEMMA 6
Proof. For any y ∈ Y, by Holder,s inequality, for any p, q ≥ 1 with P + ɪ = 1,
n [VW L-iHi']y,σ = 1 [V ]y,W LTHi'σ ≤ 1 ∣∣H∕'T W TLT ]J ∣∣p∣∣σ∣∣g
=∣∣[V ]yτ ∣∣p∣∣W t ∣∣L-i ∣∣H'''τ ||pn1/q-1 ≤ ∣∣[V ]yτ ∣∣p ∣∣W TIILim P (1-P )n-表 ∣∣GiT ∣∣J
In order to eliminate the dimension dependency on m and simultaneously enjoy a faster
convergence rate with respect to n, we choose p = 1. Then the above inequality re-
duces to 1 [VWIHi']y,σ ≤ ∣∣[V]J∣∣1∣∣Wt∣∣L-in-1∣∣Giτ∣∣1 ≤ βvβL-in-1∣∣GiT∣∣1 ≤
βvβL-in-1(βu∣∣X∕τ∣∣1 + βw∣∣Hi-1τ∣∣1). For ∣∣Hi-IT∣∣1, we have
∣∣Hi-1τ∣∣1 = ∣∣ρ(Xi-1τUT + Hi-1τWT)∣∣1 ≤ ||X”1TUT + Hi-2τWT∣∣1
≤ βu∣∣Xi-IT∣∣1 + βw∣∣Hi-2τ∣∣1，
which by induction gives
i-1
∣∣Hi-1τ∣∣1 ≤ βuXβi-1-j∣∣Xjτ∣∣1.
j=1
Therefore,
1	1
Eσ( SuP —[VWL-iHi']y,σ) ≤ — βvβu ][＞广||XjT∣∣1∙
θ∈Ω,y∈Yn	n	M
□
13
Published as a conference paper at ICLR 2020
B.5 Proof of Theorem 1
Proof. Using the notations that we have introduced earlier, the empirical Rademacher complexity
can be rewritten as
Eσ ( sup 1 Pnn=i[yLθ (Xi )]y σi)
θ∈Ω,y∈Y n
=Eσ ( sup 匕 VHL]y,σ)
θ∈Ω,y∈Y n
=Eσ ( suP 匕P3 VWL-iHi" + ψ (CX]y,σ)	,
θ∈Ω,y∈Y n
≤ PL=1 Eσ ( Sup 匕VWL-iH00]y,σ) + Eσ( sup 匕ψ(θ)X]y,σ)
θ∈Ω,y∈Y n	θ∈Ω,y∈Y n
≤ PL=1 Eσ( Sup ɪ[vwL-iH00]y,σ) + Eσ(	Sup	,[ψ(θ)X]y,σ)
θ∈Ω,y∈Y n	∣∣θ∣∣fs≤r,y∈Y n
where the second equality uses Lemma 4 and the last inequality is due to the fact that Ω ⊆ Ω and
Ω ⊆{θ : ∣∣θ∣∣fs ≤r}.
For the last term, by Lemma 3, we have
Eσ(	SuP	1[Ψ(θ)X]y,σ) ≤ rl≡F J λ	t、、.
l∣θ∣lfs≤r,y∈Yn	2n	V λmin(E(Xx ))
The other terms can be handled by Lemma 6 in the following way.
PL=1 Eσ ( Sup 1[VWL-Hnyc ≤ PL=1 (1 βvβυ Pj = 1 βW-j IIXjTIll)
θ∈Ω,y∈Y n
W PL ( 1 D R Pi EL-j I I γT I I ) _ 1 βVβU||X ll 1 (I - βW TRL )
≤ Ei=I (nβvβU2=ιβw I|X ll1) = n 1 - βw (ŋw - LeW)
12
for βw = 1, and Pi=ι Eσ( SuP —[VWL H00]yσ) ≤ ɪβvβυB2 L+L for βw = 1, where the
一	θ∈Ω,y∈Yn
second inequality uses the definition of matrix norm ∣∣∙∣∣ 1.
Collecting all terms, we establish
Eσ(	Sup	1 Pn=ι[yLθ(Xi)]yσi)	≤	r≡krλ	.	(ɪ	xt))	+1 βvβuIIXt∣∣iA.
θ∈Ω,y∈γn	乙n	V 八min(L(Xx )) n
□
C Proofs in Section 3.3
This section includes the full proofs of the generalization bound for training with random noise.
C.1 Lipschitz properties of ReLU nonlinearities and Margin operator
We first establish the Lipschitz properties of the ReLU activation function and margin operator
M(yL(X), y) := MyL (X, y).
Lemma 9. Letρ : Rn → Rn be the coordinate-wise ReLU function, then it is 1-Lipschitz according
to II ∙ IIpforanyP ≥ 1.
Proof. For any X, X0 ∈ Rn ,
iiP(X)-P(X0)"p = (X IP(X)i -P(X'F)1/' ≤ (X Ixi - xiIP)1/P = IIx - "Up.
□
14
Published as a conference paper at ICLR 2020
Lemma 10. For every j and every P ≥ 1, M(∙, j) is 2-Lipschitz wrt || ∙ ||p.
Proof. Let y, y0 ,j be given, and suppose that M (y, j) ≤ M (y0, j) without loss of generality. Select
coordinate i = j so that M(y,j) = y§ — yi. Then
M(y',j) -M(y,j) = y'j — maxy'l — yj + yi ≤ (yj — yj) + (yi — y'i) ≤ 2||y, — y"∞ ≤ 2||y - y0||p.
l=j
□
C.2 PROOF OF LEMMA 8
Proof. We prove this Lemma by induction. Let x = (xι, x2,…，xl) and x' = (x1,x2, ∙ ∙ ∙ , x1).
Denote g' := UXt + Wht-1, h] := ρ(g') and y' := Vht. Then We have
||ht - ht||∞ = ||p(gt) — PWt*∞ ≤ ||gt — g'h = UUXt + Wht-I- UXt- Wht-ιh
±|UT||i||xt-xth + ||W T||i||ht-i —ht-ιh,
where the first inequality uses Lemma 9 and the second inequality uses basic properties of || ∙ ||m.
By induction, we get
||hL - hL∖∖∞ ≤ X||Ut||i||wT||L-i||xi - xih.
i
Therefore,
||yL-yLH∞ = ||VhL-Vh≤ ||v||TO||hL-hLH∞ ≤ X||vt||i||ut||i||w7叶-1肩—必卜
i
□
C.3 PROOF OF THEOREM 3
We begin by establishing two auxiliary lemmas that we will need for the subsequent theorem.
Lemma 11. For every RNNs in (1) with weight matrices θ = (U, W, V), the following inequality
holdsfor any X = (x1,x2, ∙ ∙ ∙ ,xl ) and y.
2
|Ee^”(MyL (x,y)) - Φα(MyL (x + e,y))U≤ - EIIV T||i||U T||i||W T||L-i(Ee||ei h).
i
Proof. For any fixed x and y ,
|Ee[Φa(MyL (x, y)) - Φα(M^ (x + e, y))]| ≤ Ee|Φa(MyL (x, y)) - Φα(M^ (x + e, y))|
22
≤ — E/|yL(x)-yL(x + e)h ≤ ZEe(Pi ||VT||i||UT||i||WT||L-i||eih)	,
=-P"VT||i||UT||i||WT|次一(吼|同£)
where the first inequality uses Jensen,s inequality, the second inequality follows from the a -
Lipschitz property of Φα(∙) and Lemma 10 and the last inequality is by Lemma 8.	□
Lemma 12. Let {g}d=I be an i.i.d SequenCe of N(0,σ2) variables, then E[maxi 匕|] ≤
σ√2log(2d).
Proof. Define Z = [maxi |g|]. For any t > 0, by Jensen, inequality, we have
etE(z)≤ E(etZ) = E(maxet±l) ≤ XE(et匕1) = 2dΦ(σ2t)eσ2t2/2 ≤ 2deσ2t2/2,
i
where the second inequality uses the definition of normal distribution and Φ is the cumulative dis-
tribution function of the standard normal distribution. Taking logs on both sides and dividing by t,
we get
E(Z) ≤ 等虫 + p.
15
Published as a conference paper at ICLR 2020
Choosing t
V2log(2d)
σ
leads to the desired result,
E(Z) ≤ σ∖Jc2 log(2d).
□
We now return to the proof of Theorem 3.
Proof. For any RNNs with weight matrices θ = (U, W, V ) satisfying ||V T ||1 ≤ βV , ||WT ||1 ≤
βW, ||U T ||1 ≤ βU, we have
|Ex,y [Φα (MyL (x, y))] -Ex,,y[φα(MyL (X + e,y)X
=|Ex,y (φα(MyL (x,y)) - Ee[φα(MyL (X + 的夕))])|	,
≤ Eχ,y∣Φα(MyL (x,y)) — Ee [Φα(MyL (X + 的沙))]| ≤ £ Pi βv βu 户“吟同∣∞)
where the first equality is due to the fact that the input X and noise are independent, the first
inequality uses Jensen’s inequality and the last inequality follows from Lemma 11. The inequality
above can be rewritten as follows.
2
Ex,y [φα(MyL (X, y))] ≤ Ex,e,y [φα(MyL (X + 3 y)) + £ PieVβUβW (EeIkih) .
For the first term in the right hand side of the above inequality, by Theorem 2, with probability at
least 1 - δ, the following holds:
Ex,e,y [Φα (MyL (X + , y))
4k /r∣X + Xe∣∣F /	1
α	2n	λ λmin(E((X + E)(X + E)T))
-P Φα(MyL (Xi + Ei,yi))
n
+ nβv Bu |X T + XT || iλ) + 3
4k zr∣X + Xe∣∣F /	1
α	2n	λ λmin(E(XXT) + E(EET))
：P Φα(MyL (Xi + Ei,yi))
+ nβv Bu ||x T + XT "iλ) + 3
竺 zr||X + Xe∣∣F /	1
α 2n λ λmin(E(XXT) + σ2I)
-P Φα(MyL (Xi + Ei,yi))
+ nβv Bu ||X T + XT ||1八)+ 3
竺 zr||X + Xe∣∣F /	1
α	2n	λ λmin(E(XXT)) + σ2
-P Φα(MyL (Xi + Ei,yi))
+ nβv Bu ||x T + XT "iλ) + 3
Combining the above two inequalities together leads to
E[l MyL (χ,y)≤0] ≤ Eχ,y [Φɑ (MyL (x,j))]
1L	2
≤ n P φα(MyL (xi + Ei, yi)) + £ Pi βVβUβW (EeIIEi"∞) +
4k (rI|X + XeIIF r 入(FJTv 三 + 1 Bv Bu ∣∣XT + XTIIC +3
£	2n	λmin(E(XXT)) + σe2	n	e
where the first inequality makes use of the fact that Iu ≤ Φɑ(u). Therefore, the desired result can
be immediately obtained by substituting EeIkiII∞ with σe，2 log(2d) according to Lemma 12. □
D Supplementary figures
Figure 2 shows the behavior of generalization error for RNNs as the standard derivation of noise σe
varies for the sequence length L = 200 and 300 trained on IMDB dataset. As in the body, increasing
σe will first improve the generalization error and then, after a certain point, harm the performance
of RNNs.
16
Published as a conference paper at ICLR 2020
Figure 2: Generalization error for training with noise (mean ± standard error averaged on 5 runs).
The left and right panel are for L = 200 (smallest eigenvalue: 1 × 10-4) and L = 300 (smallest
eigenvalue: 2.5 × 10-5) respectively.
17