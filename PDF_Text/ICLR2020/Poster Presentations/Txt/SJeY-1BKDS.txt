Published as a conference paper at ICLR 2020
Understanding '4-based Dictionary Learning：
Interpretation, Stability, and Robustness
Yuexiang Zhai1,2*	Hermish Mehta1	Zhengyuan Zhou3	Yi Ma1
1Department of EECs, UC Berkeley	2ByteDance inc.	3stern school of Business, NYU
Ab stract
Recently, the '4-norm maximization has been proposed to solve the sparse dictio-
nary learning (sDL) problem. The simple MsP (matching, stretching, and pro-
jection) algorithm proposed by Zhai et al. (2019a) has shown to be surprisingly
efficient and effective. This paper aims to better understand this algorithm from its
strong geometric and statistical connections with the classic PCA and iCA, as well
as their associated fixed-point style algorithms. such connections provide a uni-
fied way of viewing problems that pursue principal, independent, or sparse com-
ponents of high-dimensional data. Our studies reveal additional good properties of
'4-maximization: not only is the MSP algorithm for sparse coding insensitive to
small noise, but it is also robust to outliers and resilient to sparse corruptions. We
provide statistical justification for such inherently nice properties. To corroborate
the theoretical analysis, we also provide extensive and compelling experimental
evidence with both synthetic data and real images.
1 Introduction
The explosion of massive amounts of high-dimensional data has become the modern-day norm for
a large number of scientific and engineering disciplines and hence presents a daunting challenge for
both computation and learning. Rising to this challenge, sparse dictionary learning (sDL) provides
a potent framework in representation learning that exploits the blessing of dimensionality: real data
tends to lie in or near some low-dimensional subspaces or manifolds, even though the ambient
dimension is often extremely large (e.g. the number of raw pixels in an image). More specifically,
sDL (Olshausen & Field (1997); Mairal et al. (2008; 2012; 2014); spielman et al. (2012); sun et al.
(2015); Bai et al. (2018); Qu et al. (2019)) concerns the problem of learning a compact, sparse
representation from raw, unlabelled data: given a data matrix Y = [y1, y2, . . . , yp] ∈ Rn×p that
contains p n-dimensional samples, one aims to find a linear transformation (i.e. a dictionary) D ∈
Rn×m and an associated maximally sparse representation X = [x1, x2, . . . , xp] ∈ Rm×p that
satisfies
Y = DX.	(1)
As the data matrix Y can represent a variety of signals (e.g. images, audios, languages, and genetics
etc) in practical applications, sDL provides a versatile structure-seeking formulation that has found
widespread applications in computational neuroscience, image processing, computer vision, and
machine learning at large (Olshausen & Field, 1996; 1997; Argyriou et al., 2008; Ranzato et al.,
2007; Elad & Aharon, 2006; Wright et al., 2008; Yang et al., 2010; Zhang et al., 2013; Mairal et al.,
2014; Zhang et al., 2014; 2019).
Related Works. Motivated by this practical significance, there has been a growing surge of interest
recently (e.g. Rambhatla et al. (2019); Bai et al. (2018); Gilboa et al. (2018); Nguyen et al. (2018);
Chatterji & Bartlett (2017); Mensch et al. (2016)) that aims to tackle sDL. in attempts to recover the
sparse signals X, these existing work adopt an '0- or '1 -penalty function to promote the underlying
sparsity and give various optimization algorithms for the resulting objectives (some of those are
heuristics while a few others have theoretical convergence guarantees). Although these penalty
*Work done when interning at ByteDance. YZ would like to thank Chong You at UC Berkeley for mean-
ingful discussions. Correspondence to: Yuexiang Zhai, ysz@berkeley.edu or Yi Ma, yima@eecs.berkeley.edu.
Codes are available at https://github.com/hermish/ZMZM-ICLR-2020.
1
Published as a conference paper at ICLR 2020
functions are indeed sparsity-promoting, the resulting optimization problems must be solved one row
at a time, hence resulting as many optimization problems as the ambient dimension n. Consequently,
'0- or '1-based objectives result only in local methods (i.e. cannot yield the entire solution at once)
and hence entail prohibitive computational burden. Another prominent approach in SDL is Sum-
of-Squares (SOS), proposed by and articulated in a series of recent work (Barak et al. (2015); Ma
et al. (2016); Schramm & Steurer (2017)). The key idea there is to utilize the properties of higher
order SOS polynomials to correctly recover one column of the dictionary at a time, for which there
are m columns in total. However, the computational complexity of these recovery methods are
quasi-polynomial, thus again resulting in large computational expense.
Very recently, in the complete dictionary learning1 setting, a novel global approach has been sug-
gested in Zhai et al. (2019a;b) that presents a formulation which can efficiently recover the sparse
signal matrix X once and for all. In particular, Zhai et al. (2019b) has shown that if the generative
model for Y = DoXo ∈ Rn×p satisfies that Do ∈ O(n; R) is orthonormal and Xo ∈ Rn×p is
Bernoulli-Gaussian sparse,2 then maximizing the '4-norm3 of AY over O(n; R):
max IkAY∣∣4 subject to A ∈ O(n; R) (or AA^ = I),	(2)
is able to find the ground truth dictionary Do up to an arbitrary signed permutation. Moreover, Zhai
et al. (2019b) has proposed the simple “Matching, Stretching, and Projection” (MSP) algorithm,
which is shown to be experimentally efficient and effective, for solving the program in equation 2:
MSP: At+1 = Po(n；R) [(AtY)°3Y*] = Ut匕*,	⑶
where UtV* are from the singular value decomposition: Ut∑t%* = SVD[(AtY)。3Y*].
In this paper, we here give an alternative (arguably simpler and more revealing) derivation of the
MSP algorithm (3). Consider the Lagrangian formulation of the constrained optimization problem
given in equation 2. The necessary condition of critical points, NA ∖ ∣∣ AY ∣∣4 = NA(A, AA* — I)
for some Lagrangian multipliers Λ, implies:
(AY )°3Y * = (Λ + Λ*)A.	(4)
As the optimization is over the orthogonal group O(n; R), restricting the condition in equation 4
onto the orthogonal group yields a necessary condition for any critical point A:4
Po(n；R) [(AY)°3Y*] = A.	(5)
Hence the critical point A can be viewed as a “fixed point” of the map: Po(n；R) [((∙)Y )°3 Y *] from
O(n; R) to itself. The MSP algorithm in equation 3 is to find the fixed point(s) of this map.
Notice that the orthonormal constraint A ∈ O(n; R) in equation 2 can be viewed as enforcing the
orthogonality of n unit vectors simultaneously. So, more flexibly and generally, one may choose to
compute any k, for 1 ≤ k ≤ n, leading orthonormal bases of Do by solving the program:
maχ1 ∣W*Y∣∣4	subject to W ∈ St(n,k;R) ⊂ Rn×k,	(6)
where St(n, k; R) is the Stiefel manifold.5 The orthogonal group O(n; R) and the unit sphere Sn-1
can be viewed as two special cases of the Stiefel manifold St(n, k; R), with k = n and k = 1, re-
spectively. In some specific tasks such as dictionary learning and blind deconvolution, optimization
over the unit sphere has been widely practiced, such as in Sun et al. (2015); Bai et al. (2018); Zhang
et al. (2018); Kuo et al. (2019). The more general setting of maximizing a convex function over any
compact set also has been studied by JOUrnee et al. (2010) in the context of sparse PCA, which has
provided convergence guarantees for this class of programs.
1Complete dictionary learning requires the learned dictionary D in equation 1 to be square and invertible.
2 Each entry xi,j of X can be represented as the product of a Bernoulli variable and a normal Gaussian
variable: xi,j = Ωi,jVij, where Ωi,j 〜iid Ber(θ) and Vi,j 〜iid N(0,1), similar for vectors or scalars. This
is the standard setting adopted in Spielman et al. (2012); Sun et al. (2015); Bai et al. (2018).
3We abuse the notation a bit, by denoting ∣∣∙k4 as the sum of element-wise 4th power of all entries of a vector
and matrix, that is, ∀a ∈ Rn, kak44 = Pin=1 ai4 and ∀A ∈ Rn×m , kAk44 = Pi,j ai4,j.
4For any symmetric matrix S ∈ Rn×n and an orthogonal matrix A ∈ O(n; R), the projection of SA onto
the orthogonal group is A: PO(n;R) [SA] = A, one may see Absil & Malick (2012) for details.
5For any 1 ≤ k ≤ n, St(n, k；R) = {W ∈ Rn×k : W*W = Ik}.
2
Published as a conference paper at ICLR 2020
Our Contributions. Our contributions are twofold. First, by taking a suitable analytical angle, we
reveal novel geometric and statistical connections between PCA, ICA and the '4-norm maximization
based SDL. We then show that algorithm-wise, the fixed-point type MSP algorithm for `4 -norm
maximization has the same nature as the classic power-iteration method for PCA (Jolliffe, 2011)
and the FastICA algorithm for ICA (Hyvarinen & Oja, 1997). This interpretation gives a unified
view for problems that pursue principal, independent, or sparse components from high-dimensional
data and enriches our understanding of low-dimensional structure recovery frameworks, classical
and new, at both formulation and algorithmic fronts.
Second, and more importantly from a practical perspective, we examine how MSP performs under
a variety of more realistic conditions, when the measurements Y could be contaminated with noise,
outliers, or sparse corruptions. We show that, similar to PCA, `4 -norm maximization and the MSP
algorithm are inherently stable to small noise. Somewhat surprisingly though, unlike PCA, the MSP
algorithm is further robust to outliers and resilient to sparse gross errors! We provide character-
izations of these desirable properties of MSP. The claims are further corroborated with extensive
experiments on both synthetic data and real images. Taken as a whole, our results contribute to
the broad landscape of dictionary learning by affirming that `4 -maximization based SDL and the
corresponding global algorithm MSP provide a valuable toolkit to the existing literature.
2 SDL versus PCA and ICA
2.1	Pursuit of Principal, Independent, or Sparse Components
Relation with the Geometric Interpretation of PCA. For a data matrix Y ∈ Rn×p , Principal
Component Analysis (PCA), aiming to find the top (top k) left singular vector (vectors) of Y ,
max1 ∣∣W*YkF	subject to W ∈ St(n,k;R),	(7)
W2	F
can be understood as finding a direction (a k-dimensional subspace) in row(Y) in which Y has the
largest '2-norm (Frobenius norm). For instance, finding the direction with the largest '2-norm over
the unit sphere can be viewed as calculating the spectral norm (or the largest singular value), of
matrix Y . In comparison, we may view equation 6:
max ɪ k W * Y k4 subject to W ∈ St(n,k; R)
as finding a direction, or a k-dimensional subspace, in row(Y) where the projection of Y has
the largest '4-norm. For instance, finding the direction with the largest '4-norm over the unit
sphere (equation 6) can be viewed as calculating the induced ∣∣∙∣∣2 4 norm of matrix Y: ∣∣ Y∣∣2 4 =
maxa∈Sn-1 ka*Yk4 .
Relation with the Statistical Interpretation of PCA. View each column yj , j ∈ [p] of data
matrix Y ∈ Rn×p as an n dimensional random vector that is i.i.d. drawn from a distribution of
random variable y and let YC denote the centered Y: YC = Y [I 一 P11*], where 1 ∈ Rp is a
vector of all 1’s. Then, finding the top k principal components ofYc: maxW ∈St(n,k;R) 1 IIW *YC∣2
is to find k uncorrelated projections of y ∈ Rn that has the top k sample variance (i.e. 2nd order
moment) (Jolliffe, 2011; Helwig, 2017). Similar to PCA, the '4-norm maximization of centered data
matrix YC: maxw∈st(n,k;R) 4 ∣∣ W*YC∣∣4 can be viewed as finding k uncorrelated projections of y
that have the top k sample 4th order moment, whose statistical meaning is better revealed below.
Relation with ICA and Nonnormality. The `4 -norm maximization over the Stiefel manifold is
strongly related to finding the maximal or minimal kurtosis in Independent Component Analysis
(ICA) (Hyvarinen & Oja, 1997; 2000): In order to identify one component of a given random vector
y ∈ Rn, ICA aims to find a unit vector (a direction) w ∈ Sn-1 that maximizes or minimizes the
kurtosis of w* y, defined as:
kurt(w* y) = E (w* y)4 一 3 ∣w∣24 .	(8)
Kurtosis is widely used for evaluating the nonnormality of a random variable; see DeCarlo (1997);
Hyvarinen & Oja (1997; 2000). According to Huber (1985), the nonnormality of data carries “ab-
normal” hence interesting information in real data for many applications (e.g. Lee et al. (2003); Cain
3
Published as a conference paper at ICLR 2020
et al. (2017)). Thus, extracting the 4th order moment helps understand such statistics of real datasets
(HyVarinen et al., 2009) and even their topology (Carlsson, 2009). One may also find that the '4-
maximization based dictionary learning formulation is similar to maximizing kurtosis (equation 8)
with spherical constraint w ∈ Sn-1, in fact, these two formulations are exactly the same if one only
wants to find one column of the dictionary. Intuitively, such coincidence occurs due to the fact that
maximizing '4-norm or kurtosis have the same effect— they both promote the “spikiness” (Zhang
et al. (2018); Li & Bresler (2018)) (or “peak” (DeCarlo, 1997)) of a distribution. More rigorous
analysis regarding the similarity between `4 -based dictionary learning and ICA is beyond the scope
of this paper and we leave them for future research.
2.2	Fixed-Point Style Algorithms
In optimization, the '4-norm maximization in equation 6 over the Stiefel manifold St(k, n; R) is a
special type of nonconvex optimization problem - convex maximization over a compact set. Al-
though JOUrnee et al. (2010); Zhai et al. (2019b) have shown that the MSP algorithm is guaranteed
to find critical points, the experiments in Zhai et al. (2019b) suggest that the MSP algorithm finds
global maxima of the `4 -norm efficiently and effectively. For better understanding, in this section
we illustrate some striking similarities between the MSP algorithm and the “power-iteration” type
algorithms for solving PCA as well as ICA.
Fixed-point Perspective of Power Iteration. For a general data matrix Y ∈ Rn×p , finding the
top singular value of Y is equivalent to solving the following optimization problem:
max 夕(W) = ɪ ∣∣w*Y∣∣2	subject to W ∈ Sn-1.	(9)
w	22
For this constrained optimization, the Lagrangian multiplier method gives the necessary condition:
Vw夕(W) = YY*w = λw, similar to equation 4. If We restrict this condition onto the sphere, We
obtain the fixed point condition W = PSn-I Vw夕(w)]∙ The classic power-iteration method
Wt+1 = Psn-1 [Vw 夕(wt)]
YY * Wt
∣YY *wt∣2,
(10)
is precisely computing this fixed point, which is arguably the most efficient and widely used algo-
rithm to solve equation 9, for PCA (or computing SVD of Y).
Fixed-point Perspective of FastICA. In order to maximize (or minimize) the kurtosis over Sn-1
max Ψ(w)	= (kurt[W*y]	= ；E[W*y]4	- f	IlWII2	subject to W ∈ Sn-1,
w	4	4	42
(11)
Hyvarinen & Oja (1997) has proposed the following fixed-point type iteration:
Wt+1 = Pgn-1 [Vw ψ(wt)]
E [y (y*wt)3] - 3 kwtk2 Wt
IIE [y (y*Wt)I- 3 Ilwtk2 wt∣∣2
(12)
which enjoys cubic (at least quadratic) rate of convergence, under the ICA model assumption.
Fixed-point Perspective of MSP. For the `4 -norm maximization program:
max φ(W) = 1 ∣W *Y ∣∣4	subject to W ∈ St(n,k; R),
through a similar derivation to that in Section 1, one can show that the MSP iteration in equation 5
for the orthogonal group generalizes to the Stiefel manifold case as:
Wt+1 = PSt(n,k;R) [VWφ(Wt)] = UtVt*,	(13)
where Ut∑tV* = SVD[Y(Y*Wt)°3]. The above iteration has the same nature as the power
iteration in equation 10 and equation 12, since they all solve a fixed-point type problem, by project-
ing gradient of the objective function V夕(∙)，Vφ(∙), Vψ(∙) onto the constraint manifold SnT and
St(n, k; R), respectively. Table 1 summarizes these striking similarities.
4
Published as a conference paper at ICLR 2020
	objectives		Constraint Sets	Algorithms	
Power iteration	夕(W)二	=2 kw*Yk2	W ∈ SnT	Wt+1	=Psn-1 [Vw 夕(Wt)]
FastiCA	ψ(w) =.	=1kurt[w*y]	w ∈ Sn-1	Wt+1	=Psn-1 [Vwψ(Wt)]
MSP	φ(W)	'=4 kW*Yk4	W ∈ St(n, k; R)	Wt+1 =	PSt(n,k;R) [VWφ(Wt)]
Table 1: Similarities among fixed-point algorithms for Power Iteration, FastICA, and MSP.
3 Stability and Robustness of '4-Norm Maximization
Even though the MSP algorithm for `4 -norm maximization is similar to power-iteration for PCA, in
real applications, PCA often requires modifications to improve its robustness (Candes et al., 2011;
Xu et al., 2010; 2012). in this section, we want to examine the stability and robustness of the `4-
maximization for different types of imperfect measurement models: small noise, outliers, and sparse
corruptions of large magnitude.
3.1	Different Models for Imperfect Measurements
We adopt the same Bernoulli-Gaussian model as in prior works (spielman et al., 2012; sun et al.,
2015; Bai et al., 2018; Zhai et al., 2019b) to test the stability and robustness of the '4-maximization
framework. Assume our clean observation matrix Y ∈ Rn×p is produced by the product ofa ground
truth orthogonal dictionary Do and a Bernoulli-Gaussian matrix Xo ∈ Rn×p :
Y = DoXo, Do ∈ O(n;R), {Xo}i,j 〜iid BG(θ).	(14)
Now let us assume we only observe different types of imperfect measurements of Y:
Noisy Measurements: YN := Y+G, where G ∈ Rn×p is matrix that satisfies gi,j 〜iid N(0,η2)
and η > 0 controls the variance of the noise.
Measurements with Outliers: YO := [Y, G0], where YO contains extra columns (G0 ∈ Rn×τp)6
generated from an independent Gaussian process g0,j 〜讥& N(0,1). Here, τ controls the portion of
the outliers w.r.t. the clean data size p.
Measurements with Sparse Corruptions: YC := Y + σB ◦ S, where σ > 0 controls the scale of
corrupting entries,7 B ∈ Rn×p is a Bernoulli matrix so bi,j 〜a& Ber(β) with β ∈ (0,1) controlling
the ratio of the sparse corruptions, and S ∈ Rn×p has entries si,j drawn i.i.d. from a Rademacher
distribution:
1	with probability 1/2
-1 with probability 1/2
(15)
3.2	Statistical Analysis and Justification
The analysis for the stability and robustness of the '4-norm maximization follows similar statistical
analysis techniques in Zhai et al. (2019b) to show that the global maximum of
W? ∈ arg max EkW *K∣∣4 , subject to W ∈ O(n; R)	(16)
W
satisfies W?;Do ∈ SP(n).8 Note here Y, denotes different imperfect measurements: noisy (Yn),
with outliers (YO), and with sparse corruptions (YC). Below we provide the expectation and con-
centration results for kW ; Y, k44 over the data distribution. We show that E kW ; Y, k44 is largely
determined by kWDo k44, a quantity that indicates a “distance” from W ;Do to SP(n). As shown
in Lemma 2.3 and Lemma 2.4 in Zhai et al. (2019b), the only global maximizers of kW ;Dok44 are
signed permutation matrices, and W ;Do converges to a signed permutation matrix as kW ;Dok44
reaches its global maximum.
6in case τp is not an integer, we round τp to the closest integer.
7in our context, σ = 1 is already corruption of large magnitude, since the variance of the sparse signal is 1.
8SP(n) is the signed permutation group, a group of orthogonal matrices that only contain 0, ±1.
5
Published as a conference paper at ICLR 2020
Proposition 3.1 (Expectation of Objective with Small Noise) ∀θ ∈ (0, 1), let Xo ∈ Rn×p,
xi,j 〜nd BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any
orthogonal matrix W ∈ O(n; R) and any random GaUSSian matrix G ∈ Rn×p, gi,j 〜ad N(0, η2)
independent of Xo, let YN = Y + G denote the data with noise. Then the expectation of
n1p ∣∣W*Ynk4 satisfies:	4
-1 Eχo,G ∣W*Ynk4 = 3θ(1 - θ) kW*Dθk4 + Cθ,η,	(17)
np o	4	n
where Cθ,η is a constant which depends on θ and η.
Proof See Appendix A.1.	■
Theorem 3.2 (Concentration of Objective with Small Noise) ∀θ ∈ (0, 1), let Xo ∈ Rn×p,
xi,j 〜iid BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = D0X0. For any
orthogonal matrix W ∈ O(n; R) and any random Gaussian matrix G ∈ Rn×p, gi,j 〜nd N(0, η2)
independent of Xo, let YN = Y + G denote the input with noise, then:
P( SUp 工 IkW *Yn k4 — EkW *Yn k4∣ ≥ δ) < 1,	(18)
∖w∈O(n;R) nP I	P
whenp = Ω ((1 + η2)4n2 lnn∕δ2).
Proof See Appendix A.2.	■
Proposition 3.3 (Expectation of Objective with Outliers) ∀θ ∈ (0,1), let Xo ∈ Rn×p, Xij 〜加
BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any orthog-
onal matrix W ∈ O(n;R) and any random Gaussian matrix G ∈ Rn×τp,g0,j 〜忧壮 N(0,1)
independent of Xo, let YO = [Y, G0] denote the data with outliers G0. Then the expectation of
np ∣∣W*Yok4 satisfies:
-1 Eχo,G0 kW*Yok4 = 3θ(1 — θ)
np o
kW * Do k44
+ Cθ,τ ,
(19)
n
where Cθ,τ is a constant depends on θ, τ.
Proof See Appendix A.3	■
Theorem 3.4 (Concentration of Objective with Outliers) ∀θ ∈ (0,1), let Xo ∈ Rn×p, Xij 〜nd
BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any orthogonal
matrix W ∈ O(n; R) and any random Gaussian matrix G ∈ Rn×τp, gi,j 〜加 N(0,1) indepen-
dent of Xo, let YO = [Y , G0] denote the input with outlier G0, then:
P∣ SUp	-1∣kW * Yo k4 — EkW *Yo k4∣ ≥ δ ) < 1,	(20)
∖w∈O(n;R) nP I	P
whenp = Ω(τ2n2 lnn∕δ2).
Proof See Appendix A.4.	■
In the above results, Proposition 3.1 and 3.3 reveal that both normalized 击E k W*Ynk4,
nipE k W*Yok4 are only determined by k W*D°k4∙ Moreover, as shown in Theorem 3.2 and 3.4,
whenp is large enough (p = Ω((1 + η2)4n2 lnn), Ω(τ2n2 lnn) respectively), both np k W*Yn∣∣4,
nip k W*Yo k4 concentrate around their expectation with high probability. Therefore, the '4-norm
maximization formulation kW*Yk44 for dictionary learning is insensitive to dense Gaussian noise
and robust to Gaussian outliers.9
9In the outlier case, Proposition 3.3 and Theorem 3.4 can be generalized to any rotation invariant distribu-
tions, e.g., uniform distribution on the sphere. More details regarding rotation invariant distributions can be
found in Chapter 4 of Bryc (2012).
6
Published as a conference paper at ICLR 2020
Proposition 3.5 (Expectation of Objective with Sparse Corruptions) ∀θ ∈ (0, 1), let Xo ∈
Rn×p, xi,j 〜iid BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo.
For any orthogonal matrix W ∈ O(n; R) and any random Bernoulli matrix B ∈ Rn×p, bi,j∙〜iid
Ber(β) independent of Xo, let YC = Y + σB ◦ S denote the data with sparse corruptions, and
S ∈ Rn×p is defined in equation 15. Then the expectation of n^ ∣∣ W * YC ∣∣4 satisfies:
kw * Do k4
-1 EXo,B,S kw*Yck4 = 3θ(1- θ)
np
n
+ σ4β(1 - 3β) ' J4 + Cθ,σ,β,	(21)
where Cθ,σ,β is a constant depending on θ, σ and β.
Proof See Appendix A.5.
Theorem 3.6 (Concentration of Objective with Sparse Corruptions) ∀θ ∈ (0, 1), let Xo ∈
Rn×p, xi,j 〜iid BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo.
For any orthogonal matrix W ∈ O(n; R) and any random Bernoulli matrix B ∈ Rn×p, bi,j∙〜iid
Ber(β) independent of Xo, let YC = Y + σB ◦ S denote the input with sparse corruptions, and
S ∈ Rn×p is defined in equation 15, then:
P wXr).kW*Kk4-EkW*Kk4∣≥ δ < p,
(22)
when P = Ω (σ8βn2 ln n∕δ2).
Proof See Appendix A.6.
Unlike the cases with noise and outliers, Proposition 3.5 and
Theorem 3.6 indicate that n1pE ∣∣ W*YC∣∣4 depends on both
k W*Do∣∣4 and ∣∣ W∣∣4; when P = Ω(σ8βn2 ln n), the objective
工 k W*YC ∣4 concentrates around this expectation with high
probability. Nevertheless, when the magnitude of σ4β(1 - 3β) is
significantly smaller than 3θ(1 - θ), the landscape of the objec-
tive ∣W*YC∣44 would largely be determined by ∣W*Do∣44 only.
As shown in Figure 1, this is indeed the case whenever: (a) the
sparsity level θ of ground truth signal Xo , is “reasonably” small
(neither diminishing to 0 nor larger than 0.5);(b) β, the sparsity
level of the corruption, is small (smaller than 0.5); (c) σ, the mag-
nitude of the sparse errors, is not significantly larger than the in-
trinsic variance of the sparse signal (the intrinsic variance of the sparse signal Bernoulli-Gaussian
model is 1).
Therefore, besides the insensitivity to small noise and robustness to outliers, the `4 -maximization
based dictionary learning also shows resilience to sparse corruptions under reasonable conditions.
Figure 1: Comparison between
y = 3x(1 - x) and y =
σ4 |x(1 - 3x)| when x ∈ [0, 1]
with different σ .
4 Simulations and Experiments
4.1	Quantitative Evaluation: Simulations on Synthetic Data
Single Trial of MSP. In this simulation, we run the MSP algorithm from equation 3, using the
imperfect measurements Y of different models (YN , YO , YC). As shown in Figure 2, the normal-
ized value of ∣W*Do ∣44 ∕n reaches global maximum with all types of inputs when varying the
level of noise, outliers, and sparse corruptions. Moreover, as the scale of imperfect measurements
increase, Figure 2 shows that (a) the iterations for convergence increases and (b) the final objective
value ∣W*Do ∣44 decreases almost negligibly. This numerical experiment suggests that the MSP
algorithm is able to identify the ground truth orthogonal transformation Do despite different types
of imperfect measurements.
Phase Transition. Next, we conduct extensive simulations to study the relation between recovery
accuracy and sample size P. We run the experiments by increasing the sample size P w.r.t. the
scale of imperfect measurements η, τ, β, respectively. As shown in Figure 3, the MSP algorithm (3)
7
Published as a conference paper at ICLR 2020
(a) n = 50, p = 20, 000, θ = 0.3,
varying η2 from 0.1 to 0.4
(b) n = 50, p = 20, 000, θ = 0.3,
varying τ from 0.1 to 0.4
(c) n = 50,p = 20, 000, θ = 0.3,
σ = 1, varying β from 0.1 to 0.4
Figure 2:	Normalized ∣∣ W * Dok 4 /n of the MSP algorithm for dictionary learning, using imperfect
measurements YN , YO , YC, respectively.
demonstrates a clear phase transition behavior w.r.t. noise, outliers, and sparse corruptions. Such
phenomena suggest that the algorithm is inherently stable and robust to certain amounts of noise,
outliers, and sparse corruptions. The results corroborate our concentration results in Theorem 3.2,
3.4 and 3.6—a larger sample size p increases the accuracy and robustness of the MSP algorithm (3)
for all types of nuisances.
(a) Noise: n = 50, θ = 0.3
(b) Outliers: n = 50, θ = 0.3
Figure 3:	Average normalized error |1 - kW*Dok44 /n| of 10 random trials for the MSP algorithm:
(a) Varying sample size p and variance of noise η2 ; (b) Varying sample size p and Gaussian Outlier
ratio τ ; (c) Varying sample size p and sparse corruption ratio β, with fixed σ = 1.
Comparison with Prior Arts. We also compare the MSP algorithm (Zhai et al., 2019b) with pre-
vious complete dictionary learning algorithms: Subgradient (SG) (Bai et al., 2018) and Riemannian
Trust Region (RTR) methods (Sun et al., 2015). While SG and RTR demonstrate slightly better
accuracy in some cases (e.g. small Gaussian noise and Gaussian outliers), both algorithms appear
unstable to sparse corruptions. Meanwhile, the MSP algorithm is stable to all imperfect measure-
ment models mentioned in this paper and runs significantly faster than the others.
n	p	Alg.	Clean		Noise				Outlier				Corruption			
			Error	Time	0.2		0.4		0.2		0.4		0.2		0.4	
					Error	Time	Error	Time	Error	Time	Error	Time	Error	Time	Error	Time
25	10k	MSP	0.34%	1.14s	0.45%	1.11s	0.99%	1.12s	1.11%	1.29s	1.82%	1.45s	1.27%	1.05s	2.85%	1.03s
		SG	0.00%	8.00m	5.54%	19.3m	25.7%	26.4m	0.00%	9.62m	0.00%	11.1m	87.0%	16.6m	88.4%	34.2m
		RTR	0.00%	3.28m	0.03%	17.0m	1.34%	20.2m	0.00%	4.63m	0.00%	5.87m	3.20%	18.0m	33.9%	16.3m
50	20k	MSP	0.34%	4.82s	0.47%	4.65s	1.02%	5.44s	1.15%	5.31s	2.01%	6.45s	1.33%	4.80s	3.04%	4.41s
		SG	0.00%	1.34h	N/A	>2h	N/A	>2h	3.42%	1.40h	0.00%	1.81h	N/A	>2h	N/A	>2h
		RTR	0.00%	23.0m	0.04%	1.57h	1.65%	1.38h	0.00%	30.7m	0.00%	41.8m	2.17%	1.25h	82.57%	1.29h
Table 2: Comparison of the MSP algorithm (Zhai et al., 2019b) with prior complete dictionary
learning algorithms: Subgradient method (Bai et al., 2018) and Riemannian Trust Region (Sun
et al., 2015) methods in different models with fixed ground truth sparsity θ = 0.3. Note that SG
only learns a unit vector each time and does not guarantee orthogonality; we therefore project the
dictionary learned to O(n; R) for fair comparison.
8
Published as a conference paper at ICLR 2020
4.2 Qualitative Evaluation: Experiments on Real Images and Patches
Besides simulations, we also conduct extensive experiments to verify the stability and robustness of
the MSP algorithm with real imagery data, at both image level and patch level. Throughout these
experiments, rather than visualize all bases, we routinely show the top bases learned—heuristically,
the top bases are those With the largest coefficients (here, in terms of '1 -norm).
Image Level. At the image level, we first vectorize all 60,000 images in the MNIST dataset (Le-
Cun et al., 1998) into a single matrix Y ∈ R784×60,000, then create imperfect measurements based
on models specified in Section 3: YN (MNIST With noise), YO (MNIST With outliers), YC (MNIST
With sparse corruptions). We run the MSP algorithm 3 With Y , YN, YC, YO and compare the learned
bases. Figure 4(a), (c), (e), and (g) shoW examples of Y , YN , YO , and YC, and Figure 4(b), (d), (f),
and (h) shoW top 10 bases learned from Y , YN , YC , YO, respectively. Despite that We use different
types of imperfect measurements of MNIST, the top bases learned from MSP algorithm 3 are very
much the same.10 This result corroborates our analysis: the '4-maximization and the MSP algorithm
is inherently insensitive to noise, robust to outliers, and resilient to sparse corruptions.
(e) MNIST With 50% outliers
SnasmaHBH
(b) Top bases from MNIST
a囹Z区用区■!但NBl
6 " Q &	8 2 Z
(d) Top bases from MNIST with noise
(f) Top bases from MNIST with outliers
9 0/5
9

7 Ll
(g)	MNIST with 50% sparse corruptions
(h)	Top bases from MNIST with sparse corruptions
Figure 4: Left: Examples of MNIST and its different imperfect measurements. Right: Learned
bases from MNIST and its different imperfect measurements using the MSP algorithm 3.
Patch Level. A classic application of dictionary learning involves learning sparse representations
of image patches (Elad & Aharon, 2006; Mairal et al., 2007). In this section, we extend the experi-
ments of Zhai et al. (2019b) to learn patches from gray scale and color images. First, we construct a
data matrix Y by vectorizing each 8 × 8 patch from the 512 × 512 gray scale image, “Barbara” (see
Figure 5). We then run the MSP algorithm with 100 iterations on both Y and a noisy version YN ,
and the learned top bases are visualized in Figure 5.
Figure 5: The top 12 bases learned from all 16 × 16 patches of Barbara, both with (b) and without
(a) noise. The noisy image is produced by adding Gaussian noise to the clean image, resulting in a
signal-to-noise ratio (SNR) of 5.87. We observed a similar effect when using an 8 × 8 patch size.
Analogously, we apply the same scheme to a 256 × 256 color image, “Duck” (see Figure 6), convert-
ing each 8 × 8 × 3 patch into a column vector (in R192) ofY . Notice this forces the algorithm to learn
bases involving all three channels simultaneously, rather than one at a time. After running the MSP
algorithm for 100 iterations, we visualize the top bases learned from both Y and corresponding YN
in Figure 6.
We next consider the problem of learning a “global dictionary” (Mairal et al., 2007) for patches from
many different images. To construct our data matrix Y , we randomly sample 100,000 8 × 8 × 3
10Bases with opposite intensity are considered as the same base, since they only differ by a sign.
9
Published as a conference paper at ICLR 2020
(a) Clean Image and Bases
(b) Noisy Image and Bases (SNR = 6.56)
Figure 6: The top 12 bases learned from all 8 × 8 × 3 color patches of the clean and noisy image,
respectively. Here, the SNR of the noisy image is 6.56.
(a) Bases from Clean Patches
U"l
3Γ[≥D⅞SK=π∣hU⅝f^≡NH
=apfian@H≡M7^rss⅝≡

(b) Bases from Noisy Patches (SNR = 6.23)
Figure 7: Top half (96) bases learned from 100, 000 random 8 × 8 × 3 patches sampled from
CIFAR-10, before and after adding Gaussian noise, with SNR 6.23.
patches from the CIFAR-10 data-set (Krizhevsky et al., 2009). A noisy data matrix YN , is then
generated by adding Gaussian noise. Again, we apply the MSP algorithm with 200 iterations to
learn 192 bases and visualize the results in Figure 7. We leave the experiments of CIFAR-10 with
outliers and sparse corruptions in the Appendix due to limited space.
In each of these experiments, the top bases in the learned dictionary remain relatively unchanged
with the addition of noise. To quantify this similarity, we take the top bases from the noisy dictionary
and find the closest top clean base for each. If the bases are nearly identical, then the inner product
of each of these pairs should be close to 1. Table 3 reports the statistics.
Minimum Lower Quartile Median Upper Quartile Maximum
Barbara	0.3048	0.8471	0.9941	0.9993	1.0000
Duck	0.2510	0.9782	0.9891	0.9971	1.0000
CIFAR-10	0.5147	0.7203	0.9892	0.9998	1.0000
Table 3: Statistics about the inner products between the top 20 noisy bases and their corresponding
closest top-20 clean bases.
5 Conclusion and Discussions
In this paper, we find the `4 -norm maximization based dictionary learning and corresponding MSP
algorithm introduced by Zhai et al. (2019b) have strong geometric and statistical connections to
classic data analysis methods PCA and ICA. These connections seem to be the reason why they all
admit similar simple and efficient algorithms. Empirically, we have observed that `4 -norm maxi-
mization is surprisingly insensitive to noise, robust to outliers, and resilient to sparse corruptions.
Moreover, such empirical observations corroborate our concentration analysis—larger data samples
(P) improve the stability and robustness of the '4-maximization based dictionary learning.
From experiments on real images, we observed that top bases learned are rather stable but tail bases
can be less stable (see Figure 10 in Appendix C). We conjecture this phenomenon happens due to
the fact that real images generally do not follow the uniformly sparse Bernoulli Gaussian model
(of equation 14). Generalizing dictionary learning to non-uniformly sparsely generated data would
be a good topic for future study. Finally, note that Bai et al. (2018) has established subgradient
descent can find global optimal solutions for dictionary learning, an interesting theoretical guarantee
since dictionary learning problems do not satisfy the standard structural (i.e. generalized convexity)
assumptions that guarantee global convergence (Ben-Tal & Nemirovski, 2001; Zhou et al., 2017a;
Nesterov, 2013; Zhou et al., 2017b; Bubeck et al., 2015; Zhou et al., 2017c; 2018; Ma et al., 2018;
Chi et al., 2019). It would be desirable, although highly challenging, to establish similar global
convergence guarantees for our MSP algorithm. We leave that for future work.
10
Published as a conference paper at ICLR 2020
References
P-A Absil and Jerome Malick. Projection-like retractions on matrix manifolds. SIAM Journal on
Optimization, 22(1):135-158, 2012.
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learn-
ing. Machine Learning, 73(3):243-272, 2008.
Yu Bai, Qijia Jiang, and Ju Sun. Subgradient descent learns orthogonal dictionaries. arXiv preprint
arXiv:1810.10702, 2018.
Boaz Barak, Jonathan Kelner, and David Steurer. Dictionary learning and tensor decomposition via
the sum-of-squares method. In STOC, 2015.
Aharon Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization: analysis, algo-
rithms, and engineering applications, volume 2. Siam, 2001.
Wlodzimierz Bryc. The normal distribution: characterizations with applications, volume 100.
Springer Science & Business Media, 2012.
Sebastien BUbeck et al. Convex optimization: Algorithms and complexity. Foundations and
TrendsR in Machine Learning, 8(3-4):231-357, 2015.
Meghan K Cain, Zhiyong Zhang, and Ke-Hai YUan. Univariate and mUltivariate skewness and
kUrtosis for measUring nonnormality: Prevalence, inflUence and estimation. Behavior Research
Methods, 49(5):1716-1735, 2017.
Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?
Journal of the ACM (JACM), 58(3):11, 2011.
Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46(2):255-
308, 2009.
Niladri Chatterji and Peter L Bartlett. Alternating minimization for dictionary learning with random
initialization. In Advances in Neural Information Processing Systems, pp. 1997-2006, 2017.
Yuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix factoriza-
tion: An overview. IEEE Transactions on Signal Processing, 67(20):5239-5269, 2019.
Lawrence T DeCarlo. On the meaning and use of kurtosis. Psychological methods, 2(3):292, 1997.
Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over
learned dictionaries. IEEE Transactions on Image processing, 15(12):3736-3745, 2006.
Dar Gilboa, Sam Buchanan, and John Wright. Efficient dictionary learning with gradient descent.
arXiv preprint arXiv:1809.10313, 2018.
Nathaniel E Helwig. Principal components analysis. 2017.
Peter J Huber. Projection pursuit. The annals of Statistics, pp. 435-475, 1985.
Aapo Hyvarinen and Erkki Oja. A fast fixed-point algorithm for independent component analysis.
Neural Computation, 9:1483-1492, 1997.
Aapo Hyvarinen and Erkki Oja. Independent component analysis: algorithms and applications.
Neural networks, 13(4-5):411-430, 2000.
Aapo Hyvarinen, Jarmo Hurri, and Patrick O Hoyer. Natural image statistics: A probabilistic ap-
proach to early computational vision., volume 39. Springer Science & Business Media, 2009.
Ian Jolliffe. Principal component analysis. Springer, 2011.
Michel Journee, Yurii Nesterov, Peter Richtarik, and Rodolphe Sepulchre. Generalized power
method for sparse principal component analysis. Journal of Machine Learning Research, 11
(Feb):517-553, 2010.
11
Published as a conference paper at ICLR 2020
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. Technical report,
Citeseer, 2009.
Han-Wen Kuo, Yenson Lau, Yuqian Zhang, and John Wright. Geometry and symmetry in short-
and-sparse deconvolution. arXiv preprint arXiv:1901.00256, 2019.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings ofthe IEEE, 86(11):2278-2324,1998.
Ann B Lee, Kim S Pedersen, and David Mumford. The nonlinear statistics of high-contrast patches
in natural images. International Journal of Computer Vision, 54(1-3):83-103, 2003.
Yanjun Li and Yoram Bresler. Global geometry of multichannel sparse blind deconvolution on the
sphere. In Advances in Neural Information Processing Systems, pp. 1132-1143, 2018.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statis-
tical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and
blind deconvolution. Foundations of Computational Mathematics, pp. 1-182, 2018.
Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-of-
squares. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS),
pp. 438-446. IEEE, 2016.
Julien Mairal, Michael Elad, and Guillermo Sapiro. Sparse representation for color image restora-
tion. IEEE Transactions on image processing, 17(1):53-69, 2007.
Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro, and Andrew Zisserman. Discrimina-
tive learned dictionaries for local image analysis. Technical report, MINNESOTA UNIV MIN-
NEAPOLIS INST FOR MATHEMATICS AND ITS APPLICATIONS, 2008.
Julien Mairal, Francis Bach, and Jean Ponce. Task-driven dictionary learning. IEEE transactions on
pattern analysis and machine intelligence, 34(4):791-804, 2012.
Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing.
Foundations and Trends in Computer Graphics and Vision, 8(2-3):85-283, 2014. ISSN 1572-
2740. doi: 10.1561/0600000058.
Arthur Mensch, Julien Mairal, Bertrand Thirion, and Gael Varoquaux. Dictionary learning for mas-
sive matrix factorization. In International Conference on Machine Learning, pp. 1737-1746,
2016.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Thanh Nguyen, Akshay Soni, and Chinmay Hegde. On learning sparsely used dictionaries from
incomplete samples. In Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 3769-3778, Stockholmsmssan,
Stockholm Sweden, 10-15 Jul 2018. PMLR.
Bruno A. Olshausen and David J. Field. Emergence of simple-cell receptive field properties by
learning a sparse code for natural images. Nature, 381(6583):607-609, 1996.
Bruno A. Olshausen and David J. Field. Sparse coding with an overcomplete basis set: A strategy
employed by V1? Vision research, 37(23):3311-3325, 1997.
Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu. Analysis of the optimization
landscapes for overcomplete representation learning. arXiv preprint arXiv:1912.02427, 2019.
Sirisha Rambhatla, Xingguo Li, and Jarvis Haupt. Noodl: Provable online dictionary learning and
sparse coding. In International Conference on Learning Representations, 2019.
Marc’Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun. Sparse feature learning for deep belief
networks. In Advances in Neural Information Processing Systems (NIPS 2007), volume 20, 2007.
12
Published as a conference paper at ICLR 2020
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.
Tselil Schramm and David Steurer. Fast and robust tensor decomposition with applications to dic-
tionary learning. arXiv preprint arXiv:1706.08672, 2017.
Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In
Conference on Learning Theory, pp. 37-1, 2012.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere. arXiv preprint
arXiv:1504.06785, 2015.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
John Wright, Allen Y Yang, Arvind Ganesh, S Shankar Sastry, and Yi Ma. Robust face recognition
via sparse representation. IEEE transactions on pattern analysis and machine intelligence, 31(2):
210-227, 2008.
Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust PCA via outlier pursuit. In Advances
in Neural Information Processing Systems, pp. 2496-2504, 2010.
Huan Xu, Constantine Caramanis, and Shie Mannor. Outlier-robust PCA: The high-dimensional
case. IEEE transactions on information theory, 59(1):546-572, 2012.
Jianchao Yang, John Wright, Thomas S Huang, and Yi Ma. Image super-resolution via sparse
representation. IEEE transactions on image processing, 19(11):2861-2873, 2010.
Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. A fast holistic algorithm for
complete dictionary learning via `4 norm maximization. Signal Processing with Adaptive Sparse
Structured Representations (SPARS), 2019a.
Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. Complete dictionary learning
via '4-norm maximization over the orthogonal group. arXiv preprint arXiv:1906.02435, 2019b.
Xiaoqin Zhang, Di Wang, Zhengyuan Zhou, and Yi Ma. Simultaneous rectification and alignment
via robust recovery of low-rank tensors. In Advances in Neural Information Processing Systems,
pp. 1637-1645, 2013.
Xiaoqin Zhang, Zhengyuan Zhou, Di Wang, and Yi Ma. Hybrid singular value thresholding for
tensor completion. In Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
Xiaoqin Zhang, Di Wang, Zhengyuan Zhou, and Yi Ma. Robust low-rank tensor recovery with
rectification and alignment. IEEE transactions on pattern analysis and machine intelligence,
2019.
Yuqian Zhang, Han-Wen Kuo, and John Wright. Structured local optima in sparse blind deconvolu-
tion. arXiv preprint arXiv:1806.00338, 2018.
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter Glynn.
On the convergence of mirror descent beyond stochastic convex programming. arXiv preprint
arXiv:1706.05681, 2017a.
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W Glynn.
Stochastic mirror descent in variationally coherent optimization problems. In Advances in Neural
Information Processing Systems, pp. 7040-7049, 2017b.
Zhengyuan Zhou, Panayotis Mertikopoulos, Aris L Moustakas, Nicholas Bambos, and Peter Glynn.
Mirror descent learning in continuous games. In 2017 IEEE 56th Annual Conference on Decision
and Control (CDC), pp. 5776-5783. IEEE, 2017c.
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter W Glynn, Yinyu Ye, Li-Jia Li,
and Fei-Fei Li. Distributed asynchronous optimization with unbounded delays: How slow can
you go? 2018.
13
Published as a conference paper at ICLR 2020
A Proof of Section 3
A.1 Proof of Proposition 3.1
Claim A.1 (Expectation of Objective with Small Noise) ∀θ ∈ (0,1), let Xo ∈ Rn×p, χi,j 〜nd
BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any orthogonal
matrix W ∈ O(n; R) and any random GaUSSian matrix G ∈ Rn×p, gi,j 〜nd N(0, η2) independent
of Xo, let YN = Y + G denote the data with noise. Then the expectation of np ∣∣ W*Yn ∣∣4 satisfies:
ɪEχo,G kW*Ynk4 = 3θ(1 — θ)
np o
kW*Dok4 +3θ2 + 6θη2 + 3η4.
(23)
n
Proof Let W*D0 = M ∈ O(n; R), notice that the orthogonal transformation (W*G) of a Gaus-
sian matrix (G) is still a Gaussian matrix and satisfies {W*G}i,j 〜 N(0,1), and it is independent
of Y (Xo). We abuse the notation a bit let G = W *G in the following calculation, since W * G is
also a Gaussian matrix independent of Xo and it will not affect the final result.
EXo,G∣W*YN∣44=EXo,G∣MXo+G∣44
4
pn	n	4
EXo,G	mi,kxk,j + gi,j
pn	n	4
XXjEXo,G X mi,kxk,j + 6EXo,G
pn	n	4	n	2
=	EXo ,G	mi,kxk,j	+ 6η2EXo	mi,kxk,j	+ 3npη4
Pn (	n	∖ 4 ]
=jEXo,G	mi,kxk,j	+ 6npθη2 +3npη4
=EXo ∣MXo ∣44 + 6npθη2 + 3npη4
=3pθ(1 - θ) ∣M ∣44 + 3npθ2 + 6npθη2 + 3npη4,
therefore,
ɪ Eχo,G k W *Yn k4 = 3θ(1 - Θ) kW *Dok4 + 3θ2 + 6θη2 + 3η4,
np o	n
which completes the proof.
(24)
(25)
A.2 Proof of Theorem 3.2
Claim A.2 (Concentration of Objective with Small Noise) ∀θ ∈ (0, 1), let Xo ∈ Rn×p,
xi,j 〜nd BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any
orthogonal matrix W ∈ O(n; R) and any random Gaussian matrix G ∈ Rn×p, gi,j 〜nd N(0, η2)
independent ofXo, let YN = Y + G denote the input with noise, then:
P (W ∈upn;R) >kW*YN k4- EkW*YN k4l≥ δ)<p,	(26)
whenP = Ω ((1 + η2)4n2 lnn∕δ2).
Proof According to Proposition 3.1, we know that
np e"g kW* ∣4 = 3θ(I-θ)
kW*Dok4 +3θ2 +6θη2 +3η4.
(27)
n
14
Published as a conference paper at ICLR 2020
Moreover, since YN = [yι + gι, y2 + g2,∙∙∙, yp + gp], whose columns are independent. Let
Z = [z1, z2,..., zp] = Do YN = [x1 + g1, x2 + g2,∙∙∙, XP + 9p] = Xo + G,	(28)
note that We abuse the notation a bit by assuming DO G = G, since DO G is also a Gaussian matrix
independent of Xo and it will not affect the final result. Define function fz (W) : O(n; R) → R as
fz(W) = kW*z∣∣4 .	(29)
Next, we will check assumption 1, 2, and 3 then apply Lemma B.4.
Assumption 1: μ(n,p). Since Zij = Xij +gi,j, we know that with probability θ, Z 〜N (0,1+η2)
and with probability 1 - θ, Z 〜N(0,1). Hence,
(-$)
P ( max ∖zij | > B
ij i,j 'J
≤ 2npθexp
+ 2np(1 — θ)exp
μ(n, p).
(30)
Specifically, we set B = ln p, which yields
μ(n,p) = 2npθ exp (-；；2)+ 2np(1 - θ) exp (- (In；)
(31)
Assumption 2: Lipschitz Constant Lf. By Proposition 3.1, ∀W ∈ O(n; R), we know that
EfZ(W) = 3θ(1 - θ) ∣∣W*Do∣∣4 + 3nθ2 + 6nθη2 + 3nη4.	(32)
Hence, ∀Wι, W2 ∈ O(n; R), we have
I E ∣W^4 - E k W1*z∣∣4∣ = 3θ(1 - θ) I ∣W*Do∣4 - k W2*Do∣∣4∣,	(33)
moreover,
X [(W[Do)4,j - (W2*Do)4,j]
i,j
=X{[(w; Do)2,j - (W2 Do)2,j][(W] Do)2,j + (W2 Do)2,j]}
i,j
≤ {χ [(W1*Do)2,j - (W2Do)2,j]2}	{χ [(W1*Do)2,j + (W2D。)%]2}	('
=∣∣(wr Do)。2 - (w： Do)o2∣∣F∣∣(wr Dofi + (w： DO)TIF
≤ kW：Do - W2Do∣F ∣W：Do + W2Do∣F (\\w：DokF + ∣W2D.∣∣F)
≤4n2 ||W：Do - W2Dok2 = 4n2 ∣W1 - W2∣∣2 .
Hence, we know that
∣E \W：z∣4 - E ∣ W：z∣4 ∣ ≤ 12n2θ(1 - θ),	(35)
which yields
Lf = 12n2θ(1 - θ).	(36)
Assumption 2: Lipschitz Constant Lf. Since fz(W) = ∣∣W *z∣∣4,wehave
∣∣∣W1* z∣4 -∣∣W2* z∣4 I= X{[(w; z)2 -(W2 z)2][(w] z)2 + (w; Z)2]}
i
21 1/2
(37)
∣∣(w;z)。2 - (w;Z)T∣2 ∣∣(w;z)。2 + (w;N)-21.
s-----------V----------------------V-----------'
Γ1	Γ2
15
Published as a conference paper at ICLR 2020
For Γ1 , we have
Γι = ∣∣(Wι*z)°2 - (Wf Z)TI2 = k(Wι*z) - (W2*z) ◦ (W：Z) + (Wfz)∣∣2
≤ kW1 — W2k2 kW1 + W2k2 kzk2 ≤ 2nB2 kW1 - W2k2 ∙	(38)
x-----{-----}l{z}
≤2	≤nB2
For Γ2, we have
γ2 = I∣(W1*Z)°2 + (WRT∣2 ≤ II(W：Z)T∣2 +1∣(* Z)T∣2
≤kW;zk2 + kW"k2 ≤ 2nB2.
Hence, we know that
∣kWι*Zk4 -kW2*Zk[ ≤ 4n2B4 kW1- W2k2 ,	(40)
which yields
L f = 4n2 B4.	(41)
Assumption 3: Upper Bound R1.	Notice that ∀W ∈ O(n; R),
∣ W：Z ∣4
fz(W) = kW*Zk4 = kW*Zk2 kW*zk2 4 ≤ kW*Zk2 = kzk4 ≤ n2B4 = Ri.	(42)
Assumption 3: Upper Bound R2.	Assume the support for x is S, so we know that know that
∀i ∈ [n],
Zi = (p1+η2vi,	ifi ∈S'	(43)
vi ,	otherwise,
where V 〜N(0, I) is a Gaussian vector. Let PS : Rn → Rn bea projection that satisfies ∀q ∈ Rn
(PS q) = ∕p1 + η2 qi,	ifi ∈S,
i	qi ,	otherwise.
Moreover, ∀W ∈ O(n; R), let wi denote the ith column vector of W, hence
2	nn
(kW*zk4)	= E XX hwi, Zi4 hwj,Zi4
i=1 j=1
n n	n n	1
≤ES XXE
[hPs Wi, Vi4 hPs Wj, v)4] ≤ ES XX 忸 hPs Wi, vi8 E hPs Wj, V〉8]2
i=1 j=1	i=1 j=1
Notice that V 〜N(0,1), therefore ∀i ∈ [n], we have
E hPS Wi, Vi8 = 105 kPSWik28 ,
hence
n n	ι	n n
Es XX [E hPsWi, vi8 E hPsWj, vi8] 2 = 105ES XX kPsWik4 kPsWik4
nn	4
=105 XX X
Es Y(I + η2 k~∈SS)w2,kι w2,k2 w2,k3 w2,k4
i=1 j=1 k1,k2.k3 ,k4	l=1
≤105(1 + η2)4 Xn Xn	X	[wi2,k1 wi2,k2 wj2,k3 wj2,k4] = 105(1 + η2)4n2.
i=1 j=1 k1,k2.k3,k4
Therefore, we can conclude that
E[fz2(W)] ≤ 105(1+η2)4=R2.
(44)
(45)
(46)
(47)
(48)
16
Published as a conference paper at ICLR 2020
Applying Lemma B.4 for Concentration. Now we apply Lemma B.4 with
1.
B = lnp,	μ(n,p) = 2npθ exp (------------J + 2np(1 — θ) exp (一
2.
Lf = 12n2θ(1 - θ), Lf = 4n2(lnp)4,
3.
R1 = n2 (lnp)4,	R2 = 105(1 +	η2)4n2,
we have
1p
SUP - Σ [fzj (W) - Efzj (W)] ≥ δ
W∈θ(n;R) np j=ι
=P I	SUP	-1IkW*Ynk4 - EkW*Ynk4∣ ≥ δ)
W∈∈O(n;R) nP 1
< exP [- R2R pn8δ>、/3 + n2 ln (12(Lf + Lf) ) + ln2∣ + μ(n,p)
32R? + 8Rιnδ∕3	n no J
(49)
(50)
(51)
(52)
< exp
C(1 + η2)3p+28n(ln "δ + ^	( 一 )十.
+ 2npθ exp (-Tɪ) + 2np(1 - θ) exp (-竽)< 1,
for a constant C > 104, whenP = Ω ((1+ η2)4n2 lnn∕δ2).
A.3 Proof of Proposition 3.3
Claim A.3 (Expectation of Objective with Outliers) ∀θ ∈ (0,1), let Xo ∈ Rn×p, Xij 〜加
BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any orthog-
OnaI matrix W ∈ O(n;R) and any random GaUSSian matrix G ∈ Rn×τp, gi0,j 〜忧壮 N(0,1)
independent of Xo, let YO = [Y , G0] denote the data with outliers G0. Then the expectation of
np ∣∣W*Yok4 satisfies:
ɪEχo,G0 kW*Yok4 = 3θ(1 - θ) kW*Dok4 + 3θ2 + 3τ.	(53)
nP o	n
Proof Notice that
Eχo,G0 kW*Yok4 = EXokW*Yk4 + Ego kW*G0k4 ,	(54)
and
EXokW*Yk4 = EXokW*DοXοk4 = 3pθ(1 - θ) k W*D.k4 + 3npθ2.	(55)
Moreover, the orthogonal rotation (W*G0) of a standard Gaussian matrix G is also a standard
Gaussian matrix, therefore,
EG0kW*G0k44 =3τnP.	(56)
Hence,
ɪEχo,G0 k W*Yok4 = 3θ(1 - θ) kW*Dθk4 + 3θ2 + 3τ,	(57)
nP o	n
which completes the proof.	■
17
Published as a conference paper at ICLR 2020
A.4 Proof of Theorem 3.4
Claim A.4 (Concentration of Objective with Outliers) ∀θ ∈ (0,1), let Xo ∈ Rn×p, χi,j 〜nd
BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any orthogonal
matrix W ∈ O(n; R) and any random GaUSSian matrix G ∈ Rn×τp, gi,j 〜加 N(0,1) indepen-
dent of Xo, let YO = [Y , G0] denote the input with outlier G0, then:
P (W XI) n1>* YOk4 - EwYO k4∣≥ δ)< p,
when P = Ω(τ2n2 ln n∕δ2).
(58)
Proof Notice that
P	W∈upn;R):kw*Yok4-EkW*Yok4∣≥δ
≤P『usR) np∣kw*Y k4 - EkW*Y k4∣≥ 2)
'-----------------------------------}
'∙^^^^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Γ1
+ P( sup	-1∣kW *G0k4 - EkW *G0k4∣≥ δ )
WV∈O(n;R) nP	2 J
X-----------------------------------------------------/
(59)
{Z
Γ2
Apply Lemma B.5 (substitute δ with ∣) to Γι, we know that
Γι =Pl	sup -1∣kW *Y k4
W∈θ(n;R) np 1
< exp -
3pδ2
cιθ + 16n(ln p)4δ
(60)
+ exp ( - g + n ln (120npδlnP)4)) + 2npθexp :
(ln p)2
^^2
1
< 2p,
for two constants c1 ≥ 4 × 104, c2 > 13, 440, and the last inequality holds when
p
Ω(θn2 lnn∕δ2). Moreover, apply Lemma B.5 again (let θ = 1, Y = G0 ∈ Rn×τp and substi-
tute δ with 2δ-), We have
γ2 =P W∈O‰)>"的4
-EkW*G0
=P W∈O‰)τ⅛ ∣kW*G0k4
-EkW*G0∣
< exp
3p(δ∕τ)2	+ 2 l (120np(lnp)4)、
c1 + 16n(lnp)4δ∕τ	" ɪɪ ∖ δ∕τ
(61)
+ exp ( - P≡^ + n2 ln (空0"T))+ 2npexp (-出胃) < 1,
for two constants c1 ≥ 4 × 104, c2 > 13, 440, and the last inequality holds when P =
Ω(τ2n2 lnn∕δ2). Combine θ ≤ 1 and T can be larger than 1, we conclude that
P ( sup	-1∣kW*Υok4 - EkW*Υok4∣≥ δ) < Γι +Γ2 < 1	(62)
∖ W∈O(n;R) nP 1	P
when p = Ω (τ2n2 ln n∕δ2).
18
Published as a conference paper at ICLR 2020
A.5 Proof of Proposition 3.5
Claim A.5 (Expectation of Objective with Sparse Corruptions) ∀θ ∈ (0, 1), let Xo ∈ Rn×p,
xi,j 〜nd BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any
orthogonal matrix W ∈ O(n; R) and any random Bernoulli matrix B ∈ Rn×p, bi,j 〜泅 Ber(β)
independent of Xo, let YC = Y + σB ◦ S denote the data with sparse corruptions, and S ∈ Rn×p
is defined in equation 15. Then the expectation of，p ∣∣ W*Yc ∣∣4 satisfies:
ɪExo,b,s k W*YCk4 = 3θ(1 - θ) kW*Dok4 + σ4β(1 - 3β)kW∣4 + 3θ2 + 6σ2θβ + 3σ4β2.
np o	n	n
(63)
Proof Let W*Do = M ∈ O(n; R), notice that
IlW*YCk4 = IIMXo + σW*(B ◦ S)k4 ,
(64)
3
+ 4	mi,kxk,j	σ	wk,ibk,j sk,j
k=1	k=1
'--------------------{z----------------
Γ5
(65)
}
Moreover,
•	Γ1 :
n
Xo,B,SΓ1 =EXoΓ1 = 3θ X mi4,k +6θ2 X	mi2,k1mi2,k2
k=1
n
3θ(1 -θ)Xmi4,k+3θ2,
k=1
1≤k1 <k2 ≤n
(66)
•	Γ2 :
2
Xo,B,SΓ2
Xo	mi,kxk,j
k=1
B,S σ X wk,ibk,jsk,j
(67)
θ X mi2,k σ2β X wk2,i = σ2θβ,
•	Γ3 :
n
EXo,B,SΓ3	=EB,SΓ3 =	σ4β X	wk4,i	+	6σ4β2	X	wk21,iwk22,i
k=1	1≤k1 <k2 ≤n
n
=σ4β(1 - 3β) X wk4,i + 3σ4β2,
k=1
(68)
19
Published as a conference paper at ICLR 2020
• Γ4, Γ5 :
EXo,B,SΓ4 = 0,	EXo,B,SΓ5 = 0.
(69)
Substitute EΓ1, EΓ2, EΓ3, EΓ4, EΓ5 back to equation 65, yields
——EXo,B,S
np o
kWYCk44 = 3θ(1-θ)
kW Dok4 +σ4β(1-3β) kWk4 +3θ2 +6σ2θβ+3σ4β2. (70)
nn
A.6 Proof of Theorem 3.6
Claim A.6 (Concentration of Objective with Sparse Corruptions) ∀θ ∈ (0, 1), let Xo ∈ Rn×p,
xi,j 〜nd BG(θ). Let Do ∈ O(n; R) be an orthogonal matrix and assume Y = DoXo. For any
orthogonal matrix W ∈ O(n; R) and any random Bernoulli matrix B ∈ Rn×p, bi,j 〜泅 Ber(β)
independent of Xo, let YC = Y +σB ◦ S denote the input with sparse corruptions, and S ∈ Rn×p
is defined in equation 15, then:
P( SUp	-1IkW *YC k4 - EkW *YC k4∣ ≥ δ ) < 1,	(71)
∖w∈O(n;R) nP 1	P
whenp = Ω (σ8βn2 lnn∕δ2).
Proof According to Proposition 3.5, we know that
ɪEχo,B,s kW*Yck4 = 3θ(1 - θ) kW*Dok4 + σ4β(1 - 3β)kWk4 + 3θ2 + 6σ2θβ + 3σ4β2.
np o	n	n
(72)
Moreover, since YC = [y1 + σb1 ◦ s1, y2 + σb2 ◦ s2 , . . . , yp + σbp ◦ sp], whose columns are
independent. Let
Z = [Z1, Z2,..., Zp]= DoYC = Xo + σC,	(73)
where C = Do(B ◦ S). Note that the columns of Z are independent, We can define function
fz(W) : O(n; R) 7→ R as
fz(W) = kW*zk4 .	(74)
Next, we will check assumption 1, 2, and 3 then apply Lemma B.4.
Assumption 1: μ(n,p).	Since Z = Xo + σC, we have
{max lzi,j| > B} = {max lxi,j+ σci,j| > B}⊆ {max |xi,j| > B - σmax lci,j|}.
i,j	i,j	i,j	i,j
Moreover, since
max ∣Cij| ≤ max |心％ ≤ √n,
i,j ,	i	1
we have
{max |Zij| > B} ⊆ {max ∣Xij∣ > B — σ√n}.
i,j ,	i,j ,
Thus we know that
P max |zij | > B ≤ P max |xij| > B -
i,j ,	i,j ,
≤ 2npθ exp -
(B - σ√n)2
2
(75)
(76)
(77)
(78)
(79)
Specifically, we set B = P 1, which yields
μ(n,p) = 2npθexp
—
∖
2
20
Published as a conference paper at ICLR 2020
Assumption 2: Lipschitz Constant Lf. By Proposition 3.5, ∀W ∈ O(n; R), we know that
Efz(W) = 3θ(1 - θ) kW*Dok4 + σ4β(1 - 3β) ∣∣W∣∣4 + 3nθ2 + 6nσ2θβ + 3nσ4β2.
Hence ∀W1 , W2 ∈ O(n; R), we have
IEkW1*zk4 - EkWwz∣4∣
≤3θ(1 — θ) ∣∣Wι*Dok4 - kW[Dok4∣ + σ4β(1 - 3β) ∣∣Wι∣4 -∣W2∣4∣,
from the derivation of equation 34, we have
∣kW[Dok4 -kW2*Dok4∣ ≤ 4n2 ∣Wι - W2k2，
and therefore, we have
∣E∣∣W[zk4 - EkWz∣4∣ ≤ [I2n2θ(1-θ) + 4n2σ4β(1 - 3β)] ∣Wι - W2k2，
which yields
Lf = 12n2θ(1 -θ) +4n2σ4β(1 -3β).
(80)
(81)
(82)
(83)
(84)
Assumption 2: LiPSchitZ Constant Lf. Since fz(W) = ∣∣ W*z∣4, apply the same derivation as
equation 37, we have
∣kW1*Zk4 -kW2*Zk4∣ ≤ 4n2B4 ∣Wι- W2k2 ,	(85)
which yields
L f =4n2B4.	(86)
Assumption 3: Upper Bound R1. Notice that ∀W ∈ O(n; R),
“	"	"	W *Z	4	„	“ C ,
fz(W) = kW*zk4 = kW*zk4 = kW*zk2 TW^	≤ kW*zk4 = kzk2 ≤ n2B4 = R1.
kW zk2 4
(87)
Assumption 3: Upper Bound R2. Assume the support for x is S, so we know that ∀i ∈ [n],
vi + ci ,	if i ∈ S,
zi =
ci ,	otherwise,
(88)
where V ~ N(0, I) is a Gaussian vector and c = D0(b ◦ S) is the sparse corruption vector after
orthogonal rotation Do* . Let PS : Rn 7→ Rn be a projection onto the support S, that is, ∀q ∈ Rn :
(PS q)i = {0i,	oth e∈WSs e.	(89)
Let M = Do W, and h = b ◦ s, we have
Efz2(W) = E kW *zk48
nn
=E XX
(hwi,xi +σ hmi,hi)4 (hwj, xi +σhmj,hi)4
8	81 1	(90)
≤	E(hwi,xi +σhmi,hi) E(hwj,xi +σ hmj, hi)
i=1 j=1
n n	ι
=XX IE (hPsWi, Vi + σ gi, hi)8 E (hPSWj, Vi + σ〈mj, hi)[ 2 .
i=1 j=1
21
Published as a conference paper at ICLR 2020
Moreover, we have:
E(hPSwi,vi + σ hmi,hi)8
=E	hPS wi, vi8	+	28σ2E	hPS wi, vi6	hmi, hi2	+	70σ4E	hPS wi,	vi4	hmi, hi4	(91)
+ 28σ6E hPSwi,vi6 hmi, hi2 + σ8E hmi, hi8 .
We now provide upper bound for E (hPSwi, vir) and E (hmi, hir) for r = 2, 4, 6, 8 respectively.
Since Eh = 0 and h = b ◦ s. Let S0 denote the support of h, we have
E hmi, hi8 = EY X mkl,ihi
l=1 k1 =1	(92)
= E	ES0 (mkι,i1kι∈S0 mk2,ilk2∈S0 mk3,ilk3∈S0 mk4,ilk4∈S0).
k1,k2,k3,k4
Now we discuss these cases separately:
•	With probability c1β4(c1 ≤ 1), all k1, k2, k3, k4 are in S, in this case, we have
E	ES0 (mk1,ilk1∈S0mk2,ilk2∈S0mk3,ilk3∈S0mk4,ilk4∈s0)
k1,k2,k3,k4
= E	(mk1,imk2,imk3,imk4,i) = L
k1,k2,k3,k4
(93)
•	With probability c2 β3(c2 ≤ 1), only three among k1 , k2 , k3, k4 are in S0, in this case, we
have
E	ES0 (^mk.ι,llk1∈s^mk2,ilk2∈s0mk3,ilk3∈s0mk4,ilk4∈s0)
k1,k2,k3,k4
E	(mk1,iml,imk3,i) = kmik4 ≤ L
k1,k2,k3
(94)
•	With probability c3β2 (c3 ≤ 1), only two among k1 , k2 , k3, k4 are in S0, in this case, we
have
E	ES0 (mk1,ilk1∈s0mk2,ilk2∈s0mk3,ilk3∈s0mk4,ilk4∈s0)
k1,k2,k3,k4
X (mk1,imk2,i) + X (mk1,imk2,i) ≤ kmik4 + kmik6 ≤ 2.
k1,k2	k1,k2
(95)
• With probability c4 β(c4 ≤ 1), only one among k1, k2 , k3, k4 is in S0, in this case, we have
E	ES0 (mk1,iik1∈S0mk2,iik2∈S0mk3,iik3∈S0mk4,iik4∈S0)
k1,k2,k3,k4
Xm8k1,i = kmik88 ≤ 1.
(96)
Hence, combine equation 93, equation 94, equation 95, and equation 96, we know that
E hmi,hi8 ≤ cβ,	(97)
for a constant c > 1. Similarly, one can conclude that
E (hmi, hir) ≤ cβ, ∀r = 2, 4, 6.	(98)
Moreover, since V 〜N(0, I), we know that
E (hPSwi, vir) = kPSwikr2 , ∀r = 2,4, 6, 8.	(99)
22
Published as a conference paper at ICLR 2020
Hence, with the similar technique applied above, we have
E hPSwi,vi8 = 105E kPSwik82
= 105 X	ES0 (w2ι,i1kι∈S0w22,i1k2∈S0w23,i1k3∈S0w24,i1k4∈S0) ≤ 105cθ,
k1,k2,k3,k4
for a constant c > 1. Similarly, we have
E (hPSwi, vir) = kPSwikr2 ≤ c(r - 1)!!θ, ∀r = 2, 4, 6,
for a constant c > 1. Therefore, combine equation 91, we have
E (hPS wi, vi + σ hmi, hi)8
=E	hPS wi, vi8	+	28σ2E	hPSwi,	vi6 hmi,	hi2	+ 70σ4E	hPSwi, vi4	hmi, hi4
+ 28σ6E hPSwi, vi6 hmi, hi2 + σ8E hmi, hi8
≤cθ + 28cσ2 θβ + 70cσ4 θβ + 28cσ6 θβ + cσ8β ≤ cσ8β,
for a constant c > 1. Hence, combine equation 90, we have
Efz(W )= E(kW *zk4)
n n	ι
=XX IE (hPsWi, Vi + σ(mi, hi)8 E (〈PsWj, Vi + σ〈mj, h〉)[ 2 ≤ cσ8βn2,
i=1 j=1
for a constant c > 1. Therefore, we can conclude that
R2 = cσ8βn2,
for a constant c > 1.
Applying Lemma B.4 for Concentration. Now we apply Lemma B.4 with
1.
1	( (P1 - σ√n)∖
B = p4,	μ(n,p = 2npθ exp ——------------ I ,
2.
Lf = 12n2θ(1 — θ) + 4n2σ4β(1 — 3β),	L f = 4n2p,
3.
R1 = n2p, R2 = cσ8 βn2 ,
for a constant c > 1, we have
P sup ɪ
W∈θ(n;R) nP
p
fzj(W) —Efzj(W)
j=1
≥δ
=P W∈Zr)n1P∣kW*YCk4-EkW*Yck4∣≥δ
- 32R2 喘in。” n2 ln ( f^ )十.
< exp
+ ln 2
3pδ2
Cσ8β + 8npδ
+ / ln”
+ 2npθ exp
<1,
p
(100)
(101)
(102)
(103)
(104)
(105)
(106)
(107)
(108)
+ μ(n,p)
for a constant C > 96, When P = Ω (σ8βn2 lnn∕δ2).
23
Published as a conference paper at ICLR 2020
B Related Lemmas and Inequalities
Lemma B.1 (Two-sided Bernstein’s Inequality) Given p random variables x1, x2, . . . xp, if ∀i ∈
[p], |xi | ≤ b almost surely, then
P
(1
p
p
[xi - E[xi]]
i=1
≥ t) ≤ 2exp - y--5Pt^------
2 Pp=ι E[x2]+2bt∕3
(109)
Proof See Proposition 2.14 in WainWright (2019), one can easily generalize it to two-sided case. ■
Lemma B.2 (Entry-wise Truncation of a Bernoulli Gaussian Matrix) Let X ∈ Rn×p, where
xi,j 〜iid BG (θ) and let ∣∣∙∣∣∞ denote the maximum element (in absolute value) of a matrix, then
P ( max ∣Xi,j | ≥ t) ≤ 2npθ exp ( — ɪ) .	(110)
Proof A Bernoulli Gaussian variable xj ∀i ∈ [n], j ∈ [p] satisfies xij = b,j ∙ gi,j, where
bi,j 〜iid Ber(θ), gi,j 〜iid N(0,1) and therefore
p( ∣χi,j∣ ≥ t) = Θ ∙ P(∣gi,j| ≥ t) ≤ 2θexp ( — t2)∙	(111)
By union bound, we have:
n p	t2
P(max ∣Xi,j∣≥ t) ≤ E£p( ∣Xi,jl≥ t) ≤ 2npθexp I — — I.	(112)
i,j	i=1 j=1	2
Lemma B.3 (—Net Covering of Stiefel Manifolds) 11 There is a covering —net S for Stiefel
manifold M = {W ∈ Rn×r∣W* W = I}, (n ≥ r) in operator norm
∀W ∈ M, ∃W0 ∈ S subject to kW — W0k2 ≤ ,	(113)
of size |Se| ≤ (6 )nr.
Proof See Lemma D.4 in Zhai et al. (2019b).	■
Lemma B.4 (Uniform Concentration bound over O(n; R)) Let Z ∈ Rn×p be a random matrix
whose columns zι, z2,..., Zp are i.i.d. drawnfrom a distribution P. ∀z 〜P, let fz (∙) : O(n; R) →
R denote a function that maps O(n; R) to R. ∀B > 0, let Z denote the truncation of Z:
z∙ , = Jzij	if	∖zi,j 1 ≤ B,
i,j 0 otherwise.
(114)
Assume that:
1.
2.
P (maxi,j ∖zi,j∖ > B) < μ(n,p), where μ(n,p) → 0 as P increase, B depends on P.
Efz(∙) is Lf -Lipschitz and fz(∙) is Lf -Lipschitz, that is, ∀Wι, W2 ∈ O(n; R):
∣Efz(Wι) — Efz(W2)∣ ≤ LfkWI- W2k2,
Ifz(Wi) — fz(W2)∣ ≤ LfkWI- W2k2 ∙
(115)
3. ∀W ∈ O(n;R), fz(W) ≤ Ri,E[fz(W)] ≤ R2.
11A similar result can be found in Lemma 4.5 of Recht et al. (2010).
24
Published as a conference paper at ICLR 2020
Then:
P SUP ɪ XX L (W) - Efzj (W)] ≥ δ
W∈∈O(n;R) np j=1	I
P	pn2δ2	2 (12(Lf + L f) ∖	]
<	exp----------------；——+ n2 ln ∣ -------— ∣ +ln2 + μ(n,p),
P L 32R2 +8Rιnδ∕3+	卜 nδ J+	+ KnP
(116)
when p > ρ, where P depends on n.
Proof By assumption 1, We have
P sup ɪ
W∈θ(n;R) np
P
EL (W) - Efzj (W)]
j=i
≥δ
≤P(Z= Z) + P
SUp
W ∈O(n;R)
1P
-	Efzj (W) - Efz(W) ≥
J=I
≥ nδ
(117)
≤μ(n,p) + P	sup
W ∈O(n;R)
1P
-	Efzj (W) - Efz(W) ≥
J=I
≥ nδ
Uniform bound over O(n; R).	∀e > 0, lemma B.3 shows there exists an e-nets Se that covers
O(n; R):
I"
Se = {W1, W2,..., W∣s,∣},	O(n; R) U U B(Wl ,e),	(118)
l = 1
2
and |Se| satisfies |Se| ≤ (6∕e)n . Together with Lipschitz assumption 2, we know that
sup
W ∈B(Wι,e)
-	P
-	Efzj (W)-Efz(W)
J=I
≤ sup
W ∈B(Wι,e)
-P
-Efzj (Wi) - Efz(Wi)
J=I
+ sup	|Efz(Wi) - Efz(W)|
W ∈B(Wι,e)
+ sup
W ∈B(W ,e)
-P	-P
P Σ fzj (Wi) - P Σ fzj (w )
(119)
≤ sup
W ∈B(Wι,e)
-	P
f ΣS fzj (Wi )- Efz(Wi) + (Lf + L f )e.
P j=i
Hence, let
nδ
2(Lf + Lf)
(120)
25
Published as a conference paper at ICLR 2020
we have
P
P sup
W ∈O(n;R)
1 EfZj (W) - EfZ(W) >
> nδ
卬
≤ X P I sup
l=1	W ∈B(W"e)
n2
P EfZj (W) - EfZ(W) >
j=ι
P sup
W ∈B(W"e)
exp n2 ln
nδ
> nδ
1P
PEfZj (Wl)-EfZ(Wl)
P sup
W ∈B(W"e)
(121)
> nδ — (Lf + Lf)e
nδ
PEfZj (Wl)- EfZ(Wl ) > W
j=1
< G
j
1
P

j
12(Lf + L f)
1
P
P
Tail bound within each B(Wl, e). Then, ∀/ ∈ [5e], we apply point-wise control to a given point
Wl ∈ S. Later we will provide a uniform concentration bound over O(n; R). By triangle inequality,
we have
P
1
P
P EfZj (Wl)	- EfZj (Wl)	≤ PEfZj	(Wl)	-	EfZ(Wl)	+∣ EfZj (Wl)	- EfZ(Wl) ∣.
j=ι
j=1
P
(122)
∀W ∈ R(Wl, e), by assumption 1 and 3, we have
IEfZj(W) - EfZj(W)∣ = ∣E [fz(W) ∙ IZ=Z] I ≤ √E2fz(W) J∣E1 Z=Z
^E2 fz(W)P(max ∖zi,j| > B) ≤ √R2μ(n,p).
(123)
Let P be the lower bound of p, such that ∀p > ρ, we have ,R2μ(n,P) < 苧.Hence, when p > ρ,
we have
P
1
P
PEfZj (Wl) - EfZj (Wl) ≤ PEfZj (Wl)- EfZj (Wl)
j=ι
nδ
+ τ,
(124)
P
j
1
which implies
P
P
P
P∑fZj (W) - EfZ(W) > J
j=i
1 ∑fZj (w) - EfZ(W) > nδ I
(125)
j=i
≤ 2 exp -
pn2δ2
32R2 + 8Rιnδ∕3 ,
1
P
≤ P
1
P
where the last inequality is achieved by Bernstein,s inequality (Lemma B.1), along with assumption
3 and EfZ(W) ≤ Ef%W) ≤ R2. Combine equation 125 and equation 121, we have
P
P sup
W ∈θ(n;R)
P EfZj (W) - EfZ(W) >
j=i
< exp n2 ln
12(Lf + Lf)
nδ
< exp
pn2 δ2
32R2 + 8Rιnδ∕3
> nδ
P sup
W ∈B(W"e)
+ n2 ln
nδ
PEfZj(Wl)- EfZ(Wl) > ~2^
pj
12(Lf + L f)
nδ
+ ln2 .
(126)
—

1

P
i
26
Published as a conference paper at ICLR 2020
Summary. Therefore, we conclude that
p
P (w X;R) jEfzj (W )-EfZj (W)]
j=1
(127)
< exp
pn2δ2
32R2 + 8R1 nδ∕3
≥δ
+ n ln(12f
nδ
+ ln2 + μ(n,p),
—
when p > ρ.
Lemma B.5 (Concentration Bound of the Clean Objective over O(n; R)) ∀θ ∈ (0, 1], if X ∈
Rnxp, χi,j 〜iid BG (θ) ,for any δ > 0, the following inequality holds
P( sup	-1h∣wχ k4
W∈O(n;R) np I
(	3pδ2
< eχp	c1 θ + 8n(lnp)4δ
+ exp (-* + n2 ln ( 60^
c2θ
(128)
(ln p)2
2
+ 2nPθ exp -
for some constants c1 > 104 , c2 > 3360. Moreover
exp (- Cιθ+3pX p)4 δ + n2 ln (60np
+ exp ( — pδθ + n2 ln (60np,nPp )) + 2npθexp I 一
(InP)2) ≤ 1
(129)
when P = Ω(θn2 ln n∕δ2).
Proof see Lemma 2.2 in Zhai et al. (2019b), note that the sparsity condition θ ∈ (0, 1)
original Lemma in Zhai et al. (2019b) can be easily generalized to θ = 1.
of the
C Additional Experimental Results
口■删1口工,力
(a)
(b)
(C)
≡l Q■腼U加力
设I □■设这现历
■I BW I
SH
Figure 8: Representations of three 16 X 16 patches in both the clean and noisy images. Each selected
patch is visualized, both with and without noise, and the 6 corresponding bases with largest absolute
coefficients are shown.
27
Published as a conference paper at ICLR 2020
(a)
(b)
Figure 9: Representations of three 8 X 8 X 3 colored patches in both the clean and noisy images.
Each selected patch is visualized, both with and without noise, and the 6 corresponding bases with
largest absolute coefficients are shown.
(a) Clean Patches
(c) Patches with Outliers
(b) Patches with Noise (SNR = 6.23)
(d) Patches with Sparse Corruptions
Figure 10: All 8 X 8 X 3 = 192 bases learned from 100, 000 random 8 X 8 colored patches
sampled from the CIFAR-10 data-set. (a) Learned Bases from clean CIFAR-10; (b) Learned Bases
from CIFAR-10 with Gaussian noise, SNR = 6.23; (c) Learned Bases from CIFAR-10 with 20%
of Gaussian outliers; (d) Learned Bases from CIFAR-10 with 50% of sparse corruptions. For all
learned bases, the resulting atoms are sorted according to the '1 -norm of their coefficients in the
sparse code.
28