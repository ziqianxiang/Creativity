Published as a conference paper at ICLR 2020
The Implicit Bias of Depth: How Incremental
Learning Drives Generalization
Daniel Gissin, Shai Shalev-Shwartz, Amit Daniely
School of Computer Science
The Hebrew University
Jerusalem, Israel
{daniel.gissin,shais,amit.daniely}@mail.huji.ac.il
Ab stract
A leading hypothesis for the surprising generalization of neural networks is that
the dynamics of gradient descent bias the model towards simple solutions, by
searching through the solution space in an incremental order of complexity. We
formally define the notion of incremental learning dynamics and derive the condi-
tions on depth and initialization for which this phenomenon arises in deep linear
models. Our main theoretical contribution is a dynamical depth separation re-
sult, proving that while shallow models can exhibit incremental learning dynam-
ics, they require the initialization to be exponentially small for these dynamics to
present themselves. However, once the model becomes deeper, the dependence
becomes polynomial and incremental learning can arise in more natural settings.
We complement our theoretical findings by experimenting with deep matrix sens-
ing, quadratic neural networks and with binary classification using diagonal and
convolutional linear networks, showing all of these models exhibit incremental
learning.
1	Introduction
Neural networks have led to a breakthrough in modern machine learning, allowing us to efficiently
learn highly expressive models that still generalize to unseen data. The theoretical reasons for this
success are still unclear, as the generalization capabilities of neural networks defy the classic statis-
tical learning theory bounds. Since these bounds, which depend solely on the capacity of the learned
model, are unable to account for the success of neural networks, we must examine additional proper-
ties of the learning process. One such property is the optimization algorithm - while neural networks
can express a multitude of possible ERM solutions for a given training set, gradient-based methods
with the right initialization may be implicitly biased towards certain solutions which generalize.
A possible way such an implicit bias may present itself, is if gradient-based methods were to search
the hypothesis space for possible solutions of gradually increasing complexity. This would suggest
that while the hypothesis space itself is extremely complex, our search strategy favors the simplest
solutions and thus generalizes. One of the leading results along these lines has been by Saxe et al.
(2013), deriving an analytical solution for the gradient flow dynamics of deep linear networks and
showing that for such models, the singular values converge at different rates, with larger values
converging first. At the limit of infinitesimal initialization of the deep linear network, Gidel et al.
(2019) show these dynamics exhibit a behavior of “incremental learning” - the singular values of the
model are learned separately, one at a time. Our work generalizes these results to small but finite
initialization scales.
Incremental learning dynamics have also been explored in gradient descent applied to matrix com-
pletion and sensing with a factorized parameterization (Gunasekar et al. (2017), Arora et al. (2018),
Woodworth et al. (2019)). When initialized with small Gaussian weights and trained with a small
learning rate, such a model is able to successfully recover the low-rank matrix which labeled the
data, even if the problem is highly over-determined and no additional regularization is applied. In
their proof of low-rank recovery for such models, Li et al. (2017) show that the model remains low-
rank throughout the optimization process, leading to the successful generalization. Additionally,
1
Published as a conference paper at ICLR 2020
Figure 1: Incremental learning dynamics in deep models. Each panel shows the evolution of the five
largest values of σ, the parameters of the induced model. All models were trained using gradient
descent with a small initialization and learning rate, on a small training set such that there are mul-
tiple possible solutions. In all cases, the deep parameterization of the models lead to “incremental
learning”, where the values are learned at different rates (larger values are learned first), leading
to sparse solutions. (a) Depth 4 matrix sensing, σ denotes singular values (see section 4.1). (b)
Quadratic networks, σ denotes singular values (see section 4.2). (c) Depth 3 diagonal networks, σ
denotes feature weights (see section 4.3). (d) Depth 3 circular-convolutional networks, σ denotes
amplitudes in the frequency domain of the feature weights (see appendix G).
Arora et al. (2019) explore the dynamics of such models, showing the singular values are learned
at different rates and that deeper models exhibit stronger incremental learning dynamics. Our work
deals with a more simplified setting, allowing us to determine explicitly under which conditions
depth leads to this dynamical phenomenon.
Finally, the learning dynamics of nonlinear models have been studied as well. Combes et al. (2018)
and Williams et al. (2019) study the gradient flow dynamics of shallow ReLU networks under re-
strictive distributional assumptions, Ronen et al. (2019) show that shallow networks learn functions
of gradually increasing frequencies and Nakkiran et al. (2019) show how deep ReLU networks cor-
relate with linear classifiers in the early stages of training.
These findings, along with others, suggest that the generalization ability of deep networks is at least
in part due to the incremental learning dynamics of gradient descent. Following this line of work,
we begin by explicitly defining the notion of incremental learning for a toy model which exhibits
this sort of behavior. Analyzing the dynamics of the model for gradient flow and gradient descent,
we characterize the effect of the model’s depth and initialization scale on incremental learning,
showing how deeper models allow for incremental learning in larger (realistic) initialization scales.
Specifically, we show that a depth-2 model requires exponentially small initialization for incremental
learning to occur, while deeper models only require the initialization to be polynomially small.
Once incremental learning has been defined and characterized for the toy model, we generalize our
results theoretically and empirically for larger linear and quadratic models. Examples of incremental
learning in these models can be seen in figure 1, which we discuss further in section 4.
2	Dynamical Analysis of a Toy Model
We begin by analyzing incremental learning for a simple model. This will allow us to gain a clear
understanding of the phenomenon and the conditions for it, which we will later be able to apply to a
variety of other models in which incremental learning is present.
2.1	Preliminaries
Our simple linear model will be similar to the toy model analyzed by Woodworth et al. (2019).
Our input space will be X = Rd and the hypothesis space will be linear models with non-negative
weights, such that:
fσ (x) = hσ, xi	σ ∈ Rd≥0	(1)
2
Published as a conference paper at ICLR 2020
We will introduce depth into our model, by parameterizing σ using w ∈ Rd≥0 in the following way:
∀i : σi = wiN
Where N represents the depth of the model. Since we restrict the model to having non-negative
weights, this parameterization doesn’t change the expressiveness, but it does radically change it’s
optimization dynamics.
Assuming the data is labeled by some σ* ∈ R≥0, We will study the dynamics of this model for
general N under a depth-normalized1 squared loss over Gaussian inputs, which will allow us to
derive our analytical solution:
'n(W) = ？12Eχ[(hσ*,xi-hwN,xi)2] = 占||wN - σ*∣∣2	⑵
2N2	2N2
We will assume that our model is initialized uniformly with a tunable scaling factor, such that:
∀i : Wi(0) = Nσ0
(3)
2.2	Gradient Flow Analytical Solutions
Analyzing our toy model using gradient flow allows us to obtain an analytical solution for the dy-
namics of σ(t) along with the dynamics of the loss function for a general N. For brevity, the
following theorem refers only to N = 1, 2 and N → ∞, however the solutions for 3 ≤ N < ∞ are
similar in structure to N → ∞, but more complicated. We also assume σ* > 0 for brevity, however
we can derive the solutions for σ* = 0 as well. Note that this result is a special case adaptation of
the one presented in Saxe et al. (2013) for deep linear networks:
Theorem 1. Minimizing the toy linear model described in (1) with gradient flow over the depth
normalized squared loss (2), with Gaussian inputs and weights initialized as in (3) and assuming
σ* > 0 leads to the following analytical solutions for different values of N:
N = 1 ：	σi(t) = σi + (σo - σ*)e-t
N=2：
N→∞:
.小 _	σoσ*eσ^t
σi(t) = σo(e'#一1) + σi
t =^-log (σi 例σ0-引)-工(ɪ - 口
Ey g Jo(σi(t) - σ力	σ八σi(t)	σ°^
Proof. The gradient flow equations for our model are the following:
Wi = -Vwi' = NWN-1(σ* - WN)
Given the dynamics of the w parameters, we may use the chain rule to derive the dynamics of the
induced model, σ :
♦	dσi ♦	2N—2/ * N ∖	2-Nf *	、
σ i = dWw i = W	(σi — Wi ) = σi	(σi — σi)
(4)
This differential equation is solvable for all N, leading to the solutions in the theorem. Taking
N → ∞ in (4) leads to σi = σ2(σ* — σi), which is also solvable.
□
1This normalization is used for mathematical convenience to have solutions of different depths exhibit
similar time scales in their dynamics. Equivalently, we can derive the solutions for the regular square loss and
then use different time scalings in the dynamical analysis.
3
Published as a conference paper at ICLR 2020
N = 2l c∑o=O.l
N = 3, (7o=O.l
N = 4, (7o=O.l
N = 4, σo=O.OOl
Time
Figure 2: Incremental learning dynamics in the toy model. Each panel shows the evolution of σσ(t)
for σ* ∈ {12, 6,4,3} according to the analytical solutions in theorem 1, under different depths
and initializations. The first column has all values converging at the same rate. Notice how the
deep parameterization with small initialization leads to distinct phases of learning, where values are
learned incrementally (bottom-right). The shallow model’s much weaker incremental learning, even
at small initialization scales (second column), is explained in theorem 2.
Analyzing these solutions, we see how even in such a simple model depth causes different factors of
the model to be learned at different rates. Specifically, values corresponding to larger optimal values
converge faster, suggesting a form of incremental learning. This is most clear for N = 2 where the
solution isn’t implicit, but is also the case for N ≥ 3, as we will see in the next subsection.
These dynamics are depicted in figure 2, where we see the dynamics of the different values of σ(t)
as learning progresses. When N = 1, all values are learned at the same rate regardless of the
initialization, while the deeper models are clearly biased towards learning the larger singular values
first, especially at small initialization scales.
Our model has only one optimal solution due to the population loss, but it is clear how this sort of
dynamic can induce sparse solutions - if the model is able to fit the data after a small amount of
learning phases, then it’s obtained result will be sparse. Alternatively, if N = 1, we know that the
dynamics will lead to the minimal `2 norm solution which is dense. We explore the sparsity inducing
bias of our toy model by comparing it empirically2 to a greedy sparse approximation algorithm in
appendix D, and give our theoretical results in the next section.
3 Incremental Learning
Equipped with analytical solutions for the dynamics of our model for every depth, we turn to study
how the depth and initialization effect incremental learning. While Gidel et al. (2019) focuses on
incremental learning in depth-2 models at the limit of σ0 → 0, we will study the phenomenon for a
general depth and for σ0 > 0.
First, we will define the notion of incremental learning. Since all values of σ are learned in parallel,
we can’t expect one value to converge before the other moves at all (which happens for infinitesimal
initialization as shown by Gidel et al. (2019)). We will need a more relaxed definition for incremental
learning in finite initialization scales.
2The code for reproducing all of our experiments can be found in https://github.com/dsgissin/
Incremental-Learning
4
Published as a conference paper at ICLR 2020
Definition 1. Given two values σ%, σ7- such that σ* > σ* > 0 and both are initialized as σi(0)=
σj (0) = σo ‹ σj, and g^ven two scalars S ∈ (0, 4) and f ∈ (3, 1), we call the learning of the
values (s, f)-incremental if there exists a t for which:
σj (t) ≤ sσj < fσ* ≤ σi(t)
In words, two values have distinct learning phases if the first almost converges (f ≈ 1) before the
second changes by much (s	1). Note that for any N, σ(t) is monotonically increasing and so
once σj (t) = sσj, it will not decrease to allow further incremental learning. Given this definition of
incremental learning, we turn to study the conditions that facilitate incremental learning in our toy
model.
Our main result is a dynamical depth separation result, showing that incremental learning is depen-
dent on 需 in different ways for different values of N. The largest difference in dependence happens
between N = 2 and N = 3, where the dependence changes from exponential to polynomial:
Theorem 2. Given two values σ%, σ7- of a toy linear model as in (1), where σ⅛ = r > 1 and the
model is initialized as in (3), and given two scalars S ∈ (0,1) and f ∈ (3, 1), then the largest
initialization value for which the learning phases of the values are (s, f)-incremental, denoted σ0th,
is bounded in the following way:
_____1_____
(1-f)(r-1)
1 — S
≤σ0h ≤ sσ*( f 厂
N=2
* (	(1 — f )(r —1) λ N-2
j 11 + (1- f)(r - 1)>
≤σ0th ≤
N N
* (丫 - 1、N—2
sσj Ir-I)
N≥3
Proof sketch (the full proof is given in appendix A). Rewriting the separable differential equation in
(4) to calculate the time until σ(t) = ασ*, we get the following:
-_ *
∕∙ɑσ
tα(σ) =
σ0
dσ
σ2-N (σ* - σ)
The condition for incremental learning is then the requirement that tf (σi) ≤ ts (σj), resulting in:
f fσ*	dσ	V f sσj	dσ
Jo0	σ2-稔(σ* — σ) — Jo0	σ2-NN (σ* — σ)
We then relax/restrict the above condition to get a necessary/sufficient condition on σ0, leading to a
lower and upper bound on σ0th .
□
Note that the value determining the condition for incremental learning is σ* - if two values are in the
σ
same order of magnitude, then their ratio will be close to 1 and we will need a small initialization
to obtain incremental learning. The dependence on the ratio changes with depth, and is exponential
for N = 2. This means that incremental learning, while possible for shallow models, is difficult
to see in practice. This result explains why changing the initialization scale in figure 2 changes the
dynamics of the N ≥ 3 models, while not changing the dynamics for N = 2 noticeably.
The next theorem extends part of our analysis to gradient descent, a more realistic setting than the
infinitesimal learning rate of gradient flow:
Theorem 3. Given two values σi, σj ofa depth-2 toy linear model as in (1), such that O* = r > 1
and the model is initialized as in (3), and given two scalars S ∈ (0, 4) and f ∈ (43, 1), and assuming
σ* ≥ 2σo, and assuming we optimize with gradient descent with a learning rate η ≤ o* for
5
Published as a conference paper at ICLR 2020
C < 2(√2 一 1) and σj the largest value of σ*, then the largest initialization value for which the
learning phases of the values are (s, f)-incremental, denoted σ0th, is lower and upper bounded in
the following way:
1 S 1 - - f S A A-ι	th S 1 - - f S B B-ι
2 口 j 于f 三)	≤ σ ≤ 口 j -T 三)
Where A and B are defined as:
A
log (1 - C异 + c2(异)2)
log (1 一 C σf 一 c4 ( σj )2)
log (1 一C ⅜^ 一 c42 (异)2
log (1 - C σ + c2(σ )2)
B
We defer the proof to appendix B.
Note that this result, while less elegant than the bounds of the gradient flow analysis, is similar in
nature. Both A and B simplify tor when we take their first order approximation around C = 0,
giving us similar bounds and showing that the condition on σ0 for N = 2 is exponential in gradient
descent as well.
While similar gradient descent results are harder to obtain for deeper models, we discuss the general
effect of depth on the gradient decent dynamics in appendix C.
4 Incremental Learning in Larger Models
So far, we have only shown interesting properties of incremental learning caused by depth for a
toy model. In this section, we will relate several deep models to our toy model and show how
incremental learning presents itself in larger models as well.
4.1	Matrix Sensing
The task of matrix sensing is a generalization of matrix completion, where our input space is X =
Rd×d and our model is a matrix W ∈ Rd×d , such that:
fw (A) = hW,Ai = tr(W T A)
Following Arora et al. (2019), we introduce depth by parameterizing the model using a product of
matrices and the following initialization scheme (Wi ∈ Rd×d):
W = WN WN-1∙∙∙ Wi	(5)
∀i ∈ [N], Wi(0) = √σοι
Note that when d = 1, the deep matrix sensing model reduces to our toy model without weight
sharing. We study the dynamics of the model under gradient flow over a depth-normalized squared
loss, assuming the data is labeled by a matrix sensing model parameterized by a PSD W * ∈ Rd×d:
'n(WNWN-1 …WI) = ；^yEA[(((WNWN-1 …WI) - W*, Ai2]
21N	(6)
=2N||(WnWn-i …Wi) - W*||F
The following theorem relates the deep matrix sensing model to our toy model, showing the two
have the same dynamical equations:
6
Published as a conference paper at ICLR 2020
Theorem 4.	Optimizing the deep matrix sensing model described in (5) with gradient flow over
the depth normalized squared loss ((6)), with weights initialized as in (5) leads to the following
dynamical equations for different values of N :
σi(t) = σi(t)2-NN (σ* - σi(t))
Where σ% and σ* are the i th singular values of W and W *, respectively, corresponding to the same
singular vector.
The proof follows that of Saxe et al. (2013) and Gidel et al. (2019) and is deferred to appendix E.
Theorem 4 shows us that the bias towards sparse solutions introduced by depth in the toy model is
equivalent to the bias for low-rank solutions in the matrix sensing task. This bias was studied in
a more general setting in Arora et al. (2019), with empirical results supporting the effect of depth
on the obtainment of low-rank solutions under a more natural loss and initialization scheme. We
recreate and discuss these experiments and their connection to our analysis in appendix E, and an
example of these dynamics in deep matrix sensing can also be seen in panel (a) of figure 1.
4.2 Quadratic Neural Networks
By drawing connections between quadratic networks and matrix sensing (as in Soltanolkotabi et al.
(2018)), we can extend our results to these nonlinear models. We will study a simplified quadratic
network, where our input space is X = Rd and the first layer is parameterized by a weight matrix
W ∈ Rd×d and followed by a quadratic activation function. The final layer will be a summation
layer. We assume, like before, that the labeling function is a quadratic network parameterized by
W* ∈ Rd×d . Our model can be written in the following way, using the following orthogonal
initialization scheme:
d
fW (x) = X(wiT x)2 = xTWTWx = hWTW, xxTi	(7)
i=1
W0T W0 = σ0I
Immediately, we see the similarity of the quadratic network to the deep matrix sensing model with
N = 2, where the input space is made up of rank-1 matrices. However, the change in input space
forces us to optimize over a different loss function to reproduce the same dynamics:
Definition 2. Given an input distribution over an input space X with a labeling function y : X → R
and a hypothesis h, the variance loss is defined in the following way:
'var(h) =	Eχ[(y(x) - h(x))2] - 116Eχ[y(x) - h(x)]2
Note that minimizing this loss function amounts to minimizing the variance of the error, while the
squared loss minimizes the second moment of the error. We note that both loss functions have the
same minimum for our problem, and the dynamics of the squared loss can be approximated in certain
cases by the dynamics of the variance loss. For a complete discussion of the two losses, including
the cases where the two losses have similar dynamics, we refer the reader to appendix F.
Theorem 5.	Minimizing the quadratic network described and initialized as in (7) with gradient flow
over the variance loss defined in (2) leads to the following dynamical equations:
σi(t) = σi(t)(σ* - σi(t))
Where σi and σi* are the ith singular values ofW and W*, respectively, corresponding to the same
singular vector.
We defer the proof to appendix F and note that these dynamics are the same as our depth-2 toy
model, showing that shallow quadratic networks can exhibit incremental learning (albeit requiring a
small initialization).
7
Published as a conference paper at ICLR 2020
4.3 Diagonal/Convolutional Linear Networks
While incremental learning has been described for deep linear networks in the past, it has been
restricted to regression tasks. Here, we illustrate how incremental learning presents itself in binary
classification, where implicit bias results have so far focused on convergence att → ∞ (Soudry et al.
(2018), Nacson et al. (2018), Ji & Telgarsky (2019)). Deep linear networks with diagonal weight
matrices have been shown to be biased towards sparse solutions when N > 1 in Gunasekar et al.
(2018), and biased towards the max-margin solution for N = 1. Instead of analyzing convergence
at t → ∞, we intend to show that the model favors sparse solutions for the entire duration of
optimization, and that this is due to the dynamics of incremental learning.
Our theoretical illustration will use our toy model as in (1) (initialized as in (3)) as a special weight-
shared case of deep networks with diagonal weight matrices, and we will then show empirical results
for the more general setting. We analyze the optimization dynamics of this model over a separable
dataset {xi, yi}im=1 where yi ∈ {±1}. We use the exponential loss (`(f (x), y) = e-yf (x)) for the
theoretical illustration and experiment on the exponential and logistic losses.
Computing the gradient for the model over w, the gradient flow dynamics for σ become:
σ i=Nm2 σ2-书 X e-yj hβ,xj ixj
j=1
We see the same dynamical attenuation of small values of σ that is seen in the regression model,
一， ......................... 2-V 「	，.	，	，.	1
caused by the multiplication by σi N . From this, we can expect the same type of incremental
learning to occur - weights of σ will be learned incrementally until the dataset can be separated by
the current support of σ. Then, the dynamics strengthen the growth of the current support while
relatively attenuating that of the other values. Since the data is separated, increasing the values of
the current support reduces the loss and the magnitude of subsequent gradients, and so we should
expect the support to remain the same and the model to converge to a sparse solution.
Granted, the above description is just intuition, but panel (c) of figure 1 shows how it is born out
in practice (similar results are obtained for the logistic loss). In appendix G we further explore
this model, showing deeper networks have a stronger bias for sparsity. We also observe that the
initialization scale plays a similar role as before - deep models are less biased towards sparsity when
σ0 is large.
In their work, Gunasekar et al. (2018) show an equivalence between the diagonal network and the
circular-convolutional network in the frequency domain. According to their results, we should ex-
pect to see the same sparsity-bias of diagonal networks in convolutional networks, when looking at
the Fourier coefficients of σ. An example of this can be seen in panel (d) of figure 1, and we refer the
reader to appendix G for a full discussion of their convolutional model and it’s incremental learning
dynamics.
5 Conclusion
Gradient-based optimization for deep linear models has an implicit bias towards simple (sparse)
solutions, caused by an incremental search strategy over the hypothesis space. Deeper models have
a stronger tendency for incremental learning, exhibiting it in more realistic initialization scales.
This dynamical phenomenon exists for the entire optimization process for regression as well as
classification tasks, and for many types of models - diagonal networks, convolutional networks,
matrix completion and even the nonlinear quadratic network. We believe this kind of dynamical
analysis may be able to shed light on the generalization of deeper nonlinear neural networks as well,
with shallow quadratic networks being only a first step towards that goal.
Acknowledgments
This research is supported by the European Research Council (TheoryDL project).
8
Published as a conference paper at ICLR 2020
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep
matrix factorization. In Advances in Neural Information Processing Systems 32, pp.
7411-7422. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/
8960-implicit-regularization-in-deep-matrix-factorization.pdf.
Remi Tachet des Combes, Mohammad Pezeshki, Samira Shabanian, Aaron Courville, and Yoshua
Bengio. On the learning dynamics of deep neural networks. arXiv preprint arXiv:1809.06848,
2018.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. In Advances in Neural Information Processing Systems, pp.
3196-3206, 2019.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772-1798, 2019.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. arXiv preprint arXiv:1712.09203,
2017.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro HP Savarese, Nathan Srebro, and Daniel
Soudry. Convergence of gradient descent on separable data. arXiv preprint arXiv:1803.01905,
2018.
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L Edelman, Fred
Zhang, and Boaz Barak. Sgd on neural networks learns functions of increasing complexity. arXiv
preprint arXiv:1905.11604, 2019.
Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal
matching pursuit: Recursive function approximation with applications to wavelet decomposition.
In Proceedings of 27th Asilomar conference on signals, systems and computers, pp. 40-44. IEEE,
1993.
Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The conver-
gence rate of neural networks for learned functions of different frequencies. In
Advances in Neural Information Processing Systems 32, pp. 4763-4772. Cur-
ran Associates, Inc., 2019.	URL http://papers.nips.cc/paper/
8723- the- convergence- rate- of- neural- networks- for- learned- functions- of- different- fr
pdf.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
9
Published as a conference paper at ICLR 2020
Francis Williams, Matthew Trager, Claudio Silva, Daniele Panozzo, Denis Zorin, and Joan Bruna.
Gradient dynamics of shallow univariate relu networks. arXiv preprint arXiv:1906.07842, 2019.
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and
deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.
A Proof of Theorem 2
Theorem. Given two values σi, σj of a toy linear model as in (1), such that 需=r > 1 and the
model is initialized as in (3), and given two scalars S ∈ (0, 1) and f ∈ (3, 1), then the largest
initialization value for which the learning phases of the values are (s, f)-incremental, denoted σ0th,
is lower and upper bounded in the following way:
____1___
*	(1 —f)(r—1)
吗
≤σ0th ≤
1 — S
*	r—1
S'
N=2
sσ; ( J -f"- L、)誓 ≤σ0 ≤ sσ; ( 3) N'2
j 1 + (1-f)(r- 1)	0 j r-S
N≥3
Proof. Our strategy will be to define the time tα for which a value reaches a fraction α of it’s optimal
value, and then require that tf(σi) ≤ ts(σj). We begin with recalling the differential equation which
determines the dynamics of the model:
σ = σ2-NN (σ* — σ)
Since the solution for N ≥ 3 is implicit and difficult to manage in a general form, we will define tα
using the integral of the differential equation. The equation is separable, and under initialization of
σ0we can describe tα (σ) in the following way:
*
ασ
tα(σ) =
σ0
dσ
σ2-NN (σ* — σ)
Incremental learning takes place when σi(tf) = fσi* happens before σj(ts) = Sσj*. We can write
this condition in the following way:
rfσ*	dσ	V /sσ*	dσ
Jo0 σ2-N (σ* — σ) — Jo0 σ2-N (σ* — σ)
Plugging in σi = rσj and rearranging, we get the following necessary and sufficient condition for
incremental learning:
∕frσ*	dσ	V M /sσ*	dσ
Jσo	σ2-N (I — r0* )	Joo	σ2-N (I — 0* )
rσj	σj
Our last step before relaxing and restricting our condition will be to split the integral on the left-hand
side into two integrals:
*
sσj*
σ0
dσ	Iifrσj	dσ	/ sσ*	dσ
σ2-NN(I — rσ°*)	Lj	σ2-NN (1 — r°*)	Jo0	σ2-NN(I —言)
jjj
(8)
At this point, we cannot solve this equation and isolate σ0 to obtain a clear threshold condition
on it for incremental learning. Instead, we will relax/restrict the above condition to get a neces-
sary/sufficient condition on σ0, leading to a lower and upper bound on the threshold value of σ0.
10
Published as a conference paper at ICLR 2020
S ufficient Condition
To obtain a sufficient (but not necessary) condition on σ0, we may make the condition stricter either
by increasing the left-hand side or decreasing the right-hand side. We can increase the left-hand side
by removing r from the left-most integral’s denominator (r > 1) and then combine the left-most
and right-most integrals:
ffrσj dσ	/ sW	dσ
Lj	σ2-NN(I - 备)	Jo0	σ2-NN(I - σ⅛)
jj
Next, we note that the integration bounds give us a bound on σ for either integral. This means we
Can replace 1 ——4 With 1 on the right-hand side, and replace 1 一
σj
side:
-^T with 1 一 f on the left-hand
rσj
1	frσjj dσ	sσjj dσ
1-f Jsoj	σ2-羚 ≤(r - 1) Jo0	σ2-羚
We may now solve these integrals for every N and isolate σ0, obtaining the lower bound on σ0th. We
start with the case where N = 2:
1
1-f
(log(frσ*) - log(sσ*)) ≤ (r 一 1)(log(sσ*) - log(σ°))
Rearranging to isolate σ0, we obtain our result:
一 / 一*
σo ≤ sσj
1
(1-f )(r-1)
For the N ≥ 3 case, we have the following after solving the integrals:
≤(r -1)(( ⅛ 广-
1	1 λ. -	1	1	1	1	…	1	.	/	1	\ 1-幕 C ♦ C
For simplicity We may further restrict the condition by removing the term (fr)	. Solving for
σ0 gives us the following:
一 / 一*
σo ≤ sσj
((I 一 f)(r 一 1) A N-
U + (1 - f )(r-1) J
Necessary Condition
To obtain a necessary (but not sufficient) condition on σ0, we may relax the condition in (8) either
by decreasing the left-hand side or increasing the right-hand side. We begin by rearranging the
equation:
rfroj	dσ	V Y /soj	dσ _ /soj	dσ
Jsoj	σ2-N (1 一 r°r)	Jo0	σ2-N (1 一 0⅛)	Joo	σ2-N (1 一 r0τ)
roj	oj	roj
Like before, we may use the integration bounds to bound σ. Plugging in σ = sσj for all integrals
decreases the left-hand side and increases the right-hand side, leading us to the following:
r
r — S
Z frojj
sojj
dσ
σ2-NN
≤ (i⅛
11
Published as a conference paper at ICLR 2020
Rearranging, we get the following inequality:
/ frσj dσ r — 1 / sσj dσ
Lj σ2--N - 1 - S Jo0	σ2-N
We now solve the integrals for the different cases. For N = 2, we have:
log(frσ*) — log(sσ*) ≤ r--ɪ (log(sσji) — log(σo)
Rearranging to isolate σ0 , we get our condition:
Finally, for N ≥ 3, we solve the integrals to give us:
((二)1-得 T 3)1-得)≤ F(( ɪ)1-得—二)1-得
∖v sσ*7	rfσj，	1	1 — s ∖vσo7	sσj
Rearranging to isolate σ0 , we get our condition:
一 / 一*
σo ≤ sσj
1、 N
r — 1 ∖ N-2
r—s
S ummary
For a given N , we derived a sufficient condition and a necessary condition on σ0 for (s, f)-
incremental learning. The necessary and sufficient condition on σ0, which is the largest initialization
value for which we see incremental learning (denoted σ0th), is between the two derived bounds.
The precise bounds can possibly be improved a bit, but the asymptotic dependence on r is the crux
of the matter, showing the dependence on r changes with depth with a substantial difference when
we move from shallow models (N = 2) to deeper ones (N ≥ 3)
□
B Proof of Theorem 3
Theorem. Given two values σ%, σ7- of a depth-2 toy linear model as in (1), such that Oj = r > 1
and the model is initialized as in (3), and given two scalars S ∈ (0, 4) and f ∈ (4, 1), and assuming
σj ≥ 2σo, and assuming we optimize with gradient descent with a learning rate η ≤ ɪ for
C < 2(√2 — 1) and σj the largest value of σ*, then the largest initialization value for which the
learning phases of the values are (S, f)-incremental, denoted σ0th, is lower and upper bounded in
the following way:
1 S 1 — — f S A A-T th S 1 — — f S B B-1
2 口 σX Ef 口) ≤ σ ≤ 口 σX ~τ 口)
Where A and B are defined as:
A
log (1 — C ⅜^ + c2( Oj )2)
log (1 — C Oj — c2 (°*∙ )2)
log (1 — C Oj— c2 (σ⅜ )2
B
log (1 — COj + c2(Oj )2)
12
Published as a conference paper at ICLR 2020
Proof. To show our result for gradient descent and N = 2, we build on the proof techniques of
theorem 3 of Gidel et al. (2019). We start by deriving the recurrence relation for the values σ(t) for
general depth, when t now stands for the iteration. Remembering that win = σi , we write down the
gradient update for wi (t):
wi(t +I) = wi(t) + nNwi(t)N-1(σi - σi(t)) = Pσi⑴ + nNσi(t)1-Nr (σi - σi(t))
Raising wi(t) to the N th power, we get the gradient update for the σ values:
σi(t +1)= (Pσi⑴ + n-1(σi - σi(t))σi(t)1-N) = σi⑴(1 + nNσi(t)1-Nr(σi - σi⑴D
(9)
Next, we will prove a simple lemma which gives us the maximal learning rate we will consider
for the analysis, for which there is no overshooting (the values don’t grow larger than the optimal
values).
2-2
Lemma 1. For the gradient update in (9), assuming σ%(0) < σ*, if n ≤ (*) N and N ≥ 2,
then:
∀t : σi(t) ≤ σ*
2-2
Proof. Plugging in n = C(") N for C ≤ 1, We have:
σ,(t +1) = σ,(t) (1 + N (等)1-—N (1-(等)))N ≤ σi(t)e(岑)^1-(*D
Defining r =箸 and dividing both sides by σ*, We have:
σi
ri(t + 1) ≤ ri(t)eri(t)ι N (1-ri(t))
1--2
It is enough to show that for any 0 ≤ r ≤ 1, we have that rer (1-r) ≤ 1, as over-shooting occurs
When ri(t) > 1. Indeed, this function is monotonic increasing in 0 ≤ r ≤ 1 (since the exponent is
non negative), and equals 1 when r = 1. Since r = 1 is a fixed point and no iteration that starts at
r < 1 can cross 1, then ri(t) ≤ 1 for any t. This concludes our proof.	□
Under this choice of learning rate, we can now obtain our incremental learning results for gradient
descent when N = 2. Our strategy will be bounding σi (t) from below and above, which will give
us a lower and upper bound for tα (σi). Once we have these bounds, we will be able to describe
either a necessary or a sufficient condition on σ0 for incremental learning, similar to theorem 2.
The update rule for N = 2 is:
σi(t + 1) = σi⑴(1 + 1 n(σi - σi(t)))
Next, we plug in n = -cr for c < 2(√2 - 1) < 1 and denote Ri = W ≤ 1 and %(t) = σi(t) ≤ 1 to
σ1	σ1	σi
get:
σi(t + 1) = σi(t) (1 + 2Ri(1 - ri(t)))
13
Published as a conference paper at ICLR 2020
Following theorem 3 of Gidel et al. (2019), We bound σ1t):
11	1
σi(t +I) ―σi(t)(1 + CRi(1 - ri(t)))2
_ 1	1
=σi⑴ 1 + (1 - r*cRi + c2R2(1 - r,(t)))
1	1
≥σi(t)1 + (1 - ∏(t))(cRi + c2R2)
1	c2
≥ σ(t) (1 - (1 — ri(t))(cRi + ɪ Ri ))
1	1	1	c2
=―布 — ( —----------7) (CRi + 五Ri )
σi(t)	σi(t)	σi	4
Where in the fourth line we use the inequality 1++x ≥ 1 — x, ∀x ≥ 0. We may now subtract "
from both sides to obtain:
ɪ —	ɪ ≥	( , 1、— ɪ)(1 — CRi- c2R2)	≥	(ɪ —	ɪ)(1 —	CRi —	c2R2)t
σi(t)	σ* ≥	σσt{t — 1) σ“∖ i 4 ij	≥	&	σ"'	"	4 M
We may now obtain a bound on tα(σi) by plugging in σi(t) = ασi7 and taking the log:
log ( α( σl - 1)) ≥ tα ∙ log (1 — cRi — ɪR2)
Rearranging (note that log (1 — CRi — c2R) < 0 and that our choice of C keeps the argument of
the log positive), we get:
tα(σi) ≥
log")
log (1 — CRi — c2 R2)
Next, we follow the same procedure for an upper bound. Starting with our update step:
1	11
σi(t + 1) = σi(t) 1 + (1 — /(I)) (CRi + c2R2(1 — ri(t)))
	4i 1	1 一σi(t) 1 + (1 — ri(t))CRi ≤-77? (1 — (1 - ri(t))CRi + (1 — ri(t))2C2R2) σi(t)	i
Where in the last line we use the inequality yq+χ ≤ 1 — X + x2, ∀χ ≥ 0. Subtracting ± from both
sides, we get:
-ɪ — - ≤ (-τ1-- — ɪ)(1 — CRi + C2R2) ≤ ( -------------)(1 — CRi + C2R2)t
σi(t)	σ* ≤ vσi(t - 1)	σ * λ	i +	')≤J	σ"'	" + l,
Rearranging like before, we get the bound on the α-time:
log
tα(σi) ≤
1 —α
M ⅞0 T)
log(1 — CRi + c2R2)
14
Published as a conference paper at ICLR 2020
Given these bounds, we would like to find the conditions on σ0 that allows for (s, f)-incremental
learning. We will find a sufficient condition and a necessary condition, like in the proof of theorem
2.
S ufficient Condition
A sufficient condition for incremental learning will be one which is possibly stricter than the exact
condition. We can find such a condition by requiring the upper bound of tf (σi) to be smaller than
the lower bound on ts (σj). This becomes the following condition:
Defining A
log
1 —f	σo
f σj-σο
log
1-s σο
S	b；一σο
log (1 - cRi + c2R2) ≤ log (1 - CRj-亨R2)
log(1 —cRi+c2R2)
log (1-cRj-亨 Rj )
and rearranging, we get the following:
log
1 - f	σ0
f	σi - σ0
)≥ A log (∖⅛
We may now take the exponent of both sides and rearrange again, remembering σ⅜ = r > 1, to get
σj
the following condition:
(0； - σ0)A	≥ f ( 1 - S)A
σA-1(rσ; - σ°) ^ 1-f (丁)
Now, we will add the very reasonable assumption that σj ≥ 2σ0, which allows Us to replace &*二
J	rσ j σo
with * and replace (σj-σo)A-1 with (2σ7- )A—1, only making the condition stricter. This simplifies
the expression to the following:
σ八A—1、 rf /2 - 2s、a
σo)	≥ τ-7(—)
Now we can rearrange and isolate σ0 to get a sufficient condition for incremental learning:
σ0 ≤
l⅛( 12-ff E 产
Necessary Condition
A necessary condition for incremental learning will be one which is possibly more relaxed than the
exact condition. We can find such a condition by requiring the lower bound of tf(σi) to be smaller
than the upper bound on ts (σj). This becomes the following condition:
loo. ( 1-f σ0 )	log ( 1-s —σθ—)
log ∖ f σ*-σo )	≤ g V S σ*-σo J
log (1 - CRi-亨R2) ≤ log (1 - cRj + c2R2)
Defining B
log (1-cRi-c4R2)
log (1 —cRj + c2R2)
and rearranging, we get the following:
log (1-f	) ≥ B log (ɪ--ɪ
f σ -σ - σoj	s S σj - σo
15
Published as a conference paper at ICLR 2020
We may now take the exponent of both sides and rearrange again, remembering σ⅛ = r > 1, to get
σj
the following condition:
(歹-σ0)B	≥ f / 1 — S)B
σB-1(rσ* - σ0) - 1 - f S
We may now relax the condition further, by removing the r from the denominator of the left-hand
side and the σ0 from the numerator. This gives us the following:
σj ∖B-1 ≥ f ( 1 - S ∖B
σoj	≥ 1-f
Finally, rearranging gives us the necessary condition:
σ0 ≤
S * /1 — f S 、B-1
L σ*( 丁 L)
□
C DISCUSSION OF GRADIENT DESCENT FOR GENERAL N
While we were able to generalize our result to gradient descent for N = 2, our proof technique
relies on the ability to get a non-implicit solution for σ(t) which we discretized and bounded. This
is harder to generalize to larger values of N, where the solution is implicit. Still, we can informally
illustrate the effect of depth on the dynamics of gradient descent by approximating the update rule
of the values.
We start by reminding ourselves of the gradient descent update rule for σ , for a learning rate η
c(⅛∙)2--N:
To compare two values in the same scales, we will divide both sides by the optimal value σi* and
look at the update step of the ratio Iri = σi(t), also denoting R = σ⅛:
σi	σ1
ri(t + 1)= ∏(t)(1 + NX-得ri(t)1-NN (1 - r (t))) N
We will focus on the early stages of the optimization process, where r 1. This means we can
neglect the 1 — ri (t) term in the update step, giving us the approximate update step we will use to
compare the general i, j values:
ri(t + 1) ≈ ri(t)(1 + NR-ri(t)1-Nr)
We would like to compare the dynamics of ri and rj , which is difficult to do when the recurrence
relation isn’t solvable. However, we can observe the first iteration of gradient descent and see how
depth affects this iteration. Since we are dealing with variables which are ratios of different optimal
values, the initial values of r are different. Denoting r = σ⅛, we can describe the initialization of rj
σj
using that of ri :
rj(0) = rri(0)
Plugging in the initial conditions and noting that Ri = rRj , we get:
16
Published as a conference paper at ICLR 2020
/ CR N 2	2 ∖Ν
ri(1) ≈ ri(0)(l+——N— ri(0)1 N)
/ 一 ，1、N-1 cR2-N ， 2 ∖N
Tj⑴ ≈ ri(0)( √r + (『)N —N— ri(0)1 N)
We see that the two ratios have a similar update, with the ratio of optimal values playing a role in
how large the initial value is versus how large the added value is. When we use a small learning
rate, we have a very small c which means we can make a final approximation and neglect the higher
order terms of c:
Ti(1) ≈ri(0) + cR2-WTi(0)2--N
Tj(1) ≈rri(0) + (I)-ɪcR2-NTi(0)2-N
We can see that while the initial conditions favor rj , the size of the update for ri is larger by a
factor of r N- when the initialization and learning rates are small. This accumulates throughout the
optimization, making Ti eventually converge faster than Tj .
The effect of depth here is clear - the deeper the model, the larger the relative step size ofTi and the
faster it converges relative to Tj .
D Comparison of The Toy Model and OMP
Learning our toy model, when it’s incremental learning is taken to the limit, can be described as
an iterative procedure where at every step an additional feature is introduced such that it’s weight
is non-zero and then the model is optimized over the current set of features. This description is
also relevant for the sparse approximation algorithm orthogonal matching pursuit (Pati et al., 1993),
where the next feature is greedily chosen to be the one which most improves the current model.
While the toy model and OMP are very different algorithms for learning sparse linear models, we
will show empirically that they behave similarly. This allows us to view incremental learning as a
continuous-time extension ofa greedy iterative algorithm.
To allow for negative weights in our experiments, we augment our toy model as in the toy model of
Woodworth et al. (2019). Our model will have the same induced form as before:
fσ(x) = hσ, xi
However, we parameterize σ using w+ , w- ∈ Rd in the following way:
NN
σi = w+N,i - w-N,i
∀i, w+,i(0) = w-,i(0) = √σ0
We can now treat this algorithm as a sparse approximation pursuit algorithm - given a dictionary
D ∈ Rd×n and an example x ∈ Rd, we wish to find the sparsest α for which Dα ≈ x by minimizing
the '0 norm of α subject to ∣∣Dα - x||2 = 03. Under this setting, we can compare OMP to our toy
model by comparing the sets of features that the two algorithms choose for a given example and
dictionary.
In figure 3 we run such a comparison. Using a dictionary of 1000 atoms and an example of dimen-
sionality 80 sampled from a random hidden vector of a given sparsity s, we run both algorithms and
record the first s features chosen3 4.
3 Note that this problem is equivalent to learning the toy model over the squared loss, where the examples in
the original learning problem play the role of the dictionary.
4For the toy model, we define a feature to be “chosen” once it’s σi value passes some small threshold in
absolute value.
17
Published as a conference paper at ICLR 2020
O	5 IO 15	20	25
Sparsity of Solution
Figure 3: Empirical comparison of the dynamics of the toy model to OMP. The toy model has a
depth of 5 and was initialized with a scale of 1e-4 and a learning rate of 3e-3. We compare the
fraction of agreement between the sets of first s features selected of the two algorithms for every
given sparsity level s, averaged over 100 experiments (the shaded regions are empirical standard
deviations). For example, for sparsity level 3, we look at the sets of first 3 features selected by each
algorithm and calculate the fraction of them that appear in both sets.
For every sparsity s, we plot the mean fraction of agreement between the sets of features chosen
by OMP and the toy model over 100 experiments. We see that the two algorithms choose very
similar features at the beginning, suggesting that the deep model approximates the discrete behavior
of OMP. Only when the number of features increases do we see that the behavior of the two models
begins to differ, caused by the fact that the toy model has a finite initialization scale and learning
rate.
These experiments demonstrate the similarity between the incremental learning of deep models and
the discrete behavior of greedy approximation algorithms such as OMP. Adopting this view also
allows us to put our finger on another strength of the dynamics of deep models - while greedy algo-
rithms such as OMP require the analytical solution or approximation of every iterate, the dynamics
of deep models are able to incrementally learn any differentiable function. For example, looking
back at the matrix sensing task and the classification models in section 4, we see that while there
isn’t an immediate and efficient extension of OMP for these settings, the dynamics of learning deep
models extends naturally and exhibits the same incremental learning as OMP.
E	Incremental Learning in Matrix Sensing
E.1 Proof of Theorem 4
Theorem. Minimizing the deep matrix sensing model described in (5) with gradient flow over the
depth normalized squared loss (6), with Gaussian inputs and weights initialized as in (5) leads to
the following dynamical equations for different values of N:
σi(t) = σi(t)2-NN (σ* - σi(t))
Where σi and σ* are the i th singular values of W and W *, respectively, corresponding to the same
singular vectors.
18
Published as a conference paper at ICLR 2020
Proof. We will adapt the proof from Saxe et al. (2013) for multilayer linear networks. The gradient
flow equations for Wn , n ∈ [N] are:
Wn = NWTn-i(W*- W)WT+in
Where we denote Wj:k = Qik=j Wi .
Since We assumed W * is (symmetric) PSD, there exists an orthogonal matrix U for which D* =
UW*UT where D* is diagonal. Under the initialization in (3), U diagonalizes all Wn matrices at
initialization such that Dn = UWnUT = N√σ0I. Making this change of variables for all Wn, we
get:
Wn = N U T DLn-IU (W * - W )U T D5N U
Rearranging, we get a set of decoupled differential equations for the singular values of Wn :
1
Dn = ND1:n-1(D - D)Dn+1:N
Note that since these matrices are all diagonal at initialization, the above dynamics ensure that they
remain diagonal throughout the optimization. Denoting σn,i as the i’th singular value of Wn and σi
as the i’th singular value of W, we get the following differential equation:
σn,i =
N (σ* - σi) Y σj,i
j6=n
Since we assume at initialization that ∀n, m, i : σ%i(0) = σm,i(0) = √σ0, the above dynamics
are the same for all singular values and we get ∀n, m, i : σ%i(t) = σm,,i(t)= 弋σi(t). We may
now use this to calculate the dynamics of the singular value ofW, since they are the product the the
singular values of all Wn matrices. Denoting σ-n,i = k6=n σk,i and using the chain rule:
σi
N
X σ-n,iσn,i = σ2-N (σ* - σi)
n=1
□
E.2 Empirical Examination
Our analytical results are only applicable for the population loss over Gaussian inputs. These condi-
tions are far from the ones used in practice and studied in Arora et al. (2019), where the problem is
over-determined and the weights are drawn from a Gaussian distribution with a small variance. To
show our conclusions regarding incremental learning extend qualitatively to more natural settings,
we empirically examine the deep matrix sensing model in this natural setting for different depths
and initialization scales as seen in figure 4.
Notice how incremental learning is exhibited even when the number of examples is much smaller
than the number of parameters in the model. While we can’t rely on our theory for describing the
exact dynamics of the optimization for these kinds of over-determined problems, the qualitative
conclusions we get from it are still applicable.
Another interesting phenomena we should note is that once the dataset becomes very small (the
second row of the figure), we see all “currently active” singular values change at the beginning of
every new phase (this is best seen in the bottom-right panel). This suggests that since there is more
than one optimal solution, once we increase the current rank of our model it may find a solution that
has a different set of singular values and vectors and thus all singular values change at the beginning
of a new learning phase. This demonstrates the importance of incremental learning for obtaining
19
Published as a conference paper at ICLR 2020
Figure 4: Evolution of the top-5 singular values of the deep matrix sensing model, with Gaussian
initialization with variance such that the initial singular values are in expectation 1e-4. The model’s
size and data are in R50×50. The columns correspond to different parameterization depths, while the
rows correspond to different dataset sizes. In both cases the problem is over-determined, since the
number of examples is smaller than the number of parameters. Since the original matrix is rank-4,
we can recognize an unsuccessful recovery when all five singular values are nonzero, as seen clearly
for both depth-1 plots.
sparse solutions - once the initialization conditions and depth are such that the learning phases are
distinct, gradient descent finds the optimal rank-i solution in every phase i. For these dynamics to
successfully recover the optimal solution at every phase, the phases need to be far enough apart from
each other to allow for the singular values and vectors to change before the next phase begins.
F Incremental Learning in Quadratic Networks
F.1 Proof of Theorem 5
Theorem. Minimizing the quadratic network described and initialized as in (7) with gradient flow
over the variance loss defined in (2) with Gaussian inputs leads to the following dynamical equa-
tions:
σi(t) = σi(t)(σ↑ - c(t))
Where σi and σ* are the ith singular values of W and W *, respectively, corresponding to the same
singular vectors.
Proof. Our proof will follow similar lines as the analysis of the deep matrix sensing model. Taking
the expectation of the variance loss over Gaussian inputs for our model gives us:
'var(W) =1 Eχ[(hWTW* - WtW,xxTi)2] - ɪExKWTW* - WtW,xxTi]2
16	16
= 1||WTW* - WTW||F + 1-Tr(WTW* - WTW)2 - 1-Tr(WTW* - WTW)2
8	*	*	F 16	*	*	16	*	*
=1 ||WTW* - WTW||F
8
Following the gradient flow dynamics over W leads to the following differential equation:
20
Published as a conference paper at ICLR 2020
W = 1 w (WT w* - w t w )
We can now calculate the gradient flow dynamics of WT W using the chain rule:
WTW = WTW + WTW = 1 (WTW(W*TW* - WTW) + (W*TW* - WTW)WTW) (10)
Now, under our initialization w0T w0 = σ0I, we get that wT w and w*T w* are simultaneously
diagonalizable at initialization by some matrix U, such that the following is true for diagonal D and
D*:
D(0) = UTW(0)TW(0)U
D* = UTW*TW*U
Multiplying equation (10) by U and UT gives us the following dynamics for the singular values of
WTW:
D = 1 (D(D* - D) + (D* - D)D) = D(D* - D)
These matrices are diagonal at initialization, and remain diagonal throughout the dynamics (the off-
diagonal elements are static according to these equations). We may now look at the dynamics of a
single diagonal element, noticing it is equivalent to the depth-2 toy model:
σi = σi(σ* - σi)
□
F.2 Discussion of the Variance Loss
It may seem that the variance loss is an unnatural loss function to analyze, since it isn’t used in prac-
tice. While this is true, we will show how the dynamics of this loss function are an approximation
of the square loss dynamics.
We begin by describing the dynamics of both losses, showing how incremental learning can’t take
place for quadratic networks as defined over the squared loss. Then, we show how adding a global
bias to the quadratic network leads to similar dynamics for small initialization scales.
F.2.1 Dynamical Derivation
in the previous section, we derive the differential equations for the singular values of WT W under
the variance loss:
σi = σi(σ* - σi)
We will now derive similar equations for the squared loss. The scaled squared loss in expectation
over the Gaussian inputs is:
'(W) =1 Eχ[(hWTW* - WTW,xxτi)2]
16
=1 ||WTW* - WTW||F + ɪTr(WTW* - WTW)2
8	16
21
Published as a conference paper at ICLR 2020
Defining ∆ = WT W* — WT W for brevity, the differential equations for W become:
11
W = -W △+ %Tr(∆)W
Calculating the dynamics of WT W after noting that it is simultaneously diagonalizable with W*T W*
(as in the derivation for the variance loss) leads to the following differential equations for the singular
values of WT W :
σ i=σi (σt - σi+2 χ(σ*—σj))
j
We see that the equations are now coupled and so we cannot solve them analytically. Another issue
is that for our initialization, all singular values have very similar dynamics at initialization due to
the coupling. For example, values corresponding to small optimal singular values grow much faster
than in the variance loss dynamics, due to the effect the large optimal singular values have on them.
We see from these equations that we shouldn’t expect quadratic networks optimized over the squared
loss to exhibit incremental learning behavior. We next show how adding a global bias to our model
can help.
F.2.2 Variance Loss Approximates the S quared Loss
To see how the variance loss can have dynamics resembling those of the squared loss, we will add
a global (trainable) bias to our model. This means our model is now parameterized by W ∈ Rd×d
and a scalar b ∈ R:
fW,b(x) = hWTW, xxTi + b
We may now analyze gradient flow of the squared loss. Following the same methods as before, this
leads us to the following differential equations:
-σ + 1 X(σj-σ ) + 1(b*-b))
j
b=∑(σ* - σj) + b*-b
j
Notice how, ifb is at it’s optimum at a given time (b = j (σj* - σj) + b*), the dynamics of σi align
with those of the variance loss. Alternatively, when b = b*, we recover the dynamics of the squared
loss without the global bias term.
To convince ourselves that the dynamics of this model resemble those of the variance loss, we would
need to explain why the global bias is at it’s optimum “most of the time”, such that the singular values
don’t change much during the times when it is not at it’s optimum.
Observing the differential equations for b and for σi, we see they are similar (if we ignore the σi* - σi
value which doesn’t change the order of magnitude of the entire expression when there haven’t been
many learning phases yet). The only difference being a multiplication by σi . This means that we
may informally write:
》 ≈ σi(t)
b(t)
Since at initialization and until the learning phase of σi takes place, we have σi(t) 1, we see
that the global bias optimizes much faster than the singular values for which the learning phase
hasn’t begun yet. This means these singular values will remain small during the times in which
22
Published as a conference paper at ICLR 2020
Figure 5: Quadratic model’s evolution of top-5 singular values for a rank-4 labeling function. The
rows correspond to whether or not a global bias is introduced to the model. The first two columns
are for a large dataset (one optimal solution) and the last two columns are for a small dataset (over-
determined problem). When a bias is introduced, it is initialized to it’s optimal value at initialization.
Note how without the bias, the singular values are learned together and there is over-shooting of the
optimal singular value caused by the coupling of the dynamics of the singular values. For the small
datasets, we see that the model with no bias reaches a solution with a larger rank. Once a global bias
is introduced, the dynamics become more incremental as in the analysis of the variance loss. Note
that in this case the solution obtained for the small dataset is the optimal low-rank solution.
the bias isn’t optimal, and so incremental learning can still take place (assuming a small enough
initialization).
Under these considerations, we say that the dynamics of the squared loss for a quadratic network
with an added global bias resemble the idealized dynamics of the variance loss for a depth-2 linear
model which we analyze formally in the paper. In figure 5 we experimentally show how adding a
bias to a quadratic network does lead to incremental learning similar to the depth-2 toy model.
G Incremental Learning in Classification
G.1 Diagonal Networks
In section 4.3 we viewed our toy model as a special case of the deep diagonal networks described
in Gunasekar et al. (2018), expected to be biased towards sparse solutions. Figure 6 shows the
dynamics of the largest values of σ for different depths of the model. We see that the same type of
incremental learning we saw in earlier models exists here as well - the features are learned one by
one in deeper models, resulting in a sparse solution. The leftmost panel shows how the initialization
scale plays a role here as well, with the solution being more sparse when the initialization is small.
We should note that these results do not defy the results of Gunasekar et al. (2018) (from which we
would expect the initialization not to matter), since their results deal with the solution at t → ∞.
G.2 Convolutional Networks - Preliminaries
The linear circular-convolutional network of Gunasekar et al. (2018) deals with one-dimensional
convolutions with the same number of outputs as inputs, such that the mapping from one hidden
layer to the next is parameterized by wn and defined to be:
d-1
hn[i] =	wn[k]hn-1[(i + k) mod d] = (hn-1 ?wn)[i]
k=0
23
Published as a conference paper at ICLR 2020
Figure 6: Incremental learning in binary classification. A model as in section 4.3 is trained over 200
i.i.d. random Gaussian examples, where d = 100. The data is labeled by a weight vector with 4
nonzero values, making the problem realizable with a sparse solution while the max-margin solution
isn’t sparse. The left panel describes the obtained solution’s correlation with the sparse labeling
vector for different depths and initializations. The results are averaged over 100 experiments, with
shaded regions denoting empirical standard deviations. We see that depth-1 models reach results
similar to the max-margin SVM solution as predicted by Gunasekar et al. (2018), while deeper
models are highly correlated with the sparse solution, with this correlation increasing when the
initialization scale is small. The other panels show the evolution of the absolute values of the top-5
weights of σ for the smallest initialization scale. Note that as we increase the depth, incremental
learning is clearly presented.
The final layer is a fully connected layer parameterized by wN ∈ Rd, such that the final model can
be written in the following way:
fσ (x) = ((((X?Wl) ?W2)…)?WN-I)TWN	(11)
Lemma 3 from Gunasekar et al. (2018) shows how we can relate the Fourier coefficients of the
weight vectors to the Fourier coefficients of the linear model induced by the model:
Lemma. For the circular-convolutional model as in (11):
σ = diag(Wι) ∙∙∙ diag(W N-i)Wn ,
where for n = 1,2,…,N,Wn ∈ Cd are the Fourier coefficients of the parameters Wn ∈ Rd.
This lemma connects the convolutional network to the diagonal network, and thus we should expect
to see the same incremental learning of the values of the diagonal network exhibited by the Fourier
coefficients of the convolutional network.
G.3 Convolutional Networks - Empirical Examination
In figure 7 we see the same plots as in figure 6 but for the Fourier coefficients of the convolutional
model. We see that even when the model is far from the toy parameterization (there is no weight
sharing and the initialization is with random Gaussian weights), incremental learning is still clearly
seen in the dynamics of the model. We see how the inherent reason for the sparsity towards sparse
solution found in Gunasekar et al. (2018) is the result of the dynamics of the model - small ampli-
tudes are attenuated while large ones are amplified.
24
Published as a conference paper at ICLR 2020
Correlation vs Depth
Depth 1
Depth 2
Depth 3
Figure 7: Incremental learning in convolutional networks. A model as in appendix G is trained over
200 i.i.d. random Gaussian examples, where d = 100. The weights are initialized randomly and the
data is labeled by a weight vector with 4 nonzero frequencies, making the problem realizable with a
sparse solution in the frequency domain. The left panel describes the obtained solution’s correlation
in the frequency domain with the sparse labeling vector for different depths and initializations. The
results are averaged over 9 experiments, with shaded regions denoting empirical standard deviations.
We see that depth-1 models reach results similar to the max-margin SVM solution, while deeper
models are highly correlated with the optimal sparse solution. The other panels show the evolution
of the amplitudes of the top-5 frequencies of σ for the smallest initialization scale. Note that as we
increase the depth, incremental learning is clearly presented.
25