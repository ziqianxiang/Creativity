Published as a conference paper at ICLR 2020
On the Global Convergence of Train-
ing Deep Linear ResNets
Difan Zou
Department of Computer Science
University of California, Los Angeles
knowzou@cs.ucla.edu
Philip M. Long
Google
plong@google.com
Quanquan Gu
Department of Computer Science
University of California, Los Angeles
qgu@cs.ucla.edu
Ab stract
We study the convergence of gradient descent (GD) and stochastic gradient de-
scent (SGD) for training L-hidden-layer linear residual networks (ResNets). We
prove that for training deep residual networks with certain linear transformations
at input and output layers, which are fixed throughout training, both GD and SGD
with zero initialization on all hidden weights can converge to the global minimum
of the training loss. Moreover, when specializing to appropriate Gaussian random
linear transformations, GD and SGD provably optimize wide enough deep linear
ResNets. Compared with the global convergence result of GD for training stan-
dard deep linear networks (Du & Hu, 2019), our condition on the neural network
width is sharper by a factor of OpκLq, where κ denotes the condition number of
the covariance matrix of the training data. We further propose a modified identity
input and output transformations, and show that a pd ` kq-wide neural network
is sufficient to guarantee the global convergence of GD/SGD, where d, k are the
input and output dimensions respectively.
1 Introduction
Despite the remarkable power of deep neural networks (DNNs) trained using stochastic gradient
descent (SGD) in many machine learning applications, theoretical understanding of the properties
of this algorithm, or even plain gradient descent (GD), remains limited. Many key properties of the
learning process for such systems are also present in the idealized case of deep linear networks. For
example, (a) the objective function is not convex; (b) errors back-propagate; and (c) there is potential
for exploding and vanishing gradients. In addition to enabling study of systems with these properties
in a relatively simple setting, analysis of deep linear networks also facilitates the scientific under-
standing of deep learning because using linear networks can control for the effect of architecture
choices on the expressiveness of networks (Arora et al., 2018; Du & Hu, 2019). For these reasons,
deep linear networks have received extensive attention in recent years.
One important line of theoretical investigation of deep linear networks concerns optimization land-
scape analysis (Kawaguchi, 2016; Hardt & Ma, 2016; Freeman & Bruna, 2016; Lu & Kawaguchi,
2017; Yun et al., 2018; Zhou & Liang, 2018), where major findings include that any critical point
of a deep linear network with square loss function is either a global minimum or a saddle point, and
identifying conditions on the weight matrices that exclude saddle points. Beyond landscape analy-
sis, another research direction aims to establish convergence guarantees for optimization algorithms
(e.g. GD, SGD) for training deep linear networks. Arora et al. (2018) studied the trajectory of gra-
dient flow and showed that depth can help accelerate the optimization of deep linear networks. Ji
& Telgarsky (2019); Gunasekar et al. (2018) investigated the implicit bias of GD for training deep
linear networks and deep linear convolutional networks respectively. More recently, Bartlett et al.
(2019); Arora et al. (2019a); Shamir (2018); Du & Hu (2019) analyzed the optimization trajectory of
1
Published as a conference paper at ICLR 2020
GD for training deep linear networks and proved global convergence rates under certain assumptions
on the training data, initialization, and neural network structure.
Inspired by the great empirical success of residual networks (ResNets), Hardt & Ma (2016) con-
sidered identity parameterizations in deep linear networks, i.e., parameterizing each layer’s weight
matrix as I ` W, which leads to the so-called deep linear ResNets. In particular, Hardt & Ma (2016)
established the existence of small norm solutions for deep residual networks with sufficiently large
depth L, and proved that there are no critical points other than the global minimum when the maxi-
mum spectral norm among all weight matrices is smaller than Op1{Lq. Motivated by this intriguing
finding, Bartlett et al. (2019) studied the convergence rate of GD for training deep linear networks
with identity initialization, which is equivalent to zero initialization in deep linear ResNets. They
assumed whitened data and showed that GD can converge to the global minimum if (i) the training
loss at the initialization is very close to optimal or (ii) the regression matrix Φ is symmetric and
positive definite. (In fact, they proved that, when Φ is symmetric and has negative eigenvalues, GD
for linear ResNets with zero-initialization does not converge.) Arora et al. (2019a) showed that GD
converges under substantially weaker conditions, which can be satisfied by random initialization
schemes. The convergence theory of stochastic gradient descent for training deep linear ResNets is
largely missing; it remains unclear under which conditions SGD can be guaranteed to find the global
minimum.
In this paper, we establish the global convergence of both GD and SGD for training deep linear
ResNets without any condition on the training data. More specifically, we consider the training of
L-hidden-layer deep linear ResNets with fixed linear transformations at input and output layers. We
prove that under certain conditions on the input and output linear transformations, GD and SGD can
converge to the global minimum of the training loss function. Moreover, when specializing to appro-
priate Gaussian random linear transformations, we show that, as long as the neural network is wide
enough, both GD and SGD with zero initialization on all hidden weights can find the global mini-
mum. There are two main ingredients of our proof: (i) establishing restricted gradient bounds and
a smoothness property; and (ii) proving that these properties hold along the optimization trajectory
and further lead to global convergence. We point out the second aspect is challenging especially
for SGD due to the uncertainty of its optimization trajectory caused by stochastic gradients. We
summarize our main contributions as follows:
•	We prove the global convergence of GD and SGD for training deep linear ResNets. Specifically,
we derive a generic condition on the input and output linear transformations, under which both
GD and SGD with zero initialization on all hidden weights can find global minima. Based on this
condition, one can design a variety of input and output transformations for training deep linear
ResNets.
When applying appropriate Gaussian random linear transformations, we show that as long as
the neural network width satisfies m “ Ω(krκ2), with high probability, GD can converge to
the global minimum up to an -error within Opκ logp1{qq iterations, where k, r are the output
dimension and the rank of training data matrix X respectively, and κ “ }X}22{σr2 pXq denotes
the condition number of the covariance matrix of the training data. Compared with previous
convergence results for training deep linear networks from Du & Hu (2019), our condition on
the neural network width is independent of the neural network depth L, and is strictly better by a
factor of OpLκq.
•	Using the same Gaussian random linear transformations, we also establish the convergence guar-
antee of SGD for training deep linear ResNets. We show that if the neural network width satisfies
m “ Ω (krκ2 log2 (1∕e)∙ n2{B2), with constant probability, SGD can converge to the global min-
imum UP to an e-error within Or(K2e—1 log(1∕e)∙ n∕B) iterations, where n is the training sample
size and Bis the minibatch size of stochastic gradient. This is the first global convergence rate of
SGD for training deep linear networks. Moreover, when the global minimum of the training loss
is 0, we prove that SGD can further achieve linear rate of global convergence, and the condition
on the neural network width does not depend on the target error e.
As alluded to above, we analyze networks with d inputs, k outputs, and memax{d, k} nodes
in each hidden layer. Linear transformations that are fixed throughout training map the inputs to
the first hidden layer, and the last hidden layer to the outputs. We prove that our bounds hold with
high probability when these input and output transformations are randomly generated by Gaussian
distributions. If, instead, the input transformation simply copies the inputs onto the first d compo-
2
Published as a conference paper at ICLR 2020
nents of the first hidden layer, and the output transformation takes the first k components of the last
hidden layer, then our analysis does not provide a guarantee. There is a good reason for this: a slight
modification of a lower bound argument from Bartlett et al. (2019) demonstrates that GD may fail
to converge in this case. However, we describe a similarly simple, deterministic, choice of input and
output transformations such that wide enough networks always converge. The resulting condition on
the network width is weaker than that for Gaussian random transformations, and thus improves on
the corresponding convergence guarantee for linear networks, which, in addition to requiring wider
networks, only hold with high probability for random transformations.
1.1	Additional related work
In addition to what we discussed above, a large bunch of work focusing on the optimization of
neural networks with nonlinear activation functions has emerged. We will briefly review them in
this subsection.
It is widely believed that the training loss landscape of nonlinear neural networks is highly noncon-
vex and nonsmooth (e.g., neural networks with ReLU/LeakyReLU activation), thus it is fundamen-
tally difficult to characterize the optimization trajectory and convergence performance of GD and
SGD. Some early work (Andoni et al., 2014; Daniely, 2017) showed that wide enough (polynomial
in sample size n) neural networks trained by GD/SGD can learn a class of continuous functions (e.g.,
polynomial functions) in polynomial time. However, those works only consider training some of the
neural network weights rather than all of them (e.g., the input and output layers) 1. In addition,
a series of papers investigated the convergence of gradient descent for training shallow networks
(typically 2-layer networks) under certain assumptions on the training data and initialization scheme
(Tian, 2017; Du et al., 2018b; Brutzkus et al., 2018; Zhong et al., 2017; Li & Yuan, 2017; Zhang
et al., 2018). However, the assumptions made in these works are rather strong and not consistent
with practice. For example, Tian (2017); Du et al. (2018b); Zhong et al. (2017); Li & Yuan (2017);
Zhang et al. (2018) assumed that the label of each training data is generated by a teacher network,
which has the same architecture as the learned network. Brutzkus et al. (2018) assumed that the
training data is linearly separable. Li & Liang (2018) addressed this drawback; they proved that
for two-layer ReLU network with cross-entropy loss, as long as the neural network is sufficiently
wide, under mild assumptions on the training data SGD with commonly-used Gaussian random
initialization can achieve nearly zero expected error. Du et al. (2018c) proved the similar results
of GD for training two-layer ReLU networks with square loss. Beyond shallow neural networks,
Allen-Zhu et al. (2019); Du et al. (2019); Zou et al. (2019) generalized the global convergence re-
sults to multi-layer over-parameterized ReLU networks. Chizat et al. (2019) showed that training
over-parameterized neural networks actually belongs to a so-called “lazy training” regime, in which
the model behaves like its linearization around the initialization. Furthermore, the parameter scaling
is more essential than over-paramterization to make the model learning within the “lazy training”
regime. Along this line of research, several follow up works have been conducted. Oymak &
Soltanolkotabi (2019); Zou & Gu (2019); Su & Yang (2019); Kawaguchi & Huang (2019) improved
the convergence rate and over-parameterization condition for both shallow and deep networks. Arora
et al. (2019b) showed that training a sufficiently wide deep neural network is almost equivalent to
kernel regression using neural tangent kernel (NTK), proposed in Jacot et al. (2018). Allen-Zhu
et al. (2019); Du et al. (2019); Zhang et al. (2019) proved the global convergence for training deep
ReLU ResNets. Frei et al. (2019) proved the convergence of GD for training deep ReLU ResNets
under an over-parameterization condition that is only logarithmic in the depth of the network, which
partially explains why deep residual networks are preferable to fully connected ones. However, all
the results in Allen-Zhu et al. (2019); Du et al. (2019); Zhang et al. (2019); Frei et al. (2019) require
a very stringent condition on the network width, which typically has a high-degree polynomial de-
pendence on the training sample size n. Besides, the results in Allen-Zhu et al. (2019); Zhang et al.
(2019) also require that all data points are separated by a positive distance and have unit norm. As
shown in Du & Hu (2019) and will be proved in this paper, for deep linear (residual) networks, there
is no assumption on the training data, and the condition on the network width is significantly milder,
which is independent of the sample size n. While achieving a stronger result for linear networks
than for nonlinear ones is not surprising, we believe that our analysis, conducted in the idealized
deep linear case, can provide useful insights to understand optimization in the nonlinear case.
1In Daniely (2017), the weight changes in all hidden layers make negligible contribution to the final output,
thus can be approximately treated as only training the output layer.
3
Published as a conference paper at ICLR 2020
Two concurrent works analyze gradient descent applied to deep linear (residual) networks (Hu et al.,
2020; Wu et al., 2019). Hu et al. (2020) consider deep linear networks with orthogonal initialization,
and Wu et al. (2019) consider zero initialization on the last layer and identity initialization for the
rest of the layers, which are similar to our setting. However, there are several differences between
their work and ours. One major difference is that Hu et al. (2020) and Wu et al. (2019) only prove
global convergence for GD, but our results cover both GD and SGD. In addition, Hu et al. (2020)
focuses on proving the global convergence of GD for sufficiently wide networks, while we provide
a generic condition on the input and output linear transformations for ensuring global convergence.
Wu et al. (2019) assumes whitened data and proves a OpL3 logp1{qq bound on the number of
iterations required for GD to converge, where we establish a Oplogp1{qq2 bound.
1.2	Notation.
We use lower case, lower case bold face, and upper case bold face letters to denote scalars, vectors
and matrices respectively. For a positive integer, we denote the set t1, . . . , ku by rks. Given a vector
x, We use ∣∣x}2 to denote its '2 norm. We use N(μ, σ2) to denote the Gaussian distribution with
mean μ and variance σ2. Given a matrix X, we denote }X}f, }X}2 and }X}2,8 as its Frobenious
norm, spectral norm and `2,8 norm (maximum `2 norm over its columns), respectively. In addition,
we denote by σminpXq, σmaxpXq and σr pXq the smallest, largest and r-th largest singular values
of X respectively. For a square matrix A, we denote by λmin pAq and λmax pAq the smallest and
largest eigenvalues of A respectively. For two sequences {ak}心。and {bk}『，we say a+ “ Opbkq
if ak ≤ Cibk for some absolute constant Ci, and use ak “ Ω(bk) if ak》C2bk for some absolute
constant C2. Except the target error e, we use O(∙)and Ω(∙) to hide the logarithmic factors in O(∙)
and Ω(∙) respectively.
2	Problem Setup
Model. In this work, we consider deep linear ResNets defined as follows:
fw(x) “ B(I ' Wl) ... (I ' Wi)Ax,
where X P Rd is the input, fw(x) P Rk is the corresponding output, A P Rm'd, B P RkXm denote
the weight matrices of input and output layers respectively, and Wi,..., WL P RmXm denote the
weight matrices of all hidden layers. The formulation of ResNets in our paper is different from that
in Hardt & Ma (2016); Bartlett et al. (2019), where the hidden layers have the same width as the
input and output layers. In our formulation, we allow the hidden layers to be wider by choosing the
dimensions of A and B appropriately.
Loss Function. Let t(xi, yi)ui“i,...,n be the training dataset, X “ (xi, . . . , xn)P RdXn be the
input data matrix and Y “(yi,..., Yn) P RkXn be the corresponding output label matrix. We
assume the data matrix X is of rank r, where r can be smaller than d. Let W “ tWi, . . . , WLu be
the collection of weight matrices of all hidden layers. For an example (x, y), we consider the square
loss defined by
'(W；χ, V) = 2}fwpx) ´ y}2.
Then the training loss over the training dataset takes the following form
n1
L(W) := ∑'(W; Xi, yi) = 5}B(I + WL)∙∙∙(I + Wi)AX ´ Y}F.
i“i	2
Algorithm. Similar to Allen-Zhu et al. (2019); Zhang et al. (2019), we consider algorithms that
only train the weights W for hidden layers while leaving the input and output weights A and B
unchanged throughout training. For hidden weights, we follow the similar idea in Bartlett et al.
(2019) and adopt zero initialization (which is equivalent to identity initialization for standard linear
network). We would also like to point out that at the initialization, all the hidden layers automatically
satisfy the so-called balancedness condition (Arora et al., 2018; 2019a; Du et al., 2018a). The
optimization algorithms, including GD and SGD, are summarized in Algorithm 1.
2Considering whitened data immediately gives κ “ 1.
4
Published as a conference paper at ICLR 2020
Algorithm 1 (Stochastic) Gradient descent with zero initialization
1:	input: Training data txi, yiuiPrns , step size η, total number of iterations T, minibatch size B,
input and output weight matrices A and B.
2:	initialization: For all l P rLs, each entry of weight matrix Wlp0q is initialized as 0.
__________________________________Gradient Descent______________________________________
3:	for t “ 0, . . . , T ´ 1 do
4:	wpt'1q “ Wptq ´ ηVwlL(Wptq q forall l P 固
5:	end for
6:	output: WpTq
______________________________Stochastic Gradient Descent_______________________________
7:	for t “ 0, . . . , T ´ 1 do
8:	Uniformly sample a subset Bptq of size B from training data without replacement.
9:	For all' P [L], compute the stochastic gradient Gptq “ Bn XipB(t)VWl'(Wptq; Xi, Niq
10:	For all l P [L], Wpt'ιq “ Wptq ´ ηGptq
11:	end for
12:	output: tWptqut“0,...,T
3	Main Theory
Itis clear that the expressive power of deep linear ResNets is identical to that of simple linear model,
which implies that the global minima of deep linear ResNets cannot be smaller than that of linear
model. Therefore, our focus is to show that GD/SGD can converge to a point W* with
L(W*)“ min 1 }ΘX ´ Y}F,
θpRkxd 2
which is exactly the global minimum of the linear regression problem. It what follows, we will show
that with appropriate input and output transformations, both GD and SGD can converge to the global
minimum.
3.1	Convergence guarantee of gradient descent
The following theorem establishes the global convergence of GD for training deep linear ResNets.
Theorem 3.1. There are absolute constants C and C1 such that, if the input and output weight
matrices satisfy
σmm(A)σmm(B) ,C 冈2(L(W⑼)´ L(W*)) 1/2
}A}2}B}2	"	σr(X)
and the step size satisfies
1
η ≤ C「
L}A}2}B}2}X}2 ∙ (√LW0)+}A}2}B}2}X}2),
then for all iterates of GD in Algorithm 1, it holds that
L(Wp)) ´ L(W*q ≤
I ´ 〃--耳皿依方宗在可蟾迷）
)∙ (L(W⑼)一L(W*)).
(
e
Remark 3.2. Theorem 3.1 can imply the convergence result in Bartlett et al. (2019). Specifically, in
order to turn into the setting considered in Bartlett et al. (2019), we choose m “ d “ k,A“I,B “
I, L(W*q “ 0 and XXJ “ I. Then it can be easily observed that the condition in Theorem 3.1
becomes L(Wp0q) — L(W*) ≤ C´2. This implies that the global convergence can be established
as long as L(Wp0qq ´ L(W*q is smaller than some constant, which is equivalent to the condition
proved in Bartlett et al. (2019).
In general, L(Wp0q) — L(W*) can be large and thus the setting considered in Bartlett et al. (2019)
may not be able to guarantee global convergence. Therefore, it is natural to ask in which setting
5
Published as a conference paper at ICLR 2020
the condition on A and B in Theorem 3.1 can be satisfied. Here we provide one possible choice
which is commonly used in practice (another viable choices can be found in Section 4). We use
Gaussian random input and output transformations, i.e., each entry in A is independently generated
from Np0, 1{mq and each entry in B is generated from Np0, 1{kq. Based on this choice of transfor-
mations, we have the following proposition that characterizes the quantity of the largest and smallest
singular values of A and B, and the training loss at the initialization (i.e., LpWp0qq). The following
proposition is proved in Section A.2.
Proposition 3.3. In Algorithm 1, if each entry in A is independently generated from N p0, α2q and
each entry in B is independently generated from N (0, β2), then if meC ∙(d ' k ' log(1∕δ)) for
some absolute constant C, with probability at least 1 ´ δ, it holds that
σmin(A)= C(a?m), σmaχ(A)= O(a?mq, Gmin(B)= Ω (e?m), σmaχ(B)= O(e?m),
and L(Wpoq) W O'α2β2kmlog(n∕δ)}X}F ' }Y}F).
Then based on Theorem 3.1 and Proposition 3.3, we provide the following corollary, proved in
Section 3.4, which shows that GD is able to achieve global convergence if the neural network is
wide enough.
Corollary 3.4. Suppose }Y}F = O(}X}F ). Then using Gaussian random input and output
transformations in Proposition 3.3 with α = β = 1, if the neural network width satisfies
m = Ω(max{krκ2 log(n∕δ), k ' d ' log(1∕δ)}) then, with probability at least 1 一 δ, the output of
GD in Algorithm 1 achieves training loss at most L(W*) ' e within T = O(K log(1∕e)) iterations,
where κ = }X}22∕σr2(X) denotes the condition number of the covariance matrix of training data.
Remark 3.5. For standard deep linear networks, Du & Hu (2019) proved that GD with Gaussian
random initialization can converge to a e-suboptimal global minima within T = Ω(κlog(1∕e))
iterations if the neural network width satisfies m = O(Lkrκ3 ` d). In stark contrast, training
deep linear ResNets achieves the same convergence rate as training deep linear networks and linear
regression, while the condition on the neural network width is strictly milder than that for training
standard deep linear networks by a factor of O(Lκ). This improvement may in part validate the
empirical advantage of deep ResNets.
3.2	Convergence guarantee of stochastic gradient descent
The following theorem establishes the global convergence of SGD for training deep linear ResNets.
Theorem 3.6. There are absolute constants C, Ci and C2, such for any 0 < δ ≤ 1∕6 and e > 0, if
the input and output weight matrices satisfy
σiLn(A)σ2min(B)
}A}2}B}2
> C
n}X}2 ∙lθg(L(Wp0q)∕e)
BGr (χ)
• √L(Wpoq),
and the step size and maximum iteration number are set as
Bσimin(A)σimin(B)σr(X)	i "	e____________________B______________
Ln}A}2}B}4}X}2	∣}X}2,8L(W*), n}X}2 • log(T∕δ) log(L(Wpoq)∕e)
T = C2 •
1
ηLσLn(A)σmnin(B)σr(X)
• log
L(Wpoq) ´ L(W*)
e
(
,
then with probability3 at least 1∕2 (with respect to the random choices of mini batches), SGD in
Algorithm 1 can find a network that achieves training loss at most L(W*) ` e.
By combining Theorem 3.6 and Proposition 3.3, we can show that as long as the neural network is
wide enough, SGD can achieve global convergence. Specifically, we provide the condition on the
neural network width and the iteration complexity of SGD in the following corollary.
Corollary 3.7. Suppose }Y}F = O(}X}F ). Then using Gaussian random input and output trans-
formations in Proposition 3.3 with ɑ = β = 1, for sufficiently small e > 0, if the neural network
width satisfies m = Ω'krκ2 log2(1∕e) • n2 ∕B2 ` d , with constant probability, SGD in Algorithm 1
can find a point that achieves training loss at most L(W*)' e within T = O(κ2e-1 log(1∕e) • n∕B)
iterations.
3One can boost this probability to 1 ´ δ by independently running logp1{δq copies of SGD in Algorithm 1.
6
Published as a conference paper at ICLR 2020
From Corollaries 3.7 and 3.4, we can see that compared with the convergence guarantee of GD, the
condition on the neural network width for SGD is worse by a factor of Orpn2 log2 p1{q{B2q and the
iteration complexity is higher by a factor of Or(Ke´1 ∙ n∕B). This is because for SGD, its trajectory
length contains high uncertainty, and thus we need stronger conditions on the neural network in
order to fully control it.
We further consider the special case that L(W*) “ 0, which implies that there exists a ground
truth matrix Φ such that for each training data point (xi , yiq we have yi “ Φxi. In this case, we
have the following theorem, which shows that SGD can attain a linear rate to converge to the global
minimum.
Theorem 3.8. There are absolute constants C, and Ci such that for any 0 < δ < 1, if the input and
output weight matrices satisfy
“minpA-min(Bq > c. n}X}2 . bL(wP。))
}A}2}B}2	>	Bσr(X) V ( q,
and the step size is set as
JlV C	B2σmin(A)σmin(B)σrpχq
η'	1 Ln2}A}4}B}2}X}4 ∙log(T∕δ),
for some maximum iteration number T, then with probability at least 1 ´ δ, the following holds for
all t V T,
L(Wptq) V 2L(Wp0q) . 1 ´
ηLσm2 in (A)σm2 in (B)σr2 (X)
e
Y
Similarly, using Gaussian random transformations in Proposition 3.3, we show that SGD can achieve
global convergence for wide enough deep linear ResNets in the following corollary.
Corollary 3.9. Suppose }Y}F “ O(}X}F ). Then using Gaussian random transformations in
Proposition 3.3 with α “ β “ 1, for any e V O B}X}22,8∕(n}X}22) , if the neural network width
satisfies m “ rr'krκ2 ∙ n2∕B2 ' d), with high probability, SGD in Algorithm 1 can find a network
that achieves training loss at most e within T “ Or'κ2 log(1∕e) ∙ n2∕B2) iterations.
4	Discussion on Different Input and Output Linear
Transformations
In this section, we will discuss several different choices of linear transformations at input and output
layers and their effects to the convergence performance. For simplicity, we will only consider the
condition for GD.
As we stated in Subsection 3.1, GD converges if the input and output weight matrices A and B
σminA⅛I(B) > CJXXr (L(WpOq) ´L(W*q)1/2.	"D
Then it is interesting to figure out what kind of choice of A and B can satisfy this condition. In
Proposition 3.3, we showed that Gaussian random transformations (i.e., each entry of A and B is
generated from certain Gaussian distribution) satisfy this condition with high probability, so that GD
converges. Here we will discuss the following two other transformations.
Identity transformations. We first consider the transformations that A “ [L^d, 0dxpm—d)SJ and
B“∙∖∕m∕k . rIk^k, 0k^pm一kqS. ^^hiCh is equivalent to the setting in Bartlett et al. (2019) ^^hen
m “ k “ d. Then it is clear that
σmin(B) “ σmaχ(B) “ √m∕k and σmin(A) “ σmaχ(A) “ 1.
Now let us consider L(WpOq). By our choices ofB and A and zero initialization on weight matrices
in hidden layers, in the case that d “ k, we have
L(Wp0q) “ 1 }BAX ´ Y}F “ 1>am∕kX ´ Y>F.
7
Published as a conference paper at ICLR 2020
We remark that >am/kX - Y>F{2 could be as big as 1 'm}X}2F {k ` }Y}F) (for example, when
X and Y are orthogonal). Then plugging these results into (4.1), the condition on A and B becomes
amk > C J⅛ ∙ ^2 (m}χ}F/k + }γ}F) ´ L(W*)) 1/2》C JXXy ∙ CmXF,
σr pXq	2	σr pXq	2k
where the second inequality is due to the fact that L(W*) ≤ }Y}F/2. Then it is clear if }X}f》
λ∕2/C, the above inequality cannot be satisfied for any choice of m, since it will be cancelled out on
both sides of the inequality. Therefore, in such cases, our bound does not guarantee that GD achieves
global convergence. Thus, it is consistent with the non-convergence results in (Bartlett et al., 2019).
Note that replacing the scaling factor ʌ/m/k in the definition of B with any other function of d, k
and m would not help.
Modified identity transformations. In fact, we show that a different type of identity transforma-
tions of A and B can satisfy the condition (4.1). Here we provide one such example. Assuming
m 2 d + k, we can construct two sets Si, S2 U [m] satisfying |S1| “ d, ∣S21 “ k and Si X S2 “ H.
Let S1 “ ti1, . . . , idu and S2 “ tj1, . . . , jku. Then we construct matrices A and B as follows:
A " 1	pi, jq “ pij , jq	B " α pi, jq “ pi, jiq
Aij “	0 otherwise Bij “	0 otherwise
where α is a parameter which will be specified later. In this way, it can be verified that BA “ 0,
σminpAq “ σmaxpAq “ 1, and σminpBq “ σmaxpBq “ α. Thus it is clear that the initial training
loss satisfies LpWp0qq “}Y}2F/2. Then plugging these results into (4.1), the condition on A and
B can be rewritten as
α 2 C∙ j⅛ ∙ (MF/2 ´ L(W*))1/2.
σr2 pXq	F
The R.H.S. of the above inequality does not depend on α, which implies that we can choose suf-
ficiently large α to make this inequality hold. Thus, GD can be guaranteed to achieve the global
convergence. Moreover, it is worth noting that using modified identity transformation, a neural net-
work with m “ d + k suffices to guarantee the global convergence of GD. We further remark that
similar analysis can be extended to SGD.
5	Experiments
In this section, we conduct various experiments to verify our theory on synthetic data, including i)
comparison between different input and output transformations and ii) comparison between training
deep linear ResNets and standard linear networks.
5.1 Different input and output transformations
To validate our theory, we performed simple experiment on 10-d synthetic data. Specifically, we
randomly generate X P R5 * * * * 10'1000 from a standard normal distribution and set Y “ ´X + 0.1 ∙ E,
where each entry in E is independently generated from standard normal distribution. Consider
10-hidden-layer linear ResNets, we apply three input and output transformations including identity
transformations, modified identity transformations and random transformations. We evaluate the
convergence performances for these three choices of transformations and report the results in Figures
1(a)-1(b), where we consider two cases m “ 40 and m “ 200. It can be clearly observed that
gradient descent with identity initialization gets stuck, but gradient descent with modified identity
initialization or random initialization converges well. This verifies our theory. It can be also observed
that modified identity initialization can lead to slightly faster convergence rate as its initial training
loss can be smaller. In fact, with identity transformations in this setting, only the first 10 entries of
the m hidden variables in each layer ever take a non-zero value, so that, no matter how large m is,
effectively, m “ 10, and the lower bound of Bartlett et al. (2019) applies.
5.2 Comparison with standard deep linear networks
Then we compare the convergence performances with that of training standard deep linear networks.
Specifically, we adopt the same training data generated in Section 5.1 and consider training L-
hidden-layer neural network with fixed width m. The convergence results are displayed in Figures
8
Published as a conference paper at ICLR 2020
Figure 1: (a)-(b):Convergence performances for three input and output transformations on a 10-
hidden-layer linear ResNets. (c)-(d) Comparison between the convergence performances of training
deep linear ResNets with zero initialization on hidden weights and standard deep linear network with
Gaussian random initialization on hidden weights, where the input and output weights are generated
by random initialization, and remain fixed throughout the training.
(d) m “ 200
1(c)-1(d), where we consider different choices of L. For training linear ResNets, we found that the
convergence performances are quite similar for different L, thus we only plot the convergence result
for the largest one (e.g., L “ 20 for m “ 40 and L “ 100 for m “ 200). However, it can be
observed that for training standard linear networks, the convergence performance becomes worse as
the depth increases. This is consistent with the theory as our condition on the neural network width
is m “ Opkrκ2q (please refer to Corollary 3.4), which has no dependency in L, while the condition
for training standard linear network is m “ OpLkrκ3q (Du & Hu, 2019), which is linear in L.
6 Conclusion
In this paper, we proved the global convergence of GD and SGD for training deep linear ResNets
with square loss. More specifically, we considered fixed linear transformations at both input and
output layers, and proved that under certain conditions on the transformations, GD and SGD with
zero initialization on all hidden weights can converge to the global minimum. In addition, we fur-
ther proved that when specializing to appropriate Gaussian random linear transformations, GD and
SGD can converge as long as the neural network is wide enough. Compared with the convergence
results of GD for training standard deep linear networks, our condition on the neural network width
is strictly milder. Our analysis can be generalized to prove similar results for different loss func-
tions such as cross-entropy loss, and can potentially provide meaningful insights to the convergence
analysis of deep non-linear ResNets.
Acknowledgement
We thank the anonymous reviewers and area chair for their helpful comments. This work was initi-
ated when Q. Gu and P. Long attended the summer program on the Foundations of Deep Learning
at the Simons Institute for the Theory of Computing. D. Zou and Q. Gu were sponsored in part
by the National Science Foundation CAREER Award IIS-1906169, BIGDATA IIS-1855099, and
Salesforce Deep Learning Research Award. The views and conclusions contained in this paper are
those of the authors and should not be interpreted as representing any funding agencies.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with
neural networks. In International Conference on Machine Learning, pp. 1908-1916, 2014.
Sanjeev Arora, Nadav Cohen, and Elad E Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In 35th International Conference on Machine Learning,
ICML 2018, pp. 372-389, 2018.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient de-
scent for deep linear neural networks. In International Conference on Learning Representations,
2019a.
9
Published as a conference paper at ICLR 2020
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On ex-
act computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, 2019b.
Peter L Bartlett, David P Helmbold, and Philip M Long. Gradient descent with identity initializa-
tion efficiently learns positive-definite linear transformations by deep residual networks. Neural
computation, 31(3):477-502, 2019.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, 2019.
Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pp. 2422-2430, 2017.
Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
International Conference on Machine Learning, pp. 1655-1664, 2019.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Sys-
tems, pp. 384-395, 2018a.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? In
International Conference on Learning Representations, 2018b.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018c.
Yuguang Fang, Kenneth A Loparo, and Xiangbo Feng. Inequalities for the trace of matrix product.
IEEE Transactions on Automatic Control, 39(12):2489-2490, 1994.
Daniel C Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
In International Conference on Learning Representations, 2016.
Spencer Frei, Yuan Cao, and Quanquan Gu. Algorithm-dependent generalization bounds for over-
parameterized deep residual networks. In Advances in Neural Information Processing Systems,
2019.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in op-
timizing deep linear networks. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=rkgqN1SYvr.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In ICLR,
2019.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
10
Published as a conference paper at ICLR 2020
Kenji Kawaguchi and Jiaoyang Huang. Gradient descent finds global minima for generalizable deep
neural networks of practical sizes. arXiv preprint arXiv:1908.02419, 2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradi-
ent descent on structured data. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems,pp. 8168-8177, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU acti-
vation. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pp. 597-607. Curran Associates Inc., 2017.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint
arXiv:1702.08580, 2017.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-
gence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674, 2019.
Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear
neural networks. arXiv preprint arXiv:1809.08587, 2018.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approxi-
mation prospective. arXiv preprint arXiv:1905.10826, 2019.
Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 3404-3413. JMLR. org, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Lei Wu, Qingcan Wang, and Chao Ma. Global convergence of gradient descent for deep linear
residual networks. arXiv preprint arXiv:1911.00645, 2019.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks.
In International Conference on Learning Representations, 2018.
Huishuai Zhang, Da Yu, Wei Chen, and Tie-Yan Liu. Training over-parameterized deep resnet is
almost as easy as training a two-layer network. arXiv preprint arXiv:1903.07120, 2019.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer ReLU
networks via gradient descent. arXiv preprint arXiv:1806.07808, 2018.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 4140-4149. JMLR. org, 2017.
Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and land-
scape properties. 2018.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. Machine Learning Journal, 2019.
A Proof of Main Theorems
We first provide the following lemma which proves upper and lower bounds on }VwιL(W)}F
when W is staying inside a certain region. Its proof is in Section B.1.
11
Published as a conference paper at ICLR 2020
Lemma A.1. For any weight matrices satisfying maxip[L] } Wi ∣∣2 ≤ 0.5/L, it holds that,
}VwlL(W)}F > eσmm(A)σmm(B)σr(X)(L(W) ´ L(W*)),
}VwlL(W)}F ≤ 2e}A}2}B}2}X}2(L(W L(W*))
}Vwl'(W;xi,yi)}F ≤ 2e}A}2}B}2}xi}2'(W;Xi,yi).
In addition, the stochastic gradient Gi in Algorithm 1 satisfies
}G1}F ≤
where B is the minibatch size.
2en2}A}22}B}22}X}22
B2
LpWq,
The gradient lower bound can be also interpreted as the Polyak-LojasieWicz condition, which is
essential to the linear convergence rate. The gradient upper bound is crucial to bound the trajectory
length, since this lemma requires that maxip[L] } Wi} ≤ 0.5/L.
The following lemma proves the smoothness property of the training loss function L(W) when W
is staying inside a certain region. Its proof is in Section B.2.
Lemma A.2. For any two collections of weight matrices, denoted by W “ tW1, . . . , WLu and
W = {Wι,..., Wl}, satisfying maxip[L] }Wi}f, maxip[L] } Wi }f ≤ 0.5/L that, it holds that
L
.∙^-z .	.	.	X~~5 .	.	. ∙^-z
L(W) ´ L(W) ≤ XxVWlL(W), Wi´ Wiy
i“1
L _
+ L}A}2}B}2}x}2'a2eL(W) + 0.5e}A}2}B}2}x}2) £ }W i ´ Wι}F.
i“1
Based on these two lemmas, we are able to complete the proof of all theorems, which are provided
as follows.
A.1 Proof of Theorem 3.1
Proofof Theorem 3.1. In order to simplify the proof, we use the short-hand notations Xa, μA, Xb
and μB to denote ∣∣A}2, σmin(A), }B}2 and σmin(B) respectively. Specifically, we rewrite the
condition on A and B as follows
μAμB > w¾X}2 ∙ (L(Wpoq) ´ L(W*))1{2.
XAXB	σr2 (X)
We prove the theorem by induction on the update number s, using the following two-part inductive
hypothesis:
⑴ maxip[L] }Wpsq}F ≤ 0.5/L,
(ii) L(WPsq) ´ L(W*) ≤(1 ´
ηLμAμBσ2pχq
)∙ (L(WPOq) ´ L(W*)).
e
First, it can be easily verified that this holds for s “ 0. Now, assume that the inductive hypothesis
holds for S < t.
Induction for Part (i): We first prove that maxip[L] } Wptq }f ≤ 0.5/L. By triangle inequality and
the update rule of gradient descent, we have
t´1
}Wptq}F ≤ £ η}VwlL(Wpsq)}f
s“0
t´1
≤ η £ WXAλB}X|2 ∙ (L(Wpsq) ´ L(W*))1{2
s“0
t´ 1 /
wWηλaλB }X|2 ∙ (L(Wpoq) ´ L(W*))1{2 ∙ £
s“0
1 ´ nLμAμB σ2(χ)

e
12
Published as a conference paper at ICLR 2020
where the second inequality follows from Lemma A.1, and the third inequality follows from the
inductive hypothesis. Since √1 ´ x ≤ 1 一 x/2 for any X P [0,1], We further have
t´ 1 /
}wPtq}F ≤ EnXAXB }X}2 ∙ (L(Wpoq) ´ L(W*))1/2 ∙ ∑
s“0
1 ´ nL〃A〃Bσ2(χ)
2e
)s
W %λλ2}X}2 ' (L(W⑼)t(W*))"2.
LμAμBσr (X)
Under the condition that μAμB/(XaXb)22?8e3}X}2 (L(Wpoq) — L(W*)) 1{2{σr (X), it can be
readily verified that }Wlptq}F W 0.5{L. Since this holds for all l P rLs, we have proved Part (i) of
the inductive step, i.e., maxlPrLs }Wlptq}F W 0.5/L.
Induction for Part (ii): Now we prove Part (ii) of the inductive step, bounding the improvement in
the objective function. Note that we have already shown that Wptq satisfies maxlPrLs }Wlptq}F W
0.5/L, thus by Lemma A.2 we have
L
L(Wptq) ≤ L(WptT)) ´ η E >VwιL(Wptτ))>F
l“1
' η2LλA λB ∣∣X}2 ∙ (b eL(WptT)) ' 0∙5eλAλB}χ}2) ∙ X ∣∣vWι L(WptT))}F,
l“1
where We use the fact that WPtq — WPtTq “ —ηVw,L(WplTq). Note that L(WptT)) W
L(Wp0q) and the step size is set to be
1
η	/	,
2LXaXb}X∣2 ∙ (√eL(Wpoq) + ONXaXb}X|2))
so that we have
L
L(Wptq) — L(WptTq) W -2 X >VwιL(WptTq)>F
2 l“1
V	ηLμAμB σr(X)
W
(L(WptTq)- l(w*)),
e
where the second inequality is by Lemma A.1. Applying the inductive hypothesis, we get
L(Wptq) — L(W*) W
W
^1 - ηLμAμBσr(X)) ∙ (L(WptTq) - L(W*))
^1 —如可σr(X))' ∙ (L(WpOq) - L(W*)),
(A.1)
which completes the proof of the inductive step of Part (ii). Thus we are able to complete the proof.
□
A.2 Proof of Proposition 3.3
Proof of Proposition 3.3. We prove the bounds on the singular values and initial training loss sepa-
rately.
Bounds on the singular values: Specifically, we set the neural network width as
m2 100 ∙ (amax{d, k} + /2log(12∕δ))2
By Corollary 5.35 in Vershynin (2010), we know that for a matrix U P Rd1'd2 (died2
with entries independently generated by standard normal distribution, with probability at least
1 — 2 exp(—t2/2), its singular values satisfy
√d1 - √d2 - t W σmin(U) W σmaχ(U) W √d? + √d2 + t.
13
Published as a conference paper at ICLR 2020
Based on our constructions of A and B, We know that each entry of 1B and 1A follows standard
Gaussian distribution. Therefore, set t “ 2∕log(12∕δ) and apply union bound, with probability at
least 1 ´ δ{3, the following holds,
a'?m ´? ´	2aiog(12∕δ))	≤	σmin (A)	W	σmax(A) W α'?m ' ?d ' 2aiog(12∕δ))
β '?m ´? ´	2aiog(12∕δ))	W	σmin(B)	W	σmaχ(B)W e'?m '? ' 2aiog(12∕δ)),
where we use the facts that σmin(κU) “ κσmin(U) and σmax(κU) “ κσmax(U) for any scalar κ
and matrix U. Then applying our choice of m, we have with probability at least 1 ´ δ∕3,
0.9a?m W σmin(A)	W	σmaχ(A)	W	1.1a?m	and	0.9e?m	W	σmin(B) W	σmaχ(B) W 1.ie?m.
This completes the proof of the bounds on the singular values of A and B.
Bounds on the initial training loss: The proof in this part is similar to the proof of Proposition 6.5
in Du & Hu (2019). Since we apply zero initialization on all hidden layers, by Young’s inequality,
we have the following for any (x, y),
'(Wp0q; x, y) “ 2}BAx ´ y}2 WllBAX}2 + }y}2.	(A.2)
Since each entry of B is generated from N (0, β2), conditioned on A, each entry of BAx is dis-
tributed according to N(0,β2∣∣Ax∣∣2), so }晨逑 follows a Xk distribution. Applying a standard
tail bound for χ2k distribution, we have, with probability at least 1 ´ δ1,
}BAx12
}Ax}2
W β2k(1 + 2√log(1∕δ1)∕k + 2log(1∕δ1)).
Note that by our bounds of the singular values, if m2100 ∙ (amax{d, k} + /2log(8∕δ))2, we
have with probability at least 1 — δ∕3, 1 Ag W 1.1a?m, thus, it follows that with probability at
least 1 ´ δ1 ´ δ,
∣∣BAx12 W 1.21α2β2km“1 + 2ʌ∕log(1∕δ1) + 2log(1∕δ1)‰}x}2.
Then by union bound, it is evident that with probability 1 — nδ1 — δ∕3,
n
}BAX∣F “ £ IlBAXi∣2 W 1.21α2β2km[l + 2aiog(1∕δ1) + 2log(1∕δ1)‰}X∣F.
i“1
Set δ1 “ δ∕(3n), suppose log(1∕δ1)21, we have with probability at least 1 — 2δ∕3,
L(Wp0q) “ 1 IlBAX — Y∣F W }BAX∣F + }Y∣F W 6.05α2β2kmlog(2n∕δ)}X∣F + }Y∣F.
This completes the proof of the bounds on the initial training loss.
Applying a union bound on these two parts, we are able to complete the proof.
□
A.3 Proof of Corollary 3.4
Proof of Corollary 3.4. Recall the condition in Theorem 3.1:
σ2⅛( A)σ21m (B)
1A 图 B∣2
> C σXX) ∙ 'L(Wpoq) — L(W*))1{2.
By Proposition 3.3, we know that, with probability 1 — δ,
σiLn( A)σmm(B)
1A 图 B∣2
“ Θ(m),
JX2 ∙ 'L(Wpoq) — L(W*))1/2
“O
(√km log(n∕δ) + 1)}X}f }X}2
)
(A.3)
σr(X)
14
Published as a conference paper at ICLR 2020
Note that }X}f ≤ "}X}2, thus the condition (A.3) can be satisfied if m “ Ω(krκ2 log(n∕δ))
where κ “ }X}22{σr2 pXq.
Theorem 3.1 implies that L(Wptq) — L(W*) ≤ e after T = O (ηLσ2. (Aqσ2. (Bqσ:pxq log ɪ)
iterations. Plugging in the value ofη, we get
T=O
}A}2}B}2}X}2: ' LWp0)+}A}2}B}2}X}2)
σmin(A)σmm(B)σr(X)
loge).
By Proposition 3.3, we have
T=O
}A}2}B}2}X}2 :	km log(n∕δ)}X}F + }A}2}B}2}X}2)
σmin(A)σmm(B)σr(X)
loge)
=O
}X}2，	km log(n∕δ)}X}F ' m}X}2)
mσr2(X)
=O
}X}2，(√kr log(n∕δ)∕m}χ}2 + }X}2)
σr2(X)
loge)
loge)
(
(
(
=O (Klog；)
for m = Ω(kr log(n∕δ)), completing the proof.
□
A.4 Proof of Theorem 3.6
Proofof Theorem 3.6. The guarantee is already achieved by Wp0q if eeL(Wp0q) — L(W*), so
We may assume without loss of generality that e < L(Wp0q) — L(W*).
Similar to the proof of Theorem 3.1, we use the short-hand notations λa, μA, Xb and μB to denote
}A}2, σmin(A), }B}2 and σmin(B) respectively. Then we rewrite the condition on A and B, and
our choices of η and T as follows
〃A〃B >
XaXb /
√8e3n}X}2 ∙ log(L(Wp0q)∕e1)
Bσr2(X)
b〃a〃bσr(χq	min
6e3LnλAλB }X}2
∙ j22(Wp0q)
e1
log2 (2)B
}X}2,sL(W*) , 3n}X}2 ∙ log(T∕δ) log(L(Wp0q)∕e1)
T=
ηLμAμBσr(χ)
∙ log
L(Wp0q) — L(W*)
e1
η ≤
e
,
,
where we set e1 = e∕3 for the purpose of the proof.
We first prove the convergence guarantees on expectation, and then apply the Markov inequality.
For SGD, our guarantee is not made on the last iterate but the best one. Define Et to be the event
that there is no s ≤ t such that L(Wptq) — L(W*) ≤ e1. If 1(Et) = 0, then there is an iterate Ws
with s ≤ t that achieves training loss within e1 of optimal.
Similar to the proof of Theorem 3.1, we prove the theorem by induction on the update number s,
using the following inductive hypothesis: either 1(Es) = 0 or the following three inequalities hold,
⑴ maxiρ[L] }w(s)}f ≤ ®.飞皿}X}2 ∙ √2L(Wp0q)∙
(ii)	E[(L(Wpsq) — L(W*))‰ ≤ ´l — ηLμAμBσ2pxq)s ∙ (L(Wp0q) — L(W*))
(iii)	L(Wpsq) ≤ 2L(Wp0q),
where the expectation in Part (ii) is with respect to all of the random choices of minibatches. Clearly,
if 1(Es) = 0, we have already finished the proof since there is an iterate that achieves training loss
15
Published as a conference paper at ICLR 2020
within e1 of optimal. Recalling that e < L(Wp°q) — L(W*), it is easy to verify that the inductive
hypothesis holds when s “ 0.
For the inductive step, We will prove that if the inductive hypothesis holds for S < t, then it holds
for s “ t. When 1(Eji) “ 0, then 1(Et) is also 0 and we are done. Therefore, the remaining
part is to prove the inductive hypothesis for S “ t under the assumption that 1(Et—i) “ 1, which
implies that (i), (ii) and (iii) hold for all S ≤ t — 1. For Parts (i) and (ii), we will directly prove that
the corresponding two inequalities hold. For Part (iii), we will prove that either this inequality holds
or 1(Etq “ 0.
Induction for Part (i): As we mentioned, this part will be proved under the assumption
1(Etτ) “ 1. Besides, combining Part (i) for S “ t — 1 and our choice of η and T implies that
maxip[L] }Wpt―1q }f ≤ 0.5/L. Then by triangle inequality, we have the following for ∣∣Wptq }f,
}wpt)}F w}Wpjq}F + η∣Gpjq}F.
By Lemma A.1, we have
∣GptTq}F & 岳"“y }X}2 . bL(W(tτ)q.
Then we have
}Wptq}F ≤ (}Wptτq}F + η}Gptτq}F)
& }WPtτq}f + WnnλBλB}X}2 , JL(WptTq).	(a.4)
By Part (iii) for S “ t — 1, we know that L(WptTqq W 2L(Wp0q). Then by Part (i) for S “ t — 1,
it is evident that
}Wpptq }f ≤ Hη%λ }X}2 . b2L(WP0qq..	(A.5)
This completes the proof of the inductive step of Part (i).
Induction for Part (ii): As we previously mentioned, we will prove this part under the as-
sumption 1(E-ι) “ 1. Thus, as mentioned earlier, the inductive hypothesis implies that
maxpp[L] }Wptτq}f ≤ 0.5/L. By Part (i) for S “ t, which has been verified in (A.5), it can
be proved that maxlPrLs ∣Wlptq ∣F W 0.5{L, then we have the following by Lemma A.2,
L
L(WPtqq — L(WPtTqq W —η ENWlL(WPtTq), GptTqD
l“1
__________ L
+ η2LλAλB }X∣2 . (JeL(Wptτqq + 0.5eλaλB}X}2)，£ }Gpτq}F.
l“1
(A.6)
By our condition on A and B, it is easy to verify that
2∙√2eTL(Wp0q)
∣X∣2
λAλB > μAμB >
λAλB
Then by Part (iii) for S “ t — 1 (A.6) yields
L
L
L(Wptq) — L(WPtTqqW -η £〈Vwl L(WPtTqq, Gp 一)〉+ eη2LλAλB }X12 W IGp fF.
l“1
l“1
(A.7)
Taking expectation conditioning on WPt´1) gives
L
E[L(Wptqq∣Wptτq‰ — L(WPtTqq W —ηX >VwlL(WptTqq∣F
l“1
L
+ eη2LλAλB}X}2 EE[}Gpjq}FWPjq‰. (A.8)
l“1
16
Published as a conference paper at ICLR 2020
Note that, for i sampled uniformly from {1,…，n}, the expectation E[}Gpt-1q}F|Wpt—1qS can be
upper bounded by
E[}Gp jq}F WptTqs= E[}Gptτq ´ VWlL(WPtTq)}F∣wptτq‰ + 弓L(WPt7)2
n2
≤ 五E[}Vwl'(Wptτq;xi,%)}FWPtTqs + }VwlL(WPjq)}F.
B
(A.9)
By Lemma A.1, we have
E[}Vwl '(Wptτq; xi, %)}F WPtTq s≤ 2eλA λj3 E[}xi }2'(Wptτq; x” yi)∣Wptτqs
≤ 2eλAλB £ }xi}2'(Wptτq;Xi,Vi
n	i“1
& 2eλAλB}X}2,8L(Wptτq)
n
Plugging the above inequality into (A.9) and (A.8), we get
E[L(Wptq)Wptτq‰ ´ L(WPtTq)
L
w´n∑ >VwlL(WPTq)}F
l“1
+ eη2LλAλB }X}2 S ( 22ZB }XB，8L(Wjqq + }VwlL(Wptτq)}F )
Recalling that η ≤ 1/(6eLλAλB }X(2), We have
E[L(Wptq)Wp jq‰ ´ L(WPjqq W ´56η Z >VwlL(WPtTq)}F
l“1
2e2η2L2nλAλ4B }X}2 IX^gL(WptTq)
B
By Lemma A.1, We have
L
∑ }VwlL(Wptτq)}F > 2e―1L〃A〃Bσr(X)(L(Wptτqq ´ L(W*q).
l“1
If We set
≤	B〃A〃B蟾㈤
η ≤ 6e3LnλAλB}X12}X}2,g
(A.10)
(A.11)
then (A.10) yields
E[L(Wptqq∣Wptτq‰ ´ L(WPtTqq
≤ ´
5ηLμAμBσr(χq
3e
(L(WPtTqq ´ L(W*q)
+
2e2η2L2nλAλB}X}2}X}2,8(L(WPtTqq ´ l(w*))
+
B
2e2η2L2nλAλB }X}2}X}2,8 L(W*q
≤ ´
4ηLμAμBσr(xq
3e
B
(L(WPtTqq ´ L(w*q) +
2e2 η2L2nλAλB }X}2}X}2,8L(W*q
BL2
. (A.12)
Define
f⅛3,	2e2η2L2 nλAλB }X12}X12,8L(W*q
-Z-
3e
B
17
Published as a conference paper at ICLR 2020
rearranging (A.12) further gives
E[L(Wptq)∣Wptτ)‰ ´ L(W*) ≤ (1 ´ ηγo) ∙ (L(Wpt)) ´ L(W*)) + η2γι.	(A.13)
Therefore, setting the step size as
γ01
η ≤ ^j- =
4γ1
BμAμB σ2pXq
--Z---：--：--------- ------.
6e3LnλAλB 区图区0 L(W*),
1
,
We further have
E[L(Wptq) ´ L(W*)∣Wptτq‰ ≤ “(1 ´ ηγo) ∙ rL(Wptτq) ´ L(W*)S + η2γι‰
W (1 ´ 3ηγo/4) ∙ rL(Wptτq) ´ L(W*)],	(A.14)
Where the second inequality is by (A.13) and the last inequality is by the fact that We assume
1(Etτ) “ 1, which implies that L(WptT)) — l(w*)ee>24γm/γo. Further taking expectation
over WPtTq, we get
E[L(Wptq) — L(W*)‰ W (1 — 3ηγo∕4)∙ E[L(Wptτq) — L(W*)‰
W (1 — 3ηγ0{4)t ∙ (L(Wp0)) — L(W*)),
where the second inequality follows from Part (ii) for s “ t — 1 and the assumption that 1(E0) “ 1.
Plugging the definition ofγ0, we are able to complete the proof of the inductive step of Part (ii).
Induction for Part (iii): Recalling that for this part, we are going to prove that either
L(Wpt)) W 2L(Wp0)) or 1(Et) “ 0, which is equivalent to L(Wpt)) ∙ 1(Et) W 2L(Wp0))
since L(Wp0)) and L(Wpt)) are both positive. We will prove this by martingale inequality.
Let Ft “ σ{Wp°q,…，Wptq} be a σ-algebra, and F “ {Ft}t>ι be a filtration. We first
prove that ErL(WPt)) 1(Et)∣Ftτ] W L(WPtT)) i(e1). Apparently, this inequality holds
when 1(E-ι) “ 0 since both sides will be zero. Then if 1(Etτ) “ 1, by (A.14) we have
ErL(W(tq)∣WptτqS W L(Wptτq) since L(W*) is the globalminimum. Therefore,
ErL(WPt)) I(Et)FtT, WPj),1(EtT) = 1] W ErL(WPt))∣Ftτ, 1(EtT) = 1
W L(WPtT)).
Combining these two cases, by Jensen’s inequality, we further have
E[log (L(WPt)) 1(Et))∣Ftτ‰ W log (E[L(WPt)) 1(Et)∣Ftτ])
W log(L(WPI) 1(Etτi)),
which implies that {log (L(WPt))∙ 1(Et))Ut⅛o is a super-martingale. Then we will upper bound the
martingale difference log (L(WPt)) ∙ 1(Et)) — log (L(WPtT)) . 1(Ej1)). Clearly this quantity
would be zero if 1(E-1) = 0. Then if 1(E1) = 1, by (A.7) we have
LL
L(WPt))W L(WPtT))+ ηE }VwιL(W(tT))IlF恒尸)}尸 + eη2LλAλB}X}2 E 恒尸)底.
l“1	l“1
By Part (i) for s = t — 1, Lemma A.1, we further have
L(Wptq) W	1 +
W 1+
2eηLnλAλB }X}2 + 2e2n2η2L2λAλB }X}2
B
3eηnLλAλB }X}2) L(WPt´i))
B2
)L(WptTq)
B
Where the second inequality folloWs from the choice of η that
B
η W 2enLλAλB }X}2 .
Using the fact that 1(Et) ≤ 1 and 1(Etτ) “ 1, We further have
log (L(Wptq) ∙ 1(Et)) ≤ log (L(Wptτq) ∙ 1(Eτ)) +
3eηLnλ2Aλ2B }X}22
B
(A.15)
18
Published as a conference paper at ICLR 2020
which also holds for the case 1(E1) = 0. Recall that {log (LpWPtq) ∙ 1(Et))}t>o is a SUPer-
martingale, thus by one-side Azuma’s inequality, we have with probability at least 1 ´ δ1,
log (LpWptq) ∙ 1(Et)) ≤ log (L(Wp0q)) + 3enLn"B"B}X}2 , atlog(1∕δ1).
Setting δ1 “ δ∕T, using the fact that t ≤ T and leveraging our choice of T and η, We have with
Probability at least 1 ´ δ∕T,
?Tn “___________回逛_______________
3e√2log(δ∕T)LnλAλB }X}2 ,
which imPlies that
L(WPtq) 1(Et) ≤ exp ”log (L(Wp°q)) + log(2)] ≤ 2L(Wp0q).	(A.16)
This comPletes the Proof of the inductive steP of Part (iii).
Note that this result holds with Probability at least 1 ´ δ∕T. Thus aPPlying union bound over all
iterates {Wptq}t=o,...,τ yields that all induction arguments hold for all t ≤ T with probability at
least 1 ´ δ.
Moreover, plugging our choice of T and η into Part (ii) gives
E[L(Wptq) ´ L(W*)‰ ≤ e1.
By Markov inequality, we further have with probability at least 2∕3, it holds that rL(WPT q) ´
L(W*)]∙1(Et) ≤ 3e1 “ e. Therefore, by union bound (together with the high probability arguments
of (A.16)) and assuming δ < 1∕6, we have with probability at least 2∕3 — δ21∕2, one of the iterates
of SGD can achieve training loss within e1 of optimal. This completes the proof.	□
A.5 Proof of Corollary 3.7
Proof of Corollary 3.7. Recall the condition in Theorem 3.6:
σiLn(A)σ21in(B)
}A}2}B}2
n}X}2 ∙log(L(Wp0q)∕e)
Bσr (X)
∙ √L(Wp0q),
(A.17)
> C ∙
Then plugging in the results in Proposition 3.3 and the fact that }X(f ≤ ∕}X}2, we obtain that
condition (A.17) can be satisfied ifm “ O(krκ2 log2 (1∕e) ∙ B∕n).
In addition, consider sufficiently small e such that e ≤ Or(B}X}2,8∕(n}X}2)),then and use the fact
that }X}2,8 ≤ ∣∣X}2 we have η “ θ(kBe∕(LmnK}X}2)) based on the results in Proposition 3.3.
Then in order to achieve e-suboptimal training loss, the iteration complexity is
T “ -%…X) log (L(WPOq ´ L(W*))) “ O~ log(1∕e)∙ n∕B).
ηLσm2 inσm2 in (B)σr2 (X)	e
This completes the proof.	□
A.6 Proof of Theorem 3.8
Proof of Theorem 3.8. Similar to the proof of Theorem 3.6, we set the neural network width and
step size as follows,
μ⅛ > 4⅛e⅛ ∙ b2L(W⅞
λAλB	Bσr2 (X)
<	log(2)B2μAμB (B)σr(X)
η、54e3Ln2λAλB }X}2 ∙ log(T∕δ),
where λa, μA, Xb and μB denote }A}2, σmin(A), }B}2 and σmin(B) respectively.
Different from the proof of Theorem 3.6, the convergence guarantee established in this regime is
made on the last iterate of SGD, rather than the best one. Besides, we will prove the theorem by
induction on the update parameter t, using the following two-part inductive hypothesis:
19
Published as a conference paper at ICLR 2020
⑴ maxip[L] }Wptq}F ≤ 0.5/L
(ii) Lpwptqq ≤ 2Lpw(0q) ∙(
1 ´ sηLμAμB σ2(χ)
)s.
e
Induction for Part (i) We first prove that maxip[L] }Wptq}F ≤ 0.5/L. By triangle inequality and
the update rule of SGD, we have
t´1
}wptq}F ≤ £ η}Gι}F
s“0
≤ η £1 ?'2B}X12 'Lpw(s)q ´ L(w*))1/2
s“0	B
≤
≤
√2eηnλAλB }X}2
B
"nMB }X}2
BLμAμB σrpχq
t´1
∙ 'L(Wp0qq ´ L(W*))1/2 ∙£ (
s“0
∙ 'L(Wp0qq ´ L(W*q)1/2
1 ´ ηLμAμB σ∕(χ)
2e
J
where the second inequality is by Lemma A.1,the third inequality follows from Part (ii) for all S < t
and the fact that p1 ´ xq1{2 ≤ 1 ´ x{2 for all x P r0, 1s. Then applying our choice ofm implies that
}wlpt)}F≤0.5{L.
Induction for Part (ii) Similar to Part (ii) and (iii) of the induction step in the proof of Theorem 3.6,
we first prove the convergence in expectation, and then use Azuma’s inequality to get the high-
probability based results. It can be simply verfied that
λAλB > μAμB >
λAλB
4√2e3n}X}2 ∙log(L(W(0)q/e)
Bσr2(Xq
∙ J22(W(0))》
2√2e-1L(W(0qq
}χ}2
≤	log(2)B2〃A〃B (B)σr(χ)	≤	BμAμB σ2(χ)
n ≤ 96e3Ln2λAλB }χ}4 ∙ log(T/δ) ≤ 6e3LnXA』B 1'图区}2,8
Thus, we can leverage (A.12) and obtain
E[L(W(t))∣W(t-1)‰ ´ L(W(t-1)) ≤ ´
4ηLμAμBσr(χ)
3e
L(W(tτq),
where we use the fact that L(W*q “ 0. Then by Jensen’s inequality, we have
E[log'L(W(tqq)∣W(tτq‰ ≤ log'L(WptT))) + log (
1´ 4nL〃A〃Bσr(χ)
3e
≤ log 'L(Wpτq)) ´
4ηLμA μB σ2(χ)
3e
,
where the second inequality is by log(1 + x) ≤ x. Then similar to the proof of Theorem 3.6, we
are going to apply martingale inequality to prove this part. Let Ft “ σtW(0) , ∙ ∙ ∙ , W(t) u be a
σ-algebra, and F “ {Ft}t>ι be a filtration, the above inequality implies that
E[log(L(W(tq))∣Ftτ‰ +
4tηLμAμB σ2(χ)
3e
≤ Iog(L(W(tτq)) +
4(t — DnLμAμB σ2(χ)
3e
(A.18)
which implies that {log(L(W(t))) + 4tηLμAμBσr(χ){(3e)} is a super-martingale. Besides, by
(A.15), we can obtain
log (L(W(tq)) ≤ log (L(W(t-1q)) +
3eηLnλ2A λ2B}χ}22
which implies that
log(L(W(t))) +
4tnLμAμB σ2(χ)
3e
≤ log(L(W(tT))) +
4(t — 1qnLμAμBσr(χ) ` 4enLnλAλB }χ}
3e
B
2
2
B
20
Published as a conference paper at ICLR 2020
where We again use the fact that log(1 ' X)W x. Thus, by the one-sided Azuma,s inequality We
have with probability at least 1 ´ δ1 that
log(L(W(t))) ≤ log(L(W(0))) ´ AMR?σ2pxq + 4eηLnλAλB}χ}2 - a%三/乃
W lθg (L(W(O))) ´ tηLμAμQBσrpxq + 96e3ηLn2λAλB }χ}2 log(1∕δ1)
'	'	e	e	B2μAμB σr(xq
W log (L(W(0q)) ´ tηLμAμσr(X) + log(2),
where the second inequality follows from the fact that —at + bʌ/t W b2∕a, and the last inequality is
by our choice of η that
log(2)B2μAμB ⅛∣(X
η W 96e3Ln2λAλB }X}4 . log(1∕δ1).
Then it is clear that with probability at least 1 — δ1,
L(Wpt)) W 2L(Wp0q) . exp ^ — tηLμAμBσr(X))	(A.19)
which completes the induction for Part (ii).
Similar to the proof of Theorem 3.6, (A.19) holds with probability at least 1 — δ1 for a given t. Then
we can set δ1 “ δ∕T and apply union bound such that with probability at least 1 — δ, (A.19) holds
for all t W T. This completes the proof.	□
A.7 Proof of Corollary 3.9
Proof of Corollary 3.9. Recall the condition in Theorem 3.8:
σ21in(A)σ21in(B)	C n}X}2	∕rr w(0 八
WW》C.瓯西∙ VL(Wp)),	⑶2。)
Then plugging in the results in Proposition 3.3 and the fact that }X}f W ∕}X}2, we obtain that
condition (A.17) can be satisfied ifm “ O krκ2 . B∕n .
In addition, it can be computed that η “ O kB2∕(Lmn2κ}X}22) based on the results in Proposition
3.3. Then in order to achieve -suboptimal training loss, the iteration complexity is
T “ L2	2 %2(X)log ( L(WpO))´ L(W*)) “ O'κ2 log(1∕e).后©).
ηLσm2 inσm2 in(B)σr2 (X)
This completes the proof.	□
B	Proofs of Technical Lemmas
B.1 Proof of Lemma A.1
We first note the following useful lemmas.
Lemma B.1 (Claim B.1 in Du & Hu (2019)). Define Φ “ arg minΘPRk^d }ΘX — Y}F, then for
any U P RkXd it holds that
}ux ´ Y}F “}ux ´ φx}F + }φx ´ Y}F∙
Lemma B.2 (Theorem 1 in Fang et al. (1994)). Let U, V P RdXd be two positive definite matrices,
then it holds that
λmin(U)Tr(V) W Tr(UV) W λmax(U)Tr(V).
The following lemma is proved in Section B.3.
21
Published as a conference paper at ICLR 2020
Lemma B.3. Let U P RdXr be a rank-r matrix. Then for any V P RrXk, it holds that
σmin(U)}V}F ≤ }UV}F ≤ σmaχ(U)}V}F.
Proof of Lemma A.1. Proof of gradient lower bound: We first prove the gradient lower bound. Let
U = B(I' Wl) ... (I' Wι) A, by Lemma B.1 and the definition of L(W*), We know that there
exist a matrix Φ P Rkxd SUch that
L(W) = 1 }UX ´ ΦX}F + L(W*).	(B.1)
Therefore, based on the assumption that maxip[L] } W? }f ≤ 0.5/L, we have
}VwιL(W)}F = >[B(I + Wl)∙∙∙ (I + Wi'i)‰J'UX ´ ΦX)[(I + W1)…AX‰J>；
>	σmm((I + Wl)∙∙∙ (I + Wi'i)) ∙ σmm((I + WlT)…(I + Wi))
∙ }BJ(U ´ Φ)XXJAJ}2F
>	'1 ´ 0.5∕L)2l∙}Bj(U ´ Φ)XXjAj}F,
where the last inequality follows from the fact that σmin(I + Wl)21 — } Wl ∣∣221 — } Wl }f》
1	´ 0.5∕L. Applying Lemma B.2, we get
}Bj(U — Φ)XXjAj}F = Tr(BBJ (U — Φ)XXj AJAXXJ(U — Φ)j)
)λmin(BBJ) ∙ Tr(AJAXXJ(U — Φ)j(U — Φ)XXj)
》λmin(BBJ)∙ λmin(AJA)∙}(U — Φ)XXJ}F .
Note that X is of r-rank, thus there exists a full-rank matrix Xr P RdXr such that XXJ = XXj .
Thus we have
}(U — Φ)X}F = Tr((U — Φ)XXj(U — Φ)J) = Tr((U — Φ)X XJ(U — Φ)J) = >(U — Φ)X>F.
(B.2)
Therefore,
∣(U — Φ)XXJ∣2F = ››(U — Φ)Xr Xr J››2F
= Tr (U — Φ)Xr Xr JXr Xr J (U — Φ)J
,〜-I-〜.	...	.〜..C
> λmin(XJX)∙}(U — Φ)X}F
= 2σr2(X) ∙ (L(W) — L(W*)),	(B.3)
where the inequality follows from Lemma B.2 and the last equality follows from (B.2), (B.1) and
the fact that λma(XJX) = λr (XXj) = σr(X). Note that we assume d,k ≤ m and d ≤ n. Thus
it follows that λmin (BBJ) = σm2 in (B) and λmin (AJA) = σm2 in (A). Then putting everything
together, we can obtain
}VwιL(W)}F > 2σmm(B)σmm(A)σr(X)(1 — 0.5∕L)2l∙(L(W — L(W*)).
Then using the inequality (1 — 0.5/L)2L-22e´1, we are able to complete the proof of gradient
lower bound.
Proof of gradient upper bound: The gradient upper bound can be proved in a similar way. Specif-
ically, Lemma B.3 implies
}VwιL(W)}F = >[B(I + Wl)∙∙∙ (I + Wl'i)‰J(UX — ΦX)[(I + wl´i)…AX‰J>；
≤ σmax((I + WL) ∙ ∙ ∙ (I + Wl'iqq ∙ σmax((I + Wl— 1)∙ ∙ ∙ (I + WI))
∙ ∣BJ (U — Φ)XXJAJ ∣2F
≤ σ21aχ((I + Wl)∙∙∙ (I + wl`i)) ∙ σmaχ((I + Wl—i)…(I + Wi))
∙ ∣B∣22∣A∣22 ∙ ∣(U — Φ)XXJ∣2F
≤(1 + 0.5/L)2L-2}B}2}A}2 ∙}(u — φ)xxj}F,
22
Published as a conference paper at ICLR 2020
where the last inequality is by the assumption that maxip[L] } Wi }f ≤ 0.5/L. By (B.3), We have
}(U ´ Φ)XXJ}F “>pu ´ Φ)(XXJ)1/2(XXJ)1/2>F
≤ λmaχ(XXJ) ∙}(U ´ Φ)(XXJ)1{2}F
“ λmaχ(XXJ)∙}(U ´ Φ)X}F
“ 2}X}2 ∙ (L(W) ´ L(W*)),
where the inequality is by Lemma B.3 and the second equality is by (B.2). Therefore, combining
the above results yields
}VwιL(W)}F ≤ 2σmaχ(B)σmaχ(A)}X}2(1 + 0.5/L产´2(L(W ´ L(W*)).
Using the inequality (1 + 0.5/L)2L一2 ≤ (1 + 0.5∕L)2l ≤ e, we are able to complete the proof of
gradient upper bound.
Proof of the upper bound of }Vw%'(W; Xi, yi)}F: Let U = B(I + WL)…(I + Wι)A, we
have
Vwi'(W;Xi,%) = [B(I + Wl)∙∙∙ (I + W+)] J(UXi ´ %)“(I + WlTq …AXJJ.
Therefore, by Lemma B.3, we have
}vwi'(W； χi, yi)}F ≤ σmax(α+wl)∙ (I+wl`i)) ∙ σmax(α+Wlt>∙∙(i+WI))
∙ }BJ(UXi ´ yi)XiAJ}2F
≤(1 + 0.5/L产´2 ∙}B}2}A}2}Xi}2 ∙ }UXi ´ yi}F
≤ 2e}A}2}B}2 }Xi}2'(W; Xi, yj
where the last inequality is by the fact that (1 + 0.5/L)2L-2 ≤ e.
Proof of the upper bound of stochastic gradient: Define by B the set of training data points used
to compute the stochastic gradient, then define by X and Y the stacking of {Xi}ipB and {yi}ipB
respectively. Let U “ B(I + WL) ∙ ∙ ∙ (I + W1)A, the minibatch stochastic gradient takes form
Gl “ Bn X VWl '(w； Xi, Vi
iPB
nJ	J
“B [B(I + Wl)∙∙∙ (I + Wl'i)‰ 1 (UX ´ Y)[(I + Wlτ)∙∙∙ AX‰ J .
Then by Lemma B.3, we have
n2
IIGl}F ≤ Bσmax((I + WL) ∙(I + Wl'l)) ∙ σmaχ((I + Wl´l)∙∙∙(I + WI))
∙ }BJ(UX ´ Y)XJAj}F
≤ B ∙(i + 0.5∕l)2l∙ ∙ }b}2}a}2}X}2 ∙}UX ´ Y}F
≤ B}b}2}a}2}X}2 ∙}UX´ Y}F.
where the second inequality is by the assumptions that maxlp[L] }Wl∣F ≤ 0.5/L, and the last
inequality follows from the the fact that (1 + 0.5/L)2L一2 ≤ (1 + 0.5∕L)2l ≤ e. Note that X and
Y are constructed by stacking B columns from X and Y respectively, thus we have }X}2 ≤ }X}2
and }UX — Y}F ≤ }UX — Y}F = 2L(W). Thenitfollowsthat
2en2
}Gl}F & -Br}b}2}a}2}x}2 ∙ l(w).
This completes the proof of the upper bound of stochastic gradient.	□
23
Published as a conference paper at ICLR 2020
B.2 Proof of Lemma A.2
ProofofLemma A.2. Let U “ B(I ' WL)…(I ' WQA and U “ B(I ' W l)∙∙∙(I ' W ι)A
and ∆ “ U ´ U. We have
l(W) ´ L(W) = 2'}UX ´ Y}F — }UX ´ Y}F)
“ 1 '}(u + ∆)X ´ y}F — }UX ´ y}F )
“1 '}UX ´ Y + ∆X}F — }UX ´ Y}F)
“1 (}2〈ux ´ Y, ∆Xy + }∆X}F)
“@UX ´ Y, 'U ´ U)XD + 2>(U ´ U)X}F.	(B.4)
We begin by working on the first term. Let V = (I + Wl)∙∙∙ (I + Wι) and V = (I + WL)…(I +
rx 、	~ _ 一 ,r`e 、 .	. .	_	.	_	_ ——.ι	___、
W ι),so that U—U = B(V—V)A. Breaking down the effect of transforming V = nj=L(I+Wj)
1
into V = j1“L(I + Wj) into the effects of replacing one layer at a time, we get
V - V = W «( ∏ (I + Wj ))(fl (I + Wj))-(ll[ (I + Wj))( fl (I + W j ))ff
l“1	j“L	j ∖j=l	j	∖j=L	j ∖j=-1
and, for each l, pulling out a common factor of (口；=L(I + Wj)) ^∏ι∑l (I + Wj)) gives
L
V — V = A(I + Wl)∙∙∙ (I + Wi'i)(W ι — Wi )(I + W I—ι)…(I + W ι)
L
=W (I + Wl)∙∙ ∙ (I + Wi'i)(W ι — Wi )(I + Wi—i)∙∙∙(I + Wι)
l=1
loooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooon
y	-
V1
L
+ W (I + Wl)∙∙ ∙ (I + Wι+ι)(W i — Wi)
loooooooooooooooooooooooooooooooooooooooooooooooooon
y	-
V2
• [(I + Wi—i)∙∙∙(I + W1) —(I + Wi—i)∙∙∙(I + Wι)‰ .	(B.5)
loooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooon
y	-
V2
24
Published as a conference paper at ICLR 2020
The first term V1 satisfies
xUX ´ Y, BV1AXy
)
“ UX ´ Y, B
E a
l“1
. . . ∙^-z	.. . .
+ WL)…(I + Wι+ι)(W i´ Wl)(I + W-ι)∙∙∙(I + Wi)
AX
)
L
“ Σ(UX ´ Y, B(I + WL)…(I + Wl'i)(Wl ´ Wl)(I + wl´i
l“1
)∙∙∙(I + W1)AX)
L
“ ∑ Tr((UX ´ Y)JB(I + Wl)∙∙∙ (I + Wl'i)(Wl ´ Wl)(I + Wlτ)…(I + WI)AX)
l“1
L
“ ∑ Tr((I + Wlτ)…(I + Wι)AX(UX ´ Y)JB(I + WL)…(I + Wl'i)(Wl ´ Wl))
l“1
L
“ ∑ @[B(I + Wl)∙∙∙ (I + Wl'i)SJ(UX ´ Y)r(I + W1)…AX]J, Wl ´ WlD
l“1
L
“ ∑〈VWi L(W), Wl ´ Wly,	(B.6)
l“1
where the first equality is by the definition of V1. Now we focus on the second term V2 of (B.5),
L
V2 “ ∑(I + Wl) ∙∙∙ (I + Wl'i)(Wl ´ Wl)
l“1
l´1
∙ X (I + Wl´i)∙∙∙(i + Ws'l)(Ws ´ Ws)(I + Ws´i)∙∙∙(i + W 1).
s“1
C 11∙	,1	, IlXVT Il IIXTr7- Il - CL / T 。	11 7 Γ T 1	1	, •	1	1 • ,	1
Recalling that }Wl }f, }Wl }f ≤ 0.5/L for all l P [L], by triangle inequality We have
}V2}F ≤ (1 + 0.5∕L)l ∙	£	}W l ´ Wl}F ∙ }W s´ Ws}F
l,sp[L] : l>s
≤ (1 + 0.5∕L)l ∙
^∑L }Wl ´ Wl}f),
where we use the fact that Xl,sp[L] ： l>s alas ≤ Xl,sp[L] alas “(Xl al)2 holds for all aι,...,aL2
0. Therefore, the folloWing holds regarding V2 :
XUX ´ Y, BV2 AX〉≤ }UX ´ Y}f}BV2 AX}f
& a2L(W)}B}2}A}2}X}2}V2}F
≤
WaLW)}B}2}A}2}X}2 ^ A }W l ´ Wl }f )
(B.7)
where the third inequality follows from the fact that (1 + 0.5∕L)l =(1 + 0.5∕L)l ≤ ?e. Next,
we are going to upper bound the second term of (B.4): ɪ }(U — U)X}F. Note that, since }(U —
U)X}2F “ }B(Vr ´ V)AX}2F ≤ }A}22}B}22}X}22}Vr ´ V}2F, it suffices to bound the norm }Vr ´
V}F . By (B.5), we have
›L	›
}V — V}f = ∑(I + Wl)∙∙∙ (I + Wl'i)(Wl — Wl)(I + Wlτ)∙∙∙(I + Wi)
›l“1	›F
L
≤ (1 + 0.5∕L)l ∑ }Wl — Wl}f.	(B.8)
25
Published as a conference paper at ICLR 2020
Plugging (B.6), (B.7) and (B.8) into (B.4), we have
L(Wq ´ L(W)
“ @UX ´ Y, B(Vι + V2)X〉+ 2>b'V ´ V) AX}F
L
≤ ∑XVwiL(W), W1— WI)
l“1
+ }A}2}B}2}X}2' √2eL(W)+ 0.5e}A}2}B}2}X}2)
(£ }W 1 ´ Wi}f)
L
≤ ∑〈VWiL(W), W1 ´ WTy
1“1
L
+ L}A}2}B}2}X}2'a2eL(W) + 0.5e}A}2}B}2}X}2) £ }W 1 ´ W1}F,
1“1
(B.9)
□
where the last inequality is by Jesen’s inequality. This completes the proof.
B.3 Proof of Lemma B.3
Proof of Lemma B.3. Note that we have
}UV}2F “ Tr(UVVJUJ) “ Tr(UJUVVJ).
By Lemma B.2, it is clear that
λmin(UJU)Tr(VVJ) ≤ Tr(UJUVVJ)W λmaχ(UJU)Tr(VVj).
Since U P RdXr is of r-rank, thus We have λmin(UJU) “。/^(⑺.Then applying the facts that
λmax(UJU) “ σm2 ax(U) and Tr(VVJ) “ }V}2F, we are able to complete the proof.
□
26