Published as a conference paper at ICLR 2020
On the Need For Topology-Aware Generative
Models for Manifold-based Defenses
Uyeong Jang
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI, USA
wjang@cs.wisc.edu
Susmit Jha
Computer Science Laboratory
SRI International
Menlo Park, CA, USA
susmit.jha@sri.com
Somesh Jha
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI, USA
XaiPient
Princeton, NJ, USA
jha@cs.wisc.edu
Ab stract
Machine-learning (ML) algorithms or models, especially deep neural networks
(DNNs), have shown significant promise in several areas. However, researchers
have recently demonstrated that ML algorithms, especially DNNs, are vulnerable to
adversarial examples (slightly perturbed samples that cause misclassification). The
existence of adversarial examples has hindered the deployment of ML algorithms in
safety-critical sectors, such as security. Several defenses for adversarial examples
exist in the literature. One of the important classes of defenses are manifold-
based defenses, where a sample is “pulled back” into the data manifold before
classifying. These defenses rely on the assumption that data lie in a manifold of
a lower dimension than the input space. These defenses use a generative model
to approximate the input distribution. In this paper, we investigate the following
question: do the generative models used in manifold-based defenses need to be
topology-aware? We suggest the answer is yes, and we provide theoretical and
empirical evidence to support our claim.
1	Introduction
Machine-learning (ML) algorithms, especially deep-neural networks (DNNs), have had resounding
success in several domains. However, adversarial examples have hindered their deployment in
safety-critical domains, such as autonomous driving and malware detection. Adversarial examples
are constructed by an adversary adding a small perturbation to a data-point so that it is misclassified.
Several algorithms for constructing adversarial examples exist in the literature (Biggio et al., 2013;
Szegedy et al., 2013; Goodfellow et al., 2014b; Kurakin et al., 2016a; Carlini & Wagner, 2017; Madry
et al., 2017; Papernot et al., 2017). Numerous defenses for adversarial examples also have been
explored (Kurakin et al., 2016b; Guo et al., 2017; Sinha et al., 2017; Song et al., 2017; Tramer et al.,
2017; Xie et al., 2017; Dhillon et al., 2018; Raghunathan et al., 2018; Cohen et al., 2019; Dubey et al.,
2019).
In this paper, we focus on “manifold-based” defenses (Ilyas et al., 2017; Samangouei et al., 2018). The
general idea in these defenses is to “pull back” the data point into the data manifold before classifica-
tion. These defenses leverage the fact that, in several domains, natural data lies in a low-dimensional
manifold (henceforth referred to as the manifold assumptions) (Zhu & Goldberg, 2009). The data
distribution and hence actual manifold that the natural data lies in is usually unknown, so these
defenses use a generative model to “approximate” the data distribution. Generative models attempt
to learn to generate data according to the underlying data distribution. (The input to a generative
model is usually random noise from a known distribution, such as Gaussian or uniform.) There are
1
Published as a conference paper at ICLR 2020
various types of generative models in the literature, such as variational autoencoder (VAE) (Kingma
& Welling, 2013), generative adversarial network (GAN) (Goodfellow et al., 2014a) and reversible
generative models, e.g., real-valued non-volume preserving transform (Real NVP) (Dinh et al., 2016).
This paper addresses the following question:
Do manifold-based defenses need to be aware of the topology of the underlying
data manifold?
In this paper, we suggest the answer to this question is yes. We demonstrate that if the generative
model does not capture the topology of the underlying manifold, it can adversely affect these defenses.
In these cases, the underlying generative model is being used as an approximation of the underlying
manifold. We believe this opens a rich avenue for future work on using topology-aware generative
models for defense to adversarial examples.
Contributions and Roadmap. We begin with a brief description of related work in Section 2.
Section 3 provides the requisite mathematical background. Our main theoretical results are provided
in Section 4. Informally, our result says that if the generative model is not topology-aware, it can
lead to a ”topological mismatch” between the distribution induced by the generative model and the
actual distribution. Section 5 describes our experimental verification of our theoretical results and
investigates their ramifications on a manifold-based defenses called Invert-and-Classify (INC) (Ilyas
et al., 2017; Samangouei et al., 2018).
2	Related work
2.1	Generative models
As a method for sampling high-dimensional data, generative models find applications in various
fields in applied math and engineering, e.g., image processing, reinforcement learning, etc. Methods
for learning data-generating distribution with neural networks include well-known examples of
Variational Autoencoders (VAEs) (Kingma & Welling, 2013) and variations of Generative Adversarial
Networks (GANs) (Goodfellow et al., 2014a; Radford et al., 2015; Zhao et al., 2016).
These generative models learn how to map latent variables into generated samples. The VAE is a
variational Bayesian approach, so it approximates a posterior distribution over latent vectors (given
training samples) by a simpler variational distribution. Similar to other variational Bayesian methods,
VAE tries to minimize the KUllback-Leibler divergence between the posterior distribution and the
variational distribution by minimizing the reconstruction error of the autoencoder. GANs represent
another approach to learning how to transform latent vectors into samples. Unlike other approaches,
the GAN learns the target distribution by training two networks - generator and discriminator -
simultaneously.
In addition to generating plausible samples, some generative models construct bijective relations
between latent vector and generated samples, so that the probability density of the generated sample
can be estimated. Due to their bijective nature, such generative models are called to be reversible.
Some examples are normalizing flow (Rezende & Mohamed, 2015), Masked Autoregressive Flow
(MAF) (Papamakarios et al., 2017), Real NVP (Dinh et al., 2016), and Glow (Kingma & Dhariwal,
2018).
2.2	Applications of generative models in adversarial machine learning
The DNN-based classifier has been shown to be vulnerable to adversarial attacks (Szegedy et al.,
2013; Goodfellow et al., 2014b; Moosavi-Dezfooli et al., 2016; Papernot et al., 2016; Madry et al.,
2017). Several hypothesis try explaining such vulnerability (Szegedy et al., 2013; Goodfellow et al.,
2014b; Tanay & Griffin, 2016; Feinman et al., 2017), and one explanation is that the adversarial
examples lie far away from the data manifold. This idea leads to defenses making use of the geometry
learned from the dataset - by projecting the input to the nearest point in the data manifold.
To learn a manifold from a given dataset, generative models can be exploited. The main idea is to
approximate the data-generating distribution with a generative model, to facilitate searching over
data manifold by searching over the space of latent vectors. The term Invert-and-Classfy (INC) was
coined to describe this type of defense (Ilyas et al., 2017), and different types of generative models
were tried to detect adversarial examples (Ilyas et al., 2017; Song et al., 2017; Samangouei et al.,
2018). Usually, the projection is done by searching the latent vector that minimizes the geometric
2
Published as a conference paper at ICLR 2020
distance (Ilyas et al., 2017; Samangouei et al., 2018). However, despite the promising theoretical
background, all of those methods are still vulnerable (Athalye et al., 2018; Ilyas et al., 2017).
3	Background
We formally describe data generation, based on the well-known manifold assumption; data lies close
to a manifold whose intrinsic dimension is much lower than that of the ambient space. In our model
of data generation, we provide a formal definition of data-generating manifold M on which the
data-generating distribution lies such that M conforms to the manifold assumption.
3.1	Requirements
Real-world data tends to be noisy, so the data does not easily correspond to an underlying manifold.
We first focus on an ideal case where data is generated solely from the manifold M without noise.
In the setting of classification with l labels, we consider manifolds M1, . . . , Ml ⊂ Rn that correspond
to the generation of data in each class i ∈ {1, . . . , l}, respectively. We assume those manifolds
are pair-wise disjoint, i.e., Mi ∩ Mj = 0 for any i = j.We set the data-generating manifold M as
the disjoint union of those manifolds, M = L∣i=ι Mi. We assume M to be a compact Riemannian
manifold with a volume measure dM induced by its Riemannian metric. When a density function
pM defined on M satisfies some requirements, it is possible to compute probabilities over M via
∫x∈M pM (x)dM(x). We call such M equipped with pM an dM as a data-generating manifold. We
refer to Appendix A and Appendix D.1 for details about definitions and requirements on pM .
In practice, data generation is affected by noise, so not all data lie on the data-generating manifold.
Therefore, we incorporate the noise as an artifact of data-generation and extend the density pM on M
to the density p on the entire Rn by assigning local noise densities on M. We consider a procedure
that (1) samples a point xo from M first, and (2) adds a noise vector n to get an observed point
X = Xo + n. Here, the noise n is a random vector sampled from a probability distribution, centered at
Xo, whose noise density function is Vxo, satisfying Vχ(n) = Vx(X — x) = PM (X∣ x。= x).
3.2	Extending density
When M is equipped with a density function pM and a measure dM that we can integrate over M,
we can compute the density after random noise is added as follows.
p(X) = ∫ ʌʃ Vx(X- x)p(x)dM(x)	(1)
Since νx(X - x) is a function on X when X is fixed, computing this integration can be viewed as the
computing expectation of a real-valued function defined on M . Computing such expectation has been
explored in Pennec (1999). A demonstrative example is provided in Appendix B, and this extension
is further discussed in Appendix D.2.
3.3	Generative models
A generative model tries to find a statistical model for joint density p(X, y) (Ng & Jordan, 2002).
We mainly discuss a specific type that learns a transform from one distribution DZ to another target
distribution DX. Commonly, a latent vector Z Z DZ is sampled from a simpler distribution, e.g.,
Gaussian, then a pre-trained deterministic function G maps to a sample X = G(z).
Specifically, we focus on reversible generative models to facilitate the comparison between the
density of generated samples and the target density. In this approach, the dimensions of latent vectors
are set to be the same as those of the samples to be generated. Also, for a given X, the density of X is
estimated by the change of variable formula (equation (2) in Section 5.1).
3.4	Invert and classify (INC) approach for robust classification
As the data-generating manifold M contains class-wise disjoint manifolds, there is a classifier f on
Rn separating these manifolds. If f separates the manifolds of M , any misclassified point should lie
out of M . Therefore, to change a correct classification near a manifold, any adversary would pull a
sample further out of the manifold. By projecting misclassified points to the nearest manifold, we
may expect the classification to be corrected by the projection. The INC method (Ilyas et al., 2017;
Samangouei et al., 2018) implements this using a generative model.
The main idea of INC is to invert the perturbed sample by projecting to the nearest point on the
data-generating manifold. Ideally, the data-generating manifold M is accessible. For any point (X, y)
with f (X) = y, out-of-manifold perturbation is reduced by projecting X to x* on M. The manifold M
3
Published as a conference paper at ICLR 2020
is unknown in practice. However, as M is the data-generating manifold of DX, a generative model G
for DX is trained to approximate M . Then, searching over M is replaced by searching over latent
vectors of G. More details about INC implementations are described in Section 5.1.
4	Topological properties of data from generative models
In this paper, we study the significance of differences in the topological properties of the latent
vector distribution and the target distribution in learning generative models. Initial information about
the topology of target distribution1 is crucial to the generative model performance. Specifically, if
there is a difference in the number of connected components in the superlevel set between the target
distribution and the distribution of the latent vector, then any continuous generative model G cannot
approximate the target distribution properly (irrespective of the training method). Due to the space
limit, all proofs are presented in Appendix C.
4.1	Topology of distributions based on superlevel sets
The data-generating manifold is a geometric shape that corresponds to the distribution. However,
this manifold is not accessible in most cases and we only have indirect access via the distribution
extended from it. Therefore, we consider finding a shape from the extended density so that this
“shape” successfully approximates the data-generating manifold.
λ-density superlevel set. We use the concept of λ-density superlevel set to capture geometric
features of the density function. Simply put, for a density function p and a threshold λ > 0, the
λ-density superlevel set Lp,λ is the inverse image p-1 [λ, ∞]. Our theoretical contribution is the
conditional existence of a λ-density superlevel set reflecting the topology of the data-generating
manifold under proper conditions on the noise density.
Assumptions on noise density. For a family of densities
{νx}x∈M, we require the noise νx to satisfy a number of as-
sumptions. These assumptions facilitate theoretical discussion
about the superlevel set reflecting the data-generating manifold.
In the following definition, we denote a Euclidean ball of radius
δ centered at x by Bδ (x).
Definition 1. Let νx be a family of noise densities.
•	λ is small-enough if Lνx,λ is nonempty for all x ∈ M,
•	λ-bounding radius δχ,λ ：= min{δ ∣ Lνx,λ ⊆ Bδ(0)} is
the smallest radius that Bδ (0) contains LVx,λ. When
maxx∈M δx,λ exists for some λ, we denote the maxi-
mum value as δλ .
Figure 1: Example superlevel set Lx,λ
with λ-bounding radius δx,λ and λ-
guaranteeing radius x,λ.
•	λ-guaranteeing radius x,λ ：= max{ ∣ B(0) ⊆ Lνx,λ}
is the largest radius that Lνx,λ contains Be(0). When
minx∈M x,λ exists for some λ, we denote the mini-
mum value as λ .
Sufficient conditions for the existence of these radii are dis-
cussed in Appendix D.3. The properties of these radii are summarized in Lemma 1. (The proof
follows from Definition 1).
Lemma 1. Let νx be a family of noise densities and let λ be small-enough. Then,
Ilx - X Il > δλ =⇒ Vx(X - x) < λ
Ilx - X I ≤ 6λ =⇒ Vx(X - x) ≥ λ
whenever δλ and λ exist.
Figure 1 shows an example of superlevel set Lx,λ of noise νx at a point x and its λ-bounding radius
δx,λ and λ-guaranteeing radius x,λ .
1The term topology of distributions, refers to the topology of shapes that correspond to the distributions.
4
Published as a conference paper at ICLR 2020
Finally, we define the continuous variation of noise densities νx over changes of x ∈ M . For the
continuous variation, we require the continuity of both radii δx,λ and x,λ as real-valued functions of
x ∈ M for any fixed value of λ.
Definition 2 (Continuously varying radii). Noise densities νx have continuously varying radii if, for
a fixed small-enough λ, both λ-bounding radius δx,λ and λ-guaranteeing radius x,λ are continuous
functions of x ∈ M .
When noise densities have continuously varying radii, with the compactness of M, we can apply the
extreme value theorem to guarantee the existence of both δλ = maxx∈M δx,λ and λ = minx∈M x,λ.
4.2	Main theorem
Our main theorem establishes, under the assumptions on noise densities from Section 4.1, the
existence of a λ such that,
•	(Inclusion) The λ-density superlevel set Lp,λ includes the data-generating manifold M.
•	(Separation) The λ-density superlevel set Lp,λ consists of connected components such that
each component contains at most one manifold Mi .
Definition 3. Consider a data-generating manifold M with density function pM. For a radius > 0,
We define ωe to be the minimum (over X ∈ M) probability of sampling x' ∈ M in an e-ball Be(x).
ωe ：= min Pr [x' ∈ Be(x)]
X∈M X'~PM
Definition 4 (Class-wise distance). Let (X, d) be a metric space and let M = Ui=ι Mi be a data-
generating manifold in X. The class-Wise distance dcW of M is defined as,
dcw = min min d(x, x')
cw i,j∈[l] x∈Mi
i=j x'∈Mj
With the definitions above, we proved the following main theorem.
Theorem 1. Pick any small-enough threshold λ. Fix a value λ* ≤ ωeλ and let δ* = δλ* be the λ*-
bounding radius. If dcw of M is larger than 2δ*, then the superlevel set Lp,λ* satisfies the following
properties.
•	Lp,λ* contains the data-generating manifold M.
•	Each connected component of Lp,λ* contains at most one manifold Mi of class i.
4.3	Application to the generative model
We show an application of Theorem 1. We denote the target distribution by DX, the latent distribution
by DZ, and the distribution of G(Z) where Z Z DZ by Dg(z). Similarly, we denote the corresponding
λ-density superlevel sets of densities by LλX, LλZ, and LλG(Z). We assume the generative model G to
be continuous. Then, we get the following theorem regarding the difference between LλX and LλG(Z),
in the number of connected components. 2
Theorem 2. Let DZ be a mixture of nZ multivariate Gaussian distributions, and let the data-
generating manifold of DX contain nX components. Let G be a continuous generative model for
DX using latent vectors from DZ. Let λ* be the threshold value from the Theorem 1. If nz < nχ,
LX* and LGIZ) do not agree on the number of connected components.
We can use this theorem to deduce the need for adequate information about the target distribution
when training a generative model, especially if it is used for a security-critical application, e.g., INC.
Corollary 1. If Theorem 2 is satisfied, there is a point X ∈ Rn such that X ∈ LX* but X ∈ LGIZ).
As a result, with density at least λ*, G generates a point X that is unlikely to be generated by the target
distribution. Since INC is based on generations of G, the INC method can output an out-of-manifold
point as a solution of OPtimizatiOn (12).
2In Appendix D.4, Theorem 2 is generalized for more topological properties.
5
Published as a conference paper at ICLR 2020
two-moons	spirals	circles
M0：{(X1,X2)| X2=SOSθ} Mi：{(xiT X1 = 1 -CX2 } for θ ∈ [0, π]	Mo：{(xi,x2)| x2=1 et cos(t)} M-{(χ1,χ2)∣ x2=3 et cn(tt+爵} M2：{(xi,x2)| X2=3etcn(tt+4∏)} for t ∈ [0,T] whereT = ln ( √5 + 1)	Mo：{(xi,x2)l x2=cnsθ} M-{(χ1,χ2)∣ x2=1 cnsθ} forθ ∈ [0,2π]
Table 1: Parameterizations of dataset used in the experiments.
5	Experimental results
In this section, we empirically demonstrate the consequence of the two theorems and explore
their implication for the INC defense. Our main goals are to provide (1) empirical support for
the applicability of Theorem 2 and Corollary 1 via toy datasets, and (2) the improvement in INC
performance using a class-aware generative model. The main questions and the corresponding
answers are shown below.
(Q1) Can we experimentally verify the results of section 4.3? Specifically, can we find cases that
the superlevel sets of DX and DG(Z) have different numbers of connected components?
(Q2) How does INC fail when the generative model is ignorant of topology information?
(Q3) Does the class-aware generative model improve the INC performance?
(A1) Theorem 2 and Corollary 1 can be verified by plotting the λ-density superlevel set. Especially,
we visualize the λ-density superlevel set of DG(Z) reflecting Theorem 2 and Corollary 1.
(A2) When generative model is not trained with topology information, naive INC may fail. We
found out two possible reasons regarding INC failure: (1) choice of a bad initial point and
(2) out-of-manifold search due to non-separation of density superlevel set.
(A3) The performance of INC is improved by training generative models with topology informa-
tion on the target distribution. We improved the average INC performance by decreasing the
error induced by projection to 30% compared to the class-ignorant counterpart.
In the rest of this section, we provide a more detailed description of our experiments. First, we briefly
describe the experimental setup in Section 5.1: datasets, latent vector distributions, training method,
and INC implementation. Then, Sections 5.2-5.4 describe the experimental results regarding the
findings summarized above. Section 5.5 contains an additional experiment illustrating the changes of
decision boundaries by INC application.
5.1	Experimental setup
Datasets. For all experiments, we use three toy datasets in R2 : two-moons, spirals, and circles.
Table 1 summarizes the parameterizations3 of each data-generating manifold and Figure 2 shows
the plots of the corresponding data-generating manifolds. To construct the training set, we first
sample 1000 points uniformly from each manifold Mi , then each point is perturbed by isotropic
Gaussian noise N(0, σ2I2) with σ = 0.05. Before the training, each training set is standardized by a
preprocessing of Scikit-learn package.
Latent vector distributions. For latent vector distributions DZ, we prepared three different mix-
tures of nZ Gaussian distributions with nZ ∈ {1, 2, 3}. When nZ = 1, we simply use N(0, I2).
When nZ = 2, 3, we arranged nZ Gaussian distributions along a circle of radius R = 2.5, so that i-th
Gaussian has mean at μi = (-RSin (誓),Rcos (誓))with σ = 0.5 for n = 2 and σ = 0.3 for n = 3.
Then, the uniform mixtures of the arranged Gaussian are used as DZ. In Figure 3 (top row), we
visualize the connected components corresponding to the latent vector distributions.
Training generative models. Our experiments mostly use the Tensorflow Probability (Dillon
et al., 2017) library that contains the implementation of reversible generative models. Specifically,
the Tensorflow Probability library contains an implementation of the Real NVP coupling layer that
we used as a building block of our models. The default template provided by Tensorflow Probability
3The value T is from the reparameterization t = ln (s/∖p2 + 1) for S ∈ [0,15] for uniform sampling.
6
Published as a conference paper at ICLR 2020
Figure 2: Data-generating manifolds used in the experiments
library was used to construct each Real NVP coupling layer with two hidden layers of 128 units.
Each model uses eight coupling layers that are followed by permutations exchanging two dimensions
of R2 except for the last coupling layer.
We describe the details of the training procedure of the generative models used in the experiments.
We prepared two different types of generative models: class-ignorant and class-aware.
The class-ignorant type is the usual Real NVP model. This model uses the empirical estimation of
negative log-likelihood over a training batch {x1, . . . , xm} as its training loss.
1m
'ci = 一一 ∑ lθg(PX (Xt))
m t=1
The density pX of DX is estimated by applying the change of variables formula,
PX(X) = PZ(Z) ∣det ( dG(T) )1	(2)
where PZ is the density of DZ and dG(z) is the Jacobian of G as a function from Rn to itself.
The class-aware type is the Real NVP model trained with information about the number of connected
components, i.e. the number of class labels l. Using the number of labels, the densities pX and pZ
can be decomposed as follows.
PX (X) =	∑	Pr[y = i] PX,i(X)
i∈{1,...,l}
PZ(Z) =	∑	Pr[y = i] PZ,i(Z)
i∈{1,...,l}
(3)
where pX,i(x) = pX(x ∣y = i) and each pZ,i is the i-th Gaussian component described above. Since
Pr[y = i] is not generally known, the uniform distribution Pr[y = i] = : is used, where l is the
number of classification labels.
The main idea is class-wise training, i.e., training each pX,i from each pZ,i. Applying the change of
variables formula for each class i,
Px,i(x) = Pz,i(z) |det ( dGZZ))I	(4)
Combining equations (3) and (4), we get the change of variables formula (2). We define the class-wise
loss function `i for class-wise training as follows.
1m
'i = -	∑ Myt= i] log (pχ,i(χt))
mi t=1
where mi is the number of training samples in class i. Then, we train a generative model using the
weighted sum of `i as the training loss function.
'ca =	∑ Pr[y = i] 'i
i∈{1,...,l}
Each model was trained for 30,000 iterations. For each iteration, a batch of 200 random samples was
chosen from two-moons and circles dataset, and a batch of 300 random samples was chosen from the
spirals dataset. For the choices of latent vector distribution, we chose the mixture of l - 1 Gaussians
for the class-ignorant type, whereas we chose the mixture of l Gaussians for the class-aware type.
7
Published as a conference paper at ICLR 2020
Figure 3: λ-density superlevel sets of DZ and DG(Z) with λ = 0.01. Top row: DZ for nZ = 1, 2, 3.
Middle row: DG(Z), class-ignorant model. Bottom row: DG(Z), class-aware model.
5.2	Visual verification of theorems
The goal of this section is to verify Theorem 2 and the Corollary 1 by visualizing the superlevel
set reflecting the statements. Figure 3 shows the λ-density superlevel sets of densities of DG(Z)
using the same threshold λ = 0.01. The first row and the second row show the results from the
class-ignorant version and those from the class-aware version, respectively. Each column corresponds
to each dataset. All distributions are scaled for the standardization preprocessing before the training.
In general, superlevel set components are separated when the generative model is class-aware. On the
contrary, the class-ignorant generative models introduce connections between the components, as
anticipated by Corollary 1. Due to this connection, the class-ignorant generative models contain fewer
connected components in their superlevel sets; this verifies Theorem 2 for our choice of λ* = 0.01.
5.3	INC failure due to the lack of information on the distribution topology
We present how the non-separation of superlevel set components influences the performance of the
INC. We provide two possible explanations of why the INC fails. First, the bad initialization causes a
suboptimal solution on a manifold not-the-nearest to the input. Second, an artifact induced by the
topological difference produces an out-of-manifold solution.
Figure 4 presents three visualized examples of INC with a class-ignorant generative model for two-
moons. In each plot, the black dot is the given point X, and cyan dot is the initial point from choosing
Z randomly from the latent vector distribution - N(0, b), and magenta dot is the final point output
by INC. All intermediate points of the optimization are plotted with dots, changing colors gradually
from cyan to magenta. The training set for two-moon used in the training procedure is plotted in gray.
8
Published as a conference paper at ICLR 2020
^°'75	-1.0	-0.5	0.0	0.S LO LS ZO	^°'5	-1.0	-0.5	0.0	05 LO L5 ZO	-1.0	-0.5	0.0	05 LO L5 ZO
(a) INC with an ideal initialization (b) INC with a bad initialization (c) INC searching out of manifold
Figure 4: Successful and failed cases of INC using class-ignorant generative model of two-moon.
Two-moons		Spirals		Circles	
class-ignorant	class-aware	class-ignorant	class-aware	class-ignorant	class-aware
0.647(0.666)~~	0.148(0.208)一	1.523(1.338「	0.443(0.440)一	0.699 (0.491)~^	0.180 (0.259)一
Table 2: Comparison of the projection errors of INC based on the class-awareness of the model.
Figure 4a is the INC optimization with an ideal start. The initial point lies in the same manifold as
the manifold closest to X. Then, the INC optimization searches along the manifold, converging to
a point close to X. Figure 4b shows a case in which INC fails because of a bad initialization. The
initial point was chosen on a manifold not containing the desired solution, so the INC converged to a
local optimum on the wrong manifold. Our class-aware INC performs manifold-wise initialization to
circumvent this issue. Figure 4c shows that the INC failed due to an out-of-manifold search. The INC
converged in a wrong manifold, and a nontrivial amount of intermediate points were out of manifold,
resulting in an out-of-manifold solution (see Figure 3d).
5.4	INC improvement via class-aware generative model
We demonstrate that INC performance is improved by using class-aware generative models. To
measure the performance of the INC, 100 points are chosen uniformly from each manifold Mi . Then,
each point X is perturbed by nx normal to the manifold at X, generating 200 adversarial points
X = X ±r nχ. For all datasets, r = 0.2 is used for perturbation size. We expect two types of INC to
map X back to the original point x, as X is the optimal solution to (11). We define the projection error
of INC as IlINC(X) - X ∣∣ 2, and collect the statistics of projection errors over all X.
Table 2 shows the projection error statistics for two types of INC. Each pair of columns show the
results on the indicated dataset. For each pair, one column shows the error of the class-ignorant INC
and the other column shows that of the class-aware counterpart. Numbers in each cell are averages
and standard deviations (in parenthesis) of the projection error. For any dataset, the class-aware INC
achieves lower projection errors. Histograms of the projection errors are provided in Appendix E.
5.5	Additional experiments for the INC performance.
Finally, we present experiments to demonstrate the effect of the superlevel set discrepancy on the INC
performance. First, we begin with training support vector machines (SVMs) performing classification
tasks for our target distributions. For training data, we randomly sampled 1000 training points from
each data-generating manifold. The baseline SVMs were intentionally ill-trained by using the high
kernel coefficient γ = 100. 4 After training SVMs, we formed other classifiers by applying INC to
ill-trained SVMs To explain, for each dataset, we have four types of classifiers as follows.
(1)	Ill-trained SVM: Baseline classifier
(2)	Ideal INC: Classifier with INC using a direct access to the data-generating manifolds
(3)	Class-ignorant INC: Classifier with INC using a topology-ignorant generative model
(4)	Class-aware INC: Classifier with INC with using a topology-aware generative model
We want to emphasize that direct access to the data-generating manifold is not possible in general.
However, applying INC using direct access gives us an INC purely based on the geometry, so it is an
ideal form of INC that should be approximated. Also, since the class-ignorant INC is affected by a
bad choice of an initial point, we reduced the effect of bad initialization by sampling more initial
points and taking the best solution among the projection results. For this number of initial choices, we
4In general, choosing an unnecessarily high kernel coefficient γ causes overfitting (Chaudhuri et al., 2017),
inducing decision boundary close to the training data.
9
Published as a conference paper at ICLR 2020
(a) Ill-trained SVM
(b) Ideal INC
Figure 5: Changes in the decision boundaries of ill-trained SVM after the INC applications.
(c) Class-ignorant INC
(d) Class-aware INC
chose as many initial points as the number of manifolds, which was exactly the same as the number
of initial points for the topology-aware INC model.
To demonstrate the improvement in the robustness of the model, we visualize the effect by depicting
the decision boundary of each classifier. Specifically, we form a 300 × 300 grid on the domain of
[-3, 3] × [-3, 3] and compute the classification result. The depicted decision boundaries are presented
in Figure 5. Each row corresponds to each dataset: two moons, spirals, and circles, respectively. Each
column corresponds to classifiers 1-4 described above, from the first column to the fourth column,
respectively. From Figure 5, it is visually evident that the class-aware INC models provide more
proper approximations to the ideal INC model compared to the class-ignorant INC models.
6	Conclusion
We theoretically and experimentally discussed the necessity of topology awareness in the training
of generative models, especially in security-critical applications. A continuous generative model is
sensitive to the topological mismatch between the latent vector distribution and the target distribution.
Such mismatch leads to potential problems with manifold-based adversarial defenses utilizing
generative models such as INC. We described two cases in which the INC failed: the bad initialization
and the artifacts from the topological difference. We experimentally verified that topology-aware
training effectively prevented these problems, thereby improving the effectiveness of generative
models in manifold-based defense. After topology-aware training of generative models, the INC
projection errors represented 30% of the errors of the topology-ignorant INC.
7	Acknowledgement
Dr. Susmit Jha and Uyeong Jang’s internship at SRI International were supported in part by U.S.
National Science Foundation (NSF) grants #1740079, #1750009, U.S. Army Research Laboratory
Cooperative Research Agreement W911NF-17-2-0196, and DARPA Assured Autonomy under
contract FA8750-19-C-0089. The views, opinions and/or findings expressed are those of the author(s)
and should not be interpreted as representing the official views or policies of the Department of
Defense or the U.S. Government. This work is partially supported by Air Force Grant FA9550-18-1-
0166, the National Science Foundation (NSF) Grants CCF-FMitF-1836978, SaTC-Frontiers-1804648
and CCF-1652140 and ARO grant number W911NF-17-1-0405.
10
Published as a conference paper at ICLR 2020
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Battista Biggio,Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio
Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases, pp. 387-402. Springer,
2013.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Arin Chaudhuri, Deovrat Kakde, Carol Sadek, Laura Gonzalez, and Seunghyun Kong. The mean and
median criteria for kernel bandwidth selection for support vector data description. In 2017 IEEE
International Conference on Data Mining Workshops (ICDMW), pp. 842-849. IEEE, 2017.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary dif-
ferential equations. In Advances in Neural Information Processing Systems, pp. 6571-6583,
2018.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran
Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense.
arXiv preprint arXiv:1803.01442, 2018.
Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore,
Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorflow distributions. arXiv
preprint arXiv:1711.10604, 2017.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Abhimanyu Dubey, Laurens van der Maaten, Zeki Yalniz, Yixuan Li, and Dhruv Mahajan. De-
fense against adversarial images using web-scale nearest-neighbor search. arXiv preprint
arXiv:1903.01612, 2019.
Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014b.
Will Grathwohl, Ricky TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. arXiv preprint
arXiv:1810.01367, 2018.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial
images using input transformations. arXiv preprint arXiv:1711.00117, 2017.
Andrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, and Alexandros G Dimakis.
The robust manifold defense: Adversarial training using generative models. arXiv preprint
arXiv:1712.09196, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
11
Published as a conference paper at ICLR 2020
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems ,pp.10215-10224, 2018.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016a.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016b.
John M Lee. Introduction to smooth manifolds. Graduate Texts in Mathematics, 218, 2003.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2574-2582, 2016.
James R Munkres. Topology prentice hall. Inc., Upper Saddle River, 2000.
Andrew Y Ng and Michael I Jordan. On discriminative vs. generative classifiers: A comparison of
logistic regression and naive bayes. In Advances in Neural Information Processing Systems, pp.
841-848, 2002.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In Advances in Neural Information Processing Systems, pp. 2338-2347, 2017.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium
on Security and Privacy (EuroS&P), pp. 372-387. IEEE, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on
Asia Conference on Computer and Communications Security, pp. 506-519. ACM, 2017.
Xavier Pennec. Probabilities and statistics on riemannian manifolds: Basic tools for geometric
measurements. In Nonlinear Signal and Image Processing, pp. 194-198. Citeseer, 1999.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2, 2017.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. arXiv
preprint arXiv:1710.10766, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Thomas Tanay and Lewis Griffin. A boundary tilting persepective on the phenomenon of adversarial
examples. arXiv preprint arXiv:1608.07690, 2016.
12
Published as a conference paper at ICLR 2020
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
E Weinan. A proposal on machine learning via dynamical systems. Communications in Mathematics
and Statistics, 5(1):1-11, 2017.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects
through randomization. arXiv preprint arXiv:1711.01991, 2017.
Linfeng Zhang, Lei Wang, et al. Monge-amp/ere flow for generative modeling. arXiv preprint
arXiv:1809.10188, 2018.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv
preprint arXiv:1609.03126, 2016.
Xiaojin Zhu and Andrew B Goldberg. Introduction to semi-supervised learning. Synthesis Lectures
on Artificial Intelligence and Machine Learning, 3(1):1-130, 2009.
13
Published as a conference paper at ICLR 2020
A	Mathematical background
A.1 General topology
We introduce definitions and theorems related to general topology appeared in the paper. For more
details, all the definitions and theorems can be found in Munkres (2000).
Definitions in general topology. We first provide the precise definitions of the terms we brought
from the general topology.
Definition 5 (Topological space). A topology on a set X is a collection T of subsets of X having
the following properties.
1.	0 and X are in T.
2.	The union of the elements of any subcollection of T is in T.
3.	The intersection of the elements of any finite subcollection of T is in T.
A set X for which a topology T has been specified is called a topological space.
For example, a collection of all open sets in Rn is a topology, thus Rn is a topological space. If a
topology can be constructed by taking arbitrary union and a finite number of intersections of a smaller
collection B of subsets of X , we call B is a basis of the topology.
Pick a metric d in Rn and consider B a set of all open balls in Rn using the metric d. The topology
of Rn can be constructed by taking B as a basis. When this construction is possible, metric d is said
to induce the topology.
Definition 6 (Metrizable space). If X is a topological space, X is said to be metrizable if there exists
a metric d on the set X that induces the topology of X . A metric space is a metrizable space X
together with a specific metric d that gives the topology of X .
Since Rn is equipped with Euclidean metric that induces its topology, Rn is metrizable.
Continuity and the extreme value theorem. Let X and Y be topological spaces. In the field of
general topology, a function f : X → Y is said to be continuous, if for any subset V open in Y, its
inverse image f-1 (V) is open in X. Moreover, if f is a continuous bijection whose inverse is also
continuous, f is called a homeomorphism. The notion of homeomorphism is important as it always
preserves topological property, e.g., connectedness, compactness, etc., and this will be used in the
further generalization of Theorem 2.
Here, we only introduce the generalized statement of extreme value theorem.
Theorem 3	(Extreme value theorem). Let f : X → Y be continuous, where Y is an ordered set. If
X is compact, then there exist points X and X in X such that f (χ) ≤ f (x) ≤ f (x) for every X ∈ X.
Specifically, if a manifold M is a compact subset in Rn , we may use X = M and Y = R.
Normal space and Urysohn’s lemma. The Urysohn’s lemma was used to prove the Corollary 1.
We first introduce the notion of normal space.
Definition 7 (Normal space). Let X be a topological space that one-point sets in X are closed. Then,
X is normal if for each pair A, B of disjoint closed sets ofX, there exist disjoint open sets containing
A and B, respectively.
Urysohn’s lemma is another equivalent condition for a space to be normal.
Theorem 4	(Urysohn’s lemma). Let X be a normal topological space; let A and B be disjoint closed
subsets in X. Let [a, b] be a closed interval in the real line. Then there exists a continuous map
f : X 一＞ [a, b]
such that f(X) = a for every X in A, and f(X) = b for every X in B.
To apply this lemma to Rn , we only need the following theorem.
Theorem 5.	Every metrizable space is normal.
Since Rn is metrizable, it is a normal space by Theorem 5. Therefore, we can apply Urysohn’s lemma
to any pair of disjoint subsets in Rn, to show the existence ofa continuous map f : X → [0, 1].
14
Published as a conference paper at ICLR 2020
A.2 Differential geometry
We provide the definitions from differential geometry (Lee, 2003) used in the paper.
Manifold and tangent space. Formally, topological manifold is defined as follows.
Definition 8 (Manifold). Suppose M is a topological space. We say M is a topological manifold of
dimension k if it has the following properties.
1.	For any pair of distinct points x1, x2 ∈ M, there are disjoint open subsets U1, U2 ⊂ M such
that x1 ∈ U and x2 ∈ V .
2.	There exists a countable basis for the topology of M .
k
3.	Every point has a neighborhood U that is homeomorphic to an open subset U of Rk .
There are different ways to define tangent space of k-dimensional manifold M . Informally, it can be
understood as geometric tangent space to M ⊂ Rn at a point x ∈ M, which is a collection of pairs
(x, v) where v is a vector tangentially passing through x. Here we put a more formal definition of
tangent space. Consider a vector space C∞(M), a set of smooth functions on M.
Definition 9 (Tangent space). Let X bea point of a smooth manifold M. A linear map X : C∞(M) →
R is called a derivation at x if it satisfies
X(fg) = f(X)Xg + g(X)Xf
for all f, g ∈ C∞ (M).
The set of all derivations of C∞(M) at X forms a vector space called the tangent space to M at X,
and is denoted by Tx(M).
Riemannian metric. As tangent space Tx(M) is a vector space for each X ∈ M, we can consider
a inner product gbfx defined on Tx(M).
Definition 10 (Riemannian metric). A Riemannian metric g on a smooth manifold M is a smooth
collection of inner products gx defined for each Tx(M). The condition for smoothness of g is that,
for any smooth vector fields X, Y on M, the mapping X ↦ gx(X∣x, Y∣x).
A manifold M equipped with a Riemannian metric g is called a Riemannian manifold.
B Examples
Computing density pM over a Riemannian manifold M. This section presents example compu-
tations of the probability computations from Section D.1 and Section 3.2 As a concrete example of
computing density over a manifold, we use the following simple manifolds, so called two-moons in
R2.
M0={(x1,x2)∣ xx21 ==scionsθθ forθ∈[0,π]}
Mi = {(x1,x2)∣ x1 = 1- Cosθ 1	for θ ∈ [0,π]}
1	11 1	21 x2 = 1 - sin θ + 2	L ，」j
We take M = M0 ∪ M1 as our example manifold. Figure 6a shows the manifold of two-moons
dataset plotted in different colors: M0 in red and M1 in blue.
First recall the following equation (equation (8) from the Section D.1).
∫ MpM(x)dM(x) = ∫ DpM(X(u))∖∕∣det[gx(u)]∣dU
where [gX(u)] is the k × k matrix representation of the inner product gX(u) at X(u) ∈ M.
Especially, when a manifold in Rn is of dimension 1, i.e., parameterized curve γ : [a, b] → Rn, the
integration (8) can be written in simpler way.
∫	Pm(x)dM(x) = ∫ pM(γ(t))∣∣γ'(t)∣∣dt
x∈M	t=a
(5)
where γ'(t) is the n-dimensional velocity vector at t ∈ [a, b].
15
Published as a conference paper at ICLR 2020
(a) Plot of the two-moons mani-
fold in R2
Figure 6: Density extension example from two-moons manifold.
(b) Extended density function over R2 from
the two-moons dataset
Let pM be a probability density function defined on M . As M is composed of two disjoint manifolds
M0 and M1 , we consider conditional densities p0 , p1 as follows.
p0(x) = pM(x∣x ∈ M0) =
PM Mo (x)
Pr[x ∈ Mo]
PI(X)=PM (XIX ∈MI)=PMMIMI：
(6)
Here, pM ∣M0 and pM ∣M1 represent the density function pM with its domain restricted to M0 and
M1, respectively. By our definition of data-generating manifolds, Pr[x ∈ Mi] corresponds to the
probability of data generation for class i, i.e. Pr[y = i]. For a concrete example of such density,
uniform density for each manifold Mi can be defined as Pi(X) = ∏ for all X ∈ Mi.
Note that each manifold has parameterized curves in R2,
γo : θ ↦ (Cos θ, sinθ)
γι : θ ↦ (1 - Cos θ, 1 - sinθ + 0.5)
with constant speed ∣∣ γ0(θ)∣∣ = ∣∣γ1(θ)∣∣ = 1 atall θ ∈ [0, ∏]. Therefore, from equation (5),
∫	PM ∣M0 (X)dM0(X) = ∫ PM (γ0(θ))dθ
x∈M0	θ=0
∫	PM ∣M1 (X)dM1 (X) = ∫ PM (γ1 (θ))dθ
x∈M0	θ=0
(7)
For any measurable subset A ⊆ M, the probability for an event that X is in A can be computed as
follows.
Pr[X ∈ A] = ∫	PM (X)dM (X)
x∈A⊆M
= ∫	PM ∣M0 (X)dM0(X) + ∫	PM ∣M1 (X)dM1(X)
x∈A∩M0	x∈A∩M1
=∫θ=[0,π] PM I Mo (YO(O))dθ+ ∫θ=[0,∏] PM∣M1 (YI(O)) dθ CO))
γ0 (θ)∈A	γ1 (θ)∈A
= Pr[X ∈ M0] ∫ θ∈[0,π] P0 (Y0 (θ))dθ
γo (θ)∈A
+ Pr[x ∈ M1] [θ∈[0,π] Pl (YI(O))dθ (I,⑹)
γ1 (θ)∈A
1
π
Pr[X ∈ M0] ∫ θ∈[0,π] 1dO + Pr[X ∈ M1] ∫ θ∈[0,π] 1dO
γo (θ)∈A	γ1 (θ)∈A
We can briefly check all the requirements (R1), (R2), and (R3). The computation of Pr[X ∈ A] is
based on (R1), so (R1) is satisfied trivially. Also, pM is a function defined only on M , thus (R2) is
clear, i.e. supp(pM) = {X ∈ Rn ∣ p(X) > 0} ⊆ M. To check (R3), when A = Mi, computing this
integration will result in the exact probability Pr[X ∈ Mi] = Pr[y = i], so when A = M, computing
the integration will result in Pr[y = 0] + Pr[y = 1] = 1, as desired in the requirements.
16
Published as a conference paper at ICLR 2020
Extending density to Rn . We extend the domain to Rn for the example of two-moon. In Section
3, we defined the noise density function to satisfy the following requirement.
(R0) The translated noise density function, Vx(X - x), is the density of noise n = X - X being
chosen for a given x. Given x° = x, since adding noise n is the only way to generate X by
perturbing x0, p(x∣ x0 = x) is equal to νx(n).
Under a proper noise density function, We show an example construction of the density extended
from M satisfying the requirement (R0). For simplicity, we choose isotropic Gaussian distribution,
N(0, σ2I) with the standard deviation σ for each dimension as the noise density function νx for all
x ∈ M. Such noise density νx defined in Rn can be written as follows.
z ʌ 1 Cnx 周
Vχ(nχ) =	exp --
√2∏σ2	∖	2σ2 )
By putting nx = X - X to density equation above,
P(X) = ∫	√Λ 2 exp (- 'x t-X '2) PM(X)dM (X)
x∈M 2πσ2	2σ2
Specifically, We assume an isotropic Gaussian distribution with σ = 0.05 as the noise density νx for
all x ∈ M.
By the equation (1), We have the following computation of density on X.
p(X) = ∫ ʌʃ Vx(X - x)pm(x)dM(x)
=∫	Vx(X - x)pm∣Mo (x)dMo(x) + ∫	Vx(X - x)pm∣Mι (x)dMι(x)
x∈M0	x∈M1
ππ
=∫	Vx(X - x)pm∣Mo (γo(θ))dθ + ∫ Vx(X - x)pm∣Mι (γι(θ))dθ (∙.∙(5))
θ=0	θ=0
π
=Pr[x ∈ Mo] ∫	Vx(X - x)po(γo(θ))dθ
θ=0
π
+Pr[X∈M1]∫θ=0Vx(X-X)PI(Y1(θ))dθ (∙・.⑼
∏√⅛ HX ∈ Mo] ∫θ=πo exp (-⅛≠卜θ
+ Pr[x ∈ Mi] (π exp (- UX-X B ) dθ
We can also check that the requirement (R0) is satisfied by the construction; our construction
(equation (1)) is based on (R0). The computed density is shown in Figure 6b.
C Proofs
In this section, we provide the proofs for statements that appeared in Section 4.
C.1 Proof of Theorem 1
To begin with, pick a value λ such that the λ-density superlevel set Lνx,λ is nonempty for all x ∈ M.
As we use noise densities νx described in Section 4.1, it is safe to assume that both λ-bounding radius
δλ = maxx∈M δx,λ and λ-guaranteeing radius λ = minx∈M x,λ exist.
Then, we can prove that, with a proper choice of threshold λ, the λ-density superlevel set includes
the data-generating manifold.
Lemma 2. Assume that noise densities have radii in Definition 1 for all x ∈ M and a small enough
λ > 0. Then, for any x ∈ M, the density p(x) is at least ωλ, i.e. p(x) ≥ ωλ, where = λ.
Proof. By Lemma 1,
X' ∈ Be(X) ^⇒ X ∈ Be(x') = Beλ (x') (v€ = 6λ)
=⇒ Vx' (x - X') ≥ λ
17
Published as a conference paper at ICLR 2020
Then, we can lower bound the density pM (x) as follows.
P(X) = ∫	Vχ'(x - x')pM (x')dM (x/)
Jx'∈M
≥ ∫	Vχ' (x - x')pM (x')dM(x')
Jx'∈M∩Be (x)
≥ λ ∫	PM(x')dM(x')
Jx'∈M ∩Be
=λ Pr [x'∈ Be(x)]
x'∈M
≥ ωλ
□
This lemma shows that the thresholding the extended density P with threshold λ* ≤ ωeλ guarantees
the superlevel set to include the entire manifold M .
Corollary 2. For any threshold λ* ≤ ωeλ, the corresponding λ*-density superlevel set Lp,λ* of the
extended density p includes the data-generating manifold M .
Similarly, we show that, with a proper choice of threshold λ, each connected component of λ-density
superlevel set contains at most one manifold.
Lemma 3. Assume a family of noise densities satisfies the assumptions of Section 4.1. Let λ > 0 be
a value such that the λ-density superlevel set Lνx ,λ is nonempty for any x ∈ M . Also, let δ = δλ be
the maximum λ-bounding radius over M. Then, for any X / Nδ(M), the extended density value is
smaller than λ, i.e. p(X) < λ.
Proof. By Lemma 1,
X / Nδ(M) ^⇒ X / Bδ(x) = Bδλ (x) for any X ∈ M (vδ = δλ)
=⇒ Vx (X - x) < λ for any X ∈ M
Then, We can upper bound the density p(X) as follows.
P(X) = ∣~
x
Vx(X - X)PM(X)dM(x)
<λ
(∙.∙X / 冷入(M))
=λ
□
This lemma says that the λ-density superlevel set is included by the δ-neighborhood Nδ(M) of the
data-generating manifold M .
Now, we can deduce the following main result.
Theorem 1. Pick any λ* ≤ ωλ threshold value satisfying the Corollary 2. If the class-wise distance
of data-generating manifold is larger than 2δ* where δ* = δλ* (the λ*-bounding radius), then the
superlevel set Lp,λ* satisfies the followings.
•	Lp,λ* contains the data-generating manifold M.
•	Each connected component of Lp,λ* contains at most one manifold Mi of class i.
Proof. The first property is a direct application of Corollary 2 for λ* = ωλ.
For the second property, since the class-wise distance of M is larger than 2δ*, the δ*-neighborhood
of manifolds are pairwise disjoint, i.e, Nδ* (Mi) ∩ Nδ*(Mj) = 0 for each i = j. Therefore, Nδ*(M)
has exactly k connected components Ni = Nδ* (Mi),s.
By Lemma 3, δ*-neighborhood Nδ* (M) contains the superlevel set Lp,λ*, thus each connected
component of Lp,λ* is in exactly one of Nis. Since M is contained in Lp,λ*, each Mi is contained
in some connected component C of Lp,λ* which is in Ni. Then, for any j = i, Mj ⊂ C ⊂ Ni, since
Mj is in Nj which is disjoint to Ni . Therefore, if a connected component C contains a manifold Mi ,
then it cannot contain any other manifold.	□
18
Published as a conference paper at ICLR 2020
C.2 Proofs for Section 4.3
Theorem 2. Let DZ be a mixture of nZ multivariate Gaussian distributions, and let DX be the target
distribution from a data-generating manifold with nX manifolds. Let G be a continuous generative
model for DX using latent vectors from DZ. Assume the Theorem 1 is satisfied, and let λ* be the
threshold value from the Theorem 1. If nz < nχ, LX and LGIZ) do not agree on the number of
connected components.
Proof. Since L^ is the results of Theorem 1, the number of connected components of LX* is at least
nX.
However, since DZ is a mixture of Gaussians, for any value of λ (including the special case λ = λ*),
LλZ can never have more than nZ connected components. Since G is continuous, it preserves the
number of connected components, thus LλG*(Z) = G(LλZ* ) has at most nZ connected components. As
nz < nχ, LX* and LGIZ) can never agree on the number of connected components.	□
Corollary 1. If Theorem 2 is satisfied, there is a point X ∈ Rn such that X ∈ LX* but X ∈ LGIZ).
Proof. Since n% < nχ, there exists a connected components C of LGIZ) containing at least two
connected components of SX*. Without loss of generality, assume C contains exactly two connected
components C and C ′ . By definition, λ-superlevel set is a closed set, so C and C ′ are disjoint closed
sets. In Euclidean space Rn, the Urysohn’s lemma tells us that for any disjoint pair of closed sets
A, A in Rn, there is a continuous function f such that f ∣a(x) = 0 and f ∣A'(x) = 1 for any X ∈ Rn.
Especially, when A = C and A' = C', there exists a continuous function f such that,
•	f(X) =0 forallXinC
•	f (x) = 1 for all X in C'
Consider S = f-1( 2) which is a separating plane separating C and C'. If C ∩ S = 0, then
C ∩ S = f-1 [0,1) and C ∩ S = f-1( 2, 1] will be two open set in subspace C, whose union is C.
This implies that C is disconnected, which is a contradiction. Therefore, C ∩ S should be nonempty,
and any point X in C ∩ S is not in LX*.	□
D	Further discussions
D	. 1 Computing density over a data-generating manifold
When M is a Riemannian manifold equipped with a Riemannian metric g, we can compute probabili-
ties over M. There are two essential components of probability computation: (a) a density function
pM and (b) a measure dM over M . We assume pM and dM to satisfy the followings.
(R1) For any measurable subset A ⊂ M, i.e., Pr[X ∈ A] = ∫x∈ApM(X)dM(X).
(R2) p is zero everywhere out of M, i.e., supp(pM) = {X ∈ Rn ∣ pM (X) > 0} ⊆ M
(R3) For any (X, y), X is sampled from Mi if and only ify = i, i.e. Pr[X ∈ Mi] = Pr[y = i]
When equipped with such pM and dM, we call M as a data-generating manifold.
Probability over a Riemannian manifold. We show how to compute a probability of X being
generated from a Riemannian manifold M . We assume a k-dimensional manifold M equipped with
a Riemannian metric g, a family of inner products gx on tangent spaces TxM . In this case, g induces
the volume measure dM for integration over M. IfM is parameterized by X = X(u) for u ∈ D ⊆ Rk,
the integration of a density function pM on M is as follows.
∫ MpM(X)dM(x) = ∫ DpM(X(u))∖∕∣det[gx(u)]∣dU
(8)
where [gX(u)] is the k × k matrix representation of the inner product gX(u) at X(u) ∈ M.
In Appendix B, a concrete example of this computation will be provided.
19
Published as a conference paper at ICLR 2020
D.2 Density extension of the Section 3.2
This section introduces some remaining discussions regarding our data-generating process from a
data-generating manifold.
Relation to kernel density estimation. While this extension is computing the density of compound
distribution, it can be interpreted as computing expectation over a family of locally defined densities.
Such an expected value can be observed in previous approaches of density estimation. For example, if
νx is isotropic Gaussian for each x, the above integration is equivalent to the kernel density estimation,
with Gaussian kernel, over infinitely many points on M .
Observed property of the extended density. In Figure 6b in Appendix B, we can observe that
the extended density achieved higher values near the data-generating manifold. We formalize this
observation to discuss its implication to the INC approach.
Let d(X, M) to be the minimum distance from X to the manifold M.
(C1) For any given X, let y* be the class label whose conditional density p(X∣y = y*) dominates
p(X∣y = i) for i / y*,
y* ∈ arg maxp(x∣y = i)	(9)
i∈[l]
and let My* be the manifold corresponding to y*.
(C2) For y* satisfying (C1), we choose y* such that the distance of X from the manifold
d(X, My*) is the smallest.
If there are multiple y* satisfying both of (C1) and (C2), we expect the following property to be true
for all of those y*.
(P1) Consider the shortest line from X to the manifold My*. As X goes closer to My* along
this line, X should be more likely to be generated as the influence of noise decreases when
moving away from the manifold. Therefore, we expect our density pM to have the following
property.
x* ∈ arg min d(x, x)
x∈My*	(10)
=⇒ P(X) ≤ p((1 - λ)X + λ x*) for all λ ∈ [0,1]
Actually, this provides another justification of INC. In reality, the density conditioned by the label
is not available even after running a generative model, so finding y* with (C1) is relatively hard. If
we only consider (C2) without filtering y* via (C1), we are finding a point X ∈ M achieving the
minimum distance to X, which is the optimization (11) above. Then projecting X to the x*, i.e. the
solution of the optimization 11, can be explained by 10; when λ = 1, p is the highest along the
shortest line between X and x* .
D.3 Sufficient conditions for the existence of radii
We discuss the sufficient conditions guaranteeing the existence of radii introduced in Definition 1.
Those sufficient conditions are derived from natural intuition about the properties of distributions in
most machine-learning contexts.
The first intuition is that the influence of noise should diminish as observed sample X moves away
from a source point Xo . Therefore, we formalize the noise whose density decreases as the noise
n = X - Xo gets bigger. We formalize boundedness of noise densities via the boundedness of their
λ-density superlevel sets and continuity of noise density via the continuity of individual νx .
Definition 11 (Center-peaked noise density). Noise density functions νx are center-peaked, if for any
source point X ∈ M and any noise vector n ∈ Rn with ∣∣ n ∣∣ > 0, Vx(n) < Vχ(λn) for all λ ∈ [0,1).
Definition 12 (Bounded noise density). Noise density functions νx are bounded, if a λ-density
superlevel set is nonempty, there is a radius δ by which the λ-density superlevel set is bounded, i.e.,
LVx ,λ ⊆ Bδ(0) where Bδ (0) is the closed ball of radius δ centered at 0.
Definition 13 (Continuous noise density). Noise density functions νx are continuous, if νx is
continuous in Rn , for any X ∈ M.
20
Published as a conference paper at ICLR 2020
Under the conditions above, the radii in Definition 1 always exist.
Proposition 1. If noise densities νx are center-peaked, bounded, and continuous, any nonempty
λ-density superlevel set Lνx ,λ has both λ-bounding radius δx,λ and λ-guaranteeing radius x,λ.
Proof. Let νx be a center peaked, superlevel set bounded family of continuous noise densities. Since
νx is continuous, superlevel set Lνx,λ = νx-1 [λ, ∞) is closed as an inverse image of νx. Therefore, its
boundary ∂Lνx,λ is contained in Lνx,λ .
Because Vx is superlevel set bounded, superlevel set Lνx,λ is bounded by a closed ball Bδ(0) With
radius δ ≥ 0. Since νx is center peaked, a nonempty superlevel set Lνx ,λ always contains 0 as the
maximum is achieved at 0. Moreover, there exists a closed neighborhood ball Be(0) with radius
≥ 0 contained in the superlevel set Lνx,λ. NoW it is enough to shoW that the minimum of δ and the
maximum of exist.
Since Lνx ,λ is bounded, its boundary ∂Lνx ,λ is also bounded. ∂Lνx,λ is closed and bounded, thus
it is a compact set. Therefore, the Euclidean norm, as a continuous function, should achieve the
maximum r and the minimum r on ∂Lνx ,λ by the extreme value theorem. From the choice of δ and
e, we can get,
e ≤ r ≤ r ≤ δ
Therefore, we can find the minimum δx,λ = r and the maximum ex,λ = r.	□
D.4 Generalization of the Theorem 1
We try generalizing the Theorem 2 to handle more concepts in topology. Theorem 2 mainly uses a
fact that the number of connected components of λ-density superlevel set is preserved by a continuous
generative model G.
In algebraic topology, each connected component corresponds to a generator of 0-th homology group
H0 , and continuity of a function is enough to preserve each component. In general, generators of i-th
homology group Hi for i > 0 are not preserved by a continuous map, so we need to restrict G further.
By requiring G to be a homeomorphism, we can safely guarantee that all topological properties are
preserved by G; therefore, we can generalize the Theorem 2 with a homeomorphic generative model
G.
To generalize the proof of the Theorem 2, we first provide the sketch of the proof.
⑴ λ*-density superlevel set L:* of a mixture of nz Gaussian distributions has at most nz
connected components.
(2)	Since G is continuous, the number of connected components of LG (Z) = G(LZ) is same
to the number of connected components of L:*, so it is also at most nz.
(3)	We choose λ* so that LXi is included in δ*-neighborhood of M.
(4)	By assumption on the class-wise distance of M, δ*-neighborhood of M has exactly same
number of connected components to M, i.e., nX . Therefore LλXi has at least nX connected
components.
(5)	By (2) and (4), we conclude that LλGi (Z) and LλXi do not agree on the number of connected
components as long as nz < nX .
In this proof, nz corresponds to the maximal 0-th Betti number of Lλzi, i.e. the number of generators
of H0 (Lλzi). If we keep using a mixture of Gaussians as latent vector distribution, all components of
Lλzi are contractible, so we may use 0 as the maximal i-th Betti number.
Also, nX corresponds to the 0-th Betti number of M and it worked as the minimal 0-th Betti number
of LλXi. The condition on the class-wise distance of M is used to ensure nX to be a lower bound.
Combining these observations, we can get the following generalized statement.
Theorem 3. Let Dz be a mixture of multivariate Gaussian distributions, and let DX be the target
distribution from data-generating manifold M . Let ni be the i-th Betti number of M.
Consider a generative model G is used to approximate DX using the latent vectors sampled from
Dz . Assume that G is a homeomorphism from Rn to itself. Assume that data-generating manifold
satisfies the conditions of the Theorem 1, and let λ* be the threshold value that LX corresponds
21
Published as a conference paper at ICLR 2020
to that superlevel set. Assume that for some j > 0, the homomorphism ∣* induced by the inclusion
ι: M → Nδ* (M) is injective.5
If 0 < n7-, LX and LG(Z) do not agree on the number of connected component.
Proof. Since LR* is the results of Theorem 1, it includes M and is included by δ*-neighborhood
Nδ* (M) of M. Define inclusions ∣ι, ∣2 as,
•	ι1 : M → LλX*
•	ι2 : LλX* → Nδ* (M)
Clearly, ι = ∣2 Q ∣ι.
Let ∣↑ and 琦 be induced homomorphisms of ∣ι and ∣2, resp.
By the assumption, any generator [a] in Hj (M) is injectively mapped to a nonzero generator ∣* ([a])
in Hj (Nδ* (M)). Therefore, the j-th Betti number of Nδ* (M) is equal to that of M, i.e. nj. Note
that j-th Betti number is the rank of j-th homology group rank(Hj(Nδ* (M))) Because 琦 is a
homomorphism from Hj (LλX*) to Hj (Nδ* (M)), rank(LλX* ) ≥ rank(Hj (Nδ* (M))). Therefore the
j-th Betti number of LλX* is at least nj .
However, since DZ is a mixture of Gaussians, for any value of λ (including the special case λ = λ*),
LλZ does not have any generator of j-th homology group, so it has j-th Betti number 0 for all
j > 0. Since G is homeomorphic, it preserves all the Betti numbers, thus LλG*(Z) = G(LλZ*) has the
same j-th Betti number. As 0 < nj, LλX* and LλG*(Z) can never agree on the number of connected
components.	口
In Section 5.2, we see the Figure 3i from the circles dataset, which is a remarkable example that
LλG(Z) has the same number of connected components, but does not have any loop (non-contractible
circle). This is empirical evidence of Theorem 3, so it is explained by mismatches in the topology
of distributions. Each concentric circle has Z as its first homology group as circle contains exactly
one generator. However, latent vector distribution always has a trivial first homology group, as any
superlevel set ofa mixture of Gaussians is a set of contractible connected components.
D.5 Details of INC implementations in the Section 5
INC implementation. We start from introducing the optimization for the ideal INC projection
when the data-generating manifold M is available.
x* = arg min d(x, X)	(11)
x∈M
where d is a metric defined on the domain X . If perfect classification on M is assumed (model is well-
trained on M) and X is close enough to the manifold of correct label, classification f (x*) is likely to
be correct, since x* is likely to lie on the correct manifold. Since the data-generating manifold M is
unknown, the INC approach runs the following optimization with before the classification.
x* = G(z*) where z* = arg min d(G(z),X)	(12)
z~D Z
where d is a metric defined on the domain X .
When INC is implemented with a reversible generative model G, for any given X ∈ Rn there exists a
trivial solution z* = G-I(X) to the optimization (12), achieving d(G(z*), x) = 0. This is even true
for X out of the manifold, resulting in the situation that the output x* = G(z*) = X is still out of the
data-generating manifold.
To manage this problem, we add another term penalizing a low density of latent vector to the objective
function. Thus, in our INC implementation, we solve the following optimization problem.
x* = G(z*) where z* = arg min [d(G(z),X) + α(M -PZ(z))]	(13)
z~D Z
5Any generator of the j-th homology group Hj(M) of M is mapped to a nonzero generators of the j-th
homology group Hj (Nδ* (M)) of δ* -neighborhood of M.
22
Published as a conference paper at ICLR 2020
where α is the regularization factor and M is the maximum possible value of the density pZ of the
latent vector distribution. For the choice of regularization factor, we used the same value α = 1 during
the entire experiment.
To solve each optimization problem, we used the built-in adam optimizer (Kingma & Ba, 2014) in
Tensorflow package. For optimization parameters, we ran 100 iterations of adam optimizer using
learning rate 0.01 with random sampling of z.
When implementing INC using a class-aware generative model, we used the following strategy to
improve its robustness.
•	As the class-aware generative model generates each manifold from each Gaussian compo-
nent, we first sample initial points from each manifold by randomly choosing latent vectors
z1, . . . , zl from each Gaussian component.
•	We run INC for i-th manifold by solving the following optimization.
x； = G(Zi) where Z = arg min [d(G(z),x) + α(Mi -pz,i(z))]
z~Dz
where Mi is the maximum value of i-th Gaussian component. The regularization term is
designed to penalize z which is unlikely to be generated by i-th Gaussian component, so we
only search in the range of i-th Gaussian component, i.e., i-th manifold.
•	We choose the final solution XW achieving the minimum d(x" X), breaking ties randomly.
Since each search is performed only on each submanifold, the artifact observed in Section 5.3 never
appears during the optimization process. Also, choosing initial points from each manifold prevents
the initialization problem mentioned in Section 5.3.
D.6 Discussion about the limitation of topological information
Given a sufficient number of connected components in the latent vector distribution, does the class-
aware training suggested in this paper result in a generative model that achieves manifold separation?
For this question, the answer is no, and the manifold separation depends on other factors, e.g.,
alignment of latent vector distribution, choice of training parameter, etc.
Mixture of 2 Gaussians SZ= 2)	Two-moons： Class-aware
(a) Superlevel set of Dz	(b) Superlevel set of DG(z)
Figure 7: Failure cases of class-aware training.
Figure 7b shows the superlevel set of DG(Z) from a class-aware training to learn the two-moons
dataset when latent vector distribution is a mixture of two Gaussian distributions aligned horizontally
(Figure 7a). It is clear that in this case, the generative model induced a connection artifact even when
the class-aware training was used.
We explain this by interpreting reversible generative models as dynamical systems (Weinan, 2017;
Chen et al., 2018; Grathwohl et al., 2018; Zhang et al., 2018). To elaborate, a reversible generative
23
Published as a conference paper at ICLR 2020
model can be viewed as a dynamical system moving the latent vector distribution to the target
distribution continuously in time. When two Gaussian mixtures are aligned vertically, a reversible
generative model is likely to learn how to move the upper (and lower) Gaussian distribution toward
the upper moon (and the lower moon, respectively), without being affected by the entanglement
of two moons. However, moving the left (and right) Gaussian distribution toward the left moon
(and the right moon, respectively) continuously in time is required to avoid the entanglement of
two moons during the transition. This case alludes that information about the topological properties
may not be enough to learn a generative model separating manifolds, because it does not provide an
understanding of information about how data-generating manifolds are aligned.
E More experimental results
We present more experimental results about the INC performance comparing topology-aware genera-
tive model to its topology-ignorant counterpart.
Histogram for projection error distributions in 5.4. Figure 8 presents the histogram of the
projection errors distributed from 0 to the diameter of the distribution. Each row corresponds to each
dataset, whereas the first column and the second column represent the results from the topology-
ignorant model and the topology-aware model, respectively. All histograms are normalized so that
the sum of values adds up to 1. To explain, the y-axis of each histogram is the estimated probability
that INC achieves the projection error on the x-axis. Not only can we observe the improved mean of
projection errors in the histograms, but we can also check the reduced standard deviation, i.e., we get
more consistent projection errors near the mean.
LO
(c) Spirals, topology-ignorant
Circles： Class-ignorant
(d) Spirals, topology-aware
Circles： Class-aware
LO ---------------------------
(f) Circles, topology-aware
Figure 8: Histograms of the projection errors of INC. Each y-axis represents the estimated probability
that INC incurs the projection error on the corresponding x-axis.
8 6 4 2
0.0.0.0.
A≡qs0Jd
SQES
(a) Two-moons, topology-ignorant
Projection error
(e) Circles, topology-ignorant
8 6 4 2
ao.da
A≡qeqωd
PalBEQSa
24