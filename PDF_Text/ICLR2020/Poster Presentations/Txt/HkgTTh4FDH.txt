Published as a conference paper at ICLR 2020
Implicit Bias of Gradient Descent based Ad-
versarial Training on Separable Data
Yan Li, Huan Xu, Tuo Zhao
H. Milton Stewart School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA 30318
{yli939, huan.xu, tourzhao}@gatech.edu
Ethan X.Fang
Department of Statistics
Pennsylvania State University
University Park, PA 16802
xxf13@psu.edu
Ab stract
Adversarial training is a principled approach for training robust neural networks.
Despite of tremendous successes in practice, its theoretical properties still remain
largely unexplored. In this paper, we provide new theoretical insights of gradi-
ent descent based adversarial training by studying its computational properties,
specifically on its implicit bias. We take the binary classification task on linearly
separable data as an illustrative example, where the loss asymptotically attains its
infimum as the parameter diverges to infinity along certain directions. Specifically,
we show that for any fixed iteration T , when the adversarial perturbation during
training has proper bounded '2-norm, the classifier learned by gradient descent
based adversarial training converges in direction to the maximum '2-norm margin
classifier at the rate of O(1/ T), significantly faster than the rate O (1/ logT)
of training with clean data. In addition, when the adversarial perturbation during
training has bounded 'q-norm with q ≥ 1, the resulting classifier converges in
direction to a maximum mixed-norm margin classifier, which has a natural inter-
pretation of robustness, as being the maximum '2-norm margin classifier under
worst-case 'q-norm perturbation to the data. Our findings provide theoretical back-
ups for adversarial training that it indeed promotes robustness against adversarial
perturbation.
1	Introduction
Deep neural networks have achieved remarkable success on various tasks, including visual and speech
recognitions, with intriguing generalization abilities to unseen data (Krizhevsky et al., 2012; Hinton
et al., 2012). One salient feature of deep models is its overparameterization, with the number of
parameters several orders of magnitude larger than the training sample size. As a consequence of such
overparameterization, it is likely that the empirical loss function, in addition to being non-convex,
can have substantial amount of global minimizers (Choromanska et al., 2015), while only a small
subset of global minimizers have the desired generalization properties (Brutzkus et al., 2018).
Contrary to the worst-case reasoning above, researchers have observed that simple first-order algo-
rithm such as Stochastic Gradient Descent (SGD) 1, performs surprisingly well in practice, even
without any explicit regularization terms in the objective function (Zhang et al., 2017). Inspired
by classical computational learning theories, one plausible explanation of such a remarkable phe-
nomenon is that the training algorithm enjoys some implicit bias. That is, the training algorithm
tends to converge to certain kinds of solutions (Neyshabur et al., 2015b;c), and SGD converges to
low-capacity solutions with the desired generalization property (Brutzkus et al., 2018). Recently,
some exciting works have related the implicit bias to specific first-order algorithms (Wilson et al.,
1In conjunction with Dropout (Srivastava et al., 2014) and Batch Normalization (Ioffe and Szegedy, 2015)
1
Published as a conference paper at ICLR 2020
2017), stopping time (Hoffer et al., 2017), and optimization geometry (Gunasekar et al., 2018a;
Keskar et al., 2017). Some practical suggestions based on these findings have also been proposed to
further improve the generalization ability of deep networks (Neyshabur et al., 2015a).
Despite the aforementioned phenomenal success achieved by deep neural networks, it is observed that
adversarially constructed small perturbation to the input can potentially fool the network into making
wrong predictions with high confidence (Szegedy et al., 2014; Goodfellow et al., 2015). This issue
raises serious concerns about using neural network for some security-sensitive tasks (Papernot et al.,
2017). Researchers have devised various mechanisms to generate and defend against adversarial
perturbations (Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016; Carlini and Wagner, 2017;
Athalye et al., 2018; Xie et al., 2018; Papernot et al., 2016). However, most of the defense mechanisms
are heuristic or ad-hoc, which lack principled theoretical justification (Carlini and Wagner, 2016;
He et al., 2017). Inspired by literatures in robust optimization (Wald, 1939; Ben-Tal et al., 2009),
Feige et al. (2015); Madry et al. (2018) formalize the notion of achieving adversarial robustness (i.e.,
having small adversarial risk) as solving the following minimax optimization problem
mRd LEdv (θ)=min	[max '(θ,X+δ, T，
(1)
where ∆ is the set that each sample could be contaminated by arbitrary perturbation chosen within
this set. As a common practice, adversarial training refers to the finite-sample empirical version of (1)
without access to the underlying distribution D that
N
min Ladv(θ)
θ∈Rd
min	max `(θ
θ∈Rd	δi ∈∆
i=1
, xi + δ, yi).
(2)
A commonly adopted approach to solving (2) is the the Gradient Descent based Adversarial Training
(GDAT) method. At each iteration, GDAT first solves the inner maximization problem (approxi-
mately) for adversarial perturbations, and then uses the gradient of the loss function evaluated at
the perturbed samples to perform a gradient descent step on the parameter θ. A natural question
is then how adversarial training helps the trained model in achieving adversarial robustness. Some
recent theoretical results partially answer this question, such as deriving adversarial risk bound
(Athalye et al., 2018), relating it to the distributionally robust optimization (Sinha et al., 2018), and
characterizing trade-offs between robustness and accuracy via regularization (Zhang et al., 2019).
Yet, all existing results neglect the algorithmic effect during the training process in promoting
adversarial robustness. Inspired by the significant role of algorithmic bias in the generalization of
neural networks, it is natural to ask
Does gradient descent based adversarial training enjoy any implicit bias property?
If so, does the implicit bias provide insights on how adversarial training promotes robustness?
Motivated by these questions, in this paper, we study the algorithmic effect of adversarial training by
investigating the implicit bias of GDAT. Due to current technical limits in directly analyzing deep
neural networks, we analyze a simpler model, with the key characteristics that the model overfits
the training data while being able to generalize well. Specifically, we take the binary classification
with linearly separable data as an example. This helps us focus on the effect of implicit bias without
dealing with complicated structures of neural networks.
Main Contributions. We summarize our main theoretical findings below.
•	Our first part of result shows an interesting interplay between adversarial perturbation and implicit
bias of the gradient descent (GD). By exploiting this interplay, we show a property of adversarial
training that is not known in the literature before: adversarial training accelerates convergence.
Specifically, when the perturbation is bounded by '2-norm, i.e., ∆ = {δ ∈ Rd : ∣∣δ∣∣2 ≤ c}, with
proper choice of c, the gradient descent based adversarial training is directionally convergent that
θt
lιmt→∞ ∣∣θt∣∣2 = u2, where u2 is the maximum '2-norm margin hyperplane (i.e., standard SVM)
of the training data. In addition, when the perturbation level c is set according to T appropriately,
the rate of convergence is Oe(1/√T)2, which is exponentially faster than the rate O (1/ log T) when
we use standard clean training, i.e., training with clean data using gradient descent. Based on this,
we establish that the convergence of training loss on clean data using GDAT is almost exponentially
faster than standard clean training using GD.
2 Oe hides logarithmic factor.
2
Published as a conference paper at ICLR 2020
•	Our second part of result shows that adversarial training adapts the implicit bias of gradient descent
for different adversarial perturbation geometry. Specifically, when the perturbation is bounded
by 'q-norm for q ≥ 1, i.e., ∆ = {δ ∈ Rd : ∣∣δ∣∣q ≤ c}, with proper choice of c, the gradient
θt
descent based adversarial training is directionally convergent that limt→∞ 情而=u2,q, where
u2,q is the maximum mixed-norm margin hyperplane of the training data. We further reveal natural
interpretation of robustness that we obtain the maximum '2-norm margin classifier under worst-case
'q -norm perturbation.
Notations. For two vectors x, y ∈ Rd, hx, yi = Pjd=1 xjyj denotes their Euclidean inner product.
For a vector θ ∈ Rd, ∣∣θ∣∣p defined by ∣∣θ∣∣p = Pd=ι ∣θj∣p denotes its p-norm forP ∈ [1, ∞), and
∣∣θ∣∣∞ = maxj∈[d] ∣θj|, where [d] = {1,...,d}. For any general norm ∣∣∙∣∣,we denote its dual norm
by ||x||* = max∣∣y∣∣≤ι hx, y〉. The sign function is Sign(V) = I(v≥0) - l(v<o). For a linear subspace
L ∈ Rd, we denote its orthogonal subspace by L⊥.
2	Background
We consider a binary classification problem using a dataset S = {(xi, yi)}in=1 ⊂ Rd × {-1, +1}. We
aim to learn a linear decision boundary f(x) = hθ, xi and its associated classifier yb(x) = sign (f (x)),
by solving the empirical risk minimization problem:
n
min L(θ; S) = min £'(yix>θ), where '(∙) is some loss function.	(3)
∈	∈	i=1
In what follows, we suppress the explicit presentation of S when the context is clear, and we focus on
the exponential loss '(r) = exp(-r). We point out that our analysis can be further extended to other
smooth loss functions with tight exponential tail such as logistic loss.
We assume the dataset S is linearly separable, i.e., there exists u such that mi□i∈[n] yiχ>u > 0.
Under this assumption, one notable feature of problem (3) is that there is no finite minimizer, and
L(θ) → 0 only if ∣∣θ∣∣2 → ∞ along certain directions. In fact, there is a polyhedral cone C, such that
for any U ∈ C, we have lim0→∞ L(au) = 0.
Several recent results have studied the implicit bias of gradient descent algorithm on separable dataset.
Soudry et al. (2018) study the implicit bias of the gradient descent algorithm (GD) on (3), and
show that limt→∞ ∣∣θt∣∣2 = ∞, while θt converges in direction to the maximum '2-norm margin
classifier (i.e., the standard SVM). Ji and Telgarsky (2018) further study the convergence of risk and
parameter without separability condition. (Ji and Telgarsky, 2019) and (Gunasekar et al., 2018b)
study the implicit bias for training deep linear network and linear convolutional networks, respectively.
Gunasekar et al. (2018a) also analyze the implicit bias of steepest descent in general norm ∣∣∙∣∣, and
show that θt converges in direction to the maximum || ∙ ∣∣*-norm margin hyperplane.
Throughout this paper, we assume the perturbation set is an 'q-normball with radius c, i.e., ∆ = {δ ∈
Rd ： l∣δ∣∣q 6 c}. Under the general framework of adversarial training in (2), we aim to minimize the
empirical adversarial risk	n
min Ladv(θ) = min — Tmaxexp (—yi(Xi + δi)τθ) .	(4)
θ∈Rd	θ∈Rd n	δi∈∆
i=1
Note that, given any θ, the inner maximization problem in (4) admits a closed form solution. Then the
gradient descent based adversarial training (GDAT) algorithm runs iteratively that at the t-th iteration,
we first solve the inner maximization problem by deriving the worst adversarial perturbation of each
sample. It is not difficult to see that for each sample, the worst perturbation is δi = cy%δt, where δJ==
argminδ(∣δ∣∣q≤ι hδ,θt). Then, letting each sample,s perturbed counterpart be (xtt,yi) = (Xi + δi,yi),
we take gradient of the loss function evaluated at the perturbed samples and perform a gradient descent
step, i.e., θt+1 = θt — ηtVθL (θt; {(鹫,yi)}n=ι)), where ηt > 0 is some prespecified stepsize. We
present the outline of GDAT in Algorithm 1.
3	Theoretical Results
In this section, we show that the GDAT algorithm possesses implicit bias, which depends on the
perturbation set during training. We provide explicit characterization of the implicit bias, and further
conclude that such implicit bias indeed promotes robustness against adversarial perturbation.
3
Published as a conference paper at ICLR 2020
Let us start with some definitions. Con-
sider a dataset S = {(xi, yi)}in=1 ⊂ Rd ×
{—1, +1}. Given p, q > 0 such that 1/p +
1/q = 1, the 'q-norm margin of H on S is
defined as Yq(θ) = mi%∈[n] yχ>θ∕∣∣θ∣∣p.
Note that for Xi ∈ Rd, ∣θ>x∣∕∣∣θ∣∣p mea-
sures the 'q distance between xi and the
hyperplane Hθ = {x ∈ Rd : θ>x = 0}.
Since yi ∈ {—1, +1}, when Hθ correctly
classifies all samples, Yq(θ) measures the
minimal 'q distance between the samples
in S and Hθ. Given that Yq(θ) is scale-
invariant with respect to θ, without loss of
generality, We restrict ∣∣θ∣∣p = 1. We also
Algorithm 1 Gradient Descent based Adversarial Train-
ing (GDAT) With 'q-norm Perturbation
Input: Number of iterations T , perturbation level c,
stepsizes {ηt}tT=0, samples {xi, yi}in=1.
Initialize: θ0 - 0.
for t = 0, . . . , T — 1 do
for i = 1, . . . , n do
Compute δt = Cyi argmin∣∣δ∣∣q≤ι hδ,θti
Let (et,yi) — (Xi + δt,yi).
θt+1 - θt — ηt Pn=I exp (—yie>θt) (—yiXi).
end for
identify the hyperplane Hθ by its normal vector θ.
Definition 3.1. For p, q > 0 with 1/p +1/q = 1, the maximum 'q-norm margin hyperplane Uq of
S = {(xi, yi)}n=ι ⊂ Rd X { — 1, +1} and its associated 'q-norm margin Yq are defined as
uq ∈ argmax min yixi>θ, γq = max min yixi>θ.
∣∣θ∣∣p = 1 i∈[n]	i	l∣θ∣∣P=1 i∈[n]	i
(5)
We denote SV(S) as the support vectors of S, i.e., SV(S) = argmin(x,y)∈S huq, yxi.
By the separability assumption, uq is an optimal hyperplane that correctly classifies all samples
With the maximal margin Yq > 0. Next, by the notion of margin defined above, We characterize the
landscape of empirical adversarial risk in (4) based on the perturbation level c.
Proposition 3.1. Let p, q > 0 satisfy 1/p + 1/q = 1. Given a nonnegative scalar c, where
0 ≤ c < γq = max∣∣θ∣∣ ≤ι mini∈[n] yiX> θ, problem (4) has infimum 0 but does not admit a finite
minimizer. When c > Yq, problem (4) has a unique finite minimizer θ(c), and is equivalent to the
standard clean training with explicit 'p-norm regularization. That is, there exists λ(c) > 0 such that
n
1>
θ(c) = argmax -〉exp(—yiXi θ) + λ(c)∣∣θ∣∣p.
θ∈Rd n i=1	i
It is not difficult to see that for c < γq, any perturbed dataset S = {(xei, yi)}in=1, with ||xi — xei||q 6 c
for all i, is still linearly separable, which directly follows from the definition of γq above. On the other
hand, when c > γq, by the definition of γq, there exists some perturbed dataset S = {(xei, yi)}in=1,
with ||xi — xei ||q 6 c for all i, such that Se is no longer linearly separable.
3.1	ADVERSARIAL PERTURBATION WITH BOUNDED '2 -NORM
In this subsection, We analyze both the empirical adversarial risk convergence and the parameter
convergence of the case When the perturbation set ∆ in (4) is an '2-norm ball With radius c.
Adversarial Risk Convergence. We first analyze the convergence of empirical adversarial risk (4)
using GDAT. One substantial roadblock of minimizing (4) is its non-smoothness, in the sense that
Ladv(θ) is not differentiable at the origin, and its Hessian V2Ladv(θ) explodes around the origin. To
address the challenge, our key observation is that, by the next lemma, at each iteration, there exists an
acute angle betWeen the update on θt and the maximum '2-norm margin hyperplane u2. This gives a
lower bound on || θt || 2.
Lemma 3.1. Take ∆ =	{δ	∈	Rd	:	∣∣δ∣∣2	≤	c}	in problem (4).	Given C <	γ2,	we have that
h-VLadv(0),U2)≥ Ladv(θ)(γ2 — C) > 0fforany θ ∈ Rd.
We highlight that despite its simple proof, Lemma 3.1 and its generalization to 'q-perturbation is a
crucial step for analyzing both adversarial risk and implicit bias. In addition, our techniques here can
also be adapted to simplify the proof of Lemma 10 in (Gunasekar et al., 2018a), which, in comparison,
is more technically involved.
Since we initialize GDAT (Alg. 1) using θ0 = 0, any perturbation inside ∆ will have no effect on
the adversarial loss. Hence we take clean samples as adversarial examples at the first iteration of
4
Published as a conference paper at ICLR 2020
GDAT. From Lemma 3.1, we have the following simple corollary showing that our whole solution
path {θt }tT=1 is bounded away from the origin.
Corollary 3.1. Let θ0 = 0 in Algorithm 1 with q = 2, we have: ∣∣θt∣∣2 ≥ η0γ2 for all t ≥ 1.
By Corollary 3.1, we bypass the non-differentiability issue at the origin and also control the Hessian
V2Ladv(θ) throughout the entire training process. Similar to (Ji and Telgarsky, 2018), in the next
theorem, we show that the loss Ladv(θ), although not uniformly smooth, is locally Ladv(θ)-smooth.
Consequently, by the smoothness based analysis of the gradient descent algorithm, we establish the
convergence of the empirical adversarial risk.
Theorem 3.1. Suppose ∣∣Xi∣∣2 ≤ 1 for all i = 1 ...n. For GDAT (Alg. 1)with '2 -norm perturbation,
i.e., ∆ = {δ ∈ Rd : ∣∣δ∣∣2 6 c}, we set c < γ2, η0 = 1 andηt = η ≤ min{ (i+c^Y2/lcq+c), 1} for
t ≥ 1, then we have
-X maχexp (—yi(Xi + δi)>θt) = O ( Jog t ) .	(6)
n i=1 δi∈∆	tη(γ2 - c)2
In comparison with the standard clean training using GD (Ji and Telgarsky, 2018), this theorem states
that we pay an extra (γ2 — c)-2 factor in the risk convergence of adversarial training. However,
this direct comparison is too pessimistic since we compare the adversarial risk with the standard
risk (corresponding to ∆ = {0}). Interestingly, as seen later in Corollary 3.2, we prove that the
convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean
training using GD.
Parameter Convergence. We then show that if we set the perturbation level c < γ2 in the GDAT
algorithm, GDAT with '2-norm perturbation possesses the same implicit bias as the standard clean
training using GD, i.e., We have limt→∞ 温∣2 = u2. Intuitively, GDAT with '2-norm perturbation
searches for a decision hyperplane that is robust to '2-norm perturbation. Since the learned decision
hyperplane in the standard clean using GD converges to u2 , which is already the most robust decision
hyperplane against '2-norm perturbation to the data, GDAT retains the implicit bias of standard clean
training using GD.
Surprisingly, even though both GDAT in the adversarial training and GD in the standard clean training
converge in directions to u2, their rates of directional convergence are significantly different as shown
later. Specifically, letting the perturbation level c depend on the total number of iterations T in the
GDAT algorithm, the directional error after T iterations in GDAT algorithm can be significantly
smaller than the error of GD in the standard clean training.
We first show that the projection of θt onto the orthogonal subspace of span(u2) is bounded.
Lemma 3.2. Define α(S) = min∣∣ξ∣∣2=1,ξ∈span(u2)⊥ max(χ,y)∈sv(s) hξ,yxi, where we assume
SV(S) spans Rd. Let θ⊥ be the projection of vector θ onto span(u2)⊥. Then there exists a constant K
that only depends on a(S) and log n, such that ∣∣θ⊥ ∣∣2 ≤ K forany t ≥ 0 in the GDAT algorithm.
Note that the same α(S) is defined in (Ji and Telgarsky, 2019) and proved to be positive with
probability 1 if the data is sampled from absolutely continuous distribution. We then show in the next
lemma that ∣∣θt∣∣2 goes to infinity, where we provide a refined analysis to establish the acceleration of
the directional convergence in comparison with the standard clean training.
Lemma 3.3. Under the same conditions in Theorem 3.1, and let α = α(S) defined in Lemma 3.2.
Then for all t ≥ 0, we have
l∣θt∣∣2 ≥ log
tη(γ2 — c)2
n1+1/a log2 t
/(γ2 — c).
Lemma 3.3 provides the key insight to establish the acceleration of directional convergence. Specifi-
cally, it allows us to set C depending on the total number of iterations T, so that || θτ || 2 is sublinear in
T, in comparison with being logarithmic in T in standard clean training as in Ji and Telgarsky (2018).
We are now ready to present the main theorem for parameter convergence.
Theorem 3.2 (Speed-up of Parameter Convergence). Under same conditions in Theorem 3.1, and
let α = α(S) and K be defined in Lemma 3.2. In GDAT with '2-norm perturbation, let c and total
5
Published as a conference paper at ICLR 2020
1 1 + 1/ ai E、1/2	—T	八 T
number of iterations T satisfy γ2 一 C = (n——”10 )	, and define θ = ∣∣^θT ∣∣2 ∙ We have
1 - GT ,U2)= O
n(1+1/a)/2K log τ
√η√τ
(7)
One might argue that the polynomial dependence on sample size n in (7) is too pessimistic, making
the GDAT unfavorable in comparison with the standard clean training. We show that this is not an
-77T
issue by a direct ComParision of iteration complexity to achieve ∣∣θ 一 u2∣∣2 ≤ E fora given precision
e > 0. Specifically, given e > 0, to achieve ∣∣ΘT 一 u2∣∣2 ≤ e, GDAT needs O (n(1+1/a)€-2) number
of iterations. In comparison, the standard clean training by GD needs Oe nexp E-1 number of
iterations (Ji and Telgarsky, 2018), which has exponential dependence on precision E.
Finally, by Theorem 3.1 and Lemma 3.3, we show that the empirical clean risk after T iterations of
GDAT is almost exponentially smaller than its counterpart in the standard clean training.
Corollary 3.2 (Speed-up of Clean Risk Convergence). Under the same conditions in Theorem 3.2,
we have
L(ΘT) = O (exp (-μ√T∕log T)),
where μ is a constant dependent on η, α, n.
Note that the empirical clean risk decreases at the rate of O 卜xp(-√T)) up to a logarithmic factor in
the exponent. In comparison, using standard clean training with GD, we only have L(θT) = O (1/T)
(Soudry et al., 2018).
3.2	Adversarial Perturbation with Bounded 'q-Norm
In this subsection, we generalize our results to the case where the perturbation set is some bounded
'q -norm ball. To facilitate our discussion, We first define a robust version of SVM.
Definition 3.2. For a given separable dataset S with 'q-norm margin Yq and C < Yq, letting
1/p + 1/q = 1, the robust SVM against 'q-norm perturbation parameterized by C is
min 1 l∣θ∣∣2 s.t. yix>θ ≥ c∣∣θ∣∣p + 1,∀i =1,...,n.	(8)
θ∈Rd 2
Remark 3.1 (Maximum Mixed-norm Margin). Note that problem (8) is equivalent to solving for
a maximum mixed-norm margin hyperplane. Specifically, by the KKT condition of (8), there exists
η(C) > 0, such that (8) is equivalent to the following problem:
min ∣∣θ∣∣2 + η(c)∣∣θ∣∣p s.t. yx>θ ≥ 1,∀i = 1,... ,n.	(9)
θ∈Rd
Now define || ∙ || = || ∙ ∣∣2 + η(c)∣∣ ∙ ||p, it is clear that ∣∣∙∣∣ defines a norm which is a mixture of
'2 and 'p norm. Let || ∙ ||* be its dual norm. Then we have that the solution to (9) is the maximum
|| ∙ ||*-norm margin hyperplane.
Note that the constraint in (8) is equivalent to min∣∣δi∣∣q ≤c yi(xi + δi)τθ ≥ 1, ∀i = 1,... ,n. By a
simple scaling argument, in the folloWing lemma, We see the robust nature of (8).
Lemma 3.4. Under the same notations in Definition 3.2, problem (8) is equivalent to:
Y2,q(C)
max min min yi(xi + δi)τθ.
l∣θ∣∣2 = 1 i∈[n] I∣δi∣∣q6c
(10)
We denote the (unique) solution to problem (10) as u2,q(C). in What folloWs, We surpress explicit
presentation of C When the context is clear.
The equivalent formulation (10) provides a clear interpretation on the robustness of (10). in particular,
the robust SVM against 'q-norm perturbation parameterized by C is in fact the SVM problem on the
the dataset S(C, q), Which is generated from S by placing a 'q-norm ball With radius C around each
samples, i.e., S(C, q) = {(x, y) : ∃i ∈ [n], s.t., ||x 一 xi||p 6 C, y = yi}. in other Words, u2,q is the
maximum '2 -norm margin classifier under Worst case 'q-norm perturbation bounded by C.
in the remaining part of this section, We first analyze the convergence of the empirical adversarial
risk, and then establish the implicit bias of GDAT With 'q perturbation for q ∈ [1, ∞]. our analysis
6
Published as a conference paper at ICLR 2020
for q ∈ {1, ∞} is based on approximation argument. For ease of presentation, we only discuss when
q ∈ (1, ∞) in the main text, and defer the discussion for q ∈ {1, ∞} in Appendix D.
Adversarial Risk Convergence. Our analysis is similar to the analysis for GDAT with `2 perturba-
tion, where we use similar techniques to address issues such as non-differentiability at the origin and
Hessian explosion of Ladv (θ) around the origin.
Theorem 3.3. Suppose ∣∣Xi∣∣2 ≤ 1 for i = 1,...,n, and let P + 1 = 1. In the GDATwith 'q-norm
perturbation, setting c < Yq and letting MP = [(1 + c√d)2 + CY-I) d2p-2j exp (一γ2,q + c√d^,
set η0 = 1 and ηt = η ≤ min{ M, 1} for t ≥ 1. We have that
1n
一EmaxexP (-ygi + δi)τθt) = O
n	δi∈∆
i=1
(11)
We point out here that (6) is a special case of (11). In particular, by the definition of γ2,q (c), we have
that γ2,2(c) = γ2 - c, which recovers bound (6) from (11).
Parameter Convergence. We show that if we set c < γq in the GDAT algorithm with stepsizes
specified in Theorem 3.3, with 'q perturbation, the algorithm still possesses implicit bias property,
i.e., θt still has directional convergence, and the limiting direction depends on the perturbation set ∆.
Theorem 3.4 (Implicit Bias of GDAT with 'q-norm Perturbation). Under the same conditions in
Theorem 3.3, define θ
θt
ΠR∣2,
then we have:
1-
(θt,u2,q
E Y)
Combining Theorem 3.4 and Lemma 3.4, we conclude that GDAT with 'q-norm perturbation indeed
promotes robustness against 'q perturbation. Using GDAT with 'q-norm perturbation will result in a
classifier which is the maximum '2-norm margin classifier under worst case 'q-norm perturbations to
the samples bounded by c. The learned classifier will have 'q-norm margin at least c. As we increase
perturbation level c to γq , the learned classifier will converge to maximum 'q-norm margin classifier.
4	Numerical Experiment
In this section, we first conduct numerical experiments on linear classifiers to backup our theoretical
findings. We further empirically extend our method to neural networks, where our numerical results
demonstrate that our theoretical results can be potentially generalized.
Linear Classifiers. We investigate the empirical performance of the GDAT algorithm on linear classi-
fiers, with training set S = {((-0.5, 1), +1) , ((-0.5, -1), -1) , ((-0.75, -1), -1) , ((2, 1), +1)}.
It is straightforward to verify that the maximum '2-norm margin classifier isu2 = (0, 1).
Considering '2-norm perturbations, we first run standard clean training with GD, and GDAT with
'2-norm perturbation (c = 0.95γ2), for 2.5 × 104 number of iterations. In both GD and GDAT
we take constant stepsizes, with η = 1 and η = 0.1, respectively. By Figure 1(a), we see that the
convergence rate of adversarial loss using GDAT is similar to the convergence rate of clean loss
using GD. However, when we directly compare the clean losses of GDAT and GD, GDAT clearly
demonstrates an exponential speed-up in comparison with GD, which is consistent with Corollary 3.2.
Additionally, as pointed out by Theorem 3.2, GDAT also enjoys significant speed-up in terms of
the directional convergence of θt to u2. We also compare the norm growth ∣∣θt∣∣2, and observe that
the norm generated by GDAT grows much faster than the norm generated by GD, which is also in
alignment with our discussions in Section 3.1.
We further run GDAT with '∞-norm perturbation (c = 0.5). By Lemma 3.4, we have that u2,∞ =
(0, 1). Note that the Hausdorff distance between 'q-norm ball and '∞-norm ball distance goes to
zero as q goes to infinity. Thus, we have that (10) for q = 1000 is a close approximation of (10) for
q = ∞. We run two versions of GDAT, where one uses 'q-norm perturbation with q = 1000, and
the other uses '∞-norm perturbation. We run both algorithms with stepsize η = 0.1 for 5.0 × 105
number of iterations, and we present the results in Figure 1(b). We find that the two training methods
behave similarly. In addition, the empirical directional convergence rates of θt just differ slightly.
Neural Networks. It is seen above that GDAT with '2-norm perturbations converges significantly
faster than GD for linear classifiers in adversarial training. A natural question is whether this is still the
7
Published as a conference paper at ICLR 2020
Clean Training
'2 Perturbation
(a). Clean Training v.s. GDAT ('2 perturbation)
(b). GDAT ('∞ perturbation) v.s. GDAT ('q perturbation)
Figure 1: GDAT of Linear Classifiers.
case on adversarial training of more complicated neural networks. We conduct experiments on neural
network with one hidden layer. We take the two classes from MNIST dataset with label “2" and “9" to
form our training setS. We also vary the width of the hidden layer in {64 × 64, 128 × 128, 256 × 256}.
One major difference from the case of linear classifiers is that we cannot solve the inner maximization
problem of (2) exactly as it does not admits a closed-form solution. Instead, we solve the inner
problem approximately using projected gradient descent with 20 iterations and stepsize 0.01. We test
two versions of GDAT, where one adopts '2-norm perturbations (C = 2.8), and the other uses '∞-
norm perturbations (c = 0.1). For standard clean training and the outer minimization problem in (2),
we use the stochastic gradient descent algorithm with batch size 128 and constant stepsize 10-5.
We compare the loss and classification accuracy, which are evaluated using the clean training
samples, of standard clean training and GDAT. By Figure 2, we see that GDAT indeed accelerates the
convergence of both loss and classification accuracy on clean training samples. The performance gap
is most obvious when the width of the hidden layer is small, and reduces gradually as we increase the
width of the hidden layer. We argue that such reduction comes from the fact that as network width
increases, the margin on the samples outputted by the hidden layer also increases. As suggested
by Theorem 3.2, in this case, a larger perturbation level c should be used. We conduct additional
experiments with various perturbation level in Appendix E to empirically verify our argument.
BaeP Ire-ɔ -SSOa
100	200 _ 300	400	100	200 _ 300	400	100	200 _ 300	400	100	200 _ 300	400
Iterations	Iterations	Iterations	Iterations
gdp ue-ɔ⅛ɔe,ɪnɔɔv
100	200 . 300	400
Iterations
(a). Width = 64 × 64
100	200 . 300	400	0	100	200 . 300	400
Iterations	Iterations
(b). Width = 128 × 128	(c). Width = 256 × 256
100	200 . 300	400
Iterations
(d). Width = 512 × 512
Figure 2: GDAT of Neural Network on MNIST Dataset.
8
Published as a conference paper at ICLR 2020
5	Discussions
We investigate the implicit bias of GDAT for linear classifier. There are several plausible natural
extensions. For example, we can represent a linear classifier using a deep linear network, which
is significantly overparameterized. Some recent results characterize the implicit bias of gradient
descent for training deep linear networks (Ji and Telgarsky, 2019) and linear convolutional networks
(Gunasekar et al., 2018b). Motivated by these results, investigating the implicit bias of GDAT in
training deep linear networks worths future investigations.
Meanwhile, investigating implicit bias in deep nonlinear networks is a more important and chal-
lenging direction: (1) For linear classifiers, adding adversarial perturbations during training can be
understood as a form of regularization, which explains the faster convergence in training. Although
observed empirically, the potential acceleration of adversarial training is not yet understood in the
current literature, to the best of our knowledge. (2) The notion of margin for neural networks still
lacks proper definition, which we need to define to facilitate investigations on the effect of adversarial
training in promoting robustness. (3) Ultrawide nonlinear networks have been shown to evolve
similarly to linear networks using gradient descent (Ghorbani et al., 2019; Lee et al., 2019). We shall
further investigate if our results on linear classifiers can be extended to wide nonlinear networks.
6	Acknowledgements
Fang is partially supported by NSF DMS-1820702 and NSF DMS-1953196.
References
Athalye, A., Carlini, N. and Wagner, D. (2018). Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In International Conference on
Machine Learning.
BEN-TAL, A., EL GHAOUI, L. and NEMIROVSKI, A. (2009). Robust Optimization, vol. 28. Princeton
University Press.
Brutzkus, A., Globerson, A., Malach, E. and S halev- S hwartz, S. (2018). SGD learns
over-parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations.
Carlini, N. and Wagner, D. (2016). Defensive distillation is not robust to adversarial examples.
arXiv preprint arXiv:1607.04311 .
Carlini, N. and Wagner, D. (2017). Towards evaluating the robustness of neural networks. In
2017 IEEE Symposium on Security and Privacy.
Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B. and LeCun, Y. (2015). The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics.
Feige, U., Mansour, Y. and Schapire, R. (2015). Learning and inference in the presence of
corrupted inputs. In Conference on Learning Theory.
Ghorbani, B., Mei, S., Misiakiewicz, T. and Montanari, A. (2019). Linearized two-layers
neural networks in high dimension. arXiv preprint arXiv:1904.12191 .
Goodfellow, I., Shlens, J. and Szegedy, C. (2015). Explaining and harnessing adversarial
examples. In International Conference on Learning Representations.
Gunasekar, S., Lee, J., S oudry, D. and Srebro, N. (2018a). Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning.
Gunasekar, S., Lee, J. D., Soudry, D. and Srebro, N. (2018b). Implicit bias of gradient
descent on linear convolutional networks. In Advances in Neural Information Processing Systems.
He, W., Wei, J., Chen, X., Carlini, N. and Song, D. (2017). Adversarial example defense:
Ensembles of weak defenses are not strong. In 11th USENIX Workshop on Offensive Technologies.
9
Published as a conference paper at ICLR 2020
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A.-r., Jaitly, N., Senior, A., Van-
houcke, V., Nguyen, P. and Kingsbury, B. (2012). Deep neural networks for acoustic
modeling in speech recognition. IEEE Signal Processing Magazine 29.
Hoffer, E., Hubara, I. and Soudry, D. (2017). Train longer, generalize better: closing the
generalization gap in large batch training of neural networks. In Advances in Neural Information
Processing Systems.
Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning.
JI, Z. and TELGARS KY, M. (2018). Risk and parameter convergence of logistic regression. arXiv
preprint arXiv:1803.07300 .
Ji, Z. and Telgarsky, M. (2019). Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations.
Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M. and Tang, P. T. P. (2017).
On large-batch training for deep learning: Generalization gap and sharp minima. In International
Conference on Learning Representations.
Krizhevsky, A., S uts kever, I. and Hinton, G. E. (2012). Imagenet classification with deep
convolutional neural networks. In Advances in Neural Information Processing Systems.
Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Sohl-Dickstein, J. and Pennington, J.
(2019). Wide neural networks of any depth evolve as linear models under gradient descent. arXiv
preprint arXiv:1902.06720 .
Madry, A., Makelov, A., Schmidt, L., Tsipras, D. and Vladu, A. (2018). Towards
deep learning models resistant to adversarial attacks. In International Conference on Learning
Representations.
Moosavi-Dezfooli, S.-M., Fawzi, A. and Frossard, P. (2016). Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition.
Neyshabur, B., S alakhutdinov, R. R. and Srebro, N. (2015a). Path-SGD: Path-normalized
optimization in deep neural networks. In Advances in Neural Information Processing Systems.
Neyshabur, B., Tomioka, R. and Srebro, N. (2015b). In search of the real inductive bias:
On the role of implicit regularization in deep learning. In International Conference on Learning
Representations.
Neyshabur, B., Tomioka, R. and Srebro, N. (2015c). Norm-based capacity control in neural
networks. In Conference on Learning Theory.
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B. and Swami, A. (2017).
Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia
Conference on Computer and Communications Security.
Papernot, N., McDaniel, P., Wu, X., Jha, S. and Swami, A. (2016). Distillation as a defense
to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security
and Privacy.
Sinha, A., Namkoong, H. and Duchi, J. (2018). Certifying some distributional robustness with
principled adversarial training. In International Conference on Learning Representations.
S oudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. and Srebro, N. (2018). The
implicit bias of gradient descent on separable data. The Journal of Machine Learning Research 19
2822-2878.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R.
(2014). Dropout: a simple way to prevent neural networks from overfitting. The Journal of
Machine Learning Research 15 1929-1958.
10
Published as a conference paper at ICLR 2020
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and
FERGUS, R. (2014). Intriguing properties of neural networks. In International Conference on
Learning Representations.
WALD, A. (1939). Contributions to the theory of statistical estimation and testing hypotheses. The
Annals OfMathematical Statistics 10 299-326.
Wilson, A. C., Roelofs, R., Stern, M., Srebro, N. and Recht, B. (2017). The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems.
Xie, C., Wu, Y., van der Maaten, L., Yuille, A. and He, K. (2018). Feature denoising for
improving adversarial robustness. arXiv preprint arXiv:1812.03411 .
Zhang, C., Bengio, S., Hardt, M., Recht, B. and Vinyals, O. (2017). Understanding deep
learning requires rethinking generalization. In International Conference on Learning Representa-
tions.
ZHANG, H., YU, Y., JIAO, J., XING, E. P., GHAOUI, L. E. and JORDAN, M. I. (2019). Theoretically
principled trade-off between robustness and accuracy. In International Conference on Machine
Learning.
A Proof of Proposition 3.1
Proof. Suppose c < γq . Letting θα = αuq for α > 0, we have
1n	>
LadV(Oa) = —): eχp (-yixi θα + c"θα ||p)
n i=1
1n
= —	exp (-ayix> Uq + Ca)
n i=1
1n
≤ —,/ exp (—aγq + Ca).
n i=1
Letting a → ∞, we obtain limα→∞ Ladv(θα) = 0, which implies inf θ∈Rd Ladv(θ) = 0. Note that
L(θ) does not admit any finite minimizer since Ladv(θ) > 0 for any θ ∈ Rd.
If c > γq ,by the definition of maximum 'q -norm margin, for any θ ∈ Rd, there exists (yi,xi) ∈ S
for some i ∈ [n] such that yix>θ 6 Yq∣∣θ∣∣p. Hence, Ladv(θ) > exp (n-1(c - Yq)∣∣θ∣∣p). Then it is
easy to see that Ladv(θ) has bounded sublevel set and hence a finite minimizer θ. Since Ladv(θ) is
convex, we examine its first-order KKT condition, given by
1n
一Xexp (-yiχ>θ + c∣∣θ∣∣p) (-yiXi + Cd||b||p) 3 0.
n i=1
Consider the regularized problem With regularization parameter η :
1n
miq —exp --yχ>θ) + η∣∣θ∣∣p.
θ∈Rd n
i=1
Its first-order KKT condition is
1n	>
n∑exp (-yiχ>θ) (-yiXi) + η∂∣∣θ∣∣p 3 0.
(12)
(13)
i=1
Looking at (12) and (13) together, by taking η = PZi exp (-yix>θ + c∣∣θ∣∣p), We have that the
solution to the adversarial training problem θ is also the solution to the regularized problem. □
To facilitate our later discussions, we point out that by the conjugacy of 'p-norm and 'q -norm, (4)
has the folloWing equivalent form that
1n
min Ladv(θ) = min — TeXP Jix> + c∣∣θ∣∣p).
θ∈Rd	θ∈Rd n
i=1
(14)
In fact, one can verify that the GDAT algorithm is equivalent to gradient descent algorithm on (14).
11
Published as a conference paper at ICLR 2020
B Proofs for Section 3.1
ProofofLemma 3.1. Recall we have Ladv(θ) = 1 ENi max∣∣δ∣∣2<c exp (-yi(χ% + δi)τθ). For
each sample (χi,yi) ∈ S, given a classifier θ, the worse case perturbation is δi =
argmax∣∣δ∣∣2<c exp (-yi(xi + δ)τθ) = argmin∣∣δ∣∣2<c yiδτθ. The corresponding loss is Ladv(θ)=
1 PNI exp (-yi(xi + δi)τθ)
Since for a fixed δi, the function exp (-yi(χi + δi)τθ) is convex in θ, hence the gradient of Ladv(θ)
is
1 n	〜	〜
-VLadv(θ) = — X : exp (-yi (Xi + δi) θ) yi (Xi + δi) ∙
n i=1
Then from the definition of u2 (5), we have
<-VLadv(θ),U2> = X exp
i=i
-yi(xi + ei)τθ)(yi(Xi + δi), U2,
(15)
n
≥ Eexp (-yi(xi + ei)τθ^) (hyiXi,u2i - c)	(16)
i=i
n
≥ X exp (-yi(Xi + ei)τθ) (γ2 — C)= Ladv(θ)(γ2 — C),	(17)
i=i
1	∙ .<	-I ∙	1 ∙ . 1 11 ∙	IlUI I ,	111 Il	T	I-I
where in the second inequality holds since ∣∣δ∕∣2 6 C and ∣∣u2∣∣2 = 1.	□
ProofofCorollary C.1. Since Ladv(θ) is not differentiable at θ0 = 0, we use subgradient (note that
Ladv(θ) is convex) at 0. Specifically, we take VLadv(θ0) = 1 Pn=Izi ∈ ∂Ladv(θ0). Then we have
(θ1,U2) = ηn Pi hzi, u2)≥ η0γ2, where the last inequality uses the definition of γ2.
By Lemma 3.1, we have hθt,如)≥ η0γ2 for all t ≥ 1, which also implieshv,犯〉≥ η0γ2 and hence
∣∣vt∣∣2 ≥ η0γ2 for V ∈ [θt,θt+1].	□
ProofofTheorem 3.1. For simplicity, we let Zi = yiXi, where we have ∣∣zi∣∣2 ≤ 1 as we assume
IlXiIl2 ≤ 1. We have
1n	θ
VLadv(θ) = — Eexp (-zi θ + c∣∣θ∣∣2) (-zi + cii^η-),
n M	∖	∣∣θ∣∣2√
1n
V2Ladv(θ) = n EeXP (-zjθ + c∣∣θ∣∣2)
i=1
1n
+	EeXP (-zJθ+c∣∣θ∣∣2
n z—z
i=1
1n
二 n EeXP (-zJθ+c∣∣θ∣∣2)
i=1
Cz>θ
ZiZJ - 2 ⅛ + c2θθτ∕∣∣θ∣∣2 + cI∕∣∣θ∣∣2-cθθτ∕∣∣θ∣H∙
Note that the Hessian expression indicates that the objective is highly non-smooth around origin, and
the loss is not even differentiable at origin. However, we shall prove that starting from origin, every
iteration generated by GADT stays away from the origin with distance bounded below.
Using Taylor,s expansion, and by definition θt+1 = θt - ηtVLadv(θt), we have
Ladv(θt+1) ≤ Ladv(θt) - ηt∣∣VLadv(θt)∣∣2 +
(ηt ∣∣VLadv (θt)∣∣)2
where λ(H(V))max denotes the largest eigenvalue of H(v), where
max λ(H(v))max, (18)
v∈[θt,θt+1]
/、 1 J / τ ll ll ʌ
H(v) = — Enexp (-zτv + c ∣ ∣ v 11 2)
n i=1
-	T	-
ZiZJ - 21⅛ + c2vvτ∕ ∣ ∣ V ∣ ∣2 + 四 ∣ ∣ V ∣ ∣2 -cvvτ∕ ∣ ∣ V ∣ ∣ ；
To upper bound H(v), we need a lower bound on ∣∣v∣∣, which is readily given by Corollary C.1. That
is, ∣∣v∣∣2 ≥ η0γ2.
2
12
Published as a conference paper at ICLR 2020
We now analyze (18) for t ≥ 1, where We show that Ladv(θt) is locally smooth with parameter
proportional to Ladv(θt), and with proper stepsize, the risk is monotonely decreasing. Note that
ziz[ ≤ I, -2ciz>∣v ≤ 2cI, c2vv>/||v||| ≤ c2I. Now since ∣∣vt∣∣2 ≥ η0γ2, wehave cl∕∣∣v∣∣2 -
cvv>/||v||3 ≤ η⅛^∙I. Plugging them in, we have
H(V) ≤ 1 Xexp (-z>v + c∣∣v∣∣2) (1 + 2C + c2 + -2c-)I
n -	∖	η0Y2)
=LadV (V) (1+2c + C +
and (18) reduces to
Ladv(θt+1) ≤Ladv(θt) — ηt∣∣VLadv(θt)∣∣2
+ (η'UVLadVQ)||)2 Γ(1 + c)2 + 2s] max {Ladv(θt), Ladv(θt+1)} .	(19)
2 L	η0Y2 J
Suppose Ladv(θt+1) > Ladv(θt), and let M = [(1 + c)2 + 言].We have
Ladv(θt+1) ≤ Ladv(θt) — ηt ∣∣VLadv (θt)∣∣2 + (" "VLadVG) ” )2 M Ladv(θt+1),
which implies
Ladv(θt+1) ≤ (1 - Mnɪ∣∣VLadv(θt)∣∣2)	(Ladv(θt) - ηt∣∣VLadv(θt)∣∣2) .	(20)
Meanwhile, if we choose ηt satisfying
ηtM = ηtLadv(θt) (1 + c)2 + 行 ≤ 1,	(21)
then we have the right hand side of (20) is upper bounded by Ladv(θt), and we have
Ladv(θt+1) ≤ (1 - M^ ∣∣VLadv(θt)∣∣2)	(Ladv(θt) - ηt∣∣VLadv(θt)∣∣2) < Ladv(θt),
which is clearly a contradiction. Hence, if ηt satisfies (21), by (19) we have
Ladv (θt+1) ≤ Ladv (θt) - ηt∣∣VLadv(θt)∣∣2 十(""VLadV(Ot) ”	Γ(1 + c)2 + ^ 1 Ladv(θt)
2 L	η072.
≤ Ladv(θt) - η2t ∣∣VLadv(θt)∣∣2,	(22)
where the last inequality holds by the choice of ηt in (21).
Note that if (21) holds for t = 1 for η1 = η, by induction it is easy to see that with constant
stepsize ηt = η for t ≥ 1, (21) holds for all t ≥ 1. Hence for t ≥ 1, we choose stepsize η such
that ηLadV(θ1) [(1 + c)2 + n2γ2i ≤ 1. Note that LadV(θ1) = ɪ Pn=I exp (-zJθ1 + c∣∣θ1∣∣2) ≤
exp ((1 + c)η0) since ∣∣θ1∣∣ ≤ η0. Then we only require
η ≤ exp (-(1 + c)η0) ∙ c	^η γ2Q
(1 + c)2η0γ2 + 2c
exp (-(1 + c)η0)
η0(1 + c)γ2∕(1 + C)
(1 + c)2η0γ2 + 2c
≤_________γ2∕e___________
—(1 + c)3γ2 + 2c(1 + c)，
where in the last inequality we take η0 = 1 and use basic inequality exp(-x)x ≤ e-1 for x ≥ 1. In
summary, we choose η0 = 1 and ηt = η = min{ (1+0)3^+,0(1+^), 1} for t ≥ 1, then by previous
argument, we have (22) holds for all t ≥ 1.
Now we are ready to apply the standard smoothness-based analysis of gradient descent using (22),
take any θ ∈ Rd, we have
∣∣θt+1 - θ∣∣2 = ∣∣θt - θ∣∣2 - 2ηt (VLadv(θt),θt - θ> + (ηt)2∣∣VLadv(θt)∣∣2
≤ ∣∣θt	- θ∣∣2 - 2ηt	(Ladv(θt)	- Ladv(θ))	+	(ηt)2∣∣VLadv(θt)∣∣2
≤ ∣∣θt	- θ∣∣2 - 2ηt	(Ladv(θt)	- Ladv(θ))	+	2ηt (Ladv(θt) - Ladv(θt+1))
=∣∣θt - θ∣∣2 - 2ηt (Ladv(θt+1) - Ladv(θ)),
13
Published as a conference paper at ICLR 2020
where the first inequality holds by the convexity of Ladv(θ), and the second inequality holds by (22).
Now sum up the above inequality from S = 1 to t — 1. By ηt = η ≤ 1 = η0 and Ladv(θs+1) ≤
Ladv(θs), we have
Ladv(θt) 一 LadV⑹ ≤ ^-∖∖θ1 — θlli ≤ τ~ (llθlli ÷ llθ1lli) ∙
2tη	tη
Now since θ is arbitrary, letting θ = log-) ∙ m, we have
llθlli÷llθ1 lli ≤ U-NA÷(1÷c)i,
and
Ladv(θ) = 1 XX exp JUi ∙ 3 ÷ c ∙
n =	∖	Yi —C
1
≤ t,
which yields
LadV(θt) ≤ t ÷(*-⅛ ÷(1÷c)i)
□
ProofofLemma 3.2. For simplicity, we let Zi = yixi and '(θ) = exp (-z>θ ÷ dl/li). Define
α = min max hξ, zi)
l∣ξ∣∣2=1,ξ∈span(u2)⊥ i∈sv(S)
where SV(S) denotes the set of support vectors. It has been shown in Ji and Telgarsky (2019) (Lemma
2.10)thatα > 0 with probability 1 if the data is sampled from absolutely continuous distribution.
We have
(VLadv(θt),θ⊥> = n (X exp (—z>θt ÷ cllθtlli) J ÷ Cɪɪ) ,θ⊥)
1	ʌ	1 JL /	θt	∖
=n X 'i(θ M-%θ⊥> ÷n Σ 'i(θ K m 电)
1	ʌ ..
≥	n X 'Mθ)〈-% θ⊥>
≥	n 'j(θt)〈—zj,θ⊥> ÷	X	'i(θt)〈—Zi,θ⊥> ,	(23)
hzi,θ⊥ i≥0,i=j
where Zj ∈ S is arbitrary, by definition of α:(一zj, θ⊥) ≥ αllθ⊥lli.
We bound the first term as
'j(θt)〈—zj, θ⊥> ≥ exp (一(Zj)Tθt ÷ cllθtlli)。同g
=	exp (—(Zj)Tθ⊥ — (Zj)TθU2 ÷ cllθtlli) αllθ⊥g
≥	exp (— (θt,γiUi)) exp (αllθ⊥g) allθ⊥g exp (cllθtlli),
where the second inequality uses〈Zj, uQ ≥ ”
On the other hand, we can bound the second term in (23) as
1 X 'i(θt)〈—Zi, θ⊥> ≥ n	X	exp ( —ZTθt ÷ dQMi)〈—Zi, θ⊥)
{zi ,θ⊥ i≥0,i=j	{zi,θ⊥ i≥0,i=j
=n	X	exp (—ZT θu 2— z> θ⊥ ÷ d^tWi)〈—% θ⊥)
{zi,θt i≥0,i=j
≥ exp (— <θt, γiUi>) expSlWtllGexpJZT θ⊥)〈—Zi, θ⊥ )
≥ exp (一 <θt, γiUi)) exp(c||θt||i)( —；),
14
Published as a conference paper at ICLR 2020
where in the last inequality holds since hθt, U2)≥ 0,〈zM θtu j = z> (u>θt) u2 ≥ γ2 Gl U2)and
—x exp(-x) ≥ — e for x ≥ 0.
Plugging the two bounds above into (23), we have
(VLadv(θt),θ⊥> ≥ exp (-<θt,γ2"2)) exp(c∣∣θt∣∣2) ∣-exp (α∣∣θ⊥∣∣2) α∣∣θ⊥∣∣2 - ɪ ,
which is non-negative when ∣∣θ⊥∣∣2 ≥ K0 = 1+^gn.
Supposing ∣∣θ⊥∣∣2 ≥ K0, by gradient descent update, we have,
∣∣θ及1∣∣2 = ∣∣θ⊥∣∣2 - 2ηt (VLadv(θt),θ⊥> + (ηt)2∣∣VLadv(θt)∣∣2
≤ ∣∣θ⊥∣∣2 + 2ηt∣∣VLadv(θt)∣∣2
≤ ∣∣θ⊥ ∣∣2 +2 (Ladv(θt) -Ladv(θt+1)) ,	(24)
where the last inequality uses (22).
Now let t0 satisfy ∣∣θt0-1∣∣2 < K0 and ∣∣θt0-1∣∣2 ≥ K0. Define tι = min{s ≥ t0 : ∣∣θ⊥∣∣2 < K0},
when ∣∣θ⊥∣∣2 ≥ K0 for all S ≥ t° we define t1 = ∞. That is for any t ∈ {t0,..., tι - 1}, we have
∣∣θ⊥ ∣∣2 ≥ K0. then for any S such that to ≤ s < t1, summing (24) up from t° to S - 1 yields:
∣∣θ⊥ ∣∣2 ≤ ∣∣θt0 ∣∣2 +2 (Ladv(θt0 )-Ladv(θs))
≤ ∣∣θ⊥ ∣∣2 +2exp(1 + c)
≤∣∣θ⊥ ∣∣2 + 18,
where we use Ladv(θt) ≤ Ladv(θ1) ≤ exp(1 + C) and c < 1. This inequality shows that for
θt ∈{θt0 ,∙..,θt1-1}c{θ : ∣∣θ⊥∣∣2 ≥ K 0},
∣∣θ⊥ ∣∣2 ≤∣∣θf ∣∣2 + 18.
Then, we only need to bound ∣∣θf ∣∣2 to conclude the proof, where t° is the first time θt enters
(θ : ∣∣θ⊥∣∣2 ≥ K0}. Wehave
θ⊥ = θt0-1 + ηt0-1p⊥ (1 X仅即T)(Zi-C∣∣θ⅛)),
where P⊥(∙) denotes the projection onto SPan(U2)⊥. Note that to is the first time θt (re)-enters the
region {θ : ∣∣θ⊥∣∣2 ≥ K0}, and thus ∣∣θt0-1∣∣2 < K0. We have
∣∣θf ∣∣2 ≤ K0 + ηt0-1(1 + c) ≤ K0 + 1 + c<K0 + 2,
where the last inequality we use c < γ2 ≤ 1.
In summary, we have shown that for any t such that ∣∣θ⊥ ∣∣2 ≥ K0, we have ∣∣θ⊥ ∣∣2 ≤ K 0 + 20, and
we conclude that ∣∣θ⊥ ∣∣2 = K0 + 20 = K for all t ≥ 0. Note that K only depends α(S) and sample
size n.	口
ProofofLemma 3.3. To obtain a lower bound on ∣∣θt∣∣2, we first denote θt = θ2 + θ⊥, where θ2
denotes the projection of θ onto SPan(U2), and θ⊥ denotes the projection of θ onto SPan(U2)⊥. We
have
1Xeχp(-z>θU - z>θ⊥) ≤ + Jog t、2 eχp(-c∣∣θt∣∣2).
n M	tη(γ2 - C)2
Let us assume that ∣∣θ⊥∣∣ is bounded so that exp(∣∣θ⊥∣∣) ≤ M, which will be verified immediately.
Choosing an arbitrary support vector Zi, we have 0 <hZi, θ2) =(Zi, U2) (θt, U2) = γ2 hθt, U2)=
72∣∣θU∣∣2 ≤ γ2∣∣θt∣∣2, hence the previous inequality becomes:
exp(-72∣∣θt∣∣2) ≤「g ∖2 exp(-c∣∣θt∣∣2)M,
tη(γ2 - c)2
which is equivalent to
∣∣θt∣∣2 ≥ log ("2 -c；) /(Y2 - c).	(25)
∖ nM log t )
Now we only need to show that ∣∣θ⊥ ∣∣ ≤ M for all t for some M. Since we have shown in Lemma 3.2
that ∣∣θt∣∣2 ≤ K, we choose M = eκ ≤ exp (20+)gn) = O(ni), and the lower bound (25)
becomes	∣∣θt∣∣2 ≥ log (⅛―¾) /(Y2 - c),	(26)
∖n1十1∕ɑ log t
which concludes our proof.	口
15
Published as a conference paper at ICLR 2020
Proof of Theorem 3.2. We denote θt = θut + θ⊥t , where θut denotes the projection of θ onto span(u2),
and θ⊥t denotes the projection of θ onto span(u2)⊥. Combine Lemma 3.2 and Lemma 3.3, we have
1 _/ Jt_	∖ =1_ 您 2 ,U2〉+ hθ⊥ ,U2i	M 2 ,U2〉	K
∖l∣θtl∣2 ,u2/ =	l∣θtl∣2	≤	l∣θtl∣2 +忖 ∣∣2
=1	∣∣θu 21∣2+ɪ ≤ 1 _ikji+ɪ
=	忖 l∣2 +忖 l∣2≤ 忖 112 + l∣θtl∣2
=BI + ɪ
=I∣θtll2 + l∣θtl∣2
K2	K
≤ W + 眄K.
1	1 + 1∕ɑ ,	2 E1 1/2
By our choice of C and T that γ2 - C = (n——ητlog-) , together Lemma 3.3, the Theorem holds
as desired.	□
Proof of Corollary 3.2. By Lemma 3.3 and the the choice of parameters that γ2 -
n n1 + 1/a log2 T∖ 1/2	L
(-----ητo- )	, We have:
C
吗2 ≥ (n⅛ J
Together With Theorem 3.1, We have
L(θτ) = Ladv(θτ)exp (-c∣∣θτ∣∣2)
6 log2 T	(_ ( ηT	Y/2)
' Tη(γ2 - c)2	y C In(I+1/a) log2 T) J
O
卜C ( Efc门).
Where the last equality holds by the parameter choice γ2 - C =
μ = c (ni+1/a)) 1/2, the claim follows immediately.
(n1 + 1/a log2 τ
I ητ
1/2
. Finally, letting
□
C Proofs for Section 3.2
In this section, we consider general 'q-norm perturbations. In short, we show that no matter how
small the perturbation is, adversarial training changes the implicit bias of standard clean training
using gradient descent, and adapt it to specific norm we choose for adversarial training.
Intuitively, we might expect that under the 'q -norm perturbation the implicit bias of gradient descent
algorithm changes to converging in direction to 'q-norm max margin solution Uq. We provide a
counter example here. Consider S = {z1 = (x1,y1),z2 = (x2,y2)} with x1 = (10, 1), x2 =
(-10, -1) and y1 = 1,y2 = -1.
It is easy to see that the '∞-norm max margin solution is u∞ = (1,0) with γ∞ = 10, and the
'2-norm max margin solution is U = (√∣ι, √^) with γ2 = √101.
Without perturbation, we have that the gradient descent initialized at the origin converges in direction
to '2-norm max margin solution U with one step. Now we take l∞-norm perturbation with C = 0.5,
the negative gradient is given by: -VLadv(θ) = '12(θ)(zι - C ∙ sign(θ)) + '22(θ)(z2 - C ∙ sign(θ)).
We initialize gradient descent at the origin with any constant step size. By the symmetry of the
training data, we have that θt always stays always inside quadrant I, and converges in direction to
u = (34361, -7=), which is neither u∞ or u2, but inside the interior of convex hull of u∞ and U2. In
362	362
fact, u exactly equals to the u2,∞ defined in (10).
16
Published as a conference paper at ICLR 2020
ProofofLemma 3.4. We prove that solutions to (10) and the robust SVM against 'q-norm PertUr-
bation parameterized by c (8) are equal up to a constant factor. We first have that γ2,q(c) in (10) is
equivalent to
(27)
γ2,q = Jmax min yix>θ - cllθllp.
∣∣θ∣∣2≤1 i∈[n]
We denote the unique solution to (27) as U,,q. It is not difficulty to see that
yixi>U,,q - c||U,,q||, > γ,,q,∀i = 1, . . ., n.
We define U2,q = u2,q-, then:
_	yiX>U2,q — c∖∖U2,q l∣2 > 1,∀i =1,...,n.	_
It is now clear that U2,q is a feasible solution to (8). We denote the optimal solution to (8) as U, then
We have by the optimality of U that ∣∣U∣∣2 ≤ ∣∣U2,q ∣∣2 ≤ llu2,q ||2, and feasibility of U that
c,i>∕~ -ʌ c∣∣~ -I	γ2,q
Thr .	.	.yixi(γ2,qU) -cllY2,qu⅛, ≥ γ2,q∀i = 1,. ...,n.
Then from previous two inequalities we have γ2,qU is a feasible solution to (27) with objective value
equal to the optimal objective value of (27). Since the optimal solution to (27) is unique, this implies
that U = u2q, which concludes our proof.	□
γ2,q
We extend Lemma 3.1 to bounded 'q-norm perturbation set.
Lemma C.1. Recall the definition of γ.q in (10). Forany C < Yq, we have that(一PLadv(θ) u2,qi ≥
Ladv(θ)γ2,q for all θ ∈ Rd.
Proof. Recall that we have Ladv(θ) = 1 Pin=ι max∣∣δ∣∣q≤c exp (-yi(xi + δi)>θ). For
each sample (xi, yi) ∈ S, given a classifier θ, the worst case perturbation is δi =
argmax∣∣δ∣∣q≤cexp (-yi(xi + δ)>θ) = argmin∣∣δ∣∣q≤cyiδ>θ. The corresponding loss is then
Ladv(θ) = n Pn=I eχp (-yiE + δi)>θ).
Since for a fixed δi, the function exp -yi(xi + δi)>θ is convex in θ, hence the gradient of Ladv(θ)
is
1n
-VLadv(θ) =/ ^Xeχp ^—y((Xi + δi) θ) yi(xi + δi).
i=1
Then by the definition of U2,q, we havne
h-PLadv (θ), U2i = X exp -yi(xi + δei)>θ	yi(xi + δei), U2,q
i=1
(28)
n
≥ Xexp -yi(xi + δei)>θ γ2,q = Ladv(θ)γ2,q,	(29)
i=1
where the second inequality holds by ∖∖δi∖∖q ≤ c, and the definitions of U2,q and γ2,q in Lemma 3.4.
, , □
Note that for q = 2, by the fact that γ2,2(c) = γ2 - c, we immediately have Lemma 3.1 holds.
As a direct corollary of Lemma C.1, we have ∖∖θt∖∖2 is bounded away from 0 for all t ≥ 1.
Corollary C.1. Let θ0 = 0 in Algorithm 1, we have: ∖∖θt∖∖2 ≥ η0γ2,qfor al t ≥ 1.
Proof. The proof is similar to Corollary 3.1, we omit the details here.	□
Proof of Theorem 3.3. For simplicity, we define zi = yixi and have ∖∖zi∖∖2 ≤ 1 since kxik2 ≤ 1. We
have for θ 6= 0	n
VLadv(θ) = ~ X exp (-z>θ + c∖∖θ∖∖p) (-Zi + cd∖∖θIlp),
n i=1
1n
V2Ladv(θ) = — ɪ2 exp (-Z>θ + c∖∖θ∖∖p) (-Zi + cd∖∖θ∖∖p) (-zi + cd∖∖θ∖∖P)T
n i=1
1n
+ -∑exp (-z>θ + c∖∖θ∖∖p) c ((1 - P)∖∖θ∖∖p-2p(Θp-1θ)(Θp-1θ)> + (P - 1)∖∖θ∖∖p-pdiag(Θp-2θ)),
n
i=1
17
Published as a conference paper at ICLR 2020
where Θp-1θ denotes taking element-wise (P — 1)-th power of θ.
Note that we have ∣∣∂∣∣θ∣∣p∣∣q = 1. By the conjugacy of 'p-norm and 'q-norm with P + ɪ = 1, we
have∣∣θ∣∣p = max∣∣s∣∣q<1 <θ,s). Hence we upper bound the first term in Hessian V2Ladv(θ) above
by	"
1	n
—£exp (—z)θ + c∣∣θ∣∣p) (—Zi + c∂∣∣θ∣∣p) (—Zi + c∂∣∣θ∣∣p)>	(30)
n i=1
1 n
≤ — EeXP (—z)θ + c∣∣θ∣∣p) (1 + c√d∣∣θ∣∣2)2.	(31)
n
i=1
We further have:	2
(P- 1)∣∣限1diagb2θ) ≤(P- 1)d⅛Θ⅞?
∞
≤(P- 1)d 总 W
3p-2 I
≤(p — 1)d R 晅K.
Together with the fact that P ≥ 1, we bound the Hessian V2Ladv(θ) as:
V2Ladv(θ) ≤ Ladv(θ) (1 + C√d)2 + c(p — 1)d3p-2 需
∣∣θ∣∣2
I.
Note that the Hessian expression indicates that the objective is highly non-smooth around origin.
However, as shown in Corollary C.1, starting from origin, θt always stays away from the origin with
distance bounded below.
Using Taylor expansion, and by θt+1 = θt — ηtVLadv(θt), we have
Ladv(θt+1) ≤ Ladv (θt) — ηt∣∣VLadv(θt)∣∣2 + W "VLadV("
where λ (H(v))maX denotes the largest eigenvalue of H(v), and
max	λ (H(v))maχ
v∈[θt,θt+1]	max
(32)
H(V) = LadV(V) (1 + c√d)2 + c(p — 1)d2P-2 ʃɪ- I.
L	∣∣v∣^
Since η0 = 1, by Corollary C.1, for any t ≥ 1, we have ∣∣θt∣∣2 > γ2,q. Letting mp = (1 + c√d)2 +
3p-2	1
c(p — 1)d2p-2 十,and since that Ladv(θ) is a convex function, we obtain that
Ladv(θt+1) ≤ Ladv(θt) — ηt∣∣VLadv(θt)∣∣2 + WIIVLadV 的 " R max{Ladv (θt+1, Ladv (θt)}.
We then show by contradiction that we have Ladv(θt+1) < Ladv(θt). Assume this is not the case,
then we have:	1
Ladv(θt+1) ≤ (1 — Mnɪ ∣∣VLadv(θt)∣∣2)	(Ladv(θt) — ηt∣∣VLadv(θt)∣∣2)
However, if we choose ηt satisfying ηt ≤ m L ：⑼),We have the right hand side of previous
inequality strictly smaller than Ladv(θt), which is clearly a constradiction. Hence when we choose
ηt ≤ mqLajv(θt), we have Ladv(θt+1) < Ladv(θt) and
Ladv (θt+1) ≤ Ladv (θt ) — ηt∣∣VLadv(θt)∣∣2 + W "VLa；v (" ) " )2 mpLadv (θt).	(33)
Now by induction, if we choose ηt = η ≤ ——L 1 for t ≥ 1, then we have (33) holds for all
mq LadV (θ )
t ≥ 1. Note that we have an upper bound of Ladv(θ1), which is
1	n
Ladv(θ1) = — Xexp (—yi(Xi + Z)>θ1))
n i=1
1	n	〜	〜
=-Xexp (—yi(Xi + ei)>θU — yi(xi + Si)>θ⊥)
n i=1
1 n
≤ n X exP (一Y2,q + (1 + c√d)) = exP (—Y2,q + (1 + c√d)) ,	(34)
n i=1
18
Published as a conference paper at ICLR 2020
. T .	...	.	....	…一	. .. c 八1	/	、
where δi denotes the worst case perturbation to Xi, and θ1 denotes projection of θ1 onto SPan("q),
and θ⊥ denotes projection of θ1 onto SPan("q)⊥.
In summary, we have that if
ηt = η ≤ min{而-,1} for all t ≥ 1, where MP
we have
mp exp
(-γ2,q + (1 + c√d))
Ladv(θt+1 ) ≤ Ladv(θt) - η∣∣VLadv(θt)∣∣∣ +
S"VLa『Wil)2 mpLadvW)
(35)
(36)
≤ Ladv (θt) - 2 ∣∣VLadv (θt)∣∣2	(37)
where the last inequality holds since ηmτLadv(θt) ≤ ηmpLadv(θ1) ≤ 1. Now for any θ ∈ Rd, we
have
∣∣θt+1 - θ∣∣2 = ∣∣θt - θ∣∣2 - 2ηt (VLadv(θt),θt - θ> + (ηt)2∣∣VLadv(θt)∣∣2
≤	∣∣θt	- θ∣∣2	-	2ηt	(LadV(θ^	- LadV⑹)+ M)2 ∣∣VLadV (θt ) ∣∣ 2
≤	∣∣θt	- θ∣∣2	-	2ηt	(LadV(θ^	- LadV⑹)+ 2ηt (LadV(θD - LadV(θt+1 ))
=∣∣θt - θ∣∣2 - 2ηt (LadV(θt+1)- LadV⑹),
where the first inequality holds by the convexity of Ladv(θ), and the second inequality holds by (37).
Summing up the above inequality from S = 1 to t - 1 and by ηt = η ≤ 1
Ladv(θs+1) ≤ Ladv(θs), we have
LadV(θD - LadV⑹ ≤	∣∣θ1 - θ∣∣2 ≤ — (∣∣θ∣∣2+ ∣∣θ1∣∣2)
2tη	tη
Since θ is arbitrary, by choosing θ = log(t) ∙ u2 q, we have
八 J	〜	γ2,q	Z 2
∣∣θ∣∣2+ ∣∣θ1∣∣2 ≤ T-+ (1 + CVd)2,
γ2,q
and
η0 together with
(38)
which yields
1 n	(
Ladv(θ) = - V"exp - min (Zi + δi)>
n =	∖ I也 ||q ≤c
LadV(θt) ≤1 + X
+ (1 + c√d)2
(39)
□
Parameter Convergence: Intuition. Before we formally prove the implicit bias of GDAT, we
At
provide some intuitions here for better understanding. We claim that u∞ = limt→∞ ∣∣θ^∙ is in the
same direction as the solut1ion to
min -∣∣θ∣∣2 + η(c)∣∣θ∣∣p,	s.t. z>θ ≥ 1,∀i = 1,...n.	(40)
θ 2
Note that θt is a conic combination of (zi - cα∣∣θt∣∣p}i∈[n], and ∂∣∣θt∣∣p only depends on the
direction of θt. Hence by normalizing the norm of θt and using limt→∞ ∣∣θt∣∣2 = ∞, if the limit
At
u∞ = limt→∞ ∣∣jt∣∣2 exists, it satisfies the following condition under proper scaling that
n
θ = X αi(zi - c∂∣∣θt∣∣p),
i=1
s.t. ai ≥ 0,z>θ ≥ 1, ∀i = 1,...n,
ai(z> θ — 1) = 0, ∀i = 1,...n.
Defining a = (αι,..., an) and (θ, a) = ((∣∣θ∣∣pC + 1)θ, (∣∣θ∣∣pC + 1)α), it is easy to see that (θ, a)
is a solution to the following system
n
θ = Xαi(zi - c∂∣∣θt∣∣p),	(41)
i=1
s.t.: ai ≥ 0,z>θ ≥ c∣∣θ∣∣p + 1,∀i = 1,...n.	(42)
ai(z)θ — c∣∣θ∣∣p — 1) = 0, ∀i = 1,...n.	(43)
19
Published as a conference paper at ICLR 2020
Notice that the above set of equations (41)-(43) is exactly the first-order KKT condition of the
following optimization problem
min W∣∣θ∣∣2 s.t. z>θ ≥ c∣∣θ∣∣p + 1,∀i = 1,...n.	(44)
θ2
(44)	has a robust reformulation as maximizing the '2-norm margin under the worse case 'q -norm
perturbation bounded by c that
min 3∣θ∣∣2 s.t. min (Zi + δ^)τθ ≥ 1,∀i = 1,...n,
θ 2	l∣δi∣∣q≤C	一
or equivalently
yi(χi + δi)τθ
∣Rk-
max min min
θ i=1,...,n ∣∣δi∣∣∞≤c
(45)
We note that (45) is a Support Vector Machine problem over an uncoutable data set that is generated
by norm-bounded perturbation S(c, q) = {(x, y) : where ∃i ∈ [n], ||x - xi||q ≤ c, y = yi}. By the
separability and c < γq, we have that S(c, q) is well defined.
By the first-order KKT condition we have that (44) is equivalent to
min ∣∣θ∣∣2 + η(c)∣∣θ∣∣p s.t. zτθ ≥ 1,∀i = 1,...n.
θ
for some proper η(c) that depends on c. Hence in summary, if u∞ = limt→∞ 温^ exists, it is in
the same direction as the solution to the mixed ('2, 'ι)-norm max margin solution of (40).
Claim: In general, for 'q-norm perturbation bounded by c, θt converges in direction to the solution to
min 3∣θ∣∣2	s.t. min (Zi +	δi)τθ	≥	1,∀i	= 1,...n.
θ 2	l∣δi∣∣q≤C	一
or
min ∣∣θ∣∣2 + η(c)∣∣θ∣∣p s.t. z>θ ≥ 1,∀i = 1,...n.
θ
for some proper η(c) that depends on c.
Proof of Theorem 3.4. Recall that in Theorem 3.3 we showed in (36) the following recursion
Ladv(θt+1 ) ≤ Ladv(θt) — η∣∣VLadv(θt)∣∣2 + ㈤"干(0'' 11 )2 mpLadv(θt)
≤ exp (-η ”"adv*2 + mp η2 l∣VLadv(θt)∣∣2)
Ladv(θt)	2
≤ exp ^-ηγ2,q IlVLadv(θt)∣∣2 + mp -2- ||VLadv(θt)H2).
where the last inequality holds by Lemma C.1.
Applying the previous inequality recursively from s = 1 to t - 1, we have
t-1	t-1	2
-ηγ2,q X IIVLadv (θs )∣∣2 + X mp η2 ∣∣VLadv(θs)∣∣2
Now since in the proof of Theorem 3.3 we showed that ηmp < 1 (35), combining the above inequality
this with (37), we have
t-1	2	t-1
Xmpη-∣∣VLadv(θs)∣∣2 = X ηIIVLadv(θs)∣∣2 = Ladv(θ1) - Ladv(θt) ≤ Ladv(θ1).
s=1	s=1
Combining this inequality with the upper bound on Ladv (θ1) in (34), we have
Ladv (θt) ≤ exp -ηγ2,q X IIVLadv(θs)II2 - γ22,q + (1 +
s=0
Now for all i ∈ [n], we have:
exp (-局iqn≡cyi(Xi + δi'τθt) ≤ nexp ",q XIIVLadv®)"2 - γ2,q+ (1 +
20
Published as a conference paper at ICLR 2020
which yields
t-1
min yi(xi + δi)>θt ≥ ηγ2,q k ∣∣VLadv(θs)∣∣2 + γ2 q -(1 + Cy)-Iogn.
MM ≤c	s=0	,
Dividing both sides by ∣∣θ∣∣2, and since limt→∞ Ladv(θt) = 0, we have limt→∞ ∣∣θt∣∣2 = ∞. Hence,
lim min yg + δi)>ɪ-θ^
t→∞ι∣δi∣∣q≤c	∣∣θt∣∣2
t-1
≥ tl→im∞ ηγ2,q X
s=0
l∣VLadv(θs)∣∣2
-n-
1 + c√d + log n
∣ra
(46)
where the last inequality
≥ γ2,q,	t 1
holdsby llθtll2 ≤ η Ps=0 llvLadv(θS)||2.
Hence in summary, we have
min yi(xi + δi)> lim IlZVll ≥ γ2q.
ι∣δi∣∣q≤c	t→∞ ∣∣θt∣∣2 ,q
Hence, we have limt→∞ θt∕∣∣θt ∣∣2 is a solution to (10), but notice that the solution to (10) is unique
since a multiple of its o1ptimal solution would be the solution to (8) that
min -∣∣θ∣∣2 s.t. min yi(Xi + δi)>θ ≥ 1,∀i = 1,...,n,
θ∈Rd 2	2	δi ∈∆i (q)
which is a convex program with strongly convex objective. By this fact, we conclude that
θt
limt→∞ ∣∣θt∣∣2 = u2,q. To further get the rate of convergence, we use the convergence of ad-
versarial risk in (39), and establish the lower bound on ∣∣θt∣∣2: ∣∣θt∣∣2 = Ω (logt). Combining this
with (46), the claim follows immediately.	□
D '∞-Norm Perturbation
Recall that the robust SVM against '∞-norm perturbation parameterized by C is formulated as
γ2 ∞ = max min min y^x： +，) °,	(47)
γ2,∞ θ ：=m…,n l∣δm∞≤c	∣∣θ∣∣2	,	()
and its associated max-margin classifier is
u2,∞ = argmax min min yi(xi + δi)>θ.
'	∣∣θ∣∣2 = 1 i=1,---,n llδill∞≤c
It is easy to see that for C < γ∞, both γ2,∞ and u2,∞ are well defined, and γ2,∞ > 0.
Before showing parameter convergence, we first prove that the adversarial risk goes to zero. To
avoid analyzing '∞-perturbation directly, which can go messy. For λ > 0, we define a smooth
approximation of `1 -norm that
______	d
hλ(θj) = θ∕θ+ + λ, and Hλ(θj) = X hλ(θj).
j=1
Note that as λ → 0, Hλ(θ) → ∣∣θ∣∣ι uniformly. We then define a smoothified version of (47) that
we let perturbation set be ∆i(λ) = {δ : ∀j ∈ [d], ∣δj | ≤ Chλ∣(θj)}, and the corresponding γ2,∞ and
u2,∞ become
γ2 λ = max min min
, θ i=1,...,n δi∈∆i(λ)
yi(xi + δi)>θ
l∣θ∣∣2
(48)
u2,λ = argmax min min yi (xi + δi)>θ.	(49)
,	∣∣θ∣∣2 = l i=1,---,n δi∈∆i (λ)
Note that the Hausdorff distance between ∆i(λ) and {δ : ∣∣δ∣∣∞ ≤ c} converges to 0 as λ goes
to 0. It can be seen that when λ → 0, the smoothified problem (48) reduces to (47). That is,
limλ→0 γ2,λ = γ2,∞ and limλ→0 u2,λ = u2,∞.
Theorem D.1. Let perturbation set be ∆i(λ) = {δ : ∀j ∈ [d], | δj | ≤ C hλ(θj)}
adversarial risk be
1n
Ladv(θ) = n iζ
δ ∈m∆ax(λ) exp -yi(xi + δi)>θ .
and let its associated
For C < γ2,λ, letting η
(1+2cλ-ι∕2)2, we have
L (θt <O (log2 t(1 + 2cλ-i∕2)2 A
Ladv(θ ) ≤ O (	,	I .
tγ2,λ
21
Published as a conference paper at ICLR 2020
Proof. By the definition of perturbation set that ∆ = {δ : ∀j ∈ [d], ∖δj | ≤ C"渭j)}, We have
1 n
Ladv(θ) = n EeXP (-y∕[θ + cHλ(θ)).
i=1
By some simple calculation, We have
vHλ ⑻=(Pj⅛'….p2⅛) , v2Hλ ⑻=diag ((好 +λ)3∕2 '…'% +λ)3∕2 ).
Then, it holds that
1 n
VLadv(θ)=二 EeXP (-z>θ + cHλ(θ)) — + cVHλ(θ)),
i=1
1 n
V2Ladv(θ)=万 EeXP (-z>θ + cHλ(θ)) (&z> + c2VHλ(θ)VHλ(θ)τ - 2z>VHλ(θ) + cV2Hλ(θ)).
i=1
It can be verified that V2Ladv(θ) ≤ (1 + 底)2Ladv(θ)I. By Talyer expansion, we have
Ladv(θt+1) ≤ Ladv (θt) - η∣∣VLadv(θt)∣∣2 + (1 + ^ W max{Ladv(θt), Ladv(θt+1>∖∣VLadv(θt) ||2 .
(50)
Now we show that Ladv(θt+1) ≥ Ladv(θt) does not hold when η ≤ q+2cλ-i∕2)2l cl(/). Suppose
the contrary holds. By (50), we have
Ladv(θt+1) ≤ (1- η2"VLadVW"2 (1 + √2c= )2)	(Ladv(θt)- η∣∣VLadv(θt)∣∣2) < Ladv(θt).
where the last inequality holds by η =
__________1__________
(1 + 2cλ-1/2)2LadV (θt).
Hence we obtain a contradiction.
Note that Ladv(θ0) = 1, and if η ≤ (+c∖ι∕2)2, η ≤(+”-"产Lad) holds for t = 0,
and Ladv(θ1) ≤ 1. Consequently, we can inductively show that Ladv(θt) ≤ 1 for all t, and
η ≤(i+2ci-i/2)2Ladv(伊)alwaysholds if we let η =(1 + 2c；-1/2)2 .
By the choice of η, we obtain the following recursion taht
Ladv (θt+1) ≤ Ladv (θt) - η∣∣VLadv(θt)∣∣2 + (1 + ^| )2 LadVe) ∣∣VLadv(θt) ∣∣2	(51)
=Ladv (θt) - 2 ∣∣VLadv(θt)∣∣2.	(52)
Using the previous recursion we have that for any θ ∈ Rd,
∣∣θt+1 - θ∣∣2 = ∣∣θt - θ∣∣2 - 2η (VLadv(θt),θt-θ> + η2∣∣VLadv (θt)∣∣2
≤ Hθt - θH2 - 2η (LadV(θ^ - LadV⑹)+ 2η (LadV(的一LadV(θt+1))
=Hθt - θH2 - 2η (LadV(θt+1)- LadV⑹),
where the second inequality holds by convexity and (51). Summing up the previous inequality from
S = 0 to S = t - 1 and by Ladv(θs+1) ≤ Ladv(θs), we have
LadV (θD - LadV⑹ ≤ 2tη nθn2.
Taking θ = Ilgt	we have
，	1 n
Ladv(θ) = — £ maχ exp (-mE + δi)τθ)
n	δi∈∆⅛(λ)
i=1
1 ∖~r	T ( S、T Sg t	ʌ 1
=max exp 一阴(0 + δi)	U2,λ ≤ 二.
n g δi∈∆i(λ)	V	Y2,λ	) t
where the last inequality holds by max^∈∆i %(Xi + δi)τu2,λ ≥ γ2,λ. Hence we obtain
LadV(θD ≤ -
log21
+ 7-----
tγ2,λη
(log2
t(1 + 2cλ-1/2)2
tY2,λ
O
□
22
Published as a conference paper at ICLR 2020
Before showing parameter convergence, We need the following lemma which is a generalization of
Lemma 10 in Gunasekar et al. (2018a), but with much simpler proof.
Lemma D.1. Fix c < γ2,λ ,for any θ ∈ RRd, we have
'	∣∣VLadv(θ)∣∣2 ≥Ladv(θ)γ2,λ.
Proof.
LC /八、	1 S /	〜、〜
-VLadV(θ) = —)： exp (-yiχei) yixei -
i=1
where 卫=argminx<-xi∈∆i(λ) yi(xi)>θ. Then by the definition of γ2,λ and u2,λ (48), We have
hyiX,U2,λ)≥ Y2,λ
From which we obtain h-VLadv(θ), u2,λ) ≥ Ladv(θ)γ2,λ, the claim follows by Cauchy-Schwarz
inequality.	□
Theorem D.2. Under the same setting as in Theorem D.1, we have
1.	θt
lim ,,”=u2 λ.
t→∞ ∣∣θt∣∣2	2,λ
Proof. Recall that in Theorem D.1 we showed in (51) that
Ladv(θt+1) ≤ Ladv (θt) - η∣∣VLadv(θt)∣∣2 + (1 + 宗 "^*' ∣∣VLadv(θt) ∣∣2
≤ exp (-η∣∣VjdV^)∣∣2 +(1 + -√c= )24IVLadV(θt)∣∣2)
∖	Ladv (θt)	√λ	2	)
≤ exp (-"γ2,λ∣∣VLadV(θt)∣∣2 + (1 + √λ )2 ^2^ ∣∣VLadV(θt )∣∣2^ ,
where the last inequality holds by Lemma D.1. Applying the previous inequality recursively from
S = 0 to t - 1, we have
LadV (θ' ) ≤ exp (-nγ2,λ)： ∣∣ VLadV (θ, ) ∣∣2 +X(1+气2 η>Ladv(θs)∣∣2).
Now by (51), we have
t-1	2	2	t—1
X(1 + √⅛ )2 12 ∣∣VLadv (θs)∣∣2 = X 2 ∣∣VLadv(θs)∣∣2 = Ladv(θ0) - Ladv(θt) ≤ 1,
s=0 V	s=0
which yields
Ladv(θt) ≤ exp (-nY2,λ X ∣∣VLadv(θs)∣∣2 +	∙
Next for all i ∈ [n], we have
exp ( - min yi(xi + δi)τθt )=exp Jix：θ + cHx(θt))
∖ δi∈∆i(λ)	)
≤ nexp (-nY2,λ X ∣∣VLadv(θs)∣∣2 + 1^ ,
which implies
t—1
min yi(xi + δi)τθt ≥ n”入 X ∣∣VLadv(θs)∣∣2 - 1 - logn.
δi 八。)	s=0
Dividing both sides by ∣∣θ∣∣2, and since limt→∞ Ladv(θt) = 0, we have limt→∞ ∣∣θt∣∣2 = ∞. Hence,
1∙	(	ICT	θt	、ι∙	∣∣VLadV(θs)∣∣2	1 + log n、
lim	min yi(xi	+ δi)	≥ lim	ηγ2,λ〉,-----rrɪʒ----------rrɪʒ— ≥ Y2,λ,
t→∞δi∈∆i(λ)	∣∣θt∣∣2	t→∞	, s=0	∣∣θt∣∣2	∣∣θt∣∣2	,
where the last inequality holds by ∣∣θt∣∣2 ≤ η Pt=0 ∣∣VLadv(θs)∣∣2.
23
Published as a conference paper at ICLR 2020
In summary, we have
θt
min yi(xi + δi)> lim ≥ γ2,λ.
δi∈∆i(λ)	t→∞ ∣∣θt∣∣2	,
Hence limgt ∣∣θt∣∣2 is a solution to (48). Note that the solution to (48) is unique since it is equivalent
to
θ∈Rd1 llθll2	s∙t∙
min yi(xi + δi)>θ ≥ 1, ∀i = 1, . . . , n.
δi∈∆i(λ)
We thus conclude that limt→∞ ∣向加 =u2,λ∙	□
To summarize, We have shown that for all λ > 0, limt→∞ 温 发 =u2,λ. The '∞-norm perturbation
corresponds to the case when λ → 0, it is natural to conclude that for '∞ perturbation, we have
limt→∞ 温∣2 = u2,∞∙ The discussion for q = 1 follows similar argument, hence we omit the
details here∙
E Additional Experiments on Perturbation Level and Speed-up
We provide additional experiments on the connection of perturbation level c and the speed-up effect
of adversarial training for neural networkds. We run GDAT with '∞-norm perturbation. The setup of
the experiments is exactly the same as the setup in Section 4∙ We will vary the perturbation level c
used in GDAT algorithm in {0.1, 0.15, 0.2}∙
From Figure 3 we could see that GDAT indeed accelerates convergence of loss and accuracy on clean
training samples∙ Moreoever, the acceleration effect is stronger when we use larger perturbation level,
and this relationship is consistent across different width of hidden layer∙
Similar speed-up effects on the test loss and test accuracy evaluated on clean test samples are also
observed for GDAT∙ From Figure 4, we see that the speed-up effects become stronger when we
use larger perturbation level, and this relationship is consistent across different width of hidden
layer∙ Traditionally, the benefit of adversarial training is understood as two fold: 1∙ it improves the
robustness of the learning algorithm, i∙e∙, the solution has better loss toward adversarilly perturbed
seample; 2∙ it has better generalization ability∙ Our experiments demonstrate a third property
of adverserial training that is not known in literature before, i∙e∙, adversarial training accelerates
convergence∙
IOO 200	300	400	100	200	300	400	100	200	0	20	40	60	80

(a). Width = 64 × 64	(b). Width = 128 × 128	(c). Width = 256 × 256	(d). Width = 512 × 512
Figure 3: GDAT with Different Perturbation Level: Clean Training Loss
24
Published as a conference paper at ICLR 2020
Accuracy: clean data Loss: clean data
(a). Width = 64 × 64	(b). Width = 128 × 128	(c). Width = 256 × 256	(d). Width = 512 × 512
Figure 4: GDAT with Different Perturbation Level: Clean Test Loss
25