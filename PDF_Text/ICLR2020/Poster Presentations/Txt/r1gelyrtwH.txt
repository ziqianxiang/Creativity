Published as a conference paper at ICLR 2020
Physics-aware Difference Graph Networks
for Sparsely-Observed Dynamics
Sungyong Seo； Chuizheng Meng； Yan Liu
Department of Computer Science
University of Southern California
{sungyons,chuizhem,yanliu.cs}@usc.edu
Ab stract
Sparsely available data points cause numerical error on finite differences which
hinders us from modeling the dynamics of physical systems. The discretization
error becomes even larger when the sparse data are irregularly distributed or
defined on an unstructured grid, making it hard to build deep learning models to
handle physics-governing observations on the unstructured grid. In this paper, we
propose a novel architecture, Physics-aware Difference Graph Networks (PA-DGN),
which exploits neighboring information to learn finite differences inspired by
physics equations. PA-DGN leverages data-driven end-to-end learning to discover
underlying dynamical relations between the spatial and temporal differences in
given sequential observations. We demonstrate the superiority of PA-DGN in the
approximation of directional derivatives and the prediction of graph signals on the
synthetic data and the real-world climate observations from weather stations.
1	Introduction
Modeling real world phenomena, such as climate observations, traffic flow, physics and chemistry
simulation (Li et al., 2018; Geng et al., 2019; Long et al., 2018; de Bezenac et al., 2018; Sanchez-
Gonzalez et al., 2018; Gilmer et al., 2017), is important but extremely challenging. While deep
learning has achieved remarkable successes in prediction tasks by learning latent representations from
data-rich applications such as image recognition (Krizhevsky et al., 2012), text understanding (Wu
et al., 2016), and speech recognition (Hinton et al., 2012), we confront many challenging scenarios in
modeling natural phenomena with deep neural networks when only a limited number of observations
are available. Particularly, the sparsely available data points cause substantial numerical error when
we utilize existing finite difference operators and the limitation requires a more principled way to
redesign deep learning models.
While many methods have been proposed to model physics-simulated observations using deep
learning, many of them are designed under the assumption that input is on a continuous domain.
For example, Raissi et al. (2017a;b) proposed physics-informed neural networks (PINNs) to learn
nonlinear relations between input (spatial- and temporal-coordinates (x, t)) and output simulated
with a given partial differential equation (PDE). Since Raissi et al. (2017a;b) use the coordinates as
input and compute derivatives based on the coordinates to represent the equation, the setting is only
valid when the data are densely observed over spatial and temporal space.
Prior knowledge related to physics equations has been combined with data-driven models for var-
ious purposes. Chen et al. (2015) proposed a nonlinear diffusion process for image restoration
and de Bezenac et al. (2018) incorporated the transport physics (advection-diffusion equation) with
deep neural networks for forecasting sea surface temperature by extracting the motion field. Lutter
et al. (2019) introduced deep Lagrangian networks specialized to learn Lagrangian mechanics with
learnable parameters. Seo & Liu (2019) proposed a physics-informed regularizer to impose data-
specific physics equations. In common, the methods in Chen et al. (2015); de Bezenac et al. (2018);
Lutter et al. (2019) are not efficiently applicable to sparsely discretized input as only a small number
of data points are available and continuous properties on given space are not easily recovered. It is
* Equally contributed.
1
Published as a conference paper at ICLR 2020
unsuitable to directly use continuous differential operators to provide local behaviors because it is
hard to approximate the continuous derivatives precisely with the sparse points (Shewchuk, 2002;
Amenta & Kil, 2004; Luo et al., 2009). Furthermore, they are only applicable when the specific
physics equations are explicitly given and still hard to be generalized to incorporate other types of
equations.
As another direction to modeling physics-simulated data, Long et al. (2018) proposed PDE-Net which
uncovers the underlying hidden PDEs and predicts the dynamics of complex systems. Ruthotto
& Haber (2018) derived new CNNs: parabolic and hyperbolic CNNs based on ResNet (He et al.,
2016) architecture motivated by PDE theory. While Long et al. (2018); Ruthotto & Haber (2018) are
flexible to uncover hidden physics from the constrained kernels, it is still restrictive to a regular grid
where the proposed constraints on the learnable filters are easily defined.
The topic of reasoning physical dynamics of discrete objects has been actively studied (Sanchez-
Gonzalez et al., 2018; Battaglia et al., 2016; Chang et al., 2016) as the appearance of graph-based
neural networks (Kipf & Welling, 2017; Santoro et al., 2017; Gilmer et al., 2017). Although these
models can handle sparsely located data points without explicitly given physics equations, they are
purely data-driven so that the physics-inspired inductive bias for exploiting finite differences is not
considered at all. In contrast, our method consists of physics-aware modules allowing efficiently
leveraging the inductive bias to learn spatiotemporal data from the physics system.
In this paper, we propose physics-aware difference graph networks (PA-DGN) whose architecture is
motivated to leverage differences of sparsely available data from the physical systems. The differences
are particularly important since most of the physics-related dynamic equations (e.g., Navier-Stokes
equations) handle differences of physical quantities in spatial and temporal space instead of using
the quantities directly. Inspired by the property, we first propose spatial difference layer (SDL) to
efficiently learn the local representations by aggregating neighboring information in the sparse data
points. The layer is based on graph networks (GN) as it easily leverages structural features to learn
the localized representations and share the parameters for computing the localized features. Then,
the layer is followed by recurrent graph networks (RGN) to predict the temporal difference which
is another core component of physics-related dynamic equations. PA-DGN is applicable to various
tasks and we provide two representative tasks; the approximation of directional derivatives and the
prediction of graph signals.
Our contributions are:
•	We tackle a limitation of the sparsely discretized data which cause numerical error when
we model the physical system by proposing spatial difference layer (SDL) for efficiently
exploiting neighboring information under the limitation of sparsely observable points.
•	We combine SDL with recurrent graph networks to build PA-DGN which automatically
learns the underlying spatiotemporal dynamics in graph signals.
•	We verify that PA-DGN is effective in approximating directional derivatives and predicting
graph signals in synthetic data. Then, we conduct exhaustive experiments to predict climate
observations from land-based weather stations and demonstrate that PA-DGN outperforms
other baselines.
2	Physics-aware Difference Graph Network
In this section, we introduce the building module used to learn spatial differences of graph signals
and describe how the module is used to predict signals in the physics system.
2.1	Difference Operators on Graph
As approximation of derivatives in continuous domain, difference operators have been used as a
core role to compute numerical solutions of (continuous) differential equations. Since it is hard to
derive closed-form expressions of derivatives in real-world data, the difference operators have been
considered as alternative tools to describe and solve PDEs in practice. The operators are especially
important for physics-related data (e.g., meteorological observations) because the governing rules
behind the observations are mostly differential equations.
2
Published as a conference paper at ICLR 2020
(a) Original graph signals (b) Detected edge (c) Sharpened signals (d) Modulated gradients
Figure 1: Examples of difference operators applied to graph signal. Filters used for the processing
are (b) Pj (fi - fj) (c) Pj(1.1fi - fj), (d) fj - 0.5fi.
Graph signals Given a graph G = (V, E) where V is a set of vertices V = {1, . . . , Nv } and
E is a set of edges E ⊆ {(i, j)|i, j ∈ V} (|E| = Ne), graph signals on all nodes at time t are
f (t) = {fi (t) | i ∈ V} where fi : V → R. Graph signals on edges can also be defined similarly,
F(t) = {Fij (t) | (i, j) ∈ E} where Fij : E → R. Both signals can be multidimensional.
Gradient on graph The gradient (▽) of a function on nodes of a graph is represented by finite
difference
V : L2 (V) → L2(E),	(Vf )ij = (fj - fi) if (i,j) ∈ E and 0 otherwise,
where L2(V) and L2(E) denote Hilbert spaces for node/edge functions, respectively. The gradients
on a graph provide finite differences of graph signals and they become edge (i, j) features.
Laplace-Beltrami operator Laplace-Beltrami operator (or Laplacian, ∆) in graph domain is
defined as
∆ : L2(V) → L2(V),	(∆f)i = X (fi-fj) ∀i,j ∈V
jGj)∈E
This operator is usually regarded as a matrix form in other literature, L = D - A where A is an
adjacency matrix and D = diag(Pj:j 6=i Aij) is a degree matrix.
2.2	Difference Operators on Triangulated Mesh
According to Crane (2018), the gradient and Laplacian operators on the triangulated mesh can be
discretized by incorporating the coordinates of nodes. To obtain the gradient operator, the per-face
gradient of each triangular face is calculated first. Then, the gradient on each node is the area-weighted
average of all its neighboring faces, and the gradient on edge (i, j) is defined as the dot product
between the per-node gradient and the direction vector eij. The Laplacian operator can be discretized
with Finite Element Method (FEM):
Of)i = 1 X (COt αj +cotβ)(fj- fi),
/(i,j)∈E
where node j belongs to node i’s immediate neighbors (j ∈ Ni) and (αj, βj) are two opposing angles
of the edge (i, j).
2.3	Spatial Difference Layer
While the difference operators are generalized in Riemannian manifolds (Lai et al., 2013; Lim, 2015),
there exist numerical error compared to those in continuous space and the error can be larger when
the nodes are spatially far from neighboring nodes because the connected nodes (j ∈ Ni) of i-th
node fail to represent local features around the node. Furthermore, the error is even larger if available
data points are sparsely distributed (e.g., sensor-based observations). In other words, the difference
operators are unlikely to discover meaningful spatial variations behind the sparse observations since
they are highly limited to immediate neighboring information only. To mitigate the limitation, we
propose spatial difference layer (SDL) which consists of a set of parameters to define learnable
3
Published as a conference paper at ICLR 2020
Figure 2: Physics-aware Difference Graph Networks for graph signal prediction. Blue boxes have
learnable parameters and all parameters are trained through end-to-end learning. The nodes/edges
can be multidimensional.
difference operators as a form of gradient and Laplacian to fully utilize neighboring information:
(WVf)ij= w(g1)(fj- w(j2)fi),	(W∆f)i= X w(j1)(fi- w(j2)fj)	⑴
/(i,j)∈E
where wij are the parameters tuning the difference operators along with the corresponding edge
direction eij . The two forms (Eq 1) are associated with edge and node features, respectively. The
superscript in WV and W ∆ denotes that the difference operators are functions of the learnable
parameters w. wi(jg) and wi(jl) are obtained by integrating local information as follow:
wij = g({fk, Fmn | k, (m, n) ∈ h-hop neighborhood of edge (i, j)})	(2)
While the standard difference operators consider two connected nodes only (i and j) for each edge
(i, j ), Eq 2 uses a larger view (h-hop) to represent the differences between i and j nodes. Since graph
networks (GN) (Battaglia et al., 2018) are efficient networks to aggregate neighboring information,
We use GN for g(∙) function and Wij are edge features of output of GN. Eq 2 can be viewed as a
higher-order difference equation because nodes/edges which are multi-hop apart are considered.
wij has a similar role of parameters in convolution kernels of CNNs. For example, while the standard
gradient operator can be regarded as an example of simple edge-detecting filters, the operator can be
a sharpening filter if w(g1)= 1 and Wij2 = 年：1 for i node and the operators over each edge are
summed. In other words, by modulating wij , it is readily extended to conventional kernels including
edge detection or sharpening filters and even further complicated kernels. On top of Wij , the difference
forms in Eq 1 make an optimizing process for learnable parameters based on the differences instead
of the values intentionally. Eq 1 thus naturally provides the physics-inspired inductive bias which is
particularly effective for modeling physics-related observations. Furthermore, it is easily possible to
increase the number of channels for Wi(jg) and Wi(jl) to be more expressive. Figure 1 illustrates how the
exemplary filters convolve the given graph signals.
2.4 Recurrent Graph Networks
Difference graph Once the modulated spatial differences (W Vf (t),W ∆f (t)) are obtained, they are
concatenated with the current signals f(t) to construct node-wise (zi) and edge-wise (zij) features
and the graph is called a difference graph. The difference graph includes all information to describe
spatial variations.
Recurrent graph networks Given a snapshot (f (t), F (t)) of a sequence of graph signals, one
difference graph is obtained and is used to predict next graph signals. While a non-linear layer can be
used to combine the learned spatial differences to predict the next signals, it is limited to discover
spatial relations only among the features in the difference graph. Since many equations describing
physics-related phenomena are non-static (e.g., Navier-Stokes equations), we adopt recurrent graph
networks (RGN) (Sanchez-Gonzalez et al., 2018) with a graph state Gh as input to combine the spatial
4
Published as a conference paper at ICLR 2020
differences with temporal variations. RGN returns a graph state (G力=(h*(v), h*(e))) and next graph
signal Zi and Zij The update rule is described as follow:
1.	(zj, hi(e)) - φe(zj, Zi, Zj, h(e)) for all (i,j) ∈ E pairs,
2.	(zi, hi(V)) - φv(zi, Zi, h(V)) for all i ∈ V,
Zi is an aggregated edge attribute related to the node i,
where φe , φV are edge and node update functions, respectively, and they can be any recurrent unit.
Finally, the prediction is made through a decoder by feeding the graph signal, Zii and Ziij .
Learning objective Let f and F denote predictions of the target node/edge signals. PA-DGN is
trained by minimizing the following objective:
L = £Ilfi-fill2 + E	IlFij-Fijll2.
i∈V	(i,j)∈E
(3)
For multistep predictions, L is summed over all predicting steps. If only one type (node or edge) of
signal is given, the corresponding term in Eq 3 is used to optimize the parameters in SDL and RGN
simultaneously.
3	Effectivenes s of Spatial Difference Layer
To investigate if the proposed spatial difference forms (Eq 1) can be beneficial to learning physics-
related patterns, we use SDL on two different tasks: (1) approximate directional derivatives and (2)
predict synthetic graph signals.
3.1	Approximation of Directional Derivatives
As we claimed in Section 2.3, the standard difference forms (gradient
and Laplacian) on a graph can cause significant numerical error easily
because they are susceptible to a distance of two points and variations
of a given function. To evaluate the applicability of the proposed SDL,
we train SDL to approximate directional derivatives on a graph. First, we
define a synthetic function and its gradients on 2D space and sample 200
points (xi, yi). Then, we construct a graph on the sampled points by using
k-NN algorithm (k = 4). With the known gradient (▽/ = (df, df)) at
each point (a node in the graph), we can compute directional derivatives
by projecting Vf to a connected edge ej (See Figure 3). We compare
against four baselines: (1) the finite gradient (FinGrad) (2) multilayer Figure 3: Directional
perceptron (MLP) (3) graph networks (GN) (4) a different form of Eq 1 derivative on graph
(One-w). For the finite gradient ((fj - fi)/llxj - xill), there is no
learnable parameter and it only uses two points. For MLP, we feed (fi , fj , xi , xj) as input to see
whether learnable parameters can benefit the approximation or not. For GN, we use distances of
two connected points as edge features and function values on the points as node features. The edge
feature output of GN is used as a prediction for the directional derivative on the edge. Finally, we
modify the proposed form as (w Vf)ij = wij fj - fi. GN and the modified form are used to verify
the effectiveness of Eq 1. Note that we define two synthetic functions (Figure 4) which have different
property; (1) monotonically increasing from a center and (2) periodically varying.
Table 1: Mean squared error (10-2) for approximation of directional derivatives.
Functions		FinGrad	MLP	GN	One-w	SDL
f1 (x, y)	= 0.1x2 + 0.5y2	6.42±0.47	2.12±0.32	1.05±0.42	1.41±0.44	0.97±0.39
f2 (x, y)	= sin(x) + cos(y)	5.90±0.04	2.29±0.77	2.17±0.34	6.73±1.17	1.26±0.05
5
Published as a conference paper at ICLR 2020
Figure 4: Gradients and graph structure of sampled points. Left: the synthetic function is fι(χ, y)
0.1x2 + 0.5y2. Right: the synthetic function is f2(x, y) = sin(x) + cos(y).
Approximation accuracy As shown in Table 1, the proposed spatial difference layer outperforms
others by a large margin. As expected, FinGrad provides the largest error since it only considers two
points without learnable parameters. It is found that the learnable parameters can significantly benefit
to approximate the directional derivatives even if input is the same (FinGrad vs. MLP). Note that
utilizing neighboring information (GN, One-w, SDL) is generally helpful to learn spatial variations
properly. However, simply training parameters in GN is not sufficient and explicitly defining
difference, which is important to understand spatial variations, provides more robust inductive
bias. One important thing we found is that One-w is not effective as much as GN and it can be
even worse than FinGrad. It is because of its limited degree of freedom. As implied in the form
(Vw f )j = Wij * fj — fi, only one Wij adjusts the relative difference between f and f, and this
is not enough to learn whole possible linear combinations of fi and fj . The unstable performance
supports that the form of SDL is not ad-hoc but more rigorously designed.
3.2 Graph S ignal Prediction
We evaluate PA-DGN on the synthetic data sampled from the simulation of specific convection-
diffusion equations, to provide if the proposed model can predict next signals of the simulated
dynamics from observations on discrete nodes only. For the simulated dynamics, we use an equation
slightly modified based on the one in Long et al. (2018).
T = a(i)(Vf )x + b(i)(Vf )y + c(i)∆f,	fi(0) = fo(i),	(4)
where the index i points the i-th node whose coordinate is (xi, yi) in the 2D space ([0, 2π] × [0, 2π])
and X and y indicate X- and y-direction in the space. a(i) = 0.5(cos(yi) + xi(2π 一 Xi)Sin(xi))+0.6,
b(i) = 2(cos(yi) + Sin(Xi)) + 0.8, and c(i) = 0.5(1 — "xi-^^^yi-"，) ∙ Then, We uniformly
sample 250 points in the above 2D space. The task is to predict signal values of all points in the
future M steps given observed values of the first N steps. For our experiments, we choose N = 5
and M = 15. Since there is no a priori graph structure on sampled points, we construct a graph with
k-NN algorithm (k = 4) using the Euclidean distance. Figure 5 shows the dynamics and the graph
structure of sampled points.
To evaluate the effect of the proposed SDL on the above prediction task, we cascade SDL and a linear
regression model as our prediction model since the dynamics follows a linear partial differential
equation. We compare its performance with four baselines: (1) vector auto-regressor (VAR); (2)
multilayer perceptron (MLP); (3) StandardOP: the standard approximation of differential operators
in Section 2.1 followed by a linear regressor; (4) MeshOP: similar to StandardOP but use the
discretization on triangulated mesh in Section 2.2 for differential operators.
Table 2: Mean absolute error (10-2) for graph signal prediction.
VAR	MLP	StandardOP	MeshOP SDL
16.84±0.41	15.75±0.53 11.99±0.29 12.82±0.06 10.87±0.98
Prediction Performance Table 2 shows the prediction performance of different models measured
with mean absolute error. The prediction model with our proposed spatial differential layer outper-
forms other baselines. All models incorporating any form of spatial difference operators (StandardOP,
6
Published as a conference paper at ICLR 2020
t = 1	t = 5	t = 10	t = 15	t = 20
Figure 5: Synthetic dynamics and graph structure of sampled points.
MeshOP and SDL) outperform those without spatial difference operators (VAR and MLP), showing
that introducing spatial differences information inspired by the intrinsic dynamics helps prediction.
However, in cases where points with observable signal are sparse in the space, spatial difference
operators derived with fixed rules can be inaccurate and sub-optimal for prediction since the locally
linear assumption which they are based on no longer holds. Our proposed SDL, to the contrary,
is capable of bridging the gap between approximated difference operators and accurate ones by
introducing learnable coefficients utilizing neighboring information, and thus improves the prediction
performance.
4	Prediction: Graph Signals on Land-based Weather Sensors
We evaluate the proposed model on the task of predicting climate observations (Temperature) from
the land-based weather stations located in the United States.
4.1	Experimental Set-up
Data and task We sample the weather stations located in the United States from the Online Climate
Data Directory of the National Oceanic and Atmospheric Administration (NOAA) and choose the
stations which have actively measured meteorological observations during 2015. We choose two
geographically close but meteorologically diverse groups of stations: the Western and Southeastern
states. We use k-Nearest Neighbor (NN) algorithm (k = 4) to generate graph structures and the
final adjacency matrix is A = (Ak + Ak>)/2 to make it symmetric where Ak is the output adjacency
matrix from k-NN algorithm.
Figure 6 shows the distributions of the land-based weather
stations and their connectivity. Since the stations are not
synchronized and have different timestamps for the obser-
vations, We aggregate the time series hourly. The 1-year
sequential data are split into the train set (8 months), the
validation set (2 months), and the test set (2 months), re-
spectively.
Our main task is to predict the next graph signals based
on the current and past graph signals. All methods we
evaluate are trained through the objective (Eq 3) with the
Adam optimizer and we use scheduled sampling (Bengio
et al., 2015) for the models with recurrent modules. We
Figure 6: Weather stations in (left) west-
ern (right) southeastern states in the
United States and k-NN graph.
evaluate PA-DGN and other baselines on two prediction tasks, (1) 1-step and (2) multistep-ahead
predictions. Furthermore, we demonstrate the ablation study that provides how much the spatial
derivatives from our proposed SDL are important signals to predict the graph dynamics.
4.2	Graph S ignal Predictions
We compare against the widely used baselines (VAR, MLP, and GRU) for 1-step and multistep
prediction. Then, we use RGN (Sanchez-Gonzalez et al., 2018) to examine how much the graph
structure is beneficial. Finally, we evaluate PA-DGN to verify if the proposed architecture (Eq 1) is
able to reduce prediction loss. Experiment results for the prediction task are summarized in Table 3.
Overall, RGN and PA-DGN are better than other baselines and it implies that the graph structure
provides useful inductive bias for the task. It is intuitive as the meteorological observations are
7
Published as a conference paper at ICLR 2020
continuously changing over the space and time and thus, the observations at the i-th station are
strongly related to those of its neighboring stations.
PA-DGN outperforms RGN and the discrepancy comes from the fact that the spatial derivatives (Eq 1)
we feed in PA-DGN are beneficial and this finding is expected because the meteorological signals at
a certain point are a function of not only its previous signal but also the relative differences between
neighbor signals and itself. Knowing the relative differences among local observations is particularly
essential to understand physics-related dynamics. For example, Diffusion equation, which describes
how physical quantities (e.g., heat) are transported through space over time, is also a function of
relative differences of the quantities (dt = D∆f) rather than values of the neighbor signals. In other
words, spatial differences are physics-aware features and it is desired to leverage the features as input
to learn dynamics related to physical phenomena.
Table 3: Graph signal prediction results (MAE) on multistep predictions. In each row, we report the
average with standard deviations from all baselines and PA-DGN. One step is 1-hour time interval.
Region	Method	1-step	6-step	12-step
	VAR	0.1241 ± 0.0234	0.4295 ± 0.1004	0.4820 ± 0.1298
	MLP	0.1040 ± 0.0003	0.3742 ± 0.0238	0.4998 ± 0.0637
	GRU	0.0913 ± 0.0047	0.1871 ± 0.0102	0.2707 ± 0.0006
West	RGN	0.0871 ± 0.0033	0.1708 ± 0.0024	0.2666 ± 0.0252
	RGN(StandardOP)	0.0860 ± 0.0018	0.1674 ± 0.0019	0.2504 ± 0.0107
	RGN(MeshOP)	0.0840 ± 0.0015	0.2119 ± 0.0018	0.4305 ± 0.0177
	PA-DGN	0.0840 ± 0.0004	0.1614 ± 0.0042	0.2439 ± 0.0163
	VAR	0.0889 ± 0.0025	0.2250 ± 0.0013	0.3062 ± 0.0032
	MLP	0.0722 ± 0.0012	0.1797 ± 0.0086	0.2514 ± 0.0154
	GRU	0.0751 ± 0.0037	0.1724 ± 0.0130	0.2446 ± 0.0241
SouthEast	RGN	0.0790 ± 0.0113	0.1815 ± 0.0239	0.2548 ± 0.0210
	RGN(StandardOP)	0.0942 ± 0.0121	0.2135 ± 0.0187	0.2902 ± 0.0348
	RGN(MeshOP)	0.0905 ± 0.0012	0.2052 ± 0.0012	0.2602 ± 0.0062
	PA-DGN	0.0721 ± 0.0002	0.1664 ± 0.0011	0.2408 ± 0.0056
4.3	Contribution of Spatial Derivatives
We further investigate if the modulated spatial derivatives (Eq 1) are effectively advantageous
compared to the spatial derivatives defined in Riemannian manifolds. First, RGN without any spatial
derivatives is assessed for the prediction tasks on Western and Southeastern states graph signals.
Note that this model does not use any extra features but the graph signal, f (t). Secondly, we add
(1) StandardOP, the discrete spatial differences (Gradient and Laplacian) in Section 2.1 and (2)
MeshOP, the triangular mesh approximation of differential operators in Section 2.2 separately as
additional signals to RGN. Finally, we incorporate with RGN our proposed Spatial Difference Layer.
Table 3 shows the contribution of each component. As expected, PA-DGN provides much higher drops
in MAE (3.56%, 5.50%, 8.51% and 8.73%, 8.32%, 5.49% on two datasets, respectively) compared to
RGN without derivatives and the results demonstrate that the derivatives, namely, relative differences
from neighbor signals are effectively useful. However, neither RGN with StandardOP nor with
MeshOP can consistently outperform RGN. We also found that PA-DGN consistently shows positive
effects on the prediction error compared to the fixed derivatives. This finding is a piece of evidence to
support that the parameters modulating spatial derivatives in our proposed Spacial Difference Layer
are properly inferred to optimize the networks and to improve the prediction performance.
5	Conclusion
In this paper, we introduce a novel architecture (PA-DGN) that approximates spatial derivatives to
use them to represent PDEs which have a prominent role for physics-aware modeling. PA-DGN
8
Published as a conference paper at ICLR 2020
effectively learns the modulated derivatives for predictions and the derivatives can be used to discover
hidden physics describing interactions between temporal and spatial derivatives.
Acknowledgements
This work is supported in part by NSF Research Grant IIS-1254206 and MINERVA grant N00014-17-
1-2281, granted to co-author Yan Liu in her academic role at the University of Southern California.
The views and conclusions are those of the authors and should not be interpreted as representing the
official policies of the funding agency, or the U.S. Government. Last but not least, we appreciate
anonymous reviewers for your thorough comments and suggestions.
References
Nina Amenta and Yong Joo Kil. Defining point-set surfaces. In ACM Transactions on Graphics
(TOG), volume 23,pp. 264-270. ACM, 2004.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. In Advances in neural information processing
systems, pp. 4502-4510, 2016.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Advances in Neural Information Processing Systems,
pp. 1171-1179, 2015.
Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B Tenenbaum. A compositional
object-based approach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016.
Yunjin Chen, Wei Yu, and Thomas Pock. On learning optimized reaction diffusion processes for
effective image restoration. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 5261-5269, 2015.
Keenan Crane. Discrete differential geometry: An applied introduction. Notices of the AMS,
Communication, 2018.
Emmanuel de Bezenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical processes:
Incorporating prior scientific knowledge. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=By4HsfWAZ.
Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang, Jieping Ye, and Yan Liu. Spa-
tiotemporal multi-graph convolution network for ride-hailing demand forecasting. In 2019 AAAI
Conference on Artificial Intelligence (AAAI’19), 2019.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017.
9
Published as a conference paper at ICLR 2020
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
Honal neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012.
Rongjie Lai, Jiang Liang, and Hongkai Zhao. A local mesh method for solving pdes on point clouds.
Inverse Problems & Imaging, 7(3), 2013.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network:
Data-driven traffic forecasting. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=SJiHXGWAZ.
Lek-Heng Lim. Hodge laplacians on graphs. arXiv preprint arXiv:1507.05379, 2015.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. Interna-
tional Conference on Machine Learning, 2018.
Chuanjiang Luo, Issam Safa, and Yusu Wang. Approximating gradients for meshes and point clouds
via diffusion metric. In Computer Graphics Forum, volume 28, pp. 1497-1508. Wiley Online
Library, 2009.
Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model
prior for deep learning. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=BklHpjCqKm.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part i):
Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561,
2017a.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part ii):
Data-driven discovery of nonlinear partial differential equations. arXiv preprint arXiv:1711.10566,
2017b.
Lars Ruthotto and Eldad Haber. Deep neural networks motivated by partial differential equations.
arXiv preprint arXiv:1804.04272, 2018.
Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller,
Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and
control. International Conference on Machine Learning, 2018.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In
Advances in neural information processing systems, pp. 4967-4976, 2017.
Sungyong Seo and Yan Liu. Differentiable physics-informed graph networks. arXiv preprint
arXiv:1902.02950, 2019.
Jonathan Richard Shewchuk. What is a good linear finite element? interpolation, conditioning,
anisotropy, and quality measures (preprint). University of California at Berkeley, 73:137, 2002.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation sys-
tem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144,
2016.
10
Published as a conference paper at ICLR 2020
A Appendix
A. 1 S imulated Data
For the simulated dynamics, we discretize the following partial differential equation similar to the one
in Long et al. (2018) to simulate the corresponding linear variable-coefficient convection-diffusion
equation on graphs.
In a continuous space, we define the linear variable-coefficient convection-diffusion equation as:
d f = a(x,y)fχ + b(x,y)fy +C(X,yDf	(5)
f |t=0 = f0(x, y)
,with Ω = [0, 2π] X [0, 2π], (t, x, y) ∈ [0,0.2] X Ω,	a(x, y)	= 0.5(cos(y) +	x(2π	— x)	Sin(X))	+
0.6, b(x, y) = 2(cos(y) + sin(x)) + 0.8, c(x, y) =	0.5 (l	— √(xi 7√√+(yi	") ).
We follow the setting of initialization in Long et al. (2018):
f0(x, y) =	λk,l cos(kx + ly) + γk,l sin(kx + ly)	(6)
∣k∣,∣ι∣≤N
, where We use s	V = 9, λk,ι, γk,ι 〜 N(0,510), and k and l are chosen randomly. patial difference operators to approximate spatial derivatives: f r z	、	1 , “	、，/	、、	1 / M	、 fχ(χi,yi)	= 丁(f(χi,yi) ― f(xi ― s,yi)) ― (f(χ(Xi加 2s	2s			— f(Xi + s, yi))	
	fy (xi,yi)二 fxx (Xi, yi )	= 2S (f (Xi,yi) =s2(〃"	-	f(Xi, yi - S)) - ；(f(Xi, yi) 2s -	f (Xi - s,yi)) + s2(J(Xi,y)	- f(Xi, yi + s)) - f(Xi + s, yi))	(7)
, where s Then we	fyy (Xi 加	=s2(f (χi,yi) — f(xi,yi — S)) + s2(f (Xi,y) is the spatial grid size for discretization. rewrite (5) with difference operators defined on graphs:			- f(Xi, yi + s))	
ʃ f = a(i)(Vf )^ + b(i)(Vf )y + c(i)((∆f )^χ + (∆f )yy)
fi (0) = fo(i)
, where
	(a(Xi,yi)			
	I	—2s—	if Xi	Xj + s, yi = yj	
a(i)(Xj, yj) =	a(Xi, yi ) (,			(9)
	I	2s-	if Xi	Xj - s, yi = yj	
	(b(Xi,yi)			
		if Xi =	Xj , yi = yj + s	
b(i)(Xj, yj) =	b(Xi, yi ) (,			(10)
	I	2s-	if Xi =	Xj , yi = yj - s	
c(i)(Xj, yj) =	c =— s2			(11)
Then we replace the gradient w.r.t time in (8) with temporal discretization:
f (t + 1) = ∆t(a(i)(Vf)^ + b(i)(Vf )y + c(i)((∆f )xx + (∆f)yy)) + f (t)
fi (0) = fo(i)
(12)
, where ∆t is the time step in temporal discretization.
Equation (12) is used for simulating the dynamics described by the equation (5). Then, we uniformly
sample 250 points in the above 2D space and choose their corresponding time series of u as the
dataset used in our synthetic experiments. We generate 1000 sessions on a 50 X 50 regular mesh with
time step size ∆t = 0.01. 700 sessions are used for training, 150 for validation and 150 for test.
11
Published as a conference paper at ICLR 2020
A.2 Experiment Settings
Here we provide additional details for models we used in this work, including model architecture
settings and hyper-parameter settings.
A.2.1 Model Settings
Unless mentioned otherwise, all models use a hidden dimension of size 64.
•	VAR: A vector autoregression model with 2 lags. Input is the concatenated features of
previous 2 frames. The weights are shared among all nodes in the graph.
•	MLP: A multilayer perceptron model with 2 hidden layers. Input is the concatenated
features of previous 2 frames. The weights are shared among all nodes in the graph.
•	GRU: A Gated Recurrent Unit network with 2 hidden layers. Input is the concatenated
features of previous 2 frames. The weights are shared among all nodes in the graph.
•	RGN: A recurrent graph neural network model with 2 GN blocks. Each GN block has
an edge update block and a node update block, both of which use a 2-layer GRU cell as
the update function. We set its hidden dimension to 73 so that it has the same number of
learnable parameters as our proposed model PA-DGN.
•	RGN(StandardOP): Similar to RGN, but use the output of difference operators in Sec-
tion 2.1 as extra input features. We set its hidden dimension to 73.
•	RGN(MeshOP): Similar to RGN(StandardOP), but the extra input features are calculated
using opeartors in Section 2.2. We set its hidden dimension to 73.
•	PA-DGN: Our proposed model. The spatial derivative layer uses a message passing neural
network (MPNN) with 2 GN blocks using 2-layer MLPs as update functions. The forward
network part uses a recurrent graph neural network with 2 recurrent GN blocks using 2-layer
GRU cells as update functions.
The numbers of learnable parameters of all models are listed as follows:
Table 4: Numbers of learnable parameters.
Model	VAR	MLP	GRU	RGN	RGN(StandardOP)	RGN(MeshOP)	PA-DGN
# Params	3	4,417	37,889	345,876	341,057	342,152	340,001
A.2.2 Training Settings
The number of evaluation runs We performed 3 times for every experiment in this paper to report
the mean and standard deviations.
Length of prediction For experiments on synthetic data, all models take first 5 frames as input and
predict the following 15 frames. For experiments on NOAA datasets, all models take first 12 frames
as input and predict the following 12 frames.
Training hyper-parameters We use Adam optimizer with learning rate 1e-3, batch size 8, and
weight decay of 5e-4. All experiments are trained for a maximum of 2000 epochs with early stopping.
All experiments are trained using inverse sigmoid scheduled sampling with the coefficient k = 107.
Environments All experiments are implemented with Python3.6 and PyTorch 1.1.0, and are run
with NVIDIA GTX 1080 Ti GPUs.
A.3 Effect of different graph structures
In this section, we evaluate the effect of 2 different graph structures on baselines and our models:
(1) k-NN: a graph constructed with k-NN algorithm (k = 4); (2) TriMesh: a graph generated with
Delaunay Triangulation. All graphs use the Euclidean distance.
12
Published as a conference paper at ICLR 2020
Table 5: Mean absolute error (10-2) for graph signal prediction on the synthetic dataset.
VAR	MLP	StandardOP		MeshOP		SDL	
		k-NN	TriMesh	k-NN	TriMesh	k-NN	TriMesh
17.30	16.27	12.00	12.29	12.87	12.82	11.04	12.40
Table 6: Graph signal prediction results (MAE) on multistep predictions. In each row, we report the
average with standard deviations from all baselines and PA-DGN. One step is 1 hour time interval.
Region	Method	Graph	1-step	6-step	12-step
	VAR	-	0.1241 ± 0.0234	0.4295 ± 0.1004	0.4820 ± 0.1298
	MLP	-	0.1040 ± 0.0003	0.3742 ± 0.0238	0.4998 ± 0.0637
	GRU	-	0.0913 ± 0.0047	0.1871 ± 0.0102	0.2707 ± 0.0006
	RGN	k-NN	0.0871 ± 0.0033	0.1708 ± 0.0024	0.2666 ± 0.0252
West		TriMesh	0.0897 ± 0.0030	0.1723 ± 0.0116	0.2800 ± 0.0414
	RGN	k-NN	0.0860 ± 0.0018	0.1674 ± 0.0019	0.2504 ± 0.0107
	(StandardOP)	TriMesh	0.0842 ± 0.0011	0.1715 ± 0.0027	0.2517 ± 0.0369
	RGN	k-NN	0.0840 ± 0.0015	0.2119 ± 0.0018	0.4305 ± 0.0177
	(MeshOP)	TriMesh	0.0846 ± 0.0017	0.2090 ± 0.0077	0.4051 ± 0.0457
	PA-DGN	k-NN	0.0840 ± 0.0004	0.1614 ± 0.0042	0.2439 ± 0.0163
		TriMesh	0.0849 ± 0.0012	0.1610 ± 0.0029	0.2473 ± 0.0162
	VAR	-	0.0889 ± 0.0025	0.2250 ± 0.0013	0.3062 ± 0.0032
	MLP	-	0.0722 ± 0.0012	0.1797 ± 0.0086	0.2514 ± 0.0154
	GRU	-	0.0751 ± 0.0037	0.1724 ± 0.0130	0.2446 ± 0.0241
	RGN	k-NN	0.0790 ± 0.0113	0.1815 ± 0.0239	0.2548 ± 0.0210
SouthEast		TriMesh	0.0932 ± 0.0105	0.2076 ± 0.0200	0.2854 ± 0.0211
	RGN	k-NN	0.0942 ± 0.0121	0.2135 ± 0.0187	0.2902 ± 0.0348
	(StandardOP)	TriMesh	0.0868 ± 0.0132	0.1885 ± 0.0305	0.2568 ± 0.0328
	RGN	k-NN	0.0913 ± 0.0016	0.2069 ± 0.0031	0.2649 ± 0.0092
	(MeshOP)	TriMesh	0.0877 ± 0.0020	0.2043 ± 0.0026	0.2579 ± 0.0057
	PA-DGN	k-NN	0.0721 ± 0.0002	0.1664 ± 0.0011	0.2408 ± 0.0056
		TriMesh	0.0876 ± 0.0096	0.2002 ± 0.0163	0.2623 ± 0.0180
Table 5 and Table 6 show the effect of different graph structures on the synthetic dataset used in
Section 3.2 and the real-world dataset in Section 4.2 separately. We find that for different models
the effect of graph structures is not homogeneous. For RGN and PA-DGN, k-NN graph is more
beneficial to the prediction performance than TriMesh graph, because these two models rely more
on neighboring information and a k-NN graph incorporates it better than a Delaunay Triangulation
graph. However, switching from TriMesh graph to k-NN graph is harmful to the prediction accuracy
of RGN(MeshOP) since Delaunay Triangulation is a well-defined method for generating triangulated
mesh in contrast to k-NN graphs. Given the various effect of graph structures on different models, our
proposed PA-DGN under k-NN graphs always outperforms other baselines using any graph structure.
13
Published as a conference paper at ICLR 2020
0.30
0.25
0.20
0.15
0.10
0.35
0.05
0.«
«.35
0.30
«.25
0.20
0.15
0.10
Figure 7: MAE across the nodes.
A.4 The distribution of prediction error across nodes
Figure 7 provides the distribution of MAEs across the nodes of PA-DGN applied to the graph signal
prediction task of the west coast region of the real-world dataset in Section 4.2. As shown in the
figure, nodes with the highest prediction error for short-term prediction are gathered in the inner part
where the observable nodes are sparse, while for long-term prediction nodes in the area with a limited
number of observable points no longer have the largest MAE. This implies that PA-DGN can utilize
neighboring information efficiently even under the limitation of sparsely observable points.
A.5 Evaluation on NEMO sea surface temperature (SST) dataset
We tested our proposed method and baselines on the NEMO sea surface temperature (SST) dataset1.
We first download the data in the area between 50N0 -65N° and 75W0 -10W0 starting from 2016-01-
01 to 2017-12-31, then we crop the [0, 550] × [100, 650] square from the area and sample 250 points
from the square as our chosen dataset. We divide the data into 24 sequences, each lasting 30 days,
and truncate the tail. All models use the first 5-day SST as input and predict the SST in the following
15 and 25 days. We use the data in 2016 for training all models and the left for testing.
For StandardOP, MeshOP and SDL, we test both options using linear regression and using RGN
for the prediction part and report the best result. The results in Table 7 show that all methods
incorporating spatial differences gain improvement on prediction and that our proposed learnable
SDL outperforms all other baselines.
Table 7: Mean absolute error (10-2) for SST graph signal prediction.
	VAR	MLP	GRU	RGU	StandardOP	MeshOP	SDL
15-step	15.123	15.058	15.101	15.172	14.756	14.607	14.382
25-step	19.533	19.473	19.522	19.705	18.983	18.977	18.434
1Available at http://marine.copernicus.eu/services-portfolio/access-to-products/?option=com_csw&view=
details&product_id=GLOBAL_ANALYSIS_FORECAST_PHY_001_024.
14
Published as a conference paper at ICLR 2020
A.6 Evaluation on datasets with different sparsity
We changed the number of nodes to control the sparsity of data. As shown in Table 8, our proposed
model outperforms others under various settings of sparsity on the synthetic experiment in Section
3.2.
Table 8: Mean absolute error (10-2) for graph signal prediction with different sparsity.
#Nodes	VAR	MLP	StandardOP	MeshOP	SDL
250	0.1730	0.1627	0.1200	0.1287	0.1104
150	0.1868	0.1729	0.1495	0.1576	0.1482
100	0.1723	0.1589	0.1629	0.1696	0.1465
Furthermore, we sampled 400 points and trained SDL as described in Section 3.1, and resampled
fewer points (350,300,250,200) to evaluate if SDL generalizes less sparse setting. As Table 9 shows,
MSE increases when fewer sample points are used. However, SDL is able to provide much more
accurate gradients even if it is trained under a new graph with different properties. Thus, the results
support that SDL is able to generalize the c setting.
Table 9: Mean squared error (10-2) for approximations of directional derivatives of function
f2(x, y) = sin (x) + cos (y) with different sparsity.
Method	350 Nodes	300 Nodes	250 Nodes	200 Nodes
FinGrad	2.88 ± 0.11	3.42 ± 0.14	3.96 ± 0.17	4.99 ± 0.31
SDL	1.03 ± 0.09	1.14 ± 0.12	1.40 ± 0.10	1.76 ± 0.10
15