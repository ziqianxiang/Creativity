Published as a conference paper at ICLR 2020
Depth-Adaptive Transformer
Maha Elbayad*
Univ. Grenoble Alpes
Jiatao Gu, Edouard Grave, Michael Auli
Facebook AI Research
Ab stract
State of the art sequence-to-sequence models for large scale tasks perform a fixed
number of computations for each input sequence regardless of whether it is easy or
hard to process. In this paper, we train Transformer models which can make out-
put predictions at different stages of the network and we investigate different ways
to predict how much computation is required for a particular sequence. Unlike
dynamic computation in Universal Transformers, which applies the same set of
layers iteratively, we apply different layers at every step to adjust both the amount
of computation as well as the model capacity. On IWSLT German-English trans-
lation our approach matches the accuracy of a well tuned baseline Transformer
while using less than a quarter of the decoder layers.
1	Introduction
The size of modern neural sequence models (Gehring et al., 2017; Vaswani et al., 2017; Devlin et al.,
2019) can amount to billions of parameters (Radford et al., 2019). For example, the winning entry
of the WMT’19 news machine translation task in English-German used an ensemble totaling two
billion parameters (Ng et al., 2019). While large models are required to do better on hard examples,
small models are likely to perform as well on easy ones, e.g., the aforementioned ensemble is prob-
ably not required to translate a short phrase such as "Thank you". However, current models apply
the same amount of computation regardless of whether the input is easy or hard.
In this paper, we propose Transformers which adapt the number of layers to each input in order to
achieve a good speed-accuracy trade off at inference time. We extend Graves (2016; ACT) who
introduced dynamic computation to recurrent neural networks in several ways: we apply different
layers at each stage, we investigate a range of designs and training targets for the halting module and
we explicitly supervise through simple oracles to achieve good performance on large-scale tasks.
Universal Transformers (UT) rely on ACT for dynamic computation and repeatedly apply the same
layer (Dehghani et al., 2018). Our work considers a variety of mechanisms to estimate the network
depth and applies a different layer at each step. Moreover, Dehghani et al. (2018) fix the number
of steps for large-scale machine translation whereas we vary the number of steps to demonstrate
substantial improvements in speed at no loss in accuracy. UT uses a layer which contains as many
weights as an entire standard Transformer and this layer is applied several times which impacts
speed. Our approach does not increase the size of individual layers. We also extend the resource
efficient object classification work of Huang et al. (2017) and Bolukbasi et al. (2017) to structured
prediction where dynamic computation decisions impact future computation. Related work from
computer vision includes Teerapittayanon et al. (2016); Figurnov et al. (2017) and Wang et al. (2018)
who explored the idea of dynamic routing either by exiting early or by skipping layers.
We encode the input sequence using a standard Transformer encoder to generate the output sequence
with a varying amount of computation in the decoder network. Dynamic computation poses a chal-
lenge for self-attention because omitted layers in prior time-steps may be required in the future.
We experiment with two approaches to address this and show that a simple approach works well
(§2). Next, we investigate different mechanisms to control the amount of computation in the de-
coder network, either for the entire sequence or on a per-token basis. This includes multinomial
and binomial classifiers supervised by the model likelihood or whether the argmax is already correct
as well as simply thresholding the model score (§3). Experiments on IWSLT14 German-English
*Work done during an internship at Facebook AI Research.
1
Published as a conference paper at ICLR 2020
□ State QCopied state 日 Classifier ■■■■> Copy
(a) Aligned training
Figure 1: Training regimes for decoder networks able to emit outputs at any layer. Aligned training
optimizes all output classifiers Cn simultaneously assuming all previous hidden states for the current
layer are available. Mixed training samples M paths of random exits at which the model is assumed
to have exited; missing previous hidden states are copied from below.
(b) Mixed training
translation (Cettolo et al., 2014) as well as WMT’14 English-French translation show that we can
match the performance of well tuned baseline models at up to 76% less computation (§4).
2	Anytime structured prediction
We first present a model that can make predictions at different layers. This is known as anytime
prediction for computer vision models (Huang et al., 2017) and we extend it to structured prediction.
2.1	Transformer with multiple output classifiers
We base our approach on the Transformer sequence-to-sequence model (Vaswani et al., 2017). Both
encoder and decoder networks contain N stacked blocks where each has several sub-blocks sur-
rounded by residual skip-connections. The first sub-block is a multi-head dot-product self-attention
and the second a position-wise fully connected feed-forward network. For the decoder, there is an
additional sub-block after the self-attention to add source context via another multi-head attention.
Given a pair of source-target sequences (x, y), x is processed with the encoder to give representa-
tions s = (s1, . . . , s|x|). Next, the decoder generates y step-by-step. For every new token yt input
to the decoder at time t, the N decoder blocks process it to yield hidden states (htn)1≤n≤N :
ht0 = embed(yt),	htn = blockn (hn≤-t 1, s),	(1)
where blockn is the mapping associated with the nth block and embed is a lookup table.
The output distribution for predicting the next token is computed by feeding the activations of the
last decoder layer htN into a softmax normalized output classifier W :
p(yt+1 |htN) = softmax(W htN)	(2)
Standard Transformers have a single output classifier attached to the top of the decoder network.
However, for dynamic computation we need to be able to make predictions at different stages of the
network. To achieve this, we attach output classifiers Cn parameterized by Wn to the output htn of
each of the N decoder blocks:
∀n, p(yt+1|htn) = softmax(Wnhtn)	(3)
The classifiers can be parameterized independently or we can share the weights across the N blocks.
2.2	Training multiple output classifiers
Dynamic computation enables the model to use any of the N exit classifiers instead of just the final
one. Some of our models can choose a different output classifier at each time-step which results in
an exponential number of possible output classifier combinations in the sequence length.
2
Published as a conference paper at ICLR 2020
We consider two possible ways to train the decoder network (Figure 1). Aligned training optimizes
all classifiers simultaneously and assumes all previous hidden states required by the self-attention
are available. However, at test time this is often not the case when we choose a different exit for
every token which leads to misaligned states. Instead, mixed training samples several sequences of
exits for a given sentence and exposes the model to hidden states from different layers.
Generally, for a given output sequence y, we have a sequence of chosen exits (n1, . . . , n|y|) and we
denote the block at which we exit at time t as nt .
2.2.1	Aligned training
Aligned training assumes all hidden states h1n-1, . . . , htn-1 are available in order to compute self-
attention and it optimizes N loss terms, one for each exit (Figure 1a):
|y|	1 N
LLn = logp(yt∖hn-1),	LLn = X LLn,	Ldec(X, y) = -q-------- X ωnLLn .	(4)
t=1	n ωn n=1
The compound loss Ldec(x, y) is a weighted average of N terms w.r.t. to (ω1, . . . ωN). We found
that uniform weights achieve better BLEU compared to other weighing schemes (c.f . Appendix A).
At inference time, not all time-steps will have hidden states for the current layer since the model
exited early. In this case, we simply copy the last computed state to all upper layers, similar to mixed
training (§2.2.2). However, we do apply layer-specific key and value projections to the copied state.
2.2.2	Mixed training
Aligned training assumes that all hidden states of the previous time-steps are available but this as-
sumption is unrealistic since an early exit may have been chosen previously. This creates a mismatch
between training and testing. Mixed training reduces the mismatch by training the model to use hid-
den states from different blocks of previous time-steps for self-attention. We sample M different
exit sequences (nɪm),... n(m))1< <乂 and evaluate the following loss:
|y|	1 M
LL(nι,...,n∣y∣) = X log p(yt∣hn- J,	Ldec(x, y) = - M X LL(n1jm),..., n(/)).	(5)
t=1	m=1
When nt < N, we copy the last evaluated hidden state htn to the subsequent layers so that the
self-attention of future time steps can function as usual (see Figure 1b).
3	Adaptive depth estimation
We present a variety of mechanisms to predict the decoder block at which the model will stop
and output the next token, or when it should exit to achieve a good speed-accuracy trade-off. We
consider two approaches: sequence-specific depth decodes all output tokens using the same block
(§3.1) while token-specific depth determines a separate exit for each individual token (§3.2).
We model the distribution of exiting at time-step t with a parametric distribution qt where qt (n)
is the probability of computing block1 , . . . , blockn and then emitting a prediction with Cn . The
parameters of qt are optimized to match an oracle distribution qJ= with cross-entropy:
Lexit(χ, y) = X H(qt(χ, y),qt(χ))	(6)
t
The exit loss (Lexit) is back-propagated to the encoder-decoder parameters. We simultaneously
optimize the decoding loss (Eq. (4)) and the exit loss (Eq. (6)) balanced by a hyper-parameter α to
ensure that the model maintains good generation accuracy. The final loss takes the form:
L(x, y) = Ldec(x, y) + αLexit(x, y),	(7)
In the following we describe for each approach how the exit distribution qt is modeled (illustrated
in Figure 2) and how the oracle distribution qtJ is inferred.
3
Published as a conference paper at ICLR 2020
StaState □ Copiedstate (I)Halting decision 日 CIaSSifiier …ACoPy
回
□ □
AAA
□	□
A	A
S
∙→ 国□ ■	»国□ ■
ΔΔΔ	A _ A . ʌ

Decoding step	Decoding step
(a) Sequence-specific depth (b) Token-specific - Multinomial
Decoding step
(c) Token-specific - Geometric-like
Figure 2: Variants of the adaptive depth prediction classifiers. Sequence-specific depth uses a multi-
nomial classifier to choose an exit for the entire output sequence based on the encoder output s (2a).
It then outputs a token at this depth with classifier Cn . The token-specific multinomial classifier
determines the exit after the first block and proceeds up to the predicted depth before outputting the
next token (2b). The token geometric-like classifier (2c) makes a binary decision after every block
to dictate whether to continue (C) to the next block or to stop (S) and emit an output distribution.
3.1	Sequence-specific depth:
For sequence-specific depth, the exit distribution q and the oracle distribution q* are independent of
the time-step so we drop subscript t. We condition the exit on the source sequence by feeding the
average s of the encoder outputs to a multinomial classifier:
S = ɪ ^X st,	q(n∣x) = Softmax(Whs + bh) ∈ RN,
(8)
where Wh and bh are the weights and biases of the halting mechanism. We consider two oracles to
determine which of the N blocks should be chosen. The first is based on the sequence likelihood
and the second looks at an aggregate of the correctly predicted tokens at each block.
Likelihood-based: This oracle is based on the likelihood of the entire sequence after each block
and we optimize it with the Dirac delta centered around the exit with the highest sequence likelihood.
q*(x, y) = δ(arg max LLn).
n
We add a regularization term to encourage lower exits that achieve good likelihood:
q*(x, y) = δ(arg max LLn -λn).
n
(9)
Correctness-based: Likelihood ignores whether the model already assigns the highest score to
the correct target. Instead, this oracle chooses the lowest block that assigns the largest score to
the correct prediction. For each block, we count the number of correctly predicted tokens over
the sequence and choose the block with the most number of correct tokens. A regularization term
controls the trade-off between speed and accuracy.
n
Cn = #{t | yt = arg max p(y|htn-1)}, q*(x, y) = δ(arg max Cn - λn).
(10)
y
Oracles based on test metrics such as BLEU are feasible but expensive to compute since we would
need to decode every training sentence N times. We leave this for future work.
3.2	Token-specific depth:
The token-specific approach can choose a different exit at every time-step. We consider two options
for the exit distribution qt at time-step t: a multinomial with a classifier conditioned on the first
decoder hidden state ht1 and a geometric-like where an exit probability χtn is estimated after each
block based on the activations of the current block htn .
4
Published as a conference paper at ICLR 2020
Multinomial qt :
qt(n|x, y<t) = softmax(Whht1 + bh),	(11)
The most probable exit argmaxqt(n|x, y<t) is selected at inference.
Geometric-like qt :
∀n ∈ [1..N -1], χtn = sigmoid(wh>htn+bh),	(12)
(Xn Q (1-χn0), if n<N
qt(n|x,y<t)=	n0<n n0	(13)
<t	(1-χtn ), otherwise
n0<N
where, d is the dimension of the decoder states, Wh ∈ RN ×d and wh ∈ Rd are the weights of
the halting mechanisms, and bh their biases. During inference the decoder exits when the halting
signal χtn exceeds a threshold τn which we tune on the valid set to achieve a better accuracy-speed
trade-off. If thresholds (τn)1≤n<N have not been exceeded, then we default to exiting at block N.
The two classifiers are trained to minimize the cross-entropy with respect to either one the following
oracle distributions:
Likelihood-based: At each time-step t, we choose the block whose exit classifier has the highest
likelihood plus a regularization term weighted by λ to encourage lower exits.
qt (x, y) = δ(argmaxLL; —λn)	(14)
n
This oracle ignores the impact of the current decision on the future time-steps and we therefore
consider smoothing the likelihoods with an RBF kernel.
|t-t0|2	|y|
κ(t,t0) = e-J, LLn = ^X κ(t,t0) LLn, qt (x, y) = 6(argmaxLL： — λn),	(15)
t0=1	n
where we control the size of the surrounding context with σ the kernel width. We refer to this oracle
as LL(σ, λ) including the case where we only look at the likelihood of the current token with σ → 0.
Correctness-based: Similar to the likelihood-based oracle we can look at the correctness of the
prediction at time-step t as well as surrounding positions. We define the target qtt as follows:
|y|
Cn = l[yt = argmaxp(y∣hn-ι)], Cn = X κ(t,t0) Cn,
y	t0=1
t
qtt (x, y) = δ(arg max Ctn — λn).
(16)
(17)
Confidence thresholding Finally, we consider thresholding the model predictions (§2), i.e., exit
when the maximum score of the current output classifier p(yt+1 |htn) exceeds a hyper-parameter
threshold τn. This does not require training and the thresholds τ = (τ1, . . . , τN-1) are simply
tuned on the valid set to maximize BLEU. Concretely, for 10k iterations, we sample a sequence of
thresholds T 〜U(0, I)N-1, decode the valid set with the sampled thresholds and then evaluate the
BLEU score and computational cost achieved with this choice of τ . After 10k evaluations we pick
the best performing thresholds, that is τ with the highest BLEU in each cost segment.
4 Experiments
4.1	Experimental setup
We evaluate on several benchmarks and measure tokenized BLEU (Papineni et al., 2002):
IWSLT’14 German to English (De-En). We use the setup of Edunov et al. (2018) and train on
160K sentence pairs. We use N = 6 blocks, a feed-forward network (ffn) of intermediate-dimension
5
Published as a conference paper at ICLR 2020
Uniform			n = 1	n = 2	n=3	n=4	n=5	n = 6 I Average	
Baseline		-	34.2	35.3	35.6	35.7	35.6	35.9	35.4
Aligned (ωn	1)	35.5	34.1	35.5	35.8	36.1	36.1	36.2	-35.6-
Mixed M =	1	34.1	32.9	34.3	34.5	34.5	34.6	34.5	34.2
Mixed M =	3	35.1	33.9	35.2	35.4	35.5	35.5	35.5	35.2
Mixed M =	6	35.3	34.2	35.4	35.8	35.9	35.8	35.9	35.5
Table 1: Aligned vs. mixed training on IWSLT De-En. We report valid BLEU for a uniformly
sampled exit n 〜U([1..6]) at each token, a fixed exit n ∈ [1..6] for all tokens, as well as the average
BLEU over the fixed exits. As baseline we show six standard Transformer models with 1-6 blocks.
1024, 4 heads, dropout 0.3, embedding dimension denc = 512 for the encoder and ddec = 256 for
the decoder. Embeddings are untied with 6 different output classifiers. We evaluate with a single
checkpoint and a beam of width 5.
WMT’14 English to French (En-Fr). We also experiment on the much larger WMT’14 English-
French task comprising 35.5m training sentence pairs. We develop on 26k held out pairs and test
on newstest14. The vocabulary consists of 44k joint BPE types (Sennrich et al., 2016). We use
a Transformer big architecture and tie the embeddings of the encoder, the decoder and the output
classifiers ((Wn)1≤n≤6; §2.1). We average the last ten checkpoints and use a beam of width 4.
Models are implemented in fairseq (Ott et al., 2019) and are trained with Adam (Kingma & Ba,
2015). We train for 50k updates on 128 GPUs with a batch size of 460k tokens for WMT’14 En-Fr
and on 2 GPUs with 8k tokens per batch for IWSLT’14 De-En. To stabilize training, we re-normalize
the gradients if the norm exceeds gclip = 3.
For models with adaptive exits, we first train without exit prediction (α = 0 in Eq. (7)) using the
aligned mode (c.f. §2.2.1) for 50k updates and then continue training with α 6= 0 until convergence.
The exit prediction classifiers are parameterized by a single linear layer (Eq. (8)) with the same input
dimension as the embedding dimension, e.g., 1024 for abig Transformer; the output dimension is N
for a multinomial classifier or one for geometric-like. We exit when χt,n > 0.5 for geometric-like
classifiers.
4.2	Training multiple output classifiers
We first compare the two training regimes for our model (§2.2). Aligned training performs self-
attention on aligned states (§2.2.1) and mixed training exposes self-attention to hidden states from
different blocks (§2.2.2).
We compare the two training modes when choosing either a uniformly sampled exit or a fixed exit
n = 1, . . . , 6 at inference time for every time-step. The sampled exit experiment tests the robustness
to mixed hidden states and the fixed exit setup simulates an ideal setting where all previous states
are available. As baselines we show six separate standard Transformers with N ∈ [1..6] decoder
blocks. All models are trained with an equal number of updates and mixed training with M=6 paths
is most comparable to aligned training since the number of losses per sample is identical.
Table 1 shows that aligned training outperforms mixed training both for fixed exits as well as for ran-
domly sampled exits. The latter is surprising since aligned training never exposes the self-attention
mechanism to hidden states from other blocks. We suspect that this is due to the residual connections
which copy features from lower blocks to subsequent layers and which are ubiquitous in Transformer
models (§2). Aligned training also performs very competitively to the individual baseline models.
Aligned training is conceptually simple and fast. We can process a training example with N exits
in a single forward/backward pass while M passes are needed for mixed training. In the remaining
paper, we use the aligned mode to train our models. Appendix A reports experiments with weighing
the various output classifiers differently but we found that a uniform weighting scheme worked
well. On our largest setup, WMT’14 English-French, the training time ofan aligned model with six
output classifiers increases only marginally by about 1% compared to a baseline with a single output
classifier keeping everything else equal.
6
Published as a conference paper at ICLR 2020
5
.
5
3
(a) Token-specfic
0 5 0 5
............................
5443
3333
UELB
123456
Average exit (AE)
X
(b) Sequence-specific depth
∩wjm
123456
Average exit (AE)
0 5 0 5
............................
5443
3333
UELB
(c) Confidence thresholding
35.5
■ Baseline
Aligned
▼ TOk-C Geometric-Iike
▲ Tok-LL Geometric-like
❸ Confidence thresholding
123456
Average exit (AE)

Figure 3:	Trade-off between speed (average exit or AE) and accuracy (BLEU) for depth-adaptive
methods on the IWSLT14 De-En test set.
)EA( tixe egarev
Regularization parameter λ
(a) Effect of λ on AE
)EA( tixe egarevA
0123
RBF kernel width σ
(b) Effect of σ on AE
Figure 4:	Effect of the hyper-parameters σ and λ on the average exit (AE) measured on the valid set
of IWSLT’14 De-En.
4.3 Adaptive depth estimation
Next, we train models with aligned states and compare adaptive depth classifiers in terms of BLEU
as well as computational effort. We measure the latter as the average exit per output token (AE).
As baselines we use again six separate standard Transformers with N ∈ [1..6] with a single output
classifier. We also measure the performance of the aligned mode trained model for fixed exits n ∈
[1..6]. For the adaptive depth token-specific models (Tok), we train four combinations: likelihood-
based oracle (LL) + geometric-like, likelihood-based oracle (LL) + multinomial, correctness based
oracle (C) + geometric-like and correctness-based oracle (C) + multinomial. Sequence-specific
models (Seq) are trained with the correctness oracle (C) and the likelihood oracle (LL) with different
values for the regularization weight λ. All parameters are tuned on the valid set and we report results
on the test set for a range of average exits.
Figure 3 shows that the aligned model (blue line) can match the accuracy of a standard 6-block
Transformer (black line) at half the number of layers (n = 3) by always exiting at the third block.
The aligned model outperforms the baseline for n = 2, . . . , 6.
For token specific halting mechanisms (Figure 3a) the geometric-like classifiers achieves a better
speed-accuracy trade-off than the multinomial classifiers (filled vs. empty triangles). For geometric-
like classifiers, the correctness oracle outperforms the likelihood oracle (Tok-C geometric-like vs.
Tok-LL geometric-like) but the trend is less clear for multinomial classifiers. At the sequence-level,
likelihood is the better oracle (Figure 3b).
The rightmost Tok-C geometric-like point (σ = 0, λ = 0.1) achieves 34.73 BLEU at AE = 1.42
which corresponds to similar accuracy as the N = 6 baseline at 76% fewer decoding blocks.
7
Published as a conference paper at ICLR 2020
(a) BLEU vs. AE (test)
44.0
43.5
43.0
42.5
42.0
41.5
Average exit (AE)
(b) BLEU vs. FLOPs (test)
Figure 5:	Speed and accuracy on the WMT’14 English-French benchmark (c.f. Figure 3).
The best accuracy of the aligned model is 34.95 BLEU at exit 5 and the best comparable Tok-C
geometric-like configuration achieves 34.99 BLEU at AE = 1.97, or 61% fewer decoding blocks.
When fixing the budget to two decoder blocks, Tok-C geometric-like with AE = 1.97 achieves
BLEU 35, a 0.64 BLEU improvement over the baseline (N = 2) and aligned which both achieve
BLEU 34.35.
Confidence thresholding (Figure 3c) performs very well but cannot outperform Tok-C geometric-
like.
Ablation of hyper-parameters In this section, we look at the effect of the two main hyper-
parameters on IWSLT’14 De-En: λ the regularization scale (c.f. Eq. (9)), and the RBF kernel width
σ used to smooth the scores (c.f. Eq. (15)). We train Tok-LL Geometric-like models and evaluate
them with their default thresholds (exit if χtn > 0.5). Figure 4a shows that higher values of λ lead
to lower exits. Figure 4b shows the effect of σ for two values of λ. In both curves, we see that wider
kernels favor higher exits.
4.4	Scaling the adaptive-depth models
Finally, we take the best performing models form the IWSLT benchmark and test them on the large
WMT’14 English-French benchmark. Results on the test set (Figure 5a) show that adaptive depth
still shows improvements but that they are diminished in this very large-scale setup. Confidence
thresholding works very well and sequence-specific depth approaches improve only marginally over
the baseline. Tok-LL geometric-like can match the best baseline result of BLEU 43.4 (N = 6) by
using only AE = 2.40 which corresponds to 40% of the decoder blocks; the best aligned result
of BLEU 43.6 can be matched with AE = 3.25. In this setup, Tok-LL geometric-like slightly
outperforms the Tok-C counterpart.
Confidence thresholding matches the accuracy of the N=6 baseline with AE 2.5 or 59% fewer de-
coding blocks. However, confidence thresholding requires computing the output classifier at each
block to determine whether to halt or continue. This is a large overhead since output classifiers pre-
dict 44k types for this benchmark (§4.1). To better account for this, we measure the average number
of FLOPs per output token (details in Appendix B). Figure 5b shows that the Tok-LL geometric-like
approach provides a better trade-off when the overhead of the output classifiers is considered.
4.5	Qualitative results
The exit distribution for a given sample can give insights into what a Depth-Adaptive Transformer
decoder considers to be a difficult task. In this section, for each hypothesis ye, we will look at
the sequence of selected exits (n1, . . . , n|ye|) and the probability scores (p1, . . . p|ye|) with pt =
p(yet|htn-t 1) i.e. the confidence of the model in the sampled token at the selected exit.
Figures 6 and 7 show hypotheses from the WMT’14 En-Fr and IWSLT’14 De-En test sets, respec-
tively. For each hypothesis we state the exits and the probability scores. In Figure 6a, predicting
8
Published as a conference paper at ICLR 2020
八 SR
s—P①UlUllf
SjUoUl 能UEqO
SoP
♦
OJPUOJjF
CS
SFd
JUow >0 P
əu
Ss胃 SFd
Sol
S
654321
8642
■ i i ■
(a) Src: Chi@@rac , the Prime Minister , was there . (b) Src: But passengers shoul@@dn't expect changes to
Ref: Chi@@rac , Premier ministre , est la .	happen immediately .
Ref: Mais les passagers ne devraient pas s' attendre a
des changements immediats .
Figure 6: Examples from the WMT'14 En-Fr test set (newstest14) with Tok-LL geometric-like depth
estimation. Token exits are in blue and confidence scores are in gray. The ‘@@' are due to BPE or
subword tokenization. For each example the source (Src) and the reference (Ref) are provided in
the caption.
(a) Src: diesen trick konnen Sie ihren freunden Und nachbarn vor@@fuhren . danke .
Ref: there is a trick you can do for your friends and neighb@@ors . thanks .
Figure 7: Example from the IWSLT’14 De-En teSt Set with Tok-LL geometric-like depth eStimation.
See Figure 6 for more detailS.
’present' (meaning 'present') is hard. A straightforward translation is '6tait la, but the model chooses
‘preSent’ which iS alSo appropriate. In Figure 6b, the model uSeS more computation to predict the
definite article ‘les' since the source has omitted the article for ‘passengers'.
A clear trend in both benchmarks is that the model requires less computation near the end of de-
coding to generate the end of sequence marker </s> and the preceding full-stop when relevant. In
Figure 8, we show the distribution of the exits at the beginning and near the end of test set hypothe-
ses. We consider the beginning of a sequence to be the first 10% of tokens and the end as the last
10% of tokens. The exit distributions are shown for three models on WMT'14 En-Fr: Model1 has
an average exit ofAE = 2.53, Model2 exits at AE = 3.79 on average and Model3 with AE = 4.68.
Within the same models, deep exits late are used at the beginning of the sequence and early exits are
selected near the end. For heavily regularized models such as Model1 with AE = 2.53, the disparity
between beginning and end is less severe as the model exits early most of the time. Model2 and
Model3 are less regularized (higher AE) and tend to use late exits at the beginning of the sequence
and early exits near the end. On the other hand, the more regularized Model1 with AE = 2.53 exits
Model1	Model2	Model3
642
...
000
ycneuqer
123456 123456 123456
Exit
Figure 8: WMT'14 En-Fr test set: exit distributions in the beginning (relative-position: rpos<0.1)
and near the end (rpos>0.9) of the hypotheses of three models.
9
Published as a conference paper at ICLR 2020
0.9-1.0
0.8-0.9
0.7-0.8
0.6-0.7
0.5-0.6
0.4-0.5
0.3-0.4
0.2-0.3
0.1-0.2
0.0-0.1
Exit
Figure 9: Joint histogram of the exits and the confidence scores for 3 Tok-LL geometric-like models
on newstest14.
early most of the time. There is also a correlation between the model probability and the amount
of computation, particularly in models with low AE . Figure 9 shows the joint histogram of the
scores and the selected exit. For both Model1 and Model2 , low exits (n ≤ 2) are used in the high
confidence range [0.8 - 1] and high exits (n ≥ 4) are used in the low-confidence range [0 - 0.5].
Model3 has a high average exit (AE = 4.68) so most tokens exit late, however, in low confidence
ranges the model does not exit earlier than n = 5.
5 Conclusion
We extended anytime prediction to the structured prediction setting and introduced simple but effec-
tive methods to equip sequence models to make predictions at different points in the network. We
compared a number of different mechanisms to predict the required network depth and find that a
simple correctness based geometric-like classifier obtains the best trade-off between speed and ac-
curacy. Results show that the number of decoder layers can be reduced by more than three quarters
at no loss in accuracy compared to a well tuned Transformer baseline.
Acknowledgments
We thank Laurens van der Maaten for fruitful comments and suggestions.
References
Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks
for efficient inference. In Proc. of ICML, 2017.
M. Cettolo, J. Niehues, S. Stuker, L. Bentivogli, and M. Federico. Report on the 11th iwslt evaluation
campaign. In IWSLT, 2014.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
transformers. In Proc. of ICLR, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proc. of NAACL, 2019.
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. Classical
structured prediction losses for sequence to sequence learning. In Proc. of NAACL, 2018.
Michael Figurnov, Artem Sobolev, and Dmitry P. Vetrov. Probabilistic adaptive computation time.
In ArXiv preprint, 2017.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional
sequence to sequence learning. In Proc. of ICML, 2017.
Alex Graves. Adaptive computation time for recurrent neural networks. In ArXiv preprint, 2016.
10
Published as a conference paper at ICLR 2020
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q Weinberger.
Multi-scale dense networks for resource efficient image classification. In Proc. of ICLR, 2017.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. of ICLR, 2015.
Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. Facebook fair’s
wmt19 news translation task submission. In Proc. of WMT, 2019.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. Fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL,
2019.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a method for automatic evaluation of
machine translation. In Proc. of ACL, 2002.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. In Technical report, OpenAI., 2019.
R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword
units. In Proc. of ACL, 2016.
Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via
early exiting from deep neural networks. In ICPR, 2016.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin.
Attention is all you need. In Proc. of NeurIPS, 2017.
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dy-
namic routing in convolutional networks. In Proc. of ECCV, 2018.
11
Published as a conference paper at ICLR 2020
Appendix A Loss scaling
In this section we experiment with different weights for scaling the output classifier losses. Instead
of uniform weighting, we bias towards specific output classifiers by assigning higher weights to their
losses. Table 2 shows that weighing the classifiers equally provides good results.
Uniform			n = 1	n=2	n = 3	n=4	n=5	n=6	Average
Baseline		-	34.2	35.3	35.6	35.7	35.6	35.9	35.4
ωn	=1	35.5	34.1	35.5	35.8	36.1	36.1	36.2	-35.6-
ωn	=n	35.3	32.2	35.0	35.8	36.0	36.2	36.3	35.2
ωn	二 √n	35.4	33.3	35.2	35.8	35.9	36.1	36.1	35.4
ωn =	'/√n	35.6	34.5	35.4	35.7	35.8	35.8	35.9	35.5
ωn =	1/n	35.3	34.7	35.3	35.5	35.7	35.8	35.8	35.5
				(a) IWSLT De-En -		Valid			
Uniform			n = 1	n=2	n = 3	n=4	n=5	n=6	Average
Baseline		-	33.7	34.6	34.6	34.6	34.6	34.8	34.5
ωn	=1	34.4	33.2	34.4	34.8	34.9	35.0	34.9	-345-
ωn	=n	34.2	31.4	33.8	34.7	34.8	34.8	34.9	34.1
ωn	二 √n	34.4	32.5	34.1	34.8	34.9	35.0	35.1	34.4
ωn =	∖∣√n	34.6	33.7	34.3	34.6	34.8	34.8	34.9	34.5
ωn =	1/n	34.2	33.8	34.3	34.5	34.6	34.7	34.7	34.4
(b) IWSLT De-En - Test
Table 2: Aligned training with different weights (ωn) on IWSLT De-En. For each model we report
BLEU on the dev set evaluated with a uniformly sampled exit n 〜U([1..6]) for each token and a
fixed exit n ∈ [1..6] throughout the sequence. The average corresponds to the average BLEU over
the fixed exits.
Gradient scaling Adding intermediate supervision at different levels of the decoder results in
richer gradients for lower blocks compared to upper blocks. This is because earlier layers affect
more loss terms in the compound loss of Eq. (4). To balance the gradients of each block in the
decoder, we scale up the gradients of each loss term (- LLn) when it is updating the parameters
of its associated block (blockn with parameters θn ) and revert it back to its normal scale before
back-propagating it to the previous blocks. Figure 10 and Algorithm 1 illustrate this gradient scaling
procedure. The θn are updated with γn-amplifiedgradients from the block’s supervision and (N -n)
gradients from the subsequent blocks. We choose Yn = Y(N - n) to control the ratio y：1 as the
ratio of the block supervision to the subsequent blocks’ supervisions.
Table 3 shows that gradient scaling can benefit the lowest layer at the expense of higher layers.
However, no scaling generally works very well.
hn-1
▽ LLN
Figure 10: Illustration of gradient scaling.
12
Published as a conference paper at ICLR 2020
Algorithm 1 Pseudo-code for gradient scaling (illustrated for a single step t)
1:	for n ∈ 1..N do
2:	htn = blockn (htn-1)
3:	p(yt+1|htn) = softmax(Wnhtn)
4:	p(yt+1|htn) = SCALE_GRADIENT(p(yt+1|htn), γn)
5:	if n < N then hn = SCALE_GRADIENT(h7,--------)
γn+1
6:	end for
7:	function SCALE_GRADIENT(Tensor x, scale γ)
8:	return Yx + (1 - Y)STOP_GRADIENT(X)
9:	. S TOP_GRADIENT in PyTorch with x.detach().
10:	end function
Uniform			n = 1	n=2	n = 3	n=4	n=5	n = 6 I Average	
Baseline		-	34.2	35.3	35.6	35.7	35.6	35.9	35.4
	0	35.5	34.1	35.5	35.8	36.1	36.1	36.2	35.6
Y	= 0.3	35.1	33.7	34.7	35.3	35.7	35.8	36.0	35.2
Y	= 0.5	35.4	34.8	35.4	35.6	35.6	35.7	35.6	35.4
Y	= 0.7	34.9	34.6	35.1	35.1	35.2	35.4	35.3	35.1
Y	= 0.9	34.9	34.8	35.3	35.3	35.3	35.4	35.5	35.3
Y	= 1.1	35.1	34.9	35.2	35.3	35.3	35.3	35.3	35.2
(a) IWSLT De-En - Valid
Uniform			n = 1	n=2	n = 3	n=4	n=5	n = 6 I Average	
Baseline		-	33.7	34.6	34.6	34.6	34.6	34.8	34.5
	0	34.4	33.2	34.4	34.8	34.9	35.0	34.9	34.5
Y	= 0.3	34.2	32.8	33.9	34.3	34.6	34.8	35.0	34.2
Y	= 0.5	34.5	33.8	34.2	34.6	34.5	34.7	34.7	34.6
Y	= 0.7	34.0	33.7	34.2	34.3	34.3	34.3	34.3	34.2
Y	= 0.9	34.1	34.0	34.2	34.3	34.4	34.4	34.4	34.3
Y	= 1.1	34.2	34.0	34.3	34.3	34.3	34.3	34.2	34.2
(b) IWSLT De-En - Test
Table 3: Aligned training with different gradient scaling ratios Y : 1 on IWSLT’14 De-En. For each
model We report the BLEU4 score evaluated with a uniformly sampled exit n 〜 U([1..6]) for each
token and a fixed exit n ∈ [1..6]. The average corresponds to the average BLEU4 of all fixed exits.
13
Published as a conference paper at ICLR 2020
Appendix B	FLOPS approximation
This section details the computation of the FLOPS we report. The per token FLOPS are for the
decoder network only since we use an encoder of the same size for all models. We breakdown
the FLOPS of every operation in Algorithm 2 (blue front of the algorithmic statement). We omit
non-linearities, normalizations and residual connections. The main operations we account for are
dot-products and by extension matrix-vector products since those represent the vast majority of
FLOPS (we assume batch size one to simplify the calculation).
dddedf|xt
_____
Parameters
decoder embedding dimension.
encoder embedding dimension.
The feed-forward network dimension.
source length.
Current time-estep (t ≥ 1).
output vocabulary size.
Operation FLOPS
Dot-product (d)
Linear din → dout
2d - 1
2din dout
Table 4: FLOPS of basic operations, key parameters and variables for the FLOPS estimation.
With this breakdown, the total computational cost at time-step t of a decoder block that we actually
go through, denoted with FC, is:
FC(x, t) = 12d2d + 4df dd + 4tdd + 4|x|dd + 4[[FirstCall]]|x|ddde,
where the cost of mapping the source’ keys and values is incurred the first time the block is called
(flagged with FirstCall). This occurs at t = 1 for the baseline model but it is input-dependent with
depth adaptive estimation and may never occur if all tokens exit early.
If skipped, a block still has to compute the keys and value of the self-attention block so the self-
attention of future time-steps can function. We will denote this cost with FS and we have FS = 4d2d .
Depending on the halting mechanism, an exit prediction cost, denoted wit FP, is added:
Sequence-specific depth:	FP(t, q(t)) = 2[[t = 1]]N dd
Token-specific Multinomial:	FP(t, q(t)) = 2Ndd
Token-specific Geometric-like:	FP(t, q(t)) = 2ddq(t)
Confidence thresholding:	FP(t, q(t)) = 2q(t)V dd
For a set of source sequences {x(i) }i∈I and generated hypotheses {y(i)}i∈I, the average flops per
token is:
Baseline (N blocks): Pi 标明 Pi Pt=(：1 (N FC(X⑺,t) + 2Vdd)
Adaptive depth: Pi|y ⑸ | Pi Pt=：" (q(t)FC(x⑺,t) + (N - q(t))FS + FΡ(t, q(t)) + 2 Vdd)
In the case of confidence thresholding the final output prediction cost (2Vdd) is already accounted
for in the exit prediction cost FP.
14
Published as a conference paper at ICLR 2020
Algorithm 2 Adaptive decoding with Tok-geometric-like
1:	Input: source codes s, incremental state
2:	Initialization: t = 1, y1 = <s>
3:	for n ∈ 1 . . . N do
4:	FirstCall[n] = True. . A flag signaling if the source’ keys and values should be evaluated.
5:	end for
6:	while yt 6= </s> do
7:	Embed the last output token yt .
8:	for n ∈ 1 . . . N do
9:	. Self-attention.
10:	- Map the input into a key (k) and value (v). FLOPS=4d2d
11:	- Map the input into a query q. FLOPS=2d2d
12:	- Score the memory keys with q to get the attention weights α. FLOPS=4tdd
13:	- Map the attention output. FLOPS=2d2d
14:	. Encoder-Decoder interaction.
15:	if FirstCall[n] then
16:	Map the source states into keys and values for the nth block. FLOPS=4|x|dedd
17:	FirstCall[n] = False
18:	end if
19:	- Map the input into a query q. FLOPS=2d2d
20:	- Score the memory keys with q to get the attention weights α. FLOPS=4|x|dd
21:	- Map the attention output. FLOPS=2d2d
22:	Feed-forward network. FLOPS=4dddf
23:	Estimate the halting probability χt,n. FLOPS=2dd
24:	if χt,n > 0.5 then
25:	Exit the loop (Line 8)
26:	end if
27:	end for
28:	if n < N then
29:	. Skipped blocks.
30:	for ns ∈ n + 1 . . . N do
31:	Copy and map the copied state into a key (k) and value (v). FLOPS=4d2d
32:	end for
33:	end if
34:	Project the final state and sample a new output token. FLOPS=2V dd
35:	t++
36:	end while
15