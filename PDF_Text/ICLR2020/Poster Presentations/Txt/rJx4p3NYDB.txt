Published as a conference paper at ICLR 2020
Lazy-CFR: fast and near-optimal regret min-
IMIZATION FOR EXTENSIVE GAMES WITH IMPERFECT
INFORMATION
Yichi Zhou, Tongzheng Ren, Dong Yan, Jialian Li, Jun Zhu*
Dept. of Comp. Sci. & Tech., BNRist Center, Institute for AI, Tsinghua University; RealAI
{vofhqn,rtz19970824,sproblvem}@gmail.com,lijialian7@163.com,
dcszj@tsinghua.edu.cn
Ab stract
Counterfactual regret minimization (CFR) methods are effective for solving two-
player zero-sum extensive games with imperfect information. However, the vanilla
CFR has to traverse the whole game tree in each round, which is time-consuming
in large-scale games. In this paper, we present Lazy-CFR, a CFR algorithm that
adopts a lazy update strategy to avoid traversing the whole game tree in each round.
We prove that the regret of Lazy-CFR is almost the same as the regret of the vanilla
CFR and only needs to visit a small portion of the game tree. Thus, Lazy-CFR is
provably faster than CFR. Empirical results consistently show that Lazy-CFR is
fast in practice.
1	Introduction
Extensive games provide a mathematical framework for modeling the sequential decision-making
problems with imperfect information, which is common in economic decisions, negotiations and
security. We focus on solving two-player zero-sum extensive games with imperfect information
(TEGI). In a TEGI, there is an environment with uncertainty and two players on opposite sides (Koller
& Megiddo, 1992).
Counterfactual regret minimization (CFR) (Zinkevich et al., 2008) provides a state-of-the-art approach
for solving TEGIs with much progress in practice (Brown & Sandholm, 2017b; Moravcik et al.,
2017; Brown & Sandholm, 2019a). Regret minimization techniques are first introduced to solve
TEGIs based on the observation that minimizing the regrets of both players makes the time-averaged
strategy converge to the Nash Equilibrium (NE) (Nisan et al., 2007). CFR (Zinkevich et al., 2008)
further bounds the original regret with a summation of many immediate counterfactual regrets on each
information set (infoset). These immediate counterfactual regrets are defined by the counterfactual
rewards and can be iteratively minimized by existing online learning algorithms, e.g., regret matching
(RM) (Blackwell et al., 1956).
A limitation of CFR is that it requires traversing the whole game tree in each round, which is time-
consuming in large-scale games due to the fact that we have to apply RM to every immediate regret
in each round. Existing works on avoiding traversing the whole game tree can be mainly divided
into two categories: Pruning-based CFR (Brown & Sandholm, 2015; 2017a) and Monte-Carlo CFR
(MC-CFR) (Lanctot et al., 2009). These algorithms can significantly speed up the vanilla CFR in
practice. However, pruning-based algorithms may degenerate in the worst case. And the performance
of MC-CFR depends on the structure of the game and the chosen online learning algorithm.
Contributions: In this paper, we explore another approach to address the problem.
•	We present an algorithm named Lazy-CFR, which exploits a lazy update technique to avoid
traversing the whole game tree. Lazy-CFR divides the time horizon into segments, i.e.,
disjoint subsets with consecutive elements, and updates the strategy only at the beginning of
each segment and keeps the strategy the same within each segment. By this way we only
need to access each infoset at the beginning of each segment.
*J.Zis the corresponding author.
1
Published as a conference paper at ICLR 2020
•	We then extend the analysis on the regret of the vanilla CFR in (Burch, 2018) to Lazy-
CFR. We show that the regret of Lazy-CFR is almost the same as that of the vanilla CFR.
Combining with the analysis on the number of updates in Lazy-CFR, we show that Lazy-
CFR converges much faster than the vanilla CFR in theory, with some extra cost of memory.
•	Finally, we bound the regret from below. By comparing with the regret lower bound, we
show that the regret upper bounds of Lazy-CFR and the vanilla CFR are near-optimal.
We empirically evaluate Lazy-CFR, the vanilla CFR, MC-CFR (Lanctot et al., 2009), CFR+ (Bowling
et al., 2017) and MC-CFR+ on the standard benchmarks, Leduc Hold’em (Southey et al., 2005) and
heads-up flop hold’em poker (Brown et al., 2019). It is noteworthy that the same idea of Lazy-CFR
can also be applied to CFR+, and we name the resulted algorithm Lazy-CFR+. The analysis on
Lazy-CFR can be directly applied to Lazy-CFR+. Experiments show that Lazy-CFR and Lazy-CFR+
works well in practice as suggested by the theory.
2	Notations and Preliminaries
We first introduce the notations and definitions of extensive games and TEGIs. Then we introduce an
online learning concept of regret minimization, followed by CFR, which is based on the connection
between TEGIs and regret minimization. For clarity, most important notations appeared in this work
and their descriptions are listed in the look-up table in appendix A.
2.1	Extensive games
Extensive games (see Osborne & Rubinstein, 1994, pg. 200 for a formal definition) compactly model
the decision-making problems with sequential interactions among multiple agents. An extensive
game can be represented by a game tree H of histories, where a history is a sequence of actions in the
past. Suppose that there are N players participating in an extensive game and let c denote the chance
player which is usually used to model the uncertainty in the environment. Let [N ] := {1,…，N}.
A player function P is a mapping from H to [N] ∪ {c} such that P(h) is the player who takes an
action after h. And each player i ∈ [N] receives a reward ui(h) ∈ [-1, 1] at a terminal history h.
Let A(h) denote the set of valid actions of P (h) after h, that is, ∀a ∈ A(h), (h, a) ∈ H. Let
A = maxh |A(h)|. A strategy of player i is a function σi that assigns h a distribution over A(h) if
P(h) = i. A strategy profile σ consists of the strategy for each player, that is, σ = {σ1, ∙∙∙ , σN}.
We will use σ-i to refer to all the strategies in σ except σi. And we use the pair (σi, σ-i) to denote
the full strategy profile. In games with imperfect information, actions of other players are partially
observable to player i ∈ [N]. So for player i, some different histories may not be distinguishable.
Thus, the game tree can be partitioned into disjoint information sets (infoset). Let Ii denote the
collection of player i’s infosets. We have that two histories h, h0 ∈ I ∈ Ii are not distinguishable to
player i. Thus, σi must assign the same distribution over actions to all histories in infoset I ∈ Ii
if P(I) = i. With a little abuse of notations, we let σi (I) denote the strategy of player i on infoset
I ∈ Ii. It is clear that infosets of a player also form a tree, called infoset tree.
We let πσ(h) denote the probability of arriving at history h. Obviously, we can decompose πσ(h)
into the product of each player’s contribution, that is, πσ (h) = Q[N]∪{c} πσi (h). Similarly, we
define πσ(I) = Ph∈I πσ(h) as the probability of arriving at infoset I and let πσi (I) denote the
corresponding contribution of player i. Letπσ-i(h) and πσ-i(I) denote the product of the contributions
on arriving at h and I, respectively, of all players except player i.
With the above notations, a two-player zero-sum extensive game with imperfect information (TEGI)
is an extensive game with N = 2 and u1 (h) + u2 (h) = 0 for all terminal histories.
In game theory, the solution of a game is often referred to a Nash equilibrium (NE) (Osborne &
Rubinstein, 1994). In this paper, we concern on computing an approximation of an NE, namely an
-NE (Nisan et al., 2007). With a little abuse of notations, let ui (σ) denote the expected reward
of player i if all players take actions according to σ . An -NE is a strategy profile σ such that
∀i ∈ [N], ui (σ) ≥ maxσ0,i ui((σ0,i, σ-i)) - . And the -NE in a TEGI can be efficiently computed
by regret minimization; see later in this section.
2
Published as a conference paper at ICLR 2020
2.2	Regret minimization
Now we introduce regret, a core concept in online learning (Cesa-Bianchi & Lugosi, 2006). Many
powerful online learning algorithms can be framed as minimizing some kinds of regret, therefore
known as regret minimization algorithms. Generally, the regret is defined as follows:
Definition 1 (Regret). Consider the case where a player takes actions repeatedly. At each round, the
player selects an action wt ∈ Σ, where Σ is the set of valid actions. At the same time, the environment
selects a reward function ft. Then, the overall reward of the player is PtT=1 ft(wt), and the regret is
defined as RT = maxw0 ∈Σ PtT=1 ft(w0) - PtT=1 ft(wt).
One important example of online learning is online linear optimization (OLO) in which ft is a linear
function. If Σ is the set of distributions over some discrete set, an OLO can be solved by standard
regret minimization algorithms, e.g., regret matching (RM) (Blackwell et al., 1956; Abernethy
et al., 2011) or AdaHedge (Freund & Schapire, 1997). As CFR employs RM or AdaHedge as a
sub-procedure, we summarize them as follows:
Definition 2 (Online linear optimization (OLO), regret matching (RM) and AdaHedge). Consider
the online learning problem with linear rewards. In each round t, an agent plays a mixed strategy
wt ∈ ∆(A), where ∆(A) is the set of distributions, while an adversary selects a vector ct ∈ R|A|.
The reward of the agent at this round ishwt, Cti Where h∙,∙ denotes the operator of inner product.
The goal of the agent is to minimize the regret: RoTlo = maxw∈∆(A) PtT=1 hw, cti - PtT=1hwt, cti.
Let RoTl,o+ (a) = max(0,PtT=1ct(a) -PtT=1hwt,cti), in RM, wt+1(a) = Rto,l+o(a)/Pa0Rto,l+o(a0),
if maxao Rf,+ (a0) > 0, and wt+ι(a)=看 otherwise. According to the result in (Blackwell et al.,
1956), RM has the following regret bound:1
RTolo ≤ O
T
|A|	max ct2 (a)
t=1 a
(1)
Let st(a) = exp(-ηt Ptt0=1 ct0 (a)), AdaHedge picks wt(a) = st (a)/(Pa0 st (a0)), where ηt is the
learning rate that can be tuned adaptively (De Rooij et al., 2014). According to (De Rooij et al.,
2014), Adahedge has the regret bound RTolo ≤ O
log |A| PtT=1 maxa∈A ct2 (a) .
2.3	Counterfactual regret minimization (CFR)
CFR is developed on a connection between -NE and regret minimization. This connection is
naturally established by considering repeatedly playing a TEGI as an online learning problem. It is
worthy to note that there are two online learning problems in a TEGI, one for each player.
Suppose player i takes σti at time step t and let σt = (σt1, σt1 2). Consider the online learning
problem for player i by setting wt := σti and fti(σi) := ui((σi, σt-i)). The regret for player i is
RiT := maxσi RiT (σi), where RiT (σi) := PtT=1 ui((σi, σt-i)) - PtT=1 ui((σti, σt-i)). Furthermore,
i	Pt πσi (I)σti (I)
consider the time-averaged strategy σT (I) := P σ∏ ⑴-.Then, it is well-known that:
Lemma 1 ((Nisan et al., 2007)). If ∀i, TRT ≤ e/2, then (σT, σTr) is an E-NE.
Specifically, let σ∣ι→σθ(i) denote the strategy generated by modifying σ(I) to σ0(I) and ui(σ, I)
denote the reward of player i conditioned on arriving at the infoset I if the strategy σ is executed.
(Zinkevich et al., 2008) decomposes the regret RiT into the summation of immediate regrets as 2:
RT (σ) = X	X	∏σ (I Kti(I )(ui(σt∣ι→σ(i ),I )-ui(σt,I))	⑵
t I∈Ii,P(I)=i
1In this work, we use second-order bounds of RM and AdaHedge. These bounds can be easily derived from
known results, and we put the derivation of them in Appendix B
2(Zinkevich et al., 2008) directly upper bounded RiT by the counterfactual regret, i.e., Eq. (3), and omitted
the derivation of Eq. (2). So we present the derivation of Eq. (2) in Appendix C.
3
Published as a conference paper at ICLR 2020
Further, (Zinkevich et al., 2008) upper bounds Eq. (2) by the counterfactual regret:
RT(σ) ≤	X	(Xmax(0,∏-ti(I)(ui(σt∣ι→σ(i),I) - Ui(σt,I))) j	⑶
I∈Ii,P (I)=i	t
For convenience, We call ∏-i(I)ui(σt∣ι→a, I) the Counterfactual reward of action a at round t.
Notice that Eq. (3) essentially decomposes the regret of a TEGI into O(|I|) OLOs. So that, in each
round, We can apply RM directly to each individual OLO to minimize the counterfactual regret. And
the original regret maxσ RiT (σ) is also minimized since the counterfactual regret is an upper bound.
3	Lazy-CFR: a Lazy Update Algorithm for TEGIs
The above CFR procedure that applies RM to solve each OLO has to traverse the Whole game tree,
Which is very time-consuming in large scale games. In this section, We present Lazy-CFR, an efficient
CFR algorithm With a lazy update strategy based on the insight that updating the strategy on every
infoset is not indispensable. Intuitively, this is because the regret is determined by the norm of the
vector of counterfactual reWard on each node (see Eq. (1)); and on most nodes, the corresponding
norm is very small, since πσ-ti is a probability (see Eq. (3)), thereby can be updated in a lazy manner.
3.1 Lazy update for OLOs
The original OLO
We start by presenting a lazy update strategy for
an OLO in Defn. 2. As illustrated in Fig. 1, a
lazy update algorithm for OLOs consists of tWo
steps: (1) It divides the set of time steps [T] into n
intervals, that is, {ti,ti + 1, •一,ti,+ ι - 1}n=ι where
1 = tι < t2 …< tn+1 = T + 1. (2) It updates
wt at time steps t = ti for some i and keeps wt the
same within each segment. That is, the OLO with
T steps collapses into a new OLO with n steps and
accordingly the vector selected by the adversary in
the collapsed OLO at time step j is c0j = Pttj=+t1-1 ct,
in the original OLO at time step t.
IiL Iv	I1 卜，= t： 一 C
1 2 ↑ 3 4	5 6 I > τime
C,1 = C1-∖-C2 C,2 = c3 + C4 + C5 + C6
The collapsed OLO with tɪ = 1,立=3, 23 = 7
Figure 1: An illustration on RM with lazy UP-
date for OLOs. On the top is the standard
RM; on the bottom is the RM with lazy update.
Their lengths of time are 6 and 2 respectively.
where ct is the vector selected by the adversary
According to the known result in Eq. (1),
the regret of the RM with lazy update (Lazy-
RM) is bounded by O(PA Pn=ι iaxa ci2(a))∙
Thus, if the segmentation rule is under a rea-
sonable design, that is, Pin=1 maxa c0i (a)2 ≈
PjT=1 maxa cj (a)2, then the regrets of Lazy-
RM and the vanilla RM are similar in amount.
As we shall show soon, we can expect a reason-
able segmentation rule in CFR.
Though Lazy-RM does not need to update the
strategy at each round, a straightforward imple-
mentation of Lazy-RM still has a running time
of O(AT), which is the same as applying RM
directly. This is because we have to compute
Pti=+t1i+1 ct. Fortunately, this problem can be
addressed in TEGIs by exploiting the structure
of the game tree (see Sec. 3.2).
3.2 Lazy-CFR
Algorithm 1 Lazy-CFR
Input: a constant B > 0.
while t < T do
for all i ∈ {1, 2} do
Q = {Ir } where Ir is the root of the infos-
ets tree.
while Q is not empty. do
Pop I from Q.
Update the strategy on I via RM.
For I0 ∈ γ(I), if mt (I0) ≥ B, push I0
into Q, i.e., Q = Q ∪ {I0}.
end while
end for
for all h ∈ H such that the strategy on some
history after h has been modified. do
Update the reward vector on h.
end for
end while
Output the time-averaged strategy.
We now use lazy update to solve TEGIs. According to Eq. (3), the regret minimization procedure can
be divided into O(|I|) OLOs, one for each infoset. Specifically, for each infoset I ∈ Ii, P(I) = i,
we divide the set of time steps [T] into n(I) segments {tj (I), ∙∙∙ , tj+1(I) - 1}jn=(I1), following step
4
Published as a conference paper at ICLR 2020
Time step t+1
which a are modified
Figure 2: An illustration on how to update α. In time step t, the strategy on h1, h2, h5 is modified
with P (h1), P(h2), P(h5) 6= i. β can be updated in a similar way.
α(∙) += (ɪi + l)Γ(h2, a2)
α(∕⅛) += ((ɪi + l)Γ(h1,a1)+x2)Γ(h2,a1)
α(⅛)+=(((⅜ + l)Γ(h1, a1) + x2)Γ(h2, a1) + x3) Γ(∕ι5, α1)
(1). Let rj(I, a) = Pjt1(I)-1 ∏-i(I)ui(σt∖ι→a, I) denote the summation of the CoUnterfactUal
rewards over segment j, and let rj(I) = [rj(I, a)]a∈A(I) denote the vector consisting of rj(I, a).
Similar to lazy Update for OLOs, we only Update the strategy on infoset I at tj(I). Let σj0 (I) denote
the strategy after the j-th Update on infoset I, that is, σt(I) := σj0 (I) for t ∈ [tj (I), tj+1(I) - 1].
According to Eq. (1), we can boUnd the regret of the collapsed OLO on infoset I as RlTazy(I) :=
maxσ∈∑(∣)Pn=I) hσ—σj (I ),rj (I )i ≤ qA Pn=I) max。啜/⑷.
The above procedUre is qUite straightforward. However, as discUssed above, in order to have an
efficient implementation, one critical step is to define a proper segmentation rUle of each OLO. Below,
we present one rUle for Lazy-CFR, with which we can: 1) achieve a regret similar in amoUnt to the
regret of the vanilla CFR; 2) avoid Updating the whole game tree; and 3) compUte rj (I) efficiently.
Specifically, let τt (I) denote the last time step we Update the strategy on infoset I before time t.
We have τt(h) = τt(I) for h ∈ I. Let mt (I) := Ptτ =τ (I)+1 πσ-τi(I) denote the sUmmation of
reach probabilities after τt(I), which is contribUted by all players except i. Let subt(I) denote the
sUbtree rooted at infoset I. For convenience, for I ∈ Ii if P (I) = i, we call I a decision point.
Let γ(I) denote the set of decision points after I sUch that every I0 in the path from I to I00 ∈ γ(I)
is not a decision point. Formally, γ(I) is a sUbset of subt(I) sUch that ∀I0 ∈ γ(I), P(I0) = i and
∀I00 ∈ subt(I), if I00 is an ancestor of I0 ∈ γ(I), then P(I00) 6= i or I00 = I. For convenience, we
sUppose P (Ir) = i where Ir is the infoset of the empty history. Then, oUr segmentation rUle is
defined as a procedUre that Updates the strategies on infosets recUrsively as follows: 1) We Update
the strategy on Ir in every roUnd; 2) For infoset I, if we Update the strategy on I at time step t, the
time steps from τt(I) + 1 to t forms a segment in the Lazy-RM of I. That is, we compUte rj(I) for
the corresponding j and then apply RM to I; 3) after Updating the strategy on infoset I, we keep on
Updating the strategies on the infosets from γ(I) with mt ≥ B where B > 0 is a constant.
Alg. 1 presents an oUtline of Lazy-CFR. We’ll formally analyze its performance in Sec. 4 and now we
briefly discUss why Alg. 1 converges faster than CFR. Let’s tentatively assUme that we can compUte
mt and rj efficiently (See Sec. 3.2.1 for details), then the convergence rate of Lazy-CFR depends on
the total nUmber of Updates on strategy and its regret. As for the nUmber of Updates on strategy, it
is obviously upper bounded by B PT=I PI ∏-i(I) which is much smaller than T∖I∖ as ∏-i(I) is
probability. As for the regret, it is easy to see that in Alg. 1, mt (I) ≤ d(I)B where d(I) is the depth
of I in the tree. With krj (I)k2 ≤ maxt mt(I) and Eq. (1), we can upper bound the regret of the
Lazy-RM on I by O(Bd(I) pAn(I)) ≤ O(Bd(I)√AT). Therefore, the overall regret, i.e., Eq. (2),
of Lazy-CFR is O(∖I∖DB√AT). This regret bound is the same to the bound of CFR in (Zinkevich
et al., 2008) within a gap ofDB which is usually a logarithm of ∖I∖. In Sec. 4, we further refine this
bound and show it is comparable with the best known regret bound of CFR in (Burch, 2018).
5
Published as a conference paper at ICLR 2020
3.2.1	Implementation
As mentioned in Sec. 3.1, if we compute mt(I) and rj(I) directly, we still have to spend O(AT)
time on each infoset. In this section, we show how to efficiently implement Alg. 1 by exploiting the
structure of the game tree. More specifically, we define data structures (DS) used in the computations
of mt , rj and then show when and how to update them. Updating these data structures can be
implemented by depth first search (DFS) and we only spend O(1) time to update the DSs on each
history visited by DFS. The detailed pseudo-code is in Appx E.
W.L.O.G., We consider the DSs used in Lazy-CFR for player i. First, We store ui(h∣σt) in DS
U(h) and the strategy profile in Γ(I), i.e., U(h) = Ui(h∣σj Γi(I) = σ∖(I) at time step t.
And hoW to update U and Σ is standard as other CFR algorithms. Recall that our target is to
compute mt (I) = Ph∈I mt(h) = Ph∈IPtt0=τt(I)+1πσ-ti0(h) and rj(I,a) = Phrj(h,a) =
Ph∈I Ptt0=Tt(I)+i ∏-i (h)ui((h, a)∣σto) where (h, a) denotes the successor of h after action a. It is
noteWorthy that We do not need to compute mt and rj for every history in every round. Instead, We
just need to make sure mt(h) and rj(h, a) can be computed efficiently When Alg. 1 visits h.
Before diving into details, We intuitively explain hoW to efficiently compute rj (h) and mt(h).
For mt (h), suppose We have stored Ptτ=t(τpa((hh))) πt-i (h) in some data structure α(h), then We can
compute mt(h) as mt(h) = Pho%∈subt(h0) α(h0)Γ-i(h0, h) where Γ(h0, h) denotes the probability
of reaching h from h0 contributed by Γ-i. As for rj (h), suppose the strategy on histories in subt(h)
is never modified during [τt(h), t], then we can compute rj (h) = mt (h)U (h) in time O(1), as U(h)
is also fixed in [τt(h), t]. In Alg. 1, it is possible that the opponent’s strategy on some h00 ∈ subt(h)
is modified during [τt (h), t], thus, we need more data structures to maintain the cumulative reach
probability and cumulative counterfactual reward.
More specifically, we use two additional DSs α(h) and β(h, a). In α(h), we store summations of
reach probabilities and we store a summation of counterfactural rewards in β(h, a). These DSs
should satisfy the following properties:
Property 1. In round t, α, β, α should satisfy:
L Mh) = pt1=ht(h)+ι π-ti(h)，β(h,a) = Pt1=ht(h)+ι π-ti0 (h)ui((h, a)lσt0) αnd α(h) =
Ptt02=(ht) (h)+1 πσ-i0 (h) for τt(h) ≤ t1(h) ≤ t2(h) ≤ t. We will introduce t1, t2 soon.
2.	1) If h0 = pa(h), then t1(h0) = t2(h); 2) If h is the root, t2(h) = t. So that [τt(h), t] =
Uh"h∈subt(h,)[tι(h0) + 1,t2 (h0)].
3.	For all h0 ∈ subt(h), P(h0) 6= i, the strategy on h0 is never modified on time steps between
t1(h) andt2(h). So that for t0 ∈ [t1(h), t2(h)] and h0 ∈ subt(h), πσ-i0 (h, h0) = πΓ-i(h, h0).
Intuitively, t1 (h) is the last time step that U(h) is modified and t2 (h) is the last time step that the
strategy on pa(h) is modified. Before discussing how to update these DSs, we discuss how to use
them to compute mt and rj . It is easy to check that Eq. (4) is true with Prop. 1, we have:
mt (h) = X α(h0)πΓ-i (h0, h), rj (h, a) = β(h, a) + (mt(h) - α(h))U ((h, a))	(4)
h0 ∖h∈subt(h0)
where πΓ-i(h0, h) denotes the probability from h0 to h contributed by -i with strategy profile Γ. We
postpone the derivation of Eq. (4) to Appx. E. With the fact that mt(h) = mt(pa(h))πΓ-i(pa(h), h) +
+α(h) where pa(h) is the parent of h, computing mt only spends O(1) time on each node as when
visiting h, mt(pa(h)) must have been computed in DFS.
The last challenge is how to update these DSs to satisfy the desired properties in Prop 1. Specifically,
let S1,t denote the set of histories such that if h ∈ S, the strategy on subt(h) is modified at round
t and S2,t = {h : h ∈/ S1,t, pa(h) ∈ S1,t}. As illustrated in Fig. 2, we only update DSs on
St = S1,t ∪ S2,t as:
Update on the DSs: 1) If Γ on the infoset of h is modified, set α(h) = β(h, a) = 0; else 2) for
h ∈ St, and set α(h) = mt(h), β(h, a) = mt (h)U ((h, a)).
6
Published as a conference paper at ICLR 2020
With above update rule, it is easy to check that if Prop 1 is true at t, then it is still true at t + 1.
Lazy-CFR+ and Lazy-LCFR: We can also apply lazy update to CFR+ (Bowling et al., 2017) and
LCFR (Brown & Sandholm, 2019a), which are improvements of CFR. To get Lazy-CFR+ and
Lazy-LCFR, we only need to replace RM by the corresponding OLO solvers, and use their methods
of computing time-averaged strategy as in (Bowling et al., 2017) and (Brown & Sandholm, 2019a)
respectively.
4	Theoretical analysis
We now present the theoretical results, starting with the regret for members of CFR with lazy update.
4.1	Regret upper bound
We extend the regret analysis on the vanilla CFR in (Burch, 2018) to the members of CFR with
lazy update. SPeCifiCally, let ξi(σ) = PD=I JPI：d(i)=d ∏σ(I), ξ
maxi,σ ξ* i (σ) and η(σ)
ξi (σ) JA maxIj (PM(I)+1 πσ-ti(I)) whiCh are parameters depending on the struCture of the game
and segmentation rule. Thm. 1 provides a regret bound for a CFR algorithm with an arbitrary
segmentation rule. By applying Thm. 1, we Can get the regret bound of CFR and Lazy-CFR whiCh
are Comparable with Corollary 2 in (BurCh, 2018).
Theorem 1. The regret ofCFR with lazy update can be bounded as RT(σ) ≤ O(√Tη(σ)).
Lemma 2. With RM, the regret of the vanilla CFR is bounded by O(ξ√AT) and the regret of
Lazy-CFR is bounded by O(ξ√DAT).
Proof. It is easy to see that for the vanilla CFR, We have η(σ) ≤ ξi(σ)√A and for Lazy-CFR, We
have η(σ) ≤ ξi(σ)√AD. With Thm. 1, we get the regret bounds.	□
4.2 Time and space complexity
With the implementation in SeC. 3.2.1 and Appx E, the running time of Lazy-CFR is bounded by
O(Pt |St|). Obviously, Pt |St| = O(maxσ Pt)πσ-i(h). Thus, We Can bound Lazy-CFR’s time
Complexity as:
Lemma 3 (Time Complexity). The time complexity of Alg. 1 is O(T maxσ Ph πσ-i (h)).
To shoW hoW small maxσ PI πσ-i (I) is, We make the folloWing mild assumption WhiCh leads to
Corollary 1.
Assumption 1. 1, IfP (h) = i, then P ((h, a)) 6= i; 2, The tree of infosets for each player is a full
A-ary tree; 3, Every infoset in the tree of infosets is corresponding to n nodes in the history tree.
Corollary 1. Ifa TEGI satisfies Assumption 1, then ∀σ, Ph三τi π-i(h) = O(n,|Ii|).
According to Lemmas 2 3 and 1, the regret of Lazy-CFR is about O(√D) times larger than that of
CFR, whilst the running time is about O(∖∕[I∖) times faster than CFR per round under Assumption
1. Thus, according to Lemmas 1, 2 and with a bit algebra calculation, we know that Alg. 1 is
about O( a∕∖I∖∕D) times faster than the vanilla CFR to achieve the same approximation error. The
improvement is significant in large scale TEGIs.
Space complexity: A potential limitation of Lazy-CFR is that its space complexity is O(|H|) as we
have to store the data structures α, β, α on histories. In contrast, the space complexity is O(|I|) for
some popular implementation of CFR. In heads-up flop hold’em poker (FHP) (Brown et al., 2019),
we have |H| ≈ 1012 and |I| ≈ 109. Accordingly, Lazy-CFR needs about 10 TB to store the data
structures while CFR needs about 10 GB, which is still affordable in common storage systems. Note
that we do not optimize the space complexity in this work. It is worth of a systematical investigation
to derive an algorithm as fast as Lazy-CFR without additional memories, e.g., by designing a better
segmentation rule as well as a better implementation.
7
Published as a conference paper at ICLR 2020
(a) Leduc-6
(b) Leduc-8
(c) FHP
Figure 3: Convergence for Lazy-(CFR, CFR+, LCFR), MC-(CFR, CFR+, LCFR), CFR, CFR+,
LCFR and MC-LCFR-P.
4.3	Regret lower bound
In the analysis of lower bound, we consider the standard adversarial setting in online learning, in
which an adversary selects σt-i and a reward function uit : Z → [-1, 1] where Z is the set of terminal
nodes in the infoset tree of player i. Thus, to get a lower bound, we need to explicitly construct σt-i
and uit (I), I ∈ Z. This setting is consistent with the analysis of the regret upper bound, in which we
do not make any assumption on how uit , σt-i changes over rounds. Intuitively, by extending the lower
bound analysis of OLO (Cesa-Bianchi & Lugosi, 2006) to infoset tree structured problem, we bound
Eq. 2 from below as in Thm. 2 and for the proof, see Appendix D.
Theorem 2. For an algorithm A generating σti given the history in the past, let RiT,A denote the
Ri
regret of A In the first T rounds, we have lιmA→∞ IimT →∞ mmA maxπ-i Ut『,『/m =T ≥ >
By comparing the regret lower bound in Theorem 2 and the regret upper bounds of CFR and
Lazy-CFR as in Lem 2, we can see that the regret of CFR and Lazy-CFR are both near-optimal.
5	Related work
Monte-Carlo and pruning-based methods: There are several variants of CFR which attempt to
avoid traversing the whole game tree at each round. Monte-Carlo based CFR (MC-CFR) (Lanctot
et al., 2009; Burch N, 2012) uses Monte-Carlo sampling to avoid updating the strategy on infosets
with small probability of arriving at. Pruning-based variants (Brown & Sandholm, 2017a; 2015) skip
the branches of the game tree if they do not affect the regret, but their performance can deteriorate to
the vanilla CFR in the worst case. And dynamic thresholding (Brown et al., 2017) directly prunes the
branches with small reach probabilities. In this work, we do not compare with pruning-based method
(Brown & Sandholm, 2019b) since the pruning technique is orthogonal to lazy update.
Existing analyses of regret: Lanctot et al. (2009); Burch N (2012); Burch (2018) refined the regret
upper bound of CFR. Our analysis is essentially an extension of the regret analysis on the vanilla
CFR in (Burch, 2018) to other variants of CFR with lazy-update.
6	Experiment
In this section, we empirically evaluate our algorithm against existing CFR variants. We measure the
exploitability of these algorithms. The exploitability of a strategy (σ1, σ2) can be interpreted as the
approximation error to the Nash equilibrium. The exploitability is defined as maxσ0,1 u1((σ0,1, σ2)) +
maxσ0,2 u2((σ1 , σ0,2)).
Experiments are conducted on variants of two common benchmarks in imperfect-information game
solving: (1) the Leduc hold’em (Southey et al., 2005) which is a simplifed version of the heads-up
no-limit hold’em poker with 6 cards; and (2) heads-up flop hold’em poker (Brown et al., 2019) (FHP)
which is similar to heads-up no-limit Texas hold’em poker without the last two rounds of betting.
In our experiments of Leduc Hold’em poker, we restrict players not to bid more than 6 or 8 times the
big blind. The number of infosets in the generated game trees are about 40000 and 455000 for each
player, respectively. We run each algorithm on each Leduc Hold’em game for 30000 seconds. In the
experiments of FHP, to control the size of the game, we consider a simplified game such that in the
8
Published as a conference paper at ICLR 2020
deck, there are 2 suits and 5 cards in each suit. And we restrict players not to bid more than 7 times
the big blind. The size of the game is about 109. We measure algorithms by the number of nodes
touched which is independent with implementation and hard-ware.
We empirically compare Lazy-(CFR,CFR+,LCFR) with existing methods, including the vanilla CFR,
CFR+, LCFR, MC-(CFR,CFR+,LCFR) and MC-LCFR with negative regret-pruning (MC-LCFR-P)
which was used in developing Pluribus (Brown & Sandholm, 2019b). In our experiments, we use
RM (RM+) as the OLO solver. In Lazy-(CFR,CFR+,LCFR), we set B ∈ {0.1, 1.0}. We evaluate the
CFR, CFR+, LCFR variants which prunes the histories with πσ-i (h) = 0 in the recursive tree walk as
they don’t affect the regret. For MC-(CFR,CFR+,LCFR), we use the external-sampling scheme. In
the experiments on MC-LCFR-P, we use the following parameters: we run MC-LCFR in the first 20
minutes; after that, in each iteration, we run MC-LCFR with probability 0.05 and run MC-LCFR-P
with probability 0.95; in MC-LCFR-P, we prune those branches with average regret less than -2
times the big blind.
Fig. 3 presents the results. We can see that the performance of Lazy-CFR(+) has a similar performance
to CFR(+) on the variants of Leduc Hold’em. This is because in the experiments of CFR(+) on Leduc
Hold’em, there are a large portion of histories with πt-i(h) = 0 on average. And on the larger game,
Lazy-CFR(+) significantly outperforms CFR(+). The performance of Lazy-LCFR is much worse
than LCFR on all games. This might be because our segmentation rule is designed for OLO with a
uniform weight in each iteration. And LCFR assigns more weights on later iterations.
7	Conclusions
In this work, we propose a new framework to develop efficient variants of CFR with an analysis
shows that our algorithm is provably faster than the vanilla CFR. The final algorithm runs fast in
practice, but with some extra cost on space complexity. It is worth of a systematical study to reduce
the space complexity.
Acknowledgement
This work was supported by the National Key Research and Development Program of China (No.
2017YFA0700904), NSFC Projects (Nos. 61620106010, U19B2034, U1811461), Beijing NSF
Project (No. L172037), Beijing Academy of Artificial Intelligence (BAAI), Tsinghua-Huawei Joint
Research Program, a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for Intelligent
Computing, the JP Morgan Faculty Research Program and the NVIDIA NVAIL Program with
GPU/DGX Acceleration.
References
Jacob Abernethy, Peter L Bartlett, and Elad Hazan. Blackwell approachability and no-regret learning
are equivalent. In Proceedings of the 24th Annual Conference on Learning Theory, pp. 27-46,
2011.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino:
The adversarial multi-armed bandit problem. In focs, pp. 322. IEEE, 1995.
David Blackwell et al. An analog of the minimax theorem for vector payoffs. Pacific Journal of
Mathematics, 6(1):1-8, 1956.
Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em
poker is solved. Communications of the ACM, 60(11):81-88, 2017.
Noam Brown and Tuomas Sandholm. Regret-based pruning in extensive-form games. In Advances
in Neural Information Processing Systems, pp. 1972-1980, 2015.
Noam Brown and Tuomas Sandholm. Reduced space and faster convergence in imperfect-information
games via regret-based pruning. In Workshops at the Thirty-First AAAI Conference on Artificial
Intelligence, 2017a.
9
Published as a conference paper at ICLR 2020
Noam Brown and Tuomas Sandholm. Safe and nested subgame solving for imperfect-information
games. In Advances in Neural Information Processing Systems, pp. 689-699, 2017b.
Noam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted regret
minimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp.
1829-1836, 2019a.
Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885-890, 2019b.
Noam Brown, Christian Kroer, and Tuomas Sandholm. Dynamic thresholding and pruning for regret
minimization. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret min-
imization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learn-
ing Research, pp. 793-802, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL
http://proceedings.mlr.press/v97/brown19b.html.
Neil Burch. Time and space: Why imperfect information games are hard. 2018.
Szafron D Burch N, Lanctot M. Efficient monte carlo counterfactual regret minimization in games
with many player actions. In Advances in Neural Information Processing Systems. 2012, 2012.
Nicolo Cesa-Bianchi and Ggbor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online
optimization. In Advances in Neural Information Processing Systems, pp. 345-352, 2008.
Steven De Rooij, Tim Van Erven, Peter D Grunwald, and Wouter M Koolen. Follow the leader if you
can, hedge if you must. The Journal of Machine Learning Research, 15(1):1281-1316, 2014.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.
Daphne Koller and Nimrod Megiddo. The complexity of two-person zero-sum games in extensive
form. Games and economic behavior, 4(4):528-552, 1992.
Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte carlo sampling for
regret minimization in extensive games. In Advances in neural information processing systems, pp.
1078-1086, 2009.
Matej Moravcik, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.
Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V Vazirani. Algorithmic game theory.
Cambridge University Press, 2007.
Martin J Osborne and Ariel Rubinstein. A course in game theory. MIT press, 1994.
Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings,
and Chris Rayner. Bayes’ bluff: opponent modelling in poker. In Proceedings of the Twenty-First
Conference on Uncertainty in Artificial Intelligence, pp. 550-558. AUAI Press, 2005.
Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. In Advances in neural information processing systems, pp.
1729-1736, 2008.
10
Published as a conference paper at ICLR 2020
A The table of notations
We provide a table of some important notations in our paper in this section.
Notation	Explanation
[n] [a, b] H I D d(I) and d(h) c h Z⊆H (h, a) (I, a) Ii , I P σi σ σ-i σ(h), σ(I) πσ(h), πσ(I) πσi (h), πσi (I) πσ-i(h), πσ-i(I) π-i (h0, h) ui(h), h ∈ Z ui (σ) ui(h∣σ) σ上 →σ0(I) pa(h), pa(I) subt(h), subt(I) γ(I),I∈Ii	the set {1, 2,…,n} the set {a, a +1,…，b} The tree of histories Infomation set the depth of the game tree the depths of I and h the chance player history the set of leaves of the game tree the successor of h after action a the successor of I after action a Ii is the infoset tree of player i and I = I1 ∪ I2 The player function, i.e., P(h) and P(I) are the players who take actions on h and I, respectively. the strategy of player i; the strategy of all players; the strategy of all players except player i σ(h) = σP (h)(h) and σ(I) = σP(I)(I) the reach probability of h and I contributed	by	all players the reach probability of h and I contributed	by	player i the reach probability of h and I contributed	by	all players except	i the reach probability from h0 to h contributed by all players	except i if h ∈ subt(h0) the reward received by player i at leaf h the reward received by player i with strategy profile σ the reward received by player i at h with strategy profile σ the strategy profile generated from σ by replacing σ(I) by σ0(I) the parents of h and I respectively the subtree rooted at h and I respectively a subset of subt(I) such that for all I0 ∈ γ(I) then P (I0) = i; consider I00 ∈ subt(I), if I00 is an ancestor of I0, then P(I00) 6= i
ha, bi wt ct RTolo ∆(A) RTi (σi) RTi σT τt (I), τt (h) mt(h) mt(I) rj(h, a) rj(I,a) U(h) Σ α, β, α	the inner product of vectors a and b the action selected by the player at time step t the reward vector at time step t the regret of the OLO, i.e., RoTlo = maxw∈∆(A) PtT=1hw, cti - PtT=1hwt,cti the set of distributions over set A RiT (σi) = PtT=1ui(σi,σt-i) - PtT=1ui((σti,σt-i)) the regret of player i, i.e., RiT = maxσi RiT (σi) the time-averaged strategy with σT(I) = PPTnbn(I)(1)(I) the last time step we update the strategy on infoset It (history h) before time t the sum of reach probabilities, mt(h) = Ptt0=τ (h)+1 πσ-i0 (h) mt(I) = Ph∈I mt(h) the sum of counterfactural rewards over the j -th segment of h rj(I,a) = Ph∈Irj(h,a) data structure with U(h) = Ui(h∣σt) at time step t data structure with Σ(I) = σt (I) at time step t data structures such that, at time step t, mt(h) = α(h) + £九0%『乜皿九)α(h)∏Γ2(h) and r (h, a)= βj (h, a) + U ((h, a)) Ph∈subg a(h)π-i(h),
ξi(σ) ξ η(σ)	a parameter depends on the structure of the game tree with ξi(σ) = PI∈Ii,P(I)=i πσi (I) ξ = maxi,σ ξi(σ) a parameter depends on the segmentation rule with η(σ) = Aξi (σ) maxI,j (Ptj=+t1j(I)+1 πσ-ti(I))
11
Published as a conference paper at ICLR 2020
B Derivation of regret bounds of RM and AdaHedge
The regret upper bound of RM and AdaHedge used in this work can be easily derived from previous
works. We include their derivations only for completeness.
Derivation of Eq. (1). The standard analysis of RM in Cor 2.1 on pg. 13 in (Cesa-Bianchi & Lugosi,
2006) shows the regret of RM is bounded by:
(
∖
RTolo ≤ O
T |A|	|A|
∑∑l∑wt(a0)ct(a0) - ct(a)
t=1 a=1	a0=1
Obviously, we have | P|aA0=| 1 wt(a0)ct(a0)| ≤ maxa |ct(a)| since wt is a vector of probability. We
have:
1A| / 1A1	∖ 2
wt(a0)ct(a0) - ct(a)	≤	4 max ct2 (a) = 4|A| max ct2 (a).
a=1 a0=1	a
We finished the proof.	□
Derivation of the regret bound of AdaHedge. Let vt denote the variance of reward if we take a ran-
dom action according to the distribution wt defined in AdaHedge. Theorem 6 in (De Rooij et al.,
2014) provides a second-order bound as:
RoTlo ≤
K jl°gAIX
Vt + 4 log IAI + 2
where κ is a constant. It is known that for a random variable x ∈ [a, b], its variance is no more than
(a - b)2/4. Thus, Vt ≤ max。c2(a). We finished the proof.	□
12
Published as a conference paper at ICLR 2020
C Proof of Theorem 1
We now extend the proof of the Theorem 1 in (Burch, 2018) to CFR with lazy update. We first intro-
duce some additional terminology. Consider B ⊆ Ii, let ξ(B) = maxσ-i	I∈B h∈Iπσ--ii(h).
Now we prove Theorem 1.
Proof. Let κ denote the constant involved in Eq. (1). With Eq. (2) and the regret bound of RM, we
have
TRT(σ) = T X X	∏σ(IKti(I)(ui(σt∣ι→σ(i),I) - ui(σt,I))
t I∈Ii,P (I)=i
/	F(I)	∖
≤ κ X	πσi (I) XAmaax(rj(I,a))2/T2
I∈Ii,P (I)=i	j=1
And then apply Jensen’s inequality and with some calculations, we have
1
T RT (σ)
n(I)
≤ κ X πσi (I)tAXmax rj2 (I, a)/T 2
I∈Ii	j=1 a
D
κX
d=1
D
κX
d=1
D
κX
d=1
D
κX
d=1
PI0 ,t,d(I)=d ∏σ (I0)∏-ti(Io)
PI 0,t,d(I 0)=d ∏σ (I 0)∏-ti(I 0)
πσi (I 0)πσ-ti(I 0)
I0 ,t,d(I0)=d
πσi (I 0)πσ-ti(I 0)
I， ,t,d(I，)=d
πσi (I 0)πσ-ti(I 0)
I， ,t,d(I，)=d
n(I)
X ∣∏σ(I)t aXmax rj2 (I, a)/T 2
I:d(I)=d	j=1 a
Σ
I:d(I)=d
πσi (I)
Pi，,t,d(i，)=d ∏σ (I 0)∏-ti(I0)
n(I)
A X max rj2(I, a)/T 2
j=1 a
X	∏σ(I) Pt ∏-i(I)
I:d(I)=d 5,t,d(g ∏σ (I0)∏-ti(I0)
X	Pt ∏σ (I )∏-ti(I)
I:d(I)=d ∖Pιo,t,d(i o)=d ∏σ (I 0)∏-ti(I0)
A Pn=I maxa Tj(I, a) ʌ
t	(Pt∏-ti(I))2 T2 )
A Pn=I maxa rj(I, a) ʌ
t	(Pt∏-ti(I))2 T2 )
≤KX( X	∏σ(I0)∏-ti(I0)) It
d=1 I， ,t,d(I ， )=d
X p	Pt ∏σ (I)∏-i(I)	! ∙ p Pn=I) maχa r2(I，a)! ∖
I:d(I)=d PI0,t,d(I0)=d∏σ(I0)∏-ti(I0)) ∙ ∖ (Pt ∏-i(I))2 T2	〃
The Jensen’s inequality applies here, in which we move the term of probability outside the square root into it.
XdA ( X ∏σ(I0)∏-ti(I0) X 卜(I户PmaxaIrTI,α)
d=1	I，,t,d(I，)=d	I:d(I)=d	t πσt (I)T
D
≤κX
d=1
(“ X ∏σ(I) PnirX-r2II,1
I:d(I)=d	t=1 πσt (I)
The last inequality utilizes the fact that PI，,t,d(I，)=d πσi (I0)πσ-ti (I0) ≤ T as πσ-ti (I)πσi (I) is the
probability of arriving at infoset I under strategy (σi, σt-i) and there is at most 1 infoset been arrived
at each level.
13
Published as a conference paper at ICLR 2020
It is easy to see that maxa rj2(I, a) ≤ (Pttj=+t1((II))+1 πσ-i(I))2. With straight-forward computations,
we finish the proof:
A
I:d(I)=d
πσii(I)
Pn=I) (Pt=j(I3 ∏σt(I ))2
Ptπσ-ti(I)
tj+1
≤ A X πσi i (I) mjax( X
I:d(I)=d	t=tj (I)+1
π-i(I)), this is because 石 Xi ≤ maxXi if Xi ≥ 0,
σt	i xi	i
xi > 0
i
≤A
πσi (I)	mI,ajx
tj+1
X
t=tj(I)+1
πσ-ti(I)
□
D The regret lower b ound analysis
Recall that the analysis on the regret upper bound is under the standard adversarial setting in online
learning, that is, it does not depend on how the opponent’s strategy and the utility vary over time
steps. So we here make the same adversarial assumption that there is an adversary choose both σt-i
and uit . It is worthy to note that in our construction of the adversary, the utility function may also
vary over time.
For convenience, let -i denote all the players except i, and let ζ = {σbi : σbi =
arg maxσi PdD0=1PI∈Ii,P(I)=iπσii(I)}. Let D := {I ∈ Ii : ∃σi ∈ ζ,πσii(I) > 0}. It can be
shown that D forms a subtree of Ii . Intuitively, our construction on σt-i and uit can be divided into
two stages:
1.	For I ∈ Z, I ∈/ D, uit(I) = -1 for all t. This enforces player i take actions on D, otherwise,
it will always receive reward -1.
2.	In each round t, for I ∈ D,I ∈ Z, P (I) = i, We first generate a random variable a(I)〜
Multinomial(1, A(Iy ),3 and then set σ-i(I, a(I)) = 1, and σ-i(I, a) = 0 for a = a(I).
Intuitively, this step separates RiT into O((ξi)2) isolated OLOs, each of Which is of A
actions and Would be repeated for about O(T /(ξi)2) rounds, since only one of them Will be
triggered on in each time step according to our construction on σt-i. Thus, combined With the
lower bound proved by (Cesa-Bianchi & Lugosi, 2006), each OLO incurs a regret of about
Ω(√T log A∕ξi), and we can informally provide a lower bound of Ω((ξi)2∕ξi√T log A)=
Ω(ξi√T log A), which is formally described in Theorem 2.
Before proving Theorem 2, we first address some trivial cases and show some intuitions on the way
we construct the worst case of σt-i and uit .
Let 夕i(σ) = Pι∈ii ∏ir(I). It is easy to see that 夕i(σ) > (ξi(σ))2∕D. And we are going to
show that limA→∞ limτ→∞ minA max∏-i,u RTA/，夕iT log A ≥ 1. To start, we make an implicit
assumption that |A(I)| ≥ 2, ∀I ∈ Ii, P(I) = i. Otherwise we can merge these infosets as we have
no choice but choose the only action, which contributes nothing to the regret. In addition, we assume
i and -i take actions alternatively.
We now show that it is sufficient to focus on the subtree D (Note that D is a subtree rooted at Ir). By
setting ut(I) = -1 for I ∈ Z and I ∈∕ D, we know that player i will not take actions to go out of
D. And if player -i goes out of D, this round will contribute nothing to the regret as player i will
always receive reward -1.
Moreover, we assume P(pa(I)) = i, ∀I ∈ Z. Otherwise, we can merge the subtree rooted at
pa(I), I ∈ Z into one single leaf node (i.e. an infoset I ∈ Z after the mergence), and P(pa(I)) =
i, ∀I ∈ Z in the new merged infoset tree.
3This step is informal here. See later in this section for a formal construction.
14
Published as a conference paper at ICLR 2020
The following property will be useful in our proof.
Lemma 4. Let 夕 D (σi) = Pi∈d ∏q,σ i (I). Moreover, let Z 0 = {σi : σi(I, a) = 0, for P (I) = i, I ∈
D, (I, a) ∈ D}. Then ∀σDj, (σD)0 ∈ Z, we have that 夕D 仃ɪ=夕D / y.
Proof. To start, let φD(σσ,I) = πτ1(7) P1 o∈subt(1),P(r)=i ∏iσi (10),I ∈ D, where Subt(I) means
the subtree rooted at I (Note that it's possible that 10 ∈ subt(I), 10 ∈ D). Let 夕D(I)=
maxσi 夕 D (σi,I), and σi,* = arg maXσi 夕 D (σi, I). We first show that, ∀I ∈ D, P (I) = i,
夕D(Jσ(I,aj))=夕D(Jσ(I,ak)), j, k ∈ |Ad(I)| where AD(I) consists of a ∈ A(I) such that
Jσ(I, a) ∈ D.
With the definition of D, ∀I ∈ D, ∃σ0 ∈ Z, s.t. ∏σ i (I) > 0. If 夕 D (Ji(I,aj))=2 D (Ji(I,aQ)
W.L.O.G. we assume 夕 D (J i(I, aj)) < 夕 D (J i(I, ak)), then ∀σ0 such that n； i (I) > 0, we have
程(σ0,I)=1+ X σ0(I,aWD(σ0,Ji(I,a))
a∈A(I)
≤1+ X σ0(I,aWD(Ji(I,a))
a∈A(I)
≤1 + [	X	σ0 (I, a)P D ( J i(Ir, a))] + [σ0 (I, aj ) + σ0 (I, ak )]^i(JD (I, ak ))
a∈A(I ),a6=aj ,ak
With the last inequality, we can construct a (σ0)0 with 夕D((σ0)0, Ir) ≥ 夕D(σ0, Ir) and (σ0)0(10)=
σ0i (I0) only if I0 ∈ subt(I). Formally, we define (σ0i )0 as:
•	(σ0i)0(I0) =σ0i(I0),I0 ∈/ subt(I)
•	(σ0)0(10) = σi,*(10), 10 ∈ subt(I),10 = I
•	(σ0i)0(I, a) = σ0i (I, a), a 6= aj, ak
•	(σ0i )0(I, aj) = 0
•	(σ0i)0(I, ak) = σ0i(I,aj) + σ0i (I, ak)
As σ0 ∈ Z,夕D((σ0)0,Ir)=夕D(σ0,Ir), which means σ0(I, aj) = 0, due to n；i(I) > 0. Thus,
∀σ0i such that πσi i (I) > 0, we have πσi i (Ji(I, aj)) = 0, which means Ji (I, aj) ∈/ D (notice that
if n；i (I) = 0, n；i (Ji(I,aj)) = 0), that is contradict to our assumption Ji(I,aj) ∈ D. Thus
PD(Ji(I,aj))=2D(Ji(I,ak)),aj,ak ∈ AD(I).
Now we can show that 夕D(σi, I)=夕D(I), ∀σi, ∀I with mathematical induction.
Let D denotes the depth of D. As we assume P(pa(I)) = i, ∀I ∈ Z, P(Ir) = i if D is odd and
P(Ir) = -i if D is even. We separately discuss them as follows:
•	D = 1: Obviously 夕D(σi, Ir)=夕D(Ir) = 1.
•	D is even: If ∀a ∈ AD(Ir), Ji(Ir, a) satisfies that 夕D(σi, Ji(Ir,a))=夕D(Ji(Ir,a)),
then
曲(σi,Ir )= X ^D (σi ,J i(Ir ,a))
a∈AD(I)
= X 曲(J i(Ir ,a))
a∈AD (I )
=2D (Ir )
Thus we get ^D(σi,Ir)=心(Ir).
15
Published as a conference paper at ICLR 2020
• D is odd and D = 1: If ∀a ∈ AD(Ir), JIi(Ir,a) satisfies that 夕D(σi,JIi(Ir,a))=
夕D(Ji(Ir, a)), then ∀σi ∈ Z0,
品(σi,Ir)
=1+ X σi(Ir ,aWD (σi,J U ,a))
a∈AD (I)
=1+ X σi(Ir ,a)&(J i(Ir ,a))
a∈AD (I)
=1 + [ E	σi(Ir,a)]	∙夕D(Ji(Ir,aj)),∀aj	∈	AD(I)	(due to 夕D(Ir,aj)=夕D(Ir,aQ, ∀aj,ak	∈	AD(I))
a∈AD (I)
=1 + P D (J i(Ir ,aj))
=φD (Ir) (as ∀σi,夕 D (σi,Ir ) = 1 + 夕 D (J i(Ir ,a)), a ∈ AD (I) is independent of σi)
We can get 夕D(σi, Ir)=夕D(Ir) as well.
So ∀σi, ∀A, φD(σi,Ir)= 夕D(Ir). AS 夕D σi = 夕D(σi,I)= 夕D(I) is independent from σi, we
finally prove this lemma.	口
As 夕D σi is independent of the choice of σi, we will drop the subscript σi, that is, we will use 夕D
instead of 夕D σi in the following proof.
Now we prove Theorem 2.
Proof. As we discussed before, it’s sufficient to focus on the subtreee D, so all of the terms (e.g. the
regret) in the following proof are defined on D, not Ii .
For D, P (pa(I)) = i, ∀I ∈ Z, we use the following procedure to generate σt-i and uit for each
round:
•	ut(I)〜Bernoulli(0.5), ∀I ∈ Z.
•	∀I ∈ D, P (I) 6= i, we define pD (I) as:
pD(I) = [P
/ j∙
ψiD (Ji(I,aι))
夕 D (Ji(I,a∣AD (I)I))
∙a∈AD(I)夕D ( Ji (I，a))
,	a∈AD(I) 机(J i(I,a))
]
Notice that this term only depends on D , so once we determine D, we can immediately get
this pD(I). Each turn we first sample a(I) from Multinomial(1, pD (I)), then let σt-i(I) =
a(I).
In the following proof we denote this generating procedure as M and use the notation EM [∙] as the
expectation over this generating procedure.
Let nt(I), I ∈ D denotes the cumulative arriving time player i arrives at I in the first t rounds. With
a little abuse of notation, we use the term RiT (I), I ∈ D to represent the regret of the subtree rooted
at I in the first T turns, i.e.
T
RiT (I) = maix X ui((σi, σt-i), I) - ui((σti, σt-i), I)),	I∈D
σ t=1
We will prove that limA→∞ limτ→∞ EMRT aig/ ∖ WDT；og A ≥ 1, ∀ alg with mathematical induc-
tion, where RiT,alg is defined in Theorem 2. Notice that under the assumption that P (pa(I)) =
i, ∀I ∈ Z, P(Ir) = i if D is odd while P(Ir) = -i if D is even. In our proof we will discuss them
separately.
16
Published as a conference paper at ICLR 2020
• D = 1: As (Auer et al., 1995; Freund & Schapire, 1997; Cesa-Bianchi & Lugosi,
2006; Dani et al., 2008) have shown, for K-arm online linear optimization problem,
limK→∞ limT
→∞
中 D = 14.
EM RT,aig
√
T log K
2
≥ 1, ∀ alg, which is consistent to our proposition with
•	D is even: From the definition We can get that 夕 D (Ir) = £@三/。(Ir)夕 D (J i(Ir, a)).
If limA→∞ limτ →∞ EMRT (Ji(Ir,a))/1(J JlogA ≥ 1, ∀a ∈ AD (Ir), ∀alg,
then
lim lim
A→∞ T→∞
lim lim
A→∞ T→∞
EMRT,alg(Ir )
q ^D (Ir )T log A
Pa∈AD (Ir) EMRnτ(Ji(Ir,a)),alg(Ji(Ir，a))
q ψD (Ir )T log A
lim
|Ad (Ir )∣→∞
Σ
a∈AD (Ir)
lim lim
A→∞ nT (J i(Ir ,a))→∞
EMRnT (J i(I,a)),alg (J JI(Ir，a))
WD(Ji(Ir,a))nτ(Ji(Ir,a))log A
×
夕D(Ji(Ir, a))nτ(Ji(Ir, a))
PD (Ir )T
.	夕 D (Ji(I,a))
≥ lim lim >	1 X ——K	，/丁、、
A→∞ t一∞ a∈A⅛ Ir)	Ea∈AD (Ir) % (J i(I, a))
=1
In the first equation, We decompose the overall expected regret to the summation of regrets
on subtrees. This decomposition can be derived in a similar Way to that of Eq. (2) in
Appendix C.
In the second equation We transform the limitation of T into limitation of
nT(Ji(Ir, a)), ∀a ∈ AD(Ir). As the adversary selects action aj With probability
pD (Ir)j > 0 Where pD (Ir)j denotes the j-th element of pD (Ir ), When T → ∞, the
nT(Ji(Ir, a)) → ∞, ∀a ∈ AD(Ir) as Well.
The inequality is from induction and limT→∞nT(Ji(Ir, a))/T = pD(I)j, aj ∈ AD(Ir)
due to the strong laW of large numbers.
•	D is odd and D	6=	1: For convenience, let RiT,alg,imm (Ir)	=
maXa∈AD(Ir) PT=I ut(σt∣Ir→a,Ir) — ut(σt,Ir) denote the immediate regret on
the root node for algorithm alg. We first shoW that under our construction
EMRiT,alg,imm(Ir) ≥ 0, ∀ alg.
4(Cesa-Bianchi & Lugosi, 2006) proves that for K-arm online linear optimization problem,
ERT,alg
√T log K/2
supT,K maxut
≥ 1, hoWever the supremum can only get With K → ∞ and T/K → ∞ as
they use the property of maximum of infinite normal random variable and central limit theorem (CLT) corre-
spondingly. Here for clarity We equivalently change supremum into limit in our proof.
17
Published as a conference paper at ICLR 2020
Notice that in our construction, EM PT=I ut(σt∣ιr→a,Ir) = 0.5T, ∀a ∈ AD(Ir). Mean-
while, EM PtT=1 ut (σt , Ir) = 0.5T as well. Thus,
T1
EME ut(σt,Ir )= A	E	EM ut(σt∣ir→a ,Ir)
t=1	a∈AD(Ir)
=EM A	X ut(σ⅛lIr→a ,Ir)
a∈AD (Ir)
≤Em maχ ∖ ut(σt|i「→a,Ir)
a∈AD (Ir)
This inequality is true for all σt (i.e. alg), and we can get
T
EMRr,alg,imm (Ir ) = EMa maχι)Nut 出 | Ir-a ,Ir ) - ut(σt,Ir ) ≥ 0,	∀ alg
Then, similar to the case when D is even, we can get:
lim lim
A→∞ T →∞
lim lim
A→∞ r→∞
EMRrmg(Ir)
q WD(Ir )r log A
EM [Rr,alg,imm (Ir ) + Pa∈A0 (17)[成『(J W(Ir ,a)),alg (J i(Ir，a))]]
q wD(Ir )T logT
≥0+
lim lim
A→∞ r→∞
E
a∈AD (Ir ),nT (Ji (Ir ,a))<∞
EMRn τ (J i(Ir,a)),alg (J i(Ir , a))
q WD (Ir )T logT
+ lim	lim lim
IAD (Ir)l→∞ a∈AD (Ir),nTj i(Ir,a))→∞ A→∞ nT (J JH
EMRn τ (J i(Ir ,a)),alg (J JI(Ir，a))
q WD (IrK IogA
≥0+o+a→j→∞	x	J (Jta)「吟
a∈AD (Ir),nT (J i (Ir,a))→∞	D r
≥ lim lim ʌ/Pa∈AD (Ir ),nτ (Ji(Ir,a))→∞ 性(J HnTJ^
A→∞ T→∞ V	夕D (Ir)T
=lim S 卜D Fr ) -1
a→∞ V	夕D (Ir )
=1
Similarly, we decompose the overall expected regret to each subtree in the first equation.
NOtiCe that ∀ alg, limT →∞ Pa∈AD (Ir ),nτ (J i(Ir,a))<∞ EMRn T (J J(Ir,a)),alg < ∞,
limT→∞ Pa∈AD(Ir),nτ(Ji(Ir,a))<∞ EMRnT(Ji(Ir,a)),alg/√ = 0∙ ThUS the SeCond in-
equality is true. The last inequality is true by Pi √a ≥ √∑^^i.
Notice that pa∈AD(Ir) φD(Ji(Ir, a))nT(Ji(Ir,a)) = WD(Ir) - 1)T (We can see
this by nT (J i(Ir,a)) = PT=I σi(Ir,a) and ∀t,Pa∈AD(Ir)检(J i (Ir ,a))σi (Ir ,a) =
夕D(Ir) - 1, which can be simply derived by the definition of 夕D and P(Ir) = i), and
limT→∞ Pa∈AD(Ir),nτ(Ji(Ir,a))<∞ WZ(Ji(IrffrTT'""" = 0∙ ThUSWeCangetthe
limitation of T → ∞.
18
Published as a conference paper at ICLR 2020
□
Thus, with mathematical induction, we prove that limT →∞,A→∞ EM
RT
r yd τ log A
≥ 1, while with
the definition of D and Lemma 4, We can get 夕D = φi, thus We can get the mini-max lower bound in
Theorem 2.
E	The details of implementation
We noW discuss the detailed implementation of Lazy-CFR. To start, We derive Eq. (4). According to
Property 1, We have:
t
mt(h) =	X	πσ-ti0 (h)
t0=τt (h)+1
t2(h0)
= X	X	πσ-ti0 (h)
hLh∈subt(h0) t0 = tι(h0)+1
t2(h0)
=	X	X πσ-ti0 (h0)πσ-ti0 (h0, h)
h0∙.h∈subt(h0) t0 = tι(h0)+1
=	X	π-r(h0, h)α(h0)
h0∙.h∈subt(h0)
The first equation is the definition of mt; the second line is according to Prop 1.2; the third line is
derived from the definition of α and πσ-i and the last line is due to Property 1.3. Similarly, We can
Write rj as:
t
rj(Aa)= X	π-ti0(h)ui((h,a)lσto)
t0=τt (h)+1
t2(h0)
=X X	π-ti (h)ui((h, a)lσt0)
h0∙.h∈subt(h0) t0=tι (h0) + 1
t2(h0)
=α(h) +	X	X	π-ti0(h0)π-t/(h0,h)ui((h, a)lσt0)
h0 =h∙.h∈subt(h0) t0 = tι(h0)+1
=α(h) + U((h, a))	X	π-i(h0, h)α(h0)
h0 = h∙.h∈subt(h0)
We noW present hoW to implement of the ideas in 3.2.1.
19
Published as a conference paper at ICLR 2020
Algorithm 2 A detailed implementation of Lazy-CFR	
1 2 3 4 5 6 7 8 9 10 11 12	A two-player zero-sum extensive game. : Randomly initialize Γ. ∀h ∈ H, i ∈ {1,2}, compute the Counterfactual reward Ui(h) = ui(h∣Γ), flag(h) = -1. : ∀I ∈ I, s(I) = 0,∀a ∈ A(I),r(I,a) = 0. : while t< T do : for all i ∈ {1, 2} do :	αi (hr)+ = 1.0. :	UPDATE1(Iri , i) where Iri is the root of infoset tree Ii . : end for :	∀i ∈ {1, 2} UPDATE2(hr, i, t) where hr denotes the root of the history tree. : end while RETURN σ.
	
Algorithm 3 UPDATE1(I, i, t)	
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20	m(I) = 0 : for all h ∈ I do θi(h) = (θi(pɑ(h)) + OI(Pa(h))) X π-z(pa(h), h). :	m(I)+ = αi(h) + θi(h) : end for : if m(I) ≥ 1 then :	if P (I) = i then :	for all a ∈ A(I ) do :	UPDATE1((I, a), t). :	end for :	∀h ∈ I, updflag(h, t, i). :	∀a ∈ A(I), r (I, a) = 0. :	for all a ∈ A(I),h ∈ Ido :	r(I, a)+ = β(h, a) + θi(h)Ui((h, a)). :	end for :	Γi(I) =RM(r(I)). : else :	∀a ∈ A(I), UPDATE1((I,a),i, t). : end if : end if
	
Algorithm 4 updflag(h, t, i)	
1 2 3 4	: if h is not the root of history tree and flagi (h) 6= t then :	flagi (h) = t. :	updf lag (pa(h), t, i). : end if
	
Algorithm 5 UPDATE2(h, i, t)	
1 2 3 4 5 6 7 8 9 10 11 12	: Let i0 denote the opponent of player i. : if flagi(h) = t then :	∀a ∈ A(h) UPDATE2((h, a), i, t). : update U1 (h) and U2(h). :	αi(h) = 0. αi0(h) = 0. :	∀a ∈ A(h), βi0 (h, a) = 0. : else αi (h)+ =(俨(pa(h)) + Oi (pa(h))) × π-0i (pa(h), h). Oi0(h)+ = (θio(pa(h)) + Oi0(pa(h))) × π,0i (pa(h), h). ∀a ∈ A(h), βi0(h, a)+ = Ui'(h, a)(θi'(pa(h)) + Oi'(pa(h))) × π-0 (pa(h), h). end if
20