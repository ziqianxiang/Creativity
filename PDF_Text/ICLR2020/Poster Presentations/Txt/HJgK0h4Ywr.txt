Published as a conference paper at ICLR 2020
Theory and Evaluation Metrics for Learning
Disentangled Representations
Kien Do and Truyen Tran
Applied AI Institute, Deakin University, Geelong, Australia
{dkdo,truyen.tran}@deakin.edu.au
Ab stract
We make two theoretical contributions to disentanglement learning by (a) defin-
ing precise semantics of disentangled representations, and (b) establishing robust
metrics for evaluation. First, we characterize the concept “disentangled represen-
tations" used in supervised and unsupervised methods along three dimensions-
informativeness, separability and interpretability -which can be expressed and
quantified explicitly using information-theoretic constructs. This helps explain the
behaviors of several well-known disentanglement learning models. We then pro-
pose robust metrics for measuring informativeness, separability, and interpretability.
Through a comprehensive suite of experiments, we show that our metrics correctly
characterize the representations learned by different methods and are consistent
with qualitative (visual) results. Thus, the metrics allow disentanglement learning
methods to be compared on a fair ground. We also empirically uncovered new
interesting properties of VAE-based methods and interpreted them with our formu-
lation. These findings are promising and hopefully will encourage the design of
more theoretically driven models for learning disentangled representations.
1	Introduction
Disentanglement learning holds the key for understanding the world from observations, transferring
knowledge across different tasks and domains, generating novel designs, and learning compositional
concepts (Bengio et al., 2013; Higgins et al., 2017b; Lake et al., 2017; Peters et al., 2017; Schmidhuber,
1992). Assuming the observation x is generated from latent factors z via p(x|z), the goal of
disentanglement learning is to correctly uncover a set of independent factors {zi } that give rise to
the observation. While there has been a considerable progress in recent years, common assumptions
about disentangled representations appear to be inadequate (Locatello et al., 2019).
Unsupervised disentangling methods are highly desirable as they assume no prior knowledge about the
ground truth factors. These methods typically impose constraints to encourage independence among
latent variables. Examples of constraints include forcing the variational posterior q(z|x) to be similar
to a factorial p(z) (Burgess et al., 2018; Higgins et al., 2017a), forcing the variational aggregated prior
q(z) to be similar to the priorp(z) (Makhzani et al., 2015), adding total correlation loss (Kim & Mnih,
2018), forcing the covariance matrix of q(z) to be close to the identity matrix (Kumar et al., 2017), and
using a kernel-based measure of independence (Lopez et al., 2018). However, it remains unclear how
the independence constraint affects other properties of representation. Indeed, more independence
may lead to higher reconstruction error in some models (Higgins et al., 2017a; Kim & Mnih, 2018).
Worse still, the independent representations may mismatch human’s predefined concepts (Locatello
et al., 2019). This suggests that supervised methods - which associate a representation (or a group
of representations) zi with a particular ground truth factor yk - may be more adequate. However,
most supervised methods have only been shown to perform well on toy datasets (Harsh Jha et al.,
2018; Kulkarni et al., 2015; Mathieu et al., 2016) in which data are generated from multiplicative
combination of the ground truth factors. It is still unclear about their performance on real datasets.
We believe that there are at least two major reasons for the current unsatisfying state of disentangle-
ment learning: i) the lack of a formal notion of disentangled representations to support the design of
proper objective functions (Tschannen et al., 2018; Locatello et al., 2019), and ii) the lack of robust
evaluation metrics to enable a fair comparison between models, regardless of their architectures or
1
Published as a conference paper at ICLR 2020
design purposes. To that end, we contribute by formally characterizing disentangled representations
along three dimensions, namely informativeness, separability and interpretability, drawing from
concepts in information theory (Section 2). We then design robust quantitative metrics for these prop-
erties and argue that an ideal method for disentanglement learning should achieve high performance
on these metrics (Section 3).
We run a series of experiments to demonstrate how to compare different models using our proposed
metrics, showing that the quantitative results provided by these metrics are consistent with visual
results (Section 4). In the process, we gain important insights about some well-known disentanglement
learning methods namely FactorVAE (Kim & Mnih, 2018), β-VAE (Higgins et al., 2017a), and AAE
(Makhzani et al., 2015).
2 Rethinking Disentanglement
Inspired by (Bengio et al., 2013; Ridgeway, 2016), we adopt the notion of disentangled representation
learning as “a process of decorrelating information in the data into separate informative representa-
tions, each of which corresponds to a concept defined by humans”. This suggests three important
properties of a disentangled representation: informativeness, separability and interpretability, which
we quantify as follows:
Informativeness We formulate the informativeness of a particular representation (or a group of
representations) zi w.r.t. the data x as the mutual information between zi and x:
I(x,
zi)= x z
pD(x)q(zi|x) log
q(z∕χ)
q(Zi)
dz dx
(1)
where q(zi) = x pD(x)q(zi|x) dx. In order to represent the data faithfully, a representation zi
should be informative of x, meaning I(x, zi) should be large. Because I(x, zi) = H(zi) - H(zi|x),
a large value of I(x, zi) means that H(zi|x) ≈ 0 given that H(zi) can be chosen to be relatively
fixed. In other words, if zi is informative w.r.t. x, q(zi|x) usually has small variance. It is important
to note that I(x, zi) in Eq. 1 is defined on the variational encoder q(zi|x), and does not require a
decoder. It implies that we do not need to minimize the reconstruction error over x (e.g., in VAEs) to
increase the informativeness of a particular zi .
Separability and Independence Two representations zi, zj are separable w.r.t. the data x if they
do not share common information about x, which can be formulated as follows:
I(x, zi, zj) = 0	(2)
where I(x, zi, zj) denotes the multivariate mutual information (McGill, 1954) between x, zi and zj.
I(x, zi, zj) can be decomposed into standard bivariate mutual information terms as follows:
I(x,zi,zj) = I(x,zi) + I(x,zj) - I(x, (zi,zj)) = I(zi,zj) - I(zi,zj|x)
I(x, zi, zj) can be either positive or negative. It is positive if zi and zj contain redundant information
about x. The meaning of a negative I(x, zi , zj) remains elusive (Bell, 2003).
Achieving separability with respect to x does not guarantee that zi and zj are separable in general. zi
and zj are fully separable or statistically independent if and only if:
I(zi,zj) = 0	(3)
If we have access to all representations z, we can generally say that a representation zi is fully
separable (from other representations z6=i) if and only if I(zi , z6=i) = 0.
Note that there is a trade-off between informativeness, independence and the number of latent
variables which we discuss in Appdx. A.7.
Interpretability Obtaining informative and independent representations does not guarantee in-
terpretability by human (Locatello et al., 2019). We argue that in order to achieve interpretability,
we should provide models with a set of predefined concepts y. In this case, a representation zi is
2
Published as a conference paper at ICLR 2020
interpretable with respect to yk if it only contains information about yk (given that zi is separable
from all other z6=i and all yk are distinct). Full interpretability can be formulated as follows:
I(zi, yk) = H(zi) = H(yk)
(4)
Eq. 4 is equivalent to the condition that zi is an invertible function of yk. If we want zi to generalize
beyond the observed yk (i.e., H(zi) > H (yk)), we can change the condition in Eq. 4 into:
I(zi,yk) = H(yk) or H(yk|zi) = 0
(5)
which suggests that the model should accurately predict yk given zi . If zi satisfies the condition in
Eq. 5, it is said to be partially interpretable w.r.t yk.
In real data, underlying factors of variation are usually correlated. For example, men usually have
beard and short hair. Therefore, it is very difficult to match independent latent variables to different
ground truth factors at the same time. We believe that in order to achieve good interpretability, we
should isolate the factors and learn one at a time.
2.1	An information-theoretic definition of disentangled representations
Given a dataset D = {xi}iN=1, where each data point x is associated with a set of K labeled
factors of variation y = {y1 , ..., yK}. Assume that there exists a mapping from x to m groups
of latent representations z = {z1, z2, ..., zm} which follows the distribution q(z|x). Denoting
q(zi|x) = Pz q(z|x) and q(zi) = EpD(x) [q(zi|x)]. We define disentangled representations for
unsupervised cases as follows:
Definition 1 (Unsupervised). A representation or a group of representations zi is said to be “fully dis-
entangled” w.r.t a ground truth factor yk if zi is fully separable (from z6=i) and zi is fully interpretable
w.r.t yk. Mathematically, this can be written as:
I(zi, z6=i) = 0 and I(zi, yk) = H(zi, yk)
(6)
The definition of disentangled representations for supervised cases is similar as above except that
now we model q(z|x, y) instead of q(z|x) and q(z) = Px,y pD (x, y)q(z|x, y).
Recently, there have been several works (Eastwood & Williams, 2018; Higgins et al., 2018; Ridgeway
& Mozer, 2018) that attempted to define disentangled representations. Higgin et. al. (Higgins et al.,
2018) proposed a definition based on group theory (Cohen & Welling, 2014) which is (informally)
stated as follows: “A representation z is disentangled w.r.t a particular subgroup yk (from a symmetry
group y = {yk}kK=1) if z can be decomposed into different subspaces {zi}iH=1 in which the subspace
zi should be independent of all other representation subspaces z6=i , and zi should only be affected
by the action of a single subgroup yk and not by other subgroups y6=k .”. Their definition shares
similar observation as ours. However, it is less convenient for designing models and metrics than our
information-theoretic definition.
Eastwood et. al. (Eastwood & Williams, 2018) did not provide any explicit definition of disentangled
representations but characterizing them along three dimensions namely “disentanglement”, “com-
pactness”, and “informativeness” (between z any yk). A high “disentanglement” score (≈ 1) for zi
indicates that it captures at most one factor, let’s say yk. A high “completeness” score (≈ 1) for yk
indicates that it is captured by at most one latent zj and j is likely to be i. A high “informativeness”
score1 for yk indicates that all information of yk is captured by the representations z . Intuitively,
when all the three notions achieve optimal values, there should be only a single representation zi that
captures all information of the factor yk but no information from other factors y6=k. However, even in
that case, zi is still not fully interpretable w.r.t yk since zi may contain some information in x that
does not appear in yk. This makes their notions only applicable to toy datasets on which we know
that the data x are only generated from predefined ground truth factors y = {yk}kK=1. Our definition
can handle the situation where we only know some but not all factors of variation in the data. The
notions in (Ridgeway & Mozer, 2018) follow those in (Eastwood & Williams, 2018), hence, suffer
from the same disadvantage.
1In (Eastwood & Williams, 2018), the authors consider the prediction error of yk given z instead. High
“informativeness” score means this error should be close to 0.
3
Published as a conference paper at ICLR 2020
3 Robust Evaluation Metrics
We argue that a robust metric for disentanglement should meet the following criteria: i) it supports
both supervised/unsupervised models; ii) it can be applied for real datasets; iii) it is computationally
straightforward, i.e. not requiring any training procedure; iv) it provides consistent results across
different methods and different latent representations; and v) it agrees with qualitative (visual)
results. Here we propose information-theoretic metrics to measure informativeness, independence
and interpretability which meet all of these robustness criteria.
3.1	Metrics for informativeness
We measure the informativeness of a particular representation zi w.r.t. x by computing I(x, zi). If zi
is discrete, we can compute I(x, zi) exactly by using Eq. 1 but with the integral replaced by the sum.
If zi is continuous, we estimate I(x, zi) via sampling or quantization. Details about these estimations
are provided in Appdx. A.10.
If H(zi) is estimated via quantization, we will have 0 ≤ I(x, zi) ≤ H(zi). In this case, we can
divide I(x, zi) by H(zi) to normalize it to the range [0, 1]. However, this normalization may change
the interpretation of the metric and lead to a situation where a representation zi is less informative
than zj (i.e., I(x, zi) < I(x, zj)) but still has a higher rank than zj because H(zi) < H(zj). A better
way is to divide I(x, zi) by log(#bins).
3.2	Metrics for separability and independence
MISJED We can characterize the independence between two latent variables zi , zj based on
I(zi, zj). However, a serious problem of I(zi, zj) is that it generates the following order among pairs
of representations:
I (zf,i , zf,j ) > I (zf,i , zn,j ) > I (zn,i , zn,j ) ≥ 0
where zf,i, zf,j are informative representations and zn,i, zn,j are uninformative (or noisy) represen-
tations. This means if we simply want zi , zj to be independent, the best scenario is that both are
noisy and independent (e.g. q(zi|x) ≈ q(zj|x) ≈ N(0, I)). Therefore, we propose a new metric for
independence named MISJED (which stands for Mutual Information Sums Joint Entropy Difference),
defined as follows:
MISJED(Zi, Zj) = I(zi,Zj) = H(Zi) + H(Zj) — H(Zi, Zj)
= H(Zi) + H(Zj) - H(zi,zj) + H(zi,zj) - H(Zi,Zj)
= I(Zi , Zj ) + H(Zi , Zj ) - H(ZZi , ZZj )	(7)
where ZZi = Eq(zi |x) [Zi] and q(ZZi) = EpD(x) [q(ZZi|x)]. Since q(ZZi) and q(ZZj) have less variance than
q(Zi) and q(Zj), respectively, H(Zi, Zj) - H(ZZi, ZZj ) ≥ 0, making I(Zi, Zj ) ≥ 0.
To achieve a small value of I(Zi, Zj ), two representations Zi, Zj should be both independent and
informative (or, in an extreme case, are deterministic given x). Using the MISJED metric, we
can ensure the following order: 0 ≤ I(Zf,i, Zf,j) < I(Zf,i, Zn,j) < I(Zn,i, Zn,j). If H(Zi), H(Zj),
and H(ZZi, ZZj ) in Eq. 7 are estimated via quantization, we will have I(Zi, Zj) ≤ H(Zi) + H(Zj ) ≤
2 log(#bins). In this case, We can divide I(Zi, Zj) by 2 log(#bins) to normalize it to [0, 1].
WSEPIN and WINDIN A theoretically correct way to verify that a particular representation Zi is
both separable from other Z6=i and informative W.r.t x is considering the amount of information in x
but not in Z6=i that Zi contains. This quantity is the conditional mutual information betWeen x and Zi
given Z6=i , Which can be decomposed as folloWs:
I(x, Zi|Z6=i) = I(x, Zi) - I(x, Zi, Z6=i)
= I(x, Zi) - (I(Zi, Z6=i) - I(Zi, Z6=i|x))
= I(x, Zi) - I(Zi, Z6=i) + I(Zi, Z6=i|x)	(8)
I(x, Zi|Z6=i) is useful for measuring hoW disentangled a representation Zi is in the absence of ground
truth factors. I(x, Zi|Z6=i) is close to 0 if Zi is completely noisy and is high if Zi is disentangled2.
2Note that only informativeness and separability are considered in this case.
4
Published as a conference paper at ICLR 2020
For models that use factorized encoders, zi and z6=i are usually assumed to be independent given
x, hence, I(zi, z6=i|x) ≈ 0 and I(x, zi|z6=i) ≈ I(x, zi) - I(zi, z6=i) which is merely the difference
between the informativeness and full separability of zi . For models that use auto-regressive encoders,
I(zi, z6=i|x) > 0 which means zi and z6=i can share information not in x.
We can also compute I(x, zi|z6=i) in a different way as follows:
I(x,zi|z6=i) = I(x, (zi, z6=i)) - I(x, z6=i)
= I(x, z) - I(x, z6=i)
If we want zi to be both independence of z6=i and informative w.r.t x, we can only use the first two
terms in Eq. 8 to derive another quantitive measure:
I(x, zi|z6=i) = I(x, zi) - I(zi, z6=i)
= I(x, zi|z6=i) - I(zi, z6=i|x)	(9)
However, unlike I(x, zi|z6=i), I(x, zi|z6=i) can be negative.
To normalize I(x, zi|z6=i), we divide it by H(zi) (H(zi) must be estimated via quantization). Note
that taking the average of I(zi, x|z6=i) over all representations to derive a single metric for the whole
model is not appropriate because models with more noisy latent variables will be less favored. For
example, if model A has 10 latent variables (5 of them are disentangled and 5 of them are noisy), and
model B has 20 latent variables (5 of them are disentangled and 15 of them are noisy), B will always
be considered worse than A despite the fact that both are equivalent in term of disentanglement (since
5 disentangled representations are enough to capture all information in x so additional latent variables
should be noisy). We propose two solutions for this issue. In the first approach, we sort I(x, zi|z6=i)
over all representations in descending order and only take the average over the top k latents (or groups
of latents). This leads to a metric called SEPIN@k3 which is similar to Precision@k:
1 k-1
SEPIN@k = k XI(x,Zri∣Z=ri)
k i=0
where r1, ..., rL is the rank indices of L latent variables by sorting I(x, zi|z6=i) (i = 1, ..., L).
In the second approach, we compute the average over all L representations z0, ..., zL-1 but weighted
by their informativeness to derive a metric called WSEPIN:
L-1
WSEPIN = X ρil(x,zi∖z=i)
i=0
where Pi = PL-XzL.). If zi is a noisy representation, I(x, zi) ≈ 0, thus, zi contributes almost
j=0 I (x,zj )
nothing to the final WSEPIN.
Similarly, using the measure I(x, zi∣z=i) in Eq. 9, We can derive other two metrics INDIN@k4 and
WINDIN as follows:
1 k-1	L-1
INDIN@k = - X I(x,Zri ∣z=ri) and WINDIN = X ρiI(x,zi∣z=i)
i=0	i=0
3.3 Metrics for interpretability
Recently, several metrics have been proposed to quantitatively evaluate the interpretability of repre-
sentations by examining the relationship between the representations and manually labeled factors
of variation. The most popular ones are Z-diff score (Higgins et al., 2017a; Kim & Mnih, 2018),
SAP (Kumar et al., 2017), MIG (Chen et al., 2018). Among them, only MIG is theoretically
sound and provides correct computation of I(x, zi). MIG also matches with our formulation of
“interpretability” in Section 2 to some extent. However, MIG has only been used for toy datasets
3SEPIN stands for SEParability and INformativeness
4INDIN stands for INDependence and INformativeness
5
Published as a conference paper at ICLR 2020
GT factor
input
latent code
GT factor
input
latent code
GT factor
input
latent code
MIG
input
GT factor
latent code
RMIG
MIG
RMIG


(a) Unsupervised
(b) Supervised
Figure 1: Differences in probabilistic assumption of MIG and Robust MIG.
like dSprites (Matthey et al., 2017). The main drawback comes from its probabilistic assumption
P(Zi,yk,χ5n) = q(zi|x(n))p(x(n)|yk)p(yk) (see Fig. 1). Note thatp(x(n)|yk) is a distribution over
the high dimensional data space, and is very hard to robustly estimate but the authors simplified it
to be p(n∣yk) if x(n) ∈ Dyk (Dykis the support set for a particular value yk) and 0 otherwise. This
equation only holds for toy datasets where we know exactly how X is generated from y. In addition,
since p(n∣yk) depends on the value of yk, it will be problematic if yk is continuous.
RMIG Addressing the drawbacks of MIG, we propose RMIG (which stands for Robust MIG),
formulated as follows:
RMIG(yk) = I(zi*,yk) - I(zj。,y)	(10)
where I(zi*, yk) and I(zj。, yk) are the highest and the second highest mutual information values
computed between every Zi and yk; zi* and Zj。are the corresponding latent variables. Like MIG, we
can normalize RMIG(yk) to [0, 1] by dividing it by H(yk) but it will favor imbalanced factors (small
H(yk)).
RMIG inherits the idea of MIG but differs in the probabilistic assumption (and other technicalities).
RMIG assumes that p(Zi, yk, x(n)) = q(Zi|x(n))p(yk |x(n))p(x(n)) for unsupervised learning and
p(Zi, yk, x(n)) = q(Zi|yk(n), x(n))p(yk(n), x(n)) for supervised learning (see Fig. 1). Not only this
eliminates all the problems of MIG but also provides additional advantages. First, we can estimate
q(Zi, yk) using Monte Carlo sampling on p(x(n)). Second, p(yk|x(n)) is well defined for both
discrete/continuous yk and deterministic/stochastic p(yk |x(n)). If yk is continuous, we can quantize
p(yk |x(n)). If p(yk |x(n)) is deterministic (i.e., a Dirac delta function), we simply set it to 1 for the
value of yk corresponding to x(n) and 0 for other values ofyk. Our metric can also use p(yk |x(n))
from an external expert model. Third, for any particular value yk, we compute q(Zi |x(n)) for all
x(n) ∈ D rather than just for x(n) ∈ Dyk, which gives more accurate results.
JEMMIG A high RMIG value of yk means that there is a representation Zi* that captures the factor
yk . However, Zi* may also capture other factors y6=k of the data. To make sure that Zi* fits exactly
to yk , we provide another metric for interpretability named JEMMIG (standing for Joint Entropy
Minuses Mutual Information Gap), computed as follows:
JEMMIG(yk) = H(zi* ,yk) - I(zi* Jyk) + I(zj。Jyk)
where I(Zi* , yk) and I(Zj。, yk) are defined in Eq. 10.
If we estimate H(zi* , yk) via quantization, we can bound JEMMIG(yk) between 0 and H(yk) +
log(#bins) (please check Appdx. A.12 for details). A small JEMMIG(yk) score means that zi*
should match exactly to yk and zj。 should not be related to yk. Thus, we can use JEMMIG(yk) to
validate whether a model can learn disentangled representations w.r.t a ground truth factor yk or not
which satisfies the definition in Section 2.1. Note that if we replace H(zi* , yk) by H(yk) to account
for the generalization of zi* over yk , we obtain a metric equivalent to RMIG (but in reverse order).
To compute RMIG and JEMMIG for the whole model, we simply take the average of RMIG(yk) and
JEMMIG(yk) over all yk (k = 1, ..., K) as follows:
1	K-1	1 K-1
RMIG= — X RMIG(yk) and JEMMIG = — X JEMMIG(y®)
k=0	k=0
6
Published as a conference paper at ICLR 2020
3.4 Comparison with existing metrics
In Table 1, we compare our proposed metrics with existing metrics for learning disentangled repre-
sentations. For deeper analysis of these metrics, we refer readers to Appdx. A.8. One can easily see
that only our metrics satisfy the aforementioned robustness criteria. Most other metrics (except for
MIG and Modularity) use classifiers, which can cause inconsistent results once the settings of the
classifiers change. Moreover, most other metrics (except for MIG) use Eq(zi |x) [zi] instead of q(zi|x)
for computing mutual information. This can lead to inaccurate evaluation results since Eq(zi |x) [zi]
is theoretically different from Zi 〜q(z∕x). Among all metrics, JEMMIG is the only one that can
quantify “disentangled representations” defined in Section. 2.1 on its own.
Metrics	#classifiers	classifier	nonlinear relationship	use q(zi |x)	continuous factors	real data
Z-diff	1	Iinear/majority-vote	×	×	×	×
SAP	L × K	threshold value	×	×	X	-	X
MIG	0	none	X	X	×	×
Disentanglement	K	LASSO/ random forest	×∕X	×	×	×
Completeness						
Informativeness						
Modularity	0	none	X	×	X	×
Explicitness	K	one-vs-rest logistic regressor	×	×	×	×
WSEPINt	0	none	X	X	X	X
WINDINt	0	none	X	X	X	-	X
RMIG	0	none	X	X	X	X
JEMMIG*	0	none	X	X	X	X
Table 1: Analysis of different metrics for disentanglement learning. L and K are the numbers of
latent variables and ground truth factors, respectively. Metrics marked with * are self-contained.
Metrics marked with * do not require ground truth factors of variation.
4	Experiments
We use our proposed metrics to evaluate three representation learning methods namely FactorVAE
(Kim & Mnih, 2018), β-VAE (Higgins et al., 2017a) and AAE (Makhzani et al., 2015) on both real
and toy datasets which are CelebA (Liu et al., 2015) and dSprites (Matthey et al., 2017), respectively.
A brief discussion of these models are given in Appdx. A.1. We would like to show the following
points: i) how to compare models based on our metrics; ii) the advantages of our metrics compared
to other metrics; iii) the consistence between qualitative results produced by our metrics and visual
results; and iv) the ablation study of our metrics.
Due to space limit, we only present experiments for the first two points. The experiments for points
(iii) and (iv) are put in Appdx. A.4 and Appdx. A.5, respectively. Details about the datasets and
model settings are provided in Appdx. A.2 and Appdx. A.3, respectively. In all figures below, “TC”
refers to the γ coefficient of the TC loss in FactorVAEs (Kim & Mnih, 2018), “Beta” refers to the β
coefficient in β-VAEs (Higgins et al., 2017a).
Informativeness In Figs. 2a and 2b, we show the average amount of information (of x) that
a representation zi contains (the mean of I(zi, x)) and the total amount of information that all
representations z contain (I(z, x)). It is clear that adding the TC term to the standard VAE loss does
not affect I(z, x) much (Fig. 2b). However, because zi and zj in FactorVAEs are more separable
than those in standard VAEs, FactorVAEs should produce smaller I(zi, x) than standard VAEs on
average (Fig. 2a). We also see that the mean of I(zi, x) and I(z, x) consistently decrease for β-VAEs
with higher β .
Separability and Independence If we only evaluate models based on the separability of represen-
tations, β-VAE models with large β are among the best. These models force latent representations
7
Published as a conference paper at ICLR 2020
Figure 2: The informativeness and the total information of some FactorVAE and β-VAE models. For
each hyperparameter, we report the mean and the standard error of 4 different runs.
4 3 2 1c
αulz 二Z)-U-E∕UBE∕XBE
10	20	50	100	1	4	10	20	50
model
(a) max/mean/min of I (zi , z6=i)
10	20	50	100	1	4	10	20	50
model
(b) WSEPIN
10	20	50	100	1	4	10	20	50
model
(c) SEPIN@3
Figure 3: I(zi,z6=i),WSEPINandSEPIN@3 of some FactorVAE and β-VAE models.
to be highly separable (as in Fig. 3a, we can see that the max/mean/min values of I (zi , z6=i) are
equally small for β-VAEs with large β). In FactorVAEs, informative representations usually have
poor separability (large value) and noisy representations usually have perfect separability (≈ 0)
(Fig. 4a). Increasing the weight of the TC loss improves the max and mean of I(zi , z6=i) but not
significance (Fig. 3a).
Using WSEPIN and SEPIN@3 gives us a more reasonable evaluation of the disentanglement capabil-
ity of these models. In Fig. 3b, we see that β-VAE models with β = 10 achieve the highest WSEPIN
and SEPIN@3 scores, which suggests that their informative representations usually contain large
amount of information of x that are not shared by other representations. However, this type of infor-
mation may not associate well with the ground truth factors of variation (e.g., z3 , z6 in Fig. 4c). The
representations of FactorVAEs, despite containing less information of x on their own, usually reflect
the ground truth factors more accurately (e.g., z5 , z8 , z7 in Fig. 4a) than those of β-VAEs. These
results suggest that ground truth factors should be used for proper evaluations of disentanglement.
z[5] (SEP=2.7832f INFO=4.5231)
z[8] (SEP=2.7307, INFO=4.5229)
Iz[0] (SEP=2.6833, INFO=3.9097)
z[7] (SEP=2.3561r INFO=2.8971)
z[9] (SEP=1.4175f INFO=I.5442)
z[5] (SEP=4.4006, INFO=4.7944)
z[8] (SEP=4.3106f INFO=4.7764)
∣z[2] (SEP=3.7150f INFO=3.8164)
z[6] (SEP=2.9649, INFO=3.2606)
z[9] (SEP=2.8458, INFO=3.1706)
z[3] (SEP=0.0002, INFO=0.0001)
z[4] (SEP=0.0002f INFO=0.0001)
z[l] (SEP=0.0002, INFO=0.0001)
z[0] (SEP=0.0002, INFO=0.0001)
z[7] (SEP=0.0001, INFO=0.0001)
(a) FactorVAE (TC=10)
(b) VAE
z[3] (SEP=0.1424, INFO=I.9369)
z[6] (SEP=0.1306, INFO=O.7636)
∣z[2] (SEP=0.0734, INFO=2.8331)
z[7] (SEP=0.0722, INFO=2.8457)
z[9] (SEP=0.0425, INFO=0.0000)
z[5] (SEP=0.0423, INFO=O.0000)
z[8] (SEP=0.0419, INFO=O.0001)
z[4] (SEP=0.0417f INFΘ=O.OOOO)
z[l] (SEP=0.0400f INFO=0.0001)
z[0] (SEP=0.0398, INFO=O,0000)
(c) β-VAE (β = 10)
Figure 4:	Visualization of the representations learned by representative FactorVAE, VAE, and β-VAE
models with separability (I (zi , z6=i)) and informativeness (I (zi , x)) scores. Representations are
sorted by their separability scores.
Interpretability Using JEMMIG and RMIG, we see that FactorVAE models can learn represen-
tations that are more interpretable than those learned by β-VAE models. Surprisingly, the worst
FactorVAE models (with TC=10) clearly outperform the best β-VAE models (with β = 10). This
result is sensible because it is accordant with the visualization in Figs. 4a and 4c.
Comparison with Z-diff In (Chen et al., 2018), the authors have already shown that MIG is more
robust than Z-diff (Higgins et al., 2017a) so we compare our metrics with MIG directly.
8
Published as a conference paper at ICLR 2020
O2

10	20	50 IOO 1	4	10	20	50
model
(a)	JEMMIG
0.0
10	20	50 IOO 1	4	10	20	50
model
(b)	RMIG
Beta
(c) JEMMIG vs. RMIG
■ +
Figure 5:	(a) and (b): Unnormalized JEMMIG and RMIG scores of several FactorVAE and β-VAE
models. (c): Correlation between JEMMIG and RMIG.
Comparison with MIG On toy datasets like dSprites, RMIG produces similar results as MIG
(Chen et al., 2018). Please check Appdx. A.13 for more details.
-ju3E3-6usuθs-α
8-tc50
L4tcl00
21 beta4
0-⅝10∙-
∙∙∙
7_tc20
32_beta50
28_beta20
I7_betai
ILbetaI
12 tclOO
0	12	3
JEMMIG
(a)	JEMMIG vs. Disentanglement
-ju3E3-6usuθs-α
17_betal
ILbetaI
32_beta50
28_beta20
12 tcl∞
10-tc50
8_tc50
7_tc20
UJclOO
28JMta20
2i-beta4	32_beta50
SS3U3"一 dE。。
0-⅛10
Ul8	7te20
I7_betai
i8-betai
12-tcl∞
0	12	3
JEMMIG
(b)	JEMMIG vs. Completeness
28_beta20
32-be⅛50	2i-beta4
■ ■ ■
Ooo
SS3U3一 dE。。
I7_betai
I8_betai
12-tcl∞
O-tclO
10-tc50
8-tc50
14 tcl∞
7 tc2O
05050505
76633443
Oooooooo
∙10jj
0.70
0.65
0.60
g 0.55
uj 0.50
0.45
0.40
0.35
0	12	3
JEMMIG
(c) JEMMIG vs. Error
0.25 0.50 0.75 1.00 1.25 1.50 1.75	0.25 0.50 0.75 1.00 1.25 1.50 1.75	0.25 0.50 0.75 1.00 1.25 1.50 1.75
RMIG	RMIG	RMIG
(d) RMIG vs. Disentanglement (e) RMIG vs. Completeness	(f) RMIG vs. Error
Figure 6:	Comparison between JEMMIG/RMIG and the metrics in (Eastwood & Williams, 2018).
Because the competing metrics do not apply for categorical factors (see Appdx. A.8 for detailed
analysis), we exclude the “shape” factor during computation. Following (Eastwood & Williams,
2018), we use LASSO classifiers with the L1 coefficient is α = 0.002. Blue dots denote FactorVAE
models and orange dots denote β-VAE models.
Comparison with “disentanglement”, “completeness” and “informativeness” In Fig. 6, we
show the differences in evaluation results between JEMMIG/RMIG and the metrics in (Eastwood &
Williams, 2018). We can easily see that JEMMIG and RMIG are much better than “disentanglement”,
“completeness” and “informativeness” (or reversed classification error) in separating FactorVAE and
β-VAE models. Among the three competing metrics, only “informativeness” (or I(z, yk)) seems to
be correlated with JEMMIG and RMIG. This is understandable because when most representations
are independent in case of FactorVAEs and β-VAEs, we have I(z,yk) ≈ I(zi* ,yk) ≈ I(zi* ,yk)-
I(zj。,yk). “Disentanglement” and “completeness"，by contrast, are strongly uncorrelated with
JEMMIG and RMIG. While JEMMIG consistently grades standard VAEs (β = 1) worse than
other models (Fig. 5a), “disentanglement” and “completeness” usually grade standard VAEs better
than some FactorVAE models, which seems inappropriate. Moreover, since “disentanglement” and
“completeness” are not well aligned, using both of them at the same time may cause confusion. For
example, the model “28_beta20” has lower “disentanglement” score yet higher “completeness” score
9
Published as a conference paper at ICLR 2020
I I I I
fau9-BUWS≡
10	20	50
Figure 7: “disentanglement”, “completeness” and “informativeness” (error) scores of several Factor-
VAE and β-VAE models.
10	20	50 100	1	4	10	20	50
model
(c) Error
(a) Disentanglement
(b) Completeness
0.950-
32J>βU5O
21J>βa4
0.925
⅞ 0.650-
0.B25
0.600-
284320
12JBIOO
I 0.675
I7_bea)
18J>βtal
IEMMIG
I
ffHinpow
iilUiifi ⅛UU∏11
10	20	50 100	1	4	10	20	50
model
0.80
10	20	50 IOO 1	4	10	20	50
model
⑶ JEMMIG vs. Modularity (b) Modularity (original)	(C) Modularity (correct)
Figure 8: (a): Comparison between JEMMIG and “modularity” (#bins=100). (b) and (c): “modularity”
scores of several FactorVAE and β-VAE models. The original version computes I(zi,yk) using
Eq(zi∣χ)[zi] while the correct version compute I(zi, yk) using q(zi∣x).
than the model “32_beta50” (Figs. 6a and 6b) so it is hard to know which model is better than the
other at learning disentangled representations.
From Figs. 7a and 7b, We see that “disentanglement” and “completeness” blindly favor β-VAE
models with high β without concerning about the fact that representations in these models are less
informative than representations in FactorVAEs (Fig. 7c). Thus, they are not good for characterizing
disentanglement in general.
Disentanglement and “completeness” are computed based on a weight matrix with an assumption
that the weight magnitudes for noisy representations are close to 0. However, this assumption is often
broken in practice, thus, may lead to inaccurate results (please check Appdx. A.9 for details).
Comparison with “modularity” “modularity” and “explicitness” (Ridgeway & Mozer, 2018)
are similar to “disentanglement” and “informativeness” (Eastwood & Williams, 2018) in terms of
concept, respectively. However, they are different in terms of formulation. We exclude “explicitness”
in our experiment because computing it on dSprites is time consuming. In Fig. 8a, we show the
correlation between JEMMIG and “modularity”. We consider two versions of “modularity”. In the
first version (Fig. 8b), I(zi, yk) is computed from the mean of Zi 〜 q(z∕x). This is the original
implementation provided by (Ridgeway & Mozer, 2018). In the second version (Fig. 8c), I(zi, yk) is
computed from q(zi|x). We can see that in either case, “modularity” often gives higher scores for
standard VAEs than for FactorVAEs. It means that like “disentanglement”, “modularity” itself does
not fully specify “disentangled representations” defined in Section 2.1.
5 Conclusion
We have proposed an information-theoretic definition of disentangled representations and designed ro-
bust metrics for evaluation, along three dimensions: informativeness, separability and interpretability.
We carefully analyze the properties of our metrics using well known representation learning models
namely FactorVAE, β-VAE and AAE on both real and toy datasets. Compared with existing metrics,
our metrics are more robust and produce more sensible evaluations that are compatible with visual
results. Based on our definition of disentangled representation in Section 2.1, WSEPIN/JEMMIG are
the two key metrics in case ground truth labels are unavailable/available, respectively.
10
Published as a conference paper at ICLR 2020
References
Error function. https://en.wikipedia.org/wiki/Error_function, May 2019.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Anthony J Bell. The co-information lattice. In Proceedings of the Fifth International Workshop on
Independent Component Analysis and Blind Signal Separation: ICA, volume 2003, 2003.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828,
2013.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner. Understanding disentangling in beta-vae. arXiv preprint
arXiv:1804.03599, 2018.
Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement
in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018.
Taco Cohen and Max Welling. Learning the irreducible representations of commutative lie groups.
In International Conference on Machine Learning, pp. 1755-1763, 2014.
Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of
disentangled representations. 2018.
Ananya Harsh Jha, Saket Anand, Maneesh Singh, and VSR Veeravasarapu. Disentangling factors
of variation with cycle-consistent variational auto-encoders. In Proceedings of the European
Conference on Computer Vision (ECCV), pp. 805-820, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017a.
Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matko Bosnjak,
Murray Shanahan, Matthew Botvinick, Demis Hassabis, and Alexander Lerchner. Scan: Learning
hierarchical compositional visual concepts. arXiv preprint arXiv:1707.03389, 2017b.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. ICML, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional
inverse graphics network. In Advances in Neural Information Processing Systems, pp. 2539-2547,
2015.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled
latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Scholkopf, and Olivier
Bachem. Challenging common assumptions in the unsupervised learning of disentangled represen-
tations. ICML, 2019.
11
Published as a conference paper at ICLR 2020
Romain Lopez, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Information constraints on auto-
encoding variational bayes. In Advances in Neural Information Processing Systems, pp. 6114-6125,
2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann
LeCun. Disentangling factors of variation in deep representation using adversarial training. In
Advances in Neural Information Processing Systems, pp. 5040-5048, 2016.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
William McGill. Multivariate information transmission. Transactions of the IRE Professional Group
on Information Theory, 4(4):93-111, 1954.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference: foundations
and learning algorithms. MIT press, 2017.
Karl Ridgeway. A survey of inductive biases for factorial representation learning. arXiv preprint
arXiv:1612.05299, 2016.
Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic
loss. In Advances in Neural Information Processing Systems, pp. 185-194, 2018.
Michal Rolinek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca directions
(by accident). arXiv preprint arXiv:1812.06775, 2018.
Jurgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation,
4(6):863-879, 1992.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611-622, 1999.
Michael Tschannen, Olivier Bachem, and Mario Lucic. Recent advances in autoencoder-based
representation learning. arXiv preprint arXiv:1812.05069, 2018.
12
Published as a conference paper at ICLR 2020
A	Appendix
A. 1 REVIEW OF FACTORVAES, β-VAES AND AAES
Standard VAEs are trained by minimizing the variational upper bound LVAE of - log pθ (x) as follows:
Lvae = EpD(x) [Eqφ(z∣χ) [- logPθ(x|z)] + Dkl Sφ(Zlx)IIp(Z))]	(II)
where qφ(z∣x) is an amortized variational posterior distribution. However, this objective does not
lead to disentangled representations (Higgins et al., 2017a).
β-vAEs (Higgins et al., 2017a) penalize the KL term in the original vAE loss more heavily with a
coefficient β 1:
Le-VAE = EpD(x) [Eqφ(z∣χ) [-logPθ(x|z)] + eDKL (qφ(z∣x)kp(z))]
SinceEpD(x) [DKL (qφ(Zlx)Ip(Z))] = Iφ(x, Z) + DKL (qφ(Z)Ip(Z)), more penalty on the KL term
encourages qφ(Z) to be factorized but also forces Z to discard more information in x.
FactorVAEs (Kim & Mnih, 2018) add a constraint to the standard VAE loss to explicitly impose
factorization of qφ(Z):
LFactorVAE = LVAE + γDKL qφ(Z)I Yqi(Zi)	(12)
where DKL (qφ(Z)I Qi qi(Zi)) ≥ 0 is known as the total correlation (TC) of Z. Intuitively, γ can be
large without affecting the mutual information between Z and x, making FactorVAE more robust than
β-VAE in learning disentangled representations. Other related models that share similar ideas with
FactorVAEs are are β-TCVAEs (Chen et al., 2018) and DIP-VAEs (Kumar et al., 2017).
The loss of AAEs (Makhzani et al., 2015) is derived from the standard VAE loss by removing the
term Iφ(x, Z):
Laae = EpD(x) [Eqφ(z∣x) [-logpθ(x|z)]] + DKL (qφ(z)kp(z))
Different from the losses of β-VAEs and FactorVAEs, AAE loss is not a valid upper bound on
- logpθ(x).
A.2 Datasets
The CelebA dataset (Liu et al., 2015) consists of more than 200 thousands face images with 40 binary
attributes. We resize these images to 64 × 64. The dSprites dataset (Matthey et al., 2017) is a toy
dataset generated from 5 different factors of variation which are “shape” (3 values), “scale” (6 values),
“rotation” (40 values), “x-position” (32 values), “y-position” (32 values). Statistics of these datasets
are provided in Table 2.
Dataset	#Train	#TeSt	Image size
CelebA	162,770	19,962	64×64×3
dSprites	737,280	0	64×64×1
Table 2: Summary of datasets used in experiments.
A.3 Model settings
For FactorVAE, β-VAE and AAE, we used the same architectures for the encoder and decoder (see
Table 3 and Table 45), following (Kim & Mnih, 2018). We trained the models for 300 epochs with
mini-batches of size 64. The learning rate is 10-3 for the encoder/decoder and is 10-4 for the
discriminator over Z. We used Adam (Kingma & Ba, 2014) optimizer with β1 = 0.5 and β2 = 0.99.
Unless explicitly mentioned, we use the following default settings: i) for CelebA: the number of
5Only FactorVAE and AAE use a discriminator over z
13
Published as a conference paper at ICLR 2020
latent variables is 65, the TC coefficient in FactorVAE is 50, the value for β in β-VAE is 50, and the
coefficient for the generator loss over z in AAE is 50; ii) for dSprites: the number of latent variables
is 10.
Encoder	Decoder	Discriminator Z
X dims: 64x64x3	z dim: 65	z dim: 65
conv (4, 4, 32), Stride 2, ReLU	FC 1x1x256, ReLU	5x[FC 1000, LReLU]
conv (4, 4, 32), Stride 2, ReLU	deconv (4, 4, 64), stride 1, valid, ReLU	FCl
conv (4, 4, 64), stride 2, ReLU	deconv (4, 4, 64), stride 2, ReLU	D(z):1
conv (4, 4, 64), stride 2, ReLU	deconv (4, 4, 32), stride 2, ReLU	
conv (4, 4, 256), stride 1, valid, ReLU	deconv (4, 4, 32), Stride 2, ReLU 一	
FC 65	一	deconv (4, 4, 3), stride 2, ReLU	
Z dim: 65	x dim: 64x64x3	
Table 3: Model architectures for CelebA.
Encoder	Decoder	Discriminator Z
x dims: 64x64x 1	z dim: 10	Z dim: 10
conv (4, 4, 32), stride 2, ReLU	FC 128, ReLU	5x[FC 1000, LReLU]
conv (4, 4, 32), stride 2, ReLU	FC4x4x64, ReLU	FCI
conv (4, 4, 64), stride 2, ReLU	deconv (4, 4, 64), stride 2, ReLU	D(Z):1
conv (4, 4, 64), stride 2, ReLU	deconv (4, 4, 32), stride 2, ReLU	
FC 128, ReLU	deconv (4, 4, 32), stride 2, ReLU	
FC 10	deconv (4, 4, 1), stride 2, ReLU	
z dim: 10	x dim: 64x64x 1	
Table 4: Model architecture for dSprites.
A.4 Consistence between quantitative and qualitative results
A.4. 1 CelebA
Informativeness We sorted the representations of different models according to their informative-
ness scores in the descending order and plot the results in Fig. 9. There are distinct patterns for
different methods. AAE captures equally large amounts of information from the data while FactorVAE
and β-VAE capture smaller and varying amounts. This is because FactorVAE and β-VAE penalize
the informativeness of representations while AAE does not. Recall that I(zi, x) = H(zi) - H(zi|x).
For AAE, H(zi|x) = 0 and H(zi) is equal to the entropy ofN(0, I). For FactorVAE and β-VAE,
H(zi|x) > 0 and H(zi) is usually smaller than the entropy ofN(0, I) due to a narrow q(zi)6.
Figure 9: Normalized informativeness scores (bins=100) of all latent variables sorted in descending
order.
6 Note that H(zi) does not depend on whether q(zi) is zero-centered or not
14
Published as a conference paper at ICLR 2020
In Fig. 9, We see a sudden drop of the scores to 0 for some FaCtorVAE's and β-VAE's representations.
These representations zi are totally random and contain no information about the data (i.e., q(zi|x) ≈
N(0, I)). We call them “noisy” representations and provide discussions in Appdx. A.7.
We visualize the top 10 most informative representations for these models in Fig. 10. AAE’s
representations are more detailed than FaCtorVAE's and β-VAE,s, suggesting the effect of high
informativeness. HoWever, AAE’s representations mainly capture information Within the support of
pD (x). This explains Why We still see a face When interpolating AAE's representations. By contrast,
FactorVAE's and β-VAE's representations usually contain information outside the support ofpD(x).
Thus, When We interpolate these representations, We may see something not resembling a face.
(a) FactorVAE (TC=50)
(b) β-VAE (β=50)
Figure 10: Visualization of the top informative representations. Scores are unnormalized.
(c) AAE (Gz=50)
Separability and Independence Table 5 reports MISJED scores (Section 3.2) for the top most
informative representations. FactorVAE achieves the loWest MISJED scores, AAE comes next and β-
VAE is the Worst. We argue that this is because FactorVAE learns independent and nearly deterministic
representations, β-VAE learns strongly independent yet highly stochastic representations, and AAE,
on the other extreme side, learns strongly deterministic yet not very independent representations. From
Table 5 and Fig. 11, it is clear that MISJED produces correct orders among pairs of representations
according to their informativeness.
	MISJED (unnormalized)					
	Zl,Z2	Z1 ,Z3	Zl,Z-1	Z1 ,Z-2	Z-1,Z-2	Z-1 ,Z-3
FactorVAE	0.008	0.009	:2.476	2.443 :	:4.858	4.892
β-VAE	0.113	0.131	3.413	3.401	6.661	6.739
AAE	0.022	0.023	0.022	0.021	0.021	0.020
Table 5: Unnormalized MISJED scores (#bins = 50, 10% data). z1, z2, z3 and z-1, z-2, z-3 denote
the top 3 and the bottom 3 latent variables sorted by the informativeness scores in descending order.
Boldness indicates best results.
匚
0.6
0.5
0.4
0.3
0.2
0.1
0	5	10 15 20 25 30 35 40 45 50 55 60
0
5
10
15
20
25
30
35
40
45
50
55
60
(a) FactorVAE (TC=50)
o-
10-
20-
25-
30-
40-
45-
50-
60-
0.0
0	5	10 15 20 25 30 35 40 45 50 55 60
0	5	10 15 20 25 30 35 40 45 50 55 60
(c) AAE (Gz=50)
Figure 11: Normalized MISJED scores of all latent pairs sorted by their informativeness.
(b) β-VAE (β=50)
15
Published as a conference paper at ICLR 2020
Interpretability We report the RMIG scores and JEMMIG scores for several ground truth factors
in the CelebA dataset in Tables 6 and 7, respectively. In general, FactorVAE learns representations
that agree better with the ground truth factors than β-VAE and AAE do. This is consistent with
the qualitative results in Fig. 12. However, all models still perform poorly for interpretability since
their RMIG and JEMMIG scores are very far from 1 and 0, respectively. We provide the normalized
JEMMIG and RMIG scores for all attributes in Fig. 13.
	RMIG (normalized)					
	Bangs	Black Hair	Eyeglasses	Goatee	Male	Smiling
	H=0.4256	H=0.5500	H=0.2395	H=0.2365	H=0.6801	H=0.6923
FactorVAE	0.1742	0.0430	0.0409	0.0343	0.0060	0.0962
β-VAE	00176	0.0223	0:0045	0:0325	0.0094	0:0184
AAE	0.0035	0.0276	0.0018	0.0069	0.0060	0.0099
Table 6: Normalized RMIG scores (#bins=100) for some factors. Higher is better.
	JEMMIG (normalized)					
	Bangs	Black Hair	Eyeglasses	Goatee	Male	Smiling
	H=0.4256	H=0.5500	H=0.2395	H=0.2365	H=0.6801	H=0.6923
FactorVAE	0.6118	0.6334	0.6041	0.6616	0.6875	0.6150
β-VAE	0:8632	0.8620	0:8602	0:8600	0:8690	0:8699
AAE -	0.8463	0.8613	0.8423	0.8496	0.8644	0.8575
Table 7: Normalized JEMMIG scores (#bins=100) for some factors. Lower is better.
Bangs (Ml gap=0.1742, H=0.4256)
z[52] (0.0830)
z[63] (0.0089)
z[46] (0.0047)
z[43] (0.0030)
z[3] (0.0024)
(a) FactorVAE (TC=50)
(b) β-VAE (β=50)
(c) AAE (Gz=50)
Figure 12: Top 5 representations that are most correlated with some ground truth factors. For each
representation, we show its mutual information with the ground truth factor.
A.4.2 dSprites
Informativeness From Fig. 14, we see that 5 representations of AAE have equally high informa-
tiveness scores while the remaining 5 representations have nearly zeros informativeness scores. This
is because AAE needs only 5 representations to capture all information in the data. FactorVAE also
16
Published as a conference paper at ICLR 2020
⑶ JEMMIG (normalized)
Figure 13: Normalized JEMMIG and RMIG scores for all attributes in the CelebA dataset. We sorted
the JEMMIG and RMIG scores of the FactorVAE in ascending and descending orders, respectively.
needs only 5 representations but some are less informative than those of AAE. Note that the number
of ground truth factors of variation in dSprites dataset is also 5.
Figure 14: Normalized informativeness scores (bins=100) of all latent variables sorted in descending
order.
0
5
0	5	0	5	0	5
■i KIIR
(a) FactorVAE	(b) β-VAE	(c) AAE
Figure 15: Normalized MISJED scores (bins=100) of all latent pairs sorted by their informativeness.
Separability and Independence Fig. 15 shows heat maps of MISJED scores for the three models.
Interpretability From Tables. 8 and 9, we see that FactorVAE is very good at disentangling “scale”,
“x-position” and “y-position” but fails to disentangling “shape” and “rotation”. However, FactorVAE
still performs much better than β-VAE and AAE. These results are consistent with the visual results
in Fig. 16.
17
Published as a conference paper at ICLR 2020
Also note that in FactorVAE, the RMIG scores for “scale” and “x-position” are quite similar but
the JEMMIG score for “scale” is higher than that for “x-position”. This is because the quantized
distribution (with 100 bins) of a particular representation zi fits better to the distribution of “x-position”
(having 32 possible values) than to the distribution of “scale” (having only 6 possible values).
	Shape	Scale	Rotation	Pos X	PosY
FactorVAE	0.2412	0.7139	0.0523	0.7198	0.7256
β-VAE	0.0481	0.1533	0.0000	0.4127	0.4193
AAE -	0.0053	0.0786	0.0098	0.3932	0.4509
Table 8: Normalized RMIG scores (bins=100).
	Shape	Scale	Rotation	Pos X	PosY
FactorVAE	0.6841	0.3422	0.7204	0.2908	0.2727
β-VAE	0.8642	0.8087	0.9199	0.5629	0.5576
AAE	0.8426	0.8143	0.8665	0.5738	0.5258
Table 9: Normalized JEMMIG scores (bins=100).
Shape (RMIG=0.2412f JEMMI=0.6841, H=1.0986)
(a) FactorVAE (Shape)
(b) β-VAE (Shape)
(c) AAE (Shape)
Scale (RMIG=0.7139, JEMMI=O.3422, H = 1.7918)
(d) FactorVAE (Scale)
(e) β-VAE (Scale)
Scale (RMIG=0.0786f JEMMI=O.8143, H = 1.7918)
(f) AAE (Scale)
Rotation (RMIG=0.0523f JEMMI=0.7204, H=3.6889)
(g) FactorVAE (Rotation)
(h) β-VAE (Rotation)
(i) AAE (Rotation)
Pos_x (RMIG=0.7198, JEMMI=O.2908, H=3.4657)
(j) FactorVAE (Pos X)
(k) β-VAE (Pos X)
(l) AAE (Pos X)
(n) β-VAE (Pos Y)
(m) FactorVAE (Pos Y)
(o) AAE (Pos Y)
Figure 16: Top 3 representations sorted by their mutual information with different ground truth
factors.
A.5 Ablation study of our metrics
Sensitivity of the number of bins When estimating entropy and mutual information terms using
quantization, we need to specify the value range and the number of bins (#bins) in advance. In this
paper, we fix the value range to be [-4, 4] since most latent values fall within this range. We only
investigate the effect of #bins on the RMIG and JEMMIG scores for different models and show the
results in Fig. 17 (left, middle).
18
Published as a conference paper at ICLR 2020
We can see that when #bins is small, RMIG scores are low. This is because the quantized distributions
Q(zi*) and Q(ZjO) look similar, causing I*(zi*,yk) and I。(2尸,yk) to be similar as well. When
#bins is large, the quantized distribution Q(zi*) and Q(Z尸)look more different, leading to higher
RMIG scores. RMIG scores are stable when #bins > 200, which suggests that finer quantizations do
not affect the estimation of I(Zi, yk) much.
Unlike RMIG scores, JEMMIG scores keep increasing when we increase #bins. Note that JEMMIG
only differs from RMIG in the appearance of H(Zi* , yk). Finer quantizations of Zi* introduce more
information about Zi* , hence, always lead to higher H (Zi* , yk) (see Fig. 17 (right)). Larger JEMMIG
scores also reflect the fact that finer quantizations of Zi* make Zi* look more continuous, thus, less
interpretable w.r.t the discrete factor yk.
We provide a detailed explanation about the behaviors of RMIG and JEMMIG w.r.t #bins in Ap-
pdx. A.11. Despite the fact that #bins affects the RMIG and JEMMIG scores of a single model, the
relative order among different models remains the same. It suggests that once we fixed the #bins, we
can use RMIG and JEMMIG scores to rank different models.
Figure 17:	Dependences OfRMIG (normalized), JEMMIG (normalized) and 志 PK=-01 H(zi*,yk)
on the number of bins. The dataset is dSprite.
Sensitivity of the number of samples From Fig. 18 (left, right), it is clear that the sampling
estimation is unbiased and is not affected much by the number of samples.
Sensitivity of sampling in high dimensional space One thing that we should concern about is the
performance of our metrics when the number of latent representations (#latents) is large (or Z is
high-dimensional). In Fig. 19a, we see that the informativeness of an individual representations Zi
is not affected by #latents. When we increase #latents, additional representations are usually noisy
(I(Zi, x) ≈ 0). The total amount of information captured by the model (I(x, Z)), by contrast, highly
depends on #latents (Fig. 19b). Unusually, increasing #latents reduces I(x, Z) instead of increasing
it. We have not found the final answer for this phenomenon but possible hypotheses are: i) On a
high dimensional space where most latent representations are noisy (e.g. #latents=20), q(Z|x) may
look more similar to q(z), causing the wrong calculation of log q(zZX), or ii) when #latents is large,
q(Z|x) = QiL=-01 q(Zi |x) is very tiny, thus, may lead to floating point imprecision7. In Fig. 19c, we
7We tried q(z|x) = exp PiL=-01 log q(zi|x) and it gives similar results as q(z|x) = QiL=-01 q(zi|x).
3.0
2.5
10000 20000 30000 40000 50000
#samples
Figure 18:	Dependences of JEMMIG and WSEPIN on the number of samples. All models have 10
latent variables. The dataset is dSprites.
19
Published as a conference paper at ICLR 2020
see that increasing #latents increases I(zi , z6=i). This makes sense because larger #latents means
that z6=i will contain more information. However, the change of I(zi , z6=i ) is sudden when #latents
change from 10 to 15, which is different from the change of #latents from 5 to 10 or 15 to 20. Recall
that I(zi , z6=i) = H(zi) + H(z6=i) - H(z). Since H(zi) can be computed stably, we only plot
H(z6=i) - H(z) and show it in Fig. 19d. We can see that when #latents = 20, H(z6=i) ≈ H(z) which
means we cannot differentiate between q(z6=i) and q(z). The instability of computation for high
dimensional latents becomes clearer in Fig. 19e as I(x, zi|z6=i) = I(x, z) - I(x, z6=i) can be < 0
when #latents = 15 or 20. This causes the instability of WSEPIN in Fig. 19f despite the results look
reasonable. JEMMIG and RMIG are calculated on individual latents so they are not affected by
#latents and can provide consistent evaluations for models with different #latents.
3.0
2.5
2.0
≡1.5
气。
0.5
0.0
5	10	15	20
#latents
(a) I(x, zi)
3
αulz×=∙(Zx-
5	10	15	20
#latents
(b) I(x, z)
5	10	15	20
#latents
1.50
1.25
o 1.00
N1 0.75
U 0.50
0.25
0.00
0.00
-0.25
-0.50
-0.75
-1.00
-1.25
-1.50
5	10	15	20
#latents
5	10	15	20
#latents
(c) I(zi , z6=i)	(d) H(z6=i) - H(z)
nξwsm
s⅛ω-≡≡ω--
5	10	15	20
#latents
(g) JEMMIG(yk)
s2==M
5	10	15	20
# Iatents
(h) JEMMIG
10	15	20
#latents
(e) I(x, z) - I(x, z6=i)	(f) WSEPIN
，(_u zɪ
Figure 19:	Dependences of various quantitative measures on the number of latents. All measures are
computed via sampling. The model used in this experiment is β-VAE with β = 10.
A.6 Evaluating independence with correlation matrix
For every χ(n) sampled from the training data, We generated m = 1 latent samples z(n,m) 〜
q(zi|x(n)) and built a correlation matrix from these samples for each of the models FactorVAE,
β-VAE and AAE. We also built another version of the correlation matrix Which is based on the
Eq(zi |x(n)) [zi] (called the conditional means) instead of samples from q(zi|x(n)). Both are shoWn in
Fig. 20. We can see that the correlation matrices computed based on the conditional means incorrectly
describe the independence betWeen representations of FactorVAE and β-VAE. AAE is not affected
because it learns deterministic zi given x. Using the correlation matrix is not a principled Way to
evaluate independence in disentanglement learning.
20
Published as a conference paper at ICLR 2020
(d) FactorVAE (deterministic)
(e) β-VAE (deterministic)
(f) AAE (deterministic)
Figure 20: Correlation matrix of representations learned by FactorVAE, β-VAE and AAE.
A.7 Trade-off between informativeness, independence and the number of
LATENT VARIABLES
Before starting our discussion, we provide the following fact:
Fact 2. Assume we try to fill a fixed-size pool with fixed-size balls given that all the balls must be
inside the pool. The only way to increase the number of the balls without making them overlapped is
reducing their size.
AAE
FactorVAE
Figure 21: Illustration of representations learned by AAE and FactorVAE. A big red circle represents
the total amount of information that x contains orH(x) which is limited by the amount of training data.
Blue circles are informative representations zf and the size of these circle indicates the informativeness
of zf. Green circles are noisy representations zn . AAE does not contain zn, only FactorVAE does.
In the context of representation learning, a pool is x with size H(x) which depends on the training
data. Balls are zi with size H(zi). Fact. 2 reflects the situation of AAE (see Fig. 21 left). In AAE,
all zi are deterministic given x so the condition “all balls are inside the pool” is met. H(zi) ≈
the entropy of N(0, I) which is fixed so the condition “fixed-size balls” is also met. Therefore,
when the number of latent variables in AAE increases, all zi must be less informative (i.e., H (zi)
must decrease) given that the independent constraint on the latent variables is still satisfied. This
is empirically verified in Fig. 22 as We See the distribution of Eq(Zi ∣x(n))[zi] over all x(n) 〜PD (x)
becomes narrower when we increase the number of representations from 65 to 200. Also note that
increasing the number of latent variable from 65 to 100 does not change the distribution. This
suggests that 65 or 100 latent variables are still not enough to capture all information in the data.
FactorVAE, hoWever, handles the increasing number of latent variables in a different Way. Thanks
to the KL term in the loss function that forces q(zi|x) to be stochastic, FactorVAE can break the
21
Published as a conference paper at ICLR 2020
!«<»•
12W9
IWW
xx»
β<xw∙
«<»
O
(a) z_dim=65
(b) z_dim=100	(c) z_dim=200
Figure 22: Distribution of Eq(Zi∣χ(n)) [zi] over all x(n)〜PD(x) of a particular representation Zi for
different AAE models.
constraint in Fact 2 and allows the balls to stay outside the pool (see Fig. 21 right). If we increase
the number of latent variables but still enforce the independence constraint on them, FactorVAE will
keep a fixed number of informative representations and make all other representations “noisy” with
zero informativeness scores. We refer to that capability of FactorVAE as code compression.
A.8 Analysis of existing metrics for disentanglement
In this section, we analyze recent metrics, including Z-diff score (Higgins et al., 2017a; Kim & Mnih,
2018), Separated Attribute Predictability (SAP) (Kumar et al., 2017), Mutual Information Gap (MIG)
(Chen et al., 2018), Disentanglement/Compactness/Informativeness (Eastwood & Williams, 2018),
Modularity/Explicitness (Ridgeway & Mozer, 2018).
The main idea behind the Z-diff score (Higgins et al., 2017a; Kim & Mnih, 2018) is that if a
ground truth generative factor yk (k ∈ {0, 1, ..., K}) is well aligned with a particular disentangled
representation zi (although we do not know which i), we can use a simple classifier to predict k
using information from z. Higgins et al. (Higgins et al., 2017a) use a linear classifier while Kim et.
al. (Kim & Mnih, 2018) use a majority-vote classifier. The main drawback of this metric is that it
assumes knowledge about all ground truth factors that generate the data. Hence, it is only applicable
for a toy dataset like dSprites. Another drawback lies in the complex procedure to compute the
metric, which requires training a classifier. Since the classifier is sensitive to the chosen optimizer,
hyper-parameters and weight initialization, it is hard to ensure a fair comparison.
The SAP score (Kumar et al., 2017) is computed based on the correlation matrix C between the
latent variables z and the ground truth factors y. If a latent zi and a factor yk are both continuous,
the (square) correlation C%# between them is equal to VCozk)Vayy)) and is in [0, 1]. However, if
the factor yk is discrete, computing the correlation between continuous and discrete variables is not
straightforward. The authors handled this problem by learning a classifier that predicts yk given zi
and used the balanced8 prediction accuracy as a replacement. Then, for each factor yk, they sorted
C:,k in the descending order and computed the difference between the top two scores. The mean of
the difference scores for all factors was used as the final SAP score. The intuition for this metric is
that if a latent zi is the most representative for a factor yk (due to the highest correlation score), then
other latent variables z6=i should not be related to yk, and thus, the difference score for yk should
be high. We believe the SAP score is more sensible than Z-diff but it is only suitable when both the
ground truth factors and the latent variables are continuous as no classifier is required. Moreover, if
we have K discrete ground truth factors and L latent variables, the number of classifiers we need to
learn is L × K, which is unmanageable when L is large.
The MIG score (Chen et al., 2018) shares the same intuition as the SAP score but is computed based
on the mutual information between every pair of zi and yk instead of the correlation coefficient. Thus,
the MIG score is theoretically more appealing than the SAP score since it can capture nonlinear
relationships between latent variables and factors while the SAP score cannot. The MIG score, to
some extent, reflects the concept “interpretability” that we discussed in Section 2 in the main text.
8To achieve balance, the classifier uses the same number of samples for all categories of yk during training
and testing
22
Published as a conference paper at ICLR 2020
Eastwood et. al. (Eastwood & Williams, 2018) proposed three different metrics namely “disentan-
glement”, “completeness”, and “informativeness” to quantify disentangled representations. These
metrics are computed based on a so-called “important matrix” R whose element Rik is the relative im-
portance ofzi (w.r.t other z6=i) in predicting yk. More concretely, for each factor yk (k = 0, ..., K- 1),
they train a regressor (LASSO or Random Forest) to predict yk from z and use the weight vector
provided by this regressor to define R∙k. The “disentanglement" score Di quantifies the degree to
which a latent Zi captures at most one generative factor yk. Di is computed as Di = (1 - HK(Pi.))
where HK (Pi) = PKoI -Pik log Pik and Pik = K—ik — which can be seen as the “proba-
k0=0 Rik0
bility” of predicting yk instead of y6=k from zi . Similarly, the “completeness” score Ck quantifies
the degree to which a ground truth factor yk is captured by a single latent zi (i = 0, ..., L - 1),
computed as Ck = 1 - HL(Pk) where HL(Pk) = P匕O -Pik log Pik and Pik = PLRikR . The
=	i0=0 Ri0k
“informativeness” score describes the total amount of information of a particular factor yk captured
by all representations z. However, the authors use the prediction error Ek of the k-th regressor to
quantify “informativeness” instead of I(yk , z). Despite being well-motivated, these metrics still have
several drawbacks. First, using three different metrics to quantify disentangled representations is
not as convenient as using a single metric like MIG (Chen et al., 2018). For example, how can we
compare two models A and B if A has a better “disentanglement” score but a worse “completeness”
score than B? Second, these metrics do not apply for categorical factors with C classes since in this
case the model weight is not a vector but an L × C matrix. Third, defining the pseudo-distribution
Pik =	K⅜ - seems ad hoc because i) the weight magnitudes Rik are unbounded and can vary
k0=0 Rik0
significantly (see Appdx. A.9), and ii) Pik strongly depends on the available ground truth factors (e.g.
the value of Pik will change if we only consider 2 instead of 5 factors).
Ridgeway et. al. (Ridgeway & Mozer, 2018) proposed two metrics called “modularity” and
“explicitness” that have similar interpretations as “disentanglement” and “informativeness” dis-
cussed above but differ in implementation. Specifically, they compute the “modularity” score
Mi for a representation Zi as Mi = 1 - Pk=O (I(Zi常"Ti? where k* = argmaxk I(zi, yk) and
i (zi,yk* )×(K-1)	Jz e 、 ，一 ,
Tik
I(Zi,yk*)
0
if k = k
otherwise
Like the “disentanglement” score Di , Mi is also ad hoc and is
undefined when the number of ground truth factors is 1. The “explicitness” score Ek for each ground
truth factor yk is computed as the ROC curve of a logistic classifier that predicts yk from Z. It turns
out that Ek is just a way to bypass computing I(yk, Z).
A.9 THE MUTUAL INFORMATION MATRIX I(Zi, yk) AND THE IMPORTANCE MATRIX Rik
In Fig. 23, we compare our mutual information matrix I(Zi, yk) with the counterpart in (Ridgeway
& Mozer, 2018) and the importance matrix Rik in (Eastwood & Williams, 2018). It is clear that
all matrices can capture disentangled representations (those highlighted in red) well since their
corresponding values are high compared to other values in the same column. However, the matrix
I(Zi, yk) in (Ridgeway & Mozer, 2018) usually overestimates noisy representations since it uses
Eq(zi|x)[Zi] instead of q(Zi|x). The matrix Rik in (Eastwood & Williams, 2018) sometimes assign
very high absolute values for noisy representations since the regressor’s weights are unbounded. These
flaws make the metrics in (Ridgeway & Mozer, 2018) and in (Eastwood & Williams, 2018) inaccurate
and unstable, especially “modularity” and “disentanglement” since they require normalization over
rows.
A.10 Computing metrics for informativeness, separability and interpretability
The metrics for informativeness, separability and interpretability in Section. 3 requires computing
H(Zi), H(Zi|x), H(Z6=i), H(Z), and H(Zi, yk). We can compute these entropies via quantization
or sampling. Quantization is only applicable when Zi is a scalar. If Zi is a high-dimensional vector,
we need to use sampling. Below, we describe how to compute H(Z) via sampling and H(Zi) via
quantization. Other cases can be derived similarly.
23
Published as a conference paper at ICLR 2020
[[0.6368	0.0056 ∣1.8789∣ 0	0Θ55 0.0056]
[Θ.ΘΘΘ1	Θ.ΘΘΘ2 0.	O	0ΘΘ3 O .	]
[Θ.	0.0001 0.	θ	0.00Θ3]
[Θ.	Θ.ΘΘ02 0.00Θ1 Θ	0ΘΘ2 Θ.ΘΘΘ1]
[Θ.	0.0001 θ.00Θ3 0	Θ.00Θ1]
[0.0918	0.0412 0.0377 ∣2	7327∣ 0.0253]
[Θ,	O.QQQ1 Q.	O	0004 O.ΘΘΘ1]
[0.2654	∣1.Θ612∣ 0.04Θ4 θ	0148 Θ.Θ124]
[Θ.Θ776	0.0162 0.0375 Θ	0578 ∣2.657∣ ]
[0.2371	0.05	0.1256 θ	0Θ41 0.0023]]
[[θ.639 Θ.ΘΘ56 ∣1.9237∣ Θ.ΘΘ57 Θ.ΘΘ53]
∏0.2008 0.Θ377 0.1189 Θ.1497 Θ.Θ243Π
[0.1254 0.0946 0.038 Θ.0262 0.2099∣
[0.146 0.0734 0.0753 Θ.0591 0.154 ∣
Co.2576 0.0712 G.1421 026 0.0419^
[θ.072 θ.0357 0.0315 [2.714# θ.0211]
[0.1011 G.0537 θ.1408 0.193 0.Θ164]
[0.2833 ∣1.Θ856∣ 0.Θ649 Θ.0197 Θ.Θ19 ]
[0.0597 0.013 Θ.0328 Θ.044 ∣2.6578∣]
[0.2991 0.0614 0.1647 Θ.0075 0.ΘΘ44]]
(a) I(zi,yk) w. q(zi |x)
(b) I(zi,yk) w.o. q(zi |x)
[[0.0942 ∣Θ.64Θ2∣ 0.0351 0.0109]
[0.	∣T.6432∣ 0.	0.	]
[θ.	3.9ΘΘ4 θ.	θ.	]
[1.353 ∣6G.784∣ θ. 0.	]
[O. ∣53.685%∣ 1.3094 0.	]
[0.0Θ58 Θ.55Θ9 ∣8.8005∣ 0.054 ]
[O.	O. θ. ]
[∣1.5829∣ 0.4147 Θ,1Θ26 0.0272J
[0.0Θ39 1.1383 0.0004 ∣9.5193∣∣
[0.5873 1.1907 0.1657 θ.0747]]
(c) Rik
Figure 23: (a): Our mutual information matrix I(zi, yk), (b): The mutual information matrix
I(zi, yk) in (Ridgeway & Mozer, 2018), (c): The importance matrix Rik in (Eastwood & Williams,
2018). In (a) and (b), the columns corresponding to the following ground truth factors: “shape”,
“scale”, “rotation”, “x-position”, “y-position”. In (c), the column for “shape” is excluded because the
metrics in (Eastwood & Williams, 2018) do not support categorical factors. Values corresponding to
disentangled representations are highlighted in red. Defective values are highlighted in green. The
model is FactorVAE with TC=20.
(13)
(14)
Computing H(z) via sampling
H(z) = -Eq(z) [log q(z)]
= -Eq(z,x) log EpD (x) [q(z|x)]
MN
=-M X log N X q (z(m) lx(n))
m=1	n=1
=- M X "logN X (Yq (z"x(n)))
m=1	n=1 i=1
In Eq. 13, we use Monte Carlo sampling to estimate the expectations outside and inside the log
function. The corresponding sample sizes are M and N. In Eq. 14, we use the assumption
q (z(m) |x(n)) = QL=I q (z(m) |x(n)). Please note that the entropy H(Z) computed via sampling can
be negative if z is continuous since we use the density function q(z|x).
Computing H(zi) via quantization We can compute H(zi) via quantization as follows:
H(zi) = - X Q(si) log Q(s)
si∈S
where S is a set of all quantized bins si corresponding to zi; Q(si) is the probability mass function
of si . To ensure consistency among different zi as well as different models, we apply the same value
range for all latent variables. In practice, we choose the range [-4, 4] since most of the latent values
fall within this range. We divide this range into equal-size bins to form S .
We can compute Q(si) as follows:
N
n=1
We compute Q si |x(n) based on its definition, which is:
b
Q(si|x(n)) =	q(zi |x(n)) dzi
a
where a, b are two ends of the bin si .
(15)
There are two ways to compute Q(si|x(n)). In the first way, we simply consider the unnormalized
Q0(si|x(n)) as the area of a rectangle whose width is b 一 a and height is q(Zi|x(n)) with Zi at the
center value of the bin si. Then, we normalize Q0 (si|x(n)) over all bins to get Q(si|x(n)). In
the second way, if q(zi |x(n)) is a Gaussian distribution, we can estimate the above integral with a
closed-form function (see Appdx. A.14 for detail).
24
Published as a conference paper at ICLR 2020
0	1	2	3	0.0	0.5	1.0	1.5
JEMMIG (sampling)	RMIG (sampling)
(a)	(b)
Figure 24: Correlation between the sampling (#samples=10000) and quantized (value range=[-4, 4],
#bins=100) estimations of JEMMIG/RMIG. In the subplot (a), the red line is y = x - log(bin width)
while in the subplot (b), the red line is y = x. Blues denotes FactorVAE models and oranges denotes
β-VAE models. The dataset is dSprites.
A.11 Relationship between sampling and quantization
Denote Hs(zi|x) and Hq(zi|x) to be the sampling and quantization estimations ofan entropy H(zi|x),
respectively. Because Hs(zi|x) is the expectation of log q(zi |x), Hq(zi|x) is the expectation of
log Q(zi|x), and Q(zi|x) ≈ q(zi|x) × bin width if the bin width is small enough, there exists a gap
between Hs(zi|x) and Hq(zi|x), specified as follows:
Hq(zi|x) = Hs(zi|x) - log(bin width)
= Hs(Zi⑶-log (”)
= Hs(zi|x) - log (value range) + log (#bins)
Since Q(zi) = EpD(x) [Q(zi|x)] and q(zi) = EpD(x) [q(zi|x)], we have Q(zi) ≈ q(zi) × bin width.
Thus, Hq (zi ) and Hs(zi) also exhibit a similar gap as Hs(zi|x) and Hq(zi|x):
Hq(zi) = Hs (zi) - log(bin width)
However, this gap disappears when computing the mutual informationI (zi, x) since:
Iq (zi , x) = Hq (zi ) - Hq(zi |x)
= (Hs(zi) - log(bin width)) - (Hs(zi|x) - log(bin width))
= Hs(zi) - Hs(zi|x)
= Is (zi , x)
In fact, one can easily prove that:
lim	Iq(zi , x) = Is (zi , x)
⅛^biπs→+∞
.
Similar relatioπships betweeπ sampliπg aπd quaπtizatioπ also apply for H(zi, yk) aπd I(zi, yk). They
are clearly showπ iπ Fig. 24.
Iπ summary,
•	Sampliπg eπtropies such as Hs(zi|x) or Hs(zi) are usually fixed but can be negative siπce
q(zi|x) or q(zi) caπ be > 1. However, these eπtropies can still be used for ranking though
it is πot easy to iπterpret them.
•	Quaπtized eπtropies such as Hq(zi|x) or Hq(zi) caπ be positive if the biπ width is small
enough (or #bins is large enough). The growth rate is - log(biπ width) (or log(#bin)).
Because limx→+∞ log x = +∞, Hq (zi |x) aπd Hq(zi) cannot be upper-bounded.
25
Published as a conference paper at ICLR 2020
• The mutual information I(zi, x) is consistent via either quantization or sampling. Unlike the
entropies, I(zi, x) is well-bounded even when zi is continuous, thus, is suitable to be used
in a metric. However, when #bins is small, the approximation Q(zi) ≈ q(zi) × bin width
does not hold and quantization estimation can be inaccurate.
A.12 NORMALIZING JEMMIG
Recall that the formula of the unnormalized JEMMIG(yk) is H (zi* ,yk) - I (zi* ,yk) + I (zj。,yk).
If We estimate H(zi*, yk) via quantization, the value of the unnormalized JEMMIG(yk) will vary
according to the bin width (or value range and #bins) (as shown in Fig. 17 (left)). However, we can
still rank models by forcing them using the same bin width (or the same value range and #bins).
To avoid setting these hyper-parameters, we can estimate H(zi* , y) via sampling. In this case,
the value of the unnormalized JEMMIG(yk) only depends on q(zi |y) which is fixed after learning.
Ranking disentanglement models using the unnormalized JEMMIG(yk) is somewhat similar to
ranking generative models using the log-likelihood.
Using the unnormalized JEMMIG(yk) causes interpretation difficulty. We could normalize
JEMMIG(yk) as follows:
Hq(Zi) + H(yk ) - 2I(Zi* , yk ) + I(ZjO , yk)
Hq(u) + H(yk)
(16)
where Hq(Zi) is a quantization estimation of H(Zi), hence, greater than 0; Hq(u) is an entropy
that bounds Hq(Zi) but does not depend on q(Zi|x). Intuitively, u should be uniform. The main
problem is how to find an effective value range [a, b] of Zi that satisfies 2 conditions: i) most of
the mass of Zi falls within that range, and ii) H(u) is the bound of H(Zi) if u ∈ [a, b]. However,
before solving this question, we try to answer a similar yet easier question: “Given a Gaussian
random variable Z 〜N(μ, σ), what is the value range of a uniform random variable U such that
H(u) ≥ H(Z)?”. Assume u ∈ [a, b], the entropy of u is H(u) = log(b - a) while the entropy of Z
is H(Z) = 0.5 log(2πeσ2). We have:
H(Z) ≤ H(u)
⇔0.5 log(2πeσ2) ≤ log(b - a)
⇔σ√2πe ≤ b 一 a
Thus, to ensure H(u) to be an upper bound of H(Z), we should choose the value range of u to be at
least σ√2πe. If σ = 1, this range is about 4.1327. If we also want [a, b] to capture most of the mass
of z, a should be μ — σ √2πe and b should be μ + σ √2πe.
Come back to the main problem, since q(Zi) = EpD(x) [q(Zi |x)] and q(Zi|x) is usually a Gaussian
distribution N(μ%, σ∕we can choose a, b as follows:
One may wonder that different methods can choose different value ranges [a, b] to normalize JEMMIG
so how to ensure a fair comparison among them using the normalized JEMMIG. A simple solution is
using the same value range [a, b] for different models. In this case, b - a should be large enough to
cover various distributions. We can write Eq. 16 as follows:
26
Published as a conference paper at ICLR 2020
Figure 25: Left: Correlation between our RMIG (#bins=100) and the original MIG (Chen et al., 2018)
(#samples=10000). Right: Correlation between our RMIG (#bins=100) and the implementation of
MIG in (Locatello et al., 2019) (#bins=100). Experiments are conducted on the dSprites dataset.
Hq(Zi) + H(yk ) - 2I(Zi* ,yk ) + I(Zj。, yk )
Hq(U) + H (yk )
Hs(Zi) - log(valuerange) + log(#bins) + H®-2I(zi*Bk) + I(zj。,y)
Hs(U) - log(value range) + log(#bins) + H(yk)
HS(Zi) - log(valuerange) + log(#bins) + H(yk - 2I(zi*,yk) + I(zj『yk)
log(#bins) + H(y)
(17)
Since the fraction in Eq. 17 is smaller than 1, increasing #bins will increase this fraction but still
ensure that it is smaller than 1. This means the normalized JEMMIG is always in [0, 1] despite #bins.
A.13 Comparing RMIG with other MIG implementations
RMIG has several advantages compared to the original MIG (Chen et al., 2018) which we refer as
MIG1: i) RMIG works on real datasets, MIG1 does not; ii) RMIG supports continuous factors, MIG1
does not. On toy datasets such as dSprites, RMIG produces almost the same results as MIG1 (Fig. 25
(left)). We argue that the small differences between RMIG and MIG1 scores in some models are
caused by either the quantization error of RMIG (when #bins=100) or the sampling error of MIG1
(when #samples=10000).
Locatello et. al. (Locatello et al., 2019) provided an implementation9 of MIG which we refer as MIG2.
MIG2 is theoretically incorrect in two points: i) it only uses the mean of the distribution q(Zi|x(n))
instead of the whole distribution q(Zi|x(n)), and ii) the bin range and width varies for different Zi.
The performance of MIG2 is, thus, unstable. We can easily see this problem by comparing the
right plot with the left plot in Fig. 25. MIG2 usually overestimates the true MIG1 when evaluating
β-VAE models with a large β (e.g. β ∈ {20, 30, 50}). We guess the reason is that in these models,
q(Zi |x(n)) usually has high variance, hence, using the mean of q(Zi|x(n)) like MIG2 leads to the
wrong estimation of I(Zi, yk).
A.14 Definite integral of a Gaussian density function
Assume that We have a Gaussian distribution N(μ, σ). The definite integral of its density function
within the range [a, b] denoted as G(a, b) can be computed as follows:
G(a, b)
9https://github.com/google-research/disentanglement_lib
27
Published as a conference paper at ICLR 2020
Although erf(∙) does not have analytical form, We can compute its values with high precision using
polynomial approximation. For example, the following approximation provides a maximum error of
5 × 10-4 (Def, 2019):
1
erf(x) ≈1 -访蒜+*+QEf，x> 0
where a1 = 0.278393, a2 = 0.230389, a3 = 0.000972, a4 = 0.078108.
A.15 Representations learned by FactorVAE
We empirically observed that FactorVAE learns the same set of disentangled representations across
different runs with varying numbers of latent variables (see Appdx. A.18). This behavior is akin to
that of deterministic PCA which uncovers a fixed set of linearly independent factors10 (or principal
components). Standard VAE is theoretically similar to probabilistic PCA (pPCA) (Tipping & Bishop,
1999) as both assume the same generative process p(χ, Z) = pθ(χ∣z)p(z). Unlike deterministic PCA,
pPCA learns a rotation-invariant family of factors instead of an identifiable set of factors. However,
in a particular pPCA model, the relative orthogonality among factors is still preserved. This means
that the factors learned by different pPCA models are statistically equivalent. We hypothesize that
by enforcing independence among latent variables, FactorVAE can also learn statistically equivalent
factors (or q(zi|x)) which correspond to visually similar results. We provide a proof sketch for the
hypothesis in Appdx. A.16. We note that Rolinek et. al. (Rolinek et al., 2018) also discovered the
same phenomenon in β-VAE.
A.16 Why FactorVAE can learn consistent representations ?
Inspired by the variational information bottleneck theory (Alemi et al., 2016), we rewrite the standard
VAE objective in an equivalent form as follows:
min I(x, z) s.t. Rec(x) ≤ β	(18)
q(z|x)
where Rec(x) denotes the reconstruction loss over x and β is a scalar.
In the case of FactorVAE, since all latent representations are independent, we can decompose I(x, z)
into Pi I(x, zi). Thus, we argue that FactorVAE optimizes the following information bottleneck
objective:
min X I(x, zi) s.t. Rec(x) ≤ β	(19)
q(z|x)
i
We assume that Rec(x) represents a fixed condition on all qi(z|x). Because I(x, zi) is a convex
function of q(zi|x) (see Appdx. A.17), minimizing Eq. 19 leads to unique solutions for all q(zi |x)
(Note that we do not count permutation invariance among zi here).
To make Rec(x) a fixed condition on all qi(z|x), we can further optimize p(x|z) with z sampled
from a fixed distribution like N(0, I). This suggests that we can add a GAN objective to the original
FactorVAE objective to achieve more consistent representations.
A.17 I(x, z) IS A CONVEX FUNCTION OF p(z|x)
Let us first start with the definition of a convex function and some of its known properties.
Definition 3. Let X be a set in the real vector space RD and f : X → R be a function that output a
scalar. f is convex if∀x1, x2 ∈ X and ∀λ ∈ [0, 1], we have:
f(λx1 + (1 - λ)x2) ≤ λf(x1) + (1 - λ)f(x2)
Proposition 4. A twice differentiable function f is convex on an interval if and only its second
derivative is non-negative there.
10When we mention factors in this context, they are not really factors of variation. They refer to the columns
of the projection matrix W in case of PCA and the component encoding functions q(zi |x) in case of deep
generative models.
28
Published as a conference paper at ICLR 2020
Proposition 5 (Jensen’s inequality). Let x1, ..., xn be real numbers and let a1, ..., an be positive
weights on x1, ..., xn such that in ai = 1. If f is a convex function on the domain of x1, ..., xn, then
f	X aixi
n
≤	aif(xi)
i=1
Equality holds if and only if all xi are equal or f is a linear function.
Proposition 6 (Log-sum inequality). Let a1, ..., an and b1, ..., bn be non-negative numbers. Denote
a =	in=1 ai and b =	in=1 bi. We have:
n
X ai log τ≥ ≥ a log τ
i=1	bi	b
With equality ifand only if ai are equal for all i.
Armed with the definition and propositions, we can now prove that I(x, z) is a convex function of
p(z∣x). Letpι(z∣x) andp2(z∣x) be two distributions and letp?(z|x) = λpι(z∣x) + (1 — λ)p2(z∣x)
with λ ∈ [0, 1]. p?(z|x) is a valid distribution since p?(z|x) > 0 ∀z and x z p?(z|x)p(x) dz dx =
1. In addition, we have:
p?(z) =	p?(z|x)p(x) dx
x
=	/ (λpι(z∣x) + (1 — λ)p2(z∣x))p(x) dx
x
= λ	p1 (z|x)p(x) dx + (1 — λ)	p2(z|x)p(x) dx
xx
= λp1(z) + (1 — λ)p2(z)
We write I(x, z) = λI1(x, z) + (1 — λ)I2(x, z) as follows:
I(x,z) =λ/p(x) ∕pι(z∣x)log P17: dz dx+
+ (1 — λ) /p(x) ∕p2(z∣x)log Pp(ZzX) dz dx
=/加X) / (χp 1(z∣x)lo. λp1(ZIX) +(1 - λ‰(z∣x)log(1 - λ)p2(zlx)
= Jxp(X) JzcpI(Z|x)log λpι(z) +(I	λ)p2(ZIX) log(ι-λ)p2(z∣x)
Zp?(z|x)
p(x) J p?(z|x) log-dz dx
=I? (x, Z )
where the inequality in Eq. 20 is the log-sum inequality. This completes the proof.
dz dx
(20)
A.18 Experiments to show that FactorVAE learns consistent representations
We first trained several FactorVAE models with 3 latent variables on the CelebA dataset. After
training, for each model, we performed 2D interpolation on every pair of latent variables Zi , Zj
(i ≤ j ) and decoded the interpolated latent representations back to images for visualization. We
found that the learned representations from these models share visually similar patterns, which is
illustrated in Fig. 26. It is apparent that all images in Fig. 26 are derived from a single one (e.g. we
can choose the first image as a reference) by switching the rows and columns and/or flipping the
whole image vertically/horizontally. The reason why switching happens is that all latent variables of
FactorVAE are permutation invariant. Flipping happens due to the symmetry of q(Zi) which is forced
to be similar to p(Zi) = N(0, I).
29
Published as a conference paper at ICLR 2020
(a) TC=50
Figure 26: Random traversal on the latent space of FactorVAE. We can easily see the visual resem-
blance among image regions corresponding the same number.
(b) TC=10
(d) TC=50, z_dim=200
(b) TC=50, z_dim=65 (c) TC=50, z_dim=100
Figure 27:	Top 10 representations sorted by the variance of the distribution of Eq(z |x(n)) [zi] over all
x(n) .
We then repeated the above experiment on FactorVAE models containing 65, 100, 200 latent variables,
but replacing 2D interpolation on pairs of latent variables with conditional 1D interpolation on
individual latent variables to account for large numbers of combinations. We sorted the latent
variables zi of each model according to the variance of the distribution of Eq(zi|x(n)) [zi] over all data
samples x(n) 〜 pD (x) in descending order. Fig. 27 shows results for the top 10 latent variables (of
each model). We can see that some factors of variation are consistently learned by these models, for
example, those that represent changes in color of the image background. Because these factors usually
appear on top, we hypothesize that the learned factors should follow some fixed order. However,
many pronounced factors do not appear at the top, suggesting that the sorting criterion is inadequate.
We then used the informativeness metric defined in Sec. 4.1 to sort the latent variables. Now the
“visual consistency” and “ordering consistency” patterns emerge, (see Fig. 28). We also observed that
the number of learned factors is relatively fixed (around 38-43) for all models despite that the number
of latent variables varies significantly from 65 to 200.
(a) TC=10, z_dim=65
(b) TC=50, z_dim=65
(c) TC=50, z_dim=100
(d) TC=50, z_dim=200
Figure 28:	Top 10 representations sorted by informativeness scores. We can clearly see the consistency
of representations across different runs.
30