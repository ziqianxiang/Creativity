Published as a conference paper at ICLR 2020
Continual	Learning with Bayesian Neural
Networks for Non-Stationary Data
Richard Kurle*12	Botond Cseke1	Alexej Klushyn12
Patrick van der Smagt1	Stephan Gunnemann2
1Volkswagen Group	2Technical University of Munich
Ab stract
This work addresses continual learning for non-stationary data, using Bayesian
neural networks and memory-based online variational Bayes. We represent the
posterior approximation of the network weights by a diagonal Gaussian distribution
and a complementary memory of raw data. This raw data corresponds to likelihood
terms that cannot be well approximated by the Gaussian. We introduce a novel
method for sequentially updating both components of the posterior approximation.
Furthermore, we propose Bayesian forgetting and a Gaussian diffusion process
for adapting to non-stationary data. The experimental results show that our update
method improves on existing approaches for streaming data. Additionally, the
adaptation methods lead to better predictive performance for non-stationary data.
1	Introduction
Continual learning (CL), also referred to as lifelong learning, is typically described informally by the
following set of desiderata for computational systems: the system should (i) learn incrementally from
a data stream, (ii) exhibit information transfer forward and backward in time, (iii) avoid catastrophic
forgetting of previous data, and (iv) adapt to changes in the data distribution (Ring, 1997; Silver
et al., 2013; Chen & Liu, 2016; Ruvolo & Eaton, 2013; Parisi et al., 2018). The necessity to adapt to
non-stationary data is often not reconcilable with the goal of preventing forgetting. This problem is
also known as the stability-plasticity dilemma (Grossberg, 1987).
The majority of current CL research is conducted in the context of online multi-task learning (Nguyen
et al., 2018; Kirkpatrick et al., 2017; Schwarz et al., 2018; Rusu et al., 2016; Fernando et al., 2017),
where the main objective is to prevent catastrophic forgetting of previously learned tasks. This focus
is reasonable since changes in the statistics of the data distribution are usually an artefact of learning
different tasks sequentially. However, changes in the statistics of the data can also be real properties of
the data-generating process. Examples include models of energy demand, climate analysis, financial
market, or user-behavior analytics (Ditzler et al., 2015). In such applications, the statistics of the
current data distribution are of particular interest. Old data may be outdated and can even deteriorate
learning if the drift in the data distribution is neglected. Consequently, CL systems for non-stationary
data require adaptation methods, which deliberately forget outdated information.
In this work, we develop an approximate Bayesian approach for training Bayesian neural networks
(BNN) (Hinton & van Camp, 1993; Graves, 2011; Blundell et al., 2015) incrementally with non-
stationary streaming data. Similar to variational continual learning (VCL) (Nguyen et al., 2018)
and the Virtual Vector Machine (VVM) (Minka et al., 2009), we approximate the posterior using a
Gaussian distribution and a complementary memory of previous data. Both components are updated
sequentially, while adapting to changes in the data distribution. Our main contributions are as follows:
•	We propose an online approximation consisting of a diagonal Gaussian distribution and a
running memory, and we provide a novel sequential update method for both components.
•	We extend the online approximation by two alternative adaptation methods, thereby general-
ising online variational Bayes with Bayesian neural networks to non-stationary data.
We compare our sequential update method to VCL in the online-inference setting on several popular
datasets, demonstrating that our approach is favorable. Furthermore, we validate our adaptation
methods on several datasets with concept drift (Widmer & Kubat, 1996), showing performance
improvements compared to online variational Bayes without adaptation.
* Correspondence to richard.kurle@tum.de
1
Published as a conference paper at ICLR 2020
2	Background: Online Inference
Consider a stream of datasets {Dtk}kK=1, where tk are the time points at which datasets Dtk are
observed. For the moment, we assume that these datasets and the samples within are generated
independently and identically distributed (i.i.d.). Methods for non-i.i.d. data are considered in Sec. 4.
In the Bayesian approach to online learning, we want to infer the posterior distribution p(w|Dt1:tk)
of our model parameters, with the restriction that the data is processed sequentially.1 Using Bayes
rule, a recursive posterior inference equation emerges naturally:
P(WIDtI:tk) X P(WIDtI:tk-1)P(Dtk|w, Dtrtk-J = P(WIDtI:tk-1)P(DtkIW),	(I)
where the last step follows from the i.i.d. assumption of the data.
In this paper, we consider Gaussian and multinomial likelihoods, parametrised by a neural network
with weights W and prior P(WM) = po(w) = N(w; μo,σ0). Furthermore, We consider supervised
leaming, Where Dtk= {dtn) }n = {(χ(n), y(n))}n andP(d(n)|W)=P(y(n) | NN(Xtn); w)).
2.1	Online Variational Bayes
Since exact Bayesian inference is intractable for non-trivial models, various approximations have
been developed. Prominent examples include sequential Monte Carlo (Liu & Chen, 1998), assumed
density filtering (Maybeck, 1982), and online variational Bayes (Opper, 1998; Ghahramani, 2000;
Sato, 2001; Broderick et al., 2013). Online variational Bayes (VB) approximates the posterior of
Eq. (1) by a parametrised distribution qθt (W) ≈ P(WIDt1:tk) through a sequence of projections:
qθtk (W) = argminKLqθ(W) II Zt-k1 qθtk-1 (W) P(Dtk IW),	(2)
qθ
where Ztk is the normalisation constant. The above minimisation is equivalent to maximising the
evidence lower bound (ELBO) Ltk (θ; Dtk) = Eqθ(w) log P(Dtk IW) - KLqθ(W) II qθtk-1 (W). In
this work, we consider diagonal Gaussian posterior approximations qθt (W) for the neural network
weights, similar to Nguyen et al. (2018).
2.2	Online Variational Bayes with Memory
Online approximate Bayesian inference methods inevitably suffer from an information loss due
to the posterior approximation at each time-step. An alternative approach to online learning is to
store and update a representative dataset/generative model—and to use it as a memory—in order to
improve inference (Robins, 1995; Lopez-Paz & Ranzato, 2017; Shin et al., 2017; Kamra et al., 2017).
Memory-based online learning has also been combined with online Bayesian inference methods
(Minka et al., 2009; Nguyen et al., 2018). A common property of these approaches is to represent the
(current) posterior approximation by a product of two factors
P(WIDt1:tk) ≈ qθtk (W) P(Mtk IW)	(3)
and update them sequentially as new data Dtk is observed. The factor P(Mtk IW) = QmM P(mt(km) IW)
is the likelihood of a set of M = IMI data points, which we refer to as running memory; and qθt (W)
is a Gaussian distribution, which summarises the rest of the data Di：tk = Di：tk\Mtk.
In case of VCL, the factors in Eq (3) are updated in two steps, which we refer to as (i) memory update
and (ii) Gaussian update: (i) a new memory Mtk ⊂ Dtk ∪ Mtk-1 is selected using heuristics such
as random selection or the k-center method (a greedy algorithm that selects K data points based on
geometric properties of Dtk ∪ Mtk-1 .); (ii) the Gaussian distribution is updated with the remaining
data Dtk = Dtk ∪ Mtk-ι \ Mtk (using Eq. (2)) to obtain qθtk (w) ≈ Qθtk-ι (w)P(Dtk |w).
Note that we cannot sample directly from the posterior approximation in Eq. (3) and thus we cannot
easily evaluate quantities such as the posterior predictive distribution. VCL therefore performs a
second projection
qθtk (w)
argmin KL 限(w) || Z-Iqθtk (w) P(Mtk |w)].
(4)
This distribution should not be confused with the recursively updated variational distribution (Eq. (2)).
1 A strict definition of online learning requires single data samples at each time step instead of batches Dtk .
2
Published as a conference paper at ICLR 2020
3	Improving Memory-Based Online Variational Bayes
In this section, we focus on two problems of existing approaches using online VB with a running
memory: (i) the memory update does not take into account the approximation error or approximation
capabilities of the variational distribution; (ii) the Gaussian update—performed by optimising the
ELBO(Eq. (2)) only with data DDtk —can fail for streaming data. This is because VB yields poor
posterior approximations if the dataset is too small or the neural network architecture has too much
capacity (cf. Ghosh et al. (2018), Fig. 1). In Secs. 3.2 and 3.3, we propose improvements to these
two update methods. The mathematical background for our approach is provided in Sec. 3.1.
3.1	Properties of the Gaussian Variational Approximation
There are two important properties of the Gaussian variational approximation that we will exploit
later: (i) Gaussian approximate posterior distributions factorise into a product of Gaussian terms
corresponding to the prior and each likelihood term; (ii) the ELBO can be written as the sum of the
approximation’s normalisation constant and a sum of residuals corresponding to these factors.
Letpo(w) = N(w; μo, ∑o) be a Gaussian prior andp(D∣w) = Qnp(d(n)|w) be the likelihood of
the observed data D Furthermore, let q§ (w) = N(w; μ, Σ) denote the corresponding Gaussian vari-
ational approximation with θ = {μ, Σ}. Assume that μ and Σ are the optimal parameters correspond-
ing to a (local) maximum of the ELBO L(μ, Σ; D). The optimality conditions ∂*L(μ, Σ; D) = 0
and ∂∑L(μ, Σ; D) = 0 can be rewritten as follows (Knowles & Minka, 2011; Opper & Archambeau,
2008; Cseke et al., 2013) (cf. App. C):
∑-1μ = ∑-1μo + X (∂μEqθ(w)[logp(dS)|w)] - 2∂∑Eqθ(w)[logp(dS)IW)]μ),	(5a)
Σ-1 = Σ0-1 - 2 X ∂ΣEqθ(w) log p(d(n)|w).	(5b)
n
Since the sum of natural parameters corresponds to a product in distribution space, the above
equations show that—at a local optimum—the approximation qθ(w) factorises in the same way as the
posterior p(w|D). It can be written in the form qθ(w) = Zq-1p0(w) Qn r(n) (w), where the factors
r(n) (w) are Gaussian functions with natural parameters given by Eqs. (5a) and (5b), and where
Zq = p0(w) Qn r(n) (w) dw is the normalisation constant. These Gaussian functions r(n) (w)
each correspond to the contribution of the likelihood p(d(n) |w) to the posterior approximation qθ(w).
The resulting factorisation implies that the ELBO L(μ, Σ; D) can be written in the form (Opper &
Winther, 2005) (c.f. App. D)
L(μ, ∑; D) = log Zq + X Eqθ(w) [ logp(d(n) ∣w) - log r(n) (w)].	(6)
n
If the terms p(d(n) |w) were (diagonal) Gaussian in w, they would each cancel with the corresponding
(diagonal) Gaussian term, leaving only log Zq. Intuitively, the residual terms in Eq. (6) can be used
to quantify the quality of the Gaussian approximation.
3.2	Memory Update
The authors of VCL propose to use a memory to compensate the information loss resulting from the
Gaussian approximation of the posterior distribution. However, their memory update is independent
of the approximation error that is due to the chosen distributional family (diagonal Gaussian). An
alternative memory update, which specifically targets the above mentioned information loss, has
been introduced previously for VVM. Although the latter method was developed for expectation
propagation in a (linear) logistic regression model—and is thus not directly applicable to online VB—
we show that some of its properties can be transferred to the variational inference setting. The central
idea is to replace the likelihood terms that can be well approximated by a Gaussian distribution by their
Gaussian proxies p(dtk |w) ≈ rtk (w; dtk) resulting in qθt (w); and retain the data corresponding
to the rest of the likelihood terms in the memory. To score a candidate memory, Minka et al.
(2009) proposed to maximise the KL divergence between the model given in the form of Eq. (3)
3
Published as a conference paper at ICLR 2020
and a Gaussian posterior approximation, that is, maximise KL [Z-1 qθa (W) p(M∣w) || 丽飙(w)].
However, this score function is intractable, because the expectation in the KL includes the likelihood
p(M|w). In the following, we develop a tractable score function applicable to VB. Intuitively, we
can use Eq. (6) to test how much L(μ, Σ; D) changes if We replace the exact likelihood terms (of all
data which is not contained in the candidate memory) by their Gaussian approximations.
To achieve this, we need to find Gaussian approximations for every data point in the candidate
memory. We first approximate the posterior distribution using both Dtk and Mtk-1 :
qθtk (W) =argminKLhqθ(W) || Z-Iqθtk_1 (W)P(DtkIW)P(Mtk-i|w)].	⑺
Next, we use Eqs. (5a) and (5b) to calculate the natural parameters of all Gaussian terms. In practice,
we estimate the natural parameters using (unbiased) Monte-Carlo estimators for the expectations. We
have now available the likelihood terms and their Gaussian approximations. This allows us to write
L(θtk; Dtk ∪ Mtk-1) in the form of Eq. (6):
L(Otk ； Dtk ∪ Mtk-1 ) = log Zqtk + X EM(W) [log P(dtk |W) - log rtk (w； dtk)],
dtk ∈ Dtk ∪ Mtk-1
where dtk are the samples in Dtk ∪ Mtk-1 and where rtk (W; dtk) are the Gaussian approximation
of the corresponding likelihood terms. Note that rtk does not only depend on dtk , however, we omit
the dependence on the remaining data for notational convenience.
If the likelihood P(dtk |W) is close to the Gaussian rtk (W; dtk) in expectation w.r.t. the approximate
posterior qθt (W), then its contribution to L(θtk ; Dtk ∪ Mtk-1 ) is small. Similarly, likelihood terms
that cannot be well approximated by the respective Gaussian have a large contribution, and, hence,
the corresponding data should be kept in the memory. For this reason, we propose the score function
Stk (M； Dtk ∪ Mtk-I)=	X	(w)[ log P(dtk |w) - log rtk (w； dtk )],	⑻
dtk ∈ M
and the corresponding memory update Mtk = argmaxM Stk (M; Dtk ∪ Mtk-1 ). Note that since
all residual terms are computed independently, the update results in selecting the top M terms.
3.3	Gaussian Update
The Gaussian update follows from the memory update presented in the previous section: once the
memory Mtk has been selected, we update the Gaussian distribution with the approximations corre-
sponding to the rest of the data Dtk ∪ Mtk-1 \ Mtk. We can update qθt (W) in two equivalent ways:
qθtk (W) = qθtk-1 (W)	∏ rtk(Wptk),	(9a) qθtk (W)= qθtk(W) /	∏ rtk(Wptk).	(9b)
dtk 6∈ Mtk	dtk ∈ Mtk
Note again that the natural parameters of rtk (W; dtk ) are estimated using Monte Carlo and the
products in the above equations imply a summation of the natural parameters. In order to reduce the
variance of this sum of estimators, we use Eq. (9a) if |Dtk | ≤ |Mtk|,andEq.(9b)if|Dtk| > |Mtk|.
Furthermore, we can compute the average bias from all natural parameter estimates (see App. C). We
reduce the bias of our estimates by subtracting the average bias from all estimates. Note that a further
option to update qtk (W) would be to use VB on the data Dtk ∪ Mtk-1 \ Mtk to compute the update
qθt (W) ≈ qθt (W) P(Dtk ∪ Mtk-1 \ Mtk |W). The latter approach is numerically more stable
but computationally more expensive. It also turned out that it is less favorable to the update using
Eq. (9a) or Eq. (9b) in case of small datasets Dtk, because VB applied to BNNs with small datasets
often leads to a poor fit.
Previous work hypothesised that this problem is an artifact of the ELBO and not an optimisation
problem (Trippe & Turner, 2018; Turner & M. Sahani, 2011). We provide further evidence in Fig. 1,
where we infer the posterior of a Bayesian neural network with VB, using 70 and 100 data samples
respectively and compare it to posterior inference with MCMC. In case of VB with 70 samples, the
posterior approximation yields a model that is almost linear. These difficulties of posterior inference
with variational Bayes are especially problematic in case of the streaming data setting, where the
number of observations at each time-step is typically very small. The Gaussian update proposed
above can alleaviate the problem of having to train BNNs with small datasets. Specifically, we have
Ntk + M instead of Ntk data points to find a better optimum of the ELBO.
4
Published as a conference paper at ICLR 2020
(a) MCMC, 70 samples (b) VB, 70 samples (c) VB, 100 samples (d) ELBO terms during training
Figure 1: Posterior predictive distribution in the xy-plane (grey) of a Bayesian neural network with 2 layers of
16 units, tanh activations, prior p0 (w) = N(w; 0, 1), and Bernoulli likelihood. In case of variational Bayes
(Figs. 1b, 1c), the KL divergence of the ELBO is annealed from β = 0 to β = 1 over many iterations (450k
annealing, 50k ELBO). Fig. 1d shows that the approximation trades off the expected log-likelihood for a better
KL divergence as β is increased. With 70 data points, the annealed KL jumps to a significantly lower value,
resulting in an almost linear decision boundary. By contrast, MCMC yields a much better predictive distribution
for the same number of samples. Data is visualised in red and blue.
IOOOOO 200000	300000	400000	500000
iteration
4 Variational Bayes with Model Adaptation
The incremental learning methods discussed so far assume i.i.d. data (cf. Sec. 2, Eq. (1)). This
assumption can be reasonable even in scenarios with changing data distributions, e.g. when the data
drift is an algorithmic artifact rather than a real phenomenon. For example, in online multi-task or
curriculum learning we want to learn a model of all tasks, but we may choose to learn the tasks
incrementally for various reasons (e.g. Nguyen et al., 2018; Kirkpatrick et al., 2017; Schwarz et al.,
2018; Rusu et al., 2016; Fernando et al., 2017). However, such approaches are not applicable for
modeling non-stationary data: one of the properties of online VB is that the variance of the Gaussian
posterior approximation shrinks at a rate of O(N), where N is the total amount of data (e.g. Opper,
1998). Consequently, learning comes to a halt as t → ∞. To overcome this issue, the model needs
to be extended by a method that enables it to adapt to changes in the data distribution, e.g., by
deliberately forgetting the belief inferred from previous data.
In the following, we describe two alternative methods for adapting to changing data. In Sec. 4.1, we
impose Bayesian exponential forgetting, which forgets previous data exponentially by weighting
the likelihood terms (or their approximations). In Sec. 4.2, we implement the adaptation through a
diffusion process applied to the neural network parameters. Compared to the online learning scenario,
we make the following assumptions: (i) we observe datasets Dtk at potentially non-equidistant time
steps tk; (ii) data within Dtk is assumed i.i.d., however, not between different datasets Dtk and Dtk+1 .
In both approaches, we realise adaptation by an additional forgetting step before observing the
new data Dtk+1 . We denote the distribution, which results from applying the forgetting step to the
posterior approximation qθtk (w) p(Mtk |w) byptk+1(w).
4.1 Adaptation with Bayesian Forgetting
Model adaptation through forgetting can be achieved by decaying the likelihood based on the temporal
recency of the data (Graepel et al., 2010; Honkela & Valpola, 2003). It has been explored previously
as an alternative to filtering and is referred to as Bayesian exponential forgetting (KUlhaVy & Zarrop,
1993). This approach defines a forgetting operator that yields p(wtk+1 |Dt1:tk) directly. Here, we use
a continUoUs-time version of this forgetting operation that can be formUlated as
K	t κ -tk
p(w∣Dtι ：t K) X Po (W) ∏P(Dtk∣w)(1-° T ,	(10)
k=1
where τ is a time-constant corresponding to the average of the time-lags ∆tk+1 = tk+1 - tk . The
distribUtion defined in Eq. (10) can be formUlated recUrsively (cf. App. F) as
P(w|Dti：tk+i) Y Po(W)I-(1-Lk+1/Tp(WIDtI：tk)(1-—k+1/Tp(Dtk+」w).	(II)
This eqUation can be viewed as Bayes rUle (Eq.(1)) applied after the forgetting step. The first two
terms of Eq. (11) can be identified as the forgetting operation, applied to the cUrrent posterior. In
5
Published as a conference paper at ICLR 2020
5-
4
3
2 ■
1 ■
O
6	50 IOO 150	200	250
t
(a) μt
O 50 IOO 150	200	250
t
(b) σt
O 50 IOO 150	200	250
t
(c) μt ± σt, € = 0.05
5-
4
3
I
d 2 ∙
1 ∙
O
O 50 IOO 150	200	250
t
(d) μt
O 50 IOO 150	200	250
t
(e) σt
O 50 IOO 150	200	250
t
(f) μt ± σt, θ = 0.025
Figure 2: Time-evolution of distribution parameters of Bayesian Forgetting (top) and the Ornstein-Uhlenbeck
process (bottom) for different adaptation parameter values. The initial distribution (at t = 0) can be seen as the
approximate posterior at some time-step tk .
order to apply this operation to our posterior approximation qθt (w) p(Mtk |w), we modify it by an
additional weighting factor for each likelihood term in the memory. Denoting the age of a memory
item m by ∆tk(m), the forgetting operation for this new posterior approximation then results in
r	1 (1-€产气+1/T
Ptk+1 (W) H P0(w)1-(1-e) fc+1/T X qθtk(w)	Y p(m∣w)(1-e) fc /τ
m∈Mtk
=P0(w)1-(1-Lk+1/Tq&k(w)(1-Lk+1/T × Y p(m|w)(1-Lk+1(m)/T, (12)
m∈Mtk
where ∆tk+1 (m) = ∆tk(m) + ∆tk+1. As can be seen from Eq. (12), BF acts on both factors of the
posterior approximation independently: in case of the memory, it re-weights the respective likelihood
terms by updating ∆tk+1(m). For the Gaussian term qθt (w), BF leads to a weighted product with
the prior distribution (i.e. the first two terms of Eq. (12)), resulting in a Gaussian with parameters
σ-+ι =(1 - (1 - e广k+1∕τ)σ-2 + (1 - e严+1∕τσ-2,
。京μtk+ι =(1 - (1 - e严+1/T)σ-2μ0 + (1 - e严+1∕τσ-2μt% .
For ∆tk+1 → ∞, the likelihood term in Eq. (12) converges to the uniform distribution and the
Gaussian term reverts to the prior. We note, however, that while Eq. (11) is an exact recursive form
of Eq. (10), the online VB approximation of Eq. (11) is not generally identical to the (offline) VB
approximation of Eq. (10) due to its successive approximations. For tuning the hyperparameter e,
We note that the weighting of likelihood terms corresponds to an effective dataset size of 1/e ∙ N (if
all datasets are of equal size N). In Fig. 2, we also visualise the forgetting operation applied to the
Gaussian part of the posterior approximation for multiple values of e.
4.2 Adaptation with Diffusion Processes
Model adaptation can also be realised by using dynamic model parameters that evolve according to a
stochastic process. In this case, adaptation is achieved by the stochastic transition ptk+1,tk (w0 |w)
resulting in a prediction distribution
ptk+1 (w0) =
ptk+1,tk (w0|w) p(w|Dt1:tk) dw,
(13)
where we consider Gaussian transitions ptk+1,tk (w0|w). However, this operation is generally not
tractable for our posterior approximation qθt (w) p(Mtk |w). Moreover, the forgetting operation
implied by the transition does not retain the product form as in the case of BF. For this reason,
we consider only a Gaussian posterior approximation (without memory) for this approach, that is
ptk+ι (WO) = R ptk+ι,tk (WiW) qθtk (W) dw
6
Published as a conference paper at ICLR 2020
As mentioned in Sec. 4.1, BF yields the prior distribution for ∆tk+1 → ∞. This is a desirable
property, since it corresponds to forgetting all information conveyed by the data. In case of a Gaussian
prior, the only Gaussian process that fulfills this requirement is the OrnStein-UhIenbeck (OU) process
given by the stochastic differential equation dwt = θ ∙ (μo - Wt) dt + σ0√2θdWt, where θ is the
stiffness parameter which controls the drift rate towards μ0. To decouple the adaptation parameter
from the rate at which data is observed, we rescale the stiffness parameter as θ = a∕τ. The resulting
prediction distribution pt” (w) = Nμtk++,, σ2fc+ι) is defined by the parameters
e-a ,
μtk + 1
1-
+ e-a
~k + 1
τ
μtk
2
σtk+1
(1-e-2。— "0 + e-2a，
2
tk .
σ
An interesting observation is that both parameters evolve independently of each other. In contrast to
BF, the mean and variance—instead of the natural parameters—follow an exponential decay. The
hyperparameter a can be determined e.g. through the half-time of the exponential decay of the mean
parameter, given as 丁“ = 1∕θ. We visualise the time evolution of the above parameters in Fig. 2.
5	Related Work
There are many Bayesian approaches to online learning, which differ mostly in the approximation of
the posterior distribution at each time-step. Sequential Monte Carlo (Liu & Chen, 1998) approximates
the posterior by a set of particles. Assumed Density Filtering (ADF) (Maybeck, 1982) and Bayesian
online learning (Opper, 1998) are deterministic posterior approximations based on moment matching.
Other deterministic approaches are based on Laplace’s approximation (MacKay, 1992): Kirkpatrick
et al. (2017) use multiple diagonal Gaussian posterior approximations of previous time-steps to
regularise future tasks; Ritter et al. (2018) use a single (block-diagonal) posterior approximation,
summarising all previous time-steps. The latter method is closer to Bayesian online inference, as it is
an approximation of Eq. (1). Our work is based on online VB (Opper, 1998; Ghahramani, 2000; Sato,
2001; Broderick et al., 2013), which approximates the posterior at every time-step by minimising the
KL-divergence between a parametric (here Gaussian) and the true posterior distribution. In contrast
to online VB, we approximate the posterior by a Gaussian distribution and a running memory.
Other approaches are based on various types of episodic memory, motivated by their empirical
success in preventing catastrophic forgetting. The basic idea of rehearsal (Ratcliff, 1990) is to train on
both the new data and a subset of previous data or pseudo samples (Robins, 1995; Shin et al., 2017;
Kemker & Kanan, 2017) sampled from a generative model. The memory-based online inference
methods most similar to our approach are VCL (Nguyen et al., 2018) and VVM (Minka et al., 2009).
Both methods use a Gaussian distribution and a running memory to approximate the posterior. VCL
uses heuristics such as random Selection or the k-center method to update the memory. However,
both heuristics select the memory independently of the Gaussian approximation. By contrast, VVM
updates the memory with data that cannot be well approximated by the Gaussian distribution. VVM
uses expectation propagation for the posterior approximation in a logistic regression model and,
therefore, it is not directly applicable to our work. We transferred the main idea of VVM to online VB
and developed the corresponding memory update method. In our case, the memory is updated with
data for which the ELBO changes most if the corresponding likelihood functions are approximated
by a Gaussian. In contrast to these two approaches, we extend our model by an adaptation method
that allows to cope with non-stationary data.
Many adaptation methods were developed in the context of concept drift, however, few of these
approaches operate in the Bayesian framework. For example McInerney et al. (2015) treat the learning
dynamics of their model as a non-stationary process that allows for adaptation. In contrast, our
approach uses an evolving prior and a well defined forgetting mechanism that gives a better control
over the learning process. A more closely related approach uses the extended Kalman filter to estimate
the optimal parameters of a logistic regression classifier Su et al. (2008). However, they consider a
transition model which is equivalent to a Wiener process (in unit-time) and therefore does not revert to
the prior. By contrast, our approach (Sec. 4.2) models the dynamics as a prior-reverting OU process.
BF (Kulhavy & Zarrop, 1993) has been applied as an alternative to adaptation with an explicit
transition model (e.g. Honkela & Valpola, 2003; Graepel et al., 2010). Compared to previous work,
we used a continuous-time version of BF and extended it to our posterior appoximation consisting of
a Gaussian and a running memory.
7
Published as a conference paper at ICLR 2020
(a) t0
(b) t25
(c) t50
(d) LML
Figure 3: Two-moons dataset. Predictive distribution (Figs. 3a - 3c) of a BNN (gray) and running memory
(rectangular shapes, size is proportional to the score), chosen by the memory update proposed in Sec. 3.2. Data
from tk and t<k-1 is visualised as large circles and small dots, respectively. Fig. 3d shows the one-step-ahead
(predictive) LML (divided by the number of samples) for data that will be selected for the memory and data that
will be evicted. Data that will be selected in the memory tends to have a significantly lower predictive likelihood.
	GRS (ours)	k-center	random	Table 1: average test LML, av- eraged over the last 10% time- steps. Mean and std. deviations
Concrete	-0.779 ± 0.039	-0.798 ± 0.039	-0.800 ± 0.039	
Boston	-0.619 ± 0.111	-0.638 ± 0.093	-0.664 ± 0.156	are computed over 16 indepen-
Energy	0.365 ± 0.440	-0.119 ± 0.128	-0.078 ± 0.087	dent runs. The memory size
Yacht	1.925 ± 0.229	1.658 ± 0.291	1.648 ± 0.254	is 15 for Concrete, Boston, En-
Spam	-0.216 ± 0.016	-0.219 ± 0.015	-0.217 ± 0.016	ergy and Yacht, 25 for Spam and
Wine	-1.165 ± 0.056	-1.212 ± 0.059	-1.194 ± 0.070	Wine, and 150 for MNIST. Bold
MNIST	-0.148 ± 0.005	-0.158 ± 0.005	-0.153 ± 0.005	indicates best (average) results.
6	Experiments
We validate our proposed inference methods in two stages. In Sec. 6.1, we compare our memory
update and Gaussian update (Sec. 3) to existing memory-based online inference methods on several
standard machine learning datasets. In Sec. 6.2, we evaluate our adaptation methods (Sec. 4)
on commonly used datasets with concept drift (Widmer & Kubat, 1996), where the conditional
distribution of labels given the features changes over time (i.e. non-stationary data in the context of
predictive models).
We found that training (variational) Bayesian neural networks on streaming data is challenging, specif-
ically, our approach requires model parameters very close to a local optimum since Eqs. (5a) and (5b)
hold only at local extrema of the ELBO. To overcome these difficulties, we use several methods to
reduce the variance of the gradient estimates for learning: (i) we apply the local reparametrisation trick
(Kingma et al., 2015); (ii) we use the Adam optimiser (Kingma & Ba, 2014); and (iii) we use multiple
Monte Carlo samples to estimate the gradients (cf. Tab. 2 for details). Furthermore, we developed
methods for determining hyperparameters of the Gaussian prior and the initialisation distribution
of Bayesian neural networks. The idea is similar to the initialisation method proposed by Glorot &
Yoshua Bengio (2010) and He et al. (2015): we choose the prior and the posterior initialisation such
that the mean and standard deviation of the activations in every layer are approximately zero and one,
respectively. We refer to App. H and App. I for a derivation and further details.
We use the following metrics for evaluation: (i) the avg. test log-marginal likelihood (LML)
N-ItPn log Eqθtfc (w) [P(d(nt |W)] , where d(nStis a sample from a heldout test dataset; (ii) the avg. one-
step-ahead LML N-+1 Pn log Eqθt, (W) [p(d(；+ 1 |w)], where d(n+1 is data observed at time-step tk+ι.
Both metrics measure the predictive performance, however (i) can be used in the online setting, where
the data is i.i.d.; and (ii) is typically used to evaluate models with non-stationary streaming data.
6.1	Online Learning
In this section, we evaluate our running memory (Sec. 3) in an online learning setting. To illustrate
how our memory update works, we start our evaluation with a qualitative assessment: we train a
model on 2-dimensional toy data (two-moons), where we can visualise the selected memory. The
BNN has 2 layers with 16 units and tanh activations, and has a prior p0 (w) = N(w; 0, 1) on all
8
Published as a conference paper at ICLR 2020
UCl Concrete
PMM=s=,pspadXa-Bo-
5	10	15	20	25
memory size
UCI Concrete
PMM=SI=,Pspadxa-H
UCI Energy
5	10	15	20	25
memory size
PooU--P-TPaPgXTBOI
UCI Spam
O 10	20	30	40	50
memory size
PooU--P-TPaPgXTBOI
MNIST
一(M50 ∙
-0.155 ■
-0.160
-0.165
0	50 IOD 150	200	250
memory size
PoOM=®n'Papadxe,Bo_
-0.25
-0.30
-0.35
UCI Spam
O
-0.200
-0.150
-80£=8=-.-xfl£s.ff-
——GRS 15
---IacenterlS
---random 15
---no memory
UCI Energy
---GRS 30
---kcenter 30
---random 30
---no memory
MNIST
Figure 4: Average test LML, evaluated for several memory sizes (top), and evaluated over time (bottom) for a
specific memory size (cf. corresponding legend). Cf. Sec. 6.1 for details and App. A for further results.
weights and biases. The memory-size is M = 30. The model observes 150 data samples at time-step
t0 and 15 samples at all consecutive time-steps. In Fig. 3, we visualise the selected memory and
the corresponding scores for time-steps t0, t25, and t50, respectively. We can make the empirical
observation that our method favors data close to the decision boundary. Furthermore, in Fig. 3d, we
visualise the one-step-ahead LML for data that will be selected and evicted (in the next time-step),
respectively. This shows that our memory update tends to select data for which the model has a low
predictive LML. These observations support our intuition that the memory is indeed complementary
to the Gaussian approximation, selecting data for which the likelihood cannot be well approximated
by a Gaussian function. In Fig. 9 of the supplementary material, we visualised the running memory
for a model trained on MNIST, showing that the memory also accumulates diverse samples over time.
We evaluate our memory-based online inference method (Sec. 3) quantitatively on several standard
machine learning datasets, including regression (UCI Boston, UCI Concrete, UCI Energy, UCI Yacht)
and classification (MNIST, UCI Spam, UCI Wine) tasks. Here, we refer to our approach as Gaussian
Residual Scoring (GRS). We compare GRS to the respective memory update and Gaussian update
methods proposed in VCL (Nguyen et al., 2018) (cf. Sec. 2.2). Refer to App. B for an explanatory list
of compared update methods. Online learning is performed by observing Ntk samples per time-step
(cf. Tab. 2 for the experiment setup and hyperparameters.). For evaluation, we use a random held-out
test dataset (20% of the data). We perform each experiment with 16 different random data splits and
random seeds for the model parameter initialisation. In Fig. 4, we plot the test LML, averaged over the
16 runs, against the memory size, and the LML over all time-steps. In most cases, random selection
and the k-center method start with a worse initial fit at t0 . This is because these methods perform
the initial Gaussian update by optimising the ELBO with Nt0 - M samples at t0 ; by contrast, GRS
uses a Gaussian update that first optimises the ELBO with Nt0 samples and subsequently discounts
the contribution of the memory. In Tab. 1, we report the mean and std. deviation of the LML, where
the mean and std. deviation are taken over the 16 independent runs, each averaged over the last 10%
time-steps. The results demonstrate the superior predictive performance of our update methods. We
also note that the experiments on the smaller datasets (cf. Tab. 2 in App. B) result in a high variance
among the random data splits and random seeds. This is the case for all compared methods and it
could not be remedied e.g. by using annealing or a different prior.
6.2	Adaptation
In this section, we evaluate our adaptation methods (Sec. 4) in settings with concept drift. We begin
with a simple logistic regression problem, where the data Dtk = {(xtk , ytk)}n, xtk ∈ R2, ytk ∈ R
is sampled from Xtk 〜 Uniform(-3,3), Ytk 〜 Bernoulli(σ(wtkXtk)). The true model has two
time-dependent parameters w] = 10sin(α ∙ tk), Wtk = 10cos(α ∙ tk), where α = 5deg /sec and
where we observe data at tk ∈ [0, 1, . . . , 720]. Fig. 5 shows the learned model parameters for
standard online learning (without adaptation), OU process transitions, and Bayesian forgetting. If
the time-dependence of the data is ignored (in case of online VB), the class labels are distributed
with equal probability in the whole input space. Consequently, as t → ∞, the weights of the model
without adaptation shrink to 0. By contrast, the posterior means of BF and the OU process follow a
sinusoidal curve as the parameters of the true model.
9
Published as a conference paper at ICLR 2020
(a) Online (no adaptation)
(b) Ornstein-Uhlenbeck process
(c) Bayesian forgetting
Figure 5: Mean and std deviation of the approximate posterior distributions of a logistic regression model over
720 time-steps. The model is trained on a toy classification problem with rotating class boundaries (cf. Sec. 6.2).
Online VB (left) quickly converges to zero mean, whereas Bayesian forgetting and OU-process transitions lead
to a sinusoidal curve as in the true model.
Wiener process
Ornstein-Uhlenbeck process
Bayesian forgetting
POOll=，-Fpsu°,dxw6O-
Bayesian forgetting with memory
0.0	0.5	1.0	1.5	2.0	2.5	0	1	2	3	4	0.0	0.5	1.0	1.5	0	50	100	150	200
diffusion rate (OdtffE) Ie-2	stiffness (θ)	Ie-2	forgetting rate (ε) Ie-I	memory size
Figure 6: One-step ahead LML on Covertype dataset. Subplots show 3 different adaptation methods (3 left
plots), evaluated for several values of the respective adaptation parameter, and Bayesian forgetting with = 0.11,
evaluated for multiple memory sizes (right).
We also evaluate our adaptation methods quantitatively on 3 datasets with concept drift (Weather, Gas
Sensor Array Drift, Covertype). We compare online VB (without adaptation), the Wiener process (a
special case of the OU process), the OU process, and Bayesian Forgetting (with and without memory).
All compared variants use the same model architecture and hyperparameters (cf. Tab. 2 in the
supplementary material). We report the one-step-ahead LML, where the expectation is approximated
with 500 Monte Carlo samples. Results are averaged over the last 50% time-steps, because we are
interested in the continual learning performance, and the first few time-steps will be similar for most
methods. We report the mean and std. deviation over 8 independent runs with different random seeds.
In Fig. 6 (and Fig. 10 in the appendix), we plot the LML against 10 adaptation parameter values (of
the respective adaptation method), where the value zero corresponds to online VB. The LML for
BF with different memory sizes and a fixed forgetting rate = 0.11 is shown in Fig. 6. As can be
seen from the results, all adaptation methods significantly improve the performance compared to
online VB. Interestingly, the Ornstein-Uhlenbeck process performs better than Bayesian Forgetting,
however, using a running memory with Bayesian Forgetting closes the gap.
7	Conclusion
In this work, we have addressed online inference for non-stationary streaming data using Bayesian
neural networks. We have focused on posterior approximations consisting of a Gaussian distribution
and a complementary running memory, and we have used variational Bayes to sequentially update
the posteriors at each time-step. Existing methods update these two components without having an
interaction between them, and they lack methods to adapt to non-stationary data. We have proposed a
novel update method, which treats both components as complementary, and two novel adaptation
methods (in the context of Bayesian neural networks with non-stationary data), which gradually
revert to the prior distribution if no new data is observed.
Future research could extend our work by drift detection methods and use them to infer the adaptation
parameters. This work could also be extended by developing adaptation methods for gradual, abrupt,
or recurring changes in the data distribution. Finally, we observed that variational Bayesian neural
networks with a uni-modal approximate posterior often find poor local minima if the dataset is small
and models are complex. This is especially challenging in scenarios with streaming data. While
our Gaussian update alleviates this problem to a certain degree, further research in extending the
approximation family beyond Gaussians could be beneficial. Progress in this direction would improve
our proposed methods and allow to scale them to more complex models.
10
Published as a conference paper at ICLR 2020
References
C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks.
In Proceedings of the 32Nd International Conference on International Conference on Machine
Learning - Volume 37, pp. 1613-1622, 2015.
T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational bayes.
In Proceedings of the 26th International Conference on Neural Information Processing Systems -
Volume 2, NIPS’13, pp. 1727-1735, USA, 2013. Curran Associates Inc.
Z. Chen and B. Liu. Lifelong Machine Learning. Morgan & Claypool Publishers, 2016.
B.	Cseke, M. Opper, and G. Sanguinetti. Approximate inference in latent gaussian-markov models
from continuous time observations. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 971-979.
Curran Associates, Inc., 2013.
G. Ditzler, M. Roveri, C. Alippi, and R. Polikar. Learning in nonstationary environments: A survey.
Computational Intelligence Magazine, IEEE, 10:12-25, 11 2015.
C.	Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wierstra.
Pathnet: Evolution channels gradient descent in super neural networks. CoRR, abs/1701.08734,
2017.
Z. Ghahramani. Online variational Bayesian learning. NIPS Workshop on Online Learning, 2000.
S.	Ghosh, J. Yao, and F. Doshi-Velez. Structured variational learning of Bayesian neural networks
with horseshoe priors. In J. Dy and A. Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80, pp. 1744-1753, 2018.
X. Glorot and Y. Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Y. W Teh and M. Titterington (eds.), Proceedings of the Thirteenth International
Conference on Artificial Intelligence and Statistics, pp. 249-256, 2010.
T.	Graepel, J. Q. Candela, T. Borchert, and R. Herbrich. Web-scale Bayesian click-through rate
prediction for sponsored search advertising in Microsoft’s Bing search engine. In Proceedings of
the 27th International Conference on Machine Learning, ICML, pp. 13-20, USA, 2010. Omnipress.
A. Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R. S. Zemel,
P. L. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), Advances in Neural Information Processing
Systems 24, pp. 2348-2356. Curran Associates, Inc., 2011.
S. Grossberg. Competitive learning: From interactive activation to adaptive resonance. Cognitive
Science, 11(1):23 - 63, 1987.
K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level perfor-
mance on imagenet classification. In Proceedings of the 2015 IEEE International Conference on
Computer Vision (ICCV), pp. 1026-1034, Washington, DC, USA, 2015. IEEE Computer Society.
G. E. Hinton and D. van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings of the Sixth Annual Conference on Computational Learning
Theory, COLT ’93, pp. 5-13, New York, NY, USA, 1993. ACM.
A. Honkela and H. Valpola. On-line variational bayesian learning. In In Proc. of the 4th Int. Symp.
on Independent Component Analysis and Blind Signal Separation, pp. 803-808, 2003.
N. Kamra, U. Gupta, and Y. Liu. Deep Generative Dual Memory Network for Continual Learning.
arXiv e-prints, art. arXiv:1710.10368, Oct 2017.
R. Kemker and C. Kanan. Fearnet: Brain-inspired model for incremental learning. CoRR,
abs/1711.10563, 2017.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.
11
Published as a conference paper at ICLR 2020
D. P. Kingma, T. Salimans, and M. Welling. Variational dropout and the local reparameterization
trick. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 28, pp. 2575-2583. Curran Associates, Inc., 2015.
J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan,
T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell.
Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of
Sciences, 114(13):3521-3526, 2017.
D. A. Knowles and T. P. Minka. Non-conjugate variational message passing for multinomial and
binary regression. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems 24, pp. 1701-1709. Curran Associates,
Inc., 2011.
T. KUlhavy and M. B. Zarrop. On a general concept of forgetting. International Journal ofControl,
58(4):905-924, 1993.
J. S. Liu and R. Chen. Sequential monte carlo methods for dynamic systems. Journal of the American
Statistical Association, 93(443):1032-1044, 1998.
D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. In Proceedings
of the 31st International Conference on Neural Information Processing Systems, NIPS’17, pp.
6470-6479, USA, 2017. Curran Associates Inc. ISBN 978-1-5108-6096-4.
D. J. C. MacKay. A practical Bayesian framework for backpropagation networks. Neural Computu-
tation, 4(3):448-472, May 1992.
P. S. Maybeck. Stochastic Models, Estimation and Control. Mathematics in science and engineering.
Academic Press, 1982.
J. McInerney, R. Ranganath, and D. Blei. The population posterior and bayesian modeling on streams.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 28, pp. 1153-1161. Curran Associates, Inc., 2015.
T. P. Minka, R. Xiang, and Y. A. Qi. Virtual Vector Machine for Bayesian online classification. In
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pp.
411-418, Arlington, Virginia, USA, 2009. AUAI Press.
C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. In International
Conference on Learning Representations, 2018.
M. Opper. A Bayesian approach to on-line learning. In D. Saad (ed.), On-line Learning in Neural
Networks, pp. 363-378. Cambridge University Press, New York, NY, USA, 1998. ISBN 0-521-
65263-4.
M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural Computation,
21:786-92, 10 2008.
M. Opper and O. Winther. Expectation consistent free energies for approximate inference. In L. K.
Saul, Y. Weiss, and L. Bottou (eds.), Advances in Neural Information Processing Systems 17, pp.
1001-1008. MIT Press, 2005.
G.	I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter. Continual lifelong learning with neural
networks: A review. ArXiv, abs/1802.07569, 2018.
R. Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and
forgetting functions. Psychological review, 97 2:285-308, 1990.
M. B. Ring. Child: A first step towards continual learning. Machine Learning, 28(1):77-104, 1997.
H.	Ritter, A. Botev, and D. Barber. Online structured laplace approximations for overcoming
catastrophic forgetting. In Proceedings of the 32Nd International Conference on Neural Information
Processing Systems, NIPS’18, pp. 3742-3752, USA, 2018. Curran Associates Inc.
12
Published as a conference paper at ICLR 2020
A. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123-146,
1995.
A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu,
and R. Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016.
P. Ruvolo and E. Eaton. ELLA: An efficient lifelong learning algorithm. In In Proc. of the 30th
International Conference on Machine Learning, pp. 507-515, 2013.
M. Sato. Online model selection based on the variational bayes. Neural Computation, 13(7):
1649-1681, 2001.
J. Schwarz, J. Luketina, W. Czarnecki, A. Grabska-Barwinska, Y. W. Teh, R. Pascanu, and R. Hadsell.
Progress & compress: A scalable framework for continual learning. In Proceedings of the 35th
International Conference on Machine Learning, 2018.
H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 30, pp. 2990-2999. Curran Associates, Inc., 2017.
D. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algorithms. In
AAAI Spring Symposium Series, 2013.
B. Su, Shen, Y-D., and Xu, W. Modeling concept drift from the perspective of classifiers. In 2008
IEEE Conference on Cybernetics and Intelligent Systems, pp. 1055-1060, Sep. 2008.
B. Trippe and R. Turner. Overpruning in Variational Bayesian Neural Networks. arXiv e-prints, pp.
arXiv:1801.06230, Jan 2018.
R. E. Turner and M. M. Sahani. Two problems with variational expectation maximisation for time-
series models. In D. Barber, T. Cemgil, and S. Chiappa (eds.), Bayesian Time series models,
chapter 5, pp. 109-130. Cambridge University Press, 2011.
G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Machine
Learning, 23:69-101, 1996.
13
Published as a conference paper at ICLR 2020
8	Appendix
A Further Experimental Results
A. 1 Memory
Here we provide additional experimental results for the memory update and Gaussian update from
Sec. 3. We conducted experiments on 3 additional datasets (UCI Boston, UCI Yacht, UCI Red Wine).
The influence of the memory size and the performance over time (for a specific memory size) are
shown in Fig. 7 (corresponding to Fig. 4 in the main text).
PooU=B-TPaPaCIXa-60-
UCI Boston
IKO=s一'Papadxe-H
UCIYacht
0	5	10	15	20	25
memory size
-80£=8=-.-xfl£XTff-
UCl Red Wine
0	10	20	30	40	50
memory size
UCI Boston	UCI Yacht
-00£=8=_.-stiKXTCT0-
GRS 15
læenter 15
random 15
no memory
UCl Red Wine
0 5 0
∙,∙,∙3
Poola-pal3adxa-6o-
0	5	10	15	2。	25	30	0.0	2.5	5.0	7.5 10.0 12.5	0	25	50	75 IOO 125 150
t	t	t
Figure 7:	Average test LML on further datasets not included in the main text. Evaluated for several memory
sizes (top), and evaluated over time (bottom) for a specific memory size. Cf. Sec. 6.1, App. A.1 for details.
Furthermore, we test the memory update and Gaussian update of GRS separately on UCI Energy
and UCI Concrete. For this purpose, i) we combine the k-center method with our Gaussian update
from Sec. 3.3; and ii) we use our memory update from Sec. 3.2 and update the Gaussian distribution
by optimizing Eq. (2) with Dtk ∪ Mtk-1 \Mtk (re-fitting) . The results are shown in Fig. 8. As
can be seen, GRS performs better than one of the components used in combination with a baseline
method. GRS with refit performs especially bad, similar to the baselines k-center and random. This is
because refitting requires optimising the ELBO with a small dataset. As mentioned in Sec. 3.3 (cf.
Fig. 1), Bayesian neural networks with VB perform bad on small datasets due to over-regularisation.
Consequently, in case of refitting, a good memory update can lead to a worse overall performance due
to a much worse Gaussian update. While this general issue with Bayesian neural networks (learned
with VB) is beyond the scope of this work, it is an important future research direction.
UCI Energy
PMM=H 十 PalSadxa-Bo-
UCI Energy
PMM-I'Palsadxa-H
UCI Energy
——GRS 15
--- GRS with k-center 15
—— GRS with refit 15
0.25
0.00
-0.25 -
-0.50
-0.75 -
-1.00
UCI Energy
POOq=s=,pspadXa-Bo-
0	5	10	15	20
memory size
POOq=SI=,Pspadxa-H
25	0	20	40	50
t
POOq=SI=,Pspadxa-H
GRS 15
GRS with k-center 15
GRS with neflt 15
0	20	40	50
t
POOq=SI=,Pspadxa-H
GRS 25
GRS with k-center 2 5
GRS with neflt 25

Figure 8:	Average test LML on UCI Energy (top) and UCI Concrete (bottom). GRS denotes our approach
(Sec. 3), GRS with k-center replaces our memory update by the k-center method; GRS with refit replaces our
Gaussian update by the optimization of Eq. (2) with Dtk ∪ Mtk-1 \Mtk . Evaluated for several memory sizes
(left), and evaluated over time (right) for 3 different memory sizes. Hyperparameters are chosen as in Sec. 6.1.
14
Published as a conference paper at ICLR 2020
To better understand our memory update using the score function from Eq. (8), we visualise the
running memory for a model trained on MNIST in Fig. 9.
SQBSEinHIiIQQ ΞBQHBΠI3E1BE1
QQBBE3QSE1QQ HSB□QHE]HE]Q
ħsbbbsqq□ξ bbqei□ddbdħ
∏Q∏HHd□□Bn Bdqdqbbeibq
beihħe9ξξe]bb Eiqhhqbhseib
ΞBS!E1SBI3QBE1 BS□E1BQHDΞS!
ΞDB□E1ΞI3ΞQΞ E3ΞEI□ΞHE10BQ
I3QDE!QBIQE1BH DElBHQnSQHQ
QHSEIE1BDQHΞ ΞΞ≡ΞBE1EIQ□H
SQHnQBHSHEl QBEIBQHBEIBB
(a) t25	(b) t50
SSQQBQSnElEl SQBBSEIBEIEIQ
ElEIBEa目国EaEIe目□ElBΞQαE]□Bα
HQBHB□QHHΠ QOQBDQSElQEi
HΠQBSaΞ□HB □□HBΞBΞΞDB
ΞBBBΞDBBQE3 OI3HQ□E3E1E]BH
Eideidqqheihb bqξsqξξbbξ
HQQΠQBDBQΠ SOBΞDBQ□BB
□BΞBE1ΞI3EIΞB BΠE1ΞH□BI≡IΠE1
DBQSQI≡□D□a Q□HE1E1QBHE!S
(c) t75	(d) t100
Figure 9:	Running memory at different time-steps on MNIST (cf. Sec. 6.1), with a memory size N = 100. The
memory update tends to select non-typical data while showing diversity.
A.2 Adaptation
Wiener process
pooq=ω~'pa,tta,dxa,,6o一
Ornstein-Uhlenbeck process
0.0	0.5	1.0	1.5	2.0
stiffness (θ)	le-3
POOq=ω-'pa,tta,dxa,l60-
Bayesian forgetting
0.0	0.5	1.0	1.5	2.(
forgetting rate (ε) le-2
POOq=ω-'pa,tta,dxa,l60-
0.0	0.5	1.0	1.5	2.0	2.5
diffusion rate (Odiff/%)	Ie-3
Wiener process
Ornstein-Uhlenbeck process
0.0	0.5	1.0	1.5	2.0	2.5
diffusion rate (Odiff/。。)	le-3
0.0	0.5	1.0	1.5	2.0
stiffness (θ)	le-3
pooq=tu-'ptutttudx*o-

Figure 10: One-step ahead LML on Gas Sensor Array Drift dataset (top) and Weather dataset (bottom). Subplots
show 3 different adaptation methods (left), evaluated for several values of the respective adaptation parameter,
and Bayesian forgetting with = 0.0095 (Gas Sensory Array Drift) and = 0.031 (Weather), evaluated for
multiple memory sizes (right).
We also evaluated our adaptation methods on 2 additional datasets (Gas Sensor Array Drift, Weather).
In Fig. 10, we visualise the influence of the adaptation parameter for these datasets. Note that the
range of the adaptation parameters is on a much smaller range compared to the experiments on
Covertype (Sec. 6.2). For larger values, the performance starts to degrade. Surprisingly, the memory
degrades the performance in case of the Gas Sensor Array Drift dataset.
A.3 Catastrophic Forgetting with Online VB and Bayesian Neural Networks
Here we provide further experimental results for the behavior of online VB (Secs. 2.1, 3) in case of
non-stationary data. For this purpose, we train Bayesian neural networks with different architectures
on the toy classification problem with a rotating decision boundary from Sec. 6.2, however, with
150 data samples per time-step. In Fig. 11, we visualise the training LML for different architectures,
including a linear model. It can be seen that Bayesian neural networks with higher complexity (i.e.
more layers or more units) drop slower in performance compared to the linear model. However,
this is not a desired property for online VB, since exact online Bayesian inference would yield
the same posterior distribution as offline Bayesian inference. In case of our toy classification data
(where the time dependence is ignored), online inference should not be able to classify the data as
t → ∞. Instead, this learning behavior shows that online VB with Gaussian approximate posterior
distributions is prone to catastrophic forgetting.
15
Published as a conference paper at ICLR 2020
VHERjval-Sof=S-=I-UBUVaXUIaO-
V-AEsjvaI-USf-S-=I-StiVaSIraO-
O IOO 200	300	400	500	600	700
t
(a) no adaptation (1 layer)
VHERjval-Sof=S-=I-UBUVaXUIaO-
O IOO 200	300	400	500	600	700
t
(b) no adaptation (2 layers)
VHERjval-Sof=S-=I-UBUVaXUIaO-
O IOO 200	300	400	500	600	700
t
V-AEsjvaI-USf-S-=I-StiVaSIraO-
——8x8, BF
---8x8, BF, 10 samples
---8x8, BF, 50 samples
(c) no adaptation (3 layers)
V-AEsjvaI-USf-S-=I-StiVaSIraO-

0	100	200	300	400	500	600	700	0	100	200	300	400	500	600	700	0	100	200	300	400	500	600	700
t	t	t
(d) BF and OU (2 layers)	(e) BF with memory	(f) no adaptation with memory
Figure 11: LML for toy classification problem with rotating class boundaries (cf.Sec. A)
B Experiment Setup
The following is an explanatory list of the update methods used in Sec. 6.1:
•	k-center (VCL): Uses the k-center method (Sec.2.2) for the memory update and Eq. (2) with
(Dtk ∪ Mtk-1) \ Mtk for the Gaussian update.
•	random (VCL): Uses random selection (Sec.2.2) for the memory update and Eq. (2) with
(Dtk ∪ Mtk-1) \ Mtk for the Gaussian update.
•	GRS (Gaussian Residual Scoring, ours): Uses Eq. (8) for the memory update (Sec. 3.2) and
performs the Gaussian update by first using Eqs. (2) with (Dtk ∪ Mtk-1 ) and subsequently
using Eqs. (5a), (5b) for removing the local contributions of Mtk (cf. Sec. 3.3).
Similarly, the following list summarises the adaptation methods used in Sec. 6.2:
•	Wiener process: Posterior approximation consists of qθt (w) only. Transition p(wtk+1 |wtk)
is given by a random walk. We used a diffusion that is proportional to the prior standard
deviation in every neural network layer (cf. Sec. 4.2). No memory used.
•	Ornstein-Uhlenbeck process: Posterior approximation consists of qθt (w) only. Transition
p(wtk+1 |wtk) is given by the Ornstein-Uhlenbeck process (cf. Sec. 4.2). No memory used.
•	Bayesian forgetting: Posterior approximation consists of qθt (w) only. No state-space
model assumption, instead uses Bayesian exponential forgetting (cf. Sec. 4.1).
•	Bayesian forgetting with memory: Posterior approximation consists of qθt (w) and Mt. No
state-space model assumption, instead uses Bayesian exponential forgetting (cf. Sec. 4.1).
In Tab. 2, we summarise experimental setup (hyperparameters) used for Secs. 6.1 and 6.2.
C Factorisation Property of the Gaussian Variational Approximation
Here we derive the factorisation property of the Gaussian variational approximation distribution by
expressing the natural parameters of the Gaussian approximation as a sum. This can be shown for
the Gaussian approximation at a local optimum of the ELBO. For a Gaussian prior and posterior the
16
Published as a conference paper at ICLR 2020
Table 2: Experiment setup for online experiments. Nt0 is the number of observed samples at the first
time-step and Nt1:k is the dataset size of all other time-steps. M refers to the number of samples
in the memory. We evaluated 5 different memory sizes for each experiment in Sec. 3 and 10 sizes
for experiments in Sec. 4, where the sizes are equally spaced in the given range. Ktrain and Kterm
is the number of MC samples used for training and for estimating the Gaussian terms respectively.
It0 and It1:K refer to the number of iterations for the first time-step and all subsequent time-steps,
respectively. The architecture denotes the number of units for each hidden layer (e.g. [16, 16] denotes
2 hidden layers with 16 units each).
Nt0	Nt1:k	M	Ktrain	Kterm	It0	It1:K	Architecture
Concrete	100	10	[5..	. 25]
Boston	100	10	[5..	. 25]
Energy	100	10	[5..	. 25]
Yacht	100	10	[5..	. 25]
Spam	250	25	[10.	. . 50]
Wine	250	25	[10.	. . 50]
MNIST	2.5k	250	[50.	. . 250]
kkkk000
kkkkkkk
0000000
5555555
kkkkkkk
0000000
5555555
0k0k0k0k0k0k0k
6666224
1111336
6666224
GasSensorArray	500	50	[5...50]	200	100k	50k	10k	[8, 8]
Weather	1k	100	[5...50]	100	100k	50k	10k	[16, 16]
Covertype	1k	1k	[10...200]	400	100k	50k	10k	[32, 32]
ELBO is given as
L(μ*, Σ*) = -1 (log ∣∑o∣- log ∣Σ*∣- d + (μ* - μo)T∑-1(μ* - μo) + Tr(Σ*Σ-1
N
+ X Ew〜qθ*(w) [logp(d(n)∣w)].
n=1
At a local optimum, We have ""dʃ ) = 0, which yields
∂
X ∂^TEW〜qθ*(W) [logp(d IW)] = ς0 (μ - μO).
n=1 04
Hence, we obtain
N∂
μ* = μo + ∑o X ∂^τ Ew 〜qθ* (w) [ log p(d(n) |w)].
n=1 04
Similarly, We have 叱炉*)= 0, which yields
N∂	1	1
X	Ew〜qθ*(w) [logp(d(n)∣w)] = - 2(∑*)-1 + 2∑-1.
n=1
Hence, we obtain
N ∂	-1
ς* = (ς-1- 2 X
∂∑* Ew~qθ* (w) log p(d(n) |w)	-1.
n=1
Next, we calculate the natural parameters from Eqs. (14), (16):
N∂
λ =λ0 + X -2∂∑7Ew〜qθ* (w) [ logp(d	IW)]
n=1
N
= Λ0 + XΛ(n).
(14)
(15)
(16)
17
Published as a conference paper at ICLR 2020
N
η* = Λ*μ* =(Ao + X Λ⑺)μ*
n=1
N∂
A0 W0 + ςo £ 加E
n=1
N
'，W〜q0* (w) [ logp(d(n) |w)]) + X Λ⑺μ*
n=1
~q0*(w)[ logp(dRw)i +Λ⑺μ*)
A0μ0+x( ∂μ Ew,
n=1
N
η0+Xη(n).
n=1
Monte Carlo estimation: The natural parameters Λ(n), η(n) can be estimated with Monte Carlo,
by replacing the expectation with an empirical mean. Since the parameters A* and Ao (and η*, no
respectively) are known, the total bias of the parameter estimates can be computed:
N
Ab=(A*-A0)-XA(n),
n=1
We use this to reduce the bias for the individual terms:
A(n) = As)- NNAb,
N
ηb = (η* - η0) - X η(n).
n=1
n ⑺=伊-N nb.
D ELBO in Residuals Form
Here we show how the ELBO can be written in the form of Eq. (6). Let us define the variational
distribution in the factorised form qθ(w) = Zq-1p(w) QnN=1 r(n) (w) (cf. Sec.3.1). We can then
write the ELBO as
N
L(μ, ∑; D) = Eqθ(w) X log p(d(n)∣w)+log P(W)- log qθ (W)
n=1
N
=Eqθ(w) X (logp(d(n)∣w) + logr(n)(w) - logr(n)(w)) + logP(W)- logqθ(w)
n=1
=Eqθ(w) X (logp(d(n)∣w) - logr(n)(w)) + log P(W) Qn=I r)(W)
n=1	qθ (W)
=Eqθ(w) X (logp(d(n)∣W) - logr(n)(W)) + log Zq ∙ qθ(W)
n=1	qθ (W)
= log Zq + X Eqθ (w) log P(d(n)|W) - log r(n)(W).
n
E Memory Update Score Function
In Eq. 8, the expectation involving Gaussian terms can be calculated analytically:
E,2θtfc (w) h log r(m)(W)] = / qθtk(W)(n(n)W - 2A(n)W2)dW
n(n)μ(n) - ∣A(n)((μ*)2 + ∑*)
Yn)(A*)-1n* - |A(n)((A*)-1n*)2 - ∣A(n)(A*)-1
The expectation involving non-Gaussian terms (in Eq. 8) has no closed-form solution. We therefore
estimate E<jθtfc (w)[ logp(d(n) |w)]
using Monte-Carlo.
18
Published as a conference paper at ICLR 2020
F Bayesian Forgetting - Recursive Formulation
Here we show how Bayesian forgetting can be rearranged into a recursive formulation. We first bring
this formula into a similar form as Eq. (1), extracting the most recent likelihood term:
K + 1	tK+1 -tk
P(WIDti：tK+i ) (X PO(W) ∙ ∏ P(Dtkw)(I) T
k=1
K	tK + 1-tk
= PO(W) ∙ Up(DtkIW)(1-e)	T
k=1
• P(Dtκ + ι |w).
The first two terms can be rewritten as
K	tK + 1 -tk
PO(W) • ɪɪr(DtkIW)O T
k=1
K
= P0(W) • Y P(Dtk |W)(1-)
k=1
K
= P0(W) • Y P(Dtk |W)(1-)
k=1
tK+1 -tK +tK-tk
tK - tk ,	、
T ∙(1-e)
tK + 1 -tK
τ
τ
tK + 1 -tK
K	tκ-tk、(1 —e)
= PO(W) •( ∏P(Dtkw)(Ii) T )
k=1
τ
X P0 (W) •
P(WIDt1:tK) (1-)
P0 (W)
tK + 1 -tK
tK + 1 -tK
• P(W|Dt1:tK)(1-)
tK + 1 -tK
τ
τ
τ
Hence, we have shown that the posterior can be expressed recursively as
P(WIDti：tk + i) Y PO(W)1-(1-产"+1/TP(WIDti:tk )(1-产"+1/TP(Dtk+1|W).
The parameters of the Gaussian part qθt (W) of the posterior approximation (after applying the
forgetting operation) can be calculated easily from the above equation.
Natural parameters:
Λtk+1 =Λo • (1 - (1 - e) tk+-k) + Λtk ∙ ((1 - e)tk+-k)
,	,	、tk+1 -tk、	，，	、tk + 1 -tk、
ηtk+ι = η •(1 — (1 — e)	T ) + ηtk •((I — e) T )
Covariance parameter:
Σtk+1 = (Λo • (1 - (1 - e)i)+Λtk • ((1 - e).)「
=(∑-1 • (1 - (1 - e)Tk) + ∑tk1 • ((1 - e) - ))T
Location parameter:
μtk + 1 = 'tk + 1 • ηtk + 1
=Lk+ι (ηO •(1 - (1 - e) k+T k) + ηtk •((1 - e) k+T k))
=Lk+ι(£-1〃o •(1 - (1 - e) k+T k) + ς⅛1 μtk •((1 - e) k+T k))
G Pseudo-Algorithm
We provide the pseudo algorithm of GRS (Sec. 3) with Bayesian forgetting in Alg. 1. The compu-
tational complexity (at each of the K time-steps) is dominated by i) the minimisation of the KL
19
Published as a conference paper at ICLR 2020
divergence, ii) estimating the Gaussian factors, and iii) scoring the memory. The KL minimisation
requires Itk sequential iterations with Ntk + M data samples and Ktrain Monte Carlo samples. The
latter can both be processed in parallel on parallel hardware. The estimation of the Gaussian factors
requires Ntk + M sequential iterations with Kterm parallel Monte Carlo samples. The dominating
computation for calculating the scores is the evaluation of the likelihood for Ntk + M data samples
and Ktrain Monte Carlo samples, both of which can be processed in parallel. The highest scoring can-
didate memory is given by the top-M highest scoring data points, thus, the computational complexity
of Eq. (8) is only linear in the number of samples.
Algorithm 1 Gaussian Residual Scoring with Bayesian forgetting. The function EstimateGaussian-
Factors corresponds to Eqs. (5a), (5b) (cf. also App. C). The function ApplyForgetting corresponds
to Eq. (12). Note that ptk (w) includes the adapted likelihood of the memory and all subsequent
functions involving the memory use this adapted likelihood.
Inputs: p0(w), qθt0 (w), τ, K
for k in [0...K] do
tk = GetTimeStamp(k)
∆tk = tk - tk-1
Dtk = GetData(tk)
if k == 0 then
ptk (w) = p0 (w)
else
Ptk (W) = ApplyForgetting(po(w),qθtk-ι, Mt—, ∆tk) {Sec. 4.1}
end if
加tk (w) = argmi%θ KL [qθ(W) | Z-Iptk (W)p(DtJw)] {Sec. 3.2, Sec. 4.1}
{rtk (w; dtk )}dtk ∈Dtk∪Mtk-1 = EstimateGaussianFactors(⅞θtk (w), Dtk, Mtk-I) {Sec. 3.1}
Mtk = argmaxM Stk (M; Dtk∪ Mtk-1) {Sec. 3.2}
if IDtk I ≤ Mtkl then
qθtk (W) =ptk(W)	dtk 6∈ Mtk rtk (W; dtk) {Sec. 3.3}
else
qθtk (W) = qθtk (W)/ Qdtk ∈Mtk rtk(w; dtk) {Sec. 3.3}
end if
end for
H Prior parameters
Here we develop a heuristic to choose the initial prior p0 (W). As this will not be specific to the online
or continual setting, we drop the time index in this section, denoting the prior as p(W). Furthermore,
we consider only Gaussian distributions with a diagonal covariance matrix. Assume that the data
is standardised, that is, the first two moments are zero and one. A reasonable choice for the prior
parameters is such that the first two moments of the prior-predictive distribution equals the first two
moments of the data distribution. We go one step further and constrain the pre-activations of every
neural network layer to have moments zero and one. Denote all weight matrices and weight biases
by W = {Wl}l ∪ {bl}l, and let x0 denote the input data. Let us further denote the pre-activation
(before non-linearity) of layer l and unit i as follows.
xli =	Wli,jfl-1(xlj-1)+bli.
j
The constraints are then given as follows.
E
W 〜P(W)
[Exο~p(D)
0,
W 〜P(W)
艮0 〜。(D)[(xi)2]]
1.
E
The first constraint can be easily fulfilled by setting the prior mean to zero for all parameters.
μi,j = 0.
20
Published as a conference paper at ICLR 2020
This follows immediately from Wl ⊥ fl-1(xl-1) and the expectation of products of independent
random variables. The second moment can then be calculated as follows.
Ew~p(w),xo~p(D)
Ew~p(w) [Exo~p(D) [(xli)2]
Ew~p(w)
~p(D) [xi] + 0
Ew~p(w)
VarX0 ~p (D)
Nl-1
X Wli,jfl-1(xlj-1)+bli
j
Ew~p(w)
Nl-1
Nl-1	2
X (Wij • Varx0~p(D) fi-i(xj-i)]
j
l-1
X Ew~p(w)	(Wij
j
Nl-1
E E EW~p(w)
j
• Varxo~p(D) fl-1 (xlj-1)
Varxo~p(D) fl-1(Xj-I)]
Nl-1
:E Ew~p(w)
j
• Ew-p(w)
Nl-1
cf1-1 • X varw~p(w) [Wi,j]
j
N1-1 ∙ Cfι-ι ∙ VaMp(w) IWj .
Here we introduced cfl-1 to denote a correction factor for the non-linearity fl-1. In case of the
linear function, we will have cfl-1 = 1. For arbitrary non-linearities, we can estimate this factor
numerically, assuming that the pre-activations are distributed according to N(0, 1).
cf1-1 = Varxj-1~N(0,1) fl-1(Xj-I)]
This can be done beforehand and the factors for common activation functions can be stored in a
lookup table. Finally, plugging in the constraint for the second moment in the above equation, we
obtain the following prior variance.
(σi,j)2 = Vajw)[亚了] = N;「	(17)
I Posterior Initialisation
It is known that a proper initialisation of standard neural networks is crucial for the optimisation
process Glorot & Yoshua Bengio (2010); He et al. (2015). In Bayesian neural networks, the matter
becomes even more complicated, since we have to deal additionally with the variance of the Monte
Carlo estimator due to re-parametrisation. Analogous to the choice of prior parameters, we seek a
posterior initialisation that yields the first two moments of zero and one. A naive attempt would be to
initialise the posterior with the prior parameters. However, the significant noise in the Monte Carlo
estimation typically leads to bad optimisation results and even numerical instabilities. We propose an
initialisation method which fulfills our constraints but allows us determine the variance of the initial
posterior with two hyperparameters α and β .
Let Us denote the mean and log-scale parameters of the approximate posterior as θ = {θμ, θ]0g σ}.
We choose the following initialisation distributions.
q(w) = N(w; θμ, e2θlogσ),
where
p(θμ) = N(θμ; μθμ ,σθμ),
21
Published as a conference paper at ICLR 2020
and
p(θlog σ ) = N(θlog σ ; μθ∖ogσ , σ2log σ ) .
Here and in the following, we dropped the time index for the approximate posterior, as well as the
indices l, i, and j for the model parameters θ.
We follow a similar derivation as in Sec. H. As for the prior, the mean of the initialisation distribution
will be zero for all parameters.
μθμ = 0.
For the second moment, the derivation is as follows.
Eθ〜p(θ),W〜q(W∣θ),xo〜P(D) [(Xl) = = Eθ〜p(θ)
EW 〜q(w∣θ)
Eθ 〜p(θ)
EW 〜q(w| θ)
Nl-1
VarxO 〜P(D)	X Wli,j •fl-1(xlj-1) +0
j
Eθ 〜p(θ)
Eθ 〜p(θ)
EW 〜q(w∣θ)
EW 〜q(w| θ)
Nl-1
X (Wli,j)2
j
Nl-1
X (Wli,j)2
j
VarxO 〜P(D) [xj-i]
1]'Eθ 〜。(θ)
EW 〜q(w| θ)
VarXo~p(D) fl-1 (xlj-1)
Nl-1
E eθ 〜p(θ)
j
Nl-1
EW 〜q(W∣θ) (Wli,j)2
：X eθ〜p(θ) [θμ+…> cfl-1
j
Nl-1
•	cfl-1
Nl-1
Nl-1
•	cfl-1
Nl-1
•	cfl-1
Nl-1
•	cfl-1
•	Eθ 〜p(θ)
•	Eθ〜p(θ) [θμ + e2∙θlogσ]
)hθμ i+ Eθ 〜p(θ) he2∙θlog
• cfl-1
EW 〜q(w∣θ)
V	arxO〜P(D) fl-1(Xj-I)]
•	(Eθ〜p(θ) [θμ]2 + Varθ〜p(θ) [θμ] + e2E[θlog σ]+2var[θlog σ]
•	(〃2. 十 σ2μ 十 e2"θlθgσ +2σθlogσ
•卜24 + e2μθlogσ +2σ¾ogσ
Hence, the second constraint is as follows.
-----1------- = σ2 + e2μθlog σ +2σ 力og σ
Nl-1 • cfl-1	θμ +
In contrast to Sec. H, we are now under-constrained by 2 parameters. We therefore introduce two
hyperparameters α and β. We first determine α := σθlog σ, for which we generally choose small
values α ≈ 0 (α = 0 corresponds to initialising all posterior variances in the given layer with the
same value). The second hyperparameter β ∈ [0, 1] determines how much of the total variance is
due to the variance of the location parameter and how much variance is due to the variance of the
log-scale parameter. Inserting α and introducing β we obtain the following equations.
L 2	_	β
θμ = N1- cfl-1,
and
e2μθlog σ +2α2 = 1 - β
Nl-1 • cfl-1
22
Published as a conference paper at ICLR 2020
Solving the last equation for 仙加区 σ, the result is as follows.
1	(1 - β) ∙ e-2α2
4θlogb = lo l°g F---------------
g 2	Nl-1 ∙ Cfl-I
We choose α = 0.001 and β = 0.999 in all experiments.
A note on the relation to initialisation methods for deterministic neural networks. Our result is
similar to the initialisation methods from Glorot & Yoshua Bengio (2010) and He et al. (2015). The
difference is in the correction factor cfl-1 . Whereas Glorot & Yoshua Bengio (2010) considers linear
functions (or tanh in the linear regime), both methods base their derivation on the assumption that
every data sample x0 is processed by a different, random neural network with independent weights,
drawn from the initialisation distribution. The assumption is made explicit in (He et al., 2015) by the
use of the variance of products of independent variables rule. We note that this assumption is false
for both the initialisation of deterministic neural networks, as well as the graphical model assumption
in Bayesian neural networks. Consequently, (He et al., 2015) obtains different correction factors (in
their case for relu and leaky relu), taking into account the mean after the forward-pass through the
non-linearity.
23