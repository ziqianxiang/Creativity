Published as a conference paper at ICLR 2020
RNNs Incrementally Evolving on an
Equilibrium Manifold: A Panacea for
Vanishing and Exploding Gradients ?
Anil Kag
ECE Department
Boston University
Boston, MA 02215, USA
anilkag@bu.edu
Ziming Zhang
ECE Department
Worcester Polytechnic Institute
Worcester, MA 01609, USA
zzhang15@wpi.edu
Venkatesh Saligrama
ECE Department
Boston University
Boston, MA 02215, USA
srv@bu.edu
Ab stract
Recurrent neural networks (RNNs) are particularly well-suited for modeling long-
term dependencies in sequential data, but are notoriously hard to train because the
error backpropagated in time either vanishes or explodes at an exponential rate.
While a number of works attempt to mitigate this effect through gated recurrent
units, skip-connections, parametric constraints and design choices, we propose a
novel incremental RNN (iRNN), where hidden state vectors keep track of incre-
mental changes, and as such approximate state-vector increments of Rosenblatt’s
(1962) continuous-time RNNs. iRNN exhibits identity gradients and is able to
account for long-term dependencies (LTD). We show that our method is computa-
tionally efficient overcoming overheads of many existing methods that attempt to
improve RNN training, while suffering no performance degradation. We demon-
strate the utility of our approach with extensive experiments and show competitive
performance against standard LSTMs on LTD and other non-LTD tasks.
1	Introduction
Recurrent neural networks (RNNs) in each round store a hidden state vector, hm ∈ RD , and upon
receiving the input vector, xm+1 ∈ Rd, linearly transform the tuple (hm, xm+1) and pass it through
a memoryless non-linearity to update the state over T rounds. Subsequently, RNNs output an
affine function of the hidden states as its prediction. The model parameters (state/input/prediction
parameters) are learnt by minimizing an empirical loss. This seemingly simple update rule has had
significant success in learning complex patterns for sequential input data.
Nevertheless, that training RNNs can be challenging, and that performance can be uneven on tasks
that require long-term-dependency (LTD), was first noted by Hochreiter (1991), Bengio et al. (1994)
and later by other researchers. Pascanu et al. (2013b) attributed this to the fact that the error gradient
back-propagated in time (BPTT), for the time-step m, is dominated by product of partials of hidden-
state vectors, QT=-m1 "hɪ+1, and these products typically exhibit exponentially vanishing decay or
explosion, resulting in incorrect credit assignment during training and test-time.
Rosenblatt (1962), on whose work we draw inspiration from, introduced continuous-time RNN
(CTRNN) to mimic activation propagation in neural circuitry. CTRNN dynamics evolves as follows:
τg(t) = -αg(t) + φ(Ug(t) + Wx(t) + b), t ≥ t0.	(1)
Here, χ(t) ∈ Rd is the input signal, g(t) ∈ RD is the hidden state vector of D neurons, gi(t) is the
rate of change of the i-th state component; τ, α ∈ R+, referred to as the post-synaptic time-constant,
impacts the rate of a neuron’s response to the instantaneous activation φ(U g(t) + W x(t) + b); and
U ∈ RD×D, W ∈ RD×d, b ∈ RD are model parameters. In passing, note that recent RNN works
that draw inspiration from ODE’s (Chang et al., 2019) are special cases of CTRNN (τ = 1, α = 0).
1
Published as a conference paper at ICLR 2020
Vanishing Gradients. The qualitative aspects of the CTRNN dynamics is transparent in its integral
form:
g(t) = e-αt-τt0g(to) + 1 Z e-α二φ(Ug(s) + Wx(s) + b)ds
τ t0
(2)
This integral form reveals that the partials of hidden-state vector with respect to the initial condition,
∂g(tt), gets attenuated rapidly (first term in RHS), and so We face a vanishing gradient problem. We
will address this issue later but we note that this is not an artifact of CTRNN but is exhibited by ODEs
that have motivated other RNNs (see Sec. 2).
Shannon-Nyquist Sampling. A key property of CTRNN is that the time-constant τ together With
the first term -g(t), is in effect a loW-pass filter With bandWidth ατ-1 suppressing high frequency
components of the activation signal, φ((U g(s)) + (W x(s)) + b). This is good, because, by virtue
of the Shannon-Nyquist sampling theorem, We can noW maintain fidelity of discrete samples With
respect to continuous time dynamics, in contrast to conventional ODEs (α = 0). Additionally, since
high-frequencies are already suppressed, in effect We may assume that the input signal x(t) is sloWly
varying relative to the post-synaptic time constant τ .
Equilibrium. The combination of loW pass filtering and sloWly time varying input has a significant
bearing. The state vector as Well as the discrete samples evolve close to the equilibrium state, i.e.,
g(t) ≈ φ(U g (t) + W x(t) + b) under general conditions (Sec. 3).
Incremental Updates. Whether or not system is in equilibrium, the integral form in Eq. 2 points
to gradient attenuation as a fundamental issue. To overcome this situation, We store and process
increments rather than the cumulative values g(t) and propose dynamic evolution in terms of in-
crements. Let us denote hidden state sequence as hm ∈ RD and input sequence xm ∈ Rd. For
m = 1, 2, . . . , T, and a suitable β > 0
τg(t) = -α(g(t) ± hm-i) + φ(U(g(t) ± hm-i) + Wxm + b), g(0) =0,t ≥ 0	(3)
hm , hmτ , g(β ∙ T)
Intuitively, Say system is in equilibrium and -α(μ(xm/, hm-ι))+φ(Uμ(xm, hm-ι)+Wxm+b) = 0.
We note state transitions are marginal changes from previous states, namely, hm = μ(xm, hm-ι)-
hm-1. NoW for a fixed input xm, as to Which equilibrium is reached depends on hm-1, but are
nevertheless finitely many. So encoding marginal changes as states leads to “identity” gradient.
Incremental RNN (iRNN) achieves Identity Gradient. We propose to discretize Eq. 3 to realize
iRNN (see Sec. 3). At time m, it takes the previous state hm-1 ∈ RD and input xm ∈ Rd and
outputs hm ∈ RD after simulating the CTRNN evolution in discrete-time, for a suitable number of
discrete steps. We shoW that the proposed RNN approximates the continuous dynamics and solves the
vanishing/exploding gradient issue by ensuring identity gradientIn general, We consider tWo options,
SiRNN, Whose state is updated With a single CTRNN sample, similar to vanilla RNNs, and, iRNN,
With many intermediate samples. SiRNN is Well-suited for sloWly varying inputs.
Contributions. To summarize, We list our main contributions:
(A)	iRNN converges to equilibrium for typical activation functions. The partial gradients of hidden-
state vectors for iRNNs converge to identity, thus solving vanishing/exploding gradient problem!
(B)	iRNN converges rapidly, at an exponential rate in the number of discrete samplings of Eq. 1.
SiRNN, the single-step iRNN, is efficient and can be leveraged for sloWly varying input sequences. It
exhibits fast training time, has feWer parameters and better accuracy relative to standard LSTMs.
(C)	Extensive experiments on LTD datasets shoW that We improve upon standard LSTM accuracy as
Well as other recent proposals that are based on designing transition matrices and/or skip connections.
iRNNs/SiRNNs are robust to time-series distortions such as noise paddings
(D)	While our method extends directly (see Appendix A.1) to Deep RNNs, We deem these extensions
complementary, and focus on single-layer to highlight our incremental perspective.
2	Related Work
Gated Architectures. Long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) is
Widely used in RNNs to model long-term dependency in sequential data. Gated recurrent unit
(GRU) (Cho et al., 2014) is another gating mechanism that has been demonstrated to achieve similar
2
Published as a conference paper at ICLR 2020
performance of LSTM with fewer parameters. Some recent gated RNNs include UGRNN (Collins
et al., 2016), and FastGRNN (Kusupati et al., 2018). While mitigating vanishing/exploding gradients,
they do not eliminate it. Often, these models incur increased inference, training costs, and model size.
Unitary RNNs. Arjovsky et al. (2016); Jing et al. (2017); Zhang et al. (2018); Mhammedi et al. (2016)
focus on designing well-conditioned state transition matrices, attempting to enforce unitary-property,
during training. Unitary property does not generally circumvent vanishing gradient (Pennington et al.
(2017)). Also, it limits expressive power and prediction accuracy while also increasing training time.
Deep RNNs. These are nonlinear transition functions incorporated into RNNs for performance
improvement. For instance, Pascanu et al. (2013a) empirically analyzed the problem of how to
construct deep RNNs. Zilly et al. (2017) proposed extending the LSTM architecture to allow step-to-
step transition depths larger than one. Mujika et al. (2017) proposed incorporating the strengths of
both multiscale RNNs and deep transition RNNs to learn complex transition functions. While Deep
RNNs offer richer representations relative to single-layers, it is complementary to iRNNs.
Residual/Skip Connections. Jaeger et al. (2007); Bengio et al. (2013); Chang et al. (2017); Campos
et al. (2017); Kusupati et al. (2018) feed-forward state vectors to induce skip or residual connections,
to serve as a middle ground between feed-forward and recurrent models, and to mitigate gradient
decay. Nevertheless, these connections, cannot entirely eliminate gradient explosion/decay. For
instance, Kusupati et al. (2018) suggest hm = αmhm-1 + βm φ(U hm-1 + Wxm + b), and learn
parameters so that αm ≈ 1 and βm ≈ 0. Evidently, this setting can lead to identity gradient, observe
that setting βm ≈ 0, implies little contribution from the inputs and can conflict with good accuracy,
as also observed in our experiments.
Linear RNNs. (Bradbury et al., 2016; Lei et al., 2018; Balduzzi & Ghifary, 2016) focus on speeding
up RNNs by replacing recurrent connections, such as hidden-to-hidden interactions, with light weight
linear components. This reduces training time, but results in significantly increased model size. For
example, Lei et al. (2018) requires twice the number of cells for LSTM level performance.
ODE/Dynamical Perspective. Few ODE inspired architectures attempt to address stability, but do
not end up eliminating vanishing/exploding gradients. Talathi & Vartak (2015) proposed a modified
weight initialization strategy based on a dynamical system perspective on weight initialization process
to successfully train RNNs composed of ReLUs. Niu et al. (2019) analyzed RNN architectures using
numerical methods of ODE and propose a family of ODE-RNNs. Chang et al. (2019), propose
Antisymmetric-RNN. Their key idea is to express the transition matrix in Eq. 1, for the special
case α = 0, τ = 1, as a difference: U = V - VT and note that the eigenspectrum is imaginary.
Nevertheless, Euler discretization, in this context leads to instability, necessitating damping of the
system. As such vanishing gradient cannot be completely eliminated. Its behavior is analogous to
FastRNN Kusupati et al. (2018), in that, identity gradient conflicts with high accuracy. In summary,
we are the first to propose evolution over the equilibrium manifold, and demonstrating identity
gradients. Neural ODEs (Chen et al., 2018; Rubanova et al., 2019) have also been proposed for
time-series prediction to deal with irregularly sampled inputs. They parameterize the derivative of the
hidden-state in terms of an autonomous differential equation and let the ODE evolve in continuous
time until the next input arrives. As such, this is not our goal, our ODE explicitly depends on the
input, and evolves until equilibrium for that input is reached. We introduce incremental updates to
bypass vanishing/exploding gradient issues, which is not of specific concern for these works.
3	Method
We use Euler’s method to discretize Eq. 3 in steps δ = ητ. Denoting the kth step as gk = g(kδ)
T —~~-ι-1 =	-α(gk-1	+	hm-1) +	φ(U(gk-1 + hm-1)	+ Wxm	+ b),	k ∈	[K]	3 (4)
δ
Rearranging terms we get a compact form for iRNN (see Fig. 1). In addition we introduce a learnable
parameter ηmk and let it be a function of time m and the recursion-step k.
gk = gk-1 + ηmk (φ(U (gk-1 + hm-1 ) + Wxm + b) - α(gk-1 + hm-1 )), k ∈ [K]	(5)
hm = gK
We run the recursion for k ∈ [K] with some suitable initial condition. This could be g0 = 0 or
initialized to the previous state, i.e., g0 = hm-1	at time m.
3
Published as a conference paper at ICLR 2020
go = h
X
m
Figure 1: iRNN depicted by unfolding into K recursions for one transition from g0 = hm-1 to
hm = gκ. Here,2(x, g, h) = φ(U(g + h) + Wx + b) - α(g + h). See Sec. A.2 for implementation
and pseudo-code. This resembles Graves (2016), who propose to vary K with m as a way to attend
to important input transitions. However, the transition functions used are gated units, unlike our
conventional ungated functions. As such, while this is not their concern, equilibrium may not even
exist and identity gradients are not guaranteed in their setup.
In many of our examples, we find the input sequence is slowly varying, and K = 1 can also realize
good empirical performance. We refer to this as single-step-incremental-RNN (SiRNN).
h1m = g0 + ηm(φ(U(g0 + hm-1) + Wxm + b) - α(hm-1 + g0))	(6)
For both iRNN and SiRNN we drop the superscript whenever it is clear from the context.
Root Finding and Transitions. The two indices k and m should not be confused. The index
m ∈ [T] refers to the time index, and indexes input, xm and hidden state hm over time horizon T.
The index k ∈ [K] is a fixed-point recursion for converging to the equilibrium solution at each time
m, given input xm and the hidden state hm-1. We iterate over k so that at k = K, gK satisfies,
φ(U (gK + hm-1) + Wxm + b) - α(gK + hm-1) ≈ 0
The recursion (Eq. 5) at time m runs for K rounds, terminates, and recursion is reset for the new input,
xm+1. Indeed, Eq. 5 is a standard root-finding recursion, with gk-1 serving as the previous solution,
plus a correction term, which is the error, φ(U (gk-1 + hm-1) + Wxm + b) - α(gk-1 + hm-1). If
the sequence converges, the resulting solution is the equilibrium point. Proposition 2 guarantees a
geometric rate of convergence.
Identity Gradient. We will informally (see Theorem 1) show here that partial gradients are identity.
Say we have for sufficiently large K, hm = gK is the equilibrium solution. It follows that,
φ(U (hm + hm-1 ) + Wxm + b) - α(hm + hm-1 )) = 0
Taking derivatives, we have,
vφ(∙)U (	dhm	+	I)	-α	(	dhm	+	I)	=0	=⇒	(Vφ(∙)U - αI)	(	dhm	+	I)	=0.⑺
∂hm-1	∂hm-1	∂hm-1
Thus if the matrix (Vφ(∙)U - ɑI) is not singular, it follows that (∂hhm1 + I )=0.
SiRNN vs. iRNN. SiRNN approximates iRNN. In particular, say xm is a constant in the segment,
m ∈ [m0, m0 + K], then SiRNN trajectory of hidden states, denoted as h1m +K is equal to the iRNN
hidden state hKm , when both SiRNN and iRNN are initialized with g0 = hm-1. Thus, for slowly
time-varying inputs we can expect SiRNN to closely approximate iRNN.
Residual Connections vs. iRNN/SiRNN. As such, our architecture is a special case of skip/residual
connections. Nevertheless, unlike skip connections, our connections are structured, and the dynamics
driven by the error term ensures that the hidden state is associated with equilibrium and leads to
identity gradient. No such guarantees are possible with unstructured skip connections. Note that for
slowly varying inputs, after a certain transition-time period, we should expect SiRNN to be close to
equilibrium as well. Without this imposed structure, general residual architectures can learn patterns
that can be dramatically different (see Fig. 2).
3.1 Identity Gradient Property and Convergence Guarantees.
Let us now collect a few properties of Eq. 3 and Eq. 5. First, denote the equilibrium solutions for an
arbirary input x ∈ Rd, arbitrary state-vector ν ∈ RD, in an arbitrary round:
Meq(x, V) = {μ ∈ RD | α(μ + V) = φ(U(μ + V) + Wx + b)}
4
Published as a conference paper at ICLR 2020
Whenever the equilibrium set is a singleton, we denote it as a function heq(x, ν). For simplicity, we
assume below that ηki is a positive constant independent of k and i.
Proposition 1. Suppose, φ(∙) is a I-LiPshitzfunction in the norm induced by ∣∣ ∙ ∣∣, and ∣∣U∣∣ < α,
then for any xm ∈ Rd and hm-1 ∈ RD, it follows that Meq (x, ν) is a singleton and as K → ∞,
the iRNN recursions converge to this solution, namely, hm = limK→∞ gK = heq (xm, hm-1)
Proof. Define T : RD → Rd, with T(g) = (1 - ηα)g + η(φ(U (g + hm-1) + Wxm + b) - hm-1).
It follows that T(∙) is a contraction:
∣T(g) - T(g0)∣ ≤(1 - ηα)∣g - g0∣ + η ∣φ(U (g + hm-1) + Wxm + b) - φ(U (g0 + hm-1) + Wxm + b)∣
≤(1-ηα+∣U∣η)∣g-g0∣ <∣g-g0∣.
We now invoke the Banach fixed point theorem, which asserts that a contractive operator on a
complete metric space converges to a unique fixed point, namely, TK (g) → g*. Upon substitution,
we see that this point g* must be such that, φ(U(g* + hm-ι) + Wxm + b) -(g* + hm-ι) = 0. Thus
equilibrium point exists and is unique. Result follows by setting hm，heq(xm, hm-ι).	口
Handling ∣U ∣ ≤ α. In experiments, we set α = 1, and do not enforce ∣U∣ ≤ α constraint. Instead,
we initialize U as a Gaussian matrix with IID mean zero, small variance components. As such, the
matrix norm is smaller than 1. Evidently, the resulting learnt U matrix does not violate this condition.
Next we show for η > 0, iRNN converges at a linear rate, which follows directly from Proposition 1.
Proposition 2. Under the setup in Proposition 1, it follows that,
∣hm - heq (xm , hm-1 )∣ , ∣gK - heq (xm , hm-1 )∣ ≤ (1 - αη + η∣U ∣) ∣g1 - heq (xm , hm-1 )∣
Remark. Proposition 1 accounts for typical activation functions ReLU, tanh, sigmoids as well as
deep RNNs (appendix A.1).
In passing we point out that, in our experiments, we learn parameters ηmk , and a result that accounts
for this case is desirable. We describe this case in Appendix A.3. A fundamental result we describe
below is that partials of hidden-state vectors, on the equilibrium surface is unity. For technical
simplicity, we assume a continuously differentiable activation, which appears to exclude ReLU
activations. Nevertheless, we can overcome this issue, but requires more technical arguments. The
main difficulty stems from ensuring that derivatives along the equilibrium surface exist, and this can
be realized by invoking the implicit function theorem (IFT). IFT requires continuous differentiability,
which ReLUs violate. Nevertheless, recent results 1 suggests that one can state implicit function
theorem for everywhere differentiable functions, which includes ReLUs.
Theorem 1. Suppose φ(∙) is a continuously differentiable, I-LiPShitzfunction, with ∣∣U∣∣ < α. Then
as K → ∞,舶m------------→ dheq (xm,hm-I) = -I. Furthermore, as K → ∞ the partial gradients over
∂ hm-1	∂ hm-1
arbitrary number of rounds for iRNN is identity.
dhr — TT	dhm — / 1∖r-sτ 一
∂hs = J ∂hm-i =(I)I ⇒
r≥m>s
∂hτ
∂hS
1.
(8)
Proof. Define, ψ(g, hm-1) = φ(U (g + hm-1) + Wxm + b) - α(g + hm-1). We overload notation
and view the equilibrium point as a function of hm-ι, i.e., g*(hm,-ι) = heq(xm, hm-ι). Invoking
standard results2 in ODE's, it follows that g*(hm,-ι) is a smooth function, so long as the Jacobian,
Vg ψ(g*, hm-ι) with respect to the first coordinate, g*, is non-singular. Upon computation, we see
that, Vg ψ(g*, hm-ι) = Vφ(g*, hm-ι)U — αI, is non-singular, since ∣∣Vφ(g* ,hm-ι)U ∣ ≤ ∣∣U∣∣.
It follows that we can take partials of the state-vectors. By taking the partial derivatives w.r.t. hm-1
in Eq. 5, at the equilibriumpoints we have [Vφ(g*, hm-ι)U — αI][∂^——+1] = 0 (see Eq. 7). The
∂hm-1
rest of the proof follows by observing that the first term is non-singular.	口
1terrytao.wordpress.com/2011/09/12/the-inverse-function-theorem-for-everywhere-differentiable-maps/
2http://cosweb1.fau.edu/~jmirelesjames/ODE_course/lectureNotes_
shortVersion_day1.pdf
5
Published as a conference paper at ICLR 2020
Figure 2: Phase-space trajectory with tanh activa-
tion of RNN, FastRNN, iRNN. X-axis denotes 1st
dimension, and Y-axis 2nd dimension of 2D hidden
state subject to random walk input with variance
10 for 1000 time-steps. Parameters U, W, b are ran-
domly initialized. RNN states are scaled to fit plot
since FastRNN is not required to be in the cube.
r = State_Space_on_lD_Random_Wa lk_i RN N_vs_RN Ns
Remark. We notice that replacing hm-ι with 虫
-hm-ι in Eq. 12 will lead to 肃\ = I, which
also has no impact on magnitudes of gradients. As
a result, both choices are suitable for circumventing
vanishing or exploding gradients during training,
but still may converge to different local minima and
thus result in different test-time performance. Fur-
thermore, notice that the norm preserving property
is somewhat insensitive to choices of α, so long as
the non-singular condition is satisfied.
3.2 iRNN Design Implications:
Low-Rank Model Parametrization
Fig. 2 depicts phase portrait and illustrates salient
differences between RNN, FastRNN (RNN with
skip connection), and iRNN (K=5). RNN and Fas-
tRNN exhibit complex trajectories, while iRNN trajectory is smooth, projecting initial point (black
circle) onto the equilibrium surface (blue) and moving within it (green). This suggests that iRNN
trajectory belongs to a low-dimensional manifold.
Variation of Equilibrium w.r.t. Input. As before, heq be an equilibrium solution for some tuple
(hm-1, xm). It follows that,
(αI — Vφ(U(heq + hm-1)+ WXm + b) U)∂heq = Vφ(U(heq + hm-l) + WXm + b)W∂Xm
This suggests that, whenever the input undergoes a slow variation, we expect that the equilibrium
point moves in such a way that U∂heq must lie in a transformed span of W. Now W ∈ RD×d with
d D, which implies that (αI - Vφ(U (heq + hm-1) + WXm + b)U is rank-deficient.
Low Rank Matrix Parameterization. For typical activation functions, note that whenever the
argument is in the unsaturated regime, Vφ(∙) ≈ I. We then approximately get span(αI — U) ≈
span(W). We can express these constraints as U = αI + V H with low-rank matrices V ∈
RD×d1 , H ∈ Rd1 ×D, and further map both Uhm and WXm onto a shared space. Since in our
experiments the signal vectors we encounter are low-dimensional, and sequential inputs vary slowly
over time, we enforce this restriction in all our experiments. In particular, we consider,
φ (P [U (hm + hm-1) + WXm + b]) - (hm + hm-1) = 0.	(9)
The parameter matrix P ∈ RD×D maps the contributions from input and hidden states onto the same
space. To decrease model-size we let P = U = (I + V H) learn these parameters.
4	Experiments
We organize this section as follows. First, the experimental setup, competing algorithms will be
described. Then we present an ablative analysis to highlight salient aspects of iRNN and justify some
of our experimental choices. We then plot and tabulate experimental results on benchmark datasets.
4.1	Experimental Setup and Baselines
Choice of Competing Methods: We choose competing methods based on the following criteria:
(a) methods that are devoid of additional application or dataset-specific heuristics, (b) methods that
leverage only single cell/block/layer, and (c) methods without the benefit of complementary add-ons
(such as gating, advanced regularization, model compression, etc.). Requiring (a) is not controversial
since our goal is methodological. Conditions (b),(c) are justifiable since we could also leverage
these add-ons and are not germane to any particular method3. We benchmark iRNN against standard
RNN, LSTM (Hochreiter & Schmidhuber, 1997), (ungated) AntisymmetricRNN (Chang et al., 2019),
(ungated) FastRNN (Kusupati et al., 2018).
3These conditions eliminate some potential baselines. We provide specific justifications in the appendix A.5.
6
Published as a conference paper at ICLR 2020
Trainina Ste□s
Sequence length = 200
STM
astRNN(eta-0.01)
—FaStRNN (eta-OQ 01)
---Aπtisym(g" 0.01 .β"0.1)
——Antisym(gB 0.01 .β"0.001)
——IRNN(K-I)
——iRNN(K-5)
—IRNN(K-IO)
O 200	400	600	800 IOOO
Training Steps
(a)	(b)
Figure 3: Exploratory experiments for the Add task (a) Convergence with varying K; (b) Ratio ∣∣ ∂T k/k ∂hhTl k
illustrates VaniShing/Exploding gradient (∣ ∂^hTl k and loss gradients are omitted but displayed in A.7.8. For
iRNN (a) and (b) together show strong correlation of gradient with accuracy in contrast to other methods.
Unitary RNN Variants. Results for methods based on unitary transitions (such as Arjovsky et al.
(2016); Wisdom et al. (2016); Vorontsov et al. (2017); Zhang et al. (2018)) are not reported in the
main paper (when available reported in appendix) for the following reasons: (a) They are substantially
more expensive, and requiring large model sizes; (b) Apart from the benchmark copy and add tasks,
results tabulated by FastRNN and Antisymmetric authors (see Zhang et al. (2018); Chang et al.
(2019)) show that they are well below SOTA; (c) iRNN dominates unitary-RNN variants on add-task
(see Sec. 4.3.1); (d) On copy task, while unitary invariants are superior, Vorontsov et al. (2017)
attributes it to modReLU or leaky ReLU activations. Leaky ReLUs allow for linear transitions, and
copy task being a memory task benefits from it. With hard non-linear activation, unitary RNN variants
can take up to 1000’s of epochs for even 100-length sequences (Vorontsov et al. (2017)).
Implementation. For all our experiments, we used the parametrized update formulation in Eq. 9 for
iRNN . We used tensorflow framework for our experiments. For most competing methods apart from
AntisymmetricRNN, which we implemented, code is publicly available. All the experiments were
run on an Nvidia GTX 1080 GPU with CUDA 9 and cuDNN 7.0 on a machine with Intel Xeon 2.60
GHz CPU with 20 cores.
Datasets. Pre-processing and feature extraction details for all publicly available datasets are in the
appendix A.4. We replicate benchmark test/train split with 20% of training data for validation to tune
hyperparameters. Reported results are based on the full training set, and performance achieved on the
publicly available test set. Table 4 (Appendix) and A.4 describes details for all the data sets.
Hyper Parameters We used grid search and fine-grained validation wherever possible to set the
hyper-parameters of each algorithm, or according to the settings published in (Kusupati et al., 2018;
Arjovsky et al., 2016) (e.g. number of hidden states). Both the learning rate and n's were initialized
to 10-2 . The batch size of 128 seems to work well across all the data sets. We used ReLU as the
non-linearity and Adam (Kingma & Ba (2015)) as the optimizer for all the experiments.
4.2	Ablative Analysis
We perform ablative analysis on the benchmark add-task (Sec 4.3.1) for sequence length 200 for
1000 iterations and explore mean-squared error as a metric. Fig. 3 depicts salient results.
(a)	Identity Gradients & Accuracy: iRNN accuracy is correlated with identity gradients. Increasing
K improves gradients, and correlates with increased accuracy (Fig. 3). While other models ht =
αht-1 + βφ((U - γI)ht-1 + Wxt), can realize identity gradients for suitable choices; linear
(α = 1, β = 1, γ = 0, U = 0), FastRNN (α ≈ 1, β ≈ 0, γ = 0) and Antisymmetric (α =
1, β = 1, U = V - V T, kUk ≤ γ), this goal may not be correlated with improved test accuracy.
FastRNN(η = 0.001), Antisymmetric (γ = 0.01, = 0.001) have good gradients but poorer test
accuracy relative to FastRNN(η = 0.01), Antisymmetric(γ = 0.01, = 0.1), with poorer gradients.
(b)	Identity gradient implies faster convergence: Identity gradient, whenever effective, must be
capable of assigning credit to the informative parts, which in turn results in larger loss gradients, and
significantly faster convergence with number of iterations. This is borne out in figure 3(a). iRNN for
larger K is closer to identity gradient with fewer (unstable) spikes (K = 1, 5, 10). With K = 10,
iRNN converges within 300 iterations while competing methods take about twice this time (other
baselines not included here exhibited poorer performance than the once plotted).
7
Published as a conference paper at ICLR 2020
Sequence length = 200
Sequence length = 500
0.40
0.14
0.35
0.12
0.30
费 0.25
W 0.08
e o.o6
0.04
0.10
£ 0.20
io.15
——iRNN
RNN
——LSTM
---Antisymmetric
FastRNN
、0.10
a.
0.02
0.05
Training Steps
Training Steps
(b)
(a)
iRNN
RNN
LSTM
----Antisymmetric
FastRNN
(c)
Figure 4: Following Arjovsky et al. (2016) we display average Cross Entropy for the Copy Task (Sequence
Length (with baseline memoryless strategy)): (a) 200 (0.09) (b) 500 (0.039). Mean Squared Error for the Add
Task, baseline performance is 0.167 (Sequence Length) : (c) 200 (d) 750. For both tasks, iRNN runs K = 5.
Training steps
(d)


(c)	SiRNN (iRNN with K = 1 delivers good performane in some cases. Fig. 3(a) illustrates that
iRNN K = {5, 10} achieves faster convergence than SiRNN, but the computational overhead per
iteration roughly doubles or triples in comparison. SiRNN is faster relative to competitors. For this
reason, we sometimes tabulate only SiRNN, whenever it is SOTA in benchmark experiments, since
accuracy improves with K but requires higher overhead.
4.3	Long-term Dependency and Other Tasks
We list five types of datasets, all of which in some way require effective gradient propagation: (1)
Conventional Benchmark LTD tasks (Add & Copy tasks) that illustrate that iRNN can rapidly learn
long-term dependence; (2) Benchmark vision tasks (pixel MNIST, perm-MNIST) that may not require
long-term, but nevertheless, demonstrates that iRNN achieves SOTA for short term dependencies but
with less resources. (3) Noise Padded (LTD) Vision tasks (Noisy MNIST, Noisy CIFAR), where a
large noise time segment separates information segments and the terminal state, and so the learner
must extract information parts while rejecting the noisy parts; (4) short duration activity embedded in
a larger time-window (HAR-2, Google-30 in Appendix Table 4 and many others A.7), that usually
arise in the context of smart IoT applications and require a small model-size footprint. Chang
et al. (2019) further justify (3) and (4) as LTD, because for these datasets where only a smaller
unknown segment(s) of a longer sequence is informative. (5) Sequence-sequence prediction tasks
(PTB language modeling) that are different from terminal prediction (reported in appendix A.7).
4.3.1	S tandard Benchmark LTD Tasks : Addition & Copy Memory
Addition and Copy tasks (Hochreiter & Schmidhuber, 1997) have long been used as benchmarks in
the literature to evaluate LTD (Hori et al., 2017; Zhang et al., 2018; Arjovsky et al., 2016; Martens &
Sutskever, 2011). We follow the setup described in Arjovsky et al. (2016) to create the adding and
copying tasks. See appendix A.4 for detailed description. For both tasks we run iRNN with K = 5.
Figure 4 show the average performance of various methods on these tasks. For the copying task
we observe that iRNN converges rapidly to the naive baseline and is the only method to achieve
zero average cross entropy. For the addition task, both FastRNN and iRNN solves the addition
task but FastRNN takes twice the number of iterations to reach desired 0 MSE. 4 In both the tasks,
4 Note that LSTM solves the addition problem in Arjovsky et al. (2016) only with more than 10k iterations.
We only use 2k iterations in our experiments to demonstrate the effectiveness of our method.
8
Published as a conference paper at ICLR 2020
iRNN performance is much more stable across number of online training samples. In contrast, other
methods either takes a lot of samples to match iRNN ’s performance or depict high variance in the
evaluation metric. This shows that iRNN converges faster than the baselines (to the desired error).
These results demonstrate that iRNN easily and quickly learns the long term dependencies . We
omitted reporting unitary RNN variants for Add and Copy task. See Sec. 4.1 for copy task. On
Add-task we point out that our performance is superior. In particular, for the longer T = 750 length,
Arjovsky et al. (2016), points out that MSE does not reach zero, and uRNN is noisy. Others either
(Wisdom et al., 2016) do not report add-task or report only for shorter lengths (Zhang et al., 2018).
Table 1: Results for Pixel-by-Pixel MNIST and Permuted MNIST datasets. K denotes pre-defined recursions
embedded in graph to reach equilibrium.
Data set	Algorithm	Accuracy (%)	Train Time (hr)	#ParamS
Pixel-MNIST	FastRNN=	96.44	15.10	33k
	RNN	94.10	45.56	14k
	LSTM	97.81	26.57	53k
	Antisymmetric	98.01	8.61	14k
	iRNN (K=1)	97.73	2.83	4k
	iRNN (K=3)	98.13	2.93	4k
Permute-MNIST	-_FastRNN	92.68	9.32	-^8.75k
	LSTM	92.61	19.31	35k
	Antisymmetric	93.59	4.75	14k
	iRNN (K=1)	95.62	2.41	8k
4.3.2	Non LTD Vision Tasks: Pixel MNIST, Permute MNIST
Next, we perform experiments on the sequential vision tasks: (a) classification of MNIST images on
a pixel-by-pixel sequence; (b) a fixed random permuted MNIST sequence (Lecun et al., 1998). These
tasks typically do not fall in the LTD categories (Chang et al., 2019), but are useful to demonstrate
faster training, which can be attributed to better gradients.
For the pixel-MNIST task, Kusupati et al. (2018) reports that it takes significantly longer time for
existing (LSTMs, Unitary, Gated, Spectral) RNNs to converge to reasonable performance. In contrast,
FastRNN trains at least 2x faster than LSTMs. Our results (table 1) for iRNN shows a 9x speedup
relative LSTMs, and 2x speedup in comparison to Antisymmetric. In terms of test accuracy, iRNN
matches the performance of Antisymmetric, but with at least 3x fewer parameters. We did not gain
much with increased K values5 . For the permuted version of this task, we seem to outperform the
existing baselines 6. In both tasks, iRNN trained at least 2x faster than the strongest baselines. These
results demonstrate that iRNN converges much faster than the baselines with fewer parameters.
4.3.3	Noise padding Tasks: Noisy-MNIST, Noisy-CIFAR
Additionally, as in Chang et al. (2019), we induce LTD by padding CIFAR-10 with noise exactly
replicating their setup, resulting in Noisy-CIFAR. We extend this setting to MNIST dataset resulting
in Noisy-MNIST. Intuitively we expect our model to be resilient to such perturbations. We attribute
iRNN’s superior performance to the fact that it is capable of suppressing noise. For example, say
noise is padded at t > τ and this results in Wxt being zero on average. For iRNN the resulting states
ceases to be updated. So iRNN recalls last informative state hτ (modulo const) unlike RNNs/variants!
Thus information from signal component is possibly better preserved.
Results for Noisy-MNIST and Noisy-CIFAR are shown in Table 2. Note that almost all timesteps
contain noise in these datasets. LSTMs perform poorly on these tasks due to vanishing gradients. This
5 For some existing comparisons LSTM have achieved roughly 98.9 with dataset specific heuristics (Cooij-
mans et al., 2016), but we could not achieve this performance in our comparison (and so have many others like
(Kusupati et al., 2018; Zhang et al., 2018; Arjovsky et al., 2016)).
6Note that there’s no standard permutation in the literature. This may be the main reason we could not
replicate Chang et al. (2019) performance on the permute MNIST task.
9
Published as a conference paper at ICLR 2020
Table 2: Results for Noise Padded CIFAR-10 and MNIST datasets. Since the equilibrium surface is smooth and
resilient to small perturbations, iRNN achieves better performance than the baselines with faster convergence.
Data set	Algorithm	Accuracy (%)	Train Time (hr)	#Params
Noisy-MNIST=	FastRNN	-^98.12^^	8.93	11k
	LSTM	10.31	19.43	44k
	Antisymmetric	97.76	5.21	10k
	iRNN (K=1)	98.48	2.39	6k
Noisy-CIFAR	FastRNN	-^4576^^	11.61	-^16k-
	LSTM	11.60	23.47	64k
	Antisymmetric	48.63	5.81	16k
	iRNN (K=1)	54.50	2.47	11.5k
is consistent with the earlier observations (Chang et al., 2019). iRNN outperforms the baselines very
comprehensively on CIFAR-10, while on MNIST the gains are smaller, as it’s a relatively easier task.
These results show that iRNN is more resilient to noise and can account for longer dependencies.
Table 3: Results for Activity Recognition Datasets. iRNN outperforms the baselines on all metrics even with
K = 1. Its worth noticing that although K = 5 increases test time, it’s well within LSTM’s numbers, the overall
train time and resulting performance are better than K = 1.
Data set	Algorithm	Accuracy (%)	Train Time (hr)	#Params	Test Time (ms)
HAR-2	FastRNN	-^9450^^	0.063	7.5k	0.01
	RNN	91.31	0.114	7.5k	0.01
	LSTM	93.65	0.183	16k	0.04
	Antisymmetric	93.15	0.087	7.5k	0.01
	iRNN (K=1)	95.32	0.061	4k	0.01
	iRNN (K=5)	96.30	0.018	4k	0.03
Google-30	FastRNN	-^91.60^^	1.30	18k	0.01-
	RNN	80.05	2.13	12k	0.01
	LSTM	90.31	2.63	41k	0.05
	Antisymmetric	90.91	0.54	12k	0.01
	iRNN (K=1)	93.77	0.44	8.5k	0.01
	iRNN (K=5)	94.23	0.44	8.5k	0.05
4.3.4	Short Duration Embedded Activity Recognition Tasks: HAR-2, Google-30
We are interested in detecting activity embedded in a longer sequence with small footprint RNNs
(Kusupati et al. (2018)): (a) Google-30 (Warden, 2018), i.e. detection of utterances of 30 commands
plus background noise and silence, and (b) HAR-2 (Anguita et al., 2012), i.e. Human Activity
Recognition from an accelerometer and gyroscope on a Samsung Galaxy S3 smartphone.
Table 3 shows accuracy, training time, number of parameters and prediction time. Even with
K = 1, we compare well against competing methods, and iRNN accuracy improves with larger K .
Interestingly, higher K yields faster training as well as moderate prediction time, despite the overhead
of additional recursions. These results show that iRNN outperforms baselines on activity recognition
tasks, and fits within IoT/edge-device budgets.
5	Conclusion
Drawing inspiration from Rosenblatts Continuous RNNs, we developed discrete time incremental
RNN (iRNN). Leveraging equilibrium properties of CTRNN, iRNN solves exploding/vanishing
gradient problem. We show that iRNN improved gradients are directly correlated with improved test
accuracy. A number of experiments demonstrate iRNNs responsiveness to long-term dependency
tasks. In addition, due to its smooth low-dimensional trajectories, it has a lightweight footprint that
can be leveraged for IoT applications.
10
Published as a conference paper at ICLR 2020
Acknowledgments
The authors would like to thank the Area Chair and the reviewers for their constructive comments.
This work was supported partly by the National Science Foundation Grant 1527618, the Office of
Naval Research Grant N0014-18-1-2257 and by a gift from ARM corporation.
References
Kerem Altun, Billur Barshan, and Orkun TungeL Comparative study on classifying human activities
with miniature inertial and magnetic sensors. Pattern Recogn., 43(10):3605-3620, October 2010.
ISSN 0031-3203. doi: 10.1016/j.patcog.2010.04.019. URL http://dx.doi.org/10.1016/
j.patcog.2010.04.019.
Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge L. Reyes-Ortiz. Human
activity recognition on smartphones using a multiclass hardware-friendly support vector ma-
chine. In Proceedings of the 4th International Conference on Ambient Assisted Living and Home
Care, IWAAL’12, pp. 216-223, Berlin, Heidelberg, 2012. Springer-Verlag. ISBN 978-3-642-
35394-9. doi: 10.1007/978-3-642-35395-6_30. URL http://dx.doi.org/10.1007/
978-3-642-35395-6_30.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International Conference on Machine Learning, pp. 1120-1128, 2016.
David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. In Proceedings
of The 33rd International Conference on Machine Learning, pp. 1292-1300, 2016.
Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is
difficult. Trans. Neur. Netw., 5(2):157-166, March 1994. ISSN 1045-9227. doi: 10.1109/72.279181.
URL http://dx.doi.org/10.1109/72.279181.
Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in optimiz-
ing recurrent networks. 2013 IEEE International Conference on Acoustics, Speech and Signal
Processing, pp. 8624-8628, 2013.
James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural
networks. CoRR, abs/1611.01576, 2016. URL http://arxiv.org/abs/1611.01576.
ViCtor Campos, Brendan Jou, Xavier Gir6-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip rnn:
Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834,
2017.
Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. AntisymmetricRNN: A dynamical system
view on recurrent neural networks. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=ryxepo0cFX.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael
Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.
In Advances in Neural Information Processing Systems, pp. 77-87, 2017.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary dif-
ferential equations. In Advances in Neural Information Processing Systems, pp. 6571-6583,
2018.
Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of
neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo. Capacity and Trainability in Recurrent
Neural Networks. arXiv e-prints, art. arXiv:1611.09913, November 2016.
Tim Cooijmans, Nicolas Ballas, CeSar Laurent, CagIar Gulgehre, and Aaron Courville. Recurrent
batch normalization. arXiv preprint arXiv:1603.09025, 2016.
Ron S Dembo, Stanley C Eisenstat, and Trond Steihaug. Inexact newton methods. SIAM Journal on
Numerical analysis, 19(2):400-408, 1982.
11
Published as a conference paper at ICLR 2020
Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: frequency-agnostic
word representation. In Advances in Neural Information Processing Systems, pp. 1334-1345, 2018.
Alex Graves. Adaptive computation time for recurrent neural networks. CoRR, abs/1603.08983,
2016. URL http://arxiv.org/abs/1603.08983.
Michiel Hermans and Benjamin Schrauwen. Training and analysing deep recurrent neu-
ral networks. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp.
190-198. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/
5166-training-and-analysing-deep-recurrent-neural-networks.pdf.
Josef Hochreiter.	Untersuchungen zu dynamischen neuronalen net-
zen.	1991.	URL http://people.idsia.ch/~juergen/
SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang, Bret Harsham, John R Hershey, Tim K
Marks, and Kazuhiko Sumi. Attention-based multimodal fusion for video description. In ICCV,
pp. 4203-4212, 2017.
Herbert Jaeger, Mantas Lukosevicius, Dan Popovici, and Udo Siewert. Optimization and applications
of echo state networks with leaky-integrator neurons. Neural networks : the official journal of the
International Neural Network Society, 20:335-52, 05 2007. doi: 10.1016/j.neunet.2007.04.016.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and
Marin Soljacic. Tunable efficient unitary neural networks (eunn) and their application to rnns. In
International Conference on Machine Learning, pp. 1733-1741, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICML, 2015.
Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma.
Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In
Advances in Neural Information Processing Systems, 2018.
Yann Lecun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278-2324, 1998.
Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly
parallelizable recurrence. In Empirical Methods in Natural Language Processing (EMNLP), 2018.
James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization.
In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 1033-
1040, 2011.
Zakaria Mhammedi, Andrew D. Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal
parametrisation of recurrent neural networks using householder reflections. CoRR, abs/1612.00188,
2016. URL http://arxiv.org/abs/1612.00188.
Asier Mujika, Florian Meier, and Angelika Steger. Fast-slow recurrent neural networks. In Advances
in Neural Information Processing Systems, pp. 5915-5924, 2017.
Murphy Yuezhen Niu, Lior Horesh, and Isaac Chuang. Recurrent neural networks in the eye of
differential equations. arXiv preprint arXiv:1904.12933, 2019.
Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep
recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013a.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International Conference on Machine Learning, pp. 1310-1318, 2013b.
Razvan Pascanu, CagIar GUIgehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep
recurrent neural networks. CoRR, abs/1312.6026, 2013c.
12
Published as a conference paper at ICLR 2020
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 30,pp. 4785-4795. 2017.
F. Rosenblatt. Principles of neurodynamics. Spartan Books, Washington, D.C., 1962.
Yulia Rubanova, Ricky T. Q. Chen, and David Duvenaud. Latent odes for irregularly-sampled time
series. CoRR, abs/1907.03907, 2019. URL http://arxiv.org/abs/1907.03907.
Sachin S Talathi and Aniket Vartak. Improving performance of recurrent neural network with relu
nonlinearity. arXiv preprint arXiv:1511.03771, 2015.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. In ICML, pp. 3570-3578, 2017. URL http:
//proceedings.mlr.press/v70/vorontsov17a.html.
Pete Warden. Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition. arXiv
e-prints, art. arXiv:1804.03209, April 2018.
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-
capacity unitary recurrent neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29,
pp. 4880-4888. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6327-full-capacity-unitary-recurrent-neural-networks.pdf.
Inc. Yelp. Yelp dataset challenge. 2017. URL https://www.yelp.com/dataset/
challenge.
Jiong Zhang, Qi Lei, and Inderjit S. Dhillon. Stabilizing gradients for deep neural networks via
efficient svd parameterization. In ICML, 2018.
Jiong Zhang, Qi Lei, and Inderjit S Dhillon. Stabilizing gradients for deep neural networks via
efficient SVD parameterization. arXiv preprint arXiv:1803.09327, 2018.
Julian Georg Zilly, RUPesh Kumar Srivastava, Jan Koutnik, and Jurgen Schmidhuber. Recurrent
highway networks. In ICML, pp. 4189-4198. JMLR. org, 2017.
A Appendix
A.1 Multi-Layer Deep RNN Networks.
We point out in passing that our framework readily admits deep multi-layered networks within a
single time-step. Indeed our setup is general; it applies to shallow and deep nets; small and large time
steps. As a case in point, the Deep Transition RNN Pascanu et al. (2013c):
hm+1 = fh(hm,xm+1) = φh(WLφL-1(WL-1 . . .W1φ1(Uhm + W xm+1))
is readily accounted by Theorem 1 in an implicit form:
hm+1
fh (hm+1 + hm , xm+1 ) -
hm.
So is Deep-RNN Hermans & Schrauwen (2013). The trick is to transform hm → hm + hm+1 and
hm+1 → hm + hm+1. As such, all we need is smoothness of fh, which has no restriction on # layers.
On the other hand, that we do not have to limit the number of time steps is the point of Theorem 1,
which asserts that the partial differential of hidden states (which is primarily why vanishing/exploding
gradient arises Pascanu et al. (2013b) in the first place) is identity!!
A.2 Pseudo Code and Implementation
Given an input sequence and iRNN model parameters, the hidden states can be generated with the
help of subroutine 1. This routine can be plugged into standard deep learning frameworks such as
Tensorflow/PyTorch to learn the model parameters via back-propagation.
13
Published as a conference paper at ICLR 2020
Algorithm 1: Pseudo Code for computing iRNN hidden states for one input sequence
Data: Input sequence {xm }Tm=1
Require: Number of recursion steps K, Model parameters U, W, b, α, {ηmk }
1	Initial hidden state h0 = 0
2	form= 1 to T do
3	Initialize g0 to zero or hm-1
4	for k = 1 to K do
5	Lgk = gk-1 + ηm (φ(U(gk-1 + hm-1) + Wxm + b) - α(gk-1 + hm-1))
6	_ hm = gK
Output: hidden states {hm}Tm=1
Table 4: Dataset Statistics & Long Term Dependence
Dataset	Avg. Activity Time	InpUt Time	Sequence Ratio	#Train	#Fts	#Steps	#Test
Google-30	25ms	1000ms	3/99	51,088	32	99	6,835
HAR-2	256ms	2560ms	13/128	7,352	9	128	2,947
Noisy-MNIST	28	1000	7/250	60,000	28	1000	10,000
Noisy-CIFAR	32	1000	4/125	60,000	96	1000	10,000
Pixel-MNIST				60,000	1	784	10,000
PermUted-MNIST				60,000	1	784	10,000
A.3 Convergence Guarantees for General Learning Rates.
Theorem 2 (Local Convergence with Linear Rate). Assume that the function F (gi) , φ(U (gi +
hk-1) + Wxk + b) - (gi + hk-1) and the parameter ηk(i) in Eq. 5 satisfies
[ηki)]2kVF(gi)F(gi)k2 + 2ηki6(gi)>VF(gi)F(gi) < 0,∀k,∀i.	(10)
Then there exists > 0 such that if kg0 - heq k ≤ where heq denotes the fixed point, the sequence
gi generated by the Euler method converges to the equilibrium solution in Meq(hk-1, xk) locally
with linear rate.
The proof is based on drawing a connection between the Euler method and inexact Newton methods,
and leverages Thm. 2.3 in Dembo et al. (1982). See appendix Sec. A.8.1 Thm. 3 and Sec. A.7.5 (for
proof, empirical verification).
Corollary 1. If kI + ηk(i)VF(gi)k < 1, ∀k, ∀i, the forward propagation (Eq. 13) is stable and the
sequence {gi} converges locally at a linear rate.
The proof is based on Thm. 2.3 in Dembo et al. (1982), Thm. 2 and Prop. 2 in Chang et al. (2019).
See appendix A.8.1 Corollary. 2
A.4 Dataset Details
Table 4 and table 6 lists the statistics of all the datasets described below.
Google-12 & Google-30: Google Speech Commands dataset contains 1 second long utterances of
30 short words (30 classes) sampled at 16KHz. Standard log Mel-filter-bank featurization with 32
filters over a window size of 25ms and stride of 10ms gave 99 timesteps of 32 filter responses for
a 1-second audio clip. For the 12 class version, 10 classes used in Kaggle’s Tensorflow Speech
Recognition challenge7 were used and remaining two classes were noise and background sounds
(taken randomly from remaining 20 short word utterances). Both the datasets were zero mean - unit
variance normalized during training and prediction.
7https://www.kaggle.com/c/tensorflow-speech- recognition-challenge
14
Published as a conference paper at ICLR 2020
HAR-28: Human Activity Recognition (HAR) dataset was collected from an accelerometer and
gyroscope on a Samsung Galaxy S3 smartphone. The features available on the repository were
directly used for experiments. The 6 activities were merged to get the binarized version. The classes
Sitting, Laying, Walking_Upstairs and Standing, Walking, Walking_Downstairs were merged to
obtain the two classes. The dataset was zero mean - unit variance normalized during training and
prediction.
Penn Treebank: 300 length word sequences were used for word level language modeling task using
Penn Treebank (PTB) corpus. The vocabulary consisted of 10,000 words and the size of trainable
word embeddings was kept the same as the number of hidden units of architecture. This is the setup
used in (Kusupati et al., 2018; Zhang et al., 2018).
Pixel-MNIST: Pixel-by-pixel version of the standard MNIST-10 dataset 9. The dataset was zero
mean - unit variance normalized during training and prediction.
Permuted-MNIST: This is similar to Pixel-MNIST, except its made harder by shuffling the pixels
with a fixed permutation. We keep the random seed as 42 to generate the permutation of 784 pixels.
Noisy-MNIST: To introduce more long-range dependencies to the Pixel-MNIST task, we define a
more challenging task called the Noisy-MNIST, inspired by the noise padded experiments in Chang
et al. (2019). Instead of feeding in one pixel at one time, we input each row of a MNIST image at
every time step. After the first 28 time steps, we input independent standard Gaussian noise for the
remaining time steps. Since a MNIST image is of size 28 with 1 RGB channels, the input dimension
is m = 28. The total number of time steps is set to T = 1000. In other words, only the first 28 time
steps of input contain salient information, all remaining 972 time steps are merely random noise. For
a model to correctly classify an input image, it has to remember the information from a long time ago.
This task is conceptually more difficult than the pixel-by-pixel MNIST, although the total amount of
signal in the input sequence is the same.
Noisy-CIFAR: This is exactly replica of the noise paded CIFAR task mentioned in Chang et al.
(2019). Instead of feeding in one pixel at one time, we input each row of a CIFAR-10 image at
every time step. After the first 32 time steps, we input independent standard Gaussian noise for the
remaining time steps. Since a CIFAR-10 image is of size 32 with three RGB channels, the input
dimension is m = 96. The total number of time steps is set to T = 1000. In other words, only the first
32 time steps of input contain salient information, all remaining 968 time steps are merely random
noise. For a model to correctly classify an input image, it has to remember the information from a
long time ago. This task is conceptually more difficult than the pixel-by-pixel CIFAR-10, although
the total amount of signal in the input sequence is the same.
Addition Task: We closely follow the adding problem defined in (Arjovsky et al., 2016; Hochreiter
& Schmidhuber, 1997) to explain the task at hand. Each input consists of two sequences of length T.
The first sequence, which we denote x, consists of numbers sampled uniformly at random U [0, 1].
The second sequence is an indicator sequence consisting of exactly two entries of 1 and remaining
entries 0. The first 1 entry is located uniformly at random in the first half of the sequence, whilst the
second 1 entry is located uniformly at random in the second half. The output is the sum of the two
entries of the first sequence, corresponding to where the 1 entries are located in the second sequence.
A naive strategy of predicting 1 as the output regardless of the input sequence gives an expected mean
squared error of 0.167, the variance of the sum of two independent uniform distributions.
Copying Task: Following a similar setup to (Arjovsky et al., 2016; Hochreiter & Schmidhuber,
1997), we outline the copy memory task. Consider 10 categories, {ai }i9=0. The input takes the form
of a T + 20 length vector of categories, where we test over a range of values of T. The first 10
entries are sampled uniformly, independently and with replacement from {ai}i7=0, and represent the
sequence which will need to be remembered. The next T - 1 entries are set to a8, which can be
thought of as the ’blank’ category. The next single entry is a9 , which represents a delimiter, which
should indicate to the algorithm that it is now required to reproduce the initial 10 categories in the
output. The remaining 10 entries are set to a8 . The required output sequence consists of T + 10
repeated entries of a8 , followed by the first 10 categories of the input sequence in exactly the same
order. The goal is to minimize the average cross entropy of category predictions at each time step of
8https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+ smartphones
9http://yann.lecun. com/exdb/mnist/
15
Published as a conference paper at ICLR 2020
the sequence. The task amounts to having to remember a categorical sequence of length 10, for T
time steps.
A simple baseline can be established by considering an optimal strategy when no memory is available,
which we deem the memoryless strategy. The memoryless strategy would be to predict a8 for T + 10
entries and then predict each of the final 10 categories from the set {ai }i7=0 i=0 independently and
uniformly at random. The categorical cross entropy of this strategy is 1T+g(^
DSA-1910: This dataset is based on Daily and Sports Activity (DSA) detection from a resource-
constrained IoT wearable device with 5 Xsens MTx sensors having accelerometers, gyroscopes and
magnetometers on the torso and four limbs. The features available on the repository were used for
experiments. The dataset was zero mean - unit variance normalized during training and prediction.
Yelp-5: Sentiment Classification dataset based on the text reviews11. The data consists of 500,000
train points and 500,000 test points from the first 1 million reviews. Each review was clipped or
padded to be 300 words long. The vocabulary consisted of 20000 words and 128 dimensional word
embeddings were jointly trained with the network.
A.5 Baseline Justification
In our experiments section, we stated that some of the potential baselines were removed due to
experimental conditions enforced in the setup. Here we clearly justify our choice. Mostly the
reasoning is to avoid comparing complementary add-ons and compare the bare-bone cells.
•	Cooijmans et al. (2016) is removed since its an add-on and can be applied to any method.
Besides its pixel-mnist results involve dataset specific heuristics.
•	Gong et al. (2018) is also an add-on and hence can be applied to any method.
•	Zilly et al. (2017); Pascanu et al. (2013a); Mujika et al. (2017) denote deep transitioning
methods. They are add-ons for any single recurrent block and hence can be applied to any
recurrent cell.
•	Gating variants of single recurrent cells (Chang et al., 2019; Kusupati et al., 2018) have also
been removed. Since iRNN can be extended to a gating variant and hence its just an add-on.
Table 5: Various hyper-parameters to reproduce results
Dataset Hidden Units
Google-30
HAR-2
Pixel-MNIST
Permuted-MNIST
Noisy-MNIST
Noisy-CIFAR
Addition Task
Copying Task
PTB
8888886
808012121212121225
A.6 Hyper-parameters for reproducibility
We report various hyper-parameters we use in our experiments for reproduciblity. As mentioned
earlier we mainly use ’ReLU’ as the non-linearity and Adam as the optimizer. Apart from this, other
hyper-parameters are mentioned in table 5.
16
Published as a conference paper at ICLR 2020
Table 6: Other Dataset Statistics & Long Term Dependence
Dataset	Avg. Acitivity Time	Input Time	Sequence Ratio	#Train	#Fts	#Steps	#Test
Google-12	25ms	1000ms	3/99	22,246	32	99	3,081
DSA-19	500ms	5000ms	13/125	4,560	45	125	4,560
Yelp-5	20	300	1/15	500,000	128	300	500,000
PTB				929,589	300	300	82,430
(a)
Figure 5: Mean Squared Error shown for the Add Task (Sequence Length) : (c) 100 (d) 400
(b)
A.7 Additional Experiments
A.7.1 Copying and Addition Tasks
Figure 5 shows the results for remaining experiments for the addition task for length 100, 400.
Table 7: Results for Pixel-by-Pixel MNIST and Permuted MNIST datasets. K denotes pre-defined
recursions embedded in graph to reach equillibrium.
Data set	Algorithm	Accuracy (%)	Train Time (hr)	#Params
Pixel-MNIST	FastRNN	96.44	15.10	33k
	FastGRNN-LSQ	98.72	12.57	14k
	RNN	94.10	45.56	14k
	SpectralRNN	97.7		6k
	LSTM	97.81	26.57	53k
	URNN	95.1		16k
	Antisymmetric	98.01	8.61	14k
	iRNN (K=1)	97.73	2.83	4k
	iRNN (K=2)	98.13	3.11	4k
	iRNN (K=3)	98.13	2.93	4k
PermUte-MNIST	FastRNN	92.68	9.32	-^8.75k
	SpectralRNN	92.7		8.5k
	LSTM	92.61	19.31	35k
	URNN	91.4		12k
	Antisymmetric	93.59	4.75	14k
	iRNN (K=1)	95.62	2.41	8k
10https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities
11https://www.yelp.com/dataset/challenge
17
Published as a conference paper at ICLR 2020
A.7.2 Traditional Datasets
Table 7 shows the results including left out baselines for Pixel-MNIST and permute-MNIST task.
Here we also include star rating prediction on a scale of 1 to 5 of Yelp reviews Yelp (2017). Table 8
shows the results for this dataset.
Table 8: Results for Yelp Dataset.
Data set	Algorithm	Accuracy (%)	Model Size (KB)	Train Time (hr)	Test Time (ms)	#ParamS
Yelp-5	FastRNN	55.38	130	3.61	0.4	32.5k
	FastGRNN-LSQ	59.51	130	3.91	0.7	32.5k
	FastGRNN	59.43	8	4.62		
	RNN	47.59	130	3.33	0.4	32.5k
	SpectralRNN	56.56	89	4.92	0.3	22k
	EURNN	59.01	122	72.00		
	LSTM	59.49	516	8.61	1.2	129k
	GRU	59.02	388	8.12	0.8	97k
	Antisymmetric	54.14	130	2.61	0.4	32.5k
	UGRNN	58.67	258	4.34		
	iRNN (K=1)	58.16	97.67	0.31	0.4	25k
	iRNN (K=2)	59.01	98.84	0.31	0.7	25k
	iRNN (K=3)	59.34	100	1.16	1.0	25k
A.7.3 Activity Recognition Datasets
We also include activity recognition tasks: (a)Google-12 Warden (2018) , i.e. detection of utterances
of 10 commands plus background noise and silence and (b) DSA-19 Altun et al. (2010), Daily and
Sports Activity (DSA) detection from a resource-constrained IoT wearable device with 5 Xsens MTx
sensors having accelerometers, gyroscopes and magnetometers on the torso and four limbs. Table
9 shows results for these activities along with some other baselines for activity recognition tasks
mentioned in Sec. 4.3.4 and described in Sec. A.4.
A.7.4 PTB Language Modelling
We follow (Kusupati et al., 2018; Zhang et al., 2018) to setup our PTB experiments. We only pursue
one layer language modelling, but with more difficult sequence length (300). Table 10 reports all
the evaluation metrics for the PTB Language modelling task with 1 layer as setup by Kusupati et al.
(2018), including test time and number of parameters (which we omitted from the main paper due to
lack of space).
A.7.5 LINEAR RATE OF CONVERGENCE TO FIXED POI
Empirically We verify the local convergence to a fixed
point with linear rate by comparing the Euclidean distance
(k)
between the approximate solutions, h； ', using Eq. 11
with go = 0 and the fixed points, h；, computed using
FSOLVE from scipy. The learnable parameters are initial-
ized suitably and then fixed. We illustrate our results in
Fig. 6, which clearly demonstrates that the approximate
solutions tend to converge with linear rate.
Figure 6: Linear convergence in iRNN .
gi = gi-1 + ηi(φ(U(gi-1 + ht-1) + Wxt + b) - α(gi-1 + ht-1))	(II)
h；K = gK
18
Published as a conference paper at ICLR 2020
A.7.6 Theoretical Verification
Here we include some experiments to show that our theoretical assumptions hold true.
Ie7
2.5-
2.0-
1.5 -
1.0-
0.5-
ɑɑ-
— 100	—80	—60	—40	—20	0
Figure 7: Histogram of the eigenvalues of VφU — I for iRNN on HAR-2 dataset.
Non-Singularity of the matrix D For our iRNN parametrization to satisfy the conditions of having
equilibrium points to be locally asymptotically stable, the eigen values of the matrix D = (Vφ(∙)U -
γI ) should be negative. We plot a histogram of the eigenvalues of D for all the points in the HAR-2
dataset. As illustrated in the figure 7, all the eigenvalues are negative.
A.7.7 IDENTITY GRADIENT COMPARISON iRNN VS RNN
To verify Theorem. 1 empirically, we train RNN and iRNN on the HAR-2 data set (see more details
in Sec. 4), respectively, and plot in Fig. 8 the magnitude of gradient of the last layer hT w.r.t. the first
(由-EUS 3^l) SPn=U 6euj -U3-PE」C)
(a)
Figure 8: Comparison between RNN and iRNN on the magnitudes of gradients.
19
Published as a conference paper at ICLR 2020
layer h1 in log scale to confirm that our approach leads to no vanishing or exploding gradients when
the error is back-propagated through time. We also conducted experiments to verify that the gradient
of iRNN is norm preserving (see Sec. A.7.8 and Figure . 3). As we see clearly, RNN suffers from
serious vanishing gradient issue in training, while iRNN’s backpropagated gradients is close to 1,
and the variance arises mainly our approximation of fixed points and stochastic behavior in training
networks, demonstrating much better training stability of iRNN.
A.7.8 Gradientnormwrt. loss ∣∣ ∂L k
In addition to the gradient ratio we plot in Sec.4.2, we also show in figure 9, the more popular quantity
captured in earlier works (Arjovsky et al., 2016; Zhang et al., 2018), i.e. the gradient norm w.r.t.
loss k ∂∂L∣∣. We emphasize that this quantity alone is misleading in the context of resolving the
issue of VaniShing/exploding gradients. Since ∣∣∂∂L∣∣ = ∣∣∂Lk*∣∂T∣∣. The long term component
controlling the gradients is ∣∣ ∂T ∣∣, but the other component, ∣ ∂=L ∣∣ could become zero by the virtue
that the loss is nearly zero. This happens in our addition task experiment, because MsE is close to
zero, we experience nearly 0 value for this quantity. But this is clearly because the MsE is 0. Also
note that none of our graphs have log scale, which is not the case in earlier works. The conclusion
that can be drawn from the loss-gradient is that it is somewhat stable, and can inform us about quality
of convergence.
We also plot ∣∣ 肃葭 ∣∣ in figure 9 in order to show that indeed iRNN achieves identity gradients
everywhere in the time horizon, since fig. 3 had shown that the ratio of ∣∣ ∂hT Il and ∣∣ ∂τrT~ Il equals 1
for iRNN .
A.7.9 Different Activation Function
We also performed some experiments for sigmoid activation on HAR-2 dataset. The results for this
variant also follow similar pattern as we saw in ReLU variant.
A.8 Proofs
A.8.1 Local Convergence with Linear Rate
Recall that we rewrite the fixed-point constraints in our iRNN as the following oDE:
gk0 (t) =	F(gi)	=	φ(U (gi	+ hk-1) + Wxk + b) -	(gi	+	hk-1);	g(0) =	0.	(12)
Then based on the Euler method, we have the following update rule for solving fixed-points:
gi+1 = gi + ηk(i)F (gi)	(13)
= gi + ηk( ) [φ(U (gi + hk-1) + Wxk + b) - (gi + hk-1)].	(14)
Inexact Newton methods Dembo et al. (1982) refer to a family of algorithms that aim to solve the
equation system F(z) = 0 approximately at each iteration using the following rule:
Zi+1 = Zi + Si, ri = F(Zi) + VF(Zi)Si,	(15)
where VF denotes the (sub)gradient of function F, and r denotes the error at the i-th iteration
between F(Zi) and 0.
By drawing the connection between Eq. 13 and Eq. 15, we can set Zi ≡ gi and Si ≡ ηk(i)F(gi). Then
based on Eq. 15 we have
ri = F(gi) + ηk(i)VF(gi)F(gi).
Lemma 1 (Thm. 2.3 in Dembo et al. (1982)). Assume that
∣Wi!≤τ<1,,
(16)
(17)
where ∣∣ ∙ ∣ denotes an arbitrary norm and the induced operator norm. There exists ε > 0 such that,
if ∣∣zo 一 z*∣ ≤ ε, then the sequence of inexact Newton iterates {zi} converges to z*. Moreover, the
convergence is linear in the sense that ∣Zi+ι 一 z*∣* ≤ T∣∣Zi - z*∣*, where ∣∣y∣* = ∣∣VF(z*)y∣∣.
20
Published as a conference paper at ICLR 2020
Sequence length = 200
200
400
600
800
Training Steps
(a)
3
——LSTM
FastRNN(eta=0.01)
——FastRNN(eta=0.001)
---Antisym(g=0.01fe=0.1)
----Antisym(g=0.01fe=0.001)
——iRNN(K=l)
iRNN(K=5)
——iRNN(K=10)
1000
3.0
2.5
2.0
1.5
1.0
0.5
0.0
Sequence length = 200
O	200	400	600	800 IOOO
Training Steps
(b)
Figure 9: Exploratory experiments for the Add task : (a) Gradient norms w.r.t. loss ∣∣ 条 k, (b)
Gradient norms ∣∣ *τ ∣∣. This together with Figure 3 shows that the gradients are identity everywhere
for K = 10	-
21
Published as a conference paper at ICLR 2020
Theorem 3 (Local Convergence with Linear Rate). Assume that the function F in Eq. 12 and the
(i)
parameter ηk in Eq. 13 satisfy
[ηki)]2kVF(gi)F(gi)k2 + 2ηk"F(gi)>VF(gi)F(gi) < 0,∀i,∀k.	(18)
Then there exists > 0 such that if kg0 - heq k ≤ where heq denotes the fixed point, the sequence
{gi} generated by the Euler method converges to the equilibrium solution in Meq(hk-1, xk) locally
with linear rate.
Proof. By substituting Eq. 16 into Eq. 17, to prove local convergence we need to guarantee
kF(gi)+ηk(i)VF(gi)F(gi)k < kF(gi)k.	(19)
By taking the square of both sides in Eq. 19, we can show that Eq. 19 is equivalent to Eq. 18. We
then complete our proof.
□
Corollary 2. Assume that kI + ηk(i)VF(gi)k < 1, ∀i, ∀k holds. Then the forward propagation using
Eq. 13 is stable and our sequence {gi} converges locally with linear rate.
Proof. By substituting Eq. 16 into Eq. 17 and based on the assumption in the corollary, we have
krik	= kF (gi)+ ηki)VF Igi)F (gi)k
kF (gi)k =	kF (gi)k
≤
kI + ηki)VF (gi )kkF (gi)k
kF (gi)k	.
(20)
Further based on Prop. 2 in Chang et al. (2019) and Thm. 2, We then complete our proof. 口
22
Published as a conference paper at ICLR 2020
Table 9: Results for Activity Recoginition Datasets.
Data set	Algorithm	Accuracy (%)	Model Size (KB)	Train Time (hr)	Test Time (ms)	#Params
HAR-2	FastRNN	94.50	29	0.063	0.01	7.5k
	FastGRNN-LSQ	95.38	29	0.081	0.03	7.5k
	FastGRNN	95.59	3	0.10		
	RNN	91.31	29	0.114	0.01	7.5k
	SpectralRNN	95.48	525	0.730	0.04	134k
	EURNN	93.11	12	0.740		
	LSTM	93.65	74	0.183	0.04	16k
	GRU	93.62	71	0.130	0.02	16k
	Antisymmetric	93.15	29	0.087	0.01	7.5k
	UGRNN	94.53	37	0.120		
	iRNN (K=1)	95.32	17	0.061	0.01	4k
	iRNN (K=3)	95.52	17	0.081	0.02	4k
	iRNN (K=5)	96.30	18	0.018	0.03	4k
DSA-19	FastRNN	84.14	97	0.032	0.01	-^17.5k
	FastGRNN-LSQ	85.00	208	0.036	0.03	35k
	FastGRNN	83.73	3.25	2.10m		
	RNN	71.68	20	0.019	0.01	3.5k
	SpectralRNN	80.37	50	0.038	0.02	8.8k
	LSTM	84.84	526	0.043	0.06	92k
	GRU	84.84	270	0.039	0.03	47k
	Antisymmetric	85.37	32	0.031	0.01	8.3k
	UGRNN	84.74	399	0.039		
	iRNN (K=1)	88.11	19	0.015	0.01	3.5k
	iRNN (K=3)	85.20	19	0.020	0.02	3.5k
	iRNN (K=5)	87.37	20	0.005	0.03	3.5k
Google-12	FastRNN	92.21	56	0.61	0.01	12k-
	FastGRNN-LSQ	93.18	57	0.63	0.03	12k
	FastGRNN	92.10	5.5	0.75		
	RNN	73.25	56	1.11	0.01	12k
	SpectralRNN	91.59	228	19.0	0.05	49k
	EURNN	76.79	210	120.00		
	LSTM	92.30	212	1.36	0.05	45k
	GRU	93.15	248	1.23	0.05	53k
	Antisymmetric	89.91	57	0.71	0.01	12k
	UGRNN	92.63	75	0.78		
	iRNN (K=1)	93.93	36	0.20	0.01	8.1k
	iRNN (K=3)	94.16	37	0.33	0.03	8.1k
	iRNN (K=5)	94.71	38	0.17	0.05	8.1k
Google-30	FastRNN	91.60	96	1.30	0.01	18k-
	FastGRNN-LSQ	92.03	45	1.41	0.01	8.5k
	FastGRNN	90.78	6.25	1.77		
	RNN	80.05	63	2.13	0.01	12k
	SpectralRNN	88.73	128	11.0	0.03	24k
	EURNN	56.35	135	19.00		
	LSTM	90.31	219	2.63	0.05	41k
	GRU	91.41	257	2.70	0.05	48.5k
	Antisymmetric	90.91	64	0.54	0.01	12k
	UGRNN	90.54	260	2.11		
	iRNN (K=1)	93.77	44	0.44	0.01	8.5k
	iRNN (K=3)	91.30	44	0.44	0.03	8.5k
	iRNN (K=5)	94.23	45	0.44	0.05	8.5k
23
Published as a conference paper at ICLR 2020
Algorithm	Test Perplexity	Model Size (KB)	Train Time (min)	Test Time (ms)	#Params
FastRNN	127.76	513	11.20	1.2	52.5k
FastGRNN-LSQ	115.92	513	12.53	1.5	52.5k
FastGRNN	116.11	39	13.75		
RNN	144.71	129	9.11	0.3	13.2k
SpectralRNN	130.20	242	-	0.6	24.8k
LSTM	117.41	2052	13.52	4.8	210k
UGRNN	119.71	256	11.12	0.6	26.3k
iRNN (K=1)	115.71	288	7.11	0.6	29.5k
Table 10: PTB Language Modeling: 1 Layer. To be consistent with our other experiments we used
a low-dim U; For this size our results did not significantly improve with K. This is the dataset of
Kusupati et al. (2018) which uses sequence length 300 as opposed to 30 in the conventional PTB.
Data set	Algorithm	Accuracy (%)	Model Size (KB)	Train Time (hr)	Activation	#Params
HAR-2	iRNN (K=1)	95.32	17	0.061	ReLU	4k
	iRNN (K=3)	95.52	17	0.081	ReLU	4k
	iRNN (K=5)	96.30	18	0.018	ReLU	4k
	iRNN (K=1)	92.16	17	0.065	Sigmoid	4k
	iRNN (K=3)	93.35	17	0.078	Sigmoid	4k
	iRNN (K=5)	95.30	18	0.020	Sigmoid	4k
Table 11: HAR-2 dataset (Sigmoid, ReLU activations): K denotes pre-defined recursions embedded
in graph to reach equillibrium.
24