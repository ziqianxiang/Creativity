Published as a conference paper at ICLR 2020
Deep Imitative Models for
Flexible Inference, Planning, and Control
Nicholas Rhinehart
UC Berkeley
nrhinehart@berkeley.edu
Rowan McAllister
UC Berkeley
rmcallister@berkeley.edu
Sergey Levine
UC Berkeley
svlevine@berkeley.edu
Ab stract
Imitation Learning (IL) is an appealing approach to learn desirable autonomous
behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast,
planning-based algorithms use dynamics models and reward functions to achieve
goals. Yet, reward functions that evoke desirable behavior are often difficult to
specify. In this paper, we propose “Imitative Models” to combine the benefits
of IL and goal-directed planning. Imitative Models are probabilistic predictive
models of desirable behavior able to plan interpretable expert-like trajectories to
achieve specified goals. We derive families of flexible goal objectives, including
constrained goal regions, unconstrained goal sets, and energy-based goals. We
show that our method can use these objectives to successfully direct behavior. Our
method substantially outperforms six IL approaches and a planning-based approach
in a dynamic simulated autonomous driving task, and is efficiently learned from
expert demonstrations without online data collection. We also show our approach
is robust to poorly specified goals, such as goals on the wrong side of the road.
1 Introduction
Imitation learning (IL) is a framework for learning a model to mimic behavior. At test-time, the model
pursues its best-guess of desirable behavior. By letting the model choose its own behavior, we cannot
direct it to achieve different goals. While work has augmented IL with goal conditioning (Dosovitskiy
& Koltun, 2016; Codevilla et al., 2018), it requires goals to be specified during training, explicit goal
labels, and are simple (e.g., turning). In contrast, we seek flexibility to achieve general goals for
which we have no demonstrations.
In contrast to IL, planning-based algorithms like model-based reinforcement learning (MBRL)
methods do not require expert demonstrations. MBRL can adapt to new tasks specified through
reward functions (Kuvayev & Sutton, 1996; Deisenroth & Rasmussen, 2011). The “model” is a
dynamics model, used to plan under the user-supplied reward function. Planning enables these
approaches to perform new tasks at test-time. The key drawback is that these models learn dynamics
of possible behavior rather than dynamics of desirable behavior. This means that the responsibility of
evoking desirable behavior is entirely deferred to engineering the input reward function. Designing
reward functions that cause MBRL to evoke complex, desirable behavior is difficult when the space
of possible undesirable behaviors is large. In order to succeed, the rewards cannot lead the model
astray towards observations significantly different than those with which the model was trained.
Our goal is to devise an algorithm that combines the advantages of MBRL and ILby offering MBRL’s
flexibility to achieve new tasks at test-time and IL’s potential to learn desirable behavior entirely from
offline data. To accomplish this, we first train a model to forecast expert trajectories with a density
function, which can score trajectories and plans by how likely they are to come from the expert. A
probabilistic model is necessary because expert behavior is stochastic: e.g. at an intersection, the
expert could choose to turn left or right. Next, we derive a principled probabilistic inference objective
to create plans that incorporate both (1) the model and (2) arbitrary new tasks. Finally, we derive
families of tasks that we can provide to the inference framework. Our method can accomplish new
tasks specified as complex goals without having seen an expert complete these tasks before.
1
Published as a conference paper at ICLR 2020
Imitative Model
Region Goal Task
Go somewhere in this region
[Plan preference (low to hig中
Figure 1: Our method: deep imitative models. Top Center. We use demonstrations to learn a
probability density function q of future behavior and deploy it to accomplish various tasks. Left: A
region in the ground plane is input to a planning procedure that reasons about how the expert would
achieve that task. It coarsely specifies a destination, and guides the vehicle to turn left. Right: Goal
positions and potholes yield a plan that avoids potholes and achieves one of the goals on the right.
and Potholes Task
goal WhiIe avoiding PgtholeS
How would a human go to one of
those goals while avoiding potholes?
We investigate properties of our method on a dynamic simulated autonomous driving task (see Fig. 1).
Videos are available at https://sites.google.com/view/imitative-models. Our contributions are
as follows:
1. Interpretable expert-like plans with minimal reward engineering. Our method outputs multi-
step expert-like plans, offering superior interpretability to one-step imitation learning models. In
contrast to MBRL, our method generates expert-like behaviors with minimal reward engineering.
2. Flexibility to new tasks: In contrast to IL, our method flexibly incorporates and achieves goals
not seen during training, and performs complex tasks that were never demonstrated, such as
navigating to goal regions and avoiding test-time only potholes, as depicted in Fig. 1.
3. Robustness to goal specification noise: We show that our method is robust to noise in the goal
specification. In our application, we show that our agent can receive goals on the wrong side of
the road, yet still navigate towards them while staying on the correct side of the road.
4. State-of-the-art CARLA performance: Our method substantially outperforms MBRL, a custom
IL method, and all five prior CARLA IL methods known to us. It learned near-perfect driving
through dynamic and static CARLA environments from expert observations alone.
2 Deep Imitative Models
We begin by formalizing assumptions and notation. We model continuous-state, discrete-time,
Partially-Observed Markov Decision Processes (POMDPs). For brevity, we call the components of
state of which we have direct observations the agent’s “state”, although we explicitly assume these
states do not represent the full Markovian world state. Our agent’s state at time t is st ∈ RD ; t = 0
refers to the current time step, and φ is all of the agent’s observations. Variables are bolded. Random
variables are capitalized. Absent subscripts denote all future time steps, e.g. S =. S1:T ∈ RT×D. We
denote a probability density function of a random variable S as p(S), and its value as p(s) =. p(S = s).
To learn agent dynamics that are possible and preferred, we construct a model of expert behavior.
We fit an “Imitative Model” q(SLT∣φ) = QT=I q(St|S1:t-1, φ) to a dataset of expert trajectories
D = {(si, φi )}N=1 drawn from a (unknown) distribution of expert behavior si 〜p(S∣φi). By training
q(S∣φ) to forecast expert trajectories with high likelihood, we model the scene-conditioned expert
dynamics, which can score trajectories by how likely they are to come from the expert.
2
Published as a conference paper at ICLR 2020
2.1	Imitative Planning to Goals
After training, q(S∣φ) can generate trajectories that resemble those that the expert might generate -
e.g. trajectories that navigate roads with expert-like maneuvers. However, these maneuvers will not
have a specific goal. Beyond generating human-like behaviors, we wish to direct our agent to goals
and have the agent automatically reason about the necessary mid-level details. We define general
tasks by a set of goal variables G. The probability of a plan s conditioned on the goal G is modelled
by a posterior p(s∣G, φ). This posterior is implemented with q(s∣φ) as a learned imitation prior and
p(G|s, φ) as a test-time goal likelihood. We give examples ofp(G|s, φ) after deriving a maximum a
posteriori inference procedure to generate expert-like plans that achieve abstract goals:
s* = argmax logp(s∣G,φ) = argmax log q(s∣φ) + logp(G∣s,φ) - logp(G∣Φ)
ss
= argmax log q(s∣φ) + log p(G∣s,φ).	(1)
S	l-{z-}	、	、7/
imitation prior goal likelihood
We perform gradient-based optimization of Eq. 1, and defer this discussion to Appendix A. Next, we
discuss several goal likelihoods, which direct the planning in different ways. They communicate goals
they desire the agent to achieve, but not how to achieve them. The planning procedure determines how
to achieve them by producing paths similar to those an expert would have taken to reach the given
goal. In contrast to black-box one-step IL that predicts controls, our method produces interpretable
multi-step plans accompanied by two scores. One estimates the plan’s “expertness”, the second
estimates its probability to achieve the goal. Their sum communicates the plan’s overall quality.
Our approach can also be viewed as a learning-based method to integrate mid-level and high-level
controllers together, where demonstrations from both are available at train-time, only the high-
level controller is available at test-time, and the high-level controller can vary. The high-level
controller’s action specifies a subgoal for the mid-level controller. A density model of future
trajectories of an expert mid-level controller is learned at train-time, and is amenable to different types
of direction as specified by the high-level controller. In this sense, the model is an “apprentice”, having
learned to imitate mid-level behaviors. In our application, the high-level controller is composed
of an A* path-planning algorithm and one of a library of components that forms goal likelihoods
from the waypoints produced by A*. Connecting this to related approaches, learning the mid-
level controller (Imitative Model) resembles offline IL, whereas inference with an Imitative Model
resembles trajectory optimization in MBRL, given goals provided by the high-level controller.
2.2	Constructing Goal Likelihoods
Constraint-based planning to goal sets (hyperparameter-free): Consider the setting where we
have access to a set of desired final states, one of which the agent should achieve. We can model this
by applying a Dirac-delta distribution on the final state, to ensure it lands in a goal set G ⊂ RD :
p(G∣s, φ) J δsτ (G), δsτ (G) = 1 if ST ∈ G, δsτ (G) = 0 if st ∈ G.	(2)
δsT (G)’s partial support of sT ∈ G ⊂ RD constrains sT and introduces no hyperparameters into
p(G|s, φ). For each choice of G, we have a different way to provide high-level task information to
the agent. The simplest choice for G is a finite set of points: a (A) Final-State Indicator likelihood.
We applied (A) to a sequence of waypoints received from a standard A* planner (provided by the
CARLA simulator), and outperformed all prior dynamic-world CARLA methods known to us. We
can also consider providing an infinite number of points. Providing a set of line-segments as G yields
a (B) Line-Segment Final-State Indicator likelihood, which encourages the final state to land along
one of the segments. Finally, consider a (C) Region Final-State Indicator likelihood in which G
is a polygon (see Figs. 1 and 4). Solving Eq. 1 with (C) amounts to planning the most expert-like
trajectory that ends inside a goal region. Appendix B provides derivations, implementation details,
and additional visualizations. We found these methods to work well when G contains “expert-like”
final position(s), as the prior strongly penalizes plans ending in non-expert-like positions.
Unconstrained planning to goal sets (hyperparameter-based): Instead of constraining that the
final state of the trajectory reach a goal, we can use a goal likelihood with full support (sT ∈ RD),
centered at a desired final state. This lets the goal likelihood encourage goals, rather than dictate them.
If there is a single desired goal (G= {gT}), the (D) Gaussian Final-State likelihoodp(G|s, φ) J
3
Published as a conference paper at ICLR 2020
N (gT ; sT , I) treats gT as a noisy observation of a final future state, and encourages the plan to
arrive at a final state. We can also plan to K successive states G = (gT-K+1, . . . , gT) with a (E)
Gaussian State Sequence: p(G∣s, φ) J IlR=T-k+1 N(gk ； Sk, 6I) if a program wishes to specify a
desired end velocity or acceleration when reaching the final state gT (Fig. 2). Alternatively, a planner
may propose a set of states with the intention that the agent should reach any one of them. This is
possible by using a (F) Gaussian Final-State Mixture: p(G∣s, φ) J A Pk=1 N(gT； ST, eI) and
is useful if some of those final states are not reachable with an expert-like plan. Unlike A-C, D-F
introduce a hyperparameter “”. However, they are useful when no states in G correspond to observed
expert behavior, as they allow the imitation prior to be robust to poorly specified goals.
Costed planning: Our model has the additional flexibility to accept arbitrary user-specified costs c
at test-time. For example, we may have updated knowledge of new hazards at test-time, such as a
given map of potholes or a predicted cost map. Cost-based knowledge C(Si ∣φ) can be incorporated as
an (G) Energy-based likelihood: p(G∣s, φ) a QT=1 e-c(Stlφ) (Todorov, 2007; Levine, 2018). This
can be combined with other goal-seeking objectives by simply multiplying the likelihoods together.
Examples of combining G (energy-based) with F (Gaussian mixture) were shown in Fig. 1 and are
shown in Fig. 3. Next, we describe instantiating q(S∣φ) in CARLA (Dosovitskiy et al., 2017).
Designing general goal likelihoods can be considered a form of reward engineering if there are
no restrictions on the goal likelihoods. This connection is best seen in (G), which has an explicit
cost term. One reason why it is easier to design goal likelihoods than to design reward functions is
that the task of evoking most aspects of goal-driven behavior is already learned by the prior q(s∣φ),
which models desirable behavior. This is in contrast to model-free RL, which entirely relies on the
reward design to evoke goal-driven behavior, and in contrast to model-based RL, which heavily
relies on the reward design to evoke goal-driven behavior, as its dynamics model learns what is
possible, rather than what is desirable. Additionally, it is easy to design goal likelihoods when goals
provide a significant amount of information that obviates the need to do any manual tuning. The main
assumption is that one of the goals in the goal set is reachable within the model’s time-horizon.
Figure 2: Imitative planning with the Figure 3: Costs can be assigned to “pot-
Gaussian State Sequence enables fine- holes” only seen at test-time. The plan-
grained control of the plans.	ner prefers routes avoiding potholes.
Figure 4: Goal
regions can be
coarsely specified
to give directions.
2.3	Applying Deep Imitative Models to Autonomous Driving
In our autonomous driving application, we model the agent’s state at time t as St ∈ RD with D=2; St
represents our agent’s location on the ground plane. The agent has access to environment perception
φ J {s-τ:0, χ, λ}, where T is the number of past positions we condition on, X is a high-dimensional
observation of the scene, and λ is a low-dimensional traffic light signal. χ could represent either
LIDAR or camera images (or both), and is the agent’s observation of the world. In our setting, we
featurize LIDAR to χ = R200×200×2 , with χij representing a 2-bin histogram of points above and
at ground level in a 0.5m2 cell at position (i,j). CARLA provides ground-truth S-T:0 and λ. Their
availability is a realistic input assumption in perception-based autonomous driving pipelines.
Model requirements: A deep imitative model forecasts future expert behavior. It must be able to
compute q(s∣φ)∀s ∈ RT×D. The ability to compute Nsq(s∣φ) enables gradient-based optimization
for planning. Rudenko et al. (2019) provide a recent survey on forecasting agent behavior. As many
forecasting methods cannot compute trajectory probabilities, we must be judicious in choosing q(S∣φ).
A model that can compute probabilities R2P2 (Rhinehart et al., 2018), a generative autoregressive flow
(Rezende & Mohamed, 2015; Oord et al., 2017). We extend R2P2 to instantiate the deep imitative
4
Published as a conference paper at ICLR 2020
Figure 5: Illustration of our method applied to autonomous driving. Our method trains an imitative
model from a dataset of expert examples. After training, the model is repurposed as an imitative
planner. At test-time, a route planner provides waypoints to the imitative planner, which computes
expert-like paths to each goal. The best plan is chosen according to the planning objective and
provided to a low-level PID-controller in order to produce steering and throttle actions. This
procedure is also described with pseudocode in Appendix A.
model q(S∣φ). R2P2 was previously used to forecast vehicle trajectories: it was not demonstrated
or developed to plan or execute controls. Although we used R2P2, other future-trajectory density
estimation techniques could be used - designing q(s∣φ) is not the primary focus of this work. In
R2P2, qθ(S∣φ) is induced by an invertible, differentiable function: S = fθ(Z; φ): RT×2 → RT×2;
fθ warps a latent sample from a base distribution Z 〜q0 = N(0, I) to S. θ is trained to maximize
qθ(S∣φ) of expert trajectories. fθ is defined for 1..T as follows:
St = ft (ZLt) = μθ (Si：t-i ,φ) + σ (Si：t-i ,Φ)Zt,	(3)
where μθ(Si：t-i,φ) = St- + (St- - St-2) + me(SLt-I,φ) = 2St-ι - St-2 + me(Si：t-i,φ)
encodes a constant-velocity inductive bias. The mθ ∈ R2 and σθ ∈ R2×2 are computed by expressive
neural networks. The resulting trajectory distribution is complex and multimodal (Appendix C.1
depicts samples). Because traffic light state was not included in the φ of R2P2’s “RNN” model, it
could not react to traffic lights. We created a new model that includes λ. It fixed cases where q(S∣φ)
exhibited no forward-moving preference when the agent was already stopped, and improved q(S∣φ),s
stopping preference at red lights. We used T=40 trajectories at 10Hz (4 seconds), and τ =3. Fig. 12
in Appendix C depicts the architecture of μθ and σθ.
2.4	Imitative Driving
We now instantiate a complete autonomous driving framework based on imitative models to study
in our experiments, seen in Fig. 5. We use three layers of spatial abstraction to plan to a faraway
destination, common to autonomous vehicle setups: coarse route planning over a road map, path
planning within the observable space, and feedback control to follow the planned path (Paden et al.,
2016; Schwarting et al., 2018). For instance, a route planner based on a conventional GPS-based
navigation system might output waypoints roughly in the lanes of the desired direction of travel,
but not accounting for environmental factors such as the positions of other vehicles. This roughly
communicates possibilities of where the vehicle could go, but not when or how it could get to them,
or any environmental factors like other vehicles. A goal likelihood from Sec. 2.2 is formed from the
route and passed to the planner, which generates a state-space plan according to the optimization
in Eq. 1. The resulting plan is fed to a simple PID controller on steering, throttle, and braking.
Pseudocode of the driving and inference algorithms are given in Algs 1 and 2. The PID algorithm is
given in Appendix A.
3 Related Work
A body of previous work has explored offline IL (Behavior Cloning - BC) in the CARLA simulator
(Li et al., 2018; Liang et al., 2018; Sauer et al., 2018; Codevilla et al., 2018; 2019). These BC
approaches condition on goals drawn from a small discrete set of directives. Despite BC’s theoretical
drift shortcomings (Ross et al., 2011), these methods still perform empirically well. These approaches
and ours share the same high-level routing algorithm: an A * planner on route nodes that generates
waypoints. In contrast to our approach, these approaches use the waypoints in a Waypoint Classifier,
which reasons about the map and the geometry of the route to classify the waypoints into one of several
directives: {Turn left, Turn right, Follow Lane, Go Straight}. One of the original motivations for
5
Published as a conference paper at ICLR 2020
Algorithm 1 IMITATIVEDRIVING(ROUTEPLAN, IMITATIVEPLAN, PIDCONTROLLER, qθ , f, p, H)
1:	φ J ENVIRONMENT(0) {Initialize the robot}
2:	while not at destination do
3:	G J ROUTEPLAN(φ) {Generate goals from a route}
4:	s1G:T J IMITATIVEPLAN(qθ, f,p, G, φ) {Plan path}
5:	for h = 0 to H do
6:	u J PIDCONTROLLER(φ, s1G:T , h, H)
7:	φ J ENVIRONMENT(u) {Execute control}
8:	end for
9:	end while
Algorithm 2 IMITATIVEPLAN(qθ , f, p, G, φ)
1:	Initialize zi：T 〜qo
2:	while not converged do
3:	zi：T J zi：T+Vzi：T [logq(f(zi:T)∣φ)+logp(G∣f(zi:T),φ)] {Gradient ascent on Eq. 1}
4:	end while
5:	return si：T = f (zi：T)
these type of controls was to enable a human to direct the robot (Codevilla et al., 2018). However, in
scenarios where there is no human in the loop (i.e. autonomous driving), we advocate for approaches
to make use of the detailed spatial information inherent in these waypoints. Our approach and several
others we designed make use of this spatial information. One of these is CIL-States (CILS): whereas
the approach in Codevilla et al. (2018) uses images to directly generate controls, CILS uses identical
inputs and PID controllers as our method. With respect to prior conditional IL methods, our main
approach has more flexibility to handle more complex directives post-training, the ability to learn
without goal labels, and the ability to generate interpretable planned and unplanned trajectories.
These contrasting capabilities are illustrated in Table 1.
Our approach is also related to MBRL. MBRL can also plan with a predictive model, but its model
only represents possible dynamics. The task of evoking expert-like behavior is offloaded to the
reward function, which can be difficult and time-consuming to craft properly. We know of no MBRL
approach previously applied to CARLA, so we devised one for comparison. This MBRL approach
also uses identical inputs to our method, instead to plan a reachability tree (LaValle, 2006) over an
dynamic obstacle-based reward function. See Appendix D for further details of the MBRL and CILS
methods, which we emphasize use the same inputs as our method.
Several prior works (Tamar et al., 2016; Amos et al., 2018; Srinivas et al., 2018) used imitation
learning to train policies that contain planning-like modules as part of the model architecture. While
our work also combines planning and imitation learning, ours captures a distribution over possible
trajectories, and then plan trajectories at test-time that accomplish a variety of given goals with
high probability under this distribution. Our approach is suited to offline-learning settings where
interactively collecting data is costly (time-consuming or dangerous). However, there exists online IL
approaches that seek to be safe (Menda et al., 2017; Sun et al., 2018; Zhang & Cho, 2017).
4	Experiments
We evaluate our method using the CARLA driving simulator (Dosovitskiy et al., 2017). We seek to
answer four primary questions: (1) Can we generate interpretable, expert-like plans with offline
learning and minimal reward engineering? Neither IL nor MBRL can do so. It is straightforward
to interpret the trajectories by visualizing them on the ground plane; we thus seek to validate
whether these plans are expert-like by equating expert-like behavior with high performance on the
CARLA benchmark. (2) Can we achieve state-of-the-art CARLA performance using resources
commonly available in real autonomous vehicle settings? There are several differences between
the approaches, as discussed in Sec 3 and shown in Tables 1 and 2. Our approach uses the CARLA
toolkit’s resources that are commonly available in real autonomous vehicle settings: waypoint-based
routes (all prior approaches use these), LIDAR and traffic-light observations (both are CARLA-
6
Published as a conference paper at ICLR 2020
Table 1: Desirable attributes of each approach. A green check denotes that a method has a desirable
attribute, whereas a red cross denotes the opposite. A “*" indicates an approach We implemented.
Approach	Flexible to New Goals	Trains without goal labels Outputs Plans		Trains Offline	Has Expert P.D.F.
CIRL* (Liangetal., 2018)	X	X	X	X	X
CAL* (Saueretal., 2018)	X	X	X	✓	X
MT* (Lietal.,2018)	X	X	X	✓	X
CIL* (Codevillaetal., 2018)	X	X	X	✓	X
CILRS* (Codevilla et al.,2019)	X	X	X	✓	X
CILSt	X	✓	X	✓	X
MBRLt	✓	✓	✓	X	X
Imitative Models (Ours)t	✓	✓	✓	✓	✓
Table 2: Algorithmic components of each approach. A “			t ” indicates an approach we implemented.		
Approach	Control Algorithm	—Learning Algorithm	—Goal-Generation Algorithm	—Routing Algorithm	High-Dim. Obs.
CIRL* (Liang et al., 2018)	Policy	Behavior Cloning+RL	Waypoint Classifier	A* Waypointer	Image
CAL* (Sauer et al., 2018)	PID	Affordance Learning	Waypoint Classifier	A* Waypointer	Image
MT* (Li et al., 2018)	Policy	Behavior Cloning	Waypoint Classifier	A* Waypointer	Image
CIL* (Codevilla et al., 2018)	Policy	Behavior Cloning	Waypoint Classifier	A* Waypointer	Image
CILRS* (Codevilla et al., 2019)	Policy	Behavior Cloning	Waypoint Classifier	A* Waypointer	Image
CILSt	PID	Trajectory Regressor	Waypoint Classifier	A* Waypointer	(LIDAR,λ)
MBRLt	Reachability Tree	State Regressor	Waypoint Selector	A* Waypointer	(LIDAR,λ)
Imitative Models (Ours)t	Imitative Plan+PID	Traj. Density Est.	Goal Likelihoods	A* Waypointer	(LIDAR,λ)
provided, but only the approaches we implemented use it). Furthermore, the two additional methods
of comparison we implemented (CILS and MBRL) use the exact same inputs as our algorithm.
These reasons justify an overall performance comparison to answer (2): whether we can achieve
state-of-the-art performance using commonly available resources. We advocate that other approaches
also make use of such resources. (3) How flexible is our approach to new tasks? We investigate
(3) by applying each of the goal likelihoods we derived and observing the resulting performance. (4)
How robust is our approach to error in the provided goals? We do so by injecting two different
types of error into the waypoints and observing the resulting performance.
We begin by training q(S∣φ) on a dataset of 25 hours of driving we collected in Town01, detailed in
Appendix C.2. Following existing protocol, each test episode begins with the vehicle starting in one
of a finite set of starting positions provided by the CARLA simulator in Town01 or Town02 maps
in one of two settings: static-world (no other vehicles) or dynamic-world (with other vehicles). We
ran the same benchmark 3 times across different random seeds to quantify means and their standard
errors. We construct the goal set G for the Final-State Indicator (A) directly from the route output by
CARLA’s waypointer. B’s line segments are formed by connecting the waypoints to form a piecewise
linear set of segments. C’s regions are created a polygonal goal region around the segments of (B).
Each represents an increasing level of coarseness of direction. Coarser directions are easier to specify
when there is ambiguity in positions (both the position of the vehicle and the position of the goals).
Further details are discussed in Appendix B.3. Visualizations of (C) are shown in Figures 6 and 7.
Visualizations of (A) and (B) are shown in Figures 8 and 9. We use three metrics: (a) success rate in
driving to the destination without any collisions (which all prior work reports); (b) red-light violations;
and (c) proportion of time spent driving in the wrong lane and off road. With the exception of metric
(a), lower numbers are better.
Results: Towards questions (1) and (3) (expert-like plans and flexibility), we apply our approach
with a variety of goal likelihoods to the CARLA simulator. Towards question (2), we compare our
methods against CILS, MBRL, and prior work. These results are shown in Table 3. The metrics
for the methods we did not implement are from the aggregation reported in Codevilla et al. (2019).
We observe our method to outperform all other approaches in all settings: static world, dynamic
world, training conditions, and test conditions. We observe the Goal Indicator methods are able to
perform well, despite having no hyperparameters to tune. We found that we could further improve
our approach’s performance if we use the light state to define different goal sets, which defines a
“smart” waypointer. The settings where we use this are suffixed with “S.” in the Tables. We observed
the planner prefers closer goals when obstructed, when the vehicle was already stopped, and when a
red light was detected; we observed the planner prefers farther goals when unobstructed and when
green lights or no lights were observed. Examples of these and other interesting behaviors are best
seen in the videos on the website (https://sites.google.com/view/imitative-models). These
behaviors follow from the method leveraging q(S∣φ)'s internalization of aspects of expert behavior
in order to reproduce them in new situations. Altogether, these results provide affirmative answers
to questions (1) and (2). Towards question (3), these results show that our approach is flexible to
different directions defined by these goal likelihoods.
7
Published as a conference paper at ICLR 2020
Table 3: We evaluate different autonomous driving methods on CARLA’s Dynamic Navigation task.
A "*" indicates methods We have implemented (each observes the same waypoints and LIDAR as
input). A “*" indicates results reported in Codevilla et al. (2019). A “一" indicates an unreported
statistic. A ""indicates an optimistic estimate in transferring a result from the static setting to the
dynamic setting. “S.” denotes a “smart” waypointer reactive to light state, detailed in Appendix B.2.
Results accompanied by standard errors are computed with N = 3 trials across environment seeds.
Dynamic Nav. Method	Town01 (training conditions)				Town02 (test conditions)			
	Success↑	RanRed LighU	Wrong laneJ	Off roadJ	Success↑	Ran Red LightJ	Wrong laneJ	Off roadJ
CIRL*(Lianget al., 2018)	82%	-	-	-	41%	-	-	-
CAL*(Saueretal., 2018)	83%	-	-	-	64%	-	-	-
MT* (Lietal” 2018)	81%	—	—	—	53%	—	—	—
CIL*(Codevilla et al., 2018)	83%	83%i	-	-	38%	82%i	-	-
CILRS*(Codevilla et al., 2019)	92%	27%去	-	-	66%	64%i	-	-
CILS, Waypoint InPUtt	17%	0.0%	0.20%	12.1%	36%	0.0%	1.11%	11.70%
MBRL, Waypoint InPUtt	64%	72%	11.1%	2.96%	48%	54%	20.6%	13.3 %
Our method, Region Final-St. Indicator S.t	96%±1.9	0.89%±o.4	0.05%±o.oι	0.11%±o.oι	88%±3.3	2.60%±o.O4	0.49%±o.32	2.60%士ι.ι
Our method, Region FinaI-St Indicatort	93%±2.2	18%±o.5	0.023%±o.O02	O.195%±0.004	81%±2.2	54.7%±i.5	0.12%±0.oi	1.32%±o.69
Our method, Line Segment Final-St. Indicatort	91%±1.1	32%±i.3	0.055%±o.O02	0.O13%±o.00i	88%±3.3	35.2%±2.4	0.52%±o.O3	0.18%±o.O2
Our method, Final-State Indicatort	92%	26%	0.05%	0.012%	84%	35%	0.13%	0.38%
Our method, Gaussian Final-St. Mix.t	92%	6.3%	0.04%	0.005%	100%	12%	0.11%	0.04%
Our method, Gaussian Final-St. Mix. S.t	100%	1.7%	0.03%	0.005%	92%	0.0%	0.05%	0.15%
		Town01 (training conditions)				Town02 (test conditions)		
Static Nav. Method	Success↑	Ran Red LightJ	Wrong laneJ	Off roadJ	Success↑	Ran Red LightJ	Wrong laneJ	Off roadJ
CIRL* (Liangetal.,2018)	93%	-	-	-	68%	-	-	-
CAL* (Saueretal.,2018)	92%	-	-	-	68%	-	-	-
MT* (Lietal.,2018)	81%	-	-	-	78%	-	-	-
CIL* (Codevllaetal.,2018)	86%	83%	-	-	44%	82%	-	-
CILRS*(Codevilla et al., 2019)	95%	27%	-	-	90%	64%	-	-
CILS, Waypoint Inputt	28%	0.0%	0.38%	10.23%	36%	0.0%	1.69%	16.82%
MBRL, Waypoint InPUtt	96%	78%	14.3%	1.94%	96%	73%	19.6 %	0.75%
OUr method, FinaI-State Indicatort	100%	48%	0.05%	0.002%	100%	52%	0.10%	0.13%
OUr method, Gaussian Final-St. Mixturet	96%	0.83%	0.01%	0.08%	96%	0.0%	0.03%	0.14%
Our method, Gaussian Final-St. Mix. S.t	96%	0.0%	0.04%	0.07%	92%	0.0%	0.18%	0.27%
Figure 6: Planning with the Region Final State
Indicator yields plans that end inside the region.
The orange polygon indicates the region. The red
circles indicate the chosen plan.
Figure 7: Even with a wider goal region than
Fig. 6, the vehicle remains in its lane. Despite
their coarseness, these wide goal regions still
provide useful guidance to the vehicle.
4.1	Robustness to Errors in Goal-Specification
Towards questions (3) (flexibility) and (4) (noise-robustness), we analyze the performance of our
method when the path planner is heavily degraded, to understand its stability and reliability. We use
the Gaussian Final-State Mixture goal likelihood.
Navigating with high-variance waypoints. As a test of our model’s capability to stay in the
distribution of demonstrated behavior, we designed a “decoy waypoints” experiment, in which
half of the waypoints are highly perturbed versions of the other half, serving as distractions for
our Gaussian Final-State Mixture imitative planner. We observed surprising robustness to decoy
waypoints. Examples of this robustness are shown in Fig. 10. In Table 4, we report the success
rate and the mean number of planning rounds for failed episodes in the “1/2 distractors” row. These
numbers indicate our method can execute dozens of planning rounds without decoy waypoints
causing a catastrophic failure, and often it can execute the hundreds necessary to achieve the goal.
See Appendix E for details.
Navigating with waypoints on the wrong side of the road. We also designed an experiment to
test our method under systemic bias in the route planner. Our method is provided waypoints on the
8
Published as a conference paper at ICLR 2020
Figure 8: Planning with the Final State Indicator
yields plans that end at one of the provided loca-
tions. Orange diamonds indicate the locations in
the goal set. Red circles indicate the chosen plan.
Figure 9: Planning with the Line Segment Final
State Indicator yields plans that end along a seg-
ment. Orange diamonds indicate line segment
endpoints. Red circles indicate the chosen plan.
Figure 10: Tolerating bad goals. The planner prefers goals in the distribution of
expert behavior (on the road at a reasonable distance). Left: Planning with 1/2
decoy goals. Right: Planning with all goals on the wrong side of the road.
Figure 11: Test-
time plans steering
around potholes.
wrong side of the road (in CARLA, the left side), and tasked with following the directions of these
waypoints while staying on the correct side of the road (the right side). In order for the value of
q(s∣φ) to outweigh the influence of these waypoints, We increased the e hyperparameter. We found
our method to still be very effective at navigating, and report results in Table 4. We also investigated
providing very coarse 8-meter wide regions to the Region Final-State likelihood; these always include
space in the wrong lane and off-road (Fig. 7 in Appendix ?? provides visualization). Nonetheless, on
Town01 Dynamic, this approach still achieved an overall success rate of 48%. Taken together towards
question (4), our results indicate that our method is fairly robust to errors in goal-specification.
4.2	Producing Unobserved Behaviors to Avoid Novel Obstacles
Table 4: Robustness to waypoint noise and test-time pothole adaptation. Our method is robust to
waypoints on the wrong side of the road and fairly robust to decoy waypoints. Our method is flexible
enough to safely produce behavior not demonstrated (pothole avoidance) by incorporating a test-time
cost. Ten episodes are collected in each Town.
Waypointer	Extra Cost	Town01 (training conditions)			Town02 (test conditions)		
		Success	Wrong lane	Potholes hit	Success	Wrong lane	Potholes hit
Noiseless waypointer		100%	0.00%	177/230	100%	0.41%	82/154
Waypoints wrong lane		100%	0.34%	-	70%	3.16%	-
1/2 waypoints distracting		70%	一		50%	一	—
Noiseless waypointer	Pothole	90%	1.53%	10/230	70%	1.53%	35/154
To further investigate our model’s flexibility to test-time objectives (question 3), we designed a
pothole avoidance experiment. We simulated potholes in the environment by randomly inserting
them in the cost map near waypoints. We ran our method with a test-time-only cost map of the
simulated potholes by combining goal likelihoods (F) and (G), and compared to our method that
did not incorporate the cost map (using (F) only, and thus had no incentive to avoid potholes). We
recorded the number of collisions with potholes. In Table 4, our method with cost incorporated
avoided most potholes while avoiding collisions with the environment. To do so, it drove closer to the
centerline, and occasionally entered the opposite lane. Our model internalized obstacle avoidance by
9
Published as a conference paper at ICLR 2020
staying on the road and demonstrated its flexibility to obstacles not observed during training. Fig. 11
shows an example of this behavior. See Appendix F for details of the pothole generation.
5	Discussion
We proposed “Imitative Models” to combine the benefits of IL and MBRL. Imitative Models are
probabilistic predictive models able to plan interpretable expert-like trajectories to achieve new goals.
Inference with an Imitative Model resembles trajectory optimization in MBRL, enabling it to both
incorporate new goals and plan to them at test-time, which IL cannot. Learning an Imitative Model
resembles offline IL, enabling it to circumvent the difficult reward-engineering and costly online
data collection necessities of MBRL. We derived families of flexible goal objectives and showed
our model can successfully incorporate them without additional training. Our method substantially
outperformed six IL approaches and an MBRL approach in a dynamic simulated autonomous driving
task. We showed our approach is robust to poorly specified goals, such as goals on the wrong side
of the road. We believe our method is broadly applicable in settings where expert demonstrations
are available, flexibility to new situations is demanded, and safety is paramount. Future work could
investigate methods to handle both observation noise and out-of-distribution observations to enhance
the applicability to robust real systems — we expand on this issue in Appendix E. Finally, to facilitate
more general planning, future work could extend our approach to explicitly reason about all agents in
the environment in order to inform a closed-loop plan for the controlled agent.
Acknowledgements
This research was supported by ONR N000141712623, DARPA Assured Autonomy, ARL DCIST
CRA W911NF-17-2-0181, Google, NVIDIA, and Amazon.
References
Brandon Amos, Ivan Dario Jimenez Rodriguez, Jacob Sacks, Byron Boots, and Zico Kolter. Differ-
entiable MPC for end-to-end planning and control. In Neural Information Processing Systems
(NeurIPS), 2018.
FeliPe Codevilla, Matthias Miiller, Antonio L6pez, Vladlen Koltun, and Alexey Dosovitskiy. End-
to-end driving via conditional imitation learning. In International Conference on Robotics and
Automation (ICRA),pp.1-9.IEEE, 2018.
Felipe Codevilla, Eder Santana, Antonio M L6pez, and Adrien Gaidon. Exploring the limitations of
behavior cloning for autonomous driving. arXiv preprint arXiv:1904.08980, 2019.
Marc Deisenroth and Carl E Rasmussen. PILCO: A model-based and data-efficient approach to
policy search. In International Conference on Machine Learning (ICML), pp. 465472, 2011.
Alexey Dosovitskiy and Vladlen Koltun. Learning to act by predicting the future. arXiv preprint
arXiv:1611.01779, 2016.
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA:
An open urban driving simulator. In Conference on Robot Learning (CoRL), pp. 1-16, 2017.
Leonid Kuvayev and Richard S. Sutton. Model-based reinforcement learning with an approximate,
learned model. In Yale Workshop on Adaptive and Learning Systems, pp. 101-105, 1996.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Neural Information Processing Systems (NeurIPS),
pp. 6402-6413, 2017.
Steven M LaValle. Planning algorithms. Cambridge University Press, 2006.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909, 2018.
10
Published as a conference paper at ICLR 2020
Zhihao Li, Toshiyuki Motoyoshi, Kazuma Sasaki, Tetsuya Ogata, and Shigeki Sugano. Rethinking
self-driving: Multi-task knowledge for better generalization and accident explanation ability. arXiv
preprint arXiv:1809.11100, 2018.
Xiaodan Liang, Tairui Wang, Luona Yang, and Eric Xing. CIRL: Controllable imitative reinforcement
learning for vision-based self-driving. arXiv preprint arXiv:1807.03776, 2018.
Kunal Menda, Katherine Driggs-Campbell, and Mykel J Kochenderfer. DropoutDAgger: A Bayesian
approach to safe imitation learning. arXiv preprint arXiv:1709.06166, 2017.
Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg, et al. Parallel
WaveNet: Fast high-fidelity speech synthesis. arXiv preprint arXiv:1711.10433, 2017.
Brian Paden, Michal Cdp, Sze Zheng Yong, Dmitry Yershov, and Emiho Frazzoli. A survey of
motion planning and control techniques for self-driving urban vehicles. Transactions on Intelligent
Vehicles,1(1):33-55, 2016.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
Conference on Machine Learning (ICML), pp. 1530-1538, 2015.
Nicholas Rhinehart, Kris M. Kitani, and Paul Vernaza. R2P2: A reparameterized pushforward policy
for diverse, precise generative path forecasting. In European Conference on Computer Vision
(ECCV), September 2018.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Artificial Intelligence and Statistics (AISTATS), pp.
627-635, 2011.
Andrey Rudenko, Luigi Palmieri, Michael Herman, Kris M Kitani, Dariu M Gavrila, and Kai O Arras.
Human motion trajectory prediction: A survey. arXiv preprint arXiv:1905.06113, 2019.
Axel Sauer, Nikolay Savinov, and Andreas Geiger. Conditional affordance learning for driving in
urban environments. arXiv preprint arXiv:1806.06498, 2018.
Wilko Schwarting, Javier Alonso-Mora, and Daniela Rus. Planning and decision-making for au-
tonomous vehicles. Annual Review of Control, Robotics, and Autonomous Systems, 1:187-210,
2018.
Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal plan-
ning networks: Learning generalizable representations for visuomotor control. In International
Conference on Machine Learning (ICML), pp. 4739-4748, 2018.
Liting Sun, Cheng Peng, Wei Zhan, and Masayoshi Tomizuka. A fast integrated planning and
control framework for autonomous driving via imitation learning. In Dynamic Systems and Control
Conference. American Society of Mechanical Engineers, 2018.
Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In
Neural Information Processing Systems (NeurIPS), pp. 2154-2162, 2016.
Emanuel Todorov. Linearly-solvable Markov decision problems. In Neural Information Processing
Systems (NeurIPS), pp. 1369-1376, 2007.
Jiakai Zhang and Kyunghyun Cho. Query-efficient imitation learning for end-to-end simulated
driving. In Association for the Advancement of Artificial Intelligence (AAAI), pp. 2891-2897,
2017.
11
Published as a conference paper at ICLR 2020
A Algorithms
In Algorithm 1, we provided pseudocode for receding-horizon control via our imitative model. In
Algorithm 2 we provided pesudocode that describes how we plan in the latent space of the trajectory.
In Algorithm 3, we detail the speed-based throttle and position-based steering PID controllers.
Algorithm 3 PIDCONTROLLER(φ = {s0, s-ι,... }, sG?, h, H; Kp, Kp)
1: i J T - H + h {Compute the index of the target position}
2： Sprocess-speed J (so,x - s-ι,x) {Compute the current forward speed from the observations}
3:	ssetpoint-position J siG,x {Retrieve the target position x-coordinate from the plan}
4:	Ssetpoint-speed J SSetPoint-POSition/i {Compute the forward target speed}
5:	eS j- ssetpoint-speed - sprocess-speed {COmPUte the forward SPeed error}
6:	US J Kpes {Compute the accelerator control with a nonzero proportional term}
7:	throttle J l(e > 0) ∙ U + l(e ≤ 0) ∙ 0 {Use the control as throttle if the speed error is positive}
8:	brake J l(e > 0) ∙ 0 + l(e ≤ 0) ∙ U {Use the control as brake if the speed error is negative}
9:	αprocess J arctan(s0,y - s-1,y, s0,x - s-1,x) {Compute current heading}
10:	αsetpoint J arctan(siG,y - s0,y, |siG,x - s0,x|) {Compute target forward heading}
11:	eα J αsetpoint - αprocess {Compute the heading error}
12:	steering J Kpα eα {Compute the steering with a nonzero proportional term}
13:	U J [throttle, steering, brake]
14:	return U
A. 1 Latent Plan Optimization
Since s1:T = f (z1:T) in our implementation, and f is differentiable, we can perform gradient
descent of the same objective in terms of z1:T, as shown in Algorithm 2.Since q is trained with
zi：T 〜N(0, I), the latent space is likelier to be better numerically conditioned than the space of
s1:T, although we did not compare the two approaches formally. We implemented the following
optimizations to improve this procedure’s output and practical run time. 1) We started with N = 120
different z initializations, optimized them in batch, and returned the highest-scoring value across the
entire optimization. 2) We observed the resulting planning procedure to usually converge quickly, so
instead of specifying a convergence threshold, we simply ran the optimization for a small number of
steps, M = 10, and found that we obtained good performance. Better performance could be obtained
by performing a larger number of steps.
B Goal Details
B.1	Optimizing Goal Likelihoods with Set Constraints
We now derive an approach to optimize our main objective with set constraints. Although we
could apply a constrained optimizer, we find that we are able to exploit properties of the model
and constraints to derive differentiable objectives that enable approximate optimization of the cor-
responding closed-form optimization problems. These enable us to use the same straightforward
gradient-descent-based optimization approach described in Algorithm 2.
Shorthand notation: In this section we omit dependencies on φ for brevity, and use short hand
μt = μθ(si:t-i) and ∑t = ∑θ(srt-ι)∙ For example, q(st |si：t-i) = N ®; μt, ∑t).
Let us begin by defining a useful delta function:
δsT (G) =.	10
if sT ∈ G
if sT 6∈ G,
(4)
12
Published as a conference paper at ICLR 2020
which serves as our goal likelihood when using goal with set constraints: p(G|si：T) J δsτ (G). We
now derive the corresponding maximum a posteriori optimization problem:
sl:T
arg max p(s1:T|G)
s1:T ∈R2T
arg max p(G|si：T) ∙ q(si：T) ∙ PT(G)
s1:T ∈R2T
arg max p(G|si：T) ∙ q(si：T)
SLT ∈R2T '-----{z---}	S一{z一}
goal likelihood imitation prior
arg max δsτ (G) ∙ q(si：T)
s 1:T ∈R2T ^~{^~}	s~{^~}
set constraint imitation prior
arg max	q(s1:T)
s1:T ∈R2T	0
if sT ∈ G
if sT 6∈ G
arg max	q(s1:T)
s1:T -1 ∈R2(T -1) ,sT ∈G
T-1
arg max arg max q(sT |s1:T -1)	q(st|s1:t-1)
SLT-1∈R2(T-1) ST ∈G	t=1
T-1
arg max arg max N (ST; μT, ∑t )	N (st； μt, ∑t).
SLT-1∈R2(T-1) ST ∈G	t=ι
(5)
By exploiting the fact that q(sT|si：T-ι) = N (ST; μT, ∑t), we can derive closed-form solutions for
ST = arg max N (ST; μT, ∑t )
ST∈G
(6)
when G has special structure, which enables us to apply gradient descent to solve this constrained-
optimization problem (examples below). With a closed form solution to equation 6, we can easily
compute equation 5 using unconstrained-optimization as follows:
T-1
si：T = arg max arg max q(sT|si：T-i) ɪɪ q(st |si：t-i)	(7)
S1:T -1 ∈R2(T -1) ST ∈Gline-segment	t=1
Q*
S1:T -1
arg max
SLT-1∈R2(T-1)
、------V-------}
unconstrained optimization
T-1
q(S*T|S1:t-1)	q(St|S1:t-1).
t=1
(8)
'---------------------7----------------
objective function of S1:T -1
}
Note that equation 8 only helps solve equation 5 if equation 6 has a closed-form solution. We detail
example of goal-sets with such closed-form solutions in the following subsections.
B.1.1 Point goal-set
The solution to equation 6 in the case of a single desired goal g ∈ RD is simply:
Gpoint =. {gT},	(9)
sT,point = arg max N (ST ； μT, ∑t )
ST ∈Gpoint
= gT.	(10)
More generally, multiple point goals help define optional end points for planning: where the agent
only need reach one valid end point (see Fig. 8 for examples), formulated as:
Gpoints =. {gTk }kK=1 ,
ST,points = arg max N (gT ； μτ, ∑t ).
gkT ∈Gpoints
(11)
(12)
13
Published as a conference paper at ICLR 2020
B.1.2 Line-segment goal-set
We can form a goal-set as a finite-length line segment, connecting point a ∈ RD to point b ∈ RD :
giine(U) = a + U ∙ (b — a), U ∈ R,
Gline-segment = {gline(u) : u ∈ [0, 1]}.
The solution to equation 6 in the case of line-segment goals is:
STjine-Segment =	argmaχ N (sT； μτ, ET)
sT ∈Glain→e-sbegment
—a + min /1 max/θ (b - a)>.T1(〃T - a) ʌʌ /b	ʌ
a 十 UIin 1, UlaX 0,	∙	∙ (D a).
+	, I , (b — a)>∑T1 (b — a)	(	)
(13)
(14)
(15)
(16)
Proof:
To solve equation 15 is to find which point along the line gline(u) maximizes N (∙; μτ, Σt) subject
to the constraint 0 ≤ U ≤ 1:
U* = arg max N (gline(u); μτ, ∑t))
u∈[0,1]
=argmin (gline(U) - μT) ςt (gline(U) 一 μT) .
u∈[0,1] '---------------------{z-------------------}
Lu(u)
(17)
Since Lu is convex, the optimal value U* is value closest to the unconstrained arg max of Lu(U),
subject to 0 ≤ U ≤ 1:
U*R =. arg max Lu (U),	(18)
u∈R U* = arg min Lu(U) u∈[0,1] = min (1, max (0, U*R )) . We now solve for U*R : *	dL(〃)	d ((gline(U) - μT),夕T (gline(U) - μT)) Ql   Q 1 , I 1   	 	 、			(19)
UKP — U ： 0 —	_	—	_ R	dU	dU D d(gline(U) — μT)> v-1∕ =2 • 	j	∑t (gline(U) 一 μτ) dU	T —2 ∙ d(a 十 U ∙ (b- a)- μT)> ∑T1(a 十 U ∙ (b 一 a) — μτ) dU	T —2 ∙ (b — a) T ∑t ((a + U ∙ (b — a) 一 μτ), * _ (b - a)>.T1(〃T - a) UR - (b — a)>∑T1(b — a) , which gives us:	(20)
,line-segment	gline (U ) 二 a 十 U* ∙ (b — a) = a 十 min (1, max (0, uR)) ∙ (b — a)
a + min 1, max 0,
"aX) )) ∙ (b - a)
(21)
B.1.3	Multiple-line-segment goal-set:
More generally, we can combine multiple line-segments to form piecewise linear “paths” we wish
to follow. By defining a path that connects points (p0, p1, ..., pN), we can evaluate Liu for each
Glpinie→-sepgim+e1nt, select the optimal segment i* = arg maxi Liu, and use the segment i*’s solution to U* to
compute s*T . Examples shown in Fig. 9.
14
Published as a conference paper at ICLR 2020
B.1.4	Polygon goal-set
Instead of a route or path, a user (or program) may wish to provide a general region the agent should
go to, and state within that region being equally valid. Polygon regions (including both boundary
and interior) offer closed form solution to equation 6 and are simple to specify. A polygon can be
specified by an ordered sequence of vertices (p0, p1, ..., pN) ∈ RN×2. Edges are then defined as
the sequence of line-segments between successive vertices (and a final edge between first and last
vertex): ((p0, p1), ..., (pN-1, pN), (pN, p0)). Examples shown in Fig. 6 and 7.
Solving equation 6 with a polygon has two cases: depending whether μτ is inside the polygon, or
outside. If μτ lies inside the polygon, then the optimal value for ST that maximizes N(ST; μτ, ∑t)
is simply μτ: the mode of the Gaussian distribution. Otherwise, if μτ lies outside the polygon, then
the optimal value ST will lie on one of the polygon,s edges, solved using B.1.3.
B.2	Waypointer Details
The waypointer uses the CARLA planner’s provided route to generate waypoints. In the constrained-
based planning goal likelihoods, we use this route to generate waypoints without interpolating
between them. In the relaxed goal likelihoods, we interpolate this route to every 2 meters, and use
the first 20 waypoints. As mentioned in the main text, one variant of our approach uses a “smart”
waypointer. This waypointer simply removes nearby waypoints closer than 5 meters from the vehicle
when a green light is observed in the measurements provided by CARLA, to encourage the agent
to move forward, and removes far waypoints beyond 5 meters from the vehicle when a red light is
observed in the measurements provided by CARLA. Note that the performance differences between
our method without the smart waypointer and our method with the smart waypointer are small: the
only signal in the metrics is that the smart waypointer improves the vehicle’s ability to stop for red
lights, however, it is quite adept at doing so without the smart waypointer.
B.3	Constructing Goal Sets
Given the in-lane waypoints generated by CARLA’s route planner, we use these to create Point
goal sets, Line-Segment goal sets, and Polygon Goal-Sets, which respectively correspond to the (A)
Final-State Indicator, (B) Line-Segment Final-State Indicator, and (C) Final-State Region Indicator
described in Section 2.2. For (A), we simply feed the waypoints directly into the Final-State Indicator,
which results in a constrained optimization to ensure that ST ∈ G = {gTk }kK=1. We also included
the vehicle’s current position in the goal set, in order to allow it to stop. The gradient-descent based
optimization is then formed from combining Eq. 8 with Eq. 12. The gradient to the nearest goal of
the final state of the partially-optimized plan encourage the optimization to move the plan closer
to that goal. We used K = 10. We applied the same procedure to generate the goal set for the (B)
Line Segment indicator, as the waypoints returned by the planner are ordered. Finally, for the (C)
Final-State Region Indicator (polygon), we used the ordered waypoints as the “skeleton” of a polygon
that surrounds. It was created by adding a two vertices for each point vt in the skeleton at a distance
1 meter from vt perpendicular to the segment connecting the surrounding points (vt-1, vt+1). This
resulted in a goal set Gpolygon ⊃ Gline-segment, as it surrounds the line segments. The (F) Gaussian
Final-State Mixture goal set was constructed in the same way as (A), and also used when the pothole
costs were added.
For the methods we implemented, the task is to drive the furthest road location from the vehicle’s
initial position. Note that this protocol more difficult than the one used in prior work Codevilla et al.
(2018); Liang et al. (2018); Sauer et al. (2018); Li et al. (2018); Codevilla et al. (2019), which has no
distance guarantees between start positions and goals, and often results in shorter paths.
C Architecture and Training Details
The architecture of q(S∣φ) is shown in Table 5.
C.1 Prior Visualization and Statistics
We show examples of the priors multimodality in Fig. 13
15
Published as a conference paper at ICLR 2020
High-Ddmensional
Figure 12: Architecture of mθ and σθ, which parameterize qθ(S∣φ = {χ, s-τ:0, λ}). Inputs: LIDAR
χ, past-states s-τ:0, light-state λ, and latent noise ZLT. Output: trajectory SLT. Details in
Appendix C.
Table 5: Detailed Architecture that implements s1:T = f (z1:T, φ). Typically, T = 40, D = 2,H =
W = 200.
Component	Input [dimensionality]	Layer or Operation	Output [dimensionality]	Details
Static featurization of context: φ = {χ		∖ S-A：o}.		
MapFeat	χ [H, W, 2]	2D Convolution	1χ [H,W, 32]	3 × 3 stride 1, ReLu
MapFeat	i-1χ [H, W, 32]	2D Convolution	iχ [H, W, 32]	3 × 3 stride 1, ReLu, i ∈ [2,... ,8]
MapFeat	8χ [H, W, 32]	2D Convolution	Γ [H, W, 8]	3 × 3 stride 1, ReLu
PastRNN	s-τ:0 [τ + 1,D]	RNN	a [32]	GRU across time dimension
Dynamic generation via loop: for t ∈		{0,...,T-1}.		
MapFeat	st [D]	Interpolate	Yt = γ(St) [8]	Differentiable interpolation
JointFeat	γt , s0 , 2η, α, λ	Yt㊉S0㊉2η㊉α㊉λ	ρt [D+50+32+1]	Concatenate (㊉)
FutureRNN	ρt [D + 50 + 32 + 1]	RNN	1ρt [50]	GRU
FutureMLP	1ρt [50]	Affine (FC)	2ρt [200]	Tanh activation
FutureMLP	2ρt[200]	Affine (FC)	mt [D], ξt [D, D]	Identity activation
MatrixExp	ξt [D, D]	expm(ξt + ξta,transpose)	σt [D, D]	Differentiable Matrix Exponential Rhinehart et al. (2018)
VerletStep	st, st-1, mt, σt, zt	2St - St-1 + mt + σtzt	st+ι [D]	
C.1.1	Statistics of Prior and Goal Likelihoods
Following are the values of the planning criterion on N ≈ 8 ∙ 103 rounds from applying the “Gaussian
Final-State Mixture” to Town01 Dynamic. Mean of log q (s*∣φ) ≈ 104. Mean of log p(G∣s*, φ) = -4.
This illustrates that while the prior’s value mostly dominates the values of the final plans, the Gaussian
Final-State Goal Mixture likelihood has a moderate amount of influence on the value of the final plan.
C.2 Dataset
Before training q(S∣φ), we ran CARLA,s expert in the dynamic world setting of Town01 to collect
a dataset of examples. We have prepared the dataset of collected data for public release upon
publication. We ran the autopilot in Town01 for over 900 episodes of 100 seconds each in the
presence of 100 other vehicles, and recorded the trajectory of every vehicle and the autopilot’s LIDAR
observation. We randomized episodes to either train, validation, or test sets. We created sets of 60,701
train, 7586 validation, and 7567 test scenes, each with 2 seconds of past and 4 seconds of future
position information at 10Hz. The dataset also includes 100 episodes obtained by following the same
procedure in Town02.
D Baseline Details
D.1 Conditional Imitation Learning of States (CILS):
We designed a conditional imitation learning baseline that predicts the setpoint for the PID-controller.
Each receives the same scene observations (LIDAR) and is trained with the same set of trajectories as
our main method. It uses nearly the same architecture as that of the original CIL, except it outputs
setpoints instead of controls, and also observes the traffic light information. We found it very effective
for stable control on straightaways. When the model encounters corners, however, prediction is more
16
Published as a conference paper at ICLR 2020
Figure 13: Left: Samples from the prior, q(S∣φ), go left or right. Right: Samples go forward or right.
difficult, as in order to successfully avoid the curbs, the model must implicitly plan a safe path. We
found that using the traffic light information allowed it to stop more frequently.
D.2 Model-Based Reinforcement Learning:
Static-world To compare against a purely model-based reinforcement learning algorithm, we propose
a model-based reinforcement learning baseline. This baseline first learns a forwards dynamics model
st+1 = f (st-3:t, at) given observed expert data (at are recorded vehicle actions). We use an MLP
with two hidden layers, each 100 units. Note that our forwards dynamics model does not imitate the
expert preferred actions, but only models what is physically possible. Together with the same LIDAR
map χ our method uses to locate obstacles, this baseline uses its dynamics model to plan a reachability
tree LaValle (2006) through the free-space to the waypoint while avoiding obstacles. The planner opts
for the lowest-cost path that ends near the goal C(s1:T ; gT) = ||sT - gT ||2 + PtT=1 c(st), where
cost of a position is determined by C(St) = 1.51(St < 1 meters from any obstacle) + 0.751(1 <=
st < 2 meters from any obstacle) + s..t..
We plan forwards over 20 time steps using a breadth-first search over CARLA steering angle
{-0.3, -0.1, 0., 0.1, 0.3}, noting valid steering angles are normalized to [-1, 1], with constant
throttle at 0.5, noting the valid throttle range is [0, 1]. Our search expands each state node by the
available actions and retains the 50 closest nodes to the waypoint. The planned trajectory efficiently
reaches the waypoint, and can successfully plan around perceived obstacles to avoid getting stuck. To
convert the LIDAR images into obstacle maps, we expanded all obstacles by the approximate radius
of the car, 1.5 meters.
Dynamic-world We use the same setup as the Static-MBRL method, except we add a discrete
temporal dimension to the search space (one R2 spatial dimension per T time steps into the future).
All static obstacles remain static, however all LIDAR points that were known to collide with a vehicle
are now removed: and replaced at every time step using a constant velocity model of that vehicle. We
found that the main failure mode was due to both to inaccuracy in constant velocity prediction as
well as the model’s inability to perceive lanes in the LIDAR. The vehicle would sometimes wander
into the opposing traffic’s lane, having failed to anticipate an oncoming vehicle blocking its path.
E Robustness
E.1	Decoy Waypoints Experiments
In the decoy waypoints experiment, the perturbation distribution is N(0, σ = 8m): each waypoint is
perturbed with a standard deviation of 8 meters. One failure mode of this approach is when decoy
waypoints lie on a valid off-route path at intersections, which temporarily confuses the planner about
the best route. Additional visualizations are shown in Fig. 14.
17
Published as a conference paper at ICLR 2020
Figure 14: Tolerating bad waypoints. The planner prefers waypoints in the distribution of expert
behavior (on the road at a reasonable distance). Columns 1,2: Planning with 1/2 decoy waypoints.
Columns 3,4: Planning with all waypoints on the wrong side of the road.
E.2 Plan Reliability Estimation
Besides using our model to make a best-effort attempt to reach a user-specified goal, the fact that our
model produces explicit likelihoods can also be leveraged to test the reliability of a plan by evaluating
whether reaching particular waypoints will result in human-like behavior or not. This capability can
be quite important for real-world safety-critical applications, such as autonomous driving, and can be
used to build a degree of fault tolerance into the system. We designed a classification experiment
to evaluate how well our model can recognize safe and unsafe plans. We planned our model to
known good waypoints (where the expert actually went) and known bad waypoints (off-road) on
1650 held-out test scenes. We used the planning criterion to classify these as good and bad plans and
found that we can detect these bad plans with 97.5% recall and 90.2% precision. This result indicates
imitative models could be effective in estimating the reliability of plans.
We determined a threshold on the planning criterion by single-goal planning to the expert’s final
location on offline validation data and setting it to the criterion’s mean minus one stddev. Although
a more intelligent calibration could be performed by analyzing the information retrieval statistics
on the offline validation, we found this simple calibration to yield reasonably good performance.
We used 1650 test scenes to perform classification of plans to three different types of waypoints
1) where the expert actually arrived at time T (89.4% reliable), 2) waypoints 20m ahead along the
waypointer-provided route, which are often near where the expert arrives (73.8% reliable) 3) the
same waypoints from 2), shifted 2.5m off of the road (2.5% reliable). This shows that our learned
model exhibits a strong preference for valid waypoints. Therefore, a waypointer that provides expert
waypoints via 1) half of the time, and slightly out-of-distribution waypoints via 3) in the other half,
an “unreliable” plan classifier achieves 97.5% recall and 90.2% precision.
E.3 Out-of-Distribution Robustness
The existence of both (1) observation noise and (2) uncertain/out-of-distribution observations is an
important practical issue for autonomous vehicles. Although our current method only conditions
on our current observation, several extensions could help mitigate the negative effects of both (1)
and (2). For (1), a Bayesian filtering formulation is arguably most ideal, to better estimate (and
track) the location of static and dynamic obstacles under noise. However, such high-dimensional
filtering are often intractable, and might necessitate approximate Bayesian deep learning techniques,
RNNs, or frame stacking, to benefit from multiple observations. Addressing (2) would ideally be
done by placing a prior over our neural network weights, to derive some measure of confidence in our
density estimation of how expert each plan is, such that unfamiliar scenes generate large uncertainty
on our density estimate that we could detect, and react cautiously (pessimistically) to. One way
to address the situation if the distributions are very different is to adopt an ensembling approach
Lakshminarayanan et al. (2017) in order for the method to determine when the inputs are out of
distribution — the ensemble will usually have higher variance (i.e. disagree) when each element of
18
Published as a conference paper at ICLR 2020
the ensemble is provided with an out-of-distribution input. For instance, this variance could be used
as a penalization in the planning criterion.
E.4 Traffic-Light Noise
As discussed, our model assumes access to the traffic-light state provided by the simulator, which we
call λ. However, access to this state would be noisy in practice, because it relies on a sensor-based
(usually image-based) detection and classification module.
We performed an experiment to assess robustness to noise in λ: we simulated noise in λ by “flipping"
the light state with 20% probability, corresponding to a light state detector that has 80% accuracy on
average. “Flipping" means that if the light is green, then changingλ to indicate red, and if the light
is red, then changing λ to indicate green. We performed this following the experimental method of
“Region Final-St. Indicator S.” in dynamic Town02, and ran it with three separate seeds. The means
and their standard errors are reported in Table 6. The conclusion we draw is that the approach can still
achieve success most of the time, although it tends to violate red-lights more often. Qualitatively, we
observed the resulting behavior near intersections to sometimes be “jerky”, with the model alternating
between stopping and non-stopping plans. We hypothesize that the model itself could be made more
robust if the noise in λ was also present in the training data.
Table 6: We evaluate the effect of noise in the traffic-light state (λ) on CARLA’s Dynamic Navigation
task. Noise in the light state predictably degrades overall and red-light performance, but not to the
point of preventing the method from operating at all.
Town02 Dynamic Navigation Method	Success	Ran Red Light	Wrong lane	Off road
Region Final-St. Indicator S. (original)	88% ± 3.3	2.57% ± 0.04	0.49% ± 0.32	2.6% ± 1.06
Region Final-St. Indicator S. (noisy λ)	76% ± 5.0	34.8% ± 2.4	0.15% ± 0.04	1.79% ± 0.34
F	Pothole Experiment Details
We simulated potholes in the environment by randomly inserting them in the cost map near each
waypoint i with offsets distributedNi(μ=[-15m, 0m], Σ = diag([1,0.01])),(i.e. mean-centered on
the right side of the lane 15m before each waypoint). We inserted pixels of root cost -1e3 in the cost
map at a single sample of each Ni , binary-dilated the cost map by 1/3 of the lane-width (spreading the
cost to neighboring pixels), and then blurred the cost map by convolving with a normalized truncated
Gaussian filter of σ = 1 and truncation width 1.
G Baseline Visualizations
See Fig. 15 for a visualization of our baseline methods.
H	Hyperparameters
In order to tune the hyperparameter of the unconstrained likelihoods, we undertook the following
binary-search procedure. When the prior frequently overwhelmed the posterior, we set e J 0.2e, to
yield tighter covariances, and thus more penalty for failing to satisfy the goals. When the posterior
frequently overwhelmed the prior, we set e J 5e, to yield looser covariances, and thus less penalty for
failing to satisfy the goals. We executed this process three times: once for the “Gaussian Final-State
Mixture” experiments (Section 4), once for the “Noise Robustness” Experiments (Section 4.1), and
once for the pothole-planning experiments (Section 4.2). Note that the Constrained-Goal Likelihoods
introduced no hyperparameters to tune.
19
Published as a conference paper at ICLR 2020
Figure 15: Baseline methods we compare against. The red crosses indicate the past 10 positions of
the agent. Left: Imitation Learning baseline: the green cross indicates the provided goal, and the
yellow plus indicates the predicted setpoint for the controller. Right: Model-based RL baseline: the
green regions indicate the model’s predicted reachability, the red regions are post-processed LIDAR
used to create its obstacle map.
20