Published as a conference paper at ICLR 2020
On Generalization Error Bounds of Noisy
Gradient Methods for Non-Convex Learning
Jian Li * *
Tsinghua University
Xuanyuan Luo *
Tsinghua University
Mingda Qiao ^
Stanford University
Ab stract
Generalization error (also known as the out-of-sample error) measures how well
the hypothesis learned from training data generalizes to previously unseen data.
Proving tight generalization error bounds is a central question in statistical learn-
ing theory. In this paper, we obtain generalization error bounds for learning
general non-convex objectives, which has attracted significant attention in re-
cent years. We develop a new framework, termed Bayes-Stability, for proving
algorithm-dependent generalization error bounds. The new framework combines
ideas from both the PAC-Bayesian theory and the notion of algorithmic stabil-
ity. Applying the Bayes-Stability method, we obtain new data-dependent gener-
alization bounds for stochastic gradient Langevin dynamics (SGLD) and several
other noisy gradient methods (e.g., with momentum, mini-batch and acceleration,
Entropy-SGD). Our result recovers (and is typically tighter than) a recent result in
Mou et al. (2018) and improves upon the results in Pensia et al. (2018). Our ex-
periments demonstrate that our data-dependent bounds can distinguish randomly
labelled data from normal data, which provides an explanation to the intriguing
phenomena observed in Zhang et al. (2017a). We also study the setting where the
total loss is the sum of a bounded loss and an additional `2 regularization term. We
obtain new generalization bounds for the continuous Langevin dynamic in this set-
ting by developing a new Log-Sobolev inequality for the parameter distribution at
any time. Our new bounds are more desirable when the noise level of the process
is not very small, and do not become vacuous even when T tends to infinity.
1	Introduction
Non-convex stochastic optimization is the major workhorse of modern machine learning. For in-
stance, the standard supervised learning on a model class parametrized by Rd can be formulated as
the following optimization problem:
WmRd ZED [F (w,z)]，
where w denotes the model parameter, D is an unknown data distribution over the instance space Z,
and F : Rd × Z → R is a given objective function which may be non-convex. A learning algorithm
takes as input a sequence S = (z1, z2, . . . , zn) of n data points sampled i.i.d. from D, and outputs a
(possibly randomized) parameter configuration W ∈ Rd.
A fundamental problem in learning theory is to understand the generalization performance of learn-
ing algorithms—is the algorithm guaranteed to output a model that generalizes well to the data
distribution D? Specifically, we aim to prove upper bounds on the generalization error errgen(S) =
L(W, D) - L(W,S), where L(W, D) = Ez〜D [L(W,z)] and L(W,S) = 1 Pi=1 L(W,zi) are the
population and empirical losses, respectively. We note that the loss function L (e.g., the 0/1 loss)
could be different from the objective function F (e.g., the cross-entropy loss) used in the training
process (which serves as a surrogate for the loss L).
* lijian83@mail.tsinghua.edu.cn
* luo-xy19@mails.tsinghua.edu.cn
^mqiaO@Stanford.edu
1
Published as a conference paper at ICLR 2020
Classical learning theory relates the generalization error to various complexity measures (e.g., the
VC-dimension and Rademacher complexity) of the model class. Directly applying these classical
complexity measures, however, often fails to explain the recent success of over-parametrized neural
networks, where the model complexity significantly exceeds the amount of available training data
(see e.g., Zhang et al. (2017a)). By incorporating certain data-dependent quantities such as margin
and compressibility into the classical framework, some recent work (e.g., Bartlett et al. (2017); Arora
et al. (2018); Wei & Ma (2019)) obtains more meaningful generalization bounds in the deep learning
context.
An alternative approach to generalization is to prove algorithm-dependent bounds. One celebrated
example along this line is the algorithmic stability framework initiated by Bousquet & Elisseeff
(2002). Roughly speaking, the generalization error can be bounded by the stability of the algorithm
(see Section 2 for the details). Using this framework, Hardt et al. (2016) study the stability (hence
the generalization) of stochastic gradient descent (SGD) for both convex and non-convex functions.
Their work motivates recent study of the generalization performance of several other gradient-based
optimization methods: Kuzborskij & Lampert (2018); London (2016); Chaudhari et al. (2017); Ra-
ginsky et al. (2017); Mou et al. (2018); Pensia et al. (2018); Chen et al. (2018).
In this paper, we study the algorithmic stability and generalization performance of various iterative
gradient-based method, with certain continuous noise injected in each iteration, in a non-convex
setting. As a concrete example, we consider the stochastic gradient Langevin dynamics (SGLD)
(see Raginsky et al. (2017); Mou et al. (2018); Pensia et al. (2018)). Viewed as a variant of SGD,
SGLD adds an isotropic Gaussian noise at every update step:
Wt 一 Wt-1 - Ytgt(Wt-1)+ √√∣N(0,Id),	⑴
where gt(Wt-1) denotes either the full gradient or the gradient over a mini-batch sampled from
training dataset. We also study a continuous version of (1), which is the dynamic defined by the
following stochastic differential equation (SDE):
dWt = -VF(Wt) dt + P2β-1 dBt,	(2)
where Bt is the standard Brownian motion.
1.1	Related Work
Most related to our work is the study of algorithm-dependent generalization bounds of stochastic
gradient methods. Hardt et al. (2016) first study the generalization performance of SGD via algorith-
mic stability. They prove a generalization bound that scales linearly with T, the number of iterations,
when the loss function is convex, but their results for general non-convex optimization are more re-
stricted. London (2017) and Rivasplata et al. (2018) also combine ideas from both PAC-Bayesian
and algorithm stability. However, these works are essentially different from ours. In London (2017),
the prior and posterior are distributions on the hyperparameter space instead of distributions on the
hypothesis space. Rivasplata et al. (2018) study the hypothesis stability measured by the distance
on the hypothesis space in a setting where the returned hypothesis (model parameter) is perturbed
by a Gaussian noise. Our work is a follow-up of the recent work by Mou et al. (2018), in which
they provide generalization bounds for SGLD from both stability and PAC-Bayesian perspectives.
Another closely related work by Pensia et al. (2018) derives similar bounds for noisy stochastic gra-
dient methods, based on the information theoretic framework of XU & Raginsky (2017). However,
their bounds scale as O(，T/n) (n is the size of the training dataset) and are sub-optimal even for
SGLD.
We acknowledge that besides the algorithm-dependent approach that we follow, recent advances
in learning theory aim to explain the generalization performance of neural networks from many
other perspectives. Some of the most prominent ideas include bounding the network capacity by the
norms of weight matrices Neyshabur et al. (2015); Liang et al. (2019), margin theory Bartlett et al.
(2017); Wei et al. (2019), PAC-Bayesian theory Dziugaite & Roy (2017); Neyshabur et al. (2018);
Dziugaite & Roy (2018), network compressibility Arora et al. (2018), and over-parametrization Du
et al. (2019); Allen-Zhu et al. (2019); Zou et al. (2018); Chizat et al. (2019). Most of these results are
stated in the context of neural networks (some are tailored to networks with specific architecture),
whereas our work addresses generalization in non-convex stochastic optimization in general. We
2
Published as a conference paper at ICLR 2020
also note that some recent work provides explanations for the phenomenon reported in Zhang et al.
(2017a) from a variety of different perspectives (e.g., Bartlett et al. (2017); Arora et al. (2018; 2019)).
Welling & Teh (2011) first consider stochastic gradient Langevin dynamics (SGLD) as a sampling
algorithm in the Bayesian inference context. Raginsky et al. (2017) give a non-asymptotic analysis
and establish the finite-time convergence guarantee of SGLD to an approximate global minimum.
Zhang et al. (2017b) analyze the hitting time of SGLD and prove that SGLD converges to an approx-
imate local minimum. These results are further improved and generalized to a family of Langevin
dynamics based algorithms by the subsequent work of Xu et al. (2018).
1.2	Overview of Our Results
In this paper, we provide generalization guarantees for the noisy variants of several popular stochas-
tic gradient methods.
The Bayes-Stability method and data-dependent generalization bounds. We develop a new
method for proving generalization bounds, termed as Bayes-Stability, by incorporating ideas from
the PAC-Bayesian theory into the stability framework. In particular, assuming the loss takes value
in [0, C], our method shows that the generalization error is bounded by both 2C Ez [,2KL(P, Qz)]
and 2CEz[，2KL(Qz,P)], where P is a prior distribution independent of the training set S, and
Qz is the expected posterior distribution conditioned on zn = z (i.e., the last training data is z). The
formal definition and the results can be found in Definition 5 and Theorem 7.
Inspired by Lever et al. (2013), instead of using a fixed prior distribution, we bound the KL-
divergence from the posterior to a distribution-dependent prior. This enables us to derive the fol-
lowing generalization error bound that depends on the expected norm of the gradient along the
optimization path:
(3)
Here S is the dataset and ge(t) = Ew-i [ 1 Pn=IIl VF(Wt-ι, Zi)k2] is the expected empirical
squared gradient norm at step t; see Theorem 11 for the details.
Compared with the previous O (LnC，Pt	bound in (Mou et al., 2018, Theorem 1), where L
is the global Lipschitz constant of the loss, our new bound (3) depends on the data distribution and
is typically tighter (as the gradient norm is at most L). In modern deep neural networks, the worst-
case Lipschitz constant L can be quite large, and typically much larger than the expected empirical
gradient norm along the optimization trajectory. Specifically, in the later stage of the training, the
expected empirical gradient is small (see Figure 1(d) for the details). Hence, our generalization
bound does not grow much even if we train longer at this stage.
Our new bound also offers an explanation to the difference between training on correct and random
labels observed by Zhang et al. (2017a). In particular, we show empirically that the sum of expected
squared gradient norm (along the optimization path) is significantly higher when the training labels
are replaced with random labels (Section 3.1, Figure 1, Appendix C.2).
We would also like to mention the PAC-Bayesian bound (for SGLD with `2 -regularization) proposed
by Mou et al. (2018). (This bound is different from what We mentioned before; see Theorem 2 in
their paper.) Their bound scales as O(1∕√n) and the numerator of their bound has a similar sum
of gradient norms (with a decaying weight if the regularization coefficient λ > 0). Their bound is
based on the PAC-Bayesian approach and holds with high probability, while our bound only holds
in expectation.
Extensions. We remark that our technique allows for an arguably simpler proof of (Mou et al., 2018,
Theorem 1); the original proof is based on SDE and Fokker-Planck equation. More importantly, our
technique can be easily extended to handle mini-batches and a variety of general settings as follows.
1. Extension to other gradient-based methods. Our results naturally extends to other noisy
stochastic gradient methods including momentum due to Polyak (1964) (Theorem 26), Nes-
3
Published as a conference paper at ICLR 2020
terov’s accelerated gradient method in Nesterov (1983) (Theorem 26), and Entropy-SGD
proposed by Chaudhari et al. (2017) (Theorem 27).
2.	Extension to general noises. The proof of the generalization bound in Mou et al. (2018)
relies heavily on that the noise is Gaussian1, which makes it difficult to generalize to other
noise distributions such as the Laplace distribution. In contrast, our analysis easily carries
over to the class of log-Lipschitz noises (i.e., noises drawn from distributions with Lipschitz
log densities).
3.	Pathwise stability. In practice, it is also natural to output a certain function of the entire
optimization path, e.g., the one with the smallest empirical risk or a weighted average. We
show that the same generalization bound holds for all such variants (Remark 12). We note
that the analysis in an independent work of Pensia et al. (2018) also satisfies this property,
yet their bound is O (j C 2L2n_1 PT=I η2∕σ2) (See Corollary 1 in their work), which
scales at a slower O(1/√n) rate (instead of O(1∕n)) when dealing with C-bounded loss.2
Generalization bounds with `2 regularization via Log-Sobolev inequalities. We also study the
setting where the total objective function F is the sum of a C-bounded differentiable objective F0
and an additional '2 regularization term λ kwk2. In this case, F can be treated as a perturbation of a
quadratic function, and the continuous Langevin dynamics (CLD) is well understood for quadratic
functions. We obtain two generalization bounds for CLD, both via the technique of Log-Sobolev
inequalities, a powerful tool for proving the convergence rate of CLD. One of our bounds is as
follows (Theorem 15):
(4)
The above bound has the following advantages:
1.	Applying e-x ≥ 1 - x, one can see that our bound is at most O(√T∕n), which matches
the previous bound in (Mou et al., 2018, Proposition 8)3.
2.	As time T grows, the bound is upper bounded by and approaches to 2e4βCCLn-1 ,β∕λ
(unlike the previous O(√T∕n) bound that goes to infinity as T → +∞).
3.	If the noise level is not so small (i.e., β is not very large), the generalization bound is quite
desirable.
Our analysis is based on a Log-Sobolev inequality (LSI) for the parameter distribution at time t,
whereas most known LSIs only hold for the stationary distribution of the Markov process. We prove
the new LSI by exploiting the variational formulation of the entropy formula.
2	Preliminaries
Notations. We use D to denote the data distribution. The training dataset S = (z1, . . . , zn) is a
sequence of n independent samples drawn from D. S, S0 ∈ Zn are called neighboring datasets if
and only if they differ at exactly one data point (we could assume without loss of generality that
zn 6= zn0 ). Let F(w, z) and L(w, z) be the objective and the loss functions, respectively, where
W ∈ Rd denotes a model parameter and Z ∈ Z is a data point. Define F(w, S) = * Pnn==ι F(w, Zi)
and F(w, D) = Ez〜D [F(w, z)]; L(w, S) and L(w, D) are defined similarly. A learning algorithm
A takes as input a dataset S, and outputs a parameter w ∈ Rd randomly. Let G be the set of all
possible mini-batches. Gn = {B ∈ G : n ∈ B} denotes the collection of mini-batches that contain
the n-th data point, while Gn = G \ Gn. Let diam(A) = suPχ,y∈A ∣∣x - y∣∣2 denote the diameter
of a set A.
1In particular, their proof leverages the Fokker-Planck equation, which describes the time evolution of the
density function associated with the Langevin dynamics and can only handle Gaussian noise.
2They assume the loss is sub-Gaussian. By Hoeffding’s lemma, C -bounded random variables are sub-
Gaussian with parameter C.
3The proof of their O(√T∕n) bound can be easily extended to our setting with '2 regularization.
4
Published as a conference paper at ICLR 2020
Definition 1 (L-lipschitz). A function F : Rd × Z → R is L-lipschitz if and only if |F(w1, z) -
F(w2, z)| ≤ L kw1 - w2k2 holds for any w1, w2 ∈ Rd and z ∈ Z.
Definition 2 (Expected generalization error). The expected generalization error of a learning algo-
rithm A is defined as
errgen := E [errgen(S)] = E	[L(A(S), D) - L(A(S), S)].
S 〜Dn	S 〜Dn,A
Algorithmic Stability. Intuitively, a learning algorithm that is stable (i.e., a small perturbation
of the training data does not affect its output too much) can generalize well. In the seminal work
of Bousquet & Elisseeff (2002) (see also Hardt et al. (2016)), the authors formally defined algorith-
mic stability and established a close connection between the stability of a learning algorithm and its
generalization performance.
Definition 3 (Uniform stability). (Bousquet & Elisseeff (2002); Elisseeff et al. (2005)) A randomized
algorithm A is n-uniformly stable w.r.t. loss L, if for all neighboring sets S, S0 ∈ Zn, it holds that
sup |EA[L(wS, z)] - EA[L(wS0, z)]| ≤ n,
z∈Z
where wS and wS0 denote the outputs of A on S and S0 respectively.
Lemma 4 (Generalization in expectation). (Hardt et al. (2016)) Suppose a randomized algorithm
A is n-uniformly stable. Then, |errgen | ≤ n.
3	Bayes-Stability Method
In this section, we incorporate ideas from the PAC-Bayesian theory (see e.g., Lever et al. (2013))
into the algorithmic stability framework. Combined with the technical tools introduced in previous
sections, the new framework enables us to prove tighter data-dependent generalization bounds.
First, we define the posterior of a dataset and the posterior of a single data point.
Definition 5 (Single-point posterior). Let QS be the posterior distribution of the parameter for a
given training dataset S = (z1, . . . , zn). In other words, it is the probability distribution of the
output of the learning algorithm on dataset S (e.g., for T iterations of SGLD in (1), QS is the pdf of
WT). The single-point posterior Q(i,z) is defined as
Q(i,z) =	E	Q(z1,...,zi-1 ,z,zi+1 ,...,zn) .
(z1 ,...,zi-1 ,zi+1 ,...zn )
For convenience, we make the following natural assumption on the learning algorithm:
Assumption 6 (Order-independent). For any fixed dataset S = (z1, . . . , zn) and any permutation
p, QS is the same as QSp, where Sp = (zp1 , . . . , zpn).
Assumption 6 implies Q(i,z)= •… = Q(n,z), So We use Qz as a shorthand for Q(i,z) in the fol-
lowing. Note that this assumption can be easily satisfied by letting the learning algorithm randomly
permute the training data at the beginning. It is also easy to verify that both SGD and SGLD satisfy
the order-independent assumption.
NoW, We state our neW Bayes-stability frameWork, Which holds for any prior distribution P over the
parameter space that is independent of the training dataset S.
Theorem 7 (Bayes-Stability). Suppose the loss function L(w, z) is C -bounded and the learning al-
gorithm is order-independent (Assumption 6). Then for any prior distribution P not depending on S,
the generalization error is bounded
by both 2C Ez [p2KL(P, Qz)]
and 2CEz [,2KL(Qz, P)].
Remark 8. Our Bayes-Stability framework originates from the algorithmic stability framework,
and hence is similar to the notions of uniform stability and leave-one-out error (see Elisseeff et al.
(2003)). However, there are important differences. Uniform stability is a distribution-independent
property, while Bayes-Stability can incorporate the information of the data distribution (through the
prior P). Leave-one-out error measures the loss of a learned model on an unseen data point, yet
Bayes-Stability focuses on the extent to which a single data point affects the outcome of the learning
algorithm (compared to the prior).
5
Published as a conference paper at ICLR 2020
To establish an intuition, we first apply this framework to obtain an expectation generalization bound
for (full) gradient Langevin dynamics (GLD), which is a special case of SGLD in (1) (i.e., GLD uses
the full gradient PwF(W1,S) as gt(Wt-ι)).
Theorem 9. Suppose that the loss function L is C -bounded. Then we have the following expected
generalization bound for T iterations of GLD:
errgen ≤ 2√Ct	E n "X γ2ge(t),
n	S 〜Dn	σ
t=1 t
where ge(t) = Ew〜w1 [n1 PZi l∣VF(w, zi)k2] is the empirical squared gradient norm, and Wt
is the parameter at step t of GLD.
Proof The proof builds upon the following technical lemma, which we prove in Appendix A.2.
Lemma 10. Let (W0, . . . , WT) and (W00, . . . , WT0 ) be two independent sequences of random vari-
ables such that for each t ∈ {0, . . . , T }, Wt and Wt0 have the same support. Suppose W0 and W00
follow the same distribution. Then,
T
KL(W≤T,W≤0T) =X E	[KL(Wt|W<t =w<t,Wt0|W<0t = w<t)],
t=i w<t〜W<t
where W≤t denotes (W0, . . . , Wt) and W<t denotes W≤t-1.
Define P = ES〜0n-ι [Q(S 0)], where 0 denotes the zero data point (i.e., F(w, 0) = 0 for any w).
Theorem 7 shows that
errgen ≤ 2CE Vz2KL(Qz,P).
z
By the convexity of KL-divergence, for a fixed z ∈ Z, we have
KL(Qz,P) = KL (j[Q(S,z)],E[Q(S,o)fj ≤ EhKL(Q(S,z),Q(S,o))i .
(5)
(6)
Let (Wt)t≥0 and(Wt0)t≥o be the training process of GLD for S (S, z) and S	(S, 0), re-
spectively. Note that for a fixed w<t, both Wt|W<t = w<t and Wt0 |W<0 t = w<t are Gaussian
distributions. Since KL(N(μι,σ2I),N(μ2, σ21)) = k*1-,2'2 (see Lemma 18 in Appendix A.2).
KL(Wt∣W<t = w<t, Wt|W<t = w<t) = γ2 kVF(Wt-1,z)k2.
<	σt2 n2
Applying Lemma 10 and KL(WT , WT0 ) ≤ KL(W≤T , W≤0 T ) gives
1 T γ 2
KL(QS,QS0) ≤ —2 X-2	E	∣∣vf(w,z)k2.
n ≈1 σt w〜Wt-I
Recall that Wt_1 is the parameter at step t 一 1 using S = (S, Z) as dataset. In this case, We can
rewrite z as zn since it is the n-th data point of S. Note that SGLD satisfies the order-independent
assumption, we can rewrite z as zi for all i ∈ [n]. Together with (5), (6), and using 1 ∑i=1 √Xi ≤
Jɪ pn=ι Xi, we can prove this theorem.	■
More generally, we give the following bound for SGLD. The proof is similar to that of Theorem 9;
the difference is that we need to bound the KL-divergence between two Gaussian mixtures instead
of two Gaussians. This proof is more technical and deferred to Appendix A.3.
Theorem 11. Suppose that the loss function L is C -bounded and the objective function f is L-
lipschitz. Assume that the following conditions hold:
1. Batch size b ≤ n/2.
6
Published as a conference paper at ICLR 2020
2.	Learning rate Yt ≤ σt∕(20L).
Then, the following expected generalization error bound holds for T iterations of SGLD (1):
err gen ≤ 812Ct sE n [x γ2 ge(t)
n S S〜Dn [t=ι σt
(empirical norm)
where ge(t) = Ew〜w1 [n Pn=Ill VF(w, zi)k2] is the empirical squared gradient norm, and Wt
is the parameter at step t of SGLD.
Furthermore, based on essentially the same proof, we can obtain the following bound that depends
on the population gradient norm:
errgen ≤ 8.12C 'E [χ γ2	E [ E kVF (w,z)k2Λ .
n ∖ S0 M σt w〜W0t-1 Iz〜D	_|
The full proofs of the above results are postponed to Appendix A, and we provide some remarks
about the new bounds.
Remark 12. In fact, our proof establishes that the above upper bound holds for the two se-
quences W≤t and W≤t: KL(W≤t,W≤t) ≤ 8n12 PT=I σ⅛ge(t). Hence, our bound holds
for any sufficiently regular function over the parameter sequences: KL(f (W≤T), f (W≤0 T)) ≤
8n12 PT=I σ2ge(t). In particular, our generalization error bound automatically extends to sev-
eral variants of SGLD, such as outputting the average of the trajectory, the average of the suffix of
certain length, or the exponential moving average.
Remark 13 (High-probability bounds). By relaxing the expected SqUared gradient norm term to L2
and using the uniform stability framework, our proof can be adapted to recover the O(LC√T∕n)
bound in (Mou et al., 2018, Theorem 1). Then, we can apply the recent results of Feldman &
Vondrak (2019) to provide a generalization error bound of O(LC√T∕n + 1∕√n) that holds with
high probability. (Here O hides poly-logarithmic factors.) When T is at least linear in n, the
additional 1∕√n term is not dominating.
3.1	Experiment
Distinguish random from normal. Inspired by Zhang et al. (2017a), we run both GLD (Figure 1)
and SGLD (Appendix C.2) to fit both normal data and randomly labeled data (see Appendix C for
more experiment details). As shown in Figure 1 and Figure 3 in Appendix C.2, a larger random label
portion p leads to both much higher generalization error and much larger generalization error bound.
Moreover, the shapes of the curves of our bounds look quite similar to those of the generalization
error curves.
Note that in (b) and (c) of Figure 1, the scales in the y-axis are different. We list some possible
reasons that may explain why our bound is larger than the actual generalization error. (1) as we
explained in Remark 12, our bounds (Theorem 9 and 11) hold for any trajectory-based output, and
are much stronger than upper bounds for the last point on the trajectory. (2) The constant we can
prove in Lemma 21 may not be very tight. (3) The variance of Gaussian noise σt2 is not large enough
in our experiment. However, if we choose a larger variance, fitting the random labeled training data
becomes quite slow. Hence, we use a small data size (n = 10000) for the above reason. We also
run an extra experiment for GLD on the full MNIST dataset (n = 60000) without label corruption
(see Figure 2 in the Appendix C). We can see that our bound is non-vacuous (since GLD—which
computes the full gradients—took a long time to converge, we stopped when we achieved 90%
training accuracy). 4
4 We highlight another difficulty in proving non-vacuous generalization error bounds when the data are
randomly labeled. Consider a 10-class classification setting where all the labels are random. For any sufficiently
small data size, there is always a deep neural network that perfectly fits the dataset. Thus, the training error is
zero while the population error is 90%. In this case, any valid generalization error bound should be larger than
0.9. Then, the theoretical bound would still be vacuous even if it is only loose by a factor of 2.
7
Published as a conference paper at ICLR 2020
104step
(a)
_ ___ ____ ___ ___
Uoc",z=",∙l9u96
—p=0
p=0.25
p=0.5
—p=0.75
2	4	6	8	10
104step
(b)
104step
(c)
4	6	8	10
104step
(d)
Figure 1: Training MLP with GLD (σt = 0.2√2γt) on a smaller version of MNIST with different
random label portion p. (a) shows the training accuracy. (b) shows the generalization error, i.e.,
the gap between the 0/1 loss L01 on the training data and on the test data. (c) plots our bound in
Theorem 9. (d) shows that for p = 0, the gradient norms become much smaller at later stages of
training.
Relax the step size constraint. The condition on the step size in Theorem 11 may seem restrictive
in the practical use.5 We provide several ways to relax this constraint:
1.
2.
The proof of Theorem 11 still goes through if we replace L with
maxi∈[n] ∣∣VF(Wt-ι, zi)k2 in the constraint.
The maximum gradient norm can be controlled by gradient clipping, i.e., multiplying
min(CL,kVF (Wt-1,zi)k2)
kVF (Wt-ι ,Zi)k2
to each VF (Wt-1, zi).
3.	Replacing the constant 20 with 2 in this constraint will only increase the constant of our
bound from 8.12 to 84.4.
We also provide an experiment combining the above ideas to make our Theorem 11 applicable in
the practical use (see Figure 4 in Appendix C).
4 GENERALIZATION OF CLD AND GLD WITH `2 REGULARIZATION
In this section, we study the generalization error of Continuous Langevin Dynamics (CLD) with `2
regularization. Throughout this section, we assume that the objective function over training set S is
defined as F(w, S) = F0(w, S) + λ ∣∣w∣2, and moreover, the following assumption holds.
Assumption 14. The loss function L and the original objective F0 are C -bounded. Moreover, F0 is
differentiable and L-lipschitz.
The Continuous Langevin Dynamics is defined by the following SDE:
dWt = -VF(Wt, S) dt + √2β-1 dBt, W0 〜μo,	(CLD)
where (Bt)t≥0 is the standard Brownian motion on Rd and the initial distribution μo is the centered
Gaussian distribution in Rd with covariance 志Id. We show that the generalization error of CLD is
upper bounded by O (e4βCn-1,β∕λ), which is independent of the training time T (Theorem 15).
Furthermore, as T goes to infinity, we have a tighter generalization error bound O βC2n-1 (The-
orem 39 in Appendix B). We also study the generalization of Gradient Langevin Dynamics (GLD),
which is the discretization of CLD:
Wk+1 = Wk — ηVF(Wk, S) + √2ηβ-1ξk,	(GLD)
where ξk is the standard Gaussian random vector in Rd. By leveraging a result developed in Ragin-
sky et al. (2017), we show that, as Kη2 tends to zero, GLD has the same generalization as CLD (see
Theorems 15 and 39). We first formally state our first main result in this section.
5The condition Yt = O(σt∕L) is also required in (Mou et al., 2018, Theorem 1)
8
Published as a conference paper at ICLR 2020
Theorem 15. Under Assumption 14, CLD (with initial probability measure dμ0 = -1 e 入乎" dw)
has the following expected generalization error bound:
2e4βCCL
err ge" ≤
(7)
In addition, if L is M-smooth and non-negative, by setting λβ > 2, λ > 0 and η ∈ [0,1 ∧ 8M),
GLD (running K iterations with the same μo as CLD) has the expected generalization error bound:
err gen ≤ 2C √2KCm2 +
(8)
where C1 is a constant that only depends on M, λ, β, b, L and d.
The following lemma is crucial for establishing the above generalization bound for CLD. In partic-
ular, We need to establish a Log-Sobolev inequality for μt, the parameter distribution at time t, for
every time step t > 0. In contrast, most known LSIs only characterize the stationary distribution of
the Markov process. The proof of the lemma can be found in Appendix B.
Lemma 16. Under Assumption 14, let μt be the probability measure of Wt in CLD (with dμo =
-∣	— λβ k w k 2
-e 2	dw). Let V be a probability measure that is absolutely continuous with respect to μt.
Suppose dμt = ∏t(w) dw and dν = Y(W) dw. Then, it holds that
KL(γ, πt) ≤
exp(8βC)
2λβ
/
Rd
V log MI2 γ(w)dw.
We sketch the proof of Theorem 15, and the complete proof is relegated to Appendix B.
Proof Sketch of Theorem 15 Suppose S and S0 are tWo neighboring datasets. Let (Wt)t≥0 and
(Wt0)t≥0 be the process of CLD running on S and S0, respectively. Let γt and πt be the pdf of Wt0
and Wt. Let FS (w) denote F(w, S). We have
2γ
dw + J YthV log Yt, VFS -VFsoi dw
dw + β / Jt kVFs -VFso k2 dw.
-λ	2βL2
≤ e8βcKL(Yt, πt)+ -n-
(Lemma 16)
Solving this inequality gives KL(Yt, πt) ≤ n2λ2βL2e8βC(1 — e-λt∕e8βC). Hence the generaliza-
tion error of CLD can
be bounded by 2C J1 KL(YT, ∏t )
, Which proves the first part. The second
part of the theorem folloWs from Lemma 36 in Appendix B.
Our second generalization bound for CLD (Theorem 39 in Appendix B) is
8βC2 C	(—XT、 E
errgen ≤	+4Ceχp (e4βc) √βC.
The high level idea to prove this bound is very similar to that in Raginsky et al. (2017). We first
observe that the (stationary) Gibbs distribution μ has a small generalization error. Then, we bound
the distance from μt to μ. In our setting, we can use the Holley-Stroock perturbation lemma which
allows us to bound the Logarithmic Sobolev constant, and we can thus bound the above distance
easily.
5	Future Directions
In this paper, we prove new generalization bounds for a variety of noisy gradient-based methods. Our
current techniques can only handle continuous noises for which we can bound the KL-divergence.
9
Published as a conference paper at ICLR 2020
One future direction is to study the discrete noise introduced in SGD (in this case the KL-divergence
may not be well defined). For either SGLD or CLD, if the noise level is small (i.e., β is large), it may
take a long time for the diffusion process to reach the stable distribution. Hence, another interesting
future direction is to consider the local behavior and generalization of the diffusion process in finite
time through the techniques developed in the studies of metastability (see e.g., Bovier et al. (2005);
Bovier & den Hollander (2006); Tzen et al. (2018)). In particular, the technique may be helpful for
further improving the bounds in Theorems 15 and 39 (when T is not very large).
6	Acknowledgement
We would like to thank Liwei Wang for several helpful discussions during various stages of the
work. The research is supported in part by the National Natural Science Foundation of China Grant
61822203, 61772297, 61632016, 61761146003, and the Zhongguancun Haihua Institute for Frontier
Information Technology and Turing AI Institute of Nanjing.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 6155-6166, 2019.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning (ICML),
pp. 254-263, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 8139-8148, 2019.
Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and geometry of Markov diffusion
operators, volume 348. Springer Science &amp; Business Media, 2013.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 6240-
6249, 2017.
Olivier BoUsqUet and Andre Elisseeff. Stability and generalization. Journal of Machine Learning
Research (JMLR), 2:499-526, 2002.
Anton Bovier and Frank den Hollander. Metastability: a potential theoretic approach. In Interna-
tional Congress OfMathematicians, volume 3, pp. 499-5l8. Eur. Math. Soc. Zurich, 2006.
Anton Bovier, Veronique Gayrard, and Markus Klein. Metastability in reversible diffusion processes
ii: Precise asymptotics for small eigenvalues. Journal of the European Mathematical Society, 7
(1):69-99, 2005.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradi-
ent descent into wide valleys. In International Conference on Learning Representations (ICLR),
2017.
Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization
algorithms. arXiv preprint arXiv:1804.01619, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 2933-2943, 2019.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning (ICML), pp.
1675-1685, 2019.
John Duchi. Derivations for linear algebra and optimization. Berkeley, California, 3, 2007.
10
Published as a conference paper at ICLR 2020
Gintare Karolina Dziugaite and Daniel Roy. Entropy-SGD optimizes the prior of a PAC-Bayes
bound: Generalization properties of entropy-SGD and data-dependent priors. In International
Conference on Machine Learning (ICML), pp.1377-1386, 2018.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Uncertainty
in Artificial Intelligence (UAI), 2017.
Andre Elisseeff, Massimiliano PontiL et al. Leave-one-out error and stability of learning algorithms
with applications. NATO science series sub series iii computer and systems sciences, 190:111-
130, 2003.
Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning
algorithms. Journal of Machine Learning Research (JMLR), 6(Jan):55-79, 2005.
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable al-
gorithms with nearly optimal rate. In Conference on Learning Theory (COLT), pp. 1270-1279,
2019.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: stability of
stochastic gradient descent. In International Conference on Machine Learning (ICML), pp. 1225-
1234, 2016.
Richard Holley and Daniel Stroock. Logarithmic sobolev inequalities and stochastic ising models.
Journal of statistical physics, 46(5):1159-1194, 1987.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pp.
1097-1105, 2012.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
International Conference on Machine Learning (ICML), pp. 2815-2824, 2018.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Guy Lever, Francois Laviolette, and John Shawe-Taylor. Tighter pac-bayes bounds through
distribution-dependent priors. Theoretical Computer Science, 473:4-28, 2013.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geome-
try, and complexity of neural networks. In International Conference on Artificial Intelligence and
Statistics (AISTATS), pp. 888-896, 2019.
Ben London. Generalization bounds for randomized learning with application to stochastic gradient
descent. In NIPS Workshop on Optimizing the Optimizers, 2016.
Ben London. A pac-bayesian analysis of randomized learning with application to stochastic gradient
descent. In Advances in Neural Information Processing Systems (NeurIPS), pp. 2931-2940, 2017.
Georg Menz, Andre Schlichting, et al. Poincare and logarithmic sobolev inequalities by decompo-
sition of the energy landscape. The Annals of Probability, 42(5):1809-1884, 2014.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of SGLD for non-
convex learning: Two theoretical viewpoints. In Conference on Learning Theory (COLT), pp.
605-638, 2018.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate
O(1/k2). In Dokl. Akad. Nauk SSSR, volume 269, pp. 543-547, 1983.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory (COLT), pp. 1376-1401, 2015.
11
Published as a conference paper at ICLR 2020
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A PAC-Bayesian
approach to spectrally-normalized margin bounds for neural networks. In International Confer-
ence on Learning Representations (ICLR), 2018.
Grigorios A Pavliotis. Stochastic processes and applications: diffusion processes, the Fokker-Planck
and Langevin equations, volume 60. Springer, 2014.
Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative algo-
rithms. In International Symposium on Information Theory (ISIT), pp. 546-550, 2018.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gra-
dient langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory (COLT),
pp. 1674-1703, 2017.
Hannes Risken. Fokker-planck equation. In The Fokker-Planck Equation, pp. 63-95. Springer,
1996.
Omar Rivasplata, Csaba Szepesvari, John S Shawe-Taylor, Emilio Parrado-Hernandez, and Shiliang
Sun. Pac-bayes bounds for stable algorithms with instance-dependent priors. In Advances in
Neural Information Processing Systems (NeurIPS), pp. 9214-9224, 2018.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. In International Conference on Machine Learning (ICML),
pp. 1139-1147, 2013.
Flemming Topsoe. Some inequalities for information divergence and related measures of discrimi-
nation. IEEE Transactions on Information Theory, 46(4):1602-1609, 2000.
Belinda Tzen, Tengyuan Liang, and Maxim Raginsky. Local optimality and generalization guaran-
tees for the langevin algorithm via empirical metastability. In Conference On Learning Theory
(COLT), pp. 857-875, 2018.
Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz
augmentation. In Advances in Neural Information Processing Systems (NeurIPS), pp. 9722-9733,
2019.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 9709-9721, 2019.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
International Conference on Machine Learning (ICML), pp. 681-688, 2011.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. In Advances in Neural Information Processing Systems (NeurIPS), pp. 2524-
2533, 2017.
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics
based algorithms for nonconvex optimization. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 3126-3137, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations (ICLR), 2017a.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient
langevin dynamics. In Conference on Learning Theory (COLT), pp. 1980-2022, 2017b.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
12
Published as a conference paper at ICLR 2020
A Proofs in Section 3
A. 1 Bayes-Stability Framework
Lemma 17. Under Assumption 6, for any prior distribution P not depending on the dataset S
(z1, . . . , zn), the generalization error is upper bounded by
E E L(w, z) - E L(w, z)
Z W〜P	W〜Qz
+ E E L(w) - E	L(w)
Z W〜P	W〜Qz
where L(w) denotes the population loss L(w, D).
Proof of Lemma 17 Let errtrain = ES EW〜QS L(w, S) and errtest = ES EW〜QS L(w). We can
rewrite generalization error as errgen = errtest - errtrain, where
errtest = E E	L(w) = E E L(w)	(Assumption 6)
E	(QZ (w) - P (w))L(w) dw +	P (w)L(w) dw.
and
errtrain
1n
—£e E L(w,Zi)
n = SW 〜QS
1n
—y^E E L(w,z) = E E L(w,z)
(Assumption 6)
E	(QZ (w) - P (w))L(w, z) dw +	P(w) E L(w, z) dw
(P is a prior)
E	(QZ (w) - P (w))L(w, z) dw +	P (w)L(w) dw.	(definition of f (w))
Thus, we have
|errgen | = |errt
est - errtrain |
= E	(QZ (w) - P (w))L(w) dw - E	(QZ (w) - P (w))L(w, z) dw
≤ E E L(w, z) - E L(w, z)
Z W〜Qz	W〜P
+ E E L(w) - E L(w)
Now we are ready to prove Theorem 7, which we restate in the following.
Theorem 7 (Bayes-Stability). Suppose the loss function L(w, z) is C -bounded and the learning al-
gorithm is order-independent (Assumption 6), then for any prior distribution P not depending on S,
the generalization error is bounded
by both 2C EZ [p2KL(P, Qz)]
and 2CEZ [,2KL(Qz, P)].
Proof By Lemma 17,
errgen ≤ E E L(w, z) - E L(w, z)
Z W〜P	W〜Qz
+ E E L(w) - E L(w)
Z W〜P	W〜Qz
≤ E [2C ∙TV(P, Qz) + 2C ∙TV(P, QZ)]	(C-boundedness)
Z
≤ 4C E
Z
JIKL(P,Qz )
(Pinsker’s inequality)
The other bound follows from a similar argument.
13
Published as a conference paper at ICLR 2020
A.2 Technical Lemmas
Now we turn to the proof of Theorem 11. The following lemma allows us to reduce the proof of
algorithmic stability to the analysis of a single update step.
Lemma 10. Let (W0, . . . , WT) and (W00, . . . , WT0 ) be two independent sequences of random vari-
ables such that for each t ∈ {0, . . . , T}, Wt and Wt0 have the same support. Suppose W0 and W00
follow the same distribution. Then,
T
KL(W≤T,W≤0T) = X	E	[KL(Wt|W<t =w<t,Wt0|W<0t = w<t)],
t=1 w<t 〜W<t
where W≤t denotes (W0, . . . , Wt) and W<t denotes W≤t-1.
Proof By the chain rule of the KL-divergence,
KL(W≤t, W≤0 t) = KL(W<t, W<0 t) + E	[KL(Wt|W<t = w<t, Wt0|W<0 t = w<t)].
—	w<t 〜W<t
The lemma follows from a summation over t = 1,...,T.	■
The following lemma (see e.g., (Duchi, 2007, Section 9)) gives a closed-form formula for the KL-
divergence between two Gaussian distributions.
Lemma 18. Suppose that P = N(μι, ∑ι) and Q = N(μ2, ∑2) are two Gaussian distributions on
Rd. Then,
KL(P,Q) = 1 (tr(Σ-1∑1) + (μ2 - μι)>Σ-1(μ2 - μι) - d + lnde^∣4).
2	det(Σ1 )
The following lemma (Topsoe, 2000, Theorem 3) helps us to upper bound the KL-divergence.
Definition 19. Let P and Q be two probability distributions on Rd. The directional triangular
discrimination from P to Q is defined as
+∞
△* (P, Q) = X 2k ∙ ∆ (2-kP + (1 - 2-k)Q, Q),
k=0
where
△ ( p Q) _ [ (P(W) - Q(W))2 d
(，Q)= JRd P(w) + Q(w) d .
Lemma 20. For any two probability distributions P and Q on Rd,
KL(P7Q) ≤ ln2 ∙ ∆*(P,Q).
Recall that G is the set of all possible mini-batches. Gn = {B ∈ G : n ∈ B} denotes the collection
of mini-batches that contain n, while Gn = G \ Gn. diam(A) = suPχ,y∈A Ilx - yk denotes the
diameter of set A. The following technical lemma upper bounds the KL-divergence between two
Gaussian mixtures induced by sampling a mini-batch from neighbouring datasets.
Lemma 21. Suppose that batch size b ≤ n/2. {μB : B ∈ G} and {μ0B : B ∈ G} are two
collections of points in Rd labeled by mini-batches of size b that satisfy the following conditions for
some constant β ∈ [0, σ]:
1.	∣μB — μB k ≤ β for B ∈ Gn and μB = μB for B ∈ Gn.
2.	diam({μB : B ∈ G}∪ {μB : B ∈ G}) ≤ σ∕10.
Let pμ,σ denote the GauSSian distribution N(μ,号Id). Let P = 点 Pb∈gP*b,σ and P0
或 Σ2b∈g PμlB,σ be two mixture distributions over all mini-batches. Then,
8.23b2β2
σ2n2
KL(P, P 0) ≤
14
Published as a conference paper at ICLR 2020
Proof of Lemma 21 By Lemma 20, KL(P, P0) is bounded by
+∞
ln2 • △* (P, P0) = ln2 • X 2k • ∆ (2-kP +(1 - 2-k)P0, P)
k=0
= ln2 X2k [	4-k(P(W)- P0(w))2	dw
=^ k=0 2 IRd 2-kP(w) + (2 - 2-k)P0(w)d .
The numerator of the above integrand is upper bounded by
4-k (P - P0)2 = 4-k (ɪ X (pμB ,σ- PμB ,σ ))
4-k|Gn|2
|G|2
4-kb2
≤ -ʃ
n2
ɪ X (PμB ,σ- Pub ,σ )了
n B∈Gn
(9)
• |G7 X (PNB ,σ - pμB,σ )2,
|Gn| B∈Gn
while the denominator can be lower bounded as follows:
-k
2-kP + (2 - 2-k)P0 ≥ —
|G|
2-2-k
p-Bp+ ,σ +	|G|	p-Bp^,B ,σ
B∈Gn	B∈Gn
⅛ X
B∈Gn
pμB,σ
(μB = μB for B ∈ Gn)
1
=
|Gn|
1
≥
一|Gn|
2(n - b)
n	p-B pμB ,σ
B∈Gn
ΣPμB ,σ ,
B∈Gn
(b ≤ n/2)
which implies, by the convexity of 1/x, that
1
2-kP +(2 - 2-k )P0 ≤ 尚 Pb∈g7 p“B ,σ
1
≤ |Gn| 8
'nl B∈Gn
pμB ,σ
(10)
1
1
Inequalities (9) and (10) together imply
∆ (2-kP +(1 - 2-k)P0,P0) ≤
4-kb2
n2∣Gn∣∣Gn∣
XX
A∈GnB∈GnRdd
(P“B,σ (W) - PμB,σ (W))2
PμA,σ (W)
dW.
(11)
Now we bound the right-hand side of (11) for fixed A and B . By applying a translation and a
rotation, We can assume without loss of generality that μA = 0, and the last d - 2 coordinates of
μB and μB are all zero. Note that the integral is unchanged when we project the space to the two-
dimensional subspace corresponding to the first two coordinates. Thus, it suffices to prove a bound
for d = 2. We rewrite (11) as
∆(2-kP+(1-2-k)P0, P0) ≤
4-kb2
n2∣Gn∣∣Gn∣
Σ Σ
A∈Gn B∈Gn
e-kTk2 - e
w-μB∣12∖ 2
e-k σ k2
dW.

(12)
15
Published as a conference paper at ICLR 2020
Let I be the integral in the right-hand side of (12). Note that ∣∣ 等 ∣∣, ∣∣ μB^ ≤ 0.1 and ∣∣ μB-μB ∣∣ ≤
β∣. Let δ = σ and r = ∣∣ W ∣∣.OUrgoaIistoboUnd maxδ∈[0,0.i](Iδ-2). Let (x)+ = max(x, 0).
Since
I ≤ σ2 /∞ maxy∈[(r-0.1)+,r+0.1"-y2 - e-(y+δ)2 户 2∏r 4兀
_	Jo	e-r2
We have
max 1 ≤ 我厂 er22πr	max max L - ,"+"2 2) dr.
δ∈[0,0.1] δ2	0	y∈[(r-0.1)+,r+0.1] δ∈[0,0.1]	δ2
Let φ(y, δ) = (e y -e (y+δ) )2, We make two claims which We will prove later:
1.	Forall y,δ ≥ 0, φ(y,δ) ≤ 2.
2.	For all y ≥ √1), φ(y, δ) is non-increasing in δ.
The above claims imply that:
1.	For any r ∈ [0, √1) + 0.1] , maXy∈[(r-0.1)+,r+0.1],δ∈[0,0.1] [φ(y, δ)] ≤ 号∙
2.	For any r ∈ (√) + 0.1, +∞),wehave
max	φ(y, δ) ≤	max	lim [φ(y, δ)]
y∈[(r-0.1)+,r+0.1],δ∈[0,0.1]	y∈[(r-0.1)+,r+0.1] δ→0
=	max	4y2 e-2y2
y∈[(r-0.1)+,r+0.1]
= 4(r -0.1)2e-2(r-0.1)2.
The last step holds since y → y2e-2y2 is decreasing on √), +∞).
ThUs we have
I
max -ɪ
δ∈[0,0.1] δ2
2 √2+ +0.1 r2	2	2
≤ σ2 j	e 2πr ∙ - dr + σ2
Z +	er22πr ∙ 4(r - 0.1)2e-2(r-0.1)2 dr
J √+0.1
2
≤ 18.6487σ2.
PlUgging the above into (12) gives
4-k b2	4-k b2
∆(2-kP + (1 — 2-k)P0,P0) ≤	δ2 max (I∕δ2) ≤	∙ 18.6487δ2.
n2σ2π δ∈[0,0.1]	n2π
We conclUde that
+∞
KL(P, P0) ≤ ln2 X 2k ∙ ∆(2-kP + (1 - 2-k)P0, Pr)
k=0
37.297-	b2δ2ln2 ≤ 8≡2
n2π	n2σ2
Finally, we prove the two claims Used above:
1.	For all y, δ ≥ 0, let h(x) = e-x2, we have e-y2 - e-(y+δ)2 = Ryy+δ h0(t) dt. Since
∣h0(t)| = ∣e-t2(-2t)∣ = e-t2(2t). Let 品∣h0(t)∣ = 0, we have t = 1∕√2. Thus, ∣h0(t)∣ ≤
e1/2V2 and e-y2 - e-(y+δ)2 ≤ δ,2∕e.
2.	Suppose y ≥ 1/√2, we have
∂ ee-y2 - e-(y+δ)2 ) _ e-(y+δ)2 (2yδ + 2δ2 + 1) - e-y2
∂δ ( δ = =	δ2
-y2
=ɪ [e-2yδ-δ (2yδ + 2δ2 + 1) - 1].
Let g(y, δ) = e-2yδ-δ2 (2yδ + 2δ2 + 1) - 1. Note that
16
Published as a conference paper at ICLR 2020
(a)	limδ→o ∂∂δ (e-y2-e-(y+δ)2 )=0.
(b)	∂g(1∕√2, δ) = -4e-δ(δ+√2)δ2(δ + √2) < 0 and g(1∕√2, 0) = 0.
It implies that 品(e (1") ) (1∕√2+δ) ) ≤ 。for δ > 0. Since ∂yg(y,δ)
-4δ2e-δ(δ+2y)(δ + y) < 0 for y ≥ 0, We conclude that for any y ≥ 1∕√2:
ɪ (e-y2 -≤ 焉(L -「"J2) ≤ 0.
∂δ	δ	∂δ	δ
A.3 Main Theorem
Recall that SGLD on dataset S is defined as
Wt J Wt-1
- YtVwF(Wt-1, SBt) + √=N(0, Id).
Here γt is the step size. Bt = {i1, . . . , ib} is a subset of {1, . . . , n} of size b, and SBt =
(ziι,..., Zib) is the mini-batch indexed by Bt. Recall that F(w, S) denotes 寺 Pi=1 F(w, zi).
We restate and prove Theorem 11 in the folloWing.
Theorem 11. Suppose that the loss function L is C -bounded and the objective function F is L-
lipschitz. Assume that the following conditions hold:
1.	Batch size b ≤ n∕2.
2.	Learning rate γt ≤ σt∕(20L).
Then, the following expected generalization error bound holds for T iterations of SGLD (1):
errgen ≤ 812Ct E n [X γt2 ge(t)
n S S 〜Dn	σ t
t=1 t
(empirical norm)
where ge(t) = Ew〜w1 [* Pn=IkVF(w, Zi)k2] is the empirical squared gradient norm, and Wt
is the parameter at step t of SGLD.
Proof By Theorem 7, We have
errgen ≤ 2CE Vz2KL(Qz,P)
z
(13)
for any prior distribution P. In particular, we define the prior as P(w) = ES〜0n-ι [%(w)], where
PS(w) = Q(S 0). By the convexity of KL-divergence,
KL(Qz ,P) = KL (j[Q(s,z)], E[Q(s,o)]j ≤ E [KL (Q(Ez),Q(s,。)) ].	(14)
Fix a data point Z ∈ Z. Let (Wt)t≥o and (Wj0)t≥o be the training process of SGLD for S = (S,ζ)
and S0 = (S, 0), respectively. Fix a time step t and w<t = (wo,..., wt-1). Let Pt and Pt denote
the distribution of Wt and Wt0 conditioned on W<t = w<t and W<0 t = w<t, respectively. By the
definition of SGLD, we have Pt =由 PB∈GP“b and Pt =由 PB∈GP“B, where 〃b = wt-i —
YtVwF(Wt-i,Sb), μB = wt-i 一 YtVwF(wt-ι,SB), and pμ denotes the Gaussian distribution
σ2
N(μ, -ɪId). We note that:
1. kμB 一 μB k ≤
Ytk▽F(WtT,z)k2 for B ∈ Gn and μB = μB for B ∈ Gn.
17
Published as a conference paper at ICLR 2020
2. diam({μB : B ∈ G}∪ {μp : B ∈ G}) ≤ 2γtL ≤ σt∕10.
By applying Lemma 21 with β
YtkVF (w：
t-1,z)k2 andσ = σt,
KL(Pt,Pt0) ≤
8.23γ2 RF(wt-i,z)k2
σ2n2
b
By Lemma 10,
T
KL(W≤T,W≤0T) = X
t=1
T
≤X
t=1
E	[KL(Pt, Pt0)]
w<t 〜W<t
E
w^Wt-1
8.23γ2 kVF(w,z)k2
σ2n2
which implies that
KL(QS, QS0) =KL(WT,WT0) ≤KL(W≤T,W≤0T)
≤ 8n23 Xγ2 WJLIhkVF (w,z)k2i.
Together with (13) and (14), we have
errgen ≤ 2C E
z
2 E
S
8n3 Xγ2 WJLIhkVF (w,z)k2i
≤ 2Ct
2E
S
8n23 X σ2 wJLι
kVF (w, zn)k22	.
(concavity of √X)
Since SGLD is order-independent, We can replace VF(w, Zn) with VF(w, Zi) for any i ∈ [n] in the
right-hand side of the above bound. Our theorem then follows from the concavity of √X. Further-
more, if we bound KL(P, Qz) instead of KL(Qz, P ) in the above proof, we obtain the following
bound that depends on the population squared gradient norm:
8.12C u	γt2	2
errgen ≤ — tE [X σ2 WJLzEDkVF (W,z)k2
A.4 Extension to General Noises
We can extend the generalization bounds in previous sections, which require the noise to be Gaus-
sian, to other general noises, namely the family of log-lipschitz noises.
Definition 22 (Log-Lipschitz Noises). A probability distribution on Rd with density p is L-log-
lipschitz if and only if kV ln p(w)k ≤ L holds for any w ∈ Rd. A random variable ζ is called an
L-log-lipschitz noise if and only if it is drawn from an L-log-lipschitz distribution.
The analog of SGLD, noisy momentum method (Definition 24), and noisy NAG (Definition 25)
can be naturally defined by replacing the Gaussian noise ζt at each iteration with an independent
L-log-lipschitz noise in the definition.
The following lemma is an analog of Lemma 21 under L-log-lipschitz noises. Recall that G denotes
a collection of mini-batches of size b. Lemma 23 readily implies the analogs of Theorems 11,
26 and 27 under more general noise distributions.
18
Published as a conference paper at ICLR 2020
Lemma 23. Suppose that batch size b ≤ n/2 and N is an Lnoise-log-lipschitz distribution on Rd.
{μB : B ∈ G} and {μB : B ∈ G} are two collections of points in Rd that satisfy the following
Conditionsfor some constant β ∈ [θ, LI^]:
1.	kμB - μB k ≤ β for B ∈ Gn and μB = μB for B ∈ Gn.
2.	diam({μB : B ∈ G}∪ {μB : B ∈ G}) ≤ 1.
For μ ∈ Rd, letpμ denote the distribution of Z+μ when Z is drawnfrom N. Let P 二高 Σ2b∈g P*b
and P0 二 高 ∑2b∈g Pμ,β be mixture distributions over all mini-batches. Then,
KL(P,P0) ≤ C0b2β2
n2
for some constant C0 that only depends on Lnoise.
Proof of Lemma 23 Following the same argument as in the proof of Lemma 21, we have
+∞
KL(P,P0) ≤ ln2 ∙ X 2k ∙ ∆(2-kP + (1 - 2-k)P0,P0)
k=0
(15)
where
∆(2-kP + (1 -2-k)P0,P0) ≤
4-kb2
n2∣Gn∣∣Gn∣
XX
A∈GnB∈Gn Rdd
(P“B (W) - P“B(w))2 dw
(16)
Fixed A ∈ Gn and B ∈ Gn. Let pnoise denote the density of the noise distribution N. Since
k〃A - μB k ≤ 1 andPnoiSe is Lnoise-Iog-Iipschitz, We have
PμB (w) = Pnoise (w - "B ) ≤ Pnoise (w - 〃A)∙ eLnoisek"A-"B k ≤ eLnoise p“A (w).
Similarly, since ∣∣μB - μB ∣∣ ≤ β,we have
e-βLnoisePμB (w) ≤ PμB (w) ≤ eβLnoisePμB (w).
Then, it follows from βLnoise ≤ 1 that
(PμB (W)- Pμ'B (w))2 ≤ (eβLnoise - 1)2PμB (w)2 ≤ β2LnoisePμB (w)2∙
Therefore, the integral on the righthand side of (16) can be upper bounded as follows:
Z (P"B (W)-pμB (W))2 dw <β2Lnoij PB^
JRd	PμA (w)	JRd PμA (W)
<β Lnoise / JμB (w) ∙
2	2	Lnoise
=β noisee .
dw
eLnoise dw
Plugging the above inequality into (15) and (16) gives
4-kb2	4-kb2β2
∆(2-kP + (1 - 2-k)P0, P) <	X X β2L2oiseeLnoise = L2oiseeLnoise ——：∙
n lGnllGn1 A∈Gn B∈Gn	n
and
+∞	4-kb2β2	b2β2
KL(P,P0) < ln2 ∙ £2kLnoiseeLnoise-------ɪ = 2足2乙2^eLnoise ∙ ɪ.
nose	n2	nose	n2
k=0
19
Published as a conference paper at ICLR 2020
A.5 Extension to Other Gradient-Based Methods
A.5.1 Stability Bound for Momentum and Nesterovs Accelerated Gradient
We adopt the formulation of Classical Momentum and Nesterov’s Accelerated Gradient (NAG)
methods in Sutskever et al. (2013) and consider the noisy versions of them.
Definition 24 (Noisy Momentum Method). Noisy Momentum Method on objective function F(w, z)
and dataset S is defined as
(Vt J nVt-1 - INwF(Wt-1, SBt) + Zt
[呜 J Wt-1 + 匕
Definition 25 (Noisy Nesterovs Accelerated Gradient). Noisy Nesterovs Accelerated Gradient
(NAG) on objective function F (w, z ) and dataset S is defined as
Vt J ηVt-ι - γtVwF(Wt-ι + ηVt-ι, SBt) + Zt,
Wt J Wt-1 + Vt .
In both definitions, γt is the step size, mini-batch Bt is drawn uniformly from G, Zt is a Gaussian
σ2
noise drawn from N(0,寸Id), and η ∈ [0,1] is the momentum coefficient.
Theorem 26.	Under the same assumptions on the loss function, objective function, batch size and
learning rate as in Theorem 11, the generalization bounds in Theorem 11 still hold for noisy mo-
mentum method and noisy NAG.
Proof of Theorem 26 For any time step t and w<t = (w0, w1, ..., wt-1), let Pt and Pt0 denote the
distribution ofWt and Wt0 conditioned on W<t = w<t and W<0 t = w<t, respectively. By definition,
We have Pt = |GG| Pb∈G pμB and Pt = |G| Pb∈G pμ0B .
If t = 1, for both noisy momentum method and noisy NAG, we have
μB = Wt-1 — YtVw F (wt-1,SB ),
μB = Wt-1 — YtVw F (Wt-1,SB).
For t > 1, if noisy momentum method is used, we have
μB = Wt-1 + η(wt-1 — Wt-2) — YtVwF(wt-1, SB),
μB = Wt-1 + η(wt-1 — Wt-2) — YtVwF(Wt-1, SB).
Similarly, the following holds under noisy NAG:
μB = Wt-1 + η(Wt-1 — Wt-2) — YtVw F(Wt-1 + η(Wt-1 — Wt-2), SB ),
μB = Wt-1 + η(Wt-1 — Wt-2) — YtVw F(Wt-1 + η(Wt-1 — Wt-2), SB).
In either case, it can be verified that the conditions of Lemma 21 hold for β = 2γ-tL and σ = σt.
The rest of the proof is the same as the proof of Theorem 11.	■
A.5.2 Stability Bound for Entropy- S GD
In the Entropy-SGD algorithm due to Chaudhari et al. (2017), instead of directly optimizing the
original objective F (W), we minimize the negative local entropy defined as follows:
—E(w, y) = — log J exp (—F(Wz) — Y ∣∣w — W0∣∣2) dW
(17)
Intuitively, a wider local minimum has a lower loss (i.e., —E(W, Y)) than sharper local minima.
See Chaudhari et al. (2017) for more details. The Entropy-SGD algorithm invokes standard SGD to
minimize the negative local entropy. However, the gradient of negative local entropy
—VwE(W, Y) = Y W — E
∖	w0 〜P
P(W) Y exp(—F(WZ) — Y ∣∣w — wz∣2)	(18)
is hard to compute. Thus, the algorithm uses exponential averaging to estimate the gradient in the
SGLD loop; see Algorithm 1 for more details.
We have the following generalization bound for Entropy-SGD.
20
Published as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 1: Entropy-SGD
Input: Training set S = (z1, .., zn) and loss function g(w, z).
Hyper-parameters: Scope γ, SGD learning rate η, SGLD step size η0 and batch size b.
for t = 1 to T do
//SGD iteration
Wt,o,μt,0 — Wt-ι,κ+ι;
for k = 0 to K - 1 do
//SGLD iteration
Bt,k J mini-batch with size b;
Wt,k + 1 J Wt,k - η0Vwg(Wt,k, SBt,k ) + η0γ(Wt-1,K+1 - wt,k ) + √η0EN(0, 2Id);
μt,k + 1 J (I - α)μt,k + αWt,k;
end
Wt,κ+ι J Wt,κ — ηγ(Wt,κ - μt,κ);
end
return WT,K+1;
Theorem 27.	Suppose that the loss function L is C-bounded and the objective function F is L-
Iipschitz. Ifbatch size b ≤ n/2 and √η7 ≤ ε∕(20L), the following expected generalization error
bound holds for Entropy-SGD:
errgen ≤
8.12CE
εn t
T K-1
ES	ge(t,k) ,
t=1 k=0
(empirical norm)
where ge(t,k) = Ew〜Wt,Jn Pn=IIl VF(w,zi)k2] is the empirical SqUared gradient norm, and
Wt,k denotes the training process with respect to S.
Since ge(t, k) is at most L2, it further implies the generalization error of Entropy-SGD is bounded
by o (* √tk).
Proof of Theorem 27 Define the history before time step (t, k) as follows:
W≤(t,k) = (W0,0, ..., W0,K+1, ..., Wt-1,0, ..., Wt-1,K+1, Wt,0, ..., Wt,k).	(19)
Since μ is only determined by W, We only need to focus on W. This proofis similar to the proof of
Theorem 11. By setting P = ES[Q(s °)]. Suppose S = (S, Z) and S0 = (S, 0) are fixed, let W and
W0 denote their training process, respectively. Considering the following 3 cases:
1.
2.
Wt,0 J Wt-1,K+1: In this case, for a fixed w≤(t-1,K+1), we have
KL (Wt,0∣w≤(t-l,κ+l) = w≤(t-i,κ+ι),wt,0lW≤(t-i,κ+ι) = w≤(t-1,K+1)) = 0∙
Wt,k+1 J Wt,k - η0Vwg(Wt,k, SBt,k ) + η0γ(Wt-1,K+1 - Wt,k ) + √η0εN(0, 11d): In
this case, fix a w≤(t,k), applying Lemma 21 gives
3.
KL (Wt,k+1|WS(t,k) = w≤(t,k),Wtz,k+1lW≤(t,k) = w≤(t,k)) ≤
8.23η0 IVF(wt,k, z)I22
Wt,κ+ι J Wt,κ — ηγ(Wt, K — μt,κ): In this case, for a fixed w
ε2n2
≤(t,K), we have
KL (Wt,K+1W≤(t,K) = w≤(t,K), Wt',K+1W≤(t,K) = w≤(t,K)) = °.
By applying Lemma 10, we have
KL(WT,K+1,WT0,K+1) ≤
答 XXIge(t,k),
ε2n2
and Where ge (t, k) is the empirical squared gradient norm of the k-th SGLD iteration in the t-th
SGD iteration, respectively. The rest of the proof is the same as the proof of Theorem 11.	■
21
Published as a conference paper at ICLR 2020
B Proofs in Section 4
B.1	Markov Semigroup and Log-Sobolev Inequality
The continuous version of the noisy gradient descent method is the Langevin dynamics, described
by the following stochastic differential equation:
dWt = -VF(Wt) dt + √2β-1 dBt,	Wo 〜μo,	(20)
where Bt is the standard Brownian motion. To analyze the above Langevin dynamics, we need some
preliminary knowledge about Log-Sobolev inequalities.
Let pt (w, y) denote the probability density function (i.e., probability kernel) describing the distri-
bution of Wt starting from w. For a given SDE such as (20), we can define the associated diffusion
semigroup P:
Definition 28 (Diffusion Semigroup). (see e.g., (Bakry et al., 2013, p. 39)) Given a stochastic
differential equation (SDE), the associated diffusion semigroup P = (Pt)t≥0 is a family of operators
that satisfy for every t ≥ 0, Pt is a linear operator sending any real-valued bounded measurable
function f on Rd to
Ptf(w) =E[f(Wt)|W0 =w] =
/
Rd
f (y)pt (w, dy).
The semigroup property Pt+s = Pt ◦ Ps holds for every t, s ≥ 0. Another useful property of Pt
is that it maps a nonnegative function to a nonnegative function. The carre du champ operator Γ of
this diffusion semigroup (w.r.t (20)) is (Bakry et al., 2013, p. 42)
Γ(f,g)=β-1hVf,Vgi.
We use the shorthand notation Γ(f) = Γ(f, f) = β-1 kVf k22, and define (with the convention that
0 log 0 = 0)
Entμ (f)
/
Rd
f log f dμ 一
/
Rd
f dμ log
URd f W)
Definition 29 (Logarithmic Sobolev Inequality). (see e.g., (Bakry et al., 2013, p. 237)) A probability
measure μ is said to satisfy a logarithmic Sobolev inequality LS( α) (with respect to Γ), if for all
functions f : Rd → R+ in the Dirichlet domain D(E),
Ent" (f) ≤ ； Z 邛 dμ.
2 Rd f
D(E) is the set of functions f ∈ L2(μ) for which the quantity 1 JRd f (f 一 Ptf) dμ has a finite
(decreasing) limit as t decreases to 0.
A well-known Logarithmic Sobolev Inequality is the following result for Gaussian measures.
Lemma 30 (Logarithmic Sobolev Inequality for Gaussian measure). (Bakry et al., 2013, p. 258)
Let μ be the centered Gaussian measure on Rd with covariance matrix σ2Id. Then μ satisfies the
following LSI:
Ent" (f) ≤ σ2 / kvfk2 dμ
2	Rd	f
Lemma 30 states that the centered Gaussian measure with covariance matrix σ2Id satisfies LS(βσ2)
(with respect to Γ), where Γ = β-1hVf, Vgi is the carre du champ operator of the diffusion Semi-
group defined above.
Before proving our results, we need some known results from Markov diffusion process. It is well
known that the invariant measure (Bakry et al., 2013, p. 10) of the above CLD is the Gibbs measure
dμ = Z- exp(-βF(W)) dw (Menz et al., 2014, (1.3)). In other words, μ satisfies JRd Ptfdμ =
JRd fdμ for every bounded positive measurable function f, where Pt is the Markov semigroup
in Definition 28. The following lemma by Holley and Stroock Holley & Stroock (1987) (see also
(Bakry et al., 2013, p. 240)) allows us to determine the Logarithmic Sobolev constant of the invariant
measure μ.
22
Published as a conference paper at ICLR 2020
Lemma 31 (Bounded perturbation). Assume that the probability measure ν satisfies LS(α) (with
respect to Γ). Let μ be a probability measure such that 1/b ≤ dμ∕dν ≤ b for some constant b > 1.
Then μ satisfies LS(b2α) (with respect to Γ).
In fact, Lemma 31 is a simple consequence of the following variational formula in the special case
that φ(x) = x log x, which we will also need in our proof:
Lemma 32 (Variational formula). (see .g., (Bakry et al., 2013, p. 240)) Let φ : I → R on some
open interval I ⊂ R be convex of class C2. For every (bounded or suitably integrable) measurable
function f : Rd → R with values in I,
rin∈fI Rd[φ(f)-φ(r)
-Φ0(r)(f — r)] dμ.
(21)
It is worth noting the integrand of the right-hand side is nonnegative due to the convexity of φ.
B.2	Logarithmic Sobolev Inequality for CLD
Recall that FS (w) = F(w, S) := F0(w, S) + λ kwk22/2 is the sum of the empirical original ob-
jective F°(w, S) and '2 regularization. Let dμ = 六 exp(-βFs(W)) dw be the invariant (Gibbs)
measure of CLD, and V is the centered Gaussian measure dν = Z- exp(-βλ ∣∣wk2 /2) dw. In-
voking Lemma 30 with σ2 = λβ shows that V satisfies LS(1∕λ) (with respect to Γ). Consider the
density h(w)= 器 = fν exp(-βFo(w, S)). If the original objective function Fo is C-bounded,
μ
we have exp(-2βC) ≤ h(w) ≤ exp(2βC). By applying Lemma 31 with b = exp(2βC), we have
the following lemma.
Lemma 33. Under Assumption 14, let Γ(f,g) = β-1hVf, Vgi be the carre du champ operator
of the diffusion semigroup associated to CLD, and μ be the invariant measure of the SDE. Then, μ
satisfies LS(e4β C /λ) with respect to Γ.
Let μt be the probability measure of Wt. By definition of Pt, for any real-valued bounded measur-
able function f on Rd and any s, t ≥ 0,
E [f (w)] = E [Ptf(w)].	(22)
W 〜μt+s	W 〜μs
In particular, if the invariant measure μ = μ∞ exists, We have
E [f(w)] = E [Ptf(w)] = E [f(w)] = E [Ptf(w)].	(23)
W〜μ	W〜μ∞	W〜μt+∞	W〜μ
The following lemma is crucial for establishing the first generalization bound for CLD. In fact, we
establish a Log-Sobolev inequality for μt, the parameter distribution at time t, for any time t > 0.
Note that our choice of the initial distribution μo is important for the proof.6
Lemma 34. Under Assumption 14, let μt be the probability measure of Wt in (CLD) with initial
—	-λβ k w k 2
probability measure dμo = 1 e	2	dw. Let Γ be the carre du champ operator of diffusion
semigroup associated to (CLD). Then, for any f : Rd → R+ in D(E):
Ent"t (f) ≤ > Z 邛 dμf,
2	Rd f
Proof Let μ be the invariant measure of CLD. By Lemma 33 and Definition 29,
Ent”(f) ≤
≡f⅛ dμ.
(24)
6 For arbitrary initial distribution, it is impossible to prove similar inequality for any t ≥ 0 (unless the loss
is strongly convex).
23
Published as a conference paper at ICLR 2020
By applying Lemma 32 with φ(x) = x log x, we rewrite the left-hand side as
Entμ(f) :=//logf dμ — // d〃log
inf
r∈I
inf
r∈I
/ JΦ(f) - Φ(r) - Φ0(r)(f — r)] dμ
La'Pt9(f)— ℃ — Φ0(r)(f -r))]dμ.
where the last equation holds by the definition of invariant measure / Ptf dμ = Jf dμ. Thus, We
have
rni LdPWf)—φ(r)—φ0(r)(f-r))]dμ=Entμ (f) ≤ W Ld ɪ dμ,
Let μt be the probability measure of Wt. Lemma 32 and (22) together imply that
(25)
Entμt (f) = inf
r∈I
inf
r∈I
/ JΦ(f) — Φ(r) — Φ0(r)(f — r)] dμt
JJPt (φ(f) — φ(r) — φ0(r)(f - r))] dμo
(26)
Since Pt(φ(f) — φ(r) — φ0(r)(f — r)) ≥ 07 and dμ0 ≤ exp(2βC), we have
Ent”,(f) = rnf ZJPt (Φ(f) — Φ(r) — Φ0(r)(f - r))]dg0 dμ
≤ exp(2βC )Entμ(f) ≤ £C Z kf- dμ.
2λβ Rd f
Since ddμ ≤ exp(2βC) and μ is the invariant measure, we conclude that
(27)
Entμt (f) ≤
≤
Pt
Pt
kVfk；	e6βC
丁 dμ =而
Wk2 ʌ dμ
f
Wk2
f
dμo
dμo
dμo
β-1kVfk2
f
dμt
Pt
Rd
dμ
(28)
e8βC/λ / Γ(f)
~2~ JRd f dμt
Lemma 16. Under Assumption 14, let μt be the probability measure of Wt in CLD (with dμo =
-Z e 2 dw). Let V be a probability measure that is absolutely continuous with respect to μt.
Suppose dμt = ∏t(w) dw and dν = γ(w) dw. Then it holds that:
KL(γ, πt) ≤
exp(8βC)
-2λβ-
/
Rd
V log πww 卜(W)dw.
(29)
Proof Let f (w) = γ(w)∕∏t(w), by Lemma 34 and JRd f dμt = 1, we have
/ f log f dμt ≤ 皆 Z f∙ dμt	(30)
Rd	2λβ Rd	f
7 This is because φ is convex and Pt is a positive operator.
24
Published as a conference paper at ICLR 2020
We can see that the left-hand side is equal to KL(γ, πt) 8, and the right-hand side is equal to
2
e8βC
2λβ
V ∏YtW)
Rd Y(W)/nt (W)
πt(w) dw
e8βC /
2λβ JRd
V log π(w) 12Y(W)dw.
2
This concludes the proof.
B.3	The Discretization Lemma from Raginsky et al. (2017)
Let h(w, z) = F0(w, z) + ʌk^k2. We can rewrite FS(w) = 1 PZi h(w, Zi). Define μs,k and νs,t
as the probability measure of Wk (in GLD) and Wt (in CLD), respectively. Raginsky et al. (2017)
provided a bound of KL(μs,k, νs,ηκ) under Assumption 35. This bound enables Us to derive a
generalization error bound for the discrete GLD from the bound for the continuous CLD. We use
the assumption from Raginsky et al. (2017). Their work considers the following SGLD:
Wk+1 = Wk - ηgs (Wk) + √2¾β-1ξk.
Where gS(Wk) is a conditionally unbiased estimate of the gradient VFS (Wk). In our GLD setting,
gS(Wk) is equal to VFS (Wk).
Assumption 35. Let FS(w) = 1 Pn=ι h(w, Zi) = Fo(w, S) + λ ∣∣wk2.
1.	The function h takes non-negative real values, and there exist constants A, B ≥ 0, such
that
|h(0, z)| ≤ A and ∣Vh(0, z)∣2 ≤ B ∀z ∈ Z.
2.	For each Z ∈ Z, thefUnction h(∙, z) is M-smooth: for some M > 0,
∣Vh(w, z) - Vh(v, z)∣2 ≤ M ∣w - v ∣2 ,	∀w, v ∈ Rd.
3.	For each Z ∈ Z, thefunCtion h(∙, z) is (m, b)-dissipative: for some m > 0 and b ≥ 0,
hw, Vh(w, z)i ≥ m ∣w∣22 - b,	∀w ∈ Rd.
4.	There exists a constant δ ∈ [0, 1), such that, for each S ∈ Zn,
E[∣gS(w) -VFS(w)∣22] ≤ 2δ M2 ∣w∣22 + B2 ,	∀w ∈Rd.
5.	The probability law μo of the initial hypothesis W0 has a bounded and Strictly positive
density p0 with respect to the Lebesgue measure on Rd, and
κ0 := log
f
Rd
ekwk22 p0 (w) dw < ∞.
Lemma 36. (RaginSky et al., 2017, Lemma 7) Suppose that Assumption 35 holds and set μs,0
νs,0 = μo. Then, for any k ∈ N and any η ∈ (0,1 ∧ 4^2), the following inequality holds
KL(μs,k,νs,ηk) ≤ (C0βδ + Cιη)kη,
where C0 and C1 are constants that only depend on M, κ0, m, b, β, B and d.
B.4	Proofs for Main Theorems
1	-λβkwk2
Theorem 15. Under Assumption 14, CLD (with initial probability measure dμ0 =三 e 2	dw)
has the following expected generalization error bound:
8Indeed, RRdflogf dμt = RRd πγt log(γ)πtdw = KL(γ,πt)
(31)
25
Published as a conference paper at ICLR 2020
In addition, if F0 is also M -smooth and non-negative, by setting λβ > 2, λ > 0 and η ∈ [0, 1 ∧
8MM2), the GLD (running K iterations with the same μo as CLD) has the expected generalization
error bound:
err gen ≤ 2C vfKCC2η+ +
(32)
where C1is a constant that only depends on M, λ, β, b, L and d.
Proof of Theorem 15 We apply the uniform stability framework. Suppose S and S0 are two neigh-
boring datasets that differ on exactly one data point. Let (Wt)t≥0 and (Wt0)t≥0 be the process of
CLD running on S and S0, respectively. Let γt and πt be the pdf of Wt0 and Wt . We have
ddtKL(Yt ,πt) = d∣ ZdYt log πtdw
[d dYt 1	Yt .	πt ddtt πt - Yt 唔!
=JRd (m log πt+ Yt ∙ Yt .一∏2一)
=Ld (P log ∏t)dw - Ld (∏t dπt)dw
(33)
According to Fokker-Planck equation (see Risken (1996)) for CLD, we know that
∂Yt	1	∂πt	1
=∆Yt + V ∙ (YtVFS0),	=— = -∆∏t + V ∙ (∏tVFs).
∂t	β	∂t	β
It follows that
I:
log Yt j dw
πt
L (1∆Yt + V ∙ (YtVFsο)) log ∏t dw
hV log ∏t，VYti dw - Ldl * *hV log ∏t，YtVFsoi dw，
(integration by parts)
and
J:
Πt?) dw
Y Yt (1∆∏t + V ∙ (∏tVFs)) dw
Rd πt β
hvYt, V∏ti dw - j hVYt,∏tVFsi dw.
Together with (33), we have
ddt KL(Yt,a = I — J
(integration by parts)
VYt	Vπt	VYt	YtVπt
丁— -,VYti-IF - -^r,
Yt
Yt V log —
πt
Yt
Yt V log —
πt
Rd	Yt	πt
log Yt ,YtVFsο i
πt
dw
dw
πt
πt
Yt	Yt
----〈▽ log 一，∏t
dw + / YthVlog Yt, VFS -VFsoi dw
dw + β Z Yt kVFs -VFsοk2 dw.
2	2 Rd
2
26
Published as a conference paper at ICLR 2020
The last step holds because ha∕√β, b√βi ≤ kak2 +
Lemma 16, we have
βkbk2. Since M -VFs，k2
≤ 4L2,by
which implies
e8βc
KL(Yt,πt) ≤ ɪr	Yt
2λβ Rd
-λ
e8βc KL(Yt ,πt)
γt
▽ log Yt
πt
2
2
▽ log Yt
πt
dw,
2
dw.
2
Hence,
d	-λ	2βL2
dtKL(Yt, πt) ≤ e8βcKL(Yt, πt) +	n2~, With KL(Yo, πo) = 0.
Solving this differential inequality gives
τ∕τ, 、	2βL2e8βc(1 - e-λ"e8βC)
KL(Yt,∏t) ≤ -----------⅛-------------L
(34)
(35)
By Pinsker’s inequality, we can finally see that
呼呼L(WT,z)-L(WT,z)]l≤ 2CJIKL(YT,πτ) ≤
2e4βCCL u β 1 -
_ λT
e-e8βc
By Lemma 4, the generalization error of CLD is bounded by the right-hand side of the above in-
equality.
Now, we prove the second part of the theorem. Let (Wk)k≥0 and (Wk0 )k≥0 be the (discrete) GLD
processes training on S and S0, respectively. Then for any z ∈ Z :
| E[L(WK, z)] - E[L(WK0 , z)]|
≤ 2C ∙ TV(μs,κ, μso,κ)
(C-boundedness)
≤ 2C ∙ (TV(μs,K, νs,ηK) + TV(νs,ηK, νs0,ηK) + TV(μs0,K, νs0,ηK)).
Since λβ > 2 and λ > 2, Assumption 35 holds with A = C, B = L, m = λ, b = L2, δ = 0 and
κ0 = d log ( 1 +
. By applying Pinskers inequality and Lemma 36, we have
TV(μs,κ,νs,ηK) ≤ '2KL(MS,k,νs,ηK) ≤ ʌ/2KCιη2
(36)
and
TV(μso,κ ,νs0,ηK) ≤ 4 2KL(MS0,k , νs0,ηK) ≤ '2 KCιη2.
From (35), we have
(37)
TV(VS,ηK,νs0,ηK) ≤ ʌ/2KL(VSZK^ZZK) ≤ ∖
Combining (36), (37) and (38), we have
ληK
βL2e8βc (1 — e-e8βC
n2λ
(38)
|E[L(Wk,z)] — E[L(WK,z)]∣ ≤ 2C√2K0ιη2 +
2CLe4βC u β 1- e
ληK
e8βC
—∖
λ

—∖
λ
n .
By Definition 3, GLD is n -uniformly stable. Applying Lemma 4 gives the generalization bound of
GLD.	■
Lemma 37 (Exponential decay in entropy). (Bakry et al., 2013, Theorem 5.2.1) The logarithmic
Sobolev inequality LS(α)for the probability measure μ is equivalent to Saying thatfor every positive
function P in L1(μ) (with finite entropy),
Entμ(PtP) ≤ e-2"αEnt"(ρ)
for every t ≥ 0.
27
Published as a conference paper at ICLR 2020
The following Lemma shows that Pt(翌)=μt in our diffusion process.
Lemma 38. Let P denote the diffusion semigroup ofCLD. Let μ denote the invariant measure of P
and let μt denote the probability measure of Wt. Then Pt( dμ0) = μt.
Proof Let dμ = μ(x) dx and dμt = μt(x) dx. AS shown in (Pavliotis, 2014, page 118), our dif-
fusion process (SmOlUchOWSki dynamics) is reversible, which means μ(x)pt(x, y) = μ(y)pt(y, x).
Thus for any g(x), we have
E [g(x)] =
X 〜m(dμμo)	J
=Z
=Z
=Z
g(x) μ(x)(Pt(dμo∕dμ))(x)dx
g(x)μ(x)dx / μo(y)pt(x,y)∕μ(y)dy
g(x)μ(x)dx / μo(y)pt(y,x)∕μ(x)dy
g(x)μ(x)μt(x)/μ(x)dx = E [g(x)].
X〜μt
Since g is arbitrary, Pt(dμ0) and μt must be the same.
Theorem 39. Suppose that n > 8βC. Under Assumption 14, CLD (with initial distribution dμo
λ	λβ k w k 2
z1 e	2- dw) has the following expected generalization error bound:
8βC2	C (-λT∖ E
err gen ≤ ———+ 4C exp ( e4^ ) √βC.
In addition, if Fo is also M-smooth and non-negative, by setting λβ > 2, λ > 1 and η ∈
[0,1 ∧ 8λ-1), the GLD process (running K iterations with the same μo as CLD) has the expected
generalization error bound:
errgen ≤ 2CP2KCin2 +------+ 4C eχp ( —4Cc ) PβC,
n	e4βC
where Ci is a constant that only depends on M, λ, β, b, L and d.
Proof of Theorem 39 Suppose S and S0 are two datasets that differ on exactly one data point. Let
(Wt)t≥o and (Wt)t≥o be their processes, respectively. Let dμt = ∏t(w) dw and dμt = ∏0(w) dw
be the probability measure of Wt and Wt0 , respectively. The invariant measure of CLD for S and S0
are denoted as μ and μ0, respectively. Recall that
dμ = ɪe-βFS(W) dw,	dμ0 = ɪe-βFs0 (W) dw.
Zμ	Zμ0
The total variation distance of μ and μ0 is
TV(μ,μ0) = 1 /
2 Rd
2 ∕d
1	dμ0 r,
1 - 丁 dμ
dμ
1 - Zr- exp(-β(Fso(w) - FS(W)))
Zμ0
ɪe-βFS(W) dw.
Z”
(39)
Since 况 exp(-β(FSo (w) - FS(w))) ∈ [e-4βC, e4βC] and 4βc < 1/2, We have
TV(μ,μ0) ≤ max	1 - e-4βC) , 2 (e4βC - 1)} ≤ 4^.
(40)
Since μ and μ0 satisfy LS(e4βC") (Lemma 33), applying Lemma 37 with P = dμ0 and ρ0 = 给
and Lemma 38 yields:
KL(μt,μ) ≤ exp
KL(μo,μ),
KL(μt,μ0) ≤ exp (-2λt) KL(μ0,μ0).	(41)
28
Published as a conference paper at ICLR 2020
Since KL(μo,μ) and KL(μ0,μ0) are upper bounded by 2βC, Pinsker's inequality implies that
TV(μt,μ) and TV(μt,μ0) are upper bounded by JexP (-4βCt) βC.
note that TV(μt, μt) ≤ TV(μt, μ) + TV(μ, μ0) + TV(μt, μ0), We have
Combining with (40) and
SUP | E[L(Wt,z)-L(WT,z)]∣ ≤ 2C ∙ TV(μt,μt) ≤ 4C jexp (券)IeC + 8^^2
By Lemma 4, the generalization error of CLD is bounded by the right-hand side.
The proof for GLD proceeds in the same Way as the second part of the proof of Theorem 15.
C Experiment Details
We first present the general setup of our experiments:
Dataset: We use MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky & Hinton, 2009) in our
experiments.
Neural network: In our experiments, We test tWo different neural netWorks: a smaller version of
AlexNet (Krizhevsky et al., 2012) and MLP. The structures of the netWorks are similar to What are
used in Zhang et al. (2017a).
•	Small AlexNet: k is the kernel size, d is the depth of a convolution layer, fc(m) is the fully-
connected layer that has m neurons. The ReLU activation are used in the first 6 layers.
1	2	3	4	5	6	7
conv(k:5,d:64)	pool(k:3)	conv(k:5,d:192)	pool(k:3)	fc(384)	fc(192)	fc(10)
•	MLP: The MLP used in our experiment has 3 hidden layers, each having Width 512. We
also use ReLU as the activation function in MLP.
Objective function: For a data point z = (x, y) in MNIST, the objective function is
F (W, z) = - ln(softmax(netW (x))[y]),
where SoftmaX(a)[i] = pi0a[[a ⑺,and netw (x) is the output of the neural network (10 dimensional
vector). Note that the objective function F is exactly the cross-entropy loss.
0/1 loss : The 0-1 loss L01 is defined as:
L01(W,(x,y))= 10
(arg maxi netW (x)[i]) 6= y,
otherwise.
(42)
Random labels: Suppose the dataset contains n datapoint, and the corruption portion is p. We
randomly select n ∙ P data points, and replace their labels with random labels, as in Zhang et al.
(2017a).
C.1 Experimental results for GLD
The result of this experiment (see Figure 1) is discussed in Section 3.1. Here we present our imple-
mentation details.
We repeat our experiment 5 times. At every individual run, we first randomly sample 10000 data
points from the complete MNIST training data. The initial learning rate γ0 = 0.003. It decays 0.995
after every 60 steps, and it stops decaying when it is lower than 0.0005. During the training, we keep
σt = 0.2√2γt. Recall that the empirical squared gradient norm ge(t) in our bound (Theorem 9)
29
Published as a conference paper at ICLR 2020
is Ewt-ι [n Pn=1 ∣∣Vf (Wt-ι,z)k2]. Since it is time-consuming to compute the exact ge(t), in our
experiment, we use an unbiased estimation instead. At every step, we randomly sample a mini-batch
B with batch size 200 from the training data, and use 焉 Pi∈B ∣∣Vf (Wt-ι,zi)∣2 as ge(t) to com-
pute our bound in Figure 1. The estimation of ge(t) at every step t is shown in Figure 1(d). Since
ge(t) is not very stable, in our figure, we plot its moving average over a window of size 100 to make
the curve smoother (i.e., gavg(t)= 忐 pT=t0° ge(τ)).
C.2 Experimental Results for SGLD
In this subsection, we present some experiment results for running SGLD on both MNIST and
CIFAR10 datasets, to demonstrate that our bound (see Theorem 11), in particular the sum of the em-
pirical squared gradient norms along the training path, can distinguish normal dataset from dataset
that contains random labels. As shown in Figure 3, the curves of our bounds look quite similar to the
generalization curves. Due to the sub-optimal constants in our bound, the bound is currently greater
than 1, and hence we omit the numbers on the y-axis.
We note that in our experiments presented in Figure 3, the learning rate that we choose is larger than
that required by the second condition of Theorem 11. This is because the global Lipschitz constant
L is hard to estimate and the model is not able to fit training data under a very large noise. As
discussed in Section 3.1, we can relax σt ≥ (20L)γt to σt ≥ (2 maxi∈[n] ∣VF(Wt-1, zi)∣)γt. By
applying gradient clipping trick, we can further relax this condition to
σt ≥ min{CL, (2 max ∣VF (Wt-1, zi)∣)}γt,	(43)
where CL is defined in Section 3.1. In order to show that our observation (“random > normal”) still
holds when the step size satisfies the requirement of our theory, we run an experiment that using
gradient clipping trick with CL = 1. The model is trained on a small subset of MNIST as fitting
the original data set with random labels under such a large Gaussian noise is extremely slow. As
shown in Figure 4, the experimental results remain unchanged when all the conditions of our bound
are met.
These experiments indicate that the sum of squared empirical gradient norms is highly related to the
generalization performance, and we believe by further optimizing the constants in our bound, it is
possible to achieve a generalization bound that is much closer to the real generalization error.
Figure 2: Training MLP with GLD (σt = 0.2γt) on the full MNIST dataset without label corruption.
Learning rate Yt = 0.01 ∙ 0.95bt/60」.Note that in the early stage, the testing accuracy is even higher
than the training accuracy, thus we plot the absolute value of generalization error. As shown in this
figure, even when the training accuracy approaches 90%, our bound is still relatively small.
30
Published as a conference paper at ICLR 2020
Leieiei 6
>UE3uuπ 6u-u 一ro」l
0	1	2	3	4	5
104step
(a)
(b)
PUnOq -rou--dEα)
1.0 1.5 2.0 2.5 3.0
104step
(d)	(e)
1.0 1.5 2.0 2.5 3.0
104step
(g)
(h)
PUnOq -8μ-dEα,
0.0 0.5 1.0 1.5 2.0 2.5 3.0
104step
(f)
PUnOq -rouμ-dEα,
0.0 0.5 1.0 1.5 2.0 2.5 3.0
104step
(i)
Figure 3: SGLD fitting random labels. The meaning of these plots are the same as those in Figure 1.
(a-c): CIFAR10 + MLP; (d-f): MNIST+AlexNet; (g-i): MNIST+MLP; For each data set, only 5000
data points that randomly sampled from the complete dataset are used for training. Mini-batch size
b = 500. Learning rate Yt = max(0.0005,0.003 ∙ 0.995bt/60c). Noise level σt = 0.002√2γt.
(a)
PUnoq-",UEdE9
p=0
p=0∙20
step
(c)
(d)
(b)

Figure 4: Training SGLD with AlexNet on a subset of MNIST (250 data points) with different
random label portion p. The meanings of the y-labels are the same as that in Figure 1. We use
the gradient clipping trick to force the gradient norms of every data points are within CL = 1.
We set σt according to (43) with replacing "≥" with "=”. Mini-batch size b = 10. Learning rate
Yt = max(10-5,0.0005 ∙ 0.9bt/1000c).
31