Published as a conference paper at ICLR 2020
Sampling-Free Learning of
Bayesian Quantized Neural Networks
Jiahao Su
Department of Electrical and Computer Engineering
University of Maryland
College Park, MD 20740
jiahaosu@umd.edu
Milan Cvitkovic
Amazon Web Services
Seattle, WA, USA
cvitkom@amazon.com
Furong Huang
Department of Computer Science
University of Maryland
College Park, MD 20740
furongh@cs.umd.edu
Ab stract
Bayesian learning of model parameters in neural networks is important in sce-
narios where estimates with well-calibrated uncertainty are important. In this pa-
per, we propose Bayesian quantized networks (BQNs), quantized neural networks
(QNNs) for which we learn a posterior distribution over their discrete parameters.
We provide a set of efficient algorithms for learning and prediction in BQNs with-
out the need to sample from their parameters or activations, which not only allows
for differentiable learning in QNNs, but also reduces the variance in gradients.
We evaluate BQNs on MNIST, Fashion-MNIST, KMNIST and CIFAR10 image
classification datasets. compared against bootstrap ensemble of QNNs (E-QNN).
We demonstrate BQNs achieve both lower predictive errors and better-calibrated
uncertainties than E-QNN (with less than 20% of the negative log-likelihood).
1 Introduction
A Bayesian approach to deep learning considers the network’s parameters to be random variables
and seeks to infer their posterior distribution given the training data. Models trained this way, called
Bayesian neural networks (BNNs) (Wang & Yeung, 2016), in principle have well-calibrated uncer-
tainties when they make predictions, which is important in scenarios such as active learning and
reinforcement learning (Gal, 2016). Furthermore, the posterior distribution over the model parame-
ters provides valuable information for evaluation and compression of neural networks.
There are three main challenges in using BNNs: (1) Intractable posterior: Computing and storing
the exact posterior distribution over the network weights is intractable due to the complexity and
high-dimensionality of deep networks. (2) Prediction: Performing a forward pass (a.k.a. as prob-
abilistic propagation) in a BNN to compute a prediction for an input cannot be performed exactly,
since the distribution of hidden activations at each layer is intractable to compute. (3) Learning:
The classic evidence lower bound (ELBO) learning objective for training BNNs is not amenable to
backpropagation as the ELBO is not an explicit function of the output of probabilistic propagation.
These challenges are typically addressed either by making simplifying assumptions about the distri-
butions of the parameters and activations, or by using sampling-based approaches, which are expen-
sive and unreliable (likely to overestimate the uncertainties in predictions). Our goal is to propose a
sampling-free method which uses probabilistic propagation to deterministically learn BNNs.
A seemingly unrelated area of deep learning research is that of quantized neural networks (QNNs),
which offer advantages of computational and memory efficiency compared to continuous-valued
models. QNNs, like BNNs, face challenges in training, though for different reasons: (4.1) The non-
1
Published as a conference paper at ICLR 2020
differentiable activation function is not amenable to backpropagation. (4.2) Gradient updates cease
to be meaningful, since the model parameters in QNNs are coarsely quantized.
In this work, we combine the ideas of BNNs and QNNs in a novel way that addresses the aforemen-
tioned challenges (1)(2)(3)(4) in training both models. We propose Bayesian quantized networks
(BQNs), models that (like QNNs) have quantized parameters and activations over which they learn
(like BNNs) categorical posterior distributions. BQNs have several appealing properties:
•	BQNs solve challenge (1) due to their use of categorical distributions for their model parameters.
•	BQNs can be trained via sampling-free backpropagation and stochastic gradient ascent of a dif-
ferentiable lower bound to ELBO, which addresses challenges (2), (3) and (4) above.
•	BQNs leverage efficient tensor operations for probabilistic propagation, further addressing chal-
lenge (2). We show the equivalence between probabilistic propagation in BQNs and tensor con-
tractions (Kolda & Bader, 2009), and introduce a rank-1 CP tensor decomposition (mean-field
approximation) that speeds up the forward pass in BQNs.
•	BQNs provide a tunable trade-off between computational resource and model complexity: using
a refined quantization allows for more complex distribution at the cost of more computation.
•	Sampling from a learned BQN provides an alternative way to obtain deterministic QNNs .
In our experiments, we demonstrate the expressive power of BQNs. We show that BQNs trained
using our sampling-free method have much better-calibrated uncertainty compared with the state-
of-the-art Bootstrap ensemble of quantized neural networks (E-QNN) trained by Courbariaux et al.
(2016). More impressively, our trained BQNs achieve comparable log-likelihood against Gaussian
Bayesian neural network (BNN) trained with stochastic gradient variational Bayes (SGVB) (Shrid-
har et al., 2019) (the performance of Gaussian BNNs are expected to be better than BQNs since
they allows for continuous random variables). We further verify that BQNs can be easily used to
compress (Bayesian) neural networks and obtain determinstic QNNs. Finally, we evaluate the effect
of mean-field approximation in BQN, by comparing with its Monte-Carlo realizations, where no
approximation is used. We show that our sampling-free probabilistic propagation achieves similar
accuracy and log-likelihood — justifying the use of mean-field approximation in BQNs.
Related Works. In Appendix A, we survey different approaches for training Bayesian neu-
ral networks including sampling-free assumed density filtering (Minka, 2001; Soudry et al.,
2014; Hemandez-Lobato & Adams, 2015; Ghosh et al., 2016), sampling-based variational infer-
ence (Graves, 2011; Blundell et al., 2015; Shridhar et al., 2019), as well as sampling-free varia-
tional inference (Wu et al., 2018), probabilistic neural networks (Wang et al., 2016; Shekhovtsov
& Flach, 2018; Gast & Roth, 2018), quantized neural network (Han et al., 2015; Courbariaux
et al., 2015; Zhu et al., 2016; Kim & Smaragdis, 2016; Zhou et al., 2016; Rastegari et al., 2016;
Hubara et al., 2017; Esser et al., 2015; Peters & Welling, 2018; Shayer et al., 2017), and tensor net-
Works and tensorial neural networks (Grasedyck et al., 2013; Orus, 2014; Cichocki et al., 2016;
2017; Su et al., 2018; Newman et al., 2018; Robeva & Seigal, 2017).
Contributions:
•	We propose an alternative evidence lower bound (ELBO) for Bayesian neural networks such that
optimization of the variational objective is compatible with the backpropagation algorithm.
•	We introduce Bayesian quantized networks (BQNs), establish a duality between BQNs and hierar-
chical tensor networks, and show prediction a BQN is equivalent to a series of tensor contractions.
•	We derive a sampling-free approach for both learning and inference in BQNs using probabilistic
propagation (analytical inference), achieving better-calibrated uncertainty for the learned models.
•	We develop a set of fast algorithms to enable efficient learning and prediction for BQNs.
2	Bayesian Neural Networks
Notation. We use bold letters such as θ to denote random variables, and non-bold letters such as
θ to denote their realizations. We abbreviate Pr[θ = θ] as Pr[θ] and use bold letters in an equation
if the equality holds for arbitrary realizations. For example, Pr[x, y] = Pr[y|x] Pr[x] means
Pr[x = x, y = y] = Pr[y = y|x = x] Pr[x = x], ∀x ∈ X , y ∈ Y.
2
Published as a conference paper at ICLR 2020
2.1	Problem Setting
Given a dataset D = {(xn, yn)}nN=1 of N data points, we aim to learn a neural network with model
parameters θ that predict the output y ∈ Y based on the input x ∈ X . (1) We first solve the
learning problem to find an approximate posterior distribution Q(θ; φ) over θ with parameters φ
such that Q(θ; φ) ≈ Pr[θ∣D]. (2) We then solve the prediction problem to compute the predictive
distribution Pr[y|x, D] for arbitrary input x = x given Q(θ; φ). For notational simplicity, we will
omit the conditioning on D and write Pr[y|x, D] as Pr[y|x] in what follows.
In order to address the prediction and learning problems in BNNs, we analyze these models in their
general form of probabilistic graphical models (shown in Figure 3b in Appendix B). Let h(l), θ(l)
and h(l+1) denote the inputs, model parameters, and (hidden) outputs of the l-th layer respectively.
We assume that θ(l) ’s are layer-wise independent, i.e. Q(θ; φ) = QlL=-01 Q(θ(l) ; φ(l) ), and h(l)
follow the Markovian property, i.e. Pr[h(l+1) |h( : l) , θ( : l)] = Pr[h(l+1) |h(l) , θ(l)].
2.2	The Prediction Problem
Computing the predictive distribution Pr[y|x, D] with a BNN requires marginalizing over the ran-
dom variable θ. The hierarchical structure of BNNs allows this marginalization to be performed in
multiple steps sequentially. In Appendix B, we show that the predictive distribution of h(l+1) given
input x = x can be obtained from its preceding layer h(l) by
1⅛ = LM
Pr[h(l+
X------
p (h(I+1)[ψɑ+1))
Pr[h(l+1)|h(l), θ(l)] Q(θ(l); φ(l)) Pr[h(l)|x] dh(l)dθ(l)
、----V-----}
P (h⑴;ψ(I))
(1)
z
This iterative process to compute the predictive distributions layer-by-layer sequentially is known as
probabilistic propagation (SoUdry et al., 2014; Hernandez-Lobato & Adams, 2015; Ghosh et al.,
2016). With this approach, we need to explicitly compute and store each intermediate result
Pr[h(l) |x] in its parameterized form P(h(l); ψ(l)) (the conditioning on x is hidden in ψ(l), i.e. ψ(l)
is a function of x). Therefore, probabilistic propagation is a deterministic process that computes
ψ(l+1) as a function of ψ(l) and φ(l), which we denote as ψ(l+1) = g(l) (ψ(l), φ(l)).
Challenge in Sampling-Free Probabilistic Propagation. If the hidden variables h(l)’s are con-
tinuous, Equation (1) generally can not be evaluated in closed form as it is difficult to find a family
of parameterized distributions P for h(l) such that h(l+1) remains in P under the operations of a
neural network layer. Therefore most existing methods consider approximations at each layer of
probabilistic propagation. In Section 4, we will show that this issue can be (partly) addressed if we
consider the h(l)’s to be discrete random variables, as in a BQN.
2.3	The Learning Problem
Objective Function. A standard approach to finding a good approximation Q(θ; φ) is variational
inference, which finds φ? such that the KL-divergence KL(Q(θ;φ)∣∣Pr[θ∣D]) from Q(θ;φ) to
Pr[θ∣D] is minimized. In Appendix B, We prove that to minimizing the KL-divergence is equivalent
to maximizing an objective function known as the evidence lower bound (ELBO), denoted as L(φ).
N
max L⑷=-KL(Q(θ; Φ)∣∣Pr[θ∣D]) = X Ln(φ)+ R⑷	,ɔ.
φ	n=1	(2)
where Ln(φ) = EQ [logPr[yn∣xn, θ]] andR(φ) = EQ [log(Pr[θ])] + H(Q)
Probabilistic Backpropagation. Optimization in neural networks heavily relies on the gradient-
based methods, where the partial derivatives ∂L(φ)∕∂φ of the objective L(φ) w.r.t. the parameters φ
are obtained by backpropagation. Formally, if the output produced by a neural network is given by
a (sub-)differentiable function g(φ), and the objective L(g(φ)) is an explicit function of g(φ) (and
not just an explicit function of φ), then the partial derivatives can be computed by chain rule:
dL(g(0))/d。= dL(g(O))/dg⑷∙ ¾<φ"dφ
(3)
3
Published as a conference paper at ICLR 2020
The learning problem can then be (approximately) solved by first-order methods, typically stochastic
gradient descent/ascent. Notice that (1) For classification, the function g(φ) returns the probabilities
after the softmax function, not the categorical label; (2) An additional regularizer R(φ) on the
parameters will not cause difficulty in backpropagation, given ∂R(φ)∕∂φ is easily computed.
Challenge in Sampling-Free Probabilistic Backpropagation. Learning BNNs is not amenable to
standard backpropagation because the ELBO objective function L(φ) in (4b) is not an explicit (i.e.
implicit) function of the predictive distribution g(φ) in (4a):
gn(φ) = EQ [Pr[yn|xn, θ]] =	Pr[yn|xn, θ]Q(θ; φ)dθ
θ
Ln(φ) = EQ [log(Pr[yn|xn, θ])] =	log (Pr[yn|xn, θ]) Q(θ; φ)dθ
θ
(4a)
(4b)
Although Ln(φ) is a function of φ, it is not an explicit function of gn(φ). Consequently, the chain
rule in Equation (3) on which backpropagation is based is not directly applicable.
3	Proposed Learning Method for Bayesian Neural Networks
Alternative Evidence LowerBound. We make Iearningjn BNNS amenable to backpropagation
by developing a lower bound Ln(φ) ≤ Ln(φ) such that ∂Ln(φ)∕∂φ can be obtained by chain rule
(i.e. Ln(φ) is an explicit function of the results from theforward pass.) With Ln(φ) in hand, We can
(approximately) find φ? by maximizing the alternative objective via gradient-based method:
_	N	N _	∖
φ? = arg max L(φ) = arg max R(φ) +	Ln(φ)	(5)
φ	φ	n=1
In Appendix C.1, We proved one feasible Ln(φ) which only depends on second last output h(L-1).
Theorem 3.1 (Alternative Evidence Lower Bound). Define each term Ln(φ) in L(φ) as
Ln(φ) := Eh(L-I)〜P； Θ(L-1)〜Q [log (Pr[yn∣h(Lτ),。5-1)川	(6)
then Ln(φ) is a lower bound of Ln(φ), i.e. Ln(φ) ≤ Ln(φ). The equality Ln(φ) = Ln(φ) holds if
h(L-1) is deterministic given input x and all parameters before the last layer θ(: L-2).
Analytic Forms of Ln(φ). While the lower bound in Theorem 3.1 applies to BNNs with arbitrary
distributions P on hidden variables h, Q on model parameters θ, and any problem setting (e.g^clas-
sification or regression), in practice sampling-free probabilistic backpropagation requires that Ln (φ)
can be analytically evaluated (or further lower bounded) in terms of φ(L-1) and θ(L-1). This task
is nontrivial since it requires redesign of the output layer, i.e. the function of Pr[y|h(L-1), θ(L-1)].
In this paper, we develop two layers for classification and regression tasks, and present the classifi-
cation case in this section due to space limit. Since Ln(φ) involves the last layer only, we omit the
superscripts/subsripts of h(L-1), ψ(L-1), φ(L-1), xn, yn, and denote them as h, ψ, φ, x, y .
Theorem 3.2 (Analytic Form of Ln(φ) for Classification). Let h ∈ RK (with K the number of
classes) be the pre-activations ofa softmax layer (a.k.a. logits), and φ = s ∈ R+ be a scaling factor
that adjusts its scale such that Pr[y = c|h, s] = exp(hc/s)/PkK=1 exp(hk /s). Suppose the logits
{hk}kK=1 are pairwise independent (which holds under mean-field approximation) and hk follows
a Gaussian distribution hk 〜N(μk,νk) (therefore ψ = {μk,νk}K=ι) and S is a deterministic
parameter. Then Ln(φ) isfurther lower bounded as Ln(φ) ≥ μc 一 log (PK=I exp (μSk + 卷)).
The regression case and proofs for both layers are deferred to Appendix C.
4	Bayesian Quantized Networks (BQNs)
While Section 3 provides a general solution to learning in BNNs, the solution relies on the ability
to perform probabilistic propagation efficiently. To address this, we introduce Bayesian quantized
4
Published as a conference paper at ICLR 2020
networks (BQNs) — BNNs where both hidden units h(l)'s and model parameters θ(l)'s take discrete
values — along with a set of novel algorithms for efficient sampling-free probabilistic propagation
in BQNs. For simplicity of exposition, we assume activations and model parameters take values
from the same set Q, and denote the degree of quantization as D = |Q|, (e.g. Q = {-1, 1}, D = 2).
4.1	Probabilistic Propagation as Tensor Contractions
Lemma 4.1 (Probabilistic Propagation in BQNs). After quantization, the iterative step of proba-
bilistic propagation in Equation (1) is computed with a finite sum instead of an integral:
P (h(l+1); ψ(l+1)) = Xh ,θ Pr[h(l+1)|h(l), θ(l)] Q(θ(l); φ(l)) P (h(l); ψ(l))	(7)
and a categorically distributed h(l) results in h(l+1) being categorical as well. The equation holds
without any assumption on the operation Pr[h(l+1) |h(l), θ(l)] performed in the neural network.
Notice all distributions in Equation (7) are represented in high-order tensors: Suppose there are I
input units, J output units, and K model parameters at the l-th layer, then h(l) ∈ QI, θ(l) ∈ QK,
and h(l+1) ∈ QJ, and their distributions are characterized by P (h(l); ψ(l)) ∈ RDI, Q(θ(l); φ(l))
∈ RDK, P (h(l+1); ψ(l+1)) ∈ RDJ, and Pr[h(l+1) |h(l), θ(l)] ∈ RDJ ×DI ×DK respectively. There-
fore, each step in probabilistic propagation is a tensor contraction of three tensors, which establishes
the duality between BQNs and hierarchical tensor networks (Robeva & Seigal, 2017).
Since tensor contractions are differentiable w.r.t. all inputs, BQNs thus circumvent the difficulties in
training QNNs (Courbariaux et al., 2015; Rastegari et al., 2016), whose outputs are not differentiable
w.r.t. the discrete parameters. This result is not surprising: if we consider learning in QNNs as an
integer programming (IP) problem, solving its Bayesian counterpart is equivalent to the approach to
relaxing the problem into a continuous optimization problem (Williamson & Shmoys, 2011).
Complexity of Exact Propagation. The computational complexity to evaluate Equation (7) is
exponential in the number of random variables O(DIJK), which is intractable for quantized neural
network of any reasonable size. We thus turn to approximations.
4.2	Approximate Propagation via Rank- 1 Tensor CP Decomposition
We propose a principled approximation to reduce the computational complexity in probabilistic
propagation in BQNs using tensor CP decomposition, which factors an intractable high-order prob-
ability tensor into tractable lower-order factors (Grasedyck et al., 2013). In this paper, we consider
the simplest rank-1 tensor CP decomposition, where the joint distributions of P and Q are fully
factorized into products of their marginal distributions, thus equivalent to the mean-field approxima-
tion (Wainwright et al., 2008). With rank-1 CP decomposition on P (h(l); ψ(l)), ∀l ∈ [L], the tensor
contraction in (7) reduces to a standard Tucker contraction (Kolda & Bader, 2009)
P(hjl+1); ψjl+1)) ≈ Xh(i),θ(i) Pr[hjl+1)∣θ(l),h(l)] Yk Q(θkl); φkl)) Yi P(h(l); ψ(l))	(8)
where each term of ψi(l), φ(kl) parameterizes a single categorical variable. In our implementation, we
store the parameters in their log-space, i.e. Q(θk(l) = Q(d)) = exp(ψk(l) (d))/PqD=1 exp(φ(kl) (q)).
Fan-in Number E. In a practical model, for the l-th layer, an output unit h(jl+1) only (condition-
ally) depends on a subset of all input units {hi(l)} and model parameters {θk(h)} according to the con-
nectivity pattern in the layer. We denote the set of dependent input units and parameters for h(jl+1)
as Ij(l+1) and M(jl+1), and define the fan-in number E for the layer as maxj Ij(l+1) + M(jl+1) .
Complexity of Approximate Propagation. The approximate propagation reduces the computa-
tional complexity from O(DIJK) to O(JDE), which is linear in the number of output units J if we
assume the fan-in number E to be a constant (i.e. E is not proportional to I).
5
Published as a conference paper at ICLR 2020
4.3	Fast Algorithms for Approximate Propagation
Different types of network layers have different fan-in numbers E, and for those layers with E
greater than a small constant, Equation (8) is inefficient since the complexity grows exponential in
E. Therefore in this part, we devise fast(er) algorithms to further lower the complexity.
Small Fan-in Layers: Direct Tensor Contraction. If E is small, we implement the approximate
propagation through tensor contraction in Equation (8). The computational complexity is O(JDE)
as discussed previously. See Appendix D.1 for a detailed discussion.
Medium Fan-in Layers: Discrete Fourier Transform. If E is medium, we implement approx-
imate propagation through fast Fourier transform since summation of discrete random variables is
equivalent to convolution between their probability mass function. See Appendix D.2 for details.
With the fast Fourier transform, the computational complexity is reduced to O(JE2D log(ED)).
Large Fan-in Layers: Lyapunov Central Limit Theorem. In a typical linear layer, the fan-in E is
large, and a super-quadratic algorithm using fast Fourier transform is still computational expensive.
Therefore, we derive a faster algorithm based on the Lyapunov central limit theorem (See App D.3)
With CLT, the computational complexity is further reduced to O(J ED).
Remarks: Depending on the fan-in numbers E, we adopt CLT for linear layers with sufficiently large
E such as fully connected layers and convolutional layers; DFT for those with medium E such as
average pooling layers and depth-wise layers; and direct tensor contraction for those with small E
such as shortcut layers and nonlinear layers.
5	Experiments
In this section, we demonstrate the effectiveness of BQNs on the MNIST, Fashion-MNIST, KM-
NIST and CIFAR10 classification datasets. We evaluate our BQNs with both multi-layer perceptron
(MLP) and convolutional neural network (CNN) models. In training, each image is augmented by
a random shift within 2 pixels (with an additional random flipping for CIFAR10), and no augmen-
tation is used in test. In the experiments, we consider a class of quantized neural networks, with
both binary weights and activations (i.e. Q = {-1,1}) with sign activations σ(∙) = sign(∙). For
BQNs, the distribution parameters φ are initialized by Xavier’s uniform initializer, and all models are
trained by ADAM optimizer (Kingma & Ba, 2014) for 100 epochs (and 300 epochs for CIFAR10)
with batch size 100 and initial learning rate 10-2, which decays by 0.98 per epoch.
Methods
MNIST
E-QNN on MLP
BQN on MLP
E-QNN on CNN
BQN on CNN
NLL (10-3)	% Err.
546.6±157.9 3.30 ±0.65
130.0±3.5	2.49±0.08
KMNIST
Fashion-MNIST
NLL(10-3)	%Err. NLL(10-3) %Err.
2385.6±432.3 17.88±1.86 2529.4±276.7 13.02±0.81
457.7±13.8	13.41±0.12	417.3 ±8.1	9.99±0.20
CIFAR10
425.3±61.8 0.85±0.13
41.8±1.6	0.85±0.06
3755.7±465.1 11.49±1.16 1610.7±158.4 3.02±0∙37
295.5±1.4	9.95±0.15	209.5±2.8	4.65±0.15
NLL (10-3)	% Err.
N/A	N/A
N/A	N/A
7989.7 ± 600.2 15.92 ± 0.72
530.6 ± 23.0	13.74 ±0.47
Table 1: Comparison of performance of BQNs against the baseline E-QNN. Each E-QNN is an
ensemble of 10 networks, which are trained individually and but make predictions jointly. We report
both NLL (which accounts for prediction uncertainty) and 0-1 test error (which doesn’t account for
prediction uncertainty). All the numbers are averages over 10 runs with different seeds, the standard
deviation are exhibited following the ± sign.
Training Objective of BQNs. To allow for customized level of uncertainty in the learned Bayesian
models, we introduce a regularization coefficient λ in the alternative ELBO proposed in Equation (5)
(i.e. a lower bound of the likelihood), and train the BQNs by maximizing the following objective:
NN
L(φ) = X Ln(φ) + λR(φ) = λ 1∕λ X Ln(φ) + R(φ)	(9)
n=1	n=1
where λ controls the uncertainty level, i.e. the importance weight of the prior over the training set.
Baselines. (1) We compare our BQN against the baseline - Bootstrap ensemble ofquantized neural
networks (E-QNN). Each member in the ensemble is trained in a non-Bayesian way (Courbariaux
et al., 2016), and jointly make the prediction by averaging over the logits from all members. Note
6
Published as a conference paper at ICLR 2020
10 102 -
BNN
E-QNN
BQN
-4	10-3
λ level of model uncertainty
104
—BNN
—E-QNN
—BQN
103
103
—BNN
—E-QNN
—BQN
102.5
1----空-------停----一妾 .
103.5
103
-*---------*-----------*-------共--------*
10-4	10-3
λ level of model uncertainty
BNN
E-QNN
BQN
10-9	10-8	10-7
λ level of model uncertainty
(a) NLL MNIST
.9 .8
0. 0.
rorrE egatnecreP
10-4	10-3
λ level of model uncertainty
rorrE egatnecreP
(b) NLL FMNIST
BNN
E-QNN
BQN
10-4	10-3
λ level of model uncertainty
(c) NLL KMNIST
10-4	10-3
λ level of model uncertainty
rorrE egatnecre
rorrE egatnecre
(d) NLL CIFAR10
(e) Error MNIST	(f) Error FMNIST (g) Error KMNIST (h) Error CIFAR10
Figure 1: Comparison of the predictive performance of our BQNs against the E-QNN as well as the
non-quantized BNN trained by SGVB on a CNN. Negative log-likelihood (NLL) which accounts
for uncertainty and 0-1 test error which doesn’t account for uncertainty are displayed.
that Courbariaux et al. (2016) is chosen over other QNN training methods as the baseline since it
trains QNN from random initialization, thus a fair comparison to our approach. Details are discussed
in Appendix A. (2) To exhibit the effectiveness of our BQN, we further compare against continuous-
valued Bayesian neural network (abbreviated as BNN) with Gaussian parameters. The model is
trained with stochastic gradient variational Bayes (SGVB) augmented by local re-parameterization
trick (Shridhar et al., 2019). Since the BNN allows for continuous parameters (different from BQN
with quantized parameters), the predictive error is expected to be lower than BQN.
Evaluation of BQNs. While 0-1 test error is a popular metric to measure the predictive perfor-
mance, it is too coarse a metric to assess the uncertainty in decision making (for example it does
not account for how badly the wrong predictions are). Therefore, we will mainly use the negative
log-likelihood (NLL) to measure the predictive performance in the experiments.
Once a BQN is trained (i.e. an approximate posterior Q(θ) is learned), we consider three modes to
evaluate the behavior of the model: (1) analytic inference (AI), (2) Monte Carlo (MC) sampling and
(3) Maximum a Posterior (MAP) estimation:
1.	In analytic inference (AI, i.e. our proposed method), we analytically integrate over Q(θ) to obtain
the predictive distribution as in the training phase. Notice that the exact NLL is not accessible
with probabilistic propagation (which is why we propose an alternative ELBO in Equation (5)),
we will report an upper bound of the NLL in this mode.
2.	In MC sampling, S sets of model parameters are drawn independently from the posterior pos-
terior。$ 〜 Q(θ), ∀s ∈ [S], and the forward propagation is performed as in (non-Bayesian)
quantized neural network for each set θs, followed by an average over the model outputs. The
difference between analytic inference and MC sampling will be used to evaluate (a) the effect of
mean-field approximation and (b) the tightness of the our proposed alternative ELBO.
3.	MAP estimation is similar to MC sampling, except that only one set of model parameters θ?
is obtained θ? = arg maxθ Q(θ). We will exhibit our model’s ability to compress a Bayesian
neural network by comparing MAP estimation of our BQN with non-Bayesian QNN.
5.1 Analysis of Results
Expressive Power and Uncertainty Calibration in BQNs. We report the performance via all
evaluations of our BQN models against the Ensemble-QNN in Table 1 and Figure 1. (1) Com-
pared to E-QNNs, our BQNs have significantly lower NLL and smaller predictive error (except for
7
Published as a conference paper at ICLR 2020
60
50
40
30
20
10-6	10-5	10-4	10-3
λ level of model uncertainty
(a) NLL MNIST
rorrE egatnecreP
10-6	10-5	10-4	10-3
λ level of model uncertainty
8
6 -
Monte Carlo Sampling
Analytical Inference
Difference
LLN
200
150
100
Monte Carlo Sampling
Analytical Inference
Difference
10-4	10-3
λ level of model uncertainty
(c) NLL KMNIST
4
----Monte Carlo Sampling
3	Analytical Inference
----Difference
rorrE egatnecre
rorrE egatnecreP
×—
0
10-4
10-3
λ level of model uncertainty
0
10-4
10-3
λ level of model uncertainty
4
2
(d) Error MNIST	(e) Error FMNIST	(f) Error KMNIST
Figure 2: Illustration of mean-field approximation and tightness of alternative ELBO on a CNN.
The performance gap between our analytical inference and the Monte Carlo Sampling is displayed.
Fashion-MNIST with architecture CNN). (2) As we can observe in Figure 1, BQNs impressively
achieve comparable NLL to continuous-valued BNN, with slightly higher test error. As our model
parameters only take values {-1, 1}, small degradation in predictive accuracy is expected.
Evaluations of Mean-field Approximation and Tightness of the Alternative ELBO. If analytic
inference (by probabilistic propagation) were computed exactly, the evaluation metrics would have
been equal to the ones with MC sampling (with infinite samples). Therefore we can evaluate the
approximations in probabilistic propagation, namely mean-field approximation in Equation (8) and
relaxation of the original ELBO in Equation (5), by measuring the gap between analytic inference
and MC sampling. As shown in Figure 2, such gaps are small for all scenarios, which justifies the
approximations we use in BQNs.
To further decouple these two factors of mean-field approximation and relaxation of the original
ELBO, we vary the regularization coefficient λ in the learning objective. (1) For λ = 0 (where
the prior term is removed), the models are forced to become deterministic during training. Since the
deterministic models do not have mean-field approximation in the forward pass, the gap between an-
alytic inference and MC-sampling reflects the tightness of our alternative ELBO. (2) As λ increases,
the gaps increases slightly as well, which shows that the mean-field approximation becomes slightly
less accurate with higher learned uncertainty in the model.
Methods	MNIST		KMNIST		Fashion-MNIST	
	NLL(10-3)	%Err.	NLL(10-3)	% Err.	NLL(10-3)	% Err.
QNN on MLP	522.4±42.2	4.14±0.25	2019.1±281.2	19.56±1.97	2427.1±193.5	15.67±1.19
MAP of BQN on MLP	137.60±4.40	3.69±0.09	464.60± 12.80	14.79±0.21	461.30± 13.40	12.89±0.17
QNN on CNN	497.4±139.5	^^1.08±0.2	4734.5±1697.2	14.2±2.29	1878.3±223.8	3.88±0.33
MAP of BQN on CNN	30.3±1.6	0.92±0.07	293.6±4.4	10.82±0.37	179.1±4.4	5.00±0.11
Table 2: Deterministic model compression through direct training of QNN (Courbariaux et al.,
2016) v.s. MAP estimation in our proposed BQN. All the numbers are averages over 10 runs with
different seeds, the standard deviation are exhibited following the ± sign.
Compression of Neural Networks via BQNs. One advantage of BQNs over continuous-valued
BNNs is that deterministic QNNs can be obtained for free, since a BQN can be interpreted as an
ensemble of infinite QNNs (each of which is a realization of posterior distribution). (1) One simple
approach is to set the model parameters to their MAP estimates, which compresses a given BQN to
1/64 of its original size (and has the same number of bits as a single QNN). (2) MC sampling can be
8
Published as a conference paper at ICLR 2020
Methods	MNIST		KMNIST		Fashion-MNIST	
	NLL(10-3)	% Err.	NLL(10-3)	% Err.	NLL(10-3)	% Err.
E-QNN on MLP	546.60±157.90^^	3.30 ±0.65	2385.60±432.30~~	17.88±1.86	2529.40±276.70	13.02±0.81
MC of BQN on MLP	108.9±2.6	2.73±0.09	429.50± 11.60	13.83±0.12	385.30±5.10	10.81±0.44
E-QNN on CNN	-425.3±61.80	0.85±0.13	3755.70±465.10~~	11.49±1.16	1610.70±158.40	3.02±0.37
MC of BQN on CNN	29.2±0.6	0.87±0.04	286.3±2.7	10.56±0.14	174.5±3.6	4.82±0.13
Table 3: Bayesian Model compression through direct training of Ensemble-QNN vs a Monte-Carlo
sampling on our proposed BQN. Each ensemble consists of 5 quantized neural networks, and for
fair comparison we use 5 samples for Monte-Carlo evaluation. All the numbers are averages over
10 runs with different seeds, the standard deviation are exhibited following the ± sign.
interpreted as another approach to compress a BQN, which reduces the original size to its S/64 (with
the same number of bits as an ensemble of S QNNs). In Tables 2 and 3, we compare the models
by both approaches to their counterparts (a single QNN for MAP, and E-QNN for MC sampling)
trained from scratch as in Courbariaux et al. (2016). For both approaches, our compressed models
outperform their counterparts (in NLL) . We attribute this to two factors: (a) QNNs are not trained
in a Bayesian way, therefore the uncertainty is not well calibrated; and (b) Non-differentiable QNNs
are unstable to train. Our compression approaches via BQNs simultaneously solve both problems.
6 Conclusion
We present a sampling-free, backpropagation-compatible, variational-inference-based approach for
learning Bayesian quantized neural networks (BQNs). We develop a suite of algorithms for efficient
inference in BQNs such that our approach scales to large problems. We evaluate our BQNs by
Monte-Carlo sampling, which proves that our approach is able to learn a proper posterior distribution
on QNNs. Furthermore, we show that our approach can also be used to learn (ensemble) QNNs by
taking maximum a posterior (or sampling from) the posterior distribution.
References
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Andrzej Cichocki, Namgil Lee, Ivan V Oseledets, Anh Huy Phan, Qibin Zhao, and D Mandic.
Low-rank tensor networks for dimensionality reduction and large-scale optimization problems:
Perspectives and challenges part 1. arXiv preprint arXiv:1609.00893, 2016.
Andrzej Cichocki, Anh-Huy Phan, Qibin Zhao, Namgil Lee, Ivan Oseledets, Masashi Sugiyama,
Danilo P Mandic, et al. Tensor networks for dimensionality reduction and large-scale optimiza-
tion: Part 2 applications and future perspectives. Foundations and TrendsR in Machine Learning,
9(6):431-673, 2017.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems, pp. 3123-3131, 2015.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.
Steve K Esser, Rathinakumar Appuswamy, Paul Merolla, John V Arthur, and Dharmendra S Modha.
Backpropagation for energy-efficient neuromorphic computing. In Advances in Neural Informa-
tion Processing Systems, pp. 1117-1125, 2015.
Yarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.
Jochen Gast and Stefan Roth. Lightweight probabilistic deep networks. In Proceedings of the IEEE
Conference on Computer Vision and Patter Recognition, pp. 3369-3378, 2018.
9
Published as a conference paper at ICLR 2020
Soumya Ghosh, Francesco Maria Delle Fave, and Jonathan S Yedidia. Assumed density filtering
methods for learning bayesian neural networks. In AAAI,, pp. 1589-1595, 2016.
Lars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor
approximation techniques. GAMM-Mitteilungen, 36(1):53-78, 2013.
Alex Graves. Practical variational inference for neural networks. In Advances in neural information
processing systems, pp. 2348-2356, 2011.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Jose MigUel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of bayesian neural networks. In International Conference on Machine Learning, pp. 1861-
1869, 2015.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. Journal
of Machine Learning Research, 18:187-1, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Mohammad Khan. Variational learning for latent Gaussian model of discrete data. PhD thesis,
University of British Columbia, 2012.
Minje Kim and Paris Smaragdis. Bitwise neural networks. arXiv preprint arXiv:1601.06071, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Thomas Peter Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,
Massachusetts Institute of Technology, 2001.
Elizabeth Newman, Lior Horesh, Haim Avron, and Misha Kilmer. Stable tensor neural networks for
rapid deep learning, 2018.
Roman OrUs. A practical introduction to tensor networks: Matrix product states and projected
entangled pair states. Annals of Physics, 349:117-158, 2014.
Jorn WT Peters and Max Welling. Probabilistic binary neural networks. arXiv preprint
arXiv:1809.03368, 2018.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European Conference on Computer
Vision, pp. 525-542. Springer, 2016.
Elina Robeva and Anna Seigal. Duality of graphical models and tensor networks. Information and
Inference: A Journal of the IMA, 2017.
Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameteri-
zation trick. arXiv preprint arXiv:1710.07739, 2017.
Alexander Shekhovtsov and Boris Flach. Feed-forward propagation in probabilistic neural networks
with categorical and max layers. 2018.
Kumar Shridhar, Felix Laumann, and Marcus Liwicki. A comprehensive guide to bayesian convo-
lutional neural network with variational inference. arXiv preprint arXiv:1901.02731, 2019.
10
Published as a conference paper at ICLR 2020
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free train-
ing of multilayer neural networks with continuous or discrete weights. In Advances in Neural
Information Processing Systems,pp. 963-971, 2014.
Jiahao Su, Jingling Li, Bobby Bhattacharjee, and Furong Huang. Tensorized spectrum preserving
compression for neural networks. arXiv preprint arXiv:1805.10352, 2018.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and TrendsR in Machine Learning, 1(1-2):1-305, 2008.
Hao Wang and Dit-Yan Yeung. Towards bayesian deep learning: A survey. arXiv preprint
arXiv:1604.01662, 2016.
Hao Wang, SHI Xingjian, and Dit-Yan Yeung. Natural-parameter networks: A class of probabilistic
neural networks. In Advances in Neural Information Processing Systems, pp. 118-126, 2016.
David P Williamson and David B Shmoys. The design of approximation algorithms. Cambridge
university press, 2011.
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E Turner, Jose MigUel Hernandez-Lobato,
and Alexander L Gaunt. Fixing variational bayes: Deterministic variational inference for bayesian
neural networks. arXiv preprint arXiv:1810.03958, 2018.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
11
Published as a conference paper at ICLR 2020
Appendix: Sampling-Free Learning of
Bayesian Quantized Neural Networks
A Related Work
Probabilistic Neural Networks and Bayesian Neural Networks These models consider weights
to be random variables and aim to learn their distributions. To further distinguish two families
of such models, we call a model Bayesian neural network if the distributions are learned using a
prior-posterior framework (i.e. via Bayesian inference) (SoUdry et al., 2014; Hernandez-Lobato &
Adams, 2015; Ghosh et al., 2016; Graves, 2011; Blundell et al., 2015; Shridhar et al., 2019), and
otherwise probabilistic neural network (Wang et al., 2016; Shekhovtsov & Flach, 2018; Gast &
Roth, 2018). In particUlar, oUr work is closely related to natural-parameters networks (NPN) (Wang
et al., 2016), which consider both weights and activations to be random variables from exponential
family. Since categorical distribUtion (over qUantized valUes) belongs to exponential family, oUr
BQN can be interpreted as categorical NPN, bUt we learn the distribUtions via Bayesian inference.
For Bayesian neUral networks, varioUs types of approaches have been proposed to learn the posterior
distribUtion over model parameters.
(1)	Sampling-free Assumed Density Filtering (ADF), inclUding EBP (SoUdry et al., 2014) and
PBP (Hernandez-Lobato & Adams, 2015), is an online algorithm which (approximately) updates
the posterior distribUtion by Bayes’ rUle for each observation. If the model parameters θ are GaUs-
sian distributed, Minka (2001) shows that the Bayes’ rule can be computed in analytic form based
on ∂log(gn(φ))∕∂φ, and EBP Soudry et al. (2014) derives a similar rule for Bernoulli parameters
in binary classification. Notice that ADF is compatible to backpropagation:
∂log(gn(φ))	1	∂gn(φ)
∑	—	: _ ∙	∑
∂φ	gn(φ) ∂φ
(10)
assuming gn (φ) can be (approximately) computed by sampling-free probabilistic propagation as
in Section 2. However, this approach has two major limitations: (a) the Bayes’ rule needed to be
derived case by case, and analytic rule for most common cases are not known yet. (b) it is not
compatible to modern optimization methods (such as SGD or ADAM) as the optimization is solved
analytically for each data point, therefore difficult to cope with large-scale models.
(2)	Sampling-based Variational inference (SVI), formulates an optimization problem and solves
it approximately via stochastic gradient descent (SGD). The most popular method among all is,
Stochastic Gradient Variational Bayes (SGVB), which approximates Ln(φ) by the average of multi-
ple samples (Graves, 2011; Blundell et al., 2015; Shridhar et al., 2019). Before each step of learning
or prediction, a number of independent samples of the model parameters {θs}sS=1 are drawn accord-
ing to the current estimate of Q, i.e. θs 〜Q, by which the predictive function gn(φ) and the loss
Ln(φ) can be approximated by
1S	1S
gn(φ) ≈ SX Pr[yn∣Xn,θs] = S X fn(θs)	(11a)
s=1	s=1
SS
Ln(Φ) ≈ S X lθg(Pr[yn∣Xn,θs]) — S X lθg(fn(θs))	(11b)
s=1	s=1
where fn(θ) — Pr[yn|xn, θ] denotes the predictive function given a specific realization θ of the
model parameters. The gradients of Ln(φ) can now be approximated as
∂Ln(φ) ≈ 1 X ∂Ln(φ) ∂fn(θs)空
∂φ ≈ S M ∂fn (θs) ∙ ∂θs ^ ∂φ
(12)
This approach has multiple drawbacks: (a) Repeated sampling suffers from high variance, besides
being computationally expensive in both learning and prediction phases; (b) While gn (φ) is differ-
entiable w.r.t. φ, fn (θ) may not be differentiable w.r.t. θ. One such example is quantized neural
networks, whose backpropagation is approximated by straight through estimator (Bengio et al.,
12
Published as a conference paper at ICLR 2020
2013). (3) The partial derivatives ∂θs∕∂φ are difficult to compute with complicated reparameteri-
zation tricks (Maddison et al., 2016; Jang et al., 2016).
(3) Deterministic Variational inference (DVI) Our approach is most similar to Wu et al. (2018),
which observes that if the underlying model is deterministic, i.e. Pr[h(l+1) |h(l), θ(l)] is a dirac
function
Ln(O)= Eh(L-I)〜P； θ(L-1)〜Qhlog Pr[yn|h(L-1), θ(L-1)]i	(13)
Our approach considers a wider scope of problem settings, where the model could be stochastic, i.e.
Pr[h(l+1) |h(l), θ(l)] is an arbitrary function. Furthermore, Wu et al. (2018) considers the case that
all parameters θ are Gaussian distributed, whose sampling-free probabilistic propagation requires
complicated approximation (Shekhovtsov & Flach, 2018).
Quantized Neural Networks These models can be categorized into two classes: (1) Partially
quantized networks, where only weights are discretized (Han et al., 2015; Zhu et al., 2016); (2)
Fully quantized networks, where both weights and hidden units are quantized (Courbariaux et al.,
2015; Kim & Smaragdis, 2016; Zhou et al., 2016; Rastegari et al., 2016; Hubara et al., 2017). While
both classes provide compact size, low-precision neural network models, fully quantized networks
further enjoy fast computation provided by specialized bit-wise operations. In general, quantized
neural networks are difficult to train due to their non-differentiability. Gradient descent by back-
propagation is approximated by either straight-through estimators (Bengio et al., 2013) or prob-
abilistic methods (Esser et al., 2015; Shayer et al., 2017; Peters & Welling, 2018). Unlike these
papers, we focus on Bayesian learning of fully quantized networks in this paper. Optimization of
quantized neural networks typically requires dedicated loss function, learning scheduling and initial-
ization. For example, Peters & Welling (2018) considers pre-training of a continuous-valued neural
network as the initialization. Since our approach considers learning from scratch (with an uniform
initialization), the performance could be inferior to prior works in terms of absolute accuracy.
Tensor Networks and Tensorial Neural Networks Tensor networks (TNs) are widely used in
numerical analysis (GrasedyCk et al., 2013), quantum PhysisCs (OrUs, 2014), and recently machine
learning (Cichocki et al., 2016; 2017) to model interactions among multi-dimensional random ob-
jects. Various tensorial neural networks (TNNs) (Su et al., 2018; Newman et al., 2018) have been
proposed that reduce the size of neural networks by replacing the linear layers with TNs. Recently,
(Robeva & Seigal, 2017) points out the duality between probabilistic graphical models (PGMs) and
TNs. I.e. there exists a bijection between PGMs and TNs. Our paper advances this line of thinking
by connecting hierarchical Bayesian models (e.g. Bayesian neural networks) and hierarchical TNs.
B Supervised Learning with Bayesian neural networks (BNNs)
The problem settings of general Bayesian model and Bayesian neural networks for supervised learn-
ing are illustrated in Figures 3a and 3b using graphical models.
(b) Graphical model depiction of a Bayesian neural network
(a) Graphical model depiction of the problem as a hierarchical model, where predicting y from x can be
setting in Bayesian neural networks.	performed iteratively through the hidden variables h(l) ’s.
Figure 3: Graphical models.
General Bayesian model Formally, the graphical model in Figure 3a implies the joint distribution
of the model parameters θ, the observed dataset D = {(xn, yn)}nN=1 and any unseen data point
13
Published as a conference paper at ICLR 2020
(x, y) is factorized as follows:
Pr[x, y, D, θ] = (Pr[y∣x, θ]Pr[x] (Pr[D∣θ])) Pr[θ]
=(Pr[y|x, θ]Pr[x] (口 Pr[yn∣Xn, θ]Pr[xn])) Pr[θ]
(14)
(15)
where Pr[xi]’s and Pr[x] are identically distributed, and so are the conditional distributions
Pr[yi∣Xi, θ]'s and Pr[y∣x, θ]. In other words, We assume that (1) the samples (χn,yn)s (and
unseen data point (x, y)) are are identical and independent distributed according to the same data
distribution; and (2) xn (or x) and θ together predict the output yn (or y) according to the same
conditional distribution. Notice that the factorization above also implies the following equations:
Pr[y|x, D, θ] = Pr[y|x, θ]
Pr[θ∣x, D] = Pr[θ∣D]
(16a)
(16b)
With these implications, the posterior predictive distribution Pr[y|x, D] can now expanded as:
Pr[y|x, D]
Pr[y∣x,θ, D]Pr[θ∣x,
D]dθ =
θ
Pr[y∣x, θ] Pr[θ∣D] dθ
、~'{z}
≈Q(θ; φ)
(17)
θ
where we approximate the posterior distribution Pr[θ∣D] by a parameterized distribution Q(θ; φ).
Variational Learning The reason we are learning an approximate posterior Q and not the exact
distribution Pr[θ∣D] is that for complex models the latter is intractable to compute. The exact
posterior Pr[θ∣D] generally does not take the form of Q(θ; φ) even if its prior Pr[θ] does.
A standard approach to finding a good approximation Q(θ; φ) is variational inference, which finds
φ? such that the KL-divergence KL(Q(θ; φ)∣∣Pr[θ∣D]) of Q(θ; φ) from Pr[θ∣D] is minimized (or
alternatively the negative KL-divergence is maximized.)
φ? = arg max (-KL(Q(θ; φ)∣∣Pr[θ∣D]))	(18)
φ
=argmax f- Q Q(θ; Φ) log (Q(^φ)[ dθ)	(19)
φ θ	Pr[θ∣D]
where Pr[θ∣D] is obtained via standard Bayes' rule, i.e. Pr[θ∣D] = Pr[D∣θ]Pr[θ]∕Pr[D]. Now
we are able to decompose the maximization objective into two terms by plugging the rule into (19):
L⑷=— θθ 软仇 φ) log (qG φ) ∙ Pr[pPDD∣θ]) dθ	QO)
=X [ log(Pr[yn∣Xn,θ]) Q(θ; φ)dθ + Q Q(θ; φ) log (Q(θ;φ) ) dθ + const. (21)
n=1 θ	θ	Pr[θ]
N
= XEQ [log (Pr[yn∣xn, θ])] + KL(Q(θ; φ)∣∣Pr[θ]) - log (Pr[D])	(22)
n=1、「/	，{z	{z
n=1	Ln (φ)	R(φ)	const.
where (1) Ln (φ) is the expected log-likelihood, which reflects the predictive performance of the
Bayesian model on the data point (xn, yn); and (2) R(φ) is the KL-divergence between Q(θ; φ)
and its prior Pr[θ], which reduces to entropy H(Q) if the prior of θ follows a uniform distribution.
Hierarchical Bayesian Model A Bayesian neural network can be considered as a hierarchical
Bayesian model depicted in Figure 3b, which further satisfies the following two assumptions:
Assumption B.1 (Independence of Model Parameters θ(l)). The approximate posterior Q(θ; φ)
over the model parameters θ are partitioned into L disjoint and statistically independent layers
{θ(l)}lL=-01 (where each φ(l) parameterizes θ(l) in the l-th layer) such that:
L-1
Q(θ; φ)= Y Q(θ(l); φ(l))	(23)
l=0
14
Published as a conference paper at ICLR 2020
Assumption B.2 (Markovianity of Hidden Units h(l)). The hidden variables h = {h(l) }lL=0 sat-
isfy the Markov property that h(l+1) depends on the input x only through its previous layer h(l):
Pr[h(l+1)∣h(:l), θ(:l)] = Pr[h(l+1)∣h(l), θ(l)]	(24)
where we use short-hand notations h(: l) and θ(: l) to represent the sets of previous layers {h(k)}lk=0
and {θ(k)}lk=0. For consistency, we denote h(0) = x and h(L) = y.
Proof of probabilistic prorogation Based on the two assumptions above, we provide a proof for
probabilistic propagation in Equation (1) as follows:
P (h(l+1); ψ(l+1))
z{
Pr[h(l+1)∣x] =∣	Pr[h(l+1)∣x,θ(:I)] Q(θ(:l); φCl)) dθCl)
θ(:l)
Z (Z	Pr[h(l+1)∣h(l),θ(l)]Pr[h(l)∣x,θ(: l-1)]dh(l)^ Q(θ(: l); φ(: l)) dθ(: l)
θ(:l)	h(l)
Pr[h(l+1)|h(l), θ(l)]Q(θ(l); φ(l))
h(l),θ(l)
(Z	Pr[h(l)∣x,θ(:l-1)]Q(θ(:l-1); φUlτ))dθClτ)) dh(l)dθ(l)
θ(: l-1)
/
h(l),θ(l)
Pr[h(l+1)|h(l), θ(l)]Q(θ(l); φ(l)) Pr[h(l)|x] dh(l)dθ(l)
X------{-------}
P (h(l); ψ(l))
(25)
(26)
(27)
(28)
C Alternative Evidence Lower B ound and its Analytic Forms
C.1 Alternative Evidence Lower B ound (Proof for Theorem 3.1)
The steps to prove the inequality (6) almost follow the ones for probabilistic propagation above:
Ln(φ) = EQ [log(Pr[yn|xn, θ])]
log(Pr[yn|xn, θ]) Q(θ; φ)dθ
θ
Z log Z
θ	h(L-1)
Zlog (Z
θ	h(L-1)
Pr[yn, h(L-1)|xn, θ]dh(L-1) Q(θ; φ)dθ
Pr[yn∣h(LT),θ(LT)]Pr[h(LT)∣xn,θ0L-2)]dh(LT) ) Q(θ; φ)dθ
≥ / (/@)log(Pr[yn|h(LT),θ(LT)]) Pr[h(LT)Ixn,θ(0:LT)]dh(LT)) Q(θ; φ)dθ
Z	log Pr[yn|h(L-1), θ(L-1)] Q(θ(L-1); φ(L-1))
h(L-1),θ(L-1)
(Z	Pr[h(LT)Ixn"0:L-2)]Q(θ(O:L-2); φ(O:L-2))dθ(O:L-2)[ dh(LT)dθ(LT)
"θ(0,L-2)	)
l	log (Pr[yn∣h(LT),θ(LT)D Q(θ(LT))Pr[h(LT)∣xn]dh(LT)dθ(LT)
h(L-1),θ(L-1)
Eh(L-1)~P; θ(L-1) 〜Q [log(Pr[yn|h(LT),θ(LT)川=Ln(φ)
(29)
(30)
(31)
(32)
(33)
(34)
(35)
(36)
where the key is the Jensen’s inequality EQ [log(∙)] ≥ log (EQ H) in Equation (33). Notice that if
θ(LT) is not random variable (typical for an output layer), Ln(φ) can be simplified as:
Ln(φ) = [ log
h(L-1)
Pr[ynIh(L-1); φ(L-1)] P(h(L-1); ψ(L-1))dh(L-1)
(37)
15
Published as a conference paper at ICLR 2020
where we write Pr[h(L-1) |x] in its parameterized form P(h(L-1); ψ(L-1)). Now, the gradient
∂Ln (φ)∕∂φ(LT) can be obtained by differentiating over Equation (37), while other gradients
∂Ln(φ)∕φ(:L-2) further obtained by chain rule:
∂ Ln(φ) _ ∂ Ln(φ)	∂ψ(LT)
--:-Z-——----rτ--7- , -:-Z-TT-
∂φ(: Lf ∂ψ(LT) ∂φ(: L-2)
(38)
which requires Us to compute ∂Ln(φ)∕∂ψ(LT) and ∂ψ(LT)∕∂φ(: L-2). While ∂Ln(φ)∕∂ψ(LT)
can be derived from Equation (37), ∂ψ(L-1)∕∂φ(: L-2) can be obtained by backpropagating outputs
of the (L - 2)th layer obtained from probabilistic propagation in Equation (1). In other words:
since P(h(L-1); ψ(L-1)) is an intermediate step of the forward pass, ψ(L-1) is a function of all
parameters from previous layers φ(: L-2), and if each step ψ(l+1) —— g(l) (ψ(l), φ(l)) is differentiable
w.r.t. ψ(l) and φ(l), the partial derivatives ∂ψ(L-1)∕∂φ(: L-2) can be obtained by iterative chain rule.
C.2 Softmax Layer for Classification Problem
In this part, we first prove the alternative evidence lower bound (ELBO) for Bayesian neural net-
works with softmax function as their last layers. Subsequently, we derive the corresponding back-
propagation rule for the softmax layer. Finally, we show a method based on Taylor’s expansion to
approximately evaluate a softmax layer without Monte Carlo sampling.
Theorem C.1 (Analytic Form of Ln(φ) for Classification). Let h ∈ RK (with K the number of
classes) be the pre-activations ofa softmax layer (a.k.a. logits), and φ —— s ∈ R+ be a scaling factor
that adjusts its scale such that Pr[y —— c|h, s] —— exp(hc∕s)∕PkK=1 exp(hk ∕s). Suppose the logits
{hk}kK=1 are pairwise independent (which holds under mean-field approximation) and hk follows
a Gaussian distribution hk 〜N(μk,νk) (therefore ψ ——{μk,νk}K=ι) and S is a deterministic
parameter. Then Ln (φ) can be further upper bound by the following analyticform:
Ln(φ) ≥ ' - log
s
ʌ ʌ
，L(Φ)
(39)
Proof. The lower bound follows by plugging Pr[y|h, s] and Pr[hk|x] into Equation (6).
Ln(φ) = l log(Pr[yn ——c∣h; s]) Pr[h∣x]dh
h
log exp
k=1
Y Pr[hk|x] dh
k=1
1 Z hcPr[hc∣Xn]dhc - J' log (X exp (h)) (Y Pr[hk [x]) dh
μ - Z log (X exp (hO(YlIPr[hk[x] Jh
≥ μc- log UhX exp
Pr[hk|x]dhk
Y Pr[hk|x] dh
k=1
exp
exp
μc 1
——------log
s
μc 1
——------log
s
μc 1
——------log
s
1
exp
2πνk
ʌ
L(Φ)
(hk - μk )2
2νk
dhk
(40)
(41)
(42)
(43)
(44)
(45)
(46)
(47)
—
16
Published as a conference paper at ICLR 2020
where the last equation follows
∕h
hk
Z
hk
Z
hk
exp
1
exp
2πVk
1
exp
2πVk
1
exp
2πVk
(hk - μk )2
2νk
dhk
hk - 2(μk + Vk/s)hk + 标
2Vk
(hk - (μk + Vk))2
2νk
dhk
dhk ∙exp (μk+2⅛
(48)
(49)
(50)
—
—
—
}
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
where the under-braced term is unity since it takes the form of Gaussian distribution.
□
From Equation (43) to (44), we use the Jensen’s inequality to achieve a lower bound for integral of
log-sum-exp. The bound can be tighten With advanced techniques in Khan (2012).
Derivatives of Ln (φ) in (39) To use probabilistic backpropagation to obtain the gradients w.r.t. the
parameters from previous layers, we first need to obtain the derivatives w.r.t. ψ(L-1) = {μk, Vk}K=∖.
∂Ln(φ) _ 1 (	exp (〃k/s + Vk/2s2)
∂μk
∂Ln(φ)
∂νk
1
2s2
Pk=I exp (μk/s + Vk/2s2)
e exp (μk/s + Vk/2s2)
- 1[k = c]
(51a)
PK=I exp (μk/s + Vk/2s2)
(51b)
—
s
—
Furthermore, the scale s can be (optionally) updated along with other parameters using the gradient
∂Ln(φ)
∂s
μc .
-《+
PK=I (μk/s2 + Vk/s3) exp μk//s + Vk/2s2)
PK=I exp (μk/s + Vk/2s2)
(52)
Prediction with Softmax Layer Once we learn the parameters for the Bayesian neural network,
in principle we can compute the predictive distribution of y by evaluating the following equation:
Pr[y = c|x] = J Pr[y = c|h, s]Pr[h∣x]dh = J 'c(h)Pr[h∣x]dh	(53)
(Mean-field assumption)=	…	'c(h) I ɪɪ Pr[hk |x] I dhι …dhk
h1	hK	k=1
(54)
where we denote the softmax function as 'c(h) = exp(hc/s)/[E卜 exp(hk/s)]. Unfortunately, the
equation above can not be computed in closed form. The most straight-forWard Work-around is
to approximate the integral by Monte Carlo sampling: for each hk we draw S samples {hsk}sS=1
independently and compute the prediction:
1S
Pr[y = c|x] ≈ s E'c(hs), ∀c ∈ [K]
s=1
(55)
Despite its conceptual simplicity, Monte Carlo method suffers from expensive computation and high
variance in estimation. Instead, we propose an economical estimate based on Taylor’s expansion.
First, We expand the function 'c(h) by Taylor's series at the point μ (UP to the second order):
>
'c(h) = 'c(μ)
1
1	丁「d2'「 1	.	〜
(h -μ)+ 2(h - μ	~∂h2(μ) (h - μ)+ O (kh - Ck )
(56)
2
K
'c(μ) + X
k=1
∂hk (μ)] (hk-μk
KK
)+XX
i=1 j =1
「∂ %	1	.	c、
(μ 7 (μ) (hi - μi)(hj - μj) + O (kh - μk )
∂hhj
(57)
17
Published as a conference paper at ICLR 2020
Before we derive the forms of these derivatives, we first show the terms of odd orders do not con-
tribute to the expectation. For example, if 'c(h) is approximated by its first two terms (i.e. a linear
function), Equation (54) can be written as
Pr[y = c|x] ≈ Z …Z ('c(μ) + ^X
∂hk (M)Jhk-μk)
Pr Pr [hk [x] ) dhι …dhk
k=1
(58)
'c(μ) + X ∂hc(μ)
hk
--I —
((hk - μk) Pr[hk ∣x]dhk ) = 'c(μ)
hk
(59)
where the second term is zero by the symmetry of Pr[hk |x] around μk (or simply the definition of
μk's). Therefore, the first-order approximation results exactly in a (deterministic) softmax function
of the mean vector μ. In order to incorporate the variance into the approximation, we will need to
derive the exact forms of the derivatives of 'c(h). Specifically, the first-order derivatives are obtained
from the definition of 'c(h).
华(h) = 1 ∙ exp(hc∕s)- exp(2hc∕s) = 1 G(h) - '2(h))	(60a)
∂hc	s	PkK=1 exp(hk/s)	s
M(h) = -1 ∙ eχp(hc∕s) ∙ exP®/2) =-1 'c(h)'k(h), ∀k = C (60b)
∂hk	s	PkK=1exp(hk∕s)2 s
and subsequently the second-order derivatives from the first ones:
^hc(h)	= S (∂hcc(h) - 2'c(h) ∂hc (h))	=	S2	(Qc(h)- 3'C(h)+2'3(h))	(61a)
W工；(h) =----(^77c~ (h)'k (h) + 'c(h) pT~(h)^	=	^^2	(-'c(h)'k (h) + 2'2(h)'k (h))	, ∀k = C
∂h2k	s ∂hc	∂hc	s2	c
(61b)
with these derivatives we can compute the second-order approximation as
(]K K ∂ 2'	ʌ (K	∖
'c(μ) + 5 XX
》—―(μ)(hi - μi)(hj - μj) I ( Y Pr[hk[x] )dhι …dhK
2 i=ι j=ι dμiμj	) ;k=1	)
(62)
一、1 ∂ "、f	、o_..... IL ∂ "、f	、」.........
'c (μ) + ʒ 2 2 (μ)	(hc - μc) Pr[hc|x]dhc + ʒ ∕J 2 2 (μ)	(hk - μk) pr[hk |x]dhk
2 dμc	Jhc	2 k=c dμk	Jhk
(63)
'c (μ) + 2S2 ('c(μ) - 3'2(μ) + 2'C(μ)) Vc + 2S2 X (-'c(μ)'k(μ) + 2'2(μ)'k (μ)) Vk
k6=c
(64)
'c (μ) + 2-2 ('c(μ) - 2'2(μ)) (Vc- X 'k(μ)νk)	(65)
s	k=1
The equation above can be further written in vector form as:
Pr[y∣x] ≈ '(μ) + -12 ('(μ) - '(μ)°2) ◦ (ν - '(μ)>ν)	(66)
C.3 Gaussian Output Layer for Regression Problem
In this part, we develop an alternative evidence lower bound (ELBO) for Bayesian neural networks
with Gaussian output layers, and derive the corresponding gradients for backpropagation. Despite
the difficulty to obtain an analytical predictive distribution for the output, we show that its central
moments can be easily computed given the learned parameters.
18
Published as a conference paper at ICLR 2020
Theorem C.2 (Analytic Form of Ln(φ) for Regression). Let h ∈ RI be the output oflast hidden
layer (with I the number of hidden units), and φ = (w, s) ∈ RI × R+ be the parameters that define
the predictive distribution over output y as
Pr[y∣h; w, s] = √1= exp (- (y - W h )	(67)
2πs	2s
Suppose the hidden units {hk}kK=1 are pairwise independent (which holds under mean-field approx-
imαtion), and each hi has mean μi and variance Vi, then Ln(φ) takes an analytic form:
=-(y -w>〃产 + (W°2)>V - log(2πs)	(68)
2s	2
where (wo2)i = w2 and μ = [μι, ∙∙∙ ,μI]> ∈ RI and V = [νι,…,νI]> ∈ RI are vectors ofmean
and variance of the hidden units h.
Proof. The Equation (68) is obtained by plugging Pr[y|h; W, s] into Equation (6).
Ln(φ) = X …X log(Pr[y∣hι,…，hI; w,s]) (Y Pr[hi∣Xn])
h1 hI	i=1
=-X …X(G Y[1 H +号)(Y Pwn]
h1	hI	i=1
=W X …X (y - XWih) 2 (YPr[hi∣χn]) - loφs)
2s h1	hI	i=1	i=1	2
where the long summation in the first term can be further simplified With notations of μ and V:
∑∙∙∙∑
h1 hI
y -	Wihi	Pr[hi|xn]
i=1	i=1
X …X y2 - 2y X Wihi +XWi2hi2+XXWjWkhjhk	Y Pr[hi|xn]
h1	hI	i=1	i=1	j=1 k6=j	i=1
I
=y2 - 2y	Wi
i=1
XhiPr[hi |x]
hi
I
+ X Wi2
i=1
hi2Pr[hi|xn]
+	WjWk	hjPr[hj|xn]	hkPr[hk|xn]
j=1 k6=j	hj	hk
II	I
=y2 - 2y £wiMi + ɪ2w2(μ2 + Vi) + £ EwjWkμjμk
2	I	I 2	(I ) I
=y2 - 2y工 WiMi + 工 Wi2Vi +
r Wj μj I Wk wk μk
=y2 - 2y W>μ + (w^2)>v + (w>μ)2
=(y - W>μ)2 + (w02)>v
(69)
(70)
(71)
(72)
(73)
(74)
(75)
(76)
(77)
(78)
where w°2 denotes element-wise square, i.e. w°2 = [w2,…，W2]>.
□
19
Published as a conference paper at ICLR 2020
Derivatives of Ln(φ) in Equation (68) It is not difficult to show that the gradient of Ln(φ) can
be backpropagated through the last layer. by computing derivatives of Ln(φ) w.r.t. μ and V:
∂Ln(φ) _	(y - w>μ)w
—二----=-------------
∂μ	S
∂Ln(φ)	W°2
----	=	
∂ν--------2s
(79a)
(79b)
Furthermore, the parameters {w, s} can be updated along with other parameters with their gradients:
∂Ln(φ) _	(y - w>μ)μ + (w ◦ V)
—二----=---------------------
∂w	s
∂Ln(φ) = -_1 + (y - w>μ)2 + (W02)>ν
∂s = - 2s +	2S2
(80a)
(80b)
Prediction with Gaussian Layer Once we determine the parameters for the last layer, in principle
we can compute the predictive distribution Pr[y|x] for the output y given the input x according to
Pr[y∣x] = £Pr[y|h; w, s]Pr[h∣x] = E ∙ ••£Pr[y∣h;w, s]	Pr[hi |x]
h	h1 hI	i=1
=X …X √2∏s eχp(- 'y p；1' J) (YPr[hiiχ]!	(81)
h1	hI	i=1
Unfortunately, exact computation of the equation above for arbitrary output value y is intractable in
general. However, the central moments of the predictive distribution Pr[y|x] are easily evaluated.
Consider we interpret the prediction as y = w>h + e, where e ~ N(0, s), its mean and variance
can be easily computed as
E [y|x] = w>E [h] = w>μ	(82a)
V [y|x] = (w02)>V [h] + V [e] = (w02)>ν + S	(82b)
Furthermore, if we denote the (normalized) skewness and kurtosis of hi as γi and κi :
Yi = E [(hi - μi)3∣x] /ν3/2 = X(hi - μi)3Pr[hi∣x]∕ν3/2	(83a)
hi
Ki = E [(hi - μi)4∣x] /νi2 = X(hi - μi)4Pr[hi∣x]∕ν2	(83b)
hi
Then the (normalized) skewness and kurtosis of the prediction y are also easily computed with the
vectors of Y = [γι,…，YI]> ∈ RI and K = [κι, ∙∙∙ ,κι] ∈ RI.
Y[y|x] =	E [(y — w>μ)3∣x] -V [y|x]3/2	-	(W°3)>(γ ◦ V°3/2) [(w°2)>ν + s]3/2	(84a)
K[y|x] =	E [(y — w>μ)4∣x]	(W04)>(κ ◦ V02) + S(W02)>ν	(84b)
	-	V [y|x]2	-	[(w°2)>V + s]2	
D	Probabilistic Propagation in Bayesian Quantized Networks
In this section, we present fast(er) algorithms for sampling-free probabilistic propagation (i.e. eval-
uating Equation (8)). According to Section 4, we divide this section into three parts, each part for a
specific range of fan-in numbers E.
D. 1 Small Fan-in Layers: Direct Tensor Contraction
If E is small, tensor contraction in Equation (8) is immediately applicable. Representative layers of
small E are shortcut layer (a.k.a. skip-connection) and what we name as depth-wise layers.
20
Published as a conference paper at ICLR 2020
Shortcut Layer With a skip connection, the output h(l+1) is an addition of two previous layers
h(l) and h(m). Therefore and the distribution of h(l+1) can be directly computed as
P (h(l+1); Ψ(l+1)) = X δ[h(l+1) = h(l) + h(m)] P (h(l); ψ(I)) P (him); ψ(m))	(85)
h(l),h(m)
i,i
Depth-wise Layers In a depth-wise layer, each output unit hi(l+1) is a transformation (parameter-
ized by θi(l)) of its corresponding input hi(l), i.e.
P(h(l+1); ψ(l+1)) = X Pr[hil+1)∣h(l),θ(l)] Q(θ(l); φim)) PMl; ψ(l))	(86)
hi(l),θi(l)
Depth-wise layers include dropout layers (where θ(l) are dropout rates), nonlinear layers (where
θ(l) are threshold values) or element-wise product layers (where θ(l) are the weights). For both
shortcut and depth-wise layers, the time complexity is O(JD2) since E <= 2.
D.2 Medium Fan-in Layers: Discrete Fourier Transform
In neural networks, representative layers with medium fan-in number E are pooling layers, where
each output unit depends on a medium number of input units. Typically, the special structure of
pooling layers allows for faster algorithm than computing Equation (8) directly.
Max and Probabilistic Pooling For each output, (1) a max pooling layer picks the maximum
value from corresponding inputs, i.e. h(jl+1) = maxi∈I(j) hi(l), while (2) a probabilistic pooling
layer selects the value the inputs following a categorical distribution, i.e. Pr[h(jl+1) = hi(l)] = θi.
For both cases, the predictive distribution of h(jl+1) can be computed as
Max:	P (h(jl+1) ≤	q)	=	Y	P (hi(l) ≤ q)	(87)
i∈I(j)
Prob:	P (hj(l+1) =	q)	=	X	θiP (hi(l) = q)	(88)
i∈I(j)
where P (hi(l) ≤ q) is the culminative mass function of P. Complexities for both layers are O(ID).
Average Pooling and Depth-wise Convolutional Layer Both layers require additions of a
medium number of inputs. We prove a convolution theorem for discrete random variables and show
that discrete Fourier transform (DFT) (with fast Fourier transform (FFT)) can accelerate the additive
computation. We also derive its backpropagation rule for compatibility of gradient-based learning.
Theorem D.1 (Fast summation via discrete Fourier transform). Suppose ui take values in
{bi, bi + 1, . . . , Bi} between integers bi and Bi, then the summation v = PiE=1 ui takes values
between b and B, where b = PiE=1 bi and B = PiE=1 Bi. Let Cv, Cui be the discrete Fourier
transforms of Pv, Pui respectively, i.e.
B
Cv(f) = XPv(v) exp(-j2π(v - b)f/(B - b +1))	(89a)
v=b
Bi
CUi(f) = X PUi(Ui) exp(-j2π(% - bi) f/(Bi- bi + 1))	(89b)
ui =bi
Then Cv (f) is the element-wise product of all Fourier transforms CUi(f), i.e.
E
Cv(f) = YCUi(f),∀f	(90)
i=1
21
Published as a conference paper at ICLR 2020
Proof. We only prove the theorem for two discrete random variable, and the extension to multiple
variables can be proved using induction. Now consider u1 ∈ [b1, B1], u2 ∈ [b2, B2] and their sum
v = u1 + u2 ∈ [b, B], where b = b1 + b2 and B = B1 + B2. Denote the probability vectors of
u1, u2 and v as P1 ∈ 4B1-b1 , P2 ∈ 4B2-b2 and P ∈ 4B-b respectively, then the entries in P are
computed with P1 and P2 by standard convolution as follows:
B1	B2
P(V) = X P1(u1)P2(v - Ui) = X Pι(v - u2)P2(u2), ∀v ∈{b,…，B}	(91)
u1 =b1	u2=b2
The relation above is usually denoted as P = Pi * P2, where * is the symbol for convolution. Now
define the characteristic functions C, C1 , and C2 as the discrete Fourier transform (DFT) of the
probability vectors P, Pi and P2 respectively:
B	2π
C(f ) = EP(V) exp -jR(v-b)f , f ∈ [R]
v=b
Bi	2π
Ci(f) =	Pi (ui)exp (-j 五(Ui- bi )f},f e [R]
ui =bi
(92a)
(92b)
where R controls the resolution of the Fourier transform (typically chosen as R = B - b + 1,
i.e. the range of possible values). In this case, the characteristic functions are complex vectors of
same length R, i.e. C, Ci, C2 ∈ CR, and we denote the (functional) mappings as C = F(P) and
Ci = Fi(Pi). Given a characteristic function, its original probability vector can be recovered by
inverse discrete Fourier transform (IDFT):
1	R-i	2π
P (V) = RE C(f )eχp (j R (V- b)f ) , ∀v ∈ {b, ∙∙∙ ,B}
R	f=0	R
1	R-i	2π
Pi(Ui) =	R ɪ2 Ci(f) exp ( j R (Ui-	bi)f ) ,	∀ui ∈ {bi,…，Bi}
f=0
(93a)
(93b)
which we denote the inverse mapping as P = F-i(C) and Pi = Fi-i(Ci). Now we plug the con-
volution in Equation (91) into the characteristic function C(f) in (92a) and rearrange accordingly:
C(f) = X (X PI(UI)P2(V-UI)) exp (-j2∏(V - b)f'
v=b u1 =b1
B1	B2	2π
(Let U2 = V - Ui ) = Σ Σ PI(UI)P2(U2)exp ( -jR(UI + u2 - b)f
u1=b1 u2=b2
(Since b = bi + b2)
^X PI(UI) exp (—jR(UI - bi)f
u1 =b1
X P2 (U2)exp (-j 2π (U2 - b2)f
u2 =b2
Ci(f) ∙C2(f)
(94)
(95)
(96)
(97)
The equation above can therefore be written as C = Ci ◦C2, where we use ◦ to denote element-wise
product. Thus, we have shown summation of discrete random variables corresponds to element-wise
product of their characteristic functions.	□
With the theorem, addition of E discrete random variables can be computed efficiently as follows
PV = Pu1 * Pu2 *∙∙∙* PUE	(98)
=FT (F(Pu1) ◦ F(Pu2) ◦…。F (PUE))	(99)
where F denotes the Fourier transforms in Equations (93a) and (93b). If FFT is used in computing
all DFT, the computational complexity of Equation (99) is O(ERlogR) = O(E2D log(ED))
(since R = O(ED)), compared to O(DE) with direct tensor contraction.
22
Published as a conference paper at ICLR 2020
Backpropagation When fast Fourier transform is used to accelerate additions in Bayesian quan-
tized network, we need to derive the corresponding backpropagation rule, i.e. equations that relate
∂L∕∂P to {∂L∕∂Pi}i=ι. For this purpose, We break the computation in Equation (99) into three
steps, and compute the derivative for each of these steps.
	= Fi (Pi ) =	∂L	=R ∙ F-1 (	^ ∂C∖	
Ci		⇒砧二		WJ	(100a)
C =C1		∂L	C ∂L		
	◦ ♦♦♦ ◦ CI =		 ξ ∂Ci	=Ci ◦ ∂C		(100b)
	F-1(C)=	∂L	二 R-1∙F (	∂L	
P=		=	= ∂C		∂P)	(100c)
Where in (100b) We use C/Ci to denote element-Wise division. Since Pi lies into real domain, We
need to project the gradients back to real number in (100c). Putting all steps together:
∂l=< {f-1( CCi ◦F (∂L))}, ∀i ∈ [I]	(IOI)
D.3 Large Fan-in Layers: Lyapunov Central Limit Approximation
In this part, We shoW that Lyapunov central limit approximation (Lyapunov CLT) accelerates proba-
bilistic propagation in linear layers. For simplicity, We consider fully-connected layer in the deriva-
tions, but the results can be easily extended to types of convolutional layers. We conclude this part
by deriving the corresponding backpropagation rules for the algorithm.
Linear Layers Linear layers (followed by a nonlinear transformations σ(∙))are the most important
building blocks in neural netWorks, Which include fully-connected and convolutional layers. A linear
layer is parameterized by a set of vectors θ(l)s, and maps h(l) ∈ RI to h(l+1) ∈ RJ as
hjl+1) = σ X X 明∙ h(lɔl = σ X X Uji)] = σ (vjl+1))	(102)
i∈I(j)	i∈I(j)
where u(i) = θj ∙ h(l) and vjl+1) = Pi∈ιj) Uji). The key difficulty here is to compute the distri-
bution of vj(l+1) from the ones of {u(jli)}iI=1, i.e. addition of a large number of random variables.
Theorem D.2 (Fast summation via Lyapunov Central Limit Theorem). Let Vj = σ(vj-) =
σ( i∈I(j) θjiUi) be an activation ofa linear layer followed by nonlinearity σ. Suppose both inputs
{Ui}i∈I and parameters {θji}i∈I(j) have bounded variance, then for sufficiently large |I (j)|, the
distribution of Vj converges to a Gaussian distribution N(μj ,νj-) with mean and variance as
I
μj =)： mjiμi	(103a)
i=1
I
Vj =): mjiνi + vjiμi + vjiνi	(103b)
i=1
where mji = E [θji], Vji = V [θji] and μi = E [Ui], Vi = V [Ui]. And if the nonlinear transform
σ is a sign function, each activation Vj follows a Bernoulli distribution P (Vj = 1) = Φ(μj-/ʌ/vj),
where Φ is the culminative probability function ofa standard Gaussian distribution N(0, 1).
Proof. The proof follows directly from the definitions of mean and variance:
II
μj = E X θji hi = X E [θji hi]
i=1	i=1
II
= E[θji]E[hi]=	mjiμi
i=1	i=1
(104)
(105)
23
Published as a conference paper at ICLR 2020
■ I	I I
Vj= V X θji hi = X V [θji hi]	(106)
_i=1	_|	i=1
I
=X (E [θ2i] E [h2] - E [θ2i] E [h2])	(107)
i=1
I
=X [(m2i+ vi) (μ2+ Vji) - mji^i]	(108)
i=1
I
=E (m2i% + Vjiμ2 + VjiVi)	(109)
i=1
For fully-connected layers, these two equations can be concisely written in matrix forms:
μ = Mμ	(110a)
ν= (Mo2) V + V (μo2 + ν)	(110b)
where Mo2 and μo2 are element-wise square of M and μ respectively.	□
Backpropagation With matrix forms, the backpropagation rules that relate ∂L∕∂ψ0+1) =
{∂L∕∂μ,∂L∕∂V} to ∂L∕∂φ⑴={∂L∕∂M,∂L∕∂V} and ∂L∕∂ψ⑷={∂L∕∂μ, ∂L∕∂ν} can
be derived with matrix calculus.
(111a)
(111b)
(111c)
(111d)
Notice that these equations do not take into account the fact that V implicitly defined with M (i.e.
Vji is defined upon mji). Therefore, We adjust the backpropagation rule for the probabilities: denote
Qji(d) = Q(θji = Q(d); φj?), then the backpropagation rule can be written in matrix form as
∂ L
∂Q(d)
∂ ∂L	∂L ∂V) ∂M ∂L ∂ν
∂M∖+ + ∂v ∙ ∂M∂ ∂Q(d) + ∂V ∙ ∂Q(d)
∂L
Q(d) ∙ ∂M + 2(Q(d) - M)Q
(112)
(113)

Lastly, We derive the backpropagation rules for sign activations. Let Pj denote the probability that
the hidden unit Vj is activated as Pj = Pr[v7- = 1|x], ∂L∕pj relates to {∂L∕∂μtj, ∂L∕∂Vj} as:
∂pj
∂Vj
也=N Jj
∂μj	∖ Fj
--j W (p⅛
(114a)
(114b)
E Supplementary Material for Experiments
E.1 network Archiectures
(1) For MNIST, Fashion-MNIST and KMNIST, we evaluate our models on both MLP and CNN.
For MLP, we use a 3-layers network with 512 units in the first layer and 256 units in the second; and
24
Published as a conference paper at ICLR 2020
for CNN, we use a 4-layers network with two 5 × 5 convolutional layers with 64 channels followed
by 2 × 2 average pooling, and two fully-connected layers with 1024 hidden units. (2) For CIFAR10,
we evaluate our models on a smaller version of VGG (Peters & Welling, 2018), which consists of
6 convolutional layers and 2 fully-connected layers: 2 X 128C3 - MP2 - 2 X 256C3 - MP2 - 2 x
512C3 - MP2 - 1024FC - SM10.
E.2 More Results for Multi-layer Perceptron (MLP)
102.5
月
102
BNN
E-QNN
BQN
103.5
103
101.5	I
10-4	10-3
λ level of model uncertainty
104
——BNN
——E-QNN
——BQN
103
10-4	10-3
λ level of model uncertainty
BNN
E-QNN
BQN
102.5
10-4	10-3
λ level of model uncertainty
(c) NLL KMNIST
(a) NLL MNIST
4
rorrE egatnecreP
BNN
E-QNN T
BQN
-×
(b) NLL FMNIST
rorrE egatnecreP
10-4	10-3
λ level of model uncertainty
X-相
14 12 10 8
rorrE egatnecreP
BNN
E-QNN
BQN
10-4	10-3
λ level of model uncertainty
(d)	Error MNIST
(e)	Error FMNIST
10-4	10-3
λ level of model uncertainty
(f)	Error KMNIST

Figure 4:	Comparison of the predictive performance of our BQNs against the E-QNN as well as the
non-quantized BNN trained by SGVB on a MLP. Negative log-likelihood (NLL) which accounts for
uncertainty and 0-1 test error which doesn’t account for uncertainty are displayed.
140
120
400 -
500
Monte Carlo Sampling
Analytical Inference
Difference
400
100
Monte Carlo Sampling
Analytical Inference
Difference
300 -
60
200
200
Monte Carlo Sampling
Analytical Inference
Difference
40
100
100
20
10-4
10-3
λ level of model uncertainty
10-4
(a) NLL MNIST
3
10-4
10-3
λ level of model uncertainty
Monte Carlo Sampling
Analytical Inference
Difference
(b) NLL FMNIST
rorrE egatnecreP
rorrE egatnecreP
Monte Carlo Sampling
Analytical Inference
Difference
10 8 6 4
rorrE egatnecreP
(c) NLL KMNIST
10-3
λ level of model uncertainty
Monte Carlo Sampling
Analytical Inference
Difference
m 80
j 300

10-4	10-3
λ level of model uncertainty
0
10-4
10-3
λ level of model uncertainty
10-4	10-3
λ level of model uncertainty
(d)	Error MNIST
(e)	Error FMNIST
(f)	Error KMNIST
Figure 5:	Illustration of mean-field approXimation and tightness of alternative ELBO on a MLP.
The performance gap between our analytical inference and the Monte Carlo Sampling is displayed.
25
Published as a conference paper at ICLR 2020
E.3 Regression on Boston Housing Dataset
In this part, we evaluate our proposed BQN on Boston housing dataset, a regression benchmark
widely used in testing Bayesian neural networks (Hernandez-Lobato & Adams, 2015; Ghosh et al.,
2016) and Probabilistic neural networks (Wang et al., 2016). The dataset consists of 456 training
and 50 test samples, each sample has 13 features as input and a scalar (housing) price as output.
Following Hemandez-Lobato & Adams (2015); Ghosh et al. (2016); Wang et al. (2016), We train a
two-layers network with 50 hidden units, and report the performance in terms of root mean square
error (RMSE) in Table 4. The results show that our BQN achieves lower RMSE compared to other
models trained in a probabilistic/Bayesian way.
Dataset ∣ BQN	PBP(GhOShetal.,2016)	EBP(SOudryetal.,2014) NPN(Wangetal.,2016)
Boston I 2.04 ± 0.07	2.79 ± 0.16	3.14 ± 0.93	2.57± NA
Table 4: Performance of different networks in terms of RMSE. The numbers for BQN are averages
over 10 runs with different seeds, the standard deviation are exhibited following the ± sign. The
results for PBP, EBP are from Ghosh et al. (2016), and the one for NPN is from (Wang et al., 2016).
26