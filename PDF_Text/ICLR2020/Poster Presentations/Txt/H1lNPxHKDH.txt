Published as a conference paper at ICLR 2020
A Function Space View of B ounded Norm Infi-
nite Width ReLU Nets: The Multivariate Case
Greg Ongie
Department of Statistics
University of Chicago
Chicago, IL 60637, USA
gongie@uchicago.edu
Daniel Soudry
Electrical Engineering Department
Technion, Israel Institute of Technology
Haifa, Israel
daniel.soudry@technion.ac.il
Rebecca Willett
Department of Statistics & Computer Science
University of Chicago
Chicago, IL 60637, USA
willett@uchicago.edu
Nathan Srebro
Toyota Technological Institute at Chicago
Chicago, IL 60637, USA
nati@ttic.edu
Ab stract
We give a tight characterization of the (vectorized Euclidean) norm of weights
required to realize a function f : Rd → R as a single hidden-layer ReLU network
with an unbounded number of units (infinite width), extending the univariate char-
acterization of Savarese et al. (2019) to the multivariate case.
1 Introduction
It has been argued for a while, and is becoming increasingly apparent in recent years, that in terms
of complexity control and generalization in neural network training, “the size [magnitude] of the
weights is more important then the size [number of weights or parameters] of the network” (Bartlett,
1997; Neyshabur et al., 2014; Zhang et al., 2016). That is, inductive bias and generalization are not
achieved by limiting the size of the network, but rather by explicitly (Wei et al., 2019) or implicitly
(Nacson et al., 2019; Lyu & Li, 2019) controlling the magnitude of the weights.
In fact, since networks used in practice are often so large that they can fit any function (any labels)
over the training data, it is reasonable to think of the network as virtually infinite-sized, and thus
able to represent essentially all functions. Training and generalization ability then rests on fitting
the training data while controlling, either explicitly or implicitly, the magnitude of the weights.
That is, training searches over all functions, but seeks functions with small representational cost,
given by the minimal weight norm required to represent the function. This “representational cost
of a function” is the actual inductive bias of learning—the quantity that defines our true model
class, and the functional we are actually minimizing in order to learn. Understanding learning with
overparameterized (virtually infinite) networks thus rests on understanding this “representational
cost”, which is the subject of our paper. Representational cost appears to play an important role in
generalization performance; indeed Mei & Montanari (2019) show that minimum norm solutions
are optimal for generalization in certain simple cases, and recent work on “double descent” curves
is an example of this phenomenon (Belkin et al., 2019; Hastie et al., 2019).
We can also think of understanding the representational cost as asking an approximation theory
question: what functions can we represent, or approximate, with our de facto model class, namely
the class of functions representable with small magnitude weights? There has been much celebrated
work studying approximation in terms of the network size, i.e., asking how many units are necessary
in order to approximate a target function (Hornik et al., 1989; Cybenko, 1989; Barron, 1993; Pinkus,
1999). But if complexity is actually controlled by the norm of the weights, and thus our true model
class is defined by the magnitude of the weights, we should instead ask how large a norm is necessary
in order to capture a target function. This revised view of approximation theory should also change
how we view issues such as depth separation: rather then asking how increasing depth can reduce
1
Published as a conference paper at ICLR 2020
the number of units required to fit a function, we should instead ask how increasing depth can reduce
the norm required, i.e., how the representational cost we study changes with depth.
Our discussion above directly follows that of Savarese et al. (2019), who initiated the study of the
representational cost in term of weight magnitude. Savarese et al. considered two-layer (i.e., single
hidden layer) ReLU networks, with an unbounded (essentially infinite) number of units, and where
the overall Euclidean norm (sum of squares of all the weights) is controlled. (Infinite width networks
of this sort have been studied from various perspectives by e.g., Bengio et al. (2006); Neyshabur
et al. (2015); Bach (2017); Mei et al. (2018)). For univariate functions f : R → R, corresponding
to networks with a single one-dimensional input and a single output, Savarese et al. obtained a crisp
and precise characterization of the representational cost, showing that minimizing overall Euclidean
norm of the weights is equivalent to fitting a function by controlling:
max (/ |f00(x)∣dx, |f0(-∞) + f0(+∞)∣) .	(1)
While this is an important first step, we are of course interested also in more than a single one-
dimensional input. In this paper we derive the representational cost for any function f : Rd → R in
any dimension d. Roughly speaking, the cost is captured by:
kf∣∣R≈kR{∆3+1"2f}kι ≈ ∣∣∂d+1 R{f}kι	(2)
where R is the Radon transform, ∆ is the Laplacian, and ∂b is a partial derivative w.r.t. the offset in
the Radon transform (see Section 3 for an explanation of the Radon transform). This characterization
is rigorous for odd dimensions d and for functions where the above expressions are classically well-
defined (i.e., smooth enough such that all derivatives are finite, and the integrand in the Radon
transform is integrable). But for many functions of interest these quantities are not well-defined
classically. Instead, in Definition 1, we use duality to rigorously define a semi-norm kfkR that
captures the essence of the above quantities and is well-defined (though possibly infinite) for any
f in any dimension. We show that kf kR precisely captures the representational cost of f, and in
particular is finite if and only if f can be approximated arbitrarily well by a bounded norm, but
possibly unbounded width, ReLU network. Our precise characterization applies to an architecture
with unregularized bias terms (as in Savarese et al. (2019)) and a single unregularized linear unit—
otherwise a correction accounting for a linear component is necessary, similar but more complex
than the term ∣f0(-∞) + f 0(+∞)∣ in the univariate case, i.e., (1).
As we uncover, the characterization of the representational cost for multivariate functions is unfor-
tunately not as simple as the characterization (1) in the univariate case, where the Radon transform
degenerates. Nevertheless, it is often easy to evaluate, and is a powerful tool for studying the rep-
resentational power of bounded norm ReLU networks. Furthermore, as detailed in Section 5.5,
there is no kernel function for which the associated RKHS norm is the same as (2); i.e., training
bounded norm neural networks is fundamentally different from kernel learning. In particular, using
our characterization we show the following:
•	All sufficiently smooth functions have finite representational cost, but the necessary degree
of smoothness depends on the dimension. In particular, all functions in the Sobolev space
Wd+1,1(Rd), i.e., when all derivatives up to order d + 1 are L1-bounded, have finite representa-
tional cost, and this cost can be bounded using the Sobolev norm. (Section 5.1)
•	We calculate the representational cost of radial “bumps”, and show there are bumps with finite
support that have finite representational cost in all dimensions. The representational cost increases
as 1∕ε for “sharp” bumps of radius ε (and fixed height). (Section 5.2)
•	In dimensions greater than one, we show a general piecewise linear function with bounded support
has infinite representational cost (i.e., cannot be represented with a bounded norm, even with
infinite networks). (Section 5.3)
•	We obtain a depth separation in terms of norm: we demonstrate a function that is representable
using a depth three ReLU network (i.e., with two hidden layers) with small norm weights, but
cannot be represented by any bounded-norm depth two (single hidden layer) ReLU network. As
far as we are aware, this is the first depth separation result in terms of the norm required for
representation. (Section 5.4)
2
Published as a conference paper at ICLR 2020
Related Work Although the focus of most previous work on approximation theory for neural
networks was on the number of units, the norm of the weights was often used as an intermediate
step. However, this use does not provide an exact characterization of the representational cost, only
a (often very loose) upper bound, and in particular does not allow for depth separation results where
a lower bound is needed. See Savarese et al. (2019) for a detailed discussion, e.g., contrasting with
the work of Barron (1993; 1994).
The connection between the Radon transform and two-layer neural networks was previously made
by Carroll & Dickinson (1989) and Ito (1991), who used it to obtain constructive approximations
when studying approximation theory in terms of network size (number of units) for threshold and
sigmoidal networks. This connection also forms the foundation of ridgelet transform analysis of
functions Candes & Donoho (1999); Candes (1999). More recently, Sonoda & MUrata (2017) used
ridgelet transform analysis to study the approximation properties of two-layer neural networks with
unbounded activation functions, including the ReLU.
While working on this manuscript, we learned through discussions with Matus Telgarsky of his
related parallel work. In particular, Bailey et al. (2019); Ji et al. (2019) obtained a calculation
formula for the norm required to represent a radial function, paralleling our calculations in Section
5.2, and used it to show that sufficiently smooth radial functions have finite norm in any dimension,
and studied how this norm changes with dimension.
2 Infinite Width ReLU Networks
We repeat here the discussion of Savarese et al. (2019) defining the representational cost of infinite-
width ReLU networks, with some corrections and changes that we highlight. Consider the collection
of all two-layer networks having an unbounded number of rectified linear units (ReLUs), i.e., all
gθ : Rd → R defined by
k
gθ (x) =	ai [wi>x - bi]+ + c, for all x ∈ Rd
(3)
i=1
with parameters θ = (k, W = [w1, ..., wk], b = [b1, ..., bk]>, a = [a1, ..., ak]>, c), where the width
k ∈ N is unbounded. Let Θ be the collection of all such parameter vectors θ. For any θ ∈ Θ we
let C(θ) be the sum of the squared Euclidean norm of the weights in the network excluding the bias
terms, i.e.,
1	1k
C(θ) = 2 (kW kF + kak2) = 2∑ (kwik2 + ㈤2),
(4)
i=1
and consider the minimal representation cost necessary to exactly represent a function f ∈ Rd → R
R(f) := inf C(θ) s.t. f=gθ.
(5)
By the 1-homogeneity of the ReLU, it is shown in Neyshabur et al. (2014) (see also Appendix A of
Savarese et al. (2019)) that minimizing C(θ) is the same as constraining the inner layer weight
vectors {wi}1k=ι to be unit norm while minimizing the '1-norm of the outer layer weights a.
Therefore, letting Θ0 be the collection of all θ ∈ Θ with each wi constrained to the unit sphere
Sd-1 := {w ∈ Rd : kwk = 1}, we have
R(f) = inf0 kak1 s.t. f = gθ.
θ∈Θ0
(6)
However, we see R(f) is finite only if f is exactly realizable as a finite-width two layer ReLU
network, i.e., f must be a continuous piecewise linear function with finitely many pieces. Yet, we
know that any continuous function can be approximated uniformly on compact sets by allowing the
number of ReLU units to grow to infinity. Since we are not concerned with the number of units, only
their norm, we modify our definition of representation cost to capture this larger space of functions,
and define1
R(f) := lim Gnf, C(θ) s.t. ∣gθ(X)- f(x)l ≤ ε ∀kxk≤ 1∕ε and gθ(0) = f(0)]	(J)
ε→0 θ∈Θ0
1Our definition of R(f) differs from the one given in Savarese et al. (2019). We require ∣gθ(x) — f (x)| ≤ ε
on the ball of radius 1∕ε rather than all of Rd, and We additionally require gθ (0) = f (0). These modifications
are needed to ensure (7) and (9) are equivalent. Also, we note the choice of zero in the condition gθ (0) = f(0)
is arbitrary and can be replaced with any point x0 ∈ Rd .
3
Published as a conference paper at ICLR 2020
In words, R(f) is the minimal limiting representational cost among all sequences of networks Con-
verging to f uniformly (while agreeing with f at zero).
Intuitively, if R(f) is finite this means f is expressible as an “infinite-width” two layer ReLU net-
work whose outer-most weights are described by a density α(w, b) over all weight and bias pairs
(w, b) ∈ Sd-1 × R. To make this intuition precise, let M (Sd-1 × R) denote the space of signed
measures α defined on (w, b) ∈ SdT X R with finite total variation norm ∣∣αkι = 八壮—1×r d∣ɑ∣
(i.e., the analog of the L1-norm for measures), and let c ∈ R. Then we define the infinite-width
two-layer ReLU network hα,c (or “infinite-width net” for short) by2
hα,c(x):= /	([w>x — b]+ 一 [—b]+) dα(w,b) + C	(8)
Sd-1 ×R
We prove in Appendix C that R(f) is equivalent to
R(f) =	min	∣∣α∣ι s.t. f = hα,c.	(9)
α∈M (Sd-1 ×R),c∈R
Hence, learning an unbounded width ReLU network gθ by fitting some loss functional L(∙) while
controlling the Euclidean norm of the weights C(θ) by minimizing
inf L(gθ) + λC(θ)	(10)
θ∈Θ
is effectively the same as learning a function f by controlling R(f):
min L(f)+ λR(f).	(11)
f:Rd→R
In other words, R(f) captures the true inductive bias of learning with unbounded width ReLU
networks having regularized weights. Our goal is then to calculate R(f) for any function f : Rd →
R, and in particular characterize when it is finite in order to understand what functions can be
approximated arbitrarily well with bounded norm but unbounded width ReLU networks.
2.1 Simplification via unregularized linear unit
Every two-layer ReLU network decomposes into the sum of a network with absolute value units
plus a linear part3. As demonstrated by Savarese et al. (2019) in the 1-D setting, the weights on the
absolute value units typically determine the representational cost, with a correction term needed if
the linear part has large weight. To allow fora cleaner formulation of the representation cost without
this correction term, we consider adding in one additional unregularized linear unit v>x (similar to
a “skip connection”) to “absorb” any representational cost due to the linear part.
Namely, for any θ ∈ Θ and v ∈ Rd we define the class of unbounded with two-layer ReLU networks
gθ,v with a linear unit by gθ,v(x) = gθ(x) + v>x where gθ is as defined in (3), and associate gθ,v
with the same weight norm C(θ) as defined in (4) (i.e., we exclude the norm of the weight v on the
additional linear unit from the cost). We then define the representational cost Rι(f) for this class of
networks by
Rι(f) ：= lim
ε→0
(12)
Likewise, for all α ∈ M (Sd-1 × R), v ∈ Rd, c ∈ R, we define an infinite width net with a linear
unit by hα,v,c(x) ：= hα,c(x) + v>x. We prove in Appendix C that Rι(f) is equivalent to:
Rι(f)=	min	∣∣α∣ι s.t. f = hα,v,c.	(13)
α∈M (Sd-1 ×R),v∈Rd,c∈R
In fact, we show the minimizer of (13) is unique and is characterized as follows:
2Our definition of hα,c also differs from the one given in Savarese et al. (2019). To ensure the integral is
well-defined, we include the additional -[-b]+ term in the integrand. See Remark 1 in Appendix B for more
discussion on this point.
3Such a decomposition follows immediately from the identity [t]+ = 2 (|t| + t)
θ∈Θnf∈Rd S") V * (X)-f (X)I ≤ ε ∀kxk≤ W and gθ (O)
4
Published as a conference paper at ICLR 2020
Lemma 1. Rι(f) = ∣∣α+∣∣ι where α+ ∈ M(SdT X R) is the unique even measure4 such that
f = hα+,v,c for some v ∈ Rd, c ∈ R.
The proof of Lemma 1 is given in Appendix D. The uniqueness in Lemma 1 allows for a more
explicit characterization Rι(f) in function space relative to R(f), as We show in Section 4.
3 The Radon transform and its dual
Our characterization of the representational cost in Section 4 is posed in terms of the Radon trans-
form — a transform that is fundamental to computational imaging, and whose inverse is the basis
of image reconstruction in computed tomography. For an investigation of its properties and appli-
cations, see Helgason (1999). Here we give a brief review of the Radon transform and its dual as
needed for subsequent derivations; readers familiar with these topics can skip to Section 4.
The Radon transform R represents a function f : Rd → R in terms of its integrals over all possible
hyperplanes in Rd, as parameterized by the unit normal direction to the hyperplane w ∈ Sd-1 and
the signed distance of the hyperplane from the origin b ∈ R:
R{f}(w, b) :=	f(x) ds(x) for all (w, b) ∈ Sd-1 × R,
w>x=b
(14)
where ds(x) represents integration with respect to (d-1)-dimensional surface measure on the hyper-
plane w>x = b. Note the Radon transform is an even function, i.e., R{f}(w, b) = R{f}(-w, -b)
for all (w, b) ∈ Sd-1 × R, since the equations w>x = b and -w>x = -b determine the same
hyperplane. See Figure 1 for an illustration of the Radon transform in dimension d = 2.
The Radon transform is invertible for many common spaces of functions, and its inverse is a compo-
sition of the dual Radon transform R (i.e., the adjoint of R) followed by a filtering step in Fourier
domain. The dual Radon transform R maps a function 夕： Sd-1 × R
→ R to a function over
x ∈ Rd by integrating over the subset of coordinates (w, b) ∈ Sd-1 × R corresponding to all
hyperplanes passing through x:
R*{0(X):/
φ(w, w>x) dw for all X ∈ Rd
(15)
where dw represents integration with respect to the surface measure of the unit sphere Sd-1. The
filtering step is given by a (d 一 1)/2-POWer of the (negative) Laplacian (-∆)(d-1)/2, where for any
s > 0 the operator (-∆)s/2 is defined in Fourier domain by
----7∖	^
(-∆)s/2 f (ξ) = kξksf(ξ),	(16)
using b(ξ) := (2π)-d/2 Rg(x)e-iξ>xdx to denote the d-dimensional Fourier transform at the
Fourier domain (frequency) variable ξ ∈ Rd. When d is odd, (-∆)(d-1)/2 is the same as applying
the usual Laplacian (d _ 1)/2 times, i.e., (-∆)(d-1)/2 = (-l)(dT)/245-1"2, while if d is even
it is a pseudo-differential operator given by convolution with a singular kernel. Combining these
two operators gives the inversion formula f = γd(一∆)(d-1"2R*{R{f}}, where Yd is a constant
depending on dimension d, which holds for f belonging to many common function spaces (see,
e.g., Helgason (1999)).
The dual Radon transform is also invertible by a similar formula, albeit under more restrictive con-
ditions on the function space. We use the following formula due to Solmon (1987) that holds for all
Schwartz class functions5 on Sd-1 × R, which we denote by S (Sd-1 × R):
Lemma 2 (Solmon (1987)). If 夕 is an even function6, i.e.,夕(一w, —b)=夕(w, b) for all (w, b) ∈
Sd-1 × R, belonging to the Schwartz class S (Sd-1 × R), then
YdR{(-△产-1"2R*{0} =夕，	(17)
Where Yd = 2(2π)d-1.______________
4Roughly speaking, a measure α is even if α(w, b) = α(-w, -b) for all (w, b) ∈ Sd-1 × R;
see Appendix
B for a precise definition.
5i.e., functions 夕：SdT × R → R that are C∞-smooth such that g(w,b) and all its partial derivatives
decrease faster than O(|b|-N) as |b| → ∞ for any N ≥ 0
6The assumption that φ is even is necessary since odd functions are annihilated by R*.
5
Published as a conference paper at ICLR 2020
4 REPRESENTATIONAL COST IN FUNCTION SPACE: THE R-NORM
Our starting point is to relate the Laplacian of an infinite-width net to the dual Radon transform of
its defining measure. In particular, consider an infinite width net f defined in terms of a smooth
density α(w, b) over Sd-1 × R that decreases rapidly to zero with |b| → ∞, so that we can write
f(x) =
Sd-1×R
[w>x - b]+ - [-b]+ α(w, b) dw db + v>x + c.
(18)
Differentiating twice inside the integral, the Laplacian ∆f (x) = Pid=1 ∂x2i f (x) is given by
∆f(x) =	δ(w>x - b)α(w, b) dw db =	α(w, w>x) dw.
(19)
where δ(∙) denotes a Dirac delta7. We see that the right-hand side of (19) is precisely the dual
Radon transform of α, i.e., We have shown ∆f = R*{a}. Applying the inversion formula for the
dual Radon transform given in (17) to this identity, and using the characterization of Rι(f) given in
Lemma 1, immediately gives the following result.
Lemma 3. Suppose f = hα,v,c for some α ∈ S Sd-1 × R with α even, and v ∈ Rd, c ∈ R. Then
α = -γdR{(-△产+1"2f}, andRι(f) = γd∣∣R{(-△产+1"2f}∣∣1 where Yd =	.
This result suggests that more generally if we are given any function f, we ought to be able to
compute Ri (f) using the formula in Lemma 3. The following result, proved in Appendix D, shows
this is indeed the case assuming f is integrable and sufficiently smooth, which for simplicity we
state in the case of odd dimensions d. 8.
Proposition 1. Suppose d is odd. If both f ∈ L1(Rd) and △(d+1)/2 f ∈ L1(Rd), then
Ri(f) = YdkR{∆(d+1)/2f}kι = Ydkdbd+1R{f}kι < ∞.	(20)
Here we used the intertwining property of the Radon transform and the Laplacian to write
R{∆(d+1"2f} = ∂d+1R{f} (see Appendix A for more details).
Given these results, one might expect for an arbitrary function f we should have Ri (f) equal to
one of the expressions in (20). However, for many functions of interest these quantities are not
classically well-defined. For example, the finite-width ReLU net f(x) = Pin=i ai [wi>x - bi]+ is
a piecewise linear function that is non-smooth along each hyperplane wi>x = bi, so its derivatives
can only be understood in the distributional sense. Similarly, in this case the Radon transform of f
is not well-defined since f is unbounded and not integrable along hyperplanes.
Instead, we use duality to define a functional (the “R-norm”) that extends to the more general
case where f is possibly non-smooth or not integrable along hyperplanes. In particular, we define a
functional on the space of all Lipschitz continuous functions9. The main idea is to re-express the Li-
norm in (20) as a supremum of the inner product over a space of dual functions ψ : Sd-i × R → R,
i.e., using the fact R is the adjoint of R and the Laplacian △ is self-adjoint we write
kR{∆(d+1”2f}kι = sup hR{∆(d+1”2f},ψi = sup hf, ∆(d+1”2R*{ψ}i	(21)
kψk∞≤i	kψk∞≤i
then restrict ψ to a space where ∆(d+1"2R*{ψ} is always well-defined. More formally, we have:
Definition 1. For any Lipschitz continuous function f : Rd → R define its R-norm10 kfkR by
kf kR := sup {-γdhf, (-∆)(d+1"2R*{ψ}i : ψ ∈ S(SITX R), ψ even , kψk∞ ≤ l} .	(22)
where Yd = 2(2∏l)d-1, S(SdT × R) is the space of Schwartz functions on SdT × R, and hf, g):=
Rd f (x)g (x)dx. Iff is not Lipschitz we define kfkR = +∞.
7Here we use Dirac deltas informally; for a formal derivation of (19) see Lemma 9 in Appendix D.
8For d even, Proposition 1 holds with the pseudo-differential operators (—∆)(d+1)/2 and (—∂2)(d+1)/2 in
place of ∆(d+1)/2 and ∂d+1; see Section 3.
9Recall that f is Lipschitz continuous if there exists a constant L (depending on f) such that
|f(x) - f (y)| ≤ Lkx - yk forallx,y ∈ Rd.
10Strictly speaking, the functional |卜||冗 is not a norm, but it is a semi-norm on the space of functions for
which it is finite; see Appendix F.
6
Published as a conference paper at ICLR 2020
We prove in Appendix D that the R-norm is well-defined, though not always finite, forall LiPschitz
functions and, whether finite or infinite, it is always equal to the representational cost Rι(∙):
Theorem 1. Rι(f) = IlfkR for all functions f. In particular, Rι(f) is finite if and only if f is
Lipschitz and kf kR is finite.
We give the proof of Theorem 1 in Appendix D, but the following example illustrates many of its
key elements.
Example 1. We compute Rι(f) = IlfkR in the case where f is a finite-width two-layer ReLU
network. First, consider the case where f consists ofa single ReLU unit: f(x) = a1 [w1>x - b1]+
for some a1 ∈ R and (w1, b1) ∈ Sd-1. Note that ∆f (x) = a δ(w1>x -b1) in a distributional sense,
i.e., for any smooth test function 夕 we have〈△/,夕)=hf, △夕)=aι / 夕(x)δ(w> X — bι)dx =
aιR{φ}(w1,b1). Sofor any even ψ ∈ S(SdT X R) we have
—Ydhf, (—∆)3+1”2R*{ψ}i = Ydh∆f, (—∆)3-1"2R*{ψ}i	(23)
=aιγdR{(-∆)3-1"2R*{ψ}}(wι,bι)	(24)
= a1ψ(w1,b1)	(25)
where in the last step we used the inversion formula (17). Since the supremum defining kfkR is over
all even ψ ∈ S (SdT × R) such that kΨk∞ ≤ ',taking any ψ* such that ψ*(wι, bi) = sign (aι) and
∣ψ*(wι,bι)∣ ≤ 1 otherwise, we see that kf Ir = |ai|. The general case nowfollows by linearity: let
f(x) = Pik=1ai[wi>x —bi]+ such that all the pairs {(wi, bi)}ik=1 ∪ {(—wi, —bi)}ik=1 are distinct.
Then for any ψ ∈ S (Sd-1 × R) we have
k
—Ydhf, (—∆)(d+1”2R*{ψ}i = Xaiψ(wi,bi).	(26)
i=1
Letting ψ* be any even Schwartzfunction such that ψ*(wi, bi) = @*(—Wi, —bi) = sign(ai) for all
i = 1,...,k and ∣ψ*(w, b)| ≤ 1 otherwise, we see that Ri(f) = kf Ir = Pk=i |a/.
The representational cost R(f) defined without the unregularized linear unit is more difficult to
characterize explicitly. However, We prove that R(f) is finite if and only if kf IIr is finite, and give
bounds for R(f) in terms OfkfkR and the norm of the gradient of the function “at infinity”, similar
to the expressions derived in Savarese et al. (2019) in the 1-D setting.
Theorem 2. R(f) is finite ifand only if kf ∣∣r is finite, in which case we have the bounds
max{kf∣R , 2∣∣Vf(∞)k} ≤ R(f) ≤ kf Ir + 2∣∣Vf(∞)k,	(27)
where Vf (∞) := limr-∞ Cdrd-I Hkxk=r Vf (x)ds(x) ∈ Rd with cd := RSd-I dw = Γ2∏d∕2).
In particular, if Vf (∞) = 0 then R(f) = Ri(f) = kf Ir .
We give the proof of Theorem 2 in Appendix E. The lower bound max{kf kR , 2kVf (∞)k} is anal-
ogous to the expression for theJD representational cost (1) obtained in Savarese et al. (2019). From
this, one might speculate that R(f) is equal to max{∣f Ir , 2∣Vf (∞)k2}. However, in Appendix
E we show this is not the case: there are examples of functions f in all dimensions such that R(f)
attains the upper bound in a non-trivial way (e.g., f(x, y) = |x| + y in d = 2).
4.1 PROPERTIES OF THE R-NORM
In Appendix F we prove several useful properties for the R-norm. In particular, we show the R-
norm is in fact a semi-norm, i.e., it is absolutely homogeneous and satisfies the triangle inequality,
while kf kR = 0 if and only if f is affine. We also show the R-norm is invariant to coordinate
translation and rotations, and prove the following scaling law under contractions/dilation:
Proposition 2. If fε(x) := f(x∕ε) for any ε > 0, then ∣fε∣R = ε-i ∣f ∣r
Proposition 2 shows that “spikey” functions will necessarily have large R-norm. For example, let
f be any non-negative function supported on the ball of radius 1 with maximum height 1 such that
kf kR is finite. Then the contraction fε is supported on the ball of radius ε with maximum height
7
Published as a conference paper at ICLR 2020
1, but kfεkR = ε-1 kf kR blows up as ε → 0. From a generalization perspective, the fact that the
R-norm blows up with contractions is a desirable property, since otherwise the minimum norm fit to
data would be spikes on data points. In particular, this is what would happen if the representational
cost involved derivatives lower than d + 1, and so in this sense it is not a coincidence that kfkR
involves derivatives of order d + 1; for more discussion on this point see Section 5.1.
Finally, we also show that having finite R-norm implies strong conditions in Fourier domain. In
particular, we show that if the R-norm of an L1 function is finite then its Fourier transform must
decay rapidly along every ray. A precise statement is given in Proposition 12 in Appendix F.
5 Consequences, Applications and Discussion
Our characterization of the representational cost for multivariate functions in terms of the R-norm
is unfortunately not as simple as the characterization in the univariate case. Nevertheless, it is often
easy to evaluate, and is a powerful tool for studying the representational power of bounded norm
ReLU networks.
5.1	Sobolev spaces
Here we relate Sobolev spaces and the R-norm. The key result is the following upper bound, which
is proved in Appendix G.
Proposition 3. If f : Rd → R is Lipschitz and (—∆)5+1"2f exists in a weak senseIIthen
kfkR ≤ cdγdk(-△产+1"2fk1.	(28)
where Cd = Rsd-ι dw = Γ2∏d/2)，and Yd = 2(2π∣d-1 ∙
Recall that if the dimension d is odd then (—∆)(d+1)∕2 is just an integer power of the negative
Laplacian, which is a linear combination of partial derivatives of order d + 1. Hence, we have
k(—∆)S"2f∣∣ι ≤ cd γd kf kW d+1,1 , where kf kW d+1,1 is the Sobolev norm given by the sum of
the L1-norm of f and the L1-norms of all its weak partial derivatives up to order d + 1. This gives
the following immediate corollary to Proposition 3:
Corollary 1. Suppose d is odd. If f belongs to the Sobolev space Wd+1,1(Rd), i.e., f and all its
weak derivatives up to order d + 1 are in L1(Rd), then kfkR is finite and kfkR ≤ cdγdkfkWd+1,1.
The (d + 1)-order smoothness in Corollary 1 is optimal in the sense that the R-norm cannot be
uniformly bounded by a L1-type Sobolev norm with smoothness of a lower order. To see this, let
Ds represent any s-order partial derivative, and let f be any compactly supported function such
that kDsfkι and IIfkR are finite. Then the contraction fε(x) := f(x∕ε) obeys the scaling law
kDsfεk1 = εd-skDsfk1and so kfεkWs,1 = O(εd-s) as ε → 0. Also, by Proposition 2 we have
kfεkR = ε-1kfkR. So the only way kfεkR could be uniformly bounded by kfε kW s,1 as ε → 0 is
ifs ≥ d+ 1.
The smoothness requirements in Corollary 1 are also necessary from a generalization perspective.
In particular, if the representational cost were bounded by L1-norm of derivatives of order strictly
less than d, we could not expect generalization from finite data. This is because the data could be fit
exactly with a sum of spike functions whose representational cost could be made arbitrarily small
by shrinking the support of the spikes.
Here it is also interesting to compare with (Bach, 2017, Proposition 5). There it is shown that the
space of all two-layer infinite-width ReLU networks defined by L2-bounded densities contains a
Sobolev space Ws,2 with s > (d + 1)/2. The s > d/2 scaling of smoothness is necessary when
considering an L2-type Sobolev norm, since by a similar argument to the above, it is the scaling that
ensures contractions fε have unbounded norm as ε → 0.
11 i.e., for all compactly supported smooth functions φ there exists a locally integrable function g ∈ LI)C(Rd)
such that R f (-∆)(d+1"2φdx = R gφdx.
8
Published as a conference paper at ICLR 2020
5.2	Radial bump functions
To help build intuition with the R-norm and illustrate its scaling with dimension, here we study the
case where f is a radially symmetric function, i.e., f(x) = g(kxk) for some function g : [0, ∞) →
R. In this case, the R-norm is expressible entirely in terms of derivatives of the radial profile function
g, as shown in the following result, which is proved in Appendix H.
Proposition 4. Suppose d ≥ 3 is odd. If f ∈ L1(Rd) with f(x) = g(kxk) then
2	∞	∞
kfkR = (d - 2)! J0 ∣∂(d+1)ρ(b)∣ db where P(b) ：= g(t)(t2 - b2产3)/2 tdt.	(29)
For example, in the d = 3 dimensional case, we have
kfkR = 2 Γ ∣b∂3g(b) + 3∂2g(b)∣db,	(d = 3)
0
More generally, for any odd dimension d ≥ 3 a simple induction shows (29) is equivalent to
kfkR =7Γ⅛Γ Γ ∣Qd{g}(b)∣db
(d - 2)! 0
(30)
(31)
where Qd is a differential operator of degree (d + 3)/2 having the form Qd = P(kd=+23)/2 pk,d(b)∂k
where each pk,d(b) is apolynomial in b of degree k-2. In particular, if the weak derivative d(d+1)/2g
exists and has bounded variation, then kf kR is finite.
Example 2.	Suppose d ≥ 3 is odd. Consider the radial bump function fd,k (x) = gd,k(kxk) with
x ∈ Rd where for any integer k > 0 we define
gd,k(r)
(1 - r2 )k if 0 ≤ r < 1
0	if r ≥ 1,
(32)
We prove kfd,k ∣∣r is finite if and only if k ≥ d++1 (see Appendix H). To illustrate the scaling with
dimension d, in Appendix H we also prove that for the choice kd = (d+1)/2+2 we have the bounds
(d + 5)d ≤ kfd,kd ∣r ≤ 2d(d + 5) ,hence kfd,kd IlR 〜d2. Similarly, by the dilation property (2), a
contraction of fdk to the ball ofradius ε has R-norm scaling as 〜d2 /ε.
The next example12 shows there there is a universal choice of radial bump function in all (odd)
dimensions with finite R-norm:
Example 3.	Suppose d ≥ 3 is odd. Consider the radial bump function f(x) = g(kxk) with x ∈ Rd
where
g(r)=[e- 1-1r2 if0 ≤r< 1	(33)
0	ifr ≥ 1.
Since g is C ∞ -smooth and its derivatives of all orders are L1-bounded, f has finite R-norm by
Proposition 4.
5.3	Piecewise Linear functions
Every finite-width two-layer ReLU network is a continuous piecewise linear function. However, the
reverse is not true. For example, in dimensions two and above no compactly supported piecewise
linear function is expressible as a finite-width two-layer ReLU network. A natural question then is:
what piecewise linear functions are represented by bounded norm infinite-width nets, i.e., have finite
R-norm? In particular, do compactly supported piecewise linear functions have finite R-norm? Here
we show this is generally not the case.
Proposition 5. Suppose f : Rd → R is a continuous piecewise linear function with compact
support, where the boundary sets between regions satisfy additional mild conditions. Then f has
infinite R-norm.
12The existence of such a radial function was noted in parallel work by Matus Telgarsky. Discussions with
Telgarsky motivated us to construct and analyze it using the R-norm.
9
Published as a conference paper at ICLR 2020
See Appendix I for proof and for the precise conditions needed on the boundary sets. Roughly
speaking, the result holds for any “generic” piecewise linear function with compact support, i.e., if
a function does not satisfy these conditions, then some small perturbation of the function does.
This result suggests that the space of piecewise linear functions expressible as a bounded norm
infinite-width two-layer ReLU network is not qualitatively different the space of finite-width net-
works. We go further and make the following conjecture:
Conjecture 1. A continuous piecewise linear function f has finite R-norm if and only if it is exactly
representable by a finite-width two-layer ReLU network.
5.4	Depth Separation
In an effort to understand the power of deeper networks, there has been much work showing how
some functions can be much more easily approximated in terms of number of required units by
deeper networks compared to shallower ones, including results showing how functions that can be
well-approximated by three-layer networks require a much larger number of units to approximate
if using a two-layer network (e.g. Pinkus (1999); Telgarsky (2016); Liang & Srikant (2016); Safran
& Shamir (2017); Yarotsky (2017)). The following example shows that, also in terms of the norm,
such a depth separation exists for ReLU nets:
Example 4. The pyramid function f(x) = [1 - kxk1]+ is a compactly supported piecewise linear
function satisfying the conditions needed for Proposition 5 to hold13, hence has infinite representa-
tional cost as a two-layer ReLU network (R(f) = Rι(f) = +∞), but can be exactly represented
as a finite-width three-layer ReLU network.
Interestingly, this result shows that, in terms of the norm, we have a qualitative rather then quantita-
tive depth separation: the minimal norm of the weights needed to represent the function with three
layers is finite, while with only two layers it is not merely very large, but infinite. In contrast, in
standard depth separation results, the separation is quantitative: we can compensate for a decrease
in depth and use more neurons to achieve the same approximation quality. It would be interesting
to further strengthen Example 4 by obtaining a quantitative lower bound on the norm required to
-approximate the pyramid with an infinite-width two-layer ReLU network.
5.5	THE R-NORM IS NOT A RKHS NORM
There is an ongoing debate in the community on whether neural network learning can be simulated
or replicated by kernel machines with the “right” kernel. In this context, it is interesting to ask
whether the inductive bias we uncover can be captured by a kernel, or in other words, whether the
R-norm is an RKHS-norm14 The answer is no:
Proposition 6. The R-norm is not an RKHS norm.
This is seen immediately by the failure of the parallelogram law to hold. For example,
if f1(x) = [w1>x]+, f2 = [w2>x]+ with w1, w2 ∈ Sd-1 distinct, then by Exam-
ple 1 we have kf1kR = kf2kR = 1, while kf1 + f2kR = kf1 - f2kR = 2, and so
2(kf1k2R+kf2k2R) 6= kf1 + f2k2R + kf1 - f2k2R.
5.6	Generalization implications
NeyshabUr et al. (2015) shows that training an unbounded-width neural network while regularizing
the '2 norm of the weights results in a sample complexity proportional to a variant15 of R(f). This
paper gives an explicit characterization of R(f) and thus of the sample complexity of learning a
function using regularized unbounded-width neural networks.
13More precisely, it satisfies condition (b) of Proposition 16 in Appendix I.
14To be precise, the R-norm is only a semi-norm on the space of functions for which it is finite, but it
becomes a norm on the quotient space obtained by modding out by all functions with zero R-norm, i.e., all
affine functions. The question is whether the R-norm on this quotient space is an RKHS norm.
15Their analysis does not allow for unregularized bias, but can be extended to allow for it.
10
Published as a conference paper at ICLR 2020
Acknowledgments
We are grateful to Matus Telgarsky (University of Illinois, Urbana-Champaign) for stimulating dis-
cussions, including discussing his yet unpublished work with us. In particular, Telgarsky helped us
refine our view of radial bumps and realize a fixed radial function can have finite norm in all dimen-
sions. We would also like to thank Guillaume Bal (University of Chicago) for helpful discussions
regarding the Radon transform, and Jason Altschuler (MIT) for pointers regarding convergence of
measures and Prokhorov’s Theorem. Some of the work was done while DS and NS were visiting
the Simons Institute for Theoretical Computer Science as participants in the Foundations of Deep
Learning Program. NS was partially supported by NSF awards 1764032 and 1546500. DS was
partially supported by the Israel Science Foundation (grant No. 31/1031), and by the Taub Foun-
dation. RW and GO were partially supported by AFOSR award FA9550-18-1-0166, DOE award
DE-AC02-06CH11357, and NSF awards OAC-1934637 and DMS-1930049.
References
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research,18(1):629-681, 2017.
Bolton Bailey, Ziwei Ji, Matus Telgarsky, and Ruicheng Xian. Approximation power of random
neural networks. arXiv preprint arXiv:1906.07709, 2019.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information Theory, 39(3):930-945, 1993.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning, 14(1):115-133, 1994.
Peter L Bartlett. For valid generalization the size of the weights is more important than the size of
the network. In Neural Information Processing Systems (NeurIPS), pp. 134-140, 1997.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Neural Information Processing Systems (NeurIPS), pp. 123-130, 2006.
Vladimir I Bogachev. Measure theory, volume 2. Springer Science & Business Media, 2007.
Jan Boman and FiliP Lindskog. Support theorems for the Radon transform and Cramer-Wold theo-
rems. Journal of theoretical probability, 22(3):683-710, 2009.
EmmanUel J Candes. Harmonic analysis of neural networks. Applied and Computational Harmonic
Analysis, 6(2):197-218, 1999.
Emmanuel J Candes and David L Donoho. Ridgelets: A key to higher-dimensional intermittency?
Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and
Engineering Sciences, 357(1760):2495-2509, 1999.
Sean M. Carroll and Bradley W. Dickinson. Construction of neural nets using the Radon transform.
In International Joint Conference on Neural Networks, volume 1, pp. 607-611, 1989. doi: 10.
1109/IJCNN.1989.118639.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Sigurdur Helgason. The Radon transform. Springer, 1999.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
11
Published as a conference paper at ICLR 2020
Yoshifusa Ito. Representation of functions by superpositions of a step or sigmoid function and their
applications to neural network theory. Neural Networks, 4(3):385-394,1991.
Ziwei Ji, Matus Telgarsky, and Ruicheng Xian. Neural tangent kernels, transportation mappings,
and universal approximation. arXiv preprint arXiv:1910.06956, 2019.
Shiyu Liang and R. Srikant. Why Deep Neural Networks for Function Approximation? In ICLR,
2016. URL http://arxiv.org/abs/1610.04161.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Paul Malliavin. Integration and probability, volume 157. Springer Science & Business Media, 2012.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Mor Shpigel Nacson, Suriya Gunasekar, Jason D Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. arXiv
preprint arXiv:1905.07325, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory (COLT), pp. 1376-1401, 2015.
Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta numerica, 8:
143-195, 1999.
Itay Safran and Ohad Shamir. Depth-Width Tradeoffs in Approximating Natural Functions with
Neural Networks. ICML, pp. 1-27, 2017. ISSN 1938-7228. URL http://arxiv.org/
abs/1610.09887.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded
norm networks look in function space? In Conference on Learning Theory (COLT), 2019.
Donald C Solmon. Asymptotic formulas for the dual Radon transform and applications. Mathema-
tische Zeitschrift, 195(3):321-343, 1987.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal
approximator. Applied and Computational Harmonic Analysis, 43(2):233-268, 2017.
Matus Telgarsky. Benefits of depth in neural networks. COLT, Feb 2016. URL http://arxiv.
org/abs/1602.04485.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets v.s. their induced kernel. arXiv preprint arXiv:1810.05369, 2019.
Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks,
94:103-114, 2017. ISSN 18792782. doi: 10.1016/j.neunet.2017.07.002.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
12
Published as a conference paper at ICLR 2020
(a) Radon transform	(b) Dual Radon transform
Figure 1: Radon transform. (a) Illustration of the Radon transform in equation (14) in dimension d = 2. The
red line of points x satisfying w>x = b defines the domain of the integral over f (x), where w determines the
line orientation (angle relative to the coordinate axes) and b determines its offset from the origin. (b) Illustration
of the support of the Radon transform for f (x) = δ(x - (-1, -1)) (red), f (x) = δ(x - (1, 0)) (green), and
f(x) = δ(x - (0, 1)) (blue). If a function f is a superposition of such δ functions, then R{f} is the sum of the
curves in (b); this is typically referred to as a “sinogram”. Furthermore, the dual Radon transform in equation
(15) integrates any function φ(w, b) over all curves like one of the three in (b).
Appendices
A Additional properties of the Radon Transform
Figure 1 illustrates the Radon transform and its dual in dimension d = 2.
We will often use the fact that the Radon transform is a bounded linear operator from L1 (Rd) to
L1(Sd-1 ×R),i.e.,iff ∈ L1(Rd)thenR{f} ∈ L1(Rd). In particular, if f ∈ L1(Rd)thenR{f}is
defined almost everywhere on Sd-I X R, and the function R{f }(w, ∙) is in L1 (R) for all W ∈ Sd-1.
Here we also recall the Fourier slice theorem for the Radon transform (see, e.g., Helgason (1999)):
Let f ∈ L1(Rd), then for all σ ∈ R and w ∈ Sd-1 we have
——一一， 、 ʌ, 、
Fb R{f }(w, σ) = f(σ ∙ w)	(34)
where Fb indicates the 1-D Fourier transform in the offset variable b. From this it is easy to establish
the following intertwining property of the Laplacian and the Radon transform: assuming f and ∆f
are in L1 (Rd), we have
R{∆f} = ∂b2R{f}	(35)
where ∂b is the partial derivative in the offset variable b. More generally for any positive integer s,
assuming f and (—∆)s/2f are in L1 (Rd) We have
R{(-∆)s/2 f } = (-∂b )s/2 R{f }	(36)
where fractional powers of -∂b2 can be defined in Fourier domain, same as fractional powers of
the Laplacian. In particular, if d is odd, (-∂b)(d+1)/2 = (-ι)(d+1)/2∂d+1, while if d is even,
(-∂b)(d+1)/2 = (Hdb)d+1 where H is the Hilbert transform in the offset variable b.
B Infinite-width Nets
Measures and infinite-width nets Let α be a signed measure 16 defined on Sd-1 × R, and let
IlakI = J d∣α∣ denote its total variation norm. We let M(SdT × R) denote the space of measures
on Sd-1 ×R with finite total variation norm. Since Sd-1 ×R is a locally compact space, M (Sd-1 ×R)
16To be precise, we assume α is a signed Radon measure; see, e.g., Malliavin (2012) for a formal definition.
We omit the word “Radon” and simply call α a measure to avoid confusion with the Radon transform, which
is central to this work.
13
Published as a conference paper at ICLR 2020
is the Banach space dual of C0(Sd-1 × R), the space of continuous functions on Sd-1 × R vanishing
at infinity (Malliavin, 2012, Chapter 2, Theorem 6.6), and
IlakI = sup I/ Wda :夕 ∈ Co(Sd-1 × R), k夕k∞ ≤ l} .	(37)
For any α ∈ M(SdT X R) and φ ∈ Co(Sd-1 X R), We often use hα,0 to denote R Wda.
Any a ∈ M (Sd-1 ×R) can be extended uniquely to a continuous linear functional on Cb(Sd-1 ×R),
the space continuous and bounded functions on Sd-1 XR. In particular, since the function W(w, b) =
[w>x - b]+ - [-b]+ belongs to Cb(Sd-1 X R), We see that the infinite-Width net
hα(x) :=	([w>x - b]+ - [-b]+)da(w, b)
Sd-1 ×R
(38)
is Well-defined for all x ∈ Rd .
Remark 1. Our definition ofan infinite-Width net in differs slightly from Savarese et al. (2019): We
integrate a constant shift of the ReLU [w>x - b]+ - [-b]+ With respect to the measure a(w, b)
rather than [w>x - b]+ as in Savarese et al. (2019). As shoWn above, this ensures the integral
is alWays Well-defined for any measure a With finite total variation. Alternatively, We could have
restricted to measures that have finite first moment, i.e., Sd-1 ×R |b| d|a|(w, b) < ∞, Which ensures
the definition ehα(x) := RSd-1×R[w>x - b]+da(w, b) proposed in Savarese et al. (2019) is alWays
Well-defined. HoWever, restricting to measures With finite first moment complicates the function
space description, and excludes from our analysis certain functions that are still naturally defined
as limits of bounded norm finite-Width netWorks, and so We opt for the definition above instead. In
the case that a has a finite first moment the difference betWeen definitions is immaterial since hα
and ha_are equal∏p to an additive constant, which implies they have the same representational cost
under R(∙) and Rι(∙).
Even and odd measures We say a ∈ M (Sd-1 X R) is even if
W(w, b)da(w, b) =	W(-w, -b)da(w, b) for all W ∈ C0 (Sd-1 X R)	(39)
or a is odd if
W(w, b)da(w, b) = -	W(-w, -b)da(w, b) for all W ∈ C0(Sd-1 X R). (40)
It is easy to show every measure a ∈ M (Sd-1 X R) is uniquely decomposable as a = a+ + a-
where a+ is even and a- is odd, which we call the even and odd decomposition of a. For example,
if α has a density μ(w, b) then a+ is the measure with density μ+(w, b) = 11 (μ(w, b)+μ(-w, -b))
and α- is the measure with density μ-(w, b) = 11 (μ(w, b) 一 μ(-w, -b)).
We let M(Pd) denote the subspace of all even measures in M (Sd-1 X R), which is the Banach
space dual of C0(Pd), the subspace of all even functions W ∈ C0(Sd-1 X R). Even measures play
an important role in our results because of the following observations.
Let a ∈ M (Sd-1 X R) with even and odd decomposition a = a+ + a-. Then we have hα =
hα+ + ha-. By the identity [t]+ + [-t]+ = |t| we can show
hα+ (x) = 2 /	(∣w>x + b∣-∣b∣)dα+(w,b).	(41)
Likewise, by the identity [t]+ 一 [-t]+ = t we have
hα- (x) = v0>x.	(42)
where v° = ɪ JSd-i×r Wda-(W,b). Hence, hα decomposes into a sum of a component with
absolute value activations anda linear function. In particular, iff = hα,v,c for some a ∈ M (Sd-1 X
R), v ∈ Rd, c ∈ R, letting a+ be the even part of a, we always have f = hα+ ,v0,c for some
v0 ∈ Rd . In other words, we lose no generality by restricting ourselves to infinite width nets of the
form f = hα,v,c where a is even (i.e., a ∈ M (Pd)).
We will need the following fact about even and odd decompositions of measures under the total
variation norm:
14
Published as a conference paper at ICLR 2020
Proposition 7. Let α ∈ M (Sd-1 × R) with α = α+ + α- where α+ is even and α- is odd. Then
kα+k1 ≤ kαk1 and kα-k1 ≤ kαk1.
Proof. For any 夕 ∈ Co(Sd-1 X R) We can write 夕 = 夕+ + 夕- where 夕+ (w, b) = 2 (φ(w, b) +
夕(一w, -b)) is even and 夕-(w,b) = 2(夕(w,b) 一 夕(一w, -b)) is odd. Note that / 夕dα+ =
J 夕+ dα+ since J φ-dα+ = 0. Furthermore, if 3(w, b)| ≤ 1 for all (w, b) ∈ SdT × R we see
that 3+(w,b)∣ ≤ 2(3(w,b)∣ + ∣g(-w,-b)|) ≤ 1 for all (w,b) ∈ SdT × R. Therefore, in
the dual definition of kα+ k1 given in (37) it suffices to take the supremum over all even functions
φ ∈ Co(Sd-1 × R). Hence,
IlakI	=	sup I/ Wda :夕 ∈ Co(Sd-1	× R), k夕k∞ ≤ l}	(43)
Pda-:夕 ∈ Co(Sd-1 X R),∣∣H∣∞	≤	l}	(44)
≥	sup	W da+ + W da- :	W ∈ C0(Sd-1 × R),	kWk∞	≤	1, W even	(45)
Wda+ : W ∈ C0(Sd-1 X R), kWk∞ ≤ 1, W even	(46)
= ka+k1	(47)
A similar argument shows ∣∣a-∣ι ≤ ∣∣a∣ι.	□
sup	W da+ +
Lipschitz continuity of infinite-width nets Define Lip(Rd) to be the space of all real-valued
LiPSchitz continuous functions on Rd. For any f ∈ Lip(Rd), define ∣∣f ∣l := supχ=y f (Xx-y：y)|,
i.e., the smallest possible Lipschitz constant. The following result shows that Lip(Rd) is a natural
sPace to work in when considering infinite-width nets:
Proposition 8 (Infinite-width nets are LiPschitz). Let f = hα,v,c for any a ∈ M (Sd-1 X R), v ∈
Rd, c ∈ R. Then f ∈ Lip(Rd) with ∣f∣L ≤ ∣a∣1 + ∣v∣.
Proof. First we prove for all even a ∈ M(Pd), ∣∣hα∣∣L ≤ ∣∣a∣∣ι∕2.
By the reverse triangle inequality we have |w>x 一 b| 一 |w>y 一 b| ≤ w>(x 一 y) for all x, y ∈
Rd, w ∈ Sd-1, b ∈ R. Therefore, using identity (41), for all x, y ∈ Rd we see that
Iw>x — bI — Iw>y — bI da(w, b)
Sd-1×R
(48)
Iha(X) - hα (y) | = $
≤ 1 [	∣∣w>x — b∣-∣w>y — b|| d∣a∣(w,b)	(49)
2	Sd-1×R
≤ 1 [	∣w>(x — y)∣d∣a∣(w,b)	(50)
2	Sd-1×R
≤ 1 kχ 一 ykkakι	(51)
which shows ha is Lipschitz with ∣∣hakL ≤ ∣∣a∣ι∕2.
More generally, for any infinite-width net f = hα,v,c with a ∈ M (Sd-1 X R), v ∈ Rd and
c ∈ R. From the even and odd decomposition a = a+ + a- we have f = ha+,v0+v,c, where
vo = 1 JSd-I ×r wda-(w, b). Hence, ∣vo∣∣2 ≤ IIa- ∣∣ι∕2, Therefore, by the triangle inequality,
IIfkL ≤ ∣∣a+ ∣ι∕2+ ∣∣a-∣ι∕2+ ∣∣v∣ ≤ ∣∣a∣ι + ∣∣v∣, which gives the claim.	□
C Optimization characterization of representational cost
Here we establish the optimization equivalents of the representational costs R(f) and R∖(f) given
in (9) and (13).
15
Published as a conference paper at ICLR 2020
As an intermediate step, We first give equivalent expressions for R(f) and Rι(f) in terms of Se-
quences finite-width two-layer ReLU networks converging pointwise to f . For this we need to
introduce some additional notation and definitions.
We let A(Sd-1 × R) denote the space of all measures given by a finite linear combination of Diracs,
i.e., all α ∈ M (Sd-1 × R) of the form α = Pik=1 aiδ(wi,bi) for some ai ∈ R, (wi, bi) ∈ Sd-1 × R,
i = 1, ..., k, Where δ(w,b) denotes a Dirac delta at location (w, b) ∈ Sd-1 × R. We call any
α ∈ A(Sd-1 × R) a discrete measure.
Note there is a one-to-one correspondence betWeen discrete measures and finite-Width tWo layer
ReLU nets (up to a bias term). Namely, for any θ ∈ Θ0 defining a finite-Width net gθ (x) =
Pik=1 ai[wi>x - bi]+ + c, setting α = Pik=1 aiδ(wi,bi) We have f = hα,c0 With c0 = gθ(0).
We write θ ∈ Θ0 什 α ∈ A(Sd-I X R) to indicate this correspondence. Furthermore, in this case
C(θ) = Pik=1 |ai| = kαk1.
We also recall some facts related to the convergence of sequences of measures. Let Cb(Sd-1 × R)
denote the set of all continuous and bounded functions on Sd-1 × R. A sequence of measures
{αn}, with αn ∈ M (Sd-1 × R) is said to converge narrowly to a measure α ∈ M (Sd-1 × R) if
/ 夕 dan → / φda for all 夕 ∈ Cb(Sd-I × R). Also, a sequence {an} is called tight if for all ε > 0
there exists a compact set K ⊂ SdT × R such that ∣an∣(KC) ≤ ε for all n sufficiently large. Every
narrowly convergent sequence of measures is tight (Malliavin, 2012, Theorem 6.8). Conversely, any
sequence {αn} that is tight and uniformly bounded in total variation norm has a narrowly convergent
subsequence; this is due to a version of Prokhorov’s Theorem for signed measures (Bogachev, 2007,
Theorem 8.6.2).
Now we establish the following equivalent expressions for the representational costs R(∙) and Ri (∙).
Lemma 4. For any f : Rd → R let fo denote the function fo(x) = f(x) 一 f (0). For R(f) as
defined in (7) and Ri(f) as defined in (12), we have
R(f) = inf l IimsuP ∣∣αnkι : an ∈ A(SdT × r), h@n → /°pointwise, {an} tight > .	(52)
n→∞
and
Ri(f) = inf l limsup IIankI : an ∈ A(SdTX r), Vn ∈ Rd, hαn,vn,0 → fo pointwise, {an} tight
n→∞
(53)
Proof. We prove the identity in (52) for R(f); the identity in (53) for Ri(f) follows by the same
argument. Define
Rε(f) ：= infz C (θ) s.t. ∣gθ (x) - f (x)∣≤ ε ∀∣∣x∣∣≤ 1∕ε and gθ (0)= f (0)	(54)
θ∈Θ0
so that R(f) = limε→o Rε(f). Also, let L(f) denote the right-hand side of (52).
First, suppose R(f) is finite. Let εn = 1∕n. Then by definition of R(f), for all n there exists
θn ∈ Θ0 such that C(θn) ≤ Rεn(f) + εn, while 血九(x) — f(x)∣ ≤ εn for ∣∣x∣ ≤ 1∕εn and
gθn(0) = f (0). Note that θn ∈ Θ0 什 an ∈ M(SdT X R) with gθn = hαn,c where C = gθn(0)=
f (0) and ∣∣ankι = C(θn). Hence, hαn(x) = gθn(X)- f (0), and we have %九(X)- fo(x)∣ =
∣gθn(x) - f(x)∣ ≤ εn for ∣∣x∣ ≤ ∖∕" Therefore, hα,n → fo pointwise, while
limsup ∣∣an∣ι ≤ IimsuP(Rεn(f) + εQ = R(f),	(55)
n→∞	n→∞
which shows L(f) ≤ R(f). Finally, it suffices to show {an} has a tight subsequence, since we
can reproduce the steps above with respect to the subsequence. Towards this end, define qn (X) =
>
|w X - b|d|an |(w, b), which is well-defined since an is discrete and has compact support. Then
qn is Lipschitz with ∣qn ∣L ≤ ∣an ∣1 ≤ B for some finite B, hence the sequence {qn} is uniformly
Lipschitz. By the Arzela-Ascoli Theorem, {qn} has a subsequence {qnk} that converges uniformly
on compact subsets. In particular, qnk (0) = |b|d|ank |(w, b) ≤ L < ∞ for some L, which implies
the sequence {ank } is tight.
16
Published as a conference paper at ICLR 2020
Conversely, suppose L(f) is finite. Fix any ε > 0. Then by definition ofL(f) there exists a sequence
αn ∈ M(SdT X R)什 θn ∈ Θ0 such that limn→∞ ∣∣αn∣∣ι exists with limn→∞ ∣∣αn∣∣ι < L(f) + ε,
while fn := hαn,c = gθn with c = f(0) converges to f pointwise and satisfies fn (0) = f(0)
for all n. Since, limn→∞ ∣αn ∣1 < L(f) + ε, there exists an N1 such that for all n ≥ N1 we
have ∣αn ∣1 ≤ L(f) + ε. By Proposition 8, the Lipschitz constant of fn is bounded above by
∣αn ∣1 for all n, hence the sequence fn is uniformly Lipschitz. This implies fn → f uniformly on
compact subsets, and so there exists an N such that IIfn(X) — f (x)∣ ≤ ε for all ∣∣x∣ ≤ 1∕ε and
fn(0) = f (0) for all n ≥ N2. For all n ≥ N, f satisfies the constraints in the definition of Rε(∙).
Therefore, for all n ≥ max{N1, N2} we have
Rε(f) ≤ C(θn) = ∣αn∣1 ≤ L(f) + ε.	(56)
Taking the limit as ε → 0, We get R(f) ≤ L(f). Therefore, We have shown R(f) is finite if and
only if L(f) is finite, in which case R(f) = L(f), giving the claim.	□
The following lemma shows every infinite-width net is the pointwise limit of a sequence of finite-
width nets defined in terms of sequence of measures uniformly bounded in total variation norm.
Lemma 5. Let f = hα,v,c for any α ∈ M (Sd-1 × R),v ∈ Rd, and c ∈ R. Then there exists a
sequence of discrete measures αn ∈ A(Sd-1 × R) with ∣αn ∣1 ≤ ∣α∣1 such that fn = hαn ,v,c
converges to f pointwise.
Proof. For any α ∈ M (Sd-1 × R) there exists a sequence of discrete measures αn converging
narrowly to α such that ∣αn∣1 ≤ ∣α∣1 (Malliavin, 2012, Chapter 2, Theorem 6.9). Let fn =
hαn,v,c. Since the function (w, b) 7→ [w>x - b]+ - [-b]+ is continuous and bounded, we have
fn (x) → f (x) for all X ∈ Rd, i.e., fn → f pointwise.	□
Lemma 6. We have the equivalences
R(f) =	min ∣∣α∣ι s.t. f = hα,c,	(57)
α∈M (Sd-1 ×R),c∈R
and
Rι(f) =	min	∣∣α∣ι s.t. f = hα,v,c	(58)
α∈M (Sd-1 ×R),v∈Rd,c∈R
Proof. We prove the R(f) case; the Rι(f) case follows by the same argument. Throughout the
proof we use the equivalence of R(f) given in Lemma 4, and let M(f) denote the right-hand side
of (57).
Assume R(f) is finite. Then there exists a tight sequence {ɑn}, an ∈ A(Sd-I × R) , that is uni-
formly bounded in total variation norm such that hαn → f0 pointwise. Therefore, by Prokhokov’s
Theorem, {αn} has a subsequence {αnk } converging narrowly to a measure α, hence f0 = hα.
Furthermore, narrow convergence implies ∣α∣1 ≤ lim supk→∞ ∣αnk ∣1 ≤ lim supn→∞ ∣αn∣1,
and so M(f) ≤ lim supn→∞ ∣αn∣1. Taking the infimum over all such sequences {αn}, we have
M(f) ≤ Rff).
Conversely, assume M(f) is finite. Let α ∈ M (Sd-1 × R) be any measure such that f0 = hα.
By Lemma 5 there exists a sequence {ɑn}, an ∈ A(Sd-I × R), such that han → fo pointwise,
while ∣∣an ∣∣ι ≤ ∣∣a∣ι. Hence, R(f) ≤ lim supn→∞ IIankI ≤ ∣∣a∣ 1. Since this holds for any a with
fo = hα, we see that R(f) ≤ M(f), proving the claim.	□
Now we show that if f is an infinite-width net, Ri (f) is equal to the minimal total variation norm of
all even measures defining f (in fact, later we show for every infinite-width net is defined in terms
of a unique even measure, whose total variation norm is equal to Ri(f); see Lemma 10).
Lemma 7. We have
Ri(f )= α+∈M (PminCRdMRg+11 S.t. f = hα+,v,c.
(59)
where the minimization is over all even a+ ∈ M(Pd).
17
Published as a conference paper at ICLR 2020
Proof. Suppose f = hα,v,c for some α ∈ M (Sd-1 × R), v ∈ Rd, c ∈ R. If α has even and odd
decomposition α = α+ + α- then f = hα+,0,0 + hα-,v,c = hα+,v0,c for some v0 ∈ Rd. Also, by
Proposition 7, We have ∣∣ɑ+∣∣ι ≤ ∣∣ɑ+ + α-kι = ∣∣ɑkι for any a- odd. Hence, the optimization
problem describing Ri (f) in (58) reduces to (59).	□
D EXTENSION OF R-NORM TO LIPSCHITZ FUNCTIONS AND PROOF OF THEOREM 1
To simplify notation We let S(Pd) denote the space of even SchWartz functions on Sd-1 ×R, i.e., ψ ∈
S(Pd) ifψ ∈ S(Sd-1 × R) With ψ(w, b) = ψ(-w, -b) for all (w,b) ∈ Sd-1 × R.
We Will need a finer characterization of the image of SchWartz functions under the dual Radon
transform than What is given in Lemma 9, Which is also due to Solmon (1987):
Lemma 8 (Solmon (1987), Theorem 7.7). Let ψ ∈ S(Pd) and define 夕=γd(-∆)(d-1"2R*{ψ}.
Then φ ∈ C∞(Rd) with 夕(x) = O(∣x∣-d) and △夕(x) = O(kxk-d-2) as IlxIl → ∞. Moreover,
R{2} = ψ.
Using the above result We shoW the functional ∣f ∣R given in Definition 1 is Well-defined:
Proposition 9. For any f ∈ LiP(Rd) ,the map Lf(ψ) := -γdhf, (—∆)(d+1"2R*{ψ}i isfinitefor
all ψ ∈ S(Pd), hence ∣f∣R = sup Lf (ψ) : ψ ∈ S(Pd), ∣ψ∣∞ ≤ 1 is a well-defined functional
taking on values in [0, +∞].
Proof. Since f is globally Lipschitz We have |f (x)| = O(∣x∣), While for any ψ ∈ S(Pd) We
have |(—∆)(d+1"2R*{ψ}∣ = O(kxk-d-2) by Lemma 8, hence |f (x)(—∆)(d+1"2R*{ψ}(x)∣ =
O(kxk-d-1) is absolutely integrable, and so hf, (-∆)(d+1"2R*{ψ}i is finite. If
hf, (—∆)(d+1"2R*{ψ}i = 0, We can choose the sign of ψ so that the inner product is positive,
which shows that ∣f ∣r ≥ 0.	□
In Section 4 we showed ∆hα = R*{α} when α was a measure with a smooth density having rapid
decay. The next key lemma shows this equality still holds in the sense of distributions when α is any
measure in M(Pd).
Lemma 9. Let f = hα,v,c for any α ∈ M(Pd), V ∈ Rd, c ∈ R. Then we have hf, ∆φ)=
hα,R{0i for all 夕 ∈ C∞(Rd) such that 夕(x) = O(kxk-d) and △夕(x) = O(kxk-d-2) as
∣x∣ → ∞.
Proof. Consider the ridge function rw,b(x) := 11 ∣w>x — b|, which is generated by the even measure
αo(w0,b0) = 2 (δ(w0 — w,b0 — b) + δ(w0 + w,b0 + b)). An easy calculation shows that ∆rw,b(χ)=
δ(w>x — b) in the sense of distributions, i.e., for all test functions 夕 ∈ S(Rd) we have
/ rw,b(x)△夕(x) dx
/
w>x=b
夕(x) ds(x) = R{q}(w, b).
(60)
Since R{q}(w, b) is well-defined for all 夕 ∈ C∞(Rd) with decay like O(kx∣-d), by continuity
△rw,b(x) extends uniquely to a distribution acting on this larger space of test functions.
Now consider the more general case of f = ha with a ∈ M(Pd). Then for all 夕 ∈ C∞(Rd) with
夕(x) = O(kxk-d) and △夕(x) = O(IlxlI-d-2) as ∣∣x∣ → ∞ we have
J f (x)△夕(x) dx
-1×R
Sd-1×R	Rd
Sd-1×R	Rd
2(∣
2(∣
w>x — b| 一 |b|) dα(w, b) I ∆g(x) dx
w>x — b| 一 ∣b∣)∆g(x) dx I dα(w, b)
rw,b(x)△2(x) dx ) dα(w, b)
(61)
(62)
(63)
I	R{0(w,b) dα(w,b)
Sd-1 ×R
(64)
18
Published as a conference paper at ICLR 2020
where in (62) we applied Fubini’s theorem to exchange the order of integration, whose application
is justified since
h∣α∣(x) = 2∕d]	(∣w>x - b| - |b|) d∣α∣(w,b)二||。|用罔|	(65)
and by assumption ∆g(x) = O(kxk-d-2), hence h∣ɑ∣(x)∣∆g(X)I =O(Ilxk)-d-1, and so
J h∣ɑ∣(x)∣△夕(x)I dx < ∞.
Finally, if f = hα,v,c for any α ∈ M(Pd), v ∈ Rd, c ∈ R, since affine functions vanish under
the Laplacian We have hf, △夕)=(hα, △夕i, reducing this to the previous case, which gives the
claim.	□
The following lemma shows kf kR is finite if and only if f is an infinite-width net, in which case
kf kR is given by the total variation norm of the unique even measure defining f .
Lemma 10. Let f ∈ Lip(Rd). Then kfkR is finite if and only if there exists a unique even measure
α ∈ M (Pd) and unique v ∈ Rd, c ∈ R with f = hα,v,c, in which case kfkR = kαk1.
Proof. Suppose kf kR is finite. Then by definition f belongs to Lip(Rd) and the linear functional
Lf(ψ) = -Ydhf, (-∆)Wτ"2R"{Ψ}i is continuous on S(Pd) with norm kf∣∣R. Since S(Pd) is a
dense subspace of Co(Pd), by continuity there exists a unique extension Lf to all of Co (Pd) with
the same norm. Hence, by the Riesz representation theorem, there is a unique measure α ∈ M(Pd)
such that Lf(ψ) = R ψda for all ψ ∈ Co(Pd) and ∣∣f ∣r = ∣∣α∣ι.
We now show f = hα,v,c for some V ∈ Rd, C ∈ R. First, we prove ∆f = ∆ha as tempered
distributions (i.e., as linear functionals on the space of Schwartz functions S(Rd)). By Lemma 9 we
have h∆hα,0 =〈a, R{0} for any 夕 ∈ S(Rd), hence
(△ha,0 = ha, R{ni	(66)
~ , _ , 、、
=Lf(R{0)	(67)
=Lf(R{0)	(68)
=Ydhf, (-∆)3+1"2R*{R{0}i	(69)
=-Ydhf, △(-∆)3τ"2R*{Rg}}i	(70)
=hf, △6	(71)
=(△/, 0	(72)
where in (68) we used the fact that R{φ} ∈ S(Pd) for all φ ∈ S(Rd) (Helga-
son, 1999, Theorem 2.4), and in (71) we used the inversion formula for Radon transform:
-Yd(-^)(d-1)/2R*{R{0}} = φ for all φ ∈ S(Rd) (Helgason, 1999, Theorem 3.1).
Hence, we have shown △f = △ha as tempered distributions. This means f 一 ha is in null space of
the Laplacian acting on tempered distributions, which implies f- hα = p where p is some harmonic
polynomial (i.e., pis a polynomial in x = (x1, ..., xd) such that △p(x) = 0 for all x ∈ Rd). Finally,
since both f and hα are Lipschitz they have at most linear growth at infinity, so must p. This implies
p must be an affine function p(x) = v>x + c, which shows f = hα,v,c as claimed.
Conversely, suppose f = hα,v,c for some α ∈ M (Pd), v ∈ Rd, c ∈ R. Let ψ ∈ S(Pd). By
Lemma 8, the function φ = -γd(-QW-DSRYS) is in C∞(Rd) with 0(x) = O(∣x∣-d),
△0(x) = O(∣x∣-d-2) as ∣x∣ → ∞, and ψ = R{0}. Hence, by Lemma 9 we have
Lf (ψ) = hf, △0i = hα, R{0}i = hα, ψi.	(73)
This shows
IIfkR = sup{hα, ψi : ψ ∈ S(Pd), kψk∞ ≤ 1}	(74)
=sup{ha, ψi : ψ ∈ CO(Pd), kΨk∞ ≤ 1}	(75)
= kαk1	(76)
19
Published as a conference paper at ICLR 2020
where the second to last equality holds since S(Rd) is a dense subspace of C0 (Rd), and the last
equality is by the dual characterization of the total variation norm.
Finally, to show uniqueness, suppose hα,v,c = hβ,v0 ,c0 for some other even β ∈ M(Pd), v0 ∈ Rd,
c0 ∈ R. Then the function hα,v,c - hβ,v0,c0 = hα-β,v-v0,c-c0 is identically zero, hence by the
argument above khα-β,v-v0,c-c0 kR = kα - βk1 = 0, which implies α = β. Therefore, hα,v,c =
hα,v0,c0, which also implies v0 = V and C = c0.	□
Note that Lemma 1 is essentially a corollary of the uniqueness in the preceding result; we give the
proof here for completeness.
ProofofLemma 1. Suppose Rι(f) is finite. Then by the optimization characterization in Lemma
7, we have f = hα,v,c for some even α ∈ M(Pd), v ∈ Rd, c ∈ Rd, and Rι(f) is the minimum
of kα+k1 over all even measures α+ ∈ M(Pd) and v0 ∈ Rd, c0 ∈ R such that f = hα+,v0,c0. By
Lemma 10, there is a unique even measure α+ ∈ M(Pd), v ∈ Rd, and c ∈ R such that f = hα+,v,c.
Hence, Rι(f) = ∣∣α+∣∣ι.	□
Now We give the proof of our main theorem, which shows IlfkR = Rι(f).
ProofofTheorem 1. Suppose Rι(f) is finite. By Lemma 1, Rι(f) = ∣∣α∣ι where α ∈ M(Pd) is
the unique even measure such that f = hα,v,c for some v ∈ Rd, c ∈ R. Furthermore, kfkR = kαk1
by Lemma 10. Hence, Rι(f) = ∣∣f ∣r. Conversely, if kf ∣∣r is finite, then by Lemma 10 we have
f = hα,v,c for a unique even measure α ∈ M(Pd), and again by Lemma 1, kfkR = kαk1 =
Rι(f). ' '	口
Proof of Proposition 1. The Radon transform is a bounded linear operator from L1(Rd) to
LI(SdT X R) (see, e.g., Boman & Lindskog (2009)). Hence, if ∆5+1"2f ∈ LI(Rd) then
R{∆5+1"2f} ∈ LI(Rd). Let α ∈ M(Pd) be the even measure on SdT × R with density
YdR{∆(d+1"2f}. Then ∣∣α∣ι = γdkR{∆(d+1"2f }∣ι, i.e., the total variation norm of a coincides
with the L1-norm of its density. Therefore, by definition of ∣f∣R we have
kf ∣R = sup{γdhf, ∆0+1"2R*{ψ}i : ψ ∈ S(Pd), kΨk∞ ≤ 1}	(77)
=sup{hγdR{∆0+1”2f}, ψi : ψ ∈ S(Pd), kΨk∞ ≤ 1}	(78)
= sup{hα,ψi : ψ ∈S (Pd), kΨk∞ ≤ 1}	(79)
=kαkι= γdkR{∆3+1"2f}kι∙	(80)
where we used the fact that the Schwartz class S(Pd) is dense in C0 (Pd) and the dual definition of
the total variation norm (37). If additionally f ∈ LI(Rd), we have R{∆(d+1"2f} = ∂d+1 R{f}
by the Fourier slice theorem, which gives kfl∣R = Ydkdd+1R{f }∣∣ι∙	□
E Proof of Theorem 2
We show how our results change WithoUt the addition of the unregularized linear unit v>X in (3).
Specifically, we want to characterize R(f) given in (7) (or equivalently its optimization formulation
(9)). Unlike in the univariate setting, R(f) does not have a simple closed form expression in higher
dimensions. However, for any f ∈ Lip(Rd) we prove the bounds
max{kf Ir , 2∣Nf(∞)∣∣}≤ R(f) ≤ kf Ir + 2∣Nf (∞)k	(81)
where the vector Vf (∞) ∈ Rd can be thought of as the gradient of the function f “at infinity”; see
below for a formal definition. In particular, if f (x) vanishes at infinity then Vf (∞) = 0 and we
have R(f ) = kf Ir = Rι(f).
20
Published as a conference paper at ICLR 2020
For any f ∈ Lip(Rd), define Vf (∞) ∈ Rd by17
Vf (∞) := lim —1-ι，	Vf (x) ds(x),	(82)
r→∞ cdrd-1 kxk=r
where cd = Sd-1 dw. We will relate Vf (∞) to the “linear part” of an infinite-width net. Towards
this end, define V : M (Sd-1 × R) → Rd to be the linear operator given by
V(α) = ɪ J	W da(w,b).	(83)
Note that if α = α+ + α- where α+ is even and α- is odd, then V(α) = V(α-) since
Sd-1 ×R w dα+(w, b) = 0. In particular, if we set v0 = V(α-), then hα- (x) = v0>x.
Lemma 11. Suppose f = hα,c for any α ∈ M (Sd-1 × R), c ∈ R. Then, Vf (∞) = V (α).
Proof. A simple calculation shows the weak gradient of f = hα,c is given by
Vf(x) =
Sd-1×R
H(w>x - b)w dα(w, b)
(84)
where H is defined as H(t) = 1 if t ≥ 0 and H(t) = 0 if t < 0 otherwise. Therefore, we have
lim J1 V	Vf (x) ds(x) = lim
r→∞ rd-1 kxk=r
H (rw>w0 - b)w dw0dα(w, b)
dα(w, b)
Finally, dividing both sides by cd = Sd-1 dw gives the result.
(85)
(86)
(87)
□
Lemma 12. If f (x) = v>x + C then R(f) = 2∣∣vok.
Proof. Note that f = hα,c only if α is odd and V(α) = v0. Hence, we have
R(f) = min ∣∣αkι s.t. V(α) = vo
α odd
(88)
The adjoint V * : Rd → Cb(SdT X R) is given by [V *y](w, b) = 2 wτy. Therefore, the dual of the
convex program above is given by
max	v0τ y = max v0τy = 2∣v0 ∣	(89)
y∈Rd	0	kyk≤2 0
kV*yk∞≤ι
where We used the fact that ∣V*y∣∣∞ = maXw∈sd-ι 2IIwTyk ≤ 1 holds if and only if ∣∣y∣ ≤ 2.
This means 2∣vo∣ is a lower bound for R(f). Since this bound is reached with the primal feasible
choice α defined by
α(w,b) = kv0k (δ w- -就।，b)-δ Iw+高，b
(90)
we have R(f) = 2∣vo∣ as claimed.
Now we give the proof of Theorem 2.
17Note every Lipschitz function has a weak gradient Vf ∈ L∞ (Rd), so Vf (∞) is well-defined.
□
21
Published as a conference paper at ICLR 2020
ProofofTheorem 2. Suppose IlfkR is finite. Set v° = Vf (∞). Then by Lemma 10, there is a
unique even measure α+ such that f = hα+,v0,c for some unique v0 ∈ Rd, c ∈ R, with kfkR =
∣∣α+ k ι. Therefore, R(f) is equivalent to the optimization problem
R(f) = min ∣∣α+ + α-∣ι s.t. V(α-) = vo	(91)
α- odd
Since ∣∣α+ + α-∣ι ≤ ∣∣α+∣ι + ∣∣α-∣ι, by Lemma 12 We see that R(f) ≤ ∣∣α+∣ι + 2∣vo∣.
Now we show the lower bound. By Proposition 7 we have∣α+ + α-∣1 ≥ ∣α+∣1 = ∣f∣R, Which
givesR(f) ≥ ∣∣f ∣R. By Proposition 7 We also have ∣α+ +α- ∣1 ≥ ∣α- ∣1. By the proof of Lemma
12, the minimum of ∣α- ∣1 over all odd α- such that V(α-) = v0 is given by 2∣v0∣ = 2∣Vf (∞)∣.
Therefore, R(f) ≥ max{∣f ∣r , 2∣Vf (∞)∣}, as claimed.
□
Finally, We shoW there are examples Where the upper bound in Theorem 2 is attained.
Proposition 10. There exist f : Rd → R in all dimensions d such that
R(f) = ∣f ∣R + 2∣IVf (∞)∣∣.	(92)
Proof.	Let	w+,	w- ∈	Sd-1 be orthogonal. Consider f = hα defined by	α	=	α+ +	α-	With
α+ = δ(w - w+ , b) + δ(w + w+ , b)	(93)
α- = δ(w - w-, b) - δ(w + w-, b)	(94)
Hence,	f(x)	=	|w+>x|	+ w->x (e.g., in 2-D one such function is f(x, y)	=	x	+ |y|).	Replacing the
TV-norm with its dual definition, the dual problem for R(f) in this instance is given by:
sup w>y + (α+,0	(95)
φ∈Co (Sd-1×R),y∈Rd
kV*y+wk∞≤ι
Set y* = 2w-, and let 夕* be a continuous approximation to sign(α+) whose support is localized
to an arbitrarily small neighborhood of ±(w+, 0). Then the pair (夕*, y*) is dual feasible since
ψ(w, b) := [V*y*](w, b) + 夕*(w, b) = w>w- + 夕*(w, b)
and so ∣ψ(w, b)| ≤ 1. For these choices of (夕*, y*) the dual objective is 2∣w-∣ + ∣f ∣r, which gives
a lower bound on R(f). But this is also an upper bound on R(f) hence R(f) = ∣∣f∣∣R + 2∣w-∣.
Since Vf (∞) = w-, the result follows.	□
F PROPERTIES OF THE R-NORM
Here we prove the properties of R-norm discuseed in Section 4.1, including Proposition 2.
Proposition 11. The R-norm has the following properties:
•	(1-homogeneity and triangle inequality) If ∣∣f ∣r , ∣∣g∣∣R < ∞, then ∣∣c ∙ f ∣r = ∣c∣∣∣f ∣∣r
for all c ∈ R and ∣f + g ∣R ≤ ∣f ∣R+∣g∣R, i.e., ∣∣∙∣∣r is a semi-norm.
•	(Annihilation of affine functions) ∣f∣R = 0 if and only iff is affine, i.e., f(x) = v>x + c
for some v ∈ Rd, c ∈ R.
•	(Translation and rotation invariance) Ifg(x) = f(Ux + y) where y ∈ Rd and U ∈ Rd×d
is any orthogonal matrix, then ∣g∣R = ∣f ∣R.
• (Scaling with dilations/Contractions) Suppose ∣∣f ∣r < ∞. Let fε(x) := f (x∕ε), then
∣fε∣R = ε-1 ∣f∣R.
1	if w = ±w+ and b = 0
w>w-	else
22
Published as a conference paper at ICLR 2020
Proof. The 1-homogenity and triangle inequality properties follow immediate from the linearity of
all operations and the definition by way of a set supremum.
Clearly kfkR = 0 if f is affine. Conversely, suppose kfkR = 0 then by the uniqueness in Lemma
10, we have α = 0, and so f = h0,v,c for some v ∈ Rd and c ∈ R, hence f is affine.
For simplicity we demonstrate proofs of the remaining properties under the same condi-
tions of Proposition 1, i.e., d odd, and where f, ∆(d+1)/2f ∈ LI(Rd) so that IIfkR =
YdkR{∆3+1"2f}kι = Ydkdd+1R{f}k1 < ∞. The general case follows from standard duality
arguments.
To show translation invariance, define f(y)(x) := f(x - y). Then since ∆ commutes with transla-
tions we have ∆(d+1"2f(y) = [∆(d+1"2f](y). Also, for any function g we see that
Therefore,
R{g(y)}(w, b) = R{g}(w, b + w>y),	(96)
∣∣f(y)∣∣R = Z	∣R{∆3+1”2f(y)}(w,b)∣ dw db Sd-1 ×R	(97)
=f	∣R{∆(d+1"2f}(w,b + w>y)∣ dwdb Sd-1 ×R	(98)
=Z	∣R{∆3+1"2f}(w,b)∣ dw db = kf Ir . Sd-1 ×R	(99)
To show rotation invariance, let fU (x) = f(Ux) where U is any orthogonal d × d matrix.
Then, using the fact that the Laplacian commutes with rotations, We have ∆(d+1"2fu(x) =
∆(d+1"2f (Ux), and since R{gu}(w, b) = R{g}(Uw, b), We see that R{∆(d+1"2fu}(w, b)=
R{∆W+1"2f }(Uw, b), and so
kfU kR = kfkR.		(100)
To show the scaling under contractions/dilations (i.e., Proposition 2), let fε(x) = f (x∕ε) for ε > 0. Then		
	R{fε}(w,b)= /	f(x∕ε)ds(x) w>x=b	(101)
	=εd-1 /	f(X)ds(x) W w>X=b∕ε	(102)
Hence, we have	= εd-1R{f}(w, b∕ε).	(103)
	∣∂d+1R{fε}(w, b)| = εd-1ε-d-1∣∂d+1R{f }(w, b∕ε)∣	(104)
and so	=ε-2∣∂d+1R{f }(w,b∕ε)∣	(105)
Z Sd-1×R	∣∂d+1R{fε}(w,b)∣ dw db = ε-2 /	∣∂d+1R{f }(w,b∕ε)∣ dw db Sd-1 ×R	(106)
		
	=ε-1 /	∣∂d+1R{f }(w, b)| dw db Sd-1 ×R	(107)
	= ε-1 kfkR.	(108) □
Fourier estimates For any Lipschitz function f we can always interpret ∆f in a distributional
sense. An interesting special case is when ∆f is a distribution of order zero, i.e., when there exists
a constant C such that ∣(∆f,夕)| ≤ C k 夕 k ∞ for all smooth compactly supported functions 夕 so that
∆f extends uniquely to a measure having finite total variation. In this case, the Fourier transform
23
Published as a conference paper at ICLR 2020
of ∆f, defined as ∆cf (ξ) := h∆f, e-j2πx ξi for all ξ ∈ Rd, is a continuous and bounded function,
and we can make use of an extension of the Fourier slice theorem to Radon transforms of measures
(see, e.g., Boman & Lindskog (2009)) to analyze properties of kf kR. In particular, the following
result shows that in order for the kf kR to be finite, the Fourier transform of ∆f (or the Fourier
transform of f if it exists classically) must decay at a dimensionally dependent rate.
Proposition 12. Suppose ∆f is a distribution oforder zero. Then kf∣∣R Isfiniteonlyif ∆f (σ ∙ W)=
O(∣σ∣-(dτ)) as ∣σ∣ → ∞ forall W ∈ SdT. IfadditionaUy f ∈ LI(Rd) ,then kf∣∣R is finite only if
f(σ ∙ W) = O(∣σ∣-(d+1)) as ∣σ∣ → ∞ forall W ∈ SdT.
Proof. If ∆f ∈ M(Rd) is a finite measure then its Radon transform R{∆f} ∈ M (Pd) exists as a
finite measure, i.e., We can define R{∆f} via duality as hR{∆f},夕i = (∆f, R*{夕}〉for all φ ∈
Co(Rd) (see, e.g., Boman & Lindskog (2θ09)). Additionally, the restriction R{∆f }(w, ∙) ∈ M(R)
is a Well-defined finite measure for all W ∈ Sd-1, and its 1-D Fourier transform in the b variable is
given by
FbR{∆f}(w,σ) = ∆f (σ ∙ W) for all W ∈ Sd-1,σ ∈ R.	(109)
By Lemma 10, kf ∣∣r is finite if and only if the functional Lf (ψ) = -Ydhf, (-∆)(d+1)/2R*{ψ})
defined for all ψ ∈ S(Pd) extends to a unique measure α ∈ M(Pd). We compute the Fourier
transform of α in the b variable via duality: for all 夕 ∈ S(Pd) we have
hFbα,心二	二 ha, FbΨi	(110)
	=-Ydhf, (-∆)3+1"2R*{Fb6)	(111)
	= Ydh∆f, (-∆)(d-1"2R*{Fb0i	(112)
	= Ydh∆f, R*{(-∂2)(d-1"2Fb0i	(113)
	= Ydh∆f, R*{Fb(∣σ∣d-%)})	(114)
	= YdhR{∆f}, Fb(∣σ∣d-%))	(115)
	= YdhFbR{∆f},∣σ∣d-10	(116)
	= Ydh∣σ∣d-1FbR{∆f},0	(117)
This shows Fba = γd∣σ∣d-1FfcR{∆f} in the sense of distributions. Since FbR{∆f} is defined
pointwise for all (W, b) ∈ Sd-1 × R so is Fbα and we have
Fba(W,σ) = Yd∣σ∣d-1F6R{∆f }(w, σ) = γd∣σ∣d-1∆f(σ ∙ w).	(118)
Finally, since α is a finite measure, we know kFbαk∞ ≤ kαk1 = O(1), which gives the first result.
If additionally f ∈ LI(Rd) then we have ∆f(ξ) = ∣∣ξ∣∣2f (ξ), and so (Fba)(W, b) = ∣σ∣d+1f (σ ∙w)
which gives the second result.	□
G Upper and Lower bounds
Here we prove several upper and lower bounds for the R-norm. Proposition 3 is an immediate
corollary of the following upper bound:
Proposition 13. If (—∆)5+1"2f is a finite measure, then
kfkR ≤ Ydcdk(-△产+1”2fkι,	(119)
In particular, if (—△)(d+1)/2f exists in a weak sense then ∣∣ ∙ kι can be interpreted as the L1 -norm.
Proof. Straight from definitions we have
kf ∣R = SUp {γdhf, (-∆)3+1”2R*{ψ}i	: ψ ∈ S (Pd), kΨk∞ ≤	1}	(120)
=SUp {γdh(-∆)3+1"2f, R*{ψ}i	： ψ ∈ S(Pd), kΨk∞ ≤	1}	(121)
≤ sup {γdh(-∆)3+1”2f, 0 :中 ∈	Co(Rd), ∣∣T∞ ≤ cd}	(122)
=Ydcdk (-∆)3+1"2fkι	(123)
24
Published as a conference paper at ICLR 2020
where We used the fact that R*{^} ∈ Co(Rd) for 夕 ∈ S(Pd) (Solmon,1987, Corollary 3.6) and We
have ∣∣R*{夕}k∞ ≤ Cd for all 夕 ∈ S(Pd) such that k夕k∞ ≤ 1 since
∣R*2}(χ)l ≤ / 3(w, w>x)∣ dw ≤ / dw = Cd.
(124)
□
The folloWing result also gives a useful loWer bound on the R-norm.
Proposition 14. If f ∈ Lip(Rd) then
kf kR ≥ sup {hf, ∆0 ：夕 ∈ S (Rd), ∣∣R{0∣∣∞ ≤ 1} .	(125)
Proof. Let SH(Pd) ⊂ S(Pd) denote the image ofS(Rd) under the Radon transform. Then
kf kR	= sup	{γdhf, (-△产+1”2R*{ψ}i : ψ	∈	S (Pd), kΨk∞ ≤ 1}	(126)
≥ sup	{γdhf, (-∆)3+1”2R*{ψ}i : ψ	∈	SH(Pd), kΨk∞ ≤ l}	(127)
=sup {γdhf, (-∆)3+1"2R*{R{0}i :。∈ S(Rd), ∣∣R{0k∞ ≤ l}	(128)
=sup	{hf, ∆0 :q ∈ S(Rd), kR{0k∞	≤ 1}	(129)
Where in the last step We used the inversion formula:	φ	= γd(-∆)3τ"2R*{R{0} for all ψ ∈
S (Rd).	□
Further simplifying the loWer bound above gives the folloWing.
Proposition 15. If f ∈ Lip(Rd) then
kf kR ≥ sup {hf, ∆0 :中 ∈S (Rd),kTι ≤ 1} .	(130)
In particular, if △f exists in a weak sense then kfkR ≥ k∆fk∞ ∙
Proof. If ∣W∣1 = R 3(x)1 dx ≤ 1 then clearly ∣R{0(w,b)∣ = | Rw>x=夕(x)ds(x)∣ ≤
Jw>x=b 3(x)1 ds(x) ≤ 1. Hence ∣3∣ι ≤ 1 implies IlR{夕}k∞ ≤ 1. Combining this with the
previous proposition gives the first bound. Additionally, by the dual definition of the L∞ norm, and
since S(Rd) is dense in LI(Rd), the second bound follows.	□
H Radial Bump Functions
Proof of Proposition 4. Assume f ∈ L1 (Rd) so that its Radon transform R{f} is well-
defined, and for simplicity assume d is odd. Note that for a radially symmetric function we have
R{f}(w, b) = ρ(b) for some even function ρ ∈ L1(R), i.e., the Radon transform of a radially
symmetric function does not depend on the unit direction w ∈ Sd-1. Supposing ∂(d+1)ρ(b) exists
either as a function or a measure, we have
kf Ir = Ydkdd+1R{f}kι = YdCd Z ∣∂d+1ρ(b)∣db,	(131)
where Cd = Sd-1 dw
2nd/2
Γ(d∕2).
Now we derive an expression for ρ(b) in terms of g. First, since ρ(b) = R{f}(w, b) for any
w ∈ Sd-1, we can choose w = e1 = (1, 0, ..., 0), which gives
ρ(b) = R{f }(eι,b) = [	g(kxk)dx2 …dxd = [	g(pb2 + ∣∣X∣∣2)dx
x1=b	Rd-1
where we have set X = (χ2,…，xd). Changing to polar coordinates over Rd-1, we have
ρ(b) = [ g(pb2 + kXk2 )dX = Cd-1 [ g(pb2 + r2 )rd-2dr.
Rd-1	0
(132)
(133)
25
Published as a conference paper at ICLR 2020
By the change of variables t2 = b2 + r2, t > 0, we have
∞
b
ρ(b)
cd-1
g(t)(t2 - b2)(d-3)/2t dt.
(134)
Hence, we see that
kfkR=(d⅛ hd+1)DΓ g(t)(t2- b2 产 3"2tdM
(135)
where we used the fact that γdcdcd-1
1
(d-2)!.
Calculations in Example 2. Let f(x) = gd,k(kxk) with x ∈ Rd where
gd,k(r)
(1 - r2)k
0
if 0 ≤ r < 1
if r ≥ 1.
(136)
for any k > 0. Then a straightforward calculation using (134) gives
ρ(b) = {Cd,k(I-卬-
if |b| < 1
if b ≥ 1.
(137)
∖x∕hαrα r( ∣ ɪ — r((d 3)∕2)T(1+k) ‰Γp∏r'f1 ∖x∕α hcnro Il fllC finifp if CITlrI γyπ"I∖7 if AdC(卜、hoc heiinrlɑrl
Where Cd k —	2Γ((d+i)/2)∣ k) . -∏-ence, We have 11 f kR unite If and Only If ∂b ρ(b) has bounded
variation, which is true if and only if k - d + d-1 ≥ 0, or equivalently, k ≥ d∣1. For example, if
d — 3 then We need k ≥ 2in order for kf kR to be finite, consistent With the previous example.
To illustrate scaling of kfkR with dimension d, we set k — (d + 1)/2+ 2— (d + 5)/2 so that
ρ(b) — Cd,(d+5)∕2(l - b2)d+2 for |b| ≤ 1 and ρ(b) — 0 otherwise. Then we can show that
∣∂d+1ρ(b)∣ ≤ ∣∂d+1ρ(0)∣ for |b| ≤ 1 and ∂d+1 ρ(b) — 0 for all |b| ≥ 1. Therefore,
l1	2
kfkR = ELyd+1p(b)l≤ EIdd+1P(0)1	(138)
Performing a binomial expansion of ρ(b) and taking derivatives, we obtain
7-⅛!∣∂d+1ρ(0)∣ — 2Cd,(d+5)∕2 ( d+2 ) (d +1)d(d - 1) — 2d(d +5)	(139)
(d - 2)!	(d + 1)/2
for all odd d ≥ 3. By the lower bound in Proposition 15, we also have kf kR ≥ k∆fk∞ —
∣∆f(0)I — d(d + 5). Hence kf ∣∣r 〜d2.
I Piecewise Linear Functions
Here we state and prove a more formal version of Proposition 5. Before stating our result, we
will need a few definitions relating to the geometry of piecewise linear functions. Recall that any
piecewise linear function (with finitely many pieces) is divided into polyhedral regions separated
by a finite number of boundaries. Each boundary is (d - 1)-dimensional and contained in a unique
hyperplane. Hence, with every boundary we can associate the unique (up to sign) unit normal to the
hyperplane containing it, which we call the boundary normal. Additionally, in the case of compactly
supported piecewise linear functions, every boundary set that touches the complement of the support
set we call an outer boundary, otherwise we call it an inner boundary.
Proposition 16. Suppose f : Rd → R is a continuous piecewise linear function with compact
support such that one (or both) of the following conditions hold:
(a)	at least one of the boundary normals is not parallel with every other boundary normal, or
(b)	f is everywhere convex (or everywhere concave) when restricted to its support, and at least
one of the inner boundary normals is not parallel with all outer boundary normals.
Then f has infinite R-norm.
26
Published as a conference paper at ICLR 2020
Proof. Assume f is a continuous piecewise linear function with compact support satisfying assump-
tion (a) or (b). Let B1, ..., Bn denote the boundaries between the regions. Since f is piecewise linear
and continuous, the distributional Laplacian ∆f decomposes into a linear combination of Dirac mea-
sures supported on the d -
have
1 dimensional boundary sets Bk, i.e., for all smooth test functions φ We
n
h∆f,0 = Eck / ψ(x) ds(x).
k=1	Bk
(140)
for some non-zero coefficients ck ∈ R, Where ds indicates integration With respect to the d - 1
dimensional surface measure on Bk. In particular, if Bk is the boundary separating neighboring
regions Rp and Rq, then ck = ±kgp - gq k Where gp and gq are the gradient vectors of f in the
region Rp and Rq, respectively, With sign determined by Whether the function is locally concave (+)
or convex (-) at the boundary. Note that ∆f is a distribution of order zero, i.e., it can be identified
With a measure having finite total variation, and it has a Well-defined Fourier transform given by
n
∆cf (ξ) = ck
k=1
/
Bk
e-i2πξ>x ds(x).
(141)
We shoW that ∆f(ξ) violates the necessary decay requirements of Proposition 12 in order for f to
have finite R-norm. In particular, We shoW under both conditions (a) and (b) there exists a w such
that ∆f (σ ∙ W) is asymptotically constant as ∣σ∣ → ∞, which gives the claim.
For all k = 1, ..., n, let wk denote a boundary normal to the boundary Bk (i.e., a vector wk ∈ Sd-1
such that wk>x = 0 for all x ∈ Bk, which is unique up to sign).
We first prove the claim under condition (a). Suppose, without loss of generality, that the boundary
normal w1 is not parallel with all the others, i.e., w1 6= wk for all k = 2, ..., n. We will write
-λ". , _ , , _ ,.
∆f(σ ∙ wι) = Fι(σ) + F2(σ)
(142)
where F1(σ) = c1 RB e-i2πσw1>xds(x) and F2(σ) = Pkn=2 ck RB e-i2πσw1>xds(x), and give
decay estimates for F1 and F2 separately.
First, consider F1(σ). Since w1>x = 0 for all x ∈ B1 we have
F1(σ) =
B
e-i2πσw1>xds(x)
B1
ds(x)
s(B1),
(143)
where s(B1) is the (d - 1)-dimensional surface measure of B1. In particular F(σ) is a non-zero
constant for all σ ∈ R.
Now consider F2(σ). In this case, the integrand of RB e-i2πσw1>xds(x) for all k = 2, ..., n is
not constant, since by assumption w1 not parallel with any of the boundary normals w2, ..., wn.
By an orthogonal change of coordinates, we can rewrite the surface integral over Bk as a vol-
ume integral over a set Bk embedded in (d - 1)-dimensional space X = (xι,…，xd-ι), so that
RBk e-i2πσw>xds(χ) = RBk e-i2πσw>χdχ for some for some non-zero W1 ∈ Rd-1. Observe
that g(X) := — i2∏w1W] k e-i2πσw>χ has divergence V ∙ g(X) = e-i2πσw>x. Therefore, by the
divergence theorem we have
[e-i2πσw>xdX= [ V∙ g(X)dX
JBk	JBk
=Φ	g(xc)τ n(xc)ds(xc)
JdBk
—
i2∏σ商工 ke-i2πσw>x w>n(χ)ds(x)
where n(X) is the outward unit normal to the boundary ∂Bk. This gives the estimate
/_ e-i2πσw>xdχ
Bk
O(1∕σ), ∣σ∣→∞,
(144)
(145)
(146)
(147)
27
Published as a conference paper at ICLR 2020
which holds for any k = 2,…，n. Therefore, F2(σ) = Pn=2ck RBk e-i2πσw>x ds(x) = O(1∕σ)
as ∣σ∣ → ∞. This shows that ∆f (σ ∙ wι) → c1s(B1), i.e., ∆f (σ ∙ wι) is asymptotically constant,
which proves the claim.
Now we prove the claim under condition (b). Without loss of generality, let w1 be an inner boundary
normal that is not parallel with any outer boundary normal, and assume f is concave when restricted
to its support. Let I1 be the indices of all inner boundary normals parallel with w1 (including itself),
let I2 be the indices of all inner boundary normals that are not parallel with w1 , and let O be the
indices of all outer boundary normals. Then we write
-λ". ,	_ , ,	_ , ,	_ ,.
∆f(σ ∙ wι) = Fiι (σ) + F2(σ) + FO(σ)	(148)
where FI1 (σ) = Pk∈I1 ck RBk e-i2πσw1>xds(x), FI2 (σ) = Pk∈I2 ck RBk e-i2πσw1>xds(x), and
FO (σ) = k∈O ck B e-i2πσw1>xds(x). By the same argument as above, we can show FI1 (σ) =
Pk∈I ck s(Bk ). Since the function is concave when restricted to its support, all of the ck with
k ∈ I1 are positive, hence the sum Pk∈I ck s(Bk ) is non-zero, which shows FI1 (σ) is a non-
zero constant for all σ ∈ R. Likewise, by the same argument as above, we can show FI1 (σ) =
O(1∕σ) and FO(σ) = O(1∕σ). Therefore, ∆f (σ ∙ wι) is asymptotically constant, which proves the
claim.	□
28