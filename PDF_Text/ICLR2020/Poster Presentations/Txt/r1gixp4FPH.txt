Published as a conference paper at ICLR 2020
Accelerating SGD with momentum for over-
PARAMETERIZED LEARNING
Chaoyue Liu
Department of Computer Science
The Ohio State University
Columbus, OH 43210
liu.2656@osu.edu
Mikhail Belkin
Department of Computer Science
The Ohio State University
Columbus, OH 43210
mbelkin@cse.ohio-state.edu
Ab stract
Nesterov SGD is widely used for training modern neural networks and other ma-
chine learning models. Yet, its advantages over SGD have not been theoretically
clarified. Indeed, as we show in this paper, both theoretically and empirically,
Nesterov SGD with any parameter selection does not in general provide acceler-
ation over ordinary SGD. Furthermore, Nesterov SGD may diverge for step sizes
that ensure convergence of ordinary SGD. This is in contrast to the classical re-
sults in the deterministic setting, where the same step size ensures accelerated
convergence of the Nesterov’s method over optimal gradient descent.
To address the non-acceleration issue, we introduce a compensation term to Nes-
terov SGD. The resulting algorithm, which we call MaSS, converges for same step
sizes as SGD. We prove that MaSS obtains an accelerated convergence rates over
SGD for any mini-batch size in the linear setting. For full batch, the convergence
rate of MaSS matches the well-known accelerated rate of the Nesterov’s method.
We also analyze the practically important question of the dependence of the con-
vergence rate and optimal hyper-parameters on the mini-batch size, demonstrating
three distinct regimes: linear scaling, diminishing returns and saturation.
Experimental evaluation of MaSS for several standard architectures of deep net-
works, including ResNet and convolutional networks, shows improved perfor-
mance over SGD, SGD+Nesterov and Adam.
1	Introduction
Many modern neural networks and other machine learning models are over-parametrized (5). These
models are typically trained to have near zero training loss, known as interpolation and often have
strong generalization performance, as indicated by a range of empirical evidence including (23; 3).
Due to a key property of interpolation - automatic variance reduction (discussed in Section 2.1),
stochastic gradient descent (SGD) with constant step size is shown to converge to the optimum of a
convex loss function for a wide range of step sizes (12). Moreover, the optimal choice of step size
η* for SGD in that setting can be derived analytically.
The goal of this paper is to take a step toward understanding momentum-based SGD in the interpo-
lating setting. Among them, stochastic version of Nesterov’s acceleration method (SGD+Nesterov)
is arguably the most widely used to train modern machine learning models in practice. The popular-
ity of SGD+Nesterov is tied to the well-known acceleration of the deterministic Nesterov’s method
over gradient descent (15). Yet, has not not theoretically clear whether Nesterov SGD accelerates
over SGD.
As we show in this work, both theoretically and empirically, Nesterov SGD with any parameter
selection does not in general provide acceleration over ordinary SGD. Furthermore, Nesterov SGD
may diverge, even in the linear setting, for step sizes that guarantee convergence of ordinary SGD.
Intuitively, the lack of acceleration stems from the fact that, to ensure convergence, the step size of
SGD+Nesterov has to be much smaller than the optimal step size for SGD. This is in contrast to the
deterministic Nesterov method, which accelerates using the same step size as optimal gradient de-
scent. As we prove rigorously in this paper, the slow-down of convergence caused by the small step
1
Published as a conference paper at ICLR 2020
size negates the benefit brought by the momentum term. We note that a similar lack of acceleration
for the stochastic Heavy Ball method was analyzed in (9).
To address the non-acceleration of SGD+Nesterov, we introduce an additional compensation term
to allow convergence for the same range of step sizes as SGD. The resulting algorithm, MaSS
(Momentum-added Stochastic Solver)1 updates the weights w and u using the following rules (with
the compensation term underlined):
Wt+1 J Ut — ηι Vf(Ut),
Ut+1 J (1 + γ)wt+1 — γwt + η2Vf(Ut).
Here, V represents the stochastic gradient. The step Size
η1, the momentum parameter γ ∈ (0, 1) and the compen-
sation parameter η2 are independent of t.
We proceed to analyze theoretical convergence proper-
ties of MaSS in the interpolated regime. Specifically, we
show that in the linear setting MaSS converges exponen-
(1)
Figure 1: Non-acceleration of Nesterov
SGD and fast convergence of MaSS.
tially for the same range of step sizes as plain SGD, and the optimal choice of step size for MaSS
is exactly η* which is optimal for SGD. Our key theoretical result shows that MaSS has accelerated
convergence rate over SGD. Furthermore, in the full batch (deterministic) scenario, our analysis
selects η2 = 0, thus reducing MaSS to the classical Nesterov’s method (15). In this case our con-
vergence rate also matches the well-known convergence rate for the Nesterov’s method (15; 4). This
acceleration is illustrated in Figure 1. Note that SGD+Nesterov (as well as Stochastic Heavy Ball)
does not converge faster than SGD, in line with our theoretical analysis. We also prove exponential
convergence OfMaSS in more general convex setting under additional conditions.
We further analyze the dependence of the convergence rate
e-s(m)t and optimal hyper-parameters on the mini-batch
size m. We identify three distinct regimes of dependence
defined by two critical values m1* and m2*: linear scaling, di-
minishing returns and saturation, as illustrated in Figure 2.
The convergence speed per iteration s(m), as well as the
optimal hyper-parameters, increase linearly as m in the lin-
ear scaling regime, sub-linearly in the diminishing returns
regime, and can only increase by a small constant factor in
the saturation regime. The critical values m1* and m2* are
derived analytically. We note that the intermediate “dimin-
Figure 2: Convergence speed per it-
eration s(m). Larger s(m) indicates
faster convergence per iteration.



ishing terurns” regime is new and is not found in SGD (12). To the best of our knowledge, this is
the first analysis of mini-batch dependence for accelerated stochastic gradient methods.
We also experimentally evaluate MaSS on deep neural networks, which are non-convex. We show
that MaSS outperforms SGD, SGD+Nesterov and Adam (10) both in optimization and general-
ization, on different architectures of deep neural networks including convolutional networks and
ResNet (7).
The paper is organized as follows: In section 2, we introduce notations and preliminary results.
In section 3, we discuss the non-acceleration of SGD+Nesterov. In section 4 we introduce MaSS
and analyze its convergence and optimal hyper-parameter selection. In section 5, we analyze the
mini-batch MaSS. In Section 6, we show experimental results.
1.1	Related Work
Over-parameterized models have drawn increasing attention in the literature as many modern ma-
chine learning models, especially neural networks, are over-parameterized (5) and show strong gen-
eralization performance (16; 23; 2). Over-parameterized models usually result in nearly perfect fit
(or interpolation) of the training data (23; 18; 3). Exponential convergence of SGD with constant
step size under interpolation and its dependence on the batch size is analyzed in (12).
1Code url: https://github.com/ts66395/MaSS
2
Published as a conference paper at ICLR 2020
There are a few works that show or indicate the non-acceleration of existing stochastic momentum
methods. First of all, the work (9) theoretically proves non-acceleration of stochastic Heavy Ball
method (SGD+HB) over SGD on certain synthetic data. Furthermore, these authors provide experi-
mental evidence that SGD+Nesterov also converges at the same rate as SGD on the same data. The
work (22) theoretically shows that, for sufficiently small step-sizes, SGD+Nesterov and SGD+HB is
equivalent to SGD with a larger step size. However, the results in (22) do not exclude the possibility
that acceleration is possible when the step size is larger. The work (11) concludes that “momentum
hurts the convergence within the neighborhood of global optima”, based on a theoretical analysis of
SGD+HB. These results are consistent with our analysis of the standard SGD+Nesterov. However,
this conclusion does not apply to all momentum methods. Indeed, we will show that MaSS provably
improves convergence over SGD.
There is a large body of work, both practical and theoretical, on SGD with momentum, includ-
ing (10; 8; 1). Adam (10), and its variant AMSGrad (17), are among the most practically used SGD
methods with momentum. Unlike our method, Adam adaptively adjusts the step size according
to a weight-decayed accumulation of gradient history. In (8) the authors proposed an accelerated
SGD algorithm, which can be written in the form shown on the right hand side in Eq.8, but with
different hyper-parameter selection. Their ASGD algorithm also has a tail-averaging step at the
final stage. In the interpolated setting (no additive noise) their analysis yields a convergence rate
of O(Poly(κ, K) exp(-9√r=l))compared to O(exp(-√t^))for。皿 algorithm With batch size 1.
We provide some experimental comparisons between their ASGD algorithm and MaSS in Fig. 4.
The Work (21) proposes and analyzes another first-order momentum algorithm and derives conver-
gence rates under a different set of conditions - the strong growth condition for the loss function in
addition to convexity. As shoWn in Appendix F.3, on the example ofa Gaussian distributed data, the
rates obtained in (21) can be slower than those for SGD. In contrast, our algorithm is guaranteed to
never have a slower convergence rate than SGD. Furthermore, in the same Gaussian setting MaSS
matches the optimal accelerated full-gradient Nesterov rate.
Additionally, in our work we consider the practically important dependence of the convergence rate
and optimal parameter selection on the mini-batch size, which to the best of our knowledge, has not
been analyzed for momentum methods.
2	Notations and Preliminaries
Given dataset D = {(xi, yi)}in=1 ⊂ Rd × C, we consider an objective function of the form f(w) =
W pn=1 fi(w), where f only depends on a single data point (xi, yi). Let Vf denote the exact
gradient, and Vm f denote the unbiased stochastic gradient evaluated based on a mini-batch of size
m. For simplicity, we also denote Vf (w) := V1f (w). We use the concepts of strong convexity and
smoothness of functions, see definitions in Appendix B.1. For loss function with μ-strong convexity
and L-smoothness, the condition number K is defined as K = L∕μ.
In the case of the square loss, fi(w) = 11 (wTXi - yi)2, and the Hessian matrix is H :=
W pn=1 XiXT. Let L and μ be the largest and the smallest non-zero eigenvalues of the Hessian
respectively. Then the condition number is then K = L∕μ (note that zero eigenvalues can be ignored
in our setting, see Section 4).
Stochastic Condition Numbers. For a quadratic loss function, let Hm :=* Pm=I XiXT denotes
a mini-batch estimate of H. Define L1 be the smallest positive number such that E kXK k2XKXKT
L1H, and denote
Lm := L1∕m + (m - 1)L∕m.	(2)
Given a mini-batch size m, we define the m-stochastic condition number as Km := Lm∕μ.
Following (8), we introduce the quantity KK (called statistical condition number in (8)), which is the
smallest positive real number such that E kXKk2H-1XKXKT	KKH.
Remark 1. We note that L1 ≥	L,	since E	kXKk2XKXKT	= E[HK12]	=	H2	+ E[(HK1	-	H)2]	H2.
Consequently, according to the definition of Lm in Eq.2, we have
Lm ≥ L, ∀m ≥ 1; Lm → L, as m → ∞.	(3)
3
Published as a conference paper at ICLR 2020
Hence, the quadratic loss function is also Lm-smooth, for all m ≥ 1. By the definition of κm, we
also have
κm ≥ κ, ∀m ≥ 1; κm → κ, as m → ∞.	(4)
Remark 2. It is important to note that K ≤ κι, since E [∣∣X∣∣H-1XXT] W ɪE [kX∣∣2XXτ] W κιH.
2.1	Convergence of SGD for Over-parametrized Models and Optimal Step Size
We consider over-parametrized models that have zero training loss solutions on the training data
(e.g., (23)). A solution fi(w) which fits the training data perfectly f (W) = 0, ∀i = 1,2,…，n, is
known as interpolating. In the linear setting, interpolation implies that the linear system {XiT w =
yi }in=1 has at least one solution.
A key property of interpolation is Automatic Variance Reduction (AVR), where the variance of the
stochastic gradient decreases to zero as the weight W approaches the optimal w*.
VarMmf(w)] W ∣∣w — w*∣∣2E[(Hm — H)2].	(5)
For a detailed discussion of AVR see Appendix B.2.
Thanks to AVR, plain SGD with constant step size can be shown to converge exponentially for
strongly convex loss functions (13; 19; 14; 12). The set of acceptable step sizes is (0, 2/Lm), where
Lm is defined in Eq.2 and m is the mini-batch size. Moreover, the optimal step size η* (m) of SGD
that induces fastest convergence guarantee is proven to be 1/Lm (12).
3	Non-acceleration of SGD+Nesterov
In this section we prove that SGD+Nesterov, with any constant hyper-parameter setting, does not
generally improve convergence over optimal SGD. Specifically, we demonstrate a setting where
SGD+Nesterov can be proved to have convergence rate of (1 — O(1∕κ))t, which is same (UP to a
constant factor) as SGD. In contrast, the classical accelerated rate for the deterministic Nesterov's
method is (1 — 1∕√κ)t.
We will consider the following two-dimensional data-generating component decoupled model. Fix
an arbitrary w* ∈ R2 and randomly sample z from N(0, 2). The data points (X, y) are constructed
as follow:
σ	σι	∙ Z ∙	eι,	w.p.	0.5,
σ	σ2	∙ z ∙	e2,	w.p.	0.5,
and y = hw*, Xi,
(6)
where e1 , e2 ∈ R2 are canonical basis vectors, σ1 > σ2 > 0. It can be seen that L1 = σ12,
κι = 6σ2∕σ2 and K = 6 (See Appendix F.1). This model is similar to that used to analyze the
stochastic Heavy Ball method in (9).
The following theorem gives a lower bound for the convergence of SGD+Nesterov, regarding the
linear regression problem on the component decoupled data model. See Appendix C for the proof.
Theorem 1 (Non-acceleration of SGD+Nesterov). Let {(Xi, yi)}in=1 be a dataset generated accord-
ing to the component decoupled model. Consider the optimization problem of minimizing quadratic
function 2n Ei(XTW 一 yi)2∙ For any step size η > 0 and momentum parameter Y ∈ (0,1) of
SGD+Nesterov with random initialization, with probability one, there exists a T ∈ N such that
∀t > T,
E[f (wt)] — f (w*) ≥ (1 一 K) [f (wo) — f (w*)],
(7)
where C > 0 is a constant.
Compared with the convergence rate (1 — 1∕κ)t of SGD (12), this theorem shows that SGD+Nesterov
does not accelerate over SGD. This result is very different from that in the deterministic gradient
scenario, where the classical Nesterov’s method has a strictly faster convergence guarantee than
gradient descent (15).
Intuitively, the key reason for the non-acceleration of SGD+Nesterov is a condition on the step size η
required for non-divergence of the algorithm. Specifically, when momentum parameter γ is close to
4
Published as a conference paper at ICLR 2020
1, η is required to be less than 2(1 - Y)/3 + O((1 - γ)2) (precise formulation is given in Lemma 1 in
Appendix C). The slow-down resulting from the small step size necessary to satisfy that condition
cannot be compensated by the benefit of the momentum term. In particular, the condition on the
step-size of SGD+Nesterov excludes η* that achieves fastest convergence for SGD. We show in the
following corollary that, with the step size η*, SGD+Nesterov diverges. This is different from the
deterministic scenario, where the Nesterov method accelerates using the same step size as gradient
descent.
Corollary 1. Consider the same optimization problem as in Theorem 1. Let step-size η =六=612
and acceleration parameter γ ∈ [0.6, 1], then SGD+Nesterov, with random initialization, diverges
with probability 1.
We empirically verify the non-acceleration of SGD+Nesterov as well as Corollary 1, in Section 6
and Appendix F.2.
4 MaSS: Accelerating SGD
In this section, we propose MaSS, which introduces a compensation term (see Eq.1) onto
SGD+Nesterov. We show that MaSS can converge exponentially for all the step sizes that re-
sult in convergence of SGD, i.e., η ∈ (0, 2∕Lm). Importantly, We derive a convergence rate
exp(-t/√κικ), where K ≤ κ1, for MaSS which is faster than the convergence rate for SGD
exp(-t∕κ1). Moreover, We give an analytical expression for the optimal hyper-parameter setting.
For ease of analysis, we rewrite update rules of MaSS in Eq.1 in the following equivalent form
(introducing an additional variable v):
wt+ι - Ut - η^f (ut),
Vt+1 J (1 - α)vt + αut - δVf (ut),	(8)
ut+ι J ι+ααvt+ι + ι+1αwt+ι.
There is a bijection between the hyper-parameters (η1, η2, γ) and (η, α, δ), which is given by:
Y = (1 - α)∕(1 + α), ηι = η, η2 = (η - αδ)∕(1 + α).	(9)
Remark 3 (SGD+Nesterov). In the literature, the Nesterov’s method is sometimes written in a
similar form as the R.H.S. of Eq.8. Since SGD+Nesterov has no compensation term, δ has to be
fixed as η∕α, which is consistent with the parameter setting in (15).
wt+1 J ut - η1Vf(ut),
,	,	〜 -, , <∖ ⇒
ut+1 J (1 + Y)wt+1 - Ywt + η2Vf(ut)
Assumptions. We first assume square loss function, and later extend the analysis to general convex
loss functions under additional conditions. For square loss function, the solution set W * := {w ∈
Rd|f (w) = 0} is an affine subspace in the parameter space Rd. Given any w, we denote its closest
solution as w* := argminv∈w* ∣∣w - v∣∣, and define the error E = W - w*. Be aware that different
w may correspond to different w*, and that and (stochastic) gradients are always perpendicular to
W* (see discussion in Appendix B.3). Hence, no actual update happens along W*. For this reason,
we can ignore zero eigenvalues of H and restrict our analysis to the span of the eigenvectors of the
Hessian with non-zero eigenvalues.
Based on the equivalent form of MaSS in Eq.8, the following theorem shows that, for square loss
function in the interpolation setting, MaSS is guaranteed to have exponential convergence when
hyper-parameters satisfy certain conditions.
Theorem 2 (Convergence of MaSS). Consider minimizing a quadratic loss function in the inter-
Polation setting. Let μ be the smallest non-zero eigenvalue of the HeSSian matrix H. Let Lm be
as defined in Eq.2. Denote κKm := κK∕m + (m - 1)∕m. In MaSS with mini batch of size m, if the
positive hyper-parameters η, α, δ satisfy the following two conditions:
α∕δ ≤ μ,	αδKm + η(ηLm - 2) ≤ 0,	(10)
then, after t iterations,
E IIvt - w*kH-ι + αl∣wt - w*k2 ≤ (1 - α)t (∣∣V0 - w*kH-ι + αkw0 - w*k2).
Consequently,
IlWt — w*∣2 ≤ C • (1 — α)t,
for some constant C > 0 which depends on the initialization.
5
Published as a conference paper at ICLR 2020
Remark 4. By condition Eq.10, the admissible step size η is (0, 2/Lm), exactly the same as SGD
for interpolated setting (12).
Remark 5. One can easily check that the hyper-parameter setting of SGD+Nesterov does not satisfy
the conditions in Eq.10.
Proof sketch for Theorem 2. Denote Ft := E kvt+1 - w*kH-ι + δ kwt+1 -w*k2], We show
that, under the update rules of MaSS in Eq.8,
Ft+1 ≤ (I - α)Ft + + lα∣μ - δ) kut - w* k2 + Wkm + δη2 Lm/α - 2n6/a) kut - w* kH.
x-------------------------{----}	'------------V------------/
c1	c2
By the condition in Eq.10, c1 ≤ 0, c2 ≤ 0, then the last two terms are non-positive. Hence,
Ft+1 ≤ (1 - α)Ft ≤ (1 - α)t+1F0.	(11)
UsingthatkWt-w*k2 ≤ αFt/δ, we get the final conclusion. See detailed proof in Appendix D. □
Hyper-parameter Selection. From Theorem 2, we observe that the convergence rate is deter-
mined by (1 -α)t. Therefore, larger α is preferred for faster convergence. Combining the conditions
in Eq.10, we have
α2 ≤ η(2 - ηLm)μ∕Km.	(12)
By setting η* = 1/Lm_Which maximizes the right hand side of the inequality, we obtain the optimal
selection a* = 1∕√κmκm. Note that this setting of η* and α* determines a unique δ* = α*∕μ by
the conditions in Eq.10. In summary,
- 、	1	 、	1	…、	η*
η (m)=厂，α (m) = /	〜,δ (m) =	.	(13)
Lm	κmκm	α κm
By Eq.9, the optimal selection of (η1, η2, γ) would be:
η↑(m) = LL,	η2(m) = √√mκm (l- ɪ) , γ*(m)= √SE - ： .	(14)
Lm	κmκm + 1 κm	κmκm + 1
Km is usually larger than 1, which implies that the coefficient η2 of the compensation term is
non-negative. The non-negative coefficient η2 indicates that the weight ut is “over-descended” in
SGD+Nesterov and needs to be compensated along the gradient direction.
It is important to note that the optimal step size for MaSS as in Eq.13 is exactly the same as the op-
timal one for SGD (12). With such hyper-parameter selection given in Eq.14, we have the following
theorem for optimal convergence:
Theorem 3 (Acceleration of MaSS). Under the same assumptions as in Theorem 2, ifwe set hyper-
parameters in MaSS as in Eq.13, then after t iteration of MaSS with mini batch of size m,
∣∣Wt - W*k2 ≤ C ∙ (1 - 1∕√KmKm)t,
(15)
for some constant C > 0 which depends on the initialization.
Remark 6. With the optimal hyper-parameters in Eq.13, the asymptotic convergence rate of MaSS
is
^O( e-1/√κm Km
),
(16)
which is faster than the rate O(e-"κm) of SGD (see (12)), since Km ≥ Km,.
Remark 7 (MaSS Reduces to the Nesterov’s method for full batch). In the limit of full batch m →
∞, we have κm → κ, κKm → 1, the optimal parameter selection in Eq.14 reduces to
ηι = 1/L,	Y* = (√κ - 1)/(√κ + 1),	η2 = 0.	(17)
It is interesting to observe that, in the full batch (deterministic) scenario, the compensation term
vanishes and η1* and γ* are the same as those in Nesterov’s method. Hence MaSS with the optimal
hyper-parameter selection reduces to Nesterov,s method in the limit of full batch. Moreover, the
convergence rate in Theorem 3 reduces to O(e-t/VZκ), which is exactly the well-known convergence
rate of Nesterov’s method (15; 4).
6
Published as a conference paper at ICLR 2020
Extension to Convex Case. First, we extend the definition of L1 to convex functions, L1 :=
inf{c ∈ R | E[∣Nf (w)k2] ≤ 2c (f (W) — f*)}, and keep the definition of Lm the same as Eq.2.
It can be shown that these definitions of Lm are consistent with those in the quadratic setting. We
assume that the smallest positive eigenvalue of Hessian H(x) is lower bounded by μ > 0, for all x.
Theorem 4. Suppose there exists a L -strongly convex and ɪ -smooth non-negative function g :
Rd → R such that g(w*) = 0 and Wg(X), Nf(Zyi ≥ (1 — c)〈x — w*, Z — w*〉, ∀x, Z ∈ Rd, for
some > 0. In MaSS, if the hyper-parameters are set to be:
η 二 1∕(2Lm),	α = (1 - e)∕(2κm),	δ = 1∕(2Lm),
then after t iterations, there exists a constant C such that, E[f (wt)] ≤ C ∙(1 — 21--)t.
(18)
5	Linear Scaling Rule and the Diminishing Returns
Based on our analysis, we discuss the effect of selection of mini-batch size m. We show that the
domain of mini-batch size m can be partitioned into three intervals by two critical points: m1* =
min(Lι∕L, κ), m2 = max(Lι∕L, K). The three intervals/regimes are depicted in Figure 2, and the
detailed analysis is in Appendix G.
Linear Scaling: m < m*1. In this regime, we have Lm ≈ L1∕m, κm ≈ κ1∕m and κKm ≈ κK∕m.
The optimal selection of hyper-parameters is approximated by:
η*(m) ≈ m ∙ η*(1), α*(m) ≈ m ∙ ɑ*(1), δ*(m) ≈ m ∙ δ*(1),
and the convergence rate in Eq.16 is approximately O(e-m-tNκ1κ). ThiS indicates linear gains in
convergence when m increases.
In the linear scaling regime, the hyper-parameter selec-
tions follow a Linear Scaling Rule (LSR): When the
mini-batch size is multiplied by k, multiply all hyper-
parameters (η, α, δ) by k. This parallels the linear scaling
rule for SGD which is an accepted practice for training
neural networks (6).
Moreover, increasing m results in linear gains in the con-
vergence speed, i.e., one MaSS iteration with mini-batch
size m is almost as effective as m MaSS iterations with
mini-batch size 1.
Diminishing Returns: m ∈ [m1* , m2*) In this regime,
increasing m results in sublinear gains in convergence
speed. One MaSS iteration with mini-batch size m is less
effective than m MaSS iterations with mini-batch size 1.
Figure 3: Convergence speed per itera-
tion s(m). Larger s(m) indicates faster
convergence. Red solid curve: experi-
mental results. Blue dash curve: theo-
retical lower bound Kmmmm- Critical
mini-batch sizes: m1* ≈ 10, m2* ≈ 50.
Saturation: m ≥ m2*. One MaSS iteration with mini-batch size m is nearly as effective (up to a
multiplicative factor of 2) as one iteration with full gradient.
This three regimes partition is different from that for SGD (12), where only linear scaling and satu-
ration regimes present. An empirical verification of the dependence of the convergence speed on m
is shown in Figure 3. See the setup in Appendix G.
6	Empirical Evaluation
Synthetic Data. We empirically verify the non-acceleration of SGD+Nesterov and the fast conver-
gence of MaSS on synthetic data. Specifically, We optimize the quadratic function 2n Ei(XTW —
yi)2, where the dataset {(xi, yi)}in=1 is generated by the component decoupled model described
in Section 3. We compare the convergence behavior of SGD+Nesterov with SGD, as well as our
proposed method, MaSS, and several other methods: SGD+HB, ASGD (8). We select the best
hyper-parameters from dense grid search for SGD+Nesterov (step-size and momentum parameter),
7
Published as a conference paper at ICLR 2020
SGD+HB (step-size and momentum parameter) and SGD (step-size). For MaSS, we do not tune
the hyper-parameters but use the hyper-parameter setting suggested by our theoretical analysis in
Section 4; For ASGD, We use the setting provided by (8).
Fig. 4	shows the convergence behaviors of these algorithms
on the setting of σ12 = 1, σ22 = 1/29. We observe that
the fastest convergence of SGD+Nesterov is almost iden-
tical to that of SGD, indicating the non-acceleration of
SGD+Nesterov. We also observe that our proposed method,
MaSS, clearly outperforms the others. In Appendix F.2, We
provide additional experiments on more settings of the com-
ponent decoupled data, and Gaussian distributed data. We
also shoW the divergence of SGD+Nesterov With the same
step size as SGD and MaSS in Appendix F.2.
Real data: MNIST and CIFAR-10. We compare the op-
timization performance of SGD, SGD+Nesterov and MaSS
on the folloWing tasks: classification of MNIST With a fully-
connected netWork (FCN), classification of CIFAR-10 With a convolutional neural netWork (CNN)
and Gaussian kernel regression on MNIST. See detailed description of the architectures in Ap-
pendix H.1. In all the tasks and for all the algorithms, We select the best hyper-parameter setting
over dense grid search, except that We fix the momentum parameter γ = 0.9 for both SGD+Nesterov
and MaSS, Which is typically used in practice. All algorithms are implemented With mini batches of
size 64 for neural netWork training.
Figure 4: Comparison on com-
ponent decoupled data. Hyper-
parameters: We use optimal param-
eters for SGD+Nesterov, SGD+HB
and SGD; the setting in (8) for
ASGD; Eq. 13 for MaSS.
Fig. 5	shoWs the training curves of MaSS, SGD+Nesterov and SGD, Which indicate the fast conver-
gence of MaSS on real tasks, including the non-convex optimization problems on neural netWorks.
Figure 5: Comparison of SGD, SGD+Nesterov and MaSS on (left) fully-connected neural netWork,
(middle) convolutional neural netWork, and (right) kernel regression.
Test Performance. We shoW that the solutions found by MaSS have good generalization perfor-
mance. We evaluate the classification accuracy of MaSS, and compare With SGD, SGD+Nesterov
and Adam, on different modern neural netWorks: CNN and ResNet (7). See description of the ar-
chitectures in Appendix H.1. In the training processes, We folloW the standard protocol of data
augmentation and reduction of learning rate, Which are typically used to achieve state-of-the-art
results in neural netWorks. In each task, We use the same initial learning rate for MaSS, SGD and
SGD+Nesterov, and run the same number of epochs (150 epochs for CNN and 300 epochs for
ResNet-32). Detailed experimental settings are deferred to Appendix H.2.
Table 6 compares the classification
accuracy of these algorithms on the
test set of CIFAR-10 (average of 3
independent runs).
We observe that MaSS produces the
best test performance. We also
note that increasing initial learning
rate may improves performance of
MaSS and SGD, but degrades that of
	η	SGD	SGD+Nesterov	MASS	ADAMt
CNN	0.01	81.40%	83.06%	83.97%	82.65%
	0.3	82.41%	75.79%]	84.48%	
RESNET-32	0.1	91.92%	92.60%	92.77%	92.27%
	0.3	92.51%	91.13%]	92.71%	
] Average of 3 runs that converge. Some runs diverge.
↑ Adam uses initial step size 0.00L
Table 1: Classification accuracy on test set of CIFAR-10.
SGD+Nesterov. Moreover, in our experiment, SGD+Nesterov With large step size η = 0.3 diverges
in 5 out of 8 runs on CNN and 2 out of 5 runs on ResNet-32 (for random initialization), While MaSS
and SGD converge on every run.
8
Published as a conference paper at ICLR 2020
Acknowledgements
This research was in part supported by NSF funding and a Google Faculty Research Award. GPUs
donated by Nvidia were used for the experiments. We thank Ruoyu Sun for helpful comments
concerning convergence rates. We thank Xiao Liu for helping with the empirical evaluation of our
proposed method.
References
[1]	Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages
1200-1205. ACM, 2017.
[2]	Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National
Academy of Sciences, 116(32):15849-15854, 2019.
[3]	Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to
understand kernel learning. arXiv preprint arXiv:1802.01396, 2018.
[4]	Sebastien BUbeck et al. Convex optimization: Algorithms and complexity. Foundations and
TrendsR in Machine Learning, 8(3-4):231-357, 2015.
[5]	Alfredo Canziani, Adam Paszke, and EUgenio CUlUrciello. An analysis of deep neUral network
models for practical applications. arXiv preprint arXiv:1605.07678, 2016.
[6]	Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, AaPo Kyrola,
Andrew TUlloch, Yangqing Jia, and Kaiming He. AccUrate, large minibatch sgd: training
imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
[7]	Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pages 770-778, 2016.
[8]	Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Ac-
celerating stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.
[9]	Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M Kakade. On the insuffi-
ciency of existing momentum schemes for stochastic optimization. International Conference
on Learning Representations (ICLR), 2018.
[10]	Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
[11]	Tianyi Liu, Zhehui Chen, Enlu Zhou, and Tuo Zhao. Toward deeper understanding of noncon-
vex stochastic optimization with momentum using diffusion approximations. arXiv preprint
arXiv:1802.05155, 2018.
[12]	Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding
the effectiveness of sgd in modern over-parametrized learning. International Conference on
Machine Learning (ICML), 2018.
[13]	Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation al-
gorithms for machine learning. In Advances in Neural Information Processing Systems, pages
451-459, 2011.
[14]	Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sam-
pling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing
Systems, pages 1017-1025, 2014.
[15]	Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2013.
9
Published as a conference paper at ICLR 2020
[16]	Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:
On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
[17]	Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond.
International Conference on Learning Representations (ICLR), 2018.
[18]	Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis
of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
[19]	Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a
strong growth condition. arXiv preprint arXiv:1308.6370, 2013.
[20]	Thomas Strohmer and Roman Vershynin. A randomized kaczmarz algorithm with exponential
convergence. Journal of Fourier Analysis and Applications, 15(2):262, 2009.
[21]	Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for
over-parameterized models and an accelerated perceptron. arXiv preprint arXiv:1810.07288,
2018.
[22]	Kun Yuan, Bicheng Ying, and Ali H Sayed. On the influence of momentum acceleration on
online learning. The Journal ofMachine Learning Research, 17(1):6602-6667, 2016.
[23]	Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. International Conference on Learning
Representations (ICLR), 2017.
10
Published as a conference paper at ICLR 2020
A Pseudocode for MaSS
Algorithm 1: MaSS-Momentum-added Stochastic Solver
Require: Step-size η1, secondary step-size η2, acceleration parameter γ ∈ (0, 1).
Initialize: u0 = w0 .
while not converged do
Wt+1 — Ut — ηιVf (ut),
Ut+1 - (1+ γ)wt+1 — YWt — η2Vf (ut).
end while
Output: weight wt.
Note that the proposed algorithm initializes the variables w0 and U0 with the same vector, which
could be randomly generated.
As discussed in section 4, MaSS can be equivalently implemented using the following update rules:
Wt+1	—	Ut — ηVf(ut),	(19a)
Vt+1	-	(1 — α)vt + αut — δV f (ut),	(19b)
α1
ut+1	-	TTa vt+1 + τ+αwt+1.	(19C)
In this case, variables U0, v0 and w0 should be initialized with the same vector.
There is a bijection between the hyper-parameters (η1, η2, γ) and (η, α, δ), which is given by:
1 — a
1 + a
η — αδ
η1 = η, η2 =斤+a
(20)
B Additional Preliminaries
B.1	Strong Convexity and Smoothness of Functions
Definition 1 (Strong Convexity). A differentiablefUnction f : Rd → R is μ-strongly convex (μ > 0),
if
f(x) ≥ f (Z) + hVf(z), X — Zi + 2IlX — zk2,	∀x,z ∈ Rd.	(21)
Definition 2 (Smoothness). A differentiable function f : Rd → R is L-smooth (L > 0), if
f(x) ≤ f(z) + hVf(z), x — Zi + LIlX — z∣2, ∀x, z ∈ Rd.	(22)
B.2	Automatic Variance Reduction
In the interpolation setting, one can write the square loss as
f (w) = 2(w — w*)T H (w — w*) = TkW — w*kH.	(23)
A key property of interpolation is that the variance of the stochastic gradient of decreases to zero as
the weight W approaches an optimal solution w*.
Proposition 1 (Automatic Variance Reduction). For the square loss function f in the interpolation
setting, the stochastic gradient at an arbitrary point w can be written as
V mf (w) = Hm(w — w*) = HmJ	(24)
Moreover, the variance of the stochastic gradient
Var[Vmf(w)] W ke∣∣2E[(Hm — H)2].	(25)
Since E[(Hm — H)2] is independent of w, the above proposition unveils a linear dependence of
variance of stochastic gradient on the norm square of error J. This observation underlies exponential
convergence of SGD in certain convex settings (20; 13; 19; 14; 12).
11
Published as a conference paper at ICLR 2020
B.3	On ignoring zero eigenvalues of the Hessian
Consider the square loss function, f (W) = * Pi(XTW - yi)2. The (stochastic) gradient is
,ɪ .,、
V mf(w)
1m	1m
m XXi(XiW - yi) = m XXixi (w - W ) = Hm(W - W ),
i=1
i=1
where the stochastic gradient is computed based on a randomly sampled batch of size m.
Recall that the solution set W * := {w ∈ Rd∣f (w) = 0} is an affine subspace in the parameter
space, and that w* is the solution in W * that is closest to w. Hence, W — w* is perpendicular to
W *, i.e., w — w* ∈ (W *)⊥. Note that (W *)⊥ is an invariant space of Hm,, hence V mf (w)=
Hm(w - w*) ∈ (W*)⊥. Hence the (stochastic) gradient is perpendicular to W*.
C Proof of Theorem 1
The key proof technique is to consider the asymptotic behavior of SGD+Nesterov in the decoupled
model of data when the condition number becomes large.
Notations and proof setup. Recall that the square loss function based on the component decoupled
dataD, define in Eq.6, is in the interpolation regime, then for SGD+Nesterov, we have the recurrence
relation
wt+1 = (1 + γ)(1 - ηH)wt - γ(1 - ηH)wt-1,	(26)
where H = diag((χ[1])2, (χ[2])2). It is important to note that each component of Wt evolves inde-
pendently, due to the fact that H is diagonal.
With := w - w*, we define for each component j = 1, 2 that
)
1 :=E
[tj+]
[j]
t+1
S[tj]
, j=1,2,
(27)
0 S
where [j] is the j-th component of vector .
The recurrence relation in Eq.26 can be rewritten as
(28)
with
B[j]
E
/ (ι + γ)2(ι — η(x[j])2)
(I + Y)(I-n(x[j])2)
(1 + γ)(ι-η(x[j])2)
2
1
∖
-γ(ι + γ)(ι - η(x[j])2)2
0
—γ(ι — η(x[j])2)
0
—γ(ι + γ)(ι - η(x[j])2)2
—γ(ι — η(x[j])2)
γ2(1 — η(x[j])2)2 ʌ
0
0
0
0
0
/
/
∖
(1 + Y)2A[j]
(1 + Y)(1 - ησj2)
(1 + Y)(1 - ησj2)
1
-γ(1 + γ)A[j]
0
-Y(I - ησ2)
0
-Y(1 + Y)A[j]
-Y (1 - ησj2 )
0
0
Y2A[j]
0
0
0
、
/
where A[j] := (1 - ησj2)2 + 5(ησj2)2.
For the ease of analysis, we define u := 1 - γ ∈ (0, 1] and tj := ησj2, j = 1, 2. Without loss of
generality, We assume σ2 = 1 in this section. In this case, tι = η and t2 = η∕κ, where K is the
condition number.
12
Published as a conference paper at ICLR 2020
Elementary analysis gives the eigenvalues of B[j] :
λ1
λ2
λ3
λ4
(1 - u)(1 - tj),
T0 -
k2 + 8 + 4T3)/
—	.	L. 21/3	T	.	1
To + (I - i√3)-T-  -----------I/3 + (I + iij73) O 91/3
6 (T2 + PTT+4T3]1/	6 • 21/3
21/3	T	i
To + (I + i√3)-T-  -----------I/3 +(1 - iij73) a 01/3
6 (T2 + PTΤ+4Γ3}1/	6 • 21/3
where
T0 = 1 - U + i— 3tj + 3 Utj - 3 u2tj,
T1	=	-(-3 + 3u - u2 + 7tj - 7utj + 2u2tj)2 +3(3- 6u+4u2 -u3 - 10tj + 20utj - 13u2tj +3u3tj),
T2	= 9U4 - 9U5 + 2U6 - 72U2tj + 144U3tj - 141U4tj + 69U5tj - 12U6tj + 252tj2 - 756Utj2
+1185U2tj2 - 1110U3tj2 + 615U4tj2 - 186U5tj2 + 24U6tj2 - 686tj3 + 2058Utj3 - 2646U2tj3
+1862U3tj3 - 756U4tj3 + 168U5tj3 - 16U6tj3.
Proof idea. For the two-dimensional component decoupled data, we have
E[f(wt)] - f(w") ≥ σ∣E [kwt - w*k2] = σ2E [怕|[2] .	(30)
By definition of Φ in Eq.27, we can see that the convergence rate is lower bounded by the conver-
gence rates of the sequences {kΦ[tj] k}t. By the relation Eq.28, we have that the convergence rate of
the sequence {kΦtk}t is controlled by the magnitude of the top eigenvalue λmax ofB, if Φt has non-
zero component along the eigenvector of B with eigenvalue λmaχ(B). Specifically, if ∣λmaχ∣ > 1,
kΦj] k grows at a rate of ∣λmaχ∣t, indicating the divergence of SGD+Nesterov; if ∣λmaχ| < 1, then
∣∣Φj]k converges at a rate of ∣λmaχ∣t.
In the following, We use the eigen-systems of matrices B[j], especially the top eigenvalue, to analyze
the convergence behavior of SGD+Nesterov with any hyper-parameter setting. We show that, for any
choice of hyper-parameters (i.e., step-size and momentum parameter), at least one of the following
statements must holds:
•	B[1] has an eigenvalue larger than 1.
•	B[2] has an eigenvalue of magnitude 1 - O (1∕κ).
This is formalized in the following two lemmas.
Lemma 1. For any U ∈ (0, 1], if step size
/、	-3 - 2u + 2u2 + √9 + 84u - 164u2 + 100u3 - 20u4
η > η0(U) = -----------------2(9 - 15u + 6u2)----------------
then, B[1] has an eigenvalue larger than 1.
We will analyze the dependence of the eigenvalues on κ, when κ is large to obtain
Lemma 2. For any U ∈ (0, 1], if step size
/、	-3 - 2u + 2u2 + √9 + 84u - 164u2 + 100u3 - 20u4
η ≤ ηo(U) = ------------------2(9 - 15u + 6u2)-----------------
then, B[2] has an eigenvalue ofmagnitude 1 — O (1∕κ).
(31)
(32)
13
Published as a conference paper at ICLR 2020
Finally, we show that Φt has non-zero component along the eigenvector of B with eigenvalue λmax ,
hence the convergence of SGD+Nesterov is controlled by the eigenvalue of B[j] with the largest
magnitude.
Lemma 3. Assume SGD+Nesterov is initialized with w° such that both components hw0 — w*, e。
andhwo — w*, e* are non-zero. Then, for all t > 2, Φj] has a non-zero component in the eigen
direction of B[j] that corresponds to the eigenvalue with largest magnitude.
Remark 8. When W is randomly initialized, the conditions hw0 — w*, eι)= 0 and (w0 — w*,吟=
0 are satisfied with probability 1, since complementary cases form a lower dimensional manifold
which has measure 0.
By combining Lemma 1, 2 and 3, we have that SGD+Nesterov either diverges or converges at a
rate of (1 — O(1∕κ))t, and hence, We conclude thenon-aCceleration of SGD+Nesterov. In addition,
Corollary 1 is a special case of Theorem 1 and is proven by combining Lemma 1 and 3.
In high level, the proof ideas of Lemma 1 and 3 is analogous to those of (9), Which proves the
non-acceleration of stochastic Heavy Ball method over SGD. But the proof idea of Lemma 2 is
unique.
Proof of Lemma 1. The characteristic polynomial of Bj, j = 1, 2, are:
Dj (λ) = λ4 — (1 + γ)2Ajλ3 + 2γ(1 + γ)2(1 — ησj2)Aj — γ2(1 — ησj2)2 — γ2Aj λ2
-Y2(I + Y)2(I- ησj)2Ajλ + γ4(1 — ησ2)2Aj	(33)
First note that limλ→∞ Dj(λ) = +∞ > 0. In order to shoW B1 has an eigenvalue larger than 1, it
suffices to verity that D1(λ = 1) < 0, because Dj (λ) is continuous.
Replacing Y by 1 — u and ησ12 by t1 , We have
D1(1)	=	4u2t1 — 2u3t1	— 2ut21	— 10u2t12	+	6u3t21	— 6t31	—	16ut13	+ 38u2t31	— 16u3t13	— 18t41
+48ut41 — 42u2t14 + 12u3t14.	(34)
Solving for the inequality D1(1) < 0, We have
_	—3	—	2u	+ 2u2	+	√9 +	84u — 164u2	+ 100u3	— 20u4
η = t1 >	2(9 — 15u + 6u2)	,
for positive step size η.	□
ProofofLemma 2. We will show that at least one of the eigenvalues of B[2] is 1 — O(1∕κ), under
the condition in Eq.32.
First, we note that t? = t∖∕κ = η∕κ, which is O(1∕κ). We consider the following cases separately:
1) u is Θ(t20.5); 2) u is o(t02.5) and ω(t2); 3) u is O(t2); 4) u is ω(t20.5) and o(1); and 5) u is Θ(1),
the last of which includes the case where momentum parameter is a constant.
Note that, for cases 1-4, u is o(1). In such cases, the step size condition Eq.32 can be written as
η < 3U + o(u).	(35)
It is interesting to note that η must be o(1) to not diverge, when u is o(1). This is very different to
SGD where a constant step size can result in convergence, see (12).
Case 1:	u is Θ(t20.5). In this case, the terms u6, u4t2, u2t22 and t32 are of the same order.
We find that
T0 =	1 — u + O(t2),
T1 = —3u2 + 12t2 + O(t2 ),
T2 = 9u4 — 72u2t2 + 252t22 + O(t52/2).
14
Published as a conference paper at ICLR 2020
Hence,
λ1	=	1 - u -	t2 + ut2
λ2	=	1 - U -	21/3---12t2	- 3U：、+—1τ7^(108(4t2	- u2)3)1∕6	+ O(t2)
2	3 (108(4t2 - u2)3)1/6	3 ∙ 21/3'	、 2	27
L	21/3	120	— 3u2	LI	C	C	τ,c
λ3	=	1 - U + (I - i√3)~^~ (↑(}Q(A/-------2、3、1/6	+ (I +	i√3) a	91∕3 (108(4t2	- U ) ) /	+ O(t2)
6	(108(4t2	— u2)3)1/6	6	∙	21/3
L	21/3	120	— 3u2	LI	c	c	―
λ4	=	1 - U + (1 + i√3)-7— ∩∩ft∕ .,-------2、3、1/6	+ (I -	i√3) a	91∕3 (108(4t2	- U ) ) /	+ O(t2)
6	(108(4t2	— U2)3)1/6	6	∙	21/3
Write t2 = cU2 asymptotically for some constant c. If 4t2 ≥ U2 , i.e., 4c - 1 ≥ 0, then
λ2	1 - U + O(t2),
λ3	.	√4c -1	〜、 =1 — U +	τ=—U + O(t2), 3
λ4	√4C-ɪ	,〜，、 —1 — U + 	尸	U + 0(t2). 3
If 4t2 ≤ U2, i.e., 4c - 1 ≤ 0, then	
λ2	= 1 - U + O(t2),
λ3	.	.√1 - 4c	…、 =1 — U + i	尸—U + O(t2), 3
λ4	.	.√1 - 4c	…、 =1 — U + i	——U + O(t2). 3
In either case, the first-order term is of order U.
Recall that t? = η∕κ, then We have
U2	2cU	cU =Ctt = MR < IK ≤ ɪ	(36)
hence, U < c∕κ = O(1∕κ). Therefore, λi,i = 1,2,3,4 are 1 一 O(1∕κ).
Case 2:	U is o(t20.5) and ω(t2). In this case,
T0 T1 T2	=	1 - U - 7t2/3 + o(t2), =	12t2 + o(t2), = 252t22 - 72U2t2 + o(t22 ).
Hence,
λ1	=	1	-	U	- t2 + Ut2,
λ2	=	1	-	U	+ O(t2),
λ3	=	1	—	U	+ 2i√t2 +	O(t2),
λ4	=	1	—	U	— 2i√t2 +	O(t2).
Assume U 〜力号.When α ∈ (0.5,1), note that t2 = η∕κ ≤ IU = O(tα∕κ), we have t2-α =
O(1∕κ), hence t2 = o(1∕κ2). This implies that t2.5 = o(1∕κ) and U = o(1∕κ). Therefore, all the
eigenvalues λ% are of order 1 一 O(1∕κ).
Case 3:	U is O(t2). This case if forbidden by the assumption of this lemma. This is because,
t2 = η∕κ ≤ 3K = o(u) (1/k is o(1)). This is contradictory to U is O(t2).
Case 4:	U is Ω(t0∙5) and o(1). In this case, we first consider the terms independent of t2, i.e.,
constant term and U-only terms. These terms can be obtained by putting t2 = 0. In such a setting,
the eigenvalues are simplified as:
λ1 |t2 =0 = 1 - U, λ2 |t2=0 = 1, λ3 |t2 =0 = 1 - 2U + U , λ4 |t2 =0 = 1 - U.	(37)
15
Published as a conference paper at ICLR 2020
Note that the u-only terms cancel in λ2, so the first order after the constant term must be oft2 (could
be t2/u2 , t2 /u etc.). In the following we are going to analyze the t2 terms.
Since U is Ω(t2∙5), u2 has lower order than t2, and t2/u2 is o(1). This allows Us to do Taylor
expansion:
Ti = -3u2(1 - 4-t2-) + f (u) + higher order terms,
u2
T2 = 9u4(1 - 8-t2-) + g(u) + higher order terms,
u2
where f(u) and g(u) are u-terms only, which, by the above analysis in Eq.37, are shown to con-
tribUte nothing to λ2 . Hence, we Use the first terms of T1 and T2 above to analyze the first order
term of λ2. PlUgging in these term to the expression of λ2, and keeping the lowest order of t2, we
find a zero coefficient of the lowest order t2-term: t2/u2 .
Hence, λ2 can be written as:
λ2 = 1 - CtU2 + 0(t2),	(38)
where cis the coefficient.
On the other hand, by Eq.35 and t2 = η∕κ, We have
t2 = O(1∕κ).	(39)
u
Therefore, we can write Eq.38 as λ2 = 1 - O(1∕κ).
Case 5:	u is O(1). This is the case where the momentUm parameter is κ-independent. Using the
same argUment as in case 4, we have zero u-only terms. Then, directly taking Taylor expansion with
respect to t2 resUlts in:
λ2 = 1 - O(t2) = 1 - O(1∕κ).	(40)
□
Proof of Lemma 3. This proof follows the idea of the proof for stochastic Heavy Ball method in (9).
The idea is to examine the subspace spanned by Φj], t = 0,1,2,…，j = 1, 2, and to prove that the
eigenvector of B[j] corresponding to top eigenvalUe (i.e., eigenvalUe with largest magnitUde) is not
orthogonal to this spanned subspace. This in turn implies that there exists a non-zero component of
Φ[tj ] in the eigen direction of B[j] corresponding to top eigenvalue, and this decays/grows at a rate of
λtmax (B[j]).
Recall that Φ[tj+] 1 = B[j]Φ[tj]. Hence, if Φ[tj] has non-zero component in the eigen direction of B[j]
with top eigenvalue, then Φ[tj] should also have non-zero component in the same direction. Thus,
it suffices to show that at least one of Φ[0j] , Φ[1j] , Φ[2j] and Φ[3j] has non-zero component in the eigen
direction with top eigenvalue.
Since H is diagonal for this two-dimensional decoupled data, w[i] and w[2] evolves independently,
and we can analyze each component separately. In addition, it can be seen that each of the initial
values Wj] - (w*) [j], which is non-zero by the assumption of this lemma,just acts as a scale factor
during the training. Hence, without loss of generality, we can assume Wj] - (w* )[j] = 1, for each
j . Then, according to the recurrence relation of SGD+Nesterov in Eq.26,
[[0jj]]	=11,	[[1jj]]	=s1[1j],	[[2jj]]	=	(1 + γ)s[1j][sj][2j]	-	γs[2j]	,
-1	0	1	s1
[3j]	!	s[3j]	h(1 +	γ)2s[1j]s[2j]	-γ(1	+γ)s[2j]i	-γs[3j]	h(1 +	γ)s[1j]s[2j]	-γs[2j]i!
[2j]	=	(1+γ)s[1j]s[2j] -γs[2j]	, (41)
where sj] = 1 - η(Xj] )2, t = 1, 2, 3, ∙∙∙.
16
Published as a conference paper at ICLR 2020
Denote the vectorized form of Φ[tj] as vec(Φ[tj] ), which is a 4 × 1 column vector. We stack the
vectorized forms of Φ[0j] , Φ[1j], Φ[2j] , Φ[3j] to make a 4 × 4 matrix, denoted as M[j]:
M[j] = [vec(Φ[0j] ) vec(Φ[1j] ) vec(Φ[2j] ) vec(Φ[3j] )].	(42)
Note that Φ[tj] , t = 0, 1, 2, 3, are symmetric tensors, which implies that M[j] contains two identical
rows. Specifically, the second and third row of M[j] are identical. Therefore, the vector V =
(0, -1/√2,1 /√2, 0)t is an eigenvector of M [j] with eigenvalue 0. In fact, V is also an eigenvector
of B[j] with eigenvalue γ(1 - ησj2). Note that
det(B[j]) = γ4(1 - ησ2)2 ((1 - ησ2)2 + 5(ησ2)2) ≥ γ4(1 - ησ2)4.
Hence, V is not the eigenvector along top eigenvalue, and therefore, is orthogonal to the eigen space
with top eigenvalue.
In order to prove at least one of Φ[tj] , t = 0, 1,2, 3, has a non-zero component along the eigen
direction of top eigenvalue, it suffices to verify that M[j] is rank 3, i.e., spans a three-dimensional
space. Equivalently, we consider the following matrix
(e0j	小	4	∖
M0 := E	0,1-1,1	0,11,1	1,12,1	,	(43)
2-1,1	02,1	21,1
where we omitted the superscript [j] for simplicity of the expression. If the determinant of M0[j] is
not zero, then it is full rank, and hence M[j] spans a three-dimensional space.
Plug in the expressions in Eq.41, then we have
det(M0[j]) =	(1	-；Utj (36t2 - 18Utj + 6utj + 3tj - 3u + 1),	(44)
where tj	= ησj2 and u= 1 -	γ.	det(M0[j] ) = 0 if and only if the polynomial	36tj2	- 18utj2 +
6Utj + 3tj - 3U + 1 = 0. Solving for this equation, we have
-1 - 2u 土 √(1 + 2u)2 +8(3 - u)(u - 2)
12(2 - U)
(45)
We note that, for all u ∈ [0, 1) both tj are not positive. This means that, for all u and positive tj, the
determinant det(M0[j]) can never be zero.
Therefore, for each j = 1, 2, M0[j] is full rank, and M[j] spans a three-dimensional space, which in-
cludes the eigenvector with the top eigenvalue of B[j]. Hence, at least one ofΦ[tj], t ∈ {0, 1, 2, 3} has
non-zero component in the eigen direction with top eigenvalue. By Φ[tj+] 1 = B[j]Φ[tj], all succeeding
Φj] also have non-zero component in the eigen direction with top eigenvalue of BjL	□
Proof of Theorem 1. Lemma 1 and 2 show that, for any hyper-parameter setting (η, γ) with η > 0
and Y ∈ (0,1), either top eigenvalue of B[1] is larger than 1 or top eigenvalue of B[2] is 1 - O(1∕κ).
Hence, ∣λmaχ∣ is either greater than 1 or is 1 - O(1∕κ). Lemma 3 shows that Φt has non-zero
component along the eigenvector of B with eigenvalue λmax (B).
By Eq.28 and Lemma 1 and 2, the sequence {kΦt k}t either diverges or converges at a rate of
(1 - O(1∕κ)t. By definition of Φ in Eq.27, we have that {∣∣^t∣∣}t either diverges or converges at a
rate of (1 - O(1∕κ))t.
Note that, for the two-dimensional component decoupled data, we have
E[f(wt)] - f(w*) ≥ σ2E [kwt - w*k2]=理E [怕『].	(46)
Therefore, the convergence rate of SGD+Nesterov is lower bounded by (1 - O(1∕κ))t. Note that the
convergence rate of SGD on this data is (1 - O(1∕κ))t, hence SGD+Nesterov does not accelerate
SGD on the two-dimensional component decoupled dataset.	□
17
Published as a conference paper at ICLR 2020
ProofofCorollary 1. When η = 1/L1 and Y ∈ [0.6,1], the condition in Eq.31 is satisfied. By
Lemma 1, the top eigenvalue λmax of B[1] is larger than 1. By Lemma 3, Φ[1] has non-zero compo-
nent along the eigenvector with this top eigenvalue λmaχ. Hence, |(wt - w*, e1)∣ grows at a rate of
(λm]αx)t.	□
D Proof of Theorem 2
We first give a lemma that is useful for dealing with the mini-batch scenario:
Lemma 4. Ifsquare loss f is in the interpolated setting, i.e., there exists w* such that f (w*) = 0,
then E[HιnHTHm] - H W m (K - 1) H.
Proof.
E[HmHTHm]=，2E XXixiTHTxixiT + XXixiTHTxjXT W -KH + ——1H.
m2 L	」 m	m
i=ι	i=j
□
Proof of Theorem 2. By Eq.8, We have
Ii vt+ι- w*ι∣H-ι	=	∣∣(1 - α)vt+ ɑut- w*ι∣H-ι+ δ2 IlHm(ut- w* )∣ιH-I
`-----------{-----------} `--------V--------}
A	B
-2δ(Hm(ut - w*), (1 - Q)Vt + QUt - w*}h-1
X------------------V-------------------Z
C
Using the convexity of the norm ∣∣ ∙ ∣∣h-i and the fact that μ is the smallest non-zero eigenvalue of
the Hessian, we get
E[A] ≤ (1 - Q)IlVt- w*∣∣H-ι+ QkUt- w*ι∣H-ι ≤(1 - Q)IIVt- w*∣∣H-ι +—ι∣ut- w*ιι2.
μ
Applying Lemma 4 on the term B, we have
E[B] ≤ δ2Km∣Ut- w*∣H.	(47)
Note that E[Hm] = H, then
--Yr	_ .	,	、	,.
E[C] = -2δ(Ut — W , (1 — Q)Vt + QUt — W)
=-2δ(Ut - w*, Ut - w* + 1Q(Ut - Wt))
Q
= -2δ∣Ut — w*∣2 +----δ (∣wt — w*∣2 — kUt — w*∣2 — ∣∣wt — Ut||2)
Q
≤ -----δ∣wt — w*∣2---——δ∣Ut — w*∣2.	(48)
QQ
Therefore,
E [l∣vt+ι - w*IlH-1 ] ≤ (I-Q)kvt-w*kH-ι +-δ∣∣wt-w*||2+ (---δ) IlUt-W*∣∣2+δ2κm∣∣ut-w*IlH.
Q	∖μ Q )
On the other hand, by the fact that E[H京]W +E[H2] + m-1 H2, (see (12)),
E[∣∣wt+ι - w*∣∣2]	= EblUt- w* - ηHm(Ut - w*)∣∣2]
≤ l∣Ut - w*I2 - 2η∣∣Ut - w*∣∣H + η2Lm∣∣Ut - w*∣∣H∙	(49)
18
Published as a conference paper at ICLR 2020
Hence,
_ Γ..	.∙∙c δ..	...J
E ∣∣vt+1 - W IlH-I + αIlwt+1 - W Il2
≤	(1 -	α)∣∣vt	-	w*∣∣H-ι	+------δ∣∣wt	-	w*∣∣2 + (α∕μ -	δ)	IlUt	-	w*∣∣2
Q	×----V----'
C1
+ 3km + δη2Lm∕α - 2ηδ∕α) IIUt - w*∣∣H.
、-------------V--------------}
C2
By the condition in Eq.10, c1 ≤ 0, c2 ≤ 0, then the last two terms are non-positive. Hence,
E	l∣vt+ι	- x*llH-I	+—l∣wt+ι	-	w*Il2	≤	(1 - q)	(IIVt	- w*∣∣H-ι	+—IlWt - w*ll2)
L	α	」	∖	ɑ	)
≤ (I - α)t+1 (∣∣v0- w* ∣∣H-ι+ Qι∣w0- w*∣∣2)
□
E Proof of Theorem 4
Proof. The update rule for variable V is, as in Eq.8:
vt+1 = (1 - Q)Vt + QUt - δVm∕(Ut).	(50)
By 1 ∕μ-strong convexity of g, We have
g(vt+1) ≤ g ((I - Q)Vt + QUt) + hVg ((I - Q)Vt + QUt) , -δvmf (Ut) + δ2 7J- IlVmf (ut)∣∣2∙
2μ
(51)
Taking expectation on both sides, we get
E[g(vt+ι)] = g ((I- Q)Vt+ QUt) + hVg ((I- Q)Vt+ QUt), -δv f (Uty)+ δ2 ʒ-E[∣∣v mf (Ut )∣∣2]
2μ
≤	(1 - Q)g(vt) + Qg(Ut) - δ(1 - e)h(1 - Q)Vt + QUt - w*, Ut - w*) + δ22Lmf (Ut),
2μ
where in the last inequality we used the convexity of g and the assumption in the Theorem.
By Eq.48,
-δ(1-e)((1-Q)vt+QUt-w*, Ut-w*) ≤( —ɪ (---------------∣∣wt - w*∣∣2---——∣∣Ut - w*∣∣2).
2 Q	Q
By the strong convexity of g,
Qg(Ut) ≤ TlMUt - w*ll2.
2μ
Hence,
E[g(vt+ι)] ≤ (I-Q)g(vt)+(I-Q)ɪʒ—[IlWt-W112+(5--------------( + J-------ŋ IlUt-W*∣∣2+δ2κmf(口).
2q	∖2μ	2q	)
(52)
on the other hand,
E[∣∣wt+ι - W1I2] = IlUt- w*∣∣2 - 2η(Ut - w*, Vf(Ut)) + η2E[∣∣Vmf(Ut)∣∣2]
≤ IlUt- w 1I2 - 2ηf (Ut) + η2 ∙ 2Lmf (Ut),	(53)
where in the last inequality we used the convexity of f.
19
Published as a conference paper at ICLR 2020
Multiply a factor of δ(2-e) onto Eq.53 and add it to Eq.52, then
E [g(vt+1)+ δ⅛-1) kwt+1-w*k2
≤ (I -a)g(vt) +(I - α)-^τ;—)IlWt -w*ιι2
2α
+2 (μ -δ(I - E)) kut - w*k2
+ (α (1 - e)(η2Lm - η) + δ2κm) f (Ut) . (54)
If the hyper-parameters are selected as:
_	1	Q_ 1 -E	δ- 1
η = 2Lm,	α =^m,	= 2Lm,
then the last two terms in Eq.54 are non-positive. Hence,
E g(Vt+1)+—^^2α~k^kwt+ι -w*k2 ≤(1 -α)E g(Vt)+—^^2α~k^kwt -w*k2
which implies
E g(Vt) + ~⅛~~—kwt - w*k2 ≤ (1 - ʒ~~-)tE g(VO) + ~⅛~~—kw0 - w*k2
2α	2κm	2α
(55)
(56)
(57)
Since f (Wt) ≤ L/2 ∙ ∣∣wt - w*∣2 (by smoothness), then We have the final conclusion
1E
E[f (wt)] ≤ C ∙ (1 - -- )t,	(58)
2κm
with C being a constant.	□
F Evaluation on Synthetic Data
F.1 Discussion on Synthetic Datasets
Component Decoupled Data. The data is defined as follows (also defined in section 3):
Fix an arbitrary w* ∈ R2 and let Z be randomly drawn from the zero-mean Gaussian distribution
with variance E[z2] = 2, i.e. Z 〜N(0,2). The data points (x, y) ∈ D are constructed as follow:
σ σι ∙ z ∙ eι w.p. 0.5
σ σ2 ∙ z ∙ e2 w.p. 0.5,
and y = hw* , xi,
(59)
where e1 , e2 ∈ R2 are canonical basis vectors, σ1 > σ2 > 0.
Note that the corresponding square loss function on D is in the interpolation regime, since f (w*)
0. The Hessian and stochastic Hessian matrices turn out to be
Note that H is diagonal, which implies that stochastic gradient based algorithms applied on this
data evolve independently in each coordinate. This allows a simplified directional analysis of the
algorithms applied.
Here we list some useful results for our analysis. The fourth-moment of Gaussian variable E[z4] =
3E[z2]2 = 12. Hence, E[(x[j])2] = σj2 and E[(x[j])4] = 6σj4, where superscript j = 1, 2 is the index
for coordinates in R2. It is easy to check that κι = 6σ2∕σ2 and K = 6.
20
Published as a conference paper at ICLR 2020
Figure 7: Fast convergence of MaSS and non-acceleration of SGD+Nesterov on 3-d Gaussian data.
(left) σ12 = 1 and σ22 = 1/29; (right) σ12 = 1 and σ22 = 1/212.
Figure 6: Fast convergence of MaSS and non-acceleration of SGD+Nesterov on component decou-
pled data. (left) σ12 = 1 and σ22 = 1/212; (right) σ12 = 1 and σ22 = 1/215.
Gaussian Data. Suppose the data feature vectors {xi } are zero-mean Gaussian distributed, and
yi = (w*,Xii, ∀i, where w* is fixed but unknown. Then, by the fact that E[z1 z2z3z4] =
E[z1z2]E[z3z4] + E[z1z3]E[z2z4] + E[z1z4]E[z2z3] for zero-mean Gaussian random variables
z1 , z2 , z3 and z4, we have
-~ ~ - ， ， 、、
E[HH] = (2H + tr(H)) H,
E [HH H T H] = E[X X T H-1X X T ] = (2 + d)H,
where d is the dimension of the feature vectors. Hence L1 = 2λmax (H) + tr(H) and K = 2 + d,
and κ1 = L1 /μ. This implies a convergence rate of O(e-t/V(2+d)κ1) OfMaSS when batch size is
1. Particularly, if the feature vectors are n-dimensional, e.g., as in kernel learning, then MaSS with
mini batches of size 1 has a convergence rate of O(e-t/Vnκ1).
F.2 Evaluation of Fast Convergence of MaSS and non-acceleration of
SGD+Nesterov
In this subsection, we show additional empirical verification for the fast convergence of MaSS,
as well as the non-acceleration of SGD+Nesterov, on synthetic data. In addition, we show the
divergence of SGD+Nesterov when using the same step size as SGD and MaSS, as indicated by
Corollary 1.
We consider two families of synthetic datasets:
•	Component decoupled: (as defined in Section 3). Fix an arbitrary w* ∈ R2 with all com-
ponents non-zero. Xi is drawn from N(0, diag(2σ12, 0)) or N(0, diag(0, 2σ22)) with prob-
ability 0.5 each. yi = hw*, Xii for all i.
•	3-d Gaussian: Fix an arbitrary w * ∈ R3 with all components non-zero. Xi are indepen-
dently drawn from N(0, diag(σ12, σ12, σ22)), and yi = hw*, Xii for all i.
Non-acceleration of SGD+Nesterov and accelerated convergence of MaSS. We compare MaSS
with SGD and SGD+Nesterov on linear regression with the above datasets. Each comparison is
21
Published as a conference paper at ICLR 2020
Ooooo
Ooooo
5 4 3 2 1
SSOl 6u-£e」J_ 60-j
Figure 8: Divergence of SGD+Nesterov With large step size. Step size: η* = 1/L1 = 1/6, and
momentum parameter: γ is 0.9 or 0.99.
performed on either 3-d Gaussian or component decoupled data With fixed σ1 and σ2 . For each
setting of (σ1, σ2), We randomly select w*, and generate 2000 samples for the dataset. Batch sizes
for all algorithms are set to be 1. We report the performances of SGD, SGD+Nesterov and SGD+HB
using their best hyper-parameter setting selected from dense grid search. On the other hand, We do
not tune hyper-parameters of MaSS, but use the suggested setting by our theoretical analysis, Eq. 14.
Specifically, We use
Component decoupled:
3-d Gaussian:
15
η =	,	η?	=--,	Y	=
1	6,	2	36 + 6σ2,	Z
*	1	*	1	√20	*
η1 = 4, η2 = 5√20τ^2, Y
6 - σ2 .
6 + σ2;
_ √20 - σ2
=√20 + σ2
(61a)
(61b)
For ASGD, We use the setting suggested by (8).
Figure 6 (in addition to Fig 4) and Figure 7 shoW the curves of the compared algorithms under
various data settings. We observe that: 1) SGD+Nesterov With its best hyper-parameters is almost
identical to the optimal SGD; 2) MaSS, With the suggested hyper-parameter selections, converges
faster than all of the other algorithms, especially SGD. These observations are consistent With our
theoretical results about non-acceleration of SGD+Nesterov, Theorem 1, and accelerated conver-
gence of MaSS, Theorem 3.
Recall that MaSS differs from SGD+Nesterov by only a compensation term, this experiment illus-
trates the importance of this term. Note that the vertical axis is log scaled. Then the linear decrease
of log losses in the plots implies an exponential loss decrease, and the slopes correspond to the
coefficients in the exponents.
Divergence of SGD+Nesterov with large step size. As discussed in Corollary 1, SGD+Nesterov
diverges With step size η* = 1/L1 (When Y ∈ [0.6, 1]), Which is the optimal choice of step size
for both SGD and MaSS. We run SGD+Nesterov, With step size η* = 1/L1, to optimize the square
loss function on component decoupled data mentioned above. Figure 8 shoWs the divergence of
SGD+Nesterov With tWo common choices of momentum parameter Y: 0.9 and 0.99.
F.3 Comparison of Vaswani’s method (21) with SGD and MaSS on
Gaussian-distributed data
The analysis in VasWani et. al. (21) is based on the strong growth condition (SGC). Assuming SGC
with the parameter P on the loss function they prove convergence rate(1 — pi∕ρ2K) of their
method (called SGD With Nesterov acceleration in their paper), Where t is the iteration number. In
the following, we show that, on a simple (zero-mean) Gaussian distributed data, this rate is slower
than that of SGD, which has a rate of (1 — 1∕κ)t. On the other hand, MaSS achieves the accelerated
rate (1 — 1/P(2 + d)κ).
22
Published as a conference paper at ICLR 2020
Consider the problem of minimizing the squared loss, f (W) = Pi fi(w) = 2 Pi(WTXi - yi)2,
over a zero-mean Gaussian distributed dataset, as defined in F.1. Then,
EikVfi(w)k2 = (w - w*)TEi IxiXTXiXT] (w - w*)
=(w — w*)T(2H2 + tr(H )H )(w — w*),	(62)
whereas,
∣∣Vf (w)k2	= (w — w*)T H 2(w — w*).	(63)
According to the definition of SGC,
Ei ∣Vfi(w)∣2 ≤ ρ∣Vf(w)∣2 .	(64)
Hence the SGC parameter ρ must satisfy
tr(H)
P ≥ + λmin(H) >K，
(65)
where κ is the condition number.
In this case, the convergence rate(1 - p1∕ρ2κ) of Vaswani,s method would be slower than
(1 - 1/K3/2)t, which is slower than SGD.
On the other hand, MaSS accelerates over SGD on this dataset. Recall from Section F.1 that, for this
Gaussian data, ⅛ = 2 + d, where d is the dimension of the the data. According to Theorem 3, the
convergence rate of MaSS is (1 - 1//(2 + d)κ) , which is faster than that of SGD.
G	Mini-batch Dependence of Convergence Speed and Optimal
Hyper-parameters
G. 1 Analysis
The two critical points are defined as follow:
ml = min(Lι∕L, κ), m2 = max(Lι∕L, κ).
When m < m；, we have m < Lι∕L and m < κ, which implies Lm ≈ Lι∕m, Km ≈ κ∖∕m
and Km ≈ K∕m. Plugging into Eq.13, we find that the optimal selection of hyper-parameters is
approximated by:
η* (m) ≈ m ∙ η*(1), α* (m) ≈ m ∙ α*(1), δ*(m) ≈ m ∙ δ*(1),
and the convergence rate in Eq.16 is approximately O(e-m∙t//^ικ1κ), the latter of which indicates a
linear gains in convergence when m increases.
Now consider the following two cases: (i) For m ≥ L1∕L, we have Km ≤ 2K following the
definition of Lm in Eq.2; (ii) For m ≥ K⅛, K⅛m = K⅛∕m + (m - 1)∕m ≤ 2. When either case holds
m ≥ m；, the convergence rate (Eq.16) becomes
(√___________√m∙ t________√m∙ t__、、
max J e √2κ(⅛+m-i), e √2κι+2(m-1)κ }),
indicating that increasing the mini-batch size results in sublinear gain in the convergence speed, of at
most √m. Moreover, in the saturation regime m > mg (i.e., when both conditions (i) and (ii) hold),
the convergence rate is then upper-bounded by O(exp(-2√κ)). That implies that a single iteration
of MaSS with mini-batch size m is equivalent up to a multiplicative factor of 2 to an iteration with
full gradient.
23
Published as a conference paper at ICLR 2020
G.2 Empirical Verification
We empirically verify the three-regime partition observed in section 5 using zero-mean Gaussian
data.
In this evaluation, we set the covariant matrix of the (zero-mean) Gaussian to be:
Σ = diag(1,…，1, 2-10,…，2-10).	(66)
×---{z~} X------{-----}
8	40
and generate 2000 samples following the Gaussian distribution N(0, Σ). Recall from Example 1
that, for zero-mean Gaussian data, Li = 2λmaχ(H)+tr(H) and K = 2+d. Inthis case, Li/L ≈ 10
and K ≈ 50. The two critical values mɪ, m2 of mini-batch size are then:
m； ≈ 10, m； ≈ 50.
In the experiments, we run MaSS with a variaty
of mini-batch size m, ranging from 1 to 160,
on this Gaussian dataset. For each training pro-
cess, we compute the convergence speed s(m),
which is defined tobe the inverse of the number
of iterations needed to achieve a training error
of ε. Running 1 iteration of MaSS with mini-
batch of size m is almost as effective as running
s(m)/s(1) iterations of MaSS with mini-batch
of size 1.
Figure 9 demonstrates the convergence speed
s(m) as a function of the mini-batch size m.
We see that the three regimes defined by m；i
and m2； coincide with the analysis in subsec-
tion 5: left (m < mi；), almost linear; middle
(m ∈ [mi；, m2；]), sublinear; right (m > m2；),
saturation.
Figure 9: Convergence speed per iteration as a
function of mini-batch size m. Red solid curve:
experimental results. Larger s(m) indicates faster
convergence. Blue dash curve: theoretical lower
bound √KmKm. Critical mini-batch size values:
mi； ≈ 10, m；2 ≈ 50.
H	Experimental Setup
H.1 Neural Network Architectures
Fully-connected Network. The fully-connected neural network has 3 hidden layers, with 100
ReLU-activated neurons in each layer. After each hidden layer, there is a dropout layer with keep
probability 0.5. This network takes 784-dimensional vectors as input, and has 10 softmax-activated
output neurons. It has ≈99k trainable parameters in total.
Convolutional Neural Network (CNN). The CNN we considered has three convolutional layers
with kernel size of 5 × 5 and without padding. The first two convolutional layers have 64 channels
each, while the last one has 128 channels. Each convolutional layer is followed by a 2 × 2 max
pooling layer with stride of 2. On top of the last max pooling layer, there is a fully-connected
ReLU-activated layer of size 128 followed by the output layer of size 10 with softmax non-linearity.
A dropout layer with keep probability 0.5 is applied after the full-connected layer. The CNN has
≈576k trainable parameters in total.
Residual Network (ResNet). We train a ResNet (7) with 32 convolutional layers. The ResNet-32
has a sequence of 15 residual blocks: the first 5 blocks have an output of shape 32 × 32 × 16, the
following 5 blocks have an output of shape 16 × 16 × 32 and the last 5 blocks have an output of shape
8 × 8 × 64. On top of these blocks, there is a 2 × 2 average pooling layer with stride of2, followed by
a output layer of size 10 with softmax non-linearity. The ResNet-32 has ≈467k trainable parameters
in total.
We use the fully-connected network to classify the MNIST dataset, and use CNN and ResNet to
classify the CIFAR-10 dataset.
24
Published as a conference paper at ICLR 2020
H.2 Experimental setup for test performance of MaSS
The 3-layer CNN we use for this experiment is slightly different from the aforementioned one. We
insert a batch normalization (BN) layer after each convolution computation, and remove the dropout
layer in the fully-connected phase.
Reduction of Learning Rate. On both CNN and ResNet-32, we initialize the learning rate of
SGD, SGD+Nesterov and Mass using the same value, while that of Adam is set to 0.001, which is
common default setting. On CNN, the learning rates for all algorithms drop by 0.1 at 60 and 120
epochs, and we train for total 150 epochs. On ResNet-32, the learning rates drop by 0.1 at 150 and
225 epochs, and we train for total 300 epochs.
Whenever the learning rate of MaSS reduced, we restart the MaSS with the latest learned weights.
The reason for restarting is to avoid the mismatch between the large momentum term and small
gradient descent update after the reduction of learning rate.
Data Augmentation. We augment the Cifar-10 training data by enabling random horizontal flip,
random horizontal shift and random vertical shift of the images. For the random shift of images, we
allow a shift range of 10% of the image width or height.
Hyper-parameter Selection for MaSS. Since the reduction of learning rate may affect the sug-
gested value of δ (c.f. Eq.13), We consider (η,α, Km) as independent hyper-parameters instead,
where Km = η∕(ɑδ). Observing the fact that Km ≤ κm,, we select Km in the range (1,1∕α). In our
experiments, We set α = 0.05, corresponding to the “momentum parameter” γ being 0.90, Which is
commonly used in Nesterov’s method. We find that good choices of KKm often locate in the interval
[2, 25] for mini-batch size m = 64 in our experiments.
The classification results of MaSS in Table 6 are obtained under the following hyper-parameter
settings:
•	CNN:
η = 0.01 (initial), α = 0.05, KKm = 3;
η = 0.3 (initial), α = 0.05, KKm = 6.
•	ResNet-32:
η = 0.1 (initial), α = 0.05, KKm = 2;
η = 0.3 (initial), α = 0.05, KKm = 24.
25