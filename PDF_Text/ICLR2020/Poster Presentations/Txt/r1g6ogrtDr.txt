Published as a conference paper at ICLR 2020
Co-Attentive Equivariant Neural Networks:
Focusing Equivariance On Transformations
Co-Occurring In Data
David W. Romero
Vrije Universiteit Amsterdam
d.w.romeroguzman@vu.nl
Mark Hoogendoorn
Vrije Universiteit Amsterdam
m.hoogendoorn@vu.nl
Ab stract
Equivariance is a nice property to have as it produces much more parameter ef-
ficient neural architectures and preserves the structure of the input through the
feature mapping. Even though some combinations of transformations might never
appear (e.g., an upright face with a horizontal nose), current equivariant archi-
tectures consider the set of all possible transformations in a transformation group
when learning feature representations. Contrarily, the human visual system is able
to attend to the set of relevant transformations occurring in the environment and
utilizes this information to assist and improve object recognition. Based on this
observation, we modify conventional equivariant feature mappings such that they
are able to attend to the set of co-occurring transformations in data and general-
ize this notion to act on groups consisting of multiple symmetries. We show that
our proposed co-attentive equivariant neural networks consistently outperform
conventional rotation equivariant and rotation & reflection equivariant neural net-
works on rotated MNIST and CIFAR-10.
1	Introduction
Thorough experimentation in the fields of psychology and neuroscience has provided support to the
intuition that our visual perception and cognition systems are able to identify familiar objects despite
modifications in size, location, background, viewpoint and lighting (Bruce & Humphreys, 1994).
Interestingly, we are not just able to recognize such modified objects, but are able to characterize
which modifications have been applied to them as well. As an example, when we see a picture
of a cat, we are not just able to tell that there is a cat in it, but also its position, its size, facts
about the lighting conditions of the picture, and so forth. Such observations suggest that the human
visual system is equivariant to a large transformation group containing translation, rotation, scaling,
among others. In other words, the mental representation obtained by seeing a transformed version
of an object, is equivalent to that of seeing the original object and transforming it mentally next.
These fascinating abilities exhibited by biological visual systems have inspired a large field of re-
search towards the development of neural architectures able to replicate them. Among these, the
most popular and successful approach is the Convolutional Neural Network (CNN) (LeCun et al.,
1989), which incorporates equivariance to translation via convolution. Unfortunately, in counterpart
to the human visual system, CNNs do not exhibit equivariance to other transformations encountered
in visual data (e.g., rotations). Interestingly, however, if an ordinary CNN happens to learn rotated
copies of the same filter, the stack of feature maps becomes equivariant to rotations even though
individual feature maps are not (Cohen & Welling, 2016). Since ordinary CNNs must learn such
rotated copies independently, they effectively utilize an important number of network parameters
suboptimally to this end (see Fig. 3 in Krizhevsky et al. (2012)). Based on the idea that equivari-
ance in CNNs can be extended to larger transformation groups by stacking convolutional feature
maps, several approaches have emerged to extend equivariance to, e.g., planar rotations (Dieleman
et al., 2016; Marcos et al., 2017; Weiler et al., 2018; Li et al., 2018), spherical rotations (Cohen
et al., 2018; Worrall & Brostow, 2018; Cohen et al., 2019), scaling (Marcos et al., 2018; Worrall &
Welling, 2019) and general transformation groups (Cohen & Welling, 2016), such that transformed
copies of a single entity are not required to be learned independently.
1
Published as a conference paper at ICLR 2020
Figure 1: Our visual system infers object identities according to their size, location and orientation
in a scene. In this blurred picture, observers describe the scene as containing a car and a pedestrian
in the street. However, the pedestrian is in fact the same shape as the car, except for a 90° rotation.
The atypicality of this orientation for a car within the context defined by the street scene causes the
car to be recognized as a pedestrian. Extracted from Oliva & Torralba (2007).
Although incorporating equivariance to arbitrary transformation groups is conceptually and theo-
retically similar1, evidence from real-world experiences motivating their integration might strongly
differ. Several studies in neuroscience and psychology have shown that our visual system does not
react equally to all transformations we encounter in visual data. Take, for instance, translation and
rotation. Although we easily recognize objects independently of their position of appearance, a large
corpus of experimental research has shown that this is not always the case for in-plane rotations. Yin
(1969) showed that mono-oriented objects, i.e., complex objects such as faces which are customarily
seen in one orientation, are much more difficult to be accurately recognized when presented upside-
down. This behaviour has been reproduced, among others, for magazine covers (Dallett et al., 1968),
symbols (Henle, 1942) and even familiar faces (e.g., from classmates) (Brooks & Goldstein, 1963).
Intriguingly, Schwarzer (2000) found that this effect exacerbates with age (adults suffer from this
effect much more than children), but, adults are much faster and accurate in detecting mono-oriented
objects in usual orientations. Based on these studies, we draw the following conclusions:
•	The human visual system does not perform (fully) equivariant feature transformations to visual
data. Consequently, it does not react equally to all possible input transformations encountered
in visual data, even if they belong to the same transformation group (e.g., in-plane rotations).
•	The human visual system does not just encode familiarity to objects but seems to learn through
experience the poses in which these objects customarily appear in the environment to assist and
improve object recognition (Freire et al., 2000; Riesenhuber et al., 2004; Sinha et al., 2006).
Complementary studies (Tarr & Pinker, 1989; Oliva & Torralba, 2007) suggest that our visual system
encodes orientation atypicality relative to the context rather than on an absolute manner (Fig. 1).
Motivated by the aforementioned observations we state the co-occurrence envelope hypothesis:
The Co-occurrence Envelope Hypothesis. By allowing equivariant feature mappings to detect
transformations that co-occur in the data and focus learning on the set formed by these co-occurrent
transformations (i.e., the co-occurrence envelope of the data), one is able to induce learning of more
representative feature representations of the data, and, resultantly, enhance the descriptive power of
neural networks utilizing them. We refer to one such feature mapping as co-attentive equivariant.
Identifying the co-occurrence envelope. Consider a rotation equivariant network receiving two
copies of the same face (Fig. 2a). A conventional rotation equivariant network is required to perform
inference and learning on the set of all possible orientations of the visual patterns constituting a face
regardless of the input orientation (Fig. 2b). However, by virtue of its rotation equivariance, it is
able to recognize rotated faces even if it is trained on upright faces only. A possible strategy to
simplify the task at hand could be to restrict the network to react exclusively to upright faces (Fig.
2c). In this case, the set of relevant visual pattern orientations becomes much smaller, at the expense
of disrupting equivariance to the rotation group. Resultantly, the network would risk becoming
unable to detect faces in any other orientation than those it is trained on. A better strategy results
from restricting the set of relevant pattern orientations by defining them relative to one another
1It is achieved by developing feature mappings that utilize the transformation group in the feature mapping
itself (e.g., translating a filter in the course of a feature transformation is used to obtain translation equivariance).
2
Published as a conference paper at ICLR 2020
(a)	(b)	(c)	(d)
Figure 2: Effect of multiple attention strategies for the prioritization of relevant pattern orientations
in rotation equivariant networks for the task of face recognition. Given that all attention strategies
are learned exclusively from upright faces, we show the set of relevant directions for the recognition
of faces in two orientations (Fig. 2a) obtained by: no attention (Fig. 2b), attending to the pattern
orientations of appearance independently (Fig. 2c) and, attending to the pattern orientations of
appearance relative to one another (Fig. 2d). Built upon Figure 1 from Schwarzer (2000).
(e.g., mouth orientation relative to the eyes) as opposed to absolutely (e.g., upright mouth) (Fig.
2d). In such a way, we are able to exploit information about orientation co-occurrences in the data
without disrupting equivariance. The set of co-occurrent orientations in Fig. 2d corresponds to the
co-occurrence envelope of the samples in Fig. 2a for the transformation group defined by rotations.
In this work, we introduce co-attentive equivariant feature mappings and apply them on existing
equivariant neural architectures. To this end, we leverage the concept of attention (Bahdanau et al.,
2014) to modify existing mathematical frameworks for equivariance, such that co-occurrent trans-
formations can be detected. It is critical not to disrupt equivariance in the attention procedure as to
preserve it across the entire network. To this end, we introduce cyclic equivariant self-attention, a
novel attention mechanism able to preserve equivariance to cyclic groups.
Experiments and results. We explore the effects of co-attentive equivariant feature mappings for
single and multiple symmetry groups. Specifically, we replace conventional rotation equivariant
mappings in p4-CNNs (Cohen & Welling, 2016) and DRENs (Li et al., 2018) with co-attentive
ones. We show that co-attentive rotation equivariant neural networks consistently outperform their
conventional counterparts in fully (rotated MNIST) and partially (CIFAR-10) rotational settings.
Subsequently, we generalize cyclic equivariant self-attention to multiple similarity groups and apply
it on p4m-CNNs (Cohen & Welling, 2016) (equivariant to rotation and mirror reflections). Our
results are in line with those obtained for single symmetry groups and support our stated hypothesis.
Contributions.
•	We propose the co-occurrence envelope hypothesis and demonstrate that conventional equiv-
ariant mappings are consistently outperformed by our proposed co-attentive equivariant ones.
•	We generalize co-attentive equivariant mappings to multiple symmetry groups and provide, to
the best of our knowledge, the first attention mechanism acting generally on symmetry groups.
2	Preliminaries
Equivariance. We say that a feature mapping f : X → Y is equivariant to a (transformation) group
G (or G-equivariant) if it commutes with actions of the group G acting on its domain and codomain:
f(TgX(x))=TgY(f(x)) ∀g∈G,x∈X	(1)
where Tg(O denotes a group action in the corresponding space. In other words, the ordering in
which we apply a group action Tg and the feature mapping f is inconsequential. There are multiple
reasons as of why equivariant feature representations are advantageous for learning systems. Since
group actions TgX produce predictable and interpretable transformations TgY in the feature space,
the hypothesis space of the model is reduced (Weiler et al., 2018) and the learning process simplified
(Worrall et al., 2017). Moreover, equivariance allows the construction of L-layered networks by
3
Published as a conference paper at ICLR 2020
stacking several equivariant feature mappings {f(1), ..., f (l), ..., f(L)} such that the input structure
as regarded by the group G is preserved (e.g., CNNs and input translations). As a result, an arbitrary
intermediate network representation (f (l) ◦ ... ◦ f (1))(x), l ∈ L, is able to take advantage of the
structure of x as well. Invariance is an special case of equivariance in which TgY = IdY , the
identity, and thus all group actions in the input space are mapped to the same feature representation.
Equivariant neural networks. In neural networks, the integration of equivariance to arbitrary
groups G has been achieved by developing feature mappings f that utilize the actions of the group
G in the feature mapping itself. Interestingly, equivariant feature mappings encode equivariance as
parameter sharing with respect to G, i.e., the same weights are reused for all g ∈ G. This makes
the inclusion of larger groups extremely appealing in the context of parameter efficient networks.
Conventionally, the l-th layer of a neural network receives a signal x(l) (u, λ) (where u ∈ Z2 is the
spatial position and λ ∈ Λl is the unstructured channel index, e.g., RGB channels in a color image),
and applies a feature mapping f (l) : Z2 × Λl → Z2 × Λl+1 to generate the feature representation
x(l+1) (u, λ). In CNNs, the feature mapping f(l) := fT(l) is defined by a convolution2 (?R2) between
the input signal x(l) and a learnable convolutional filter Wλ(l0),λ, λ0 ∈ Λl, λ ∈ Λl+1:
x(l+1) (u, λ) = [x(l) ?R2 Wλ(l0),λ](u, λ) = X x(l)(u + u0, λ0)Wλ(l0),λ(u0)	(2)
λ0,u0
By sliding Wλ(l0),λ across u, CNNs are able to preserve the spatial structure of the input x through the
feature mapping fTl and successfully provide equivariance to the translation group T = (Z2 , +).
The underlying idea for the extension of equivariance to larger groups in CNNs is conceptually
equivalent to the strategy utilized by LeCun et al. (1989) for translation equivariance. Consider, for
instance, the inclusion of equivariance to the set of rotations by θr degrees3: Θ = {θr = r 产}凿.
To this end, we modify the feature mapping f(l) := fR(l) : Z2× Θ × Λl → Z2 × Θ × Λl+1 to include
the rotations defined by Θ. Let x(l) (u, r, λ) and Wλ(0l),λ(u, r) be the input and the convolutional filter
with an affixed index r for rotation. The roto-translational convolution (?R2 乂㊀)fR) is defined as:
x0+1)(u,r,λ) = [x0) *r2oθ Wλ0,λ](u,r,λ)= X x°)(u + u0,r0,λ0)wλθ,λ(θru0,r0 - r) (3)
λ0,r0,u0
Since fR(l) produces (dim(Θ) = rmax) times more output feature maps than fT(l), we need to learn
much smaller convolutional filters Wλ(l0),λ to produce the same number of output feature channels.
Learning equivariant neural networks. Consider the change of variables g = u, G = Z2, g ∈ G
and g = (u, r), G = Z2o Θ, g ∈ G in Eq. 2 and Eq. 3, respectively. In general, neural networks are
learned via backpropagation (LeCun et al., 1989) by iteratively applying the chain rule of derivation
to update the network parameters. Intuitively, the networks outlined in Eq. 2 and Eq. 3 obtain
feedback from all g ∈ G and, resultantly, are inclined to learn feature representations that perform
optimally on the entire group G. However, as outlined in Fig. 2 and Section 1, several of those
feature combinations are not likely to simultaneously appear. Resultantly, the hypothesis space of
the model as defined by Weiler et al. (2018) might be further reduced.
Note that this reasoning is tightly related to existing explanations for the large success of spatial (Xu
et al., 2015; Woo et al., 2018; Zhang et al., 2018) and temporal (Luong et al., 2015; Vaswani et al.,
2017; Mishra et al., 2017; Zhang et al., 2018) attention in deep learning architectures.
3	Co-Attentive Equivariant Neural Networks
In this section we define co-attentive feature mappings and apply them in the context of equivariant
neural networks (Figure 3). To this end, we introduce cyclic equivariant self-attention and utilize
it to construct co-attentive rotation equivariant neural networks. Subsequently, we show that cyclic
equivariant self-attention is extendable to larger symmetry groups and make use of this fact to con-
struct co-attentive neural networks equivariant to rotations and mirror reflections.
2Formally it is as a correlation. However, we hold on to the standard deep learning terminology.
3The reader may easily verify that Θ (and hence Z2 o Θ, with (o) the semi-direct product) forms a group.
4
Published as a conference paper at ICLR 2020
(l□,H,[l,∙)j-------(IL∙Q,∙)
Figure 3: Co-attentive equivariant feature mappings acting on the groups p4 (top) and p4m (bottom).
In order to learn co-attentive equivariant representations, cyclic equivariant self-attention AC is
applied on top of the output of a conventional equivariant feature mapping (here p4 and p4m group
convolutions, respectively). Resultantly, the group convolution responses are modulated based on
their assessed relevance. For multiple symmetry groups, the group convolution responses must
be rearranged in a vector structure so that the permutation laws of AC correspond to those of the
composing group symmetries. Same colors in AC denote equal weights. The circulant (block)
structure of AC ensures that equivariance to the corresponding group is preserved through the course
of attention. Consequently, if the input is rotated (or mirrored in p4m), the attention mask shown
here is transformed accordingly. Built upon Figures 1 and 2 from Cohen & Welling (2016).
3.1	Co-Attentive Rotation Equivariant Neural Networks
To allow rotation equivariant networks to utilize and learn co-attentive equivariant representations,
we introduce an attention operator A(l) on top of the roto-translational convolution fR(l) with which
discernment along the rotation axis r of the generated feature responses x(l) (u, r, λ) is possible.
Formally, our co-attentive rotation equivariant feature mapping fR(l) is defined as follows:
X(I+1) = fR)(X(I)) = A(I)(fR)(X(I))) = A(I)([x(I) *R2χθ Wλ0)λ])	⑷
Theoretically, A(l) could be defined globally over fR(l) (x(l)) (i.e., simultaneously along u, r, λ)
as depicted in Eq. 4. However, we apply attention locally to: (1) grant the algorithm enough
flexibility to attend locally to the co-occurrence envelope of feature representations and, (2) utilize
attention exclusively along the rotation axis r, such that our contributions are clearly separated from
those possibly emerging from spatial attention. To this end, we apply attention pixel-wise on top of
fR(l) (X(l)) (Eq. 5). Furthermore, we assign a single attention instance A(λl) to each learned feature
representation and utilize it across the spatial dimension of the output feature maps4:
x(l+1)(u,r,λ) = AS) ({x(l+1)(u,^,λ)}^ma1)(r)	(5)
Attention and self-attention. Consider a source vector X = (X1 , ..., Xn) and a target vector y =
(y1 , ..., ym ). In general, an attention operator A leverages information from the source vector X (or
multiple feature mappings thereof) to estimate an attention matrix A ∈ [0, 1]n×m, such that: (1) the
element Ai,j provides an importance assessment of the source element Xi with reference to the target
element yj and (2) the sum of importance over all Xi is equal to one: i Ai,j = 1. Subsequently,
the matrix A is utilized to modulate the original source vector X as to attend to a subset of relevant
source positions with regard to yj : Xj = (A：,j)T 0 X (where is the Hadamard product). A special
case of attention is that of self-attention (Cheng et al., 2016), in which the target and the source
vectors are equal (y := X). In other words, the attention mechanism estimates the influence of the
sequence X on the element Xj for its weighting.
4For a more meticulous discussion on how Eq. 5 attains co-occurrent attention, see Appendix A.
5
Published as a conference paper at ICLR 2020
In general, the attention matrix5 A ∈ [0, 1]n×m is constructed via nonlinear space transforma-
tions fa : Rn → Rn×m of the source vector x, on top of which the Softmax function is applied:
A：,j = Softmax(fa(x)：,j). This ensures that the properties previously mentioned hold. Typically,
the mappings /区 found in literature take feature transformation pairs of X as input (e.g., {s, H} in
RNNs (Luong et al., 2015), {Q, K} in self-attention networks (Vaswani et al., 2017)), and perform
(non)-linear mappings on top of it, ranging from multiple feed-forward layers (Bahdanau et al.,
2014) to several operations between the transformed pairs (Luong et al., 2015; Vaswani et al., 2017;
Mishra et al., 2017; Zhang et al., 2018). Due to the computational complexity of these approaches
and the fact that We do extensive pixel-wise usage of fa on every network layer, their direct integra-
tion in our framework is computationally prohibitive. To circumvent this problem, we modify the
usual self-attention formulation as to enhance its descriptive power in a much more compact setting.
Compact local self-attention. Initially, we relax the range of values ofA from [0, 1]n×n to Rn×n.
This allows us to encode much richer relationships between element pairs (xi, xj) at the cost of
less interpretability. Subsequently, we define A = XT Θ A, where A ∈ Rn×n is a matrix of
learnable parameters. Furthermore, instead of directly applying softmax on the columns of A, we
first sum over the contributions of each element Xi to obtain a vector a = {Pi Ai,j}jn=1, which
is then passed to the softmax function. Following Vaswani et al. (2017), we prevent the softmax
function from reaching regions of low gradient by scaling its argument by (,dim(A)) T = (1 /n):
a = Softmax((1 /n) a). Lastly, we counteract the contractive behaviour of the softmax function by
normalizing a before weighting X as to preserve the magnitude range of its argument. This allows
us to use A in deep architectures. Our compact self-attention mechanism is summarized as follows:
a = {PiAi,j}jn=1 = Pi(XT Θ Aa)i,j = XAa	(6)
aa = Softmax((1 / n) a)	(7)
X = A(X) = (a / max(a)) Θ X	(8)
The cyclic equivariant self-attention operator AC. Consider {X(u, r, λ)}rrm=ax1, the vector of re-
sponses generated by a roto-translational convolution fR stacked along the rotation axis r. By
applying self-attention along r, we are able to generate an importance matrix A ∈ Rrmax ×rmax relat-
ing all pairs of (θi, θj)-rotated responses in the rotational group Θ at a certain position. We refer to
this attention mechanism as full self-attention (AF). Although AF is able to encode arbitrary linear
source-target relationships for each target position, it is not restricted to conserve equivariance to
Θ. Resultantly, we risk incurring into the behavior outlined in Fig. 2c. Before we further elaborate
on this issue, we introduce the cyclic permutation operator Pi, which induces a cyclic shift of i
positions on its argument: σPi (Xj) = X(j+i)mod(dim(x)), ∀Xj ∈ X.
Consider a full self-attention operator AF acting on top of a roto-translational convolution fR . Let
P be an input pattern to which /r only produces a strong activation in the feature map X(r)=
/r(p)(r), r ∈ {r}r=1∙ Intuitively, during learning, only the corresponding attention coefficients
Aa；,r in AF would be significantly increased. Now, consider the presence of the input pattern θ%p, a
θi-rotated variant ofp. By virtue of the rotational equivariance property of the feature mapping fR,
we obtain (locally) an exactly equal response to that ofp up to a cyclic permutation ofi positions on
r, and thus, we obtain a strong activation in the feature map Pi(X(r)) = X(σpi(r)). We encounter
two problems in this setting: AF is not be able to detect that p and θip correspond to the exact same
input pattern and, as each but the attention coefficients Aa:,j is small, the network might considerably
damp the response generated by θip. As a result, the network might (1) squander important feedback
information during learning and (2) induce learning of repeated versions of the same pattern for
different orientations. In other words, AF does not behave equivariantly as a function of θi .
Interestingly, we are able to introduce prior-knowledge into the attention model by restricting the
structure ofA. By leveraging the idea of equivariance to the cyclic group Cn, we are able to solve the
problems exhibited by AF and simultaneously reduce the number of additional parameters required
by the self-attention mechanism (from rm2 ax to rmax). Consider again the input patterns p and θip. We
incorporate the intuition that p and θip are one and the same entity, and thus, fR (locally) generates
the same output feature map up to a cyclic permutation Pi: fR(θip) = Pi (fR(p)). Consequently,
the attention mechanism should produce the exact same output for both p and θip up to the same
cyclic permutation Pi. In other words, A (and thus Aa) should be equivariant to cyclic permutations.
5Technically, each column of A is restricted to a simplex and hence A lives in a subspace of [0, 1]n×m.
6
Published as a conference paper at ICLR 2020
A well-known fact in mathematics is that a matrix A is equivariant with respect to cyclic permuta-
tions of the domain if and only if it is Circulant (Alkarni, 2001; Ahlander & MUnthe-Kaas, 2005).
We make use of this certitude and leverage the concept of circulant matrices to impose cyclic equiv-
ariance to the structure of A. Formally, a circulant matrix C ∈ Rn×n is composed of n cyclic
permUtations of its defining vector c = {ci}in=1, sUch that its j-th colUmn is a cyclic permUtation of
j - 1 positions of c: C:,j = Pj-1(c)T. We construct our cyclic equivariant self-attention operator
Ac by defining A as a circulant matrix specified by a learnable attention vector ac = {ac}鸣：
A = {P j-1(ac )T j=ι
(9)
and subsequently applying Eqs. 6 - 8. Resultantly, Ac is able to assign the responses generated
by fR for rotated versions of an input pattern p to a unique entity： fR(θip) = Pi(fR(p)), and dy-
namically adjust its output to the angle of appearance θi , such that the attention operation does not
disrupt its propagation downstream the network： Ac (fR(θip)) = Pi(Ac (fR(p))). Consequently,
the attention weights ac are updated equally regardless of specific values of θi . Due to these prop-
erties, Ac does not incur in any of the problems outlined earlier in this section. Conclusively, our
co-attentive rotation equivariant feature mapping fR(l) is defined as follows：
χ(l + 1)(u,r, λ) = fR) (X(I))(U,r, R = AC(I) ([x(I) ?R2o© Wv,λ])(u,r, λ)	(IO)
Note that a co-attentive equivariant feature mapping fR is approximately equal (up to a normalized
softmax operation (Eq. 8)) to a conventional equivariant one fR, ifA = αI for any α ∈ R.
3.2 EXTENDING Ac TO MULTIPLE SYMMETRY GROUPS
The self-attention mechanisms outlined in the previous section are easily extendable to larger groups
consisting of multiple symmetries. Consider, for instance, the group θrm of rotations by θr degrees
and mirror reflections m defined analogously to the group p4m in Cohen & Welling (2016). Let
p(u, r, m, λ) be an input signal with an affixed index m ∈ {m0, m1} for mirror reflections (m1
indicates mirrored) and fθrm be a group convolution (Cohen & Welling, 2016) on the θrm group.
The group convolution fθrm produces two times as many output channels (2rmax : m0rmax+m1rmax)
as those generated by the roto-translational convolution fR (Eq. 3, Fig. 3).
Full self-attention AF can be integrated directly by modulating the output of fθrm as depicted in
Sec. 3.1 with A ∈ R2rmax ×2rmax. Here, AF relates each of the group convolution responses with one
another. However, just as for fR, AF disrupts the equivariance property of fθrm to the θrm group.
Similarly, the cyclic equivariant self-attention operator Ac can be extended to multiple symmetry
groups as well. Before we continue, we introduce the cyclic permutation operator Pi,t , which
induces a cyclic shift of i positions on its argument along the transformation axis t. Consider
the input patterns p and θip outlined in the previous section and mp, a mirrored instance of
p. Let x(u, r, m, λ) = fθrm(p)(u, r, m, λ) be the response of the group convolution fθrm for
the input pattern p. By virtue of the rotation equivariance property of fθrm, the generated re-
sponse for θip is equivalent to that of p up to a cyclic permutation of i positions along the ro-
tation axis r： fθrm(θip)(u,r,m, λ) = Pi,r(fθrm(p))(u, r, m, λ) = x(u, σPi (r), m, λ). Sim-
ilarly, by virtue of the mirror equivariance property of fθrm, the response generated by mp is
equivalent to that of p up to a cyclic permutation of one position along the mirroring axis m：
fθr m (mp)(u, r, m, λ) = P 1,m (fθr m (p))(u, r, m, λ) = x(u, r, σP1 (m), λ). Note that if we take
two elements from a group g, h, their composition (gh) is also an element of the group. Resultantly,
fθrm((mθi)p)(u, r, m, λ) = (P1,m ◦ Pi,r)(fθrm(p))(u,r,m, λ) = P1,m(Pi,r(x))(u,r,m,λ) =
P 1,m (x)(u, σPi (r), m, λ) = x(u, σPi (r), σP1 (m), λ).
In other words, in order to extend Ac to the θrm group, it is necessary to restrict the structure of
A such that it respects the permutation laws imposed by the equivariant mapping fθrm. Let us
rewrite x(u, r, m, λ) as x(u, g, λ), g = (mr) ∈ {mo, mi} X {r}^. In this case, We must impose
a circulant block matrix structure on A such that: (1) the composing blocks permute internally
as defined by Pi,r and (2) the blocks themselves permute with one another as defined by P1,m .
Formally, A is defined as:
Ai
A2
Ai
(11)
7
Published as a conference paper at ICLR 2020
1	r Λ^ _ TrhY V r 1 ∙ _ C-ι rʌl	∙	1	, ∙	∕τr-<	r∖∖ ɪ	, ,1 ,1	1	r∙ . 1
where {Ai ∈ Rrmax×rmax}, i ∈ {1, 2} are circulant matrices (Eq. 9). Importantly, the ordering of the
permutation laws in A is interchangeable if the input vector is modified accordingly, i.e., g = (rm).
Conclusively, cyclic equivariant self-attention AC is directly extendable to act on any G-equivariant
feature mapping fG, and for any symmetry group G, if the group actions TgY produce cyclic permu-
tations on the codomain of fG. To this end, one must restrict the structure of A to that ofa circulant
block matrix, such that all permutation laws of TgY hold: TgY (AC(fG)) = AC (TgY (fG)), ∀g ∈ G.
4	Experiments
Experimental Setup. We validate our approach by exploring the effects of co-attentive equivari-
ant feature mappings for single and multiple symmetry groups on existing equivariant architectures.
Specifically, we replace conventional rotation equivariant mappings in p4-CNNs (Cohen & Welling,
2016) and DRENs (Li et al., 2018) with co-attentive equivariant ones and evaluate their effects
in fully (rotated MNIST) and partially (CIFAR-10) rotational settings. Similarly, we evaluate co-
attentive equivariant maps acting on multiple similarity groups by replacing equivariant mappings
in p4m-CNNs (Cohen & Welling, 2016) (equivariant to rotation and mirror reflections) likewise.
Unless otherwise specified, we replicate as close as possible the same data processing, initialization
strategies, hyperparameter values and evaluation strategies utilized by the baselines in our exper-
iments. Note that the goal of this paper is to study and evaluate the relative effects obtained by
co-attentive equivariant networks with regard to their conventional counterparts. Accordingly, we
do not perform any additional tuning relative to the baselines. We believe that improvements on our
reported results are feasible by performing further parameter tuning (e.g., on the network structure
or the used hyperparameters) on the proposed co-attentive equivariant networks.
The additional learnable parameters, i.e., those associated to the cyclic self-attention operator (A)
are initialized identically to the rest of the layer. Subsequently, we replace the values of A along
the diagonal by 1 (i.e., diag(Ainit) = 1) such that Ainit approximately resembles the identity I and,
hence, co-attentive equivariant layers are initially approximately equal to equivariant ones.
Rotated MNIST. The rotated MNIST dataset (Larochelle et al., 2007) contains 62000 gray-scale
28x28 handwritten digits uniformly rotated on the entire circle [0, 2π). The dataset is split into
training, validation and tests sets of 10000, 2000 and 50000 samples, respectively. We replace
rotation equivariant layers in p4-CNN (Cohen & Welling, 2016), DREN and DRENMaxPooling
(Li et al., 2018) with co-attentive ones. Our results show that co-attentive equivariant networks
consistently outperform conventional ones (see Table 1).
CIFAR-10. The CIFAR-10 dataset (Krizhevsky et al., 2009) consists of 60000 real-world 32x32
RGB images uniformly drawn from 10 classes. Contrarily to the rotated MNIST dataset, this dataset
does not exhibit rotation symmetry. The dataset is split into training, validation and tests sets of
40000, 10000 and 10000 samples, respectively. We replace equivariant layers in the p4 and p4m
variations of the All-CNN (Springenberg et al., 2014) and the ResNet44 (He et al., 2016) proposed
by Cohen & Welling (2016) with co-attentive ones. Likewise, we modify the r_x4-VariationS of
the NIN (Lin et al., 2013) and ResNet20 (He et al., 2016) models proposed by Li et al. (2018) in
the same manner. Our results show that co-attentive equivariant networks consistently outperform
conventional ones in this setting as well (see Table 1).
Training convergence of equivariant networks. Li et al. (2018) reported that adding too many ro-
tational equivariant (isotonic) layers decreased the performance of their models on CIFAR-10. As a
consequence, they did not report results on fully rotational equivariant networks for this setting and
attributed this behaviour to the non-symmetricity of the data. We noticed that, with equal initial-
ization strategies, rotational equivariant CNNs were much more prone to divergence than ordinary
CNNs. This behaviour can be traced back to the additional feedback resulting from roto-translational
convolutions (Eq. 3) compared to ordinary ones (Eq. 2). After further analysis, we noticed that the
data preprocessing strategy utilized by Li et al. (2018) leaves some very large outlier values in the
data (|x| >100), which strongly contribute to the behaviour outlined before.
In order to evaluate the relative contribution of co-attentive equivariant neural networks we con-
structed fully equivariant DREN architectures based on their implementation. Although the obtained
results were much worse than those originally reported in Li et al. (2018), we were able to stabilize
8
Published as a conference paper at ICLR 2020
Table 1: Comparison of conventional equivariant and co-attentive equivariant neural networks.
Values between parenthesis correspond to relevant results obtained from our own experiments.
	Rotated MNIST					CIFAR-10			
Network	Test Error (%)	Param.	Network	Test Error (%)	Param.
Z2CNN	5.03 ± 0.002	21.75k	All-CNN	9.44	1.372M
p4-CNN	2.28 ± 0.0004	19.88k	p4-All-CNN	8.84	1.371M
a-p4-CNN	2.06 ± 0.0429	20.76k	a-p4-All-CNN	7.68	1.373M
DREN	1.78 (1.99)	19.88k	p4m-All-CNN	7.59	1.219M
a-DREN	1.674	20.76k	a-p4m-All-CNN	6.92	1.223M
DRENMaxPool.	1.56 (1.60)	24.68k	ResNet44	9.45 (9.85)	2.639M
a-DRENMaxPool.	1.34	25.68k	p4m-ResNet44	6.46 (9.47)	2.623M
			a-p4m-ResNet44	9.12	2.632M
			NIN	10.41 (15.92)	0.967M
			r-NINx4	14.96	0.958M
			a-r-NINx4	13.67	0.968M
			ResNet20	9.00 (12.32)	0.335M
			r-ResNet20x4	12.31	0.333M
			a-r-ResNet20x4	11.32	0.339M
training by clipping input values to the 99 percentile of the data (|x| ≤2.3) and reducing the learning
rate to 0.01, such that the same hyperparameters could be used across all network types. The ob-
tained results (see Table 1) signalize that DREN networks are comparatively better than CNNs both
in fully and partially rotational settings, contradictorily to the conclusions drawn in Li et al. (2018).
This behaviour elucidates that although the inclusion of equivariance to larger transformation groups
is beneficial both in terms of accuracy and parameter efficiency, one must be aware that such benefits
are directly associated with an increase of the network susceptibility to divergence during training.
5	Discussion and Future Work
Our results show that co-attentive equivariant feature mappings can be utilized to enhance conven-
tional equivariant ones. Interestingly, co-attentive equivariant mappings are beneficial both in par-
tially and fully rotational settings. We attribute this to the fact that a set of co-occurring orientations
between patterns can be easily defined (and exploited) in both settings. It is important to note that
we utilized attention independently over each spatial position u on the codomain of the correspond-
ing group convolution. Resultantly, we were restricted to mappings of the form xA, which, in turn,
constraint our attention mechanism to have a circulant structure in order to preserve equivariance
(since group actions acting in the codomain of the group convolution involve cyclic permutations
and cyclic self-attention is applied in the codomain of the group convolution).
In future work, we want to extend the idea presented here to act on the entire group simultaneously
(i.e., along u as well). By doing so, we lift our current restriction to mappings of the form xA and
therefore, may be able to develop attention instances with enhanced descriptive power. Following
the same line of though, we want to explore incorporating attention in the convolution operation
itself. Resultantly, one is not restricted to act exclusively on the codomain of the convolution, but
instead, is able to impose structure in the domain of the mapping as well. Naturally, such an ap-
proach could lead to enhanced descriptiveness of the incorporated attention mechanism. Moreover,
we want to utilize and extend more complex attention strategies (e.g., Bahdanau et al. (2014); Lu-
ong et al. (2015); Vaswani et al. (2017); Mishra et al. (2017)) such that they can be applied to large
transformation groups without disrupting equivariance. As outlined earlier in Section 3.1, this be-
comes very challenging from a computational perspective as well, as it requires extensive usage of
the corresponding attention mechanism. Resultantly, an efficient implementation thereof is manda-
tory. Furthermore, we want to extend co-attentive equivariant feature mappings to continuous (e.g.,
Worrall et al. (2017)) and 3D space (e.g., Cohen et al. (2018); Worrall & Brostow (2018); Cohen
et al. (2019)) groups, and for applications other than visual data (e.g., speech recognition).
9
Published as a conference paper at ICLR 2020
Finally, we believe that our approach could be refined and extended to a first step towards deal-
ing with the enumeration problem of large groups (Gens & Domingos, 2014), such that functions
acting on the group (e.g., group convolution) are approximated by evaluating them on the set of co-
occurring transformations as opposed to on the entire group. Such approximations are expected to
be very accurate, as non-co-occurrent transformations are rare. This could be though of as sharping
up co-occurrent attention to co-occurrent restriction.
6	Conclusion
We have introduced the concept of co-attentive equivariant feature mapping and applied it in the
context of equivariant neural networks. By attending to the co-occurrence envelope of the data,
we are able to improve the performance of conventional equivariant ones on fully (rotated MNIST)
and partially (CIFAR-10) rotational settings. We developed cyclic equivariant self-attention, an
attention mechanism able to attend to the co-occurrence envelope of the data without disrupting
equivariance to a large set of transformation groups (i.e., all transformation groups G, whose action
in the codomain of a G-equivariant feature mapping produce cyclic permutations). Our obtained
results support the proposed co-occurrence envelope hypothesis.
Acknowledgments
We gratefully acknowledge Jan Klein, Emile van Krieken, Jakub Tomczak and our anonymous
reviewers for their helpful and valuable commentaries. This work is part of the Efficient Deep
Learning (EDL) programme (grant number P16-25), which is partly founded by the Dutch Research
Council (NWO) and Semiotic Labs.
References
Krister Ahlander and Hans MUnthe-Kaas. Applications of the generalized fourier transform in nu-
merical linear algebra. BIT Numerical Mathematics, 45(4):819-850, 2005.
SH Alkarni. Statistical applications for equivariant matrices. International Journal of Mathematics
and Mathematical Sciences, 25(1):53-61, 2001.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Richard M Brooks and Alvin G Goldstein. Recognition by children of inverted photos of faces.
Child Development, 1963.
Vicki Bruce and Glyn W Humphreys. Recognizing objects and faces. Visual cognition, 1(2-3):
141-180, 1994.
Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. arXiv preprint arXiv:1601.06733, 2016.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990-2999, 2016.
Taco S. Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. CoRR,
abs/1801.10130, 2018. URL http://arxiv.org/abs/1801.10130.
Taco S Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolu-
tional networks and the icosahedral cnn. arXiv preprint arXiv:1902.04615, 2019.
Kent Dallett, Sandra G Wilcox, and Lester D’andrea. Picture memory experiments. Journal of
Experimental Psychology, 76(2p1):312, 1968.
Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in convo-
lutional neural networks. arXiv preprint arXiv:1602.02660, 2016.
Alejo Freire, Kang Lee, and Lawrence A Symons. The face-inversion effect as a deficit in the
encoding of configural information: Direct evidence. Perception, 29(2):159-170, 2000.
10
Published as a conference paper at ICLR 2020
Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in neural information
processing Systems, pp. 2537-2545, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Mary Henle. An experimental investigation of past experience as a determinant of visual form
perception. Journal of Experimental Psychology, 30(1):1, 1942.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empir-
ical evaluation of deep architectures on problems with many factors of variation. In Proceedings
of the 24th international conference on Machine learning, pp. 473-480. ACM, 2007.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-
bard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541-551, 1989.
Junying Li, Zichen Yang, Haifeng Liu, and Deng Cai. Deep rotation equivariant network. Neuro-
computing, 290:26-33, 2018.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field
networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5048-
5057, 2017.
Diego Marcos, Benjamin Kellenberger, Sylvain Lobry, and Devis Tuia. Scale equivariance in cnns
with vector fields. arXiv preprint arXiv:1807.11783, 2018.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. arXiv preprint arXiv:1707.03141, 2017.
Aude Oliva and Antonio Torralba. The role of context in object recognition. Trends in cognitive
sciences, 11(12):520-527, 2007.
Maximilian Riesenhuber, Izzat Jarudi, Sharon Gilad, and Pawan Sinha. Face processing in humans
is compatible with a simple shape-based model of vision. Proceedings of the Royal Society of
London. Series B: Biological Sciences, 271(SUPPl_6):S448-S450, 2004.
Gudrun Schwarzer. Development of face processing: The effect of face inversion. Child develop-
ment, 71(2):391-401, 2000.
Pawan Sinha, Benjamin Balas, YUri Ostrovsky, and Richard RUssell. Face recognition by hUmans:
Nineteen resUlts all compUter vision researchers shoUld know aboUt. Proceedings of the IEEE, 94
(11):1948-1962, 2006.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolUtional net. arXiv preprint arXiv:1412.6806, 2014.
Michael J Tarr and Steven Pinker. Mental rotation and orientation-dependence in shape recognition.
Cognitive psychology, 21(2):233-282, 1989.
11
Published as a conference paper at ICLR 2020
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing Systems,pp. 5998-6008, 2017.
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation equiv-
ariant cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 849-858, 2018.
Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block
attention module. In Proceedings of the European Conference on Computer Vision (ECCV), pp.
3-19, 2018.
Daniel Worrall and Gabriel Brostow. Cubenet: Equivariance to 3d rotation and translation. In
Proceedings of the European Conference on Computer Vision (ECCV), pp. 567-584, 2018.
Daniel E Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. arXiv preprint
arXiv:1905.11697, 2019.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028-5037, 2017.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048-2057, 2015.
Robert K Yin. Looking at upside-down faces. Journal of experimental psychology, 81(1):141, 1969.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. arXiv preprint arXiv:1805.08318, 2018.
12
Published as a conference paper at ICLR 2020
A Obtaining Co-Occurrent Attention via Equation 5
Figure 4: Synchronous movement of feature mappings and attention masks as a function of input
rotation in the group p4 (rmax = 4).
In this section, we provide a meticulous description on how co-occurrent attention is obtained via
the method presented in the paper. Intuitively, a direct approach to address the problem illustrated in
the introduction (Section 1) and Figure 2 requires an attention mechanism that acts simultaneously
on r and λ (see Eq. 3). However, we illustrate how the depicted problem can be simplified such that
attention along r is sufficient by taking advantage of the equivariance property of the network.
Let p be the input of a roto-translational convolution fR : Z2 × Θ × Λ0 → Z2 × Θ × Λ1 as defined in
Eq. 3, and Θ be the set of rotations by θr degrees: Θ = {θr = r-2π}警.Let fR(P)(U) ∈ RrmaX×λi
rmax
be the matrix consisting of the rmax oriented responses for each λ ∈ Λ1 learned representation
at a certain position u. Since the vectors fR(p)(u, λ) ∈ Rrmax, λ ∈ Λ1 permute cyclically as a
result of the rotation equviariance property of fR , it is mandatory to ensure equivariance to cyclic
permutations for each fR(p)(u, λ) during the course of the attention procedure (see Section 3).
At first sight, one is inclined to think that there is no connection between multiple vectors
fR(p)(u, λ) in fR(p)(u), and, therefore, in order to exploit co-occurences, one must impose addi-
tional constraints along the λ axis. However, there is indeed an implicit restriction in fR(p)(u) along
λ resulting from the rotation equivariance property of the mapping fR, which we can take advantage
from to simplify the problem at hand. Consider, for instance, the input θip, a θi-rotated version of
p. By virtue of the equivariance property of fR, we have (locally) that fR(θip) = Pi (fR(p)).
Furthermore, we know that this property must hold for all the learned feature representations
fR(p)(u, λ),∀λ ∈ Λ1 . Resultantly, we have that:
fR(θip)(u,r,λ) =Pi(fR(p)(u,r,λ)) , ∀λ∈Λ1	(12)
In other words, if one of the learned mappings fR(p)(u, r, λ) experiences a permutation Pi along r,
all the learned representations fR(p)(u, r, λ), ∀λ ∈ Λ1 must experience the exact same permutation
Pi as well. Resultantly, the equivariance property of the mapping fR ensures that all the Λ1 learned
feature representations fR(p)(u, λ) “move synchronously” as a function of input rotation θi.
Likewise, if we apply a cyclic equivariant attention mechanism ACλ independently on top of each λ
learned representation fR(p)(u, λ), we obtain that the relation
ACλ(fR(θip))(u,r,λ) =Pi(ACλ(fR(p))(u,r,λ)) , ∀λ∈Λ1	(13)
must hold as well. Similarly to the case illustrated in Eq. 12 and given that ACλ is equivariant to
cyclic permutations on the domain, we obtain that all the Λ1 learned attention masks ACλ “move
synchronously” as a function of input rotation θi as well (see Fig. 4).
From Eq. 13 and Figure 4, one can clearly see that by utilizing ACλ independently along r and taking
advantage from the fact that all Λ1 learned feature representations are tied with one another via fR ,
one is able to prioritize learning of feature representations that co-occur together as opposed to the
much looser formulation in Eq. 12, where feedback is obtained from all orientations.
13