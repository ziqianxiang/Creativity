Published as a conference paper at ICLR 2020
SCALOR: Generative World Models
with Scalable Object Representations
Jindong Jiangt, Sepehr Janghorbani*, Gerard de Melo & Sungjin Ahn
Rutgers University
Ab stract
Scalability in terms of object density in a scene is a primary challenge in unsu-
pervised sequential object-oriented representation learning. Most of the previous
models have been shown to work only on scenes with a few objects. In this pa-
per, we propose SCALOR, a probabilistic generative world model for learning
SCALable Object-oriented Representation of a video. With the proposed spatially-
parallel attention and proposal-rejection mechanisms, SCALOR can deal with
orders of magnitude larger numbers of objects compared to the previous state-
of-the-art models. Additionally, we introduce a background module that allows
SCALOR to model complex dynamic backgrounds as well as many foreground
objects in the scene. We demonstrate that SCALOR can deal with crowded scenes
containing up to a hundred objects while jointly modeling complex dynamic back-
grounds. Importantly, SCALOR is the first unsupervised object representation
model shown to work for natural scenes containing several tens of moving ob-
jects. https://sites.google.com/view/scalor/home
1	Introduction
Unsupervised structured representation learning for visual scenes is a key challenge in machine
learning. When a scene is properly decomposed into meaningful entities such as foreground objects
and background, we can benefit from numerous advantages of abstract symbolic representation.
These include interpretability, sample efficiency, the ability of reasoning and causal inference, as
well as compositionality and transferability for better generalization. In addition to symbols, another
essential dimension is time. Objects, agents, and spaces all operate under the governance of time.
Without accounting for temporal developments, it is often much harder if not impossible to discover
certain relationships in a scene.
Among a few methods that have been proposed for unsupervised learning of object-oriented repre-
sentation in temporal scenes, SQAIR (Kosiorek et al., 2018) is by far the most complete model.
As a probabilistic temporal generative model, it can learn object-wise structured representation
while modeling underlying stochastic temporal transitions in the observed data. Introducing the
propagation-discovery model, SQAIR can also handle dynamic scenes where objects may disap-
pear or be introduced in the middle of a sequence. Although SQAIR provides promising ideas and
shows the potential of this important direction, afew key challenges remain, limiting its applicability
merely to synthetic toy tasks that are far simpler than typical natural scenes.
The first and foremost limitation is scalability. Sequentially processing every object in an image,
SQAIR has a fundamental limitation in scaling up to scenes with a large number of objects. As
such, the state-of-the-art remains at the level of modeling videos containing only a few objects,
such as MNIST digits, per image. Considering the complexity of typical natural scenes as well
as the importance of scalable unsupervised object perception for applications such as self-driving
systems, it is thus a challenge of the highest priority to scale robustly to scenes with a large number
of objects. Scaling up the object-attention capacity is an important problem because it allows us to
maximize the modern parallel computation that can maximize search capacity. This is contrary to
humans, who can attend only to a few objects at a time in a time-consuming sequential manner. The
,Authors with equal contribution. Email addresses: { jindong.jiang, Sepehr .janghorbani,
gerard.demelo, sungjin.ahn}@rutgers.edu
1
Published as a conference paper at ICLR 2020
AlphaGo system (Silver et al., 2017) is an example demonstrating the power of such parallel search
(attention) beyond human capacity.
The second limitation is that previous models including SQAIR lack any form of background mod-
eling and thus only cope with scenes without background, whereas natural scenes usually have a
dynamic background. Thus, a temporal generative model that can deal with dynamic backgrounds
along with many foreground objects is an important step toward natural video scene understanding.
In this paper, we propose a model called SCALable Sequential Object-Oriented Representation
(SCALOR). SCALOR resolves the aforementioned key limitations and hence can model complex
videos with several tens of moving objects along with dynamic backgrounds, eventually making the
model applicable to natural videos. In SCALOR, we achieve scalability with respect to the object
density by parallelizing both the propagation and discovery processes, reducing the time complexity
of processing each image from O(N) to O(1), with N being the number of objects in an image.
We also observe that the sequential object processing in SQAIR, which is based on an RNN, not
only increases the computation time but also deteriorates discovery performance. To this end, we
propose a parallel discovery model with superior discovery capacity and performance. SCALOR
can also be regarded as a generative tracking model since it not only detects object trajectories but
is also able to predict trajectories into the future. In our experiments, we demonstrate that SCALOR
can model videos with nearly one hundred moving objects along with a dynamic background on
synthetic datasets. Furthermore, we showcase the ability of SCALOR to operate on natural-scene
videos containing tens of objects with a dynamic background.
The contributions of this work are: (i) We propose the SCALOR model, which significantly im-
proves (two orders of magnitude) the scalability in terms of object density. It is applicable to nearly
a hundred objects while providing more efficient computation time than SQAIR. (ii) We propose
parallelizing the propagation-discovery process by introducing the ProPose-reject model, reducing
the time complexity to O(1). (iii) SCALOR can model scenes with a dynamic background. (iv)
SCALOR is the first probabilistic model demonstrating its working on a significantly more complex
task, i.e., natural scenes containing tens of objects as well as background.
2	Preliminaries: Sequential Attend Infer Repeat (SQAIR)
SQAIR models a sequence of images x = x1:T by assuming that observation xt at time t is gener-
ated from a set of object latent variables ztO = {zt,n}n∈Ot with Ot the set of objects present at time
t. Latent variable zt,n corresponding to object n consists of three factors (ztp,rnes, ztw,hnere, ztw,hnat), which
represent the existence, pose, and appearance of the object, respectively. SQAIR also assumes that
an object can disappear orbe introduced in the middle ofa sequence. To model this, it introduces the
propagation-discovery model. In propagation, a subset of currently existing objects is propagated
to the next time-step and those not propagated (e.g., moved out of the scene) are deleted. In discov-
ery, after deciding how many objects Dt will be discovered, Dt objects are newly introduced into
the scene. Combining the propagated set Pt and discovered set Dt , we obtain the set of currently
existing objects Ot . The overall process can be formalized as:
T
p(x1:T, zf1g:T, D1:T) = p(D1,z1D) Yp(xt|zftg)p(Dt,ztD|ztP)p(ztP|zftg-1) .	(1)
t=2
For SQAIR, we use zftg (“fg” standing for foreground) to denote ztO because SQAIR does not have
any latent variables for background. Due to the intractable posterior, SQAIR is trained through
variational inference with the following posterior approximation:
T
q(D1:T, zf1g:T |x1:T) =	q(Dt,ztD|xt,ztP)	q(zt,n|zt-1,n, x≤t) .	(2)
SQAIR is trained using the importance-weighted autoencoder (IWAE) objective (Burda et al., 2015).
The VIMCO estimator (Mnih & Rezende, 2016) is used to backpropagate through the discrete ran-
dom variables while using the reparameterization trick (Kingma & Welling, 2013; Williams, 1992)
for continuous variables. SQAIR has two main limitations in terms of scalability. First, for prop-
agation, SQAIR relies on an RNN, which sequentially processes each object by conditioning on
previously processed objects. Second, the discovery is also sequential because it uses RNN-based
2
Published as a conference paper at ICLR 2020
discovery based on AIR (Eslami et al., 2016). Consequently, SQAIR has a time complexity of
O(|Ot|) per step t. In Crawford & Pineau (2019b), the authors demonstrated that this sequential
discovery can easily fail beyond the scale of a few objects. Moreover, SQAIR lacks any model for
the background and its temporal transitions, which is important in modeling natural scenes.
3 SCALOR
In this section, we describe the proposed model, SCALOR. We first describe the generative process
along with the proposal-rejection mechanism, which is designed to prevent propagation collapse,
and then the inference process and learning.
3.1 Generative Process
SCALOR assumes that an image xt is generated by background latent ztbg and foreground latent
zftg. The foreground is further factorized into a set of object representations zftg = {zt,n}n∈Ot . In
SCALOR, we represent an object by zt,n = (ztp,rnes, ztw,hnere, ztw,hnat) similarly to SQAIR. The appearance
representation ztw,hnat is a continuous vector representation, and ztw,hnere is further decomposed into
the center position ztp,ons , scale zstc,nale, and depth ztd,enpth. The depth representation, which is missing
in SQAIR, represents the relative depth between objects from the camera viewpoint. This depth
modeling helps deal with object occlusion. The foreground mask mt,n obtained from ztw,hnat is used to
distinguish background and foreground. We adopt the propagation-discovery model from SQAIR,
but improve it in such a way to resolve the scalability problem. The generative process of SCALOR
can be written as:
T
z
p(x1:T, z1:T) =p(z1D)(zb1g)	p(xt|zt) p(ztbg|zb<gt,zftg) p(ztD |ztP) p(ztP |z<t),
t=2、{}、-----------{-----}、—{—‘、—{z—}
rendering background transition discovery
(3)
∙^^^^{^^^β"
propagation
where zt = (ztbg, zftg). As shown, the generation process is decomposed into four modules: (i)
propagation, (ii) discovery, (iii) background transition, and (iv) rendering.
Propagation. The propagation in SCALOR is modeled as follows:
pres
p(zP ∣z<t) = U P(ZprnS∣z<t,n) {p(zwn*z<t,n) P(Zwnat∣z<t,n)}zt,n ,	(4)
n∈Ot
whereP(ztp,rnes|z<t,n) is a Bernoulli distribution with parameter βt,n. The distributions of “what” and
“where” are defined only when the object is propagated. To implement this, for each object n we
assign an object-tracker RNN denoted by its hidden state ht,n. The RNN is updated by input zt,n for
all t where the object n is present in the scene. The parameter βt,n is obtained as βt,n = fmlp(ht,n).
If ztp,rnes = 0, the object n is not propagated and the tracker RNN is deleted. Importantly, unlike the
RNN-based sequential propagation in SQAIR, the propagation in SCALOR is fully parallel.
Discovery by Proposal-Rejection. The main contribution in making our model scalable with re-
spect to the the number of objects is our new discovery model that consists of two phases: proposal
and rejection. In the proposal phase, we assume that the target image can be covered by H × W
latent grid cells, and we propose an object latent variable Zt,h,w per grid cell. This proposal phase
can be written as:
HW
HW
pres
P(ZD∣zp)=	γ	p(zDh,w∣zp)=	γ	P(Zprhsw∣zP){p(zwherw*P(ZWhatw∣zP)}zt,h,w.⑸
h,w=1
h,w=1
In the rejection phase, our goal is to reject some of the proposed objects ifa proposed object largely
overlaps with a propagated object. We realize this by using the mask variable mt,n. Specifically,
if the overlapping area between the mask of a proposed object and that of a propagated object is
over a threshold τ, we reject the proposed object. This procedure can be described as (i) pro-
posal: ZD 〜P(ZDIZP) and (ii) accept-reject: ZD = faccept-reject(ZD, ZP, τ). In this way, the final
discovery set ZD is always a subset of the proposal set ZD, i.e., ZD ⊆ ZD. Although we use a
deterministic function for the rejection, it can be a design choice to implement this as a stochas-
tic decision. While one rationale behind this design is to reflect an inductive bias of a Gestalt
3
Published as a conference paper at ICLR 2020
Figure 1: SCALOR inference procedure: (A) Proposal, (B) Accept-Reject, (C) Propagation, (D) Background
Module and Rendering Process. (A) The proposal module takes the input image and propagation mask and
combines them to make the proposal representation. (B) From the proposal representation, the proposal mask
is generated, and then compared to the propagation mask to make an accept-reject decision. Only the accepted
proposals are considered as discovered objects. (C) The tracker RNNs decide what and where to propagate after
looking at the input image. The gray boxes represent what is not propagated. (D) Given inferred foreground
objects and the input image, the background module infers the background representation. The rendering
process combines the foreground and background representations according to the foreground mask assignment
principle saying that two objects cannot coexist in the same position, we shall also see later fur-
ther reasons as to why this design is effective. The final discovery model can then be written as:
P(ZD IZP) = P(ZDIZP) QHW=Ip(ZDhwIZP, zD,τ), where the accePtance model is:
accePt	accePt
p(ZDhw∣ZP, zd ,τ) = f(z：chew>p, ZD ,τ )p(Zwrhw )zt,h,wp(Zwhatw A…
(6)
Background Transition. Unlike SQAIR, SCALOR is endowed with a background model. The
background image is encoded into a D-dimension continuous vector Ztbg from the background tran-
sition P(Ztbg IZb<gt, Zftg). The background RNN encodes the temPoral transition of background images.
Rendering. The imPlementation of the rendering Process is the same as in SPAIR (Crawford &
Pineau, 2019b), excePt that we Process the objects in Parallel. ImPlementation details are shown in
APPendix A.
3.2 Learning and Inference
Due to the intractability of the true Posterior distribution P(Z1:T Ix1:T), we train our model using
variational inference with the following Posterior aPProximation:
TT
q(Z1:T Ix1:T) = Y q(ZtIZ<t, x≤t) = Yq(ZtbgIZftg,xt) q(ZtDIZtP,x≤t)q(ZtPIZ<t,x≤t) .	(7)
t=1	t=1
Posterior Propagation. q(ZtP IZ<t, x≤t) is similar to the ProPagation in generation, excePt that we
now Provide observation x≤t through an RNN encoding. Here, the ProPagation for each object n
is done by q(Zt,nIZ<t,n, x≤t) using attention at,n = fatt(x≤t) on the feature maP for object n. To
comPute the attention, we use the Previous Position ZtP-os1,n as the center Position and extract half
the width and height of the convolutional feature maP using bilinear interPolation. This attention
mechanism is motivated by the observation that only Part of the image contains information for
tracking an object and an inductive bias that objects cannot move a large distance within a short
time sPan (i.e., objects do not telePort).
Posterior Discovery. The Posterior discovery also consists of ProPosal and rejection Phases. The
main difference is that we now comPute the ProPosal in spatially-parallel manner by conditioning on
the observations x≤t, i.e., q(ZD IZP, x≤t) =QHwL q(zDh,w IzP , χ≤t). Here, the observation x≤t
is encoded into the feature maP of dimensionality H×W ×D using a Convolutional LSTM (Xingjian
et al., 2015). Then, from each feature We obtain ZDhW ∙ Importantly, this is done over all the feature
cells h, w in Parallel. A similar aPProach is used in SPAIR (Crawford & Pineau, 2019b), but it
infers the object latent representations sequentially and thus is difficult to scale to a large number
4
Published as a conference paper at ICLR 2020
of objects (Lin et al., 2020). Even if this spatially-parallel proposal plays a key role in making our
model scalable, we also observe another challenge due to this high capacity of the discovery module.
The problem is that the discovery module tends to dominate the propagation module and thus most
of the objects in an image are explained by the discovery module, i.e., objects are rediscovered at
every time-step while nothing is propagated. We call this problem propagation collapse.
Why would the model tend to explain an image through discovery while suppressing propagation?
First, the model does not care where—either from discovery or propagation—an object is sourced
from as long as it can make an accurate reconstruction. Second, the propagation step performs a
much harder task than the discovery. For the propagation to properly predict, tracker n needs to
learn to find the matching object from an image containing many objects. Although the propaga-
tion attention plays an important role in balancing the discovery and propagation, we found that it
does not eliminate the problem of re-discovery, and without rejection, its effectiveness varies across
different experiment settings. On the contrary, the discovery module does not need to solve such a
difficult association problem because it only performs local image-to-latents encoding without as-
sociating latents of the previous time-step. Therefore, it is much easier for the discovery encoder
to produce latents that are more accurate than those inferred from propagation. If we limit the ca-
pacity of the discovery module and sequentially process objects like in SQAIR, we may mitigate
this problem because the propagation module is naturally encouraged to explain what this weakened
discovery module cannot. This approach, however, cannot scale.
We employ two techniques to resolve the aforementioned problem. First, we simply bias the initial
network parameter so that it has a high propagation probability at the beginning of the training. This
helps the model prefer to explain the observation first through propagation. The second technique is
our proposal-rejection mechanism, which is implemented the same way as in the generation process.
This prevents the discovery model from redundantly explaining what is already explained by the
propagation module. The posterior for the discovery model can be written as:
HW
q(ZD IZP, x≤t) = q(ZD IZP, x≤t) paCC PaCCept(ZDh,w IZP, ZD) ,
(8)
h,w=1
where the aCCeptanCe model is paCCept(ZtD,h,w IZP, ZD)=P(Zprhsw ∣zP , ZD)(P(Zwhew )ρ(Zw,hatw ))zprh-.
Posterior Background. The posterior of the baCkground q(ZtbgIZftg, xt) is Conditioned on the input
image and Currently existing objeCts. Here, we provide the foreground latents so that the remaining
parts in the image Can be explained by the baCkground module.
Training. We train our model by maximizing the following evidenCe lower bound L(θ, φ) =
T
X Eqφ(z<t∣x<t) [Eqφ(zt∣z<t,x≤t) [log Pθ (Xt|Zt)] - KL [qφ (zt lz<t, x≤t) ∣∣ Pθ (zt|z<t)]].⑼
t=1
We use the reparameterization triCk (Williams, 1992; Kingma & Welling, 2013) for Continuous ran-
dom variables suCh as Zwhat, and the Gumbel-Softmax triCk (Jang et al., 2016) for disCrete variables
suCh as zpres. We found that our proposed model works well and robustly with these simpler training
methods than what is used in SQAIR, i.e., VIMCO and IWAE.
4	Related Work
Different approaChes have been taken to taCkle the problem of unsupervised sCene representation
learning. ObjeCt-oriented models suCh as AIR (Eslami et al., 2016) and SPAIR (Crawford & Pineau,
2019b) deCompose sCenes into latent variables representing the appearanCe, position, and size of the
underlying objeCts. While AIR makes use of a reCurrent neural network, SPAIR applies spatially
invariant attention to extraCt loCal feature maps. Although the latter provides better sCalability than
AIR, it is still limited as it performs sequential inferenCe on objeCts. SQAIR (Kosiorek et al., 2018),
disCussed in SeCtion 2, extends the ideas proposed in AIR to temporal sequenCes. On the other
hand, sCene-mixture models (Greff et al., 2017; Van Steenkiste et al., 2018; Burgess et al., 2019;
Greff et al., 2019; EngelCke et al., 2019) deCompose sCenes into a ColleCtion of Components, eaCh
being a full-image level representation. Although suCh models allow deComposition of the input
image into Components, they are not objeCt-wise disentangled as multiple objeCts Can be in the same
5
Published as a conference paper at ICLR 2020
(a) Dataset Density	(b) Dataset Density
Figure 2: Quantitative result showing superior performance of SCALOR (SC) compared to SQAIR (SQ). (a)
Tracking Accuracy (b) Object Count and Reconstruction Error
component. Furthermore the obtained representation does not contain explicit interpretable features
like position and scale, etc. SPACE (Lin et al., 2020) combines both of the above approaches by
using object detection for foreground and mixture decomposition for background. It improves upon
SPAIR by parallelizing the latent inference process.
DDPAE (Hsieh et al., 2018) is another object-oriented sequential generative model that models each
object with an appearance and a position vector. The model assumes the appearance of an object to
be fixed, and thus shares the content vector across different time-steps. NEM (Greff et al., 2017) and
RNEM (Van Steenkiste et al., 2018) introduce a spatial mixture model to disentangle the scene into
multiple components representing each entity. Since each component generates a full scene image,
the latent representations are not interpretable. Tracking-By-Animation (He et al., 2019) introduces
a deterministic model to tackle the task of object tracking in an unsupervised fashion. Furthermore,
there is a substantial amount of work on object tracking from the computer vision community using
the same “bounding box”-representation approach proposed in SCALOR (Kosiorek et al., 2017;
Ning et al., 2017; Nam & Han, 2016; Tao et al., 2016), but such methods use provided labels to tackle
the problem of object tracking and thus are usually fully or semi-supervised and not probabilistic
object-oriented models.
We also note that Crawford & Pineau (2019a) has independently and concurrently developed a
similar architecture to ours. This model also emphasizes the scalability problem with a similar idea
motivated by parallelizing SPAIR and extending it to sequential modeling. The main differences are
the usage of the proposal-rejection mechanism and the background modeling that make our model
work on complex natural scenes.
5	Experiments
In this section, we describe the experiments conducted to empirically evaluate the performance of
SCALOR. We propose two tasks, (i) synthetic MNIST/dSprites shapes and (ii) natural-scene CCTV
footage of walking pedestrians. We will show SCALOR’s abilities to detect and track objects,
to generate future trajectories, and to generalize to unseen settings. Furthermore, we provide a
quantitative comparison to state-of-the-art baselines.
5.1	Task 1: Large-Scale MNIST and dSprites Shapes
We first evaluate our model on datasets of moving dSprites shapes as well as moving MNIST digits.
In all experiments, the image sequence covers a 64 × 64 partial view of the center of the whole
environment. Therefore, while there is a fixed number of objects in the environment at each time-
step, only a subset of them are visible in the observed image. The environment size is customized
for each setting in a way that objects can conveniently move out of the viewpoint completely and
re-enter within a few time-steps. We test on five different scale settings. In each setting, the number
of objects in each trajectory is sampled uniformly from an interval [min, max]. Each scale setting
is specified with a triplet (min, avg, max), where min and max are as mentioned and avg represents
the average number of visible objects in the trajectories in that setting. The five settings are referred
to as Very Low Density (VLD) [(2, 2.9, 4)], Low Density (LD) [(8, 8, 11)], Medium Density (MD)
[(18, 20, 24)], High Density (HD) [(50, 55, 64)] and Very High Density (VHD) [(90, 90, 110)]. For
6
Published as a conference paper at ICLR 2020
Figure 3: Qualitative results of SCALOR for Moving dSprites and Moving MNIST (HD) tasks: a) Inferred
bounding boxes superimposed on the original image sequence. White circles indicate discovery at that timestep,
b) Reconstructed sequence, c) Per-object reconstruction
(a)
(b)
Figure 4: Qualitative samples of tracking on Moving dSprites task with dynamic background: a) Original
image sequence with inferred bounding boxes, b) Reconstructed sequence
example, in the MD setting, there are always between 18 to 25 objects in the overall environment,
while only about 20 of them are visible on average in each frame.
Experiment 1 - Tracking Performance. This experiment evaluates the tracking performance in
an environment without background. We use the following tracking metrics: Multi-Object Track-
ing Accuracy (MOTA), precision-recall of the inferred bounding boxes (Bernardin & Stiefelhagen,
2008), reconstruction mean squared error, and normalized counting mean absolute error (Count-
MAE). CountMAE measures the difference between the number of predicted objects and the actual
number of objects, normalized by the latter. MOTA measures the tracking accuracy and consistency
of the bounding boxes. Precision-recall measures the number of false-positive and negatives, re-
gardless of the associated IDs. For computing the MOT metrics, we choose the Euclidean distance
threshold to be twice the actual object size in each setting. Figure 2 shows the quantitative results of
SCALOR compared to baselines. More detailed quantitative results are given in Appendix B.
We compare the performance of the proposed model with SQAIR in the VLD setting for both
MNIST and dSprites datasets, and LD setting for dSprites. Note that we were not able to make
SQAIR work on other settings due to the high object density. As shown in Figure 2, SCALOR
outperforms SQAIR in all these settings, obtaining significantly higher accuracy and recall. SQAIR
either misses some objects or misidentifies distinct objects as one. In addition, SCALOR has lower
values of CountMAE in comparison to SQAIR, showing that SCALOR can infer the number of
objects in the scene more accurately. Furthermore, we observe that increasing the number of ob-
jects in the scene does not significantly impede SCALOR’s tracking ability, which demonstrates the
strength of SCALOR when applied to images with a high number of objects. SCALOR can achieve
relatively high precision-recall even for scenes containing about 100 objects. Note that in the VHD
case, the number of objects (about 90) in the first time-step exceeds the number of detection grid
cells (8×8) the discovery model has. Thus, the model can only detect up to 64 objects at the first
time-step, and detects the rest in the following time-steps. This results in lower performance on the
tracking and detection accuracy in the VHD case. This is demonstrated in Figure 10 in Appendix C.
Figure 3 demonstrates the qualitative performance of SCALOR on dSprites and MNIST (HD). To
clarify tracking consistency, bounding boxes with distinct ids are represented by distinct colors.
Discovered objects are emphasized by white circles. As shown in Figure 3(a), the discovery module
of SCALOR can identify newly introduced objects and put them in the propagation list while the
propagation module keeps tracking existing objects. Figure 3(c) shows object-wise rendering of
inferred zwhat latent variables. For clear visualization, object-wise rendering is shown only for a
7
Published as a conference paper at ICLR 2020
background inference/generation
subset of objects present in all the time-steps. SCALOR infers consistent object-wise representation
even when objects are largely occluded.
Experiment 2 - Dynamic Background. Another interesting yet challenging setting is the presence
of a dynamic background. As we can see in Figure 4, SCALOR can decompose the video sequence
to a set of foreground objects as well as a dynamic background. While the background module can
model the background dynamics, it can also model the dynamics of the whole environment including
the moving objects. This usually brings competition between background module and foreground
module on explaining the foreground part. However, the background module and foreground mod-
ule cooperate properly in SCALOR. We believe this is because of our modeling that the background
module obtains information from the foreground part by conditioning on the foreground latent vari-
ables. Table 1 also provides the tracking performance of the model for this setting to show how
complex images affect the tracking quality. We observe that it achieves comparable performance to
the default no-background setting. These results are provided in Table 1 Under “SCALOR - BG”.
Experiment 3 - Future Time Prediction/Generation. This section showcases the generation abil-
ity of SCALOR via two experiments. The first experiment is aimed at showing conditional gen-
eration. Here, the model is provided with the first 5 frames, and then tested to generate the next
5 frames. Latent variables at each time-step are sampled independently from the prior distribution
of the propagation step conditioned on latent variables from previous time-step. Because the focus
is on conditional generation of background and objects, discovery prior of zpres is manually set to
zero. Figure 5 shows a generated sample. The first 5 frames (before the red line) correspond to
reconstruction while the next 5 frames represent conditional generation1 . As we can see in Figure 5,
SCALOR can generate dynamic background and object trajectories that are reasonably consistent.
The second experiment showcases video generation from scratch. In this setting, the model gener-
ates all latent variables from the discovery prior at the first time-step. For the following time-steps,
latent variables are sampled by conditioning on the latent variables of the previous time-steps. The
object generation probability for each grid cell at the first time-step, i.e. zp1r,ehs,w, is set to 0.2. Figure 6
shows one generated sample. At the first time-step where all variables are sampled from a fixed prior,
the background is generated reasonably, but the objects are generated only partially. This behavior
is expected because newly introduced objects are mostly partially observed at image boundaries.
This makes the inference network learn a zwhat representation corresponding to the partial views.
Furthermore, since the zwhat prior is a standard Gaussian, it is hard to model the multi-modal object
appearance. Yet, it is interesting to see that the conditional generation of each object’s zwhat grad-
ually converges to a consistent complete view of an object. This demonstrates that in our model,
1Examples with longer duration are in the project website
8
Published as a conference paper at ICLR 2020
struction of objects and background, (c) reconstruction of the extracted background, (d) segmentation for each
object, colors indicate tracking ID, (e) extracted object trajectories
the distribution of ztw≥ha2t in propagation can actually achieve multi-modality when conditioned on
z1what from the first time-step. Furthermore, it shows that the propagation prior network is capable of
maintaining a consistent representation of the object along the sequence as is reflected in the dataset.
Therefore, SCALOR is capable of generating objects with independent appearance and behavior.
5.2	Task 2: Real-World Scenario
This section considers the performance of SCALOR on real-world natural video, which previous
models have not been able to handle due to the scalability issue and lack of a background model.
Compared to synthetic data, the challenges in this setting are significantly more difficult. Specifi-
cally, we consider the Crowded Grand Central Station dataset (Zhou et al., 2012), which was col-
lected from CCTV cameras at New York City’s Grand Central Station. Due to the complex pedes-
trian behavior, the density of the dataset can be considered a mix of LD and HD.
Figure 7 shows the tracking result on a sample sequence. We can see that SCALOR performs rea-
sonably well on this pedestrian tracking dataset by maintaining consistent temporal trajectories. As
shown in Figure 7(c), the background module infers the background component and reconstructs
the extracted background correctly. As for object detection, SCALOR succeeds in accurate pedes-
trian detection and tracking. Furthermore, the foreground mask produced by zwhat provides the
segmentation of each individual pedestrian, as shown in Figure 7(d). We draw tracking trajectories
in Figure 7(e) for each pedestrian in the natural scene. Trajectories in different colors correspond
to different pedestrian ids inferred by the identity of the latent variable of each object. Additional
figures and the dataset details are provided in Appendix D.
Figure 14 in Appendix D shows the conditional generation. The first 5 frames are the inference
reconstruction while the last 5 frames are model generation. Starting from the 6th frame, the latent
transition of the propagation trackers is modeled by the sequential prior network. In the generation
process, a different prior of zpres is introduced in the discovery module to introduce new objects
emerging in the scene (see Appendix E for more details). As shown in the figure, the model tends
to generate movement aligned with the previous frames. This also applies to newly generated ob-
jects from the discovery phase as shown in Figure 14(f). Although the trajectories are consistent,
the appearance of generated objects fails to maintain its consistency across different time frames.
Noticeably, in the generated sequence the segmentation mask for each object tends to deform into a
different shape. This may stem from imperfections during the inference that makes the learning of
appearance transition difficult. Additional figures for generation are provided in Appendix D.
The ground truth trajectories of the Grand Central Station Dataset were not available when this ex-
periment was conducted. We instead use negative log-likelihood (NLL) to compare our model with
two baselines, a sequential VAE and a Recurrent Latent Variable Model (VRNN). The sequential
VAE has one latent variable z of dimensionality 64, and a sequential priorp(zt|z<t). It is similar to
our background module with the same number of latent variables for encoder and decoder. As for the
VRNN, we implement a VRNN with 128 dimensions of the LSTM hidden state and latent variable z
9
Published as a conference paper at ICLR 2020
Training iteration (k)
(a)
Training iteration (k)
(b)
(c)	(d)
Figure 8: Attention-Rejection Ablation study and computational efficiency comparison. (a) Propagation rate,
(b) Number of discovered objects, (c) Inference time and, (d) Training convergence time
and choose a convolutional network as the image encoder and decoder. The NLL value for our model
is 28.30, and for the VAE and VRNN baseline it is 27.59 and 27.79 respectively. While learning a
highly structured representation, SCALOR can still obtain a comparable generation quality.
5.3	Ablation Study and Computational Efficiency Comparison
Ablation Study. We perform an ablation study on the rejection mechanism and the propagation
attention mechanism. Here, we train our model on the moving dSprites dataset where the number of
objects varies from 6 to 36. We use two metrics to evaluate different architectures. The first metric
is the propagation rate, which measures how long an object is propagated. Since, by design, objects
always stay in a scene for more than one frame, the propagation rate can be regarded as a proxy
to measure the success rate of tracking through propagation. The second metric counts the number
of discoveries that occurred in the whole sequence. If the discovery module dominates, a high
value will be observed. As we can see in Figure 8 (a), with no rejection (Rej) or attention (Att) in
propagation, the propagation rate goes down to 30% in early training iterations and keeps decreasing
as the training progresses. In this setting, we found that the model re-discovers objects in every frame
without tracking them properly. This is also shown in Figure 8 (b), where it has a significantly larger
number of discoveries. With the propagation attention mechanism, the propagation rate increases
to 95%. This is because the attention mechanism reduces the cost of finding the matching object
between time-steps in propagation. The model thus favors tracking in propagation over re-discovery.
It also prevents the discovery module from propagation collapse. However, as we can see in Figure 8
(b), the re-discovery problem still exists in this setting. The rejection mechanism, however, makes
the propagation rate converge to 1 even without propagation attention. The propagation attention
together with the rejection produced more accurate boxes.
Computational Efficiency. In Figure 8 (c) and (d), we measure the inference time (c), and training
convergence time (d). For measuring the inference time, we consider a hypothetical situation with
N objects in the first frame. The model is desired to discover and propagate them accordingly.
Furthermore, we set the discovery capacity of SQAIR to be the same as the number of discovery
grid cells in SCALOR. Figure 8(c) demonstrates how much time one forward step takes, as the
number of objects/discovery capacity increases from 4 to 64. SQAIR’s speed decreases linearly as
both its discovery and propagation mechanisms process objects sequentially. In contrast, SCALOR
does not suffer from such phenomena as discovery and propagation are done in parallel. Figure 8
(d) shows MSE convergence vs. training time when both models are trained on the MNIST VLD
setting. SCALOR converges to a lower MSE than SQAIR and does so orders of magnitude faster.
6	Conclusion
We introduce SCALOR, a probabilistic generative world model aimed at visually modeling an
environment of crowded dynamic objects. With the proposed parallel discovery-propagation and
proposal-rejection mechanism, we improve the capacity of object density from a few objects to up
to a hundred objects. Unlike previous models, the proposed model can also deal with dynamic back-
grounds. These contributions consequently makes our model applicable, for the first time in this line
of research, to natural scenes. An interesting future direction is to introduce more structure to the
background and to model interactions among objects and the background.
Acknowledgments
SA thanks Kakao Brain and Center for Super Intelligence (CSI) for their support.
10
Published as a conference paper at ICLR 2020
References
Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear
mot metrics. Journal on Image and Video Processing, 2008:1, 2008.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt
Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and represen-
tation. arXiv preprint arXiv:1901.11390, 2019.
Eric Crawford and Joelle Pineau. Exploiting spatial invariance for scalable unsupervised object
tracking. arXiv preprint arXiv:1911.09033, 2019a.
Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolu-
tional neural networks. In Proceedings of AAAI, 2019b.
Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative
scene inference and sampling with object-centric latent representations, 2019.
SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, and Geoffrey E
Hinton. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in
Neural Information Processing Systems,pp. 3225-3233, 2016.
Klaus Greff, Sjoerd van Steenkiste, and Jurgen Schmidhuber. Neural expectation maximization. In
Advances in Neural Information Processing Systems, pp. 6691-6701, 2017.
Klaus Greff, Raphael Lopez Kaufmann, Rishab Kabra, Nick Watters, Chris Burgess, Daniel Zoran,
Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning
with iterative variational inference. arXiv preprint arXiv:1903.00450, 2019.
Zhen He, Jian Li, Daxue Liu, Hangen He, and David Barber. Tracking by animation: Unsupervised
learning of multi-object attentive trackers. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1318-1327, 2019.
Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-Fei, and Juan Carlos Niebles. Learning to de-
compose and disentangle representations for video prediction. In Advances in Neural Information
Processing Systems, pp. 517-526, 2018.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Adam Kosiorek, Alex Bewley, and Ingmar Posner. Hierarchical attentive recurrent tracking. In
Advances in Neural Information Processing Systems, pp. 3053-3061, 2017.
Adam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer, repeat:
Generative modelling of moving objects. In Advances in Neural Information Processing Systems,
pp. 8606-8616, 2018.
Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong
Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial
attention and decomposition. In International Conference on Learning Representations, 2020.
Andriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. arXiv preprint
arXiv:1602.06725, 2016.
Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural networks for vi-
sual tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 4293-4302, 2016.
11
Published as a conference paper at ICLR 2020
Guanghan Ning, Zhi Zhang, Chen Huang, Xiaobo Ren, Haohong Wang, Canhui Cai, and Zhihai He.
Spatially supervised recurrent convolutional neural networks for visual object tracking. In 2017
IEEE International Symposium on Circuits and Systems (ISCAS),pp. 1-4. IEEE, 2017.
Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel
Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient
sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1874-1883, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
Ran Tao, Efstratios Gavves, and Arnold WM Smeulders. Siamese instance search for tracking. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1420-1429,
2016.
Sjoerd Van Steenkiste, Michael Chang, Klaus Greff, and Jurgen Schmidhuber. Relational neural ex-
pectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint
arXiv:1802.10353, 2018.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Ad-
vances in neural information processing systems, pp. 802-810, 2015.
Bolei Zhou, Xiaogang Wang, and Xiaoou Tang. Understanding collective crowd behaviors: Learn-
ing a mixture model of dynamic pedestrian-agents. In 2012 IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2871-2878. IEEE, 2012.
12
Published as a conference paper at ICLR 2020
A Algorithms
Algorithm 1: Discovery Proposal-Rejection Inference
Input: xi：t - image sequence UP to current time-step
MtP - propagated mask from current time-step
τ - rejection hyper-parameter
/* Feature encoding	*/
eitmg = ConvLSTM(x1:t)
etmask = MaskEncoder(MtP)
etagg = Concat[eitmg, etmask]
/* Proposal-Rejection (done in parallel)	*/
AcceptedListt = []
for h — 1 to H do
for W — 1 to W do
/* Objects proposal	*/
萃hSw 〜Bern(∙∣fnns(eαgh,w))
if 芍e,w ==0 then
I continue
end
Zwherw -N(∙fwnere(eαgh,w))
gtth,w = STN(Xt, zw,hw) // Attended Glimpse
Zwhatw -N(・|GlimPseEnc(gtth,w))
Ot,h,w, mt,h,w = STNT(GlimPseDeC(Zwhaw)), Zwherw)
/* Accept-Reject Test	*/
A(mtDh ∩MtP)
δ = A(tmhD——)^-	// A - pixel area
if δ < τ then,,
I ACCePtedListt.Add(Zwhatw, ZwheW Hhsw)
end
end
end
Output: ACCePtedListt
13
Published as a conference paper at ICLR 2020
Algorithm 2: Propagation Inference
Input: xi：t - image sequence UP to current time-step
PropListt-1 = {zt-1,n, ht-1,n}n∈Ot-1 - latent variable from previous time-step
/* Feature encoding	*/
eitmg = ConvLSTM(x1:t)
/* Object tracking (done in parallel)	*/
PropListt = PropListt-1
for n — 1 to Ot-ι do
eta,ttn = fnatnt (STN(eitm,ng, (ztp-os1,n,zstc-al1e,n)))	// Feature map attention
ht,n = GRU(fnn([eta,ttn,ztw-ha1t,n,ztp-os1,n,zstc-al1e,n,ztp-res1,n]),ht-1,n)
agg agg att what pos scale
et,n = fnn ([et,n, zt-1,n, zt-1,n, zt-1,n , ht,n])
zpos zscale N( |f where (eagg))
zt,n, zt,n N (.|fnn (et,n))
gatt,tn = STN(xt, (zpto,ns,zstc,anle))	// Attented Glimpse
Zwnt 〜N(∙∣ GlimpseEnc (gtttn))
zdepth -N(∙∣fnn(eαtn,zWnat,ht,n))
ZPreS 〜Bern(l fPres(zPos zscale ZWhat h+ ))
zt,n 〜Ben1( |fnn (zt,n, zt,n , zt,n , ht,n))
where Pos scale dePth
Zt,n = (Zt,n, Zt,n , zt,n )
if ztp,rnes == 1 then
I ProPLiStt,n.Update(zWnt, ZWlr, zp：：S)
else
I ProPLis%n .Delete ()
end
end
Output: ProPListt
Algorithm 3: Background Module and Rendering	
Input: Xt - image at current time-step,	
ot = {ot,n}n∈Ot - object RGB glimPses mt = {mt,n}n∈Ot - object masks glimPses,	
{(ZtP,ons, Zstc,nale), ztP,rnes}n∈Ot - object latents for Position and Presence	
/* Foreground object rendering (done in parallel) for n — 1 to Ot do	*/
Xftg,n = STN-1(ot,n, (ZtP,ons,Zstc,nale))	
STN-1(	Pres ( dePth) ( Pos scale)) Yt,n = STN (mt,n zt,n QLZKn ), (zt,n, TLtn )) γt,n =normalize(γt,n,∀n)	
end	
Xftg = Pn∈Ot Xftg,nγt,n	
/* Foreground mask rendering for n — 1 to Ot do	*/
I Mt,n = SmT (mt,：, (zpon 花：即))	
end	
Mt = min(Pn∈Ot Mt,n, 1) /* Background rendering	*/
ebg = BackgroundEncoder(Concat[Xt, (1 - Mt)]) Zbg 〜N(.∣fnn (ebg))	
Xtbg = BackgroundDecoder(Zbg)	
/* Foreground background combination	*/
Xt = Xftg + (1 - Mt) Xtbg Output: Xt	
14
Published as a conference paper at ICLR 2020
B	Additional Quantitative result
Table 1	includes Multi Object Tracking Accuracy (MOTA) as well as precision-recall of the inferred
bounding boxes (Bernardin & Stiefelhagen, 2008).
Table 2	provides a comparison of SCALOR to SQAIR and VRNN in terms of the reconstruction
error (MSE) and negative log-likelihood (NLL). NLL is computed per pixel across the whole se-
quence.
	Experimental Setting	MOTA ↑	Precision ↑	Recall ↑	MAE J
	SCALOR ― VHD	84.6%	96.8%	88.1%	0.091
	SCALOR - HD	92.8%	97.6%	95.4%	0.033
	SCALOR - MD	93.5%	97.8%	95.9%	0.041
	SCALOR-LD	92.5%	97.9%	94.9%	0.059
	SCALOR - VLD	93.8%	96.3%	98.9%	0.067
	SQAIR-LD	20.2%	97.6%	64.1%	0.335
	SQAIR - VLD	72.5%	98.4%	75.4%	0.239
	SCALOR-MD	86.9%	-99.0%-	87.9%	0.113
	SCALOR-LD	85.9%	99.0%	87.1%	0.124
	SCALOR - VLD	94.8%	97.7%	98.8%	0.040
	SQAIR - VLD	78.5%	99.9%	78.8%	0.214
	SCALOR - BG	91.9%	-97.2%-	95.3%	0.034
	SCALOR-LG	92.3%	97.0%	95.7%	0.032
Table 1: Quantitative results of SCALOR for different experimental settings. “SCALOR - BG” refers to the
dynamic background task for Experiment 3. "SCALOR - LG” refer to the length generalization experiment in
Section C.4
		dSprites					MNIST			
Method		VLD	LD	MD	HD	VLD	LD	MD
	SCALOR	22.16	22.41	22.56	22.88	7.45	7.54	7.82
u Z	VRNN	22.33	22.80	23.28	23.30	7.62	7.70	7.71
	SQAIR	22.63	40.50	-	-	7.75	-	-
S n	SCALOR	0.0010	0.0030	0.0024	0.0015	0.0029	0.0026	0.0010
	VRNN	0.0018	0.0071	0.0105	0.0190	0.0022	0.0054	0.0035
	SQAIR	0.0135	0.0918	-	-	0.0084	-	-
Table 2: Negative Log Likelihood and Mean Squared Error for SCALOR vs. baselines
15
Published as a conference paper at ICLR 2020
C Additional experiments on dSprites and MNIST
C.1 Frequent Dense Discovery
Figure 9: Frequent Dense Discovery experiment: a) First row: inferred bounding boxes superimposed on the
original sequence. b) Second row: discovery bounding boxes. c) Third row: discovery reconstruction. d) Last
row: propagation reconstruction
This experiment evaluates the ability to discover many newly introduced objects across time-steps.
This is important because in many applications only key-frames of a video are available, i.e., frames
at which significant changes happen. An example of a key-frame is a sudden change in the observer’s
viewpoint. This change of viewpoint introduces many new objects in the frame. Figure 9 shows one
such instance. In this setting, 10-15 objects are introduced at the first, fifth, and ninth time-steps,
respectively. As is shown, SCALOR is able to discover many new objects in each frame.
C.2 Very High Density
Figure 10: Sample from Very High Density setting: a) First row: inferred bounding boxes, b) Second row:
overall reconstruction, c) Third row: discovery bounding boxes, d) Fourth row: discovery reconstruction, d)
Fifth row: propagation bounding boxes, e) Sixth row: propagation reconstruction
16
Published as a conference paper at ICLR 2020
Figure 10 shows samples from the Very High Density experiment. This experiment places 90-110
objects in the overall environment, around 90 of which are visible at every time-step on average.
The discovery module contains 8 × 8 detection grid cells and thus can only detect up to 64 objects.
Interestingly, as is shown, the model will identify as many objects as possible in the first time-step
and discover the remaining in the second time-step. Furthermore, if too many objects are densely
packed in one local region of the space, SCALOR will perform similarly detecting them through
multiple frames.
C.3 Ability to handle overlap and occlusion
Figure 11 shows a sample of the MNIST settings where objects move towards each other more
aggressively. In this setting, there is a higher chance of overlapping and occlusion. As shown, the
identity of the objects is preserved when objects overlap with each other.
C.4 Generalization Test
Figure 12: Generalization with respect to longer sequences. a) First row: bounding boxes for the first 10
time-steps. b) Second row: bounding boxes for the last 10 time-steps
Figure 13: Generalization Experiment: a) First row: generalization to unseen shapes. b) Second row: general-
ization to a larger number of objects
We conduct three sets of experiments on generalization. In the first experiment, we investigate
generalization to longer sequences. In this setting, the model is trained on trajectories of length 10
while being tested on trajectories of length 20. In the second experiment, we evaluate generalization
in more crowded scenes. In this setting, the model is trained on 15-25 objects and tested on 50-60
objects. The third experiment tests the generalization of the model to unseen objects. The model
is trained only on moving MNIST images containing digits 0 to 5 while being tested on images
containing digits 6 to 9. Figures 12 and 13 show samples from these experiments. “SCALOR - LG”
in Table 1 also provides tracking results for the “length generalization” setting.
17
Published as a conference paper at ICLR 2020
D Experiment Detail and Additional Qualitative results on
Grand Central Station Dataset
For natural-scene experiments, we spatially split the video into 8 parts and create a dataset of 400k
frames in total. We choose the first 360k frames for training and 40k frames for testing. Since the
movement of pedestrians is too slow under 25 fps in the original video, we treat every other 7 frames
as two consecutive frames in the dataset sequence. The length of the input sequence is 10, and each
image is resized to 128 × 128.
5 frames are generated (after the red line). (a) Overall reconstruction and generation, (b) Conditional genera-
tion of background, (c) Conditional generation of segmented objects, (d) Conditional generation of movement
trajectories
The generation result mentioned in Section 5.2 is provided in Figure 14. We provide additional
qualitative results in Figures 15 to 20. Rows from top to bottom represent input sequence, overall
reconstruction, extracted background, foreground segmentation mask with IDs, center position from
each ztp,ons latent, and extracted trajectories based on the transition of center position. Additionally,
Figures 18 to 20 show examples of conditional generation, where images starting from time-step 6
(after the red line) are generated sequences. Rows from top to bottom are conditional generations of
the overall images, conditional generations of background, extracted conditional generation of each
segmentation mask, and conditional generation of trajectories.
18
Published as a conference paper at ICLR 2020
Figure 15: Tracking sample 1
19
Published as a conference paper at ICLR 2020
Figure 17: Tracking sample 3
20
Published as a conference paper at ICLR 2020
21
Published as a conference paper at ICLR 2020
E Model Architecture Details
In this section, we provide additional details of the architecture and hyperparameters used for pedes-
trian detection. When a new frame is provided, the network uses a fully convolutional encoder to
obtain a H ×W feature map. The feature map is fed into a convolutional LSTM to model the sequen-
tial information along the sequence. The convolutional LSTM is shared across discovery module
and propagation module. The discovery module and propagation module also share the zwhat en-
coder and decoder. The encoder has a convolutional network followed by one fully connected layer,
while the glimpse decoder uses a fully convolutional network with sub-pixel layer (Shi et al., 2016)
for upsampling. The background module shares a similar structure with the zwhat encoder and de-
coder. It takes a 4-dimensional input, i.e., RGB and foreground mask, and outputs a 3-dimensional
image. We use GRUs in propagation trackers and in prior transition networks.
We choose a batch size of 20 for the natural scene experiments and a batch size of 16 for
MNIST/dSprites experiments. The learning rate is fixed at 4e-5 for natural image experiments and
5e-4 for dSprites/MNIST experiments. We use RMSprop for optimization during training. The
standard deviation of the image distribution is chosen to be 0.1 for natural experiments and 0.2 for
toy experiments. The prior for all Gaussian posteriors is set to standard normal. For the pedestrian
tracking dataset, we constrain the range of zscale so that the inferred width can vary from 5.2 pixels
to 11.7 pixels, and the height can vary from 12.0 to 28.8, and both with a prior of the middle value
in discovery. Similarly, we constrain zscale on synthetic datasets so that it can vary from half to 1.5
times the actual object size. The zpos variable in the propagation phase is modeled as the deviation
of the position from the previous time-step instead of the global coordinate. The prior for zpres in
discovery is set to be 0.1 at the beginning of training and to quickly anneal to 1e-3 for natural image
experiments and 1e-4 for dSprites/MNIST experiments. The temperature used for modelling zpres is
set to be 1.0 at the beginning and anneal to 0.3 after 20k iterations.
Full details of the architecture will be released along with our code.
22