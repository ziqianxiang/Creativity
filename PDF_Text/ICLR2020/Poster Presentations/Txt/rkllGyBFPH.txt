Published as a conference paper at ICLR 2020
Beyond Linearization: On Quadratic and
Higher-Order Approximation of Wide Neural
Networks
Yu Bai
Salesforce Research
yu.bai@salesforce.com
Jason D. Lee
Princeton University
jasonlee@princeton.edu
Ab stract
Recent theoretical work has established connections between over-parametrized
neural networks and linearized models governed by the Neural Tangent Kernels
(NTKs). NTK theory leads to concrete convergence and generalization results,
yet the empirical performance of neural networks are observed to exceed their
linearized models, suggesting insufficiency of this theory.
Towards closing this gap, we investigate the training of over-parametrized neu-
ral networks that are beyond the NTK regime yet still governed by the Taylor
expansion of the network. We bring forward the idea of randomizing the neu-
ral networks, which allows them to escape their NTK and couple with quadratic
models. We show that the optimization landscape of randomized two-layer net-
works are nice and amenable to escaping-saddle algorithms. We prove concrete
generalization and expressivity results on these randomized networks, which lead
to sample complexity bounds (of learning certain simple functions) that match the
NTK and can in addition be better by a dimension factor when mild distributional
assumptions are present. We demonstrate that our randomization technique can be
generalized systematically beyond the quadratic case, by using it to find networks
that are coupled with higher-order terms in their Taylor series.
1 Introduction
Deep Learning has made remarkable impact on a variety of artificial intelligence applications such
as computer vision, reinforcement learning, and natural language processing. Though immensely
successful, theoretical understanding of deep learning lags behind. It is not understood how non-
linear neural networks can be efficiently trained to approximate complex decision boundaries with a
relatively few number of training samples.
There has been a recent surge of research on connecting neural networks trained via gradient descent
with the neural tangent kernel (NTK) (Jacot et al., 2018; Du et al., 2018a;b; Chizat & Bach, 2018b;
Allen-Zhu et al., 2018a; Arora et al., 2019a;b). This line of analysis proceeds by coupling the
training dynamics of the nonlinear network with the training dynamics of its linearization in a local
neighborhood of the initialization, and then analyzing the expressiveness and generalization of the
network via the corresponding properties of its linearized model.
Though powerful, NTK is not yet a completely satisfying theory for explaining the success of deep
learning in practice. In theory, the expressive power of the linearized model is roughly the same
as, and thus limited to, that of the corresponding random feature space (Allen-Zhu et al., 2018a;
Wei et al., 2019) or the Reproducing Kernel Hilbert Space (RKHS) (Bietti & Mairal, 2019). While
these spaces can approximate any regular (e.g. bounded Lipschitz) function up to arbitrary accu-
racy, the norm of the approximators can be exponentially large in the feature dimension for certain
non-smooth but very simple functions such as a single ReLU (Yehudai & Shamir, 2019). Using
NTK analyses, the sample complexity bound for learning these functions can be poor whereas ex-
perimental evidence suggests that the sample complexity is mild (Livni et al., 2014). In practice,
kernel machines with the NTK have been experimentally demonstrated to yield competitive results
on large-scale tasks such as image classification on CIFAR-10; yet there is still a non-neglible per-
1
Published as a conference paper at ICLR 2020
formance gap between NTK and full training on the same convolutional architecture (Arora et al.,
2019a; Lee et al., 2019). It is an increasingly compelling question whether we can establish theories
for training neural networks beyond the NTK regime.
In this paper, we study the optimization and generalization of over-parametrized two-layer neural
networks via relating to their higher-order approximations, a principled generalization of the NTK.
Our theory starts from the fact that a two-layer neural network fW0+W (x) (with smooth activation)
can be Taylor expanded with respect to the weight matrix W as
m	m	∞m
fW0+W (x) =	arσ((w0,r + wr)>x) =	arσ(w0>,rx) +
r=1
r=1
X--------
k=1 r=1
/	、
σ(k)(w>,rχ)".
k! r
________ - /
fW0(x)	fW(k)0,W(x)
Above, fW0 does not depend on W, and f(1) corresponds to the NTK model, which is the dominant
W-dependent term when {wr} are small and leads to the coupling between the gradient dynamics
for training neural net and its NTK f(1).
Our key observation is that the dominance of f(1) is deduced from comparing the upper bounds—
rather than the actual values—of fW(k),W(x). It is a priori possible that there exists a subset of W’s
in which the dominating term is not f(1) but some other f(k), k ≥ 2. If we were able to train in
that set, the gradient dynamics would be coupled with the dynamics on f(k) rather than f(1) and
thus could be very different. That learning is coupled with f(k) could further offer possibilities for
expressing certain functions with parameters of lower complexities, or generalizing better, as f(k)
is no longer a linearized model. In this paper, we build on this perspective and identify concrete
regimes in which neural net learning is coupled with higher-order f(k)’s rather than its linearization.
The contribution of this paper can be summarized as follows.
•	We demonstrate that after randomization, the linear NTK f(1) is no longer the dominant term, and
so the gradient dynamics of the neural net is no longer coupled with NTK. Through a simple sign
randomization, the training loss of an over-parametrized two-layer neural network can be coupled
with that of a quadratic model (Section 3). We prove that the randomized neural net loss exhibits a
nice optimization landscape in that every second-order stationary point has training loss not much
higher than the best quadratic model, making it amenable to efficient minimization (Section 4).
•	We establish results on the generalization and expressive power of such randomized neural nets
(Section 5). These results lead to sample complexity bounds for learning certain simple functions
that matches the NTK without distributional assumptions and are advantageous when mild isotropic
assumptions on the feature are present. In particular, using randomized networks, the sample com-
plexity bound for learning polynomials (and their linear combination) on (relatively) uniform base
distributions is O(d) lower than using NTK.
•	We show that the randomization technique can be generalized to find neural nets that are domi-
nated by the k-th order term in their Taylor series (k > 2) which we term as higher-order NTKs.
These models also have expressive power similar as the linear NTK, and potentially even better
generalization and sample complexity (Section 6 & Appendix D).
1.1	Prior work
We review prior work on the optimization, generalization, and expressivity of neural networks.
Neural Net and Kernel Methods Neal (1996) first proposed the connection between infinite-
width networks and kernel methods. Later work (Daniely et al., 2016; Williams, 1997; Lee et al.,
2018; Novak et al., 2019; Matthews et al., 2018) extended this connection to various settings includ-
ing deep networks and deep convolutional networks. These works established that gradient descent
on only the output layer weights is well-approximated by a kernel method for large width.
More recently, several groups discovered the connection between gradient descent on all the param-
eters and the neural tangent kernel (Jacot et al., 2018). Li & Liang (2018); Du et al. (2018b) utilized
the coupling of the gradient dynamics to prove that gradient descent finds global minimizers of the
2
Published as a conference paper at ICLR 2020
training loss of two-layer networks, and Du et al. (2018a); Allen-Zhu et al. (2018b); Zou et al. (2018)
generalized this to deep residual and convolutional networks. Using the NTK coupling, Arora et al.
(2019b); Cao & Gu (2019a;b) proved generalization error bounds that match the kernel method.
Despite the close theoretical connection between NTK and training deep networks, Arora et al.
(2019a); Lee et al. (2019); Chizat & Bach (2018b) empirically found a significant performance gap
between NTK and actual training. This gap has been theoretically studied in Wei et al. (2019); Allen-
Zhu & Li (2019); Yehudai & Shamir (2019); Ghorbani et al. (2019a) which established that NTK
has provably higher generalization error than training the neural net for specific data distributions
and architectures.
The idea of randomization is initiated by Allen-Zhu et al. (2018a), who use randomization to prov-
ably learn a three-layer network; however it is unclear how the sample complexity of their algorithm
compares against the NTK. Inspired by their work, we study the potential gains of coupling with
a non-linear approximation over the linear NTK — we compare the performance of a quadratic
approximation model with the linear NTK on two-layer networks and find that under mild data
assumptions the quadratic approximation reduces sample complexity under mild data assumptions.
Outside the NTK Regime It is believed that the success of SGD is largely due to its algorithmic
regularization effects. A large body of work Li et al. (2017); Nacson et al. (2019); Gunasekar et al.
(2018b;a; 2017); Woodworth et al. (2019) shows that asymptotically gradient descent converges to
a max-margin solution with a strong regularization effect, unlike the NTK regularization1.
For two-layer networks, a series of works used the mean field method to establish the evolution of the
network parameters via a Wasserstein gradient flow (Mei et al., 2018b; Chizat & Bach, 2018a; Wei
et al., 2018; Rotskoff & Vanden-Eijnden, 2018; Sirignano & Spiliopoulos, 2018). In the mean field
regime, the parameters move significantly from their initialization, unlike NTK regime, however it
is unclear if the dynamics converge to solutions of low training loss.
Finally, Li et al. (2019) showed how a combination of large learning rate and injected noise amplifies
the regularization from the noise and outperforms the NTK of the corresponding architecture.
Landscape Analysis Many prior works have tried to establish favorable landscape properties such
as every local minimum is a global minimum (Ge et al., 2017; Du & Lee, 2018; Soltanolkotabi et al.,
2018; Hardt & Ma, 2016; Freeman & Bruna, 2016; Nguyen & Hein, 2017a;b; Haeffele & Vidal,
2015; Venturi et al., 2018). Combining with existing advances in gradient descent avoiding saddle-
points (Ge et al., 2015; Lee et al., 2016; Jin et al., 2017), these show that gradient descent find the
global minimum. Notably, Du & Lee (2018); Ge et al. (2017) show that gradient descent converges
to solutions also of low test error, with lower sample complexity than their corresponding NTKs.
Complexity Bounds Recently, researchers have studied norm-based generalization based (Bartlett
et al., 2017; Neyshabur et al., 2015; Golowich et al., 2017), tighter compression-based bounds (Arora
et al., 2018), and PAC-Bayes bounds (Dziugaite & Roy, 2017; Neyshabur et al., 2017) that identify
properties of the parameter that allow for efficient generalization.
2	Preliminaries
Problem setup We consider the standard supervised learning task, in which we are given a labeled
dataset D = {(x1, y1), . . . , (xn, yn)}, where (xi, yi) ∈ X × Y are sampled i.i.d. from some
distribution P, and we wish to find a predictor f : X → Y. Without loss of generality, we assume
that X = Sd-1(Bx) ⊂ Rd for some Bx > 0 (so that the features are d-dimensional with norm Bx.)
Let ' : YX R → R≥o be a loss function such that '(y, 0) ≤ 1, and Z → '(y, Z) is convex, 1-
Lipschitz, and three-times differentiable with the second and third derivatives bounded by one for
1As a concrete example, Woodworth et al. (2019) showed that for matrix completion the NTK solution
estimates zero on all unobserved entries and the max-margin solution corresponds to the minimum nuclear
norm solution.
3
Published as a conference paper at ICLR 2020
all y ∈ Y . This includes for example the logistic and soft hinge loss for classification. We let
1n
L(f) := ED['(y, f(x))] := — E'(yi,f(xi)) and LP(f):= E(χ,y)〜P['(y,f(x))]
ni=1
denote respectively the empirical risk and population risk for any predictor f : X → Y .
Over-parametrized two-layer neural network We consider learning an over-parametrized two-
layer neural network of the form
1	1m
fw(x) = fa,w(x) := √ma>σ(W>x) = √m ɪ^arσ(w>x),
(1)
where W = [wι,..., w" ∈ Rd×m is the first layer and a = [aι,..., am]> ∈ Rm is the second
layer. The 1/√m factor is chosen to account for the effect of over-parametrization and is consistent
with the NTK-type scaling of (Du et al., 2018b; Arora et al., 2019b). In this paper we fix a and only
train W (and thus use fW to denote the network.)
Throughout this paper we assume that the activation is second-order smooth in the following sense.
Assumption A (Smooth activation). The activation function σ ∈ C2 (R), and there exists some
absolute constant C > 0 such that ∣σ0(t)∣ ≤ Ct2, ∣σ00(t)| ≤ C|t|, and σ00(∙) is C-Lipschitz.
An example is the cubic ReLU σ(t) = relu3(t) = max {t, 0}3. The reason for requiring σ to be
higher-order smooth (and thus excluding ReLU) will be made clear in the subsequent text2.
2.1	Notation
We typically reserve lowercases a, b, α, β, . . . for scalars, bold lowercases a, b, α, β, . . . for vec-
tors, and bold uppercases A, B, . . . for matrices. For a matrix A = [a1, . . . , am] ∈ Rd×m, its
2,p norm is defined as kAk2,p := (Prm=1 kark2p)1/p for all p ∈ [1, ∞]. In particular we have
∣∣∙k2 2 = I"" We let B2,p(R) := {W : ∣∣W∣∣2p ≤ R} denote a 2,p-norm ball of radius R. We
use standard Big-Oh notation: a = O(b) for stating a ≤ Cb for some absolute constant C > 0,
and a = O(b) for a ≤ Cb where C depends at most logarithmically in b and all other problem
parameters. For a twice-differentiable function f : Rd → R, x? is called a second-order stationary
point if Vf (x?) = 0 and V2f (x?)上 0.
3	Escaping NTK via randomization
To motivate our study, we now briefly review the NTK theory for over-parametrized neural nets and
provide insights on how to go beyond the NTK regime.
Let W0 denote the weights in a two-layer neural network at initialization and W denote its move-
ment from W0 (so that the current weight matrix is W0 + W.) The observation in NTK theory, or
the theory of lazy training (Chizat & Bach, 2018b), is that for small W the neural network fW0+W
can be Taylor expanded as
fwo+w (x) = √m X arσ((wo,r + Wr )>x)
r≤m
:√m X arσ(w>rx) + √m X arσ'(W[rX)(Wrx) +O I √m ^X (Wrx),
r≤m
S-------{-
fW0 (x)
r≤m
} '---------------7—
:=fWL (x)
r≤m
}
2We note that the only restrictive requirement in Assumption A is the Lipschitzness of σ00, which guarantees
second-order smoothness of the objectives. The bounds on derivatives (and specifically their bound near zero)
are merely for technical convenience and can be weakened without hurting the results.
4
Published as a conference paper at ICLR 2020
so that the network can be decomposed as the sum of the initial network fW0, the linearized model
fWL , and higher order terms. Specifically (ignoring fW0 for the moment), when m is large and
kwrk2 = O(m-1/2), we expect fWL = O(1) and higher order terms to be om(1), which is in-
deed the regime when we train fW0 +W via gradient descent. Therefore, the trajectory of training
fW0+W is coupled with the trajectory of training fW0 +fWL , which is a convex problem and enjoys
convergence guarantees (Du et al., 2018b).
Our goal is to find subsets of W so that the dominating term is not fL but something else in the
higher order part. The above expansion makes clear that this cannot be achieved through simple
fixes such as tuning the leading scale 1 /√m or the learning rate — the domination of f L appears to
hold so long as the movements wr are small.
Randomized coupling with quadratic model We now explain how the idea of randomization,
initiated in (Allen-Zhu et al., 2018a), can help get rid of the domination of fL. Let W be a fixed
weight matrix. Suppose for each weight vector wr, we sample a random variable Σrr ∈ R and
consider instead the random weight matrix
WΣ := Wdiag({Σrr}rm=1) = [Σ11w1, . . . , Σrrwr],
then the second-order Taylor expansion of fW0 +WΣ can be written as
1m
fwo+w∑(x) = √mɪ^ar σ ((W0,r + Wr ∑rr )>x)
r=1
1m	1 m
: fWo (x) + √mɪ2ar σ0(w>r x)(∑rr W>x) + ^^m^a σ00(w›r x)∑rKw>x)2 + ...,
r=1	r=1
|
}|
{^^"∖^^^^^
=fWL Σ (x)
^^^^{^^^^^
:=fWQ Σ (x)
}
where we have defined in addition the quadratic part fWQ Σ . Due to the existence of {Σrr }, each
original weight wr now has an additional a scalar that is different in fL and fQ. Specifically, ifwe
choose
Σrr 吧 Unif{±1}	(2)
to be random signs, then we have Σr2r ≡ 1 and thus fWQ Σ(x) ≡ fWQ (x), whereas E[Σrr] = 0 so
that E[fWL Σ(x)] ≡ 0. Consequently, fQ is not affected by such randomization whereas fWL Σ is now
mean zero and thus can have substantially lower magnitude than fWL .
More precisely, when kwr k2	m-1/4, the scalings of fL and fQ compare as follows:
•	We have EΣ [fWL Σ(x)] = 0 and
1m	1m
E∑ [(fW ∑(X))2] = m£ar/(w>rX)2 (w>X)2 = O (m EkWrk2) =OkmT 1/),
r=1	r=1
so we expect fWL Σ(X) = O(m-1/4) over a random draw of Σ.
•	The quadratic part scales as
mm
fW ∑(x) = fW (x) = 2√= X ar σ00(w>rX)(W>x)/ = O	X kwr k2 )=。⑴.
2 m r=1	r=1
Therefore, at the random weight matrix WΣ, fQ dominates fL and thus the network is coupled
with its quadratic part rather than the linear NTK.
3.1 Learning randomized neural nets
The randomization technique leads to the following recipe for learning W: train W so that kwr k2 =
O(m-1/4) and WΣ has in expectation low loss. We make this precise by formulating the problem
as minimizing a randomized neural net risk.
5
Published as a conference paper at ICLR 2020
d
Randomized risk Let L : Rd×m → R denote the vanilla empirical risk for learning fW :
L(W) = ED ['(y, fW0+W(X))],
where we have reparametrized the weight matrix into W0 + W so that learning starts at W = 0.
Following our randomization recipe, we now formulate our problem as minimizing the expected risk
. . 一~ . ., 一 . ..,
L(W)= e∑[l(wςP = e∑,d ['(y,fWo+w∑(X))],
where Σ ∈ Rm×m is a diagonal matrix with ∑rr 吧 Unif{±1}. To encourage k w/=O(m-1/4 * *)
and improve generalization, we consider a regularized version of L and L with `2,4 regularization:
〜
〜
28,4 and Lλ(W) = L(W)+λkWk82,4=EΣ[Leλ(WΣ)].
Lλ(W):= L(W) + λ IlWk
Our regularizer penalizes W, i.e. the distance from initialization, similar as in (Hu et al., 2019).3
Symmetric initialization We initialize the parameters (a, W0) randomly in the following way:
set
a1
am/2 = +1, am/2+1
am = -1,
wo,r = wo,r+m∕2 蚓 N(0,B-2Id), VT ∈ [m∕2].
(3)
Above, we set half of the ai’s as +1 and half as -1, and the weights w0,r are i.i.d. in the +1 half
and copied exactly into the -1 half. Such an initialization is almost equivalent to i.i.d. random W0,
but has the additional benefit that fW0 (x) ≡ 0 and also leads to simple expressivity arguments. Our
initialization scale B-2 is chosen so that for a random draw of wo, We have w>X 〜N (0,1), which
is on average O(1)4. For technical convenience, we also assume henceforth that the realized {w0,r}
satisfies the bound
max(Bχ ∣∣w0,r∣∣2) = O (Pd + log(m∕δ)) = Oe(√d).
r∈[m]
(4)
This happens with probability at least 1 - δ under random initialization (see proof in Appendix A.3),
and ensures that maxr∈m] |w>/x| ≤ O(√d) simultaneously for all x.
4 Optimization
In this section, we show that Lλ enjoys a nice optimization landscape.
4.1	Nice landscape of clean risk
As the randomized loss L induces coupling of the neural net fW0+WΣ with the quadratic model
fWQ , we expect its behavior to resemble the behavior of gradient descent on the following clean risk:
1n	1n	1
LQ(W) := — E'(yi,fW(Xi)) = — E' yi, y7=〈XiX>, WDiW>〉.
n	W	n	2m
i=1	i=1
Above, we have defined diagonal matrices Di = diag( arσ00(w0>,rXi) r∈[m] ) ∈ Rm×m which are
not trained.
We now show that the clean risk LQ, albeit non-convex, possesses a nice optimization landscape.
Lemma 1 (Landscape of clean risk). Suppose there exists W? ∈ Rd×m such that LQ(W?) ≤ OPT.
Let Σ0 ∈ Rm×m be a diagonal matrix with Σ∖ 吧 Unif {±1} ,then we have
E∑o N2LQ(W)[W*Σ0, W*Σ0]]
≤ (VLQ (W), W〉- 2(LQ(W) - OPT)+ 0(dBX |四|h |W?k2,4 m-1
(5)
3Our specific choice of ∣∣∙∣∣2 4 norm is needed for measuring the average magnitude of f!QQ, whereas the
high (8-th) power is not essential and can be replaced by any (4 + )-th power without affecting the result.
4Our choice covers two commonly used scales in neural net analyses: BX = 1, wo,r 〜 N(0,Id) in
e.g. (Arora et al., 2019b; Allen-Zhu et al., 2018a); BX = vzd, wo,r 〜N(0,Id/d) in e.g. (Ghorbani et al.,
2019b).
6
Published as a conference paper at ICLR 2020
This result implies that, for W in a certain ball and large m, every point of higher loss than W?
will have either a first-order or a second-order descent direction. In other words, every approximate
second-order stationary point of LQ is also an approximate global minimum. Our proof utilizes the
fact that LQ is similar to the loss function in matrix sensing / learning quadratic neural networks, and
builds on recent understandings that the landscapes of these problems are often nice (Soltanolkotabi
et al., 2018; Du & Lee, 2018; Allen-Zhu et al., 2018a). The proof is deferred to Appendix B.1.
4.2	Nice landscape of randomized neural net risk
With the coupling between fW0+WΣ (x) and fWQ (x) in hand, we expect the risk L(W) =
E[L(WΣ)] to enjoy similar guarantees as the clean risk does LQ (W) in Lemma 1. We make
this precise in the following result.
Theorem 2 (Landscape of L). Suppose there exists W? ∈ B2,4 (Bw,?) such that LQ(W?) ≤ OPT,
and that
m ≥ O([BX2BW2 + d4BXBW + d”X0BW0]e-4 + d5BXBWe-2).
for some fixed e ∈ (0, 1] and Bw ≥ Bw,?, then for all W ∈ B2,4 (Bw), we have
E∑o[V2L(W)[W*Σ0, W*Σ0]] ≤〈VL(W), Wi — 2(L(W) — OPT) + e.
(6)
(7)
As an immediate corollary, we have a similar characterization of the regularized loss Lλ.
Corollary 3 (Landscape of Lλ). For any Bw ≥ Bw,?, under the conditions of Theorem 2, we have
for all λ > 0 and all W ∈ B2,4 (Bw ) that
E∑o[V2Lλ(W)[W*∑0, W*∑0]]
≤ hVLλ(W), Wi- 2(Lλ(W) - OPT) — λ ∣W∣∣2,4 + Cλ ∣∣W*% + e,
where C = O(1) is an absolute constant.
(8)
Theorem 2 follows directly from Lemma 1 through the coupling between L and LQ (as well as
their gradients and Hessians). Corollary 3 then follows by controlling in addition the effect of the
regularizer. The full proof of Theorem 2 and Corollary 3 are deferred to Appendices B.4 and B.5.
We now present our main optimization result, which follows directly from Corollary 3.
Theorem 4 (Optimization of Lλ). Suppose there exists W? such that
LQ(W?) ≤OPT and kW?k2,4 ≤ Bw,?	(9)
for some OPT > 0. For any γ = Θ(1) and e > 0, we can choose λ suitably and m ≥
1
O(poly(d, BxBw,?, e-1)) such that the regularized loss Lλ satisfies the following: any second order
stationary point W has low loss and bounded norm:
Lλ(Wc) ≤ (1+γ)OPT+e and	Wc	≤ O(Bw,?).	(10)
Proof sketch. The proof of Theorem 4 consists of two stages: first “localize” any second-order sta-
tionary point into a (potentially very big) norm ball using the ∣∣∙∣∣2 4 regularizer, then use Corollary 3
in this ball to further deduce that Lλ is low and Wc ≤ O(kW?k2,4). The full proof is deferred
to Appendix B.6.
Efficient optimization & allowing large learning rate Theorem 4 states that when the over-
parametrization is enough, any second-order stationary point (SOSP) Wc of Lλ has loss competitive
with OPT, the performance of best quadratic model. Consequently, algorithms that are able to find
SOSPs (escape saddles) such as noisy SGD (Jin et al., 2019) can efficiently minimize Lλ to up
to a multiplicative / additive factor of OPT. Further, by sampling a fresh Σ at each iteration and
using the stochastic gradient VWLλ (WΣ) (rather than computing the full VLλ (W)), the noisy
SGD iterates can be computed efficiently. We note in passing that our coupling results work in any
`2,4 ball of O(1) size further allows the use of a large learning rate: as soon as the learning rate
is bounded by O(m1/4), we would have kWtk2,4 ≤ O(1) in a constant number of iterations, and
thus our coupling and landscape results would hold. This is in contrast with the NTK regime which
requires the learning rate to be bounded by O(1) (Du et al., 2018b).
7
Published as a conference paper at ICLR 2020
5 Generalization and Expressivity
We now shift attention to studying the generalization and expressivity of the (randomized) neural
net Wc learned in Theorem 4.
5.1 Generalization
As Wc is always coupled (through randomization) with the quadratic model fcQ , we begin by study-
ing the generalization of the quadratic model.
Generalization of quadratic models Let
FQ(Bw) := nx 7→ fWQ (x) : kWk2,4 ≤Bwo
denote the class of quadratic models for W in a `2,4 ball. We first present a lemma that relates the
Rademacher complexity of FQ(Bw) to the expected operator norm of certain feature maps.
Lemma 5 (Bounding generalization of fQ via feature operator norm). For any non-negative loss
' SUCh that Z → '(y,z) is I-LiPsChitz and '(y, 0) ≤ 1 for all y ∈ Y, we have the RademaCher
complexity bound
Eσ,x
1n	Q
SUp — fσi'(yi, fWW(Xi))
kk2,4 ≤Bw ni=1
≤ Bw2 Eσ,x
max
r∈[m]
where σi iid Unif {±1} are Rademacher variables.
Operator norm based generalization Lemma 5 suggests a possibility for the quadratic model
to generalize better than the NTK model: the Rademacher complexity of FQ (Bw) depends on the
“feature maps” n P2ι σiσ00(w>r Xi)Xix> through their matrix operator norm. Compared with the
(naive) Frobenius norm based generalization bounds, the operator norm is never worse and can be
better when additional structure on X is present. The proof of Lemma 5 is deferred to Appendix C.1.
We now state our main generalization bound on the (randomized) neural net loss L, which con-
cretizes the above insight.
Theorem 6 (Generalization of randomized neural net loss). For any data-dePendent Wc suCh that
≤ Bw, we have
2,4
Ewo,d [l(W) - LP(W)] ≤ O (BB√Mx,op + √n) + O(b3Bwm-1/4 + d2B马Bwm-1/2),
1/2
where Mx,θp := (B-2Ex，ɪ P>ι XiX ∣∣°p]) is the (rescaled) operator norm of the empiri-
cal covariance matrix. In particular, Mχ,op ≤ 1 always holds; ifin addition vτx is K ,Var(v>x)
Sub-GaUSSian for all V ∈ SdT(I) and K(Cov(X)) ≤ K, then Mx,θp ≤ κ∕√d whenever
n ≥ O(K4d).
The generalization bound in Theorem 6 features two desirable properties:
(1)	For large m (e.g. m & n4), the bound scales at most logarithmically with the width m, therefore
allowing learning with small samples and extreme over-parametrization;
(2)	The main term O(BxBwMx,op∕√n) automatically adapts to properties of the feature distri-
bution and can lower the generalization error than the naive bound by at most O(1∕√d) without
requiring US to tune any hyperparameter. Concretely, We have Mx,op ≤ O(1∕√d) when X has an
isotropic distribution such as Unif(Sd-1(Bx)) or Unif{±Bx∕√d}d.
Theorem 6 follows directly from Lemma 5 and a matrix concentration Lemma. The proof is deferred
to Appendix C.2.
8
Published as a conference paper at ICLR 2020
5.2 Expressivity and Sample Complexity through Quadratic Models
In order to concretize our generalization result, we now study the expressive power of quadratic
models through the concrete example of learning functions of the form Pj≤k αj (βj>x)pj , i.e. sum
of “one-directional” polynomials (for consistency and comparability with (Arora et al., 2019b).)
Theorem 7 (Expressing a sum of polynomials through fQ). Suppose {(ar , w0,r)} are generated
according to the Symmetric initialization (3) and we use σ(t) = 6relu3(t) (so that σ00(t) = relu(t).)
If f?(x) = Pk=ι αj(β>x)pj achieves training loss L(f?) ≤ e0, where Pj — 2 ∈ {1} ∪ {2'}'≥0,.
Then so long as the width is sufficiently large:
m ≥ O (ndk2 XPjaj(Bx kβjk2)2pje-2 j ,
we have with probability at least 1 — δ (over W0) that there exists W? ∈ Rd×m such that
LQ(W?) ≤ OPT = eo + e and ∣∣W*kj,4 ≤ Bw,? = O 卜XPjajB")kβk2pj δ-1 j .
The proof of Theorem 7 is based on a reduction from expressing degree P polynomials using
quadratic models to expressing degree P — 2 polynomials using random feature models. The proof
can be found in Appendix C.4.
Comparison between quadratic and linearized (NTK) models We now illustrate our results in
Theorem 6 and 7 in three concrete examples, in which we compare the sample complexity bounds
of the randomized (quadratic) network and the linear NTK when m is sufficiently large.
Learning a single polynomial. Suppose f? (x) = a(β>x)p satisfies L(f?) ≤ e, and we wish to
find Wc with O(e) test loss. By Theorem 7 we can choose W? such that LQ (W? ) ≤ OPT = 2e,
and by Theorem 4 we can find W such that L(W) ≤ Lλ (W) ≤ 3e and kWkj,4 = O(Bw,?). Take
Bx = 1, and assume X is sufficiently isotropic so that Mx,op = O(√), the sample complexity from
Theorem 6 is
n ≥ O(BxBwMx,op)= O(Ipjgl⅜
nQ.
In contrast, the sample complexity for linear NTK (Arora et al., 2019b; Cao & Gu, 2019a) to reach
e test loss is
n ≥ O(P2Rβk2p
ej
nL.
We havenQ /nL = O(P/d), a reduction by a dimension factor unless P d. We note that the above
comparison is simply comparing upper bounds, since in general the lower bound on the sample
complexity of linear NTK is unknown.
Learning a noisy 2-XOR. Wei et al. (2019) established a sample complexity lower bound of linear
NTK of n ≥ ul = Ω(d2) to achieve constant generalization error on the noisy 2-XOR problem,
which allows for a rigorous comparison against the quadratic model.
The ground truth function in 2-XOR is f?(x) = x1xj = ([(e1 + ej)>x]j — [(e1 — ej)>x]j)/4,
where x ∈ {±1}d, and f? attains constant margin on the training distribution constructed in Wei
et al. (2019). By Theorem 7, f? can be e-approximated by fWQ with Bw4 ,? ≤ O(1). Thus by
Theorem 6 the sample complexity for learning noisy 2-XOR through the randomized net Wc is
n ≥ nQ
BxBw ,*Mj,°p
ej
This is O(d) better than the sample complexity lower bound of linear NTK and thus provably better.
9
Published as a conference paper at ICLR 2020
Low-rank matrix sensing. Suppose we wish to learn a symmetric low-rank matrix A? ∈ Rd×d
with kA? kop ≤ 1 and rank(A?) ≤ r through n rank-one observations of the form yi = f? (xi) =
(A?, XiX>>where X 〜 Unif(Sd-1(√d)). This ground truth function can be written as f?(Xi)=
pr=ι αj (v>Xi)2 where ∣αj | ≤ 1 are the eigenvalues of A and Vj ∈ Rd are the corresponding
eigenvectors. For any 1-Lipschitz loss such as the absolute loss, by Theorem 7, there exists W?
such that LQ(W?)	≤ OPT =	and Bw4 ,?	≤	O(r	Pjr=1	kvj k42)	= O(r2).	Thus by Theorem 6, the
sample complexity of reaching 2 test loss
ExA?,XX> - fWc (X)	≤ 2
through the randomized net Wc is
n ≥ nQ
BxBw ,*M2,op
e2
O
This compares favorably against the sample complexity upper bound for linear NTK, which needs
n ≥ nL
Bx∙(Pj=ι αjkVj k2)2
Z2
samples.
6	Higher-order NTKs
We demonstrate that our idea of randomization for changing the dynamics of learning neural net-
works can be generalized systematically — through randomization we are able to obtain over-
parametrized neural networks in which the k-th order term dominates the Taylor series. Due to
space constraints, we provide our concrete results on the coupling, generalization, and expressivity
of higher-order models in Appendix D.
7	Conclusion
In this paper we proposed and studied the optimization and generalization of over-parametrized neu-
ral networks through coupling with higher-order terms in their Taylor series. Through coupling with
the quadratic model, we showed that the randomized two-layer neural net has a nice optimization
landscape (every second-order stationary point has low loss) and is thus amenable to efficient min-
imization through escape-saddle style algorithms. These networks enjoy the same expressivity and
generalization guarantees as linearized models but in addition can generalize better by a dimension
factor when distributional assumptions are present. We extended the idea of randomization to show
the existence of neural networks whose Taylor series is dominated by the k-th order term.
We believe our work brings in a number of open questions, such as how to better utilize the ex-
pressivity of quadratic models, or whether the study of higher-order expansions can lead to a more
satisfying theory for explaining the success of full training. We also note that the Taylor series is
only one avenue to obtaining accurate approximations of nonlinear neural networks. It would be
of interest to design other approximation schemes for neural networks that are coupled with the
network in larger regions of the parameter space.
Acknowledgment
The authors would like to thank Wei Hu, Tengyu Ma, Song Mei, and Andrea Montanari for their
insightful comments. JDL acknowledges support of the ARO under MURI Award W911NF-11-1-
0303, the Sloan Research Fellowship, and NSF CCF #1900145. The authors also thank the Simons
Institute Summer 2019 program on the Foundations of Deep Learning, and the Institute of Advanced
Studies Special Year on Optimization, Statistics, and Theoretical Machine Learning for hosting the
authors.
10
Published as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? arXiv
preprint arXiv:1905.10337, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint
arXiv:1905.12173, 2019.
Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019a.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. arXiv preprint arXiv:1905.13210, 2019b.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, pp. 3040-3050, 2018a.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018b.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with
quadratic activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540, 2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pp. 797-842, 2015.
11
Published as a conference paper at ICLR 2020
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks. arXiv preprint arXiv:1906.08899, 2019a.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. arXiv preprint arXiv:1904.12191, 2019b.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems,pp. 6151-6159, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018b.
Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv preprint arXiv:1506.07540, 2015.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Wei Hu, Zhiyuan Li, and Dingli Yu. Understanding generalization of deep neural networks trained
with noisy labels. arXiv preprint arXiv:1905.11368, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape sad-
dle points efficiently. In Proceedings of the 34th International Conference on Machine Learning,
pp. 1724-1732, 2017.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. Stochastic gradient
descent escapes saddle points efficiently. arXiv preprint arXiv:1902.04811, 2019.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
B1EA-M-0Z.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on Learning Theory, pp. 1246-1257, 2016.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. arXiv preprint arXiv:1712.09203,
2017.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019.
12
Published as a conference paper at ICLR 2020
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. In Advances in neural information processing Systems, pp. 855-863, 2014.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018.
Song Mei, Yu Bai, Andrea Montanari, et al. The landscape of empirical risk for nonconvex losses.
The Annals of Statistics, 46(6A):2747-2774, 2018a.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layers neural networks. In Proceedings of the National Academy of Sciences, volume 115, pp.
E7665-E7671, 2018b.
Ha Quang Minh, Partha Niyogi, and Yuan Yao. Mercer’s theorem, feature maps, and smoothing. In
International Conference on Computational Learning Theory, pp. 154-168. Springer, 2006.
Mor Shpigel Nacson, Suriya Gunasekar, Jason D Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. arXiv
preprint arXiv:1905.07325, 2019.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29-53.
Springer, 1996.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A PAC-
Bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv
preprint arXiv:1704.08045, 2017a.
Quynh Nguyen and Matthias Hein. The loss surface and expressivity of deep convolutional neural
networks. arXiv preprint arXiv:1710.10928, 2017b.
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolafia, Jeffrey
Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many chan-
nels are gaussian processes. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=B1g30j0qF7.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.
arXiv preprint arXiv:1805.00915, 2018.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv
preprint arXiv:1805.01053, 2018.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and TrendsR
in Machine Learning, 8(1-2):1-230, 2015.
Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension
have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge University Press, 2018.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
13
Published as a conference paper at ICLR 2020
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. arXiv preprint arXiv:1810.05369, 2018.
Colin Wei, D Lee Jason, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. arXiv preprint arXiv:1810.05369, 2019.
Christopher KI Williams. Computing with infinite networks. In Advances in neural information
processing Systems,pp. 295-301, 1997.
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and
deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. arXiv preprint arXiv:1904.00687, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
A Technical tools
A. 1 A matrix operator norm concentration bound
Lemma 8 (Variant of Theroem 4.6.1, (Tropp et al., 2015)). Suppose {Ar,i}r∈[m],i∈[n] are fixed
symmetric d X d matrices, and {。力记同]吧 Unif{±1} are Rademacher variables. Letting
n
Yr =	σiAr
i=1
,i,
then we have
Eσ max kYrkop ≤ 4
r∈[m]
max v(Yr) log(2md),
r∈[m]
where
v(Y) := Eσ[Y2]op.
Proof. Applying the high-probability bound in (Tropp et al., 2015, Theorem 4.6.1) and the union
bound, we get
P max kYr kop ≥ t ≤ 2d	exp(-t2/2v(Yr))
r∈[m]	r∈[m]
≤ 2dm exp(-12∕2 max V(Yr)) = exp (-------------------
r∈[m]	r	2 maxr∈[m] v(Yr)
+ log(2dm)
Let V := maxr∈[m] v(Yr), we have by integrating the above bound over t that
E
mX kYr"/ ≤' min 卜P (-2ty +log(2dm)), l/t
≤
≤
ʌ/ 4V log(2dm) +
∞
√4V log(2dm)
exp(-t2 /2V + log(2dm))dt
ʌ/ 4V log(2dm) +
∞
√4V log(2dm)
exp(-t2 /4V)dt
≤ p4V log(2dm) + Y干厂 ≤ 4,V log(2dm).
2dm
□
14
Published as a conference paper at ICLR 2020
A.2 Expressing polynomials with random features
Lemma 9. Let σ(t) = relu(t) and w0 〜N(0, B-2Id) be Gaussian random features. For any
P ∈ {1}∪ {2'}'≥o and β ∈ Rd, there exists a random variable a = a(wo) such that
Ew0 [σ(w0>x)a] = α(β>x)p
and a satisfies the `2 norm bound
Ew0[a2] ≤ 2π(p∨ 1)3α2Bx2(p-1)d kβk22p.
Proof. Consider the ReLU random feature kernel
K(X, XO) = Ew0〜N(0,B-2ld)[relU(W>X)relu(W>x0儿
and let HK denote the RKHS associated with this kernel. By the equivalence of feature maps (Minh
et al., 2006, Proposition 1), for any feature map φ : Sd-1(Bx) 7→ H (where H is a Hilbert space)
that generates K in the sense that
K(X, X0) = hφ(X), φ(X0)iH,
we have for any function f that
kf k2HK = ai∈nHf nkak2H : f?(x) ≡ ha, φ(X)io,	(11)
and the infimum over a is attainable whenever it is finite.
For the ReLU random feature kernel K, let u := X>X0/Bx2 and N2 (ρ) denote a bivariate normal
distribution with marginals N(0, 1) and correlation ρ ∈ [-1, 1]. We have that
K (X, XO) = Ewo 〜N(0,B-2IdJrelU(W>x)relU(WO]
=E(Zι ,Z2)〜N2 (u) [relu(Zι )relu(Z2)]
=；(u(π — arccos U) + pl — u2)
_ 1 (1	∏ X∞	(2' — 3)!!	2'!
=2∏ 1 + 2U + 2(2' — 2)!!(2' — 1)(2')U J
= X	cp(X>XO)pBx-2p
p∈{0,1}∪{2'}'≥ι
E	<√pB-pX叱 √pB-p(X0产〉,
p∈{0,1}∪{2'}'≥ι
where the constants {cp} satisfy
c0 = 1/(2n), cI = 1/4,
c2' ≥ 2π(2'∖)2 (2') for ' ≥ 1，
(and thus Cp ≥ (2∏(p ∨ 1)3)T for all p), and XOk ∈ Rdk denote the k-wise tensor product of x.
Therefore, if we define feature map
Φ(x) ：= [√CpB-pX0p]
p∈{0,1}∪{2'}'≥ι
we have K(X, XO) = hφ(X), φ(XO)i. With this feature map, the function f?(X) = α(β>X)p can be
represented as
f?(x) ≡ hc?, φ(x)i where c? = [0,..., 0,α ∙ c—1∕2Bpβ0p, 0,...].
Thus by the feature map equivalence (11), we have f? ∈ HK and
kfk2HK ≤ kc?k2 = α2cp-1Bx2p kβk22p ≤ 2π(p ∨ 1)3α2Bx2p kβk22p.
Now apply the feature map equivalence (11) again with the random feature map
x→ {relu(w>x)}w0
(which maps into the inner product space of w0 〜 N(0, B-2Id)), We conclude that there exists
a = a(W0) such that f? = Ew0 [relu(W0>X)a] and
Ew0[a2] ≤ kf?k2HK ≤ 2π(p ∨ 1)3α2Bx2p kβk22p.
□
15
Published as a conference paper at ICLR 2020
A.3 Proof of Equation (4)
Let N be an 1/2-covering of Sd-1(1). We have |N | ≤ 5d and for any vector w ∈ Rd that kwk2 ≤
2 supv∈N (v>w) (see e.g. (Mei et al., 2018a, Section A).) We thus have
P max Bx kw0,rk2 ≥ t
≤ maxBx(v>w0,r) ≥ t
≤ exp(-t2/8 + log |N | + log m) ≤ exp(-t2/8 + dlog5 + log m).
Setting t =，8(dlog5 + log(m∕δ)) = O(，d + log(m∕δ)) ensures that the above probability
does not exceed δ as desired.	□
B Proofs for Section 4
B.1 Proof of Lemma 1
Computing the gradient of LQ, we obtain
2n	1
▽LQ(W) = — V' (yi,fW (Xi)) —^χiχ>WDi.
n	2m
i=1
Further computing the Hessian gives
2n	1
V2Lq(W)[W*Σ', W*Σ0] = n E∕(yi,fW(Xi)) ∙ 2√m〈Xix>, W?Σ0Di∑0W>>
i= i=1	、----------------------}
'∙^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^
fWQ ? (xi)
+ 4 XX'0(yi,fW(Xi)) ∙ ( 2√1m〈XiX>, WDiWl〃〉)
~^^^^^^^{^^^^^^^^^^^^^^^}
:=yei
2n	4n
n ∑',(yi,fW (Xi))fW ? (Xi) + n f'"(yi,fW (Xi))演
=1	i=1
'---------V----------} '-------V--------}
I	II
Taking expectation over Σ0 , and using that `00	≤ 1, term II can be bounded as
EΣ0 [II] ≤ EΣ0
2
EΣ0,D — σ (W>rX)2(w>X)2(£rrw>rX)2
m
r≤m
≤ C ∙ ED -1 X (w>rX)2(w>X)2(w>rX)2
m
r≤m
≤ CBx rem]*n](W>rXi^ ^ X ^2 ^?"
r≤m
≤ OedBx4kWk22,4 kW?k22,4m-1,
where the last step used Cauchy-Schwarz on {kWrk2} and kW?,r k2 .
16
Published as a conference paper at ICLR 2020
Term I does not involve Σ0 and can be deterministically bounded as
I = 2ED ['0(y, fW (x))fW? (X)]
=2ED['0(y,fW(X)) fW(x)] + 2ED['0(y,fW(x))(fW?(x) - fW(x))]
(i)
≤〈VLQ(W), W + 2ED['(y,fW*(x)) - '(y,fW(x))]
(=) (VLq(W), W - 2(LQ(W) - OPT).
where (i) follows directly by computing〈VLQ(W), W〉and the convexity of z → '(y,z), and (ii)
follows from the assumption that LQ (W?) ≤ OPT. Combining the bounds for terms I and II gives
the desired result.	□
B.2 Coupling lemmas
Lemma 10 (Bound on fQ). For any W ∈ Rd×m, the quadratic model fWQ satisfies the bound
IfW(x)∣ ≤ O(√dBX kW∣∣2J
for all x ∈ Sd-1 (Bx).
Proof. We have
IfW (X)I= √m X ar/(W>rX)(W>x)2
∣	r≤m	∣
≤ √m X C|w>rx| ∙ (w>x)2 ≤ C√mB2 max Iw>rx| ∙ -m X kwrk2
r≤m	r≤m
≤ C √mBXO(√d) ∙ (- X kwr∣t]	= O(√dB2Rw,0 kWk2j
r≤m
□
Lemma 11 (Coupling between f and fQ). We have for all x ∈ Sd-1 (Bx) that
(a)	EΣ[fWL Σ(x)] = 0 and EΣ[(fWL Σ(x))2] ≤ Oe(d2Bx2 kWk2,4 m-1/2).
(b)	∣∆W∑(x)∣ ≤ O(BxkWk3 4 m-1/4) (almost Surelyforall Σ.)
Proof. (a) Recall that
fW∑(x) = √m X arσ0(w>rx)(∑rrw>x).
r≤m
As Σrr has mean zero, we have EΣ [fL] = 0 and
E∑[(fL)2] = J X arσ0(w>rx)2(w>x)2
r≤m
≤ ɪ X C(W>rx)4(W>x)2 (≤, C maχ (W>rx)4 ∙ 1 X Bx kwrk2
m r	r∈[m]	m r
1/2
J X kw"∣4)=O(d2Bx IIWII2,4 m-1/2).
r≤m
Above, (i) follows from the assumption that ∣σ0(t)∣ ≤ Ct2, (ii) is Cauchy-Schwarz, (iii)
uses the bound (4), and (iv) uses the power mean inequality on kwr k2.
(≤ O(d2Bx) ∙ m X kwrk2(2 CBxRw,0 ∙
r≤m
17
Published as a conference paper at ICLR 2020
(b) We have by the Lipschitzness of σ00 that
∆W (x)| = I √m E ar (σ((wo,r +∑rr Wr )>x) - G(W>/X)
r≤m
- σ0(w0>,rx)(Σrrwr>x) - σ00(w0>,rx)(Σrrwr>x)2III
≤	√m X CWrrw>xl3 ≤ C√mBX ∙ m X kwrk2
r≤m	r≤m
≤	C√mB3 ∙ (m X kwrk4)
r≤m
=O Bx3 kWk23,4 m-1/4 ,
where again (i) uses the power mean inequality on kwr k2 .
□
B.3 Closeness of landscapes
Lemma 12 (LQ close to L). We have for all W ∈ Rd×m that
|L(W) - LQ(W)| ≤ OeBx3 kWk32,4 m-1/4 + d2Bx2 kWk22,4 m-1/2.
Proof. Recall that
L(W) = E∑,d ['(y,fwo+w∑(x))] and LQ(W) = ED['(y,fW(x))].
By the I-LiPschitzness of z → '(y, Z) We have
IIL(W) - LQ(W)II ≤ EΣ,D hIIIfW0+WΣ(x) - fWQ (x)IIIi
≤ EΣ,D h(fWL Σ(x) + ∆QWΣ(x))2i
≤ (2E∑,d [(fW∑(x))2] +2E∑,d h(∆W∑(x))2i)1/2
= OeBx3 kWk23,4 m-1/4 +d2Bx2 kWk22,4 m-1/2,
where the last step uses Lemma 11.	□
Lemma 13 (Closeness of directional gradients). We have
IhVL(W), Wi -〈VLQ(W), W)∣
≤ 0((dBχ IWk2,4 + √dBX IWk2,4 + Bx kWk2jm-" + 淤陇 ∣Wk2,4 m-1/2).
Proof. Differentiating L and LQ and taking the inner product with W, we get
hVL(W), Wi = EΣ,D
'03,fW0+WΣ(X)) ∙
ar σ 0 ((w0,r + Σrrwr)>x)(Σrrwr>x)
r≤m
and
(VLQ(W), W) = ED '0(y,fW(x)) ∙ √m X arσ00(w>rx) ∙ (w>x)2
r≤m
18
Published as a conference paper at ICLR 2020
Therefore, by expanding σ0 ((w0,r + Σrrwr)>x) and noticing that Σr2r ≡ 1, we have
KVL(W)-VLQ(W), W〉| = E∑,d '0(y,fwo+w∑(x))
1m X arσ0(w>rx)(∑rrW>x)
r≤m
-- ■
^{z
I
+ e∑,d ('03,fwo+w∑(X))- '0(y, fW(X)))
S---------------------------7—
II
E arσ"(w>rχ) ∙ (w>χ)2
r≤m
+ e∑,d '0(y,fWo+w∑(X))
S
1m X ar (σ0((W0,r + ∑rr Wr )>x) - σ0(w>,x) - σ00(w>r x)(∑rr W>x))(Σ为 W>x) I.
r≤m
一___ 〃
"{z
III
We now bound the three terms separately. Recall that ∣'0∣ ≤ 1 and '0(y, Z) is I-LiPschitz in z. For
term I we have by Cauchy-Schwarz that
|I| ≤ 卜-m X ɑ2σ (w>rx)2(w>x)jj
≤ (C max. ,(w>rxi)4 ∙ — X kwrk2 B2 )
r∈[m],i∈[n]	m
r≤m
≤ O(dBx) ∙ (m XkWrk4)	= O(dBx kWk2,4 m-1/4).
For term II, we have
|II| ≤) (e∑,d [(fwo+w∑(x) - fW(x))2i)1/2 ∙ (ED h(fW(x))2i)1/2
(ii)
≤ O (Bx kWk3,4 m-1/4 + d2BX kWk2,4 m-1/2) ∙ O(√dBX IWk2,4)
=θ(√dB5 IWk2,4 m-1/4 + d2∙5BX IWk2,4 m-1/2).
where (i) uses Cauchy-Schwarz and (ii) uses the bounds in Lemma 10 and 11. For term III we first
note by the smoothness of σ0 that
Iar (σθ((W0,r + ∑rrWr)>x) - σ0(w>,x)-二”(亚>户)(£”W>x))(∑rrW>x) ∣
≤ CIIΣrrwr>xII3 ≤ CBx3 kwrk32 .
Substituting this bound into term III yields
|III| ≤√m X CBx kWrk3 ≤ C√mB3 ∙ m X kWrk3
r≤m	r≤m
≤ C√mB3 ∙ (- X kwrk2)	= O(Bx kWkx,4 m-1/4).
r≤m
Putting together the bounds for term I, II, III gives the desired result.	□
19
Published as a conference paper at ICLR 2020
Lemma 14 (Closeness of Hessians). Let Σ0 denote a diagonal matrix with diagonal entries drawn
i.i.d. from Unif {±1}. We have for all W, W? ∈ Rd×m that
∣E∑0 [(V2L(W) - V2Lq(W)) [W*Σ0, W*Σ0]] I
≤ Oe
IIWIl2,4 + √dB5 IIWII3,4) ∣∣w*∣∣2,4 m-1∕4
+ d2.5Bx4 kWk22,4 kW?k22,4 + Bx2(d2 + kWk24,∞Bx4) kW?k22,4 m-1/2
+ dBx4 kWk22,4 kW?k22,4m-1
Proof. Differentiating L and LQ twice on the direction W*Σ0, We get
V2L(W)[W*Σo, W*Σ0]
and
EΣ,Σ0,D
'00(y, fwo+w∑(X)) ∙
+ EΣ,D '0(y,fWo+WΣ(X)) ∙
r≤m
{^^
II(L)
V2Lq(W)[W*Σ0, W*Σ0] = E∑0,d '00(y,fW(x)) ∙
1m X arσ00(w>rX)(W>x)(*rrw>rx))
r≤m
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^≡
I(LQ)
+ EΣ0,D '0(y,fW(X)) ∙ —p
r≤m
{Z
II(LQ)
We first bound the terms I(L) and I(LQ). We have
I(L) = 2EΣ,Σ0,D
-1 X a»((wo,r + ∑rrWr)>x)2(w7,rx)2
r≤m
≤ C ∙ SUp — X ((W0,r + ∑rrWr)>x)4(wjrx)2
kxk2=Bx mr≤m	,
≤ CBx ∙ :m X (O(d2)+ kWrk4 BX) kw?,r k2
r≤m
≤ OeBx2d2+kWk42,∞Bx4 kW?k22,4m-1/2.
Using similar arguments on I(LQ) gives the bound
I(LQ) ≤ OedBx4 kWk22,4 kW?k22,4 m-1.
We now shift attention to bounding II(L) - II(LQ). First note that
b；" (X)I = ∣ar (σ00 ((W0,r + £rr Wr )>χ) - σ00(w>r x))(w>r x)2∣
≤ CMrrw>x| ∙ (w>rx)2 ≤ CBx kwrk2 kw*,rk2 .
(12)
|
|
|
}
}
}
|
}
20
Published as a conference paper at ICLR 2020
Then we have, by applying the bounds in Lemma 10 and 11,
II(L) - II(LQ)
=e∑,d '0(y,fwo+w∑(χ)) ∙ √m X ∆(x) + e∑,dh('0(y,fWo+w∑(χ)) -'0(y,fW⑼)∙2fW*(χ)]
r≤m
≤ C ∙√m X Bx kwrk2 kw*,rk2 + C yE[(fW ∑(x) + ∆W∑(x))2 ∙ (EhfW *(x)2D1/2
≤ O(Bx kWk2,4 kW*k2,4 m-1/4) + O(Bx ∣Wk2,4 m-1/4 + d2Bx ∣Wk2,4 m-1/2) ∙ C)(√dBx kW?^).
=O((Bx IWk2,4 + √dBx IW情,4) kW*k2,4 m-1/4 + d2.5B4 kWk2,4 kW?k2,4 m-1/2) ∙
Combining all the bounds gives the desired result.	□
B.4 Proof of Theorem 2
We apply Lemma 12, 13, and 14 to connect the neural net loss L to the “clean risk” LQ . First, by
Lemma 12, we have for all the assumed W that
L(W) - LQ(W) ≤Oe Bxx kWkx2,4 m-1/4 + d2Bx2 kWk22,4 m-1/2 .
Therefore we have L(W) - LQ (W) ≤ /6 so long as
m ≥ O(Bx2BW2e-4 + d4BxBWe-2).	(13)
Applying Lemma 1, we obtain that
E∑0 [V2Lq(W)[W*Σ0, W*Σ0]] -〈VLq(W), W〉
≤ 2(LQ(W) - OPT) + /3 ≤ 2(L(W) - OPT) + 2/3	()
provided that the error term in Lemma 1 is bounded by /3, which happens when
m ≥ O(dBxBWBw,*e-1).	(15)
Finally, we choose m sufficiently large so that
∣E∑0 [(V2L(W) - V2Lq(W))[W*Σ0, W*∑o]] I ≤ e/6
and
KVL(W) -VLq(W), W〉| ≤ e/6,
which combined with (14) yields the desired result. By Lemma 13 and 14, it suffices to choose m
such that, to satisfy the closeness of directional gradients,
m ≥ O((d4BxBw + d2Bx°Bw° + Bx2Bw2)e-4 + d5BxBwe-2),	(16)
and to satisfy the closeness of Hessian quadratic forms,
m ≥ O[Bx12Bw4 Bw8,? + d2Bx20Bw12Bw8,?e-4
(17)
+ [d5Bx8Bw4Bw4,?+d4Bx4Bw4,?+Bx12Bw8Bw4,?e-2+dBx4Bw2Bw2,?e-1 .
Collecting the requirements on m in (13), (15), (16), (17) and merging terms using e ≤ 1 and
Bw,? ≤ Bw , the desired result holds whenever
m ≥ θ([Bx2Bw2 + d4B4Bw + d2Bx0Bw0]e-4 + d5BxBwe-2).
This completes the proof.
□
21
Published as a conference paper at ICLR 2020
B.5 Proof of Corollary 3
Proof. For all λ ≥ 0 define
Aλ ：= E∑o [V2Lλ(W)[W*∑0, W*Σ0]] - "Lι(W), Wi + 2(L.(W) - OPT),
By Lemma 2, it suffices to show that
Aλ - A0 ≤ Cλ kW?k28,4 - λ kWk82,4
for some absolute constant C.
Recall that Lλ(W) = L(W) + λ kW?k28,4. By differentiating A 7→ kAk28,4 we get
hV(Lλ - L)(W), Wi = 2 kWk4,4 ∙ X (4λ kwrk2 Wr, wj = 8λ kWk2,4
r≤m
and
V2 (Lλ - L)(W)[W*Σ0, W*Σ0]
=8λ IIWk4,4 X kwr k2 kw*,r Erk2 +2 hwr , W?,r 工1户 + 32λ IIWk4,4 ∙ ( X kwrk2 hwr, W?,r Eri)
r≤m	r≤m
≤ 56λ kWk2,4 X kWrk2 kw*,rk2 (≤) 56λ kW% kW*k2,4 7 14λα kWk∣,4 + 378λ kW?晦,4 ,
r≤m	α
where ⑴ Used CaUchy-SchWarz and (ii) used the AM-GM inequality p3q ≤ αp4∕4 + 27q4∕(4α3)
for all p, q and α > 0. Substituting the above expressions into Aλ - A0 yields
Aλ - A0
378λ
≤ 14λα kWk2,4 + K kW?k8,4 - 8λ kWk8,4 + 2λ kW吼
378λ
= (14λɑ - 6λ) kWk2,4 + κ kW?k2,4 .
Choosing α = 5/14 gives the desired result.
□
B.6 Proof of Theorem 4
We begin by choosing the regularization strength as
λ = λ0Bw-,8?,
where λ0 is a constant to be determined. Let be an accuracy parameter also to be determined.
Localizing second-order stationary points We first argue that any second order stationary point
W has to satisfy kWk2,4 ≤ Bw,0 for some large but controlled Bw,0. We first note that for the
clean risk LQ, we have for any W ∈ Rd×m that
(VLq(W), W〉= ED h'0(y,fW(x)) ∙ 2fW(x)]
(i)	(ii)
= 2Ed 卜(y,fW(x)) ∙ (fW(x) - fQ(x))] ≥ 2(Lq(W) - Lq(0)) ≥ -2,
where (i) Uses convexity of ' and (ii) uses the assumption that '(y, 0) ≤ 1 for all y ∈ Y.
Now, applying the coupling Lemma 13, and combining with the fact that VW(λ kWk28,4), W =
8λ kWk82,4, we have simultaneously for all W that
hVLλ(W), Wi
≥ DVW(λ kWk82,4), WE + VLQ(W), W〉 - V(L - LQ)(W), W〉
≥ 8λ kWk∣,4 - 2 - 0((dBχ kWk2,4 + √dBX kWk5,4 + Bx kWk2Jm-" + d2.5B4 kWk4,4 m-1/2).
22
Published as a conference paper at ICLR 2020
Therefore we see that any stationary point W has to satisfy
kWk2,4 ≤ Bw,0
:=δfλ-1/8 + (λ-1dBχm-1/4)1/7 + (λ-1√dB5 m-1/4)1/3 + (λ-1B3 )1/5 + (λ-1 d2∙5B4 m-1/2
By Corollary 3, choosing m ≥ poly(λ0-1, d, Bw,?Bx, ), the coupling error is bounded by in
B2,4(Bw,0), i.e. for all W ∈ B2,4(Bw,0) we have that
E∑o[V2Lλ(W)[W*∑0, W*∑0]]
≤ hVLλ(W), Wi- 2(Lλ(W) - OPT) — λ ∣W∣∣2,4 + Cλ ∣∣W*% + e,
(18)
where C = O(1) is an absolute constant.
Bounding loss and norm Choosing
λo = 1 ∙ (2YOPT + e),
C
we get that CλBw8 ,? = 2γOPT + e, and thus the bound (18) reads
E∑o[V2Lλ(W)[W*∑0, W*∑0]]
≤ hVLλ(W), Wi - 2(Lλ(W) - OPT) - λ kWk28,4 + 2γOPT + 2e.
For the second-order stationary point Wc , the gradient term vanishes and the Hessian term is non-
negative, so we get
2(Lλ(Wc) - OPT) ≤ 2(γOPT + e) - λ Wc 8 ≤ 2(γOPT + e)
and thus
Lλ(Wc) ≤ (1+γ)OPT+e.
Further, by re-writing (18), we obtain
λ IlWil8 4 ≤ CλBW,? + 2(OPT - Lλ(W)) + e ≤ CλB*,? + 2OPT + e
≤ CλBw,? ∙ (ι+IOPT⅛)=。⑴∙ λBw,?,
for any γ = O(1). This is the desired result.
□
23
Published as a conference paper at ICLR 2020
C Proofs for Section 5
C.1 Proof of Lemma 5
As the loss '(y, Z) is I-LiPschitz in Z for all y, by the Rademacher contraction theorem (Wainwright,
2019, Chapter 5) we have that
1n	Q
SUp — E σi'(y ,fW(xi))
kWk2,4≤Bw n i=1
n
n
sup
kWk2,4≤Bw
1n	1n
n XσifW(Xi) +Eσ,x n Xσi'(yi, 0)
i=1
i=1
max
r∈[m]
sup max
kWk2,4≤Bw r∈[m]
1n
1X。上
n i=1
sup	X 1X X σi°rσ00(w>rXi)XiX>, WrW>	+ l7
∣W∣∣2,4≤Bw √m ⅛m ∖n i=ι	,	∕j ≠
1
where the last step used the power mean (or CaUchy-SchWarz) inequality on {k Wr k 2}.	□
C.2 Proof of Theorem 6
We first relate the generalization of L to that of LQ through
LP(Wc) - L(Wc) ≤ LP(Wc) - LPQ(Wc) +LPQ(Wc) - LQ(Wc) +LQ(Wc) - L(Wc).
By Lemma 12, we have simultaneously for all W ∈ B2,4(Bw) that
L(W) -LQ(W) ≤ OeBx3Bw3m-1/4+d1 2Bx2Bw2m-1/2.	(19)
Further, from the proof we see that the argument does not depend on the distribution of X (it holds
uniformly for all X ∈ Sd-1(Bx), therefore for the population version we also have the bound
LP (W) - LPQ(W) ≤ OeBx3Bw3 m-1/4 + d2Bx2Bw2 m-1/2.	(20)
These bounds hold for all W ∈ B2,4(Bw) so apply to Wc. Therefore it remains to bound LPQ (Wc) -
LQ (Wc), i.e. the generalization of the quadratic model.
Generalization of quadratic model By symmetrization and applying Lemma 5, we have
EW0,D LPQ(Wc) -LQ(Wc)
≤ EW0,D	sup	LPQ(W) -LQ(W)
kWk2,4≤Bw
1n	Q
≤ 2EWo,σ,x	SUP	— σ ʌ σi'(yi, fw (Xi))
kWk2,4≤Bw n i=1
(21)
≤ 4Bw2 EW0 ,σ,x max
r∈[m]
24
Published as a conference paper at ICLR 2020
We now focus on bounding the expected max operator norm above. First, we apply the matrix
concentration Lemma 8 to deduce that
EW0,σ,x max
r∈[m]
1n
∣Eσiσ00(w>rχi )χiχ>
i=1
op
≤ 4√log(2dm) • Ewo,x
max
r∈[m]
≤ 4Bχ /ɪogfmɪ (Ewο,x
1n
n Eσ00(w>rXi)2 kχik2 χiχ>
i=1
max σ00
r,i
1 n
(W>rXi)2 ^	n X
max σ00(W0>rχi)2
r,i	,
op
i=1
XiXi>
op
Exl nχ χiχ>
1/2
op
As ∣σ00(t)∣ ≤ Ct and W>rXi 〜N(0,1) for all (r, iɪ by standard expected max bound on SUb-
exponential variables we have
EW0,x max σ00(W0>,rχi)2 ≤ O(log(mn)) = Oe(1).
Therefore defining
Mx,op := Bx-2
1n
• Exn X
i=1
χ∙χ>1!1/2
χiχi
and substituting the above bound into (21) yields that
Ewo,d [lQ(W) - LQ(Wɪi ≤ O(BB√x,op +
Combining the bound with the coupling error (19) and (20), we arrive at the desired result.
For Mx,op we have two versions of bounds:
(a)	We always have ∣∣Pi≤n χiχ>/n∣∣	≤ B2 and thus Mχ,°p ≤ 1.
(b)	If, in addition, χ is uniformly distributed on the sphere Sd-1(Bx) or the hypercube
{±Bx/√d} , then We have by standard covariance concentration (Vershynin, 2018, The-
∖
≤ 4Bx JlogIdm) • Ewo,x
∖
n
n
orem 4.7.1) that Ex ∣∣Pi≤n χiχ>/n∣^
≤ B2/d • O(1 + Pd/n + d/n) = O(B2∕d)
when n ≥ d. More generally, if for all v ∈ Sd-1 (1) we have
∣∣v>χ∣∣ψ2 ≤ K Jv>cov(χ)v,
and that κ(Cov(χ)) ≤ κ, then we have IlCoV(X))%p ≤ KBix/d. Applying (Vershynin,
2018, Theorem 4.7.1), we get Mx,op ≤ κ/√d whenever n ≥ O(K4d).
□
C.3 Expressive power of infinitely wide quadratic models
Lemma 15 (Expressivity of f Q with infinitely many neurons). Suppose f?(x) = α(β>χ)p for
some a ∈ R, β ∈ Rd, andP ≥ 2 and such that P 一 2 ∈ {1}∪ {2'}'≥0. SuPPosefUrther that we use
σ(t) = 6relu3(t) (so that σ00(t) = relu(t)), then there exists choices of (w+, w-) that depends on
W0 such that
Ewo [σ00(w>X)((W>χ)2 — (W[χ)2)] = f?(X)
and further satisfies the norm bound
Ew0hIw+I42+Iw-I42i ≤ 2π((P - 2) ∨ 1)3α2Bx2(p-2) IβI22p.
25
Published as a conference paper at ICLR 2020
Proof. Our proof builds on reducing the problem from representing (β>x)p via quadratic networks
to representing (β>x)p-2 through a random feature model. More precisely, we consider choosing
(w+, w-) = (p[a]+ ∙ β, p[a]- ∙ β),	(22)
where a is a real-valued random scalar that can depend on w0, and β is the fixed coefficient vector
in f? . With this choice, the quadratic network reduces to
Ewo [σ00(W>X)((W>X)2 - (w>x)2)]
=Ewo [σ00(w>x)(α+ (β>x)2 - α-(β>x)2)] = (β>x)2Ew0 [σ00(w>x)α].
Therefore, to let the above express f?(X) = α(β>X)p, it suffices to choose a such that
E[σ00(w0>x)a] ≡ α(β>x)p-2	(23)
for all x. By Lemma 9, there exists a = a(w0) satisfying (23) and such that
Ewo[a2] ≤ 2π((p - 2) ∨ 1)3α2Bx2(p-2) kβk22(p-2).
Using this a in (22), the quadratic network induced by (w+, w-) has the desired expressivity, and
further satisfies the expected 4th power norm bound
Ewo[kw+k24+ kw-k42]
=Ewo [[a]+ + [a]-] ∙ kβk2 = Ewo [a2] kβk4 ≤ 2∏((p - 2) ∨ 1)3α2B2(P-2) kβk2p .
This is the desired result.
□
C.4 Proof of Theorem 7
We begin by stating and proving the result for k = 1 in Appendix C.4.1, i.e. when f? = α(β>x)p is
a single “one-directional” polynomial. The main theorem then follows as a straightforward extension
of the k = 1 case, which we prove in Appendix C.4.2.
C.4. 1 Expressing a single “one-directional” polynomial
Theorem 16 (Expressivity of fQ). Suppose {(ar, w0,r)} are generated according to the symmetric
initialization (3), and f?(x) = α(β> x)p where P — 2 ∈ {1}∪ {2'}'≥0. Suppose further that we
use σ(t) = 1 relu3(t) (So that σ00(t) = relu(t)), then so long as the width is sufficiently large:
m ≥ O(ndp3α2(Bχ ∣∣β∣∣2)2pe-2),
we have with probability at least 1 - δ (over W0) that there exists W? ∈ Rd×m such that
LQ(W?)-L(f?) ≤e and kW?k42,4 ≤ Bw4,? = Op3α2Bx2(p-2) kβk22p δ-1.
Proof of Theorem 16 We build on the infinite-neuron construction in Lemma 15. Given the
symmetric initialization {w0,r}rm=1, for all r ∈ [m/2], we consider W? ∈ Rd×m defined through
(w?,r, w?,r+m/2) = 2m-1/4w+(w0,r), 2m-1/4w- (w0,r) ,
where We recall (w+(wo), w-(wo)) = (,a+(wo)β, ,a-(wo)β). We then have
fW*(x)=2√m X σ00(w>rx)h(w>rx)2 - (w>r+m∕2x)2i
r≤m∕2
2
=m £ σ (w>rx)[(w+(w0,r )>x)2 - (W-(W0,r)>x)2]
r≤m∕2
=m/2 X σ00(w>rx)a(w0,r) ∙ (β>x)2.
r≤m∕2
26
Published as a conference paper at ICLR 2020
Bound on kW?k2,4 As f?(x) = α(β>x)p, Lemma 15 guarantees that the coefficient a(w0)
involved above satisfies that
R2a := Ew0 [a(w0)2] ≤ 2π((p - 2) ∨ 1)3α2Bx2(p-2) kβk22(p-2).
By Markov inequality, We have with probability at least 1 - δ∕2 that
m/2 X a(w0,r)2 ≤ 4∏((p - 2) ∨ 1)3α2B2(P-2) kβk2(p-2) δ-1,
r≤m
which yields the bound
kWk42,4 = X kw?,rk42
r≤m
≤ kβk2 ∙ X 16mτa(w0,r)2=8 kβk2 ∙ m/2 X a(W0,r)2
r≤m∕2	r≤m∕2
≤ 32π[(p - 2)3 ∨ 1]α2Bx2(p-2) kβk22p δ-1.
Concentration of function Let fm(x) = . Pr<m∕2 σ00(w>rx)a(wo,r). We now show the Con-
centration of fm to f?,p-2(x) := α(β>x)p-2 over the dataset {x1, . . . , xn}. We perform a trunca-
tion argument: let R be a large radius (to be chosen) satisfying
Pwo ( SUp ∣∣W0,r k2 ≥ RB-I) ≥ 1 - δ∕2.	(24)
r∈[m]
On this event we have
fm(X)= : X σ00(w>rX)a(w0,r )1 {kw0,rk2 ≤ RB-1} := fR (X) ∙
r≤m
Letting f?R,p-2(X) := Ew0 [σ00(w0>X)a(w0)1 {∣w0∣2 ≤ RBx-1}], we have
Ewo[(fm(x)-fRp-2(x))2i = m Ewo [σ00 (w>x)a2(w0)l {∣w012 ≤ R}] ≤ CRRa.
Applying Chebyshev inequality and a union bound, we get
P(max ∣fm(xi) - f?,p-2(Xi)| ≥ t) ≤ CnR Ra ∙
i	mt2
For any > 0, by substituting in t = Bx-2 ∣β∣2-2 ∕2, we see that
m ≥ OnR2Ra2Bx4	∣β∣24	-2	= OnR2(p -	2)3α2Bx2p	∣β∣22p	-2	(25)
ensures that
max|fm(xi) - f?R,p-2(xi)| ≤ Bx-2 ∣β∣-2∕2.	(26)
Next, for any x we have the bound
f?R,p-2(x) - f?,p-2(x) = Ew0 [σ00(w0>x)a(w0)1 {∣w0∣2 > R}]
≤ E[a(w0)2]1/2 ∙ E[σ00(w>x)4]1/4 ∙ P(kw0k2 > R)1/4
≤ Ra ∙ C∕√d ∙ P(kw0k2 > R)1/4.
Choosing R such that
P(∣w0 ∣2 > R) ≤
√de4
c—
RaBX kβk8
ensures that
maxfRp-2 (Xi) - f?,p-2 (Xi)I ≤ eBx 2"k2 .
(27)
(28)
27
Published as a conference paper at ICLR 2020
Combining (35) and (37), we see that with probability at least 1 - δ,
max fWW*(xi) - f*(xi)∣ = max Ifm(Xi) - f?,p-2(Xi)| ∙ (β>Xi)2
i∈[n]	?	i∈[n]
≤ 2 ∙ eB-22βk2 2 ∙ Bx kβk2 = e
and thus
LQ (W?) -L(f?) ≤.	(29)
To satisfy the requirements for m and R in (36) and (34), We first set R = O(√d) (with sufficiently
large log factor) to satisfy (36) by standard Gaussian norm concentration (cf. Appendix A.3), and
by (34) it suffices to set m as
m ≥ Oknd(P - 2)3α2(Bx ∣∣β∣∣2)2pe-2).
for (38) to hold.	□
C.4.2 Proof of main theorem
We apply Theorem 16 k times: let
f?,j (x) := αj(βj>x)pj ,
so that f? = Pj≤kf?,j. Associate each j with an independent set of initialization (a(0j), W0(j)). By
Theorem 16, there exists W(?j) ∈ Rd×mj , where
mj = Okndk2pj02(Bx ∣∣βj∣∣2)2pje-2)
such that with probability at least 1 - δ∕k We have
maχfW*j) (Xi)- f?(Xi) I ≤ e/k
and the norm bound
W?(j)	≤ Okpj3αj2Bx2(pj-2) ∣βj∣22pj δ-1.
(Note we have slightly abused notation, so that now fQ (j)	use a disjoint set of initial weights
(a(0j), W(0j)).) Concatenating all the (W?(j), a(0j), W0(j)) and applying a union bound, we have the
following: so long as the width
k	/ k	∖
m ≥ Xmj = Oe ndk2 Xpj3αj2(Bx ∣βj ∣2)2pj e-2 ,
j=1	j=1
with probability at least 1 - δ (over a0 ∈ Rm and W0 ∈ Rd×m), there exists W? ∈ Rd×m such
that	I	I
max IIfWQ * (Xi) -f?(Xi)II ≤ e,
which by the 1-Lipschitzness of the loss implies that
LQ(W?) ≤ Lf?)+ e = eo + e.
Further, as W? is the concatenation of W?(j)	, we have the norm bound
kk
∣w*∣4,4 = XI W?IL = oj X Pjw) kβj∣2pj δ-1j.
This is the desired result.
□
28
Published as a conference paper at ICLR 2020
D Existence, generalization, and expressivity of higher-order
NTKS
In this section we formally study the generalization and expressivity of higher-order NTKs outlined
in Section 6. Let k ≥ 2 be an integer, and recall for any W ∈ Rd×2m the definition of the k-th order
NTK
fW0,W(X) = √1m X σ	(W0,rx) [(W>,rX)k -(W>,rX)k] ,	(3O)
r≤m
D. 1 COUPLING f AND f(k) VIA RANDOMZIATION
Recall that for analytic σ we have the expansion
1	∞ (k)
fWo+W (X) = √m	σ((W0,r + w+,r )>X) - σ((w0,r + w-,r )>X) =	fW0,W (X),
r≤m	k=0
For an arbitrary W such that kW+,r k2 , kW-,r k2 = om(1), we expect that f(1) (X) is the dominating
term in the expansion.
Extracting the k-th order term We now describe an approach to finding W so that
fW0+W (X) = fW0,W (X) + om (1),
that is, the neural net is approximately the k-th order NTK plus an error term that goes to zero as
m → ∞, thereby “escaping” the NTK regime. Our approach builds on the following randomization
technique: let z+ , z- be two random variables (distributions) such that
E[z+j ] = E[z-j ] for j = 0, 1, . . . , k - 1 and E[z+k] = E[z-k] + 1.
Set (W+,r, W-,r) = (z+,rW?,r, z-,rW?,r), and take kW?,rk2 = O(m-1/2k), we have
)0,w (X) = F
X j11 σ(j) (W>rX)(z+,r - z-,r ) (w>,r X)j = OplmT/2 )
r≤m
、 一_ _ 一
{z
mean zero
}|
'	{z
O(m-j/2k)
}
for all = 1, . . . , k - 1, and
fW0,W(X) = √1m X k! σ(k)(w>r x) (z+,r - z-,r) (w>,r X)k = OP ⑴，
V	r≤m '	`-----{-----'、-{---}
mean=1	O(m-1/2 )
and
f(k+1) (χ) = 1 X  1 σ(k+1)(w> x) (zk+1 - zk+1) (w> χ)k+1 = Op(m-1∕2k)
fWoW(X)= √m	(k +1)!σ	(W0,rX)(Z+,r z-,r ) (W?,rX)	= OP(m	).
V	r≤m	S------{------} X---y-------}
O(m-(k+1)/2k)
Therefore, with high probability, all f(1), . . . , f (k-1) as well as the remainder term f - Pj≤kf(j)
has order O(m-1/2k), and the k-th order NTK f(k) can express an O(1) function.
D.2 GENERALIZATION AND EXPRESSIVITY OF f(k)
We now turn to studying the generalization and expressivity of the k-th order NTK f(k), Throughout
this subsection, we assume (for convenience) that
σ2(t) := ɪσ(k) (t) ≡ relu(t)
k!
is the ReLU activation.
As We have seen in Section 6, We have f (k) = O(1) by choosing Wr 〜 O(m-1∕2k), therefore We
restrict attention on such W’s by considering the constraint set {W : kWk22,k2k ≤ Bw2k} for some
Bw = Om(1).
29
Published as a conference paper at ICLR 2020
Overview of results This subsection establishes the following results for the k-th order NTK.
• We bound the generalization of f (k) through the tensor operator norm of a certain k-tensor in-
volving the features (Lemma 17). Consequently, the generalization of the k-th order NTK for
kWk2,2k ≤ Bw, when the base distribution of x is uniform on the sphere, scales as
O
1
√ndk-1
1
+ —
n
+
(Theorem 19). Compared with the distribution-free bound BkBw/√n, the leading term is better by
a factor of，min {dk-1,n}. In particular, when n ≥ dk-1, the generalization is better by a factor
of √dk-1 than the distribution-free bound.
• For the polynomial f?(x) = α(β>x)p with p ≥ k (and p - k is even or one), when m is
sufficiently large, there exists a W? expressing f? such that
kW?k22,k2k ≤ Op3α2Bx2(p-k) kβk22p.
(Theorem 20). Substituting into the generalization bound yields the following generalization error
for learning f? :
θ(p3α2(Bχ kβ∣∣2)p[√^ + 1]).
2	ndk-1	n
In particular, the leading multiplicative factor is the same for all k (including the linear NTK with
k = 1), but the sample complexity is lower by a factor of dk-1 when n ≥ dk-1. This shows
systematically the benefit of higher-order NTKs when distributional assumptions are present.
IIL= sup/*, B
Tensor operator and nuclear norm Our result requires the definition of operator norm and nu-
clear norm for k-tensors, which we briefly review here. The operator norm ofa symmetric k-tensor
A ∈ Rdk is defined as
l∣Akop := sup〈A, v0k)= sup A[v,..., v].
kvk2=1	kvk2=1
The nuclear norm ∣∙∣ * is defined as the dual norm of the operator norm:
l∣A∣l* :=	sup 〈A, Bi .
kBkop=1
Specifically, for any rank-one tensor u0k, We have
lul2k ,
i.e. its nuclear norm equals its operator norm (and also the Frobenius norm).
D.2. 1 Generalization
We begin by stating a generalization bound for f(k), which depends on the operator norm of a k-th
order tensor feature, generalizing Lemma 5.
Lemma 17 (Bounding generalization of f(k) via tensor operator norm). For any non-negative loss
' SUCh that Z → '(y,z) is I-LiPsChitz and '(y, 0) ≤ 1 for all y ∈ Y, we have the RademaCher
complexity bound
Eσ,x
sup
kWk2,2k≤B
1n
~ X σi'(yi, fW0,W (Xi))
n
i=1
≤ 2Bw Eσ,χ max
1n
1X
n i=1
σiσk (w>rxi)xTk
op
1
+
where σi iid Unif {±1} are Rademacher variables.
30
Published as a conference paper at ICLR 2020
Proof. The proof is analogous to that of Lemma 5. As the loss '(y, z) is I-LiPschitz in Z for all y,
by the Rademacher contraction theorem (Wainwright, 2019, Chapter 5) we have that
sup
kWk2,2k≤Bw
1n
n y^σi'(yi, fW0,W(Xi))
i=1
n
n
sup
kWk2,2k≤B
1n	1n
n ^X σifWo,W (Xi) + Eσ,x n ^X σi'(yi, 0)
w i=1
i=1
i
sup
kWk2,2k≤Bw
1m X * 1 X σiar σk (WjrXi)Xflk, W 产 +j + √1
sup max
kWk2,2k≤Bw r∈[m]
max
r∈[m]
1n
n £a/σiσk(w>rxi)xfk
i=1
1n
nEσiσk(W>rχi)χfk
i=1
op
op
1m Xm
r≤m
sup
kWk2,2k≤Bw
、------------
≤Bwk
17 kwr kk +"1
1
+ 丁
}
where the last step used the power mean (or Cauchy-Schwarz) inequality on {kwrk2}.
□
□
Bound on tensor operator norm It is straightforward to see that the expected tensor operator
norm can be bounded as	〜
O(Bk∕√n)
without any distributional assumptions on X. We now provide a bound on the expected tensor opera-
tor norm appearing in Lemma 17 in the special case of uniform features, i.e. X 〜 Unif(Sd-1(Bx)).
Lemma 18 (Tensor operator norm bound for uniform features). Suppose Xi 吧 Unif(Sd-1(Bx)).
Then for any k ≥ 3 and k = O(1), we have (with high probability over W0)
max
r∈[m]
1n
-Eσiσk (w>rXi)XTk
n i=1	op
≤ Oe Bxk
11
+
√ndk-1 n
(31)
Substituting the above bound into Lemma 17 directly leads to the following generalization bound
forf(k):
Theorem 19 (Generalization for f(k) with uniform features). Suppose Xi 吧 Unif(Sd-1(Bx)).
Then for any k ≥ 3 and k = O(1), we have (with high probability over W0 )
ED	SUP	(LP(fW0,W)- LfWOW)) ≤ O (BxBw √ k 1 + ~— + ~ι=).
JWk2,2k≤Bw ∖	0,	0, j∖ x	L√ndk-1	n」√n√
The proof of Lemma 18 is deferred to Appendix D.3.
D.2.2 Expressivity
Theorem 20 (Expressivity off(k)). Suppose {(ar, W0,r)} are generated according to the symmetric
initialization (3), and f?(X) = α(β>X)p where P 一 k ∈ {1} ∪ {2'}'≥0. Suppose further that σ is
such that σk (t) = relu(t), then so long as the width is sufficiently large:
m ≥ O(ndp3α2(Bx ∣∣β∣∣2)2pe-2),
we have with probability at least 1 一 δ (over W0 ) that there exists W? ∈ Rd×m such that
ILQ(W?) - L(f?)| ≤ e and ∣∣W*k2k2k ≤ Bwk? = O(p3α2Bx(Pi) ∣∣β∣∣2p δ-1).
The proof of Theorem 20 is deferred to Appendix D.4.
31
Published as a conference paper at ICLR 2020
D.3 Proof of Lemma 18
We begin by observing for any symmetric tensor A ∈ Rdk that
kAloP ≤ 1⅛ V* AM …, V],
where N() is an -covering of unit sphere Sd-1(-). (The proof follows by bounding A[u, . . . , u]
by A[v, . . . , v] +k ∣A∣op through replacing u by v one at a time). Taking = -/(2k), we have
Pσ x max
,	r∈[m]
1n
nfσ2k (W>rXi)Xf)k
i=1
≥t
op
≤ Pσ,x	max	- Xσiσk(w>rXi)(v>Xi)k ≥ t/2 ).
,r∈[m],v∈N(1∕(2k)) n M	0,r	J
We now perform a truncation argument to upper bound the above probability. Let M > 0 be a
truncation level to be determined, we have by the Bernstein inequality that
Pσ x max
,	r∈[m]
1n
nEσiσk (W>rχi)χfk
i=1
≥t
op
≤ Pσ,x[	max - Xσiσk(w>rXi)(v>Xi)k 1 {∣σk(w>rXi)∣ ≤ M} ≥ t/2
,r∈[m],v∈N(1∕(2k)) n =	0,r	0,r
+ Px max ∣∣σk(w0>,rXi)∣∣ ≥ M
≤ exp -c min
nt2
nt
。⑴∙ B2kd-k ,MBk
M2
+ d log 6k + log m + exp I ——ð-+ log mn I,
where the。⑴∙ Bx2k d-k comes from computing the variance of
Zi := σiσk(W0>,rχi)(v> χi)k
using that χi are uniform on the sphere (see, e.g. (Ghorbani et al., 2019b, Proof of Lemma 4)); MBxk
is the bound on the variable Zi, and the Oe(1) comes from the fact that kW0,r ∣∣2 ≤ O(√dB-1) with
high probability. Now, choosing
M = (nt/Bxk)1/2 ,
the above bound reads
nt2
exp —c mm ---------------
O(1) ∙ Bxkd
+o(d))+exp (一 nð(-ɔ + O(I)) :=pt.
It remains to bound t∞=0 pt to give an expectation bound on the desired tensor operator norm. This
follows by adding up the following three bounds:
(1)	For the main branch “nt2/Oe(Bx2kd-k )” we have
广 mm" ~ nt2
0	Oe(Bx2kd-k )
+O(d)卜卜 ≤ OiyniE).
一 .	~ 一一	∙	.	.	■	..... -	~	CT z /-cτ 一~71—ττ^τ—............
This follows by integrating the “1” branch for t ≤ O( Bx2kd-(k-1)/n) (which yields
the right hand side) and integrating the other branch otherwise (the integral being upper
bounded by O( Bx2kd-k/n), dominated by the right hand side).
(2)	The branch “(nt/Bxk)1/2” is taken only when
nt 1/2
BkJ	<
nt2
^z:7--------- i.e.
Oe(Bx2kd-k)
t > Oe n-1/3Bxkd2k/3 .
32
Published as a conference paper at ICLR 2020
On the other hand, the inequality (nt/Bxk)1/2 > O(d) happens when
t > O(d2Bk/n),
which is implied by the preceding condition so long as k ≥ 3. Therefore, when this branch
is taken, the O(d) can already be absorbed into the main term, so the contribution of this
branch can be bounded as
∞
exp
Oe(n-1/3Bxkd2k/3)
(3)	We have
dt ≤ O(B
Zmin {eχp (- 2θe(ix +O(I)), 1 1t ≤ O(Bk/n),
using a similar argument as part (1).
Putting together the above three bounds, we obtain
max
r∈[m]
1n
n£c (w>r χi)χfk
i=1
∞
≤	pt dt ≤ Oe
0
op
11
+
.√ndk-1	n.
the desired result.
□
D.4 Proof of Theorem 20
Our proof is analogous to that of Theorem 16, in which we first look at the case of infinitely many
neurons and then use concentration to carry the result onto finitely many neurons.
Expressivity with infinitely many neurons We first consider expressing f?(X) = α(β>X)p with
infinite-neuron version of f(k), that is, we wish to find random variables (W+, W-) such that
Ewo [relu(w>x)((w>x)k - (w>x)k)] = f?(x).
Choosing
(w+, w-) = (([a]+)1/kβ, ([a]-)1/kβ)
for some real-valued random scalar a (that depends on W0), we have
Ewo [relu(w>x)((w+x)k - (w>x)k)] = (β>x)k ∙ Ew° [relu(w>x)a],
therefore the task reduces to finding a = a(W0) such that Ewo relu(W0>x)a = α(β>x)p-k. By
Lemma 9, there exists a = a(W0) satisfying the above and such that
Ew0[a2] ≤ 2π((p - k) ∨ 1)3α2Bx2(p-k) kβk22(p-k)
(32)
Using this a, the k-th order NTK defined by (W+, W-) expresses f? and further satisfies the bound
Ewo [kw+k2k + kw-k2ki = Ewo[a2] ∙ kβk2k ≤ 2∏((p - k) ∨ 1)3α2Bx(Pi) kβk2p .
Finite neurons Given the symmetric initialization {w0,r}rm=1, for all r ∈ [m], we consider W? ∈
Rd×m defined through
(w?,r, w?,r+m) = m-1/2kw+(w0,r),m-1/2kw-(w0,r) ,
where we recall (w+(w0), w-(w0)) = (a+(wo)1/kβ, a—(wo)1/kβ). Wethenhave
fW0 ,W?(X) = √1= X σk (w>rx)[(w>rx)k - (w>r+mx)k ]
m r≤m
=ɪ X σk(w>rx) [(w+(w0,r )>x)k - (W-(W0,r )>x)k]
m
r≤m
m1 X σk (W>rX)a(W0,r )	∙ (β>x)k .
r≤m
33
Published as a conference paper at ICLR 2020
Bound on kW? k2,2k As f?(x) = α(β>x)p, (32) guarantees that the coefficient a(w0) involved
above satisfies that
R2a :=Ew0[a(w0)2] ≤ 2π((p - k) ∨ 1)3α2Bx2(p-k) kβk22(p-k).
By Markov inequality, We have with probability at least 1 - δ∕2 that
m X a(wo,r)2 ≤ 4π((p - k) ∨ 1)3α2BX(Pi) kβk2(p-k) δ-1,
r≤m
which yields the bound
kWk2w,k2k = X kw?,rk22k
r≤2m
≤ kβk2k ∙ X m-1a(wo,r)2 ≤ 4π[(p - k)3 ∨ 1]α2Bx(Pi) |臼陵 δ-1.
Concentration of function Let fm(x)= * Pr≤m σk(w>rx)a(w0,r). We now show the Con-
centration of fm to f?,P-k(x) := α(β>x)P-k over the dataset {x1, . . . , xn}. We perform a trunca-
tion argument: let R be a large radius (to be chosen) satisfying
Pwo( SUp kwo,r k2 ≥ RB-I) ≥ 1 - δ∕2.	(33)
r∈[m]
On this event we have
fm(x) = - X σk(w>rx)a(w0,r)1 {kw0,rk2 ≤ RB-1} := fR(x).
m
r≤m
Lettingf?R,P-k(x) := Ew0[σk(w0>x)a(w0)1 kw0k2 ≤ RBx-1 ], we have
2	1	R2 R2
Ewo [(fm(x)- fRp-k(x)) ] = mEwo [σk(w>x)a2(wo)1 {|即||2 ≤ R}] ≤ C--Ra.
Applying Chebyshev inequality and a union bound, we get
P (max ∣fm(xi) - f?,p-k (xi)∣≥ t) ≤ Cn- Ra .
i	mt2
For any > 0, by substituting in t = Bx-k kβk2-k ∕2, we see that
m ≥ O(nR2R2aBx2k kβk22k -2 = O(nR2(p - k)3α2Bx2p kβk22p -2
ensures that
(34)
max|fm(xi) - f?R,P-2(xi)| ≤ Bx-k kβk-k∕2.
i∈[n]
(35)
Next, for any x we have the bound
f?R,p-k(x) - f?,p-k(x) = Ew0[σk(w0>x)a(w0)1{kw0k2 > R}]
≤ E[a(wo)2]1/2 ∙ E[σk(w>x)4]1/4 ∙ P(∣∣wok2 > R)1/4
≤ Ra ∙ C∕√d ∙ P(kwok2 >R)1∕4.
Choosing R such that
√d 4
P(kwok2 >R) ≤ C---------ιr
RaBxk kβk4k
ensures that
max IfRP-2(xi) - f*,p-2(Xi)I ≤ eBx 2βk2
(36)
(37)
34
Published as a conference paper at ICLR 2020
Combining (35) and (37), we see that with probability at least 1 - δ,
max IfW?(xi) - f?(Xi)I =maχ ∣fm(xi) - f?,p-k(Xi)∣∙ (β>Xi)k
i∈[n]	W?	i∈[n]
≤ 2 ∙ eBTk ∙ Bx kβkk = e
and thus
ILQ(W?) - L(f?)I ≤ .
(38)
To satisfy the requirements for m and R in (36) and (34), We first set R = O(√d) (with sufficiently
large log factor) to satisfy (36) by standard Gaussian norm concentration (cf. Appendix A.3), and
by (34) it suffices to set m as
m ≥ O(nd[(p — k)3 ∨ 1]α2(Bχ IleIl2)2pe-2
for (38) to hold.
35