Published as a conference paper at ICLR 2020
Denoising and Regularization via Exploiting
the Structural Bias of Convolutional Gener-
ATORS
Reinhard Heckel	Mahdi Soltanolkotabi
Dept. of Electrical and Computer Engineering Dept. of Electrical and Computer Engineering
Technical University of Munich	University of Southern California
reinhard.heckel@tum.de	soltanol@usc.edu
Ab stract
Convolutional Neural Networks (CNNs) have emerged as highly successful tools
for image generation, recovery, and restoration. A major contributing factor to
this success is that convolutional networks impose strong prior assumptions about
natural images. A surprising experiment that highlights this architectural bias to-
wards natural images is that one can remove noise and corruptions from a natural
image without using any training data, by simply fitting (via gradient descent) a
randomly initialized, over-parameterized convolutional generator to the corrupted
image. While this over-parameterized network can fit the corrupted image per-
fectly, surprisingly after a few iterations of gradient descent it generates an almost
uncorrupted image. This intriguing phenomena enables state-of-the-art CNN-
based denoising and regularization of other inverse problems. In this paper we
attribute this effect to a particular architectural choice of convolutional networks,
namely convolutions with fixed interpolating filters. We then formally character-
ize the dynamics of fitting a two layer convolutional generator to a noisy signal
and prove that early-stopped gradient descent denoises/regularizes. Our proof re-
lies on showing that convolutional generators fit the structured part of an image
significantly faster than the corrupted portion.
1 Introduction
Convolutional neural networks are extremely popular for image generation. The majority of im-
age generating networks is convolutional, ranging from deep convolutional Generative Adversarial
Networks (DC-GANs) (Radford et al., 2015) to the U-Net (Ronneberger et al., 2015). It is well
known that convolutional neural networks incorporate implicit assumptions about the signals they
generate, such as pixels that are close being related. This makes them particularly well suited for
modeling distributions of images. It is less well known, however, that those prior assumptions build
into the architecture are so strong that convolutional neural networks are useful even without ever
being exposed to training data.
The latter was first shown in the Deep Image Prior (DIP) paper (Ulyanov et al., 2018). Ulyanov
et al. (2018) observed that when ‘training’ a standard convolutional auto-encoder such as the popu-
lar U-net (Ronneberger et al., 2015) on a single noisy image and regularizing by early stopping, the
network denoises the image with excellent performance. This is based on the empirical observation
that un-trained convolutional auto-decoders fit a single natural image faster when optimized with
gradient descent than pure noise. Many elements of an auto-encoder, however are irrelevant for this
effect: A more recent paper (Heckel & Hand, 2019) proposed a much simpler image generating
network, termed the deep decoder. This network can be seen as the relevant part of a convolutional
generator architecture to function as an image prior, and can be obtained from a standard convolu-
tional autoencoder by removing the encoder, the skip connections, and perhaps most notably, the
trainable convolutional filters of spatial extent larger than one. Thus, the deep decoder does not use
learned or trainable convolutional filters like conventional convolutional networks do, and instead
only uses convolutions with fixed convolutional kernels to generate an image.
1
Published as a conference paper at ICLR 2020
0
Figure 1: Fitting an over-parameterized Deep Decoder (DD-O) and the deep image prior (DIP) to
a (a) noisy image, (b) clean image, and (c) pure noise. Here, MSE denotes Mean Square Error of
the network output with respect to the clean image in (a) and fitted images in (b) and (c). While the
network can fit the noise due to over-parameterization, it fits natural images in significantly fewer
iterations than noise. Hence, when fitting a noisy image, the image component is fitted faster than
the noise component which enables denoising via early stopping.
(c) fitting noise
0.03
0.02
0.01
102	104
iteration
original image
noisy image
19.1dB
BM3D
28.6dB
DIP
29.2dB
DD-U
29.6dB
DD-O
29.9dB
Figure 2:	Denoising with BM3D and various convolutional generators. The relative ranking of
algorithms in this picture is representative and maintained on a larger set of test images (see Ap-
pendix A). DD-U is an under-parameterized deep decoder. DIP and DD-O are over-parameterized
convolutional generators and with early stopping outperform the BM3D algorithm, the next best
method that does not require training data.
In this paper, we study such a simple, untrained convolutional network theoretically. We consider
the over-parameterized regime where the network has sufficiently many parameters to represent an
arbitrary image (including noise) perfectly and show that:
Fitting convolutional generators via early stopped gradient descent provably denoises “natural” images.
To prove this statement, we characterize how the network architecture governs the dynamics of
fitting over-parameterzed networks to a single (noisy) image. In particular we prove:
Convolutional generators optimized with gradient descent fit natural images faster than noise.
We depict this phenomena in Figure 1 where we fit a randomly initialized over-parameterized convo-
lutional generator to an image via running gradient descent on the objective L(C) = kG(C) - yk22.
Here, G(C) is the convolutional generator with weight parameters C, and y is either a noisy image,
a clean image, or noise. This experiment demonstrates that an over-parameterized convolutional
network fits a natural image (Figure 1b) much faster than noise (Figure 1c). Thus, when fitting the
noisy image (Figure 1a), early stopping the optimization enables image denoising. This effect is
so strong that it gives state-of-the-art denoising performance (see Figure 2 and Appendix A). Be-
yond denoising, this effect also enables significant improvements in regularizing a variety of inverse
problems such as compressive sensing (Veen et al., 2018; Heckel, 2019).
1.1	Contributions and overview of results
In this paper we take a step towards understanding why fitting a convolutional generator via early
stopped gradient descent leads to such surprising denoising and regularization capabilities.
2
Published as a conference paper at ICLR 2020
——ʃ z∖M ≡1
1	2	6	21
Figure 3:	The 1st, 2nt, 6th, and 21th trigonometric basis functions in dimension n = 300.
•	We first show experimentally that this denoising phenomena is primarily attributed to con-
volutions with fixed interpolation kernels, typically implicitly implemented by bi-linear
upsampling operations in the convolutional network.
•	We then show that fitting an over-parameterized convolutional networks (with fixed convo-
lutional filters) via early stopped gradient descent to a signal provably denoises it.
Specifically, let x ∈ Rn be a smooth signal that can be represented exactly as a linear
combination of the p orthogonal trigonometric functions of lowest frequency (defined in
equation (5), see Figure 3 for a depiction). A smooth signal is a good model for a natural
image since natural images are well approximated by low-frequency components. Specif-
ically, Figure 4 in (Simoncelli & Olshausen, 2001) shows that the power spectrum of a
natural image (i.e., the energy distribution by frequency) decays rapidly from low frequen-
cies to high frequencies. In contrast, Gaussian noise has a flat power spectrum.
Our goal is to obtain an estimate of this signal from a noisy observation y = x + z where
2
Z 〜 N(0, SnI) is Gaussian noise. Let y = G(CT) be the estimate obtained from early
stopping the fitting of a two-layer convolutional generator to y. We prove this estimate
achieves the denoising rate
ky - χk2 ≤ cpς2,
with c a constant. This rate is optimal up to a constant factor.
• Our denoising result follows from a detailed analysis of gradient descent applied to fitting a
convolutional generator G(C) to a noisy signal y = χ+z with χ representing the signal and
z the noise. We show that there are weights σ1 , . . . , σn associated with the trigonometric
basis function that only depend on the convolutional filter used, and which typically decay
quickly from the low-frequency to the high-frequency trigonometric basis functions. These
weights in turn determine the speed at which the associated components of the signal are
fitted. Specifically, we show that the dynamics of gradient descent are approximately given
by
n
n
G(Cτ)
-χ
≈	wi hwi,χi (1 - ησi2)τ+	wi hwi,zi ((1 - ησi2)τ - 1) .
i=1
l^.
{^^^^^^^^^≡
error in fitting signal
i=1
} 、
l^^^^{^^^^―
fit of noise
The convolutional filters commonly used are such that the weights σ1, . . . , σn decays
quickly, implying that low-frequency components in the trigonometric expansion are fit-
ted significantly faster than high frequency ones. So if the signal mostly consists of low-
frequency components, we can choose an early stopping time such that the error in fitting
the signal is very low, and thus the signal part is well described, whereas at the same time
only a small part of the noise, specifically the part aligned with the low-frequency compo-
nents has been fitted.
2 Convolutional generators
A convolutional generator maps an input tensor B0 to an image only using upsampling and con-
volutional operations, followed by channel normalization (a special case of batch normalization)
and applications of non-linearities, see Figure 4. All previously mentioned convolutional generator
networks (Radford et al., 2015; Ronneberger et al., 2015) including the networks considered in the
DIP paper (Ulyanov et al., 2018) primarily consist of those operations.
For motivating the architecture of the convolutional generators studied in this paper, we first demon-
strate in Section 2.1 that convolutions with fixed interpolation filters are critical to the denoising
3
Published as a conference paper at ICLR 2020
learned kernels
^^^ConVolUtion + ReLU + normalization
^^^lin. combinations, sigmoid
O
e
k
->Bd÷
P
e
k
0.15
0.1
0.05
0.01
0.08
0.06
0.04
0.02
0
fixed kernels
1
4
-------16
FigUre 4: Left panel: ConVolUtional generators. The oUtpUt is generated throUgh repeated con-
VolUtional layers, channel normalization, and applying ReLU non-linearities. Right panel: Fitting
the phantom MRI and noise with different architectUres of depth d = 5, for different nUmber of
oVer-parameterization factors (1,4, and 16). Gradient descent on conVolUtional generators inVolVing
fixed conVolUtional matrixes fit an image significantly faster than noise.
performance with early stopping. Specifically, we empirically show that conVolUtions with fixed
conVolUtional kernels are critical for conVolUtional generators to fit natUral images faster than noise.
In Section 2.2 we formally introdUce the class of conVolUtional generators stUdied in this paper, and
finally, in Section 2.3, we introdUce a minimal conVolUtional architectUre which is the focUs of oUr
theoretical resUlts.
2.1	The importance of fixed convolutional filters
ConVolUtions with fixed conVolUtional kernels are critical for denoising with early stopping, becaUse
they are critical for the phenomena that natUral images are fitted significantly faster than noise. To
see this, consider the experiment in FigUre 4 in which we fit an image and noise by minimizing the
least-sqUares loss Via gradient descent with i) a conVolUtional generator with only fixed conVolU-
tional filters (see Section 2.2 below for a precise description) and ii) a conVentional conVolUtional
generator with trainable conVolUtional filters (essentially the architectUre from the popUlar DC-GAN
generators, see Appendix B for details and additional nUmerical eVidence). As illUstrated in FigUre 4,
the conVolUtional network with fixed filters fits the natUral image mUch faster than noise, whereas
the network with learned conVolUtional filters, only fits it slightly faster, and this effect Vanishes as
the network becomes highly oVerparameterized. ThUs, fixed conVolUtional filters enable Un-trained
conVolUtional networks to fUnction as highly effectiVe image priors. We note that the Upsampling op-
eration present in most architectUres implicitly incorporates a conVolUtion with a fixed conVolUtional
(interpolation) filter.
2.2	Architecture of convolutional generator with fixed convolutions
In this section, we describe the architectUre of the deep decoder (Heckel & Hand, 2019), a conVo-
lUtional network with fixed conVolUtional operators only. In this architectUre, the channels in the
(i + 1)-th layer are giVen by
Bi+1 = cn(ReLU(UiBiCi)), i = 0,...,d- 1,
and finally, the oUtpUt of the d-layer network is formed as
x = BdCd+1.
Here, the coefficient matrices Ci ∈ Rk×k and Cd+1 ∈ Rk×kout contain the weights of the network.
The nUmber of channels, k, determines the nUmber of weight parameters of the network, giVen by
dk2 + kout k. Each colUmn of the tensor BiCi ∈ Rni ×k is formed by taking linear combinations of
the channels of the tensor Bi in a way that is consistent across all pixels, and the ReLU actiVation
function is given by ReLU(t) = max(0, t). Then, cn(∙) performs the channel normalization opera-
tion which normalizes each channel indiVidUally and can be Viewed as a special case of the popUlar
batch normalization operation (Ioffe & Szegedy, 2015).
4
Published as a conference paper at ICLR 2020
The operator Ui ∈ Rni+1 ×ni is a tensor implementing an upsampling and most importantly a
convolution operation with a fixed kernel. This fixed kernel was chosen in all experiments above as
a triangular kernel so that U performs bi-linear 2x upsampling (this is the standard implementation
in the popular software packages pytorch and tensorflow). As mentioned earlier this convolution
with a fixed kernel is critical for fitting natural images faster than complex ones.
2.3 Two layer convolutional generator studied theoretically in this paper
The simplest model to study the denoising capability of convolutional generators and the phenomena
that a natural image is fitted faster than a complex one theoretically is a network with only one hidden
layers and one output channel i.e., G(C) ∈ Rn. Then, the generator becomes
G(C) = ReLU(UB0C0)c1,
where U ∈ Rn×n is a circulant matrix that implements a convolution with a filter u. In this paper we
consider the over-parameterized regime where k ≥ 2n. Note that scaling the i-th column of C0 with
a non-negativ factor is equivalent to scaling the i-th entry of the output weights ci with the same
factor. We therefore fix the output weights to ci = v, where V = [1,..., 1, -1,..., -1]/√k. Next,
note that both the generators with the fixed output weights and non-fixed output weights have the
same range in the regime k ≥ 2n. Next, because the matrix B0 is Gaussian, with probability one, it
has full rank and thus spans Rn . It follows that optimizing over the parameter C0 in C = B0C0 is
equivalent to optimizing over the matrix C ∈ Rn×k. We therefore consider the generator
G(C) = ReLU(UC)v,	(1)
where V = [1,..., 1, -1,..., -1]∕√k is fixed and C ∈ Rn×k is the new coefficient matrix We
optimize over. Figure 11 in the appendix shows that even this simple two-layer convolutional net-
work fits a simple image faster than noise. This is the simplest model in which the phenomena that
a convolutional networks fits structure faster than noise can reliably be observed. As a consequence,
the dynamics of training the model (1) are the focus of the remainder of this paper.
3 Warmup: Dynamics of gradient descent on least squares
As a prelude for studying the dynamics of fitting convolutional generators via a non-linear least
square problem, we study the dynamics of gradient descent applied to a linear least squares problem.
We demonstrate how early stopping can lead to denoising capabilities even with a simple linear
model. We consider a least-squares problem of the form
L(C) = 2ky - Jc∣∣2,
and study gradient descent with a constant step size η starting at c0 = 0. The updates are given by
CT+ι = CT —，NL(CT), VL(c)=JT(Jc-y).
The following simple proposition characterizes the trajectory of gradient descent.
Proposition 1. Let J ∈ Rn×m be a matrix with left singular vectors wi, . . . , wn ∈ Rn and corre-
sponding singular values σi ≥ σ2 ≥ . . . ≥ σn. Then the residual after τ steps, rT = y - JCT, of
gradient descent starting at C0 = 0 is
n
rT =	wi hwi, yi (1 - ησi2)T.
i=i
Suppose that the signal y lies in the column span of J, and that the stepsize is chosen sufficiently
small (i.e., η ≤ 1/ kJk2). Then, by Proposition 1, gradient descent converges to a zero-loss solution
and thus fits the signal perfectly. More importantly, gradient descent fits the components of y corre-
sponding to large singular values faster than it fits the components corresponding to small singular
values.
To explicitly show how this observation enables regularization via early stopped gradient descent,
suppose our goal is to find a good estimate of a signal x from a noisy observation
y=x+z,
5
Published as a conference paper at ICLR 2020
iσ eulav ralugnis
2kτcJ-1wk
iττ
Figure 5: Gradient descent on the least squares problem of minimizing ky - Jcτ k2 , where
J ∈ R100×100 has decaying singular values (left panel) and the observation is the sum of a signal
component, equal to the leading singular vector wι of J, and a noisy component Z 〜N(0, (1∕n)I),
i.e., y = w1 + z. The signal component w1 is fitted significantly faster than the other components
(right panel), thus early stopping enables denoising.
where the signal x lies in a signal subspace that is spanned by the p leading left-singular vectors of
J. Then, by Proposition 1, the signal estimate after τ iterations, Jcτ, obeys
kJcτ - xk2 ≤ (1 - ησp2)τ kxk2 + E(z), E(z) := t
n
X((1 - ησi2)τ - 1)2 hwi,zi2 .	(2)
i=1
Thus, after a few iterations most of the signal has been fitted (i.e., (1 - ησp)τ is small). Furthermore,
if we assume that the ratio σp /σp+1 is sufficiently large so that the spread between the two singular
values separating the signal subspace from the rest is sufficiently large, most of the noise outside the
signal subspace has not been fitted (i.e., ((1 - ησi2)τ - 1)2 ≈ 0 for i = p + 1, . . . , n).
2
In particular, suppose the noise vector has a Gaussian distribution given by Z 〜 N(0, SnI). Then
E(Z) ≈ ςʌ/p so that after order τ = log(e)/ log(1 - ησp) iterations, with high probability,
kJcT - xk2 ≤ e kxk2 + cς√p
This demonstrates, that provided the signal lies in a subspace spanned by the leading singular vec-
tors, early stoped gradient descent reaches the optimal denoising rate of ZIpln after a few itera-
tions. See Figure 5 for a numerical example demonstrating this phenomena.
4 Dynamics of gradient descent on convolutional generators
We are now ready to study the implicit bias of gradient descent towards natural/structured images
theoretically. Consider the two-layer network (introduced in Section 2.3) of the form
G(C) = ReLU(UC)v,
where V = [1,..., 1, -1,..., -1]/√k, and with weight parameter C ∈ Rn×k, and recall that U is
a circulant matrix implementing a convolution with a kernel u ∈ Rn. We fit the generator to a signal
y ∈ Rn by minimizing the non-linear least squares objective
L(C) = Iky - G(C)k2	⑶
with (early-stopped gradient) descent with a constant stepsize η starting at a random initialization
C0 of the weights. The iterates are given by
Cτ+1 = Cτ - ηVL(Cτ).	(4)
In our warmup section on linear least squares we saw that the singular vectors and values of the ma-
trix J determine the speed at which different components of the noisy signal y are fitted by gradient
descent. The main insight that enables us to extend this intuition to the nonlinear case is that the role
of the matrix J can be replaced with the Jacobian of the generator, defined as J(C) := ∂CG(C).
6
Published as a conference paper at ICLR 2020
Figure 6: Triangular and Gaussian kernels and the weights associated to low-frequency trigono-
metric functions they induce, for a generator network of output dimension n = 300. The wider the
kernels are, the more the weights are concentrated towards the low-frequency components of the
signal.
Contrary to the linear least squares problem, however, in the nonlinear case, the Jacobian is not
constant and changes across iterations. Nevertheless, we show that the eigen-values and vectors of
the Jacobian at the random initialization govern the dynamics of fitting the network throughout the
iterative updates.
For the two-layer convolutional generator that we consider, the left eigenvectors of the Jacobian
mapping can be well approximated by the trigonometric basis functions, defined below, throughout
the updates. Interestingly, the form of these eigenvectors only depends on the network architecture
and not the convolutional kernel used.
Definition 1. The trigonometric basis functions w1, . . . , wn are defined as
{1	i = 0
√2cos(2πji∕n) i = 1,..., n/2 — 1
(-1)j	i = n/2
√2sin(2πji∕n) i = n/2 + 1,...,n — 1
(5)
Figure 3 depicts some of these eigenvectors.
In addition to the left eigenvectors we can also approximate the spectrum of the Jacobian throughout
the updates by an associated filter/kernel that only depends on the original filter/kernel used in the
network architecture.
Definition 2 (Dual kernel). Associated with a kernel u ∈ Rn we define the dual kernel σ ∈ Rn as
σ = kuk2 t
u ~ u	1	cos-1 (t)
Fg(TUkdl with M = 2 (	~^)t.
Here, for two vectors u, v ∈ Rn, u ~ v denotes their circular convolution, the scalar non-linearity
g is applied entrywise, and F is the discrete Fourier transform matrix.
In Figure 6, we depict two commonly used interpolation kernels u, namely a triangular and a Gaus-
sian kernel (recall that the standard upsampling operator is a convolution with a triangle), along with
the induced dual kernel σ . The figure shows that the dual kernel σ induced by these kernels has
a few large values associated with the low frequency trigonometric functions, and the other values
associated with high frequencies are very small.
With these definitions in place we are ready to state our main denoising result. A denoising result
requires a signal model—we assume a low-frequency signal x that can be represented as a linear
combination of the first p-trigonometric basis functions. Note that this is a good model for a natu-
ral image since natural images are well approximated by low-frequency components. Specifically,
Figure 4 in (Simoncelli & Olshausen, 2001) shows that the power spectrum of a natural image (i.e.,
the energy distribution by frequency) decays rapidly from low frequencies to high frequencies. In
contrast, Gaussian noise has a flat power spectrum.
7
Published as a conference paper at ICLR 2020
Theorem 1	(Denoising with early stopping). Let x ∈ Rn be a signal in the span of the first p
trigonometric basis functions, and consider a noisy observation
y=x+z,
where Z is Gaussian noise with distribution N(0, SnI), for some variance ς2 ≥ 0. To denoise this
signal, we fit a two layer generator network G(C) = ReLU(UC)v with,
k ≥ Cun/8,	(6)
channels, for some > 0, and with convolutional kernel u of the convolutional operator U and
associated dual kernel σ, to the noisy signal y. Here, Cu a constant only depending on the convolu-
tional kernel u. Then, with probability at least 1 一 e-k2 一 n⅛ ,the reconstruction error obtained after
T = log(1 一 ,p/n)/log(1 — ησp+ι) iterations of gradient descent (4) with step size η ≤
1
kFUk∞
(Fu is the Fourier transform of U) starting from Co with i.i.d. N(0, ω2), entries, ω H k√n2, is
bounded by
kG(Cτ) 一
χk2 ≤ (I 一 ησP)TkXk2 + ς,2p + dlylh
(7)
Note that for this choice of stopping time, provided that dual kernel decays sharply around the p-th
singular value, the first term in the bound (7) (i.e., (1 一 ησp)τ ≈ 0) essentially vanishes and the error
bound becomes O(ςʌ/p). The dual kernel decays sharply around the leading eigenvalues provided
the kernel is for example a sufficiently wide triangular or Gaussian kernel (see Figure 6).
This result demonstrates that when the noiseless signal χ is sufficiently structured (e.g. contains only
the p lowest frequency components in the trigonometric basis) and the convolutional generator has
sufficiently many channels, then early stopped gradient descent achieves a near optimal denoising
performance proportional to ς ʌ/n[. This theorem is obtained from a more general result stated in the
appendix which characterizes the evolution of the reconstruction error obtained by the convolutional
generator.
Theorem 2	(Reconstruction dynamics of convolutional generators). Consider the setting and as-
sumptions of Theorem 1 but now with a fixed noise vector z, and without an explicit stopping time.
Then, for all iterates T obeying T ≤ 102 and provided that k ≥ CUn/F ,for some E ∈ (0, — ], with
ησp	σ1
k2	1
probability at least 1 — e — nɪ, the reconstruction error obeys
kG(Cτ) 一 χk2 ≤ (1 一 ησp2)τ kχk2 + t
n
X((1 一 ησi2)τ 一 1)2 hwi, zi2 + Ekyk2.
i=1
This theorem characterizes the reconstruction dynamics of convolutional generators throughout the
updates. In particular, it explains why convolutional generators fit a natural signal significantly faster
than noise, and thus early stopping enables denoising and regularization. To see this, note that as
mentioned previously each of the basis functions wi has a (positive) weight σi > 0 associated with
it that only depends on the convolutional kernel used in the architecture (through the definition of
the dual kernel). These weights determine how fast the different components of the noisy signal are
fitted by gradient descent. As we demonstrated earlier in Figure 6, for typical convolutional filters
those weights decay very quickly from low to high frequency basis functions. As a result, when the
signal χ is sufficiently structured (i.e. lies in the range of the p trigonometric functions with lowest
frequencies), after a few iterations most of the signal is fitted (i.e., (1 一 ησp2 )τ is small), while most
of the noise has not been fitted (i.e., ((1 一 ησi2)τ 一 1)2 ≈ 0 for i = p + 1, . . . , n). Thus, early
stopping achieves denoising.
The proof, provided in the appendix, is based on associating the following linear least-squares prob-
lem with the non-linear least squares problem (3):
1
Llin(C) = £ IlG(CO) + J(C - Vect(CO))- y∣∣2∙
8
Published as a conference paper at ICLR 2020
Here, vect(C0) ∈ Rkn is a vectorized version of the matrix C0, and J ∈ Rn×nk approximates
the Jacobian of G(C) around the initialization C0 . The linear least-squares problem is obtained
by linearizing the non-linear problem around the initialization. In the proof we show that if i)
the network is sufficiently over-parameterized and if ii) the network is randomly initialized, then
the linearized problem is a good approximation of the non-linear problem. We then show that the
singular values of the Jacobian J are the trigonometric basis functions, and the singular values are
the values of the dual kernel. The proof is then concluded by characterizing the trajectory of gradient
descent applied to the linear least-squares problem above.
4.1 Multilayer networks and moderate over-parameterization
Our theoretical results rely on the finding that for over-parameterized single hidden-layer networks,
the leading singular vectors of the Jacobian are the trigonometric functions throughout all itera-
tions, and that the associated weights (i.e., the singular values) are concentrated towards the low
frequency components. In this regime, the dynamics are well approximated by a linear model. This
general strategy can be extended to multi-layer networks, however if the network is not highly over-
parameterized, an associated linear model might not be a good approximation.
In order to understand whether our finding of low-frequency components being being fitted
faster than high-frequency ones carries over to multi-layer networks and to the moderately over-
parameterized regime, in this section, we study muli-layer networks in the moderately overparame-
terized regime numerically. We show that for a multilayer network, the spectrum of the Jacobian is
concentrated towards singular vectors/functions that are similar to the low-frequency components.
We also show that throughout training those functions do vary, albeit the low frequency components
do not change significantly and the spectrum remains concentrated towards the low frequency com-
ponents. This shows that the implications of our theory continue to apply to muli-layer networks.
In more detail, we take a standard one dimensional deep decoder with d = 4 layers with output in
R512 and with k = 64 channels in each layer. Recall that the standard one dimensional decoder ob-
tains layer i + 1 from layer i by linearly combining the channels of layer i with learnable coefficients
followed by linear upsampling (which involves convolution with the triangular kernel [1/2, 1, 1/2]).
The number of parameters is d X k2 = 32 ∙ 512, so the network is over-parameterized by a factor of
32. In Figure 7, we display the singular values as well as the leading singular vectors/function of the
Jacobian at initialization (t = 1) and after t = 50 and t = 3500 iterations of gradient descent. As
can be seen the leading singular vectors (s = 1-5) are close to the trigonometric basis functions and
do not change dramatically throughout training. The singular vectors corresponding to increasingly
smaller singular values (s = 20, 50, 100, 150) contain increasingly higher frequency components
but are far from the high-frequency trigonometric basis functions.
5 Related literature
As mentioned before, the DIP paper (Ulyanov et al., 2018) was the first to show that over-
parameterized convolutional networks enable solving denoising, inpainting, and super-resolution
problems well even without any training data. Subsequently, the paper (Heckel & Hand, 2019) pro-
posed a much simpler image generating network, termed the deep decoder. The papers (Veen et al.,
2018; Heckel, 2019; Jagatap & Hegde, 2019; Bostan et al., 2020) have shown that the DIP and the
deep decoder also enable solving or regularizing compressive sensing problems and other inverse
problems.
Since the convolutional generators considered here are image-generating deep networks, our work
is also related to methods that rely on trained deep image models. Deep learning based methods are
either trained end-to-end for tasks ranging from compression (Toderici et al., 2016; Agustsson et al.,
2017; Theis et al., 2017; Burger et al., 2012; Zhang et al., 2017) to denoising (Burger et al., 2012;
Zhang et al., 2017), or are based on learning a generative image model (by training an autoencoder
or GAN (Hinton & Salakhutdinov, 2006; Goodfellow et al., 2014)) and then using the resulting
model to solve inverse problems such as compressed sensing (Bora et al., 2017; Hand & Voroninski,
2018), denoising (Heckel et al., 2018), or phase retrieval (Hand et al., 2018), by minimizing an
associated loss. In contrast to the method studied here, where the optimization is over the weights
9
Published as a conference paper at ICLR 2020
——at t = 1——at t = 50 ——at t = 3500
(f) s = 5	(g) s = 20	(h) s = 50	(i) s = 100
Figure 7: The Singular value distribution of the Jacobian of a four-layer deep decoder after t = 50
and t = 3500 iterations of gradient descent (panel (a)), along with the corresponding singular
vectors/function (b-i). The singular functions corresponding to the large singular vectors are close
to the low-frequency Fourier modes and do not change significantly through training.
of the network, in all the aforementioned methods, the weights are adjusted only during training and
then are fixed upon solving the inverse problem.
A large body of work focuses on understanding the optimization landscape of the simple nonlin-
earities or neural networks (Candes et al., 2015; Soltanolkotabi, 2017; Alon Brutzkus & Globerson,
2017; Zhong et al., 2017; Oymak, 2018; Fu et al., 2018; Tu et al., 2016) when the labels are created
according to a planted model. Our proofs rely on showing that the dynamics of gradient descent on
an over-parameterized network can be related to that of a linear network or a kernel problem. This
proof technique has been utilized in a variety of recent publication (Soltanolkotabi et al., 2018; Ven-
turi et al., 2019; Du et al., 2018; Oymak & Soltanolkotabi, 2019a;b; Arora et al., 2019; Oymak et al.,
2019). Two recent publication have used this proof technique to show that functions are learned at
different rates: Basri et al. (2019) have shown that functions of different frequencies are learned
at different speeds, and Arora et al. (2019) has provided a theoretical explanation of the empirical
observation that a simple 2-layer network fits random labels slower than actual labels in the context
of classification. A recent publication (Li et al., 2019) focuses on demonstrating how early stopping
leads to robust classification in the presence of label corruption under a cluster model for the input
data. Neither of the aforementioned publication however, does address denoising in a regression
setting or fitting convolutional generators of the form studied in this paper.
Code and Acknowledgements
Code to reproduce the experiments is available at https://github.com/MLI-lab/
overparameterized_convolutional_generators.
R. Heckel is partially supported by NSF award IIS-1816986, acknowledges support of the NVIDIA
Corporation in form of a GPU, and would like to thank Tobit Klug for proofreading a previous
version of this manuscript. M. Soltanolkotabi is supported by the Packard Fellowship in Science
and Engineering, a Sloan Research Fellowship in Mathematics, an NSF-CAREER under award
#1846369, the Air Force Office of Scientific Research Young Investigator Program (AFOSR-YIP)
under award #FA9550-18-1-0078, an NSF-CIF award #1813877, DARPA under the Learning with
Less Labels (LwLL) program, and a Google faculty research award.
10
Published as a conference paper at ICLR 2020
References
Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca
Benini, and Luc V Gool. Soft-to-hard vector quantization for end-to-end learning compressible
representations. In Advances in Neural Information Processing Systems, pp. 1141-1151, 2017.
A. Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with Gaus-
sian inputs. In International Conference on Machine Learning, 2017.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, 2019.
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural
networks for learned functions of different frequencies. In Advances in Neural Information Pro-
cessing Systems, 2019.
A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models. In
International Conference on Machine Learning, 2017.
Emrah Bostan, Reinhard Heckel, Michael Chen, Michael Kellman, and Laura Waller. Deep
phase decoder: Self-calibrating phase microscopy with an untrained deep neural network.
arXiv:2001.09803 [physics], 2020.
H.	C. Burger, C. J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete
with BM3d? In IEEE Conference on Computer Vision and Pattern Recognition, pp. 2392-2399,
2012.
Emmanuel J. Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow:
Theory and algorithms. IEEE Transactions on Information Theory, 64, 2015.
K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-D transform-
domain collaborative filtering. IEEE Transactions on Image Processing, 16(8):2080-2095, 2007.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances in Neural Information
Processing Systems, pp. 2253-2261, 2016.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018.
Haoyu Fu, Yuejie Chi, and Yingbin Liang. Local geometry of one-hidden-layer neural networks for
logistic regression. arXiv:1802.06463, 2018.
I.	Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A Courville, and
Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems,
pp. 2672-2680. 2014.
Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by
empirical risk. In Conference on Learning Theory, 2018. arXiv:1705.07576.
Paul Hand, Oscar Leong, and Vladislav Voroninski. Phase retrieval under a generative prior. In
Advances in Neural Information Processing Systems, 2018.
Reinhard Heckel. Regularizing linear inverse problems with convolutional neural networks.
arXiv:1907.03100, 2019.
Reinhard Heckel and Paul Hand. Deep decoder: Concise image representations from untrained
non-convolutional networks. In International Conference on Learning Representations, 2019.
Reinhard Heckel, Wen Huang, Paul Hand, and Vladislav Voroninski. Deep denoising: Rate-optimal
recovery of structured signals with a deep prior. arXiv:1805.08855, 2018.
11
Published as a conference paper at ICLR 2020
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504-507, 2006.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Gauri Jagatap and Chinmay Hegde. Algorithmic guarantees for inverse imaging with untrained
network priors. In Advances in Neural Information Processing Systems, 2019.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. arXiv:1903.11680, 2019.
Samet Oymak. Stochastic gradient descent learns state equations with nonlinear activations.
arXiv:1809.03019, 2018.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? In International Conference on Machine Learning, 2019a.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: Global conver-
gence guarantees for training shallow neural networks. arXiv:1902.04674, 2019b.
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees
for neural networks via harnessing the low-rank structure of the jacobian. arXiv:1906.05392,
2019.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv:1511.06434 [cs], 2015.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. Lecture Notes in Computer Science, 2015.
Eero P Simoncelli and Bruno A Olshausen. Natural image statistics and neural representation.
Annual Review of Neuroscience, 24(1):1193-1216, 2001.
Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. In Advances in Neural Information
Processing Systems, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 2018.
Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszar. Lossy image compression with
compressive autoencoders. arXiv:1703.00395, 2017.
George Toderici, Sean M. OMalley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet
Baluja, Michele Covell, and Rahul Sukthankar. Variable rate image compression with recurrent
neural networks. In International Conference on Learning Representations, 2016.
Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-
tional Mathematics, 12(4):389-434, Aug 2011.
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank
solutions of linear matrix equations via procrustes flow. In International Conference on Machine
Learning, 2016.
D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior. In Conference on Computer Vision
and Pattern Recognition, 2018.
Dave Van Veen, Ajil Jalal, Mahdi Soltanolkotabi, Eric Price, Sriram Vishwanath, and Alexan-
dros G. Dimakis. Compressed sensing with deep image prior and learned regularization.
arXiv:1806.06438, 2018.
L. Venturi, A. Bandeira, and J. Bruna. Spurious valleys in two-layer neural network optimization
landscapes. Journal on Machine Learning Research, 2019.
12
Published as a conference paper at ICLR 2020
K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a Gaussian denoiser: Residual learning
of deep CNN for image denoising. IEEE Transactions on Image Processing, 26(7):3142-3155,
2017.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In International Conference on Machine Learning, 2017.
A Denoising performance of untrained convolutional
GENERATORS
In this section, we provide further details on the denoising performance of deep neural networks. We
compare the denoising performance of four methods: i) the BM3D algorithm (Dabov et al., 2007)
as a standard baseline for denoising, ii) the deep image prior (Ulyanov et al., 2018), which is a U-net
like encoder-decoder convolutional architecture applied with exactly the parameters as proposed for
denoising in the paper (Ulyanov et al., 2018), and early stopped after 1900 iterations, again with
the same stopping time as proposed in the original paper, iii) an under-parameterized deep decoder
with five layers and k = 128 channels in each layers, trained for 3000 iterations (which is close
to convergence), iv) an over-parameterized deep decoder with five layers and k = 512 channels in
each layer, early stopped at 1500 iterations. We compared the performance of those four methods
on denoising 100 randomly chosen images from the ImageNet validation set. The code to reproduce
the results contains a list of the images we considered. Each image has 512 × 512 pixels and three
color channels. We added the same random Gaussian noise to each color channel, because we are
interested in evaluating the performance by imposing structural assumptions on the image. Table 1
below show that the overparameterized deep decoder with early stopping performs best for this task.
Noise level	20.60
Deep decoder with k=512 and early stopping	28.08dB
deep decoder with k=128 without early stopping 27.84dB
DIP	25.76 dB
BM3D	25.52 dB
Table 1: Average performance for denoising images color image with the same Gaussian noise
added to each color channel with BM3D and un-trained convolutional neural networks. The over-
parameterized deep decoder performs best for this task.
B A numerical study of the implicit bias of convolutional
NETWORKS
In this section, we empirically demonstrate that convolutions with fixed convolutional kernels are
critical for convolutional generators to fit natural images faster than noise. Towards this goal,
we study numerically the following four closely related architectural choices, which differ in
the upsampling/no-upsampling and convolutional operations which generate the activations in the
(i + 1)-st layer, Bi+1, from the activations in the i-th layer, Bi:
i)	Bilinear upsampling and linear combinations. Layer i + 1 is obtained by linearly com-
bining the channels of layer i with learnable coefficients (i.e., performing one-times-one
convolutions), followed by bi-linear upsampling. This is the deep decoder architecture
from (Heckel et al., 2018).
ii)	Fixed interpolation kernels and linear combinations. Layer i + 1 is obtained by lin-
early combining the channels of layer i with learnable coefficients followed by convolving
each channel with the same 4x4 interpolation kernel that is used in the linear upsampling
operator.
iii)	Parameterized convolutions: Layer i + 1 is obtained from layer i though a convolutional
layer.
iv)	Deconvolutional network: Layer i + 1 is obtained from layer i though a deconvolution
layer. This is essentially the DC-GAN (Radford et al., 2015) generator architecture.
13
Published as a conference paper at ICLR 2020
bilinear fixed kernels param. convs deconvolutions
0.15
0.1
0.05
0
0.08
0.06
0.04
0.02
0
optimizer steps	optimizer steps	optimizer steps	optimizer steps
Figure 8:	Fitting the phantom MRI and noise with different architectures of depth d = 5, for
different number of over-parameterization factors (1,4, and 16). Gradient descent on convolutional
generators involving fixed convolutional matrixes fit an image significantly faster than noise.
To emphasize that architectures i)-iv) are structurally very similar operations, we recall that each
operation consists only of upsampling and convolutional operations. Let T(c) : Rn → Rn be the
convolutional operator with kernel c, let u the linear upsampling kernel (equal to u = [0.5, 1, 0.5]
in the one-dimensional case), and let U : Rn → R2n be an upsampling operator, that in the one
dimensional case transforms [x1, x2, . . . , xn] to [x1, 0, x2, 0, . . . , xn, 0]. In each of the architec-
tures i)-iv), the `-th channel of layer i + 1 is obtained from the channels in the i-th layer as:
bi+ι,' = ReLU (Pk=I M(Cij`)b) , where the linear operator M is defined as follows for the
four architectures
i)	M(c) = cT(u)U, ii) M(c) = cT(u), iii) M(c) = T(c), iv) M(c) = T(c)U.
The coefficients associated with the i-th layer are given by Ci = {cj'}, and all coefficients of the
networks are C = {cj'}. Note that here, the coefficients or parameters of the networks are the
weights and not the input to the network.
B.1	Demonstrating implicit bias of convolutional generators
We next show that convolutional generators with fixed convolutional operations fit natural or simple
images significantly faster than complex images or noise. Throughout this section, for each image
or signal x* We fit weights by minimizing the loss
L(C) = ∣∣G(C)- x*k2
with respect to the network parameters C using plain gradient descent with a fixed stepsize.
In order to exemplify the effect, we fit the phantom MRI image as well as noise for each of the
architectures above for a 5-layer network. We choose the number of channels, k, such that the
over-parameterization factor (i.e., the ratio of number of parameters of the network over the output
dimensionality) is 1, 4, and 16, respectively. The results in Figure 8 show that for architectures i)
and ii) involving fixed convolutional operations, gradient descent requires more than one order of
magnitude fewer iterations to obtain a good fit of the phantom MRI image relative to noise. For
architectures iii) and iv), with trainable convolutional filters, we see a smaller effect, but the effect
essentially vanishes when the network is highly over-parameterized.
This effect continues to exist for natural images in general, as demonstrated by Figure 10 which
depicts the average and standard deviation of the loss curves of 100 randomly chosen images from
the imagenet dataset.
We also note that the effect continues to exist in the sense that highly structured images with a large
number of discontinuities are difficult to fit. An example is the checkerboard image in which each
pixel alternates between 1 and 0; this image leads to the same loss curves as noise.
In our next experiment, we highlight that the distance between final and initial network weights is
a key feature that determines the difference of fitting a natural image and noise. Towards this goal,
14
Published as a conference paper at ICLR 2020
oousSIP°A=-oJ
natural image	noise
0.008
0.006
0.004
0.002
0
0	50,000 1 ∙ 105	0	50,000 1 ∙ 105
optimizer steps optimizer steps
Figure 9:	The relative distances of the weights in each layer from its random initialization. The
weights need to change significantly more to fit the noise, compared to an image, thus a natural
image lies closer to a random initialization than noise.
100	102	104
(b) fitting pure noise
0.08
0.06
0.04
0.02
optimizer steps	optimizer steps
Figure 10:	The loss curves for architecture i), a convolutional generator with linear upsampling
operations, averaged over 100 3 × 512 × 512 (color) images from the Imagenet dataset. The error
band is one standard deviation. Convolutional generators fit natural images significantly faster than
noise.
we again fit the phantom MRI image and noise for the architecture i) and an over-parameterization
factor of 4 and record, for each layer i the relative distance kCi(t) - Ci(0) k/kCi(0) k, where Ci(0) are
the weights at initialization (we initialize randomly), and Ci(t) are the weights at the optimizer step
t. The results, plotted in Figure 9, show that to fit the noise, the weights have to change significantly,
while for fitting a natural image they only change slightly.
C	The spectrum of the Jacob ian of the deep decoder and deep
IMAGE PRIOR
Our theoretical results predict that for over-parameterized networks, the parts of the signal that is
aligned with the leading singular vectors of the Jacobian at initialization is fitted fastest. In this
section we briefly show that natural images are much more aligned with the leading singular vectors
than with Gaussian noise, which is equally aligned with each of the singular vectors.
(a) step function (b) step function fit MSE (c) noise	(d) noise fit MSE
Figure 11: Fitting a step function and noise with a two-layer deep decoder: Even for a two-layer
network, the simple image (step function) is fitted significantly faster than the noise.
15
Published as a conference paper at ICLR 2020
(a)	deep decoder
(b)	deep image prior
ycneuqer
0.006
0.004
0.002
0
0
0.5
y=Noise
y=Img
“Mil
Jτ(θ0)yll2	∙104
y=Noise
y=Img
0	0.2 0.4	0.6 0.8	1
Jτ(θ0)yll2	∙105
1
Figure 12: The distribution of the '2 -norm of the inner product of the Jacobian of a deep decoder (a)
and a deep image prior (b) at a random initialization, with an image y = x* and noise y = z, both
of equal norm. For both deep decoder and deep image prior, this quantity is significantly smaller for
noise than fora natural image. Thus, a natural image is better aligned with the leading singular vec-
tors of the Jacobian than noise. This demonstrates that the Jacobian of the networks is approximately
low-rank, with natural images lying in the space spanned by the leading singularvectors.
Towards this goal, we compute the norm of the product of the Jacobian at a random initalization, C0,
with a signal y as this measures the extent to which the signal is aligned with the leading singular
vectors, due to
JT(C0)y2=VΣWTy2=Xσi2hwi,yi2,
i
where J (C0) = WΣVT is the singular value decomposition of the Jacobian.
Figure 12 depicts the distribution of the norm of the product of the Jacobian at initialization,J (C0),
with an image y* or noise z of equal norm (ky* k = kzk). Since for both the deep decoder and
the deep image prior, the norm of product of the Jacobian and the noise (i.e., JT(C0)z ) is
significantly smaller than that with a natural image (i.e., J T (C0)y* ), it follows that a structured
image is much better aligned with the leading singular vectors of the Jacobian than the Gaussian
noise, which is approximately equally aligned with any of the singular vectors. Thus, the Jacobian
at random initialization has an approximate low-rank structure, with natural images lying in the
space spanned by the leading singularvectors.
16
Published as a conference paper at ICLR 2020
D Proofs and formal s tatement of results
The results stated in the main text are obtained from a slightly more general result which applies
beyond convolutional networks. Consider neural network generators of the form
G(C) = ReLU(UC)v,	(8)
with C ∈ Rn×k, and U ∈ Rn×n an arbitrary fixed matrix, and V ∈ Rk, with half of the entries
equal to +1/ √k and the other half equal to -1/ √k.
The (transposed) Jacobian of ReLU(Uc) is UT diag(ReLU0 (Uc)). Thus the Jacobian of G(C) is
given by
"vιUT diag(ReLU0(Ucι ))
JT(C)=	.	∈ Rnk×n,	(9)
vkUT diag(ReLU0(Uck))
where ReLU0 is the derivative of the activation function. Next we define a notion of expected
Jacobian. Towards this goal, we first define the matrix
Σ(U) := E J (C)J T (C) ∈ Rn×n
associated with the generator G(C) = ReLU(UC)v. Here, expectation is over C with iidN (0, ω2)
entries. Note that since the derivative of the ReLU non-linearity is invariant to the scaling of C, so
is the Jacobian J (C), but to be consistent throughout we take expectation over C with iidN (0, ω2)
entries. Consider the eigenvalue decomposition of Σ(U) given by
n
Σ(U) = Xσi2wiwiT.
i=1
Our results depend on the largest and smallest eigenvalue of Σ(U), defined throughout as
α = σn2, β = σ12 = kUk.
With these definitions in place we are ready to state our result about neural generators.
Theorem 3. Consider a noisy signal y ∈ Rn given by
y=x+z,
where x ∈ Rn is assumed to lie in the signal subspace spanned by the p leading singular vectors
w1, . . . , wn of Σ(U), andz ∈ Rn is an arbitrary noise vector. Suppose that the number of channels
obeys
k ≥ cnξ-8(l + ξαηTβ] βl8	W
4 β	α18
where C is a fixed numerical constant and ξ an error tolerance parameter obeying 0 < ξ ≤
1/y 32log (2n). Moreover, T is the maximal number Ofgradient descent steps, and we assume
25β2
It obeys 1 ≤ T ≤ ηξɑ∙ Wefit the neural generator G(C) to the noisy signal y ∈ Rn by mιnιmιz-
ing the loss
L(C) = 2kG(C)- yk2	(11)
via running gradient descent StartingfrOm Co with i.i.d. N(0, ω2) entries, ω = √n2ξα2, and Step
size obeying η ≤ 1∕β2. Then, with probability at least 1 — e-k2 — δ ,for all iterations T ≤ T,
kx - G(Cτ)k2 ≤(1-ησp2)τkxk2+t
n
X((1 - ησi2)τ - 1)2 hwi,zi2 + ξkyk2.	(12)
i=1
17
Published as a conference paper at ICLR 2020
D.1 Proof of Theorem 2
Theorem 2 stated in the main text follow directly from Theorem 3 above as follows. We first note
that for a convolutional generator (where U implements a convolution and thus is circulant) the
eigenvectors of the matrix Σ(U) are given by the trigonometric basis functions per Definition 1 and
the eigenvalues are the square of the entries of the dual kernel (Definition 2). To see this, we note
that as detailed in Section H,
因(U)]ij=2(1 -COS-1( kUui,ujk) 〃) hui, Uj i.	(13)
Because the matrix U implements a convolution with a kernel u that is equal to its first column, the
matrix Σ(U) is again a circulant matrix and is also Hermitian. Thus, its spectrum is given by the
Fourier transform of the first column of the circulant matrix, and its left-singular vectors are given
by the trigonometric basis functions defined in equation (5) and depicted in Figure 3.
Furthermore, using the fact that the eigenvalues of a circulant matrix are given by its discrete Fourier
transform we can substitute β = kUk = kFUk∞ and α = σn > 0. With the assumption τ ≤ T =
η1∣2, and using that
1 + ξαTβ2 ≤ 1 + 32ξ- ≤ 33,
where we used the assumption ξ ≤ σp, the condition (10) is implied by the assumption k ≥ Cun/S,
with Cu 8 αβ18 (see equation (6), where this assumption is made). This yields Theorem 2.
D.2 Proof of Theorem 1
Finally, we note that to obtain the simplified final expression in Theorem 1 from Theorem 2 we also
used the fact that for a Gaussian vector z, the vector WTz is also Gaussian. Furthermore, by the
concentration of Lipschitz functions of Gaussians with high probability we have
nn
X((1 - ησi2)τ - 1)2hwi,zi2 ≈ E X((1 - ησi2)τ - 1)2hwi,zi2
i=1	i=1
2n
=ςn X((1-ησ2)τ - 1)2
i=1
≤) ς 2 2P.
n
Here, equation (i) follows from Ri, Zi being zero mean Gaussian with variance ς2/n (since Z 〜
N(0, (ς2∕n)I), and ∣∣Wi∣∣2 = 1). Finally, (ii) follows by choosing the early stopping time so that it
obeys T ≤ log(1 - /p/n)/log(1 - ησp+1) which in turn implies that (1 -ησ2)τ ≥ 1 - √pTn,
for all i > p, yielding that ((1 - ησi2)τ - 1)2 ≤ p/n, for all i > p.
E The dynamics of linear and nonlinear least-squares
Theorem 3 builds on a more general result on the dynamics of a non-linear least squares problem
which applies beyond convolutional networks, and that is stated and discussed in this section. Con-
sider a nonlinear least-squares fitting problem of the form
L(θ) = 2 kf (θ)-yk2.
Here, f : RN → Rn is a non-linear model with parameters θ ∈ RN . To solve this problem, we run
gradient descent with a fixed stepsize η, starting from an initial point θ0 , with updates of the form
θτ+1 = θτ - NL(θτ) where VL(θ) = JT(θ)(f(θ) - y).	(14)
Here, J(θ) ∈ Rn×N is the Jacobian associated with the nonlinear map f with entries given by
[J(θ)]ij = d∂θθ). In order to study the properties of the gradient descent iterates (14), we relate
18
Published as a conference paper at ICLR 2020
the non-linear least squares problem to a linearized one in a ball around the initialization θ0 . We
note that this general strategy has been utilized in a variety of recent publications (Du et al., 2018;
Arora et al., 2019; Oymak & Soltanolkotabi, 2019b; Oymak et al., 2019). The associated linearized
least-squares problem is defined as
12
Llin(θ) = 2 kf (θo) + J(θ - θo) - yk2.	(15)
Here, J ∈ Rn×p , refered to as the reference Jacobian, is a fixed matrix independent of θ that
approximates the Jacobian mapping at initialization, J (θ0). Starting from the same initial point θ0,
the gradient descent updates of the linearized problem are
θeτ+1 =θeτ -ηJT f (θ0) + J(θeτ -θ0) -y
=θeτ -ηJTJ θeτ -θ0 -ηJT (f(θ0) -y).	(16)
To show that the non-linear updates (14) are close to the linearized iterates (16), we make the fol-
lowing assumptions:
Assumption 1 (Bounded spectrum). We assume the singular values of the reference Jacobian obey
α ≤ σn ≤ σ1 ≤ β.	(17)
Furthermore, we assume that the Jacobian mapping associated with the nonlinear model f obeys
kJ (θ)k ≤ β for all θ ∈ RN.	(18)
Assumption 2 (Closeness of the reference and initialization Jacobians). We assume the reference
Jacobian and the Jacobian of the nonlinearity at initialization J (θ0) are 0-close in the sense that
kJ (θ0) - Jk ≤ 0.	(19)
Assumption 3 (Bounded variation of Jacobian around initialization). We assume that within a radius
R around the initialization, the Jacobian varies by no more than in the sense that
kJ (θ)-J (θo)k ≤ I，for all θ ∈Br (θo),	(20)
where BR(θ0) := {θ : kθ - θ0k ≤ R} is the ball with radius R around θ0.
Our first result shows that under these assumptions the nonlinear iterative updates (14) are intimately
related to the linear iterative updates (16). Specifically, we show that the residuals associated with
these two problems defined below
nonlinear residual: rτ := f(θτ) - y	(21)
linear residual:	eτ :=(I - ηJJT)τ r0,	(22)
are close in the proximity of the initialization.
Theorem 4 (Closeness of linear and nonlinear least-squares problems). Assume the Jacobian map-
ping J(θ) ∈ Rn×N associated with the function f(θ) obeys Assumptions 1, 2, and3 around an ini-
tial point θ0 ∈ RN with respect to a reference Jacobian J ∈ Rn×N and with parameters α, β, I0 , I,
and R. Furthermore, assume the radius R is given by
2 := ∣∣j* ro∣∣2 + α2 (IO+I)(I+2ηTβ2)ι∣rok2,	(23)
with T a constant obeying 1 ≤ T ≤ 2^, and Jt the Pseudo-inverse of J. We run gradient descent
with stepsize η ≤ e2 on the linear and non-linear least squares problem, starting from the same
initialization θ0. Then, for all τ ≤ T the iterates of the original and the linearized problems and
their corresponding residuals obey
krτ - eτk2 ≤ 2α (10 + I) kr0k2	(24)
∣∣θτ- θτ∣L ≤ α2 (|o+I) (1 + 2ητβ2) uro k2∙	(25)
Moreover, for all iterates τ ≤ T,
kθτ - θ0k2 ≤ 2.	(26)
19
Published as a conference paper at ICLR 2020
The above theorem formalizes that in a (small) radius around the initialization, the non-linear prob-
lem behaves similarly as its linearization. Thus, to characterize the dynamics of the nonlinear prob-
lem, it suffices to characterize the dynamics of the linearized problem. This is the subject of our
next theorem.
Theorem 5. Consider a linear least squares problem (15) and let J = WΣVT ∈ Rn×N =
in=1 σiwiviT be the singular value decomposition of the matrix J. Then the residual erτ after τ
iterations of gradient descent with updates (16) is
n
£(1 - ησ2)τ Wi hwi, roi
i=1
Moreover, using a step size satisfying η ≤ σ12, the linearized iterates (16) obey
1 - (1 - ησi2 )τ 2
σi
(27)
(28)
In the next section we combine these two general theorems to provide guarantees for denoising using
general neural networks.
E.1 Proof of Theorem 4 (closeness of linear and non-linear least-squares)
The proof is by induction. We suppose the statement, in particular the bounds (24), (25), and (26)
hold for iterations t ≤ τ - 1. We then show that they continue to hold for iteration τ in four steps.
In Step I, we show that a weaker version of (26) holds, specifically that kθτ - θ0 k2 ≤ R. Using
this result, in Steps II and III we show that the bounds (24) and (25) hold, respectively. Finally, in
Step IV we utilize Steps I-III to complete the proof of equation (26).
Step I: Next iterate obeys θτ ∈ BR(θ0). To prove θτ ∈ BR(θ0), first note that by the triangle
inequality and the induction assumption (26) we have
kθτ-θ0k2≤kθτ-θτ-1k2+kθτ-1-θ0k2,
≤kθτ - θτ-1k2 + ^2 .
So to prove kθτ - θ0k2 ≤ R it suffices to show that kθτ - θτ-1 k2 ≤ R/2. To this aim note that
1 kθτ- θτ-1k2 = kVL(θτ-1 )k2
= JT(θτ-1)rτ-12
≤ JT(θτ-1)erτ-12+kJ(θτ-1)kkrτ-1-erτ-1k2
≤ JTerτ-12+kJ(θτ-1)-Jkkerτ-1k2+kJ(θτ-1)kkrτ-1-erτ-1k2
≤ JT erτ -1 2 + ( + 0)kerτ-1k2 + βkrτ-1 - erτ-1k2
(ii) 1	β2
≤ η∣IJtr0∣∣2 + (E + EO) kr0k2 + 2 α2 (e0 + E) kr0k2
R
≤ 2.
20
Published as a conference paper at ICLR 2020
In the above (i) follows from Assumptions 1, 2, and 3. For (ii), we bounded the last term with the
induction hypothesis (24), the middle term with kerτ -1 k2 ≤ kr0k2, and the first term with the bound
JTerτ-12= JT(I-ηJJT)τ-1r02
= Σ(I - ηΣ2)τ-1WT r02
n
X σj2 hwj, r0i2
j=1
≤ t
n 1
≤ β ∖ X -2 hwj, r0i2
j=1 σj
= β2Jtr0∣∣2
≤ ImOh
Finally, the last inequality follows by definition of R in (23) together with the fact that T ≥ 1.
Step II: Original and linearized residuals are close: In this step, we bound the deviation of the
residuals of the original and linearized problem defined as
eτ := rτ - erτ .
This step relies on the following lemma, which is a variant of (Oymak et al., 2019, Lem. 6.7).
Lemma 1 (Bound on growth of perturbations). Suppose that Assumptions 1, 2, and 3 hold and that
θτ, θτ +1 ∈ Br(Θo). Then, provided the stepsize obeys η ≤ 1∕β2, the deviation of the residuals
obeys
keτ+Ik2 ≤ ηβ (S + e) keτk2 + (1 + ηe2) ∣∣eτ∣∣2∙	(29)
By the previous step, θτ, θτ+1 ∈ BR(θ0). We next bound the two terms on the right hand side
of (29). Regarding the first term, we note that an immediate consequence of Theorem 5 is the
following bound on the residual of the linearized iterates:
keτk2 ≤ (1 - ηα2)τ ∣ro∣2.	(30)
In order to bound the second term in (29), namely, keτ k2, we used the following lemma, proven
later in Section E.1.1.
Lemma 2. Suppose that for positive scalars α, η, ρ, ξ > 0, η ≤ 1∕α2, the sequences reτ and eτ obey
e ≤ (1 — ηα2)τ P	(31)
eτ ≤ (1 + ηe2) e「一i + ηξrτ-1	(32)
Then, for all T ≤ 2n尹,we have that
eτ ≤ 2ξ≠2.	(33)
α2
With these lemmas in place we now have all the tools to prove that the original and linear residuals
are close. In particular, from Step I, we know that θτ ∈ BR(θ0) so that the assumptions of Lemma 1
are satisfied. Lemma 1 implies that the assumption (32) of Lemma 2 is satisfied with ξ = β(0 + )
and the bound (30) implies that the assumption (31) of Lemma 2 is satisfied with ρ = kr0k2. Thus,
Lemma 2 implies that for all T ≤ 2^
β
keτ∣∣2 ≤ 2^^2 (S + E)kr0∣2∙	(34)
2 α2	2
This concludes the proof of (24).
21
Published as a conference paper at ICLR 2020
Step III: Original and linearized parameters are close: First note that by the triangle inequality
and Assumptions 2 and 3 we have
kJ(θτ)-Jk ≤ kJ(θτ)-J(θ0)k +kJ(θ0)-Jk ≤0+.	(35)
The difference between the parameter of the original iterate θ and the linearized iterate θ obey
1∖∖θτ - θτL
τ-1
X VL(θt) - VLlin(θt)
t=0
2
τ-1	∖
X J T (θt)rt - JT ert ∖∖
t=0	∖2
τ-1
≤ X ∖∖(J T (θt) - JT)ert∖∖2 + ∖∖J T (θt)(rt - ert)∖∖2
t=0
τ-1
(i)
≤	(0 + )kertk2 + βketk2
t=0
(ii)
≤
τ -1
τ -1
品 + e) X (1 - ηɑ2)τ-1 ∣∣rok2 + B X ∣∣etk2
t=0	t=0
=(e° + e)1-⅛ηα-r ∣ro∣2 + βX ∣et∣2
ηα	t=0
≤ ROkr0k2 +2τβ2-(S + e)kr0k2
ηα2	α2
=η0^ (EO+E) (1+2ητβ2) ιιrok2.
Here, inequality (i) follows from (35) and an application of Assumption 1, inequality (ii) from (30),
inequality (iii) from η ≤ 1∕β2 which implies (1 一 ηɑ2) ≥ 0 and (34). This concludes the proof of
the bound (25).
Step IV: Completing the proof of (26): By the triangle inequality
∣θτ - θ0 ∣2 ≤ ∖∖θeτ - θ0 ∖∖	+ ∖∖θτ - θeτ ∖∖
≤ ∖∖j^r0∖∖2 + 0^(EO + E) (1 + 2ητβ2) Ilr0k2
(ii)
≤ R/2,
where inequality (i) follows from the bound (25), which we just proved, and the fact that, from
equation (36) in Theorem 5,
2n
θeτ-θO∖∖2=Xhwi,rOi2
2 i=1
(1-(1-ησ2)τ)2
σ2
n
≤ X hwi , rOi2 /σi2
i=1
= ∖∖jtro∖∖2∙
Finally, inequality (ii) follows from the definition of R in equation (23) along with τ ≤ T , by
assumption.
22
Published as a conference paper at ICLR 2020
E.1.1 Proof of Lemma 2
We prove the result by induction. Assume equation (33) holds true for some τ. We prove that then
it also holds true for τ + 1. By the two assumptions in the lemma,
eτ+1 - eτ ≤ η2 eτ + ηξreτ
≤ η2 eτ + ηξ(1 - ηα2 )τρ
≤ ηξ (e2αp + (I - ηα2)τP),
where (i) follows from the induction assumption (33). Summing up the difference of the errors gives
eτ τ	(et+1 - et)
飞二工 ξ
t=0
≤ τηe2α2 + ηρ X(1 - η02)t
t=0
1 - (1 - ηα2)τ
22ρ
τη6 F + ηρ
α2
ηα2
≤ ≠2 (η2τe2 + 1)
α2
≤ 2 JP,
α2
where the last inequality follows from the assumption that T ≤ ^^.
E.2 Proof of Theorem 5
The proof of identity (27) is equivalent to the proof of Proposition (1). Regarding inequality (28),
note that
τ-1	τ-1	τ-1
θeτ - e0 = -η X VLiin(M) = -η X JTe = -ηV	X ς (I- η∑2)t
t=0	t=0	t=0
hw1,r0i
Thus
i, θeτ - θe0 = -ησi hwi, r0i
1 - ησi2)t = -ησi hwi, r0i
1 - (1 - ησi2)τ
-Ξ2
ησi2
(36)
This in turn implies that
θeτ
i=1
2n
θeτ-θe0E2=X
i=1
roiI-(I-ησ2)τ)2
σi
F	Proofs for neural network generators (proof of Theorem 3)
The proof of Theorem 3 relies on the fact that, in the overparameterized regime, the non-linear least
squares problem is well approximated by an associated linearized least squares problem. Studying
the associated linear problem enables us to prove the result.
We apply Theorem 4, which ensures that the associated linear problem is a good approximation of
the non-linear least squarest problem, with the nonlinearity G(C) = ReLU(UC)v and with the
parameter given by θ = C. Recall that V is a fixed vector with half of the entries 1∕√k, and the
other half -1∕√k.
As the reference Jacobian in the associated linear problem, we choose a matrix J ∈ Rn×nk,
that is very close to the original Jacobian at initialization, J (C0), and that obeys JJT =
23
Published as a conference paper at ICLR 2020
E J (C)J T (C) . The concrete choice of J is discussed in the next paragraph. We apply The-
orem 4 with the following choices of parameters:
α = σn ⑵CU)), β = kUk ,	£0 = β (4l0gkδɪ)	, £ = 8ββ2 , ω = √k2ξ∣2.
A few comments on the intuition behind those choices: As shown below, α and β are chosen so that
Assumption 1 is satisfied. Regarding the choices of , 0, and ω, recall that the difference between
the linear and non-linear residual is bounded by 2务& + E)∣∣r0∣∣2 (by Theorem 4). We want this
error to be bounded by ξ kyk2. This bound is guaranteed by our choice of , by choosing 0 such
that it obeys 0 ≤ , and by choosing ω so that the initial residual is bounded by kr0 k2 ≤ 2kyk2 .
Note that 0 ≤ holds by assumption (10).
We next verify by applying a series of Lemmas proven in Appendix I that the conditions of Theo-
rem 4 are satisfied (specifically, Assumptions 1, 2, 3 on the Jacobians of the non-linear map and the
associated linearized map, to be bounded and sufficiently close to each other).
Choice of reference Jacobian and verification of assumption 2: We chose the reference Jaco-
bian J so that it is 0 close to the random initialization J CC0 ) (with high probability), and thus
satisfies assumption 2. Towards this goal, we require the following two lemmas.
Lemma 3 (Concentration lemma). Consider GCC) = ReLUCUC)v with v ∈ Rk and U ∈ Rn×k
and associated Jacobian J CC) (9). Let C ∈ Rn×k be generated at random with i.i.d. N C0, ω2 )
entries. Then,
JCC)JTCC)-ΣCU) ≤ kUk2
∖
holds with probability at least 1 - δ.
To see that Lemma 3 implies the condition (19), we use the following lemma.
Lemma 4 ((Oymak et al., 2019, Lem. 6.4)). Let X ∈ Rn×N, N ≥ n and let Σ be n × n psd matrix
obeying XXT - Σ ≤ 02 /4, for a scalar ≥ 0. Then there exists a matrix J ∈ Rn×N obeying
Σ = JJT such that
kJ-Xk ≤0.
In order to verify the condition (19), note that using the fact that Pk v4 = 1 by Lemma 3, with
probability at least 1 - δ, we have
J (Co)J T (Co)- ∑(U)∣∣ ≤ kUk2 ʌ/logsɪ =岛/2)2,
(37)
Combining inequality (37) with Lemma 4 (applied with X =J(Co)), it follows that condition (19)
holds for the chosen value of o, concluding the proof of Assumption 2 being satisfied. This
proof specifies the reference Jacobian J as the matrix that is o-close toJ(Co), and that exists
by Lemma 4.
Verifying Assumption 1: To verify Assumption 1, note that by definition JJT = Σ(U) thus
trivially σn (Σ(U)) ≥ α holds. Furthermore, Lemma 5 below combined with the fact that kvk2 = 1
implies that kJk ≤ β and kJ (C)k ≤ β for all C. This completes the verification of Assumption 1.
It remains to show that the Jacobian has bounded spectrum:
Lemma 5 (Spectral norm of Jacobian). Consider G(C) = ReLU(UC)v with v ∈ Rk and
U ∈ Rn×k and associated JacobianJ(C) (9), and let J be any matrix obeying JJT =
EJ(C)JT(C) , where the expectation is over a matrix C with iid N (0, ω2) entries. Then
kJ (C)k ≤ kvk2 kUk and kJk ≤ kvk2 kUk.
24
Published as a conference paper at ICLR 2020
Bound on initial residual: For what follows, we require a bound on the initial residual. To prove
this bound, we apply the following lemma.
Lemma 6 (Initial residual). Consider G(C) = ReLU(UC)V, and let C ∈ Rn×k be generated at
random with i.i.d. N(0, ω2) entries. Suppose half of the entries of V are v/ʌ/k and the other half
are -ν∕√k,forsome constant ν > 0. Then, with probability at least 1 - δ,
kG(C)k2 ≤ νωP8log(2n∕δ) |口||尸.
Now, the initial residual can be upper bounded as
3
kr0k2 ≤kyk2 + kG(C0)k2 ≤ 2kyk2,	(38)
where we used that, by Lemma 6,
kG(C0)k2 ≤ ωP8log(2n∕δ)刖尸 ≤〔切京P8log(2n∕δ) ≤ 11切卜.	(39)
Here, the second inequality follows from IlUkF ≤ √nkUk and our choice of the parameter ω, and
the last inequality follows from ξ ≤ 1/,32log(2n∕δ) and a∕β ≤ 1.
Verifying Assumption 3: Verification of the assumption requires us to control the perturbation of
the Jacobian matrix around a random initialization.
Lemma 7 (Jacobian perturbation around initialization). Let C0 be a matrix with i.i.d. N (0, ω2 )
entries. Then, for all C obeying
∣∣c - Cok ≤ ωR with R ≤ 1 √k,
the Jacobian mapping (9)associated with the generator G(C) = ReLU(UC)V obeys
kJ(C)-J(C0)k ≤ kVk∞ 2(kRe)1/3 kUk,
with probability at least 1 — ne- 1 R4/3k7/3.
In order to verify Assumption 3, first note that the radius in the theorem, defined in equation (23),
obeys
R = 2∣∣j*r0∣∣2 + α2(eo + E) (1 + 2ηTβ2) kro∣∣2
(i)	2	2
≤ (α + α^(eo+ e) (1 + 2ηTβ), kr0∣2
≤ 2	(1 + 主(1 + 2ηTβ2)∖	2ω√nξ-1ββ2	= 4 (1 +	主(1 + 2ηTβ2)∖	3√ξ-β3
α α	α2	α	α3
≤)43 (2+ξβηTβ 2) √nξ-1 α3
(≤) 3 (16 彳)3 √k
:= 3R.
Here, (i) follows from the fact that JSro∣∣ ≤ 1 ∣∣r0k2, (ii) from the bound on the initial resid-
ual (38), using that E0 ≤ E, as well as our choice of 3. Moreover (iii) follows from the definition of
E and (iv) from the lower bound on k in assumption (10).
Note that since ξ6 α2 ≤ 2 for this choice of radius R, by Lemma 7 We have, with ∣∣v∣∞ = 1∕√k,
that
1 ξ α2	ξ α2
kJ (C) — J (Co)k ≤ kvk∞ 2(kR)1/3 kUk = √2 条次(k ∙ k1/2)1/3e = ∣-r = E.
k 16 β2	8 β
25
Published as a conference paper at ICLR 2020
holds with probability at least
1	- 1 Rk7∕3
1 - ne 2 Rk
-2-i7ξ4α8k3 (i) 1 N
1 — ne	β8	≥ 1 — δ,
where in (i) we used (10) together with ξ ≤
probability by our choice of E = 8 α2.
y 8 log (2n). Therefore, Assumption 3 holds with high
Verifying the bound on number of iterations: Finally we note that the constraint on the number
of iterations, T, T ≤ ɪ, from TheOrem 4 is Satisfied Under the number of COnstraints of Theorem
3 bV ɪ — 25β2
3 by 2ηe2 = ηξ2α4 .
Concluding the proof of Theorem 3: Now that we have verified the conditions of Theorem 4 we
can apply the theorem. This allows us to conclude that
kG(Cτ) — x∣∣2 = kG(Cτ )+Z — yk2
= krτ + zk2
= kerτ + Z + rτ — erτ k2
(i)
≤ krτ + zk2 + krτ ― eτ∣∣2
(≤ii) kerτ + Zk2 + ξkyk2
(=]W (I - η∑2)τ WTro + z∣2 + 2ξky∣∣2
=) ∣∣W (I - η∑2)τ WT(G(Co) - x) - (W (I - η∑2)τ WT - i) z[ +1ξ∣∣r0k2
≤ ∣∣(I - η∑2)τ WTχ∣∣2 + ∣∣((I - η∑2)τ - i) WTz∣∣2 + kG(C0)k2 +1 ξky∣∣2
n
X((1 - ησi2)τ - 1)2 hwi, zi2 + ξkyk2.
i=1
Here, (i) follows from the triangular inequality, (ii) from Theorem 4 equation (24) combined with
our choice for E and ∣∣r0k2 ≤ 2∣∣y∣∣2, shown above. Moreover, (iii) follows from Theorem 5, (iv)
from r0 = G(C0) -y = G(C0) -x-z, (v) from the triangular inequality. Finally, for (vi) we used
the fact that x ∈ span{w1, w2, . . . , wp} combined with the fact that ∣W I - ηΣ2 τ WT∣ ≤ 1,
as well as using the bound ∣∣G(C0)∣b ≤ 2 ∣∣y∣∣2 proven in (39). This proves the final bound (12), as
desired.
(vi)
≤ (1 - ησp2)τkxk2+ t
G Proof of Proposition 1 and equation (2)
The residual of gradient descent at iteration τ is
rτ = y - Jcτ
= y -J(cτ-1 -ηJT(Jcτ-1 -y))
= (I - ηJJT)(y - Jcτ-1)
= (I -ηJJT)τ(y -Jc0)
= (I-ηJJT)τy
= (I - ηWΣ2WT)τy
where we used that c0 = 0 and the SVD J = WΣVT . Expanding y in terms of the singular
vectors wi (i.e., the columns of W), as y = Pi wi hwi, yi, and noting that (I - ηWΣ2WT)τ =
Pi(1 - ησi2)τ wiwiT we get
rτ =	(1 - ησi2)τ wi hwi, yi ,
i
as desired.
26
Published as a conference paper at ICLR 2020
Proof of equation (2): By proposition 1 and using that x and lies in the signal subspace
pn
x - Jcτ = X wi(1 - ησi2)τ hwi,xi + X wi((1 - ησi2)τ - 1) hwi,zi .
i=1	i=1
By the triangle inequality,
p
kx - Jcτ k2 ≤	wi(1 - ησi2)τ hwi,xi	+
i=1	2
n
Xwi((1 - ησi2)τ - 1) hwi,zi
i=1	2
un
≤ (1 - ησp2)τkxk2 + uX((1 - ησi2)τ - 1)2 hwi,zi2 ,
i=1
where the second inequality follows by using orthogonality of the wi and by using (1 - ησi2) ≤ 1,
from η ≤ 1∕σi2aχ. This concludes the proof of equation (2).
H	The expected Jacob ian for convolutional generators
We first prove the closed form expression of the expected Jacobian, or more precisely of the matrix
Σ(U) defined in (13). By the expression for the Jacobian in equation (9), we have that
k
J (C)J T (C) = X v2diag(σ0(Uc'))UUT diag(σ0(UcQ)
'=1
k
=X v2σ0(Uc')σ0(Uc')T Θ UUT
'=1
= σ0(UC)diag(v12,..., vk2)σ0(UC)T Θ UUT,
where Θ denotes the entrywise product of the two matrices. Then,
k
E [J (C)J T (C)] = X v' E [σ0 (Uc')σ0(Uc')τ] Θ UUT	(40)
'=1
Next, we have with (Daniely et al., 2016, Sec. 4.2) and using that the derivative of the ReLU function
is the step function,
hE ”(uc'M(uc')Tiiij=2 (1-cosτ( k⅛⅛)/n).
Using that kvk2 = 1, we get
[E [j (C)J T(C)]]j=2 (1 -cos-1 (k；;；Uji∣2) /π) hui, uji,
where ui are the rows of U. This concludes the proof of equation (13).
We next briefly comment on the singular value decomposition of a circulant matrix and explain
that the singular vectors are given by Definition 1. Recall, that U ∈ Rn×n is a circulant matrix,
implementing the convolution with a filter. Assume for simplicity that n is even. It is well known
that the discrete Fourier transform diagonalizes U, i.e.,
U = FTU F,
where F ∈ Cn×n is the DFT matrix with entries
[F]jk = ei2njk/n, j,k = 0,...,n - 1,
and U is a diagonal matrix with diagonal Fu, where U is the first column of the CircUlant matrix
U. From this, we can compute the singular value decomposition of U by using that U = Fu is
conjugate symmetric (since u is real) so that
[U]	n-k+2 = u *, k = 2,...,n∕2.
Let U = WΣVT be the singular value decomposition of U. The entries of the left singular vectors
are given by the trigonometric basis functions defined in (5), and the singular values are given by
the absolute values of U.
27
Published as a conference paper at ICLR 2020
I Proofs of Lemmas for neural network denoisers (Proofs of
auxiliary lemmas in Section F)
I.1	Proof of Lemma 7: Jacobian perturbation around initialization
The proof follows that of (Oymak & Soltanolkotabi, 2019b, Lem. 6.9).
Step 1: We start by relating the perturbation of the Jacobian to a perturbation of the activation
patterns. For any C, C0, we have that
kJ(C)-J(C0)k ≤ kvk∞kUkmjaxσ0(ujTC)-σ0(ujTC0)2.	(41)
To see this, first note that, by (9),
J(C)-J(C0) = [... vj (diag(σ0(Ucj)) - diag(σ0(Ucj))0)U...].
This in turn implies that
kJ (C) - J (C0)k2 = (J(C)-J(C0))(J(C)-J(C0))T
= (σ0(UC)-σ0(UC0))diag(v12,...,vk2)(σ0(UC)-σ0(UC0))TUUT2
≤(i) kUk2 mjax (σ0(ujT C) - σ0(ujT C0))diag(v)2
≤ kvk2∞kUk2 mjax σ0(ujT C) - σ0(ujT C0)2,
where for (i) we used that for two positive semidefinite matrices A, B, λmax (A	B) ≤
λmax(A) maxi Bii. This concludes the proof of equation (41).
Step 2: Step one implies that we only need to control σ0 (Ucj ) around a neighborhood of c0j .
Since σ0 is the step function, we need to count the number of sign flips between the matrices UC
and UC0. Let ∣v∣∏(q) be the q-th smallest entry of V in absolute value.
Lemma 8. Suppose that, for all i, and q ≤ k,
kc - C0k ≤√q
∣uTC0∣∏(q)
Iluik
Then
max ∣∣σ0(uτC)- σ0(UTCO)Il ≤ V2q
Proof. Suppose that σ0(uiTC) and σ0(uiT C0) have 2q many different entries, then the conclusion
of the statement would be violated. We show that this implies the assumption is violated as well,
proving the statement by contraction. By the contradiction hypothesis,
kC - C0k2 ≥ ∣∣uT(C - C0)∣∣2∕kUik2
≥q
∣uTC0l∏(q)
kUik2
where the last inequality follows by noting that at least 2q many entries have different signs, thus
their difference is larger than their individual magnitudes, and at least q many individual magnitudes
are lower bounded by the q-th smallest one.	□
Step 3: Next, we note that, with probability at least 1 - ne-kq2/2, the q-th smallest entry of
uiT C0 ∈ Rk obeys
IUTCln(G ≥ ɪv
kUik	— 2k
for all i = 1, . . . , n.
(42)
28
Published as a conference paper at ICLR 2020
We note that it is sufficient to prove this result for ν = 1. This follows from anti-concentration
of Gaussian random variables. Specifically, with the entries of C0 being iid N (0, 1) distributed,
the entries of g = uiTC0/kuik ∈ Rk are iid standard Gaussian random variables as well. We
show that with probability at least 1 一 e-kq /2, at most q entries are larger than ɪ. Let γδ be the
number for which P [|g'| ≤ γδ] ≤ δ, where g is a standard Gaussian random variable. Note that
γδ ≥ ,∏∕2δ ≥ δ. Define the random variable
δ'
if lg`l ≤ γδ,
otherwise.
with δ = ɪ. With E [δ'] = δ, by Hoeffding's inequality,
k
P ^X δ' ≥ m
一'=ι	一
k
P X δ' 一 E [δ'] ≥ m/2
一'=ι	一
≤ e-2k(m/2)2 = e-km2/2
(43)
Thus, with probability at least 1 一 ke-kq2/2 no more than m entries are smaller than γδ ≥ δ = ɪ.
The results now follows by taking the union bound over all i = 1, . . . , n.
We are now ready to conclude the proof of the lemma. By equation (41),
kJ (C)- J (C0)k ≤ kvk∞kUk max 忖(UT C) - σ0 (UT C0)∣∣
≤WUUk Pq
provided that
kC - C0k ≤ √qMV
2k
with probability at least 1 一 ne-kq2/2 . Setting q = (2kR)2/3 concludes the proof (note that the
assumption R ≤ 2√k ensures q ≤ k).
I.2	Proof of Lemma 5: Bounded Jacobian
By the expression of the Jacobian in equation (9),
kJ (C)k2 = ∣∣∣J (C)J (C)T ∣∣∣
= ∣∣∣σ0(UC)diag(v12,...,vk2)σ0(UC)TUUT∣∣∣2
≤(i) kUk2 mjax ∣∣σ0(UjT C)diag(v)∣∣22
≤ kvk22 kUk2 ,
where for (i) we used that for two positive semidefinite matrices A, B, λmax(A	B) ≤
λmax(A) maxi Bii. To prove the second inequality note that
∣∣JJT ∣∣ = kΣ(U)k
= kvk2 胆SN(0,I) [ReLU0(Uc)ReLU0(Uc)Ti ® (UUT)||
≤(i)kvk22∣∣UUT∣∣ .
Here, (i) follows from the fact that for two positive semidefinite matrices A, B, λmax(A	B) ≤
λmax (A) maxi Bii .
I.3	Proof of Lemma 3: Concentration lemma
We begin by defining the zero-mean random matrices
S' = V (σ0(UC')σ0(UC')τ 一 E p(UC')σ0(Ue`)[) Θ (UUT).
29
Published as a conference paper at ICLR 2020
With this notation,
k
J(C)JT(C)- ∑(U) = Xv2 (σ0(Ue`)σ0(UC')T - E "(UC')σ0(Ue`)[) Θ (UUT)
'=1
k
=X S'.
'=1
To show concentration we use the matrix Hoeffding inequality. To this aim note that the summands
are centered in the sense that E [S'] = 0. Next note that
(σ0CUC')σ0(UC')T - E "(UC')σ0(Ue`)[) Θ (UUT) W 卜①C')σ0(UC')T) Θ (UUT)
=diag σ0(Ue c')T UeUeTdiag σ0(Ue c')
WB2UeUeT
Similarly,
(σ0(Ue c')σ0(Ue c')T -E hσ0(Ue c')σ0(Ue c')T i) Θ (UeUeT)	-E hσ0(Uec')σ0(Uec')Ti Θ (UeUeT)
- B2UeUeT.
Thus,
-v'2 B2UeUeT W S' W v'2B2UeUeT .
Therefore, using A' := v'2B2UeUeT we have
S'2 W A'2 ,
and
k
σ2 := XA'2 ≤B4
'=1
To continue we will apply matrix Hoeffding inequality stated below.
Theorem 6 (Matrix Hoeffding inequality, Theorem 1.3 (Tropp, 2011)). Consider a finite sequence
S' of independent, random, self-adjoint matrices with dimension n, and let {A'} be a sequence of
fixed self-adjoint matrices. Assume that each random matrix satisfies
E[S'] = 0 and S'2 W A'2 almost surely.
Then, for all t ≥ 0,
k
P X S' ≥ t
'=1
__tL
≤ 2ne 8σ2 where
σ2 :
k
XA'2
'=1
Therefore, applying matrix Hoeffding inequality we get that
P
k	]	t2
X S'	≥ t ≤ 2ne-BlWUl4,
'=1
which concludes the proof.
I.4 Proof of Lemma 6 (bound on initial residual)
Without loss of generality we prove the result for ν = 1. First note that by the triangle inequality
kr0k2 = kσ(UC)v - yk2 ≤ kσ(UC)vk2 + kyk2.
30
Published as a conference paper at ICLR 2020
We next bound kσ(UC)vk2. Consider the i-th entry of the vector σ(UC)v ∈ Rn, given by
σ(uTC)v, and note that qj = (σ(uTCj) - σ(uTCn-j))∕∣∣Uik2 is SUb-GaUssian With parameter
2, i.e., P [|qj | ≥ t] ≤ 2e-2t2. It follows that
. k/2	]	2
P Eqj ≥ β√k ≤ 2e-三.
j=1
ThUs,
β2
P [∣σ(uTC)Vl ≥ kuik2ξβ] ≤ 2e-ɪ ,
where We Used that |vj-1 = ξ∕√k. Taking a union bound over all n entries,
P [kσ(UC)vk2 ≥ kUkF ξ2β2i ≤ 2ne-黑
Choosing β =，8 log(2n∕δ) concludes the proof.
31