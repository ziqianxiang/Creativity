Published as a conference paper at ICLR 2020
CLN2INV: Learning Loop Invariants with
Continuous Logic Networks
Gabriel Ryant*, Justin Wongt *, Jianan Yaot *, Ronghui Gu甘，Suman Janat
^Department of Computer Science, Columbia University, New York, NY, USA
{gabe,jianan,rgu,suman}@cs.columbia.edu
{justin.wong}@columbia.edu
QertiK, New York, NY, USA
{ronghui.gu}@certik.org
Ab stract
Program verification offers a framework for ensuring program correctness and
therefore systematically eliminating different classes of bugs. Inferring loop in-
variants is one of the main challenges behind automated verification of real-world
programs, which often contain many loops. In this paper, we present the Continu-
ous Logic Network (CLN), a novel neural architecture for automatically learning
loop invariants directly from program execution traces. Unlike existing neural net-
works, CLNs can learn precise and explicit representations of formulas in Satisfia-
bility Modulo Theories (SMT) for loop invariants from program execution traces.
We develop a new sound and complete semantic mapping for assigning SMT for-
mulas to continuous truth values that allows CLNs to be trained efficiently. We use
CLNs to implement a new inference system for loop invariants, CLN2INV, that
significantly outperforms existing approaches on the popular Code2Inv dataset.
CLN2INV is the first tool to solve all 124 theoretically solvable problems in the
Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each
problem, which is 40× faster than existing approaches. We further demonstrate
that CLN2INV can even learn 12 significantly more complex loop invariants than
the ones required for the Code2Inv dataset.
1	Introduction
Program verification offers a principled approach for systematically eliminating different classes
of bugs and proving the correctness of programs. However, as programs have become increas-
ingly complex, real-world program verification often requires prohibitively expensive manual ef-
fort (Wilcox et al., 2015; Gu et al., 2016; Chajed et al., 2019). Recent efforts have focused on
automating the program verification process, but automated verification of general programs with
unbounded loops remains an open problem (Nelson et al., 2017; 2019).
Verifying programs with loops requires determining loop invariants, which captures the effect of
the loop on the program state irrespective of the actual number of loop iterations. Automatically
inferring correct loop invariants is a challenging problem that is undecidable in general and difficult
to solve in practice (Blass & Gurevich, 2001; Furia et al., 2014). Existing approaches use stochastic
search (Sharma & Aiken, 2016), heurstics-based search (Galeotti et al., 2015), PAC learning based
on counterexamples (Padhi & Millstein, 2017), or reinforcement learning (Si et al., 2018). However,
these approaches often struggle to learn complex, real-world loop invariants.
In this paper, we introduce a novel approach to learning loop invariants by modeling the loop be-
havior from program execution traces with a new type of neural architecture. We note that inferring
loop invariants can be posed as learning formulas in Satisfiability Modulo Theories (SMT) (Biere
et al., 2009) over program variables collected from program execution traces (Nguyen et al., 2017).
In principle, neural networks are well suited to this task because they can act as universal function
* Co-student leads listed in alphabetical order; each contributed equally.
1
Published as a conference paper at ICLR 2020
approximators and have been successfully applied in various domains that require modeling of ar-
bitrary functions (Hornik et al., 1989; Goodfellow et al., 2016). However, loop invariants must be
represented as explicit SMT formulas to be usable for program verification. Unfortunately, existing
methods for extracting logical rules from general neural architectures lack sufficient precision (Au-
gasta & Kathirvalavakumar, 2012), while inductive logic learning lacks sufficient expressiveness for
use in verification (Evans & Grefenstette, 2018).
We address this issue by developing a novel neural architecture, Continuous Logic Network (CLN),
which is able to efficiently learn explicit and precise representations of SMT formulas by using
continuous truth values. Unlike existing neural architectures, CLNs can represent a learned SMT
formula explicitly in its structure and thus allow us to precisely extract the exact formula from a
trained model.
In order to train CLNs, we introduce a new semantic mapping for SMT formulas to continuous truth
values. Our semantic mapping builds on BL, or basic fuzzy logic (Hajek, 2013), to support general
SMT formulas in a continuous logic setting. We further prove that our semantic model is sound (i.e.,
truth assignments for the formulas are consistent with their discrete counterparts) and complete (i.e.,
all formulas can be represented) with regard to the discrete SMT formula space. These properties
allow CLNs to represent any quantifier-free SMT formula operating on mixed integer-real arithmetic
as an end-to-end differentiable series of operations.
We use CLNs to implement anew inference system for loop invariants, CLN2INV, that significantly
outperforms state-of-the-art tools on the Code2Inv dataset by solving all 124 theoretically solvable
problems in the dataset. This is 20 problems more than LoopInvGen, the winner of the SyGus 2018
competition loop invariant track (Padhi & Millstein, 2017). Moreover, CLN2INV finds invariants
for each program in 1.1 second on average, more than 40 times faster than LoopInvGen. We also
demonstrate that CLN2INV is able to learn complex, real-world loop invariants with combinations
of conjunctions and disjunctions of multivariable constraints. Our source code and benchmarks are
publicly available on Github1.
Our main contributions are:
•	We introduce anew semantic mapping for assigning continuous truth values to SMT formu-
las that is theoretically grounded and enables learning formulas through backpropagation.
We further prove that our semantic model is sound and complete.
•	We develop a novel neural architecture, Continuous Logic Networks (CLNs), that to the
best of our knowledge is the first to efficiently learn precise and explicit SMT formulas by
construction.
•	We use CLNs to implement a new loop invariant inference system, CLN2INV, that is the
first to solve all 124 theoretically solvable problems in the Code2Inv dataset, 20 more than
the existing methods. CLN2INV is able to find invariants for each problem in 1.1 second
on average, 40× faster than existing systems.
•	We further show that CLN2INV is able to learn 12 more complex loop invariants than the
ones present in the Code2Inv dataset with combinations of multivariable constraints.
Related Work. Traditionally, loop invariant learning relies on stochastic or heuristics-guided search
(Sharma & Aiken, 2016; Galeotti et al., 2015). Other approaches like NumInv analyze traces and
discover conjunctions of equalities by solving a system of linear equations (Sharma et al., 2013;
Nguyen et al., 2017). LoopInvGen uses PAC learning of CNF using counterexamples (Padhi et al.,
2016; Padhi & Millstein, 2017). By contrast, Code2Inv learns to guess loop invariants using re-
inforcement learning with recurrent and graph neural networks (Si et al., 2018). However, these
approaches struggle to learn complex invariants. Unlike these works, CLN2INV efficiently learns
complex invariants directly from execution traces.
There is extensive work on PAC learning of boolean formulas, but learning precise formulas requires
a prohibitively large number of samples (Kearns et al., 1994). Several recent works use differentiable
logic to learn boolean logic formulas from noisy data (Kimmig et al., 2012; Evans & Grefenstette,
2018; Payani & Fekri, 2019) or improving adversarial robustness by applying logical rules to training
1https://github.com/gryan11/cln2inv
2
Published as a conference paper at ICLR 2020
(Fischer et al., 2019). By contrast, our work learns precise SMT formulas directly by construction,
allowing us to learn richer predicates with compact representation in a noiseless setting.
A variety of numerical relaxations have been applied to SAT and SMT solving. Application-specific
approximations using methods such as interval overapproximation and slack variables have been
developed for different classes of SMT (Eggers et al., 2008; Nuzzo et al., 2010). More recent work
has applied recurrent and graph neural networks to Circuit SAT problems and unsat core detection
(Amizadeh et al., 2019; Selsam et al., 2019; Selsam & Bj0rner, 2019). FastSMT uses embeddings
from natural language processing like skip-gram and bag-of-words to represent formulas for search
strategy optimization (Balunovic et al., 2018). Unlike these approaches, we relax the SMT semantics
directly to generate a differentiable representation of SMT.
2	Background
In this section, we introduce the problem of inferring loop invariants and provide a brief overview
of Satisfiability Modulo Theories (SMT), which are used to represent loop invariants. We provide
background into fuzzy logic, which we extend with our new continuous semantic mapping for SMT.
2.1	Loop Invariants
Loop invariants capture loop behavior irrespective of number of iterations, which is crucial for
verifying programs with loops. Given a loop, while(LC){C}, a precondition P, and a post-
condition Q, the verification task involves finding a loop invariant I that can be concluded from the
precondition and implies the post-condition (Hoare, 1969). Formally, it must satisfy the following
three conditions, in which the second is a Hoare triple describing the loop:
P =⇒ I
{I ∧ LC} C {I}
LC ∧ I =⇒ Q
Example of Loop Invariant. Consider the example loop in Fig.1. For a loop invariant to be usable,
it must be valid for the precondition (t = 10 ∧ u = 0), the inductive step when t 6= 0, and the
post-condition (u = 20) when the loop condition is no longer satisfied, i.e., t = 0. The correct and
precise invariant I for the program is (2t + u = 20).
//pre: t=10 /∖ u=0
while (t != 0){
t = t - 1;
u = u + 2;
}
//post: u=20
(a) Example loop
The desired loop invariant I for the left program is a boolean
function over program variables t, u such that:
t = 10 ∧ u = 0	=⇒	I(t, u)	(pre)
∀t u,	I(t, u) ∧ (t 6= 0)	=⇒	I(t - 1,	u + 2)	(inv)
I(t, u) ∧ (t = 0)	=⇒	u = 20	(post)
(b) The desired and precise loop invariant I is (2t + u = 20).
Figure 1: Example Loop Invariant inference problem.
2.2	Satisfiability Modulo Theories
Satisfiability Modulo Theories (SMT) are an extension of Boolean Satisfiability that allow solvers
to reason about complex problems efficiently. Loop invariants and other formulas in program veri-
fication are usually encoded with quantifier-free SMT. A formula F in quantifier-free SMT can be
inductively defined as below:
F := E1 ./ E2 | F | F1 ∧ F2 | F1 ∨ F2	./∈ {=, 6=, <, >, ≤, ≥}
where E1 and E2 are expressions of terms. The loop invariant (2t + u = 20) in Fig. 1 is an SMT
formula. Nonlinear arithmetic theories admit higher-order terms such as t2 and t * u, allowing them
to express more complex constraints. For example, (-(2 ≥ t2)) is an SMT formula that is true when
the value of the high-order term t2 is larger than 2.
3
Published as a conference paper at ICLR 2020
2.3	Basic Fuzzy Logic (BL)
Basic fuzzy logic (BL) is a class of logic that uses continuous truth values in the range [0, 1] and
is differentiable almost everywhere2 (Hajek, 2013). BL defines logical conjunction with functions
called t-norms, which must satisfy specific conditions to ensure that the behavior of the logic is
consistent with boolean First Order Logic. Formally, a t-norm (denoted 0) in BL is a binary operator
over truth values in the interval [0, 1] satisfying the following conditions:
1.	associativity and commutativity: the order in which a set of t-norms on continuous truth
values are evaluated should not change the result: x0(y0z) = (x0y)0z and x0y = y0x.
2.	monotonicity: increasing any input value to a t-norm operation should not cause the result
to decrease: x1 ≤ x2 =⇒ x1 0 y ≤ x2 0 y
3.	consistency: the result of any t-norm applied to a truth value and 1 should be 1, and the
result of any truth value and 0 should be 0: 1 0 x = x and 0 0 x = 0
Besides these conditions, BL also requires that t-norms be continuous. Given a t-norm 0, its asso-
ciated t-conorm (denoted ㊉)is defined with DeMorgan's law: t ㊉ U，—(—t 0 -u), which can be
considered as logical disjunction. A common t-norm is the product t-norm X 0 y = X ∙ y with its
associated t-conorm X ㊉ y = X + y 一 X ∙ y.
3	Continuous Satisfiability Modulo Theories
We introduce a continuous semantic mapping, S, for SMT on BL that is end-to-end differentiable.
The mapping S associates SMT formulas with continuous truth values while preserving each for-
mula’s semantics. In this paper, we only consider quantifier-free formulas. This process is analogous
to constructing t-norms for BL, where a t-norm operates on continuous logical inputs.
We define three desirable properties for continuous semantic mapping S that will preserve formula
semantics while facilitating parameter training with gradient descent:
1.	S(F) should be consistent with BL. For any two formulas F and F0, where F (X) is sat-
isfied and F0(X) is unsatisfied with an assignment X of formula terms, we should have
S(F0)(X) < S (F)(X). This will ensure the semantics of SMT formulas are preserved.
2.	S(F) should be differentiable almost everywhere. This will facilitate training with gradient
descent through backpropogation.
3.	S(F) should be increasing everywhere as the terms in the formula approach constraint
satisfaction, and decreasing everywhere as the terms in the formula approach constraint
violation. This ensures there is always a nonzero gradient for training.
Continuous semantic mapping. We first define the mapping for “>” (greater-than) and "≥”
(greater-than-or-equal-to) as well as adopting definitions for “-"，“八”，and "V" from BL. All other
operators can be derived from these. For example, "≤" (less-than-or-equal-to) is derived using "≥”
and "-“，while "=" (equality) is then defined as the conjunction of formulas using "≤" and "≥."
Given constants B > 0 and e > 0, we first define the the mapping S on ">" and "≥" using shifted
and scaled sigmoid functions:
S(t > U) , 1 + e-B(t-u-e)	S(t ≥ U) , 1 + e-B(t-u+e)
We illustrate these functions in Figure 2. The validity of our semantic mapping lie in the following
facts, which can be proven with basic algebra.
lim
→0+
B∙e→+∞
1
1 + e—B(t—u—C)
1 t>U
0 t≤U
lim
C→0+
B∙e→十∞
1
1 + e—B(t—u+∈)
1 t≥U
0 t<U
2Almost everywhere indicates the function is differentiable everywhere except for a set of measure 0. For
example, a Rectified Linear Unit is differentiable almost everywhere except at zero.
4
Published as a conference paper at ICLR 2020
(a) Plot of S(x ≥ 0), S(x > 0) with sigmoid
(b) Plot of S(x = 0) with product t-norm
Figure 2: Illustration of the mapping S on >, ≥, = when B = 20 and = 0.2
When e goes to zero and B * e goes to infinity, our continuous mapping of “>" and "≥" will preserve
their original semantics. Under these conditions, our mapping satisfies all three desirable properties.
In practice, for small e and large B, the properties are also satisfied if |t - u| > e.
Next we define the mapping S for boolean operators “八”，“V" and “-" using BL. Given a specific
t-norm 0 and its corresponding t-conorm ㊉，it is straightforward to define mappings of “八”，“V”
and “”:
S (Fι ∧ F2)，S (FI)	0 S (F2)	S (Fι	V F⅛)，S (FI)㊉	S (F⅛)	S (-F)，1 —	S (F)
Based on the above definitions, the mapping for other operators can be derived as follows:
S(t < u) = S(-(t ≥ U))
1
1 + eB(t-u+e)
S(t ≤ U)= S(-(t > U))
1
1 + eB(t-u-e)
S(t = U)= S((t ≥ u) ∧ (t ≤ U)) = 1 + e-B(t-u+e) 0 1 + eB1t-u-e)
Figure 2The mapping S on “=” is valid since the following limit holds (see Appendix A for the
proof).
lim S(t = u) = lim -------------0-----ʌ 0--------ɪ------r = < 1 t = U
e→0+	e→0+ 1 + e-B(t-u+O 1 + eB(t-u-0 ɪ 0 t = U
B ∙e→+∞	B∙e→+∞
The mapping for other operators shares similar behavior in the limit, and also fulfill our desired
properties under the same conditions.
Using our semantic mapping S, most of the standard operations of integer and real arithmetic,
including addition, subtraction, multiplication, division, and exponentiation, can be used normally
and mapped to continuous truth values while keeping the entire formula differentiable. Moreover,
any expression in SMT that has an integer or real-valued result can be mapped to continuous logical
values via these formulas, although end-to-end differentiability may not be maintained in cases
where specific operations are nondifferentiable.
4	Continuous Logic Networks
In this section, we describe the construction of Continuous Logic Networks (CLNs) based on our
continuous semantic mapping for SMT on BL.
CLN Construction. CLNs use our semantic mapping to provide a general neural architecture for
learning SMT formulas. In a CLN, the learnable coefficients and smoothing parameters correspond
to the learnable parameters in a standard feedforward network, and the continuous predicates, t-
norms, and t-conorms operate as activation functions like ReLUs in a standard network. In this
5
Published as a conference paper at ICLR 2020
paper, we focus on shallow networks to address the loop invariant inference problem, but we envision
deeper general purpose CLNs that can learn arbitrary SMT formulas. When constructing a CLN,
we work from an SMT Formula Template, in which every value is marked as either an input term, a
constant, or a learnable parameter. Given an SMT Formula Template, we dynamically construct a
CLN as a computational graph. Figure 3 shows a simple formula template and the constructed CLN.
We denote the CLN model constructed from the formula template S(F) as MF .
CLN Training. Once the CLN has been constructed based on a formula template, it is trained
with the following optimization. Given a CLN model M constructed from an SMT template with
learnable parameters W, and a set X of valid assignments for the terms in the SMT template, the
expected value of the CLN is maximized by minimizing a loss function L that penalizes model
outputs that are less than one. A minimum scaling factor β is selected, and a hinge loss is applied to
the scaling factors (B) to force the differentiable predicates to approach sharp cutoffs. The offset
is also regularized to ensure precision. The overall optimization is formulated as:
max E[M(X； W,B,c)]= min X L(M(x; W,B,c))+ λ X Lhinge(β, B) + Y∣∣d∣2
{W,B,}	{W,B,} x∈X	B∈B
where λ and γ are hyperparameters respectively governing the weight assigned to the scaling factor
and offset regularization. Lhinge (β, B) is defined as max(0, β - B), and L is any loss function
strictly decreasing in domain [0, 1].
Given a CLN that has been trained to a loss approaching 0 on a given set of valid assignments, we
show that the resulting continuous SMT formula learned by the CLN is consistent with an equivalent
discrete SMT formula. In particular, we prove that such a formula is sound, (i.e., a CLN will learn a
correct SMT formula with respect to the training data), and that our continuous mapping is complete,
(i.e., CLNs can represent any SMT formula that can be represented in discrete logic), where these
properties are defined for CLNs as follows:
Soundness. Given the SMT formula F, the CLN model MF constructed from S(F ) always
preserves the truth value of F . It indicates that given a valid assignment to the terms x in F,
F(X)= True ^⇒ MF(x) = 1 and F(X)= False ^⇒ MF(x) = 0.
Completeness. For any SMT formula F, a CLN model M can be constructed representing that
formula. In other words, CLNs can express all SMT formulas on integers and reals.
We further prove that CLNs are guaranteed to converge to a globally optimal solution for formulas,
which can be expressed as the conjunction of linear equalities. We provide formal definitions and
proofs for soundness and completeness in Appendix B and optimality in Appendix C.
5	Loop Invariant Learning
We use CLNs to implement a new inference system for loop invariants, CLN2INV, which learns
invariants directly from execution traces. CLN2INV follows the same overall process as other loop
invariant inference systems such as LooPInvGen and Code2Inv - it iterates through likely candidate
invariants and checks its validity with an SMT solver. The key difference between our method and
other systems is that it learns a looP invariant formula directly from trace data. Figure 3 Provides an
overview of the architecture.
Preprocessing. We first Perform static analysis and instrument the Program to PrePare for train-
ing data generation. In addition to collecting the given Precondition and Post-condition, the static
analysis extracts all constants in the Program, along with the looP termination condition. We then
instrument the Program to record all Program variables before each looP execution and after the looP
termination. We also restrict the looP to terminate after a set number of iterations to Prevent looPs
running indefinitely (for exPeriments in this PaPer, we set the max looP iterations to 50). We also
strengthen the Precondition to ensure looP execution (see APPendix D).
Training Data Generation. We generate training data by running the Program rePeatedly on a set
of randomly initialized inPuts that satisfy the Preconditions. Unconstrained variables are initialized
from a uniform distribution centered on 0 with width r, where r is a hyPerParameter of the samPling
Process. Variables with either uPPer or lower bound Precondition constraints are initialized from
a uniform distribution adjacent to their constraints with width r, while variables with both uPPer
6
Published as a conference paper at ICLR 2020
Training Data and Template Generation	CLN Construction and Training	Invariant Checking
Figure 3: System architecture and CLN construction from SMT templates.
and lower bounds in the precondition are sampled uniformly within their bounds. For all of our
experiments in this paper, we set r to 10. When the number of uninitialized variables is small (i.e.,
less than 3), we perform this sampling exhaustively. An example of training data generation is
provided in Appendix E.
Template Generation. We generate templates in three stages with increasing expressiveness:
1.	We first generate templates directly from the pre-condition and post-condition.
2.	We next extract the individual clauses from the pre- and post-condition as well as the loop
condition, and generate templates from conjunctions and disjunctions of each possible pair
of clauses.
3.	We finally generate more generic templates of increasing complexity with a combination of
one or more equality constraints on all variables combined with conjunctions of inequality
constraints, which are based on the loop condition and individual variables.
We describe the template generation in detail in Appendix E. To detect when higher order terms may
be present in the invariant, we perform a log-log linear regression on each variable relative to the
loop iteration, similarly to Sharma et al. (2013). If the loop contains one or more variables that grow
superlinearly relative to the loop iteration, we add higher order polynomial terms to the equality
constraints in the template, up to the highest degree detected among the loop variables.
CLN Construction and Training. Once a template formula has been generated, a CLN is con-
structed from the template using the formulation in §4. As an optimization, we represent equality
constraints as Gaussian-like functions that retain a global maximum when the constraint is satisfied
as discussed in Appendix F. We then train the model using the collected execution traces.
Invariant Checking. Invariant checking is performed using SMT solvers such as Z3 (De Moura &
Bj0rner, 2008). After the CLN for a formula template has been trained, the SMT formula for the
loop invariant is recovered by normalizing the learned parameters. The invariant is checked against
the pre, post, and inductive conditions as described in §2.1. If the correct invariant is not found, we
return to the template generation phase to continue the search with a more expressive template.
6	Experiments
We compare the performance of CLN2INV with two existing methods and demonstrate the efficacy
of the method on several more difficult problems. Finally, we conduct two ablation studies to justify
our design choices.
Test Environment. All experiments are performed on an Ubuntu 18.04 server with an Intel Xeon
E5-2623 v4 2.60GHz CPU, 256Gb of memory, and an Nvidia GTX 1080Ti GPU.
System Configuration. We implement CLNs in PyTorch and use the Adam optimizer for training
with learning rate 0.01 (Paszke et al., 2017; Kingma & Ba, 2014). Because the performance of CLN
is dependent on weight initialization, the CLN training randomly restart if the model does not reach
termination within 2, 000 epochs. Learnable parameters are initialized from a uniform distribution
in the range [-1, 1], which we found works well in practice.
7
Published as a conference paper at ICLR 2020
Test Dataset. We use the same benchmark used in the evaluation of Code2Inv. We have removed
nine invalid programs from Code2Inv’s benchmark and test on the remaining 124. The removed pro-
grams are invalid because there are inputs which satisfy the precondition but result in a violation of
the post-condition. See Appendix G for details on the removed problems. The benchmark consists
of loops expressed as C code and corresponding SMT files. Each loop can have nested if-then-else
blocks (without nested loops). Programs in the benchmark may also have uninterpreted functions
with boolean return values (emulating external function calls) in branches or loop termination con-
ditions.
6.1	Comparison to existing solvers
(-,uUJcunα
O 20	40	60	80 IOO 120
Number of Instances Solved
(a) Runtime performance.
——CLN2INV
Code2lnv
—LooplnvGen
0,¾>0i°,
Illl
S=emN JO-JaqEnN
IO0
0	20	40	60	80	100	120
Number of Instances Solved
(b) SMT solver calls.
Figure 4:	Performance evaluation.
Performance Comparison. We compare CLN2INV to two state-of-the-art methods: Code2Inv
(based on neural code representation and reinforcement learning) and LoopInvGen (PAC learning
over synthesized CNF formulas) (Si et al., 2018; Padhi & Millstein, 2017). We limit each method to
one hour per problem in the same format as the SyGuS Competition (Alur et al., 2019). CLN2INV is
able to solve all 124 problems in the benchmark. LoopInvGen solves 104 problems while Code2inv
solves 90.3
Figure 4a shows the measured runtime on each evaluated system. CLN2INV solves problems in
1.1 second on average, which is over 40× faster than LoopInvGen, the second fastest system in the
evaluation. It spends the most time on solver calls (0.6s avg.) and CLN training (0.5s avg.), with
negligible time spent on preprocessing, data generation, and template generation on each problem (
less than 20ms ea.)4.
In Table 1 we provide a more detailed analysis of how much time is spent on each stage of the
invariant inference pipeline in CLN2INV. Measurements are averaged over 5 runs. The system
spends most time on solver calls (0.6s avg.) and CLN training (0.5s on avg.) with negligible time
spent on preprocessing, sampling, and template generation. For most problems in the Code2Inv
benchmark, CLN2INV completes CLN training quickly (less than 0.2s) and spends most of its time
performing solver checks, but it requires more epochs to train on some complex problems with many
variables.
Table 1: Time spent on each stage of CLN2INV Pipeline.
	Preprocessing	Sampling	Template Gen.	CLN Training	Checking
Avg. Time	5ms	2ms	4ms	0.5s	0.6s
Max Time	18ms	12ms	8ms	7.2s	5.3s
In general, CLN2INV has similar performance to LoopInvGen on simple problems but is able to
scale efficiently to complex problems. Figure 4b shows the number ofZ3 calls made by each method.
3The Code2Inv authors originally reported solving 92 problems using the same one hour timeout. We
believe that the difference may be caused by changes in the testing environment or randomized model initial-
ization.
4At most 6171 data points are generated for a given program in the benchmark, and 1041 data points per
program on average, using the sampling strategy in §5.
8
Published as a conference paper at ICLR 2020
Table 2: Results and summary statistics for performance evaluation.
Method	Number Solved	Avg Time (S)	Avg Z3 Calls	Time/Z3 Call (s)
Code2Inv	90	266.71	16.62	50.89
LoopInvGen	104	45.11	3,605.43	0.08
CLN2INV	124	1.07	31.77	0.17
For almost all problems, CLN2INV requires fewer Z3 calls than the other systems, although for
some difficult problems it uses more Z3 calls than Code2Inv.
Table 2 summarizes results of the performance evaluation. Code2Inv require much more time on
average per problem, but minimizes the number of calls made to an SMT solver. In contrast, Loop-
InvGen is efficient at generating a large volume of guessed candidate invariants, but is much less ac-
curate for each individual invariant. CLN2INV can be seen as balance between the two approaches:
it searches over candidate invariants more quickly than Code2Inv, but generates more accurate in-
variants than LoopInvGen, resulting in lower overall runtime.
6.2 More Difficult Loop Invariants
We consider two classes of more difficult loop invariant inference problems that are not present in
the Code2Inv dataset. The first require conjunctions and disjunctions of multivariable constraints,
and the second require polynomials with many higher order terms. Both of these classes of problems
are significantly more challenging because they are more complex and cause the space of possible
invariants to grow much more quickly.
To evaluate on problems that require invariants with conjunctions and disjunctions of multivariable
constraints, we construct 12 additional problems. We specifically design these problems to contain
loops with invariants that cannot be easily inferred with pre- and post-condition based heuristics. In
this section Figure 5, we show one of these problems. By plotting the trace as shown in Figure 5b,
it is easy to see that the points lie on one of the two lines expressible as linear equality constraints.
The correct loop invariant for the program is ((t + u = 0) ∨ (t - u = 0)) ∧ (u ≤ 0).
//pre: t=-20∕∖u=-20
while (U != 0) {
u++;
if (t > 0)
t = -t + 1;
else
t = -t - 1;
}
//post: t=0
20
0
-20
-20 -15 -10 -5	0
(b) Plotted trace of program
(a) Pseudocode for Problem 1
Figure 5:	Finding the loop invariant for Problem 1, which involves a disjunction of equalities
Another problem with multivariable conjunction is shown in Figure 6. In this program, we use
unknown to denote an external function call that returns either true or false. As we cannot assume
much about the function, we model the function call as sampling from a Bernoulli distribution with
success probability 0.5. Although the branching behavior is may not be deterministic, we know
(t + u = 0) ∧ (v + w = 0) ∧ (u + w ≥ 0) is a correct invariant, as it holds regardless of which
branch is taken. Our CLN2INV can learn this invariant within 20 seconds, while both Code2inv and
LoopInvGen time out after one hour without finding a solution.
To evaluate on problems with higher order polynomial invariants, we test CLN2INV on the power
summation problems in the form u = Ptk=0 td for a given degree d, which have been used in
evaluation for polyonomial loop invariant inference (Sharma et al., 2013; Nguyen et al., 2017). We
discuss these problems in more detail in Appendix H. CLN2INV can correctly learn the invariant
for 1st and 2nd order power summations, but cannot learn correct invariants for 3rd, 4th or 5th order
9
Published as a conference paper at ICLR 2020
//pre: t=-10 /∖ u=10 /∖ v=-10 /∖ w=10
while (u + w > 0) {
if (unknown()) {
t++； u--；
} else {
v++; W——;
}
}
//post: t=w /∖ u=v
Figure 6:	Pseudocode for Problem 2, which involves a conjunction of equalities
summations, which have many more higher order terms. We do not evaluate the other methods on
these problems because they are not configured for nonlinear arithmetic by default.
6.3 Ablation Studies
Effect of CLN Training on Performance. CLN2INV relies on a combination of heuristics using
static analysis and learning formulas from execution traces to correctly infer loop invariants. In
this ablation we disable model training and limit CLN2INV to static models with no learnable pa-
rameters. Static CLN2INV solves 91 problems in the dataset. Figure 7 shows a comparison of full
CLN2INV with one limited to static models. CLN2INV’s performance with training disabled shows
that a large number of problems in the dataset are relatively simple and can be inferred from basic
heuristics. However, for more difficult problems, CLN learning is key to inferring correct invariants.
2 10
Ooo
111
(S)E4un
20	40	60	80	100	120
Number of Instances Solved
2 1
O O
1 1
eo mN JO-JaqEnN
(a) Runtime performance.	(b) SMT solver calls.
Figure 7:	Ablation study comparing static vs trained models.
7 conclusion
We develop a novel neural architecture that explicitly and precisely learns SMT formulas by con-
struction. We achieve this by introducing a new sound and complete semantic mapping for SMT
that enables learning formulas through backpropagation. We use CLNs to implement a loop invari-
ant inference system, CLN2INV, that is the first to solve all theoretically solvable problems in the
Code2Inv benchmark and takes only 1.1 second on average. We believe that the CLN architecture
will also be beneficial for other domains that require learning SMT formulas.
Acknowledgments
This work is sponsored in part by NSF grants CNS-18-42456, CNS-18-01426, CNS-16-17670,
CCF-1918400; ONR grant N00014-17-1-2010; an ARL Young Investigator (YIP) award; an NSF
CAREER award; a Google Faculty Fellowship; a Capital One Research Grant; a J.P. Morgan Fac-
ulty Award; a Columbia-IBM Center Seed Grant Award; and a Qtum Foundation Research Gift.
Any opinions, findings, conclusions, or recommendations expressed herein are those of the authors,
and do not necessarily reflect those of the US Government, ONR, ARL, NSF, Google, Capital One
J.P. Morgan, IBM, or Qtum.
10
Published as a conference paper at ICLR 2020
References
Rajeev Alur, Dana Fisman, Saswat Padhi, Rishabh Singh, and Abhishek Udupa. Sygus-comp 2018:
Results and analysis. CoRR, abs/1904.07146, 2019. URL http://arxiv.org/abs/1904.
07146.
Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-SAT: An
unsupervised differentiable approach. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=BJxgz2R9t7.
M. Gethsiyal Augasta and T. Kathirvalavakumar. Rule extraction from neural networks a compara-
tive study. International Conference on Pattern Recognition, Informatics and Medical Engineer-
ing (PRIME-2012),pp. 404-408, 2012.
Mislav Balunovic, Pavol Bielik, and Martin Vechev. Learning to solve smt formulas. In Advances
in Neural Information Processing Systems, pp. 10317-10328, 2018.
A. Biere, H. van Maaren, and T. Walsh. Handbook of Satisfiability: Volume 185 Frontiers in Arti-
ficial Intelligence and Applications. IOS Press, Amsterdam, The Netherlands, The Netherlands,
2009. ISBN 1586039296, 9781586039295.
Andreas Blass and Yuri Gurevich. Inadequacy of computable loop invariants. ACM Transactions on
Computational Logic (TOCL), 2(1):1-11, 2001.
Tej Chajed, Joseph Tassarotti, M. Frans Kaashoek, and Nickolai Zeldovich. Verifying concurrent,
crash-safe systems with perennial. In Proceedings of the 27th Symposium on Operating Systems
Principles. ACM, 2019.
Leonardo De Moura and Nikolaj Bj0rner. Z3: An efficient Smt solver. In International conference
on Tools and Algorithms for the Construction and Analysis of Systems, pp. 337-340. Springer,
2008.
Andreas Eggers, Martin Franzle, and Christian Herde. Sat modulo ode: A direct sat approach
to hybrid systems. In International Symposium on Automated Technology for Verification and
Analysis, pp. 171-185. Springer, 2008.
Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of
Artificial Intelligence Research, 61:1-64, 2018.
Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin
Vechev. Dl2: Training and querying neural networks with logic. In International Conference
on Machine Learning, pp. 1931-1941, 2019.
Carlo A Furia, Bertrand Meyer, and Sergey Velder. Loop invariants: Analysis, classification, and
examples. ACM Computing Surveys (CSUR), 46(3):34, 2014.
Juan P Galeotti, Carlo A Furia, Eva May, Gordon Fraser, and Andreas Zeller. Inferring loop in-
variants by mutation, dynamic analysis, and static checking. IEEE Transactions on Software
Engineering, 41(10):1019-1037, 2015.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Ronghui Gu, Zhong Shao, Hao Chen, Xiongnan Newman Wu, Jieung Kim, Vilhelm Sjoberg, and
David Costanzo. Certikos: An extensible architecture for building certified concurrent os kernels.
In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI), pp. 653-
669, 2016.
Petr Hajek. Metamathematics offuzzy logic, volume 4. Springer Science & Business Media, 2013.
Charles Antony Richard Hoare. An axiomatic basis for computer programming. Communications
of the ACM, 12(10):576-580, 1969.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
11
Published as a conference paper at ICLR 2020
Michael J Kearns, Umesh Virkumar Vazirani, and Umesh Vazirani. An introduction to computational
learning theory. MIT press, 1994.
Angelika Kimmig, Stephen Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. A short
introduction to probabilistic soft logic. In Proceedings of the NIPS Workshop on Probabilistic
Programming: Foundations and Applications, pp. 1-4, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Luke Nelson, Helgi Sigurbjarnarson, Kaiyuan Zhang, Dylan Johnson, James Bornholt, Emina Tor-
lak, and Xi Wang. Hyperkernel: Push-button verification of an os kernel. In Proceedings of the
26th Symposium on Operating Systems Principles, pp. 252-269. ACM, 2017.
Luke Nelson, James Bornholt, Ronghui Gu, Andrew Baumann, Emina Torlak, and Xi Wang. Scaling
symbolic evaluation for automated verification of systems code with serval. In Symposium on
Operating Systems Principles, 2019.
ThanhVu Nguyen, Timos Antonopoulos, Andrew Ruef, and Michael Hicks. Counterexample-guided
approach to finding numerical invariants. In Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering, pp. 605-615. ACM, 2017.
Pierluigi Nuzzo, Alberto Puggelli, Sanjit A Seshia, and Alberto Sangiovanni-Vincentelli. Calcs:
Smt solving for non-linear convex constraints. In Formal Methods in Computer Aided Design,
pp. 71-79. IEEE, 2010.
Saswat Padhi and Todd D. Millstein. Data-driven loop invariant inference with automatic feature
synthesis. CoRR, abs/1707.02029, 2017. URL http://arxiv.org/abs/1707.02029.
Saswat Padhi, Rahul Sharma, and Todd Millstein. Data-driven precondition inference with learned
features. ACM SIGPLAN Notices, 51(6):42-56, 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Ali Payani and Faramarz Fekri. Inductive logic programming via differentiable deep neural logic
networks. arXiv preprint arXiv:1906.03523, 2019.
Daniel Selsam and Nikolaj Bj0rner. Guiding high-performance sat solvers with Unsat-Core pre-
dictions. In International Conference on Theory and Applications of Satisfiability Testing, pp.
336-353. Springer, 2019.
Daniel Selsam, Matthew Lamm, Benedikt BUnz, Percy Liang, Leonardo de Moura, and David L.
Dill. Learning a SAT solver from single-bit supervision. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=HJMC_iA5tm.
Rahul Sharma and Alex Aiken. From invariant checking to invariant inference using randomized
search. Formal Methods in System Design, 48(3):235-256, 2016.
Rahul Sharma, Saurabh Gupta, Bharath Hariharan, Alex Aiken, Percy Liang, and Aditya V Nori. A
data driven approach for algebraic loop invariants. In European Symposium on Programming, pp.
574-592. Springer, 2013.
Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, and Le Song. Learning loop invariants
for program verification. In Advances in Neural Information Processing Systems, pp. 7751-7762,
2018.
James R Wilcox, Doug Woos, Pavel Panchekha, Zachary Tatlock, Xi Wang, Michael D Ernst, and
Thomas Anderson. Verdi: a framework for implementing and formally verifying distributed
systems. ACM SIGPLAN Notices, 50(6):357-368, 2015.
12
Published as a conference paper at ICLR 2020
A PROOF OF LIMIT OF S(t = u)
eli→0+ S(t = U) = eli→0+ 1+ e-B(i+e)氧 1 + eB1t-u-e) = { O
B∙e→+∞	B∙e→十∞
t=U
t 6= U
Proof. Let f (t, u; B, e) = 1+e-B(t-u+«)and g(t, u; B, e) = 1+eB(t_u_). Then What We want to
prove becomes
lim+ (f (t, u； B, e)乳 g(t, U B,e)) = | O t = UU
B ∙e→十∞
Because all f, g, 0 are continuous in their domain, we have
lim (f (t, u; B, e) 0 g(t,u; B,e))
→0+
B ∙e→十∞
lim f(t, U; B, )
→0+
B∙e→十∞
0 lim g(t, U; B, )
→0+
∖B∙e→十∞
Using basic algebra, we get
lim f(t, U; B, ) =	1 t ≥ U	lim g(t, U; B, ) =	1 t ≤ U
→0+	O t < U	→0+	O t > U
B∙e→+∞	B∙e→十∞
Combing these results, we have
O01 t<U
101 t=U
10O t>U
lim f(t, U; B, )	0 lim g(t, U; B, )
→0+	→0+
B∙e→十∞	B	∖B∙e→十∞
For any t-norm, we have O 0 1 = O, 1 0 1 = 1, and 1 0 O = O. Put it altogether, we have
lim (f (t, U; B, ) 0 g(t, U; B, )) =	1O
→0+	O
B ∙e→十∞
t=U
t 6= U
which concludes the proof.
□
B	Soundness and Completeness of CLNs
In this section we provide formal descriptions and proofs of CLN soundness and completeness when
trained to 0 loss. These properties are defined for CLNs as follows:
Soundness. Given the SMT formula F, the CLN model MF constructed from S(F ) always
preserves the truth value of F . It indicates that given a valid assignment to the terms x in F,
F(X)= True ^⇒ MF(x) = 1 and F(X)= False ^⇒ MF(x) = O.
Completeness. For any SMT formula F, a CLN model M can be constructed representing that
formula. In other words, CLNs can express all SMT formulas on integers and reals.
We formally state these properties in Theorem 1. Before that we need to define a property for
t-norms.
Property 1. ∀t u, (t > O) and (u > O) implies (t 0 u > O).
The product t-norm and Godel t-norm have this property, while the Lukasiewicz t-norm does not.
Theorem 1.	For any quantifier-free linear SMT formula F, there exists CLN model M, such that
∀X B , O ≤ M(X; B, ) ≤ 1	(1)
∀x, F(X) = True ^⇒ lim M(x; B, e) = 1	(2)
→0+
B∙e-∞
∀x, F(X)= False ^⇒ lim M(x; B, e) = O	(3)
→0+
B ∙eτ∞
as long as the t-norm used in building M satisfies Property 1.
13
Published as a conference paper at ICLR 2020
Proof. For convenience of the proof, we first remove all <, ≤, = and 6= in F, by transforming
t < u into —(t ≥ u), t ≤ U into —(t > u), t = U into (t ≥ U) ∧ — (t > u), and t = U into
(t > u) ∨ —(t ≥ u). Now the only operators that F may contain are >, ≥, ∧, ∨, -. We prove
Theorem 1 by induction on the constructor of formula F. In the following proof, we construct
model M given F and show that it satisfied Eq.(1)(2). We leave the proof for why M also satisfied
Eq.(3) to readers.
Atomic Case. When F is an atomic clause, then F will be in the form of X * W + b > 0 or
X * W + b ≥ 0. For the first case, We construct a linear layer with weight W and bias b followed by
a sigmoid function scaled with factor B and right-shifted with distance . For the second case, we
construct the same linear layer followed by a sigmoid function scaled with factor B and left-shifted
with distance . Simply evaluating the limits for each we arrive at
∀x, F(X)= True ^⇒ lim M(x; B,e) = 1
→0+
B∙e→∞
And from the definition of sigmoid function we know 0 ≤ M(X; B, ) ≤ 1.
Negation Case. If F = —F0, from the induction hypothesis, F0 can be represented by models M0
satisfying Eq.(1)(2)(3). Let p0 output node of M0. We add a final output node p = 1 - p0. So
M(X; B, ) = 1 - M0(X; B, ). Using the induction hypothesis 0 ≤ M0(X; B, ) ≤ 1, we conclude
Eq.(1) 0 ≤ M(X; B, ) ≤ 1.
Now we prove the “ =⇒ ” side of Eq.(2). If F(X) = True, then F0 (X) = F alse. From the
induction hypothesis, we know lim →0+ M0(X; B, ) = 0. So
B∙e→∞
lim M(X; B, ) = lim 1 - M0(X; B, ) = 1 - 0 = 1
→0+	→0+
B∙e→∞	B∙e→∞
Next we prove the “ U= " side. If lim e→o+ M(x; B, E) = 1, we have
B∙e→∞
lim M0(X;B,) = lim 1 -M(X;B,) = 1 - 1 =0
→0+	→0+
B∙e→∞	B∙e→∞
From the induction hypothesis we know that F0(X) = F alse. So F(X) = —F0 (X) = T rue.
Conjunction Case. If F = F1 ∧ F2, from the induction hypothesis, F1 and F2 can be represented
by models M1 and M2, such that both (F1, M1) and (F2, M2) satisfy Eq.(1)(2)(3). Let p1 and p2
be the output nodes of Mi and M2. We add a final output node P = pi 0 p2. So M(x; B, E)=
Mi(x; B, E) 0 M2(x; B, e). Since (0) is continuous and so are Mi(x; B, E) and M2(x; B, e), we
know their composition M(x; B, E) is also continuous. (Readers may wonder why M 1(x; B, E)
is continuous. Actually the continuity of M(x; B, E) should be proved inductively like this proof
itself, and we omit it for brevity.) From the definition of (0), we have Eq.(1) 0 ≤ M(x; B, E) ≤ 1.
Now we prove the =⇒ side of Eq.(2). For any x, if F(x) = True which means both Fi(x) =
True and F2(x) = T rue, from the induction hypothesis we know that lim →0+ Mi(x; B, E) = 1
B ∙ e-'^∞)
and lim →0+ M2 (x; B, E) = 1. Then
B∙e→∞
lim M(x; B, E) = lim Mi (x; B, E) 0 M2(x; B, E) = 1 0 1 = 1
→0+	→0+
B∙e→∞	B∙e→∞
Then we prove the U= side. From the induction hypothesis we know that Mi (x; B, E) ≤ 1 and
M2(x; B, E) ≤ 1. From the non-decreasing property of t-norms (see §2.3), we have
Mi(x; B, E) 0 M2 (x; B, E) ≤ Mi(x; B, E) 0 1
Then from the consistency property and the commutative property, we have
Mi (x; B, E) 0 1 = Mi (x; B, E)
Put them altogether we get
M(x; B, E) ≤ Mi(x; B, E) ≤ 1
14
Published as a conference paper at ICLR 2020
Because we know lim →0+ M(x; B, ) = 1, according to the squeeze theorem in calculus, we get
B∙e→∞
lim M1 (x; B, ) = 1
→0+
B∙e→∞
From the induction hypothesis, we know that F1 (x) = T rue. We can prove F2(x) = True in the
same manner. Finally we have F(x) = F1(x) ∧ F2(x) = T rue.
Disjunction Case. For the case F = F1 ∨ F2, we construct M from M1 and M2 as we did in the
conjunctive case. This time We let the final output node be P = pi ㊉ p2. From the continuity of (0)
and the definition of (㊉)(t ㊉ U = 1 一 (1 一 t) 0 (1 一 u)),(㊉)is also continuous. We conclude
M(x; B, ) is also continuous and 0 ≤ M(x; B, ) ≤ 1 by the same argument as F = F1 ∧ F2.
NoW We prove the “ =⇒ ” side of Eq.(2). For any assignment x, if F(x) = True Which means
F1(x) = True or F2(x) = True. Without loss of generality, We assume F1(x) = True. From the
induction hypothesis, We knoW lim →0+ M1(x; B, ) = 1.
B∙e→∞
For any (㊉)and any 0 ≤ t,t0 ≤ 1, if t ≤ t0, then
t ㊉ u = 1 一 (1 一 t) 0 (1 一 U) ≤ 1 一 (1 一 t0) 0 (1 一 U)= t0 ㊉ u
Using this property and the induction hypothesis M2 (x; B, ) ≥ 0, We have
Mi(x; B, e)㊉ 0 ≤ Mi(x; B, E)㊉ M2(x; B, E) = M(x; B, E)
From the induction hypothesis we also have Mi (x; B,e) ≤ 1. Using the definition of (㊉)and the
consistency of (0) (0 0 X = 0), we get Mi(x; B, e)㊉ 0 = Mi(x; B, e). Put them altogether we get
Mi (x; B, E) ≤ M(x; B, E) ≤ 1
Because we know lim →0+ Mi(x; B, E) = 1, according to the squeeze theorem in calculus, we
B ∙ e—→^c
get lim →0+ M(x; B, E) = 1.
B∙e→∞
Then we prove the " U= ” side. Here we need to use the existence of limit:
lim M(x; B, E)
e→0+
B∙e→∞
This property can be proved by induction like this proof itself, thus omitted for brevity.
Let
ci = lim Mi (x; B, E)	c2 = lim M2 (x; B, E)
e→0+	e→0+
B∙e→∞	B∙e→∞
Then
lim	M(x; B,	e)	= lim	Mi(x; B, e)㊉	M2(x; B,	e)	=	ci	㊉	c2	= 1 — (1 — ci)	0 (1 — c>
e→0+	e→0+
B∙e→∞	B∙e→∞
Since we have lim e→0+ M(x; B, E) = 1, we get
B∙e→∞
(1 一 ci) 0 (1 一 c2) = 0
Using Property 1 of (0) (defined in §4), we have ci = 1 ∨ c2 = 1. Without loss of generality, we
assume ci = 1. From the induction hypothesis, we know that Fi(x) = TrUe. Finally, F(x) =
Fi(x) ∨ F2(x) = True.	□
Careful readers may have found that if we use the continuous mapping function S in §3, then we
have another perspective of the proof above, which can be viewed as two interwoven parts. The first
part is that we proved the following lemma.
15
Published as a conference paper at ICLR 2020
Corollary 1. For any quantifier-free linear SMT formula F,
∀x B , 0 ≤ S(F ; B, )(x) ≤ 1
∀x, F(X) = True ^⇒ lim S(F; B, e)(x) = 1
→0+
B∙e→∞
∀x, F(X)= False ^⇒ lim S(F; B, e)(x) = 0
→0+
B∙e→∞
Corollary 1 indicates the soundness of S . The second part is that we construct a CLN model given
S(F). In other words, we translate S(F) into vertices in a computational graph composed of differ-
entiable operations on continuous truth values.
C Optimality of CLNs
Optimality. For a subset of SMT formulas (conjunctions of multiple linear equalities), CLNs are
guaranteed to converge at the global minimum. We formally state this in Theorem 2. We first define
another property similar to strict monotonicity.
Property 2. ∀tι t2 t3, (tι < t2) and (t3 > 0) implies (tι 013 < t2 013).
Theorem 2.	For any CLN model MF constructed from a formula, F, by the procedure shown in the
proof of Theorem 1, if F is the conjunction of multiple linear equalities then any local minimum of
MF is the global minimum, as long as the t-norm used in building MF satisfies Property 2.
Proof. Since F is the conjunction of linear equalities, it has the form
n li
F = ^(Xwijtij = 0)
i=1 j=1
Here W = {wij } are the learnable weights, and {tij } are terms (variables). We omit the bias bi in
the linear equalities, as the bias can always be transformed into a weight by adding a constant of 1
as a term. For convenience, We define f(x) = S(X = 0) = 1+e-B(。+«)0 1+eB(x-«).
Given an assignment X of the terms {tij}, if We construct our CLN model MF folloWing the proce-
dure shoWn in the proof of Theorem 1, the output of the model Will be
n li
M(X;W,B,)=Of(Xwijtij)
i=1	j=1
When We train our CLN model, We have a collection of m data points {tij1}, {tij2}, ..., {tijm},
Which satisfy formula F. If B and are fixed (unlearnable), then the loss function Will be
m	m	n	li
L(W)=XL(M(X;W,B,))=XL(Of(Xwijtijk))	(4)
k=1	k=1	i=1 j=1
Suppose W* = {wj} is a local minima of L(W). We need to prove W* is also the global minima.
To prove this, We use the definition of a local minima. That is,
∃δ > 0, ∀W, ||W - W*|| ≤ δ =⇒ L(W) ≥ L(W*)	(5)
For convenience, We denote uik = Plji=1 wijtijk . Then We reWrite Eq.(4) as
mn
L(W) = XL(Of(uik))
If We can prove at W* , ∀ 1 ≤ i ≤ n, 1 ≤ k ≤ m, uik = 0. Then because (i) f reaches its global
maximum at 0, (ii) the t-norm (0) is monotonically increasing, (iii) L is monotonically decreasing,
We can conclude that W* is the global minima.
16
Published as a conference paper at ICLR 2020
Now we prove ∀ 1 ≤ i ≤ n, 1 ≤ k ≤ m, uik = 0. Here we just show the case i = 1. The proof for
i > 1 can be directly derived using the associativity of (0).
Let ak = Nn=2 f (Uik). Since f (x) > 0 for all X ∈ R, using Property 2 of our t-norm (0), We
know that αk > 0. Now the loss function becomes
m
L(W) = X L(f(u1k) 0 αk)
k=1
From Eq.(5), We have
mm
∃0 <δ0 < 1, ∀γ, ∣γ∣ ≤ δ0 =⇒ X L(f(uik(1 + Y)) 0 αk) ≥ X L(f (Uik) 0 αQ	(6)
k=1	k=1
Because (i) f(x) is an even function decreasing on x > 0 (Which can be easily proved), (ii) (0) is
monotonically increasing, (iii) L is monotonically decreasing, for -δ0 < γ < 0, We have
mm
X L(f(uik(1 + Y)) 0 αk) = X L(f(∣uik(1 + γ)∣) 0 αk) ≤
k=1	k=1
mm
XL(f(|Uik|) 0 αk) = X L(f(Uik) 0αk)	(7)
k=1	k=1
Combing Eq.(6) and Eq.(7), We have
mm
L(f (Uik (1 +Y)) 0 αk) =	L(f(Uik) 0αk)
k=1	k=1
NoW We look back on Eq.(7). Since (i) L is strictly decreasing, (ii) the t-norm We used here has
Property 2 (see §4 for definition), (iii) αk > 0, the only case When (=) holds is that for all 1 ≤
k ≤ m, We have f (|Uik (1 + Y)|) = f (|Uik |). Since f(x) is strictly decreasing for x ≥ 0, We have
∣Uik(1 + γ)∣ = |uik|. Finally because -1 < -δ0 < γ < 0, we have Uik = 0.	□
17
Published as a conference paper at ICLR 2020
D Precondition Strengthening
Theorem 3.	Given a program C: assume(P); while (LC) {C} assert(Q);
If we can find a loop invariant I0 for program C0: assume(P ∧ LC); while (LC) {C} assert(Q);
and P ∧ LC =⇒ Q, then I0 ∨ (P ∧ LC) is a correct loop invariant for program C.
Proof. Since I0 is a loop invariant of C0, we have
(P ∧ LC) ∧ LC =⇒ I0 (a)	{I0 ∧ LC}C{I0} (b) I0 ∧ LC =⇒ Q (c)
We want to prove I0 ∨ (P ∧ LC) is a valid loop invariant of C, which means
P ∧ LC =⇒ I0 ∨ (P ∧ LC)	{(I0 ∨ (P ∧ LC)) ∧ LC}C{I0 ∨ (P ∧ LC)}
(I0 ∨ (P ∧ LC)) ∧ LC =⇒ Q
We prove the three propositions separately. To prove P ∧ LC =⇒ I0 ∨ (P ∧ LC), we transform
it into a stronger proposition P ∧ LC =⇒ I0, which directly comes from (a).
For {(10 ∨ (P ∧-LC)) ∧ LC }C{10 ∨ (P ∧-LC)}, after simplification it becomes {10 ∧ LC }C {10 ∨
(P ∧ LC)}, which is a direct corollary of (b).
For (I0∨ (P∧ LC)) ∧ LC =⇒ Q, after simplification it will become two separate propositions,
I0 ∧ LC =⇒ Q and P ∧ LC =⇒ Q. The former is exactly (c), and the latter is a known
condition in the theorem.	□
E Implementation Details
Training Data Generation Example. Figure 8 provides an example of our training data generation
procedure. The uninitialized variable k is sampled according to the precondition k ≤ 8 within the
predefined width r = 10. So we end up enumerating k = 8, 7, ..., -2. For each k, the loop is
executed repeatedly until termination, thus generating a small set of samples. The final training set
is the union of these small sets.
//pre: t=10
//pre: u=0
//pre: k<=8
//invariant: 2t+u=20
while (t != k){
t = t - 1;
u = u + 2;
}
//post: u=20-2k
// run with k={-2..8}
k = atoi(argv[1]);
t=10; u=0;
while (t != k){
log(t, u, k)；
t = t - 1;
u = u + 2;
}
log(t, u, k);
(a)	The original loop program.
(b)	The sampling procedure.
Figure 8: Illustration of how training data is generated. After the sampling procedure in (b) we have
a collection of 88 samples which will later be fed to the CLN model.
Template Generation. Templates are first generated from the pre- and post-conditions, followed by
every pair of clauses extracted from the precondition, post-condition, and loop condition. Generic
templates are then constructed consisting of one or more general equality constraints containing all
variables conjoined with inequality constraints.
Three iterations of the generic template generation are shown here:
1	:	(WιX = aι) ∧ (xι ≤ Ui) ∧ (xι ≥ lι) ∧∙∙∙∧	(Xn	≤ Un) ∧ (Xn ≥ ln)
2	:	((WiX = ai) ∧ (W2X = a2)) ∧ (xi ≤ Ui) ∧	(xi	≥ ll) ∧∙∙∙∧ (Xn ≤	Un)	∧ (Xn	≥	In)
3	:	((WiX = ai) ∨ (W2X = a2)) ∧ (xi ≤ Ui) ∧	(xi	≥ li) ∧∙∙∙∧ (Xn ≤	Un)	∧ (Xn	≥	ln)
Algorithm 1 summarizes the template generation process.	In it the following functions are defined:
18
Published as a conference paper at ICLR 2020
construct-template:
extract-clauses:
estimate_degrees:
polynomial-kernel:
is_single_constraint:
extractJoop.constraint:
Construct a template given an smt formula.
Extract individual clauses from smt formulas.
Performs log-log linear regression to estimate degree of each variable.
Executes polynomial kernel on variables and data for a given degree.
Checks if condition is a single inequality constraint.
Converts the loop condition to learnable smt template.
Note that templates are generated on demand, so each template is used to infer a possible invariant
before the next is generated.
Algorithm 1 Template Generation Algorithm.
Input: Pre J precondition PoSt J post-condition LC J loop condition Max-Template-Len J max template eq clauses VarS J vars in program X J training data	
1:	ConstrUctjemPlate(Post)
2:	constrUctjemPlate(Pre)
3:	clauses J extract-clauses(Pre, Post, LC)
4:	for (c1,c2) in alLpairs(clauses) do
5:	construct-template(c1 ∧ c2)
6:	construct-template(c1 ∨ c2)
7:	end for
8:	max.degree jestimate_degrees(X)
9:	if max_degree > 1 then
10:	Vars J polynomial-kernel(vars, max_degree)
11:	end if
12:	bound_Constraints J ()
13:	for var in vars do
14:	bound_constraints J bound_contraints ∧ (Var ≤ Uvar)
15:	bound_Constraints J bound_contraints ∧ (var ≥ lvar)
16:	end for
17:	if is_single_constraint(LC) then
18:	bound_Constraints J bound.contraints∧extract-loop-constraint(LC)
19:	end if
20:	construct_tempIate((W ∙ Vars = b)) ∧ bound_Constraints)
21:	eq-clauses J [(W ∙ vars = b)]
22:	templateJen J 1
23:	while template Jen ≤ MaxHemplate_Len do
24:	for eq_clause in eq_clauses do
25:	construct_template((eq_clause ∧ (W ∙ vars = b)) ∧ bound-constraints)
26:	eq_clauses J [eq.clauses, eq_clause ∧ (W ∙ vars = b)]
27:	construct_template((eq_clause ∨ (W ∙ vars = b)) ∧ bound-constraints)
28:	eq_clauses J [eq.clauses, eq_clause ∨ (W ∙ vars = b)]
29:	end for
30:	templσteJen J templσteJen + 1
31:	end while
19
Published as a conference paper at ICLR 2020
F Properties of Gaussian Function
(t-u)2
We use a GaUssian-like function S(t = U) = exp(-( 逮 )to represent equalities m our exper-
iments. It has the following two properties. First, it preserves the original semantic of = when
σ → 0, similar to the mapping S(t = U)= 1+一日1—+«)X 1+二(二一)We defined in §3.
lim exp(-H) = [ 1
σ→0+ p(	2σ2 ) I 0
t=U
t 6= U
Second, if We vieW S(t = U) as a function over t - U, then it reaches its only local maximum at
t - U = 0, Which means the equality is satisfied.
G Invalid Problems from Code2Inv Dataset
Here We provide an example Which is Problem 106 in the dataset. The post-condition a ≥ m is
Wrong if We start from a = 0, m = 1, and k = 0.
int k = 0;
int a, m;
assume(a <= m);
while (k < 1) {
if (m < a) m = a;
k = k + 1;
}
assert(a >= m);
Executing the program With these inputs results in a = 0, m = 1, and k = 1 as the if condi-
tion is never satisfied. But clearly, the post condition a ≥ m is violated. BeloW We tabulate the
counterexamples invalidating the nine removed problems from the dataset:
Table 3: Invalid problems from Code2Inv dataset
Problem	Counterexample starting state	
26	x, n	= 0, 0
27	x, n	= 0, 0
31	x, n	= 0, 0
32	x, n	= 0, 0
61	c, n	= 0, 1
62	c, n	= 0, 1
72	c, y =	0, 128
75	c, y =	0, 128
106	a, m, k	=0,1,0
H Polynomial Invariants
Here We provide results and an example of the higher order polynomial problems; more precisely,
the poWer summation problems in the form U = Ptk=0 td for a given degree d. We found that
CLN2INV Was able to learn invariants for programs Which computes the sum of consecutive in-
tegers, Which has 10 monomial terms and a maximum degree of 2. Since CLN2INV simply uses
the polynomial kernel the number of terms quickly become unWieldy. We observe that CLN2INV
struggles beginning With sum of squares, Which has 20 monomial terms up to degree 3. Table 4
summarizes these results.
For further illustration, We describe in detail the case Where d is 3, i.e. sum of cubes.
20
Published as a conference paper at ICLR 2020
Table 4: Results on power summation polynomial problems.
Number Terms	Highest Degree	Solved?
5	1	✓
10	2	✓
20	3	X
35	4	X
56	5	X
//pre: t=u=0∕∖k>=0
while (t < k) {
t++;
u += t * t * t;
}
//post: 4u == k**2 * (k + 1)**2
Figure 9: Pseudocode for Polynomial Invariant Problem
The example loop in Figure 9 computes the sum of the first k cubes. We know this sum has a closed
form solution:
k
Xk3
i=0
k2(k + 1)2	k4+2k3+k2
4
4
For this problem, we would hope to extract the invariant:
(4u = t4 + 2t3 + t2) ∧ (t <= k)
However, by naively using the polynomial kernel just as methods like NumInv suggest (Nguyen
et al., 2017), we will have 35 monomials of degree at most four over three variables as candidate
terms (t3u, t2k2, tu2k, ...), and the model must learn to ignore all the terms except u, t4, t3, and t2.
We observe our model finds far more accurate coefficient for t4 but than for lower ordered terms.
We hypothesize that by nature of polynomials the highest order term is a good approximation for
the whole polynomial. Thus, u = t4 is a good approximation based on the data. The difficulty of
learning polynomial invariants using CLN is an interesting direction for future studies.
21