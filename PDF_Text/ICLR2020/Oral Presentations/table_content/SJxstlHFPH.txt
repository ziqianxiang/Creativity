Table 1: (Left) MetaQA and (Right) WikiData Hits @1 for 1-3 hop sub-tasks. ots: off-the-shelf without re-training. f: obtained from Sun et al. (2019). cascade: adapted to multi-hop setting by repeatedly applying Eq. 2.
Table 2: (Left) Retrieval performance on the HotpotQA benchmark dev set. Q/s denotes the number of queriesper second during inference on a single 16-core CPU. Accuracy @k is the fraction where both the correctpassages are retrieved in the top k. t: Baselines obtained from Das et al. (2019b). For DrKIT, We report theperformance when the index is pretrained using the WikiData KB alone, the HotpotQA training questionsalone, or using both. *: Measured on different machines with similar specs. (Right) Overall performance onthe HotpotQA task, When passing 10 retrieved passages to a doWnstream reading comprehension model (Yanget al., 2018). ^: From Das etal. (2019b). â—Š: From Qi et al. (2019). t: Results on the dev set.
Table 3: Official leaderboard evaluation on the test set of HotpotQA. #Bert refers to the number of calls toBERT (Devlin et al., 2019) in the model. s/Q denotes seconds per query (using batch size 1) for inference on asingle 16-Core CPU. Answer, SuP Fact and Joint are the official evaluation metrics for HotPotQA. *: This is theminimum number of BERT calls based on model and hyperparameter descriptions in the respective papers. ^:Computed using code released by authors, using a batch size of 1. ^: Estimated based on the number of BERTcalls, using 0.8s as the time for one call (without considering overhead due to other computation in the model).
Table 4: WikiData dataset							Details of the collected WikiData dataset are shown in Table 4.
