Table 1: Comparison of R@10, ABX scores, and bit-rates between different configurations andbaseline models trained on ZeroSpeech 2019 data or Places Audio Caption. All quantizers reflectedin this table used a codebook size of 1,024 vectors. We do not compute RLE or segment scores forthe FHVAE-DPGMM model, since we did not re-implement that model.
Table 2: ABX scores and RLE bitrates for various SNRs on the noisy ZeroSpeech19 English test set.
Table 3: Performance of the VQ3 layer from the “{3} → {2, 3}” model when codes are treated asword detectors. Codes are ranked by the highest F1 score among the retrieved words for a givencode. Word hypotheses for a given code are ranked by the F1 score. P denotes precision, R recall,and occ the number of co-occurrences of the code and word in the data.
Table 4: ABX scores and bitrates for various codebook sizes on the clean ZeroSpeech19 Englishtest set, using the “0 → {2}” model.
Table 5: ABX scores on the Ze-roSpeech 19 English test set usingfeatures derived from the output ofthe Res3 block of the ResDAVEnetaudio branch (pre-quantization).
Table 6: The number of codebook vectors at a particular VQ layer that learned to be a detector forany word with an F1 score greater than 0.5.
Table 7: Performance of the VQ3 layer from the “{3} → {2, 3}” model when codes are treated asword detectors. Codes are ranked by the highest F1 score among the retrieved words for a givencode. Word hypotheses for a given code are ranked by the F1 score.
Table 8: Performance of the VQ3 layer from the “{2} → {2, 3}” model when codes are treated asword detectors. Codes are ranked by the highest F1 score among the retrieved words for a givencode. Word hypotheses for a given code are ranked by the F1 score.
