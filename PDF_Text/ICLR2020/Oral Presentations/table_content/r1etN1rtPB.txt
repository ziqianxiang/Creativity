Table 1: List of algorithms studied in this work, with their crucial properties. Step method refers tothe method used to build each training step, PPO clipping refers to the use of clipping in the step(as in Equation (2)), and PPO optimizations refer to the optimizations listed in Section 3.
Table 2: Full ablation of step choices (PPO or TRPO) and presence of code-level optimizationsmeasuring agent performance on benchmark tasks. TRPO+ is a variant of TRPO that uses PPOinspired code-level optimizations, and PPO-M is a variant of PPO that does not use PPOâ€™s code-level optimizations (cf. Section 3). We find that varying the use of code-level optimizations impactsperformance significantly more than varying whether the PPO or TRPO step is used. We detail ourexperimental setup in Appendix A.1. We train at least 80 agents for each estimate (more for somehigh-variance cases). We present 95% confidence intervals computed via a 1000-sample bootstrap.
Table 3: Comparison of PPO performance to PPO without clipping. We find that there is littledifference between the rewards attained between the two algorithms on the benchmark tasks. Notethat both algorithms use code-level optimizations; our results indicate that the clipping mechanism isoften of comparable or lesser importance to the use of code-level optimizations. We detail our exper-imental setup in Appendix A.1. We train at least 80 agents for each estimate (for some high-variancecases, more agents were used). We present 95% confidence intervals computed via a 1000-samplebootstrap. We also present results from the OpenAI baselines (Dhariwal et al., 2017) repositorywhere available.
Table 4: Hyperparameters for all algorithms for Walker2d-v2.
Table 5: Hyperparameters for all algorithms for Humanoid-v2.
Table 6: Hyperparameters for all algorithms for Hopper-v2.
