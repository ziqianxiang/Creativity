Table 1: Trained models summary for VGG-9 trained on CIFAR-10 as shown in Figure 2Method	FedAvg	FedProx	Ensemble	FedMAFinal Accuracy(%)	86.29	85.32	75.29	87.53Best local epoch(E)	20	20	N/A	150Model growth rate	1×	1×	16×	1.11×Table 2: Trained models summary for LSTM trained on Shakespeare as shown in Figure 2Method	FedAvg	FedProx	Ensemble	FedMAFinal Accuracy(%)	46.63	45.83	46.06	49.07Best local epoch(E)	2	5	N/A	5Model growth rate	1×	1×	66×	1.06×Handling data bias Real world data often exhibit multimodality within each class, e.g. geo-diversity. It has been shown that an observable amerocentric and eurocentric bias is present in thewidely used ImageNet dataset (Shankar et al., 2017; Russakovsky et al., 2015). Classifiers trainedon such data “learn” these biases and perform poorly on the under-represented domains (modalities)since correlation between the corresponding dominating domain and class can prevent the classifierfrom learning meaningful relations between features and classes. For example, classifier trainedon amerocentric and eurocentric data may learn to associate white color dress with a “bride” class,therefore underperforming on the wedding images taken in countries where wedding traditions aredifferent (Doshi, 2018).
Table 2: Trained models summary for LSTM trained on Shakespeare as shown in Figure 2Method	FedAvg	FedProx	Ensemble	FedMAFinal Accuracy(%)	46.63	45.83	46.06	49.07Best local epoch(E)	2	5	N/A	5Model growth rate	1×	1×	66×	1.06×Handling data bias Real world data often exhibit multimodality within each class, e.g. geo-diversity. It has been shown that an observable amerocentric and eurocentric bias is present in thewidely used ImageNet dataset (Shankar et al., 2017; Russakovsky et al., 2015). Classifiers trainedon such data “learn” these biases and perform poorly on the under-represented domains (modalities)since correlation between the corresponding dominating domain and class can prevent the classifierfrom learning meaningful relations between features and classes. For example, classifier trainedon amerocentric and eurocentric data may learn to associate white color dress with a “bride” class,therefore underperforming on the wedding images taken in countries where wedding traditions aredifferent (Doshi, 2018).
Table 3: The datasets used and their associated learning models and hyper-parameters.
Table 4: Detailed information of the VGG-9 architecture used in our experiments, all non-linear activationfunction in this architecture is ReLU; the shapes for convolution layers follows (Cin , Cout , c, c)Parameter	Shape	Layer hyper-parameterlayer1.conv1.weight	3 × 32 × 3 × 3	stride:1;padding:1layer1.conv1.bias	32	N/Alayer2.conv2.weight	32 × 64 × 3 × 3	stride:1;padding:1layer2.conv2.bias	64	N/Apooling.max	N/A	kernel size:2;stride:2layer3.conv3.weight	64× 128 × 3 × 3	stride:1;padding:1layer3.conv3.bias	128	N/Alayer4.conv4.weight	128× 128×3×3	stride:1;padding:1layer4.conv4.bias	128	N/Apooling.max	N/A	kernel size:2;stride:2dropout	N/A	p = 5%layer5.conv5.weight	128 ×256 ×3×3	stride:1;padding:1layer5.conv5.bias	256	N/Alayer6.conv6.weight	256 × 256 × 3 × 3	stride:1;padding:1layer6.conv6.bias	256	N/Apooling.max	N/A	kernel size:2;stride:2dropout	N/A	p= 10%
Table 5: Detailed information of the LSTM architecture in our experimentParameter	Shapeencoder.weight	80 × 8lstm.weight.ih.l0	1024 × 8lstm.weight.hh.l0	1024 × 256lstm.bias.ih.l0	1024lstm.bias.hh.l0	1024decoder.weight	80 × 256decoder.bias	8014Published as a conference paper at ICLR 2020Table 6: Detailed information of the LSTM architecture in our experimentParameter	Shape	Growth rate (#global / #original params)encoder.weight	80 X 21	2.63 × (1, 680/640)lstm.weight.ih.l0	1028 × 21	2.64 × (21, 588/8, 192)lstm.weight.hh.l0	1028 × 257	1.01 × (264, 196/262, 144)lstm.bias.ih.l0	1028	1.004 × (1, 028/1, 024)lstm.bias.hh.l0	1028	1.004 × (1, 028/1, 024)decoder.weight	80 × 257	1.004 × (20, 560/20, 480)decoder.bias	80	1×
Table 6: Detailed information of the LSTM architecture in our experimentParameter	Shape	Growth rate (#global / #original params)encoder.weight	80 X 21	2.63 × (1, 680/640)lstm.weight.ih.l0	1028 × 21	2.64 × (21, 588/8, 192)lstm.weight.hh.l0	1028 × 257	1.01 × (264, 196/262, 144)lstm.bias.ih.l0	1028	1.004 × (1, 028/1, 024)lstm.bias.hh.l0	1028	1.004 × (1, 028/1, 024)decoder.weight	80 × 257	1.004 × (20, 560/20, 480)decoder.bias	80	1×Total Number of Parameters	310,160	1.06× (310, 160/293, 584)Table 7: Detailed information of the final global VGG-9 model returned by FRB; the shapes for convolutionlayers follows (Cin , Cout , c, c)Parameter	Shape	Growth rate (#global / #original params)layer1.conv1.weight	3 × 47 × 3 × 3	1.47 × (1, 269/864)layer1.conv1.bias	47	1.47 × (47/32)layer2.conv2.weight	47 × 79 × 3 × 3	1.81 × (33, 417/18, 432)layer2.conv2.bias	79	1.23 × (79/64)layer3.conv3.weight	79 × 143 × 3 × 3	1.38 × (101, 673/73, 728)layer3.conv3.bias	143	1.12 × (143/128)layer4.conv4.weight	143 × 143 × 3 × 3	1.24 × (184, 041/147, 456)
Table 7: Detailed information of the final global VGG-9 model returned by FRB; the shapes for convolutionlayers follows (Cin , Cout , c, c)Parameter	Shape	Growth rate (#global / #original params)layer1.conv1.weight	3 × 47 × 3 × 3	1.47 × (1, 269/864)layer1.conv1.bias	47	1.47 × (47/32)layer2.conv2.weight	47 × 79 × 3 × 3	1.81 × (33, 417/18, 432)layer2.conv2.bias	79	1.23 × (79/64)layer3.conv3.weight	79 × 143 × 3 × 3	1.38 × (101, 673/73, 728)layer3.conv3.bias	143	1.12 × (143/128)layer4.conv4.weight	143 × 143 × 3 × 3	1.24 × (184, 041/147, 456)layer4.conv4.bias	143	1.12 × (143/128)layer5.conv5.weight	143 × 271 × 3 × 3	1.18 × (348, 777/294, 912)layer5.conv5.bias	271	1.06 × (271/256)layer6.conv6.weight	271 × 271 × 3 × 3	1.12 × (660, 969/589, 824)layer6.conv6.bias	271	1.06 × (271/256)layer7.fc7.weight	4336 × 527	1.09 × (2, 285, 072/2, 097, 152)layer7.fc7.bias	527	1.02 × (527/512)layer8.fc8.weight	527 × 527	1.05×, (277, 729/262, 144)layer8.fc8.bias	527	1.02 × (527/512)layer9.fc9.weight	527 × 10	1.02 × (5, 270/5, 120)
