Table 1: Methods’ Performance on the Base model. For easy, medium and difficult level verification properties,we compare methods’ average solving time, average number of branches required and the percentage of timedout properties. GNN-Online outperforms other methods in all aspects.
Table 2: Methods’ Performance on Large Models. For verification properties on Wide large model and Deeplarge model, we compare methods’ average solving time, average number of branches required and the per-centage of timed out properties. GNN-Online outperforms other methods in all aspects.
Table 3: For each CIFAR experiment, the network architecture used and the number of verificationproperties tested.
Table 4: Evaluating GNN’s dependence on the fail-safe strategy. Given a CIFAR model, we collected thepercentage of times GNN branching decision is used and the percentage of times the fail-safe heuristic (BaBSRin our case) is employed for each verification property. We report the average ratio of all verification propertiesof the same model. To account for extreme cases, we also list the minimum and maximum usage ratios of thefail-safe heuristic for each model.
Table 5: Measuring the importance of features used by GNN. For easy, medium and difficult level verificationproperties, we compare methods’ average solving time, average number of branches required and the percentageof timed out properties.
Table 6: Methods’ performance on randomly selected properties. We show methods’ average solving time,average number of branches required and the percentage of timed out properties. We emphasize that MIPplanetbranch number is not comparable with those of other methods.
Table 7: Methods’ Performance on the Base model. For easy, medium and difficult level verification properties,we compare methods’ geometric average solving time, geometric average number of branches required and thepercentage of timed out properties. GNN-Online outperforms other methods in all aspects.
Table 8: Methods’ Performance on Large Models. For verification properties on Wide large model and Deeplarge model, we compare methods’ geometric average solving time, geometric average number of branchesrequired and the percentage of timed out properties. GNN-Online outperforms other methods in all aspects.
Table 9: For each MNIST experiment, the network architecture used and the number of verificationproperties tested.
Table 10: Methods’ Performance on different models. For the Base, Wide and Deep model, we comparemethods’ average solving time, average number of branches required and the percentage of timed out propertiesrespectively. MIPplanet performs the best on the Base model while GNN-Online outperforms other methodson the Wide and the Deep model.
Table 11: Evaluating GNN’s dependence on the fail-safe strategy. Given a MNIST model, we collected thepercentage of times GNN branching decision is used and the percentage of times the fail-safe heuristic (BaBSRin our case) is employed for each verification property. We report the average ratio of all verification propertiesof the same model. To account for extreme cases, we also list the minimum and maximum usage ratios of thefail-safe heuristic for each model.
Table 12: Methods’ Performance on different models. For the Base, Wide and Deep model, we comparemethods’ geometric average solving time, geometric average number of branches required and the percent-age of timed out properties respectively. MIPplanet performs the best on the Base model while GNN-Onlineoutperforms other methods on the Wide and the Deep model.
