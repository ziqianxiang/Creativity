Table 1: CIFAR10 Hybrid modeling Results. Residual Figure 2: CIFAR10 class-Flow (Chen et al., 2019), Glow (Kingma & Dhariwal, 2018), conditional samples.
Table 2: Histograms for OOD detection. All models trained on CIFAR10. Green corresponds to thescore on (in-distribution) CIFAR10, and red corresponds to the score on the OOD dataset.
Table 3: OOD Detection Results. Models trained on CIFAR10. Values are AUROC.
Table 4: HyperparametersB S ample Quality EvalutionIn this section we describe the details for reproducing the Inception Score (IS) and FID resultsreported in the paper. First we note that both IS and FID are scores computed based on a pretrainedclassifier network, and thus can be very dependent on the exact model/code repository used. For amore detailed discussion on the variability of IS, please refer to Barratt & Sharma (2018). To gaugeour model against the other papers, we document our attempt to fairly compare the scores across13Published as a conference paper at ICLR 2020papers in Table 6. As a direct comparison of IS, we got 8.76 using the code provided by Du &Mordatch (2019), and is better than their best reported score of 8.3. For FID, we used the officialimplementation from Heusel et al. (2017). Note that FID computed from this repository assignedmuch worse FID than reported in Chen et al. (2019).
Table 5: Conditional vs. unconditional Inception Scores.
Table 6: The headings: B&S, D&M, and H denotes scores computed using code provided by Barratt& Sharma (2018), Du & Mordatch (2019),Heusel et al. (2017). *denotes numbers copied from Chenet al. (2019), but not the original papers. As unfortunate as the case is with Inception Score and FID(i.e., taking different code repository yields vastly different results), from this table we can still seethat our model performs well. Using D&M Inception Score we beat their own model, and using theofficial repository for FID we beat the Glow 3model by a big margin.
Table 7: OOD Detection Results. Values are AUROC.
