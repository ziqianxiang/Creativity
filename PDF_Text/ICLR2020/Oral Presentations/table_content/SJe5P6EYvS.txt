Table 1: Word-level perplexities of near state-of-the-art models, our LSTM baseline and the Mogrifier on PTBand Wikitext-2. Models with Mixture of Softmaxes (Yang et al. 2017) are denoted with MoS, depth N with dN.
Table 2: Bits per character on character-based datasets of near state-of-the-art models, our LSTM baselineand the Mogrifier. Previous state-of-the-art results in italics. Depth N is denoted with dN. MC stands forMonte-Carlo dropout evaluation. Once again the Mogrifier strictly dominates the LSTM and sets a new state ofthe art on all but the Enwik8 dataset where with dynamic evaluation it closes the gap to the Transformer-XL ofsimilar size 什 KraUse et al. (2019), ∣ Ben Krause, personal communications, May 17, 2019). On most datasets,model size was set large enough for underfitting not to be an issue. This was very much not the case with Enwik8,so we grouped models of similar sizes together for ease of comparison. Unfortunately, a couple of dynamicevaluation test runs diverged (NaN) on the test set and some were just too expensive to run (Enwik8, MC).
Table 3: PTB ablation study validationperplexities with 24M parameters.
Table 4: Hyperparameter tuning ranges for all tasks except Enwik8.
Table 5: Hyperparameter tuning ranges for Enwik8.
Table 6: Hyperparameter tuning ranges for dynamic evaluation.
