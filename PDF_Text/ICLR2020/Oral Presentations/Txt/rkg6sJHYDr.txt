Published as a conference paper at ICLR 2020
Intrinsically Motivated Discovery of Diverse
Patterns in Self-Organizing Systems
Chris Reinke∖ Mayalen Etcheverry∖ Pierre-Yves Oudeyer
Flowers Team
Inria, Univ. Bordeaux, Ensta ParisTech (France)
{chris.reinke,mayalen.etcheverry,pierre-yves.oudeyer}@inria.fr
Ab stract
In many complex dynamical systems, artificial or natural, one can observe self-
organization of patterns emerging from local rules. Cellular automata, like the
Game of Life (GOL), have been widely used as abstract models enabling the study
of various aspects of self-organization and morphogenesis, such as the emergence
of spatially localized patterns. However, findings of self-organized patterns in
such models have so far relied on manual tuning of parameters and initial states,
and on the human eye to identify “interesting” patterns. In this paper, we for-
mulate the problem of automated discovery of diverse self-organized patterns in
such high-dimensional complex dynamical systems, as well as a framework for
experimentation and evaluation. Using a continuous GOL as a testbed, we show
that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs),
initially developed for learning of inverse models in robotics, can be transposed
and used in this novel application area. These algorithms combine intrinsically-
motivated goal exploration and unsupervised learning of goal space representa-
tions. Goal space representations describe the “interesting” features of patterns
for which diverse variations should be discovered. In particular, we compare vari-
ous approaches to define and learn goal space representations from the perspective
of discovering diverse spatially localized patterns. Moreover, we introduce an ex-
tension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a
goal representation using a deep auto-encoder, and the use of CPPN primitives for
generating initialization parameters. We show that it is more efficient than several
baselines and equally efficient as a system pre-trained on a hand-made database
of patterns identified by human experts.
1	Introduction
Self-organization of patterns that emerge from local rules is a pervasive phenomena in natural and
artificial dynamical systems (Ball, 1999). It ranges from the formation of snow flakes, spots and rays
on animal’s skin, to spiral galaxies. Understanding these processes has boosted progress in many
fields, ranging from physics to biology (Camazine et al., 2003). This progress relied importantly on
the use of powerful and rich abstract computational models of self-organization (Kauffman, 1993).
For example, cellular automata like Conway’s Game of Life (GOL) have been used to study the
emergence of spatially localized patterns (SLPs) (Gardener, 1970), informing theories of the origins
of life (Gardener, 1970; Beer, 2004). SLPs, such as the famous glider in GOL (Gardner et al., 1983),
are self-organizing patterns that have a local extension and can exist independently of other patterns.
However, finding such novel self-organized patterns, and mapping the space of possible emergent
patterns, has so far relied heavily on manual tuning of parameters and initial states. Moreover, the
dependence of this exploration process on the human eye to identify “interesting” patterns is strongly
limiting further advances.
We formulate here the problem of automated discovery of a diverse set of self-organized patterns
in such high-dimensional, complex dynamical systems. This involves several challenges. A first
* Equal contribution.
Source code and supplementary material at https://automated-discovery.github.io/
1
Published as a conference paper at ICLR 2020
challenge consists in determining a representation of patterns, possibly through learning, enabling to
incentivize the discovery of diverse “interesting” patterns. Such a representation guides exploration
by providing a measure of (di-)similarity between patterns. This problem is particularly challenging
in domains where patterns are high-dimensional as in GOL. In such cases, scientists have a limited
intuition about what useful features are and how to represent them. Moreover, low-dimensional
representations of patterns are needed for human browsing and the visualization of the discoveries.
Representation learning shall both guide exploration, and be fed by self-collected data.
A second challenge consists in how to automate exploration of high-dimensional, continuous initial-
ization parameters to discover efficiently “interesting” patterns, such as SLPs, with a limited budget
of experiments. Sample efficiency is important to enable the later use of such discovery algorithms
for physical systems (Grizou et al., 2020), where experimental time and costs are strongly bounded.
For example, in the continuous GOL used in this paper as a testbed, initialization consists in deter-
mining the values of a real-valued, high-dimensional 256 × 256 matrix besides 7 additional dynamics
parameters. The possible variations of this matrix are too large for a simple random sampling to be
efficient. More structured methods are needed.
To address these challenges, we propose to leverage and transpose recent intrinsically motivated
learning algorithms, within the family of population-based Intrinsically Motivated Goal Exploration
Processes (POP-IMGEPs - denoted simply as IMGEPs below, Baranes & OUdeyer (2013); Pere et al.
(2018)). They were initially designed to enable autonomous robots to explore and learn what effects
can be prodUced by their actions, and how to control these effects. IMGEPs self-define goals in a
goal space that represents important featUres of the oUtcomes of actions, sUch as the position reached
by an arm. This allows the discovery of diverse novel effects within their goal representations. It
was recently shown how deep neUronal aUto-encoders enabled UnsUpervised learning of goal repre-
sentations in IMGEPs from raw pixel perception of a robot’s visUal scene (Laversanne-Finot et al.,
2018). We propose to Use a similar mechanism for aUtomated discovery of patterns by UnsUpervised
learning of a low-dimensional representation of featUres of self-organized patterns. This removes
the need for hUman expert knowledge to define sUch representations.
Moreover, a key ingredient for sample efficient exploration of IMGEPs for robotics has been the
Use of strUctUred motion primitives to encode the space of body motions (Pastor et al., 2013). We
propose to Use a similar mechanism to handle the generation of strUctUred initial states in GOL-like
complex systems, based on specialized recUrrent neUral networks (CPPNs) (Stanley, 2006).
In sUmmary, we provide in this paper the following contribUtions:
•	We formUlate the problem of aUtomated discovery of diverse self-organized patterns in
high-dimensional and complex game-of-life types of dynamical systems.
•	We show how to transpose POP-IMGEPs algorithms to address the associated joint chal-
lenge of learning to represent interesting patterns and discovering them in a sample efficient
manner.
•	We compare varioUs approaches to define or learn goal space representations for the sample
efficient discovery of diverse SLPs in a continUoUs GOL testbed.
•	We show that an extension of a state-of-the-art POP-IMGEP algorithm, with incremen-
tal learning of a goal space Using a deep aUto-encoder, is eqUally efficient than a system
pretrained on a hand-made database of patterns.
2	Related Work
Automated Discovery in Complex Systems AUtomated processes have been widely Used to ex-
plore complex dynamical systems. For example, evolUtionary algorithms have been applied to
search specific patterns or rUles of cellUlar aUtomata (Mitchell et al., 1996; Sapin et al., 2003).
However, their objective is to optimize a specific goal instead of discovering a diversity of patterns.
Another line of experiments represent active inqUiry-based learning strategies which qUery which
set of experiments to perform to improve a system model, i.e. a mapping from parameters to the
system oUtcome. SUch strategies have been Used in biology (King et al., 2004; 2009), chemistry
(RaccUglia et al., 2016; Reizman et al., 2016; DUros et al., 2017) and astrophysics (Richards et al.,
2011). However, these approaches have relied on expert knowledge, and focUsed on aUtomated opti-
2
Published as a conference paper at ICLR 2020
mization of a pre-defined target property. Here, we are interested to automatically discover and map
a diversity of unseen patterns without prior knowledge of the system. An exception is the concurrent
work of Grizou et al. (2020), which showed how a simple POP-IMGEP algorithm could be used to
automate discovery of diverse patterns in oil-droplet systems. However, it used a low-dimensional
input space, and a hand-defined low-dimensional representation of goal spaces, identified as a major
limit of the system.
Intrinsically motivated learning Intrinsically-motivated learning algorithms (Baldassarre &
Mirolli, 2013; Baranes & Oudeyer, 2013) autonomously organize an agent’s exploration curriculum
in order to discover efficiently a maximally diverse set of outcomes the agent can produce in an un-
known environment. They are inspired from the way children self-develop open repertoires of skills
and learn world models. Intrinsically Motivated Goal Exploration Processes (IMGEPs) (Baranes &
Oudeyer, 2013; Forestier et al., 2017) are a family of curiosity-driven algorithms developed to allow
efficient exploration of high-dimensional complex real world systems. Population-based versions
of these algorithms, which leverage episodic memory, hindsight learning, and structured dynamic
motion primitives to parameterize policies, enable sample efficient acquisition of high-dimensional
skills in real world robots (Forestier et al., 2017; Rolf et al., 2010). The discovered repertoires of di-
verse behaviors can be reused to solve hard exploration downstream tasks (Colas et al., 2018; Conti
et al., 2018). Recent work (Laversanne-Finot et al., 2018; Pere et al., 2018) studied how to auto-
matically learn the goal representations with the use of deep variational autoencoders. However,
training was done passively and in an early stage on a precollected set of available observations.
Recent approaches (Nair et al., 2018; Pong et al., 2019) introduced the use of an online training of
variational autoencoders VAEs to learn the important features ofa goal space similar to the methods
in this paper. However, these approaches focused on the problem of sequential decisions in MDPs,
incurring a cost on sample efficiency. This problem is observed in various intrinsically motivated
RL approaches (Bellemare et al., 2016; Burda et al., 2019b). These approaches are orthogonal to
the automated discovery framework considered here with independent experiments allowing the use
of memory-based sample efficient methods. A related family of algorithms in evolutionary com-
putation is novelty search (Lehman & Stanley, 2008) and quality-diversity algorithms (Pugh et al.,
2016), which can be formalized as special kinds of population-based IMGEPs.
Representation learning We are using representation learning methods to learn autonomously
goal spaces for IMGEPs. Representation learning aims at finding low-dimensional explanatory fac-
tors representing high-dimensional input data (Bengio et al., 2013). It is a key problem in many
areas in order to understand the underlying structure of complex observations. Many state-of-the-art
methods (Tschannen et al., 2018) have built on top of Deep VAEs (Kingma & Welling, 2013), using
varying objectives and network architectures. However, studies of the interplay between represen-
tation learning and autonomous data collection through exploration of an environment have been
limited so far.
3	Algorithmic Methods for Automated Discovery
3.1	Population-based Intrinsically Motivated Goal Exploration Processes
An IMGEP is an algorithmic process generating a sequence of experiments to explore the param-
eters of a system by targeting self-generated goals (Fig. 1). It aims to maximize the diversity of
observations from that system within a budget of N experiments. In population-based IMGEPs, an
explicit memory of the history H of experiments and observations is used to guide the process.
The systems are defined by three components. A parameter space Θ corresponding to the control-
lable system parameters θ. An observation space O where an observation o is a vector representing
all the signals captured from the system. For this paper, the observations are a time series of images
which depict the morphogenesis of activity patterns. Finally, an unknown environment dynamic D:
Θ → O which maps parameters to observations.
To explore a system, an IMGEP uses a goal space T that represents relevant features of its obser-
vations, computed by an encoding function g = R(o). For an exploration of patterns, such features
may describe their form or extension. The exploration process iterates N times through: 1) sample
a goal from a goal sampling distribution g 〜 G(H); 2) infer corresponding parameter θ using a
3
Published as a conference paper at ICLR 2020
Ok InitiaI SyStem State
H=I =
Observation Space O
Parameter Space Θ
Parameter for Dynamics
R =13
T = IO
μ = OΛ
σ = 0.02
Bl = 0.2
A = 0.4
尸3=O∙8
H=200
4) encode observation
3) perform experiment ；
History "
[01,01,7^.(01)]…[θk9 0∣ζ9 5^(θfc)]
2) select parameter: θ∣i^U (gfc,")
Deep Neural
Aut∞ncoder R
-oo--oo_
J/
Oo--Oo-

Sk
1) sample goal:
嬴~G


RQ)
Figure 1: Population-based intrinsically motivated goal exploration process with incremental learn-
ing of a goal space (IMGEP-OGL algorithm) used to explore a continuous GOL model.
parameter sampling policy Π = Pr(θ; g, H); 3) roll-out an experiment with θ, observe outcome o,
compute encoding R(o); 4) store (θ, o, R(o)) in history H. The parameter sampling policy Π and in
some cases the goal sampling distribution G take into account previous explorations stored in history
H. Therefore, the history is first populated through exploring Ninit randomly sampled parameters.
After this initial phase the described intrinsically motivated goal exploration process starts.
Different goal and parameter sampling mechanisms can be used within this architecture (Baranes &
Oudeyer, 2013; Forestier & Oudeyer, 2016). In the experiments below, goals are sampled uniformly
over a hyperrectangle defined in T. The hyperrectangle is chosen large enough to allow a sampling
of a large goal diversity. The parameters are sampled by 1) given a goal, selecting the parameter
from the history whose corresponding outcome is most similar in the goal space; 2) then mutating it
by a random process.
3.2	Online Learning of Goal Spaces WITH Deep Auto-Encoders
For IMGEPs the definition of the goal space T and its corresponding encoder R are a critical part,
because it biases exploration of the target system. One approach is to define a goal space by selecting
features manually, for example by using computer vision algorithms to detect the positions of a
pattern and its form. The diversity found by the IMGEPs will then be biased along these pre-defined
features. A limit of this approach is its requirement of expert knowledge to select helpful features,
particularly problematic in environments where experts do not know in advance what features are
important, or how to formulate them.
Another approach is to learn goal space features by unsupervised representation learning. The aim
is to learn a mapping R(o) from the raw sensor observations o to a compact latent vector z ∈ Rd .
This latent mapping can be used as a goal space where a latent vector z = g is interpreted as a goal.
Previous IMGEP approaches already learned successfully their goal spaces with VAEs (Laversanne-
Finot et al., 2018; Pere et al., 2018). However, the goal spaces were learned before the start of the
exploration from a prerecorded dataset of observations from the target environment. During the
exploration the learned representations were kept fixed. A problem with this pretraining approach
is that it limits the possibilities to discover novel patterns beyond the distribution of pretraining
examples, and requires expert knowledge.
4
Published as a conference paper at ICLR 2020
Algorithm 1: IMGEP-OGL
Initialize goal space encoder VAE R with random weights
for i — 1 to N do
if i < Ninit then	// Initial random iterations to populate H
L Sample θ 〜U(Θ)
else	// Intrinsically motivated iterations
Sample a goal g 〜G(H) based on space represented by R
_ Choose θ 〜Π(g, H)
Perform an experiment with θ and observe o
Encode reached goal g = R(o)
Append (θ, o, ^) to the history H
if i mod K == 0 then	// Periodically train the network
for E epochs do
L Train R on observations in H with importance sampling
for (θ,o,g) ∈ H do	// Update the database of reached goals
L H[^] J R(o)
In this paper we attempt to address this problem by continuously adapting the learned representation
to the novel observations encountered during the exploration process. For this purpose, we propose
an online goal space learning IMGEP (IMGEP-OGL), which learns the goal space incrementally
during the exploration process (Algorithm 1). The training procedure of the VAE is integrated in the
goal sampling exploration process by first initializing the VAE with random weights (Appendix B.6).
The VAE network is then trained every K explorations for E epochs on the observation collected in
the history H. Importance sampling is used to give more weight to recently discovered patterns by
using a weighted random sampler. It samples for 50% of the training batch samples patterns from
the last K iterations and for the other 50% patterns from all other previous iterations.
3.3	Structuring the parameter space in IMGEPs: from DMPs to CPPNs
A key role in the generation of patterns in dynamical systems is their initial state At=1. IMGEPs
sample these initial states and apply random perturbations to them during the exploration. For the
experiments in this paper this state is a two-dimensional grid with 256 × 256 cells. Performing
directly a random sampling of the 256 × 256 grid cells results in initial patterns that resemble white
noise. Such random states result mainly in the emergence of global patterns that spread over the
whole state space, complicating the search for spatially localized patterns. This effect is analogous
to a similar problem in the exploration of robot controllers. Direct sampling of actions for individual
actuators at a microscopic time scale is usually inefficient. A key ingredient for sample efficient
exploration has been the use of structured primitives (dynamic motion primitives - DMPs) to encode
the space of possible body motions (Pastor et al., 2013).
We solved the sampling problem for the initial states by transposing the idea of structured primitives.
Indeed, “actions” consist here in deciding the parameters ofan experiment, including the initial state.
We propose to use compositional pattern producing networks (CPPNs) (Stanley, 2006) to produce
structured initial patterns similar do DMPs. CPPNs are recurrent neural networks that allow the
generation of structured initial states. (Appendix B.4, Fig. 14). They are defined by their network
structure (number of neurons, connections between neurons) and their connection weights. Thus,
instead of using directly an initial state as part of the parameters θ to control the dynamical system,
a CPPN is used. If a system roll-out for the parameters θ are performed, then the initial state At=1
is generated by the CPPN. Moreover, instead of sampling and mutating directly an initial state, the
weights and structure of the CPPN are randomly generated and mutated. Please note, the number of
parameters in θ is therefore not fixed and open-ended (yet starts small) because the structure and the
number of weights of randomly sampled and mutated CPPNs differ.
5
Published as a conference paper at ICLR 2020
(b) Lenia animals	(C) Lenia animals
discovered by IMGEP-OGL discoveredby Chan (2019)
(a) Evolution in Lenia
from CPPN to animal
t=100	t=200
Figure 2: Example patterns produced in the continuous GOL system (Lenia). Illustration of the
dynamical morphing from an initial CPPN image to an animal (a). The automated discovery (b) is
able to find similar complex animals as a human-expert manual search (c) by Chan (2019).
4	Experimental methods
We describe here the continuous Game of Life (Lenia) we use as a testbed representing a large
class of high-dimensional dynamical systems, as well as the experimental procedures, the evaluation
methods used to measure diversity and detect SLPs, and the used algorithmic baselines and ablations.
Implementation details and parameter settings for all procedures are given in Appendix B.
4.1	Continous Game of Life as a testbed
Lenia (Chan, 2019) is a continuous cellular automaton (Wolfram, 1983) similar to Conway’s Game
of Life (Gardener, 1970). Lenia, in particular, represents a high-dimensional complex dynamical
system where diverse visual structures can self-organize and yet are hard to find by manual explo-
ration. It features the richness of Turing-complete game-of-life models. It is therefore well suited to
test the performance of pattern exploration algorithms for unknown and complex systems. The fact
that GOL models have been used widely to study self-organization in various disciplines, ranging
from physics to biology and economics (Bak et al., 1989), also supports their generality and potential
of reuse of our approach for discovery in other computational or wet high-dimensional systems.
Lenia consists of a two-dimensional grid of cells A ∈ [0, 1]256×256 where the state of each cell is
a real-valued scalar activity At(x) ∈ [0, 1]. The state of cells evolves over discrete time steps t
(Fig. 2, a). The activity change is computed by integrating the activity of neighboring cells. Lenia’s
behavior is controlled by its initial pattern At=1 and several settings that control the dynamics of the
activity change (R, T, μ, σ, β1,β2, β3).
Lenia can be understood as a self-organizing morphogenetic system. Its parameters for the initial
pattern At=1 and dynamics control determine the development of morphological patterns. Lenia can
produce diverse patterns with different dynamics (stable, non-stable or chaotic). Most interesting,
spatially localized coherent patterns that resemble in their shapes microscopic animals can emerge
(Fig. 2, b, c). These pattern types, which we will denote “animals” as a short name, are a key reason
scientists have used GOL models to study theories of the origins of life (Gardener, 1970; Beer,
2004). Therefore, in our evaluation method based on measures of diversity (see below), we will in
particular study the performance of IMGEPs, and the impact of using various approaches for goal
space representation, on finding a diversity of animal patterns. We implemented for this purpose
different pattern classifiers to analyze the exploration results. Initially we differentiate between dead
and alive patterns. A pattern is dead if the activity of all cells are either 0 or 1. Alive patterns are
separated into animals and non-animals. Animals are a connected areas of positive activity which
are finite, i.e. which do not infinitely cross several borders. All other patterns are non-animals whose
activity usually spreads over the whole state space.
6
Published as a conference paper at ICLR 2020
4.2	Evaluation based on the diversity of Patterns
The algorithms are evaluated based on their discovered diversity of patterns. Diversity is measured
by the spread of the exploration in an analytic behavior space. This space is externally defined by
the experimenter as in previous evaluation approaches in the IMGEP literature. For example, in Pere
et al. (2018), the diversity of discovered effects of a robot that manipulates objects is measured by
binning the space of object positions and counting the number of bins discovered. A difference here
is that the experimenter does not have access to an easily interpretable hand-defined low-dimensional
representation of possible patterns, equivalent to the cartesian coordinate of rigid objects. The space
of raw observations O, i.e. the final Lenia patterns At=200 , is also too high-dimensional for a mean-
ingful measure of spread in it. We constructed therefore an external evaluation space. First, a latent
representation space was build through the training of a β-VAE (Higgins et al., 2017) to learn the
important features over a large dataset of 42500 Lenia patterns identified during the many exper-
iments over all evaluated algorithms. This large dataset enabled to cover a diversity of patterns
orders of magnitude larger than what could be found in any single algorithm experiment, which ex-
perimental budget was order of magnitude smaller. We then augmented that space by concatenating
hand-defined features (the same features were used for a hand-defined goal space IMGEP).
For each experiment, all explored patterns were projected into the analytic behavior space. The
diversity of the patterns is then measured by discretizing the space into bins of equal size by splitting
each dimension into 7 sections (results were found to be robust to the number of bins per dimension,
see Appendix B.7). This results in 713 bins. The number of bins in which at least one explored
entity falls is used as a measure for diversity.
We also measured the diversity in the space of parameters Θ by constructing an analytic parameter
space. The 15 features of this space consisted of Lenia,s parameters (R, T, μ, σ, βι, β2, β3) and
the latent representation of a β-VAE. The β-VAE was trained on a large dataset of initial Lenia
states (At=1) used over the experimental campaign. This diversity measures also used 7 bins per
dimension.
4.3	Algorithms
The exploration behaviors of different IMGEP algorithms were evaluated and compared to a random
exploration. The IMGEP variants differ in their way how the goal space is defined or learned.
Random exploration: The IMGEP variants were compared to a random exploration that sampled
randomly for each of the N exploration iterations the parameters θ including the initial state At=1.
IMGEP-HGS - Goal exploration with a hand-defined goal space: The first IMGEP uses a hand-
defined goal space that is composed of 5 features used in Chan (2019). Each feature measures a
certain property of the final pattern At=200 that emerged in Lenia: 1) the sum over the activity of all
cells, 2) the number of activated cells, 3) the density of the activity center, 4) an asymmetry measure
of the pattern and 5) a distribution measure of the pattern.
IMGEP-PGL - Goal exploration with a pretrained goal space: For this IMGEP variant the goal
space was learned with a β-VAE approach on training data before the exploration process started.
The training set consisted of 558 Lenia patterns: half were animals that have been manually identi-
fied by Chan (2019); the other half randomly generated with CPPNs (see Section 4.4).
IMGEP-OGL - Goal exploration with online learning of the goal space: Algorithm 1.
IMGEP-RGS - Goal exploration with a random goal space: An ablated IMGEP using a goal
space based on the encoder of a VAE with random weights.
4.4	Experimental Procedure and hyperparameters
For each algorithm 10 independent repetitions of the exploration experiment were conducted. Each
experiment consisted of N = 5000 exploration iterations. This number was chosen to be compatible
with the application of the algorithms in physical experimental setups similar to Grizou et al. (2020),
planned in future work. For IMGEP variants the first Ninit = 1000 iterations used random parameter
sampling to initialize their histories H. For the following 4000 iterations each IMGEP approach
sampled a goal g via an uniform distribution over its goal space. The ranges for sampling in the hand-
7
Published as a conference paper at ICLR 2020
defined goal space (HGS) are defined in Table 3 (Appendix B.5). The ranges for the β-VAE based
goal spaces (PGL, OGL) were set to [-3, 3] for each of their latent variables. Then, the parameter θk
from a previous exploration in H was selected whose reached goal gk had the minimum euclidean
distance to the current goal g within the goal space. This parameter was then mutated to generate
the parameter θ that was explored.
The parameters θ consisted of a CPPN (Section 3.3) that generates the initial state At=1 for Lenia
and the settings defining Lenia's dynamics: θ = [CPPN → At=1 ,R,T,μ,σ,β1,β2, β3]. CPPNS
were initialized and mutated by a random process that defines their structure and connection weights
as done by Stanley (2006). The random initialization of the other Lenia settings for the dynamics was
done by an uniform distribution and their mutation by a Gaussian distribution around the original
values. The meta parameters to initialize and mutate the parameters were the same for all algorithms.
They were manually chosen without optimizing them for a specific algorithm.
5	Results
We address several questions evaluating the ability of IMGEP algorithms to identify a diverse set of
patterns, and in particular diverse “animal” patterns (i.e. spatially localized patterns).
Does goal exploration outperform random parameter exploration? In robotics/agents contexts
where scenes are populated with rigid objects, various forms of goal exploration algorithms outper-
form random parameter exploration (Laversanne-Finot et al., 2018). We checked whether this still
holds in continuous GOL which have very different properties. Measures of the diversity in the
analytic behavior space confirmed the advantage of IMGEPs with hand-designed (HGS) or learned
goal spaces (PGL/OGL) over random explorations (Fig. 3, b). The organization resulting from goal
exploration is also visible through the diversity in the space of parameters. IMGEPs focus their
(a) Diversity in Parameter Space
5000
4000
口…C
30 3000
J
CU
.≥ 2000
P
1000
0
0	1000	2000	3000	4000	5000
explorations
(b) Diversity in Behavior Space
(d) Behavior Space Diversity for Non-Animals
(c) Behavior Space Diversity for Animals
Figure 3: (a) Although random explorations reach the highest diversity in the analytic parameter
space, (b) IMGEPs reach a higher diversity in the analytic behavior space (except when using ran-
dom representations). (c) IMGEPs with a learned goal space discovered a larger diversity of animals
compared to a hand-defined goal space. (d) Learned goal spaces are as efficient as a hand-defined
space for finding diverse non-animals. Overall, IMGEPs with unsupervised learning of goal features
are efficient to discover a diversity of diverse patterns. Depicted is the average diversity (n = 10)
with the standard deviation as shaded area (for some not visible because it is too small). The final
diversity is significantly different (Welchs t-test, p < 0.01) for algorithms between the braces plotted
on the right of each figure. See Figs. 6 to 10 for a qualitative visual illustration of these results.
8
Published as a conference paper at ICLR 2020
(a) IMGEP-HGS Goal Space
Figure 4: (a) Hand-defined and (b) learned goal spaces have major differences shown here by a t-
SNE visualization. The different number and size of clusters of animals or non-animals can explain
the differences in their resulting diversity between the algorithms (Fig. 3).
(b) IMGEP-OGL Goal Space
dead
non-animal
animal
exploration on subspaces that are useful for targeting new goals. In contrast, random parameter
exploration is unguided, resulting in a higher diversity in the parameter space (Fig. 3, b).
What is the impact of learning a goal space vs. using random or hand-defined features?
We compared also the performance of random VAE goal spaces (RGS) to learned goal spaces
(PGL/OGL). For reinforcement learning problems, using intrinsic reward functions based on random
features of the observations can result in a high or boosted performance (Burda et al., 2019a;b). In
our context however, using random features (RGS) collapsed the performance of goal exploration,
and did not even outperform random parameter exploration for all kinds of behavioral diversity
(Fig. 3). Results also show that hand-defined features (HGS) produced significantly less global di-
versity and less “animal” diversity than using learned features (PGL/OGL). However, HGS found
an equal diversity of “non-animals”. These results show that in this domain, the goal-space has a
critical influence on the type and diversity of patterns discovered. Furthermore, unsupervised learn-
ing is an efficient approach to discover a diversity of diverse patterns, i.e. both efficient at finding
diverse animals and diverse non-animals.
Is pretraining on a database of expert patterns necessary for efficient discovery of diverse
animals? A possibility to bias exploration towards patterns of interest, such as “animals”, is to
pretrain a goal space with a pattern dataset hand-made by an expert. Here PGL is pretrained with
a dataset containing 50% animals. This leads PGL to discover a high diversity of animals. How-
ever, the new online approach (OGL) is as efficient as PGL to discover diverse patterns (Fig. 3,
b,c,d). Taken together, these results uncover an interesting bias of using learned features with a VAE
architecture, which strongly incentivizes efficient discovery of diverse spatially localized patterns.
How do goal space representations differ? We analyzed the goal spaces of the different IMGEP
variants to understand their behavior by visualizing their reached goals in a two-dimensional space.
T-SNE (Maaten & Hinton, 2008) was used to reduce the high-dimensional goal spaces.
The hand-defined (HGS) and learned (OGL) goal spaces show strong differences between each other
(Fig. 4). We believe this explains their different abilities to find either a high diversity of non-animals
or animals (Fig. 3, c, d). The goal space of the HGS shows large areas and several clusters for non-
animal patterns (Fig. 4, a). Animals form only few and nearby clusters. Thus, the hand-defined
features seem poor to discriminate and describe animal patterns in Lenia. As a consequence, when
goals are uniformly sampled within this goal space during the exploration process, then more goals
are generated in regions that describe non-animals. This can explain why HGS explored a higher
diversity of non-animal patterns but only a low diversity of animal patterns. In contrast, features
learned by OGL capture better factors that differentiate animal patterns. This is indicated by the
several clusters of animals that span a wide area in its goal space (Fig. 4, b).
We attribute this effect to the difficulty of VAEs to capture sharp details (Zhao et al., 2017). They
therefore represent mainly the general form of the patterns but not their fine-grained structures.
Animals differ often in their form whereas non-animals occupy often the whole cell grid and differ
9
Published as a conference paper at ICLR 2020
in their fine-grained details. The goal spaces learned by VAEs seem therefore better suited for
exploring diverse sets of animal patterns.
6	Conclusion
We formulated a novel application area for machine learning: the problem of automatically discover-
ing self-organized patterns in complex dynamical systems with high-dimensions both in the param-
eter space and in the observation space. We showed that this problem calls for advanced methods re-
quiring the dynamic interaction between sample efficient autonomous exploration and unsupervised
representation learning. We demonstrated that population-based IMGEPs are a promising algorith-
mic framework to address this challenge, by showing how it can discover diverse self-organized
patterns in a continuous GOL. In particular, we introduced a new approach for learning a goal space
representation online via data collected during the exploration process. It enables sample efficient
discovery of diverse sets of animal-like patterns, similar to those identified by human experts and
yet without relying on such prior expert knowledge (Fig. 2). We also analyzed the impact of goal
space representations on the diversity and types of discovered patterns.
The continuous game of life shares many properties with other artificial or natural complex systems,
explaining why GOL models have been used in many disciplines to study self-organization, see Bak
et al. (1989). We therefore believe this study shows the potential of IMGEPs for automated discovery
in other systems encountered in physics, chemistry or even computer animation. In further work, we
aim to apply this approach in robotized wet experiments such as the one presented in Grizou et al.
(2020) addressing the fundamental understanding of how proto-cells can self-organize. We are also
working together with bio-chemists to map the space of behaviors of certain complex biochemistry
systems for which no appropriate model exists. Once such a map has been discovered, it can be used
to optimize the system for target properties by leveraging the diversity of patterns found through the
unsupervised discovery process.
Acknowledgments
We thank Bert Wang-Chak Chan for his helpful discussions about the Lenia system. We also thank
Jonathan Grizou and Cedric Colas for useful comments on the paper.
References
Per Bak, Kan Chen, and Michael Creutz. Self-organized criticality in the’game of life. Nature, 342
(6251):780, 1989.
Gianluca Baldassarre and Marco Mirolli. Intrinsically motivated learning in natural and artificial
systems. Springer, 2013.
Philip Ball. The self-made tapestry: pattern formation in nature, volume 198. Oxford University
Press Oxford, 1999.
Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically mo-
tivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013.
Randall D Beer. AUtoPoiesis and cognition in the game of life. Artificial Life, 10(3):309-326, 2004.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying coUnt-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016.
YoshUa Bengio, Aaron CoUrville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
YUri BUrda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros.
Large-scale stUdy of cUriosity-driven learning. 2019a.
10
Published as a conference paper at ICLR 2020
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2019b.
Scott Camazine, Jean-Louis Deneubourg, Nigel R Franks, James Sneyd, Eric Bonabeau, and Guy
Theraula. Self-organization in biological systems. Princeton university press, 2003.
Bert Wang-Chak Chan. Lenia: Biology of artificial life. Complex Systems, 28(3):251-286, 2019.
Cedric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GeP-Pg: Decoupling exploration and ex-
ploitation in deep reinforcement learning algorithms. In International Conference on Machine
Learning (ICML), 2018.
Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, and Jeff
Clune. Improving exploration in evolution strategies for deep reinforcement learning via a pop-
ulation of novelty-seeking agents. In Advances in neural information processing systems, pp.
5027-5038, 2018.
Vasilios Duros, Jonathan Grizou, Weimin Xuan, Zied Hosni, De-Liang Long, Haralampos N Miras,
and Leroy Cronin. Human versus robots in the discovery and crystallization of gigantic polyox-
ometalates. Angewandte Chemie, 129(36):10955-10960, 2017.
Sebastien Forestier and Pierre-Yves Oudeyer. Modular active curiosity-driven discovery of tool
use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp.
3965-3972. IEEE, 2016.
Sebastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal explo-
ration processes with automatic curriculum learning. preprint arXiv:1708.02190, 2017.
Martin Gardener. Mathematical games: The fantastic combinations of john conway’s new solitaire
game” life,”. Scientific American, 223:120-123, 1970.
Martin Gardner, Martin Gardner, Martin Gardner, and Martin Gardner. Wheels, life, and other
mathematical amusements, volume 86. WH Freeman New York, 1983.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Jonathan Grizou, Laurie J Points, Abhishek Sharma, and Leroy Cronin. A curious formulation robot
enables the discovery of a novel protocell behavior. Science Advances, 6(5):eaay4237, 2020.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
volume 3, 2017.
I. T. Jolliffe. Principal Component Analysis and Factor Analysis, pp. 115-128. Springer New York,
New York, NY, 1986. ISBN978-1-4757-1904-8. doi:10.1007/978-1-4757-1904-8_7.
Stuart A Kauffman. The origins of order: Self-organization and selection in evolution. OUP, 1993.
Ross D King, Kenneth E Whelan, Ffion M Jones, Philip GK Reiser, Christopher H Bryant,
Stephen H Muggleton, Douglas B Kell, and Stephen G Oliver. Functional genomic hypothesis
generation and experimentation by a robot scientist. Nature, 427(6971):247, 2004.
Ross D King, Jem Rowland, Stephen G Oliver, Michael Young, Wayne Aubrey, Emma Byrne, Maria
Liakata, Magdalena Markham, Pinar Pir, Larisa N Soldatova, et al. The automation of science.
Science, 324(5923):85-89, 2009.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2013.
11
Published as a conference paper at ICLR 2020
Adrien Laversanne-Finot, Alexandre Pere, and Pierre-Yves Oudeyer. Curiosity driven exploration
of learned disentangled goal spaces. In Proceedings of The 2nd Conference on Robot Learning,
volume 87 of PLMR, 2018.
Joel Lehman and Kenneth O Stanley. Exploiting open-endedness to solve problems through the
search for novelty. In ALIFE,pp. 329-336, 2008.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Melanie Mitchell, James P Crutchfield, Rajarshi Das, et al. Evolving cellular automata with genetic
algorithms: A review of recent work. In Proceedings of the First International Conference on
Evolutionary Computation and Its Applications (EvCA96), volume 8. Moscow, 1996.
Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Vi-
sual reinforcement learning with imagined goals. In Advances in Neural Information Processing
Systems, pp. 9191-9200, 2018.
Peter Pastor, Mrinal Kalakrishnan, Franziska Meier, Freek Stulp, Jonas Buchli, Evangelos
Theodorou, and Stefan Schaal. From dynamic movement primitives to associative skill mem-
ories. Robotics and Autonomous Systems, 61(4):351-361, 2013.
Alexandre Pere, Sebastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised Learn-
ing of Goal Spaces for Intrinsically Motivated Goal Exploration. In ICLR2018 - 6th International
Conference on Learning Representations, Vancouver, Canada, April 2018.
Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-
fit: State-covering self-supervised reinforcement learning. preprint arXiv:1903.03698, 2019.
Justin K Pugh, Lisa B Soros, and Kenneth O Stanley. Quality diversity: A new frontier for evolu-
tionary computation. Frontiers in Robotics and AI, 3:40, 2016.
Paul Raccuglia, Katherine C Elbert, Philip DF Adler, Casey Falk, Malia B Wenny, Aurelio Mollo,
Matthias Zeller, Sorelle A Friedler, Joshua Schrier, and Alexander J Norquist. Machine-learning-
assisted materials discovery using failed experiments. Nature, 533(7601):73, 2016.
Brandon J Reizman, Yi-Ming Wang, Stephen L Buchwald, and Klavs F Jensen. Suzuki-miyaura
cross-coupling optimization enabled by automated feedback. Reaction chemistry & engineering,
1(6):658-666, 2016.
Joseph W Richards, Dan L Starr, Henrik Brink, Adam A Miller, Joshua S Bloom, Nathaniel R
Butler, J Berian James, James P Long, and John Rice. Active learning to overcome sample
selection bias: application to photometric variable star classification. The Astrophysical Journal,
744(2):192, 2011.
Matthias Rolf, Jochen J Steil, and Michael Gienger. Goal babbling permits direct learning of inverse
kinematics. IEEE Transactions on Autonomous Mental Development, 2(3):216-229, 2010.
Emmanuel Sapin, Olivier Bailleux, and Chabrier Jean-Jacques. Research of a cellular automaton
simulating logic gates by evolutionary algorithms. In European Conference on Genetic Program-
ming, pp. 414-423. Springer, 2003.
Kenneth O Stanley. Exploiting regularity without development. In Proceedings of the AAAI Fall
Symposium on Developmental Systems, pp. 37. AAAI Press Menlo Park, CA, 2006.
Kenneth O Stanley and Risto Miikkulainen. Efficient evolution of neural network topologies. In
Proceedings of the 2002 Congress on Evolutionary Computation., volume 2. IEEE, 2002.
Michael Tschannen, Olivier Bachem, and Mario Lucic. Recent advances in autoencoder-based
representation learning. preprint arXiv:1812.05069, 2018.
Stephen Wolfram. Statistical mechanics of cellular automata. Rev. of mod. physics, 55(3):601, 1983.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards deeper understanding of variational
autoencoding models. preprint arXiv:1702.08658, 2017.
12
Published as a conference paper at ICLR 2020
A Additional Results and Figures
We provide visualizations of discovered patterns, as well as of learned goal-spaces. A
dataset and an interactive visualization of all discovered patterns can be found at https://
automated-discovery.github.io/. The visualization serves as a support in qualitatively
evaluating the diversity of discovered patterns. Integrated into an interactive interface, these graphs
are also useful for a human to easily explore and visualize the different types of found patterns. In
addition, we provide statistical results quantifying the proportion of sub types of discoverd patterns.
A. 1 Discovered Patterns
Fig. 6 to 10 illustrate examples of discovered patterns per class (animal, non-animal, dead) for
each algorithm. The patterns have been randomly sampled from the results of a single exploration
repetition experiment. Please note that the shown examples represent only a small fraction of all
discovered patterns. A database with all patterns can be found on the website for the paper.
A.2 Visualization of Goal Spaces
The goal spaces of all IMGEP algorithms are visualized (Fig. 11) via two-dimensional reductions:
PCA (Jolliffe, 1986) and t-Distributed Stochastic Neighbor Embedding (t-SNE) (Maaten & Hinton,
2008). The visualizations were constructed by using for each algorithm its goal space represen-
tations of all its discovered patterns from a single repetition experiment. All goal representations
were normalized so that the overall minimum value became 0 and the maximum value 1 for each
dimension. PCA was performed to detect the two principle components. T-SNE was executed using
the default standard Euclidean distance metric and perplexity set to 50.
A.3 Proportion of Discovered Patterns of Different Classes
We used the measure of diversity of the found patterns to compare the performance of algorithms.
Another measure to compare the algorithms is the average proportion of dead, animals, non-animals
patterns discovered by each algorithm (Fig. 5). For animal patterns the results follow the diversity
results (Fig. 3), i.e. OGL and PGL find the highest proportion of animal patterns, followed by
HGS, then RGS and Random. A corollary is that RGS, random and HGS find in proportion a
higher percentage of non-animal patterns than OGL and PGL. As the number of different non-
animal patterns is as high for OGL and PGL, this shows the higher sample efficiency of OGL and
PGL to find diverse patterns. In contrast, Random, RGS and HGS approaches tend to find non-
animal patterns that are more similar to each other on average.
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
——⅝
1
□ dead
□ non-animals
□ animals
⅛
琮
晅
0l-------------------------------------------------------------------
Random IMGEP-RGS IMGEP-HGS IMGEP-PGL IMGEP-OGL
Figure 5: Proportion of patterns for each class and algorithm. Each dot besides the boxplot shows
the proportion of found patterns for each repetition (n = 10). The box ranges from the upper to
the lower quartile. The whiskers represent the upper and lower fence. The mean is indicated by the
dashed line and the median by the solid line.
13
Published as a conference paper at ICLR 2020
Random Exploration
dead:
34.6%
Figure 6: Randomly selected examples of patterns discovered by the random exploration algorithm
during a single exploration with 5000 iterations.
14
Published as a conference paper at ICLR 2020
IMGEP-RGS
non-aninιal
82.4%
Figure 7: Randomly selected examples of patterns discovered by the IMGEP-RGS algorithm during
a single exploration with 5000 iterations.
15
Published as a conference paper at ICLR 2020
IMGEP-HGS
Figure 8: Randomly selected examples of patterns discovered by the IMGEP-HGS algorithm during
a single exploration with 5000 iterations.
16
Published as a conference paper at ICLR 2020
IMGEP-PGL
Figure 9: Randomly selected examples of patterns discovered by the IMGEP-PGL algorithm during
a single exploration with 5000 iterations.
17
Published as a conference paper at ICLR 2020
IMGEP-OGL
Figure 10: Randomly selected examples of patterns discovered by the IMGEP-OGL algorithm dur-
ing a single exploration with 5000 iterations.
'∙≡:
i-::i-::llr'
—=J

18
Published as a conference paper at ICLR 2020
PCA
RGS
HGS
PGL
OGL
T-SNE
, dead
■ non-animal
♦ animal
Figure 11: PCA and t-SNE visualization of the goal spaces for the IMGEP variants show that HGS
has more area (PCA) and clusters (t-SNE) for non-animals compared to learned goal spaces (PGL
and OGL) and vice versa for animals. t-SNE shows that the hand-defined goal space (HGS) and
learned goal spaces (PGL and OGL) structure and cluster more the discovered patterns compared to
random goal space (RGS).
19
Published as a conference paper at ICLR 2020
B Implementation Details and Hyperparameter S ettings
This section provides implementation details and hyperparameter settings of all experiments. The
content is organized in the following manner:
•	B.1: Settings of Lenia
•	B.2: Description of the classifiers for dead, animal and non-animal Lenia patterns
•	B.3: Description of the statistical measures about Lenia patterns used as features for the
IMGEP-HGS goal space and the analytic behavior space.
•	B.4: Sampling mechanisms for Lenia’s initial state via CPPNs and Lenia’s other dynamic
parameters.
•	B.5: Details about the IMGEP-HGS
•	B.6: Details about IMGEPs utilizing deep variational autoencoders (RGS, PGL, OGL).
•	B.7: Description of the measurement method of diversity in the analytic parameter and
behavior space
B.1 Lenia Settings
A full description of Lenia can be found in (Chan, 2019). For all experiments the following config-
urations of Lenia were used:
•	Grid size: 256 × 256 (A ∈ [0, 1]256×256)
•	Number of steps: M = 200
• Exponential growth mapping: G(u; μ,σ)
2eχp (- (u-σ2))
-1
•	Exponential kernel function: KC (r) = exp (α —4『(L J, With ɑ = 4
•	Kernel shell: KS (r; β) = βbBrc KC(Br mod 1), with β = (β1, β2, β3)
The controllable parameters of Lenia are the kernel size R ∈ N, time step T ∈ N, μ ∈ R and σ ∈ R
that control the growth mapping, and β1, β2, β3 ∈ R that control the kernel shell. Additionally the
initial state At=1 ∈ [0, 1]256×256 controls the system dynamics.
B.2	Classification of Lenia Patterns
We classified 3 types of Lenia patterns: dead, animals and non-animals. The categories were used to
analyze if the exploration algorithms showed differences in their behaviors by identifying different
types of patterns. The classifiers only categorize the final pattern into which the Lenia system morphs
after M = 200 time steps.
Dead Classifier: For dead patterns the activity of all cells are either 0 or 1.
Animal Classifier: A pattern is classified as an animal if it is a finite and connected pattern of
activity. Cells x, y are connected as a pattern if both are active (A(x) ≥ 0.1 and A(y) ≥ 0.1) and if
they influence each other. Cells influence each other when they are within their radius of the kernel
K as defined by the parameter R.
Furthermore, the connected pattern must be finite. In Lenia finite and infinite patterns can be differ-
entiated because the opposite borders of Lenia’s cell grid are connected, so that the space is similar
to a ball surface. Thus, a pattern can loop around this surface making it infinite. We identify infi-
nite patterns by the following approach. First, all connected patterns are identified for the case of
assuming an infinite grid cell, i.e. opposite grid cell borders are connected. Second, all connected
patterns for the case of a finite grid cell, i.e. opposite grid cell borders are not connected, are iden-
tified. Third, for each border pair (north-south and east-west) it is tested if cells within a distance
of R from both borders exists, that are part of a connected pattern for the infinite and finite grid cell
case. If such a pattern exists than it is assumed to be infinite, because it loops around the grid cell
surface of Lenia (Fig. 12, a). All other patterns are considered to be finite (Fig. 12, b). Please note
20
Published as a conference paper at ICLR 2020
that this method has a drawback. It can not identify certain infinite patterns that loop over several
borders, for example, if a pattern connects the north to east and then the west to south border (see
the third animal in Fig. 6 for an example).
Moreover, there are two additional constraints that an animal pattern must fulfill. First, the cells of
the connected pattern P = {x1, . . . , xn} must have at least 80% of all activation, i.e. Px∈P A(x) ≥
0.8 P∀y A(y). Second, a pattern must exists for the last two time steps (t = M and t = M - 1).
Both constraint are used to avoid that too small patterns or chaotic entities which change drastically
between time steps are classified as animals. See Figs. 6, 8, 9 and 10 for examples of animal patterns.
Non-Animal Classifier: We also classified non-animal patterns which are all entities that were not
dead and not an animal. These patterns spread usually over the whole state space and are connected
via borders. See Figs. 6, 8, 9 and 10 for examples of non-animal patterns.
B.3	Statistical Measures for Lenia Patterns
We defined five statistical measurements for the final patterns At=M that emerge in Lenia. The
measures were used as features for hand-defined goal spaces of IMGEPs and to define partly the
analytic behavior space in which the results of the exploration experiments were compared.
Activation mass MA: Measures the sum over the total activation of the final pattern and normalizes
it according to the size of the Lenia grid:
MA = J X At=M (x),
x
where L2 = 256 ∙ 256 is the number of cells of the Lenia system.
Activation volume VA: Measures the number of active cells and normalizes it according to the size
of the Lenia grid:
1
L2
∣{∀x : At=M(χ) > e} I with e = 10-4.
Activation density DA : Measures how dense activation is distributed on average over active cells:
DA
MA
VA.
Activation asymmetry AA : Measures how symmetrical the activation is distributed according to
an axis that starts in the center of the patterns activation mass and goes along the last movement
direction of this center. This measure was introduced to especially characterize animal patterns. The
center of the activity mass is usually also the center of the animals and analyzing the activity along
their movement axis measures how symmetrical they are.
As a first step, the center of the activation mass is computed for every time step of the Lenia simu-
lation and the Lenia pattern recentered to this location. This ensures that the center is all the time
borders which are identified if a segment is connected between two borders in the infinite and finite
segmentation. Finite patterns form no loops. They have connected segments between borders in the
infinite but not finite segmentation. Segments are colorized in yellow, green and blue.
21
Published as a conference paper at ICLR 2020
correctly computed in the case the animal moves and reaches one border to appear on the opposite
border in the uncentered pattern. The center (x, y)t for time step t is calculated by:
(x,y)t =( MO ,M1) with MPq = XX Xpyq At(x,y) ,
xy
where Mpq measures the image moment (or raw moment) of order (p + q) for p, q ∈ N.
Based on the center (x, y)t the pattern At is recentered to AC by shifting the X and	y	indexes
according to the center:
AC(x, y) = At((x — X) mod L, (y — y) mod L) ,	(1)
where L	is width and length of the Lenia grid and the indexing is X, y = 0, . . . , L - 1.	After	each
time step the center is recomputed and the pattern recentered:
At=1 -→ AC=1	-→	At=2 -→ AC=2	-→	....
recenter	Lenia step	recenter	Lenia step
Please note, the simulations and all figures of patterns in the paper are done with the uncentered
pattern. The centered version is only computed for the purpose of statistical measurements.
The recenter step by (x, y)t defines also the movement direction of the activity center:
(mχ, my)t = (x, y)t — (χmid, ymid) = (X — χmid, y — ymid),
where χmid, ymid = L-I are the coordinates for the grid,s middle point. A line can be defined that
starts in the midpoint Xmid , ymid of the final centered pattern AtC=M and goes in and opposite to
the movement direction of the activity mass center (mx, my)t=M. This line separates the grid in two
equal areas. The asymmetry is computed by comparing the activity amount in the grid right MAright
and leftMAleft of the line. The normalized difference between both sides is the final measure:
AA = 3(MMt- Mf
MA
Activation centeredness CA : Measures how strong the activation is distributed around the activity
mass center:
CA = U XX Wxy ∙ At=M(x,y) with Wxy = (1-----------------d(x-y)-r),
MA x y	C	maxy,x d(X, y)
where d(x, y) = ,(X — Xmid)2 + (y — ymid)2 is the distance from the point (x, y) to the center
point Xmid, ymid . AtC=M (X, y) is the centered activation that is updated every time step as for the
asymmetry measure (Eq. 1). The weights Wxy decrease the farer a point is from the center. Thus,
patterns that are concentrated around the center have a high value for CA close to 1. Whereas,
patterns whos activity is distributed throughout the whole grid have a smaller value. For patterns
that are equally distributed (∀x,x0 : A(X) = A(X0)) is CA = 0 defined as centeredness measure.
B.4	Sampling of Parameters for Lenia
All algorithms explore Lenia by sampling the parameters θ that control Lenia. The parameters
are comprised of the initial pattern At=1 and the parameters which control the dynamic behavior
(R, T, μ, σ, βι, β2,β3). There are two operations to sample parameters: 1) random initialization and
2) mutating an existing parameter θ. CPPNs are used for the random initialization and mutation of
the initial pattern At=1. The details of this process are described in the next section. Afterwards, the
initialization and mutation of Lenia’s parameter that control its dynamics are described.
B.4	. 1	Sampling of Start Patterns for Lenia via CPPNs
Compositional Pattern Producing Networks (CPPNs) are recurrent neural networks (Stanley, 2006)
which we to generate and mutate the initial state of Lenia At=1. The CPPNs generate Lenia activity
patterns cell by cell by taking as input a bias value, the X and y coordinate of the cell (mapped to
22
Published as a conference paper at ICLR 2020
CPPN:
Pattern:
1
x
y
d
2
y
-2
-2x2
Figure 13: CPPNs are recurrent neural networks. Their input is a bias of 1, the X and y coordinate
of a grid cell and its center distance d. Their output is the activity value of the grid cell.
x = [-2, 2] and y = [-2, 2]) and its distance d to the grid center (Fig. 13). Their output p defines
the activity of the cell (A(x, y) = 1 - |p|) between 0 and 1 for the given (x, y) coordinate.
CPPNs consist of several hidden neurons (typically 4 to 6 in our experiments) that can have recurrent
and self connections. Each CPPN has one output neuron. Two activation functions were used for
the hidden neurons and the output neuron. The first is Gaussian and the second is sigmoidal:
gauss(x) = 2 exp -(2.5x)2 - 1 ,
Sigm(X) = 2 (l + exP(-5x)) - 1 .
(2)
To randomly initialize a Lenia initial pattern At=1 a CPPN is randomly sampled by sampling the
number of hidden neurons, the connections between inputs and neurons and neurons to neurons,
their connection weights and the activation functions for neurons. Afterwards the initial pattern is
generated by it. In the history H of the IMGEPs is then the CPPN as part of the parameter θ added.
If the parameter is mutated, then the weights, connections and activation functions of the CPPN
are mutated and the new initial pattern At=1 generated by it. A CPPN is defined over its network
structure (number of nodes, connections of nodes) and its connection weights.
We used the neat-python2 package for the random generation and mutation of CPPNs. It is based
on the NeuroEvolution of Augmenting Topologies (NEAT) algorithm for the evolution of neural
networks (Stanley & Miikkulainen, 2002). The meta-parameters for the initialization and mutation
of CPPNs are listed in Table 1. The random sampling and mutation of CPPNs allows to generate
complex patterns as illustrated in Fig. 14. Please note, the neat-python package allows also the
setting and mutation of response and bias weights for each neuron. Those settings were not used
Figure 14: CPPNs can generate complex patterns via their random initialization and successive
mutations. Each row shows generated patterns by one CPPN and its mutations.
2https://github.com/CodeReclaimers/neat-python
23
Published as a conference paper at ICLR 2020
Parameter	Value
Initial number of hidden neurons Initial activation functions Initial connections Initial synapse weight Synapse weight range Mutation neuron add probability Mutation neuron delete probability Mutation connection add probability Mutation connection delete probability Mutation rate of activation functions Mutation rate of synapse weights Mutation replace rate of synapse weights Mutation power of synapse weights σM Mutation enable/disable rate of synapse weights	4 gauss, sigm random connections with probability 0.6 Gaussian distribution with μ = 0, σ = 0.4 [-3, 3] 0.02 0.02 0.05 0.01 0.1 0.05 0.06 1 0.02
Table 1: Settings for the sampling of CPPNs to generate Lenia’s initial states.
for the experiments. Moreover, we adjusted the sigmoid and Gaussian function in the neat-python
package to the ones defined in Eq. 2 to be able to replicate similar images as in Stanley (2006).
B.4.2 Sampling of Lenia’ s Dynamic Parameters
The parameters that control the dynamics ofLenia (R, T, μ, σ, βι ,β2,β3) are initialized and mutated
via uniform and Gaussian distributions. Table 2 lists for each parameter the meta-parameters for their
initialization and mutation. Each parameter is initialized by an uniform sampling θ% 〜 U (a, b) with
a and b as upper and lower border. An existing parameter θi is mutated by the following equation:
θi - [θi + N (O,σM )]a ,
where σM is the mutation power and [n]ba = min(max(n, a), b) is the clip function. For natural
numbers θi ∈ N the resulting value is rounded.
Parameter	Type	Value Range	Mutation σM
R	N	^[2?20]	0.5
T	N	[1, 20]	0.5
μ	R	[0, 1]	0.05
σ	R	[0.001, 0.3]	0.01
β1 , β2 , β3	R	[0,1]	0.05
Table 2: Settings for the initialization and mutation of Lenia system parameters θ.
B.5	IMGEP-HGS Details
The 5 statistical features that are given in Appendix B.3 were used to define the goal space of the
IMGEP-HGS approach (Algorihm 2). Goals in this space were sampled from a uniform distribution
within the ranges defined in Table 3.
Feature	min	max	Feature	min	max
mass MA	0	1	asymmetry AA	-1	1
volume VA	0	1	centeredness CA	0	1
density DA	0	1			
Table 3: HGS Goal Space Ranges
24
Published as a conference paper at ICLR 2020
B.6	IMGEPs with Deep Variational Autoencoders
B.6	. 1 VAE Framework and Implementation Details
We applied deep variational autoencoders (VAEs) to learn latent representations of Lenia patterns.
VAEs have two components: a neural encoder and decoder. The encoder q(z|x, χ) represents a
given data point x in a latent representation z. In variational approaches the encoder describes a
data point by a representative distribution in the latent space of reduced dimension d. A standard
Gaussian prior p(z) = N(0, I) and a diagonal Gaussian posterior q(z∣x, χ) = N(μ, σ) are used.
Given a data point x, the encoder outputs the mean μ and variance σ of the representative distribution
in the latent space. The decoder p(x|z, ψ) tries to reconstruct the original data x from a sampled
latent representation z for the distribution given by the encoder. Under these assumptions, training
is done by maximizing the computationally tractable evidence lower bound (with β = 1):
L(χ,ψ) = EZ〜qχ(z∣χ)[logpψ(x|z)] -β X DκL[qχ(z∣x)kp(z)] .	(3)
'------{z------}	'------{Z----}
ab
The first term (a) represents the expected reconstruction accuracy while the second (b) is the KL
divergence of the approximate posterior from the prior.
d
b = DKL[N(μ(x), ∑(x))kN(0,I)] = X DκL[N(μ(x)i,σ(x)i)kN(0,1)].
一 |----------------------------------------------------------/
(4)
i=1
{z
bi
In particular, we used the β-VAE framework (Higgins et al., 2017) which re-weights the b term
by a factor β > 1, aiming to enhance the disentangling properties of the learned latent factors.
The weight factor was set to β = 5 for all experiments. All β-VAEs used for this paper use the
same architecture (Table 4). The encoder network has as input the Lenia pattern and as outputs for
each latent variable Zi the mean μ% and log-variance log(σ2). The decoder takes as input during
the training for each latent variable a sampled value Zi 〜 N(μi, σ2). For validation runs and the
generation of all reconstructed patterns shown in figures the decoder takes the mean Zi = μ% as
input. Its output is the reconstructed pattern.
The training objective (Eq. 3) results in the following loss function for a batch:
d
Loss(x, x, μ, σ) = —a + β ɪ2 b ,	(5)
i=1
where X are the input patterns, X are the reconstructed patterns, μ, σ are the outputs of the decoder
network and d is the number of latent dimensions. The reconstruction accurray part a of the loss is
given by a binary cross entropy with logits:
N L2
a = ]1∑∑(Xj,n • logσ(Xj,n) + (1 — Xj,n) ∙ log(1 — σ(Xj,n))),
n=1 j=1
where the index j is for the single cells (pixel) of the pattern and n for the datapoint in the current
batch, N is the batch size and σ(x) = ι+1-x. The KL divergence terms b are given by:
1N
bi = 2∙N E S* + μi,n — log(σi,n) - 1) ∙
n=1
Encoder
Input pattern A: 256 × 256 × 1
Conv layer: 32 kernels 4	×	4,	stride	2, 1-padding +	ReLU
Conv layer: 32 kernels 4	×	4,	stride	2, 1-padding +	ReLU
Conv layer: 32 kernels 4	×	4,	stride	2, 1-padding +	ReLU
Conv layer: 32 kernels 4	×	4,	stride	2, 1-padding +	ReLU
FC layers : 256 + ReLU, 256 + ReLU, FC: 2 × 8
Decoder
Input latent vector z: 8 × 1
FC layers : 256 + ReLU, 16 × 16 × 32 + ReLU
TransposeConv layer:	32	kernels 4	×	4,	stride	2,	1-padding	+ ReLU
TransposeConv layer:	32	kernels 4	×	4,	stride	2,	1-padding	+ ReLU
TransposeConv layer:	32	kernels 4	×	4,	stride	2,	1-padding	+ ReLU
TransposeConv layer:	32	kernels 4	×	4,	stride	2,	1-padding
Table 4: β-VAE architecture for the pretrained and online experiments.
25
Published as a conference paper at ICLR 2020
All β-VAEs were trained for 2000 epochs and initialized with pytorch default initialization. We used
the Adam optimizer (Kingma & Ba, 2014)	(lr	= 1e-3, β1	= 0.9,	β2	=	0.999,	=	1e-8,	weight
decay=1e-5) with a batch size of 64.
The patterns from the datasets were augmented by random x and y translations (up to half the pattern
size and with probability 0.3), rotation (up to 40 degrees and with probability 0.3), horizontal and
vertical flipping (with probability 0.2). The translations and rotations were preceded by spherical
padding to preserve Lenia spherical continuity.
B.6.2	IMGEP Variants
IMGEP-RGS (random goal space): IMGEP with a goal space defined by an encoder network
with random weights (Algorithm 2). The weights were initialized according to the Xavier method
(Glorot & Bengio, 2010). The network architecture of the encoder is the same that the one of the
VAEs used for IMGEP with learned goal spaces. In the other IMGEP algorithms (HGS/PGL/OGL),
the goals are sampled uniformly within fixed-range boundaries that are chosen in advance. However,
in the case of random goal spaces, we do not know in advance in which region of the space goals
will be encoded. Therefore, we set the sampling range for each latent variable to the minimum and
maximum value of the latent representations of all so far explored patterns.
IMGEP-PGL (prelearned goal space): IMGEP with a goal space defined by a β-VAE that was
trained before the exploration starts (Algorithm 2). The β-VAE was trained on a dataset consisting
of 558 precollected Lenia patterns (training: 75%, validation: 10%, testing: 15%). Half of the
patterns (279) were manually identified animal patterns by Chan (2019). The other half (279) were
randomly initialized CPPN patterns (see Section B.4.1). The best β-VAE model obtained during
training (highest accuracy on the validation data) was used as goal space for the exploration. During
the exploration, goals were uniformly sampled in a [-3, 3]8 hypercube. These values were chosen
because the encoder of the β-VAE is trained to match a prior standard normal distribution. Therefore,
we can assume that most area of the covered goal space will fall into that hypercube.
IMGEP-OGL (online learned goal space): IMGEP with a goal space that was online learned
during the exploration via an β -VAE (Algorithm 1). Every K = 100 explorations the β-VAE model
was trained for 40 epochs resulting in 2000 epochs in total (less if there is not enough data after
the first T runs to start the training). The datasets were constructed by collecting non-dead patterns
during the exploration. One pattern in every ten is added to the validation set (10%). All others are
used for the training set. At the initial period of training, the training dataset has approximately 50
patterns and at the last period approximately 3425 patterns. The validation dataset only serves for
checking purposes and has no influence on the learned goal space. Importance sampling is used to
give the patterns a different weight during the training. A weighted random sampler is used that
samples newly discovered patterns half of the time. Each pattern that has been added to the training
dataset during the last period of 100 explorations has a probability of 0N5 to be sampled (N is the
total number of new patterns in the dataset). Older patterns are with probability ∣00.-N. AS a result,
newer discovered patterns have a higher weight and a stronger influence on the training.
Algorithm 2: IMGEP-HGS, IMGEP-RGS and IMGEP-PGL
Initialize goal space encoder R with handdefined features (HGS), random weights (RGS), or
pretrained weights (PGL)
for i J 1 to N do
if i < Ninit then	// Initial random iterations to populate H
Sample θ 〜U(Θ)
else	// Intrinsically motivated iterations
Sample a goal g 〜G(H) based on space represented by R
_ Choose θ 〜Π(g, H)
Perform an experiment with θ and observe o
_ Append (o,θ, R(o)) to the history H
26
Published as a conference paper at ICLR 2020
B.6.3	VAE Training Results
The β-VAE learning saturates for both the precollected dataset and the online collected dataset
(Fig. 15). Their ability to reconstruct patterns based on the encoded latent representation is also
qualitatively similar. For both datasets the β-VAEs are able to learn the general form of the activity
pattern (Fig. 16). Nonetheless, the compression of the images to a 8-dimensional vector results in a
general blurriness in the reconstructed patterns. As a result, the β-VAEs are not able to encode finer
details and textures of patterns.
IMGEP-PGL - β-VAE
40k
10k
0
30k
S
ʃ
Q 、
20 20k
1000
epoch
Figure 15: Averaged learning curves (n = 10) of the β-VAEs for the IMGEP-PGL and OGL
experiments.
a) Reconstruction Examples of the β-VAE used for the IMGEP-PGL
b) Reconstruction Examples of the β-VAE used for the IMGEP-OGL
Figure 16: Examples of patterns (left) and their reconstruction (right) by β-VAE networks used
for the IMGEP-PGL (a) and OGL (b). The patterns are sampled from their validation dataset. For
the PGL (a) the dataset is composed of animal patterns (row 1) from Chan (2019) and randomly
generated CPPN patterns (row 2). For the OGL (b) it is composed of animal patterns (row 1) and
non-animal patterns (row 2).
27
Published as a conference paper at ICLR 2020
B.7 Measurement of Diversity in the Analytic Parameter and B ehavior Space
B.7.1	Diversity Measure
Diversity is measured by the area that explored parameters or observations cover in their respective
spaces. The parameter space consisted of the initial start state of Lenia (At=1 ∈ [0, 1]256×256)
and the settings for Lenia's dynamics (R, T, μ, σ, β1,β2,β3). The space consist therefore of 2562
dimensions, each for a single grid cell of the initial pattern, plus 7 dimensions for the dynamic
settings. The observation space consists of the final patterns At=200 ∈ [0, 1]256×256 resulting in
2562 dimensions for the space. Each single exploration results in a new point in those spaces.
The diversity measures how much area the algorithms explored in those spaces (Fig. 17). The mea-
surement is done by discretizing the space with a spatial grid and counting the number of discretized
areas in which at least one point falls. For the discretization a minimum and maximum border
is defined for each space dimension. Each dimension is then split in equally sized bins between
those borders. The areas with values falling below the minimum or above the maximum border are
counted as two additional bins.
The number of dimensions of the original parameter and observation space are too large to measure
diversity, because the initial and the final pattern have 2562 dimensions. We constructed therefore
an analytic parameter and behavioral space where the latent representations of a β-VAE were used
to reduce the high-dimensional patterns to 8 dimensions. 5 bins (7 with the out of range values) per
dimension were used for the discretization of those spaces for all experiments in the paper.
Figure 17: Illustration of the diversity measure in a two-dimensional space. Ranges for the dimen-
sions are [-5, 5] and [0, 0.3]. The number bins per dimension is 7 resulting in 72 = 49 discretized
areas. The diversity is the number of areas (here 12) in which points exist (grey areas).
B.7.2	Analytic Parameter and B ehavior Space
The analytic parameter space was constructed by the 7 Lenia parameters that control its dynamics
and 8 latent representation dimensions ofa β-VAE (Table 5). The β-VAE was trained on initial pat-
terns At=1 used during the experiments. The analytic behavior space was constructed by combining
the 5 statistical measures for final Lenia patterns (Section B.3) and also 8 latent representation di-
mensions of a β-VAE (Table 5). This β-VAE was trained on final patterns At=200 observed during
the experiments. The datasets for both β-VAEs were constructed by randomly selecting 42500 pat-
terns (37500 as training set, 5000 as validation set) from the experiments of all algorithms and each
of their 10 repetitions. The β-VAEs used the same structure, hyper-parameters, loss function and
Analytic Parameter Space Definition	Analytic Behavior Space Definition
Parameter	min	max	Parameter	min	max
R	1	20	mass MA	0	1
T	2	10	volume VA	0	1
μ	0	1	density DA	0	1
σ	0	0.3	asymmetry AA	-1	1
β1 , β2 , β3	0	1	centeredness CA	0	1
β-VAE latent 1 to 8 (trained on intial states At=1)	-5	5	β-VAE latent 1 to 8 (trained on final patterns At=M)	-5	5
Table 5: Features of the analytic parameter and behavior space with their min and max values.
28
Published as a conference paper at ICLR 2020
a) Reconstruction Examples of the Analytic Parameter Space β-VAE
b) Reconstruction Examples of the Analytic Behavior Space β-VAE
Figure 18: Examples of patterns (left) and their reconstruction (right) by the β-VAE used for the
analytic parameter (a) and behavior space (b). The patterns are sampled from the validation dataset.
learning algorithm as described in Section B.6. They were trained for more than 1400 epochs.The
encoder which resulted in the minimal validation set error during the training was used. According
to their reconstructed patterns they can represent the general form of patterns but often not individual
details such as their texture (Fig. 18).
B.7.3	Dependence of the Diversity on the Number of Bins per Dimension
The diversity measure has as parameter the number bins per dimension. We analyzed the influences
of this parameter on the diversity measure (Fig. 19). Although the diversity difference between
algorithms depends on the number of bins per dimension for each space, the order of the algorithms
is generally invariant to it. Only if the number of bins per dimension grows large (>10) the order
of the algorithms changes in some cases. The order starts to follow the same order as seen for the
proportion of identified patterns (Fig. 5). In this case the discretization of the space becomes too
fine. Each pattern falls into its own discretized area. We chose therefore a smaller number of bins
per dimension of 7 to compare the algorithms in a meaningful way.
5	10	15	20	25
bins per dimension
(d) Behavior Space Diversity for Non-Animals
5	10	15	20	25
bins per dimension
(c) Behavior Space Diversity for Animals
bɪsjəAI
5	10	15	20	25
bins per dimension
Figure 19: Influence of the number of bins per dimensions on the diversity measure. Depicted is the
average diversity (n = 10) with the standard deviation as shaded area.
3000
∙Qθ00
∙≥
P
1000
5	10	15	20	25
bins per dimension
29