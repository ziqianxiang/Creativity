Published as a conference paper at ICLR 2020
Deep Batch Active Learning by
Diverse, Uncertain Gradient Lower Bounds
Jordan T. Ash
Princeton University
Chicheng Zhang
University of Arizona
Akshay Krishnamurthy
Microsoft Research NYC
John Langford
Microsoft Research NYC
Alekh Agarwal
Microsoft Research Redmond
Ab stract
We design a new algorithm for batch active learning with deep neural network models. Our
algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples
groups of points that are disparate and high magnitude when represented in a hallucinated
gradient space, a strategy designed to incorporate both predictive uncertainty and sample
diversity into every selected batch. Crucially, BADGE trades off between uncertainty
and diversity without requiring any hand-tuned hyperparameters. While other approaches
sometimes succeed for particular batch sizes or architectures, BADGE consistently
performs as well or better, making it a useful option for real world active learning problems.
1	Introduction
In recent years, deep neural networks have produced state-of-the-art results on a variety of important super-
vised learning tasks. However, many of these successes have been limited to domains where large amounts of
labeled data are available. A promising approach for minimizing labeling effort is active learning, a learning
protocol where labels can be requested by the algorithm in a sequential, feedback-driven fashion. Active
learning algorithms aim to identify and label only maximally-informative samples, so that a high-performing
classifier can be trained with minimal labeling effort. As such, a robust active learning algorithm for deep
neural networks may considerably expand the domains in which these models are applicable.
How should we design a practical, general-purpose, label-efficient active learning algorithm for deep neural
networks? Theory for active learning suggests a version-space-based approach (Cohn et al., 1994; Balcan
et al., 2006), which explicitly or implicitly maintains a set of plausible models, and queries examples for which
these models make different predictions. But when using highly expressive models like neural networks,
these algorithms degenerate to querying every example. Further, the computational overhead of training deep
neural networks precludes approaches that update the model to best fit data after each label query, as is often
done (exactly or approximately) for linear methods (Beygelzimer et al., 2010; Cesa-Bianchi et al., 2009).
Unfortunately, the theory provides little guidance for these models.
One option is to use the network’s uncertainty to inform a query strategy, for example by labeling samples
for which the model is least confident. In a batch setting, however, this creates a pathological scenario where
data in the batch are nearly identical, a clear inefficiency. Remedying this issue, we could select samples
to maximize batch diversity, but this might choose points that provide little new information to the model.
For these reasons, methods that exploit just uncertainty or diversity do not consistently work well across
model architectures, batch sizes, or datasets. An algorithm that performs well when using a ResNet, for
1
Published as a conference paper at ICLR 2020
example, might perform poorly when using a multilayer perceptron. A diversity-based approach might work
well when the batch size is very large, but poorly when the batch size is small. Further, what even constitutes
a “large” or “small” batch size is largely a function of the statistical properties of the data in question. These
weaknesses pose a major problem for real, practical batch active learning situations, where data are unfamiliar
and potentially unstructured. There is no way to know which active learning algorithm is best to use.
Moreover, in a real active learning scenario, every change of hyperparameters typically causes the algorithm
to label examples not chosen under other hyperparameters, provoking substantial labeling inefficiency. That
is, hyperparameter sweeps in active learning can be label expensive. As a result, active learning algorithms
need to “just work”, given fixed hyperparameters, to a greater extent than is typical for supervised learning.
Based on these observations, we design an approach which creates diverse batches of examples about
which the current model is uncertain. We measure uncertainty as the gradient magnitude with respect
to parameters in the final (output) layer, which is computed using the most likely label according to the
model. To capture diversity, we collect a batch of examples where these gradients span a diverse set of
directions. More specifically, we build up the batch of query points based on these hallucinated gradients
using the k-MEANS++ initialization (Arthur and Vassilvitskii, 2007), which simultaneously captures both the
magnitude of a candidate gradient and its distance from previously included points in the batch. We name the
resulting approach Batch Active learning by Diverse Gradient Embeddings (BADGE).
We show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well
as or better than the best baseline across our experiments, which vary all of the aforementioned environmental
conditions. We begin by introducing our notation and setting, followed by a description of the BADGE
algorithm in Section 3 and experiments in Section 4. We defer our discussion of related work to Section 5.
2	Notation and setting
Define [K] := {1, 2, . . . , K}. Denote by X the instance space and by Y the label space. In this work
we consider multiclass classification, so Y = [K]. Denote by D the distribution from which examples
are drawn, by DX the unlabeled data distribution, and by DY |X the conditional distribution over labels
given examples. We consider the pool-based active learning setup, where the learner receives an unlabeled
dataset U sampled according to DX and can request labels sampled according to DY|X for any x ∈
U . We use ED to denote expectation under the data distribution D. Given a classifier h : X → Y,
which maps examples to labels, and a labeled example (x, y), we denote the 0/1 error of h on (x, y) as
'oι(h(x), y) = I(h(χ) = y). The performance of a classifier h is measured by its expected 0/1 error, i.e.
ED ['oι(h(χ), y)] = Pr(x,y)〜D(h(χ) = y). The goal of pool-based active learning is to find a classifier with
a small expected 0/1 error using as few label queries as possible. Given a set S of labeled examples (x, y),
where each x ∈ S is picked from U, followed by a label query, we use ES as the sample averages over S.
In this paper, we consider classifiers h parameterized by underlying neural networks f of fixed architecture,
with the weights in the network denoted by θ. We abbreviate the classifier with parameters θ as hθ since the
architectures are fixed in any given context, and our classifiers take the form hθ(x) = argmaxy∈[K] f(x; θ)y,
where f(x; θ) ∈ RK is a probability vector of scores assigned to candidate labels, given the example x and
parameters θ. We optimize the parameters by minimizing the cross-entropy loss ES['ce(∕(x; θ), y)] over the
labeled examples, where 'ce(p, y) = PK=I I(y = i)ln 1/pi = ln 1/py.
2
Published as a conference paper at ICLR 2020
Algorithm 1 BADGE: Batch Active learning by Diverse Gradient Embeddings
Require: Neural network f(x; θ), unlabeled pool of examples U, initial number of examples M, number of
iterations T, number of examples in a batch B.
1:	Labeled dataset S J M examples drawn uniformly at random from U together with queried labels.
2:	Train an initial model θι on S by minimizing ES['ce(∕(x; θ), y)].
3:	for t = 1, 2, . . . , T : do
4:	For all examples x in U \ S :
1.	Compute its hypothetical label y(x) = hθt(x).
2.	Compute gradient embedding gχ = ∂∂^'cE(f (x; θ),y(x))∣θ=θt, where θ∩ut refers to param-
eters of the final (output) layer.
5:	Compute St, a random subset of U \ S, using the k-MEANS++ seeding algorithm on {gx : x ∈ U \ S}
and query for their labels.
6:	S J S ∪ St .
7:	Train a model θt+ι on S by minimizing ES['ce(∕(x; θ), y)].
8:	end for
9:	return Final model θT+1.
3 Algorithm
BADGE, described in Algorithm 1, starts by drawing an initial set of M examples uniformly at random from
U and asking for their labels. It then proceeds iteratively, performing two main computations at each step t: a
gradient embedding computation and a sampling computation. Specifically, at each step t, for every x in the
pool U, we compute the label y(x) preferred by the current model, and the gradient gχ of the loss on (x, y(x))
with respect to the parameters of the last layer of the network. Given these gradient embedding vectors
{gx : x ∈ U}, BADGE selects a set of points by sampling via the k-MEANS++ initialization scheme (Arthur
and Vassilvitskii, 2007). The algorithm queries the labels of these examples, retrains the model, and repeats.
We now describe the main computations — the embedding and sampling steps — in more detail.
The gradient embedding. Since deep neural networks are optimized using gradient-based methods, we
capture uncertainty about an example through the lens of gradients. In particular, we consider the model
uncertain about an example if knowing the label induces a large gradient of the loss with respect to the model
parameters and hence a large update to the model. A difficulty with this reasoning is that we need to know the
label to compute the gradient. As a proxy, we compute the gradient as if the model’s current prediction on the
example is the true label. We show in Proposition 1 that, assuming a common structure satisfied by most natural
neural networks, the gradient norm with respect to the last layer using this label provides a lower bound on the
gradient norm induced by any other label. In addition, under that assumption, the length of this hypothetical
gradient vector captures the uncertainty of the model on the example: if the model is highly certain about the
example’s label, then the example’s gradient embedding will have a small norm, and vice versa for samples
where the model is uncertain (see example below). Thus, the gradient embedding conveys information both
about the model’s uncertainty and potential update direction upon receiving a label at an example.
The sampling step. We want the newly-acquired labeled samples to induce large and diverse changes to
the model. To this end, we want the selection procedure to favor both sample magnitude and batch diversity.
Specifically, we want to avoid the pathology of, for example, selecting a batch of k similar samples where
even just a single label could alleviate our uncertainty on all remaining (k - 1) samples.
A natural way of making this selection without introducing additional hyperparameters is to sample from
a k-Determinantal Point Process (k-DPP; (Kulesza and Taskar, 2011)). That is, to select a batch of k points
3
Published as a conference paper at ICLR 2020
⅛Labels queried
匕DPP --------- fc-means++
⅛Labels queried
⅛Labels queried
Figure 1: Left and center: Learning curves for k-MEANS++ and k-DPP sampling with gradient embeddings
for different scenarios. The performance of the two sampling approaches nearly perfectly overlaps. Right:
A run time comparison (seconds) corresponding to the middle scenario. Each line is the average over five
independent experiments. Standard errors are shown by shaded regions.
with probability proportional to the determinant of their Gram matrix. Recently, Derezinski and Warmuth
(2018) showed that in experimental design for least square linear regression settings, learning from samples
drawn from a k-DPP can have much smaller mean square prediction error than learning from iid samples.
In this process, when the batch size is very low, the selection will naturally favor points with a large length,
which corresponds to uncertainty in our space. When the batch size is large, the sampler focuses more on
diversity because linear independence, which is more difficult to achieve for large k, is required to make
the Gram determinant non-zero.
Unfortunately, sampling from a k-DPP is not trivial. Many sampling algorithms (Kang, 2013; Anari et al.,
2016) rely on MCMC, where mixing time poses a significant computational hurdle. The state-of-the-art
algorithm of Derezinski (2018) has a high-order polynomial running time in the batch size and the embedding
dimension. To overcome this computational hurdle, we suggest instead sampling using the k-MEANS++
seeding algorithm (Arthur and Vassilvitskii, 2007), originally made to produce a good initialization for
k-means clustering. k-MEANS++ seeding selects centroids by iteratively sampling points in proportion to
their squared distances from the nearest centroid that has already been chosen, which, like a k-DPP, tends
to select a diverse batch of high-magnitude samples. For completeness, we give a formal description of the
k-MEANS++ seeding algorithm in Appendix A.
Example: multiclass classification with softmax activations. Consider a neural network f where the last
nonlinearity is a softmax, i.e. σ(z)i = ezi/PjK=1 ezj . Specifically, f is parametrized by θ = (W, V ), where
θout = W = (W1 , . . . , WK)> ∈ RK ×d are the weights of the last layer, and V consists of weights of all
previous layers. This means that f (x; θ) = σ(W ∙ ζ(x; V)), where Z is the nonlinear function that maps
an input x to the output of the network’s penultimate layer. Let us fix an unlabeled sample x and define
pi = f(x; θ)i. With this notation, we have
'cE(f(x; θ),y)=ln (X eWj∙z(x;V) - Wy ∙ z(x; V).
Define gXy =吊'ce(∕(x; θ), y) for a label y and gx = gy as the gradient embedding in our algorithm, where
y = argmaxi∈[κ] pi. Then the i-th block of gχ (i.e. the gradients corresponding to label i) is
∂
(gχ)i = mλτ'cE(f (x； θ),y~) = (Pi- I (y = Z))Z(X;V).	⑴
∂Wi
Based on this expression, we can make the following observations:
4
Published as a conference paper at ICLR 2020
1.	Each block of gx is a scaling of z(x; V ), which is the output of the penultimate layer of the network.
In this respect, gx captures x’s representation information similar to that of Sener and Savarese
(2018).
2.	Proposition 1 below shows that the norm of gx is a lower bound on the norm of the loss gradient
induced by the example with true label y with respect to the weights in the last layer, that is
kgx k ≤ kgxy k. This suggests that the norm of gx conservatively estimates the example’s influence on
the current model.
3.	If the current model θ is highly confident about x, i.e. vector p is skewed towards a standard basis
vector ej, then y = j, and vector (Pi - I(y = i))K=ι has a small length. Therefore, gχ has a
small length as well. Such high-confidence examples tend to have gradient embeddings of small
magnitude, which are unlikely to be repeatedly selected by k-MEANS++ at iteration t.
Proposition 1. Forall y ∈ {1,..., K} ,let gX = ∂w 'ce(∕ (x； θ), y) ∙ Then
K
kgxyk2 = Xpi2 + 1 - 2py kz(x; V)k2.
i=1
Consequently, y = argminy∈[κ] ∣∣g^ ∣∣.
Proof. Observe that by Equation (1),
KK
kgy k2 = X(Pi- I(y = i))2kz(x; V)k2 = (Xp2 + 1 - 2py) kz(x; V)k2.
i=1	i=1
The second claim follows from the fact that y = argmaxy∈ 陷 Py.	□
This simple sampler tends to produce diverse batches similar to a k-DPP. As shown in Figure 1, switching
between the two samplers does not affect the active learner’s statistical performance but greatly improves
its computational performance. Appendix G compares run time and test accuracy for both k-MEANS++ and
k-DPP based sampling based on the gradient embeddings of the unlabeled examples.
Figure 2 illustrates the batch diversity and average gradient magnitude per selected batch for a variety of
sampling strategies. As expected, both k-DPPs and k-MEANS++ tend to select samples that are diverse
(as measured by the magnitude of their Gram determinant) and high magnitude. Other samplers, such
as furthest-first traversal for k-Center clustering (FF-k-CENTER), do not seem to have this property. The
FF-k-CENTER algorithm is the sampling choice of the CORESET approach to active learning, which we
describe in the proceeding section (Sener and Savarese, 2018). Appendix F discusses diversity with respect
to uncertainty-based approaches.
Appendix B provides further justification for why BADGE yields better updates than vanilla uncertainty
sampling in the special case of binary logistic regression (K = 2 and z(x; V) = x).
4	Experiments
We evaluate the performance of BADGE against several algorithms from the literature. In our experiments,
we seek to answer the following question: How robust are the learning algorithms to choices of neural network
architecture, batch size, and dataset?
To ensure a comprehensive comparison among all algorithms, we evaluate them in a batch-mode active
learning setup with M = 100 being the number of initial random labeled examples and batch size B varying
from {100, 1000, 10000}. The following is a list of the baseline algorithms evaluated; the first performs
representative sampling, the next three are uncertainty based, the fifth is a hybrid of representative and
uncertainty-based approaches, and the last is traditional supervised learning.
5
Published as a conference paper at ICLR 2020
∕c-DPP
Rand --------- FF κ-ceπter
j=Seq ∣o IUeU - uuα,sp CTOJ
#Labels queried
κ-meaπs++
Figure 2: A comparison of batch selection algorithms using our gradient embedding. Left and center: Plots
showing the log determinant of the Gram matrix of the selected batch of gradient embeddings as learning
progresses. Right: The average embedding magnitude (a measurement of predictive uncertainty) in the
selected batch. The FF-k-CENTER sampler finds points that are not as diverse or high-magnitude as other
samplers. Notice also that k-MEANS++ tends to actually select samples that are both more diverse and
higher-magnitude than a k-DPP, a potential pathology of the k-DPP’s degree of stochastisity. Standard errors
are shown by shaded regions.
1.	Coreset: A diversity-based approach using coreset selection. The embedding of each example
is computed by the network’s penultimate layer and the samples at each round are selected using
a greedy furthest-first traversal conditioned on all labeled examples (Sener and Savarese, 2018).
2.	CONF (Confidence Sampling): An uncertainty-based active learning algorithm that selects B
examples with smallest predicted class probability, maxiK=1 f(x; θ)i (e.g. Wang and Shang, 2014).
3.	MARG (Margin Sampling): An uncertainty-based active learning algorithm that selects the bottom B
examples sorted according to the example's multiclass margin, defined as f (x; θ)y - f (x; θ)y,, where
y and y0 are the indices of the largest and second largest entries of f (x; θ) (Roth and Small, 2006).
4.	ENTROPY: An uncertainty-based active learning algorithm that selects the top B examples
according to the entropy of the example’s predictive class probability distribution, defined as
H((f(x; θ)y)yK=1), whereH(p) = PiK=1 pi ln 1/pi (Wang and Shang, 2014).
5.	ALBL (Active Learning by Learning): A bandit-style meta-active learning algorithm that selects
between Coreset and Conf at every round (Hsu and Lin, 2015).
6.	RAND: The naive baseline of randomly selecting k examples to query at each round.
We consider three neural network architectures: a two-layer Perceptron with ReLU activations (MLP),
an 18-layer convolutional ResNet (He et al., 2016), and an 11-layer VGG network (Simonyan and
Zisserman, 2014). We evaluate our algorithms using three image datasets, SVHN (Netzer et al., 2011),
CIFAR10 (Krizhevsky, 2009) and MNIST (LeCun et al., 1998) 1, and four non-image datasets from the
OpenML repository (#6, #155, #156, and #184). 2 We study each situation with 7 active learning algorithms,
including BADGE, making for 231 total experiments.
For the image datasets, the embedding dimensionality in the MLP is 256. For the OpenML datasets, the
embedding dimensionality of the MLP is 1024, as more capacity helps the model fit training data. We fit
1Because MNIST is a dataset that is extremely easy to classify, we only use MLPs, rather than convolutional networks,
to better study the differences between active learning algorithms.
2The OpenML datasets are from openml.org and are selected on two criteria: first, they have at least 10000
samples; second, neural networks have a significantly smaller test error rate when compared to linear models.
6
Published as a conference paper at ICLR 2020
ALBL -------- Conf -------- Coreset -------- BADGE ---------- Entropy ------- Marg --------- Rand
#Labels queried	#Labels queried	#Labels queried
(a)
(b)	(c)
Figure 3: Active learning test accuracy versus the number of total labeled samples for a range of conditions.
Standard errors are shown by shaded regions.
models using cross-entropy loss and the Adam variant of SGD until training accuracy exceeds 99%. We
use a learning rate of 0.001 for image data and of 0.0001 for non-image data. We avoid warm starting and
retrain models from scratch every time new samples are queried (Ash and Adams, 2019). All experiments are
repeated five times. No learning rate schedules or data augmentation are used. Baselines use implementations
from the libact library (Yang et al., 2017). All models are trained in PyTorch (Paszke et al., 2017).
Learning curves. Here we show examples of learning curves that highlight some of the phenomena we
observe related to the fragility of active learning algorithms with respect to batch size, architecture, and dataset.
Often, we see that in early rounds of training, it is better to do diversity sampling, and later in training, it is
better to do uncertainty sampling. This kind of event is demonstrated in Figure 3a, which shows Coreset
outperforming confidence-based methods at first, but then doing worse than these methods later on.
In this figure, BADGE performs as well as diversity
sampling when that strategy does best, and as well
as uncertainty sampling once those methods start
outpacing Coreset. This suggests that BADGE
is a good choice regardless of labeling budget.
Separately, we notice that diversity sampling only
seems to work well when either the model has
good architectural priors (inductive biases) built
in, or when the data are easy to learn. Otherwise,
penultimate layer representations are not meaning-
ful, and diverse sampling can be deleterious. For
this reason, Coreset often performs worse than
random on sufficiently complex data when not using
a convolutional network (Figure 3b). That is, the
diversity induced by unconditional random sampling
can often yield a batch that better represents the data.
Even when batch size is large and the model has
helpful inductive biases, the uncertainty information
in BADGE can give it an advantage over pure
diversity approaches (Figure 3c). Comprehensive
plots of this kind, spanning architecture, dataset,
and batch size are in Appendix C.
BADGE ALBL Coreset Conf Marg Entropy Rand
Figure 4: A pairwise penalty matrix over all experiments.
Element Pij corresponds roughly to the number of times
algorithm i outperforms algorithm j. Column-wise averages
at the bottom show overall performance (lower is better).
7
Published as a conference paper at ICLR 2020
Pairwise comparisons. We next show a comprehensive pairwise comparison of algorithms over all datasets
(D), batch sizes (B), model architectures (A), and label budgets (L). From the learning curves, it can be ob-
served that when label budgets are large enough, all algorithms eventually reach similar performance, making
the comparison between them uninteresting in the large sample limit. For this reason, for each combination of
(D, B, A), we select a set of labeling budgets L where learning is still progressing. We experimented with three
different batch sizes and eleven dataset-architecture pairs, making the total number of (D, B, A) combinations
3 × 11 = 33. Specifically, we compute n0, the smallest number of labels where RAND’s accuracy reaches
99% of its final accuracy, and choose label budget L from {M + 2m-1B : m ∈ [[log((no - M)∕B)[]}.
The calculation of scores in the penalty matrix P follows the following protocol: For each (D, B, A, L)
combination and each pair of algorithms (i,j), We have 5 test errors (one for each repeated run),
ei1 , . . . , ei5 and ej1, . . .,ej} respectively. We compute the t-score as t = √5μ∕^, where
15	u15
μ = 5 X(ei- ej),	σ = t 4 X(ei- ej- μ)2.
5 l=1	4 l=1
We use the two-sided t-test to compare pairs of algo-
rithms: algorithm i is said to beat algorithm j in this
setting if t > 2.776 (the critical point of p-value be-
ing 0.05), and similarly algorithm j beats algorithm i
if t < -2.776. For each (D, B, A) combination, suppose
there are nD,B,A different values of L. Then, for each L,
if algorithm i beats algorithm j , we accumulate a penalty
of 1∕nD,B,A to Pi,j; otherwise, if algorithm j beats al-
gorithm i, we accumulate a penalty of 1∕nD,B,A toPj,i.
The choice of the penalty value 1∕nD,B,A is to ensure that
every (D, B, A) combination is assigned equal influence
in the aggregated matrix. Therefore, the largest entry of P
is at most 33, the total number of (D, B, A) combinations.
Normalized error
----BADGE ------- Coreset ------ Marg -------- Rand
---ALBL --------- Conf -------- Entropy
Figure 5: The cumulative distribution function of
normalized errors for all acquisition functions.
Intuitively, each row i indicates the number of settings in which algorithm i beats other algorithms and each
column j indicates the number of settings in which algorithm j is beaten by another algorithm.
The penalty matrix in Figure 4 summarizes all experiments, showing that BADGE generally outperforms
baselines. Matrices grouped by batch size and architecture in Appendix D show a similar trend.
Cumulative distribution functions of normalized errors. For each (D, B, A, L) combination, we com-
pute the average error for each algorithm i as & = 1 P5=ι ei∙ To ensure that the errors of these algorithms
are on the same scale in all settings, we compute the normalized error of every algorithm i, defined as
nei = βi∕βr, where r is the index of the RAND algorithm. By definition, the normalized errors of the RAND
algorithm are identically 1 in all settings. Like with penalty matrices, for each (D, B, A) combination, we only
consider a subset of L values from the set M + 2m-1B : m ∈ [blog((n0 - M)∕B)c] . We assign a weight
proportional to 1∕nD,B,A to each (D, B, A, L) combination, where there are nD,B,A different L values for
this combination of (D, B, A). We then plot the cumulative distribution functions (CDFs) of the normalized
errors of all algorithms: for a value of x, the y value is the total weight of settings where the algorithm has
normalized error at most x; in general, an algorithm that has a higher CDF value has better performance.
We plot the generated CDFs in Figures 5, 22 and 23. We can see from Figure 5 that BADGE has the best
overall performance. In addition, from Figures 22 and 23 in Appendix E, we can conclude that when batch
size is small (100 or 1000) or when an MLP is used, both BADGE and Marg perform best. However, in the
regime when the batch size is large (10000), Marg’s performance degrades, while BADGE, ALBL and
Coreset are the best performing approaches.
8
Published as a conference paper at ICLR 2020
5 Related work
Active learning is a been well-studied problem (Settles, 2010; Dasgupta, 2011; Hanneke, 2014). There are
two major strategies for active learning—representative sampling and uncertainty sampling.
Representative sampling algorithms select batches of unlabeled examples that are representative of the
unlabeled set to ask for labels. It is based on the intuition that the sets of representative examples chosen,
once labeled, can act as a surrogate for the full dataset. Consequently, performing loss minimization on
the surrogate suffices to ensure a low error with respect to the full dataset. In the context of deep learning,
Sener and Savarese (2018); Geifman and El-Yaniv (2017) select representative examples based on core-set
construction, a fundamental problem in computational geometry. Inspired by generative adversarial learning,
Gissin and Shalev-Shwartz (2019) select samples that are maximally indistinguishable from the pool of
unlabeled examples.
On the other hand, uncertainty sampling is based on a different principle—to select new samples that
maximally reduce the uncertainty the algorithm has on the target classifier. In the context of linear classifi-
cation, Tong and Koller (2001); Schohn and Cohn (2000); Tur et al. (2005) propose uncertainty sampling
methods that query examples that lie closest to the current decision boundary. Some uncertainty sampling
approaches have theoretical guarantees on statistical consistency (Hanneke, 2014; Balcan et al., 2006). Such
methods have also been recently generalized to deep learning. For instance, Gal et al. (2017) use Dropout
as an approximation of the posterior of the model parameters, and develop information-based uncertainty
reduction criteria; inspired by recent advances on adversarial examples generation, Ducoffe and Precioso
(2018) use the distance between an example and one of its adversarial examples as an approximation of
its distance to the current decision boundary, and uses it as the criterion of label queries. An ensemble of
classifiers could also be used to effectively estimate uncertainty (Beluch et al., 2018).
There are several existing approaches that support a hybrid of representative sampling and uncertainty sam-
pling. For example, Baram et al. (2004); Hsu and Lin (2015) present meta-active learning algorithms that
can combine the advantages of different active learning algorithms. Inspired by expected loss minimization,
Huang et al. (2010) develop label query criteria that balances between the representativeness and informa-
tiveness of examples. Another method for this is Active Learning by Learning (Hsu and Lin, 2015), which
can select whether to exercise a diversity based algorithm or an uncertainty based algorithm at each round of
training as a sequential decision process.
There is also a large body of literature on batch mode active learning, where the learner is asked to select a
batch of samples within each round (Guo and Schuurmans, 2008; Wang and Ye, 2015; Chen and Krause;
Wei et al., 2015; Kirsch et al., 2019). In these works, batch selection is often formulated as an optimization
problem with objectives based on (upper bounds of) average log-likelihood, average squared loss, etc.
A different query criterion based on expected gradient length (EGL) has been proposed in the as well (Settles
et al., 2008). In recent work, Huang et al. (2016) show that the EGL criterion is related to the T -optimality
criterion in experimental design. They further demonstrate that the samples selected by EGL are very different
from those by entropy-based uncertainty criterion. Zhang et al. (2017a) use the EGL criterion in active
sentence and document classification with CNNs. These approaches differ most substantially from BADGE
in that they do not take into account the diversity of the examples queried within each batch.
There is a wide array of theoretical articles that focus on the related problem of adaptive subsampling for
fully-labeled datasets in regression settings (Han et al., 2016; Wang et al., 2018; Ting and Brochu, 2018).
Empirical studies of batch stochastic gradient descent also employ adaptive sampling to “emphasize” hard or
representative examples (Zhang et al., 2017b; Chang et al., 2017). These works aim at reducing computation
costs or finding a better local optimal solution, as opposed to reducing label costs. Nevertheless, our work is
inspired by their sampling criteria, which also emphasize samples that induce large updates to the model.
9
Published as a conference paper at ICLR 2020
As mentioned earlier, our sampling criterion has resemblance to sampling from k-determinantal point
processes (Kulesza and Taskar, 2011). Note that in multiclass classification settings, our gradient-based
embedding of an example can be viewed as the outer product of the original embedding in the penultimate
layer and a probability score vector that encodes the uncertainty information on this example (see Section 3).
In this view, the penultimate layer embedding characterizes the diversity of each example, whereas the
probability score vector characterizes the quality of each example. The k-DPP is also a natural probabilistic
tool for sampling that trades off between quality and diversity (See Kulesza et al., 2012, Section 3.1). We
remark that concurrent to our work, Biyik et al. (2019) develops k-DPP based active learning algorithms
based on this principle by explicitly designing diversity and uncertainty measures.
6 Discussion
We have established that BADGE is empirically an effective deep active learning algorithm across different
architectures and batch sizes, performing similar to or better than other active learning algorithms. A funda-
mental remaining question is: "Why?" While deep learning is notoriously difficult to analyze theoretically,
there are several intuitively appealing properties of BADGE:
1.	The definition of uncertainty (a lower bound on the gradient magnitude of the last layer) guarantees
some update of parameters.
2.	It optimizes for diversity as well as uncertainty, eliminating a failure mode of choosing many
identical uncertain examples in a batch, and does so without requiring any hyperparameters.
3.	The randomization associated with the k-MEANS++ initialization sampler implies that, even for
adversarially constructed datasets, it eventually converges to a good solution.
The combination of these properties appears to generate the robustness that we observe empirically.
References
David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine learning,
1994.
Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In International
Conference on Machine Learning, 2006.
Alina Beygelzimer, Daniel J Hsu, John Langford, and Tong Zhang. Agnostic active learning without
constraints. In Neural Information Processing Systems, 2010.
Nicolo Cesa-Bianchi, Claudio Gentile, and Francesco Orabona. Robust bounds for classification via selective
sampling. In International Conference on Machine Learning, 2009.
David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In ACM-SIAM
symposium on Discrete algorithms, 2007.
Alex Kulesza and Ben Taskar. k-dpps: Fixed-size determinantal point processes. In International Conference
on Machine Learning, 2011.
MichaI Derezinski and Manfred K Warmuth. Reverse iterative volume sampling for linear regression. The
Journal of Machine Learning Research, 19(1), 2018.
Byungkon Kang. Fast determinantal point process sampling with application to clustering. In Neural
Information Processing Systems, 2013.
10
Published as a conference paper at ICLR 2020
Nima Anari, Shayan Oveis Gharan, and Alireza Rezaei. Monte carlo markov chain algorithms for sampling
strongly rayleigh distributions and determinantal point processes. In Conference on Learning Theory, 2016.
MichaI Derezinski. Fast determinantal point processes via distortion-free intermediate sampling. arXiv
preprint, 2018.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In
International Conference on Learning Representations, 2018.
Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint
conference on neural networks, 2014.
Dan Roth and Kevin Small. Margin-based active learning for structured output spaces. In European
Conference on Machine Learning, 2006.
Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In Association for the advancement of
artificial intelligence, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings ofthe IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint, 2014.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. IEEE, 1998.
Jordan T Ash and Ryan P Adams. On the difficulty of warm-starting neural network training. arXiv preprint,
2019.
Yao-Yuan Yang, Shao-Chuan Lee, Yu-An Chung, Tung-En Wu, Si-An Chen, and Hsuan-Tien Lin. libact:
Pool-based active learning in python. arXiv preprint, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Burr Settles. Active learning literature survey. University of Wisconsin, Madison, 2010.
Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 2011.
Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends in Machine Learning,
2014.
Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. arXiv preprint, 2017.
Daniel Gissin and Shai Shalev-Shwartz. Discriminative active learning. arXiv preprint, 2019.
Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification.
Journal of machine learning research, 2001.
Greg Schohn and David Cohn. Less is more: Active learning with support vector machines. In International
Conference on Machine Learning, 2000.
11
Published as a conference paper at ICLR 2020
Gokhan Tur, Dilek Hakkani-Tur, and Robert E SchaPire. Combining active and semi-supervised learning for
spoken language understanding. Speech Communication, 2005.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In
International Conference on Machine Learning, 2017.
Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin based
approach. arXiv preprint, 2018.
William H Beluch, Tim Genewein, Andreas Nurnberger, and Jan M Kohler. The power of ensembles for
active learning in image classification. In IEEE Conference on Computer Vision and Pattern Recognition,
2018.
Yoram Baram, Ran El Yaniv, and Kobi Luz. Online choice of active learning algorithms. Journal of Machine
Learning Research, 2004.
Sheng-Jun Huang, Rong Jin, and Zhi-Hua Zhou. Active learning by querying informative and representative
examples. In Neural Information Processing Systems, 2010.
Yuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. In Neural Information
Processing Systems, 2008.
Zheng Wang and Jieping Ye. Querying discriminative and representative samples for batch mode active
learning. Transactions on Knowledge Discovery from Data, 2015.
Yuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular
optimization. In International Conference on Machine Learning.
Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In
International Conference on Machine Learning, 2015.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for
deep bayesian active learning. In Neural Information Processing Systems 32, 2019.
Burr Settles, Mark Craven, and Soumya Ray. Multiple-instance active learning. In Neural Information
Processing Systems, 2008.
Jiaji Huang, Rewon Child, and Vinay Rao. Active learning for speech recognition: the power of gradients.
arXiv preprint, 2016.
Ye Zhang, Matthew Lease, and Byron C Wallace. Active discriminative text representation learning. In AAAI
Conference on Artificial Intelligence, 2017a.
Lei Han, Kean Ming Tan, Ting Yang, and Tong Zhang. Local uncertainty sampling for large-scale multi-class
logistic regression. arXiv preprint, 2016.
HaiYing Wang, Rong Zhu, and Ping Ma. Optimal subsampling for large sample logistic regression. Journal
of the American Statistical Association, 2018.
Daniel Ting and Eric Brochu. Optimal subsampling with influence functions. In Neural Information
Processing Systems, 2018.
Cheng Zhang, Hedvig Kjellstrom, and Stephan Mandt. Determinantal point processes for mini-batch
diversification. Uncertainty in Artificial Intelligence, 2017b.
12
Published as a conference paper at ICLR 2020
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neural
networks by emphasizing high variance samples. In Neural Information Processing Systems, 2017.
Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Foundations and Trends
in Machine Learning, 2012.
Erdem Bιyιk, Kenneth Wang, Nima Anari, and Dorsa Sadigh Batch active learning using determinantal point
processes. arXiv preprint, 2019.
Stephen Mussmann and Percy S Liang. Uncertainty sampling is preconditioned stochastic gradient descent
on zero-one loss. In Neural Information Processing Systems, 2018.
A THE k-MEANS++ SEEDING ALGORITHM
Here we briefly review the k-MEANS++ seeding algorithm by (Arthur and Vassilvitskii, 2007). Its basic
idea is to perform sequential sampling of k centers, where each new center is sampled from the ground
set with probability proportional to the squared distance to its nearest center. It is shown in (Arthur and
Vassilvitskii, 2007) that the set of centers returned is guaranteed to approximate the k-means objective
function in expectation, thus ensuring diversity.
Algorithm 2 The k-MEANS++ seeding algorithm (Arthur and Vassilvitskii, 2007)
Require: Ground set G ⊂ Rd, target size k.
Ensure: Center set C of size k.
Ci J {cι}, where ci is sampled uniformly at random from G.
for t = 2, . . . , k : do
DefineDt(x) := minc∈Ct-1 kx - ck2.
Ct J Sample X from G with probability P DDt(X)2.
Ct J Ct-i ∪ {ct}.
end for
return Ck . B
B BADGE for binary logistic regression
1+e-z
We consider instantiating BADGE for binary logistic regression, where Y = {-1, +1}. Given a linear
classifier w, we define the predictive probability of W on X as Pw(y∖x, θ) = σ(yw ∙ x), where σ(z) — —1—
is the sigmoid funciton.
Recall that y = y(x) is the hallucinated label:
+1, pw (+1∖X, θ) > 1/2,
y(X) = -1, pw(+1∖X,θ) ≤ 1/2.
The binary logistic loss of classifier w on example (X, y) is defined as:
'(w, (x, y)) = ln(1 + exp(-yw ∙ x)).
Now, given model W and example x, we define gχ = ∂L'(w, (χ,y)) = (1 一Pw (y∖χ, θ)) ∙ (-y ∙ x) as the loss
gradient induced by the example with hallucinated label, and Ox = ∂L '(w, (x,y)) = (I-Pw (y∖χ, θ))∙(-y ∙x)
as the loss gradient induced by the example with true label.
13
Published as a conference paper at ICLR 2020
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 6: Full learning curves for OpenML #6 with MLP.
#Labels queried	#Labels queried	#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 7: Full learning curves for OpenML #155 with MLP.
Suppose that BADGE only selects examples from region Sw = {x : W ∙ X = 0}, then as Pw (+1|x, θ)=
Pw (-1∣χ,θ) = 1, we have that for all X in Sw, gχ = Sx ∙ gχ for some Sx ∈ {±1}. This implies that, sampling
from a DPP induced by gx's is equivalent to sampling from a DPP induced by gx's. It is noted in Mussmann
and Liang (2018) that uncertainty sampling (i.e. sampling from D|Sw) implicitly performs preconditioned
stochastic gradient descent on the expected 0-1 loss. In addition, it has been shown that DPP sampling over
gradients may reduce the variance of the mini-batch stochastic gradient updates (Zhang et al., 2017b); this
suggests that BADGE, when restricted its sampling over low-margin regions (Sw), improves over uncertainty
sampling by collecting examples that together induce lower-variance updates on the gradient direction of
expected 0-1 loss.
C All learning curves
We plot all learning curves (test accuracy as a function of the number of labeled example queried) in Figures 6
to 12. In addition, we zoom into regions of the learning curves that discriminates the performance of all
algorithms in Figures 13 to 19.
D	Pairwise comparis ons of algorithms
In addition to Figure 4 in the main text, we also provide penalty matrices (Figures 20 and 21), where the
results are aggregated by conditioning on a fixed batch size (100, 1000 and 10000) or on a fixed neural
network model (MLP, ResNet and VGG). For each penalty matrix, the parenthesized number in its title is the
14
Published as a conference paper at ICLR 2020
Accuracy	Accuracy
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 8: Full learning curves for OpenML #156 with MLP.
2500 5000 7500 1000012500150001750020000
#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 9: Full learning curves for OpenML #184 with MLP.
15
Published as a conference paper at ICLR 2020
Accuracy	Accuracy	Accuracy	Accuracy
#Labels queried	#Labels queried	#Labels queried
#Labels queried	#Labels queried	#Labels queried
SVHN1 VGG1 Batch size: 100
#Labels queried
#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 10: Full learning curves for SVHN with MLP, ResNet and VGG.
#Labels queried	#Labels queried	#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 11: Full learning curves for MNIST with MLP.
16
Published as a conference paper at ICLR 2020
Accuracy	Accuracy	Accuracy	Accuracy
#Labels queried
#Labels queried
#Labels queried	#Labels queried
5000 10000 15000 20000 25000 300∞ 350∞ 400∞
#Labels queried
5000 10000 15000 20000 25000 300∞ 350∞ 400∞
#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 12: Full learning curves for CIFAR10 with MLP, ResNet and VGG.
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 13:	Zoomed-in learning curves for OpenML #6 with MLP.
17
Published as a conference paper at ICLR 2020
#Labels queried
OPenML#155, MLP, Batch size: 1000
OPenML#155, MLP, Batch size: 10000
#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 14:	Zoomed-in learning curves for OpenML #155 with MLP.
OPenML#156, MLP1 Batch size: 1000
5000 10000 15000 20000 25000 300∞ 350∞ 400∞
#Labels queried
ALBL -------- Conf --------- Coreset -------- BADGE ---------- Entropy ------- Marg --------- Rand
Figure 15:	Zoomed-in learning curves for OpenML #156 with MLP.
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 16:	Zoomed-in learning curves for OpenML #184 with MLP.
18
Published as a conference paper at ICLR 2020
Accuracy	Accuracy	Accuracy	Accuracy
SVHN1 MLP1 Batch size: 100
#Labels queried
#Labels queried
#Labels queried
SVHN1 VGG1 Batch size: 100
#Labels queried
#Labels queried
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
#Labels queried
#Labels queried
SVHN1 VGG1 Batch size: 10000
5000 10000 15000 20000 25000 300∞ 350∞ 400∞
#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 17: Zoomed-in learning curves for SVHN with MLP, ResNet and VGG.
#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 18:	Zoomed-in learning curves for MNIST with MLP.
19
Published as a conference paper at ICLR 2020
Accuracy	Accuracy	Accuracy
#Labels queried
CIFAR10, MLP1 Batch size: 1OOO
50∞ 10000 15000 20000 25000 300∞ 350∞ 400∞
#Labels queried
#Labels queried	#Labels queried	#Labels queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 19:	Zoomed-in learning curves for CIFAR10 with MLP, ResNet and VGG.
20
Published as a conference paper at ICLR 2020
Batch size: IOO(Il)
BADGE ALBL Coreset Conf Marg Entropy Rand
Batch size: IOOO(Il)
BADGE ALBL Coreset COnf Marg Entropy Rand
Batch size: 10000(11)
BADGE ALBL Coreset COnf Marg Entropy Rand
Aoalnoo4
BADGE
ALBL
Coreset
Conf
Marg
Entropy
Rand
0.0	4.44	5.35	5.36 ∣ 0.76 ∣ 5.B1	4.09
0.2	0.0	2.71	1.8	0.31	3.19	1.49
0.2	0.34	0.0	2.71	0.87	3.2B	1.76
0.21	0.67	3.021	0.0	0.0	2.08	2.1B
0.46	3.9	4.77	4.B2	0.0	5.25	3.77
0.31	0.59	2.6	I	0.33	0.1	0.0	1.5B
0.2	2.26	3.53	3.45	0.3	4.53	0.0
0.23	1.74	3.14	2.64	0.33	345	2.12
-5
-4
-3
-2
-1
-0
BADGE
ALBL
Coreset
Conf
Marg
Entropy
Rand
0.0	3.5B	3.96	4.37	1.96	∣	4.B5	4.37
0.14	0.0	2.14	1.02	0.0	2.12	2.2
0.62	1.19	0.0	2.0	1.04	3.02	2.8
0.0	1.12	3.261	0.0	0.0	2.71	2.65
0.17	2.24	4.1 J	3.51	0.0 | 4.5B	3.99
0.0	0.62	2.451	0.33	0.0	0.0	11.95
0.14	2.2	2.5	3.05	1.84	3.95	0.0
0.15	1.56	2.63	2.04	0.69	3.03	2.57
BADGE
ALBL
Co re set
Conf
Marg
Entropy
Rand
0.0	1.17	1.67	2.B3 ∣ 1.17 ∣ 2.5	1.58
0.0	0.0	0.33	0.33	0.0	1.5	1.25
0.63	0.5	0.0	1.B3	1.17	1.83	2.0
0.33	1.0	1.33	0.0	0.33	1.17	1.25
0.33	1.0	2.0	1.0	0.0	1.5	1.5B
0.0	0.58	1.0	0.58	0.25	0.0	11.58
0.5	1.17	1.58	2.17	0.83	2.17	0.0
0.29	0.77	1.13	1.25	0.54	1.52	1.32
-2.5
-2.0
-1.5
-1.0
-0.5
-0.0
Figure 20:	Pairwise penalty matrices of the algorithms, grouped by different batch sizes. The parenthesized
number in the title is the total number of (D, B, A) combinations aggregated, which is also an upper bound
on all its entries. Element (i, j) corresponds roughly to the number of times algorithm i beats algorithm j.
Column-wise averages at the bottom show aggregate performance (lower is better). From left to right: batch
size = 100, 1000, 10000.
MLP(21)	ResNet(6)	VGG(6)
BADGE ALBL Coreset Conf Marg Entropy Rand	BADGE ALBL Coreset COnf Marg Entropy Rand	BADGE ALBL Coreset COnf Marg Entropy Rand
AoaIn004
BADGE	0.0	8.31	10.67	11.0	2.77	10.94	B.39	-10	BADGE	0.0	0.29	0.3	1.03	0.82	1.3	0.64	-1.6	BADGE	0.0	0.59	0.0	0.53	0.29	0.92	1.02	-1.6
ALBL	0.0	0.0	5.18	2.33	0.11	5.2	3.26		ALBL	0.34	0.0	0.0	0.49	0.1	O-BB	0.39	-1.4	ALBL	0.0	0.0	0.0	0.33	0.1	0.73	1.3	-1.4
Coreset	1.17	1.64	0.0	5.32	2.4	5.9	4.02	-8	Coreset	0.2	0.0	0.0	1.03	0.53	1.2B	0.B4	-1.2	Coreset	0.29	0.39	0.0	0.2	0.14	0.96	1.69	-1.2
Conf	0.21	2.31	7.12	0.0	0.0	5.96	4.77	-6	Conf	0.0	0.14	0.34	0.0	0.0	0.0	0.64	-1.0	Conf	0.33	0.33	0.14	0.0	0.33	0.0	0.68	-1.0
M*r≡	0.62	7.0	10.2	8.65	0.0	10.34	7.45	-4	Marg	0.0	0.0	0.34	0.49	0.0	0.59	0.64	-0.8	Marg	0.33	0.14	0.33	0.2	0.0	0.4	1.25	-0.8
																	-0.6									-0.6
Entropy	0.11	1.1	5.81	0.67	0.0	0.0	3.66		Entropy	0.1	0.58	0.24	0.58	0.35	0.0	0.83		Entropy	0.1	0.1	0.0	0.0	0.0	0.0	0.63	-0.4
																	-0.4									
Rand	0.5	4.B	6.82	6.44	1.49	7.92	0.0	-2	Rand	0.1	0.29	0.59	1.4	1.2	1.75	0.0	-0.2	Rand	0.24	0.54	0.2	0.82	0.29	0.9B	0.0	0.2
																										
	0.37	3.59	6.54	4.92	0.97	6.61	4.51	-0		0.11	0.19	0.26	0.72	0.43	0.63	0.57	-0.0		0.19	0.3	0.1	0.3	0.16	0.57	0.94	-0.0
Figure 21:	Pairwise penalty matrices of the algorithms, grouped by different neural network models. The
parenthesized number in the title is the total number of (D, B, A) combinations aggregated, which is also an
upper bound on all its entries. Element (i, j) corresponds roughly to the number of times algorithm i beats
algorithm j . Column-wise averages at the bottom show aggregate performance (lower is better). From left to
right: MLP, ResNet and VGG.
total number of (D, B, A) combinations aggregated; as discussed in Section 4, this is also an upper bound on
all its entries. It can be seen that uncertainty-based methods (e.g. Marg) perform well only in small batch
size regimes (100) or when using MLP models; representative sampling based methods (e.g. Coreset)
only perform well in large batch size regimes (10000) or when using ResNet or VGG models. In contrast,
BADGE’s performance is competitive across all batch sizes and neural network models.
21
Published as a conference paper at ICLR 2020
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 22: CDFs of normalized errors of the algorithms, group by different batch sizes. Higher CDF indicates
better performance. From left to right: batch size = 100, 1000, 10000.
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 23:	CDFs of normalized errors of the algorithms, group by different neural network models. Higher
CDF indicates better performance. From left to right: MLP, ResNet and VGG.
E	CDFs of normalized errors of different algorithms
In addition to Figure 5 that aggregates over all settings, we show here the CDFs of normalized errors by
conditioning on fixed batch sizes (100, 1000 and 10000) in Figure 22, and show the CDFs of normalized
errors by conditioning on fixed neural network models (MLP, ResNet and VGG) in Figure 23.
F	Batch uncertainty and diversity
Figure 24 gives a comparison of sampling methods with gradient embedding in two settings (OpenML # 6,
MLP, batchsize 100 and SVHN, ResNet, batchsize 1000), in terms of uncertainty and diversity of examples
selected within batches. These two properties are measured by average `2 norm and determinant of the Gram
matrix of gradient embedding, respectively. It can be seen that, k-MEANS++ (BADGE) induces good batch
diversity in both settings. Conf generally selects examples with high uncertainty, but in some iterations of
OpenML #6, the batch diversity is relatively low, as evidenced by the corresponding log Gram determinant
being -∞. These areas are indicated by gaps in the learning curve for CONF. Situations where there are
22
Published as a conference paper at ICLR 2020
OpenML #6, MLP, Batch size: 100
qsBq°IlJB-EUJ」S8P Bo~∣
O 2000 4000 6000 8000 10000 12l	∣00
#Labels queried
(a)
OPenML #6, MLP, Batoh SiZe: 100
(c)
O 10000	20000	30000	40000	50000
#Labels queried
(b)
SVHNj ResNet, Batch size: 10OO
O
0 8 6 4 2 0
qBqW lu」ou① B 也①
10000	20000	30000	40000	50000
#Labels queried
(d)
-----匕DPP ---------- ∕c-means++ -------- Rand -------- FFk-Center -------- Conf
Figure 24:	A comparison of batch selection algorithms in gradient space. Plots a and b show the log
determinants of the Gram matrices of gradient embeddings within batches as learning progresses. Plots c
and d show the average embedding magnitude (a measurement of predictive uncertainty) in the selected
batch. The k-centers sampler finds points that are not as diverse or high-magnitude as other samplers. Notice
also that k-MEANS++ tends to actually select samples that are both more diverse and higher-magnitude than
a k-DPP, a potential pathology of the k-DPP’s degree of stochastisity. Among all algorithms, CONF has
the largest average norm of gradient embeddings within a batch; however, in OpenML #6, and the first few
interations of SVHN, some batches have a log Gram determinant of -∞ (shown as gaps in the curve), which
shows that Conf sometimes selects batches that are inferior in diversity.
many gaps in the Conf plot seem to correspond to situations in which Conf performs poorly in terms of
accuracy (see Figure 13 for the corresponding learning curve). Both k-DPP and FF-k-CENTER (an algorithm
that approximately minimizes k-center objective) select batches that have lower diversity than k-MEANS++
(BADGE).
G COMPARIS ON OF k-MEANS++ AND k-DPP IN BATCH SELECTION
In Figures 25 to 31, we give running time and test accuracy comparisons between k-MEANS++ and k-DPP
for selecting examples based on gradient embedding in batch mode active learning. We implement the k-DPP
sampling using the MCMC algorithm from (Kang, 2013), which has a time complexity of O(T ∙ (k2 + kd))
23
Published as a conference paper at ICLR 2020
k-DPP
----∕c-means++
Figure 25: Learning curves and running times for OpenML #6 with MLP.
OPenM曲55, MLP, Batch size： 100
xi而PenML#155, MLP, Batch size: 100
OpenML#155, MLP, Batch size: 1000	χi曲PenML#155, MLP, Batch size: 1000
10000	20000	3008	40000
#LabeIS queried
10000	2008	30000	40000
#LabeIS queried
40000
40000
10000
10000
k-DPP -----匕means++
Figure 26:	Learning curves and running times for OpenML #155 with MLP.
and space complexity of O(k2 + kd), where τ is the number of sampling steps. We set τ as b5k ln kc in
our experiment. The comparisons for batch size 10000 are not shown here as the implementation of k-DPP
sampling runs out of memory.
It can be seen from the figures that, although k-DPP and k-MEANS++ are based on different sampling criteria,
the classification accuracies of their induced active learning algorithm are similar. In addition, when large
batch sizes are required (e.g. k = 1000), the running times of k-DPP sampling are generally much higher
than those of k-MEANS++.
OPenM曲56, MLP, Batch size： 100
XIOt)PenML#156, MLP, Batch size: 100
OPenML#156, MLP, Batch size: 1∞0	Kl而PenML#156, MLP, Batch size: 10∞
10000	20000	3008	40000
#LabeIS queried
10000	2008	30000	40000
#LabeIS queried
40000
40000
10000
10000
k-DPP -----匕means++
Figure 27:	Learning curves and running times for OpenML #156 with MLP.
24
Published as a conference paper at ICLR 2020
Accuracy	Accuracy	Accuracy	Accuracy
_ _ _ _	■ — — — — — — — — — _ _ _ _
OPenMLJ¢184, MLP, Batch size： 100	xiot)pθnML^184, MLP, Batch size： 100
OPenML#184, MLP, Batch size: 1∞0
Xldt)PenM邙 184, MLP Batch size： 10∞
200。4000 600。800。10000 12000 14000
#LabeIS queried
2000 4000 6000 8000 10000 12000 14000
#LabeIS queried
2000 4000 6000 60∞ 10000 12000 14000
#LabeIS queried
2000 4000 6000 808 10000 12000 14000
#LabeIS queried
k-DPP
----∕c-means++
Figure 28: Learning curves and running times for OpenML #184 with MLP.
×1O6 SVHN, ResNet Batch size: 100
5000 10000 15∞0 20000 250∞
#LabeIS queried
SVHN, RθSNθt Batch size: 1000
SLabeIs queried
10000	2008	30000	40000	500(
#LabeIS queried
188	20000	30000	40000	5001	10000	20000	3008	40000
#LabeIS queried	#Labels queried
AoBJn84
×1O4 SVHN, ResNet Batch size: 10∞
1∞∞	2008	3008	40000
#LabeIS queried
k-DPP
----∕c-means++
Figure 29: Learning curves and running times for SVHN with MLP and ResNet.
MNIST, MLP, Batch size: 100
×1O4 MNIST, MLP, Batch size: 100
0.95
,0.90
0.85
0.80
0.75
0.70
MNIST, MLP, Batch size: 1000
×1O4 MNIST, MLP, Batch size: 1000
k-DPP -----匕means++
Figure 30: Learning curves and running times for MNIST with MLP.
25
Published as a conference paper at ICLR 2020
Accura<^ _	_ _ A∞uracy
CIFAR1O, ResNet Batch size： 1OO
10000	20000	30000	40000
#LabeIS queried
10000	20000	3008	40000
#LabeIS queried
188	2008	3008	40000
#LabeIS queried
10000	20000	30000	40000
#LabeIS queried
k-DPP
----∕c-means++
Figure 31: Learning curves and running times for CIFAR10 with MLP and ResNet.
26