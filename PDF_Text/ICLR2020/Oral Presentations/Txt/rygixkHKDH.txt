Published as a conference paper at ICLR 2020
Geometric Analysis of Nonconvex Optimiza-
tion Landscapes for Overcomplete Learning
Qing Qu*
Center for Data Science
New York University
qq213@nyu.edu
Yuqian Zhang
Electrical & Computer Engineering
Rutgers University
yqz.zhang@rutgers.edu
Yuexiang Zhai
EECS
UC Berkeley
ysz@berkeley.edu
Xiao Li
Eletronic Engineering
CUHK
xli@ee.cuhk.edu.hk
Zhihui Zhu
Electrical & Computer Engineering
University of Denver
zhihui.zhu@du.edu
Ab stract
Learning overcomplete representations finds many applications in machine learning
and data analytics. In the past decade, despite the empirical success of heuristic
methods, theoretical understandings and explanations of these algorithms are still
far from satisfactory. In this work, we provide new theoretical insights for several
important representation learning problems: learning (i) sparsely used overcom-
plete dictionaries and (ii) convolutional dictionaries. We formulate these problems
as '4 -norm optimization problems over the sphere, and study the geometric ProP-
erties of their nonconvex optimization landscapes. For both problems, we show
the nonconvex objectives have benign (global) geometric structures, which enable
development of efficient optimization methods finding the target solutions. Finally,
our theoretical results are justified by numerical simulations.
1	Introduction
High dimensional data often has low-complexity structures (e.g., sparsity or low rankness). The
performance of modern machine learning and data analytical methods heavily depends on appropriate
low-complexity data representations (or features) which capture hidden information underlying the
data. While we used to manually craft representations in the past, it has been demonstrated that
learned representations from the data show much superior performance (Elad, 2010). Therefore,
(unsupervised) learning of latent representations of high-dimensional data becomes a fundamental
problem in signal processing, machine learning, theoretical neuroscience and many other fields
(Bengio et al., 2013). Moreover, overcomplete representations for which the number of latent features
exceeds the data dimensionality, have shown better representation of the data in various applications
compared to complete representations (Lewicki & Sejnowski, 2000; Chen et al., 2001; Rubinstein
et al., 2010). In this paper, we study the following overcomplete representation learning problems.
•	Overcomplete dictionary learning (ODL). One of the most important unsupervised representa-
tion learning problems is learning sparsely used dictionaries (Olshausen & Field, 1997), which
finds many applications in image processing and computer vision (Wright et al., 2010; Mairal
et al., 2014). The task is given data
lomn “ lmn ∙ lx，	(1.1)
data	dictionary sparse code
We want to learn the compact representation (or dictionary) A P RnXm along with the sparse code
X P Rmχp. For better representation of the data, it is often more desired that the dictionary A is
overcomplete m > n, where it provides greater flexibility in matching structures in the data.
•	Convolutional dictionary learning (CDL). Inspired by deconvolutional networks (Zeiler et al.,
2010), the convolutional form of sparse representations (Bristow et al., 2013; Garcia-Cardona &
*The full version of this work can be found at https://arxiv.org/abs/1912.02 42 7.
1
Published as a conference paper at ICLR 2020
Wohlberg, 2018) replaces the unstructured dictionary A with a set of convolution filters ta0kukK“1.
Namely, the problem is that given multiple circulant convolutional measurements
K
yi = 3∖ a0k f	Xik ,	1 ≤ i ≤ p,	(1.2)
lomon	lomon
k“1
“ filter sparse code
one wants to learn the filters ta0kukK“1 along with the sparse codes. The problem resembles a lot
similarities to classical ODL. Indeed, one can show that Equation (1.2) reduces to Equation (1.1)
in overcomplete settings by reformulation (Huang & Anandkumar, 2015). The interest of studying
CDL was spurred by its better modeling ability of human visual and cognitive systems and the
development of more efficient computational methods (Bristow et al., 2013), and has led to a
number of applications in which the convolutional form provides state-of-art performance (Gu
et al., 2015; Papyan et al., 2017b; Lau et al., 2019). Recently, the connections between CDL and
convolutional neural network have also been extensively studied (Papyan et al., 2017a; 2018).
In addition, variants of finding overcomplete representations appear in many other problems beyond
the dictionary learning problems we introduced here, such as overcomplete tensor decomposition
(Anandkumar et al., 2017; Ge & Ma, 2017), overcomplete ICA (Lewicki & Sejnowski, 1998; Le
et al., 2011), and short-and-sparse blind deconvolution (Zhang et al., 2017; 2018; Kuo et al., 2019).
Prior arts on dictionary learning (DL). In the past decades, numerous heuristic methods have
been developed for solving DL (Lee et al., 2007; Aharon et al., 2006; Mairal et al., 2010). Despite
their empirical success (Wright et al., 2010; Mairal et al., 2014), theoretical understandings of when
and why these methods work are still limited.
When the dictionary A is complete (Spielman et al., 2012) (i.e., square and invertible, m “ n), by
the fact that the row space of Y equals to that of X (i.e., rowpY q “ rowpXq), Sun et al. (2016a)
reduced the problem to finding the sparsest vector in a subspace (Demanet & Hand, 2014; Qu et al.,
2016). By considering a (smooth) variant of the following '1-minimization problem over the sphere,
min IqJY∣∣1,	s.t. q P sn´1,	(1.3)
Sun et al. (2016a) showed that the nonconvex problem has no spurious local minima when the sparsity
level1 θ P Op1q, and every local minimizer q‹ is a global minimizer with q‹JY corresponding to one
row of X . The new discovery has led to efficient, guaranteed optimization methods for complete DL
from random initializations (Sun et al., 2016b; Bai et al., 2018; Gilboa et al., 2019).
However, all these methods critically rely on the fact that rowpY q “ rowpX q for complete A, there
is no obvious way to generalize the approach to the overcomplete setting m > n. On the other
hand, for learning incoherent overcomplete dictionaries, with sparsity θ P O(1/?n)and stringent
assumptions on X, most of the current theoretical analysis results are local (Geng et al., 2011;
Arora et al., 2015; Agarwal et al., 2016; Chatterji & Bartlett, 2017), in the sense that they require
complicated initializations that could be difficult to implement in practice. Therefore, the legitimate
question remains: why do heuristic methods solve ODL with simple initializations?
Contributions. In this work we study the geometry of nonconvex landscapes for overcom-
plete/convolutional DL, where our result can be simply summarized by the following statement.
There exists nonconvex formulations for ODL/CDL with benign optimization landscapes, that
descent methods can learn overcomplete/convolutional dictionaries with simple2 initializations.
Our approach follows the spirit of Sun et al. (2016a), while we overcome the aforementioned obstacles
for overcomplete dictionaries by directly finding columns of A instead of recovering sparse rows of
X. We achieve this by reducing the problem to maximizing the '4-norm3 of Y Jq over the sphere,
1Here, the sparsity level θ denotes the proportion of nonzero entries in X .
2Here, for ODL simple means random initializations; for CDL, it means simple data-driven initializations.
3The use of `4 -norm can also be justified from the perspective of sum of squares (SOS) (Barak et al., 2015;
Ma et al., 2016; Schramm & Steurer, 2017). One can utilize properties of higher order SOS polynomials
(such as 4-th order polynomials) to correctly recover columns of A. But the complexity of these methods are
quasi-polynomial, and hence much more expensive than the direct optimization approach we consider here.
2
Published as a conference paper at ICLR 2020
which is known to promote the spikiness of the solution (Zhang et al., 2018; Li & Bresler, 2018; Zhai
et al., 2019). In particular, we show the following results for ODL and CDL, respectively.
1.	For the ODL problem, when A is unit norm tight frame and incoherent, our nonconvex objective
is strict saddle (Ge et al., 2015; Sun et al., 2015b) in the sense that any saddle point can be escaped
by negative curvature and all local minimizers are globally optimal. Furthermore, every local
minimizer is close to a column of A.
2.	For the CDL problem, when the filters are self and mutual incoherent, a similar nonconvex
objective is strict saddle over a sublevel set, within which every local minimizer is close to a target
solution. Moreover, we develop a simple data-driven initialization that falls into this sublevel set.
Our analysis on ODL provides the first global characterization for nonconvex optimization landscape
in the overcomplete regime. On the other hand, our result also gives the first provable guarantee for
CDL. Indeed, under mild assumptions, our landscape analysis implies that with simple initializations,
any descent method with the ability of escaping strict saddle points4 provably finds global minimizers
that are close to our target solutions for both problems. Moreover, our result opens up several
interesting directions on nonconvex optimization that are worth of further investigations.
2	Overcomplete Dictionary Learning
In this section, we start stating our result with ODL. In Section 3, we will show how our geometric
analysis here can be extended to CDL in a nontrivial way.
2.1	Basic Assumptions
We study the DL problem in Equation (1.1) under the following assumptions for A P RnXm and
X P Rmχp. In particular, our assumption for the dictionary A can be viewed as a generalization of
orthogonality in the overcomplete setting (Mixon, 2016).
Assumption 2.1 (Tight frame and incoherent dictionary A) We assume that the dictionary A is
unit norm tight frame (UNTF) (Mixon, 2016), in the sense that
—AAJ = I,	}ai} = 1 P1 ≤ i ≤ m),	(2.1)
m
and its columns satisfy the μ-incoherence condition. Namely, let A “ [aι a2 ∙∙∙ am,],
μpAq :“ max	Hɪʌ, m j )∣P p0,1q.	(2.2)
-m ∣∖}ai} }aj } /∣
We assume the coherence of A is small, i.e., μ(A) ! 1.
Assumption 2.2 (Random Bernoulli-Gaussian X) We assume entries ofX „i.i.d. BGpθq5, that
X “ BdG, Bij„i.i.d. Berpθq,	Gij „i.i.d. N p0, 1q,
where the Bernoulli parameter θ P p0, 1q controls the sparsity level ofX.
Remark 1. The coherence parameter μ plays an important role in shaping the optimization land-
scape. A smaller coherence μ implies that the columns of A are less correlated, and hence easier
for optimization. For matrices with `2 -normalized columns, classical Welch bound (Welch, 1974;
Foucart & Rauhut, 2013a) suggests that the coherence μ is lower bounded by μ(A) 2

m´n
(m´ 1)n
which is achieved when A is equiangular tight frame (Sustik et al., 2007). For a generic random6
matrix A, w.h.p. it is approximately UNTF, with coherence μ(A)
Jlonm roughly achieving
«
the order of Welch bound. For a typical dictionary A under Assumption 2.1, this suggests that the
COherenCe Parameter μ(A) often decreases w.r.t. the feature dimension n.
4Recent results show that methods such as trust-region (Absil et al., 2007; Boumal et al., 2018), cubic-
regularization (Nesterov & Polyak, 2006), curvilinear search (Goldfarb et al., 2017), and even gradient descent
(Lee et al., 2016) can provably escape strict saddle points.
5Here, we use BG pθq for abbreviation of Bernoulli-Gaussian distribution, with sparsity level θ P p0, 1q.
6For instance, when A is random Gaussian matrix, with each entry aij „i.i.d. N p0, 1{nq.
3
Published as a conference paper at ICLR 2020
(a)夕T(q), n = 3, m = 4
(b)夕T(q), n = 3, m = 4
Figure 1: Plots of landscapes ST Pq) and SDL (q) over S2.
Both function values are normalized to r0, 1s. The overcomplete
dictionary A is generated to be UNTF, with n “ 3 and m “ 4.
The sparse coefficient X „ BGpθ) with θ “ 0.1 and p “
2 ^ 104. Black dots denote columns of A (target).
spikiness ρ(ζ)
Figure 2: Spikiness %pζ) vs.
}ζ}44{}q}4. We generate UNTF
A, randomly draw many points
q P Snτ, and compute }Z}4 and
spikiness %pζ) as in (2.6) with
ζ “ AJq. On the plot, we mark
each point q P SnT by "+”.
2.2	Problem formulation
We solve DL in the overcomplete regime by considering the following problem
min SDLPq) ：“ ´cDpL >qJY>4 “ ´cDpL >qJAX>4 , st }q}2 = 1,	(2.3)
where cDL > 0 is a normalizing constant. At the first glance, our objective looks similar to
Equation (1.3) in complete DL, but we tackle the problem from a very different aspect — we directly
find columns of A instead of recovering sparse rows of X. Given UNTF A and random X „ BGPθ),
our intuition of solving Equation (2.3) originates from the fact (Lemma D.1)
EX rsDL(q)s	=	ST(q)´ 2p1 ´ °)´mn),	ST(q)：“	´4	>AJq>4,	(2.4)
where STPq) can be viewed as the objective for 4th order tensor decomposition in Ge & Ma (2017).
When p is large, this tells us that optimizing Equation (2.3) is approximately maximizing '4-norm of
ζ “ AJq over the sphere (see Figure 1). If q equals to one of the target solutions (e.g., q “ a1),
Z(q) :“ AJq =	}aι}2 QJQ2 …QJQm	,	(2.5)
loomoon loomoon loomoon
“1 H < μ	H < μ
then Z is spiky when μ is small (e.g., μ ! 1). Here, we introduce a notion of spikiness % for a vector
ζ P Rm by
%(Z)	:=	∣Z(1)I	{∣Zp2q∣,	∣Zpιq∣	"Z(2)∣)…"Z(m)∣,	(2.6)
where ζpiq denotes the ith ordered entry of Z. Figure 2 shows that larger %(Z) leads to larger }Z}44
with '2 -norm fixed. This implies that maximizing '4-norm over the sphere promotes the spikiness of
Z (Zhang et al., 2018; Li & Bresler, 2018; Zhai et al., 2019). Thus, from Equation (2.5), we expect
the global minimizer q‹ of Equation (2.3) is close to one column of A. Ge & Ma (2017) proved
that for ST (q) there is no spurious local minimizer below a sublevel set whose measure over sn´1
geometrically shrinks w.r.t. the dimension n, and without providing valid initialization into the set.
Therefore, the challenge still remains: can simple descent methods solve the nonconvex objective
Equation (2.3) to global optimality? In this work, we show that the answer is affirmative. Under
proper assumptions, we show that our objective actually has benign global geometric structure,
explaining why descent methods with random initialization solve the problem to the target solutions.
2.3	Geometric Analysis of Nonconvex Optimization Landscape
To characterize the landscape of SDL (q) over the sphere sn´1, let us first introduce some basic tools
from Riemannian optimization (Absil et al., 2009a). For any function f : sn´1 — R, we have
grad f(q) :“ PqK Vf (q),	Hess f(q) :“ PqK 'V2 f (q)—〈q, Vf(q)") PqK
4
Published as a conference paper at ICLR 2020
to be the Riemannian gradient and Hessian7 of f (q). In addition, We partition SnT into two regions
RN	:= {q P Snτ∣3τ(q)	>	一Sdl μ2{3	}Z(q)}2),	(2.7)
RC	：= {q P SnTl 3τ(q)	≤	—Sdl *	}Z(q)}2},	(2.8)
for some fixed numerical constant §dl > 0. Unlike the approach in SUn et al.(2016a), our partition
and landscape analysis are based on function value 夕τ(q) instead of target solutions. This is
because in overcomplete case the optimization landscape is more irregular compared to that of the
complete/orthogonal case, which introduces extra difficulties for explicit partition of the sphere. In
particular, for each region we show the following results.
Theorem 2.3 (Global geometry of nonconvex landscape for ODL) Suppose we have
K :“ m{n, θ P 'm´1,3-1) , §dl > 26, μ P '0,40T) ,	(2.9)
and assume Y = AX such that A and X satisfy Assumption 2.1 and Assumption 2.2, respectively.
1.	(Negative curvature in RN) W.h.p. over the randomness of X, whenever
PeCθK4n6log(θn∕μ) and K ≤ 3 ∙ ´l ' 6μ ' 6ξDLμ2/5),
any point q P RN exhibits negative curvature in the sense that
D V P Sn´1,	s.t. VJ Hess ^DL(q)v W —3 }Z}4 }Z}8 .
2.	(No bad critical points in RC) W.h.p. over the randomness of X, whenever
peCθK3 max { μ∙,Kn2} n3 log(θn∕μ) and K ≤ ξ3{2∕8,
every critical point qc of 夕DL (q) in RC is either a strict saddle point that exhibits negative
curvature for descent, or it is near one of the target solutions (e.g. a1) such that
χa1/}a1} , qcy J 1 ´ 5ξDL/2.
Here C > 0 is a universal constant.
Remark 2. A combination of our geometric analysis for both regions provides the first global
geometric analysis for ODL with θ P O(1), which implies that 夕dl(q) has no spurious local
minimizers over sn´1: any critical point is either a strict saddle point that can be efficiently escaped,
or it is near one of the target solutions. Moreover, recent results show that nonconvex problems with
this type of optimization landscapes can be solved to optimal solutions by using (noisy) gradient
descent methods with random initializations (Lee et al., 2016; Jin et al., 2017; Lee et al.; Criscitiello
& Boumal, 2019). In addition, we point out several limitations of our result for future work.
•	As we have only characterized properties of critical points, our result does not directly lead to
convergence rate for descent methods. To show polynomial-time convergence, as suggested by
Sun et al. (2016a; 2018); Li & Bresler (2018); Kuo et al. (2019), we need finer partitions of the
sphere and uniform controls of derivatives in each region8. We leave this for future work.
•	Our analysis in RN says that when μ is sufficiently small9 the maximum overcompleteness K
allowed is roughly K “ 3, which is smaller than that of RC (which could be a large constant).
We believe this is mainly due to loose bounds for controlling norms of A in RC . Moreover, our
experiment result in Section 4 suggests that there is a substantial gap of K between our theory and
practice: the phase transition in Figure 3a shows that gradient descent with random initialization
works even in the regime m W n2. We leave improvement of our result as an open question.
7The Riemannian derivatives are similar to ordinary derivatives in Euclidean space, but they are defined in
the tangent space of the manifold M “ sn´1. We refer readers to Absil et al. (2009a) for more details.
8Our preliminary investigation indicates that our premature analysis is not tight enough to achieve this.
9From Remark 1, for a typical A, we expect μ P OppnK)´"2) to be diminishing w.r.t. n.
5
Published as a conference paper at ICLR 2020
Brief sketch of analysis. From Equation (2.4), We know that 夕dl(q) reduces to 夕τ(q) in large
sample limit as P → 8. This suggests an expectation and concentration type of analysis: (i) We first
characterize critical points and negative curvature for the deterministic function 夕τ(q) in RC and
RN (see Appendix B); (ii) for any small δ > 0, we show the measure concentrates in the sense that
for a finitely large P》Ω(δ-poly(n)),
sup }grad夕dl(q) — grad夕τ(q)} ≤ δ, SuP }Hess夕dl(q) — Hess夕τ(q)} ≤ δ
qpSn_1	qpSn_1
holds w.h.p. over the randomness of X. Thus we can turn our analysis of 夕T (q) to that of 夕DL (q)
by a perturbation analysis (see Appendix C & D). Here, it should be noticed that grad 夕dl(q)
and Hess 夕dl(q) are 4th-order polynomials of X, which are heavy-tailed empirical processes over
q P Sn´1. To control suprema of heavy-tailed processes, we developed a general truncation and
concentration type of analysis similar to Zhang et al. (2018); Zhai et al. (2019), so that we can utilize
classical bounds for sub-exponential random variables (Boucheron et al., 2013) (see Appendix F).
3 Convolutional Dictionary Learning
3.1	Problem Formulation
Recall from Section 1, the basic task of CDL is that given convolutional measurements in the form of
Equation (1.2), we want to recover kernels ta0kukK“1. Here, by reformulating10 CDL in the form of
ODL, we generalize our analysis from Section 2.3 to CDL with a few new ingredients.
Reduction from CDL to ODL. For any Z P Rn, let Cz P RnXn be the circulant matrix generated
from z. From Equation (1.2), the properties of circulant matrix imply that
K
Cyi	=	CXK=I aokfXik	“	Ca	CaOk Cxik	“	A0	,	xi,	1 W i ≤ p,
k“1
with A0	=	[Ca0i Ca02	…CaOKS and Xi	=	“。■	CJ	…CJiK‰J, so	that
Ao p RnXnK is overcomplete and structured. Thus, contencating all Cyi, we have
“Cyi	Cy2	… Cyp ‰ =	Ao ∙ [Xi Xl2	… Xps	=^	Y	= Ao ∙ X.
loooooooooooooooooooooooooooon	loooooooooooomoooooooooooon
Y PRnxnp	X PRnKxnp
This suggests that we can view the CDL problem as ODL: if we can recover a column of the
overcomplete dictionary Ao, we find one of the filters aok (1 W k W Kq up to a circulant shift11.
Nonconvex problem formulation and preconditioning. To solve CDL, one may consider the
same objective Equation (2.3) as ODL. However, for many applications our structured dictionary Ao
could be badly conditioned and not tight frame, which results in bad optimization landscape and even
spurious local minimizers. To deal with this issue, we whiten our data Y by preconditioning12
PY = PA0X,	P = ∣(θK2np)T YYJ] 1{2 .	(3.1)
—1/2 .
For large p, we approximately have P « (KTAOAJ)	(see Appendix E.5), so that
PY « (KTAOAJ)T/2 AO ∙ X = A ∙ X,	A := (KTAOAJ)T/2 Ao,
where A is automatically tight frame with K—1AAJ = I. This suggests to consider
min OCDLpqq := ´cCDpL >qJ(PYq>4，s.t.	}q}2 = 1，	(3.2)
10Similar formulation ideas also appeared in (Huang & Anandkumar, 2015) with no theoretical guarantees.
11The CDL problem exhibits shift symmetry in the sense that aok f Xik = s` [aok] f s´` [xik], where s` []
is a circulant shift operator by length `. This implies we can only hope to solve CDL up to a shift ambiguity.
12Again, the θ here is only for normalization purpose, which does not affect optimization landscape. Similar
P is also considered in Sun et al. (2016a); Zhang et al. (2018); Qu et al. (2019).
6
Published as a conference paper at ICLR 2020
Algorithm 1 Finding one filter with data-driven initialization
Input: data Y P RnXp
Output: an esimated filter a‹
1:	preconditioning. Cook up the preconditioning matrix P in Equation (3.1).
2:	initialization. Initialize ⑦而 “ PSnT (Pyg) with a random sample yg, 1 ≤ ' ≤ p.
3:	optimization with escaping saddle points. Optimize Equation (3.2) to a local minimizer q‹, by
using a descent method such as Goldfarb et al. (2017) that escapes strict saddle points.
4:	return an estimated filter a< “ P&n—i (P´1 q*).
for some normalizing constant CCDL > 0, So that is close to optimizing
pCDL(q) := ´
cnPL >qjAx>4 « ocDL(q),
for a tight frame dictionary A (we make this rigorous in Appendix E.4). To study the problem, we
make assumptions on the sparse signals xik „i.i.d. BG(θ) similar to Assumption 2.2. Furthermore,
we assume A0 and A satisfy the following properties which serve as counterparts to Assumption 2.1.
Assumption 3.1 (Properties of A0 and A) We assume the filter matrix A0 has minimum singular
value σmin(Ao) > 0 with bounded condition number κ(Ao) := σmaχ(Ao){σmin(Ao). In addition,
we assume the columns of A are mutually incoherent: maxi“j |( }ai}, }j})| ≤ μ.
3.2 Geometric Analysis and Nonconvex Optimization
Optimization landscape for CDL. We characterize the geometric structure of 夕cdl(q) over
Rcdl := {q P SnTl w(q) ≤ —§cdl μ2{3κ4{3(Ao) }Z(q)}3 ) ,	(3.3)
for some fixed numerical constant ξcDL > 0, where Z(q) = AJq and 夕τ(q) = —4-1 }Z(q)}4 as
introduced in Equation (2.4). We show 夕cdl(q) satisfies the following properties.
Theorem 3.2 (Local geometry of nonconvex landscape for CDL) Let us denote m := Kn, and
let Co > 5 and η < 2-6 be some positive constants. Suppose we have
θ P 'm´1, 3T) ,	ξcDL = Co ∙ L, μ P '0,40.1),
and assume that Assumption 3.1 and Xik 〜i.i.d. BG(θ) hold. There exists some constant C > 0,
w.h.p. over the randomness ofxiks, whenever
PeCθK2μ~2n4 max
"K6κ6(Ao)
t*山区。)
n log6
(n{μ) and K < Co,
every critical point qc in RCDL is either a strict saddle point that exhibits negative curvature for
descent, or it is near one of the target solutions (e.g. aι) such that〈a，}aι} , qCy 2 1 — 5κ-2η.
Remark 3. The analysis is similar to that of ODL in RC (see Appendix D). In contrast, our sample
complexity p and RCDL have extra dependence on κ(Ao) due to preconditioning in Equation (3.1).
On the other hand, because our preconditioned dictionary A is tight frame but not necessarily UNTF,
in the worst case we cannot exclude existence of spurious local minima in RCDL ∩ SnT for CDL.
From geometry to optimization. Nonetheless, in Algorithm 1 we propose a simple data-driven
initialization qinit such that qinit P RCDL . Since RCDL does not have bad local minimizers, by
proving that all iterates stay within RCDL, it suffices to show global convergence of Algorithm 1.
We initialize q by randomly picking a preconditioned data sample Pyg with ' P [p], and set
qinit = Psn-1 (Pyg) , s.t. Zinit = AJqinit « KPPSnK´ 'AJAxg) .	(3.4)
For generic A, small μ(A) implies that AJA is close to a diagonal matrix13, so that Zinit is spiky for
sparse xg. Therefore, we expect large }Zinit}4 and qinit P RCDL by leveraging sparsity of xg.
13This is because the off diagonal entries are bounded roughly by JKμ, which are tiny when μ is small.
7
Published as a conference paper at ICLR 2020
(≡0IMOI
3
1.5	1.7	1.9	2.1	2.3	2.5	2.7
IOglOm)
0
0
0∙8
S日'V pjəAoɔəJO 0e
500	1000	1500 2000 2500 3000
# of RUNS
I—•—τn∕n = 21
-θ-m/n = 3
—I—m/n = 4
∖-^-m∕n = 5
Ir-Tn∕τt = 6∣
(a) Filter 1
(a) Asymptotic ODL: Phase transition. (b) Asymptotic ODL: Recover full A.
1
Mω.8
0 0.6
I 0.4
含
0.2
0
0.5	1
1.5	2	2.5	3	3.5	4	4.5	5
P	x104
(c) ODL: Recovery probability vs. p.
θ
(d) ODL: Recovery probability vs. θ .
Figure 3: Simulations for ODL. (a) θ “ 0.1; (b) n “ 64; (c) n “ 64, θ “
0.1; (d) m = 3n,p = 5 X 104.
(b) Filter 2
(c) Filter 3
Figure 4: CDL Simulation.
Parameters: n “ 64, θ “ 0.1,
K = 3, P = 1 X 104.
Proposition 3.3 (Convergence of Algorithm 1 to target solutions) With m “ Kn, suppose
Jogm	μ∙{3	κ4{3 Kμj 1	OC
c1 F W θ ≤ c2 κ4∕3m log m，min ɪ 再,m2log^ 卜	(3.5)
W.h.p. over the randomness of xiks, whenever
P > CθK2μ12 max { K6κ6pAo)∕σ21inpAoq, n( n4 log6 (m∕μ),
we have qinit P RCDL, and all future iterates of Algorithm 1 stay within RCDL and converge to an
approximate solution (e.g., some circulant shift s` rao1s of aok with 1 W ` W n) in the sense that
IlPSnT (PTq<) ´ s` ra01 s >∣ ≤ 3
where e is a small numerical constant. Here, c1, , c2, C > 0 are some numerical constants.
Remark 4. Our result (Equation (3.5)) suggests that there is a tradeoff between μ and θ for
optimization. For generic filters (e.g. drawn uniformly from the sphere), we approximately have14
μ P OpmT2) and K P O(1), so that our theory suggests the maximum sparsity allowed is θ P
Opm´2/3). For other smoother filters which may have larger μ and κ, the sparsity θ allowed tends
to be smaller. Improving Equation (3.5) is the subject of future work. On the other hand, our result
guarantees convergence to an approximate solution of constant error. We left exact recovery for
future work. Finally, although we write CDL in the matrix-vector form, the optimization could be
implemented very efficiently using fast Fourier transform (FFT) (see Appendix G).
4	Experiments
In this section, we experimentally demonstrate our proposed formulation and approach for ODL and
CDL. We solve our nonconvex problems in Equation (2.3) and Equation (3.2) using optimization
methods15 with random initializations introduced in Appendix G.
14See Figure 3 of Zhang et al. (2018) for an illustration of these estimations.
15For simplicity, we use power method (see Algorithm 3) for optimizing without tuning step sizes. In practice,
we find both power method and Riemannian gradient descent have similar performance.
8
Published as a conference paper at ICLR 2020
Experiments on ODL. We generate data Y “ AX, with dictionary A P RnXm being UNTF16,
and sparse code X P RmXp 〜i.i.d. BG(θ). To judge the success recovery of one column of A, let
ρe “ min p1 ´ |xq‹,ai{}ai}y|q .
1¾i¾m
We have Pe “ 0 when q< “ P&n—i (ai), thus we assume a recovery is successful if Pe < 5 X lθ´2.
•	Overcompleteness. First, we fix θ “ 0.1, and test the limit of the overcompleteness K “ m{n
we can achieve by plotting the phase transition on pm, nq in log scale. To get rid of the influence
of sample complexity p, we run our algorithm on 夕τ(q) which is the sample limit of 夕dl(q). For
each pair of pm, nq, we repeat the experiment for 12 times. As shown in Figure 3a, it suggests that
the limit of overcompleteness is roughly m « n2, which is much larger than our theory predicts.
•	Recovering full matrix A. Second, although our theory only guarantees recovery of one column
of A, Figure 3b suggests that we can recover the full dictionary A by repetitive independent trials.
As the result shows, O(m log mq independent runs suffice to recover the full A.
•	Recovery with varying θ and p. Our simulation in Figure 3c implies that we need more samples
p when the overcompleteness K increases. Meanwhile, Figure 3d shows successful recovery even
when sparsity θ « 0.3. The maximum θ seems to remain as a constant when n increases.
Experiments on CDL. Finally, for CDL, we generate measurement according to Equation (1.2)
with K = 3, where the filters {a°k}K“i are drawn uniformly from the sphere sn´1, and Xik 〜i.i.d.
BG(θq. Figure 4 shows that our method can approximately recover all the filters by running a few
number of repetitive independent trials.
5	Conclusion and Discussion
In this work, we showed that nonconvex landscapes of overcomplete representation learning also
possess benign geometric structures. In particular, by reducing the problem to an `4 optimization
problem over the sphere, we proved that ODL has no spurious local minimizers globally: every
critical point is either an approximate solution or a saddle point can be efficiently escaped. Moreover,
we showed that this type of analysis can be carried over to CDL with a few new ingredients, leading
to the first provable method for solving CDL globally. Our results have opened several interesting
questions that are worth of further investigations, that we discuss as follows.
Tighter bound on overcompleteness for ODL. As shown in Theorem 2.3, our bound on the
overcompleteness K “ m{n is an absolute constant, which we believe is far from tight (see
experiments in Figure 3a). In the high overcompleteness regime (e.g., n ! m ≤ n2), one conjecture
is that spurious local minimizer does exist but descent methods with random initializations implicitly
regularizes itself such that bad regions are automatically avoided Ma et al. (2017); another conjecture
is that there is actually no spurious local minimizers. We tend to believe the latter conjecture is true.
Indeed, the looseness of our analysis appears in the region RN (see Appendix B.2), for controlling
the norms of A.
One idea might be to consider i.i.d. Gaussian dictionary instead of the deterministic incoherent
dictionary A, and use probabilistic analysis instead of the worst-case deterministic analysis. However,
our preliminary analysis suggests that elementary concentration tools for Gaussian empirical processes
are not sufficient to achieve this goal. More advanced probabilistic tools might be needed here.
Another idea that might be promising is to leverage more advanced tools such as the sum of squares
(SoS) techniques Lasserre (2001); Blekherman et al. (2012). Previous results Barak et al. (2015); Ma
et al. (2016); Hopkins et al. (2015) used SoS as a computational tool for solving this type of problems,
while the computational complexity is often quasi-polynomial and hence cannot handle problems of
large-scale. In contrast, our idea here is to use SoS to verify the geometric structure of the optimizing
landscape instead of computation, to have a better uniform control of the negative curvature in RN .
If we succeeded, this might lead to a tighter bound on the overcompleteness. Moreover, analogous
to building dual certificates for convex relaxations such as compressive sensing CandeS & Wakin
(2008); Candes & Plan (2011) and matrix completion Candes & Recht (2009); Candes et al. (2011),
it could potentially lead to a more general approach for verifying benign geometry structures for
nonconvex optimization.
16The UNTF dictionary is generated by Tropp et al. (2005): (i) generate a standard Gaussian matrix A0 , (ii)
from A0 alternate between preconditioning the matrix and normalize the columns until convergence.
9
Published as a conference paper at ICLR 2020
Composition rules for nonconvex optimization? Another interesting phenomenon we found
through understanding ODL is that under certain scenarios the benign nonconvex geometry can
be preserved under nonnegative addition. Indeed, if we separate our dictionary A into several
subdictionaries as A “ [Ai … AN], then the asymptotic version of nonconvex objective for
ODL (Equation (2.3)) can be rewritten as
1	4N	1	4
°τ(q)	= ´4	>AJq>4	= £	中Tpq),中Tpq) :“ ´4	>AJq>4	p1 ≤ k ≤ N)∙	⑸I)
k“1
Presumably, every function 夕Tpqq also possess benign geometry for each submatrix Ak. This
discovery might suggest more general properties in nonconvex optimization - benign geometry
structures can be preserved under certain composition rules. Analogous to the study of convex
functions Boyd & Vandenberghe (2004), discovering composition rules can potentially lead to
simpler analytical tools for studying nonconvex optimization problems and hence have broad impacts.
Finding all components over Stiefel or Oblique manifolds. The nonconvex formulations consid-
ered in this work is only guaranteed to recover one column/filter at a time for ODL/CDL. Although
our experimental results in Section 4 implies that the full dictionary or all the filters can be recovered
by using repetitive independent trials, it is more desirable to have a formulation that can recover the
whole dictionary/filters in one shot. This requires us to consider optimization problems constraint
over more complicated manifolds rather than the sphere, such as Stiefel and Oblique manifolds Absil
et al. (2009a). Despite of recent empirical evidences Lau et al. (2019); Li et al. (2019) and study of
local geometry Zhai et al. (2019); Zhu et al. (2019), more technical tools need to be developed towards
better understandings for nonconvex problems constraint over these more complicated manifolds.
Miscellaneous. Finally, we summarize several small questions that might be also worth of pursuing.
•	Exact recovery. Our results only lead to approximate recovery of the target solutions. To obtain
exact solutions, one might need to consider similar rounding steps as introduced in Qu et al. (2016);
Sun et al. (2016b); Qu et al. (2019).
•	Designing better loss functions. The `4 objective we considered here for ODL and CDL is
heavy-tailed for sub-Gaussian random variables, resulting in bad sample complexity and large
approximation error. It would be nice to design better loss functions that also promotes spikiness
of the solutions.
•	Non-asymptotic convergence for descent methods. Unlike the results in Sun et al. (2016a;c);
Kuo et al. (2019), our geometric analysis here does not directly lead to non-asymptotic convergence
guarantees of any descent methods to global minimizers. This is because we only characterized
the geometric properties of critical points on the function landscape. To show non-asymptotic con-
vergence of methods introduced in Appendix G, we need to uniformly characterize the geometric
properties over the sphere.
•	Finer models for CDL. Finally, for CDL, it is worth noting that in many cases the length of the
filters ta0k ukK“1 is often much shorter than the observations tyiuip“1 Zhang et al. (2017); Kuo et al.
(2019); Zhang et al. (2018); Lau et al. (2019), which has not been considered in this work. The
extra structure leads to the so-called short-and-sparse CDL Lau et al. (2019), where the lower
dimensional model can lead to fewer samples for recovery. Based on our results, we believe the
short structure can be dealt with by developing finer analysis such as that in Kuo et al. (2019).
Acknowledgement
Part of this work was done when QQ and YXZ were at Columbia University. QQ thanks the generous
support of the Microsoft graduate research fellowship and Moore-Sloan fellowship. XL would like to
acknowledge the support by Grant CUHK14210617 from the Hong Kong Research Grants Council.
YQZ is grateful to be supported by NSF award 1740822. ZZ was partly supported by NSF Grant
1704458. The authors would like to thank Joan Bruna (NYU Courant), Yuxin Chen (Princeton
University), Lijun Ding (Cornell University), Han-wen Kuo (Columbia University), Shuyang Ling
(NYU Shanghai), Yi Ma (UC Berkeley), Ju SUn (University of Minnesota, Twin Cities), Rene Vidal
(Johns Hopkins University), and John Wright (Columbia University) for helpful discussions and
inputs regarding this work.
10
Published as a conference paper at ICLR 2020
References
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton
University Press, 2009a.
Pierre-Antoine. Absil, Christopher G. Baker, and Kyle A. Gallivan. Trust-region methods on Riemannian
manifolds. Foundations of Computational Mathematics, 7(3):303-330, 2007.
Pierre-Antoine. Absil, Robert Mahoney, and Rodolphe Sepulchre. Optimization Algorithms on Matrix Manifolds.
Princeton University Press, 2009b.
Alekh Agarwal, Animashree Anandkumar, Prateek Jain, and Praneeth Netrapalli. Learning sparsely used
overcomplete dictionaries via alternating minimization. SIAM Journal on Optimization, 26(4):2775-2799,
2016.
Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcomplete
dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):4311-4322, 2006.
Animashree Anandkumar, Rong Ge, and Majid Janzamin. Analyzing tensor power method dynamics in
overcomplete regime. The Journal of Machine Learning Research, 18(1):752-791, 2017.
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse
coding. Journal of Machine Learning Research, 40(2015), 2015.
Yu Bai, Qijia Jiang, and Ju Sun. Subgradient descent learns orthogonal dictionaries. arXiv preprint
arXiv:1810.10702, 2018.
Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the
sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,
pp. 143-151. ACM, 2015.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.
Grigoriy Blekherman, Pablo A Parrilo, and Rekha R Thomas. Semidefinite optimization and convex algebraic
geometry. SIAM, 2012.
St6phane Boucheron, GAbor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of
independence. Oxford university press, 2013.
Nicolas Boumal, Pierre-Antoine Absil, and Coralia Cartis. Global rates of convergence for nonconvex optimiza-
tion on manifolds. IMA Journal of Numerical Analysis, 39(1):1-33, 2018.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Hilton Bristow, Anders Eriksson, and Simon Lucey. Fast convolutional sparse coding. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 391-398, 2013.
Emmanuel J Candes and Yaniv Plan. A probabilistic and ripless theory of compressed sensing. IEEE transactions
on information theory, 57(11):7235-7254, 2011.
Emmanuel J CandeS and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of
Computational mathematics, 9(6):717, 2009.
Emmanuel J Candes and Michael B Wakin. An introduction to compressive sampling [a sensing/sampling
paradigm that goes against the common knowledge in data acquisition]. IEEE signal processing magazine,
25(2):21-30, 2008.
Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of
the ACM (JACM), 58(3):11, 2011.
Niladri Chatterji and Peter L Bartlett. Alternating minimization for dictionary learning with random initialization.
In Advances in Neural Information Processing Systems, pp. 1997-2006, 2017.
Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis pursuit. SIAM
review, 43(1):129-159, 2001.
C. Criscitiello and N. Boumal. Efficiently escaping saddle points on manifolds. In To appear in the proceedings
of NeurIPS, 2019.
11
Published as a conference paper at ICLR 2020
Laurent Demanet and Paul Hand. Scaling law for recovering the sparsest element in a subspace. Information
and Inference: A Journal of the IMA, 3(4):295-309, 2014.
Michael Elad. Sparse and redundant representations: from theory to applications in signal and image processing.
Springer Science & Business Media, 2010.
Simon Foucart and Holger Rauhut. An invitation to compressive sensing. In A mathematical introduction to
compressive sensing, pp. 1-39. Springer, 2013a.
Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing. Springer, 2013b.
Cristina Garcia-Cardona and Brendt Wohlberg. Convolutional dictionary learning: A comparative review and
new algorithms. IEEE Transactions on Computational Imaging, 4(3):366-381, 2018.
Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. In Advances in Neural
Information Processing Systems, pp. 3653-3663, 2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for
tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pp. 797-842, 2015.
QUan Geng, HUan Wang, and John Wright. On the local correctness of l^ 1 minimization for dictionary learning.
arXiv preprint arXiv:1101.5672, 2011.
Dar Gilboa, Sam BUchanan, and John Wright. Efficient dictionary learning with gradient descent. In International
Conference on Machine Learning, pp. 2252-2259, 2019.
Donald Goldfarb, CUn MU, John Wright, and ChaoxU ZhoU. Using negative cUrvatUre in solving nonlinear
programs. Computational Optimization and Applications, 68(3):479-502, 2017.
ShUhang GU, Wangmeng ZUo, Qi Xie, DeyU Meng, XiangchU Feng, and Lei Zhang. ConvolUtional sparse coding
for image sUper-resolUtion. In Proceedings of the IEEE International Conference on Computer Vision, pp.
1823-1831, 2015.
SamUel B Hopkins, Jonathan Shi, and David SteUrer. Tensor principal component analysis via sUm-of-sqUare
proofs. In Conference on Learning Theory, pp. 956-1006, 2015.
FUrong HUang and Animashree AnandkUmar. ConvolUtional dictionary learning throUgh tensor factorization. In
Feature Extraction: Modern Questions and Challenges, pp. 116-129, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points
efficiently. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
1724-1732. JMLR. org, 2017.
Michel Journ6e, Yurii Nesterov, Peter Richtdrik, and Rodolphe Sepulchre. Generalized power method for sparse
principal component analysis. Journal of Machine Learning Research, 11(Feb):517-553, 2010.
Han-Wen Kuo, Yuqian Zhang, Yenson Lau, and John Wright. Geometry and symmetry in short-and-sparse
deconvolution. In International Conference on Machine Learning, pp. 3570-3580, 2019.
Jean B Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on
optimization, 11(3):796-817, 2001.
Yenson Lau, Qing Qu, Han-Wen Kuo, Pengcheng Zhou, Yuqian Zhang, and John Wright. Short-and-sparse
deconvolution-a geometric approach. arXiv preprint arXiv:1908.10959, 2019.
Quoc V Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Y Ng. Ica with reconstruction cost for efficient
overcomplete feature learning. In Advances in neural information processing systems, pp. 1017-1025, 2011.
Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y Ng. Efficient sparse coding algorithms. In Advances in
neural information processing systems, pp. 801-808, 2007.
Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin Recht.
First-order methods almost always avoid strict saddle points. Mathematical Programming, pp. 1-27.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to
minimizers. In Conference on Learning Theory, pp. 1246-1257, 2016.
Michael S Lewicki and Terrence J Sejnowski. Learning nonlinear overcomplete representations for efficient
coding. In Advances in neural information processing systems, pp. 556-562, 1998.
12
Published as a conference paper at ICLR 2020
Michael S Lewicki and Terrence J Sejnowski. Learning overcomplete representations. Neural computation, 12
(2):337-365, 2000.
Xiao Li, Shixiang Chen, Zengde Deng, Qing Qu, Zhihui Zhu, and Anthony Man Cho So. Nonsmooth
optimization over stiefel manifold: Riemannian subgradient methods. arXiv preprint arXiv:1911.05047,
2019.
Yanjun Li and Yoram Bresler. Global geometry of multichannel sparse blind deconvolution on the sphere. In
Advances in Neural Information Processing Systems, pp. 1132-1143, 2018.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical
estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution.
arXiv preprint arXiv:1711.10467, 2017.
Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-of-squares. In
2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pp. 438-446. IEEE, 2016.
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and
sparse coding. Journal of Machine Learning Research, 11(Jan):19-60, 2010.
Julien Mairal, Francis Bach, Jean Ponce, et al. Sparse modeling for image and vision processing. Foundations
and TrendsR in Computer Graphics and Vision, 8(2-3):85-283, 2014.
Dustin G Mixon. Unit norm tight frames in finite-dimensional spaces. Finite Frame Theory: A Complete
Introduction to Overcompleteness, 93:53, 2016.
Yurii Nesterov and Boris T. Polyak. Cubic regularization of newton method and its global performance.
Mathematical Programming, 108(1):177-205, 2006.
Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by
v1? Vision research, 37(23):3311-3325, 1997.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via convolutional
sparse coding. The Journal of Machine Learning Research, 18(1):2887-2938, 2017a.
Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Convolutional dictionary learning via local
processing. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5296-5304, 2017b.
Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Theoretical foundations of deep learning
via sparse representations: A multilayer sparse model and its connection to convolutional neural networks.
IEEE Signal Processing Magazine, 35(4):72-89, 2018.
Qing Qu, Ju Sun, and John Wright. Finding a sparse vector in a subspace: Linear sparsity using alternating
directions. IEEE Transactions on Information Theory, 62(10):5855-5880, 2016.
Qing Qu, Xiao Li, and Zhihui Zhu. A nonconvex approach for exact and efficient multichannel sparse blind
deconvolution. arXiv preprint arXiv:1908.10776, 2019.
Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. Dictionaries for sparse representation modeling.
Proceedings of the IEEE, 98(6):1045-1057, 2010.
Tselil Schramm and David Steurer. Fast and robust tensor decomposition with applications to dictionary learning.
Proceedings of Machine Learning Research vol, 65:1-34, 2017.
Daniel A. Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In Conference
on Learning Theory, 2012.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere. arXiv preprint
arXiv:1504.06785, 2015a.
Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint arXiv:1510.06096,
2015b.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and the geometric
picture. IEEE Transactions on Information Theory, 63(2):853-884, 2016a.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere ii: Recovery by riemannian
trust-region method. IEEE Transactions on Information Theory, 63(2):885-914, 2016b.
13
Published as a conference paper at ICLR 2020
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retreival. arXiv preprint arXiv:1602.06664,
2016c.
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of Computational
Mathematics,18(5):1131-1198, 2018.
Mdtyds A Sustik, Joel A TroPP,Inderjit S Dhillon, and Robert W Heath Jr. On the existence of equiangular tight
frames. Linear Algebra and its applications, 426(2-3):619-635, 2007.
Joel A TroPP, Inderjit S Dhillon, Robert W Heath, and Thomas Strohmer. Designing structured tight frames via
an alternating Projection method. IEEE Transactions on information theory, 51(1):188-209, 2005.
Joel A TroPP et al. An introduction to matrix concentration inequalities. Foundations and TrendsR in Machine
Learning, 8(1-2):1-230, 2015.
Lloyd Welch. Lower bounds on the maximum cross correlation of signals (corresP.). IEEE Transactions on
Information theory, 20(3):397-399, 1974.
John Wright, Yi Ma, Julien Mairal, Guillermo SaPiro, Thomas S Huang, and Shuicheng Yan. SParse rePresenta-
tion for comPuter vision and Pattern recognition. Proceedings of the IEEE, 98(6):1031-1044, 2010.
Matthew D Zeiler, DiliP Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional networks. In 2010
IEEE Computer Society Conference on computer vision and pattern recognition, PP. 2528-2535. IEEE, 2010.
Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. ComPlete dictionary learning via `4 -norm
maximization over the orthogonal grouP. arXiv preprint arXiv:1906.02435, 2019.
Yuqian Zhang, Yenson Lau, Han-wen Kuo, Sky Cheung, Abhay PasuPathy, and John Wright. On the global
geometry of sPhere-constrained sParse blind deconvolution. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, PP. 4894-4902, 2017.
Yuqian Zhang, Han-Wen Kuo, and John Wright. Structured local oPtima in sParse blind deconvolution. arXiv
preprint arXiv:1806.00338, 2018.
Zhihui Zhu, Tianyu Ding, Daniel Robinson, Manolis Tsakiris, and Ren6 Vidal. A linearly convergent method for
non-smooth non-convex oPtimization on the grassmannian with aPPlications to robust subsPace and dictionary
learning. In Advances in Neural Information Processing Systems, PP. 9437-9447, 2019.
14
Published as a conference paper at ICLR 2020
Appendix
The Appendix is organized as follows. In Appendix A, we introduce the basic notations and technical
tools for analysis. Appendix B provides a determinsitic characterization of the optimization landscape
in population. In Appendix C, we turn our analysis of Appendix B into finite sample version.
Appendix D and Appendix E provide the detailed proof for ODL and CDL, respectively. The detailed
concentration bounds are postponed to Appendix F. Finally, Appendix G provides some ideas of
optimization methods.
A Notations and Basic Tools
A. 1 Basic Notations
Throughout this paper, all vectors/matrices are written in bold font a/A; indexed values are written
as a” Aij. We use sn´1 to denote an n-dimensional unit sphere in the Euclidean space Rn. We
let [m] “ {1,2,…，m}. We use d to denote Hadamard product between two VectorS/matrices.
For V P Rn, we use Vdr to denote entry-wise power of order m, i.e., Vdr = [vj, ∙∙∙ ,vnS . Let
Fn P Cnxn denote a unnormalized n X n DFT matrix, with }Fn} “ ?n, and F´1 “ n´1 Fni. In
many cases, we just use F to denote the DFT matrix.
Some basic operators. We use Pv and PvK to denote projections onto V and its orthogonal
complement, respectively. We let P&n—i to be the '2 -normalization operator. To sum up, we have
VVJ	VVJ	V
PvKU “ U ´mv, PvU “Mu, PSnTV =H.
Circular convolution and circulant matrices. The convolution operator f is circular with
modulo-m: (a f x)i = Xm“01 ajxi_j. For V P Rm, let s`[v] denote the cyclic shift of V with
length '. Thus, we can introduce the circulant matrix Cv P RmXm generated through V P Rm,
	v1	vm	… V3	v2	
	v2	v1	vm	v3	
Cv =	. . .	v2	. v1	. .	. . .	=[so [v] s1 [v] … sm´l [vSS .	(A.1)
	vm-1		.. ..	vm fl	
	vm	vm´1	… V2	v1	
Now the circulant convolution			can also be written in a simpler matrix-vector product form. For		
instance, for any U P Rm and V P Rm ,
u f V = Cu ∙ V = Cv ∙ u,	Cufv = CuCv.
In addition, the correlation between U and V can be also written in a similar form of convolution
operator which reverses one vector before convolution.
Basics of Riemannian derivatives. Here, we give a brief introduction to manifold optimization
over the sphere, and the forms of Riemannian gradient and Hessian. We refer the readers to the book
(Absil et al., 2009b) for more backgrounds. Given a point q P Sn´1, the tangent space TqSn_1 is
defined as TqSn_1 = {v | VJq = 0}. Therefore, we have the projection onto TqSn_1 equal to PqK.
For a function f (q) defined over Sn_1, we use grad f and Hess f to denote the Riemannian gradient
and the Hessian of f, then we have
grad f(q) = PqK Vf(q),	Hess f(q) = PqKW2 f(q) ´(q, Vf (q)〉I) PqK,
where Vf (q) and V2 f (q) are the normal first and second derivatives in Euclidean space. For
example, for the function 夕τ(q) defined in Equation (2.4), direct calculations give that
m
grad^τ(q) = ´PqK A 'AJq)d3 = ´PqK £ 'ajq)3 ak,
k“1
Hess ^τ(q) = -PqK ”3A diag ((AJq)d2) AJ ´ >Aj q>： 11 PqK.
15
Published as a conference paper at ICLR 2020
A.2 Basic Tools
Lemma A.1 (Norm Inequality) If P > r > 0,thenfor X P Rn, we have
}x}p ≤ }x}r ≤ n1/rT∕p }x}p .
Lemma A.2 Let z, r P R. We have
pi '	z)r	≤ 1 +	(2r	´ 1)z, @ Z P [0, IS, r P R∖(0,1),
pi '	z)r	≤ 1 '	rz, @ Z P r´i, '8),	r P [0,1],
where the second inequality reverse when r P Rz(0, 1).
Lemma A.3 (Moments of the Gaussian Random Variable) If X „ N 0, σX2 , then it holds for
all integer m21 that
E [|X∣mS ≤ σm (m — 1)!!, k = [m∕2U∙
Lemma A.4 (Noncentral moments of the χ Random Variable) If Z „ χ pm), then it holds for
all integer P21 that
E [ZPs = 2p{2 γ (臂 + m/2q ≤ p!! mp{2.
Γ (m∕2)
Lemma A.5 (Bernstein’s Inequality for R.V.s (Foucart & Rauhut, 2013b)) Let X1, . . . , Xp be
i.i.d. real-valued random variables. Suppose that there exist some positive numbers R and σX2
such that
m!
E [|Xk | S ≤ -^-σXR	, forall integers m22.
Let S “ P Xk “i Xk, thenfor all t > 0, it holds that
PrIS ´ E rS s1> ts≤ 2exp (´ 2σχ‰).
Lemma A.6 (Bernstein’s Inequality for Random Vectors (Sun et al., 2015a)) Let x1, . . . , xP P
Rd be i.i.d. random vectors. Suppose there exist some positive number R and σX2 such that
m!
E [}xk} S ≤ -^-σXR	, forall integers m22.
Let S “ p Xp=I Xk, thenfor any t > 0, it holds that
P [}s ´ E [sS} > tS ≤ 2(d + 1) exp (´ 2吸p").
Lemma A.7 (Bernstein’s Inequality for Bounded R.M.s, Theorem 1.6.2 of Tropp et al. (2015))
Let Xi, X2, ∙∙∙ , Xp p Rd1 Xd be i.i.d. random matrices. Suppose we have
}Xi} ≤ R almost surely,	max{>E[XiXJ‰> , >E[XJXi‰>( ≤ σX, 1 ≤ i ≤ P.
Let S “ p Xp=I Xi, then we have
P (}S ´ E[SS} > t) ≤ (di + d2) exp (´2τ2 pt4Rt3) ∙
2σX + 4Rt∕3
Lemma A.8 (Bernstein's Inequality for Bounded Random Vectors) Let xi, x2,…，xp P Rd
be i.i.d. random vectors. Suppose we have
}xi} ≤ R almost surety,	E ∣}xi}2] ≤ σX, 1 ≤ i ≤ p.
Let s “ p Xp= i Xi, then we have
P (}s ´ E rss} N tq ≤ dexP (´ 2 2 pt4Rt∕3).
2σX + 4Rt∕3
16
Published as a conference paper at ICLR 2020
3t2
´ 2t + 6
4 }△}
σmin(B),
4 }△}
σ3{in (B),
Lemma A.9 (Lemma A.4 of (Zhang et al., 2018)) Let v P Rd with each entry following i.i.d.
Berpθq distribution, then
PP∣}v}o — θd∣ 2tθdq ≤ 2exp
Lemma A.10 (Matrix Perturbation Bound, Lemma B.12 of (Qu et al., 2019)) Suppose B > 0
is a positive definite matrix. For any symmetric perturbation matrix ∆ with }∆} ≤ 1 σmin (B), it
holds that
>	(B + ∆)T/2 ´ BT{2> ≤
>	(B + ∆)1{2 bT2 ´ I> ≤
where σmin(B) denotes the minimum singular value of B.
Lemma A.11 For any q, q1, q2 P Snτ, we have
>	PqK> ≤ 1, }Pqι ´ Pq2 } ≤
Proof The first is obvious, and for the second inequality we have
>pqκ ´ PqK>	=	>qιqJ ´	q2qj>	≤	>qιqJ	´	qιqJ> +	>qιqJ	´	q2qJ>	≤ 2 }qι ´	q2},
as desired.
2 }q1 ´ q2 } .
Lemma A.12 For any nonzero vectors u and v, we have
uv
}u} }v}
2
& H }u ´ v}.
Proof We have
uv
}u} }v}
“ HlM > }v}U ´ H v>
“ }u}1}v} >}v} U ´ }v} V + }v} V ´ }u} v>
12
≤ }u} }v} (}v} }u ´ v} + }v} |}u} ´ }v}|) ≤ 西}u ´ v},
as desired.
B Analysis of Asymptotic Optimization Landscape
In this part of the appendix, we present the detailed analysis of the optimization landscape of the
asymptotic objective
mmin ψτ(q) “ ´1 >AJq>4 ,	st q P SnT
over the sphere. We denote the overcompleteness of the dictionary A P RnXm and the correlation of
columns of A with q by
K ：“ m,	Z(q) ：“ AJq = [Q …ζm]J.
n
Without loss of generality, for a given q P sn´1, We assume that
∣Q∣》∣ζ2∣》…》∣ζm∣ .
17
Published as a conference paper at ICLR 2020
Assumption. We assume that the dictionary A is tight frame with `2 -norm bounded columns
^AAJ = I,	}ai} ≤ M P1 ≤ i ≤ m).	(B.1)
K
We also assume that the columns of A satisfy the μ-incoherence condition. Namely, We have
"(4) .	1 ≤^φj^≤mIB ⅛, ⅛ FIP p0,1q,	(BZ
such that μ is sufficiently small. Based on the function value of the objective 夕τ(q), we partition the
sphere into tWo regions
Rc(q； ξ) = {q P	Snτ |	}Z}4	> ξμ2{3	}Z}2),	(B.3)
RN(q； ξ) = {q P	SnT |	}Z}4	≤ ξμ2{3	}Z}3),	(B.4)
where ξ > 0 is some scalar. In the following, for appropriate choices of K, μ, and ξ, we first show
that RC does not have any spurious local minimizers by characterizing all the critical points within
the region. Second, under more stringent condition that A is `2 column normalized, for the region
RN we show that there exhibits large negative curvature throughout the region, such that there is no
local/global minimizer within RN .
B.1	GEOMETRIC ANALYSIS OF CRITICAL POINTS IN RC
In this subsection, we show that all the critical points of 夕τ(q) in RC are either ridable saddle points,
or satisfy second-order optimality condition and are close to the target solutions.
Proposition B.1 Suppose we have
KM < 4T∙ξ32 M3 < η ∙ ξ3{2, μ < 2O	(B.5)
for some constant η < 2-6. Then any critical point q P RC with grad 夕τ(q) = 0, either is a
ridable (strict) saddle point, or it satisfies second-order optimality condition and is near one of the
components e.g., a1 in the sense that
(}a⅛, q)
>
1 ´ 5g-3{2M321 ´ 5η.
First, in Appendix B.1.1 we characterize some basic properties of critical points of 夕τ(q). Based on
this, we prove Proposition B.1 in Appendix B.1.2.
B.1.1	Basic Properties of Critical Points
Lemma B.2 (Properties of critical points) For any point q P sn´1, if q is a critical point of 夕 T (q)
over the sphere, then it satisfies
f(Zi) = Z3 ´ αiZi + βi = 0	(B.6)
for all i P rms with ζ(qq = AJq, where
}ζ}4 β	∑j‰i <ai, aj〉Zj
αi :=	2 ,	βi :=	∣∣2
}ai}	}ai}
(B.7)
Proof For any point q P sn´1, if q is a critical point of 夕τ(q) over the sphere, then its Riemannian
gradient satisfies
grad 3τ(q) = PqK AZd3 = 0	=	AZd3 ´ }Z}4 q = 0.
Multiple aJ (1 ≤ i ≤ m) on both sides of the equality, we obtain
}ai}2Z3 -}Z}4 Zi + ∑ Si, aj〉Z3 = 0.
j‰i
18
Published as a conference paper at ICLR 2020
β
z
Figure 5: Illustration of f (Z) in Equation (B.8) When β > 0.
By replacing αi and βi defined in Equation (B.7) into the equation above, we obtain the necessary
condition in Equation (B.6) as desired.	■
Since the roots of f (z) correspond to the critical points of 夕τ(q), we characterize the properties of
the roots as folloWs.
Lemma B.3 Consider the following cubic polynomial
f(z) “ z3 ´ αz ` β
with
0 < ∣β∣ ≤ ；a3{2,	α > 0.
Then the roots of the function f(z) is contained in one of the nonoverlapping intervals:
(B.8)
(B.9)
I1 :“ z P R
I3 :“ z P R
∣z∣ ≤ 2a|}, I ：= {z p R ∣z — ?a| w 2αβl),
Proof By our construction ∣β∣ ≤ 1 α3{2 and a > 0 in Equation (B.9), it is obvious that the intervals
I1, I2 , and I3 are nonoverlapping. Without loss of generality, let us assume that β is positive. We
have
f(√αq = f(´√αq = f(0)“ β > 0.	(b.io)
Thus, as illustrated in Figure 5, if we can show that
f ^2β) < 0, f (—?a—空)< 0,	f (?a- 2β) < 0,
α	αα
(B.11)
then this together with Equation (B.10) suffices to show that there exists at least one root in each
of the three intervals I1, I2, and I3. Next, we show Equation (B.11) by direct calculations. First,
notice that
f (空)“ ^空)3-β “ 4 -3) “ 乌 ^1 α3-α3) & ´1 β < 0,
α	α	α3	α3 2	2
Second, we have
f (—?a´ 空)“(—?a´ 2β)3 ´ ɑ J-空)+ β
αα	α
-81 ―商…—M + α3{2 + 3β =-W ´
α3	α3{2	α3
峪 ´ 3β < 0.
α3{2
Similarly, we have
(
2β∖ _	8β3	,	12β2	_	_ √ 8β"	12β A	/
瓦)“ ´ 嬴	+	R	´3β	= β C 嬴 +	谓	´ 3)	<

8β3
—< < 0.
α3
This proves Equation (B.11). Similar argument also holds for β < 0. Thus, we obtain the desired
results.	■
19
Published as a conference paper at ICLR 2020
B.1.2	GEOMETRIC CHARACTERIZATIONS OF CRITICAL POINTS IN RC
Based on the results in Appendix B.1.1, we prove Proposition B.1, showing that there is no spurious
local minimizers in RC .
Proof [Proof of Proposition B.1] First recall from Lemma B.2, we defined
α. “ J< > 0	β, “ ∑j‰i〈a" %〉Zj
αi	2 > 0,	βi	II ∣∣2	,
}ai}	}ai}
Then for any q P RC, we have
∣βi∣	∣∑j‰ixai, ajyζ3∣ }ai}
3/2
αi
}ζ}4
V μM3 }ζ}3 V M3ξ-3∕2
W }Z}6 V Mξ ,
(B.12)
where for the first inequality We used the fact that for any i P [m], } ai} ≤ M and
Σ〈ai，ajyζj	v ∑ B}a⅛,	}a⅛FZj	}ai}1mjaχm}aj}V	μM2 ∑	1zi13	“ μM2}ζ}3,
∣j‰i	∣	∣j‰i	i j ∣	i“1
and the last inequality derives from the fact that q P RC. Thus, by Equation (B.5) and Equation (B.12),
we obtain
ML/2 V 1 一叫 V 1.
4	αi	4
This implies that the condition in Equation (B.9) holds, so that we can apply Lemma B.3 to char-
acterize the critical points. Based on Lemma B.3, we classify critical points q P RC into three
categories
1.	All |Zi| (1 V i V m) are smaller than 2^；
2.	Only |Zi| is larger than ɪθeɪi;
3.	At least ∣Zι | and Q | are larger than 2Oe^ and 20^, respectively.
For Case 1, Lemma B.4 shows that this type of critical point does not exist under the assumption in
Equation (B.5). For Case 2, under the same assumption, Lemma B.5 implies that such a critical point
q P RC satisfies the second-order optimality condition, and it is near one of the target solution with
(}a⅛, q)
>
1 ´ 5ξ-3/2M321 ´ 5η.
for some η < 2-6. Finally, for Case 3, Lemma B.6 proves that this type of critical points q P RC
is ridable saddle, for which the Riemannian Hessian exhibits negative eigenvalue. Therefore, the
critical points in RC are either ridable saddle or near target solutions, so that there is no spurious
local minimizer in RC.	■
In the following, we provided more detailed analysis for each case.
Case 1:	no critical points with small entries.
First, we show by contradiction that ifq P RC and is a critical points, then there is at least one
coordinate, e.g., |Zi|22Oβ1∖. This implies that Case 1 (i.e., all |Zi| (1 V i V m) are smaller than
2iβii) is impossible to happen. In other words, this means that any critical point q P RC should be
αi
close to superpositions of columns of A.
Lemma B.4 Suppose we have
M4/3K1/3 < 4T3ξ.
If q P RC is a critical point, then there exists at least one i P rms such that the entry Zi of ζpqq
satisfies
IZiI > 2-≡.
αi
20
Published as a conference paper at ICLR 2020
Proof Suppose there exists a q P RC such that all entries Zi satisfying |Zi| <
2lβil. Then We have
αi
max |Zi|
1¾i¾m
}Z}8 ≤
2∣∑m^ Xaι, aky Z3∣ < 2M2μ ©3
}ζ}4	'	}ζ}4
This implies that
}ζ}4 < }ζ}8 }ζ}2 < 4M4μ2Jζ}3 }ζ}2	-
}ζ}4
=^
Hll2 < 4M4μ2 }Z}6 }Z}2
}Z}4 < 41{3M4{3K 1{3μ2{3 }Z}3 ,
Where We used the fact that }ζ}2 “ K according to Equation (B.1). Thus, by our assumption, We
have
M4{3K 1{3 < ξ{41{3	F	}Z}4 < ξμ2{3 }Z}3.
This contradicts With the fact that q P RC .
Case 2:	critical points near global minimizers
Second, We consider the case that there exists only one big ζ1, for Which the critical point satisfies
second-order optimality and is near a true component.
Lemma B.5 Suppose ξ is sufficiently large such that
M3 < η ∙ ξ3{2,	KM < 4—1Y3{2,	(B.13)
for some constant η < 2´6. For any critical point q P RC if there is only one entry in Z such that
ζι > ⅛ι,
B⅛, M N 1 ´ 5ξ-3/2M3 N 1 ´ 5η.
Moreover, such a critical point q P RC satisfies the second-order optimality condition: for any
V P sn´1 with V K q,
VJ HeSS 3τ(q)v N 20 }Z}4 .
Proof We first shoW that under our assumptions the critical point q P RC is near a target solution.
FolloWing this, We prove that q also satisfies second-order optimality condition.
Closeness to target solutions. First, if q is a critical point such that there is only one Zi N 2O^,
We shoW that such q is very close to a true component. By Lemma B.2 and Lemma B.3, We knoW
that Z1 needs to be upper bounded by
Z2 < (“# 了
2∑葭2 Si, aQZ3∣ Y
}Z14	)
<
}ζ}4 A ` 2μ }Z}3 }a1}2 maXlWjWm }aj }
By using the fact that q P RC and }aj } < M (1 < j < mq, We have
}ai }2 Zi2 <
32	2
(1 +	2"}ζ13	}a1}	max1WjWm	}aj}	)	}Z}4	<(1 + 2ξ-3/2M3)	}Z}4	. (B.14)
}ζ}4
21
Published as a conference paper at ICLR 2020
On the other hand, by using the fact that Q | ≤ 2lβkl for all k22, We have
αk
ζ4 > }ζ}4 ´ ζ2 £ ζ2 > }ζ}4 ´ 4⅛^K > }ζ}4 fι ´ ⅛∣?KM4)
k“2	α2	}ζ}4
>	}Z}4 '1 ´ 4g—3KM4) .	(B.15)
Combining the loWer	and upper bounds	in Equation (B.14) and Equation (B.15), We	obtain
B a1	F2	_	Z2	1	´	4g—3KM4	'1 ´ 4g—3KM4)
∖}a1l ,	q/	—	}aι}2?'1	+ 2ξ-3{2M 3)2》1 + 6/3{2 M 3
“	1 — 2g-3 M3(3g3{2 +	2KM)
>	1 — 8ξT2 M3 > 1 — 8η,
Where the second inequality folloWs by Lemma A.2, and the last inequality folloWs from Equa-
tion (B.13). This further gives
a a1	∖	1 — 8g-3{2M3	1 — 8g-3{2M3	-3
∖M , q/ 乡 '-8ξ-3∕2M 3 严当-4ξ-3∕2M 3 = 1 ´ 5ξ	M 乡 1 ´ 5η. (B.16)
Second-order optimality condition. Second, We check the second order optimality condition for
the critical point. Let V P Sn ´1 be any vector such that V K q, then
VJ Hess 夕τ(q)v “ 一3VJAdiag 'Zd2) AJV +}Z}4
m
“一3xaι, Vy2 ζ2 一 3 £ Xak, v〉2 ζ2 + }ζ}4
k“2
》一3 xa1, v〉2 ζ2 ´ 3ζ2 >>AJV>> + }ζ }4
“ 一3 Xa1, v〉2 ζ2 - 3Kζ2 + }ζ}4	(B.17)
Next, We control xa1, V〉2 ζ12 and Kζ22 in terms of lζ l44, respectively. By Equation (B.14) and
Xq, V〉 “ 0,
xa1,Vy2 ∙ζ2 “ B1⅛τ, VF (}aι}2 ζ2)
›	›2	2
Wl肃-q>> ´1+2∙3/2M3) }ζ}4
“2(一B 肃，qF) ´1+2∙3{2 M 3)Z
≤ 10ξ-3/2M3(1 + 2g-3{2M3)2}Z14 ≤ 1 }Z}4 .	(B.18)
On the other hand, for q P RC, using Equation (B.13) We have
KZ ≤ K4⅞^ ≤ 4KM4μ2Jζ^ ∙}Z}4 ≤ 4KM4ξT }Z}4 ≤ ɪ }Z}4 .	(B.19)
α2	lζ l4	15
Thus, combining the results in Equation (B.17), Equation (B.18), and Equation (B.19), We obtain
VJ Hess夕T(q)V2^1 一 4 一 5) }ζ}4 > 2o }ζ}4.
This completes our proof.	■
22
Published as a conference paper at ICLR 2020
Case 3: critical points are ridable saddles.
Finally, We consider the critical points q P RC that at least two entries k | and Q | are larger than
2αβ1l and 2αβ2l, respectively. For this type of critical points in RC we show that they are ridable
saddle points: the Hessian is nondegenerate and exhibits negative eigenvalues.
Lemma B.6 Suppose we have
M3 < η∙ξ3/2,	μ < 210,	(B.20)
for some Constant η < 2-6 For any critical point q P RC if there are at least two entries in Z (q)
such that ∣Zi∣ > 2αβil Pi P [m]), then q is a strict saddle point: there exists some V P SnT with
v K q, such that
vJ Hess Pτ(q)v ≤ ´ }Z}4 .
Proof Without loss of generality, for any critical point q P RC, we assume that ζ1 “ a1Jq and
ζ2 “ a2Jq are the two largest entries in ζ(qq. We pick a vector v P span
v K q with v P sn´1. Thus,
J a1	a2 }
}a }a1} , }a2} J
such that
vJ Hess 夕τ(q)v “ 一3VJAdiag (Zd2) AJv ' }Z}4
&	一3 }a1}2	ζ2	B }a⅛，vF	一 3 }a2}2	ζ2	B 篙，vF	+}ζ}4.
Since ∣Zι | 2 20βd and Q |22-021, by Lemma B.2, Lemma B.3, and the fact that q P Re, we have
}aι}2 Z2 2 }alP (M 一 ≡1 )22 (1 一 2μM [/g }a1} [ζ}4
α1	}Z }4
2 (-2ξT2M3)2}Z}4 .
In the same vein, we can also show that
}a2}2 Z > (1一2§-3{2M3)2}Z}4 .
Therefore, combining the results above, we obtain
vJ
HeSSPτ(q)v ≤ }Z}4 1 ´ 3 (1 — 2「3/2M3) (B }a1}, VF '
AS V P span I 岛,尚卜 we Can write
a1	a2
V = c1	+ c2τ-----
}a1}	}a2}
for some coefficients ci, c2 P R. As V P sn´1, we observe
}v}2
“ c2+c2+2c1c2 (}⅛, }a⅛) “1
A	c1 + c2 2 1 一 2 | cic2 | μ 2 1 一 4μ,
where the last inequality follows from Lemma B.7. Thus, we observe
Bτa⅛, vF+ B涓,vF “
^c1+Cc B 肃,涓 F) + ^c2+ciB 肃,涓 F)
'c2+c2)+'c2+c2)B含,* F+ 4c1c2 B¥1}, *F
2
2
1 一 4μ - (1 一 4μ) μ2 - 4---μ^ μ
1 一 μ2
1 一 10μ
23
Published as a conference paper at ICLR 2020
By the fact in Equation (B.20) and combining all the bounds above we obtain
VJ Hess^τ(q)v ≤ 1 ´ 3 ´l ´ 2g-3{2M3)2 (1 ´ 10μ)忆}： ≤ ´4 }Z}4 .
This completes the proof.
LemmaB.7 Suppose K}^, }a^)∣ ≤ μ with μ < 1/2. Let V P span
}v} “ 1 and V “ ci}a^ ' c? }02}, then we have
a a1	a2 ]
t }ɑ1} , }ɑ2} ʃ
such that
∣ClC2∣ ≤
Proof By the fact that Kv, }a⅛Xv, }a⅛}∣
≤ 1, We have
Kci ` Cc (}a⅛, ιS∑Kc2 ` ci<}⅛, ι⅛>)∣ w1
Which further implies that
2	2	ai	a2	ai	a2	2 ∣
c1c2 ' (c1 ' c2)∖H1,西/ ' c1c2 ∖M,同)& 1
Since }V } “ 1, We also have
c2i ` c22 “ 1
´ 2cic2
1 ' μ
t´^ ,
Combining the tWo (in)equalities above, We obtain
icc | (1 ´ B }a⅛, }aι F )」B }a⅛, }aι Fb |"2|—.
Thus, We obtain the desired result.
B.2 NEGATIVE CURVATURE IN RN
Finally, We make more stringent assumption on A that each column of A is `2 normalized, i.e.,
}ai} “ 1,	1 w i w m.
We show that the function 夕τ(q) exhibits negative curvature in the region Rn. Namely, the Rieman-
nian Hessian for any points q P RN has a negative eigenvalue, such that the Hessian is negative in a
certain direction.
Lemma B.8 Suppose each column of A is `2 normalized and
K ≤ 3 ´1 + 6μ + 6g3{5〃2{5)T.
For any point q P RN, there exists some direction d P sn´1, such that
dJ Hess4Tpqqd V ´4 }ζ}4 }ζ}8.
24
Published as a conference paper at ICLR 2020
Proof By definition, we have
aJ Hess 夕τ(q)aι
“ ´ 3aJPqKAdiag 'Zd2) AfPqKaι + }Z}4 >PqKaι>2
“ ´ 3ajAdiag 'Zd2) AJaI + 6 忆吆 ZJ diag 'Zd2) AJai ´ 3 }Z}8 }Z}4 + }Z}4 (}aι}2 — }Z}8)
&	3 }Z}8 }aι}4 + 6 }Z}8 }aι}2 + 6μ 忆吆底 ´ 3 }Z}8 }Z}4 + }aι}2 }Z}4 TZ}8 }Z}4
“一3 }ζ}8 + 6 }ζ}8 + 6μ Hz。}ζ∣∣3 ´ 4 }ζ∣∣8 }ζ}4 + ©4
& ©8(—3 + 6 }Z}8 + 6μ }Z}2 ´ 4 }Z}4 + }Z}2)
“ }Z}8 (—3 + 6 }Z}8 + 6μK ´ 4 }Z}4 + K)
where for the second inequality We used the fact that}Z}4 ≤ }ζ}8 }ζ}2, and for the last equality We
applied that }ζ}2 “ qJAAJq “ K. Moreover, as q P RN, we have
}Z}8 & }Z}2 & ξ1{2μ1{3 }Z}3
}Z}3 “
m	1{3
3l3)	W
}Z}18{3 K1{3.
Thus, We obtain
}Z}8 & g1" }Z}83 K 1/3
Hence, We have
}Z}8 ≤ ξ3{5(〃kq2{5.
=^
aJ Hess 依(9)。1 ≤ }Z}8 (-3 + 6ξ3/5 (μK)2/5 + 6μK — 4 }Z}4 + K) ≤ —4 }Z}4 }Z}8 ,
Whenever
K ≤ 3(1 + 6μ + 6ξ3∕5μ2∕5)T.
Thus, We obtain the desired result.
C Optimization Landscape in Finite S ample
In this section, We Will shoW that the finite sample objective functions in the overcomplete
dictionary learning and convolutional dictionary learning have similar geometric properties as
夕 τ(q) “ ´ 1 >Ajq>4 analyzed in Appendix B. Specifically, we will analyze the geometric properties
of objective function 夕(q) (which could be 夕DL (q) and 夕CDL (q)) whose gradient and Hessian are
close to 夕τ(q). We denote by
δg (q) ：= grad φ(q) — grad 中T (q),
Δh(q) := HeSSφ(q) — HeSS 依(q),
(C.1)
both of which will be proved to be small for overcomplete dictionary learning and convolutional
dictionary learning in Appendix F.
C.1 GEOMETRIC ANALYSIS OF CRITICAL POINTS IN RC
Proposition C.1 Assume
}δg(q)}W μM }Z}3 and	}Δh(q)} < 20 }Z}4 .
Also suppose we have
KM < 8T∙ξ3/2,	M3 < 2η∙ξ3/2, μ < ɪ	(C.2)
25
Published as a conference paper at ICLR 2020
for some constant η < 2-6. Then any critical point q P Re, with grad 夕(q) “ 0, either is a
ridable (strict) saddle point, or it satisfies second-order optimality condition and is near one of the
components e.g., a1 in the sense that
(}a11, q) N
1 ´ 5g-3{2M3 N 1 ´ 5η.
(C.3)
Proof [Proof of Proposition C.1] With the same argument in Lemma B.2, we have that any critical
point q P SnT satisfies
f (Zi) “ ζ3 ´ αiZi + βi = 0,
for all i P rms with ζ “ AJq, where
}ζ}4	o,	xδg(q)，aiy + ∑j‰i xai, ajy Zj	R . xδgpqq, aiy
Qi “ 石,	βi =	= βi +
同2 i	'I2	同2
(C.4)
with βi =
∑j‰ixai,aj yζ3
}0d2
which is defined in equation B.7.
Recall that a widely used upper bound for βi in Appendix B.1 is:
|ei| =
l∑j‰i xai, ajy Zj|	μM ©3
∣ai12	W -jair
which together with ∣∣δg(q)} ≤ μM }Z}j gives
β1 = βi +
xδg pqq, aiy
SI2
2 μM∣Z∣3
∣ai∣
(C.5)
W
To easily utilize the proofs in Appendix B.1, We define ξ1 = 21>3ξ such that ξ-3/2 = 2ξ-3/2.
Plugging the assumption M3ξ1T2 W 4 into equation C.5, we have
同 μM∣Z∣3 IlaiII2
QF	W 2 一屈一
W 2"M316ζ∣3 W 2M3g-3{2 W 2M313{2 W ɪ.
This implies that the condition in equation B.9 holds, so that we can apply Lemma B.3 based on
which we classify critical points q P RC into three categories
1.	All |Zi| (1 W i W m) are smaller than 2Oil;
2.	Only ∣Z1∣ is larger than 2011;
2∣β11	, 2∣β21	. 1
3.	At least ∣Zι ∣ and ∣Z2∣ are larger than ɪ-L and ɪ-ɪ, respectively.
For Case 1, using the same argument as in Lemma B.4 we can easily show that this type of critical
point does not exist. For Case 2, with the same argument as in Lemma B.5, we obtain that such a
critical point is near one of the target solution with
(高,q)N
1 ´ 5g―3{2M3 N 1 ´ 5η,
and satisfies the second-order optimality condition, i.e., for any V P sn´1 with V K q, we have
VJ Hess Wq)V N VJ Hess 中T(q)v TΔh(q)} N 20 }Z14 ´ }Δh(q)口.
Finally, for Case 3, with the same V constructed in Lemma B.6 and using the assumption ∣∣Δh(q)} <
210 IZ∣4,we have
VJ HessHq)V W VJ Hess 夕T(q)V +}Δh(q)} W ´ }Z14 + }Δh(q)} < 0,
indicating that this type of critical points q P RC is ridable saddle, for which the Riemannian Hessian
exhibits negative eigenvalue. Therefore, the critical points in RC are either ridable saddle or near
target solutions, so that there is no spurious local minimizer in R0.	■
26
Published as a conference paper at ICLR 2020
C.2 NEGATIVE CURVATURE IN RN
By directly using Lemma B.8, We obtain the negative curvature of 夕(q) in Rn.
Lemma C.2 Assume
}Δh (q)}<}Z}4}Z}8∙
Also suppose each column of A is `2 normalized and
K ≤ 3 ´l + 6μ + 6g3{5〃2{5)T.
For any point q P RN ,there exists some direction d P sn´1, such that
dJ HeSS vpqqd V ´3 }ζ}4}ζ}8.
Proof First, it follows Lemma B.8 that for any point q P Rn, there exists some direction d P sn´1,
such that
dJ HeSsoτ(q)d V ´4 }ζ}4}ζ}8,
which together with the assumption }∆H (qq} V }Z}4 }Z}8 and the fact dJ Hessφ(q)d “
dJ Hess 夕 T (q)d ' dj Δh (q)d ≤ dJ Hess 夕 τ(q)d ' }Δh (q)} completes the proof.	■
D Overcomplete Dictionary Learning
In this section, we consider the nonconvex problem of
min 皿⑷“——I1— θ)p >qJγ>4 “——J θ)p >qJAx>4，s.t. }q} “1
We characterize its expectation and optimization landscape as follows.
D. 1 Expectation Case: Overcomplete Tensor Decomposition
First, we show that 夕dl(q) reduces to 夕τ(q) in expectation w.r.t. X.
Lemma D.1 When X is i.i.d. drawn from Bernoulli Gaussian distribution as in Assumption 2.2,
then we have
θ m2
EX rWDL(qqs = OTpq) ´ 2(T—i)⑴.
Proof Let Z = AJq P Rm with }Z}2 = mm .By using the fact that
X = rxι x2 … xpS, xk = bk d gk, bk 〜Ber(θ), gk 〜N(0, I),
we observe
EX MLpq)S = ´ 12(1 ´ θ)θpEX ∣>ζJx>41
1
12(1 — θ)θp
1
p
∑ Exkl(ZJxk)4]
k“1
12(1 ´ θ)θEb,g
xζdb,gy4



4(T⅛Eb ”}Z d b}41.
Write }z d b}2 “ Xm=I (Zkb®)2, we obtain
EX r3DL(q)s = ´ 4(1 ´ θ)θ Eb- (g (Zk bk q2)
´4 }z}4 ´
m
∑ Z4 ´
k“1
θ
2(1—^)
2(⅛ 平 zj2
i-“j
}z}4

1
w´θ
WTpqq ´ 2(d⅛ ´m),
27
Published as a conference paper at ICLR 2020
as desired.
D.2 Main Geometric Result
Combining Proposition C.1 and Lemma C.2 together with the concentration results of the gradient
and Hessian in Proposition F.3 and Proposition F.6, we obtain the following geometry results of
overcomplete dictionary learning.
Theorem D.2 Suppose A satisfies Equation (2.2) and X P RmXp follows BG(θ) with θ P '∖, 2).
Also suppose we have
K < max "8T∙ξ3/2,3(1 +	6μ	+	6ξ3{5μ2{5) ´1 * ,	1 <	2η ∙	ξ3{2,	μ < 20
for some constant η < 2—6.
log(θn7{2 ∕μ)
• If PeCθK3n3 max
μ
,Kn2 log(θn2)} , then with probability at least 1 — ep´2, any
critical point q P RC of 夕DL (q) either is a ridable (strict) saddle point, or itsatisfies second-order
optimality condition and is near one of the components e.g., a1in the sense that
Bɪa^, qF21 ´ 5「3/2M3 > 1 ´ 5η.
}a1}
• If peCθK4n6 log(θn5), then with probability at least 1 — ep´2, any critical point q P RN of
夕DL(q) is a ridable (strict) saddle point.
Here, c, C > 0 are some numerical constants.
Proof First note that for overcomplete dictionary A in Equation (2.2), it satisfies Equation (F.9) with
M “ 1. Now it follows from Proposition F.3 and Proposition F.6 that when
5>Γ0K5^ogpθ ax# logpθKn{μ }ζ}3q Kn B * logpθKn{}ζ}4q +	(Dn
p N CθK n max	,,, ,	(D.1)
μ2 }Z}6	}Z}8	r
then with probability at least 1 — ep´2,
sup }grad 夕DL(q) — grad 夕τ(q)} ≤ μM }Z}3 ,
qeSnτ
SUp }Hess ^DL(q) — Hess ^τ(q)} < ɪ }Z}4 ,
qpSn-1	20
which together with Proposition C.1 implies that any critical point q P RC of 夕dl(q) either is a
ridable (strict) saddle point, or it satisfies second-order optimality condition and is near one of the
components e.g., a1in the sense that
B ι⅛, qFN1—5η∙
We complete the proof for q P RC by plugging inequalities }Zʤ N m´1{6 }Z。“ K 1∕3n_1/6 and
}Z∣∣4 N m´1/4 ∣∣Z}2 “ K 1∕4nT∕4 intoEquation(D.1).
Similarly, by Proposition F.6, when
N CθK6n3
p
log(θKn∕}Z}4}Z∣∣8)
}Z ∣4 }Z ∣8
(D.2)
then with probability at least 1 — ep´2,
sup }Hess 夕DL(q)— Hess 夕τ(q)} < max }Z∣8 }Z}4 ,
qpSn-1
which together with Lemma C.2 implies that any critical point q P RN of 夕DL (q) either is a ridable
(strict) saddle point. The proof is completed by plugging ∣∣Z∣8 N n11/2into Equation (D.2).
28
Published as a conference paper at ICLR 2020
E Convolutional Dictionary Learning
Y
xi
In this part of appendix, we provide the detailed analysis for CDL. Recall from Section 3, we denote
Cy1	Cy2	■	■ ■	Cyp	P RnXp	,	Ao = rCa1	Ca2	■ ■ ■	CaK S P R"m,
» Xi1 fi			» Cxi1 fi			
— Xi2 ffi . .	P Rm ,	Xi =	Cxi2 ffi . .	P Rmxn,	X =	X1	X2	■■■ XpsP Rnxnp,
. XiK			. CxiK			
A “ (KTAoAj)T/2 Ao, m “ nK.
For simplicity we let
Recall from Section 3, for CDL we make the following assumptions on A0, A and X.
Assumption E.1 (Properties of A0 and A) We assume the matrix A0 has full row rank with
minimum singular value: σmin(Ao) > 0, condition number: κ(A0):“
In addition, we assume the columns of A are mutually incoherent in the sense that
σmaxp A0q
σminpA0q
max
i-“j
μ.
Assumption E.2 (Bernoulli-Gaussian xik) We assume entries of xik „i.i.d. BGpθq that
Xik “ bik d gik,	bik 〜i.i.d. Ber(θ), gik 〜i.i.d. N(0, I),	1 ≤ i ≤ p, 1 ≤ k ≤ K.
In comparison with Assumption 2.1, it should be noted that the preconditioning does not necessarily
result in '2-normalized columns of A. But their norms are still bounded in the sense that
}ak}2 ≤ >AJak> ≤ ?K}ak}	=^	}ak} ≤ ?K, 1 ≤ k ≤ nK.	(E.1)
Because of the unbalanced columns of A, unlike the ODL problem, the CDL problem
qmnnι OCDLpq) “ ´ 12θ(1 ´ θ)np >qJpY>4 “ ´ 12θJ θ)p >qJPA0X>4
does not have global geometric structures in the worst case. But still we can show that the problem is
benign in local regions in the following. Moreover, we also show that we can cook up data driven
initialization which falls into the local region.
E.1 Main Result of Optimization Landscape
In this part, we show our main result for optimization landscape for CDL. Namely, consider the
region introduced in Equation (3.3) as
Rcdl := {q P SnTl Oτ(q) W —ξcDL κ4{3μ2{3 }Z(q)}2 ),
where ξcDL > 0 is a fixed numerical constant. We show the following result.
Theorem E.3 (Local geometry of nonconvex landscape for CDL) Let Co > 5 be some constant
and η < 2-6. Suppose we have
θ p(~~t7,	, ξCDL = Co ∙犷2/3& μ <	, k < Co,
nK 3	40
and we assume Assumption E.1 and Assumption E.2 hold. There exists some constant C > 0, with
probability at least 1 — ci (nK)一°2 over the randomness of Xiks, whenever
p
2CθK2μ-2n4 max
"K6κ6(Ao)
1 σmm(Ao),
n log6
(m∕μq,
every critical point qc of OCDL (q) in RCDL is either a strict saddle point that exhibits negative
curvature for descent, or it is near one of the target solutions (e.g. a1) such that
29
Published as a conference paper at ICLR 2020
Proof Noting Equation (E.1), We set M “ VK in Proposition C.1. It follows from Proposition E.11
that when
P > CθK 4n2 log5(mK q max "	, n* ∙ max [联"；总；Z}3), ⅛Kn^ [,
I σminpA0q	)	[	μ2K }ζ}3	}ζ}4	J
(E.2)
then with probability at least 1 — cιPnK)一°2,
SUp }grad^cdl(q) — grad^τ(q)} ≤ μ√K }Z}3 ,
qpSnτ
SUp }Hess^cDL(q) — Hess^τ(q)} V ɪ }Z}4 .
qpSnτ	20
Thus, by using Proposition C.1, we have that any critical point q。P RCDL of 夕cdl(q) either is a
ridable (strict) saddle point, or it satisfies second-order optimality condition and is near one of the
components, e.g., a1 in the sense that
B Ja^, qc)21—5ξcD{2 K 3{2K~221—5ηκ-2,
where we have plugged M “ √Κ and ξ “ ξcDLκ4/3 in Equation (C.3). Finally, we complete the
proofby using inequalities }Zʤ 2 m´^6 }Z∣∣2 = K1{3n_1{6 and ∣∣Z}4 2 m´1" }Z∣∣2 = K 1{4n-1{4
in Equation (E.2).
E.2 Proof of Optimization
In the following, we show that with high probability Algorithm 1 with initialization returns an
approximate solution of one of the kernels up to a shift.
Proposition E.4 (Global convergence of Algorithm 1) With m “ nK, suppose
logm	向-2{3
c1 F W θ ≤ Cc mm log m ' min
W *
m2 log m .
(E.3)
Whenever
p 2 CθK2μ-2 max " *c K P„ , n* n4 log6 (m∕μ),
σm2 in (A0q
our initialization in Algorithm 1 satisfies
qinit P RCDL
q P SnT | 2τ(q) ≤ —ξcDL μ2{3κ4{3K) U Rcdl,
(E.4)
such that all future iterates of Algorithm 1 stays within RCDL and converge to an approximate
solution (e.g., a circulant shift s` ra01s of a01) in the sense that
>Psn—i (PTq*) — s` [αoι]> ≤ e,
where is a small numerical constant.
Proof Note that RCDL J RCDL is due to the fact that
››AJq››32 W ››AJq››2 “ K.
We show that the iterates of Algorithm 1 converge to one of the target solutions by the following.
Initialization falls into Rcdl. From Proposition E.5, taking ξ “ ξcDLK4/3, with θ satisfies
Equation (E.3), whenever
,「K2 κ1o∕3(Ao) 一、
P2 C1 而σ□Ar log(mq,
w.h.p. our initialization qinit satisfies gT(qinit) ≤ —2ξcDL μ2{3K4{3K.
30
Published as a conference paper at ICLR 2020
Iterate stays within the region. Let tqpkqu be the sequence generated by Algorithm 1 with qp0q “
qinit. From Proposition E.12, we know that whenever
P > C2
θK2
μ43κ{3 max
K6κ6pA0q
σmin(A0),
n} n2 log (θnμ∙∕3κT∕3) log5(mK),
we have
su´i 卜CDL(q)(* 2(1⅛K2)
≤ 1 ξcDL μ2∕3κ4∕3K,
which together with the fact that the sequence {qpkq} satisfies 夕cDL(qpkq) ≤ 夕cDL(qp0q) implies
gτ(qpkq) ≤ ^CDLpqpkqq ' 9∩ K 'K2 ' lξCDL μ23κ43K
2(1 ´ θq	2
≤ 3CDL(q(0q) + ^l^K2 + 1 ξcDL μ2∕3κ4∕3K
2(1 ´ θq	2
W 夕τ(qp0q) + ξCDL μ2/3κ4/3K W —ξCDL μ2{κ4{K.
Closeness to the target solution. From Theorem E.3, we know that whenever
p
2CθΚ2μ~2n4 max
"K6κ6(A0)
1 σmm(A0),
n log6
(m∕μ),
the function 夕cdl(q) has benign optimization landscape, that whenever our method can efficient
escape strict saddle points, Algorithm 1 produces a solution q‹ that is close to one of the target
solutions (e.g. a1, the first column of A) in the sense that
with ε “ κ´2η. In the following, we show that our final output a< “ Psn-ι (P11q*) should be
correspondingly close to a circulant shift of one of the kernels ta0kukK“1. Without loss of generality,
suppose q‹ “ a1 , then the corresponding solution should be a01 with zero shift (or in other words,
the first column a01 of A0). In the following, we make this rigorous. Notice that
>>PSn-1 (P hJ ´ a。/1 “ PSn-I (P hJ ´ PSn-I
*}} W 2 ">PTq",
where for the last inequality we used Lemma A.12. Next, by triangle inequality, we have
IlPSnT (PTq*) ´ a01>>
W 2}a1} ›P
´1 a1	a01
}a1}	}a1}
“ 2∣´PT(KTA0AJ)T∕2 — I)a01
W 2 l^看YYJy2 (A0AJ)T2
+ 2}a1}
Let δ P (0, 1q be a small constant. From Lemma E.18 and Corollary E.19, we know that whenever
P…K3σ≤A⅛产…
we have
|(就 YY J )1/2(A0AJ)T2
—I W δ, ∣PT| W 2Kτ∕2∣A0}.


Therefore, we obtain
IPSn-I (P-1q<) — a0i>> W 2δ + 4?2 IlA0} ?£
W 2δ + 4?2?nGmax(A0)KT W 2δ + 4?2?n W e
when η is sufficiently small. Here, is a small numerical constant.
31
Published as a conference paper at ICLR 2020
E.3 Proof of Initialization
In this subsection, we show that we can cook up a good data-driven initialization. We initialize the
problem by using a random sample (1 ≤ ' ≤ P)
qinit = Psn-1 (Pyg),	1 ≤ ' ≤ P,
which roughly equals to
qinit « Psn-i (Axg),	AJqinit « KpPsm— (AjAχg).
For generic kernels, AJ A is a close to a diagonal matrix, as the magnitudes of off-diagonal entries are
bounded by column mutual incoherence. Hence, the sparse property of xg should be approximately
preserved, so that Ajqinit is spiky with large ›Ajqinit ›44. We define
Zinit = AJqinit,	Pinit = VKP&m-1 (AJAxg).
By leveraging the sparsity level θ, one can make sure that such an initialization qinit suffices.
Proposition E.5 Let m = nK. Suppose the sparsity level θ satisfies
log m
c1-----
m
Kμ73
≤ Cc ξmogm ∙ min
μT	*
m2 logm .
≤ θ
Whenever
K2	κ6(Ao)
P / Cμ4{3ξ2θσmin(Ao) lOg(m)，
for some ξ > 0 we have
} Zinit} 4 爪 ξKμ2{3
holds with probability at least 1 — cm~cc. Here, ci, cc, c, C, C > 0 are some numerical constants.
Proof By using the convexity of '4-loss, We can show that the values of }Zinit}4 and >>Zinit >> are
close,
} Zinit }4 N Ipinit |4 + 4 Apdit, Zinit
Zpinit ››4 — 4
4
›4
Zpidni3t ›› ››Zinit — Zpinit
^ lζinit∣4
— 4K3{2 ››Zinit — Zpinit ››
loooooooooooon
small
(E.5)

Thus, it is enough to lower bound >>Zinit >> . Let I = supp(x'), and let PI : Rm → Rm that maps all
off support entries to zero and all on support entries to themselves. Thus, we have
Zpinit›4
>
K2 MJAx'∣L∣∣AJAx'>>4
K2 (>>PI (Aj Axe) >>2 + IlPIC (AjAxe) F) ´2 >>PI (AJ Ax') >>4
K2	4
77—2 IIPSn-i (PI (AJAx'))l>4 ,
(1 ` ρ)	4
}Pic (AJAx')} W
with P :=	帜工(AJAxg)[	∙ By Lemma E.7 and Lemma E.9, whenever
logm	〃-2
c1 F ≤ θ ≤ c2 mogm,
32
Published as a conference paper at ICLR 2020
we have
>Pic (AJAx')> ≤ CιKμmOθlogm,	>Pi (AJAxg) >>?K√θm
holding with probability at least 1 —。3恒一°4, so that
P “	(AJAx`)) W C2μ2mlogm
Thus, we have
Bnit>4 > K2(1 + ρ)∙>Psmτ (PIAJAxg)>4 > μ4：；K2 m >Psmτ (PIAJAxg)>4 .
By Lemma E.10, we have
>psmτ (PIAJAx`) >4 > 2θm
with probability at least 1 — C5m~c6. Thus, with high probability, We have
>pinit>4 >	4 C3K22	∙ ɪ > 2ξKμ2{3,	(E.6)
4 μ4m2 log2 m 2θm
whenever
θ W C κμ"2{ ∙	1	.
ξm μ4m2 log2 m
Finally, Lemma E.6 implies that for any δ P (0, 1q, whenever
P > C5θTK3 fpAAq. s´2 log(m),
σmin (A0q
it holds that
>>ζinit — ζpinit >> W δ,
with probability at least 1 — c7(m)—c8. Choose δ such that
4K3{2 >Zinit — ζinit> W 4K3{2δ W ξKμ2{3 一 δ W C6ξKT{2〃2{3,	(E.7)
then by Equations (E.5) to (E.7) we have
}Zinit}4 > >Zinit>4 ´ 4K 3{2 >Zinit ´ Pmit > > ξKμ2{3.
Summarizing all the result above, We obtain the desired result.	■
Lemma E.6 Let δ P (0, 1q. Whenever
P > CθTK3 fpAAq δ´2 log(m),
σmin (A0q
we have
>>ζinit — ζpinit >> W δ
with probability at least 1 — c1(Knq~c2. Here, ci, c2, C > 0 are some numerical constants.
33
Published as a conference paper at ICLR 2020
Proof By definition, we observe
>Zinit ´ Pinit> = >AJPsnτ (Pye) ´ ?KPsn-1 (AJAX')>
“ AJPSn´i ((θKmpYYJ) 1 Aox) ´?KPSnT 'AJAX')
´	´ -112
_ AJ (θmpYYJ)	A0x'	AJAxe
“	>(就YYJ)T2A0X'> ´ 1Axer
≤ -2}A}>^ɪYYJ) 1{2 AoX' ´ 'AoAJ)-1{2 AoX'
}Axe} › θmp	0	›
& 2?KJX⅛}Ao} 11^ɪYYJ) 112 ´ 'AoAJ)-112
}Axe}	› θmp	0	›
=2?K}”(Im YYJ)T2TA0aj)t]
where for the first inequality we invoked Lemma A.12, and the last equality follows the fact that
minimum singular value of A is unity. Next, by Lemma E.18, for some P (0, 1), whenever
P 2 Cθ-1K2 K (AOq e-2 log(m),
σm4 in (A0)
we have
>Zinit ´ Zinit] ≤ 8?K ∣A0∣∣ e
holding with probability at least 1 — cι(m)-c2. Here, ci, ,c1, C 〉 0 are some numerical constants.
Replace δ = 8√K ∣∣Ao ∣∣ e, we obtain the desired result.	■
Lemma E.7 Suppose the columns of A are μ-incoherent and satisfies Assumption 3.1, and suppose
xe satisfies Assumption E.2. LetI = supp (xeq. For any t 2 0, we have
>Pic (AJAXe)> ≤ >offdiag (AJA) X'> ≤ t
holds with probability at least 1 —
4m eχp (´ min {4κ∕2θm2，4κμm√m }).
Proof Since we have
>Pic (AJAX')> ≤ >offdiag (AJA) X'> ,	(E.8)
we could bound ›PIcAJAxe› via controlling ›offdiag AJA xe› . Let
m
M = offdiag (AJA) = [mi … rn∙m]p Rm'm,
and
s = Mxe
mkxek .
k“1
Thus, we can apply vector version Bernstein inequality. By Lemma A.3 and the fact that ∣mk ∣∣ ≤
K μy∕m,
Ersks	= 0, ErIlskIIps	= θ ImklIpEg„Np0,1q r∣g∣ps ≤	m2!θ (Kμ√m)p.
Therefore, by applying Lemma A.6, we obtain
P (>offdiag (AjA) X'> 2 t) = P [χ sk — E [ss
2t
≤ 2(m ' 1) exp (—
t2
2μ2K2θm2 ' 2Kμm√mt) .
Finally, Equation (E.8) gives the desired result.
34
Published as a conference paper at ICLR 2020
Lemma E.8 We have
›diag (AJA) X'>2 ≤ K2θm ' t
(E.9)
with probability at least 1 — exp (— 1 min {κ⅛m, Ktm
Proof First, let
d “ diag (AJ A),
s“
>diag (AJA) X'>2
m
dk x2k,
k“1lomon
sk
where by Lemma A.4, we have
E r∣sfcIpS ≤ θK2pp2p, ErSs “ θ >diag (AJA) >F < K2θm.
Thus, by Bernstein inequality in Lemma A.5, we obtain
P (Miag (AJA) x'>2 ´ K2θm N DW eχp (´4K4，m' A"),
as desired.
LemmaE.9 Suppose X' satisfies Assumption E.2. Suppose X' satisfies Assumption E.2. Let I “
SuPP (x'). Whenever θ satisfies	ci 4 W θ W C2	,	(E.10) m	m log m
we have	>PI (AJAx')>2 N 2K2θm	(E.11)
with probability at least 1 — mT Here, c, ci, c2 > 0 are some numerical constants.
Proof Notice that
>Pi (AjAx')>2
“ ››diag (AJA) X' ' PI (offdiag (AJA) X') >2
“ >diag (AJA) X'>2 ' >PI (offdiag (AJA) X')>2 ' 2 @diag (AJA) X',PI (offdiag (AJA) xg))
N ››diag (AJA) xg>2 ´ 2 >diag (AJA) xg> >PI (offdiag (AJA) xg)> .
By Lemma A.9, Lemma E.7, and Lemma E.8, we have
››diag (AJA) X'>2 ≤ K2θm ' CiK2aθmlogm
>PI (offdiag (AJA) xg)> W C2θKμm√log m
holds with probability at least 1 — m~c0. Thus, We obtain
>PI (AJAx')>2 N K2θm
(l ´ CiCIogmm — C3μVθm log m).
Finally, by using Equation (E.10), we have
>PI (AJAx')>2 N 1K2θm
as desired.
35
Published as a conference paper at ICLR 2020
Lemma E.10 Suppose X' satisfies Assumption E.2. Let I “ SuPP (xg). Whenever θ P
then we have
2
IIPsmT (PI(AJAx'))>[4 > 2^m
with probability at least 1 — m~c.
Proof By Lemma A.1, we know that for any z,
}z}4》}zh1 }z}4 ,
andthefaCtthat >>Psm—i (PI (AJAx`))h = }x'}0,wehave
∣∣Psmτ 'PI (AJAx'))∣∣4》}x'}”
By Lemma A.9, we have
}x'}o ≤ 2θm -	IlPSmT(Pi (AJAx'))>>4 > 2θ1m
holds with probability at least 1 — m´C
E.4 Concentration and Perturbation
We prove the following concentration results for Riemannian gradient and Hessian, and its function
value.
Proposition E.11 For some small δ P (0, 1q, whenever the sample complexity satisfies
P N cδ"θK 4maχ" Kmn^, n*n2 log ^ θKn )log5(mKq,
we have
SuP }grad夕cdl(q) — grad夕τ(q)} ≤ δ
qeSnτ
SuP }Hess夕CDL(q)— HeSS夕τ(q)} ≤ δ
qpSn—1
hold with probability at least 1 — cι(mKq~c2. Here, ci, c2, C > 0 are some numerical constants.
Proof Let 0cdl(q) be introduced as Equation (E.12)
1
pCDL(q)“—
12θ(1 — θqnp
IIqJAXII44,
so that we bound the Riemannian gradient and Hessian separately using triangle inequalities via
PCDL (q).
Riemannian gradient. Notice that
SuP }grad 夕 cdl(q) — grad 夕 T (q)}
qeSnτ
≤ SuP }grad 夕cdl(q) — grad Pcdl (q)} ' SuP }grad Pcdl (q) — grad 夕τ(q)}.
qpSn_1	qpSn-1
From Proposition E.13, we know that whenever
P N CiθK10 TA?、δ-2n2 log5(mK),
σmin (A0 q
we have
δ
SuP }grad夕cdl(q)— grad夕cdl(q)} W -
qeSnτ	2
36
Published as a conference paper at ICLR 2020
with probability at least 1 — cιPmKq´c2. On the other hand, Corollary F.9 implies that whenever
P > C2δ∙θK5n2 log
θKn
(，,
we have
SUp }gradPcdl(q) — grad心(q)} ≤
qpSn-1
δ
2
holds with probability at least 1 — c3np~2. Combining the bounds above gives the desired result on
the gradient.
Riemannian Hessian. Similarly, we have
SUp }Hess 夕 cdl(q) — Hess 夕 T (q)}
qpSn—1
≤ sup }Hess夕CDL(q)— HessPcdl(q)} ' sup }HessPcdl(q)— Hess夕τ(q)}.
qpSn—1	qpSn—1
From Proposition E.15, we know that whenever
P > C3θK10 TAP、δ∙n2 log5(mK),
σmin(A0)
we have
δ
sup }Hess 夕cdl(q)— Hess 夕cdl(q)} W -
qpSn—1	2
with probability at least 1 — c4(mK)´c5. On the other hand, Corollary F.10 implies that whenever
p N C4θK6δ∙n3 log (θKn{δ),
we have
δ
sup }Hess夕DL(q)— Hess夕τ(q)} < 3
qpSn—1	2
holds with probability at least 1 — c4np~2. Combining the bounds above gives the desired result on
the Hessian.	■
Similar to Lemma D.1, for convolutional dictionary learning, asymptotically we have
Ex [夕cDL(q)s « Eχ rpcDL(q)s “ 夕T(q) — 2(1 :。)K2, 夕τ(q) “ 一4 >qjA>4.
Next, we turn this asymptotical results into finite sample for the function value via concentration and
preconditioning.
Proposition E.12 For some small δ P (0, 1), whenever the sample complexity satisfies
P > Cr2*4 ma” Kmn^, n*n2log ^ 竽)log5(mκ q,
we have
sup
qpSn—1
夕 CDL(q)—(2T(q) — 2(1 — θ)
Wδ
hold with probability at least 1 — cι(mK)—c2. Here, ci, c2, C > 0 are some numerical constants.
Proof By triangle inequality, we have
qsu―1 卜 CDLpq)—卜 TpqX 2(⅛ KI
W sup I^CDL (q) — 0CDL(q)∣ + sup |pCDL(q) — EX [PcDL(q)]∣.
qpSn—1	qpSn—1
loooooooooooooooooooooooooooooooooooon	loooooooooooooooooooooooooooooooooooooooooon
T1	T2
Thus, by using Corollary E.14 we can control T1. For T2, we can control in a similar way as Corollary
F.9 or Corollary F.10. For simplicity, we omitted here.	■
37
Published as a conference paper at ICLR 2020
E.5 Preconditioning
In this part of appendix, let us introduce
OCDLpqq = ´ 12θ(l)θ)np >qjppA0qx> , pCDLPqq :“ ´ I2θ(l∙θ)np >qJAX> .
(E.12)
In the following, we show that the differences of function value, Riemannian gradient, and Hessian
of those two functions are small by preconditioning analysis. For simplicity, let us also introduce
v0(qq “ XJ (PA0qJq,	v(qq “ XJAJq.	(E.13)
E.5.1 Concentration and preconditioning for Riemannian gradient and
FUNCTION VALUE
First, the gradients of OCDL (qq and OpCDL(qq and their Riemannian variants can be written as
▽夕CDLpq) “ 一帖门 1 Zn—PAOXvd3,	VpCDLpqq “ ——门 1 Zn—AXvd3,
3θ(1 ´ θqnp	0	3θ(1 ´ θqnp
grad OCDLpqq “ PqK VOCDLpqq,	grad OpCDL pqq “ PqK VOpCDLpqq,
where recall from Section 3 that we introduced the following preconditioning matrix
P “ ^ɪYYJ)T2 “ «A0(ɪ W XiXJ) AJffT2.
θKmp	θKmp i“1
In the following, we show that the difference between grad OCDL pqq and grad OpCDLpqq is small.
Proposition E.13 Suppose θ P (ɪ, 1). For any δ P (0,1), whenever
P > CθK10 fpAOq、δ∙n2 log5(mK),
σmin(A0q
we have
sup }grad夕CDLpq)— gradPCDLpq)} ≤ δ
qeSnτ
sup } V夕CDLpq) — VpCDLpq)} ≤ δ
qeSnτ
with probability at least 1 — cιpmK)—c2. Here, ci, c2, C > 0 are some numerical constants.
Proof Notice that we have
sup }grad OCDL pq) — grad OpCDL pq)}
qeSnτ
≤ sup }V 夕 CDLpq) — VpCDLpq)}
qρSnτ
& 一1 1 小 sup >PAoXvd3 ´ AXvd3 >
3θp1 — θ)np qpsn´1
≤ 3θp1 —	θbD	^	sup	>PA0X	“vd3 — vd3‰> +	sup >pPA0 —	A)Xvd3>).
3θp1	θ)np ∖ qpSn — 1	qpSn — 1	J
loooooooooooooooooooooooooooooooooooon	loooooooooooooooooooooooooooooooon
y	-	y	-
T1	T2
Controlling T1 . For the first term, we observe
T W 3θp1⅛p }PA0}}X}qPX >vd3- vd3>,
38
Published as a conference paper at ICLR 2020
where for all q P SnT We have
>vd3 - vd3> ≤ >vd2 ´ vd2>8 H + }v ´ v0} }v0}8
≤ ?K ´?K +}PAo })}PAo ´ A}(maχ∕Xek}) }X}
+ }PAo ´ A}}X}}PA0}2 ^max }Xek}12
IWkWnp
& ´?K +}PAo})2}X} ^1mwnp}Xek})2}PA。— a}
where for the last two inequalities we used Lemma E.16. Thus, we have
2	2
T & (?K +}PAo}) }PAo}}X}2(1mwnp}Xek}) }PAo ´ A}.
Controlling T2. For the second term, by Lemma E.16, we have
T ≤ }PAo ´ A}}X}}v}3 ≤ K3/2}X}2^ max }Xe® jllPA。— A}.
1WkWnp
Summary. Putting all the bounds together, we have
sup }grad 夕 cdl(q) — grad Ocdl (q)}
qpSnτ
≤ *	„´?K + }PAo})2 }PAo} + K3{21 }X}2 ^ max }Xe® j }PA。— A}.
3θ(1 — θqnp	1WkWnp
By Lemma E.17 and Lemma E.20, we have
}X} ≤ ?aθmp,	max }Xek} ≤ 4?θm Iog(Kp)
1WkWnp
with probably at least 1 — 2p-2. On the other hand, by Lemma E.19, there exists some constant
C > 0, for any e P (0,1) whenever
P N C0-1K3 K6(A?、e´2 log(mK),
σmin (A0 )
we have
}PAo ´ A} ≤ e,	}PAo} ≤ 2?K
hold with probability at least 1 — ci(mK)—c2 for some numerical constants ci, c2 > 0. These
together give
Ti ≤ CK5{2θm log2 (Km)e.
Replacing δ = CK5{2θm log2 (Km) e gives the desired result.	■
Here, the perturbation analysis for gradient also leads to the following result
Corollary E.14 For some small δ P (0, 1), under the same setting of Proposition E.13, we have
SUp |OCDL(q) — PcDL(q)| ≤ δ
qpSn-1
hold with probability at least 1 — cι(mK)—c2. Here, ci, c2 > 0 are some numerical constants.
Proof Under the same setting of Proposition E.13, we have
sup |OCDL (q) — OpCDL (q)|
qeSnτ
“ SUp	4 3θ(1 1θM4	}v0}4	— 3θ(1	1	θ∖rι”	}v}4
qpsn—i	4 3θ(1 — θ)np	3θ(1	—	θ)np
“ sup 1 一门 1 Zn	@q, PAOXvd3 — AXvd3D
qpSn—1 4 3θ(1 — θ)np
1δ
≤ SUp }Vocdl(q) — Ocdl(q)} ≤	,
4 qpSnτ	4
as desired.
39
Published as a conference paper at ICLR 2020
E.5.2 Concentration and preconditioning for Riemannian Hessian
For simplicity, let v0 and v be as introduced in Equation (E.13). Similarly, the Riemannian Hessian
of 夕cdl(q) and Pcdl (q) can be written as
Hess ℃DL(q) “ ´	1	PqK ”3 (PA。)X diag (v92) XJ(PAO)J ´ }vo}4 l] PqK,
3θ(1 ´ θqnp	4
HessPcdl(q) “ ´3θp1 ´ θqnpPqK ”3AXdiag (vd2) XJAJ — }v「I] PqK,
respectively. In the following, we show that the difference between grad 夕CDL (q) and grad PCDL (q)
is small.
Proposition E.15 Suppose θ P (ɪ, 1). For any δ P (0,1), whenever
P > CθK10 f (A0q、δ∙n2 log5(mK),
σmin(A0)
we have
sup }Hess夕cdl(q)— HessPcdl(q)} ≤ δ
qpSn—1
with probability at least 1 — cι(mK)´c2. Here, ci, c2, C > 0 are some numerical constants.
Proof Notice that
sup }Hess 夕CDL (q) ´ Hess Pcdl (q)}
qeSnτ
1
θ(1 — θ)np
≤
sup >(PA。— A) Xdiag(vd2) XJ(PAO)J
qpSn—1 U
loooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo
v
T1
+ Arl	Izn- sup >AXdiag (vd2) X (PAo — A)j>
θ(1 — θ)np qpSn—1 Il	Il
loooooooooooooooooooooooooooooooooooooooooooooooooooon
y	-
T2
+ = :	sup >AX diag (vd2 — vd2) XJ (PAo)J >
θ(1 — θ)nP qpSn-1 Il	Il
loooooooooooooooooooooooooooooooooooooooooooooooooooooooon
v^
T3
+ -n 1 Q- sup ∣}v}4 — }vo}4∣.
3θ(1 — θ)np qpsn-ι l__________________4n
V
T4
By using Lemma E.16, we have
Ti ≤ }PAo} }X}2 }PAo — A} sup }vo}8 ≤ }PAo}3 }X}2 ^型学 IXek) }PAo — A},
qpSn — 1	IWkWnp
T2 ≤ }A}}X}2 sup }v}8 ≤ K3/2}X}2^1 max }Xek})2}PA。— A} ∙
qpSn—1	iWkWnp
Similarly, Lemma E.16 implies that
T3 ≤ }PAO} }A}}X}2 sup ›vOd2 — vd2›8
qPSn-1
≤ ?K ´?K +}PAo}) }PAo}}X}2 ^imθχ∕Xek) }PAo — A},
40
Published as a conference paper at ICLR 2020
and
T4 ≤ sup ∣}v}4 — M}4∣ ≤
qpSn—1 I	1
≤
2 sup Kv — vo, 4vd3D∣
qpSn—1
8 sup }v — v0} }v}36
qeSnτ
≤
8K3{2}X}2^ max }Xek}) }PAo — A}.
IWkWnp
Thus, combining all the results above, we obtain
SUp }Hess 3cdl (q) — Hess Pcdl (q)}
qpSn—1
≤ 二 1Q	”(？K +}PA0}) }PAo}2 + K }PAo} ' 4K3{21 }X}2 ^ max }Xek}^2 }PAo — A}.
θ(1 — θqnp	1WkWnp
By Lemma E.17 and Lemma E.20, we have
}X} ≤ ?aθmp,	max }Xek} ≤ 4?θm Iog(Kp)
1WkWnp
with probably at least 1 — 2p-2. On the other hand, by Lemma E.19, there exists some constant
C > 0, for any e P (0,1) whenever
P N Cθ-1K3 K6(A?、e´2 log(mK),
σmin (A0 )
we have
}PAo — A} ≤ e,	}PAo} ≤ 2?K
hold with probability at least 1 — ci(mK)—c2 for some numerical constants ci, c2 > 0. These
together gives
SUP }Hess夕cdl(q) — Hess0cdl(q)} ≤ C 1K5/2θmlog2 (Kp) e.
qpSn—1
Replacing δ “ C1K5{2θm log2 (Kp) e gives the desired result.	■
E.5.3 Auxiliary Results
Lemma E.16 Let v0 and v be defined as in Equation (E.13), with
v0(q) “ XJ (PA0)J q,	v(q) “ XJAJq,
For all q P Sn´1, we have
}v}8
}v}
}v—v0}
≤
≤
≤
≤
λ∕K max }Xek},
iWkWnp
?k }χ},	}v}6
}v0}8 ≤ }PA0} iWmkaWxnp }Xek} ,
≤K3 }X}2 ^1 max }Xek}),
iWkWnp
´?K +}PA0})}PA0 — A} ^1max }Xek}),
}PA0—A}}X}.
Proof In the following, we bound each term, respectively.
41
Published as a conference paper at ICLR 2020
Bounding norms of V and v°. For the '2 -norm, notice that
H ≤ }X}}A} ≤ ?K}X}
On the other hand, for the '8-norm, We have
}v}8 “ max >eJXJAJq> W ?K max }Xek}
IWkWnp	IWkWnp
}vo}8 “ ImaxjkJXJ(PA0qjq>W }PAo} 1wmwnp}χek}.
Thus, the results above give
}v}6 W }v}8}v}2 W K3 }X}2 ^ max }Xek}[
1WkWnp
Bounding the difference between v and v0. First, We bound the difference in '2-norm,
}v ´ vo} “ >XJ (PAo ´ AqJ q> W }PAo ´ A}}X}.
On the other hand, We have
>vd2 ´ Vd2>8 W }v ´ v0}8 }v + v0}8 W (}v}8 + }v0}8)}v ´ v0}8 ,
Where
}v — v0}8
max ››ekJXJ (PA0 ´ AqJ q›› W }PA0 ´ A} max }Xek} ,
1WkWnp	1WkWnp
Thus, We obtain
vd2
´ vd2>8 W ´?K '}PAo}) }PA0 ´ A} ^ιmwnp }Xek ∣j,
as desired.
Lemma E.17 Suppose X satisfies Assumption E.2, we have
max }Xek} W 4?θmIog(Kp)
1WkWnp
with probability at least 1 — p—2,m,
Proof Let us Write
Xi “ [Xi1 Xi2 ∙∙∙	XinS , With Xij
sj´1 rxi1s
.
.
.
sj´1 rXiKs
1 W i W p, 1 W j W n,
where s` [•] denotes circulant shift of length '. Given X “ [Xi … XpS, we have
max }Xek}
1WkWnp
1WiWmp,a1xWjWn}Xrij}
max
1WiWp,1Wj Wn
K
∑ }SjT [Xi'S}2
'=1
W VK	max	}xi'}.
lWiWp,lW'WK
(-4nθ‰)
Next, we bound maxιwiwp,ιw'wκ }xi'}. By using Bernstein inequality in Lemma A.5, we obtain
P Q}xi'}2 ´ nθ∣ > t) W 2exρ
Thus, by using a union bound, we obtain
max }xi'} W 4√θnlog(Kp),
lWiWp,lW'WK
with probability at least 1 — p-2θm. Summarizing the bounds above, we obtain the desired result. ■
42
Published as a conference paper at ICLR 2020
E.6 Intermediate results for preconditioning
Lemma E.18 Suppose X satisfies Assumption E.2. For any δ P p0, 1q, whenever
p
> C0-1K2
κ4PA0q
σ41inpA0q
s´2 log pmq,
we have
θ⅛ YY J)T2TA0AJ)T2
念 YYJy2(A0AJ)T2 ´ I
≤ δ,
& σmm PA°q∙ δ,
hold with probability at least 1 — cι(mKq—c2. Here, ci, c2, C > 0 are some numerical constants.
Proof Notice that
TT^-YY J
θmp
A A0XXJAJ = A0AJ ' A0 ( ~λ--------------
θmp	loomoon θmp
B looooooooooooo
XXJ — I A0J .
"V"
∆
By Lemma E.20, for any P p0, 1{K q, whenever
PeC0-1K2e-2 log(mK),
we have
X--XX J — I
θmp
≤ e,
with probability at least 1 — ciPmK)—c2. Thus, by the first inequality in Lemma A.10 We observe
“ >(B + ∆)T/2 — B―1{2
≤ 4σm2n (B) }∆}
≤
4κ2 pA0)
σminpA0q θmP
ɪXXJ — I
4κ2(A0)
σm□Aτ e.
≤
On the other hand, by using the second inequality in Lemma A.10, we have
θmp YYJ) 1/2'A0AJ)T2
—I
“ ››pB ` ∆)1{2 Bm1{2 — I››
≤ 4σmmi3n{2 pB) }∆}
≤ 4κ2pA0q ɪXXJ —
σmin pA0) › θmp
4κ2pA0)
----7~:~T ∙ e.
σmin pA0)
Choose e “
4κ2(Ao)
σ21in(A0)
)m1 δ,
we obtain the desired results.
≤
(
Given the definition of preconditioning matrix P, the result above leads to the following corollary.
Corollary E.19 Under the same settings of Lemma E.18, for any δ P p0, 1), whenever
P N Cθm1K3 TAP、δm2 log(mK),
σmin pA0)
we have
}PA0 — A} ≤ δ,	>Pm1> ≤ 2Km1{2 }A0},
}PA0} ≤ }Α} ` δ ≤ ?K + δ
hold with probability at least 1 — cι(mK)mc2. Here, ci, c2, C > 0 are some numerical constants.
43
Published as a conference paper at ICLR 2020
Proof For the first inequality, we have
}PAo ´ A} W ?K(θmmpYYJ) i/2 — (AoAj)T{2 }A0}.
Thus, for any δ P p0, 1q, Lemma E.18 implies that whenever
P > CθTK34⅜⅛δ∙ log(mKq,
σminpA0q
we have
}PAo ´ A} ≤ δ,	}PAo} ≤ }A} + }PAo ´ A} W ?K + δ
with probability at least 1 — cιPmKq´c2. On the other hand, by Lemma E.18 We have
>pT> W >PT _ (KTAOAO严> + >'κtaoao)*
W >'KTAoAo)1{2> ´l + >PT(KTAoAo)T/2 — I>)
W Kτ{2}Ao}(1 + >^θmpYYJy2 (AOAJ)T2 — I) W 2Kτ{2}Ao},
as desired.
Lemma E.20 Suppose X satisfies Assumption E.2. For anyδ P p0, 1q, we have
θmmpXXJT W δ,
with probability at least 1 — cimK exp (—c2θp min {(K )2 ,叠}). Here, ci, c2 > 0 are some
numerical constants.
Proof By using the fact that X = [Xi X2 …
Xps, we observe
» Cx fi
xk1
Xk “
CxkK
For any Z P Sn´1, write Z “
ɪXXJ — I
θmp
sup
zpSn—1
sup
zpSn—1
p
XXJ “	XkXkJ ,
k“1
J
J—I
Z—}Z}2
)
)
z
sup
zpSn—1
sup
zpSn—1
θmPς (SCxikzk)仁ICxikZk)-}Z:2
1pK
θm∑ Σ I Σ Zk CXikCxik zk+ 2X zk CXik Cx
P i“1 ∖k = i	k“'
W
W
sup
zpSn—1
K
∑ ZJ
k“1
K›
θmp
p
CxJikCxik
i“1
— I Zk + 2
ZkJ
k“'
Σ θnp i“i CJikCx
k“1
ik — I
+ 2K T
Σ 焉 Σ CJikCx
k“'
i“1
i`
K T
p
1
p
44
Published as a conference paper at ICLR 2020
By Lemma E.21, we obtain
ɪ XXT — I ≤ t1 + 2Kt2 ≤ δ
θmp	›
With probability at least
1 — 2mexp '—ciθpmin { δ2, δ}) — 2mKexp (—c2θpmin {(Kl1δ)2 , K-1δ}).
Finally, the second inequality directly follows from the fact that
as desired.
X--XXJ ― I ≤ δ
θmp	›
-}X}2 ≤ (θmp)(1 ' δ),
Lemma E.21 Suppose xij satisfies Assumption E.2. For any j P rKs, we have
1p
C- Cx CxijCxij 一 I ≤ ti
θnp xij ij ›
holding With probability at least 1 — 2m exp (—θp min { t1 ,tι })∙ Moreover, for any k,' P [KS
with k -“ `, we have
1p
而 Σ CxikCxi'
i“1
≤ t2
holding with probability at least 1 — 2m2 exp (—θ2p min "2，/2().
Proof Notice that
CJijCxij = F * diag (∣Fxj∣d2) F,	CJik。. “ F * diag 'Fxk d FxQ F. (E.14)
Bounding > 就 ∑p=1 CxijCx” 一 I>. From Equation (E.14), We have
1p
C- CX Cxij Cxij — I
θnp xij ij
F* diag (θnp W IFxjId2 — 1) F
1p
会与 ∣Fχj∣d2 ´1
i“1
Let fk be a row of F, by Lemma A.3 we have for any '21,
E”|f*xj |2'1 ≤ ^^2^ Ebk~Ber(θ) ”}bk d fk}2'l
Thus, by Bernstein inequality in Lemma A.5, we have
`!
≤ ɪθ(2n)'.
1p
丽 ∑ lf*xjl
i“1
d2
´1
> tι) ≤ 2exp (一
pθt21
8'4tι
P
≤
8
.
Thus, by using union bounds, we obtain
›› 1 p
——CJc J Cx . — I
θnp — xij xij
1p
焉 >Fxjid2-1
i“1
≤ t1
8
≤
for all 1 ≤ j ≤ K with probability at least 1 — 2nK exp (— θp min { t1,tι}) ∙
45
Published as a conference paper at ICLR 2020
Bounding >> 就 XP=ι CJikCxi' ∣.On the other hand, by Equation (E.14), we know that
1p
θnp ∑ CxikCxi`
1 P _______
而 i“i Fxik d Fxi'
≤
8
Let zkd' “ fdxikfdxi' “ xJk fdfdxi' (1 ≤ d ≤ n), We have its moments for S21
E IIzkdT1 ≤ E [∣xik fd∖s] E [lf>i'∣s‰ ≤ 2Ebd~Ber(θ) ”}bd d fd}2sl ≤ 2θns.
Thus, by Bernstein inequality in Lemma A.5, we obtain
P
ʌ V
θnp
i“1
k`
zid
> t2) ≤ 2 exp (一
θpt22
2'2t2
1
p
.
Thus, by applying union bounds, we have
p
θnp i“1 CJikCx
i`
≤ t2
for all 1 ≤ k,' ≤ K and k “ ' with probability at least 1 — 2mK exp (一 θp min "2,t2}).
F Measure Concentration
In this part of the appendix, we show measure concentration of Riemannian gradient and Hessian
for both 夕DL (q) and 夕CDL (q) over the sphere. Before that, we first show the following preliminary
results that are key for our proof. For simplicity, we also use K “ m{n throughout the section.
F.1 Preliminary Results
Here, as the gradient and Hessian of `4 -loss is heavy-tailed, traditional concentration tools do not
directly apply to our cases. Therefore, we first develop some general tools for concentrations of
superema of heavy-tailed empirical process over the sphere. In later part of this appendix, we will
apply these results for concentration of Riemannian gradient and Hessian for both overcomplete
dictionary learning and convolutional dictionary learning.
Theorem F.1 (Concentration of heavy-tailed random matrices over the sphere) Let
Z1, Z2, ∙∙∙ , Zp	P	Rn1xn2 be i.i.d. centered Subgaussian random matrices, with
Zi ”d Z p1 ≤ i ≤ pq and
E rZij s “ 0,
P p∣ Zij | > tq ≤ 2exp
(´2σ2)
For a fixed q P Sn´1 ,let us define a function fq (∙) : Rn 1 xn2 →Rd1 χd2, such that
1.	fq (Z) is a heavy tailed process of Z, in the sense of P (}fq (Z)}2t) ≤ 2exp (—Cγzt).
2.	The expectation E rfq pZ qs is bounded and Lf -Lipschitz, i.e.,
}Erfq(Zqs}	≤	Bf,	and	}Erfq1(Zqs—Erfq2(Zqs} ≤ Lf}q1—q2}, @q1,	q2	P	Sn
(F.1)
3.	Let Z be a truncated random matrix of Z, such that
Z _ Z ,	Z	7—"Zij	if IZijl< B,
Z “ Z `	Z,	Zij “	(F.2)
0 otherwise.
with B “ 2σʌ/log (n1n2p). For the truncated matrix Z, wefurther assume that
>>fq(列 ≤ Rι(σ),	max {∣∣E [fq(Z)τfq(Z)‰>> , f (Z)fq(Z)J>>( ≤ R2(σ),	(F.3)
>>fqι (Z) — fq2 (Z)>> ≤ Lf (σ) }q1 — q2} , @ qι, q2 P SnT.	(F.4)
1
46
Published as a conference paper at ICLR 2020
Thenfor any δ P(0, 6R2), whenever
P > C max # min td1 ,d2u Bf , S-2R2 «n log ( 6 (Lf『)' log(dι ' d?)]]
n1n2δ	δ
we have
1 &	—
SUp -Xfq(ziq´Erfq(Zqs ≤ δ,
qPSnT P i“1
holding with probability at least 1 — (ninzp)´2 — n~c log(pLf 'Lf q{δq for some constant c, C > 0.
Proof As aforementioned, traditional concentration tools does not directly apply due to the heavy-
tailed behavior of fq(Zq. To circumvent the difficulties, we first truncate Z and introduce bounded
random variable Z as in Equation (F.2), with truncation level B “ 2σʌ/log (n1n2p). Thus, We have
P sUp
qPSn—1
≤ P(SUp
qPSn—1
1p
；Σ fq O ：
P i“1
1 P
-Σ fq (Z，)-
› P i“1
V
P1ptq
Erfq(zqs > t)
Erfq(zqs > t)
ooooooooooooooooooooooooon
` P (m晦区}8》B).
loooooooooooooooooooooooooooon
P2
As fq(Zq is also bounded, then we can apply classical concentration tools to Pi (t), and bound P by
using subgaussian tails of Z. In the following, we make this argument rigorous with more technical
details.
Tail bound for P2. Since Zjik is centered subgaussian, by an union bound, we have
P2 “ P (max }Zi∣∣8 N B) ≤ nin2pP (IZjkl N B) ≤ exp (-
Choose B “ 2σʌ/log (n1n2p), we obtain
P2 “ P (max ∣∣Zi∣∣8 N B) ≤ (ninzp)´ .
∖1≤i≤p
B2
2σ2
' log (n1n2Pq .
Tail Bound for > P XP=Ifq(Zi) - E [fq(Z)] > with a fixed q P Sn-1. First, we control the quan-
tity for a given q P Sn_1. Later, we will turn the tail bound result to a uniform bound over the sphere
for all q P Sn_1. We first apply triangle inequality, where we have
1昌 —
-Xfq (Zi)— E rfq (ZqS ≤
› P i“1	›
such that
1 P _	_
-Σ fq (Zi) - E [fq (Z)‰
P i“1
' >E rfq (Z qs— Efq (Z)] >,
P O P
& P O P
P _
Xfq(Ziq- Erfq(Z qs
i“1
P _	_
Efq (Z i) - E [fq1 (Z q‰
i“1
> t)
> t — >e rfq(Zqs — E “fq(Zq‰).
Notice that
>E “fq(Zq‰ - E rfq(Zqs> ≤ >E [fq(Z) d 1z‰z‰>F ≤ IIE rfq(Z川F >E “1 Z‰Z]>尸
≤ min 演㈤ Bf 言 P (Zij “ Zij)
≤ min {d1,d2} Bf {n1n2 exp (-B2 ),
47
Published as a conference paper at ICLR 2020
where for the second inequality we used Cauchy-Schwarz inequality, the third one follows from
and the last one follows from the fact in Z is SUbgaUssian. With B “ 2σʌ/log PnIn2p), We obtain
|E “fq(Z)]´ ErfqPZ)S∣ W min n%uBf,
so that
P(I P A fqPZi)´ EfqPZ)S > t
)& P (|P W fq(Zi)´ E“fq(Z)‰	> t ´
Next, We need to show concentration of ∣ P 2f“i fqPZi) — E [fqPZ)‰∣ to finish this part of proof.
By oUr assUmption in EqUation (F.3), we apply boUnded Bernstein’s ineqUality in Lemma A.7, sUch
that
P (|P E fqPZi)´ e [fqPZ)‰
》t1) W Pd1 + d2q exp (´ 2R2 '4Rιt2/3).
Choose P large enoUgh sUch that
2 min td1 ,d2 u Bf
P X	nint
Thus, for a fixed q P Sn´1, we have
P (|P E fq(Zi)´ Erfq(Z)S > t
min td1 ,d2 u Bf	t
W
n1 n2P	2
W P (| P A fq (Zi) — E [fq (Z)‰ > t/2
W (d1 + d2 * q exp (´ 8R2 ；：Rit/3).
Bounding Pi (t) Via covering over the sphere Sn´1. Finally, we finish by .
epsilon net of the sphere, where we know that
Let N P) be an
@ q P Snf	D q1 P N(e),	s.t. |q — q11 W e,
and #N P) W
G )n´1
ThUs, we have
sup
qpSn—1
1 P
P Σ fq(Zi)´
i“1
ErfqPZ)s
sup
1 P	__
qipN - ∑1 fq1'e(Zi)—Erfq*qs
W sup
q1PN()
+
1自一、
- X fq1(Zi)— E rfq1 (Z)S	+
P i“1	|
sup
q1pN (e),}e}We
sup	}E rfqi'e(Z )S — E [fq，(Z)S}.
q1PN (e),}e}We
1 Λ 一
- X fq1'e (Zi)-
Pi“1
1 Λ —
X Σ fqi (Zi)
P i“1
By our Lipschitz continuity assumption in Equation (F.1) and Equation (F.4), for any q P sn´1, we
obtain
}E [fqi'e(Z)S — E [fqi(Z)S} W Lf}e},
1 P	_	1 P _
-Σfqi'e(Zi) — -∑ fqi (Zi)
P i“1	P i“1
W |fqi'e(Z)— fqi (Z) | W Lf }e},
which implies that
sup
qPSn—1
1昌 —
-Xfq (Zi)— E [fq (Z)S	W
P i“1	|
sup
qiPN ()
1 P _
-E fqi (Zi)— E [fqi (Z )S
P i“1
+ (Lf + Lf) e∙
48
Published as a conference paper at ICLR 2020
Therefore, for any t > 0, choose
t
2pLf ' Lf q
so that we obtain
P sup
\qPSn—1
1 Λ	一一
- Efq (Z i)´ Erfq (Z qS
p i“1
≤ P sup
q1PN pq
1 P __
-Xfqi (Zi)— E rfqi (Z)S
pi“1
≤ P sup
q1PNpq
1 P _
-∑ fqi (Zi)´ E rfqi (Z )S
p i“1
≤ #N(e)- P ( P W fq(Zi) ´ Erfq(Z)S > t/2
≤ ^3)	(di + d2q exp (´ 32R ；：6R j/3 )
E	32R2 ` 16R1 t{3
≤ exp
(´ min "矗,箸 * + n 凄 (ffi)
+ log(d1 + d2)
Summary of the results.
whenever
Therefore, combining all the results above, for any δ P
(0,6 R2),
e ≤
> t)
》t ´ (Lf + Lf) E
> t{2
)
)
P > C max∣min⅛d⅛, S—2R2 卜 log (6fL!) ` log@ + d?)]1,
we have
1P
SUp	-X fq (Zi) ´ Erfq(Z)S
qPSnT P i“1
≤ δ,
holding with probability at least 1 — (ninzp)´2 — n~clog(pLf'Lf "δ) for some constant c, C > 0. ■
Corollary F.2 (Concentration of heavy-tailed random vectors over the sphere) Let
zι, z2, ∙∙∙ , ZP P Rn1 be i.i.d. centered Subgaussian random matrices, with Zi ”d Z (1 ≤ i ≤ P)
and
E rziS “ 0,
P (|zi| > t) ≤ 2exp
(´2σ2)
For a fixed q P sn´1, let us define afunction fq (∙) : Rn1 → Rd1, such that
1.	fq (z) is a heavy tailed process of Z, in the sense of P (}fq (z)}2t) ≤ 2exp (—C√t).
2.	The expectation E rfq (Z )S is bounded and Lf -Lipschitz, i.e.,
}E[fq(z)S} ≤	Bf,	and	}E	[fqi	(z)S	— E	rfq2	(z)S} ≤ Lf }qι — q?}	, @ qι,	q2	P	sn´1.
(F.5)
3.	Let z be a truncated random matrix of Z, such that
z = Z + P,	Zi
Zi if ∣Zi∣ < B,
0	otherwise.
with B = 2σʌ/log (nip). For the truncated matrix Z, Wefurtherassume that
}fq(Z)} ≤ Ri。)，E [fq(Z)}21 ≤ R2(σ),
}fqι (Z) — fq2 (Z)}	≤	Lf (σ)	}qi —	q2}	, @	qi,	q?	P	sn´1.
(F.6)
(F.7)
(F.8)
49
Published as a conference paper at ICLR 2020
Thenfor any δ P(0, 6R2)
, whenever
PeC max
δ-R2
n log
(6 (Lf + Lf)
+ logpd1 q
we have
)
,
sup
qPSn—1
1p
一 Xfq(Zi)— E [fq(z)S
pi“1
≤ δ,
holding with probability at least 1 — (n1p)-2 — n~c IOg(PLf'Lf q{δq for Some constant c, C > 0.
Proof The proof is analogous to that of Theorem F.1. The slight difference is that we need to apply
vector version Bernstein’s inequality in Lemma A.8 instead of matrix version in Lemma A.7, by
utilizing our assumption in Equation (F.7). We omit the detailed proof here.	■
F.2 Concentration for Overcomplete Dictionary Learning
In this part of appendix, We assume that the dictionary A is tight frame with '2-norm bounded
columns
ɪAAJ “ I, }ai} ≤ M (1 ≤ i ≤ m).	(F.9)
K
for some M with 1 ≤ M ≤ ?K.
F.2.1 Concentration of grad夕dl(∙)
First, we show concentration of grad 夕DL (q) to its expectation E [grad 夕DL (q)S “ grad 夕T (q),
1p3
gradQL(q) = ´3θ(1 ´ θ) PqK ∑ (qJAχk) (Axkq
k“1
-*	grad ^τ(q)= ´PqK A (AJq)d3 ,
where xk follows i.i.d. BG(θ) distribution in Assumption 2.2. Concretely, we have the following
result.
Proposition F.3 (Concentration of grad 夕dl(∙)) Suppose A satisfies Equation (F.9) and X P
Rmxp follows BG(θ) with θ P (m,1). For any given δ P (0, cK2{(m log2 P log2 np)), when-
ever
peCδ12θK 5n2 log (tn)，
we have
SuP }grad夕dl(q) — grad夕τ(q)} < δ
qeSnτ
holds with probability at least 1 — e'p´2. Here, c,c1,C > 0 are some numerical constants.
Proof Since we have
1p3
grad3DL(q) = ´3θ(1 ´ θqpPqK ∑ (qJAxk) (Axkq,
we invoke Corollary F.2 to show this result by letting
fq(x) = — 3θ(1) θ) (qJAx)3 PqK Ax P Rn,	(F.10)
where x „ BG(θq and we need to check the conditions in Equation (F.5), Equation (F.7), and
Equation (F.8).
50
Published as a conference paper at ICLR 2020
Calculating subgaussian parameter σ2 for x and truncation. Since each entry of x follows
xi „i.i.d. BGpθq, its tail behavior is very similar and can be upper bounded by the tail of Gaussian,
i.e.,
P P∣Xi∣ > tq ≤ exp(—12∕2),
so that We choose the truncation level B “ 2∕log PnP.
Calculating Ri and R2 in Equation (F.7). First, for each i (1 ≤ i ≤ p),we have
}fq (xi)} “ 3θ(1) θq >(qJAxi)3 PqK Axi>
V	}AXi}4	V
、3θ(1 — θ) V
}A}4}Xi}4 < K2 }Xi}4
3θ(1 ´ θ) 、3θ(1 ´ θ).
By Lemma A.9 and a union bound, we know that for any 1 V i V p,
}xi}o V 4θmlogp,	}Xi}o V 4θmlogP	=^	}Xi}2 V B2 闻}° “ 4B2θmlogP
(F.11)
with probability at least 1 — pI2θm. Thus, by our truncation level, we have w.h.p.
6θ
}fq(xi)} V -.-k2b2B4m2 log2p “ Ri.
(1 — θq
On the other hand, by Lemma F.5, for the second moment we have
E ”}fq(Xi)}2] V E ”Ifq(Xi)}2] V cθK4m
for some constant C > 0. Thus, we obtain
6θ
Ri =	K2B4m2 log p,	R2 “ cθK4m.	(F.12)
(1 — θ)
Calculating Lf in Equation (F.8). Notice that for any qi, q2 P sn´1, let ζi “ AJqi (i “ 1, 2),
by Lemma F.4 we have
}fqi (X) — fq2 (X)} “
V
V
V
V
湎L > (Jx)3 PqKAx - (ZJx)3 PqK AX>
信Xy >(ZJX)3 PqK - (ZJx)3 PqK >
≡ IKJxwqK—PqK> + I(ZJx)3TZJxHI
IA”2 }A13 }x13 }qi — q2}+ 3 }A13 }x13 }qi — q21l
3θ(1 — θ)
2 }A14 }x}4 l.	.l
θ(i — θ)	}qi —q2}.
where for the last two inequalities we used Lemma A.11 and
∣(ZJx)3 — (ζjx)3∣ “ ∣(Zι — Z2)τx∣∣(ζJx)2 + (ZJx)(ZJx) + (ζJx)21
V }A}}x}}qι — q2} ”(ZJx)2 + (ZJx)2 + ∣ZJx∣ ∣Zjx∣1
V 3 }A13 }x13 }qi — q2}.
Furthermore, by Equation (F.11) we obtain
}fqι(x)	— fq2(x)}	V	2 }A}	}x}	}qi	—	q2}	V	32θτK2B4m2 log2p	}qi	—	q2}.
θ(1 — θ)	1 — θ
This gives
-	32θ
Lf = -~~-K2B4m2 log2 p.
1 — θ
(F.13)
51
Published as a conference paper at ICLR 2020
Calculating Bf and Lf in Equation (F.5). From Lemma F.4 we know that E rfqpxqs “
PqK Aζd3, so that
}E rfq (x)]} “ >PqK A (AJq)d3> ≤ >PqK>}A}>AJq>6
≤ }A} >AJq>3 ≤ }A}4 “ K2 = Bf,	(F.14)
where we used Lemma A.1 for the second inequality. Moreover, we have
}E rfq1 (xqs ´ E rfq2 (xqs}
≤ >PqK AZd3 ´ PqK AZd3> + >Pq"Zd3- PqK AZd3 ||
≤ }A} >Zd3 ´ Zd3> + >PqK ´ PqK > }A} >Zd3 >
≤ }A}>(Zi — Z2)d (ζd2 + Zi d Z2 + ζd2) > + 2 }A}}Z2 }3 }qι ´ q2}
≤ 5 }A}4 }qi — q2} = 5K2 }qi ´ q2} = Lf }qi — q2} .	(F.15)
where for the last inequality, we used the fact that
>pζi - ζ2q d 'ζd2 + ζi d ζ2 + ζd2)> ≤ }ζi ´ ζ2}4 >ζd2 + ζi d ζ2 + ζd2>4
≤ >AJ(qi-q2)>(>Zd2>+}Zi dZ2} + >Zd2>)
≤ 3 }A}3 }qi ´ q2}.
Thus, from Equation (F.14) and Equation (F.15), we obtain
Bf = K2 ,	Lf = 5K2 .	(F.16)
Final calculation. Finally, we are now ready to put all the estimations in Equations
and (F.16) together and apply Corollary F.2 to obtain our result. For any δ P
(0,6 Ri)
(F.12), (F.13)
, whenever
P > Cδ∙θK5n2 log (θKn∕δ),
we have
sup
qPSn—1
1p
-Efq O E rfq (Z)S
p i“1
≤ δ,
holding with probability at least 1 — (np)´2 — n-c1 logpθκn{δq — p´2θm for Some constant ci,C > 0.
Lemma F.4 (Expectation of grad 夕DL (∙)) Vq P SnT, the expectation of grad 夕dl (∙) satisfies
grad 夕 dl(q) = grad 夕 τ(q)= —PqK A (AJ q)d3
Proof Direct calculation.
Lemma F.5 Suppose x „ BG (θq and let fq (xq be defined as Equation (F.10), then we have
E ”}fq(x)}2] ≤ CθK4m (K = m∕nq.
Proof Since x „ BG(θq, we write x = b d g with „ Ber(θq and g „ N (0, Iq. Let I be the
nonzero support of X with I = SuPP x. And let PI(∙) be an operator that restricts a vector to the
support I, so that we can write x = PI (gq. Notice that
m
E }fq(x)}2 = E ∑ [fd2(xq‰k
k“1
& m max E[fd2(x)‰k.
kPrms
52
Published as a conference paper at ICLR 2020
Let W “ PqKA with wk being the kth row of W. For @k P rns,
[Efd2pxq‰k = 9θ2(1l θq2 E (qJAx)6(W wk,ixi)
1	1
W9θ2p1 ´ θq2 (E@AJq，xD12) 2 (EXwk, xy4)2
1	1
“9θ2(一θ)2 (E@PI (AJq),g〉T (EXPI Pwkq,g〉4)2.
Notice that
@PI (AJq), vD „Np0,››PI(AJq)››2q and XPIpwkq,v〉 „Np0,}PIpwkq}2q,
hence
(E @PI (AJq) , vD12)1 “ ？1W (EI >PI (AJq) >12)1 .
Let AJq “ ζ, then we have
EI >PI (AJq) >12 “	∑	mk1 Ik1PI Zk lk2PI Zk lk3pI Z2, RpI 受 5 ‰∣ζk Z2,lk6PI,旧7)
k1,k2,...,k6
for bounding equation F.17, we discuss the following cases:
•	When only one index among k1 , k2 , . . . , k6 is in I:
EI >PI (AJq) >12“ θ ∑ Zk2 ≤ θk6
k1
•	When only two indices among k1 , k2 , . . . , k6 are in I:
EI >PI (AJq) >12 = θ2 ∑ (Z21 Z10 + Z41Z82 ` Z61 Z62) & 3θ2κ6
k1,k2
•	When only three indices among k1 , k2 , . . . , k6 are in I:
EI >PI	(AJq) >12 “	θ3	∑	(Z21 Z22Z83	+	Z21Z4Z63	+	Z41Z4Z4) &	3θ3κ6
k1,k2,k3
•	When only four indices among k1, k2 , . . . , k6 are in I:
EI >PI (AJq) >12 “ θ4	∑	(Z21 Z22Z23Z64 + Z21Z22Z4,Z4,) & 2θ4κ6
k1,k2,k3,k4
•	When only five indices among k1 , k2 , . . . , k6 are in I:
EI >PI (AJq)>12 “ θ5	∑	(Z21 Z22 Z23 Z24 Z4s) ≤ Θ5K6
k1,k2,k3,k4,k5
•	When all six indices of k1 , k2 , . . . , k6 are in I:
EI >PI (AJq) >12 “ θ6	∑	(Z21 Z22 Z23 Z24 Z25 Z26) ≤ θ6K6.
k1,k2,k3,k4,k5,k6
Hence, we have
EI›PI (AJq)›12 “ θK6 + 3θ2K6 + 3θ3K6 + 2θ4K6 + θ5K6 + θ6K6 W C1θK6
for a constant C1 > 11. Similarly, We have
(E XPI (wk), v〉4) 1 “？ (EI }PI (wk)}4 )2,
53
Published as a conference paper at ICLR 2020
and
2
EI }PI pwkq}4 “ Σ w2,kι IkIPIw2,k2 1k2Pl ≤ C2θn^,
k1,k2
for a constant C2 > 2. Hence, We have
(E@PI (AJq),g〉12)1 (EXPIWk,g〉4)2 ≤ C3θm,
for a constant C3 > 829. Hence, we know that @k P [n],
[Efd2(x)‰k & θ(i⅜m “ CθK4,
for a constant C4 > 93. Therefore
E }fq(x)}2 ≤ CθK4m,
for a constant C > S2(9、称.
F.2.2 Concentration of Hess 夕dl(∙)
Proposition F.6 (Concentration of Hess 夕dl(∙)) Suppose A satisfies Equation (F.9) and X P
Rmxp follows BG(θ) with θ P (m,1). For any given δ P (0, CK2{(log2 P log2 np)), whenever
PeCδ-2θK6n3 log (θKn∕δ),
we have
sup }Hess夕DL(q) — Hess夕τ(q)} < δ
qpSn—1
holds with probability at least 1 — e'p´2. Here, c,c1,C > 0 are some numerical constants.
Proof Since we have
1p	2	4
Hess ^DL(q)	= — 3θ(1	―	θqp	∑	PqK	[3 (qJAxk)	Axk(Axk)T —	(qjAxk)	l]	PqK,
we invoke Theorem F.1 to show our result by letting
fq(x) = — QZn PqK ”3 (qJ Ax)2 Ax (Ax)J ´ (qJAx)4 八 PqK P Rn",	(F.18)
3θ(1 — θ)
where x „ BG(θ) and we need to check the conditions in Equation (F.1), Equation (F.3), and
Equation (F.4).
Calculating subgaussian parameter σ2 for x and truncation. Since each entry of x follows
xi „i.i.d. BG(θ), its tail behavior is very similar and can be upper bounded by the tail of Gaussian,
i.e.,
P (∣Xi∣ > t) ≤ exp (—12∕2),
so that we choose the truncation level B = 2∕log PnP). By Lemma A.9 and a union bound, we
know that for any 1 ≤ i ≤ p,
}xi}o ≤ 4θmlogp,	}xi}o ≤ 4θmlogP	=^ 闻}2 ≤ B2 闻}。= 4B2θmlogP
(F.19)
with probability at least 1 —夕一2"。
54
Published as a conference paper at ICLR 2020
Calculating Ri and R2 in Equation (F.3). For simplicity, let ξ “ Ax. First of all, We have
}fq(x)} “ 3θp1k^BqK ∣3(qJξ)2ξξJ´ 'qJξ)411 PqK>
& J 'qJξ)2HξJ(qJξ)2 I>
& J >ξ>4 < J }A}4 }x}4 & TOK2m2log2P.
On the other hand, by Lemma F.7, We have
>E “fq(XqfqpxqJ‰> = >E “fq(X)Jfq(x)] || ≤ IIE[fq(X)Jfq(x)] || ≤ c1θK4m2,
for some numerical constant ci > 0. In summary, We obtain
64B4
Ri =	θK2m2 log p,	R2 = c1K4θm2.	(F.20)
3(1 ´θ)
Calculating Lf in Equation (F.4). For any qi, q2 P sn´1, we have
}fqi (X)- fq2 (X)}
=3θ(1l-l) BqKl3 (qjξ)2 ξξJ - 'qjξ)4 11 PqK - PqK I3 (qJξ)2 ξξJ - 'qJξ)4 11 PqK >
W θ∏% >PqK 'qjξ)2 ξξJPqK-PqK ④守 ξξJPqK > + lθ^ >'qjξ)4 PqK- (qJξ)4 PqK>,
V^	、	-V"	，
T1	T2
Where by Lemma A.11, We have
Ti & >PqK 'qjξ)2 ξξJPqK- PqK (qjξ)2	+ EKS^)2 ξξjPqK- PqK Wξ) 2 .Pq/
≤ 同。PqK- PqK > + llPqK'q^)2 ξ^- PqK W^) 2	+ >PqK 'qj ξ^- PqK Wjξ) 2
W 即 IIPqK-PqK > + >ξ>2'qjξ + qJξ)(qJξ-qJξ)
≤	4>ξ>4}qι-	q2}	W 4 }A}4	}X}4	}qi	- q2}	W	64K2B4θ2m2 log2P }qi -	q2},
and
T W >'qjξ)4 PqK-(qJξ)4 PqKII + ∣∣(qJξ)4 PqK - 'qJξ)4 PqK >
W	((qJξ)	+	(qJξ)	)	(qi	+ q2)J	ξξ	(qi	- q2) + 2 >ξ>	}qi	-	q2}
W 6 >ξ>4 }qi - q2} W 6 }A}4 }X}4 }qi - q2} W 96K2B4θ2m2 log2P }qi - q2},
Where for the last inequality We used Equation (F.19). Therefore, We have
96θ
}fqι(Xq- fq2(x)} W KBm log2P }qi- q2},
1-θ
so that
一	96θ	2 4 2	2
Lf =  ----K2B4m2 log p
1-θ
(F.21)
Calculating Bf and Lf in Equation (F.1). We have
}Efq (X)S} “ IIPqK [3A diag'Zd2) AJ -}Z}4I I PqK >
W 卜Adiag'ζd2) AJ -}Z}4III
W 3 }A}2}A}2_2 + }A}4 W K '3M2 + K),
55
Published as a conference paper at ICLR 2020
where }A}'i一引 “ maxι≤k≤m }ak} ≤ M. On the other hand, for any qι, q2 P sn´1, Wehave
}E rfq1 pxqs ´ E rfq2 pxqs}
“ >PqK ”3Adiag (ζd2) AJ ´ }Zι}41] PqK ´ Pqκ ”3Adiag 怎2) AJ -Q}： I] PqK>
≤ 3>PqK A diag(Zd2) AJPqK ´ PqK A diag 'ζd2) AJPqJ + >}Zl}4 PqK TZ2}4 PqK > .
loooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooon	loooooooooooooooooooooooooooon
V	v^
L1	L2
By direct calculation, we have
Li ≤ >PqK A diag 'ζd2) AJPqK ´ PqK A diag 'ζd2) AJPqK >
≤	>PqK Adiag 'Zd2) AJ	´PqK	´ PqK) > +	] IPqK Adiag	'ζd2)	´	PqK Adiag	'ζd2)1	AJPqK	>
≤ }A}2 }Z1}8>PqK ´ PqK > + }A} (>(PqK_ PqK) A diag'Z?2)] +舄 A diag 'Zd2 ´ Zd2) >)
≤ 2 }A}2 }Z1}8 }qi ´ q2} + 2 }A}2 }Z1}8 }qi ´ q2} + }A}2 }Q + ¢2/¢-¢2葭
≤ 6}A}2 }A}2j'2 }q1 ´ q2} ≤ 6KM2 }qi — q2} ,
and
L ≤	}Z1I4 >PqK	´ PqK > + ∣}Zi}4 TQ }4l>PqK >
≤	2	}A14	}qi	´ q2}	+	∣}Z1}4TZ2}4∣(}Z114 +	}Z2}4q	(}Zi12	+	}Z214)
≤	2	}A}4	}q1	´ q2}	+	IKi ´ ζ2} (}Z1} + }ζ2}q	(}Q}2	+	}ζ212)
≤	6	}A}4	}qi	´ q2}	“	6K2 }qi ´ q2}.
These together give us
}E rfqι (x)s— Efq2 (x)]} ≤ 6K 'K + M2) }qi ´ q2}.
Summarizing everything together, we have
Bf “ K '3M2 + K) , Lf “ 6K 'K+ M2) .	(F.22)
Final calculation. Finally, we are now ready to put all the estimations in Equations (F.20) to (F.22)
together and apply Theorem F.1 to obtain our result. For any δ P
(0, 6 R2),whenever
PeCδ∙θK6n3 log (θKn∕δ),
we have
sup
qpSn—1
1p
一 Xfq (Zi)— E rfq (N)]
p i“i
≤ δ,
holding with probability at least 1 — (np)´2 — n-c1 logpθκn{δq — p´2θm for some constant ci,C > 0.
Lemma F.7 Suppose θ P (A, ɪ). Let fq(x) be defined as in Equation (F.18). We have
>E“fq(x)Jfq(x)‰> ≤ CK4θm2
for some numerical Constant C > 0.
Proof Let x “ b d g with b „ Ber(θ) and g „ N (0, I). First, let ξ “ Ax, we have
>E “fq (x)Jfq (x)‰> “ >E ”9 'qJξ)4 PqK ξξJ PqK ξξJPqK — 6 'qJξ)6 PqK ξξPqK + 'qJξ)8 PqK ||
≤ 9>E ∣'qJξ)4 PqK ξξJPqK ξξJPqK]> +6>PqK E ['qJ ξ) 6 ξξJ] PqK > + E ∣'qJξ)8].
loooooooooooooooooooooooooooooooooooooooooon	loooooooooooooooooooooooooooooooon	loooooooooon
V-	V	V
T1	T2	T3
56
Published as a conference paper at ICLR 2020
Bound
T “ >E ∣(qJξ)4 PqK ξξJPqK ξξjPqκ]> ^E ['qjξ)4 ξξJPqK EEJ]]
“ >E ['qJξ)4 >PqKξ>2 ξξJl > ≤ E ['qJξ)4 }ξ}41 ≤ { E(qJξ)8(1{2 {e }ξ}8广2
“ !E [@PI (AJq), gD81)1{2 "´ mn )4 E[(χJx)4 * *‰*	,
where
{e [@PiAJq, gD81) 1 =M (Ei>PiAJq>8)1 ≤ C1Θ ´m)2	(F.23)
the proof of the last inequality is omitted, more details can be found in Lemma F.5, and
E ['xJx)4] = E [XPix, Pix〉4] = E [@Pi(1m),gd2〉]
≤ cimθ ' c2m2θ2 ' c3m3θ3 ' c4m4θ4.	(F.24)
combine, equation F.23 and equation F.24, yield
Ti ≤ Ciθ3m2 ´mn)4 .
T2 = >PqK E ['qjξ)6 ξξJl PqKHE ['qJξ)6 ξξJl > = E ['qJξ)6 }ξ}2]w {E(qJξ)12}"2 {e }ξ}4)“2
i{2
≤ {e@AJq,x〉12) / {e}ax}4} / = {e@Pi(AJq),g〉12}，{(：) E(XJx)21
≤ C2Eι
[>Pl(AJq)>121 /	´m) '3mθ + m(m ´
1{2 ≤ c2E mn)4.
the proof of the first inequality in the last line is omitted, more details can be found in Lemma F.5.
T = E [@Pl (AJq) , g〉81 ≤ C3E1 [>Pl 'AJq)>81 ≤ C3θ }A}8 ≤ C3θ ´mm)4 .
Hence, summarizing all the results above, we obtain
>E fq(X)Jfq(x)]> ≤ Cθm2 ´mm)
as desired.	I
Lemma F.8 (Expectation of Hess 夕dl (∙)) @q P SnT,the expectation of Hess 夕dl (∙) satisfies
Hess ^dl(q) = HeSS ^τ(q) = ´PqK ∣3Adiag ((AqJ)d2) AJ — >qJA>： l] PqK
Proof Direct calculation.
F.3 Concentration for Convolutional Dictionary Learning
In this section, we show concentration for the Riemannian gradient and Hessian of the following
objective for convolutional dictionary learning,
1	4	1p	4
Pcdl(G = ´"	`	>qJAx>4 = ´`	∑ >qJAXi>4
12θ (1 ´ θ) np	4	12θ (1 ´ θ) np i“i	4
with
Cxi1
X = [Xi X2 …	Xps,	Xi =	.	,	(F.25)
57
Published as a conference paper at ICLR 2020
as we introduced in Section 3, where xij follows i.i.d. BGpθq distribution as in Assumption E.2.
Since Cxij is a circulant matrix generated from xij , it should be noted that each row and column
of X is not statistically independent, so that our concentration result of dictionary learning in the
previous subsection does not directly apply here. However, from Lemma D.1, asymptotically we still
have
Eχ rpcDLpqqs	“	夕TPqq	—	2(1 ´ J,K2,	2TPqq	“	´4	>qjA>4,
in the following We prove finite sample concentration of 0cdl(q) to its expectation 夕TPq) by
leveraging our previous results for overcomplete dictionary learning in Proposition F.3 and Proposition
F.6.
F.3.1 Concentration FOR grad 0cdl(∙)
Corollary F.9 (Concentration of grad Pcdl (•)) Suppose A satisfies Equation (F.9) and X P
RmXnp is generated as in Equation (F.25) with Xij 〜u. BG(θ) (1 ≤ i ≤ p, 1 ≤ j ≤ K)
and θ P (mm；, 1). Forany given δ P (0,cK2{(m log2 P log2 np),whenever
peCδ-2θK5n2 log
θKn
we have
sup }gradPCDLpq)´ grad夕τ(q)} < δ
qeSnτ
holds with probability at least 1 — Cnp´. Here, c,c1,C > 0 are some numerical constants.
Remark. Note that our prove have not utilized the convolutional structure of the problem, so that
our sample complexity could be loose of a factor of order n.
Proof Let us write
Xi “ [Xi1	Xi2	•… XinS , With Xij
sj´1 rxi1 s
.
.
.
sj´1 rXiK s
1 ≤ i ≤ p, 1 ≤ j ≤ n,
(F.26)
where s` [∙] denotes CircU山nt shift of length '. Thus, the Riemannian gradient of PCDL Pq) can be
Written as
1	pn	3
grad ⅞pcDLpq) = ´ 3θp1 ´ θqnpPqK ∑ ∑ 'q AXij) pAxijq
1n
1 S
n y
j“1

1p	3
Wz^PqK £(qJAxij)	pAxijq
ooooooooooooooooooooooooooooooooooooooooooooooooon
y	-
gradj 0CDL(q)
so that for each j with 1 ≤ j ≤ n,
gradj PCDLpq)“

1p	3
WZ^ PqK ∑'qj Axij)(Axijq
is a summation of independent random vectors across p. Hence, we have
sup }grad PCDLpq) — grad 夕τ(q)} <
qeSnτ
Sn
j“1
sup
qpSn—1
> gradj Pcdl (q) — grad 3τ(q)>
1
n
)
where for each j we can apply concentration results in Proposition F.3 for controlling each individual
quantity > gradj 0cdl(q) — grad 夕τ(q)>. Therefore, by using a union bound we can obtain the
desired result.	■
58
Published as a conference paper at ICLR 2020
Table 1: Gradient for each different loss function
Problem	Overcomplete Tensor	ODL	CDL
Loss Hq)	-4 MJq>>4	-4p W	›	›4 -4np∑p=1 yp f q 4	
Gradient ▽夕(q)	-A (AJq)d3	-P Y(YJq) d3	-np ∑p=ι yp f (yp f q)d3
F.3.2 Concentration FOR Hess Pcdl (∙)
Corollary F.10 (Concentration of Hess 0cdl(∙)) Suppose A satisfies Equation (F.9) and X P
RmXnp is generated as 说 Equation (F.25) with Xij 〜i.i.d. BG(θ) (1 ≤ i ≤ p, 1 ≤ j ≤ K) and
θ P (mm-, 2) .For any given δ P (0,cK 2{(m log2 P log2 np)), whenever
PeCδ-2θK6n3 log (θKn{δ),
we have
sup }Hess夕dl(q) — Hess夕τ(q)} < δ
qpSn—1
holds with probability at least 1 — CnP´. Here, c,c1,C > 0 are some numerical constants.
Proof Similar to the proof of Corollary F.9, the Riemannian Hessian of Pcdl (q) can be written as
Hess Pcdl (q)
pn
“ ―3θ(1.θqnp ΣΣ PqK [3 'qJAxij )2 AXk(Axij )J — (qJAxij )4 I] PqK
1n	1 p	2	4
“ n Σ {—	3θ(θqp	Σ	PqK	I3 (qJ Axij)	Axk(Axij )J —	(qJAxij)	I]	PqK	),
loooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooon
^"v
HeSSj 0cdl(q)
so that for each j with 1 ≤ j ≤ n,
p
Hessj PCDLpq)	=	— 3θ(1 — θ)p	∑PqK	[3	(qJAxij)2	Axk(Axij)J — (qJAxij)4	I]	PqK
is a summation of independent random vectors across P. Hence, we have
sup }Hess Pcdl (q) — Hess 夕 τ(q)}
qeSnτ
Σn
j“
sup
qpSn—1
}Hessj Pcdl (q)一
Hess 夕 τ(q)}
<
1
n
)
where for each j we can apply concentration results in Proposition F.6 for controlling each individual
quantity } Hessj PCDL (q) — Hess 夕τ(q)}. Therefore, by using a union bound we can obtain the
desired result.	■
G Optimization Algorithms
G. 1 Optimization
In this part of the appendix, we introduce algorithmic details for optimizing the following problem
min g(q),	q P sn´1,
q
where the loss function 夕(q) and its gradient ▽夕(q) for different problems are listed in Table 1.
59
Published as a conference paper at ICLR 2020
Algorithm 2 Projected Riemannian Gradient Descent Algorithm
Input: Data Y P RnXp
Output: the vector q‹
1: Initialize the iterate qp0q randomly, and set a stepsize τp0q .
2:
3:
4:
while not converged do
Compute Riemannian gradient grad 夕(qpkq) “
Update the iterate by
PqpkDK ▽奴 qpkq).
q(k'1) “ PSnT (qpkq ´ Tpkq grad奴q(k))).
5:	Choose a new stepsize Tpk'1q, and set k D k ` 1.
6:	end while
Algorithm 3 Power Method
Input: Data Y P RnXp
Output: the vector q‹
1:	Randomly initialize the iterate q(0q .
2:	while not converged do
3:	Compute the gradient ▽夕(qpkq).
4:	Update the iterate by
qpk'1q “ Psn-1 (´v奴q(k))).
5:	Set k D k ` 1.
6:	end while
Riemannian gradient descent. To optimize the problem, the most natural idea is starting from a
random initialization, and taking projected Riemannian gradient descent steps
q D PSnT (q — T ∙ grad g(q)), grad p(q) = PqK V^(q),	(G.1)
where T is the stepsize that can be chosen via linesearch or set as a small constant. We summarize
this simple method in Algorithm 2.
Power method. In Algorithm 3 We also introduce a simple power method17 JoUrnee et al. (2010)
by noting that the loss function Hq) is concave so that the problem is equivalent to maximizing a
convex function. For each iteration, we simply update q by
q D PSnT (´VHq))
which is parameter-free and enjoys much faster convergence speed. We summarized the method in
Algorithm 3. Notice that the power iteration can be interpreted as the Riemannian gradient descent
with varied step sizes in the sense that
Psn-1 (q — T ∙ grad 夕(q)) “ PSnT
(´ τVφ(q) + '1 ´ T ∙ JVyooq)[ q)
“0
=PSnT ( —VHq))
by setting T = qτv⅛q).
G.2 Fast Implementation of CDL via FFT
Given the problem setup of CDL in Section 3, in the following we describe more efficient implemen-
tation of solving CDL using convolution and FFTs. Namely, we show how to rewrite the gradient
of yCDL (q) in the convolutional form. Notice that the preconditioning matrix can be rewrite as a
17Similar approach also appears in (Zhai et al., 2019).
60
Published as a conference paper at ICLR 2020
circulant matrix by
1 I P	、T{2
P “(θnp ∑ CyiCyj
where ypi “ F yi . Thus, we have
F * diag (Pp) F = Cp, P “ F T
1 I P	∖T∕2
(⅛ρ 产)
PCyi	“	Cp Cyi	“	Cpfyi	“	Cyip , yiP	“ P f yi	,
so that
min 3CDLpqq = ´4np g >Cfyiq>4 “— 4np £>|f q>4,	s∙t∙ q P Sn?
i“1	i“1
Thus, we have the gradient
▽g CDL(q)
´ ηρ W yp f (If q)d3
where V denote a cyclic reversal of any v P Rn, i.e., v = [vι,vn, vn´1,…，v2SJ.
61