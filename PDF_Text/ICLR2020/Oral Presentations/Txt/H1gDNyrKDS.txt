Published as a conference paper at ICLR 2020
Understanding and Robustifying
Differentiable Architecture Search
Arber Zela1, Thomas Elsken2,1, Tonmoy Saikia1, Yassine Marrakchi1,
Thomas Brox1 & Frank Hutter1,2
1	Department of Computer Science, University of Freiburg
{zelaa, saikiat, marrakch, brox, fh}@cs.uni-freiburg.de
2	Bosch Center for Artificial Intelligence
Thomas.Elsken@de.bosch.com
Ab stract
Differentiable Architecture Search (DARTS) has attracted a lot of attention due to
its simplicity and small search costs achieved by a continuous relaxation and an
approximation of the resulting bi-level optimization problem. However, DARTS
does not work robustly for new problems: we identify a wide range of search
spaces for which DARTS yields degenerate architectures with very poor test per-
formance. We study this failure mode and show that, while DARTS successfully
minimizes validation loss, the found solutions generalize poorly when they coin-
cide with high validation loss curvature in the architecture space. We show that
by adding one of various types of regularization we can robustify DARTS to find
solutions with less curvature and better generalization properties. Based on these
observations, we propose several simple variations of DARTS that perform sub-
stantially more robustly in practice. Our observations are robust across five search
spaces on three image classification tasks and also hold for the very different do-
mains of disparity estimation (a dense regression task) and language modelling.
1 Introduction
Neural Architecture Search (NAS), the process of automatically designing neural network archi-
tectures, has recently attracted attention by achieving state-of-the-art performance on a variety of
tasks (Zoph & Le, 2017; Real et al., 2019). Differentiable architecture search (DARTS) (Liu et al.,
2019) significantly improved the efficiency of NAS over prior work, reducing its costs to the same or-
der of magnitude as training a single neural network. This expanded the scope of NAS substantially,
allowing it to also be applied on more expensive problems, such as semantic segmentation (Chenxi
et al., 2019) or disparity estimation (Saikia et al., 2019).
However, several researchers have also reported DARTS to not work well, in some cases even no
better than random search (Li & Talwalkar, 2019; Sciuto et al., 2019). Why is this? How can these
seemingly contradicting results be explained? The overall goal of this paper is to understand and
overcome such failure modes of DARTS. To this end, we make the following contributions:
1.	We identify 12 NAS benchmarks based on four search spaces in which standard DARTS yields
degenerate architectures with poor test performance across several datasets (Section 3).
2.	By computing the eigenspectrum of the Hessian of the validation loss with respect to the archi-
tectural parameters, we show that there is a strong correlation between its dominant eigenvalue
and the architecture’s generalization error. Based on this finding, we propose a simple variation
of DARTS with early stopping that performs substantially more robustly (Section 4).
3.	We show that, related to previous work on sharp/flat local minima, regularizing the inner objective
of DARTS more strongly allows it to find solutions with smaller Hessian spectrum and better
generalization properties. Based on these insights, we propose two practical robustifications of
DARTS that overcome its failure modes in all our 12 NAS benchmarks (Section 5).
Our findings are robust across a wide range of NAS benchmarks based on image recognition and
also hold for the very different domains of language modelling (PTB) and disparity estimation. They
1
Published as a conference paper at ICLR 2020
consolidate the findings of the various results in the literature and lead to a substantially more robust
version of DARTS. We provide our implementation and scripts to facilitate reproducibility1.
2	Background and Related Work
2.1	Relation between flat/sharp minima and generalization performance
Already Hochreiter & Schmidhuber (1997) observed that flat minima of the training loss yield better
generalization performance than sharp minima. Recent work (Keskar et al., 2016; Yao et al., 2018)
focuses more on the settings of large/small batch size training, where observations show that small
batch training tends to get attracted to flatter minima and generalizes better. Similarly, Nguyen et al.
(2018) observed that this phenomenon manifests also in the hyperparameter space. They showed
that whenever the hyperparameters overfit the validation data, the minima lie in a sharper region of
the space. This motivated us to conduct a similar analysis in the context of differentiable architecture
search later in Section 4.1, where we see the same effect in the space of neural network architectures.
2.2	Bi-level Optimization
We start by a short introduction of the bi-level optimization problem (Colson et al., 2007). These
are problems which contain two optimization tasks, nested within each other.
Definition 2.1. Given the outer objective function F : RP × RN → R and the inner objective
function f : RP × RN → R, the bi-level optimization problem is given by
min F (y, θ*(y))	(1)
y∈RP
St θ*(y) ∈ arg min f(y,θ),	(2)
θ∈RN
where y ∈ RP and θ ∈ RN are the outer and inner variables, respectively. One may also see the
bi-level problem as a constrained optimization problem, with the inner problem as a constraint.
In general, even in the case when the inner objective (2) is strongly convex and has an unique
minimizer θ* (y) = argminj∈RN f (y, θ), it is not possible to directly optimize the outer objective
(1). A possible method around this issue is to use the implicit function theorem to retrieve the
derivative of the solution map (or response map) θ* (y) ∈ F ⊆ RN w.r.t. y (Bengio, 2000; Pedregosa,
2016; Beirami et al., 2017). Another strategy is to approximate the inner problem with a dynamical
system (Domke, 2012; Maclaurin et al., 2015; Franceschi et al., 2017; 2018), where the optimization
dynamics could, e.g., describe gradient descent. In the case that the minimizer of the inner problem
is unique, under some conditions the set of minimizers of this approximate problem will indeed
converge to the minimizers of the bilevel problem (1) (see Franceschi et al. (2018)).
2.3	Neural Architecture Search
Neural Architecture Search (NAS) denotes the process of automatically designing neural network
architectures in order to overcome the cumbersome trial-and-error process when designing archi-
tectures manually. We briefly review NAS here and refer to the recent survey by Elsken et al.
(2019b) for a more thorough overview. Prior work mostly employs either reinforcement learning
techniques (Baker et al., 2017a; Zoph & Le, 2017; Zhong et al., 2018; Zoph et al., 2018) or evo-
lutionary algorithms (Stanley & Miikkulainen, 2002; Liu et al., 2018b; Miikkulainen et al., 2017;
Real et al., 2017; 2019) to optimize the discrete architecture space. As these methods are often
very expensive, various works focus on reducing the search costs by, e.g., employing network mor-
phisms (Cai et al., 2018a;b; Elsken et al., 2017; 2019a), weight sharing within search models (Sax-
ena & Verbeek, 2016; Bender et al., 2018; Pham et al., 2018) or multi-fidelity optimization (Baker
et al., 2017b; Falkner et al., 2018; Li et al., 2017; Zela et al., 2018), but their applicability still often
remains restricted to rather simple tasks and small datasets.
1 https://github.com/automl/RobustDARTS
2
Published as a conference paper at ICLR 2020
2.4	Differentiable Architecture Search (DARTS)
A recent line of work focuses on relaxing the discrete neural architecture search problem to a con-
tinuous one that can be solved by gradient descent (Liu et al., 2019; Xie et al., 2019; Casale et al.,
2019; Cai et al., 2019). In DARTS (Liu et al., 2019), this is achieved by simply using a weighted
sum of possible candidate operations for each layer, whereas the real-valued weights then effec-
tively parametrize the network’s architecture. We will now review DARTS in more detail, as our
work builds directly upon it.
Continuous relaxation of the search space. In agreement with prior work (Zoph et al., 2018;
Real et al., 2019), DARTS optimizes only substructures called cells that are stacked to define the
full network architecture. Each cell contains N nodes organized in a directed acyclic graph. The
graph contains two inputs nodes (given by the outputs of the previous two cells), a set of intermediate
nodes, and one output node (given by concatenating all intermediate nodes). Each intermediate node
x(j) represents a feature map. See Figure 1 for an illustration of such a cell. Instead of applying
a single operation to a specific node during architecture search, Liu et al. (2019) relax the decision
which operation to choose by computing the intermediate node as a mixture of candidate operations,
applied to predecessor nodes x(i), i < j, χ(j) = P< P°∈o P exp(αo,j)*jjo (x(i)), where O
denotes the set of all candidate operations (e.g., 3 × 3 convolution, skip coonnection, 3 × 3 max
pooling, etc.) and α = (αio,j)i,j,o serves as a real-valued parameterization of the architecture.
Gradient-based optimization of the search space. DARTS then optimizes both the weights of
the search network (often called the weight-sharing or one-shot model, since the weights of all
individual subgraphs/architectures are shared) and architectural parameters by alternating gradient
descent. The network weights and the architecture parameters are optimized on the training and
validation set, respectively. This can be interpreted as solving the bi-level optimization problem (1),
(2), where F and f are the validation and training loss, Lvalid and Ltrain, respectively, while y and
θ denote the architectural parameters α and network weights w, respectively. Note that DARTS only
approximates the lower-level solution by a single gradient step (see Appendix A for more details).
At the end of the search phase, a discrete cell is obtained by choosing the k most important incoming
operations for each intermediate node while all others are pruned. Importance is measured by the
operation weighting factor
exp(aθ,j)
Po0∈o exp(aOoj)
3	WHEN DARTS FAILS
We now describe various search spaces and demonstrate that standard DARTS fails on them. We
start with four search spaces similar to the original CIFAR-10 search space but simpler, and evaluate
across three different datasets (CIFAR-10, CIFAR-100 and SVHN). They are quite standard in that
they use the same macro architecture as the original DARTS paper (Liu et al., 2018a), consisting of
normal and reduction cells; however, they only allow a subset of operators for the cell search space:
S1:	This search space uses a different set of only two operators per edge, which we identified
using an offline process that iteratively dropped the operations from the original DARTS
search space with the least importance. This pre-optimized space has the advantage of
being quite small while still including many strong architectures. We refer to Appendix B
for details on its construction and an illustration (Figure 9).
S2:	In this space, the set of candidate operations per edge is {3 × 3 SepConv, SkipConnect}.
We choose these operations since they are the most frequent ones in the discovered cells
reported by Liu et al. (2019).
S3:	In this space, the set of candidate operations per edge is {3 × 3 SepConv, SkipConnect,
Zero}, where the Zero operation simply replaces every value in the input feature map by
zeros.
S4:	In this space, the set of candidate operations per edge is {3 × 3 SepConv, Noise}, where
the Noise operation simply replaces every value from the input feature map by noise
E 〜N(0,1). This is the only space out of S1-S4 that is not a strict subspace of the
3
Published as a conference paper at ICLR 2020
skip connect
skip conιιιecl
skip Co
skip conned
:{k-2}
skip connect
:{k-1}
skip connect
(a) Space 1
I----1 skip connect
R-2} Sk≡cf
skip connect
skip_conniect
skip connect
skip connect
skip_COmeCt
(b) Space 2
k
t
iip
nne
C {k-1;
c {k-2;
skip connect
skip_connect
(c) Space 3




Figure 1: The poor cells standard DARTS finds on spaces S1-S4. For all spaces, DARTS chooses
mostly parameter-less operations (skip connection) or even the harmful N oise operation. Shown
are the normal cells on CIFAR-10; see Appendix G for reduction cells and other datasets.
original DARTS space; we intentionally added the Noise operation, which actively harms
performance and should therefore not be selected by DARTS.
We ran DARTS on each of these spaces, using exactly the same setup as Liu et al. (2019). Figure
1 shows the poor cells DARTS selected on these search spaces for CIFAR-10 (see Appendix G for
analogous results on the other datasets). Already visually, one might suspect that the found cells are
suboptimal: the parameter-less skip connections dominate in almost all the edges for spaces S1-S3,
and for S4 even the harmful Noise operation was selected for five out of eight operations. Table
1 (first column) confirms the very poor performance standard DARTS yields on all of these search
spaces and on different datasets. We note that Liu et al. (2019) and Xie et al. (2019) argue that the
Zero operation can help to search for the architecture topology and choice of operators jointly, but
in our experiments it did not help to reduce the importance weight of the skip connection (compare
Figure 1b vs. Figure 1c).
We emphasize that search spaces S1-S3 are very natural, and, as strict subspaces of the original
space, should merely be easier to search than that. They are in no way special or constructed in an
adversarial manner. Only S4 was constructed specifically to show-case the failure mode of DARTS
selecting the obviously suboptimal Noise operator.
S5:	Very small search space with known global optimum. Knowing the global minimum has
the advantage that one can benchmark the performance of algorithms by measuring the regret of
chosen points with respect to the known global minimum. Therefore, we created another search
space with only one intermediate node for both normal and reduction cells, and 3 operation choices
in each edge, namely 3 × 3 SepConv, SkipConnection, and 3 × 3 MaxPooling. The total number of
possible architectures in this space is 81, all of which we evaluated a-priori. We dub this space S5.
We ran DARTS on this search space three times
for each dataset and compared its result to the
baseline of Random Search with weight shar-
ing (RS-ws) by Li & Talwalkar (2019). Fig-
ure 2 shows the test regret of the architectures
selected by DARTS (blue) and RS-ws (green)
throughout the search. DARTS manages to find
an architecture close to the global minimum,
but around epoch 40 the test performance de-
teriorated. Note that the search model valida-
tion error (dashed red line) did not deteriorate
but rather converged, indicating that the archi-
tectural parameters are overfitting to the vali-
dation set. In contrast, RS-ws stays relatively
constant throughout the search; when evaluat-
ing only the final architecture found, RS-ws in-
deed outperformed DARTS.
L2 factor: 0.0003
(％)∙*j3j63j !Sα,J.
---- DARTS test regret
----DARTS one-shot val. error
---- RS-ws test regret
(％rOXJα,UoAep=e>
O ŋ O O
5 4 3 2
O
O IO 20	30	40
Search epoch
Figure 2: Test regret of found architectures and
validation error of the search model when running
DARTS on S5 and CIFAR-10. DARTS finds the
global minimum but starts overfitting the architec-
tural parameters to the validation set in the end.
10
50
S6:	encoder-decoder architecture for disparity estimation. To study whether our findings gen-
eralize beyond image recognition, we also analyzed a search space for a very different problem:
finding encoder-decoder architectures for the dense regression task of disparity estimation; please
4
Published as a conference paper at ICLR 2020
Search epoch
Figure 3: (left) validation error of search model; (middle) test error of the architectures deemed by
DARTS optimal (right) dominant eigenvalue of NQaLValid throughout DARTS search. Solid line and
shaded areas show mean and standard deviation of 3 independent runs. All experiments conducted
on CIFAR-10.
refer to Appendix E for details. We base this search space on AutoDispNet (Saikia et al., 2019),
which used DARTS for a space containing normal, downsampling and upsampling cells. We again
constructed a reduced space. Similarly to the image classification search spaces, we found the nor-
mal cell to be mainly composed of parameter-less operations (see Figure 25 in Appendix G). As
expected, this causes a large generalization error (see first row in Table 2 of our later experiments).
4	THE ROLE OF DOMINANT EIGENVALUES OF N2αLvalid
We now analyze why DARTS fails in all these cases. Motivated by Section 2.1, we will have a
closer look at the largest eigenvalue λαmax of the Hessian matrix of validation loss NQαLvalid w.r.t.
the architectural parameters α.
4.1	Large architectural eigenvalues and generalization performance
One may hypothesize that DARTS performs poorly because its approximate solution of the bi-level
optimization problem by iterative optimization fails, but we actually observe validation errors to
progress nicely: Figure 3 (left) shows that the search model validation error converges in all cases,
even though the cell structures selected here are the ones in Figure 1.
Rather, the architectures DARTS finds do not gen-
eralize well. This can be seen in Figure 3 (middle).
There, every 5 epochs, we evaluated the architecture
deemed by DARTS to be optimal according to the α
values. Note that whenever evaluating on the test set,
we retrain from scratch the architecture obtained af-
ter applying the argmax to the architectural weights
α. As one can notice, the architectures start to de-
generate after a certain number of search epochs,
similarly to the results shown in Figure 2. We hy-
pothesized that this might be related to sharp local
minima as discussed in Section 2.1. To test this hy-
pothesis, we computed the full Hessian NQαLvalid of
Figure 4: Correlation between dominant
eigenvalue of NQαLvalid and test error of cor-
responding architectures.
the validation loss w.r.t. the architectural parameters
on a randomly sampled mini-batch. Figure 3 (right)
shows that the dominant eigenvalue λαmax (which
serves as a proxy for the sharpness) indeed increases
in standard DARTS, along with the test error (middle) of the final architectures, while the validation
error still decreases (left). We also studied the correlation between λαmax and test error more directly,
by measuring these two quantities for 24 different architectures (obtained via standard DARTS and
the regularized versions we discuss in Section 5). For the example of space S1 on CIFAR-10, Figure
4 shows that λαmax indeed strongly correlates with test error (with a Pearson correlation coefficient
of 0.867).
5
Published as a conference paper at ICLR 2020
Figure 6: Local average (LA) of the dominant eigenvalue λαmax throughout DARTS search. Markers
denote the early stopping point based on the criterion in Section 4.3. Each line also corresponds to
one of the runs in Table 1.
4.2	Large architectural eigenvalues and performance drop after pruning
One reason why DARTS performs poorly when the
architectural eigenvalues are large (and thus the min-
imum is sharp) might be the pruning step at the
end of DARTS: the optimal, continuous α* from the
search is pruned to obtain a discrete αdisc, some-
where in the neighbourhood of α*. In the case of
a sharp minimum α*, αdisc might have a loss func-
tion value significantly higher than the minimum α*,
while in the case of a flat minimum, αdisc is ex-
pected to have a similar loss function value. This
is hypothetically illustrated in Figure 5a, where the
y-axis indicates the search model validation loss and
the x-axis the α values.
To investigate this hypothesis, we measured the per-
formance drop: Lvalid(adisc, w*) - Lvalid(α* ,w*)
w.r.t. to the search model weights incurred by this
discretization step and correlated it with λαmax . The
results in Figure 5b show that, indeed, low curva-
ture never led to large performance drops (here we
actually compute the accuracy drop rather than the
loss function difference, but we observed a similar
relationship). Having identified this relationship, we
now move on to avoid high curvature.
(b)
Figure 5: (a) Hypothetical illustration of the
loss function change in the case of flat vs.
sharp minima. (b) Drop in accuracy after dis-
cretizing the search model vs. the sharpness
of minima (by means of λαmax).
4.3	Early stopping
BASED ON LARGE EIGENVALUES OF 睦Lvalid
We propose a simple early stopping methods to
avoid large curvature and thus poor generalization.
We emphasize that simply stopping the search based
on validation performance (as one would do in the case of training a network) does not apply here
as NAS directly optimizes validation performance, which — as We have seen in Figure 2 — keeps on
improving.
Instead, we propose to track λαmax over the course of architecture search and stop whenever it in-
creases too much. To implement this idea, we use a simple heuristic that worked off-the-shelf
without any tuning. Let λmax (i) denote the value of λmax smoothed over k = 5 epochs around
i; then, we stop if λmax(i - k)/λmax(i) < 0.75 and return the architecture from epoch i - k.
6
Published as a conference paper at ICLR 2020
Figure 7: Effect of regularization strength via ScheduledDropPath (during the search phase) on the
test performance of DARTS (solid lines) and DARTS-ES (dashed-lines). Results for each of the
search spaces and datasets.
By this early stopping heuristic, we do not only avoid exploding
eigenvalues, which are correlated with poor generalization (see Fig-
ure 4), but also shorten the time of the search.
Table 1 shows the results for running DARTS with this early stop-
ping criterion (DARTS-ES) across S1-S4 and all three image classi-
fication datasets. Figure 6 shows the local average of the eigenvalue
trajectory throughout the search and the point where the DARTS
search early stops for each of the settings in Table 1. Note that we
never use the test data when applying the early stopping mecha-
nism. Early stopping significantly improved DARTS for all settings
without ever harming it.
5	Regularization of inner objective
IMPROVES GENERALIZATION OF ARCHITECTURES
Table 1: Performance of
DARTS and DARTS-ES.
(mean ± std for 3 runs each).
Benchmark		DARTS	DARTS-ES
C10	^^ST^	4.66 ± 0.71	3.05 ± 0.07 :
	^^S2^^	4.42 ± 0.40	3.41 ± 0.14
	^^S3""	4.12 ± 0.85	3.71 ± 1.14
	^^S4^	6.95 ± 0.18	4.17 ± 0.21
C100	^^ST^	29.93 ± 0.41	28.90 ± 0.81
	^^S2^^	28.75 ± 0.92	24.68 ± 1.43
	^^S3""	29.01 ± 0.24	26.99 ± 1.79
	^^S4^	24.77 ± 1.51	23.90 ± 2.01
SVHN	^^ST^	9.88 ± 5.50 ;	2.80 ± 0.09 :
	^^S2^^	3.69 ± 0.12	2.68 ± 0.18
	^^S3""	4.00 ± 1.01	2.78 ± 0.29
	~S4^	2.90 ± 0.02	2.55 ± 0.15 一
As we saw in Section 4.1, sharper minima (by means of large eigenvalues) of the validation loss
lead to poor generalization performance. In our bi-level optimization setting, the outer variables’
trajectory depends on the inner optimization procedure. Therefore, we hypothesized that modifying
the landscape of the inner objective Ltrain could redirect the outer variables α to flatter areas of the
architectural space. We study two ways of regularization (data augmentation in Section 5.1 and L2
regularization in Section 5.2) and find that both, along with the early stopping criterion from Section
4.3, make DARTS more robust in practice. We emphasize that we do not alter the regularization of
the final training and evaluation phase, but solely that of the search phase. The setting we use for all
experiments in this paper to obtain the final test performance is described in Appendix C.
5.1	Regularization via data augmentation
We first investigate the effect of regularizing via data augmentation, namely masking out parts of
the input and intermediate feature maps via Cutout (CO, DeVries & Taylor (2017)) and Scheduled-
DropPath (DP, Zoph et al. (2018)) (ScheduledDropPath is a regularization technique, but we list
it here since we apply it together with Cutout), respectively, during architecture search. We ran
DARTS with CO and DP (with and without our early stopping criterion, DARTS-ES) with different
maximum DP probabilities on all three image classification datasets and search spaces S1-S4.
Figure 7 summarizes the results: regularization improves the test performance of DARTS and
DARTS-ES in all cases, sometimes very substantially, and at the same time kept the dominant eigen-
value relatively low (Figure 13). This also directly results in smaller drops in accuracy after pruning,
as discussed in Section 4.2; indeed, the search runs plotted in Figure 5b are the same as in this sec-
tion. Figure 17 in the appendix explicitly shows how regularization relates to the accuracy drops.
We also refer to further results in the appendix: Figure 11 (showing test vs. validation error) and
Table 5 (showing that overfitting of the architectural parameters is reduced).
7
Published as a conference paper at ICLR 2020
Figure 8: Effect of L2 regularization of the inner objective during architecture search for DARTS
(solid lines) and DARTS-ES (dashed).
Similar observations hold for disparity estimation on S6, where we vary the strength of standard data
augmentation methods, such as shearing or brightness change, rather then masking parts of features,
which is unreasonable for this task. The augmentation strength is described by an “augmentation
scaling factor” (Appendix E). Table 2 summarizes the results. We report the average end point error
(EPE), which is the Euclidean distance between the predicted and ground truth disparity maps. Data
augmentation avoided the degenerate architectures and substantially improved results.
5.2 INCREASED L2 REGULARIZATION
As a second type of regularization, we also tested
different L? regularization factors 3i ∙ 10-4 for i ∈
{1, 3, 9, 27, 81}. Standard DARTS in fact does al-
ready include a small amount of L2 regularization;
i = 1 yields its default. Figure 8 shows that DARTS’
test performance (solid lines) can be significantly
improved by higher L2 factors across all datasets and
spaces, while keeping the dominant eigenvalue low
(Figure 14). DARTS with early stopping (dashed
lines) also benefits from additional regularization.
Again, we observe the implicit regularization effect
on the outer objective which reduces the overfitting
of the architectural parameters. We again refer to
Table 2 for disparity estimation; Appendix F shows
TreeBank).
Table 2: Effect of regularization for dispar-
ity estimation. Search was conducted on
FlyingThings3D (FT) and then evaluated on
both FT and Sintel. Lower is better.
Aug. Scale	Search model valid EPE	FT test EPE	Sintel test EPE	Params (M)
0.0	4.49	3.83	5.69	9.65
0.1	3.53	3.75	5.97	9.65
0.5	3.28	3.37	5.22	9.43
1.0	4.61	3.12	5.47	12.46
1.5	5.23	2.60	4.15	12.57
2.0	7.45		2.33	3.76	12.25
L2 reg.	Search model valid	FT test	Sintel test	Params
factor	EPE	EPE	EPE	(M)
3 × 10-4	395	3.25	6.13	11.00
9 × 10-4	5.97	2.30	4.12	13.92
27 × 10-4	4.25	2.72	4.83	10.29
81 × 10-4	4.61		2.34	3.85	12.16
similar results for language modelling (Penn
5.3 Practical Robustification of DARTS by Regularizing the Inner Objective
Based on the insights from the aforementioned analysis and empirical results, we now propose two
alternative simple modifications to make DARTS more robust in practice without having to manually
tune its regularization hyperparameters.
DARTS with adaptive regularization One option is to adapt DARTS’ regularization hyperpa-
rameters in an automated way, in order to keep the architectural weights in areas of the validation
loss objective with smaller curvature. The simplest off-the-shelf procedure towards this desiderata
would be to increase the regularization strength whenever the dominant eigenvalue starts increasing
rapidly. Algorithm 1 (DARTS-ADA, Appendix D.1) shows such a procedure. We use the same
stopping criterion as in DARTS-ES (Section 4.3), roll back DARTS to the epoch when this criterion
is met, and continue the search with a larger regularization value R for the remaining epochs (larger
by a factor of η). This procedure is repeated whenever the criterion is met, unless the regularization
value exceeds some maximum predefined value Rmax .
Multiple DARTS runs with different regularization strength Liu et al. (2019) already sug-
gested to run the search phase of DARTS four times, resulting in four architectures, and to return
8
Published as a conference paper at ICLR 2020
the best of these four architectures w.r.t. validation performance when retrained from scratch for a
limited number of epochs. We propose to use the same procedure, with the only difference that
the four runs use different amounts of regularization. The resulting RobustDARTS (R-DARTS)
method is conceptually very simple, trivial to implement and likely to work well if any of the tried
regularization strengths works well.
Table 3 evaluates the performance Table 3: Empirical evaluation of practical robustified ver- of our practical robustifications sions of DARTS. Each entry is the test error after retraining of DARTS, DARTS-ADA and the selected architecture as usual. The best method for each R-DARTS (based on either L2 or setting is boldface and underlined, the second best boldface. ScheduledDropPath regularization),								
by comparing them to the original DARTS, DARTS-ES and Random Search with weight sharing (RS-ws). For each of these methods, as pro- posed in the DARTS paper (Liu et al., 2019), we ran the search four inde- pendent times with different random seeds and selected the architecture used for the final evaluation based on	Benchmark		RS-Ws	DARTS	R-DARTS(DP)	R-DARTS(L2)	DARTS-ES	DARTS-ADA
	C10	^^SΓ"	3.23	3.84	3.11	=	2.78	=	3.01	3.10
		^^S2^	3.66	4.85	348	331	-326	3.35
		^^S3^^	2.95	3.34	293	251	274	259
		^^S4^	8.07	7.20	358	356	371	484
	C100	^^SΓ"	23.30	29.46	25.93	=	24.25	=	28.37	24.03
		^^S2-	21.21	26.05	22:30	2224	-2325-	2352
		^^S3^^	23.75	28.90	2236	2399	-2373-	2337
		^^S4^	28.19	22.85	22:18	2194	-2126-	2320
	SVHN	=Sf=	2.59	4.58	2.55	=	4.79	=	2.72	2.53
		^^S2^	2.72	3.53	252	2H	260	254
		^^S3^^	2.87	3.41	249	248	250	250
		^^S4^	3.46	3.05	261	250	251	246
a validation run as described above.								
As the table shows, in accordance with Li & Talwalkar (2019), RS-ws often outperformed the
original DARTS; however, with our robustifications, DARTS typically performs substantially
better than RS-ws. DARTS-ADA consistently improved over standard DARTS for all benchmarks,
indicating that a gradual increase of regularization during search prevents ending up in the bad
regions of the architectural space. Finally, RobustDARTS yielded the best performance and since it
is also easier to implement than DARTS-ES and DARTS-ADA, it is the method that we recommend
to be used in practice.
Finally, since the evaluations in this paper have so far
focussed on smaller subspaces of the original DARTS
search space, the reader may wonder how well Robust-
DARTS works on the full search spaces. As Table 4
shows, RobustDARTS performed similarly to DARTS
for the two original benchmarks from the DARTS paper
(PTB and CIFAR-10), on which DARTS was developed
and is well tuned; however, even when only changing the
dataset to CIFAR-100 or SVHN, RobustDARTS already
performed significantly better than DARTS, underlining
its robustness.
Table 4: DARTS vs. RobustDARTS on
the original DARTS search spaces. We
show mean ± stddev for 5 repetitions
(based on 4 fresh subruns each as in Ta-
ble 3); for the more expensive PTB we
could only afford 1 such repetition.
Benchmark	DARTS	R-DARTS(L2)
C10	2.91 ± 0.25 二	2.95 ± 0.2厂
-C100-	20.58 ± 0.44	18.01 ± 0.26
-SVHN-	2.46 ± 0.09	2.17 ± 0.09
PTB	58.64	=	57.59 =
6 Conclusions
We showed that the generalization performance of architectures found by DARTS is related to the
eigenvalues of the Hessian matrix of the validation loss w.r.t. the architectural parameters. Stan-
dard DARTS often results in degenerate architectures with large eigenvalues and poor generaliza-
tion. Based on this observation, we proposed a simple early stopping criterion for DARTS based
on tracking the largest eigenvalue. Our empirical results also show that properly regularizing the
inner objective helps controlling the eigenvalue and therefore improves generalization. Our findings
substantially improve our understanding of DARTS’ failure modes and lead to much more robust
versions. They are consistent across many different search spaces on image recognition tasks and
also for the very different domains of language modelling and disparity estimation. Our code is
available for reproducibility.
Acknowledgments
The authors acknowledge funding by the Robert Bosch GmbH, support by the European Research
Council (ERC) under the European Unions Horizon 2020 research and innovation programme
through grant no. 716721, and by BMBF grant DeToL.
9
Published as a conference paper at ICLR 2020
References
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network archi-
tectures using reinforcement learning. In International Conference on Learning Representations,
2017a.
Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating Neural Architecture
Search using Performance Prediction. In NIPS Workshop on Meta-Learning, 2017b.
Ahmad Beirami, Meisam Razaviyayn, Shahin Shahrampour, and Vahid Tarokh. On optimal gener-
alizability in parametric learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 3455-3465. Curran Associates, Inc., 2017.
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understand-
ing and simplifying one-shot architecture search. In International Conference on Machine Learn-
ing, 2018.
Y. Bengio. Gradient-based optimization of hyperparameters. Neural Computation, 12(8):1889-
1900, Aug 2000. ISSN 0899-7667. doi: 10.1162/089976600300015187.
D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical
flow evaluation. In A. Fitzgibbon et al. (Eds.) (ed.), European Conf. on Computer Vision (ECCV),
Part IV, LNCS 7577, pp. 611-625. Springer-Verlag, October 2012.
Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by
network transformation. In AAAI, 2018a.
Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-Level Network Transforma-
tion for Efficient Architecture Search. In International Conference on Machine Learning, June
2018b.
Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target
task and hardware. In International Conference on Learning Representations, 2019.
Francesco Casale, Jonathan Gordon, and Nicolo Fusi. Probabilistic neural architecture search. arXiv
preprint, 2019.
Liu Chenxi, Chen Liang Chieh, Schroff Florian, Adam Hartwig, Hua Wei, Yuille Alan L., and
Fei Fei Li. Auto-deeplab: Hierarchical neural architecture search for semantic image segmenta-
tion. In Conference on Computer Vision and Pattern Recognition, 2019.
Benot Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization, 2007.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Justin Domke. Generic methods for optimization-based modeling. In Neil D. Lawrence and Mark
Girolami (eds.), Proceedings of the Fifteenth International Conference on Artificial Intelligence
and Statistics, volume 22 of Proceedings of Machine Learning Research, pp. 318-326, La Palma,
Canary Islands, 21-23 Apr 2012. PMLR.
A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbay, V. Golkov, P. v.d. Smagt, D. Cremers,
and T. Brox. Flownet: Learning optical flow with convolutional networks. In IEEE International
Conference on Computer Vision (ICCV), 2015.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Simple And Efficient Architecture Search
for Convolutional Neural Networks. In NIPS Workshop on Meta-Learning, 2017.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efficient multi-objective neural architec-
ture search via lamarckian evolution. In International Conference on Learning Representations,
2019a.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.
Journal of Machine Learning Research, 20(55):1-21, 2019b.
10
Published as a conference paper at ICLR 2020
Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and efficient hyperparameter op-
timization at scale. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re-
search, pp. 1437-1446, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/falkner18a.html.
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In Doina Precup and Yee Whye Teh (eds.), Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 1165-1173, International Convention Centre, Sydney, Aus-
tralia, 06-11 Aug 2017. PMLR.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 1568-1577, Stockholmsmssan, Stockholm
Sweden, 10-15 Jul 2018. PMLR.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural ComPut, 9(1):1T2, January 1997.
ISSN 0899-7667. doi: 10.1162/neco.1997.9.1.1.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
PrePrint arXiv:1609.04836, 2016.
L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: Bandit-based
configuration evaluation for hyperparameter optimization. In Proceedings of the International
Conference on Learning RePresentations (ICLR’17), 2017. Published online: iclr.cc.
Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search.
CoRR, abs/1902.07638, 2019.
H. Liu, K. Simonyan, O. Vinyals, C.Fernando, and K. Kavukcuoglu. Hierarchical representations
for efficient architecture search. In International Conference on Learning RePresentations (ICLR)
2018 Conference Track, April 2018a.
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hier-
archical Representations for Efficient Architecture Search. In International Conference on Learn-
ing RePresentations, 2018b.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In
International Conference on Learning RePresentations, 2019.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 2113-2122, Lille, France, 07-09 Jul 2015. PMLR.
N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large
dataset to train convolutional networks for disparity, optical flow, and scene flow estimation.
In IEEE International Conference on ComPuter Vision and Pattern Recognition (CVPR), 2016.
arXiv:1512.02134.
Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala
Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving Deep
Neural Networks. In arXiv:1703.00548, March 2017.
Thanh Dai Nguyen, Sunil Gupta, Santu Rana, and Svetha Venkatesh. Stable bayesian optimization.
International Journal of Data Science and Analytics, 6(4):327-339, Dec 2018. ISSN 2364-4168.
doi: 10.1007/s41060-018-0119-9.
11
Published as a conference paper at ICLR 2020
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Maria Florina Bal-
can and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Ma-
chine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 737-746, New
York, New York, USA, 20-22 JUn 2016. PMLR.
Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture
search via parameter sharing. In International Conference on Machine Learning, 2018.
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan,
Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research, pp. 2902-2911, International Conven-
tion Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Aging Evolution for Image Classi-
fier Architecture Search. In AAAI, 2019.
T. Saikia, Y. Marrakchi, A. Zela, F. Hutter, and T. Brox. Autodispnet: Improving disparity estimation
with automl. In IEEE International Conference on Computer Vision (ICCV), 2019. URL http:
//lmb.informatik.uni-freiburg.de/Publications/2019/SMB19.
Shreyas Saxena and Jakob Verbeek. Convolutional neural fabrics. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems
29, pp. 4053-4061. Curran Associates, Inc., 2016.
Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating
the search phase of neural architecture search. arXiv preprint, 2019.
Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topolo-
gies. Evolutionary Computation, 10:99-127, 2002.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search.
In International Conference on Learning Representations, 2019.
Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis
of large batch training and robustness to adversaries. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 31, pp. 4949-4959. Curran Associates, Inc., 2018.
Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Ef-
ficient joint neural architecture and hyperparameter search. In ICML 2018 Workshop on AutoML
(AutoML 2018), July 2018.
Zhao Zhong, Jingchen Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. Practical block-wise neural
network architecture generation. In CVPR. IEEE Computer Society, 2018.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Interna-
tional Conference on Learning Representations, 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures
for scalable image recognition. In Conference on Computer Vision and Pattern Recognition, 2018.
12
Published as a conference paper at ICLR 2020
A More detail on DARTS
Here we present a detailed description of DARTS architectural update steps. We firstly provide
the general formalism which computes the gradient of the outer level problem in (1) by means of
the implicit function theorem. Afterwards, we present how DARTS computes the gradient used to
update the architectural parameters α.
A.1 Derivative with smoothed non-quadratic lower level problem
Consider the general definition of the bi-level optimization problem as given by (1) and (2). Given
that f is twice continuously differentiable and that all stationary points are local minimas, one
can make use of the implicit function theorem to find the derivative of the solution map θ*(y)
w.r.t. y (Bengio, 2000). Under the smoothness assumption, the optimality condition of the lower
level (2) is Vθf (y,θ) = 0, which defines an implicit function θ*(y). With the assumption that
minθ f (y, θ) has a solution, there exists a (y, θ*) such that V f (y, θ*) = 0. Under the condition
that Vθf (y, θ*) = 0 is continuously differentiable and that θ*(y) is continuously differentiable at
y, implicitly differentiating the last equality from both sides w.r.t. y and applying the chain rule,
yields:
∂(Vθ f)	∂θ*	∂(Vθ f)
-wy (y,θ) ∙西(y) + ~wy (y,θ) = 0.
Assuming that the Hessian Vθf (y, θ*) is invertible, We can rewrite (3) as follows:
箸(y) = -(v2f(y,θ*))-1∙联(y,θ*).
Applying the chain rule to (1) for computing the total derivative of F with respect to y yields:
dF _ ∂F ∂θ* ∂F
dy ∂θ ∂y + ∂y
where we have omitted the evaluation at (y, θ*). Substituting (4) into (5) and reordering yields:
dFF = dF - IF ∙ (v2f)	∙ ∂θ∂y.
(3)
(4)
(5)
(6)
equation 6 computes the gradient of F, given the function θ*(y), which maps outer variables to the
inner variables minimizing the inner problem. However, in most of the cases obtaining such a map-
ping is computationally expensive, therefore different heuristics have been proposed to approximate
dF /dy (Maclaurin et al., 2015; Pedregosa, 2016; Franceschi et al., 2017; 2018).
A.2 DARTS architectural gradient computation
DARTS optimization procedure is defined as a bi-level optimization problem where Lvalid is the
outer objective (1) and Ltrain is the inner objective (2):
min Lvalid(α,w*(α))	(7)
α
s.t. w*(α) = arg min Ltrain(α,w)	(8)
w
where both losses are determined by both the architecture parameters α (outer variables) and the
network weights w (inner variables). Based on Appendix A.1, under some conditions, the total
derivative of Lvalid w.r.t. a evaluated on (α, w* (α)) would be:
-dα'id = V aLvalid - Vw Lvalid(Vw Ltrain)	vIwL
train,
(9)
where Va = 急,Vw = ∂w and V^w = ∂⅛w ∙ Computing the inverse of the Hessian is in general
not possible considering the high dimensionality of the model parameters w, therefore resolving to
gradient-based iterative algorithms for finding w* is necessary. However, this would also require to
13
Published as a conference paper at ICLR 2020
optimize the model parameters w till convergence each time α is updated. If our model is a deep
neural network it is clear that this computation is expensive, therefore Liu et al. (2019) propose
to approximate w*(α) by updating the current model parameters W using a single gradient descent
step:
w*(α) ≈ W - ξVwLtrain(α,w)	(10)
where ξ is the learning rate for the virtual gradient step DARTS takes with respect to the model
weights w. From equation 10 the gradient of w*(α) with respect to α is
dw* , 一、 P2 C ,一、
^∂α (α) = -ξvα,w Ltrain (α,w),
(11)
By setting the evaluation point w* = W - ξVwLtrain(α, W) and following the same derivation as
in Appendix A.1, we obtain the DARTS architectural gradient approximation:
dLvaid (α) = VLvalid(α, w*) - ξVwLvalid(α, W*)VVw Ltrain(α, W*),	(12)
dα	α,w
where the inverse Hessian V2w Lt-r1ain in (9) is replaced by the learning rate ξ. This expression
however contains again an expensive vector-matrix product. Liu et al. (2019) reduce the complexity
by using the finite difference approximation around W± = W ± Vw Lvalid(α, W*) for some small
= 0.01/ kVwLvalid(α, W*)k2 to compute the gradient of VαLtrain (α, W*) with respect to W as
Vα,w Ltrain (α, W ) ≈
VαLtrain (α, w^^ ) - VαLtrain (α, w )
2eVwLvaIid(α, w*
⇔
Vw Lvaiid(α,W*)Vα,wLtrain(α,W*) ≈ “ "Ltang^++ -J °Lta"g'优1 .	(13)
In the end, combining equation 12 and equation 13 gives the gradient to compute the architectural
updates in DARTS:
—va^ ( (α) = IVaLValid(α, w*) -"弋VaLtrain (α, w+) - V aLtrain(α, W ))	(14)
dα	2
In all our experiments we always use ξ = η (also called second order approximation in Liu et al.
(2019)), where η is the learning rate used in SGD for updating the parameters W.
B	Construction of S1 from Section 3
We ran DARTS two times on the default search space to find the two most important operations per
mixed operation. Initially, every mixed operation consists of 8 operations. After the first DARTS
run, we drop the 4 (out of 8) least important ones. In the second DARTS run, we drop the 2 (out
of the remaining 4) least important ones. S1 is then defined to contain only the two remaining most
important operations per mixed op. Refer to Figure 9 for an illustration of this pre-optimized space.
C Final Architecture Evaluation
Similar to the original DARTS paper (Liu et al., 2019), the architecture found during the search are
scaled up by increasing the number of filters and cells and retrained from scratch to obtain the final
test performance. For CIFAR-100 and SVHN we use 16 number of initial filters and 8 cells when
training architectures from scratch for all the experiments we conduct. The rest of the settings is the
same as in Liu et al. (2019).
On CIFAR-10, when scaling the ScheduledDropPath drop probability, we use the same settings for
training from scratch the found architectures as in the original DARTS paper, i.e. 36 initial filters
and 20 stacked cells. However, for search space S2 and S4 we reduce the number of initial filters
to 16 in order to avoid memory issues, since the cells found with more regularization usually are
composed only with separable convolutions. When scaling the L2 factor on CIFAR-10 experiments
we use 16 initial filters and 8 stacked cells, except the experiments on S1, where the settings are the
same as in Liu et al. (2019), i.e. 36 initial filters and 20 stacked cells.
14
Published as a conference paper at ICLR 2020
Figure 9: Search space S1.
Note that although altering the regularization factors during DARTS search, when training the final
architectures from scratch we always use the same values for them as in Liu et al. (2019), i.e.
ScheduledDropPath maximum drop probability linearly increases from 0 towards 0.2 throughout
training, Cutout is always enabled with cutout probability 1.0, and the L2 regularization factor is set
to 3 ∙ 10-4.
15
Published as a conference paper at ICLR 2020
D Additional empirical results
{%) ⅛uσe ⅛WH
:
⅛ factor: 0.0009	I2 factor: 0.0018
----DARTStest negnst
----DARTS one-shot val. error
----RS-wstest regret
(s)-jaJβaJ"3J.
0	10	20	30	40	50
Search epoch
60
50
40
30
20
-10
0	10	20	30	40	50
Search epoch
(srωJ9 uo=es->
L2 factor： 0.0027
L2 factor： 0.0081
L2 factor： 0.0243
O
O
0	IO	20	30	40
Search epoch
50	0	10	20	30	40	50
Search epoch
4Q
20	30
Search epoch
10
Figure 10: Test regret and validation error of the search (one-shot) model when running DARTS
on S5 and CIFAR-10 with different L2 regularization values. The architectural parameters’ overfit
reduces as we increase the L2 factor and successfully finds the global minimum. However, we notice
that the architectural parameters start underfitting as we increase to much the L2 factor, i.e. both
validation and test error increase.
(％) JEB uo=ep=e>
∞4mmw
..........................
Table 5: Validation (train) and test accuracy on CIFAR-10 of the search and final evaluation models,
respectively. The values in the last column show the maximum eigenvalue λαmax (computed on a
random sampled mini-batch) of the Hessian, at the end of search for different maximum drop path
probability). The four blocks in the table state results for the search spaces S1-S4, respectively.
Drop Prob.		seed 1	Valid acc. seed 2	seed 3	seed 1	Test acc. seed 2	seed 3	seed 1	Params seed 2	seed 3	seed 1	λα max seed 2	seed 3
	0.0	87.22	87.01	86.98	96.16	94.43	95.43	2.24	1.93	2.03	1.023	0.835	0.698
S1	0.2	84.24	84.32	84.22	96.39	96.66	96.20	2.63	2.84	2.48	0.148	0.264	0.228
	0.4	82.28	82.18	82.79	96.44	96.94	96.76	2.63	2.99	3.17	0.192	0.199	0.149
	0.6	79.17	79.18	78.84	96.89	96.93	96.96	3.38	3.02	3.17	0.300	0.255	0.256
	0.0	88.49	88.40	88.35	95.15	95.48	96.11	0.93	0.86	0.97	0.684	0.409	0.268
S2	0.2	85.29	84.81	85.36	95.15	95.40	96.14	1.28	1.44	1.36	0.270	0.217	0.145
	0.4	82.03	82.66	83.20	96.34	96.50	96.44	1.28	1.28	1.36	0.304	0.411	0.282
	0.6	79.86	80.19	79.70	96.52	96.35	96.29	1.21	1.28	1.36	0.292	0.295	0.281
	0.0	88.78	89.15	88.67	94.70	96.27	96.66	2.21	2.43	2.85	0.496	0.535	0.446
S3	0.2	85.61	85.60	85.50	96.78	96.84	96.74	3.62	4.04	2.99	0.179	0.185	0.202
	0.4	83.03	83.24	83.43	97.07	96.85	96.48	4.10	3.74	3.38	0.156	0.370	0.184
	0.6	79.86	80.03	79.68	96.91	94.56	96.44	4.46	2.30	2.66	0.239	0.275	0.280
	0.0	86.33	86.72	86.46	92.80	93.22	93.14	1.05	1.13	1.05	0.400	0.442	0.314
S4	0.2	81.01	82.43	82.03	95.84	96.08	96.15	1.44	1.44	1.44	0.070	0.054	0.079
	0.4	79.49	79.67	78.96	96.11	96.30	96.28	1.44	1.44	1.44	0.064	0.057	0.049
	0.6	74.54	74.74	74.37	96.42	96.36	96.64	1.44	1.44	1.44	0.057	0.060	0.066
D. 1 Adaptive DARTS details
We evaluated DARTS-ADA (Section 5.3) with R = 3 ∙ 10-4 (DARTS default), Rmax = 3 ∙ 10-2
and η = 10 on all the search spaces and datasets we use for image classification. The results are
shown in Table 3 (DARTS-ADA). The function train_and_eval conducts the normal DARTS
search for one epoch and returns the architecture at the end of that epoch’s updates and the stop
value if a decision was made to stop the search and rollback to stop_epoch.
16
Published as a conference paper at ICLR 2020
Algorithm 1: DARTS _ADA
/* E: epochs to search; R: initial regularization value; Rmax : maximal
regularization value; stop_criter: stopping criterion; η:
regularization increase factor	*/
Input : E, R, Rmax, Stop_Criter, η
/* start search for E epochs	*/
for epoch in E do
/* run DARTS for one epoch and return stop=True together with the
stop_epoch	*/
/* and the architecture at stop_epoch if the criterion is met	*/
stop, stop_epoch, arch — train_and_eval(stop-Criter)；
if stop & R ≤ Rmax then
/* start DARTS from stop_epoch with a larger R	*/
arch — DARTS_ADA(E - stop_epoch, η ∙ R, Rmax, stop_criter, η)；
break
end
end
Output: arch
Sl ClO
S2 ClO	S3 ClO	S4 ClO
f%)」CUJ,υa0i1-
16
14
Figure 11: Test errors of architectures along with the validation error of the search (one-shot) model
for each dataset and space when scaling the ScheduledDropPath drop probability. Note that these
results (blue lines) are the same as the ones in Figure 8.
Test error
One-shotval.
0.0 0.1 0.2 0.3 0.4 0.5 0.6	0.0 0.1 0.2 0.3 0.4 0.5 0.6	0.0 0.1 0.2 0.3 0.4 0.5 0.6	0.0 0.1 0.2 0.3 0.4	0.5 0.6
DP max. prob.	DP max. prob.	DP max. prob.	DP max. prob.
17
Published as a conference paper at ICLR 2020
(％)s81-
Sl ClOO
S2 ClOO	S3 ClOO	S4 ClOO
(％)」CUJ,υa0i1-
L2 factor
One-shot Val. error (%)	One-shot Val. error (%)	One-shot Val. error (%)
Figure 12:	Test errors of architectures along with the validation error of the search (one-shot) model
for each dataset and space when scaling the L2 factor. Note that these results (blue lines) are the
same as the ones in Figure 7.
18
Published as a conference paper at ICLR 2020
O IO 20	30	40	50
Epoch
O 10	20	30	40	50
Epoch
O 10	20	30	40	50
Epoch
O 10	20	30	40	50
Epoch
<E 9n->u96ffi.Xew
<E 9n-Λuθβi=.Xew
O 10	20	30	40	50
Epoch
O 10	20	30	40	50
Epoch
O 10	20	30	40	50
Epoch
O 10	20	30	40	50
Epoch
Figure 13:	Local average of the dominant EV λαmax throughout DARTS search (for different drop
path prob. values). Markers denote the early stopping point based on the criterion in Section 4.3.
Figure 14: Effect of L2 regularization no the EV trajectory. The figure is analogous to Figure 13.
19
Published as a conference paper at ICLR 2020
Eigen, distribution： S2 clfarlθ	Eigen, distribution： S3 ClfarlO	Eigen, distribution： 54 clferlθ
Eigen, distribution： Sl ClfarlOO
Eigen, distribution： S2 ClfarlOO	Eigen, distribution： S3 ClfarlOO	Eigen, distribution： 54 ClfarlOO
Figure 15: Effect of ScheduledDropPath and Cutout on the full eigenspectrum of the Hessian at the
end of architecture search for each of the search spaces. Since most of the eigenvalues after the 30-th
largest one are almost zero, we plot only the largest (based on magnitude) 30 eigenvalues here. We
also provide the eigenvalue distribution for these 30 eigenvalues. Notice that not only the dominant
eigenvalue is larger when dp = 0 but in general also the others.
20
Published as a conference paper at ICLR 2020
Elgenspectrum: 52 svhn
0.35-1------------------------------
0.30-
0.25-
0.20-
0.15-
0-10-
0.05-
0.00-
-0.05-
0	10	20	30
l-th eigenvalue
Eigen, distribution： 52 CtfarlO
Eigen, distribution： Sl ClfarlO
Eigen, distribution： S2 ClfarlOO
Eigen, distribution： S3 ClfarlOO
Eigen, distribution： 54 ClfarlOO
Figure 16: Effect of L2 regularization on the full eigenspectrum of the Hessian at the end of archi-
tecture search for each of the search spaces. Since most of the eigenvalues after the 30-th largest one
are almost zero, we plot only the largest (based on magnitude) 30 eigenvalues here. We also provide
the eigenvalue distribution for these 30 eigenvalues. Notice that not only the dominant eigenvalue is
larger When L? = 3 ∙ 10-4 but in general also the others.
21
Published as a conference paper at ICLR 2020
8 6 4 2
(％) dojp AuJnUUIIoqp-->
0	.
0.0
Max. Drop probability
Figure 17: Drop in accuracy after discretizing the search model for different spaces, datasets and
drop path regularization strengths.. Example of some of the settings from Section 5.
More L2 regularization
(求)」0」」04jsωl
0.0003	0.0009	0.0027	0.0081	0.0243	0.0729	0.2187
L2 value
More DropPath regularization
(求二 oxlω4jsωl
0.0	0.2	0.4	0.6	0.8
Drop prob, value
Figure 18: Effect of more regularization on the performance of found architectures by DARTS.
Table 6: Performance of architectures found by DARTS (-ES / -ADA) vs. RandomNAS with weight
sharing. For each of the settings we repeat the search 3 times and report the mean ± std of the 3
found architectures retrained from scratch.
Setting		RandomNAS	DARTS	DARTS-ES	DARTS-ADA
C10	S1	3.17 ± 0.lh	4.66 ± 0.71=	3.05 ± 0.07 =	3.03 ± 0.08=
	S2	3.46 ± 0.15	4.42 ± 0.40	3.41 ± 0.14	3.59 ± 0.31
	S3	2.92 ± 0.04	4.12 ± 0.85	3.71 ± 1.14	2.99 ± 0.34
	S4	89.39 ± 0.84	6.95 ± 0.18	4.17 ± 0.21	3.89 ± 0.67
C100	S1	25.81 ± 0.39=	29.93 ± 0.4Γ	28.90 ± 0.81=	24.94 ± 0.81=
	S2	22.88 ± 0.16	28.75 ± 0.92	24.68 ± 1.43	26.88 ± 1.11
	S3	24.58 ± 0.61	29.01 ± 0.24	26.99 ± 1.79	24.55 ± 0.63
	S4	30.01 ± 1.52	24.77 ± 1.51	23.90 ± 2.01	23.66 ± 0.90
SVHN	S1	2.64 ± 0.09=	9.88 ± 5.5O=	2.80 ± 0.09 =	2.59 ± 0.07=
	S2	2.57 ± 0.04	3.69 ± 0.12	2.68 ± 0.18	2.79 ± 0.22
	S3	2.89 ± 0.09	4.00 ± 1.01	2.78 ± 0.29	2.58 ± 0.07
	S4	3.42 ± 0.0厂	2.90 ± 0.0展	2.55 ± 0.15~~	2.52 ± 0.06~~
22
Published as a conference paper at ICLR 2020
D.2 A closer look at the eigenvalues
Over the course of all experiments from the paper, we tracked the largest eigenvalue across all
configuration and datasets to see how they evolve during the search. Figures 13 and 14 shows the
results across all the settings for image classification. It can be clearly seen that increasing the inner
objective regularization, both in terms of L2 or data augmentation, helps controlling the largest
eigenvalue and keeping it to a small value, which again helps explaining why the architectures
found with stronger regularization generalize better. The markers on each line highlight the epochs
where DARTS is early stopped. As one can see from Figure 4, there is indeed some correlation
between the average dominant eigenvalue throughout the search and the test performance of the
found architectures by DARTS.
Figures 15 and 16 (top 3 rows) show the full spectrum (sorted based on eigenvalue absolute values)
at the end of search, whilst bottom 3 rows plot the distribution of eigenvalues in the eigenspectrum.
As one can see, not only the dominant eigenvalue is larger compared to the cases when the regu-
larization is stronger and the generalization of architectures is better, but also the other eigenvalues
in the spectrum have larger absolute value, indicating a sharper objective landscape towards many
dimensions. Furthermore, from the distribution plots note the presence of more negative eigenvalues
whenever the architectures are degenerate (lower regularization value) indicating that DARTS gets
stuck in a point with larger positive and negative curvature of the validation loss objective, associated
with a more degenerate Hessian matrix.
E	Disparity Estimation
E.1 Datasets
We use the FlyingThings3D dataset (Mayer et al., 2016) for training AutoDispNet. It consists of
rendered stereo image pairs and their ground truth disparity maps. The dataset provides a training
and testing split consisting of 21, 818 and 4248 samples respectively with an image resolution of
960 × 540. We use the Sintel dataset ( Butler et al. (2012)) for testing our networks. Sintel is another
synthetic dataset from derived from an animated movie which also provides ground truth disparity
maps (1064 samples) with a resolution of 1024 × 436.
E.2 Training
We use the AutoDispNet-C architecture as described in Saikia et al. (2019). However, we use
the smaller search which consists of three operations: M axP ool3 × 3, S epC onv3 × 3, and
SkipConnect. For training the search network, images are downsampled by a factor of two and
trained for 300k mini-batch iterations. During search, we use SGD and ADAM to optimize the
inner and outer objectives respectively. Differently from the original AutoDispNet we do not warm-
start the search model weights before starting the architectural parameter updates. The extracted
network is also trained for 300k mini-batch iterations but full resolution images are used. Here,
ADAM is used for optimization and the learning rate is annealed to 0 from 1e - 4, using a cosine
decay schedule.
E.3 Effect of regularization on the inner objective
To study the effect of regularization on the inner objective for AutoDispNet-C we use experiment
with two types of regularization: data augmentation and of L2 regularization on network weights.
We note that we could not test the early stopping method on AutoDispNet since AutoDispNet relies
on custom operations to compute feature map correlation (Dosovitskiy et al., 2015) and resampling,
for which second order derivatives are currently not available (which are required to compute the
Hessian).
Data augmentation. Inspite of fairly large number of training samples in FlyingThings3D, data
augmentation is crucial for good generalization performance. Disparity estimation networks employ
spatial transformations such as translation, cropping, shearing and scaling. Additionally, appearance
transformations such as additive Gaussian noise, changes in brightness, contrast, gamma and color
23
Published as a conference paper at ICLR 2020
are also applied. Parameters for such transformations are sampled from a uniform or Gaussian distri-
bution (parameterized by a mean and variance). In our experiments, we vary the data augmentation
strength by multiplying the variance of these parameter distributions by a fixed factor, which we dub
the augmentation scaling factor. The extracted networks are evaluated with the same augmentation
parameters. The results of increasing the augmentation strength of the inner objective can be seen
in Table 2. We observe that as augmentation strength increases DARTS finds networks with more
number of parameters and better test performance. The best test performance is obtained for the
network with maximum augmentation for the inner objective. At the same time the search model
validation error increases when scaling up the augmentation factor, which again enforces the argu-
ment that the overfitting of architectural parameters is reduced by this implicit regularizer.
L2 regularization. We study the effect of increasing regularization strength on the weights of the
network. The results are shown in Table 2. Also in this case best test performance is obtained with
the maximum regularization strength.
F Results on Penn Treebank
Here we investigate the effect of more L2 regularization on the inner objective for searching re-
current cells on Penn Treebank (PTB). We again used a reduced search space with only ReLU and
identity mapping as possible operations. The rest of the settings is the same as in (Liu et al., 2019).
We run DARTS search four independent times with different random seeds, each with four L2
regularization factors, namely 5 × 10-7 (DARTS default), 15 × 10-7, 45 × 10-7 and 135 × 10-7.
Figure 19 shows the test perplexity of the architectures found by DARTS with the aforementioned
L2 regularization values. As we can see, a stronger regularization factor on the inner objective makes
the search procedure more robust. The median perplexity of the discovered architectures gets better
as we increase the L2 factor from 5×10-7 to 45×10-7, while the search model (one-shot) validation
mean perplexity increases. This observation is similar to the ones on image classification shown in
Figure 10, showing again that properly regularizing the inner objective helps reduce overfitting the
architectural parameters.
Effect of more L2 regularization on generalization
Figure 19: Performance of recurrent cells found with different L2 regularization factors on the inner
objective on PTB. We run DARTS 4 independent times with different random seeds, train each of
them from scratch with the evaluation settings for 1600 epochs and report the median test perplexity.
The blue dashed line denotes the validation perplexity of the search model.
24
Published as a conference paper at ICLR 2020
G Discovered cells on search spaces S1-S4 from Section 3 on
OTHER DATASETS
max pool 3x3
dil conv 3x3
max pool 3x3
Sep C(IIv 3x3
sep_conv 3x3
skp_c<oinect
skp_c(omect
skip come。
Se cfoιv 3x;
Sep connv 3x3
Sep coιv
skip connect
:ep conv 3x3
*ip conned
:{k-2]
*ip conned
:ep conv 3x3
:{k-1!
:ep conv 3x3
:ep conv 3x3
1
:ep conv 3x3
nv 3x3
ep_
nv 3x3
C {k-1;
ep
nv 3x3
ep
nv 3x3
ep
nv 3x3
ep
c {k-2;
nv 3x3
ep
ep_conv_3x3






(a) S1	(b) S2	(c) S3	(d) S4
Figure 20:	Reduction cells found by DARTS when ran on CIFAR-10 with its default hyperparame-
ters on spaces S1-S4. These cells correspond with the normal ones in Figure 1.
skip connect
*ip CCnnec
*ip Connec
*ip conned
*ip conned
skip connect
*ip connecl
skip_connect
(b) S2 (C100)
(d) S4 (C100)



(a) S1 (C100)
(c) S3 (C100)
skip connect
*ip connecl
skip conn
skip connect
skip connect
skip connecl
Ikip c
skip Ctonlect
5k» collect
skip conned
skip cioaied
skip domed
skip Conied
skip Conied
IP C3"'
(f) S2 (SVHN)
(h) S4 (SVHN)


(e) S1 (SVHN)
(g) S3 (SVHN)
Figure 21:	Normal cells found by DARTS on CIFAR-100 and SVHN when ran with its default
hyperparameters on spaces S1-S4. Notice the dominance of parameter-less operations such as skip
connection and pooling ops.
____________avg _pool_3x3
E*-2}∣‰ _pool_3<3_ ~Z	dil-ccotv-5,5
_______ max_pool 3x3
∣c,!k-1}∣==--------^
5ep⅛⅛-⅜cov3x3.
IU ctoiv 5x5
IU C(OIV 5X5
sep_conv_3x 3
Sep CIonv_3 3
SeP-CθnV-3x 3	----------.
skip cornect
Sktl .ctaa^ect	_ ,—
-----Zi	sep_con'_3X33 2
j⅛⅛n⅛>-⅞j;-CtTOect 3-
(a)	S1 (C100)
(b)	S2 (C100)
Sep conv 3x3
:ep conv 3x3
1
:ep conv 3x3
:ep conv 3x3
:ep conv 3x3
:{k-2]
:ep conv 3x3
*ip connecl
:{k-1!
:ep conv 3x3
nv 3x3
ep
nv 3x3
C {k-1;
ep
nv 3x3
ep
nv 3x3
ep
nv 3x3
ep
c {k-2;
nv 3x3
ep
P COnV 3x3
(d)	S4 (C100)
(c) S3 (C100)
avg pool 3x3
max pool 3x3
skip clamed
skip ciomed
max pool 3x;
a,g pool 3x;
max pool 3x3
cjk-1)
:{k-2}
max pool 3x;
Skip-Connlect
却 conv 3x3
kp co∣nιιec
skip connect
sep conv 3x3
skip connect
sep conv 3x3
skip connect
Sk»_conn”
skip_coniec
s>»_connec
sep_coiw_3x3
SriBjconneC
Skip-Conred
Skm conned
(e)	S1 (SVHN)
(f)	S2 (SVHN)
(g)	S3 (SVHN)
Sep conv 3x3
Sep conv 3x3
2
Sep conv 3x3
sep conv 3x3
sep conv 3x3
c {k-2}
sep conv 3x3
sep conv 3x3
c {k-1}
sep conv 3x3



0
F
1





(h) S4 (SVHN)
Figure 22:	Reduction cells found by DARTS on CIFAR-100 and SVHN when ran with its default
hyperparameters on spaces S1-S4.
25
Published as a conference paper at ICLR 2020
skip_connect
da COIV 3X3
ikap_conre«
(a) S1 (C10)	(b) S2 (C10)
skip_connect
0
3
Ic_{k}
2
1
Skip-ConneCt,
Skip-ConnecI,
S ∕k^ Skip-Connecv
EΞ{k¾kSonne^
∕∖ Skip-Connect.
I c」；，} N 誓pconnect
sep conv 3x3
(d) S4 (C10)
(c) S3 (C10)
max_pool_3x3
_«njXect
dil conv 3x3
dil conv 3x3
C {k-1}
_«njXect
c {k-2}
Skp-CCnnect
Skip_connect
(e) S1 (C100)	(f) S2 (C100)
Sep conv 3x3
2
Sep conv 3x3
Sep conv 3x3
Sep conv 3x3
Sep conv 3x3
sep conv 3x3
sep conv 3x3
(g)	S3 (C100)
(h)	S4 (C100)
sep_conv_3x3
⅛4n "e"",
dil Conv ”
Iil CCnv 3X3
dl CaLV 5X5
p_ccnneci
skp Connect
Mp-Onnect
sep conv 3x3
c {k-1}
sep conv 3x3
sep Ccnv 3x3
c {k-2}
∣j{k∙21-
|c_{>-i}|
sep
conv
3x^3'
sep
conv
3x¾
<Jep_
conv
3x3
sep
conv
3x3)
conv
3x3
Xsep
conv
3x37
I J{k}
(i)	S1 (SVHN)
(j)	S2 (SVHN)
∙⅞?
conv
3x3,
(k)	S3 (SVHN)
(l)	S4 (SVHN)
∖Jeψ
0
B

1
Figure 23:	Normal cells found by DARTS-ES when ran with DARTS default hyperparameters on
spaces S1-S4.
m ax_poool_3x3
m ax_pool_3x3
mmg_ m ax_poool_3x3_J
I c_{k-2} I ∕mαχipooiz3x⅞
∕∖ m ax_po*ol_3x3^
"{kyl∖mx⅛X3
|c_{k}
m ax_poool_3x3
Xm ax_poool_3x3
I J{k-2}
sep_c onv_3x3
c_{k}
SkiP-ConneCt
sep_conv_3x3.
2
c-{k-1}
sep_c onv_3x3
sep_c onv_3x3
1「skip_c onnect
` I SkiP-Connect^ 3
(a)	S1 (C10)
(b)	S2 (C10)
skip_connect
sep_conv_3x3
sep_conv_3x3
2
1
sep_conv_3x3
sep_conv_3x3
c_{ k-2}
sep_conv_3x3
sep_conv_3x3
c_{ k-1}
sep_conv_3x3
Sep-Conv_3x3
Sep-Conv_3x3
0
Sep-Conv_3x3
J{k2
Sep-Conv_3x3
3
2
Sep_Conv_3x3
Sep_Conv_3x3
Sep_Conv_3x3
J{k-1}
Sep_Conv_3x3
(c)	S3 (C10)
(d)	S4 (C10)
max_pool3x3
sep conv 3x3
I I sep conv 3x：
skip connect
skip connect
Sep conv ：x：
SeP conv 3x：
SeP conv 3x：
(e)	S1 (C100)
s,p_connect
Sep Ccnv 3x3
skip connect
Sep Conv 3x3
Sep conv 3X3
Sep conv 3X3
SKip connect
sep conv 3x3
sep conv 3x3
sep conv 3x3
sep conv 3x3
sep_conv_3x:
sep conv 3x3
sep_conv_3x3
(f)	S2 (C100)
(g)	S3 (C100)
(h)	S4 (C100)
max_pool_3x3_____________
da Cov 5x5
c_{k-2} max_pool_3x3
nax_pool_3X3
naxL_pool_3X3
Ia conv 5x5
sp_conv_3x3
,jp_env_313
naxL,pooL3X3
conv_3x3
sep_conv_3X3
sep conv 3x3
(i)	S1 (SVHN)
(j)	S2 (SVHN)	(k) S3 (SVHN)
Sep-Conv 3x3
3x3
seP
J N1J
3x3
seP
J N1J
3x3
seP
J N1J
3x3
seP
J N1J
2
3x3
seP
J N1J
3x3
Sep
J N1J
3x3
Sep
J N1J
0
3
2
1







(l) S4 (SVHN)
Figure 24:	Reduction cells found by DARTS-ES when ran with DARTS default hyperparameters on
spaces S1-S4.
26
Published as a conference paper at ICLR 2020
_Conv_3x3
skip Connect
sep_conv_3x3
Sepconv3X3
Sepconv 3X3
(b) Reduction cell
rnai_pool 3x3
sep Conv 3x;
C {k-2]
Sep C(oιv 3x3
max_pool_3X3
C {k-1!
max pool 3x3
Sep Ctoiv 3x3
Sep Ctoiv 3x3
sep C(oιv 3x3
(c) Upsampling cell
Figure 25:	Cells found by AutoDispNet when ran on S6-d. These cells correspond to the results for
augmentation scale 0.0 of Table 2.
I C {k-2} I-------------skip---------------------
I	L sk∙b-Cf∣rπ⅛ect	sep-C non∙-3x3
|c_{k-1}	Sep-Con■二邑
sep c toiv 3x3
sep_c onv_3x3
(b) Reduction cell
c_{k}
，ep conv 3x3
:{k-2}
sep Conv 3x3
sep conv 3x
卜一*»卜
c {k-l}
s。 conv 3x;
:Hed {k-1}
，。conv 3x
skip conned
(c) Upsampling cell
0
1
2
Figure 26:	Cells found by AutoDispNet when ran on S6-d. These cells correspond to the results for
augmentation scale 2.0 of Table 2.
(a) Normal cell
(b) Reduction cell
["k-1” ,e—3x3
:{k-2}
Sep Cmv 3x3
Sep Cmv 3x3
Skap ComeCt
C skip
Skap Comect
(c) Upsampling cell
∣Γik1i	s‰convjχ
I ' I	S kipjcoι0ι⅛ct
Figure 27:	Cells found by AutoDispNet when ran on S6-d. These cells correspond to the results for
L2 = 3 ∙ 10-4 of Table 2.
sep_conv_3x3
Sep conv 3x3
Sep conv 3x3
sepconv3x3
sep_coniv_3x3
(a) Normal cell
(b) Reduction cell
sep conv 3x;
SeP conv 3x3
(c) Upsampling cell
Figure 28:	Cells found by AutoDispNet when ran on S6-d. These cells correspond to the results for
L2 = 81 ∙ 10-4 ofTable 2.
27
Published as a conference paper at ICLR 2020
skip_connect
0
skip_connect
c_{k-2}
skip_connect
3
c_{k}
skip_connecV
2
I c_{k-1}
skip_connect
Skip-Connect，-
--------------ɪflɪ
Skip-Connect J'-π-
noise
Sep conv 3x3
(d) S4
(c) S3
max pool 3x3
max pool 3x3
max pool 3x3
2
max pool 3x3
max pool 3x3
dil conv 5x5
max pool 3x3
max_pool_3x3
(f) S2
(g) S3
Sep conv 3x3
Sep conv 3x3
2
Sep conv 3x3
sep conv 3x3
sep conv 3x3
c {k-2}
sep conv 3x3
sep conv 3x3
c {k-1}
sep conv 3x3



(e) S1
(h) S4
Figure 29:	Normal (top row) and reduction (bottom) cells found by DARTS on CIFAR-10 when ran
with its default hyperparameters on spaces S1-S4. Same as Figure 1 but with different random seed
(seed 2).
skip_cotneict
∣j{k-2}卜
skip_connect
dil Conv 3x
skip corned
skip correct skip Conrnect
Skip-Clomect
Skp Cene-t
sep_conv_3x3
sep_conv_3
Xip connect
∣J{N}∣
XJep_aov_»3i
p-conιe∣ct
(a) S1
(b) S2
skip_connect
0
skip_connect
c_{k-2}
skip_connect
3
c_{k}
skip_connect
2
c_{k-1}
sep_conv_3x3
skip_connect
1
Skip_Connect J'-π-
C {k-1} L Sep-Conv_3x3
Sep Conv 3x3
Sep conv 3x3
C-∣k-2!
(c) S3
(d) S4
Ccnnec
max pool_3x3
max pod_3x3
skip ConXeC
max pool 3x
sep_conv_3x3
-e-:
.conv 3x3
-e-:
.conv 3x3
-e-:
.conv 3x3
-e-:
.conv 3x3
conv 3x3
.conv 3x3
-e-:
(e) S1
(f) S2
(g) S3
(h) S4


Figure 30:	Normal (top row) and reduction (bottom) cells found by DARTS on CIFAR-10 when ran
with its default hyperparameters on spaces S1-S4. Same as Figure 1 but with different random seed
(seed 3).
28