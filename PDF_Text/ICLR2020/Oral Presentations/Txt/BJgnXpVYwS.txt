Published as a conference paper at ICLR 2020
Why gradient clipping accelerates training:
A theoretical justification for adaptivity
Jingzhao Zhang, Tianxing He, Suvrit Sra & Ali Jadbabaie
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{jzhzhang, tianxing, suvrit, jadbabai}@mit.edu
Ab stract
We provide a theoretical explanation for the effectiveness of gradient clipping in
training deep neural networks. The key ingredient is a new smoothness condition
derived from practical neural network training examples. We observe that gradient
smoothness, a concept central to the analysis of first-order optimization algorithms
that is often assumed to be a constant, demonstrates significant variability along
the training trajectory of deep neural networks. Further, this smoothness posi-
tively correlates with the gradient norm, and contrary to standard assumptions in
the literature, it can grow with the norm of the gradient. These empirical observa-
tions limit the applicability of existing theoretical analyses of algorithms that rely
on a fixed bound on smoothness. These observations motivate us to introduce a
novel relaxation of gradient smoothness that is weaker than the commonly used
Lipschitz smoothness assumption. Under the new condition, we prove that two
popular methods, namely, gradient clipping and normalized gradient, converge
arbitrarily faster than gradient descent with fixed stepsize. We further explain why
such adaptively scaled gradient methods can accelerate empirical convergence and
verify our results empirically in popular neural network training settings.
1	Introduction
We study optimization algorithms for neural network training and aim to resolve the mystery of
why adaptive methods converge fast. Specifically, we study gradient-based methods for minimizing
a differentiable nonconvex function f : Rd → R, where f (x) can potentially be stochastic, i.e.,
f (x) = Eξ [F (x, ξ)]. Such choices of f cover a wide range of problems in machine learning, and
their study motivates a vast body of current optimization literature.
A widely used (and canonical) approach for minimizing f is the (stochastic) gradient descent (GD)
algorithm. Despite its simple form, GD often achieves superior empirical (Wilson et al., 2017)
performances and theoretical (Carmon et al., 2017) guarantees. However, in many tasks such as
reinforcement learning and natural language processing (NLP), adaptive gradient methods (e.g.,
Adagrad (Duchi et al., 2011), ADAM (Kingma and Ba, 2014), and RMSProp (Tieleman and Hinton,
2012)) outperform SGD. Despite their superior empirical performance, our understanding of the fast
convergence of adaptive methods is limited. Previous analysis has shown that adaptive methods are
more robust to variation in hyper-parameters (Ward et al., 2018) and adapt to sparse gradients (Duchi
et al., 2011) (a more detailed literature review is in Appendix A). However, in practice, the gradient
updates are dense, and even after extensively tuning the SGD hyperparameters, it still converges
much slower than adaptive methods in NLP tasks.
We analyze the convergence of clipped gradient descent and provide an explanation for its fast con-
vergence. Even though gradient clipping is a standard practice in tasks such as language models (e.g.
Merity et al., 2018; Gehring et al., 2017; Peters et al., 2018), it lacks a firm theoretical grounding.
Goodfellow et al. (2016); Pascanu et al. (2013; 2012) discuss the gradient explosion problem in re-
current models and consider clipping as an intuitive work around. We formalize this intuition and
prove that clipped GD can converge arbitrarily faster than fixed-step gradient descent. This result is
shown to hold under a novel smoothness condition that is strictly weaker than the standard Lipschitz-
gradient assumption pervasive in the literature. Hence our analysis captures many functions that are
not globally Lipschitz smooth. Importantly, the proposed smoothness condition is derived on the
1
Published as a conference paper at ICLR 2020
basis of extensive NLP training experiments, which are precisely the same type of experiments for
which adaptive gradient methods empirically perform superior to gradient methods.
By identifying a a new smoothness condition through experiments and then using it to analyze
the convergence of adaptively-scaled methods, we reduce the following gap between theory and
practice. On one hand, powerful techniques such as Nesterov’s momentum and variance reduction
theoretically accelerate convex and nonconvex optimization. But, at least for now, they seem to
have limited applicability in deep learning (Defazio and Bottou, 2018). On the other hand, some
widely used techniques (e.g., heavy-ball momentum, adaptivity) lack theoretical acceleration guar-
antees. We suspect that a major reason here is the misalignment of the theoretical assumptions with
practice. Our work demonstrates that the concept of acceleration critically relies on the problem
assumptions and that the standard global Lipschitz-gradient condition may not hold in the case of
some applications and thus must be relaxed to admit a wider class of objective functions.
1.1	Contributions
In light of the above background, the main contributions of this paper are the following:
Inspired and supported by neural network training experiments, we introduce a new smoothness
condition that allows the local smoothness constant to increase with the gradient norm. This
condition is strictly weaker than the pervasive Lipschitz-gradient assumption.
We provide a convergence rate for clipped GD under our smoothness assumption (Theorem 3).
We prove an upper-bound (Theorem 6) and a lower-bound (Theorem 4) on the convergence rate
of GD under our relaxed smoothness assumption. The lower-bound demonstrates that GD with
fixed step size can be arbitrarily slower than clipped GD.
We provide upper bounds for stochastic clipped GD (Theorem 7) and SGD (Theorem 8). Again,
stochastic clipped GD can be arbitrarily faster than SGD with a fixed step size.
We support our proposed theory with realistic neural network experiments. First, in the state of art
LSTM language modeling (LM) setting, we observe the function smoothness has a strong correlation
with gradient norm (see Figure 2). This aligns with the known fact that gradient clipping accelerates
LM more effectively compared to computer vision (CV) tasks. Second, our experiments in CV
and LM demonstrate that clipping accelerates training error convergence and allows the training
trajectory to cross non-smooth regions of the loss landscape. Furthermore, gradient clipping can
also achieve good generalization performance even in image classification (e.g., 95.2% test accuracy
in 200 epochs for ResNet20 on Cifar10). Please see Section 5 for more details.
2	A New Relaxed Smoothness Condition
In this section, we motivate and develop a relaxed smoothness condition that is weaker (and thus,
more general) than the usual global Lipschitz smoothness assumption. We start with the traditional
definition of smoothness.
2.1	Function smoothness (Lipschitz gradients)
Recall that f denotes the objective function that we want to minimize. We say that f is L-smooth if
INf (X)- Vf (y)k ≤ Lkx - yk, for all x,y ∈ Rd.	(1)
For twice differentiable functions, condition (1) is equivalent to ∣∣V2f (x)k ≤ L,∀x ∈ Rd. This
smoothness condition enables many important theoretical results. For example, Carmon et al. (2017)
show that GD with h = 1/L is up to a constant optimal for optimizing smooth nonconvex functions.
But the usual L-smoothness assumption (1) also has its limitations. Assuming existence ofa global
constant L that upper bounds the variation of the gradient is very restrictive. For example, simple
polynomials such as f(x) = x3 break the assumption. One workaround is to assume that L exists
in a compact region, and either prove that the iterates do not escape the region or run projection-
based algorithms. However, such assumptions can make L very large and slow down the theoretical
convergence rate. In Section 4, we will show that a slow rate is unavoidable for gradient descent
with fixed step size, whereas clipped gradient descent can greatly improve the dependency on L.
2
Published as a conference paper at ICLR 2020
The above limitations force fixed-step gradient descent (which is tailored for Lipschitz smooth func-
tions) to converge slowly in many tasks. In Figure 1, we plot the estimated function smoothness at
different iterations during training neural networks. We find that function smoothness varies greatly
at different iterations. From Figure 1, we further find that local smoothness positively correlates
with the full gradient norm, especially in the language modeling experiment. A natural question is:
Can we find a fine-grained smoothness condition under which we can design the-
oretically and empirically fast algorithms at the same time?
To answer this question, we introduce the relaxed smoothness condition in the next section, which
is developed on the basis of extensive experiments— Figure 1 provides an illustrative example.
(SsbueOOUJS)OO-
-3	-2	-1 O
log(gradient norm)
uθ⅛至
Figure 1: Gradient norm vs local gradient Lipschitz constant on a log-scale along the training trajectory for
AWD-LSTM (Merity et al., 2018) on PTB dataset. The colorbar indicates the number of iterations during
training. More experiments can be found in Section 5. Experiment details are in Appendix H.
2.2	A new relaxed smoothness condition
We observe strong positive correlation between function smoothness and gradient norm in language
modeling experiments (Figure 1(a)). This observation leads us to propose the following smoothness
condition that allows local smoothness to grow with function gradients.
Definition 1. A second order differentiable function f is (L0, L1)-smooth if
∣N2f(x)k≤ Lo + LikVf(x)k.	⑵
Definition 1 strictly relaxes the usual (and widely used) L-smoothness. There are two ways to
interpret the relaxation: First, when we focus on a compact region, we can balance the constants L0
and L1 such that L0 L while L1	L. Second, there exist functions that are (L0, L1)-smooth
globally, but not L-smooth. Hence the constant L for L-smoothness gets larger as the compact set
increases but L0 and L1 stay fixed. An example is given in Lemma 2.
Remark 1. It is worth noting that we do not need the Hessian operator norm and gradient norm to
necessarily satisfy the linear relation (2). As long as these norms are positively correlated, gradient
clipping can be shown to achieve faster rate than fixed step size gradient descent. We use the linear
relationship (2) for simplicity of exposition.
Lemma 2. Let f be the univariate polynomial f(x) = Pid=1 aixi. When d ≥ 3, then f is (L0, L1)-
smooth for some L0 and L1 but not L-smooth.
Proof. The first claim follows from limχ-∞ ∣ /〃(；))∣ = limχ--∞ ∣ ff(X) ∣ = ∞. The second claim
follows by the unboundedness of f00(χ).	□
2.3	Smoothness in neural networks
We saw that our smoothness condition relaxes the traditional smoothness assumption and is moti-
vated empirically (Figure 1). Below we develop some intuition for this phenomenon. We conjecture
that the proposed positive correlation results from the common components in expressions of the
gradient and the Hessian. We illustrate the reasoning behind this conjecture by considering an `-
layer linear network with quadratic loss—a similar computation also holds for nonlinear networks.
3
Published as a conference paper at ICLR 2020
The L2 regression loss of a deep linear network is L(Y, f (X)) := ∣∣Y - w` .… WiX∣∣2, where
Y denotes labels, X denotes the input data matrix, and Wi denotes the weights in the ith layer.
By (Lemma 4.3 Kawaguchi, 2016), we know that
Vvec(Wi)L(Y, f(X)) = ((W …Wi+i)乳(Wi-1 …W2WiX)t)t vec(f(X) - Y),
where vec(∙) flattens a matrix in Rm×n into a vector in Rmn; 0 denotes the Kronecker product. For
constants i, j such that ` ≥ j > i > 0, the second order derivative
Vvec(wj)Vvec(wi)L(Y, f(X)) =
((w` …Wi+ι) 0 (Wi-ι …W2W1X)T)T((w` …Wj+ι) 0 (Wj-ι ∙∙∙ W2W1X)T)+
((W- ∙∙∙ Wi+ι) 0 (Wi-I …W2W1X ))(10 ((f (X) - Y )w` …Wj+ι)).
When j = i, the second term equals 0. Based on the above expressions, we notice that the gradient
norm and Hessian norm may be positively correlated due to the following two observations. First,
the gradient and the Hessian share many components such as the matrix product of weights across
layers. Second, if one naively upper bounds the norm using Cauchy-Schwarz, then both upper-
bounds would be monotonically increasing with respect to ∣Wi∣ and ∣f(X) - Y∣.
3	Problems setup and algorithms
In this section, we state the optimization problems and introduce gradient based algorithms for them
that work under the new smoothness condition (2). Convergence analysis follows in Section 4.
Recall that we wish to solve the nonconvex optimization problem minx∈Rd f (x). Since in general
this problem is intractable, following common practice we also seek an -stationary point, i.e., a
point x such that ∣Vf (x)∣ ≤ . Furthermore, we make the following assumptions to regularize the
function class studied and subsequently provide nonasymptotic convergence rate analysis.
Assumption 1. The function f is lower bounded by f * > -∞.
Assumption 2. The function f is twice differentiable.
Assumption 3 ((L0, Li)-smoothness). The function f is (L0, Li)-smooth, i.e., there exist positive
constants Lo and Li such that ∣V2f (χ)∣ ≤ Lo + LIkVf (χ)∣—see condition (2).
The first assumption is standard. Twice differentiability in Assumption 2 can relaxed to first-order
differentiability by modifying the definition of (Lo, Li)-smoothness as
limsup gf(x)-”+"k ≤ LIkVf(x)∣ + Lo.
δ→~o
The above inequality implies Vf (x) is locally Lipschitz, and hence almost everywhere differen-
tiable. Therefore, all our results can go through by handling the integrations more carefully. But to
avoid complications and simplify exposition, we assume that the function is twice differentiable.
To further relax the global assumptions, by showing that GD and clipped GD are monotonically
decreasing in function value, we require the above assumptions to hold just in a neighborhood de-
termined by the sublevel set S1 for a given initialization xo, where
S := {x | ∃ y such that f(y) ≤ f(xo), and kx - yk ≤ 1}.	(3)
3.1	Gradient descent algorithms
In this section, we review a few well-known variants of gradient based algorithms that we analyze.
We start with the ordinary gradient descent with a fixed step size η,
xk+i = xk - ηVf(xk).	(4)
This algorithm (pedantically, its stochastic version) is widely used in neural network training. Many
modifications of it have been proposed to stabilize or accelerate training. One such technique of
particular importance is clipped gradient descent, which performs the following updates:
xk+i = Xk - hcVf (xk),	where hc := min{ηc, ^fx^}∙	(5)
1The constant “1” in the expression (3) is arbitrary and can be replaced by any fixed positive constant.
4
Published as a conference paper at ICLR 2020
Another algorithm that is less common in practice but has attracted theoretical interest is normalized
gradient descent. The updates for normalized GD method can be written as
Xk+1 = Xk - hnVf(Xk), where hn ：= gf 1¾k+β∙	(6)
The stochastic version of the above algorithms replace the gradient with a stochastic estimator.
We note that Clipped GD and NGD are almost equivalent. Indeed, for any given ηn and β, if we set
γηc = ηn and ηc = ηn /β, then we have
2 hc ≤ hn ≤ 2hc.
Therefore, clipped GD is equivalent to NGD up to a constant factor in the step size choice. Conse-
quently, the nonconvex convergence rates in Section 4 and Section 4.2 for clipped GD also apply to
NGD. We omit repeating the theorem statements and the analysis for conciseness.
4 Theoretical analysis
In this section, we analyze the oracle complexities of GD and clipped GD under our relaxed smooth-
ness condition. All the proofs are in the appendix. We highlight the key theoretical challenges that
needed to overcome in Appendix B (e.g., due to absence of Lipschitz-smoothness, already the first-
step of analysis, the so-called “descent lemma” fails).
Since we are analyzing the global iteration complexity, let us recall the formal definition being used.
We follow the notation from Carmon et al. (2017). For a deterministic sequence {Xk}k∈N, define
the complexity of {Xk}k∈N for a function f as
Z({xt}t∈N,f)：= inf{t ∈ N∣kVf(xt)k ≤ e}.	⑺
For a random process {Xk}k∈N, we define the complexity of {Xk}k∈N for function f as
Te({xt}t∈N,f) ：= inf {t ∈ N∣Prob(kVf(xk)k ≥ e for all k ≤ t) ≤ 1}.	(8)
In particular, if the condition is never satisfied, then the complexity is ∞. Given an algorithm Aθ,
where θ denotes hyperparameters such as step size and momentum coefficient, we denote Aθ [f, X0]
as the sequence of (potentially stochastic) iterates generated by A when operating on f with ini-
tialization X0. Finally, we define the iteration complexity of an algorithm class parameterized by p
hyperparameters, A = {Aθ}θ∈Rp on a function class F as
N(A, F, e) ：= inf sup T(Aθ [f, X0], f).	(9)
Aθ∈A x0∈Rd,f∈F
The definition in the stochastic setting simply replaces the expression (7) with the expression (8). In
the rest of the paper, “iteration complexity” refers to the quantity defined above.
4.1	Convergence in the deterministic setting
In this section, we present the convergence rates for GD and clipped GD under deterministic setting.
We start by analyzing the clipped GD algorithm with update defined in equation (5).
Theorem 3.	Let F denote the class of functions that satisfy Assumptions 1, 2, and3 in setS defined
in (3). Recall f * is a global lower boundforfunction value. With % = ɪo^, γ = min{ η1-, 7 j 叮},
we can prove that the iteration complexity of clipped GD (Algorithm 5) is upper bounded by
20Lo(f (x0)- f*)	20max{1,L2}(f(x0) - f*)
e2	+	l0	.
The proof of Theorem 3 is included in Appendix C.
Now, we discuss the convergence of vanilla GD. The standard GD is known to converge to first order
e-stationary points in O((L(f (X0) - f*))e-2) iterations for (L, 0)-smooth nonconvex functions.
By Theorem 1 of Carmon et al. (2017), this rate is up to a constant optimal.
However, we will show below that gradient descent is suboptimal under our relaxed (L0, L1)-
smoothness condition. In particular, to prove the convergence rate for gradient descent with fixed
step size, we need to permit it benefit from an additional assumption on gradient norms.
5
Published as a conference paper at ICLR 2020
Assumption 4. Given an initialization x0, we assume that
M := sup{kVf (x)k | X such that f(x) ≤ f (xo)} < ∞.
This assumption is in fact necessary, as our next theorem reveals.
Theorem 4.	Let F be the class of objectives satisfying Assumptions 1, 2, 3, and 4 with fixed con-
stants L0 ≥ 1, L1 ≥ 1, M > 1. The iteration complexity for the fixed-step gradient descent
algorithms parameterized by step size h is at least
LiM(f(xo)- f *- 5〃8)
8e2(log M +1)	.
The proof can be found in Appendix D.
Remark 5. Theorem 1 of Carmon et al. (2017) and Theorem 4 together show that gra-
dient descent with a fixed step size cannot converge to an -stationary point faster than
Ω ((LιM∕log(M) + L0)(f(x0) — f*)e-2). Recall that clipped GD algorithm converges as
O (Lo(f (χo) 一 f*)e-2 + L2(f(χo) 一 f*)L-1). Therefore, clipped GD can be arbitrarily faster
than GD when L1M is large, or in other words, when the problem has a poor initialization.
Below, we provide an iteration upper bound for the fixed-step gradient descent update (4).
Theorem 6. Suppose assumptions 1, 2, 3 and 4 hold in set S defined in (3). If we pick parameters
such that h = (2(ml1+lci)), then we Can prove that the iteration complexity of GD with a fixed step
size defined in Algorithm 4 is upper bounded by
4(ML1 + L0)(f(x0) 一 f*)e-2.
Please refer to Appendix E for the proof. Theorem 6 shows that gradient descent with a fixed step
size converges in O((M L1 + L0)(f(x0) 一 f*)/e2) iterations. This suggests that the lower bound
in Remark 5 is tight up to a log factor in M .
4.2 Convergence in the stochastic setting
In the stochastic setting, we assume GD and clipped GD have access to an unbiased stochastic
gradient Vf (x) instead of the exact gradient Vf (x). For simplicity, we denote gk = Vf(xk)
below. To prove convergence, we need the following assumption.
Assumption 5. There exists τ > 0, such that kVf (x) 一 Vf (x)k ≤ τ almost surely.
Bounded noise can be relaxed to sub-gaussian noise if the noise is symmetric. Furthermore, up
to our knowledge, this is the first stochastic nonconvex analysis of adaptive methods that does not
require the gradient norm kVf (x)k to be bounded globally.
The main result of this section is the following convergence guarantee for stochastic clipped GD
(based on the stochastic version of the update (5)).
Theorem 7. Let Assumptions 1-3 and 5 hold globally with Li > 0. Let h =
min{ i6ηL](1gjck+τ), η} where η = min{20^, ⑵LIT, √T }∙ Then we Can show that iteration
complexity for stochastic clipped GD after of update (5) is upper bounded by
128Li 4∆ 80L0 + 512Liτ
δ maχ{	, -4^,	2	},
e	e4	e2
where ∆ = (f (xo) — f* + (5Lo + 2Lιτ)τ2 + 9τL2∕Lι).
In comparison, we have the following upper bound for ordinary SGD.
Theorem 8. LetAssumptions 1-3, and 5 hold globally with Li > 0. Let h = min{力,L(M+/)}∙
Then the iteration complexity for the stochastic version of GD (4) is upper bounded by
(f (xo) — f * + (5Lo + 4Li M)(M + T )2)2e-4.
We cannot provide a lower bound for this algorithm. In fact, lower bound is not known for SGD even
in the global smoothness setting. However, the deterministic lower bound in Theorem 4 is still valid,
though probably loose. Therefore, the convergence of SGD still requires additional assumption and
can again be arbitrarily slower compared to clipped SGD when M is large.
6
Published as a conference paper at ICLR 2020
(SSeU£00ES)bo-
600
100
0
0.75
0.50
0.25
0.00
-0.25
-0.50
-0.75
-1.00
(SS0,uq°0E-6-
uoe-l
_ _ _ __
-3	-2	-1
log(gradient norm)
-ι.o
0 5 0 5
■ ■ ■ ■
Iooo
-
(SseuWOOESWO-
U-l一
_ _ __ __ __ __
-3	-2	-1
loq(qradient norm)
Uo4ra」£
Oooooo
Oooooo
6 5 4 3 2 1 0
(a) Learning rate 30, with clipping. (b) Learning rate 2, without clipping. (c) Learning rate 2, with clipping.
Figure 2:	Gradient norm vs smoothness on log scale for LM training. The dot color indicates the iteration
number Darker ones correspond to earlier iterations Note that the spans of X and y axis are not fixed
(SseUSOOESBO-
UORB」*
O	1	O	1	O	1
log(gradient norm)	log(gradient norm)	log(gradient norm)
(a) SGD with momentum. (b) Learning rate 1, without clipping. (c) Learning rate 5, with clipping.
Figure 3:	Gradient norm vs smoothness on log scale for ResNet20 training. The dot color indicates the
iteration number.
5	Experiments
In this section, we summarize our empirical findings on the positive correlation between gradient
norm and local smoothness. We then show that clipping accelerates convergence during neural
network training. Our experiments are based on two tasks: language modeling and image classifi-
cation. We run language modeling on the Penn Treebank (PTB) (Mikolov et al., 2010) dataset with
AWD-LSTM models (Merity et al., 2018)2. We train ResNet20 (He et al., 2016) on the Cifar10
dataset (Krizhevsky and Hinton, 2009). Details about the smoothness estimation and experimental
setups are in Appendix H. An additional synthetic experiment is discussed in Appendix I.
First, our experiments test whether the local smoothness constant increases with the gradient norm,
as suggested by the relaxed smoothness conditions defined in (2) (Section 2). To do so, we evaluate
both quantities at points generated by the optimization procedure. We then scatter the local smooth-
ness constants against the gradient norms in Figure 2 and Figure 3. Note that the plots are on a
log-scale. A linear scale plot is shown in Appendix Figure 5.
We notice that the correlation exists in the default training procedure for language modeling (see
Figure 2a) but not in the default training for image classification (see Figure 3a). This difference
aligns with the fact that gradient clipping is widely used in language modeling but is less popular in
ResNet training, offering empirical support to our theoretical findings.
We further investigate the cause of correlation. The plots in Figures 2 and 3 show that correlation
appears when the models are trained with clipped GD and large learning rates. We propose the fol-
lowing explanation. Clipping enables the training trajectory to stably traverse non-smooth regions.
Hence, we can observe that gradient norms and smoothness are positively correlated in Figures 2a
and 3c. Without clipping, the optimizer has to adopt a small learning rate and stays in a region where
local smoothness does not vary much, otherwise the sequence diverges, and a different learning rate
is used. Therefore, in other plots of Figures 2 and 3, the correlation is much weaker.
As positive correlations are present in both language modeling and image classification experiments
with large step sizes, our next set of experiments checks whether clipping helps accelerate conver-
gence as predicted by our theory. From Figure 4, we find that clipping indeed accelerates conver-
gence. Because gradient clipping is a standard practice in language modeling, the LSTM models
trained with clipping achieve the best validation performance and the fastest training loss conver-
2Part	of the code is available at https://github.com/JingzhaoZhang/
why- clipping- accelerates
7
Published as a conference paper at ICLR 2020
—Ir3θclipθ.25 ——lrlθclipθ.25 ——Ir5clipθ.25 ——Ir2unclipped ——Irlunclipped ——lrθ.5unclipped
3.0
2.5
0	50	100	150	200
Epochs
(a) Training loss of LSTM With different op-
timization parameters.
0 5 0 5
■ ■ ■ ■
6 5 5 4
SSO-IUoep=e>
4.0
0	50	100	150	200
Epochs
(b) Validation loss of LSTM with different
optimization parameters.
----Irlunclipped ----- IrSuncIipped ---- IrO.ImomentumO.9	---- Irlclipped ---- Ir5clipped
050505050
075207520
■ ■■■■■■■■
211110000
sσl 6UJ1
(c) Training loss of ResNet20 with different
optimization parameters.
(d) Test accuracy of ResNet20 with different
optimization parameters.

Figure 4: Training and validation loss obtained with different training methods for LSTM and ResNet
training. The validation loss plots the cross entropy. The training loss additionally includes the weight
regularization term. In the legend, ‘lr30clip0.25’ denotes that clipped SGD uses step size 30 and that the
L2 norm of the stochastic gradient is clipped by 0.25. In ResNet training, we threshold the stochastic
gradient norm at 0.25 when clipping is applied.
gence as expected. For image classification, surprisingly, clipped GD also achieves the fastest con-
vergence and matches the test performance of SGD+momentum. These plots show that clipping can
accelerate convergence and achieve good test performance at the same time.
6	Discussion
Much progress has been made to close the gap between upper and lower oracle complexities for
first order smooth optimization. The works dedicated to this goal provide important insights and
tools for us to understand the optimization procedures. However, there is another gap that separates
theoretically accelerated algorithms from empirically fast algorithms.
Our work aims to close this gap. Specifically, we propose a relaxed smoothness assumption that
is supported by empirical evidence. We analyze a simple but widely used optimization technique
known as gradient clipping and provide theoretical guarantees that clipping can accelerate gradient
descent. This phenomenon aligns remarkably well with empirical observations.
There is still much to be explored in this direction. First, though our smoothness condition relaxes
the usual Lipschitz assumption, it is unclear if there is an even better condition that also matches the
experimental observations while also enabling a clean theoretical analysis. Second, we only study
convergence of clipped gradient descent. Studying the convergence properties of other techniques
such as momentum, coordinate-wise learning rates (more generally, preconditioning), and variance
reduction is also interesting. Finally, the most important question is: “can we design fast algorithms
based on relaxed conditions that achieve faster convergence in neural network training?”
Our experiments also have noteworthy implications. First, though advocating clipped gradient de-
scent in ResNet training is not a main point of this work, it is interesting to note that gradient
8
Published as a conference paper at ICLR 2020
descent and clipped gradient descent with large step sizes can achieve a similar test performance as
momentum-SGD. Second, we learned that the performance of the baseline algorithm can actually
beat some recently proposed algorithms. Therefore, when we design or learn about new algorithms,
we need to pay extra attention to check whether the baseline algorithms are properly tuned.
7	Acknowledgement
SS acknowledges support from an NSF-CAREER Award (Number 1846088) and an Amazon Re-
search Award. AJ acknowledges support from an MIT-IBM-Exploratory project on adaptive, robust,
and collaborative optimization.
References
N. Agarwal, B. Bullins, X. Chen, E. Hazan, K. Singh, C. Zhang, and Y. Zhang. The case for full-
matrix adaptive regularization. arXiv preprint arXiv:1806.02958, 2018.
Z. Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. The Journal
ofMachine Learning Research, 18(1):8194-8244, 2017.
L. Armijo. Minimization of functions having Lipschitz continuous first partial derivatives. Pacific
Journal of mathematics, 16(1):1-3, 1966.
F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence
rate o(1/n). In Advances in Neural Information Processing Systems, pages 773-781, 2013.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-
lems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points i.
arXiv preprint arXiv:1710.11606, 2017.
Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Accelerated methods for nonconvex optimiza-
tion. SIAM Journal on Optimization, 28(2):1751-1772, 2018.
X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for
non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.
K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio.
Learning phrase representations using rnn encoder-decoder for statistical machine translation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics.
URL http://www.aclweb.org/anthology/D14-1179.
Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-XL: At-
tentive language models beyond a fixed-length context. CoRR, abs/1901.02860, 2019. URL
http://arxiv.org/abs/1901.02860.
A. Defazio and L. Bottou. On the ineffectiveness of variance reduced optimization for deep learning.
arXiv preprint arXiv:1812.04529, 2018.
A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support
for non-strongly convex composite objectives. In NIPS, pages 1646-1654, 2014.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
C. Fang, C. J. Li, Z. Lin, and T. Zhang. Spider: Near-optimal non-convex optimization via stochastic
path integrated differential estimator. arXiv preprint arXiv:1807.01695, 2018.
J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence
learning. ArXiv e-prints, May 2017.
S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic
composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22
(4):1469-1492, 2012.
S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic
programming. Mathematical Programming, 156(1-2):59-99, 2016.
9
Published as a conference paper at ICLR 2020
P. Gong and J. Ye. Linear convergence of variance-reduced stochastic gradient without strong con-
vexity. arXiv preprint arXiv:1406.1102, 2014.
I.	Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.
E. Hazan, K. Levy, and S. Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimiza-
tion. In Advances in Neural Information Processing Systems, pages 1594-1602, 2015.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780,
1997.
C. Jin, P. Netrapalli, and M. I. Jordan. Accelerated gradient descent escapes saddle points faster than
gradient descent. In Conference On Learning Theory, pages 1042-1085, 2018.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduc-
tion. In Advances in Neural Information Processing Systems, pages 315-323, 2013.
K. Kawaguchi. Deep learning without poor local minima. In Advances in neural information pro-
cessing systems, pages 586-594, 2016.
D. P. Kingma and J. Ba. ADAM: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
J.	Konecny and P. Richtarik. Semi-stochastic gradient descent methods. arXiv:1312.1666, 2013.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical
report, Citeseer, 2009.
K.	Y. Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint
arXiv:1611.04831, 2016.
X. Li and F. Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes.
arXiv preprint arXiv:1805.08114, 2018.
H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for first-order optimization. In Advances
in Neural Information Processing Systems, pages 3384-3392, 2015.
S.	Merity, N. S. Keskar, and R. Socher. Regularizing and optimizing LSTM language models. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=SyyGPP0TZ.
T.	Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. KhUdanpur. Recurrent neural network based
language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech
Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045-1048,
2010. URL http://www.isca-speech.org/archive/interspeech_2010/i10_
1045.html.
Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2).
In Soviet Mathematics Doklady, volume 27, pages 372-376, 1983.
Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM
Journal on Optimization, 22(2):341-362, 2012.
R.	Pascanu, T. Mikolov, and Y. Bengio. Understanding the exploding gradient problem. CoRR,
abs/1211.5063, 2, 2012.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In
International conference on machine learning, pages 1310-1318, 2013.
M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep
contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 4(5):1-17, 1964.
B. T. Polyak. Introduction to optimization. optimization software. Inc., Publications Division, New
York, 1, 1987.
S.	J. Reddi, S. Kale, and S. Kumar. On the convergence of ADAM and beyond. arXiv preprint
arXiv:1904.09237, 2019.
S.	Santurkar, D. Tsipras, A. Ilyas, and A. Madry. How does batch normalization help optimization?
In Advances in Neural Information Processing Systems, pages 2483-2493, 2018.
10
Published as a conference paper at ICLR 2020
M.	Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient.
Mathematical Programming, 162, 2017.
S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regu-
Iarized loss minimization. In International Conference on Machine Learning, pages 64-72, 2014.
N.	Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple
way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:
1929-1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.
M. Staib, S. J. Reddi, S. Kale, S. Kumar, and S. Sra. Escaping saddle points with adaptive gradient
methods. arXiv preprint arXiv:1901.09149, 2019.
M. SUndermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In
INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication As-
sociation, Portland, Oregon, USA, September 9-13, 2012, pages 194-197, 2012. URL http:
//www.isca- speech.org/archive/interspeech_2012/i12_0194.html.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural net-
works. In Advances in Neural Information Processing Systems 27: Annual Confer-
ence on Neural Information Processing Systems 2014, December 8-13 2014, Montreal,
Quebec, Canada, pages 3104-3112, 2014. URL http://papers.nips.cc/paper/
5346-sequence-to-sequence-learning-with-neural-networks.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31, 2012.
L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural networks us-
ing DropConnect. In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th Inter-
national Conference on Machine Learning, volume 28 of Proceedings of Machine Learning
Research, pages 1058-1066, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL http:
//proceedings.mlr.press/v28/wan13.html.
R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes,
from any initialization. arXiv preprint arXiv:1806.01811, 2018.
A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The marginal value of adaptive
gradient methods in machine learning. In Advances in Neural Information Processing Systems,
pages 4148-4158, 2017.
L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction.
SIAM Journal on Optimization, 24(4):2057-2075, 2014.
T. Young, D. Hazarika, S. Poria, and E. Cambria. Recent trends in deep learning based natural
language processing. CoRR, abs/1708.02709, 2017. URL http://arxiv.org/abs/1708.
02709.
D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the convergence of adaptive gradient methods
for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018a.
D. Zhou, P. Xu, and Q. Gu. Stochastic nested variance reduction for nonconvex optimization. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pages 3925-3936. Curran Associates Inc., 2018b.
Z. Zhou, Q. Zhang, G. Lu, H. Wang, W. Zhang, and Y. Yu. Adashift: Decorrelation and convergence
of adaptive learning rate methods. arXiv preprint arXiv:1810.00143, 2018c.
F. Zou and L. Shen. On the convergence of weighted adagrad with momentum for training deep
neural networks. arXiv preprint arXiv:1808.03408, 2018.
F. Zou, L. Shen, Z. Jie, W. Zhang, and W. Liu. A sufficient condition for convergences of ADAM
and RMSProp. arXiv preprint arXiv:1811.09358, 2018.
11
Published as a conference paper at ICLR 2020
A More related work on accelerating gradient methods
Variance reduction. Many efforts have been made to accelerate gradient-based methods. One
elegant approach is variance reduction (e.g. Schmidt et al., 2017; Johnson and Zhang, 2013; Defazio
et al., 2014; Bach and Moulines, 2013; Konecny and Richtarik, 2013; Xiao and Zhang, 2014; Gong
and Ye, 2014; Fang et al., 2018; Zhou et al., 2018b). This technique aims to solve stochastic and
finite sum problems by averaging the noise in the stochastic oracle via utilizing the smoothness of
the objectives.
Momentum methods. Another line of work focuses on achieving acceleration with momentum.
Polyak (1964) showed that momentum can accelerate optimization for quadratic problems; later,
Nesterov (1983) designed a variation that provably accelerate any smooth convex problems. Based
on Nesterov’s work, much theoretical progress was made to accelerate different variations of the
original smooth convex problems (e.g. Ghadimi and Lan, 2016; 2012; Beck and Teboulle, 2009;
Shalev-Shwartz and Zhang, 2014; Jin et al., 2018; Carmon et al., 2018; Allen-Zhu, 2017; Lin et al.,
2015; Nesterov, 2012).
Adaptive step sizes. The idea of varying step size in each iteration has long been studied. Armijo
(1966) proposed the famous backtracking line search algorithm to choose step size dynamically.
Polyak (1987) proposed a strategy to choose step size based on function suboptimality and gradient
norm. More recently, Duchi et al. (2011) designed the Adagrad algorithm that can utilize the sparsity
in stochastic gradients.
Since 2018, there has been a surge in studying the theoretical properties of adaptive gradient meth-
ods. One starting point is (Reddi et al., 2019), which pointed out that ADAM is not convergent and
proposed the AMSGrad algorithm to fix the problem. Ward et al. (2018); Li and Orabona (2018)
prove that Adagrad converges to stationary point for nonconvex stochastic problems. Zhou et al.
(2018a) generalized the result to a class of algorithms named Padam. Zou et al. (2018); Staib et al.
(2019); Chen et al. (2018); Zhou et al. (2018c); Agarwal et al. (2018); Zhou et al. (2018b); Zou
and Shen (2018) also studied different interesting aspects of convergence of adaptive methods. In
addition, Levy (2016) showed that normalized gradient descent may have better convergence rate
in presence of injected noise. However, the rate comparison is under dimension dependent set-
ting. Hazan et al. (2015) studied the convergence of normalized gradient descent for quasi-convex
functions.
B	Challenges in the proofs
In this section, we highlight a few key challenges in our proofs. First, the analysis convergence
under the relaxed smoothness condition is more difficult than the traditional setup. In particular,
classical analyses based on Lipschitz-smooth gradients frequently exploit the descent condition:
f(y) ≤ f(χ) + Wf(X),y - Xi + 2ky - χk2.	(10)
However, under our relaxed smoothness condition, the last term will increase exponentially in
ky - xk2. To solve this challenge, We bound the distance moved by clipping and apply GronWall's
inequality.
Second, our algorithm specific loWer bound proved in Theorem 4 is novel and tight up to alog factor.
To our knoWledge, the Worst case examples used have not been studied before.
Last, proving the convergence of adaptive methods in the nonconvex stochastic setting suffers from
a fundamental challenge: the stochastic gradient is dependent on the update step size. This problem
is usually circumvented by either assuming gradients have bounded norms or by using a lagging-by-
one step-size to decouple the correlation. The situation is even Worse under the relaxed smoothness
assumption. In our case, We overcome this challenge by a novel analysis that divides the proof into
the large gradient scenario and the small gradient scenario.
C Proof of Theorem 3
We start by proving a lemma that is repeatedly used in later proofs. The lemma bounds the gradient
in a neighborhood of the current point by Gronwall's inequality (integral form).
12
Published as a conference paper at ICLR 2020
Lemma 9. Given x such that f(x) ≤ f(x0), for any x+ such that kx+ - xk ≤ min{1/L1, 1}, we
have ∣∣Vf(x+)k ≤ 4(Lo∕Lι + ∣∣Vf(x)k).
Remark 10. Note that the constant “1” comes from the definition ofS in (3). If Assumption 3 holds
globally, then we do not need to constrain ∣x+ - x∣ ≤ 1. This version will be used in Theorem 7.
Proof. Let γ(t) be a curve defined below,
γ(t) = t(x+ - x) + x, t ∈ [0, 1].
Then we have
Vf(γ(t)) = Z t V(2)f(γ(τ))(x+
0
- x)dτ + Vf(γ(0)).
By Cauchy-Schwarz’s inequality, we get
∣Vf(γ(t))∣ ≤ ∣x+
- x∣ Z t
0
∣V(2)f(γ(τ))∣dτ+∣Vf(x)∣
≤ : J" + LιkVf(Y(T 川沟+ kVf(X):
The second inequality follows by Assumption 3. Then We can apply the integral form of GronWall's
inequality and get
kVf (γ(t))k ≤ L0 + kVf (x)k + Zot (L0 + kVf (x)k) exp(t - T)dτ.
The Lemma folloWs by setting t = 1.
□
C.1 Proof of the theorem
We parameterize the path betWeen xk and its updated iterate xk+1 as folloWs:
γ(t) = t(xk+1 - xk) + xk,∀t ∈ [0, 1].
Since xk+1 = xk-hkVf(xk), using Taylor’s theorem, the triangle inequality, and Cauchy-SchWarz,
We obtain
f(xk+ι) ≤ f(xk) - hkkVf(Xk)k2 + kxk+1 - Xxk2 Z 1 kV2f(γ(t))kdt.
20
Since
h ≤ γη ≤ min J -□___________________________________1_____I
k ≤ kVf(χ)k ≤ IkVf(X)k,LιkVf(χx)k 卜
We knoW by Lemma 9
kVf(γ(t)k ≤ 4(L + kVf(x)k).
Then by Assumption 3, We obtain the “descent inequality”:
f(xk+ι) ≤ f(xk) — hkkVf(xx)k2 + 5L0 + 4L2kVf(Xk)k kVf(xx)k2hx.
Therefore, as long as hk ≤ 1/(5L0 + 4L1kVf(Xk)k) (Which folloWs by our choice of η, γ), We can
quantify the descent to be
hxkVf(Xk )k2
f (xx+1) ≤ f(xx)------2-----.
When kVf (xk)k ≥ L0/L1, We have
hxkVf(xx)k2 ≥	Lo	,
13
Published as a conference paper at ICLR 2020
When e ≤ ∣Vf (Xk)k ≤ L0/L1, we have
hkINf(χk)k2 ≥ INf(Xk)k2 ≥ 上
2 一	20Lo	- 20Lo.
Therefore,
f (Xk+I) ≤ f (xk)-min { 20maL0ι,L2}，2^ } .
Assume that e ≤ ∣∣Vf (Xk)Il for k ≤ T iterations. By doing a telescopic sum, We get
T-1	L	e2
X f (Xk+1) - f (Xk) ≤ -T min 120max01,L2}，近卜
Rearranging we get
T < 20Lo(f (xo) - f *) 20max{1, L2}(f (x°) - f *)
≤	e2	+	l0	.
D Proof of Theorem 4
We will prove a lower bound for the iteration complexity of GD with fixed step size. The high level
idea is that if GD converges for all functions satisfying the assumptions, then the step size needs to
be small. However, this small step size will lead to very slow convergence for another function.
Recall that the fixed step size GD algorithm is parameterized by the scaler: step size h. First, we
show that when h > 2log(M )+2,
ML1
sup T(Ah[f, X0], f) = ∞
x0∈Rd,
f∈F
We start with a function that grows exponentially. Let L1 > 1, M > 1 be fixed constants. Pick the
initial point X0 = (log(M) + 1)/L1. Let the objective be defined as follows,
e-Lιx
Li e ,
for X
-L11，
<
f (x) =	L22x2 + 2L1,	for x
[-L11，L11 ]，
∈
eLix
Tie,
for X
1
Ll.
>
We notice that the function satisfies the assumptions with constants
L0 = 1,	L1 > 1,	M > 1.
(11)
When h > 2X0/M, we would have |X1 | > |X0|. By symmetry of the function and the super-linear
growth of the gradient norm, we know that the iterates will diverge. Hence, in order for gradient
descent with a fixed step size h to converge, h must be small enough. Formally,
h	2x0 _ 2log(M) + 2
≤ M = - Ml1 -.
Second, We show that when h ≤ 2 IoMMI)+2,
SUp Te(Ah[f,X0],f) ≥ ∆LιM∕(4e2(logM + 1))
x0∈Rd,
f∈F
Now, let’s look at a different objective that grows slowly.
f-2e(x + 1) + 苧,for X < -1,
f(x) =	4(6x2 — x4),	for x ∈ [-1,1],
(2e(x - 1) + 54^,	for x > 1.
14
Published as a conference paper at ICLR 2020
This function is also second order differentiable and satisfies the assumptions with constants in (11).
If we set xo = 1 + ∆∕e for some constant ∆ > 0, we know that f(xo) - f * = 2∆ + 5e∕4. With the
step size choice h ≤ (2 log M + 2)/(M L1), we know that in each step, xk+1 ≥ xk - (4(log M +
1))/(L1 M). Therefore, for k ≤ ∆L1 M /(4e2 (log M + 1)),
l∣vf (xk 州=2e.
After combining these two points, we proved the theorem by definition (9).
E Proof of Theorem 6
We start by parametrizing the function value along the update,
f (γ(t)):= f (xk - thVf(xk)),t ∈ [0,1].
Note that with this parametrization, we have γ(0) = xk, γ(1) = xk+1. Now we would like to argue
that if f (xk) ≤ f (xo), then ∣∣Vf (x(t))k ≤ M, ∀t ≤ 1. Assume by contradiction that this is not
true. Then there exists e > 0, t ∈ [0,1] such that ∣Vf (x(t))∣ ≥ M + e. Since e can be made
arbitrarily small below a threshold, we assume e < M. Denote
t*=inf{t| lVf(x(t))l ≥M+e}.
The value t* exists by continuity of lVf (x(t))l as a function oft. Then we know by Assumption 4
that f (x(t*)) > f(xk). However, by Taylor expansion, we know that
f(x(t*)) ≤ f(xk) - thlVf(xk)l2 + (th)2lVf(xk)l2 Z t lV(2)f(x(τ))ldτ
≤ f(xk) - thlVf(xk)l2 + (th)2lVf(xk)l2(L1(M + e) + L0)
≤ f(xk).
The last inequality follows by h = 1/(2(M L1 + L0)). Hence we get a contradiction and conclude
that for all t ≤ 1, lVf (x(t))l ≤ M. Therefore, following the above inequality and Assumption 3,
we get
f(xk+ι) ≤ f(xk) - h∣Vf(xk)k2 + h2
L1M+L0
kVf (xk )k2
≤ f(xk) -
e2
4(ML1 + L0).
2
The conclusion follows by the same argument as in Theorem 3 via a telescopic sum over k.
F Proof of Theorem 7
Recall that we set the following parameters
hk = min{ E⅛E，n}
η = min{ 20L0,12⅛, √T}
Similar to proof of Theorem 3, we have
E[f (xk+ι)∣] ≤f(xk) - E[hkhgk, Vf(xk)i] + 5L0 + 4L2kvf(Xk)k E[hk∣gkk2]
≤f(xk) - E[hkhgk, Vf(xk)i] + 5L0 + 4L2kvf(Xk)k E[hk(kVf(xk)k2
+ lgk - Vf(xk)l2 + 2hVf(xk), gk - Vf(xk)i)]
≤f(Xk) + E[-hk + 5L0 + 4L2kVf(Xk)k hk]∣Vf(Xk)k2
+ E[hk (-1 + (5L0 + 4L1 kVf (xk)k)hk)hVf (xk), gk - Vf (xk)i]
+ 5L0 + 4L2kVf(Xk)k E[hk(kgk -Vf(Xk)∣2)]
(12)
(13)
15
Published as a conference paper at ICLR 2020
First we show (5L0 + 4LJNf (Xk)k)hk ≤ 2. This follows by 5Lohk ≤ 11 ,hk4LJNf (Xk)k ≤
hk4Lι(kgkk + T) ≤ 4. Substitute in(15) and we get
E[f(xk+ι)∣] ≤f(xk) + E[-牛]kVf(xk)k2	(14)
+ E[-hkhVf(xk),gk -Vf(Xk)i]
'-------------{z-----------}
T1
+E[(5L0+4L1kVf(Xk)k)h2khVf(Xk),gk -Vf(Xk)i]
X----------------------{--------------------}
T2
+ 5L0 +4L2kVf(Xk)k E[hk(kgk-Vf(Xk)k2)]
'-------------------V------------------}
T3
Then we bound T1 , T2 , T3 in Lemma 11,12,13 and get
E[f(Xk+ι)∣] ≤f(Xk) + E[-与]kVf(Xk)k2	(15)
+ (5L0 + 2L1τ)η2τ2 + 9η2τ L02/L1
Rearrange and do a telescopic sum, we get
E[X hkVf(Xk)k2] ≤ f (xo) - f* + η2T((5Lo + 2Lιτ)τ2 + 9τL0/Li)
k≤T
≤ f (xO)- f * + ((5L0 + 2L1τ)τ2 + 9τL2/Li)
Furthermore, we know
hkkVfkk2 = min{η, 16Lι(kVfkk + T) }kVfkk2
≥ min{η, 32Lι1Vfkk，32⅛}kVfkk2
≥ min{η, 32L⅛Π MVfkk2
Hence along with η ≤ T-i/2, we get
E[ X min{ηkVfk『,kfk }] ≤ f (xo) - f * + ((5Lo + 2Lιτ )τ2 + 9τL2∕Lι)
32L
i
Let U = {k∣η∣∣Vfk ∣∣2 ≤ ɪjfɪ }, we know that
E[X ηkVfkk2] ≤ f(X0) - f* + ((5L0 + 2LiT)T2 + 9TL02/Li),
k∈U
and
E[ X kfk ] ≤ f (xo) - f * + ((5L0 + 2Lιτ )τ2 + 9τL2∕Lι).
32Li
k∈U c	i
Therefore,
E[mjn ∣Vf (Xk)∣] ≤ E[min{ɪ X ∣Vfk∣], 1- X ∣∣Vfk∣∣}]
k	|U| k∈U	IU c| k∈Uc
≤ Emm{J卷 X后k2, ⅛∣ X kVfkk}]
k∈U	k∈Uc
≤ max{ jf (xo) - f * + ((5L0 + 2Lιτ)τ2 + 9τL0∕Lι)
(f (xo) - f* + (5Lo + 2Lιτ)τ2 + 9τL0∕Lι)64L1}.
TT + 20L0 + 128L1T
T
16
Published as a conference paper at ICLR 2020
The last inequality follow by the fact that either |U | ≥ T/2 or |Uc | ≥ T /2. This implies that
E[mink≤τ ∣∣Vf(xk)k] ≤ 2e when
128L1 4∆ 80L0 + 512L1τ
T ≥ ∆max{	e—, -r,-----e2-----},
where ∆ = (f (xo) — f * + (5Lo + '2L∖τ)τ2 + 9τL2∕Lι). By Markov inequality,
P{min kVf (xk)k ≤ e} ≥ 1.
k≤T	2
The theorem follows by the definition in (9).
F.1 Technical lemmas
Lemma 11.
1
E[-hkhVf(xk),gk — Vf(xk)i] ≤ 4E[hk]∣Vf(xk)k .
Proof. By unbiasedness of gk and the fact that η is a constant, we have
E[—hk hVf(xk), gk — Vf(xk)i] = E[(η — hk)hVf (xk), gk — Vf(xk)i]
=E[(n - hk 乂Vf(Xk ), gk- Vf(Xk)i1{kgkk≥ i6L1τη -τ}
≤ η∣Vf(Xk)∣E[∣gk — Vf(Xk)∣i{kgkk≥ 16LL1η-τ}]
≤ η∣Vf(xk)∣232L1E[hk]τ
The second last inequality follows by hk ≤ η and Cauchy-Schwartz inequality. The last inequality
follows by
kVf (xk)k ≥ kgkk - T = 16Lιhk - 2τ ≥ 16Lιhk - 32Lιη ≥ 32Lιhk .	(16)
The equality above holds because hk = 16.工](Igk 八十「). The lemma follows by 32ηL∖τ ≤ 1/4. □
Lemma 12.
E[(5Lo + 4Lι∣Vf(xk)k)hkNf(Xk),gk- Vf(Xk)〉] ≤ 9η2τL0∕Lι + 1 E[h®]∣Vf(xk)∣2.
8
Proof. When ∣Vf (Xk)∣ ≥ L0/L1,
E[(5L0+4L1∣Vf(Xk)∣)h2khVf(Xk),gk - Vf(Xk)i] ≤9L1∣Vf(Xk)∣E[h2k]∣Vf(Xk)∣τ
≤ 1 E[hk ]∣Vf(Xk )∣2
8
The last inequality follows by (12).
When ∣Vf (Xk)∣ ≤ L0/L1,
E[(5Lo +4Lι∣Vf(Xk)∣)hkEf(Xk),gk-Vf(Xk)〉] ≤9η2τL0/Li
□
Lemma 13.
5Lo + 4LIkVf(Xk)k em(∣gk - Vf(Xk)∣2)] ≤ (5Lo + 2Lιτ)η2τ2 + 1 ∣Vf(Xk)∣2E[hk].
28
Proof. When kVf (Xk)k ≥ L0/L1 + τ, we get
5Lo +4LIkVf(Xk)k E[hk(∣gk - Vf(Xk)k2)] ≤ 5Lι∣Vf(Xk)k2E[hk]ητ ≤ 1 ∣Vf(Xk)∣2E[hk].
28
The first inequality follows by hk ≤ η and kgk - Vf (Xk)k ≤ τ ≤ kVf(Xk)k.The last inequality
follows by (12).
When kVf (Xk)k ≤ Lo/L1 +τ, we get
5Lo + 4LIkVf (Xk)k 曰同(kgk - Vf(Xk)k2)] ≤ (5Lo + 2Lιτ)η2τ2.
□
17
Published as a conference paper at ICLR 2020
G Proof of Theorem 8
Similar to proof of Theorem 3, we have
E[f (xk+ι)∣] ≤f(xk) — E[hkhgk, Vf(Xk)i] + 5L0 + 4L2|Vf (xk)k E[hkkgkk2]
,ft 1	1	2 q 5Lo + 4LiM(M + T)2
≤f(xk) - √tkVf(Xk州 +-------------2T---------
Sum across k ∈ {0, ..., T - 1} and take expectations, then we can get
0 ≤f (xo) - E[f (XT)] - ∖ XXX EhkVf(Xk)k2i + 5L0 + 4LIM(M + T)2
T k=1
Rearrange and we get
1 X	2^∣ / 1	* . 5LO + 4L1M(M + T)2、
TNE[kVf(Xk)k	≤√τ(f(χo) -f +--------------2---------J
By Jensen’s inequality,
1T
T ∑E[kVf (Xk)k] ≤
k=1
By Markov inequality,
P(T X hkVf (Xk)k2i > √T (f (XO) - f* + 5L0 + 4LIM(M + T)2) j ≤ 0.5
The theorem follows by the definition in (9) and Jensen’s inequality.
f(X0) - f* +
5L0 + 4L1M(M + T)2
2
H	Experiment details
In this section, we first briefly overview the tasks and models used in our experiment. Then we
explain how we estimate smoothness of the function. Lastly, we describe some details for generating
the plots in Figure 2 and Figure 3.
H.1 Language modelling
Clipped gradient descent was introduced in (vanilla) recurrent neural network (RNN) language
model (LM) (Mikolov et al., 2010) training to alleviate the exploding gradient problem, and has
been used in more sophisticated RNN models (Hochreiter and Schmidhuber, 1997) or seq2seq mod-
els for language modelling or other NLP applications (Sutskever et al., 2014; Cho et al., 2014). In
this work we experiment with LSTM LM (Sundermeyer et al., 2012), which has been an important
building block for many popular NLP models (Young et al., 2017).
The task of language modelling is to model the probability of the next word wt+1 based on word
history (or context). Given a document of length T (words) as training data, the training objective is
to minimize negative log-likelihood of the data -1 ∑T=ι log P(Wt |wι…wt-1).
We run LM experiments on the Penn Treebank (PTB) (Mikolov et al., 2010) dataset, which has been
a popular benchmark for language modelling. It has a vocabulary of size 10k, and 887k/70k/78k
words for training/validation/testing.
To train the LSTM LM, we follow the training recipe from 3 (Merity et al., 2018). The model is
a 3-layer LSTM LM with hidden size of 1150 and embedding size of 400. Dropout (Srivastava
et al., 2014) of rate 0.4 and DropConnect (Wan et al., 2013) of rate 0.5 is applied. For optimization,
clipped SGD with clip value of 0.25 and a learning rate of 30 is used, and the model is trained for
500 epochs. After training, the model reaches a text-set perplexity of 56.5, which is very close to
the current state-of-art result (Dai et al., 2019) on the PTB dataset.
3https://github.com/salesforce/awd-lstm-lm
18
Published as a conference paper at ICLR 2020
H.2 Image classification
As a comparison, we run the same set of experiments on image classification tasks. We train the
ResNet20 (He et al., 2016) model on Cifar10 (Krizhevsky and Hinton, 2009) classification dataset.
The dataset contains 50k training images and 10k testing images in 10 classes.
Unless explicitly state, we use the standard hyper-parameters based on the Github repository4. Our
baseline algorithm runs SGD momentum with learning rate 0.1, momentum 0.9 for 200 epochs. We
choose weight decay to be 5e-4. The learning rate is reduced by 10 at epoch 100 and 150. Up to our
knowledge, this baseline achieves the best known test accuracy (95.0%) for Resnet20 on Cifar10.
The baseline already beats some recently proposed algorithms which claim to improve upon SGD
momentum.
2 0 8 6 4 2
■ ■■■■■
Iloooo
ssUWooES
00
0.00
0.25 0.50 0.75	1.00
Gradient norm
O
uo=e-ω-
0 5 0 5 0 5 0
■ ■■■■■■
0 0 1 1 2 2 3
..............
(SSBU£00ES) 60-
120000
100000
80000
60000
40000
20000
(b) Figure 2a with 200 epochs.
-3.5
-3	-2	-1 O
log(gradient norm)
(a) Figure 2a plotted on a linear scale.
Figure 5: Auxiliary plots for Figure 2a. The left subfigure shows the values scattered on linear scale.
The right subfigure shows more data points from 200 epochs.
0.8
-64
E」ou lupe」
0	50	100	150	200	250	300
Iterations
(a) Estimation for gradient norm.
0.7
0.1
6 5 4 3 2
-----
Ooooo
SS ① U£00ES
0	50	100	150	200	250	300
Iterations
(b) Estimation for local smoothness.
Figure 6: Estimated gradient norm and smoothness using 10% data versus all data. The values are
computed from checkpoints of the LSTM LM model in the first epoch. This shows that statistics
evaluated from 10% of the entire dataset provides accurate estimation.
uo=u*
0
H.3 Estimating smoothness
Our smoothness estimator follows a similar implementation as in (Santurkar et al., 2018). More pre-
cisely, given a sequence of iterates generated by training procedure {xk }k, we estimate the smooth-
ness L(xk) as follows. For some small value δ ∈ (0, 1), d = xk+1 - xk,
f /	∖
L(xk)
max
γ∈{δ,2δ,...,1}
∣∣Vf(X + Yd) -Vf(x)k
Pk
(17)
4https://github.com/kuangliu/pytorch-cifar
19
Published as a conference paper at ICLR 2020
This suggests that we only care about the variation of gradient along xk+1 - xk . The motivation
is based on the function upper bound (10), which shows that the deviation of the objective from its
linear approximation is determined by the variation of gradient between xk+1 and xk.
H.4 Additional plots
The plots in Figure 2a show log-scale scattered data for iterates in the first epoch. To supplement
this result, we show in Figure 5a the linear scale plot of the same data as in Figure 2a. In Figure 5b,
we run the same experiment as in Figure 2a for 200 epochs instead of 1 epoch and plot the gradient
norm and estimated smoothness along the trajectory.
In Figure 2, we plot the correlation between gradient norm and smoothness in LSTM LM training.
We take snapshots of the model every 5 iterations in the first epoch, and use 10% of training data
to estimate gradient norm and smoothness. As shown in Figure 6, using 10% of the data provides a
very accurate estimate of the smoothness computed from the entire data.
I A Synthetic Experiment
In this section, we demonstrate the different behaviors of gradient descent versus clipped gradient
descent by optimizing a simple polynomial f (x) = x4. We initialize the point at x0 = 30 and run
both algorithms. Within the sublevel set [-30, 30], the function satisfies
f00(x) ≤ 12 × 302 = 1.08 × 104
f00(x) ≤ 10f0(x) + 0.1.
Therefore, we can either pick L1 = 0, L0 = 1.084 for gradient descent or L1 = 10.L0 = 0.1 for
clipped GD. Since the theoretical analysis is not tight with respect to constants, we scan the step
sizes to pick the best parameter for both algorithms. For gradient descent, we scan step size by
halving the current steps. For clipped gradient descent, we fix threshold to be 0.01 and pick the step
size in the same way. The convergence results are shown in Figure 7. We can conclude that clipped
gradient descent converges much faster than vanilla gradient descent, as the theory suggested.
(b) Gradient descent f (x) = x4.
65432101
Ooooooo-
Iiiiiiio
O 200 400 600 800
iterations
(c) Gradient descent f0 (x)
IO6
IOOO
二 4x3.
(a) Gradient descent |x|.
IO3
IO0
ιo-3-
IO-6
10-9
step: 128.000000
step： 64.000000
step: 32.000000
——step： 16.000000
200 400 600 800
iterations
IOOO
(d) Clipped GD |x|.	(e) Clipped GD f(x) = x4.	(f) Clipped GD f0 (x) = 4x3.
Figure 7:	An synthetic experiment to optimize f (x) = x4. (a) Gradient descent with different step
size. (b) Clipped gradient descent with different step size and threshold = 0.01.
J A quantitative comparison of theorems and experiments
To quantify how much the result align with the theorem, we assume that the leading term in the
iteration complexity is the e dependent term. For GD, the term scales as O( M√+L0), while for
Clipped GD, the term scales as O(√0).
20
Published as a conference paper at ICLR 2020
First, we start with the synthetic experiment presented in I. From theory, we infer that the im-
Provement of f (χXGD))≈ mlL+l ≈ 1e5. In experiment, the best performing GD reaches
f0(xτ) = 0.36, while for clipped GD, the value is 1.3e - 8, and the ratio is f(XGDD))≈ 1e7.
This suggests that in this very adversarial (against vanilla GD) synthetic experiment, the theorem is
correct but conservative.
Next, we test how well the theory can align with practice in neural network training. To do so,
we rerun the PTB experiment in 5 with a smaller architecture (2-Layer LSTM with 200 embedding
dimension and 512 inner dimension). We choose hyperparameters based on Figure 4a. For clipped
GD, we choose clipping threshold to be 0.25 and learning rate to be 10. For GD, we use a learning
rate 2. One interesting observation is that, though GD makes steady progress in minimizing function
value, its gradient norm is not decreasing.
To quantify the difference between theory and practice, we follow the procedure as in the synthetic
experiment. First, we estimate ML1 + L0 = 25 for GD from Figure 8(c). Second, we estimate
L1 = 10, L0 = 5 for clipped GD’s trajectory from subplot (d). Then the theory predicts that the
ratio between gradients should be roughly 25/5 = 5. Empirically, we found the ratio to be ≈ 3
by taking the average (as in Theorem 3 and Theorem 6). This doesn’t exactly align but is of the
same scale. From our view, the main reason for the difference could be the presence of noise in
this experiment. As theorems suggested, noise impacts convergence rates but is absent in our rough
estimates ML1+LO.
O 50 IOO 150	200
Epoch
(a) Gradient descent.
7 6 5
sso-j
10
ssəuɪaoo UJS
O
2
0
0.0	0.2	0.4
Gradient Norm
0.6
(c)
0	50	100	150	200
Epoch
(b) Clipped gradient descent.
SS ① U£001US
0	2	4
Gradient Norm
(d)
Figure 8:	(a) Gradient norm. (b) Loss curves. (c)The scatter points of smoothness vs gradient norm
for the model trained with gradient descent.(d)The scatter points of smoothness vs gradient norm
for the model trained with clipped GD.
21