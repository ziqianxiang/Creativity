Published as a conference paper at ICLR 2020
Learning Hierarchical Discrete Linguistic
Units from Visually-Grounded Speech
David HarWath； Wei-Ning Hsu*, and James Glass
Computer Science and Artificial Intelligence Lab
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{dharwath,wnhsu,glass}@csail.mit.edu
Ab stract
In this paper, we present a method for learning discrete linguistic units by incorpo-
rating vector quantization layers into neural models of visually grounded speech.
We show that our method is capable of capturing both word-level and sub-word
units, depending on how it is configured. What differentiates this paper from prior
work on speech unit learning is the choice of training objective. Rather than us-
ing a reconstruction-based loss, we use a discriminative, multimodal grounding
objective which forces the learned units to be useful for semantic image retrieval.
We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a
27.3% reduction in ABX error rate over the top-performing submission, while
keeping the bitrate approximately the same. We also present experiments demon-
strating the noise robustness of these units. Finally, we show that a model with
multiple quantizers can simultaneously learn phone-like detectors at a lower layer
and word-like detectors at a higher layer. We show that these detectors are highly
accurate, discovering 279 words with an F1 score of greater than 0.5.
1	Introduction
By 8 months of age, human infants learn to recognize not only the names of their caregivers and
common objects, but also the contrast between the different vowels and consonants which comprise
these words (Dupoux, 2018). Nearly all toddlers learn to carry a conversation long before they can
read and write. Humans learn to model the discrete, hierarchical, and compositional nature of their
native language not from written text, but from speech audio - a continuous, time-varying waveform
which is the product not only of the underlying words which were spoken, but also the physical
properties of the speaker’s vocal tract, the speaker’s health and emotional state, and the noise and
reverberation present in the environment. The question of how such a complex symbolic system is
inferred from continuous and noisy sensory input data is of interest not only to the cognitive science
community, but also to machine learning researchers who aim to reproduce this ability with comput-
ers. A more comprehensive understanding of human language acquisition has practical significance
in real-world applications, such as automatic speech recognition (ASR) and natural language under-
standing (NLU) systems. In the past several decades, enormous progress has been made in speech
recognition research, and nowadays ASR systems are able to achieve human-level accuracy in many
domains (Chiu et al., 2018). Unfortunately, the techniques that have been developed to achieve these
levels of performance are extremely data-hungry, requiring many thousands of hours of speech au-
dio recordings for training. Since supervised machine learning algorithms form the basis of ASR
training, the data also needs to be annotated by expert humans. Due to the immense cost of collect-
ing and annotating speech data, ASR technology currently exists for approximately 120 (Google,
2019) out of the nearly 7,000 (Lewis et al., 2016) human languages spoken worldwide. It is highly
unlikely that purely supervised machine learning techniques will be able to scale to include all hu-
man languages, necessitating the development of alternative methods by researchers which are able
to function with far fewer annotations, or even no annotations at all. Because human beings provide
an existence proof of language acquisition from speech completely without language supervision, it
is plausible that this ability could be replicated by a machine learning algorithm.
* Equal contribution
1
Published as a conference paper at ICLR 2020
In this paper, we present a method for discovering discrete and hierarchical representations of speech
units both at the sub-word level and the word level. Previously proposed linguistic unit discovery
methods have only leveraged the speech audio modality in isolation, relying on objective functions
that attempt to capture statistical regularities within the speech signal. The key innovation in our
work is that we discover units by training models with explicit discretization layers to associate
speech waveforms with visual images using a cross-modal grounding objective. This forces our
models to learn representations which capture semantic information at the highest layers of the
network. Because semantics are predominantly carried by words, and words are composed of sub-
word units (such as phones and syllables), the visual grounding objective indirectly forces the model
to learn speaker- and noise-invariant representations of speech units. By incorporating trainable
quantization layers into our networks, we are able to capture these units in discrete inventories.
Whether these units correspond to word-like or sub-word units depends on where the quantization
layers are inserted, and how they are trained.
2	Related Work
Prior work on unsupervised modeling of the speech signal has generally focused on learning repre-
sentations which either disentangle or isolate the latent factors that are of interest for downstream
tasks. In most cases the primary latent factor of interest is the phonetic or lexical identity of a given
segment of speech, but other factors, such as the identity of the speaker, are sometimes of interest
as well. Because the factors of interest are often inherently discrete (e.g. words and phones), many
of the proposed approaches attempt to perform segmentation and clustering of the surface features
in one way or another. One family of techniques is based upon Segmental Dynamic Time Warping
(S-DTW) (Park & Glass, 2005; 2008; Jansen et al., 2010; Jansen & Van Durme, 2011), which uses
a self-comparison algorithm to identify relatively long duration (on the order of a second) patterns
which frequently reoccur in a speech corpus; these patterns tend to capture words or short phrases.
A different line of work employs probabilistic graphical models to jointly segment and cluster the
speech signal (Varadarajan et al., 2008; Zhang & Glass, 2009; Gish et al., 2009; Lee & Glass, 2012;
Siu et al., 2014; Lee et al., 2015; Ondel et al., 2016; Kamper et al., 2016; 2017a). With an ap-
propriately designed model, it is possible to learn multiple, hierarchical categories of speech units.
However, in order to enable efficient inference, the conditional distributions of these models tend to
be simple and therefore have limited modeling power.
Deep neural network models have been successfully used to learn powerful speech representations
using weakly or unsupervised objectives (Thiolliere et al., 2015; Kamper et al., 2015; Hsu et al.,
2017a;b; Hsu & Glass, 2018; Holzenberger et al., 2018; Milde & Biemann, 2018; van den Oord
et al., 2018; Chung et al., 2019; Pascual et al., 2019). These representations have predominantly
been continuous in nature, as discrete latent variables are not trivially compatible with backpropaga-
tion. To obtain discrete representations, a post-hoc clustering step can be applied to the continuous
representations (Kamper et al., 2017b; Feng et al., 2019). More recently, several papers have pro-
posed ways of directly incorporating discrete variables into neural network models, including using
Gumbel-Softmax (Eloff et al., 2019b) or straight-through estimators (van den Oord et al., 2017;
Chorowski et al., 2019; Razavi et al., 2019).
A different method for learning meaningful representations of speech is via a multimodal ground-
ing objective, which encourages the learning of speech representations that are predictive of the
contextual information contained in a separate but accompanying modality, such as vision. Visual
grounding of speech is a form of self-supervised learning (Virginia de Sa, 1994), which is powerful
in part because it offers a way of training models with a discriminative objective that does not depend
on traditional transcriptions or annotations. The first work in this direction relied on phone strings
to represent the speech (Roy & Pentland, 2002; Roy, 2003), but more recently this learning has been
shown to be possible directly on the speech signal (Synnaeve et al., 2014; Harwath & Glass, 2015;
Harwath et al., 2016). Subsequent work on visually-grounded models of speech has investigated
improvements and alternatives to the modeling or training algorithms (Leidal et al., 2017; Kamper
et al., 2017c; Havard et al., 2019a; Merkx et al., 2019; ChrUPaIa et al., 2017; Scharenborg et al.,
2018; Kamper et al., 2019b;a; Surls et al., 2019; Ilharco et al., 2019; Eloff et al., 2019a), application
to mUltilingUal settings (Harwath et al., 2018a; KamPer & Roth, 2017; AzUh et al., 2019; Havard
et al., 2019a), analysis of the linguistic abstractions, such as words and phones, which are learned by
the models (Harwath & Glass, 2017; Harwath et al., 2018b; Drexler & Glass, 2017; Alishahi et al.,
2
Published as a conference paper at ICLR 2020
2017; Harwath et al., 2019; Harwath & Glass, 2019; Havard et al., 2019b), and the impact of jointly
training with textual input (Holzenberger et al., 2019; ChrUPala, 2019; Pasad et al., 2θ19). RePre-
sentations learned by models of visually grounded speech are also well-suited for transfer learning
to suPervised tasks, being highly robust to noise and domain shift (Hsu et al., 2019).
3	Data and Models
3.1	Dataset
For training our models, we utilize the MIT Places 205 dataset (Zhou et al., 2014) and their ac-
comPanying sPoken audio caPtions (Harwath et al., 2016; 2018b). The caPtion dataset contains
aPProximately 400,000 sPoken audio caPtions, each of which describes a different Places image.
These caPtions are free-form sPontaneous sPeech, collected from over 2,500 different sPeakers and
covering a 40,000 word vocabulary. The average caPtion duration is aPProximately 10 seconds,
and each caPtion contains on average 20 words. For vetting our models during training, we use a
held-out validation set of 1,000 image-caPtion Pairs.
3.2	Neural Models of Visually-Grounded Speech
We base our model uPon the Residual DeeP Audio-Visual Embedding network (ResDAVEnet) ar-
chitecture (Harwath et al., 2019), which contains two branches of fully convolutional networks, one
for images and the other for audio. Each branch encodes samPles of the corresPonding modality into
a d-dimensional sPace, regardless of the original dimensionality of the samPles. This is achieved by
aPPlying global sPatial mean Pooling and global temPoral mean Pooling to the image branch outPut
and the audio branch outPut, resPectively. The image branch is adaPted from ResNet50 (He et al.,
2016), where the final softmax layer and the Preceding fully-connected layers are removed, rePlaced
with a 1x1 linear convolutional layer in order to Project the feature maP to the desired dimension.
To model the audio inPuts, a 17-layer fully convolutional network with residual connections is used.
The inPut is a log Mel-frequency sPectrogram with 40 frequency bins and 25 ms-wide, Hamming-
windowed frames with a shift of 10 ms. The first layer of this network is a 1-D convolution that
sPans the entire frequency axis of the sPectrogram, while the remaining 16 convolutional layers are
1-D across the time axis. These 16 layers are divided into four residual blocks of 4 layers each, and
downsamPling between these blocks is accomPlished by aPPlying the first convolution of each block
with a stride of 2. For full details of the model, refer to Harwath et al. (2019).
3.3	Learning Hierarchical Discrete Units with Vector Quantizing Layers
Previous analyses reveal that ResDAVEnet-like models learn linguistic abstractions at different lev-
els, including words (Harwath & Glass, 2017) and robust Phonetic features (Harwath & Glass, 2019;
Hsu et al., 2019). To exPlicitly learn hierarchical discrete linguistic units within this framework, we
ProPose to incorPorate multiPle vector quantization (VQ) layers (van den Oord et al., 2017) into the
ResDAVEnet audio branch; we refer to this new architecture as ResDAVEnet-VQ.
VQ layers can be understood as a tyPe of bottleneck, which constrain the amount of information
that can flow through. While these layers have been used to learn discrete sub-word units (van den
Oord et al., 2017; Chorowski et al., 2019; Razavi et al., 2019), Previous work injects VQ layers
into autoencoders that are trained with a reconstruction loss. As a result, the embedding dimen-
sion of each code and the number of codes need to be carefully tuned (Liu et al., 2019). When the
embedding dimension is too low or the codebook size too small, the model does not have enough
exPressive Power to caPture linguistic variability. When it is too large, the model starts to encode
non-linguistic information in order to imProve reconstruction. In contrast, the learning signal of
ResDAVEnet-VQ is Provided by the visual-semantic grounding objective. Rather than encoding as
much information about inPut as Possible, the learned codes in ResDAVEnet-VQ only need to caP-
ture semantic information. Since semantics in sPeech are Predominantly transmitted by words, and
words are comPosed of sub-word units like Phones, the grounding objective Places Pressure on the
model to robustly infer both from sPeech. Since words and Phones are inherently discrete symbols,
rePresenting them with learned discrete units may not even hurt the grounding Performance.
3
Published as a conference paper at ICLR 2020
Figure 1 illustrates the proposed ResDAVEnet-VQ model. We add a quantization layer after each of
the first two residual blocks of the ResDAVEnet-VQ model, denoted as VQ2 and VQ3, respectively,
with the intention that they should capture discrete sub-word-like and word-like units. A VQ layer
K×D
is defined as E ∈ R , where K represents the codebook size, and D represents the output
dimensionality of the input features to the codebook. Denoting the tth temporal frame of the input
to the quantization layer as xt, quantization is performed according to qt = Ek,:, where k =
arg minj ||xt - Ej,: ||2 The quantized output is then fed as input to the subsequent residual block. As
in van den Oord et al. (2017), we use the straight-through estimator (Bengio et al., 2013) to compute
the gradient passed from qt to xt . We use the exponential moving average (EMA) codebook updates
proposed by van den Oord et al. (2017).

ConV Res ΓVQ∏I Res LTVOT ReS » ReS
1_	2	2	3	3	4 J 5
√
Figure 1: Diagram of the ResDAVEnet-VQ model. On the left, we show the placement of the vector
quantization blocks in the audio branch. Note that each “Res” block is comprised of a stack of
multiple sub-layers (see Harwath et al. (2019) for details). The right half of the figure depicts the
quantization mechanism of each VQ block, as well as the bypass path when the block is disabled.
3.4	Codebook Learning Schedules
We include multiple VQ layers in the ResDAVEnet-VQ model, each of which can be independently
enabled or bypassed without changing the rest of the architecture configuration. When all model
weights, including the VQ codebooks, are trained jointly in a single training run we call this a “cold-
start” model. Alternatively, a model can be “warm-started” by copying the weights from another
trained model that has fewer (or no) VQ layers enabled, and randomly initializing the codebook of
the newly activated VQ layer(s). This gives rise to the questions of how many quantizers should
be used and in what order they should be enabled. It is unclear whether models with the same
VQ layers activated would learn the same representation at each layer regardless of the training
curriculum. Let Am denote a subset of all VQ layers, and Am-1 ⊂ Am. We use “A1 → ... → AM”
to denote a model that is obtained by sequentially training models “A1 → ... → Am” initialized
from “A1 → ... → Am-1”, where the model A1 is initialized from scratch, and the final model
would have VQ layers in AM activated. For instance, a model initialized from scratch with no VQ
layers enabled is denoted as “0”，and a model initialized with that and with both layers enabled is
denoted as "0 → {2,3}”.
3.5	Training with the Triplet Loss
We train our models using the same loss function as Harwath et al. (2019). This loss function blends
two triplet loss terms (Weinberger & Saul, 2009), one based on random sampling of negative exam-
ples, and the other based on semi-hard negative mining (Jansen et al., 2018), in order to find more
challenging negative samples. Specifically, let the sets of output embedding vectors for a minibatch
of B audio/image training pairs respectively be A = {a1, . . . , aB} and I = {i1, . . . , iB}. To com-
pute the randomly-sampled triplet loss term, we select impostor examples for the j th input according
to aj ~ UniformCategoriCal({a1,..., aB}\aj∙) andij∙ ~ UniformCategoriCal({i1,..., iB}\ij∙).
The randomly-sampled triplet loss is then computed as:
B
Ls = X ( max(0, ijaj — ijQj + 1) + max(0, IjQj- ijQj + 1))	(1)
j=1
For the semi-hard negative triplet loss, we first define the sets of impostor Candidates for the j th
example as Aj∙ = {a ∈ A|ijQ < ijQj} and Ij = {i ∈ IIiTQj < ijQj}. The semi-hard negative
4
Published as a conference paper at ICLR 2020
loss is then computed as:
B
Lh =( max(0, max(i∕a) — Ijaj + 1) + max(0, max(iTaj) — Ijaj + 1))	(2)
λ a	a∈Aj	i∈Ij	)
j=1	j	j
Finally, the overall loss function is computed by combining the two above losses, L = Ls + Lh,
which was found by (Harwath et al., 2019) to outperform either loss on its own.
3.6	Implementation Details
All of our models were trained for 180 epochs using the Adam optimizer (Kingma & Ba, 2014)
with a batch size of 80. We used an exponentially decaying learning rate schedule, with an initial
value of 2e-4 that decayed by a factor of 0.95 every 3 epochs. Following van den Oord et al. (2017),
we use an EMA decay factor of γ = .99 for training each VQ codebook. Our core experimental
results all use a codebook size of 1024 vectors for all quantizers, but in the supplementary material
we include experiments with smaller and larger codebooks. Following Chorowski et al. (2019),
the jitter probability hyperparameter for each quantization layer was fixed at 0.12. While we do
not apply data augmentation to the input spectrograms, during training we perform standard data
augmentation techniques to the images. We resize each raw image so that its smallest dimension
is 256 pixels, and then we apply an Inception-style random crop which is resized to 224 pixels
square. During training, we also flip each image horizontally with a probability of 0.5. During
evaluation, the center 224 pixel square crop is always taken from the image. Finally, the RGB pixel
values are mean and variance normalized. We trained each model on the Places audio caption train
split, and computed the image and caption recall at 10 (R@10) scores on the validation split of
the Places audio captions after each training epoch. The model snapshot that achieved the highest
average R@10 score on the validation set from each training is used for all evaluation. To extract
embeddings and units from our models, we simply perform a forward pass through the speech branch
of the ResDAVEnet-VQ network and retain the outputs from the target layer at a uniform frame-rate.
The frame-rate is determined by the downsampling factor at the target layer relative to the input. For
non-quantized layers, these outputs will be continuous embeddings. For quantized layers, these will
be quantized embedding retrieved from the assigned entry in the codebook.
4 Experiments
4.1	Sub-Word Unit Learning on the ZeroSpeech 2019 ABX Task
Evaluation metrics Learning unsupervised speech representations that are indicative of phonetic
content is of high interest to the speech community, and recently has been the focus of the Ze-
roSpeech Challenge (Versteegh et al., 2015; Dunbar et al., 2017; 2019). One of the core evaluations
is the minimal-pair ABX task (Schatz et al., 2013), which aims to benchmark representations in
terms of their discriminability between different sub-word speech units. In this task, a model is
tasked with extracting representations for a triplet of speech waveform segments denoted by A, B,
and X. A and B are constrained to be a triphone minimal pair; that is, both segments capture
three phones, but differ only in the identity of their center phone. The third segment, X is cho-
sen to contain the same underlying triphone sequence as A. Supposing f (∙) denotes the model's
mapping function from a waveform segment to a sequence of embedding vectors, the ABX er-
ror rate under a given similarity metric S(∙, ∙) is defined as the fraction of ABX triples in which
S (f (A), f(X)) > S(f(B), f(X)). An ABX error rate of 50% indicates random assignment, while
an ABX of 0% reflects perfect phone discriminability. In the ZeroSpeech challenge, S(∙, ∙) is imple-
mented using Dynamic Time Warping (DTW) with various distance measures (cosine, KL, etc.). In
our evaluation, we use the cosine distance.
The ZeroSpeech 2019 challenge in particular emphasizes on discovering an inventory of discrete
sub-word units, rather than continuous representations. Therefore, in addition to an ABX error rate,
a bitrate is also computed for each model which reflects the amount of information carried by the
learned units. A lower bitrate can be achieved by having a more compact inventory of learned units
or having a smaller number of codes per second. The full details of the evaluation can be found
in Dunbar et al. (2019). To be clear, all of our ResDAVEnet-VQ models were not trained on the
5
Published as a conference paper at ICLR 2020
Table 1: Comparison of R@10, ABX scores, and bit-rates between different configurations and
baseline models trained on ZeroSpeech 2019 data or Places Audio Caption. All quantizers reflected
in this table used a codebook size of 1,024 vectors. We do not compute RLE or segment scores for
the FHVAE-DPGMM model, since we did not re-implement that model.
Model ID	Layer	R@10	ABX	Frame-B Bitrate	ased RLE Bitrate	Segme ABX	nt-Based Bitrate
							
FHVAE-DPGMM (ZS)	N/A	N/A	21.67	413.23	-	-	-
WaveNet-VQ (ZS)	N/A	N/A	19.98	151.55	136.74	20.48	126.17
WaveNet-VQ (PA)	N/A	N/A	24.87	149.00	136.27	25.23	126.22
“0”	Res2	.735	11.35	N/A	N/A	N/A	N/A
	Res3		10.86	N/A	N/A	N/A	N/A
“{2}”	VQ2	.753	12.33	433.30	361.09	12.78	332.86
“0 → {2}”	VQ2	.760	11.79	390.61	317.66	12.66	289.11
“{3}”	VQ3	.734	38.21	213.92	129.65	38.68	108.84
“0 → {3}”	VQ3	.794	15.04	182.93	140.04	16.53	121.26
“{2, 3}”	VQ2	.667	25.62	408.75	258.37	26.32	217.58
	VQ3		32.23	218.76	156.69	32.49	136.90
“0 → {2, 3}”	VQ2	.787	13.15	405.43	334.39	13.30	303.03
	VQ3		14.95	199.91	172.05	15.60	159.07
“{2} → {2, 3}”	VQ2 VQ3	.764	12.51 14.52	415.13 167.84	341.85 136.11	13.06 15.68	311.82 121.17
“{3} → {2, 3}”	VQ2	.760	13.55	421.23	271.91	14.38	232.87
	VQ3		33.70	208.63	117.37	33.58	98.29
ZeroSpeech training data, but instead on the Places audio captions, thus there is a domain mismatch
between training and testing these models.
In addition to the frame-based bitrate and ABX scores computed by the ZeroSpeech 2019 evaluation
toolkit, we implement our own extensions to these metrics. Because it is common for successive
frames to be assigned to the same codebook entry and phonetic information is not encoded at a fixed
frame rate, lossless run length encoding (RLE) can be a more reasonable measure of the bitrate of
a frame-based model. RLE does not change the ABX score since it can be trivially inverted, but it
does change the bitrate. For computing the RLE bitrate, we modify the bitrate calculation specified
in Dunbar et al. (2019) so that a unique symbol is defined as the tuple (unit, length) where length is
the number of frames assigned to a given unit with in a segment. We also consider segment-based
ABX and bitrate, which is similar to the RLE metrics except in this case we outright discard the
frame length information. This typically results in an even greater reduction in bitrate, but also an
accompanying deterioration in ABX score.
Baseline models In Table 1, we compare our results to those derived from two of the top-
performing submissions to the ZeroSpeech 2019 challenge: a re-implementation of WaveNet-
VQ (Chorowski et al., 2019) provided by Cho et al. (2019) and FHVAE-DPGMM (Feng et al., 2019).
Using the code accompanied with the WaveNet-VQ submission, we were able to train their model
on the set of 400,000 Places audio captions to make a fairer comparison with our ResDAVEnet-VQ
models in terms of the amount of speech data used. In addition, when trying to reproduce the re-
ported WaveNet-VQ results, we obtain better performance than previously reported by training for
more steps. Table 1 shows that WaveNet-VQ achieves similar bitrates regardless of the training data.
However, ABX deteriorates from 19.98 to 24.87, implying the model cannot utilize data of a larger
scale but out-of-domain relative to the test set. A similar degradation when testing on out-of-domain
data with FHVAE models was observed in Hsu et al. (2019). We did not re-train the model submitted
by Feng et al. (2019), and instead compare against the scores reported in Dunbar et al. (2019).
ABX discrimination without using quantization Our first experiment investigates exactly which
layer in the ResDAVEnet-VQ model is most suited for ABX phone discrimination, and would thus
make a good candidate for learning of quantized sub-word units. The leftmost plot in Figure 2
shows that layers 2 and 3 of a ResDAVEnet-VQ model without any quantization enabled perform
6
Published as a conference paper at ICLR 2020
Figure 2:	R@10 and ABX tracked at various training epochs. The “0" model achieves a final R@10
of .735, with ABX scores of 19.77, 11.35, 10.86, and 14.05 for the conv1, res2, res3, and res4 layers.
the best in terms of ABX error rate on the ZeroSpeech 2019 English test set; the exact numbers for
this model are displayed in the caption of Figure 2. Because layers 2 and 3 achieve the lowest ABX
error rates without quantization, we focus our attention on the impact of quantization there.
Quantizing one layer When quantizing only one layer, we examine quantization of layer 2 vs.
layer 3, and using cold-start training vs. warm-start initialization from model “0”. The ABX and
bitrate results for these models, as well as the R@10 scores on the Places validation set, are shown in
Table 1. In all cases, quantization applied at the output of layer 2 achieves a better ABX score than
quantization at layer 3, but VQ3 achieves a better bitrate. Quantization barely impacts the perfor-
mance of layer 2, whose ABX score very slightly rises from 11.35 to 11.79. Warm-start initialization
is beneficial to R@10 and ABX score in both cases, but we notice an intriguing anomaly when ap-
plying cold-start quantization to layer 3: the ABX score deteriorates significantly, rising from 10.86
in the case of the non-quantized model to 38.21. This indicates that while VQ2 is capable of learn-
ing a finite inventory of units that are highly predictive of phonetic identity from either a warm-start
or cold-start initialization, cold-start training of VQ3 results in very little phonetic information cap-
tured by the quantizer. Interestingly, this model is still learning to infer visual semantics from the
speech signal, as evidenced by a high R@10 score; we later show in Section 4.2 that the reason for
this anomaly is because cold-start training of VQ3 results in the learning of word detectors. In all
cases except for model “{3}”, we note that the ABX scores achieved by our models are significantly
better than the baselines. Our best model in terms of ABX (“0 → {2}”) achieves a 41.0% reduction
in ABX over the WaveNet-VQ baseline, at a cost of a 132.3% increase in RLE bitrate; however,
model “0 → {3}” achieves a 24.7% reduction in ABX error rate with only a 2.4% increase in RLE
bitrate. These results do not constitute a fair comparison, however, because the WaveNet-VQ and
ResDAVEnet-VQ models were trained on different datasets; when training the WaveNet-VQ model
on the same set of audio captions used to train ResDAVEnet-VQ (but without the accompanying
images, since WaveNet-VQ is not a multimodal model), the ABX error rate increases to 24.87%,
tipping the results even more in favor of the ResDAVEnet-VQ models.
Quantizing two layers Quantizing multiple layers at once offers the possibility of learning a hi-
erarchy of units. Thus, we aim to capture phonetic information in a lower layer quantizer and
word-level information at a higher layer quantizer. Cold-start training of two quantizers (“{2, 3}”)
results in a significant drop in ABX performance for both VQ2 and VQ3, but also a drop in R@10 on
the Places validation set. We see much better results in terms of R@10 and ABX for the remaining
3 models which were initialized from the “0” model or a model with only one quantizer enabled;
for example, model “{2} → {2, 3}” achieves an ABX of 14.52 with an RLE bitrate of 136.11,
representing a 27.3% ABX improvement over the best baseline while keeping the bitrate approxi-
mately the same. We see in model “{3} → {2, 3}” that the same phenomenon observed with model
“{3}” persists: VQ3 achieves relatively poor ABX, despite a high overall R@10 and strong ABX
with VQ2 at 13.55%. We confirm in Section 4.2 that the VQ3 layer of model “{3} → {2, 3}” does
7
Published as a conference paper at ICLR 2020
indeed capture word-level information, indicating that this model has successfully localized pho-
netic unit identity in the second layer and lexical unit identity in the third layer. Overall, our results
suggest that when learning hierarchical quantized representations with a ResDAVEnet-VQ model,
the nature of the representations learned is highly dependent on the training curriculum.
Table 2: ABX scores and RLE bitrates for various SNRs on the noisy ZeroSpeech19 English test set.
“R-B” stands for “RLE-Bitrate,” and (n) denotes a model trained on the noisy Places Audio dataset.
For the WaveNet-VQ models, (ZS) and (PA) respectively denote training on the ZeroSpeech 19
English training set, and the clean Places Audio dataset.
Model	Layer	Clean		20-30 dB		10-20 dB		0-10 dB	
		ABX	R-B	ABX	R-B	ABX	R-B	ABX	R-B
WaveNet-VQ (ZS)	N/A	19.98	136.74	21.22	141.07	27.51	144.28	42.55	126.96
WaveNet-VQ (PA)	N/A	24.87	136.27	27.18	137.70	33.29	132.34	42.67	110.50
“0”	Res2	11.35	N/A	11.63	N/A	13.17	N/A	19.44	N/A
“0”	Res3	10.86	N/A	11.16	N/A	12.96	N/A	19.43	N/A
“0 → {2}”	VQ2	11.79	317.66	12.15	325.40	14.62	332.21	23.96	327.15
“{2} → {2, 3}”	VQ2	12.51	341.85	12.56	350.28	14.82	362.73	25.02	330.54
“{2} → {2, 3}”	VQ3	14.52	136.11	14.73	137.68	17.44	143.14	27.68	133.13
“{3} → {2, 3}”	VQ2	13.55	271.91	13.65	272.46	15.69	267.70	24.06	244.52
“{3} → {2, 3}”	VQ3	33.70	117.37	32.56	118.22	34.65	115.40	39.82	102.48
“0” (n)	Res2	13.32	N/A	12.30	N/A	12.97	N/A	16.91	N/A
“0” (n)	Res3	11.85	N/A	11.90	N/A	12.44	N/A	16.09	N/A
“0 → {2}” (n)	VQ2	12.64	342.53	12.20	348.57	13.34	359.43	18.82	373.60
“{2} → {2, 3}” (n)	VQ2	13.42	365.89	13.71	359.14	14.57	370.67	18.78	392.10
“{2} → {2, 3}” (n)	VQ3	14.39	179.19	14.92	180.36	15.38	182.27	19.58	188.32
“{3} → {2, 3}” (n)	VQ2	16.52	223.28	16.47	223.61	17.75	225.72	22.68	230.01
“{3} → {2, 3}” (n)	VQ3	26.21	187.31	25.88	187.92	26.34	188.49	31.26	191.28
Training and testing on noisy data In Hsu et al. (2019), it was shown that representations learned
by a ResDAVEnet model were far more robust to train/test domain mismatch in terms of background
noise, channel characteristics, and speaker identity than standard spectral features when training a
supervised speech recognizer. Here, we examine whether this robustness is also exemplified by
the quantized versions of this model. We construct three additional test sets using the ZeroSpeech
2019 English testing data by adding noise sampled from the AudioSet (Jansen et al., 2018) dataset.
For each ZeroSpeech testing waveform, we randomly sampled an AudioSet waveform of the same
duration and performed linear mixing with a signal-to-noise ratio (SNR) selected randomly within
a specified range. We construct low, medium, and high noise testing sets, corresponding to SNRs
of 20-30 dB, 10-20 dB, and 0-10 dB. We then perform the ABX discrimination task on these noisy
waveforms, displaying the results in Table 2. We find that for all models, a worsening SNR results in
a deterioration in ABX performance. However, the ResDAVEnet-VQ models prove to be far more
noise robust than the Wavenet-VQ model; even in the high noise testing set, the best ResDAVEnet-
VQ model achieves an ABX of 23.96%, while the WaveNet-VQ models degrade to nearly-random
ABX scores of 42.55% and 42.67%.
Given that a ResDAVEnet-VQ model trained on the “clean” Places Audio captions is highly robust
to additive noise on the ABX discrimination task, we investigated whether adding noise to the Places
Audio captions themselves would result in an even higher degree of noise robustness. To that end,
we followed a similar data augmentation approach to create a noisy version of the Places Audio
captions, where the SNR of each caption was randomly chosen to sit within the range of 0-30 dB.
The bottom half of Table 2 shows the results of training several ResDAVEnet-VQ models on the
noisy Places Audio captions and testing on the clean and noisy ZeroSpeech ABX tasks. In general,
we observe a degradation ABX score in the clean conditions, but with a significantly higher degree
of noise robustness in the noisier conditions.
Visualization of learned units To better measure the correspondence between the VQ units and
English phones, we compute corpus-level co-occurrence statistics (at the frame-level) across the
TIMIT training set, excluding the sa dialect sentences. To facilitate visualization, we use the
“0 → {2}“ model with a codebook size of 128. We display the conditional probability matrix
P (phone|unit) in Figure 3, with the rows and columns ordered via spectral co-clustering with 10
8
Published as a conference paper at ICLR 2020
uwgJ≡*p* 部w0w咒 4用 V
s&qel 3UOLIdt≡LL
Figure 3:	Conditional probability matrix displaying P(phone∣unit) using the "0 → {2}“ model
with a VQ2 codebook size of 128. For visualization, We saturate the color scaling at probability 0.5.
clusters in order to group together phones that share similar sets of VQ codes. Visually, there is a
strong mapping between TIMIT phone labels and ResDAVEnet-VQ codes. In some cases, redun-
dant codes are used for the same phone label (this is especially the case for the silence label), and
in other cases we see that phones belonging to the same manner class often tend to share codebook
units. We can numerically quantify the mapping between the phone and unit labels with the normal-
ized mutual information measure (NMI), which we found to be .378 in this case. We also include
several caption spectrograms with their time-aligned unit sequences in Figures 5, 6, and 7 in the
supplementary material.
Table 3: Performance of the VQ3 layer from the “{3} → {2, 3}” model when codes are treated as
word detectors. Codes are ranked by the highest F1 score among the retrieved words for a given
code. Word hypotheses for a given code are ranked by the F1 score. P denotes precision, R recall,
and occ the number of co-occurrences of the code and word in the data.
rank	code	word	Top Hypotheses		R	occ	Second Hypotheses				
			F1	P			word	F1	P	R	occ
											
1	918	pantry	90.67	88.29	93.18	41	spice	3.96	2.20	20.00	1
2	596	kitchen	90.08	91.59	88.63	304	countertop	1.64	0.84	29.63	8
3	88	classroom	88.97	89.05	88.89	72	classrooms	5.01	2.57	100.00		2
4	58	baseball	88.71	88.63	88.78	182	player	3.01	1.65	17.11	13
5	706	background	87.86	91.93	84.14	838	ground	0.58	0.39	1.18	4
198	237	lobby	68.43	56.77	• ∙ 86.11	• 31	waiting	9.93	7.86	13.46	14
199	829	shirt	68.41	71.49	65.58	322	shirts	18.28	10.37	76.79	43
200	59	grass	68.31	56.53	86.28	503	grassy	15.30	8.67	65.35	83
4.2 From Phones to Words: Learning a Hierarchy of Units
As shown in Table 1, all of the ResDAVEnet-VQ models which underwent cold-start training of VQ3
exhibited a similar phenomenon in which the ABX error rate of that layer was particularly high,
despite the model performing well at the image-caption retrieval task. We hypothesized that this
could be due to VQ3 learning to recognize higher level linguistic units, such as words. To examine
this empirically, we inferred the VQ3 unit sequence for every audio caption in the Places Audio
training set according to several different models. Using the estimated word-level transcriptions of
the utterances (provided by the Google SpeechRecognition API), we computed precision, recall,
and F1 scores for every unique (word, VQ3 code) pair for a given model and quantization layer.
We then ranked the VQ codes in descending order according to their maximum F1 score for any
word in the vocabulary. Table 3 shows a sampling of these statistics for model “{3} → {2, 3}”. In
the supplementary material, we include many more examples for this model in Table 7, as well as
9
Published as a conference paper at ICLR 2020
Words of Top 250 Fl-Scores
1.0
0.0
0.0	0.2	0.4	0.6
Precision
• "{2}→{2,3}'
-{3}→{2,3}'
• ,{3}-
O4O3
1 1
SPJoMjO JSqEnN
= sωtt
Number of Words above Fl Threshold
10≡Io1
——-{2}→{2,3}'
-,{3}→{2,3}'
—,{3}-
——"{2,3)-
——'0-*{2,3},
——"0→{3},
10°	∖u-∏τ∖
0.8	1.0	0.0	0.2	0.4	0.6	0.8	1.0
Fl Threshold
Figure 4: Visualization of the precision, recall, and F1 scores of individual VQ3 codes when treated
as word detectors on the Places Audio captions.
examples for the “{2} → {2, 3}” model (which did not learn VQ3 word detectors) in Table 8. It
should be emphasized that these models are exactly the same in all respects, except for the order in
which their quantizers were trained.
We examine the overall performance of VQ3 as a word detector for these models in Figure 4. The
right hand side of Figure 4 displays the number of VQ3 codes whose maximum F1 score is above
a given threshold, while the left hand side shows the distribution of precision and recall scores for
the top 250 words ranked by F1. This gives an approximate indication of how many VQ3 codes
have learned to specialize as detectors for a specific word. We see that the VQ3 layer of model
“{3} → {2, 3}” learns 279 codebook entries with an F1 score above 0.5. In contrast, the VQ3 layer
of model “{2} → {2, 3}” learns only a handful of word-detecting codebook entries with an F1 of
greater than 0.5. This experiment supports the notion that the reason for the poor ABX performances
of the VQ3 layer in models “{3}” and “{3} → {2, 3}” is in fact due to its specialization for detecting
specific words, and that this specialization only emerges when the VQ3 layer is learned before
the VQ2 layer. Section A.2 in the supplementary material examines this phenomenon in greater
experimental detail.
5 Conclusions
In this paper, we demonstrated that the neural vector quantization layers proposed by van den Oord
et al. (2017) can be integrated into the visually-grounded speech models proposed by Harwath et al.
(2019). This resulted in the ability of the speech model to directly represent speech units, such as
phones and words, as discrete latent variables. We presented extensive experiments and analysis
of these learned representations, demonstrating significant improvements in phone discrimination
ability over the current state-of-the-art models for sub-word speech unit discovery. We demonstrated
that these units are also far more robust to noise and domain shift than units derived from previously
proposed models. These results supported the notion that semantic supervision via a discriminative,
multimodal grounding objective has the potential to be more powerful than reconstruction-based
objectives typically used in unsupervised speech models.
We also showed how multiple vector quantizers could be employed simultaneously within a single
ResDAVEnet-VQ model, and that these quantizers could be made to specialize in learning a hier-
archy of speech units: specifically, phones in the lower quantizer and words in the upper quantizer.
Our analysis showed that hundreds of codebooks in the upper quantizer learned to perform as word
detectors, and that these detectors were highly accurate. Our experiments also revealed that this
behavior only emerged when VQ3 was trained before VQ2. These results suggest the importance
of the learning curriculum, which should be more deeply investigated in future work. Future work
should attempt to make explicit what kind of compositional rules are implicitly encoded by these
models when mapping sequences of codes from the lower quantizer to word-level units in the up-
per quantizer; the automatic derivation of a sub-word unit inventory, vocabulary, and pronunciation
lexicon could serve as the starting point for a fully unsupervised speech recognition system. Future
work should also investigate whether layers above VQ3 could be made to learn even higher-level
linguistic abstractions, such as grammar, syntax, and compositional reasoning.
10
Published as a conference paper at ICLR 2020
References
Afra Alishahi, Marie Barking, and Grzegorz ChrUPala. Encoding of phonology in a recurrent neural
model of grounded speech. In Proc. ACL Conference on Natural Language Learning (CoNLL),
2017.
Emmanuel Azuh, David Harwath, and James Glass. Towards bilingual lexicon discovery from visu-
ally grounded speech audio. In Proc. Annual Conference of International Speech Communication
Association (INTERSPEECH), 2019.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng
Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, et al. State-of-the-art
speech recognition with sequence-to-sequence models. In Proc. International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2018.
Suhee Cho, Yeonjung Hong, Yookyunk Shin, and Youngsun Cho. VQVAE with speaker adversarial
training, 2019. URL https://github.com/Suhee05/Zerospeech2019.
Jan Chorowski, Ron J. Weiss, Samy Bengio, and Aaron van den Oord. Unsupervised speech rep-
resentation learning using wavenet autoencoders. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 2019.
Grzegorz Chrupala. Symbolic inductive bias for visually grounded learning of spoken language. In
Proc. Annual Meeting of the Association for Computational Linguistics (ACL), 2019.
Grzegorz Chrupala, Lieke Gelderloos, and Afra Alishahi. Representations of language in a model of
visually grounded speech signal. In Proc. Annual Meeting of the Association for Computational
Linguistics (ACL), 2017.
Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James R. Glass. An unsupervised autoregressive
model for speech representation learning. In Proc. Annual Conference of International Speech
Communication Association (INTERSPEECH), 2019.
Jennifer Drexler and James Glass. Analysis of audio-visual features for unsupervised speech recog-
nition. In Proc. Grounded Language Understanding Workshop, 2017.
Ewan Dunbar, Xuan Nga Cao, Juan Benjumea, Julien Karadayi, Mathieu Bernard, Laurent Besacier,
Xavier Anguera, and Emmanuel Dupoux. The zero resource speech challenge 2017. In Proc.
IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2017.
Ewan Dunbar, Robin Algayres, Julien Karadayi, Mathieu Bernard, Juan Benjumea, Xuan-Nga Cao,
Lucie Miskic, Charlotte Dugrain, Lucas Ondel, Alan W. Black, Laurent Besacier, Sakriani Sakti,
and Emmanuel Dupoux. The zero resource speech challenge 2019: TTS without T. In Proc.
Annual Conference of International Speech Communication Association (INTERSPEECH), 2019.
Emmanuel Dupoux. Cognitive science in the era of artificial intelligence: A roadmap for reverse-
engineering the infant language-learner. In Cognition, 2018.
Ryan Eloff, Herman Engelbrecht, and Herman Kamper. Multimodal one-shot learning of speech and
images. In Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP),
2019a.
Ryan Eloff, Andre Nortje, Benjamin van Niekerk, Avashna Govender, Leanne Nortje, Arnu Preto-
rius, Elan van Biljon, Ewald van der Westhuizen, Lisa van Staden, and Herman Kamper. Un-
supervised acoustic unit discovery for speech synthesis using discrete latent-variable neural net-
works. In Proc. Annual Conference of International Speech Communication Association (INTER-
SPEECH), 2019b.
Siyuan Feng, Tan Lee, and Zhiyuan Peng. Combining adversarial training and disentangled speech
representation for robust zero-resource subword modeling. In Proc. Annual Conference of Inter-
national Speech Communication Association (INTERSPEECH), 2019.
11
Published as a conference paper at ICLR 2020
Herb Gish, Man-Hung Siu, Arthur Chan, and William Belfield. Unsupervised training of an HMM-
based speech recognizer for topic classification. In Proc. Annual Conference of International
Speech Communication Association (INTERSPEECH), 2009.
Google. Google cloud speech-to-text API. https://cloud.google.com/
speech-to-text/, 2019. Accessed: 2019-09-16.
David Harwath and James Glass. Deep multimodal semantic embeddings for speech and images. In
Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.
David Harwath and James Glass. Learning word-like units from joint audio-visual analysis. In Proc.
Annual Meeting of the Association for Computational Linguistics (ACL), 2017.
David Harwath and James Glass. Towards visually grounded sub-word speech unit discovery. In
Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019.
David Harwath, Antonio Torralba, and James R. Glass. Unsupervised learning of spoken language
with visual context. In Proc. Neural Information Processing Systems (NeurIPS), 2016.
David Harwath, Galen Chuang, and James Glass. Vision as an interlingua: Learning multilingual
semantic embeddings of untranscribed speech. In Proc. International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2018a.
David Harwath, Adria Recasens, Dldac Suris, Galen Chuang, Antonio Torralba, and James Glass.
Jointly discovering visual objects and spoken words from raw sensory input. In Proc. IEEE
European Conference on Computer Vision (ECCV), 2018b.
David Harwath, Adria RecaSens, Dldac Suris, Galen Chuang, Antonio Torralba, and James Glass.
Jointly discovering visual objects and spoken words from raw sensory input. International Jour-
nal of Computer Vision, 2019.
William Havard, Jean-Pierre Chevrot, and Laurent Besacier. Models of visually grounded speech
signal pay attention to nouns: a bilingual experiment on English and Japanese. In Proc. Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019a.
William N. Havard, Jean-Pierre Chevrot, and Laurent Besacier. Word recognition, competition, and
activation in a model of visually grounded speech. In Proc. ACL Conference on Natural Language
Learning (CoNLL), 2019b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Nils Holzenberger, Mingxing Du, Julien Karadayi, Rachid Riad, and Emmanuel Dupoux. Learning
word embeddings: Unsupervised methods for fixed-size representations of variable-length speech
segments. In Proc. Annual Conference of International Speech Communication Association (IN-
TERSPEECH), 2018.
Nils Holzenberger, Shruti Palaskar, Pranava Madhyastha, Florian Metze, and Raman Arora. Learn-
ing from multiview correlations in open-domain videos. In Proc. International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2019.
Wei-Ning Hsu and James Glass. Scalable factorized hierarchical variational autoencoder training. In
Proc. Annual Conference of International Speech Communication Association (INTERSPEECH),
2018.
Wei-Ning Hsu, Yu Zhang, and James Glass. Learning latent representations for speech generation
and transformation. In Proc. Annual Conference of International Speech Communication Associ-
ation (INTERSPEECH), 2017a.
Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled and interpretable
representations from sequential data. In Proc. Neural Information Processing Systems (NeurIPS),
2017b.
12
Published as a conference paper at ICLR 2020
Wei-Ning Hsu, David Harwath, and James Glass. Transfer learning from audio-visual grounding to
speech recognition. In Proc. Annual Conference of International Speech Communication Associ-
ation (INTERSPEECH), 2019.
Gabriel Ilharco, Yuan Zhang, and Jason Baldridge. Large-scale representation learning from visu-
ally grounded untranscribed speech. In Proc. ACL Conference on Natural Language Learning
(CoNLL), 2019.
Aren Jansen and Benjamin Van Durme. Efficient spoken term discovery using randomized algo-
rithms. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),
2011.
Aren Jansen, Kenneth Church, and Hynek Hermansky. Toward spoken term discovery at scale with
zero resources. In Proc. Annual Conference of International Speech Communication Association
(INTERSPEECH), 2010.
Aren Jansen, Manoj Plakal, Ratheet Pandya, Daniel P.W. Ellis, Shawn Hershey, Jiayang Liu,
R. Channing Moore, and Rif A. Saurous. Unsupervised learning of semantic audio represen-
tations. In Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP),
2018.
Herman Kamper and Michael Roth. Visually grounded cross-lingual keyword spotting in speech.
In Proc. of the Workshop on Spoken Language Technologies for Under-Resourced Languages
(SLTU), 2017.
Herman Kamper, Aren Jansen, and Sharon Goldwater. Fully unsupervised small-vocabulary speech
recognition using a segmental Bayesian model. In Proc. Annual Conference of International
Speech Communication Association (INTERSPEECH), 2015.
Herman Kamper, Aren Jansen, and Sharon Goldwater. Unsupervised word segmentation and lexicon
discovery using acoustic word embeddings. IEEE Transactions on Audio, Speech and Language
Processing, 2016.
Herman Kamper, Aren Jansen, and Sharon Goldwater. A segmental framework for fully-
unsupervised large-vocabulary speech recognition. Computer Speech and Language, 46(3):154-
174, 2017a.
Herman Kamper, Karen Livescu, and Sharon Goldwater. An embedded segmental k-means model
for unsupervised segmentation and clustering of speech. In Proc. IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU), 2017b.
Herman Kamper, Shane Settle, Gregory Shakhnarovich, and Karen Livescu. Visually grounded
learning of keyword prediction from untranscribed speech. In Proc. Annual Conference of Inter-
national Speech Communication Association (INTERSPEECH), 2017c.
Herman Kamper, Aristotelis Anastassiou, and Karen Livescu. Semantic query-by-example speech
search using visual grounding. In Proc. International Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2019a.
Herman Kamper, Gregory Shakhnarovich, and Karen Livescu. Semantic speech retrieval with a
visually grounded model of untranscribed speech. IEEE Transactions on Audio, Speech and
Language Processing, 2019b.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Interna-
tional Conference on Learning Representations (ICLR), 2014.
Chia-Ying Lee and James Glass. A nonparametric Bayesian approach to acoustic model discovery.
In Proc. Annual Meeting of the Association for Computational Linguistics (ACL), 2012.
Chia-Ying Lee, Timothy J. O’Donnell, and James Glass. Unsupervised lexicon discovery from
acoustic input. In Proc. Annual Meeting of the Association for Computational Linguistics (ACL),
2015.
13
Published as a conference paper at ICLR 2020
Kenneth Leidal, David Harwath, and James Glass. Learning modality-invariant representations for
speech and images. In Proc. IEEE Workshop on Automatic Speech Recognition and Understand-
ing (ASRU), 2017.
M. Paul Lewis, Gary F. Simon, and Charles D. Fennig. Ethnologue: Languages of the World,
Nineteenth edition. SIL International. Online version: http://www.ethnologue.com, 2016.
Andy T Liu, Po-chun Hsu, and Hung-yi Lee. Unsupervised end-to-end learning of discrete linguistic
units for voice conversion. In Proc. Annual Conference of International Speech Communication
Association (INTERSPEECH), 2019.
Danny Merkx, Stefan L. Frank, and Mirjam Ernestus. Language learning using speech to image
retrieval. In Proc. Annual Conference of International Speech Communication Association (IN-
TERSPEECH), 2019.
Benjamin Milde and Chris Biemann. Unspeech: Unsupervised speech context embeddings. In Proc.
Annual Conference of International Speech Communication Association (INTERSPEECH), 2018.
Lucas OndeL LUkas BUrgeL and Jan Cernocky. Variational inference for acoustic unit discovery.
In Proc. of the Workshop on Spoken Language Technologies for Under-Resourced Languages
(SLTU), 2016.
Alex Park and James Glass. Towards unsupervised pattern discovery in speech. In Proc. IEEE
Workshop on Automatic Speech Recognition and Understanding (ASRU), 2005.
Alex Park and James Glass. Unsupervised pattern discovery in speech. IEEE Transactions on Audio,
Speech and Language Processing, 2008.
Ankita Pasad, Bowen Shi, Herman Kamper, and Karen Livescu. On the contributions of visual
and textual supervision in low-resource semantic speech retrieval. In Proc. Annual Conference of
International Speech Communication Association (INTERSPEECH), 2019.
Santiago Pascual, Mirco Ravanelli, Joan Serra, Antonio Bonafonte, and Yoshua Bengio. Learning
problem-agnostic speech representations from multiple self-supervised tasks. In Proc. Annual
Conference of International Speech Communication Association (INTERSPEECH), 2019.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. arXiv preprint arXiv:1906.00446, 2019.
Deb Roy. Grounded spoken language acquisition: Experiments in word learning. IEEE Transactions
OnMultimedia, 5(2):197-209, 2003.
Deb Roy and Alex Pentland. Learning words from sights and sounds: a computational model.
Cognitive Science, 26:113-146, 2002.
Odette Scharenborg, Laurent Besacier, Alan W. Black, Mark Hasegawa-Johnson, Florian Metze,
Graham Neubig, Sebastian Stuker, Pierre Godard, Markus Muller, Lucas Ondel, Shruti Palaskar,
Philip Arthur, Francesco Ciannella, Mingxing Du, Elin Larsen, Danny Merkx, Rachid Riad, Lim-
ing Wang, and Emmanuel Dupoux. Linguistic unit discovery from multi-modal inputs in unwrit-
ten languages: Summary of the ”Speaking Rosetta” JSALT 2017 workshop. In Proc. International
Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.
Thomas Schatz, Vijayaditya Peddinti, Francis Bach, Aren Jansen, Hynek Hermansky, and Em-
manuel Dupoux. Evaluating speech features with the minimal-pair ABX task: Analysis of the
classical MFC/PLP pipeline. In Proc. Annual Conference of International Speech Communica-
tion Association (INTERSPEECH), 2013.
Man-Hung. Siu, Herb Gish, Arthur Chan, William Belfield, and Steve Lowe. Unsupervised training
of an HMM-based self-organizing unit recognizer with applications to topic classification and
keyword discovery. Computer Speech and Language, 28(1):210-223, 2014.
Dldac Sur´s, Adria Recasens, David Bau, David Harwath, James Glass, and Antonio Torralba.
Learning words by drawing images. In Proc. IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2019.
14
Published as a conference paper at ICLR 2020
Gabriel Synnaeve, Maarten Versteegh, and Emmanuel Dupoux. Learning words from images and
speech. In Proc. Neural Information Processing Systems (NeurIPS), 2014.
R. Thiolliere, E. Dunbar, G. Synnaeve, M. Versteegh, and E. Dupoux. A hybrid dynamic time
warping-deep neural network architecture for unsupervised acoustic modeling. In Proc. Annual
Conference of International Speech Communication Association (INTERSPEECH), 2015.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. In Proc. Neural Information Processing Systems (NeurIPS), 2017.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive Predic-
tive coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Emmanuel Dupoux. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08: HLT, Short Papers, 2008.
Martin Versteegh, Roland Thiolliere, Thomas Schatz, Xuan Nga Cao, Xavier Anguera, Aren Jansen,
and Emmanuel Dupoux. The zero resource speech challenge 2015. In Proc. Annual Conference
of International Speech Communication Association (INTERSPEECH), 2015.
Virginia de Sa. Learning classification with unlabeled data. In Proc. Neural Information Processing
Systems (NeurIPS), 1994.
Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest
neighbor classification. Journal of Machine Learning Research (JMLR), 2009.
Yaodong Zhang and James Glass. Unsupervised spoken keyword spotting via segmental dtw on
gaussian posteriorgrams. In Proc. IEEE Workshop on Automatic Speech Recognition and Under-
standing (ASRU), 2009.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep
features for scene recognition using places database. In Proc. Neural Information Processing
Systems (NeurIPS), 2014.
15
Published as a conference paper at ICLR 2020
A Appendix
A. 1 Varying the codebook size.
In Table 4, We examine the impact of varying the codebook size of model "0 → {2}" from 128
through 2048. We find that the ABX score is best for 1024 codebook vectors, although the perfor-
mance is quite good for all models. Unsurprisingly, models With smaller codebooks also achieve
loWer bitrates.
Table 4: ABX scores and bitrates for various codebook sizes on the clean ZeroSpeech19 English
test set, using the “0 → {2}” model.
Codebook size	R@10	ABX	Bitrate	RLE-Bitrate	Segment-ABX	Segment-Bitrate
128	.772	14.25	295.65	212.27	15.42	179.38
256	.756	12.95	341.18	260.10	14.21	228.07
512	.761	12.59	363.95	288.64	13.10	259.94
1024	.760	11.79	390.61	317.66	12.66	289.11
2048	.768	12.41	360.04	283.68	13.15	254.23
Table 5: ABX scores on the Ze-
roSpeech 19 English test set using
features derived from the output of
the Res3 block of the ResDAVEnet
audio branch (pre-quantization).
Model ID	Res3 ABX
“0”	10.86
“0 → {2}”	11.61
“0 → {3}”	10.91
“0 → {2, 3}”	12.68
"{2}”	11.45
"{2}→{2, 3}”	11.37
"{3}”	32.24
“{3}→{2, 3}”	28.33
A.2 The Impact of the VQ Training Curriculum on the Localization of Word
Detectors
In Section 4.2, We shoWed that cold-start training of the VQ3
layer caused its codebook vectors to specialize as Word de-
tectors, Whereas Warm-start training did not. Our subsequent
experiments (Table 6) revealed that When adding a third quan-
tization layer at the Res4 position to a model that did not learn
Word detectors at VQ3, the VQ4 layer did in fact learn many
Word detectors. This suggests implicit Word recognition ability
can be localized at different layers in the ResDAVEnet audio
model, and exactly Where it emerges depends upon the VQ
training curriculum. We hypothesize that this is due in part to
tWo factors:
1.	In a Warm-start model, Whatever type of informa-
tion (subWord-like, Word-like) the continuous model
learned to encode at a particular convolutional layer
(or residual block) does not change after a quantizer
is appended to that layer.
2.	In a cold-start model, each active quantization layer
forms a potential bottleneck, restricting the amount of
information that is able to pass through to subsequent
layers.
According to this hypothesis, if Word-level recognition tends to emerge at a particular layer in an
unconstrained netWork With no quantization bottleneck, it Will stay there When quantization is in-
troduced for fine-tuning. HoWever, When a quantization bottleneck is introduced from the very
beginning of training, the gradient floWing doWn into the loWer netWork layers is more constrained
during the initial training epochs (When the gradient tends to be the largest). This may have the ef-
fect of steering the optimizer into a different part of the parameter space, in Which Word recognition
occurs at a different layer than it otherWise Would.
We present results from tWo experiments that support this vieW. In Table 5, We shoW the ABX
scores of the Res3 layer prior to quantization (if present) during the course of three different training
curricula resulting in a “{2, 3}” final model. We observe that the ABX error rate changes very little
Within each individual curriculum. This indicates that the initial model sets the stage for Which layer
learns to capture phonetic information With the highest salience, and that subsequent training steps
do not tend to move this information elseWhere.
16
Published as a conference paper at ICLR 2020
Table 6: The number of codebook vectors at a particular VQ layer that learned to be a detector for
any word with an F1 score greater than 0.5.
# Quantized Layers	Model ID	VQ Layer	# Word Detectors (F1 > 0.5)
1	"{3}”	3	210
	“0 → {3}"	3	20
	“{2, 3}"	2 3	93 100
2	“0 → {2, 3}"	2 3	1 18
	“{2} → {2, 3}"	2 3	0 5
	“{3} → {2, 3}"	2 3	32 279
		2	0
3	“{2}一{2, 3}→{2, 3,4}“	3	8
		4	253
Table 6 displays the number of word detectors learned by various VQ layers across different training
curricula. Here, we claim that a codebook vector belonging to a particular VQ layer has learned to
be a word detector if its F1 score for any word appearing in the test set exceeds 0.5 (as measured on
the test set). There are several interesting things to note here. First, we only observe a significant
number of word detectors at the VQ3 layer when that layer is trained from a cold-start. Even when
adding a second VQ layer as in the “{3} → {2, 3}” model, these detectors remain at VQ3.
Jointly training VQ2 and VQ3 together from a cold-start results in the word detectors being di-
vided between those layers. While this experiment demonstrates that it is possible to jointly train
two quantizers at once, the {2, 3} model learned the smallest total number of word detectors of
any model. Additionally, we were unable to successfully train a cold-start {2, 3, 4} model; these
experiments suggest that training quantizers one by one may be easier in general.
For all models beginning from a "0" or “{2}" initial model, We do not observe any word detectors
at either VQ2 or VQ3. However, a third quantizer at the VQ4 position in the “{2} → {2, 3} →
{2, 3, 4}" model was able to capture words. We hypothesize that in all models not trained from a
“{3}" initialization, word recognition is implicitly learned by the Res4 layer, and adding a quantizer
to the output of this layer serves to make the categorical nature of those representations explicit.
A.3 Word Detector Tables for Various Models
In Table 7, we show a sampling of 50 word-detecting codebook entries from the VQ layer of the
“{3} → {2, 3}" model (many word detectors learned). Analagous results for the “{2} → {2, 3}"
model (few word detectors learned) are shown in Table 8.
A.4 Unit Visualization for Individual Caption Spectrograms
To provide a better intuitive understanding of what the units learned by our models look like, in
Figures 5, 6, and 7, we display speech spectrograms for several Places caption fragments. Along
with each spectrogram we display the time-aligned, ground-truth, word-level text (top transcription),
the inferred unit sequence for the VQ4 layer (middle transcription), and the unit sequence for the
VQ3 (bottom transcription) layer. All VQ unit alignments in these figures are derived from the
“{2} → {2, 3} → {2, 3, 4}" model.
17
Published as a conference paper at ICLR 2020
Table 7: Performance of the VQ3 layer from the “{3} → {2, 3}” model when codes are treated as
word detectors. Codes are ranked by the highest F1 score among the retrieved words for a given
code. Word hypotheses for a given code are ranked by the F1 score.
rank	code	word	Top Hypotheses		R	occ	Second Hypotheses				
			F1	P			word	F1	P	R	occ
											
1	918	pantry	90.67	88.29	93.18	41	spice	3.96	2.20	20.00	1
2	596	kitchen	90.08	91.59	88.63	304	countertop	1.64	0.84	29.63	8
3	88	classroom	88.97	89.05	88.89	72	classrooms	5.01	2.57	100.00	2
4	58	baseball	88.71	88.63	88.78	182	player	3.01	1.65	17.11	13
5	706	background	87.86	91.93	84.14	838	ground	0.58	0.39	1.18	4
6	736	museum	87.35	93.44	82.00	41	museums	5.47	2.81	100.00	1
7	274	subway	87.26	88.34	86.21	75	assembly	5.32	2.85	40.00	4
8	116	construction	87.07	89.78	84.52	131	constructed	2.43	1.25	38.46	5
9	892	walking	87.06	87.57	86.55	412	walk	7.07	3.97	31.94	23
10	557	concrete	86.53	90.98	82.50	99	concur	1.21	0.61	100.00	1
11	48	desert	86.50	90.30	83.01	171	dozen	2.76	1.49	18.18	2
12	534	background	86.18	81.95	90.86	905	back	8.95	5.83	19.34	64
13	44	patio	85.82	90.87	81.29	113	patios	1.56	0.79	100.00	1
14	625	background	85.17	92.92	78.61	783	back	1.63	1.01	4.23	14
15	732	closet	84.92	94.64	77.01	67	closets	4.96	2.68	33.33	2
16	30	waterfall	84.90	75.73	96.61	57	waterfalls	14.26	7.68	100.00	7
17	388	courtyard	84.89	92.16	78.69	48	graveyard	5.50	3.24	18.18	4
18	560	hospital	84.70	91.48	78.85	41	horses	1.55	1.92	1.30	1
19	18	driveway	84.56	90.10	79.66	47	driveways	3.52	1.82	50.00	1
20	598	palm	84.39	82.08	86.84	99	plum	1.57	0.79	100.00	1
21	85	yellow	84.30	83.93	84.66	574	yellowish	1.98	1.00	100.00	7
22	584	playground	84.18	77.37	92.31	36	play	6.32	4.75	9.43	5
23	162	stadium	83.82	84.50	83.15	74	boardwalk	9.12	4.91	63.16	12
24	769	bamboo	83.79	93.68	75.79	72	baboons	2.03	1.03	100.00	1
25	193	small	83.55	90.46	77.63	791	smaller	2.15	1.10	50.00	15
26	412	podium	83.53	76.73	91.67	22	auditorium	8.69	5.82	17.14	6
27	108	highway	83.52	79.58	87.88	58	highlights	5.44	2.87	50.00	3
28	394	church	83.34	75.98	92.28	227	religious	6.34	3.45	39.39	13
29	661	distance	83.32	78.96	88.19	351	lounge	1.63	0.86	14.71	5
30	708	distance	82.97	96.32	72.86	290	farmland	1.35	0.68	55.56	5
31	14	gallery	82.97	85.08	80.95	17	art	11.39	12.15	10.71	9
32	996	large	82.95	87.05	79.21	1753	very	2.83	1.72	8.01	94
33	944	cathedral	82.78	79.22	86.67	52	feed	2.74	1.41	50.00	1
34	122	purple	82.63	91.98	75.00	138	proportion	1.66	0.84	50.00	1
35	630	trees	82.52	80.39	84.77	1258	tree	15.48	9.47	42.33	171
186	375	boy	69.90	65.45	75.00	93	boys	20.87	13.09	51.43	18
187	634	ground	69.78	73.92	66.08	224	playground	7.45	3.94	69.23	27
188	69	courtyard	69.55	57.98	86.89	53	plaza	28.89	19.87	52.94	9
189	281	wooden	69.50	57.89	86.92	525	wood	23.55	14.52	62.20	153
190	812	lighthouse	69.41	59.74	82.81	53	lighthouses	11.69	6.21	100.00	5
191	225	house	69.13	61.59	78.78	516	houses	18.29	10.31	80.73	88
192	705	dark	69.11	68.57	69.66	186	darker	3.05	1.56	75.00	6
193	980	building	69.10	77.48	62.35	1161	buildings	25.72	15.71	70.78	281
194	844	grass	69.01	61.85	78.04	455	grassy	21.70	12.42	85.83	109
195	446	lake	68.69	78.63	60.98	125	late	5.02	2.90	18.52	5
196	182	trash	68.64	66.79	70.59	48	boulders	10.16	6.27	26.67	4
197	437	photograph	68.63	59.06	81.89	588	photographs	30.56	18.46	88.73	181
198	237	lobby	68.43	56.77	86.11	31	waiting	9.93	7.86	13.46	14
199	829	shirt	68.41	71.49	65.58	322	shirts	18.28	10.37	76.79	43
200	59	grass	68.31	56.53	86.28	503	grassy	15.30	8.67	65.35	83
18
Published as a conference paper at ICLR 2020
Table 8: Performance of the VQ3 layer from the “{2} → {2, 3}” model when codes are treated as
word detectors. Codes are ranked by the highest F1 score among the retrieved words for a given
code. Word hypotheses for a given code are ranked by the F1 score.
rank	code	word	Top Hypotheses			occ	Second Hypotheses				
			F1	P	R		word	F1	P	R	occ
1	924	people	76.71	67.49	88.85	1665	computer	2.17	1.12	40.40	40
2	749	white	76.47	66.92	89.21	2265	one	4.15	2.50	12.14	134
3	530	building	75.47	64.84	90.28	1681	buildings	23.93	13.81	89.67	356
4	505	blue	59.12	46.90	79.96	1093	pool	10.74	5.89	60.80	152
5	581	snow	57.61	41.77	92.83	466	snowy	16.63	9.12	94.50	103
6	778	building	52.10	36.71	89.69	1670	buildings	14.30	7.78	88.16	350
7	144	with	49.12	41.58	59.99	3386	wooden	6.32	3.34	60.60	366
8	299	small	47.83	32.78	88.42	901	snow	30.55	18.35	91.24	458
9	550	large	45.13	30.50	86.76	1920	car	8.57	4.52	82.13	216
10	76	trees	44.82	29.76	90.77	1347	tree	15.21	8.31	89.36	361
11	831	water	41.84	27.50	87.43	1210	wall	17.57	9.87	79.97	491
12	1015	large	39.59	26.06	82.29	1821	cars	6.58	3.42	84.21	224
13	80	red	39.15	26.13	78.05	992	bed	9.14	4.91	66.67	168
14	719	woman	38.71	24.95	86.31	687	women	7.75	4.09	72.41	126
15	614	people	37.71	25.10	75.83	1421	table	22.10	12.75	82.93	656
16	816	water	37.71	24.25	84.75	1173	river	5.72	2.97	80.22	215
17	457	sky	35.71	23.20	77.47	540	skies	11.81	6.33	88.12	141
18	480	has	34.88	25.01	57.64	1204	house	9.88	5.48	50.38	330
19	245	yellow	34.14	21.17	88.05	597	flowers	13.44	7.27	89.43	237
20	968	picture	34.11	22.32	72.33	1686	pictures	16.79	9.38	79.77	698
21	985	trees	33.88	22.04	73.25	1087	tree	10.96	5.93	72.52	293
22	536	man	33.53	20.71	88.06	1128	standing	9.06	4.86	66.17	532
23	0	black	33.49	20.81	85.77	1163	background	21.15	12.28	76.20	759
24	815	with	33.21	22.76	61.36	3463	white	8.60	4.85	38.05	966
25	293	large	33.13	23.80	54.50	1206	bridge	19.09	10.78	83.18	371
26	870	trees	32.87	20.79	78.44	1164	train	12.97	7.00	88.03	353
27	153	yellow	32.42	19.94	86.73	588	area	15.39	9.47	40.92	354
28	243	front	32.13	20.97	68.69	895	from	14.34	8.51	45.49	358
29	538	black	31.40	19.64	78.24	1061	glass	8.28	4.36	82.90	223
30	526	small	31.34	19.08	87.83	895	large	5.91	4.04	11.03	244
31	395	picture	31.32	20.90	62.46	1456	pictures	13.98	7.80	67.54	591
32	133	white	29.82	18.98	69.55	1766	black	16.34	9.32	66.37	900
33	715	white	29.45	19.73	58.05	1474	like	9.92	5.89	31.29	388
34	39	picture	29.37	22.51	42.26	985	like	9.82	5.79	32.26	400
35	740	trees	29.31	18.52	70.35	1044	showcasing	3.88	2.00	61.51	155
186	129	area	6.62	3.69	32.37	280	snow	6.08	3.23	51.20	257
187	288	picture	6.61	3.76	27.63	644	pictures	4.91	2.58	52.91	463
188	374	there’s	6.61	3.73	28.71	775	that	5.14	2.91	21.81	614
189	1003	front	6.59	3.65	34.00	443	some	4.56	2.64	16.84	345
190	396	multi	6.45	6.25	6.67	1	towers	5.41	6.25	4.76	1
191	790	photo	6.23	3.35	44.27	247	by	5.66	3.06	37.60	229
192	522	field	6.14	3.22	63.22	385	photo	6.01	3.19	51.79	289
193	42	man	5.96	3.19	45.12	578	middle	3.85	1.98	68.11	314
194	801	sitting	5.91	3.15	48.56	405	with	5.00	3.65	7.92	447
195	181	trees	5.89	3.16	43.19	641	with	5.13	3.22	12.72	718
196	791	with	5.84	3.58	15.86	895	that	3.21	1.79	15.88	447
197	975	purple	5.72	2.99	64.13	118	parked	4.67	2.42	65.93	120
198	38	structure	5.62	6.20	5.14	13	graph	4.03	2.19	25.00	2
199	432	parking	5.61	2.90	83.12	133	park	5.56	2.87	82.89	126
200	329	standing	5.60	2.97	49.63	399	woman	3.09	1.62	34.67	276
19
Published as a conference paper at ICLR 2020
Figure 5: Two different captions containing the phrase “many people.” In both cases, the VQ4 layer
infers the same unit sequence (872, 360, 712, middle transcription) beneath the phrase. The VQ3
units are somewhat noisier, but contain the common subsequence (956, 265, 80, 401, 262, 762, 246,
774, 828, 386, bottom transcription).
20
Published as a conference paper at ICLR 2020
Figure 6: Two different captions containing word “train”. In both cases, the VQ4 layer infers the
same unit sequence (680, 248, top transcription) surrounding the word “train”. The VQ3 alignments
contain the same common subsequence (358, 306, 908, 564, 950, 770, bottom transcription). Notice
that the same (358, 306) VQ3 unit sequence is aligned to the /tr/ phone cluster at the beginning of
both instances of the word “train,” as well as the /tr/ at the beginning of both instances of “tree” in
Figure 7. Unit 358 is also found covering the /tr/ at the beginning of the word “tracks” in the topmost
spectrogram (although unit 306 is absent in this case).
21
Published as a conference paper at ICLR 2020
Figure 7: Two different captions containing word “trees”. In both cases, the VQ4 layer infers
the same unit sequence (8, 412, 50, top transcription) surrounding the word “trees”. The VQ3
alignments contain the same common subsequence (358, 306, 648, 677, 730, bottom transcription).
Notice that the (358, 306) VQ3 unit sequence is aligned to the /tr/ phone cluster at the beginning of
both instances of “trees,” and this same unit sequence is inferred for the /tr/ phone sequence in both
instances of “train” in Figure 6.
22