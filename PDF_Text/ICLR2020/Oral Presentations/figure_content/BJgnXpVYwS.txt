Figure 1: Gradient norm vs local gradient Lipschitz constant on a log-scale along the training trajectory forAWD-LSTM (Merity et al., 2018) on PTB dataset. The colorbar indicates the number of iterations duringtraining. More experiments can be found in Section 5. Experiment details are in Appendix H.
Figure 2:	Gradient norm vs smoothness on log scale for LM training. The dot color indicates the iterationnumber Darker ones correspond to earlier iterations Note that the spans of X and y axis are not fixed(SseUSOOESBO-UORB」*O	1	O	1	O	1log(gradient norm)	log(gradient norm)	log(gradient norm)(a) SGD with momentum. (b) Learning rate 1, without clipping. (c) Learning rate 5, with clipping.
Figure 3:	Gradient norm vs smoothness on log scale for ResNet20 training. The dot color indicates theiteration number.
Figure 4: Training and validation loss obtained with different training methods for LSTM and ResNettraining. The validation loss plots the cross entropy. The training loss additionally includes the weightregularization term. In the legend, ‘lr30clip0.25’ denotes that clipped SGD uses step size 30 and that theL2 norm of the stochastic gradient is clipped by 0.25. In ResNet training, we threshold the stochasticgradient norm at 0.25 when clipping is applied.
Figure 5: Auxiliary plots for Figure 2a. The left subfigure shows the values scattered on linear scale.
Figure 6: Estimated gradient norm and smoothness using 10% data versus all data. The values arecomputed from checkpoints of the LSTM LM model in the first epoch. This shows that statisticsevaluated from 10% of the entire dataset provides accurate estimation.
Figure 7:	An synthetic experiment to optimize f (x) = x4. (a) Gradient descent with different stepsize. (b) Clipped gradient descent with different step size and threshold = 0.01.
Figure 8:	(a) Gradient norm. (b) Loss curves. (c)The scatter points of smoothness vs gradient normfor the model trained with gradient descent.(d)The scatter points of smoothness vs gradient normfor the model trained with clipped GD.
