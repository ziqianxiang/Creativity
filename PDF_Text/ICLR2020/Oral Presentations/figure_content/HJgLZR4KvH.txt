Figure 1: A humanoid agent discovers diverse locomotion primitives without any reward using DADS. Weshow zero-shot generalization to downstream tasks by composing the learned primitives using model predic-tive control, enabling the agent to follow an online sequence of goals (green markers) without any additionaltraining.
Figure 2: The agent π interaCts with the environment to produCe a transition s → s0 . IntrinsiC reward isComputed by Computing the transition probability under q for the Current skill z, Compared to random samplesfrom the prior p(z). The agent maximizes the intrinsiC reward Computed for a batCh of episodes, while qmaximizes the log-probability of the aCtual transitions of (s, z) → s0.
Figure 3: At test time, the planner executes simulates the transitions in environment using skill-dynamics q,and updates the distribution of plans according to the computed reward on the simulated trajectories. After afew updates to the plan, the first primitive is executed in the environment using the learned agent π .
Figure 4: Skills learned on different MuJoCo environments in the OpenAI gym. DADS can discover diverseskills without any extrinsic rewards, even for problems with high-dimensional state and action spaces.
Figure 5: (Left, Centre) X-Y traCes of Ant skills and (Right) Heatmap to visualize the learned Continuous skillspaCe. TraCes demonstrate that the Continuous spaCe offers far greater diversity of skills, while the heatmapdemonstrates that the learned spaCe is smooth, as the orientation of the X-Y traCe varies smoothly as a funCtionof the skill.
Figure 6: (Top-Left) Standard deviation of Ant’s position as a function of steps in the environment, averagedover multiple skills and normalized by the norm of the position. (Top-Right to Bottom-Left Clockwise) X-Ytraces of skills learned using DIAYN with x-y prior, DADS with x-y prior and DADS without x-y prior, wherethe same color represents trajectories resulting from the execution of the same skill z in the environment. Highvariance skills from DIAYN offer limited utility for hierarchical control.
Figure 7: (Left) The results of the MPPI controller on skills learned using DADS-c (continuous primitives)and DADS-d (discrete primitives) significantly outperforms state-of-the-art model-based RL. (Right) Planningfor a new task does not require any additional training and outperforms model-based RL being trained for thespecific task.
Figure 8: (Left) A RL-trained meta-controller is unable to compose primitive learned by DIAYN to navigateAnt to a goal, while it succeeds to do so using the primitives learned by DADS. (Right) Goal-Conditioned RL(GCRL-dense/sparse) does not generalize outside its training distribution, while MPPI controller on learnedskills (DADS-dense/sparse) experiences significantly smaller degrade in performance.
Figure 9: Graphical model for the world P in whichthe trajectories are generated while interacting withthe environment. Shaded nodes represent the distri-butions we optimize.
Figure 10: Graphical model for the world N which isthe desired representation of the world.
Figure 11:	Graphical model for the world P rep-resenting the stationary state, action distribution.
Figure 12:	Graphical model for the world N usingwhich we is the representation we are interested in.
Figure 13: Interpolation in the continuous primitive space learned using DADS on the Ant environment cor-responds to interpolation in the trajectory space. (Left) Interpolation from z = [1.0, 1.0] (solid blue) toz = [-1.0, 1.0] (dotted cyan); (Middle) Interpolation from z = [1.0, 1.0] (solid blue) to z = [-1.0, -1.0](dotted cyan); (Right) Interpolation from z = [1.0, 1.0] (solid blue) to z = [1.0, -1.0] (dotted cyan).
Figure 14: (Left) Prediction error in the Ant’s co-ordinates (normalized by the norm of the actual position)for skill-dynamics. (Right) X-Y traces of actual trajectories (colored) compared to trajectories predicted byskill-dynamics (dotted-black) for different skills.
