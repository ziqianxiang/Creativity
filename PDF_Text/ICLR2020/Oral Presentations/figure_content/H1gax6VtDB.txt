Figure 1: The C-SWM model is composed of the following components: 1) a CNN-based objectextractor, 2) an MLP-based object encoder, 3) a GNN-based relational transition model, and 4) anobject-factorized contrastive loss. Colored blocks denote abstract states for a particular object.
Figure 2: Example observations from block pushing environments (a-b), Atari 2600 games (c-d) and a 3-body gravitational physics simulation (e). In the grid worlds (a-b), each block canbe independently moved into the four cardinal directions unless the target position is occupied byanother block or outside of the scene. Best viewed in color.
Figure 3: Discovered object masks (left) and direct visualization of the 2D abstract state spaces andtransition graphs for a single object (right) in the block pushing environments. Nodes denote stateembeddings obtained from a test set experience buffer with random actions and edges are predictedtransitions. The learned abstract state graph clearly captures the underlying grid structure of theenvironment both in terms of object-specific latent states and in terms of predicted transitions, butis randomly rotated and/or mirrored. The model further correctly captures that certain actions donot have an effect if a neighboring position is blocked by another object (shown as colored spheres),even though the transition model does not have access to visual inputs.
Figure 4: Qualitative results for 3-body physics environment for a single representative test setepisode (left) and for a dataset of 50 test episodes (right). The model learns to smoothly embed objecttrajectories, with the circular motion represented in the latent space (projected from four to twodimensions via PCA). In the abstract state transition graph, orange nodes denote starting states fora particular episode, green links correspond to ground truth transitions and violet links correspondto transitions predicted by the model. One trajectory (in the center) strongly deviates from typicaltrajectories seen during training, and the model struggles to predict the correct transition.
Figure 5: Abstract state transition graphs per object slot for a trained C-SWM model on the 3Dcubes environment (with all objects allowed to be moved, i.e., none are fixed in place). Edge colordenotes action type. The abstract state graph is nearly identical for each object, which illustrates thatthe model successfully represents objects in the same manner despite their visual differences.
Figure 6: Abstract state transition graphs per object slot for a trained SWM model without con-trastive loss, using instead a loss in pixel space, on the 3D cubes environment. Edge color denotesaction type.
Figure 7: Object filters (left) and abstract state transition graphs per object slot (right) for a trainedC-SWM model on unseen test instances of the 3-body physics environment (seed 1).
Figure 8: Object filters (left) and abstract state transition graphs per object slot (right) for a trainedC-SWM model on unseen test instances of the 3-body physics environment (seed 2).
Figure 9: Object filters (left) and abstract state transition graphs per object slot (right) for a trainedC-SWM model with K = 3 object slots on unseen test instances of the Atari Pong environment.
Figure 10: Object filters (left) and abstract state transition graphs per object slot (right) for a trainedC-SWM model with K = 3 object slots on unseen test instances of the Space Invaders environment.
Figure 11: Qualitative model comparison in pixel space on a hold-out test instance of the 2D shapesenvironment. We train a separate decoder model for 100 epochs on both the C-SWM and the WorldModel baseline using all training environment instances to obtain pixel-based reconstructions formultiple prediction steps into the future.
Figure 12: Quantitative model comparison inpixel space on a hold-out test set of the 2Dshapes environment. The plot shows meansquared reconstruction error (MSE) in pixelspace for multiple transition model predic-tion steps into the future (lower is better),averaged over 4 runs. Shaded area denotesstandard error.
Figure 13: Reconstructions from the la-tent code of a trained VAE-based WorldModel baseline.
