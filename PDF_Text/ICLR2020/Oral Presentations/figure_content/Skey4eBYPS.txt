Figure 1: (a) Illustration of the ConvCNP forward pass in the off-the-grid case and pseudo-code for(b) off-the-grid and (c) on-the-grid data. The function pos : R → (0, ∞) is used to enforce positivity.
Figure 2: Example functions learned by the AttnCNP (top row), and ConvCNP (bottom row),When trained on a Matern-2 kernel with length scale 0.25 (first and second column) and sawtoothfunction (third and fourth column). Columns one and three show the predictive posterior of themodels when data is presented in same range as training, with predictive posteriors continuing beyondthat range on either side. Columns two and four show model predictive posteriors when presentedwith data outside the training data range. Plots show means and two standard deviations.
Figure 3: Left and centre: two samples from the Lotka-Volterra process (sim). Right: ConvCNPtrained on simulations and applied to the Hudson’s Bay lynx-hare dataset (real). Plots show meansand two standard deviations.
Figure 4: Zero shot generalization to tasks that require translation equivariance.
Figure 5: Qualitative evaluation of the ConvCNP(XL). For each dataset, an image is randomlysampled, the first row shows the given context points while the second is the mean of the estimatedconditional distribution. From left to right the first seven columns correspond to a context set with 3,1%, 5%, 10%, 20%, 30%, 50%, 100% randomly sampled context points. In the last two columns, thecontext sets respectively contain all the pixels in the left and top half of the image. ConvCNPXL isshown for all datasets besides ZSMM, for which we show the fully translation equivariant ConvCNP.
Figure 6: Example functions learned by the (top) ConvCNP, (center) AttnCNP, and (bottom)CNP when trained on an EQ kernel (with length scale parameter 1). “True function” refers to thesample from the GP prior from which the context and target sets were sub-sampled. “Ground TruthGP” refers to the GP posterior distribution when using the exact kernel and performing posteriorinference based on the context set. The left column shows the predictive posterior of the modelswhen data is presented in same range as training. The centre column shows the model predictingoutside the training data range when no data is observed there. The right-most column shows themodel predictive posteriors when presented with data outside the training data range.
Figure 7: Example functions learned by the (top) ConvCNP, (center) AttnCNP, and (bottom) CNPwhen trained on a Matern-5/2 kernel (with length scale parameter 0.25). “True function” refers to thesample from the GP prior from which the context and target sets were sub-sampled. “Ground TruthGP” refers to the GP posterior distribution when using the exact kernel and performing posteriorinference based on the context set. The left column shows the predictive posterior of the modelswhen data is presented in same range as training. The centre column shows the model predictingoutside the training data range when no data is observed there. The right-most column shows themodel predictive posteriors when presented with data outside the training data range.
Figure 8: Example functions learned by the (top) ConvCNP, (center) AttnCNP, and (bottom)CNP when trained on a random sawtooth sample. The left column shows the predictive posterior ofthe models when data is presented in the same range as training. The centre column shows the modelpredicting outside the training data range when no data is observed there. The right-most columnshows the model predictive posteriors when presented with data outside the training data range.
Figure 9: AttnCNP performance on two samples from the Lotka-Volterra process (sim).
Figure 10: Samples from our generated Zero Shot Multi MNIST (ZSMM) data set.
Figure 11: Log-likelihood and qualitative comparisons between AttnCNP and ConvCNP on fourstandard benchmarks. The top row shows the log-likelihood distribution for both models. The imagesbelow correspond to the context points (top), ConvCNP target predictions (middle), and AttnCNPtarget predictions (bottom). Each column corresponds to a given percentile of the ConvCNPdistribution. AttnCNP could not be trained on CelebA64 due to its memory inefficiency.
Figure 12: Qualitative evaluation of a ConvCNP (center) and AttnCNP (right) trained on CelebA32and tested on a downscaled version (146 × 259) of Ellen’s Oscar selfie (DeGeneres, 2014) with 20%of the pixels as context (left).
Figure 13: First filter learned by ConvCNPXL, ConvCNP, and ConvCNP EQ for all our datasets.
Figure 14: Log-likelihood and qualitative results on ZSMM. The top row shows the log-likelihooddistribution for both models. The images below correspond to the context points (top), ConvCNPtarget predictions (middle), and ConvCNPXL target predictions (bottom). Each column correspondsto a given percentile of the ConvCNP distribution.
Figure 15: Effect of the receptive field size on ZSMM’s log-likelihood. The line plot shows the meanand standard deviation over 6 runs. The blue curve corresponds to a model with zero padding, whilethe orange one corresponds to “circular” padding.
