Figure 1: (a) To obtain an accuracy, the required rounds first decrease and then increase when weincrease the local steps E. (b) In Synthetic(0,0) dataset, decreasing the numbers of activedevices each round has little effect on the convergence process. (c) In mnist balanced dataset,Scheme I slightly outperforms Scheme II. They both performs better than the original scheme.
Figure 2: The left figure shows that the global objective value that FedAvg converges to is notoptimal unless E = 1. Once we decay the learning rate, FedAvg can converge to the optimal even ifE> 1.
Figure 3: The impact of K on four datasets. To show more clearly the differences between the curves,we zoom in the last few rounds in the upper left corner of the box.
Figure 4: The performance of four schemes on two synthetic datasets. The Scheme I performs stablyand the best. The original performs the second. The curve of the Scheme II fluctuates and has no signof convergence. Transformed Scheme II has a lower convergence rate than Scheme I.
