Figure 1: How well does meta-RL work? Aver-age returns on validation tasks compared for twoprototypical meta-RL algorithms, MAML (Finnet al., 2017) and PEARL (Rakelly et al., 2019),with those of a vanilla Q-learning algorithm namedTD3 (Fujimoto et al., 2018b) that was modified toincorporate a context variable that is a represen-tation of the trajectory from a task (TD3-context).
Figure 2: Average undiscounted return of TD3 andTD3-context compared with PEARL for validationtasks from four meta-RL environments. The agentfails to learn if the policy is conditioned only on the state.
Figure 3: Comparison of the average undiscounted return of MQL (orange) against existing meta-RL al-gorithms on continuous-control environments. We compare against four existing algorithms, namely MAML(green), RL2 (red), PROMP (purple) and PEARL (blue). In all environments except Walker-2D-Params andAnt-Goal-2D, MQL is better or comparable to existing algorithms in terms of both sample complexity and finalreturns.
Figure 4: Ablation studies to examine various components of MQL.
Figure 5: Evolution of λ and β(z) duringmeta-training.
Figure 6:	Comparison of the average return of MQL (orange) against existing PEARL algorithms (blue).
Figure 7:	(a,b) Comparison of the average return of MQL (orange) against TD3-context (blue). Fig. 7ashows the comparison with the same test protocol as the other experiments in this paper. In particular, we collect200 time-steps from the new task and use it for adaptation in both MQL and TD3-context. Since this task isparticularly hard, we also ran an experiment where 1200 time-steps (6 episodes) where results are shown inFig. 7b. In both cases, we see that MQL is better than TD3-context by a large margin (the standard deviationon these plots is high because the environment is hard). (c,d) Evolution of λ and β(z) during meta-training.
Figure 8: (a) Comparison of the average return of MQL (orange) against TD3-context (blue). For theseexperiments, we collected 800 time-steps from the new task from the same episode, the results are shownin Fig. 8a. We again notice that MQL results in slightly higher rewards than TD3-context in spite of the fact thatboth the algorithms suffer large degradation in performance as compared to Figs. 7a and 7b. (b) Evolution of λand β(z) during meta-training shows the evolution of the proximal penalty coefficient λ and the propensityscore β(z).
Figure 9: Ablation studies to examine various components of MQL. Fig. 9a shows that the adaptation inMQL in (18) and (19) improves performance. One reason for that is because test and training tasks in Walker-2D-Params are very similar as it shown in Fig. 10b. Next, we evaluate the importance of the additional data fromthe replay buffer in MQL. Fig. 9b compares the performance of MQL with and without updates in (19). Wesee that the old data, even if it comes from different tasks, is useful to improve the performance on top of (18).
Figure 10: Evolution of λ and β(z) during meta-training shows the evolution of the proximal penalty coeffi-cient λ and the propensity score β(z). We see in Fig. 10a that β(z) stays around 0.4 which demonstrates MQLautomatically adjusts the adaptation if the test task is different from the training tasks. Fig. 10b shows that testand training tasks are very similar as β(z) is around 0.6.
