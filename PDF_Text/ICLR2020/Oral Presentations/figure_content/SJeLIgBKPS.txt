Figure 1: (a) Training CNNs with and without bias on MNIST, using SGD with learning rate 0.01.
Figure 2: A plot for the Mexican Hat function f(u, v).
Figure 3: Training CNNs with and without bias on MNIST, using SGD with learning rate 0.01. Thetraining accuracy (left) increases to 100% after about 100 epochs, and the normalized margin withthe original definition (right) keeps increasing after the model is fitted.
Figure 4: Training CNNs with and without bias on MNIST, using SGD with the loss-based learningrate scheduler. The training accuracy (left) increases to 100% after about 20 epochs, and the nor-malized margin with the original definition (middle) increases rapidly after the model is fitted. Theright figure shows the change of the relative learning rate Î±(t) (see (27) for its definition) duringtraining.
Figure 5: Training VGGNet with and without bias on CIFAR-10, using SGD with learning rate 0.1.
Figure 6: Training VGGNet with and without bias on CIFAR-10, using SGD with the loss-basedlearning rate scheduler.
Figure 7: (Left). Training and test accuracy during training CNNs without bias on MNIST, usingSGD with the loss-based learning rate scheduler. Every number is averaged over 10 runs. (Right).
Figure 8: L2-robustness of the models of CNNs without bias trained for different number of epochs(see Table 1 for the statistics of each model). Figures on the first row show the robust accuracy on thetraining set, and figures on the second row show that on the test set. On every row, the left figure andthe right figure plot the same curves but they are in different scales. From model-1 to model-4,noticeable robust accuracy improvements can be observed. The improvement of model-5 uponmodel-4 is marginal or nonexistent for some , but the improvement upon model-1 is alwayssignificant.
