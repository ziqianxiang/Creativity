Figure 1: The best achievable accuracy across retraining times by one-shot pruning.
Figure 2: The best achievable accuracy across retraining times by iteratively pruning.
Figure 3:	Accuracy curves across networks and compression ratios using unstructured pruning.
Figure 4:	Accuracy curves across networks and compression ratios using structured pruning.
Figure 5: Accuracy versus Parameter-Efficiency tradeoff of our pruning algorithm.
Figure 6: Accuracy versus Parameter-Efficiency tradeoff of our pruning algorithm.
Figure 7: The best achievable accuracy across retraining times by one-shot pruning.
Figure 8:	The best achievable accuracy across retraining times by iteratively pruning.
Figure 9:	Accuracy curves across different networks and compressions using unstructured pruning.
Figure 10:	Accuracy curves across different networks and compressions using structured pruning.
Figure 11: Speedup over original network for different retraining techniques and networksIterative pruning FLOP speedup over fine-tuningLearning rate rewindingWeight rewindingFine-tuningReSNet-56 SPeedUP of best-validated FLOPS over fine-tuningover fine-tuningResNet-501.30×0.90×dnK10×dS1.00×0.90 ×ResNet-110 speedup of best-validated FLOPs1.30×1.56×	2.44×	3.81×	5.96×
Figure 12: Speedup over original network for different retraining techniques and networksF	Compression Ratio vs FLOPsIn the main body of the paper, we use the compression ratio as the metric denoting the Efficiencyof a given neural network. However, the compression ratio does not tell the full story: networks ofdifferent compression ratios can require different amounts of floating point operations (FLOPs) toperform inference. For instance, pruning a weight in the first convolutional layer of a ResNet resultsin pruning more FLOPs than pruning a weight in the last convolutional layer. Reduction in FLOPscan (but does not necessarily) result in wall clock speedup (Baghdadi et al., 2019). In this appendix,we analyze the number of FLOPs required to perform inference on pruned networks acquired throughfine-tuning and rewinding. Our methodology for one-shot pruning uses the same initial trainednetwork for our comparisons between all techniques, and prunes using the same pruning technique.
