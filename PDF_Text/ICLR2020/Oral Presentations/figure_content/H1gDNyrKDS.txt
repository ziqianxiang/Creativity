Figure 1: The poor cells standard DARTS finds on spaces S1-S4. For all spaces, DARTS choosesmostly parameter-less operations (skip connection) or even the harmful N oise operation. Shownare the normal cells on CIFAR-10; see Appendix G for reduction cells and other datasets.
Figure 2: Test regret of found architectures andvalidation error of the search model when runningDARTS on S5 and CIFAR-10. DARTS finds theglobal minimum but starts overfitting the architec-tural parameters to the validation set in the end.
Figure 3: (left) validation error of search model; (middle) test error of the architectures deemed byDARTS optimal (right) dominant eigenvalue of NQaLValid throughout DARTS search. Solid line andshaded areas show mean and standard deviation of 3 independent runs. All experiments conductedon CIFAR-10.
Figure 4: Correlation between dominanteigenvalue of NQαLvalid and test error of cor-responding architectures.
Figure 6: Local average (LA) of the dominant eigenvalue λαmax throughout DARTS search. Markersdenote the early stopping point based on the criterion in Section 4.3. Each line also corresponds toone of the runs in Table 1.
Figure 5: (a) Hypothetical illustration of theloss function change in the case of flat vs.
Figure 7: Effect of regularization strength via ScheduledDropPath (during the search phase) on thetest performance of DARTS (solid lines) and DARTS-ES (dashed-lines). Results for each of thesearch spaces and datasets.
Figure 8: Effect of L2 regularization of the inner objective during architecture search for DARTS(solid lines) and DARTS-ES (dashed).
Figure 9: Search space S1.
Figure 10: Test regret and validation error of the search (one-shot) model when running DARTSon S5 and CIFAR-10 with different L2 regularization values. The architectural parameters’ overfitreduces as we increase the L2 factor and successfully finds the global minimum. However, we noticethat the architectural parameters start underfitting as we increase to much the L2 factor, i.e. bothvalidation and test error increase.
Figure 11: Test errors of architectures along with the validation error of the search (one-shot) modelfor each dataset and space when scaling the ScheduledDropPath drop probability. Note that theseresults (blue lines) are the same as the ones in Figure 8.
Figure 12:	Test errors of architectures along with the validation error of the search (one-shot) modelfor each dataset and space when scaling the L2 factor. Note that these results (blue lines) are thesame as the ones in Figure 7.
Figure 13:	Local average of the dominant EV λαmax throughout DARTS search (for different droppath prob. values). Markers denote the early stopping point based on the criterion in Section 4.3.
Figure 14: Effect of L2 regularization no the EV trajectory. The figure is analogous to Figure 13.
Figure 15: Effect of ScheduledDropPath and Cutout on the full eigenspectrum of the Hessian at theend of architecture search for each of the search spaces. Since most of the eigenvalues after the 30-thlargest one are almost zero, we plot only the largest (based on magnitude) 30 eigenvalues here. Wealso provide the eigenvalue distribution for these 30 eigenvalues. Notice that not only the dominanteigenvalue is larger when dp = 0 but in general also the others.
Figure 16: Effect of L2 regularization on the full eigenspectrum of the Hessian at the end of archi-tecture search for each of the search spaces. Since most of the eigenvalues after the 30-th largest oneare almost zero, we plot only the largest (based on magnitude) 30 eigenvalues here. We also providethe eigenvalue distribution for these 30 eigenvalues. Notice that not only the dominant eigenvalue islarger When L? = 3 ∙ 10-4 but in general also the others.
Figure 17: Drop in accuracy after discretizing the search model for different spaces, datasets anddrop path regularization strengths.. Example of some of the settings from Section 5.
Figure 18: Effect of more regularization on the performance of found architectures by DARTS.
Figure 19: Performance of recurrent cells found with different L2 regularization factors on the innerobjective on PTB. We run DARTS 4 independent times with different random seeds, train each ofthem from scratch with the evaluation settings for 1600 epochs and report the median test perplexity.
Figure 20:	Reduction cells found by DARTS when ran on CIFAR-10 with its default hyperparame-ters on spaces S1-S4. These cells correspond with the normal ones in Figure 1.
Figure 21:	Normal cells found by DARTS on CIFAR-100 and SVHN when ran with its defaulthyperparameters on spaces S1-S4. Notice the dominance of parameter-less operations such as skipconnection and pooling ops.
Figure 22:	Reduction cells found by DARTS on CIFAR-100 and SVHN when ran with its defaulthyperparameters on spaces S1-S4.
Figure 23:	Normal cells found by DARTS-ES when ran with DARTS default hyperparameters onspaces S1-S4.
Figure 24:	Reduction cells found by DARTS-ES when ran with DARTS default hyperparameters onspaces S1-S4.
Figure 25:	Cells found by AutoDispNet when ran on S6-d. These cells correspond to the results foraugmentation scale 0.0 of Table 2.
Figure 26:	Cells found by AutoDispNet when ran on S6-d. These cells correspond to the results foraugmentation scale 2.0 of Table 2.
Figure 27:	Cells found by AutoDispNet when ran on S6-d. These cells correspond to the results forL2 = 3 ∙ 10-4 of Table 2.
Figure 28:	Cells found by AutoDispNet when ran on S6-d. These cells correspond to the results forL2 = 81 ∙ 10-4 ofTable 2.
Figure 29:	Normal (top row) and reduction (bottom) cells found by DARTS on CIFAR-10 when ranwith its default hyperparameters on spaces S1-S4. Same as Figure 1 but with different random seed(seed 2).
Figure 30:	Normal (top row) and reduction (bottom) cells found by DARTS on CIFAR-10 when ranwith its default hyperparameters on spaces S1-S4. Same as Figure 1 but with different random seed(seed 3).
