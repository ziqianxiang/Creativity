Figure 1: (a) Feature-embedding and (b) Target-embedding autoencoders. Solid lines correspond to the (prim-ary) prediction task; dashed lines to the (auxiliary) reconstruction task. Shared components are involved in both.
Figure 2: Functions and objectives in (a) FEAs and (b) TEAs. Blue and red identify supervised and representa-tion learning components. FEAs are parameterized by (Φ, Wd, Wr) of (φ, d, r), and TEAs by (Θ, Wu , We)of (θ, u, e). Solid lines indicate forward propagation of data; dashed lines indicate backpropagation of gradients.
Figure 3: Sensitivities on '2 -regularization coefficient V, Strength-of-prior coefficient λ, and training size N fordirect prediction (REG) and with target-embedding (TEA) on linear model with UKCF. For sensitivities on λ,we perform joint training only, so that we isolate the effect of the joint reconstruction task (i.e. removing theconfounding effect of staged training). Standard errors are indicated with shaded regions. For full results, seeTables 25-28 for sensitivities on V, Tables 29-30 for sensitivities on λ, and Tables 31-34 for Sensitivites on N.
Figure 4: TEAs consist of a shared forward model θ, upstream predictor u, and target-embeddingfunction e, parameterized by (Θ, Wu, We). Blue and red respectively identify the supervised andrepresentation learning components in each arrangement. Solid lines indicate forward propagation ofdata; dashed lines indicate backpropagation of gradients. (a) First, the autoencoding components e, θare trained to learn target representations. (b) Next, using the inputs, the prediction arm u is trained toregress the learned embeddings generated by the encoder. (c) Finally, all three components are jointlytrained on both prediction and reconstruction losses. (d) In the indirect variant (Girdhar et al., 2016),22Published as a conference paper at ICLR 2020(e) Training (Stage 3), Tea(lp)Figure 4: (continued) the predictor continues to regress the learned embeddings, and the latent lossbackpropagates through both u and e (TEA(L)). (e) The TEA(LP) variant combines the previous two:both the latent loss and prediction loss are trained jointly together with the reconstruction loss. (f) Atinference time, the target-embedding arm is dropped, leaving the hypothesis h = θ ◦ u for prediction.
Figure 4: (continued) the predictor continues to regress the learned embeddings, and the latent lossbackpropagates through both u and e (TEA(L)). (e) The TEA(LP) variant combines the previous two:both the latent loss and prediction loss are trained jointly together with the reconstruction loss. (f) Atinference time, the target-embedding arm is dropped, leaving the hypothesis h = θ ◦ u for prediction.
Figure 5: Component functions and training objectives for comparators in experiments. Blue and redrespectively identify the supervised and representation learning components in each arrangement.
Figure 5: (continued) model φ, downstream predictor d, and feature reconstructor r, parameterizedby (Φ, Wd, Wr). (c) TEAs consist of a shared forward model θ, upstream predictor u, and target-embedding function e, parameterized by (Θ, Wu, We). (d) As an additional sensitivity, F/TEAscombine the previous two, forcing intermediate representations to encode both features and targets.
Figure 6: Data generating process for synthetic example. Latent vectors p, u linearly generate feature vectorsx and target vectors y. In this situation, yP is in principle predictable from x, while yU is impossible to predict.
Figure 7: Synthetic scenario where prediction is directly at odds with reconstruction. The prior (that we can lev-erage compact and predictable representations of targets) is hugely incorrect in this case; as a result, not only istarget-autoencoding not beneficial—it is positively harmful. For Teas, we observe that as the strength-of-priorcoefficient λ increases, the overall prediction error actually increases (while the reconstruction error decreases).
