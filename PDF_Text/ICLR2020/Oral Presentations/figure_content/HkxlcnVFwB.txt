Figure 1: Stationary DistributionEstimation on BA and real-worldgraphs. Each plot shows the log KL-divergence of GenDICE and model-based method towards the number ofsamples.
Figure 2: Results on Taxi Domain. The plots show log MSE of the tabular estimator across differenttrajectory lengths, different discount factors and different behavior policies (x-axis).
Figure 3: Results on Reacher. The left three plots in the first row show the log MSE of estimated av-erage per-step reward over different numbers of trajectories, truncated lengths, and behavior policies(M1 and M2 mean off-policy set collected by multiple behavior policies with α = [0.0, 0.33] andα = [0.0, 0.33, 0.66]). The right two figures show the loss curves towards the optimization steps.
Figure 4: Results on HalfCheetah. Plots from left to the right show the log MSE of estimated averageper-step reward over different truncated lengths, numbers of trajectories, and behavior policies indiscounted and average reward cases.
Figure 5: Results of ablation study with different learning rates and activation functions. The plotsshow the log MSE of estimated average per-step reward over training and different behavior policies.
Figure 6: Results of ablation study with constraint penalty and discount factors. The left two figuresshow the effect of ratio constraint on estimating average per-step reward. The right three figuresshow the log MSE for average per-step reward over training and different discount factor γ.
Figure 7: Results of ablation studywith (a) different divergence and (b)weight of penalty λ. The plots showthe log K L-Divergence of OPR onBarabasi-Albert graph.
Figure 8: Degree statistics and visualization of different graphs.
Figure 9: Results on Cartpole. Each plot in the first row shows the estimated average step rewardover training and different behavior policies (higher α corresponds to a behavior policy closer to thetarget policy; the same in other figures); M1:a = [0.0,0.33]; M2: α = [0.0,0.33,0.66])E.2 Additional Results on Continuous ControlIn this section, we show more results on the continuous control tasks, i.e., HalfCheetah and Reacher.
Figure 10: Results on Reacher. Each plot in the first row shows the estimated average step rewardover training and different behavior policies (higher α corresponds to a behavior policy closer to thetarget policy; the same in other figures).
Figure 11: Results on Reacher. Each plot in the first row shows the estimated average step rewardover training and different behavior policies (higher α corresponds to a behavior policy closer to thetarget policy; the same in other figures).
Figure 12: Results on HalfCheetah. Each plot in the first row shows the estimated average stepreward over training and different behavior policies (higher α corresponds to a behavior policycloser to the target policy.
Figure 13: Results of ablation study with different learning rates and activation functions. The plotsshow the estimated average step reward over training and different behavior policies .
