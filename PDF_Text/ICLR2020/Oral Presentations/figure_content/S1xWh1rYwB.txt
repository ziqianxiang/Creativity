Figure 1: Exemplary heatmapof the Per-Sample Bottleneckfor the VGG-16.
Figure 2: Per-Sample Bottleneck: The mask (blue) contains an αi for each ri in the intermediatefeature maps R (green). The parameter α controls how much information is passed to the next layer.
Figure 3: Readout Bottleneck: In the first forward pass ①，feature maps are collected at differentdepths. The readout network uses a resized version of the feature maps to predict the parameters forthe bottleneck layer. In the second forward pass ②，the bottleneck is inserted and noise added. Allparameters of the analyzed network are kept fixed.
Figure 4: Effect of varying layer depth and β on the Per-Sample Bottleneck for the ResNet-50. Thecolor bars measure the bits per input pixel. The resulting output probability of the correct classp = p(y|x, β) is decreasing for higher β .
Figure 5: Heatmaps of all implemented methods for the VGG-16 (see Appendix A for more).
Figure 6: First row drop in SSIM score when network layers are randomized. Best viewed in color.
Figure 7: Mean MoRF and LeRFfor the Per-Sample Bottleneck. Thearea is the final degradation score.
Figure 8: Blue indicates negative relevance and red positive. The authors promise that the sampleswere picked truly randomly, no cherry-picking, no lets-sample-again-does-not-look-nice-enough.
Figure 9: Development of DKL(Q(Z|X)||Q(Z)) of the Per-SamPle Bottleneck for layer conv1_3of the ResNet-50. Red indiCate areas with maximal information flow and semi-transparent green forzero information flow. Top row: without smoothing the mask exhibits a grid structure. Bottom row:smoothing with σs = 2. The smoothing both prevents the artifacts and reduces overfitting to smallareas.
Figure 10: EffeCt of varying layer depth and β on the Per-Sample BottleneCk for the VGG-16. Theresulting ouput probability for the CorreCt Class is given as p.
Figure 11: MoRF and LeRF for the ResNet-50 network using 14x14 tiles.
Figure 12: MoRF and LeRF paths for the VGG-16 network using 14x14 tiles.
