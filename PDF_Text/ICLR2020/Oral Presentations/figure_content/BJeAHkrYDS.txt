Figure 1: VISR model diagram. In practice wt is also fed into ψ as an input, which alsoallows for GPI to be used (see Algorithm 1 in Appendix). For the random feature baseline,the discriminator q is frozen after initialization, but the same objective is used to train ψ .
Figure 2: (a) Median human-normalized scores over all 57 games, comparing VISR with andwithout GPI. Averaged over three seeds. (b) Human-normalized performance of VISR acrossall 57 Atari games after fast task inference. Reward regression in blue, random search inred. Regression outperforms search in all but two games. (c) Number of environment framesrequired for DQN to match VISR’s performance after 100k steps of RL. The green blockshows the games in which VISR outperforms DQN using 200 million transitions, the redblock shows the games in which VISR is outperformed by DQN using 1 million transitions,and yellow block shows the games that do not fall in either of the previous categories. Lightblue bars denote games in the 26 game set of Pathak et al. (2017).
Figure 3: VISR features φ learned by a variational distribution q(w|s) in a 10-by-10 gridworld.
Figure 4: 49 randomly sampledπM=TJ*F*%κ 一嘲reward functions learned by VISR.
Figure 6: Fast-inference performance during unsupervised training. Full VISR in blue,random feature VISR in red. x-axis is training time in millions of frames. y-axis is human-normalized score post-task inference.
