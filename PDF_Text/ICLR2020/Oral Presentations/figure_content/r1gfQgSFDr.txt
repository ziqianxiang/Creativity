Figure 1: Residual blocks used in the model. Convolutional layers have the same number of inputand output channels and no dilation unless stated otherwise. h - hidden layer representation, l -linguistic features, z - noise vector, m - channel multiplier, m = 2 for downsampling blocks (i.e. iftheir downsample factor is greater than 1) and m = 1 otherwise, M- G’s input channels, M = 2Nin blocks 3, 6, 7, and M = N otherwise; size refers to kernel size.
Figure 2: Multiple Random Window Discriminator architecture. The discriminator combines out-puts from 5 unconditional (uRWDs, left) and 5 conditional (cRWDs, right) discriminators; one ofeach group is detailed in the diagram. The number of downsampling blocks is fixed for uRWDs anddepends on the input window size ωk for cRWDs, see Table 3. Next to each block we present thedimensionality and frequency of its outputs; ch. - number of output channels.
Figure 3: Masking scheme for sampling different-length samples. Top: processing a batch of sam-ples of different lengths padded with zeros leads to interference between padding and non-paddingafter the second convolution, not seen during training. Bottom: masking after each convolutionensures that the meaningful input seen by each layer is padded with zeros only.
Figure 4: Learning curve for the GAN-TTS model in terms of cFDSD.
