Figure 1: Mogrifier with 5 rounds of updates. The previous state h0 = hPrev is transformed linearly (dashedarrows), fed through a sigmoid and gates XT = X in an elementwise manner producing x1. Conversely, thelinearly transformed x1 gates h0 and produces h2. After a number of repetitions of this mutual gating cycle, thelast values of h* and x* sequences are fed to an LSTM cell. Theprev subscript of h is omitted to reduce clutter.
Figure 2: “No-zigzag” Mogrifier for the ablation study. Gating is always based on the original inputs.
Figure 3: Perplexity vs the rounds r in the PTB ablation study.
Figure 4: Cross-entropy vs sequence length in the reverse copy task with i.i.d. tokens. Lower is better. TheMogrifier is better than the LSTM even in this synthetic task with no resemblance to natural language.
Figure 5: Average per-word validation cross-entropies for hyperparameter combinations in the neighbourhood ofthe best solution for a 2-layer LSTM with 24M weights on the Penn Treebank dataset.
Figure 6: Average per-word validation cross-entropies for hyperparameter combinations in the neighbour-hood of the best solution for a 2-layer Mogrifier LSTM with 24M weights on the Penn Treebank dataset.
