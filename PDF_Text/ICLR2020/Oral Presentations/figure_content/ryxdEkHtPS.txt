Figure 1:	Empirical variance of the estimated gradient (c.f. (1)) as a function of the number of state-action pairs used in estimation in the MuJoCo Humanoid task. We measure the average pairwisecosine similarity between ten repeated gradient measurements taken from the same policy, with the95% confidence intervals (shaded). For each algorithm, we perform multiple trials with the samehyperparameter configurations but different random seeds, shown as repeated lines in the figure.
Figure 2:	Convergence of gradient estimates (c.f. (1)) to the “true” expected gradient in the MuJoCoHumanoid task. We measure the mean cosine similarity between the “true” gradient approximatedusing ten million state-action pairs, and ten gradient estimates which use increasing numbers ofstate-action pairs (with 95% confidence intervals). For each algorithm, we perform multiple trialswith the same hyperparameter configurations but different random seeds. The vertical line (at x =2K) indicates the sample regime used for gradient estimation in standard implementations of policygradient methods. Observe that although it is possible to empirically estimate the true gradient,this requires several-fold more samples than are used commonly in practical applications of thesealgorithms. See additionally that the estimation task becomes more difficult further into training.
Figure 3: Quality of value prediction in terms of mean relative error (MRE) on heldout state-actionpairs for agents trained to solve the MuJoCo Walker2d-v2 task. We observe in (left) that the agentsdo indeed succeed at solving the supervised learning task they are trained for—the MRE on theGAE-based value loss (Vold + AGAE)2 (c.f. (4)) is small. On the other hand, in (right) we see thatthe returns MRE is still quite high—the learned value function is off by about 50% with respect tothe underlying true value function. Similar plots for other MuJoCo tasks are in Appendix A.5.
Figure 4: Efficacy of the value network as a variance reducing baseline for Walker2d-v2 (top) andHopper-v2 (bottom) agents. We measure the empirical variance of the gradient (c.f. (1)) as a functionof the number of state-action pairs used in estimation, for different choices of baseline functions:the value network (used by the agent in training), the “true” value function (fit to the returns us-ing 5 ∙ 106 state-action pairs sampled from the current policy) and the “zero” value function (i.e.
Figure 5:	True reward landscape concentration for TRPO on Humanoid-v2. We visualize the land-scape at a training iteration 150 while varying the number of trajectories used in reward estimation(each subplot), both in the direction of the step taken and a random direction. Moving one unit alongthe “step direction” axis corresponds to moving one full step in parameter space. In the random di-rection one unit corresponds to moving along a random norm 2 Gaussian vector in the parameterspace. In practice, the norm of the step is typically an order of magnitude lower than the randomdirection. While the landscape is very noisy in the low-sample regime, large numbers of samplesreveal a well-behaved underlying landscape. See Figures 20, 19 of the Appendix for additional plots.
Figure 6:	True reward and surrogate objective landscapes for TRPO on the Humanoid-v2 MuJoCotask. We visualize the landscapes in the direction of the update step and a random direction (as inFigure 5). The surrogate objective corresponds to the actual function optimized by the algorithm ateach step. We estimate true reward with 106 state-action pairs per point. We compare the landscapesat different points in training and with varying numbers of state-action pairs used in the update step.
Figure 7:	True reward and surrogate objective landscapes for PPO on the Humanoid-v2 MuJoCotask. See Figure 6 for a description. We observe that early in training the true and surrogate land-scapes align well. However, later increasing the surrogate objective leads to lower true reward.
Figure 8:	Mean reward for the studied policy gradient algorithms on standard MuJoCo benchmarktasks. For each algorithm, we perform 24 random trials using the best performing hyperparameterconfiguration, with 10 of the random agents shown here.
Figure 9:	Empirical variance of the gradient (c.f. (1)) as a function of the number of state-actionpairs used in estimation for policy gradient methods. We obtain multiple gradient estimates usinga given number of state-action pairs from the policy at a particular iteration. We then measure theaverage pairwise cosine similarity between these repeated gradient measurements, along with the95% confidence intervals (shaded). Each of the colored lines (for a specific algorithm) representsa particular trained agent (we perform multiple trials with the same hyperparameter configurationsbut different random seeds). The dotted vertical black line (at 2K) indicates the sample regime usedfor gradient estimation in standard practical implementations of policy gradient methods.
Figure 10: Convergence of gradient estimates to the “true” expected gradient (c.f. (1)). We measurethe cosine similarity between the true gradient (approximated using around 1M samples) and gradi-ent estimates, as a function of number of state-action pairs used to obtain the later. For a particularpolicy and state-action pair count, we obtain multiple estimates of this cosine similarity and thenreport the average, along with the 95% confidence intervals (shaded). Each of the colored lines (fora specific algorithm) represents a particular trained agent (we perform multiple trials with the samehyperparameter configurations but different random seeds). The dotted vertical black line (at 2K)indicates the sample regime used for gradient estimation in standard practical implementations ofpolicy gradient methods.
Figure 11:	Quality of value prediction in terms of mean relative error (MRE) on train state-actionpairs for agents trained to solve the MuJoCo tasks. We see in that the agents do indeed succeed atsolving the supervised learning task they are trained for - the train MRE on the GAE-based valueloss (Vold + AGAE)2 (c.f. (4)) is small (left column). We observe that the returns MRE is quite smallas well (right column).
Figure 12:	Quality of value prediction in terms of mean relative error (MRE) on heldout state-actionpairs for agents trained to solve MuJoCo tasks. We see in that the agents do indeed succeed atsolving the supervised learning task they are trained for - the validation MRE on the GAE-basedvalue loss (Vold + AGAE)2 (c.f. (4)) is small (left column). On the other hand, we see that thereturns MRE is still quite high - the learned value function is off by about 50% with respect to theunderlying true value function (right column).
Figure 13: HUmanoid-v2 - PPO reward landscapes.
Figure 14: HUmanoid-v2 - TRPO reward landscapes.
Figure 15: Walker2d-v2-PPO reward landscapes.
Figure 16: Walker2d-v2 - TRPO reward landscapes.
Figure 17: HoPPer-V2 - PPO reward landscapes.
Figure 18: HoPPer-V2 - TRPO reward landscapes.
Figure 19: Humanoid-v2 TRPO landscape concentration (see Figure 5 for a description).
Figure 20: Humanoid-v2 PPO landscape concentration (see Figure 5 for a description).
