Figure 1: The approximate rank and MSE of Q(t) during value iteration. (a) & (b) use vanilla valueiteration; (c) & (d) use online reconstruction with only 50% observed data each iteration.
Figure 2:	An illustration of the proposed SVP algorithm for leveraging low-rank structures.
Figure 3:	Performance comparison between optimal policy and the proposed SVP policy.
Figure 4:	Approximate rank of different Atari games: histogram (red) and empirical CDF (blue).
Figure 5: An illustration of the proposed SV-RL scheme, compared to the original value-based RL.
Figure 6: Results of SV-RL on various value-based deep RL techniques. First row: results on DQN.
Figure 7: Interpretation of deep RL results. We plot games where the SV-based method performsdifferently. More structured games (with lower rank) can achieve better performance with SV-RL.
Figure 8:	Performance comparison between optimal policy and the reconstructed “low-rank” policy.
Figure 9:	Comparison of the policy trajectories and the input torques between the two schemes.
Figure 10: The policy trajectories and the input torques of the proposed SVP scheme.
Figure 11: Performance comparison between optimal policy and the reconstructed “low-rank” policy.
Figure 12:	Comparison of the policy trajectories and the input changes between the two schemes.
Figure 13:	Performance of the proposed SVP policy, with different amount of observed data.
Figure 14:	The policy trajectories and the input changes of the proposed SVP scheme.
Figure 15:	Performance comparison between optimal policy and the reconstructed “low-rank” policy.
Figure 16: Comparison of the policy trajectories and the input changes between the two schemes.
Figure 17: Performance of the proposed SVP policy, with different amount of observed data.
Figure 18:	The policy trajectories and the input changes of the proposed SVP scheme.
Figure 19:	Performance comparison between optimal policy and the reconstructed “low-rank” policy.
Figure 20:	Comparison of the policy trajectories and the input changes between the two schemes.
Figure 21:	Performance of the proposed SVP policy, with different amount of observed data.
Figure 22:	The policy trajectories and the input changes of the proposed SVP scheme.
Figure 23: Additional results of SV-RL on DQN (Part A).
Figure 24: Additional results of SV-RL on DQN (Part B).
Figure 25: Additional results of SV-RL on DQN (Part C).
Figure 26: Additional results of SV-RL on DQN (Part D).
Figure 27: Additional results of SV-RL on double DQN.
Figure 28: Additional results of SV-RL on dueling DQN.
Figure 29: Additional study on discretization scale. We choose three different discretization valueon the Inverted Pendulum task, i.e. 400 (states, 20 each dimension) × 100 (actions), 2500 (states, 50each dimension) × 1000 (actions), and 10000 (states, 100 each dimension) × 4000 (actions). Firstrow reports the optimal policy, second row reports the SVP policy with 20% observation probability.
Figure 30: Additional study on batch size. We select two games for illustration, one with a smallrank (Frostbite) and one with a high rank (Seaquest). We vary the batch size with 32, 64, and 128,and report the performance with and without SV-RL.
