Figure 1:	An ablation study on the first four optimizations described in Section 3 (value clipping,reward scaling, network initialization, and learning rate annealing). For each of the 24 possible con-figurations of optimizations, we train a Humanoid-v2 (top) and Walker2d-v2 (bottom) agent usingPPO with five random seeds and a grid of learning rates, and choose the learning rate which givesthe best average reward (averaged over the random seeds). We then consider all rewards from the“best learning rate” runs (a total of 5 × 24 agents), and plot histograms in which agents are parti-tioned based on whether each optimization is on or off. Our results show that reward normalization,Adam annealing, and network initialization each significantly impact the rewards landscape with re-spect to hyperparameters, and were necessary for attaining the highest PPO reward within the testedhyperparameter grid. We detail our experimental setup in Appendix A.1.
Figure 2:	Per step mean reward, maximum ratio (c.f. (2)), mean KL, and mean KL for agentstrained to solve the MuJoCo Humanoid-v2 task. The quantities are measured over the state-actionpairs collected in the training step. Each line represents a training curve from a separate agent. Theblack dotted line represents the 1 + ratio constraint in the PPO algorithm, and we measure eachquantity every twenty five steps. We take mean and max KL over the KL divergences between theconditional distributions induced by the current and previous policy on the observed states in training(at each step). In the left plot we see the reward for each trained agent. From in the middle plot, wecan see that the PPO variants’ maximum ratios consistently violates the ratio “trust region.” In theright plot, we see that both PPO and PPO-M constraint the KL well (compared to the TRPO boundof 0.07). The two methods exhibit different behavior: while PPO-M KL trends up as the numberof iterations increases, PPO KL peaks halfway through training before trending down again. Wemeasure the quantities over a heldout set of state-action pairs and find little qualitative differencein the results (seen in Figure 4 in the appendix), suggesting that TRPO does indeed enforce a meanKL trust region. We show plots for additional tasks in the Appendix in Figure 3. We detail ourexperimental setup in Appendix A.1.
Figure 3: Per step mean reward, maximum ratio (c.f. (2)), mean KL, and maximum versus meanKL for agents trained to solve the MuJoCo Humanoid task. The quantities are measured over thestate-action pairs collected in the training step. Each line represents a training curve from a separateagent. The black dotted line represents the 1 + ratio constraint in the PPO algorithm, and wemeasure each quantity every twenty five steps. Compare the results here with Figure 4; they arequalitatively nearly identical.
Figure 4: Per step mean reward, maximum ratio (c.f. (2)), mean KL, and maximum versus meanKL for agents trained to solve the MuJoCo Humanoid task. The quantities are measured over state-action pairs collected from heldout trajectories. Each line represents a curve from a separate agent.
