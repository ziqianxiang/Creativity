Figure 1: BackPACK integrates with PyTorch to seamlessly extract more information from thebackward pass. Instead of the variance (or alongside it, in the same pass), BackPACK can computeindividual gradients in the mini-batch, their `2 norm and 2nd moment. It can also compute curvatureapproximations like diagonal or Kronecker factorizations of the ggn such as kfac, kflr & kfra.
Figure 2: Schematic rep-resentation of the standardbackpropagation pass formodule i with N samples.
Figure 3:	Computing individual gradients in a batch usinga for-loop (i.e. one individual forward and backward passper sample) or using vectorized operations with Back-PACK. The plot shows computation time, comparing toa traditional gradient computation, on the 3c3d network(See §4) for the CIFAR- 1 0 dataset (Schneider et al., 2019).
Figure 4:	Schematic representa-tion of the individual gradients’extraction in addition to the stan-dard backward pass at the ithmodule for N samples.
Figure 5:	Schematic of the addi-tional backward pass to compute asymmetric factorization of the ggn,G(θ) = Pn [Jθfn]>SnSn> [Jθfn]alongside the gradient at the ithmodule, for N samples.
Figure 6: Overhead benchmark for computing the gradient and first- or second-order extensions onreal networks, compared to just the gradient. Most quantities add little overhead. kflr and Diagggnpropagate 100× more information than KFAC and DiagGGN-MC on CIFAR- 1 00 and are two ordersof magnitude slower. We report benchmarks on those, and the Hessian’s diagonal, in Appendix B.
Figure 7: Median performance with shaded quartiles of the DEEPOBS benchmark for (a) 3c3dnetwork (895,210 parameters) on CIFAR-10 and (b) ALL-CNN-C network (1,387,108 parameters)on cifar-100. Solid lines show baselines of momentum sgd and Adam provided by DeepOB S.
Figure 8: kflr and Diagggn are more expensive to run on large networks. The gradient takes lessthan 20ms to compute, but KFLR and DiagGGN are approximately 100× more expensive.
Figure 9: Diagonal of the Hessian vs. the ggn.
Figure 10:	Median performance with shaded quartiles of the best hyperparameter settings chosenby DeepOBS for logistic regression (7,850 parameters) on mnist. Solid lines show well-tunedbaselines of momentum SGD and Adam that are provided by DeepOBS.
Figure 11:	Median performance with shaded quartiles of the best hyperparameter settings chosenby DEEPOBS for the 2c2d network (3,274,634 parameters) on FASHION-MNIST. Solid lines showwell-tuned baselines of momentum SGD and Adam that are provided by DeepOBS.
