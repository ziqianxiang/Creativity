title,year,conference
 Unitary evolution recurrent neural networks,2016, In InternationalConference on Machine Learning
 Neural machine translation by jointly learning toalign and translate,2014, arXiv preprint arXiv:1409
 Trellis networks for sequence modeling,2018, arXiv preprintarXiv:1810
 Reinforcement learning with long short-term memory,2002, In Advances in neural informationprocessing systems
 Synthetic and natural noise both break neural machine translation,2017, arXivpreprint arXiv:1711
 Gated feedback recurrent neuralnetworks,2015, In International Conference on Machine Learning
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Bert: Pre-training of deep bidirectionaltransformers for language understanding,2018, arXiv preprint arXiv:1810
 Finding structure in time,1990, Cognitive science
 Googlevizier: A service for black-box optimization,2017, In Proceedings of the 23rd ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining
 Frage: frequency-agnostic wordrepresentation,2018, In Advances in Neural Information Processing Systems
 Lstm can solve hard long time lag problems,1997, In Advances in neuralinformation processing systems
 Universal language model fine-tuning for text ClassifiCation,2018, arXiv preprintarXiv:1801
 Tying word veCtors and word Classifiers: A loss frameworkfor language modeling,2016, CoRR
 Adversarial example generation withsyntaCtiCally Controlled paraphrase networks,2018, arXiv preprint arXiv:1804
 Adversarial examples for evaluating reading comprehension systems,2017, arXiv preprintarXiv:1707
 Learning to create and reuse words in open-vocabularyneural language modeling,2017, arXiv preprint arXiv:1704
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 Multiplicative LSTM for sequence modelling,2016, CoRR
 Dynamic evaluation of neural sequencemodels,2017, arXiv preprint arXiv:1709
 Dynamic evaluation of transformer languagemodels,2019, arXiv preprint arXiv:1904
 Assessing the ability of lstms to learn syntax-sensitivedependencies,2016, Transactions of the Association for Computational Linguistics
 Building a large annotated corpus ofenglish: The Penn treebank,1993, Computational linguistics
 Pushingthe bounds of dropout,2018, arXiv preprint arXiv:1805
 Pointer sentinel mixture models,2016, CoRR
 An analysis of neural language modeling at multiplescales,2018, arXiv preprint arXiv:1803
 Recurrent neuralnetwork based language model,2010, In Interspeech
 On the difficulty of training recurrent neural networks,2013, InInternational conference on machine learning
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Using the output embedding to improve language models,2016, CoRR
 Learning representations by back-propagatingerrors,1988, Cognitive modeling
 Long short-term memory based recurrent neuralnetwork architectures for large vocabulary speech recognition,2014, CoRR
 Neural machine translation of rare words with subwordunits,2015, arXiv preprint arXiv:1508
 Sequence to sequence learning with neural networks,2014, In Advancesin neural information processing systems
 On multiplicativeintegration with recurrent neural networks,2016, In Advances in neural information processing systems
 Breaking the softmax bottleneck: ahigh-rank rnn language model,2017, arXiv preprint arXiv:1711
 Neural architecture search with reinforcement learning,2016, CoRR
