title,year,conference
 How tobackdoor federated learning,2018, arXiv preprint arXiv:1807
 Practical secure aggregation for privacy-preserving machine learning,2017, In Proceedings of the 2017 ACM SIGSAC Conference on Computerand Communications Security
 Iterative parameter mixing for distributed large-margin training of struc-tured predictors for natural language processing,2015, PhD thesis
 Deep models under the GAN: infor-mation leakage from collaborative deep learning,2017, In ACM SIGSAC Conference on Computer andCommunications Security
 Gradient primal-dual algorithm convergesto second-order stationary solution for nonconvex distributed optimization over networks,2018, InInternational Conference on Machine Learning (ICML)
 Distributed optimization: algorithms and convergence rates,2013, PhD
 First analysis of local gd on heteroge-neous data,2019, arXiv preprint arXiv:1909
 Federated optimization: distributedoptimization beyond the datacenter,2015, arXiv preprint arXiv:1511
 Federated learning: strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Communication efficient distributedmachine learning with the parameter server,2014, In Advances in Neural Information Processing Systems(NIPS)
 AIDE: fast andcommunication efficient distributed optimization,2016, arXiv preprint arXiv:1608
 Robust andcommunication-efficient federated learning from non-iid data,2019, arXiv preprint arXiv:1903
 Communication-efficient distributed optimization usingan approximate Newton-type method,2015, In International conference on machine learning (ICML)
 GIANT: Globallyimproved approximate newton method for distributed optimization,2018, In Conference on NeuralInformation Processing Systems (NeurIPS)
 Federated multi-tasklearning,2017, In Advances in Neural Information Processing Systems (NIPS)
 Local SGD converges fast and communicates little,2018, arXiv preprintarXiv:1805
 Sparsified SGD with memory,2018, InAdvances in Neural Information Processing Systems (NIPS)
 Cooperative SGD: A unified framework for the design and analysis ofcommunication-efficient SGD algorithms,2018, arXiv preprint arXiv:1808
 Asynchronous federated optimization,2019, arXiv preprintarXiv:1903
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In AAAI Conferenceon Artificial Intelligence
 Deep learning with elastic averaging SGD,2015, InAdvances in Neural Information Processing Systems (NIPS)
 DiSCO: distributed optimization for self-concordant empirical loss,2015, InInternational Conference on Machine Learning (ICML)
 Federatedlearning with non-iid data,2018, arXiv preprint arXiv:1806
 A general distributed dual coordinate optimizationframework for regularized loss minimization,2016, arXiv preprint arXiv:1604
 On the convergence properties of a k-step averaging stochastic gradientdescent algorithm for nonconvex optimization,2017, arXiv preprint arXiv:1708
 Federated reinforcementlearning,2019, arXiv preprint arXiv:1901
 Since FedAvg requires a communication each E steps,2020, Therefore
 Lemma 4 shows the mentioned two sampling and averagingschemes are unbiased,2017, In expectation
