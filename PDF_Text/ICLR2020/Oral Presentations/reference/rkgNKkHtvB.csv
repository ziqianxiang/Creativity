title,year,conference
 Character-levellanguage modeling with deeper self-attention,2018, CoRR
 Practicaland optimal LSH for angular distance,2015, CoRR
 Layer normalization,2016, arXiv preprintarXiv:1607
 Hierarchical memory networks,2016, arXiv preprint arXiv:1605
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 The reversible residual net-work: Backpropagation without storing activations,2017, In Advances in neural information processingsystems
 The goldilocks principle: Readingchildrenâ€™s books with explicit memory representations,2015, CoRR
 Music transformer: Generating music withlong-term structure,2018, arXiv preprint arXiv:1809
 Large memory layers with product keys,2019, CoRR
 Generating wikipedia by sUmmarizing long seqUences,2018, CoRR
 Scaling neUral machine trans-lation,2018, In Proceedings of the Third Conference on Machine Translation: Research Papers
 A call for clarity in reporting BLEU scores,2018, In Proceedings of the Third Conferenceon Machine Translation: Research Papers
 Languagemodels are unsupervised multitask learners,2019, 2019
 Scaling memory-augmented neural networks with sparse readsand writes,2016, In Advances in Neural Information Processing Systems
 Stand-alone self-attention in vision models,2019, CoRR
 One-shot learning with memory-augmented neural networks,2016, CoRR
 Mesh-tensorflow: Deep learning for supercomputers,2018, CoRR
 Low-memory neural network training: A technical report,2019, CoRR
 Adaptive atten-tion span in transformers,2019, CoRR
 AUg-menting self-attention with persistent memory,2019, CoRR
 Attention is all you need,2017, CoRR
 Memory networks,2014, CoRR
