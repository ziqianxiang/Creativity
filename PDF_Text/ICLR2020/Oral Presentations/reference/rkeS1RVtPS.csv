title,year,conference
 Distributed stochastic gradient MCMC,2014, In ICML
 Exponentially many local minima for singleneurons,1996, In NIPS
 Towards scaling UP Markov Chain Monte Carlo:an adaptive subsampling approach,2014, In ICML
 Weight uncertainty inneural networks,2015, ICML
 Weighted CSiSzar-kullback-pinsker inequalities and applicationsto transportation inequalities,2005, In Annales de la FacuIte des sciences de Toulouse
 Not MNIST Dataset,2011, 2011
 On the convergence of stochastic gradient MCMCalgorithms with high-order integrators,2015, In NIPS
 A unified particle-optimization framework for scalable bayesian sampling,2018, arXiv preprint arXiv:1805
 An efficient minibatch acceptance test forMetropolis-Hastings,2016, arXiv preprint arXiv:1610
 Stochastic gradient Hamiltonian Monte Carlo,2014, InICML
 Diffusion for global optimization in rn,0363, SIAM J
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 User-friendly guarantees for the langevin monte carlowith inaccurate gradient,0304, Stochastic Processes and their Applications
 Safe-bayesian generalizedlinear regression,2019, arXiv preprint arXiv:1910
 Bayesian recurrent neural networks,2017, arXivpreprint arXiv:1704
 Cyclicalannealing schedule: A simple approach to mitigating KL vanishing,2019, NAACL
 ScalableBayesian learning of recurrent neural networks for language modeling,2016, ACL
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Reversible jump MCMC computation and bayesian model determination,1995, Biometrika
 Deep residual learning for image recog-nition,2016, In CVPR
 Probabilistic backpropagation for scalable learn-ing of Bayesian neural networks,2015, In ICML
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Austerity in MCMC land: Cutting theMetropolis-Hastings budget,2014, In ICML
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In NIPS
 Preconditioned stochasticgradient Langevin dynamics for deep neural networks,2016, In AAAI
 Understanding mcmc dynamics as flows on the wassersteinspace,2019, arXiv preprint arXiv:1902
 Sgdr: Stochastic gradient descent with warm restarts,2016, 2016
 A complete recipe for stochastic gradient MCMC,2015, InNIPS
 Construction of numerical time-average andstationary measures via Poisson equations,2010, SIAM J
 Adding gradient noise improves learning for very deep networks,2016, ICLR workship
 Estimating theintegrated likelihood via posterior simulation using the harmonic mean identity,2006, 2006
 Non-convex learning via stochasticgradient Langevin dynamics: a nonasymptotic analysis,2017, arXiv preprint arXiv:1702
 Bayesian GAN,2017, NIPS
 Super-convergence: Very fast training of residual networksusing large learning rates,2017, arXiv preprint arXiv:1708
 Parallel Markov Chain Monte Carlo,2013, arXivpreprint arXiv:1312
 Bayesian learning via stochastic gradient Langevin dynamics,2011, InICML
 Global convergence of Langevin dynamicsbased algorithms for nonconvex oPtimization,2017, arXiv preprint arXiv:1707
 A hitting time analysis of stochastic gradient Langevin dy-namics,2017, In COLT
6 in Raginsky et al,2020, (2017)
