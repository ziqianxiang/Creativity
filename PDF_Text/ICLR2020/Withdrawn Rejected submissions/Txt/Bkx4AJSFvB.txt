Under review as a conference paper at ICLR 2020
Efficient Bi-Directional Verification of ReLU
Networks via Quadratic Programming
Anonymous authors
Paper under double-blind review
Ab stract
Neural networks are known to be sensitive to adversarial perturbations. To investi-
gate this undesired behavior we consider the problem of computing the distance to
the decision boundary (DtDB) from a given sample for a deep neural net classifier.
In this work we present an iterative procedure where in each step we solve a convex
quadratic programming (QP) task. Solving the single initial QP already results
in a lower bound on the DtDB and can be used as a robustness certificate of the
classifier around a given sample. In contrast to currently known approaches our
method also provides upper bounds used as a measure of quality for the certificate.
We show that our approach provides better or competitive results in comparison
with a wide range of existing techniques.
1	Introdution
The high predictive power of neural network classifiers makes them the method of choice to tackle
challenging classification problems in many areas. However, questions regarding the robustness of
their performance under slight input perturbations still remain open, severely limiting the applicability
of deep neural net classifiers to sensitive tasks that require certification of the obtained results.
In recent years this issue gained a lot of attention, resulting in a large variety of methods tackling
tasks ranging from adversarial attacks and defenses against these to robustness verification and robust
training. In this work we focus on robustness verification of deep classifiers. That is, computing
the distance from a given anchor point x0 in the input space to its closest adversarial, i.e. a point
that is assigned a different class label by the network. This problem plays a fundamental role in
understanding the behavior of deep classifiers and essentially provides the only reliable way to
assess classifier robustness. Unfortunately, its complexity class does not allow a polynomial time
algorithm. For deep classifiers with ReLU activation the verification problem can equivalently be
reformulated as a mixed integer programming (MIP) task and was shown to be NP-complete by
Katz et al. (2017). Even worse, Weng et al. (2018) showed that an approximation of the minimum
adversarial perturbation of a certain (high) quality cannot be found within polynomial time.
Related work There exist three streams of related work on robustness verification of deep ReLU
classifiers. This categorization is based on whether they are solving the verification problem exactly
or verifying a bound on the distance to the decision boundary (DtDB).
The first group of methods are exact verification approaches. As mentioned above, the verification
task can be modeled using MIP techniques. Katz et al. (2017) present a modification of the simplex
algorithm that can be used to solve the verification task exactly for smaller ReLU networks based
on satisfiable modulo theory (SMT). Other approaches (see Ehlers (2017)) rely on SMT solvers
when solving the described task. Bunel et al. (2018) provide an overview and comparison of those.
Other exact methods (Dutta et al. (2018); Lomuscio and Maganti (2017); Tjeng et al. (2017)) deploy
MIP solvers together with presolving to find a tight formulation of the MIP problem or (Jordan et al.
(2018)) use an algorithm to find the largest ball around the anchor point that touches the decision
boundary.
The second popular class of methods for verifying classifier robustness deals with verification of an
-neighborhood: given an anchor point x0 and an > 0, the task is to verify whether an adversarial
point exists within the neighborhood of x0 which is defined with respect to a certain norm in the
1
Under review as a conference paper at ICLR 2020
input space. All existing methods relax the initial problem and require bounds on activation inputs in
each layer. These bounds should be as tight as possible to ensure good final results. Raghunathan
et al. (2018a;b); Dvijotham et al. (2018; 2019) consider semidefinite (SDP) and linear (LP) problems
as relaxations of the -verification problem. Wong and Kolter (2018) replace ReLU constraints by
linear constraints and consider the dual formulation of the obtained LP relaxation. Weng et al. (2018)
present an approach that also uses linear functions (later extended to quadratic functions by Zhang
et al. (2018)) to deal with nonlinear activation functions and propagate the layer-wise output bounds
until the final layer. Finally Hein and Andriushchenko (2017); Tsuzuku et al. (2018) use the Lipschitz
constant of the transformations within classifier’s architecture.
Our approach belongs to the third group of approaches dealing with constructing lower bounds on
DtDB without restricting admissible adversarial points to a given neighborhood. The -verification
task is closely related to the problem of constructing lower bounds on DtDB: each -neighborhood that
can be certified as adversarial-free immediately provides a lower bound on the minimal adversarial
perturbation magnitude. It is also a common strategy for the methods from the previous group to
use a binary search or a Newton method on top of their algorithm to find the largest such that the
-neighborhood around x0 can still be verified as robust. Croce et al. (2019) leverage the piecewise
affine nature of the outputs of a ReLU classifier and compute lower bounds on DtDB by assuming
that the classifier behaves globally the same way it does in the linear region around the given anchor
point.
Adversarial attacks Constructing misclassified examples that are close to the anchor point can be
considered as a complementary research direction to robustness verification since each adversarial
example by definition provides an upper bound on the DtDB. Many methods were proposed to
construct such points (Szegedy et al. (2014); Goodfellow et al. (2015); Kurakin et al. (2016); Papernot
et al. (2016); Madry et al. (2017); Carlini and Wagner (2017).
Robust training The question of how to actually train a robust classifier is closely related to
robustness verification since the latter might allow us to construct some type of robust loss based
on the insights from the verification procedure (see Hein and Andriushchenko (2017); Madry et al.
(2017); Wong and Kolter (2018); Raghunathan et al. (2018a); Tsuzuku et al. (2018); Wang et al.
(2018); Croce et al. (2019)). We leave this direction for future work.
Contributions
1.	We propose a novel relaxation of the DtDB problem in form of a QP task allowing efficient
computation of high quality lower bounds on the DtDB in l2-norm with an extension to l∞-norm. For
networks with up to two hidden layers we reach state-of-the-art performance with an improvement of
over 50% when compared to the bounds obtained from methods based on LP relaxations (CROWN
by Zhang et al. (2018) and ConvAdv by Wong and Kolter (2018)). Furthermore, our method
performs much faster than methods based on SDP relaxations (Raghunathan et al., 2018b), while
providing smaller lower bounds. This is a fundamental property due to the difference in computational
complexity between SDP and QP tasks.
2.	Unlike -verification techniques, we provide a lower bound on DtDB without an initial guess
and without computing bounds for the neuron activation values in each layer. Such bounds have
to be tight enough to verify non-trivial neighborhoods and play an important role in other relaxation
techniques such as the SDP based approaches by Raghunathan et al. (2018b); Dvijotham et al. (2019).
3.	In addition to computing the lower bounds after solving a single QP, our approach allows to
construct adversarial examples by repeatedly choosing a new anchor on the boundary of the
verified region and solving its associated QP. One of the main limitations of competing inexact
approaches is the fact that the situation around the given anchor point outside of the certified radius
remains unclear. Since these approaches rely on relaxations of the initial problem, failed certification
does not indicate the presence of an adversarial example. Instead, when using these methods, one
must rely on externally constructed adversarial examples to obtain an upper bound. We present
a bi-directional robustness verification technique that uses the verification mechanism itself to
construct upper bounds and assess the quality of the lower bounds.
2
Under review as a conference paper at ICLR 2020
2	Notation and idea
We consider a neural network consisting ofL linear transformations representing dense, convolutional,
skip or average pooling layers and L - 1 ReLU activations (no activation after the last hidden layer).
The number of neurons in layer l is denoted as nl for l = 0, . . . , L, meaning that the data has
n0 features and nL classes. Furthermore, we present our analysis for the l2-norm as perturbation
magnitude measure since only few available methods are applicable to this setting. To make our
method comparable with Raghunathan et al. (2018b) a generalization to l∞-perturbations is addressed
in Appendix A.
Given sample x0 ∈ Rn0, weight matrices Wl ∈ Rnl ×nl-1, and bias vectors bl ∈ Rnl, we define the
output of the i-th neuron in the l-th layer after the ReLU activation as
xli = Wilxl-1 +bli+ and	(1)
fi (x0) = xiL = WiLxL-1 + biL
where [x]+ is the positive part of x and f(x0) = xL denotes the output of the complete forward pass
through the network. We start with the observation that the ReLU activation function acts as one of
two linear functions depending on its input’s sign. That allows us to reformulate it (see Section 3.1)
and obtain an optimization problem with so called linear complementarity constraints (also used by
Raghunathan et al. (2018b); Dvijotham et al. (2019) for -verification). Note that for each pair of
scalars a and b the following holds.
b = [a]+ ^⇒ b ≥ 0, b — a ≥ 0, b(b — a) = 0	(2)
3	Optimization problem
3.1	Formulation of DtDB
For a given sample X0, pre-trained neural net f, predicted label y and target label y We aim to find
the closest point to X0 in Rn0 that has a larger or equal probability of being classified as y compared
to the initial label. This can be done by solving the folloWing optimization problem.
min ∣∣x0 — X0k2, s.t. (ey — ey)tf (x0) ≤ 0,	(DtDB)
Where ei is the i-th unit vector in RnL and kxk denotes the Euclidean norm of x. Note that We explore
the boundary between classes y and y around X0. To compute the distance from X0 to the (full)
decision boundary, one needs t compute the solution for all target labels y = 1,..., nL except y.
We can “unfold” the above optimization problem using (1), where X denotes a container with all
variables X0, . . . , XL .
min ∣∣x0 — X0∣2, s.t. (ey — ey)tXL ≤ 0, XL = WLXL-I + bL
Xl = ReLU(WlXl-1 +bl) forl = 1, . . .,L — 1
We apply (2) to reformulate the problem and eliminate XL, such that from now on X contains only
the remaining variables X0, . . . , XL-1 and n = n0 + . . . + nL-1.
min ∣∣x0 — X0∣2, s.t. (ey — ey)t (WLXL-I + bL) ≤ 0	(DtDB)
(Xl)T (x1 — (W lXl-1 + bl)) = 0 for l = 1,...,L — 1	(3)
Xl — (W IXlT + bl) ≥ 0, Xl ≥ 0 for l = 1,..., L — 1	(4)
3
Under review as a conference paper at ICLR 2020
3.2	QP Relaxation
Next we consider a Lagrangian relaxation of DtDB without constraints (3). We will refer to the
resulting problem as QPRel(λ), when the explicit dependency on the multipliers is required.
L-1
min ∣∣x0 - X0∣∣2 + X λι (Xl)T (Xl- (WlxlT + bl)), s.t.	(QPRel)
x∈R	l=1
(ey - ey)T (WLxL-1 + bL) ≤ 0	(5)
xl — (Wlxl-1 + bl) ≥ 0, xl ≥ 0 for l = 1,...,L — 1	(6)
The obtained problem is indeed a QP with linear constraints. We need to clarify two questions.
How does the problem QPRel help us with solving DtDB and how can this problem itself be solved
efficiently?
QPRel vs. DtDB First, we describe properties of the solution of QPRel providing information
about DtDB. To do so, we define for arbitrary vectors x0 ∈ Rn0, . . . , xL-1 ∈ RnL-1 and λ ∈ RL+-1
L-1
c(x, λ) := X λl (Xl)T (Xl-(WlXl-1 + bl))	⑺
l=1
as the corresponding propagation gap. It follows directly from the definition that for arbitrary
non-negative λ it holds that:
•	if X is feasible for DtDB we have c(X, λ) = 0, meaning that X equals the vector obtained by
propagating X0 through the neural net as defined in (1),
•	if X is feasible for QPRel we have c(X, λ) ≥ 0, meaning that there might be a slack between
the true output of layer l when getting X0 as an input and the value of Xl obtained by QPRel.
In general the following holds for the relation between the solution of QPRel and DtDB (see also
Figure 1, every point in the hatched area is certified to be classified as belonging to the class y).
Lemma 1. Denote the solution of QPRel by Xqp and the square root of its optimal objective value by
dqp, let d be the square root of the optimal objective value of DtDB. The following holds:
1.	dqp ≤ d and when c(Xqp, λ) = 0 we have dqp = d and Xqp is optimal for DtDB.
2.	For two non-negative λ1, λ2 with λ1 ≤ λ2 elementwise it holds that dqp(λ1) ≤ dqp(λ2).
The first result from Lemma 1 ensures that dqp provides a
radius of a certified neighborhood around the anchor point.
Whereas the second part indicates that we should choose λ
as large as possible to get our lower bound closer to DtDB.
Unfortunately, as we show below, the problem QPRel becomes
non-convex for large values of λ. While one could try to tackle
a non-convex QP with proper optimization methods, we will
address necessary conditions such that QPRel is guaranteed to
be convex and can be solved efficiently next.
Convexity of QPRel To look into the problem QPRel in
more detail we introduce the Hessian Mλ (which is a constant
matrix) of its objective function.
Figure 1: Setting of the optimal solu-
tions for DtDB Xadv and QPRel Xqp.
Definition 1. Let El ∈ Rnl ×nl be the identity matrix of the corresponding dimension. We define
Mλ ∈ Rn×n as the following symmetric block tridiagonal matrix
D0 M1T
Mi	Di	MT	l
Mλ ：=	M ..	..	with Ml :=  1 λlWl,Dl := λlEl and λo :=1.
∖
DL-1
4
Under review as a conference paper at ICLR 2020
Using this matrix we can rewrite the objective function from QPRel as (see Appendix C, Lemma 2):
min xτMλx + xTb(λ, X0) + c(x0), s.t. (5), (6)
x∈Rn
where b and c influence only the linear and constant terms and are therefore not relevant in this section.
From this reformulation we can clearly see that the matrix Mλ determines the (non-)convexity of
the objective function. The following theorem provides sufficient and necessary conditions on λ
depending on the weights Wl assuring that Mλ is positive semi-definite. This allows us to use
off-the-shelf QP-solvers with excellent convergence properties.
Theorem 1.	Let W1, . . . , WL-1 be the weights of an arbitrary pre-trained neural net and kW k the
spectral norm ofan arbitrary matrix. Then the following two conditions for λ provide correspondingly
a sufficient and a necessary criterion for the matrix Mλ to be positive semi-definite.
(suf. condition)	λ1 ≤	2λo	八 L E and λl ≤	λi-i WrF	for l =	2, .	..,L-1	(8)
(nec. condition)	λl ≤	4λl-1 WrF		for l =	1,.	..,L-1	(9)
Furthermore, we define λ and λ that correspondingly satisfy conditions (8) and (9) with equality:
11
λl = 2 U kwkk2,	λl = 4 U kwkk2.
In case with a single hidden layer choosing λ = λ from (9) guarantees Mλ to be positive-semi
definite.
We use (8), (9) and our previous results as guidelines for the choice of λ. Since QPRel(λ) is
monotonous in the sense of Lemma 1 we perform a binary search between λ and X from Theorem 1
to find the point closest to X such that the QP remains convex. This preprocessing step does not
considerably affect the runtime since checking whether a matrix is positive semi-definite is done
efficiently by Cholesky decomposition. However, it improves the final bounds by up to a factor two
compared to the bounds obtained when using λ = λ without binary search.
Note that this procedure has to be done once for a given classifier. The obtained λ can subsequently
be used to solve QPRel for all anchor points and target labels. This is a significant computational
advantage compared to SDP based -verification procedures. For example, the method by Dvijotham
et al. (2019) includes the dual multipliers as variables in the SDP problem that has to be solved for
each combination of the anchor point, target label and verified epsilon.
4 Upper bounds
1
2
3
4
5
6
7
8
9
10
11
Now we describe an iterative procedure generating a sequence of points in Rn0 that converges to a
point on the decision boundary. This procedure provides an upper bound on the distance to DtDB
and thus allows for bi-directional verification of a classifier.
Algorithm 1: Algorithm to construct an adversarial example by iteratively solving QPRel.
Data: X0, trained neural net and λ such that Mλ is positive semi-definite, ctol, δ.
Result: dub , xa0dv
begin
x04——X0; C4-------+∞
while c > cm, Pred(X0) = pred(x0) do
Solve QPRel(λ) from the anchor point x0:
Xqp, dqp《— optimal solution and square root of the objective function value
C4 c(* * * xqp, λ)
Choose the next anchor point on the boundary of the current certified region:
if 广、r*+ ι	fIipn	TO	√_ τ0 —|—	d ―/qp__ plcp TO √__ TO _i_，	(1	_i_ ―/qp__
if c > ctol	Ihen	X	y	X +	dqp ∣∣χ0 -χ0 k else X y	X +	dqp(1	+ υ) ∣∣χ0 -χ0 ∣∣
end
x0dv 4— x0; dub <— ∣∣x0 - XOdVk
end
5
Under review as a conference paper at ICLR 2020
Idea In each step we verify a certain neighborhood around the current anchor point x0 and then
expand the verified region further. For that we choose the next anchor point as follows. 1) Choice
of direction: we go from x0 towards the solution of QPRel since, if the QP relaxation is tight, its
solution should be close to an optimal solution of DtDB which is the closest adversarial point to x0 .
2) Choice of step size: we know that there are no adversarial points within the ball of the verified
radius d around the anchor, so every step size smaller than d would be unnecessary small. On the
other hand, if we proceed with a new anchor point that is strictly farther away than d, we might miss
an adversarial point lying close to the boundary of the d-ball around x0 . Therefore, we choose the
next anchor point to be on the boundary of the currently verified region, so that every -ball that we
manage to verify around the new point would add to the overall robust set. Algorithm 1 provides a
pseudo-code for this procedure.
Termination The algorithm terminates as soon as the propagation gap c(x, λ) (see (7)) becomes
small enough or the anchor point gets misclassified. Note that c(x, λ) = 0 means that the solution
x provides the optimal objective function value of the DtDB problem (see Lemma 1) and, thus, an
adversarial example. However, the termination condition c ≤ ctol from Algorithm 1, line 3 cannot
ensure that the optimal point xq0p from the last iteration belongs to a different class. Therefore, if we
stop with c ≤ ctol and the second termination condition is not satisfied, we take an additional step on
the boundary of the ball of radius dqp(1 + δ), where dqp is the verified radius and δ is a tiny offset.
Additionally, we check in each step whether the next anchor point is already misclassified before
the condition c ≤ ctol is reached (this can happen in a multi-class setting; and is indeed observed
frequently). We empirically verified that all points obtained this way are indeed true adversarials.
This means that the sequence of anchor points converges towards the boundary and then, if no
adversarial point was found yet, makes a step across the boundary using a positive, small δ. Note,
while the most powerful attacks as proposed by Carlini and Wagner (2017) were empirically shown
to succeed for every considered sample, their success in not guaranteed and might require tuning of
hyper-parameters. In contrast, we provide a formal proof of our method’s convergence below.
Figure 2 illustrates how the constructed sequence of anchor
points and solutions of QPRel might look like for the setting
of Figure 1. Here only the first three steps are shown assuming
that we would reach either a small value for the propagation
gap or an adversarial point in the last step. Blue arrows show
how we proceed from the solution of QPRel to the next anchor
point in each iteration (compare to the update formula from
Algorithm 1, line 8).
Convergence The following result holds for the conver-
gence rate of Algorithm 1 under the assumption that the
propagation gap c(x, λ) is not too large with respect to the op-
timal objective function value for the QPRel problem solved
in each step.
x
0
adv
Figure 2: Three steps of Algorithm 1.
Theorem 2.	For k = 0, 1, . . . let ck be the propagation gap as defined in (7) and d2k the optimal
objective function value we obtain after solving QPRel in the k’th iteration of Algorithm 1. Assume
that there exists α ∈ [0, 1] such that for all k the following holds.
ck	kx0 - xq0p k
-2 ≤ 1 一 α2 or equivalently≥ ɑ
d2k	dk
(10)
where x0 is the anchor point and xqp is the optimal solution of QPRel in iteration k (we omit the
iteration index k on x’s). Then for all k
d2k+1 ≤ 2(1 一 α)d2k, ck ≤ (2(1 一 α))k+1d02.
(11)
1
Furthermore, if a > 1 then the number ofιterαtιons k SUCh thatAlgorithm 1 terminates with Ck ≤ Ctol
is bounded by
log Ctol 一 2 log d0
k ≤   --------；------：——
log 2(1 — α)
(12)
6
Under review as a conference paper at ICLR 2020
First of all, this result ensures that Algorithm 1 terminates after a final number of steps if (10) holds
for some α ∈ (2, 1]. Moreover, it provides a hint about how many iterations We would have to do if
we decreased the threshold ctol by a factor of 10. In this case the upper bound (12) on the number of
iterations would increase by ∆kɑ = -(log2(1 - a))-1 with ∆kα → ∞ when a → ɪ and ∆kα → 0
when α → 1. Note that α being almost 1 means that ck is close to 0. In this regime the relaxation is
exact, as discussed in Lemma 1, part 1. Therefore, considering α as a measure of tightness of the
relaxation QPRel we can conclude that, when the relaxation is tight (i.e. α ≈ 1), decreasing ctol by
an order of magnitude will not be as costly as when α is smaller.
Related work In relation to the methodology proposed in this section we consider the work on exact
DtDB computation by Jordan et al. (2018). The proposed algorithm GeoCert iteratively computes the
radius of the largest lp-ball that stays within a certain polyhedral complex. The latter is incrementally
updated in each step until the decision boundary is reached. This way one get a sequence of lower
bounds on DtDB until the true distance is found. GeoCert is significantly different to our method:
1.) It does not employ a relaxation of (DtDB). 2.) It does not change the anchor point, but rather
expands the considered polyhedral complices by analyzing the linear regions of a ReLU classifier.
3.) While it can use an upper bound to speed up the algorithm, the complexity of computing its own
exact distance is comparable to MIP-based approaches as shown in Jordan et al. (2018).
5	Experiments
5.1	Setup
The values of the parameters we used as well as the runtime of the compared algorithms is shown in
Appendix B.1 and B.2. To solve the QP tasks we use Gurobi Optimization (2018).
Datasets and classifiers The experiments are performed using the MNIST and Fashion-MNIST
datasets scaled such that the feature values lie in [0, 1] interval. For each of the datasets we use
the correctly classified samples from 1200 train points (label distribution preserved) to evaluate
the verification approaches. The exact number of used samples is reported for each experiment in
Appendix D.
For classification we take ReLU networks consisting of dense linear layers. The architectures we
used are named by the number of hidden layers and the number of neurons per hidden layer: L1N50,
L2N50, L3N50, L4N50, L5N50 and L1N300 with the same number of neurons in each hidden layer.
For each architecture we use normally trained classifiers as well as robustly trained ones (indicated
by suffix R, e.g. L1N50R) using the method by Wong and Kolter (2018) with = 1.58 in l2-setting.
The weights will be available in the project repository.
Competitors We compare our approach QPRel with the following -verification methods: ConvAdv
by Wong and Kolter (2018) based on the LP relaxation of ReLU constraints (we use its implementation
supporting the l2-norm by Croce et al. (2019)), CROWN by Zhang et al. (2018) which is a layerwise
bound propagation technique including performance boosting quadratic approximations and warm
start (for each setting we report only the best result from these two competitors), and SDPRel by
Raghunathan et al. (2018b) based on a SDP relaxation solved by MOSEK. Finally, we take the
iterative PGD attack with 200 steps and a step size of 2.5/200 (in accordance with Madry et al.
(2017)) as a competitor for QPRel-UB. We use the notation QPRel-LB or QPRel-UB, when we want
to emphasize what bounds are meant.
5.2	Results
Results on MNIST in l2-setting are shown in Table 1. We include the complete results in Appendix
D. We run all methods for each of the considered samples and report the following metrics.
AvgBound: The average value of the bounds obtained from QPRel and the corresponding competitor.
Both lower and upper bounds are reported separately.
7
Under review as a conference paper at ICLR 2020
MedRelDiff to QPRel: The median of the relative difference between the bounds (e.g. QPRel-LB
minus CROWN and then divided by CROWN). Positive values for the lower bounds mean our bounds
are better in average over the samples; negative values are better for the upper bounds.
to hit 50% LB-verified: The number of samples with an adversarial-free radius of is monotonically
decreasing in . Therefore, to assess the performance of a verification procedure like QPRel-LB or
CROWN we report the smallest such that exactly 50% of the samples can be verified. The larger
this value, the better.
VerRatio worst: While the previous metric only considers the lower bound, we can consider both
bounds (LB, UB) jointly by evaluation the verification ratio: For a given , it corresponds to the
fraction of samples for which we can either prove that the -neighborhood is adversarial-free or
contains a misclassified point in it. In short: either the lower bound is larger then or the upper bound
is smaller then .
Note, that for very small values almost all samples will be verified as -robust by any algorithm
providing non-trivial lower bounds. On the other side, when is large, attacks will be able to find an
adversarial that is even closer to the anchor and verify almost every sample as -non-robust. Between
these two extremes, the verification ratio drops to its minimum for a certain value of 0 . In this setting,
the considered algorithm is able to verify the SmalleStfraCtiOn ofsamples - that is, it corresponds to
the worst case performance of the bi-directional verification method. We report this smallest fraction
in the table, denoted with VerRatio worSt. The larger, the better.
MedRelDiff UB-LB: Lastly, we report the median of the relative difference between the different
bounds for each approach (QPRel-UB vs. QPRel-LB or PGD vs. CROWN/ConvAdv). It shows the
tightneSS of the certificates when compared to the upper bounds. The smaller, the better.
Table 1: Better bounds and verification ratio for the compact networks (l2 -perturbations)
MNIST, l2		AvgBound		MedRelDiff to QPRel (%)		to hit 50% LB-verified	VerRatio worst (%)	MedRelDiff UB-LB (%)
L/N/R	Method	LB	UB	LB	UB			
1/50	QPRel	0.42	0.49	-	-	0.398	87.5	13.7
1/50	CROWN+PGD	0.25	0.72	+63.0	-30.8	0.239	31.4	168.7
1/50/R	QPRel	1.69	1.84	-	-	1.698	90.3	5.3
1/50/R	CROWN+PGD	1.52	2.68	+12.8	-30.8	1.567	49.8	72.0
2/50	QPRel	1.20	1.21	-	-	1.174	99.3	<0.01
2/50	ConvAdv+PGD	0.89	1.80	+30.1	-25.0	0.891	51.2	89.8
2/50/R	QPRel	1.39	1.81	-	-	1.388	74.8	25.3
2/50/R	ConvAdv+PGD	1.47	2.62	-2.9	-30.8	1.527	52.4	76.3
3/50	QPRel	0.18	0.55	-	-	0.181	26.0	200.2
3/50	ConvAdv+PGD	0.19	0.71	-9.3	-18.2	0.199	19.9	246.4
3/50/R	QPRel	1.10	1.78	-	-	1.091	59.5	57.7
3/50/R	ConvAdv+PGD	1.43	2.55	-21.9	-30.8	1.488	51.1	74.3
4/50	QPRel	0.09	0.58	-	-	0.106	9.1	484.6
4/50	ConvAdv+PGD	0.19	0.70	-50.4	-18.2	0.191	19.5	255.4
4/50/R	QPRel	0.69	1.76	-	-	0.698	35.7	142.5
4/50/R	ConvAdv+PGD	1.41	2.51	-49.8	-25.0	1.497	51.2	71.6
5/50	QPRel	0.02	0.42	-	-	0.037	9.5	1370.6
5/50	ConvAdv+PGD	0.16	0.63	-84.4	-35.7	0.167	19.5	274.2
5/50/R	QPRel	0.37	1.76	-	-	0.391	20.0	357.0
5/50/R	ConvAdv+PGD	1.41	2.58	-73.1	-30.8	1.466	49.3	79.9
1/300	QPRel	0.26	0.28	-	-	0.243	94.0	5.5
1/300	CROWN+PGD	0.10	0.53	+149.1	-43.8	0.101	17.2	385.2
1/300/R	QPRel	0.78	1.81	-	-	0.782	42.8	120.4
1/300/R	CROWN+PGD	1.43	2.76	-44.1	-30.8	1.418	46.6	83.5
8
Under review as a conference paper at ICLR 2020
State-of-the-art bounds For the normally trained networks with a smaller number of hidden layers
the lower bounds computed by QPRel are tighter in comparison to the competitors in average and for
most individual images (see Table 1, AvgBound and MedRelDiff). This results in larger values of to
hit 50% VerRatio as well. It seems that the competitors tend to underestimate robustness of these
networks. For example, for 50% of the MNIST images classified with L1N300 we have improved
the lower bound from CROWN by over 149%. This discrepancy in the quality of the lower bounds
decreases with network’s depth and if it was trained using the robust training approach by Wong
and Kolter (2018) so that the competitors outperform QPRel’s lower bound for robustly trained deep
networks.
Noteworthy, the upper bounds derived via QPRel always outperform the one computed by the
competing methods - independent of the neural network architecture and the training procedure.
That is, the 200-step PGD attack was not able to construct adversarial examples that are closer to the
anchor point then those found by QPRel-UB as described in Section 4 in all settings (see Table 1,
columns AvgBound/UB and MedRelDiff to QPRel/UB). Therefore, in the settings where the bounds
from QPRel-LB are also tighter as the competitors’ we get a much higher VerRatio worst value and
smaller MedRelDiff UB-LB that ensure the good quality of QPRel-LB bounds without computing the
DtDB exactly. For example, for L2N50 the gap between QPRel-LB and QPRel-UB vanishes resulting
in VerRatio worst of over 99%. That means the computed lower bound is almost the exact DtDB for
the majority of the samples. Note that no exact method computing DtDB via MIP techniques is used
to achieve that. For larger networks we observe a decay of VerRatio worst and looser MedRelDiff
UB-LB.
Comparison with SDP-relaxations in l∞-setting In order to compare our method with Raghu-
nathan et al. (2018b) we generalize QPRel to the l∞-setting as described in Appendix A. Note,
that the resulting relaxation is looser then the initial QPRel for the l2 -setting since we introduce an
additional penalty term to make the problem convex. That leads to worse results shown in Tables
9 and 10. To compute the largest such that the SDP verification succeeds we perform a binary
search between the lower bounds dqp computed by QPRel-LB and dmax = 1.0 which is the maximal
perturbation for the l∞-norm on images. Since this approach takes longer to run we test it only
on the L1N50R net trained with = 0.1 (MNIST test data). Further, we speed up this approach
by modifying MOSEK parameters (see Table 6 for details) such that the optimization procedure
terminates earlier (approximately after a half of the usual number of iterations). We can still rely
on the obtained results since we are not interested in the exact value of the SDP objective, but only
whether it is positive or negative which was observed to be determined far sooner during the solution
process then when the solver would reach a true optimum.
While QPRel-LB in this setting is able to provide tighter bounds then the LP-based approaches for
larger computational cost, our bounds are about 1.54 times larger then the ones of SDPRel (see Table
10) - though computed three orders of magnitude faster (see Appendix B.2). This shows that the QP
relaxation is less suited then SDPRel for obtaining tight bounds in l∞-setting as already indicated by
the arguments above and in Appendix A but trades this off by much better efficiency.
6	Conclusion
In this work we present a novel approach to solve the problem of approximating the minimal
adversarial perturbations for ReLU networks based in a convex QP relaxation of DtDB. We show
that the lower bounds computed with QPRel improve the results of the available LP based methods
and allow certification of larger neighborhoods. Since convexity of the underlying QP determines
computational efficiency of our approach we derive the necessary and sufficient conditions on the
Lagrangian multipliers for it. Additionally, a method that constructs a sequence of points converging
to the point on the decision boundary provides us upper bounds on DtDB. Quality of the obtained
bounds in the l2-setting is shown to be good enough to verify consistently over 80% of the considered
samples for MNIST and FashionMNIST data and simple networks for all values of the perturbation
magnitude, while competitors’ verification ratio might drop below 20%.
With our contribution we make a step towards robustness verification of deep ReLU-based classifiers.
To be able to apply the approach on a wider class of networks it should be generalized to popular
architectures beyond ReLU activations and linear layers. Moreover, for deeper networks the proposed
9
Under review as a conference paper at ICLR 2020
relaxation was shown to provide looser bounds than the other methods indicating an important
direction of the future work. Furthermore, there are several points about Theorem 2 that will be
addressed in our future work. It is an open question how to verify (10) a priori. Experiments show
that for the current choice of λ (see the discussion after Theorem 1) this condition does hold for small
networks, but not for the larger ones.
10
Under review as a conference paper at ICLR 2020
References
Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda. A unified
view of piecewise linear neural network verification. In Advances in Neural Information Processing
Systems 31, pages 4790-4799. Curran Associates, Inc., 2018.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pages 39-57. IEEE, 2017.
Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu networks
via maximization of linear regions. AISTATS, 2019.
Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Output range analysis
for deep feedforward neural networks. In NASA Formal Methods, pages 121-138. Springer
International Publishing, 2018.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A. Mann, and Pushmeet Kohli. A
dual approach to scalable verification of deep networks. Proceedings of the Conference on Uncer-
tainty in Artificial Intelligence, 2018. URL http://auai.org/uai2018/proceedings/
papers/204.pdf.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and Pushmeet
Kohli. Efficient neural network verification with exactness characterization. Proceedings of
the Conference on Uncertainty in Artificial Intelligence, 2019. URL http://auai.org/
uai2019/proceedings/papers/164.pdf.
Rudiger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Automated
Technology for Verification and Analysis, pages 269-286. Springer International Publishing, 2017.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015. URL http://
arxiv.org/abs/1412.6572.
LLC Gurobi Optimization. Gurobi optimizer reference manual, 2018. URL http://www.gurobi.
com.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. In Advances in Neural Information Processing Systems 30, pages
2266-2276. Curran Associates, Inc., 2017.
Matt Jordan, Justin Lewis, and Alexandros G. Dimakis. Provable certificates for adversarial examples:
Fitting a ball in the union of polytopes. In Advances in Neural Information Processing Systems 33.
2018.
Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An
efficient smt solver for verifying deep neural networks. In Computer Aided Verification, pages
97-117. Springer International Publishing, 2017.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
In International Conference on Learning Representations, 2016. URL http://arxiv.org/
abs/1607.02533.
Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward ReLU
neural networks. arXiv e-prints, art. arXiv:1706.07351, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium
on Security and Privacy (EuroS&P), pages 372-387. IEEE, 2016.
11
Under review as a conference paper at ICLR 2020
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. In International Conference on Learning Representations, 2018a.
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems 31,
pages 10877-10887. Curran Associates, Inc., 2018b.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. arXiv preprint arXiv:1711.07356, 2017. URL http://arxiv.org/
abs/1711.07356.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification
of perturbation invariance for deep neural networks. In Advances in Neural Information Processing
Systems 31, pages 6541-6550. Curran Associates, Inc., 2018.
Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Mixtrain: Scalable training of verifiably
robust neural networks. arXiv preprint arXiv:1811.02625, 2018.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 5276-5285. PMLR, 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pages 5286-5295. PMLR, 2018.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network
robustness certification with general activation functions. In Advances in Neural Information
Processing Systems 31, pages 4939-4948. Curran Associates, Inc., 2018.
12
Under review as a conference paper at ICLR 2020
A GENERALIZATION TO l∞
For comparison with Raghunathan et al. (2018b) we show how our method can be applied to compute
bounds on the distance to the closest adversarial measured using the l∞ norm. A straight forward way
would be to modify the objective function accordingly. By introducing a new variable m representing
∣∣x0 - x0k∞ = maxi |x0 - X0∣2 and no new quadratic constraints We get the following versions of
QPRel. Note that the quadratic constraints do not harm the complexity since they describe a convex
cone and can be handled by the QP-solvers including gurobi.
min m + c(x, λ), s.t.
x∈Rn, m∈R
(x0 - X0)2 ≤ m, i = 1,...,no
(ey — ey)T (WLxL-I + bL) ≤ 0
xl - (WIxl-I + bl) ≥ 0, xl ≥ 0
While this formulation is of a similar structure as the QPRel (quadratic objective and linear plus
quadratic constraints), the Hessian of the objective function is not positive semi-definite for any value
of λ. Since c(x, λ) is the only source of quadratic terms now (squared distance to the anchor point is
now replaced by m), the new Mλ is of the same form as in Lemma 2, but with λ0 = 0. To see that
we cannot affect the convexity of the objective function by the parameter λ anymore consider vector
x with an arbitrary x0 ∈ Rn0, x1 = αW1x0 with 0 < α < 1 and xl = 0 for l > 1. Then
xτMλx = λι (kx1∣∣2 — (x1)T W 1x0) = λι(α2 — α)∣ W 1x0k2 < 0
meaning that Mλ cannot be positive semi-definite.
To overcome this issue we utilize the new quadratic constraints. We return back to a convex QP by
considering the following problem with a positive μ.
n0
min m + c(x,λ) + μ > ((x0 — x0)2
x∈Rn , m∈R	i i
i=1
- m , with
(x0 - x0)2 ≤ m, i = 1,...,no
(ey — ey)T (WLxL-I + bL) ≤ 0
xl - (Wlxl-1 + bl) ≥ 0, xl ≥ 0
Clearly, for 0 < μ ≤ n-1 the solution of this problem is a finite lower bound on DtDB with the
l∞ -norm. On the other side we are back in the setting of Theorem 1 with λo = μ allowing us to use
the same framework as before. Results in the l∞-setting were obtained by solving this problem with
μ = (2no) i.
B Experiments S etup and Runtime
B.1	Setup
λ is chosen for each classifier according to Theorem 1 and the discussion afterwards such that an
accuracy level of 10-4 is achieved during the binary search in each λl . For the values of other
parameters in Algorithm 1 we choose for all tests ctol = 10-4 and δ = 10-4. Other methods are
tested with the default settings as provided in the corresponding repositories.
B.2	Runtime and Number of Iterations
Tables 2, 3, 4 and 5 show the average runtime and its standard deviation for networks L1N50R,
L1N100, L1N300 and L2N100 as well as the number of iterations Algorithm 1 needs to terminate.
During the binary search procedure we apply with SDPRel we always make 10 bisection steps. All
tasks necessary for the computation of bounds on DtDB for one sample are run on a single CPU
(including the solution of QPs and SDPs with Gurobi Optimization (2018) and MOSEK respectively).
13
Under review as a conference paper at ICLR 2020
Here, we do not consider ConvAdv as CROWN performs always faster. From the comparison of
the time for the computation of our lower bounds (column Runtime-LB (s)) and the overall runtime
(column Runtime (s)) we conclude that obtaining the upper bounds takes n - 1 times longer than
solving the initial QP where n is approximately the overall number of the iterations in Algorithm 1.
While in almost all considered settings n remains smaller than 4 it might become a limiting factor
when applying QPRel-UB on larger networks. That means further investigation of the convergence
properties of Algorithm 1 is necessary for generalizing the described method. From the comparison
of QPRel-LB and CROWN we see the clear advantage of the latter since it doesn’t involve any
optimization task. However, this advantage comes in cost of the verification properties as discussed
above. On the other hand, SDPRel with a binary search provides better bounds, but is about 2000
times slower then QPRel-LB (see the last line in the MNIST section in Table 5).
Table 2: Runtime comparison, train data, l2-setting
Setting
Data L/N NrPts Method
Runtime-LB (s) Runtime-Full (s) Nr. iterations(-UB)
mean std mean std mean std
ISlNn - ISlNn—工
1/100	1086	QPRel	1.467	0.142	4.200	2.091	2.845	1.416
1/100	1086	CROWN	0.021	0.002	—	—	—	—
2/100	965	QPRel	0.280	0.017	0.472	0.140	1.679	0.484
2/100	965	CROWN	0.034	0.013	—	—	—	—
1/300	1142	QPRel	5.421	0.309	12.631	3.414	2.317	0.629
1/300	1142	CROWN	0.064	0.016	—	—	—	—
1/50	1180	QPRel	0.253	0.018	0.805	0.257	3.176	1.002
1/50	1180	CROWN	0.017	0.006	—	—	—	—
1/100	1083	QPRel	1.441	0.156	4.534	4.124	3.142	2.687
1/100	1083	CROWN	0.025	0.009	—	—	—	—
2/100	924	QPRel	0.289	0.019	0.992	0.469	3.431	1.600
2/100	924	CROWN	0.031	0.012	—	—	—	—
1/300	1094	QPRel	5.385	0.317	18.782	12.443	3.503	2.313
1/300	1094	CROWN	0.063	0.018	—	—	—	—
Setting
Table 3: Runtime comparison, test data, l2-setting
Data L/N NrPts Method
Runtime-LB (s) Runtime-Full (s) Nr. iterations(-UB)
mean std mean std mean std
ISlNn - ISlNn—工
1/100	1137	QPRel	1.451	0.142	4.225	1.994	2.898	1.342
1/100	1137	CROWN	0.024	0.009	—	—	—	—
2/100	1028	QPRel	0.280	0.018	0.473	0.141	1.683	0.486
2/100	1028	CROWN	0.033	0.012	—	—	—	—
1/300	1166	QPRel	5.353	0.328	12.954	3.386	2.398	0.619
1/300	1166	CROWN	0.059	0.017	—	—	—	—
1/50	1212	QPRel	0.257	0.018	0.817	0.256	3.183	0.993
1/50	1212	CROWN	0.016	0.006	—	—	—	—
1/100	1061	QPRel	1.456	0.148	4.635	4.186	3.161	2.676
1/100	1061	CROWN	0.024	0.008	—	—	—	—
2/100	949	QPRel	0.276	0.022	0.985	0.455	3.557	1.613
2/100	949	CROWN	0.031	0.012	—	—	—	—
1/300	1069	QPRel	5.406	0.301	19.585	12.080	3.638	2.251
1/300	1069	CROWN	0.058	0.016	—	—	—	—
14
Under review as a conference paper at ICLR 2020
Table 4: Runtime comparison, train data, l∞-setting
Setting	Runtime-LB (s) Runtime-Full (s) Nr. iterations(-UB)
Data L/N NrPts Method mean std mean std mean std
ISlNn - ISINn 山
1/100	1086	QPRel	6.202	1.283	19.104	63.168	2.752	8.231
1/100	1086	CROWN	0.019	0.005	—	—	—	—
2/100	965	QPRel	1.392	0.143	4.110	1.738	2.988	1.293
2/100	965	CROWN	0.029	0.011	—	—	—	—
1/300	1142	QPRel	10.470	0.854	14.567	7.457	1.400	0.732
1/300	1142	CROWN	0.054	0.017	—	—	—	—
1/50	1180	QPRel	2.634	0.916	16.813	19.141	5.818	3.660
1/50	1180	CROWN	0.016	0.006	—	—	—	—
1/100	1083	QPRel	5.750	1.314	18.447	66.807	2.651	7.627
1/100	1083	CROWN	0.018	0.004	—	—	—	—
2/100	924	QPRel	1.500	0.179	3.496	2.978	2.348	1.954
2/100	924	CROWN	0.026	0.010	—	—	—	—
1/300	1094	QPRel	10.636	0.882	11.541	4.031	1.085	0.379
1/300	1094	CROWN	0.053	0.017	—	—	—	—
Table 5: Runtime comparison, test data, l∞-setting
Setting
Runtime-LB (s)
Runtime-Full (s) Nr. iterations(-UB)
Data L/N NrPts Method mean std
ISlNn - ISlNn—工
mean std mean std
1/100	1137	QPRel	6.208	1.250	22.898	75.986	3.145	8.924
1/100	1137	CROWN	0.020	0.007	—	—	—	—
2/100	1028	QPRel	1.321	0.145	3.912	1.619	3.002	1.266
2/100	1028	CROWN	0.029	0.011	—	—	—	—
1/300	1166	QPRel	10.512	0.844	15.852	8.343	1.515	0.808
1/300	1166	CROWN	0.048	0.014	—	—	—	—
1/50	1054	QPRel	2.683	1.041	17.439	16.539	6.102	3.307
1/50	1054	CROWN	0.016	0.006	—	—	—	—
1/50	1054	SDPRel	4338.017	949.572	—	—	10	0.000
1/100	1061	QPRel	5.805	1.193	21.649	73.836	3.107	8.900
1/100	1061	CROWN	0.020	0.007	—	—	—	—
2/100	949	QPRel	1.454	0.164	3.407	2.689	2.352	1.827
2/100	949	CROWN	0.025	0.009	—	—	—	—
1/300	1069	QPRel	10.686	0.927	11.829	5.051	1.107	0.466
1/300	1069	CROWN	0.048	0.014	—	—	—	—
15
Under review as a conference paper at ICLR 2020
C Proofs
Lemma 1. Denote the solution of QPRel by xqp and the square root of its optimal objective value by
dqp, let d be the square root of the optimal objective value of DtDB. The following holds:
1.	dqp ≤ d and when c(x, λ) = 0 we have dqp = d and x is optimal for DtDB.
2.	For two non-negative λ1, λ2 with λ1 ≤ λ2 elementwise it holds that dqp(λ1) ≤ dqp(λ2).
Proof. Assume xadv is the optimal solution of DtDB. Then it is an admissible point of QPRel as well
and c(xadv, λ) = 0 since xladv = W l xla-dv1 for l = 1, . . . , L - 1. Since xqp is optimal for QPRel and
xadv is just its admissible point we get that
d2 = ∣∣x0dv - x0k2 = kx0dv - x0k2 + C(Xadv, λ) ≥ ∣∣x0p - x0k2 + c(xqp, λ) = d*
proving the first claim. The second one follows from the fact that c(x, λ) for a given x is a linear
function of λ:
c(x, e1)
c(x, λ) = λτ	. I
c(x, eL-1)
where each c(χ, eι) = (xl)T (xl - (Wlχl-1 + bl)) is non-negative for admissible X because of the
non-negativity constraints (4). Therefore the claim follows immediately from the assumption that
λl1 ≤ λl2 for all l
L-1	L-1
c(x, λ1 ) =	λl1c(x, el) ≤	λl2c(x, el) = c(x, λ2 ).
l=1	l=1
□
Lemma 2. Objective function of QPRel can be reformulated as
dqp(λ, x) = XTMλx + xTb(λ, X0) + c(X0)
where terms b and C don't depend on X.
Proof. The proof is done by sorting the quadratic, linear and constant terms in the objective function
of the initial formulation.
L-1
dqp(λ,X) = ∣X0 - X0k2 + X λl (Xl)T (Xl-(WlXl-1 + bl))
l=1
L-1
=(X0)T x0 - 2 (x0)t X0 + ∣∣X0k2 + X λl ((Xl)T Xl - (x1 )^γ WlXl-I-(Xl)，bl)
l=1
L-1	L-1	L-1
X λl (xl)T Xl - X λl (xl)T Wlxl-1 -2 (x0)T X0 - X λl (xl)T bl + ∣∣X0k2
l=0
।
l=1
{^^^"'v"^^^^~
quadratic term
l=1
linear and constant terms
From the quadratic term we can identify now the blocks of Mλ
diagonal: Mlλ,l = λlEl
sub-diagon壮 Ml+ι,l = (Mlλl+JT = -1 λlWl
l = 0, . . . , L - 1,
l = 0, . . . , L - 2.
□
16
Under review as a conference paper at ICLR 2020
Theorem 1.	Let W1, . . . , WL-1 be the weights of an arbitrary pre-trained neural net and kW k the
spectral norm ofan arbitrary matrix. Then the following two conditions for λ provide correspondingly
a sufficient and a necessary criterion for the matrix Mλ to be positive semi-definite.
(suf. condition)
(nec. condition)
2λ0	λl-1
λ1 ≤ kwιk2 and λl ≤ kwψ
4λl-1
l≤kWψ
for l = 2, . . . , L - 1
for l = 1, . . . , L - 1
(8)
(9)
Further, we define λ and λ that correspondingly satisfy conditions (8) and (9) with equality:
l1
λl = 2 U kwkk2,
l1
λl = 4 U kwkk2.
In case with a single hidden layer Mλ with λ = λ from (9) is guaranteed to be positive-semi definite.
Proof. Let the assumptions hold and x be an arbitrary vector from Rn . First we prove the sufficient
condition by deriving a lower bound on xTMλx that is non-negative if (8) holds.
L-1
L-1
XTMλx = X λι∣∣xlk2 - X λι (Xl)T Wlxl-1
l=0	l=1
=λ0 Mk2 + λL2-1 kχL-1k2
L-1
+ X λlkxlk2- λl (Xl)T WlXl-1 + λ-kxl-1k2
l=1
=λ0 Mk2 + ” kχL-1k2
L-1
+ X λlkxlk2 - λl (Xl)T Wlxl-1 + λlkWlχlτ∣∣2
+ IX λ2-1 kXl-1k2- BkWlXl-1k2
l=1
λ0 kX0k2 + λL2=1 kXL-1k2 + X λ kXl- WlXl-1k2
l=1
+ X λ-kXl-1k2- λlkWlXl-1k2
l=1
亨 kXL-1k2 + IX λ kXl- WlXl-1k2+
λI
l=1
+ (λ0∣∣X0k2 - λ1 ∣∣W1X0k2) + X λ-1 kXl-1k2 -号kWlXl-1k2
l=2
I-1
引 XL-1k2 + X λ kXl- WlXl-1k2+
l=1
1	1 I-1
+ 2 (2λo - λ1kW1k2) ∣∣X0∣∣2 + -E (λl-1 - λl∣∣Wlk2) ∣∣Xl-1k2
l=2
where we applied the sub-multiplicativity property of the spectral norm, i.e. kW l Xl-1 k ≤
kWlkkXl-1 k, to obtain the last inequality. We see that under the assumption (8) on λ and W’s
it holds that
λ1kW1k2 ≤ 2λ0andλlkWlk2 ≤ λl-1 forl = 2, ...,L -
17
Under review as a conference paper at ICLR 2020
and the lower bound on xT Mλx in the last line is a sum of non-negative terms meaning that
xT Mλx ≥ 0 for all x ∈ Rn.
To prove the necessary condition consider for each l = 1,...,L - 1 a special vector X (We don't
explicitly label it as dependent on l to avoid overloaded notation) which is everywhere zero except
X1-1 := arg max R and X1：= 1 Wlxl-L
x∈Rnl-1 kxk	2
For Mλ in order to be positive semi-definite it has to satisfy
0 ≤ XTMλX
"l-∖T Cl-IEl-1	-2λl (Wl)T∖ "l-∖
I χl) I-1λlwl	λlEl	∏χl J
=λl-1kXlτ∣∣2 - λl (Xl)T WlxlT + λl∣∣Xlk2
=λlH∣xlτ∣∣2 - 4λl∣∣WlXlτ∣∣2 =卜l-ι - ɪλl∣∣Wl∣∣2) ∣∣Xl-11∣2
Which results in the necessary condition (9) as stated above. It remains to prove the sufficiency of (9)
if the considered netWork contains one hidden layer. For that We can reuse the last computation and
obtain noW for an arbitrary X ∈ Rn that
xTMλx = λokx0∣∣2 — λι (χ1)T W 1χ0 + λιkx1∣∣2
=λokx0k2 - 4λι∣∣W1 x0k2 + λιk2W1X0 - x1k2
≥ 卜0 - 4λι∣∣W1∣∣2) ∣∣X0∣∣2 + λιk4W1X0-X1II2.
We see that the last term remains non-negative in case of λ1 = ∣W0ρ∙ for all x.	口
Theorem 2.	For k = 0, 4, . . . let ck be the propagation gap as defined in (7) and d2k the optimal
objective function value we obtain after solving QPRel in the k’th iteration of Algorithm 1. Assume
that there exists α ∈ [0, 4] such that for all k the following holds.
dd2 ≤ 1 一 ɑ2 or equivalently ——“	≥ ɑ
(10)
where X0 is the anchor point and Xqp is the optimal solution of QPRel in k’s iteration (we omit the
iteration index k on X’s). Then for all k
d2k+1 ≤2(4-α)d2k, ck ≤ (2(4 - α))k+1d02.
(11)
1
Furthermore, If a > 1 then the number ofιteratιons k such thatAlgorithm 1 terminates with Ck ≤ Ctol
is bounded by
log Ctol -2logd0
k ≤   -----∙.--：——
log 2(1 — α)
(12)
Proof. First, We prove that given (10) it holds that d2k+1 ≤ 2(1 - α)d2k . We denote the anchor point in
iteration k as X0 and the optimal solution for QPRel With that anchor point as Xqp. The corresponding
objects in the next iteration k + 1 are denoted by y0 and yqp. Then according to the choice of the next
anchor point (see Algorithm 1, line 8)
X0 - X0
y = X0 + dk η-qF——q77 ⇒ ∣∣y0 - X0Pll = dk - ∣∣X0 - X* ∣∣ (compare to Figure 2)	(13)
kXq0p - X0k	qp	qp
and it folloWs that
d2k+1 = x:m(5)i,n(6) ∣y0 - X0∣2 + C(X, λ) ≤ ∣y0 - Xq0p∣2 + C(Xqp, λ)
(=)(dk -∣x0 - x0p∣)2 + Ck (≤) (1 - a)2"k + (1 - a2)"k = 2(1 - αd.
18
Under review as a conference paper at ICLR 2020
Now, applying this relation on every two subsequent iterations up to the k’s we obtain (here we use
1 - α2 = (1 + α)(1 - α) ≤ 2(1 - α) to get the last inequality)
ck ≤ (1 - α2)d2k ≤ (1 - α2)(2(1 - α))k d02 ≤ (2(1 - α))k+1d02.
(10)
It remains to prove the last inequality (12). Note that for 1 ≥ a > 2 the derived upper bound on Ck
is monotonically decreasing and converges to 0 as k → ∞ since 0 ≤ 2(1 - α) < 1. Therefore, to
find the first k such that the upper bound on ck becomes smaller or equal ctol we have to solve the
equation ctol = (2(1 - α))k+1d02 for k resulting in the upper bound on k as claimed in (12).
□
D Tables
Table 6: MOSEK parameters we use to run SDPRel and their default values
Parameter 1	New value	Default value
MSK_IPAR_NUM_THREADS	1	0
MSK_DPAR_INTPNT_CO_TOL_MU_RED	10-4	10-8
MSK_DPAR_INTPNT_CO_TOL_REL_GAP	10-4	10-8
MSK_DPAR_INTPNT_CO_TOL_INFEAS	10-6	10-12
MSK_DPAR_INTPNT_CO_TOL_DFEAS	10-4	10-8
MSK_DPAR_INTPNT_CO_TOL_PFEAS	10-4	10-8
1https://docs.mosek.com/9.0/pythonapi/parameters.html contains the full list of pa-
rameters including their description.
19
Under review as a conference paper at ICLR 2020
Table 7: Results for train data, l2-perturbations
Setting
Data L/N NrPts Method
MedRelDiff to hit 50% VerRatio MedRelDiff
to QPRel (%) LB-verified worst (%) UB-LB (%)
ISlNn - ISlNn—工
1/100	1086	QPRel-LB	—	0.246	82.8	13.4
1/100	1086	CROWN+FGSM	+82.4	0.129	8.1	1033.7
2/100	965	QPRel-LB	—	1.174	99.3	<0.01
2/100	965	ConvAdv+FGSM	+30.1	0.891	23.3	277.9
1/300	1142	QPRel-LB	—	0.243	94.0	5.5
1/300	1142	CROWN+FGSM	+149.1	0.101	10.9	627.8
1/50	1180	QPRel-LB	—	0.872	82.4	14.3
1/50	1180	CROWN+FGSM	+67.0	0.506	8.8	419.4
1/100	1083	QPRel-LB	—	0.252	819~	17.2
1/100	1083	CROWN+FGSM	+49.1	0.159	15.9	678.8
2/100	924	QPRel-LB	—	0.332	83.0	20.3
2/100	924	ConvAdv+FGSM	+68.0	0.205	22.2	375.1
1/300	1094	QPRel-LB	—	0.183	78.8	23.1
1/300	1094	CROWN+FGSM	+56.6	0.122	12.8	902.2
Table 8: Results for test data, l2-perturbations
Setting
Data L/N NrPts Method
MedRelDiff to hit 50% VerRatio MedRelDiff
to QPRel (%) LB-verified worst (%) UB-LB (%)
ISlNn - ISlNn—工
1/100	1137	QPRel-LB	—	0.268	81.9	14.6
1/100	1137	CROWN+FGSM	+84.6	0.146	6.7	1097.2
2/100	1028	QPRel-LB	—	1.251	99.4	0.0
2/100	1028	ConvAdv+FGSM	+30.2	0.959	23.6	281.8
1/300	1166	QPRel-LB	—	0.267	93.9	5.9
1/300	1166	CROWN+FGSM	+154.6	0.111	7.0	658.5
1/50	1212	QPRel-LB	—	0.937	80.0	15.2
1/50	1212	CROWN+FGSM	+68.5	0.562	7.4	421.8
1/100	1061	QPRel-LB	—	0.258	79.8~	19.2
1/100	1061	CROWN+FGSM	+50.3	0.164	13.7	703.0
2/100	949	QPRel-LB	—	0.355	79.9	21.3
2/100	949	ConvAdv+FGSM	+71.6	0.213	19.3	375.2
1/300	1069	QPRel-LB	—	0.194	77.0	24.7
1/300	1069	CROWN+FGSM	+59.3	0.122	10.6	890.9
20
Under review as a conference paper at ICLR 2020
Table 9: Results for train data, l∞-perturbations
Setting	MedRelDiff to hit 50% VerRatio MedRelDiff
Data L/N NrPts Method	to QPRel (%) LB-Verified worst (%) UB-LB (%)
ISlNn - ISlNn—工
1/100	1086	QPRel-LB	—	0.010	5.8	1478.1
1/100	1086	CROWN+FGSM	+33.4	0.010	13.6	882.5
2/100	965	QPRel-LB	—	0.057	5.3	2484.0
2/100	965	ConvAdv+FGSM	+16.8	0.057	32.4	220.4
1/300	1142	QPRel-LB	—	0.011	3.6	1691.4
1/300	1142	CROWN+FGSM	+104.3	0.011	13.0	575.1
1/50	1180	QPRel-LB	—	0.033	1.6	3344.0
1/50	1180	CROWN+FGSM	+2.2	0.033	16.0	286.8
1/100	1083	QPRel-LB	—	0.010	6.9	1520.8
1/100	1083	CROWN+FGSM	+5.5	0.010	17.8	599.5
2/100	924	QPRel-LB	—	0.013	3.6	1855.8
2/100	924	ConvAdv+FGSM	+17.1	0.013	33.1	341.6
1/300	1094	QPRel-LB	—	0.007	3.0	1579.8
1/300	1094	CROWN+FGSM	+11.0	0.007	14.2	741.8
Table 10: Results for test data, l∞-perturbations
Setting
Data L/N NrPts Method
MedRelDiff to hit 50% VerRatio MedRelDiff
to QPRel (%) LB-verified worst (%) UB-LB (%)
ISlNn - ISlNn—工
1/100	1137	QPRel-LB	—	0.011	7.2	1439.0
1/100	1137	CROWN+FGSM	+33.6	0.011	10.5	946.5
2/100	1028	QPRel-LB	—	0.115	5.2	2480.9
2/100	1028	ConvAdv+FGSM	+16.5	0.058	28.7	218.6
1/300	1166	QPRel-LB	—	0.011	2.4	1704.3
1/300	1166	CROWN+FGSM	+105.7	0.011	8.1	610.7
1/50	1054	QPRel-LB	—	0.037	1.8	3462.1
1/50	1054	CROWN+FGSM	+2.3	0.037	15.5	280.8
1/50	1054	SDPRel+FGSM	-54.5	0.074	51.1	68.1
1/100	1061	QPRel-LB	—	0.011	6.8	1520.9
1/100	1061	CROWN+FGSM	+5.5	0.011	19.3	605.4
2/100	949	QPRel-LB	—	0.012	2.5	1938.1
2/100	949	ConvAdv+FGSM	+18.2	0.012	28.6	344.7
1/300	1069	QPRel-LB	—	0.007	3.9	1543.1
1/300	1069	CROWN+FGSM	+12.6	0.007	12.9	736.9
21