Under review as a conference paper at ICLR 2020
Quantum Semi-Supervised Kernel Learning
Anonymous authors
Paper under double-blind review
Ab stract
Quantum machine learning methods have the potential to facilitate learning us-
ing extremely large datasets. While the availability of data for training machine
learning models is steadily increasing, oftentimes it is much easier to collect fea-
ture vectors that to obtain the corresponding labels. One of the approaches for ad-
dressing this issue is to use semi-supervised learning, which leverages not only the
labeled samples, but also unlabeled feature vectors. Here, we present a quantum
machine learning algorithm for training Semi-Supervised Kernel Support Vector
Machines. The algorithm uses recent advances in quantum sample-based Hamilto-
nian simulation to extend the existing Quantum LS-SVM algorithm to handle the
semi-supervised term in the loss, while maintaining the same quantum speedup as
the Quantum LS-SVM.
1	Introduction
Data sets used for training machine learning models are becoming increasingly large, leading to
continued interest in fast methods for solving large-scale classification problems. One of the ap-
proaches being explored is training the predictive model using a quantum algorithm that has access
to the training set stored in quantum-accessible memory. In parallel to research on efficient architec-
tures for quantum memory (Blencowe, 2010), work on quantum machine learning algorithms and
on quantum learning theory is under way (see for example Refs. (Biamonte et al., 2017; Dunjko
& Briegel, 2018; Schuld & Petruccione, 2018) and (Arunachalam & de Wolf, 2017) for review).
An early example of this approach is Quantum LS-SVM (Rebentrost et al., 2014a), which achieves
exponential speedup compared to classical LS-SVM algorithm. Quantum LS-SVM uses quadratic
least-squares loss and squared-L2 regularizer, and the optimization problem can be solved using
the seminal HHL (Harrow et al., 2009) algorithm for solving quantum linear systems of equations.
While progress has been made in quantum algorithms for supervised learning, it has been recently
advocated that the focus should shift to unsupervised and semi-supervised setting (Perdomo-Ortiz
et al., 2018).
In many domains, the most laborious part of assembling a training set is the collection of sample
labels. Thus, in many scenarios, in addition to the labeled training set of size m we have access to
many more feature vectors with missing labels. One way of utilizing these additional data points
to improve the classification model is through semi-supervised learning. In semi-supervised learn-
ing, we are given m observations x1, ..., xm drawn from the marginal distribution p(x), where the
l (l m) first data points come with labels y1, ..., yl drawn from conditional distribution p(y|x).
Semi-supervised learning algorithms exploit the underlying distribution of the data to improve clas-
sification accuracy on unseen samples. In the approach considered here, the training samples are
connected by a graph that captures their similarity.
Here, we introduce a quantum algorithm for semi-supervised training of a kernel support vector ma-
chine classification model. We start with the existing Quantum LS-SVM (Rebentrost et al., 2014a),
and use techniques from sample-based Hamiltonian simulation (Kimmel et al., 2017) to add a semi-
supervised term based on Laplacian SVM (Melacci & Belkin, 2011). As is standard in quantum
machine learning (Li et al., 2019), the algorithm accesses training points and the adjacency matrix
of the graph connecting samples via a quantum oracle. We show that, with respect to the oracle,
the proposed algorithm achieves the same quantum speedup as LS-SVM, that is, adding the semi-
supervised term does not lead to increased computational complexity.
1
Under review as a conference paper at ICLR 2020
2	Preliminaries
2.1	Semi-Supervised Least-Squares Kernel Support Vector Machines.
Consider a problem where we are aiming to find predictors h(x) : X → R that are functions from
a RKHS defined by a kernel K. In Semi-Supervised LS-SVMs in RKHS, we are looking for a
function h ∈ H that minimizes
γl	1	1
min	9	E (yi	-	(h(xi)	+ b)) +	X||h||H	+	5 lVhllE,
h∈H,b∈R	2	2	2
,	i=1
where γ is a user define constant allowing for adjusting the regularization strength. The last term
captures the squared norm of the graph gradient on the graph G that contains all training samples as
vertices, and expresses similarity between samples through, possibly edges Gu,v
2 ||vh||E = 2 X Gu,v (h U — hv )2 = hT Lh,
u~v
where hu is the function value h(xi) for the vertex U corresponding to training point xi, and L is the
combinatorial graph Laplacian matrix such that L[i, j] = Dj — Gi,j .
The Representer Theorem states that if H is RKHS defined by kernel K : X × X → R, then the
solution minimizing the problem above is achieved for a function h that uses only the representers
of the training points, that is, a function of the form h(x) = Pjm=1 cj Kxj (x) = Pjm=1 cjK(xj, x).
Thus, we can translate the problem from RKHS into a constrained quadratic optimization problem
over finite, real vectors
min	Y ξ2 + UcTKc + LcTKLKc
c,ξ,b	2j ςi 2	2
i=1
m
s.t. 1 —yi b+	cjK [i,j] = ξi
j=1
where l ≤ m is the number of training points with labels (these are grouped at the beginning of the
training set), and h = Kc, since function h is defined using representers Kxi. The semi-supervised
term, the squared norm of the graph gradient of h, 1∕2∣∣Vh∣∣E, penalizes large changes of function
h over edges of graph G. In defining the kernel K and the Laplacian L and in the two regularization
terms we use all m samples. On the other hand, in calculating the empirical quadratic loss we only
use the first l samples.
The solution to the Semi-Supervised LS-SVMs is given by solving the following (m + 1) ×(m+ 1)
system of linear equations
0	1T	b 0
1K	+KLK	+ γ-11 α = y ,
(1)
where y = (y1, ...,ym)T, 1 =(1, ..., 1)T, 1 is identity matrix, K is kernel matrix, L is the graph
Laplacian matrix, γ is a hyperparameter and α = (α1, ..., αm)T is the vector of Lagrangian multi-
pliers.
2.2 Quantum Computing and Quantum LS-SVM
Quantum computers are devices which perform computing according to the laws of quantum me-
chanics, a mathematical framework for describing physical theories, in language of linear algebra.
Quantum Systems. Any isolated, closed quantum physical system can be fully described by a
unit-norm vector in a complex Hilbert space appropriate for that system; in quantum computing,
the space is always finite-dimensional, Cd . In quantum mechanics and quantum computing, Dirac
notation for linear algebra is commonly used. In Dirac notation, a vector x ∈ Cd and its complex
conjugate xT, which represents a functional Cd → R, are denoted by |xi (called ket) and hx| (called
bra), respectively. We call {|eii}id=1 the computational basis, where |eii =(0, ..., 1, ...0)T with
exactly one 1 entry in the i-th position. Any |vi = (v1, ..., vd)T can be written as |vi = Pid=1 vi|eii;
coefficient vi ∈ C are called probability amplitudes. Any unit vector |xi ∈ Cd describes a d-level
2
Under review as a conference paper at ICLR 2020
quantum state. Such a system is called a pure state. An inner product of |x1i, |x2i ∈ Cd is written
as hχ1∣χ2i. A two-level quantum state ∣ψi = α∣0) + e|1)，where |0)= (1,0)T, |1)= (0,1)T and
α,β ∈ C, ∣a∣2 + ∣β∣2, is called a quantum bit, or qubit for short. When both a and β are nonzero,
We say ∣ψi is in a superposition of the computational basis |0)and |1)； the two superposition states
|+i = √= (|0i + |1))and |-〉= √= (|0)- |1))are very common in quantum computing.
A composite quantum state of two distinct quantum systems |x1i ∈ Cd1 and |x2i ∈ Cd2 is described
as tensor product of quantum states |x。0 ∣x2i ∈ Cd1 0 Cd2. Thus, a state of an n-qubit system is
a vector in the tensor product space (C2)0n = C2 0 C2 0 ... 0 C2, and is written as P2=-1 αi∖1i,
where i is expressed using its binary representation; for example for n = 4, we have |2i = |0010i =
∖0i 0 ∖0i 0 ∖1i 0 ∖0i.
Transforming and Measuring Quantum States. Quantum operations manipulate quantum states
in order to obtain some desired final state. Two types of manipulation of a quantum system are al-
lowed by laws of physics: unitary operators and measurements. Quantum measurement, if done in
the computational basis, stochastically transforms the state of the system into one of the computa-
tional basis states, based on squared magnitudes of probability amplitudes; that is, √= (|0)+ |1〉)
will result in |0i and |1i with equal chance. Unitary operators are deterministic, invertible, norm-
preserving linear transforms. A unitary operator U models a transformation of a quantum state |ui
to |vi = U |ui. Note that U|u1i + U |u2i = U (|u1i + |u2i), applying a unitary to a superposition
of states has the same effect as applying it separately to element of the superposition. In quan-
tum circuit model unitary transformations are referred to as quantum gates - for example, one of
the most common gates, the single-qubit Hadamard gate, is a unitary operator represented in the
computational basis by the matrix
H ：= ɪ f 1	1
:= √2k1 -1
Note that H∖0i = ∖+i and H∖1i = ∖-i.
(2)
Quantum Input Model. Quantum computation typically starts from all qubits in |0i state. To
perform computation, access to input data is needed. In quantum computing, input is typically given
by a unitary operator that transforms the initial state into the desired input state for the computation
一 such unitaries are commonly referred to as oracles, and the computational complexity of quantum
algorithms is typically measured with access to an oracle as the unit. For problems involving large
amounts of input data, such as for quantum machine learning algorithms, an oracle that abstracts
random access memory is often assumed. Quantum random access memory (qRAM) uses log N
qubits to address any quantum superposition of N memory cell which may contains either quantum
or classical information. For example, qRAM allows accessing classical data entries xij in quantum
superposition by a transformation
mp	mp
√mp X=X X li,jil0 …0i---→√mρ X χιi,ji咛，
where |xij i is a binary representation up to a given precision. Several approaches for creating quan-
tum RAM are being considered (Giovannetti et al., 2008; Arunachalam et al., 2015; Biamonte et al.,
2017), but it is still an open challenge, and subtle differences in qRAM architecture may erase any
gains in computational complexity of a quantum algorithm Aaronson (2015).
Quantum Linear Systems of Equations. Given an input matrix A ∈ Cn×n and a vector b ∈ Cn ,
the goal of linear system of equations problem is finding x ∈ Cn such that Ax = b. When A is
Hermitian and full rank, the unique solution is x = A-1b. IfA is not a full rank matrix then A-1 is
replaced by the Moore-Penrose pseudo-inverse. HHL algorithm introduced an analogous problem in
quantum setting: assuming an efficient algorithm for preparing b as a quantum state b = Pin=1 bi ∖ii
using dlog ne + 1 qubits, the algorithm applies quantum subroutines of phase estimation, controlled
rotation, and inverse of phase estimation to obtain the state
∖xi
A-1∖bi
k A-1∖bik.
(3)
3
Under review as a conference paper at ICLR 2020
Intuitively and at the risk of over-simplifying, HHL algorithm works as follows: if A has spec-
tral decomposition A = Pin=1 λiviviT (where λi and vi are corresponding eigenvalues and eigen-
states of A), then A-1 maps λiVi → -vi. The vector b also can be written as the linear
λi
combination of the A’s eigenvectors vi as b = Pin=1 βivi (we are not required to compute βi).
Then ATb = Pn=ι βi —vi. In general A and AT are not unitary (unless all A's eigenvalues
λi
have unit magnitude), therefore we are not able to apply A-1 directly on |bi. However, since
U = eiA = Pin=1 eiλiviviT is unitary and has the same eigenvectors as A and A-1, one can im-
plement U and powers of U on a quantum computer by Hamiltonian simulation techniques; clearly
for any expected speed-up, one need to enact eiA efficiently. The HHL algorithm uses the phase
estimation subroutine to estimate an approximation of λi up to a small error. The Next step com-
putes a conditional rotation on the approximated value of λi and an auxiliary qubit |0i and outputs
J |0)+ ʌ/l-ɪlɪi. The last step involves the inverse of phase estimation and quantum measure-
ment for getting rid of garbage qubits and outputs our desired state |x)= A-1∣bi = Pn=ι βi ɪvi.
λi
Density Operators. Density operator formalism is an alternative formulation for quantum me-
chanics that allows probabilistic mixtures of pure states, more generally referred to as mixed states.
A mixed state that describes an ensemble {pi, lψii} is written as
k
ρ =	pilψiihψil,
i=1
(4)
where Pik=1 pi = l forms a probability distribution and ρ is called density operator, which in a
finite-dimensional system, in computational basis, is a semi-definite positive matrix with Tr(ρ) = l.
A unitary operator U maps a quantum state expressed as a density operator P to U PU *, where U * is
the complex conjugate of the operator U .
Partial Trace of Composite Quantum System. Consider a two-part quantum system in a state
described by tensor product of two density operators P 0 σ. A partial trace, tracing out the second
part of the quantum system, is defined as the linear operator that leaves the first part of the system in
a state Tr2 (P 0 σ) = Ptr (σ), where Tr (σ) is the trace of the matrix σ.
To obtain Kernel matrix K as a density matrix, quantum LS-SVM (Rebentrost et al., 2014b) relies
on partial trace, and on a quantum oracle that can convert, in superposition, each data point {xi}im=1,
Xi ∈ Rp to a quantum state |xii = kX^ PP=ι(xi)t∣ti, where (xi)t refers to the tth feature value in
data point xi and assuming the oracle is given k xi k and yi . Vector of the labels is given in the same
fashion as |y)= pɪɪ Pm=I y/i). For preparation the normalized Kernel matrix K0 = tr(1K)K
where K = XTX, we need to prepare a quantum state combining all data points in quantum
superposition ∣X) = √	1	=
im=1 kxi
by discarding the training set state,
2	im=1 li) 0 k xi k lxi). The normalized Kernel matrix is obtained
K0 = Tr2(lX)hXl)
m
Pm U .∣∣2 X k Xi k k Xjk hxilxj)|i)hj|.
i=1 k xi k i,j =1
(5)
The approach used above to construct density matrix corresponding to linear kernel matrix can be
extended to polynomial kernels (Rebentrost et al., 2014b).
LMR Technique for Density Operator Exponentiation. In HHL-based quantum machine learn-
ing algorithms , including in the method proposed here, matrix A for the Hamiltonian simulation
within the HHL algorithm is based on data. For example, A can contain the kernel matrix K cap-
tured in the quantum system as a density matrix. Then, one need to be able to efficiently compute
e-iK∆t, where K is scaled by the trace of kernel matrix. Since K is not sparse, a strategy similar
to (Lloyd et al., 2014) is adapted for the exponentiation ofa non-sparse density matrix:
Tri {e-i^t(K 0 σ)ei"t} = σ - i∆t[K, σ]+ O (∆t2) ≈ e-iK∆tσeiK∆t,	(6)
4
Under review as a conference paper at ICLR 2020
where S = i,j |iihj| 0 ∣jihi∣ is the SWaP operator and the facts Tri {S(K 0 σ)} = Kσ and
Tri {(K 0 σ)S} = σK are used. The equation (6) summarizes the LMR technique: approximating
e-iK∆tσeiK∆t up to error O(∆t2) is equivalent to simulating a sWap operator S, applying it to
the state K 0 σ and discarding the first system by taking partial trace operation. Since the sWap
operator is sparse, its simulation is efficient. Therefore the LMR trick provides an efficient Way to
approximate exponentiation of a non-sparse density matrix.
Quantum LS-SVM. Quantum LS-SVM (Rebentrost et al., 2014b) uses partial trace to construct
density operator corresponding to the kernel matrix K. Once the kernel matrix K becomes available
as a density operator, the quantum LS-SVM proceeds by applying the HHL algorithm for solving
the system of linear equations associated With LS-LSVM, using the LMR technique for performing
the density operator exponentiation e-iK∆t Where the density matrix K encodes the kernel matrix.
3 Quantum Semi-Supervised Least S quare SVM.
Semi-Supervised Least Square SVM involves solving the folloWing system of linear equations
b0
α1
K+KL1KT+γ-11-1y0=A-1y0
(7)
In quantum setting the task is to generate |b, αi = A-1 |0, yi, Where the normalized A
A
Tr(A).
The linear system differs from the one in LS-SVM in that instead of K, We have K + KLK .
While this difference is of little significance for classical solvers, in quantum systems We cannot just
multiply and then add the matrices and then apply quantum LS-SVM - we are limited by the unitary
nature of quantum transformations.
In order to obtain the solution to the quantum Semi-Supervised Least Square SVM, we will use the
following steps. First, we will read in the graph information to obtain scaled graph Laplacian matrix
as a density operator. Next, we will use polynomial Hermitian exponentiation for computing the
matrix inverse (K + KLK)-1.
3.1	Quantum Input Model for the Graph Laplacian
In the semi-supervised model used here, we assume that we have information on the similarity of
the training samples, in a form of graph G that uses n edges to connect similar training samples,
represented as m vertices. We assume that for each sample, G contains its k most similar other
samples, that is, the degree of each vertex is d. To have the graph available as a quantum density
operator, we observe that the graph Laplacian L is the Gram matrix of the rows of the m × n graph
incidence matrix GI, L = GIGIT. We assume oracle access to the graph adjacency list, allowing us
to construct, in superposition, states corresponding to rows of the graph incidence matrix GI
That is, state E〉has probability amplitude 册 for each edge |t〉incident with vertex i, and null
probability amplitude for all other edges. In superposition, we prepare a quantum state combining
rows of the incidence matrix for all vertices, to obtain
1m
IGIi =	√——ʒ X |ii 0 |vii
md
i=1
The graph Laplacian matrix L, composed of inner products of the rows of GI, is obtained by dis-
carding the second part of the system,
1m	1m
L = Tr2(IGIihGII) = md X liihjl 0dhvi∖vji = m X hvi|vji|iihj|.	(8)
i,j=1	i,j=1
5
Under review as a conference paper at ICLR 2020
3.2	Polynomial Hermitian Exponentiation for Semi Supervised Learning
For computing the matrix inverse (K + KLK)-1 on a quantum computer that runs our quan-
tum machine algorithm and HHL algorithm as a subroutine, we need to efficiently compute
e-i(K+KLK)∆tσei(K+KLK)∆t. For this purpose we adapt the generalized LMR technique for
simulating Hermitian polynomials proposed in (Kimmel et al., 2017) to the specific case of
e-i(K+KLK)∆tσei(K+KLK)∆t. Simulation of e-iK∆t follows from the original LMR algorithm,
and therefore we focus here only on simulation e-iKLK∆t. The final dynamics (K + KLK)-1 can
be obtained by sampling from the two separate output states for e-iKLK∆t and e-iK∆t.
Simulating eiKLK∆t. Let D(H) denote the space of density operators associated with state space
H. Let K 1 ,K,L ∈ D (H) be the density operators associated with the kernel matrix and the LaPla-
cian, respectively. We will need two separate systems with the kernel matrix K, to distinguish
between them We Win denote the first as K * and the second as K; since K is real and symmetric,
these are indeed equal. The kernel and Laplacian matrices K * ,K,L are not sparse therefore we
adapt the generalized LMR technique for simulating Hermitian polynomials for our specific case
B = K*LK.
For adapting the generalized LMR technique to our problem we need to generate a quantum state
ρ0 = |0〉〈0| 0 ρ" + |1〉〈1| 0 ρ"0 with Tr(ρ00 + ρ000) = 1 , such that
Tri {Tr3 {e-iS0δ (ρ0 0 σ) eiS'& }} = σ - i[B,σ] + O(∆2) = e-iBtσeiBt + O(∆2),	(9)
where B = ρ00 — P000 = ɪ K * LK + ɪ KLK * = KLK and S0 := |0〉〈0| 0 S + |1)〈1| 0 (-S) is a
controlled partial swap in the forward (+S) and backward direction (-S) in time, and
e-iS0∆ = |0ih0| 0 e-iS∆ + |1ih1| 0 eiS∆.
Therefore with one copy of ρ0, we obtain the simulation of e-iB∆ up to error O(∆2). If we choose
the time slice ∆ = δ∕t and repeating the above procedure for t2/δ times, we are able to simulate
e-iBt up to error O(δ) using n = O (t2 /δ) copies of ρ0.
Generating ρ0 = |0ih0| 0 ρ00 + |1ih1| 0 ρ000. Figure 1 shows the quantum circuit for creating ρ0 =
|0ih0| 0 ρ00 + |1ih1| 0 ρ000 such that T r(ρ00 + ρ000) = 1 and B = ρ00 - ρ000 = KLK.
Figure 1: Quantum circuit for creating ρ0 = |0ih0| 0ρ00+ |1ih1| 0 ρ000. The circuit is to be read from
left-to-right. Each wire at left shows its corresponding input state. The vertical rectangle denotes the
cyclic permutation operator P on K, L, K defined in (10). H is the Hadamard gate, and the waste
bins show partial trace. The measurement on the first quantum state is in computational basis.
The analysis of the steps preformed by the circuit depicted in Fig.1 is as follows. Let P be the cyclic
permutation of three copies of HA that operates as P |j1,j2,j3i = |j3,j1,j2i. In operator form it
can be written as
dim HA
P := X	|j3i hj1| 0 |j1i hj2| 0 |j2i hj3 |	(10)
j1 ,j2 ,j3=1
6
Under review as a conference paper at ICLR 2020
The input state to the circuit depicted in Fig. 1 is
|+ih+| 0 Kt 乳 L 乳 K = 1 X ∣iihj| 0 Kt 乳 L 乳 K.
i,j∈{0,1}
Applying P on Kt , L, K gives
I = 2[∣0ih0∣ 0 Kt 乳 L ㊈ K + |0〉(1| 乳(Kt 乳 L 乳 K) P
+ |1ih0| 0 P (Kt 乳 L 乳 K) + |1ih1| 0 P (Kt 乳 L 乳 K) P .
After discarding the third and second register sequentially by applying corresponding partial trace
operators, we get
II = Tr2 [Tr3(I)] = ∣0ihO∣ ㊈ 2Kt + |0ih1| 乳 2KtLK + |1ih0| 乳 1KLKt + |1ih1| 乳 2K,
in this step KLK term where the last line obtained from
Tr2 [Tr3 ](Kt 乳 L 乳 K) P]] = KtLK,
Tr2 [Tr3 [P(Kt 乳 L 乳 K)]] = KLKt,
Tr2 [Tr3 [P(Kt 乳 L 乳 K)P]] = K.
After applying a Hadamard gate H = √12 [(|0)+ |1〉)(0| + (|0)- |1〉)(1|] on the first qubit of II, We
get
III = H X 1(II )H X 1 =
1 (|0ih0| + ∣0ih1∣ + ∣1ih0∣ + ∣1ih1∣) X1Kt + 1 (∣0ih0∣-∣0ih1∣ + ∣1ih0∣-∣1ih1∣) X1K tLK
+1 (∣0ih0∣ + ∣0ih1∣-∣1ih0∣-∣1ih1∣) X 2 KLKt +1 (∣0iho∣-∣0ih1∣-∣1iho∣ + 田(1|)X1K
=∣oiho∣X 2	(1 Kt + 2 K tLK +2 KLKt + 2 K 22	2	2	∣+ ∣0ih1∣X 2	(2 k t -	1KtLK + 1KLKt - 1K
+ ∣1iho∣X 2(	'1 Kt + 1KtLK - 1KLKt - 1K ) ∖2	' 2	2	2	)	+ ∣1ih1∣X 2(	'1 Kt - ∖2	111 -KtLK - -KLKt + -K
The last step is applying a measurement in computational basis {|0ih0|, |1ih1|} on the first register
to obtain our desired state ρ0 ,
IV = ∣0ih0∣X 1( 1 Kt + 1K tLK + 1KLKt + 1 K)
22	2	2	2
+ ∣1ih1∣ X 2 (2Kt -1KtLK- 2KLKt + 2K
We can see that by defining ρ00 = ∣ (21 Kt + ɪ K tLK + 21 KLKt + 2 K) and ρ000 =
2 (2Kt 一 1KtLK _ 1KLKt + 1 K) the final state is in the form of ρ0 = |0〉〈0回夕"+|1〉〈1回夕000
where Tr(ρ00 + PD = 1, and we obtain ρ00 - ρ000 = 2KtLK + 2KLKt = B.
NoW With having the output state ρ0 We are ready to apply the generalized LMR in (9) to sim-
ulate e-iKLK∆tσeiKLK∆t up to error O(∆2). Comparing the LMR technique in equation (6)
with the generalized LMR for the spacial case of KLK in equation (9), we see approximating
e-iKLK∆tσeiKLK∆t up to error O(∆t2) is equivalent to simulating the controlled partial swap op-
erator S0, applying it to the state ρ0 X σ and discarding the third and first systems by taking partial
trace operations, respectively. Since S0 is also sparse, and its simulation is efficient, the generalized
LMR technique offers an efficient approach for simulating eiKLK∆t .
7
Under review as a conference paper at ICLR 2020
Algorithm 1 Quantum Semi-Supervised LS-SVM
Input: The datapoint set {x1, ...xl, ...xm} with the first l data points labeled and the rest unlabeled,
y = (y1, ...,yl) and the graph G
Output: The classifier ∣α, b)= A-1∣yi
1: Quantum data preparation. Encode classical data points into quantum data points using
quantum oracles Ox : {x1, ...xl, ...xm } 7→ |Xi
1
Pm=IIii 乳 k Xi k ∣Xii and
Ox : y 7→ |yi.
2:	Quantum Laplacian preparation. Prepare quantum density matrix using oracle access to G.
3:	Matrix inversion. Compute the matrix inversion ∣a,b) = A-1∣y) via HHL algorithm. A
quantum circuit for the HHL algorithm has three main steps:
4:	Phase estimation, including efficient Hamiltonian simulation involving KLK (Section 3.2)
5:	Controlled rotation
6:	Uncomputing
7:	Classification. Based on Swap test algorithm, same as in Quantum LS-SVM.
3.3	Quantum Semi-Supervised LS-SVM Algorithm and its Complexity
The quantum LS-SVM in (Rebentrost et al., 2014b) offers exponential speedup O(log mp) over
the classical time complexity for solving SVM as a quadratic problem, which requires time
O(log(-1)poly(p, m)), where is the desired error. The exponential speedup in p occurs as the
result of fast quantum computing of kernel matrix, and relies on the existence of efficient oracle ac-
cess to data. The speedup on m is due to applying quantum matrix inversion for solving LS-SVM,
which is inherently due to fast algorithm for exponentiation of a resulting non-sparse matrix. Our
algorithm introduces two additional steps: preparing the Laplacian density matrix, and Hamiltonian
simulation for KLK. The first step involves oracle access to a sparse graph adjacency list represen-
tation, which is at least as efficient as the oracle access to non-sparse data points. The Hamiltonian
simulation involves simulating a sparse conditional partial swap operator, which results an efficient
strategy for applying e-iKLK∆t in time O(log(m)∆t), where the notation O hides more slowly
growing factors in the simulation (Berry et al., 2007).
3.4	Comparison with Alternative Approaches
Considerable effort has been devoted into designing fast classical algorithms for training SVMs. The
decomposition-based methods such as SMO (Platt, 1998) are able to efficiently manage problems
with large number of features p, but their computational complexities are super-linear in m. Other
training strategies (Suykens & Vandewalle, 1999; Fung & Mangasarian, 2005; Keerthi & DeCoste,
2005) are linear in m but scale quadratically in p in the worst case. The Pegasos algorithm (Shalev-
Shwartz et al., 2011) for non-linear kernel improves the complexity to O (m∕(λe)), where λ, and E
are the regularization parameter of SVM and the error of the solution, respectively.
Beyond the classical realm, three quantum algorithms for training linear models have been proposed,
the quantum LS-SVM that involves L2 regularizer (Rebentrost et al., 2014a), a recently proposed
Quantum Sparse SVM which is limited to a linear kernel (Arodz & Saeedi, 2019), and a quantum
training algorithm that solves a maximin problem resulting from a maximum - not average - loss
over the training set (Li et al., 2019).
References
Scott Aaronson. Read the fine print. Nature Physics, 11(4):291, 2015.
Tom Arodz and Seyran Saeedi. Quantum sparse support vector machines. arXiv preprint
arXiv:1902.01879, 2019.
Srinivasan Arunachalam and Ronald de Wolf. A survey of quantum learning theory. ACM SIGACT
News, 48(2):41-67, 2017.
8
Under review as a conference paper at ICLR 2020
Srinivasan Arunachalam, Vlad Gheorghiu, Tomas Jochym-O’Connor, Michele Mosca, and
Priyaa Varshinee Srinivasan. On the robustness of bucket brigade quantum RAM. New Jour-
nal of Physics, 17(12):123010, 2015.
Dominic W Berry, Graeme Ahokas, Richard Cleve, and Barry C Sanders. Efficient quantum algo-
rithms for simulating sparse Hamiltonians. Communications in Mathematical Physics, 270(2):
359-371, 2007.
Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.
Quantum machine learning. Nature, 549(7671):195, 2017.
Miles Blencowe. Quantum computing: Quantum RAM. Nature, 468(7320):44, 2010.
Vedran Dunjko and Hans J Briegel. Machine learning & artificial intelligence in the quantum do-
main: a review of recent progress. Reports on Progress in Physics, 81(7):074001, 2018.
Glenn M Fung and Olvi L Mangasarian. Multicategory proximal support vector machine classifiers.
Machine Learning, 59(1-2):77-97, 2005.
Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access memory. Physical
Review Letters, 100(16):160501, 2008.
Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of
equations. Physical Review Letters, 103(15):150502, 2009.
S Sathiya Keerthi and Dennis DeCoste. A modified finite newton method for fast solution of large
scale linear SVMs. Journal of Machine Learning Research, 6:341-361, 2005.
Shelby Kimmel, Cedric Yen-Yu Lin, Guang Hao Low, Maris Ozols, and Theodore J Yoder. Hamil-
tonian simulation with optimal sample complexity. npj Quantum Information, 3(1):13, 2017.
Tongyang Li, Shouvanik Chakrabarti, and Xiaodi Wu. Sublinear quantum algorithms for training
linear and kernel-based classifiers. In Proceedings of the 36th International Conference on Ma-
chine Learning, pp. 3815-3824, 2019.
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis.
Nature Physics, 10(9):631, 2014.
Stefano Melacci and Mikhail Belkin. Laplacian support vector machines trained in the primal.
Journal of Machine Learning Research, 12(Mar):1149-1184, 2011.
Alejandro Perdomo-Ortiz, Marcello Benedetti, John Realpe-GOmez, and RUPak Biswas. OPPor-
tunities and challenges for quantum-assisted machine learning in near-term quantum computers.
Quantum Science and Technology, 3(3):030502, 2018.
John Platt. SeqUential minimal oPtimization: A fast algorithm for training sUPPort vector machines.
Technical RePort MSR-TR-98-14, Microsoft, 1998.
Patrick Rebentrost, MasoUd Mohseni, and Seth Lloyd. QUantUm sUPPort vector machine for big
data classification. Physical Review Letters, 113(13):130503, 2014a.
Patrick Rebentrost, MasoUd Mohseni, and Seth Lloyd. QUantUm sUPPort vector machine for big
data classification. Physical Review Letters, 113(13):130503, 2014b.
M. SchUld and F. PetrUccione. Supervised Learning with Quantum Computers. SPringer NatUre,
2018.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated
sUb-gradient solver for svm. Mathematical Programming, 127(1):3-30, 2011.
Johan AK SUykens and Joos Vandewalle. Least sqUares sUPPort vector machine classifiers. Neural
Processing Letters, 9(3):293-300, 1999.
9