Under review as a conference paper at ICLR 2020
Refining the variational posterior through
ITERATIVE OPTIMIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Variational inference (VI) is a popular approach for approximate Bayesian infer-
ence that is particularly promising for highly parameterized models such as deep
neural networks. A key challenge of variational inference is to approximate the
posterior over model parameters with a distribution that is simpler and tractable
yet sufficiently expressive. In this work, we propose a method for training highly
flexible variational distributions by starting with a coarse approximation and iter-
atively refining it. Each refinement step makes cheap, local adjustments and only
requires optimization of simple variational families. We demonstrate theoretically
that our method always improves a bound on the approximation (the Evidence
Lower BOund) and observe this empirically across a variety of benchmark tasks.
In experiments, our method consistently outperforms recent variational inference
methods for deep learning in terms of log-likelihood and the ELBO. We see that
the gains are further amplified on larger scale models, significantly outperforming
standard VI and deep ensembles on residual networks on CIFAR10.
1	Introduction
Uncertainty plays a crucial role in a multitude of machine learning applications, ranging from
weather prediction to drug discovery. Poor predictive uncertainty risks potentially poor outcomes,
especially in domains such as medical diagnosis or autonomous vehicles where some forms of high
confidence errors may be especially costly (Amodei et al., 2016). Thus, it is becoming increasingly
important that the underlying model provides high quality uncertainty estimates along with its pre-
dictions. Yet, possibly the most widely used models, deep neural networks (LeCun et al., 2015), are
unable to accurately quantify model uncertainty. They are often overconfident in their predictions,
even when their predictions are incorrect (Guo et al., 2017; Ovadia et al., 2019).
By marginalizing over a posterior distribution over the parameters given the training data, Bayesian
inference provides a principled approach to capturing uncertainty. In contrast, standard training of
neural networks employs a point estimate of the parameters, which cannot account for model un-
certainty. Unfortunately, exact Bayesian inference is intractable in general for neural networks. To
model epistemic uncertainty, variational inference (VI) instead approximates the true posterior with
a simpler distribution. The most widely used one for neural networks is the mean-field approxi-
mation, where the posterior is represented using an independent Gaussian distribution over all the
weights. Variational inference is appealing since it reduces the problem of inference to an optimiza-
tion problem, minimizing the discrepancy between the true posterior and the variational posterior.
The key challenge, however, is the task of training expressive posterior approximations that can
capture the true posterior without significantly increasing the computational costs.
This paper describes a novel method for training highly flexible posterior approximations. The idea
is to start with a coarse, mean-field approximation q(w) and make iterative, local refinements to it.
The regions of the local refinements are determined by sampling the values of additive auxiliary
variables. The model parameters w are expressed using a number of auxiliary variables ak (Figure 1
left) for k = 1, . . . , K that leave the marginal distribution unchanged. In each iteration, we sample
the value ofan auxiliary variable according to the current variational approximation q(ak) and refine
the approximation by conditioning on the newly sampled value q(w) ≈ p(w|x, y, a1:k) (Figure 1
right illustrates the process). Each refinement step makes cheap, local adjustments to the variational
posterior in the region of the sampled auxiliary variables. At the end, we draw one sample from
1
Under review as a conference paper at ICLR 2020
Figure 1: (Left) The supervised learning model and augmented model respectively where w is ex-
pressed as a sum of independent auxiliary variables. (Right) Illustration of the refining algorithm.
In each iteration the value of an auxiliary variable is fixed and the posterior is locally adjusted. In
the final iteration, a sample is drawn from w. Through the iterations, the variational distribution is
able to approximate well the true posterior in a small region.
the refined q(w). The refinement iterations have to be repeated for each posterior sample. The
algorithm results in samples from a highly complex distribution, starting from a simple mean-field
approximation. While the distribution of the samples is difficult to quantify, we show that it is not
limited to factorized, uni-modal forms, and that the procedure is guaranteed to improve the resulting
ELBO without posing a significant computational overhead.
Summary of contributions:
•	A novel algorithm for refining a variational distribution, increasing its flexibility.
•	We show that the refinement steps are guaranteed to improve the quality of the variational
distribution under mild conditions.
•	We showcase the effectiveness of the method on Bayesian neural networks using a set
of UCI regression and image classification benchmarks. We set a new state-of-the-art in
uncertainty estimation using variational inference at ResNet scale (ResNet-20, (He et al.,
2016)) scale on CIFAR10.
2	Methods
In this section, we first describe standard variational inference (VI), followed by a detailed descrip-
tion of the iterative refinement algorithm. While VI and our proposed methodology are generally
applicable to latent variable models, in this work, we consider the application to Bayesian neural
networks (Figure 1), where x are inputs, y outputs, and w the weights of network.
2.1	Variational Inference in Bayesian Neural Networks
Exact Bayesian inference in Bayesian neural networks (BNN) (Figure 1) is often intractable and
is NP-hard in the general case. Variational inference attempts to approximate the true posterior
p(w|x, y) with an approximate posterior qφ(w), typically from a simple family of distributions,
for example independent Gaussians over the weights, i.e. the mean-field approximation. To en-
sure that the approximate posterior is close to the true posterior, the parameters of qφ(w), φ are
optimized to minimize their Kullback-Leibler divergence: DKL (qφ (w) k p(w|x, y)). At the limit
of DKL (qφ (w) k p(w|x, y)) = 0, the approximate posterior exactly captures the true posterior,
although this might not be achievable if p(w|x, y) is outside of the distribution family of qφ(w).
In order to minimize the KL-divergence, variational inference optimizes the Evidence Lower Bound
(ELBO) w.r.t. φ (denoted as L(φ)), which is a lower bound to the log marginal likelihood log p(y|x).
Since the the marginal likelihood can be expressed as the sum of the KL-divergence and the ELBO,
maximizing the ELBO is equivalent to minimizing the KL divergence:
log p(y|x) =	DKL qφ (w)	p(w|x, y)	+L(φ)	≥	L(φ) =	Eqφ	log p(y|x, w)	-DKL	qφ(w)	p(w) ,
X----------{----------}
≥0
due to non-negativity of the KL-divergence.
2
Under review as a conference paper at ICLR 2020
1:	procedure REFINEANDSAMPLE(φ)
2:	for m = 1, . . . , M do
3:	φo - φ
4:	for k = 1, . . . , K do
5：	ak 〜qφk-ι (ak)	. Sample ak
6：	qφk (W) - qφk-ι (w|ak)	. Initialize φk
7：	Φk J arg maxφk。同求(Φk)	. Optimize φk
8:	end for
9：	Wm 〜qφκ (w)	. Sample the refined posterior
10：	end for
11：	return W1:M
12： end procedure
Figure 2： The ELBO staircase：
ELBOaux is increasing as we sam-
ple the auxiliary variables (sin-
gle sample Monte Carlo estimate,
LeNet-5/CIFAR10).1
Algorithm 1： Pseudocode for the iterative refinement of the
variational posterior
Following the optimization of φ, the model can be used to make predictions on unseen data. For a
new inputx0, the predictive distributionp(y0|x0, y, x) can be approximated by stochastically drawing
a small number (around M ≤ 10) of sample model parameters and averaging their prediction in an
ensemble model：
1M
wi：M 〜qφ(w), p(y0lχ0, y,x) ≈ M £p(y0|x0, Wi).
M i=1
2.2	Refining the posterior
The main issue with variational inference is the inflexibility of the posterior approximation. The
most widely used variant of variational inference, mean-field variational inference, approximates the
posterior with independent Gaussians across all dimensions. This approximation is too simplistic
to capture the complexities of the posterior for complicated models such as BNNs. Our idea is to
refine the posterior approximation through iterative optimization. Using the refinement procedure,
it is feasible to train a detailed posterior in the regions of the posterior samples used for prediction
while relying on a coarse-grained approximation further away from these samples.
The graphical model is augmented with a finite number of auxiliary variables a1:K as shown in
Figure 1. The constraints are that (x, y) must be conditionally independent of the auxiliary variables
given W and that they must not affect the prior distribution p(W). This is important in justifying the
use of the initial variational approximation. A significant way in which we distinguish ourselves
from hierarchical variational models (Ranganath et al., 2016) is that the model is unaffected by the
presence of the auxiliary variables. Their purpose is solely to aid the inference procedure. Given a
Gaussian prior N(0, σw1 2 ) over W, we express W as a sum of independent auxiliary variables2
K
W =	ak ,	with p(ak) = N(0, σa2 ) for k = 1, . . . , K ,
k=1
while ensuring that PkK=1 σa2 = σw2 so that the prior p(W) = N (0, σw2 ) is unchanged.
Locally refining the approximate posterior refers to iteratively sampling the values of the auxiliary
variables a1:K and then approximating the posterior conditional on the sampled values, i.e. qφk (W)
approximates p(W|x, y, a1:k) for iterations k = 1, . . . , K (Algorithm 1). Starting from the initial
mean field approximation qφ(w), We sample the value of ai from qφ(aι) = Jp(aι∣w)qφ(w) dw,
1The sudden drops after sampling are optimizer artefacts due to having to reset the parameters of Adam.
2While We are focusing on one specific definition of the auxiliary variables, additive auxiliary variables,
note that all of our results straight-forWardly generalize to arbitrary joint distributions p(w, a1:K) that meet the
constraints.
3
Under review as a conference paper at ICLR 2020
(a)
Figure 3: A simple multi-modal example demonstrating how our method can capture a more com-
plex distribution by refining a simple mean-field posterior. In (a) the true posterior is too com-
plex to be well approximated by a Gaussian. (b) The Gaussian approximate posterior after opti-
mizing the ELBO (ELBO = -1.79). (c) After sampling a1, we optimize the conditional ELBO
w.r.t. φι∣aι. Wm is drawn from qg.ai(w). (d) Samples from the refined posterior approximation.
ELBO ≥ -1.45.
then optimize the approximation to the conditional posterior: qφ1 (w) ≈ p(w|x, y, a1). This proce-
dure is then iteratively repeated for a2:K . In iteration k,
1)	ak 〜/ p(ak |ai：k-i, w)qφk-ι (w)dw 2) φk = arg min Dkl (qφk (W)Il p(w|x,y,ai：k)).
Analogously to variational inference, the KL divergence is minimized through the opti-
mization of the conditional ELBO in each iteration: L|a1:k (φk) = Eqφ log p(y |x, W) -
DKL(qφk (W) k p(W|a1:k)). In order to get independent samples from the variational posterior,
we have to repeat the iterative refinement for each ensemble member W1:M .
Toy example We use a toy example to demonstrate the procedure. In this toy example, we have
a single weight W with prior p(W) = N (0, 1) and a complicated posterior with four modes (syn-
thetically generated data). We express W as the sum of two auxiliary variables W = a1 + a2 with
p(a1) = N(0, 0.8) andp(a2) = N(0, 0.2) (which recovers p(W) as per the constraint).
As Figure 3b shows, a Gaussian approximation to the posterior fails to capture the multimodal na-
ture of the true posterior. The first step of the refinement is to sample using q@: aι 〜q©(aι)=
J p(aι∣w)qφ(w) dw. Next, we condition on the value of aι and initialize the parameters of the vari-
ational posterior accordingly: q$、(w) = N(w∣μφι ,o^) J q©(w|ai). Both qφ(aι) and qφ(w∖aι)
can be computed analytically for Gaussian distributions. After optimizing φ1, the approximate pos-
terior qφ1 (W) is able to capture a good, local approximation to the posterior p(W∖a1, x, y) (Figure
3c). In Figure 3d, we can see the histogram of the refined posterior, that is, the distribution we are
generating samples from (for each sample from W, we drew a sample from qφ(a1) and optimized
qφ1 (W)). Clearly, it is a much better approximation to the true posterior than the Gaussian approx-
imation we started with, although it is important to note that the true posterior is not recovered
exactly.
2.3	Theoretical justification
Our theoretical claims are twofold. Firstly, that through this procedure, we are optimizing a lower
bound to the ELBO and secondly, that the refinement cannot result in a worse posterior approxima-
tion than the initial mean-field approximation that we start with (in the ELBO sense). That is,
ELBOref ≥ ELBOaux ≥ ELBOinit ,
where ELBOref denotes the ELBO of the refined posterior qref, ELBOaux refers to the objective that
the refinement process is optimizing, and ELBOinit is the ELBO of the initial mean-field approxi-
mation.
Lower bound to the ELBO Consider the case with two auxiliary variables a1 and a2. The initial
training optimizes the ELBOinit = Eqφ log p(y∖x, W) - DKL qφ(W) I p(W) and the refinement
step optimizes the conditional ELBO, L|a1 (φ1) = Eqφ log p(y∖x, W) -DKL qφ1 (W) I p(W∖a1) .
4
Under review as a conference paper at ICLR 2020
The key observation is that we can define ELBOaux that is a lower bound to ELBOref and is increased
both by the initial training and the refinement steps:
ELBOaux = Eqφ hEqφ1 h logp(y∣x, W)- log ^Iw] - log q^]
φ φ1	p(w|a1)	p(a1)
= Eqref logp(y|x, w) - DKL qref(w, a1) p(w, a1)
≤ Eqref [ logp(y∣x,w)] - DκL(qref(w) Il P(W)) =ELBOref ,
since the KL divergence of the joint distribution is greater than or equal to that of the marginals.
Guarantee of improvement Improvement in the ELBO (ELBOref ≥ ELBOinit) is guaranteed
under two assumptions. First, that the conditional variational posterior, qψ(w∖a1), is within the
variational family of qφ1 . Second, that the process that optimizes φ1 does not make it worse than
the value it was initialized with. The first assumption holds for Gaussian families: qφ(w∖a1) is
Gaussian and can be computed in closed form. The second assumption is reasonable to assume for
most optimizers and, in addition, it can be ensured by comparing the initial value to the final value
and choosing the one with the more desirable objective value.
The argument goes as follows. By initializing qφ1 such that it coincides withqφ(w∖a1), we can
ensure that ELBOaux ≥ ELBOinit, since they are equal at initialization time and the optimization
process does not decrease ELBOaux. From this combined with our previous result, it follows that
ELBOref ≥ ELBOinit and therefore ensuring that the ELBO improves through the refining steps.
Note that this also implies that it is not necessary to optimize until convergence: any amount of
optimization increases the ELBO.
Figure 2 shows that the ELBO improvement occurs on real world datasets. With the sampling of
each auxiliary variable, the ELBO improves forming a staircase pattern. Table 1 and 2 serve as
further empirical evidence that the ELBO improves as a result of the refinement steps.
Extending to multiple auxiliary variables For simplicity, we stated the arguments with two aux-
iliary variables, but they straight-forwardly extend to any finite number of auxiliary variables. In
this scenario, there are K ELBOaux-s, one for each auxiliary variable, upper bounded by ELBOref
and lower bounded by ELBOinit.
3	Related works
While in theory, the Bayesian approach can accurately capture uncertainty, in practice we find that
exact inference is computationally infeasible in most scenarios and thus we have to resort to approx-
imate inference methods. There is a wealth of research on approximate inference methods, here we
focus on works closely related to this paper.
Variational inference (Hinton & Van Camp, 1993) tries to approximate the true posterior distribution
over parameters with a variational posterior from a simple family of distributions. Mean-field VI,
which for neural networks traces back to Peterson (1987), uses independent Gaussian distributions
over the parameters to try to capture the posterior. The advantage of the mean-field approximation
is that the network can be efficiently trained using the reparameterization trick (Kingma & Welling,
2013) and the variational posterior has a proper density over the parameter space which then can be
used across tasks like continual learning (Osawa et al., 2019; Nguyen et al., 2017) and contextual
bandits (Riquelme et al., 2018). Recently, Louizos & Welling (2017) showed that normalizing flows
can be employed to further increase the flexibility of the variational posterior. Zhang et al. (2018a)
provide a detailed survey of recent advances in VI.
Our method is a novel variant of the auxiliary variable approaches to VI (Agakov & Barber, 2004;
Ranganath et al., 2016) that increase the flexibility of the variational posterior through the use of aux-
iliary variables. The key distinction, however, is that instead of trying to train a complex variational
approximation over the joint distribution, we iteratively train simple, mean-field approximations at
the sampled values of the auxiliary variables. While this poses an O(MK) (K is the number of aux-
iliary variables and M is the number of posterior samples) overhead over mean-field VI, the training
itself is kept straightforward and efficient. The introduction of every new auxiliary variable increases
5
Under review as a conference paper at ICLR 2020
the flexibility of the posterior approximation. In contrast to MCMC methods, it is unclear whether
the algorithm approaches the true posterior in the limit of infinitely many auxiliary variables.
There are also numerous methods that start with an initial variational approximation and refine it
through a few MCMC steps (Salimans et al., 2015; Zhang et al., 2018b; Ruiz & Titsias, 2019). The
distinction from our algorithm is that we refine the posterior starting at large scale and iteratively
move towards smaller scale refinements whereas these methods only refine the posterior at the scale
of the MCMC steps. Guo et al. (2016); Miller et al. (2017) and Locatello et al. (2018) used boosting
to refine the variational posterior, iteratively adding parameters such as mixture components to min-
imize the residual of the ELBO. Our method does not add parameters at training time but instead
iteratively refines the samples through the introduction of auxiliary variables. We did not include
these models among our baselines because they have yet to be applied to Bayesian multi-layer neural
networks.
Further related works include methods that iteratively refine the posterior in latent variable mod-
els (Hjelm et al., 2016; Cremer et al., 2018; Kim et al., 2018; Marino et al., 2018). These methods,
however, focus on reducing the amortization gap and they do not increase the flexibility of the vari-
ational approximation.
Lastly, there are non-Bayesian strategies for estimating epistemic uncertainty in deep learning. Boot-
strapping (Breiman, 1996) and deep ensembles (Lakshminarayanan et al., 2017) are perhaps the
most promising. Deep ensembles, in particular, have been demonstrated to achieve strong perfor-
mance on benchmark regression and classification problems and uncertainty benchmarks includ-
ing out-of-distribution detection (Lakshminarayanan et al., 2017) and prediction under distribution
shift (Ovadia et al., 2019). Both methods rely on constructing a set of independently trained models
to estimate the uncertainty. Intuitively, the amount of disagreement across models reflects the uncer-
tainty in the ensemble prediction. To induce diversity among the ensemble members, bootstrapping
subsamples the training set for each member while deep ensembles use the randomness in weight
initialization and mini-batch sampling.
4	Experiments
The goal of the experiments is twofold. First, we empirically confirm the improvement in the ELBO
as claimed in Section 2.3. Second, we quantify the improvement in the uncertainty estimates due
to the refinement. We conducted experiments on a selection of regression and classification bench-
marks using Bayesian neural networks as the underlying model. We looked at the marginal log-
likelihood of the predictions as well as accuracy in classification tasks.
Refinement (Refined VI) After training the initial mean-field approximation, we refine M = 10
ensemble members, each with K = 5 auxiliary variables. The means on their prior distributions are
fixed at 0., and their variances form a geometric series (each auxiliary variable has variance 0.3 times
the previous one, which roughly halves the standard deviation of the prior each time): σa21 = 0.7σw2 ,
σ2	=	0.21σ2 ,	σ2	=	0.063σ2 ,	σ2	=	0.0189σ2 ,	and σ2 = 0.0081σ2	. In each refinement
a2	w a3	w	a4	w	a5	w
iteration, we optimized the posterior with Adam (Kingma & Ba, 2014) for 200 iterations. To keep
the training stable, we scaled the learning rate according to the standard deviation of the conditional
k
posterior: in iteration k,lr=0.320.001.
4.1	Baselines
We used three baselines. First, mean-field variational inference in order to quantify the improvement
provided by refining. Second, multiplicative normalizing flows (MNF) to compare against a more
flexible posterior approximation and third, deep ensemble models to compare against a state-of-the-
art non-Bayesian approach. For all methods we used a batch size of 256 and the Adam optimizer
with the default learning rate of 0.0013.
3The hyperparameters of each baseline were tuned using a Bayesian optimization package, however we
found batch size and learning rate to be consistent across methods.
6
Under review as a conference paper at ICLR 2020
Variational inference (VI) (Graves, 2011; Blundell et al., 2015) Naturally, we investigate the
improvement over variational inference with a mean-field Gaussian posterior approximation. We do
inference over all weights and biases with a Gaussian prior centered at 0. The variance of the prior
is tuned through empirical Bayes. This model is trained for 30000 iterations.
Multiplicative Normalizing Flows (MNF), (Louizos & Welling, 2017) To measure the perfor-
mance against a more flexible class of posterior approximations, we look at Multiplicative Nor-
malizing Flows. In this work, the posterior means are augmented with a multiplier from a flexible
distribution parameterized by the masked RealNVP. This model is trained with the default flow
parameters for 30000 iterations.
Deep ensemble models, (Lakshminarayanan et al., 2017) Deep ensemble models are shown to
be surprisingly effective at quantifying uncertainty. While they are less principled than Bayesian
methods, they are certainly a competitive baseline. For the regression datasets, we used adversarial
training ( = 0.01) whereas in classification we did not use adversarial training (adversarial training
did not give an improvement on the classification benchmarks). For each dataset, we trained 10
ensemble members for 5000 iterations each.
4.2	Regression benchmarks
Following Hernandez-Lobato & Adams (2015), We evaluate the methods on a set ofUCI regression
benchmarks (Table 1). The datasets used a random 80-20 split for training and testing. The model
used for these datasets is a feed forward neural network with a single hidden layer containing 50 units
with a ReLU activation function. As it is common practice, we utilize the local reparameterization
trick (Kingma et al., 2015).
On these benchmarks, we are able to confirm that the refinement step consistently improves both the
ELBO and the uncertainty estimates over VI. On 7 out of the 9 datasets, Refined VI is one of the
best performing approaches.
	Deep Ensemble MLL	MNF MLL	VI		Refined VI (This work)	
			MLL	ELBO	MLL	ELBO
boston_housing	-9.136±5.719	-2.920±0.133	-2.874±0.151	-668.272±7.647	-2.851±0.185	≥ -630.379±7.716
COnCrete-Strength	-4.062±0.130	-3.202±0.055	-3.138±0.063	-3248.137±68.575	-3.131±0.062	≥ -3071.124±64.046
navaLpropulsion	3.995±0.013	3.473±0.007	5.969±0.245	53440.701±2047.340	6.128±0.171	≥ 54882.656±1228.361
energy .efficiency	-0.666±0.058	-0.756±0.054	-0.749±0.068	-1296.721±66.310	-0.707±0.094	≥ -1192.337±62.089
yacht_hydrodynamics	-0.984±0.104	-1.339±0.170	-1.749±0.232	-928.758±112.928	-1.626±0.231	≥ -790.052±84.716
kin8nm	1.135±0.012	1.125±0.022	1.066±0.019	6071.268±61.758	1.069±0.018	≥ 6172.709±67.659
power-Plant	-3.935±0.140	-2.835±0.033	-2.826±0.020	-22496.579±130.487	-2.820±0.024	≥ -22368.965±85.308
PrOteinstructure	-3.687±0.013	-2.928±0.007	-2.926±0.010	-108806.007±174.522	-2.923±0.009	≥ -108597.593±158.482
wine	-0.968±0.079	-0.963±0.027	-0.973±0.054	-1346.130±18.004	-0.968±0.056	≥ -1311.898±17.487
Table 1: Results on the UCI regression benchmarks with a single hidden layer containing 50 units.
Metrics: marginal log-likelihood (MLL, higher is better), and the evidence lower bound (ELBO
higher is better). The mean values and standard deviations are shown in the table.
4.3	Classification benchmarks
We examine the performance on commonly used image classification benchmarks (Table 2). The
architecture used for this experiment is the LeNet5 (LeCun et al., 1995) architecture containing
three convolutional layers with 6, 16 and 120 channels respectively followed by a feed-forward
layer with 84 units and an output layer with ReLu activations throughout the network. We use the
local reparameterization trick (Kingma et al., 2015) for the dense layers and Flipout (Wen et al.,
2018) for the convolutional layers to reduce the gradient noise.
On the classification benchmarks, we again are able to confirm that the refinement step consistently
improves both the ELBO and the uncertainty estimates over VI. While Refined VI is unable to
outperform Deep Ensembles in classification accuracy, it does outperform them in MLL on the
largest dataset, CIFAR10.
7
Under review as a conference paper at ICLR 2020
	Deep Ensemble MLL & Acc	MNF MLL & Acc	VI		Refined VI (This work)	
			MLL & Acc	ELBO	MLL & Acc	ELBO
mnist	-0.017±0.001	-0.034±0.002	-0.032±0.001	-7618.533±47.589	-0.025±0.001	≥ -6310.824±42.357
	99.4%±0.0	99.1%±0.1	99.1%±0.1		99.2%±0.0	
fashion_mnist	-0.201±0.002	-0.255±0.004	-0.255±0.003	-22830.330±232.654	-0.241±0.004	≥ -20438.955±79.672
	93.1%±0.1	90.7%±0.2	90.7%±0.1		91.3%±0.2	
cifar10	-0.791±0.009	-0.795±0.013	-0.815±0.004	-57257.887±299.570	-0.768±0.007	≥ -50989.217±238.976
	76.3%±0.3	72.8%±0.6	72.3%±0.5		73.5%±0.5	
Table 2: Results on image classification benchmarks with the LeNet-5 architecture, without data
augmentation. Metrics: marginal log-likelihood (MLL, higher is better), accuracy (Acc, higher
is better), and the evidence lower bound (ELBO higher is better). The mean values and standard
deviations are shown in the table.
4.4	Large scale models
To demonstrate the performance on larger scale models, we apply the refining algorithm to Residual
Networks (He et al., 2016) with 20 layers (based on Keras’s ResNet implementation). We look at
two models. Firstly, a model where we do inference over all of the residual blocks and secondly,
following Ovadia et al. (2019), a hybrid model (ResNet Hybrid) where inference is only done over
the final layer of each residual block and every other layer is treated as a regular layer. For this model,
we used a batch-size of 256 and we decayed the learning rate starting from 0.001 over 200 epochs.
We used 10 auxiliary variables each reducing the prior variance by a factor of 0.5. Furthermore
we investigate the effect of Batch Normalization (Ioffe & Szegedy, 2015). While it is difficult to
incorporate batch normalization into the Bayesian framework, its positive effects are undeniable.
Regarding batch normalization, we can confirm the findings of Osawa et al. (2019), that it provides
a substantial improvement for VI, although interestingly, this improvement disappears in the case of
the hybrid model. To our knowledge, the refined hybrid model is state-of-the-art in terms of MLL.
It outperforms Osawa et al. (2019) in both MLL and accuracy.
Deep Ensemble	VI	Refined VI (This work)
	MLL	Acc	MLL	Acc	MLL	Acc
ResNet	-0.698	82.7%	-0.795	72.6%	-0.696	75.5%
ResNet + BatchNorm	-0.561	83.6%	-0.672	77.6%	-0.593	79.7%
ResNet Hybrid	-0.698	82.7%	-0.465	84.2%	-0.432	85.8%
ResNet Hybrid + BatchNorm	-0.561	83.6%	-0.465	84.0%	-0.423	85.6%
Table 3: Results on CIFAR10 with the ResNet architecture, without data augmentation. Metrics:
marginal log-likelihood (MLL, higher is better), accuracy (Acc, higher is better), and the evidence
lower bound (ELBO higher is better). Note that the non-hybrid and the hybrid models are equivalent
when trained deterministically.
4.5	Computational costs
When introducing a novel algorithm for variational inference, we have to discuss the computational
costs. Table 4 shows the wall-time required to train each model including the possibility of parallel
training. Deep ensembles require fewer training iterations and parallelize very well, although it is
important to note that the system we tested on is heavily optimized towards training these models
both in hardware and software. For Refined VI, training the initial mean-field approximation can-
not be parallelized, but the following refinement iterations can be straight-forwardly split into M
threads.
	No parallelism	Maximum paralleliSm
Deep Ensemble	433.7 S	43.4 S
MNF	990.9 s	990.9 S
VI	531.5 S	531.5 S
Refined VI	708.6 S	566.9 S
Table 4: The training time of each method (LeNet-5/CIFAR10) on a P100 using Tensorflow.
8
Under review as a conference paper at ICLR 2020
4.6	Conclusions
In this paper, we describe a novel algorithm for refining a coarse variational approximation to the
Bayesian posterior. We show, both theoretically and empirically, that the refined posterior is a better
approximation to the posterior than the initial variational distribution. Our method outperforms the
baseline variational approximations in both uncertainty estimation as well as computational require-
ments. It sets a new state-of-the-art in uncertainty estimation using variational inference at ResNet
scale (ResNet-20) on CIFAR10.
References
Felix V Agakov and David Barber. An auxiliary variational method. In International Conference
on Neural Information Processing, pp. 561-566. Springer, 2004.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Con-
crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613-1622, 2015.
Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoen-
coders. arXiv preprint arXiv:1801.03558, 2018.
Alex Graves. Practical variational inference for neural networks. In Advances in neural information
processing systems, pp. 2348-2356, 2011.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, 2017.
Fangjian Guo, Xiangyu Wang, Kai Fan, Tamara Broderick, and David B Dunson. Boosting varia-
tional inference. arXiv preprint arXiv:1611.05559, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Jose MigUel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of Bayesian neural networks. In International Conference on Machine Learning, pp. 1861-
1869, 2015.
Geoffrey Hinton and Drew Van Camp. Keeping neural networks simple by minimizing the descrip-
tion length of the weights. In in Proc. of the 6th Ann. ACM Conf. on Computational Learning
Theory. Citeseer, 1993.
Devon Hjelm, Ruslan R Salakhutdinov, Kyunghyun Cho, Nebojsa Jojic, Vince Calhoun, and Juny-
oung Chung. Iterative refinement of the approximate posterior for directed belief networks. In
Advances in Neural Information Processing Systems, pp. 4691-4699, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. Semi-amortized
variational autoencoders. arXiv preprint arXiv:1802.02550, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. cite
arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference
for Learning Representations, San Diego, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
9
Under review as a conference paper at ICLR 2020
Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-
zation trick. In Advances in Neural Information Processing Systems. 2015.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems,pp. 6402-6413, 2017.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series.
The handbook of brain theory and neural networks, 3361(10):1995, 1995.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Francesco Locatello, Gideon Dresdner, Rajiv Khanna, Isabel Valera, and Gunnar Raetsch. Boosting
black box variational inference. In Advances in Neural Information Processing Systems, 2018.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 2218-2227. JMLR. org, 2017.
Joseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. arXiv preprint
arXiv:1807.09356, 2018.
Andrew C. Miller, Nicholas J. Foti, and Ryan P. Adams. Variational boosting: Iteratively refining
posterior approximations. In International Conference on Machine Learning, 2017.
Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning.
arXiv preprint arXiv:1710.10628, 2017.
Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard E Turner, Rio
Yokota, and Mohammad Emtiyaz Khan. Practical deep learning with Bayesian principles. Ad-
vances in Neural Information Processing Systems, 2019.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V Dil-
lon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? eval-
uating predictive uncertainty under dataset shift. Advances in Neural Information Processing
Systems, 2019.
Carsten Peterson. A mean field theory learning algorithm for neural networks. Complex systems, 1:
995-1019, 1987.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International
Conference on Machine Learning, pp. 324-333, 2016.
Carlos Riquelme, George Tucker, and Jasper Roland Snoek. Deep Bayesian bandits showdown. In
International Conference on Representation Learning, 2018.
Francisco Ruiz and Michalis Titsias. A contrastive divergence for combining variational inference
and mcmc. In International Conference on Machine Learning, pp. 5537-5545, 2019.
Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational
inference: Bridging the gap. In International Conference on Machine Learning, pp. 1218-1226,
2015.
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-
independent weight perturbations on mini-batches. arXiv preprint arXiv:1803.04386, 2018.
Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in variational
inference. IEEE transactions on pattern analysis and machine intelligence, 2018a.
YichUan Zhang, Jose MigUel Hernandez-Lobato, and ZoUbin Ghahramani. Ergodic measure pre-
serving flows. arXiv preprint arXiv:1805.10377, 2018b.
10
Under review as a conference paper at ICLR 2020
A Closed forms of the sampling distributions
In Section 2.2, We claim that qφ(aι) (used to sample aι) and qφ(w∣aι) (used to initialize qgj have
analytic solutions for additive Gaussians. For p(w) = N(0,σW), q(w) = N(μφ,σφ), p(aι)=
N(0,σa21) andp(a2) = N(0, σa22),
σ2	σ2σ4
qφ(aι)=	p(aι∣w)qφ(w)dw = N(μφ号,φ 4a1
σw	σw
22
+ ^⅛^2 )
σ2
w
qφι(W) = N (μφι ,σφ1) - qφ(wlaι) = N(
aισφ σW + μφσa2 σW	σφ σW σ222
(1)
σφσaι + σWσ22	, σaισφ + σWσa2
These are derived by applying Bayes rule to the Gaussian probability density functions.
B NOTE ON ELBOAUX ≥ ELBOINIT
We formally shoW that ELBOaux ≥ ELBOinit under the assumption that the conditional variational
posterior, qφ(w∣αι), is within the variational family of qφι for all values of aι.
For a given aι, let Φi0∣a1 be a set of variational parameters such that ∀w, q©5∣aι (w) = qφ(w∣aι).
Such φio∣aι must exist as a result of the initial assumption. The formula for Gaussian distributions
is given in Appendix A.
Set φι∣aι4 such that
Φ∙ι I	if E	h lop, γ)( Q ∣∖χ w)	— lop,	q@10 |a1(——i ≥ E	h lop, jɔ(2 11 χ W) — lop, -Iopt |a1-]
φ10la1	if Eqφl0∣a1	[l°g p(y∖x,w) log p(w∣al)	≥ Eqφlopt∣a1	[l°g P(y\X,W) lθg	p(w∣al)
φ1opt∣a1 otherwise
(2)
where Φιopt∣aι is the result of the stochastic optimizer that attempts to maximize
Eqφl∣ai h lθg P(y\X，W)- lθg ⅛⅛? i .
By construction, for all a1 ,
Eqφl∣aihlθg p(y∖x,w)- log qP(way i ≥
“ioiaihlog P(y∖χ,w)- log ¾⅛2 i =	⑶
Eqφ(w∣aι) hlogp(y∖χ, w) — log qφ(w∖a?i .
φ 1	p(W∖a1 )
Substituting this into ELBOaux gives
ELBOaux
Eqφ hEqφ1, h logP(y∖x, W) - log qΦ；1a1 I：) i φ L φ1laι L	p(w∖aι) J	-log 锣 i ≥ p(a1)
Eqφ hEqφ(w∣al) h logp(y∖x, W)- log qφ(Wwaa1)) i	-log qφ(a1) i =	(4) p(a1)
Eqφ log p(y∖x, W)	-log * i = p(W)
	ELBOinit
concluding the proof.
4We use the notation φι∣aι instead of φι to emphasize the dependence on the value of aι.
11