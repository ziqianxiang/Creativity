Under review as a conference paper at ICLR 2020
MMD GAN with Random-Forest Kernels
Conference Submissions
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a novel kind of kernel, random forest kernel, to en-
hance the empirical performance of MMD GAN. Different from common forests
with deterministic routings, a probabilistic routing variant is used in our innovated
random-forest kernel, which is possible to merge with the CNN frameworks. Our
proposed random-forest kernel has the following advantages: From the perspec-
tive of random forest, the output of GAN discriminator can be viewed as feature
inputs to the forest, where each tree gets access to merely a fraction of the features,
and thus the entire forest benefits from ensemble learning. In the aspect of kernel
method, random-forest kernel is proved to be characteristic, and therefore suitable
for the MMD structure. Besides, being an asymmetric kernel, our random-forest
kernel is much more flexible, in terms of capturing the differences between distri-
butions. Sharing the advantages of CNN, kernel method, and ensemble learning,
our random-forest kernel based MMD GAN obtains desirable empirical perfor-
mances on CIFAR-10, CelebA and LSUN bedroom data sets. Furthermore, for
the sake of completeness, we also put forward comprehensive theoretical analysis
to support our experimental results.
1	Introduction
Generative adversarial nets (GANs; Goodfellow et al., 2014) are well-known generative models,
which largely attribute to the sophisticated design of a generator and a discriminator which are
trained jointly in an adversarial fashion. Nowadays GANs are intensely used in a variety of practical
tasks, such as image-to-image translation (Tang et al., 2019; Mo et al., 2019); 3D reconstruction
(Gecer et al., 2019); video prediction (Kwon & Park, 2019); text-to-image generation (Zhu et al.,
2019); just to name a few.
However, it’s well-known that the training of GANs is a little tricky, see e.g. (Salimans et al., 2016).
One reason of instability of GAN training lies in the distance used in discriminator to measure the
divergence between the generated distribution and the target distribution. For instance, concerning
with the Jensen-Shannon divergence based GANs proposed in Goodfellow et al. (2014), Arjovsky &
Bottou (2017) points out that if the generated distribution and the target distribution are supported on
manifolds where the measure of intersection is zero, Jensen-Shannon divergence will be constant and
the KL divergences be infinite. Consequently, the generator fails to obtain enough useful gradient
to update, which undermines GAN training. Moreover, two non-overlapping distributions may be
judged to be quite different by the Jensen-Shannon divergence, even if they are nearby with high
probability.
As a result, to better measure the difference between two distributions, Integral Probability Metrics
(IPM) based GANs have been proposed. For instance, Arjovsky et al. (2017) utilizes Wasserstein
distance in GAN discriminator, while Li et al. (2017) adopts maximum mean discrepancy (MMD),
managing to project and discriminate data in reproducing kernel Hilbert space (RKHS). To mention,
the RKHS with characteristic kernels including Gaussian RBF kernel (Li et al., 2017) and rational
quadratic kernel (BinkoWski et al., 2018) has strong power in the discrimination of two distributions,
see e.g. (Sriperumbudur et al., 2010).
In this paper, inspired by non-linear discriminating power of decision forests, we propose anew type
of kernel named random-forest kernel to improve the performance of MMD GAN discriminator. In
order to fit with back-propagation training procedure, we borrow the decision forest model with
1
Under review as a conference paper at ICLR 2020
stochastic and differentiable decision trees from Kontschieder et al. (2015) in our random-forest
kernel. To be specific, each dimension of the GAN discriminator outputs is randomly connected
to one internal node of a soft decision forest, serving as the candidate to-be-split dimension. Then,
the tree is split with a soft decision function through a probabilistic routing. Other than the typical
decision forest used in classification tasks where the value of each leaf node is a label, the leaf value
of our random forest is the probability of a sample xi falling into a certain leaf node of the forest.
If the output of the discriminator is denoted as hθN (xi) and the probability output of the t-th tree is
denoted as μt(hθN (xi); Θf ), the random forest kernel Urf can be formulated as
1T
kRF(xi, Xj； Θf) = TE <μt(hθN (xi); Θf), μt(hθN (Xj); Θf)),
t=1
where T is the total number of trees in the forest, θN and θF denote the parameters of the GAN
discriminator and the random forest respectively.
Recall that random forest and deep neural networks are first combined in Kontschieder et al. (2015),
where differentiable decision tree model and deep convolutional networks are trained together in
an end-to-end manner to solve classification tasks. Then, Shen et al. (2017) extends the idea to
label distribution learning, and Shen et al. (2018) makes further extensions in regression regime.
Moreover, Zuo & Drummond (2017) , Zuo et al. (2018) and Avraham et al. (2019) also introduce
deep decision forests. Apart from the typical ensemble method that averages the results across trees,
they aggregate the results by multiplication. As for the combination of random forest and GAN, Zuo
et al. (2018) introduce forests structure in GAN discriminator, combining CNN network and forest
as a composited classifier, while Avraham et al. (2019) uses forest structure as one of non-linear
mapping functions in regularization part.
On the other hand, in the aspect of relationship between random forest and kernel method, Breiman
(2000) initiates the literature concerning the link. He shows the fact that a purely random tree
partition is equivalent to a kernel acting on the true margin, of which form can be viewed as the
probability of two samples falling into the same terminal node. Shen & Vogelstein (2018) proves
that random forest kernel is characteristic. Some more theoretical analysis can be found in Davies
& Ghahramani (2014), Arlot & Genuer (2014), Scornet (2016). However, despite their theoretical
breakthroughs, forest decision functions used in these forest kernels are non-differentiable hard
margins rather than differentiable soft ones, and thus cannot be directly used in back propagation
regime.
To the best of our knowledge, MMD GAN with our proposed random-forest kernel is the first to
combine random forest with deep neural network in the form of kernel MMD GAN. Through the-
oretical analysis and numerical experiments, we evaluate the effectiveness of MMD GAN with our
random-forest kernel. From the theoretical point of view, our random-forest kernel enjoys the prop-
erty of being characteristic, and the gradient estimators used in the training process of random-forest
kernel GAN are unbiased. In numerical experiments, we evaluate our random-forest kernel under
the setting of both the original MMD GAN (Li et al., 2017) and the one with repulsive loss (Wang
et al., 2019). Besides, we also compare our random-forest kernel with Gaussian RBF kernel (Li
et al., 2017), rational quadratic kernel (BinkoWski et al., 2018), and bounded RBF kernel (Wang
et al., 2019). As a result, MMD GAN with our random-forest kernel outperforms its counterparts
With respect to both accuracy and training stability.
This paper is organized as folloWs. First of all, We introduce some preliminaries of MMD GAN
in Section 2. Then We revieW the concept of deep random forest and shoW hoW it is embedded
Within a CNN in 3.1. After that, random-forest kernels and MMD GAN With random-forest kernels
are proposed in 3.2 and 3.3 respectively. Besides, the training techniques of MMD GAN With
random-forest kernel are demonstrated in Section 3.4 and the theoretical results are shoWn in Section
3.5. Eventually, Section 4 presents the experimental setups and results, including the comparison
betWeen our proposed random-forest kernel and other kernels. In addition, all detailed theoretical
proofs are included in the Appendices.
2	MMD GAN
GANs are recently introduced as a novel Way of training a generative model. To learn a distribution
PX, We build an adversarial model composed of tWo parts, the generator G and the discriminator D.
2
Under review as a conference paper at ICLR 2020
The generative model captures the data distribution PX, by building a mapping function G : Z → X
from a prior noise distribution PZ to data space. While the discriminative model D : X → R is
used to distinguish generated distribution PY from real data distribution PX .
Taking X, X0 〜PX and Y, Y0 〜PY := PG(Z) where Y := G(Z) and Y0 := G(Z0), the squared
MMD is expressed as
MMD2 [PX, PY] =EX,X0k(X,X0) -2EX,Yk(X,Y)+EY,Y0k(Y,Y0).	(1)
The loss of generator and discriminator in MMD GAN proposed in Li et al. (2017) is:
min LG = MMD2 [PX, PY],	min LD = -MMD2 [PX , PY].
Wang et al. (2019) proposed MMD GAN with repulsive loss, where the objective functions for G
and D are:
min LG = MMD2[PX,PY],	min LD = EX,X0 k(X, X0) -EY,Y0k(Y,Y0).
Given i.i.d. samples DX = {x1, . . . , xn} 〜 PX and DY = {G(z1), . . . , G(zm)} 〜 PY, we can
write an unbiased estimator of the squared MMD in terms of k as
2	1 nn
MMDuDX ,Dy ] = n(nη -1)	k(xi, xj)
1 mm	2 nm
+ m(m-i)XX k(yi,yj)-嬴 XX Mxi，%).
(2)
When k is a characteristic kernel, we have MMD2 [PX , PY] ≥ 0 with equality applies if and only
if PX = PY . The best-known characteristic kernels are gaussian RBF kernel and rational quadratic
kernel (Binkowski et al., 2018).
3	Random-Forest Kernel
In this section, we review a stochastic and differentiable variant of random forest and how it is
embedded within a deep convolutional neural network proposed in Kontschieder et al. (2015). Then
we propose random-forest kernel and we apply it in MMD GAN. We illustrate the advantages of our
random-forest kernel, show the training technique of MMD GAN with random-forest kernel, and
study its theoretical properties.
3.1	Deep Random Forest
Suppose that a random forest consists of T ∈ N random trees. For the t-th tree in the forest,
t ∈ {1, . . . , T}, we denote Nt := {dtj}|jN=t1| as the set of its internal nodes and if T trees have the
same structure, then we have |Nt | = |N |, see Figure 1. Furthermore, we denote Lt as the set of its
leaf nodes and θFt as the parameters of the t-th tree.
Here we introduce the routing function μ'(x; θ%) which indicates the probability of the sample x
falling into the `-th leaf node of the t-th tree. In order to provide an explicit form for the routing
function μ'(x; θ5), e.g. the thick black line in Figure 1, We introduce the following binary relations
that depend on the tree structure: ` . dtj is true, if ` belongs to the left subtree of node dtj , and
` & dtj is true, if ` belongs to the right subtree of node dtj . Moreover, let ptj (x; θFt ) be the decision
function of the j -th internal node in the t-th tree, that is the probability of the sample x falling into
the left child of node dj in the t-th tree. Then, μ' can be expressed by these relations as
μ'(XθF) = Y Pj(x;θF)1'.dj(I-Pj(x;θF))1'&dj,
dj ∈R'
where R' denotes the unique path from node 1 to node ' of the t-th tree.
3
Under review as a conference paper at ICLR 2020
Figure 1: Example of a random forest with T trees: blue nodes denote the internal nodes N := {d1 , ..., d7}
while purple nodes are the leaf nodes L := {'ι,…，'8 }. The black thick path illustrates the route
of a sample x falling into the `6 leaf node of the t-th tree.
Now, let us derive the explicit form of the decision function ptj (x; θFt ). Here, to utilize the power of
deep learning, we consider using the convolutional neural network to construct decision functions
of random forest.
To be specific, given the parameter θN trained from a CNN network, We denote h(∙; θN) as
the d0 -dimension output of a convolutional neural network, which is the unit of the last fully-
connected layer in the CNN, and hi(∙; Θn) is the i-th element of the CNN output. We denote
C : {1, . . . , T |N|} → {1, . . . , d0} as the connection function, which represents the connec-
tion between the internal node dtj and the former CNN output hi . Note that during the whole
training process, the form of the connection function C is not changed and every internal node
dj is randomly assigned to an element hC(T(t-1)+j∙)(∙; θ). If we choose the sigmoid function
σ(x) = (1+e-x)-1as the decision function, and let the parameters of the t-th tree be θFt := (wt, bt)
with wt = (w1t, . . . , w|tN|) and bt = (bt1, . . . , b|tN |), then the decision functions ptj delivering a
stochastic routing can be defined as
Pj (x； Θn,θF) = σ(wj ∙ hc(T(t-i)+j) (x; Θn) + bj).
For example, we have the probability p11(x; θN, θF1) = σ(w11hC(1) (x; θN) + b11) for the node in the
first layer, see Figure 2 for an explicit example of the random forest.
Figure 2: Example of a connection between CNN network d0-dimensional output h(∙; Θn ) and random forest
with T trees and depth 2. The internal node d31 in the left tree has the probability
p13(x; θN, θF1 ) = σ(w31h1(x; θN) + b13) and therefore the routing function of leaf node `14 is
μ'4(x; Θn,θp) =(1 - σ(w1 hd，(x; Θn) + b1 ))(1 - σ(w3hi(x; Θn) + b3)).
4
Under review as a conference paper at ICLR 2020
Every leaf node ' in each tree has a unique road R' from node 1 to node ' with length |R'| =
log2(|Lt |). Then, for the every leaf node ` of the t-th tree, we have
μ'(x)= Y σ(wjh0(τ(t-i)+j)(x； Θn) + bj)1j∈Lf(1 - σ(wjhc(τ(t-i)+j)(x; Θn) + bj))1-1j∈Lf
j∈R'
(3)
where Lf denotes the set of all left son nodes of its father node.
3.2	Random-Forest Kernel
Here, we propose the random-forest kernel as follows:
Definition 1 (Random-Forest Kernel) Let x, y be a pair of kernel input, let θFt = (wt, bt) denotes
the weights and bias of the t-th tree of the random forest, and θF := (θFt )tτ=1. The random-forest
kernel can be defined as
k RF (χ, y; OF) = T XX μ'(X； θF )μ'(y; θF)
t t=1 '∈Lt
1τ
=t X〈/(x; θF ),μt(y; θF))
t=1
=T D〃(T )(x； Θf ),μ(τ) (y; OF)E,
Where Lt denotes the set ofleafnodes in the t-th tree, μt = (μ')'∈Lt and μ(τ) = (μt)tτ=1.
3.3	MMD GAN with Random-Forest Kernels
We write
k(x,x0) := kRF hθN (x), hθN (x0); OF
and introduce the objective functions of MMD GAN with random-forest kernel by
min LG = MMD2 [PX, PY]
ψ
= EX,X0k(X, X0) - 2EX,Y k(X, Y) + EY,Y 0k(Y, Y 0),
min LD = -MMD2[PX,PY] +R
θN,θF
= -EX,X0k(X, X0) + 2EX,Y [k(X, Y)] -EY,Y0k(Y,Y0)+R,
where y = Gψ(z), z is noise vector, and R is the regularizer of random-forest kernel (the detail is
shown in Section 3.4). In addition, the objective functions of MMD GAN with repulsive loss are
min LG = MMD2 [PX , PY ]
ψ
= EX,X0k(X, X0) -2EX,Yk(X,Y)+EY,Y0k(Y,Y0),
min LD = -MMD2[PX,PY] +R
θN,θF
= EX,X0k(X, X0) -EY,Y0k(Y,Y0)+R.
Random-forest kernel MMD GAN enjoys the following advantages:
•	Our proposed random-forest kernel used in MMD GAN benefits from ensemble learning.
From the perspective of random forest, the output of MMD GAN discriminator h(∙; Θn)
can be viewed as feature inputs to the forest. To mention, each tree only gets access to
merely a fraction of the features by random connection functions, and thus the entire forest
benefits from ensemble learning.
5
Under review as a conference paper at ICLR 2020
•	Our random-forest kernel MMD GAN enjoys the advantages of three powerful discrimi-
native methods, which are CNN, kernel method, and ensemble learning. To be specific,
CNN is good at extracting useful features from images; Kernel method utilize RKHS for
discrimination; Ensemble learning utilizes the power of randomness and ensemble.
•	Our proposed random-forest kernel has some good theoretical properties. In one aspect,
random-forest kernel is proved to be characteristic in Shen & Vogelstein (2018). In another,
in Section 3.5, the unbiasedness of the gradients of MMD GAN with random-forest kernel
is proved.
3.4	Random-forest Kernel Training Technique
In Frosst & Hinton (2017), the authors mention that the tree may get stuck on plateaus if internal
nodes always assign the most of probability to one of its subtree. The gradients will vanish because
the gradients of the logistic-type decision function will be very closed to zero. In order to stabi-
lize the training of random-forest kernel and avoid the stuck on bad solutions, we add penalty that
encourage each internal node to split in a balanced style as Frosst & Hinton (2017) does, that is,
we penalize the cross entropy between the desired 0.5, 0.5 average probabilities of falling into two
subtrees and the actual average probability α, 1 - α.
The actual average probability of the i-th internal node αi is
=Px∈Ω P i(χ)Pi(χ)
αi=	Px∈ω Pi(χ),
where Pi (x) is the routing probability ofx from root node to internal node i, pi (x) is the probability
of X falling into the left subtree of the i-th internal node, and Ω is the collection of mini-batch
samples.
Then, the formulation of the regularizer is:
R(Ω) = 一λ	^X	0.5 log (a. + 0.5 log (1 — ai),
i∈Internal Nodes
where λ is exponentially decayed with the depth of d of the internal node by multiplying the coeffi-
cient 2-d, for the intuition that less balanced split in deeper internal node may increase the non-linear
discrimination power.
When training random-forest kernel, a mini-batch of real samples X and generated pictures Y are
both fed into the discriminator, and then k(X, X), k(X, Y ) and k(Y, Y ) are calculated, where k :=
kRF ◦ h(∙; Θn). Here, to notify, We find that the Ω in the regularizer formulation does matter in
forest-kernel setting. It's better to calculate a% and R(Ω) in the case of Ω = X, Ω = Y, Ω = X ∪ Y
respectively, and then sum up three parts of regularizer as final regularizer R.
Therefore, the formulation of regularizer R added in the training of random-forest kernel is
R = R(X) + 2 ∙ R(X ∪ Y) + R(Y)	(4)
3.5	Theoretical Results
In this subsection, We present our main theoretical results.
Theorem 2 (Unbiasedness) Let X be the true data on X with the distribution PX and Z be the
noise on Z with the distribution PZ satisfying EPX kX kα < ∞ and EPZ kZ kα < ∞ for some
α ≥ 1. Moreover, let Gψ : Z → X be a generator network, hθN : X → Rd0 be a discriminator
network, kRF be the random-forest kernel, and θD := (θN, θF) be the parameters of the GAN
discriminator Then, for μ-almost all Θd ∈ R1 θD | and ψ ∈ R1 ψl, there holds
EDX 〜Pm,Dz 〜PZ [dθD ,ψ MMDU(hθN (DX ), hθN (Gψ (DZ )))]
= ∂θD,ψMMD2(hθN(PX),hθN(Gψ(PZ))).
In other Words, during the training process of Random-Forest Kernel MMD GAN, the estimated
gradients of MMD With respect to the parameters ψ and ΘD are unbiased, that is, the expectation
and the differential are exchangeable.
6
Under review as a conference paper at ICLR 2020
4	Experiments
In this section, we evaluate our proposed random-forest kernel in the setting of MMD GAN in (Li
et al., 2017) and the MMD GAN with repulsive loss (Wang et al., 2019). To illustrate the efficacy
of our random-forest kernel, we compare our random-forest kernel with Gaussian kernel (Li et al.,
2017), rational quadratic kernel (BinkoWski et al., 2018) in the setting of the original MMD GAN
loss, and compare our random-forest kernel with bounded Gaussian kernel (Wang et al., 2019) in
the setting of MMD GAN With repulsive loss.
4.1	Experimental setup
4.1.1	Datasets
The experiments are evaluated on three benchmark datasets:
(1)	the Cifar10 dataset of 32 × 32 pictures (Krizhevsky et al., 2009);
(2)	the LSUN dataset of bedroom pictures resized to 64 × 64 (Yu et al., 2015);
(3)	the CelebA dataset of celebrity faces pictures randomly cropped and resized to 160 × 160
(Liu et al., 2015). The images are scaled to range [0, 1].
4.1.2	Kernels and Losses
Under the setting of MMD GAN loss proposed in Li et al. (2017), We compare our proposed random-
forest kernel With the Gaussian RBF kernel (Li et al., 2017) With mixture of kernel scales σ and the
Rational quadratic kernel (BinkoWski et al., 2018) with mixture of kernel scales a. Moreover, under
another setting of MMD GAN With repulsive loss proposed in Wang et al. (2019), We compare our
random-forest kernel With the bounded RBF kernel (Wang et al., 2019).
(a) Gaussian RBF kernel With	(b) Rational quadratic kernel With (c) Random-forest kernel With
mixture of kernel scales σ	mixture of kernel scales α	T = 10 and dep = 3
Figure 3: Visualization of three kernels by draWing filled contours: (a) & (b) - A direct visualization of
2-dimensional kernels With reference to (0, 0); (c) - A 2-dimensional visualization of the multi-
dimensional random-forest kernel With the help of t-SNE (Maaten & Hinton, 2008). The details
of visualization are shoWn in Appendix A.2.
As is shoWn in Figure 3, the shapes of the Gaussian RBF kernel and the rational quadratic kernel are
both symmetric. HoWever, the local structure of random-forest kernel (W.r.t reference points except
70-dimensional zero vector) is asymmetric and very complex. The asymmetry and complexity of
random-forest kernel may be helpful to discriminate tWo distributions in MMD GAN training.
4.1.3	Network Architecture
For dataset Cifar10 and dataset LSUN bedroom, DCGAN (Radford et al., 2016) architecture With
hyper parameters from Miyato et al. (2018) is used for both generator and discriminator; and for
dataset CelebA, We use a 5-layer DCGAN discriminator and a 10-layer ResNet generator. Further
details of the netWork architecture are given in Appendix A.3. We mention that in all experiments,
batch normalization (Ioffe & Szegedy, 2015) is used in the generator and spectral normalization
(Miyato et al., 2018) is used in the discriminator. The hyper-parameter details of kernels used in
7
Under review as a conference paper at ICLR 2020
Table 1: Score evaluation results for three datasets: Inception Score (IS), Fr6chet Inception Distance (FID) and
Kernel Inception Distance (KID). The best score within two setups are marked in bold
Kernel	Loss	Dimension of Output Layer	CIFAR-10			CelebA		LSUN	
			IS	FID	KID ×103	FID	KID ×103	FID	KID ×103
Real data			11.28	0.217	0.00	0.589	0.00	0.670	0.00
mix-rbf	mmd	16	6.91	28.99	20.92	22.08	14.07	68.36	41.67
mix-rq	mmd	16	7.12	28.23	20.13	34.02	16.96	82.08	56.41
mix-rbf	mmd	70	6.96	27.56	19.51	21.64	14.39	65.48	40.34
mix-rq	mmd	70	7.19	27.68	18.88	29.78	11.91	78.89	48.82
forest	mmd	70	7.16	24.74	17.29	25.07	17.63	24.99	24.35
rbf-b	mmd-rep	16	7.37	23.54	17.27	27.81	15.53	17.99	16.04
rbf-b	mmd-rep	70	7.60	24.80	16.38	30.82	18.36	18.28	16.57
forest	mmd-rep	70	7.42	22.32	15.46	21.11	14.29	17.60	16.94
the discriminator are shown in Appendix A.1. For the sake of comparison with forest kernel, the
dimension of discriminator output layer s is set to be 70 for random-forest kernel and to be 16 for
other kernels following the previous setting of Binkowski et al. (2018); Wang et al. (2019).
4.1.4	Training Hyper-parameters
We set the initial learning rate 10-4 and decrease the learning rate by coefficient 0.8 in iteration
30000, 60000, 90000, and 120000. Adam optimizer (Kingma & Ba, 2015) is used with momentum
parameters β1 = 0.5 and β2 = 0.999. The batch size of each model is 64. All models were trained
for 150000 iterations on CIFAR-10, CelebA, and LSUN bedroom datasets, with five discriminator
updates per generator update.
4.1.5	Evaluation metrics
The following three metrics are used for quantitative evaluation: Inception score (IS) (Salimans
et al., 2016), FreChet inception distance (FID) (HeUSel et al., 2017), and Kernel inception distance
(KID) (BinkOWSki et al., 2018). In general, higher IS and Lower FID, KID means better quality.
However, outside the dataset Imagenet, the metric IS has some problem, especially for datasets
celebA and LSUN bedroom. Therefore, for inception score, we only report the inception score of
CIFAR-10. Quantitative scores are calculated based on 50000 generator samples and 50000 real
samples.
4.2	Experimental Results
We compare our proposed random-forest kernel with mix-rbf kernel and mix-rq kernel in the setting
of the MMD GAN loss, and compare our proposed random-forest kernel with rbf-b kernel in the
setting with MMD GAN repulsive loss. The Inception Score, the Frechet Inception Distance and
the Kernel Inception Distance of applying different kernels and different loss functions on three
benchmark datasets are shown in table 1.
We find that, in the perspective of the original MMD GAN loss, our newly proposed random-forest
kernel shows better performance than the mix-rbf kernel and the mix-rq kernel in CIFAR-10 dataset
and LSUN bedroom dataset; and in the perspective of the repulsive loss, the performance of our
newly proposed random-forest kernel is comparable or better than the rbf-b kernel. The efficacy of
our newly proposed random-forest kernel is shown under the setting of both MMD GAN loss and
MMD GAN repulsive loss.
Some randomly generated pictures of model learned with various kernels and two different loss
functions are visualized in Appendix D.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In International Conference on Learning Representations, 2017.
Martin Arjovsky, Soumith Chintala, andL6on Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223, 2017.
Sylvain Arlot and Robin Genuer. Analysis of purely random forests bias. arXiv preprint
arXiv:1407.3939, 2014.
Gil Avraham, Yan Zuo, and Tom Drummond. Parallel optimal transport gan. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 4411-4420, 2019.
Mikolaj BinkoWski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD
GANs. In International Conference on Learning Representations, 2018.
Leo Breiman. Some infinity theory for predictor ensembles. Technical report, Technical Report 579,
Statistics Dept. UCB, 2000.
Djork-Ame Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (ELUs). In International Conference on Learning Represen-
tations, pp. 1-14, 2016.
Alex Davies and Zoubin Ghahramani. The random forest kernel and other kernels for big data from
random partitions. arXiv preprint arXiv:1402.4293, 2014.
Gerald B. Folland. Real analysis. John Wiley & Sons, Inc., New York, second edition, 1999.
Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. arXiv
preprint arXiv:1711.09784, 2017.
Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. Ganfit: Generative adversar-
ial network fitting for high fidelity 3d face reconstruction. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1155-1164, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13:723-773, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448-456,
2015.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Interna-
tional Conference on Learning Representations, 2015.
Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo. Deep neural
decision forests. In Proceedings of the IEEE international conference on computer vision, pp.
1467-1475, 2015.
9
Under review as a conference paper at ICLR 2020
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yong-Hoon Kwon and Min-Gyu Park. Predicting future frames using retrospective cycle gan. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1811-
1820, 2019.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabgs P6czos. Mmd gan:
Towards deeper understanding of moment matching network. In Advances in neural information
processing systems, pp. 2203-2213, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Boris Mityagin. The zero set of a real analytic function. arXiv preprint arXiv:1512.07276, 2015.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.
Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Instance-aware image-to-image translation. In Inter-
national Conference on Learning Representations, 2019.
George Piranian. The set of nondifferentiability of a continuous function. The American Mathemat-
ical Monthly, 73(4):57-61, 1966.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Repre-
sentations, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
Erwan Scornet. Random forests and kernel methods. IEEE Transactions on Information Theory, 62
(3):1485-1500, 2016.
Cencheng Shen and Joshua T Vogelstein. Decision forests induce characteristic kernels. arXiv
preprint arXiv:1812.00029, 2018.
Wei Shen, Kai Zhao, Yilu Guo, and Alan L Yuille. Label distribution learning forests. In Advances
in Neural Information Processing Systems, pp. 834-843, 2017.
Wei Shen, Yilu Guo, Yan Wang, Kai Zhao, Bo Wang, and Alan L Yuille. Deep regression forests
for age estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2304-2313, 2018.
Bharath K Sriperumbudur, Arthur Gretton, Kenji FUkUmizu, Bernhard Scholkopf, and Gert RG
Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine
Learning Research, 11(Apr):1517-1561, 2010.
Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J Corso, and Yan Yan. Multi-channel attention
selection gan with cascaded semantic guidance for cross-view image translation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2417-2426, 2019.
Wei Wang, Yuan Sun, and Saman Halgamuge. Improving MMD-GAN training with repulsive loss
function. In International Conference on Learning Representations, 2019.
10
Under review as a conference paper at ICLR 2020
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adver-
sarial networks for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 5802-5810, 2019.
Yan Zuo and Tom Drummond. Fast residual forests: Rapid ensemble learning for semantic segmen-
tation. In Conference on Robot Learning, pp. 27-36, 2017.
Yan Zuo, Gil Avraham, and Tom Drummond. Generative adversarial forests for better conditioned
adversarial learning. arXiv preprint arXiv:1805.05185, 2018.
11
Under review as a conference paper at ICLR 2020
A Supplementary Methodology
A.1 Kernels and Losses
The formulation of our proposed random-forest kernel is
kRF (χ,y; Θf ) = T <μ(T )(χ; Θf ),μ(T )(y; Θf )).
In experiments, we take the number of trees in the forest T = 10 and the depth of trees dep = 3.
Thus, the total number of internal nodes is 70. To notify, in general, the parameters θF = (θFt )tT=1
are trainable, where θFt = (wt, bt), wt = (w1t, . . . , w|tN|), bt = (bt1, . . . , b|tN | ) and N is the set of
every internal nodes of a tree. However, for the sake of experimental simplification, we fix each
wjt = 1, btj = 0 untrainable.
In the setting of MMD GAN loss proposed in Li et al. (2017), we compare our random-forest kernel
with the two following kernels: Gaussian kernel (Li et al., 2017) with mixture of kernel scales σ,
that is,
krmbf(x, y) = exp
σ∈Σ
where Σ = {2,5,10,20,40,80}, and rational quadratic kernel (BinkoWski et al., 2018) with mixture
of kernel scale α, that is,
(kx - yk2)
磴(χ,y) = X(1+⅛yf Γ
α∈A
where A = {0.2, 0.5, 1, 2, 5}.
In the setting of the MMD GAN with repulsive loss proposed in Wang et al. (2019), we compare our
forest kernel with bounded RBF kernel (Wang et al., 2019), that is,
max{kx - yk2 3 4, bl}
exp I--------2σ2-------- if x,y ∈ {Pγ}
min{kx - yk2,bu}
eχpI---------2σ---------)	if x,y ∈ {PX},
with σ = 1, bl = 0.25 and bu = 4.
A.2 Visualization details of kernels
In figure 3, we compare the contours of three different kernels (the detail of kernels is shown in A.1).
We directly plot the filled contours of 2-dimensional Gaussian kernel and rational quadratic kernel
with reference to (0, 0); As for random-forest kernel with T = 10 and dep = 3, where the input
dimension is 70, the steps of a 2-dimensional visualization are as follows:
1) We randomly generate 25000 points from the uniform distribution U [-1, 1]70 and set
(0.5, . . . , 0.5) ∈ R70 as the reference point. To notify, if the reference point is 70-
dimensional zero vector, the values of random-forest kernel will be constant;
2) We calculate the 25000 output values of random-forest kernel;
3) We transform 70-dimensional 25000 randomly generated points and reference point to-
gether by t-SNE to 2-dimensional points;
4) We show the filled contour of the neighborhood of transformed reference point. We try to
visualize the local structure of random-forest kernel.
A.3 Network Architecture
In the experiments of the CIFAR-10 and LSUN bedroom datasets, we use the DCGAN architecture
following Miyato et al. (2018), and for the experiments of the CelebA dataset, we use a 5-layer
DCGAN discriminator and a 10-layer ResNet generator as in BinkoWski et al. (2018). The first few
layers of the ResNet generator consist of a linear layer and 4 residual blocks as in Gulrajani et al.
(2017). The network architecture details are shown in Table 2 and 3.
12
Under review as a conference paper at ICLR 2020
Table 2: Network architecture used in the image generation on CIFAR-10 dataset and LSUN bedroom dataset.
In terms of the shape parameter h and w in the table, we take h = w = 4 for CIFAR-10 and
h = w = 8 for LSUN bedroom. As for the output dimension of discriminator s, we take s = 70 for
random-forest kernel and S = 16, 70 for other kernels. To notify, 16 is the setting in BinkoWski et al.
(2018); Wang et al. (2019) and 70 is set for the comparison of random-forest kernel.
(a) Generator
noise input Z ∈ R128 〜U[-1,1]
128 → h X W X 512, dense, BN, ReLU
4 x 4, stride 2 deconv, 256, BN, ReLU
4 x 4, stride 2 deconv, 128, BN, ReLU
4 x 4, stride 2 deconv, 64, BN, ReLU
3 x 3, stride 1 deconv, 3, Sigmoid
(b) Discriminator
RGB picture x ∈ [0, 1]H×W ×3
3 x 3, stride 1 conv, 64, LReLU
4 x 4, stride 2 conv, 128, LReLU
3 X 3, stride 1 conv, 128 LReLU
4 X 4, stride 2 conv, 256, LReLU
3 X 3, stride 1 conv, 256 LReLU
4 x 4, stride 2 conv, 512, LReLU
3 x 3, stride 1 conv, 512 LReLU
dense → s
Table 3: NetWork architecture used in the image generation on CelebA dataset. For the output dimension of
discriminator s, We take s = 70 for random-forest kernel and s = 16, 70 for other kernels.
(a) Generator	(b) Discriminator
noise input Z ∈ R128 〜U [-1,1]	RGB picture X ∈ [—1, 1]h×w×3
128 → 5 x 5 x 1024, dense	5 x 5, stride 2 conv, 64, LReLU
ResBlock up, 512	5 x 5, stride 2 conv, 128, LReLU
ResBlock up, 256	5 x 5, stride 2 conv, 256, LReLU
ResBlock up, 128	5 x 5, stride 2 conv, 512, LReLU
ResBlock up, 64, BN	5 x 5, stride 2 conv, 1024, LReLU
5 x 5, stride 2 deconv, 3, Sigmoid	dense → s
13
Under review as a conference paper at ICLR 2020
B	Theoretical Analysis
In Section B, we will show the main propositions used to prove the Theorem 2. To be specific, in
Section B.1, we represent neural networks as computation graphs. In Section B.2, we consider a
general class of piecewise analytic functions as the non-linear part of neural networks. In Section
B.3, we prove the Lipschitz property of the whole discriminators. In Section B.4, we discover that
for PX almost surely, the network is not differential for its parameters θN and ψ. Fortunately, we
prove that the measure of bad parameters set is zero.
In Section C, we will show the explicit proof of main propositions in Section B and Theorem 2.
B.1	Neural Networks (NN)
B.1.1	Convolutional Neural Networks (CNN)
Historical attempts to scale up GANs using CNNs to model images have been unsuccessful. The
original CNN architecture is made up of convolution, non-linear and pooling. Now for our model,
we adopt the deconvolution (Zeiler & Fergus, 2014) net to generate the new data with spatial upsam-
pling. Moreover, batch normalization (Ioffe & Szegedy, 2015) is a regular method which stabilizes
learning by normalizing the input to each unit to have zero mean and unit variance. Furthermore,
relu functions are used both in generator and discriminator networks as non-linear part. Here we
avoid spatial pooling such as max-pooling and global average pooling.
B.1.2	Computational Graph Representation of Neural Networks
Throughout this paper, we always denote by
h(θ) := h(x; θ) := {h1(x; θ), . . . , hd0 (x; θ)},	(5)
the output of a fully connected layer, where d0 represents the number of neurons in the output.
The general feed-forward networks including CNN and FC can be formulated as a directed acyclic
computation graph G consisting of L + 1 layers, with a root node i = 0 and a leaf node i = L. For
a fixed node i, we use the following notations:
π(i): the set of parent nodes of i;
j < i: j is a parent node of i.
Each node i > 0 corresponds to a function fi that receives a Rdπ(i) -valued input vector, which
is the concatenation of the outputs of each layer in π(i), and outputs a Rdi -valued vector, where
dπ(i) = Pj∈π(i) dj and d0 = d > 0. According to the construction of the graph G, the feed-forward
network that factorizes with functions fi recursively can therefore be defined by h0 = X, and for
all 0 < i ≤ L,
hi := fi(hπ(i)),
where hπ(i) is the concatenation of the vectors hj, j ∈ π(i). Here, the functions fi can be of the
following different types:
(i)	Linear/Affine: If the weights Wi are mi-dimensional vectors, and the function gi : Rmi →
Rdi ×(dπ(i)+1) is a linear operator on the weights Wi, e.g. convolutions, then the functions fi are of
the linear form
fi(Y)=gi(Wi)(Y 1)T =:gi(Wi)Ye.
(ii)	Non-linear: Such functions fi including ReLU, max-pooling and ELU, have no learnable
weights, can potentially be non-differentiable, and only satisfy some weak regularization conditions,
see Definition 3 and the related Examples 2, 3, and 4.
In the following, we denote by I the set of nodes i such that fi is non-linear, and its complement by
Ic := {1, . . . , L} \ I, that is, the set of all of all linear modules. We write θN as the concatenation
of parameters
Θn ：= (Wi)i∈ic ∈ RlθN1,
14
Under review as a conference paper at ICLR 2020
where ∣Θn| denotes the total number of parameters of Θn. Moreover, the feature vector of the
network corresponds to the output neurons GL of the last layer L and will be denoted by
hθ ：= hLN ∈ R- 1,
where the subscript θ stands for the parameters of the network. IfX is random, we will use hθN (X)
to denote explicit dependence on X, and otherwise when X is fixed, it will be omitted.
B.2 Piecewise Analytic Nonlinear Layers
Throughout this paper, in the construction of neural networks, we only consider activation functions
that are piecewise analytic which can be defined as follows:
Definition 3 (Piecewise Analytic Functions) Let {fi}i∈I be non-linear layers in neural networks,
then fi is said to be an piecewise analytic function if there exists a partition of Rdπ(i) with Ji pieces,
that is, UJ=I DDj = Rdπ(i) and DjT Di = 0 for j = j0, such that there exist
(i)	Sij real analytic functions gij,s : Rdπ(i) → R such that
Di = {Y ∈ Rd∏(i) I gi,s(Y) > 0forall S ∈{1,..., Sj}};
(ii)	Ji real analytic functions (fij )j ∈{1,...,Ji} such that
fi (Y )= fi (Y)	forall Y ∈Dj.
The sets Dii are called analytic domains and the functions fii are called analytic value.
The following examples show that in practice, the vast majority of deep networks satisfy the condi-
tions in the above definition, and therefore are piecewise analytic.
Example 1	(Sigmoid) Let fi outputs the sigmoid activation function on the inputs. Then we need
no partition, and hence there exist
(i)	Sii = 1 real analytic functions gi1,1 (Y) = 1 such that
Dii = Rdπ(i)
corresponding to the whole space;
(ii)	Ji = 1 real analytic function fi1 := fi.
Here, we mention that the case in Example 1 corresponds to most of the differentiable activation
functions used in deep learning, more examples include the softmax, hyperbolic tangent, and batch
normalization functions. On the other hand, in the case that the functions are not differentiable, as
is shown in the following examples, many commonly used activation functions including the ReLU
activation function are at least piecewise analytic.
Example 2	(ReLU) Let fi outputs the ReLU activation function on two inputs. Then we have a
partition of Ji = 4 pieces, with each Dii, j ∈ {1, 2, 3, 4}, corresponding to a quadrant of the real
plane, such that there exist
(i)	Sii = 2 real analytic functions gii,1 (Y) := ±Y1 and gii,2(Y) := ±Y2 such that
Dii = {Y | gii,1 (Y) > 0 and gii,2(Y) > 0};
(ii)	Ji	= 4 real analytic functions	fi1	:=	(Y1 , Y2),	fi2	:=	(Y1 , 0),	fi3	:= (0, 0),	and	fi4	:=
(0, Y2 ) such that
fi(Y)=fii(Y)	forallY∈Dii.
15
Under review as a conference paper at ICLR 2020
Besides the ReLU activation function, other activation functions, such as the Max-Pooling and the
ELU (Clevert et al., 2016) also satisfy the form of Definition 3 are therefore piecewise analytic.
Example 3	(Max-Pooling) Let fi outputs max-pooling on two inputs. Then we have a partition of
Ji = 2 pieces with each domain Dij, j = 1, 2, corresponding to a half plane, such that there exist
(i)	Si1 = Si2 = 1 real analytic function gi1,1 (Y ) := Y1 - Y2 and gi2,1(Y ) = Y2 - Y1 such that
Di1 ={Y | gi1,1(Y) >0} and	Di2={Y | gi2,1(Y) >0};
(ii)	Ji = 2 real analytic functions fi1 := Y1 and fi2 := Y2 such that
fi(Y) =fij(Y)	for all Y ∈Dij.
Example 4	(ELU) Let fi outputs the ELU activation function on two inputs. Then we have a parti-
tion of Ji = 4 pieces, with each Dij, j ∈ {1, 2, 3, 4}, corresponding to a quadrant of the real plane,
such that there exist
(i)	Sij = 2 real analytic functions gij,1(Y ) := ±Y1 and gij,2(Y ) := ±Y2 such that
Dij = {Y | gij,1(Y) > 0 and gij,2(Y) > 0};
(ii)	Ji = 4 real analytic functions fi1 := (Y1, Y2), fi2 := (Y1, 0), fi3 := (0, 0), and fi4 :=
(0, Y2) such that
fi(Y) =fij(Y)	for all Y ∈Dij.
B.3	Bounding the Network Growth
In this section, we investigate the Lipschitz property of the proposed discriminative networks, which
is formulated as follows:
Denote u := (x, y), then the linear kernel kL(x, y) can be written as a univariate function KL(u),
that is,
K(u) := KL(u) := kL(x, y).	(6)
Proposition 4 Let Θd be the parameters of discriminators and Br (Θd ) ⊂ RlθDl be the ball with
center θD and radius r ∈ (0, ∞). Then, for all θD0 ∈ Br(θD) and all x ∈ Rd, there exists a regular
function c(x) with EPX [c(x)] < ∞ such that
∣K(μθT)(χ)) - K(μθT)(χ)) ∣ ≤ c(χ)kθD - θDk∙
B.4	Effectless of Critical Parameters
In the analysis of section B.3, we intentionally ignore the situation when samples fall into the "bad
sets", where the network is not differentiable for data sets x with nonzero measure. And in proposi-
tion 5, we show that the measure of the so called "bad sets" is zero.
To better illustrate this proposition, we first denote some notations as follows:
For a fixed Θn,o ∈ RlθN|, We denote
N (θN,0) = {x ∈ Rd | θN 7→ hθN (x) is not differentiable at θN,0}	(7)
as the set of input vectors x ∈ Rd such that hθN (x) is not differentiable With respect to θN at the
point θN,0. Then We call
ΘPX = {θN | PX(N(θN)) > 0}	(8)
the set of critical parameters, Where the netWork is not differentiable for data sets x With nonzero
measure.
Proposition 5 Let the set ΘPX be as in equation 8. Then, for any distribution PX, we have
μ(θPx) = 0.
16
Under review as a conference paper at ICLR 2020
C Proofs
C.1 Proofs of Section B.3
To prove Proposition 4, we first introduce Lemma 6 and Lemma 7.
Lemma 6 describes the growth and Lipschitz properties of general networks including convolutional
neural networks and fully connected neural networks introduced in Section B.1.
Lemma 6 Let hθN be the neural networks defined as in Section B.1. Then there exist continuous
functions a, b : RlθN | → R and α,β : R2lθN | → R such thatfor all X ∈ Rd and all Θn,θN ∈ R1 θN |,
there hold
khθN (x)k ≤ b(θN) + a(θN)kxk,	(9)
khθN(X)-hθN(x)k ≤ kθN — θNk(β(θNWN) + α(θNWN)kx∣∣).	(10)
Proof [of Lemma 6] We proceed the proof by induction on the nodes of the network.
Obviously, for i = 0, the inequalities hold with b0 = 0, a0 = 1, β0 = 0 and α0 = 0. Then, for the
induction step, let Us fix an index i. Assume that for all X ∈ Rd and all Θn, θ0N ∈ RlθN|, there hold
hθπN(i)(X) ≤ bπ(i)(θN) + aπ(i)(θN)kXk,
UhnNo(X)-h∏N⅜)U ≤kθN -θNk(β∏(i)(θN,θN) + α∏(i)(θN,θN)kxk),
where aπ(i)(θN), bπ(i)(θN), απ(i)(θN, θN0 ) and βπ(i)(θN, θN0 ) are some continuous functions.
(i) If i ∈ I, that is, if i is a linear layer, then the following growth condition
MΘn k ≤ Ugi(W i)h∏N)∖∖ ≤ kgikkWik(Ilhn NoIl+1) ≤ bi(θN) + ai (ON )k-k
holds with ai(θN) = kgikkW ikaπ(i)(θN) and bi(θN) = kgikkW ikbπ(i)(θN). Moreover, concern-
ing with the Lipschitz property, we have
∖∖hiθN(x) -hθN(X)W = Wgi(Wi)h∏N°(χ) - gi(Wi,)h∏N°(x)∖∖
≤	∖gi(Wi)(hn(i)(X) - h∏N)(X))∖∖ + ∖∖(gi(Wi - Wi/))h¾i)(X)∖∖
≤	kgik(kWik∖∖h∏(i)(X) - h∏Ni)(X)∖∖ + kWi - W/(∖∖h∏(i)(X)∖∖ + 1))
≤	kθN -θNk(βi(θN,θN) + αi(θN,θN)kXk),
where we used the notations
αi(θN,θN ):= kgik((kWik + kW")α∏(i)(θN,θN ) + (a∏(i)(θN) + a∏(i)(θN)))
βi(θN, θN):= kgik ((kWik + kW")βn(i)(θN, θN) + (bn(i)(θN) + bn(i)(θN)) + 1).
(ii) If i ∈ Ic, that is, i is not a linear layer, here we only consider the sigmoid and ReLU functions.
We first show that both of them are Lipschitz continuous. Concerning the sigmoid function,
σ(X)
1
1 + e-x
we obviously have
σ0(X)=------------
2 + e-x + ex
1
(e-x/2 + ex/2 )2
1
≤ 4,
for all X ∈ R. Consequently, the sigmoid function is Lipschitz continuous with Lipschitz constant
∣σ∣ι := 1/4. Next, for the ReLU function,
σ(X) = max{0, X},
we have for all X ∈ R,
σ0 (X) = max{0, 1} ≤ 1.
17
Under review as a conference paper at ICLR 2020
Therefore, the ReLU function is LiPschitz continuous with LiPschitz constant ∣σ∣ι := 1. Thus,
non-linear layer fi is Lipschitz continuous with Lipschitz constant M := |fi|1.
Consequently, by recursion, we obtain continuous functions αi = M απ(i), βi = M βπ(i),
ai = Ma∏(i),and b = Mb∏(i).	■
Next, we investigate the growth conditions and LiPschitz ProPerty of the random forest. For the ease
of notations, We write the function μ(τ) as
μ(τ )(x; Θn ,Θf ) =: μ(τ )(h(x; Θn ); Θf ) =: μ(τ )(h).
Moreover, it is easily seen that T ∙ |N| equals the number of internal nodes in the random forest.
Lemma 7 Let h(x; θN) be the input vector of random trees and θF := (w, b) where w and b denote
the weights and bias of the random forests, respectively. Then, for all h ∈ Rd0 andθF, θF0 ∈ R2T|N|,
there exist continuous functions c1, c2, and constants c3, c4, c5 such that
∣∣μ(τ)(h(x; Θn); Θf)∣∣	≤	cι(b) + c2(w)kh(x; Θn)k;	(11)
∣∣μ(τ)(h(x; Θn); Θf)	- μ(τ)(h(x; Θn); θF)∣∣	≤	c3∣∣w - w0∣∣∣∣hk + c4∣∣b - b0k;	(12)
∣∣μ(T)(h(x; Θn); Θf)	- μ(τ)(h(x; θN); Θf)∣∣	≤	C5∣w∣∣h(x; Θn) - h(x; θN)∣.	(13)
Proof [ofLemma 7] For t = 1,...,T, denote μ' as the '-th leaf node of the t-th tree. By convenient
abuse of notation, we write the function μ' as
μ'(x; Θn,wt,bt) =: μ'(h(x; Θn); wt, bt) =: μg(h).
First of all, let us verify the growth condition equation 11. From the definition of the function T we
immediately know that for any node i in the random forest, there exists an element j = T(i) such
that
pi(h) = 1 + eχp-(wihj+bi).
Obviously, we have
max{pi,1 -pi}≤ 1+eχp-∣wihj +bi∣.
For any leaf node ` ∈ Lt, that is, for any node in the t-th tree, in the following, for the ease of
notation, we always set μ' = μ', Wi = wt, hj = hj, b = bt, if not otherwise mentioned. Then μ'
2+χ |wi hj-+bi|
i∈R'
2 + X lwihj + bi| ≤ 2 + X(Iwihj | + |bi|).
i=1	i=1
in equation 3 can be uPPer bounded by
1
μ' ≤ Q (1 + e-∣Wihj+bi∣) ≤
i∈R'
≤
Taking the summation on both sides of the above inequality with resPect to all of the nodes in the
random forest, that is, w.r.t. ` ∈ L, we obtain
∣∣μ(T)∣∣ ≤ 彳 + INIXIbtI + INIXIwtIIhT(i)∣
i,t	i,t
≤ E + INIILIkbk + INI(X IwtI2)1/2(X IhT(i)I2)
≤ 与 + INIILIkbk + INIkwkjd⅛k
: c1 (b) + c2(w)khk,
18
Under review as a conference paper at ICLR 2020
where the second inequality follows from the Cauchy-Schwartz inequality and the third inequality is
due to the fact that the number that the nodes pi in the random forest assigned to the corresponding
node hj equals bT |N |/d0c or bT|N |/d0c + 1, which are less than |L|/d0.
Now, we show the Lipschitz properties equation 12 and equation 13 of the random forest. From
the equivalent form equation 3 concerning the value of the leaf node, We easily see that μg can be
written as a product of probability functions pi or 1 - pi . Therefore, without loss of generality, we
can assume μ' are of the product form
μ' : = pR'(1) ∙ ... ∙ pR'(H).
(14)
For a fixed t = 1, . . . , T , recall that Tt denotes the connection function of the t-th random tree.
Then, the Lipschitz property of the sigmoid function and the continuously differentiability of the
linear transform yield
|pi -p0i|	≤	|pi (h; θF) -pi(h; θF0 )| ≤	|wihi +	bi	-	wi0hi	-	b0i|	≤	|wi	- wi0||hi|	+	|bi	- b0i|.	(15)
Then, equation 14 together With equation 15 implies
lμ'(h; θF) - μ'(h; θF)| = lpR'(1) X …X pR'(H) - pR'(l) X …X PR'(H)1
≤ |pR'(1) - PR'(1)| X pR'(2) X …X pR'(H)
+	+ pR'(1) X …X PR'(H-1) X lpR'(H) - PR'(H) |
H	|N|
≤	|PR'(i) - P0R'(i) | ≤	|Pk - P0k|
i=1	k=1
|N|
≤ X |wkt - wk0 ||hi| + |btk - b0k|
k=1
≤ wt
+ |N|kbt-bt0k,
Where the last inequality folloWs from the Cauchy-SchWartz inequality. Consequently, concerning
the random forest, We obtain
kμ(T)(h； OF) — μ(T)(h； θF)k ≤ |N| X kwt - wt0k (X hTt(k)) 1/2 + NI2 X kbt - bt0k
≤ INIkw - w0k (X hTt(k)) 1/2 + ∣N∣2√Tkb - b0k
L
≤∣N∣kw - w0kLkhk + ∣N∣∣L∣kb-b0k
=: c3kw - w0kkhk +c4kb - b0k,
Where the second inequality is again due to then Cauchy-SchWartz inequality.
Analogously, for a fixed t = 1, . . . ,T and any i, We have
IPi(h； θF) - Pi(h0； θF)I ≤ Iwih0j + bi - wih0j - biI ≤ IwiIIhj - h0j I,
19
Under review as a conference paper at ICLR 2020
and consequently we obtain
μ'(h; OF) - μ'(h0; θF) = lpR'(1) X …X pR'(H) - pR'(1) X …X PR'(H) |
≤ |pR'(1) - pR'(1)| X pR'(2) X …X pR'(H)
+	+ pR'(1) X …X pRR'(H-1) X |pR'(H) - pR'(H) |
H	|N|
≤ X |PR'(j) - pR'(j) | ≤ X |pt - Pi |
j=1	i=1
|N|
≤ X |wit||hTt(i) - h0Tt(i) |
i=1
|N|	1/2
≤ kwtk X |hTt(i) - h0Tt(i)|2
and for the random forest, there holds
L
≤∣N∣kwkdo kh-h0k
=: c5kwkkh- h0k,
which completes the proof.
The next proposition presents the growth condition and the Lipschitz property of the composition of
the neural network and the random forest.
Lemma 8 Let Br (Θd) ⊂ RlθD| be the ball with center Θd and radius r ∈ (0, ∞). Then, for all
θD0 ∈ Br (θD), all x ∈ Rd, there exist continuous functions c6, c7, c8, and c9 such that
∣∣μ(τ)(x;OD)∣∣ ≤ c7(θD) + ce(θ∏)kxk;
∣∣μ(T)(x;θD) - μ(T)(x;θD)∣∣ ≤ kθD - θDH(c8(θD,θD)+ c9(θD,θD)kxk).
Proof [of Lemma 8] Combining equation 9 in Lemma 6 with equation 11 in Lemma 7, we obtain
the growth condition of the form
∣∣μ(T)(x; On, Of)∣∣ ≤ cι(b) + c2(w)∣∣h(x; On)k
≤ c1(b) + c2(w)(b(ON) + a(ON)kxk)
= c1(b) + c2(w)b(ON) + c2(w)a(ON)kxk
=: c7(b, w, ON) + c6(w, ON)kxk
= c7(OD) + c6(OD)kxk.
Concerning the Lipschitz property, using equation 12 and equation 13, we get
∣∣μ(T )(x; On ,Of ) - μ(T )(x; ON ,θF )∣∣
≤ ∣∣μ(T )(x; On ,Of ) - μ(T )(x; ONN ,Of )∣∣ + ∣∣μ(T) (x; ONN ,Of ) - μ(T )(x; ON ,w0,b0)∣∣
≤ c5kwkkh(x; ON) - h(x; ON0 )k + c3kw - w0kkh(x; ON0 )k + c4kb - b0k
≤ c5kwkkON - ON0 k(β (ON, ON0 ) + α(ON, ON0 )kxk)
+ c3kw - w0k(b(ON) + a(ON)kxk) + c4kb - b0k,
20
Under review as a conference paper at ICLR 2020
where the last inequality follows from equation 9 and equation 10 established in Proposition 6. With
the concatenation θD := (θN , θF ) := (θN , w, b) we obtain
∣∣μ(T)(χ;Θd) -μ(T)(χ;θD)∣∣ ≤ I∣Θd -ODIKOd/D) + c9(θD,θD)kχk)
and thus the assertion is proved.	■
Proof [of Proposition 4] First we give the growth conditions for the linear kernel. Let k be the linear
kernel kL(x, y) := hx, yi, then we have
|kL(x, y)| ≤ IxI2 + IyI2 + 1,
kVχ,ykL(x,y)k ≤ (kxk2 + kyk2)1/2 + 1.
If we denote u := (x, y), then the above linear kernel kL (x, y) can be written a as a univariate
function KL(u), that is,
K(u) := KL(u) := kL(x, y).	(16)
Therefore we have K is continuously differentiable satisfying the following growth conditions:
|K(u)| ≤ IuI2 + 1,	(17)
IVuK(u)I ≤ IuI +1.	(18)
Let us define the function f : [0, 1] → R by
f(t) = K(tu + (1 - t)v),	t ∈ [0, 1].
Then we have f(0) = K(v) and f(1) = K(u). Moreover, f is differentiable with derivative
f0(t) = VK((tu + (1 - t)v))(u - v).
The growth condition equation 18 implies
|f0(t)| = IVK((tu + (1 - t)v)(u - v)I
≤ IVK(tu + (1 - t)v)II(u - v)I
≤ (kt(U - v) + Vk + 1)ku — Vk
≤ ((Iu -Vk + llvk) + 1)ku - vk.
Using the mean value theorem, we obtain that for all u, V ∈ RL, there holds
|K (u) - K(v)l≤ ((ku-Vk + kvk) + l)ku-vk.
With u := μθT)(x) and V := μθT)(x), one gets
IK(μθT)(X)) -K(μθT)(X))I ≤ (∣∣μθT)(X)-μθT)(X)∣∣ + ∣∣μθT)(X)∣∣+ 1)∣∣μθT)(X)-μθT)(X)∣∣.
Proposition 8 tells us that
∣∣μθT)(X)-μθT)(X)II ≤ (c8(θD,θD) + c9(θD,θD)kek)kθp-θDk,
∣∣μθT) (X)II ≤ c7(θD ) + c6(θD )kXk.
holds for some continuous functions c6, c7, c8, and c9, which are also bounded by certain constant
B > 0 on the ball Br(OD). Some elementary algebra shows that
∣k(μθT>)) - κ(μθT)(X))∣ ≤ (b2(『 + 1)(1 + k-k)2 + b(i + ∣X∣))kθD-。Dk∙
Since t 7→ (1+t1/2)2 is concave on t ≥ 0, Jensen’s inequality together with the moment assumption
EkXk2 < ∞ implies
E(i + ∣Xk)2 ≤ (ι + (EkXk2)1/2)2 < ∞
and also E(1 + kXk) < ∞ by the moment assumption. Therefore, the regular function c(X) defined
by
C(X) = (B2(r + 1)(1 + ∣∣X∣)2 + B(1 + ∣∣x∣))
is integrable and thus our assertion is proved.	■
21
Under review as a conference paper at ICLR 2020
C.2 Proofs of Section B.4
To reach the proof of Proposition 5, we first introduce Lemma 9 and Lemma 11.
Let i be a fixed node. To describe paths through the network’s computational graph, we need to
introduce the following notations:
P := {(i,j, S) ∈ N3 | i ∈ {0,1,...,L},j ∈{1,..., Ji}, S ∈{1,..., Si,j }}；
A(i) := {i0 | i0 is an ancestor of i};
i := {(i0, j, S) ∈ P | i0 ∈ A(i) or i0 = i}；
「∏(i) := U -i0 := {(i0,j,s) ∈P∣i0∈A(i)};
i0 ∈π(i)
∂i := {(i,j,s) ∈P}.
Obviously, We always have ∂i ⊆ —i and -i = ∂i ∪ -π(i).
We define a backward trajectory starting from node i by
q := (i0, ji0 )i0∈A(i)∪{i},	ji0 ∈ {1, . . . , Ji0}.
The set of all backward trajectories for node i will be denoted by Q(i).
Lemma 9 Let i be a fixed node in the network graph. If Θn ∈ RlθN| \ ∂S-i, then there exist a
constant η > 0 and a trajectory q ∈ Q(i) such that for all θN0 ∈ Bη (θN), there holds
hiθN0 = fq(θN0 ),
where fq is the real analytic function on RlθN | with the same structure as hθ`, only replacing each
nonlinear fi0 with the analytic function fij0i0 for (i0, ji0 ) ∈ q.
Proof [of Lemma 9] We proceed by induction on the nodes of the network. If i = 0, we obviously
have h0N = x, which is real analytic on R1 θN |. For the induction step We assume that the assertion
holds for —∏(i) and let Θn ∈ RlθN| \ ∂S-i. Then there exist a constant η > 0 and a trajectory
q ∈ Q(π(i)) such that for all θN0 ∈ Bη (θN), there holds
hθπN0(i) = fq(θN0 )	(19)
with fq : R1 θN | → R being a real analytic function.
(i)	If Θn ∈ Sdi, then there exists a sufficiently small constant W > 0 such that B犷 (Θn) ∩ Sdi = 0.
Therefore, there exists a j ∈ {1, . . . , Ji} such that for all θN0 ∈ Bη0 (θN), there holds
hiθN0 = fij (hθπN0(i) )
where fij is one of the real analytic functions in the definition of fi. Then, equation 19 implies that
for all θN0 ∈ Bmin{η,η0}(θN), there holds
hiθ0N = fij (fq(θN0 )).	(20)
(ii)	Consider the case θN ∈ S∂i . By assumption, we have θN 6∈ ∂S∂i . Then there exists a small
enough constant η0 > 0 such that Bη0 (θN) ⊂ S∂i. Ifwe denote
A := {p = (i, j, S) ∈ ∂i | θN ∈ SP},
then A 6= 0 since θN ∈ S∂i . Therefore, we have
θN ∈ \ Sp and	θN 6∈ [ Sp .
p∈A	p∈Ac
22
Under review as a conference paper at ICLR 2020
Now we show by contradiction that for η0 small enough, there holds
Bη0(θN) ⊆ \ Sp.
p∈A
To this end, we assume that there exists a sequence of parameter and index pairs (θN,n , pn) such
that pn ∈ Ac, θN,n ∈ Spn , and θN,n → θN as n → ∞. Since Ac is finite, there exists a constant
subsequence {pni} ⊂ {pn} and some constant p0 ∈ Ac with pni = p0 for all i. Then the continuity
of the network and gp0 imply that Sp0 is a closed set and consequently we obtain θN ∈ Sp0 by taking
the limit, which contradicts the fact that θN 6∈ Sp∈Ac Sp. Therefore, for η0 small enough, there holds
Bη0 (θN) ⊆ Tp∈A Sp, which contradicts the assumption θN 6∈ ∂S∂i. Consequently, there exists a
j ∈ {1,..., Ji} satisfying equation 20. By setting fq0 = f (fq) with qo = ((i,j)㊉ q) ∈ Q(i),
where ㊉ denotes concatenation, then for for all θN ∈ Bmin{%η,}(θN), there holds
hiθN0 = fq0 (θN0 )
and the assertion is proved.	■
Now, for a fixed p = (i, j, s) ∈ P, we denote the set of network parameters θN that lie on the
boundary of p by
SP = {θN ∈ RlθNl I gj,s(h∏Ni)) = 0},
where the functions gij,s are as in Definition 3. As usual, the boundary of the set Sp is denoted by
∂Sp and the set of the boundaries is denoted by
∂SP := [ ∂Sp.	(21)
p∈P
Finally, for the ease of notations, ifP0 ⊂ P, we write
SP0 := [ Sp0,	∂SP0 := [ ∂Sp0 .	(22)
p0∈P0	p0∈P0
Obviously, We have Θn ∈ Rm \ ∂S-π(i). To prove Lemma 11, we need the following lemma which
follows directly from Mityagin (2015) and hence we omit the proof.
Lemma 10 Let Θn→ F (Θn ) : R1 θN | → R be a real analytic function and define
M := {Θn ∈ RlθN| I F(Θn) = 0}.
Then we have either μ(M) = 0 or F = 0.
Lemma 11 Let the set of boundaries ∂SP be as in equation 21. Then we have
μ(∂ S P) = 0.
Proof [of Lemma 11] We proceed the proof by induction. For i = 0, we obviously have ∂S-0 = 0
and therefore μ(∂S-0) = 0. For the induction step let US assume that
μ(∂S-π(i))=0.
For s = (p, q), the pair of an index p ∈ ∂i and a trajectory q ∈ Q(i), we define
Ms ：= {Θn ∈ RlθNl I gp(fq(Θn)) = 0},
where the analytic function fq is defined as in Proposition 9. Then we prove by contradiction that
for any Θn ∈ ∂Sdi \ ∂S-π(i), there exists an S ∈ ∂i X Q(i) such that Θn ∈ Ms and
μ(Ms) = 0.
23
Under review as a conference paper at ICLR 2020
To this end, let Θn ∈ ∂Sdi \ ∂S-π(i), then for small enough η > 0, there holds Bn(Θn) ⊂ RlθN| \
∂S-π(i). By Proposition 9, there exists a trajectory q ∈ Q(π(i)) such that for all θN ∈ Bn(Θn),
there holds
hθπ0(i) = fq (θN0 ).	(23)
Moreover, since θN ∈ ∂S∂i, there exists an index p ∈ ∂i such that gp(hθπ(i)) = 0. This means that
for s = (p, q), we have θN ∈ Ms. Therefore, we have
∂Sdi \ ∂S-π⑴ ⊂ U Ms,	(24)
s∈A
where A ⊆ P × UL=0 Q(j) is finite. Suppose that μ(Ms) > 0, then by Lemma 10, We have
Ms = RlθN| and hence Bn(Θn) ⊂ Ms. By equation 23, we then have Bn(Θn) ⊂ Sdi, which
contradicts the fact that θN ∈ ∂S∂i and hence we have
μ(Ms)=0.	(25)
Combing equation 24 and equation 25, we obtain
μ(∂Sdi \ ∂S-n(i)) ≤ X μ(Ms) = 0.
s∈A
By the assumption μ(∂S-π(i)) = 0, We conclude that μ(∂S-i) = 0. Since for the last node L, we
have -L = P and therefore μ(∂SP) = 0.	■
Note that the random forest μ(τ)(∙) can be considered as the composition of affine transformations
and sigmoid functions and hence are always continuously differentiable with respect to θF, we only
need to investigate whether the neural network hθN (x) is differentiable with respect to θN. For a
fixed x ∈ Rd, we write
Θχ = {Θn,o ∈ RlθN| | Θn → hθN (x) is not differentiable at Θn,o}	(26)
as the set of parameters for which the network is not differentiable.
Lemma 12 Let the set Θx be as in equation 26. Then, for any x ∈ Rd, we have
μ(Θχ) = 0.
Proof [of Lemma 12] Let the boundary set ∂SP be defined as in equation 22 and θN,0 ∈ Θx.
Obviously, we have θ0 ∈ SP. We proceed the proof of the inclusion Θx ⊆ ∂SP by contradiction.
To this end, we assume that θD,0 6∈ ∂SP. When Lemma 9 applied to the output layer, there exist
some η > 0 and a real analytic function f(θN) such that for all θN ∈ Bn(θN,0), there holds
hθN = f(θN).
Consequently, the network is differentiable at θN,0, contradicting the fact that θN,0 ∈ Θx .
Therefore, we have Θχ ⊆ ∂SP and hence μ(Θχ) = 0 since μ(∂SP) = 0 according to Lemma 11.
Proof [of Proposition 5] Let the sets N(θN), ΘPX, and Θx be as in equation 7, equation 8, and
equation 26, respectively. Consider the sets
Si =	{(Θn, x)	∈ RlθN|	× Rd	|	Θn ∈	Θpχ and X	∈ N(Θn)},
S2 =	{(Θn,x)	∈ RlθN|	× Rd	|	Θn ∈	Θχ}.
Since the network is continuous and not differentiable, Theorem I in Piranian (1966) implies that
the sets S1 and S2 are measurable. Obviously, we have S1 ⊂ S2 and therefore we obtain ν(S1) ≤
ν(S2), where V := PX ③ μ. On the one hand, Fubini,s theorem implies
ν(S2)
dμ(θN) dΡχ (x)=
Rd
μ(Θχ) dPχ(x).
24
Under review as a conference paper at ICLR 2020
By Lemma 12, We have μ(Θχ) = 0 and therefore V(S2) = 0 and hence ν(S1) = 0. On the other
hand, Fubini’s theorem again yields
ν(S1)
/	/	dPχ(x) dμ(θN) = /	PX(N(°n)) &四(。N)∙
By the definition of Θpχ we have PX(N(Θn)) > 0 for all Θn ∈ Θpχ. Therefore, V(Si) = 0
implies that μ(θpχ )=0.	■
C.3 Proofs of Section 3.5
To prove Theorem 2, we also need the following lemma.
Lemma 13 Let (θn)n∈N be a sequence in Rm converging towards θ0, i.e., θn 6= θ0 as n → ∞.
Moreover, let f : Rm → R be a function and g be a vector in Rm. If
∣f(θn)- f(°0)- g ∙ (0n - 00)| 一 0
k°n-θθk
holds for all sequences (θn)n∈N, then f is differentiable at θ0 with differential g.
Proof [of Lemma 13] The definition ofa differential tell us that g is the differential of f at θ0, if
δ→→m ^pklf (θo+δ)- f(°。)- g ∙δ =0.
By the sequential characterization of limits, we immediately obtain the assertion.
Proof [of Theorem 2] Consider the following augmented networks:
h(1D ,Ψ)(X,Z ) = (μθT)(X ),μθT)(Gψ (Z))),
磕 DMIZZ0 = (μθT)(Gψ (ZL(Gψ (Z0))),
hθD(x,x 0) = (μθT)(X ),μθT)(X 0)).
Without loss of generality, in the following, we only consider the network h((θ1) ,ψ)(X, Z) with inputs
from PX 0 PZ, which satisfies EPXOPZ ∣∣(X, Z)∣∣2 < ∞.
By the expression of definition 1, we have
kRF(hθN(X),hθN(Gψ(Z)); 0F) = T D〃(T)(hθz(X); 0F),μ(T)(hθN(Gψ(Z));吟
=T DμθT)(X ),μθT)(Gψ (Z ))〉
where we denote μθT )(x) := μ(T )(hθw (x); °f ). Due to the linear kernel kL(x,y) :=(x,y)，there
holds
kRF(hθN(X)— (Gψ(Z)); Θf) ∙ T = k.(μθT)(X),μθT)(Gψ(Z)))
=K (h(θD,ψ)(χ, Z))
=：K (hλ(u))),
where u := (X, Z), λ := (0D, ψ), and the second equation is due to equation 16. Suppose that there
exists a λo = (0d,o, Ψo) SUch that the function (°d, ψ) → (μθT)(x), GY(z)) is differentiable at λo
for PX -almost all x and PZ -almost all z. Then, according to Proposition 5, this statement holds for
μ-almost all 0d ∈ R1 θD| and all ψ ∈ R|Y|. Consider a sequence (λn)n∈N that converges to λo, then
25
Under review as a conference paper at ICLR 2020
there exists a δ > 0 such that kλn - λ0k < δ for all n ∈ N. For a fixed u = (x, z) ∈ Rd × R|z|,
Proposition 4 states that there exists a regular function c(u) with EPX c(X) < ∞ such that
|K(hλn (U))- K(hλ0 (u))∣≤ c(u)kλn- λok
and consequently we have
∣∂λK(hλo(u))| ≤ c(u)
for PX 0 PZ-almost all U ∈ R|u|.
For n ∈ N, we define the sequence gn(x) by
gn(U)
|K(hλn (U))- K(hλ0 (U))- ∂λK(hλ0 (u))(λn - λo)∣
kλn - λ0k
Obviously, the sequence gn(x) converges pointwise to 0 and is bounded by the integrable function
2c(U). By the dominated convergence theorem, see e.g., Theorem 2.24 in Folland (1999), we have
EPX 0Pz gn (U) → 0.
Moreover, for n ∈ N, we define the sequence gn(x) by
g _ |EpxK(hλ(U))- EPXK(hλo (U))- EPX∂λK①(U))(λn - λo)∣
'n	kλn - λ0k
Clearly, the sequence In(X) is upper bounded by EPXOPZgn(〃)and therefore converges to 0. By
Lemma 13, EPXopz [K(hλ(U))] is differentiable at λo with differential
EPX OPZ ∂λ K (hλ (U)).
Since similar results as above hold also for the networks h(2) and h(3), and Lemma 6 in Gretton
et al. (2012) states that MMD2u is unbiased, our assertion follows then from the linearity of the form
of MMDu in equation 1.	■
D	Samples of Generated Pictures
Generated samples on the datasets CIFAR-10, CelebA, and LSUN bedroom are shown in Figure
4, 5, and 6, respectively.
26
Under review as a conference paper at ICLR 2020
(a) Real Samples
(b) MMD loss with mix-rbf kernel
(c) MMD loss with mix-rq kernel
(d) MMD loss with random-forest kernel
(e) MMD repulsive loss with rbf-b kernel
Figure 4: Generated Pictures with different kernels and two different loss functions on 32x32 CIFAR-10
dataset.
(f) MMD repulsive loss with random-forest kernel
27
Under review as a conference paper at ICLR 2020
(a) Real Samples
(c) MMD loss with mix-rq kernel
(d) MMD loss with random-forest kernel
(f) MMD repulsive loss with random-forest kernel
(e) MMD repulsive loss with rbf-b kernel
Figure 5: Generated Pictures with different kernels and two different loss functions on 160x160 CelebA dataset.
(b) MMD loss with mix-rbf kernel
28
Under review as a conference paper at ICLR 2020
(a) Real Samples
(b) MMD loss with mix-rbf kernel
(c) MMD loss with mix-rq kernel
(d) MMD loss with random-forest kernel
(e) MMD repulsive loss with rbf-b kernel
Figure 6: Generated Pictures with different kernels and two different loss functions on 64x64 LSUN bedrooms
dataset.
(f) MMD repulsive loss with random-forest kernel
29