Under review as a conference paper at ICLR 2020
Stochastic Gradient Descent with
Biased but Consistent Gradient Estimators
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient descent (SGD), which dates back to the 1950s, is one of the
most popular and effective approaches for performing stochastic optimization. Re-
search on SGD resurged recently in machine learning for optimizing convex loss
functions and training nonconvex deep neural networks. The theory assumes that
one can easily compute an unbiased gradient estimator, which is usually the case
due to the sample average nature of empirical risk minimization. There exist,
however, many scenarios (e.g., graphs) where an unbiased estimator may be as
expensive to compute as the full gradient because training examples are intercon-
nected. Recently, Chen et al. (2018) proposed using a consistent gradient estima-
tor as an economic alternative. Encouraged by empirical success, we show, in a
general setting, that consistent estimators result in the same convergence behavior
as do unbiased ones. Our analysis covers strongly convex, convex, and noncon-
vex objectives. We verify the results with illustrative experiments on synthetic
and real-world data. This work opens several new research directions, including
the development of more efficient SGD updates with consistent estimators and the
design of efficient training algorithms for large-scale graphs.
1	Introduction
Consider the standard setting of supervised learning. There exists a joint probability distribution
P(x, y) of data x and associated label y and the task is to train a predictive model, parameterized by
w, that minimizes the expected loss ` between the prediction and the ground truth y. Let us organize
the random variables as ξ = (x, y) and use the notation `(w; ξ) for the loss. If ξi = (xi, yi),
i = 1, . . . , n, are iid training examples drawn from P, then the objective function is either one of
the following well-known forms:
1n
expected risk f (W) =E['(w; ξ)];	empirical risk f (W) = 一 £'(w; ξi).	(1)
ni=1
Stochastic gradient descent (SGD), which dates back to the seminal work of Robbins & Monro
(1951), has become the de-facto optimization method for solving these problems in machine learn-
ing. In SGD, the model parameter is updated until convergence with the rule1
Wk+1 = Wk - γkgk ,	k = 1, 2,. . .	,	(2)
where Yk is a step size and gk is an unbiased estimator of the gradient Vf (Wk). Compared with the
full gradient (as is used in deterministic gradient descent), an unbiased estimator involves only one
or a few training examples ξi and is usually much more efficient to compute.
1.1	Limitation of Unbiased Gradient and Remedy: Consistent Gradient
This scenario, however, does not cover all learning settings. A representative example that leads to
costly computation of the unbiased gradient estimator V'(w, ξi) is graph nodes. Informally speak-
ing, a graph node ξi needs to aggregate information from its neighbors. If information is aggregated
1For introductory purpose we omit the projection operator for constrained problems. All analysis in this
work covers projection.
1
Under review as a conference paper at ICLR 2020
across neighborhoods, ξi must request information from its neighbors recursively, which results in
inquiring a large portion of the graph. In this case, the sample loss ` for ξi involves not only ξi ,
but also all training examples within its multihop neighborhood. The worst case scenario is that
computing V'(w, ξi) costs O(n) (e.g., for a complete graph or small-world graph), as opposed to
O(1) in the usual learning setting because only the single example ξi is involved.
In a recent work, Chen et al. (2018) proposed a consistent gradient estimator as an economic al-
ternative to an unbiased one for training graph convolutional neural networks, offering substantial
evidence of empirical success. A summary of the derivation is presented in Section 2. The subject of
this paper is to provide a thorough analysis of the convergence behavior of SGD when gk in (2) is a
consistent estimator of Vf (wk). We show that using this estimator results in the same convergence
behavior as does using unbiased ones.
Definition 1. An estimator gN ofh, where N denotes the sample size, is consistent ifgN converges
to h in probability: plimN→∞ gN = h. That is, for any > 0, limN→∞ Pr(kgN - hk > ) = 0.
1.2	Distinctions between Unbiasedness and Consistency
It is important to note that unbiased and consistent estimators are not subsuming concepts (one does
not imply the other), even in the limit. This distinction renders the departure of our convergence
results, in the form of probabilistic bounds on the error, from the usual SGD results that bound
instead the expectation of the error.
In what follows, we present examples to illustrate the distinctions between unbiasedness and con-
sistency. To this end, we introduce asymptotic unbiasedness, which captures the idea that the bias
of an estimator may vanish in the limit.
Definition 2. An estimator gN of h, where N denotes the sample size, is asymptotically unbiased
ifE[gN] → h.
An estimator can be (asymptotically) unbiased but inconsistent. Consider estimating the mean
h = μ of the normal distribution N(μ,σ2) by using N independent samples Xi,... ,Xn. The
estimator gN = X1 (i.e., always use X1 regardless of the sample size N) is clearly unbiased because
E[Xi] = μ; but it is inconsistent because the distribution of Xi does not concentrate around μ.
Moreover, the estimator is trivially asymptotically unbiased.
An estimator can be consistent but biased. Consider estimating the variance h = σ2 of the
normal distribution N(μ, σ2) by using N independent samples Xi,..., Xn. The estimator gN =
PN=I(Xi - X)2/N, where X = PN=I Xi/N, has mean σ2(N - 1)/N and variance 2σ4(N -
1)/N2. Hence, it is consistent owing to a straightforward invocation of the Chebyshev inequality,
by noting that the mean approaches σ2 and the variance approaches zero. However, the estimator
admits a nonzero bias σ2 /N for any finite N.
An estimator can be consistent but biased even asymptotically. In the preceding example, the
bias σ2 /N approaches zero and hence the estimator is asymptotically unbiased. Other examples
exist for the estimator to be biased even asymptotically. Consider estimating the quantity h = 0
with an estimator gN that takes the value 0 with probability (N - 1)/N and the value N with
probability 1/N . Then, the probability that gN departs from zero approaches zero and hence it is
consistent. However, E[gN] = 1 and thus the bias does not vanish as N increases.
1.3	Contributions of This Work
To the best of our knowledge, this is the first work that studies the convergence behavior of SGD
with consistent gradient estimators, which result from a real-world graph learning scenario that will
be elaborated in the next section. With the emergence of graph deep learning models (Bruna et al.,
2014; Defferrard et al., 2016; Li et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer
et al., 2017; VeliCkOviC et al., 2018), the scalability bottleneck caused by the expensive computation
of the sample gradient becomes a pressing challenge for training (as well as inference) with large
graphs. We believe that this work underpins the theoretical foundation of the efficient training of a
series of graph neural networks. The theory reassures practitioners of doubts on the convergence of
their optimization solvers.
2
Under review as a conference paper at ICLR 2020
Encouragingly, consistent estimators result in a similar convergence behavior as do unbiased ones.
The results obtained here, including the proof strategy, offer convenience for further in-depth analy-
sis under the same problem setting. This work opens the opportunity of improving the analysis, in a
manner similar to the proliferation of SGD work, from the angles of relaxing assumptions, refining
convergence rates, and designing acceleration techniques.
We again emphasize that unbiasedness and consistency are two separate concepts; neither subsumes
the other. One may trace that we intend to write the error bounds for consistent gradient estimators
in a manner similar to the expectation bounds in standard SGD results. Such a resemblance (e.g., in
convergence rates) consolidates the foundation of stochastic optimization built so far.
2	M otivating Application: Representation Learning of Graph
Nodes
For a motivating application, consider the graph convolutional network model, GCN (Kipf &
Welling, 2017), that learns embedding representations of graph nodes. The l-th layer of the net-
work is compactly written as
H(l+1) = σ(AbH(l)W(l)),	(3)
where Ab is a normalization of the graph adjacency matrix, W (l) is a parameter matrix, and σ is
a nonlinear activation function. The matrix H (l) contains for each row the embedding of a graph
node input to the l-th layer, and similarly for the output matrix H(l+1) . With L layers, the network
transforms an initial feature input matrix H(0) to the output embedding matrix H(L). For a node v,
the embedding H(L) (v, :) may be fed into a classifier for prediction.
Clearly, in order to compute the gradient of the loss for v, one needs the corresponding row of H(L),
the rows of H(L-1) corresponding to the neighbors of v, and further recursive neighbors across each
layer, all the way down to H(0). The computational cost of the unbiased gradient estimator is rather
high. In the worst case, all rows of H(0) are involved.
To resolve the inefficiency, Chen et al. (2018) proposed an alternative gradient estimator that is bi-
ased but consistent. The simple and effective idea is to sample a constant number of nodes in each
layer to restrict the size of the multihop neighborhood. For notational clarity, the approach may be
easier to explain for a network with a single layer; theoretical results for more layers straightfor-
wardly follow that of Theorem 1 below, through induction.
The approach generalizes the setting from a finite graph to an infinite graph, such that the matrix
expression (3) becomes an integral transform. In particular, the input feature vector H(0)(u, :) for a
node u is generalized to a feature function X (u), and the output embedding vector H(1) (v, :) for a
node v is generalized to an embedding function Z(v), where the random variables u and v in two
sides of the layer reside in different probability spaces, with probability measures P (u) and P (v),
respectively. Furthermore, the matrix A is generalized into a bivariate kernel A(v, u) and the loss `
is written as a function of the output Z(v). Then, (1) and (3) become
f = Ev〜P(V)['(Z(v))] With Z(V)= σ( / A(v,u)X(U)WdP(U)) .	(4)
Such a functional generalization facilitates sampling on all network layers for defining a gradient
estimator. In particular, defining B(v) = A(v, U)X(U) dP (U), simple calculation reveals that the
gradient With respect to the parameter matrix W is
G:
Vf
q(B(v)) dP (v),
where q(B) = BT Vh(BW) and h = ' ◦ σ.
Then, one may use t iid samples of U in the input and s iid samples of v in the output to define an
estimator of G:
1s	1t
Gst ：= — Eq(Bt(Viy), Vi 〜P(v),	with Bt(v):=彳£才3%)X(Uj),	Uj 〜P(u).
s i=1	t j=1
The gradient estimator Gst so defined is consistent; see a proof in the supplementary material.
Theorem 1.	If q is continuous and f is finite, then plims,t→∞ Gst = G.
3
Under review as a conference paper at ICLR 2020
3 Setting and Notations
We now settle the notations for SGD. We are interested in the (constrained) optimization problem
minf(w),
w∈S
where the feasible region S is convex. This setting includes the unconstrained case S = Rd . We
assume that the objective function f : Rd → R is subdifferentiable; and use ∂f (w) to denote the
subdifferential at w. When it is necessary to refer to an element of this set, we use the notation h. If
f is differentiable, then clearly, ∂f (W) = {Vf (w)}.
The standard update rule for SGD is wk+1 = ΠS (wk - γkgk), where gk is the negative search
direction at step k, γk is the step size, and ΠS is the projection onto the feasible region: ΠS(w) :=
argminu∈S kw - uk. For unconstrained problems, the projection is clearly omitted: wk+1 = wk -
γkgk.
Denote by w* the global minimum. We assume that w* is an interior point of S, so that the subdiffer-
ential of f at w* contains zero. For differentiable f, this assumption simply means that Vf (w*) = 0.
Typical convergence results are concerned with how fast the iterate wk approaches w*, or the func-
tion value f(wk) approaches f (w*). Sometimes, the analysis is made convenient through a con-
vexity assumption on f, such that the average of historical function values f(wi), i = 1, . . . , k, is
lowered bounded by f (wk), with wk being the cumulative moving average wk = 1 Pk=I Wi.
The following definitions are frequently referenced.
Definition 3. We say that f is l-strongly convex (with l > 0) if for all w, u ∈ Rd and hu ∈ ∂f(u),
f (W) - f(u) ≥ hhu,w - Ui + 2∣∣w - u∣∣2.
Definition 4. We say that f is L-smooth (with L > 0) if it is differentiable and for all w, u ∈ Rd,
∣Vf (w) - Vf (u)∣ ≤ L∣w - u∣.
4 Convergence Results
Recall that an estimator gN of h is consistent if for any > 0,
lim Pr(kgN - hk > ) = 0.
N→∞
(5)
In our setting, h corresponds to an element of the subdifferential at step k; i.e., hk ∈ ∂f (wk), gN
corresponds to the negative search direction gk, and N corresponds to the sample size Nk. That gkNk
converges to hk in probability does not imply that gkNk is unbiased. Hence, a natural question asks
what convergence guarantees exist when using gkNk as the gradient estimator. This section answers
that question.
First, note that the sample size Nk is associated with not only gkNk, but also the new iterate wkN+k1.
We omit the superscript Nk in these vectors to improve readability.
Similar to the analysis of standard SGD, which is built on the premise of the unbiasedness of gk and
the boundedness of the gradient, in the following subsection we elaborate the parallel assumptions
in this work. They are stated only once and will not be repeated in the theorems that follow, to avoid
verbosity.
4.1 Assumptions
The convergence (5) of the estimator does not characterize how fast it approaches the truth. One
common assumption is that the probability in (5) decreases exponentially with respect to the sample
size. That is, we assume that there exists a step-dependent constant Ck > 0 and a nonnegative
function τ(δ) on the positive axis such that
Pr kgk -hkk ≥ δkhkk g1,...,gk-1 ≤ Cke-Nkτ(δ)
(6)
4
Under review as a conference paper at ICLR 2020
for all k > 1 and δ > 0. A similar assumption is adopted by Homem-de-Mello (2008) that studied
stochastic optimization through sample average approximation. In this case, the exponential tail
occurs when the individual moment generating functions exist, a simple application of the Chernoff
bound. For the motivating application GCN, the tail is indeed exponential as evidenced by Figure 3.
Note the conditioning on the history g1 , . . . , gk-1 in (6). The reason is that hk (i.e., the gradient
Vf (Wk) if f is differentiable) is by itself a random variable dependent on history. In fact, a more
rigorous notation for the history should be filtration, but we omit the introduction of unnecessary
additional definitions here, as using the notion g1, . . . , gk-1 is sufficiently clear.
Assumption 1. The gradient estimator gk is consistent and obeys (6).
The use ofa tail bound assumption, such as (6), is to reverse-engineer the required sample size given
the desired probability that some event happens. In this particular case, consider the setting where
T SGD updates are run. For any δ ∈ (0, 1), define the event
Eδ = kg1 - h1k ≤ δkh1k and kg2 - h2k ≤ δkh2k and . . . and kgT - hTk ≤ δkhT k .
Given (6) and any ∈ (0, 1), one easily calculates that if the sample sizes satisfy
Nk ≥ τ(δ)-1 log(TCk/),
(7)
for all k, then,
Pr(Eδ) ≥ Y (1 - Cke-Nkτ(δ) ) ≥ Y (1 - /T) ≥ 1 - .
k=1	k=1
Hence, all results in this section are established under the event Eδ that occurs with probability at
least 1 - , a sufficient condition of which is (7).
The sole purpose of the tail bound assumption (6) is to establish the relation between the required
sample sizes (as a function of δ and ) and the event Eδ, on which convergence results in this work
are based. One may replace the assumption by using other tail bounds as appropriate. It is out of the
scope of this work to quantify the rate of convergence of the gradient estimator for a particular use
case. For GCN, the exponential tail that agrees with (6) is illustrated in Section 5.4.
Additionally, parallel to the bounded-gradient condition for standard SGD analysis, we impose the
following assumption.
Assumption 2. There exists a finite G > 0 such that khk ≤ G for all h ∈ ∂f (w) and w ∈ S.
4.2 Results
Let us begin with the strongly convex case. For standard SGD with unbiased gradient estimators,
ample results exist that indicate O(1/T) convergence2 for the expected error, where T is the number
of updates; see, e.g., (2.9)-(2.10) of Nemirovski et al. (2009) and Section 3.1 OfLacoSte-JUlien
et al. (2012). We derive similar results for consistent gradient estimators, as stated in the following
Theorem 2. Different from the unbiased case, it is the error, rather than the expected error, to be
bounded. The tradeoff is the introduction of the relative gradient estimator error δ, which relates to
the sample sizes as in (7) for guaranteeing satisfaction of the bound with high probability.
Theorem 2.	Let f be l-strongly convex with l ≤ G∕∣∣wι — w*∣∣. Assume that T updates are run,
with diminishing step size Yk = [(l — δ)k]-1 for k = 1, 2,... ,T, where δ = ρ∕T and ρ < l is
an arbitrary constant independent ofT. Then, for any such ρ, any ∈ (0, 1), and sufficiently large
sample sizes satisfying (7), with probability at least 1 — , we have
∣∣WT — W*
k2 ≤ G
(1 + ρ∕T )2 + P(l—ρ∕T)
(l—P/T )2
and
f(WT) — f(w*) ≤ G P + (Il-P∕T)2(1+logT)
2Ignoring the logarithmic factor, if any.
(8)
(9)
5
Under review as a conference paper at ICLR 2020
Note the assumption on l in Theorem 2. This assumption is mild since if f is l-strongly convex, it is
also l0 -strongly convex for all l0 < l. The assumption is needed in the induction proof of (8) when
establishing the base case ∣∣wι - w*∣∣. One may remove this assumption at the cost of a cumbersome
right-hand side of (8), over which we favor a neater expression in the current form.
With an additional smoothness assumption, we may eliminate the logarithmic factor in (9) and
obtain a result for the iterate WT rather than the running average WT. The result is a straightforward
consequence of (8).
Theorem 3.	Under the conditions of Theorem 2, additionally let f be L-smooth. Then, for any
ρ satisfying the conditions, any ∈ (0, 1), and sufficiently large sample sizes satisfying (7), with
probability at least 1 - , we have
f(wτ)-f(w*) ≤ 拿 « + P/T)2 + P(I-P/TH
(I - p/T )2
(10)
In addition to O(1/T) convergence, it is also possible to establish linear convergence (however)
to a non-vanishing right-hand side, as the following result indicates. To obtain such a result, we
use a constant step size. Bottou et al. (2016) show a similar result for the function value with an
additional smoothness assumption in a different setting; we give one for the iterate error without the
smoothness assumption using consistent gradients.
Theorem 4.	Under the conditions of Theorem 2, except that one sets a constant step size γk = c
with 0 < c < (2l - δ)-1 for all k, for any ρ satisfying the conditions, any ∈ (0, 1), and sufficiently
large sample sizes satisfying (7), with probability at least 1 - , we have
∣wt - w*∣2 ≤ (1 - 2cl + cδ)TTkwI- w*k2 + δ + ；(1+ δ)2 G2.	(11)
2l - δ
Compare (11) with (8) in Theorem 2. The former indicates that in the limit, the squared iterate
error is upper bounded by a positive term proportional toG2; the remaining part of this upper bound
decreases at a linear speed. The latter, on the other hand, indicates that the squared iterate error in
fact will vanish, although it does so at a sublinear speed O(1/T).
For convex (but not strongly convex) f, typically O(1∕√T) convergence is asserted for unbiased
gradient estimators; see., e.g., Theorem 2 of Liu (2015). These results are often derived based on
an additional assumption that the feasible region is compact. Such an assumption is not restrictive,
because even if the problem is unconstrained, one can always confine the search to a bounded region
(e.g., an Euclidean ball). Under this condition, we obtain a similar result for consistent gradient
estimators.
Theorem 5.	Let f be convex and the feasible region S have finite diameter D > 0; that is,
suPw,u∈s kw — Uk = D. Assume that T updates are run, with diminishing step size Yk = c∕√k for
k = 1, 2,... ,T and for some c > 0. Let δ = p∕√T where p > 0 is an arbitrary constant inde-
pendent ofT. Then, for any such ρ, any ∈ (0, 1), and sufficiently large sample sizes satisfying (7),
with probability at least 1 - , we have
(12)
One may obtain a result of the same convergence rate by using a constant step size. In the case of
unbiased gradient estimators, see Theorem 14.8 of Shalev-Shwartz & Ben-David (2014). For such
a result, one assumes that the step size is inversely proportional to √T. Such choice of the step size
is common and is also used in the next setting.
For the general (nonconvex) case, convergence is typically gauged with the gradient norm. One
again obtains O(1∕√T) convergence results for unbiased gradient estimators; see, e.g., Theorem 1
of Reddi et al. (2016) (which is a simplified consequence of the theory presented in Ghadimi & Lan
(2013)). We derive a similar result for consistent gradient estimators.
Theorem 6.	Let f be L-smooth and S = Rd. Assume that T updates are run, with constant step size
Yk = Df∕[(1 + δ)G√T] for k = 1, 2,..., T, where Df = [2(f (wι) — f (w*))∕L]1, and δ ∈ (0,1)
6
Under review as a conference paper at ICLR 2020
is an arbitrary constant. Then, for any such δ, any ∈ (0, 1), and sufficiently large sample sizes
satisfying (7), with probability at least 1 - , we have
.minι l∣Vf (wk )k2 ≤
k=1,...,T
(1 + δ)LGDf
(1 - δ)√T
(13)
4.3	Interpretation
All the results in the preceding subsection assert convergence for SGD with the use of a consistent
gradient estimator. As with the use of an unbiased one, the convergence for the strongly convex case
is O(1/T), or linear if one tolerates a non-vanishing upper bound, and the convex and nonconvex
cases O(1/√T). These theoretical results, however, are based on assumptions of the sample size Nk
and the step size γk that are practically challenging to verify. Hence, in a real-life machine learning
setting, the sample size and the learning rate (the initial step size) are treated as hyperparameters to
be tuned against a validation set.
Nevertheless, these results establish a qualitative relationship between the sample size and the opti-
mization error. Naturally, to maintain the same failure probability , the relative gradient estimator
error δ decreases inversely with the sample size Nk. This intuition holds true in the tail bound con-
dition (6) with (7), when τ(δ) is a monomial or a positive combination of monomials with different
degrees. With this assumption, the larger is Nk, the smaller is δ (and also ρ, the auxiliary quantity
defined in the theorems); hence, the smaller are the error bounds (8)-(13).
4.4	Remarks
Theorem 4 presents a linear convergence result for the strongly convex case, with a non-vanishing
right-hand side. In fact, itis possible to obtain a result with the same convergence rate but a vanishing
right-hand side, if one is willing to additionally assume L-smoothness. The following theorem
departs from the set of theorems in Section 4.2 on the assumption of the sufficient sample size Nk
and the gradient error δ .
Theorem 7.	Let f be l-strongly convex and L-smooth with l < L. Assume that T updates are
run with constant step size γk = 1/L for k = 1, 2, . . . ,T. Let δk, k ≥ 1 be a sequence where
limk→∞ δk+ι∕δk ≤ L Then, for any positive η < l/L, E ∈ (0,1), and sample sizes
Nk ≥ τ (δk /lhk l)-1 log(TCk/) for k = 1, 2, . . . ,T,
with probability at least 1 - E, we have
f(wτ) - f(w*) ≤ (1 - l∕L)T-1[f (wι) - f (w*)] + O(ET),	(14)
where ET = max{δT2 , (1 - l/L + η)T}.
Here, δk is the step-dependent gradient error. If it decreases to zero, then so does ET . Theorem 7
is adapted from Friedlander & Schmidt (2012), who studied unbiased gradients as well as noisy
gradients. We separate Theorem 7 from those in Section 4.2 only for the sake of presentation clarity.
The spirit, however, remains the same. Namely, consistent estimators result in the same convergence
behavior (i.e., rate) as do unbiased ones. All results require an assumption on sufficient sample size
owing to the probabilistic convergence of the gradient estimator.
5 Numerical Illustrations
In this section, we report several experiments to illustrate the convergence behavior of SGD by using
consistent gradient estimators. We base the experiments on the training of the GCN model (Kipf &
Welling, 2017) motivated earlier (cf. Section 2). The code repository will be revealed upon paper
acceptance.
5.1	Data Sets
We use three data sets for illustration, one synthetic and two real-world benchmarks.
7
Under review as a conference paper at ICLR 2020
The purpose ofa synthetic data set is to avoid the regularity in the sampling of training/validation/test
examples. The data set, called “Mixture,” is a mixture of three overlapping Gaussians. The points
are randomly connected, with a higher probability for those within the same component than the
ones straddling across components. See the supplementary material for details of the construction.
Because of the significant overlap, a classifier trained with independent data points unlikely predicts
well the component label, but a graph-based method is more likely to be successful.
Additionally, we use two benchmark data sets, Cora and Pubmed, often seen in the literature. These
graphs are citation networks and the task is to predict the topics of the publications. We follow the
split used in Chen et al. (2018). See the supplementary material for a summary of all data sets.
5.2	(Strongly) Convex Case
The GCN model is hyperparameterized by the number of layers. Without any intermediate layer,
the model can be considered a generalized linear model and thus the cross-entropy loss function is
convex. Moreover, with the use of an L2 regularization, the loss becomes strongly convex. The
predictive model reads P = softmax(AbX W (0) ), where X is the input feature matrix and P is
the output probability matrix, both row-wise. One easily sees that the only difference between this
model and logistic regression P = softmax(XW (0)) is the neighborhood aggregation AbX.
Standard batched training in SGD samples a batch (denoted by the index set I1) from the training
set and evaluates the gradient of the loss of softmax(Ab(I1, :)XW(0)). In the analyzed consistent-
gradient training, we additionally uniformly sample the input layer with another index set I0 and
evaluate instead the gradient of the loss of Softmax(人 A(Iι, Io)X(Io, ：)W(0)).
epoch ( k / num_batch )
epoch ( k / num_batch )
----SGD consistent (sampl 400)
----SGD consistent (sampl 800)
—SGD consistent (sampl 1600)
----SGD consistent (SamPl 3200)
—SGD consistent (sampl n/2)
—SGD unbiased
----ADAM unbiased
1.2
1
0.8
0.6
0.4
0.2
0	20	40	60	80	100
epoch ( k / num_batch )
(a) Mixture	(b) Cora	(c) Pubmed
Figure 1: Convergence history for 1-layer GCN, under different training algorithms.
Figure 1 shows the convergence curves as the iteration progresses. The plotted quantity is the over-
all loss on all training examples, rather than the batch loss for only the current batch. Hence, not
surprisingly the curves are generally quite smooth. We compare standard SGD with the use of con-
sistent gradient estimators, with varying sample size |Io|. Additionally, we compare with the Adam
training algorithm (Kingma & Ba, 2015), which is a stochastic optimization approach predominantly
used in practice for training deep neural networks.
One sees that for all data sets, Adam converges faster than does standard SGD. Moreover, as the
sample size increases, the loss curve with consistent gradients approaches that with an unbiased
one (i.e., standard SGD). This phenomenon qualitatively agrees with the theoretical results; namely,
larger sample size improves the error bound. Note that all curves in the same plot result from the
same parameter initialization; and all SGD variants apply the same learning rate.
It is important to note that the training loss is only a surrogate measure of the model performance;
and often early termination of the optimization acts as a healthy regularization against over-fitting.
In our setting, a small sample size may not satisfy the assumptions of the theoretical results, but it
proves to be practically useful. In Table 1 (left), we report the test accuracy attained by different
training algorithms at the epoch where validation accuracy peaks. One sees that Adam and standard
SGD achieves similar accuracies, and that SGD with consistent gradient sometimes surpasses these
accuracies. For Cora, a sample size 400 already yields an accuracy noticeably higher than do Adam
and standard SGD.
8
Under review as a conference paper at ICLR 2020
Table 1: Test accuracy (in percentage) and epoch number (inside parentheses) for different GCN
architectures and training algorithms. For the same architecture, initialization is the same. The
epoch number is the one when best validation accuracy occurs.
Uayer GCN__________________________2-layer GCN
	Mixture	Cora	Pubmed	Mixture	Cora	Pubmed
SGD (400)	78.0 (68)	85.8 (97)	86.2 (15)	86.7 (76)	87.1 (34)	87.5 (88)
SGD (800)	77.8 (46)	86.1 (86)	87.9 (68)	86.9 (87)	85.8 (13)	87.6 (87)
SGD (1600)	77.9 (87)	-	88.6 (35)	86.8 (94)	-	88.3 (85)
SGD (3200)	-	-	88.9 (98)	-	-	88.1 (88)
SGD unbiased	78.1 (93)	84.2 (87)	88.1 (75)	86.8 (66)	87.4 (27)	87.9 (90)
Adam unbiased	80.0 (95)	84.9 (21)	88.4 (20)	87.6 (94)	87.0 (04)	88.0 (06)
5.3	Nonconvex Case
When GCN has intermediate layers, the loss function is generally nonconvex. A 2-layer GCN reads
P = Softmax(A ∙ ReLU(AXW(0)) ∙ W(1)), and a GCN with more layers is analogous.
We repeat the experiments in the preceding subsection. The results are reported in Figure 2 and Ta-
ble 1 (right). The observation of the loss curve follows the same as that in the convex case. Namely,
Adam converges faster than does unbiased SGD; and the convergence curve with a consistent gradi-
ent approaches that with an unbiased one.
epoch ( k / num_batch )
(a) Mixture
epoch ( k / num_batch )
(b) Cora
1.2
1
0.8
S
£0.6
0.4
0.2
0
Figure 2: Convergence history for 2-layer GCN, under different training algorithms.
----SGD consistent (sampl 400)
----SGD consistent (sampl 800)
—SGD consistent (sampl 1600)
----SGD consistent (sampl 3200)
—SGD consistent (sampl n/2)
—SGD unbiased
----ADAM UnbiaSed________________
0	20	40	60	80	100
epoch ( k / num_batch )
(c) Pubmed
On the other hand, compared with 1-layer GCN, 2-layer GCN yields substantially higher test ac-
curacy for the data set Mixture, better accuracy for Cora, and very similar accuracy for Pubmed.
Within each data set, the performances of different training algorithms are on par. In particular, a
small sample size (e.g., 400) suffices for achieving results comparable to the state of the art (cf.
Chen et al. (2018)).
5.4	Probability Convergence
The nature of a consistent estimator necessitates a characterization of the speed of probability con-
vergence for building further results, such as the ones in this paper. The speed, however, depends on
the neural network architecture and it is out of the scope of this work to quantify it for a particular
use case. Nevertheless, for GCN we demonstrate empirical findings that agree with the exponential
tail assumption (6). In Figure 3 (solid curves), we plot the tail probability as a function of the sample
size N at different levels of estimator error δ, for the initial gradient step in 1-layer GCN. For each
N, 10,000 random gradient estimates were simulated for estimating the probability. Because the
probability is plotted in the logarithmic scale, the fact that the curves bend down indicates that the
convergence may be faster than exponential.
Additionally, the case of 2-layer GCN is demonstrated by the dashed curves in Figure 3. The curves
tend to be straight lines in the limit, which indicates an exponential convergence.
9
Under review as a conference paper at ICLR 2020
sample size N
Figure 3: Failure probability versus sample size at different levels of estimator error δ. Solid: 1-layer
GCN; dashed: 2-layer GCN.
6 Concluding Remarks
To the best of our knowledge, this is the first work that studies the convergence behavior of SGD
with consistent gradient estimators, and one among few studies of first-order methods that employ
biased (d’Aspremont, 2008; Schmidt et al., 2011) or noisy (Friedlander & Schmidt, 2012; Devolder
et al., 2014; Ge et al., 2015) estimators. The motivation originates from learning with large graphs
and the main message is that the convergence behavior is well-maintained with respect to the unbi-
ased case. While we analyze the classic SGD update formula, this work points to several immediate
extensions. One direction is the design of more efficient update formulas resembling the variance
reduction technique for unbiased estimators (Johnson & Zhang, 2013; Defazio et al., 2014; Bottou
et al., 2016). Another direction is the development of more computation- and memory-efficient
training algorithms for neural networks for large graphs. GCN is only one member of a broad fam-
ily of message passing neural networks (Gilmer et al., 2017) that suffer from the same limitation
of neighborhood aggregation. Learning in these cases inevitably faces the costly computation of
the sample gradient. Hence, a consistent estimator appears to be a promising alternative, whose
construction is awaiting more innovative proposals.
We are grateful to an anonymous reviewer who inspired us of an interesting use case (other than
GCN). Learning to rank is a machine learning application that constructs ranking models for in-
formation retrieval systems. In representative methods such as RankNet (Burges et al., 2005) and
subsequent improvements (Burges et al., 2007; Burges, 2010), si is the ranking function for docu-
ment i and the learning amounts to minimizing the loss
X sj - si + log(1 + esi-sj ),
(i,j)
where the summation ranges over all pairs of documents such that i is ranked higher than j . The
pairwise information may be organized as a graph and the loss function may be similarly generalized
as a double integral analogous to (4). Because of nonlinearity, Monte Carlo sampling of each integral
will result in a biased but consistent estimator. Therefore, a new training algorithm is to sample i
and j separately (forming a consistent gradient) and apply SGD. The theory developed in this work
offers guarantees of training convergence.
References
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. arXiv:1606.04838v3, 2016.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In ICLR, 2014.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hul-
lender. Learning to rank using gradient descent. In ICML, 2005.
10
Under review as a conference paper at ICLR 2020
Christopher J. Burges, Robert Ragno, and Quoc V. Le. Learning to rank with nonsmooth cost
functions. In NIPS, 2007.
Christopher J.C. Burges. From RankNet to LambdaRank to LambdaMART: An overview. Technical
Report MSR-TR-2010-82, Microsoft Research, 2010.
Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks
via importance sampling. In ICLR, 2018.
Alexandre d’Aspremont. Smooth optimization with approximate gradient. SIAM J. Optim., 19(3):
1171-1183, 2008.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In NIPS, 2014.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In NIPS, 2016.
Olivier Devolder, Francois Glineur, and Yurii Nesterov. First-order methods of smooth convex
optimization with inexact oracle. Mathematical Programming, 146(1-2):37-75, 2014.
Michael P. Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.
SIAM J. Sci. Comput., 34(3):A1380-A1405, 2012.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points — online stochastic
gradient for tensor decomposition. In COLT, 2015.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex
stochastic programming. SIAM J. Optim., 23(4):2341-2368, 2013.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In ICML, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, 2010.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NIPS, 2017.
Tito Homem-de-Mello. On rates of convergence for stochastic optimization problems under nonin-
dependent and identically distributed sampling. SIAM J. Optim., 19(2):524-551, 2008.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In NIPS, 2013.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an O(1/t)
convergence rate for the projected stochastic subgradient method. arXiv:1212.2002v2,
2012.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. In ICLR, 2016.
Ji Liu. CSC 576: Stochastic gradient “descent” algorithm. https://www.cs.rochester.
edu/u/jliu/CSC-576/class-note-10.pdf, 2015.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM J. Optim., 19(4):1574-1609, 2009.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In ICML, 2016.
11
Under review as a conference paper at ICLR 2020
Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann. Math. Statist., 22(3):
400-407, 1951.
Mark Schmidt, Nicolas L. Roux, and Francis R. Bach. Convergence rates of inexact proximal-
gradient methods for convex optimization. In NIPS, 2011.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
A Proofs
A.1 Lemmas
Here are a few lemmas needed for the proofs in sUbseqUent sUbsections.
Lemma 8. Projection is nonexpanding, i.e.,
kΠS (w) - ΠS (u)k ≤ kw - uk,	∀w,u ∈ Rd.
Proof. Let w0 = ΠS (w) and u0 = ΠS(u). By the convexity of S, we have
hw - w0, u0 - w0i ≤ 0 and hu - u0, w0 - u0i ≤ 0.
SUmming these two ineqUalities, we obtain hw - u, w0 - u0i ≥ hw0 - u0, w0 - u0i. Then, by
CaUchy-Schwarz,
kw0 - u0 k2 ≤ hw - u, w0 - u0i ≤ kw - ukkw0 - u0 k,
which concludes the proof.	□
Lemma 9. If f is l-strongly convex, then
hhu, U — w*i ≥ l∣∣u — w*∣∣2, ∀U ∈ Rd and hu ∈ ∂f (u).
Proof. Applying Definition 3 twice
f(w*) — f(U) ≥ hhu,w* — Ui + ；||w* — uk2
f(u) — f(w*) ≥	+2ku — w*k2,
and summing these two inequalities, we conclude the proof.	□
Lemma 10. For any w ∈ S,
kwk+1 — wk2 ≤ kwk — wk2 — 2γkhgk,wk — wi + γk2kgkk2.
Proof. It is straightforward to verify that
kwk+1 — wk2 = kΠS (wk — γkgk) — wk2
≤ kwk — γkgk — wk2
= kwk — wk2 — 2γkhgk,wk — wi + γk2kgkk2,
where the inequality results from Lemma 8.	□
Lemma 11. If kgk — hkk ≤ δkhkk, then
(1 — δ)khkk ≤ kgkk ≤ (1 + δ)khkk,
and
—2(Ilhkk2 + Ilwk — w* k2) ≤ hgk — hk,wk — w*i ≤ 2 (Ilhk k2 + Ilwk — w* k2).
12
Under review as a conference paper at ICLR 2020
Proof. For the first displayed inequality, it is straightforward to verify the upper bound
kgkk ≤ khkk+kgk-hkk ≤(1+δ)khkk,
and similarly the lower bound. For the second displayed inequality, CaUchy-SchWarz leads to the
upper bound
δ
hgk - hk,wk - w i ≤ Ilgk - hkk ∙ Ilwk - w k ≤ δ∣∣hkk ∙ ∣∣wk - w k≤ 2(|Ihk∣∣2 + Ilwk - W ∣∣2).
The lower bound is similarly proved.	□
A.2 Proof of Theorem 1
By the weak law of large numbers, Bt(v) → B(v) in probability for any v, where the probability
space is with respect to u. Then, q(Bt(v)) → q(B(v)) in probability by the continuous mapping the-
orem. Applying the law of large numbers again, now for v on a separate probability space different
from that of u, we conclude that Gst → G in probability.
A.3 Proof of Theorem 2, Inequality (8)
Applying Lemma 10 with W = w*, we have
∣∣Wk+ι - w*k2 ≤ l∣wk - w*k2 - 2γkhhk,wk - w*i - 2γkhgk - hk,wk - w*i + YkIlgk∣∣2.
Applying Lemma 9 with u = wk and Lemma 11, we have
Iwk+1 - w* I2 ≤ Iwk - w* I2 - 2γk lIwk - w* I2 + γkδ(Ihk I2 + Iwk - w* I2) + γk2 (1 + δ)2 Ihk I2
= (1 - 2γkl + γkδ)Iwk - w* I2 + (γkδ + γk2 (1 + δ)2)G2.	(15)
In what follows, we show by induction on k that
Iwk - w* I2 ≤
-(1 + δ)2 + ɪ
_(l - δ)2k + l - δ
G2.
Then, setting k = T we can conclude the proof.
First, in the base case when k = 1, by assumption we have
Iwk - w*∣2 ≤ G2 ≤	G2 ≤
l2	(l - δ)2
-(1 + δ)2 + ɪ
_(l - δ)2 + l - δ
G2.
Then, in the induction step, taking γk = [(l - δ)k]-1 as defined in the theorem on (15) and using
the induction hypothesis, we have
Iwk+1 - w* I2 ≤
lk - δk - 2l + δ
(l - δ)k
-(1 + δ)2 + ɪ
_(l - δ)2k + l - δ
G2 +
.δ + (1 + δ)2 ] g2
(l - δ)k + (l - δ)2k2J
=(lk - δk - 2l + δ)(1 +	δ)2	g2	+ (lk - δk -	2l	+ δ)δg2	+ δ	g2	+ (1 + δ)2	G2
=	(l	- δ)3k2	+	(l	- δ)2k	+ (l -	δ)k	+ (l -	δ)2k2
≤ (k - 2)(1 + δ)2 g2 + δ(k -1) g2 + δ	g2 + (1 + δ)2 G2
一	(l	-	δ)2k2	+	(l	- δ)k	+ (l -	δ)k	+ (l -	δ)2k2	.
For the right-hand side, combine the first and the fourth term, and the second and the third term, we
obtain
Iwk+1 - w*I2 ≤
(k -1)(1 + δ)2 g2 + δ	g2 ≤	(1 + δ)2	g2 + δ	g2
(l - δ)2k2	+ (l - δ)	— (l - δ)2(k + 1)	+ (l - δ)
which completes the induction step.
13
Under review as a conference paper at ICLR 2020
A.4 Proof of Theorem 2, Inequality (9)
Applying Lemma 10 with W = w*, we have
∣∣Wk+ι - w*k2 ≤ Ilwk - w*k2 - 2γkhhk,Wk - w*i - 2γkhgk - hk,Wk - w*i + YkIlgk||2.
Applying the definition of strong convexity and Lemma 11, we have
Iwk+1 - w* I2 ≤ Iwk - w* I2 - 2γk [f(wk) - f (w*)] - γklIwk - w* I2
+ γkδ(Ihk I2 + Iwk - w* I2) + γk2(1 + δ)2 Ihk I2
= -2γk [f(wk) - f(w*)] + (1 - γkl + γkδ)Iwk - w* I2 + (γkδ + γk2 (1 + δ)2)G2.
Rearranging, we have
2[f (wk) - f(w*)] ≤ (γk-1 - l + δ)Iwk - w* I2 - γk-1 Iwk+1 - w* I2 + (δ + γk(1 + δ)2)G2.
Noting that the step size γk = [(l - δ)k]-1, we have
2[f(wk)- f(w*)] ≤ (l- δ)(k - 1)∣wk- w*k2-(l- δ)k∣wk+ι - w*∣2 + G2 δ + (1 + ；；].
Summing from k = 1 to k = T and multiplying by 1/(2T), we have
1ʌ *( *n l - δ∣∣	*112 , G2 XT_L(1 + δ)2 Vλ 1
T Wf(wk) - f(W)] ≤ --2-kwτ+ι - Wk + 2T δT + ι - § Tk .
k=1	k=1
By the convexity of f and using the bound PT=I 1/k ≤ 1 + log T, and noting that δ = p/T, we
have
f(wτ) - f(w*) ≤ -~-2~kwτ +1 - w*k2 + 2T p + Ql-p/T) (I +logT) .
Relaxing the right-hand side through omitting the negative term, we thus conclude the proof.
A.5 Proof of Theorem 3
The L-smoothness property implies a second order condition for convex functions:
f(wk) - f(w*) ≤ LIwk - w*k2.
Then, applying (8) with k = T, we conclude the proof.
A.6 Proof of Theorem 4
We reuse (15) in the proof of inequality (8) in Theorem 2:
Iwk+1 - w* I2 ≤ (1 - 2γkl + γk δ)Iwk - w* I2 + (γkδ + γk2 (1 + δ)2)G2 .
Applying step size γk = c, we have
Iwk+1 - w* I2 ≤ (1 - 2cl + cδ)Iwk - w* I2 + (cδ + c2(1 + δ)2)G2 .
Unrolling recursion with respect to k, we have
k-1
Iwk+1 - w*I2 ≤ (1 -2cl+cδ)kIw1 - w*I2 + (cδ+c2(1 + δ)2) X(1 - 2cl + cδ)iG2.
i=0
Because 0 < 1 - 2cl + cδ < 1 by assumption, we have
k-1
X(1 -2cl+cδ)i <
i=0
1
2cl — cδ
which concludes the proof.
14
Under review as a conference paper at ICLR 2020
A.7 Proof of Theorem 5
Applying Lemma 10 with W = w*, we have
∣∣Wk+ι - w*k2 ≤ Ilwk - w*k2 - 2γkhhk,Wk - w*i - 2γkhgk - hk,Wk - w*i + YkIlgk||2.
Applying a property of convex functions and Lemma 11, we have
Iwk+1 - w* I2 ≤ Iwk - w* I2 - 2γk [f(wk) - f (w*)] + γkδ(Ihk I2 + Iwk - w* I2) + γk2 (1 + δ)2 Ihk I2
= -2γk [f(wk) - f(w*)] + (1 + γk δ)Iwk - w* I2 + (γkδ + γk2 (1 + δ)2)G2.
Rearranging, we have
2[f (wk) - f (w*)] ≤ (γk-1 + δ)Iwk - w* I2 - γk-1 Iwk+1 - w* I2 + (δ + γk(1 + δ)2)G2.
Summing from k = 1 to k = T, relaxing the negative term -γT-1 IwT +1 - w* I2 on the right-hand
side, and multiplying by 1/(2T), we have
1 T	γ-1 + δ
T Ef(Wk) - f (w*)] ≤ ⅛+-∣W1 - w*k2
k=1
T	-1	-1	2	T
γk + δ - γk-1	* 2 G	2
+ X---------2T-------Ilwk - Wk + 2T δT + (1 + δ) X Yk
k=2	k=1
Applying IWk - W* I2 ≤ D2 for all k, we have
T XXX [f (Wk) - f (w*)] ≤(Y-1 +TT )D2 + GT2 δT +(1 + δ)2 XX Yk
k=1	k=1
Noting that Yk = c/ √k and δ = ρ/ √T, We have
1 T	1	1	G2
T ∑[f (Wk) - f(W )] ≤ 2√T(c + P)D +2T
By the convexity of f and using the bound PT=I 1 / √k ≤ √T + 1, we have
1	1	G2
f (WT)- f(W ) ≤ 2√τ(c + P)D +2T
P√T +
which concludes the proof.
A.8 Proof of Theorem 6
The L-smoothness property implies that
f(Wk+l) ≤ f (Wk ) + Wf(Wk),Wk+1 - Wk i + L ∣∣Wk+1 - Wk k2.
Noting that Wk+1 - Wk = -Ykgk (because S = Rd) and applying Lemma 11, we have
f(Wk+ι) ≤ f(Wk) - Ykhhk,gki + L*k2 ≤ f(Wk) - Yk(1 - δ)∣hkk2 + LYk(I+J2①k2
Rearranging, we have
IVf(Wk)I2 ≤ [Yk(1 - δ)]-1 [f (Wk) - f (Wk+ι)] + LLY :：+ δ)2G2 .
2(1 - δ)
Summing from k = 1 tok = T, multiplying by 1/T, and noting that Yk is constant, we have
min ∣∣Vf (Wk)k2 ≤ [Y1(1 - δ)]-1 [f (w1) - f (wt +1)] + LYI(11 + 牛。2 .
k	T	1	T +1	2(1	δ)	.
Because f (WT +1) ≥ f (w*) and Y1 = Df/[(1 + δ)G√T], we have
min ∣Vf (Wk)∣2 ≤ [Y1(1- δ)]-1 [f (W1) - f (w*)] + LYI(1 + *2 = (1 + δ)L√Df ,
k k f( k )k — T f( 1) f(	)]+	2(1 - δ)	(1 - δ)√T
which concludes the proof.
15
Under review as a conference paper at ICLR 2020
A.9 Proof of Theorem 7
Theorem 2.2 of Friedlander & Schmidt (2012) states that when the gradient error
kgk - hkk < δk for all k ≥ 1,	(16)
inequality (14) holds. It remains to show that the probability that (16) happens is at least 1 - .
The assumption on the sample size Nk means that
Cke-NkTa/khkk) ≤ e/T.
Then, substituting δk = δkhk k into assumption (6) yields
Pr (∖∖gk — hk k ≥ δk∣gι,...,gk-i) ≤ Ck e-Nk T (Sk/khkk) ≤ e/T.
Hence, the probability that (16) happens is
T
Y 1 - Pr ∖gk - hk ∖ ≥ δk g1, . . . , gk-1
k=1
T
≥ Y(1 - e/T) ≥ 1 - e,
k=1
which concludes the proof.
B Experiment Details
B.1 The “Mixture” Data Set
The data set is a Gaussian mixture with c = 3 components in d = 2 dimensions. The components
N(μi,σ2l) with μι = [-0.5,0], σι = 0.75, μ2 = [0.5,0], σ2 = 0.5, μ3 = [0,0.866], and
σ3 = 0.25 are equally weighted but significantly overlap with each other. Random connections are
made between every pair of points. For points in the same component, the probability that they are
connected is pintra = 1e-3; for points straddle across components, the probability is pinter = 2e-4.
See Figure 4(a) for an illustration of the Gaussian mixture and Figure 4(b) for the graph adjacency
matrix.
2.5
2
1.5
1
0.5
0
-0.5
-1
-1.5
-2
-2.5
-3-2-10123
(a) Overlapping Gaussians
0
1000
2000
3000
4000
5000
0	1000	2000	3000	4000	5000	6000
nz = 33418
(b) Adjacency matrix
Figure 4: The “Mixture” data set (input features and graph).
6000
B.2	Summary of Data Sets
See Table 2 for a summary of the data sets used in this work.
16
Under review as a conference paper at ICLR 2020
Table 2: Data sets.
	Mixture	Cora	Pubmed
# Nodes	6,000	2,708	19,717
# Edges	16,709	5,429	44,338
# Classes	3	7	3
# Features	2	1,433	500
# Training	2,400	1,208	18,217
# Validation	1,200	500	500
# Test	2,400	1,000	1,000
Table 3: Hyperparameters for different GCN architectures and training algorithms.
(a) 1-layer GCN
	Mixture	Cora	Pubmed
Batch size	-^256^^	256	256
Regularization	0	0	0
SGD learning rate	1e+0	1e+3	1e+3
Adam learning rate	1e-2	1e-1	1e-1
(b) 2-layer GCN
	Mixture	Cora	Pubmed
Batch size	-^256^^	256	256
Regularization	0	0	0
Hidden unit	16	16	16
SGD learning rate	1e+0	1e+2	1e+1
Adam learning rate	1e-2	1e-1	1e-1
B.3	(Hyper)Parameters
See Table 3 for the hyperparameters used in the experiments. For parameter initialization, we use
the Glorot uniform initializer (Glorot & Bengio, 2010).
B.4	Run Time
See Table 4 for the run time (per epoch). As expected, a smaller sample size is more computationally
efficient. SGD with consistent gradients runs faster than the standard SGD and Adam, both of which
admit approximately the same computational cost.
Table 4: Time per epoch in seconds.
1-layer GCN				2-layer GCN		
	Mixture	Cora	Pubmed	Mixture	Cora	Pubmed
SGD (400)	0.0035	0.0269	0.1991	0.0103	0.0868	2.5014
SGD (800)	0.0018	0.0455	0.3554	0.0103	0.0974	2.5684
SGD (1600)	0.0027	-	0.7129	0.0142	-	3.2032
SGD (3200)	-	-	1.1847	-	-	3.8895
SGD unbiased	0.0044	0.0737	2.2425	0.0130	0.2031	7.9478
Adam unbiased	0.0049	0.0741	2.2313	0.0143	0.2080	7.9037
17