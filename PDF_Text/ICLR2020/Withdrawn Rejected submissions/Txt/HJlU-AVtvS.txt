Under review as a conference paper at ICLR 2020
A Fine-Grained Spectral Perspective on
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Are neural networks biased toward simple functions? Does depth always help learn
more complex features? Is training the last layer of a network as good as training
all layers? How to set the range for learning rate tuning? These questions seem
unrelated at face value, but in this work we give all of them a common treatment
from the spectral perspective. We will study the spectra of the Conjugate Kernel,
CK, (also called the Neural Network-Gaussian Process Kernel), and the Neural
Tangent Kernel, NTK. Roughly, the CK and the NTK tell us respectively “what
a network looks like at initialization” and “what a network looks like during and
after training.” Their spectra then encode valuable information about the initial
distribution and the training and generalization properties of neural networks. By
analyzing the eigenvalues, we lend novel insights into the questions put forth at
the beginning, and we verify these insights by extensive experiments of neural
networks. We believe the computational tools we develop here for analyzing the
spectra of CK and NTK serve as a solid foundation for future studies of deep neural
networks. We have open-sourced the code for it and for generating the plots in this
paper at github.com/jxVmnLgedVwv6mNcGCBy/NNspectra.
1	Introduction
Understanding the behavior of neural networks and why they generalize has been a central pursuit of
the theoretical deep learning community. Recently, Valle-Perez et al. (2018) observed that neural
networks have a certain “simplicity bias” and proposed this as a solution to the generalization question.
One of the ways with which they argued that this bias exists is the following experiment: they drew
a large sample of boolean functions by randomly initializing neural networks and thresholding
the output. They observed that there is a bias toward some "simple" functions which get sampled
disproportionately more often. However, their experiments were only done for relu networks. Can
one expect this “simplicity bias” to hold universally, for any architecture?
A priori, this seems difficult, as the nonlinear nature seems to present an obstacle in reasoning about
the distribution of random networks. However, this question turns out to be more easily treated if
we allow the width to go to infinity. A long line of works starting with Neal (1995) and extended
recently by Lee et al. (2018); Novak et al. (2018); Yang (2019) have shown that randomly initialized,
infinite-width networks are distributed as Gaussian processes. These Gaussian processes also describe
finite width random networks well (Valle-Perez et al., 2018). We will refer to the corresponding
kernels as the Conjugate Kernels (CK), following the terminology of Daniely et al. (2016). Given the
CK K, the simplicity bias of a wide neural network can be read off quickly from the spectrum of K :
If the largest eigenvalue of K accounts for most of tr K, then a typical random network looks like a
function from the top eigenspace of K .
In this paper, we will use this spectral perspective to probe not only the simplicity bias, but more
generally, questions regarding how hyperparameters affect the generalization of neural networks.
Via the usual connection between Gaussian processes and linear models with features, the CK can be
thought of as the kernel matrix associated to training only the last layer of a wide randomly initialized
network. It is a remarkable recent advance (Jacot et al., 2018; Allen-Zhu et al., 2018a;c; Du et al.,
2018) that, under a certain regime, a wide neural network of any depth evolves like a linear model
even when training all parameters. The associated kernel is call the Neural Tangent Kernel, which
is typically different from CK. While its theory was initially derived in the infinite width setting,
Lee et al. (2019) confirmed with extensive experiment that this limit is predictive of finite width
neural networks as well. Thus, just as the CK reveals information about what a network looks like at
1
Under review as a conference paper at ICLR 2020
initialization, NTK reveals information about what a network looks like after training. As such, if
we can understand how hyperparameters change the NTK, we can also hope to understand how they
affect the performance of the corresponding finite-width network.
Our Contributions In this paper, in addition to showing that the simplicity bias is not universal,
we will attempt a first step at understanding the effects of the hyperparameters on generalization from
a spectral perspective.
At the foundation is a spectral theory of the CK and the NTK on the boolean cube. In Section 3, we
show that these kernels, as integral operators on functions over the boolean cube, are diagonalized by
the natural Fourier basis, echoing similar results for over the sphere (Smola et al., 2001). We also
partially diagonalize the kernels over standard Gaussian, and show that, as expected, the kernels over
the different distributions (boolean cube, sphere, standard Gaussian) behave very similarly in high
dimensions. However, the spectrum is much easier to compute over the boolean cube: while the
sphere and Gaussian eigenvalues would require integration against a kind of polynomials known as
the Gegenbauer polynomials, the boolean ones only require calculating a linear combination of a
small number of terms. For this reason, in the rest of the paper we focus on analyzing the eigenvalues
over the boolean cube.
Just as the usual Fourier basis over R has a notion of frequency that can be interpreted as a measure
of complexity, so does the boolean Fourier basis (this is just the degree; see Section 3.1). While not
perfect, we adopt this natural notion of complexity in this work; a “simple” function is then one that
is well approximated by “low frequencies.”
This spectral perspective immediately yields that the simplicity bias is not universal (Section 4).
In particular, while it seems to hold more or less for relu networks, for sigmoidal networks, the
simplicity bias can be made arbitrarily weak by changing the weight variance and the depth. In the
extreme case, the random function obtained from sampling a deep erf network with large weights is
distributed like a “white noise.” However, there is a very weak sense in which the simplicity bias does
hold: the eigenvalues of more “complex” eigenspaces cannot be bigger than those of less “complex”
eigenspaces (Thm 4.1).
Next, we examine how hyperparameters affect the performance of neural networks through the lens
of NTK and its spectrum. To do so, we first need to understand the simpler question of how a
kernel affects the accuracy of the function learned by kernel regression. A coarse-grained theory,
concerned with big-O asymptotics, exists from classical kernel literature (Yao et al., 2007; Raskutti
et al., 2013; Wei et al.; Lin and Rosasco; ScholkoPf and Smola, 2002). However, the fine-grained
details, required for discerning the effect of hyperparameters, have been much less studied. We
make a first attemPt at a heuristic, fractional variance (i.e. what fraction of the trace of the kernel
does an eigensPace contribute), for understanding how a minute change in kernel effects a change in
Performance. Intuitively, if an eigensPace has very large fractional variance, so that it accounts for
most of the trace, then a ground truth function from this eigensPace should be very easy to learn.
Using this heuristic, we make two Predictions about neural networks, motivated by observations in
the sPectra of NTK and CK, and verify them with extensive exPeriments.
•	DeePer networks learn more comPlex features, but excess dePth can be detrimental as well.
SPectrally, dePth can increase fractional variance of an eigensPace, but Past an optimal
depth, it will also decrease it. (Section 5) Thus, deePer is not always better.
•	Training all layers is better than training just the last layer when it comes to more comPlex
features, but the oPPosite is true for simPler features. SPectrally, fractional variances of
more “comPlex” eigensPaces for the NTK are larger than the correPonding quantities of the
CK. (Section 6)
Finally, we use our sPectral theory to Predict the maximal nondiverging learning rate (“max learning
rate”) of SGD (Section 7).
In general, we will not only verify our theory with exPeriments on the theoretically interesting
distributions, i.e. uniform measures over the boolean cube and the sPhere, or the standard Gaussian,
but also confirm these findings on real data like MNIST and CIFAR10 1.
1The code for comPuting the eigenvalues and for reProducing the Plots of this PaPer is available at github.
com/jxVmnLgedVwv6mNcGCBy/NNspectra, which will be oPen sourced uPon Publication.
2
Under review as a conference paper at ICLR 2020
For space concerns, we review relevant literature along the flow of the main text, and relegate a more
complete discussion of the related research landscape in Appendix A.
2	Kernels Associated to Neural Networks
As mentioned in the introduction, we now know several kernels associated to infinite width, randomly
initialized neural networks. The most prominent of these are the neural tangent kernel (NTK) (Jacot
et al., 2018) and the conjugate kernel (CK) (Daniely et al., 2016), which is also called the NNGP
kernel (Lee et al., 2018). We briefly review them below. First we introduce the following notation
that we will repeatedly use.
Definition 2.1. For φ : R → R, write Vφ for the function that takes a PSD (positive semidefinite)
kernel function to a PSD kernel of the same domain by the formula
Vφ(K)(x, x0) = E	φ(f (x))φ(f (x0)).
'	f 〜N (0,K)
Conjugate Kernel Neural networks are commonly thought of as learning a high-quality embedding
of inputs to the latent space represented by the network’s last hidden layer, and then using its final
linear layer to read out a classification given the embedding. The conjugate kernel is just the kernel
associated to the embedding induced by a random initialization of the neural network. Consider an
MLP with widths {nl}l, weight matrices {Wl ∈ Rnl ×nl-1 }l, and biases {bl ∈ Rnl }l, l = 1, . . . , L.
For simplicity of exposition, in this paper, we will only consider scalar output nL = 1. Suppose it is
parametrized by the NTK parametrization, i.e. its computation is given recursively as
h1(x) = σw- W 1x + σbb1 and hl(x) = T Wl φ(hl-1(x)) + σbbl	(MLP)
n0	nl-1
with some hyperparameters σw, σb that are fixed throughout training2. At initialization time, suppose
Wαlβ,blα 〜N(0,1) for each α ∈ [nl], β ∈ [nl-1]. It can be shown that, for each α ∈ [nl], ha is a
Gaussian process with zero mean and kernel function Σl in the limit as all hidden layers become
infinitely wide (nl → ∞, l = 1, . . . , L - 1), where Σl is defined inductively on l as
Σ1(x, x0) d=ef σw2 (n0)-1hx,x0i + σb2,	Σl d=efσw2Vφ(Σl-1) +σb2	(CK)
The kernel ΣL corresponding the the last layer L is the network’s conjugate kernel, and the associated
Gaussian process limit is the reason for its alternative name Neural Network-Gaussian process kernel.
In short, if we were to train a linear model with features given by the embedding x 7→ hL-1(x) when
the network parameters are randomly sampled as above, then the CK is the kernel of this linear model.
See Daniely et al. (2016); Lee et al. (2018) and Appendix F for more details.
Neural Tangent Kernel On the other hand, the NTK corresponds to training the entire model
instead of just the last layer. Intuitively, if we let θ be the entire set of parameters {Wl }l ∪ {bl }l of
Eq. (MLP), then for θ close to its initialized value θ0, we expect
hL(x; θ) - hL(x; θo) ≈ "θhL(x; θo),θ - θoi
via a naive first-order Taylor expansion. In other words, hL (x; θ) - hL (x; θ0) behaves like a linear
model with feature of X given by the gradient taken w.r.t. the initial network, Vθ hL (x; θ0), and the
weights of this linear model are the deviation θ - θ0 of θ from its initial value. It turns out that, in the
limit as all hidden layer widths tend to infinity, this intuition is correct (Jacot et al., 2018; Lee et al.,
2018; Yang, 2019), and the following inductive formula computes the corresponding infinite-width
kernel of this linear model:
Θ1 d=ef Σ1, Θl (x, x0) d=ef Σl (x, x0) +σw2Θl-1(x,x0)Vφ0(Σl-1)(x,x0).	(NTK)
Computing CK and NTK While in general, computing Vφ and Vφ0 requires evaluating a multi-
variate Gaussian expectation, in specific cases, such as when φ = relu or erf, there exists explicit,
efficient formulas that only require pointwise evaluation of some simple functions (see Facts F.1
and F.2). This allows us to evaluate CK and NTK on a set X of inputs in only time O(|X |2L).
2SGD with learning rate α in this parametrization is roughly equivalent to SGD with learning rate α/width
in the standard parametrization with Glorot initialization; see Lee et al. (2018)
3
Under review as a conference paper at ICLR 2020
What Do the Spectra of CK and NTK Tell Us? In summary, the CK governs the distribution
of a randomly initialized neural netWork and also the properties of training only the last layer of a
netWork, While the NTK governs the dynamics of training (all parameters of) a neural netWork. A
study of their spectra thus informs us of the “implicit prior” of a randomly initialized neural netWork
as Well as the “implicit bias” of GD in the context of training neural netWorks.
In regards to the implicit prior at initialization, We knoW from Lee et al. (2018) that a randomly
initialized netWork as in Eq. (MLP) is distributed as a Gaussian process N(0, K), Where K is the
corresponding CK, in the infinite-Width limit. If We have the eigendecomposition
K =): λiu 0 Ui
i≥1
(1)
with eigenvalues λi in decreasing order and corresponding eigenfunctions ui , then each sample from
this GP can be obtained as
E vzλiωiUi, ωi 〜N(0, 1).
i≥1
If, for example, λ1 Pi≥2λi, then a typical sample function is just a very small perturbation of u1.
We Will see that for relu, this is indeed the case (Section 4), and this explains the “simplicity bias” in
relu networks found by Vane-Perez et al. (2018).
Training the last layer of a randomly initialized netWork via full batch gradient descent for an infinite
amount of time corresponds to Gaussian process inference with kernel K (Lee et al., 2018; 2019). A
similar intuition holds for NTK: training all parameters of the network (Eq. (MLP)) for an infinite
amount of time yields the mean prediction of the GP N(0, NTK) in expectation; see Lee et al. (2019)
and Appendix F.4 for more discussion.
Thus, the more the GP prior (governed by the CK or the NTK) is consistent with the ground truth
function f *, the more we expect the Gaussian process inference and GD training to generalize well.
We can measure this consistency in the “alignment” between the eigenvalues λi and the squared
coefficients a2 of f *,s expansion in the {ui}i basis. The former can be interpreted as the expected
magnitude (squared) of the Ui-component of a sample f 〜N(0, K), and the latter can be interpreted
as the actual magnitude squared of such component of f *. In this paper, we will investigate an even
cleaner setting where f * = Ui is an eigenfunction. Thus we would hope to use a kernel whose ith
eigenvalue λi is as large as possible.
Neural Kernels From the forms of the equation Eqs. (CK) and (NTK) and the fact that
Vφ(K)(x, x0) only depends on K(x, x), K(x, x0), and K(x0, x0), We see that CK or NTK of MLPs
takes the form
RV ʌ φ/ hx,yi Ilxk2 kyk2
K(x,y) = φ h≡FF
(2)
for some function Φ : R3 → R. We will refer to this kind of kernel as Neural Kernel in this paper.
Kernels as Integral Operators We Will consider input spaces of various forms X ⊆ Rd equipped
With some probability measure. Then a kernel function K acts as an integral operator on functions
f∈L2(X)by
Kf(x) = (Kf)(x) = E K(x,y)f(y).
y〜X
We Will use the “juxtaposition syntax” Kf to denote this application of the integral operator. 3
Under certain assumptions, it then makes sense to speak of the eigenvalues and eigenfunctions
of the integral operator K . While We Will appeal to an intuitive understanding of eigenvalues
and eigenfunctions in the main text beloW, We include a more formal discussion of Hilbert-Schmidt
operators and their spectral theory in Appendix G for completeness. In the next section, We investigate
the eigendecomposition of neural kernels as integral operators over different distributions.
3In cases when X is finite, K can be also thought of as a big matrix and f as a vector — but do not confuse
Kf With their multiplication! If We use ∙ to denote matrix multiplication, then the operator application Kf is
the same as the matrix multiplication K ∙ D ∙ f where D is the diagonal matrix encoding the probability values
of each point in X .
4
Under review as a conference paper at ICLR 2020
3 The S pectra of Neural Kernels
3.1 B oolean Cube
We first consider a neural kernel K on the boolean cube X = dd= =f {±1}d, equipped with the uniform
measure. In this case, since each x ∈ X has the same norm, K(x, y)
hhχ,y
kxk2, kf)
effectively only depends on hx, yi, so we will treat Φ as a single variate function in this section,
Φ(c) = Φ(c, 1, 1).
Brief review of basic Fourier analysis on the boolean cube CSd (O'Donnell (2014)). The space
of real functions on ㈤d forms a 2d-dimensional space. Any such function has a unique expansion into
a multilinear polynomial (polynomials whose monomials do not contain xip , p ≥ 2, of any variable
xi). For example, the majority function over 3 bits has the following unique multilinear expansion
maj3 : IXP → ㈤1, maj3(xι, x2, x3) = (χXχ+ + x2 + x3 - xιx2x3).
In the language of Fourier analysis, the 2d multilinear monomial functions
χS(x)d=ef xS d=ef Y xi, for each S ⊆ [d]	(3)
i∈S
form a Fourier basis of the function space L2 (㈤d) = {f :㈤d → R}, in the sense that their inner
products satisfy
E χS(x)χT (x) = I(S = T).
X 〜Isd
Thus, any function f :㈤d → R can be always written as
f (X) = X f(S)Xx(X)
S⊆[d]
for a unique set of coefficients {f (S)}S ⊆[d] .
It turns out that K is always diagonalized by this Fourier basis {χS}S⊆[d] .
Theorem 3.1. On the d-dimensional boolean cube Sd ,for every S ⊆ [d], XS is an eigenfunction of
K with eigenvalue
μlSl =f E xSK(x, 1) = E xSΦ
I I	x∈ ㈤d	x∈ ㈤d
X xi/d ,
(4)
where 1 = (1,..., 1) ∈ Sd. This definition of μ∣S∣ does not depend on the choice S, only on the
cardinality of S. These are all of the eigenfunctions of K by dimensionality considerations.4
Define Tδ to be the shift operator on functions over [-1,1] that sends Φ(∙) to Φ(∙ - ∆). Then we
can re-express the eigenvalue as follows.
Lemma 3.2. With μk as in Thm 3.1,
(5)
where
μk = 2-d(I-T∆)k (I + TAYI-φ(1)
(6)
Cd-k,k d=ef
Cr	=
(7)
Φ
Eq. (5) will be important for computational purposes, and we will come back to discuss this more in
Section 3.5. It also turns out μk affords a pretty expression via the Fourier series coefficients of Φ.
As this is not essential to the main text, we relegate its exposition to Appendix H.1.
4Readers familiar with boolean Fourier analysis may be reminded of the noise operator Tρ, ρ ≤ 1 (O’Donnell,
2014, Defn 2.46). In the language of this work, TP is a neural kernel with eigenvalues μk = ρk.
5
Under review as a conference paper at ICLR 2020
3.2	Sphere
Now let's consider the case when X = √dSd-1 is the radius-√d sphere in Rd equipped with the
uniform measure. Again, because x ∈ X all have the same norm, we will treat Φ as a univariate
function with K(x, y) = Φ(hx, yi/kxkkyk) = Φ(hx, yi/d). As is long known (Schoenberg, 1942;
Gneiting, 2013; Xu and Cheney, 1992; Smola et al., 2001), K is diagonalized by spherical harmonics,
and the eigenvalues are given by the coefficients of Φ against a system of orthogonal polynomials
called Gegenbuaer polynomials. We relegate a complete review of this topic to Appendix H.2.
3.3	Isotropic Gaussian
Now let’s consider X = Rd equipped with standard isotropic Gaussian N (0, I), so that K behaves
like
Kf(x)
E	K(x, y)f(y) = E Φ
y 〜N (0,I)	y 〜N (0,I)
(hx,y	kxk2
Uxkkyk, d
f(y)
for any f ∈ L2(N(0,I)). In contrast to the previous two sections, K will essentially depend on the
effect of the norms kxk and kyk on Φ.
Nevertheless, because an isotropic Gaussian vector can be obtained by sampling its direction uni-
formly from the sphere and its magnitude from a chi distribution, K can still be partially diagonalized
into a sum of products between spherical harmonics and kernels on R equipped with a chi distribution
(Thm H.14). In certain cases, we can obtain complete eigendecompositions, for example when Φ is
positive homogeneous. See Appendix H.3 for more details.
3.4	KERNEL IS SAME OVER BOOLEAN CUBE, SPHERE, OR GAUSSIAN WHEN d 1
The reason we have curtailed a detailed discussion of neural kernels on the sphere and on the standard
Gaussian is because, in high dimension, the kernel behaves the same under these distributions as under
uniform distribution over the boolean cube. Indeed, by intuition along the lines of the central limit
theorem, we expect that uniform distribution over a high dimension boolean cube should approximate
high dimensional standard Gaussian. Similarly, by concentration of measure, most of the mass of a
Gaussian is concentrated around a thin shell of radius √d. Thus, morally, We expect the same kernel
function K induces approximately the same integral operator on these three distributions in high
dimension, and as such, their eigenvalues should also approximately coincide. We verify empirically
and theoretically this is indeed the case in Appendix H.4.
3.5	Computing the Eigenvalues
As the eigenvalues of K over the different distributions are very close, we will focus in the rest of
this paper on eigenvalues over the boolean cube. This has the additional benefit of being much easier
to compute.
Each eigenvalue over the sphere and the standard Gaussian requires an integration of Φ against a
Gegenbauer polynomial. In high dimension d, these Gegenbauer polynomials varies wildly in a
sinusoidal fashion, and blows up toward the boundary (see Fig. 15 in the Appendix). As such, it is
difficult to obtain a numerically stable estimate of this integral in an efficient manner when d is large.
In contrast, we have multiple ways of computing boolean cube eigenvalues, via Eqs. (5) and (6). In
either case, we just take some linear combination of the values of Φ at a grid of points on [-1, 1],
spaced apart by ∆ = 2/d. While the coefficients Crd-k,k (defined in Eq. (7)) are relatively efficient
to compute, the change in the sign of Crd-k,k makes this procedure numerically unstable for large d.
Instead, we use Eq. (5) to isolate the alternating part to evaluate in a numerically stable way: Since
μk = ( i+Tδ ) d k (JTA) k Φ(1) ,we can evaluate Φ C=f (I-Tδ ) k Φ via k finite differences, and then
compute
(V)
d-k
Φ⑴
(8)
6
Under review as a conference paper at ICLR 2020
erf networks lose simplicity bias for large σ⅛ and depth
⅛=qeqo.Jd
probability vs rank of 104 random network on {ii}7
IOT
≠ l<⅞l<⅞ I depth
× relu I 2 I 2 I 2
erf∣l∣0∣2
o erf∣2∣0∣2
• erf∣4∣0∣2
* erf∣4∣0∣32
01234567
deg reek
Figure 1: The "simplicity bias" is not so simple. (a) Following Valle-Perez et al. (2018), We
sample 104 boolean functions {±1}7 → {±1} as follows: for each combination of nonlinearity,
weight variance σw2 , and bias variance σb2 (as used in Eq. (MLP)), we randomly initialize a network
of 2 hidden layers, 40 neurons each. Then we threshold the function output to a boolean output,
and obtain a boolean function sample. We repeat this for 104 random seeds to obtain all samples.
Then we sort the samples according to their empirical probability (this is the x-axis, rank), and plot
their empirical probability (this is the y-axis, probability). The high values at the left of the relu
curve indicates that a few functions get sampled repeatedly, while this is less true for erf. For erf
and σw2 = 4, no function got sampled more than once. (b) For different combination of nonlinearity,
σw2 , σb2, and depth, we study the eigenvalues of the corresponding CK. Each CK has 8 different
eigenvalues μ0,...,μ7 corresponding to homogeneous polynomials of degree 0,..., 7. We plot them
in log scale against the degree. Note that for erf and σb = 0, the even degree μk vanish. See main
text for explanations.
When Φ arises from the CK or the NTK ofan MLP, all derivatives of Φ at 0 are nonnegative (Thm I.3).
Thus intuitively, the finite difference Φ should be also all nonnegative, and this sum can be evaluated
without worry about floating point errors from cancellation of large terms.
A slightly more clever way to improve the numerical stability when 2k ≤ d is to note that
(I + T∆)d-k (I - T∆)k Φ(1) = (I + T∆)d-2k (I - T∆)k Φ(1) = (I + T∆)d-2k (I - T2∆)k Φ(1).
So an improved algorithm is to first compute the kth finite difference (I - T2∆)k with the larger step
size 2∆, then compute the sum (I + T∆)d-2k as in Eq. (8).
4 Clarifying the “Simplicity Bias” of Random Neural Networks
As mentioned in the introduction, Valle-Perez et al. (2018) claims that neural networks are biased
toward simple functions. We show that this phenomenon depends crucially on the nonlinearity, the
sampling variances, and the depth of the network. In Fig. 1(a), we have repeated their experiment for
104 random functions obtained by sampling relu neural networks with 2 hidden layers, 40 neurons
each, following Valle-Perez et al. (2018)’s architectural choices5 *. We also do the same for erf
networks of the same depth and width, varying as well the sampling variances of the weights and
biases, as shown in the legend. As discussed in Valle-Perez et al. (2018), for relu, there is indeed
this bias, where a single function gets sampled more than 10% of the time. However, for erf, as we
increase σw2 , we see this bias disappear, and every function in the sample gets sampled only once.
This phenomenon can be explained by looking at the eigendecomposition of the CK, which is
the Gaussian process kernel of the distribution of the random networks as their hidden widths
tend to infinity. In Fig. 1(b), we plot the normalized eigenvalues {μk/ P7=Q (7)μi}k=o for the
CKs corresponding to the networks sampled in Fig. 1(a). Immediately, we see that for relu and
σw2 = σb2 = 2, the degree 0 eigenspace, corresponding to constant functions, accounts for more than
80% of the variance. This means that a typical infinite-width relu network of 2 layers is expected to
be almost constant, and this should be even more true after we threshold the network to be a boolean
function. On the other hand, for erf and σb = 0, the even degree μk S all vanish, and most of the
variance comes from degree 1 components (i.e. linear functions). This concentration in degree 1
also lessens as σw2 increases. But because this variance is spread across a dimension 7 eigenspace,
we don’t see duplicate function samples nearly as much as in the relu case. As σw increases, we
also see the eigenvalues become more equally distributed, which corresponds to the flattening of
5Valle-Perez et al. (2018) actually performed their experiments over the {0, 1}7 cube, not the {±1}7 cube
we are using here. This does not affect our conclusion. See Appendix J for more discussion
7
Under review as a conference paper at ICLR 2020
-0.07
-0.06
-0.05
-0.04
-0.03
-0.02
-0.01
-0.00
120
100
80
60
40
20
0
(d)
-0.200
-0.175
α>
-0.150 c
-0.125,∣
-0.100®
-0.075 5
Tj
-0.050 £
-0.025
-0.000
Figure 2: The depth maximizing degree k fractional variance increases with k for both relu
and erf. For relu (a) and erf (b), we plot for each degree k the depth such that there exists some
combination of other hyperparameters (such as σb2 or σw2 ) that maximizes the degree k fractional
variance. For both relu, σb2 = 0 maximizes fractional variance in general, and same holds for erf
in the odd degrees (see Appendix D), so we take a closer look at this slice by plotting heatmaps of
fractional variance of various degrees versus depth for relu (c) and erf (d) NTK, with bright colors
representing high variance. Clearly, we see the brightest region of each column, corresponding to a
fixed degree, moves up as we increase the degree, barring for the even/odd degree alternating pattern
for erf NTK. The pattern for CKs are similar and their plots are omitted.
the probability-vs-rank curve in Fig. 1(a). Finally, we observe that a 32-layer erf network with
σw2 = 4 has all its nonzero eigenvalues (associated to odd degrees) all equal (see points marked by
* in Fig. 1(b)). This means that its distribution is a "white noise" on the space of odd functions,
and the distribution of boolean functions obtained by thresholding the Gaussian process samples is
the uniform distribution on odd functions. This is the complete lack of simplicity bias modulo the
oddness constraint.
However, from the spectral perspective, there is a weak sense in which a simplicity bias holds for all
neural network-induced CKs and NTKs.
Theorem 4.1 (Weak Spectral Simplicity Bias). Let K be the CK or NTK of an MLP on a boolean
cube ≤Pd. Then the eigenvalues μk, k = 0,...,d, satisfy
μo ≥ μ2 ≥ ∙∙∙ ≥ μ2k ≥ …，μι ≥ μ3 ≥ ∙∙∙ ≥ μ2k+1 ≥ ….	(9)
Even though it’s not true that the fraction of variance contributed by the degree k eigenspace is
decreasing with k, the eigenvalue themselves will be in a nonincreasing pattern across even and odd
degrees. In fact, if we fix k and let d → ∞, then we can show that (Thm I.6)
μk = Θ(d-k).
Of course, as we have seen, this is a very weak sense of simplicity bias, as it doesn’t prevent “white
noise” behavior as in the case of erf CK with large σw2 and large depth.
5 Deeper Networks Learn More Complex Features
In the rest of this work, We compute the eigenvalues μk over the 128-dimensional boolean cube (㈤d,
with d = 128) for a large number of different hyperparameters, and analyze how the latter affect the
former. We vary the degree k ∈ [0, 8], the nonlinearity between relu and erf, the depth (number of
hidden layers) from 1 to 128, and σb2 ∈ [0, 4]. We fix σw2 = 2 for relu kernels, but additionally vary
σw2 ∈ [1, 5] for erf kernels. Comprehensive contour plots of how these hyperparameters affect the
kernels are included in Appendix D, but in the main text we summarize several trends we see.
We will primarily measure the change in the spectrum by the degree k fractional variance, which is
just
def
degree k fractional variance =
(d)μk
Pd=o (d 加
This terminology comes from the fact that, if we were to sample a function f from a Gaussian process
with kernel K , then we expect that r% of the total variance of f comes from degree k components of
f , where r% is the degree k fractional variance. If we were to try to learn a homogeneous degree-k
8
Under review as a conference paper at ICLR 2020
Verifying best depths and NTK complexity bias, varying degree of ground truth
best loss and frac. var. are inversely related
i-0
gθ∙8
,∣O-6
^S
O 0.4
E
J 0.2
0.0
(a)
optimal depth increases with degree
Figure 3: (a) We train relu networks of different depths against a ground truth polynomial on ㈤128
of different degrees k . We either train only the last layer (marked “ck”) or all layers (marked “ntk”),
and plot the degree k fractional variance of the corresponding kernel against the best validation loss
over the course of training. We see that the best validation loss is in general inversely correlated with
fraction variance, as expected. However, their precise relationship seems to change depending on the
degree, or whether training all layers or just the last. See Appendix E for experimental details. (b)
Same experimental setting as (a), with slightly different hyperparameters, and plotting depth against
best validation loss (solid curves), as well as the corresponding kernel’s (1- fractional variance)
(dashed curves). We see that the loss-minimizing depth increases with the degree, as predicted by
Fig. 2. Note that we do not expect the dashed and solid curves to match up, just that they are positively
correlated as shown by (a). In higher degrees, the losses are high across all depths, and the variance is
large, so we omit them. See Appendix E for experimental details. (c) Similar experimental setting as
(a), but with more hyperparameters, and now comparing training last layer vs training all layers. The
color of each dot indicates the degree of the ground truth polynomial. Below the identity line, training
all layers is better than training last layer. We see that the only nontrivial case where this is not true
is when learning degree 0 polynomials, i.e. constant functions. See Appendix E for experimental
details. We also replicate (b) for MNIST and CIFAR10, and moreover both (b) and (c) over the input
distributions of standard Gaussian and the uniform measure over the sphere. See Figs. 6 to 8.
polynomial using a kernel K, intuitively we should try to choose K such that its μk is maximized,
relative to other eigenvalues. Fig. 3(a) shows that this is indeed the case even with neural networks:
over a large number of different hyperparameter settings, degree k fractional variance is inversely
related to the validation loss incurred when learning a degree k polynomial. However, this plot also
shows that there does not seem like a precise, clean relationship between fractional variance and
validation loss. Obtaining a better measure for predicting generalization is left for future work.
Before we continue, we remark that the fractional variance of a fixed degree k converges to a fixed
value as the input dimension d → ∞:
Theorem 5.1 (Asymptotic Fractional Variance). Let K be the CK or NTK of an MLP on a boolean
cube ≤Pd. Then K can be expressed as K(x,y) = Φ(<x, y)/d) forsome analyticfunction Φ : R → R.
If we fix k and let the input dimension d → ∞, then the fractional variance of degree k converges to
(k!)Tφ^(0)∕Φ(1)
(k!)-1Φ(k)(0)
Pj≥oj!)-1 Φ⑶(0)
where Φ(k) denotes the kth derivative of Φ.
For the fractional variances we compute in this paper, their values at d = 128 are already very close
to their d → ∞ limit, so we focus on the d = 128 case experimentally.
If K were to be the CK or NTK of a relu or erf MLP, then we find that for higher k, the depth of
the network helps increase the degree k fractional variance. In Fig. 2(a) and (b), we plot, for each
degree k, the depth that (with some combination of other hyperparameters like σb2) achieves this
maximum, for respectively relu and erf kernels. Clearly, the maximizing depths are increasing with k
for relu, and also for erf when considering either odd k or even k only. The slightly differing behavior
between even and odd k is expected, as seen in the form of Thm 4.1. Note the different scales of
y-axes for relu and erf — the depth effect is much stronger for erf than relu.
For relu NTK and CK, σb2 = 0 maximizes fractional variance in general, and the same holds for
erf NTK and CK in the odd degrees (see Appendix D). In Fig. 2(c) and Fig. 2(d) we give a more
fine-grained look at the σb2 = 0 slice, via heatmaps of fractional variance against degree and depth.
9
Under review as a conference paper at ICLR 2020
ntk favors higher degrees compared to ck
Figure 4: Across nonlinearities and hyperparameters, NTK tends to have higher fraction of
variance attributed to higher degrees than CK. In (a), we give several examples of the fractional
variance curves for relu CK and NTK across several representative hyperparameters. In (b), we do the
same for erf CK and NTK. In both cases, we clearly see that, while for degree 0 or 1, the fractional
variance is typically higher for CK, the reverse is true for larger degrees. In (c), for each degree k, we
plot the fraction of hyperparameters where the degree k fractional variance of NTK is greater than
that of CK. Consistent with previous observations, this fraction increases with the degree.
Brighter color indicates higher variance, and we see the optimal depth for each degree k clearly
increases with k for relu NTK, and likewise for odd degrees of erf NTK. However, note that as
k increases, the difference between the maximal fractional variance and those slightly suboptimal
becomes smaller and smaller, reflected by suppressed range of color moving to the right. The
heatmaps for relu and erf CKs look similar and are omitted.
We verify this increase of optimal depth with degree in Fig. 3(b). There we have trained relu networks
of varying depth against a ground truth multilinear polynomial of varying degree. We see clearly
that the optimal depth is increasing with degree. We also verify this phenomenon When the input
distribution changes to the standard GaUSSian or the uniform distribution over the sphere √dSd-1;
see Fig. 6.
Note that implicit in our results here is a highly nontrivial observation: Past some point (the optimal
depth), high depth can be detrimental to the performance of the network, beyond just the difficulty to
train, and this detriment can already be seen in the corresponding NTK or CK. In particular, it’s not
true that the optimal depth is infinite. We confirm the existence of such an optimal depth even in real
distributions like MNIST and CIFAR10; see Fig. 7. This adds significant nuance to the folk wisdom
that “depth increases expressivity and allows neural networks to learn more complex features.”
6	NTK Favors More Complex Features Than CK
We generally find the degree k fractional variance of NTK to be higher than that of CK when k is
large, and vice versa when k is small, as shown in Fig. 4. This means that, if we train only the last
layer of a neural network (i.e. CK dynamics), we intuitively should expect to learn simpler features
better, while, if we train all parameters of the network (i.e. NTK dynamics), we should expect to
learn more complex features better. Similarly, if we were to sample a function from a Gaussian
process with the CK as kernel (recall this is just the distribution of randomly initialized infinite width
MLPs (Lee et al., 2018)), this function is more likely to be accurately approximated by low degree
polynomials than the same with the NTK.
We verify this intuition by training a large number of neural networks against ground truth functions
of various homogeneous polynomials of different degrees, and show a scatterplot of how training the
last layer only measures against training all layers (Fig. 3(c)). This phenomenon remains true over the
standard Gaussian or the uniform distribution on the sphere (Fig. 8). Consistent with our theory, the
only place training the last layer works meaningfully better than training all layers is when the ground
truth is a constant function. However, we reiterate that fractional variance is an imperfect indicator of
performance. Even though for erf neural networks and k ≥ 1, degree k fractional variance of NTK is
not always greater than that of the CK, we do not see any instance where training the last layer of an
erf network is better than training all layers. We leave an investigation of this discrepancy to future
work.
10
Under review as a conference paper at ICLR 2020
7	Predicting the Maximum Learning Rate
In any setup that tries to push deep learning benchmarks, learning rate tuning is a painful but
indispensable part. In this section, we show that our spectral theory can accurately predict the
maximal nondiverging learning rate over real datasets as well as toy input distributions, which would
help set the correct upper limit for a learning rate search.
By Jacot et al. (2018), in the limit of large width and infinite data, the function g : X → R represented
by our neural network evolves like
gt+1= gt- 2αK(gt - g*), t = 0,1, 2,...,	(10)
when trained under full batch GD (with the entire population) with L2 loss L(f, g) = Ex〜X (f (x) —
g(x))2, ground truth g*, and learning rate a, starting from randomly initialization. If we train only the
last layer, then K is the CK; if we train all layers, then K is the NTK. Given an eigendecomposition
of K as in Eq. (1), if g0 - g* = Pi aiui is the decomposition ofg0 in the eigenbasis {ui}i, then one
can easily deduce that
gt-g*=	ai(1- 2αλi)tui.
Consequently, we must have α < (maxi λi)-1 in order for Eq. (10) to converge 6
When the input distribution is the uniform distribution over ㈤d, the maximum learning rate is
max(μo, μι) by Thm 4.1. By Thm 5.1, as long as the Φ function corresonding to K has Φ(0) = 0,
when d is large, We expect μo ≈ Φ(0) but μι 〜d-1Φ0(O)《μo. Therefore, We should predict φ^
for the maximal learning rate when training on the boolean cube. However, as Fig. 5 shows, this
prediction is accurate not only for the boolean cube, but also over the sphere, the standard Gaussian,
and even MNIST and CIFAR10!
8	Conclusion
In this work, we have taken a first step at studying how hyperparameters change the initial distribution
and the generalization properties of neural networks through the lens of neural kernels and their
spectra. We obtained interesting insights by computing kernel eigenvalues over the boolean cube
and relating them to generalization through the fractional variance heuristic. While it inspired valid
predictions that are backed up by experiments, fractional variance is clearly just a rough indicator.
We hope future work can refine on this idea to produce a much more precise prediction of test loss.
Nevertheless, we believe the spectral perspective is the right line of research that will not only shed
light on mysteries in deep learning but also inform design choices in practice.
References
Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards Characterizing Divergence in Deep
Q-Learning. arXiv:1903.08894 [cs], March 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and Generalization in Overparameterized
Neural Networks, Going Beyond Two Layers. arXiv:1811.04918 [cs, math, stat], November 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the Convergence Rate of Training Recurrent
Neural Networks. arXiv:1810.12065 [cs, math, stat], October 2018b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning via
Over-Parameterization. arXiv:1811.03962 [cs, math, stat], November 2018c.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
Exact Computation with an Infinitely Wide Neural Net. arXiv:1904.11955 [cs, stat], April 2019a.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-Grained Analysis of
Optimization and Generalization for Overparameterized Two-Layer Neural Networks. January
2019b.
6Note that this is the max learning rate of the infinite-width neural network evolving in the NTK regime, but
not necessarily the max learning rate of the finite-wdith neural network, as a larger learning rate just means that
the network no longer evolves in the NTK regime.
11
Under review as a conference paper at ICLR 2020
Boolean cube theory predicts max learning rate for real
max Ir: training the last layer (CK)
dist
• MNlST
• CIFAR10
• Gaussian
• Sphere
• BooICube
上 XeUJ -eu∙J□aJ03w
dist
MNlST
CIFAR10
Gaussian
Sphere
BooiCube
max Ir: training all layers (NTK)
10-1	IO0
empirical max Ir
10-1	IO0	IO1
empirical max Ir
(b)10^2
1O1
datasets
ιo1
10-2
(C)
max Ir: training all layers (NTK)
10≡Io-1
上 XeUJ -eu∙4□aJ03w
10-1	IO0	IO1
empirical max Ir
Figure 5: Spectral theory of CK and NTK over boolean cube predicts max learning rate for
SGD over real datasets MNIST and CIFAR10 as well as over boolean cube ㈤128, the sphere
√128S128-1, and the standard Gaussian N(0, I128). In all three plots, for different depth, nonlin-
earity, σw2 , σb2 of the MLP, we obtain its maximal nondiverging learning rate (“max learning rate”)
via binary search. We center and normalize each image of MNIST and CIFAR10 to the √dSd-1
sphere, where d = 282 = 784 for MNIST and d = 3 × 322 = 3072 for CIFAR10. See Appendix E.2
for more details. (a) We empirically find max learning rate for training only the last layer of an
MLP. Theoretically, We predict 1 /Φ(0) where Φ corresponds to the CK of the MLP. We see that our
theoretical prediction is highly accurate. Note that the Gaussian and Sphere points in the scatter plot
coincide with and hide behind the BoolCube points. (b) and (c) We empirically find max learning
rate for training all layers. Theoretically, we predict 1∕Φ(0) where Φ corresponds to the NTK of the
MLP. The points are identical between (b) and (c), but the color coding is different. Note that the
Gaussian points in the scatter plots coincide with and hide behind the Sphere points. In (b) we see
that our theoretical prediction when training all layers is not as accurate as when we train only the last
layer, but it is still highly correlated with the empirical max learning rate. It in general underpredicts,
so that half of the theoretical learning rate should always have SGD converge. This is expected,
since the NTK limit of training dynamics is only exact in the large width limit, and larger learning
rate just means the training dynamics diverges from the NTK regime, but not necessarily that the
training diverges. In (c), we see that deeper networks tend to accept higher learning rate than our
theoretical prediction. If we were to preprocess MNIST and CIFAR10 differently, then our theory is
less accurate at predicting the max learning rate; see Fig. 9 for more details.
12
Under review as a conference paper at ICLR 2020
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory, 39(3):930-945, May 1993. ISSN 0018-9448. doi: 10.1109/
18.256500.
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman. The Convergence Rate of Neural
Networks for Learned Functions of Different Frequencies. arXiv:1906.00425 [cs, eess, stat], June
2019.
Agata Bezubik, Agata DabrOWska, and Aleksander Strasburger. On spherical expansions of zonal
functions on Euclidean spheres. Archiv der Mathematik, 90(1):70-81, January 2008. ISSN
0003-889X, 1420-8938. doi: 10.1007/s00013-007-2308-y.
Alberto Bietti and Julien Mairal. On the Inductive Bias of Neural Tangent Kernels. arXiv:1905.12173
[cs, stat], May 2019.
Kenneth Blomqvist, Samuel Kaski, and Markus Heinonen. Deep convolutional Gaussian processes.
arXiv preprint arXiv:1810.03052, 2018.
Anastasia Borovykh. A gaussian process perspective on convolutional neural netWorks. arXiv
preprint arXiv:1810.10798, 2018.
John BradshaW, Alexander G de G MattheWs, and Zoubin Ghahramani. Adversarial examples,
uncertainty, and transfer testing robustness in gaussian process hybrid deep netWorks. arXiv
preprint arXiv:1707.02476, 2017.
Emmanuel J. Candes. Harmonic Analysis OfNeural Networks. Applied and Computational Harmonic
Analysis, 6(2):197-218, March 1999. ISSN 1063-5203. doi: 10.1006/acha.1998.0248.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical Isometry and a Mean
Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks. In
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 873-882, Stockholmsmassan, Stockholm Sweden, July
2018. PMLR.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in Neural
Information Processing Systems, pages 342-350, 2009.
Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial Intelligence and
Statistics, pages 207-215, 2013.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks:
The Power of Initialization and a Dual View on Expressivity. arXiv:1602.05897 [cs, stat], February
2016.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient Descent Provably Optimizes
Over-parameterized Neural Networks. arXiv:1810.02054 [cs, math, stat], October 2018.
Ronen Eldan and Ohad Shamir. The Power of Depth for Feedforward Neural Networks. In Conference
on Learning Theory, pages 907-940, June 2016.
Adri鱼 Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep Convolutional
Networks as shallow Gaussian Processes. arXiv:1808.05587 [cs, stat], August 2018.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. arXiv:1904.12191 [cs, math, stat], April 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of
Machine Learning Research, pages 249-256, Chia Laguna Resort, Sardinia, Italy, May 2010.
PMLR. 02641.
Tilmann Gneiting. Strictly and non-strictly positive definite functions on spheres. Bernoulli, 19(4):
1327-1349, September 2013. ISSN 1350-7265. doi: 10.3150/12-BEJSP06.
13
Under review as a conference paper at ICLR 2020
Boris Hanin. Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?
January 2018.
Boris Hanin and David Rolnick. How to Start Training: The Effect of Initialization and Architecture.
arXiv:1803.01719 [cs, stat], March 2018.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the Selection of Initialization and
Activation Function for Deep Neural Networks. arXiv:1805.08266 [cs, stat], May 2018.
Tamir Hazan and Tommi Jaakkola. Steps Toward Deep Kernel Methods from Infinite Neural
Networks. arXiv:1508.05133 [cs], August 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE International
Conference on Computer Vision, pages 1026-1034, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. 00000.
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal Statistics of Fisher Information in
Deep Neural Networks: Mean Field Approach. arXiv:1806.01316 [cond-mat, stat], June 2018.
Yitzhak Katznelson. An Introduction to Harmonic Analysis. Cambridge Mathematical Library.
Cambridge University Press, Cambridge, UK ; New York, 3rd ed edition, 2004. ISBN 978-0-521-
83829-0 978-0-521-54359-0.
Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep Gaussian Processes with
Convolutional Kernels. arXiv preprint arXiv:1806.01655, 2018.
Neil D Lawrence and Andrew J Moore. Hierarchical Gaussian process latent variable models. In
Proceedings of the 24th International Conference on Machine Learning, pages 481-488. ACM,
2007.
Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In Artificial Intelligence and
Statistics, pages 404-411, 2007.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha
Sohl-dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on
Learning Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient
Descent. arXiv:1902.06720 [cs, stat], February 2019.
Junhong Lin and Lorenzo Rosasco. Optimal Rates for Multi-pass Stochastic Gradient Methods.
page 47.
Alexander\ G. \ de\ G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian Process Behaviour in Wide Deep Neural Networks. In International Conference
on Learning Representations, April 2018.
Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD Thesis, University of
Toronto, 1995.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Bayesian Deep Convolutional Networks with Many Channels are
Gaussian Processes. arXiv preprint arXiv:1810.05148, 2018.
Ryan O’Donnell. Analysis of Boolean Functions. Cambridge University Press, New York, NY, 2014.
ISBN 978-1-107-03832-5.
Jeffrey Pennington and Yasaman Bahri. Geometry of Neural Network Loss Surfaces via Random
Matrix Theory. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages
2798-2806, International Convention Centre, Sydney, Australia, August 2017. PMLR. 00006.
14
Under review as a conference paper at ICLR 2020
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In Advances
in Neural Information Processing Systems, pages 2634-2643, 20l7. 00000.
Jeffrey Pennington and Pratik Worah. The Spectrum of the Fisher Information Matrix of a Single-
Hidden-Layer Neural Network. In Advances in Neural Information Processing Systems 31, page 10,
2018.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: Theory and practice. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
Processing Systems 30, pages 4788-4798. Curran Associates, Inc., 2017a. 00004.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: Theory and practice. arXiv:1711.04735 [cs, stat], November
2017b. 00005.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The Emergence of Spectral Univer-
sality in Deep Networks. arXiv:1802.09979 [cs, stat], February 2018.
George Philipp and Jaime G. Carbonell. The Nonlinearity Coefficient - Predicting Overfitting in
Deep Neural Networks. arXiv:1806.00179 [cs, stat], May 2018. 00000.
Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Ex-
ponential expressivity in deep neural networks through transient chaos. In Advances In Neural
Information Processing Systems, pages 3360-3368, 2016. 00047.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht,
Yoshua Bengio, and Aaron Courville. On the Spectral Bias of Neural Networks. arXiv:1806.08734
[cs, stat], June 2018.
Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Early stopping and non-parametric regression:
An optimal data-dependent stopping rule. arXiv:1306.3574 [stat], June 2013.
I. J. Schoenberg. Positive definite functions on spheres. Duke Mathematical Journal, 9(1):96-108,
March 1942. ISSN 0012-7094, 1547-7398. doi: 10.1215/S0012-7094-42-00908-6.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information
Propagation. 2017.
Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. Adaptive Computation and Machine Learning. MIT
Press, Cambridge, Mass, 2002. ISBN 978-0-262-19475-4.
Alex J. Smola, Zoltdn L. Ov^ri, and Robert C Williamson. Regularization with Dot-Product Kernels.
In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems 13, pages 308-314. MIT Press, 2001.
Sho Sonoda and Noboru Murata. Neural Network with Unbounded Activation Functions is Universal
Approximator. Applied and Computational Harmonic Analysis, 43(2):233-268, September 2017.
ISSN 10635203. doi: 10.1016/j.acha.2015.12.005.
P.K. Suetin.	Ultraspherical polynomials - Encyclopedia of Mathematics.
https://www.encyclopediaofmath.org/index.php/Ultraspherical_polynomials.
Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. arXiv:1501.01571 [cs, math,
stat], January 2015.
Guillermo Valle-PCrez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because
the parameter-function map is biased towards simple functions. arXiv:1805.08522 [cs, stat], May
2018.
Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional Gaussian Processes.
In Advances in Neural Information Processing Systems 30, pages 2849-2858, 2017.
Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms: A
general analysis with localized complexities. page 11.
15
Under review as a conference paper at ICLR 2020
Christopher K I Williams. Computing with Infinite Networks. In Advances in Neural Information
Processing Systems, page 7, 1997.
Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic Variational
Deep Kernel Learning. In Advances in Neural Information Processing Systems, pages 2586-2594,
2016a.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial Intelligence and Statistics, pages 370-378, 2016b.
Lechao Xiao, Yasaman Bahri, Sam Schoenholz, and Jeffrey Pennington. Training ultra-deep CNNs
with critical initialization. In NIPS Workshop, 2017.
Bo Xie, Yingyu Liang, and Le Song. Diverse Neural Network Learns True Target Functions.
arXiv:1611.03131 [cs, stat], November 2016.
Yuan Xu and E. W. Cheney. Strictly Positive Definite Functions on Spheres. Proceedings of the
American Mathematical Society, 116(4):977-981, 1992. ISSN 0002-9939. doi: 10.2307/2159477.
Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in
frequency domain. arXiv:1807.01251 [cs, math, stat], July 2018.
Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency Principle:
Fourier Analysis Sheds Light on Deep Neural Networks. arXiv:1901.06523 [cs, stat], January
2019.
Zhiqin John Xu. Understanding training and generalization in deep learning by Fourier analysis.
arXiv:1808.04295 [cs, math, stat], August 2018.
Greg Yang. Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process
Behavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv:1902.04760
[cond-mat, physics:math-ph, stat], February 2019.
Greg Yang and Sam S. Schoenholz. Deep Mean Field Theory: Layerwise Variance and Width
Variation as Methods to Control Gradient Explosion. February 2018.
Greg Yang and Samuel S. Schoenholz. Mean Field Residual Network: On the Edge of Chaos. In
Advances in Neural Information Processing Systems, 2017.
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A
Mean Field Theory of Batch Normalization. arXiv:1902.08129 [cond-mat], February 2019.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On Early Stopping in Gradient Descent
Learning. Constructive Approximation, 26(2):289-315, August 2007. ISSN 0176-4276, 1432-
0940. doi: 10.1007/s00365-006-0663-2.
Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. Explicitizing an Implicit Bias of the
Frequency Principle in Two-layer Neural Networks. arXiv:1905.10264 [cs, stat], May 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic Gradient Descent Optimizes
Over-parameterized Deep ReLU Networks. arXiv:1811.08888 [cs, math, stat], November 2018.
16
Under review as a conference paper at ICLR 2020
A Related Works
The Gaussian process behavior of neural networks was found by Neal (1995) for shallow networks
and then extended over the years to different settings and architectures (Williams, 1997; Le Roux
and Bengio, 2007; Hazan and Jaakkola, 2015; Daniely et al., 2016; Lee et al., 2018; Matthews et al.,
2018; Novak et al., 2018). This connection was exploited implicitly or explicitly to build new models
(Cho and Saul, 2009; Lawrence and Moore, 2007; Damianou and Lawrence, 2013; Wilson et al.,
2016a;b; Bradshaw et al., 2017; van der Wilk et al., 2017; Kumar et al., 2018; Blomqvist et al., 2018;
Borovykh, 2018; Garriga-Alonso et al., 2018; Novak et al., 2018; Lee et al., 2018). The Neural
Tangent Kernel is a much more recent discovery by Jacot et al. (2018) and later Allen-Zhu et al.
(2018a;c;b); Du et al. (2018); Arora et al. (2019b); Zou et al. (2018) came upon the same reasoning
independently. Like CK, NTK has also been applied toward building new models or algorithms
(Arora et al., 2019a; Achiam et al., 2019).
Closely related to the discussion of CK and NTK is the signal propagation literature, which tries to
understand how to prevent pathological behaviors in randomly initialized neural networks when they
are deep (Poole et al., 2016; Schoenholz et al., 2017; Yang and Schoenholz, 2017; 2018; Hanin, 2018;
Hanin and Rolnick, 2018; Chen et al., 2018; Yang et al., 2019; Pennington et al., 2017a; Hayou et al.,
2018; Philipp and Carbonell, 2018). This line of work can trace its original at least to the advent
of the Glorot and He initialization schemes for deep networks (Glorot and Bengio, 2010; He et al.,
2015). The investigation of forward signal propagation, or how random neural networks change
with depth, corresponds to studying the infinite-depth limit of CK, and the investigation of backward
signal propagation, or how gradients of random networks change with depth, corresponds to studying
the infinite-depth limit of NTK. Some of the quite remarkable results from this literature includes
how to train a 10,000 layer CNN (Xiao et al., 2017) and that, counterintuitively, batch normalization
causes gradient explosion (Yang et al., 2019).
This signal propagation perspective can be refined via random matrix theory (Pennington et al., 2017a;
2018). In these works, free probability is leveraged to compute the singular value distribution of
the input-output map given by the random neural network, as the input dimension and width tend
to infinity together. Other works also investigate various questions of neural network training and
generalization from the random matrix perspective (Pennington and Worah, 2017; Pennington and
Bahri, 2017; Pennington and Worah, 2018).
Yang (2019) presents a common framework, known as Tensor Programs, unifying the GP, NTK,
signal propagation, and random matrix perspectives, as well as extending them to new scenarios, like
recurrent neural networks. It proves the existence of and allows the computation of a large number of
infinite-width limits (including ones relevant to the above perspectives) by expressing the quantity of
interest as the output of a computation graph and then manipulating the graph mechanically.
Several other works also adopt a spectral perspective on neural networks (Candis, 1999; Sonoda
and Murata, 2017; Eldan and Shamir, 2016; Barron, 1993; Xu et al., 2018; Zhang et al., 2019; Xu
et al., 2019; Xu, 2018); here we highlight a few most relevant to us. Rahaman et al. (2018) studies the
real Fourier frequencies of relu networks and perform experiments on real data as well as synthetic
ones. They convincingly show that relu networks learn low frequencies components first. They also
investigate the subtleties when the data manifold is low dimensional and embedded in various ways
in the input space. In contrast, our work focuses on the spectra of the CK and NTK (which indirectly
informs the Fourier frequencies of a typical network). Nevertheless, our results are complementary to
theirs, as they readily explain the low frequency bias in relu that they found. Karakida et al. (2018)
studies the spectrum of the Fisher information matrix, which share the nonzero eigenvalues with
the NTK. They compute the mean, variance, and maximum of the eigenvalues Fisher eigenvalues
(taking the width to infinity first, and then considering finite amount of data sampled iid from a
Gaussian). In comparison, our spectral results yield all eigenvalues of the NTK (and thus also all
nonzero eigenvalues of the Fisher) as well as eigenfunctions.
Finally, we note that several recent works (Xie et al., 2016; Bietti and Mairal, 2019; Basri et al., 2019;
Ghorbani et al., 2019) studied one-hidden layer neural networks over the sphere, building on Smola
et al. (2001)’s observation that spherical harmonics diagonalize dot product kernels, with the latter
two concurrent to us. This is in contrast to the focus on boolean cube here, which allows us to study
the fine-grained effect of hyperparameters on the spectra, leading to a variety of insights into neural
networks’ generalization properties.
17
Under review as a conference paper at ICLR 2020
B Universality of Our B oolean Cube Observations in Other Input
Distributions
Using the spectral theory we developed in this paper, we made three observations, that can be roughly
summarized as follows: 1) the simplicity bias noted by Vane-Perez et al. (2018) is not universal;
2) for each function of fixed “complexity” there is an optimal depth such that networks shallower
or deeper will not learn it as well; 3) training last layer only is better than training all layers when
learning “simpler” features, and the opposite is true for learning “complex” features.
In this section, We discuss the applicability of these observations to distributions that are not uniform
over the boolean cube: in particular, the uniform distribution over the sphere √dSd-1, the standard
Gaussian N(0, Id ), as well as realistic data distributions such as MNIST and CIFAR10.
Simplicity bias The simplicity bias noted by Valle-Perez et al. (2018), in particular Fig. 1, depends
on the finiteness of the boolean cube as a domain, so we cannot effectively test this on the distributions
above, which all have uncountable support.
Optimal depth With regard to the second observation, we can test whether an optimal depth exists
for learning functions over the distributions above. Since polynomial degrees remain the natural
indicator of complexity for the sphere and the Gaussian (see Appendices H.2 and H.3 for the relevant
spectral theory), we replicated the experiment in Fig. 3(b) for these distributions, using the same
ground truth functions of polynomials of different degrees. The results are shown in Fig. 6. We see
the same phenomenon as in the boolean cube case, with an optimal depth for each degree, and with
the optimal depth increasing with degree.
For MNIST and CIFAR10, the notion of “feature complexity” is less clear, so we will not test the
hypothesis that “optimal depth increases with degree” for these distributions but only test for the
existence of the optimal depth for the ground truth marked by the labels of the datasets. We do so by
training a large number of MLPs of varying depth on these datasets until convergence, and plot the
results in Fig. 7. This figure clearly shows that such an optimal depth exists, such that shallower or
deeper networks do monotonically worse as the depth diverge away from this optimal depth.
Again, the existence of the optimal depth is not obvious at all, as conventional deep learning wisdom
would have one believe that adding depth should always help.
Training last layer only vs training all layers Finally, we repeat the experiment in Fig. 3(c) for
the sphere and the standard Gaussian, with polynomials of different degrees as ground truth functions.
The results are shown in Fig. 8. We see the same phenomenon as in the boolean cube case: for degree
0 polynomials, training last layer works better in general, but for higher degree polynomials, training
all layers fares better.
Note that, unlike the sphere and the Gaussian, whose spectral theory tells us that (harmonic) polyno-
mial degree is a natural notion of complexity, for MNIST and CIFAR10 we have much less clear idea
of what a “complex” or a “simple” feature is. Therefore, we did not attempt a similar experiment on
these datasets.
C Theoretical vs Empirical Max Learning Rates under Different
Preprocessing for MNIST and CIFAR 1 0
In the main text Fig. 5, on the MNIST and CIFAR10 datasets, we preprocessed the data by centering
and normalizing to the sphere (see Appendix E.2 for a precise description). With this preprocessing,
our theory accurately predicts the max learning rate in practice.
In general, if we go by another preprocessing, such as PCA or ZCA, or no preprocessing, our
theoretical max learning rate 1∕Φ(0) is less accurate but still correlated in general. The only
exception seems to be relu networks on PCA- or ZCA- preprocessed CIFAR10. See Fig. 9.
18
Under review as a conference paper at ICLR 2020
ss°⅛3q
optimal depths exist over standard Gaussian and sphere too
1.0
2	4	6	8	10
depth
Figure 6: Optimal depths exist over the standard Gaussian N (0, Id) and the uniform distribu-
tion over the sphere √dSd-1 as well. Here We use the exact same experimental setup as Fig. 3(b)
(see Appendix E for details) except that the input distribution is changed from uniform over the
boolean cube ≤Pd to standard Gaussian N(0, Id) (solid lines) and uniform over the sphere √dSd-1
(dashed lines), Where d = 128. We also compare against the results over the boolean cube from
Fig. 3(b), Which are draWn With dotted lines. Colors indicate the degrees of the ground truth poly-
nomial functions. The best validation loss for degree 0 to 2 are all very close no matter Which
distribution the input is sampled from, such that the curves all sit on top of each other. For degree 3,
there is less precise agreement betWeen the validation loss over the different distributions, but the
overall trend is unmistakably the same. We see that for netWorks deeper or shalloWer than the optimal
depth, the loss monotonically increases as the depth moves aWay from the optimum.
optimal depths exist for MNIST and CIFAR10
Q 。 8 6
6 6 5 5
±3⅛3⅛3q
OlyVU.D
1510
±3⅛3⅛3q
H∞-ZΣ
Figure 7: Optimal depths exist over realistic distributions of MNIST and CIFAR10. Here, we
trained relu networks with σw2 = 2, σb2 = 0 for all depths from 0 to 10. We used SGD with learning
rate 10 and batch size 256, and trained until convergence. We record the best test error throughout the
training procedure for each depth. For each configuration, we repeat the randomly initialization and
training for 10 random seeds to estimate the variance of the best test error. The rows demonstrate the
best test error over the course of training on CIFAR10 and MNIST, and the columns demonstrate
the same for training only the last layer or training all layers. As one can see, the best depth when
training only the last layer is 1, for both CIFAR10 and MNIST. The best depth when training all
layers is around 5 for both CIFAR10 and MNIST. Performance monotically decreases for networks
shallower or deeper than the optimal depth. Note that we have reached the SOTA accuracy for MLPs
reported in Lee et al. (2018) on CIFAR10, and within 1 point of their accuracy on MNIST.
19
Under review as a conference paper at ICLR 2020
training last Iayervs training all layers
ω 0.8
ω
A
ra
≡ 0.6
ro
S
° 0.4
ra
>
5 0.2
0.0
gaussιan
ι.o
0.8
0.6
0.4
0.2
0.0
deg
H
0.0	0.2	0.4	0.6	0.8	1.0	0.0	0.2	0.4	0.6	0.8	1.0
best val loss (last layer)	best val loss (last layer)
Figure 8: Just like over the boolean cube: Over the sphere √dSd-1 and the standard Gaussian
N(0, Id), training only the last layer is better for learning low degree polynomials, but training
all layers is better for learning high degree polynomials. Here we use the exact same experimental
setup as Fig. 3(c) (see Appendix E for details) except that the input distribution is changed from
uniform over the boolean cube ㈤d to standard Gaussian N(0, Id) (left) and uniform over the sphere
√dSd-1 (right), where d = 128.
XeUJ -e2⅛33fi
AHNO αaλy^l Iss EJN-N-VML
上 XeUJ -e□R903w
saw As TIq DN-N-VaL
dataset
MNIST
CIFAR10
nonlln
erf
relu
dataset
• MNIST
• CIFAR10
nonlln
• erf
« relu
Theoretical vs empirical max learning rate under different preprocessing
No Preprocessing
ZCA128
10^1
empirical max Ir
IO-1
empirical max Ir
Figure 9: We perform binary search for the empirical max learning rate of MLPs of different depth,
activations, σw2 , and σb2 on MNIST and CIFAR10 preprocessed in different ways. See Appendix E.2
for experimental details. The first row compares the theoretical and empirical max learning rates
when training only the last layer. The second row compares the same when training all layers (under
NTK parametrization (Eq. (MLP))). The three columns correspond to the different preprocessing
procedures: no preprocessing, PCA projection to the first 128 components (PCA128), and ZCA
projection to the first 128 components (ZCA128). In general, the theoretical prediction is less accurate
(compared to preprocessing by centering and projecting to the sphere, as in Fig. 5), though still well
correlated with the empirical max learning rate. The most blatant caveat is the relu networks trained
on PCA128- and ZCA128-processed CIFAR10.
PCA128
dataset
MNIST
CIFAR10
nonlln
erf
relu
10 一
empirical max Ir
20
Under review as a conference paper at ICLR 2020
relu ck: fractional variance

IO0 IO1 10a
depth
-1-0
-0.9
-0.8
-0-7
-0.6
-0-5
-0.4
-0-3
1-0
0.8
0.6
0.4
0-2
0.0
degree 2
depth
-0.16
-0.14
-0-12
-0-10
-0.08
-0.06
-0.04
-0-02
-0.00
depth
-0.0200
-0-0175
-0.0150
-0-0125
-0.0100
-0.0075
-0.0050
-0.0025
-0.0000
-0.008
-0.007
-0.006
-0.005
-0.004
-0.003
-0.002
-0.001
-0.000
1-0
0.8
0.6
0.4
0-2
0.0
degree 8
IO0 IO1 IO2
-0.0040
-0.0035
-0.0030
-0.0025
-0.0020
-0.0015
-0.0010
-0.0005
-0.0000
depth
degree 1
0.8
0-2
0.0
0.6
甘
0.4
-0.00
depth
-0.48
-0.40
-0-32
-0.24
-0.16
-0.08
-0.56
-0.040
-0.035
-0.030
-0.025
-0.020
-0.015
-0.010
-0.005
-0.000
-0.0105
-0.0000
-0.0090
-0.0075
-0.0060
-0.0045
-0.0030
-0.0015
-0.0056
-0.0000
-0-0048
-0.0040
-0.0032
-0.0024
-0.0016
-0.0008
degree O
4
3
特2
4
3
■2
depth
degree 1
-0.80
-0-72
-0.64
-0.56
-0.48
-0.40
-0-32
-0.24
-0.16
-0.56
-0.48
-0.40
-0-32
-0.24
-0.16
-0.08
-0.00
degree 2
4
3
2
4
3
2
degree 3
depth
re∣u ntk: fractional variance
-0.24
-0-21
-0.18
-0-15
-0-12
-0.09
-0.06
-0.03
-0.00
-0.08
-0-07
-0.06
-0.05
-0.04
-0.03
-0-02
-0.01
-0.00
degree 4
-0.054
-0-048
-0.042
-0.036
-0.030
-0.024
-0.018
-0-012
-0.006
-0.000
degree 6
4
3
2
-0-032
-0.028
-0.024
-0.020
-0.016
-0-012
-0.008
-0.004
-0.000
-0.040
-0.035
-0.030
-0.025
-0.020
-0.015
-0.010
-0.005
-0.000
-0.0200
-0.0175
-0.0150
-0.0125
-0.0100
-0.0075
-0.0050
-0.0025
-0.0000
-0.024
-0-021
-0.018
-0.015
-0-012
-0.009
-0.006
-0.003
-0.000
O
4
一
2
O
Figure 10:	2D contour plots of how fractional variance of each degree varies with σb2 and
depth, fixing σw2 = 2, for relu CK and NTK. For each degree k, and for each selected fractional
variance value, we plot the level curve of (depth, σb2) achieving this value. The color indicates the
fractional variance, as given in the color bars.
21
Under review as a conference paper at ICLR 2020
D	VISUALIZING THE SPECTRAL EFFECTS OF σw2 , σb2, AND DEPTH
While in the main text, we summarized several trends of interest kn several plots, they do not give the
entire picture of how eigenvalues and fractional variances vary with σw2 , σb2 , and depth. Here we try
to present this relationship more completely in a series of contour plots. Fig. 10 shows how varying
depth and σb2 changes the fractional variances of each degree, for relu CK and NTK. We are fixing
σW = 2 in the CK plots, as the fractional variances only depend on the ratio σ2∕σW; even though
this is not true for relu NTK, we fix σw2 = 2 as well for consistency. For erf, however, the fractional
variance will crucially depend on both σw2 and σb2, so we present 3D contour plots of how σw2 , σb2 ,
and depth changes fractional variance in Fig. 13. Complementarily, we also show in Figs. 11 and 12
a few slices of these 3D contour plots for different fixed values of σb2, for erf CK and NTK.
22
Under review as a conference paper at ICLR 2020
erf ck: fractional variance, fixing σ∣ = 0
IO1
depth
-0.200
-0.175
-0.150
-0.125
-0.100
-0.075
-0.050
-0.025
-0.000
IO2
degree 1
degree 3
5
-1.05
5
5
-0.90
4
4
4
晋3
3
3
-0.45
-0.30
2
10°
degree Q
degree 2
degree 4
一
-0.056
一
4
4
-0.032
当3
3
2
degree 3
一
-0.14
4
噌3
2
-0.75
-0.60
-0.15
-0.00
-0.048
-0.040
-0.024
-0.016
-0-12
-0-10
-0.08
-0.06
-0.04
-0-02
IO1
depth
-0.008
-0.000
IO2
2
2
degree 5
2
-0.00
depth
IO1
depth
-1.05
-0.90
-0-75
-0.60
-0.45
-0.30
-0-15
-0.00
4
3
2
depth
degree 1
-0.8
-0-7
-0.6
-0-5
-0.4
-0-3
-0-2
-0-1
-0.0
degree 0
4
当3
2
一
4
当3
2
degree 1
depth
depth
degree 0
-1.05
-0.90
-0-75
-0.60
-0.45
-0.30
-0-15
-0.00
-0.8
-0-7
-0.6
-0-5
-0.4
-0-3
-0-2
-0-1
-0.0
-1.04
degree 2
4
3
2
-0.024
-0.056
-0.048
-0.040
-0.032
-0.016
-0.008
degree 3
一
4
3
2
IO1
depth
depth
degree 2
degree 5
2
IO1
depth
-0.120
-0.105
-0.090
-0.075
-0.060
-0.045
-0.030
-0.015
-0.000
degree 7
5
4
3
2
IO1
depth
-0.08
-0.07
-0.06
-0.05
-0.04
-0.03
-0.02
-0.01
-0.00
erf ck: fractional variance, fixing σ⅛ = 0.1
-0-032
-0.028
-0.024
-0.020
-0.016
-0-012
-0.008
-0.004
-0.000
degree 6
depth
IO1
depth
-0.024
-0.021
-0.018
-0.015
-0-012
-0.009
-0.006
-0.003
-0.000
--0.003
IO2
degree 8
IO1
depth
-0.0200
-0-0175
卜 0.0150
-0.0125
-0.0100
-0.0075
-0.0050
-0.0025
-0.0000
--0.0025
IO2
-0.064
-0.056
-0.048
-0.040
-0-032
-0.024
-0.016
-0.008
-0.000
erf ck: fractional variance, fixing 琢=0.2
-0.000
-0-120
-0.105
-0.090
-0.075
-0.060
-0.045
-0.030
-0.015
-0.000
degree 4
4
一
2
5
4
3
2
depth
IO1
depth
-0-032
-0.028
-0.024
-0.020
-0.016
-0-012
-0.008
-0.004
-0.000
IO2
degree 5
-0.056
-0.048
-0.040
-0-032
-0.024
-0.016
-0.008
-0.000
degree 7
4
3
2
一
4
3
2
一
4
3
2
degree 1
depth
degree 6
depth
erf ck: fractional variance, fixing σ1 = 2
-0.042
-0.036
-0.030
-0.024
-0.018
-0-012
-0.006
-0.000
--0.006
-0.024
-0.021
-0.018
-0.015
-0-012
-0.009
-0.006
-0.003
-0.000
--0.003
-0.035
-0.030
-0.025
-0.020
-0.015
-0.010
-0.005
-0.000
--0.005
degree 8
-0.0200
-0.0175
-0.0150
-0.0125
-0.0100
-0.0075
-0.0050
-0.0025
-0.0000
--0.0025
degree 4
degree 6
-0.0200
-0.0175
-0.0150
-0-0125
-0.0100
-0.0075
-0.0050
-0.0025
-0.0000
-0-0120
-0.0105
-0.0090
-0.0075
-0.0060
-0.0045
-0.0030
-0.0015
-0.0000
-0.048
-0.042
-0.036
-0.030
-0.024
-0.018
-0-012
-0.006
-0.000
4
4
4
4
g3
一
一
一
-0.64
2
2
2
2
5
4
⅛3
2
-0.96
-0.88
-0.80
-0-72
-0.56
-0.48
degree 1
degree 3
degree 5
degree 1
5
5
5
2
2
IO1
depth
IO1
depth
IO1
depth
IO1
depth
degree 8
4
3
2
-0.008
-0.007
-0.006
-0.005
-0.004
-0.003
-0.002
-0.001
-0.000
-0.40
-0.35
-0.30
-0-25
-0-20
-0-15
-0-10
-0.05
-0.00
-0.040
-0.035
-0.030
-0.025
-0.020
-0.015
-0.010
-0.005
-0.000
-0.016
-0.014
-0-012
-0.010
-0.008
-0.006
-0.004
-0.002
-0.000
-0.0105
-0.0090
-0.0075
-0.0060
-0.0045
-0.0030
-0.0015
-0.0000
一
一
一
一
一
4
4
一
一
4
4
3
3
2
一
一
一
一
4
4
3
3
2
一
2
一
一
4
3
一
2
一
4
一
2
一
Figure 11:	2D contour plots of how fractional variance of each degree varies with σw2 and
depth, for different slices of σb2, for erf NTK. These plots essentially show slices of the NTK 3D
contour plots in Fig. 13. For σb = 0, μk for all even degrees k are 0, so We omit the plots. Note the
rapid change in the shape of the contours for odd degrees, going from σb2 = 0 to σb2 = 0.1. This is
reflected in Fig. 13 as Well.
23
Under review as a conference paper at ICLR 2020
erf ntk: fractional variance, fixing σ⅛ = 0
degree 1
5
4
*3
2
-0.75
-0.60
-0.45
-0.30
-0.15
-0.00
-0.90
IO1
depth
degree 3
5
4
3
2
-0.15
-0.12
-0.09
-0.Q0
-0.Q6
-0.03
-0.21
-0.18
IO1
depth
degree 5
5
4
3
2
-0.10
-0.08
-0.06
-0.00
-0.14
-0.12
-0.04
-0.02
degree 7
4
3
2
-0.075
-0.060
-0.045
IO2
-0.105
-0.090
-0.030
-0.015
IO1
depth
IO1
depth
IO0
-0.000
erf ntk: fractional variance, fixing σ⅛ = 0.1
degree 0
4
噌3
2
一
4
2
-0.00
degree 1
-0-5
-0.0
-0.60
-0.45
-0.30
-0-15
-0-7
-0.6
-0.4
-0-3
-0-2
-0-1
0.90
0-75
depth
-1.05
degree 2
degree 4
-0.090
-0.075
4
4
-0.060
一
-0.045
一
-0.030
2
2
-0.015
-0.000
degree 3
一
4
3
2
IO1
depth
degree 5
5
3
2
depth
IO1
depth
-0.064
-0.056
-0.048
-0.040
-0-032
-0.024
-0.016
-0.008
-0.000
-0.16
-0-14
-0-12
-0-10
-0.08
-0.06
-0.04
-0-02
-0.00
-0.090
-0.075
-0.060
-0.045
-0.030
-0.015
-0.000
degree 6
4
一
2
5
4
3
2
IO1
depth
-0.048
-0.042
-0.036
-0.030
-0.024
-0.018
-0-012
-0.006
-0.000
--0.006
IOa
degree 8
4
3
2
-0.040
-0.035
-0.030
-0.025
-0.020
-0.015
-0.010
-0.005
-0.000
--0.005
degree 7
-0.064
-0.056
-0.048
-0.040
-0.032
-0.024
-0.016
-0.008
-0.000
--0.008
一
一
一
一
5
一
4

erf ntk: fractional variance, fixing 琢=0.2
一
一
一
一
一
degree 0
4
当3
2
5
4
当3
2
degree 1
depth
IO1
depth
-1.05
-0.90
0-75
0.60
0.45
0.30
0.15
l- 0.00
-0.64
-0.56
-0.48
-0.40
-0-32
-0.24
-0.16
-0.08
-0.00
degree 2
4
3
2
一
4
3
2
-0.075
-0.060
-0.045
-0.030
-0.015
-0.000
degree 3
-0-14
-0-12
depth
-0-10
-0.08
-0.06
-0.04
-0-02
-0.00
depth
-0.090
degree 4
4
一
2
5
4
3
2
degree 5
deptħ
IO1
depth
-0.064
0.056
-0.048
I- 0.040
I- 0-032
I- 0.024
I- 0.016
I- 0.008
L 0.000
-0.08
-0-07
-0.06
-0.05
-0.04
-0.03
-0-02
-0.01
-0.00
degree 6
4
一
2
5
4
3
2
depth
IO1
depth
degree 7
erf ntk: fractional variance, fixing= 2
-0.048
-0.042
-	0.036
-	0.030
-	0.024
-	0.018
-	0-012
-	0.006
-	0.000
-	-0.006
IOa
-0.056
-0.048
-0.040
-0.032
-0.024
-0.016
-0.008
-0.000
--0.008
degree 8
4
3
2
-0.040
-0.035
-0.030
-0.025
-0.020
-0.015
-0.010
-0.005
-0.000
--0.005
degree 0
4
曾3
2
一
4
臂3
2
-0.60
-0.35
-0.30
-0.05
-0.00
-0.90
-0-75
-0.45
-0.30
-0-15
-0.00
-0-25
-0.20
-0-15
-0-10
depth
degree 1
-1.05
degree 2
4
3
2
depth
-0.08
-0-07
-0.06
-0.05
-0.04
-0.03
-0-02
-0.01
-0.00
-0.08
-0-07
-0.06
-0.05
-0.04
-0.03
-0-02
-0.01
-0.00
degree 4
4
3
2
-0.032
depth
-0.048
-0.040
-0.024
-0.016
-0.008
-0.000
-0.056
degree 6
4
depth
-0.040
-0.035
-0.030
-0.025
-0.020
-0.015
-0.010
-0.005
-0.000
degree 8
4
-0.032
-0.028
-0.024
-0.020
-0.016
-0-012
-0.008
-0.004
-0.000
degree 3
degree 5
4
4
3
2
-0.048
-0.042
-0.036
-0.030
-0.024
-0.018
-0-012
-0.006
-0.000
degree 7
4
-0.030
-0.005
-0.000
-0.025
-0.020
-0.015
-0.010
-0.035
一
一
一
一
一
一
一
一
一
2
2
一
一
一
2
2
Figure 12:	2D contour plots of how fractional variance of each degree varies with σw2 and
depth, for different slices of σb2 , for erf CK. These plots essentially show slices of the CK 3D
contour plots in Fig. 13. For σb = 0, μk for all even degrees k are 0, so We omit the plots. Note the
rapid change in the shape of the contours for odd degrees, going from σb2 = 0 to σb2 = 0.1. This is
reflected in Fig. 13 as Well.
24
Under review as a conference paper at ICLR 2020
3- T- T- T-?- «3-?-
Erf Kernels: Fractional Variance Contours
Figure 13: 3D contour plots of how fractional variance of each degree varies with σw2 , σb2 and
log2(depth), for erf CK and NTK. For each value of fractional variance, as given in the legend on
the right, we plot the level surface in the (σw2 , σb2 , log2 (depth))-space achieving this value in the
corresponding color. The closer to blue the color, the higher the value. Note that the contour for the
highest values in higher degree plots “floats in mid-air”, implying that there is an optimal depth for
learning features of that degree that is not particularly small nor particularly big.
25
Under review as a conference paper at ICLR 2020
E Experimental Details
E.1 FIG. 3
Fig. 3(a), (b) and (c) differ in the set of hyperparameters they involve (to be specified below), but in
all of them, we train relu networks against a randomly generated ground truth multilinear polynomial,
with input space ㈤128 and L2 loss L(f)=ExAd (f(x)- f*(x))2.
Training We perform SGD with batch size 1000. In each iteration, we freshly sample a new batch,
and we train for a total of 100,000 iterations, so the network potentially sees 108 different examples.
At every 1000 iterations, we validate the current network on a freshly drawn batch of 10,000 examples.
We thus record a total of 100 validation losses, and we take the lowest to be the “best validation loss.”
Generating the Ground Truth Function The ground truth function f *(x) is generated by
first sampling 10 monomials m1, . . . , m10 of degree k, then randomly sampling 10 coefficients
a1 , . . . , a10 for them. The final function is obtained by normalizing {ai} such that the sum of their
squares is 1:
f *(x) =f X a" X a2.	(11)
i=1	j=1
Hyperparameters for Fig. 3(a)
•	The learning rate is half the theoretical maximum learning rate7 2 max(μo, μι)-1
•	Ground truth degree k ∈ {0, 1, 2, 3}
•	Depth ∈ {0, . . . , 10}
•	activation = relu
•	σw2 = 2
•	σb2 = 0
•	width = 1000
•	10 random seeds per hyperparameter combination
•	training last layer (marked “ck”), or all layers (marked “ntk”). In the latter case, we use the
NTK parametrization of the MLP (Eq. (MLP)).
Hyperparameters for Fig. 3(b)
•	The learning rate is half the theoretical maximum learning rate ɪ max(μo, μι)-1
•	Ground truth degree k ∈ {0, 1, 2, 3}
•	Depth ∈ {0, . . . , 10}
•	activation = relu
•	σw2 = 2
•	σb2 = 0
•	width = 1000
•	100 random seeds per hyperparameter combination
•	training last layer weight and bias only
7Note that, because the L2 loss here is L(f) = Eχ∈Sd (f (x) — f *(x))2, the maximum learning rate is
λ-n1αχ = max(μo, μι)-1 (see Thm 4.1). If we instead adopt the convention L(f) = Eχ∈®d ɪ (f (x) — f *(x))2,
then the maximum learning rate would be 2λmaχ = 2 max(μo,μι)-1
26
Under review as a conference paper at ICLR 2020
Algorithm 1 Binary Search for Empirical Max Learning Rate
upper J 16 X theoretical max lr
lower J 0
tol J 0.01 × theoretical max lr
while |upper - lower| > tol do
α J (upper + lower)/2
Run SGD with learning rate α for 1000 iterations
if loss diverges then
upper J α
else
lower J α
end if
end while
Output: upper
Hyperparameters for Fig. 3(c)
• The learning rate ∈ {0.05, 0.1, 0.5}
• Ground truth degree k ∈ {0, 1, . . . , 6}
• Depth ∈ {1, . . . , 5}
• activation ∈ {relu, erf}
• σw2 = 2 for relu, but σw2 ∈ {1, 2, . . . , 5} for erf
• σb2 ∈ {0, 1, . . . ,4}
• width = 1000
• 1 random seed per hyperparameter combination
• Training all layers, using the NTK parametrization of the MLP (Eq. (MLP))
E.2 Max Learning Rate Experiments
Here we describe the experimental details for the experiments underlying Figs. 5 and 9.
Theoretical max learning rate For a fixed setup, we compute Φ according to Eq. (CK) (if only
last layer is trained) or Eq. (NTK) (if all layers are trained). For ground truth problems where the
output is n-dimensional, the theoretical max learning rate is nΦ(0)-1; in particular, the max learning
rates for MNIST and CIFAR10 are 10 times those for boolean cube, sphere, and Gaussian. This is
because the kernel for an multi-output problem effectively becomes
(K
—K ㊉n = - I 0
nn
0
00
...	0
0K
where the 1 factor is due to the ɪ factor in the scaled square loss L(f, f * *) = Ex〜X ɪ P2ι(f (x)i 一
f *(x)i)2. The top eigenvalue for ɪ K㊉n is just n times the top eigenvalue for K.
Empirical max learning rate For a fixed setup, we perform binary search for the empirical max
learning rate as in Algorithm 1.
Preprocessing In Fig. 5, for MNIST and CIFAR10, We center and project each image onto the
sphere √dSd-1, where d = 28 × 28 = 784 for MNIST and d = 3 × 32 × 32 = 3072 for CIFAR10.
More precisely, we compute the average image X over the entire dataset, and we preprocess each image
X as √d kx-∣k. In Fig. 9, there are three different preprocessing schemes. For “no preprocessing,” we
load the MNIST and CIFAR10 data as is. In “PCA128,” we take the top 128 eigencomponents of the
data, so that the data has only 128 dimensions. In “ZCA128,” we take the top 128 eigencomponents
but rotate it back to the original space, so that the data still has dimension d, where d = 28 × 28 = 784
for MNIST and d = 3 × 32 × 32 = 3072 for CIFAR10.
27
Under review as a conference paper at ICLR 2020
Hyperparameters
•	Target function: For boolean cube, sphere, and standard Gaussian, we randomly sample a
degree 1 polynomial as in Eq. (11). For MNIST and CIFAR10, we just use the label in the
dataset, encoded as a one-hot vector for square-loss regression.
•	Depth ∈ {1,2,4,8, 16}
•	activation ∈ {relu, erf}
•	σw2 = 2 for relu, but σw2 ∈ {1, 2, . . . , 5} for erf
•	σb2 ∈ {1, . . . , 4}
•	width = 1000
•	1 random seed per hyperparameter combination
•	Training last layer (CK) or all layers (NTK). In the latter case, we use the NTK parametriza-
tion of the MLP (Eq. (MLP)).
F	Review of the Theory of Neural Tangent Kernels
F.1 Convergence of Infinite-Width Kernels at Initialization
Conjugate Kernel Via a central-limit-like intuition, each unit hl (x)α of Eq. (MLP) should behave
like a Gaussian as width nl-1 → ∞, as it is a sum of a large number of roughly independent random
variables (Poole et al., 2016; Schoenholz et al., 2017; Yang and Schoenholz, 2017). The devil, of
course, is in what “roughly independent” means and how to apply the central limit theorem (CLT) to
this setting. It can be done, however, (Lee et al., 2018; Matthews et al., 2018; Novak et al., 2018),
and in the most general case, using a “Gaussian conditioning” technique, this result can be rigorously
generalized to almost any architecture Yang (2019). In any case, the consequence is that, for any
finite set S ⊆ X,
{hlα(x)}x∈S	converges in distribution to N(0, Σl(S, S)),
as min{n1, . . . , nl-1} → ∞, where Σl is the CK as given in Eq. (CK).
Neural Tangent Kernel By a slightly more involved version of the “Gaussian conditioning” tech-
nique, Yang (2019) also showed that, for any x, y ∈ X,
hVθhL(x), VθhL(y)i	converges almost surely to	ΘL(x, y)
as the widths tend to infinity, where Θl is the NTK as given in Eq. (NTK).
F.2 Fast Evaluations of CK and NTK
For certain φ like relu or erf, Vφ and Vφ0 can be evaluated very quickly, so that both the CK and NTK
can be computed in O(|X |2L) time, where X is the set of points we want to compute the kernel
function over, and L is the number of layers.
Fact F.1 (Cho and Saul (2009)). For any kernel K
VrelU(K)(x,x0) = -— (p 1 — c2 + (π — arccos C)C) PK(x, X)K(x0, x0)
2π
VrelU(K )(x,x0) = ɪ(n — arccos C)
reu	2π
where C = K(x, x0)/ K(x, x)K (x0, x0).
Fact F.2 (Neal (1995)). For any kernel K,
Verf (K)(x, x0)
V0erf (K)(x, x0)
2	K (x,x0)
π	P(K(x, x) + 0.5)(K(x0, x0) + 0.5)
4
πp(1 + 2K(x, x))(1 + 2K(x0, x0)) — 4K(x, x0)2
28
Under review as a conference paper at ICLR 2020
FactF.3. Let φ(x) = exp(x∕σ) for some σ > 0. For any kernel K,
K(x, x) + 2K (x, x0) + K(x0, x0)
2σ2
F.3 Linear Evolution of Neural Network under GD
Remarkably, the NTK governs the evolution of the neural network function under gradient descent in
the infinite-width limit. First, let’s consider how the parameters θ and the neural network function
f evolve under continuous time gradient flow. Suppose f is only defined on a finite input space
X = {x1, . . . , xk}. We will visualize
		∂L ∂f(x1)			Θ1		∂f(χ1) ∂θι	∙	∂f(xk) • ∙	∂θι
	f (χ1)							
f (X)=	. 二 .	, .	Vf L =	. 二 .	, .	θ 二	. 二., .	Vθ f =	.	. . .	. .	.
	f (xk)		∂L ∂f(χk)		θn		∂f(x1) 	1~~:  . ∂θn	∂f(xk) • • 	::	 ∂θn
(best viewed in color). Then under continuous time gradient descent with learning rate η,
∂t θt = -ηVθL(ft) = -η Vθft ∙ VfL(ft),
∂t ft = Vθft > ∙ ∂t θt = -η Vθft > ∙ Vθft ∙ Vf L(ft) = -η Θt ∙ Vf L(ft)	(12)
where Θt = Vθ f> ∙ Vθ ft ∈ Rk ×k is of course the (finite width) NTK. These equations can be
visualized as
∂t	= -η
∂t	=	∙ ∂t	= -η
-η
Thus f undergoes kernel gradient descent with (functional) loss L(f) and kernel Θt. This kernel Θt
of course changes as f evolves, but remarkably, it in fact stays constant for f being an infinitely wide
MLP (Jacot et al., 2018):
∂tft = -ηΘ ∙Vf L(ft),	(Training All Layers)
where Θ is the infinite-width NTK corresponding to f . A similar equation holds for the CK Σ if we
train only the last layer,
∂tft = -η∑ ∙ Vf L(ft).	(Training Last Layer)
If L is the square loss against a ground truth function f*, then Vf L(ft)=克Vf ∣∣ft 一 f*k2 =
1 (ft 一 f *), and the equations above become linear differential equations. However, typically We only
have a training set X train ⊆ X of size far less than |X |. In this case, the loss function is effectively
L(f) = 2x⅛	X (f(x) - f*(χ))2,
x∈X train
with functional gradient
Vf L(f ) = lX⅛ Dtrain ∙ (f - f *)，
where Dtrain is a diagonal matrix of size k × k whose diagonal is 1 on x ∈ Xtrain and 0 else. Then
our function still evolves linearly
∂tft = -η(K ∙ Dtrain) ∙ (ft - f*)	(13)
where K is the CK or the NTK depending on which parameters are trained.
29
Under review as a conference paper at ICLR 2020
F.4 Relationship to Gaussian Process Inference.
Recall that the initial f0 in Eq. (13) is distributed as a Gaussian process N(0, Σ) in the infinite width
limit. As Eq. (13) is a linear differential equation, the distribution of ft will remain a Gaussian
process for all t, whether K is CK or NTK. Under suitable conditions, it can be shown that (Lee
et al., 2019), in the limit as t → ∞, if we train only the last layer, then the resulting function f∞ is
distributed as a Gaussian process with mean f∞ given by
f∞(x) = Σ(x, Xtrain)Σ(Xtrain, Xtrain)Tf *(Xtrain)
and kernel Var f∞ given by
Varf∞(x,x0) = Σ(x, x0) - Σ(x, Xtrain)Σ(Xtrain, Xtrain)-1Σ(Xtrain, x0).
These formulas precisely described the posterior distribution of f given prior N(0, Σ) and data
{(X, f *(x))}X∈X train ∙
If we train all layers, then similarly as t → ∞, the function f∞ is distributed as a Gaussian process
with mean f∞ given by (Lee et al., 2019)
f∞(x)=Θ(x, Xtrain)Θ(Xtrain, Xtrain)Tf *(Xtrain).
This is, again, the mean of the Gaussian process posterior given priorN(0, Θ) and the training data
{(x, f *(x))}χ∈χtrain. HoWeveι, the kernel of f∞ is no longer the kernel of this posterior, but rather
is an expression involving both the NTK Θ and the CK Σ; see Lee et al. (2019).
In any case, we can make the following informal statement in the limit of large width
Training the last layer (resp. all layers) of an MLP infinitely long, in expecta-
tion, yields the mean prediction of the GP inference given prior N (0, Σ) (resp.
N (0, Θ)).
G A B rief Review of Hilbert-Schmidt Operators and Their
Spectral Theory
In this section, we briefly review the theory of Hilbert-Schmidt kernels, and more importantly, to
properly define the notion of eigenvalues and eigenfunctions. A function K : X2 → R is called a
Hilbert-Schmidt operator if K ∈ L2 (X × X), i.e.
kKk2HS d=ef E K(x, y)2 < ∞.
x,y 〜X
kKk2HS is known as the Hilbert-Schmidt norm of K. K is called symmetric if K(x, y) = K(y, x)
and positive definite (resp. semidefinite) if
E f (x)K (x, y)f(y) > 0 (resp. ≥ 0) for all f ∈ L2(X) not a.e. zero.
x,y 〜X
A spectral theorem (Mercer’s theorem) holds for Hilbert-Schmidt operators.
Fact G.1. If K is a symmetric positive semidefinite Hilbert-Schmidt kernel, then there is a sequence
of scalars λi ≥ 0 (eigenvalues) and functions fi ∈ L2(X) (eigenfunctions), for i ∈ N, such that
∀i, j, hfi, fji = I(i = j), and K(x, y) = Xλifi(x)fi(y)
where the convergence is in L2 (X × X) norm.
This theorem allows us to speak of the eigenfunctions and eigenvalues, which are important for
training and generalization considerations when K is a kernel used in machine learning, as discussed
in the main text.
A sufficient condition for K to be a Hilbert-Schmidt kernel in our case (concerning only probability
measure on X) is just that K is bounded. All Ks in this paper satisfy this property.
30
Under review as a conference paper at ICLR 2020
H Eigendecomposition of Neural Kernel on Different Domains
H.1 B o olean Cube
From the Fourier Series Perspective. We continue from the discussion of the boolean cube in the
main text. Recall that Tδ is the shift operator on functions that sends Φ(∙) to Φ(∙- ∆). Notice that,
if We let Φ(t) = eκt for some K ∈ C, then T∆Φ(s) = c-kδ ∙ eκt. Thus Φ is an “eigenfunction” of
the operator T∆ with eigenvalue e-κ∆ . In particular, this implies that
Proposition H.1. Suppose Φ(t) = e"σ2, as in the case when K is the CK orNTKofa 1-layerneural
network with nonlinearity exp(∙∕σ), up to multiplicative constant (Fact F.3). Then the eigenvalue μk
over the boolean cube ≤Pd equals
μk = 2-d(1 - exp(-∆∕σ2))k(1 + exp(-∆∕σ2))d-k ∙ exp(1∕σ2)
where ∆ = 2∕d.
It Would be nice if We can express any Φ as a linear combination of exponentials, so that Eq. (5)
simplifies in the fashion of Prop H.1 — this is precisely the idea of Fourier series.
We Will use the theory of Fourier analysis on the circle, and for this We need to discuss periodic
functions. Let Φ : [-2, 2] → R be defined as
(Φ(x)	if X ∈ [-1,1]
Φ(x) =	Φ(2 - x) if x ∈ [1, 2]
[φ(-2 - x) if x ∈ [-2,-1].
See Fig. 14 for an example illustration. Note that if Φ is continuous on [-1, 1], then Φ is continuous
as a periodic function on [-2, 2].
The Fourier basis on functions over [-2,2] is the collection {t → e 1 πist}s∈z. Under generic condi-
tions (for example if Ψ ∈ L2 [-2,2]), a function Ψ has an associated Fourier series Ps∈z Ψ(s)e1 πist.
We briefly revieW basic facts of Fourier analysis on the circle. Recall the folloWing notion of functions
of bounded variation.
Definition H.2. A function f : [a, b] → R is said to have bounded variation if
nP -1
sup	|f(xi+1) - f(xi)| < ∞,
P
i=0
Where the supremum is taken over all partitions P of the interval [a, b],
P = {x0,...,xnp }, xo ≤ xi ≤ …≤ XnP .
Intuitively, a function of bounded variation has a graph (in [a, b] × R) of finite length.
Fact H.3 (Katznelson (2004)). A bounded variation function f : [-2, 2] → R that is periodic (i.e.
f (-2) = f (2)) has a pointwise-convergent Fourier series:
lim X	Ψ(s)e1 πist → Ψ(t), Vt ∈ [-2, 2].
T→∞ s∈[-T,T]
31
Under review as a conference paper at ICLR 2020
From this fact easily follows the following lemma.
Lemma H.4. Suppose Φ is continuous and has bounded variation on [-1, 1]. Then Φ is also
continuous and has bounded variation, and its Fourier Series (on [-2, 2]) converges pointwise to Φ.
Proof. Φ is obviously continuous and has bounded variation as well, and from Fact H.3, we know a
periodic continuous function with bounded variation has a pointwise-convergent Fourier Series. □
Certainly, T∆ sends continuous bounded variation functions to continuous bounded variation func-
tions. Because T∆e1 πist
一 e-1 ∏is∆g 1 ∏ist
T∆ X Ψ(s)e2πist = X Ψ(s)e-1 "is*1 πist
s∈Z	s∈Z
whenever both sides are well defined. If Ψ is continuous and has bounded variation then T∆Ψ is
also continuous and has bounded variation, and thus its Fourier series, the RHS above, converges
pointwise to T∆Ψ.
Now, observe
d
(I - T∆)k(I + T∆)d-kΦ(x) = X Cd-k,kΦ (X - r∆)
r=0
d
(I-T∆)k (I + T∆)d-kΦ⑴=X Cd-k,kΦ
r=0
=μk
r∆
Expressing the LHS in Fourier basis, we obtain
Theorem H.5.
μk = X is(1 - e-2niS)k(1 + e- 1 niS)d-k*(s)
s∈Z
where
Φ(s) = 1 12 Φ(t)e-2πist dt
4 -2
11
=-	Φ(t)(e-2πist + (-1)se2πist) dt
4 -1
{1 R-ι Φ(t) cos( 1 πst) dt	if S is even
一22 R-1 Φ(t)sin( 1 ∏st)dt if S is odd
denote the Fourier coefficients ofΦ on [-2, 2]. (Here i is the imaginary unit here, not an index).
Recovering the values of Φ given the eigenvalues μo ,...,μd. Conversely, given eigenvalues
μo,...,μd corresponding to each monomial degree, we can recover the entries of the matrix K.
Theorem H.6. For any x,y ∈ dW with Hamming distance r,
K(x, y)
φ ((d-，)△)=X Lμ，
where CL = Pj=o(-1)k+j (d-r)(k-j) asinEq0.
Proof. Recall that for any S ⊆ [d], χS (x) = xS is the Fourier basis corresponding to S (see Eq. (3)).
Then by converting from the Fourier basis to the regular basis, we get
K(x, y)
for any x,y ∈ dw with Hamming distance r
d
E〃k E XS(X)XS⑻.
k=0	|S|=k
32
Under review as a conference paper at ICLR 2020
Figure 15: Examples of Gegenbauer Polynomials for d = 128 (or α = 63).
If x and y differ on a set T ⊆ [d], then we can simplify the inner sum
Φ ((d - r) △) = XL μk X (-1产TI= XX 〃kC「.
k=0	|S|=k	k=0
□
Remark H.7. If We let T be the operator that sends μ∙ → μ∙+ι, then We have the following operator
expression
Φ ((d -r) △) = [(1 + T)d-r(1-T)rμ]o
Remark H.8. The above shoWs that the matrix C = {Ckd-r,r}dk,r=0
satisfies
2d
H.2 Sphere
Now let,s consider the case when X = √dSd-1 is the radius-√d sphere in Rd equipped with the
uniform measure. Again, because x ∈ X all have the same norm, We Will consider Φ as a univariate
functionwithK(x,y) = Φ(hx, yi/kxkkyk) = Φ(hx, yi/d).
As is long known (Schoenberg, 1942; Gneiting, 2013; Xu and Cheney, 1992; Smola et al., 2001), K
is diagonalized by spherical harmonics. We review these results briefly below, as we will build on
them to deduce spectral information of K on isotropic Gaussian distributions.
Review: spherical harmonics and Gegenbauer polynomials. Spherical harmonics are L2 func-
d-1	d-1
tions on S that are eigenfunctions of the Laplace-Beltrami operator △S d-1 of S . They can be
described as the restriction of certain homogeneous polynomials in Rd to Sd-1. Denote by Hd-1,(l)
the space of spherical harmonics of degree l on sphere Sd-1. Then we have the orthogonal decompo-
sition L2 (S d-1) = L∞=0 Hd-1,(l). It is a standard fact that dim Hdf(I) = (d--+l) - (d--+l).
There is a special class of spherical harmonics called zonal harmonics that can be represented as
x 7→ p(hx, yi) for specific polynomials p : R → R, and that possess a special reproducing property
which we will describe shortly. Intuitively, the value of any zonal harmonics only depends on the
“height” ofx along some fixed axis y, so a typical zonal harmonics looks like Fig. 16. The polynomials
p must be one of the Gegenbauer polynomials. Gegenbauer polynomials {Cl(α)(t)}l∞=0 are orthogonal
polynomials with respect to the measure (1 - t2)ɑ- 1 on [-1,1] (see Fig. 15 for examples), and here
we adopt the convention that
∕1 Cna)(t)C(α)(t)(1 - t2)α-1 dt = π21-T((n+ 2α)I(n = l).	(14)
-1	n!(n + α)[Γ(α)]2
Then for each (oriented) axis y ∈ Sd-1 and degree l, there is a unique zonal harmonic
ZyTa ∈ Hd-1,(l), Zyd-1,(I)(X)=fc-,1 Cd-2)(hx,yi)
for any x,y ∈ Sd-1, where cd,ι = d+—- ∙ Very importantly, they satisfy the following
33
Under review as a conference paper at ICLR 2020
Figure 16: Visualization of a zonal harmonic, which depends only on the “height” of the input along
a fixed axis. Color indicates function value.
Fact H.9 (Reproducing property (Suetin)). For any f ∈ Hd-1,(m) ,
E	Zyd-1,(l)(z)f(z) = f(y)I(l = m)
Z ZS d-1 y
E 1Zd-1,(l) (Z)Zd-1,(m) (Z) = Zd-1,(I) (x)I(l = m) = c-,1 C宁)(hx,yi)I(l = m)
zZSd-1
We also record a useful fact about Gegenbauer polynomials.
Fact H.10 (Suetin).
Cl(α)(±1) = (±1)l l+2lα-1
By a result of Schoenberg (1942), we have the following eigendecomposition of K on the sphere.
Theorem H.11 (Schoenberg). Suppose Φ : [-1,1] → R is in L2((1 - t2) d-ɪT), so that it has the
Gegenbauer expansion
∞	d-2
Φ(t) = X aι C- Cl(H)(t).
l=0
Then K has eigenspaces H√-1,(l) deef {f (x∕√d) : f ∈ Hd-1,(l)} with corresponding eigenval-
ues αl. Since L∞=0 H(d-1,(I) is an orthogonal decomposition of L2 (√dSd-1), this describes all
eigenfunctions of K considered as an operator on L2 (VzdSd-1).
For completeness, we include the proof of this theorem in Appendix I.
By Bezubik et al. (2008), we can express the Gegenbauer coefficients, and thus equivalently the
eigenvalues, via derivatives of Φ:
Theorem H.12 (Bezubik et al. (2008)). If the Taylor expansion of Φ at 0,
Φ(t) = X Ftn,
n=0
is absolutely convergent on the closed interval [-1, 1], then the Gegenbauer coefficients al in
Thm H.11 in dimension d is equal to the absolute convergent series
aι =Γ (I )X
k=0
Φ(l+2k)(0)
2l+2kk!Γ (d + l + k).
(15)
34
Under review as a conference paper at ICLR 2020
As the dimension d of the sphere tends to ∞, the eigenvalues in fact simplify to the derivatives of Φ:
Theorem H.13. Let K be the CK or NTK of an MLP on the sphere √dSd-1. Then K can be
expressed as K(x, y) = Φ(hx, yi/d) for some smooth Φ : [-1, 1] → R. Let a` denote K’s
eigenvalues on the sphere (as in Thm H.11). If we fix ` and letd → ∞, then
lim d'a` = Φ(')(0),
d→∞
where Φ(') denotes the 'th derivative of Φ.
This theorem is the same as Thm I.6 except that it concerns the sphere rather than the boolean cube.
Proof. By Thm I.3, Φ's Taylor expansion around 0 is absolutely convergent on [-1,1], so that the
condition of Thm H.12 is satisfied. Therefore, Eq. (15) holds and is absolutely convergent. By
dominated convergence theorem, we can exchange the limit and the summation, and get
lim dlal = lim dlΓ
d→∞	d→∞
Φ(l+2k)(0)
2l+2kk!Γ (d + l + k)
XXΦ(l+2k)(0) lim -------dTfi——V
k=0	∖/d→∞ 2l+2k k!Γ (d + l + k)
∞	-k
X Φ(l+2k)(0) lim d	(k!)-12-2k
d→∞	2
k=0
Φ(l) (0)
as desired.
□
H.3 Isotropic Gaussian
Now let’s consider X = Rd equipped with standard isotropic Gaussian N(0, I), so that K behaves
like
Kf(x)
E K(x, y)f (y) = E Φ
y~N (0,I)	y~N (0,I)
(hx,y	kxk2
Uxkkyk,d
yΓ ) f ⑹
for any f ∈ L2(N(0, I)). In contrast to the previous two sections, K will essentially depend on the
effect of the norms kxk and kyk on Φ.
Note that an isotropic Gaussian vector Z ~ N(0, I) can be sampled by independently sampling its
direction v uniformly from the sphere S d-1 and sampling its magnitude r from a chi distribution χd
withd degrees of freedom. Proceeding along this line of logic yields the following spectral theorem:
Theorem H.14. A function K : (Rd)2 → R of the form
K(x, y)
(hx,yi kxk2
Uxkkyk, d
forms a positive semidefinite Hilbert-Schmidt operator on L2 (N(0, I)) iff Φ can be decomposed as
∞	d-2
Φ(t,q,q0) = X Al (q,q0)c-,1 C ɪ )(t)
(16)
l=0
satisfying
∞	d-2
X kAιk2c-,2kC(H)k2
∞
XkAlk2cd-,2l
π23-dΓ(l + d - 2)
l!(l + d-2)Γ (d-2)2
< ∞,
(17)
where
cd,l
d-2
d+2l-2
35
Under review as a conference paper at ICLR 2020
(d-2)	(d-2)
•	Cl 2 (t) are Gegenbauer polynomials as in Appendix H.2, with ∣∣C∣ 2 ∣∣ =
JR-ι Cl~^2)(t)2(1 —12)d-1 -1 dt denoting the norm of Cl-ɪ) in L2((1 —12)d-1 -1).
•	and Ai are positive semidefinite Hilbert-Schmidt kernels on L2 (dXd), the L2 space over
the probability measure of a χ2d-variable divided by d, and with ∣Al∣ denoting the Hilbert-
Schmidt norm of Al.
In addition, K is positive definite iff all Al are.
See Appendix I for a proof. As a consequence, K has an eigendecomposition as follows under the
standard Gaussian measure in d dimensions.
Corollary H.15. Suppose K and Φ are as in Thm H.14 and K is a positive semidefinite Hilbert-
Schmidt operator, so that Eq. (16) holds, with Hilbert-Schmidt kernels Al. Let Al have eigendecom-
position
∞
Al(q, q0) =	aliuli(q)uli(q0)	(18)
i=0
for eigenvalues ali ≥ 0 and eigenfunctions uii ∈ L2(dXd) with Eq〜ι χ2 uii(q)2 = 1. Then K has
eigenvalues {ali : l, i ∈ [0, ∞)}, and each eigenvalue ali corresponds to the eigenspace
uii 0Hd-1,(l)⅛f
uii
: f ∈ Hd-1,(i)	,
where Hd-1,(i) is the space of degree l spherical harmonics on the unit sphere Sd-1 of ambient
dimension d.
For certain simple F, we can obtain {Ai }i≥0 explicitly. For example, suppose K is degree-s
positive-homogeneous, in the sense that, for a, b > 0,
K(ax, by) = (ab)s K (x, y).
This happens when K is the CK or NTK of an MLP with degree-s positive-homogeneous. Then it’s
easy to see that Φ(t, q, q0) = (qq0)sΦ(t) for some Φ : [—1,1] → R, and
∞	d-2
φ(t,q, qO) = XgqO)Saic-1 ClH (t)
i=0
where {ai}i are the GegenbaUer coefficients of Φ,
∞	d-2
Φ(t) = X aic-1 CF )(t).
i=0
We can then conclUde with the following theorem.
Theorem H.16. Suppose K : (Rd)2 → R is a kernel given by
K (x,y) = R(kχk∕d)R(kχk∕d)Φ(hχ,yi∕kχkky∣)
for some functions R : [0, ∞) → R, Φ : [—1,1] → R. Let
∞
Φ(t)= X aic-1 Clɪ)(t)
i=0
be the Gegenbauer expansion of Φ. Also define
λ d=f r⅛≡.
Then over the standard Gaussian in Rd, K has the following eigendecomposition
36
Under review as a conference paper at ICLR 2020
• For each l ≥ 0,
λ-1RXHdT,⑷={λ-1R(kxk2∕d)f(x/IlxI∣): f ∈ Hd-1,(l)}
is an eigenspace with eigenvalue λ2al.
•	For any S ∈ L2 (dXd) that is orthogonal to R, i.e.
E S(q)R(q) = 0,
q 〜d Xd
the function
S(IxI2/d)f(x/IxI)
for any f ∈ L2(Sd-1) is in the null space of K.
Proof. The Al in Eq. (16) for K are all equal to
Alm) = λ 字 Rf.
λλ
This is a rank 1 kernel (on L2(dχd)), with eigenfunction R∕λ and eigenvalue λ2. The rest then
follows straightforwardly from Thm H.11.	□
A common example where Thm H.16 applies is when K is the CK of an MLP with relu, or more
generally degree-s positive homogeneous activation functions, so that the R in Thm H.16 is a
polynomial.
In general, we cannot expect K can be exactly diagonalized in a natural basis, as {Al }l≥0 cannot
even be simultaneously diagonalizable. We can, however, investigate the “variance due to each degree
of spherical harmonics” by computing
al d=ef	E	Al (q, q)	(19)
q〜d χd
which is the coefficient of Gegenbauer polynomials in
∞
Φd(t) =f	E 2Φ(t,q,q) = Xaιc-1 Cɪ)(t).	(20)
q〜d χd	1=0
Proposition H.17. Assume that Φ(t, q, q0) is continuous in q and q0. Suppose for any d and any
t ∈ [—1,1], the random variable Φ(t, q, q0) with q, q0 〜d-1χd has a bound ∣Φ(t, q, q0)∣ ≤ Y for
some random variable Y with E |Y| < ∞. Then for every t ∈ [-1, 1],
_ ʌ _ , ..
lim ∣Φd(t) - Φ(t, 1,1)| = 0.
d→∞
Proof. By the strong law of large number, d-1χd2 converges to 1 almost surely. Because Φ(t, q, q0)
is continuous in q and q0, almost surely we have Φ(t, q, q0) → Φ(t, 1, 1) almost surely. Since Φ is
bounded by Y , by dominated convergence, we have
Φd(t) - Φ(t,ι,i)= E ,Φ(t,q, q0) - Φ(t,ι,i) → 0
q,q0 〜d-1χd
as desired.	□
In the final part of this section, we show that in the limit of large input dimension d, the top eigenvalues
of K over the standard Gaussian can be easily described (Thm H.19). First, we need to specify some
conditions on Φ.
Definition H.18. Φ : [-1, 1] × R+ × R+ → R is called reasonable if
•	There is a Taylor expansion in t,
φ(t,q,q0) = X B⅜q21'	(21)
'=0
that is absolutely convergent on (t, q, q0) ∈ [-1,1] X R+ X R+, and such that each B' is
smooth (C∞ ) on (q, q0) ∈ R+ × R+ .
37
Under review as a conference paper at ICLR 2020
•	for all (t, q, q0) ∈ [-1, 1] × R+ × R+, and for any l ∈ [0, ∞), we have |Bl(t, q, q0)| ≤
C(1 + |q|r + |q0|r) for some constants C, r > 0 that may depend on l but not on t, q, q0.
Theorem H.19. Suppose K is a the CK or NTK of an MLP with polynomially bounded activation
function. For every degree l, K over N(0, Id) has an eigenvalue al0 at spherical harmonics degree l
(in the sense of Cor H.15) with
al0 ∈	E	Al (q, q0), E Al (q, q) ,
q,q0 〜d Xd	q 〜d Xd
where Al is as in Eq. (18). Furthermore,
lim dl	E	Al (q, q0) = lim dl
d—→^∞	q,q0~ 1 χ2	d—→^∞
Here Φ(l) is the lth derivative of Φ(t, q, q0) against t.
E Al(q, q) = Φ(l)(0, 1, 1).
q 〜d Xd
Proof. By Lem H.20 below, Φ is reasonable. Let a? = E0〜1v2 Aι(q, q) as in Eq. (19), and let
q d χd
def
bl = Eq,〜ιχ2 Aι(q, q0). Let a?i be the ith largest eigenvalue of Al as In Eq. (18), with a?o being the
largest. Note that all of these quantities al, bl, ali depend on d, but we suppress this notationally. We
seek to prove the following claims:
1.	al0 ≤ al
2.	al0 ≥ bl
3.	dlbl → Φ(l)(0, 1, 1)
4.	dlal → Φ(l) (0, 1, 1)
Claim 1: al0 ≤ al. First note that al is the trace of the operator Al, so that al = Pi∞=0 ali. Thus, any
eigenvalue ali is at most al.
Claim 2: al0 ≥ bl. Now, by Min-Max theorem, the largest eigenvalue al0 of Al is equal to
al0
sup
f
Eq© f(q)f(q0)Al(q,q0)
Eq f (q)2
where 0 = f ∈ L2 (dXd) and q, q0 〜dχd. If we set f (q) = 1 identically, then we get
al0 ≥ E Al(q, q0) = bl,
q,q0
as desired.
Claims 3 and4. Now note that we have the following equalities in the Hilbert space L2((1 -t2) d-1 -1),
because of the absolute convergence in Eq. (17):
Φ(t) =f E
q~ 1 χd
∞
Φ(t, q, q) = X
Φ(t) =f	E1 2Φ(t,q,q0
q,q0 〜d χd
'=0
∞
)=X
'=0
d-2	∞	d-2
E2 A' (q,q)c-,1 C H )(t) = X a`e- C ɪ )(t)
q 〜1χd	'=0
∞
Ei 2A'(q,q0)%1 Cɪ)(t) = Xb'c-,1 Cɪ)(t).
q,q0~ d Xd	'=0
Thus, by Eq. (15),
φ(I+2k)(o)
2l+2kk!Γ (d +1 + k)
By the absolute convergence of Eq. (21), differentiation commutes with expectation:
Φ (l+2k)(0) = ∂t+2k E Φ(0,q,q)= E Bl+2k (q,q)= E Φ(l+2k)(0,q,q),
q 〜1 χd	q 〜d χd	q 〜⅛ χd
38
Under review as a conference paper at ICLR 2020
where Bl+2k is as in Eq. (21). Furthermore, as d → ∞, because Φ(l+2k)(t, q, q0) is smooth and
polynomially bounded in q and q0, while ɪXd has exponential tails and converges a.s.to 1, We have
E Φ(l+2k)(0, q, q) → Φ(l+2k)(0, 1, 1).
q~1 Xd
Finally, by dominated convergence, we get
dlΦ (l+2k)(0)
2l+2kk!Γ (d + l + k)
as desired.
The proof of lim dlbl = Φ(l) (0, 1, 1) is similar.
□
Lemma H.20. Let Φ be the function corresponding to the CK or NTK of an MLP with polynomially
bounded activation functions. Then Φ is reasonable (Defn H.18).
Proof. The Taylor expansion Eq. (21), its absolute convergence, and the smoothness of Bl follows
from the proof of Thm I.3. The polynomial-boundedness of Bl follows trivially from the polynomial-
boundedness of activation.
□
H.4 IN HIGH DIMENSION, BOOLEAN CUBE ≈ SPHERE ≈ STANDARD GAUSSIAN
For the same neural kernel K with K(χ,y) = Φ(hχ,y)∕kχkkyk, kχk2∕d, kyk2∕d), let Φd be as
defined in Eq. (20), and let ΦS C=ef Φ(t, 1, 1). Thus ΦS is the univariate Φ that we studied in
Appendix H.2 in the context of the sphere.
Empirical verification of the spectral closeness As d → ∞, dXd converges almost surely to 1,
and each Al in Eq. (16) converges weakly to the simple operator that multiplies the input function by
Aι(1,1). We verify in Fig. 17 that, for different erf kernels, Φ d approximates Φs when d is large,
which would then imply that their spectra become identical for large d. Note that this is tautologically
true for relu kernels with no bias σb = 0 because they are positive-homogeneous.
Next, we compute the eigenvalues of K on the boolean cube and the sphere, as well as the eigenvalues
of the kernel K(x, y) C=f Φd((x, yi∕∣∣x∣∣∣∣y∣∣) on the sphere, which “summarizes” the eigenvalues
of K on the standard Gaussian distribution. In Fig. 18, we compare them for different dimensions
d up to degree 5 (there, “Gaussian” means K on the sphere). We see that by dimension 128, all
eigenvalues shown are very close to each other for each data distribution.
Theoretical verification of the spectral closeness Thms H.13, H.19 and I.6 show that when the
dimension d is sufficiently large, the top eigenvalues of K as an operator on ≤Pd is very close to the
top eigenvalues of K as an operator on √dSd-1 and those of K as an operator on N(0, Id):
Corollary H.21. Let K be the CK or NTK ofan MLP with polynomially bounded activation function.
Then K can be expressed as K (x, y) = Φ(hx, y)/d) forsomesmooth Φ : [-1,1] → R. Let μk denote
the eigenvalue of K as an operator on ㈤d corresponding to the eigenspace of degree k polynomials
(see Thm 3.1). Let ak denote the eigenvalue of K as an operator on √dSd-1 corresponding to the
eigenspace of degree k spherical harmonics (see Thm H.11). Let ak0 denote the largest eigenvalue of
K as an operator on N(0, Id) restricted to degree k spherical harmonics (see Thm H.19).
Then for any fixed k ≥ 0, for sufficiently large d, we have
lim dkμk = lim dkak = lim dkako = Φ(k)(0)
d→∞	d→∞	d→∞
where Φ(k) (0) is the kth derivative of Φ at 0. If Φ(k) (0) 6= 0, then
lim μk/ak = lim μk/ak0 = 1.
d→∞	d→∞
39
Under review as a conference paper at ICLR 2020
Φd
-----d=2
-----d=4
-----d=8
-----d=32
-----d=128
------φs
Figure 17: In high dimension d, Φd as defined in Eq. (20) approximates Φ very well. We show
for 3 different erf kernels that the function Φ defining K as an integral operator on the sphere is
well-approximated by Φd when d is moderately large (d ≥ 32 seems to work well). This suggests
that the eigenvalues of K as an integral operator on the standard Gaussian should approximate those
of K as an integral operator on the sphere.
erf CKσ w2=2,σ2b=0.001,L=2
—∙— d=16
—•— d=32
―•— d=64
—•- d=128
—∙— boolean
__X- sphere
■ +' ■ ■ GaUssian
Figure 18: In high dimension d, the eigenvalues are very close for the kernel over the boolean
cube, the sphere, and standard Gaussian. We plot the eigenvalues μk of the erf CK, with σW =
2, σb2 = 0.001, depth 2, over the boolean cube, the sphere, as well as kernel on the sphere induced by
Φd (Eq. (20)). We do so for each degree k ≤ 5 and for dimensions d = 16, 32, 64,128. We see that
by dimension d = 128, the eigenvalues shown are already very close to each other.
40
Under review as a conference paper at ICLR 2020
If we fix k and let the input dimension d → ∞, then the fractional variance of degree k converges to
(k!)-1Φ^(0”Φ⑴
(k!)-1Φ(k)(0)
Pj≥0j!)-1 Φ⑶(0)
for all three input distributions CS3d, √dSd-1, and N(0, Id).
Practically speaking, only the top eigenvalues matter. Observe that, in the empirical and theo-
retical results above, We only verify that the top eigenvalues g, ak, or ako for k small compared
tod) are close when d is large. While this result may seem very weak at face value, in practice, the
closeness of these top eigenvalues is the only thing that matters. Indeed, in machine learning, We
Will only ever have a finite number, say N, of training samples to Work With. Thus, We can only
use a finite N × N submatrix of the kernel K . This submatrix, of course, has only N eigenvalues.
Furthermore, if these samples are collected in an iid fashion (as is typically assumed), then these
eigenvalues approximate the largest N eigenvalues (top N counting multiplicity) of the kernel K
itself (Tropp, 2015). As such, the smaller eigenvalues of K can hardly be detected in the training
sample, and cannot affect the machine learning process very much.
Let,s discuss a more concrete example: Fig. 18 shows that the boolean cube eigenvalues μk are very
close to the sphere eigenvalues ak for all k ≤ 5. Over the boolean cube, μo,..., μ5 cover eigenspaces
of total dimension (d) +-----+ (5), which is 275,584,033 when d = 128. We need at least that many
samples to be able to even detect the eigenvalue μ6 and the possible difference between it and the
sphere eigenvalue a6. But note in comparison, Imagenet, one of the most common large datasets in
use today, has only about 15 million samples, 10 times less than the number above.
Additionally, in this same comparison, d = 128 dramatically pales compared to Imagenet’s input
dimension 3 × 2562 = 196608, and even to the those of the smaller common datasets like CIFAR10
(d = 3 × 322 = 3072) and MNIST (d = 242 = 576) — if we were to even use the input dimension
OfMNIST above, then μo,..., μ5 would cover eigenspaces of 523 billion total dimensions! Hence, it
is quite practically relevant to consider the effect of large d on the eigenvalues, while keeping k small.
Again, we remark that even when one fixes k and increases d, the dimension of eigenspaces affected
by our limit theorems Thms H.13, H.19 and I.6 increases like Θ(dk), which implies one needs an
increasing number Θ(dk) of training samples to see the difference of eigenvalues in higher degrees k.
Finally, from the perspective of fractional variance, we also can see that only the top k spectral
closeness matters: By Cor H.21, for any > 0, there is a k such that the total fractional variance of
degree 0 to degree k (corresponding to eigenspaces of total dimension Θ(dk)) sums up to more than
1 - , for the cube, the sphere, and the standard Gaussian simultaneously, when d is sufficiently large.
This is because the asymptotic fractional variance is completely determined by the derivatives of Φ at
t = 0.
I Omitted Proofs
Theorem 3.1. On the d-dimensional boolean cube ≤Pd ,for every S ⊆ [d], XS is an eigenfunction of
K with eigenvalue
xi/d ,
μlSl =f E XSK(x, I)= E X
I I	x∈ Isd	x∈ Isd
(4)
where 1 = (1,..., 1) ∈ ≤Pd. This definition of μ∣s∣ does not depend on the choice S, only on the
cardinality of S. These are all of the eigenfunctions of K by dimensionality considerations.8
Proof. We directly verify KXS = μ∣s∣χs. Notice first that
K(x,y) = Φ(hχ,yi) = Φ(hχ Θ y, Ii) = K(X Θy, 1)
where is Hadamard product. We then calculate
KXS(y) = E K(y, X)XS
x
=EK(1, x Θ y)(χ Θ y)Sys.
x
8Readers familiar with boolean Fourier analysis may be reminded of the noise operator Tρ, ρ ≤ 1 (O’Donnell,
2014, Defn 2.46). In the language of this work, TP is a neural kernel with eigenvalues μk = ρk.
41
Under review as a conference paper at ICLR 2020
Here we are using the fact that x and y are boolean to get xS = (x y)SyS. Changing variable
z d=ef x y , we get
KXS (y) = ys EE K (1,Z)ZS = μ∣s∣χs (y)
as desired. Finally, note that μ∣s∣ is invariant under permutation of [d], so indeed it depends only on
the size of S.	□
Theorem H.11 (Schoenberg). Suppose Φ : [-1,1] → R is in L2((1 一 t2)d-1 T), so that it has the
Gegenbauer expansion
∞	d-2
Φ(t) = X a c-,1C( ɪ )(t).
l=0
Then K has eigenspaces H√-1,(l) =f {f (x∕√d) : f ∈ Hd-1,(l)} With corresponding eigenval-
ues aι. Since L∞=0 Hd-1") is an orthogonal decomposition of L2(√dSd-1), this describes all
eigenfunctions of K considered as an operator on L2( √dS d-1).
Proof. Let f ∈ Hd-1,(n). Then for any X ∈ √dSd-1,
E	K (χ,y)f (y∕√d) = E	Φ(hχ,yi∕d)f (y∕√d)
y∈ √dSd-1	y∈ √dSd-1
=E Φ(hx∕√d,yi)f (y)
y∈sd-1
y∈Sd.ι (X a"(l(y) f(y)
an -$ι zd-dr(y)f(y)
y∈Sd-1 x/ VU
by orthogonality
by reproducing property.
Lemma 3.2. With μk as in Thm 3.1,
μk = 2T(I-T∆)k (I + T∆)d-k Φ(1)
d
2-d	C d-k,k Φ
r=0
r∆
where
Cd-k,k d=ef X(-1)r+j d- k
j =0	j
□
(5)
(6)
(7)
j
Proof. Because Pi Xi/d only takes on values {-d∆, (-d + 1)∆,..., (d 一 1)∆, d∆}, where
∆ = d, we can collect like terms in Eq. (4) and obtain
μk = 2-dX	X	(YkIJ φ ((d-r”)
r=0 xhas r ‘-1’s i=1
which can easily be shown to be equal to
proving Eq. (6) in the claim. Finally, observe that Crd-k,k is also the coefficient of Xr in the
polynomial (1 - x)k(1 + x)d-k. Some operator arithmetic then yields Eq.(5).	□
42
Under review as a conference paper at ICLR 2020
Theorem H.14. A function K : (Rd)2 → R of the form
K (X y)=φ ( hx,yi kXk2 kyk2)
( ,y)	Uxklyk, d , d J
forms a positive semidefinite Hilbert-Schmidt operator on L2(N(0,I)) iff Φ can be decomposed as
∞	d-2
Φ(t, q, q0) = X Al (q, q0)%1 C H )(t)	(16)
l=0
satisfying
X 川2c-,2kC(d-2)k2 = X Mlk2c-,2l「+ d-2； < ∞,	S
l=0	l=0	l!(l + 2~)r (
where
……一d-
•	cd,l	d+2l-2
(d-2)	(d-2)
•	Cll 2 (t) are Gegenbauer polynomials as in Appendix H.2, with ∣∣Cl 2 ∣∣ =
RR-ι C()(t)2(1 -12)d-1 -1 dt denoting the norm of C("ɪ) in L2((1 —12)d-1 -1).
•	and Al are positive semidefinite Hilbert-Schmidt kernels on L2(dXd), the L2 space over
the probability measure of a χ2d-variable divided by d, and with kAlk denoting the Hilbert-
Schmidt norm of Al.
In addition, K is positive definite iff all Al are.
Proof. Note that an isotropic Gaussian vector Z ~ N(0,I) can be sampled by independently
sampling its direction v uniformly from the sphere Sd-1 and sampling its magnitude r from a chi
distribution χd with d degrees of freedom. In the following, we adopt the following notations
x, y ∈ Rd, q = kxk2/d, q0 = kyk2/d, v = x/kxk, v0 = y/kyk
Then, by the reasoning above,
KfX/√d)=	E φ(hv,v0 i,q,qO) f (pq7v).
v0 ~Sd-1
q0~d-1χd
Here f ∈ L2(N(0,I∕d)) (so that f (∙∕√d) ∈ L2(N(0, I))) and d-1χdd is the distribution of a Xd
random variable divided by d. Because of this decomposition of N(0, I) into a product distribution
of direction and magnitude, its space of L2 functions naturally decomposes into a tensor product of
corresponding L2 spaces
L2(N(0,I)) = L2(Sd-1) 0 L2(d-1χd).
where every f ∈ L2 (N (0, I)) can be written as a sum
∞
f(x) = X Rl(q)Pl(v)
l=0
where q = kxk2/d, v = x/kxk as above, Pl ∈ Hd-1,(l) is a spherical harmonic of degree l, and
Rl ∈ L2(d-1χd) (i.e. Rl(∙∕d) ∈ L2(χd)). Here, the equality is understood as a convergence of the
RHS partial sums in the Hilbert space L2(Sd-1) 0 L2(d-1X2d).
Likewise, assuming K is Hilbert-Schmidt, then its Hilbert-Schmidt norm (measured against the
uniform distribution over the sphere) is bounded:
kKk2HS =	E Φ(hv, v7i, q, q7)2
v,v0~S d-1
q,q0~d-1χd
1
= E	Φ(t,q,q0)2(1 — t2)d-1 T dt.
q,q0~d-1χd 7-1
43
Under review as a conference paper at ICLR 2020
Thus Φ resides in the tensor product space
Φ ∈F =f L2((1 -12 尸 T)㊈ L2(d-1χd)㊈ L2 (d-1xd)
and therefore can be expanded as
∞
φ(t,q, qO) = X Ai (q,qO)C-I C( H )(t)
l=0
(d— 2 )
where C∣ 2，(t) are GegenbaUer polynomials as in Appendix H.2 and Al ∈ L2(d-1χd产2. By
Lem I.1 below, K being Hilbert-Schmidt implies that each Al is, as well. Furthermore, since
kK k2HS = kΦk2F is finite, we have
∞
kΦkF = XkAlk2c-,2kc(ɪ)k2 < ∞.
l=0
Here, ∣∣Al∣∣ = JEq©〜ɪχ2 Al(q,q0)2 is the Hilbert-Schmidt norm of Al (i.e. its norm in
L2(d-1χd)02), and ∣∣C("ɪ) ∣∣ = Rj^11 C()(t)2(1 -12)d;-1 -1 dt is the norm of C("ɪ) in
L2((1 一 t2)d-1 -1). Simplifying according to Eq. (14) yields the equality in the claim.
Conversely, if each Al is Hilbert-Schmidt satisfying Eq. (17), then K is obvioUsly a Hilbert-Schmidt
kernel as it has finite Hilbert-Schmidt norm.
□
Lemma I.1. For each l, Al is a positive semidefinite Hilbert-Schmidt kernel. It is positive definite if
K is.
Proof. With q = ∣∣x∣2∕d, V = x∕∣∣x∣∣ as above, let f (x∕√d) = f (√qv) = Rm(q)Pm(v') for some
degree m nonzero spherical harmonics Pm ∈ Hd-1,(m) and some scalar function Rm ∈ L2(d-1χ2d)
that is not a.e. zero. We have
Kφ(x∕√d) =	E	Φ(hv,v0 i,q, q)f (√q7 V)
v0 〜Sd—ɪ
q0 〜d-N
∞
=0 Ed-I RmId)PmIv)XAl(q,q,)cd,1 C(ɪ)(hv,v0i)
V 〜Sd
q'-d-1χd	l=0
∞
=0	Ed ɪ Rm(qO)Pm(VO) X Al(q, qO)Zvd-1,(l)(VO)
V0 〜Sd
qJd-'X	l=0
= Pm (V)	E	Am (q, qO)Rm(qO).
q0 〜d-iχd
Therefore,
E	f (x)Kf (x∕√d) = ( E	Pm(V)2 )( E	Rm(q)Am(q,q0)Rm(q0)
X 〜N (0,I)	∖v 〜S d—ɪ	J ∖q,q∕ 〜d—Iχd
is nonnegative, and is positive if K is positive definite, by the assumption above that f is not a.e. zero.
Since E Pm(V)2 > 0 as Pm 6= 0, we must have
E	Rm(q)Am(q, qO)Rm(qO) ≥ 0 (or > 0 if K is positive definite).
q,q'~d-ɪXd
This argument holds for any Rm ∈ L2(d-1χ2d) that is not a.e. zero, so this implies that Al is positive
semidefinite, and positive definite if K is so.	□
44
Under review as a conference paper at ICLR 2020
Weak Spectral Simplicity Bias
Theorem 4.1 (Weak Spectral Simplicity Bias). Let K be the CK or NTK of an MLP on a boolean
cube Sd. Then the eigenvalues μk, k = 0,...,d, satisfy
μo ≥ μ2 ≥ ∙∙∙ ≥ μ2k ≥ ∙∙∙ , μι ≥ μ3 ≥ ∙∙∙ ≥ μ2k+1 ≥ ….	(9)
Proof. Again, there is a function Φ such that K(x, y) = Φ(hx, yi/kxkkyk). By Thm I.3, Φ has a
Taylor expansion with only nonnegative coefficients.
Φ(c) = ao + aιC + a2C + ∙ ∙ ∙.
By Lem I.2, Eq. (9) is true for polynomials Φ(c) = cr,
μo(cr) ≥ μ2(cr) ≥ …，μι(cr) ≥ μ3(cr) ≥ ….	(22)
Then since each μk = μk (Φ) is a linear function of Φ, μk (Φ) = P∞=0 a『μk (cr) also follows the
same ordering.	□
Lemma I.2. Let Tδ be the shift operator with step ∆ that sends a function Φ(∙) to Φ(∙ 一 ∆). Let
Φ(c) = Ct for some t. Let μdk be the eigenvalue of K(x, y) = Φ(hx, yi∕∣∣x∣∣∣∣y∣∣) on the boolean cube
㈤d. Thenforany 0 ≤ k ≤ d,
μk = 0
1 ≥ μk-2 ≥ μd ≥ 0
We furthermore have the identity
if t + k is odd
if t + k is even.
(23)
(24)
for any 2 ≤ k ≤ d.
Proof. As in Eq. (5),
d
2-d(I-T∆)k (I + T∆)d-k φ⑴=2-d X CrfkΦ
r=0
where Cr-k,k C=E Pj=0(-1)r+j (d-k)(r-j). It,s easy to observe that
Crd-k,k = Cdd--rk,k ifkiseven
Crfk = -CdUk if k is odd.
In the first case, when Φ(c) = Ct with t odd, then by symmetry μk = 0. Similarly for the second case
when t is even. This finishes the proof for Eq. (23).
Note that it,s clear from the form ofEq. (5) that μk ≤ Φ(1) = 1 always. So we show via induction
on d that the rest of Eq. (24) is true for any t ∈ N. The induction will reduce the case of d to the case
of d - 2. So for the base case, we need both d = 0 and d = 1. Both can be shown by some simple
calculations.
Now for the inductive step, assume Eq. (24) for d - 2, and we observe that
μr-2 - μk = 2-d(I - T2∕d)k-2(I + T2∕d)d-k((I + T2∕d)2 -(I 一 T2∕d)2)Φ(1)
=2-d(I 一 T2∕d)k-2(I + T2∕d)d-k(4T2∕d)Φ(1)
= 2-d+2(I 一 T2∕d)k-2(I + T2∕d)d-kΦ(1 一 2∕d).
45
Under review as a conference paper at ICLR 2020
By Eq. (5), we can expand this into
2-d+2 XX Cd-k,k-2φ(1 - 2 — r2)
= 2-d+2 XXcd”1- 2 — r2)t
dd
r=0
=(⅛" A XX C- J* J
=(d-2 )t μd-2∙
By induction, μk-2 ≥ 0, so We have
—
〃k =( d-2 )t μk-2 ≥0
as desired.
□
Theorem I.3. Let K be the CK or NTK of an MLP with domain √dS d-1 ⊆ Rd. Then K (x, y)=
Φ (1高％ ) where Φ : [-1,1] → R has a Taylor series expansion around 0
Φ(c) = ao + aιc + a?C + ∙ ∙ ∙	(25)
with the properties that ai ≥ 0 for all i and i ai = Φ(1) ≤ ∞, so that Eq. (25) is absolutely
convergent on c ∈ [-1, 1].
Proof. We first prove this statement for the CK of an L layer MLP With nonlinearity φ. Let Φl be
the corresponding Φ-function for the CK Σl . It suffices to prove this by induction on depth L. For
L = 1, Σ1(x, x0) = σw2 (n0)-1hx, x0i + σb2 by Eq. (CK), so clearly Φ1(c) = a0 + a1c Where a0 and
a1 are nonnegative.
NoW for the inductive step, suppose Σl-1 satisfies the property that Φl-1 has Taylor expansion of the
desired form. We seek to shoW the same for Σl and Φl. First notice that it suffices to shoW this for
Vφ(Σl-1), since multiplication by σw2 and addition of σb2 preserves the property. But
Vφ(Σl-1)(x,x0)=Eφ(z)φ(z0)
Where
0	N(0 (Σl-1(x, x)	Σl-1(x,x0))) N(0 (Φl-1(1)	Φl-1(c)))
(z,z )	(∑l-1 (χ,χ0)	∑l-1(χ0,χ0))) = N (φl-1(c)	Φl-1 ⑴〃.
Using the notation of Lem I.4, We can express this as
Vφ(Σl-1)(x,x0)=Φl(1)Φ ( φ^C) ).	(26)
Substituting the Taylor expansion of φ-1(C) into the Taylor expansion of φ given by Lem I.4 gives Us
a Taylor series whose coefficients are all nonnegative, since those of φj(C) and φ are nonnegative
as Well. In addition, plugging in 1 for c in this Taylor series shoWs that the sum of the coefficients
equal φ⑴=1, so the series is absolutely convergent for C ∈ [-1,1]. This proves the inductive step.
For the NTK, the proof is similar, except we now also have a product step where we multiply
Vφ0 (Σl-1) with Θl-1, and we simply just need to use the fact that product of two Taylor series with
nonnegative coefficients is another Taylor series with nonnegative coefficients, and that the resulting
radius of convergence is the minimum of the original two radii of convergence.
□
46
Under review as a conference paper at ICLR 2020
Lemma I.4. Consider any σ > 0 and any φ : R → R square-integrable against N (0, σ2). Then the
function
def E φ(x)φ(y)	σ2	σ2c
φ:	[-1, 1] →	[-1,	1],	φ(C)=	E φ(x)2	,	where (x,y)	〜N	(0,	(σ2c	σ2	)),
has a Taylor series expansion around 0
2
φ(c) = ao + aιc + a2c +------ (27)
with the properties that a% ≥ 0 for all i and Pi a% = φ(1) = 1, so that Eq. (27) is absolutely
convergent on c ∈ [-1, 1].
>Λ	Z- 1-t ∙	.1	1	∙	Cla	∙> . 1	1	∙ .	i'{'	.	.1 ∙ rɪ-i 1	♦
Proof. Since the denominator of φ doesn’t depend on c, it suffices to prove this Taylor expansion
def
exists for its numerator. Let φ(c) = φ(σc). Then
E	φ(χ)φ(y)	： (χ,y)〜N	(°,	(^c /)) = E	Φ(x)Φ(y) ： (x,y)〜N	(0,	(C	C))
By Fact I.5, this is just
b2 + bic + b2c2 + …
where b are the Hermite coefficients of φ. Since φ ∈ L2(N(0, σ2)),
Xbi2<∞
i
as desired.	□
Fact I.5 (O’Donnell (2014)). Foranyφ,ψ ∈ L2 (N(0, 1)),
E	φ(x)ψ(y)	:	(x,y)〜N	(0,	(C	C))	=	aobo	+ αιbιc +	a2b2c2	+---
where ai and bi are respectively the Hermite coefficients of φ and ψ.
Fixing k, taking the d → ∞ limit In this subsection, we prove that μk 〜d-k as d → ∞ with k
fixed. More precisely,
Theorem I.6. Let K be the CK OrNTK ofan MLP on a boolean cube ≤Pd. Then K can be expressed
as K(x, y) = Φ(hx, yi/d) for some Φ : [-1, 1] → R. If we fix k and let d → ∞, then
lim dkμk = Φ(k)(0),
d→∞
where Φ(k) denotes the kth derivative of Φ.
An immediate consequence, using the fact that the degree k eigenspace has dimension (7)〜dk/k!
for large d, is
Theorem 5.1 (Asymptotic Fractional Variance). Let K be the CK or NTK of an MLP on a boolean
cube ≤Pd. Then K can be expressed as K (x,y) = Φ(hx,yi∕d) forsome analyticfunction Φ : R → R.
If we fix k and let the input dimension d → ∞, then the fractional variance of degree k converges to
(k!)Tφ(k)(0)∕Φ(1)
(k!)-1φ⑹(0)
Pj≥o(j!)-1 Φ(j)(0)
where Φ(k) denotes the kth derivative of Φ.
Let’s first give some intuition for why Thm I.6 should be true. By Eqs. (8) and (5),
μk
1 + T∆	d-k 1 - T∆
k
Φ(1)
2
2
这 C -kW](1-r∆)
r=0	r
47
Under review as a conference paper at ICLR 2020
where Φ[k] = (1 - T∆)k Φ is the kth backward finite difference with step ∆ = 2/d. Approximating
the finite difference with derivative, we thus would expect that, as suggested by Lem I.8,
Φ[k] (x) ≈ ∆kΦ(k) (x) = (2/d)k Φ(k) (x), when d is large,
where Φ(k) is the kth derivative of Φ. Since 2⅛ (d-k) is the probability mass at 1 - r∆ of the
binomial variable B d=f ^k Pd-k Xi, where Xi is the Bernoulli variable taking ±1 value with half
chance each. This random variable converges almost surely to the delta distribution at 0, by law of
large numbers. Therefore, we would expect
μk 〜(1∕d)kΦ(k)(0)
as d → ∞.
There is a single difficulty when trying to formalize this intuition. One is the possibility that the
kth derivative Φ(k) does not exist at the endpoints 1 and -1 (note that it must exist in the interior,
because by Thm I.3, Φ is analytic on (-1, 1)). In this case, we must show that the portion of the
interval [-1, 1] near the endpoints contributes exponentially little mass to the probability distribution
of the binomial variable B, that it suppresses any blowup that can happen at the edge.
We formalize this reasoning in the proof below.
Proof. We first prevent any blowup that may happen at the edge. Let Φ[k] be the kth backward
difference of Φ with step size ∆ = 2∕d, as in Lem I.8. First note the trivial bound
Φ[k] (x) = X(-1)skΦ(x - s∆) ≤ X kΦ(x - s∆)
≤ 2kΦ(x) ≤ 2kΦ(1)
since Φ is nondecreasing by Lem I.7. Then for any R ∈ [0, (d - k)∕2 - 1],
dkμk = 2d X (d -k)φ[k](i - r∆)
r=0	r
=2TXRT(d-A(I-r∆) + dk (X + X	)(d -W-).
r=R+1	r=0 r=d-k-R
Now, in the second term, Φ[k] (1 - r∆) ≤ 2kΦ(1) as above, so that, by the binomial coefficient
entropy bound (Fact I.9),
0 ≤ dkμk	- 丁XY-	W-)	≤ 2d	(X + X )(d-	>φ(i)
2 r=R+1	r	2 r=0 r=d-k-R r
≤ d k 2 ∙ 2H(P)(d-k)Φ(1)
=	dk φ(1)	(28)
—2(1-H (p))d-(1-H (p))k-1 ,	(28)
where P = R∕(d - k). If We choose R = ∖p(d - k)C for a fixedP < 1/10, then H(P) < 1, and (28)
is decreasing exponentially fast to 0 with d.
Now we formalize the law of large number intuition. It then suffices to show that
dk
2d
d-k-R-1
X d-k Φ[k](1 - r∆) → Φ(k)(0).
r=R+1	r
We will do so by presenting an upper and a lower bound which both converge to this limit.
We first discuss the upper bound. By Lem I.8,
dk
2d
d-k-R-1	d-k-R-1
X	(d-kaki) ≤ 总 X (d-k)φ(k)(1-r∆).
r=R+1	r=R+1
48
Under review as a conference paper at ICLR 2020
The RHS can be upper bounded by an expectation over a binomial random variable: Let Xi be ±1
Bernoulli variables, and let Φr) be the function
φ(*χ) = {Φ(k)(x)
if X ∈ [一1 + 2p, 1 — 1 p]
otherwise
Note that Φ(1kp)(χ) is a bounded function. Then with the binomial variable B = d Pd-k(Xi + k/(d —
k)), we have
1
2d-k
d-k-R-1
X ( 一 k)Φ(k)(1-r∆) ≤ Eφg(B).
r=R+1	× r ×	2
By strong law of large numbers, B converges almost surely to 0 as d → ∞ with k fixed. Thus the
RHS converges to
φ(kp(0) = Φ(k)(o)
as desired.
A similar argument proceeds for the lower bound, after noting that, by Lem I.8,
1 d-k-R-1 d k	dk d-k-R-1 d k
2d-k	X d -k) Φ(k)(1-(r + k)∆) ≤ dd	X	(d-k) Φ[k](1-r∆).
r=R+1	r=R+1
□
Lemma I.7. Let K be the CK or NTK of an MLP with domain √dSd-1 ⊆ Rd. Then K (x, y)=
Φ (1高％ ) where Φ : [-1,1] → R is analytic on ( — 1,1), and all derivatives of Φ are nonnegative
on (一1, 1).
Proof. Note that, by Thm I.3, Φ has a convergent Taylor expansion on (-1, 1), i.e Φ is analytic on
(-1, 1). Thus, all derivatives of Φ exist (and are finite) on the interval (-1, 1) and have the obvious
Taylor series derived from Eq. (25). For example,
Φ'(c) = aɪe + 2a2c + 3。3/+ ∙ ∙ ∙.
Such Taylor series also converge on the open interval (-1, 1) (but could diverge on the endpoints -1
and 1). We get the desired result by noting that all a® are nonnegative.	□
By expressing finite difference as an integral, it’s easy to see that
Lemma I.8. With the same setting as in Lem I.7, let Φ[k] be the kth backward finite difference with
step size ∆ = 2/d,
Φ[k](x) d=ef (1 -T∆)kΦ(x).
Then Φ[k] is analytic on (-1, 1), has all derivatives nonnegative, and
0 ≤ ∆kΦ(k)(x - k∆) ≤ Φ[k] (x) ≤ ∆kΦ(k)(x),	(29)
where Φ(k) is the kth derivative of Φ.
Proof. Everything follows immediately from
Φ[k](x) = Z …Z φ(k)(x — hi — ・•. — hk)dhι …dhk
00
and Lem I.7.
Fact I.9 (Entropy bound on sum of binomial coefficients). For any k ≤ d and R ≤ d/2,
R
X d— k ≤ 2H(p)(d-k)
r=0 r
where p = R/(d — k), and H is the binary entropy
H(p) = —p log2 p — (1 —p) log2(1 —p).
□
49
Under review as a conference paper at ICLR 2020
probability vs rank of IO4 random networks on {±1}7
IO-1
Figure 19: The same experiments as Fig. 1 but over {0, 1}7.
J	THE {0, 1}d VS THE {±1}d B OOLEAN CUBE
Valle-Perez et al. (2018) actually did their experiments on the {0,1}d boolean cube, whereas here,
we have focused on the {±1}d boolean cube. As datasets are typically centered before feeding
into a neural network (for example, using Pytorch’s torchvision.transform.Normalize),
{±1}d is much more natural. In comparison, using the {0, 1}d cube is equivalent to adding a bias
in the input of a network and reducing the weight variance in the input layer, since any x ∈ {±1}d
corresponds to 1 (X + 1) ∈ {0,1}d. As such, one would expect there is more bias toward low
frequency components with inputs from {0, 1}d.
Nevertheless, here we verify that our observations of Section 4 above still holds over the {0, 1}d cube
by repeating the same experiments as Fig. 1 in this setting (Fig. 19). Just like over the {±1}d cube,
the relu network biases significantly toward certain functions, but with erf, and with increasing σw2 ,
this lessens. With depth 32 and σw2 , the boolean functions obtained from erf network see no bias at
all.
50