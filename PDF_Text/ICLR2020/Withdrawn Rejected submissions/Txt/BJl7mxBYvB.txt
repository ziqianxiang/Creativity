Under review as a conference paper at ICLR 2020
Robust Reinforcement Learning via
Adversarial training with Langevin Dynamics
Anonymous authors
Paper under double-blind review
Ab stract
We re-think the Two-Player Reinforcement Learning (RL) as an instance of a dis-
tribution sampling problem in infinite dimensions. Using the powerful Stochastic
Gradient Langevin Dynamics, we propose a new two-player RL algorithm, which
is a sampling variant of the two-player policy gradient method. Our new algorithm
consistently outperforms existing baselines, in terms of generalization across dif-
fering training and testing conditions, on several MuJoCo environments.
1	Introduction
Reinforcement learning (RL) promise automated solutions to many real-world tasks with beyond-
human performance. Indeed, recent advances in policy gradient methods (Sutton et al., 2000; Silver
et al., 2014; Schulman et al., 2015; 2017) and deep reinforcement learning have demonstrated im-
pressive performance in games (Mnih et al., 2015; Silver et al., 2017), continuous control (Lillicrap
et al., 2015), and robotics (Levine et al., 2016) towards this grand challenge.
Despite the success of deep RL, the progress is still upset by the fragility in real-life deployments.
In particular, majority of these methods fail to perform well when there is some difference between
training and testing scenarios, thereby posting serious safety and security concerns. To this end,
learning policies that are robust to environmental shifts, mismatched configurations, and even mis-
matched control actions is becoming increasingly more important.
A powerful framework to learning robust policies is to interpret the changing of the environment
as an adversarial perturbation. This notion naturally lends itself to a two-player minimax problem
involving a pair of agents, a protagonist and an adversary, where the protagonist learns to fulfill the
original task goals while being robust to the disruptions generated by its adversary.
Two prominent examples along this research vein, differing in how they model the adversary, are the
Robust Adversarial Reinforcement Learning (RARL) (Pinto et al., 2017) and Noisy Robust Markov
Decision Process (NR-MDP) (Tessler et al., 2019). Despite achieving impressive performance in
practice, these existing frameworks heavily rely on heuristic algorithms, and hence, suffer from lack
of theoretical guarantees, even in idealistic cases with infinite data as well as computational power.
One critical challenge in robust RL setting is that while maximizing rewards is a well-studied sub-
ject in classical/deep RL, the needed two-player minimax version is significantly more complicated
to solve both in theory and practice. For instance, Tessler et al. (2019) prove that it is in fact strictly
suboptimal to directly apply (deterministic) policy gradient steps to their NR-MDP max-min objec-
tives. Owing to the lack of a better algorithm, the policy gradient is nonetheless still employed in
their experiments; similar comments also apply to (Pinto et al., 2017).
Our paper precisely bridges this gap between theory and practice in previous works, by proposing
the first theoretically convergent algorithm for robust RL. Our key idea is to switch from optimizing
the max-min reward to sampling from the optimal randomized policies, which corresponds to finding
a mixed Nash Equilibrium (NE) (Nash et al., 1950) in the max-min objective.
It is a classical fact in game theory that, while deterministic minimax objective is often ill-posed,
the mixed NE is well-behaved under very mild assumptions. Furthermore, algorithmic approaches
to finding mixed NE with finite strategies have been studied extensively (Freund & Schapire, 1999;
Nemirovski, 2004) and is recently extended to the case of infinite strategies (Hsieh et al., 2019).
In particular, (Hsieh et al., 2019) show that, by using the Stochastic Gradient Langevin Dynamics
1
Under review as a conference paper at ICLR 2020
(SGLD) (Welling & Teh, 2011) to take samples from randomized strategies, one can find a mixed
NE of Generative Adversarial Networks (Goodfellow et al., 2014).
Our work introduces the same mixed NE perspective to the max-min objectives in robust RL, and
substantiate the ensuing theoretical framework with extensive experimental evidence. We apply the
new sampling framework to the well-known Deep Deterministic Policy Gradient (DDPG) method in
the scope of NR-MDP. We demonstrate that the new algorithm achieves clearly superior performance
in its generalization capabilities.
Intriguingly, we also observe that the idea of mixed strategy in single-player RL (i.e., the non-robust
or one-player formulation) can lead to substantially more robust policies over the standard DDPG
algorithm. More precisely, we represent the agent’s policy as a distribution over deterministic poli-
cies, and aim to sample from the distribution μ(π) 8 exp( -1J(∏)) where π denotes a deterministic
policy, J the associated expected reward, and σ a temperature parameter going to 0 during training.
Our numerical evidence demonstrates that DDPG combined with this sampling approach for the
actor update leads to learned policies that generalize better to unseen MDPs, when compared to the
state-of-the-art DDPG variant of Tessler et al. (2019) while using similar computational resources.
2	Background
2.1	Stochastic Gradient Langevin Dynamics (SGLD)
For any probability distribution P (Z) 8 exp(-g (z)), the Stochastic Gradient Langevin Dynamics
(SGLD) Welling & Teh (2011) iterates as
zk+1 - Zk - Y Vzg (Z)	+ ∙∖∕2γeξk,	(I)
z=zk
where γ is the step-size, Vzg (Z) is an unbiased estimator of Vzg (Z), e > 0 is a temperature
parameter, and ξk 〜 N (0,I) is a standard normal vector, independently drawn across different
iterations. In some cases, the convergence rate of SGLD can be improved by scaling the noise using
a positive-definite symmetric matrix C. We thus define a preconditioned variant of the above update
equation 1 as follows:
zk+1 - Zk - YC 1 hVzg (Z)]	+ P2γeC 2 ξk.	(2)
z=zk
In the experiments, we use a RMSProp-preconditioned version of the SGLD (Li et al., 2016).
2.2	Infinite-Dimensional Bi-Linear Games
In this section, we review some of the key results from (Hsieh et al., 2019). We denote the set
of all probability measures on Z by P (Z), and the set of all functions on Z by F (Z). Given a
(sufficiently regular) function h : Θ X Ω → R, consider the following objective (a two-player game
with infinitely many strategies):
E [h (θ, ω)] .	(3)
ω〜q
A pair (p*, q*) achieving the max-min value in equation 3 is called a mixed Nash Equilibrium (NE).
Define the operator G : P (Ω) → F (Θ), and its adjoint operator Gt : P (Θ) → F (Ω) as follows:
Gq (θ) := E [h (θ, ω)] ∈ F(Θ)
ω〜q
Gtp (ω) := E [h (θ,ω)] ∈ F (Ω).
θ〜P
Denoting hp, gi := E [g (Z)] for any probability measure p and function g on Z, we can write f
Z〜P
as f (p, q) = hp, Gqi = Gtp, q . Furthermore, the derivative (the analogue of gradient in infinite
dimension) of f (p, q) with respect top is simply Gq, and the derivative of f (p, q) with respect to q
is Gtp; i.e., VPf (p, q) = Gq, and Vqf (p, q) = Gtp.
max min f (p, q) := E
2
Under review as a conference paper at ICLR 2020
Algorithm 1 Infinite-Dimensional Entropic Mirror Descent-v2
Input: Initial distributions p1 , q1 , and learning rate η
for t = 1, 2, . . . , T - 1 do
pt+ι (θ) Y exp (+η Ps≤t Gqs ⑻)
qt+ι (ω) γ exp (一η Ps<t Gtps (M)
end for
Output： pτ = T PT=Ipt and qτ = T PT=I qt
Algorithm 2 Approximate Infinite-Dimensional Entropic Mirror Descent
Input: ω1,θ1 J random initialization, SGLD step-size {γt}T=ι, thermal noise of SGLD {et}T=ι,
warmup steps for SGLD {Kt}tT=1, exponential damping factor β, standard normal noise ξk, ξk0 .
for t = 1, 2, . . . , T - 1 do
ωt, ω(1) J ωt ; ©t, θ(1) J θt
for k = 1, 2, . . . , Kt do
θ(k+1) J θ(k) + γt hν∖ωt)i θ*k) + √2⅞tξk
ω(k+I) J ω(k) - γt hVωh (θt,ω)iω _ Jk) + √2γtetξk
ωt J (1 - β) ωt + βω(fc+1) ; ©t J (1 - β) ©t + βθ(fc+1)
end for
ωt+ι J (1 — β) ωt + βωt ； θt+ι J (1 — β) θt + βθ
end for
Output: ωT, ©T .
Conceptually, problem (3) can be solved via the so-called infinite-dimensional entropic mirror de-
scent; see Algorithm 1. Hsieh et al. (2019) have proved the convergence (to the mixed NE) rate of
Algorithm 1. But this algorithm is infinite-dimensional and requires infinite computational power
to implement. For practical interest, by leveraging the SGLD sampling techniques and using some
practical relaxations, Hsieh et al. (2019) have proposed a simplified variant of Algorithm 1. The
pseudocode for their resulting algorithm can be found in Algorithm 2.
3	Two-Player Markov Games
Markov Decision Process: We consider a Markov Decision Process (MDP) represented by
M1 := (S, A, T1, γ, P0, R1), where the state and action spaces are denoted by S and A respec-
tively. We focus on continuous control tasks, where the actions are real-valued, i.e., A = Rd.
T1 : S × S × A → [0, 1] captures the state transition dynamics, i.e., T1 (s0 | s, a) denotes the
probability of landing in state s0 by taking action a from state s. Here γ is the discounting factor,
P0 : S → [0, 1] is the initial distribution over states S, and R1 : S × A → R is the reward.
Two-Player Zero-Sum Markov Games: Consider a two-player zero-sum Markov game Littman
(1994); Perolat et al. (2015), where at each step of the game, both players simultaneously choose an
action. The reward each player gets after one step depends on the state and the joint action of both
players. Furthermore, the transition kernel of the game is controlled jointly by both the players. In
this work, we only consider simultaneous games, not the turn-based games.
This game can be described by an MDP M2 = (S, A, A0, T2, γ, R2, P0), where A and A0 are the
continuous set of actions the players can take, T2 : S × A × A0 × S → R is the state transition
probability, and R2 : S × A × A0 → R is the reward for both players. Consider an agent executing
a policy μ : S → A, and an adversary executing a policy V : S → A0 in the environment M. At
each timestep t, both players observe the state St and take actions at = μ (St) and at = V (St). In
the zero-sum game, the agent gets a reward rt = R2 (st, at, a0t) while the adversary gets a negative
reward -rt .
3
Under review as a conference paper at ICLR 2020
max min f (p, q) := E
This two-player zero-sum Markov game formulation has been used to model the following robust
RL settings:
•	Robust Adversarial Reinforcement Learning (RARL) (Pinto et al., 2017), where the power
of the adversary is limited by the action space A0 of the adversary.
•	Noisy Robust Markov Decision Process (NR-MDP) (Tessler et al., 2019), where A0 = A,
T2 (St+1 | st, at, at) = TI (St+1 | st, at), and R2 (st, at, at) = RI (st, at), with at =
(1 - δ)at + δa0t, for δ ∈ (0, 1). The power of the adversary is limited by δ.
In our adversarial game, we consider the following performance objective:
∞
J (μ,ν) = E X Y t-1rt μ,ν, M ,
t=1
where Pt∞=1 γt-1rt be the random cumulative return. In particular, we consider the parameterized
policies {μθ : θ ∈ Θ}, and {νω : ω ∈ Ω}. By an abuse of notation, we denote J (θ, ω) = J (μθ, Vω).
We consider the following objective:
max min J (θ, ω ) .	(4)
θ∈Θ ω∈Ω
Note that J is non-convex/concave in both θ and ω. Instead of solving equation 4 directly, we
focus on the mixed strategy formulation of equation 4. In other words, we consider the set of all
probability distributions over Θ and Ω, and we search for the optimal distribution that solves the
following program:
E [J (θ, ω)] .	(5)
ω〜q
Then, we can use the techniques from Section 2.2 to solve the above problem.
4	Experiments
In this section, we demonstrate the effectiveness of using infinite-dimensional sampling techniques
to solve the robust RL problem.
Two-Player DDPG: As a case study, we consider NR-MDP setting with δ = 0.1 (as recom-
mended in Section 6.3 of (Tessler et al., 2019)). We design a two-player variant of DDPG (Lillicrap
et al., 2015) algorithm by adapting the Algorithm 2. As opposed to standard DDPG, in two-player
DDPG two actor networks output two deterministic policies, the protagonist and adversary policies,
denoted by μθ and Vω. The critic is trained to estimate the Q-function of thejoint-policy. The gradi-
ents of the protagonist and adversary parameters are given in Proposition 5 of (Tessler et al., 2019).
The resulting algorithm is given in Algorithm 3.
We compare the performance of our algorithm against the baseline algorithm proposed in (Tessler
et al., 2019) (see Algorithm 4). (Tessler et al., 2019) have suggested a training ratio of 1 : 1 for
actors and critic updates. Note that the action noise is injected while collecting transitions for the
replay buffer. In Fujimoto et al. (2018), authors noted that the action noise drawn from the Ornstein-
Uhlenbeck Uhlenbeck & Ornstein (1930) process offered no performance benefits. Thus we also
consider uncorrelated Gaussian noise.
Setup: We evaluate the performance of Algorithm 3 and Algorithm 4 on standard continuous
control benchmarks available on OpenAI Gym Brockman et al. (2016) utilizing the MuJoCo envi-
ronment Todorov et al. (2012). Specifically, we benchmark on eight tasks: Walker, Hopper, Half-
Cheetah, Ant, Swimmer, Reacher, Humanoid, and InvertedPendulum. Details of these environments
can be found in Brockman et al. (2016) and on the GitHub website.
The Algorithm 3 implementation is based on the codebase from (Tessler et al., 2019). For all the
algorithms, we use a two-layer feedforward neural network structure of (64, 64, tanh) for both actors
(agent and adversary) and critic. The optimizer we use to update the critic is Adam Kingma & Ba
(2015) with a learning rate of 10-3. The target networks are soft-updated with τ = 0.999.
4
Under review as a conference paper at ICLR 2020
Figure 1: Average performance (over 5 seeds) of Algorithm 3 (—), and Algorithm 4 (—), under the
NR-MDP setting with δ = 0.1. The evaluation is performed without adversarial perturbations, on a
range of mass values not encountered during training.
For the baseline, the actors are trained with RMSProp optimizer. For our algorithm, the actors are
updated according to Algorithm 2 with warmup steps Kt = min 15, b(1 + 10-5)tc , and thermal
noise σt = σ0 × (1 - 5 × 10-5)t. The hyperparameters that are not related to exploration (see
Table 1) are identical to both algorithms that are compared.
5
Under review as a conference paper at ICLR 2020
Noise Probability
Noise Probability
Noise Probability
Noise Probability
Figure 2: Average performance (over 5 seeds) of Algorithm 3 (—), and Algorithm 4 (—), under the
NR-MDP setting with δ = 0.1. The evaluation is performed on a range of noise probability and
mass values not encountered during training.
And we tuned only the exploration-related hyper-parameters (for both algorithms) by grid search:
(a) Algorithm 3 with (σ0,σ) ∈ {10-2, 10-3, 10-4,10-5 } × {0, 0.01, 0.1, 0.2, 0.3, 0.4} ; (b)Al-
gorithm 4 with σ ∈ {0, 0.01, 0.1, 0.2, 0.3, 0.4}. For each algorithm-environment pair, we identified
the best performing exploration hyperparameter configuration (see Tables 2 and 3).
Each algorithm is trained on 0.5M samples (i.e., 0.5M time steps in the environment). We run our
experiments, for each environment, with 5 different seeds. The exploration noise is turned off for
evaluation.
Evaluation: We evaluate the robustness of both algorithms under different testing conditions, and
in the presence of adversarial disturbances in the testing environment. We train both algorithms
with the standard mass variables in OpenAI Gym. At test time, we evaluate the learned policies by
changing the mass values (without adversarial perturbations) and estimating the cumulative rewards.
As shown in Figure 1, our Algorithm 3 outperforms the baseline Algorithm 4 in terms of robust-
ness. We also evaluate the robustness of the learned policies under both test condition changes, and
adversarial disturbances (see Figure 2).
One-Player DDPG: We evaluate the robustness of one-player variants of Algorithm 3, and Al-
gorithm 4, i.e., we consider the NR-MDP setting with δ = 0. In this case, we set Kt = 1 for
6
Under review as a conference paper at ICLR 2020
Figure 3: Average performance (over 5 seeds) of Algorithm 3 (—), and Algorithm 4 (—), under the
NR-MDP setting with δ = 0. The evaluation is performed without adversarial perturbations, on a
range of mass values not encountered during training.
Algorithm 3 (this choice of Kt makes the computational complexity of both algorithms equal). The
results are presented in Figures 3 and 4.
Here, we remark that Algorithm 3 with δ = 0, and Kt = 1 is simply the standard DDPG with
actor being updated by preconditioned version of SGLD. Thus we achieve robustness under dif-
7
Under review as a conference paper at ICLR 2020
SGLD∕Walker2d-v2
Baseline∕HalfCheetah-v2
0.1 0.2 0.3 0.4
Noise Probability
SGLD∕Humanoιd-v2
Figure 4: Average performance (over 5 seeds) of Algorithm 3 (—), and Algorithm 4 (—), under the
NR-MDP setting with δ = 0. The evaluation is performed on a range of noise probability and mass
values not encountered during training.

0.1 0.2 0.3 0.4
Noise Probability
Noise Probability
Baselιne∕Humanoιd-v2
ferent testing conditions with just a simple change in the DDPG algorithm and without additional
computational cost.
5	Conclusion
In this work, we studied the robust reinforcement learning problem. By adapting the approximate
infinite-dimensional entropic mirror descent from (Hsieh et al., 2019), we design a robust variant of
DDPG algorithm, under the NR-MDP setting. In our experiments, we evaluated the robustness of
our algorithm on several continuous control tasks, and found that our algorithm clearly outperformed
the baseline algorithm from (Tessler et al., 2019).
References
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528,
2019.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
8
Under review as a conference paper at ICLR 2020
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games
and Economic Behavior, 29(1-2):79-103, 1999.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ya-Ping Hsieh, Chen Liu, and Volkan Cevher. Finding mixed nash equilibria of generative adver-
sarial networks. In International Conference on Machine Learning, pp. 2810-2819, 2019.
Diederik P Kingma and Jimmy Ba. A method for stochastic optimization. In International Confer-
ence on Learning Representations (ICLR), volume 5, 2015.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Chunyuan Li, Changyou Chen, David E Carlson, and Lawrence Carin. Preconditioned stochastic
gradient langevin dynamics for deep neural networks. In AAAI, volume 2, pp. 4, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings. Elsevier, 1994.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
John F Nash et al. Equilibrium points in n-person games. Proceedings of the national academy of
sciences, 36(1):48-49, 1950.
Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with
lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
SIAM Journal on Optimization, 15(1):229-251, 2004.
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming
for two-player zero-sum Markov games. In International Conference on Machine Learning, 2015.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In International Conference on Machine Learning, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In ICML, 2014.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
9
Under review as a conference paper at ICLR 2020
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing Systems,pp. 1057-1063, 2000.
Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and appli-
cations in continuous control. arXiv preprint arXiv:1901.09184, 2019.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical
review, 36(5):823, 1930.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681-688,
2011.
10
Under review as a conference paper at ICLR 2020
A Algorithms and Hyperparameter Details
Most of the values for the hyperparameters (in Table 1) are chosen from Dhariwal et al. (2017). In
addition, for Algorithm 3, we set
1.	thermal noise σt = σ° X (1 一 5 X 10-5)t, where σ° ∈ {10-2,10-3,10-4,10-5}.
2.	warmup steps Kt = min 15, b(1 + 10-5)tc
The best performing values for every environment are presented in Tables 2 and 3.
Table 1: Table of Hyperparameters
Hyperparameter	Value
critic optimizer	Adam
critic learning rate	10-3
target update rate τ	0.999
mini-batch size N	128
discount factor γ	0.99
damping factor β	0.9
replay buffer size	106
action noise parameter σ	{0, 0.01, 0.1, 0.2, 0.3, 0.4}
RMSProp parameter α	0.999
RMSProp parameter	10-8
RMSProp parameter η	10-4
Table 2: Hyperparameters chosen via grid search (for NR-MDP setting with δ = 0.1)
Algorithm 3: (σ0, σ) Algorithm 4: σ		
Walker-v2	(10-2, 0.01)	0
HalfCheetah-v2	(10-2,0)	0.01
Hopper-v2	(10-3,0.2)	0.2
Ant-v2	(10-4,0.2)	0.4
Swimmer-v2	(10-5, 0.4)	0.4
Reacher-v2	(10-3,0.2)	0.4
Humanoid-v2	(10-4, 0.01)	0
InvertedPendulum-v2	(10-3, 0.01)	0.1
Table 3: Hyperparameters chosen via grid search (for NR-MDP setting with δ = 0)
	Algorithm 3: (σ0, σ)	Algorithm 4: σ
Walker-v2	(10-2,0.1)	0.01
HalfCheetah-v2	(10-2, 0.01)	0.2
Hopper-v2	(10-5, 0.3)	0.4
Ant-v2	(10-2, 0.4)	0.4
Swimmer-v2	(10-2, 0.2)	0.3
Reacher-v2	(10-3,0.2)	0.3
Humanoid-v2	(10-2,0.1)	0
InvertedPendulum-v2	(10-3,0)	0.01
11
Under review as a conference paper at ICLR 2020
B Toy Example
Setting. We consider the following simplified/modified hide and seek (running and chasing) game
from Baker et al. (2019):
•	Two agents (a hider and a seeker) are placed in 2d-square environment without any obsta-
cles. The hider wants to be away from the seeker as much as possible.
•	The state (at time t) is represented by St = ('hide, 'Seek, Vhide, Vseek) ∈ S, where 'hide is
the location of the hider, and vtseek is the velocity of the seeker. We assume that both agents
observe the full state.
•	The action space is same for both agents, i.e., athide, atseek ∈ A = {left, right, up, down}.
•	The reward function is given by Rt = -Rhide (st) = Rseek (st)
inverse of the current distance between the hider and seeker.
1
d('hide,'seek )
, i.e.,
•	The additional parameters fhide, and f seek denote the force of the hider and the seeker
respectively. We fix fhide = 4, and let f seek ∈ {2, 3, 4}.
•	The transition dynamics T (st+ι | st, ahide, aseek; f hide, fseek) depends on the parameters
fhide , and fseek .
•	We have pre-trained (three) competitive seeker policies ∏fseek for all fseek ∈ {2, 3,4}.
Training. For training, we fix setting with f hide = 4, and f seek = 4. We train the
parametric policies πθhide , and πwseek using the following two approaches (generate an episode
. . . , St, Athide, Atseek, Rt, St+1, . . . using current parameters, and define Gt = PτT=0 γτRt+τ):
•	Gradient descent ascent (GDA) approach:
θt+ι J θt- ηtGt [Vθ ln ∏hide (AhideI St)]θ=θt
wt+1 - Wt + ηtGt [Vw ln ∏week (Aseek ∣ &兀=做，
• Stochastic Gradient Langevin Dynamics (SGLD) approach:
θt+ι 一 θt- ηtGt [Vθ ln∏hide (AhideI St)]θ=θt + PntN (0,I)
wt+1 - Wt + ηtGt [Vw lnπweek (Aseek ∣ St)] w=wt + PnN (0,I)
We denote the final hider policy parameters resulting from the above methods as θGDA, and θSGLD .
Evaluation. We compare the performance of the policies πθhide , and πθhide under three different
settings: T (st+ι ∣ st, ahide,aseek; fhide,fseek), ateek 〜πvseek (∙ ∣ st), fhide = 4, and fseek ∈
{2, 3, 4}. The results (averaged over 10 initial seeds, and 100 episodes per seed) are shown in
Figure 5. In this simplified setting, on average, πθhide performs better than πθhide , but the difference
in performance is not that significant as in the MuJoCo environments.
12
Under review as a conference paper at ICLR 2020
-SrWH
-1.45 -
-1.50-
-1.55-
-1.60-
0	50	100	150	200	250	300	350
step
Figure 5: The instantaneous reward obtained by executing πθhide , and πθhide in different settings.
C Additional Results
The results for the best performing seeds are presented in Figures 6 and 7.
13
Under review as a conference paper at ICLR 2020
Figure 6: Average performance (of the best performing seed) of Algorithm 3 (—), and Algorithm 4
(—), under the NR-MDP setting with δ = 0.1. The evaluation is performed without adversarial
perturbations, on a range of mass values not encountered during training.
14
Under review as a conference paper at ICLR 2020
Algorithm 3 Two-Player DDPG with SGLD Actors (SGLD)
Hyperparameters: see Table 1
Initialize (randomly) policy parameters ω1 , θ1 , and Q-function parameter φ.
Initialize the target network parameters ωtarg J ωι, θtarg J θι, and φtarg J φ.
Initialize replay buffer D.
Initialize m J 0 ; m0 J 0.
t J 1.
repeat
Observe state s, and select actions a = μθt (s) + ξ ; a0 = Vωt (S) + ξ0, where ξ,ξ 〜N (0, σI)
Execute the action aa = (1 - δ)a + δa0 in the environment.
Observe reward r, next state s0, and done signal d to indicate whether s0 is terminal.
Store (s, aa, r, s0, d) in replay buffer D.
If s0 is terminal, reset the environment state.
if it’s time to update then
for however many updates do
ωat , ωt() J ωt ; θat , θt() J θt
for k = 1, 2, . . . , Kt do
Sample a random minibatch of N transitions B = {(s, aa, r, s0, d)} from D.
COmPUtetargetS y (r,S0,d) = r+γ (I — d) Qφtarg (s0, (1 — 6)〃他但(s0) + δνωtarg (s0)).
Update critic by one step of (preconditioned) gradient descent using VφL (φ), where
L (O)= N X	(y (r,s0,d) - qφ Ga))2.
(s,a,r,s0 ,d)∈B
Compute the (agent and adversary) policy gradient estimates:
1-δ
Vθ J (θ,ωt) = -ɪ-xvθμθ (S) vaQφ (s, a) |a=(l—δ)μθ(s)+δνωt(s)
s∈D
δ
vω J (θt, ω) = N〉/ vω νω (S) VaQ0 (s, a) |a=(1 —δ)μθt (s)+δνω(s).
s∈D
g J [vθ∖ωt)] θ=θ(k)
; m J am + (1 — α) g Θ g ； C J diag (√m^+^e)
θ(k+1) J θ(k) + ηC-1g + √2ηjσtC-2ξ, where ξ 〜N (0,I)
g0 J [vω∖ω)[=3(k)
; m0 J αm0 + (1 - α) g0 Θ g0 ; D J
diag (√m7+^^e)
ω(k+I) J ω(k) — ηD-1g0 + √2η∕σtD- 1 ξ0, where ξ0 〜N (0, I)
ωat J (1 - β) ωat + βωt(k+1) ; θat J (1 - β) θat + βθt(k+1)
Update the target networks:
φtarg J τ φtarg + (1 — τ)φ
θtarg J τ θtarg + (1 — τ)θt( +)
ωtarg J τ ωtarg + (1 — τ)ωt(k+1)
end for
ωt+1 J (1 — β) ωt + βωat ; θt+1 J (1 — β) θt + βθat
t J t+ 1.
end for
end if
until convergence
Output: ωT, θT .
15
Under review as a conference paper at ICLR 2020
Algorithm 4 Two-Player DDPG with RMSProP Actors (Baseline)
Hyperparameters: see Table 1
Initialize (randomly) Policy Parameters ω1 , θ1 , and Q-function Parameter φ.
Initialize the target network parameters ωtarg J ωι,。1&陪 J θι, and φtarg J φ.
Initialize rePlay buffer D.
Initialize m J 0 ; m0 J 0.
t J 1.
repeat
Observe state s, and select actions a = μθt (s) + ξ ; a0 = νωt (S) + ξ0, where ξ,ξ 〜N (0, σI)
Execute the action a = (1 一 δ)a + δa0 in the environment.
Observe reward r, next state s0, and done signal d to indicate whether s0 is terminal.
Store (s, aa, r, s0, d) in replay buffer D.
If s0 is terminal, reset the environment state.
if it’s time to update then
for however many updates do
Sample a random minibatch of N transitions B = {(s, aa, r, s0, d)} from D.
COmPUte targets y (r, s0,d) = r + Y (1 一 d) Qφtarg (s0, (1 — 8)〃^^ (s0) +。％七&^ (s0))∙
Update critic by one step of (preconditioned) gradient descent using VφL (φ), where
L (φ) = NN X	(y (r, s0,d) - qφ (s, a))2.
(s,a,r,s0 ,d)∈B
Compute the (agent and adversary) policy gradient estimates:
1-8'	1 一 δ
vθ J (θ,ωt) =	vθμθ (S) VaQφ (s, a) |a=(1-δ)μθ(s)+δνωt(s)
s∈D
δ
vω J (θt,ω) = N〉J Vs νω (S) Va Qφ (s, a) Ia=(I-δ)μθt (s) + δνω (s).
s∈D
g j [vθ∖ (θ,ωt)] θ=θt
θt+1 J θt + ηC-1g
g0 J [Vω∖ω)]
ω=ωt
m J am + (1 — α) g Θ g ； C J diag (Jm + E)
; m0 J αm0 +(1 — α) g0 Θ g0 ； D J diag (mm! + e)
ωt+1 J ωt 一 ηD-1g0
Update the target networks:
;
φtarg	J	τ φtarg + (1	一 τ)φ
θtarg	J	τ θtarg + (1	一 τ)θt+1
ωtarg	J	τ ωtarg + (1	一 τ)ωt+
1
t J t+ 1.
end for
end if
until convergence
Output: ωT , θT .
16
Under review as a conference paper at ICLR 2020
Figure 7: Average performance (of the best performing seed) of Algorithm 3 (—), and Algorithm 4
(—), under the NR-MDP setting with δ = 0. The evaluation is performed without adversarial
perturbations, on a range of mass values not encountered during training.
17