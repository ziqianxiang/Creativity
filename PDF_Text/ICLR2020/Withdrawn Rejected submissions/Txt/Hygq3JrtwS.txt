Under review as a conference paper at ICLR 2020
On the reflection of sensitivity in the gener-
ALIZATION ERROR
Anonymous authors
Paper under double-blind review
Ab stract
Even though recent works have brought some insight into the performance im-
provement of techniques used in state-of-the-art deep-learning models, more work
is needed to understand the generalization properties of over-parameterized deep
neural networks. We shed light on this matter by linking the loss function to the
output’s sensitivity to its input. We find a rather strong empirical relation between
the output sensitivity and the variance in the bias-variance decomposition of the
loss function, which hints on using sensitivity as a metric for comparing gener-
alization performance of networks, without requiring labeled data. We find that
sensitivity is decreased by applying popular methods which improve the general-
ization performance of the model, such as (1) using a deep network rather than a
wide one, (2) adding convolutional layers to baseline classifiers instead of adding
fully connected layers, (3) using batch normalization, dropout and max-pooling,
and (4) applying parameter initialization techniques.
1	Introduction
In machine-learning tasks, the main challenge a network designer faces is to find a model that learns
the training data and that is able to predict the output of unseen data with high accuracy. The first
part is easily achievable in current over-parameterized deep neural networks. Yet the second part,
referred to as generalization, demands careful expert hand-tuning (LeCun et al. (2015), Goodfellow
et al. (2016)). Modern convolutional neural network (CNN) architectures that achieve state-of-the-art
results in computer-vision tasks, such as ResNet (He et al., 2016) and VGG (Simonyan & Zisserman,
2014), attain high-generalization performance. Part of their success is due to recent advances in
hardware and the availability of large amounts of data. However, among models applying these
resources, some outperform others. Hence, the key questions about when and why some models
generalize, still remain a mystery (Neyshabur et al., 2017).
In this paper, by investigating the link between sensitivity and generalization, we get one step closer
to understanding the generalization properties of neural networks. Our findings suggest a relation
between the sensitivity metric and the variance term in the bias-variance decomposition of the test
loss (Geman et al. (1992), Tibshirani (1996), Neal et al. (2018)). This relation provides us with
some intuition about the link, when the bias is small, between sensitivity and loss. We present strong
empirical results on the link between the sensitivity and the test loss in well-established deep neural
networks with ReLU (Nair & Hinton, 2010) non-linearity.
Building on this relation, we can use the sensitivity metric to examine which network is more
prone to overfitting, and we rediscover the generalization properties of certain structures. We provide
numerical evidence that a significant decrease in the network output’s sensitivity to input perturbations
can be a good predictor of the generalization capability of certain techniques used in state-of-the-art
models, such as batch normalization (Ioffe & Szegedy, 2015) and He initialization (He et al., 2015).
Sensitivity also supports the commonly accepted observation that depth is better than width in deep
neural networks (Bengio & Delalleau (2011), Montufar et al. (2014), Telgarsky (2016), Mhaskar et al.
(2017)). Furthermore, in certain settings, the sensitivity of untrained networks can be used as a proxy
for the generalization error of trained networks.
1
Under review as a conference paper at ICLR 2020
2	Related Work
Recently, Sokolic et al. (2017) suggested bounding the generalization error of deep neural networks
with the spectral norm of the input-output Jacobian matrix, a measure of output sensitivity to its
inputs. Empirical support for their conclusions is provided in Novak et al. (2018) through experiments
on fully connected neural networks. Further research is needed to determine the conditions that
are required to compare the generalization performance of models by using sensitivity metrics.
For this purpose, we provide theoretical and empirical insights to the link between sensitivity and
generalization by studying the relation between sensitivity and the variance term in the bias-variance
decomposition of the loss function. To the best of our knowledge, this work is the first to find such a
relation. In our work, we not only elaborate on the relation between sensitivity and loss for a wide
range of networks (not limited to fully connected) but also empirically show that many state-of-the-art
methods improve performance alongside reducing the sensitivity of the network.
To avoid overfitting in deep-learning architectures, regularization techniques are applied: for example,
weight decay, early stopping, dropout (Srivastava et al., 2014), and batch normalization (BN) (Ioffe
& Szegedy, 2015). Ioffe & Szegedy (2015) argues that the reason for the success of BN is that it
addresses the internal-covariant-shift phenomenon. However, Santurkar et al. (2018) argues against
this belief and explains that the success of BN is due to its ability to make the optimization landscape
smoother. Here, we look at the success of dropout and BN from another perspective. These methods
decrease the output sensitivity to random input perturbations in a same manner as decreasing the test
loss, resulting in better generalization performance.
Designing neural network architectures is one of the main challenges in machine-learning tasks. One
major line of work in this regard is comparing deep and shallow networks (Bengio & Delalleau (2011),
Mhaskar et al. (2017), Wu et al. (2019), Ba & Caruana (2014), Montufar et al. (2014), and Simonyan
& Zisserman (2014)). It is shown in Telgarsky (2016) that to approximate a deep network, a shallow
network requires an exponentially larger number of units per layer. After finding a satisfactory
architecture, the trainability of the network needs to be carefully assessed. In order to avoid exploding
or vanishing gradients, Glorot & Bengio (2010) and He et al. (2015) introduce parameter initialization
techniques that are widely used in current frameworks. By linking sensitivity and generalization, we
present a new viewpoint on understanding the success of current state-of-the-art architectures and
techniques.
Previous theoretical studies attempting to solve the mystery of generalization mostly include gener-
alization error (GE) bounds that use complexity measures such as VC-dimension and Rademacher
complexities (XU & Mannor (2012), KaWagUchi et al. (2017), Arora et al. (2018), and Sokolic et al.
(2017)). GE depends not only on the architecture of the network, but also on the structure of the
data itself1. This is why the same network has a lower GE when trained with structured data than
with random data (Zhang et al., 2016). Here, we do not study GE bounds; however, in order to
find complexity measures to bound GE, we should take note of the link between sensitivity and
generalization.
There has been research on sensitivity analysis in neural networks with sigmoid and tanh activation
functions (Dimopoulos et al. (1995), Fu & Chen (1993), and Zeng & Yeung (2001)). Yang et al. (2013)
introduce a sensitivity-based ensemble approach which selects individual networks with diverse
sensitivity values from a pool of trained networks. Piche (1995) performs a sensitivity analysis in
neural networks to determine the required precision of the weight updates in each iteration. In this
work, we extend these results to networks with ReLU non-linearity with a different goal; to study the
relation between sensitivity and generalization error in the state-of-the-art deep neural networks.
The metrics used for evaluating the network performance on training data cannot reflect generalization,
because an over-parameterized model can achieve zero training loss for networks trained with
randomly labeled data (Zhang et al., 2016). There have been recent attempts to predict the test loss for
supervised-learning tasks (Novak et al. (2018), Jiang et al. (2018), and Wang et al. (2018)). Philipp
& Carbonell (2018) introduces the non-linearity coefficient (NLC) as a gradient-based complexity
measure of the neural network, which is empirically shown to be a predictor of the test error for
fully connected neural networks. According to our results on both fully connected and convolutional
neural networks, sensitivity is also a predictor of the test error, even before the networks are trained,
which suggests sensitivity as a computationally inexpensive architecture-selection metric.
1In this work, we do not investigate the structure of the data set.
2
Under review as a conference paper at ICLR 2020
Paper Outline. We formally define loss and sensitivity metrics in Section 3.1 and Section 3.2,
respectively. In Section 4, we state the main findings of the paper and present the numerical and
analytical results supporting that. Later in Section 5, we propose a possible proxy for generalization
properties of certain structures and certain methods. Finally in Section 6, we further discuss the
observations followed up by a conclusion in Section 7. The empirical results presented in the paper
are for an image-classification task on the CIFAR-10 dataset. In Appendix 8.6, we present the
empirical results for MNIST and CIFAR-100 datasets. In Appendix 8.7, we present the empirical
results for a regression task with the Boston housing dataset.
3	Preliminaries
Consider a supervised-learning task, where the model predicts ground-truth output y ∈ Y := RK
for an input x ∈ X := RD . The predictor Fθ : X → Y is a deep neural network parameterized
by the parameter vector θ that is learned on the training dataset τt by using the stochastic learning
algorithm A. The training dataset τt and the testing dataset τv consist of i.i.d. samples drawn from
the data distribution p. With some abuse of notation, We use 〜when the samples are uniformly
drawn both from a set of samples and from a probability distribution.
3.1	Loss
Our main focus is a classification task where the loss function is the cross-entropy criterion. The
average test loss can be defined as
L = Eθ* [Lθ*] = Eθ* E(x,y)〜Tv
where θ* is the random2 parameter vector found by A minimizing the train loss defined on τt, K is
the number of classes and Fθk* is the k-th entry of the vector Fθ* , which is the output of the softmax
layer, i.e., Fθ* = softmax(fθ* (x)), where fθ* (x) is the output of the last layer of the network. In
classification tasks, the output space is Y := [0, 1]K, and the output vector is the probability assigned
to each class.
3.2	Sensitivity
Let us inject an external noise to the input of the network and compute the noise in the output. If the
original input vector is X ∈ X and we add an i.i.d. normal noise vector εχ 〜N(0, σ2xI) to the input,
then the output noise due to the input noise εx ∈ X is εy = fθ(x + εx) - fθ(x) ∈ Y. We use the
variance of the output noise3, averaged over its K entries, as a measure of sensitivity: Sθ = Var(εy).
The average sensitivity is
S = Eθ [Sθ ]= Eθ [Var(εy )]= Eθ Va”.
where εky is the k-th entry of the vector εy . In the following sections, we use Sbefore = Eθ [Sθ]
and Safter = Eθ* [Sθ*] when the expectation is over the network parameters before and after training,
respectively. We consider unspecific sensitivity (the average over the entries of the output noise),
which requires unlabeled data samples, as opposed to specific sensitivity (the sensitivity of the output
of the desired class) (Tartaglione et al., 2018). By expanding Equation (2) up to the first order, the
sensitivity S is approximated by the product of the variance of the input noise vector σε2 and the
square of the Frobenius norm of the Jacobian of the output of the network (Novak et al., 2018). In
experiments, we prefer S to the Jacobian, because in order to compute S it is enough to look at the
network as a black box that given an input, generates an output, without requiring further knowledge
of the model.
2The randomness is introduced by the stochastic optimization algorithm A and the randomized parameter
initialization technique.
3Note that we consider the output of the neural network and not the output of the softmax layer. The softmax
layer can be interpreted as a normalizing layer pushing the values to be between 0 and 1.
- XK yk log Fθk* (x)	,
(1)
KX εk，	⑵
3
Under review as a conference paper at ICLR 2020
4	Sensitivity vs. Loss
4.1	Main Observation
After training, an ideal predictor is expected to have a good generalization ability. An ideal predictor
should also be robust; given similar inputs, the outputs should be close to each other. Assuming that
the unseen data is drawn from the same distribution as the training data, there should be a link between
the two concepts of robustness and generalization. Robustness here is the average-case robustness,
not the worst-case robustness (adversarial robustness). We measure the robustness by computing S
(Equation (2)), and considering near-zero train loss, we refer to the test loss L (Equation (1)) as the
generalization error. According to our observations on a wide set of experiments, we find a rather
strong relation between the sensitivity S and the generalization error L. State-of-the-art networks
decrease the generalization error alongside with the sensitivity of the output of the network with
respect to the input (Figure 1).
Alexnet
Alexnet standard normal
4 layer CNN standard normal
VGG16
VGG13 standard normal
ReSNet18
ReSNet18 standard normal
Equation (8)
Figure 1: Sensitivity Safter versus test loss L for popular CNN architectures. The parameter initialization is
Xavier (Glorot & Bengio, 2010) with uniform distribution unless stated as standard normal distribution. The
networks are trained and fully converged on a subset of the CIFAR10 training dataset and are evaluated on the
entire CIFAR10 testing dataset. Each point indicates a network with different numbers of channels and hidden
units, and is averaged over multiple runs. For more details on configurations refer to Appendix 8.1. The Pearson
correlation coefficient ρ between data points is indicated in the figure.
4.2	Numerical Experiments
Many factors influence the generalization performance of deep-learning models, among which
network topology, initialization technique, and regularization method. We study the influence of these
three factors on the sensitivity S and keep all the other factors, including the learning algorithm, the
same. We refer to fully connected neural networks as FC and convolutional neural networks as CNN.
First, we examine the relation between Safter and L for different architectures, different initialization
techniques, and different regularizations. We observe a strong link between Safter and L; a lower
sensitivity has a lower test loss (Figures 2 and 3). This strong link suggests that, by comparing the
sensitivity Safter, we can simply compare generalization performances of multiple neural networks that
are fully trained (with various hyper-parameters, types of layer and regularization methods). Using
sensitivity as a proxy for test loss is particularly advantageous in settings where labeled training data
is limited; assessing generalization performance can be then done without having to sacrifice training
data for the validation set. Next, when initialization is fixed to the standard normal distribution and
no explicit regularization technique is applied, we investigate the link between Sbefore and L (Figure
4 in Section 6.1). We see again a clear relation between Sbefore and L; Sbefore can potentially be used
as an architecture-selection metric before training the models.
4.3	Bias-Variance Decomposition
In this section, a crude approximate relation between sensitivity and generalization error is established
through the link between sensitivity and the variance term in the bias-variance decomposition of
the mean square error. First, we find the link between the cross-entropy loss and the mean square
error. Then, we present the relation between sensitivity and the variance term in the bias-variance
decomposition of the mean square error. Finally, the link between sensitivity S and generalization
error L is established.
4
Under review as a conference paper at ICLR 2020
(a)
log(Safter)	(c)
0	10	20
(d)	log(Safter)	(e)
Figure 2: Test loss versus sensitivity for networks trained on a subset of the CIFAR-10 training dataset where the
network parameters are initially drawn from a standard normal distribution. Each point indicates a network with
different numbers of channels and hidden units and is the average over multiple runs. The interval indicates
the minimum and maximum values of sensitivity over multiple runs. (a) Fully connected neural networks.
(b) 4-layer FC trained with or without regularization. (c) Convolutional neural networks. (d) 3-layer CNN
trained with or without regularization. (e) Comparison between adding a convolutional layer and adding a fully
connected layer to a baseline classifier that is a fully connected neural network with one hidden layer.
0	5	10
log(Safter)
When the predictor Fθ* (x) assigns the probability 琦* (x) to the correct class C and 1 - Fg* (x) to
another class (see Appendix 8.3 for details), the cross-entropy loss L can be approximated as
1
L ≈ E(χ,y,θ*)	√2 t 工(fΘ* (X)-yk) I .	⑶
We roughly approximate the right-hand side in Equation (3) with a/Lmse/2, where LMSE is the mean
square error (MSE) criteria and is defined as
LMSE = Eθ* [Lθ*MSE] = Eθ* [E(x,y)〜TvhkFθ* (X)- 9『]].	(4)
Consider the classic notion of bias-variance decomposition for the MSE loss (Geman et al. (1992)),
where the generalization error is the sum of three terms: bias, variance and noise, i.e., LMSE =
εbias + εvariance + εnoise. In this work, we consider the labels to be noiseless and neglect the third
term εnoise. The bias term is formally defined as
εbias = Ex,y hkEθ*[Fθ*(x)] - yk2i ,
and the variance term is formulated as
K
εvariance = X Ex Varθ*(Fθk*(x)) .	(5)
k=1
Let us now draw an again crude approximate relation between εvariance and S under strong assumptions
on the probability distributions of θ, x, and εx (refer to Appendix 8.2 for more details). Given a
feed-forward neural network with M hidden layers and Hl, 1 ≤ l ≤ M, units per layer, where the
non-linear activation function is positive homogeneous4 with parameters α and β (Equation (10) in
Appendix 8.2), we have
K - 1	σx2
εvariance ≈	—*-	S ∙ ~2-----+ X
K	σε2x
(6)
4 ReLU is a positive homogeneous function with α = 1 and β = 0.
5
Under review as a conference paper at ICLR 2020
where
M	M	22
X= K X σbι Y (丁)σWiHi,	⑺
l=1	i=l+1
and K is the number of units in the output of the softmax layer and σw2l , σb2 , σx2, and σε2x are the
second moment of weights and biases of layer l, input x and input noise εx , respectively. Equation (7)
can be extended to convolutional neural networks by replacing Hi with fanin of weights of layer i.
Given an infinite amount of training data, the bias represents the best performance of our model
which can be approximated by the training loss (Mehta et al. (2019), Ng, Fortmann-Roe (2012)). In
our experiments, the training loss is close to zero, hence if we neglect the bias term εbias in the test
loss We have,
L ≈ S 2 (K-) (S ∙ σx^χ,	⑻
Where χ is given by Equation (7). In experiments, We observe that σb2 is usually very small or zero
(for instance in ResNets bl = 0), making χ ≈ 0.
According to Equation (6) and the relation betWeen LMSE and L, to compare netWorks With the small
value of εbias (Which is usually the case in deep neural netWorks Where the bias is approximated
With the near-zero training loss), the test loss can be approximated by the sensitivity folloWing
Equation (8). Despite the strong assumptions and crude approximations to get Equation (8), the
numerical experiments shoW a rather surprisingly good match With Equation (8) (Figures 1, 2 and 3),
When χ is neglected in Equation (8). It is interesting to note that the right-hand side of Equation (8) is
computed Without requiring labeled data points.
If εbias can no longer be approximated by the training loss, Which may in part explain the poorer
match in loWer values of Figure 1, We need more training data to make this approximation valid.
In our experiments, We train the netWorks With only a subset of the training dataset, We also
shoW in Appendix 8.8 that by training With more data samples, numerical results become closer to
Equation (8).
5	Sensitivity as a proxy for generalization
In this section, We argue that methods improving the generalization performance of neural netWorks
remarkably reduce the sensitivity S. In particular, We revisit some common notions regarding
generalization in deep neural netWorks: adding convolutional layers instead of fully connected
layers, using deeper netWorks instead of Wider ones, adding dropout, batch normalization (BN) and
max-pooling layers, and initializing the netWork parameters by techniques such as Xavier and He.
5.1	Comparing different architectures
Convolutional vs Fully Connected Layers. The relation betWeen the sensitivity S and the general-
ization error L supports the common vieW that CNNs outperform FCs in image-classification tasks.
We empirically observe that, given a CNN and an FC With the same number of parameters, the CNN
has loWer sensitivity and test loss than the FC. Moreover, some CNNs With more parameters than
FCs have both loWer sensitivity and loWer test loss, even though they are more over-parameterized.
Let us start from a baseline classifier With one hidden layer (2 layers in total), and We compare the
effect of adding another fully connected layer versus adding a convolutional layer in Figure 2(e).
We vary the number of parameters of 1-layer CNNs (Which consist of 2 fc layers and 1 conv layer)
from 450k to 10M by increasing the number of channels and hidden units, Whereas the number of
parameters for 3-layer FCs varies from 320k to 1.7M. Despite the large number of parameters of
CNNs, they suffer from less overfitting and have a loWer sensitivity S than FCs. Next, let us compare
a FC to a CNN With the same number of parameters in Figure 2(e): a 3-layer FC With 140 units
in each layer (yelloW mark) and a 1-layer CNN With 5 channels and 100 units (green mark), both
have 450k parameters. The CNN has remarkably loWer sensitivity and test loss than the FC, Which
indicates better performance compared to the FC With the same number of parameters.
Depth vs Width. Consider a feedforWard FC With ReLU activation function Where all the netWork
parameters folloW the standard normal distribution and are independent from each other and from the
6
Under review as a conference paper at ICLR 2020
input. If we have M layers with H units in each hidden layer, K units in the output layer and the
input layer with D units, then (see Appendix 8.4 for details)
D
K
S
(9)
According to Equation (9), considering two neural networks with the same value for HM , one being
deep and narrow (higher M and lower H), and the other being shallow and wide (lower M and
higher H), subject to the same noise level σε2x , the deeper network has lower sensitivity S. Assuming
both networks have near-zero training losses, then, depth is better than width regarding generalization
in fully connected neural networks. The empirical results in Figure 2(a) support Equation (9). For
instance, a 4-layer FC with 500 units per layer (the last point among 4-layer FCs), has the same
value for (H)M as a 5-layer FC with 165 units per layer (the 4th point among 5-layer FCs). In
Figure 2(a), these two networks have the same value for both sensitivity and loss, and all narrower
5-layer networks (with 100, 120, and 140 units) have better performance than the wide 4-layer
network (with 500 units). Note that Equation (9) only holds for networks with parameters drawn
from the standard normal distribution and does not hold when the parameter initialization is Xavier
or He.
5.2	Regularization and Initialization Methods
Figures 2(b) and 2(d) show the sensitivity Safter versus the test loss L, for different regularization
methods. In particular, we study the effect of dropout and BN on the sensitivity in the FCs; and
we apply dropout, BN and max-pooling for the CNNs. The results are consistent with the relation
between sensitivity S and loss L. For all mentioned regularization techniques, we observe a shift of
the points towards the bottom left; this shift suggests that these techniques improve generalization by
decreasing the network sensitivity to input perturbations. This is especially noticeable in the BN case,
where both the sensitivity and test loss decrease dramatically. This suggests that batch normalization
improves performance by making the network less sensitive to input perturbations.
Another interesting observation is the effect of various parameter initialization techniques on the
sensitivity and loss values, after the networks are trained (Figure 3). We consider four initialization
techniques for network parameters in our experiments: (i) Standard Normal distribution (SN), (ii)
Xavier (Glorot & Bengio, 2010) initialization method with uniform distribution (XU), (iii) He (He
et al., 2015) initialization method with uniform distribution (HU), and (iv) He initialization method
with normal distribution (HN). As shown in Figure 3, the relation between sensitivity Safter and test
loss L provides us a new view-point of the success of the state-of-the-art initialization techniques; the
He initialization method with normal distribution has the best generalization performance, alongside
the lowest sensitivity value.
6-7 layer FC SN
6-7 layer FC XU
6-7 layer FC HU
・	6-7 layer FC HN
-------Equation (8)
log(Safter)
Figure 3: Test loss versus sensitivity for networks trained on a subset samples of the CIFAR-10 dataset where
networks are initialized with different methods. On the right, we have a zoom in plot of the left figure.
6	Discussion
6.1	Sensitivity of untrained networks as a proxy for the test loss
A similar trend, as in Section 4, is observed for neural networks that are not yet trained. In Figure 4,
the sensitivity Sbefore is measured before the networks are trained, and the test loss L is measured after
7
Under review as a conference paper at ICLR 2020
the networks are trained on a subset of the training dataset. The parameters in the fully connected
and convolutional networks are initialized by sampling from the standard normal distribution, and
no explicit regularization (dropout, BN, and max-pooling) is used in the training process. These
two conditions are necessary, because regularization techniques only affect the training process,
hence Sbefore is the same for networks with or without regularization layers, and the He and Xavier
initialization techniques force the sensitivity to be fixed regardless of the number of units in hidden
layers. Hence, under these two conditions, the generalization performance of untrained networks
with different architectures can be compared. The strong link between the sensitivity of untrained
networks Sbefore and the test loss L suggests that generalization of neural networks can be compared
before the networks are even trained, making sensitivity a computationally inexpensive architecture-
selection method.
・	2-7 layer FC
・	1-3 IayerCNN
・	Alexnet
・	VGG11
•	VGG13
------Equation (8)
log(Sbefore )
Figure 4: Test loss of trained models
versus sensitivity of untrained models
for networks whose parameters are ini-
tially drawn from the standard normal
distribution. Note that the regulariza-
tion techniques BN, dropout and max-
pooling are removed from Alexnet,
VGG11, and VGG13 configurations.
6.2 Final Remarks
It is important to note that sensitivity is not the only factor affecting generalization. The sensitivity
metric does not address the challenges brought by Zhang et al. (2016); the sensitivity of a random-
labeled input is the same as the sensitivity of a true-labeled input, with very different loss values. The
pixel-wise linear input perturbations, considered in our experiments, might not be realistic; ideally,
we would like to perturb the input in the latent space of the generative model of the input image.
Also, the relation between S and L, requires the non-linearity to be positive homogeneous, hence the
generalization properties of networks with sigmoid and tanh activation functions remain unexplained.
It is not possible to find a single metric to explain the generalization properties across a space as
large as deep neural networks and across the non-convex landscapes of deep-learning problems. But
the sensitivity metric presented in this work is strongly related to generalization, and even though
Equation (8) was derived under very strong assumptions and crude approximations, it is surprisingly
quite close to the empirical results in a wide set of settings.
The sensitivity metric can be affected by re-scaling. If we re-scale all the indices of the very last
layer of the neural network, the accuracy stays the same as the network predicts the same class,
however, both the test loss and sensitivity are multiplied by the square of the scaling factor. Therefore,
the relation highlighted in this paper between sensitivity and generalization error cannot always be
extended to a relation between sensitivity and accuracy. Also note that, if we re-scale the input x,
define a new input XneW = αx, and de-scale the output fnew(∙) = f (∙)∕α, then due to homogeneity
of f, we have LneW = L, however, SneW = S∕α2. Hence sensitivity cannot replace test loss and to
compare the generalization error of models by using their sensitivity, the comparison needs to be
made fairly; the input of both networks should be drawn from the same distribution.
7 Conclusion
We find that the sensitivity metric is a strong indicator of overfitting. Given multiple networks, with
near-zero training loss, to choose from with various hyper-parameters, the best architecture appears
to be the one with the lowest sensitivity value. Sensitivity can potentially be used as an early stopping
criterion and as an architecture-selection method; given multiple networks to choose from by only
computing the sensitivity to the given input, the best model is the one with the lowest sensitivity. One
of the advantages of the sensitivity metric is that it can provide a loose prediction of the test loss
without the use of any labeled data. This is especially important in applications where generating
labels requires expensive experiments.
8
Under review as a conference paper at ICLR 2020
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a
compression approach. arXiv preprint arXiv:1802.05296, 2018.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural information processing
SyStemS, pp. 2654-2662, 2014.
I Bellido and Emile Fiesler. Do backpropagation trained neural networks have normal weight distributions? In
International Conference on Artificial Neural NetworkS, pp. 772-775. Springer, 1993.
Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In International Conference
on Algorithmic Learning Theory, pp. 18-36. Springer, 2011.
LCon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural information
proceSSing SyStemS, pp. 161-168, 2008.
Yannis Dimopoulos, Paul Bourret, and Sovan Lek. Use of some sensitivity criteria for choosing networks with
good generalization ability. Neural ProceSSing LetterS, 2(6):1-4, 1995.
S.	Fortmann-Roe. Understanding the bias-variance tradeoff. http://scott.fortmann-roe.com/
docs/BiasVariance.html, 2012.
Li Fu and Tinghuai Chen. Sensitivity analysis for input vector in multilayer feedforward neural networks. In
Neural NetworkS, 1993., IEEE International Conference on, pp. 215-218. IEEE, 1993.
Stuart Geman, Elie Bienenstock, and RenC Doursat. Neural networks and the bias/variance dilemma. Neural
computation, 4(1):1-58, 1992.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
In ProceedingS of the thirteenth international conference on artificial intelligence and StatiSticS, pp. 249-256,
2010.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification. In ProceedingS of the IEEE international conference on computer
viSion, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
ProceedingS of the IEEE conference on computer viSion and pattern recognition, pp. 770-778, 2016.
Sobol’ IM. Sensitivity estimates for nonlinear mathematical models. Math. Model. Comput. Exp, 1(4):407-414,
1993.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap in deep
networks with margin distributions. arXiv preprint arXiv:1810.00113, 2018.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint
arXiv:1710.05468, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On
large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836,
2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In AdvanceS in neural information proceSSing SyStemS, pp. 1097-1105, 2012.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The handbook
of brain theory and neural networkS, 3361(10):1995, 1995.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre GR Day, Clint Richardson, Charles K Fisher, and
David J Schwab. A high-bias, low-variance introduction to machine learning for physicists. PhySicS reportS,
2019.
9
Under review as a conference paper at ICLR 2020
Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. When and why are deep networks better than shallow
ones? In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of
deep neural networks. In Advances in neural information processing systems, pp. 2924-2932, 2014.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings
of the 27th international conference on machine learning (ICML-10), pp. 807-814, 2010.
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-Julien, and
Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks. arXiv preprint
arXiv:1810.08591, 2018.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep
learning. In Advances in Neural Information Processing Systems, pp. 5947-5956, 2017.
Andrew Ng. Learning Thoery Lecture Notes. http://cs229.stanford.edu/notes/
cs229-notes4.pdf.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity
and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018.
George Philipp and Jaime G Carbonell. The nonlinearity coefficient-predicting overfitting in deep neural
networks. arXiv preprint arXiv:1806.00179, 2018.
Steve W Piche. The selection of weight accuracies for madalines. IEEE Transactions on Neural Networks, 6(2):
432-445, 1995.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help
optimization? In Advances in Neural Information Processing Systems, pp. 2483-2493, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and MigUel RD Rodrigues. Robust large margin deep neural
networks. IEEE Transactions on Signal Processing, 65(16):4265-4280, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):
1929-1958, 2014.
Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng Wang. meprop: Sparsified back propagation for accelerated
deep learning with reduced overfitting. arXiv preprint arXiv:1706.06197, 2017.
Enzo Tartaglione, Skjalg Leps0y, Attilio Fiandrotti, and Gianluca Francini. Learning sparse neural networks via
sensitivity-driven regularization. In Advances in Neural Information Processing Systems, pp. 3878-3888,
2018.
Matus Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.
Robert Tibshirani. Bias, variance and prediction error for classification rules. Citeseer, 1996.
Ren6 Vidal, Joan Bruna, Raja Giryes, and Stefano Soatto. Mathematics of deep learning. CoRR, abs/1712.04741,
2017. URL http://arxiv.org/abs/1712.04741.
Timothy E Wang, Jack Gu, Dhagash Mehta, Xiaojun Zhao, and Edgar A Bernal. Towards robust deep neural
networks. arXiv preprint arXiv:1810.11726, 2018.
Zifeng Wu, Chunhua Shen, and Anton Van Den Hengel. Wider or deeper: Revisiting the resnet model for visual
recognition. Pattern Recognition, 90:119-133, 2019.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423, 2012.
Jing Yang, Xiaoqin Zeng, Shuiming Zhong, and Shengli Wu. Effective neural network ensemble approach for
improving generalization performance. IEEE transactions on neural networks and learning systems, 24(6):
878-887, 2013.
Xiaoqin Zeng and Daniel S Yeung. Sensitivity analysis of multilayer perceptron to input and weight perturbations.
IEEE Transactions on Neural Networks, 12(6):1358-1366, 2001.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
10
Under review as a conference paper at ICLR 2020
8	Appendix
8.1	Experimental Details
The CIFAR-10 dataset5 is used for the experiments presented in the paper. The fully connected neural networks
have the same number of units in the hidden layers, varying from 100 to 500 with a step size of 20. For the
convolutional neural networks the number of channels in convolutional layers vary from 5 to 25 with a step
size of 5 (note that each time a channel is added in the convolutional layers, an extra 20 units is added to the
fully connected layers of the CNN accordingly). As it is computationally expensive to reach zero training loss
for the entire dataset, we choose a subset of the training set containing 1000 samples of the CIFAR-10 dataset.
Zero training loss is necessary for a fair comparison between different networks since we would like to have the
same value for εbias and εnoise among them. For the optimization algorithm, we choose the Adam optimizer with
its default parameters and we initialize the weights and biases with random values drawn from a distribution
according to the initialization technique stated in each figure. The nonlinearity is set to be the ReLU function
throughout the experiments. We stop the training when the training loss reaches below the threshold 10-5 for 10
times. In case this condition is not met, we stop the training after 2000 epochs (each epoch is iterations over the
mini-batches of the entire training set). The noise added to the input image is a random tensor with the same size
as the input and is drawn from the Gaussian distribution with zero mean and 0.1 standard deviation. The output
noise is first averaged over all its K entries (for CIFAR-10 the number of classes is K = 10), then we take its
variance over inputs of the testing dataset and the input noise, finally it is averaged over multiple runs.
Using the notations:
•	Conv(number of filters, kernel size, stride, padding)
•	Maxpool(kernel size)
•	Linear(number of units)
•	Dropout(dropout rate)
for layers of a convolutional neural network where Conv and Linear layers also include the ReLU non-linearity
except the very last linear layer. The configurations that are used are:
•	The Alexnet (Krizhevsky et al., 2012): Conv(h, 3, 2, 1) - Maxpool(2) - Conv(3*h, 3, 1, 1) - Maxpool(2)
- Conv( 6*h, 3, 1, 1) - Conv(4*h, 3, 1, 1) - Conv(4*h, 3, 1, 1) - Maxpool(2) - Dense layer - Dropout(0.5)
- Linear(4096) - Dropout(0.5) - Linear(4096) - Linear(K)
where h ∈ [16, 32, 48, 64, 80]
•	VGG13 (Simonyan & Zisserman, 2014) : 2 x Conv(64*s, 3, 1, 1) - Maxpool(2) - 2 x Conv(128*s, 3, 1,
1) - Maxpool(2) - 2x Conv(256*s, 3, 1, 1) - Maxpool(2) - 2 x Conv(512*s, 3, 1, 1) - Maxpool(2) - 2x
Conv(512*s, 3, 1, 1) - Maxpool(2) - Avgpool(2) - Dense layer - Linear(K)
where s ∈ [0.25, 0.5, 1, 1.5, 2] and all Conv layers have batch normalization
•	Each block of a ResNet (He et al., 2016) configuration: 2 x Conv(h, 3, 1, 1) + Conv(h, 1, 1, 1) which
Conv layers include BN and ReLU and the result of the summation goes into a ReLU layer and h is
the number of channels.
VGG16 is the same as VGG13 with the difference that it has three layers in the last three blocks. VGG11
configuration is the same as tVGG13 except that in the first and second block it has one convolutional layer
instead of 2. VGG19 is the same as VGG13 except that there is 4 conv layers instead of 2 in the last three blocks.
ResNet18 has 2 blocks with h=64*s, 2 blocks with h=128*s, 2 blocks with h=256*s, and 2 blocks with h=512*s
where s ∈ [0.25, 0.5, 1, 1.5, 2]. ResNet34 has 3 blocks with h=64*s, 4 blocks with h=128*s, 6 blocks with
h=256*s, and 3 blocks with h=512*s where s ∈ [0.25, 0.5, 1, 1.5, 2].
8.2	Computation of Equation (6)
Computations of this section do not depend on the stage of the training, hence θ denotes the parameter vector at
any stage of training. Let us recall the sensitivity metric (Equation (2)) definition
S = Eθ [Varx,εχ [εy]] ,
where εy = 1 /K PK=I εk and εk is the k-th entry of output noise vector εy and is
εk = fk(X + εχ)- fk(x) U εχ ∙V>fk(x),
where we apply a first order Taylor expansion of the output. For a one hidden layer neural network with D input
units, H hidden units, and K output units, we have θ = {w1 ∈ RD×H , w2 ∈ RH ×K , b1 ∈ RH , b2 ∈ RK }
5https://www.cs.toronto.edu/~kriz/cifar.html
11
Under review as a conference paper at ICLR 2020
where wl and bl are the weights and biases of layer l (l = 1 is the hidden layer and l = 2 is the output layer)
which are independently drawn from a zero-mean normal distribution: wι 〜N(0, σW 11), w2 〜N(0, σW21),
bi 〜N(0, σ2ιI), and b2 〜N(0, σ2ζI) (which has been studied in Bellido & Fiesler (1993)). We have
H
k	hk h k
fθ (x) =	w2 a(p ) + b2 ,
h=1
where wljk is the weight connecting unit j in layer l to unit k in layer l + 1, blh is the bias term added to unit h in
layer l + 1, ph is the output of the linear transformation in the hidden unit h, i.e.,
D
h	dh d h
p =	w1 x + b1 ,
d=1
and the non-linear activation function a(∙) is a positive homogeneous function of degree 1; i.e.,
αx x > 0,
a(x) =
βx otherwise,
(10)
where α and β are non-negative hyper-parameters. ReLU follows Equation (10) with α = 1 and β = 0. By
applying the chain rule we obtain
k
εy
D
Xεdx
d=1
D
∂fθ (x)
∂xd
Xd	hk dh
εx	w2 w1
d=1	h=1
∂a(ph)
∂ph .
H
Therefore, we have
——_ 1 ∖^∙^	d hk dh da(Ph)
εy = κXXXεχw2 w1	∂ph .
The network parameters are assumed to be independent, and it is assumed that x ⊥ θ, and εx ⊥ {θ, x}.
Moreove, the entries of the input vector x are independent from each other with the same second moment,
i.e., σx2 = E[(xd)2] for 1 ≤ d ≤ D. Consider the input noise εx to be a vector of zero mean random variables,
hence S = Eθ,x,εx [(εy)2]. Then sensitivity becomes
(Whk)2(wdh)2 (空)2
KHD	2	2
1	2	2	2 α +β
K2 ʌ ʌ Lx σ^εχ σw2 σwι	2
(11)
k=1 h=1 d=1
where the second equation is followed by computing the expectation for zero-mean normal parameters. Let
var = Ex [Varθ [out]] ,	(12)
where out = 1/KPk=I fk(x). Because of the homogeneity of the non-linearity a(∙), we have a(ph) =
ph ∙ da(ph). Hence
∂ph
t 1 X
out = K 上
k=1
H
Xhk
w2
h=1
Because the parameters are zero-mean, var = Eθ,x out2 and we have
k=1 h=1 d=1
α2 + β2
-2-
KH	2	2	K
1	2	2 α +β 1	2
+ K Σ/ Σ/ σw2 σb1 -2- + K Z∑σb2
12
Under review as a conference paper at ICLR 2020
which is followed by taking the expectations over the parameters with zero-mean normal distributions. Therefore,
we obtain
_ c σ2 IH 2 2 α2 + β2 l σ22
Var = S ∙ σ2~ + Kσw2 σb -2- + ɪ,
εx
where σ2 is the notation used for the second moment of a random variable. Following the same computations
for a neural network with M hidden layers, we have
2	M	M2	2
σx	1	2 α + β 2
Var = S ∙ σ2~ + K 工 σb H —2— σWi Hi,
εx	l=1	i=l+1
(13)
where K is the number of units of the output layer M + 1. We refer to second term in the right-hand side of
Equation (13) as χ. Its value is a very rough approximation given the numerous assumptions made above, but
in practice it can often be neglected because σb2 is very small or zero (the ResNet configurations do not have
biases) in most of our experiments.
The first order Taylor expansion for an arbitrary function at the average of the input is g(x) ≈ g(E[x]) +
g0 (E[x])(x - E[x]). Taking the variance of this equation we have
Var(g(x)) ≈ (g0(E[x]))2 Var(x).
Here the function g(∙) is the Softmax function
Fθk(x)
exp(fθ(x))
PK=I exPfθ(X))
The input of the softmax function is a K-dimensional vector, so the first order Taylor expansion includes the
vector-matrix multiplication of the covariance and the input vector. We assume the outputs of the last layer are
independent from each other, so the covariance matrix is considered a diagonal matrix. Because the parameters
are considered to be zero-mean, the input of the softmax has zero mean, E[fθk(x)] = 0 for 1 ≤ k ≤ K, then
Var(Fk(X)) ≈ X (fX) )2
i=1	θ
Var(fθi(X))
≈ (K ∙ (1 - K))Varfk(X))+(K) X var(fθ(X)),
ii6==k1
since softmax(0) = 1/K. Therefore,
εvariance
X Ex	hvar(Fk (x))i	≈ ( (K-I)2 + K-1)	∙ K2var =	(K-1)(S	∙ σ2-	+ χ),
y x	[ i θι 力 ∖ K4 TK4 )	∖ K )	∖ σ2x χ7	,
which completes the computations.
8.3 The relation between the cross entropy loss and the mean s quare error
We rewrite the cross-entropy loss (Equation (1)) as
L = Eθ* [Lθ*] = Ex,c,θ* [— log(Fθ* )],
where 1 ≤ c ≤ K is the index of the true class for the input X, i.e., yc = 1 and yk = 0 for k 6= c . For simplicity
we use the notation Fθc* instead of Fθc* (X) in this section. For the MSE loss we have
LMSE = Ex,y,θ*
k=1
K
Because P Fθk*
k=1
K
Fθc* + P Fθj*
j =1
j 6=c
1 the summation inside the above expectation can be rewritten as
K
XFθk*
k=1
-yk2=(1-Fθc*)2+XK	Fθj*2
j=1
j 6=c
KK
(1-Fθc*)2+(1-Fθc*)2-XXFθi*Fθj*
i=1 j =1
i6=c j6=i,c
KK
2(1-Fθc*)2-XXFθi*Fθj*.
i=1 j =1
i6=c j6=i,c
13
Under review as a conference paper at ICLR 2020
K
Since 0 ≤ Fj ≤ (1 — Fθ*) for 1 ≤ j ≤ K,j = C and P Fj = 1 — Fθ*, the above equation is bounded as
j=1
j6=c
KKI) (1 — Fθ* )2 ≤ XX (Fk* — yk)2 ≤ 2(1 — Fθ*)2 .
k=1
(14)
The minimum occurs when Fθj* = (1 — Fθc* )/(K — 1) for 1 ≤ j ≤ K, j 6= c and the maximum occurs when
all the remaining probability (i.e., 1 — Fθc* ) is given to one class besides the true class c, and the rest of the
classes are assigned with zero probability. By applying the first order Taylor expansion for logarithm we have
— log(Fθc* ) ≈ 1 — Fθc* ,
where following the inequality in Equation (14) we have
t 2 XX (Fk* — yk)2 ≤1 — Fθ* ≤ t KKi XX (Fk* — yk)2.
Intuitively, the upper bound above is preferable in practice, because we would like to be less confidant in assigning
probabilities to wrong classes. If we take the expectation of the above inequality because of Jenson’s inequality,
the upper bound is upper bounded by JK- √Lmse. However, here We consider the worst case approximation
and approximate 1 — Fθc* with the lower bound above. In practice, we observe that this approximation holds,
and the network overfits very confidently and assigns the probability 1 — Fθc* to a wrong class and zero to the
remaining classes. Hence, by roughly approximating the expectation of a squared root with the squared root of
expectation, we have
8.4 Computation of Equation (9)
Consider a feedforward FC with ReLU activation function where i.i.d. zero mean random noise εx with
variance σε2x is added to the input. Then, assuming the output noise entries are independent from each other, we
have
KK
S = K X Varhεki = K X Eh(Ek)2i.
k=1	k=1
If we have M hidden layers with Hl , 1 ≤ l ≤ M units per layer, assuming the parameters are i.i.d. and
independent from the input noise εx, and are drawn from the standard normal distribution, following the same
computations as in Equation (11) for a network with M hidden layers, D input units and K output units,
1 K	MH
S=K x Dσεχ Y ɪ.
k=1	l=1
If all the hidden layers have the same number of units, Hi = H2 =…=HM = H, then,
8.5	CIFAR- 1 0 experiments
Figure 5 presents the effect of different initialization techniques, and of adding dropout and batch normalization
layers to fully connected and convolutional neural networks trained on 1000 samples of the CIFAR-10 training
dataset, and evaluated on the entire CIFAR-10 testing dataset. We observe again the strong relation between
sensitivity Safter and generalization error L and the effect of these techniques on both Safter and L. In Figure 6, we
present the empirical results on the relation between var defined in Equation (12) and S defined in Equation (2).
We experiment for 5 cases, where we change the second moment of the input σx2 and the input noise σε2x . In
Figures 6(a) and 6(b), the original CIFAR-10 images are considered and in Figures 6(c), 6(d) and 6(e), we
normalize the inputs accordingly. In all figures, the empirical relation between var and S shows a good match
with Equation (13). We further experiment on image classification tasks for the MNIST and CIFAR-100 datasets
in Appendix 8.6. We also provide numerical experiments for a regression task for the Boston house price
prediction in Appendix 8.7.
14
Under review as a conference paper at ICLR 2020
log(Safter)
log(Safter)
(a)	Effect of initialization
(b)	Effect of adding dropout
• networks
• networks+BN
------Equation (8)
log(Safter)
(c) Effect of adding batch normalization
Figure 5: Test loss versus sensitivity for networks trained on 1000 samples of the CIFAR-10 training dataset
presenting the effect of initialization, dropout and batch normalization. Each point is the average over multiple
runs and indicates a different architecture. (a) The networks are 5 layer FC, 2-4 layer CNN where the parameters
are initially drawn from either Xavier uniform distribution (XU) or standard normal distribution (SN). (b) The
networks are 3, 5, 7 layer FC and 1-4 layer CNN. The top most right pink point is the same network architecture
as the top most right teal point when dropout is added to the configuration. Hence, for all network architectures
we observe a shift of the numerical points towards bottom left of the figure when dropout is applied. (c) The
networks are 3, 5, 7 layer FC and 1-4 layer CNN. In (b) and (c) the networks parameters are initially drawn from
the standard normal distribution.
8.6	MNIST and CIFAR-100 experiments
In this section, we present the experimental results for networks trained on 6000 samples of the MNIST6 training
dataset and evaluated on the entire MNIST testing dataset. Figures 7(a) and 7(b) show the results for fully
connected neural networks with different number of layers and units and using regularization techniques batch
normalization and dropout. In Figures 7(c) and 7(d), the results for convolutional neural networks are presented.
Finally, in Figures 7(e) 7(f), the results on the comparison of the sensitivity of untrained networks Sbefore with the
test loss L after the networks are trained, are presented. Figure 8 presents the sensitivity S versus the loss L for
networks trained on the 1000 samples of the CIFAR-100 dataset7. The results on these two datasets are consistent
with the rest of the paper, and once again we observe the relation between sensitivity and generalization and the
effect of state-of-the-art techniques on both sensitivity and generalization.
8.7	Regression Task and MSE Loss
In this section, we investigate the relation between sensitivity and generalization error for regression tasks with
mean square error criteria. The loss function in this setting is defined as
LMSE = Eθ* [Lθ*MSE] = Eθ* [E(x,y)〜Tv [fθ* (X) - y)2]].
where θ* is found by minimizing the mean square error on training dataset Tt using the stochastic learning
algorithm A. Note that in this setting the output is the last layer of the neural network, the softmax layer is not
added in this setting and the output layer has 1 unit, i.e., K = 1 and y is a scalar. The bias and variance term are
6http://yann.lecun.com/exdb/mnist/
7https://www.cs.toronto.edu/~kriz/cifar.html
15
Under review as a conference paper at ICLR 2020
)rav(go
0
000
321
0	10	20
log(Safter)
3 layer FC SN
・	3 layer FC XU
3 layer FC HN
3 layer FC SN BN
■	3 layer FC SN dropout
・	4 layer FC SN
4 layer FC XU
4 layer FC HN
・	4 layer FC SN BN
ResNet18 HN
ResNet34 HN
・	ResNet50 HN
・	VGG11 HN
VGG13 HN
VGG16 HN
・	VGG19 HN
Equation (13)
(a) σx2 = 0.2648 and σε2 = 0.01
000
21
)rav(gol
3 layer FC SN
3 layer FC XU
3 layer FC HN
3 layer FC SN BN
3 layer FC SN dropout
4 layer FC SN
4 layer FC XU
4 layer FC HN
4 layer FC SN BN
VGG11 HN
Equation (13)
0	10	20
log(Safter)
(b) σx2 = 0.2648 and σε2 = 0.04
)rav(go
3 layer FC SN
3 layer FC XU
3 layer FC HN
3 layer FC SN BN
3 layer FC SN dropout
4 layer FC SN
4 layer FC XU
4 layer FC HN
4 layer FC SN BN
VGG11 HN
Equation (13)
)rav(gol
0.9501
-10	0	10	20
log(Safter)
3 layer FC SN
3 layer FC XU
3 layer FC HN
3 layer FC SN BN
3 layer FC SN dropout
4 layer FC SN
4 layer FC XU
4 layer FC HN
4 layer FC SN BN
VGG11 HN
Equation (13)
(c) σx2 = 1 and σε2 = 0.01
000
321
log(Safter)
3 layer FC SN
3 layer FC XU
3 layer FC HN
3 layer FC SN BN
3 layer FC SN dropout
4 layer FC SN
4 layer FC XU
4 layer FC HN
4 layer FC SN BN
Equation (13)
(d)	σx2 = 1 and σε2 = 0.04
(e)	σx2 = 1.25 and σε2 = 0.01
Figure 6: var (Equation (12)) versus S (Equation (2)) for networks trained on 1000 samples of the CIFAR-10
training dataset. The non-linearity is ReLU, and χ is neglected in the computation of Equation (13) in the figures.
(a), (b) The non-normalized original CIFAR-10 input images. (c), (d) Normalized input images with zero-mean
and unit variance. (e) We normalize the inputs to have unit variance and the mean is kept the same as the original
images.
defined as
εbias = Eχ,y [(Eθ* [fθ* (x)] - y)2],
and
εvariance = Ex [Varθ* (fθ* (x))] ,
respectively. We consider the Boston housing dataset8 where the objective is to predict the price of a house given
14 features (including crime rate, distance to employment centers, etc.). Figure 9 shows the results for comparing
sensitivity and test loss among fully connected neural networks with 3-8 layers and 100-500 hidden units per
8https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html
16
Under review as a conference paper at ICLR 2020
layer; the networks are trained on 70% of the dataset and then evaluated on the remaining 30%. The networks
could not get to zero training loss, so we stopped the training after 1000 epochs. The results are consistent on the
relation between sensitivity S and generalization error which for the regression task is LMSE (Figure 9(e)). For a
more detailed view, we observe that sensitivity is related to the variance in the bias-variance decomposition of
the MSE loss (Figure 9(d)), and the MSE loss is the summation of the bias term and the variance term (Figure
9(c)).
8.8 DISCUSSION REGARDING εBIAS
In this section, we present the results comparing Safter vs. L when the networks are trained on different numbers
of training samples (Figures 10(b) and 10(c)) and at different stages of training (Figure 10(d)) to validate the
approximation made in Equation (8) where we neglect εbias and εnoise. Figure 10(a) shows sensitivity versus
loss after the networks are trained on a subset of 1000 samples of the CIFAR-10 training dataset. Clearly, the
approximations made in the computation of Equation (8) do not hold for ResNet18 and ResNet34 networks
and the match is poor, contrary to the other part. Figure 10(b) shows that this problem can be explained (at
least in part) by training the networks with more samples. For instance, in Figure 10(b) the yellow mark is
ResNet18 with s=1, and the green mark is Resnet34 with s=0.5, and the transition of the results, towards the
linear relation between log S and log L, is clearly observed as we add more training data samples in the training
process. Therefore, the larger the number of training samples, the better the approximation εbias ≈ trainloss
becomes and εvariance becomes the dominant term in the test loss. Figure 10(d) shows the effect of the stage of
the training. Both the sensitivity S and the loss L are computed at different stages of training (with some abuse
of notation for L). In this figure, we observe the effect of the stage of training on the approximations made for
computing Equation (8). At initial stages of the training, the bias εbias and noise εnoise term in the test loss cannot
be neglected.
17
Under review as a conference paper at ICLR 2020
Alexnet
Alexnet SN
4 layer CNN SN
VGG16
VGG13 SN
Resnet18
Resnet18 SN
Equation (8)
(a)	Replicate of Figure 1 for MNIST
log(Safter)
log(Safter)
(b)	Fully connected neural networks (c) 4-layer fully connected neural network
trained with or without regularization
log(Safter)
(d) Convolutional neural networks (e) 2-layer convolutional neural networks
trained with or without regularization tech-
niques
log(Sbefore)
log(Sbefore )
(f)	Sensitivity before training for FCs
(g)	Sensitivity before training for CNNs
Figure 7:	Test loss versus sensitivity for networks trained on 6000 samples of the MNIST training dataset. Each
point indicates a network with a different width and the sensitivity and test loss are averaged over multiple runs.
18
Under review as a conference paper at ICLR 2020
■ Alexnet
• Alexnet SN
4 layer CNN SN
VGG16
・ VGG13 SN
•	ReSnet18
ReSnet18 SN
------Equation (8)
(a) Replicate of Figure 1 for CIFAR-100
log(Safter)
2 layer FC
・	3 layer FC
■	4 layer FC
•	5 layer FC
■	6 layer FC
・	7 layer FC
1 layer CNN
2 layer CNN
・	3 layer CNN
-------Equation (8)
(b)	SenSitivity after training vS teSt loSS
(c)	Effect of regularization on fully connected
neural networkS
•	2 layer FC
3 layer FC
•	4 layer FC
■	5 layer FC
•	6 layer FC
・	7 layer FC
.1 layer CNN
2 layer CNN
3 layer CNN
-------Equation (8)
log(Safter)
(d) SenSitivity before training vS teSt loSS
(e) Effect of regularization on convolutional neu-
ral networkS
Figure 8:	L verSuS S for networkS trained on 1000 SampleS of the CIFAR-100 training dataSet. Each point
indicateS a network with a different width and the SenSitivity and teSt loSS are averaged over multiple runS.
19
Under review as a conference paper at ICLR 2020
• 3 layer FC
40
)ESML(gol
log(εvariance)
・ 4 layer FC
• 5 layer FC
• 6 layer FC
♦ 7 layer FC
• 8 layer FC
)ESML(gol
40
000
321
log(εbias)
(b)
• 3 layer FC
♦ 4 layer FC
• 5 layer FC
• 6 layer FC
・ 7 layer FC
• 8 layer FC
(a)
0000
4321
)ESML(gol
ρ = 0.9999
・	3 layer FC
•	4 layer FC
•	5 layer FC
・	6 layer FC
•	7 layer FC
•	8 layer FC
) ecnairavε(gol
000
42
log(Safter)
(d)
0	20	40
・	3 layer FC
•	4 layer FC
•	5 layer FC
♦	6 layer FC
•	7 layer FC
•	8 layer FC
10	20	30	40
c
0000
4321
) ESML(gol
ρ = 0.7681
40
•	3 layer FC
•	4 layer FC
•	5 layer FC
•	6 layer FC
•	7 layer FC
•	8 layer FC
log(Safter)
(e)
Figure 9:	Variance, bias and sensitivity versus test loss for a regression task using the MSE loss. The fully
connected neural networks are trained and evaluated on the Boston house price dataset. Each point indicates an
average over multiple runs of a network with different widths H.
20
Under review as a conference paper at ICLR 2020
dataset
(c) 1-2 layer CNN
12
Figure 10: Sensitivity versus test loss for networks at different stages of training and trained on different numbers
of training samples. Each point indicates an average over multiple runs of a network with a different width and
depth. (b) is the zoom in of (a) on the bottom left, and we add the results of the same networks trained with
different numbers of samples. In (b) The network parameters are drawn from a normal distribution by using the
He technique. (c) and (d) are the zoom in of (a) on the top left, and we add the results for the same networks
trained with different number of training samples in (c) and at different stages of training in (d). In (c) and (d)
the network parameters are drawn from the standard normal distribution.
10
8
6
4
log(S)
(d) 1-2 layer CNN
21