Under review as a conference paper at ICLR 2020
Continual Learning with Gated Incremental
Memories for Sequential Data Processing
Anonymous authors
Paper under double-blind review
Ab stract
The ability to learn over changing task distributions without forgetting previous
knowledge, also known as continual learning, is a key enabler for scalable and
trustworthy deployments of adaptive solutions. While the importance of continual
learning is largely acknowledged in machine vision and reinforcement learning
problems, this is mostly under-documented for sequence processing tasks. This
work focuses on characterizing and quantitatively assessing the impact of catas-
trophic forgetting and task interference when dealing with sequential data in re-
current neural networks. We also introduce a general architecture, named Gated
Incremental Memory, for augmenting recurrent models with continual learning
skills, whose effectiveness is demonstrated through the benchmarks introduced in
this paper.
1	Introduction
Continual Learning (CL) can be defined as “the unending process of learning new things on top of
what has already been learned” (Ring, 2011).
A more formal definition centered around the computational aspects of a CL algorithm is presented
in Lesort et al. (2019):
Definition 1 Given a potentially infinite sequence of unknown distributions D = {D1 , D2, D3, ...},
where each Di = {Xi, Yi} includes the input data Xi and the target labels Yi, a Continual Learning
algorithm AiCL is defined by the following signature:
∀Di ∈ D,	AiCL ≡ < hi-1 , TRi , Mi-1 , ti > → < hi, Mi >
where T Ri is the training set drawn from the corresponding data distribution Di, hi is the function
that the model has learned after having seen all the training sets up to the i-th, Mi is an external
memory in which to store patterns from TRi and ti is the label identifying the task.
In this paper, the notion of task is associated to a specific objective (e.g. learn to classify MNIST
digits). An input distribution, instead, generates data related to a particular task (e.g. subsets of
MNIST digits). Hence, each task is associated to multiple input distributions (also called subtasks).
In the CL scenario, a learning model is required to incrementally build and dynamically update
internal representations as the distribution of tasks varies across its lifetime. Ideally, part of such in-
ternal representations will be general and invariant enough to be reusable across similar tasks, while
another part should preserve and encode task specific representations. Unfortunately, continuous
plasticity of internal representations under drifting task distributions is widely known to suffer from
negative interference between the tasks that are incrementally presented to the model, yielding to
the well known stability-plasticity dilemma (Parisi et al., 2019; Zhou et al., 2012) of connectionist
models. The result of this being models which catastrophically forget previously acquired knowl-
edge (Robins, 1995; French, 1999) as new tasks become available, that is one of the main challenges
faced by CL algorithms.
While current trends in CL put particular emphasis on computer vision applications or reinforce-
ment learning scenarios, sequential data processing is rarely taken into consideration (see Sec-
tion 2). Sequential data processing in CL involves a stream D of distributions of sequences
Di = (Xi, Yi), i = 1, 2, 3, ..., where Xi are sequences of vectors and Yi are the corresponding
targets. In Machine Learning (ML), sequential data processing plays a fundamental role in impor-
tant fields like Natural Language Processing (NLP), signal processing, bioinformatics and many
1
Under review as a conference paper at ICLR 2020
others. In this context, Recurrent Neural Networks (RNNs) provide an effective means to deal with
sequential data, thanks to their adaptive memory, that is the ability of developing neural represen-
tations that capture the history of their inputs. Learning proper memory representations is a major
challenge for RNNs. Especially in a CL setting, where RNNs have also to deal with drifts in task
distributions which can greatly affect their capability of developing robust and effective memory
representations.
In this work, we provide a threefold contribution to the discussion concerning CL in sequential data
processing. First, we introduce benchmarks for the evaluation ofCL skills by adapting three sequen-
tial datasets to CL scenarios. These benchmarks can be easily tuned to varying degrees of complexity
and hardness, and they have been devised to provide an assessment that is not tailored to a specific
application domain (e.g. NLP, RL or computer vision). Rather, they aim at evaluating the intrin-
sic effectiveness of recurrent models in a CL setting. Second, we define a new dynamic approach
that imbues RNN architectures with CL skills by incrementally adding new modules to capture the
shift in task distribution while avoiding catastrophic forgetting. This architecture, named Gated In-
cremental Memory, leverages gating autoencoders to automatically recognize input distributions in
order to avoid any explicit supervision concerning their identity at inference time. Third, we eval-
uate baseline RNN models on the proposed benchmarks, comparing their performance against our
enhanced architecture. Source code and full details regarding the experimental setup are provided
as supplemental material, to ease replicability of the results and allow other researchers a direct and
fair comparison of their own CL models on our benchmarks.
The results of our empirical analysis confirm the advantages of using our enhanced architecture over
standard recurrent models. Such advantages are particularly clear when testing enhanced architec-
ture on old distributions, since it successfully prevents forgetting, when learning sequences with
increasing length (in the Copy Task) and when the input sequences end with a common suffix (in
the Sequence Classification SSMNIST Task). These results highlight some of the key differences
between feedforward and recurrent CL techniques, and pinpoint the need for solutions specifically
designed for recurrent architectures.
2	Related Work
CL literature mostly focuses on computer vision and reinforcement learning applications, with ap-
proaches ranging from regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017), to dual
models (Hinton & Plaut, 1987; French, 1997), to dynamic architectures (Rusu et al., 2016; Yoon
et al., 2018). The work of Coop & Arel (2013) is one of the first attempts to apply RNNs in CL
scenarios. In their paper, they employed a RNN with Fixed Expansion Layer (FEL) (Coop & Arel,
2013) and trained the resulting model to reconstruct auto-associative binary sequences. The popular
Copy task (Graves et al., 2014) and Sequential Stroke MNIST dataset (de Jong, 2016) have been
used by (Sodhani et al., 2018) to test the capabilities of dynamic recurrent models in presence of
increasingly longer sequences. By using dynamic external memories, Asghar et al. (2018) success-
fully applied RNNs to CL on NLP tasks with MNLI dataset. Finally, Kemker et al. (2018) applied
feedforward networks to the Audioset dataset (Gemmeke et al., 2017) in a CL setting. Our dynamic
RNN architecture is inspired by Progressive networks (Rusu et al., 2016), a CL approach popular in
the feedforward domain that deals with new distributions by dynamically adding new hidden units
to a feedforward layer and connecting them to the existing hidden units through lateral connections.
The gating autoencoders are adapted to sequential scenarios from Aljundi et al. (2017), where they
are first introduced for feedforward models.
3	Gated Incremental Memories for continual learning with
RECURRENT NEURAL NETWORKS
In this section, we introduce Gated Incremental Memory (GIM), a novel CL architecture designed
for recurrent neural models and sequential data. In particular, we show how GIM can be obtained
by combining together a recurrent version of the Progressive network (Rusu et al., 2016) and a set
of gating autoencoders (Aljundi et al., 2017) in order to avoid, at test time, any explicit supervision
about subtask label. We denote the recurrent version of the Progressive network as Augmented
models (A-LMN and A-LSTM), while the entire architecture, including both augmented archi-
tectures and gating autoencoders, is called GIM. To highlight the generality of our approach, we
2
Under review as a conference paper at ICLR 2020
detail the application of GIM to two substantially different recurrent neural models from literature
(GIM-LMN, GIM-LSTM). In the following, we indicate an entire sequence with bold notation (e.g.
x), while a single vector with plain formatting (e.g. xi).
3.1	Recurrent Neural Networks
The proposed approach is independent on the underlying recurrent architecture. In this work, we
focus our study on two different classes of RNNs, using either gated and non-gated approaches.
Gated models, like LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Cho et al., 2014), leverage
adaptive gates to enable selective memory cell updates. In our analysis, we consider LSTM as a
representative of gated architectures, given its popularity in literature and its state-of-the-art perfor-
mances in several sequential data processing benchmarks. Non-gated approaches rely on different
mechanisms to solve the vanishing gradient problem, like parameterizing recurrent connections with
an orthogonal matrix (Mhammedi et al., 2017). In our analysis, we consider the Linear Memory Net-
work (LMN) (Bacciu et al., 2019) as a representative of non-gated approaches. LMNs leverage a
conceptual separation between a nonlinear feedforward mapping computing the hidden state ht and
a linear dynamic memory computing the history dependent state htm . Briefly, in formulas:
ht = σ(Wxhxt + Wmhhtm-1)
htm = Whmht + Wmmhtm-1
where xt , ht and htm are the input vector, the functional component activation and the memory
state at time t. The memory state htm is the final output of the layer, which is given as input to
subsequent layers. It is then useful to study how both models behave in CL environments and
how their different memory representations respond to phenomena like drifting tasks distribution,
eventually resulting in catastrophic forgetting.
3.2	Gated Incremental Memory (GIM)
GIM represents a general class of dynamic, recurrent architectures that can be built on top of any
recurrent model. GIM relies on a progressive memory (Rusu et al., 2016) extension of the underlying
RNN model that, for the sake of this work, are LSTMs or LMNs. We also leverage a set of gating
autoencoders, one for each distribution, to automatically select the module which has the best match
to the current input distribution. Figures 1 and 2 provide an overview of the entire GIM architecture
during training and test. The main component of GIM is the RNN module. As soon as a new
distribution arrives, a new RNN module is added on top of the existing architecture and connected
to the previous one. The exact inter-modules connections are slightly different depending on the
underlying recurrent model. When using the A-LSTM, at each timestep, the new module takes as
additional input the current hidden state of the previous module. In the A-LMN, the hidden state
ht of the new module is computed by taking as additional input the concatenation of both previous
module’s memory htm and functional activation ht . In order to prevent forgetting, when a new
module is attached to the existing architecture, the previous module’s parameters are frozen and no
longer updated. Therefore, each module becomes an expert of its own domain. The input vector xt
at each time step t is forwarded to all modules. Each module has its own output layer and, during
training, the last module added to the network is used to generate the final output y. The forward
pass for an A-LMN with N modules can then be formalized as follows:
hTm,1,hT,1 =LMN1(x,h0m,1)
hTm,j,hT,j =LMNj([x;hTm,j-1;hT,j-1],h0m,j),j=2,...,N
where hTm,i and hT,i are the memory states and functional activations of module i after having seen
the entire input sequence X of length T and [∙; ∙] is the concatenation operator between vectors. The
aggregated output y is computed by passing the final memory state hTm,N through a linear layer.
The A-LSTM forward pass follows the same process, substituting the final hidden state hT to the
3
Under review as a conference paper at ICLR 2020
Figure 1: Training of GIM-LSTM on 3 subtasks.
memory state and functional activations of A-LMN.
Each module of the augmented architecture is associated to a single LSTM autoencoder (Srivastava
et al., 2015), which is a sequence-to-sequence model trained jointly to the corresponding module
to reconstruct the input x. The training process involves only the autoencoder associated to the last
added module. The loss function used to train the autoencoders is the reconstruction error, that is
the mean squared error (MSE) applied element-wise to the input and reconstructed sequence. The
Augmented models are trained by using standard Cross Entropy loss for classification tasks.
The following equations provide a formal description of the GIM-LMN training when tackling the
i-th distribution (the GIM-LSTM behaves in the same way):
y = A-LMN(x)
X = AEi(X)
where AEi identifies the autoencoder associated to module i. The input x is sampled from the i-th
distribution. The output y and the reconstruction X are then used in the corresponding loss functions
(Cross Entropy and MSE, respectively) to train the A-LMN and the autoencoder by gradient descent.
At inference time, the input X is forwarded to all autoencoders and the module associated to the one
producing the minimum reconstruction error is selected to produce the final output, as described by
the following equations:
Xi = AEi(X), i = 1,…,N
k = arg min MSE(Xi, x)
i
y = LMNk(X).
The algorithms describing the forward pass for both A-LMN and A-LSTM and the training and test
procedure of GIM are provided in Appendix A.1.
GIM, like Progressive networks, is capable of learning multiple distributions without being affected
by forgetting. In addition, GIM overcomes one of the major drawbacks of Progressive networks
(Rusu et al., 2016): it does not require explicit knowledge about input distributions at test time,
since gating autoencoders are able to autonomously recognize current input and use the appropri-
ate module for output. GIM also simplifies the inter-modules connections: while in Progressive
networks they are feedforward networks, created between a module and all the next ones, GIM
employs a simpler, non adaptive, version and connects only adjacent modules. Therefore, the con-
nections do not contribute to the total number of adaptive parameters, which scales linearly with the
number of modules. Finally, as described in Aljundi et al. (2017), it is possible to compute a related-
ness measure between subtasks using the autoencoders reconstruction errors. Given a new subtask
Tk, an old subtask Ta and their associated autoencoders, the relatedness is computed by using the
reconstruction errors Erk and Era of the autoencoders measured on validation data for subtask Tk .
Formally:
Rel(Tk,Ta) = 1 - (EraE Erk ) .	(1)
ra
Notice that this definition is not entirely equivalent to the one reported in Aljundi et al. (2017).
Since we assume that each autoencoder achieves the lowest error on its own training data, we have
Era > Erk . Aljundi et al. (2017) normalize relatedness by having Erk in the denominator of
(1), in place of Era . In our experiments, we noticed that this led to large negative values for the
relatedness, which are not particularly insightful. Therefore, we propose a different normalization
coefficient, using the largest reconstruction error Era in place of Erk .
4
Under review as a conference paper at ICLR 2020
4	Experiments
One of the main contributions of this pa-
per is the definition of a set of learn-
ing tasks that can be used as standard
benchmarks for CL algorithms on se-
quence processing problems. We be-
lieve that the current literature is lack-
ing a set of simple, application-agnostic,
benchmarks that can provide a fair com-
parison and evaluation of different ap-
proaches with a minimal experimental
setup. The final appendices provide all
the details regarding the experimental
setup needed to reproduce the results.
The experiments rely on three datasets:
the Copy dataset (Graves et al., 2014),
the Sequential Stroke MNIST dataset
Figure 2: Test of GIM-LSTM. Input x is presented to all
autoencoders. In this case, AE2 obtains the minimum re-
constructione error. Hence, the second module (dashed
line) is chosen to produce the output. The input is propa-
gated up until the second module.
(de Jong, 2016) and the Audioset dataset (Gemmeke et al., 2017). We adapted each dataset to
classification tasks in CL environments. We split each task in multiple subtasks, associating to each
one a subset of the possible classes. The complexity of each task can be adjusted and controlled by
the experimenter. The tasks are not designed for a specific application domain, but instead, they are
general enough to stress the main difficulties of RNN training and CL scenarios. This is particularly
useful to test the limitation of each model and allows a great degree of flexibility. The source code
is available online and we plan to release more datasets in the future1.
4.1	Copy task
The Copy task (Graves et al., 2014) con-
sists in the reconstruction of sequences Table 1: Accuracy (in percentage) on Copy task, mea-
of random binary vectors. This task sured at the end of each subtask training. Column labels
could be considered a simple CL ex- indicate sequence length for each subtask.
ample, since the data distribution for
each subtask only differs for the se-	%	5	^T2^^	19	26	33	40
quences' length. However, generaliz-	A-LMN	100	100	100	100	95	95
ing to long sequence lengths is a dif-	A-LSTM	100	100	100	95	90	85
ficult problem for traditional recurrent	LMN	100	100	100	78	-	-
architectures (Graves et al., 2014) and	LSTM	100	97	85	78	-	-
therefore the same techniques used in
more standard CL settings can be used
to solve this problem by incrementally increasing the memory capacity of the final model (Sodhani
et al., 2018). Furthermore, it is useful to evaluate the memory capacity of the Augmented archi-
tecture compared against the standard RNNs. In the Copy Task, LMN and A-LMN use orthogonal
initialization of the memory component (see Appendix A.2 for more details). Gating autoencoders
have not been employed in this task, since its objective is exactly the reconstruction of the input se-
quence. The subtasks are organized in a curriculum (Bengio et al., 2009) following sequence length,
from sequences with 5 ± 2 vectors to sequences with 26 ± 2, steps of 7 vectors (4 subtasks total).
Results, summarized in Table 1, show that LMN performs slightly better than LSTM (using more
hidden units can bridge this gap). This is expected since orthogonal models typically outperform
LSTMs on pure memorization problems (Vorontsov et al., 2017). Augmented models outperformed
both of them, with A-LMN converging faster than A-LSTM. Augmented models have been trained
also with longer sequences (up to length 40), without sensible drops in accuracy. All models ob-
tained an accuracy on par with a random classifier when tested on previous subtasks (a well-known
problem for RNNs). The Copy Task highlights the behavior of recurrent CL models trained on grow-
ing sequence lengths and proves their effectiveness when compared to standard (non CL oriented)
recurrent models. The use of dynamic modules in A-LMN and A-LSTM positively contributes to
the memory capacity, justifying the additional cost required by the Augmented architectures.
1Public link redacted to preserve anonymity, reviewers can find source code in the supplementary material
5
Under review as a conference paper at ICLR 2020
O
Single digit sequence
(a) A single digit sequence
is classified into its cor-
responding digit class.
The vertical segments
correspond to a single input
vector. The last one is the
eod.
(b) A sequence of digits (including an additional final segment), is classified
into its corresponding class. The tokens are uniquely associated to the class
index 44. The digits corresponding to the tokens are explicitly showed for
each subtask. The last token is common to all the sequences belonging to the
same subtask and its length affects the learning behavior, as showed in Table
3.
Additional
Digit Segment
Figure 4: SSMNIST Tasks description
4.2	SSMNIST TASK
Sequential Stroke MNIST (SSMNIST) (de Jong, 2016) is a
variation of the popular MNIST dataset of handwritten digits.
In this version, each digit is composed by a sequence of pen
strokes, where each pen stroke is represented by two coordi-
nates (x and y, scalars), an end of stroke bit eos and an end of
digit bit eod. We devised two different kind of experiments,
aimed at both evaluating gating autoencoders capabilities and
GIMs classification performances.
In Sodhani et al. (2018), the SSMNIST dataset has already
been employed in CL environments. However, the authors
build the CL strategy by progressively increasing the se-
quence length (e.g. by concatenating together multiple digits).
Here, we chose a different approach by limiting each subtask
to a fixed set of digit classes. Our approach is consistent with
the literature about CL in computer vision, where the different
Figure 3: A-LSTM learning curve
on Sequence Classification. Final
concatenated length of 12.
subtasks are designed over different object classes within the
images (Maltoni & Lomonaco, 2019; Lopez-Paz & Ranzato, 2017).
4.2.1	Single Digit SSMNIST
The Single Digit task (Fig. 4a), requires the models to predict the digit class associated to the
input sequence. The task has been partitioned into 5 subtasks, each of which selects input patterns
belonging to 2 digit classes. There is no intersection in the digit classes between different subtasks.
Classification performance easily reached top (100%) accuracy for all models. The only exception is
the LMN, which obtained an accuracy on par with a random classifier on subtask 3. LMN and LSTM
experienced complete forgetting of old knowledge and were not able to address any of the previous
subtasks. GIMs, instead, completely prevent forgetting through the use of gating autoencoders, that
were always able to distinguish between the 5 subtasks at test time. There was no clear difference
between GIM-LSTM and GIM-LMN performances in this task. GIMs are useful in this context not
6
Under review as a conference paper at ICLR 2020
Table 3: The table reports the values of the final digit segment length that produce, for each model,
the behavior specified by the top row. L (Learning) means that the model learns all the subtasks, NL
(No Learning) means that the model does not learn any subtask and PL (Partial Learning) means
that the model learns only one subtask.
	L	NL	PL
GIM-LMN	≤ 22	-	35
GIM-LSTM	≤ 11	≥ 14	12 - 13
LMN	16	≥ 24	22
LSTM	≤ 11	≥ 12	-
only to achieve top accuracy on all subtasks (e.g. GIM-LMN against LMN on subtask 3), but also
to prevent forgetting. Final test performances are summarized in Table 2.
4.2.2	Sequence Classification SSMNIST
The Sequence Classification task (Fig.
4b) partitions the 10 digit classes in 3
subtasks of 3 digit classes each, without
intersections (see Table 4).
Sequences are composed by 5 tokens
(either 0, 1 or 2), each one associated to
a specific digit, depending on the sub-
task. The association between tokens
and digits for each subtask is described
by Table 4. Sequences are grouped into
100 different classes, according to the
Table 2: Accuracies (in percentage) on test sets after
training on all subtasks on Single Digit SSMNIST.
%	T1	T2	T3	T4	T5
GIM-LMN	100	100	100	100	^T00^'
GIM-LSTM	100	100	100	100	100
LMN	0	0	0	0	100
LSTM	0	0	0	0	100
sequence of tokens they represent. As an example, class 44 consists of tokens (1, 0, 1, 2, 0). The
complete list of classes is presented in Appendix A.4.
The models are trained to classify each sequence into its corresponding class. This task is more dif-
ficult than the Single Digit, since the models have first to decode the digits sequence from the input
sequence (e.g. (4, 6, 4, 8, 6)), then associate such sequence to the appropriate class (44) by inferring
the correct tokens, without any intermediate help.
Gating autoencoders were still able to recognize the subtask, even if, with respect to the Single Digit,
this task increases the number of digit classes per subtask (from 2 to 3) and it is based on longer
sequences (made by multiple digit sequences). As in the Single Digit, GIMs successfully overcame
catastrophic forgetting, while LMN and LSTM were not able to address previous subtasks.
By adding a common digit segment at the end of each sequence, the classification problem becomes
much more difficult. The common suffix between all sequences requires a larger memory capacity to
distinguish the inputs. Depending on the length of the final segment, different behaviors emerged, as
showed in Table 3. Some lengths completely prevented learning (No Learning column), while some
others did not affect the performances (Learning column), still obtaining an accuracy above 95%.
The most interesting behavior (Partial Learning column) has been observed for a narrow range of
lengths. In this case, learning is prevented on the first 2 subtasks. On the third subtask, the models
suddenly achieved the top accuracy, as showed in Fig. 3.
The Partial Learning behavior highlights how the use of GIMs enables learning of more articulated
input distributions, compared to baseline RNNs.
4.3	Audioset task
Audioset (Gemmeke et al., 2017) is a collection of annotated audio events, extracted from 10
seconds audio clips and organized hierarchically in classes. The objective is the classification of a
sound from its audio clip source, embedded through a VGG-acoustic model into 10 vectors, one per
second. In order to implement a CL scenario, we selected 40 audio classes and split them among 4
subtasks (10 classes per subtask). We selected the 40 classes according to the procedure outlined
by Kemker et al. (2018). Since their classes have not been published, we randomly selected them
from the superset resulting from their preprocessing pipeline. See Appendix A.5 for a complete
description. Results showed that gating autoencoders were perfectly capable of detecting the
7
Under review as a conference paper at ICLR 2020
correct data distributions at inference time. Therefore, GIMs successfully prevented forgetting:
they obtained a final test performance comparable to the validation accuracy measured at the end
of each subtask training. Standard RNN architectures, on the contrary, were prone to catastrophic
forgetting, achieving 0% accuracy on previous subtasks. Table 5 summarizes these results.
Audioset data has already been used in literature to assess CL skills Kemker et al. (2018). However,
the work by Kemker et al. (2018) focused on the task from a static perspective, relying on the use
of feedforward models only. Since the preprocessing step provides, for each audio clip, a sequence
of fixed-size embeddings, it is possible to concatenate the vectors into a single large vector and
feed it to the network. The sequential aspect of the task, however, is completely lost. At the best
of our knowledge, we are the first to tackle Audioset in CL scenarios with recurrent models. It is
also important to notice that the task difficulty is increased when using recurrent networks, since
the model is not able to see the input in its entirety (like in feedforward networks), but it has to scan
it one timestep at a time.
4.4	Relatedness analysis
In our experiments, large relatedness
values are associated to “heteroge-
neous” distributions. We call heteroge-
neous distributions the ones that gener-
ate patterns from many different classes
(as in Audioset) or from a complex mix-
ture of few classes (as in Sequence Clas-
sification SSMNIST). We observed that,
in correspondence with heterogeneous
distributions, autoencoders training con-
verges to larger values of reconstruc-
tion error than the ones obtained with
homogeneous distributions (as in Single
Digit SSMNIST). However, reconstruc-
Tokens-to-Digits	0	1	2
T1	-2^	0	~7~'
T2	6	4	8
	T3		3	1	5
Table 4: This table associates to each subtask of Se-
quence Classification SSMNIST (T1, T2, T3) the val-
ues taken by the tokens (0, 1, 2) in the sequence classes.
As an example, sequence (1, 0, 1, 2, 0) (class 44) corre-
sponds to different multi digit sequences: (0, 2, 0, 7, 2)
for subtask 1 and (4, 6, 4, 8, 6) for subtask 2.
tion error, hence relatedness, does not tell anything about gating capabilities (i.e. the ability to select
the appropriate module at test time). Autoencoders, in fact, always performed correctly in all exper-
iments, preventing forgetting. A complete account of relatedness measure is provided in Appendix
A.3.
5 Conclusions
The main objective of this work is to
draw attention on the problem of CL Table 5: Accuracies (in percentage) on test sets after
for sequential data processing. We pro- training on all subtasks on Audioset.
posed a set of benchmarks against which
future models can assess their perfor-	%	TH~	T2	T3	T4
mances. These benchmarks test the	GIM-LMN	~1Γ~	65	53	50
main difficulties of the sequential and	GIM-LSTM	60	69	60	47
CL domains and their difficulty can be	LMN	0	0	0	43
carefully tuned. We introduced GIM, a	LSTM	0	0	0	42
recurrent CL architecture for sequential
data processing, and showed that it con-
sistently outperforms baseline RNNs on all the proposed benchmarks. The results support the claim
that recurrent architectures need to be adapted to manage CL scenarios and encourage future works
towards an in-depth study of their behaviors.
In the future, GIM could be applied to Reinforcement Learning tasks, where there are more opportu-
nities to compare performances with existing works. In order to provide a formal characterization of
the learning process, it would be useful to apply existing metrics to better describe the type of tasks
addressed by the model, in terms both of transfer learning opportunities and subtasks interference.
New metrics can also be developed, specifically tailored to sequential learning problems.
8
Under review as a conference paper at ICLR 2020
References
R. Aljundi, P. Chakravarty, and T. Tuytelaars. Expert Gate: Lifelong Learning with a Network of
Experts. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
7120-7129, July 2017.
Nabiha Asghar, Lili Mou, Kira A. Selby, Kevin D. Pantasdo, Pascal Poupart, and Xin Jiang. Pro-
gressive Memory Banks for Incremental Domain Adaptation. arXiv: 1811.00239 [cs], November
2018. arXiv: 1811.00239.
Davide Bacciu, Antonio Carta, and Alessandro Sperduti. Linear Memory Networks. In Proceedings
of the 28th International Conference on Artificial Neural Networks (ICANN 2019),, Lecture Notes
in Computer Science. Springer-Verlag, September 2019.
Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09, pp.
1-8, Montreal, Quebec, Canada, 2009. ACM Press.
Kyunghyun Cho, Bart van Merrinboer, Dzmitry Bahdanau, and Yoshua Bengio. On the Properties
of Neural Machine Translation: EncoderDecoder Approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103-111, Doha,
Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-4012.
URL https://www.aclweb.org/anthology/W14- 4012.
Robert Coop and Itamar Arel. Mitigation of catastrophic forgetting in recurrent neural networks
using a Fixed Expansion Layer. In The 2013 International Joint Conference on Neural Networks
(IJCNN), pp. 1-7, Dallas, TX, USA, August 2013. IEEE.
Edwin D. de Jong. Incremental Sequence Learning. arXiv: 1611.03068 [cs], November 2016.
arXiv: 1611.03068.
Robert M. French. Pseudo-recurrent Connectionist Networks: An Approach to the ’Sensitivity-
Stability’ Dilemma. Connection Science, 9(4):353-380, December 1997. ISSN 0954-0091.
Robert M. French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences,
3(4):128-135, April 1999. ISSN 1364-6613, 1879-307X.
J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and
M. Ritter. Audio Set: An ontology and human-labeled dataset for audio events. In 2017 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 776-780,
March 2017.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv: 1410.5401 [cs],
October 2014. arXiv: 1410.5401.
Geoffrey E Hinton and David C Plaut. Using Fast Weights to Deblur Old Memories. Proceedings
of the ninth annual conference of the Cognitive Science Society, pp. 177-186, 1987.
Sepp Hochreiter and Jurgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9:
1735-1780, 1997.
Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L. Hayes, and Christopher Kanan. Measur-
ing Catastrophic Forgetting in Neural Networks. In Thirty-Second AAAI Conference on Artificial
Intelligence, April 2018.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting
in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521, March 2017.
Timothe Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia
Daz-Rodrguez. Continual Learning for Robotics. arXiv: 1907.00182 [cs], June 2019. arXiv:
1907.00182.
9
Under review as a conference paper at ICLR 2020
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient Episodic Memory for Continual Learning.
NIPS, 2017. arXiv: 1706.08840.
Davide Maltoni and Vincenzo Lomonaco. Continuous learning in single-incremental-task scenarios.
NeuralNetworks,116:56-73, AUgUst 2019. ISSN0893-6080.
Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient Orthogonal
Parametrisation of RecUrrent NeUral Networks Using HoUseholder Reflections. In International
Conference on Machine Learning, pp. 2401-2409, JUly 2017. URL http://proceedings.
mlr.press/v70/mhammedi17a.html.
German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. ContinUal
lifelong learning with neUral networks: A review. Neural Networks, 113:54-71, May 2019. ISSN
0893-6080.
Mark Ring. RecUrrent Transition Hierarchies for ContinUal Learning: A General Overview. Work-
shops at the Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
Anthony Robins. Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.
Connection Science, 7(2):123-146, JUne 1995. ISSN 0954-0091, 1360-0494.
Andrei A. RUsU, Neil C. Rabinowitz, GUillaUme Desjardins, HUbert Soyer, James Kirkpatrick, Ko-
ray KavUkcUoglU, Razvan PascanU, and Raia Hadsell. Progressive NeUral Networks. arXiv:
1606.04671 [cs], JUne 2016. arXiv: 1606.04671.
ShagUn Sodhani, Sarath Chandar, and YoshUa Bengio. On Training RecUrrent NeUral Networks for
Lifelong Learning. arXiv: 1811.07017 [cs, stat], November 2018. arXiv: 1811.07017.
Nitish Srivastava, Elman Mansimov, and RUslan SalakhUtdinov. UnsUpervised Learning of Video
Representations Using LSTMs. ICML, 2015. arXiv: 1502.04681.
EUgene Vorontsov, Chiheb Trabelsi, SamUel KadoUry, and Chris Pal. On orthogonality and learn-
ing recUrrent networks with long term dependencies. In International Conference on Machine
Learning, pp. 3570-3578, JUly 2017.
Jaehong Yoon, EUnho Yang, Jeongtae Lee, and SUng JU Hwang. Lifelong Learning With Dynami-
cally Expandable Networks. ICLR, pp. 11, 2018.
Friedemann Zenke, Ben Poole, and SUrya GangUli. ContinUal Learning ThroUgh Synaptic Intelli-
gence. In International Conference on Machine Learning, pp. 3987-3995, JUly 2017.
GUanyU ZhoU, KihyUk Sohn, and Honglak Lee. Online Incremental FeatUre Learning with Denois-
ing AUtoencoders. Proceedings of Machine Learning Research, 22:9, 2012.
A Appendix
A. 1 Gated Incremental Memories
Algorithms 1 and 2 oUtline the forward pass for A-LSTM and A-LMN architectUres.
Algorithm 3 describes the training and test process for the GIM architectUre.
Algorithm 1 A-LSTM Forward Pass
Require: A-LSTM with N modUles, inpUt x
Ensure: N > 1
1:	Initialize hidden state of modUle 1: h1,i
2:	h1,f = A-LSTM1(x, h1,i)	. CompUte final state of modUle 1
3:	for d - 2,N do	'
4:	Initialize hidden state of modUle d: hd,i
5:	X= [x; hd-ι,f ]	. Concatenate hidden state with input
6:	hd,f = A-LSTMd(X, hd,i)	. Obtain final state of module d
7:	end for
8:	Output final hidden state of last module: hN,f
10
Under review as a conference paper at ICLR 2020
Algorithm 2 A-LMN Forward Pass
Require: A-LMN with N modules, input x
Ensure: N > 1
1:	Initialize memory state of module 1: h1m,i
2:	h1m,f, h1,f = A-LMN1(x, h1m,i)	. Compute final memory and functional state of module 1
3:	for d - 2,N do
4:	Initialize memory state of module d: hdm,i
5:	X= [x; hdm-ι,f ； hd-ι,f ]	. Concatenate functional and memory state with input
6:	hf hd,f = A-LMNi (X, hd，i) . Compute final memory and functional state of module d
7:	end for
8:	yNm = σ(WNmo hNm,f)	. Output of the final module
Algorithm 3 Training and testing of LSTM Autoencoders, including output selection from Aug-
mented architecture___________________________________________________________________________
Require: A sequence of distributions D = {D1, D2, D3, ...}
Ensure: | D |> 1
1	lae J []	. Create empty list
2	: while a new distribution Dk is available do
3	:	LSTMenc, LSTMdec = init-autoencoder()
4	:	lae.append((LSTMenc, LSTMdec))	. Append new autoencoder
5	:	Initialize the hidden state of the encoder henc,i
6	:	for training batch X ∈ Dk do
7	:	henc,f = LSTMenc(X, henc,i)	. Compute encoder state
8	:	hdec,f = LSTMdec(0, henc,f)	. Compute decoder state
9	X = Wf hdec,f	. Compute output
10	J = mSe(x, X)	. Compute reconstruction loss
11	Take an optimization step using J computed with backpropagation
12	:	end for
13	: end while
14	: for test batch X ∈ D do
15	:	lrec J []	. Create empty list
16	:	for LSTMencLSTMdec ∈ lae do
17	X J output of current autoencoder on X
18	lrec.append(MSE(x, X))	. Append the reconstruction loss
19	:	end for
20	:	m = arg min lrec	. Retrieve minimum loss autoencoder’s index
21	:	Select the output produced by the m-th module of the Augmented architecture
22	: end for
A.2 Experiment settings
A.2.1 COPY
The experiment settings used for the Copy task are listed below:
1.	Each vector in the sequence has 8 binary values. The last bit is always 0, except for the
delimiter vector which is a vector of zeros with the last bit set to 1.
2.	The subtasks draw sequences with lengths from 5 ± 2 to 26 ± 2 with steps of 2.
3.	The sequence length for each training step is drawn at random according to the lengths
corresponding to the current subtask.
4.	Binary Cross Entropy (BCE) loss, RMSProp optimizer with learning rate of 3e - 5, mo-
mentum of 0.9 and no weight decay. The gradient clipping is set to 5.0. The mini batch
size is 3.
11
Under review as a conference paper at ICLR 2020
	T1	T2	T3	T4	T5
^CT	"ɪ	~0~	~1~	~~6~	^^8--
C2	3	1	5	7	9
Table 6: This table associates to the five subtasks (T) of Single Digit SSMNIST, the corresponding
digit classes (C) (2 classes per subtask).
5.	The LSTM and GIM-LSTM use 256 hidden units, while the LMN and the GIM-LMN
use 126 memory units and 126 feedforward units. With this setting the models are better
comparable since they use the same number of units in total.
6.	For what concerns the LMN and the GIM-LMN, the memory matrices have been initialized
as orthogonal, due to the benefits on the norms of the matrices explained in Vorontsov et al.
(2017). Moreover, in order to constrain the memory matrices to remain approximately
orthogonal during training, the BCE loss has been augmented with an additional penalty
term in order to penalize non-orthogonal memory matrices (λ k WWT - I k). The
hyperparameter λ associated with this penalty term has been set to 0.1.
We tried to use a different number of hidden units in all models. Such changes affect the final result
only for LSTM and LMN with 512 hidden units, leading to a better performance (90% accuracy)
on sequences of length 19 and 26. With this configuration, LMN and LSTM obtain comparable
performances. We tried configuration with 256 and 512 hidden units for both functional and
memory component for LMN and A-LMN, and with 128 and 512 for LSTM and A-LSTM. We
varied the hyperparameter λ in the range [0.01, 0.1, 0.2, 1].
A.2.2 SSMNIST
In the Single Digit task, subtasks are numbered from 1 to 5 (following the order of the training
phase) and are associated to the digit classes according to Table 6.
The experiment settings for all SSMNIST tasks are listed below:
•	LSTM and GIM-LSTM with 64 hidden units and 1 hidden layer
•	LMN and GIM-LMN with 64 hidden units both for the memory component and the func-
tional component
•	RMSProp optimizer with learning rate of 3e - 5, L2 regularizer of 1e - 3, gradient clipping
with norm 5.0 and momentum of 0.9
•	Mini-batch size of 4 sequences
•	The LSTM autoencoders use 40 hidden units for both the encoder and the decoder in the
Single Digit SSMNIST, and 200 hidden units for the Sequence Classification SSMNIST.
They are optimized with Adam optimizer, learning rate of 1e - 4 and L2 decay of 1e - 3.
•	We use 60% of dataset for training, 20% for both validation and test.
For what concerns Sequence Classification SSMNIST Task during training, sequences are dynam-
ically generated at each step by randomly selecting a batch of class labels and then by randomly
generating sequences belonging to those classes. Each digit in the sequence is randomly sampled
from the training set. During validation and final test, we generate 5000 sequences with the same
procedure. The classes are equally balanced in the validation and test sets (50 patterns per class).
The average sequence length in SSMNIST is 40 vectors per digit, with 4 scalars in each vector. We
compact the representation in the LSTM autoencoders by a factor of 4, by using 40 hidden units
per digit. Hence, in Sequence Classification SSMNIST Task We use 40 * 5 = 200 hidden units for
autoencoders.
A.2.3 Audioset
The experiment settings are listed beloW:
12
Under review as a conference paper at ICLR 2020
Table 7: Relatedness values between subtasks on Single Digit SSMNIST.
	T1	T2	T3	T4	T5
	1.0000				
T2	0.1703	1.0000			
T3	0.5407	0.4762	1.0000		
T4	0.3642	0.2876	0.4403	1.0000	
T5	0.0102	0.4432	0.7379	0.5164	1.0000
Table 8: Relatedness values between subtasks on Sequence Classification SSMNIST.
	T1	T2	T3
T2 T3	1.0000 0.9899	1.0000 0.9867	0.9668	1.0000
•	Standard LSTM and GIM-LSTM with 32 hidden units and 1 hidden layer
•	LMN and GIM-LMN with 32 hidden units both for the memory component and the func-
tional component
•	RMSProp optimizer with learning rate of 3e - 5, L2 regularizer of 1e - 3, gradient clipping
with norm 5.0 and momentum of 0.9
•	Mini-batch size of 4 audio clips
•	The LSTM autoencoders use 200 hidden units for both the encoder and the decoder and
they are optimized with Adam optimizer, a learning rate of 3e - 5 and no decay.
We tested configurations with different number of hidden units, in the range [8, 16, 32, 64] for
all models. In the LMN and GIM-LMN the same number of hidden units has been used in both
functional and memory component. Results showed that even models with 8 hidden units achieve
accuracies that are only slightly below the ones achieved by the models with 32 units. By increasing
the units to 16, we obtained the same results reported in the paper.
Each Audioset pattern is composed by 10 vectors with 128 elements, for a total number of
elements equal to 1280. We use 200 hidden units in the LSTM autoencoders, thus compressing the
representation by a factor of 6.4.
A.3 Relatednes s between subtasks
The relatedness values between subtask are showed by Table 7, 8, 9.
A.4 SSMNIST classes
Table 10 reports the sequence classes used in the SSMNIST Sequential classification task.
A.5 Audioset classes
The input patterns used in the experiments have been restricted to the ones labeled with only 1 class.
Each class has no restrictions based on the associated ontology, is marked with a reliability > 70%
Table 9: Relatedness values between subtasks on Audioset.
	T1	T2	T3	T4
T2 T3 T4	1.0000 0.8415	1.0000 0.8713	0.9673	1.0000 0.9126	0.9444	0.9541	1.0000
13
Under review as a conference paper at ICLR 2020
ID	1	2	3	4	5
ι^r~	ɪ	ɪ	丁	2	丁
"ɪ	ɪ	ɪ	ɪ	0	ɪ
"ɪ	ɪ	丁	ɪ	2	ɪ
41~	ɪ	ɪ	ɪ	0	ɪ
5^~	ɪ	丁	丁	1	ɪ
6~β~	ɪ	ɪ	ɪ	1	丁
7j~	ɪ	ɪ	丁	0	ɪ
8~8~	ɪ	ɪ	ɪ	2	ɪ
~9~	ɪ	ɪ	ɪ	2	丁
"ɪ	ɪ	ɪ	ɪ	0	丁
"ɪ	ɪ	ɪ	丁	2	ɪ
"ɪ	ɪ	ɪ	丁	1	ɪ
T3-	ɪ	丁	丁	0	丁
"ɪ	ɪ	ɪ	ɪ	1	ɪ
"ɪ	ɪ	ɪ	ɪ	0	ɪ
^T6^	ɪ	丁	ɪ	0	ɪ
~ΓΓ	ɪ	ɪ	丁	0	ɪ
"ɪ	ɪ	ɪ	丁	2	ɪ
^T9^	ɪ	丁	丁	0	ɪ
^20^	ɪ	ɪ	ɪ	0	ɪ
^2T	ɪ	ɪ	丁	1	丁
^2^	ɪ	ɪ	ɪ	0	ɪ
^23-	ɪ	丁	丁	1	ɪ
"ɪ	ɪ	ɪ	ɪ	1	ɪ
25ξ~	ɪ	丁	丁	2	ɪ
^26^	ɪ	丁	ɪ	2	ɪ
~ΓT	ɪ	丁	丁	2	ɪ
^28^	ɪ	ɪ	丁	1	ɪ
^29^	ɪ	ɪ	ɪ	2	ɪ
^30^	ɪ	ɪ	ɪ	0	丁
	ɪ	丁	丁	1	ɪ
~"3Γ	ɪ	ɪ	ɪ	1	ɪ
~33~	ɪ	ɪ	ɪ	1	ɪ
Table 10: SSMNIST classes
ID	1	2	3	4	5
344~	ɪ	ɪ	2	丁	ɪ
33ζ~	丁	ɪ	2	丁	丁
336~	ɪ	ɪ	2	ɪ	丁
37T~	ɪ	ɪ	Γ	丁	ɪ
^38^	ɪ	ɪ	Γ	ɪ	丁
399~	ɪ	ɪ	2	ɪ	ɪ
^40^	ɪ	ɪ	2	ɪ	丁
~4Γ~	丁	ɪ	2	ɪ	ɪ
~4∑~	ɪ	丁	0	ɪ	ɪ
^43-	丁	ɪ	Γ	ɪ	丁
~44~	丁	ɪ	Γ	ɪ	ɪ
~45~	丁	丁	2	ɪ	ɪ
~46~	ɪ	丁	0	ɪ	丁
~TT~	ɪ	ɪ	0	ɪ	ɪ
~48~	丁	ɪ	2	ɪ	ɪ
~49~	ɪ	ɪ	2	ɪ	ɪ
^30^	ɪ	丁	2	ɪ	丁
~1Γ~	ɪ	ɪ	0	ɪ	ɪ
522~	丁	ɪ	2	ɪ	ɪ
-33-	ɪ	ɪ	0	丁	ɪ
544~	丁	ɪ	0	ɪ	ɪ
555~	ɪ	ɪ	2	ɪ	ɪ
~66~	ɪ	丁	Γ	ɪ	丁
~TΓ~	ɪ	ɪ	2	ɪ	丁
^38^	丁	ɪ	0	ɪ	丁
~99~	ɪ	丁	2	ɪ	丁
^^60^	ɪ	ɪ	0	ɪ	ɪ
~ΓΓ	ɪ	ɪ	Γ	ɪ	ɪ
~22~	ɪ	ɪ	Γ	ɪ	ɪ
^^63-	ɪ	丁	2	丁	ɪ
^^6^	ɪ	ɪ	0	ɪ	ɪ
^^6^	丁	丁	2	ɪ	ɪ
~66~	丁	ɪ	0	ɪ	ɪ
ID	1	2	3	4	5
~6T~	ɪ	ɪ	ɪ	ɪ	F
^68-	丁	ɪ	丁	丁	F
^69-	ɪ	ɪ	ɪ	ɪ	T
^70-	ɪ	丁	ɪ	ɪ	22
~ΓΓ	ɪ	丁	ɪ	丁	T
T2T~	丁	丁	ɪ	ɪ	F
~T~	丁	丁	ɪ	丁	T
~T~	ɪ	丁	丁	ɪ	T
T5S~	丁	ɪ	ɪ	ɪ	F
^76-	丁	ɪ	丁	ɪ	22
~TΓ	ɪ	ɪ	ɪ	ɪ	F
^78-	ɪ	ɪ	ɪ	ɪ	F
^79-	丁	ɪ	ɪ	丁	F
^80-	ɪ	丁	ɪ	丁	T
~Γ~	ɪ	丁	ɪ	ɪ	T
~22~	丁	丁	丁	ɪ	22
^83-	ɪ	ɪ	丁	ɪ	F
^84~	丁	丁	ɪ	丁	F
^8^	丁	丁	丁	ɪ	F
~66~	ɪ	ɪ	丁	ɪ	22
~T~	丁	ɪ	ɪ	ɪ	F
^88-	ɪ	丁	ɪ	丁	T
^89-	丁	ɪ	丁	丁	F
^90-	ɪ	丁	ɪ	ɪ	T
^91-	ɪ	ɪ	ɪ	ɪ	F
~2T~	ɪ	丁	丁	丁	F
^93-	丁	ɪ	ɪ	丁	T
^94~	ɪ	ɪ	ɪ	丁	T
^9^	ɪ	ɪ	ɪ	ɪ	T
~96~	ɪ	ɪ	ɪ	丁	F
~TΓ	丁	丁	ɪ	ɪ	T
^98-	ɪ	ɪ	ɪ	丁	F
^99-	ɪ	丁	ɪ	ɪ	F
W0-	ɪ	ɪ	丁	丁	22
and is not sub or super class of any other, following the procedure adopted by Kemker et al. (2018).
Table 11 reports the 40 classes (N column) selected from Audioset dataset.
The ID is the corresponding class code and Category is its description, as in the official Audioset
documentation. Train and Test list the number of patterns of the corresponding class in the entire
dataset.
The first 10 (N from 1 to 10) classes are used in the first subtask, the classes from 11 to 20, from 21
to 30 and from 31 to 40 are used in the second, third and fourth subtasks.
14
Under review as a conference paper at ICLR 2020
Table 11: Audioset classes
N	ID	Train	Test	Category
Il-	=^=	154	~~6~~	Conversation
	-^6-	175	12	Babbling
"ɪ	-T0-	-75-		WhooP
4^~	^52^	476	^T0-	Radio
5^~	T4-	296	^T6-	Screaming
6~β~		436	--8-	Whispering
7^Γ	T7-	7^~	4	Baby laughter
-8-	-T8-	124	7	Giggle
~9~	~^T~	95	"ɪ"	Chuckle chortle
10	23	483	14	Baby cry infant cry
11	368	393	11	Microwave oven
"ɪ	169^	836	-38-	Blender
-T3~	37∏T	1396	~39~	Vacuum cleaner
14	382	201	11	Electric shaver electric razor
15	386	318	4	Computer keyboard
-T6~	-387~	271	~T8―	Writing
17	390	80	14	Telephone bell ringing
18-	19T	243	~~iΓ~	Ringtone
T9^	198^	332	21	Buzzer
^^0^	^400^	284	^T3-	Fire alarm
^2T	~W^	175		Steam whistle
rɪ	ID-	Train	Test	Category
p2^	^4O7"	337	^TF^	TiCk
~33~	^41T	732	^T6-	Sewing machine
~2A~	^41^	-64-	14	Cash register
^5~	^41^	2005	41	Printer
~26~	^419^	382	~^8~	Hammer
~rΓ	^42T	-85-	~^F~	Sawing
-^8-	^44F	146	12	Chink clink
~w	^450^	382	12	-Trickle dribble-
10-	^452^	-94-		Fill(with liq≡d)"
~1Γ	^453^	1056	^T8-	Spray
~2Γ	^45^	163	~~6~~	Pump (liquid)
ʒɜ-	^46F	283	52ST~	Slap smack
	^468^	1294	~7yΓ~	Whack thwack
	^469^	339	42	Smash crash
~36~	^470^	-99-	^T0-	Breaking
~rr	^47T	-37-	^^0-	Bouncing
-38^	^48T	652	^T3-	Beep bleep
19^	^483^	384		Ding
p0~	~493~	103	~T8~	Rumble
15