Under review as a conference paper at ICLR 2020
Learning to Combat Compounding-Error in
Model-Based Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Despite its potential to improve sample complexity versus model-free approaches,
model-based reinforcement learning can fail catastrophically if the model is in-
accurate. An algorithm should ideally be able to trust an imperfect model over
a reasonably long planning horizon, and only rely on model-free updates when
the model errors get infeasibly large. In this paper, we investigate techniques for
choosing the planning horizon on a state-dependent basis, where a state’s planning
horizon is determined by the maximum cumulative model error around that state.
We demonstrate that these state-dependent model errors can be learned with Tem-
poral Difference methods, based on a novel approach of temporally decomposing
the cumulative model errors. Experimental results show that the proposed method
can successfully adapt the planning horizon to account for state-dependent model
accuracy, significantly improving the efficiency of policy learning compared to
model-based and model-free baselines.
1	Introduction
Model-free reinforcement learning (RL) aims to learn an effective behavior policy directly from
interaction with a black-box environment. This approach has recently achieved great success,
particularly in game playing (Mnih et al., 2015; Moravc´k et al., 2017; Silver et al., 2016; 2017).
Unfortunately, model-free RL techniques are hampered by poor sample efficiency, which makes their
deployment infeasible whenever data collection is expensive. A key challenge remains to improve
the sample efficiency of general purpose RL methods.
By contrast, model-based RL attempts to learn a model of an environment from direct experience
collected during training. A learned model can be either directly combined with a planning algorithm
(Hafner et al., 2018; Sutton, 1990), or applied to improve the target values for model-free RL
(Buckman et al., 2018; Feinberg et al., 2018). Model-based RL is often thought to be more sample
efficient than model-free approaches (Sutton & Barto, 2018). Recent theoretical work confirms
this intuition by showing that there exist environments where model-based approaches can be
exponentially more sample efficient than any model-free approach (Sun et al., 2018).
The performance of model-based RL heavily relies on the quality of the model a learning agent
can acquire. When an accurate model is given or can be learned with relatively little experience,
model-based RL can be significantly more data efficient than model-free approaches. However, in
noisy and complex environments, learning an accurate model can be a challenge. In such cases,
model errors can compound and render the model useless for planning, which can lead to catastrophic
failure of model-based RL. Although having an accurate model in a complex environment can be
unrealistic, it is sometimes possible to obtain a model that is accurate in local subsets of the state
space. For example in robotic control tasks, local-motion dynamics that do not consider external
environment interaction can be much easier to model than the dynamics of interaction with other
objects. In such cases, even if a pure model-based approach would fail, one might still expect to gain
advantage over model-based approaches by exploiting the accurate parts of the model.
One potential advantage of model-based RL is that a longer planning horizon can be considered
by rolling out the model for multiple steps. Ideally, with an imperfect model, one would like a
principled approach for adapting the planning horizon at different states, in order to overcome
the compounding error problem of model-based RL. When the model has large error around
some states, the learning agent should trust the model less by using a small planning horizon.
1
Under review as a conference paper at ICLR 2020
On the other hand, for states where the model
is near optimal, a large planning horizon should
be adopted. To implement this idea, we pro-
vide a few key observations that are essential
for the methods we propose: First, we charac-
terize the error in a multi-step learning target
under an approximate model as a multi-step dis-
counted cumulative model error. Second, we
show how the cumulative model error for dif-
(a) Adaptive Horizons
(b) Policy Learning
Figure 1: Illustration of adaptive planning horizon
in FourRoom with an imperfect model. The model
is perfect in three rooms while totally wrong in
the left bottom room. Non-adaptive MVE diverges
due to the large model errors (right). In contrast,
AdaMVE is able to adapt the planning horizon at
different state (see (a), darker color means longer
planning horizon), outperforming both the model-
based and model-free baselines.
ferent planning horizons can be learned based
on TD-learning. Finally, we introduce Adap-
tive Model-based Value Expansion (AdaMVE),
an extension to Model-based Value Expansion
(MVE) Feinberg et al. (2018) that adaptively
selects planning horizons for each state, based
on the learned accumulative model errors. To
illustrate, Fig. 1 provides an example of how
AdaMVE works in a FourRoom gridworld maze
with an imperfect model. This example shows
that AdaMVE can successfully adapt the plan-
ning horizon for different subsets of the state space, and significantly outperforms both MVE and
model-free baselines. We evaluate our method on both gridworld mazes with different imperfect
models as well as continuous control tasks. The experimental results suggest that the adaptive horizon
algorithm can significantly alleviate the compounding error problem in model-based RL, while
exploiting its advantage in terms of sample efficiency.
2	Background
Consider a Markov Decision Process (MDP) Sutton & Barto (2018) M = (S, A, P, R, γ) where S
is the state space, A is the action space, P(∙∣s,α) is the transition probability distribution function,
R(s, a) is the reward function and γ is the discount factor. We also assume that each state is
represented by a feature vector S ∈ Rd. The goal is to find a policy ∏(∙∣s) that maximizes the
cumulative discounted reward starting from any state S ∈ S. Let Pπ(∙∣s) denote the induced
transition distribution for policy π. For later convenience, we also introduce the notion of multi-step
transition distributions as Pt, where Pt (∙∣s) denotes the distribution over the state space after rolling
out Pπ for t steps starting from state s. For example, P0(∙∣s) is the Dirac delta function at S and
Pn (∙∣ s) = Pπ (∙∣ s). Weuse Rn (S) to denote the expected reward at state S when following policy ∏,
i.e. Rn (S) = Ea〜∏(∙∣s) [R(s, a)]. The state value function is defined by
Vn(S)= Xt=0 Y tEst 〜Pn (S)[Rn(St)] .
The action-value function (a.k.a. Q-function) can be written as
Qn (S,a) = R(S,a) + YEsO〜P(∙∣s,α) [Vn(S0)].
The optimal policy is define as the policy π that maximizes Vn(S) at all states S ∈ S. Such policy
always exists (Sutton & Barto, 2018).
Model-based reinforcement learning approaches explicitly make use of a dynamics model P ≈ P
of the environment to compute the optimal policy, while model-free approaches learn the optimal
policy without explicitly modeling P (e.g. directly learning the action-value functions from samples).
Throughout the paper we assume that the reward function R is known to the agent, hence a “model”
refers to an estimated transition dynamics P .
2.1	Multi-step Model-Based Value
One potential advantage of learning a model is to compute a multi-step target value by iteratively
rolling out the model, that is, to take the predicted state of the model and feed it in again as the state
input, projecting to a sample state two time steps later, and so on. Formally, for any policy π, given a
2
Under review as a conference paper at ICLR 2020
planning horizon H, a reference (target) value function V, and an approximate (e.g. learned) model
P, the H-step model-based value is defined as
H-1
V∏,H(S)= X YtEst〜Pn(S) [Rπ(st)] + YHEsH〜PH(s)[V(sH)] .	(1)
This value can be integrated with model-free methods in different ways based on the policy π. For
example in AIPhaGo Zero, the H-step optimal lookahead policy ∏H = argmax∏ V^∏ (S) combined
P,H
with a proper exploration strategy is used as the behavior policy of the learning agent, where ∏H is
approximated by Monte Carlo Tree Search (Silver et al., 2017).
Model-based value expansion (MVE) is another example of utilizing objective (1) (Buckman et al.,
2018; Feinberg et al., 2018). MVE applies the learning agent’s current policy as the rollout policy to
obtain VPnH(s), which is used as the update target value for TD Learning. For example in Q-learning
(Watkins & Dayan, 1992), given a sampled transition (S, a, r, S0) and a target Q-value function
Q(s,a), one can replace the target value V(s0) = maxa Q(s0, a) with the multi-step estimate VPnH(S)
or a mixture of such estimates with different values of H. The rollout policy π can be greedy with
respect to Q. Note that when H = 0, there is no rollout hence Vn °(s0) = V(s0), which recovers the
model-free update target.
When the model is perfect, MVE can reduce the biases of the targets, leading to improved performance
over model-free methods (Feinberg et al., 2018). However, the major limitation of MVE is that the
rollout horizon H needs to be tuned in a task-specific manner: in a complex environment where the
model is difficult to learn, a smaller rollout horizon usually performs better than a larger one. To
overcome this drawback, Buckman et al. propose stochastic ensemble value expansion (STEVE),
which applies stochastic ensembles both over multiple models and rollout horizons to choose the best
H dynamically (Buckman et al., 2018).
2.2	Wasserstein Distance
The Wasserstein distance is a distance metric between two distributions. Its Kantorovich-Rubinstein
dual form is defined as follows (Villani, 2008):
W(p, q) = sup Ez〜P [g(z)] - Ez〜q [g(z)]	(2)
kgkL≤1
where kgkL is the Lipschitz constant of function g. If both p and q are Dirac delta functions (i.e. de-
terministic) denoted by δzp and δzq respectively, W(p, q) = kzp - zqk2 is just the Euclidean distance.
If only one of the distributions is deterministic, e.g. q = δzq, then W(p, q) = Ez〜P [∣∣z - Zq|图.
This can be checked directly from the primal form or observing that g(z) = kz - zqk2 achieves the
superimum in the dual form (Villani, 2008).
3	Learning Multi- step Model Error
To exploit the advantage of multi-step value estimation while alleviating the negative impact of using
an imperfect model, we propose to adapt planning horizons H such that the H -step expanded value
using the approximate model is close to the one obtained using the true model. Specifically, for policy
π, consider the h-step model-based value error for an approximate model P defined by
E (h∣π,s,KP) = I 号,h(s)- VP,h(s)∣.	(3)
We aim to select an appropriate planning horizon for state S based on this error. Since the accuracy of
an approximate model could vary in different subspace of S , selecting planning horizons in a state
dependent way is particularly desirable in practice.
Given a state S, exactly computing error (3) is unfeasible as we cannot directly compute the multi-step
model-based value due to the inaccessibility of the true model P . To overcome this problem, we
propose a practical algorithm to learn the value expansion error for each state approximately, based
on the observation that the value expansion error defined in (3) can be characterized by the discounted
accumulative model error. The following theorem states such connection.
3
Under review as a conference paper at ICLR 2020
Theorem 1. Given any policy π, an approximate model P, and a reference value function V, for
planning horizon H we have
H-1
E(H∣∏,s,V,P) = ∣V‰(S)-理H(s)∣ ≤ K ∙ X Yt+1Est~Pπ(S) [W"(st)],
t=0
(4)
π
where Wπ (s) = Ea~∏(.∣s) [W (s, a)] Wlth W (s,a) = W (P (∙∣s, a), P (∙∣s, a)) being the Wasserstein
distance and K = SuPh IVJhI is the maximum Lipschitzness ofthe estimated value function over
all possible horizons.
The H -step discounted cumulative model error (RHS of (4)) can be viewed as a finite-horizon RL
objective. We can define a new MDP to learn this error, Mh,p = (S, A, P, W, γ), where the state
and action space, the transition function, and the discount factor remain unchanged from the original
problem, the W-rewardfunction W(s,a) = W(P(∙∣s,α),P(∙∣s,α)) defines the reward of the MDp
With Mh,p, we define a new state-value function named h-step state model error function, which
measures the expected cumulative model error if the agent starts in state s and follows some policy π,
En 屋 h) = ʃ 0 ….	h = 0
E (S,h)	t Ph=01 YtEst~PJ (s) [Wπ (st)]	h> 0
(5)
*	1 ∙	.	/ Λ∖ Z⅛1T /	7 ∖	∙	F	∙>C,∙>7,	111	1	1	∕C∖	.
According to (4), Eπ (s, h) is an upper bound of the h-step model-based value error (3) up to a
constant. Similarly, the h-step action model error is defined by
ʌn	J	0	h = 0
E”(s, a,h)=	ɪ	W (S, a)	+ γEsθ~p(∙∣s,a)	[En(s0,	h - 1)i h> 0
(6)
Note that Eπ(s,h) = Ea~∏ [Eπ(s,h,a)] by the definition. Learning the h-step model-based value
error under some policy π now becomes a traditional policy evaluation problem in RL with a different
reward function. In this paper we use mode-free methods to learn the model error function.
3.1	LEARNING h-STEP CUMULATIVE MODEL ERROR
We now introduce a principled method to learn the cumulative model error based on finite horizon
Bellman updates. Since Mh,p only differs from the original MDP in the reward function, we can
directly learn the h-step model error using the transition data sampled from a relay buffer. As discussed
in Section 2.2, when either the ground-truth or the approximate transition is deterministic, the W-
reward is just the expected Euclidean norm between the real and predicted next state. For stochastic
transitions, W-reward can be approximated by learning the g function in (2) as suggested in the
Wasserstein GAN (Arjovsky et al., 2017). Therefore, given a sampled transition data (st, at, rt, st+1),
we can directly compute the W-reward for learning. In addition, since the policy π used for computing
mode-based value is non-stationary during learning, which may or may not be available at the time of
evaluating the value expansion errors, we measure the value expansion error using a reference policy
∏. The choice of ∏ will be discussed later.
We use TD-learning to train the model error function. In particular, given a data sample (s, a, r, s0),
z⅛ ∙	1	.	11	♦	♦	♦	♦	.1	.	1 ʌ
E is updated by minimizing the one step Bellman error
min ɪ {w(s, a) + Y£(s0, a0, h — 1) — E(s, a, h)}
(7)
where a0 is selected using the policy ∏, and £ is the target model error value function. It is important
to note that this learning process can be combined with any model-based algorithm where a replay
buffer is used. Also, our method does not introduce additional sample complexity, since the data used
to train (7) can be sampled from the replay buffer.
Choice of the Reference Policy. Since the policy used for value expansion is changing during the
learning process and the cumulative model error is policy dependent, it is expensive to retrain the
model error for the current policy at every step. Thus, we choose the reference policy ∏ to “prepare
for the future”. There are several possibilities but we consider three that allow model error to be
efficiently learned with samples from the replay buffer.
4
Under review as a conference paper at ICLR 2020
1.	The conservative reference policy argmax∏ E(h∣π, s, V, P) targets the maximum model error.
When using the learned model error to decide a proper planning horizon, this policy allows us to
consider the worst case model error, making the selected horizon more robust in practice. To
learn the model error under this policy, we can use a0 = argmaxa E(s0, a, h - 1) when doing
the update (7). This can be viewed as an extension of Q-learning (Watkins & Dayan, 1992) to
learn the maximum model error.
2.	The greedy reference PoUcy selects argmax° Q(s, a), where Q is the current target Q-ValUe
function of the learning agent. This policy tries to measure the model error under the learning
agent’s current behavior.
3.	The replay buffer reference policy selects an action which occurred in the replay buffer at s.
This policy can be considered as a mixture policy of previous agent’s behaviors. As we only
care about the value function Eπ (s, h) and every sampled transition (s, a, r, s ) can be viewed
as “on-policy" under the replay buffer policy, We can directly estimate En (s, h) by on-policy
TD learning: ming 2{W(s, a) + YE(SIh — 1) -E(s,h)}2.
3.2	Adaptive Planning Horizon using Model Error
In this section, we introduce the Adaptive Model-based Value Expansion (AdaMVE) algorithm, as an
example of using a learned multi-step model error function to adapt planning horizon for different
states. Instead of applying a fixed horizon tuned for different environment when computing (1) as in
MVE, AdaMVE attempts to adapt the rollout horizon for different states according to a model error
function learned as described in the previous section. We suppose that the learner’s behavior policy π
is determined by Q-values. For a discrete domain, π = argmax Q(s, a). For a continuous domain, π
is trained to approximate the greedy policy over Q(s, a) as in DDPG (Lillicrap et al., 2015).
In AdaMVE, we aim to find a proper planning horizon H(s) ∈ [0, Hmax] for any state s ∈ S 1.
For state S ∈ S, we use the learned model error function E to produce the model error E(s,h) for
all rollout horizons h ∈ [0, Hmax]. Although E can be viewed as a good proxy for E in (4), it is
still difficult to directly find an appropriate planning horizon by setting a hard threshold, due to
the unknown Lipschitz constant K in Theorem 1. To resolve this issue, instead of setting a hard
threshold to get a maximum rollout horizon H(s), we use a soft weighted combination over all
horizons h ≤ Hmaχ. For horizon h with higher model error E(s,h) we set a smaller weight ωh. More
specifically, we define the weights according to a “softmax policy”
ω(h∣s) H exp
{-E(S,h”τ}
(8)
where τ is a temperature parameter. AdaMVE uses this policy as its weighting function to mix the
expansion values (1) of different rollout horizons,
Hmax
⅛mx(S) = X ω(h∣s)¾(s) .	(9)
To give a scalar measure of the “planning horizon” when using soft combination, for each state s
we define its weighted average horizon as H(S) = PhHmIx ω(h∣s) * h, which is the expected rollout
horizon under a “softmax policy”. When the model error is zero everywhere, ω(h∣s) = 1/(Hmax + 1)
and Vn	(S) is the average value estimates of all horizon. The average planing horizon is H(S)=
P,Hm
ax
Hmax/2. When the model error is infinitely large everywhere, ω(0∣s) = Land ω(h∣s) = 0 for all
h > 0 thus no rollout is being considered. The average planing horizon is H(S) = 0.
The combined value estimate (9) is used as the learning target to update Q(s, a). Specifically, at each
training step, AdaMVE samples a batch of data (s, a, r, s0) from a replay buffer B. For each s0 in the
batch, an on-policy Hmax-step rollout is computed using the model and the learning agent’s current
policy π. For each data (s, a,r, s0), we update Q by minimizing 2{r + YVPH (s0) 一 Q(s,a)}2 ,
where a target Q-value is used to compute the value at the end state of each rollout. Pseudocode of
AdaMVE is provided in the Appendix.
1 Hmax is set by computational consideration in practice. Our proposed method is valid for any large Hmax.
5
Under review as a conference paper at ICLR 2020
4	Related Work
Previous work in model-based reinforcement learning can be divided in two categories: using the
model for planning in low-dimensional state spaces, and combining the benefits of model-based
and model-free approaches. For the first category, Gal et al. (Gal et al., 2016) combine the PILCO
algorithm (Deisenroth & Rasmussen, 2011) with a neural dynamic model. Hafner et al. (Hafner
et al., 2018) propose to learn a latent dynamic model and choose actions through online planning
with the latent model. For the second category, Weber et al. use imaginary rollouts generated by the
dynamic model for policy learning (Racaniere et al., 2017). Gu et al. propose to augment imaginary
rollout data to the experience replay buffer and show that this can accelerate model-free learning (Gu
et al., 2016). In this paper, we use MVE (Feinberg et al., 2018) as the baseline algorithm to show the
effectiveness of our adaptive planning horizon algorithm. But it is important to note that our method
can be combined with any of the model-based methods discussed above.
The compounding error phenomenon of model-based RL is previously discussed in (Asadi et al.,
2018; Jiang et al., 2015; Talvitie, 2017; Wang et al., 2019). Surprisingly, relatively little work has
been done to solve this problem. Buckman et al. propose STEVE, which uses stochastic ensemble of
models and planning horizons to relax the error caused by using only one model with a fixed planning
horizon (Buckman et al., 2018). The ensemble with lowest variance is used as the learning target. In
comparison, our proposed method directly handles the compounding error by adaptively selecting
planning horizons based on a learned model error function. In addition to using a different design
idea, our approach has both the model and the model error as a single function, which is far less
computationally expensive than STEVE, that learns multiple models in an ensemble. It is also worth
noting that the model in our approach can be either hand designed, pretrained from another task, or
learned online.
5	Experiments
We conduct experiments on both gridworld and continuous control environments. For the gridworld
environment, we implement our adaptive value expansion (AdaMVE) based on DQN Mnih et al.
(2015) and compare to the vanilla DQN and its non-adaptive value expansion variant (MVE). For
continuous control, we implement AdaMVE with DDPG Lillicrap et al. (2015) and compare to the
vanilla DDPG and MVE. We perform a single update to the policy for all methods in comparison at
each environmental step.
5.1	Experiments on GridWorld
Visualizing the Adaptive Horizon. We first evaluate the learned adap-
tive planning horizon by visualizing in a gridworld with predefined im-
perfect model. If our method works correctly, we should observe a large
horizon at states where the model is accurate, but small horizon for those
states where the model deviates a lot from the true environment dynamics.
We use a FourRoom gridworld maze (Fig. 2). Each room has 9 × 9 cells.
The agent’s objective is to find the goal position (in red) starting from a
random initial position. There are five actions: left, right, up, down and
stay. The maximum length of each episode is 50. After each episode the
agent restarts in a random position. The reward is 1 when hitting the goal
Figure 2: FourRoom Env
and 0 in the other positions. A state is represented by the (x, y) coordinate. We evaluate the adaptive
planning horizon using the following models:
•	Oracle model. The transition function behaves exactly the same as the true environment.
•	3Room model. The transition function is true in three rooms, but completely wrong in the left
bottom room. For a given state and action in this room, the model simply produces a randomly
sampled next state from all possible positions.
•	NoWall model. This model ignores the existence of the wall. For example, given a state at one
side of a wall, if the agent takes the action towards the wall, this model will predict the position
overlapping with the wall as the next state, but in the true environment the agent will just stop at
the current position.
6
Under review as a conference paper at ICLR 2020
-2.0
-1.5
-1.0
-0.5
-2.5
-0.0
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
0	5	10	15	20
(b)	3Room Greedy
(c)	3Room Replay
(a) 3Room CSRV
(d) NoWall CSRV
(e) NoWall Greedy (f) NoWall Replay
0	6000	12000 18000 24000 30000
training steps
0	6000	12000 18000 24000 30000
training steps
0	6000	12000 18000 24000 30000
training steps

(g) Oracle	(h) 3Room	(i) NoWall
Figure 3: Experiment results on FourRoom. (a)-(f) Visualization of learned planning horizon for all
states. For each state, the average horizon H(S) weighted by (8) is presented. We use Hmax = 5 thus
H(S) <= Hmax/2 = 2.5 from definition. Our method can successfully adapt the planning horizon
when the model is imperfect. (g)-(i) Policy learning performance with different models. The shaded
area shows the standard error. Results clearly show that AdaMVE significantly outperforms MVE
when the model is imperfect (no wall model and 3room model).
We evaluate the three reference policies (conservative, greedy and replay buffer) discussed in Sec-
tion 3.1, denoted by csrv, greedy and replay respectively. The results are visualized in Fig. 3 (a)-(f).
For each state (position), We show its weighted average horizon H(s), as defined in Section 3.2. The
results clearly show the effectiveness of the proposed method in finding an appropriate planning
horizon for different parts of the state space. For example, in the 3room model, our method can adopt
zero planning horizon for states in the left bottom room where the model has large error.
Policy Learning Performance in FourRoom. We compare AdaMVE with MVE and DQN using
the three models described above. Results are presented in Fig. 3 (g)-(i). Each data point is averaged
over 5 runs. Each run is evaluated after every 2000 environmental steps by computing the mean
total episode reward across 10 episodes. When the oracle model is available, the model error is zero
everywhere, hence AdaMVE performs exactly the same with MVE. Both algorithms outperforms
DQN, which confirms that the model-based value expansion targets can lead to improved performance.
However, when the model is noisy, MVE diverges due to model errors. In contrast, AdaMVE still
converges, and does so significantly faster than DQN. This is because AdaMVE can adapt the rollout
horizon for states where the model has large error as shown in the visualization. For example, when
using the 3room model, AdaMVE only performs the model-free updates for states in the bottom left
room. In contrast, at these states MVE still trusts the multi-step value expansion target, which has
large error that causes diverge.
5.2	Continuous Control
We experiment on continuous control tasks to further verify the benefit of adaptive rollout horizons.
We first use Mujoco (Todorov et al., 2012) to create 3D mazes and learn to control a PointMass agent
to navigate to a goal area in the maze, starting from a random location, as shown in Fig 4(a-c). We
also test on two Mujoco control problems in OpenAI Gym (Brockman et al., 2016): HalfCheetah and
Swimmer. All results are based on 5 different runs. Each data point in the plots is evaluated by 200
test episodes for PointMass Navigation, and 50 test episodes for Mujoco control problems.
7
Under review as a conference paper at ICLR 2020
50-
100 •
150-
200 •
0	100	200	300
(a) PointNoWall
0.0	0.5	1.0	1.5	2.0
0	100	200	300	0	100	200	300	training steps (1e5)
(b) PointRoom (PR) (c) PointMaze (PM) (d) PR Pretrain PP
0 5 0 5
75-
50-
25-
0.0	0.5	1.0	1.5	2.0
training steps (1e5)
(e) PM Pretrain PP
SPLeMaL
0.0	0.5	1.0	1.5	2.0
training steps (1e5)
(f)	PR Online P
200 -
175 -
150 -
聿 125 -
N 100 -
50 -
0.0	0.5	1.0	1.5	2.0
training steps (1e5)
(g)	PM Online P
SPLeMaL
0	2	4	6	8	10
training steps (1e5)
(h) HalfCheetah
0-
SPLeMaL
0	2	4	6	8	10
training steps (1e5)
(i) Swimmer


Figure 4: Results of continuous control. (a)-(c) PointMass Navigation example environments. (d)-(e)
PointMass Navigation policy learning performance using pretrained model. (f)-(g) PointMass Navi-
gation policy learning performance using online learned model. (h)-(i) Policy learning performance
on HalfCheetah and Swimmer. AdaMVE outperforms both DDPG and MVE with pretrained model.
With online learned model, vanilla AdaMVE does not improve performance over the baselines.
SML-AdaMVE outperforms all the baseline algorithms with an online learned model.
Results with Pretrained Model. We first evaluate the adaptive method using a pretrained model on
the PointMass Navigation problem. We pretrain a model by executing a uniform-random policy in
PointNoWall (Fig 4 (a)). This pretrained model is then used as an imperfect model in PointRoom
(PR, Fig 4 (b)) and PointMaze (PM, Fig 4 (c)) without further training. This model is supposed to be
good at modeling local motions while bad at modeling interactions with the wall. For AdaMVE, we
use the replay reference policy and Hmax = 5. MVE applies a fixed rollout horizon 5. As shown in
Fig 4 (d)-(e), AdaMVE outperforms both MVE and DDPG, which further justifies the benefits of
selecting rollout horizons in an adaptive way.
Results with Online Learned Model. We next conduct experiments with an online learned model.
The model is updated by one gradient step at each environment step. We observe that in this setting it
is hard to achieve competitive performance by directly learning the model online. To fix this problem,
we propose selective model learning: for a batch B of data (s, a, s0) that are used for learning the
model, we rank the data according to the model error function E(s, hsmɪ), and use X percent of the data
that have small model errors to learn the model. By using this selective model learning approach, we
hope to learn a partially accurate model that only focus on the dynamics which are easy to be learned.
Our adaptive planning method can still benefit from such model since it is able to learn where the
model has large errors and only adopt a small planning horizon at those states. We also note that
using the model error function E is different with directly computing the model error for each data
in B, since E is learned by a reference policy and thus can provide more stable guidance for robust
model learning. Another advantage of using E is that we can tune the hsml parameter, in which case
we try to identify states whose nearby regions have large model error.
We denote AdaMVE with selective model learning by SML-AdaMVE.In all test domains, we use
the replay reference policy and Hmax = 3 for both AdaMVE and Sml-AdaMVE. We tune hsml from
{1, 2} and use x = 50 for selective model learning. For MVE, we tune the rollout horizon H from
{1, 3, 5} and report the best result. Surprisingly, we find that H = 1 gives the best results in all
test domains. We also compare our methods with STEVE, the state-of-the-art model-based value
expansion method (Buckman et al., 2018). In our implementation of STEVE, we use 3 value functions,
H = 3, and 3 independently online learned models to create ensembles. Results are presented in
Fig 4 (f)-(i). With an online learned model, vanilla AdaMVE does not improve performance over the
baselines, due to the difficulty to catch the error of an online updated model. However, by relaxing
the model learning procedure using selective model learning, SML_AdaMVE outperforms all the
baselines with an online learned model. Importantly, our proposed method has both the model and
the model error as a single function, which is far less computationally expensive than the stochastic
adaptive method STEVE, but show significantly better performance in practice.
8
Under review as a conference paper at ICLR 2020
6	Conclusion
We present a principled method to learn model errors by TD-learning in model-based reinforcement
learning. Based on the learned model errors, an adaptive approach to select state-dependent planning
horizons is introduced. Our proposed algorithm, AdaMVE, combines model-based and model-free
reinforcement learning by adaptively selecting the rollout horizons in model-based value expansion.
Empirical results shows that AdaMVE (i) successfully adapts the planning horizons according to the
local correctness of the model, (ii) outperforms model-free, non-adaptive and stochastic-adaptive
model-based baselines.
For the future work, we would like to combine our adaptive planning horizon method in other
model-based RL approaches such as model predictive control and Monte Carlo tree search. Another
future direction is how to learn a (partially correct) model online. In the experiments, we observe that
simply fitting a neural network with minibatch and training L2 losses is not good enough and a better
model learning method is needed. Our proposed selective model learning method is a preliminary
attempt to solve this problem. We believe learning a reasonably good model online in complex high
dimensional control tasks is an important problem and deserves thorough future studies.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Kavosh Asadi, Dipendra Misra, and Michael Littman. Lipschitz continuity in model-based reinforce-
ment learning. In International Conference on Machine Learning, pp. 264-273, 2018.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efficient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural
Information Processing Systems, pp. 8224-8234, 2018.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465-472, 2011.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine.
Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint
arXiv:1803.00101, 2018.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1582-1591, 2018.
Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving pilco with bayesian neural
network dynamics models. In Data-Efficient Machine Learning workshop, ICML, volume 4, 2016.
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning
with model-based acceleration. In International Conference on Machine Learning, pp. 2829-2838,
2016.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551,
2018.
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning
horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous
Agents and Multiagent Systems, pp. 1181-1189. International Foundation for Autonomous Agents
and Multiagent Systems, 2015.
9
Under review as a conference paper at ICLR 2020
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.
Matej Moravclk, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.
Sebastien Racaniere, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez
Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-
augmented agents for deep reinforcement learning. In Advances in neural information processing
systems, pp. 5690-5701, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
reinforcement learning in contextual decision processes. arXiv preprint arXiv:1811.08540, 2018.
Richard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating
dynamic programming. In Machine Learning Proceedings 1990, pp. 216-224. Elsevier, 1990.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Erik Talvitie. Self-correcting models for model-based reinforcement learning. In Thirty-First AAAI
Conference on Artificial Intelligence, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Cedric Villani. OPtimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement
learning. arXiv PrePrint arXiv:1907.02057, 2019.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
10
Under review as a conference paper at ICLR 2020
A	Appendix
A.1 Pseudocode
We provide the pseudocode for AdaMVE. We provide the pseudocode for discrete setting (based on
DQN), and the model error is learned by the replay buffer reference policy. This code can be easily
extend to continuous setting (based on DDPG) and model error learning with the other two reference
policies as discussed in Section 3.1.
Algorithm 1 Adaptive Mode-based Value Expansion
Input: maximum rollout horizon HmaX
Initialize the replay buffer B to capacity N
Initialize the approximate model Pν
Initialize action value function Qθ (and/or the policy funtion πθ if applicable)
Initialize state model error function Eφ with maximum rollout horizon Hmax
for t = 0 to T - 1 do
Sample transitions using the -greedy policy and store transitions in B
(If learn model) Sample transitions from B to learn the model Pν
Sample transitions from B to learn the model error Eφ
Sample a batch of transitions (s, a, r, s0) from B
For each s0 in the batch get Eφ(s0, h) for h ∈ [0, Hmax]
For each s0 compute the multi-step value expansion target for each h ∈ [0, Hmax]
Compute the target value V∏	(SJ) according to (9)
P ,Hmax
Update θ using model free RL by using R∏	(s0) as the target value
P ,Hmax
Update target network parameters for both policy training and model error training, θ and φ,
with exponential decay
end for
11
Under review as a conference paper at ICLR 2020
A.2 Proof of Theorem 1
Proof. For any 0 ≤ h ≤ H, define Uh to be the H -step value expansion that rolls out the true model
P for the first h steps and the approximate model P for the remaining H - h steps:
h-1	H-1
Uh= X YtEst 〜P∏(∙∣s) [R∏ (St)]+ X Y tEst 〜Pπ-h°Ph (∙∣s) [R∏ (St)]
+ YHEsH~PH-h。Pn(∙∣s) [V(SH)] ,	(IO)
where P- ◦ Ph(∙∣s) denotes the distribution over states after rolling out h steps With P and t - h
steps with P, i.e.
Plh ◦ Ph (∙∣s) = EPh (SlS)Ptπ-h(∙∣s0).
s0∈S
i -	. i t i` ∙ . ∙	ι~ τ τ	ι	τ τ	^r`τrτr /	∖	∙>	τ^ r	^r`τrτr /	∖	τ τ	ι
From the definition of Uh	we know Uo = Vh H(s)	and	UH =	VPH(s).	Hence we have
H-1
VPH(s) - V∏η(S) = Uo- UH =E(Uh- Uh+1) ∙
h=0
To analyze the difference between Uh and Uh+1 , we rearrange the terms in (10) in two different
ways:
h-1
Uh = X YtEst 〜Pn (.1s)[Rπ(St)] + YhEsh〜Pn(∙∣s)[V¾H-h(Sh)] ,	(11)
t=o
h
Uh = X YtEst 〜Pn (∙∣s) [Rh (St)]+ ThSEsh+i 〜Pn ◦Ph (∙∣s) [Vn,H-h-1(Sh+1)] .	(12)
t=o
Now applying (12) to Uh and (11) to Uh+1, we can bound Uh - Uh+1 by
h
X YtEst〜Pn(.|s)[Rh(St)] + Th+1%+1〜Pn◦Ph(∙∣s) [Vn,H-h-i (Sh+1)]
t=o
h
-X YtEst~P∏(∙ιs) [Rπ(St)] -yh+1Esh+ι~Pn+ι(∙ιs) [Vn,H-h-1(sh+1)]
t=o
=Y + Esh~Pn Gls),ah~πGlsh) E0~P(∙∣sh,ah) [VP,H-h-1(s )] - EsO zPdsh,ah) [VP,H-h-1
(S0)
≤ IlVΠ,H-h-1IlLY"+ IEsh~P∏ (∙∣s),ah~π(∙∣sh) [ sup Es0~P(∙∣sh,ah) [f (s')] - Es0~P(∙∣sh ,ah) [/3)]]
≤KYh+1Esh〜Pn(∙∣s) [Wπ(Sh)].
(13)
The bound (13) also holds for the opposite direction Uh+1 - Uh . Therefore
H-1	H-1
IVSH(s) - VnH(s)∣ ≤ X |Uh - Uh+1l ≤ K X Yh+1Esh〜Pn(∙∣s) [Wπ(Sh)],
h=o	h=o
which concludes the proof.	□
12
Under review as a conference paper at ICLR 2020
A.3 Extra Experiments on FourRoom
Comparison with MVE using other planning horizon. In Fig. 3 we compare AdaMVE with MVE
using a fixed horizon H = 5. In this section we provide experiment results compared with MVE_h1
(fixed horizon H = 1) and MVE_h3 (fixed horizon H = 3). AdaMVE still applies HmaX = 5.
AdaMVE clearly outperforms MVE-h1 and MVE-h3 on both domains in terms of sample efficiency
and final performance.
0 5 0 5 0 5 0
4 3 3 2 2 1 1
sp」eMQ」
0	6000	12000 18000 24000 30000
training steps
(a) 3Room
0 5 0 5 0 5 0
4 3 3 2 2 1 1
sp」eMQ」
0	6000	12000 18000 24000 30000
training steps
(b) No Wall
Figure 5: We compare AdaMVE with MVE_h1 (fixed planning horizon 1) and MVE_h3 (fixed
planning horizon 3). AdaMVE outperforms both methods on both domains in terms of sample
efficiency and final performance.
Transferring Adaptivety to Different Tasks If the environment dynamics is fixed, the model error
learned in one task can be directly transferred to a different task. To illustrate this, we create a new
task named FourRoom2, by changing the goal position in FourRoom from (15,15) to (2,18). In
FourRoom2, we first train a model error function in FourRoom, then directly apply the pre-trained
model error for AdaMVE. We include DQN and AdaMVE that learns from scratch in the new tasks
as the baseline algorithms for comparison. Results are shown in Fig. 6. AdaMVE with the transferred
model error is denoted by T-AdaMVE.
10
0 5 0 5 0 5
4 3 3 2 2 1
sp」eMQ」
0	6000	12000 18000 24000 30000
training steps
(a) 3Room
0 5 0 5 0 5
4 3 3 2 2 1
sp」eMQ」
10
5
0
0	6000	12000 18000 24000 30000
training steps
(b) No Wall
Figure 6: Evaluation of model error transformation.
Recall that the conservative policy tries to maximize the model error to provide a robust estimation.
Hence, the model error learned by this policy in FourRoom should still be effective for FourRoom2.
The result clearly confirms this intuition: with the pre-trained model error, AdaMVExsrv learns
much faster than the one learning from scratch. In comparison, the model error learned by the greedy
policy and replay policy are less useful for transfer between the two tasks. When the no wall model
is used, the transferred model error even brings negative effects. This is due to the fact that both
the greedy policy and replay policy are based on the learning agent’s behavior during training and
therefore would change in different tasks.
13
Under review as a conference paper at ICLR 2020
A.4 Experiment Details
Table 1 lists the parameters used in gridworld experiments. Table 2 lists the parameters used in
Mujoco Navigation. For selecting the mixing temperature τ , in Gridworld we pick the best one
(0.01) between 0.01 and 0.001. For Mujoco environments we use τ = 0.01 without further tuning.
In Swimmer, we use the double-Q trick to provide better performance (Fujimoto et al., 2018). For
selective model learning, the reported results use hsml = 2 for PointRoom, and hsml = 1 for other
domains.
Table 1: Parameters used in gridworld experiments.
Parameter	Values
Ggreedy	0.2
discount (γ)	0.98
τ	0.01
batch size	128
optimizer (all networks)	Adam
learning rate (q network)	0.001
learning rate (model error network)	0.0001
replay buffer initial size	2000
replay buffer size	106
target network update interval	1
target network mixing coefficient	0.001
Q network for policy learning	(200, 200, 200)
Q network for model error	(200, 200, 200)
nonlinearity	ReLU
Maximum rollout steps Hmax	5
Table 2: Parameters used in MUjoco Navigation.
Parameter	Values
action noise for exploration	N(0, 0.12)
discount (γ)	0.99
τ	0.01
batch size	128
optimizer (all networks)	Adam
learning rate (all networks)	0.0001
A network weight decay (DDPG)	1e-6
Q network weight decay (DDPG)	1e-3
replay buffer initial size	20000
replay buffer size	106
target netowrk update interval	1
target network mixing coefficient	0.001
Q network for both model error and policy learning	(300, 300, 300)
A network for both model error and policy learning	(300, 300)
nonlinearity	ReLU
Maximum rollout steps Hmax	5
14
Under review as a conference paper at ICLR 2020
A.5 Supporting Results
We provide supporting results in this section. Figure 7 (a) and (b) show the weighted average horizon
H during training in PointRoom and PointMaze with a pretrained model. The maximum horizon
Hmax is set to 5. We can see that H over different states has Very high variance, which is a sign
of successful adaptation since the pretrained model is wrong at the states that are next to the wall
(shorter rollout horizon) while being accurate at the states that are away from the wall (longer rollout
horizon). PointMass has shorter overall rollout horizons than PointRoom because PointMaze has an
extra wall and thus a smaller set of states where the pretrained model is accurate, hence enjoys less
benefit of using AdaMVE over DDPG than in PointRoom.
training steps (1e5)
(a) H(S) PointRoom
Figure 7: Supporting Results. When HmaX = 5, the maximum of weighted average horizon H is 2.5
according to the definition.
training steps (1e5)
(b) H(S) PointMaze
15