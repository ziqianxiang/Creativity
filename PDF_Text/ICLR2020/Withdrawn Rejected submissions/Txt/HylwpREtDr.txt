Under review as a conference paper at ICLR 2020
Active Learning for Graph Neural Networks
via Node Feature Propagation
Anonymous authors
Paper under double-blind review
Ab stract
Graph Neural Networks (GNNs) for prediction tasks like node classification or
edge prediction have received increasing attention in recent machine learning from
graphically structured data. However, a large quantity of labeled graphs is difficult
to obtain, which significantly limits the true success of GNNs. Although active
learning has been widely studied for addressing label-sparse issues with other
data types like text, images, etc., how to make it effective over graphs is an open
question for research. In this paper, we present an investigation on active learning
with GNNs for node classification tasks. Specifically, we propose a new method,
which uses node feature propagation followed by K-Medoids clustering of the
nodes for instance selection in active learning. With a theoretical bound analysis
we justify the design choice of our approach. In our experiments on four benchmark
datasets, the proposed method outperforms other representative baseline methods
consistently and significantly.
1	Introduction
Graph Neural Networks (GNN)(KiPf & Welling, 2016; VeliCkovic et al., 2017; Hamilton et al.,
2017; Wu et al., 2019) have been widely applied in many supervised and semi-supervised learning
scenarios such as node classifications, edge predictions and graph classifications over the past few
years. Though GNN frameworks are effective at fusing both the feature representations of nodes
and the connectivity information, people are longing for enhancing the learning efficiency of such
frameworks using limited annotated nodes. This property is in constant need as the budget for labeling
is usually far less than the total number of nodes. For example, in biological problems where a graph
represents the chemical structure (Gilmer et al., 2017; Jin et al., 2018) of a certain drug assembled
through atoms, it is not easy to obtain a detailed analysis of the function for each atom since getting
expert labeling advice is very expensive. On the other hand, people can carefully design a small
“seeding pool” so that by selecting “representative” nodes or atoms as the training set, a GNN can be
trained to get an automatic estimation of the functions for all the remaining unlabeled ones.
Active Learning (AL) (Settles, 2009; Bodo et al., 2011), following this lead, provides solutions
that select “informative” examples as the initial training set. While people have proposed various
methods for active learning on graphs (Bilgic et al., 2010; Kuwadekar & Neville, 2011; Moore
et al., 2011; Rattigan et al., 2007), active learning for GNN has received relatively few attention
in this area. Cai et al. (2017) and Gao et al. (2018) are two major works that study active learning
for GNN. The two papers both use three kinds of metrics to evaluate the training samples, namely
uncertainty, information density, and graph centrality. The first two metrics make use of the GNN
representations learnt using both node features and the graph; while they might be reasonable with a
good (well-trained) GNN model, the metrics are not informative when the label budget is limited
and/or the network weights are under-trained so that the learned representation is not good. On the
other hand, graph centrality ignores the node features and might not get the real informative nodes.
Further, methods proposed in Cai et al. (2017); Gao et al. (2018) only combine the scores using
simple linear weighted-sum, which do not solve these problems principally.
We propose a method specifically designed for GNN that naturally avoids the problems of methods
above1. Our method select the nodes based on node features propagated through the graph structure,
1Our code will be released upon acceptance.
1
Under review as a conference paper at ICLR 2020
making it less sensitive to inaccuracies of representation learnt by under-trained models. Then we
cluster the nodes using K-Medoids clustering; K-Medoids is similar to the conventional K-Means,
but constrains the centers to be real nodes in the graph. Theoretical results and practical experiments
prove the strength of our algorithm.
•	We perform a theoretical analysis for our method and study the relation between its classifi-
cation loss and the geometry of the propagated node features.
•	We show the advantage of our method over Coreset (Sener & Savarese, 2017) by comparing
the bounds. We also conjecture that similar bounds are not achievable if we use raw
unpropagated node features.
•	We compare our method with several AL methods and obtain the best performance over all
benchmark datasets.
2	Related Works
Active Learning (AL) aims at interactively choosing data points from the training pool to maximize
model performances, and has been widely studied both in theory (Beygelzimer et al., 2008; Hanneke,
2014) and practice (Settles, 2009; Shen et al., 2017). Recently, Sener & Savarese (2017) proposes to
compute a Coreset over the last-layer activation of a convolutional neural network. The method is
designed for general-purpose neural networks, and does not take the graph structure into account.
Early works on AL with graph-structured data (Dasarathy et al., 2015; Mac Aodha et al., 2014)
study non-parametric classification models with graph regularization. More recent works analyze
active sampling under the graph signal processing framework (Ortega et al., 2018; Chen et al., 2016).
However, most of these works have focused on the denoising setting where the signal is smooth over
the graphs and labels are noisy versions of node features. Similarly, optimal experimental design
(Pukelsheim, 2006; Allen-Zhu et al., 2017) can also apply to graph data but primarily deals with
linear regression problems, instead of nonlinear classification with discrete labels.
Graph Neural Networks (GNNs) (Hamilton et al., 2017; Velickovic et al., 2017; KiPf & Welling,
2016) are the emerging frameworks in the recent years when people try to model graph-structured
data. Most of the GNN variants follow a multi-layer Paradigm. In each layer, the network Performs a
message Passing scheme, so that the feature rePresentation of a node in the next layer could be some
neighborhood aggregation from its Previous layer. The final feature of a single node thus comPrises
of the information from a multi-hoP neighborhood, and is usually universal and “informative” to be
used for multiPle tasks. Recent works show the effectiveness of using GNNs in the AL setting. Cai
et al. (2017), for instance, ProPoses to linearly combine uncertainty, graPh centrality and information
density scores and obtains the oPtimal Performance. Gao et al. (2018) further imProves the result by
using learnable combination of weights with multi-armed bandit techniques. Instead of combining
different metrics, in this PaPer, we aPProach the Problem by clustering ProPagated node features.
We show that our one-steP active design outPerforms existing methods based on learnt network
rePresenations, in the small label setting, while not degrading in Performance for larger amounts of
labeled data.
3	Preliminaries
In this section, we describe a formal definition for the Problem of graPh-based active learning under
the node classification setting and introduce a uniform set of notations for the rest of the PaPer.
We are given a large graPh G = (V, E), where each node v ∈ V is associated with a feature vector
xv ∈ X ⊆ Rd, and a label yv ∈ Y = {1, 2, ..., C}. Let V = {1, 2, ..., n}, we denote the inPut
features as a matrix X ∈ Rn×d, where each row rePresents a node, and the labels as a vector
Y = (y1, ..., yn). We also consider a loss function l(M|G, X, Y ) that comPutes the loss over the
inputs (G, X, Y) for a model M that maps G,X to a prediction vector Y ∈ Yn.
Following Previous works on GNN(Cai et al., 2017; Hamilton et al., 2017), we consider the inductive
learning setting; i.e., a small part of Y is revealed to the algorithm, and we wish to minimize the
loss on the whole graph l(M|G, X, Y). Specifically, an active learning algorithm A is initially given
the graph G and feature matrix X. In step t of operation, it selects a subset st ⊆ [n] = {1, 2, ..., n},
2
Under review as a conference paper at ICLR 2020
and obtains yi for every i ∈ st. We assume yi is drawn randomly according to a distribution Py|xi
supported on Y; we use ηc(v) = Pr[y = c|v] to denote the probability that y = c given node v, and
η(v) = (ηι(v),..., ηc(V))T. Then A uses G, X and y% for i ∈ s0 ∪ s1 ∪∙∙∙∪ st as the training set
to train a model, using training algorithm M. The trained model is denoted as MAt. If M is the
same for all active learning strategies, we can slightly abuse the notation At = MAt to emphasize
the focus of active learning algorithms. A general goal of active learning is then to minimize the loss
under a given budget b:
min E[l(At|G, X, Y)]	(1)
s0∪…∪st
where the randomness is over the random choices of Y and A. We focus on M being the Graph
Neural Networks and their variants elaborated in detail in the following part.
3.1	Graph Neural Network Framework
Graph Neural Networks define a multi-layer feature propagation process similar to Multi-Layer
Perceptrons (MLPs). Denote the k-th layer representation matrix of all nodes as X(k), and X(0) ∈
Rn×d are the input node features. Graph Neural Networks (GNNs) differ in their ways of defining
the recursive function f for the next-layer representation:
X(k+1) J f(X(k)； g, Θk),	(2)
where Θk is the parameter for the k-th layer. Naturally, the input X satisfies X (0) = X by definition.
Graph Convolution Network (GCN). A GCN (Kipf & Welling, 2016) has a specific form of the
function f as:
X(k+1) J ReLU(SX(k)Θk),	(3)
where ReLU is the element-wise rectified-linear unit activation function (Nair & Hinton, 2010), Θk is
the parameter matrix used for transforming the size of feature representations to a different dimension
and S is the normalized adjacency matrix. Specifically, S is defined as:
S = (I + D)-2 (A +1)(I + D)-2,	(4)
where A is the original adjacency matrix associated with graph G and D is the diagonal degree matrix
of A. Intuitively, this operation updates node embeddings by the aggregation of their neighbors.
The added identity matrix I (equivalent to adding self-loops to G) acts in a similar spirit to the
residual links (He et al., 2016) in MLPs that bypasses shallow-layer representations to deep layers.
By applying this operation in a multi-layer fashion, a GCN encourages nodes that are locally related
to share similar deep-layer embeddings and prediction results thereafter.
For the classification task, it is normal to stack a linear transformation along with a softmax function
to the representation in the final layer, so that each class could have a prediction score. That is,
Y = softmax(X (K)Θκ),	(5)
where softmax(x) = exp(x)/ PcC=1 exp(xc) which makes the prediction scores have unit sum of 1
for all classes, and K is the total number of layers. We use the GCN structure as the fixed unified
model M for all the following discussed AL strategies A.
4	Active Learning Strategy & Theoretical Analysis
Traditionally, active learning algorithms choose one instance at a time for labeling, i.e., with |st| = 1.
However, for modern datasets where the numbers of training instances are very large, it would be
extremely costly if we re-train the entire system each time when a new label is obtained. Hence
we focus on the “batched” one-step active learning setting (Contardo et al., 2017), and select the
informative nodes once and for all when the algorithm starts. This is also called the optimal
experimental design in the literature (Pukelsheim, 2006; Allen-Zhu et al., 2017). Aiming to select the
b most representative nodes as the batch, our target (1) becomes:
min E[l(A0|G, X, Y )].	(6)
∣s0∣≤b
The node selection algorithm is described in Section 4.1, followed by the loss bound analysis in
Section 4.2, and the comparison with a closely related algorithm (K-Center in Coreset (Sener &
Savarese, 2017)) in Section 4.3.
3
Under review as a conference paper at ICLR 2020
4.1	Node Selection via Feature Propagation and K-Medoids Clustering
Algorithm 1 Active Learning with Distance-based Clustering
Input: Node representation matrix X, graph structure matrix G and budget b
1:	Compute a distance function dχ,c(∙, ∙) : V X V → R	#for FeatProp: use Eqn. (7)
2:	Perform clustering using dX,G with b centers	# for FeatProp: use K-Medoids
3:	Select s to be the centers
4:	Obtain labels for v ∈ s and train model M
Output: Model M
We describe a generic active learning framework using distance-based clustering in Algorithm 1. It
acts in two major steps: 1) computing a distance matrix or function dX,G using the node feature
representations X and the graph structure G; 2) applying clustering with b centers over this distance
matrix, and from each cluster select the node closest to the center of the cluster. After receiving the
labels (given by matrix Y ) of the selected nodes, we train a graph neural network, specifically GCN,
based on X, G and Y for the node classification task. Generally speaking, different options for the
two steps above would yield different performance in the down-stream prediction tasks; we detail
and justify our choices below and in subsequent sections.
Distance Function. Previous methods (Sener & Savarese, 2017; Cai et al., 2017; Gao et al., 2018)
commonly use network representations to compute the distance, i.e., dX,G(vi, vj ) = k(X(k) )i -
(X(k))j k2 for some specific k. While this can be helpful in a well-trained network, the representations
are quite inaccurate in initial stages of training and such distance function might not select the
representatitive nodes. Differently, we define the pairwise node distance using the L2 norm of the
difference between the corresponding propagated node features:
dX,G(vi,vj)= k(SKX)i-(SKX)jk2,	(7)
where (M)i denotes the i-th row of matrix M, and recall that K is the total number of layers.
Intuitively, this removes the effect of untrained parameters on the distance, while still taking the
graph structure into account.
Clustering Method. Two commonly used methods are K-Means (Cai et al., 2017; Gao et al., 2018)
and K-Center (Sener & Savarese, 2017)2. We propose to apply the K-Medoids clustering. K-Medoids
problem is similar to K-Means, but the center it selects must be real sample nodes from the dataset.
This is critical for active learning, since we cannot try to label the unreal cluster centers produced by
K-Means. Also, we show in Section 4.3 that K-Medoids can obtain a more favorable loss bound than
K-Center.
We call our method FeatProp, to emphasize the active learning strategy via node feature propagation
over the input graph, which is the major difference from other node selection methods.
4.2	Theoretical Analysis of Classification Loss B ound
Recall that we use k(SKX)i - (SKX)j k2 to approximate the pairwise distances between the
hidden representations of nodes in GCN. Intuitively, representation SKX resembles the output of a
simplified GCN (Wu et al., 2019) by dropping all activation functions and layer-related parameters in
the original structure, which introduces a strong inductive bias. In other words, the selected nodes
could possibly contribute to the stabilization of model parameters during the training phase of GCN.
The following theorem formally shows that using K-Medoids with propagated features can lead to a
low classification loss:
Theorem 1 (informal). Suppose that the label vector Y is sampled independently from the distribution
yi 〜η(i), and the loss function l is bounded by [—L, L]. Then under mild assumptions, there exists a
constant c0 such that with probability 1 - δ the expected classification loss of At satisfies
nl(Ao∣G, X,Y) ≤ n XXmin k(SKX)i — (SKX)jk2 + r L咪0	(8)
i=1 j∈s
2For a group of nodes, K-Center problem aims to find a δ-cover with at most k nodes for smallest possible δ.
4
Under review as a conference paper at ICLR 2020
To understand Theorem 1, notice that the first term Pin=1 minj∈s0 k(SKX)i - (SKX)j k2 is exactly
the target loss of K-Medoids (sum of point-center distances), and the second term
JL logn1∕δ) quickly
decays with n, where n is the total number of nodes in graph G. Therefore the classification loss
of A0 on the entire graph G is mostly dependent on the K-Medoids loss. In practice, we can utilize
existing robust initialization algorithms such as Partitioning Around Medoids (PAM) to approximate
the optimal solution for K-Medoids clustering.
The assumptions we made in Theorem 1 are pretty standard in the literature, and we illustrate the
details in the appendix. While our results share some common characteristics with Sener et al.(Sener
& Savarese, 2017), our proof is more involved in the sense that it relates to the translated features
k(SKX)i - (SKX)j k2 instead of the raw features k(X)i - (X)j k2. In fact, we conjecture that using
raw feature clustering selection for GCN will not result in a similar bound as in (8): this is because
GCN uses the matrix S to diffuse the raw features across all nodes in V , and the final predictions
of node i will also depend on its neighbors as well as the raw feature (X)i. We could see a clearer
comparison in practice in Section 5.2.
Figure 1: Visualization of Theorem 1. Consider the set of selected points s and the remaining points
in the dataset [n]\s. K-Medoids corresponds to the mean of all red segments in the figure, whereas
K-Center corresponds to the max of all red segments in the figure.
4.3 Why not K-Center
In this subsection we provide justifications on using the K-Medoids clustering method as opposed to
Coreset (Sener & Savarese, 2017). The Coreset approach aims to find a δ-cover of the training set. In
the context of using propagated features, this means solving
δ = min max min dX,G(vi, vj) = min max min k(SKX)i - (SKX)j k2	(9)
∣S0 l≤b i j∈s0	，	∣s0 ∣≤b i j∈s0
We can show a similar theorem as Theorem 1 for the Coreset approach:
Theorem 2. Under the same assumptions as in Theorem 1, with probability 1 - δ the expected
classification loss of At satisfies
L(Ao∣G,X,Y) ≤ co max min ∣∣ (SKX)i
n	i j∈s0
-(SK X )j k2 + JL02叵
(10)
Let di = minj∈s0 ∣(SKX)i - (SKX)j ∣2. It is easy to see that RHS of Eqn. (8) is smaller than RHS
of Eqn. (9), since 1 PZi di ≤ maxi di. In other words, K-Medoids can obtain a better bound than
the K-Center method (see Figure 1 for a graphical illustration). We observe superior performance of
K-Medoid clustering over K-Center clustering in our experiments as well (see Section 5.2).
5 Experiment
We evaluate the node classification performance of our selection method on the Cora, Citeseer, and
PubMed network datasets (Yang et al., 2016). We further supplement our experiment with an even
5
Under review as a conference paper at ICLR 2020
denser network dataset CoraFull (Bojchevski & Gunnemann, 2017) to illustrate the performance
differences of the comparing approaches on a large-scale setting. Table 1 summarizes the dataset
statistics.
Data I # Nodes		# Edges	# Classes	Feature size
Cora	2,708	5,429	7	3,703
Citeseer	3,327	4,732	6	1,433
PubMed	19,717	44,338	3	500
CoraFull	19,793	126,842	70	8,710
Table 1: Dataset statistics of different networks.
	Cora	Citeseer	PubMed	CoraFull
	 FeatProp	239	622	1,506	13,059
CoresetMIP	12,260	13,257	OOT	OOT
Coreset-greedy	44	46	509	636
Table 2: Comparison of running time of 5 different runs in seconds between our algorithm (FeatProp)
and Coreset. OOT denotes out-of-time. Note in order to get a more accurate solution, CoresetMIP
costs much more time than Coreset-greedy.
	Cora	Citeseer	PubMed	CoraFull
	 Random	59.83 ± 5.77	48.79 ± 4.03	71.66 ± 4.50	10.75 ± 0.92
Degree	63.30 ± 0.55	35.50 ± 0.82	60.54 ± 0.38	10.85 ± 0.30
Uncertainty	48.14 ± 8.18	39.14 ± 4.52	64.80 ± 8.21	6.76 ± 0.72
Coreset-greedy	59.99 ± 4.59	48.21 ± 3.78	68.41 ± 4.50	10.83 ± 1.28
CoresetMIP	55.86 ± 6.89	46.76 ± 3.99	-	-
AGE	65.01 ± 2.43	49.65 ± 5.19	67.96 ± 2.73	13.52 ± 0.81
ANRMAB	63.71 ± 4.34	47.29 ± 3.33	71.06 ± 4.82	11.40 ± 0.98
FeatProp	74.89 ± 2.63	51.03 ± 2.80	73.20 ± 1.81	14.86 ± 0.70
Table 3: Comparison of Macro-F1±standard_deviation averaged over different number of labeled
nodes for training. Bold fonts represent the best methods. CorsetMIP does not scale up for PubMed
and CoraFull datasets.
We evaluate the Macro-F1 of the methods over the full set of nodes. The sizes of the budgets are
fixed for all benchmark datasets. Specifically, we choose to select 10, 20, 40, 80 and 160 nodes as
the budget sizes. After selecting the nodes, a two-layer GCN 3, with 16 hidden neurons, is trained
as the prediction model. We use the Adam (Kingma & Ba, 2014) optimizer with a learning rate of
0.01 and weight decay of 5 × 10-4. All the other hyperparameters are kept as in the default setting
(β1 = 0.9, β2 = 0.999). To guarantee the convergence of the GCN, the model trained after 200
epochs is used to evaluate the metric on the whole set.
5.1	Baselines
We compared the following methods:
•	Random: Choosing the nodes uniformly from the whole vertex set.
•	Degree: Choosing the nodes with the largest degrees. Note that this method does not
consider the information of node features.
•	Uncertainty: Similar to the methods in Joshi et al. (2009), we put the nodes with max-
entropy into the pool of instances.
3In the past semi-supervised setting of citation networks, a two-layer GCN is the optimal structure for the
node classification task (Kipf & Welling, 2016).
6
Under review as a conference paper at ICLR 2020
•	Coreset (Sener & Savarese, 2017): This method performs a K-Center clustering over the
last hidden representations in the network. If time allows (on Cora and Citeseer), a robust
mixture integer programming method as in Sener & Savarese (2017) (dubbed CoresetMIP)
is adopted. We also apply a time-efficient approximation version (Coreset-greedy) for all of
the datasets. The center nodes are then selected into the pool.
•	AGE (Cai et al., 2017): This method linearly combines three metrics - graph centrality,
information density, and uncertainty and select nodes with the highest scores.
•	ANRMAB (Gao et al., 2018): This method enhances AGE by learning the combination
weights of metrics through an exponential multi-arm-bandit updating rule.
•	FeatProp: This is our method. We perform a K-Medoids clustering to the propogated
features (Eqn. (7)), where X is the input node features. In the experiment, we adopts an
efficient approximated K-Medoids algorithm which performs K-Means until convergence
and select nodes cloesest to centers into the pool.
I'o-Drow I'obrow
10	20	40	80	160
# Labeled data
PubMed
Citeseer
0.7-
0.6-
W 0.5-
0.2-
10	20	40	80	160
# Labeled data
CoraFuII
0.25-
i 0-20-
§015
ro
W 0.10
0.05-
0.00
10	20	40	80	160
# Labeled data
—Random	—Uncertainty	→- CoresetMIP	→- ANRMAB
-∙- Degree	—Coreset-greedy	—AGE	-∙- FeatProp
Figure 2:	Results of different approaches over benchmark datasets averaged from 5 different runs.
5.2 Experiment Results
In our experiments, we start with a small set of nodes (5 nodes) sampled uniformly at random from
the dataset as the initial pool. We run all experiments with 5 different random seeds and report the
averaged classification accuracy as the metric. We plot the accuracy vs the number of labeled points.
For approaches (Uncertainty, Coreset, AGE and ANRMAB) that require the current status/hidden
representations from the classification model, a fully-trained model built from the previous budget
pool is returned. For example, if the current budget is 40, the model trained from 20 examples
selected by the same AL method is used.
Main results. As is shown in Figure 2, our method outperforms all the other baseline methods in
most of the compared settings. It is noticeable that AGE and ANRMAB which use uncertainty score
as their sub-component can achieve better performances than Uncertainty and are the second best
methods in most of the cases. We also show an averaged Macro-F1 with standard deviation across
different number of labeled nodes in Table 3. It is interesting to find that our method has the second
7
Under review as a conference paper at ICLR 2020
smallest standard deviation (Degree is deterministic in terms of node selection and the variance only
comes from the training process) among all methods. We conjecture that this is due to the fact that
other methods building upon uncertainty may suffer from highly variant model parameters at the
beginning phase with very limited labeled nodes.
Efficiency. We also compare the time expenses between our method and Coreset, which also involves
a clustering sub-routine (K-Center), in Table 2. It is noticeable that in order to make Coreset more
stable, CoresetMIP uses an extreme excess of time comparing to Coreset-greedy in the same setting.
An interesting fact we could observe in Figure 2 is that CoresetMIP and Coreset-greedy do not have
too much performance difference on Citeseer, and Coreset-greedy is even better than CoresetMIP
on Cora. This is quite different from the result in image classification tasks with CNNs (Sener &
Savarese, 2017). This phenomenon distinguishes the difference between graph node classification
with traditional classification problems. We conjecture that this is partially due to the fact that the
nodes no longer preserve independent embeddings after the GCN structure, which makes the original
analysis of Coreset not applicable.
—FeatProp —FeatProp w/ network reprensentation	—>— FeatProp w/ kcenter
Figure 3:	Results of different approaches over benchmark datasets averaged from 5 different runs.
Similar to Coreset, the orange line denotes replacing the original distance function in Eqn. (7) with
L2 distance from the final GCN layer. The blue line denotes the algorithm replacing the K-Medoids
module with K-Center clustering.
Ablation study. It is crucial to select the proper distance function and clustering subroutine for
FeatProp (Line 1 and Line 2 in Algorithm 1). As is discussed in Section 4.3, we test the differences
with the variant of using the L2 distance from the final layer of GCN as the distance function and
the one by setting K-Medoids choice with a K-Center replacement. We compare these algorithms in
Figure 3. As is demonstrated in the figure, the K-Center version (blue line) has a lower accuracy than
the original FeatProp approach. This observation is compatible with our analysis in Section 4.3 as
K-Medoids comes with a tighter bound than K-Center in terms of the classification loss. Furthermore,
as final layer representations are very sensitive to the small budget case, we observe that the network
representation version (orange line) also generally shows a much deteriorated performance at the
beginning stage.
Though FeatProp is tailored for GCNs, we could also test the effectiveness of our algorithm over
other GNN frameworks. Specifically, we compare the methods over a Simplified Graph Convolution
(SGC) (Wu et al., 2019) and obtain similar observations. Due to the space limit, we put the detailed
results in the appendix.
6 Conclusion
We study the active learning problem in the node classification task for Graph Convolution Networks
(GCNs). We propose a propagated node feature selection approach (FeatProp) to comply with
the specific structure of GCNs and give a theoretical result characterizing the relation between its
classification loss and the geometry of the propagated node features. Our empirical experiments also
show that FeatProp outperforms the state-of-the-art AL methods consistently on most benchmark
datasets. Note that FeatProp only focuses on sampling representative points in a meaningful (graph)
representation, while uncertainty-based methods select the active nodes from a different criterion
8
Under review as a conference paper at ICLR 2020
guided by labels, how to combine that category of methods with FeatProp in a principled way remains
an open and yet interesting problem for us to explore.
References
Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization
for experimental design: A regret minimization approach. CoRR, abs/1711.05174, 2017. URL
http://arxiv.org/abs/1711.05174.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning.
arXiv preprint arXiv:0812.4952, 2008.
Mustafa Bilgic, Lilyana Mihalkova, and Lise Getoor. Active learning for networked data. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 79-86,
2010.
Zalan Bodo, Zsolt Minier, and Lehel Csato. Active learning with clustering. In Active Learning and
Experimental Design workshop In conjunction with AISTATS 2010, pp. 127-139, 2011.
Aleksandar Bojchevski and Stephan Gunnemann. Deep gaussian embedding of graphs: Unsupervised
inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.
Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. Active learning for graph embedding.
arXiv preprint arXiv:1705.05085, 2017.
S. Chen, R. Varma, A. Singh, and J. Kovacevic. Signal recovery on graphs: Random versus
experimentally designed sampling. IEEE Transactions on Signal and Information Processing over
Networks, special issue on Inference and Learning over Networks, 2(4):539-554, 2016.
Anna Choromanska, Yann LeCun, and Gerard Ben Arous. Open problem: The landscape of the loss
surfaces of multilayer networks. In Conference on Learning Theory, pp. 1756-1760, 2015.
Gabriella Contardo, Ludovic Denoyer, and Thierry Artieres. A meta-learning approach to one-step
active learning. arXiv preprint arXiv:1706.08334, 2017.
Gautam Dasarathy, Robert D. Nowak, and Xiaojin Zhu. S2: an efficient graph based active learning
algorithm with application to nonparametric classification. In Peter Grunwald, Elad Hazan,
and Satyen Kale (eds.), Proceedings of The 28th Conference on Learning Theory, COLT 2015,
Paris, France, July 3-6, 2015, volume 40 of JMLR Workshop and Conference Proceedings,
pp. 503-522. JMLR.org, 2015. URL http://jmlr.org/proceedings/papers/v40/
Dasarathy15.html.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
Li Gao, Hong Yang, Chuan Zhou, Jia Wu, Shirui Pan, and Yue Hu. Active discriminative network
representation learning. In Proceedings of the 27th International Joint Conference on Artificial
Intelligence, pp. 2142-2148. AAAI Press, 2018.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Steve Hanneke. Theory of active learning. Foundations and Trends in Machine Learning, 7(2-3),
2014.
9
Under review as a conference paper at ICLR 2020
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. In The Collected
Works of Wassily Hoeffding, pp. 409-426. Springer, 1994.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. arXiv preprint arXiv:1802.04364, 2018.
Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image
classification. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2372-
2379. IEEE, 2009.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information
processing systems, pp. 586-594, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Ankit Kuwadekar and Jennifer Neville. Relational active learning for joint collective classification
models. In Proceedings of the 28th international conference on machine learning (icml-11), pp.
385-392. Citeseer, 2011.
Oisin Mac Aodha, Neill D. F. Campbell, Jan Kautz, and Gabriel J. Brostow. Hierarchical subquery
evaluation for active learning on a graph. In 2014 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pp. 564-571. IEEE
Computer Society, 2014. ISBN 978-1-4799-5118-5. doi: 10.1109/CVPR.2014.79. URL https:
//doi.org/10.1109/CVPR.2014.79.
Cristopher Moore, Xiaoran Yan, Yaojia Zhu, Jean-Baptiste Rouquier, and Terran Lane. Active
learning for node classification in assortative and disassortative networks. In Proceedings of
the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
841-849. ACM, 2011.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
A. Ortega, P. Frossard, J. Kovaevi, J. M. F. Moura, and P. Vandergheynst. Graph signal processing:
Overview, challenges, and applications. Proceedings of the IEEE, 106(5):808-828, May 2018.
ISSN 0018-9219. doi: 10.1109/JPROC.2018.2820126.
Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.
Matthew J Rattigan, Marc Maier, and David Jensen. Exploiting network structure for active inference
in collective classification. In Seventh IEEE International Conference on Data Mining Workshops
(ICDMW 2007), pp. 429-434. IEEE, 2007.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489, 2017.
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2009.
Yanyao Shen, Hyokun Yun, Zachary Lipton, Yakov Kronrod, and Animashree Anandkumar. Deep
active learning for named entity recognition. Proceedings of the 2nd Workshop on Representation
Learning for NLP, 2017. doi: 10.18653/v1/w17-2630. URL http://dx.doi.org/10.
18653/v1/W17-2630.
10
Under review as a conference paper at ICLR 2020
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and YoshUa
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Felix WU, Tianyi Zhang, AmaUri Holanda de SoUza Jr, Christopher Fifty, Tao YU, and Kilian Q
Weinberger. Simplifying graph convolUtional networks. arXiv preprint arXiv:1902.07153, 2019.
KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jUmping knowledge networks. arXiv preprint
arXiv:1806.03536, 2018.
Zhilin Yang, William W Cohen, and RUslan SalakhUtdinov. Revisiting semi-sUpervised learning with
graph embeddings. arXiv preprint arXiv:1603.08861, 2016.
11
Under review as a conference paper at ICLR 2020
A Addendum to Experiments
We also evaluate the methods using the metric of Micro-F1 in Table 4.
	Cora	Citeseer	PubMed	CoraFull
	 Random	65.19 ± 4.39	57.04 ± 3.39	73.11 ± 2.61	21.78 ± 1.97
Degree	68.61 ± 0.50	46.13 ± 0.77	68.44 ± 0.32	25.12 ± 0.53
Uncertainty	58.88 ± 6.07	46.08 ± 4.44	68.49 ± 5.55	14.66 ± 1.80
Coreset-greedy	66.94 ± 2.87	55.00 ± 3.09	70.74 ± 3.15	24.61 ± 2.35
CoresetMIP	63.98 ± 5.40	55.68 ± 3.43	-	-
AGE	70.07 ± 1.36	57.27 ± 4.68	73.80 ± 1.91	28.35 ± 1.16
ANRMAB	68.62 ± 3.66	54.90 ± 3.61	73.98 ± 3.37	23.14 ± 1.79
FeatProp	77.68 ± 1.81	59.36 ± 1.98	74.64 ± 1.49	28.86 ± 1.22
Table 4: Comparison of Micro-Fl ±standard_deviation averaged over different number of labeled
nodes for training. Bold fonts represent the best methods. CorsetMIP does not scale up for PubMed
and CoraFull datasets.
We evaluate the performances of different active learning methods on a 2-layer SGC (Simplified
Graph Convolution) framework. The results can be seen in Figure 4.
—Random
—Degree
—Uncertainty —AGE —FeatProp
—Coreset-greedy —ANRMAB
Figure 4:	Results of different approaches over benchmark datasets averaged from 5 different runs on
an SGC framework.
Figure 5	shows the results of FeatProp on different GNN frameworks. We see that SGC has a slightly
inferior performance to GCN since it drops all the activation functions and in-layer parameters, but
not too much.
12
Under review as a conference paper at ICLR 2020
→- SGC →- GCN
Figure 5: Results of SGC vs GCN over benchmark datasets averaged from 5 different runs by using
FeatProp.
B Proof of Theorem 1
For simplicity, for any model M let (M)i = (M(G, X))i ∈ RC be the prediction for node i under
input G, X, and (M)i,c be the c-th element of (M)i (i.e., the prediction for class c). In order to show
Theorem 1, we make the following assumptions:
Assumption 1. We assume A0 overfits to the training data. Specifically, we assume the following
two conditions: i) A0 has zero training loss on s0 ; ii) for any unlabeled data (xi , xj ) with i 6∈ s0 and
j ∈ s0, we have (A0)i,yj ≤ (A0)j,yj and (A0)i,c ≥ (A0)j,c for all c 6= yj. The second condition
states that A0 achieves a high confidence on trained samples and low confidence on unseen samples.
We also assume that the class probabilities are given by a ground truth GCN; i.e., there exists a GCN
M* that predicts Pr[Yi = c] on the entire training set. This is a common assumption in the literature,
and (Du et al., 2018) shows that gradient descent provably achieves zero training loss and a precise
prediction in polynomial time.
Assumption 2. We assume l is Lipschitz with constant λ and bounded in [-L, L]. The loss function
is naturally Lipschitz for many common loss functions such as hinge loss, mean squared error, and
cross-entropy if the model output is bounded. This assumption is widely used in DL theory (e.g.,
Allen-Zhu et al. (2018); Du et al. (2018)).
Assumption 3. We assume that there exists a constant α such that the sum of input weights of
every neuron is less than a. Namely, We assume Pii ∣(Θκ)i,j | ≤ α. This assumption is also present
in (Sener & Savarese, 2017). We note that one can make Pii ∣(Θκ)i,j | arbitrarily small without
changing the netWork prediction; this is because dividing all input Weights by a constant t Will also
divide the output by a constant t.
Assumption 4. We assume that ReLU function activates with probability 1/2. This is a common
assumption in analyzing the loss surface of neural networks, and is also used in (Choromanska et al.,
2015; Kawaguchi, 2016; Xu et al., 2018). This assumption also aligns with observations in practice
that usually half of all the ReLU neurons can activate.
With these assumptions in place, we are able to prove Theorem 1.
Theorem 1 (restated). Suppose Assumptions 1-4 hold, and the label vector Y is sampled indepen-
dently from the distribution y。〜η(v) for every V ∈ V. Then with probability 1 - δ the expected
classification loss of At satisfies
1 i(a0∣g,x,y)≤ (λ+蜉/2)K XXmink(SKX)i - (SKXjk2+JLlogy).
n	n	j∈s0	2n
13
Under review as a conference paper at ICLR 2020
Proof. Fix yj for j ∈ s0 and therefore the resulting model A0 . Let i ∈ V \ s0 be any node and
j ∈ s0. We have
C
Ey~η(i) [l((A0)i, y)] = X Pr[Yi = c]l((A0)i,c, c)
c=1
CC
= X Pr[Yj = c]l((A0)i,c, c) + X (Pr[Yi = c] - Pr[Yj = c]) l((A0)i,c, c).
c=1	c=1
(11)
For the first term we have
CC
X Pr[Yj = c]l((A0)i,c, c) = X Pr[Yj = c] [l((A0)i,c, c) - l((A0)j,c, c)]
C
+ X Pr[Yj = c]l((A0)j,c, c)
c=1
C
= X Pr[Yj = c] [l((A0)i,c, c) - l((A0)j,c, c)]
c=1
C
≤ λ X Pr[Yj = c] |(A0)i,c - (A0)j,c|	(12)
c=1
The last inequality holds from the Lipschitz continuity of l. Now from Assumption 1, we have
(A0)i,c ≥ (A0)j,c for c 6= Yj and (A0)i,c ≤ (A0)j,c otherwise. Now taking the expection w.r.t the
randomness in ReLU we have
Eσ[(A0)i,c-(A0)j,c] =Eσ hσ((SX(K-1))iΘcK) - σ((SX(K-1))jΘcK)i
=1 Eσ h(SX(KT))iΘK -(SX(KT))jΘKi
≤ 2Eσ h(SX(KT))i - (SX(K-1))ji
≤ …≤ (2)k∣∣(SkX)i-(SKX训.	(13)
Here Eσ represents taking the expectation w.r.t ReLU. Now for (12) we have
C
Eσ X Pr[Yj = c] |(A0)i,c - (A0)j,c|
c=1
=Eσ	Pr[Yj = c] ((A0)i,c - (A0)j,c) +
c6=Yj
Eσ [Pr[Yj = yj] ((A0)j,yc - (A0)i,c)]
CK
≤ X Pr[Yj = c] (2)	II(SKX)i - (SKX训
c=1
The inequality follows from (13).
Now for the second loss in (11) We Use the property that M* computes the ground truth:
(Pr[Yi = c] — Pr[Yj = c]) l((Ao)i,c, C) = ((M*)i,c - (M*)j,c) l((Ao)i,c, C)
We now use the fact that ReLU activates with probability 1/2, and compute the expectation:
Eσ [((M*)i,c - (M*)j,c) l((Aθ)i,c, C)] = Eσ [((M*)i,c - (M*)j,c)] l((Aθ)i,c
=(Eσ [(M*)i,c] - Eσ [(M*)j,c]) l((Aθ)i,c
=2K ((SKXSKX)j) Θ1Θ2 …ΘKl((Ao)i,c
≤ L (2)Kk(SKX)i-(SKX)jk.
14
Under review as a conference paper at ICLR 2020
Here Eσ means that We compute the expectation w.r.t randomness in σ (ReLU) in M*. The last
inequality follows from definition of α, and that l ∈ [-L, L].
Combining the two parts to (11) and letj = argmin k(SKX)i - (SKX)j k, we obtain
Ey~η(i),σ [l((Ao)i, y)] ≤ (λ + L)(α∕2)K min k(SKX)i - (SKX j k.	(14)
Now notice that
l(A0|G, X, Y) = X l((A0)i, yi) + X l((A0)j, yj ) = X l((A0)i, yi).	(15)
i∈V \s0	j∈s0	i∈V \s0
Consider the following process: we first get G, X (fixed data) as input, which induces η(i) for i ∈ [n].
Note that M* gives the ground truth η(i) for every i so distributions η(i) ≡ ηχ,o(i) are fixed once
we obtain G, X 4. Then the algorithm A choose the set s0 to label. After that, we randomly sample
yj 〜η(j) for j ∈ s0 and use the labels to train model A°. At last, we randomly sample yi 〜η(i) and
obtain loss l(Ao∣G, X, Y). Note that the sampling of all yi for i ∈ V \ s0 is after we fix the model
Ao, and knowing exact values of yj for j ∈ s0 does not give any information of yi (since η(i) is only
determined by G, X). Now we use Hoeffding,s inequality (Theorem 3) with Zi = l((Ao)i, y%); we
have -L ≤ Zi ≤ L by our assumption, and recall that |V \ s01 = n - b. Let δ be the RHS of (17),
we have that with probability 1 - δ,
1
n-b
X l((A0)i,yi) - ɪEy^n(i),σ [l((A0)i,yi)] ≤ 1 LIog(I/δ) .
n - b	2(n - b)
i∈V \s0
Now plug in (14), multiply both sides by (n - b) and rearrange. We obtain that
X l((Ao)i,yi) ≤ X (λ + L)(α∕2)K min k(S K X 卜-(SK X j∣ + J L log(1/；)(n - b).
i∈V \s0	i∈V \s0
(16)
Now note that since the random draws of y% is completely irrelevant with training of Ao, we can also
sample yi together with yj for j ∈ s0 after receiving G, X and before the training of Ao (A does not
have access to the labels anyway). So (16) holds for the random drawings of all y's. Now divide both
sides of (16) by n and use (15), we have
1 l(Ao∣G,X,Y) ≤ (λ + L)(a/2)K Xmin k(SκX)i - (SkX)j ∣∣2 + JLlog(1∕δ2(n - bl
n	n	j∈s0	2n2
i=1
≤ I2K Xmisn k(sκ X )i-(sκ X )jk2+严2叵.
i=1
□
C Proof of Theorem 2
The same proof as Theorem 1 applies for Theorem 2 using the max of distances instead of averaging.
We therefore omit the details here.
D Hoeffding’s Inequality
We attach the Hoeffding’s inequality here for the completeness of our paper.
4To make a rigorous argument, we get the activation of M* in this step, meaning that we pass through the
randomness of σ in M*.
15
Under review as a conference paper at ICLR 2020
Theorem 3 (Hoeffding’s Inequality, Hoeffding (1994)). Suppose Z1, ..., Zn are independent random
variables such that ai ≤ Zi ≤ bi almost surely for 1 ≤ i ≤ n. Then we have
Pr
1n
_ X Zi
n
-E
i=1
1n
-X Zi
n
i=1
> t ≤ exp
—
2n2t2
Pn=I (bi- a2)2
(17)
16