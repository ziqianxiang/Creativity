Under review as a conference paper at ICLR 2020
Universal Approximation with Deep Narrow
Networks
Anonymous authors
Paper under double-blind review
Ab stract
The classical Universal Approximation Theorem certifies that the universal ap-
proximation property holds for the class of neural networks of arbitrary width.
Here we consider the natural ‘dual’ theorem for width-bounded networks of arbi-
trary depth. Precisely, let n be the number of inputs neurons, m be the number of
output neurons, and let ρ be any nonaffine continuous function, with a continuous
nonzero derivative at some point. Then we show that the class of neural networks
of arbitrary depth, width n+ m+ 2, and activation function ρ, exhibits the univer-
sal approximation property with respect to the uniform norm on compact subsets
of Rn . This covers every activation function possible to use in practice; in partic-
ular this includes polynomial activation functions, making this genuinely different
to the classical case. We go on to consider extensions of this result. First we
show an analogous result for a certain class of nowhere differentiable activation
functions. Second we establish an analogous result for noncompact domains, by
showing that deep narrow networks with the ReLU activation function exhibit the
universal approximation property with respect to the p-norm on Rn . Finally we
show that width of only n + m + 1 suffices for ‘most’ activation functions.
1	Introduction
The Universal Approximation Theorem (Cybenko, 1989; Hornik, 1991; Pinkus, 1999) states that
universal approximation holds for the class of neural networks with a single hidden layer of arbitrary
width, with any continuous nonpolynomial activation function:
Theorem 1.1. Let ρ : R → R be any continuous function. Let Nnρ represent the class of neural
networks with activation function ρ, with n neurons in the input layer, one neuron in the output
layer, and one hidden layer with an arbitrary number of neurons. Let K ⊆ Rn be compact. Then
Nnρ is dense in C(K) if and only if ρ is nonpolynomial.
What if arbitrary width is replaced with arbitrary depth? Put more precisely, can networks of
bounded width and arbitrary depth provide universal approximation? In some sense this poses a
question ‘dual’ to the problem answered by the classical Universal Approximation Theorem. We
refer to networks of this type as deep, narrow networks.
Furthermore we might ask how narrow the network may be, and what activation functions may be
admitted. We provide a near-complete answer to these various questions.
Universal approximation may be established with respect to more than one topology. Continuous
activation functions beget networks representing continuous functions. Thus when working with
respect to the uniform norm, it is natural to seek density in C(K; Rm) for K ⊆ Rn. When working
with respect to the p-norm, it is natural to seek density in Lp(Rn; Rm) for p ∈ [1, ∞). In this latter
case we may hope to generalise to noncompact domains, as functions in Lp(Rn; Rm) must exhibit
some sort of decay.
The primary motivation for this work stems from the work of Lu et al. (2017), who study this
question in the special case of the popular ReLU activation function, and who establish density
in L1 (Rn). The other notable result we are aware of is the work of Hanin & Sellke (2017), who
show another special case: they also consider the ReLU activation function, and establish density in
C(K; Rm) for K ⊆ Rn compact.
1
Under review as a conference paper at ICLR 2020
This article demonstrates generalisations of these results, in particular to general activation func-
tions, without relying on the strong algebraic and analytic properties of the ReLU activation func-
tion. This also improves certain results specific to the ReLU.
The rest of the paper is laid out as follows. Section 2 discusses existing work. Section 3 provides
a brief summary of our results; these are then presented in detail in Section 4. Section 5 is the
conclusion. Several proofs are deferred to the appendices, due to length and technical content.
2	Context
Some positive results have been established showing that particular classes of networks are dense in
certain spaces. Some negative results have also been established, showing that insufficiently wide
networks will fail to be dense.
Hanin & Sellke (2017) have shown that deep narrow networks with the ReLU activation function
exhibit the universal approximation property in C(K; Rm) for K ⊆ Rn compact.
Lu et al. (2017) have shown that deep narrow networks with the ReLU activation function exhibit
the universal approximation property in L1 (Rn), whilst Lin & Jegelka (2018) have shown that a
particular description of residual networks, with the ReLU activation function, also exhibit the uni-
versal approximation property in this space. We are not aware of any results for the general case of
Lp(Rn;Rm) forp ∈ [1, ∞).
We do not know of any positive results applying to activation functions other than the ReLU.
Regarding widths insufficient for a class of deep narrow networks to exhibit the universal approx-
imation property, consider the case of a network with n input neurons and a single output neuron.
For certain activation functions, Johnson (2019) shows that width n is insufficient to give density in
C(K). For the ReLU activation function, Lu et al. (2017) show that width n is insufficient to give
density in L1 (Rn), and that width n - 1 is insufficient in L1 ([-1, 1]n). For the ReLU activation
function, Hanin & Sellke (2017) shows that width n is insufficient to give density in C(K), and that
in fact that this is the greatest possible width not achieving universal approximation in this context.
The precise minimum width for activation functions other than ReLU, or for multiple output neu-
rons, remains unknown.
Everything discussed so far is in the most general case of approximating functions on Euclidean
space: in the language of machine learning, they are regression tasks. There has been some re-
lated work in the special case of classification tasks, for example Beise et al. (2018); Szymanski
& McCane (2012); Rojas (2003); Nguyen et al. (2018). There has also been some related work
in the special case of certain finite domains; Le Roux & Bengio (2010) show that networks with
sigmoid activation function and width n can approximate any distribution on {0, 1}n. See also
SUtskever & Hinton (2008). Montufar (2014) considers the analogous scenario for distributions on
{0,1,...,q-1}n.
3	Summary of Results
Definition 3.1. Let ρ: R → R and n, m, k ∈ N. Then let NNnρ,m,k represent the class of functions
Rn → Rm described by neural networks with n neurons in the input layer, m neurons in the output
layer, k neurons in each hidden layer, and an arbitrary number of hidden layers, such that every
neuron in every hidden layer has activation function ρ, and every neuron in the output layer has the
identity activation function.
Our central result is the following theorem.
Theorem 3.2. Let ρ : R → R be any continuous function which is continuously differentiable at at
least one point, with nonzero derivative at that point. Let K ⊆ Rn be compact. Then NNnρ,m,n+m+2
is dense in C(K; Rm).
The technical condition is very weak; in particular it is satisfied by every piecewise-C 1 function
not identically zero. Thus any activation function that one might practically imagine using on a
computer must satisfy this property.
2
Under review as a conference paper at ICLR 2020
Theorem 3.2 is proved by handling particular classes of activation functions as special cases. First
we have the result for nonpolynomial activation functions, for which the width can be made slightly
smaller.
Theorem 4.4. Let ρ : R → R be any continuous nonpolynomial function which is continuously
differentiable at at least one point, with nonzero derivative at that point. Let K ⊆ Rn be compact.
Then NNnρ,m,n+m+1 is dense in C(K; Rm).
We observe a corollary for noncompact domains, which generalises Lu et al. (2017, Theorem 1) to
multiple output neurons, a narrower width, and Lp for p > 1 instead of just p = 1.
Corollary 4.6. Let ρ be the ReLU activation function. Let p ∈ [1, ∞). Then NNnρ,m,n+m+1 is
dense in Lp (Rn ; Rm).
Moving on to polynomial activation functions, the smaller width of n + m + 1 also suffices for a
large class of polynomials.
Theorem 4.8. Let ρ : R → R be any polynomial for which there exists a point α ∈ R such that
ρ0(α) = 0 and ρ00(α) 6= 0. Let K ⊆ Rn be compact. Then NNnρ,m,n+m+1 is dense in C(K; Rm).
The simplest example of such a ρ is x 7→ x2. Note that in the classical arbitrary-width case it is both
necessary and sufficient that the activation function be nonpolynomial. Here, however, the same
restriction does not hold. Polynomial activation functions are a reasonable choice in this context.
The technical restrictions on the polynomial may be lifted by allowing the full n + m + 2 neurons
per hidden layer.
Theorem 4.10. Let ρ : R → R be any nonaffine polynomial. Let K ⊆ Rn be compact. Then
NNnρ,m,n+m+2 is dense in C(K; Rm).
It is clear that Theorems 4.4 and 4.10 together imply Theorem 3.2.
Finally we observe that even pathological cases not satisfying the technical condition of Theorem
3.2 may exhibit the universal approximation property.
Proposition 4.13. Let w : R → R be any bounded continuous nowhere differentiable function. Let
ρ(x) = sin(x) + w(x)e-x, which will also be nowhere differentiable. Let K ⊆ Rn be compact.
Then NNnρ,m,n+m+1 is dense in C(K; Rm).
Whilst not of direct practical application, this result exemplifies that little necessarily needs to be
assumed about an activation function to understand the corresponding class of neural networks.
Remark 3.3. Every proof in this article is constructive, and can in principle be traced so as to deter-
mine how depth changes with approximation error. We have instead chosen to focus on quantifying
the width necessary for universal approximation. In fact there are places in our arguments where we
have used a deeper network over a shallower one, when the deeper network is more easily explained.
Remark 3.4. An understanding of universal approximation in deep narrow networks is applicable
to an understanding of bottlenecks, when information must be discarded due to space constraints,
for example in autoencoders (Bengio et al., 2006). This article demonstrates that certain narrow
networks will not constitute a bottleneck; a converse example is Johnson (2019), who demonstrates
that networks of insufficient width are forced to maintain certain topological invariants.
4	Universal approximation
4.1	Preliminaries
Remark 4.1. A neuron is usually defined as an activation function composed with an affine function.
For ease, we shall extend the definition of a neuron to allow it to represent a function of the form
ψ ◦ P ◦ φ, where ψ and φ are affine functions, and P is the activation function. This does not increase
the representational power of the network, as the new affine functions may be absorbed into the
affine parts of the next layer, but it will make the neural representation of many functions easier to
present. We refer to these as enhanced neurons. It is similarly allowable to take affine combinations
of multiple enhanced neurons; we will use this fact as well.
3
Under review as a conference paper at ICLR 2020
One of the key ideas behind our constructions is that most reasonable activation functions can be
taken to approximate the identity function. Indeed, this is essentially the notion that differentiability
captures: that a function is locally affine. This makes it possible to treat neurons as ‘registers’, in
which information may be stored and preserved through the layers. This allows for preserving the
input values between layers, which is crucial to performing computations in a memory-bounded
regime. Thus our constructions have strong overtones of space-limited algorithm design in tradi-
tional computer science settings.
Lemma 4.2. Let ρ : R → R be any continuous function which is continuously differentiable at at
least one point, with nonzero derivative at that point. Let L ⊆ R be compact. Then a single enhanced
neuron with activation function ρ may uniformly approximate the identity function ι: R → R on L,
with arbitrarily small error.
Proof. By assumption, as ρ is continuously differentiable, there exists [a, b] ⊆ R with a 6= b, on
some neighbourhood of which ρ is differentiable, and α ∈ (a, b) at which ρ0 is continuous, and for
which ρ0 (α) is nonzero.
For h ∈ R \ {0}, let φh(x) = hx + α, and let
ψh(x)
x - ρ(α)
hρ0(α) .
Then
ιh = ψh ◦ ρ ◦ φh
is of the form that an enhanced neuron can represent. Then for all u ∈ [a, b], by the Mean Value
Theorem there exists ξu between u and α such that
ρ(u) = ρ(α) + (u - α)ρ0(ξu),
and hence
∣h(x) = (ψh ◦ P ◦ φh)(x)
= ψh (ρ(α) + hxρ0(ξhx+α))
=XP (ξhx+a)
P0(α)
for h sufficiently small that φh(L) ⊆ [a, b].
Now let P0 have modulus of continuity ω on [a, b]. Let ι: R → R represent the identity function.
Then for all x ∈ L,
∣∣ (χ) _ ∣(χ)∣ = |x| P (ξhx+α) - P (a)
IIh(X) - I(X)I = |x|	ρ0(α)
6 I 0xl∣ ω(hx),
IP0(α)I
and so ∣h → ι uniformly over L.	□
Notation. Throughout the rest of this paper Ih will be used to denote such an approximation to the
identity function, where Ih → I uniformly as h → 0.
An enhnaced neuron may be described as performing (for example) the computation X 7→ Ih (4X+3).
This is possible as the affine transformation X 7→ 4X + 3 and the affine transformation φh (from the
description of Ih) may be combined together into a single affine transformation.
4.2	Nonpolynomial activation functions
We consider the ‘Register Model’, which represents a simplification of a neural network.
Proposition 4.3 (Register Model). Let P : R → R be any continuous nonpolynomial function. Let
Inρ,m,n+m+1 represent the class of neural networks with n neurons in the input layer, m neurons in
the output layer, n + m + 1 neurons in each hidden layer, an arbitrary number of hidden layers, and
for which n + m of the neurons in each hidden layer have the identity activation function, and one
neuron in each hidden layer has activation function P. Let K ⊆ Rn be compact. Then Inρ,m,n+m+1
is dense in C(K; Rm).
4
Under review as a conference paper at ICLR 2020
x1	x2
Xl	X2
X1	X2
Xl	X2
Xl	X2
Xn	σ3 = σ3(X1,∙∙∙,Xn)	σι + σ2
Xn	σ2 = σ2(X1,...,Xn)	σι
Xn	I σι = σ1(X1,...,Xn)	0
Xn
Figure 1: A simple example of how to prove the Register Model. The values x1, . . . , xn are inputs
to the network, and the value PjM=1 σj is the output. Each cell represents one neuron. Each σi is of
the form ψi ◦ ρ ◦ φi , where ψi and φi are affine functions and ρ is the activation function.
See Appendix A for the proof.
A simplified depiction of the proof of the Register Model is shown in Figure 1, for the special case
of m = 1. It uses n neurons in each layer as registers to preserve the input values. A single neuron
in each layer performs a computation based off of the input values, which were preserved in the
previous layer. The remaining neuron in each layer also acts a register, gradually summing up the
results of the computation neurons. The computation neurons may be shown to exist by the classical
Universal Approximation Theorem.
The Register Model is similar to Hanin & Sellke (2017), who have a related construction specific to
the ReLU. The idea of the Register Model may also be thought of as thematically similar to residual
networks, as in Lin & Jegelka (2018): in both cases the network is almost applying the identity
transformation at each layer, with only a small amount of nonlinearity.
Theorem 4.4. Let ρ : R → R be any continuous nonpolynomial function which is continuously
differentiable at at least one point, with nonzero derivative at that point. Let K ⊆ Rn be compact.
Then NNnρ,m,n+m+1 is dense in C(K; Rm).
Proof. Let f ∈ C(K; Rm) and ε > 0. Set up a neural network as in the Register Model (Proposition
4.3), approximating f to within ε∕2. Every neuron requiring an identity activation function in the
Register Model will instead approximate the identity, in the manner of Lemma 4.2.
Uniform continuity preserves uniform convergence, compactness is preserved by continuous func-
tions, anda composition of two uniformly convergent sequences of functions with uniformly contin-
uous limits is again uniformly convergent. So as a neural network is a layer-by-layer composition of
functions then the new model can be taken within ε∕2 of the Register Model, with respect to ∣∣∙∣∣∞
in K, by taking h sufficiently small.	□
Remark 4.5. This of course implies approximation in Lp(K, Rm) for p ∈ [1, ∞). However, when
ρ is the ReLU activation function, then the next corollary shows that in fact the result may be
generalised to unbounded domains.
Corollary 4.6. Let ρ be the ReLU activation function. Let p ∈ [1, ∞). Then NNnρ,m,n+m+1 is
dense in Lp (Rn ; Rm).
See Appendix B for the proof.
Given some f ∈ Lp (Rn ; Rm), the essential idea of the proof is to choose a compact set K ⊆ Rn
on which f places most of its mass, and find a neural approximation to f on K in the manner of
Theorem 4.4. Once this is done, a cut-off function is applied outside the set, so that the network
takes the value zero in Rn \ K. The interesting bit is finding a neural representation of such cut-off
behaviour.
In particular the 'obvious' thing to do - multiply by a cut-off function - does not appear to have
a suitable neural representation, as merely approximating the multiplication operation is not neces-
sarily enough on an unbounded domain. Instead the strategy is to take a maximum and a minimum
with suitable cut-off functions.
5
Under review as a conference paper at ICLR 2020
4.3	Polynomial activation functions
For the classical Universal Approximation Theorem, it was necessary that the activation function be
nonpolynomial. However that turns out to be unnecessary here; deep narrow networks are different
to shallow wide networks, and polynomial activations functions are reasonable choices.
We begin with the simplest possible nonaffine polynomial, namely ρ(x) = x2 .
Proposition 4.7 (Square Model). Let ρ(x) = x2. Let K ⊆ Rn be compact. Then NNnρ,m,n+m+1 is
dense in C(K; Rm).
See Appendix C for the proof. As might be expected, density is established with the help of the
Stone-Weierstrass theorem, reducing the problem to the approximation of arbitrary polynomials.
We remark that it is actually straightforward to find a construction showing that NNnρ,m,n+m+2 is
dense in C(K; Rm) when ρ(x) = x2, note the increased width. This is because the square activation
function can be used to perform multiplication, via xy = ((x +y)2 - (x - y)2)/4 , and this makes it
easy to construct arbitrary polynomials. In fact this is what is done in the proof of Proposition 4.7 for
finding m - 1 of the m outputs, when there is still a ‘spare’ neuron in each layer. It is computing the
final output that actually requires the bulk of the work. The key to this argument is a width-efficient
approximation to division.
It is a consequence of Proposition 4.7 that any (polynomial) activation function which can approxi-
mate the square activation function, in a suitable manner, is also capable of universal approximation.
Theorem 4.8. Let ρ : R → R be any polynomial for which there exists a point α ∈ R such that
ρ0(α) = 0 and ρ00(α) 6= 0. Let K ⊆ Rn be compact. Then NNnρ,m,n+m+1 is dense in C(K; Rm).
Proof. Let h ∈ R \ {0}. Define ρh : R → R by
ρ(α + hx) - ρ(α)
Ph(X) =	h2ρ"(α)∕2
Then, taking a Taylor explansion around α,
ρh(x)
ρ(α) + hxρ0(α) + h2x2ρ"(α)∕2 + O(h3x3) — ρ(a)
h2ρ0 (α)∕2
x2 + O(hx3).
Let s(x) = x2. Then ρh → s uniformly over any compact set as h → 0.
Now set up a network as in the Square Model (Proposition 4.7), with every neuron using the square
activation function. Call this network N . Create a network Nh by copying N and giving every
neuron in the network the activation function ρh instead.
Uniform continuity preserves uniform convergence, compactness is preserved by continuous func-
tions, and a composition of two uniformly convergent sequences of functions with uniformly con-
tinuous limits is again uniformly convergent. So as a neural network is a layer-by-layer composition
of functions, then the difference between N and Nh with respect to ∣∣∙∣l∞ on K, may be taken
arbitrarily small by taking h arbitrarily small.
Furthermore note that ρh is just ρ pre- and post-composed with affine functions. (Note that there
is only one term in the definition of ρh (x) which depends on x.) This means that any network
which may be represented with activation function ρh may be precisely represented with activation
function ρ, by combining the affine transformations involved.	□
Remark 4.9. That ρ is polynomial is never really used in the proof of Theorem 4.8. Only a certain
amount of differentiability is required, and all such nonpolynomial functions are already covered by
Theorem 4.4, as a nonzero second derivative at α implies a nonzero first derivative somewhere close
to α. Nonetheless in principle this provides another possible construction by which certain networks
may be shown to exhibit universal approximation.
Note that the converse strategy (applying nonpolynomial techniques to the polynomial case) fails.
This is because the Register Model requires nonpolynomial activation functions due to its depen-
dence on the classical Universal Approximation Theorem.
6
Under review as a conference paper at ICLR 2020
φl (η)2 Φ2(η)2 …φn + m + l(η)2
Expand
αn+m+1,n+m+1
(αn+m+1,n+m)
α1,n+m+1 = ι(α1,n+m)
α2,n+m+1 = ι(α2,n+m)
a1,2 = ∣(α1,1)		α2,2 = (α2,1)2	
a1,1 = φι(η)	a2,1 = φ2(η)
αn+m+1,2 = ι(αn+m+1,1)
αn+m+1,1 = φn+m+1 (η)
Figure 2: A layer with square activation functions is equivalent to multiple layers with only a single
square activation function in each layer. The other neurons use the identity activation function,
denoted ι.
Theorem 4.10. Let ρ : R → R be any nonaffine polynomial. Let K ⊆ Rn be compact. Then
NNnρ,m,n+m+2 is dense in C(K; Rm).
Proof. Fix α ∈ R such that ρ00 (α) 6= 0, which exists as ρ is nonaffine. Now let h ∈ (0, ∞). Define
σh : R → R by
ρ(α + hx) - 2ρ(α) + ρ(α - hx)
h2ρ00(α)
σh(x)
Then Taylor expanding ρ(α + hx) and ρ(α - hx) around α,
ρ(α) + hxρ0(α) + h2x2ρ00(α)∕2 + O(h3x3)	2ρ(a)
σh(x =	h2ρ00(α)	h2ρ00 (α)+
ρ(α) — hxρ0(α) + h2x2ρ00(α)∕2 + O(h3x3)
h2ρ00(α)
= x2 + O(hx3).
Observe that σh needs precisely two operations of ρ on (affine transformations of) x, and so may
be computed by two enhanced neurons with activation function ρ. Thus the operation of a single
enhanced neuron with square activation function may be approximated by two enhanced neurons
with activation function ρ.
Let N be a network as in the Square Model (Proposition 4.7) with every neuron using the square
activation function. Let ` be any hidden layer of N; it contains n+m+ 1 neurons. Let η be a vector
of the values of the neurons of the previous layer. Let φi be the affine part of the ith neuron of `, so
that ` computes φ1 (η)2, . . . , φn+m+1(η)2. Then this may equivalently be calculated with n+m+ 1
layers of n + m + 1 neurons each, with n + m of the neurons in each of these new layers using the
identity function, and one neuron using the square activation function. The first of these new layers
applies the φi , and the ith layer squares the value of the ith neuron. See Figure 2.
Apply this procedure to every layer of N; call the resulting network N . It will compute exactly
the same function as N , and will have n + m + 1 times as many layers, but will use only a single
squaring operation in each layer.
Create a copy of N , call it Nh . Replace its identity activation functions with approximations in the
manner of Lemma 4.2, using activation function ρ. Replace its square activation functions (one in
each layer) by approximations in the manner described above with σh ; this requires an extra neuron
in each hidden layer, so that the network is now of width n + m + 2. Thus Nh uses the activation
function ρ throughout.
Uniform continuity preserves uniform convergence, compactness is preserved by continuous func-
tions, and a composition of two uniformly convergent sequences of functions with uniformly con-
tinuous limits is again uniformly convergent. So as a neural network is a layer-by-layer composition
7
Under review as a conference paper at ICLR 2020
of functions, then the difference between Nh and N, with respect to ∣∣∙∣∣∞ on K, may be taken
arbitrarily small by taking h arbitrarily small.	□
Remark 4.11. It is possible to construct shallower networks analogous to N. The proof of Propo-
sition 4.7 in Appendix C, uses most of the network’s neurons to approximate the identity anyway;
only a few in each layer are used to square a valued that is desired to be squared. These are the only
neurons that actually require the procedure used in Figure 2 and the proof of Theorem 4.10.
4.4 Nondifferentiable activation functions
Although not of direct practical application, results for nondifferentiable activation functions demon-
strate how certain pathological cases are still capable of being handled.
Lemma 4.12. Let w : R → R be any bounded continuous nowhere differentiable function. Let
ρ(x) = sin(x) + w(x)e-x. Let L ⊆ R be compact. Then a single enhanced neuron with activation
function ρ may uniformly approximate the identity function ι : R → R on L, with arbitrarily small
error.
Proof. For h ∈ R \ {0} and A ∈ 2πN, let φh,A (x) = hx + A, and let ψ(x) = x/h. Let
∣h,A = Ψh ◦ P ◦ Φh,A,
which is of the form that an enhanced neuron can represent. Then jointly taking h small enough and
A large enough it is clear that ∣h,A may be taken uniformly close to ∣ on L.	□
Proposition 4.13. Let w : R → R be any bounded continuous nowhere differentiable function. Let
ρ(x) = sin(x) + w(x)e-x, which will also be nowhere differentiable. Let K ⊆ Rn be compact.
Then NNnρ,m,n+m+1 is dense in C(K; Rm).
Proof. As the proof of Theorem 4.4, except substituting Lemma 4.12 for Lemma 4.2.	□
This manner of proof may be extended to other nondifferentiable activation functions as well.
5 Conclusion
There is a large literature on theoretical properties of neural networks, but much ofit deals only with
the ReLU.1 However how to select an activation function remains a poorly understood topic, and
many other options have been proposed: leaky ReLU, PReLU, RRelu, ELU, SELU and other more
exotic activation functions as well.2
Our central contribution is to provide results for universal approximation using general activation
functions (Theorems 3.2, 4.4, 4.8 and 4.10). In contrast to previous work, these results do not rely
on the nice properties of the ReLU, and in particular do not rely on its explicit description. The
techniques we use are straightforward, and robust enough to handle even the pathological case of
nondifferentiable activation functions (Proposition 4.13).
We also consider approximation in Lp norm (Remark 4.5), and generalise previous work to smaller
widths, multiple output neurons, and p > 1 in place of p = 1 (Corollary 4.6).
In contrast to much previous work, every result we show also handles the general case of multiple
output neurons.
Acknowledgements
(Redacted from anonymised submission)
1See for example Hanin & Sellke (2017); Petersen & Voigtlaender (2018); Guhring et al.; Daubechies et al.
(2019); Arora et al. (2018).
2See Maas et al. (2013); He et al. (2015); Xu et al. (2015); Clevert et al. (2016); Klambauer et al. (2017);
Molina et al. (2019); Krizhevsky (2012) respectively.
8
Under review as a conference paper at ICLR 2020
References
R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding Deep Neural Networks with Rec-
tified Linear Units. In International Conference on Learning Representations, 2018.
H.-P. Beise, S. D. Da Cruz, and U. Schroder. On decision regions of narrow deep neural networks.
CoRR, arXiv:1807.01194, 2018.
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks.
In Proceedings of the 19th International Conference on Neural Information Processing Systems,
NIPS'06,pp. 153-160, Cambridge, MA, USA, 2006. MIT Press.
D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and Accurate Deep Network Learning by
Exponential Linear Units (ELUs). In International Conference on Learning Representations,
2016.
G. Cybenko. Approximation by superpositions ofa sigmoidal function. Math. Control Signals Syst.,
2(4):303-314, 1989.
I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova. Nonlinear Approximation and
(Deep) ReLU Networks. arXiv:1905.02199, 2019.
I. Guhring, G. KUtyniok, and P. Petersen. Error bounds for approximations with deep ReLU neural
networks in Ws,p norms. Proceedings of the AMS. In press.
B. Hanin and M. Sellke. Approximating Continuous Functions by ReLU Nets of Minimal Width.
arXiv:1710.11278, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level
Performance on ImageNet Classification. In Proceedings of the 2015 IEEE International Confer-
ence on Computer Vision (ICCV), ICCV’15, pp. 1026-1034, Washington, DC, USA, 2015. IEEE
Computer Society.
K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Netw., 4(2):
251-257, 1991.
J. Johnson. Deep, Skinny Neural Networks are not Universal Approximators. In International
Conference on Learning Representations, 2019.
G.	Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter. Self-normalizing neural networks. In
Advances in Neural Information Processing Systems 30, pp. 971-980. Curran Associates, Inc.,
2017.
A. Krizhevsky. Convolutional Deep Belief Networks on CIFAR-10. 2012.
N. Le Roux and Y. Bengio. Deep Belief Networks are Compact Universal Approximators. Neural
Comput., 22(8):2192-2207, 2010.
H.	Lin and S. Jegelka. ResNet with one-neuron hidden layers is a Universal Approximator. In
Advances in Neural Information Processing Systems 31, pp. 6169-6178. Curran Associates, Inc.,
2018.
Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The Expressive Power of Neural Networks: A View
from the Width. In Advances in Neural Information Processing Systems 30, pp. 6231-6239.
Curran Associates, Inc., 2017.
A. Maas, A. Hannun, and A. Ng. Rectifier Nonlinearities Improve Neural Network Acoustic Models.
In International Conference on Learning Representations, 2013.
A. Molina, P. Schramowski, and K. Kersting. Pade Activation Units: End-to-end Learning ofFlexi-
ble Activation Functions in Deep Networks. arXiv:1907.06732, 2019.
G. F. Montufar. Universal Approximation Depth and Errors of Narrow Belief Networks with Dis-
crete Units. Neural Comput., 26(7):1386-1407, 2014. ISSN 0899-7667.
9
Under review as a conference paper at ICLR 2020
Q.	Nguyen, M. C. Mukkamala, and M. Hein. Neural Networks Should Be Wide Enough to Learn
Disconnected Decision Regions. In Proceedings of the 35th International Conference on Machine
Learning. PMLR 80, Stockholm, Sweden, 2018.
P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep
ReLU neural networks. Neural Netw.,108:296-330, 2018.
A. Pinkus. Approximation theory of the MLP model in neural networks. Acta Numer, 8:143-195,
1999.
R.	Rojas. Networks of width one are universal classifiers. In Proceedings of the International Joint
Conference on Neural Networks, volume 4, pp. 3124-3127, 2003.
I. Sutskever and G. E. Hinton. Deep, Narrow Sigmoid Belief Networks Are Universal Approxi-
mators. Neural Comput., 20(11):2629-2636, 2008. ISSN 0899-7667. doi: 10.1162/neco.2008.
12-07-661.
L. Szymanski and B. McCane. Deep, super-narrow neural network is a universal classifier. In The
2012 International Joint Conference on Neural Networks (IJCNN), pp. 1-8, 2012.
B. Xu, N. Wang, T. Chen, and M. Li. Empirical Evaluation of Rectified Activations in Convolutional
Network. arXiv:1505.00853, 2015.
10
Under review as a conference paper at ICLR 2020
A	Proof of the Register Model (Proposition 4.3)
First, we recall the classical Universal Approximation Theorem (Pinkus, 1999):
Theorem 1.1. Let ρ : R → R be any continuous function. Let Nnρ represent the class of neural
networks with activation function ρ, with n neurons in the input layer, one neuron in the output
layer, and one hidden layer with an arbitrary number of neurons. Let K ⊆ Rn be compact. Then
Nnρ is dense in C(K) if and only if ρ is nonpolynomial.
The Register Model is created by suitably reorganising the neurons from a collection of such shallow
networks.
Proposition 4.3 (Register Model). Let ρ : R → R be any continuous nonpolynomial function. Let
Inρ,m,n+m+1 represent the class of neural networks with n neurons in the input layer, m neurons in
the output layer, n + m + 1 neurons in each hidden layer, an arbitrary number of hidden layers, and
for which n + m of the neurons in each hidden layer have the identity activation function, and one
neuron in each hidden layer has activation function ρ. Let K ⊆ Rn be compact. Then Inρ,m,n+m+1
is dense in C(K; Rm).
Proof. Fix f ∈ C(K; Rm). Let f = (f1, . . . , fm). Fix ε > 0. By Theorem 1.1, there exist single-
hidden-layer neural networks g1, . . . , gm ∈ Nnρ with activation function ρ approximating f1, . . . , fm
respectively. Each approximation is to within error ε with respect to ∣∣∙∣∣∞ on K. Let each gi have
βi hidden neurons. Let σi,j represent the operation of its jth hidden neuron, for j ∈ {1, . . . , βi}. In
keeping with the idea of enhanced neurons, let each σi,j include the affine function that comes after
it in the output layer of gi , so that gi = Pjβ=i 1 σi,j. Let M = Pim=1 βi.
We seek to construct a neural network N ∈ Inρ,m,n+m+1. Given input (x1, . . . , xn) ∈ Rn, it will
output (G1, . . . , Gm) ∈ Rm, such that Gi = gi(x1, . . . , xn) for each i. That is, it will compute all
of the shallow networks g1, . . . , gm. Thus it will approximate f to within error ε with respect to
k∙k∞ on K.
The construction of N is mostly easily expressed pictorially; see Figure 3. In each cell, representing
a neuron, we define its value as a function of the values of the neurons in the previous layer. In every
layer, all but one of the neurons uses the identity activation function ι: R → R, whilst one neuron
in each layer performs a computation of the form σi,j .
The construction can be summed up as follows.
Each layer has n + m + 1 neurons, arranged into a group of n neurons, a group of a single neuron,
and a group of m neurons.
The first n neurons in each layer simply record the input (x1, . . . , xn), by applying an identity
activation function. We refer to these as the ‘in-register neurons’.
Next we consider g1 , . . . , gm, which are all shallow networks. The neurons in the hidden layers of
g1 , . . . , gm are arranged ‘vertically’ in our deep network, one in each layer. This is the neuron in
each layer that uses the activation function ρ. We refer to these as the ‘computation neurons’. Each
computation neuron performs its computation based off of the inputs preserved in the in-register
neurons.
The final group of m neurons also use the identity activation function; their affine parts gradually
sum up the results of the computation neurons. We refer to these as the ‘out-register neurons’. The
ith out-register neuron in each layer will sum up the results of the computation neurons computing
σi,j for all j ∈ {1, . . . ,βi}.
Finally, the neurons in the output layer of the network are connected to the out-register neurons of
the final hidden layer. As each of the neurons in the output layer has, as usual, the identity activation
function, they will now have computed the desired results.	□
11
UnderreVieW as a ConferenCe PaPersICLR 2020
GI = Cl,Λf Il	G2 = ¢2,Λf	I ... I Gm, = ,,ΛI + Tm,M
71,M = 0	72,M = 0		7ττ,M = 0	TM = σm,βrn(7	1,M-1, ■	■ ,7n,M-l')	Cl,M = 	"Q,⅛Γ-1)		<2,M = 	1)			ζm,M = M-I	+ ZM^-1)
71,M-I = t(71,M-2)	72,M-I = ^(72,M-2)		7n,M-l =	TM-I =	71,M-2,	■ ■ ■ ,7n,M-2')	C1,M-1 = 	—2)		<2,M-I = 	—2)			ζm,M-l = M — 2	+「Af_2)
71,/ðɪ+/S2 = √7i,^i+^2-i)	72,^i+^2 = 1-(72,β1+β2-1)		7n,β1 +β2 = 十户2 — 1 )	τβ1 +β2 = σ2,^2 (TI，B1 十户2-1，…十户2-1)	CLgl十32 = “《1,61+62 — 1)	0,31十放= "Q,6i+62T +*ι+62T)		ζm,β1+β1 = °
71,^ι+3 = √71,∕Si+2)	■72,31 十 3 = √ 72,+2)		^y,n,β1+3 = “丁九小十2)	τβ1+3 = b2,3()l,6i+2, •••，)九,61+2)	Cl,31十3 = “Q,由十2)	¢2,61 十 3 = α(C2,3ι 十 2 + TIgl 十 2)		ζτn,β1+3 = 0
71,^ι+2 = √71,∕S1 + 1)	72,^ι+2 = √72,∕3i+1)		7n,β1+2 = b{^in,βγ-∖-l )	丁Sl 十2 一 σ^2,2 (71 ,户1 十1 , ∙ ∙ ∙ ,Tlx,% 十1 )	Cl,∕Sι+2 = “0,%十1)	十2 = "*1十1)		Crn #1+2 = 0
Tl,61 + 1 = √7ι,∕Sι)	72,61 + 1 = √72,∕3i)		1n,β1+l = √7ττ,∕Sι)	τβl + l = σ2,l(Tl,^l) ■ ■ ■ ι^in,βγ')	Cl,∕Sι+l = L(CLgI +*1)	<2,∕3ι + l = 0		ζm,,^1 + l = °

7ιτ,∕S1 =	*1	=σi,β1 (TlrL1，,	■ ι^in,βγ —1 )	¢1,/31 =	02乖1	=0
√7ττ,∕Sι-l)				+ *]_1)		
71,/Si =	72,^1 =
√7ι,∕Sι-ι)	√72,∕3i-1)
71,4 = √71,3)	72,4 = ^72,3)		7ττ,4 = “7九,3)	「4 =	^1,4(71,3, ■	■ ■ J^,∕n,3 )	¢1,4 = “0,3 十 丁3)	<2,4 =	=0		ζm,4 =	=0
71,3 = √71,2)	72,3 = “72,2)		In.,3 = “"‰,2)	丁3 =	σl,3(71,2; ■	• ∙ ,^‰,2 )	<1,3 = “0,2 十 丁2)	<2,3 =	=0		tCm,, 3 =	=0
71,2 = "九1)	72,2 = “72,1)		7ττ,2 =	「2 =		■ ■ ；Tn., 1 )	<1,2 = “丁1)	<2,2 =	=0		ζm,2 =	=0
71,1 = “力 1)		72,1 = “方 2)			‰,1 = “方n )	ʃl	=σl, 1 (æl ； -	■ ■ ； ^ n.)	<1,1 = 0	<2,1 =	=0		ζm, 1	=0
方1	-∣∣ 方2 I … I 方τΓ
Figure 3: The thick lines delimit groups of layers; the zth group computes ..., σiβi. The inputs to the network are ʃi,..., χn, depicted at the bottom. The
outputs from the network are G1,..., Gm, depicted at the top. The identity activation function R → R is denoted l.
Under review as a conference paper at ICLR 2020
B Proof of Corollary 4.6
Lemma B.1. Let a, b, c, d ∈ R be such that a < b < c < d. Let Ua,b,c,d : R → R be the unique
continuous piecewise affine function which is one on [b, c] and zero on (-∞, a] ∪ [d, ∞). Then
two layers of two enhanced neurons each, with ReLU activation function, may exactly represent the
function Ua,b,c,d.
Proof. Let x ∈ R be the input. Let m1 = 1/(b - a). Let m2 = 1/(d - c). Let η1, η2 represent
the first neuron in each layer, and ζ1, ζ2 represent the second neuron in each layer. We assign them
values as follows.
η1 = max{0, m1 (x - a)},	ζ1 = max{0, m2 (x - c)},
η2 = max{0, 1 - η1},	ζ2 = max{0, 1 - ζ1}.
Then Ua,b,c,d(x) = ζ2 - η2. (This final affine transformation is allowed, in keeping with the notion
of enhanced neurons.)	□
Lemma B.2. One layer of two enhanced neurons, with ReLU activation function, may exactly rep-
resent the function (x, y) 7→ min{x, y} on [0, ∞)2.
Proof. Let the first neuron compute η = max{0, x - y}. Let the second neuron compute ζ =
max{0, x}. Then min{x, y} = Z — η.	□
Corollary 4.6. Let ρ be the ReLU activation function. Let p ∈ [1, ∞). Then NNnρ,m,n+m+1 is
dense in Lp (Rn ; Rm).
Proof. Let f ∈ Lp(Rn; Rm) and ε > 0. For simplicity assume that Rm is endowed with the ∣∣∙∣∣∞
norm; other norms are of course equivalent. Let f = (f1, . . . , fm) ∈ Cc(Rn; Rm) be such that
IIf -儿 <f/3.	(B.I)
Let
C	G/ 、 ， r	/C c、
C = sup max fi (x) + 1	(B.2)
x∈Rn i
and
c = inf min fbi (x) — 1	(B.3)
Pick a1 , b1 , . . . , an , bn ∈ R such that J defined by
J = [a1,b1] × ∙∙∙ × [an, bn]
is such that supp f ⊆ J. Furthermore, for δ > 0 that we shall fix in a moment, let
and let K be defined by
Fix δ small enough that
Ai = ai — δ,
Bi = bi + δ,
K = [A1,B1 ] ×...× [An, Bn].
∣K \ J|1/p ∙ max {∣C∣,∣c∣} < ɪ.
6
(B.4)
Let g = (g1 , . . . , gm) ∈ NNn,m,n+m+1 be such that
sup fb(x) — g(x)
x∈K
< mini —
3|J|1/p
1,
(B.5)
13
Under review as a conference paper at ICLR 2020
which exists by Theorem 4.4. Note that g is defined on all of Rn ; it simply happens to be close to
f on K. In particular it will takes values close to zero on K \ J, and may take arbitrary values in
Rn \ K. By equations (B.2), (B.3), (B.5), it is the case that
C > sup max gi (x),
x∈K i
c 6 inf min gi (x).	(B.6)
x∈K i
Now consider the network describing g; it will be modified slightly. The goal is to create a network
which takes value g on J, zero in Rn \ K, and moves between these values in the interface region
K\J. Such a network will provide a suitable approximation to f. This is done by first constructing a
function which is approximately the indicator function for J, with support in K; call such a function
U . The idea then is to construct a neural representation of Gi defined by
Gi = min{max{gi, cU}, CU}.
Provided |K \ J| is small enough then G = (G1, . . . , Gm) will be the desired approximation; this
is proved this below.
We move on to presenting the neural representation of this construction.
First we observe that because the activation function is the ReLU, then the identity approximations
used in the proof of Theorem 4.4 may in fact exactly represent the identity function on some compact
set: x 7→ max{0, x + N } - N is exactly the identity function, for suitably large N, and is of the
form that an enhanced neuron may represent. This observation isn’t strictly necessary for the proof,
but it does simplify the presentation somewhat, as the values preserved in the in-register neurons of
g are now exactly the inputs x = (x1, . . . , xn) for x ∈ K. For sufficiently negative xi, outside of
K, they will take the value -N instead, but by insisting that is N sufficiently large that
-N < Ai	(B.7)
for all i, then this will not be an issue for the proof.
So take the network representing g, and remove the output layer. (If the output layer is performing
any affine transformations then treat them as being part of the final hidden layer, in the manner of
enhanced neurons. Thus the output layer that is being removed is just applying the identity function
to the out-register neurons.) Some more hidden layers will be placed on top, and then a new output
layer will be placed on top. In the following description, all neurons not otherwise specified will be
performing the identity function, so as to preserve the values of the corresponding neurons in the
preceding layer. As all functions involved are continuous and K is compact, and compactness is
preserved by continuous functions, and continuous functions are bounded on compact sets, then this
is possible for all x ∈ K by taking N large enough.
The first task is to modify the value stored in the in-register neurons corresponding to x1. At present
it stores the value x1; by using this in-register neuron and the computation neuron in two extra layers,
its value may be replaced with UA1,a1,b1,B1 (x1), via Lemma B.1. Place another two layers on top,
and use them to replace the value of x2 in the second in-register neuron with UA2,a2,b2,B2 (x2), and
so on. The in-register neurons now store the values (UA1,a1,b1,B1(x1), . . . , UAn,an,bn,Bn(xn)).
Once this is complete, place another layer on top and use the first two in-register neurons
to compute the minimum of their values, in the manner of Lemma B.2, thus computing
min{UA1,a1,b1,B1 (x1), UA2,a2,b2,B2 (x2)}. Place another layer on top and use another two in-
register neurons to compute the minimum of this value and the value presently stored in the third
in-register neuron, that is UA3,a3,b3,B3 (x3), so that
min{UA1 ,a1 ,b1 ,B1 (x1), UA2,a2,b2,B2 (x2), UA3,a3,b3,B3 (x3)}
has now been computed. Continue to repeat this process until the in-register neurons have com-
puted.3
U = min UAi,ai,bi,Bi (xi).
i∈{1,...,n}
3It doesn’t matter which of the in-register neurons records the value of U.
14
Under review as a conference paper at ICLR 2020
Observe how U represents an approximation to the indicator function for J, with support in K,
evaluated at (x1 , . . . , xn).
This is a highly destructive set of operations: the network no longer remembers the values of its
inputs. Thankfully, it no longer needs them. Note how the small foible regarding how an in-register
neuron would only record -N instead of xi, for xi < -N, is not an issue. This is because of
equation (B.7), which implies that UAi,ai,bi,Bi(xi) = 0 = UAi,ai,bi,Bi(-N), thus leaving the value
of U unaffected.
The out-register neurons presently store the values g1, . . . , gm, where gi = gi(x1 , . . . , xn). Now
add another layer. Let the value of its out-register neurons be θ1 , . . . , θm, where
θi = max{0, gi - cU}.
Add one more hidden layer. Let the value of its out-register neurons be λ1 , . . . , λm, where
λi = max{0, -θi + (C - c)U}.
Finally place the output layer on top. Let the value of its neurons be G1 , . . . , Gm, where
Gi = -λi+CU.
Then in fact
as desired.
Gi = min{max{gi, cU}, CU}
(B.8)
All that remains to show is that G = (G1, . . . , Gm) of this form is indeed a suitable approximation.
First, as G and g coincide in J, and by equation (B.5),
f(x) - G(x)
1/p
6 |J |1/p sup fb(x) - G(x)
x∈J
= |J |1/p sup fb(x) - g(x)
x∈J
ε
< 3.
(B.9)
Secondly, by equations (B.2), (B.3), (B.6), (B.8) and then equation (B.4),
fb(x) - G(x) dx!	6 |K \ J |1/p sup fb(x) - G(x)
I X	x∈K∖J '
6 |K \ J|1/p ∙ 2max{|C|, |c|}
ε
< 3.
Thirdly,
(B.10)
Z	fb(x) - G(x)p dx!	= 0,
Rn\K
as both f and G have support in K.
So by equations (B.1), (B.9), (B.10) and (B.11),
(B.11)
ε.
fb(x) - G(x)
1/p
15
Under review as a conference paper at ICLR 2020
C Proof of the S quare Model (Proposition 4.7)
Lemma C.1. One layer of two enhanced neurons, with square activation function, may exactly
represent the multiplication function (x, y) 7→ xy on R2.
Proof. Let the first neuron compute η = (x+y)2/4. Let the second neuron compute ζ = (x-y)2/4.
Then Xy = η — Z.	□
Lemma C.2. Fix L ⊆ R2 compact. Three layers of two enhanced neurons each, with square
activation function, may uniformly approximate (x, y) 7→ (x2, y(x + 1)) arbitrarily well on L.
Proof. Let h, s ∈ R \ {0}. Let η1 , η2, η3 represent the first neuron in each layer; let ζ1, ζ2, ζ3
represent the second neuron in each layer. Let ιh represent an approximation to the identity in the
manner of Lemma 4.2. Using '≈' as an informal notation to represent 'equal to UP to the use of
∣h in place i'，just to help keep track of Why We are performing these operations, assign values to
η1,η2,η3 andζ1,ζ2,ζ3 as follows:
η1 = ιh(x)	ζ1 = (x + sy + 1)2
≈ x,	= x2 + 2sxy + s2y2 + 2x + 2sy + 1,
η2 = (η1)2	ζ2 = ιh (ζ1 — 2η1 — 1)
≈ x2,	≈ x2 + 2sxy + s2y2 + 2sy,
η3 = ∣h(η2)	Z3 = ∣h((Z2 — η2)∕2s)
≈ x2,	≈ xy + y + sy2/2.
And so η3 may be taken arbitrarily close to x2 and ζ3 may be taken arbitrarily close to y(x+ 1), With
respect to ∣∣ ∙ k∞ on L, by first taking S arbitrarily small, and then taking h arbitrarily small. □
Proposition C.3. Fix L ⊆ (0, 2) compact. Then multiple layers of two enhanced neurons each, with
square activation function, may uniformly approximate x 7→ 1∕x arbitrarily well on L.
(Unlike Lemma C.2, the number of layers necessary Will depend on the quality of approximation.)
Proof. First note that
n
Y(1+x2i)→
i=0
1
1—x
as n → ∞, uniformly over compact subsets of (—1, 1). Thus,
(2 - x) YY(1 + (1 - X)2i) = YY(1 + (1 - x)2i) → 1
i=1	i=0
uniformly over L.
This has the folloWing neural approximation: let η1 = (1 -x)2 and ζ1 = ιh(2 -x) be the neurons in
the first layer, Where ιh is some approximation of the identity as in Lemma 4.2. Let κh represent an
approximation to (x, y) 7→ (x2, y(x + 1)) in the manner of Lemma C.2, With error made arbitrarily
small as h → 0. NoW for i ∈ {1, 4, 7, 10, . . . , 3n - 2}, recursively define (ηi+3, ζi+3) = κh(ηi, ζi),
Where We increase the index by three to represent the fact that three layers are used to perform this
operation. So up to approximation, ηi+3 ≈ (ηi)2, and ζi+3 ≈ ζi(ηi + 1).
So ζ3n+1 → (2 - x) Qin=1(1 + (1 - x)2i) uniformly over L as h → 0. Thus the result is obtained
by taking first n large enough and then h small enough.
Remark C.4. Our approach to Proposition C.3 is to find a suitable polynomial approximation of
the reciprocal function, and then represent that With a netWork of multiple layers of tWo neurons
each. It is fortunate, then, that this polynomial happens to be of a form that may be represented by
such a netWork, as it is not clear that this should necessarily be the case for all polynomials. Even
16
Under review as a conference paper at ICLR 2020
if Proposition 4.7 were already known, it requires a network of width three to represent arbitrary
polynomials R → R, whereas Proposition C.3 uses a network of only width two. It remains unclear
whether an arbitrary-depth network of width two, with square activation function, is capable of
universal approximation in C(K).
Proposition 4.7 (Square Model). Let ρ(x) = x2. Let K ⊆ Rn be compact. Then NNnρ,m,n+m+1 is
dense in C(K; Rm).
Proof. Fix f ∈ C(K; Rm). Let f = (f1, . . . , fm). Fix ε > 0. By precomposing with an affine
function - which may be absorbed into the first layer of the network - assume without loss of
generality that
K⊆(1,2)n.	(C.1)
By the Stone-Weierstrass theorem there exist polynomials g1 , . . . , gm in x1 , . . . xn approximating
fι,...,fm to within ε∕3 with respect to ∣∣ ∙ ∣∣∞.
We will construct a network that evaluates arbitrarily good approximations to g1 , . . . , gn . There
are a total of n + m + 1 neurons in each layer; group them as in the proof of the Register Model
(Proposition 4.3), see Appendix A, so that in each layer there is a group of n neurons that we refer
to as ‘in-register neurons’, a single neuron that we refer to as the ‘computation neuron’, and a group
of m neurons that we refer to as the ‘out-register neurons’.
As before, every in-register neuron will simply apply an approximate identity function to the corre-
sponding in-register neuron in the previous layer, so that they preserve the inputs to the network, up
to an arbitrarily good approximation of the identity. (For now, at least - later, when constructing the
approximation to g1 , which will be the final approximation that is handled, then these neurons will
be repurposed to perform that computation.) For the sake of sanity of notation, we shall suppress
this detail in our notation, and refer to our neurons in later layers as having e.g. ‘x1’ as an input to
them to them; in practice this means some arbitrarily good approximation to x1 . The out-register
neurons will eventually store the desired outputs of the network; thus there is an out-register neuron
in each layer ‘corresponding’ to each of the gi .
Now suppose m > 1; if m = 1 then this paragraph and the next three paragraphs may simply be
skipped. It is easy to build a network approximating g2, . . . gm, as there is at least one ‘extra’ neuron
per layer that is available to use: the out-register neuron corresponding to g1 . The strategy is as
follows. Let g2 = PjN=1 δj , where each δj is a monomial. Using the computation neuron and the
‘extra’ out-register neuron in each layer, perform successive multiplications in the manner of Lemma
C.1 to compute the value of δ1. For example, if δ1 = x21x2x3, then this chain of multiplications is
x1(x1(x2x3)). The computation neuron and the ‘extra’ out-register neuron in the first layer compute
the multiplication α = x2x3, these neurons in the second layer compute the multiplication β = x1α,
and these neurons in the third layer compute x1 β. This value is then stored in the out-register neuron
corresponding to g2 - by using the affine part of the operation of this neuron - and kept through the
layers via approximate identity functions, as per Lemma 4.2.
This process is then repeated for δ2 . The result is then added on - via the affine part of the opera-
tion of the out-register neuron corresponding to g2 - to the value stored in the out-register neuron
corresponding to g2 . Repeat for all j until all of the δj have been computed and the out-register
neuron stores an approximation to g2 . This is only an approximation in that it requires the use
of approximate identity functions; other than that it is exact. As such, by taking sufficiently good
approximations of the identity function, this will be a uniform approximation to g2 over K .
Now repeat this whole process for g3 , . . . gm.
For the rest of the layers of the network, the out-register neurons corresponding to g2 , . . . , gm will
now simply apply approximate identity functions to maintain their values: these will eventually
form the outputs of the network. Let these computed values be denoted gb2, . . . , gbm. (With the ‘hat’
notation becuse of the fact that these are not the values g2 , . . . , gm , due to the approximate identity
functions in between.)
The difficult bit is computing an approximation to g1, as it must be done without the ‘extra’ neuron
in each layer. In total, then, in each layer, there are n + 2 neurons available: the n in-register neurons
(which have so far been storing the inputs x1 , . . . xn), the computation neuron, and the out-register
neuron corresponding to g1 .
17
Under review as a conference paper at ICLR 2020
Written in terms of monomials, let g1 = PjM=1 γj . Then g1 may be written as
gι = Yi(1 + γ2 fl + γ3 (…fl + γM-1 fl +	))…))).
γ1	γ2	γM-2	γM-1
Note that this description is defined over K, as K is bounded away from the origin.
In particular, let γj = Qkn=1 xθkj,k, for θj,k ∈ N0. Substituting this in,
g1
n
Y xθk1,k
k=1
1+Qn=I 靖k(1+Qn=I 靖k (…
Qn=i 罐kl	Qn=i 靖kI
(1+Qn=iXkMTk(1+ Qn=ixM !!...!!!
I	Qn=i，"八	Qn=iXkMik))	)))'
Now let sup K be defined by
supK = sup{Xi | (X1, . . . , Xn) ∈ K},
so that 1 < sup K < 2. Let r be an approximation to X 7→ 1/X in the manner of Proposition C.3,
with the L of that proposition given by
L = [(sup K)-1 - α, sup K + α] ⊆ (0, 2),	(C.2)
where α > 0 is taken small enough that the inclusion holds.
Let ra denote r composed a times. By taking r to be a suitably good approximation, we may ensure
that ge1 defined by
n
ge1 = Y r2M-2(Xk)θ1,k
k=1
1+
n
Y r2M-3(Xk)θ1,k
k=1
n
Y r2M-4(Xk)θ2,k
k=1
n
Y r2M-5(Xk)θ2,k
k=1
n
Y r2M-6(Xk)θ3,k
k=1
1+
n
Y r3(Xk)θM-2,k
k=1
n
1 + Y r(Xk )θM -1,k
k=1
n
Y r2(Xk)θM-1,k
k=1
/"#!!
)))
is an approximation to gi in K, to within ε∕3, with respect to ∣∣ ∙ ∣∣∞. This is possible by equations
(C.1) and (C.2); in particular the approximation should be sufficiently precise that
r2M-2([(sup K)-1, sup K]) ⊆ L,
which is why α > 0 is needed: note how r2, and thus r4, r6, . . . , r2M-2, are approximately the
identity function on L.
This description of ge1 is now amenable to representation with a neural network. The key fact about
this description of ge1 is that, working from the most nested set of brackets outwards, the value of ge1
may be computed by performing a single chain of multiplications and additions, and occasionally
taking the reciprocal of all of the input values.
So let the computation neuron and the out-register neuron corresponding to g1 perform the multipli-
cations, layer-by-layer, to compute Qkn=1 XθkM,k, in the manner of Lemma C.1. Store this value in
the out-register neuron.
18
Under review as a conference paper at ICLR 2020
Now use the computation neurons and the in-register neurons corresponding to x1 (across multiple
layers) to compute r(x1), in the manner of Proposition C.3: eventually the in-register neuron is now
storing r(x1) ≈ 1/x1. Repeat for the other in-register neurons, so that they are collectively storing
r(x1),..., r(xn).
Now the computation neuron and the out-register neuron may start multipying r(x1), . . . , r(xn)
onto Qkn=1 xθkM,k (which is the value presently stored in the out-register neuron) the appropriate
number to times to compute Qkn=1 r(xk)θM -1,k hQkn=1 xθkM,k i, by Lemma C.1. Store this value
in the out-register neuron. Then add one (using the affine part of a layer). The out-register neuron
has now computed the expression in the innermost bracket in the description of ge1.
The general pattern is now clear: apply r to all of the in-register neurons again to compute r2(xi),
multiply them on to the value in the out-register neuron, and so on. Eventually the out-register
neuron corresponding to g1 will have computed the value ge1 . Actually, it will have computed an
approximation gb1 to this value, because of the identity approximations involved.
By taking all of the (many) identity approximations throughout the network to be suitably precise,
the values of eι and bi may be taken within ε∕3 of each other, and the values of b2,...,bm and
g2,...,gm may be taken within 2ε∕3 of each other, in each case with respect to ∣∣ ∙ k∞ on K.
Thus (bι,...,bm) approximates f with total error no more than ε, and the proof is complete. □
19