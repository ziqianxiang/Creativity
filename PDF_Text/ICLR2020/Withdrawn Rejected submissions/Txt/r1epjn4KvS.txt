CoRelatE: Modeling the Correlation in Multi-fold
Relations for Knowledge Graph Embedding
Yan Huanga, Ke Xua, XinyU Wangb, Haili Suna, Songfeng Lua,c,*, Tongyang
Wanga , Xinfang Zhanga
aSchool of Computer Science and Technology, Huazhong University of Science and
Technology, Wuhan 430074, China.
b 709th Research Institute, China Shipbuilding Industry Corporation, Wuhan 430074, China.
cShenzhen Huazhong University of Science and Technology Research Institute, Shenzhen
518063, China.
Abstract
Representation learning of knowledge bases aims to embed both entities and
relations into a continuous vector space. Most existing models such as TransE,
TransH and TransR consider only binary relations involved in knowledge bases,
while multi-fold relations are converted to triplets and treated as instances of
binary relations, resulting in a loss of structural information. M-TransH is a
recently proposed direct modeling framework for multi-fold relations but ignores
the relation-level information that certain facts belong to the same relation.
This paper proposes a Group-constrained Embedding method which embeds
entity nodes and fact nodes from entity space into relation space, restricting the
embedded fact nodes related to the same relation to groups with Zero Constraint,
Radius Constraint or Cosine Constraint. Using this method, a new model
is provided, i.e. Gm-TransH. We evaluate our model on link prediction and
instance classification tasks, experimental results demonstrate that our approach
outperforms related methods by a significant margin.
Keywords: knowledge base, representation learning, multi-fold relation
* Corresponding author
Email addresses: platanus@hust.edu.cn (Yan Huang), lusongfeng@hust.edu.cn
(Songfeng Lu)
Preprint submitted to Journal of LATEX Templates
September 19, 2019
1. Introduction
5
10
15
20
25
30
Knowledge bases are directed graphs with nodes representing entities and
edges representing relations between entities. Great achievements have been
made in building large scale knowledge bases, such as Freebase [1], WordNet
[2], YAGO [3] and DBpedia [4]. These knowledge bases can be used in many
areas like semantic search, question answering, drug discovery and disease
diagnosis. Although the current knowledge bases contain large amounts of
entities and relations, they are far from completeness. This calls for knowledge
base completion techniques to inference or predict missing entities and unknown
links between entities based on existing ones. Furthermore, the entities and
relations in the knowledge bases are symbolic and inadequate for inference or
calculation.
To this end, representation learning [5] has been proposed as a kind of
promising approach for knowledge base completion. It embeds entities and
relations of a knowledge base into continuous vector space and preserves the
structural information of original relational data. The representation of entities
and relations are obtained by minimizing a global loss function involving entire
entities and relations. Compared with the traditional logic-based inference
approaches, representation learning shows strong feasibility and robustness in
the applications.
Despite the promising achievements, most existing techniques for knowledge
base representation learning (such as TransE [6], TransH [7], TransR [8] and
ProjE [9], ComplEx [10] etc.) consider only binary relations therein, namely
RDF(Resource Description Framework) data with triples each involving two
entities and a binary relation between them. For example, “Donald J. Trump is
the president of America” consists of two entities “Donald J. Trump”, “America”
and a binary relation “president_of_country”. However, a large amount of the
knowledge in our real life are instances with multi-fold (N-ary, N ≥ 2) relations,
involving three or even more entities in one instance (such as “Harry Potter is
a British-American film series based on the Harry Potter novels by author J.
2
35
40
45
50
55
K. Rowling”). As reported[11, 12], more than 1/3rd of the entities in Freebase
participate in non-binary relations and 61% of the relations are non-binary.
A general approach for this problem is to convert each multi-fold relation
into multiple triples with binary relations and learn the embedding of each
triple using the existing translating embedding methods [5, 6, 7, 8]. Thus, an
instance with an N-ary relation is converted to N2 triples according to the
S2C 1 conversion[11] or to N triples via RDF reification[12]. Although S2C
conversion is capable of capturing part of the structures of multi-fold relations
[13], it’s irreversible and leads to a heterogeneity of the predicates, unfavorable
for embedding. As to RDF reification method, it’s difficult to reify the test
samples and define a way to embed the newly created entities when we have little
information about them. Wen et al.[11] advocates an instance representation
of multi-fold relations and proposes a direct modeling framework “m-TransH”
for knowledge base embedding. However, we show that m-TransH has several
shortcomings:
(1)	Treats fact nodes the same as general entity nodes, this can be seen from
the loss function of m-TransH:ID1 2 in which embeddings of entities and facts of
the same N-ary relations are calculated via a linear combination operator. Thus
do harm to the learning of semantic information and to distinguishing of these
two types of nodes.
(2)	For binary relations, the relation between two entities can be easily
estimated via cost function of most existing knowledge base embedding models
(e.g. kh + r - tk22 from TransE[6] model). While for multi-fold relations, the
m-TransH:ID model, we can hardly infer or classify the relation type of a set of
entities as an instance without the fact node information.
(3)	The correlation of facts and their linked entities can not be explicitly
1Star-to-Clique Conversion: For each two entities (with relation role r1 and r2) of an N-ary
relation instance, form a labeled edge r1.r2 between them, then delete the instance node and
all edges (i.e. relations) connecting to it.
2 m-TransH:ID is a variation of m-TransH for multi-fold relational instances with fact nodes
in FACT-ID role.
3
60
65
70
75
80
85
learned through linear combination of their embeddings, which we argue to be a
key factor for multi-fold relation embedding.
(4)	Ignores the associated information of facts and relations that certain facts
belong to the same relation. It’s impossible to infer the relation type of a fact
through the fact embedding only and to discover the facts of the same relation
type.
In this paper, we first discuss the problems of existing models for multi-fold
relation embedding, why they are important, our motivation, necessity and
significance.
Then we extend the m-TransH model and present a Group-constrained
Embedding method which embeds fact nodes as well as entity nodes and their
relations into three different vector space (named as entity embedding space, fact
embedding space and relation embedding space). We model three correlations,
namely, correlation between entities and their relations, correlation between
entities and facts, correlation between facts and their relation types. For the first
correlation, we extend the m-TransH model by replacing the coefficient function
ar (ρ) of entity embeddings Pnr (t(ρ)) to a diagonal matrix Mr (ρ) to improve
it’s expressive ability. To model the correlation between entities and facts, we
utilize a graph convolutional network to learn the embedding of facts through
the connected entities. Then we restrict the embedded fact nodes related to
the same relation to groups with three different constraint strategies, i.e. zero
constraint (by making the embedded fact vector to be close to its corresponding
relation vector indefinitely), radius constraint (by forcing the Euclidean distance
between the embedded fact vector and its corresponding relation vector to be
smaller than a radius ) or cosine constraint (by rendering the cosine distance
between the embedded fact vector and its corresponding relation vector to be
zero).
The Group-constrained Embedding method for knowledge base with multi-
fold relations is named as “Gm-TransH” . In terms of the three different constraint
strategies, we advocate three variation of Gm-TransH, i.e. Gm-TransH:zero,
Gm-TransH:radius, Gm-TransH:cosine. We conduct extensive experiments on
4
90
95
100
105
110
the link prediction and instance classification tasks based on benchmark datasets
FB15K [6] and JF17K [11]. Comparing with baseline models including Trans(E,
H, R) and m-TransH, experimental results show that Gm-TransH outperforms
the previous multi-fold relation embedding methods by a large margin and
achieves up to 13.8% improvement over the comparative models.
The main contributions of our work are as follows:
(a)	Present a Group-constrained Embedding method for multi-fold relation
embedding (Gm-TransH), which not only embed entities and relations into
continuous vector space, we also learn the embedding of facts and model the
correlation with entities and relations explicitly.
(b)	Instead of modeling the correlations of entities, facts and relations as a whole,
we learn the correlation of each two of them separately. This settles the 4
shortcomings of m-TransH and is shown to be efficient for predicting and
classification.
(c)	To model the correlation between facts and relation types, we constrain the
instantiated fact embeddings to be close to their belonging relation embed-
dings and far away from others that are not. In this way, the translation
embedding model for entity-relation correlation learning and the GCN model
for entity-fact correlation learning are interacted and balanced during the
training stage.
(d)	We clean the redundant data and generate a new subset Gf act for the JF17K
datasets in which the repetitive instances are removed and the missing facts
are appended.
(e)	We compare our model with the optimal translating embedding approaches,
tensor factorization approaches and the most up to dated GCN models on
several canonical datasets and have shown continuous improvements over
the existing models.
5
115
2. Motivation and Related Work
The most related works on representation learning of knowledge bases can
be divided into two classes: binary relation embedding and multi-fold relation
embedding. These works are briefly summarized in Table 1.
2.1.	Binary Relation Embedding
120 Most of the models proposed for knowledge base embedding are based on
binary relations, datasets are in triple representation.
TransE [6] sets (h + r) to be the nearest neighbor of t when (h, r, t) holds,
far away otherwise. The cost function is defined as
fr(h,t)= kh+r-tk22	(1)
TransH [7] is developed to enable an entity to have distinct distributed
representations when involved in different relations. For a relation r, TransH
models the relation as a vector r on a hyperplane with nr as the normal vector.
For a triple (h, r, t), the entity embeddings h and t are first projected to the
hyperplane of nr , denoted as h⊥ and t⊥ . The cost function is defined as
fr (h, t) = kh⊥ + r - t⊥k22	(2)
where h⊥ = nrT hnr and t⊥ = nrT tnr.
TransR [8] models entities and relations in distinct spaces and performs
translation in relation space. For each relation r, a projection matrix Mr is used
to project entities from entity space to relation space, i.e. hr = hMr , tr = tMr .
The cost function is correspondingly defined as
fr (h, t) = khr + r - trk22	(3)
Besides TransE, TransH and TransR, there are also many other embedding
methods based on binary relations, such as Unstructured Model(UM) [14], Struc-
125 tured Embedding Model(SME) [15], Single Layer Model(SLM) [16], Semantic
Matching Energy Model(SME) [17, 14] and Neural Tensor Network Model(NTN)
[16], Latent Factor Model(LFM) [18], PTransE [5], TransA [19], TransD [20],
TranSparse [21], KG2E [22], ITransF [23], ProjE [9] and so on.
6
2.2.	Multi-fold Relation Embedding
130
135
140
For knowledge bases with multi-fold relations, S2C conversion and decompo-
sition framework [11] are usually used. Then, multi-fold relations are converted
to triples and treated as binary relations.
We find that the existing models for multi-fold relation embedding directly
without converting into binaries mostly focus on modeling either the correlation
between entities and their relations or the relatedness of entities participate
in a common instance. These methods neglect the fact information and it’s
relatedness to entity components and relation types.
Wen et al. [11] proposes m-TransH model with a direct modeling framework
to learn the embeddings of the entities and the n-ary relations, which generalizes
TransH directly to multi-fold relations. In m-TransH, the cost function fr is
defined by
2
fr (t) = X	ar(ρ)Pnr(t(ρ))+br ,t∈NM(Rr)	(4)
ρ∈M(Rr)	2
Where M(Rr) denotes roles of relation Rr , N denotes all entities in a KB,
Rr on N with roles M(Rr) is a subset of NM(Rr), t is an instance of Rr. Pnr (z)
is the function that maps a vector z ∈ U to the projection of z on the hyperplane
with normal vector nr , namely,
Pnr (z) = z - nr> znr	(5)
nr and br are unit length orthogonal vectors in U , ar ∈ RM(Rr) is a function
that
X	ar(ρ) = 0	(6)
ρ∈M(Rr)
3. Representation and Embedding Problem Definition
From an algebraic point of view, a multi-fold (n-ary) relation on set N is
defined as a subset of the cartesian product of n sets N ×N ×∙∙∙× N, namely
7
N n . Each coordinate of the n-dimensional cartesian product should be specified
to a different role of the relation.
145
150
155
160
165
170
We focus on the representation and embedding of multi-fold relations in the
form of either triples (with binary relations), instances (with n-ary relations,
n ≥ 2) or facts (each role may involves a list of entities) in knowledge bases. We
first introduce an unified way to represent knowledge base (KB) with multi-fold
relations, then based on the unified representation, we discuss the embedding
problem of these n-ary relations and give a mathematical definition of the
problem.
Unified Relation Representation
Different from the most common methods to represent KB as a collection
of entity nodes, relations and samples of triples or instances, we create a fact
node (a.k.a. instance node) to represent the relation instance with links to all
its participants (i.e. entity nodes) and employ an unified framework to represent
KB as a collection of entity nodes, fact nodes, relations and relation instances.
We design the unified representation framework as below:
For a given knowledge base (KB) G, let Ne denote the set of all entities
in G, R indexes a set of distinct multi-fold relations on Ne , T denotes a set of
instances defined over Ne and R. We create a fact (or instance) node for each
instance in T and form a fact node set Nf . For each index r ∈ R, relation Rr
on entities Ne with roles M(Rr) is a subset of NeM(Rr) , where M(Rr) is a set
of ordered role tuples {p1,p2,…，ρ∣M(Rr)∣} of relation Rr, NMIRTr denotes the
set of all functions mapping from M(Rr ) to Ne. We call Rr a J -fold or J -ary
relation if cardinality |M(Rr)| = J, J is the “fold” or “arity” of Rr, each relation
Rr is allowed to have an arbitrary arity. Let Tr be the set of instances of relation
Rr in G, each instance t ∈ Tr in the relation Rr is a vector (x1, x2, ..., x|M(Rr)| )
of entities, in which entity xi corresponds to role ρi ∈ M(Rr), and instance t
corresponds to a unique fact node uj ∈ Nf. The relation Rr can be semantically
understood from the set of all such participated vectors. Then the knowledge
base G can be specified as (Ne, Nf, R, {Tr : r ∈ R}), named as unified relation
representation.
8
We argue that this unified relation representation method can represent a
variety of relations, including binary relations as triples, n-ary relations (n ≥ 2)
as instances and relations (such as facts) whose roles may involve a list of entities.
Figure 1: Illustration of unified relation representation for an instance of 5-ary relation “who
played which role in which film directed by whom in which year”.
175 For example, as shown in Figure 1, instance “Leonardo DiCaprio played
the role of Jack Dawson in the film ‘Titanic’ directed by James Cameron in
1997” should be written as t=(Leonardo DiCaprio, Jack Dawson, Titanic, James
Cameron, 1997), which involves 5 entities each corresponding to a role in 5-fold
relation M(Rr):={ACTOR, CHARACTER, MOVIE, DIRECTOR, YEAR}.
180 An instance node u1 is attached to the instance to represent its structural
information and enable the learning of correlation of instances with entities and
relations in embedding stage below.
Figure 2: Illustration of unified relation representation for fact with role ‘ACTOR’ involve two
entities ‘Leonardo DiCaprio’ and ‘Kate Winslet’. This fact is converted into two instances, i.e.
t1 and t2 , but share the same fact node uk .
9
185
190
195
200
205
210
For multi-fold relations in the form of facts, where each role (or participant)
ρi ∈ M(Rr) of the relation Rr may involve an ordered list of individuals (i.e.
entities) rather than a single individual xi . We follow m-TransH and convert the
multi-fold relations from a fact to several instances, i.e. instance representation,
in which each role ρi corresponds to an unique entity xi . Meanwhile, we introduce
a set of fact node, denoted as Nf , instances of the same fact share a same fact
node, this information is shown to be useful in m-TransH. For example, in
Figure 2, a fact “Leonardo DiCaprio and Kate Winslet acted in the film ‘Titanic’
directed by James Cameron in 1997” can be converted into two instances of
4-fold relation “who acted in which film directed by whom in which year”, namely
t1=(Leonardo DiCaprio, Titanic, James Cameron, 1997) and t2=(Kate Winslet,
Titanic, James Cameron, 1997).
Multi-fold Relation Embedding
Multi-fold relation embedding aims to embed the entities, n-fold relations and
the instances into continuous low-dimensional vector space (a.k.a. embedding
space), and represent these elements as tensors such as vectors or matrices, while
the structural information and correlation between them are preserved.
Based on the unified relation representation, we formulate the multi-fold
relation embedding problem in KB as follows. In order to enhance the expressive
ability, we choose three different vector space over field R (typically real numbers)
as the embedding space for entity nodes, fact (or instance) nodes and relations
respectively, namely Ue , Uf and Ur . Let e ∈ Rk be an embedding vector of
entity node e in Ue , u ∈ Rl be an embedding vector of fact (or instance) node u
in Uf , and r ∈ Rm is the embedding vector of relation Rr in Ur .
The objective of multi-fold relation embedding problem is to construct a
function φe : Ne → Ue , a function φf : Nf → Uf and a subset Cr ⊂ UrM(Rr) for
each relation Rr such that ideally the following properties are satisfied.
1.	For every r ∈ R and every instance t ∈ Rr , φ ◦ t ∈ Cr , where the symbol
◦ denotes function composition.
2.	For every r ∈ R and every function t ∈ N M(Rr) \ Rr , φ ◦ t ∈/ Cr .
Here, the function φe , serving as a representation of Ne , maps an entity e
10
215
220
225
230
235
240
to its embedding vector e. The function φf , serving as a representation of Nf ,
maps a fact node u to its embedding vector u. The subsets {Cr : r ∈ R}, serving
as a representation of {Rr : r ∈ R}, define a set of constraints on the embedding
vectors which preserve the intra-relational and inter-relational structures of
{Rr : r ∈ R}.
Note that each constraint Cr may be identified with a nonnegative cost
function fr : UM(Rr) → R such that
fr (t) = 0 if t ∈ Cr, and
fr (t) > 0 if t ∈/ Cr
Denote Θ := {fr : r ∈ R}. The problem then translates to determing
(Θ, φe, φf). But {Rr : r ∈ R} is unknown, and all we have is the observed
instances {Tr : r ∈ R} and possibly some ”negative examples” {Tr- : r ∈ R},
where each Tr- ⊂ NM(Rr) \ Rr . Note that when the KB is large, for any t ∈ Tr ,
if we replace its value t(ρ) for some role ρ ∈ M(Rr) with a random entity, the
resulting function falls in NM(Rr) \ Rr with high probability. This can be used
to construct Tr- .
Treating the problem as learning (Θ, φe, φf), we may not need the property
1 above to hold strictly. Then the equality “=0” in (1) is taken as “as close to
0 as possible”. Towards a margin-based optimization formulation (which gives
better discriminative power and robustness), the threshold 0 in (2) is raised to a
positive value c. The problem can then be formulated as finding (Θ, φe , φf ) to
minimize the following global cost function.
F(Θ,φe,φf) := Pr∈R(Pt∈Tr fr(φe ◦ t, φf ◦ t) + Pt-∈Tr-[c - fr(φe ◦ t-, φf ◦
t-)]+)
where [.]+ denotes the rectifier function, namely, [a]+ := max(0, a).
What remains is to choose a proper space of Θ for this optimization problem,
which is at the heart of modeling.
11
4.	Model Description
245
250
255
260
265
Towards the goal of multi-fold relation embedding problem, we need to learn
three functions, i.e. (Θ, φe, φf). The function φe maps Ne to Ue, which can be
any of the models in “Trans series” such as TransH or DistMult. The function
φf maps Nf to Uf , and we argue that the fact nodes are correlated with the
entities participated, so we use graph convolutional network (GCN) to learn
the embedding vector of fact nodes u in Nf . As to function Θ, it models the
multi-fold relations and should catch the structural information and correlation
between entity nodes, fact nodes and their relations. We model the function as a
threefold correlation, namely, to learn the correlation between each two of entity
nodes, fact nodes and relation types.
In this section, we first propose a correlation-constrained embedding framework
for multi-fold relation embedding problem and show how to model the threefold
correlations (i.e. correlation between entity nodes and relations, correlation
between entity nodes and fact nodes, correlation between fact nodes and relations)
in detail. Then, we analyze the complexity of the proposed framework and
compare with some of the canonical models. We discuss how to learn the
proposed model and an algorithm of high-efficiency is introduced in the end.
4.1.	Our Framework
As depicted in Figure 3, based on the unified relation representation method,
we introduce a correlation-constrained embedding framework to embed entity
nodes, fact nodes and relation types into three different vector spaces respectively
and model their correlations as a whole.
First, we model correlation between entity nodes and relations which reflects
the meaning of relation types. Second, we learn the representation of fact nodes
by modeling correlation between entity nodes and fact nodes via a GCN model.
Finally, correlation between fact nodes and relations are used to model the
closeness of fact embeddings mapping to their belonging relation types.
12
270
275
280
We define the cost function for each instance t as
fr(t) = WER * gER(t) + WEF * gEF(t) + WFR * gFR (t),t ∈ N M (Rr)⑺
where grER (t) denotes the loss for modeling correlation between entity nodes and
relations, grEF (t) denotes the loss for modeling correlation between entity nodes
and fact nodes, grFR (t) denotes the loss for modeling correlation between fact
nodes and their relations.
4.2.	Modeling Correlation between Entities and Relations
For an instance t = (x1, x2, ..., x|M(Rr)| ) ∈ Tr, entity node xi, i ∈ |M(Rr)|
correlates with relation Rr via instantiating the corresponding role ρi ∈ M(Rr).
Motivated by m-TransH, ProjE and HolE models, we define the following cost
function for learning the correlation between entity nodes and relations in a
multi-fold relational knowledge base. In detail, we utilize a diagonal combination
operator to improve expressivity of the linear combination operator and choose a
diagonal matrix Dr (ρ) instead of a real number ar (ρ) as weight for the projection
of entity to each role ρ, i.e. Pnr (t(ρ)).
The cost function grER (t) for each instance t is then defined as
2
grER (t) = X	Dr(ρ)Pnr(t(ρ))+br , t ∈ NeM(Rr)	(8)
ρ∈M(Rr)	2
where M(Rr) denotes roles of relation Rr , Ne denotes all entities in a KB, Rr
on Ne with roles M(Rr) is a subset of NeM (Rr), t is an instance of Rr and t(ρ)
is the entity to role ρ. Pnr (z) is the function that maps a vector z ∈ Ue to the
pro jection of z on the hyperplane with normal vector nr , namely,
Pnr (z) = z - nr> znr	(9)
nr and br are unit length orthogonal vectors in U , Dr ∈ Rk×k is a function that
X Dr(ρ) = 0	(10)
ρ∈M(Rr)
13
Then the ob jective can be defined as a margin-based optimization problem:
LER = X(X gER(t)+ X [c-gER (t-)]+) + 2 X(kdiag( X Dr (ρ))k2)
r∈R t∈Tr	t- ∈Tr-	r∈R	ρ∈M(Rr)
(11)
where [x]+ = max(0, x), function diag (X) get the leading diagonal vector,
λ ∈ [0, 1] is a balance factor.
4.3.	Modeling Correlation between Entities and Facts
Entity node xi ∈ Ne, i ∈ |M(Rr)| links to fact/instance node t ∈ Nf via
roles of relation r ∈ Rr. To model the correlation between entity nodes and
their linked fact (or instance) nodes, we utilize the structure characteristics of
multi-fold relations in knowledge bases and employ graph convolutional network
(GCN) to learn the representation of fact nodes. GCNs [24, 25] are capable of
learning the structure of local graph neighborhoods for large-scale relational
data and are usually described as a differentiable message-passing framework:
hi(l+1) = δ( X gm(hi(l),h(jl)))	(12)
285 where hi(l) ∈ Rd(l) is the hidden state of node vi in the l-th layer of the neural
network, with d(l) being the dimensionality of this layer’s representations. In-
coming messages of the form gm(∙, ∙) are accumulated on the set of neighbors for
node Vi, i.e. Mi, and passed through an element-wise activation function δ(∙),
such as ReLU or Sigmoid.
In the circumstance of multi-fold relations in the unified representation, entity
nodes’ neighborhoods are consist of different nodes of instances they participated
in with role ρ ∈ M(Rr) and vice versa for the instance nodes. Thus we extend
R-GCN framework [26] to modeling multi-fold relation and propose the following
model for calculating the forward-pass update of an entity node (or instance
node) denoted by vi in a multi-fold relational knowledge base:
h(l+1) = δ(X	X X c-1-wρl)hjl) + W(l)h(l))	(13)
r∈R ρ∈M(Rr) j∈Niρ cr ci,ρ
14
290 where Niρ denotes the set of neighbor indices of the node i under role ρ ∈ M(Rr)
of relaiton r. cr and ci,ρ are problem-specific normalization constant chosen here
as Cr = |R| and c%,ρ = ∣Np∣.
Following R-GCN framework, we add a single self-connection of a special
relation type to each node in the data to ensure the message passing from layer l
295 to layer l + 1. To address the parameters explosion problem, we also apply bisis-
decomposition and block-diagonal-decompostion methods [26] for regularizing the
weights of each layers.
The model takes one-hot vector for each entity (or instance) node in the
graph as input and stack L GCN layers as defined above, we choose DistMult
factorization [27] as the output layer and define the cost function of each instance
t of n-ary relation (n ≥ 2) r as an average cost of triples for each role ρ ∈ M(Rr):
grEF (t)
^MM ρ∈" FeeQfPeef
(14)
where f (ee,ρ, ρ, ee,f) = eeT,ρ Mr (ρ)ee,f with ee,ρ denotes embedding vector of entity
ee,ρ in role ρ and ee,f denotes fact embedding of the instance t. Each role ρ is
300	associated with a diagonal matrix Mr ∈ Rd×d .
We sample w negative examples for each observed (positive) instance by
randomly corrupting either entity of each positive example and utilize cross-
entropy loss bellow as our optimization objective to enforce the model to score
positive examples higher than the negative ones:
LEF = - (1-W^ X(X logl(gEF(t))+ X log(c - MgEF(t-)))) (15)
(1 +w)|T| r∈R e∈Tr	e-∈Tr-
where T is the total set of observed examples and consist of instances in Tr for
every r ∈ R, l is the logistic sigmoid function, c is a constant chosen as margin
such as 1.0.
4.4.	Modeling Correlation between Facts and Relations
We argue that facts should be embedded close to their relations and fact
embeddings of the same relation should aggregate in a cluster, otherwise far
15
away from each other. To measure the similarity between embedded facts and
relations, we employ dot product to calculate the distance of fact embeddings
et,f and relation embeddings rt of instance t. We define the cost function as
grFR(t) = etT,f rt	(16)
Then we define the optimization objective for a single instance t as a margin-
based problem:
Lt = max{0, c - [grF R (t) - grF-R(t-)]}	(17)
305 where t- is a negative instance in which we replace r in t by an n-ary relation
r- (r- 6= r).
We also assume that diagonal matrix Dr (ρ) to be similar to diagonal matrix
Mr (ρ) and define the distance by absolute difference of their normalized trace
(the trace of matrix characterizes its similarity invariance addreferencehere):
d(Mr(ρ),Dr(ρ)) = ktr(Mr(ρ) - tr(Dr(ρ)))k	(18)
where function tr(∙) denotes the normalized trace of a matrix. The optimization
ob jective for each relation r is then defined as
Lr(ρ) = max{0, c - [d(Mr(ρ), Dr(ρ)), d(Mr(ρ), Dr-(ρ-))]}	(19)
where r- ∈ R is an n-ary relaton different from r, ρ- ∈ M(Rr- ).
We then define the total loss function as
LFR=	X (XLt+ X Lr(ρ))	(20)
r,r- ∈R,r6=r- t∈Tr	ρ∈M(Rr)
4.5.	Joint Optimization Problem
Our goal is to embed all the entities e ∈ Ne , facts u ∈ Nf and multi-fold
relations r ∈ Nr in knowledge base G into d-dimensional entity vector space
Ue , fact vector space Uf and relation vector space Ur . As the embedding
vectors of entities, facts and relations are shared across the proposed framework,
our solution is to collectively minimize the three optimization objectives LER,
16
LEF , LFR . To achieve the goal, we formulate a joint optimization problem as
310
315
320
325
330
minimizing the weighted combination of the three ob jectives:
min L =1er * LER + Xef * LEF +1fr * LFR	(21)
The learning of entity, fact and relation embeddings can be mutually influ-
enced via joint optimizing the global objective L, which reduces the errors in
each component and promotes more powerful representations.
5.	Model Learning and Complexity Analysis
5.1.	Learning Method Discussion
To solve the joint optimization problem in Eq.(21), an intuitive solution is
to minimize the three objectives sequentially, i.e. first learn the correlations
of entities and multi-fold relations via optimizing LER on each instances, then
utilize the learned embedding of entities to minimize LEF and train a GCN
model on the whole knowledge base, finally, apply the acquired representations
of entities, facts and relations to optimize LFR. However, such a solution can
hardly converge (training process on LFR will update the embeddings of facts
and relations which may destroy the convergence of LER and LEF). Moreover,
the learning procedure does not fully exploit the correlation between facts and
relations expressed in LF R to provide mutual feedbacks when minimizing LER
and LEF.
Thus we follow CoType [28] and exploit a stochastic sub-gradient descent
algorithm based on edge sampling strategy to efficiently solve Eq.(21), which
can be proved to converge to the local minimum. In detail, we iteratively sample
from each of the three objectives LER , LEF , LFR a batch of positive instances
(e.g., (ei, fj , rk)) and generate V negative samples for each positive ones based
on the Closed World Assumption, i.e. replace any one of the entities(or fact
node) involved in a multi-fold relation instance with other entities or fact nodes
to get a new instance that is not exist in the training set. We then update
each embedding vector based on the derivatives. The model learning process of
CoRelatE is summarized in Algorithm 1.
17
335 5.2. Computational Complexity Analysis
The objective function should be regarded as an optimization task and be
solved by proposing a new algorithm, which employs the proposed tricks to learn
the above three correlations simultaneously. The computation complexity can
be discussed at meanwhile.
Converting multi-fold relations to binary relations results in a heterogeneity
of the predicates, unfavorable for knowledge base embedding. M-TransH [11]
treats fact nodes the same as general entity nodes and ignores the relation level
information that certain facts belong to the same relation. Here, we propose an
optimizing method called Group-constrained Embedding which embeds entity
nodes and fact nodes from entity space into relation space, restricting the
embedded fact nodes related to the same relation to a specific group. The cost
function fr is defined by Eq.(7):
2
fr (t)
arPnr(t(ρ)) +br
ρ∈M(Rr)
+ β * gr (t),
2
(22)
t ∈ NM(Rr)
340 Where gr (t) is a constraint term used to restrict the embedded fact vectors
and relation vectors. β is a balance factor between 0 and 1.
As to the constraint term gr (t), we exploit three different types of constraints
as below:
•	Zero Constraint
Zero constraint adopts a rigorous constraint on the embedded fact vectors,
it requires the Euclidean distance between the embedded fact vector Pnr (ef act)
and its corresponding relation vector r to be zero. Namely,
gr(t) = kr-Pnr(efact)k2,t∈NM(Rr)	(23)
345	• Radius Constraint
Radius constraint adopts a relaxed constraint on the Euclidean distance
between Pnr (ef act) and r. If the fact is an positive instance of the relation r, we
18
need the distance to be smaller than , otherwise much bigger than . In this
way, we define gr (t) as Eq.(9),
350
355
360
365
gr(t) = max(0, kr - Pnr (ef act)k2 - ),t ∈ NM(Rr)	(24)
•	Cosine Constraint
Cosine constraint exploits the cosine distance as measurement, it renders the
distance of the embedded fact vector Pnr (ef act) and its corresponding relation
vector r to be near zero. Namely,
gr(t) = cos hr,Pnr(efact)i ,t ∈ NM(Rr)	(25)
We present an illustration of Group-constrained Embedding methods in
Figure 1, which consists of 4 subgraphs, i.e. graph A, B, C and D. The first
graph A shows the structure of the entities and multi-fold relations in the origin
vector space. The other three graphs show the Group-constrained Embedding of
multi-fold relations with Zero Constraint, Radius Constraint or Cosine Constraint
methods respectively.
In the origin vector space in graph A, we have a 3-ary relation “relation1”
(indicated by orange square) and two instances (indicated by green circle) with
FACT-ID “f act1” and “f act2”. Each of the two instances link with other three
general entities (indicated by blue triangle) through different roles (i.e. role1,
role2 and role3). We present 4 general entities e1, e2, e3 and e4 in the origin
vector space. We can see that f act1 and f act2 share the same entities on “role1”
and “role2”, differentiating on “role3”.
In graph B, C, and D, we indicate the embedded vectors of instances and
entities by adding a single quote to their names, e.g. the embedded vector of
fact node “f act1” is marked as “f act10”. We indicate the embedded multi-fold
relation “relation1” the same as it in the origin vector space since they are the
same vector and without a mapping operation.
Graph B shows the result of Group-constrained Embedding with Zero Con-
straint. As we force the Euclidean distance between the embedded fact vector
“fact10”, “f act20” and its corresponding relation vector “relation1” to be zero,
19
370
375
380
385
these three vectors fall nearly into the same point in the embedded vector space.
When using the radius constraint, as is shown in graph C, “f act10” and “f act20”
fall into a hyper sphere, “relation1” acts as the center of the sphere and the
radius is a decimal number between 0 and 1. We can see that Radius Constraint
degenerates to Zero Constraint when setting to 0. In graph C, we use the
cosine distance as measurement, thus the angles of embedded vector “f act10”,
“f act20” and “relation1” are the same, falling onto a straight line when projected
to a two-dimensional plane.
5.3.	Proposed Model
Using the Group-constrained Embedding method, we propose a new multi-
fold relation embedding model Gm-TransH as below, which consists of three
variations corresponding to the three different types of constraints.
• Group-constrained m-TransH (Gm-TransH)
To solve the problem of m-TransH described above, we propose a new
model that extends m-TransH to make the embedded fact vectors close to their
corresponding relation vectors on the hyperplane.
In detail, we use the Radius Constraint for example, the embedded fact
vectors that belong to the same relation lie in one circle, the relation vector act
as the center of the circle, and the radius is a constant . Namely, if a fact is an
instance of a relation, the distance between the embedded fact vector and the
relation vector is smaller than on the hyperplane, otherwise much bigger than
. The cost function fr is defined as Eq.(11).
2
fr (t) = X	arPnr (t(ρ)) + br	+
ρ∈M(Rr)	2
β * max(0, kbr - Pnr (efact)∣∣2 - e),
t ∈ NM(Rr)
(26)
Where Pnr is defined by TransH, namely E q.(5). Obviously, Eq.(11) is
converted from Eq.(7) by setting the constraint term gr (t) to Eq.(9).
20
We call the above Group-constrained m-TransH model with Radius Constraint
Gm-TransH:radius.
We can also use the Zero Constraint method and the Cosine Constraint
method as substitute of the constraint term gr (t). Namely, with Zero Constraint
method, the model Gm-TransH sets gr (t) to E q.(8), the cost function fr is
defined as Eq.(12)
2
fr (t) = X	arPnr (t(ρ)) + br +
ρ∈M(Rr)	2
β * ∣∣br - Pnr (efact)∣∣2 ,
t ∈ NM(Rr)
(27)
We call the Group-constrained m-TransH model with Zero Constraint Gm-
TransH:zero.
Similarly, with the Cosine Constraint method, the model Gm-TransH sets
gr (t) to Eq.(10), the cost function fr is defined as Eq.(13)
fr (t)
2
X	arPnr(t(ρ))+br	+
ρ∈M (Rr)	2
β * cos hr, Pnr (efact)i ,
(28)
t ∈ NM(Rr)
390 We call the Group-constrained m-TransH model with Cosine Constraint
Gm-TransH:cosine.
5.4.	Complexity Ayalysis
In Table 1, we compare the complexities of several models described in
Related Work and the Gm-TransH models. For binary relation embedding
models like SLM, NTN and Trans(E, H, R, D), we conduct a S2C conversion
[11] for each instance with multi-fold relation, resulting in several triples with
binary relations, which are appropriate for these models. After a S2C conversion,
21
Table 1: Complexities (the number of parameters to train and the times of multiplication
operations in each epoch) of several embedding models. Ne denotes the number of real entities,
Nf denotes the number of fact nodes. Nr represents the number of multi-fold relations (i.e.
fold ≥ 2) and Nr2 represents the number of binary relations. Nt represents the number of
instances with multi-fold relations in the knowledge base. Nt2 represents the number of triples
with binary relations. Nρ denotes the sum of the folds of all instances with multi-fold relations.
m and n are the dimensions of the entity and relation vector space respectively. d denotes the
number of clusters of a relation. k is the number of hidden nodes of a neural network and s is
the number of slice of a tensor.
Model	# Parameters	# Operations
SLM [16]	O(Nem + Nr2(2k + 2nk))	O((2mk + k)Nt2)
NTN [16]	O(Nem + Nr2(n2s + 2ns + 2s))	O(((m2 + m)s + 2mk + k)Nt2)
TransE [6]	O(Nem + Nr2n)	O(Nt2 )
TransH [7]	O(Ne m + 2Nr2n)	O(2mNt2)
TransR [8]	O(Nem + Nr2(m + 1)n)	O(2mnNt2)
CTransR [8]	O (Nem + Nr2 (m + d)n)	O(2mnNt2)
TransD [20]	O(2Nem + 2Nr2n)	O(2nNt2)
m-TransH [11]	O((Ne + Nf )m + 2Nr n + NP)	O(mNρ)
Gm-TransH:zero	O((Ne + Nf )m + 2Nr n + NP)	O(m(Nρ + Nt))
Gm-TransH:radius	O ((Ne + Nf )m + 2 Nr n + NP)	O(m(Nρ + Nt))
Gm-TransH:cosine	O((Ne + Nf )m + 2Nr n + NP)	O(m(Nρ + 3Nt))
the number of instances/triples and relations are changed as follows:
Nr2 = X nri *(nri - 1) ,ri ∈ R	(29)
i=1	2
Nt2= [t n * (nti - 1) ,ti ∈ NM(Rr)	(30)
i=1	2
Nr
Nρ = X nri, ri ∈ R	(31)
i=1
22
Where nri denotes the fold of the i-th relation ri, nti denotes the fold of the
i-th instance ti, Nr	Nr2 and Nt	Nt2 .
395 As listed in Table 1, the number of parameters of Gm-TransH models are
same as m-TransH and lower than the binary relation embedding models. The
time complexity (number of operations) of Gm-TransH models are higher than
m-TransH and close to the TransH model.
As a matter of fact, the training time of the three different Gm-TransH:(radius,
400 zero, cosine) models on the JF17K datasets with a dimension of 25 are about
35,35 and 42 minutes respectively, which are close to transH and m-TransH(30
minutes) models, but outperform the existing methods on link prediction and
relation classification tasks significantly.
6.	Experiments and Analysis
405 In this section, we empirically study and evaluate our approach on two tasks:
link prediction and instance classification.
Table 2: Statistics of the extended JF17K dataset.
Dataset	GX2c/G?2c	GX /G?	GXd/G?d	Gfact/Gfact
# Entities	17629/12282	17629/12282	17629/12282	17818/17818
# Relations	381/336	181/159	181/159	181/159
# Samples	118568/30912	89248/17842	93976/18318	36199/10560
Table 3: Statistics of the origin and extended FB15K dataset.
Dataset	# Rel	# Ent	# Train	# Valid	# Test
FB15K(Raw)	1,345	14,951	483,142	50,000	59,071
FB15K(Ext)	1,345	19,966	483,142	50,000	59,071
23
6.1.	Datasets
410
415
420
425
430
435
JF17K. We use a cleaned and extended JF17K datasets [11] in our exper-
iments. The original JF17K datasets were transformed from the full RDF
formatted Freebase data. Denote the fact representation by F . Two instance
representations T (F) (denoted by G), Tid(F) (denoted by Gid) and a triple
representation S2C (G) (denoted by Gs2c) were constructed, resulting in three
consistent datasets, i.e. G, Gid and Gs2c .
However, as the provided JF17K datasets contain many redundant samples,
which may affect the results, we first cleaned up the repetitive data. In addition,
the fact nodes (or CVT nodes) of a great quantity of instances were missing in
the Gid dataset. We found the fact nodes indicated by role FACT-ID did not
follow a 1-to-1 relationship to the multi-fold relations, which were not applicable
for our proposed models. So we extended the Gid dataset and generated a fact
node for each of these instances. Two instances which share the same relation
and the same entities except one role were assigned a same fact node. We call
the extended set Gf act and divide it into training set GfXact and testing set G?f act .
The statistics of these datasets are shown in Table 2.
FB15K. We also use FB15K dataset [6] on instance classification task. Since
FB15K dataset contains only triples with binary relations and has no fact nodes
in the triples, we extend the FB15K dataset by adding an unique fact node
to each triple. Thus, we can use the extended FB15K to train the proposed
Gm-TransH model and test its performance. We use the origin FB15K dataset
to train the NTN, TransE, TransH and TransR models, for convenience, we use
“Raw” to denote the origin FB15K dataset and use “E xt” to denote the extended
FB15K dataset. Table 3 lists the statistics of the origin and extended FB15K
datasets.
6.2.	Link Prediction
Link prediction aims to complete the missing entities for instances or triples,
i.e., predict one entity given other entities and the relation. For example, for
24
Table 4: The models and datasets used for link prediction.
Experiment	Model	Training Dataset	Testing Dataset
TransE:triple	TransE(bern)	G X Gs2c	G?2c
TransH:triple	TransH(bern)	G X Gs2c	G?2c
TransR:triple	TransR(bern)	GX2c	G?2c
m-TransH:inst	m-TransH	GX	G?
m-TransH:ID	m-TransH	GXd	G?d
Gm-TransH:zero	Gm-TransH	GX + fact	Gfact
Gm-TransH:radius	Gm-TransH	GX + fact	G?. fact
Gm-TransH:eosine	Gm-TransH	GX + fact	G? + fact
triple (h, r, t), predict t given (h, r) or predict h given (r, t). As for instances with
multi-fold relations, the missing entity can be any one of the entities associated
with the relation r. Link prediction ranks a set of candidate entities from
the knowledge graph. We use the extended JF17K datasets in this task and
440 compare with some of the canonical models including TransE, TransH, TransR
and m-TransH.
Evaluation protocol. In this task, for every instance in test set, we remove each
of the entities and then replace it with the entities in the real entity set in
turn. For fairness, we replace only the real entities appeared in the instances
445 and exclude the fact nodes. Dissimilarities of the corrupted instances are first
computed using the proposed models and then sorted by ascending order. Then
we use Hit@10(HIT) and Mean Rank (RANK) [6] of the correct entities ranked
as the performance metrics to evaluate the proposed models. The two metrics
are commonly used to evaluate the performance of knowledge base embeddings.
450 Hit@10 computes the probability of the positive entities that rank the top 10%
for all the entities. Mean Rank means the average position of the positive entities
25
ranked.
455
460
465
470
475
480
Implementation. We conduct eight kinds of experiments in this task, the training
and testing datasets for each of the experiments as well as the model they train
are shown in the Table 4.
Stochastic Gradient Descent is used for training, as is standard. We take L2
as dissimilarity and traverse all the training samples for 1000 rounds. Several
choices of the dimension d of entities and relations are studied in our experiments:
25, 50, 100, 150, 200, 250. We select learning rate λ for SGD among 0.0015,
0.005, 0.01, 0.1, the balance factor β for Gm-TransH among 0.001, 0.01, 0.05 0.1,
the margin γ among 0.5, 1.0, 2.0, and the radius in Gm-TransH:radius among
0.01, 0.05, 0.1, 0.5, 1, the batch size B among 120, 480, 960, 1920. The optimal
configurations of the three Gm-TransH models are Gm-TransH:zero: λ=0.0015,
β=0.01, γ =0.5, d=150, B=960. Gm-TransH:radius: λ=0.0015, β=0.05, γ=1.0,
=0.05, d=250, B=480. Gm-TransH:cosine: λ=0.0015, β=0.01, γ=1.0, d=200,
B=1920.
Results. Experimental results of link prediction on the cleaned and extended
JF17K datasets are shown in Figure 2 and 3, which show the Hit@10 results and
Mean Rank results of different embedding models with dimension 25, 50, 100, 150,
200, 250 respectively. The three Gm-TransH models outperform the Trans(E, H,
R) models by a large margin on both Hit@10 and Mean Rank metrics. Compared
to the m-TransH models, our models achieve an improvement on the probability
of Hit@10 and get an approximate mean rank with m-TransH:inst. The results
show that our approach is effective on improving the accuracy of multi-fold
relation embeddings. Furthermore, Gm-TransH:zero obtains better performance
than Gm-TransH:radius and Gm-TransH:cosine in most cases, showing that Zero
Constraint outperforms Radius Constraint and Cosine Constraint.
6.3.	Instance Classification
Instance classification aims to judge whether a given instance is correct or
not. This is a binary classification task, which has been explored in [16, 7]
26
DIM	25	50	100	150	200	250
⅛- TransEitriplet	29.43	30.28	31.05	31.45	29.53	29.63
→→TransHrtripIet	35.42	36.36	35.51	36.52	35.29	36.58
→-TransRztripIet	35.35	36.56	36.51	36.47	35.56	36.12
• m-TranSH:inst	62.87	66.54	68.71	68.87	67.24	68.51
I m-TransH:ID	73.37	74.06	77.51	78.07	78.55	79.36
< Gm-TransHizero	75.73	78.56	81.17	82.27	82.25	82.63
	Gm-TransHrradius	75.52	80.19	81.05	81.98	82.34	82.37
Gm-TransHxosine	74.29	79.96	80.01	80.27	81.3	82.13
Figure 3:	The probability of ranking the top 10% for different embedding dimensions.
Mean rank result for link prediction on JF17K(cleaned) dataset
Mean Rank 7 Ol 78	9。	12	3	45	67 ooooooαoooo						
						
						
						
						
						
	J ^t^f R』∙	Laai==^					
						
	25	50	IOO	150	200	250
♦ TransErtrIpIet	153.8	1595	149.2	145.6	152.5	155.5
-≡-TransHitripIet	111.2	111.7	120.1	109.2	113.1	123.5
—⅛- TransR:triplet	126.1	104.9	113.3	106.7	114 6	126.9
>⅜ m-TransH:inst	78.7	81.4	76.4	78.6	82.2	86.1
* m-TransHJD	107.2	109.9	107.5	106.1	112.1	105.3
.Gm-TransH zero	82.1	83.9	79.3	76.3	77.B	79.1
I Gm-TransH: radius	83 9	85 2	84 2	78 3	78.8	78 7
	Gm-TransHrcosine	82.7	80.1	80 4	79.7	845	83
Figure 4:	The mean rank for different embedding dimensions.
27
for evaluation. In this task, we use the extended JF17K and FB15K datasets
to evaluate our models. For comparison, we select the NTN, TransE, TransH,
TransR and m-TransH as baseline models.
485
490
495
500
505
Evaluation protocol. For instance classification task, we follow the same protocol
in NTN and TransH. Since the evaluation of classification needs negative labels,
the JF17K and FB15K datasets both contain only positive instances, we construct
negative instances following the same procedure used for FB13 in [16]. For each
golden instance, one negative instance is created.
We set a threshold δr for each relation r by maximizing the classification
accuracies on the training set. For a given instance in the testing set, if the
dissimilarity score is lower than δr , it will be classified as positive, otherwise
negative.
Implementation. For binary relation embeddings of triples, we train and eval-
uate the NTN, Trans(E, H, R) models on the origin FB15K dataset (denoted
as Raw) and the Gs2c dataset of JF17K. We use the NTN code released by
Socher [16] and the Trans(E, H, R) code released by [8] directly. For multi-fold
relation embeddings of instances, we use the m-TransH code released by [11]
and implement the Gm-TransH models to evaluate on extended FB15K(Ext)
dataset and the G, Gid, Gf act datasets of JF17K respectively. We select the
same hyperparameters as used in link prediction and get the average accuracy
of 20 repeated trials.
Results. Table 5 lists the evaluation results of instance classification in detail. We
can observe that on both FB15K and JF17K datasets, the Gm-TransH models
outperforms the baseline models including NTN, Trans(E, H, R) and m-TransH
significantly. The accuracy can reach more than 90% on Gm-TransH:zero and
Gm-TransH:cosine, achieving a new state-of-the-art performance. Moreover,
from the results on the FB15K(Raw) and the FB15K(E xt) datasets, we see
that even for binary relations (i.e. multi-fold relations whose fold equals 2), the
Group-constrained Embedding method is practicable and reliable.
28
Table 5: Evaluation accuracy(%) of instance classification.
Datasets	FB15K(Raw)	FB15K(Ext)	FB17K
NTN	68.2	—	51.3
TransE(unif/bern)	77.3/79.8	—	54.4/58.5
TransH(unif/bern)	74.2/79.9	—	55.6/59.1
TransR(unif/bern)	81.1/82.1	—	60.7/63.4
m-TransH:inst	—	83.2	72.5
m-TransH:ID	—	84.7	76.7
Gm-TransH:zero	—	90.4	89.2
Gm-TransH:radius	—	89.3	88.9
Gm-TransH:cosine	—	90.1	91.3
510 7. Conclusions and Future Work
We presented a Group-constrained Embedding method for multi-fold rela-
tions and proposed a new representation learning framework Gm-TransH using
the optimizing method. We evaluate the effectiveness and performance of the
proposed methods and models on extended FB15K and JF17K datasets. Ex-
515 perimental results show that the Gm-TransH models outperforms all baseline
models on link prediction task and instance classification task. In the future, we
will explore more representation and embedding frameworks for the increasingly
complicated data in knowledge bases, e.g. events and procedures, as well as
incorporating the most recent advances in the learning of binary relations for
520 multi-fold relation embedding.
Acknowledgements
This work was supported by the Science and Technology Program of Shenzhen
of China under Grant Nos. JCYJ20170818160208570 and JCYJ20170307160458368.
29
References
525
530
535
540
545
[1]	K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J. Taylor, Freebase: a
collaboratively created graph database for structuring human knowledge,
international conference on management of data (2008) 1247-1250.
[2]	G. A. Miller, Wordnet: a lexical database for english, Communications of
The ACM 38 (11) (1995) 39-41.
[3]	F. M. Suchanek, G. Kasneci, G. Weikum, Yago: a core of semantic knowl-
edge, international world wide web conferences (2007) 697-706.
[4]	S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, Z. G. Ives,
Dbpedia: a nucleus for a web of open data, international semantic web
conference (2007) 722-735.
[5]	Y. Lin, Z. Liu, H. Luan, M. Sun, S. Rao, S. Liu, Modeling relation paths for
representation learning of knowledge bases, empirical methods in natural
language processing (2015) 705-714.
[6]	A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, O. Yakhnenko, Trans-
lating embeddings for modeling multi-relational data, Advances in Neural
Information Processing Systems (2013) 2787-2795.
[7]	Z. Wang, J. Zhang, J. Feng, Z. Chen, Knowledge graph embedding by
translating on hyperplanes, national conference on artificial intelligence
(2014) 1112-1119.
[8]	Y. Lin, Z. Liu, M. Sun, Y. Liu, X. Zhu, Learning entity and relation
embeddings for knowledge graph completion, national conference on artificial
intelligence (2015) 2181-2187.
[9]	B. Shi, T. Weninger, Proje: Embedding projection for knowledge graph
completion, national conference on artificial intelligence (2017) 1236-1242.
30
550
555
560
565
570
[10]	T. Trouillon, J. Welbl, S. Riedel, E. Gaussier, G. Bouchard, Complex
embeddings for simple link prediction, international conference on machine
learning (2016) 2071-2080.
[11]	J. Wen, J. Li, Y. Mao, S. Chen, R. Zhang, On the representation and
embedding of knowledge bases beyond binary relations, international joint
conference on artificial intelligence (2016) 1300-1307.
[12]	B. Fatemi, P. Taslakian, D. Vazquez, D. Poole, Knowledge hypergraphs:
Extending knowledge graphs beyond binary relations., neural information
processing systems.
[13]	J. Rouces, G. De Melo, K. Hose, Framebase: Representing n-ary relations
using semantic frames, european semantic web conference (2015) 505-521.
[14]	A. Bordes, X. Glorot, J. Weston, Y. Bengio, A semantic matching en-
ergy function for learning with multi-relational data, neural information
processing systems 94 (2) (2014) 233-259.
[15]	A. Bordes, J. Weston, R. Collobert, Y. Bengio, Learning structured embed-
dings of knowledge bases, Proceedings of the Twenty-fifth AAAI Conference
on Artificial Intelligence.
[16]	R. Socher, D. Chen, C. D. Manning, A. Y. Ng, Reasoning with neural tensor
networks for knowledge base completion, neural information processing
systems (2013) 926-934.
[17]	A. Bordes, X. Glorot, J. Weston, Y. Bengio, Joint learning of words and
meaning representations for open-text semantic parsing, international con-
ference on artificial intelligence and statistics (2012) 127-135.
[18]	R. Jenatton, N. L. Roux, A. Bordes, G. Obozinski, A latent factor model for
highly multi-relational data, neural information processing systems (2012)
3167-3175.
31
575
580
585
590
595
600
[19]	H. Xiao, M. Huang, Y. Hao, X. Zhu, Transa: An adaptive approach for
knowledge graph embedding., arXiv: Computation and Language.
[20]	G. Ji, S. He, L. Xu, K. Liu, J. Zhao, Knowledge graph embedding via
dynamic mapping matrix, international joint conference on natural language
processing (2015) 687-696.
[21]	G. Ji, K. Liu, S. He, J. Zhao, Knowledge graph completion with adaptive
sparse transfer matrix, national conference on artificial intelligence (2016)
985-991.
[22]	S. He, K. Liu, G. Ji, J. Zhao, Learning to represent knowledge graphs with
gaussian embedding, conference on information and knowledge management
(2015) 623-632.
[23]	Q. Xie, X. Ma, Z. Dai, E. H. Hovy, An interpretable knowledge trans-
fer model for knowledge base completion, meeting of the association for
computational linguistics 1 (2017) 950-962.
[24]	D. Duvenaud, D. Maclaurin, J. Aguileraiparraguirre, R. Gomezbombarelli,
T. D. Hirzel, A. Aspuruguzik, R. P. Adams, Convolutional networks on
graphs for learning molecular fingerprints, neural information processing
systems (2015) 2224-2232.
[25]	T. Kipf, M. Welling, Semi-supervised classification with graph convolutional
networks, international conference on learning representations.
[26]	M. Schlichtkrull, T. Kiph, P. Bloem, R. v. d. Berg, I. Titov, M. Welling,
Modeling relational data with graph convolutional networks, European
Semantic Web Conference.
[27]	B. Yang, W. Yih, X. He, J. Gao, L. Deng, Embedding entities and relations
for learning and inference in knowledge bases, international conference on
learning representations.
32
[28]	X. Ren, Z. Wu, W. He, M. Qu, C. R. Voss, H. Ji, T. F. Abdelzaher, J. Han,
Cotype: Joint extraction of typed entities and relations with knowledge
bases, the Web conference (2017) 1015-1024.
33