Under review as a conference paper at ICLR 2020
Confidence-Calibrated Adversarial Training
and Detection: MORE ROBUST MODELS GENERALIZ-
ing Beyond the Attack Used During Training
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial training is the standard to train models robust against adversarial ex-
amples. However, especially for complex datasets, adversarial training incurs
a significant loss in accuracy and is known to generalize poorly to stronger at-
tacks, e.g., larger perturbations or other threat models. In this paper, we introduce
confidence-calibrated adversarial training (CCAT) where the key idea is to en-
force that the confidence on adversarial examples decays with their distance to the
attacked examples. We show that CCAT preserves better the accuracy of normal
training while robustness against adversarial examples is achieved via confidence
thresholding, i.e., detecting adversarial examples based on their confidence. Most
importantly, in strong contrast to adversarial training, the robustness of CCAT
generalizes to larger perturbations and other threat models, not encountered dur-
ing training. For evaluation, we extend the commonly used robust test error to
our detection setting, present an adaptive attack with backtracking and allow the
attacker to select, per test example, the worst-case adversarial example from mul-
tiple black- and white-box attacks. We present experimental results using L∞, L2,
L1 and L0 attacks on MNIST, SVHN and Cifar10.
1	Introduction
Deep neural networks have shown tremendous improvements in various learning tasks including ap-
plications in computer vision, natural language processing or text processing. However, the discov-
ery of adversarial examples, i.e., nearly imperceptibly perturbed inputs that cause mis-classification,
has revealed severe security threats, as demonstrated by attacking popular computer vision services
such as Google Cloud Vision (Ilyas et al., 2018a) or Clarifai (Liu et al., 2016; Bhagoji et al., 2017b).
As the number of safety- and privacy-critical applications is increasing, e.g., autonomous driving or
medical imaging, this problem becomes even more important.
In practice, adversarial training and its variants (Szegedy et al., 2013; Goodfellow et al., 2014; Madry
et al., 2018), i.e., training on adversarial examples, can be regarded as the state-of-the-art to obtain
models robust against adversarial examples. In contrast to many other defenses, and to the best of
our knowledge, adversarial training has not been broken so far. However, adversarial training is
known to increase test error significantly. Only on simple datasets such as MNIST (LeCun et al.,
1998), adversarial training is able to preserve accuracy. This observation is typically described as
a trade-off between robustness and accuracy (Tsipras et al., 2018; Stutz et al., 2019; Raghunathan
et al., 2019; Zhang et al., 2019). Furthermore, while adversarial training leads to robust models for
the employed threat model used during training, the obtained robustness does not translate to other
threat models, e.g., other Lp -balls (Sharma & Chen, 2017; Song et al., 2018; Madry et al., 2018;
Tramer & Boneh, 2019; Li et al., 2019a; Kang et al., 2019), larger perturbations than the ones used
during training or distal adversarial examples (Hein et al., 2019).
Contributions: We address both problems of adversarial training: the drop in accuracy on datasets
such as Cifar10 (Krizhevsky, 2009) and the poor “generalization” of robustness to larger pertur-
bations or unseen threat models. To this end, we introduce confidence-calibrated adversarial
training (CCAT) based on the idea that the predicted confidence for an adversarial example should
decrease with its distance to the attacked example. Specifically, we bias the network to predict a
1
Under review as a conference paper at ICLR 2020
convex combination of uniform and (correct) one-hot distribution for adversarial examples, which
becomes uniform as the distance to the attacked test example increases. Thus, CCAT implicitly
biases the network to predict the uniform distribution over classes beyond the adversarial examples
seen during training. In contrast, standard adversarial training forces the network to classify ad-
versarial examples correctly with maximal confidence but provides no guidance how to extrapolate
beyond adversarial examples seen during training. Overall, CCAT allows to detect previously unseen
adversarial examples, e.g., large perturbations, different Lp attacks or distal adversarial examples,
by confidence thresholding, i.e., detection.
We also discuss a worst-case evaluation methodology using an adaptive attack to compare our
detection-based approach with adversarial training. First, we generalize the commonly used robust
test error (Madry et al., 2018; Schott et al., 2019) to our detection setting. Second, for each test
example, the worst-case adversarial example, i.e., with highest confidence, is selected from multiple
white- and black-box attacks. Using an adapted attack based on (Madry et al., 2018; Dong et al.,
2018), augmented with a novel backtracking scheme, we show that CCAT produces models with im-
proved accuracy on Cifar10 and robustness to various unseen adversarial examples: larger pertur-
bations, L2, L1 and L0 attacks, distal adversarial examples and corrupted examples on MNIST(-C),
(LeCun et al., 1998; Mu & Gilmer, 2019) SVHN (Netzer et al., 2011), Cifar10(-C) (Hendrycks &
Dietterich, 2019). We will make our code for CCAT and evaluation publicly available.
Outline: After reviewing related work in Sec. 1.1, we discuss adversarial training (Madry et al.,
2018) and introduce CCAT in Sec. 2. In Sec. 3 we present our confidence-thresholded robust test
error and present experimental results in Sec. 4. Finally, we conclude in Sec. 5.
1.1	Related Work
Adversarial Examples: Adversarial examples can roughly be divided into white-box attacks, i.e.,
with access to the models, its weights and gradients, e.g. (Goodfellow et al., 2014; Madry et al.,
2017; Carlini & Wagner, 2017b), and black-box attacks, i.e., only with access to the output of
the model, e.g. (Chen et al., 2017; Brendel & Bethge, 2017; Su et al., 2017; Ilyas et al., 2018b;
Sarkar et al., 2017; Narodytska & Kasiviswanathan, 2017). Adversarial examples were also found
to be transferable between similar models (Liu et al., 2016; Xie et al., 2018). While these attacks
are intended to be (nearly) imperceptible, visible adversarial transformations, e.g., rotations or
rotations (Engstrom et al., 2017; 2019; Dumont et al., 2018; Alaifari et al., 2018; Xiao et al., 2018),
or patches (Liu et al., 2018; Lee & Kolter, 2019; Brown et al., 2017; Karmon et al., 2018; Zajac
et al., 2019) have also been proposed and transferred to the physical world (Athalye et al., 2018b;
Li et al., 2019b; Kurakin et al., 2016). We refer to recent surveys (Barreno et al., 2006; Yuan
et al., 2017; Akhtar & Mian, 2018; Biggio & Roli, 2018a) for more details.. Recently, white-box
attacks utilizing projected gradient ascent to maximize cross-entropy loss or surrogate objectives,
e.g., (Madry et al., 2017; Dong et al., 2018; Carlini & Wagner, 2017b), have become standard.
Instead, we directly maximize the confidence in any but the true class, similar to (Hein et al., 2019;
Goodfellow et al., 2019), to attack our proposed training procedure, CCAT.
Adversarial Training: Many defenses against adversarial attacks have been proposed, see, e.g.,
(Yuan et al., 2017; Akhtar & Mian, 2018; Biggio & Roli, 2018b), of which some have been shown
to be ineffective, e.g., in (Athalye et al., 2018a; Athalye & Carlini, 2018). Currently, adversarial
training, i.e., training on adversarial examples, is the de-facto standard to obtain robust models.
While adversarial training was proposed in different variants (Szegedy et al., 2013; Zantedeschi
et al., 2017; Miyato et al., 2016; Huang et al., 2015; Shaham et al., 2018; Sinha et al., 2018; Madry
et al., 2017; Wang et al., 2019), the formulation by Madry et al. (2017) received considerable atten-
tion and has been extended in various ways (Lee et al., 2017; Ye & Zhu, 2018; Liu et al., 2019). For
example, in (Shafahi et al., 2018; Perolat et al., 2018), adversarial training is applied to universal
adversarial example, in (Cai et al., 2018), curriculum learning is used, and in (Tramer et al., 2017;
Grefenstette et al., 2018) ensemble adversarial training is proposed. Adversarial training is also
used in provable/certified defenses (Kolter & Wong, 2017; Zhang & Evans, 2018). The increased
sample complexity of adversarial training (Schmidt et al., 2018) has been addressed in (Lamb et al.,
2019) by training on interpolated examples or in (Carmon et al., 2019; Uesato et al., 2019) using
unlabeled examples. The dependence on the threat model used during training was addressed in
(Tramer & Boneh, 2019) by using multiple threat models during training. Finally, the frequently
observed trade-off between accuracy and robustness has been studied both theoretically and empir-
2
Under review as a conference paper at ICLR 2020
ically (Tsipras et al., 2018; Stutz et al., 2019; Zhang et al., 2019; Raghunathan et al., 2019). CCAT
differs from standard adversarial training in the used attack objective and the imposed distribution
over the labels, which tends towards a uniform distribution for large perturbations.
Detection: Instead of correctly classifying adversarial examples, as intended in adversarial train-
ing, several works (Gong et al., 2017; Rouhani et al., 2018; Grosse et al., 2017; Feinman et al.,
2017; Liao et al., 2018; Ma et al., 2018; Amsaleg et al., 2017; Metzen et al., 2017; Bhagoji et al.,
2017a; Hendrycks & Gimpel, 2017; Li & Li, 2017; Pang et al., 2018) try instead to detect adver-
sarial examples. However, several detectors have been shown to be ineffective against adaptive
attacks, i.e., adversaries aware of the used detection mechanism (Carlini & Wagner, 2017a). Re-
cently, the detection of adversarial examples based on their confidence, similar to our approach with
CCAT, has also been discussed (Pang et al., 2018); however, the authors build on (Feinman et al.,
2017) which has been shown to be ineffective (Carlini & Wagner, 2017a), as well. Goodfellow et al.
(2019), instead, focus on evaluating confidence-based detection methods using adaptive, targeted
attacks maximizing confidence. Our attack, introduced in Sec. 2.2, is similar in spirit, however,
untargeted by nature and, thus, suitable for usage with CCAT.
2	Confidence Calibration of Adversarial Examples
Adversarial training, as described by Madry et al. (2018), has become standard to train robust mod-
els. However, accuracy on challenging datasets such as Cifar10 is reduced and robustness does not
generalize to larger perturbations, e.g., with respect to the used L∞ constraint, or different threat
models in other Lp norms. With CCAT, we intend to address both shortcomings: Instead of training
models to correctly classify adversarial examples, we want to detect adversarial examples based
on their confidence. In the following, we first review adversarial training, before discussing the
modifications needed for our CCAT, thereby turning robustness against adversarial examples into a
detection problem.
Notation: We consider a classifier f : Rd → RK where K is the number of classes and fk
denotes the confidence for class k. While we assume that the cross-entropy loss L is used for
training, our approach can also be used with other losses. Given x ∈ Rd, classified correctly as
y = argmax k fk (x), an adversarial example x + δ is defined as a “small” perturbation δ such that
argmax k fk(x + δ) 6= y, i.e., the classifier changes its decision. The strength of the change δ is
measured by some Lp-norm with p ∈ {0, 1, 2, ∞}; p = ∞ is a popular choice in the literature as
this leads to the smallest perturbation per feature/pixel.
2.1	Adversarial Training
Standard adversarial training, as formulated by Madry et al. (2018), is given as the following min-
max problem:
min E
w
max L(f (x + δ; w), y)
kδk∞ ≤
(1)
with w being the classifier’s parameters and L being the cross-entropy loss. During mini-batch
training the inner maximization problem,
max L(f (x + δ; w), y),
kδk∞≤
(2)
is approximately solved. In addition to the L∞-constraint, a box constraint, i.e., Xi = (X + δ)i ∈
[0, 1], is enforced for images. Note that maximizing the cross-entropy loss is equivalent to finding
the adversarial example with minimal confidence in the true class. For neural networks, this is
generally a non-convex optimization problem. In (Madry et al., 2018) the problem is tackled using
projected gradient descent (PGD), which is typically initialized using a random δ with kδ k∞ ≤ .
During adversarial training, PGD is applied to the batch of every iteration in order to update the
classifier’s parameters on the obtained adversarial examples. At test time one uses the best out of
several random restarts to assess robustness.
In contrast to adversarial training as proposed in (Madry et al., 2018), which computes adversarial
examples for the full batch in each iteration, others compute adversarial examples only for half the
3
Under review as a conference paper at ICLR 2020
.................. O
86420
..................
0000
ssalC rep ecnedfinoC
AT
0.02	0.04	0.06
Perturbation Distance
CCAT
ssalC rep ecnedfinoC
..................O
86420
..................
0000
ssalC rep ecnedfinoC
0.02	0.04	0.06
Perturbation Distance
ssalC rep ecnedfino
----,-----...-----Classes 1,..., 10
Figure 1: Confidence Calibration. For ad-
versarial training (AT) and our confidence-
calibrated adversarial training (CCAT) with
ρpow = 10 using the power transition in Eq. (6),
we plot the probabilities for all ten classes along
adversarial directions. Adversarial examples
were computed using our L∞-PGD-Conf at-
tack, cf. Sec. 4.1: using projected gradient de-
scent (Madry et al., 2018) the confidence of the
adversarial examples is maximized for T =
1000 and L∞-constraint	= 0.03. The ro-
bustness of AT does not generalize beyond the
= 0.03-ball as high confidence adversarial ex-
amples can be found for larger perturbations,
whereas CCAT predicts close to uniform con-
fidence after some transition phase allowing to
easily detect adversarial examples.

examples of each batch (Szegedy et al., 2013), i.e., instead of training only on adversarial examples,
each batch is divided 50% into clean and 50% into adversarial examples. Compared to Eq. (1),
50%/50% adversarial training effectively minimizes
minw E[ max∣∣δk∞≤e L(f(x + δ; w),y)] + E[L(f(x; w),y)]
×---------------------------------} ।  ..... {z_- '
(3)
{^^^^^^^^^^^≡
50% adversarial training
50% “clean” training
This improves test accuracy on clean examples compared to 100% adversarial training, i.e., Eq. (1),
but typically leads to worse robustness. Intuitively, by balancing both terms in Eq. (3), the trade-off
between accuracy and robustness can already be optimized to some extent (Stutz et al., 2019).
There are two problems with adversarial training: First, the -ball around training examples might
include examples from other classes. Then, the attack, i.e., the inner maximization in Eq. (1), will
focus on these regions such that adversarial training for these examples gets “stuck”. This case is
illustrated in our theoretical toy dataset in Sec. 2.3 and has also been considered in related work
(Jacobsen et al., 2019b;a). Here, both 100% and 50%/50% adversarial training, cf. Eq. (1) and
(3), are not able to find the Bayes optimal classifier in a fully deterministic problem, i.e., zero Bayes
error. This might contribute to the observed drop in accuracy for adversarial training on, e.g., Cifar10
(Krizhevsky, 2009). Second, and more importantly, adversarial training as in Eq. (1) does not guide
the classifier how to extrapolate beyond the used -ball during training. Even worse, it enforces high
confidence predictions within the -ball, which clearly cannot be extrapolated to arbitrary regions.
It is not surprising that adversarial examples can often be found right beyond the -ball, i.e., using
larger perturbations than at training time or other threat models, e.g., other Lp norm balls.
2.2	Confidence-Calibrated Adversarial Training
Addressing these problems requires only few but effective modifications as outlined in the description
of our proposed confidence-calibrated adversarial training (CCAT) in Alg. 1: during training, we
bias the network to predict a uniform distribution over the classes on adversarial examples that are
sufficiently far away from the original training examples. Subsequently, during testing, adversarial
examples can be detected by confidence thresholding - adversarial examples receive near-uniform
confidence while test examples receive high-confidence. Thus, given an example x, the ideal attacker
during training maximizes the confidence in any arbitrary other label k 6= y instead of minimizing
the confidence in the true label y, as in Eq. (1):
max max fk (x + δ; w)	(4)
kδk∞ ≤ k6=y k
where fk denotes the probability f assigns to class k (i.e., in practice, f is the output of the soft-
max layer). A similar objective but for targeted attacks has been used in (Goodfellow et al., 2019),
whereas our goal is an untargeted attack and thus our objective is the maximal confidence over all
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Pseudo-code of confidence-calibrated adversarial training (CCAT). The main
changes compared to regular adversarial training as, e.g., described in (Madry et al., 2018) or
(Szegedy et al., 2013), are in the attack (line 4) and the probability distribution over the classes
(line 6,7), which becomes more uniform as distance kδk∞ increases.
1:	while true do
2:	choose random batch (x1, y1), . . . , (xB, yB).
3:	for b = 1, . . . , * * * * * B/2 do
4:	{maximize confidence in other classes than true one for adversarial example Xb, Eq. (4):}
5:	δb := argmax kδk∞≤ maxk6=yb fk(xb + δ)
6:	Xb := Xb + δb
7:	{probability over classes of Xb becomes more uniform as ∣∣δbk∞ increases, Eq. (6):}
8:	λ := e-ρkδbk∞ or λ := (1 一 min(1, kδk∞∕e))ρ
9:	{yb is convex combination of one hot and uniform distribution over the classes, Eq. (5):}
10:	讥:=λone_hot(yb) + (1Kλ)l
11:	end for
12:	{corresponds to 50%/50% adversarial training, Eq. (3):}
13:	update parameters using PB=21 L(f(Xb),yb) + P=B/2 L(f (xb), y)
14:	end while
other classes. Then, during training, CCAT biases the classifier towards predicting uniform distribu-
tions on adversarial examples by using the following distribution as target in the cross-entropy loss:
p^(k) = λpy (k) + (1 — λ)u(k) k = 1, ..., K.
(5)
Here,py(k) is the original “one-hot” distribution, i.e., py (k) = 1 iff k = y andpy(k) = 0 otherwise
with y being the true label, and u(k) = 1/K is the uniform distribution. Thus, we enforce a convex
combination of the original label distribution and the uniform distribution which is controlled by
the parameter λ. We choose λ to decrease with the distance ∣δ ∣∞ of the adversarial example to
the attacked example X with the intention to enforce uniform predictions when ∣δ∣∞ = δ. Then,
the network is encouraged to extrapolate this uniform distribution beyond the used -ball. Even
if extrapolation does not work perfectly, the uniform distribution is much more meaningful for ex-
trapolation in the region between classes compared to high-confidence predictions as encouraged in
standard adversarial training. We consider two variants of transitions, i.e., controlling the trade-off
λ between one-hot and uniform distribution:
λ =e-ρkδk∞
λ =(1 — min(1, kδk∞∕e))ρ
(“exponential transition” (exp))
(“power transition” (pow))
(6)
This ensures that for δ = 0 we impose the original (one-hot) label. For growing δ, however, the
influence of the original label decays proportional to ∣δ∣∞. The speed of decay is controlled by the
parameter ρ. For the exponential transition, we always have a bias towards the true label as even
for large ρ, λ will be non-zero. In case of the power transition, λ = 0 for ∣δ ∣∞ ≥ , meaning a
pure uniform distribution is enforced. It is important to note that in CCAT in Alg. 1 we train on
50% clean and 50% adversarial examples in each batch, as in Eq. (3). Training only on adversarial
examples will not work as we the network has no incentive to predict correct labels.
2.3 Confidence-Calibrated Adversarial Training Yields Accurate Models
Proposition 1 analyzes 100% adversarial training and its 50%/50% variant as well as our confidence-
calibrated variant, CCAT, to show that there exist problems where both 100% and 50%/50% adver-
sarial training are unable to reconcile robustness and accuracy, as recently discussed (Tsipras et al.,
2018; Stutz et al., 2019; Raghunathan et al., 2019; Zhang et al., 2019). However, our CCAT is able
to obtain both robustness and accuracy given that λ in Eq. (6) is chosen appropriately.
Proposition 1. We consider a classification problem with two points X = 0 and X = in R with
deterministic labels, that is p(y = 2|X = 0) = 1 and p(y = 1|X = ) = 1 and the problem is
fully determined by the probability p0 = p(X = 0) as p(X = ) = 1 — p0. The Bayes error of this
5
Under review as a conference paper at ICLR 2020
classification problem is zero. The predicted probability distribution over the classes is p(y∣x)
egy(X)
egi(X)+eg2(X)'
where g : Rd → R2 and we assume that the function λ : R+ → [0, 1] is monotonically
decreasing and λ(0) = 1. The Bayes optimal classifier for the cross-entropy loss of
•	adversarial training on 100% adversarial examples, cf. Eq. (1), yields an error of
min{p0, 1 - p0}.
•	adversarial training with 50% adversarial and 50% clean examples per batch, cf. Eq. (3),
yields an error of min{p0 , 1 - p0}.
•	CCAT on 50% clean and 50% adversarial examples, cf. Alg. 1, yields zero error if λ() <
min n 4, 1-po O.
1-p0 , p0
3	Confidence-Thresholded Robust Test Error (RErr)
CCAT runs in a two-stage process: First, examples are rejected by confidence thresholding. Ideally,
all adversarial examples are rejected as CCAT encourages low-confidence adversarial examples.
Second, we evaluate robustness and accuracy on the non-rejected (i.e., correctly or incorrectly clas-
sified) examples. In the first stage (i.e., confidence thresholding), we consider successful adversarial
examples as negatives and correctly classified test examples as positives. Then, we report the area
under the ROC curve, i.e., ROC AUC, which shows how well we can discriminate adversarial exam-
ples from correctly classified clean examples. In the second stage (i.e., after detection by confidence
thresholding), wefix the threshold τ at a true positive rate (TPR) of 99% for correctly classified test
samples, i.e., the network is allowed to reject at most 1% of correctly classified test examples. We
note that this also improves accuracy as errors on test examples typically have lower confidence.
Then, we intend to use test error (Err) and robust test error (RErr), as also used in (Madry et al.,
2018), to evaluate adversarial training. However, extending RErr to take into account confidence-
thresholding as in our detection setting is non-trivial: for example, correctly classified examples can
can have lower confidence than their corresponding adversarial examples. Thus, given confidence
threshold τ , we define the confidence-thresholded robust test error RErr as follows:
NN
Σ ɪf (Xn) = yn lc(Xn)≥T + Σ ɪf(Xn) = Jn ɪf(Xn) = Jn L(Xn)≥T
RErr(T ) = nN------n-----n=1--------------------.	⑺
EIc(Xn)≥τ +Σlc(Xn)<τlc(Xn)≥τlf (Xn) = Jn ɪf (Xn) = yn
n=1	n=1
Here, τ is the confidence-threshold fixed on a held-out validation set, {(xn, yn)}nN=1 are test exam-
ples, c(xn) := maxk fk(xn) denotes the classifier’s confidence on xn, f(xn) := argmax k fk(xn)
denotes the classifier's decision, and Xn are adversarial examples. The numerator counts the number
of incorrectly classified test examples xn with c(xn) ≥ τ (first term) and the number of successful
adversarial examples Xn on COrreCtIy classified test examples with C(Xn) ≥ τ (second term). The
denominator counts test examples xn with c(xn) ≥ τ (first term) and the number of successful ad-
versarial examples Xn with C(Xn) ≥ T but where the corresponding test example Xn has C(Xn) < T
(second term). The latter takes care of the special case where adversarial examples have higher
confidence than their corresponding test examples, as mentioned above and is encouraged by the
objective of our attack in Eq. (4). In total this yields a correct fraction within [0, 1] and for T = 0,
Eq. (7) reduces to the “standard” RErr. The confidence-thresholded Err(T) corresponds to taking
only the first terms in both numerator and denominator.
4	Experiments
We compare normal training, adversarial training (AT) and our CCAT on MNIST, (LeCun et al.,
1998) SVHN (Netzer et al., 2011) and Cifar10 (Krizhevsky, 2009). We use ResNet-20 (He et al.,
2016), implemented in PyTorch (Paszke et al., 2017), initialized following (He et al., 2015) and
trained using stochastic gradient descent with batch size of 100 for 100 or 200 epochs (MNIST and
SVHN/Cifar10, respectively). As detailed in Sec. 3, we report ROC AUC (higher is better) as well
as our confidence-thresholded test error (Err; lower is better) and robust test error (RErr; lower is
better); Err is reported on the full test sets, while ROC AUC and RErr are reported on the first 1000
attacked test examples. More details and experimental results can be found in Appendix B.
6
Under review as a conference paper at ICLR 2020
----, ---, --,...Attacks on Different Test Examples
Figure 2: Momentum and back-
tracking. Our L∞ PGD-Conf attack,
cf. Sec. 4.1, with 40 iterations with
momentum and backtracking (left) and
without both (right). We plot the ob-
jective of Eq. (4) over iterations for 10
samples (different colors).
4.1	Attacks
White-box and adaptive attacks: We follow (Madry et al., 2018) and use projected gradient descent
(PGD) to minimize the negatives of Eq. (2) and (4); we denote them as PGD-CE and PGD-Conf. The
perturbation δ is initialized uniformly over direction and distance; for PGD-Conf, we additionally
use δ = 0 as initialization. Different from (Madry et al., 2018), we run exactly T iterations (no
early stopping) and take the perturbation corresponding to the best objective of the T iterations. In
addition to momentum, as in (Dong et al., 2018), we propose to use an adaptive learning rate in
combination with a backtracking scheme to improve the attacks: after each iteration, the computed
update is only applied if it improves the objective; otherwise the learning rate is reduced. We also
implemented PGD for L2, L1 and L0 attacks; note that for each Lp, p ∈ {∞, 2, 1, 0}, PGD-Conf
has been explicitly designed to attack our CCAT. We train on L∞ attacks using T = 40 iterations
with = 0.3 (MNIST) or = 0.03 (SVHN/Cifar10). For evaluation, we use T = 2000 iterations and
10 random retries for PGD-Conf; T = 200 with 50 random retries for PGD-CE. For L2, L1 and L0
attacks, we set to 3, 10, 15 (MNIST) or 1, 7.85, 10 (SVHN/Cifar10), respectively, partly following
(Tramer & Boneh, 2019). Learning rates have been tuned for each Lp and objective (i.e., CE or
Conf) independently.
Black-box attacks: For the L∞ case, we additionally use random sampling, the attack by Ilyas et al.
(2018a), adapted with momentum and backtracking optimizing Eq. (4) for T = 2000 iterations with
10 attempts, a variant of (Narodytska & Kasiviswanathan, 2017) with T = 2000 iterations and the
“cube” attack (Andriushchenko, 2019) with T = 5000 iterations. We also consider the cube attack
in the L2 setting and the attack of (Croce & Hein, 2019) in the L0 setting. The black-box attacks
ensure that our defense avoids, e.g., gradient masking as described in (Athalye et al., 2018a).
4.2	Worst-Case Evaluation Methodology
Instead of reporting robustness against individual attacks, as commonly done in the literature, we
use a worst-case evaluation scheme: For each individual test example, all adversarial examples
from multiple attempts (i.e., multiple random restarts) and across all (white- and black-box) attacks
are accumulated. Subsequently, per text example, only the adversarial example with highest confi-
dence is kept; these are the worst-case adversarial examples for each test example. In practice, this
scheme allows to accumulate results from different attacks and thus corresponds to a much stronger
robustness evaluation than usual.
SVHN: Attack Ablation With RErr for T @99%TPR							
	(L∞ attack With			=0.03)				
Optimization	momentum+backtrack					mom	—
Initialization	zero				rand	zero	zero
Iterations T	40	200	2000	4000	4000	300	300
-AT	38.4	46.2	49.9	50.1	51.8	38.1	30.8
AT Conf	27.4	40.5	46.9	47.3	48.1	28.5	23.8
CCAT		4.0	5.0	22.8	23.3	5.2	2.6	2.6
Table 1: Attack ablation study on SVHN. Comparison of our L∞ PGD-Conf attack with = 0.03
on the test set for different number of iterations T and configurations of momentum, backtracking
and initialization. As backtracking needs an additional forward pass per iteration, we compare
T = 200 with backtracking to T = 300 without. Attacks on adversarial training (AT) succeed
within a few iterations, but are more difficult against CCAT and require initialization at zero.
7
Under review as a conference paper at ICLR 2020
0.6
0.4
0.2
0.8
0.6
0.4
0.2
00	0.2	0.4	0.6	0.8
Test Confidence
00
0.4
0.3
∙gCCAT,Pp°w = 10 I
00	0.2	0.4	0.6	0.8	1
Adversarial Confidence
0.2	0.4	0.6	0.8	1
Adversarial Confidence
Figure 3: Confidence histograms on SVHN.
For AT (left) and CCAT (right) with ρpow = 10
(right), we show confidence histograms corre-
sponding to correctly classified test examples
(top) and successful adversarial examples (bot-
tom). We consider the worst-case adversarial ex-
amples across all tested L∞ attacks for = 0.03.
False Positive Rate (FPR)
Confidence Threshold τ
Figure 4: ROC and RErr curves on SVHN.
Left: ROC curves, i.e., FPR against TPR when
distinguishing correctly classified test examples
from successful adversarial examples by confi-
dence. Right: RErr against confidence threshold
τ. For evaluation, we choose τ in order to obtain
99%TPR. As described in the text, RErr sub-
sumes both Err and FPR. Curves based on worst-
case examples across all tested L∞ attacks.
---Normal -----AT ------CCAT, PpoW = 10
4.3	Ablation Study
Momentum and Backtracking: Fig. 2 illustrates the advantage of momentum and the proposed
backtracking scheme for PGD-Conf with T = 40 iterations on 10 test examples of SVHN. As
shown in Fig. 2 and Tab. 1, better objective values can be achieved within fewer iterations and
avoiding oscillation which is important at training time. However, also at test time, Tab. 1 shows
that attacking our CCAT model effectively requires up to T = 2000 iterations and zero initialization
so that RErr for τ @99%TPR stagnates. In contrast, PGD-Conf performs better against AT even
for smaller T and without momentum or backtracking. Thus, finding high-confidence adversarial
examples against CCAT is more difficult than for AT. Overall, this illustrates our immense effort
put into attacking our proposed defense with an adapted attack, novel optimization techniques, and
large number of iterations.
Evaluation Metrics: Fig. 4 shows ROC and RErr curves on SVHN, considering AT and CCAT with
ρpow = 10, i.e., using the power transition from Eq. (6). The ROC curves, and the corresponding
AUC value, quantify how well (successful) adversarial examples can be detected, i.e., distinguished
from (correctly classified) test examples. Note our conservative choice to use the confidence thresh-
old τ at 99%TPR, wrongly rejecting at most 1% correctly classified examples. We note that τ
depends only on correctly classified test examples, not adversarial examples, and RErr implicitly
includes FPR as well as Err. We focus on the confidence-thresholded variants of RErr and Err as
confidence thresholding will naturally also improve results for AT, resulting in a fair comparison,
i.e., AT is also allowed to benefit from our two-stage detection setting; for Err this can be seen in
Tab. 2. On SVHN and Cifar10, we found the power transition with ρpow = 10 from Eq. (6) to work
best. Up to ρpow = 10, performance regarding RErr for τ @99% TPR continuously improves and
after ρpow = 10 performance stagnates, as shown in detail in the appendix. On MNIST, interest-
ingly, exponential transition with ρexp = 7 performs best against L∞ attacks; we assume that the
	MNIST: Test Error		SVHN: Test Error		SVHN: Test Error	
	Standard	Detection	Standard	Detection	Standard	Detection
	τ = 0	T @99%TPR	τ = 0	τ @99%TPR	τ = 0	τ @99%TPR
	Err	Err	Err	Err	Err	Err
	in%	in%	in%	in%	in%	in%
Normal	04	0.1	36	26	83	74
AT	0.5	0	3.4	2.5	16.6	15.5
CCAT	0.3 (0.3)	0.1(01)	2.9	2.1	10.1	6.7
Table 2: Test errors on MNIST, SVHN and Cifar10. Err for τ = 0, i.e., standard evaluation, and
τ @99%TPR, i.e., detection evaluation, comparing normal training, AT and CCAT. Especially on
Cifar10, AT increases Err significantly, in both settings. CCAT, in contrast, is able to preserve the
Err of the normally trained model better.
8
Under review as a conference paper at ICLR 2020
MNIST: Main Results for Detection T@99%TPR (L∞, e = 0.30 during training, standard evaluation, i.e., T = 0, in Appendix B)										
	L∞, e = 0.30		L∞, e = 0.40		L2, e = 3		Li, e = 10.00		Lo, e = 15	
	(seen)		(unseen)		(unseen)		(unseen)		(unseen)	
	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%
-AΓ	0.97	1.0	0.20	100.0	0.73	81.3	0.93	5.2	0.99	2.5
CCAT, ρexp=7	0.99	7.7	0.94	40.0	1.00	1.4	0.96	14.7	0.99	7.8
CCAT, ρpow=10-	0.96	21.9	0.94	29.0	1.00	0.1	1.00	1.6	0.99	7.8
SVHN: Main Results for Detection T@99%TPR
(L∞, e = 0.03 during training, standard evaluation, i.e., T = 0, in Appendix B)
	L∞, 6 = 0.03~		L∞, 6 = 0.06		L2, 6 = 1		Li, 6 = 7.85~		Lo, 6 = 10	
	(seen)		(unseen)		(unseen)		(unseen)		(unseen)	
	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%
-AΓ	0.55	55.6	0.32	88.3	0.26	92.0	0.34	91.9	0.90	73.4
CCAT	0.70	38.5	0.70	46.0	0.91	18.5	0.89	20.8	1.00	2.7
CIFAR10: Main Results for Detection T@99%TPR
(L∞, e = 0.03 during training, standard evaluation, i.e., T = 0, in Appendix B)
	L∞, 6 = 0.03		L∞, 6 = 0.06		L2, 6 = 1		Li, 6 = 7.85		Lo, 6 = 10	
	(seen)		(unseen)		(unseen)		(unseen)		(unseen)	
	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%
-AΓ	0.64	62.3	0.35	93.6	0.59	73.9	0.61	68.0	0.80	74.1
CCAT	0.60	67.9	0.43	91.5	0.77	46.2	0.78	45.2	0.98	20.9
Table 3: Main results on MNIST, SVHN and Cifar10. Comparison of AT and CCAT on MNIST
(top), SVHN (middle) and Cifar10 (bottom). Per threat model, i.e., for L∞, L2, L1 and L0 at-
tacks, we report worst-case results across all tested attacks; the used values are reported in the
corresponding columns. During training, L∞ attacks with = 0.3 on MNIST and = 0.03 on
SVHN/Cifar10 were used; adversarial examples from the remaining threat models were not en-
countered during training. We report ROC AUC as well as confidence-thresholded Err and RErr for
τ @99%TPR. CCAT is competitive with AT on the L∞ attacks seen during training, but generalizes
significantly better to previously unseen attacks.
slight bias towards the true label preserved in the exponential transition helps. However, even on
MNIST, the power transition improves robustness against previously unseen attacks, e.g., L2, L1 or
L0 attacks, such that we include results for both.
4.4	Results
In Tab. 3, we report the main results of our paper, namely robustness across all evaluated L∞, L2, L1
and L0 attacks; for L∞ we include results for the used during training and an increased . While
on MNIST, CCAT incurs a drop of roughly 6% in RErr, and on Cifar10 a drop of roughly 5%, both
against L∞ with the same as during training, it significantly outperforms AT on SVHN, by more
than 16%. On SVHN and Cifar10, Err is additionally improved - on Cifar10, the improvement is
particularly significant with roughly 6%. For L∞ attacks with larger , robustness of AT degrades
significantly, while CCAT is able to preserve robustness to some extent, especially on SVHN. Only
on Cifar10, RErr degrades similarly to AT. In terms of generalization to previously unseen threat
models, i.e., L2, L1 and L0 attacks, CCAT outperforms AT significantly. We note that on all
datasets, the considered -balls for L2, L1 and L0 are not contained in the L∞-ball used during
training. Overall, robustness of AT degrades significantly under threat models not seen during
training, while CCAT generalizes to new threat models much better.
In Tab. 4 we also report results for detecting uniform noise and adversarial uniform noise (i.e., distal
adversarial examples) based on their confidence. For the latter, we sample uniform noise and subse-
quently use PGD-Conf to maximize the confidence (without considering any true label in Eq. (4)) in
the L∞ -ball around the noise point. Although ROC AUC values are high on uniform noise, an FPR
of 40% or higher shows that normal training and AT assigns high confidence to uniform noise. When
9
Under review as a conference paper at ICLR 2020
		MNIST L = 0.3)		SVHN (β = 0.03)		CIFAR10 (β =。03)	
		Detection T @99%TPR		Detection T @99%TPR		Detection T @99%TPR	
		ROC AUC	FPR in%	ROC AUC	FPR in%	ROC AUC	FPR in%
P U E X	Normal AT CCAT		-034- 0.99 1.00	100.0 44.5 0.0	-0.95- 1.00 1.00	-728- 0.9 0.0	-083- 0.60 1.00	-871 100.0 0.0
J S ∙ι-I Q	Normal AT CCAT		-0:34- 0.63 1.00	100.0 100.0 0.0	-043- 0.89 1.00	100.0 98.8 0.0	-0:49- 0.31 1.00	-1000- 100.0 0.0
Table 4: Results for noise and distal adversarial examples on MNIST, SVHN and Cifar10.
Robustness against uniform noise (“Rand”) and distal adversarial examples (“Dist”), i.e., high-
confidence adversarial computed on uniform noise using our L∞ PGD-Conf attack by considering
Eq. (4) without any true label; we use = 0.3 on MNIST, = 0.03 on SVHN/Cifar10. We report
ROC AUC and FPR for a confidence threshold of τ @99%TPR. In contrast to normal training and
AT, CCAT is able to reliably detect all (adversarial) noise examples.
maximizing confidence on uniform noise, FPR approaches 100% on all datasets. In contrast CCAT
allows to separate these attacks perfectly from test examples. Furthermore, Tab. 5 presents results
on MNIST-C and Cifar10-C (Mu & Gilmer, 2019; Hendrycks & Dietterich, 2019), containing the
MNIST and Cifar10 test examples with various corruptions (e.g., spatial transformations, bright-
ness/contrast changes, noise, etc.). Again, we consider our two-stage detection approach, reporting
ROC AUC for detecting corrupted examples based on confidence and confidence-thresholded Err
on the corrupted examples. The results are averaged over all available corruptions (15 on MNIST,
19 on Cifar10). As can be seen, allowing confidence thresholding is beneficial in this scenario for
AT and CCAT. However, CCAT, outperforms normal training and AT significantly.
5	Conclusion
We proposed confidence-calibrated adversarial training (CCAT) which addresses two problems
of standard adversarial training: an apparent accuracy-robustness problem, i.e., adversarial training
tends to worsen accuracy on difficult datasets; and, more importantly, the lack of “generalizable”
robustness, i.e., obtaining robust models against threat models not encountered during training.
CCAT achieves comparable or better robustness against the threat model at training time, i.e., L∞-
constrained adversarial examples, with better accuracy on Cifar10. However, in strong contrast to
adversarial training, CCAT generalizes to stronger L∞ attacks as well as L2, L1, L0 attacks and
distal adversarial examples by allowing detection based on confidence-thresholding. For our exper-
iments, we further introduced a generalized robust test error adapted to our detection-setting and
used a worst-case evaluation methodology, both of independent interest to the community.
	Corrupted MNIST: Detection T @99%TPR (training: L∞, e = 0.30)		Corrupted CIFAR10: Detection T @99%TPR (training: L∞, e = 0.03)	
	Average Results over 15 Corruptions		Average Results over 15 Corruptions	
	ROC AUC	Err in%	ROC AUC	Err in%
Normal AT CCAT		-075- 0.80 0.95	328 12.6 4.0	-057- 0.53 0.66	123 16.2 8.5
Table 5: Results on Corrupted MNIST and Cifar10. Performance on corrupted examples (spatial
transformations, brightness/contrast changes, noise etc.). We report ROC AUC and confidence-
thresholded Err on corrupted examples for τ @99%TPR. Compared to normal training and AT,
CCAT detects better corrupted examples (higher ROC AUC) and its test error Err on non-rejected
examples is lower.
10
Under review as a conference paper at ICLR 2020
References
Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision:
A survey. arXiv.org, abs/1801.00553, 2018.
Rima Alaifari, Giovanni S. Alberti, and Tandri Gauksson. Adef: an iterative algorithm to construct
adversarial deformations. arXiv.org, abs/1804.07729, 2018.
Laurent Amsaleg, James Bailey, Dominique Barbe, Sarah M. Erfani, Michael E. Houle, Vinh
Nguyen, and Milos Radovanovic. The vulnerability of learning to adversarial perturbation in-
creases with intrinsic dimensionality. In WIFS, 2017.
Maksym Andriushchenko. Provable adversarial defenses for boosting. Master’s thesis, Saarland
University, August 2019.
Anish Athalye and Nicholas Carlini. On the robustness of the CVPR 2018 white-box adversarial
example defenses. arXiv.org, abs/1804.03286, 2018.
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv.org, abs/1802.00420, 2018a.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In ICML,pp. 284-293, 2018b.
Marco Barreno, Blaine Nelson, Russell Sears, Anthony D. Joseph, and J. D. Tygar. Can machine
learning be secure? In AsiaCCS, 2006.
Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Dimensionality reduction as a defense
against evasion attacks on machine learning classifiers. arXiv.org, abs/1704.02654, 2017a.
Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Exploring the space of black-box attacks
on deep neural networks. arXiv.org, abs/1712.09491, 2017b.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine
learning. arXiv.org, abs/1712.03141, 2018a.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine
learning. Pattern Recognition, 84:317-331, 2018b.
Wieland Brendel and Matthias Bethge. Comment on ”biologically inspired protection of deep net-
works from adversarial attacks”. arXiv.org, abs/1704.01547, 2017.
Tom B. Brown, Dandelion Mane, AUrko Roy, Martin Abadi, and Justin Gilmer. Adversarial patch.
arXiv.org, abs/1712.09665, 2017.
Qi-Zhi Cai, Chang Liu, and Dawn Song. Curriculum adversarial training. In IJCAI, pp. 3740-3747,
2018.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. arXiv.org, abs/1705.07263, 2017a.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In SP,
2017b.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C. Duchi. Unlabeled
data improves adversarial robustness. arXiv.org, abs/1905.13736, 2019.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
AISec, 2017.
Francesco Croce and Matthias Hein. Sparse and imperceivable adversarial attacks. arXiv.org,
abs/1909.05040, 2019.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boost-
ing adversarial attacks with momentum. In CVPR, 2018.
11
Under review as a conference paper at ICLR 2020
John C. Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto
the l1-ball for learning in high dimensions. In ICML, 2008.
Beranger Dumont, Simona Maggio, and Pablo Montalvo. Robustness of rotation-equivariant net-
works to adversarial perturbations. arXiv.org, abs/1802.06627, 2018.
Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a
translation suffice: Fooling CNNs with simple transformations. arXiv.org, abs/1712.02779, 2017.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. Ex-
ploring the landscape of spatial robustness. In ICML, 2019.
Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv.org, abs/1703.00410, 2017.
Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. Adversarial and clean data are not twins. arXiv.org,
abs/1704.04960, 2017.
Ian Goodfellow, Yao Qin, and David Berthelot. Evaluation methodology for attacks against
confidence thresholding models, 2019. URL https://openreview.net/forum?id=
H1g0piA9tQ.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv.org, abs/1412.6572, 2014.
Edward Grefenstette, Robert Stanforth, Brendan O’Donoghue, Jonathan Uesato, Grzegorz Swirszcz,
and Pushmeet Kohli. Strength in numbers: Trading-off robustness and computation via
adversarially-trained ensembles. arXiv.org, abs/1811.09300, 2018.
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On
the (statistical) detection of adversarial examples. arXiv.org, abs/1702.06280, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In ICCV, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
confidence predictions far away from the training data and how to mitigate the problem. CVPR,
2019.
Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. arXiv.org, abs/1903.12261, 2019.
Dan Hendrycks and Kevin Gimpel. Early methods for detecting adversarial images. In ICLR, 2017.
RUitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvari. Learning with a strong adver-
sary. arXiv.org, abs/1511.03034, 2015.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. In ICML, 2018a.
Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial
attacks with bandits and priors. arXiv.org, abs/1807.07978, 2018b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, 2015.
Jom-Henrik Jacobsen, Jens Behrmann, Nicholas Carlini, Florian Tramer, and Nicolas Papernot.
Exploiting excessive invariance caused by norm-bounded adversarial robustness. arXiv.org,
abs/1903.10484, 2019a.
Jorn-Henrik Jacobsen, Jens Behrmann, Richard S. Zemel, and Matthias Bethge. Excessive invari-
ance causes adversarial vulnerability. In ICLR, 2019b.
12
Under review as a conference paper at ICLR 2020
Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, and Jacob Steinhardt. Testing robustness against
unforeseen adversaries. arXiv.org, abs/1908.08016, 2019.
Danny Karmon, Daniel Zoran, and Yoav Goldberg. Lavan: Localized and visible adversarial noise.
In ICML, 2018.
J. Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv.org, abs/1711.00851, 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv.org, abs/1607.02533, 2016.
Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial train-
ing: Achieving robust neural networks without sacrificing too much accuracy. arXiv.org,
abs/1906.06784, 2019.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proc. ofthe IEEE, 86(11):2278-2324, 1998.
Hyeungill Lee, Sungyeob Han, and Jungwoo Lee. Generative adversarial trainer: Defense to adver-
sarial perturbations with GAN. arXiv.org, abs/1705.03387, 2017.
Mark Lee and Zico Kolter. On physical adversarial patches for object detection. arXiv.org,
abs/1906.11897, 2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. On norm-agnostic robustness of ad-
versarial training. arXiv.org, abs/1905.06455, 2019a.
Juncheng Li, Frank R. Schmidt, and J. Zico Kolter. Adversarial camera stickers: A physical camera-
based attack on deep learning systems. In ICML, 2019b.
Xin Li and Fuxin Li. Adversarial examples detection in deep networks with convolutional filter
statistics. In ICCV, pp. 5775-5783, 2017.
Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against
adversarial attacks using high-level representation guided denoiser. In CVPR, 2018.
Xin Liu, Huanrui Yang, Linghao Song, Hai Li, and Yiran Chen. Dpatch: Attacking object detectors
with adversarial patches. arXiv.org, abs/1806.02299, 2018.
Xuanqing Liu, Yao Li, Chongruo Wu, and Cho-Jui Hsieh. Adv-bnn: Improved adversarial defense
through robust bayesian neural network. In ICLR, 2019.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial exam-
ples and black-box attacks. arXiv.org, abs/1611.02770, 2016.
Xingjun Ma, Bo Li, Yisen Wang adn Sarah M. Erfani, Sudanthi Wijewickrema, Michael E. Houle,
Grant Schoenebeck, Dawn Song, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. arXiv.org, abs/1801.02613, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv.org, abs/1706.06083, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ICLR, 2018.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial
perturbations. arXiv.org, abs/1702.04267, 2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
smoothing with virtual adversarial training. ICLR, 2016.
13
Under review as a conference paper at ICLR 2020
Norman Mu and Justing Gilmer. Mnist-c: A robustness benchmark for computer vision. ICML
Workshops, 2019.
Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial attacks on deep
neural networks. In CVPR Workshops, 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NeurIPS, 2011.
Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial exam-
ples. In NeurIPS,pp. 4584-4594, 2018.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NeurIPS Workshops, 2017.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine learning in Python. JMLR, 12:2825-2830, 2011.
Julien PerolaL Mateusz Malinowski, Bilal Piot, and Olivier PietqUin. Playing the game of universal
adversarial perturbations. CoRR, abs/1809.07802, 2018.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, and Percy Liang. Adversarial
training can hurt generalization. arXiv.org, abs/1906.06032, 2019.
Bita Darvish Rouhani, Mohammad Samragh, Tara Javidi, and Farinaz Koushanfar. Towards safe
deep learning: Unsupervised defense against generic adversarial attacks, 2018. URL https:
//openreview.net/forum?id=HyI6s40a-.
Sayantan Sarkar, Ankan Bansal, Upal Mahbub, and Rama Chellappa. UPSET and ANGRI : Break-
ing high performance image classifiers. arXiv.org, abs/1707.01159, 2017.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad-
versarially robust generalization requires more data. CoRR, arXiv.org, 2018.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially
robust neural network model on MNIST. In ICLR, 2019.
Ali Shafahi, Mahyar Najibi, Zheng Xu, John P. Dickerson, Larry S. Davis, and Tom Goldstein.
Universal adversarial training. arXiv.org, abs/1811.11304, 2018.
Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of supervised models through robust optimization. Neurocomputing, 307:195-204,
2018.
Yash Sharma and Pin-Yu Chen. Attacking the madry defense model with l1-based adversarial ex-
amples. arXiv.org, abs/1710.10733, 2017.
Aman Sinha, Hongseok Namkoong, and John C. Duchi. Certifiable distributional robustness with
principled adversarial training. ICLR, 2018.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Generative adversarial examples.
arXiv.org, abs/1805.07894, 2018.
David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and general-
ization. CVPR, 2019.
Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep
neural networks. arXiv.org, abs/1710.08864, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv.org, abs/1312.6199, 2013.
14
Under review as a conference paper at ICLR 2020
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations.
arXiv.org, abs/1904.13000, 2019.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick D. McDaniel. Ensemble
adversarial training: Attacks and defenses. arXiv.org, abs/1705.07204, 2017.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. arXiv.org, abs/1805.12152, 2018.
Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, and
Pushmeet Kohli. Are labels required for improving adversarial robustness?	arXiv.org,
abs/1905.13725, 2019.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In ICML, 2019.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially trans-
formed adversarial examples. arXiv.org, abs/1801.02612, 2018.
Cihang Xie, Zhishuai Zhang, Jianyu Wang, Yuyin Zhou, Zhou Ren, and Alan L. Yuille. Improving
transferability of adversarial examples with input diversity. arXiv.org, abs/1803.06978, 2018.
Nanyang Ye and Zhanxing Zhu. Bayesian adversarial learning. In NeurIPS, 2018.
Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and Xiaolin Li. Adversarial examples:
Attacks and defenses for deep learning. arXiv.org, abs/1712.07107, 2017.
Michal Zajac, Konrad Zolna, Negar Rostamzadeh, and Pedro O. Pinheiro. Adversarial framing for
image and video classification. In AAAI Workshops, 2019.
Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. Efficient defenses against adver-
sarial attacks. In AISec, 2017.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.
Xiao Zhang and David Evans. Cost-sensitive robustness against adversarial examples. arXiv.org,
abs/1810.09225, 2018.
15
Under review as a conference paper at ICLR 2020
A	Proof of Proposition 1
Proposition 2. We consider a classification problem with two points x = 0 and x = in R with
deterministic labels, that is p(y = 2|x = 0) = 1 and p(y = 1|x = ) = 1 and the problem is
fully determined by the probability p0 = p(x = 0) as p(x = ) = 1 - p0. The Bayes error of this
classification problem is zero. The predicted probability distribution over the classes is p(y∣x)=
eg1(Xg[Xg2(x), where g : Rd → R2 and we assume that thefunction λ : R+ → [0,1] is monotonically
decreasing and λ(0) = 1. The Bayes optimal classifier for the cross-entropy loss of
•	adversarial training on 100% adversarial examples, cf. Eq. (1), yields an error of
min{p0, 1 - p0}.
•	adversarial training with 50% adversarial and 50% clean examples per batch, cf. Eq. (3),
yields an error of min{p0 , 1 - p0}.
•	CCAT on 50% clean and 50% adversarial examples, cf. Alg. 1, yields zero error if λ() <
min n 4, 1-p0 O.
1-p0 , p0
Proof. First, we stress that we are dealing with three different probability distributions over the
labels: the true one p(y∣x), the imposed one during training p(y∣x) and the predicted one p(y∣x).
We also note thatP depends on λ through Eq. (5), and λ itselfis afunction ofthe norm ∣∣δk through
Eq. (6); here, this dependence is made explicit by writing p(λ)(y∣x). This makes the expressionsfor
the expected loss of CCAT slightly more complicated. We first derive the Bayes optimal classifier
and its loss for CCAT. We introduce
a = g1(0) - g2(0),	b = g1() - g2().	(8)
and express the logarithm of the predicted probabilities (confidences) of class 1 and 2 in terms of
these quantities.
Togp(y = 2∣x = X) = Tog(egιw+‰)=log(1+ eg1(xj2(x)) = {log[[：)	ifX =0.
eg1 (x)	(x)- (x)	log(1 + e-a )	ifx = 0
Togp(y =1∣x = X) = Tog lgι(x) + eg2(x) ) =log (1 + e	) = Uog (1 + e-b)	if X = e
For our confidence-calibrated adversarial training one first has to solve
δ*(j) = argmax maxp(y = k ∣ X + δ).
kδk∞ ≤ k6=j
We get with the expressions above (note that we have a binary classification problem):
δ^(1)	=argmax p(y = 2∣0 + δ)= kδk∞≤		if a > b else.
δ[2)	=argmax p(y = 1∣0 + δ)= kδk∞≤	{0	if a > b else.
S：(1) =	:argmax p(y = 2∣e + δ)= < kδk∞≤	-	if a > b else.
S：(2) =	:argmax p(y = 1∣e + δ)= < kδk∞≤	0	if a > b else.
Note that in CCAT for 50% of the batch we use the standard cross-entropy loss and for the other
50% we use
2
L(Py(λ(Mx(V)I))(X),p(X)) = — XPy(λ(Mx(U)I))(y = j ∣X = X)Iog (p(y = j ∣X = X + 蓝Cj))) ∙
j=1
16
Under review as a conference paper at ICLR 2020
The corresponding expected loss is then given by
EhL(Py (λ(Mx⑻ H))(X),p(X))i = EhEhL(Py (λ(Mx⑻ II))(X),p(X))Ixii
=P(X = O)EhL(Py(λ(k笳(y)k))(0),p(O))Ix = 0i + p(x = e)EhL(pγ(λ(kδ0(y)k))(e),p(e))lx = e],
where P(X = 0) = P0 and P(X = ) = 1-P(X = 0) = 1-P0. With the true conditional probabilities
P(y = s I X = X) we get
2
E[L(PyC(S))(X),p(X))Ix = X] = XP(y = S | X = x)L(p>s(λ(kδx(S)Il))(X),p(X))
s=1
22
=—XP(y = S I X = x) XPs(kλ(δKS)II))(y = j I X = x) log(P(y = j I X = X + δx(s)))
s=1	j=1
For our problem it holds with P(y = 2 I X = O) = P(y = 1 I X = e) = 1 (by assumption). Thus,
2
E[L(Py(λ(δ))(χ),P(χ))Iχ = 0] = — XP(λ(kδ"2)k))(y = j I X = 0)log (P(y = j I X = X + δ0(2)))
j=1
2
E[L(Py(λ(δ))(x),P(x))Ix = e] = — XPι(λ(∣虞(1))))(y = j I X = e) log (P(y = j I X = X + S：(1)))
j=1
As ∣∣δx(y)∣ is either 0 or ∣∣e∣ and λ(0) = 1 we use in the following for SimpIiCity the notation
λ = λ(IeI). Moreover, note that
-∕∖m	∙∣	、	∕λ + (1Kλ)	ify =j,
Py (λ)(y = j IX = X) = I(IK) K else
where K is the number ofclasses. Thus, K = 2 in our example and we note that λ + (1-λ) = 1++λ.
With this we can write the total loss (remember that we have half normal cross-entropy loss and half
the loss for the adversarial part with the modified “labels”) as
L(a, b) =P0 [log(1 + ea)la≥b + l0<b((I + K log(1 + eb) + (I「K log(1 + e-b))]
+ (I — p0) h log(1 + e-b) la≥b + la<b (-~2~) log(1 + e-a) + ^~~2^^I log(1 + ea))i
+ log(1 + ea)P0 + log(1 + e-b)(1 — P0),
where We have omitted a global factor 2 for better readability (the last row is the cross-entropy loss).
We distinguish two sets in the optimization. First we consider the case a ≥ b. Then it is easy to see
that in order to minimize the loss we have a = b.
ea	e-a
daL = 2	aP0 -	-a (I - P0)
1 + ea	1 + e-a
This yields ea = 1-p0 or a = log (1--p0) and the minimum for a ≥ b is attained on the boundary
of the domain of a ≤ b. The other case is a ≤ b. We get
(1 +K) —e-a	(1 — K) ea	ea
daL = Lw-a + LTT^ ] (I- P0)+ p0 TT^
(1 +K) eb	(1 — K) —eb	—e-b
dbL =	~τ+^b +	~τ+^-b ]p0 + (I- p0)τ+^-b
This yields the solution
a* = log
⅞λ (1 - p0)
P0 + 1-λ (1 - P0)
力*	1-λp0 + (1 - p0)
b =log(一马而—
17
Under review as a conference paper at ICLR 2020
It is straightforward to check that a* < b* for all 0 < po < 1, indeed we have
1++λ (I - PO)	= 1++λ (I - PO) = 1 - P0 - (I-λ (I - PO)	1-λP0 + (I - PO)
+ +	1-λ (1 -	)∩)	p∩	1 + λ	+	1-λ	p∩	1 + λ	+	1-λ	<	1 + λp∩
PO + 2	PO	PO 2 + 2	PO 2 + 2	2 PO
if 0 < PO < 1 and note that λ < 1 by assumption. We have a* < 0 and thus g2 (0) > g1(0) (Bayes
optimal decision for x = 0) if
1 › 片 λ,
and b* > 0 and thus g1() > g2() (Bayes optimal decision for x = ) if
1 > ⅛λ.
Thus we recover the Bayes classifier if
1 - PO	PO
λ< mint 丁，E r
Now we consider the approach by Madry et al. (2018) with 100% adversarial training. The expected
loss can be written as
e[max∣∣δk≤∞ L(y, f(x + δ)] = e[e[max∣∣δk≤∞ L(y,f(x + δ)∣x]]
=P(X = 0)p(y = 2|x = x) max {— log (p(y = 2|x = 0)), - log (p(y	=	2|x =	e))}
+ (1 — p(x = 0))p(y = 1|x = x) max { — log (P(y = 1|x = 0)), —	log (P(y	= 1|x = e))}
=P(X = 0) max { — log (p(y = 2|x = 0)), — log (p(y = 2|x = e))}
+ (1 — p(x = 0)) max {- log (P(y = 1|x = 0)), — log(P(y = 1|x	=	e)}
This yields in terms of the parameters a, b the expected loss:
L(a, b) =max n log(1 + ea), log(1 + eb)oPO + max log(1 + e-a), log(1 + e-b) (1 — PO)
The expected loss is minimized if a = b as then both maxima are minimal. This results in the
expected loss
L(a) = log(1 + ea)PO + log(1 + e-a)(1 - PO).
The critical point is attained at a* = b*
log (1-p0). This is never Bayes optimal for all 0 < po <
1. Note that b* = a* > 0 if PO < 1 - PO and thus the error is given by PO, similar a* = b* < 0 if
1 - PO < PO and thus the error is given by 1 - PO. We get an error of min{PO, 1 - PO} of the Bayes
optimal solution of 100% adversarial training.
Next we consider 50% adversarial plus 50% clean training. The expected loss
E maxkδk≤∞L(y,f(x+δ) +E L(y,f(x+δ) ,
can be written as
L(a, b) = max log(1 + ea), log(1 + eb) PO
+ max log(1 + e-a), log(1 + e-b) (1 - PO)
+ log(1 + ea)PO + log(1 + e-b)(1 - PO)
We make a case distinction. If a ≥ b, then the loss reduces to
L(a, b) = log(1 + ea)PO + log(1 + e-b)(1 —PO)
+ log(1 + ea)PO + log(1 + e-b)(1 — PO)
≥ L(a, a)
= 2 log(1 + ea)PO + 2 log(1 + e-a)(1 —PO)
18
Under review as a conference paper at ICLR 2020
MNIST: Attack Ablation with RErr for T @99%TPR (L∞ attack with e = 0.3)									
Optimization	momentum+backtrack					momentum		—	
Initialization	zero				rand	zero		zero	
Iterations T	~0O~	200	2000	4000	4000	^^60-	300	^^60-	300
-AΓ ATConf CCAT		~4A~ 0.8 0.6	0.4 0.8 3.3	0.4 0.8 4.9	0.3 0.8 6.8	0.6 1.1 5.5	-04- 1.0 0.9	0.6 1.1 3.8	-04- 1.0 0.0	0.4 1.0 0.1
	SVHN: Attack Ablation with RErr for τ @99%TPR									
	(L∞ attack with e				=0.03)						
Optimization		momentum+backtrack					momentum			
Initialization		zero				rand	zero		zero	
Iterations T		40	200	2000	4000	4000	60	300	60	300
-AΓ		38.4	46.2	49.9	50.1	51.8	37.7	38.1	29.9	30.8
ATConf		27.4	40.5	46.9	47.3	48.1	27.1	28.5	21.1	23.8
CCAT		4.0	5.0	22.8	23.3	5.2	2.6	2.6	2.6	2.6
	Cifar10: Attack Ablation with RErr for T@99%TPR									
	(L∞ attack with e				=0.03)						
Optimization		momentum+backtrack					momentum			
Initialization		zero				rand	zero		zero	
Iterations T		40	200	2000	4000	4000	60	300	60	300
-AΓ		60.9	60.8	60.8	60.8	60.9	60.9	60.9	57.4	57.6
ATConf		60.4	60.6	60.5	60.5	60.9	60.4	60.6	56.2	56.6
CCAT		14.8	16.2	40.2	41.3	34.9	7.2	7.2	7.2	7.2
Table 6: Detailed attack ablation studies on MNIST, SVHN and Cifar10. Complementary to
Tab. 1, we compare our L∞ PGD-Conf, as introduced in Sec. 4.1, attack with T iterations and dif-
ferent combinations of momentum, backtracking and initialization on all three datasets. We consider
AT, AT trained with PGD-Conf, and CCAT; we report RErr for confidence threshold τ @99%TPR.
As backtracking requires an additional forward pass per iteration, we use T = 60 and T = 300 for
attacks without backtracking to be comparable to attacks with T = 40 and T = 200 with backtrack-
ing.
Solving for the critical point yields a*
yields the loss
log (1-p0) = b*. Next We consider the set a ≤ b. This
L(a, b) = log(1 + eb)p0 + log(1 + e-a)(1 - p0)
+ log(1 + ea)p0 + log(1 + e-b)(1 -p0)
Solving for the critical point yields a* = log (1-p0) = b* which fulfills a ≤ b. Actually, it
coincides With the solution found already for 100% adversarial training. One does not recover the
Bayes classifier for any 0 < p0 < 1.
Moreover, we note that
a* = b* = [> 0	ifp0< 1,
< 0	if Po > 2.
Thus, we classify X = 0 correctly, if po > 1 and X = e correctly if po < 2. Thus the error is given
by min{po, 1 - po}.	□
B Experiments
We give additional details on our experimental setup, specifically regarding attacks, training and the
used evaluation metrics. Afterwards, we include additional experimental results, including ablation
studies, results for 98% true positive rate (TPR), results per attack, results per corruption on MNIST-
C (Mu & Gilmer, 2019) and Cifar10-C (Hendrycks & Dietterich, 2019) and a comparison with
(Pang et al., 2018).
19
Under review as a conference paper at ICLR 2020
Algorithm 2 Pseudo-code for the used projected gradient descent (PGD) procedure to maximize
Eq. (9) or Eq. (10) subject to the constraints Xi = Xi + δi ∈ [0,1] and ∣∣δ∣∣∞ ≤ e; in practice,
the procedure is applied on batches of inputs. The algorithm is also easily adapted to work with a
L2-norm; only the projections on line 6 and 24 needs to be adapted.
input: example X with label y
input: number of iterations T
input: learning rate γ , momentum β , learning rate factor α
input: initial δ(0), e.g., Eq. (11) or δ(0) = 0
1:	v := 0 {best objective achieved}
2:	X := x + δ(0) {best adversarial example}
3:	g(-1) := 0 {accumulated gradients}
4:	for t = 0, . . . , T do
5:	{projection onto L∞ e-ball and on [0, 1]:}
6:	clip δi(t) to [-e, e]
7:	clip Xi + δi(t) to [0, 1]
8:	{forward and backward pass to get objective and gradient:}
9:	v(t) := F(X + δ(t), y)
10:	g(t) ：= sign (Vδ(t)F(x + δ(t),y))
11:	{keep track of adversarial example resulting in best objective:}
12:	if v(t) > v then
13:	v	:=	v(t)
14:	X	:=	X + δ㈤
15:	end	if
16:	{iteration T is only meant to check whether last update improved objective:}
17:	if t = T then
18:	break
19:	end if
20:	{integrate momentum term:}
21:	g(t) := βg(t-1) + (1 - β)g(t)
22:	{“try” the update step and see if objective increases:}
23:	δ(t) := δ(t) + γg(t)
24:	clip $(t) to [-e, e]
25:	clip Xi + $(t) to [0,1]
26:	V⑴:=F(x + $⑴,y)
27:	{only keep the update if the objective increased; otherwise decrease learning rate:}
28:	if v(t) ≥ v(t)then
29:	δ(t+1) := δ(t)
30:	else
31:	Y	:=	γ/α
32:	end	if
33:	end for
34:	return X, V
B.1 Attacks
Complementary to the description of the projected gradient descent (PGD) attack by Madry et al.
(2018) and our adapted attack, we provide a detailed algorithm in Alg. 2. We note that the objective
maximized in (Madry et al., 2018) is
F(X + δ, y) = L(f (X + δ; w), y)	(9)
where L denotes the cross-entropy loss, f (∙; W) denotes the model and (x, y) is an input-label pair
from the test set. Our adapted attack, in contrast, maximizes
F(X + δ, y) = maxk6=y fk(X + δ; w).	(10)
Note that the maximum over labels, i.e., maxk6=y is explicitly computed during optimization; this
means that in contrast to (Goodfellow et al., 2019), we do not run K targeted attack and subse-
20
Under review as a conference paper at ICLR 2020
MNIST (L∞ attack with = 0.3)
Cifar10 (L∞ attack with = 0.03)
1
0.5
0
0.4
0.2
0
gCCAT”exP^7
0.4
0.6
0.4
kCCAT,Pp°W
00	0.2	0.4	0.6	0.8	1
10
0.1
CCAT, Pexp = 7
0	0.2	0.4	0.6	0.8	1
Test Confidence
0 05
Adversarial Confidence
0	0.2	0.4	0.6	0.8	1
Adversarial Confidence
0.2
0.2
0
0	0.2	0.4	0.6	0.8	1
Figure 5: Confidence histograms on MNIST and Cifar10. As in Fig. 3, we show histograms
of confidences on correctly classified test examples (top) and on successful adversarial examples
(bottom) for both AT and CCAT. Note that on AT, the number of successful adversarial examples is
usually lower than on CCAT, i.e., reflects the RErr for τ = 0 in Tab. 3; for CCAT in contrast, nearly
all adversarial examples are successful, while only a part has high confidence. Histograms obtained
for the worst-case adversarial examples across all tested L∞ attacks.
i
quently take the maximum-confidence one, where K is the number of classes. We denote these two
variants as PGD-CE and PGD-Conf, respectively. Deviating from (Madry et al., 2018), we initial-
ize δ uniformly over directions and norm (instead of uniform initialization over the volume of the
-ball):
δ0
δ = Ue而 ,δ ~N(0,I),u ~ U(0, I)	(II)
kδ0 k∞
where δ0 is sampled from a standard Gaussian and u ∈ [0, 1] from a uniform distribution. We
also consider zero initialization, i.e., δ = 0. For random initialization we always consider multiple
attempts, 10 for PGD-Conf and 50 for PGD-CE, with zero initialization, we use only 1 attempt.
Both PGD-CE and PGD-Conf can also be applied using the L2, L1 and L0 norms following the
description above. Then, gradient normalization, i.e., the signum in Line 10 in Alg. 2 for the L∞
norm, the projection, and the the initialization need to be adapted. For the L2 norm, the gradient
is normalized by dividing by the L2 norm; for the L1 norm only the 1% largest values (in absolute
terms) of the gradient are kept and normalized by their L1 norm; and for the L0 norm, the gradient
is normalized by the L1 norm. Projection is easily implemented for the L2 norm, while we follow the
algorithm of Duchi et al. (2008) for the L1 project; for the L0 projection, only the e-largest values
are kept. Similarly, initialization for L2 and L1 are simple by randomly choosing a direction (as in
Eq. (11)) and then normalizing by their norm. For L0, we randomly choose pixels with probability
2/3e/(H W D) and set them to a uniformly random values u ∈ [0, 1], where H × W × D is the image
size. In experiments, we found that tuning the learning rate for PGD with L1 and L0 constraints
(independent of the objective, i.e., Eq. (9) or Eq. (10)) is much more difficult. Additionally, PGD
using the L0 norm seems to get easily stuck in sub-optimal local optima.
Alg. 2 also gives more details on the employed momentum and backtracking scheme. These two
“tricks” add two additional hyper-parameters to the number of iterations T and the learning rate γ,
namely the momentum parameter β and the learning rate factor α. After each iteration, the computed
update, already including the momentum term, is only applied if this improves the objective. This is
checked through an additional forward pass. If not, the learning rate is divided by α, and the update
is rejected. Alg. 2 includes this scheme as an algorithm for an individual test example x with label y
for brevity; however, extending it to work on batches, which is used in our paper, is straight-forward.
In practice, for PGD-CE, with T = 200 iterations, we use β = 0.9 and α = 1.25; for PGD-Conf,
with T = 2000 iterations, we use β = 0.9 and α = 1.1.
We also give more details on the used black-box attacks. For random sampling, we apply Eq. (11)
T = 5000 times in order to maximize Eq. (10). We also implemented the black-box attack of Ilyas
et al. (2018a) using a population of 50 and variance of 0.1 for estimating the gradient in Line 10 of
Alg. 2; a detailed algorithm is provided in (Ilyas et al., 2018a). We use a learning rate of 0.001 (note
that the gradient is signed, as in (Madry et al., 2018)) and also integrated a momentum with β = 0.9
21
Under review as a conference paper at ICLR 2020
MNIST (L∞ attack with = 0.3)
Cifar10 (L∞ attack with = 0.03)
)RPT( etaR evitisoP eurT
False Positive Rate (FPR)
Confidence Threshold τ
)RPT( etaR evitisoP eurT
)rrER( rorrE tseT tsuboR
False Positive Rate (FPR)	Confidence Threshold τ
---Normal ------AT
---CCAT, peχp = 7
—Normal — AT — CCAT, ρp0w = 10
Figure 6: ROC and RErr curves on MNIST and Cifar10. ROC curves, i.e. FPR plotted against
TPR for all possible confidence thresholds τ, and RErr curves, i.e., RErr over confidence threshold
τ for AT and CCAT, including different ρ parameters. Worst-case adversarial examples across all
L∞ attacks were tested.
and backtracking as described in Alg. 2 with α = 1.1 and T = 2000 iterations. We use zero and
random initialization; in the latter case we allow 10 random retries. For the simple black-box attack
we follow the algorithmic description in (Narodytska & Kasiviswanathan, 2017) considering only
axis-aligned perturbations of size per pixel. We run the attack for T = 2000 iterations and allow
10 random retries. Finally, we use a variant of the cube attack proposed in (Andriushchenko, 2019).
We run the attack for T = 5000 iterations with a probability of change of 0.05. We emphasize
that, except for (Ilyas et al., 2018a), these attacks are not gradient-based and do not approximate the
gradient.
B.2	Training
As described in Sec. 4, we follow the ResNet-20 architecture by He et al. (2016) implemented
in PyTorch (Paszke et al., 2017). For training we use a batch size of 100 and train for 100 and
200 epochs on MNIST and SVHN/Cifar10, respectively: this holds for normal training, adversarial
training (AT) and confidence-calibrated adversarial training (CCAT). For the latter two, we use
PGD-CE and PGD Conf, respectively, forT = 40 iterations, momentum and backtracking (β = 0.9,
α = 1.5). For PGD-CE we use a learning rate of 0.05, 0.01 and 0.005 on MNIST, SVHN and
Cifar10. For PGD-Conf we use a learning rate of 0.05. For training, we use standard stochastic
gradient descent, starting with a learning rate of 0.1 on MNIST/SVHN and 0.075 on Cifar10. The
learning rate is multiplied by 0.95 after each epoch. We do not use weight decay; but the network
includes batch normalization (Ioffe & Szegedy, 2015). On SVHN and Cifar10, we use random
cropping, random flipping (only Cifar10) and contrast augmentation during training. We always
train on 50% clean and 50% adversarial examples per batch, i.e., each batch contains both clean and
adversarial examples which is important when using batch normalization.
B.3	Evaluation Metrics
For reproducibility and complementing the discussion in the main paper, we describe the used eval-
uation metrics and evaluation procedure in more detail. Adversarial examples are computed on the
first 1000 examples of the test set; the used confidence threshold is computed on the last 1000 exam-
ples of the test set; test errors are computed on all test examples minus the last 1000. As we consider
multiple attacks, and some attacks allow multiple random attempts, we always consider the worst
case adversarial example per test example and across all attacks/attempts; the worst-case is selected
based on confidence.
ROC AUC: To compute ROC curves, and the area under the curve, i.e., ROC AUC, we define
negatives as successful adversarial examples (on correctly classified test examples) and positives as
the corresponding correctly classified test examples. The ROC AUC as well as the curve itself can
easily be calculated using (Pedregosa et al., 2011). Practically, the generated curve could be used to
directly estimate a threshold corresponding to a pre-determined true positive rate (TPR). However,
this requires interpolation; after trying several constant interpolation schemes, we concluded that the
22
Under review as a conference paper at ICLR 2020
SVHN: AT with L∞ PGD-Conf, = 0.03
Perturbation Distance
1
1
0 -----=e^~~Illl
0	0.02	0.04	0.06
Perturbation Distance
0 I / I -------------------
0	0.02	0.04	0.06
Perturbation Distance
1
0.8
0.6
0.4
0.2
0
0	0.02	0.04	0.06
ssalC rep ecnedfino
Perturbation Distance
Perturbation Distance
ssalC rep ecnedfinoC
Perturbation Distance
ssalC rep ecnedfinoC
SVHN: CCAT, ρpow = 10 with L∞ PGD-Conf, = 0.03
ssalC rep ecnedfinoC ssal
0.
0.02	0.04	0.06
Perturbation Distance
0.6
1
0.8
.8 .6 .4 .2 0
0. 0. 0. 0.
ssalC rep ecnedfinoC
1
0.8
0.6
0.4
0.2
0
0	0.02	0.04	0.06
.6 .4 .2 0
0. 0. 0.
rep ecnedfinoC
Perturbation Distance
ssalC rep
Perturbation Distance
0.4
0.2
oizʃ τ I^L
0	0.02	0.04	0.06
Perturbation Distance
O
.8 .6 .4 .2 0
0. 0. 0. 0.
ssalC rep ecnedfinoC
ssalC rep
Perturbation Distance
Perturbation Distance
1
0.8
0.6
O
Ci
Ci
O
Ci
0.4
0.2
0
Perturbation Distance
Perturbation Distance
0.4
0.2
o∣ J— —
0	0.01 0.02 0.03 0.04
Perturbation Distance
Cifar10: AT with L∞ PGD-Conf,
= 0.03
1
0.8
0.6
Perturbation Distance
ssalC rep ecnedfino
1
0.8
0.6
0.4
0.2
0
rep ecnedfino
0.06
Perturbation Distance
Perturbation Distance
rep ecnedfino
ssalC rep ecnedfinoC
Cifar10: CCAT, ρpow = 10 with L∞ PGD-Conf, = 0.03
O
86420
0. 0. 0. 0.
ssalC rep ecnedfinoC
ssalC rep ecnedfinoC
0.02	0.04
Perturbation Distance
1
0.8
0.6
0.4
0.2
0
0	0.02	0.04	0.06
Perturbation Distance
ssalC rep ecnedfino
0.04	0.06
0	0.02
Perturbation Distance
ssalC rep ecnedfino
0.04	0.06
Perturbation Distance
ssalC rep ecnedfino
Perturbation Distance
Classes----- 1,----2,------3,-----4,------5,-----6,------7,-----8,------9,------10
Figure 7: Effect of confidence calibration on SVHN and Cifar10. Confidences for classes along
adversarial directions for AT and CCAT, ρpow = 10. Adversarial examples were computed using
PGD-Conf with T = 2000 iterations and γ = 0.001 and zero initialization. For both AT and CCAT,
we show the first ten examples of the test set on SVHN, and the first five examples of the test set on
Cifar10.
results are distorted significantly, especially for TPRs close to 100%. Thus, we followed a simpler
scheme as mentioned above: on a held out validation set of size 1000 (the last 1000 samples of the
test set), we sorted the corresponding confidences, and picked the confidence threshold in order to
obtain the desired TPR, e.g., 99%, on this set exactly.
23
Under review as a conference paper at ICLR 2020
MNIST (PGD-Conf, L∞ with = 0.3)
fi(i) = O.SO fi(x) = 1.00 fi(}f = 1.00 ʃ^(ɪ) = 0.41 fy(r) = 1.∞ fi(i) = 0.43 j⅛(i) = 0.O> ∕⅛(j) = QM fi(x) = 1.00 fi(i) = 1.∞ ⅛(j∙> = O.93 ⅛(j) = 1.01
S 夕 4	…YA

JV ɪvɔɔ
SVHN (PGD-Conf, L∞ with = 0.03)
Cifar10 (PGD-Conf, L∞ with = 0.03)
Jy ɪvɔo
Figure 8: Adversarial examples against AT and CCAT on MNIST, SVHN, and Cifar10. L∞-PGD-
Conf adversarial examples for MNIST, = 0.3, and SVHN/Cifar10, = 0.03, for AT and CCAT. In
all cases, twelve test examples have been attacked. On the first six examples, AT has difficulties, on
the IaSt six examples, CCAT has difficulties. We also report the true label y,the target label y ofthe
adversarial examples and the corresponding confidence fy(X).
Robust Test Error: For clarity, we repeat our confidence-integrated definition of the robust test
error:
NN
Σ ɪf (Xn) = yn lc(Xn)≥T + Σ If (χn) = yn If(Xn) = Jn Ic(Xn)≥τ
RErr(T )=弋--------N----n=--------------------,	(12)
Σ lc(Xn)≥T + Σ2 lc(Xn)<T Ic(Xn) ≥T If (χn) = yn If (Xn) = yn
n=1	n=1
As described in the main paper, Eq. (12) quantifies the performance of a classifier with reject-option
at a specific confidence threshold τ on both clean and adversarial examples. The regular robust test
error, corresponding to Eq. (12) at τ = 0, can also be written as
NN
Σ ɪf(Xn ) = Jn + Σ ɪf (Xn) = Jn ɪf (Xn) = Jn
RErr(O) = n=1-n=1------
P1
n=1
(13)
24
Under review as a conference paper at ICLR 2020
MNIST: AT with L∞ PGD-Conf, = 0.3
Interpolation Factor κ
乙在7
y=2	y=8	y=7
fy = 1	fy = 0.995	fy = 1
Interpolation Factor κ
之量“
y=2	y=6	y=4
fy = 1	fy=0.4	fy = 1
MNIST: CCAT, ρexp = 7 with L∞ PGD-Conf, = 0.3
186420
0. 0. 0. 0.
Ssbd e。UePguo
0.2	0.4	0.6	0.8	1
Interpolation Factor κ
乙在7
y = 2	y=8	y=7
fy = 1 fy=0.27 fy = 1
Interpolation Factor κ
乙
y=2	y=2	y=4
fy =1 fy=0.17 fy =1
MNIST: CCAT, ρpow = 10 with L∞ PGD-Conf, = 0.3
0. 0. 0. 0.
SssJed e。UePguo0
0.2	0.4	0.6	0.8	1
Interpolation Factor κ
乙室7
y = 2	y=4	y=7
fy = 1	fy=0.11	fy = 1
Interpolation Factor κ
乙
y=2	y=0	y=4
fy =1	fy = 0.12	fy =1
Classes---- 1,-------2,----3,-----4,------5,-----6,-------7,----8,------9,------10
Figure 9: Confidence calibration between test examples on MNIST. We plot the confidence for all
classes when interpolating linearly between test examples: (1 - κ)x1 + κx2 for two test examples
x1 and x2; x1 is fixed and we show two examples corresponding to different x2. Additionally, we
show the corresponding images for κ = 0, i.e., x1, κ = 0.5, i.e., the mean image, and κ = 1,
i.e., x2, with the corresponding labels and confidences. As can be seen, CCAT with power transi-
tion, cf. Eq. (6), is able to perfectly predict a uniform distribution between test examples, while the
exponential transition enforces a stronger bias towards the true labels.
The robust test error is easy to handle as it quantifies overall performance, including generalization
on clean examples (first term in Eq. (13)) and robustness on adversarial examples corresponding
to correctly classified clean examples (second term in Eq. (13)). Additionally, the robust test error
lies in [0, 1]. Generalizing Eq. (13) to τ > 0 is non-trivial due to the following considerations:
First, when integrating a confidence threshold, the reference set (i.e., the denominator) needs to be
adapted. Otherwise, the metric will not reflect the actual performance after thresholding, i.e., reject-
ing examples - specifically, the values are not comparable across different confidence thresholds T.
Then, Eq. (13) might be adapted as follows:
NN
f(X ɪf (xn) = yn lc(xn)≥τ + Σ2 If (χn) = yn If(Xn) = Jn Ic(Xn)≥τ
RErr(T) = n=1------------Nn=------------------------.	(14)
Σ lc(Xn)≥T
n=1
Note that in the nominator, we only consider clean and adversarial examples with confidence above
the threshold τ, i.e., c(xn) ≥ τ and C(Xn) ≥ τ. Similarly, the denominator has been adapted ac-
cordingly, as after rejection, the reference set changes, too. However, this formulation has a problem
when adversarial examples obtain higher confidence than the original clean examples. Thus, second,
we need to account for the case where C(Xn) > τ ≥ C(Xn), i.e., an adversarial example obtains a
higher confidence than the corresponding clean example. Then, the nominator may exceed the de-
nominator, resulting in a value larger than one. To mitigate this problem, we need to include exactly
this case in the denominator. Formalizing this case, we see that it corresponds exactly to the second
term in the denominator or Eq. (12):
N
〉I lc(Xn)<τlc(Xn)≥τlf (χn) = yn If (Xn) = Jn	(15)
n=1
Overall, with Eq. (12) we obtain a metric that lies within [0, 1], is comparable across thresholds τ
and, for τ = 0, reduces to the regular robust test error as used in related work (Madry et al., 2018).
25
Under review as a conference paper at ICLR 2020
MNIST: Main Results for Detection T@98%TPR (L∞, e = 0.30 during training										
	L∞, e = 0.30		L∞, e = 0.40		L2, e = 3		Li, e = 10.00		Lo, e = 15	
	(seen)		(unseen)		(unseen)		(unseen)		(unseen)	
	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%
-AΓ	0.97	-05-	0.20	100.0	0.73	67.6	0.93	~^J	0.99	0.8
CCAT, ρexp=7	0.99	5.6	0.94	29.2	1.00	0.6	0.96	10.9	0.99	5.1
CCAT, ρpow=10-	0.96	18.1	0.94	21.2	1.00	0.0	1.00	1.1	0.99	3.5
SVHN: Main Results for Detection T@98%TPR (L∞, e = 0.03 during training)										
	L∞, 6 = 0.03~		L∞, 6 = 0.06		L2, 6 = 1		Li, 6 = 7.85~		Lo, 6 = 10	
	(seen)		(unseen)		(unseen)		(unseen)		(unseen)	
	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%
-Ar	0.55	52.9	0.32	86.9	0.26	91.3	0.34	90.9	0.90	56.8
CCAT	0.70	36.5	0.70	34.8	0.91	15.6	0.89	17.6	1.00	2.1
CIFAR10: Main Results for Detection T@98%TPR (L∞, 6 = 0.03 during training										
	L∞, 6 = 0.03		L∞, 6 = 0.06		L2, 6 = 1		Li, 6 = 7.85		Lo, 6 = 10	
	(seen)		(unseen)		(unseen)		(unseen)		(unseen)	
	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%	ROC AUC	RErr in%
-Ar	0.64	61.9	0.35	93.6	0.59	73.6	0.61	67.6	0.80	71.8
CCAT	0.60	67.2	0.43	90.9	0.77	45.7	0.78	44.0	0.98	17.8
Table 7: Main results for 98%TPR on MNIST, SVHN and Cifar10. While reporting results for
99%TPR in the main paper, cf. Tab. 3, reducing the TPR requirement for confidence-thresholding to
98%TPR generally improves results for both AT and CCAT, but only slightly. Again, we report Err
and RErr for τ = 0 and τ @98%TPR as well as ROC AUC.
B.4	Ablation Study
Complementing Sec. 4, we include ablation studies for MNIST, SVHN and Cifar10; for our attack
in Tab. 6 and training in Tab. 9.
Regarding the proposed attack, i.e., PGD-Conf, using momentum and backtracking, Tab. 6 shows
that the main observations for SVHN can be transferred to MNIST and Cifar10. Only the improve-
ment of backtracking and momentum over just using momentum cannot be confirmed on MNIST.
Note that for fair comparison, T iterations with backtracking are equivalent to 3/2T iterations without
backtracking; which is why we include results for T = 60 and T = 300. However, the importance
of using enough iterations, i.e., T = 2000, and zero initialization to attack CCAT is still clearly
visible. Interestingly, against AT on SVHN, more iterations are also beneficial, while this is not
required on MNIST and Cifar10.
Tab. 9 also reports results for CCAT with different transitions, cf. Eq. (6), and values for ρ. As
mentioned before, on SVHN and Cifar10, power transition with ρpow = 10 works best; for larger
ρ performance stagnates. It is also important to note that the power transition does not preserve a
bias towards to true label, i.e., for the maximum possible perturbation (kδk∞ = ), Eq. (6) forces
the network to predict a purely uniform distribution. This is in contrast to the exponential transition,
where the true one-hot distribution always receives a non-zero weight. On MNIST, we found this to
work considerably better.
B.5	Analysis
For further analysis, Fig. 5 shows confidence histograms for AT and CCAT on MNIST and Cifar10.
The confidence histograms for CCAT reflect the expected behavior: adversarial examples are mostly
successful in changing the label, which is supported by high RErr values for confidence threshold
τ = 0, but their confidence is pushed towards a uniform distributions. For AT, in contrast, successful
adversarial examples - fewer in total - generally obtain high confidence; this results in confidence
26
Under review as a conference paper at ICLR 2020
MNIST: Supplementary Results for Detection and Standard Settings							
(L∞ attack With e = 0.30 during training)							
		Detection Setting 	T @99%TPR					Standard Setting τ=0	
Attack	Training	ROC AUC	Err in%	RErr in%	τ	Err in%	RErr in%
L∞, e = 0.30	^AT Pang et al. (2018) CCAT	-0.97- 1.00 0.99	-0.0- 0.2 0.1	10- 2.5 7.7	~0o~ 2.2 1.0	-05- 0.5 0.5	72 100.0 100.0
L2, € = 3	^AT Pang et al. (2018) CCAT		-075- 0.99 1.00	-00- 0.2 0.1	-815- 10.9 1.4	~0xΓ 2.2 1.0	-0.5- 0.5 0.5	-988 100.0 82.6
SVHN: Supplementary Results for Detection and Standard Settings							
(L∞ attack with € = 0.03 during training)							
		Detection Setting 	T @99%TPR					Standard Setting τ=0	
Attack	Training	ROC AUC	Err in%	RErr in%	τ	Err in%	RErr in%
L∞, € = 0.03	^AT Pang et al. (2018) CCAT	0.55 0.87 0.70	2.5 3.0 2.1	55.6 92.2 38.5	^06^ 2.2 0.6	-3.4- 4.0 2.9	57.3 99.1 97.8
L2, € = 1	^AT Pang et al. (2018) CCAT		-026- 0.87 0.91	-25- 3.0 2.1	-920- 81.7 18.5	^06^ 2.2 0.6	-3.4- 4.0 2.9	924 93.9 81.8
CIFAR10: Supplementary Results for Detection and Standard Settings							
(L∞ attack with € = 0.03 during training)							
		Detection Setting 	T @99%TPR					Standard Setting τ=0	
Attack	Training	ROC AUC	Err in%	RErr in%	τ	Err in%	RErr in%
L∞, € = 0.03	^AT Pang et al. (2018) CCAT	-064- 0.70 0.60	-151- 19.4 8.7	-623- 83.2 67.9	^03^ 2.2 0.4	-166- 20.0 10.1	627 99.8 96.7
L2, € = 1	^AT Pang et al. (2018) CCAT		-059- 0.66 0.77	-151- 19.4 8.7	-73.9- 83.1 46.2	^03^ 2.2 0.4	-166- 20.0 10.1	744 97.7 81.9
Table 8: Comparison with (Pang et al., 2018) on MNIST, SVHN and Cifar10. We report “stan-
dard” Err and RErr, i.e., τ = 0 (“Standard Setting”), as well as their confidence thresholded
variants for τ @99%TPR (“Detection Setting”), cf. Sec. 3. On MNIST their method is competitive
regarding RErr after confidence-thresholding, even outperforming AT when it comes to the gener-
alization to L2 attacks. However, on SVHN and Cifar10, the approach cannot be considered robust
anymore regarding our rigorous evaluation; this might also be due to the fact that no adversarial
examples are used during training.
thresholding being not effective for AT. This behavior, however, is less pronounced on MNIST.
Here, the exponential transition results in adversarial examples with confidence slightly higher than
uniform confidence, i.e., 0.1. This might be the result of preserving a bias towards the true label
through Eq. (6). In fact, for lower ρ, we found that this behavior is pronounced, until, for very small
ρ, the behavior of AT is obtained.
In Fig. 7, we plot the probabilities for all ten classes along an adversarial direction. We note that
these directions do not necessarily correspond to successful adversarial examples. Instead, we chose
the first 10 test examples on SVHN. The adversarial examples were obtained using our L∞ PGD-
Conf attack with T = 2000 iterations and zero initialization for = 0.03. For AT, we usually
observe a change in predictions along these directions; some occur within kδk∞ ≤ , corresponding
to successful adversarial examples, some occur for kδ k∞ > , corresponding to unsuccessful ad-
versarial examples (within ). However, AT always assigns high confidence. Thus, when allowing
larger adversarial perturbations at test time, robustness of AT reduces significantly. For CCAT, in
27
Under review as a conference paper at ICLR 2020
contrast, there are only few such cases; more often, the model achieves a near uniform prediction
for small kδk∞ and extrapolates this behavior beyond the -ball used for training. On SVHN, this
behavior successfully allows to generalize the robustness to larger adversarial perturbations. Fur-
thermore, these plots illustrate why using more iterations at test time, and using techniques such as
momentum and backtracking, are necessary to find adversarial examples as the objective becomes
more complex compared to AT.
Fig. 8 shows concrete adversarial examples against AT and CCAT on MNIST and SVHN. Among the
first 200 test examples, we picked the first six examples where AT is mistaken and (non-overlapping)
six examples where CCAT has difficulties, i.e., assigns high confidence. On SVHN, for example,
CCAT is able to detect adversarial examples with low confidence where AT assigns incorrect classes.
On MNIST, in contrast, CCAT, cannot recover mistakes made by AT. Similarly, there exist examples
where AT assigns the correct class while CCAT assigns the wrong class with very high confidence.
On MNIST, we additionally found adversarial examples against CCAT to represent more “localized”
changes in the L∞ norm compared to AT. Overall, however, we did not find any systematic difference
between strong adversarial against AT and CCAT.
In Fig. 9, on MNIST, we additionally illustrate the advantage of CCAT with respect to the toy example
in Proposition 1. Here, we consider the case where the -balls of two training or test examples
(in different classes) overlap. As we show in Proposition 1, adversarial training is not able to
handle such cases, resulting in the trade-off between accuracy in robustness reported in the literature
(Tsipras et al., 2018; Stutz et al., 2019; Raghunathan et al., 2019; Zhang et al., 2019). This is
because adversarial training enforces high-confidence predictions on both -balls (corresponding
to different classes), resulting in an obvious conflict. CCAT, in contrast, enforces uniform predictions
throughout the largest parts of both -balls, resolving the conflict. Finally, Fig. 9 also shows that
CCAT is indeed able to extrapolate the uniform predictions beyond the -ball used during training.
B.6 Results
In the following, we present and discuss complementary results corresponding to the main results of
our paper as presented in Tab. 3 and 4. To this end, we consider requiring only 98% TPR instead of
99% TPR, and most importantly, break down our analysis by the different white-box and black-box
attacks used.
Main results for 98% TPR: Tab. 7 reports results (corresponding to Tab. 3) requiring only 98%TPR.
This implies, that compared to 99%TPR, up to 1% more correctly classified test examples can be
rejected. For relatively simple tasks such as MNIST and SVHN, where Err values are low, this is a
significant “sacrifice”. However, as can be seen, robustness in terms of RErr only improves slightly.
We found that the same holds for 95%TPR, however, rejecting more than 2% correctly classified
examples seems prohibitive large for the considered datasets.
Per-Attack Results: Finally Tab. 10, 11 and 12 we break down the results of Tab. 3 regarding the
used attacks. For simplicity we focus on PGD-CE and PGD-Conf while reporting the used black-
box attacks together, i.e., taking the worst-case adversarial examples across all black-box attacks.
For comparison, we also include non-thresholded Err and RErr for the results from the main paper.
On MNIST, where AT performs very well in practice, it is striking that for 4/3 = 0.4 even black-
box attacks are able to reduce robustness completely, resulting in high RErr. This observation also
transfers to SVHN and Cifar10. For CCAT, black-box attacks are only effective on Cifar10, where
they result in roughly 87% RErr with τ @99%TPR. For the L2, L1 and L0 attacks we can make
similar observations, except on MNIST, where AT seems similarly robust against L1 and L0 attacks
as CCAT. Across all Lp norms, it can also be seen that PGD-CE performs significantly worse
against our CCAT compared to AT, which shows that it is essential to optimize the right objective
to evaluate the robustness of defenses and adversarially trained models, i.e., maximize confidence
against CCAT
Results on Corrupted MNIST/Cifar10: We also conducted experiments on MNIST-C (Mu &
Gilmer, 2019) and Cifar10-C (Hendrycks & Dietterich, 2019). These datasets are variants of
MNIST and Cifar10 that contain common perturbations of the original images obtained from var-
ious types of noise, blur or transformations; examples include zoom or motion blue, Gaussian and
shot noise, rotations, translations and shear. Tab. 14 presents the per-corruption results on MNIST-
C and Cifar10-C. Here, all includes all corruptions and mean reports the average results across
28
Under review as a conference paper at ICLR 2020
all corruptions; note that, due to the thresholding, leaving different numbers of corrupted examples
after detection, the distinction between all and mean is meaningful. Striking is the performance
of CCAT on noise corruptions such as gaussian_noise or shot_noise. Here, CCAT is able
to detect 100% of the corrupted examples (cf. the true negative rate (TNR) in Tab. 14), resulting
in a thresholded Err of 0%. This is in stark contrast to AT, exhibiting a Err of roughly 15% after
detection on Cifar10-C. On the remaining corruptions, CCAT is able to perform slightly better than
AT, which is often due to higher detection rate, i.e., higher ROC AUC. On, Cifar10, the generally
lower Err of CCAT also contributes to the results. Overall, this illustrates that CCAT is able to pre-
serve the inductive bias of predicting near-uniform distribution on noise similar to L∞ adversarial
examples as seen during training.
Comparison with (Pang et al., 2018): We also compare AT and CCAT with the approach by Pang
et al. (2018). We implemented the reverse cross-entropy error in PyTorch following the official
code1 . The reverse cross-entropy loss encourages the non-maximum predictions (i.e., in the case
of correct classification, the probabilities of the “other” classes) to be uniform. Then, adversarial
examples are detected based on statistics of the predicted distributions. We used the non-maximum
element entropy (nonME) detector instead of the K-density detector (Feinman et al., 2017) which
has been shown to be ineffective against adaptive attacks in (Carlini & Wagner, 2017a). We note
that (Pang et al., 2018) do not train on adversarial examples as AT or CCAT. Our evaluation
metrics need not be changed; however, thresholding is done on the nonME detector instead of on
the confidence, as for CCAT. In Tab. 8, it can be seen, that the approach works considerably well on
MNIST, regarding both L∞ and L2 attacks. Again, we report the worst-case results across PGD-
CE, PGD-Conf and the considered black-box attacks. However, on SVHN and Cifar10, we are able
to break to robustness of the approach, reaching 92% (thresholded) RErr on SVHN and 99% RTE
on Cifar10. On SVHN, the models performs slightly better against L2 attacks, with 81%, but can
not be considered robust anymore.
1https://github.com/P2333/Reverse-Cross-Entropy
29
Under review as a conference paper at ICLR 2020
MNIST: Training Ablation for Detection and Standard Settings (L∞ attack with e = 0.30 during training)						
	Detection Setting 	T @99%TPR					Standard Setting τ = 0	
	ROC AUC	Err in%	RErr in%	τ	Err in%	RErr in%
Normal AT ATConf	-034- 0.97 0.98	-01- 0.0 0.1	-100.0 0.4 1.1	~~0(Γ 1.0 1.0	-04- 0.5 0.5	-100.0- 5.6 6.0
CCAT, PexP = 3 CCAT, PexP = 4 CCAT, PexP = 5 CCAT, ρexp = 6 CCAT, ρexp = 7 CCAT, ρexp = 8 CCAT, ρexp = 9 CCAT, ρexp = 10	-0:99- 0.98 0.99 0.99 0.99 0.98 0.99 0.99	-01- 0.1 0.1 0.1 0.1 0.1 0.1 0.1	-119- 10.4 10.8 7.8 5.7 13.3 11.0 14.3	~~0(Γ 1.0 1.0 1.0 1.0 1.0 1.0 1.0	-04- 0.5 0.4 0.4 0.5 0.3 0.3 0.4	881 95.2 88.7 83.5 74.7 94.3 86.8 97.1
CCAT, ρpow = 10	0.95	0.1	17.3	^T0-	0.3	64.8
SVHN: Training Ablation for Detection and Standard Settings (L∞ attack with e = 0.03 during training)						
	Detection Setting 	T @99%TPR					Standard Setting τ = 0	
	ROC AUC	Err in%	RErr in%	τ	Err in%	RErr in%
Normal AT ATConf	-0:17- 0.55 0.61	-26- 2.5 2.8	-999- 54.9 52.5	^08^ 0.6 0.6	-3.6- 3.4 3.7	-999 56.9 58.7
CCAT, ρpow = 1 CCAT, ρpow = 2 CCAT, ρpow = 4 CCAT, ρpow = 6 CCAT, ρpow = 8 CCAT, ρpow = 10 CCAT, ρpow = 12	-074- 0.68 0.68 0.64 0.63 0.67 0.67	-22- 2.1 1.8 1.8 2.2 2.1 1.9	-43.0- 44.2 35.8 32.8 42.3 38.5 36.3	~01Γ 0.5 0.6 0.7 0.6 0.6 0.6	-27- 2.9 2.7 2.9 2.9 2.9 2.8	82.4 79.6 80.4 72.1 84.6 91.0 81.8
CCAT, PexP = 7	0.66	2.0	54.7	~05^	2.9	73.1
CIFAR10: Training Ablation for Detection and Standard Settings (L∞ attack with e = 0.03 during training)						
	Detection Setting 	T @99%TPR					Standard Setting τ = 0	
	ROC AUC	Err in%	RErr in%	τ	Err in%	RErr in%
Normal AT ATConf	-020- 0.65 0.63	-7.4- 15.1 15.1	-100.0 60.9 61.5	^06^ 0.3 0.4	-83- 16.6 16.1	-100.0- 61.3 61.7
CCAT, ρpow = 1 CCAT, ρpow = 2 CCAT, ρpow = 4 CCAT, ρpow = 6 CCAT, ρpow = 8 CCAT, ρpow = 10 CCAT, ρpow = 12	-063- 0.60 0.61 0.54 0.58 0.60 0.62	-87- 8.4 8.6 8.0 8.5 8.7 9.4	-72.4- 70.6 66.3 69.8 65.3 63.0 63.0	^03^ 0.4 0.4 0.4 0.4 0.4 0.3	-97- 9.7 9.8 9.2 9.4 10.1 10.1	95.3 95.1 93.5 94.1 93.2 95.0 96.6
CCAT, PexP = 7	0.66	11.7	68.6	~02~	13.2	77.4
Table 9: Training ablation studies on MNIST, SVHN and Cifar10. We report results for different
ρ and transitions, cf. Eq. (6). We report RErr and Err with confidence threshold τ = 0 (“Stan-
dard Setting”) and τ @99%TPR as well as ROC AUC (“Detection Setting”). The models are tested
against our L∞ PGD-Conf attack with T = 2000 iterations and zero as well as random initialization,
as discussed in Sec. 4.1. On MNIST, the exponential transition, especially ρexp = 7 performs best;
on Cifar10, the power transition with Ppow = 10 works best - performance stagnates for PPow > 10.
On SVHN, we also use ρpow = 10, although ρpow = 6 shows better results. However, against larger
-balls, we found that Ppow = 10 works significantly better.
30
Under review as a conference paper at ICLR 2020
MNIST: Supplementary Results for Detection and Standard Settings							
	(L∞, =	0.30 during training)					
			Detection Setting			Standard Setting	
			τ @99%TPR			τ=0	
Attack	Training	-ROC-	Err	RErr		Err	RErr
		AUC	in %	in %	T	in %	in %
	^AT	097	00	1.0	1.0	05	72
Worst-Case (L∞, e = 0.30)	CCAT, ρexp=7	0.99	0.1	7.7	1.0	0.5	100.0
	CCAT, ρpow = 10	0.96	0.1	21.9	1.0	0.3	90.5
	AT	097	00	0.4	1.0	05	56
PGD Conf(L∞, e = 0.30)	CCAT, ρexp=7	0.99	0.1	5.7	1.0	0.5	74.7
	CCAT, ρpow = 10	0.95	0.1	17.3	1.0	0.3	64.8
	^AT	097	00	0.8	1.0	05	67
PGD CE (L∞, e = 0.30)	CCAT, ρexp=7	1.00	0.1	4.3	1.0	0.5	100.0
	CCAT, ppow = 10	1.00	0.1	0.1	1.0	0.3	100.0
	^AT	098	00	1.0	1.0	05	72
Black-Box (L∞, e = 0.30)	CCAT, ρexp=7	1.00	0.1	0.4	1.0	0.5	100.0
	CCAT, ρpow = 10	1.00	0.1	7.4	1.0	0.3	88.6
	AT	020	00	100.0	1.0	05	-100.0-
Worst-Case (L∞, e = 0.40)	CCAT, ρexp=7	0.94	0.1	40.0	1.0	0.5	100.0
	CCAT, ρpow = 10	0.94	0.1	29.0	1.0	0.3	96.9
	^AT	036	00	97.8	1.0	05	-99.8
PGD Conf(L∞, e = 0.40)	CCAT, ρexp=7	0.96	0.1	15.4	1.0	0.5	97.1
	CCAT, ρpow = 10	0.92	0.1	20.6	1.0	0.3	72.5
	^AT	020	00	100.0	1.0	05	-100.0-
PGD CE (L∞, e = 0.40)	CCAT, ρexp=7	0.97	0.1	29.6	1.0	0.5	100.0
	CCAT, ppow = 10	1.00	0.1	2.5	1.0	0.3	100.0
	^AT	023	00	100.0	1.0	05	-100.0-
Black-Box (L∞, e = 0.40)	CCAT, ρexp=7	0.99	0.1	3.9	1.0	0.5	100.0
	CCAT, ρpow = 10	0.99	0.1	10.7	1.0	0.3	91.8
	AT	073	00	81.3	1.0	05	-98.8
WorSt-CaSe(L2, e = 3)	CCAT, ρexp=7	1.00	0.1	1.4	1.0	0.5	82.6
	CCAT, ρpow = 10	1.00	0.1	0.1	1.0	0.3	22.1
	^AT	098	00	0.2	1.0	05	3.4
PGDConf(L2, e = 3)	CCAT, ρexp=7	1.00	0.1	0.0	1.0	0.5	4.4
	CCAT, ppow = 10	1.00	0.1	0.1	1.0	0.3	9.5
	^AT	093	00	11.5	1.0	05	-292
PGDCE (L2, e = 3)	CCAT, ρexp=7	1.00	0.1	0.9	1.0	0.5	82.4
	CCAT, ρpow = 10	1.00	0.1	0.1	1.0	0.3	100.0
	AT	073	00	80.1	1.0	05	-987
Black-Box (L2, e = 3)	CCAT, ρexp=7	1.00	0.1	0.6	1.0	0.5	38.4
	CCAT, ρpow = 10	1.00	0.1	0.1	1.0	0.3	100.0
	^AT	093	00	5.2	1.0	05	180
Worst-Case (Li, e = 10.00)	CCAT, ρexp=7	0.96	0.1	14.7	1.0	0.5	35.8
	CCAT, ρpow = 10	1.00	0.1	1.6	1.0	0.3	17.7
	^AT	093	00	4.2	1.0	05	14.8
PGDConf(Li, e = 10.00)	CCAT, ρexp=7	0.96	0.1	14.5	1.0	0.5	35.7
	CCAT, ppow = 10	1.00	0.1	1.6	1.0	0.3	17.7
	^AT	096	00	3.8	1.0	05	-155-
PGD CE (Li, e = 10.00)	CCAT, ρexp=7	0.98	0.1	4.2	1.0	0.5	12.1
	CCAT,ρpow = 10		1.00	0.1	0.1	1.0	0.3	99.7
	AT	099	00	2.5	1.0	05	-93.9
Worst-Case (Lo, e = 15)	CCAT, ρexp=7	0.99	0.1	7.8	1.0	0.5	65.9
	CCAT, ρpow = 10	0.99	0.1	7.8	1.0	0.3	88.8
	^AT	098	00	0.5	1.0	05	52
PGD Conf (Lo, e = 15)	CCAT, ρexp=7	0.99	0.1	4.1	1.0	0.5	18.3
	CCAT, ppow = 10	0.99	0.1	3.7	1.0	0.3	13.4
	^AT	098	00	2.4	1.0	05	177
PGD CE (Lo, e = 15)	CCAT, ρexp=7	0.98	0.1	6.2	1.0	0.5	18.6
	CCAT, ρpow = 10	0.98	0.1	5.4	1.0	0.3	16.5
	AT	100	00	0.0	1.0	05	-93.9
BlaCk-Box(L0, e = 15)	CCAT, ρexp=7	1.00	0.1	0.1	1.0	0.5	65.7
	CCAT,ρpow = 10		1.00	0.1	1.4	1.0	0.3	88.9
Table 10: Per-attack results on MNIST. Per-attack results considering PGD-CE, as in Madry et al.
(2018), our PGD-Conf and the remaining black-box attacks for all threat models, i.e., L∞, L2, L1
and L0, see text. The used values are reported in the left-most column. For the black-box attacks,
we take the per-example worst-case across all black-box attacks.
31
Under review as a conference paper at ICLR 2020
SVHN: Supplementary Results for Detection and Standard Settings							
(L∞, e = 0.03 during training)							
		Detection Setting 	T @99%TPR					Standard Setting τ=0	
Attack	Training	ROC AUC	Err in%	RErr in%	τ	Err in%	RErr in%
Worst-Case (L∞, e = 0.03)	AT CCAT	0.55 0.70	2.5 2.1	55.6 38.5	^06^ 0.6	-34- 2.9	57.3 97.8
PGD Conf(L∞, e = 0.03)	^AT CCAT	-0.55- 0.67	-25- 2.1	-549- 38.5	^06^ 0.6	-34- 2.9	56.9 91.0
PGD CE (L∞, e = 0.03)	^AT CCAT	-0.68- 1.00	-25- 2.1	-432- 2.6	^06^ 0.6	-34- 2.9	507 94.9
BlaCk-Box (L∞, e = 0.03)	^AT CCAT	-095- 1.00	-25- 2.1	-308- 6.3	^06^ 0.6	-34- 2.9	462 79.5
Worst-Case (L∞, e = 0.06)	^AT CCAT	-0.32- 0.70	-23- 2.1	-88.3- 46.0	^06^ 0.6	-34- 2.9	89.0 99.8
PGD Conf(L∞, e = 0.06)	^AT CCAT	-0.32- 0.70	-25- 2.1	-847- 36.8	^06^ 0.6	-34- 2.9	861 98.7
PGD CE (L∞, e = 0.06)	^AT CCAT	-0.66- 0.99	-25- 2.1	-88.0- 17.2	^06^ 0.6	-34- 2.9	-88.9 100.0
Black-Box (L∞, e = 0.06)	^AT CCAT	-0.78- 1.00	-25- 2.1	-823- 4.8	^06^ 0.6	-34- 2.9	840 82.3
WorSt-CaSe(L2, e = 1)	^AT CCAT	-026- 0.91	-25- 2.1	-920- 18.5	^06^ 0.6	-34- 2.9	924 81.8
PGD Conf(L2, e = 1)	^aT CCAT	-0.50- 0.90	-25- 2.1	-771- 18.4	^06^ 0.6	-34- 2.9	787 74.4
PGD CE (L2, e = 1)	^AT CCAT	-0.27- 0.99	-25- 2.1	-920- 3.7	^06^ 0.6	-34- 2.9	924 100.0
Black-Box (L2, e = 1)	^aT CCAT	-0.98- 1.00	-25- 2.1	-137- 2.6	^06^ 0.6	-34- 2.9	298 100.0
Worst-Case (Li, e = 7.85)	^aT CCAT	-0.34- 0.89	-25- 2.1	-919- 20.8	^06^ 0.6	-34- 2.9	925 63.5
PGD Conf(L1, e = 7.85)	^AT CCAT	-0.37- 0.92	-25- 2.1	-87.9- 13.4	^06^ 0.6	-34- 2.9	88.8 50.3
PGD CE (Li, e = 7.85)	^aT CCAT	-0.51- 0.99	-25- 2.1	-88.9- 4.8	^06^ 0.6	-34- 2.9	89.6 100.0
Worst-Case (Lo, e = 10)	^aT CCAT	-0.90- 1.00	-25- 2.1	-73.4- 2.7	^06^ 0.6	-34- 2.9	892 77.1
PGD Conf(Lo, e = 10)	^AT CCAT	-0.90- 1.00	-25- 2.1	-513- 2.7	^06^ 0.6	-34- 2.9	-59.7 63.9
PGD CE (Lo, e = 10)	^aT CCAT	-089- 1.00	-25- 2.1	-60.8- 2.6	^06^ 0.6	-34- 2.9	68.9 96.9
Black-Box (Lo, e = 10)	^aT CCAT	-0.98- 1.00	-25- 2.1	-35.0- 2.6	^06^ 0.6	-34- 2.9	867 100.0
Table 11: Per-attack results on SVHN. Per-attack results considering PGD-CE, as in Madry et al.
(2018), our PGD-Conf and the remaining black-box attacks for all threat models, i.e., L∞, L2, L1
and L0, see text. The used values are reported in the left-most column. For the black-box attacks,
we take the per-example worst-case across all black-box attacks.
32
Under review as a conference paper at ICLR 2020
CIFAR10: Supplementary Results for Detection and Standard Settings							
(L∞, e = 0.03 during training)							
		Detection Setting				Standard Setting	
			τ @99%TPR			τ=0	
Attack	Training	-ROC-	Err	RErr		Err	RErr
		AUC	in%	in%	T	in%	in%
Worst-Case (L∞, e = 0.03)	AT	-064-	15.1	-623-	^03^	16.6	627
	CCAT	0.60	8.7	67.9	0.4	10.1	96.7
PGD Conf(L∞, e = 0.03)	^AT	-0:65-	-151-	-609-	^03^	-16.6-	61.3
	CCAT	0.60	8.7	63.0	0.4	10.1	95.0
PGD CE (L∞, e = 0.03)	^AT	-067-	-151-	-614-	^03^	-16.6-	623
	CCAT	0.99	8.7	9.8	0.4	10.1	100.0
BlaCk-Box (L∞, e = 0.03)	^AT	-070-	-151-	-569-	^03^	-16.6-	-573
	CCAT	0.90	8.7	49.3	0.4	10.1	96.4
Worst-Case (L∞, e = 0.06)	^AT	-035-	-151-	-93.6-	^03^	-16.6-	93.7
	CCAT	0.43	8.7	91.5	0.4	10.1	99.2
PGD Conf(L∞, e = 0.06)	^AT	-037-	-151-	-92.1-	^03^	-16.6-	922
	CCAT	0.49	8.7	66.8	0.4	10.1	97.3
PGD CE (L∞, e = 0.06)	^AT	-040-	-151-	-93.6-	^03^	-16.6-	93.7
	CCAT	0.98	8.7	10.4	0.4	10.1	100.0
Black-Box (L∞, e = 0.06)	^AT	-050-	-151-	-871-	^03^	-16.6-	87.2
	CCAT	0.78	8.7	87.0	0.4	10.1	99.5
WorSt-CaSe(L2, e = 1)	^AT	-059-	-151-	-73.9-	^03^	-16.6-	74.4
	CCAT	0.77	8.7	46.2	0.4	10.1	81.9
PGD Conf(L2, e = 1)	^aT	-063-	-151-	-64.9-	^03^	-16.6-	-653
	CCAT	0.78	8.7	45.2	0.4	10.1	80.9
PGD CE (L2, e = 1)	^AT	-061-	-151-	-73.9-	^03^	-16.6-	74.6
	CCAT	0.95	8.7	18.9	0.4	10.1	100.0
Black-Box (L2, e = 1)	^aT	-081-	-151-	-35.8-	^03^	-16.6-	36.9
	CCAT	1.00	8.7	8.8	0.4	10.1	100.0
Worst-Case (Li, e = 7.85)	^aT	-061-	-151-	-68.0-	^03^	-16.6-	68.3
	CCAT	0.78	8.7	45.2	0.4	10.1	75.7
PGD Conf(L1, e = 7.85)	^AT	-061-	-151-	-66.8-	^03^	-16.6-	-671
	CCAT	0.84	8.7	35.7	0.4	10.1	73.5
PGD CE (Li, e = 7.85)	^aT	-071-	-151-	-58.5-	^03^	-16.6-	60.5
	CCAT	0.96	8.7	17.4	0.4	10.1	100.0
Worst-Case (Lo, e = 10)	^aT	-080-	-151-	-74.1-	^03^	-16.6-	759
	CCAT	0.98	8.7	20.9	0.4	10.1	55.7
PGD Conf(Lo, e = 10)	^AT	-074-	-151-	-443-	^03^	-16.6-	-451
	CCAT	0.98	8.7	12.5	0.4	10.1	34.8
PGD CE (Lo, e = 10)	^aT	-076-	-151-	-49.4-	^03^	-16.6-	51.4
	CCAT	1.00	8.7	7.6	0.4	10.1	79.3
Black-Box (Lo, e = 10)	^aT	-091-	-151-	-69.6-	^03^	-16.6-	759
	CCAT	0.99	8.7	17.8	0.4	10.1	99.1
Table 12: Per-attack L∞ and L2 results on Cifar10. Per-attack results considering PGD-CE, as
in Madry et al. (2018), our PGD-Conf and the remaining black-box attacks for all threat models,
i.e., L∞, L2, L1 and L0, see text. The used values are reported in the left-most column. For the
black-box attacks, we take the per-example worst-case across all black-box attacks.
33
Under review as a conference paper at ICLR 2020
MNIST: Supplementary Results for Detection and Standard Settings							
(L∞ attack with e = 0.30 during training)							
		Detection Setting τ @99%TPR					Standard Setting τ=0
Corruption	Training	ROC AUC	FPR in %	TNR in %	Err in %	τ	Err in %
	Normal	075	82.8	172-	-319-	~TxΓ	36.4
all	AT	0.80	59.0	41.0	4.0	1.0	26.9
	CCAT		0.95	40.3	59.7	4.5	1.0	25.1
	Normal	075	82.8	172-	-328-	T.0-	363
mean	AT	0.80	59.0	41.0	12.6	1.0	26.9
	CCAT		0.95	40.4	59.7	4.0	1.0	25.1
	Normal	03^	100.0	00	-902-	T.0-	902
brightness	AT CCAT		0.99 1.00	0.9 0.0	99.1 100.0	36.4 0.0	1.0 1.0	84.3 88.3
	Normal	091	62.4	-376-	-348-	~TxΓ	45.6
canny_edges	AT	0.96	28.8	71.2	33.4	1.0	53.2
	CCAT		0.97	45.1	54.9	47.0	1.0	51.5
	Normal	076	85.4	146-	24	T.0-	7.6
dotted_line	AT CCAT		0.77 0.92	72.8 74.2	27.2 25.8	0.9 1.8	1.0 1.0	7.9 7.6
	Normal	038	99.9	01	-902-	T.0-	902
fog	AT	0.90	29.7	70.3	10.4	1.0	59.0
	CCAT		1.00	0.0	100.0	0.0	1.0	65.0
	Normal	087	71.6	-284-	-572-	T.0-	562
glass_blur	AT	0.82	67.4	32.6	1.2	1.0	11.0
	CCAT		1.00	0.0	100.0	0.0	1.0	6.7
	Normal	087	72.4	-276-	-792-	T.0-	813
impulse_noise	AT CCAT		0.98 1.00	13.9 0.0	86.1 100.0	18.8 0.0	1.0 1.0	61.4 47.1
	Normal	086	73.9	-26.1	-295-	T.0-	372
motion_blur	Ar CCAT		0.62 1.00	90.6 0.1	9.4 99.9	0.3 0.0	1.0 1.0	2.7 1.7
	Normal	070	92.4	7.6	17	T.0-	46
rotate	Ar CCAT		0.64 0.94	87.9 84.3	12.1 15.7	0.8 0.5	1.0 1.0	4.1 3.8
	Normal	084	89.5	105-	07	~TxΓ	31
scale	AT	0.86	78.5	21.5	0.1	1.0	3.0
	CCAT		0.85	91.0	9.0	0.3	1.0	2.3
	Normal	060	97.6	24	02	T.0-	08
shear	AT	0.56	95.1	4.9	0.1	1.0	0.9
	CCAT		0.93	89.1	10.9	0.0	1.0	0.9
	Normal	074	93.0	7.0	14	T.0-	3.6
shot_noise	AT CCAT		0.62 0.87	91.3 86.5	8.7 13.5	0.2 0.2	1.0 1.0	1.9 2.0
	Normal	085	72.8	-272-	18.9-	T.0-	281
spatter	AT	0.65	88.3	11.7	0.6	1.0	3.6
	CCAT		1.00	0.7	99.3	0.0	1.0	1.6
	Normal	092	61.1	-389-	-69.6-	T.0-	702
stripe	AT	0.98	12.2	87.8	83.8	1.0	81.3
	CCAT		1.00	0.0	100.0	0.0	1.0	75.8
	Normal	072	95.2	48	03	T.0-	16
translate	AT CCAT		0.80 0.86	74.6 91.7	25.4 8.3	0.2 0.1	1.0 1.0	4.0 1.3
	Normal	085	74.5	-255-	163-	T.0-	248
zigzag	AT	0.87	53.6	46.4	10.9	1.0	24.9
	CCAT		0.97	42.6	57.4	9.3	1.0	21.1
Table 13: Per-corruptions results on MNIST-C. Results on MNIST-C, broken down by individual
corruptions (first column); all includes all corruptions and mean are the averaged results over all
corruptions. We report ROC AUC, FPR and additionally the true negative rate (TNR) in addition to
the thresholded and unthresholded Err on the corrupted examples.
34
Under review as a conference paper at ICLR 2020
CIFAR10: Supplementary Results for Detection and Standard Settings							
(L∞ attack with e = 0.03 during training)							
		Detection Setting τ @99%TPR					Standard Setting τ=0
Corruption	Training	ROC	FPR	TNR	Err		Err
		AUC	in %	in %	in %	τ	in %
	Normal	057	97.1	2.9	122-	0~66~	137
all	AT	0.53	96.2	3.8	16.2	0.3	18.1
	CCAT		0.66	72.1	27.9	10.4	0.4	27.2
	Normal	057	97.1	29	123-	-06-	137
mean	AT	0.53	96.2	3.8	16.2	0.3	18.1
	CCAT		0.66	72.1	27.9	8.5	0.4	27.2
	Normal	050	98.1	1.9	7.5	-06-	84
brightness	AT	0.50	97.0	3.0	14.9	0.3	16.5
	CCAT		0.54	94.8	5.2	8.2	0.4	10.4
	Normal	052	98.1	1.9	8.4	-06-	94
contrast	AT	0.60	94.1	5.9	17.1	0.3	20.0
	CCAT		0.55	96.6	3.4	10.3	0.4	11.9
	Normal	050	98.1	1.9	7.4	-06-	84
defocus_blur	AT	0.51	96.8	3.2	15.5	0.3	17.2
	CCAT		0.49	97.5	2.5	9.2	0.4	10.5
	Normal	0.60	97.3	27	12.1	~6(Γ	135
elastic_transform	AT	0.57	95.8	4.2	19.0	0.3	21.1
	CCAT		0.54	96.5	3.5	13.2	0.4	14.9
	Normal	051	98.0	20	7.9	-06-	8.8
fog	AT	0.57	94.7	5.3	15.7	0.3	18.3
	CCAT		0.55	96.0	4.0	9.0	0.4	11.1
	Normal	057	97.2	2.8	11.5-	-06-	12.8
frost	AT	0.53	95.9	4.1	16.0	0.3	18.0
	CCAT		0.65	88.1	11.9	9.0	0.4	12.5
	Normal	050	98.0	20	7.4	-06-	84
gaussian_blur	AT	0.51	96.8	3.2	15.5	0.3	17.1
	CCAT		0.49	97.4	2.6	9.2	0.4	10.4
	Normal	062	96.2	3.8	157-	-06-	177
gaussian_noise	AT	0.51	96.8	3.2	15.5	0.3	17.0
	CCAT		1.00	0.0	100.0	0.0	0.4	84.9
	Normal	076	92.5	75	-40.8-	-06-	43.1
glass_blur	AT	0.58	95.4	4.6	18.0	0.3	20.2
	CCAT		0.93	31.1	68.9	14.7	0.4	31.8
	Normal	059	97.0	30	13.1	-06-	145
impulse_noise	AT	0.53	96.4	3.6	16.4	0.3	17.9
	CCAT		1.00	0.1	99.9	0.0	0.4	61.0
	Normal	059	96.9	3.1	123-	~6(Γ	137
jpeg_compression	AT	0.51	96.8	3.2	15.5	0.3	17.0
	CCAT		0.59	93.3	6.7	9.3	0.4	12.1
	Normal	058	97.3	27	109-	-06-	122
motion_blur	AT	0.55	95.9	4.1	16.8	0.3	18.9
	CCAT		0.52	96.5	3.5	11.9	0.4	13.6
	Normal	054	97.6	24	97	-06-	109
pixelate	AT	0.51	96.7	3.3	15.3	0.3	17.0
	CCAT		0.52	97.0	3.0	9.2	0.4	10.8
	Normal	055	97.5	25	100-	-06-	11.3
saturate	AT	0.55	95.3	4.7	18.2	0.3	20.5
	CCAT		0.48	97.6	2.4	11.8	0.4	13.0
	Normal	058	97.0	30	12.1	-06-	137
shot_noise	AT	0.51	97.1	2.9	15.4	0.3	16.7
	CCAT		1.00	0.0	100.0	0.0	0.4	84.5
	Normal	057	97.3	27	12.1	-06-	135
snow	AT	0.51	96.7	3.3	15.3	0.3	17.0
	CCAT		0.58	95.6	4.4	11.2	0.4	13.3
	Normal	054	97.5	25	9.4	-06-	106
spatter	AT	0.51	96.8	3.2	15.5	0.3	17.2
	CCAT		0.58	95.2	4.8	9.3	0.4	11.6
	Normal	057	97.0	30	124-	~6(Γ	13.8
Speckle-noise	AT	0.51	97.1	2.9	15.2	0.3	16.7
	CCAT		1.00	0.0	100.0	0.0	0.4	83.0
	Normal	061	96.7	33	129-	-06-	146
zoom_blur	AT	0.56	95.7	4.3	17.2	0.3	19.2
	CCAT		0.52	97.1	2.9	13.9	0.4	15.6
Table 14: Per-corruptions results on Cifar10-C. Results on Cifar10-C focusing on individual cor-
ruptions (first column); all includes all corruptions and mean are the averaged results over all
corruptions. We report ROC AUC, FPR and additionally the true negative rate (TNR) in addition to
the thresholded and unthresholded Err on the corrupted examples.
35