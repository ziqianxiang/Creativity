Under review as a conference paper at ICLR 2020
A Generative Model for Molecular Distance
Geometry
Anonymous authors
Paper under double-blind review
Ab stract
Computing equilibrium states for many-body systems, such as molecules, is a
long-standing challenge. In the absence of methods for generating statistically in-
dependent samples, great computational effort is invested in simulating these sys-
tems using, for example, Markov chain Monte Carlo. We present a probabilistic
model that generates such samples for molecules from their graph representations.
Our model learns a low-dimensional manifold that preserves the geometry of local
atomic neighborhoods through a principled learning representation that is based
on Euclidean distance geometry. In a new benchmark for molecular conformation
generation, we show experimentally that our generative model achieves state-of-
the-art accuracy. Finally, we show how to use our model as a proposal distribution
in an importance sampling scheme to compute molecular properties.
1	Introduction
Over the last few years, many highly-effective deep learning methods generating small molecules
with desired properties (e.g., novel drugs) have emerged (Gomez-Bombarelli et al., 2018; Segler
et al., 2018; Dai et al., 2018; Jin et al., 2018; Bradshaw et al., 2019a; Liu et al., 2018; You et al.,
2018; Bradshaw et al., 2019b). These methods operate using graph representations of molecules in
which nodes and edges represent atoms and bonds, respectively. A representation that is closer to
the physical system is one in which a molecule is described by its geometry or conformation. A
conformation x of a molecule is defined by a set of atoms {(i, ri)}iN=v1, where Nv is the number
of atoms in the molecule, i ∈ {H, C, O, ...} is the chemical element of the atom i, and ri ∈ R3 is
its position in Cartesian coordinates. Importantly, the relative positions of the atoms are restricted
by the bonds in the molecule and the angles between them. Due to thermal fluctuations resulting in
stretching of and rotations around bonds, there exist infinitely many conformations of a molecule.
A molecule’s graph representation and a set of its conformations are shown in Fig. 1. Under a wide
range of conditions, the probability p(x) of a conformation x, is governed by the Boltzmann dis-
tribution and is proportional to exp{-E (x)/kB T}, where E(x) ∈ R is the conformation’s energy,
kB is the Boltzmann constant, and T is the temperature.
To compute a molecular property for a molecule, one must sample from p(x). The main approach is
to start with one conformation and make small changes to it over time, e.g., by using Markov chain
Monte Carlo (MCMC) or molecular dynamics (MD). These methods can be used to accurately
sample equilibrium states of molecules, but they become computationally expensive for larger ones
(Shim & MacKerell, 2011; Ballard et al., 2015; De Vivo et al., 2016). Other heuristic approaches ex-
ist in which distances between atoms are set to fixed idealized values (Havel, 2002; Blaney & Dixon,
2007). Several methods based on statistical learning have also recently been developed to tackle the
issue of conformation generation. However, they are mainly geared towards studying proteins and
their folding dynamics (AlQuraishi, 2019). Some of these models are not targeting a distribution
over conformations but the most stable folded configuration (Evans et al., 2018; Ingraham et al.,
2019), while others are not transferable between different molecules (Lemke & Peter, 2019; Noe
et al., 2019).
This work includes the following key contributions:
•	We introduce a novel probabilistic model for learning conformational distributions of
molecules with graph neural networks.
1
Under review as a conference paper at ICLR 2020
Figure 1: Standard graph representation of a molecule (left) with a set of possible conformations
{xi } (right). Hydrogen (H), carbon (C), and oxygen (O) atoms are colored white, gray, and red,
respectively. Conformations feature the same atom types and bonds but the atoms are arranged
differently in space. These differences arise from rotations around and stretching of bonds in the
molecule.
•	We create a new, challenging benchmark for conformation generation, which is made pub-
licly available. To the best of our knowledge, this is the first benchmark of this kind.
•	By combining a conditional variational autoencoder (CVAE) with an Euclidean distance
geometry (EDG) algorithm we present a state-of-the-art approach for generating one-shot
samples of molecular conformations for unseen molecules that is independent of their size
and shape.
•	We develop a rigorous experimental approach for evaluating and comparing the accuracy of
conformation generation methods based on the mean maximum deviation distance metric.
•	We show how this generative model can be used as a proposal distribution in an importance
sampling (IS) scheme to estimate molecular properties.
2	Method
Our goal is to build a statistical model that generates molecular conformations in a one-shot fashion
from a molecule’s graph representation. First, we describe how a molecule’s conformation can be
represented by a set of pairwise distances between atoms and why this presentation is advantageous
over one in Cartesian coordinates (Section 2.1). Second, we present a generative model in Sec-
tion 2.2 that will generate sets of atomic distances for a given molecular graph. Third, we explain in
Section 2.3 how a set of predicted distances can be transformed into a molecular conformation and
why this transformation is necessary. Finally, we detail in Section 2.4 how our generative model can
be used as a proposal distribution in an IS scheme to estimate molecular properties.
2.1	Extended Molecular Graphs and Distance Geometry
In this study, a molecule is represented by an undirected graph which is defined as a tuple G =
(V, E). V = {vi }iN=v1 is the set of nodes representing atoms, where each vi ∈ RFv holds atomic
attributes (e.g., the element type i). E = {(ek, rk, sk)}kN=e 1 is the set of edges, where each ek ∈ RFe
holds an edge’s attributes (e.g., the bond type), and rk and sk are the nodes an edge is connecting.
Here, E represents the molecular bonds (and the auxiliary edges which are explained below) in the
molecule.
We assume that, given a molecular graph G, one can represent one of its conformations x by a set
of atomic distances d = {dk}kN=e 1, where dk = |rrk - rsk | is the Euclidean distance between the
positions of the atoms rk and sk in this conformation. As the set of edges between the bonded atoms
(Ebond) alone would not suffice to describe a conformation, we expand the traditional graph repre-
sentation of a molecule by adding auxiliary edges. Auxiliary edges between atoms that are second
neighbors in the original graph fix angles between atoms, and those between third neighbors fix dihe-
dral angles (denoted Eangle and Edihedral, respectively). In this work, Eangle consists of edges between
all second neighbors in the original graph. Edges between third neighbors are added according to
a heuristic (see Appendix A.1). From now on we are always referring to this extended molecular
graph when talking about molecular graphs. In Fig. 2, the process of extending the molecular graph
and the extraction of d from x and G are illustrated.
2
Under review as a conference paper at ICLR 2020
Figure 2: A) The structural formula of a molecule is converted to an extended molecular graph G
consisting of nodes representing atoms (circles, e.g., v1) and edges representing molecular bonds
(solid lines, e.g., e1 ∈ Ebond) and auxiliary edges (dotted lines, e.g., e2 ∈ Eangle and e3 ∈ Edihedral).
B) The distances d are extracted from a conformation x based on the edges E. C) Graphical model
of the variational autoencoder: generative model pθ(d|z, G)pθ(z|G) (solid lines) and variational
approximation qφ(z|d, G) (dashed lines).
d ππ□...
dl⅛⅛
A key advantage of a representation in terms of distances is its invariance to rotation and trans-
lation; by contrast, Cartesian coordinates depend on the (arbitrary) choice of origin, for example.
In addition, it reflects pair-wise physical interactions and their generally local nature. Auxiliary
edges can be placed between higher-order neighbors depending on how far the physical interactions
dominating the potential energy of the system reach.
We have a set of NG molecular graphs {Gl}lN=G1. Further, for each Gl, we have Sl conformational
samples {xl,j}jS=l 1 from the ground-truth distribution resulting in Sl sets of distances {dl,j}jS=l 1.
With this data, we will train a generative model which we detail in the following section.
2.2	Generative Model
We employ a CVAE (Kingma & Welling, 2014; Pagnoni et al., 2018) to model the distribution over
distances d given a molecular graph G. A CVAE first encodes G together with d into a latent space
Z ∈ RkNv, where k ∈ N+, with an encoder qφ(z|d, G). Subsequently, the decoder pθ(d|z, G)
decodes z back into a set of distances. A graphical model is shown in Fig. 2 C).
A conformation has, in general, 3Nv - 6 spatial degrees of freedom (dofs): one dof per spacial
dimension per atom minus three translational and three rotational dofs. Therefore, the latent space
should be proportional to the number of atoms in the molecule. In addition, the latent space should be
smaller than 3Nv as it is the role of the encoder to project the conformation into a lower-dimensional
space. As a result, we set k = 1 to avoid overfitting.1
Here, qφ(z∣d, G) and pθ(d|z, G) are Gaussian distributions, the mean and variance of which are
modeled by two artificial neural networks. At the center of this model are message-passing neural
networks (MPNNS) (Gilmer et al., 2017) with multi-head attention (Velickovic et al., 2018). In short,
an MPNN is a convolutional neural network that allows end-to-end learning of prediction pipelines
whose inputs are graphs of arbitrary size and shape. In a convolution, neighboring nodes exchange
so-called messages between neighbors to update their attributes. Edges update their attributes with
the features of the nodes they are connecting. The MPNN is a well-studied technique that achieves
state-of-the-art performance in representation learning for molecules (Kipf & Welling, 2017; Duve-
naud et al., 20l5; Kearnes et al., 2θ16; Schutt et al., 2017b; Gilmer et al., 2017; Kusner et al., 2017;
Bradshaw et al., 2019a).
In the following, we describe the details of the model.2 In Fig. 3, an illustration of the model is
shown. In the encoder qφ(z∣d, G), each dk is concatenated with the respective edge feature ek to give
e0k ∈ RFe+1. Then, each vi and each e0k are passed to Fenc,v and Fenc,e (two multilayer perceptrons,
1Experiments showed that our model performs similarly with a latent space of R2Nv and overfits with latent
spaces of R3Nv .
2The full model (including all parameters) is available online https://figshare.com/s/
1b42bf865bd78c457354
3
Under review as a conference paper at ICLR 2020
Figure 3: The molecular graph G together with the distances d are passed through the model con-
Sisting of an encoder qφ(z∣d, G) and a decoder pθ (d|z, G). See the main text for details.
MLPs), respectively, to give Ge(n0c), where Ge(ntc) = ({vi(,te)nc}iN=v1,{(e(kt,)enc,rk,sk)}kN=e1), vi(,te)nc ∈ RLv,
and e(kt,)enc ∈ RLe . Then, T MPNNs of depth 1, {MPe(tn)c}tT=1, are consecutively applied to obtain
G(T). Finally, the read-out function RenC (an MLP) takes each V(TnC to predict the mean μzi ∈ R
and the variance σz2i ∈ R of the Gaussian distribution for zi . The so-called reparametrization trick
is employed to draw a sample for zi . In summary,
vi,enc = Fenc,v (vi),	ek,enc = Fenc,e(e0i),
Ge(n1c) =MP(en0c)(Ge(n0c)),	Ge(ntc+1) =MP(etn)c(Ge(ntc)),	Ge(nTc) =MP(enTc-1)(Ge(nTc-1)),	(1)
μZi ,σZi = RenC(Vi,enC).
In the decoder pθ(d∣z, G), each Zi is concatenated with the respective node feature Vi to give Vi ∈
RFv+1. Each Vi0 and each ek are passed to Fdec,v and Fdec,e (two MLPs), respectively, to give Gd(e0c),
where Gd(etc) = ({Vi(,td)ec}iN=v1,{(e(kt,)dec,rk,sk)}kN=e1), Vi(,td)ec ∈ RLv, and e(kt,)dec ∈ RLe. Then, T MPNNs
of depth 1, {MP(dte)c}tT=1, are consecutively applied to obtain Gd(eTc). Finally, the read-out function Rdec
(an MLP) takes each ekTec to predict the mean μdk ∈ R and the variance σ2fc ∈ R of the Gaussian
distribution for dk. In summary,
Vi(,0d)ec = Fdec,v(Vi0),	e(k0,d)ec = Fdec,e(ei),
Gd(e1c) = MP(d0ec)(Gd(e0c)),	Gd(etc+1) = MP(dte)c(Gd(etc)),	Gd(eTc) = MP(dTec-1)(Gd(eTc-1)),	(2)
μdk ,σ2k = Rdec(ek,dec).
The sets of parameters in the encoder and decoder, φ and θ (i.e., parameters in Fenc,v, Fenc,e,
{MPe(tn)c}tT=1, Renc, Fdec,v, Fdec,e, {MP(dte)c}tT=1, Rdec), respectively, are optimized by maximizing
the evidence lower bound (ELBO):
L = EZ〜qφ(z∣d,G)[logPθ(d|z, G)] - Dkl[qφ(z|d, G)∣∣Pθ(z|G)],
(3)
where the prior pθ(z|G) consists of factorized Gaussians. The optimal values for the hyperpa-
rameters for the network dimensions, number of message passes, batch size, and learning rate of
the Adam optimizer (Kingma & Ba, 2014) were tuned by maximizing the validation performance
(ELBO) with a Bayesian optimizer and are reported in Appendix A.1.3.
4
Under review as a conference paper at ICLR 2020
2.3	Conformation Generation through Euclidean Distance Geometry
To compute molecular properties, quantum-chemical methods need to be employed which require
the input, i.e., the molecule, to be in Cartesian coordinates.3 Therefore, we use an EDG algorithm
to translate the set of distances {dk}kN=e1 to a set of atomic coordinates {ri}iN=v1.4
EDG is the mathematical basis for a geometric theory of molecular conformation. In the field of
machine learning, Weinberger & Saul (2006) used it for learning image manifolds, Tenenbaum
et al. (2000) for image understanding and handwriting recognition, Jain & Saul (2004) for speech
and music, and Demaine et al. (2009) for music and musical rhythms. An EDG description of
a molecular system consists of a list of lower and upper bounds on the distances between pairs
of atoms {(dk,min, dk,max)}N=「Here, pθ(d|z, G) is used to model these bounds, namely, We set
the bounds to {(μdk -仃壮卜,μ&卜 + σdk)}, where μdk and σdk are the mean and standard deviation
for each distance dk given by the CVAE. Then, an EDG algorithm determines a set of Cartesian
coordinates {ri}iN=v1 so that these bounds are fulfilled (see Appendix A.2 for details).5 Together with
the corresponding chemical elements {i}iN=v1, we obtain a conformation x.
2.4	Calculation of Molecular Properties
We can get an MC estimate of the expectation EG [O] of a property O (e.g., the dipole moment)
for a molecule represented by G by drawing conformational samples Xi 〜p(x∣G) and computing
O(xi) ∈ R with a quantum-chemical method (e.g., density functional theory). Since we cannot draw
samples from p(X|G) directly, we employ an IS integration scheme (Bishop, 2009) with our CVAE
as the proposal distribution. We assume that we can readily evaluate the unnormalized probability of
a conformation p(x∣G) = exp{-E(x)∕kB T}, where X mustbe a conformation of the molecule and
the energy E(X) is determined with a quantum-chemical method. Since the EDG algorithm is map-
ping the distribution pθ (d∣z, G) to a point mass in R3Nv, the MC estimate for the resulting distribu-
tionpprop(X∣G) is given by a mixture of delta functions, each of which is centered at the Xi resulting
from mapping p(d∣Zi, G) to R3Nv, where Zi 〜p(z∣G), that is, pprop(x∣G) ≈ N PN=I δ(x - x%).
The IS estimator for the expectation of O w. r.t. p(x∣G) then reads
人一-
E G [O]
MC 1 XXO(X)身 1 XXO(XI P(XiIG)
≈ N i=1 O(Xi) = N ⅛ O(Xi)PpropRm,
(4)
where Xi 〜P(x∕G) and Xi 〜Pprop(X0i∣G), so that the expectation of O w. r. t. the normalized
version of P(X) is then
^c∣^cι ι NL
EG[O] =EGy ≈ 3 ΣO(Xi)P(XiIG),	(5)
EG [1]	i=1
where Z ≈ PN=IP(Xi) and N is the number of samples. When dividing two delta functions we
have assumed that they take some arbitrarily large finite value.
3	Related Works
The standard approach for generating molecular conformations is to start with one, and make small
changes to it over time, e.g., by using MCMC or MD. These methods are considered the gold
standard for sampling equilibrium states, but they are computationally expensive, especially if the
molecule is large and the Hamiltonian is based on quantum-mechanical principles (Shim & MacK-
erell, 2011; Ballard et al., 2015; De Vivo et al., 2016).
A much faster but more approximate approach for conformation generation is EDG (Havel, 2002;
Blaney & Dixon, 2007; Lagorce et al., 2009; Riniker & Landrum, 2015). Lower and upper distance
3 Even though quantum-chemical methods require the input to be in Cartesian coordinates, calculated prop-
erties, such as the energy, are invariant under translation and rotation.
4There are additional constraints due to chirality. However, since they are given by G and are fixed, they are
not modeled by our method.
5Often there exist multiple solutions for the same set of bounds. As the bounds are generally tight, the
solutions are very similar. Therefore, we only generate one set of coordinates per set of bounds.
5
Under review as a conference paper at ICLR 2020
bounds for pairs of atoms in a molecule are fixed values based on ideal bond lengths, bond angles,
and torsional angles. These values are often extracted from crystal structure databases (Allen, 2002).
These methods aim to generate a low-energy conformation, not to generate unbiased samples from
the underlying distribution at a certain temperature.
There exist several machine learning approaches as well, however, they are mostly tailored towards
studying protein dynamics. For example, Noe et al. (2019) trained Boltzmann generators on the
energy function of proteins to provide unbiased, one-shot samples from their equilibrium states.
This is achieved by training an invertible neural network to learn a coordinate transformation from
a system’s configurations to a latent space representation. Further, Lemke & Peter (2019) proposed
a dimensionality reduction algorithm that is based on a neural network autoencoder in combination
with a nonlinear distance metric to generate samples for protein structures. Both models learn
protein-specific coordinate transformations that cannot be transferred to other molecules.
AlQuraishi (2019) introduced an end-to-end differentiable recurrent geometric network for protein
structure learning based on amino acid sequences. Also, Ingraham et al. (2019) proposed a neural
energy simulator model for protein structure that makes use of protein sequence information. In
contrast to amino acid sequences, molecular graphs are, in general, not linear but highly branched
and often contain cycles. This makes them unsuitable for recurrent networks.
Finally, Mansimov et al. (2019) presented a conditional deep generative graph neural network to
generate molecular conformations given a molecular graph. Their goal is to predict the most likely
conformation and not a distribution over conformations. Instead of encoding molecular environ-
ments in atomic distances, they work directly in Cartesian coordinates. As a result, the generated
conformations showed significant structural differences compared to the ground-truth and required
refinement through a force field, which is often employed in MD simulations.
We argue that our model has several advantages over the approaches reviewed above:
•	It is a fast alternative to resource-intensive approaches based on MCMC or MD.
•	Our principled representation based on pair-wise distances does not restrict our approach
to any particular molecular structure.
•	Since our model employs message-passing neural networks, it is transferable - it can ex-
trapolate from only a few graphs to unseen ones.
4	The Conf17 B enchmark
The Conf17 benchmark is the first benchmark for molecular conformation sampling.6 It is based
on the ISO17 dataset (Schutt et al., 2017a) which consists of conformations of various molecules
with the atomic composition C7H10O2 drawn from the QM9 dataset (Ramakrishnan et al., 2014).
These conformations were generated by ab initio molecular dynamics simulations at 500 Kelvin
which generates trajectories of a single molecule covering a large variety of conformations. The
Conf17 benchmark consists of 127 distinct molecular graphs each with 3380 conformations on
average. We split this dataset into multiple training and test splits, each consisting of 107 and 20
graphs, respectively (see Appendix A.1 for more details).7
In Fig. 4, (A), the structural formulae of a random selection of molecules from this benchmark
are shown. Most molecules feature highly-strained, complex 3D structures such as rings which
are typical of drug-like molecules. It is thus the structural complexity of the molecules, not their
number of degrees of freedom, that makes this benchmark challenging. In Fig. 4, (B), the frequency
of distances (in A) in the conformations are shown for each edge type. It can be seen that the
marginal distributions of the edge distances are multimodal and highly context dependent.
6Datasets such as the one published by Kanal et al. (2018) only include conformers, i.e., the stable confor-
mations of a molecule, and not a distribution over conformations.
7The Conf17 benchmark is publicly available online https://figshare.com/s/
1b42bf865bd78c457354.
6
Under review as a conference paper at ICLR 2020
HO
HO'、'
CH3
Il
A
H3C
H3C A≡
OAO
1e3
Z-O
HO
HO
(
eo'
1500 -
1000 -
⊂≡ H-C
I=I H-O
I=I C-C
lɪl C-O
1.0	1.5
Distance [A]
1e3
2000 -
1500 -
1000 -
500 -
0 -ɪ-
-HH-0;0
HHHCCO
≡i□
1e3
8 6 4 2
4unou
0
2	3	2	3	4
Distance [A]	Distance [A]
B
IUnoU
IUnoU
Figure 4: Overview of the Conf17 benchmark. (A) Structural formulae of a random selection of
molecules. (B) Distribution of distances (in A) grouped by edge (from left to right: Ebond, Eangle,
and Edihedral) and vertex type (chemical element).
5	Experiments
We assess the performance of our method, named Graph Distance Geometry (GraphDG), by com-
paring it with two state-of-the-art methods for molecular conformation generation: RD Kit (Riniker
& Landrum, 2015), a classical EDG approach, and DL4Chem (Mansimov et al., 2019), a machine
learning approach. We trained GraphDG and DL4Chem on three different training and test splits
of the CONF17 benchmark using Adam (Kingma & Ba, 2014). We generated 3000 conformations
with each method for molecular graphs in a test set.
5.1	Distributions Over Distances
We assessed the accuracy of the distance distributions of RDKit, DL4Chem, and GraphD G by
calculating the maximum mean discrepancy (MMD) (Gretton et al., 2012) to the ground-truth dis-
tribution. We compute the MMD using a Gaussian kernel, where we set the standard deviation to be
the median distance between distances d in the aggregate sample. For this, we determined the dis-
tances in the conformations from the ground-truth and those generated by RDKit and DL4Chem.
For each train-test split and each G in a test set, we compute the MMD of the joint distribution of
distances between C and O atoms (H atoms are usually ignored), the MMDs of pair-wise distances
p(di, dj |G), and the MMDs between the marginals of individual distances p(di|G). We aggregate
the results of three train-test splits, and, finally, compute the median MMDs and average rankings.
The results are summarized in Table 1. It can be seen that the samples from GraphD G are signifi-
cantly closer to the ground-truth distribution than the other methods. RDKit is slightly worse than
GraphDG while DL4 Chem seems to struggle with the complexity of the molecules and the small
number of graphs in the training set.
In Fig. 5, we showcase the accuracy of our model by plotting the marginal distributions p(di|G) for
distances between C and O atoms given a molecular graph from a test set. It can be seen that RDKit
consistently underestimates the marginal variances. This is because this method aims to predict the
most stable conformation, i.e., the distribution’s mode. In contrast, DL4Chem often fails to predict
the correct mean. For this molecule, GraphDG is the most accurate, predicting the right mean and
variance in most cases. Additional figures can be found in the Appendix A.4, where we also show
plots for the marginal distributions p(di, dj|G).
7
Under review as a conference paper at ICLR 2020
Table 1: Assessment of the accuracy of the distributions over conformations generated by three
models compared to the ground-truth. We compare the distributions with respect to the marginals
p(dk|G), p(dk, dl|G), and the distribution over all edges between C and O atoms p({dk}|G). Two
different metrics are used: median MMD between ground-truth conformations and generated ones,
and mean ranking (1 to 3) based on the MMD. Reported are the results for molecular graphs in a
test set from three train-test splits. Standard errors are given in brackets.
Median MMD
RDKit	DL4Chem	GraphDG
Mean Ranking
RDKit	DL4Chem GraphDG
p(dk|G)
p(dk, dl|G)
p({dk}|G)
1.51 (0.03)
1.43 (0.02)
1.45 (0.02)
tnuoC
0.55 (0.01)
0.53 (0.01)
0.60 (0.01)
1.11 (0.01) 0.38 (0.02)
1.09 (0.01) 0.34 (0.01)
1.07 (0.03) 0.44 (0.05)
1.71 (0.03)	2.74 (0.02)
1.66 (0.02)	2.92 (0.01)
1.58 (0.05)	2.90 (0.05)
1000 -
500 -
3Un oɔ
3Unoɔ
0
1.4	1.6 1.5	2.0	1.5	2.0	2.5	1.4	1.6	1.5	2.0
1000
500
1.5	2.0	2.5
2.0	1.0	1.5
I I Ground-truth
匚二I RDKit
⊂≡ DL4Chem
l≡≡l GraphDG
0
1.4	1.6
1000
500
0
1.0
1.5
1.4	1.6	1.5
Figure 5: Marginal distributions p(dk|G) of ground-truth and predicted bond distances (in A) be-
tween C and O atoms given a molecular graph from the test set. The atoms connected by each edge
dk are indicated in each subplot (Sk -rj In the 3D structure of the molecule, carbon and oxygen
atoms are colored gray and red, respectively. H atoms are omitted for clarity.
5.2	Generation of Conformations
We passed the distances from our generative model to an EDG algorithm to obtain conformations.
For 99.9% of the sets of distances, all triangle inequalities held. For 94% of the molecular graphs,
the algorithm succeeded which is 8 pp higher than the success rate we observed for RDKit. For
each molecular graph in a test set, we generated 50 conformations with each method. This took
DL4Chem, RDKit, and GraphD G on average around hundreds of milliseconds per molecule.8
In contrast, a single conformation in the ISO17 dataset takes around a minute to compute. In Fig. 6,
an overlay of these conformations of six molecules generated by the different methods is shown.
It can be seen that RDKit’s conformations show too little variance, while DL4Chem’s structures
are mostly invalid, which is due in part to its failure to predict the correct interatomic angles. Our
method slightly overestimates the structural variance (see, for example, Fig. 6, top row, second
column), but produces conformations that are the closest to the ground-truth.
5.3	Calculation of Molecular Properties
We estimate expected molecular properties for molecular graphs from the test set with N = 50
conformational samples each. Due to their poor quality, we could not compute properties O(x),
including the energy E(x), for conformations generated with DL4CHEM, and thus, this method
is excluded from this analysis. In Table 2, it can be seen that RDKit and GraphDG perform
8All simulations were carried out on a computer equipped with an i7-3820 CPU and a GeForce GTX 1080
Ti GPU.
8
Under review as a conference paper at ICLR 2020
Ground-truth RDKit
DL4Chem
GraPhDG
Ground-truth
RDKit
DL4Chem
GraphDG
Figure 6: Overlay of 50 conformations from the ground-truth and three models based on six random
molecular graphs from the test set. C, O, and H atoms are colored gray, red, and white, respectively.
Table 2: Median difference in average properties between ground-truth and RDKit and GraphDG:
total electronic energy Eelec (in kJ/mol), the energy of the HOMO and the LUMO LUMO and LUMO,
respectively (in eV), and the dipole moment μ (in debye). Reported are the results for molecular
graphs from the test set, averaged over three train-test splits. Standard errors are given in brackets.
RDKIT
GraphDG
Eelec
HOMO
LUMO
μ
42.7 (4.3)
0.08 (0.04)
0.15 (0.03)
0.29 (0.05)
58.0 (21.0)
0.10 (0.05)
0.09 (0.05)
0.33 (0.09)
similarly well (see Appendix A.2 for computational details). However, both methods are still highly
inaccurate for Eelec (in practice, an accuracy of less than 5 kJ/mol is required). Close inspection of
the conformations shows that, even though GraphDG predicts the most accurate distances overall,
the variances of certain strongly constrained distances (e.g., triple bonds) are overestimated so that
the energies of the conformations increase drastically.
6	Limitations
The first limitation of this work is that the CVAE can sample (with low probability) invalid sets of
distances for which there exists no 3D structure. Second, the Conf17 benchmark covers only a
small portion of chemical space. Finally, a large set of auxiliary edges would be required to capture
long-range correlations (e.g., in proteins). Future work will address these points.
7	Conclusions
We presented GraphDG, a transferable, generative model that allows sampling from a distribution
over molecular conformations. We developed a principled learning representation of conforma-
tions that is based on distances between atoms. Then, we proposed a challenging benchmark for
comparing molecular conformation generators. With this benchmark, we show experimentally that
conformations generated by GraphDG are closer to the ground-truth than those generated by other
methods. Finally, we employ our model as a proposal distribution in an IS integration scheme to
estimate molecular properties. While orbital energies and the dipole moments were predicted well,
a larger and more diverse dataset will be necessary for meaningful estimates of electronic energies.
Further, methods have to be devised to estimate how many conformations need to be generated
to ensure all important conformations have been sampled. Finally, our model could be trained on
conformational distributions at different temperatures in a transfer learning-type setting.
9
Under review as a conference paper at ICLR 2020
References
F. H. Allen. The Cambridge Structural Database: A quarter of a million crystal structures and rising.
Acta Crystallogr., Sect. B: Struct. Sci, 58(3):380-388, 2002. doi: 10.1107/S0108768102003890.
Mohammed AlQuraishi. End-to-End Differentiable Learning of Protein Structure. Cell Systems, 8
(4):292-301.e3, 2019. doi: 10.1016/j.cels.2019.03.006.
Andrew J. Ballard, Stefano Martiniani, Jacob D. Stevenson, Sandeep Somani, and David J. Wales.
Exploiting the potential energy landscape to sample free energy. WIREs Comput. Mol. Sci., 5(3):
273-289, 2015. doi: 10.1002/wcms.1217.
Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science and
Statistics. Springer, New York, 8 edition, 2009. ISBN 978-0-387-31073-2.
Jeffrey M. Blaney and J. Scott Dixon. Distance Geometry in Molecular Modeling. In Reviews in
Computational Chemistry, pp. 299-335. John Wiley & Sons, Ltd, 2007. ISBN 978-0-470-12582-
3. doi: 10.1002/9780470125823.ch6.
John Bradshaw, Matt J. Kusner, Brooks Paige, Marwin H. S. Segler, and Jose MigUel Hernandez-
Lobato. A generative model for electron paths. In International Conference on Learning Repre-
sentations, 2019a.
John Bradshaw, Brooks Paige, Matt J. Kusner, Marwin H. S. Segler, and Jose Miguel Hernandez-
Lobato. A Model to Search for Synthesizable Molecules. arXiv:1906.05221, 2019b.
Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoen-
coder for structured data. In International Conference on Learning Representations, 2018.
Marco De Vivo, Matteo Masetti, Giovanni Bottegoni, and Andrea Cavalli. Role of Molecular Dy-
namics and Related Methods in Drug Discovery. J. Med. Chem., 59(9):4035-4061, 2016. doi:
10.1021/acs.jmedchem.5b01684.
Erik D. Demaine, Francisco Gomez-Martin, Henk Meijer, David Rappaport, Perouz Taslakian, God-
fried T. Toussaint, Terry Winograd, and David R. Wood. The distance geometry of music. Com-
putational Geometry, 42(5):429-454, 2009. doi: 10.1016/j.comgeo.2008.04.005.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional Networks on Graphs for Learning Molecular
Fingerprints. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 28, pp. 2224-2232. Curran Associates, Inc.,
2015.
R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. F. G. Green, C. Qin, A. Zidek, A. Nelson,
A. Bridgland, H. Penedones, S. Petersen, K. Simonyan, S. Crossan, D. T. Jones, D. Silver,
K. Kavukcuoglu, D. Hassabis, and A. W. Senior. De novo structure prediction with deep-learning
based scoring. In Thirteenth Critical Assessment of Techniques for Protein Structure Prediction,
2018.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning - Volume 70, ICML’17, pp. 1263-1272, 2017.
Rafael Gomez-Bombarelli, Jennifer N. Wei, David Duvenaud, JoSe Miguel Hernandez-Lobato,
Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel,
Ryan P. Adams, and Alan Aspuru-Guzik. Automatic Chemical Design Using a Data-Driven
Continuous Representation of Molecules. ACS Cent. Sci., 4(2):268-276, 2018. doi: 10.1021/
acscentsci.7b00572.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A Kernel Two-Sample Test. J. Mach. Learn. Res., 13:723-773, 2012.
Wolfgang Guba, Agnes Meyder, Matthias Rarey, and Jerome Hert. Torsion Library Reloaded:
A New Version of Expert-Derived SMARTS Rules for Assessing Conformations of Small
Molecules. J. Chem. Inf. Model., 56(1):1-5, 2016. doi: 10.1021/acs.jcim.5b00522.
10
Under review as a conference paper at ICLR 2020
Timothy F. Havel. Distance Geometry: Theory, Algorithms, and Chemical Applications. In Encyclo-
pedia of Computational Chemistry. American Cancer Society, 2002. ISBN 978-0-470-84501-1.
doi: 10.1002/0470845015.cda018.
John Ingraham, Adam Riesselman, Chris Sander, and Debora Marks. Learning Protein Structure
with a Differentiable Simulator. In International Conference on Learning Representations, 2019.
V. Jain and L. K. Saul. Exploratory analysis and visualization of speech and music by locally linear
embedding. In 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing,
volume 3, pp. iii-984, 2004. doi:10.1109/ICASSP.2004.1326712.
Jan Jensen. XYZ2Mol. https://github.com/jensengroup/xyz2mol, 2019.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research,pp. 2323-2332, Stockholm, Sweden, 2018. PMLR.
Ilana Y. Kanal, John A. Keith, and Geoffrey R. Hutchison. A sobering assessment of small-molecule
force field methods for low energy conformer predictions. Int. J. Quantum Chem., 118(5):e25512,
2018. doi: 10.1002/qua.25512.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: Moving beyond fingerprints. J. Comput.-Aided Mol. Des., 30(8):595-608, 2016.
doi: 10.1007/s10822-016-9938-8.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization.
arXiv:1412.6980, 2014.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Confer-
ence on Learning Representations, 2014.
Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Net-
works. International Conference on Learning Representations, 2017.
Matt J. Kusner, Brooks Paige, and Jose Miguel Hernandez-Lobato. Grammar Variational Autoen-
coder. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Con-
ference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1945-1954, International Convention Centre, Sydney, Australia, 2017. PMLR.
David Lagorce, Tania Pencheva, Bruno O. Villoutreix, and Maria A. Miteva. DG-AMMOS: A New
tool to generate 3D conformation of small molecules using Distance Geometry and Automated
Molecular Mechanics Optimization for in silico Screening. BMC Chem. Biol., 9:6, 2009. doi:
10.1186/1472-6769-9-6.
Tobias Lemke and Christine Peter. EncoderMap: Dimensionality Reduction and Generation of
Molecule Conformations. J. Chem. Theory Comput., 2019. doi: 10.1021/acs.jctc.8b00975.
Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained Graph Varia-
tional Autoencoders for Molecule Design. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 7795-7804. Curran Associates, Inc., 2018.
Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry pre-
diction using a deep generative graph neural network. arXiv:1904.00314 [physics, stat], 2019.
Frank Noe, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators: Sampling equilibrium
states of many-body systems with deep learning. Science, 365(6457):eaaw1147, 2019. doi:
10.1126/science.aaw1147.
Artidoro Pagnoni, Kevin Liu, and Shangyan Li. Conditional Variational Autoencoder for Neural
Machine Translation. arXiv:1812.04405, 2018.
11
Under review as a conference paper at ICLR 2020
John P. Perdew, Matthias Ernzerhof, and Kieron Burke. Rationale for mixing exact exchange with
density functional approximations. J. Chem. Phys., 105(22):9982-9985, 1996. doi: 10.1063/1.
472933.
Raghunathan Ramakrishnan, Pavlo O. Dral, Matthias Rupp, and O. Anatole von Lilienfeld. Quan-
tum chemistry structures and properties of 134 kilo molecules. Sci. Data, 1:140022, 2014. doi:
10.1038/sdata.2014.22.
Sereina Riniker and Gregory A. Landrum. Better Informed Distance Geometry: Using What We
Know To Improve Conformation Generation. J. Chem. Inf. Model., 55(12):2562-2574, 2015. doi:
10.1021/acs.jcim.5b00654.
Kristof Schutt, Pieter-Jan Kindermans, HUziel Enoc SaUceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert Muller. SchNet: A continuous-filter convolutional neural net-
work for modeling quantum interactions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 30, pp. 991-1001. Curran Associates, Inc., 2017a.
KristofT. Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus R. Muller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural networks. Nat. Commun., 8:13890, 2017b.
doi: 10.1038/ncomms13890.
Marwin H. S. Segler, Thierry Kogej, Christian Tyrchan, and Mark P. Waller. Generating Focused
Molecule Libraries for Drug Discovery with Recurrent Neural Networks. ACS Cent. Sci., 4(1):
120-131, 2018. doi: 10.1021/acscentsci.7b00512.
Jihyun Shim and Alexander D. MacKerell, Jr. Computational ligand-based rational design: Role
of conformational sampling and force fields in model development. Med. Chem. Commun., 2(5):
356-370, 2011. doi: 10.1039/C1MD00044F.
Qiming Sun, Timothy C. Berkelbach, Nick S. Blunt, George H. Booth, Sheng Guo, Zhendong Li,
Junzi Liu, James D. McClain, Elvira R. Sayfutyarova, Sandeep Sharma, Sebastian Wouters, and
Garnet Kin-Lic Chan. PySCF: The Python-based simulations of chemistry framework. WIREs
Comput. Mol. Sci., 8(1), 2018. doi: 10.1002/wcms.1340.
Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A Global Geometric Frame-
work for Nonlinear Dimensionality Reduction. Science, 290(5500):2319-2323, 2000. doi:
10.1126/science.290.5500.2319.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph Attention Networks. In International Conference on Learning Representations,
2018.
Florian Weigend. Accurate Coulomb-fitting basis sets for H to Rn. Phys. Chem. Chem. Phys., 8(9):
1057-1065, 2006. doi: 10.1039/B515623H.
Florian Weigend and Reinhart Ahlrichs. Balanced basis sets of split valence, triple zeta valence and
quadruple zeta valence quality for H to Rn: Design and assessment of accuracy. Phys. Chem.
Chem. Phys., 7(18):3297-3305, 2005. doi: 10.1039/B508541A.
Kilian Q. Weinberger and Lawrence K. Saul. Unsupervised Learning of Image Manifolds
by Semidefinite Programming. Int. J. Comput. Vision, 70(1):77-90, 2006. doi: 10.1007/
s11263-005-4939-z.
Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph Convolutional
Policy Network for Goal-Directed Molecular Graph Generation. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Infor-
mation Processing Systems 31, pp. 6410-6421. Curran Associates, Inc., 2018.
12
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Conf17 Benchmark
A.1.1 Data Generation
The ISO17 dataset (Schutt et al., 2017a) was processed in the following way. First, conformations
in which the molecular connectivity was modified (i.e., bonds were broken or new ones are formed)
were discarded. For this, the tool XYZ2Mol (Jensen, 2019) was employed. Second, the molecular
graphs were augmented by adding auxiliary edges for reasons described in Section 2.1. Auxiliary
edges between all second neighbors were added. This can lead to a slight over-specification of the
system’s geometry, however, this did not pose a problem in our experiments. In addition, auxiliary
edges between third neighbors were added to fix dihedral angles. Since there are potentially many
ways of specifying a dihedral angle in a molecular system, we resorted to the works of Riniker &
Landrum (2015) and Guba et al. (2016) to decide where to place edges between third neighbors.
A.1.2 Input Features
Below we list the node and edges features in the Conf17 benchmark.
Table 3: Node features.
Feature	Data Type	Dimension
atomic number	integer	1
chiral tag	one-hot (R, S,	and N/A)	3
Table 4: Edge features.
Feature	Data Type	Dimension
kind	one-hot (indicating whether e is in Ebond, Eangle, or Edihedral)	3
stereo chemistry	one-hot (E, Z, Any, None, and N/A)	5
type	integer (single, double, triple or N/A)	1
is aromatic	binary	1
is conjugated	binary	1
is in ring of size	one-hot (3, 4, . . . , 9) and N/A	8
A.1.3 Model Architecture
The full model is available online https://figshare.com/s/1b42bf865bd78c457354.
In following, the hyperparameters of our model are specified:
Activations throughout this paper: ReLU; Lv, Le: 10; Fenc,v: neural network with depth 2, width 20;
Fenc,e: neural network with depth 3, width 60; Fdec,v, Fdec,e: neural networks with depth 2, width
70; {MPe(tn)c}tT=1, {MP(dte)c}tT=1: MPNN width depth 1 and three multi-head attention heads, T = 3,
for node and edge updates neural networks with depth 2, and width 70 were used. Renc, Rdec: neural
networks with depth 2, width 70. Batch size: 16 (conformations);
A.2 Computational Details
A.2. 1 Quantum-Chemical Calculations
All quantum-chemical calculations were carried out with the PySCF program package (version 1.5)
(Sun et al., 2018) employing the exchange-correlation density functional PBE (Perdew et al., 1996),
and the def2-SVP (Weigend & Ahlrichs, 2005; Weigend, 2006) basis set.
13
Under review as a conference paper at ICLR 2020
Conformations generated by DL4Chem did not succeed as some atoms were too close to each other.
Self-consistent field algorithms in quantum-chemical software such as PySCF do not converge for
such molecular structures.
With quantum-chemical methods, we calculate several properties that concern the states of the elec-
trons in the conformation. These are the total electronic energy Eelec, the energy of the electron in
the highest occupied molecular orbital (HOMO in eV) HOMO, the energy of the lowest unoccupied
molecular orbital (LUMO in eV) ∈l∏mo, and the norm of the dipole moment μ (in debye).
A.2.2 Euclidean Distance Geometry
We refer the reader to Havel (2002) for theory on EDG, algorithms, and chemical applications. In
summary, the EDG procedure consists of the following three steps:
1.	Bound smoothing: extrapolating a complete set of lower and upper limits on all the dis-
tances from the sparse set of lower and upper bounds.
2.	Embedding: choosing a random distance matrix from within these limits, and computing
coordinates that are a certain best-fit to the distances.
3.	Optimization: optimizing these coordinates versus an error function which measures the
total violation of the distance (and chirality) constraints.
We use the EDG implementation found in RDKit (Riniker & Landrum, 2015) with default settings.
A.3 Generation of Conformations
Ground-truth RDKit
DL4Chem GraphDG
Figure 7: Overlay of 50 conformations from the ground-truth, RDKit, DL4Chem, and GraphDG
based on two random molecular graphs from the test set. C, O, and H atoms are colored gray, red,
and white, respectively.
A.4 Distributions over Distances
Below, the marginal distributions of the distances for a variety of molecular graphs are shown.
14
Under review as a conference paper at ICLR 2020
auno。
auno。
0
1000
1000 -
500 -
0
1.25 1.50 1.75 1
1000
2 1.0	1.5
1.0	1.5	1	2	1.0	1.5
1.5 2.0	1
500
auno。 auno。
2	31
2	1.0	1.5	2
3 2.25 2.50	2.25 2.50 2.750	1	2	1.5 2.0 2.5
500
1	2	3 1	2	2.252.502.75	2.0	2.5
I I Ground-truth
匚二I RDKit
I=I DL4Chem
I=I GraphDG
Figure 8: Marginal distributions p(dk |G) of ground-truth and predicted distances (in A) between C
and O atoms given a molecular graph from the test set. The atoms connected by each edge dk are
indicated in each subplot (SkTk). In the 3D structure of the molecule, carbon and oxygen atoms are
colored gray and red, respectively. H atoms are omitted for clarity.
15
Under review as a conference paper at ICLR 2020
Figure 9: Marginal distributions p(di, dj|G) of ground-truth and predicted distances for a molecular
graph from the test set (in A). Here, di and dj are restricted to edges representing bonds between C
and O atoms. In the 3D structure of the molecule, carbon and oxygen atoms are colored gray and
red, respectively. H atoms are omitted for clarity.
16
Under review as a conference paper at ICLR 2020
4junou
4junou
4junoo4junou
Figure 10: See caption of Fig. 8
17
Under review as a conference paper at ICLR 2020
18
Under review as a conference paper at ICLR 2020
auno。
1.25 1.50 1.75 1	2 1.0	1.5	1.0	1.5	1	2	1.0	1.5	1.5	2.0	1	2
auno。
auno。 auno。
Figure 12: See caption of Fig. 8
19
Under review as a conference paper at ICLR 2020
Figure 13: See caption of Fig. 9
20
Under review as a conference paper at ICLR 2020
Eno。
4
1.0	1.5	1.5	2.0	1	2
1
2.25 2.50 2.750	1	2	1.5 2.0 2.5
0
2
.5
1
00 0
00
10 5
tnuoC
2
5
2
O
2
5
2
2.5 3.0	1	2
2.0	2.5 2	3	1	2
Figure 14: See caption of Fig. 8
2.252.502.75	2.0	2.5
I I Ground-truth
RDK RDKit
I I DL4Chem
Gap GraPhDG
21
Under review as a conference paper at ICLR 2020
22