Under review as a conference paper at ICLR 2020
S oftAdam: Unifying SGD and Adam for better
STOCHASTIC GRADIENT DESCENT
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient descent (SGD) and Adam are commonly used to optimize
deep neural networks, but choosing one usually means making tradeoffs between
speed, accuracy and stability. Here we present an intuition for why the tradeoffs
exist as well as a method for unifying the two in a continuous way. This makes
it possible to control the way models are trained in much greater detail. We show
that for default parameters, the new algorithm equals or outperforms SGD and
Adam across a range of models for image classification tasks and outperforms
SGD for language modeling tasks.
1	Introduction
One of the most common methods of training neural networks is stochastic gradient descent (SGD)
(Bottou et al. (2016)). SGD has strong theoretical guarantees, including convergence in locally
non-convex optimization problems (Lee et al. (2016)). It also shows improved generalization and
stability when compared to other optimization algorithms (Smith & Le (2018)).
There have been various efforts in improving the speed and generalization of SGD. One popular
modification is to use an adaptive gradient (Duchi et al. (2011)), which scales the gradient step size
to be larger in directions with consistently small gradients. Adam, an implementation that combines
SGD with momentum and an adaptive step size inversely proportional to the RMS gradient, has
been particularly successful at speeding up training and solving particular problems (Kingma &
Ba (2014)). However, at other problems it pays a penalty in worse generalization (Wilson et al.
(2017); Keskar & Socher (2017)), and it requires additional modifications to achieve a convergence
guarantee (Reddi et al. (2018); Li & Orabona (2018)).
Here we develop an intuition for adaptive gradient methods that allows us to unify Adam with SGD
in a natural way. The new optimizer, SoftAdam, descends in a direction that mixes the SGD with
Adam update steps. As such, it should be able to achieve equal or better optimization results across
a variety of problems.
1.1	Related Work
Several authors have recently tried to combine Adam and SGD to get the best of both worlds. How-
ever, these have not always enabled better generalization or performance across different problems.
In one study, the optimization algorithm was switched from Adam to SGD during training based on
a scale-free criterion, preventing the addition of a new hyper-parameter (Keskar & Socher (2017)).
The result is that the longer the convolutional networks were trained on Adam, the worse their
generalization performance compared to SGD. The best performance came from switching to SGD
in this case as soon as possible.
Another recent algorithm takes the approach of clipping the large Adam updates to make them more
similar to SGD as the training nears the end (Luo et al. (2019)). However, this approach requires
two new hyper-parameters: the rate at which the training is switched over, and a learning rate for
both SGD and Adam.
Similar to this work, partially adaptive methods (Chen & Gu (2018)) can allow arbitrary mixing
between SGD and Adam. However, in that work the step size is not strictly smaller than the SGD
1
Under review as a conference paper at ICLR 2020
step and so the same guarantees cannot be made about convergence. It is of interest to see whether
there is any advantage over these methods.
Other algorithms have shown improvements on SGD by averaging weights over many steps (Polyak
& Juditsky (1992); Zhang et al. (2019); Izmailov et al. (2018)). These algorithms are complementary
to the algorithm developed here, as they require an underlying algorithm to choose the step direction
at any point.
2	SGD and Adam
The fundamental idea of gradient descent is to follow the path of steepest descent to an optimum.
Stochastic gradient descent enables us to optimize much larger problems by using randomly sub-
sampled training data. The stochastic gradient descent algorithm will minimize the loss function
J (θ; x), which is parameterized by θ and takes as input training data x,
gt — [Vθ J (θ; Xt)]θ=θt-1
θt - θt-1 - agt,
where α is a learning rate that may vary with t and xt is the training data selected for the batch at
step t. The convergence rate can be improved further by using a running average of the gradient,
initializing with m° J 0. This method, known as momentum (Goh (2017)), may be written as,
gt J [VθJ(θ; Xt)]θ=θt-1
mt J β1mt-1 + (1 - β1) gt,
θt J θt-1 - αmt,
A further development that has improved convergence, especially for LSTM and language modeling
tasks, involves the second gradient as well. This specific version is known as the Adam algorithm
(Kingma & Ba (2014)),
gt J [VθJ(θ; Xt)]θ=θt-1
mt J β1mt-1 + (1 - β1) gt,
vt J β2vt-1 + (1 - β2) gt2,
θt j θt-1- -√t
where mt = mt/，1 - βt and Vt = vt/，1 - β2 are unbiased estimators of the first and second
moment respectively.
2.1	Convergence
In order to analyze the convergence of these algorithms, we can consider a second-order approxi-
mation of J on its combined argument z = (θ; X) in the region of (θt; Xt),
J (Z) = 2 ZT Htz — bτ z,
where Ht is the Hessian of J(z) around zt. This gives us the gradient,
gt = VJ (Zt) = Htzt — b,
which becomes the SGD update step,
Zt+1 = Zt — α (HtZ — b) .
2
Under review as a conference paper at ICLR 2020
Unrolling this update step can be shown to lead to an expression for the distance from the optimal
value z?, in the basis of Hessian eigenvectors ξi :
Zt- z* = X (ZO ∙ ξi)(1 - αλi)t ξi.
i
We can see that the learning is stable if the learning rate α satisfies,
|1 — αλi∣ < 1.
In addition, we find that the value for the learning rate that leads to the fastest overall convergence
is,
2
λ1 + λn
(1)
where λ1 and λn are the max and min eigenvalues of H , respectively.
2.2	Adaptive Moment Estimation
If rather than a single learning rate α, we were to use a diagonal matrix D such that the update is,
Zt+1 = Zt - Dgt ,
we may be able to modify the diagonal entries [d0, d1, . . . , dn] such that faster overall convergence
is achieved. For example, in the special case that the Hessian is diagonal, the convergence rate for
the i-th element becomes,
ri = (1 - diλi ) ,
which has the solution di = λi-1. In this situation, if the eigenvalues λi are known, the algorithm
can converge to the minimum in exactly one step. This corresponds with some intuition behind
adaptive moment methods: that taking a step with a “constant” size in every direction toward the
target will reach convergence faster than taking a step proportional to the gradient size.
Because the eigenvalues and eigenvectors not known a priori, for a practical algorithm we must rely
on an approximation to find di. One technique named AdaGrad (Duchi et al. (2011)) prescribes the
diagonal elements:
di =------- .	(2)
i e + PPgi
For building our intuition, we consider the special case where the Hessian is diagonal,
gti = (λiZti - bi) .
Combining this with Equation 2, we compare the AdaGrad coefficient to the optimal value for
di = λi-1, finding,
α1
--，	-〜--.
C + JPt (λizti - bi)2 i
As long as C	αλi , this will be true when,
α
2
b2
〜ɪ2 (Zti — Ji) 〜constant w.r.t. i,
(3)
(4)
which is the sum squared distance of the parameter value Zti to its optimum Zi? = bi /λi . This reveals
some of the tradeoffs being made by AdaGrad. It will perform ideally when all parameters start the
same distance from their targets.
3
Under review as a conference paper at ICLR 2020
This may be true on average if the initializations z0i and optima zi? can be made over the same
subspaces. That is, if zi? is uncorrelated to λi, we can expect this to have good performance on
average.
However, there can be significant errors in both overestimating and underestimating the eigenvalue.
One would expect that for a typical problem bi and λi might be drawn from uncorrelated distribu-
tions. In this case, large values of zi? will be likely to correspond to small values of λi . Since z0 can
only be drawn from the average distribution (no information is known at this point), the estimated
λi is more likely to be large, as the initialization will be far from the optimum. Intuitively, the gradi-
ent is large because the optimum is far from the initialization, but the algorithm mistakes this large
gradient for a large eigenvalue.
On the other hand, when the parameter is initialized close to its optimum, the algorithm will mistak-
enly believe the eigenvalue is small, and so take relatively large steps. Although they do not affect
the overall convergence much on their own (since the parameter is near its optimum), these steps can
add significant noise to the process, making it difficult to accurately measure gradients and therefore
find the optimum in other parameters.
This problem will be significantly worse for Adam, which forgets its initialization with some decay
factor β2 . In that case, as each parameter reaches its optimum, its estimated eigenvalue λi drops
and the step size gets correspondingly increased. In fact, the overall algorithm can be divergent as
each parameter reaches its optimum, as the step size will grow unbounded unless α is scheduled to
decline properly or a bounding method like AMSGrad is used (Reddi et al. (2018)).
In addition, reviewing our earlier assumption of small , these algorithms will perform worse for
small eigenvalues λi < /α. This might be especially bad in Adam where late in training when the
learning rate a 〜Pt (Zti - λi) is small.
We finally note that the Hessians in deep learning problems are not diagonal (Sagun et al. (2016); Li
et al. (2019)). As such, each element might be better optimized by a learning rate that better serves
both its min and max eigenvalues.
3	Smoothed Adaptive Gradient
Overall, this understanding has led us to believe that adaptive moments might effectively estimate
λi when it is large, but might be less effective when it is small. In order to incorporate this infor-
mation about large eigenvalues, as well as optimize the learning rate to account for variation in the
eigenvalues contributing to convergence of a particular component, we consider the an update to Eq.
1,
d _ 心(I+ η
i― λ + ηλi
where X is an average eigenvalue and η is a new hyper-parameter that controls the weighting of the
eigenvalue estimation. Here We have added λ to the numerator so that α does not need to absorb the
changes to the RMS error as it does in Eq. 4. This also recovers the SGD convergence guarantees,
since the step is always within a factor ofη to an SGD step. In addition, this will allow us to recover
SGD with momentum exactly in the limit η → 0. We use the adaptive gradient estimation,
λ 〜	hg2	= Vti
λ 〜昌 + hg2iavg	e2 + Vt ,
where vXt is the mean value of vt , to write,
di = —α (1+n).
1 + η Vti/ (2 + VXt)
One issue with the above estimation is that its variance is very large at the beginning of training (Liu
et al. (2019)). It was suggested that this is the reason that warmup is needed for Adam and shown
4
Under review as a conference paper at ICLR 2020
Algorithm 1 The SoftAdam Algorithm
Input: θ0 ∈ F: initial parameters, {αt > 0}tT=1: learning rate, αwd, β1, β2, η, : other hyperpa-
rameters, Jt (θ): loss function
Output: θT ∈ F
mo, vo J 0	. Initialize moments to 0
for t J 1... T do
gt J Vθ J(Z)	. Find gradient
mt J β1mt-1 + (1 - β1) gt	. Update first moment
β2t J min (β2,l — 1/t)
Vt J β2tvt-1 + (1 — β2t) g2	. Update second moment
Vt J meaneiem [vt]	. Average over all elements
Yt J sqrt h(1 — β2) / (* 1 — β2t)i
dt = α (1 + η)(1+ η — ηr + ηr pvt/ (Vt + &))	. Calculate the denominator
θt = θt-1 — mtdt	. Perform the update
end for
return θT
that rectifying it can make warmup unnecessary. Where Vt is the average of n elements and v∞ the
average of n∞, We define r = ntnt∞a and:
di
α(1 +η)
1 + η — ηr + ηr Jvti (怔 + Vt)
This finally forms the basis for our algorithm.
3.1 Algorithm
Our algorithm differs from Adam in a few other ways. First, the biased gradient estimate is used
rather than the unbiased one. This matches the SGD implementation of momentum, and also avoids
magnifying the large variance of early gradient estimates:
mt = β1mt-1 + (1 — β1) gt.
T	•>∙>♦,♦	.1	1	.	♦	1	1	.	1 ∙	[♦	1	♦	i'i'	A / > ∖	Λ
In addition, the second moment Vt is calculated in an unbiased way using an effective β2(t), or by
abuse of notation β2t :
1
β2t = min I β2,1 —-
ʌ
ʌ
Vt = β2tVt-1 + 1 — β2t gt .
This has a negligable impact on the performance of the algorithm, but makes tracking the moment
over time easier since it does not need to be un-biased later. We then calculate the ratio of the number
of samples nt used to calculate the moment Vt to the steady state number of samples in the average
n∞:
rt
—β2
ʌ
—β2t
We finally note that the weight decay should be calculated separately from this update as in AdamW
(Loshchilov & Hutter (2017)).
5
Under review as a conference paper at ICLR 2020
Table 1: Top-1 validation accuracy (%) on CIFAR-10 for different AdamW, SGD and SoftAdam.
SoftAdam is restricted to η = 1.
ARCHITECTURE	ADAMW	SGD	SOFTADAM
AlexNet	74.55 ± 0.37	77.10 ± 0.50	79.30 ± 0.25
VGG19 (BN)	92.50 ± 0.25	93.52 ± 0.18	93.89 ± 0.12
PreResNet-56	91.93 ± 0.18	94.01 ± 0.05	94.02 ± 0.03
ResNet-110	92.11 ± 0.04	94.54 ± 0.29	94.75 ± 0.15
DenseNet-100	93.25 ± 0.19	95.04 ± 0.08	95.00 ± 0.11
These steps are combined and summarized in Algorithm 1. A reference implementation for the
PyTorch package (Paszke et al. (2017)) is provided in Appendix A.
4 Empirical Results
In order to test the ability of this algorithm to reach better optima, we performed testing on a variety
of different deep learning problems. In these problems we keep η at the default value of 1. Because
the algorithm is the same as SGDM if η = 0 and is comparable to Adam when η = -1, getting
results at least as good as those algorithms is just a matter of parameter tuning. These results are
intended to show that SoftAdam performs remarkably well with a common parameter choice. For
the best performance, the hyper-parameter η and learning rate schedule α should be optimized for a
particular problem.
4.1	CIFAR- 1 0
We trained a variety of networks:1 AlexNet (Krizhevsky et al. (2012)), VGG19 with batch nor-
malization (Simonyan & Zisserman (2014)), ResNet-110 with bottleneck blocks (He et al. (2015)),
PreResNet-56 with bottleneck blocks (He et al. (2016)), DenseNet-BC with L=100 and k=12 (Huang
et al. (2016)) on the CIFAR-10 dataset (Krizhevsky (2012)) using SGD, AdamW and SoftAdam.
For each model and optimization method, the weight decay was varied over [1e-4,2e-4,5e-4,1e-
3,2e-3,5e-3]. For AdamW the learning rate was varied over [1e-4,2e-4,5e-4,1e-3,2e-3]. For each
optimizer and architecture and the best result was chosen, and three runs with separate initializa-
tions were used go generate the final data. The learning schedule reduced the learning rate by a
factor of 10 at 50% and 75% through the total number of epochs. The results are summarized in Ta-
ble 1. We find that SoftAdam equals or outperforms SGD in training classifiers on this dataset. Due
to the larger optimal weight decay constant, SoftAdam achieves lower validation loss at a higher
train loss than SGD.
4.2	Penn Treebank
We trained a 3-layer LSTM with 1150 hidden units per layer on the Penn Treebank dataset (Mikolov
et al. (2010)) in the same manner as Merity et al. (2017). For SoftAdam the weight drop was
increased from 0.5 to 0.6. Results for the average of three random initializations are shown in
Figure 2 (a) and are summarized in Table 2. For these parameters, SoftAdam outperforms SGD
significantly but does not quite achieve the same results as Adam. Note that for this experiment we
chose Adam instead of AdamW for comparison due to its superior performance.
4.3	IWSLT’ 14 German to English
We also trained a transformer using the fairseq package by Ott et al. (2019) on the IWSLT’14 Ger-
man to English dataset. Results for each method with optimized hyperparameters are summarized
in Table 3. Note that no warmup is used for training SoftAdam, but warmup is used for AdamW and
1Implementation was based on the public repository at https://github.com/bearpaw/
pytorch-classification
6
Under review as a conference paper at ICLR 2020
-adamw (Mean) — Sgd (Mean) — Softadam (Mean)
2
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
(a1) AlexNet Train Loss
-adamw (Mean) — Sgd (Mean) — Softadam (Mean)
(a2) AlexNet Val. Loss
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
(a3) AlexNet Val. Acc.
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
(b1) VGG19 Train Loss
-adamw (Mean) — Sgd (Mean) — Softadam (Mean)
(b2) VGG19 Val. Loss
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
(b3) VGG19 Val. Acc.
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
(c1) PreResNet Train Loss
-adamw (Mean) — Sgd (Mean) — Softadam (Mean)
(c2) PreResNet Val. Loss
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
(c3) PreResNet Val. Acc.
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
50	100	150
(d1) ResNet Train Loss
-adamw (Mean) — Sgd (Mean) — Softadam (Mean)
(d2) ResNet Val. Loss
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
(d3) ResNetVal. Acc.
-adamw (Mean) - Sgd (Mean) — Softadam (Mean)
(e1) DenseNet Train Loss
(e2) DenseNet Val. Loss
(e3) DenseNet Val. Acc.
Figure 1: Training results for CIFAR-10. Architectures are (a) AlexNet, (b) VGG19 (BN), (c)
PreReSNet-56, (d) ResNet-110, (e) DenseNet-100 (BC) using SoftAdam, AdamW and SGD. Each
line indicates the average of 3 training runs with random initializations.
7
Under review as a conference paper at ICLR 2020
一 Adam (Mean) — SGD (Mean) — SoftAdam (Mean)
(a) 3-layer LSTM on PTB
Figure 2: Language model validation perplexity during training for SoftAdam, Adam/AdamW and
SGD.
- AdamW - SGD - SoftAdam
(b) fairseq on IWSLT’14 De-En
Table 2: LSTM validation perplexity, test perplexity, and test loss on the Penn Treebank dataset for
SoftAdam, Adam and SGD.
OPTIMIZER VAL. PPL.
SoftAdam	63.63 ± 0.19
SGD	66.64 ±	0.85
Adam	62.03 ±	0.34
SGD. Following Liu et al. (2019), we use a linear learning rate schedule. For these parameters, we
again find that SoftAdam outperforms SGD significantly.
5 Conclusions
In this paper, we have motivated and demonstrated a new optimization algorithm that naturally
unifies SGD and Adam.
We have focused our empirical results on the default hyper-parameter setting, η = 1, and predeter-
mined learning schedules. With these parameters, the algorithm was shown to produce optimization
that is better than or equal to SGD and Adam on image classification tasks. It also performed sig-
nificantly better than SGD on language modeling tasks.
Together with finding the optimal values for η, we expect a better understanding of the learning
schedule to bring light to the way in which the adaptive gradient methods improve convergence.
SoftAdam now also makes it possible to create a learning schedule on η, which may be another
fruitful avenue of research, expanding on the work of Ward et al. (2018).
Better understanding of how adaptive gradients improve the convergence of practical machine learn-
ing models during training will enable larger models to be trained to more accurately in less time.
This paper provides a useful intuition for how that occurs and provides a new algorithm that can be
used to improve performance across a diverse set of problems.
Table 3: Best results for training IWSLT’14 De-En with different optimization algorithms.
OPTIMIZER	VAL. PPL.	BLEU SCORE
SGD	4.14	31.43
SoftAdam	4.02	32.76
AdamW	3.87	34.44
8
Under review as a conference paper at ICLR 2020
References
LA©on Bottou, Frank E. Curtis, and Jorge NoCedaL Optimization methods for large-scale machine
learning. arXiv preprint arXiv:1606.04838, 2016. 1
Jinghui Chen and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in
training deep neural networks. 2018. 1.1
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal ofMachine Learning Research, 12(Jul):2121-2159, 2011. 1, 2.2
Gabriel Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL
http://distill.pub/2017/momentum. 2
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. arXiv preprint arXiv:1512.03385, 2015. 4.1
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. arXiv preprint arXiv:1603.05027, 2016. 4.1
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks, 2016. 4.1
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-
son. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018. 1.1
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017. 1, 1.1
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. 1, 2
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05
2012. 4.1
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
Iutional neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012. 4.1
Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient descent converges
to minimizers. arXiv preprint arXiv:1602.04915, 2016. 1
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018. 1
Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen, and Arindam Banerjee. Hessian based
analysis of sgd for deep nets: Dynamics and generalization. arXiv preprint arXiv:1907.10732,
2019. 2.2
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019. 3, 4.3
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. 2017. 3.1
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. arXiv preprint arXiv:1902.09843, 2019. 1.1
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-
guage models. 2017. 4.2
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Eleventh annual conference of the international speech
communication association, 2010. 4.2
9
Under review as a conference paper at ICLR 2020
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019. 4.3
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop, 2017. 3.1
Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.
SIAM Journal on Control and Optimization,30(4):838-855,1992. 1.1
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ. 1, 2.2
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singu-
larity and beyond. arXiv preprint arXiv:1611.07476, 2016. 2.2
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014. 4.1
Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient
descent. In International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=BJij4yg0Z. 1
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018. 5
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht.
The marginal value of adaptive gradient methods in machine learning. arXiv preprint
arXiv:1705.08292, 2017. 1
Michael R. Zhang, James Lucas, Geoffrey E. Hinton, and Jimmy Ba. Lookahead optimizer: k steps
forward, 1 step back. CoRR, abs/1907.08610, 2019. URL http://arxiv.org/abs/1907.
08610. 1.1
A Reference PyTorch Implementation
import math
import torch
from torch . optim . optimizer import Optimizer
import numpy as np
class SoftAdam ( Optimizer ) :
def __init__ (
self ,
params ,
lr=1e-1,
betas =(0.9, 0.999),
eps =1e -8,
eta =1.0,
Weight_decay=0,
nesterov=False ,
):
defaults = dict (
lr=lr ,
betas=betas ,
eps=eps ,
eta=eta ,
10
Under review as a conference paper at ICLR 2020
Weight_decay = Weight_decay ,
nesterov=nesterov ,
)
SUPer(SoftAdam , self ). __init__ (params , defaults)
def step ( self , closure=None) :
loss = None
if closure is not None :
loss = closure ()
for group in self . param_groups :
for p in group [” params ”] :
if p . grad is None:
continue
grad = p. grad . data
if grad .is_sparse :
raise RuntimeError (
” SoftAdam does not support sparse gradients
)
nesterov = group [” nesterov”]
state = self. s tate [p]
# S tate initialization
if len ( state ) == 0:
state [” step”] = 0
# Exponential moving average of gradient values
state [" exp_avg "] = torch . zeros_like (
p. data
)
# Exponential moving average of
# squared gradient values
state [" exp_avg_sq "] = torch . zeros _like (
p. data
)
exp_avg , exp_avg_sq =(
state[" exp_avg "],
state[" exp_avg_sq "],
)
beta1 , beta2 = group [” betas ”]
state [” step ”] += 1
beta2_hat = min(
beta2 , 1.0 - 1.0 / ( state [” step”])
)
r_beta = (1 - beta2) / (1 - beta2_hat)
eta_hat2 =(
group [” eta ”] * group [” eta ”] * r_beta
)
#	Decay the firs t and second moment With the
#	running average coefficient
exp_avg . mul_(beta1 ). add_(1 - beta1 , grad)
exp_avg_sq . mul_( beta2_hat). addcmuL(
1 - beta2_hat , grad , grad
)
# Create temporary tensor for the denominator
11
Under review as a conference paper at ICLR 2020
denom = exp_avg_sq .mul(
eta_hat2
/(
torch .mean( exp_avg_sq )
+ group[” eps ”] * group[” eps ”]
)
)
denom. sqrt_ (). add_(
1 + group[” eta ”] 一 np.sqrt(eta_hat2 )
)
Wd = group [” weight-decay ”] * group [” lr ”]
p . data . add,(-wd, p . data)
lr _eff = group [” lr ”] * (1 + group [” eta ”])
if nesterov :
P. data .addcdiv.(
一lr_eff * beta1 , exp_avg , denom
)
P . data .addcdiv.(
一lr_eff * (1 一 beta1), grad , denom
)
else:
p . data . addcdiv_(- lr _eff , exp_avg , denom)
return loss
12