Under review as a conference paper at ICLR 2020
IMPLICIT λ-JEFFREYS AUTOENCODERS:
Taking the Best of Both Worlds
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new form of an autoencoding model which incorporates the best
properties of variational autoencoders (VAE) and generative adversarial networks
(GAN). It is known that GAN can produce very realistic samples while VAE
does not suffer from mode collapsing problem. Our model optimizes λ-Jeffreys
divergence between the model distribution and the true data distribution. We
show that it takes the best properties of VAE and GAN objectives. It consists of
two parts. One of these parts can be optimized by using the standard adversarial
training, and the second one is the very objective of the VAE model. However, the
straightforward way of substituting the VAE loss does not work well if we use an
explicit likelihood such as Gaussian or Laplace which have limited flexibility in
high dimensions and are unnatural for modelling images in the space of pixels. To
tackle this problem we propose a novel approach to train the VAE model with an
implicit likelihood by an adversarially trained discriminator. In an extensive set
of experiments on CIFAR-10 and TinyImagent datasets, we show that our model
achieves the state-of-the-art trade-off between generation and reconstruction quality
and demonstrate how we can balance between mode-seeking and mass-covering
behaviour of our model by adjusting the weight λ in our objective.
1	Introduction
Variational autoencoder (VAE) (Kingma et al., 2014; Rezende et al., 2014; Titsias & Lazaro-Gredilla,
2014) is one of the most popular approaches for modeling complex high-dimensional distributions.
It has been applied successfully to many practical problems. It has several nice properties such as
learning low-dimensional representations for the objects and ability to conditional generation. Due
to an explicit reconstruction term in its objective, one may ensure that VAE can generate all objects
from the training set. These advantages, however, come at a price. It is a known fact that VAE
tends to generate unrealistic objects, e.g., blurred images. Such behaviour can be explained by the
properties of a maximum likelihood estimation (MLE) which is used to fit a restricted VAE model
Pθ(x) in data that comes from a complex distribution p*(x). We can equivalently reformulate this
MLE objective as a minimization of the forward KL divergence DKL (p* (X)IlPθ(x)) which does not
explicitly penalize the model pθ(x) for generating unrealistic objects (Arjovsky & Bottou, 2017). As
a result, many regions with the low value p* (x) may have a high value of pθ (x) in the case when the
model pθ(x) has the limited capacity (see Figure 1a)).
Another popular generative model is a generative adversarial network (GAN) (Goodfellow et al.,
2014) which is known for its ability to sample realistic objects. However, it suffers from an inability to
cover the whole distribution p* (x) that leads to mode-seeking behavior, also known as mode-collapse
(Salimans et al., 2016; Metz et al., 2016; Goodfellow, 2016). The main reason of such behaviour is
the properties of the reverse KL divergence DKL(pθ(x)Ip*(x)) (or the Jensen-Shanon divergence
JSD(pθ(x)Ip* (x))) that is minimized during GAN training. These divergences DKL(pθ(x)Ip*(x))
and JSD(pθ(x)Ip*(x)) do not penalize the model pθ(x) for ignoring some high value regions of
p* (x) (Arjovsky & Bottou, 2017). As a result, the most probability mass of the restricted model
pθ(x) can be concentrated in a small number of modes of p*(x) (see Figure 1b), 1c)).
In this paper, we propose a new form of an autoencoding model which is based on incorporating of
VAE and GAN objectives. We consider λ-Jeffreys divergence Jλ(pθ(x)Ip*(x)) which is a weighted
sum of forward and reverse KL divergences: Jλ(pθ(x)Ip* (x)) = λDKL(p* (x)Ipθ(x)) + (1 -
1
Under review as a conference paper at ICLR 2020
Figure 1: Comparison of λ-Jeffreys for λ = 0.5 (red), Reverse KL (green), Forward KL (blue) and
JSD (orange) divergences on the task of approximating a mixture of 4 Gaussians (black dashed line)
with an equiprobable mixture of two Gaussians with learnable location and scale. Plots a)-c) show
pairwise comparisons of optimal log-densities, the plot d) compares optimal densities themselves.
λ)DκL(pθ(X)Ilp*(x)). This way, We encourage our model to be mode-seeking while still having
relatively high values of pθ (x) on all objects from a training set, thus preventing the mode-collapse.
We note that Jλ(pθ(X)Ilp*(x)) is not symmetric with respect topθ(x) andp*(x) and by the weight λ
we can balance between mode-seeking and mass-covering behaviour.
However, the straightforward way of substituting each KL term with GAN and VAE losses does
not work well in practice if we use an explicit likelihood for object reconstruction in VAE objective.
Such simple distributions as Gaussian or Laplace that are usually used in VAE have limited flexibility
and are unnatural for modelling images in the space of pixels. To tackle this problem we propose a
novel approach to train the VAE model in an adversarial manner. We show how we can estimate the
implicit likelihood in our loss function by an adversarially trained discriminator.
We theoretically analyze the introduced loss function and show that under assumptions of optimal
discriminators, our model minimizes the λ-Jeffreys divergence Jλ(pθ(X)Ilp*(x)) and we call our
method as Implicit λ-Jeffreys Autoencoder (λ-IJAE). In an extensive set of experiments, we evaluate
the generation and reconstruction ability of our model on CIFAR10 (Krizhevsky et al., 2009) and
TinyImagenet datasets. It shows the state-of-the-art trade-off between generation and reconstruction
quality. We demonstrate how we can balance between the ability of generating realistic images and
the reconstruction ability by changing the weight λ in our objective. Based on our experimental study
we derive a default choice for λ that establishes a reasonable compromise between mode-seeking and
mass-covering behaviour of our model and this choice is consistent over these two datasets.
2	Related Work
Relation to forward KL-based methods. We can say that all VAE-based models minimize the
upper bound on the forward KL DKL (P (X)Ilpθ). In recent years there have been many extensions
and improvements of the standard VAE. One direction of research is to inroduce the discriminator as
a part of data likelihood (Larsen et al., 2015; Brock et al., 2017) to leverage its intermediate layers
for measuring similarity between objects. However, these models do not have a sound theoretical
justification about what distance between pθ (x) andp*(x) they optimize. The other way isto consider
more complex variational distribution qφ(z∖χ). One can either use better variational bounds (Agakov
& Barber, 2004; Maal0e et al., 2016; Ranganath et al., 2016; Molchanov et al., 2018; Sobolev &
Vetrov, 2019) or apply the adversarial training to match qφ(z∖χ) and the prior distribution p(z)
(Mescheder et al., 2017) or to match the marginals qφ(z) andp(z) (Makhzani et al., 2016). Although
these methods improve approximate inference in VAE model, they remain in the scope of MLE
framework. As we discussed above within this framework the model with a limited capacity is going
to have the mass-covering behaviour.
Relation to reverse KL-based methods. The vanilla GAN framework is equivalent under the as-
sumption of optimal discriminator to minimization of Jensen-Shanon divergence JSD(p*(x)kpθ(x))
(Goodfellow et al., 2014). With a minor modification of a generator loss we can obtain the equiv-
alence to minimization of the reverse KL DKL (pθ(X)Ilp*(x)) (Huszar, 2016; Arjovsky & Bottou,
2017). There have been proposed many autoencoder models which utilize one of these two diver-
gences JSD(p*(x)∣∣pθ(x)) and Dkl(pθ(X)Ilp*(x)). One approach is to minimize the divergence
betweenjoint distributionsp*(X)q(z∣X) andpθ(x∖z)p(z) in a GAN framework (Donahue et al., 2017;
Dumoulin et al., 2017). ALICE model (Li et al., 2017) introduces an additional entropy loss for
dealing with the non-identifiability issues in previous works. Other methods (Chen et al., 2018;
2
Under review as a conference paper at ICLR 2020
Pu et al., 2017a; Rosca et al., 2017; Ulyanov et al., 2018; Zhu et al., 2017) use the reverse KL
Dkl(pθ(X)Ilp*(x)) as an additional term to encourage mode-seeking behaviour.
Relation to Jeffreys divergence-based methods. To the best of our knowledge, there are only two
other autoencoder models which minimize λ-Jeffreys divergence for λ = 0.5. It is an important
case when λ-Jeffreys divergence equals symmetric KL divergence. These methods are AS-VAE
(Pu et al., 2017a) and SVAE (Chen et al., 2018) and they are most closely related to our work.
AS-VAE is a special case of SVAE method therefore further we will consider only SVAE. There
are two most crucial differences between SVAE and λ-IJAE models. The first one is that SVAE
minimizes Jλ(p*(x)q(z∣x)kpθ (x∣z)p(z)) between joint distributions p*(x)q(z∣x) and pθ (x∣z)p(z)
for λ = 0.5 while λ-IJAE minimizes Jλ(p*(x)kpθ(x)) between marginal distributions p*(x) and
pθ(x) for arbitrary λ. The second difference is that the SVAE's loss Jλ(p*(x)q(z∣x)kpθ(χ∣z)p(z))
solely did not give good reconstructions in experiments. Therefore, authors introduced additional
data-fit terms Ep*(x)q0(z|x)logpθ(x|z) + Eρθ(x∣z)p(z) logqψ(z|x) where pθ(x|z) and qφ(z∣x) are
explicit densities. In contrast, λ-IJAE model achieves good generation and reconstruction quality
as it is and allows training implicitpθ(x|z) and qφ(z∖χ) distributions. These two differences make
SVAE and λ-IJAE models significantly distinct, and we observe it in practice.
3	Background
Consider samples X ∈ X from the true data distribution p* (x). The aim of generative models is to
fit a model distribution pθ(x) to p*(x). Most popular models are GAN and VAE. In practice, we
observe that they have significantly different properties. VAE tends to cover all modes of p (x) at the
cost of capturing low probability regions as well. As a result, it often generates unspecific and/or
blurry images. On the other hand, GAN is highly mode-seeking, i.e. it tends to concentrate most of
its probability mass in a small number of modes of p (x). Therefore it may not cover significant part
of p (x) which is also known as a mode collapse problem. Such radical contrast between VAE and
GAN can be explained by the fact that they optimize different divergences betweenpθ(x) andp*(x).
Variational Inference. VAE is trained by MLE: maxθ Ep*(x) logpθ(x). The distribution pθ(x) is
defined as an integral over a latent variable z: pθ(x) = pθ(x∖z)p(z)dz, and in practice it is typically
intractable. Variational inference (Hinton & Van Camp, 1993) sidesteps this issue by introducing
an encoder model (also known as a variational distribution) qφ(z∖x) and replacing the intractable
log pθ (x) with a tractable evidence lower bound (ELBO):
pθ(x∖z)p(z)
qφ(z∖x)
Ep*(x) log pθ (X) > Ep* (x)Eq^(z∣x) log
Ep*(x) [Eq^(z∣x) logpθ(x∖z) - KL(qφ(z∖x)kp(z))] = Lelbo(Θ,夕)
(1)
Then we maximize ELBO Lelbo(θ,夕)with respect to θ and 夕.One can easily derive that MLE is
equivalent to optimizing the forward KL DKL (p* kpθ):
θ*
arg max Ep*(x) log pθ (x)
θ
p*(x)^∣
arg max -Eρ*(χ) log p^y] = arg min Dkl (p kpθ)
(2)
θ
Adversarial Training. The adversarial framework is based on a game between a generator Gθ(z)
and a discriminator Dψ (x) which classifies objects from p*(x) versus ones from ρ(x):
min max
θψ
Ep*(x)logDψ(x) +Epθ(x)log(1 - Dψ(x))
(3)
Goodfellow et al. (2014) showed that the loss of the generator (3) is equivalent to the Jensen-Shanon
divergence JSD(pθ∣∣p*) given an optimal discriminator Dψ* (x) = p*(P)+χθ⑺:
Ep*(x) log Dψ* (x) + Epθ(x) log(1 — Dψ* (x)) = DKL (p* ∣∣ p⅛pθ ) + DKL (pθ ∣∣ p⅛pθ ) - log 4
It is easy to recognize this as an instance of classification-based Density Ratio Estimation (DRE)
(Sugiyama et al., 2012). Following this framework, one can consider different generator’s objectives
while keeping the same objective (3) for the discriminator. DRE relies on the fact that 1DD**(X)=
3
Under review as a conference paper at ICLR 2020
P^(X). By this approach We can obtain a likelihood-free estimator for the reverse Dkl(pθ∣∣p*)
(Huszar, 2016):
1,7	1	Dψ*(x)	— 1O 1	Pθ (X) _ n / Il n
-EPθ(x)log 1- Dψ*(x) = EPθ(x) log 可=DKL(Pθkp )
(Un)Biased Gradients in Adversarial Training. Since in practice the discriminator Dψ (x) is only
trained to Work for one particular set of generator parameters θ, We need to be cautious regarding
validity of gradients obtained by DRE approach. For example, consider the forward KL Dkl (p* ∣Pθ)∙
If we apply DRE, we will arrive at Ep*(X)log IDD(X(X). However, we can notice that in practice this
expression does not depend on θ in any way, i.e. VθEp*⑺ log I-D(；X)= 0. This is because the
forward KL depends on θ only through the ratio of densities, which is replaced by a point estimate
using a discriminator which has no idea regarding pθ ’s local behaviour.
This shows we need to be careful when designing adversarial learning objective as to ensure unbiased
gradients. Luckily, JSD(pθ ∣∣p*) and Dkl(pθ ∣∣p*) are not affected by this problem:
Proposition 1. (Mescheder et al., 2017) Let Dψ*(x) = p*(p)+p)(x) for any X. Then
-VθEpθ(χ) log ιDDψ*x(X) = VθDkl(Pθ∣P*) and V©Ep.(x)log(1 - Dψ*(x)) = VθJSD(pθ∣∣p*)
even ifwe assume that VθDψ* (X) = 0.
Proof. Given in Appendix, section A.	□
4 IMPLICIT λ-JEFFREYS AUTOENCODER
VAE provide a theoretically sound way to learn generative models with a natural and coherent encoder.
However, they are known to generate blurry and unspecific samples that have inferior perceptual
quality compared to generative models based on adversarial learning. The main cause for that is
that the root principle VAEs are built upon - MLE framework - is equivalent to minimization of the
forward KL Dkl(p*∣∣Pθ). While Dkl(p*∣∣Pθ) recovers the true data-generating process p*(χ) if the
model pθ(X) has enough capacity, in a more realistic case ofan insufficiently expressive model pθ (X)
it is known to be mass-covering. As a result, the model is forced to cover all modes of p* (x) even at
the cost of covering low-probability regions as well. This in turn might lead to blurry samples as the
model does not have the capacity to concentrate inside the modes. On the other hand, the reverse KL
DKL (pθ ∣∣p*) has mode-seeking behavior that penalizes covering low-probability regions and thus the
model pθ (x) tends to cover only a few of the modes of p* (x).
Following this reasoning, we propose a more balanced divergence - one that seeks modes, but
still does a decent job covering all modes of p* (x) to prevent mode collapse. We chose λ-Jeffreys
divergence (Jeffreys, 1998) betweenpθ(x) andp*(x): Jλ(Pθ(X)IIp*(x)) = XDkl(p*(x)∣∣pθ(x)) +
(1 - λ)DKL (pθ(X)∣p* (X)).
We illustrate the advantage of λ-Jeffreys divergence for λ = 0.5 over Forward KL, Reverse KL and
JSD divergences in the case of a model with limited capacity in Figure 1. In this figure we compared
divergences in a simple task (see Appendix, section B) of approximating a mixture of 4 Gaussians
with a mixture of just two: both Reverse KL and JSD exhibit mode-seeking behavior, completely
dropping side modes, whereas the Forward KL assigns much more probability to tails and does poor
job capturing the central modes. On a contrast, λ-Jeffreys divergence uses one mixture component to
capture the most probable mode, and the other to ensure mass-covering.
The optimization of λ-Jeffreys divergence consists of two parts. The first one is the minimization of
the reverse KL DKL(pθ(X)∣p*(X)) which can be implemented as a standard GAN optimization as
we discussed in Section 3. The second part is the optimization of the forward KL DKL(p*(X)∣pθ(X))
and we tackle it by maximization of the ELBO Lelbo (θ,夕)as in VAE. So, we obtain an upper bound
on λ-Jeffreys divergence by incorporating GAN and VAE objectives:
Lλ-IJAE(θ3) = (1 - λ)DκL(pθ (X)Ilp*(x)) - λLELBθ(θ,夕)> J λ(Pθ (X)Ilp*(x))
The ELBO term Lelbo(θ,夕)can be decomposed into two parts: (i) a reconstruction term
Ep*(x)Eq晨z∣x) logpθ(x|z); (ii) a KL term Ep*(x)KL(qφ(z∣X)∣∣p(z)). While both terms are easy
4
Under review as a conference paper at ICLR 2020
to deal with in cases of explicitp(x|z) and q(z|x), an implicit formulation poses some challenges. In
the next two sections we address them.
Implicit Conditional Likelihood. Typically to optimize the reconstruction term
Ep*(x)Eq晨z∣x) logpθ(x|z) the conditional likelihood p§(x|z) is defined explicitly as a fully
factorized Gaussian or Laplace distribution (Kingma et al., 2014; Rezende et al., 2014; Titsias &
Lazaro-Gredilla, 2014; Pu et al., 2017b; Chen et al., 2018; Rosca et al., 2017; Mescheder et al.,
2017). While convenient, such choice might limit the expressivity of the generator Gθ (z). As we
discussed previously, optimization of the forward KL(p*∣∣pθ) leads to a mass-covering behavior.
The undesired properties of this behavior such as sampling unrealistic and/or blurry images can be
more significant if a capacity of our model pθ (x) is limited. Therefore we propose a technique which
allows to extend the class of possible likelihoods for pθ(x|z) to implicit ones.
We note that typically in VAE the decoder pθ (x|z) first maps the latent code z to the space X, which
is then used to parametrize the distribution of z’s decodings x|z. For example, this is the case for
N(x∣Gθ(z),σI) or Laplace(x∣Gθ(z),σI). We also use the output of the generator Gθ(Z) ∈ X
to parametrize an implicit likelihood. In particular, we assume pθ(x|z) = r(x∣Gθ(Z)) for some
symmetric likelihood r(x|y):
Definition 1. A density r(∙∣∙) : X X X → R+ is a symmetric likelihood if
(i)	r(x = a|y = b) = r(x = b|y = a) ∀ a, b ∈ X;
(ii)	r(x = a|y = b) has a mode at a = b.
While the Gaussian and Laplace likelihoods are symmetric and explicit, in general we do not require
r(x|y) to be explicit, only being able to generate samples from r(x|y) is required.
The idea is to introduce a discriminator Dτ (x, Z, y) which classifies two types of triplets:
•	real class: (x,z,y)〜p*(x)qφ(z∣x)r(y∣x);
•	fake class: (x,z,y)〜p*(x)qφ(z∣x)r0(y∣Gθ(z)).
We note that r(y|x) and r0(y|x) can be different and we will utilize this possibility in practice. Then
we train the discriminator Dτ (x, Z, y) using the standard binary cross-entropy objective:
Ep*(x)q<√z∣x) [Er(y∣x) log Dτ (x,z,y) + E.(y∣Gθ(z)) log(1 — Dτ(x,z,y)] → max (4)
If we apply the straightforward way to obtain an objective for the generator Gθ (Z) we will derive
that we should minimize Ep*(x)qν(z∣x) log IDD (x：：X). Indeed, given an optimal discriminator for
(4) Dτ*(χ, z, y) = / I 、:(y!xM / C, we obtain:
'， τ v ,,沙/	r(y∣x)+r0(y∣Gθ(z)),
Dτ* (x, Z, x)	r(x|x)
Ea(X)qy(ZIx) log 1 - Dτ* (χ,z,χ) = Ea(X)qy(ZIx) log rθ(χ∣Gθ(Z))=
=-Ep*(x)q晨z|x) logr0(x∣Gθ(Z)) + Const
So, we see that minimizing Ep*(x)qν(z∣x) log IDD (：；；x) given the optimal D「* (x, z, y) is equivalent
to maximizing the reconstruction term with pθ(x|z) = r0(x∣Gθ(z)). However, we face in practice
the same issue as we discussed in Section 3 that VθEo*(x)σ (ZIx) log ■, DT(x,z,x)	= 0 because
p P (X)q0(ZIx)	1 —Dτ(x,z,x)
DT(x, z, x) does not depend on θ explicitly even for optimal T = T*.
We can overcome this issue by exploiting the properties of symmetric likelihoods if we minimize
a slightly different loss for the generator Gθ (z): -Ep*(x)q 晨 z∣x) log IDD (；：GG (Z(Z))). The following
theorem guarantees the gradients will be unbiased in the optimal discriminator case:
Theorem 1. Let Dτ* (x, Z, y) be the optimal solution for the objective (4) and r(y|x)
/7/7//	r0(7∕∣τ'	∩tp	li∖∣vnvnptτic	lilrolibccH&	Tbon	∖/CIR */	/ ∣ 、ICer —"τ* (x,z,Ge(Z))—	——
anct	/ (y |x)	ar e	sy∏ι∏ιet∣vc	etvC-oooc/scLo.	e en∣L	V θp* (x)IL^qo (zI x) ιog ] D (xZ Ge (Z))	—
VθEp*(x)Eq^(Z∣x) log r(χ∣Gθ(z)).
Proof. Given in Appendix, section A.	□
5
Under review as a conference paper at ICLR 2020
So, We obtain that We can maximize the reconstruction term Ep*(x)Eq^(z|x)logr(χ∣Gθ(Z)) by
minimizing -Ep*(x)Eq^(z|x)log IDD (XjGG (；?))and optimize it using gradient based methods. We
note again that we do not require an access to an analytic form of r(y ∣Gθ(z)).
It is an open question what is the best choice for the r(y∣Gθ(z)). Our expectations from r(y∣Gθ(Z))
are that it should encourage realistic reconstructions and highly penalize for visually distorted images.
In experiments, as r(y|x) we use a distribution over cyclic shifts in all directions of an image x.
This distribution is symmetric with respect to all directions and has a mode in x, therefore it is the
symmetric likelihood (see Definition 1 for details).
Although in practice we use r(y|x) which has an explicit form due to non-optimality of Dτ (x, Z, y)
(that is always the case when training on finite datasets) the ratio log IDD(；：GG(Zz))) sets implicit
likelihood of reconstructions. We can think of the non-optimality of Dτ (x, Z, y) as a form of
regularization that allows us to convert explicit function r(y|x) into implicit likelihood that has
desirable properties, i.e. encourages realistic reconstructions ofx and penalizes unrealistic ones.
Implicit Encoder. The KL term from Lelbo (θ,夕)can be computed either analytically, using the
Monte Carlo estimation or by the adversarial manner. We chose the latter approach proposed by
Mescheder et al. (2017) because it enables implicit variational distribution qφ(z∖χ) defined by a neural
sampler (encoder) Eφ(x,ξ) where ξ 〜N(∙∖0, I). For this purpose we should train a discriminator
DZ(x, Z) which tries to distinguish pairs (x, Z) fromp*(x)qφ(z∖x) versus the ones fromp*(x)p(z).
The training objective of Dζ (x, Z) is
Ep*(x)p(z) logDZ(x,z) + Ep*(x)q晨z|x) log(1 — DZ(x,z)) → max	(5)
KL(qφ(z∖χ)kp(z)) is a reverse KL with respect to parameters 夕,therefore we can substitute it by
the expression —Eq^(z∣χ) log IDD(X(；z) (see Section 3).
Final Objectives. Putting it all together we arrive at the following objective:
Lλ-IJAE(θ, φ) =(1 — λ)DκL(pθ(x)kp*(x)) — XLelbo(Θ3)=
— (1 — λ)Epθ (x) log
Dψ*(x)
1 — Dψ*(X)
—λEp*(χ)Eq 晨 z|x)
l	DT (χ,z,Gθ(Z))	+	dz* (X,z)
g 1 — Dτ* (x,Z,Gθ(z))+ g 1 — Dz*(x,z)
→ min
θ,ψ
In practice, discriminators are not optimal therefore we train our model by alternating gradients. We
maximize objectives (3), (4), (5) for Dψ(X), Dτ (X, Z, y), DZ (X, Z) respectively and minimize LG(θ)
for the generator Gθ(z), LE(夕)for the encoder Eφ(χ, ξ) where LG(θ), LE(夕)are:
LG(θ) = -λEPθ(X) log IDD(XL) - (1-λ)Ep*(x)Eq∕∣x) log IDDXi：GGz)))) → min
1 — Dψ (X)	1 — Dτ (X, Z, Gθ(Z))	θ
(6)
Dτ (X, Z, Gθ(Z))	DZ (X, Z)
LE3、= -λEp*(X)Eq,(ZIx) [log1 - Dτ(χ,Z,Gθ(z)) +log1 - Dz(x,z)
→ min
ψ
(7)
5 Experiments
In experiments, we evaluate generation and reconstruction ability of our model on datasets CIFAR-10
and TinyImageNet. We used a standard ResNet architecture (Gulrajani et al., 2017) for the encoder
EW(x, ξ), the generator Gθ(z) and for all three discriminators Dψ (x), DT(x, z, y), DZ(x, z). The
complete architecture description for all networks and hyperparameters used in λ-IJAE can be found
in Appendix, section D. To compare our method to other autoencoding methods in the best way, we
also used official and publicly available code for baselines. For AGE1 we use a pretrained model.
For SVAE2, TwoStage-VAE (2SVAE)3 we report metrics reproduced using officially provided code
and hyperparameters. For α-GAN we also use public implementation4 with same architecture as in
λ-IJAE.
1 AGE github 2 SVAE github 3 TwoStageVAE github 4 α-GAN github
6
Under review as a conference paper at ICLR 2020
sdɑ:l
CIFAR 10
0.6
2	4	6	8
IS
Tmylmagenet
2	3	4	5	6	7
IS
1.0
Figure 2: Comparison between λ-IJAE and λ-IJAE-L1, λ-IJAE-L2 models. We see that λ-IJAE
results form pareto frontier with respect to IS and LPIPS for different choice of λ.
In experiments, for symmetric likelihoods r(y|x) and r0(y|x) we use the following: r(y|x) is a
continuous distribution over cyclic shifts in all directions of an image x. In practice, we discretize
this distribution. To sample from it: (i) we sample one of four directions (top, bottom, right, left)
equally probable; (ii) then sample the size of a shift (maximum size S = 5 pixels) from 0 to S
with probabilities 1/ P 1/(i + 1) , 1/ 2 P 1/(i + 1) , . . . , 1/ (S + 1) P 1/(i + 1) ;
(iii) as a result, we shift an image x to the selected direction in a size which is sampled. For r0(y|x) in
practice we observe that the best choice is when r0(y|x) is close to a delta function δx(y). Therefore,
we use r0(y|x) = N (y|x, σI) which is clearly a symmetric likelihood. We set σ = 10-8. For
r(y|x) as an implicit likelihood we also studied a distribution over small rotations of x, however, we
observed that cyclic shifts achieve better results.
Evaluation. We evaluate our model on both generation and reconstruction tasks. The quality of the
former is assessed using Inception Score (IS) (Salimans et al., 2016). To calculate these metrics
we used the official implementation provided in tensorflow 1.13 (Abadi et al., 2015). The
reconstruction quality is evaluated using LPIPS, proposed by (Zhang et al., 2018). LPIPS compares
images based on high-level features obtained by the pre-trained network. It was show by Zhang et al.
(2018) that LPIPS is a good metric which captures perceptual similarity between images. We use the
official implementation (LPIPS github) to compute LPIPS.
Ablation Study. To show the importance of the implicit conditional likelihood r(y|x) we compare
λ-IJAE with its modification which has instead of implicit r(y|x) a standard Gaussian or Laplace
distribution. We call such models λ-IJAE-L2 and λ-IJAE-L1 respectively. In Figure 2 we compare
λ-IJAE with λ-IJAE-L2 and λ-IJAE-L1 in terms of IS (generation quality) and LPIPS (reconstruction
quality). We see that λ-IJAE significantly outperforms these baselines and allows to achieve pareto-
optimal results for different choice of λ.
Comparison with Baselines. We assess generation and reconstruction quality of λ-IJAE on CIFAR-
10 and TinyImageNet datasets. We compare the results to closest baselines with publicly available
code. We provide visual results in Appendix, section C. Quantitative results are given in Figure 3
and in Table 1. In Figure 3 we compare the methods with respect to IS and LPIPS. Considering both
metrics λ-IJAE achieves a better trade-off between reconstruction and generation quality within these
Figure 3: Evaluation of λ-IJAE on CIFAR and TinyImageNet compared to baselines. Two metrics,
LPIPS and IS metrics are considered to access reconstruction and sampling quality. Considering
both metrics λ-IJAE achieves a better trade-off between reconstruction and sampling quality within
datasets.
Tmylmagenet
1.0
0.8
LU
0.6 <
0.4 5
0.2
1.0
7
Under review as a conference paper at ICLR 2020
Table 1: Reconstruction and generation quality on CIFAR10 and TinyImagenet for models that
allow reconstructions. Baseline models were trained using publicly available code, if possible, to fill
reconstruction quality metrics. J - lower is better, ↑ - higher is better, best is marked with bold.
	Generation Quality	Reconstruction Quality
Method	IS ↑	LPIPS J
CIFAR 10		
WAE (Tolstikhin et al., 2017)	4.18 ± 0.04	
ALI (Dumoulin et al., 2017))	5.34 ± 0.04	
ALICE (Li et al., 2017)	6.02 ± 0.03	
AS-VAE (Pu et al., 2017b)	6.3	
VAE (resnet)	3.45 ± 0.02	0.09 ± 0.03
2Stage-VAE (Dai & Wipf, 2019)	3.85 ± 0.03	0.06 ± 0.03
α-GAN (Rosca et al., 2017)	5.20 ± 0.08	0.04 ± 0.02
AGE (Ulyanov et al., 2018)	5.90 ± 0.04	0.06 ± 0.02
SVAE (Chen et al., 2018)	6.56 ± 0.07	0.19 ± 0.08
λ-IJAE (λ = 0.3)	6.98 ± 0.1	0.07 ± 0.03
TinyImagenet		
AGE (Ulyanov et al., 2018)	6.75 ± 0.09	0.27 ± 0.09
SVAE (Chen et al., 2018)	5.09 ± 0.05	0.28 ± 0.08
2Stage-VAE (Dai & Wipf, 2019)	4.22 ± 0.05	0.09 ± 0.05
λ-IJAE (λ = 0.3)	6.87 ± 0.09	0.09 ± 0.03
datasets. We see that small values of λ give a good IS score while remain the decent reconstruction
quality in terms of LPIPS. However, if decrease λ further LPIPS will start to degrade. Therefore, we
chose the λ = 0.3 as a reasonable trade-off between generation and reconstruction ability of λ-IJAE.
For this choice of λ we compute the results for Table 1. From these Table 1 we see that λ-IJAE
achieves the state-of-the-art trade-off between generation and reconstruction quality. It confirms our
justification about λ-Jeffreys divergence that it takes the best properties of both KL divergences.
6 Conclusions
In the paper, we considered a fusion of VAE and GAN models that takes the best of two worlds: it has
sharp and coherent samples and can encode observations into low-dimensional representations. We
provide a theoretical analysis of our objective and show that it is equivalent to the Jeffreys divergence.
In experiments, we demonstrate that our model achieves a good balance between generation and
reconstruction quality. It confirms our assumption that the Jeffreys divergence is the right choice for
learning complex high-dimensional distributions in the case of the limited capacity of the model.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available
from tensorflow.org.
Felix V Agakov and David Barber. An auxiliary variational method. In International Conference on
Neural Information Processing, pp. 561-566. Springer, 2004.
8
Under review as a conference paper at ICLR 2020
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. International Conference on Learning Representations, 2017.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with
introspective adversarial networks. International Conference on Learning Representations, 2017.
Liqun Chen, Shuyang Dai, Yunchen Pu, Erjin Zhou, Chunyuan Li, Qinliang Su, Changyou Chen,
and Lawrence Carin. Symmetric variational autoencoder and connections to adversarial learning.
In Amos Storkey and Fernando Perez-Cruz (eds.), Proceedings of the Twenty-First International
Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning
Research, pp. 661-669, Playa Blanca, Lanzarote, Canary Islands, 09-11 Apr 2018. PMLR. URL
http://proceedings.mlr.press/v84/chen18b.html.
Bin Dai and David Wipf. Diagnosing and enhancing vae models. In International Conference on
Learning Representations, May 2019.
JeffDonahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. International
Conference on Learning Representations, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky,
and Aaron Courville. Adversarially learned inference. International Conference on Learning
Representations, 2017.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint
arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. Mar 2017. URL http://arxiv.org/abs/1704.00028v3.
Geoffrey Hinton and Drew Van Camp. Keeping neural networks simple by minimizing the description
length of the weights. In in Proc. of the 6th Ann. ACM Conf. on Computational Learning Theory.
Citeseer, 1993.
Ferenc Huszar.	An alternative update rule for generative adver-
sarial networks, 2016.	URL https://www.inference.vc/
an-alternative-update-rule-for-generative-adversarial-networks/.
Harold Jeffreys. The theory of probability. OUP Oxford, 1998.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems, pp.
3581-3589, 2014.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
research). 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. Autoen-
coding beyond pixels using a learned similarity metric. CoRR, 2015.
Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao, and Lawrence
Carin. Alice: Towards understanding adversarial learning for joint distribution matching. In
Advances in Neural Information Processing Systems, pp. 5495-5503, 2017.
Lars Maal0e, Casper Kaae S0nderby, S0ren Kaae S0nderby, and Ole Winther. Auxiliary deep
generative models. arXiv preprint arXiv:1602.05473, 2016.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. International Conference on Learning Representations, 2016.
9
Under review as a conference paper at ICLR 2020
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. In Doina Precup and Yee Whye
Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70
of Proceedings ofMachine Learning Research, pp. 2391-2400, International Convention Centre,
Sydney, Australia, 06-11 AUg 2017. PMLR. URL http://Proceedings .mlr.press/
v70/mescheder17a.html.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. arXiv preprint arXiv:1611.02163, 2016.
Dmitry Molchanov, Valery Kharitonov, Artem Sobolev, and Dmitry Vetrov. Doubly semi-implicit
variational inference. arXiv preprint arXiv:1810.02789, 2018.
Yuchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li, and Lawrence Carin.
Adversarial symmetric variational autoencoder. In Advances in Neural Information Processing
Systems, pp. 4330-4339, 2017a.
Yuchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li, and Lawrence Carin.
Adversarial symmetric variational autoencoder. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 30, pp. 4330-4339. Curran Associates, Inc., 2017b. URL http://papers.nips.
cc/paper/7020-adversarial-symmetric-variational-autoencoder.pdf.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International
Conference on Machine Learning, pp. 324-333, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. ICML, 2014.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational
approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987,
2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp.
2234-2242, 2016.
Artem Sobolev and Dmitry Vetrov. Importance weighted hierarchical variational inference. arXiv
preprint arXiv:1905.03290, 2019.
M. Sugiyama, T. Suzuki, and T. Kanamori. Density Ratio Estimation in Machine Learning. Cambridge
books online. Cambridge University Press, 2012. ISBN 9780521190176. URL https://books.
google.ru/books?id=NOQHkhcFJ0oC.
Michalis Titsias and Miguel Lazaro-Gredilla. Doubly stochastic variational bayes for non-conjugate
inference. In International conference on machine learning, pp. 1971-1979, 2014.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
Nov 2017. URL http://arxiv.org/abs/1711.01558v3.
Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. It takes (only) two: Adversarial generator-
encoder networks. In AAAI. AAAI Press, 2018.
Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE International
Conference on, 2017.
10
Under review as a conference paper at ICLR 2020
Appendix
Appendix A Proofs
Proposition 1. (Mescheder et al., 2017) Let Dψ*(x) = ?*(：)+彳)(方)for any X. Then
-VθEpθ(χ) log I-D*：：(X) = VθDkl(pθkp*) and V©Ep.(x)log(1 - Dψ*(x)) = V©JSD(pθ∣∣p*)
even ifwe assume that V©Dψ*(x) = 0.
Proof. The core of the proof is the fact that for any distribution qφ(x) we have Eqφ(x)Vφ log qφ(x) =
0. We can write down the derivative for the divergence as a sum of two terms: the first quantifies
dependence of the divergence of parameters θ through samples x and the second one quantifies
dependence through log-densities. The first one can be captured by an optimal discriminator if we
use pathwise gradients (aka reparametrization trick), while the second one is more challenging.
VpDKL(Pp||p*)二	= VpEpθ(X) log p∣7⅞ = Vp Z Pp (X)IOg p∣7⅞dX = p (X)	p (X) pp(X)	pp (X) 二 J VpPp(X)log p^X)dX + J Pp(X)Vp log p^X)dX = 二 i Vppp(x) log 1 - Dψ* (X) dX + Ppp(X)Vp logpp(x)"x = Dψ* (X) 1 — Dψ* (X)	Vppp (X) = VpEpθ(x) log Dψ*(X) + JPp(X)X = =VpEpθ(X) log 1 DDψ* (X) + Vp ZPp(X)dX = VpEpθ(X) log 1 DDψ* (X) Dψ* (X)	Dψ* (X)
The same can be shown for the Jensen-Shannon divergence:
VθJSD(pθI∣p*) = VθDKL (pθ||p:+pθ) + VθDKL (pθ||p⅛pθ)=
=vθ ∕pθ(x) log p⅛⅛ dx + vθ ∕p*(X) log p⅛⅛ dx + log4 =
/ vθPp(X) log p*(Xpθ+Pθ(x) dx + ∕pθ(X)Vθ log p*(Xpθ+Pθ(X) dx
—/ p*(x)Vθ log(p*(X) + pp (X))dX + log4 =
/ Vppp(X)log(1 — Dψ*(X))dX — /pp(x)Vθ log(p*3+ pp(x))“x
+ Jpp(X)Vp logpp(X)dX — ∕p*(x)Vθ log(ρ*(X) + Pp(x))"x + log4 =
=0
z--------------^---------------{
VpEρθ(X)log(1 — Dψ* (x)) — 2/ pθ(x)+p*(X) Vp log pθ(x)+ p*(x)dX + log4
VpEpθ(x)log(1 — Dψ* (X)) +log4
□
Theorem 1. Let Dτ* (X, z, y) be the optimal solution for the objective (4) and r(y|X)
and r0(y∣X) are Symmetric likelihoods. Then VpEp*(χ)Eq^(z∣χ) log ι--* *；；*：；?)) =
VpEp*(χ)Eq晨z∣χ) logr(X∣Gp(Z)).
11
Under review as a conference paper at ICLR 2020
Proof.
▽ Iff Iff 1	DT *(X,z, Gθ(Z)) _ κ κ ▽ ] T(Gθ (Z)Ix)I	_
VθEp*(X)Eqy(ZIx) log ι - DE2©(Z)) = Ep*(X)E(MzlxNθ log MG©⑶⑻ la=Gθ(Z) =
=Ep*(x)Eqy(z∣x)Vθ logr(Gθ(z)∣x) + Ep*(x)Eqy(z∣x) [Vθ logr0(G©(z)|a)|a=G0(z)]
Now we will show the second term is equal to zero given our assumptions:
Ep*(x)Eqy(z∣x) [Vθ logr0(Gθ(z)∣a)∣α=Gθ(z)] = Ep*(x)Eq∕∣x) dar00(Gθ(U%=G:(Z) VθGθ(z)=0
r (Gθ(z)∣Gθ(z))
Where we have used the (1) and (2) properties of the likelihoods r(x|y) (Definition 1):
得 r0(Gθ (Z)Ia)la=Gθ(z) = da r0(alGθ (Z))la=Gθ(z) = 0
□
Appendix B	Figure 1 setup
To generate the plot 1 we considered the following setup: a target distribution was a mixture:
p*(x) = 0.15N(x∣ - 8, 0.22) +0.35N(x∣ - 3,0.82) + 0.3N(x∣3,1) +0.2N(x∣8,0.22)
While the model as an equiprobable mixture of two learnable Gaussians:
pθ(x) = 0.5N(xIθ1, exp(θ2)) + 0.5N(xIθ3, exp(θ4))
The optimal θ was found by making 10,000 stochastic gradient descent iterations on Monte Carlo
estimations of the corresponding divergences with a batch size of 1000. We did 50 independent
runs for each method to explore different local optima and chose the best one based on a divergence
estimate with 100,000 samples Monte Carlo samples.
Appendix C	Images
12
Under review as a conference paper at ICLR 2020
(b) 0.2-IJAE
(a) Real Data
(c) α-GAN
(d) AGE
(e) TwoStage-VAE
(f) SVAE
Figure 4: Samples from models trained on CIFAR10 dataset. Images for baselines were obtained
running publicly available code.
Figure 5: Reconstructions on the CIFAR10 dataset for IJAE model and closest baselines. Reconstruc-
tions for baselines were obtained running publicly available code.
13
Under review as a conference paper at ICLR 2020
(b) IJAE
(d) TwoStage-VAE
(e) SVAE
Figure 6: Samples from models trained on TinyImagenet dataset. Images for baselines were obtained
running publicly available code.
(a) Real Data
(c) AGE
Figure 7: Reconstructions on the TinyImagenet dataset for IJAE model and closest baselines. Recon-
structions for baselines were obtained running publicly available code.
14
Under review as a conference paper at ICLR 2020
Appendix D	Network Architectures and Hyperparameters
Table 2: Hyperparameters used in experiments
Parameter	Value
Generator, Encoder Optimizer	Adam
betas = (0.5, 0.9)	
learning rate	0.0001
weight decay	0
latent dimension	128
Discriminators Optimizer	Adam
betas = (0.5, 0.9)	
learning rate	0.0001
weight decay	0
Table 3: Encoder Network Architecture
Shortcuts Basic Res Block Inject Noise	same as in original Resnet Linear(128, num Channels)(normal(size=128)) + image
Layer	Parameters
Basic Res Block Inject Noise Basic Block Inject Noise Basic Block Basic Block ReLU Inject Noise Flatten Linear	in channels =	3, out channels =	128, stride = 2 in channels =	128, out channels	=	128,	stride =	2 in channels =	128, out channels	=	128,	stride =	2 in channels =	128, out channels	=	128,	stride =	2 in features 512, out features = 128
#Weights = 6436224
Table 4: Generator Network Architecture
Shortcuts Upsample Basic Res Block	same as in original Resnet with upsample instead of strided convolution
Layer	Parameters
Linear Reshape to Image Upsample Basic Res Block Upsample Basic Res Block Upsample Basic Res Block Batch Norm ReLU Conv3x3 Tanh	in features 128, out features = 2048 shape=(128, 4, 4) in channels	=	128, out channels =	128, scale	factor	= 2 in channels	=	128, out channels =	128, scale	factor	= 2 in channels	=	128, out channels =	128, scale	factor	= 2 stride = 1, padding 1
#Weights=1154179	一
15
Under review as a conference paper at ICLR 2020
Table 5: Single Image Discriminator Network Architecture
Shortcuts Basic Res Block Inject Noise	same as in original Resnet Linear(128, num Channels)(normal(size=128)) + image
Layer	Parameters
Basic Res Block	in channels = 3, out channels = 128, stride = 2
Inject Noise Basic Block	in channels = 128, out channels = 128, stride = 2
Inject Noise Basic Block	in channels = 128, out channels = 128, stride = 1
Basic Block	in channels = 128, out channels = 128, stride = 1
ReLU Global Average Pooling Linear	in features 128, out features = 1
#Weights = 1053825	
Table 6: Pair Image + Latent Discriminator Network Architecture
Shortcuts Basic Res Block Inject Noise	same as in original Resnet Linear(128, num channels)(normal(size=128)) + image
Inputs:	left image, right image, latent code
Layer	Parameters
Stack Images	orient = horizontal
Basic Res Block	in channels = 3, out channels = 128, stride = 2
Inject Noise Basic Block	in channels = 128, out channels = 128, stride = 2
Inject Noise Basic Block	in channels = 128, out channels = 128, stride = 1
Basic Block	in channels = 128, out channels = 128, stride = 1
ReLU Global Average Pooling Concat Latent Code Linear	in features 256, out features = 1
#Weights = 1054081	
Table 7: Image + Latent Discriminator Network Architecture
Layer	Parameters
Basic Block	kernel size = 4in channels = 3, out channels = 64, stride = 2, padding = 1
LeakyReLU(0.2) Basic Block	kernel size = 4in channels = 64, out channels = 128, stride = 2, padding = 1
LeakyReLU(0.2) Flatten	
Linear	in features 8320, out features = 512
LeakyReLU(0.2) Linear	in features 512, out features = 256
LeakyReLU(0.2) Linear	in features 512, out features = 256
#Weights = 4526273
16