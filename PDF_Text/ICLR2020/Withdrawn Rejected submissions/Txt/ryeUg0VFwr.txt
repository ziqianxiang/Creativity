Under review as a conference paper at ICLR 2020
Striving for Simplicity in
Off-Policy Deep Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
This paper advocates the use of offline (batch) reinforcement learning (RL) to
help (1) isolate the contributions of exploitation vs. exploration in off-policy
deep RL, (2) improve reproducibility of deep RL research, and (3) facilitate the
design of simpler deep RL algorithms. We demonstrate that recent off-policy
deep RL algorithms, even when trained solely on all of the replay data of an
online DQN agent, can outperform online DQN on Atari 2600 games. We also
present Random Ensemble Mixture (REM), a simple Q-learning algorithm that
enforces optimal Bellman consistency on random convex combinations of multiple
Q-value estimates. Using the DQN replay dataset, offline REM surpasses the gains
from online C51 and outperforms offline QR-DQN. Furthermore, REM performs
comparably to QR-DQN in the online RL setting on Atari 2600 games. The DQN
replay dataset can serve as an offline RL benchmark and will be released.
1 Introduction
Deep neural networks have become a critical component of modern reinforcement learning (RL) (Sut-
ton and Barto, 2018). The seminal work of Mnih et al. (2013; 2015) on deep Q-networks (DQN)
has demonstrated that it is possible to train neural networks using Q-learning (Watkins and Dayan,
1992) to achieve human-level performance in playing Atari 2600 games (Bellemare et al., 2013)
directly from raw pixels. Recent progress in mastering Go (Silver et al., 2016) and advances in
robotic control (Levine et al., 2016; OpenAI et al., 2018; Kalashnikov et al., 2018) present additional
supporting evidence for the enormous potential of deep RL.
Off-policy RL algorithms such as Q-learning are attractive because they disentangle data collection
and policy optimization and offer more sample efficient solutions than on-policy algorithms (Sutton
et al., 2000; Schulman et al., 2015; Mnih et al., 2016). Importantly, off-policy techniques can leverage
the vast amount of existing offline logged data for real-world applications such as digital advertis-
ing (Strehl et al., 2010; Bottou et al., 2013), education (Mandel et al., 2014), and healthcare (Shortreed
et al., 2011). Since online RL is often unsafe to deploy in the real world, offline RL algorithms are
the only feasible solution for many practical decision making problems (Dulac-Arnold et al., 2019).
Nevertheless, off-policy RL algorithms when combined with neural networks can be unstable or even
divergent (Baird, 1995; Boyan and Moore, 1995; Tsitsiklis and Van Roy, 1997).
In the absence of theoretical guarantees for off-policy deep RL, recent advances (see Hessel et al.
(2018) for an overview) are largely governed by empirical results on a popular benchmark suite of
Atari 2600 games (Bellemare et al., 2013). Unfortunately, these empirical results are difficult to
reproduce (Henderson et al., 2018) and the contribution of novel RL algorithms is often conflated
with many other design choices (Clary et al., 2019; Khetarpal et al., 2018). Given the empirical nature
of deep off-policy RL, it is crucial to come up with simpler and reproducible experimental settings
and study the relative importance of different components of deep RL algorithms. It is also important
to strive for finding successful RL algorithms that are as simple as possible.
This paper advocates the use of offline (batch) RLto help (1) isolate the contributions of exploitation vs.
exploration in off-policy deep RL, (2) improve reproducibility of deep RL research, and (3) facilitate
the design of simpler off-policy deep RL algorithms. To this end, we propose a new benchmark
for offline optimization of RL agents on Atari 2600 games using all of the replay data of a DQN
agent (Mnih et al., 2015). Using this benchmark, we investigate the following questions:
1.	Is it possible to train successful Atari agents based solely on offline data?
2.	Can one design simple and effective alternatives to intricate RL algorithms in the offline setting?
3. Are the insights gained from the offline setting useful for developing effective online algorithms?
1
Under review as a conference paper at ICLR 2020
Senoos P①ZlP3uuon uπIP①
—Offline REM
—Offline QR-DQN
— Offline DQN (Nature)
SaIoOS P①ZmUoN UMPBW
Iteration
(a)	(b)
Figure 1: Median normalized online evaluation scores averaged over 5 runs (shown as traces) across stochastic
version of 60 Atari 2600 games of (a) offline RL agents trained using the DQN replay dataset containing 200
million frames. Offline REM outperforms offline QR-DQN and surpasses gains from online C51. (b) online RL
agents trained for 200 million frames (standard protocol). Online REM with 4 Q-networks performs comparably
to online QR-DQN. Each iteration corresponds to 1 million frames.
The contributions of this paper can be summarized as:
•	An offline RL benchmark is proposed for evaluating and designing RL algorithms on Atari 2600
games without exploration, based on the logged replay data of a DQN agent comprising 50 million
(observation, action, reward, next observation) tuples per game. This reduces the computation
cost of the experiments considerably and helps improve reproducibility of deep RL research by
standardizing training using a fixed offline dataset. The replay dataset used in our experiments
will be released to enable offline optimization of RL algorithms on a common ground.
•	Contrary to recent work (Zhang and Sutton, 2017; Fujimoto et al., 2019), we find that the
logged DQN data is sufficient for optimizing strong Atari agents offline without any environment
interaction. For instance, QR-DQN (Dabney et al., 2018b) trained offline on the DQN replay
dataset significantly outperforms online DQN (Mnih et al., 2015).
•	A simple and novel Q-learning algorithm called Random Ensemble Mixture (REM) is presented,
which enforces optimal Bellman consistency on random convex combinations of multiple Q-value
estimates. Offline REM, trained using the DQN replay dataset, outperforms more complex RL
algorithms such as offline QR-DQN and surpasses the gains from online C51 (Bellemare et al.,
2017) over online DQN (Figure 1a).
•	We use the insights gained from the offline experiments to develop an online variant of REM,
which performs comparably with online QR-DQN (Figure 1b).
2 Off-policy Reinforcement Learning
An interactive environment in reinforcement learning (RL) is typically described as a Markov decision
process (MDP) (S, A, R, P, γ) (Puterman, 1994), with a state space S, an action space A, a stochastic
reward function R(s, a), transition dynamics P(s0|s, a) and a discount factor γ ∈ [0, 1). A stochastic
policy ∏(∙ | S) maps each state S ∈ S to a distribution (density) over actions.
For an agent following the policy π, the action-value function, denoted Qπ (s, a), is defined as the
expectation of cumulative discounted future rewards, i.e.,
Qπ(s,a) = E [Xt=o Ytrt 卜0 = s,ɑo = a, St 〜P(∙ | St-i,at-i), a 〜∏(∙ | St),r 〜R(st"].
(1)
The goal of RL is to find an optimal policy ∏ that attains maximum expected return, for which
Qπ* (s, a) ≥ Qπ (s, a) for all π, s, a. The Bellman optimality equations (Bellman, 1957) characterize
*	-
the optimal policy in terms of the optimal Q-values, denoted Q* = Qπ , via:
Q*(s,a) = E [r + Y max Q*(s0, a0) ∣ r 〜R(s, a), s0 〜P(∙ | s, a)].
(2)
To learn a policy from interaction with the environment, Q-learning (Watkins and Dayan, 1992)
iteratively improves an approximate estimate of Q*, denoted Qθ, by repeatedly regressing the
LHS of (2) to target values defined by samples from the RHS of (2). For large and complex state
2
Under review as a conference paper at ICLR 2020
spaces, approximate Q-values are obtained using a neural network as the function approximator.
To further stabilize optimization, a target network Qθ0 with frozen parameters may be used for
computing the learning target (Mnih et al., 2013). The target network parameters θ0 are updated to the
current Q-network parameters θ after a fixed number of time steps. DQN (Mnih et al., 2013; 2015)
parameterizes Qθ with a convolutional neural network (LeCun et al., 1998) and uses Q-learning
with a target network while following an -greedy policy with respect to Qθ for data collection.
DQN minimizes the TD error using the loss L(θ) on mini-batches of agent’s past experience tuples,
(s, a, r, s0), sampled from an experience replay buffer D (Lin, 1992) collected during training:
L(θ) = Es,a,r,s0〜D ['δ (Qθ(s, a) - r - Y maaX Qθ0 (s0,a0))],
(3)
where lδ is the HUberloss (HUber, 1964) given by 'δ (U)
ʃ 2 u2
lδ(∣u∣- 2δ),
for |u| ≤ δ,
otherwise.
Q-learning is an off-policy algorithm (SUtton and Barto, 2018) since the learning target can be
compUted withoUt any consideration of how the experience was generated. In offline (batch) RL (Ernst
et al., 2005; Riedmiller, 2005; Lange et al., 2012), we assUme access to a fixed offline dataset of
experiences D, withoUt any fUrther interaction with the environment.
A family of recent off-policy deep RL algorithms, which serve as a strong baseline in this paper,
inclUde DistribUtional RL (Bellemare et al., 2017; JaqUette, 1973) methods. SUch algorithms estimate
a density over retUrns for each state-action pair, denoted Zπ(s, a), instead of directly estimating the
mean Qπ(s, a). Accordingly, one can express a form of distribUtional Bellman optimality as
Z*(s, a)	= r + YZ*(s0,	argmax。0*/ Q*(s0,a0)),	where r 〜R(s,a),	s0	〜P(∙	|	s,a),	(4)
and = denotes distributional equivalence and Q*(s0, a0) is estimated by taking an expectation with
respect to Z*(s0, a0). C51 (Bellemare et al., 2017) approximates Z*(s, a) by using a categorical
distribution over a set of pre-specified anchor points, and distributional QR-DQN (Dabney et al.,
2018b) approximates the return density by using a uniform mixture of K Dirac delta functions, i.e.,
1K	1K
Zθ (s, a) := K δθi(s,a), and Qθ(S, a)= K ɪ^θi(s,a) .	(5)
QR-DQN trains each θi for 1 ≤ i ≤ K to match the 22-1 -quantile of the target return density (RHS
of (4)) using the Huber quantile regression loss (Koenker, 2005; Dabney et al., 2018b). QR-DQN,
albeit complex, outperforms C51 and DQN and obtains state-of-the-art results on Atari 2600 games,
among agents that do not exploit n-step updates (Sutton, 1988) and prioritized replay (Schaul et al.,
2016). This paper avoids using n-step updates and prioritized replay to keep the empirical study
simple and focused on deep Q-learning algorithms.
3 Offline Optimization of Reinforcement Learning Agents
Modern off-policy deep RL algorithms perform remarkably well on common benchmarks such as the
Atari 2600 games (Bellemare et al., 2013) and continuous control MuJoCo tasks (Todorov et al., 2012).
Such off-policy algorithms (Mnih et al., 2015; Lillicrap et al., 2015) are considered “online” because
they alternate between optimizing a policy and using that policy to collect more data. Typically, these
algorithms keep a sliding window of most recent experiences in a finite replay buffer (Lin, 1992),
throwing away stale data to incorporate most fresh (on-policy) experiences. In principle, off-policy
algorithms can learn from data collected by any policy, however, Zhang and Sutton (2017) assert that
using a large replay buffer can significantly hurt the performance of Q-learning algorithms, since it
can delay rare on-policy experiences that are important for policy learning.
This paper revisits offline off-policy RL and investigates whether off-policy deep RL agents trained
solely on offline data can be successful. We advocate the use of offline RL to help isolate an
RL algorithm’s ability to exploit experience and generalize vs. its ability to explore effectively by
collecting interesting new data. The offline RL setting removes design choices related to the replay
buffer and exploration; therefore, it is much simpler to experiment with and reproduce than the typical
online off-policy learning. Also, in the offline setting, we optimize an objective (e.g., (3)) over a fixed
dataset as opposed to a changing replay buffer of an online agent. In this work, we mainly experiment
3
Under review as a conference paper at ICLR 2020
3
二卓
Offline QR-DQN
DQN
Random
,xo<ilv NOTaα
I σs-J..>oω
，aua.LS OBH
I α3κ,tt-√
I azoz s⅛,sm
,adγosa ≥s,ttao-.∙
IXEaJ∙sv
— m±d
,alw⅛2Hj, a HYN
■OLNaA
,InOXyaBm
Iwfcasoffi
IN≈≈0α 0≡
I a MYɑSlH
I saooæ 3
,NYXOOd
— RN。Z ail,LYa
，WVHMNVs-nE-
Game	Game
(a)	(b)
Figure 2: Normalized performance improvement (in %) over online DQN (Nature), per game, of (a) offline
DQN (Nature) and (b) offline QR-DQN trained using the DQN replay dataset for same number of gradient
updates as online DQN. The normalized online score for each game is 0.0 and 1.0 for the worse and better
performing agent among fully trained online DQN and random agents respectively.
with large and diverse datasets motivated by real-world RL problems where we already have access
to (usually continually growing) diverse datasets of logged experiences, e.g., historical data from
recommender systems (Bottou et al., 2013), database of robotic experiences (Dasari et al., 2019) etc.
To facilitate a study of offline RL with large and diverse datasets, we train several instances of
DQN (Nature) agents (Mnih et al., 2015) on stochastic version of 60 Atari 2600 games for 200
million frames each, with a frame skip of 4 (standard protocol) and sticky actions enabled (Machado
et al., 2018). On each game, we train 5 different agents with random initialization, and store all of the
tuples of (observation, action, reward, next observation) encountered during training into 5 replay
datasets of 50 million tuples each. These replay datasets are used for training standard off-policy RL
agents, offline, without any interaction with the environment during training. We perform an online
evaluation of the agents in the intervals of 1 million training frames, and report the best evaluation
score for each agent, averaged over 5 runs.
3.1	Experiments and Results
Given the logged replay data of the DQN agent, it is natural to ask whether training an offline variant
of DQN on this data will recover the performance of online DQN. Furthermore, whether more
recent off-policy algorithms are able to exploit the DQN replay dataset more effectively than DQN.
To investigate these questions, we train DQN (Nature) and distributional QR-DQN (Dabney et al.,
2018b) agents, offline, on the DQN replay dataset for the same number of gradient updates as online
DQN. We use the hyperparameters provided in the Dopamine baselines (Castro et al., 2018) for a
standardized comparison (Section A.4) and report scores using a normalized scale (Section A.3). We
note that the replay datasets include samples from all of the intermediate policies seen during the
optimization of the DQN (Nature) agent. Accordingly, we compare the performance of offline agents
against the best performing policy among the mixture of data collecting policies.
Results. We find that offline DQN underperforms online DQN on all except a few games while offline
QR-DQN outperforms offline DQN and online DQN on most of the games (Figure 2). C51 (Bellemare
et al., 2017) trained offline using the DQN replay dataset also considerably improves upon offline
DQN (Figure A.4). These results suggest that DQN (Nature) is somewhat ineffective at exploiting
off-policy data. However, offline QR-DQN’s impressive performance demonstrates that it is possible
to optimize strong Atari agents completely offline (see Figure A.6 for learning curves).
3.2	Offline Continuous Control Experiments
In contrast to our offline results on Atari 2600 games with discrete action spaces, discussed above,
Fujimoto et al. (2019) find that standard off-policy deep RL agents are not effective on continuous
control problems when trained offline, even when large and diverse replay datasets are used. The
results of Fujimoto et al. (2019) are based on the evaluation of a standard continuous control agent,
called DDPG (Lillicrap et al., 2015), and other more recent continuous control algorithms such as
TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018) are not considered in their study.
Motivated by the so-called final buffer setting in Fujimoto et al. (2019) (Section A.2), we train a DDPG
agent on continuous control MuJoCo tasks (Todorov et al., 2012) for 1 million time steps and store
all of the experienced transitions. Using this dataset, we train standard off-policy agents including
4
Under review as a conference paper at ICLR 2020
DQN
QR-DQN
EnsembIe-DQN
REM
Figure 3: Neural network architectures for DQN, QR-DQN and the proposed expected RL variants, i.e.,
Ensemble-DQN and REM, with the same multi-head architecture as QR-DQN. The individual Q-heads share all
of the neural network layers except the final fully connected layer.
TD3 and DDPG completely offline. Consistent with our offline results on Atari games, offline TD3
significantly outperforms the data collecting DDPG agent and offline DDPG (Figure A.1).
4	Seeking Simpler Off-Policy RL Algorithms
Section 3.1 finds that recent distributional RL algorithms such as QR-DQN are effective at exploiting
offline data. However, these algorithms are complicated as they require a generalization of Bellman
equations to distributions (4) and typically minimize a probability divergence measure from a
distributional target (Rowland et al., 2018; Dabney et al., 2018b) instead of scalar TD errors. Moreover,
despite recent efforts (Lyle et al., 2019; Bellemare et al., 2019), the source of the gains when using
distributional RL algorithms remains unclear. We investigate whether one can develop simple and
effective alternatives to more intricate RL algorithms in the offline setting. To this end, we study two
deep Q-learning algorithms, Ensemble DQN and REM, which adopt ensembling to improve stability.
4.1	Ensemble-DQN
Ensemble-DQN is a simple extension of DQN that approximates the Q-values via an ensemble
of parameterized Q-functions (Fauβer and Schwenker, 2015; Osband et al., 2016; AnscheI et al.,
2017). Each Q-value estimate, denoted Qθk (s, a), is trained against its own target Qθk0 (s, a), similar
to Bootstrapped-DQN (Osband et al., 2016). The Q-functions are optimized using identical mini-
batches in the same order, starting from different parameter initializations. The loss L(θ) takes the
form,
1K
L(θ) = K X Es,a,r,s0〜D ['δ (Qθ (s, a) - r - Y maX Qθ0 (S ,a 川,	⑹
where lδ is the Huber loss. While Bootstrapped-DQN uses one of the Q-value estimates in each
episode to improve exploration, in the offline setting, we are only concerned with the ability of
Ensemble-DQN to exploit better and use the mean of the Q-value estimates for evaluation.
4.2	Random Ensemble Mixture (REM)
Increasing the number of models used for ensembling typically improves the performance of su-
pervised learning models (Shazeer et al., 2017). This raises the question whether one can use an
ensemble over an exponential number of Q-estimates in a computationally efficient manner. Inspired
by dropout (Srivastava et al., 2014), we propose Random Ensemble Mixture for RL.
Random Ensemble Mixture (REM) uses multiple parameterized Q-functions to estimate the Q-values,
similar to Ensemble-DQN. The key insight behind REM is that one can think of a convex combination
of multiple Q-value estimates as a valid Q-value estimate itself. This is especially true at the fixed
point, where all of the Q-value estimates have converged to an identical Q-function. Using this insight,
we train a family of Q-function approximators defined by mixing probabilities on a (K - 1)-simplex.
5
Under review as a conference paper at ICLR 2020
SbUOOS PBZIɪeuɪlθN UBlPBW
Figure 4: Offline REM vs. Baselines. Normalized scores
averaged over 5 runs (shown as traces) across 60 stochastic
Atari 2600 games of offline agents trained using the DQN
replay dataset trained for 5X gradient steps.
Table 1: Median normalized best average eval-
uation scores (averaged over 5 runs) across 60
stochastic Atari 2600 games, measured as percent-
ages and number of games where an offline agent
achieves better scores than a fully trained online
DQN (Nature) agent.
Offline agent	Median > DQN
DQN (Adam)	111.9%	41
Ensemble-DQN	111.0%	39
Averaged Ensemble-DQN	112.1%	43
QR-DQN	118.9%	45
REM	123.8%	49
Specifically, for each mini-batch, we randomly draw a categorical distribution α, which defines a
convex combination of the K estimates to approximate the optimal Q-function. This approximator is
trained against its corresponding target to minimize the TD error. The loss L(θ) takes the form,
L(θ)
Es,a,r,s0 〜D
EaI,…,ακ ~P∆
]'δ (X ɑkQk(s,a) - r-YmaXX αkQk,(s0, a0))
(7)
where P∆ represents a probability distribution over the standard (K - 1)-simplex ∆K-1 = {α ∈
RK : αι + α2 + •一+ ακ = 1, αk ≥ 0, k = 1, .. ., K}.
REM considers Q-learning as a constraint satisfaction problem based on Bellman optimality con-
straints (2) and L(θ) can be viewed as an infinite set of constraints corresponding to different mixture
probability distributions. For action selection, we use the average of the K value estimates as the
Q-function, i.e., Q(s, a) = Pk Qθk(s, a)/K. REM is easy to implement and analyze (see Proposition
1), and can be viewed as a simple regularization technique for value-based RL. In our experiments,
we use a very simple distribution P∆: we first draw a set of K values i. i. d. from Uniform (0, 1) and
normalize them to get a valid categorical distribution, i.e., αk 〜 U(0,1) followed by ak = αk/ P ai.
Proposition 1. Consider the assumptions: (a) The distribution P∆ has full support over the entire
(K - 1)-simplex. (b) Only a finite number of distinct Q-functions globally minimize the loss in (3).
(c) Q* is defined in terms ofthe MDP induced by the data distribution D.(d) Q* lies in the family of
our function approximation. Then, at the global minimum of L(θ) (7) for a multi-head Q-network:
(i)	Under assumptions (a) and (b), all the Q-heads represent identical Q-functions.
(ii)	Under assumptions (a)-(d), the common global solution is Q*.
The proof of (ii) follows from (i) and the fact that (7) is lower bounded by the TD error attained by
Q*. The proof of part (i) can be found in the supplementary material.
4.3	Experiments and Results
Asymptotic performance of offline agents. In supervised learning, asymptotic performance matters
much more than performance within a fixed budget of gradient updates. Similarly, for a given sample
complexity, we prefer RL algorithms that perform the best as long as the number of gradient updates
is feasible. Since the sample efficiency of the offline agents for a given dataset is fixed, we train them
on the DQN replay dataset for five times as many gradient updates as online DQN.
Comparison with QR-DQN. QR-DQN modifies the DQN (Nature) architecture to output K values
for each action using a multi-head Q-network and replaces RMSProp (Tieleman and Hinton, 2012)
with Adam (Kingma and Ba, 2015) for optimization. To ensure a fair comparison with QR-DQN, we
use the same multi-head Q-network as QR-DQN with K = 200 heads (Figure 3), where each head
represents a Q-value estimate for REM and Ensemble-DQN. We also use Adam for optimization.
Additional Baselines. To isolate the gains due to Adam in QR-DQN and our proposed variants,
we compare against a DQN baseline which also uses Adam. We also evaluate Averaged Ensemble-
DQN, a variant of Ensemble-DQN proposed by Anschel et al. (2017), which uses the average of the
predicted target Q-values as the Bellman target for training each parameterized Q-function. This
6
Under review as a conference paper at ICLR 2020
200% 1
≡ 150%
1 100%
5 50% -
N
0%J__L
1%
Figure 5: Effect of Dataset Size. Normalized scores (averaged over 5 runs) of QR-DQN and multi-head REM
trained offline on stochastic version of 5 Atari 2600 games for 5X gradient steps using only a fraction of the
entire DQN replay dataset (200M frames) obtained via randomly subsampling trajectories.
baseline determines whether the random combinations of REM provide any significant benefit over
simply using an ensemble of predictors to stabilize the Bellman target.
REM with separate Q-networks. To investigate whether the effectiveness of REM is linked to
ensembling techniques and dropout, we compare offline REM with K Q-value estimates computed
using K separate Q-networks vs. a multi-head Q-network for K ∈ {4, 16}.
Results. Figure 4a shows the comparison of the additional baselines with REM and Ensemble-DQN
and Table 1 summarizes the asymptotic performance results. DQN with Adam noticeably bridges the
gap in asymptotic performance between QR-DQN and DQN (Nature) in the offline setting. Offline
Ensemble-DQN does not improve upon this strong DQN baseline showing that its naive ensembling
approach is inadequate. Furthermore, Averaged Ensemble-DQN performs only slightly better than
Ensemble-DQN. In contrast, REM exploits offline data more effectively than other agents, including
QR-DQN, when trained for more gradient updates. Surprisingly, using the DQN replay dataset,
offline REM surpasses the gains from online C51, a huge improvement over online DQN (Figure 1a).
Based on above empirical results, we hypothesize that gains from REM are due to the noise from
randomly ensembling Q-value estimates leading to more robust training analogous to dropout.
Furthermore, REM with separate Q-networks performs better asymptotically and learns faster than
REM with a multi-head Q-network in the offline setting (Figure A.5). This consolidates our belief
about effectiveness of REM being linked to dropout as separate Q-networks result in more variation
in individual Q-estimates leading to more robust training.
4.4	From Offline to Online Reinforcement Learning
Can we combine the insights gained from the offline setting with appropriate design choices (e.g., ex-
ploration, replay buffer) to create effective online methods? In online RL, learning and data generation
are tightly coupled, i.e., an agent that learns faster also collects more relevant training data.
We ran online REM with K separate Q-networks (with K = 4 for computational efficiency) because
of the improved convergence speed over multi-head REM in the offline setting. For data collection,
we use -greedy with a randomly sampled Q-estimate from the simplex for each episode, similar
to Bootstrapped DQN. To estimate the gains from the REM objective (7) in the online setting, we
also evaluate Bootstrapped-DQN with identical modifications (e.g., separate Q-networks) as online
REM. Figure 1b show that REM performs on par with QR-DQN and considerably outperforms
Bootstrapped-DQN (see Figure A.8 for learning curves). Based on these results, we conclude that the
offline setting can help facilitate the design of effective online off-policy RL algorithms.
5	Offline RL: Dataset Size and Composition
Our offline learning results indicate that 50 million logged experience tuples per game from DQN (Na-
ture) are sufficient to obtain good online performance on most of the Atari 2600 games. One may
argue that the size of the fixed replay and its composition play a key role in our empirical re-
sults (De Bruin et al., 2015). To study the role of the replay buffer size in the success of the offline
agents, we perform an ablation experiment with variable replay buffer size.
We conducted offline experiments with reduced data obtained via randomly subsampling entire
trajectories from the logged DQN experiences, thereby maintaining the same data distribution.
Figure 5 presents the performance of the offline REM and QR-DQN agents with N% of the tuples in
the DQN replay dataset where N ∈ {1, 10, 20, 50, 100}. As expected, performance tends to increase
7
Under review as a conference paper at ICLR 2020
Iteration
Iteration
--- Ofline QR-DQN	Ofline REM
Iteration
0	20	40	60	80	100	0	20	40	60	80	100	0	20	40	60	80	100
Iteration
Iteration
Figure 6: Effect of Dataset Quality. Normalized scores (averaged over 3 runs) of QR-DQN and multi-head
REM trained offline on stochastic version of 5 Atari 2600 games for 5X gradient steps using logged data from
online DQN trained only for 20M frames (20 iterations). The horizontal line shows the performance of best
policy found during DQN training for 20M frames which is significantly worse than fully-trained DQN.
as the fraction of data increases. With N ≥ 10%, REM and QR-DQN still perform comparably to
online DQN on most of these games. However, the performance deteriorates drastically for N = 1%.
To see the effect of quality of offline dataset, we perform another ablation where we train offline
REM and QR-DQN on the first 20 million frames in the DQN replay dataset which approximate
exploration data with suboptimal returns. Similar to our offline results with the entire replay dataset,
on most Atari games, offline REM and QR-DQN perform comparably and outperform the best policy
amongst the mixture of policies which collected the first 20 million frames (Figure 6 and Figure A.9).
6	Discussion and Future Work
Since the (observation, action, next observation, reward) tuples in DQN replay dataset are stored in
the order they were experienced by online DQN during training, various data collection strategies for
benchmarking offline RL can be induced by subsampling the replay dataset containing 200 million
frames. For example, the first k million frames from the DQN replay dataset emulate exploration
data with suboptimal returns (e.g., Figure 6) while the last k million frames are analogous to near-
expert data with stochasticity. Another option is to randomly subsample the entire dataset to create
smaller offline datasets with sufficient diversity (e.g., Figure 5). Based on the popularity and ease of
experimentation on Atari games, we hope that the DQN replay dataset can be used for benchmarking
offline RL in addition to continuous control setups Fujimoto et al. (2019); Kumar et al. (2019).
Offline RL tackles the challenge of squeezing the most performance gains out of a fixed batch of data.
This indicates the utility of offline RL for creating more sample-efficient RL agents as an online agent
should exploit the data collected prior to any given time during online training as much as possible
before further collecting new data via exploration. The proposed offline benchmark can be utilized
for developing better RL algorithms for various data regimes shown in Figure 5.
In our offline RL setup, the agents exhibit overfitting on some games (Figure 6), even with 50 million
tuples (Figure A.7), i.e., after a sufficiently large number of gradient updates, their performance
degrades. We also observe divergence w.r.t. Huber loss in our reduced data experiments with N = 1%.
Based on these findings, we posit that our offline benchmark can be used for designing techniques akin
to regularization for stable off-policy deep RL algorithms with reasonable performance throughout
learning (Garcia and Ferndndez, 2015), a requirement for many practical RL applications.
Finally, investigating Rainbow components, e.g., prioritized replay (Schaul et al., 2016), n-step
updates (Sutton, 1988), double Q-learning (Van Hasselt et al., 2016), etc., in the offline RL setting
and incorporating them with REM are interesting directions. Experience replay-based algorithms can
be more sample efficient than model-based approaches (Van Hasselt et al., 2019), and using the DQN
replay dataset on Atari 2600 games for designing non-parametric replay models (Pan et al., 2018)
and parametric world models (Kaiser et al., 2019) is quite promising for improving sample-efficiency.
We also leave further investigation of the exploitation ability of distributional RL to future work.
7	Related work
Our work is motivated by whether the complexity of recent off-policy approaches (e.g., Rainbow (Hes-
sel et al., 2018)) is necessary and is similar in spirit to work by Rajeswaran et al. (2017), which
attempts to demystify the complexity of on-policy deep RL for continuous control. Our work is
mainly related to two subareas of RL known as batch RL and distributional RL. For the sake of clarity,
we refer to batch RL as offline RL in other sections of the paper.
8
Under review as a conference paper at ICLR 2020
Batch Reinforcement Learning. While there has been increasing interest in batch RL (Lange et al.,
2012) over the last few years (Jiang and Li, 2016; Farajtabar et al., 2018; Irpan et al., 2019), much of
this focussed on off-policy policy evaluation, where the goal is to estimate the performance of a given
fixed policy. Similar to (Ernst et al., 2005; Kalyanakrishnan and Stone, 2007; Jaques et al., 2019), we
investigate batch off-policy learning, which requires learning a good policy given a fixed dataset. In
our offline setup, we only assume access to samples from the behavior policy and focus on Q-learning
methods without any importance sampling correction as opposed to (Swaminathan and Joachims,
2015; Liu et al., 2019). Recent work (Fujimoto et al., 2019; Kumar et al., 2019) documents that
standard off-policy methods with fixed datasets fail on continuous control MuJoCo tasks. Opposed to
Kumar et al. (2019), we focus on the offline setting with data collected from a diverse mixture of
policies rather than a single Markovian behavior policy. Furthermore, in contrast to Fujimoto et al.
(2019), our batch learning results on MuJoCo tasks and Atari 2600 games (Section 3) demonstrate that
standard off-policy deep RL algorithms (e.g., TD3 (Fujimoto et al., 2018), QR-DQN (Dabney et al.,
2018b)) are quite effective when learning truly off-policy from large and diverse datasets. Zhang and
Sutton (2017) show that large replay buffers can hurt the performance of simple Q-learning methods
with weak function approximators; they attribute this to the “off-policyness” of a large buffer which
might delay important transitions for learning. However, our results reveal that even with uniform
sampling, effective deep Q-learning algorithms (e.g., REM) can exploit completely off-policy logged
DQN data on Atari 2600 games, given sufficient number of gradient updates.
Distributional Reinforcement Learning. Recently, distributional RL algorithms (e.g., C51 (Belle-
mare et al., 2017), QR-DQN (Dabney et al., 2018b), IQN (Dabney et al., 2018a)) gained popularity due
to their strong performance on Atari 2600 games. We observe that distributional algorithms (e.g., QR-
DQN, C51) are quite effective at exploitation and develop much simpler REM (Section 4.2), which
is as successful as QR-DQN. In contrast to distributional RL, much of the theoretical results of
Q-learning naturally extends to REM due to its similarity to Q-learning, e.g., in the tabular setting,
REM converges to Q*, while popular distributional methods (e.g., QR-DQN, C51) need not learn
true return statistics corresponding to the underlying MDP (Rowland et al., 2019).
8	Conclusions
This work investigates off-policy deep RL using an offline RL benchmark on Atari 2600 games based
on logged experiences of a DQN agent. We demonstrate that it is possible to learn policies with high
returns that significantly outperform the policies used to collect replay data. We develop REM, a
simple variant of ensemble Q-learning, that can effectively exploit large-scale off-policy data and
shares much of the success of the more complicated distributional RL algorithms on Atari 2600 games.
The proposed benchmark can serve as a testbed for designing simple, stable and sample-efficient RL
algorithms and enable the research community to evaluate off-policy methods on a common ground.
As mentioned by Sutton and Barto (2018, Chapter 11.10): “The potential for off-policy learning
remains tantalizing, the best way to achieve it still a mystery.”
References
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization for deep
reinforcement learning. ICML, 2017.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. Machine Learning,
1995.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 2013.
Marc G Bellemare, Will Dabney, and RCmi Munos. A distributional perspective on reinforcement learning.
ICML, 2017.
Marc G Bellemare, Nicolas Le Roux, Pablo Samuel Castro, and Subhodeep Moitra. Distributional reinforcement
learning with linear function approximation. arXiv preprint arXiv:1902.03149, 2019.
Richard Bellman. Dynamic Programming. Princeton University Press, 1957.
LCon Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly,
Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example
of computational advertising. JMLR, 2013.
9
Under review as a conference paper at ICLR 2020
Justin A Boyan and Andrew W Moore. Generalization in reinforcement learning: Safely approximating the
value function. NeurIPS, 1995.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.
Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Bellemare. Dopamine: A
research framework for deep reinforcement learning. ArXiv:1812.06110, 2018.
Kaleigh Clary, Emma Tosch, John Foley, and David Jensen. Let’s play again: Variability of deep reinforcement
learning agents in atari environments. arXiv preprint arXiv:1904.06312, 2019.
Will Dabney, Georg Ostrovski, David Silver, and RCmi Munos. Implicit quantile networks for distributional
reinforcement learning. ICML, 2018a.
Will Dabney, Mark Rowland, Marc G Bellemare, and RCmi Munos. Distributional reinforcement learning with
quantile regression. AAAI, 2018b.
Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth
Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. CoRL, 2019.
Tim De Bruin, JenS Kober, Karl Tuyls, and Robert Babuska. The importance of experience replay database
composition in deep reinforcement learning. Deep reinforcement learning workshop, NIPS, 2015.
Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning.
arXiv preprint arXiv:1904.12901, 2019.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. JMLR, 2005.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-policy
evaluation. ICML, 2018.
Stefan FaUBer and Friedhelm Schwenker. Neural network ensembles in reinforcement learning. Neural
Processing Letters, 2015.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic
methods. ICML, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.
ICML, 2019.
Javier Garcia and Fernando Ferndndez. A comprehensive survey on safe reinforcement learning. JMLR, 2015.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep
reinforcement learning that matters. 2018.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,
Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement
learning. AAAI, 2018.
PJ Huber. Robust estimation of a location parameter. Ann. Math. Stat., 1964.
Alex Irpan, Kanishka Rao, Konstantinos Bousmalis, Chris Harris, Julian Ibarz, and Sergey Levine. Off-policy
evaluation via off-policy classification. NeurIPS, 2019.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones,
Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human
preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
Stratton C Jaquette. Markov decision processes with a new optimality criterion: Discrete time. The Annals of
Statistics, 1973.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. ICML, 2016.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,
Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning
for atari. arXiv preprint arXiv:1903.00374, 2019.
10
Under review as a conference paper at ICLR 2020
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan
Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. QT-Opt: Scalable deep reinforcement learning for
vision-based robotic manipulation. CoRL, 2018.
Shivaram Kalyanakrishnan and Peter Stone. Batch reinforcement learning in a complex domain. AAMAS, 2007.
Khimya Khetarpal, Zafarali Ahmed, Andre Cianflone, Riashat Islam, and Joelle Pineau. Re-evaluate: Repro-
ducibility in evaluating reinforcement learning algorithms. 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
Roger Koenker. Quantile Regression. Cambridge University Press, 2005.
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing Off-Policy Q-Learning via Bootstrapping
Error Reduction. NeurIPS, 2019.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. Reinforcement learning,
2012.
Yann LeCun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 1998.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies.
JMLR, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
and Daan Wierstra. Continuous control with deep reinforcement learning. ArXiv:1509.02971, 2015.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine
learning, 1992.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with state
distribution correction. arXiv preprint arXiv:1904.08473, 2019.
Clare Lyle, Pablo Samuel Castro, and Marc G Bellemare. A comparative analysis of expected and distributional
reinforcement learning. AAAI, 2019.
Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling.
Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents.
Journal of Artificial Intelligence Research, 2018.
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy evaluation
across representations with applications to educational games. AAMAS, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep
reinforcement learning. Nature, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David
Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. ICML, 2016.
OPenAL MarCin Andrychowicz, Bowen Baker, Maciek Chociej, RafaI Jdzefowicz, Bob McGrew, Jakub Pachocki,
Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter
Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation. CoRR, 2018.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped
dqn. NeurIPS, 2016.
Yangchen Pan, Muhammad Zaheer, Adam White, Andrew Patterson, and Martha White. Organizing experience:
a deeper look at replay mechanisms for sample-based planning in continuous state domains. IJCAI, 2018.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &
Sons, Inc., 1994.
Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards generalization and
simplicity in continuous control. NeurIPS, 2017.
11
Under review as a conference paper at ICLR 2020
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning
method. In European Conference on Machine Learning, 2005.
Mark Rowland, Marc G Bellemare, Will Dabney, R6mi Munos, and Yee Whye Teh. An analysis of categorical
distributional reinforcement learning. AISTATS, 2018.
Mark Rowland, Robert Dadashi, Saurabh Kumar, R6mi Munos, Marc G Bellemare, and Will Dabney. Statistics
and samples in distributional reinforcement learning. ICML, 2019.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. ICLR, 2016.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. ICML, 2015.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.
Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv:1701.06538, 2017.
Susan M Shortreed, Eric Laber, Daniel J Lizotte, T Scott Stroup, Joelle Pineau, and Susan A Murphy. Informing
sequential clinical decision-making through reinforcement learning: an empirical study. Machine learning,
2011.
David Silver, Aja Huang, et al. Mastering the game of Go with deep neural networks and tree search. Nature,
2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overfitting. JMLR, 2014.
Alexander L. Strehl, John Langford, Lihong Li, and Sham Kakade. Learning from logged implicit exploration
data. NeurIPS, 2010.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. NeurIPS, 2000.
Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback through counterfactual
risk minimization. JMLR, 2015.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning, 2012.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. IROS, 2012.
John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation.
NeurIPS, 1997.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-learning. AAAI,
2016.
Hado Van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement
learning? NeurIPS, 2019.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine Learning, 1992.
Shangtong Zhang and Richard S. Sutton. A deeper look at experience replay. arXiv preprint arXiv:1712.01275,
2017.
12
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Proofs
Proposition 1. Consider the assumptions: (a) The distribution P∆ has full support over the entire
(K - 1)-simplex. (b) Only a finite number of distinct Q-functions globally minimize the loss in (3).
(c) Q* is defined in terms ofthe MDP induced by the data distribution D.(d) Q* lies in the family of
our function approximation. Then at the global minimum of L(θ) (7) for multi-head Q-network :
(i)	Under assumptions (a) and (b), all the Q-heads represent identical Q-functions.
(ii)	Under assumptions (a)-(d), the common convergence point is Q*.
Proof. Part (i): Under assumptions (a) and (b), we would prove by contradiction that each Q-head
should be identical to minimize the REM loss L(θ) (7). Note that we consider two Q-functions to be
distinct only if they differ on any state s in D.
The REM loss L(θ) = Ea〜P△ [L(α,θ)] where L(α,θ) is given by
L(α,θ) = Es,a,r,s0 〜D
'δ (〉： αkQθ (S,a) - r - Y maX X ： αkQkO (s0,aO))].
(8)
If the heads Qiθ and Qjθ don’t converge to identical Q-values at the global minimum of L(θ), it can
be deduced using Lemma 1 that all the Q-functions given by the convex combination αiQiθ + αj Qjθ
such that αi + αj = 1 minimizes the loss in (3). This contradicts the assumption that only a finite
number of distinct Q-functions globally minimize the loss in (3). Hence, all Q-heads represent an
identical Q-function at the global minimum of L(θ).
Lemma 1. Assuming that the distribution P∆ has full support over the entire (K - 1)-simplex ∆K-1,
then at any global minimum of L(θ), the Q-function heads Qθk for k = 1, . . . , K minimize L(α, θ)
for any α ∈ ∆K-1.
Proof. Let Qa*,θ* = PK=I α*zQk (s,a) corresponding to the convex combination a* =
(a*, ∙∙∙ , ɑK) represents one of the global minima of L(α,θ) (8) i.e., L(α*, θ*) = min L(α,θ)
α,θ
where α ∈ ∆K-1. Any global minima of L(θ) attains a value of L(α*, θ*) or higher since,
L(θ) = Ea〜Pδ [L(α,θ)] ≥ Ea〜P△ [L(α*,θ*)] ≥ L(α*, θ*)	(9)
Let Qθ* (s, a) = wkk* ∙ fθ*(s, a) where fθ*(s, a) ∈ RD represent the shared features among the
Q-heads and wθk* ∈ RD represent the weight vector in the final layer corresponding to the k-
th head. Note that Qa*,θ* can also be represented by each of the individual Q-heads using a
weight vector given by convex combination a* of weight vectors (wθ* ,…，WK), i.e., Q(s, a)=
(PKtI α*Wk) fθ* (s,a).
Let θI be such that QkI = Qa*,θ* for all Q-heads. By definition of Qa*,θ*, for all α 〜Pδ,
L(α, θI) = L(α*, θ*) which implies that L(θI) = L(α*, θ*). Hence, θI corresponds to one of the
global minima of L(θ) and any global minima of L(θ) attains a value of L(α*, θ*).
Since L(α, θ) ≥ L(α*, θ*) for any α ∈ ∆K-1, for any θM such that L(θM) = L(α*, θ*) implies
that L(α,θM) = L(a*,θ*) for any a 〜 Pδ. Therefore, at any global minimum of L(θ), the
Q-function heads Qθk for k = 1, . . . , K minimize L(α, θ) for any α ∈ ∆K-1.
A.2 Offline continuous control experiments
We replicated the final buffer setup as described by Fujimoto et al. (2019): We train a DDPG (Lillicrap
et al., 2015) agent for 1 million time steps three standard MuJoCo continuous control environments
in OpenAI gym (Todorov et al., 2012; Brockman et al., 2016), adding N(0, 0.5) Gaussian noise to
actions for high exploration, and store all experienced transitions. This collection procedure creates
a dataset with a diverse set of states and actions, with the aim of sufficient coverage. Similar to
Fujimoto et al. (2019), we train DDPG across 15 seeds, and select the 5 top performing seeds for
dataset collection.
13
Under review as a conference paper at ICLR 2020
OffflmeTD3	Offline DDPG	Offline BCQ	DDPG
Figure A.1: We examine the performance of DDPG, TD3 and BCQ trained using identical offline data on three
standard MuJoCo environments. We plot the mean performance (evaluated without exploration noise) across
5 runs. The shaded area represents a standard deviation. The bold black line measures the average return of
episodes contained in the offline data collected using the DDPG agent (with exploration noise).
Using this logged dataset, we train standard continuous control off-policy actor-critic methods
namely DDPG and TD3 (Fujimoto et al., 2018) completely offline without any exploration. We
also train a Batch-Constrained deep Q-learning (BCQ) agent, proposed by Fujimoto et al. (2019),
which restricts the action space to force the offline agent towards behaving close to on-policy
w.r.t. a subset of the given data. We use the open source code generously provided by the authors
at https://github.com/sfujim/BCQ and https://github.com/sfujim/TD3. We
use the hyperparameters mentioned in (Fujimoto et al., 2018; 2019) except offline TD3 which uses a
learning rate of 0.0005 for both the actor and critic.
Figure A.1 show that offline TD3 significantly outperforms the behavior policy which collected the
offline data as well as the offline DDPG agent. Noticeably, offline TD3 also performs comparably to
BCQ, an algorithm designed specifically to learn from arbitrary, fixed offline data. While Fujimoto
et al. (2019) attribute the failure to learn in the offline setting to extrapolation error (i.e., the mismatch
between the offline dataset and true state-action visitation of the current policy), our results suggest
that failure to learn from diverse offline data may be linked to extrapolation error for only weak
exploitation agents such as DDPG.
A.3 Score Normalization
The improvement in normalized performance of an offline agent, expressed as a percentage, over an
online DQN (Nature) (Mnih et al., 2015) agent is calculated as: 100 × (ScoreNormalized - 1) where:
ScoreNormalized
_________ScoreAgent - min(ScoreDQN, ScoreRandom)_______
max(ScoreDQN, ScoreRandom) - min(ScoreDQN, ScoreRandom)
(10)
Here, the scores are the mean evaluation scores averaged over 5 runs. We chose not to measure
performance in terms of percentage of online DQN scores alone because a tiny difference relative to
the random agent on some games can translate into hundreds of percent in DQN score difference.
Additionally, the max is needed since DQN performs worse than a random agent on the games Solaris
and Skiing.
A.4 Hyperparameters & Experiment Details
In our experiments, we used the hyperparameters provided in Dopamine baselines (Castro et al.,
2018) and report them for completeness and ease of reproducibility in Table 2. As mentioned by
Dopamine’s GitHub repository, changing these parameters can significantly affect performance,
without necessarily being indicative of an algorithmic difference. We will also open source our code
to further aid in reproducing our results.
The Atari environments (Bellemare et al., 2013) used in our experiments are stochastic due to sticky
actions (Machado et al., 2018), i.e., there is 25% chance at every time step that the environment will
execute the agent’s previous action again, instead of the agent’s new action. All agents (online or
offline) are compared using the best evaluation score (averaged over 5 runs) achieved during training
where the evaluation is done online every training iteration using a -greedy policy with = 0.001.
We report offline training results with same hyperparameters over 5 random seeds of the DQN replay
data collection, game simulator and network initialization.
14
Under review as a conference paper at ICLR 2020
Table 2: The hyperparameters used by the offline and online RL agents in our experiments.
Hyperparameter	setting (for both variations)	
Sticky actions Sticky action probability Grey-scaling Observation down-sampling Frames stacked Frame skip (Action repetitions) Reward clipping Terminal condition Max frames per episode Discount factor Mini-batch size Target network update period Training steps per iteration Update period every Evaluation Evaluation steps per iteration Q-network: channels Q-network: filter size Q-network: stride Q-network: hidden units Multi-head Q-network: number of Q-heads Hardware		Yes 0.25 True (84, 84) 4 4 [-1, 1] Game Over 108K 0.99 32 every 2000 updates 250K 4 steps 0.001 125K 32, 64, 64 8 × 8, 4 × 4, 3 × 3 4, 2, 1 512 200 Tesla P100 GPU
Hyperparameter	Online	Offline
Min replay size for sampling	20,000	-
Training (for -greedy exploration)	0.01	-
-decay schedule	250K steps	-
Fixed Replay Memory	No	Yes
Replay Memory size	1,000,000 steps	50,000,000 steps
Replay Scheme	Uniform	Uniform
Training Iterations	200	200 or 1000
DQN replay dataset collection. For collecting the offline data used in our experiments, we use
online DQN (Nature) (Mnih et al., 2015) with the RMSprop (Tieleman and Hinton, 2012) optimizer.
The DQN replay dataset, BDQN, consists of approximately 50 million experience tuples for each run
per game corresponds to 200 million frames due to frame skipping of four, i.e., repeating a selected
action for four consecutive frames. Note that the total dataset size is approximately 15 billion tuples (
50 million tuples * 5 agents * 60 games).
agent	game
Optimizer related hyperparameters. For existing off-policy agents, step size and optimizer were
taken as published. Offline DQN (Adam) and all the offline agents with multi-head Q-network (Fig-
ure 3) use the Adam optimizer (Kingma and Ba, 2015) with same hyperparameters as online QR-
DQN (Dabney et al., 2018b) (lr = 0.00005, Adam = 0.01/32). Note that scaling the loss has the
same effect as inversely scaling Adam when using Adam.
Online Agents. For online REM shown in Figure 1b, we performed hyper-parameter tuning over
Adam in (0.01/32, 0.005/32, 0.001/32) over 5 training games (Asterix, Breakout, Pong, Q*Bert,
Seaquest) and evaluated on the full set of 60 Atari 2600 games using the best setting (lr = 0.00005,
Adam = 0.001/32). Online REM uses 4 Q-value estimates calculated using separate Q-networks
where each network has the same architecture as originally used by online DQN (Nature). Similar to
REM, our version of Bootstrapped-DQN also uses 4 separate Q-networks and Adam optimizer with
identical hyperaparmeters (lr = 0.00005, Adam = 0.001/32).
Wall-clock time for offline experiments. The offline experiments are approximately 3X faster than
the online experiments for the same number of gradient steps on a P100 GPU. In Figure 2 and 4(a),
the offline agents are trained for 5X gradient steps, thus, the experiments are 1.67X slower than
running online DQN for 200 million frames (standard protocol). Furthermore, since the offline
experiments do not require any data generation, using tricks from supervised learning such as using
much larger batch sizes than 32 with TPUs / multiple GPUS would lead to a significant speed up.
15
(a) (b)
FigUre A3 NOnnaHZed PerformanCe Improvemenr (in %) OVer online DQN (NarUre)"per game》Of (a) Oe
QR—DQN (5x) (b) offline REM (5x) Trained using The DQN replay datasef for five Hmes as many gradienf SSPS
as online DQN∙ The normaHZed OnHne SCOre for each game is 0-0 and LO for rhe WorSe and brer PerfOnng
agenr among fully rraed online DQN and random agenrespectively
980—I
Wooo l
re7s0
∞ 680
eS58。
ag4s0
er3s0
Av280
lso
ASTERlX
14s0 J
1280-U
Sooo ɪ
Soo ɪ
680-I
4s0-l
280-I
:
OfAineDQN(NatUre) O⅞inec51 ! OnlineDQN 一
FigUre A 4 Offline C51 vs Offline DQN (Nature-AVerage online SCOreS Of C51 and DQN (NarUre) agen∞
Trained offline On SsehaSfiC version Of 6 Atari 2600 games USing The DQN replay datasef for The Same number Of
gradient: SSPS as rhe OnHne DQN agenL The SCOreS are averaged OVer 5 runs (ShoWn as〔races) and sm∞rhed
OVer a SHdg window Of 5 israrns and error bands ShOW Standard deviarn∙
1R
ICeraCion
4S—J
350」
go'
25。，
200-
157
SO-
S'
一
BREAKoUT
LteraclOn
Q*BERT
288
Sooo
≡000
goo
:
-Igoo
IUU Sc 200
ICeraCion
.ICeraCion
SEAQUEST
16
% Improvement (Log scale)
% / 电 马 O
K.⅛NO.⅞ROO
Video Pinball
Akidas
Double Dunk
James Bond
ʌ : ：：s ：/：：?
⅛∙CCVAλ
Krull
Freev,⅛y
Tutankham
Tennis
Pitfall!
Bank Heist
Journey Es.cλf∙e
H.E.B.0
Star Gunner
I
I
YsISyS Kevenoe
Robotank
Fishing Derby
Kυno-Fυ Master
Crazy Climber
Boxing
Road Runner
BeAM Rider
3ent^ede
Bowling
River Raid
Montezuma's Revenge
PooYJuJ
Battle Zone
Air Raid
Enduro
NAME This GAMe
Upand Down
Ice Hockey
Phoenix
Venture
Asterix
Frostbite
Solaris
「」:"；；，〉［：
Seaquest
Private Eye
Gravitap.
Asteroids
skiing
Chopper Command
Space Invaders
Gopher
OfWor
ATTSCK
Elevatob
FigUre AwNOrmaHZed PerfOrmanCe improvement: (in %) OVer online DQN (NarUreL Per gam5cOf (a) offline
EnSemble—DQN and (b) offline REM〔rained USg rhe DQN replay datasfor Same number Of gradient: SSPS
as online DQN∙ The normaHZed OnHne SCOre for each game is 0-0 and LO for rhe WorSe and brer PerfOnng
age∏r among fully trained online DQN and random agenrespective
G
e i
-ɔ G
S I
% Improvement (Log scale)
LIl I
% / 电 马 O
Centipede
Double Dunk
James Bond
% Improvement (Log scale)
offline EnSemble，DQN
ADDlTIoNAL PLoTS 疗 TABLES

E L evλt o r' Action
Bebzerk
Atlantis
Video Pb∙ibλll
UnderreVieW as a ConferenCe PaPersICLR 2020
Under review as a conference paper at ICLR 2020
J	Offline MUlti-network REM	Offline MUlti-head REM	Offline QR-DQN ]
səjoos BJəʌv
90001
8000-
70 7000-
60 6000-
S 5000-
第 4000 -
3∙ 3000-
W 2000-
1000-
0-
16000 1
14000-
12000-
10000-
8000-
6000-
4000-
2000-
0-
16000 -∣
14000 -
12000-
Breakout
450-∣
400-
350-
300-
250-
200-
150-
100-
50-
0-
0
100	200	300	400
Iteration
SEAQUEST
10000 -
8000-
6000-
4000-
2000-
0-
0	100	200	300	400	500
Iteration
500
(a) REM with 4 Q-value estimates (K = 4)
]	Offline MUlti-network REM	Offline MUlti-head REM	Offline QR-DQN
səjoɔs ə0*BJəʌv səjoɔs ə0*BJəʌv
Iteration	Iteration	Iteration
(b) REM with 16 Q-value estimates (K = 16)
Figure A.5: REM with Separate Q-networks. Average online scores of offline REM variants with different
architectures and QR-DQN trained on stochastic version of 6 Atari 2600 games for 500 iterations using the DQN
replay dataset. The scores are averaged over 5 runs (shown as traces) and smoothed over a sliding window of 5
iterations and error bands show standard deviation. The multi-network REM and the multi-head REM employ
K Q-value estimates computed using separate Q-networks and Q-heads of a multi-head Q-network respectively
and are optimized with identical hyperparameters. Multi-network REM improves upon the multi-head REM
indicating that the more diverse Q-estimates provided by the separate Q-networks improve performance of REM
over Q-estimates provided by the multi-head Q-network with shared features.
Table 3: Median normalized scores (Section A.3) across stochastic version of 60 Atari 2600 games, measured as
percentages and number of games where an agent achieves better scores than a fully trained online DQN (Nature)
agent. All the offline agents below are trained using the DQN replay dataset. The entries of the table without any
suffix report training results with the five times as many gradient steps as online DQN while the entires with
suffix (1x) indicates the same number of gradient steps as the online DQN agent. All the offline agents except
DQN use the same multi-head architecture as QR-DQN.
Offline agent	Median	> DQN	Offline agent	Median	> DQN
DQN (Nature) (1x)	74.4%	10	DQN (Nature)	83.4%	17
DQN (Adam) (1x)	104.6%	39	DQN (Adam)	111.9%	41
Ensemble-DQN (1x)	92.5%	26	Ensemble-DQN	111.0%	39
Averaged Ensemble-DQN (1x)	88.6%	24	Averaged Ensemble-DQN	112.1%	43
QR-DQN (1x)	115.0%	44	QR-DQN	118.9%	45
REM (1x)	103.7%	35	REM	123.8%	49
17
Under review as a conference paper at ICLR 2020
I	Online DQN (NatUre)	Ofline DQN (NatUre)	Ofline QR-DQN ∣
18000
Air Raid
Amidar
Assault
Asteroids
16000
14000
12000
10000
8000
6000
4000
2000
50	100	150	200
3000
2500
2000
1500
1000
500
0
ALIEN
100	150	200
0	50	100	150	200
5000
4000
3000
2000
1000
50	100	150	200
ASTERIX
50	100	150	200
1000
800
600
400
200
Iteration
Iteration
Iteration
Iteration
Iteration
1200000
1000000
800000
600000
400000
200000
0
Q∙I∞S Q∙,e∙IQΛV
Atlantis
40000
35000
30000
25000
20000
15000
10000
5000
0
50
100	150	200
Iteration
0
50	100	150	200
Iteration
4000-
0∙
12000
10000
8000
6000
4000
2000
Frostbite
3000∙
2000∙
1000∙
50	100	150	200
Iteration
JourneyEscape
Name This Game
16000
50	100	150
Iteration
Q*BERT
200
14000
12000
10000
8000
6000
4000
2000
Solaris
Battle Zone
Beam RIDER
200
Carnival
6000
200
ELEVATOR ACTION
100	150
Iteration
100	150
Iteration
2000-
25000-
20000-
15000-
10000-
5000-
1000
0
Iteration
Gopher
Gravitar
600
500
400
300
200
100
0
	
50	100	150
Iteration
16000
14000
12000
10000
8000
6000
4000
2000
0
50	100	150	200
Iteration
Kangaroo
200
20000
15000
8000
7000
6000
5000
4000
3000
2000
1000
0
50	100	150	200
Iteration
Krull
0	50	100	150	200
Iteration
0
50
200
0
200
100	150
16000
14000
Iteration
RIVER RAID
50	100	150
Iteration
Road Runner
60000
50000
12000
40000
10000
30000
8000
20000
6000
10000
4000
0
50	100	150	200
Iteration
2000
4000
3000
2000
1000
0
7000
6000
5000
4000
3000
2000
1000
40000-
30000-
20000-
10000-
0-
0
50	100	150
Iteration
Centipede
200
5000
4000
3000
2000
1000
50	100	150
Iteration
14000
2500
200
-2000
0
ENDURO
40
20
2000
0
1500
-20
1000
-40
500
-60
0
200
50	100	150
Iteration
25000
H.E.R.O
10000
5000
35000
30000
50	100	150
Iteration
KUNG-FU MASTER
200
25000
20000
15000
10000
5000
0
50	100	150	200
Iteration
Chopper Command
12000
10000
8000
6000
4000
2000
0
50	100	150	200
Iteration
Fishing Derby
0
50	100	150
Iteration
Ice Hockey
120000
200
1000
-10
-15
-20-
3.0-
2.5-
2.0-
1.5-
1.0-
0.5-
0.0-
50	100	150	200
Iteration
CRAZY Climber
100000
80000
60000
40000
20000
0
50	100	150	200
Iteration
50
-10
0
Freeway
40
30
20
10
0
50	100	150	200
Iteration
1200
JamesBond
800
600
400
200
)	50	100	150	200
Iteration
MONTEZUMA,S Revenge
5000-
4000-
50	100	150	200
Iteration
MS. PAC-MAN
3000-
2000-
1000-
0
-0.5-
70
60
50
40
30
20
50	100	150	200
Iteration
Robotank
10000
50	100	150	200
Iteration
Seaquest
50	100	150	200
Iteration
SKIING
8000
6000
4000
2000
0
10-
0	50	100	150	200
Iteration
Wizard OF Wor
0	50	100	150	200
Iteration
50	100	150	200
Iteration
10000
Zaxxon
30000
25000
20000
15000
10000
5000
0
YARS,S REVENGE


Qws ,e,ΛV Q∙I∞S 39∙,λv Q9∙,ΛV



0
0
0

0
0
0

0
0
0
0
0
0
0

0
0	50	100	150	200
Iteration
0	50	100	150	200	0	50	100	150	200	0	50	100	150	200
Iteration	Iteration	Iteration
Figure A.6: Average evaluation scores across stochastic version of 60 Atari 2600 games for online DQN, offline
DQN and offline QR-DQN trained for 200 iterations. The offline agents are trained using the DQN replay dataset.
The scores are averaged over 5 runs (shown as traces) and smoothed over a sliding window of 5 iterations and
error bands show standard deviation. The horizontal line shows the performance of the best policy (averaged
over 5 runs) found during training of online DQN.
18
Under review as a conference paper at ICLR 2020
I Ofline DQN (Adam)	Ofline Ensemble-DQN	Offline QR-DQN	Offline REM ∣
18000
16000
14000
12000
10000
Assault
Asterix
0	200	400	600	800 1000
Q∙I∞S 3∙∙ov
45000
40000
35000
30000
25000
20000
15000
10000
5000
0	200	400	600	800 1000
Iteration
DemonAttack
40000
30000
20000
10000
0
0
0	200	400	600	800 1000	0	200	400	600	800 1000	0	200	400	600	800 1000
0
200
1000
0
1000
Iteration
200	400	600	800 1000
Iteration
200	400	600	800
Iteration
Frostbite
1000
5000
4000
3000
2000
1000
0
0
200	400	600	800
Iteration
35000
400	600	800
Iteration
Gopher
1000
30000
25000
20000
15000
10000
5000
0
-5000
0	200	400	600	800 1000
Iteration
800
700
600
500
400
300
200
100
0
-100
0	200	400	600	800 1000
Iteration
10000-
JOURNEYESCAPE
0-
16000
14000
12000
10000
8000
6000
4000
2000
0
Kangaroo
0	200	400	600	800 1000
16000
14000
12000
10000
Iteration
0
200
800
200
400	600	800
Iteration
1000
400	600
Iteration
1000
16000
14000
12000
10000
8000
6000
4000
2000
0
Q*Bert
25000
RiverRaid
20000
15000
10000
5000
0	200	400	600	800 1000
Iteration
200	400	600	800
Iteration
2500
Solaris
14000
Space Invaders
2000
1500
1000
500
60000
200	400	600	800
Iteration
UP and Down
12000
1000
400-
50000
40000
14000
12000
10000
8000
6000
4000
2000
0
-2000
Chopper Command
CRAZY CLIMBER
200000-
0	200	400	600	800 1000	0	200	400	600	800 1000
Iteration	Iteration
Fishing Derby	Freeway
0
1000
0
200	400	600	800
Iteration
Ice Hockey
25000
1000
20000
800
15000
600
10000
400
5000
200
0
0	200	400	600	800 1000
Iteration
h.e.r.o
200	400	600	800
Iteration
GRAVITAR
0
0
1000
0	200	400	600	800 1000
Iteration
200	400	600	800
Iteration
8000
7000
6000
5000
4000
3000
2000-
1000-
Krull
40000
35000
30000
25000
20000
15000
10000
5000
0
Kung-Fu Master
0	200	400	600	800 1000
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0-
-0.5-
MONTEZUMA,S REVENGE
0	200	400	600	800 1000
Iteration
Iteration
Pitfall!	―	PONG
0
1000
200	400	600	800
Iteration
200	400	600	800
Iteration
Pooyan
200	400	600	800 1000
Iteration
JamesBond
0
200	400	600	800 1000
Iteration
6000
5000
Ms. Pac-Man
4000
3000
2000
1000
1000	0
15000 -∣
200	400	600	800 1000
Iteration
Private Eye
10000-
70000
60000
50000
40000
30000
20000
10000
-10000-
1000
70000
60000
50000
40000
30000
20000
10000
0
10000
8000
6000
4000
2000
200
800
1000
400	600
Iteration
Venture
300-
30000
20000
10000
0
200
100
0	200	400	600	800 1000
Iteration
Road Runner
200	400	600	800 1000
Iteration
Star Gunner
80
70
60
50
40
30
20
10
0
0
800
1000
0
Kobotank
Seaquest
Skiing
12000
10000
8000
6000
4000
2000
200	400	600
Iteration
400	600	800 1000
Iteration
200	400	600
Iteration
0
0	200	400	600	800 1000
Iteration
200	400	600	800
Iteration
1000
0	200	400	600	800 1000
Iteration
Tennis
8000
Time Pilot
5-
0
—10 ■
-15-
-20-
-25-
7000
6000
5000
4000
3000
2000
1000
0
700000
600000
0	200	400	600	800 1000
Iteration
200	400	600	800 1000
Iteration
Video Pinball
500000
400000
300000
200000
100000
8000
10000
200	400	600	800
Iteration
Wizard OF Wor
1000
40000
35000
0	200	400	600	800 1000
Iteration
200	400	600	800
Iteration
Yars'sRevenge
1000
0	200
12000
400	600	800 1000
Iteration
Zaxxon
6000
4000
2000
30000
10000
200	400	600	800 1000
Iteration
25000
20000
15000
10000
5000
8000
6000
4000
2000
0	200	400	600	800 1000
Iteration
200	400	600	800 1000
Iteration
Q∙I∞S QWeZQAV
3.ι∞s ewe8eAV
0JO3S ewe8eAV
Q∙I∞S QWeZQAV
Q∙I∞S QWeZQAV
Q∙I∞S QWeZQAV
3.ι∞s ewe8eAV
3.ι∞s ewe8eAV
3.ι∞s ewe8eAV

0
0
0
0


0
0
0
0
0
0
0
0



0
Figure A.7: Average evaluation scores across stochastic version of 60 Atari 2600 games of DQN (Adam),
Ensemble-DQN, QR-DQN and REM agents trained offline using the DQN replay dataset. The horizontal line
for online DQN show the best evaluation performance it obtains during training. All the offline agents except
DQN use the same multi-head architecture with K = 200 heads. The scores are averaged over 5 runs (shown as
traces) and smoothed over a sliding window of 5 iterations and error bands show standard deviation.
19
Under review as a conference paper at ICLR 2020
DQN (Nature)	C51	Bootstrapped-DQN	QR-DQN	REM
14000
12000
10000
8000
6000
4000
2000
AIRRAID
6000
5000
4000
3000
2000
1000
ALIEN
3000
2500
2000
1500
1000
500
0
AMIDAR
25000-
20000-
15000-
10000-
5000-
0	50	100	150	200	0	50	100	150	200
Iteration	Iteration
0	50	100	150	200	0	50	100	150	200	0	50	100	150	200
Iteration	Iteration	Iteration
0	50	100	150	200
Iteration
1200000
1000000
800000
600000
400000
200000
ATLANTIS
BankHeist
3∙1∞s 3∙,e∙I3Λ<
3∙I∞S 3∙,e∙I3Λ<
1200
1000
800
600
400
200
0	50	100	150	200	0	50	100	150	200
Iteration	Iteration
DemonAttack
Double Dunk
16000
14000
12000
10000
8000
6000
4000
2000
50	100	150	200
Iteration
50	100	150
Iteration
JourneyEscape
16000
14000
12000
10000
8000
6000
4000
2000
0
Kangaroo
0
25000 -∣
50	100	150	200
Iteration
NAME THIS GAME
50
100	150
Iteration
Q*BERT
200
3∙1∞s 3∙,e∙I3Λ<
18000
16000
14000
12000
10000
50	100	150
Iteration
200
3∙I∞S 3∙,e∙I3Λ<
40000
35000
30000
25000
20000
15000
10000
5000
0
SOLARIS
50	100	150	200
Iteration
UP AND DOWN
40000
35000
30000
25000
20000
15000
10000
5000-
Battle Zone
0	50	100	150	200
Iteration
6000
5000
4000
3000
2000
1000
CARNIVAL
0	50	100	150	200
Iteration
100000
200
0
25000-∣
50	100	150	200
Iteration
Phoenix
0
50
100	150
25000
Iteration
KiverKaid
200
20000
15000
10000
5000
50	100	150
Iteration
20000-
15000
Elevator Action
80000
60000
40000
20000
0
0	50	100	150	200
Iteration
70000
50	100	150	200
Iteration
Koad Kunner
200
-10000
，	0
60000
50000
40000
30000
20000
10000
0
50	100	150
Iteration
200
Space Invaders
10000
5000
80000
70000
60000
50000
40000
30000
20000
10000
0
Star Gunner
1600
1400
1200
1000
800
600
400
200
0-
50	100	150	200
Iteration
Venture
600000
500000
700000
50	100	150
Iteration
Video PINBALL
200
400000
300000
200000
100000
0	50	100	150	200
Iteration
Centipede
0	50	100	150	200
Iteration
40000
35000
30000
25000
20000
15000
10000
5000
0
0
40000-∣
30000-
20000-
10000-
0	50	100	150	200
Iteration
0	50	100	150	200
Iteration
Fishing Derby
50	100	150	200
Iteration
Kung-FuMaster
0	50	100	150	200
Iteration
50
70
60
50
40
30
20
10-
0-
-10-∣-
0
30-
20-
10-
0-
-10-
-20-
—30 ■
12000
10000
0	100	150
Iteration
Kobotank
50	100
Iteration
Tennis
140
120
100
80
60
40-
20-
-20-
Bowling
0	50	100	150	200
Iteration
160000
140000
120000
100000
80000
60000
40000
20000
Crazy Climber
0	50	100	150	200
Iteration
50	Freeway
40-
30-1
0	50	100	150	200
Iteration
2500
2000
1500
1000
500
0
MONTEZuma’s Kevenge
0	50	100	150	200
Iteration
0	50	100	150	200
Iteration
0	50	100	150	200
Iteration
200
0
50
200
0
50
100	150
100	150	200
0-
0-
50	100	150
Iteration
Wizard Of WOR
200
60000
50	100	150
Iteration
Yars’s revenge
200
8000
6000
4000
2000
50000
40000
30000
20000
10000
14000
12000
10000
8000
6000
4000
2000
0-
-2000-
50	100	150	200
Iteration
Zaxxon
a.ι∞s 3we∙J3AV
a.ι∞s 3we∙J3AV
a.ι∞s 3we∙J3AV

a.ι∞s 3we∙J3AV
a.ι∞s 3we∙J3AV
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
L2
0	50	100	150	200
Iteration
50	100	150	200
Iteration
50	100	150
Iteration
0	50	100	150	200
Iteration
0	50	100	150	200
Iteration
0	50	100	150	200
Iteration
Figure A.8: Online results. Average evaluation scores across stochastic version of 60 Atari 2600 games of DQN,
C51, QR-DQN, Bootstrapped-DQN and REM agents trained online for 200 million game frames (standard
protocol). The scores are averaged over 5 runs (shown as traces) and smoothed over a sliding window of 5
iterations and error bands show standard deviation.
20
Under review as a conference paper at ICLR 2020
I OflineQR-DQN OflineREMl
-1000
40000
35000
Q∙I∞S 3∙∙ov
4500
4000
3500
3000
2500
2000
1500
1000
500
0-
Assault
Asterix
Iteration
BOWLING
20	40	60	80	100
Iteration
Breakout
20	40	60	80	100
Iteration
BOXING
5000
4500
4000
3500
3000
2500
2000
1500
1000
20	40	60	80	100
Iteration
Demon Attack
20	40	60	80
Iteration
Double Dunk
0
14000-∣
40	60
Iteration
Frostbite
-10000
-20000
-30000
0	20	40	60	80	100
Iteration
JOURNEYESCAPE
Q∙I∞S QWeZQAV
Q∙I∞S QWeZQAV
)40	60	：
Iteration
UP and Down
0	20	40	60	80	100
Iteration
40	60	80	100
Iteration
Gopher
20	40	60	80	100
Iteration
Phoenix
-2000
40	60
Iteration
3500
3000
2500
2000
1500
1000
500
0
40	60
Iteration
CARNIVAl
40	60
Iteration
Centipede
20	40	60	80
Iteration
Chopper Command
20	40	60	80
Iteration
Crazy CLIMBER
4000
3500
3000
2500
2000
1500
1000
500
140000
120000
100000
80000
60000
40000
20000
0
Space Invaders
40	60
Iteration
Venture
350000
300000
250000
200000
150000
100000
50000
0
-50000 -
I 0
Iteration
400
350
300
250
200
150
100
50
0
7000
6000
5000
4000
3000
2000
1000
0
40	60
Iteration
GRAVITAR
40	60
Iteration
KRULL
40	60
Iteration
PITFALL
40	60
Iteration
Video PINBALl
40	60
Iteration
road Runner
40	60	80
Iteration
Star Gunner
20	40	60
Iteration
Robotank
20	40	60	80
Iteration
Kung-Fu Master
40	60
Iteration
40	60
Iteration
HERO
40	60
Iteration
Enduro
0
20
20	40	60	80
40	60	80 100
0
40	60
20	40	60	80 100
-10000
1500
1000
0
40	60
40	60
40	60
40	60
Iteration
Pong
40	60
Iteration
Seaquest
40	60
Iteration
Skiing
500
-15000
-20000
-25000
-30000
-35000


Figure A.9: Effect of Dataset Quality. Normalized scores (averaged over 3 runs) of QR-DQN and multi-head
REM trained offline on stochastic version of 60 Atari 2600 games for 5X gradient steps using logged data from
online DQN trained only for 20M frames (20 iterations). The horizontal line shows the performance of best
policy found during DQN training for 20M frames which is significantly worse than fully-trained DQN.
21