Under review as a conference paper at ICLR 2020
Ellipsoidal Trust Region Methods for Neural
Network Training
Anonymous authors
Paper under double-blind review
Ab stract
We investigate the use of ellipsoidal trust region constraints for second-order
optimization of neural networks. This approach can be seen as a higher-order
counterpart of adaptive gradient methods, which we here show to be interpretable as
first-order trust region methods with ellipsoidal constraints. In particular, we show
that the preconditioning matrix used in RMSProp and Adam satisfies the necessary
conditions for provable convergence of second-order trust region methods with
standard worst-case complexities. Furthermore, we run experiments across different
neural architectures and datasets to find that the ellipsoidal constraints constantly
outperform their spherical counterpart both in terms of number of backpropagations
and asymptotic loss value. Finally, we find comparable performance to state-of-
the-art first-order methods in terms of backpropagations, but further advances in
hardware are needed to render Newton methods competitive in terms of time.
1 Introduction
We consider finite-sum optimization problems of the form
n
min L(w) := X `(f (w, xi, yi)) ,	(1)
w∈Rd
i=1
which typically arise in neural network training, e.g. for empirical risk minimization over a set
of data points (xi, yi) ∈ Rin × Rout, i = 1, . . . , n. Here, ` : Rout × Rout → R+ is a convex
loss function and f : Rin × Rd → Rout represents the neural network mapping parameterized
by the concatenation of layers w ∈ Rd, which is non-convex due to its multiplicative nature
and potentially non-linear activation functions. We assume that L is lower bounded and twice
differentiable, i.e. L ∈ C2 (Rd, R) and consider finding a first- and second-order stationary point
W for which ∣∣VL(λv) ∣∣ ≤ Eg and λmin (V2L(λv)) ≥ -EH. Non-Convex optimization problems are
ubiquitous in machine learning. Among the most prominent examples are present-day deep neural
networks, that have achieved outstanding results on core tasks such as collaborative filtering (Wang
et al., 2015), sentence classification (Kim, 2014) and image classification (Krizhevsky et al., 2012).
In the era of big data and deep neural networks, stochastic gradient descent (SGD) is one of the
most widely used training algorithms (Bottou, 2010). What makes SGD so attractive is its simplicity
and per-iteration cost that are independent of the size of the training set (n) and scale linearly in the
dimensionality (d). However, gradient descent is known to be inadequate to optimize functions that
are ill-conditioned (Nesterov, 2013; Shalev-Shwartz et al., 2017) and thus adaptive gradient methods
that employ dynamic, coordinate-wise learning rates based on past gradients—including Adagrad
(Duchi et al., 2011), RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2014)—have
become a popular alternative, often providing significant speed-ups over SGD. Yet, there exist no
theoretical proofs that these methods are faster than gradient descent (Li & Orabona, 2018).
From a theoretical perspective, Newton methods provide stronger convergence guarantees by appro-
priately transforming the gradient in ill-conditioned regions according to second-order derivatives. It
is precisely this Hessian information that allows regularized Newton methods to enjoy superlinear
local convergence as well as to provably escape saddle points (Conn et al., 2000). While second-order
algorithms have a long-standing history even in the realm of neural network training (Hagan &
Menhaj, 1994; Becker et al., 1988), they were mostly considered as too computationally and memory
1
Under review as a conference paper at ICLR 2020
expensive for practical applications. Yet, the seminal work of Martens (2010) renewed interest for
their use in deep learning by proposing efficient Hessian-free methods that only access second-order
information via matrix-vector products which can be computed at the cost of an additional backprop-
agation (Pearlmutter, 1994; Schraudolph, 2002). Among the class of regularized Newton methods,
trust region (Conn et al., 2000) and cubic regularization algorithms (Cartis et al., 2011) are the most
principled approaches in the sense that they yield the strongest convergence guarantees. Recently,
stochastic extensions have emerged (Xu et al., 2017b; Yao et al., 2018; Kohler & Lucchi, 2017;
Gratton et al., 2017), which suggest their applicability for deep learning.
We here propose a simple modification to make TR methods even more suitable for neural network
training. Particularly, we build upon the following alternative view on adaptive gradient methods:
While gradient descent can be interpreted as a spherically constrained first-order TR method, precon-
ditioned gradient methods—such as Adagrad—can be seen as first-order TR methods with ellipsoidal
trust region constraint.
This observation is particularly interesting since spherical constraints are blind to the underlying
geometry of the problem, but ellipsoids can adapt to local landscape characteristics, thereby allowing
for more suitable steps in regions that are ill-conditioned. We will leverage this analogy and investigate
the use of the Adagrad and RMSProp preconditioning matrices as ellipsoidal trust region shapes
within a stochastic second-order TR algorithm (Xu et al., 2017a; Yao et al., 2018). Since no ellipsoid
fits all objective functions, our main contribution lies in the identification of adequate matrix-induced
constraints that lead to provable convergence and significant practical speed-ups for the specific case
of deep learning. On the whole, our contribution is threefold:
•	We provide a new perspective on adaptive gradient methods that contributes to a better
understanding of their inner-workings. Furthermore, we empirically find that many neu-
ral network problems exhibit diagonally dominated Hessian matrices which suggests the
effectivity of diagonal preconditioning. (Section 3)
•	We investigate the first application of ellipsoidal TR methods for deep learning. In Theorem 1
we show that the RMSProp matrix can directly be applied as constraint inducing norm in
second-order TR algorithms while preserving all convergence guarantees (Theorem 2).
•	Finally, we provide an experimental benchmark across different real-world datasets and
architectures. We also compare against adaptive gradient methods and show results in terms
of backprogations, epochs, and wall-clock time; a comparison we were not able to find in
the literature. (Section 5)
Our main empirical results demonstrate that ellipsoidal constraints prove to be a very effective
modification of the trust region method in the sense that they constantly outperform the spherical TR
method, both in terms of number of backprogations and asymptotic loss value on a variety of tasks.
2	Related work
First-order methods The prototypical method for optimizing Eq. (1) is SGD (Robbins & Monro,
1951). While the practical success of SGD in non-convex optimization is unquestioned, the theoretical
foundation of this phenomenon is still rather limited. Recent findings suggest the ability of this
method to escape saddle points and reach local minima in polynomial time for general non-convex
problems, but they either need to artificially add noise to the iterates (Ge et al., 2015; Lee et al., 2016)
or make an assumption on the inherent noise of vanilla SGD (Daneshmand et al., 2018). For neural
network training, a recent line of research proclaims the effectiveness of SGD, but the results usually
come at the cost of fairly strong assumptions such as heavy overparametrization and Gaussian inputs
(Du et al., 2017; Brutzkus & Globerson, 2017; Li & Yuan, 2017; Du & Lee, 2018; Allen-Zhu et al.,
2018). Adaptive gradient methods (Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba,
2014) build on the intuition that larger learning rates for smaller gradient components and smaller
learning rates for larger gradient components balance their respective influences and thereby make
the methods behave as if they were optimizing a more isotropic surface. Such approaches have first
been suggested for neural networks by LeCun et al. (2012). Recently, convergence guarantees for
such methods are starting to appear (Ward et al., 2018; Li & Orabona, 2018). However, these are not
superior to the O(g-2) worst-case complexity of standard gradient descent (Cartis et al., 2012b).
2
Under review as a conference paper at ICLR 2020
Regularized Newton methods The most principled class of regularized Newton methods are trust
region (TR) and adaptive cubic regularization algorithms (ARC) (Conn et al., 2000; Cartis et al.,
2011), which repeatedly optimize a local Taylor model of the objective while making sure that the step
does not travel too far such that the model stays accurate. While the former finds first-order stationary
points within O(g-2), ARC only takes at most O(g-3/2). However, simple modifications to the TR
framework allow these methods to obtain the same accelerated rate (Curtis et al., 2017). Both methods
take at most O(-H3) iterations to find an H approximate second-order stationary point (Cartis et al.,
2012a). These rates are optimal for second-order Lipschitz continuous functions (Carmon et al.,
2017; Cartis et al., 2012a) and they can be retained even when only sub-sampled gradient and Hessian
information is used (Kohler & Lucchi, 2017; Yao et al., 2018; Xu et al., 2017b; Blanchet et al., 2016;
Liu et al., 2018; Cartis & Scheinberg, 2017). Furthermore, the involved Hessian information can be
computed solely based on Hessian-vector products, which are implementable efficiently for neural
networks (Pearlmutter, 1994). This makes these methods particularly attractive for deep learning, but
the empirical evidence of their applicability is so far very limited. We are only aware of the works of
Liu et al. (2018) and Xu et al. (2017a), which report promising first results but these are by no means
fully encompassing.
Gauss-Newton methods An interesting line of research proposes to replace the Hessian by (approx-
imations of) the generalized-Gauss-Newton matrix (GGN) within a Levenberg-Marquardt framework1
(LeCun et al., 2012; Martens, 2010; Martens & Grosse, 2015). These methods have been termed
hessian-free since only access to GGN-vector products is required. As the GGN matrix is always pos-
itive semidefinite, they cannot leverage negative curvature to escape saddles and hence, there exist no
second-order convergence guarantees. Furthermore, there are cases in neural network training where
the Hessian is better conditioned than the GGN matrix (Mizutani & Dreyfus, 2008). Nevertheless,
the above works report promising preliminary results, most notably Grosse & Martens (2016) report
that K-FAC can be faster than SGD on a small convnet. On the other hand, recent findings report
performance at best comparable to SGD on the much larger ResNet architecture (Ma et al., 2019).
Moreover, Xu et al. (2017a) reports many cases where TR and GGN algorithms perform similarly.
This line of work is to be seen as complementary to our approach since it is straight forward to replace
the Hessian in the TR framework with the GGN matrix. Furthermore, the preconditioners used in
Martens (2010) and Chapelle & Erhan (2011), namely diagonal estimates of the empirical Fisher and
Fisher matrix, respectively, can directly be used as matrix norms in our ellipsoidal TR framework.
3	An alternative view on adaptive gradient methods
Adaptively preconditioned gradient methods update iterates as wt+ι = Wt - ηt A-1/2gt, where gt
is a stochastic estimate of ▽£ (Wt) and At is a positive definite symmetric pre-conditioning matrix.
In Adagrad, Aada,t is the un-centered second moment matrix of the past gradients computed as
Aada,t := GtGt| +I,	(2)
where > 0, I is the d × d identity matrix and Gt = [g1, g2, . . . , gt]. Building up on the intuition
that past gradients might become obsolete in quickly changing non-convex landscapes, RMSprop
(and Adam) introduce an exponential weight decay leading to the preconditioning matrix
Arms,t ：= ((1 - β)Gt diag(βt,…，β0)G∣) + eI,	(3)
where β ∈ (0, 1). In order to save computational efforts, the diagonal versions diag(Aada) and
diag(Arms) are more commonly applied in practice, which in turn gives rise to coordinate-wise
adaptive stepsizes that are enlarged (reduced) in coordinates that have seen past gradient components
with a smaller (larger) magnitude. In that way, the optimization methods can account for gradients of
potentially different scales arising from e.g. different layers of the networks.
3.1	Adaptive preconditioning as ellipsoidal Trust Region
Starting from the fact that adaptive methods employ coordinate-wise stepsizes, one can take a
principled view of these methods. Namely, their update steps arise from minimizing a first-order
1This algorithm is a simplified TR method, initially tailored for non-linear least squares problems (Nocedal
& Wright, 2006)
3
Under review as a conference paper at ICLR 2020
Taylor model of the function L within an ellipsoidal search space around the current iterate wt , where
the diameter of the ellipsoid along a particular coordinate is implicitly given by ηt and kgt kA-1 .
Correspondingly, vanilla (S)GD optimizes the same first-order model within a spherical constraint.
Fig. 1 (top) illustrates this effect by showing not only the iterates of GD and Adagrad but also the
implicit trust regions within which the local models were optimized at each step.2 Since the models
are linear, the constrained minimizer is always found on the boundary.
-8-6-4-2	0	2	4	6 β	-8-6-4-2	0	2	4	6 β	-8-6-4-2	0	2	4	6 β
K = 2	K = 20	K = 20
Figure 1: Top: Iterates and implicit trust regions of GD and Adagrad on three quadratic objectives With
different condition number κ. Bottom: Average log suboptimality over iterations as well as 90% confidence
intervals of 30 runs with random initialization
It is well known that GD struggles to progress towards the minimizer of quadratics along low-
curvature directions (see e.g., Goh (2017)). While this effect is negligible for well-conditioned
objectives (Fig. 1, left), it leads to drastically slow-down when the problem is ill-conditioned (Fig. 1,
center). Particularly, once the method has reached the bottom of the valley, it struggles to make
progress along the horizontal axis. Here is precisely where the advantage of adaptive stepsize methods
comes into play. As illustrated by the dashed lines, Adagrad,s search space is damped along the
direction of high curvature (vertical axis) and elongated along the low curvature direction (horizontal
axis). This allows the method to move further horizontally early on to enter the valley with a smaller
distance to the optimizer W along the low curvature direction which accelerates convergence.
Theorem 1 (Preconditioned gradient methods as TR). A preconditioned gradient step
Wt+1 - Wt = St ：= -ηtA-1gt	(4)
with stepsize ηt > 0, symmetric positive definite preconditioner At ∈ Rd×d and gt = 0
minimizes a first-order model around Wt ∈ Rd in an ellipsoid given by At in the sense that
st =argmRnd	*(S)=L(Wt)+s|gt], s..	kskAt	≤ηtkgtkA-1.	⑸
Corollary 1 (Rmsprop). The step SrmS,t ：= -ηtArmstgt minimizes a first-order Taylor model
1/2
around Wt in an ellipsoid given by ArmS,t (Eq. 3) in the sense that
SrmS,t ：= arg min ImI(S) = L(Wt) + s|gt] , St ∣∣sk a1/2	≤ ηt∣∣gt∣∣人-”.	(6)
s∈Rd	Arms,t	^Arm,s,t
Equivalent results can be established for Adam using gadam,t ：= (1 一 β) Pk=o βt-kgt as well as for
Adagrad by replacing the matrix Aada into the constraint in Eq. (6). Of course, the update procedure
in Eq. (5) is merely a reinterpretation of the original preconditioned update, and thus the employed
trust region radii are defined implicitly by the current gradient and stepsize.
2For illustrative purposes, we only plot every other trust region.
4
Under review as a conference paper at ICLR 2020
3.2	Diagonal versus full preconditioning
A closer look at Fig. 1 reveals that the first two problems come with level sets that are perfectly
axis-aligned, which makes these objectives particularly attractive for diagonal preconditioning. For
comparison, on the right of Fig. 1, we report another quadratic problem instance, where the Hessian
is no longer zero on the off-diagonals. As can be seen, the interaction between coordinates introduces
a tilt in the level sets and reduces the superiority of diagonal Adagrad over plain GD. However, using
the full preconditioner Aada re-establishes the original speed up. Yet, non-diagonal preconditioning
comes at the cost of taking the inverse square root of a large matrix, which is why this approach has
been relatively unexplored (see Agarwal et al. (2018) for a recent exception).
Interestingly, early results by Becker et al. (1988) on the curvature structure of neural nets report
a strong diagonal dominance of the Hessian matrix V2L(w). This suggests that the loss surface
is indeed somewhat axis-aligned. However, the reported numbers are only for tiny feed-forward
networks of at most 256 parameters. Therefore, we generalize these findings in the following to
larger networks. Furthermore, we contrast the diagonal dominance of real Hessian matrices to the
expected behavior of random Wigner matrices. Of course, true Hessians do not have i.i.d. entries
but the symmetry of Wigner matrices suggests that this baseline is not completely off. Furthermore,
we also compare Hessians of Ordinary Least Squares problems (OLS) with random inputs. For this
P |Aii|
purpose, let 6a define the ratio of diagonal to overall mass of a matrix A, i.e. 6& := PP 提.∣ as
in (Becker et al., 1988).
Proposition 1 (Diagonal share of Wigner matrix). For random Gaussian3 Wigner matrix W formed
as
Wi,j = Wj,i
J~ N(O,σ汽 i < j
l~ N(O,σ2^ i =j,
(7)
where 〜StandSfor i.i.d. draws (Wigner, 1993), the diagonal mass ofthe expected absolute matrix
amounts to
δE[∣W∣]
(8)
Thus, if we suppose the Hessian at any given point w were a random Wigner matrix we would expect
the share of diagonal mass to fall with O(1/d) as the network grows in size. A similar result can be
derived for the large n limit in the case of OLS Hessians.
Proposition 2 (Diagonal share of OLS Hessian). Let X ∈ Rd×n and assume each xi,j is generated
i.i.d. with zero-mean finite second moment σ2 > O. Then the share of diagonal mass of the expected
matrix E [|Hols|] amounts to
(9)
Again, we expect the diagonal mass to fall in d and interestingly, empirical simulations suggest that
the result of Proposition 2 holds already in small n settings (see Fig.D.2) and it is likely that finite
n results can be derived when adding assumptions such as Gaussian data. As can be seen in Fig. 2
below, even for a practical batch size of n = 32 the diagonal mass δH of neural networks stays above
both benchmarks when its input is random as well as with real-world data at random initialization,
during training and after convergence.
3The argument naturally extends to any distribution with positive expected absolute values.
5
Under review as a conference paper at ICLR 2020
Figure 2: Diagonal mass of neural network Hessian 6h relative to 6e[∣w∣] and 匹㈣口印.]of corresponding
dimensionality for random inputs as well as at random initialization, middle and end of training with RMSProp
on CIFAR-10. Mean and 95% CI over 10 independent runs.
These findings are in line with Becker et al. (1988) and suggest that full matrix preconditioning is
most probably not worth the additional computational cost for neural networks. Consequently, we
use diagonal preconditioning for both first- and second-order methods in all of our experiments in
Section 5. Further theoretical elaboration of these findings present an interesting direction of future
research.
4	Second-order Trust Region Methods
Cubic regularization (Nesterov & Polyak, 2006; Cartis et al., 2011) and trust region methods belong
to the family of globalized Newton methods. Both frameworks compute parameter updates by
optimizing regularized (former) or constrained (latter) second-order Taylor models of the objective L
around the current iterate wt.4 In particular, in iteration t the update step of the trust region algorithm
is computed as
min	mt(s)	:=	L(Wt) +	g|s	+ 1 S1BtS	,	StkskAt	≤ ∆	(10)
s∈Rd	2	t
where ∆t > 0 and gt and Bt are either ▽£(Wt) and V2L(Wt) or suitable approximations. The
matrix At induces the shape of the constraint set. So far, the common choice for neural networks is
At := I, ∀t which gives rise to spherical trust regions (Xu et al., 2017a; Liu et al., 2018). By solving
the constrained problem (10), TR methods overcome the problem that pure Newton steps may be
ascending, attracted by saddles or not even computable. See Appendix B for more details.
4.1	Convergence of ellipsoidal Trust Region methods
Inspired by the success of adaptive gradient methods, we investigate the use of their preconditioning
matrices as norm inducing matrices for second-order TR methods. The crucial condition for con-
vergence is that the applied norms are not degenerate during the entire minimization process in the
sense that the ellipsoids do not flatten out (or blow up) completely along any given direction. The
following definition formalizes this intuition.
Definition 1 (Uniformly equivalent norms). The norms kWkAt := (W|AtW)1/2 induced by sym-
metric positive definite matrices At are called uniformly equivalent, if ∃μ ≥ 1 such that
11HIAt ≤ IMS ≤μkw∣",	∀w∈Rd,∀t = 1,2,....	(ii)
μ
We now establish a result which shows that the RMSProp ellipsoid is indeed uniformly equivalent.
4In the following we only treat TR methods, but we would like to emphasize that the use of matrix induced
norms can directly be transferred to the cubic regularization framework.
6
Under review as a conference paper at ICLR 2020
Lemma 1 (Uniform equivalence). Suppose ∣∣gtk2 ≤ LH for all Wt ∈ Rd, t = 1,2,... Then
there always exists e > 0 such that the proposed preconditioning matrices Arms ,t (Eq. 3) are
uniformly equivalent, i.e. Def. 1 holds. The same holds for the diagonal variant.
Consequently, the ellipsoids Arms,t can directly be applied to any convergent TR framework without
losing convergence guarantees (Conn et al. (2000), Theorem 6.6.8).5 Interestingly, this result cannot
be established for Aada,t, which reflects the widely known vanishing stepsize problem that arises
since squared gradients are continuously added to the preconditioning matrix. At least partially, this
effect inspired the development of RMSprop (Tieleman & Hinton, 2012) and Adadelta (Zeiler, 2012).
Why ellipsoids? There are many sources for ill-conditioning in neural networks such as un-centered
and correlated inputs (LeCun et al., 2012), saturated hidden units, and different weight scales in
different layers (Van Der Smagt & Hirzinger, 1998). While the quadratic term of model (10) accounts
for such ill-conditioning to some extent, the spherical constraint is completely blind towards the
loss surface. Thus, it is advisable to instead measure distances in norms that reflect the underlying
geometry (see Chapter 7.7 in Conn et al. (2000)). The ellipsoids we propose are such that they allow
for longer steps along coordinates that have seen small gradient components in past and vice versa.
Thereby the TR shape is adaptively adjusted to fit the current region of the non-convex loss landscape.
This procedure is not only effective when the iterates are in an ill-conditioned neighborhood of a
minimizer (Figure 1), but it also helps to escape elongated plateaus (see autoencoder in Section 5).
4.2	A stochastic TR framework for neural network training
Since neural network training often constitutes a large-scale learning problem in which the number
of datapoints n is high, we here opt for a stochastic TR framework in order to circumvent memory
issues and reduce the computational complexity. In particular, we adapt the framework of Yao et al.
(2018); Xu et al. (2017b) to the case of iteration-dependent norm constraints (Algorithm 1). We
prove that the O max g-2-H1, -H3 worst-case iteration complexity is retained, which is optimal
if the function is second-order smooth (Cartis et al., 2012a). Towards this end, we assume that the
derivative estimates are sufficiently accurate in the following sense.
Assumption 1 (Sufficiently accurate derivatives). The approximations of the gradient and Hessian
at step t satisfy
kgt — ▽£(Wt)Il ≤ δg and ∣∣Bt - V2L(wt)∣ ≤ 5h,
where δg ≤ (1-?)'g and 6h ≤ min { (I-?VeH , ι}, for some 0 < v < 1.
For finite-sum objectives such as Eq. 1, the above condition can be met by random sub-sampling due
to classical concentration results for sums of random variables (Xu et al., 2017b; Kohler & Lucchi,
2017; Tripuraneni et al., 2017). Following these references, we assume access to the function value
in each iteration for our theoretical analysis but we note that convergence can be retained even for
fully stochastic trust region methods (Gratton et al., 2017; Chen et al., 2018; Blanchet et al., 2016)
and indeed our experiments in Section 5 use sub-sampled function values due to memory constraints.
5 Note that the assumption of bounded batch gradients, i.e. smooth objectives, is common in the analysis of
stochastic algorithms (Allen-Zhu, 2017; Defazio et al., 2014; Schmidt et al., 2017; Duchi et al., 2011).
7
Under review as a conference paper at ICLR 2020
Algorithm 1 Stochastic Ellipsoidal Trust Region Method
1:	Input: wo ∈ Rd, γ > 1,1 > η > 0, ∆0 > 0
2:	for t = 0, 1, . . . , until convergence do
3:	Compute approximations gt and Bt. If kgtk ≤ g, set gt := 0.
4:	Set At := Arms,t orAt := diag (Arms,t) (see Eq. (3)).
5:	Obtain st by solving mt (st) approximately.
6:	Compute ratio of function over model decrease
=L(Wt) -L(Wt + St)
Pt	mt(0) - mt(st)
(12)
7:	Set
∆t+1
γ∆t
δ“y
if PS,t > η
if PS,t < η
wt+1
wt + st
wt
if Pt ≥ η (successful)
otherwise (unsuccessful)
8:	end for
Theorem 2 (Convergence rate of Algorithm 1). Assume that L(w) is second-order smooth with
Lipschitz constants Lg and LH. Furthermore, let Assumption 1 and 2 hold. Then Algorithm 1
finds an O(g, H) first- and second-order stationary point in at most O max g-2-H1, -H3
iterations.
5	Experiments
Trust region methods To validate our claim that ellipsoidal TR methods yield improved perfor-
mance over spherical ones, we run a set of experiments on two image datasets and three types of
network architectures. As can be seen in Figure 3, the ellipsoidal TR methods consistently outperform
their spherical counterpart in the sense that they reach full training accuracy substantially faster on all
problems. Moreover, their limit points are in all cases lower than those of the uniform TR method.
Interestingly, this makes an actual difference in the image reconstruction quality of autoencoders
(see Figure 12). We thus draw the clear conclusion that the ellipsoidal trust region constraints we
propose are to be preferred over their spherical counterpart when training neural networks. Both the
experimental and architectural details are provided in Appendix C.
(SSODBoi
ISINn,uoaSEH
ResNet18
0.2	04	0.6	0.8	1.0
# OfbackpropagatioTis ×104
(SsODBol
MLP
autoencoder
.0
0 12 3 4
- - - -
(SsOD601
。一或爸ɔ
O IOOO 2000 3000 4000 5000 6000 7000 SOOO
# of baclq>ropaσatlons
(SsoD£
6
6
16
I5
I 5
5
5
5
01234567
# of bac⅛jroρagatlons	×104
Figure 3:	Log loss over backpropagations. Mean and 95% CI of 10 runs. Green dotted line indicates 99% acc.
8
Under review as a conference paper at ICLR 2020
ResNet18
(Ssodboi
(SSo{¾OI
ISINn,uosSEH
-2.5
0.0	0.2	0.4	0.6	0.8	1.0
# of backpropagatioπs	X104
Adagτad
RMSprop
SGD
Adam
MLP
autoencoder
6.2
-6∙0
K
鼠8
5.6
5.4
(符 OI)βoI
。一或爸ɔ
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
# of bac⅛jroρagatlons	×105
0.0	0.5	1.0	1.5	2.0	2.5	3.0
# OfbaCipropagations	×104
2
Figure 4:	Log loss over backpropagations. Same setting as Figure 3. See Figure 10 for epoch results.
Benchmark with SGD To put the previous results into context, we also benchmark several state-
of-the-art gradient methods. We fix their sample size to 32 (as advocated e.g. in Masters & Luschi
(2018)) but grid search the stepsize since it is the ratio of these two quantities that effectively
determines the level of stochasticity (Jastrzebski et al., 2017). As the TR methods have a larger batch
size6 of 128-512, We report results both in terms of number Ofbackpropagations and epochs for a fair
comparison. A close look at Figure 4 and 10 (Appendix) indicates that the ellipsoidal TR methods can
be slightly superior in terms of backprops but at best manage to keep pace With first-order methods
in terms of epochs. Furthermore, the limit points of both first- and second-order methods yield the
same order of loss in most experiments. When taking gradient norms into account (plot omitted), We
indeed find no spurious local minima and only the autoencoders give rise to saddle points.
6 Conclusion
We investigated the use of ellipsoidal trust region constraints for neural netWorks. We have shoWn
that the RMSProp matrix satisfies the necessary conditions for convergence and our experimental
results demonstrate that ellipsoidal TR methods outperform their spherical counterparts significantly.
We thus consider the development of further ellipsoids that can potentially adapt even better to the
loss landscape such as e.g. (block-) diagonal hessian approximations (e.g. Bekas et al. (2007)) or
approximations of higher order derivatives as an interesting direction of future research.
Yet, the gradient method benchmark indicates that the value of Hessian information for neural
netWork training is limited for mainly three reasons: 1) second-order methods rarely yield better limit
points, Which suggests that saddles and spurious local minima are not a major obstacle; 2) gradient
methods can run on smaller batch sizes Which is beneficial in terms of epoch and When memory is
limited; 3) The per-iteration time complexity is noticeably loWer for first-order methods (Figure 11).
These observations suggest that advances in hardWare and distributed second-order algorithms (e.g.,
Osawa et al. (2018); Dunner et al. (2018)) will be needed before Newton-type methods can replace
(stochastic) gradient methods in deep learning.
References
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
The case for full-matrix adaptive regularization. arXiv preprint arXiv:1806.02958, 2018.
6We observed weaker performance when running with smaller batch size. We hypothesize that second-order
methods extract more information of each batch and are thus likely to "overfit" small batches in each step.
9
Under review as a conference paper at ICLR 2020
Guillaume Alain, Nicolas Le Roux, and Pierre-Antoine Manzagol. Negative eigenvalues of the
hessian in deep neural networks. 2018.
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. The
Journal of Machine Learning Research ,18(1):8194-8244, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-276,
1998.
Sue Becker, Yann Le Cun, et al. Improving the convergence of back-propagation learning with
second order methods. In Proceedings of the 1988 connectionist models summer school, pp. 29-37.
San Matteo, CA: Morgan Kaufmann, 1988.
Costas Bekas, Effrosyni Kokiopoulou, and Yousef Saad. An estimator for the diagonal of a matrix.
Applied numerical mathematics, 57(11-12):1214-1229, 2007.
Jose Blanchet, Coralia Cartis, Matt Menickelly, and Katya Scheinberg. Convergence rate analysis of
a stochastic trust region method for nonconvex optimization. arXiv preprint arXiv:1609.07428,
2016.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Leon Bottou, Frank E Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points i. arXiv preprint arXiv:1710.11606, 2017.
Coralia Cartis and Katya Scheinberg. Global convergence rate analysis of unconstrained optimization
methods based on probabilistic models. Mathematical Programming, pp. 1-39, 2017.
Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. Adaptive cubic regularisation methods for
unconstrained optimization. part i: motivation, convergence and numerical results. Mathematical
Programming, 127(2):245-295, 2011.
Coralia Cartis, Nicholas IM Gould, and Ph L Toint. Complexity bounds for second-order optimality
in unconstrained optimization. Journal of Complexity, 28(1):93-108, 2012a.
Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. How Much Patience to You Have?: A
Worst-case Perspective on Smooth Noncovex Optimization. Science and Technology Facilities
Council Swindon, 2012b.
Olivier Chapelle and Dumitru Erhan. Improved preconditioner for hessian free optimization. In NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, volume 201, 2011.
Ruobing Chen, Matt Menickelly, and Katya Scheinberg. Stochastic optimization using a trust-region
method and random models. Mathematical Programming, 169(2):447-487, 2018.
Andrew R Conn, Nicholas IM Gould, and Philippe L Toint. Trust region methods. SIAM, 2000.
Frank E Curtis and Daniel P Robinson. Exploiting negative curvature in deterministic and stochastic
optimization. arXiv preprint arXiv:1703.00412, 2017.
Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with a
worst-case iteration complexity of O(3-2) for nonconvex optimization. Mathematical Program-
ming, 162(1-2):1-32, 2017.
10
Under review as a conference paper at ICLR 2020
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with
stochastic gradients. arXiv preprint arXiv:1803.05999, 2018.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic
activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient
descent can take exponential time to escape saddle points. In Advances in Neural Information
Processing Systems, pp. 1067-1077, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Celestine Dunner, Aurelien Lucchi, Matilde Gargiani, An Bian, Thomas Hofmann, and Martin Jaggi.
A distributed second-order algorithm you can trust. arXiv preprint arXiv:1806.07569, 2018.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points-online stochastic
gradient for tensor decomposition. In COLT, pp. 797-842, 2015.
Gabriel Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL
http://distill.pub/2017/momentum.
Serge Gratton, Clement W Royer, LuiS N Vicente, and Zaikun Zhang. Complexity and global rates
of trust-region methods based on probabilistic models. IMA Journal of Numerical Analysis, 2017.
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. In International Conference on Machine Learning, pp. 573-582, 2016.
Martin T Hagan and Mohammad B Menhaj. Training feedforward networks with the marquardt
algorithm. IEEE transactions on Neural Networks, 5(6):989-993, 1994.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882,
2014.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex opti-
mization. In International Conference on Machine Learning, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on Learning Theory, pp. 1246-1257, 2016.
11
Under review as a conference paper at ICLR 2020
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Liu Liu, Xuanqing Liu, Cho-Jui Hsieh, and Dacheng Tao. Stochastic second-order methods for
non-convex optimization with inexact hessian and gradient. arXiv preprint arXiv:1809.09853,
2018.
Linjian Ma, Gabe Montague, Jiayu Ye, Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W
Mahoney. Inefficiency of k-fac for large batch size training. arXiv preprint arXiv:1903.06237,
2019.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735-742,
2010.
James Martens. New insights and perspectives on the natural gradient method. arXiv preprint
arXiv:1412.1193, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks. arXiv
preprint arXiv:1804.07612, 2018.
Eiji Mizutani and Stuart E Dreyfus. Second-order stagewise backpropagation for hessian-matrix
analyses and investigation of negative curvature. Neural Networks, 21(2-3):193-203, 2008.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance.
Mathematical Programming, 108(1):177-205, 2006.
Jorge Nocedal and Stephen J Wright. Numerical optimization, 2nd Edition. Springer, 2006.
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.
Second-order optimization method for large mini-batch: Training resnet-50 on imagenet in 35
epochs. arXiv preprint arXiv:1811.12019, 2018.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint
arXiv:1301.3584, 2013.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Luca Desmaison, Alban aComplexity bounds for second-order optimality in unconstrained
optimizationnd Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
Herbert Robbins and Sutton Monro. A stochastic approximation method. In The Annals of Mathe-
matical Statistics - Volume 22, Number 3. Institute of Mathematical Statistics, 1951.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural computation, 14(7):1723-1738, 2002.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning.
arXiv preprint arXiv:1703.07950, 2017.
Trond Steihaug. The conjugate gradient method and trust regions in large scale optimization. SIAM
Journal on Numerical Analysis, 20(3):626-637, 1983.
12
Under review as a conference paper at ICLR 2020
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networksfor machine learning, 4(2):26-31,
2012.
Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic
regularization for fast nonconvex optimization. arXiv preprint arXiv:1711.02838, 2017.
Patrick Van Der Smagt and Gerd Hirzinger. Solving the ill-conditioning in neural network learning.
In Neural networks: tricks of the trade, pp. 193-206. Springer, 1998.
Hao Wang, Naiyan Wang, and Dit-Yan Yeung. Collaborative deep learning for recommender systems.
In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 1235-1244, 2015.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
Eugene P Wigner. Characteristic vectors of bordered matrices with infinite dimensions i. In The
Collected Works of Eugene Paul Wigner, pp. 524-540. Springer, 1993.
Peng Xu, Farbod Roosta-Khorasan, and Michael W Mahoney. Second-order optimization for non-
convex machine learning: An empirical study. arXiv preprint arXiv:1708.07827, 2017a.
Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Newton-type methods for non-convex
optimization under inexact hessian information. arXiv preprint arXiv:1708.07164, 2017b.
Zhewei Yao, Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Inexact non-convex
newton-type methods. arXiv preprint arXiv:1802.06925, 2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
13
Under review as a conference paper at ICLR 2020
Appendix A: Proofs
A	Notation
Throughout this work, scalars are denoted by regular lower case letters, vectors by bold lower case
letters and matrices as well as tensors by bold upper case letters. By ∣∣ ∙ ∣∣ We denote an arbitrary norm.
For a symmetric positive definite matrix A we introduce the compact notation kwkA = (w|Aw)1/2,
where w ∈ Rd.
B	Equivalence of Preconditioned Gradient Descent and
first-order Trust Region Methods
Theorem 3 (Theorem 1 restated). A preconditioned gradient step
Wt+1 — Wt = St ：= —ηtA-1gt	(13)
with stepsize η > 0, symmetric positive definite preconditioner At ∈ Rd×d and gt = 0
minimizes a first-order local model around Wt ∈ R in an ellipsoid given by At in the sense that
St：=argmin ImI(S) = L(wt) + s|gt] , s.t. ∣s∣At ≤ ηt∣gt∣A-i.	(14)
Proof. We start the proof by noting that the optimization problem in Eq. (14) is convex. For ηt > 0
the constraint satisfies the Slater condition since 0 is a strictly feasible point. As a result, any KKT
point is a feasible minimizer and vice versa.
Let L(S, λ) denote the Lagrange dual of Eq. (5)
L(s, λ) ：= L(wt) + STgt + λ (∣s∣∣a — ηt∣gt∣A-ι) .	(15)
Any point S is a KKT point if and only if the following system of equations is satisfied
VsL(s, λ) = gt +	： Ats = 0	(16)
∣S∣At
λ(∣s∣∣At- ηtkgt∣A-ι) =0.	(17)
∣s∣At — ηt∣gt ∣At-1 ≤ 0	(18)
λ≥0.	(19)
For st as given in Eq. (4) we have that
kst l∣At = Jη2gt(A-I)TAtA-Igt = ηt Jgt A-Igt = ηtkgt ∣∣a-i .	QO)
and thus 17 and 18 hold with equality such that any λ ≥ 0 is feasible. Furthermore,
VsL(St, λ)	=	Vf(Wt)	+ 「一AtSt	=	gt	— ηt	I，λ∣——AtA-1gt	=	gt	-	-il—λ— gt
∣st ∣At	ηt ∣gt ∣At-1	t	∣gt ∣At-1
(21)
is zero for λ = ∣gt∣A-1 ≥ 0. As a result, st is a KKT point of the convex problem 5 which proves
the assertion.
□
To illustrate this theoretical result we run gradient descent and Adagrad as well as the two corre-
sponding first-order TR methods7 on an ill-conditioned quadratic problem. While the method 1st TR
7Essentially Algorithm 1 with mt based on a first order Taylor expansion, i.e. mt1(s) as in Eq. (14).
14
Under review as a conference paper at ICLR 2020
optimizes a linear model within a ball in each iteration, 1st TRada optimizes the same model over the
ellipsoid given by the Adagrad matrix Aada . The results in Figure 5 show that the methods behave
very similar to their constant stepsize analogues.
Figure 5: Iterates (left) and log suboptimality (right) of GD, Adagrad and two full-featured first-order TR
algorithms of which one (1st TR) is spherically constraint and the other (1st TRada) uses Aada as ellispoid.
C Convergence of ellipsoidal TR methods
In order to prove convergence results for ellipsoidal Trust Region methods one must ensure that the
applied norms are coherent during the complete minimization process in the sense that the ellipsoids
do not flatten out (or blow up) completely along any given direction. This intuition is formalized in
Assumption 1 which we restate here for the sake of clarity.
Definition 2 (Definition 1 restated). There exists a constant μ ≥ 1 such that
-IHIAt ≤ IMS ≤ μ∣H",	∀t,∀w ∈ Rd.	(22)
μ
Towards this end, Conn et al. (2000) identify the following sufficient condition on the basis of which
we will prove that our proposed ellipsoid Arms is indeed uniformly equivalent under some mild
assumptions.
Lemma 2 (Theorem 6.7.1 in Conn et al. (2000)). Suppose that there exists a Constant Z ≥ 1 such
that
Z ≤ σmin (At) ≤ σmax (At) ≤ Z	∀t,	(23)
then Definition 1 holds.
Having uniformly equivalent norms is sufficient to prove convergence of ellipsoidal TR methods (se
AN.1 and Theorem 6.6.8 in (Conn et al., 2000)). However, it is so far unknown how the ellipsoidal
constraints influence the convergence rate itself. We here prove that the specific ellipsoidal TR method
presented in Algorithm 1 preserves the rate of its spherically-constrained counterpart proposed in
Yao et al. (2018) (see Theorem 2 below).
First, we show that the proposed Arms,t ellipsoid satisfies Definition 1.
Lemma 3 (Lemma 1). Suppose ∣∣gt∣2 ≤ LH forall Wt ∈ Rd, t = 1,2,... Then there always
exists e > 0 such that the proposed preconditioning matrices Arms,t (Eq. 3) are uniformly
equivalent, i.e. Def. 1 holds. The same holdsfor the diagonal variant.
Proof. The basic building block of our ellipsoid matrix consists of the current and past stochastic
gradients Gt ：=[gi, g2,..., gt].
We consider Arms which is built up as follows8
8This is a generalization of the diagonal variant proposed by Tieleman & Hinton (2012), which precondition
the gradient step by an elementwise division with the square-root of the following estimate gt = (1 — β)gt-ι +
βVL(wt)2.
15
Under review as a conference paper at ICLR 2020
Arms,t
(1-β)Gdiag(βt,βt-1
X----------------
:=D
(24)
From the construction of Arms,t it directly follows that for any unit length vector u ∈ Rd \
{0}, kuk2 = 1 we have
UT ((1 - β)GDGl + 口) U
=(1 - β)ulGD"(D1/2)TGTU + e∣∣u∣∣2
=(1 - β) (D1/2)|G|U| (D1/2)|G|U + kUk22
≥ > 0,
(25)
which proves the lower bound for ζ = 1/. Now, let us consider the upper end of the spectrum of
Arms,t. Towards this end, recall the geometric series expansion
tt
Xβt-i=Xβi
1 - βt+1
1 - β
(26)
and the fact that GG> is a sum of exponentially weighted rank-one positive semi-definite matrices
of the form gigiT . Thus
λmaχ(gigl) = Tr(gigl) = kVgik2 ≤ LH,
where the latter inequality holds per assumption for any sample size |S |. Combining these facts we
get that
UT ((1 - β)GDGT + I) U
=(1 - β)UTGDGTU + kUk22
t
=(1 -β)Xβt-1UTgigiTU+kUk22
i=0	(27)
t
≤(1 - β) X βt-iL2H kUk22 + kUk22
i=0
=(1 - βt+1)L2H + .
As a result we have that
≤ λmin (Arms,t) ≤ λmax (Arms,t) ≤ 1 -βt+1 L2H +
(28)
Finally, to achieve uniform equivalence we need the r.h.s. of (28) to be bounded by 1/. This gives
rise to a quadratic equation in , namely
e2 +(1 — βt+1) LHE — 1 ≤ 0	(29)
which holds for any t and any β ∈ (0, 1) as long as
0 ≤ E ≤ 1('LH +4 - LH).	(30)
Such an E always exists but one needs to choose smaller and smaller values as the upper bound on the
gradient norm grows. For example, the usual value E = 10-8 is valid for all LH < 9.9 ∙ 107. All of
the above arguments naturally extend to the diagonal preconditioner diag(Arms).
□
16
Under review as a conference paper at ICLR 2020
Second, we note that it is no necessary to compute the update step by minimizing Eq. (10) to global
optimality. Instead, it suffices to do better than the Cauchy- and Eigenpoint simultaneously (Conn
et al., 2000; Yao et al., 2018). We here adapt this assumption for the case of iteration dependent
norms (compare (Conn et al., 2000) Chapter 6). restate this assumption here
Assumption 2 (Approximate model minimization). Each update step st yields at least as much
model decrease as the Cauchy- and Eigenpoint simultaneously, i.e.
mt(st) ≤ mt(stC) and mt(st) ≤ mt(stE),	(31)
where
SC := argminmt(-α gt ) and SE := argminmt(αut),	(32)
0≤α≤∆t	Ilgtllt	∣α∣≤∆t
where ut is an approximation to the corresponding negative curvature direction, i.e., for some
0 < V < 1, u|HtUt ≤ ν (kkutkt) λmin(Bt) and ∣∣ut∣t = 1.
In practice, improving upon the Cauchy point is easily satisfied by any Krylov subspace method such
as Conjugate Gradients, which ensures convergence to first order critical points. However, while the
Steihaug-Toint CG solver can exploit negative curvature, it does not explicitly search for the most
curved eigendirection and hence fails to guarantee mt(St) ≤ mt(StE). Thus more elaborate Krylov
descent methods such as Lanczos method might have to be employed for second-order criticality
(See also Appendix B.2 and Conn et al. (2000) Chapter 7).
We now restate two results from Conn et al. (2000) that precisely quantify the model decrease
guaranteed by Assumption 2.
Lemma 4 (Model decrease: Cauchy Point (Theorem 6.3.1. in Conn et al. (2000))). Suppose that StC
is computed as in Eq. (32). Then
mt(0) -
mt(sC) ≥ 1 ∣gtk min{	kgtk ,,, ∆t∣gt∣-}.
t(t)≥ 2 kgtk	{ ι + ∣Btk, t∣gtkt}
(33)
Lemma 5 (Model decrease: Eigenpoint (Theorem 6.6.1 in Conn et al. (2000))). Suppose that
λmin(Bt) < 0 and StE is computed as in Eq. (32). Then
mt(O)-mt(SE) ≥-2"MinCBt) (IkUI) δ2
(34)
We are now ready to prove the final convergence results. Towards this end, we closely follow the
line of arguments developed in Yao et al. (2018). First, we restate the following lemma which holds
independent of the trust region constraint choice.
Lemma 6 (Yao et al. (2018)). Assume that L(w) is second-order smooth with Lipschitz constants
Lg and LH. Furthermore, let Assumption 1 hold. Then
F(Xt + St)- F(Xt)- mt(xt) ≤ s| (VF(xt) - gt) + 1SH∣∣st∣2 + 2LH∣∣St∣3.	(35)
Second, we show that any iterate of Algorithm 1 is eventually successful as long as either the gradient
norm or the smallest eigenvalue are above (below) the critical values g and H .
Lemma 7 (Eventually successful iteration - ∣gt∣ ≥ g). Assume that L(w) is second-order smooth
with Lipschitz constants Lg and LH. Furthermore, let Assumption 1 and 2 hold and suppose that
∣gt ∣ ≥ g as well as
g<
1 - η, AVmin J 〃eg J(1 - ^^ 1 (1 -宿%
寸 %, Z ≤ min < 1+Lg ,V 12Lh μ,	3μ4
then the step St is successful.
(36)
17
Under review as a conference paper at ICLR 2020
Proof. First, by Assumption 2, Lemma 4, kgtk ≥ g and Lemma 1, we have
-mt(st) ≥ 1 kgtk min{ 呷tk 口, ∆tkgtk-}
t(t)≥ 2 kgtk { ι + kBt k, tkgtkJ
≥ 1 kgtk min{ ι + ιgBt k ,△'削}
≥ 2 kgtk min{ ITte ,才}
1	∆t
=2 % ",
(37)
where the last equality uses the above assumed upper bound on ∆t of Eq. (36). Using this result
together with Lemma 6 and the fact that kstk2 ≤ μkstkt ≤ μ∆t due to Lemma 1, We find
L(wt + st) — L(wt) — mt(st) 1	- Pt =	—rn√s) ≤ δg∆tμ + 2δHNd + 2LHδ3m3 —	2 eg δ =2 δg μ2 + δh ∆t μ3 + LH ∆2μ4 eg	eg	eg ≤ Y + (⅞ ∆t + Lf δ2) 应	(38) (39) (40) (41)
where the last inequality makes use of the upper bound assumed on δg . Now, we re-use the
result of Lemma 10 in Yao et al. (2018), which states that (δH∆t + LH∆2) ≤ 1-η for
∆t ≤ min qq(I-LBg，(飞会} to conclude that (δH∆t + LH∆2) μ4 ≤ 1-η for our assumed
bound on ∆t in Eq. (36). As a result, Eq. (38) yields
1 - ρt ≤ 1 - η,
which implies that the iteration t is successful.
□
Lemma 8 (Eventually successful iteration - λmin(Bt) ≤ -H). Assume that L(w) is second-order
smooth with Lipschitz constants Lg and LH. Furthermore, let Assumption 1 and 2 hold and suppose
that kgtk < g and λmin(Bt) < -H. If
δH <
1-η
—A—VeH, ∆t ≤
(1 — η) VeH
2μ	LH
(42)
then iteration t is successful.
Proof. First, recall Eq. (35) and note that, since both st and —st are viable search directions, we can
assume s|VF(Wt) ≤ 0 w.l.o.g.. Then
L(Wt + st) — L(wt) — mt(wt) ≤ 1 δHkstk2 + 2LHkstk3
18
Under review as a conference paper at ICLR 2020
Therefore, recalling Eq. (34) as well as the fact that 黑32 ≤ μ and kst∣∣2 ≤ μ∣∣st∣∣t ≤ μ∆t due to
Lemma 1
1 - ρt
L(wt + st) - L(wt) - mt(st)
-mt(st)
2 δH kstk2 + 2 LH kstk3
2 ∣λmin(Bt)∣∆2 μ2
2 δH kstk2 + 2 LH kstk3
2 €H ∆2μ2
2 δH δ*2 + 2 LH ∆3μ3
(43)
2 EH ∆2μ2
6h	LH ∆tμ
-----1-------
νEH	νEH
1 - η,
≤
≤
≤
<
where the last second inequality is due to the conditions in Eq. (42). Therefore, ρt ≥ η and the
□
iteration is successful.
Together, these two results allow us to establish a lower bound on the trust region radius △t .
Lemma 9. Assume that L(w) is second-order smooth with Lipschitz constants Lg and LH. Further-
more, let Assumption 1 and 2 hold. Suppose
1-η	1-η
δg < -jj— eg, δH < min{VeH, 1}∙
then for Algorithm 1 we have
△t ≥ — min {
Egμ	(I - η)eg(1 - η)eg(1 - η) VeH
1 + Lg , V 12LHμ8 ,	3μ4
2μ	LH
, ∀t= 1,2,...	(44)
Proof. The proof follows directly from △ ≥ ∆t-ι∕γ as well as the fact that any step is successful
as soon as △	falls below min ( 1gμ~,	//(1-η)e∣,	(1-η)eg,	(1—η) VeH	；	due to Lemma 7 and 8.	□
t	I 1+Lg ,	V 12Lhμ∣ ,	3μ4	,	2μ LH	I
Lemma 10 (Number of successful iterations). Under the same setting as Lemma 9, the number of
successful iterations taken by Algorithm 1 is upper bounded by
|Tsucc | ≤
L(xo) - L(x*)
CeH min{eg 舄} ,
where C := η min {C1,C2}, Ci := 2 min { SL ,Cg}，C2 ：= νμμ- min {Cg, CH }，Cg :
min J egμ / (1-η)eg- (1-η)tg I f'*	(1-η) VeH
min ɪ 1+Lg , V 12Lhμ8 ,	3μ4 ʃ, CH :=	2μ LH
Proof. Suppose Algorithm 1 does not terminate at iteration t. Then either kgtk ≥ eg or λmin(B) ≤
-eh. Ifkgtk ≥ eg , according to (33) and Lemma 1, we have
-mt (St) ≥ -kgtk min{ 1 jgtɪ Ii , △t-}
2	1 + IIHtIl	μ
≥ Keg min{ 1 Igr , Cgeg, CHeH}
2	1 + Lg
≥ C1eg min{eg, eH}.
Similarly, in the second case λmin (Bt) ≤ -h, from Lemma 1 and 5 we have
-mt(St) ≥ 1 ν%in(Bt)I^μ2 ≥ CE min局 EH}.
19
Under review as a conference paper at ICLR 2020
Let Tsucc denote the number of successful iterations. Since L(w) is monotonically decreasing, we
have
L(wo) — L(w*) ≥ XL(Wt)- L(wt+ι)
t=0
≥ X L(Wt) - L(Wt+1)
≥	-mt(st)η
t∈Tsucc
≥ X CH min{2g , 2H }
t∈Tsucc
≥ |Tsucc |CH min{g2 , 2H},
which proves the assertion.	□
We are now ready to prove the final result. Particularly, given the lower bound on ∆t established in
Lemma 9 we find an upper bound on the number of un-successful iterations, which combined with
the result of Lemma 10 on the number of successful iterations yields the total iteration complexity of
Algorithm 1.
Theorem 4 (Theorem 2 restated). Assume that L(W) is second-order smooth with Lipschitz
constants Lg and LH. Furthermore, let Assumption 1 and 2 hold. Then Algorithm 1 finds an
O(g, H) first- and second-order stationary point in at most O max g-2-H1, -H3 iterations.
Proof. The result follows by combining the lemmas 9 and 10 as in Theorem 1 OfXuetaL (2017a). □
D Diagonal Dominance in Neural Networks
In the following, we make statements about the diagonal share of random matrices. As E[χ] might
not exist for a random variable x, we cannot compute the expectation of the diagonal share but rather
of for computing the diagonal share of the expectation of the random matrix in absolute terms. Note
that this notion is still meaningful, as the average of many non-diagonally dominated matrices with
positive entries cannot become diagonally dominated.
D.1 Proof of Proposition 1
Proposition 3 (Proposition 1 restated). For random Gaussian Wigner matrix W formed as
Wi,j = Wj,i
J~N(0,σ汽 i < j
VN(0,σ2), i = j,
(45)
where 〜StandSfor i.i.d. draws (Wigner, 1993), the diagonal mass ofthe expected absolute matrix
amounts to
δE[∣W∣]
(46)
Proof.
Pk=ι E [∣Wfe,fe∣]	=	dE[Wι,ι∣]
Pd=1 P3E[Wk,ι∣] — dE [|Wi,i|]+ d(d - I)E[W/]
dσ∖pi2]π	1
dσι p2∕π + d(d - 1)σ2 p2∕π	] + Wd-I)σ2√2∕π
dσι ,2∕π
1
1+m-1)等
(47)
20
Under review as a conference paper at ICLR 2020
which simplifies to d if the diagonal and off-diagonal elements come from the same Gaussian
distribution (σι = σ%.	□
For the sake of simplicity we only consider Gaussian Wigner matrices but the above argument
naturally extends to any distribution with positive expected absolute values, i.e. we only exclude the
Dirac delta function as probability density.
Figure 6: Share of diagonal mass of the Hessian δH relative to δW of the corresponding Wigner
matrix at random initialization, after 50% iterations and at the end of training with RMSprop on
MNIST. Average and 95% confidence interval over 10 runs. See Figure 2 for CIFAR-10 results.
D.2 OLS Baseline
When considering regression tasks, a direct competitor to neural network models is the classical
Ordinary Least Squares (OLS) regression, which minimizes a quadratic loss over a linear model.
In this case the Hessian simply amounts to the input-covariance matrix Hols := X|X, where
X ∈ Rd×n. We here show that the diagonal share of the expected matrix itself also decays in d, when
n grows to infinity. However, empirical simulations suggest the validity of this result even for much
smaller values of n (see Figure D.2) and it is likely that finite n results can be derived when adding
assumptions such as Gaussian data.
Figure 7: Validity of Proposition 2 in the small n regime for Gaussian data.
Proposition 4 (Proposition 2 restated). Let X ∈ Rn×d and assume each xi,k is generated i.i.d. with
zero-mean finite second moment σ2 > 0. Then the share of diagonal mass of the expected matrix
E [|Hols|] amounts to
n
δE[∣H ois |]
√n + (d
(48)
21
Under review as a conference paper at ICLR 2020
Proof.
δ	=	Pd=ιE[∣(Hois)k,k∣]	=	Pd=IE h| Pn=ix2,k|i
E[|Hols|] 一 PLI Pd=I E[∣(Hois)k,ι∣]	Pk=I Pd=ιE[∣ Pn=IXi,kXi,ι∣]
=________________d Pn=ιE[x2,ι]_________________
d Pn=IE [x2,1 ] + d(d - I)E [|Pn=1 xi,1xn,2 |]
(49)
Where we used the fact that all xi,k are i.i.d. variables. Per assumption, we have E xi2,1 = σ2 , ∀i.
Furthermore, the products xi,1xi,2 are i.i.d with expectation 0 and variance σ4. By the central limit
theorem
1n
ZN = √= £xi,ixn,2 → Z
n i=1
in law, with Z -N (0, σ4),since E [Zn ] = nE (Pi=IXi,1 xn,2)2i = n Pi=IE[x2,i]E[x2,2]= σ4
due to the independence assumption. Then E |Zn| 1∣Zn∣≥r) ≤ E(∣Zn∣2/R) ≤ σ4∕R. This
implies that
E(∣ZnI) → E(∣Z∣) = Jlσ2
As a result, we have that in the limit of large n
n
δE[∣Hois 1]
dnσ2
dησ2 + d(d - 1)√nd∏σ2
1	=	√n
1 + (d-√√2	√n +(d - 1)qπ
(50)
□
Appendix B: Background on second-order
optimization
A Newton’ s Method
The canonical second-order method is Newton’s methods. This algorithm uses the inverse Hessian as
a scaling matrix and thus has updates of the form
Wt+1 = Wt- V2L(wt)-1VL(wt),	(51)
which is equivalent to optimizing the local quadratic model
mN(wt) ：= L(Wt) + VL(Wt)Ts + ∣slV2L(wt)s	(52)
to first-order stationarity. Using curvature information to rescale the steepest descent direction gives
Newton’s method the useful property of being linearly scale invariant. This gives rise to a problem
independent local convergence rate that is super-linear and even quadratic in the case of Lipschitz
continuous Hessians (see Nocedal & Wright (2006) Theorem 3.5), whereas gradient descent at best
achieves linear local convergence (Nesterov, 2013).
However, there are certain drawbacks associated with applying classical Newton’s method. First of
all, the Hessian matrix may be singular and thus not invertible. Secondly, even if it is invertible the
local quadratic model (Eq. 52) that is minimized in each NM iteration may simply be an inadequate
approximation of the true objective. As a result, the Newton step is not necessarily a descent step. It
may hence approximate arbitrary critical points (including local maxima) or even diverge. Finally,
the cost of forming and inverting the Hessian sum up to O(nd2 + d3) and are thus prohibitively high
for applications in large dimensional problems.
22
Under review as a conference paper at ICLR 2020
B Trust Region Methods
B.1	Outer iterations
Trust region methods are among the most principled approaches to overcome the above mentioned
issues. These methods also construct a quadratic model mt but constrain the subproblem in such a
way that the stepsize is restricted to stay within a certain radius ∆t within which the model is trusted
to be sufficiently adequate
min mt(s) = L(Wt) + VL(wt)ls + 2STV2L(Wt)s, s.t. IWk ≤ ∆	(53)
Hence, contrary to line-search methods this approach finds the step st and its length kst k si-
multaneously by optimizing (53). Subsequently the actual decrease L(Wt) - L(Wt + st) is
compared to the predicted decrease mt(0) - mt(st) and the step is only accepted if the ratio
ρ := L(Wt) - L(Wt + st)/(mt (0) - mt (st)) exceeds some predefined success threshold η1 > 0.
Furthermore, the trust region radius is decreased whenever ρ falls below η1 and it is increased
whenever ρ exceeds the "very successful" threshold η20. Thereby, the algorithm adaptively measures
the accuracy of the second-order Taylor model - which may change drastically over the parameter
space depending on the behaviour of the higher-order derivatives9 - and adapts the effective length
along which the model is trusted accordingly. See Conn et al. (2000) for more details.
As a consequence, the plain Newton step SN,t = 一 (V2Lt) 1 VLt is only taken if it lies within the
trust region radius and yields a certain amount of decrease in the objective value. Since many functions
look somehow quadratic close to a minimizer the radius can be shown to grow asymptotically under
mild assumptions such that eventually full Newton steps are taken in every iteration which retains the
local quadratic convergence rate (Conn et al., 2000).
Figure 8: Level sets of the non-convex, coercive objective function f(w) = 0.5w02 + 0.25w14 - 0.5w12.
Newton’s Method makes a local quadratic model (blue dashed lines) and steps to its critical point. It may be
thus be ascending (left) or attracted by a saddle point (right). TR methods relieve this issue by stepping to the
minimizer of that model within a certain region (green dashed line).
B.2	Subproblem solver
Interestingly, there is no need to optimize Eq. (53) to global optimality to retain the remarkable
global convergence properties of TR algorithms. Instead, it suffices to do better than the Cauchy- and
Eigenpoint10 simultaneously. One popular approach is to minimize mt(s) in nested Krylov subspaces.
These subspaces naturally include the gradient direction as well as increasingly accurate estimates of
the leading eigendirection
span{gt , Btgt, Bt2gt , . . . , Btjgt }	(54)
until (for example) the stopping criterion
kVmt(sj)k ≤ kVL(Wt)k min{κK, kVL(Wt)kθ},	κK < 1, θ ≥ 0	(55)
9 Note that the second-order Taylor models assume constant curvature.
10which are the model minimizers along the gradient and the eigendirection associated with its smallest
eigenvalue, respectively.
23
Under review as a conference paper at ICLR 2020
is met, which requires increased accuracy as the underlying trust region algorithm approaches
criticality. Conjugate gradients and Lanczos method are two iterative routines that implicitly build up
a conjugate and orthogonal basis for such a Krylov space respectively and they converge linearly on
quadratic objectives with a square-root dependency on the condition number of the Hessian (Conn
et al., 2000). We here employ the preconditionied Steihaug-Toint CG method (Steihaug, 1983) in
order to cope with possible boundary solutions of (53) but similar techniques exist for the Lanczos
solver as well for which we also provide code. As preconditioning matrix for CG we use the same
matrix as for the ellipsoidal constraint.
C Damped (Gauss-)Newton methods
An alternative approach to actively constraining the region within which the model is trusted is to
instead penalize the step norm in each iteration in a Lagrangian manner. This is done by so-called
damped Newton methods that add a λ > 0 multiple of the identity matrix to the second-order term in
the model, which leads to the update step
min mt(s) = L(Wt) + ▽£(Wt)Ts + 1 sl(V2L(wt) + λI)s
s∈Rd	2
=L(Wt) + VL(Wt)Ts + 1 slV2L(wt)s + λksk2.
(56)
This can also be solved hessian-free by conjugate gradients (or other Krylov subspace methods). The
penalty parameter λ is acting inversely to the trust region radius ∆ and it is often updated accordingly.
Such algorithms are commonly known as Levenberg-Marquardt algorithms and they were originally
tailored towards solving non-linear least squares problems (Nocedal & Wright, 2006) but they have
been proposed for neural network training already early on (Hagan & Menhaj, 1994).
Many algorithms in the existing literature replace the use of V2L(Wt) in (56) with the Generalized
Gauss Newton matrix (Martens, 2010; Chapelle & Erhan, 2011) or an approximation of the latter
(Martens & Grosse, 2015). This matrix constitutes the first part of the well-known Gauss-Newton
decomposition
1n
V2L(∙) = n E'00(fi(∙))Vfi(∙)Vfi(∙)
'—--------------------
1n
T +-∑'0(fi(∙))V2fi(∙),
n i=1
(57)
}
{^^^^^^^^^^^^^^^^^^^^
:=AGGN
where l0 and l00 are the first and second derivative of l : Rout → R+ assuming that out = 1 (binary
classification and regression task) for simplicity here.
It is interesting to note that the GGN matrix AGGN of neural networks is equivalent to the Fisher
matrix used in natural gradient descent (Amari, 1998) in many cases like linear activation function
and squared error as well as sigmoid and cross-entropy or softmax and negative log-likelihood for
which the extended Gauss-Newton is defined (Pascanu & Bengio, 2013). As can be seen in (57)
the matrix AGGN is positive semidefinite (and low rank if n < d). As a result, there exist no
second-order convergence guarantees for such methods on general non-convex problems. On the
other end of the spectrum, the GGN also drops possibly positive terms from the Hessian (see 57).
Hence it is not guaranteed to be an upper bound on the latter in the PSD sense. Essentially, GGN
approximations assume that the network is piece-wise linear and thus the GGN and Hessian matrices
only coincide in the case of linear and ReLU activations or non-curved loss functions. For any other
activation the GGN matrix may approximate the Hessian only asymptotically and if the '0(fi(∙))
terms in 57 go to zero for all i ∈ {1, . . . ,n}. In non-linear least squares such problems are called
zero-residual problems and GN methods can be shown to have quadratic local convergence there. In
any other case the convergence rate does not exceed the linear local convergence bound of gradient
descent. In practice however there are cases where deep neural nets do show negative curvature in the
neighborhood of a minimizer (Bottou et al., 2018).Finally, Dauphin et al. (2014) propose the use of
the absolute Hessian instead of the GGN matrix in a framework similar to 56. This method has been
termed saddle-free Newton even though its manifold of attraction to a given saddle is non-empty11.
11It is the same as that for GD, which renders the method unable to escape e.g. when initialized right on a
saddle point. To be fair, the manifold of attraction for GD constitutes a measure zero set (Lee et al., 2016).
24
Under review as a conference paper at ICLR 2020
Figure 9: Both, the GGN method and saddle-free Newton method make a positive definite quadratic model
around the current iterate and thereby overcome the abstractedness of pure Newton towards the saddle (compare
Figure 8). However, (i) none of these methods can escape the saddle once they are in the gradient manifold
of attraction and (ii) as reported in Mizutani & Dreyfus (2008) the GN matrix can be significantly less well
conditioned than the absolute Hessian (here KGN = 4904 870554 and k∣h∣ = 1.03 so We had to add a damping
factor of λ = 0.1 to make the GN step fit the plot.
C.1 Comparison to trust region
Contrary to TR methods, the Levenberg-Marquardt methods never take plain NeWton steps since the
regularization is alWays on (λ > 0). Furthermore, if a positive-definite Hessian approximation like
the Generalized Gauss NeWton matrix is used, this algorithm is not capable of exploiting negative
curvature and there are cases in neural netWork training Where the Hessian is much better conditioned
than the Gauss-NeWton matrix (Mizutani & Dreyfus, 2008) (also see Figure 9). While some scholars
believe that positive-definiteness is a desirable feature (Martens, 2010; Chapelle & Erhan, 2011), We
Want to point out that folloWing negative curvature directions is necessarily needed to escape saddle
points and it can also be meaningful to folloW directions of negative eigenvalue λ outside a saddle
since they guarantee O(∣λ∣3) progress, whereas a gradient descent step yields at least ∣∣Vf (w)k2
progress (both under certain stepsize conditions) and one cannot conclude a-priori Which one is better
in general (Curtis & Robinson, 2017; Alain et al., 2018). Despite these theoretical considerations,
many methods based on GGN matrices have been applied to neural network training (see Martens
(2014) and references therein) and particularly the hessian-free implementations of (Martens, 2010;
Chapelle & Erhan, 2011) can be implemented very cheaply (Schraudolph, 2002).
D Using Hessian information in Neural Networks
While many theoretical arguments suggest the superiority of regularized Newton methods over
gradient based algorithms, several practical considerations cast doubt on this theoretical superiority
when it comes to neural network training. Answers to the following questions are particularly
unclear: Are saddles even an issue in deep learning? Is superlinear local convergence a desirable
feature in machine learning applications (test error)? Are second-order methods more "vulnerable" to
sub-sampling noise? Do worst-case iteration complexities even matter in real-world settings? As a
result, the value of Hessian information in neural network training is somewhat unclear a-priori and
so far a conclusive empirical study is still missing.
Our empirical findings indicate that the net value of Hessian information for neural network training
is indeed somewhat limited for mainly three reasons: 1) second-order methods rarely yield better limit
points, which suggests that saddles and spurious local minima are not a major obstacle; 2) gradient
methods can indeed run on smaller batch sizes which is beneficial in terms of epoch and when
memory is limited; 3) The per-iteration time complexity is noticeably lower for first-order methods.
In summary, these observations suggest that advances in hardware and distributed second-order
algorithms (e.g., Osawa et al. (2018); Dunner et al. (2018)) will be needed before Newton-type
methods can replace (stochastic) gradient methods in deep learning.
25
Under review as a conference paper at ICLR 2020
Appendix C: Experiment details
A Experimental results
A. 1 Ellipsoidal Trust Region vs. First-order Optimizer
ResNet18
1
(SSOlaoɪ
O'XV巴。
Fully-Connected
Autoencoder
■n
(SSOI)BOI
(SSOsoɪ
JLm ±c≡se 工
(SSOI)BOI
Figure 10: Experiment comparing TR and gradient methods in terms of epochs. Average log loss as
well as 95% confidence interval shown.
ResNet18
(SSOI)BOI
0'XV巴。
(SsoDeOI
Fully-Connected
Autoencoder
-4T
(SsODSoi
(SSOsoɪ
JLm ±∙≡qse 工
12 3 4
- - - -
(Ssodboi
250	500	750 1000 1250
Ume
100	200	300
time
Figure 11: Experiment comparing TR and gradient methods in terms of wall-clock time. Average log
loss as well as 95% confidence interval shown. The advantage of extremely low-iteration costs of
first-order methods is particularly notable in the ResNet18 architecture due to the large network size.
B Further Experiment Details
B.1	Default parameters, architectures and datasets
Parameters Table 1 reports the default parameters we consider. Only for the larger ResNet18 on
CIFAR-10, we adapted the batch size to 128 due to memory constraints.
26
Under review as a conference paper at ICLR 2020
	ISoI	∆0	∆max	ηι	η2	γ1	γ2	κK (krylov tol.)
TRuni	512	10-4	10	10-4	0.95	1.1	1.5	0.1
TRada	512	10-4	10	10-4	0.95	1.1	1.5	0.1
TRrms	512	10-4	10	10-4	0.95	1.1	1.75	0.1
Table 1: Default parameters
Datasets We use two real-world datasets for image classification, namely CIFAR-10 and Fashion-
MNIST12. While Fashion-MNIST consists of greyscale 28 × 28 images, CIFAR-10 are colored
images of size 32 × 32. Both datasets have a fixed training-test split consisting of 60,000 and 10,000
images, respectively.
Network architectures The MLP architectures are simple. For MNIST and Fashion-MNIST we
use a 784 - 128 - 10 network with tanh activations and a cross entropy loss. The networks has
1010770 parameters. For the CIFAR-10 MLP we use a 3072 - 128- 128 - 10 architecture also with
tanh activations and cross entropy loss. This network has 4100880 parameters.
The Fashion-MNIST autoencoder has the same architecture as the one used in Hinton & Salakhutdinov
(2006); Xu et al. (2017a); Martens (2010); Martens & Grosse (2015). The encoder structure is
784 - 1000 - 500 - 250 - 30 and the decoder is mirrored. Sigmoid activations are used in all
but the central layer. The reconstructed images are fed pixelwise into a binary cross entropy loss.
The network has a total of 208330000 parameters. The CIFAR-10 autoencoder is taken from the
implementation of https://github.com/jellycsc/PyTorch-CIFAR-10-autoencoder.
For the ResNet18, we used the implementation from torchvision for CIFAR-10 as well as a mod-
ification of it for Fashion-MNIST that adapts the first convolution to account for the single input
channel.
In all of our experiments each method was run on one Tesla P100 GPU using the PyTorch (Paszke
et al., 2017) library.
B.2	Reconstructed Images from Autoencoders
Original	SGD	Adagrad Rmsprop TR Uniform TR Adagrad TR RMSprop
♦■■■aBH^留讨
□o□□□c□□π□
ΞD□目留ΞD□ΠΠ
♦■■■□lalHIM 裳
■信■■alHIslMfm
□□□□□□□□□□
□□□□□□□□□□
□o□□□□□□□□
ΞD□目留 mππ□
■信金■□lal目野日常
□白□□□□□nπ□
ffilUN∙BI∙□l”0l□l
□□□□□□□□□□
□π□□□□□π□π
■隐值∙E3I阿国魔国皆
QEI同第mH□Q≡E
ΞD目目El目QBQO
Figure 12: Original and reconstructed MNIST digits (left), Fashion-MNIST items (middle), and CIFAR-10
classes (right) for different optimization methods after convergence.
12Both datasets were accessed from https://www.tensorflow.org/api_docs/python/tf/keras/datasets
27