Under review as a conference paper at ICLR 2020
A Non-asymptotic comparison of SVRG and
SGD: tradeoffs between compute and speed
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient descent (SGD), which trades off noisy gradient updates for
computational efficiency, is the de-facto optimization algorithm to solve large-
scale machine learning problems. SGD can make rapid learning progress by per-
forming updates using subsampled training data, but the noisy updates also lead
to slow asymptotic convergence. Several variance reduction algorithms, such as
SVRG, introduce control variates to obtain a lower variance gradient estimate and
faster convergence. Despite their appealing asymptotic guarantees, SVRG-like
algorithms have not been widely adopted in deep learning. The traditional asymp-
totic analysis in stochastic optimization provides limited insight into training deep
learning models under a fixed number of epochs. In this paper, we present a
non-asymptotic analysis of SVRG under a noisy least squares regression prob-
lem. Our primary focus is to compare the exact loss of SVRG to that of SGD
at each iteration t. We show that the learning dynamics of our regression model
closely matches with that of neural networks on MNIST and CIFAR-10 for both
the underparameterized and the overparameterized models. Our analysis and ex-
perimental results suggest there is a trade-off between the computational cost and
the convergence speed in underparametrized neural networks. SVRG outperforms
SGD after the first few epochs in this regime. However, SGD is shown to always
outperform SVRG in the overparameterized regime.
1	Introduction
Many large-scale machine learning problems, especially in deep learning, are formulated as mini-
mizing the sum of loss functions on millions of training examples (Krizhevsky et al., 2012; Devlin
et al., 2018). Computing exact gradient over the entire training set is intractable for these problems.
Instead of using full batch gradients, the variants of stochastic gradient descent (SGD) (Robbins
& Monro, 1951; Zhang, 2004; Bottou, 2010; Sutskever et al., 2013; Duchi et al., 2011; Kingma &
Ba, 2014) evaluate noisy gradient estimates from small mini-batches of randomly sampled training
points at each iteration. The mini-batch size is often independent of the training set size, which
allows SGD to immediately adapt the model parameters before going through the entire training
set. Despite its simplicity, SGD works very well, even in the non-convex non-smooth deep learn-
ing problems (He et al., 2016; Vaswani et al., 2017). However, the optimization performance of
the stochastic algorithm near local optima is significantly limited by the mini-batch sampling noise,
controlled by the learning rate and the mini-batch size. The sampling variance and the slow conver-
gence of SGD have been studied extensively in the past (Chen et al., 2016; Li et al., 2017; Toulis
& Airoldi, 2017). To ensure convergence, machine learning practitioners have to either increase the
mini-batch size or decrease the learning rate toward the end of the training (Smith et al., 2017; Ge
et al., 2019).
Recently, several clever variance reduction methods (Roux et al., 2012; Defazio et al., 2014; Wang
et al., 2013; Johnson & Zhang, 2013) were proposed to alleviate the noisy gradient problem by us-
ing control-variates to achieve unbiased and lower-variance gradient estimators. In particular, the
variants of Stochastic Variance Reduced Gradient (SVRG) (Johnson & Zhang, 2013), k-SVRG (Raj
& Stich, 2018), L-SVRG (Kovalev et al., 2019) and Free-SVRG (Sebbouh et al., 2019) construct
control-variates from previous staled snapshot model parameters. These methods enjoy a superior
asymptotic performance in convex optimization compared to the standard SGD. The control-variate
techniques are shown to improve the convergence rate of SGD from a sub-linear to a linear conver-
gence rate. These variance reduction methods can also be combined with momentum (Allen-Zhu,
2017) and preconditioning methods (Moritz et al., 2016) to obtain faster convergence. Despite
1
Under review as a conference paper at ICLR 2020
9 8 7 6
(求r0u山 6-u"」1-EnE-U-W
(a) Least-squares regression (predicted).	(b) Logistic regression.
Figure 1: (a) The minimum loss achieved over a set of hyperparameters in a noisy least squares regression
problem (simulated dynamics). (b) The minimum loss achieved in real dataset MNIST (a logistic regression
model). Our theoretical prediction (a) matched with the training dynamics for real datasets, demonstrating
tradeoffs between computational cost and convergence speed. The curves in red are SVRG and curves in blue
are SGD. Different markers refer to different per-iteration computational cost, i.e., the number of backpropaga-
tion used per iteration on average.
their strong theoretical guarantees, SVRG-like algorithms have seen limited success in training deep
learning models (Defazio & Bottou, 2018). Traditional results from stochastic optimization focus on
the asymptotic analysis, but in practice, most of deep neural networks are only trained for hundreds
of epochs due to the high computational cost. To address the gap between the asymptotic benefit
of SVRG and the practical computational budget of training deep learning models, we provide a
non-asymptotic study on the SVRG algorithms under a noisy least squares regression model. Al-
though optimizing least squares regression is a basic problem, it has been shown to characterize the
learning dynamics of many realistic deep learning models (Zhang et al., 2019; Lee et al., 2019). Re-
cent works suggest that neural network learning behaves very differently in the underparameterized
regime vs the overparameterized regime (Ma et al., 2018; Vaswani et al., 2019), characterized by
whether the learnt model can achieve zero expected loss. We account for both training regimes in
the analysis by assuming a linear target function and noisy labels. In the presence of label noise, the
loss is lower bounded by the label variance. In the absence of the noise, the linear predictor can fit
each training example perfectly. We summarize the main contributions as follows:
•	We show the exact expected loss of SVRG and SGD along an optimization trajectory as a
function of iterations and computational cost.
•	Our non-asymptotic analysis provides an insightful comparison of SGD and SVRG by
considering their computational cost and learning rate schedule. We discuss the trade-offs
between the total computational cost, i.e. the total number of back-propagations performed,
and convergence performance.
•	We consider two different training regimes with and without label noise. Under noisy
labels, the analysis suggests SGD only outperforms SVRG under a mild total computational
cost. However, SGD always exhibits a faster convergence compared to SVRG when there
is no label noise.
•	Numerical experiments validate our theoretical predictions on both MNIST and CIFAR-
10 using various neural network architectures. In particular, we found the comparison of
the convergence speed of SGD to that of SVRG in underparameterized neural networks
closely matches with our noisy least squares model prediction. Whereas, the effect of
overparameterization is captured by the regression model without label noise.
1.1	Related Works
Stochastic variance reduction methods consider minimizing a finite-sum of a collection of func-
tions using SGD. In case we use SGD to minimize these objective functions, the stochasticity comes
from the randomness in sampling a function in each optimization step. Due to the induced noise,
SGD can only converge using decaying step sizes with sub-linear convergence rate. Methods such
as SAG (Roux et al., 2012), SVRG (Johnson & Zhang, 2013), and SAGA (Defazio et al., 2014), are
able to recover linear convergence rate of full-batch gradient descent with the asymptotic cost com-
parable to SGD. SAG and SAGA achieve this improvement at the substantial cost of storing the most
recent gradient of each individual function. In contrast, SVRG spends extra computation at snapshot
intervals by evaluating the full-batch gradient. Theoretical results such as Gazagnadou et al. (2019)
show that under certain smoothness conditions, we can use larger step sizes with stochastic variance
reduction methods than is allowed for SGD and hence achieve even faster convergence. In situations
where we know the smoothness constant of functions, there are results on the optimal mini-batch
size and the optimal step size given the inner loop size (Sebbouh et al., 2019). Applying variance
2
Under review as a conference paper at ICLR 2020
reduction methods in deep learning has been studied recently (Defazio & Bottou, 2018). The authors
conjectured the ineffectiveness is caused by various elements commonly used in deep learning such
as data augmentation, batch normalization and dropout. Such elements can potentially decrease the
smoothness and make the stored gradients become stale quickly. The proposed solution is to either
remove these elements or update the gradients more frequently than is practical.
Dynamics of SGD and quadratic models Our main analysis tool is very closely related to recent
work studying the dynamics of gradient-based stochastic methods. Wu et al. (2018) derived the
dynamics of stochastic gradient descent with momentum on a noisy quadratic model (Schaul et al.,
2013), showing the problem of short horizon bias. In (Zhang et al., 2019), the authors showed the
same noisy quadratic model captures many of the essential characteristic of realistic neural networks
training. Their noisy quadratic model successfully predicts the effectiveness of momentum, precon-
ditioning and learning rate choices in training ResNets and Transformers. However, these previous
quadratic models assume a constant variance in the gradient that is independent of the current param-
eters and the loss function. It makes them inadequate for analyzing the stochastic variance reduction
methods, as SVRG can trivially achieve zero variance under the constant gradient noise. Instead, we
adopted a noisy least-squares regression formulation by considering both the mini-batch sampling
noise and the label noise. There are also recent works that derived the risk of SGD, for least-squares
regression models using the bias-variance decomposition of the risk (Belkin et al., 2018; Hastie
et al., 2019). We use a similar decomposition in our analysis. In contrast to the asymptotic analysis
in these works, we compare SGD to SVRG along the optimization trajectory for any finite-time
horizon under limited computation cost, not just the convergence points of those algorithms.
Underparameterization vs overparameterization. Many of the state-of-the-art deep learning
models are overparameterized deep neural networks with more parameters than the number of train-
ing examples. Even though these models are able to overfit to the data, when trained using SGD, they
generalize well (Zhang et al., 2017). As suggested in recent work, underparameterized and overpa-
rameterized regimes have different behaviours (Ma et al., 2018; Vaswani et al., 2019; Schmidt &
Roux, 2013). Given the infinite width and a proper weight initialization, the learning dynamics of a
neural network can be well-approximated by a linear model via the neural tangent kernel (NTK) (Ja-
cot et al., 2018; Chizat & Bach, 2018). In NTK regime, neural networks are known to achieve global
convergence by memorizing every training example. On the other hand, previous convergence re-
sults for SVRG have been obtained in stochastic convex optimization problems that are similar to
that of an underparameterized model (Roux et al., 2012; Johnson & Zhang, 2013). Our proposed
noisy least-squares regression analysis captures both the underparameterization and overparameter-
ization behavior by considering the presence or the absence of the label noise.
2 Preliminary
2.1	Notations
We will primarily focus on comparing the minibatch version of two methods, SGD and SVRG (John-
son & Zhang, 2013). Denote Li as the loss on ith data point. The SGD update is written as,
θ(t+1) = θ⑴—α(t)^(t)
(1)
where g(t) = b Pb Vθ(t) Li is the minibatch gradient, t is the training iteration, and α⑴ is the
learning rate. The SVRG algorithm is an inner-outer loop algorithm proposed to reduce the variance
of the gradient caused by the minibatch sampling. In the outer loop, for every T steps, we evaluate
a large batch gradient g = N PN vθ(mT) Li, where N b, and m is the outer loop index, and we
store the parameters θ(mT). In the inner loop, the update rule of the parameters is given by,
θ(mT+t+1) = θ(mT+t) - a(t) (g(mT+t) - g(mT+t) + g
(2)
where ^(mT+t) = b Pb Vθ(mτ+t) Li is the current gradient of the mini-batch and g(mT+t) =
b Pb Vθ(mτ) Li is the old gradient. Note that in our analysis, the reference point is chosen to
be the last iterate of previous outer loop θ(mT), recommended as a practical implementation of the
algorithm by the original SVRG paper Johnson & Zhang (2013).
3
Under review as a conference paper at ICLR 2020
2.2	The noisy least squares regression model
We now define the noisy least squares regression model (Schaul et al., 2013; Wu et al., 2018). In this
setting, the input data is d-dimensional, and the output label is generated by a linear teacher model
with additive noise,
(Xi	, Ei)〜PX X Pe; yi = Xi θ + ^i,
where E[x∕ = μ ∈ Rd and Cov(Xi) = Σ, E[ei] = 0, Var(Ei) = σj. We assume WLOG θ* = 0.
We also assume the data covariance matrix Σ is diagonal. This is an assumption adopted in many
previous analysis and it is also a practical assumption as we often apply whitening to pre-process
the training data. We would like to train a student model θ that minimizes the squared loss over the
data distribution:
minL(θ) := E 1(x>θ — yi)2 .	(3)
θ2
At each iteration, the optimizer can query an arbitrary number of data points {Xi, yi}i sampled from
data distribution. The SGD method uses b data points to form a minibatch gradient:
1b	1
g(t) = b E(Xix>θ(t) - XiEi) = XbX>θ(t) - √Xbβb,	(4)
where Xb = √[xi； X2；…；Xb] ∈ Rd×b, and the noise vector Wb = [e1; e2;…;Eb]> ∈ Rb.
SVRG on the other hand, queries for N data points every T steps to form a large batch gradient
g = XNXN θ(mT) — —L XN WN, where XN and WN are defined similarly. At each inner loop step,
it further queries for another b data points, to form the update in Eq. 2.
Lastly, note that the expected loss can be written as a function of the second moment of the iterate,
L(θ㈤)=1E ](x>θ⑴ Y,2 = 2 (tr(ΣE[θ⑴θ⑴>])+ σy).
Hence for the following analysis we mainly focus on deriving the dynamics of the second mo-
ment E[θ(t)θ(t)>], denoted as A(θ(t)). When Σ is diagonal, the loss can further be reduced to
2diag(Σ)>diag(E[θ(t)θ(t)>]) + 2σj. We denote diag(E[θ(t)θ(t)>]) by m(θ(t)).
2.3 The Dynamics of SGD
Definition 1 (Formula for dynamics). We define the following functions and identities,
M(θ) = E[θθ>],	m(θ) = diag(E[θθ>]), C(M(θ)) = Ex [XX>M(θ)XX>] - ΣM(θ)Σ,
2
n = α2σ2diag(Σ),	R = (I 一 αΣ)2 + -^∙-(Σ2 + diag(Σ)diag(∑)>),
Q = α^-(Σ2 + diag(Σ)diag(Σ)>),	P = I — αΣ,	F = ɑ(七+ O) (Σ2 + diag(Σ)diag(Σ)>)
b	Nb
G = α2(0 + 1 Σ2 + ；diag (Σ) diag (Σ)>).
bb
The SGD update (Eq. 1) with the mini-batch gradient of of the noisy least squares model (Eq. 4) is,
θ(t+1) = (I — αXbX> )θ⑴ + 卷 Xb Wb.
We substitute the update rule to derive the following dynamics for the second moment of the iterate:
2	22
M(θ(t+1)) = (I — αΣ)M(θ(t))(I — αΣ) + — C(M(θ(t)))	+ —y Σ	(5)
S-----------{z------------} J	^b^
(1): gradientdescentshrinkage	c ∙ , ∙	ʃ,ʌ .,. .
2 : input noise	3 : label noise
This dynamics equation can be understood intuitively as follows. The term 1 leads to an exponential
shrinkage of the loss due to the gradient descent update. Since we are using a noisy gradient, the
second term 2 represents the variance of stochastic gradient caused by the random input Xb . The
4
Under review as a conference paper at ICLR 2020
term 3 comes from the label noise. We show in the next theorem that when the second moment of
the iterate approaches zero, 2 will also approach zero. However due to the presence of the label
noise, the expected loss is lower bounded by 3 .
When Σ is diagonal, we further analyze and decompose C(M(θ)) as a function of m(θ) so as to
derive the following dynamics and decay rate for SGD.
Theorem 2 (SGD Dynamics and Decay Rate). Given the noisy linear regression objective function
(Eq. 3), under the assumption that X 〜N(0, Σ) with Σ diagonal and θ* = 0, we can express C(θ)
as a function of m(θ):
diagCM(θ)	= Σ2 + diag(Σ)diag(Σ)>m(θ)	(6)
Then we derive following dynamics of expected second moment of θ:
m(θ(t)) = Rt (m(θ⑼)-(I- R)Tn) + (I- R)Tn,	⑺
Under the update rule of SGD, R is the decay rate of the second moment of parameters between two
iterations. And based on Theorem 2 the expected loss can be calculated by 2 diag(Σ)>m(θ(t))+2 o'.
3 A Dilemma for SVRG
By querying a large batch of datapoints XN every T steps, and a small minibatch Xb at every step,
the SVRG method forms the following update rule:
θ(mT+t+1) = (I- αXbX>) θ(mT+t) + α (XbX> - XNXN) θ(mT) + 表XN6n	(8)
To derive the dynamics of the second moment of the parameters following the SVRG update, we
look at the dynamics of one round of inner loop updates, i.e., from θ(mT) to θ((m+1)T):
Lemma 3. The dynamics of the second moment of the iterate following SVRG update rule is given
by,
2	22
M(θ(mT+t+1)) =(I - aΣ)M(θ(mT+t))(I - αΣ) + ɪC(M(θ(mT+t))) + -ɪɪΣ	(9)
{z
1 gradient descent shrinkage
2 input noise	3 label noise
+ α2N+bC(M(θ(mT))) -α2 (c(M(θ(mT))Pt) +c(ptM(θ(mT))))
|
--------------{---------------}1
(4)variance due to g(mT +t)
}
{^^^^^^^^^^^^^^^^≡
5 Variance reduction from control variate
The dynamics equation above is very illuminating as it explicitly manifests the weakness of SVRG.
First notice that terms 1 , 2 , 3 reappear, contributed by the SGD update. The additional terms,
4 and 5 , are due to the control variate. Observe that the variance reduction term 5 decays ex-
ponentially throughout the inner loop, with decay rate I - αΣ, i.e. P. We immediately notice that
this is the same term that governs the decay rate of the term 1 , and hence resulting in a conflict
between the two. Specifically, if we want to reduce the term 1 as fast as possible, we would prefer
a small decay rate and a large learning rate, i.e. α → λ~~1∑. BUt this will also make the boosts
λmax (Σ)
provided by the control variate diminish rapidly, leading to a poor variance reduction. The term 4
makes things even worse as it will maintain as a constant throughout the inner loop, contributing to
an extra variance on top of the variance from standard SGD. On the other hand, if one chooses a
small learning rate for the variance reduction to take effect, this inevitably will make the decay rate
for term 1 smaller, resulting in a slower convergence. Nevertheless, a good news for SVRG is that
the label noise (term ③)is scaled by N, which lets SVRG converge to a lower loss value than SGD
一 a strict advantage of SVRG compared to SGD.
To summarize, the variance reduction from SVRG comes at a price of slower gradient descent
shrinkage. In contrast, SVRG is able to converge to a lower loss value. This motivates the ques-
tion, which algorithm to use given a certain computational cost? We hence performed a thorough
investigation through numerical simulation as well as experiments on real datasets in Sec. 4.
Similarly done for SGD, we decompose C(θ) as a function of m(θ) and derive the following decay
rate for SVRG.
5
Under review as a conference paper at ICLR 2020
3 113
O O - -
Iloo
1 1
sso^∣ EnE-U-W
Noisy Least-square
IO0
O O O β O
1 2 35
-----
Ooooo
Illll
SSO^∣ EnE-U-W
(a) With Label Noise	(b) Without Label Noise
Figure 2: The minimum loss achieved by following SGD (blue) and SVRG (red) over a set of hyperparameters
in a noisy least-square dynamics simulation for cases with and without label noise. The plot suggests that in the
presence of label noise, there is a tradeoff between computational cost and convergence speed. In the absence
of label noise, SGD strictly dominates SVRG in convergence speed for all computational cost.
Theorem 4 (SVRG Dynamics and Decay rate). Given the noisy linear regression objective function
(Eq. 3), under the assumption that X 〜 N(0, Σ) with Σ diagonal and θ* = 0, the dynamics for
SVRG in m(θ) is given by:
m(θ((m+1)T)) =λ(α, b,T, N, Σ)m(θ(mT)) + (I - Rt)(I - R)Tn,	(10)
T-1
λ(α, b, T, N, Σ) =RT - X RkQP-k PT-1 + (I - RT)(I - R)-1F.	(11)
k=0
3.1	The dynamics without label Noise
In the absence of the label noise (i.e., σy = 0), we observe that both SGD and SVRG enjoy linear
convergence as a corollary of Theorem 2 and Theorem 4:
Corollary 5. Without the label noise, the dynamics of the second moment following SGD is given
by,
m(θ(t)) = Rtm(θ(0)),
and the dynamics of SVRG is given by,
m(θ((m+1)T)) = λ(α,b,T,N,Σ)m(θ(mT)),
where λ is defined in Eq.( 11).
Note that similar results have been shown in the past (Ma et al., 2018; Vaswani et al., 2019; Schmidt
& Roux, 2013), where a general condition known as “interpolation regime” is used to show linear
convergence of SGD. Specifically they assume that VLi(θ*) = 0 for all i, and our setting without
label noise clearly also belongs to this regime. This setting also has practical implications, as one
can treat training overparameterized neural networks as in interpolation regime. This motivates
the investigation of the convergence rate of SGD and SVRG without label noise, and was also
extensively studied in the experiments detailed as follows.
4	Experiments
In Sec. 3 we discussed a critical dilemma for SVRG that is facing a choice between effective vari-
ance reduction and faster gradient descent shrinkage. At the same time, it enjoys a strict advantage
over SGD as it converges to a lower loss. We define the total computational cost as the total number
of back-propagations performed. Similarly, per-iteration computational cost refers to the number
of back-propagations performed per iteration. In this section, we study the question, which algo-
rithm converges faster given certain total computational cost? We study this question for both the
underparameterized and the overparameterized regimes.
Our investigation consists of two parts. First, numerical simulations of the theoretical convergence
rates (Sec. 4.1). Second, experiments on real datasets (Sec. 4.2). In both parts, we first fix the
per-iteration computational cost. For SGD, the per-iteration computational budge is equal to the
minibatch size. We picked three batch size {64, 128, 256}. Denote the batchsize of SGD as b, the
equivalent batch size for SVRG is b0 = 2 (1 一 Tb )b. We then perform training with an extensive
set of hyperparameters for each method with each per-iteration computational cost. For SGD, the
hyperparameter under consideration is the learning rate α. For SVRG, besides the learning rate,
6
Under review as a conference paper at ICLR 2020
9 8 7 6
(求)」OJJ 3 6U_US」J_ EnE-U≡
Iooo O O
Oooo O O
ɪ ɪ ɪ ɪ ɪ ɪ
× × × × ×
9 8 7 6 5
(求)」OJJ 3 6U_U」J_ EnE-U
1 1 1
O O O
1 1 1
× × ×
6 5 4
(求)」OJJ 3 6U_U」J_ EnE-U
(a) Logistic Regression. (b) underparameterized MLP (c) underparameterized CNN
Figure 3: The minimum loss achieved by following SGD (blue) and SVRG (red) over a set of hyperparameters
for training on MNIST and CIFAR-10 with underparameterized models. All the results in these plot suggested
there is a tradeoff between computational cost and convergence speed when comparing SGD and SVRG.
we also ran over a set of snapshot intervals T . After running over all sets of hyperparameters,
we gather all training curves of all hyperparameters. We then summarize the performance for each
algorithm by plotting the lower bound of all training curves, i.e. each point (l, t) on the curve showed
the minimum loss l at time step t over all hyperparameters. We compared the two methods under
different computational cost.
Remarkably, we found in many cases phenomenon predicted by our theory matches with observa-
tions in practice. Our experiments suggested there is a trade-off between the computational cost and
the convergence speed for underparameterized neural networks. SVRG outperformed SGD after
a few epochs in this regime. Interestingly, in the case of overparameterized model, a setting that
matches modern day neural networks training, SGD strictly dominated SVRG by showing a faster
convergence throughout the entire training.
4.1	Simulations on Noisy least squares Regression Model
We first performed numerical simulations of the dynamics derived in Theorem 2 for SGD and Theo-
rem 4 for SVRG. We picked a data distribution, with data dimension d = 100, and the spectrum of Σ
is given by an exponential decay schedule from 1 to 0.01. For both methods, we picked 50 learning
rate from 1.5 to 0.01 using a exponential decay schedule. For SVRG, we further picked a set of
snapshot intervals for each learning rate: {256, 128, 64}. We performed simulations in both under-
parameterized and overparameterized setting (namely with and without label noise), and plotted the
lower bound curves over all hyperparameters at Figure 2. The x-axis represents the normalized total
computational cost, denoting tbN-1, which is equivalent to the notion of an epoch in finite dataset
setting. And the loss in Figure 2 does not contain bayes error (i.e. 2σj).
We have the following observations from our simulations. In the case with label noise, the plot
demonstrated an explicit trade-off between computational cost and convergence speed. We observed
a crossing point of between SGD and SVRG appear, indicating SGD achieved a faster convergence
speed in the first phase of the training, but converged to a higher loss, for all per-iteration compute
cost. Hence it shows that one can trade more compute cost for convergence speed by choosing SGD
than SVRG, and vice versa. Interestingly, we found that the the per-iteration computational cost
does not seem to affect the time crossing point takes place. For all these three costs, the crossing
points in the plot are at around the same time: 5.5 epochs. In the case ofno label noise, we observed
both methods achieved linear convergence, while SGD achieved a much faster rate than SVRG,
showing absolute dominance in this regime.
4.2	Benchmark datasets
In this section, we performed a similar investigation as in the last section, on two standard machine
learning benchmark datasets: MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky, 2009). We
present the results from underparameterized setting first, followed by the overparameterized set-
ting. We performed experiments with three batchsizes for SGD: {64, 128, 256}, and an equivalent
batchsize for SVRG. For each batch size, we pick 8 learning rates varying from 0.3 to 0.001 fol-
lowing an exponential schedule. Additionally, we chose three snapshot intervals for every computa-
tional budget, searching over the best snapshot interval given the data. Hence for each per-iteration
computational cost {64, 128, 256}, there are 24 groups of experiments for SVRG and 8 groups of
experiments for SGD.
7
Under review as a conference paper at ICLR 2020
10 12 3
O O - - -
Ilooo
111
(求)」OJJ 3 6U_U」J_ EnE-U
CIFAR-IO
2 10
Ooo
111
(求)」0JJ 3 6U_U」J_ EnE-U
(a) Over-Paremetrized MLP	⑸ OVerParameterized CNN
Figure 4: The minimum loss achieved by following SGD (blue) and SVRG (red) over a set of hyperparameters
fOr training On MNIST and CIFAR-10 with OverParameterized mOdels. In this setting we Observed strict dOmi-
nance Of SGD Over SVRG in cOnvergence sPeed fOr all cOmPutatiOnal cOst, matching Our PreviOus theOretical
PredictiOn.
4.2.1	underparameterized Setting
FOr MNIST, we trained twO underParameterized mOdel: 1. lOgistic regressiOn 784 - 10 2. a under-
Parameterized twO layer MLP 784 - 10 - 10 where the hidden layer has 10 neurOns. FOr CIFAR-10,
we chOse a underParameterized cOnvOlutiOnal neural netwOrk mOdel, which has Only twO 8-channel
cOnvOlutiOnal layers, and One 16-channel cOnvOlutiOnal layer with One additiOnal fully-cOnnected
layer. Filter size is 5. The lOwest lOss achieved Over all hyPerParameters fOr these mOdels fOr each
Per-iteratiOn cOmPutatiOnal cOst are shOwn in Figure 3.
FrOm these exPeriments, we Observe that On MNIST, the results with underParameterized mOdel
were cOnsistent with the dynamics simulatiOn Of nOisy least squares regressiOn mOdel with label
nOise. First Of all, SGD cOnverged faster in the early Phase, resulting in a crOssing POint between
SGD and SVRG. It shOwed a trade-Offs between cOmPutatiOnal cOst and cOnvergence sPeed: befOre
the crOssing POint, SGD cOnverged faster than SVRG; after crOssing POint, SVRG attained a lOwer
lOss. In additiOn, in Fig 3a, all the crOssing POints Of three cOsts matched at the same ePOch (arOund
5), which was alsO cOnsistent with the Our findings with nOisy least squares regressiOn mOdel. On
CIFAR-10, SGD achieved slightly faster cOnvergence in the early Phase, but was surPassed by SVRG
arOund 17 - 25 ePOchs, again shOwing a trade-Off between cOmPute and sPeed.
4.2.2	The overparameterized Setting
Lastly, we cOmPared SGD and SVRG On MNIST and CIFAR-10 using OverParameterized mOdels.
FOr MNIST, we used a MLP with twO hidden layers, each layer having 1024 neurOns. FOr CIFAR-
10, we chOse a large cOnvOlutiOnal netwOrk, which has One 64-channel cOnvOlutiOnal layer, One
128-channel cOnvOlutiOnal layer fOllOwed by One 3200 tO 1000 fully cOnnected layer and One 1000
tO 10 fully cOnnected layer.
The lOwest lOss achieved Over all hyPerParameters fOr these mOdels fOr each Per-iteratiOn cOmPuta-
tiOnal cOst are shOwn in Figure 4. FOr training On MNIST, bOth SGD and SVRG attained clOse tO
zerO training lOss. The results were again cOnsistent with Our dynamics analysis On the nOisy linear
regressiOn mOdel withOut label nOise. SGD has a strict advantage Over SVRG, and achieved a much
faster cOnvergence rate than SVRG thrOughOut the entire training. As fOr CIFAR-10, we stOPPed
the training befOre either Of the twO gOt clOse tO zerO training lOss due tO lack Of cOmPuting time.
But we clearly see a trend Of aPPrOaching tO zerO lOss. Similarly, we alsO had the same Observa-
tiOns as befOre, where SGD OutPerfOrms SVRG, cOnfirms the limitatiOn Of variance reductiOn in the
OverParameterized regime.
5 Discussion
In this PaPer, we studied the cOnvergence PrOPerties Of SGD and SVRG in the underParameterized
and OverParameterized settings. We PrOvided a nOn-asymPtOtic analysis Of bOth algOrithms. We
then investigated the questiOn abOut which algOrithm tO use under certain tOtal cOmPutatiOnal cOst.
We PerfOrmed numerical simulatiOns Of dynamics equatiOns fOr bOth methOds, as well as extensive
exPeriments On the standard machine learning datasets, MNIST and CIFAR-10. Remarkably, we
fOund in many cases PhenOmenOn Predicted by Our theOry matched with ObservatiOns in Practice.
Our exPeriments suggested there is a trade-Off between the cOmPutatiOnal cOst and the cOnvergence
sPeed fOr underParameterized neural netwOrks. SVRG OutPerfOrmed SGD after the first few ePOchs
in this regime. In the case Of OverParameterized mOdel, a setting that matches with mOdern day
neural netwOrks training, SGD strictly dOminated SVRG by shOwing a faster cOnvergence fOr all
cOmPutatiOnal cOst.
8
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. The
JoUrnal of Machine Learning Research ,18(1):8194-8244, 2017.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
and the bias-variance trade-off. CoRR, abs/1812.11118, 2018.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Xi Chen, Jason D. Lee, Xin T. Tong, and Yichen Zhang. Statistical inference for model parameters
in stochastic gradient descent, 2016.
Lenalc Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
CoRR, abs/1812.07956, 2018.
Aaron Defazio and Leon Bottou. On the ineffectiveness of variance reduced optimization for deep
learning. CoRR, abs/1812.04529, 2018.
Aaron Defazio, Francis R. Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Neural Information Pro-
cessing Systems (NeurIPS), pp. 1646-1654, 2014.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Nidham Gazagnadou, Robert M. Gower, and Joseph Salmon. Optimal mini-batch and step sizes for
SAGA. In International Conference on Machine Learning (ICML), volume 97 of Proceedings of
Machine Learning Research, pp. 2142-2150. PMLR, 2019.
Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A
near optimal, geometrically decaying learning rate procedure. arXiv preprint arXiv:1904.12838,
2019.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. CoRR, abs/1903.08560, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In NeurIPS, pp. 8580-8589, 2018.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Neural Information Processing Systems (NeurIPS), pp. 315-323, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Dmitry Kovalev, Samuel Horvath, and Peter Richtarik. Don'tjump through hoops and remove those
loops: Svrg and katyusha are better without the outer loop. arXiv preprint arXiv:1901.08689,
2019.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, Nov 1998. ISSN 0018-9219.
doi: 10.1109/5.726791.
9
Under review as a conference paper at ICLR 2020
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019.
Tianyang Li, Liu Liu, Anastasios Kyrillidis, and Constantine Caramanis. Statistical inference using
sgd, 2017.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of SGD in modern over-parametrized learning. In ICML, volume 80 of Proceedings
ofMachine Learning Research, pp. 3331-3340. PMLR, 2018.
Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic l-bfgs al-
gorithm. In Artificial Intelligence and Statistics, pp. 249-258, 2016.
K. B. Petersen and M. S. Pedersen. The matrix cookbook, nov 2012. URL http://localhost/
pubdb/p.php?3274. Version 20121115.
Anant Raj and Sebastian U. Stich. k-svrg: Variance reduction for large scale optimization, 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Nicolas Le Roux, Mark W. Schmidt, and Francis R. Bach. A stochastic gradient method with an
exponential convergence rate for finite training sets. In Neural Information Processing Systems
(NeurIPS), pp. 2672-2680, 2012.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In ICML (3), volume 28
of JMLR Workshop and Conference Proceedings, pp. 343-351. JMLR.org, 2013.
Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong
growth condition. arXiv preprint arXiv:1308.6370, 2013.
Othmane Sebbouh, Nidham Gazagnadou, Samy Jelassi, Francis Bach, and Robert M Gower. To-
wards closing the gap between the theory and practice of svrg. arXiv preprint arXiv:1908.02725,
2019.
Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning rate,
increase the batch size. arXiv preprint arXiv:1711.00489, 2017.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
Panos Toulis and Edoardo M. Airoldi. Asymptotic and finite-sample properties of estimators based
on stochastic gradients. Ann. Statist., 45(4):1694-1727, 08 2017. doi: 10.1214/16-AOS1506.
URL https://doi.org/10.1214/16-AOS1506.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-
parameterized models and an accelerated perceptron. In AISTATS, volume 89 of Proceedings of
Machine Learning Research, pp. 1195-1204. PMLR, 2019.
Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In Advances in Neural Information Processing Systems, pp. 181-189,
2013.
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger B. Grosse. Understanding short-horizon bias in
stochastic meta-optimization. In ICLR (Poster). OpenReview.net, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR. OpenReview.net, 2017.
10
Under review as a conference paper at ICLR 2020
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E Dahl,
Christopher J Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes?
insights from a noisy quadratic model. arXiv preprint arXiv:1907.04164, 2019.
Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algo-
rithms. In Proceedings of the Twenty-first International Conference on Machine Learning, ICML
,04, pp. 116-, New York, NY, USA, 2004. ACM. ISBN 1-58113-838-5. doi: 10.1145/1015330.
1015332. URL http://doi.acm.org/10.1145/1015330.1015332.
11
Under review as a conference paper at ICLR 2020
A Appendix
B Lemma ab out Gradient Covariance
Lemma 6 (Gradient Covariance). Given the noisy linear regression objective function (Eq. 3), under
the assumption that X 〜N(0, Σ) with Σ diagonal and θ* = 0, we have
diag(E[xx>θ ⑴ θ⑴>xx>]) = 0Σ2 + diag(Σ)diag(Σ)>)E[θ ⑴。2]
E[XbXb>θ(t)θ(t)>XbXb>] - ΣE[θ㈤θ⑴>]Σ = 1 (E[xx>θ㈤θ㈤>xx>] - ΣE[θ⑴θ⑴>]∑)
Proof. In the following proof, We define the entry-wise P power on vector X as xop. Under our
assumption μ = 0, θ* = 0 and Σ diagonal, for X 〜 N(0, Σ), x ∈ Rd, we have
Ex,θ(t)[XX>θ(t)θ (t)>XX>] =2Σ2E[θ(t)θ (t)>] + Tr(ΣE[θ(t)θ(t)>])Σ.	(12)
Eq. 12 is a conclusion from The Matrix Cookbook (See section 8.2.3 in Petersen & Pedersen (2012)).
Then, for its main diagonal term, we have:
diag(Ex,θ(t) [xx> θ⑴ θ⑴ >xx>]) =2Σ2 E[θ⑴◦2] + diag(Σ)diag(Σ)>E[θ⑴◦2]	(13)
=2Σ2 + diag(Σ)diag(Σ)>m(θ(t))	(14)
Hence, for CM(θ(t)) , we have:
diagCM(θ(t))	= Σ2 + diag(Σ)diag(Σ)>m(θ(t))	(15)
which is the first conclusion of Theorem 2.
Notice, this conclusion can be generalized to any square matrix A not only for E[θ(t)θ(t)>], i.e. for
any square matrix A ∈ Rd×d, with X 〜 N(0, Σ) and Σ diagonal, since
Ex [XX>AXX>] =2Σ2A + Tr(ΣA)Σ.	(16)
we have
diag(Ex [XX>AXX>]) =2Σ2diag(A) + diag(Σ)diag(Σ)>diag(A)	(17)
=2Σ2 + diag(Σ)diag(Σ)>diag(A)	(18)
For batch gradient XbXb>θ(t), we have
EXbX>θ㈤θ㈤>XbX>]= b12e[( X gx>)θ⑴θ⑴>(X XiX>)i	(19)
i∈[N]b	i∈[N]b
=-1 bE[xx>θ⑴θ㈤xx>] + -1(b2 - b)E[xx>]E[θ⑴θ⑴]E[xx>]
b2	b2
(20)
=1 E[xx>θ㈤ θ⑴ >xx>] + b-1 ΣE[θ㈤ θ㈤ >]Σ	(21)
where [N]b is the index set ofXb.
□
12
Under review as a conference paper at ICLR 2020
C	The Proof of Theorem 2
Theorem 2. Given the noisy linear regression objective function (Eq. 3), under the assumption that
X 〜N (0, Σ) with Σ diagonal and θ* = 0, we can express C(M(θ)) as a function of m(θ):
diagCM(θ)	= Σ2 + diag(Σ)diag(Σ)>m(θ)
Then we derive following dynamics of expected second moment of θ:
m(θ(t)) = R(m(θ⑼)-(I - R)Tn) + (I - R)Tn,
Proof.
θ(t+1)θ(t+1)> =(I - aXbX>)θ㈤θ㈤>(I - aXbX>) + √(I - aXbX>)θ⑴e>X>	(22)
2
+ √= XbEbatt (I - αXbXf> ) + b-Xb^b^> Xb	(23)
Since, E[b] = 0, and b is independent with θ(t), Xb, we have:
E[XbEbθ(t)>(I - αXbXb>)] =0	(24)
E[(I-αXbXb>)θ(t)Eb>Xb>] =0	(25)
and,
Eb,Xb	2 彳XbEbE>x>	α2 二 T EXb	hXb E[EbEb>])Xbi	(26)
		α2 二 T EXb	hXbσy2I)Xbi	(27)
		22 α2σ2 二 Y ς	.	(28)
Since Xb is independent with θ(t), we have:
Eh(I - αXbXb>)θ(t)θ(t)>(I - αXbXb>)i = (I - αΣ)E[θ(t)θ(t)>](I - αΣ) (29)
+α2 E[XbXb>θ(t)θ(t)>XbXb>] - ΣE[θ(t)θ(t)>]Σ) .	(30)
Thus,
E[θ(t+1)θ(t+1)>] =(I - αΣ)E[θ⑴θ㈤>](I - αΣ) + α2(E[xx>θ⑴θ⑴>xx>]	(31)
b
22
- ΣE[θ⑴θ㈤>]Σ) + —yΣ	(32)
b
2	22
二(I - αΣ)E[θ⑴θ㈤>](I - αΣ) + α-C(M(θ㈤))+ —yΣ	(33)
bb
For its diagonal term, we have:
m(θ(t+1)) =diag(E[θ(t+1)θ(t+1)>])	(34)
2	22
二 ((I- αΣ)2 + α(∑2 + diag(∑)diag(∑)>))m(θ(t)) + 中diag(∑)	(35)
二R ∙ m(θ(t)) + In	(36)
13
Under review as a conference paper at ICLR 2020
This formula can be written as:
m(θ(t+1)) + b-1(R - I)Tn = R(m(8⑴)+ b-1(R - I)Tn)	(37)
m(θ(t+1)) = Rt+1 (m(θ0) + b-1(R - I)Tn) - b-1(R - I)-1n,	(38)
where
R = (I - αΣ)2 + α2b-1(Σ2 + diag(Σ)diag(Σ)τ), n = α2σydiag(Σ).	(39)
□
D The Proof of Lemma 3
Lemma 3. The dynamics of the second moment of the iterate following SVRG update rule is given
by,
2	/	、 zλ,2z.γ2
M(θ(mτ+t+1)) =(I - αΣ)M(θS+力(I - αΣ) + ɪC(M(8(mT+t))) + -ɪɪΣ
yz
① gradient descent shrinkage	- .	.	_ ,,,
(2)InPUt noise	Q3)label noise
+ α2N+bC(M(e(mT))) -α (C(M(e(mT))Pt) +c(ptM(θ(mτ))))
I
-----------V-------------}:
5variance due to g(mT +t)
{^^^^^^^^^^^^^^^^≡
(5) Variance reduction from control variate
,
Proof. For SVRG update rule Eq. 8, we have:
θ(mτ+t+1) = (I- αXbX>) θ(mτ+t) + α (XbXT - XNXN) θ(mτ) + *XNeN	(40)
Using the update rule of SVRG, we can get the outer product of parameters as:
0(mT+t+1) 0(mT+t+1) T
=(I - aXbX>)θ(mτ+t)θ(mτ+t)τ(I - aXbX>)
+ α(I - αXbX>)θ(mτ+t)θ(mτ)τ(XbX> - XnXN)
+ α(XbXT - XnXNBmT)θ(mτ+t)T(I- αXbX>)
+ α2(XbXT - XnXN)θ(mτ)θ(mτ)T(XbXT - XnXN)
(41)
(42)
(43)
(44)
(45)
+ √NXNeNθ(mτ+t) (I - aXbXb ) + √N
2
+ - αXbXT)θ(mτ+t)eNXnT + 万
+ NXN eN eN XN T
XN∈nθ(mτ)T(XbXT - XnXN)
(XbXT - XnXN)θ(mτ)eNXnT
(46)
(47)
(48)
Likewise, since E[eN] = 0 and eN is independent with Xb, XN and θ(t), we have the expectation
of equation 46, equation 47 equal to 0. And same as SGD, we also have
嗅N,Xn [XNeNeNTXN] =eXn [XN嗅N [eNeN]XN]	(49)
=E [Xn (叫 XNi	(50)
=σjΣ	(51)
Then, we give a significant formula about the expectation of θ(mτ+t)θ(mτ), utilized to derive the
expected term related to variance reduction amount.
θ(mτ+t+1)θ(mτ)T =(I - aXbXbr)θ(mτ+t)θ(mτ)T
+ α(XbXbr - XnXN)θ(mτ)θ(mτ)T
+ √⅛XNeN θ(mτ )T
(52)
(53)
14
Under review as a conference paper at ICLR 2020
Since E[XNXN>] = E[XbXb>] = Σ, and N is independent with XN and θ(mT), the expectation of
Eq. 53 is equal to 0. Therefore,
E[θ(mT+t+1)θ(mT)>] =(I - αΣ)E[θ(mT+t)θ(mT)>] = (I - αΣ)t+1 E[θ(mT)θ(mT)>]	(54)
=P t+1M(θ(mT))	(55)
which suggests the covariance between g(mT+t) and g(mT+t) is exponentially decayed.
For every other term appearing in Eq. 41, we have the following conclusions. First, similar with
SGD, we have the formula about gradient descent shrinkage as:
Ex,θ[(I - αXbXb>)θ(mT+t)θ(mT+t)>(I - αXbXb>)]	(56)
= (I - αΣ)E[θ(mT+t)θ(mT+t)>](I - αΣ)	(57)
+ α2 E[XbXb>θ(mT+t)θ(mT+t)>XbXb>] - ΣE[θ(mT +t)θ(mT +t)>]Σ	(58)
= (I - αΣ)E[θ(mT+t)θ(mT+t)>](I - αΣ)	(59)
+ α2b-1 Ex [xx>M(θ(mT+t))xx>] - ΣM(θ(mT+t))Σ	(60)
Using Eq. 54, we have following conclusion for variance reduction term from control variate. We
first take expectation over θ(mT+t) θ(mT) > with Eq. 54 due to the independence among Xb, XN
and θ.
Eα(I - αXbXb>)θ(mT+t)θ(mT)>(XbXb> -XNXN>)	(61)
= EXb,XN α(I - αXbXb>)PtM(θ(mT))(XbXb> -XNXN>)	(62)
= -α2b-1Ex[xx>P tM(θ(mT))xx>] - ΣPtM(θ(mT))Σ	(63)
Eα(XbXb>-XNXN>)θ(mT)θ(mT+t)>(I-αXbXb>)	(64)
= EXb,XN α(XbXb> - XNXN>)M(θ(mT))Pt(I - αXbXb>)	(65)
= -α2b-1Ex[xx>M(θ(mT))P txx>] - ΣM(θ(mT))PtΣ	(66)
For the forth term, which represents the variance of g(mT+t), We consider the independence between
Xb and XN and get
Eα2(XbXb>-XNXN>)θ(mT)θ(mT)>(XbXb>-XNXN>)	(67)
=α2N+b (Eχ[xx>M(θ(mT))xx>] - ΣM(θ(mT,∑)	(68)
Thus,
M(θ(mT +t+1)) = I-αΣ M(θ(mT +t)) I-αΣ	(69)
+α2b-1 Ex[xx>M(θ(mT+t))xx>] - ΣM(θ(mT +t))Σ	(70)
+ α2N+b (Eχ[xx>M(θ(mT))xx>] - ΣM(θ(mT,∑)	(71)
-	α2b-1Ex[xx>M(θ(mT))P txx>] - ΣM(θ(mT))PtΣ	(72)
-	α2b-1Ex[xx>P tM(θ(mT))xx>] - ΣPtM(θ(mT))Σ	(73)
α2σ2
+ Y ∑	(74)
15
Under review as a conference paper at ICLR 2020
Under our definition, it can be expressed as:
2	22
M(θ(mT+t+1)) =(I - aΣ)M(θ(mT+t))(I - αΣ) + ɪC(M(θ(mT+t))) + -^yΣ
、	-{z	" 、	-V-	J 、 -V- J
1 gradient descent shrinkage
2 input noise	3 label noise
(75)
+ α2N+bC(M(e(mT))) -α2 (C(M(e(mT))Pt) +c(ptM(θ(mT〃))
'----------------'、---------------------------------'
{^^^^^^^^^^^^^^^^
5 Variance reduction from control variate
©variance due to g(mT+t)
□
E The Proof of Theorem 4
Theorem 4. Given the noisy linear regression objective function (Eq. 3), under the assumption that
X 〜N (0, Σ) with Σ diagonal and θ* = 0, the dynamics for SVRG in m(θ) is given by:
m(θ((m+1)T)) =λ(α, b,T, N, Σ)m(θ(mT)) + (I - Rt)(I - R)Tn,
T-1
λ(α,b,T,N,Σ) =RT - X RkQP-k PT-1 + (I - RT)(I - R)-1F
k=0
Proof. Form lemma 3 and lemma 6, we can get:
m(θ(mT +t+1)) = Rm(θ(mT+t)) -QPtm(θ(mT)) +Fm(θ(mT)) +N-1n	(76)
where
R =(I- αΣ)2 + α2(∑2 + diag(Σ)diag(Σ)>) Q =孚(∑2 + diag(∑)diag(Σ)>),	(77)
bb
F = 02(N + b) (Σ2 + diag(Σ)diag(Σ)>), P = I - αΣ, n = α2σ2diag(Σ).	(78)
Nb	y
Recursively expending the above formula from m(θ ((m+1)T)) to m(θ(mT)), we can get the follow-
ing result:
m(θ((m+1)T))
= RRm(θ(mT +T -2)) - QPT-2m(θ(mT)) +Fm(θ(mT)) +N-1n)
- QPT-1m(θ(mT)) + Fm(θ(mT)) + N-1n
1
= R2m(θ(mT +T -2)) - XRkQP-k PT-1m(θ(mT))
k=0
1
+ XRk Fm(θ(mT)) +N-1n
k=0
=♦♦♦
T-1
= RT m(θ(mT)) - X RkQP-k PT-1m(θ(mT))
k=0
T-1
+ X Rk Fm(θ(mT)) +N-1n
k=0
T-1
= RT m(θ(mT)) - X RkQP-k PT-1Qm(θ(mT))
k=0
+ (I - RT)(I - R)-1Fm(θ(mT)) +N-1n)
(79)
(80)
(81)
(82)
(83)
(84)
(85)
(86)
(87)
(88)
16
Under review as a conference paper at ICLR 2020
In other word, Eq. 79 describe the dynamic of expected second moment of iterate between two
nearby snapshots,
m(θ((m+1)T)) = λ(α, b, T, N, Σ)m(θ(mT)) + (I - Rt)(I - R)Tn,	(89)
where
T-1
λ(α, b, T, N, Σ) = RT - X RkQP-k PT-1 + (I - RT)(I - R)-1F	(90)
k=0
Since I - R and I - Rt are commutable, I-RT = (I - R)-I(I - Rt) = (I - Rt)(I - R)-1
□
F	The Proof of Corollary 5
Corollary 5. Without the label noise, the dynamics of the second moment following SGD is given
by,
m(θ(t)) = Rtm(θ(0)),
and the dynamics of SVRG is given by,
m(θ((m+1)T)) = λ(α, b, T, N, Σ)m(θ(mT)),
where λ is defined in Eq.( 11).
Proof. If without label noise, i.e. σy2 = 0, we can directly draw the corollary for the setting without
label noise, based on Theorem 2 and Theorem 4. Setting σy2 = 0, we can draw:
m(θ(t)) = Rtm(θ(0)),
and the dynamics of SVRG is given by,
m(θ((m+1)T)) = λ(α, b, T, Σ, N)m(θ(mT)),
where λ is defined in Eq.( 11).	□
G THE SENSITIVITY OF N
In our theoretical analysis (Section 3), we evaluate a large batch gradient g to control variance. That
is because any number of data points can be directly sampled form the true distribution. But in order
to compare the computational cost between SVRG and SGD, we set the number of data points used
to calculate g as N, which is slightly different with the original SVRG's setup of full-batch gradient.
Therefore, we evaluate the sensitivity of N to illustrate when N is beyond a threshold, it will cause
little difference in convergence speed for SVRG.
IO3
IO1
s
° 10^1
g
IIO-3
10-5
0
2000	4000
iterations
(a) Sensitivity of N	(b) Large-batch SGD vs. SVRG
Figure 5: Evaluate the sensitivity of N in Figure 5a with α = 0.5, T = 256 and b = 64 for SVRG
under the noisy least square model. And compare large batch SGD to SVRG in Figure 5b with
T = 256 and fixing computational budget as 2048, varying b0, N for SVRG.
17
Under review as a conference paper at ICLR 2020
From figure 5a, we can tell N has little effect on the convergence speed of SVRG under the noisy
least square model, but it determines the constant term of label noise in Eq. 9 which determines the
level of final loss.
Besides, we also compare large batch SGD to SVRG in Figure 5b under the computation budget
b = 2048 with fixed snapshot interval T = 256 for SVRG, expentionally picking 50 learning rates
from 1.5 to 0.01, varying b0, N according to b0 = 2 (1 - TNb )b.
18