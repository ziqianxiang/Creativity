Under review as a conference paper at ICLR 2020
Variational Diffusion	Autoencoders with
Random Walk Sampling
Anonymous authors
Paper under double-blind review
Ab stract
Variational inference (VI) methods and especially variational autoencoders
(VAEs) specify scalable generative models that enjoy an intuitive connection
to manifold learning — with many default priors the posterior/likelihood pair
q(z|x)/p(x|z) can be viewed as an approximate homeomorphism (and its inverse)
between the data manifold and a latent Euclidean space. However, these approx-
imations are well-documented to become degenerate in training. Unless the sub-
jective prior is carefully chosen, the topologies of the prior and data distributions
often will not match. Conversely, diffusion maps (DM) automatically infer the
data topology and enjoy a rigorous connection to manifold learning, but do not
scale easily or provide the inverse homeomorphism. In this paper, we propose a)
a principled measure for recognizing the mismatch between data and latent dis-
tributions and b) a method that combines the advantages of variational inference
and diffusion maps to learn a homeomorphic generative model. The measure,
the locally bi-Lipschitz property, is a sufficient condition for a homeomorphism
and easy to compute and interpret. The method, the variational diffusion autoen-
coder (VDAE), is a novel generative algorithm that first infers the topology of the
data distribution, then models a diffusion random walk over the data. To achieve
efficient computation in VDAEs, we use stochastic versions of both variational
inference and manifold learning optimization. We prove approximation theoretic
results for the dimension dependence of VDAEs, and that locally isotropic sam-
pling in the latent space results in a random walk over the reconstructed manifold.
Finally, we demonstrate our method on various real and synthetic datasets, and
show that it exhibits performance superior to other generative models.
1	Introduction
Recent developments in generative models such as variational auto-encoders (VAEs, Kingma &
Welling (2013)) and generative adversarial networks (GANs, Goodfellow et al. (2014)) have made
it possible to sample remarkably realistic points from complex high dimensional distributions at
low computational cost. While their methods are very different — one is derived from variational
inference and the other from game theory — their ends both involve learning smooth mappings from
a user-defined prior distribution to the modeled distribution.
These maps are closely tied to manifold learning when the prior is supported over a Euclidean
space (e.g. Gaussian or uniform priors) and the data lie on a manifold (also known as the Manifold
Hypothesis, see Narayanan & Mitter (2010); Fefferman et al. (2016)). This is because manifolds
themselves are defined by sets that have homeomorphisms to such spaces. Learning such maps is
beneficial to any machine learning task, and may shed light on the success of VAEs and GANs in
modeling complex distributions.
Furthermore, the connection to manifold learning may explain why these generative models fail
when they do. Known as posterior collapse in VAEs (Alemi et al., 2017; Zhao et al., 2017; He et al.,
2019; Razavi et al., 2019) and mode collapse in GANs (Goodfellow, 2017), both describe cases
where the forward/reverse mapping to/from Euclidean space collapses large parts of the input to a
single output. This violates the bijective requirement of the homeomorphic mapping. It also results
in degenerate latent spaces and poor generative performance. A major cause of such failings is when
1
Under review as a conference paper at ICLR 2020
Figure 1: A diagram depicting one step of the diffusion process modeled by the variational diffusion
autoencoder (VDAE). The diffusion and inverse diffusion maps ψ, ψ-1, as well as the covariance C
of the random walk on MZ , are all approximated by neural networks.
the geometries of the prior and target data do not agree. We explore this issue of prior mismatch and
previous treatments of it in Section 3.
Given their connection to manifold learning, it is natural to look to classical approaches in the field
for Ways to improve VAEs. One of the most principled methods is spectral learning (Scholkopf
et al., 1998; Roweis & Saul, 2000; Belkin & Niyogi, 2002) which involves describing data from
a manifold X ⊂ MX by the eigenfunctions of a kernel on MX . We focus specifically on DMs,
Where Coifman & Lafon (2006) shoW that normalizations of the kernel approximate a very specific
diffusion process, the heat kernel over MX . A crucial property of the heat kernel is that, like its
physical analogue, it defines a diffusion process that has a uniform stationary distribution — in other
Words, draWing from this stationary distribution draWs uniformly from the data manifold. Moreover,
Jones et al. (2008) established another crucial property of DMs, namely that distances in local neigh-
borhoods in the eigenfunction space are nearly isometric to corresponding geodesic distances on the
manifold. HoWever, despite its strong theoretical guarantees, DMs are poorly equipped for large
scale generative modeling as they are not easily scalable and do not provide an inverse mapping
from the intrinsic feature space.
In this paper We address issues in variational inference and manifold learning by combining ideas
from both. Theory in manifold learning alloWs us to better recognize prior mismatch, Whereas
variational inference provides a method to learn the difficult to approximate inverse diffusion map.
Our contributions: 1) We introduce the locally bi-Lipschitz property, a sufficient condition for a
homeomorphism, for measuring the stability of a mapping betWeen latent and data distributions. 2)
We introduce VDAEs, a class of variational autoencoders Whose encoder-decoder feedforWard pass
approximates the diffusion process on the data manifold With respect to a user-defined kernel k. 3)
We shoW that deep neural netWorks are capable of learning such diffusion processes, and 4) that
netWorks approximating this process produce random Walks that have certain desirable properties,
including Well defined transition and stationary distributions. 5) Finally, We demonstrate the utility
of the VDAE frameWork on a set of real and synthetic datasets, and shoW that they have superior
performance and satisfy the locally bi-Lipschitz property Where GANs and VAEs do not.
2	Background
Variational inference (VI, Jordan et al. (1999); WainWright et al. (2008)) is a machine learning
method that combines Bayesian statistics and latent variable models to approximate some probabil-
ity density p(x). VI assumes and exploits a latent variable structure in the assumed data generation
process, that the observations X ~ p(x) are conditionally distributed given unobserved latent vari-
2
Under review as a conference paper at ICLR 2020
ables z . By modeling the conditional distribution, then marginalizing over z, as in
Pθ(x) = I Pθ(x∣z)p(z)dz,
z
(1)
we obtain the model evidence, or likelihood that x could have instead been drawn from pθ(x).
Maximizing Eq. 1 leads to an algorithm for finding likely approximations of p(x). As the cost
of computing this integral scales exponentially with the dimension of z, we instead maximize the
evidence lower bound (ELBO):
logPθ(x) ≥ -DκL(q(z∣x)∣∣p(z)) + Ez〜q(z∣χ)[lθgPθ(x|z)],	(2)
where q(z∣x) is usually an approximation of pθ(z|x). Optimizing the ELBO is sped UP by taking
stochastic gradients (Hoffman et al., 2013), and further accelerated by learning a global function
approximator qφ in an autoencoding structure (Kingma & Welling, 2013).
Diffusion maps (DMs, Coifman & Lafon (2006)) on the other hand, are a class of kernel methods
that perform non-linear dimensionality reduction on a set of observations X ⊆ MX, where MX
is the data manifold. Given a symmetric and positive kernel k, DM considers the induced random
walk on the graph of X, where given x, y ∈ X, the transition probabilities p(y|x) = p(x, y) are
row normalized versions of k(x, y). Moreover, the diffusion map ψ embeds the data X ∈ Rm into
the Euclidean space RD so that the diffusion distance is approximated by Euclidean distance. This
is a powerful property, as it allows the arbitrarily complex random walk induced by k on MX to
become an isotropic Gaussian random walk on ψ(MX).
SpectralNet is an algorithm introduced by algorithm in Shaham et al. (2018b) to speed up the dif-
fusion map. Until recently, the method ψk could only be computed via the eigendecomposition
of K. As a result, DMs were only be tractable for small datasets, or on larger datasets by com-
bining landmark-based estimates and Nystrom approximation techniques. However, Shaham et al.
(2018b) propose approximations of the function ψ itself in the case that the kernel k is symmetric.
In particular, we will leverage SpectralNet to enforce our diffusion embedding prior.
Locally bi-lipschitz coordinates by kernel eigenfunctions. (Jones et al. (2008)) analyzed the con-
struction of local coordinates of Riemannian manifolds by Laplacian eigenfunctions and diffusion
map coordinates. They establish, for all x ∈ X, the existence of some neighborhood U (x) and d
spectral coordinates given U(x) that define a bi-Lipschitz mapping from U(x) to Rd. With a smooth
compact Riemannian manifold, U (x) can be chosen to be a geodesic ball with radius a constant
multiple of the inradius (the radius of the largest possible ball around x without intersecting with
the manifold boundary), where the constant is uniform for all x, but the indices of the d spectral
coordinates as well as the local bi-Lipschitz constants may depend on x. Specifically, the Lips-
chitz constants involve inverse of the inradius at x multiplied again by some global constants. For
completeness we give a simplified statement of the Jones et al. (2008) result in the supplementary
material.
Using the compactness of the manifold, one can always cover the manifold with m many neighbor-
hoods (geodesic balls) on which the bi-Lipschitz property in Jones et al. (2008) holds. As a result,
there are a total of D spectral coordinates, D ≤ md (in practice D is much smaller than md, since
the selected spectral coordinates in the proof of Jones et al. (2008) tend to be low-frequency ones,
and thus the selection on different neighborhoods tend to overlap), such that on each of the m neigh-
borhoods, there exists a subset of d spectral coordinates out of the D ones which are bi-Lipschitz on
the neighborhood, and the Lipschitz constants can be bounded uniformly from below and above.
3	Motivation and Related Work
Our proposed measure and model is motivated by degenerate latent spaces and poor generative
performance in a variational inference framework arising from prior mismatch: when the topologies
of the data and prior distributions do not agree. In real world data, this is usually due to two factors:
first, when the dimensionalities of the distributions do not match, and second, when the geometries
do not match. It is easy to see that homeomorphisms between the distributions will not exist in
either case: pointwise correspondences cannot be established, thus the bijective condition cannot be
met. As a result, the model has poor generative performance — for each point not captured in the
pointwise correspondence, the latent or generated distribution loses expressivity.
3
Under review as a conference paper at ICLR 2020
Though the default choice of Gaussian distribution for p(z) is mathematically elegant and computa-
tionally expedient, there are many datasets, real and synthetic, for which this distribution is ill-suited.
It is well known that spherical distributions are superior for modeling directional data (Fisher et al.,
1993; Mardia, 2014), which can be found in fields as diverse as bioinformatics (Hamelryck et al.,
2006), geology (Peel et al., 2001), material science (Krieger Lassen et al., 1994), natural image pro-
cessing (Bahlmann, 2006), and simply preprocessed datasets1. Additionally observe that no home-
omorphism exists between Rk and S1 for any k. For data distributed on more complex manifolds,
the literature is sparse due to the difficult nature of such study. However, the manifold hypothesis is
well-known and studied (Narayanan & Mitter, 2010; Fefferman et al., 2016).
Previous research on alleviating prior mismatch exists. Davidson et al. (2018); Xu & Durrett (2018)
consider VAEs with the von-Mises Fisher prior, a geometrically hyperspherical prior. (Rey et al.,
2019) further model arbitrarily complex manifolds as priors, but require explicit knowledge of the
manifold (i.e. its projection map, scalar curvature, and volume). Finally, Tomczak & Welling (2017)
consider mixtures of any pre-existing priors. But while these methods increase the expressivity of
the priors available, they do not prescribe a method for choosing the prior itself. That responsibility
still lies with the user.
Convserly, our method chooses the best prior automatically. To our knowledge, ours is the first to
take a data-driven approach to prior selection. By using some data to inform the prior, we not only
guarantee the existence of a homeomorphism between data and prior distributions, we explicitly
define it by the learned diffusion map ψ.
4	Method
In this section we propose VDAEs, a variational inference method that, given the data manifold
MX, observations X ⊂ MX, and a kernel k, models the geometry ofX by approximating a random
walk over the latent diffusion manifold MZ := ψ(MX). The model is trained by maximizing the
local evidence: the evidence (i.e. log-likelihood) of each point given its random walk neighborhood.
Points are generated from the trained model by sampling from π, the stationary distribution of the
resulting random walk.
Starting from some point x ∈ X, we can roughly describe one step of the walk as the composition
of three functions: 1) the approximate diffusion map ψΘ : MX → MZ, 2) a sampling procedure
from the learned diffusion process z0 〜 qφ(z0∣χ) = N(ψθ(χ), Cφ) on MZ, and 3) the learned
inverse diffusion map ΨS-1 : MZ → MX that produces χ0 〜 p(x0∣z0) = N(ψ-1(z0),cI) where
the constant c is user-defined and fixed.
We rely crucially on three advantages of our latent space ψΘ (X): a) that it is well-defined (given
the first D eigenvalues of k are distinct), b) well-approximated (given SpectralNet) and c) that
Euclidean distances in MZ approximate single-step random walk distances on MX (see Section 2
and Coifman & Lafon (2006)). Thus the transition probabilities induced by k can be approximated
by Gaussian kernels 2 in MZ.
1
Therefore, to model a diffusion random walk over MZ, we must learn the functions ψΘ, ψθ-1, Cφ
that approximate the diffusion map, its inverse, and the covariance of the random walk on MZ, at
1
all points z ∈ MZ. SpectralNet gives us ψΘ. To learn ψθ-1 and Cφ, we use variational inference.
4.1 The lower bound
Formally, let us define Ux := Bd(x, δ) ∩ MX, where Bd(x, δ) as the δ-ball around x with respect
to d(∙, ∙), the diffusion distance on MZ. For each X ∈ X, we define the local evidence of X as
Ex0~p(x0∣x)∣Ux log pθ (X IX),	(3)
1Any dataset where the data points have been normalized to be unit length becomes a subset of a hyper-
sphere.
2Importantly, note that the choice of a Gaussian kernel in the latent space is not dependent on the choice of
k. We have this invariance due to the aforementioned property of diffusion embeddings.
4
Under review as a conference paper at ICLR 2020
where p(x0|x)|Ux is the restriction of p(x0|x) to Ux. The resulting local evidence lower bound is:
logPθ(x0∣x) ≥ -DκL(qφ(zl∖x)∖∖Pθ(z0∣x)) + Ez0〜qφ(z0∣x) logPθ(x0∣z0) .	(4)
'-----------{z----------} '-------------{-------------}
divergence of random walk distributions neighborhood reconstruction error
Note that the neighborhood reconstruction error should be differentiated from the self reconstruction
error that is in VAEs. Eq. 4 produces the empirical loss function:
0	0	00
LDVAE = -DKL(qφ(z0∖x)∖∖pθ(z0∖x)) + logpθ(x0∖zi0),	(5)
where Zi = gφ,θ(χ, g), G 〜N(0, I). gφ,θ is the deterministic, differentiable function, depending
7	t	. 1	.	1 .1	/-rr∙	C 5 T 11 ∙	CC<c∖
on ψΘ and Cφ, that generates qφ by the reparameterization trick 3 (Kingma & Welling, 2013).
Algorithm 1 VDAE training
Θ,φ,θ J Initialize parameters
Obtain parameters Θ for the approximate diffusion map ψΘ by Shaham et al. (2018b)
while not converged do
A J Random minibatch from X
for x ∈ A do
z0 〜Pφ(z0∖ψθ(x))	. Take one step of the diffusion random walk
x0 J arg mι□y∈A∖{χ} ∖ψθ (y) — z0∖d	. Find approximate nearest neighbor(s)
g J g + 5 Vφ,θ logpθ (χ0∖χ)	. Compute gradients of the loss, i.e. Eq. equation 4
Update φ, θ using g
4.2	The sampling procedure
Here we discuss the algorithm for generating data points from p(x). Composing qφ(z0∖x)(≈
pθ(z0∖x)) with pθ(x0∖z0) gives us an approximation of pθ(x0∖x). Then the simple, parallelizable,
and fast random walk based sampling procedure naturally arises: initialize with an arbitrary point
on the manifold x° ∈ MX, then pick suitably large N andfor n = 1,..., N draw Xn 〜p(x∖xn-ι).
Eventually, our diffusion random walk converges on its stationary distribution π. By Coifman & La-
fon (2006), this is guaranteed to be the uniform distribution on the data manifold. See Section 6.2
for examples of points drawn from this procedure.
4.3	A practical implementation
We now introduce a practical implementation VDAEs, considering the case where ψΘ(x), qφ(z0∖x)
andpθ(x0∖z0) are neural network functions, as they are in VAEs and SpectralNet, respectively.
The neighborhood reconstruction error. Since qφ(z0∖x) models the neighborhood of ψΘ(x),
we may sample qφ to obtain z0 (the neighbor of x in the latent space). This gives pθ(x0∖x) ≈
ψ-1(qφ(z0∖x)), where ψ-1 exists due to the bi-Lipschitz property. We can efficiently approximate
x0 ∈ MX by considering the closest embedded data point ψΘ (x) ∈ MZ to z0 = ψΘ (x0). This is
because Euclidean distance on MZ approximates the diffusion distance on MX . In other words,
x0 〜pθ(x0∖x) ≈ ψ-1(qφ(z0∖x)) which We approximate empirically by
x0 ≈ arg min d(ψθ(y), z0) ,	z0 〜qφ(z0∖x),	(6)
y∈A
where A ⊆ X is the training batch.
On the other hand, the divergence of random walk distributions -DKL(qφ(z0∖x)∖∖pθ(z0∖x)) can
be modeled simply as the divergence of two Gaussian kernels defined on MZ. Though pθ(z0∖x)
is intractable, the diffusion map ψ gives us the diffusion embedding Z, which is an approximation
of the true distribution ofpθ(z0∖x) in a neighborhood around z = ψ(x). We estimate the first and
3Though q depends on φ and Θ, we will use qφ := qφ,Θ to be consistent with existing VAE notation and to
indicate that Θ is not learned by VI.
5
Under review as a conference paper at ICLR 2020
second moments of this distribution in RD by computing the local Mahalanobis distance of points
in the neighborhood. Then, by minimizing the KL divergence between qφ(z0∣x) and the one implied
by this Mahalanobis distance, we obtain the loss:
-DκL(qφ(Z∖x)∖∖Pθ(z0∣x)) = - log
∣α∑*∣
∖Cφ∖
+ d — tr{(αΣ*) 1Cφ},
(7)
where Cφ(χ) is a neural network function, Σ* (x) = Cov(Bd(ψ(χ), δ) ∩ Z) is the covariance of the
points in a neighborhood of z = ψ(x) ∈ Z, and α is a scaling parameter. Note that Cφ(x) does not
have to be diagonal, and in fact is most likely not. Combining Eqs. 6 and 7 we obtain Algorithm 1.
Now we consider the sampling procedure. Since we use neural networks to approximate qφ(z0∖x)
and pθ(x0∖z0), the generation procedure is highly parallelizable. We empirically observe the random
walk enjoys rapid mixing properties — it does not take many iterations of the random walk to sample
from all of MZ 4. This leads to Algorithm 2.
Algorithm 2 VDAE sampling
Xo J Initialize with points X。⊂ X
t J 0
while p(X0 ) 6≈ π do
for xt ∈ Xt do
∙-v
zt+l 〜Pφ(z0∖Ψθ(xt))
xt+1 〜Pθ(x∖zt+l)
tJt+1
. Take one step of the diffusion random walk
. Map back into input space
5	Theory
We theoretically prove that the desired inverse map ψ-1 from spectral coordinate codes back to the
manifold can be approximated by a decoder network, where the network complexity is bounded by
quantities related to the intrinsic geometry of the manifold. This section relies heavily on the known
bi-Lipschitz property of DMs Jones et al. (2008), which we are approximating with the VDAE latent
space without the need for regularization.
5.1	Theorems
The theory for the capacity of the encoder to map M to the diffusion map space ψ(M) has already
been considered in Shaham et al. (2018a) and Mishne et al. (2017). We instead focus on the decoder,
which requires a different treatment. The following theorem is proved in Appendix A.3, based upon
the result in Jones et al. (2008).
Theorem 1. Let MX ⊂ Rm be a smooth d-dimensional manifold, ψ(MX ) ⊂ RD be the diffusion
map for D ≥ d large enough to have a subset of coordinates that are locally bi-Lipschitz. Let
X = [X1, ..., Xm] be the set of all m extrinsic coordinates of the manifold. Then there exists
a sparsely-connected ReLU network fN, with 4DCMX nodes in the first layer, 8dmN nodes in the
second layer, and 2mN nodes in the third layer, and m nodes in the output layer, such that
kx(ψ(X))- fN (ψ(x))kL2(ψ(MX)) ≤ √mCψ∕√N	⑻
where the norm is interpreted as kF k2L2(ψ(M)) := kF (ψ(x))k22dψ(x). Here Cψ depends on how
sparsely X (ψ (x))U can be represented in terms of the ReLU wavelet frame on each neighborhood
Ui, and CMX on the curvature and dimension of the manifold MX.
Theorem 1 is complementary to the theorem in Shaham et al. (2018a), which provides guarantees
for the encoder, as Theorem 1 demonstrates a similar approximation theoretic argument for the
decoder. The proof is built on two properties of ReLU neural networks: 1) their ability to split curved
domains into small, almost Euclidean patches, 2) their ability to build differences of bump functions
4For all experiments in Section 6, the number of steps required to draw from π is less than 10.
6
Under review as a conference paper at ICLR 2020
SVAE
VAE
GAN
VDAE
Figure 2: Reconstructed images from the rotating bulldog example plotted in the latent space of
VDAE (left), Spherical VAE (SVAE, left-middle) and VAE (right-middle), and GAN (right)
on each patch, which allows one to borrow approximation results from the theory of wavelets on
spaces of homogeneous type. The proof also crucially uses the bi-Lipschitz property of the diffusion
embedding Jones et al. (2008). The key insight of Theorem 1 is that, because of the bi-Lipschitz
property, the coordinates of the manifold in the ambient space Rm can be thought of as functions
of the diffusion coordinates. We show that because each of coordinates function Xi is a Lipschitz
function, the ReLU wavelet coefficients of Xi are necessarily '1. This allows Us to use the existing
guarantees of Shaham et al. (2018a) to complete the desired bound.
We also discuss the connections between the distribution at each point in diffusion map space,
qφ(z|x), and the result of this distribution after being decoded through the decoder network fN(Z)
for z 〜qφ(z|X). Similar to Singer & Coifman (2008), We characterize the covariance matrix
Cov(fN(z)) := Ez∈qφ(z∣x) [fN(z)fN(z)t]. The following theorem is proved in Appendix A.3.
Theorem 2. Let fN be a neural network approximation to X as in Theorem 1, such that it
approximates the extrinsic manifold coordinates. Let C ∈ Rm×m be the covariance matrix
C = Ez∈qφ(z∣x) [fN(z)fN(z)t]. Let qφ(z|x)〜N(ψ(x), Σ) with small enough Σ that there ex-
ists a patch Uz0 ⊂ M around z0 satisfying the bi-Lipschitz property of Jones et al. (2008), and such
that Pr(z 〜qφ (z |x) ∈ ψ (Uz0)) < e. Then the number of eigenvalues of C greater than e is at most
d, and C = Jz0 ΣJzT + O() where Jz0 is the m × D Jacobian matrix at z0.
Theorem 2 establishes the relationship between the covariance matrices used in the sampling pro-
cedure and their image under the decoder fN to approximate ψ-1. Similar to Singer & Coifman
(2008), we are able to sample according to a multivariate normal distribution in the latent space.
Thus, the resulting cloud in the data space is distorted (to first order) by the local Jacobian of the
map fN . The key insight of Theorem 2 is from combining this idea with the observation of Jones
et al. (2008) that ψ-1 depends locally on only d of the coordinates in the D dimensional latent space.
6	Experimental Results
6.1	Video of Rotating Figure
We consider the problem of generating new frames from a video of rigid movement. We take 200
frames of a color video (each frame is 100 × 80 × 3) of a spinning bulldog Lederman & Talmon
(2018). Due to the spinning of figure and the fixed background, this creates a low-dimensional
approximately circular manifold.
We compare our method to VAE, the Wasserstein GAN Gulrajani et al. (2017) (with a bi-lipchitz
constraint on the critic), and the hyperspherical VAE Davidson et al. (2018). For the VAE, we use
a two dimensional Gaussian prior pθ(z), such that Z 〜N(0, I2). The noise injected to the GAN is
drawn from a two dimensional uniform distribution pθ(z), such that zi 〜U(0,1), i = 1, 2. For the
spherical VAE, we use a latent dimension of D = 2, which highlights the dimension mismatch issue
that occurs with a spherical prior. This is a benefit of VDAE, even if we choose D > d the latent
embedding will still only be locally d dimensional. We use the same architecture for all networks
which consists of one hidden layer with 512 neurons, activation function for all networks are tanh.
In Fig. 2, we present 300 generated samples, by displaying them on a scatter plot with coordinates
corresponding to their latent dimensions z1 and z2 .
6.2	Data generation from uniformly sampled manifolds
In this series of experiments, we visualize the results of the sampling procedure in Algorithm 2
on three synthetic manifolds. As discussed in 4.2, we randomly select an initial seed point, then
7
Under review as a conference paper at ICLR 2020
Figure 3: An example of distributions reconstructed from a random walk on MZ (Via Algorithm
2), given a single seed point drawn from X. (Bottom): An example of a single burst pθ(x|z). The
distributions are a loop (left), sphere (middle), and the Stanford bunny (right).
recursively sample from pθ (x0∣x) many times to simulate a random walk on the manifold. In the
top row of Fig. 3, we highlight the location of the initial seed point, take 20 steps of the random
walk, and display the resulting generated points on three learned manifolds. Clearly after a large
number of resampling iterations, the algorithm continues to generate points on the manifold, and
the distribution of sampled points converges to a uniform stationary distribution on the manifold.
Moreover, this stationary distribution is reached very quickly. In the bottom row of the same Fig. 3,
We show pθ (x0∣x) by sampling a large number of points sampled from the single seed point. As can
be seen, a single step of pθ (x0∣x) covers a large part of the latent space. The architecture also uses
one hidden layer of 512 neurons and tanh activations.
6.3	Cluster conditional sampling
In this section, we deal with the problem of generating samples from data with multiple clusters in
an unsupervised fashion (i.e. no a priori knowledge of the cluster structure). Clustered data creates
a problem for many generative models, as the topology of the latent space (i.e. normal distribution)
differs from the topology of the data space with multiple clusters.
In our first experiment, we show that our method is capable of generating new points from a particu-
lar cluster given an input point from that cluster. This generation is done in an unsupervised fashion,
which is a different setting from the approach of conditional VAEs Sohn et al. (2015) that require
training labels. We demonstrate this property on MNIST in Figure 4, and show that the newly gen-
erated points after short diffusion time remain in the equivalent class to the seeded image. Here the
architecture is a standard fully convolutional architecture. Details can be found in Appendix A.4.
Figure 4:	An example of cluster conditional sampling with our method, given a seed point (top
left of each image grid). The DVAE is able to produce examples via the random walk that stay
approximately within the cluster of the seed point, without any supervised knowledge of the cluster.
The problem of addressing difference in topologies between the latent space of a generative model
and the output data has been acknowledged in recent works about rejection sampling (Azadi et al.,
2018; Turner et al., 2018). Rejection sampling of neural networks consists of generating a large
collection of samples using a standard GAN, and then designing a probabilistic algorithm to decide
in a post-hoc fashion whether the points were truly in the support of the data distribution p(x).
In the following experiment, we compare to the standard example in the generative model literature.
The data consists of nine bounded spherical densities with significant minimal separation, lying on
a 5 × 5 grid. A standard GAN or VAE struggles to avoid generating points in the gaps between
8
Under review as a conference paper at ICLR 2020
Figure 5:	Comparison between GAN, DRS-GAN, and our samples on a 5 × 5 Gaussian grid. GAN
and DRS-GAN samples taken from Azadi et al. (2018). Shown from left-right are Original, GAN,
DRS-GAN, and our method.
these densities, and thus requires the post-sampling rejection analysis. On the other hand, our model
creates a latent space that separates each of these clusters into their own features and only generates
points that exist in the neighborhood of training data. Figure 5 clearly shows that this results in
significantly fewer points generated in the gaps between clusters, as well as eliminating the need
to generate additional points that are not in final generated set. Our VDAE architecture here uses
one hidden layer of 512 neurons and tanh activations. GAN and DRS-GAN architectures are as
described in Azadi et al. (2018).
Table 1: Mean and standard deviation of the local bi-Lipschitz constant, local to k-nearest neighbor-
hoods on MNIST where k = 3, 5, 10, 100, 1000. Lower is better.
Method	k=3	5	10	100	1000
GAN	2.24 ± 0.35	2.32 ± 0.29	2.5 ± 0.34	2.8 ± 0.42	3.11 ± 0.44
VAE	8.63 ± 2.53	8.93 ± 2.55	9.17 ± 2.59	9.51 ± 2.64	9.78 ± 2.6
SVAE	47.2 ± 19.1	50.3 ± 19.0	52.6 ± 19.1	55.2 ± 19.5	58.5 ± 19.4
VDAE	1.92 ± 0.5	2.05 ± 0.49	2.22 ± 0.50	2.64 ± 0.52	2.87 ± 0.516
6.4 Empirical evaluation of the local bi-Lipschitz measure
Here we describe a practical method for computing the local bi-Lipschitz property, then use it to
evaluate several methods on the MNIST dataset. Let Z and X be metric spaces and f : Z → X .
We define, for each z ∈ Z and k ∈ N, the function bilipk (z):
1 ∕x (f(z ),f(z 0))
bilipk (z) = min K s.t. K ≤	dz (z,z 0)
≤K,∀z0 ∈ Uz,k∩Z
where Z := f-1 (X) is the latent embedding of our dataset X 5, dX and dZ are metrics on X and
Z, and Uz,k is the k-nearest neighborhood ofz. Intuitively, increasing values of K can be thought of
as an increasing tendency of the learned map to stretch or compress regions of space. By analyzing
various statistics of the local bi-Lipschitz measure evaluated at all points of a latent space Z , we
can gain insight into how well-behaved a homeomorphism f is. In Table 1 we report the mean and
standard deviation, over 10 runs, of the local bi-Lipschitz property for several methods trained on
the MNIST dataset.
The comparison is between the Wassertein GAN (WGAN), the VAE, the hyperspherical VAE
(SVAE), and our method. We use standard architectures prescribed by their respective papers to
train the methods. For our method we use a single 500 unit hidden layer network architecture with
ReLU nonlinearities for both the encoder and decoder.
By constraining our latent space to be the diffusion embedding of the data, our method finds a
mapping that automatically enjoys the homeomorphic properties of an ideal mapping, and this is
reflected in the low values of the local bi-Lipschitz constant. Conversely, other methods do not
consider the topology of the data in the prior distribution. This is especially appparent in the VAE
5For VAE, SVAE, and our method, these are the means of the posterior distributions. For GAN it is points
drawn from the N(0, 1) prior.
9
Under review as a conference paper at ICLR 2020
and SVAE, which must generate from the entirety of the input distribution X since they minimize
a reconstruction loss. Interestingly, the mode collapse tendency of GANs alleviate the pathology of
the bi-Lipschitz constant by allowing the GAN to focus on a subset of the distribution — but this
comes at the cost, of course, of collapsing to a few modes of the dataset. Our method is able to
reconstruct the entirety of X while simultaneously maintaining a low local bi-Lipschitz constant.
References
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy.
An information-theoretic analysis of deep latent-variable models. CoRR, abs/1711.00464, 2017.
URL http://arxiv.org/abs/1711.00464.
Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and Augustus Odena. Discrim-
inator rejection sampling. arXiv preprint arXiv:1810.06758, 2018.
Claus Bahlmann. Directional features in online handwriting recognition. Pattern Recognition, 39
(1):115-125, 2006.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and
clustering. In Advances in neural information processing systems, pp. 585-591, 2002.
Ronald R Coifman and StePhane Lafon. Diffusion maps. Applied and computational harmonic
analysis, 21(1):5-30, 2006.
Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspheri-
cal variational auto-encoders. arXiv preprint arXiv:1804.00891, 2018.
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.
Journal of the American Mathematical Society, 29(4):983-1049, 2016.
Nicholas I Fisher, Toby Lewis, and Brian JJ Embleton. Statistical analysis of spherical data. Cam-
bridge university press, 1993.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems (NIPS), volume 27, pp. 2672-2680, 2014.
Ian J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160,
2017. URL http://arxiv.org/abs/1701.00160.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Thomas Hamelryck, John T Kent, and Anders Krogh. Sampling realistic protein conformations
using local structural bias. PLoS Computational Biology, 2(9):e131, 2006.
Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference
networks and posterior collapse in variational autoencoders. CoRR, abs/1901.05534, 2019. URL
http://arxiv.org/abs/1901.05534.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational infer-
ence. The Journal of Machine Learning Research, 14(1):1303-1347, 2013.
Peter W Jones, Mauro Maggioni, and Raanan Schul. Manifold parametrizations by eigenfunctions
of the laplacian and heat kernels. Proceedings of the National Academy of Sciences, 105(6):
1803-1808, 2008.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
10
Under review as a conference paper at ICLR 2020
NC Krieger Lassen, D Juul Jensen, and Knut Conradsen. On the statistical analysis of orientation
data. Acta Crystallographica Section A: Foundations OfCrystallography, 50(6):741-748,1994.
Roy R Lederman and Ronen Talmon. Learning the geometry of common latent variables using
alternating-diffusion. Applied and Computational Harmonic Analysis, 44(3):509-536, 2018.
Kantilal Varichand Mardia. Statistics of directional data. Academic press, 2014.
Gal Mishne, Uri Shaham, Alexander Cloninger, and Israel Cohen. Diffusion nets. Applied and
Computational Harmonic Analysis, 2017.
Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In
Advances in Neural Information Processing Systems, pp. 1786-1794, 2010.
David Peel, William J Whiten, and Geoffrey J McLachlan. Fitting mixtures of kent distributions
to aid in joint set identification. Journal of the American Statistical Association, 96(453):56-63,
2001.
Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with
delta-vaes. CoRR, abs/1901.03416, 2019. URL http://arxiv.org/abs/1901.03416.
LUis A. Perez Rey, Vlado Menkovski, andJacobus W. Portegies. Diffusion variational autoencoders.
CoRR, abs/1901.08991, 2019. URL http://arxiv.org/abs/1901.08991.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-
ding. science, 290(5500):2323-2326, 2000.
Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear component analysis as
a kernel eigenvalue problem. Neural computation, 10(5):1299-1319, 1998.
Uri Shaham, Alexander Cloninger, and Ronald R Coifman. Provable approximation properties for
deep neural networks. Applied and Computational Harmonic Analysis, 44(3):537-557, 2018a.
Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen Basri, and Yuval Kluger. Spectralnet:
Spectral clustering using deep neural networks. arXiv preprint arXiv:1801.01587, 2018b.
Amit Singer and Ronald R Coifman. Non-linear independent component analysis with diffusion
maps. Applied and Computational Harmonic Analysis, 25(2):226-239, 2008.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using
deep conditional generative models. In Advances in neural information processing systems, pp.
3483-3491, 2015.
Jakub M Tomczak and Max Welling. Vae with a vampprior. arXiv preprint arXiv:1705.07120, 2017.
Ryan Turner, Jane Hung, Yunus Saatci, and Jason Yosinski. Metropolis-hastings generative adver-
sarial networks. arXiv preprint arXiv:1811.11357, 2018.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and TrendsR in Machine Learning, 1(1-2):1-305, 2008.
Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. arXiv
preprint arXiv:1808.10805, 2018.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational au-
toencoders. CoRR, abs/1706.02262, 2017. URL http://arxiv.org/abs/1706.02262.
11
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Derivation of Local Evidence Lower Bound (Eq. 4)
We begin With taking the log of the random Walk transition likelihood, logpe(x0∣x) = log / pe(x0, z0∣x)dz0 z		(A.1) (A.2)
=log I Pθ(x0∣z z	0, x)p(z0∣x) q'j [ dz0 q(z0)	
=log EzO〜q(z0)	0 0 p(z0|X) Pθ(X |z ,x) q(z0)	(A.3)
p(z0 |X) ≥ Ezo〜q(zo) [logPθ (X ∣Z , X)] + Ezo〜q(zo) log 1(力		(A.4)
≥ EzO〜q(zo) [logPθ(XIz0,x)] + DKL[q(z0)||p(Zlx)]		(A.5)
where q(z0) is an arbitrary distribution. We let q(z0) to be the conditional distribution q(z0 |x).
Furthermore, if We make the simplifying assumption that pθ(x0∣z0, Z) = pθ(x0∣z0), then We obtain
Eq. 4
logPθ(x0∣x) ≥ -DκL(qφ(z0∣x)∣∣Pθ(z0∣x)) + Ez0〜勺力⑶㈤ logPθ(x0∣z0).	(A.6)
A.2 Results in Jones et al. (2008)
To state the result in Jones et al. (2008), We need the folloWing set-up:
(C1) M is a d-dimensional smooth compact manifold, possibly having boundary, equipped With a
smooth (at least C2) Riemannian metric g;
We denote the geodesic distance by dM , and the geodesic ball centering at x With radius r by
BM(x, r). Under (C1), for each point x ∈ M, there exists rM(x) Which is the inradius, that is, r is
the largest number s.t. BM(x, r) is contained M.
Let 4M be the Laplacian-Beltrami operator on M With Neumann boundary condition, Which is
self-adjoint on L2(M, μ), μ being the Riemannian volume given by g. Suppose that M is re-scaled
to have volume 1. The next condition We need concerns the spectrum of the manifold Laplacian
(C2) 4m has discrete spectrum, and the eigenvalues λ0 ≤ λι ≤ ∙∙∙ satisfy the Weyl,s estimate,
i.e. exists constant C Which only depends on M s.t.
|{j: λj ≤ T}| ≤ CTd/2.
Let ψj be the eigenfunction associated with λj, {ψj}j form an orthonormal bases of L2(M,μ). The
last condition is
(C3) The heat kernel (defined by the heat equation on M) has the spectral representation as
∞
Kt(x,y) = X e-tλj ψj (x)ψj (y).
j=0
Theorem 3 (Theorem 2 Jones et al. (2008), simplified version). Under the above setting and assume
(C1)-(C2), then there are positive constants c1, c2, c3 which only depend on M and g, s.t. for
any X ∈ M,『m(x) being the inradius, there are d eigenfunctions of 4m, ψj, •…，ψjd, which
collectively give a mapping Ψ : M → Rd by
ψχ (X) = (ψjι (x),…，ψjd (X))
satisfying that ∀y, y0 ∈ B(x, c1 rM (x)),
c2rM(z)-1dM(y, y0) ≤ kΨx(y) - Ψx(y0)k ≤ c3rM(z)-1-d/2dM(y, y0).
That is, Ψ is bi-Lipschitz on the neighborhood B(X, c1rM(X)) with the Lipschitz constants indicated
as above. The subscript X in Ψχ emphasizes that the indices jι,…，jd
may depend on X.
12
Under review as a conference paper at ICLR 2020
A.3 Proofs
Proof of Theorem 1. The proof of Theorem 1 is actually a simple extension of the following theo-
rem, Theorem 4, which needs to be proved for each individual extrinsic coordinate Xk , hence the
additional factor of m coming from the L2 norm of m functions.	□
Theorem 4. Let M ⊂ Rm be a smooth d-dimensional manifold, ψ(M) ⊂ RD be the diffusion
map for D ≥ d large enough to have a subset of coordinates that are locally bi-Lipschitz. Let one
of the m extrinsic coordinates of the manifold be denoted X (ψ(x)) for x ∈ M. Then there exists
a sparsely-connected ReLU network fN, with 4DCM nodes in the first layer, 8dN nodes in the
second layer, and 2N nodes in the third layer, such that
kX - fN kL2(ψ(M)) ≤ √N=	(A.7)
where Cψ depends on how sparsely X (ψ(x))U can be represented in terms of the ReLU wavelet
frame on each neighborhood Ui, and CM on the curvature and dimension of the manifold M.
Proof of Theorem 4. The proof borrows from the main theorem of Shaham et al. (2018a). We adopt
this notation and summarize the changes in the proof here. For a full description of the theory and
guarantees for neural networks on manifolds, see Shaham et al. (2018a). Let CM be the number of
neighborhoods Ui = B(xi, δ) ∩ M needed to cover M such that ∀x, y ∈ Ui, (1 - )kx - yk ≤
dM(x, y) ≤ (1 + )kx - yk. Here, we choose δ = min(δM, κ-1ρ) where δM is the largest δ that
preserves locally Euclidean neighborhoods and κ-1ρ is the smallest value from Jones et al. (2008)
such that every neighborhood Ui has a bi-Lipschitz set of diffusion coordinates.
Because of the locally bi-Lipschitz guarantee from Jones et al. (2008), we know for each Ui
there exists an equivalent neighborhood ψ(Ui) in the diffusion map space, where ψ(x) =
[ψi1 (x), ..., ψid (x)]. Note that the choice of these d coordinates depends on the neighborhood
Ui. Moreover, We know the Euclidean distance on ψ(Ui) is locally bi-Lipschitz w.r.t. ”m(∙, ∙) on
Ui.
First, we note that as in Shaham et al. (2018a), the first layer ofa neural network is capable of using
4D units to select the subset of d coordinates ψ(x) from ψ(x) for x ∈ Ui and zeroing out the other
TΛ T	Λ∙ .	∙ .1	1 ʌ TTTl	C J	EI	Ie "∕7∕∖∖	”///、、	_ 7- 7-
D - d coordinates with ReLU bump functions. Then we can define X(ψ(x)) = X (ψ(x)) on x ∈ Ui.
Now to apply the theorem from Shaham et al. (2018a), we must establish that X : ψ(Ui) → R
Ui
can be written efficiently in terms of ReLU functions. Because of the manifold and diffusion metrics
being bi-Lipschitz, we know at a minimum that ψ is invertible on ψ(Ui). Because of this invertibility,
we will slightly abuse notation and refer to X(ψ(x)) = X(x), where this is understood to be the
extrinsic coordinate of the manifold at the point x that cooresponds to ψ(x). we also know that
∀x, y ∈ Ui ,
|X (Ve(X))- X (IXy))I = |X(X)- X (y)|
≤ max ∣∣VX(z)∣∣d(x, y)
z∈Ui
≤ maxz∈UikVX(Z)k kψ(χ)- ψ(y)k,
1-
where VX(z) is understood to be the gradient ofX(z) at the point z ∈ M. This means X(ψ(X)) is
a Lipschitz function w.r.t. ψ(X). Because X(ψ(X)) Lipschitz continuous, it can be approximated by
step functions on a ball of radius 2-' to an error that is at most maxz∈ Ui-VX(Z) k 2-'. ThiS means the
maximum ReLU wavelet coefficient is less than maxz∈UiJVX(Z)k (2-' + 2-'+1). This fact, along
with the fact that ψ(Ui) is compact, gives the fact that on ψ(Ui), set of ReLU wavelet coefficients
is in `1 . And from Shaham et al. (2018a), if on a local patch the function is expressible in terms of
ReLU wavelet coefficients in '1, then there is an approximation rate of √= for N ReLU wavelet
terms.	□
13
Under review as a conference paper at ICLR 2020
Proof of Theorem 2. We borrow from Singer & Coifman (2008) to prove the following result. Given
that the bulk of the distribution q lies inside ψ(Uz0), we can consider only the action offN on ψ(Uz0)
rather than on the whole space. Because the geodesic on U is bi-Lipschitz w.r.t. the Euclidean
distance on the diffusion coordinates (the metric on the input space), we can use the results from
Singer & Coifman (2008) and say that on ψ(Uz0 ) the output covariance matrix is characterized by
the Jacobian of the function fN mapping from Euclidean space (on the diffusion coordinates) to the
output space, at the point z0. So the covariance of the data lying insize ψ(Uz0) is Jz0 ΣJzT0, with an
O() perturbation for the fact that fraction of the data lies outside ψ(Uz0).
The effective rank of C being at most d comes from the locally bi-Lipschitz property. We know
X (ψ(x)) only depends on the d coordinates ψ(x) as in the proof of Theorem 1, which implies
fN (ψ(x)) satisfies a similarly property if fN fully learned X(ψ(x)). Thus, while J ∈ Rm×D, it is
at most rank d, which means JΣJT is at most rank d as well.
□
A.4 Experiment Architectures
A.4. 1 Cluster Conditional Sampling with MNIST
We use the following encoder architecture:
•	Conv2D(channels=64, strides=(1,1), kernel=4)
•	Conv2D(channels=64, strides=(1,1), kernel=4)
•	MaxPooling2D(pooLsize=2)
•	Conv2D(channels=64, strides=(1,1), kernel=4)
•	Conv2D(channels=64, strides=(1,1), kernel=4)
•	MaxPooling2D(pooLsize=2)
•	Dense(512, ’relu’)
•	Dense(10, ’linear’)
and the following decoder architecture: We use the following decoder architecture:
•	Dense(7 * 7, ’relu’)
•	Conv2DTranspose(channels=64, strides=(1,1), kernel=4)
•	Conv2DTranspose(channels=64, strides=(1,1), kernel=4)
•	UPSamPling2D(pooLsize=2)
•	Conv2DTranspose(channels=64, strides=(1,1), kernel=4)
•	Conv2DTranspose(channels=64, strides=(1,1), kernel=4)
•	UPSamPling2D(pooLsize=2)
14