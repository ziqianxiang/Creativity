Under review as a conference paper at ICLR 2020
Decoupling Weight Regularization
from Batch Size for Model Compression
Anonymous authors
Paper under double-blind review
Ab stract
Batch size selection affects the generalization ability of gradient descent, and
small batch size is usually preferred. Conventionally, weight regularization is
performed for every mini-batch without considering when would be the right time
to regularize weights during optimization steps. For model compression, which
regularizes weights to follow compressed forms, compression-aware training also
performs weight compression for every mini-batch to compute the impact of com-
pression on the loss function. In this paper, we propose a new hyper-parameter
called “Non-Regularization period” or NR period during which weights are not
regularized (or compressed). We first investigate the influence of NR period on
regularization using weight decay and weight random noise insertion. Throughout
various experiments, we show that stronger weight regularization demands longer
NR period (regardless of batch size) to best utilize regularization effects. From
our empirical evidence, we argue that weight regularization for every mini-batch
allows small weight decay/noise only and limited regularization effects such that
there is a need to search for right NR period and weight regularization strength
to enhance model accuracy. Consequently, NR period becomes especially crucial
for model compression where strong weight regularization is necessary to increase
compression ratio. Using various models, we show that simple weight regulariza-
tion to comply with compression formats along with long NR period is enough to
achieve high compression ratio and model accuracy.
1	Introduction
For Deep Neural Networks (DNNs), a common training method updates weights by gradient descent
and explicit weight manipulation through regularization. To overcome some practical issues of gra-
dient descent on non-convex optimization problems, there have been several enhancements such as
learning rate scheduling and adaptive update schemes using momentum and update history (Ruder,
2016). Optimizing batch size is another way to yield efficient gradient descent. Note that large batch
size has the advantage of enhancing parallelism of the training system in order to speed up training,
critical for DNN research (Dean et al., 2012). Despite such advantages, small batch size is preferred
because it improves generalization associated with flat minima search (Keskar et al., 2016) and other
hyper-parameter explorations are more convenient (Masters & Luschi, 2018). Small batch size also
affects weight regularization if weight updates for gradient descent and weight regularization are
supposed to happen for every mini-batch. For example, for weight decay conducted for every mini-
batch, if batch size is modified, then the weight decay factor should also be adjusted accordingly
(Loshchilov & Hutter, 2017).
Weight regularization is a process that adds information to the model as a way to avoid overfitting
(Goodfellow et al., 2016; van Laarhoven, 2017). In this paper, we explore weight compression as
a form of weight regularization as it severely restricts the search space of weights (i.e., regularized
by compression forms). Moreover, model compression shrinks the effective model size, which is
an important regularization principle (Goodfellow et al., 2016) (note that improved model accuracy
by model compression is reported (Frankle et al., 2019)). Weights are regularized in numerous
ways by model compression. For example, each weight can be pruned (e.g., Han et al. (2015)) or
quantized (e.g., Guo et al. (2017)) to yield a sparse model representation or to reduce the number
of bits to represent each weight. While weight regularization for model compression can simply be
performed once after training, compression-aware training can improve model accuracy by reflecting
1
Under review as a conference paper at ICLR 2020
Figure 1: Distribution of weight noise after low-rank approximation via SVD (Left) or quantization
(Right) when R is the rank and Q is the number of quantization bits.
the impact of model compression on the loss function for every mini-batch update (Courbariaux
et al., 2015; Zhu & Gupta, 2017; Zhu et al., 2017; Guo et al., 2017). Now, our question in this paper
is the following: Should weight regularization follow batch size that is considered for gradient
descent? If the answer is “No,” then we would need to consider a new hyper-parameter search
space with the right weight regularization timing for existing weight regularization methods and
model compression schemes.
In this paper, we report the following observations: 1) Model compression tends to induce weight
decay and random weight noise; 2) For weight decay and weight noise insertion, less frequent weight
regularization allows stronger regularization to best utilize regularization effects regardless of batch
size; and 3) Similarly, if weight regularization for compression is performed less frequently, training
for model compression permits stronger weight regularization with improved model accuracy, and
thus, achieves higher compression ratios.
We confirm our above observations through various experiments and propose an occasional reg-
ularization method to decouple weight regularization from batch size selection that is optimized
for gradient descent. We verify that our simple model compression techniques (without modifying
the underlying training procedures) based on occasional weight regularization (with longer non-
regularization period) can achieve higher compression ratio and higher model accuracy compared to
previous techniques that demand substantial modifications to the training process.
2	Noise Model on Weight Compression
We first study the relationship between model compression ratio and weight regularization strength
using quantization and singular-value decomposition (SVD) as model compression techniques. We
assume a popular quantization method based on binary codes for which a weight vector w is ap-
proximated to be Piq=1 αibi for q-bit quantization, where α is a scaling factor and b(= {-1, +1}n)
is a binary vector, and n is the vector size. The quantization error ||w - Pi 0也 ||2 is minimized by
a method proposed by Xu et al. (2018) to compute α and b. For SVD, a weight matrix W ∈ Rm×n
is approximated to be W0 ∈ Rm×n by minimizing ||W - W0|| subject to rank(W0) ≤ R, where
R is the target rank.
For our experiments, we use a synthetic (2048 × 2048) weight matrix where each element is ran-
domly generated from the Gaussian distribution N(μ = 0,σ2 = 1). Then, we are interested in the
amount of change of each weight after quantization and SVD. Assuming that weight noise through
compression is expressed as in the form of w0 = w(1 + ), Figure 1 shows the distribution of
with various quantization bits or target ranks. From distributions skewed to be negative, it is
clear that weights tend to decay more with higher compression ratio, along with a wider range of
random noise. Reasonable explanations of Figure 1 would include: 1) weights generated from the
Gaussian distribution are uncorrelated such that an approximation step (by compression) using mul-
tiple weights would result in noise for each weight, 2) in the case of SVD, elements associated with
small eigenvalues are eliminated, 3) averaging effects in quantization reduce the magnitude of large
weights. For weight pruning, becomes -1 or 0 (i.e., weight decay for selected weights). Cor-
respondingly, we study on weight decay and weight noise insertion in the next two sections as an
effort to gain a part of basic knowledge on improved training for model compression, even though
actual model compression would demand much more complicated weight noise models.
2
Under review as a conference paper at ICLR 2020
N	Non-RegUlarization	j
Period=PNR
-\__Z (N+1)th \_	√(N+pNR)th∖Z-
∕Γ∖ Batch /	-∖ Batch ∕Γ∖
端#.................-3..................⅛eP5
Weight RegUlarization Weight RegUlarization
Step 2	Step 4
f Weight Updates w/o Regularization
A Weight Regularization
Figure 2: Gradient descent and weight regularization when NR period is given as a multiple of
batches. Depending on the loss surface and/or strength of regularization, regularization would lead
to step 2 (escaping from a local minimum) or step 5 (returning to a local minimum).
3	Non-Regularization Period
Since weight regularization cannot precede updates for gradient descent, in order to control the fre-
quency of weight regularization, an available option is to skip a few batches without regularization.
In this paper, we propose a new hyper-parameter, called “Non-Regularization period” or NR period,
to enable occasional regularization and to define the interval of two consecutive regularization events
as shown in Figure 2. NR period is expressed as a multiple of batches.
Strong weight regularization facilitates the chance of escaping a local minimum (depicted as step
2 in Figure 2) or require longer NR period to return to a local minimum (described as step 5 in
Figure 2). Let us estimate NR period (pNR) for a returning case. Given a parameter set w (that is
assumed to be close enough to a local minimum) and a learning rate γ, the loss function of a model
L(w) can be approximated as
L(w) ' L(w0) + (w - w0)>(H(w0)/2)(w - w0)	(1)
using a local quadratic approximation where H is the Hessian of L and w0 is a set of parameters at
a local minimum. After regularization is performed at step t, w can be updated by gradient descent
as follows:
∂L
Wt+1 = Wt - YdwL=Wt ' Wt - YH(wο)(wt - wο).	(2)
Thus, after pNR, we obtain wt+pNR = w0 + (I - γH(w0))pNR (wt - w0), where I is an identity
matrix. Suppose that H is positive semi-definite and all elements of I - YH(w0) are less than
1.0, wt+pNR can converge to w0 with long pNR which should be longer with larger (wt - w0)
(i.e., stronger weight regularization) or smaller YH(w0). In the next sections, we focus on the
relationship between pNR and the strength of weight regularization.
4	NR Period S tudy on Weight Decay and Weight Noise Insertion
Weight decay is one of the most well-known regularization techniques (Zhang et al., 2018) and
different from L2 regularization in a sense that weight decay is separated from the loss function
calculation (Loshchilov & Hutter, 2017). Weight decay is performed as
wt+1 = (1- γθwt) - YVwtL(w),	(3)
where θ is a constant weight decay factor. Weight noise insertion is another regularization tech-
nique aiminig at reaching flat minima (Goodfellow et al., 2016; Hochreiter & Schmidhuber,
1995). Suppose that random Gaussian noise is added to weights such that w0 = w + when
E 〜 N(0,ηI). Then, L(w0) = E[fw+e(x) 一 y]2 where x, y, f are input, target, and Pre-
diction function, respectively. Using Taylor-series expansion to second-order terms, we obtain
fw+(x) ≈ fw (x) + E>Vf(x) + E>V2f(x)E/2. Correspondingly, the loss function can also
be approximated as
L(w + E) ≈ E[fw(χ) - y]2 + ηE[(fw(χ) - y)V2fw(x)] + ηE∣∣Vfw(χ)∣∣2,	(4)
where the second term disappears near a local minimum and the third term induces flat minima.
Random noise insertion with other distribution models can be explained in a similar fashion (Good-
fellow et al., 2016).
3
Under review as a conference paper at ICLR 2020
(6) ,IOaUBH
5.0e-04
1.0e-03
2.0e-03
5.0e-03
1.0e-02
2.0e-02
5.0e-02
1.0e-01
2.0e-01
5.0e-01
1.0e+00
2.0e+00
5.0e+00
Non-RegUlarization Period
-91.8
!91.2
90.6
90.0
5.0e-04
1.0e-03
2.0e-03
1.0e-02
2.0e-02
1.0e-01
2.0e-01
5.0e-01
1.0e+00
2.0e+00
5.0e+00
a “ d心心令b F靖编F
Non-RegUlarization Period
93.0
92.4
-91.8
!91.2
90.6
90.0
+
e
Figure 3:	Model accuracy of ResNet-32 on CIFAR-10 using various NR period and amount of
weight decay or noise for regularization (original model accuracy without regularization is 92.6%).
(Left): Weight decay. (Right): Uniform weight noise insertion.
5.0e-05-
1.0e-04 I -
2.0e-04 I
5.0e-04 I
1.0e-03 I ■
2.0e-03 I
5.0e-03 I
116
112
-108
1.0e-02∙∙
2∙0e-02-
5.0e-02-
1.0e-01-
1.5e-01-
2.03
2∙5e-01-
3∙0e-0l∙
Non-Regularization Period
-104
100
96
((O+ col)a〜e) O
5.0e-05- ■■ I
1.0e-04 I ■ I
2.0e-04 ...
5.0e-04-	I
1.0e-03 I
2.0e-03 I ■
5.0e-03 ■■■
1.0e-02 I
2.0e-02-
5.oe-02- ∣nn
1.0e-01 ■■■■■■■■■
1.5e-01-
2.0e-01 I
2.5e-01 I
3.0e-01-
Non-Regularization Period
-115.2
■
d 6寸修◎似Fwi⅛6W滴卡
Figure 4:	Perplexity of LSTM model on PTB dataset using various NR period and amounts of weight
decay or noise. (Left): Weight decay. (Right): Uniform weight noise insertion.
We study the impact of NR period on weight decay and weight noise insertion using ResNet-32 on
CIFAR-10 model (He et al., 2016) and a long short-term memory (LSTM) model on PTB dataset
(Zaremba et al., 2014). For LSTM model, we use 2 layers with 200 hidden units and the hyper-
parameter set introduced by Zaremba et al. (2014). For weight noise model, We plug θ ~ U(-a, +a)
(uniform distribution) into Eq. (3) to simplify the experiments. Figure 3 shows model accuracy of
ResNet-32 given different NR period and weight decay factors. For both weight decay and weight
noise insertion, the choice of θ (representing the amount of each weight regularization) has a clear
correlation with NR period (refer to Appendix for training and test accuracy graphs). If we wish
to apply stronger weight regularization, then such weight regularization should be conducted less
frequently (i.e., larger weight decay factor requires longer NR period) to maximize the regularization
effect and achieve high model accuracy. Similar observations are discovered by LSTM model on
PTB as shown in Figure 4. Lower perplexity (indicating better generalization) is obtained when NR
period increases as weight regularization becomes stronger for each regularization event.
Note that compared with a conventional weight decay factor selection (i.e., θ in Eq. (3) when pNR =
1), weight decay factor can be approximately 1,000 times larger with pNR ≈ 1, 000 in Figure 3 and
Figure 4. If ew is the mean absolute difference given as (= E[|w - w0 |]), where w0 is a weight
vector after regularization, then Figure 3 and Figure 4 imply that optimal ew /pNR seems to be
constant. Consequently, optimal ew depends on pNR, and hence, we argue that a wide exploration
of various NR period (for occasional regularization) and different weight regularization strength is
necessary in order to best utilize regularization effects on a DNN model while previous attempts
employ pNR = 1 only. In addition, the observation that stronger weight regularization is enabled
by longer pNR is our basic training principle for model compression.
4
Under review as a conference paper at ICLR 2020
40
100
80
60
20
Non-Regularization Period
s-mJ。*2MUIUn.Id
(a) Perplexity when the weights are compressed by SVD.
Non-RegUlarization Period

[115.2
114.4
-113.6
I 112.8
112.0
111.2

F 120.0
118.5
I 117.0
-115.5
-114.0
-112.5
Non-Regularization Period
(c) Perplexity when the weights are compressed by magnitude-based pruning.
Figure 5: Model accuracy of an LSTM model on PTB compressed by quantization, low-rank ap-
proximation or pruning. Original perplexity without model compression is 114.6. For more details,
refer to Figure 14.
5 NR Period for Model Compression
As discussed, weight compression incurs a much more complicated weight regularization model
than weight decay or uniform weight noise insertion because 1) as shown in Figure 1, diversified
noise models need to be combined to describe weight regularization after model compression and 2)
compression-aware training methods would reduce the strength of weight regularization as training
is performed with more epochs and weights converge to a compressed form. Nonetheless, we can
conjecture that the best training scheme for model compression may require the condition of pNR 6=
1 that can be empirically justified.
We apply weight quantization, low-rank approximation (SVD), and pruning to an LSTM model on
PTB that we selected for the previous section. We do not modify underlying training principles and
use the following simple strategy:
1)	Train the model during NR period (as if model compression is not being considered.)
2)	Then, perform weight compression in the form of w0 = h(w).
3)	With new full-precision weight w0, repeat the above two steps.
h(w) can be a magnitude-based pruning (i.e., h(w)=w if |w| is larger than a certain threshold, or
h(w)=0, otherwise), αb for quantization, SVD function, or even as-yet undiscovered functions.
Figure 5 shows model accuracy associated with a number of different sets of pNR and model com-
pression strength (i.e., target rank for low-rank approximation and the number of quantization bits).
Optimal pNR for the best model accuracy is definitely much larger than 1. To explain how Figure 5
is aligned with Section 4, we investigate the relationship between model accuracy and the average
of ew/pNR (ew = E[|w - w0|], where w0 is a weight vector after weight decay, SVD, or pruning)
during entire training. We first determine optimal ew/pNR (= eopt) for the best model accuracy.
As discussed in Section 4, eopt is assumed to be constant regardless of compression ratio or decay
5
Under review as a conference paper at ICLR 2020
96
2 1
O O
1 1
8
9
(jdd)Asx-dijθd
-0.14
-0.12
-0.10
-0.08
-0.06
-0.04
-0.02
0.00
‘°翁SF裨
Non-RegUlariZation Period
ZdOal ⅛vd/^a-)jo,ɪ,ɪ0
(jdd)Asx-dijθd
(jdd)Asx-dijθd
ZdOal ⅛vd/^a-)jo,ɪ,ɪ0
-0.08
-0.06
-0.04
-0.02
-0.00
ZdOal ⅛vd/^a-)jo,ɪ,ɪ0
(a) Weight Decay (eopt=0.0007)	(b) SVD (eopt=0.08)	(c) Pruning (eopt=0.035)
Figure 6: Relationship between model accuracy and error (defined as the difference between
ew/pNR and eopt) using PTB LSTM model when weights are regularized by weight decay, SVD,
or pruning.
factor, and obtained by finding hyper-parameter sets associated with maximum model accuracy in
Figure 4 and Figure 5 and by taking the average of corresponding ew values. When error is defined
to be |ew/pNR - eopt|, Figure 6 shows test perplexity and error of PTB LSTM model with dif-
ferent pNR. Such defined error is affected by pNR as shown in Figure 6, and indeed, when error
approaches to an optimal value we gain improved model accuracy. Unlike weight decay where ew
is directly computed by decay factors, for model compression techniques, ew is not directly related
to compression-related hyper-parameters (such as ranks and pruning rates). As a result, while Fig-
ure 4 shows clear correlation between decay factors and pNR for best model accuracy, Figure 5
suggests that compression ratio and pNR are weakly correlated. Hence, pNR is a hyper-parameter
to be determined empirically for model compression. Nonetheless, optimal pNR is definitely much
larger than 1, as shown in Figure 6, and decoupled from batch size selection. That means weight
regularization for model compression needs to be conducted much less frequently compared with
gradient descent since batch size selection considers generalization ability of gradient descent, not
regularization effects.
Note that periodic compression has been introduced in the literature to gradually improve com-
pression ratio or automate hyper-parameter search process. DropPruning repeats dropping weights
randomly and retraining the model while some previously dropped weights are unpruned until prun-
ing rate reaches a target number (Jia et al., 2018). Weights are incrementally quantized to improve
model accuracy (Zhou et al., 2017) or the number of quantization bits can be controlled differently
for each layer by a loop based on reinforcement learning (Elthakeb et al., 2018). Structured pruning
and fine-tuning process can be iterated to increase pruning rate (Molchanov et al., 2016; Liu et al.,
2017). All of these previous works assume pNR = 1 (i.e., performing compression for every mini
batch) while the goal is increasing compression ratio slowly or finding a set of hyper-parameters
through iterative fine-tuning stages. Our proposed compression technique can be combined with
such periodic compression methods (incremental compression or automatic hyper-parameter selec-
tion are also applicable to our proposed method). In the work by He et al. (2018), soft filter pruning
is conducted with pNR = 1 epoch without analysis of why such occasional pruning improves model
accuracy.
6	Comparison with Previous Model Compression Techniques
In this section, we compare some of previous model compression techniques with our compression
scheme that introduces pNR and obviates special training algorithm modifications.
6.1	Fine-Grained Weight Pruning
The initial attempt of pruning weights was to locate redundant weights by computing the Hessian
to calculate the sensitivity of weights to the loss function (LeCun et al., 1990). However, such
6
Under review as a conference paper at ICLR 2020
Table 1: Pruning rate comparison using LeNet-300-100 and LeNet-5 models on MNIST dataset. DC
(Deep Compression) and Sparse VD represent a magnitude-based technique (Han et al., 2016) and
variational dropout method (Molchanov et al., 2017), respectively.
Model	Layer	Weight Size	Pruning Rate (%)			
			DC	DNS	Sparse VD	Ours
	FC1	235.2K	92	98.2	98.9	98.9
LeNet-300-100	FC2	30K	91	98.2	97.2	96.0
	FC3	1K	74	94.5	62.0	62.0
	Total	266.2K	92	98.2	98.6	98.4
	Conv1	0.5K	34	85.8	67	60.0
	Conv2	25K	88	96.9	98	97.0
LeNet-5	FC1	400K	92	99.3	99.8	99.8
	FC2	5K	81	95.7	95	95.0
	Total	430.5K	92	99.1	99.6	99.5
a technique has not been considered to be practical due to significant computation overhead for
computing the Hessian. Magnitude-based pruning (Han et al., 2015) has become popular because
one can quickly find redundant weights by simply measuring the magnitude of weights. Since
then, numerous researchers have realized higher compression ratio largely by introducing Bayesian
inference modeling of weights accompanying supplementary hyper-parameters.
For example, dynamic network surgery (DNS) (Guo et al., 2016) permits weight splicing when a
separately stored full-precision weight becomes larger than a certain threshold. Optimizing splicing
threshold values, however, necessitates extensive search space exploration, and thus, longer training
time. Variational dropout method (Molchanov et al., 2017) introduces an explicit Bayesian inference
model for a prior distribution of weights, which also induces various hyper-parameters and increased
computational complexity.
We perform magnitude-based pruning at every pNR step. As a result, even though weights are
pruned and replaced with zero at pNR steps, pruned weights are still updated in full precision during
NR period. If the amount of updates of a pruned weight grows large enough between two consecutive
regularization steps, then the weight pruned at last pNR step may not be pruned at the next pNR step.
Such a feature (i.e., pruning decisions are not fixed) is also utilized for weight splicing in DNS (Guo
et al., 2016). Weight splicing in DNS relies on a hysteresis function (demanding sophisticated fine-
tuning process with associated hyper-parameters) to switch pruning decisions. Pruning decisions
through our scheme, on the other hand, are newly determined at every pNR step.
We present experimental results with LeNet-5 and LeNet-300-100 models on MNIST dataset which
are also reported by Guo et al. (2016); Molchanov et al. (2017). LeNet-5 consists of 2 convolutional
layers and 2 fully connected layers while 3 fully connected layers construct LeNet-300-100. We
train both models for 20000 steps using Adam optimizer where batch size is 50. All the layers are
pruned at the same time and the pruning rate increases gradually following the equation introduced
in Zhu & Gupta (2017):
Pt = Pf + (pi - Pf)(1 - 7ti-) ,	(5)
tf -ti
where E is a constant, Pf is the target pruning rate, Pi is the initial pruning rate, t is the current step,
and the pruning starts at training step ti and reaches Pf at training steptf. After tf steps, pruning
rate is maintained to be Pf . For LeNet-5 and LeNet-300-100, ti , Pi , E are 8000 (step), 25(%), and
7, respectively. tf is 12000 (step) for LeNet-5 and 13000 (step) for LeNet-300-100. Note that these
choices are not highly sensitive to test accuracy as discussed in Zhu & Gupta (2017). We exclude
dropout to improve the accuracy of LeNet-300-100 and LeNet-5 since pruning already works as a
regularizer (Han et al., 2015; Wan et al., 2013). We keep the original learning schedule and the total
number of training steps (no additional training time for model compression).
Table 1 presents the comparison on pruning rates (see Appendix for test accuracy). Despite the
simplicity, our pruning scheme produces higher pruning rate compared with DNS and similar com-
7
Under review as a conference paper at ICLR 2020
pared with variational dropout technique which involves much higher computational complexity.
For Table 1, we use pNR=10 for LeNet-5 and pNR=5 for LeNet-300-100.
6.2	Low-Rank Approximation
We apply our proposed occasional regularization algorithm integrated with Tucker decomposition
(Tucker, 1966) to convolutional neural network (CNN) models and demonstrate superiority of pNR-
based scheme over conventional training methods. In CNNs, the convolution operation requires a
4D kernel tensor K = Rd×d×S×T where each kernel has d × d dimension, S is the input feature map
size, and T is the output feature map size. Then, following the Tucker decomposition algorithm, K
is decomposed into three components as
Rs Rt
Ki,j,s,t = X X Ci,j,rs,rt PsS,rs PtT,rt ,	(6)
rs=1 rt=1
where Ci,j,rs ,rt is the reduced kernel tensor, Rs is the rank for input feature map dimension, Rt is
the rank for output feature map dimension, and PS and PT are 2D filter matrices to map Ci,j,rs,rt
∙-v
to Ki,j,s,t. Each component is obtained to minimize the Frobenius norm of (Ki,j,s,t - Ki,j,s,t). As a
result, one convolution layer is divided into three convolution layers, specifically, (1 × 1) convolution
for PS, (d × d) convolution for Ci,j,rs,rt, and (1 × 1) convolution for PT (Kim et al., 2016).
In prior tensor decomposition schemes, model training is performed as a fine-tuning procedure after
the model is restructured and fixed (Lebedev et al., 2015; Kim et al., 2016). On the other hand, our
training algorithm is conducted for Tucker decomposition as follows:
Step 1: Perform normal training for pNR (batches) without considering Tucker decomposition
Step 2: Calculate C, PS, and PT using Tucker decomposition to obtain K
∙-v
Step 3: Replace K with K
Step 4: Go to Step 1 with updated K
After repeating a number of the above steps towards convergence, the entire training process should
stop at Step 2, and then the final decomposed structure is extracted for inference. Because the model
is not restructured except in the last step, Steps 2 and 3 can be regarded as special steps to encourage
wide search space exploration so as to find a compression-friendly local minimum where weight
noise by decomposition does not noticeably degrade the loss function.
Using the pre-trained ResNet-32 model with CIFAR-10 dataset (He et al., 2016; Kossaifi et al.,
2019), we compare two training methods for Tucker decomposition: 1) typical training with a de-
composed model and 2) pNR-based training, which maintains the original model structure and
occasionally injects weight noise through decomposition. Using an SGD optimizer, both training
methods follow the same learning schedule: learning rate is 0.1 for the first 100 epochs, 0.01 for the
next 50 epochs, and 0.001 for the last 50 epochs. Except for the first layer, which is much smaller
than the other layers, all convolution layers are compressed by Tucker decomposition with rank Rs
and Rt selected to be S and T multiplied by a constant number Rc (0.3 ≤ Rc ≤ 0.7 in this ex-
periment). Then, the compression ratio of a convolution layer is d2ST/(SRs + d2RsRt + TRt)
= d2ST/(S2Rc+d2Rc2ST +T2Rc), which can be approximated to be 1/Rc2 if S = T and d Rc.
pNR is chosen to be 200.
Figure 7 shows test accuracy after Tucker decomposition1 by two different training methods. Note
that test accuracy results are evaluated only at Step 3 where the training process can stop to generate
a decomposed structure. In Figure 7, across a wide range of compression ratios (determined by
Rc), the proposed scheme yields higher model accuracy compared to typical training. Note that
even higher model accuracy than that of the pre-trained model can be achieved by our method if
the compression ratio is small enough. In fact, Figure 8 shows that our technique improves training
loss and test accuracy throughout the entire training process. Initially, the gap of training loss and
test accuracy between pre-regularization and post-regularization is large. Such a gap, however, is
quickly reduced through training epochs. Overall, ResNet-32 converges successfully through the
1https://github.com/larry0123du/Decompose-CNN
8
Under review as a conference paper at ICLR 2020
SSoq^uɪurejɪ
(js)κ3BJn33v
Compression Ratio
4 2 0
♦ ♦ ♦
Ooo
(兴)⅛ɔBJnɔɔv"①h
I50
1
Figure 7: Test accuracy comparison on ResNet-32 using CIFAR-10 trained by typical training
method and the proposed training method with various compression ratios. For the proposed scheme,
test accuracy is measured only at Step 3 that allows to extract a decomposed structure, and pNR is
200.
---Typical SCheme(pNR=1)
---Proposed (pNR=200, Post-Reg.)
---Proposed (pNR=200, Pre-Reg.)
50	100	150
O
5
O
I O
O
O
2
Epoch	Epoch
Figure 8: Training loss and test accuracy of ResNet-32 using CIFAR-10. For the proposed scheme,
training loss and test accuracy are only monitored right before or after weight regularization for
compression (pre-regularization or post-regularization). Compression ratio is 2.8 with Rc=0.5.
entire training process with lower training loss and higher test accuracy compared with a typical
training method.
For ResNet-34 on ImageNet experiments and VGG19 on CIFAR-10 (including additional compres-
sion techniques), refer to Appendix.
7	Conclusion
In this paper, we show that weight regularization should be decoupled from batch size to enhance
regularization effects. To accomplish such decoupling, we introduce a new hyper-parameter called
non-regularization period or NR period during which weights are updated only for gradient compu-
tations. A larger amount of weight decay and weight noise insertion need to be supported by longer
NR period to maximize the regularization effect. We find that model compression with higher com-
pression ratio is also better trained by longer NR period. We demonstrate that various DNN models
can be compressed by pruning, quantization, and low-rank approximation successfully with longer
NR period while the underlying training algorithm do not need to be modified.
References
Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catan-
zaro, and Evan Shelhamer. cuDNN: Efficient primitives for deep learning. arXiv:1410.0759,
9
Under review as a conference paper at ICLR 2020
2014.
Minsik Cho and Daniel Brand. MEC: memory-efficient convolution for deep neural network. In
International Conference on Machine Learning (ICML), pp. 815-824, 2017.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training deep neural
networks with binary weights during propagations. In Advances in Neural Information Processing
Systems, pp. 3123-3131, 2015.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and Andrew Y. Ng. Large scale
distributed deep networks. In Advances in Neural Information Processing Systems, pp. 1223-
1231, 2012.
Ahmed T Elthakeb, Prannoy Pilligundla, Amir Yazdanbakhsh, Sean Kinzer, and Hadi Esmaeilzadeh.
Releq: A reinforcement learning approach for deep quantization of neural networks. arXiv
preprint arXiv:1811.01704, 2018.
K.	Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the efficiency of GPU algorithms for
matrix-matrix multiplication. In Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Confer-
ence on Graphics Hardware, pp. 133-137, 2004.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Stabilizing the
lottery ticket hypothesis, 2019.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient DNNs. In
Advances in Neural Information Processing Systems, 2016.
Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen. Network sketching: exploiting binary struc-
ture in deep CNNs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 4040-4048, 2017.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for
efficient neural networks. In Advances in Neural Information Processing Systems, pp. 1135-1143,
2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and Huffman coding. In International Conference on Learning
Representations (ICLR), 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-
778, 2016.
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating
deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018.
SePP Hochreiter and Jurgen Schmidhuber. Simplifying neural nets by discovering flat minima. In
Advances in Neural Information Processing Systems, pp. 529-536, 1995.
HaiPeng Jia, Xueshuang Xiang, Da Fan, Meiyu Huang, Changhao Sun, Qingliang Meng, Yang He,
and Chen Chen. DroPPruning for model comPression. arXiv preprint arXiv:1812.02035, 2018.
Norman P. JouPPi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Ba-
jwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin,
Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb,
Tara Vazir Ghaemmaghami, Rajendra GottiPati, William Gulland, Robert Hagmann, C. Richard
Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski,
Alexander KaPlan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy,
James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin,
Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi
10
Under review as a conference paper at ICLR 2020
Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda,
Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory
Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory
Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang,
Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing
unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture,
2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima, 2016.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Com-
pression of deep convolutional neural networks for fast and low power mobile applications. In
International Conference on Learning Representations (ICLR), 2016.
Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. Tensorly: Tensor learning
in python. Journal of Machine Learning Research, 20(26):1-6, 2019. URL http://jmlr.
org/papers/v20/18-277.html.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky.
Speeding-up convolutional neural networks using fine-tuned CP-decomposition. In International
Conference on Learning Representations (ICLR), 2015.
Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In Advances in Neural
Information Processing Systems, pp. 598-605, 1990.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 2736-2744, 2017.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2017.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of English: The Penn Treebank. Comput. Linguist., 19(2):313-330, 1993.
Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks, 2018.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry P. Vetrov. Variational dropout sparsifies deep
neural networks. In International Conference on Machine Learning (ICML), pp. 2498-2507,
2017.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv:1609.04747,
2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
L.	R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31:279-311,
1966.
Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint
arXiv:1706.05350, 2017.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. Regularization of neural
networks using DropConnect. In International Conference on Machine Learning (ICML), 2013.
Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin
Zha. Alternating multi-bit quantization for recurrent neural networks. In International Conference
on Learning Representations (ICLR), 2018.
11
Under review as a conference paper at ICLR 2020
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv:1409.2329, 2014.
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay
regularization, 2018.
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. In
International Conference on Learning Representations (ICLR), 2017.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for
model compression. CoRR, abs/1710.01878, 2017.
12
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Supplementary Experiments for Weight Decay and Weight Noise
(e) ,IOaUBH AB。①CI mF-①M
5.0e-04 92.0 91.6
91.3 91.3 91.2 91.2 91.3 91.2 91.3
1.0e-03- 92.6	92.0	91.5	91.5	91.3	91.4	91.2	91.4
2.0e-03- 9291	92.7	92.0	91.9	91.5	∣91.3	91.3	∣91.1
5.0e-03 ∣93.2∣	93.0∣	92.5	92.2	91.8	91.5	91.5	91.3
1.0e-02 j93.2 J	93.3[	92.9[	92.5	92.1	91.6	91.5	91.5
2.0e-02- 92.5 ∣93.0 ∣ 93.1∣ 93.0∣ 92.6 92.0 91.7 ∣91.4
5.0e-02- 90.71 92.4 j93.2 ∣ 93.1∣ 929∣ 92.5 92.1 91.7 ∣
1.0e-01 ∣87.7∣ 90.8∣ 92.4，3.1 ∣ 93.2∣ 92.9∣ 92.5 92.0
2.0e-01 14.8 J 87.11 91.4 ∣92.6 193.0 ∣ 93.01 92.8∣ 92.5
91.3
91.3
91.2
91.2
5.0e-01
2.0e+00
5.0e+00
91.0 91.0
91.3 91.3
91.3 91.3
91.3 ∣91.2
91.3 91.3
91.5 91.3 91.4
91.4 91.5 ∣91.2
91.7 91.5 91.5
92.1 91.7 91.5
5.0e-04-
1.0e-03-
2.0e-03-
5.0e-03
1.0e-02
2.0e-02
5.0e-02-
1.0e-01
2.0e-01
5.0e-01
1.0e+00
2.0e+00
5.0e+00
、,% §翁谷《松靖孝孝
Non-RegUlarization Period
91.9	91.4	91.3	91.3	91.4	91.2	91.2	91.1	91.4	91.3	91.4
92.2	91.6	91.5	91.5	91.4	91.4	91.2	91.2	91.4	91.5	91.3
92.6
92.2
92.'
92.2
92.2
91.6 91.5
91.3
92.2
91.7
91.5
92.6
92.1
92.5
91.8
92.1
92.6
92.2 91.8
92.7 92.2
Non-RegUlarization Period
91.4	91.3	91.3	91.3	91.3	91.4
91.4	91.3	91.3	91.2	91.2	91.3
91.4 91.4 91.3 91.1 91.2 91.4
91.8 ∣91.2 91.2 91.3 91.2 91.3
92.0 91.9 91.4 91.4 91.4 91.3
[93.1 [93.2] 930∣ 92.7 92.2 91.6 91.5 91.5 ∣91.3
92.2
92.5 92.0 91.7 91.6 ∣91.2
j93.1 [93.3] 93.1] 92.7 92.2 91.7 91.7
92.2
∣930 [93.2 J 93.11 92.5 92.0 92.0
91.9 92.7
92.5
92.1
92.5
Figure 9: Model accuracy of ResNet-32 on CIFAR-10 using various NR period and amount of
weight regularization (original model accuracy without regularization is 92.6%). (Left): Weight
decay. (Right): Uniform weight noise insertion.
Table 2: Model accuracy of ResNet-32 on CIFAR-10 and LSTM model on PTB with various weight
decay factor and corresponding pNR.
Model		Weight Decay Factor(θ)						
		0	1e-4	5e-4	1e-3	5e-3	1e-2	5e-2
ResNet-32	Accuracy(%)	92.6	93.3	93.2	93.2	93.3	93.2	92.9
	optimal pNR	N/A	-^2^^	-^5^^	20	100	200	1000
LSTM on PTB	Perplexity	114.6	108.1	97.7	97.1	97.1	97.0	97.2
	optimal pNR	N/A	1	1	1	5	10	100
≈)，IO"PH APɔ①α 2q∙-①M
5e-04-
1e-03-
5e-03-
1e-02-
5e-02-
1e-01-
5e-01-
92.5	92.7	93.0	92.4	91.8	91.6	90.5	89.9
91.9	92.8	93.0	92.6	92.5	92.0	91.0	90.2
88.1	91.0	92.2	93.0	93.2	92.9	92.5	92.2
85.7	87.9	91.1	92.5	92.7	93.1	92.8	92.6
50.0	79.5	85.2	87.7	90.4	91.9	92.6	92.9
51.4	60.2	80.9	84.2	88.1	90.5	91.9	92.6
34.9	39.1	48.3	60.5	79.7	83.9	87.5	86.2
93.0
91.5
90.0
-88.5
-87.0
I 85.5
Batch Size
Figure 10: Model accuracy (%) of ResNet-32 for various weight decay factors and batch size when
pNR=1. Large batch size demands larger weight decay factors that is also reported by Loshchilov
& Hutter (2017).
13
Under review as a conference paper at ICLR 2020
(e) JOZBH AB髭a U--①
5.0e-05 111.21112.71113.9,13.9'14.2,14.7|114.21114.11114.4,14.2'14.3,14.4114.51114.51114.21 114.2
1.0e-04 ∣108.4 111.2 113.4	113.7	114.0	113.8	114.3	114.1	114.5	114.5	114.5	114.2	114.2	114.4	114.6	114.5
2.0e-04 ∣990∣ 103.0 108.6	110.9	112.8	113.4	113.5	113.7	114.1	114.4	114.5	113.9	114.0	114.6	114.3	114.4
5.0e-04 198.6798.91104.3	108.5	111.5	112.1	113.1	112.7	113.7	114.0	114.6	114.3	114.4	114.4	114.4	114.4
1.0e-03 ∣118.5 106.1 198.8	99.2∣	103.2	105.5	107.5	108.6	111.2	112.5	113.4	113.6	114.0	114.1	113.8	114.1
2.0e-03 138.9 p8^ 102.9∣984∣992∣ 101.2 103.1 104.8 108.6 110.2 111.9 112.6 113.1 113.3 113.9 114.1
5.0e-03 161.3	139.1	113.9	103.5198.8	985∣ 992∣	100.3 104.4	106.9 109.8	111.2	112.1	112.7	113.1	1137
1.0e-02 233.6	152.2	123.8	108.8	100.71985 982	986∣101.7	104.3 108.3	110.1	111.3	111.7	112.6	113.3
2.0e-02 509.0	172.1	138.5	119.4	106.3	101.6199.7	98.9 99^	100.8 104.9	107.9	108.8	110.1	111.2	112.6
5.0e-02 197.2	252.1	150.0	128.1	105.6 102.0	100.31983∣	992∣ 102.7	105.4	107.0	108.2	110.3	111.8
1.0e-01 180.2137.3	163.2	139.9	119!	110.9 106.5	103.3 卜8.6	98.61 100.4	103.2	104.8	106.6	108.6	110.6
1.5e-01 178.0104.5185.7147.2	126.2	115.4	110.2	106.5199.6	98.4∣ 99?2| 101.7	103.0	104.9	107.1	109.6
2.0e-01 卜76.5195.7 235.1 153.2	131.5	120.5	113.2	109.3	101.0198.4 98.8∣ 101.0	102.3	103.8	105.9	108.9
2.5e-01 卜76.0卜83.0 317.7 1651	141.1	128.2	120.4	114.9	104.1199.8 ∣983∣ 995	100.1	101.8	104.2	107.7
3.0e-01 ∣677.5∣677.3∣639.1∣205.3∣149.5∣135.3∣127.2∣120.9	107.1	102.0198.5 | 98©	əə,p 100.7	102.4	106.3
、、C C ☆9“令«@取皮梦术£产
Non-RegUlariZation Period
(a) Initial learning rate = 1.0
(e) JozEH AEKa +q∙-①M.
5.0e-05 |110.8.2叩14.11114.51114.叩14.9114叩14.51114.71114.71115.31114.71114叩14邛14.叩14.8
1.0e-04 ∣108.0	111.0	113.1	113.9	114.3	114.7	114.3	115.0	114.6	114.6	114.9	114.4	114.7	114.5	114.6	114.7
2.0e-04 ∣103.5	107.8	111.9	113.1	113.8	114.3	114.6	114.4	114.5	114.2	114.7	114.6	114.3	115.1	115.3	114.5
5.0e-04 19771101.9	108.1	110.5	112.8	113.5	113.8	113.7	114.7	114.4	114.8	114.7	114.5	114.9	114.4	114.6
1.0e-03 卜7.1	97.81	103.5	107.8	111.4	112.7	113.0	113.0	113.5	114.0	114.3	114.5	114.6	114.8	114.7	114.8
2.0e-03 ∣101.7197*9871103.1	108.0 110.4	111.1	111.6	113.7	114.1	114.4	114.2	114.5	114.4	114.4	114.3
5.0e-03 ∣119.0 104.61971 97.6|	101.5 104.4	106.3	108.1	110.9	112.2	113.8	114.0	113.9	113.8	114.4	114.2
1.0e-02 139.1 118.5 101.8197.0	97.6∣ 99.7	101.6	103.4	108.0	110.2	112.0	113.0	113.0	113.6	114.2	113.9
2.0e-02 167.0139.8 113.4 101.7∣97.0 I 96.8∣	97■虱Tj	103.5	106.5	109.7	111.5	112.2	112.6	113.3	113.7
5.0e-02 178.1 210.9	140.2	119.6	104.9	100.0198.3	∣97.3	97.2∣ 99.8 104.0	107.3	108.5 109.7	111.0	112.4
1.0e-01 187.1193.6	173.1141.4	121Z	110.4	105.8	102.4	j97^ 970∣ 992|	101.9	103.5 105.2	107.6	110.1
1.5e-01 182.4卜94.1 234.6255.6卜33.2 121.2^^ 108.9199.7⅛7^ 975∣ 99!j 100.5 102.3 104.7 108.1
2.0e-01 685.5186.3	331.8	181.0	143.7	129.9	121.9	116.0	103.2198.9 97.1	97.81	9871 99.7	102.4	106.1
2.5e-01 卜83.7 686.1	518.7	218.7	152.0	138.1	129.6	122.8	107.2 101.21976	97.2	97.81 9O∣ 100.7	104.0
3.0e-01 ∣687.0∣685∙1∣701∙8∣253∙1∣161∙1∣146∙3∣136∙6∣129∙4∣111∙9 104.2198.7 | 97.2 | 97.2 ∣ 97^ 99?5| 102.6
、、C C ☆9“令«@取皮梦术£产
Non-RegUlariZation Period
(b)	Initial learning rate = 1.5
(e) JozEH AEKa +q∙-①ʌv
2.0e-04
5.0e-04
1.0e-03
2.0e-03
5.0e-03
1.0e-02
2.0e-02
5.0e-02
1.0e-01
1.5e-01
2.0e-01
2.5e-01
3.0e-01
101.0
107.8 111.1 113.1
103.3 107.6 111.8
105.8 197.4
102.6
102.3
108.4
119.5 105.9 101.1
101.4 104.0
1114.7	114.5	115.0	115.5	115.6	115.3	115.2	115.0∣115.1	115.4
115.6	114.9	115.2	114.9	115.2	115.5	115.1	115.1 114.9	115.0
	114.5	114.6	115.2	115.0	115.2	114.9	115.2 115.7	115.2
∣112.8	113.2	114.2	114.3	114.7	114.9	115.0	114.9 115.3	115.1
106.2	107.6 111.3		112.4	113.6	114.3	115.0	114.5 114.9	115.1
101.4	103.1	107.9	110.2	≡9	113.7	113.9	113.8∣114.1	114.2
97.3	98.5	103.0	105.9	109.7	111.6∣111.8∣112.8∣113.5			114.5
96.9	96.8	100.0	103.1	107.4	109.8	110.6	112.0∣112.6	113.6
98.2	97.4	97.2	99.1	103.5	107.1	108.1	109.4 110.8	112.4
101.8	99.5	96.7	97.7	101.2	104.2	106.1	107.6 109.1	111.4
106.4	103.4	97.5	97.0	98.7	101.5	102.6	105.0 107.2	109.5
111.4	106.6	98.9	96.8	97.5	99.7	101.0	103.5 105.8	108.6
114.8	110.6	100.5	97.9 I 97.0 I 98.9			99.6 101.3 104.3 107.4		
123.0	117.5	104.9	99.9	97.7	97.6	98.2	99.6 101.9	105.2
回.9	124.7	109.3	102.9	98.0	97.1	97.3	"98.51 99.8	103.1
Non-RegUlariZation Period
(c)	Initial learning rate = 2.0
Figure 11: Perplexity of LSTM model on PTB dataset using various NR period and amounts of
weight decay (original perplexity without regularization is 114.60).
14
Under review as a conference paper at ICLR 2020
5.0e-05 ∣114.2 114.1 114.8 114.1 114.6 114.3 114.3 114.3
1.0e-04 114.2 114.4 114.2 114.4 114.3 114.6 114.5
2.0e-04 ∣113.7 114.8 114.6 114.2 114.7 114.5 113.9
114.1
114.1
114.2 114.6 114.5 114.4 114.7 114.7 114.6
114.4 114.1 115.1 114.6
114.8
5.0e-04-114.2∏^H114.7 115.0 114.5 114.9 114.2 114.3
1.0e-03-114.1 114.1 呼@114.3 114.1 114.7 114.2 114.3
2.0e-03 便£114.1 114.4 114.2 114.5 114.3 114.7 114.6
5.0e-03 ∣139.4 114.6 113.2 114.4 114.2 114.2 114.2 115.0
114.3 114.3H13.9 114.1
114.3 114.4 114.5 114.2 114.3
114.4 114.4 114.2 114.5 114.2
114.8 114.9 114.7 114.5 114.3
114.2 114.8 114.5 114.5 114.3
114.1 114.5
114.1 Mg
N 114.7
N 114.4
114.1 114.5
114.1 114.5
1.0e-02
2.0e-02
5.0e-02
1.0e-01
1.5e-01 .8
2.0e-01 |呼3
2.5e-01
3.0e-01
114.8
114.3 113.8 1139 114.2 114.9
114.2 113.9 114.2 113.8 114.4 114.7 114.6
114.8
114.4
114.5
114.0
114.8
114.2 114.0 114.7 114.3 113.9 114.6 114.0
114.7
114.2
114.3
114.6
114.3
114.4
115.9
e
Non-RegUlariZation Period
(a)	Initial learning rate = 1.0
((e+sl)λ2 〜e)e
5.0e-05 115.2 114.6
1.0e-04 -114.3 114.6
2.0e-04 -114.6 114.5
5.0e-04-115.1 114.7
1.0e-03 ∣114.8 114.9
2.0e-03 ∣114.5 114.0
5.0e-03 114.2 114.2
114.6 114.9 114.5 114.7 114.7 114.6 114.2 114.9 114.3 114.2 114.2 114.3 114.8 115.0
114.4 114.9 114.5 114.5 114.6
114.9 114.5 115.0 114.8 114.3
114.6 115.0
114.8 114.8
114.2
114.4
114.9
114.6 114.6 115.0 114.8 114.4 114.6 114.8 114.6 114.9
115.1 114.9 114.8 114.6 114.4 114.9 114.5 114.5 114.5
114.5 114.6 114.7 114.3 114.6 114.5 114.7 114.3 114.7 114.6
114.7 114.5 114.7 114.7
114.6 115.1 115.0 114.8 114.7 114.6 115.0
114.6 114.8 114.5 114.5 114.5 114.5 114.8 114.8 114.6 114.7 114.3 114.9 114.7
1.0e-02 ∣119.2 113.8∣
2.0e-02 阿9∏^
5.0e-02 I竺8
1.0e-01
1.5e-01
2.0e-01
2.5e-01
3.0e-01
114.5 114.6 114.6 114.5 114.8 114.9 114.7 115.0 114.6 114.9
114.6
114.8 114.9
114.2 114.5 115.0 114.4 114.7 114.6 114.8 114.3 115.1 114.4 114.6 114.6 114.6
114.3
114.3 114.1 114.6 114.8 114.8 114.7 114.8 114.9 114.8 114.9
114.4 114.5
114.5
114.2
115.2
114.3
114.1
115.6
113.7 114.5 114.9 114.8
114.6∣113.3∣
115.2
132^117.1 114.5
Non-RegUlariZation Period
(b)	Initial learning rate = 1.5
((e+6l)λ?〜e)e
5.0e-05 ∣115.5	115.3	115.5	114.8	115.5	114.9	115.2	114.1	115.4	114.8	115.6	115.2	115.2	115.6	115.3	115.4
1.0e-04 115.3	115.1	115.4	114.7	114.8	114.9	115.4	115.1	115.1	114.8	115.2	115.8	114.9	115.1	115.1	115.5
2.0e-04 ∣115.3	114.6	115.4	115.5	114.8	115.5	114.7	115.0	115.0	115.5	115.1	114.6	115.3	115.1	115.1	115.8
5.0e-04 ∣114.9 115.2 115.1 115.6 116.0 115.1 115.0 115.0 115.2 115.3 115.5 115.3 114.7~^6Γ 115.3 115.2
1.0e-03 ∣114.9	114.9	115.1	114.6	115.1	115.0	115.8	115.3	115.1	115.3	115.3	115.5	115.3	115.2	115.1	114.5
2.0e-03 ∣156.7	121.0	114.7	114.7	115.2	114.8	115.4	115.3	115.3	115.4	115.3	115.6	115.0	115.5	115.1	115.3
5.0e-03 256.0	350.4	141.2	118.0	115.0	114.7	114.5	115.0	115.2	115.1	115.4	114.8	115.0	115.1	115.1	115.9
1.0e-0 2	224.7 ,46.8	325.2卜47.2	1194 116.0	114.3	115.4	114.7	114.6	114.6	114.9	114.3	115.3	115.5	114.8
2.0e-0 2	224.4 .29.4卜41.0 336.8	212.8143.5	126.9	120.8	115.0	114.6	115.0	114.8	115.0	114.8	115.0	115.4
5.0e-02 218.8	221.6	226.8	244.5	364.7	295.8	203.0	160.7	120.2	115.8	115.0	115.2	114.9	115.0	114.5	115.7
1.0e-01 206.3	217.7	224.8	229.4	235.7	286.7	377.8	352.6	159.6	128.9	116.7	114.5	114.8	115.0	114.8	114.8
1.5e-01	204.5 212.9	224.6128.6126,241.8	258.6	312.7	277.0	169.5	122.3	116.6	114.9	115.3	115.1	114.9
2.0e-01	200.8,07.1	219.3 230.2	230.2 229.7.36.4439.7卜03.9	262.6138.3	120.3	117.4	115.7	114.6	115.3
2.5e-01	198.0 201.9	212.6 223.7	229.4 230.6	232.7	233.1	261.2141.3	255.9卜51.2	133.3	122.4	116.4	114.2
3.0e-01 ∣197.6∣198.6∣208.7∣218.0∣228.6∣228.1∣231.1∣231.1∣234.5∣266.7∣551.1∣362.8∣201.8∣146.7∣123.8 115.6
、、C C ☆9“令«@取皮梦术£产
Non-RegUlariZation Period
(c) Initial learning rate = 2.0
Figure 12:	Perplexity of LSTM model on PTB dataset using various NR period and amounts of
uniform noise (original perplexity without regularization is 114.60).
15
Under review as a conference paper at ICLR 2020
(兴)AOEJnOUV
Noise (X 〜U( —a,a))
(PNR, a)
----train (2, 0.01)
test (2, 0.01)
----train (100, 1.0)
----test (100, 1.0)
train (2000, 5.0)
test (2000, 5.0)
50	100	150	200
Epoch
π
(兴)AOEJnOOV
0 5 0 5 0
0 9 9 8 8
Figure 13:	Training and test accuracy of ResNet-32 on CIFAR-10 for variuos NR period and the
amount of weight regularization. (Left): Weight decay. (Right): Uniform weight noise.
A.2 Parameter Pruning
Table 3: Model accuracy comparison using LeNet-300-100 and LeNet-5 models on MNIST dataset.
DC (Deep Compression) and Sparse VD represent a magnitude-based technique (Han et al., 2016)
and variational dropout method (Molchanov et al., 2017), respectively.
Model	Accuracy (%)			
	DC	DNS	Sparse VD	Ours
LeNet-300-100	98.4	98.0	98.1	98.1
LeNet-5	99.2	99.1	99.2	99.1
We apply pNR-based pruning to an RNN model to verify the effectiveness of pNR. We choose an
LSTM model (Zaremba et al., 2014) on the PTB dataset (Marcus et al., 1993). Following the model
structure given in Zaremba et al. (2014), our model consists of an embedding layer, 2 LSTM layers,
and a softmax layer. The number of LSTM units in a layer can be 200, 650, or 1500, depending on
the model configurations (referred as small, medium, and large model, respectively). The accuracy
is measured by Perplexity Per Word (PPW), denoted simply by perplexity in this paper. We apply
gradual pruning with E = 3, ti = 0, pi = 0, tf = 3rd epoch (for medium) or 5th epoch (for
large) to the pre-trained PTB models. pNR-based pruning for the PTB models is performed using
pNR = 100 and the initial learning rate is 2.0 for the medium model (1.0 for pre-training) and 1.0
for the large model (1.0 for pre-training) while the learning policy remains to be the same as in
Zaremba et al. (2014).
Table 4: Comparison on perplexity using various pruning rates. pf is the target pruning rates for the
embedded layer, LSTM layer, and softmax layer.
Model Size	Prnnino TVfpfhcrI					Perplexity			
	pf=	0%	80%	85%	90%	95%	97.5%
Medium	(Zhu & Gupta, 2017)	83.37	83.87	85.17	87.86	96.30	113.6
(19.8M)	Proposed Scheme	83.78	81.54	82.62	84.64	93.39	110.4
Large	(Zhu & Gupta, 2017)	78.45	77.52	78.31	80.24	87.83	103.20
(66M)	Proposed Scheme	78.07	77.39	77.73	78.28	84.69	99.69
For all of the pruning rates selected, Table 4 shows that our compression scheme improves perplexity
better than the technique in Zhu & Gupta (2017) which is based on Han et al. (2015). The superiority
of pNR-based pruning is partly supported by the observation that non-zero weights successfully
avoid to be small through retraining while the conventional pruning still keeps near-zero (unmasked)
weights as depicted in Figure 15.
16

17

s-mJo*
①2BαiMUIUnJd
IOO ɪg	■■«»1 114.3 114.2 114.3 113.8 114.3 114,0 ∣] gj 114.3 113.7 112.5 113.0 112.5 112.7 113.1 113.5 113.5 113.9 ∣∣ Q 112.7 114.0
90-∣U	3 114.1 113.9 BBEBaHBEWW Q 113.7 113.8 113.1 113.5 113.6 113.2	112/T 113.0	112.9 113.0 113.4 113.6 113.3 113.4
80-	g 113.8 114.3 BIEIta 114.0 114.1 114.3 114.2 BBEK81 113.5 113.5 113.7 112.8 112.5 Q ∣QQ 112.0 112.4 112.9 112.5 112.3 112.9 113.4 113.6
7Ω ⅜BlflEl 114.3]mKaL114.0 114,2] 113.9 BBEIBl 113.8 113.8 112.7 113.4 113.2 113.3 112.7 112,6 ∣]	Q 112.1 112.9 112.3 113.4 114.1 113.9 113.9
60- 114.0 Q 园 113.5 113.5 113.5 Q Q 113.8 113.9 113.4 113.7 113.5 112.3 112.6 212.3 Q	112.0 ■■■■«	倒 112.9 113.7 113.2 113.6 114.0 ⅛BBM1
50- 114.0 113.9 113.6 BBEIEB 113.7 ∣∣	113.9 113.7 113.5 113.3 112.9 112.1 [jL12.0j 112.4 Q	113,2 113.9 113.5 BIEK1Π1
40- 114.1 113.7 113.8 BIEIM 114.1 113.1 113.4 113.4 113.6 113.0 112.7 Wim∙IW∙∙∙∙IWlΠΠE⅛"IKiΠΠE1 112.4 112.5 113.8 114.1	114.2 ΠE⅞I"BM
30-	114.3 114.1 113.2 114.3 114.3 113.1 114.2 113.3 112.5 113.7 Q 112.2 Q	112,5 112.4 113.7 114.1 ⅛BB!E∙∙B⅝M∏ra3∙∙Kjκd
∙n-吗IBrt■部∙∙y 114.4	■■种 114.4 113.6 BBt⅜Wl 114.0 113.2 113.2 113.2 112.7 114.1 mjam■y吗）»1皿
、"3 C我于"令。$武短於春岁&峭
Non-Regularization Period
[Quantization]
4
3
2
1
111.1	111.1	112.1	112.3	111.2
118.0		112.5	112.1	111.9
133.1	128.6	123.1	120.9	118.5
138.8	135.2	131.1	128.0	126.8
	113.5 D	113.4	114.0	114.4	114.7	115.3 115.7	
		113.1	113.6	113.8	114.3	114.5	115.6
117.7	114.1	113.2	112.5	111.7	111.6	112.6	112.8
125.0	120.3	118.3	115.9	113.2	113.0	112.7	113.1
116.6	116.2	116.4	116.5	116.2	117.0	116.9	117.7	117.4	116.5	117.4
116.0	115.3	115.6	116.2	116.8	117.8	117.5	119.0	117.6	117.6	120.7
115.0	113.8	114.3	114.5	115.6	116.0	115.8	116.8	116.4	117.7	119.1
115.3	116.6	H	120.3 I	123.6 I	126.2 I	128.0 I	133.2	H	135.1 I	139.4
“Q我 蛉©少 ^裨 a 3术 6 科 & 就 I S / 6 <Q / F 豌
Non-Regularization Period
[Pruning]
0.70-
0.75-
0.80-
0.85-
0.90-
117.9	117.6	117.1	116.6	116.2	115.2	116.1	115.5	115.6	114.7	115.0	114.0	114.9	114.7	114.5	114.2	113.7	114.4	114.3	114.3	114.4	114.9	114.8	114.3	114.8	115.2
118.0	116.6 116.2		116.5	116.2	116.5	116.1	116.1	115.2	115.5	114.0	114.0	113.9	114.1		n	113.7	113.9	115.4	115.6	114.1 114.3		114.1	114.2	114.6	114.0
118.3	Ul	116.6	117.3	115.5	116.5	116.2	115.5	115.8	114.3	114.7	114.0	114.2	113.9	n	D	113.5	I 113.4	113.3	1113.3 I	114.5	114.2	114.0	112.9	114.2	115.1
117.6	117.3	117.0	116.3	116.3	115.8	115.6	115.4	114.9	114.7	114.2	113.1	113.1	112.7	113.0	m	π	n	m	114.2	114.7	114.3	114.2	114.3	114.6	115.5
117.2	118.4	117.2	116.8	115.6	115.8	116.0	115.2	114.7	113.8	114.0	113.4	113.2	113.4	113.3	114.0	114.3	114.3	115.7	116.2	H	116.8	116.4	117.4	117.6 I	118.5 I
Non-Regularization Period
Figure 14: Relationship between test perplexity and pNR using PTB LSTM model.
1-115.2
-114.4
-113.6
-112.8
[112.0
111.2
∣- 120.0
1-118.5
-117.0
-115.5
-114.0
u- 112.5
[118
117
-116
-115
-114
1113
UlIderreVieW as a COnferenCe PaPer at ICLR 2020
Under review as a conference paper at ICLR 2020
1600
1600
+unoɔ
1000
800
600
200
400
1200
1400
0
-1.0	-0.5	0.0	0.5
Weight Value
+unoɔ
1400
1200
1000
800
600
400
200
1.0
0÷-
-1.0
0.5	1.0
-0.5	0.0
Weight Value
Figure 15: Weight distribution of LSTM layer 1 of the medium PTB model after retraining with
(Left) a magnitude-based pruning and (Right) pNR-based pruning with 90% pruning rate. Our
compression scheme incurs a sharp drop in the count of near-zero weights.
A.3 Tucker Decomposition with Occasional Regularization
To investigate the effect of NR period on local minima exploration with ResNet-32 on CIFAR-10,
Figure 16 presents the changes of loss function and weight magnitude values incurred by occasional
regularization. In Figure l6(left), ∆L/L is given as the loss function increase ∆L (due to weight
regularization at pNR steps) divided by L, which is the loss function value right before weight
regularization. In Figure 16(right), ∆w is defined as ||w - W∣∣F /N(w), where W is the entire set
of weights to be compressed, W is the set of weights regularized by Tucker decomposition, N(W)
is the number of elements ofW, and ||X||2F is the Frobenius norm ofX. Initially, W fluctuates with
large corresponding ∆L. Then, both ∆L and ∆W decrease and Figure 16 shows that occasional
regularization finds flatter local minima (in the view of Tucker decomposition) successfully. When
the learning rate is reduced at 100th and 150th epochs, ∆L and ∆W decrease significantly because
of a lot reduced local minima exploration space. In other words, occasional regularization helps an
optimizer to detect a local minimum where Tucker decomposition does not alter the loss function
value noticeably.
Figure 16: Difference of training loss function and average Frobenius norm of weight values by Step
2 and Step 3 of Figure 2. Rc = 0.5 and pNR = 200 are used.
18
Under review as a conference paper at ICLR 2020
A.4 2-Dimensional SVD Enabled by Occasional Regularization
In this subsection, we discuss why 2D SVD needs to be investigated for CNNs and how occasional
regularization enables a training process for 2D SVD.
A.4. 1 Issues of 2D SVD on Convolution Layers
Convolution can be performed by matrix multiplication if an input matrix is transformed into a
Toeplitz matrix with redundancy and a weight kernel is reshaped into a T × (S × d × d) matrix
(i.e., a lowered matrix) Chetlur et al. (2014). Then, commodity computing systems (such as CPUs
and GPUs) can use libraries such as Basic Linear Algebra Subroutines (BLAS) without dedicated
hardware resources for convolution Cho & Brand (2017). Some recently developed DNN accelera-
tors, such as Google’s Tensor Processing Unit (TPU) Jouppi et al. (2017), are also focused on matrix
multiplication acceleration (usually with reduced precision).
For BLAS-based CNN inference, reshaping a 4D tensor K and performing SVD is preferred for
low-rank approximation rather than relatively inefficient Tucker decomposition followed by a low-
ering technique. However, a critical problem with SVD (with a lowered matrix) for convolution
layers is that two decomposed matrices by SVD do not present corresponding (decomposed) con-
volution layers, because of intermediate lowering steps. As a result, fine-tuning methods requiring
a structurally modified model for training are not available for convolution layers to be compressed
by SVD. On the other hand, occasional regularization does not alter the model structure for training.
For occasional regularization, SVD can be performed as a way to feed noise into a weight kernel K
for every regularization step. Once training stops at a regularization step, the final weight values can
be decomposed by SVD and used for inference with reduced memory footprint and computations.
In other words, occasional regularization enables SVD-aware training for CNNs.
Skewed Matrix
(n x r), (r x m)
Tiled Skewed Matrix
{(n x r), (r x m/4)} x 4
Figure 17: Skewed matrix and a tiling technique are illustrated on the left side, while the right side
presents distributions of weights after SVD with different tiling schemes (only positive weights are
included).
A.4.2 Tiling-Based SVD for S kewed Weight Matrices
A reshaped kernel matrix K ∈ RT ×(S×d×d) is usually a skewed matrix where row-wise dimension
(n) is smaller than column-wise dimension (m) as shown in Figure 17 (i.e., n m). A range of
available rank r for SVD, then, is constrained by small n and the compression ratio is approximated
to be n/r. If such a skewed matrix is divided into four tiles as shown in Figure 17 and the four tiles
do not share much common chateracteristics, then tiling-based SVD can be a better approximator
and rank r can be further reduced without increasing approximation error. Moreover, fast matrix
multiplication is usually implemented by a tiling technique in hardware to improve the weight reuse
rate Fatahalian et al. (2004). Hence, tiling could be a natural choice not only for high-quality SVD
but also for high-performance hardware operations.
To investigate the impact of tiling on weight distributions after SVD, we tested a (1024 × 1024)
random weight matrix in which elements follow a Gaussian distribution. A weight matrix is divided
by (1 × 1), (16 × 16), or (128 × 128) tiles (then, each tile is a submatrix of (1024 × 1024), (64 × 64),
or (8 × 8) size). Each tile is compressed by SVD to achieve the same overall compression ratio of4×
for all of the three cases. As described in Figure 17 (on the right side), increasing the number of tiles
tends to increase the count of near-zero and large weights (i.e., variance of weight values increases).
Figure 17 can be explained by sampling theory where decreasing the number of random samples (of
19
Under review as a conference paper at ICLR 2020
small tile size) increases the variance of sample mean. In short, tiling affects the variance of weights
after SVD (while the impact of such variance on model accuracy should be empirically studied).
Table 5: Test accuracy(%) of ResNet-32 model using CIFAR-10 dataset while the 9 largest con-
volution layers (T=S=64, d=3) are compressed by SVD using different tiling configurations. For
each tile size, rank r is selected to achieve compression ratio of 2× or 4×. pNR=200 is used for
occasional regularization.
Pre-Trained	Compression Ratio	Size of Each Tile
		64×64	32×32	16×I6	8×8
92.63	2× 4×	93.34 (r=16)^^93.11 (r=8)^^93.01 (r=4)^^93.23 (r=2) 92.94 (r=8)	92.97 (r=4) 93.00 (r=2)	92.81 (r=1)
We applied the tiling technique and SVD to the 9 largest convolution layers of ResNet-32 using the
CIFAR-10 dataset. Weights of selected layers are reshaped into 64 × (64 × 3 × 3) matrices with the
tiling configurations described in Table 5. We perform training with the same learning schedule and
pNR(=200) used in Section 3. Compared to the test accuracy of the pre-trained model (=92.63%),
all of the compressed models in Table 5 achieves higher model accuracy due to the regularization
effect of our compression scheme. Note that for each target compression ratio, the relationship
between tile size and model accuracy is not clear. Hence, various configurations of tile size need to
be explored to enhance model accuracy, even though variation of model accuracy for different tile
size is small.
93
93
+93.03(r=5)A93-11(r=2)
—92.98
92.92
★92.96(r=4)
A92.78(r=8)
92.78
一 *9262--
92.78(r=3
92.78(r=6
Pre-Trained(92.63)
92
92
91
91
■92.43
♦92.20
・92.09
-92.07
'91.95(r=4)
■ Typical Scheme (pNR=1)
T Tucker DecomP.(pNR=200))
. Tiled SVD with (16,32,32)(pNR=200)
★ Tiled SVD with (16,16,16)(pNR=200)
▲ Tiled SVD with (8,8,8)(pNR=200)
一 SVD(PNR=200)
n91.60(r=1)
■91.49
*91.41(r=2)
・90.96
-90.84

11223344
Compression Ratio
Figure 18: Test accuracy of ResNet-32 model using CIFAR-10 with various target compression ratio
and decomposition methods. Except the first small convolution layer, all layers are compressed by
the same compression ratio. Convolution layers can be grouped according to 3 different S values
(=16, 32, or 64). For tiled SVD, three groups (of different S) are tiled in (k1 × k1), (k2 × k2), or
(k3 × k3) tile size. (k1, k2, k3) configuration is described in legends.
A.5 Experimental Results on Low-Rank Approximation for CNNs
In this subsection, we apply low-rank approximation trained by occasional regularization to various
CNN models.
Figure 18 summarizes the test accuracy values of ResNet-32 (with CIFAR-10 dataset) compressed
by various low-rank approximation techniques. Note that tiled SVD and normal SVD are enabled
20
Under review as a conference paper at ICLR 2020
only by occasional regularization, which obviates model structure modification during training. All
configurations in Figure 18 use the same learning rate scheduling and the number of training epochs
as described in Section 3. Results show that tiled SVD yields the best test accuracy and test accu-
racy is not highly sensitive to tile configuration. SVD presents competitive model accuracy for small
compression ratios. As compression ratio increases, however, model accuracy using SVD signifi-
cantly degrades. From Figure 18, tiled SVD associated with occasional regularization is clearly the
best low-rank approximation scheme.
Table 6: Comparison on various low-rank approximation schemes of VGG19 (using CIFAR-10
dataset). To focus on convolution layers only, fully-connected layers are compressed by 8× and
trained by occasional regularization. Then, fully-connected layers are frozen and convolution layers
are compressed (except small layers of S < 128) by Tucker decomposition or tiled SVD.
Comp. Scheme	Parameter	Weight Size	FLOPs	Accuracy(%)
Pre-Trained	-	18.98M	647.87M	92.37
Tucker	Rc=0.6	9.14M (2.08 ×)	319.99M (2.02×)	91.97
Decomposition	Rc=0.5	6.71M (2.83×)	235.74M (2.75×)	91.79
(Typical	Rc=0.45	5.49M (3.45×)	191.77M (3.38×)	91.36
Scheme)	Rc=0.4	4.61M(4.11×)	161.60M (4.01×)	91.11
Tiled	64×64 (r=16)	9.49M (2.00 ×)	316.28M (2.04×)	92.42
SVD	64×64 (r=11)	6.52M (2.91×)	214.25M (3.02×)	92.33
(Occasional	64×64 (r=10)	5.93M (3.20×)	193.85M (3.34×)	92.23
Regularization,	64×64 (r=9)	5.55M (3.41×)	173.44M (3.73×)	92.22
pNR=300)	64×64 (r=8)	4.74M (4.00×)	153.04M (4.33×)	92.07
We compare Tucker decomposition trained by a typical fine-tuning process and tiled SVD trained
by occasional regularization using the VGG19 model2 with CIFAR-10. Since this work mainly dis-
cusses compression on convolution layers, fully-connected layers of VGG19 are compressed and
fixed before compression of convolution layers (refer to Appendix for details on the structure of
VGG19). Except for small layers with S < 128 (that presents small compression ratio as well),
all convolution layers are compressed with the same compression ratio. During 300 epochs to train
convolution layers, learning rate is initially 0.01 and is then halved every 50 epochs. In the case
of tiled SVD, pNR is 300 and tile size is fixed to be 64×64 (recall that the choice of pNR and
tile size do not affect model accuracy significantly). As described in Table 6, while Tucker de-
composition with conventional fine-tuning shows degraded model accuracy through various Rc,
occasional-regularization-assisted tiled SVD presents noticeably higher model accuracy.
2.5-
∙00
2L
SSoaUIUIJI
1.0 Illll
0	20	40	60	80
Epoch
70-
5 O
6 6
IdoI) A0Jn□□v +s①I
55	,	,	,
0	20	40	60	80
Epoch
Figure 19: Comparison of two compression schemes on training loss and (top-1) test accuracy of
ResNet-34 model using ImageNet. pNR=500.
2https://github.com/chengyangfu/pytorch-vgg-cifar10
21
Under review as a conference paper at ICLR 2020
We also test our proposed low-rank approximation training technique with the ResNet-34 model3
He et al. (2016) using the ImageNet dataset Russakovsky et al. (2015). A pre-trained ResNet-34 is
fine-tuned for Tucker decomposition (with conventional training) or tiled SVD (with occasional reg-
ularization) using the learning rate of 0.01 for the first 20 epochs, 0.001 for the next 30 epochs, and
0.0001 for the remaining 30 epochs. Similar to our previous experiments, the same compression ra-
tio is applied to all layers except the layers with S < 128 (such exceptional layers consist of 1.4% of
the entire model). In the case of Tucker decomposition, selected convolution layers are compressed
with Rc = 0.46 to achieve an overall compression of 3.1×. For tiled SVD, lowered matrices are
tiled and each tile of (64×64) size is decomposed with r=10 to match an overall compression of
3.1×. As shown in Figure 19, occasional-regularization-based tiled SVD yields better training loss
and test accuracy compared to Tucker decomposition with typical training. At the end of the training
epoch in Figure 19, tiled SVD and Tucker decomposition achieves 73.00% and 72.31% for top-1
test accuracy, and 91.12% and 90.73% for top-5 test accuracy, while the pre-trained model shows
73.26% (top-1) and 91.24% (top-5).
A.6 Lowering Technique for CNNs
Figure 20 describes a kernel matrix reshaped from a 4D kernel tensor and an input feature map ma-
trix in the form of a Toeplitz matrix. At the cost of redundant memory usage to create a Toeplitz
matrix, lowering enables matrix multiplication which can be efficiently implemented by BLAS li-
braries. A kernal matrix can be decomposed by 2D SVD.
x
Reshaping
to 2D Tensor
4D Tensor
for a Conv. Layer
(T,S,d,d)
Kernel
S x d x d
Figure 20: An example of lowering technique using im2col.
Input Feature Map
*when stride is 1 with no padding
(H+1-d) x (W+1-d)
A.7 Model Descriptions for Low-Rank Approximation Experiments
In this section, we describe model structures and layers selected for low-rank approximation experi-
ments. Small layers close to the input are not compressed because both weight size and compression
rate are too small.
3https://pytorch.org/docs/stable/torchvision/models.html
22
Under review as a conference paper at ICLR 2020
Table 7: Convolution Layers of ResNet-32 for CIFAR-10
# of layers TSd Weight Size Decomposed
662244
113366
3	3	0.4K( 0.1%)	No
16	3	22.5K ( 5.0%)	Yes
16	3	4.5K ( 1.0%)	Yes
32	3	81.0K (18.0%)	Yes
32	3	18.0K ( 4.0%)	Yes
64	3	324.0K (71.9%)	YeS
101919
Total	450.4K(100.0%)
Table 8: Convolution and Fully-connected (FC) Layers of VGG-19 for CIFAR-10
Type	# of layers	T	S	d	Weight Size	Decomposed
	1	64	3	ɪ	0.002M ( 0.01%)	No
	1	64	64	3	0.035M ( 0.18%)	No
	1	128	64	3	0.070M ( 0.36%)	No
Conv.	1	128	128	3	0.141M ( 0.72%)	Yes
	1	256	128	3	0.281M ( 1.44%)	Yes
	3	256	256	3	1.688M ( 8.61%)	Yes
	1	512	256	3	1.125M ( 5.74%)	Yes
	7	512	512	3	15.75M (80.37%)	Yes
FC	2	512	512	-	0.500M ( 2.55%)	Yes
	1	512	10	-	0.005M ( 0.02%)	YeS
Total					19.597M (100.0%)	
Table 9: Convolution Layers of ResNet-34 for ImageNet
# of layers	T	S	d	Weight Size	Decomposed
1	64	3	7	0.01M ( 0.04%)	No
6	64	64	3	0.21M ( 1.05%)	No
1	128	64	3	0.07M ( 0.35%)	No
7	128	128	3	0.98M ( 4.90%)	Yes
1	256	128	3	0.28M ( 1.40%)	Yes
11	256	256	3	6.18M (30.77%)	Yes
1	512	256	3	1.13M ( 5.59%)	Yes
5	512	512	3	11.25M (55.94%)	YeS
Total	20.11M(100.0%)
23