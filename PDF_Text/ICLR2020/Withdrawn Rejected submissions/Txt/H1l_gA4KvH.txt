Under review as a conference paper at ICLR 2020

SURROGATE-BASED  CONSTRAINED  LANGEVIN
SAMPLING  WITH  APPLICATIONS  TO  OPTIMAL
MATERIAL  CONFIGURATION  DESIGN

Anonymous authors

Paper under double-blind review

ABSTRACT

We consider the problem of generating configurations that satisfy physical con-
straints for optimal material nano-pattern design, where multiple (and often con-
flicting) properties need to be simultaneously satisfied.  Consider, for example,
the trade-off between thermal resistance, electrical conductivity, and mechanical
stability needed to design a nano-porous template with optimal thermoelectric
efficiency.  To that end, we leverage the posterior regularization framework and
show that this constraint satisfaction problem can be formulated as sampling from
a Gibbs distribution.  The main challenges come from the black-box nature of
those physical constraints, since they are obtained via solving highly non-linear
PDEs. To overcome those difficulties, we introduce Surrogate-based Constrained
Langevin dynamics for black-box sampling. We explore two surrogate approaches.
The first approach exploits zero-order approximation of gradients in the Langevin
Sampling and we refer to it as Zero-Order Langevin. In practice, this approach can
be prohibitive since we still need to often query the expensive PDE solvers. The
second approach approximates the gradients in the Langevin dynamics with deep
neural networks, allowing us an efficient sampling strategy using the surrogate
model. We prove the convergence of those two approaches when the target distri-
bution is log-concave and smooth. We show the effectiveness of both approaches
in designing optimal nano-porous material configurations, where the goal is to
produce nano-pattern templates with low thermal conductivity and reasonable
mechanical stability.

1    INTRODUCTION

In many real-world design problems, the optimal design needs to simultaneously satisfy multiple
constraints, which can be expensive to estimate. For example, in computational material design, the
goal       is to come up with material configurations, or samples, satisfying a list of physical 
constraints
that are given by black-box numerical Partial Differential Equations (PDE) solvers. Such solvers 
(for
example, the Boltzmann Transport Equation solver) are often complex, expensive to evaluate, and
offer no access to their inner variables or their gradients.

We pose this design-under-constraints problem as sampling from a Gibbs distribution defined on
some compact support. The problem of sampling from a distribution with unknown likelihood that
can only be point-wise evaluated is called black-box sampling (Chen & Schmeiser, 1998; Neal,
2003).  We show in this paper that constrained black-box sampling can be cast as a constrained
Langevin dynamics with gradient-free methods. Zero-order optimization via Gaussian smoothing
was introduced in Nesterov & Spokoiny (2017) and extended to black-box sampling with Langevin
dynamics in Shen et al. (2019). We extend this approach to the constrained setting from a black-box
density with compact support.

However, one shortcoming of this approach is that it is computationally very expensive since it
requires repeatedly querying PDE solvers in order to get an estimate of the gradient.  To alleviate
computational issues, we propose Surrogate Model Based Langevin dynamics, that consists of two
steps: (i) Learning (using training data) an approximation of the gradient of the potential of the 
Gibbs
distribution. We show that learning the gradient, rather than the potential itself, is important 
for the

1


Under review as a conference paper at ICLR 2020

mixing of the Langevin dynamics towards the target Gibbs distribution. We devise several objective
functions, as well as deep neural-network architectures for parameterizing the approximating 
function
class, for learning the gradient of the potential function. (ii) We then use the surrogate gradient 
model
in the constrained Langevin dynamics in lieu of  the black-box potential. Using the surrogate 
enables
more efficient sampling, since it avoids querying the expensive PDE solvers, and obtaining gradients
is as efficient as evaluating the functions themselves using automatic differentiation frameworks 
such
as PyTorch or TensorFlow.

To summarize, our main contributions are as follows:

1.  We cast the problem of generating samples under constraints in the black-box setting as sampling
from a Gibbs distribution.

2.  We introduce Constrained Zero-Order Langevin Monte Carlo,  using projection or proximal
methods, and provide the proof of its convergence to the target Gibbs distribution.

3.  We introduce Surrogate Model Based Projected Langevin Monte Carlo via learning the gradient
of the potential of the Gibbs distribution using deep neural networks or reproducing kernel spaces,
and prove its convergence to the target distribution when used in conjunction with projection
or proximal based methods.  We shed the light on the importance of the approximation of the
gradient of the potential, and we show how to achieve this using Hermite and Taylor learning.

4.  We showcase the usability and effectiveness of the proposed methods for the design of nano-
porous configurations with improved thermoelectric efficiency. The design consists of finding new
configurations with optimized pore locations, such that the resulting configurations have favorable
thermal conductivity (i.e., minimal κ) and desired mechanical stability (von Mises Stress σ     τ ,
where τ is some preset threshold). ¹

2    FROM  CONSTRAINTS  SATISFACTION  TO  SAMPLING  FROM  A  GIBBS
DISTRIBUTION: POSTERIOR  REGULARIZATION

In black-box optimization problems (such as the material design under consideration), the goal is
to find a posterior distribution q of samples satisfying a list of equality and inequality 
constraints:
ψj(x) = yk, j  = 1 . . . Cₑ, and φk(x)      bk, k  = 1 . . . Ci where x     Ω and Ω      Rd  is a 
bounded
domain. We assume a prior distribution p₀ (whose analytical form is known). The main challenge in
black-box optimization is that the functions ψj and φk can be only evaluated point-wise, and neither
do we have functional forms nor access to their gradients. For example, ψ and φ might be obtained
via aggregating some statistics on the solution of a nonlinear PDE given by a complex solver.

To make the problem of learning under constraints tractable, we choose Lagrangian parameters

λj > 0 and obtain the following relaxed objective:

Ce                                                                         Ci

min      KL(q, p₀) + Σ λjEₓ∼q(ψj(x) − yk)² + Σ λkEₓ∼q(φk(x) − bk)₊          (1)

	

The formulation in Eq. 1 is similar in spirit to the posterior regularization framework of Ganchev
et al. (2010); Hu et al. (2018). However, we highlight two differences: (i) our focus is on 
constrained
settings (where Ω is bounded), and (ii) we assume a black-box setting. We first obtain:

Lemma 1 (Constraint Satisfaction as Sampling from a Gibbs Distribution).  The solution to the
distribution learning problem given in Eq. 1 is given by:


π(x) =  exp(−U (x)) 1

Z

x∈Ω

(2)


where  U (x)  =  − log p₀(x)  +  ΣCe

λj(ψj(x)  −  yk)²  +  ΣCi

λk(φk(x)  −  bk)₊  and  Z   =


∫x∈Ω exp (−U (x)) dx.

j=1

k=1

Lemma 1 shows that the constraint satisfaction problem formulated in Eq. 1 amounts to sampling
from a Gibbs distribution defined on a compact support given in Eq.  2.  Sampling from a Gibbs

¹Note that both properties κ and σ for a given configuration are obtained by numerically solving 
highly
non-linear PDEs. The material configuration is defined by the pore locations, the material used, 
and the response
of the material to heat (thermal) or stress (mechanical) flows.

2


Under review as a conference paper at ICLR 2020

distribution (also known as Boltzmann distribution) has a long history using Langevin dynamics. In
the white-box setting when the functions defining the constraints have explicit analytical forms as
well as their gradients, Langevin dynamics for Gibbs distribution sampling defined on a compact
domain Ω and their mixing properties were actively studied in Bubeck et al. (2015); Brosse et al.
(2017). In the next Section, we provide a more detailed review.

Remark 1 (Relation to Bayesian Optimization).  While in Bayesian optimization we are interested in
finding a point that satisfies the constraints, in our setting we are interested in finding a 
distribution
of candidate samples that satisfy (black-box) constraints. See (Suzuki et al., 2019) for more 
details.

Remark 2.  For the rest of the paper, we will assume p₀ to be the uniform distribution on Ω, which
means that its gradients are zero on the support of the domain Ω.  Otherwise, if p₀ is known and
belongs to, for instance, an exponential family or a generative model prior (such as normalizing
flows), we can sample from π using a mixture of black-box sampling on the constraints (ψj, φk) and
white-box sampling on log(p₀).

3    WHITE-BOX  SAMPLING: CONSTRAINED  LANGEVIN  DYNAMICS

We review in this section Langevin dynamics in the unconstrained case (Ω = Rd) and the constrained
setting (Ω       Rd).  Below,        denotes the Euclidean norm unless otherwise specified.  We are
interested in sampling from

1

π(x) =  Z exp(−U (x))1x∈Ω,                                                  (3)

Preliminaries. We give here assumptions, definitions and few preliminary known facts that will be
useful later. Those assumptions are commonly used in Langevin sampling analysis (Dalalyan, 2017;
Bubeck et al., 2015; Brosse et al., 2017; Durmus et al., 2019).

1.  Assumption A: We assume Ω is a convex such that 0     Ω, Ω contains a Euclidean ball of radius 
r,
and Ω is contained in a Euclidean ball of radius R. (For example, Ω might encode box constraints.)

The projection onto Ω, PΩ(x) is defined as follows: for all x ∈ Ω, PΩ(x) =arg min       ǁx − zǁ  .


Let R = sup

x,x'∈Ω

||x − x′|| < ∞.

z∈Ω

2.  Assumption B: We assume that U is convex, β-smooth, and with bounded gradients:

ǁ∇ₓU (x) − ∇yU (y)ǁ ≤ β ǁx − yǁ ,     ∀x, y ∈ Ω (β-smoothness).

ǁ∇U (x)ǁ ≤ L,     ∀x ∈ Ω (Boundedness).

The Total Variation (TV) distance between two measures µ, ν is defined as follows:  TV (µ, ν) =
supA |µ(A) − ν(A)|. Pinsker Inequality relates KL divergence to TV: TV (µ, ν) ≤      2KL(µ, ν).

Unconstrained Langevin Dynamics. In the unconstrained case, the goal is to sample from a Gibbs
distribution π(x) = exp(   U (x))/Z that has unbounded support. This sampling can be done via the
Langevin Monte Carlo (LMC) algorithm, which is given by the following iteration:

Xk₊₁ = Xk − η∇ₓU (Xk) +     2ληξk,     k = 0 . . . K − 1 (LMC),                    (4)

where ξk ∼ N (0, Id), η is the learning rate, and λ > 0 is a variance term.

Constrained Langevin Dynamics.  In the constrained case, the goal is to sample from π(x)  =
exp(−U (x))/Z1x∈Ω,. We discuss two variants:

Projected Langevin Dynamics. Similar to projected gradient descent, Bubeck et al. (2015) introduced
Projected Langevin Monte Carlo (PLMC) and proved its mixing propreties towards the stationary
distribution π. PLMC is given by the following iteration :

Xk₊₁ = PΩ .Xk − η∇ₓU (Xk) +     2ληξkΣ ,     k = 0 . . . K − 1  (PLMC),             (5)

In essence, PLMC consists of a single iteration of LMC, followed by a projection on the set Ω using
the operator PΩ.

3


Under review as a conference paper at ICLR 2020

Proximal Langevin Dynamics.  Similar to proximal methods in constrained optimization, Brosse
et al. (2017) introduced Proximal LMC (ProxLMC) that uses the iteration:


Xk+1

= .1 −  η Σ X

− η∇

η

ₓU (Xk) + γ PΩ(Xk) +

√2ληξk, k = 0 . . . K − 1,  (ProxLMC)  (6)

where η is the step size and γ is a regularization parameter.  In essence, ProxLMC (Brosse et al.,
2017) performs an ordinary LMC on U γ(x) = U (x) + iγ (x), where iγ (x) is the proximal operator:

Ω                             Ω

iγ (x) = inf iΩ(x) + (2γ)−¹ ǁx − yǁ2  = (2γ)−¹ ǁx − PΩ(x)ǁ2 ,

where iΩ(x) = 0 for x ∈ Ω and iΩ(x) = ∞ for x ∈/ Ω. Therefore, the update in Eq. 6 is a regular
Langevin update (as in Eq. 4) with potential gradient ∇ₓU γ(x) = ∇ₓU (x) + γ−¹(x − PΩ(x)).

We denote by µPLMC and µPʳᵒˣLMC the distributions of XK obtained by iterating Eq. 5 and Eq. 6

K                   K

respectively.  Under Assumptions A and B, both these distributions converge to the target Gibbs
distribution π in the total variation distance.  In particular, Bubeck et al. (2015) showed that for
η = Θ˜(R²/K), we obtain:

TV (µPLMC, π) ≤ ε for K = Ω˜(ε−¹²d¹²).                                         (7)
Likewise, Brosse et al. (2017) showed that for 0 < η ≤ γ(1 + β²γ²)−¹, we obtain:

TV (µPʳᵒˣLMC, π) ≤ ε for K = Ω˜(ε−⁶d⁵),                                         (8)
where the notation αn = Ω˜(βn) means that there exists c ∈ R, C > 0 such that αn ≥ Cβn logᶜ(βn).

4    CONSTRAINED  LANGEVIN  DYNAMICS  IN  THE  BLACK-BOX  SETTING

We now introduce our variants of constrained LMC for the black-box setting where explicit potential
gradients are unavailable. We explore in this paper two strategies for approximating the gradient of
U     in the black-box setting. In the first strategy, we borrow ideas from derivative-free 
optimization
(in particular, evolutionary search).  In the second strategy we learn a surrogate deep model that
approximates the gradient of the potential. Below, let G : Ω → Rd be a vector valued function that
approximates the gradient of the potential, ∇ₓU . We make:

Assumption C. The surrogate gradient G satisfies E ǁG(Yk)ǁ2  < ∞, ∀k.

Surrogate Projected Langevin Dynamics.  Given Y₀, the Surrogate Projected LMC (S-PLMC)
replaces the potential gradient ∇ₓU in Eq. 5 with the surrogate gradient G:

Yk₊₁ = PΩ   Yk − ηG(Yk) +     2ληξk   , k = 0 . . . K − 1  (S-PLMC)                  (9)

Surrogate Proximal Langevin Dynamics. Similarly, the Surrogate Proximal LMC (S-ProxLMC)
replaces the unknown potential gradient ∇ₓU in Eq. 6 with the gradient surrogate G:

Y       = .1 −  η Σ Y   − ηG(Y  ) + η P  (Y  ) + √2ληξ  , k = 0 . . . K − 1  (S-ProxLMC)   (10)

We now present our main theorems on the approximation properties of surrogate LMC (S-PLMC,
and S-ProxLMC). We do so by bounding the total variation distance between the trajectories of the
surrogate Langevin dynamics (S-PLMC, and S-ProxLMC) and the true LMC dynamics (PLMC and
ProxLMC). Theorem 1 is an application of techniques in Stochastic Differential Equations (SDE)
introduced in Dalalyan & Tsybakov (2012) and is mainly based on a variant of Grisanov’s Theorem
for change of measures (Lipster & Shiryaev, 2001) and Pinsker’s Inequality that bounds total 
variation
in terms of Kullback-Leibler divergence.

Theorem 1 (S-PLMC and S-ProxLMC Mixing Properties).  Under Assumption C, we have:

1.  S-PLMC Convergence.  Let µPLMC be the distribution of the random variable XK obtained by
iterating PLMC Eq.  5, and µS⁻PLMC be the distribution of the random variable YK obtained by
iteration S-PLMC given in Eq. 9. We have:

.    .K−1                                                                                       Σ 1

				

4


Under review as a conference paper at ICLR 2020

2.  S-ProxLMC Convergence. Let µPʳᵒˣLMC be the distribution of the random variable XK obtained
by iterating ProxLMC Eq. 6, and µS⁻PʳᵒˣLMC be the distribution of the random variable YK obtained
by iterating S-ProxLMC given in Eq. 10. We have:

.      .K−1                                                                Σ 1

				

From Theorem 1, we see that it suffices to approximate the potential gradient    ₓU (X) (and not the
potential U (X)) in order to guarantee convergence of surrogate-based Langevin sampling. Using the
triangle inequality, and combining Theorem 1 and bounds in Eqs 7 and 8 we obtain:

Theorem 2.  (Convergence of Surrogate Constrained LMC to the Gibbs distribution.) Under assump-
tions A,B and C we have:

1.  Assume in S-PLMC that there exists δ > 0 such that E  G(Yk)        ₓU (Yk)  ²      δ,   k     
0. Set
λ = 1, and η = Θ˜(min(R²/K, α/K²)) where α = 1/(δ + β²R²) . Then for K = Ω˜(ε−¹²d¹²),
we have:

TV (µS⁻PLMC, π) ≤ ε.

2.  Assume in S-ProxLMC that there exists δ > 0 such that E  G(Xk)        ₓU (Xk)  ²      δ,   k    
 0.
Set λ = 1, and η = min(γ(1 + β²γ²)−¹,   ¹ 2 ). Then for K = Ω˜(ε−⁶d⁵) we have:

TV (µS⁻PʳᵒˣLMC, π) ≤ ε.

5    ZERO-ORDER  CONSTRAINED  LANGEVIN  DYNAMICS

In  zero-order  optimization  (Nesterov  &  Spokoiny,  2017;  Duchi  et  al.,  2015;  Ghadimi  &  
Lan,
2013; Shen et al., 2019), one considers the Gaussian smoothed potential Uν defined as Uν (x) =

Eg∼N ₍₀,I ₎U (x + νg), and its gradient is given by ∇ₓUν (x) = Eg U⁽ˣ⁺νᵍ⁾−U⁽ˣ⁾ g. The following

d                                                                                                   
                                                                      ν

is a Monte Carlo estimate of ∇ₓUν (x):  Σ .                              Σ

where g₁, . . . gn are i.i.d. standard normal vectors.

Zero-Order sampling from log-concave densities was recently studied in Shen et al. (2019). We extend
it here to the constrained sampling case of log-concave densities with compact support. We define
Constrained Zero-Order Projected LMC (Z-PLMC) and Zero-Order Proximal LMC (Z-ProxLMC)
by   setting G(x) = Gˆ nU(x) in Eq. 9 and Eq. 10 respectively.

Lemma 2 (Zero-Order Gradient Approximation(Nesterov & Spokoiny, 2017; Shen et al., 2019)).

Under Assumption B, we have for all x ∈ Ω:


Eg1 ,...,gn  ¨Gˆ

nU (x) − ∇ₓ

U (x)¨2  ≤ .βν(d + 2)³/²

1

+ (d + 1) 2 L

Σ2 /n               (14)

Thanks to Lemma 2 that ensures uniform approximation of gradients in expectation, we can apply
Theorem 2 and get the following corollary for Z-PLMC and Z-ProxLMC:

Corollary 1 (Zero-order Constrained Langevin approximates the Gibbs distribution).  Under As-


sumptions A and B, let δ ∈ [0, 1], for n ≥    βν(d + 2)³/²

bounds in expectation:

1

+ (d + 1) 2 L

Σ2 /δ, we have the following

1.  Set λ = 1, and η = Θ˜(min(R²/K, α/K²)) where α = 1/(δ + β²R²) . For K  = Ω˜(ε−¹²d¹²),
we have:


Eg1 ,...gn

TV (µZ⁻PLMC, π) ≤ ε.                                              (15)

2.  Set λ = 1, and η = min(γ(1 + β²γ²)−¹,   ¹ 2 ). For K = Ω˜(ε−⁶d⁵) we have:


Eg1 ,...gn

TV (µZ⁻PʳᵒˣLMC, π) ≤ ε.                                            (16)

Remark 3.  For simplicity, we state the above bound in terms of expectations over the randomness
in estimating the gradients.  It is possible to get finite-sample bounds using the Vector Bernstein
concentration inequality, coupled with covering number estimates of Ω but omit them due to space.

5


Under review as a conference paper at ICLR 2020

6    SURROGATE  MODEL  BASED  CONSTRAINED  LANGEVIN  DYNAMICS

Despite its theoretical guarantees, zero-order constrained Langevin (Z-PLMC and Z-ProxLMC) has
a prohibitive computation cost as it needs O(nK) black-box queries (in our case, invocations of a
nonlinear PDE solver). To alleviate this issue, we introduce in this Section a neural surrogate 
model
as an alternative to the gradient of the true potential.

6.1    HERMITE LEARNING OF GRADIENTS: JACOBIAN MATCHING OF ZERO-ODER ESTIMATES

From Theorem 2, we saw that in order to guarantee the convergence of constrained Langevin dynamics,
we need a good estimate of the gradient of the potential of the Gibbs distribution.  Recall that the
potential given in Lemma 1 depends on ψj and φk, which are scalar outputs of computationally heavy
PDE solvers in our material design problem.  To avoid this, we propose to train surrogate neural
network models approximating each PDE output and their gradients. Concretely, suppose we are
given a training set S for a PDE solver for the property ψ (dropping the index j for simplicity):

S = {(xi, yi = ψ(xi), y˜i = Gˆnψ(xi)), xi ∼ ρΩi.i.d., i = 1, . . . , N },

where ρΩ is the training distribution and Gˆnψ(.) is the zero-order estimate of the gradient of ψ 
given
in Eq. 13. We propose to learn a surrogate model belonging to a function class Hθ, fˆθ      Hθ, that
regresses the value of ψ and matches the zero-order gradient estimates as follows:

N

 1  Σ                                                          2

min             {(y  − f  (x ))   + ǁ∇  f  (x ) − y˜ ǁ  } (Z-Hermite Learning)           (17)

The problem in Eq. 17 was introduced and analyzed in Shi et al. (2010) where Hθ is a ball in a
Reproducing Kernel Hilbert Space (RKHS). Following Shi et al. (2010), we refer to this type of
learning as Hermite Learning. In the deep learning community, this type of learning is called 
Jacobian
matching and was introduced in Srinivas & Fleuret (2018); Czarnecki et al. (2017) where Hθ is a
deep neural network parameterized with weights θ. When fθ is a deep network, we can optimize this
objective efficiently using common deep learning frameworks (PyTorch, TensorFlow).

(Shi et al., 2010) have shown that when Hθ is an RKHS ball and when y˜i =     ₓψ(xi) are exact
gradients, for a sufficiently large training set with N  = O(1/ϵ¹/⁽²ʳζ⁾) (where r, ζ are exponents 
in

[0, 1] that depend on the regularity of the function ψ). Under the assumption that ψ ∈ Hθ we have:

∫Ω ǁ∇ₓfθ(x) − ∇ₓψ(x)ǁ   ρΩ(x)dx ≤ ϵ. Since we are using inexact zero-order gradients, we will

incur an additional numerical error that is also bounded as shown in Lemma 2.

6.2    TAYLOR LEARNING OF GRADIENTS

While Jacobian matching of zero-order gradients is a sound approach,  it remains expensive to
construct the dataset, as we need for each point to have 2n + 1 queries of the PDE solver. We 
exploit
in this section the Taylor learning framework of gradients that was introduced in Mukherjee & Zhou
(2006); Mukherjee & Wu (2006), and Wu et al. (2010).  In a nutshell, Mukherjee & Zhou (2006)
suggests to learn a surrogate potential fθ and gradient GΛ that are consistent with the first-order
taylor expansion. Given a training set S =   (xi, yi = ψ(xi)), x     ρΩ, i = 1 . . . N  , Wu et al. 
(2010)
suggest the following objective:


  1   Σ

min                      wσ (y

		

− f  (x  ) + ⟨G  (x ), x   − x ⟩)  (Taylor-2),            (18)

2


where wσ

=  exp . −ǁxi−xj ǁ² Σ, H

is an RKHS ball of scalar valued functions, and H ᵈ is an

RKHS ball of vector valued functions.

Under mild assumptions,  Mukherjee & Zhou (2006) shows that we have for N  =  O(1/ϵᵈ/²):

∫Ω ǁGΛ(x) − ∇ₓψ(x)ǁ   ρΩ(x)dx ≤ ϵ. We simplify the problem in Eq. 18 and propose the following

two objective functions and leverage the deep learning toolkit to parameterize the surrogate fθ:

  1   Σ

min                wσ (y  − f  (x  ) + ⟨∇  f  (x ), x   − x ⟩)²(Taylor-1),                 (19)

	

6


Under review as a conference paper at ICLR 2020


min

 1  Σ{(y −f  (x ))²+   λ

Σ wσ (y −y  +⟨∇

f  (x ), x

− x ⟩)²}, (Taylor-Reg) .  (20)

The objective in Eq. 19 uses a single surrogate to parameterize the potential and its gradient. The
objective in Eq. 20 is similar in spirit to the Jacobian matching formulation in the sense that it 
adds a
regularizer on the gradient of the surrogate to be consistent with the first-order Taylor expansion 
in
local neighborhoods. The advantage of the Taylor learning approach is that we do not need to perform
zero-order estimation of gradients to construct the training set and we rely instead on first-order
approximation in local neighborhood.

6.3    SURROGATE MODEL CONSTRAINED LMC

Consider the surrogate model fθ obtained via Hermite Learning (Eq. 17) or via Taylor learning
(Eqs 18, 19, 20). We are now ready to define the surrogate model LMC by replacing G(x) =    ₓfθ(x)
in the constrained Langevin dynamics in Eqs 9 and 10.

Both Hermite and Taylor learning come with theoretical guarantees when the approximation func-
tion space is an RKHS under some mild assumptions on the training distribution and the regu-
larity of the target function ψ.   In Hermite learning (Theorem 2 in Shi et al. (2010)) we have:


Ex∼pΩ

ǁ∇ₓfθ(x) − ∇ₓψ(x)ǁ2   ≤  ϵ for sufficiently large training set N  =  O(1/ϵ¹/⁽²ζʳ⁾) (where

exponents ζ, r ∈ [0, 1] depend on regularity of ψ). In Taylor Learning with the objective function

given in Eq. 18 (Proposition 7 in Wu et al. (2010) we have: Eₓ∼ρΩ  ǁGΛ(x) − ∇ₓψ(x)ǁ    ≤  ϵ for

N  = O(1/ϵᵈ/²). In order to apply Theorem 2 we need this gradient approximation error to hold in
expectation on all intermediate distributions in the Langevin sampling. Hence, we need the following
extra-assumption on the training distribution pΩ:

Assumption  D:  Assume  we  have  a  learned  surrogate  G  on  training  distribution  ρΩ  such  
that


Ex∼ρΩ

ǁG(x) − ∇ₓU (x)ǁ    ≤  ϵ.  Assume ρΩ(x)  >  0, ∀x  ∈  Ω and that it is a dominating mea-

sure of Langevin (PLMC, S-PLMC, Prox-LMC, S-ProxLMC ) intermediate distributions µk, i.e.
there exists C > 0 such that:

µk(x) ≤ CρΩ(x), ∀x ∈ Ω, ∀k = 0, . . . K − 1.

Under Assumption D, it follows immediately that


E ǁG(X  ) − ∇U (X  )ǁ2  = ∫

₂ µk(x)

ǁG(x) − ∇U (xǁ               ρ

(x) ≤ Cϵ

and hence we can apply Theorem 2 for δ = Cϵ, and we obtain ε-approximation of the target Gibbs
distribution in terms of total variation distance.

Remark 4.  Assumption D on the ϵ-approximation of the gradient can be achieved for a large enough
training set N, when we use Hermite learning in RKHS under mild assumptions and in Taylor
learning. The assumption on the dominance of the training distribution is natural and means that we
need   a large training set that accounts to what we may encounter in Surrogate LMC iterations.

In what follows we refer to surrogate constrained LMC, as x-PLMC or x-ProxLMC where x is one
of four suffixes ({Z-Hermite, Taylor-2, Taylor-1, Taylor-Reg}).

7    RELATED  WORK

Zero-Order Methods. Zero-order optimization with Gaussian smoothing was studied in Nesterov &
Spokoiny (2017) and Duchi et al. (2015) in the convex setting. Non-convex zero order optimization
was also addressed in Ghadimi & Lan (2013). The closest to our work is the zero-order Langevin
Shen et al. (2019) introduced recently for black-box sampling from log concave density. The main
difference in our setting, is that the density has a compact support and hence the need to appeal to
projected LMC (Bubeck et al., 2015) and Proximal LMC (Brosse et al., 2017). It is worth nothing
that Hsieh et al. (2018) introduced recently mirror Langevin sampling that can also be leveraged in
our framework.

7


Under review as a conference paper at ICLR 2020

Gradients and Score functions Estimators. We used the approach of gradient distillation (Srinivas
& Fleuret, 2018) and learning gradients of (Wu et al., 2010), since they are convenient for 
training on
different constraints and they come with theoretical guarantees. However, other approaches can be
also leveraged such as the score matching approach for learning the gradient of the log likelihood
(Hyvärinen, 2005) and other variants appealing to dual embeddings (Dai et al., 2018). Estimating
gradients can be also performed using Stein’s method as in (Li & Turner, 2017), or via maintaining a
surrogate of the gradient as in Stein descent without gradient (Han & Liu, 2018).

Optimization approaches. Due to space limitation, we restrict the discussion to the optimization
methods that are most commonly and recently used for optimal material (or molecule) design.  A
popular approach to deal with optimization of expensive black-box functions is Bayesian Optimization
(BO) (Mockus, 1994; Jones et al., 1998; Frazier, 2018).  The standard BO protocol is comprised
of estimating the black-box function from data through a probabilistic surrogate model, usually a
Gaussian process, and maximizing an acquisition function to decide where to sample next. BO is
often performed over a latent space, as in (Gómez-Bombarelli et al., 2018). Hernández-Lobato et al.
(2016) proposed an information-theoretic framework for extending BO to address optimization under
black-box constraints, which is close to current problem scenario. Genetic Algorithms (GA), a class
of meta-heuristic based evolutionary optimization techniques, is another widely used approach for
generating (material) samples with desired property (Jennings et al., 2019) and has been also used
for handling optimization under constraints (Chehouri et al., 2016). However, GA typically requires
a large number of function evaluations, can get stuck in local optima, and does not scale well with
complexity.  Finally, Zhou et al. (2019) has used deep reinforcement learning technique of Deep
Q-networks to optimize molecules under a specific constraint using desired properties as rewards.
The advantage of our framework is that we obtain a distribution of optimal configurations (as 
opposed
to a single optimized sample) that does not rely on training on a specific pre-existing dataset and 
can
be further screened and tested for their optimality for the task at hand.

8    EXPERIMENTS

In this section, we demonstrate the usability of our black-blox Langevin sampling approach for
the design of nano-porous configurations. We first show the performance of the surrogate models
in learning the potential function, showcasing the results using four different variants:  standard
regression, Taylor regularization, Taylor-1 and Taylor-2. We then show how well the surrogate-based
Langevin MC generates new samples under the thermal and mechanical constraints. We compare
the sample quality on multiple criteria between the surrogate and zero-order approaches with either
projection or proximal update step.

Data.  We want to learn surrogate models to approximate the gradient of the potential from data.
To this end, we generate a dataset of 50K nano-porous structures, each of size 100nm     100nm.
One such example is displayed in Fig. 1.  Number of pores is fixed to 10 in this study and each
pore is a square with a side length of 17.32nm. We sample the pore centers uniformly over the unit
square and construct the corresponding structure after re-scaling them appropriately. Then, using 
the
solvers OpenBTE (Romano & Grossman, 2015) and Summit (    MIT Development Group, 2018), we
obtain for each structure x a pair of values: thermal conductivity κ and von Mises stress σ. 
Finally,

we collect two datasets: {(xi, κi)}N     and {(xi, σi)}N     with the same inputs xi’s and N  = 50K

samples. More details are given in Appendices B and C on the PDEs and their corresponding solvers.

Features.  The pore locations are the natural input features to the surrogate models.  Apart from
the coordinates,  we also derive some other features based on physical intuitions.  For example,
the distances between pores and the alignment along axes are informative of thermal conductivity
(Romano & Grossman, 2016). As such, we compute pore-pore distances along each coordinate axis
and  add them as additional features.

Surrogate gradient methods. We use feed-forward neural networks to model the surrogates since
obtaining gradients for such networks is efficient thanks to automatic differentiation frameworks. 
We
use networks comprised of 4 hidden layers with sizes 128, 72, 64, 32 and apply the same architecture
to approximate the gradients for κ and σ separately. The hidden layers use ReLU activations whereas
sigmoid was used at the output layer (after the target output is properly normalized). For the 
Taylor-2
variant (in Eq. 18), we have an additional output vector of the same size as the input for the 
gradient
prediction. The networks are trained on the corresponding objective functions set up earlier by an

8


Under review as a conference paper at ICLR 2020

κ = 0.0871, σ = 0.4826             κ = 0.0802, σ = 0.6681             κ = 0.0732, σ = 0.4401

Figure 1:  Example of nano-porous structures with corresponding heat flux shown using a color
gradient. Yellow regions indicate high phonons flux. The thermal conductivity κ and von Mises stress
σ are reported below each structure. The arrows show the moving directions of the pores. (Left) A
random sample. (Middle) The sample obtained by Taylor-Reg PMLC starting from the left structure
with κ constraint. (Right) The sample obtained by Taylor-Reg PMLC with both κ and σ constraints.

Adam optimizer with learning rate 10−⁴  and decay 1.0.  We fine-tune the networks with simple
grid-search and select the best models for comparison. Due to the space constraint, we present the
results in Appendix A and emphasize that Z-Hermite is not included in the entire comparison but in 
a

small experiment performed with a more lightweight OpenBTE version.

Incorporating constraints and comparison metrics. We demonstrate the usability of our proposed
black-box Langevin sampling for the design of nano-configurations under thermal conductivity and
mechanical stability constraints that are provided by the corresponding PDE solvers. To compare
sampling outcomes, we use the following metrics. We report the minimum value of κ and Monte
Carlo estimates for both κ and σ to compare the samples generated by different sampling methods
and surrogate models. The Monte Carlo estimates are computed on 20 samples.

Single constraint.  Our first task is to design nano-configurations under the thermal conductivity
constraint where we want κ as low as possible in order to achieve high thermo-electric efficiency.
From  the  posterior  regularization  formulation  Section  2,  we  pose  the  constraint  
satisfaction  as
sampling from the following Gibbs distribution:

exp(−λκ(x)²)


π(x) = p₀(x)

Z           1x∈[0,1]20                                                                  (21)

where p₀(x) is the uniform distribution over the unit square, which is equivalent to the Poisson
process of 10 pores on the square, and κ(x) is the thermal conductivity we want to minimize. 
Starting
from 20 samples initialized from p₀(x), we run our proposed black-box Langevin MCs and obtain
20 new realizations from the target distribution π(x).  We use four different surrogates (including
simple regression, Taylor-Reg, Taylor-1 and zero-order) and each surrogate with either projection or
proximal update. We show the summary statistics of these samples in Table 1. The regression-PMLC
in    the first row and regression-ProxLMC in the fifth represent the sampling where the surrogate
model are fitted on solely the mean square error objective.  In all methods, we set λ  =  100, the
step size η  =  1e   3 and the exponential decay rate 0.8.  Since keeping track of the true κ value
is expensive, we stop after K  = 10 iterations.  We first observe that the regression-based method
(PLMC, ProxLMC) is less effective than the others simply because they do not have an implicit
objective for approximating the gradients.  Taylor-Reg and Taylor-1 demonstrate its effectiveness
in approximating the gradient and are able to achieve lower thermal conductivity.  In particular,
Taylor-1-ProxLMC and Zero-order-PLMC perform in the similar range in terms of the minimum
achieved,          but the learned surrogate offers 17x speed up (per sample) over zero order 
methods. Due
to      the space limit, we do not report Taylor-2 results in Table 1, and note that Taylor-2 works 
in the
similar vein as Taylor-1.

9


Under review as a conference paper at ICLR 2020


Model

Regression-PLMC
Taylor-Reg-PLMC
Taylor-1-PLMC
Zero-order-PLMC

Regression-ProxLMC
Taylor-Reg-ProxLMC
Taylor-1-ProxLMC
Zero-order-ProxLMC

Min κ

0.0757

0.0638

0.0637

0.0510

0.0646

0.0712

0.0575

0.0719

Mean κ

0.1206 ± 0.0480

0.1196 ± 0.0495

0.1278 ± 0.0610

0.1093 ± 0.0271

0.1282 ± 0.0531

0.1205 ± 0.0455

0.1297 ± 0.0543

0.1112 ± 0.0363

Per-sam. time (s)
1055

899

852

14967

1107

899

874

14938

Table 1: Statistics of 20 new samples obtained by running different surrogate-based Langevin MCs on
π with the thermal conductivity constraint (Eq. 21). We show the min and mean over the generated
samples and the per-sample time. Initialized samples have min κ = 0.0619 and mean σ = 0.1268.

Multiple constraints. Achieving the minimal thermal conductivity can be fulfilled without much
difficulty (e.g. structures with all pores aligned along the vertical axis), but such structures 
are often
mechanically unstable.  In the next step, we study whether adding more (conflicting) constraints
helps us design better nano-configurations.  Hence, we consider both thermal conductivity κ and
mechanical stability provided via von Mises stress σ. We want a sample x that minimizes κ(x) to
achieve high thermo-electric efficiency while maintaining σ(x) less than some threshold (which we
explain below). Like the single constraint case, we pose this as sampling from the following Gibbs
distribution:


π(x) = p₀(x)

exp(   λ  κ(x)²     λ  [σ(x)     τ ]   )

Z                           1x∈[0,1]20 ,                         (22)

where p₀(x) is the same as above, σ(x) is the von Mises stress and τ is a threshold on the maximum
value of σ. With this framework, we relax the inequality constraint to the Hinge loss term on von
Mises stress. The results are summarized in Table 2. Note that all the surrogate Langevin MCs are
initialized from the same set of 20 samples as above. In this experiment, we set τ  = 0.5, λ₁ = 100,
λ₂     = 10 the step size η = 1e   3 and the exponential decay rate 0.8. Comparing with Table 1, one
can see that not only better κ be achieved but also the σ can be reduced simultaneously.  These
results suggest that our approach can effectively sample new configurations under multiple competing
constraints. Examples of new nano-configurations are show in Fig. 1 and Appendix A Fig. 5, 6 and 7.

Model                  Min κ               Mean κ                       Mean σ                 
Per-sam. time (s)
Taylor-Reg-PLMC        0.0613       0.1256 ± 0.0538         0.6590 ± 0.2261                    952

Taylor-1-PLMC          0.0611       0.1278 ± 0.0610         0.6380 ± 0.1598                    852

Zero-order-PLMC        0.0471       0.1148 ± 0.0475         0.6511 ± 0.1916                  15677

Taylor-Reg-ProxLMC     0.0666       0.1195 ± 0.0534         0.6402 ± 0.1464                    856

Taylor-1-ProxLMC       0.0548       0.1298 ± 0.0610         0.6156 ± 0.1463                    972

Zero-order-ProxLMC     0.0354     0.1080 ± 0.0384     0.6029 ± 0.1376                15080

Table 2:  Summary statistics of 20 new samples obtained by our sampling method on π(x) with
κ and σ constraints Eq.  22.  The starting samples are reused from the single constraint case (min
κ = 0.0759, mean κ = 0.1268, and mean σ = 0.8181; note that σ can be as high as 16.)

9    CONCLUSION

In this paper we introduced Surrogate-Based Constrained Langevin Sampling for black-box sampling
from a Gibbs distribution defined on a compact support. We studied two approaches for defining the
surrogate: the first through zero-order methods and the second via learning gradient approximations
using deep neural networks.  We showed the proofs of convergence of the two approaches in the
log-concave and smooth case. While zero-order Langevin had prohibitive computational cost, learned
surrogate model Langevin enjoy a good tradeoff of lightweight computation and approximation power.
We applied our black-box sampling scheme to the problem of nano-material configuration design,
where the black box constraints are given by expensive PDE solvers, and showed the efficiency
and the promise of our method in finding optimal configurations.  Among different approaches
for  approximating  the  gradient,  the  zero-order  ones  (PLMC,  ProxLMC)  show  overall  superior
performance, at a prohibitive computational cost. We established that the deep the surrogate 
(Taylor-1

10


Under review as a conference paper at ICLR 2020

ProxLMC) is a viable alternative to zero-order methods, achieving reasonable performance, and
offering 15x speedup over zero-order methods.

REFERENCES

Nicolas Brosse, Alain Durmus, Éric Moulines, and Marcelo Pereyra. Sampling from a log-concave
distribution with compact support with proximal langevin monte carlo. In Conference on Learning
Theory, pp. 319–342, 2017.

Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected langevin monte
carlo. In Advances in Neural Information Processing Systems, pp. 1243–1251, 2015.

Adam Chehouri, Rafic Younes, Jean Perron, and Adrian Ilinca. A constraint-handling technique for
genetic algorithms using a violation factor. arXiv preprint arXiv:1610.00976, 2016.

Gang  Chen.    Nanoscale  energy  transport  and  conversion:   a  parallel  treatment  of  
electrons,
molecules, phonons, and photons. Oxford University Press, USA, 2005. URL https://www.
amazon.com/Nanoscale-Energy-Transport-Conversion-MIT-Pappalardo/
dp/019515942X.

Ming-Hui Chen and Bruce Schmeiser. Toward black-box sampling: A random-direction interior-point
markov chain approach. Journal of Computational and Graphical Statistics, 7(1):1–22, 1998.

Wojciech M. Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, and Razvan Pascanu.
Sobolev training for neural networks. In Advances in Neural Information Processing Systems 30,
pp. 4278–4287. 2017.

Bo Dai, Hanjun Dai, Arthur Gretton, Le Song, Dale Schuurmans, and Niao He. Kernel exponential
family estimation via doubly dual embedding. arXiv preprint arXiv:1811.02228, 2018.

A. S. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation and langevin monte-
carlo. J. Comput. Syst. Sci., 2012.

Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities.   Journal of the Royal Statistical Society:  Series B (Statistical Methodology), 79(3):
651–676, 2017.

John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono.  Optimal rates for
zero-order convex optimization: The power of two function evaluations. IEEE Transactions on
Information Theory, 61(5):2788–2806, 2015.

Alain Durmus, Eric Moulines, et al. High-dimensional bayesian inference via the unadjusted langevin
algorithm. Bernoulli, 25(4A):2854–2882, 2019.

Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.

Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar, et al.  Posterior regularization for structured
latent variable models. Journal of Machine Learning Research, 11(Jul):2001–2049, 2010.

Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.

Rafael  Gómez-Bombarelli,  Jennifer  N  Wei,  David  Duvenaud,  José  Miguel  Hernández-Lobato,
Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan    P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous
representation of molecules. ACS central science, 4(2):268–276, 2018.

Jun Han and Qiang Liu. Stein variational gradient descent without gradient. In Proceedings of the
35th International Conference on Machine Learning, pp. 1900–1908, 2018.

José Miguel Hernández-Lobato, Michael A. Gelbart, Ryan P. Adams, Matthew W. Hoffman, and
Zoubin Ghahramani. A general framework for constrained bayesian optimization using information-
based search. Journal of Machine Learning Research, 17(160):1–53, 2016. URL http://jmlr.
org/papers/v17/15-616.html.

11


Under review as a conference paper at ICLR 2020

Ya-Ping Hsieh, Ali Kavis, Paul Rolland,  and Volkan Cevher.   Mirrored langevin dynamics.   In

Advances in Neural Information Processing Systems, pp. 2878–2887, 2018.

Zhiting Hu, Zichao Yang, Ruslan R Salakhutdinov, LIANHUI Qin, Xiaodan Liang, Haoye Dong,
and Eric P Xing. Deep generative models with learnable knowledge constraints. In Advances in
Neural Information Processing Systems, pp. 10501–10512, 2018.

Aapo Hyvärinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn.
Res., 2005.

Paul C Jennings,  Steen Lysgaard,  Jens Strabo Hummelshøj,  Tejs Vegge,  and Thomas Bligaard.
Genetic algorithms for computational materials discovery accelerated by machine learning. npj
Computational Materials, 5(1):46, 2019.

Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive
black-box functions. Journal of Global optimization, 13(4):455–492, 1998.

Yingzhen  Li  and  Richard  E  Turner.   Gradient  estimators  for  implicit  models.   arXiv  
preprint
arXiv:1705.07107, 2017.

Robert Lipster and Albert Shiryaev. Statistics of random processes. Springer, 2001.

Jonas Mockus.  Application of bayesian approach to numerical methods of global and stochastic
optimization. Journal of Global Optimization, 4(4):347–365, 1994.

Sayan Mukherjee and Qiang Wu. Estimation of gradients and coordinate covariation in classification.

J. Mach. Learn. Res., 2006.

Sayan Mukherjee and Ding-Xuan Zhou.  Learning coordinate covariances via gradients.  J. Mach.
Learn. Res., 2006.

Radford M Neal. Slice sampling. The Annals of Statistics, 31, 2003.

Yurii Nesterov and Vladimir Spokoiny.  Random gradient-free minimization of convex functions.

Found. Comput. Math., 2017.

Giuseppe Romano and Aldo Di Carlo. Multiscale electrothermal modeling of nanostructured devices.
IEEE Trans. Nanotechnol., 10(6):1285–1292, 2011. URL http://ieeexplore.ieee.org/
document/5740609/?arnumber=5740609&tag=1.

Giuseppe  Romano  and  Jeffrey  C  Grossman.      Heat  conduction  in  nanostructured  materi-
als   predicted   by   phonon   bulk   mean   free   path   distribution.      J.   Heat   Transf., 
  137(7):
071302,   2015.      URL  https://heattransfer.asmedigitalcollection.asme.
org/article.aspx?articleid=2119334.

Giuseppe Romano and Jeffrey C Grossman. Tuning thermal transport in disordered porous materials
via phonon bottleneck identication. Phys. Rev. B, 1:3434, 2016.

Lingqing Shen, Krishnakumar Balasubramanian, and Saeed Ghadimi. Non-asymptotic results for
langevin monte carlo: Coordinate-wise and black-box sampling. arXiv preprint arXiv:1902.01373,
2019.

Lei Shi, Xin Guo, and Ding-Xuan Zhou.  Hermite learning with gradient data.  J. Comput. Appl.
Math., 2010.

Suraj Srinivas and François Fleuret.  Knowledge transfer with jacobian matching.  arXiv preprint
arXiv:1803.00443, 2018.

The     MIT Development Group.      mit, a scalable computational framework for large-scale simula-
tion of complex mechanical response of materials, 2018. URL http://summit.mit.edu.

Shinya Suzuki, Shion Takeno, Tomoyuki Tamura, Kazuki Shitara, and Masayuki Karasuyama. Multi-
objective bayesian optimization using pareto-frontier entropy. arXiv preprint arXiv:1906.00127,
2019.

12


Under review as a conference paper at ICLR 2020

Qiang Wu, Justin Guinney, Mauro Maggioni, and Sayan Mukherjee. Learning gradients: Predictive
models that infer geometry and statistical dependence. J. Mach. Learn. Res., 2010.

Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of molecules
via deep reinforcement learning. Scientific reports, 9(1):10752, 2019.

A    SUPPLEMENTAL  EXPERIMENTAL  RESULTS

Surrogate gradient methods We use feed-forward neural networks to model the surrogates since
obtaining gradients for such networks is efficient thanks to automatic differentiation frameworks. 
We
use networks comprised of 4 hidden layers with sizes 128, 72, 64, 32 and apply the same architecture
to approximate the gradients for κ and σ separately.  The hidden layers compute ReLU activation
whereas sigmoid was used at the output layer (after the target output is properly normalized). For 
the
Taylor-2 variant (in Eq. 18), we have an output vector for the gradient prediction. The networks are
trained on the corresponding objective functions set up earlier by Adam optimizer with learning 
rate

10−⁴ and decay 1.0. We fine-tune the networks with simple grid-search and select the best models
for comparison.

As emphasized throughout, our focus is more on approximating the gradient rather than learning
the true function. However, we need to somehow evaluate the surrogate models on how well they
generalize on a hold-out test set.  Like canonical regression problems, we compare the surrogate
variants against each other using root mean square error (RMSE) on the test set.  Figures 2 and 3
shows the results. The left figure shows RMSE for predicting κ and the right one shows RMSE for
the von Mises stress σ. We can see that the Taylor-Reg generalizes better and also converges faster
than Taylor-1 and Taylor-2 to target RMSE for κ, while all methods result similarly for σ 
prediction.
This is reasonable because the objectives of Taylor-1 and Taylor-2 are not to optimize the mean
square error, which we evaluate on here. Figure 3 shows the learning in terms of sample complexity.
Again, Taylor-Reg outperforms Taylor-1 and Taylor-2 for κ prediction.  In contrast, most models
work similarly for σ regression, particularly when the training size is reduced to 50% (25K).

Performance in RMSE of the surrogate model for                                           
Performance in RMSE of the surrogate model for   v


1.0

1.1

Regression
Taylor-Reg
Taylor-1
Taylor-2

0.2

Regression
Taylor-Reg
Taylor-1
Taylor-2

0.0

1.2


1.3

0.2


1.4

0.4


1.5

0              5             10            15            20            25            30

Epochs

0.6

0              5             10            15            20            25            30

Epochs

Figure 2: Comparison of the surrogate variants in testing RMSE. (Left) prediction accuracy for the 
thermal
conductivity κ. (Right) prediction accuracy for mechanical stability σ. Note the difference in 
scale of κ and σ.

Effectiveness of Z-Hermite learning Notice that Z-Hermite learning is not included in this com-
parison and as a surrogate model in the black-blox Langevin sammpling in Section 8. The reason
is that apart from the usual sample pair (xi, yi), we need the gradient y˜i (See Eq. 17).  Since we
can query the solvers, this gradient can only be estimated using finite difference. For both κ and σ
in our experiment, obtaining such data is extremely expensive. As a consequence, we do not have
the full results of the Z-Hermite model. Instead, we ran a separate study to show the effectiveness
of Z-Hermite surrogate LMC on a smaller data with a lightweight OpenBTE version (0.9.55). The
results in Table 3 shows the working of Z-Hermite learning in learning the gradient of κ(x). Here,
the entropy is based nearest neighbor estimate to demonstrate the diversity of the pore centers in 
the
unit square. With the (xp, yp)-coordinates of each pore p, the entropy estimate is given by:

n

H =  1 Σ log(n min ǁp  − p  ǁ) + log 2 + C.

j


n

i=1

j/=i

13


Under review as a conference paper at ICLR 2020


1.30

1.35

Sample complexity of the surrogate models for

Regular
Taylor-Reg
Taylor-1
Taylor-2

0.15

0.20

0.25

0.30

Sample complexity of the surrogate models for   v

Regular
Taylor-Reg
Taylor-1
Taylor-2

0.35

1.40

0.40

1.45                                                                                                
                                 0.45

0.50

1.50

0.55


10          15          20          25          30          35          40

Number of samples (x1000)

10          15          20          25          30          35          40

Number of samples (x1000)

Figure 3: Comparison of the surrogate models in RMSE on the same test set when the training size is 
varied.
Note the scale difference in the figures due to the different range of values.


Model

Zero-order PLMC
Taylor-1 PLMC

Z-Hermite PLMC
Hybrid (Zero-order + Taylor-1)

Mean κ

0.0676

0.0988

0.0946

0.0786

Mean entropy

1.960

1.745

1.739

1.867

Per-sam. time (s)
3658

253

227

2136

Table 3:  Z-Hermite learning is sample efficient at training time as well as for the Langevin 
sampling,
but collecting the training set is prohibitive. Zero-order and deep surrogate can work in a hybird 
way
and offer a better trade-off between accuracy and computation.

A hybrid algorithm between zero-order and Taylor-1 surrogate We can see in Tables 1, 2 and
3 the trade-off between computation and accuracy of our approach.  While zero-order PLMC and
ProxLMC can achieve the lowest thermal conductivity, their computational costs are prohibitive.
In contrast, deep surrogate models (including Taylor-Reg, Taylor-1) are far more time-efficient but
slightly worse in terms of achieving the optimal κ. To mitigate the trade-off, we propose a simple
hybrid method that combines the best of the zero-order and Taylor-1 surrogate models. The algorithm
is shown in Figure A that alternates between using the gradient from the zero-order estimate and the
gradient of the deep surrogate depending on whether taking this step would decrease the potential
function (i.e.  κ).  We show and compare the achieved κ and running time in Table 3.  Examples
of   the samples generated by Zero-order PLMC, Taylor-1 PLMC and the hybrid method are also
depicted in Figure 4. The hybrid achieves the thermal conductivity that is lower than Taylor-1 PMLC
while running almost 2x faster than zero-order PLMC. This suggests that the hybrid strategy offers a
better trade-off in accuracy and computation. One way to further improve the hybrid is to collect 
the
zero-order gradients while mixing and re-update the surrogate with Z-Hermite learning.

Algorithm 1 A hybrid PLMC algorithm alternating between zero-order and Taylor-1 surrogate
gradients.

Train a network fθ(x) with Taylor-1
Randomly sample x₀ from the uniform p(x)

Perform a Langevin dynamic step

for t = 1, 2, . . . , K do

if κ(x − η∇ₓfθ(x)) < κ(x) t√hen 


x      PΩ(x     η   ₓfθ(x) +

else

2ηξ)

estimate ∇κ(x) using zero-order method

end if
end for

Return a new sample x

14


Under review as a conference paper at ICLR 2020

κ = 0.063                                    κ = 0.090                                    κ = 0.076

Figure  4:  Samples  from  by  Zero-order  PLMC  (left),  Taylor-1  PLMC  (middle)  and  the  hybrid
algorithm of Zero-order and Taylor-1 PLMC (right). All are run with the κ constraint.

Additional generated samples We show additional configurations generated by our sampling ap-
proach (Taylor-Reg ProxLMC, Taylor-1 ProxLMC and Zero-order ProxLMC) in Fig.  5,  6 and
7.

κ = 0.0871, σ = 0.4826             κ = 0.0834, σ = 0.4638             κ = 0.0942, σ = 0.4566

Figure 5:  Example of nano-porous structures with corresponding heat flux shown using a color
gradient. Yellow regions indicate high phonons flux. The thermal conductivity κ and von Mises stress
σ are reported below each structure.  The arrows show the moving directions of the pores.  (Left)
A random sample.  (Middle) The sample obtained by Taylor-Reg ProxLMC starting from the left
structure with κ constraint. (Right) The sample obtained by Taylor-Reg ProxLMC with both κ and σ
constraints.

15


Under review as a conference paper at ICLR 2020

κ = 0.0871, σ = 0.4826             κ = 0.0834, σ = 0.4638             κ = 0.0942, σ = 0.4566

Figure 6:  Example of nano-porous structures with corresponding heat flux shown using a color
gradient. Yellow regions indicate high phonons flux. The thermal conductivity κ and von Mises stress
σ are reported below each structure. The arrows show the moving directions of the pores. (Left) A
random sample. (Middle) The sample obtained by Taylor-1 ProxLMC starting from the left structure
with κ constraint. (Right) The sample obtained by Taylor-1 ProxLMC with both κ and σ constraints.

κ = 0.0871, σ = 0.4826             κ = 0.0719, σ = 0.4424             κ = 0.1074, σ = 0.5110

Figure 7:  Example of nano-porous structures with corresponding heat flux shown using a color
gradient. Yellow regions indicate high phonons flux. The thermal conductivity κ and von Mises stress
σ are reported below each structure.  The arrows show the moving directions of the pores.  (Left)
A random sample.  (Middle) The sample obtained by Zero-order ProxLMC starting from the left
structure with κ constraint. (Right) The sample obtained by Zero-order ProxLMC with both κ and σ
constraints.

B    BACKGROUND  ON  MODELING  NANOSCALE  HEAT  TRANSPORT

At the nanoscale, heat transport may exhibit strong ballistic behaviour and a non-diffusive model 
must
be used (Chen, 2005). In this work we use the Boltzmann transport equation under the relaxation
time approximation and in the mean-free-path (MFP) formulation (Romano & Grossman, 2015)

Λˆs · ∇T (Λ) + T (Λ) = ∫  α(Λ′)⟨T (Λ′)⟩dΛ′,                                    (23)

where T (Λ) is the effective temperature associated to phonons with MFP Λ and direction ˆs; the
notation ⟨.⟩ stands for an angular average. The coefficients α(Λ′) are given by

16


Under review as a conference paper at ICLR 2020

K(Λ′) Σ∫   K(Λ′′)       Σ−1

where K(Λ′) is the bulk MFP distribution.  In general, such a quantity can span several orders of
magnitude; however, for simplicity we assume the gray model, i.e. all phonons travel with the same
MFP, Λ₀. Within this approximation, we have K(Λ) = κbulkδ(Λ     Λ₀). In this work we choose Λ₀

= 10 nm, namely as large as the unit cell, so that significant phonons size effects occur. With no 
loss
of generality, we set κbulk = 1 Wm−¹K−¹ . Eq. 23 is an integro-differential PDE, which is solved
iteratively for each phonon direction over an unstructured mesh (Romano & Di Carlo, 2011). We
apply periodic boundary conditions along the unit cell while imposing a difference of temperature
of ∆T  = 1 K along the x-axis. At the pores’ walls we apply diffusive boundary conditions. Upon

convergence, the effective thermal conductivity is computed using Fourier’s law, i.e.


κeff

=        L   

∆TA

J   nˆdS,                                                  (25)

A

where J = (κbulk/Λ₀)  T (Λ₀)ˆs  nˆ is the heat flux, L is the size of the unit cell, A is the area 
of the
cold contact (with normal nˆ). Throughout the text we use the quantity κ = κₑff /κbulk as a measure
of phonon size effects.

C    BACKGROUND  ON  MODELING  MECHANICAL  STRESS

We model mechanical stress by using the continuum linear elasticity equations

∂


∂xj

σij = fi,                                                              (26)

where fi is the body force (which is zero in this case), and σij is the stress tensor. Note that we 
used
the Einstein notation, i.e. repeated indexes are summed over. The strain ϵkl is related to the 
stress via
the fourth-rank tensor elastic constant Cijkl

σij = Cijklϵkl.                                                            (27)

The strain is then related to the displacement u via

ϵ    =  1 . ∂uk  +  ∂ul Σ .                                                    (28)

We apply periodic boundary conditions along the unit-cell and applied solicitation is a small 
in-plane
expansion. Once the stress tensor is calculated, we compute the von Mises stress as


σV M

=      1 (σ

2     3

− σ₂)   + (σ₃

− σ₁)   + (σ₂

− σ₁)  ,                              (29)

where σi are the principal stress axis. As a mechanical stability estimator we use σ = maxₓ  D(σV M 
)
where D is the simulation domain.  To avoid material’s plasticity, σ needs to be smaller than the
yield stress of a given material.   For mechanical simulation we used the SUMIT code (    MIT
Development Group, 2018).

D    BACKGROUND  ON  STOCHASTIC  DIFFERENTIAL  EQUATIONS  (SDE):
CHANGE  OF  MEASURE  AND  GRISANOV’S  FORMULA

Theorem 3 (Grisanov Theorem, Change of Measure for Brownian Motion (Lipster & Shiryaev,
2001), Theorem 6.3 page 257).  Let (Wt, Ft) be a Wiener process (Brownian motion) and (βt, Ft) a
random process such that for any T  > 0


T

ǁβtǁ

dt < ∞ a.s

17


Under review as a conference paper at ICLR 2020


Then the random process : dW˜   = dW

− β dt or written equivalently: W˜   = W  − ∫ t β  ds, is a

	    


densities are given by:  ᵈP W˜

= exp

.∫ T

⟨βs, dWs⟩ −     ∫    ǁβsǁ   dsΣ. It follows that:

1      T

 


W       W˜            1

Σ∫  T           2       Σ


KL(PT   , PT   ) =    EP W

2     T

ǁβsǁ   ds

(30)

Theorem 4 (Grisanov Theorem, Change of Measure for Diffusion Processes, (Lipster & Shiryaev,
2001), ()).  Let (Xt)t≥₀ and (Yt)t≥₀

dXt = αt(X)dt + dWt

dYt = βt(Y )dt + dWt

where X₀ = Y₀ is an F₀ measurable random variable. Suppose that the non-anticipative functionals
αt(x) and βt(x) are such that a unique continuous strong solutions exits for both processes. If for
any T  > 0:


T

ǁαs(X)ǁ

+ ǁβs(X)ǁ

ds < ∞(a.s) and

T

ǁαs(Y )ǁ

+ ǁβs(Y )ǁ2

ds < ∞(a.s).

Let P X = L (X[₀,T ]), and P Y  = L (Y[₀,T ]).


T                                           T

(X) = exp .− ∫    ⟨αs(X) − βs(X), dXs⟩ +

dP Y

1 ∫  T

(ǁαs(X)ǁ − ǁβs(X)ǁ2)dsΣ .


dP X

0

X      Y            1

2   0

Σ∫  T                                      2       Σ


KL(PT  , PT  ) =    EP X

2     T

ǁαs(X) − βs(X)ǁ   ds

.                            (31)

E    BACKGROUND  ON  ZERO-ORDER  OPTIMIZATION  (GRADIENT-FREE)

Consider the smoothed potential Uν defined as follows:

Uν (x) = Eg∼N ₍₀,Id )U (x + νg)


its gradient is given by:

∇ₓUν (x) = Eg

U (x + νg) − U (x) g,
ν


A monte carlo estimate of ∇ₓUν (x) is:

n

ˆ

n              n

j=1

. U (x + νgj) − U (x) Σ

gj,

where g₁, . . . gn are iid standard Gaussians vectors.

Using known results in zero order optimization under assumptions on smoothness and bounded
gradients of the gradients we have for all x ((Nesterov & Spokoiny, 2017; Shen et al., 2019)):


₂      .

E  ¨Gˆ  (x) − ∇  U (x)¨   ≤    βν(d + 2)³/² + (d + 1) 1  ǁ∇

U (x)  Σ2

.βν(d + 2)³/²

1     Σ₂


g ¨   1                    x          ¨

2           x          ǁ      ≤

+ (d + 1) 2 L


¨                          ¨₂

.βν(d + 2)³/² + (d + 1) 1 L  2

2


Eg1 ,...,gn     Gˆn(x) − ∇ₓU (x)     ≤

18

(32)

n


Under review as a conference paper at ICLR 2020

F    PROOFS

Proof of Lemma 1.  Define the Lagrangian:


L(q, η) =

log

Ω

q(x)

p₀(x)

q(x)dx +

Ce

λj

j=1

∫  (ψj(x) − yj)²q(x)dx


Ci

+       λk

k=1

∫x∈Ω

(φk(x) − bk)₊q(x)dx + η

.1 −

∫x∈Ω

q(x)Σ

Setting first order optimality conditions on q, we have for x ∈ Ω:


log

q(x)

p₀(x)

+ 1 +

Σj=1

λj(ψj(x) − yj)² +

Ci

k=1

λk(φk(x) − bk)₊ − η = 0


Hence we have:

exp .− ΣCe

λj(ψj(x) − yj)² − ΣCi      λk(φk(x) − bk)₊Σ


q(x) = p₀(x)

and

e exp −η                                          , x ∈ Ω

q(x) = 0, x ∈/ Ω,

First order optimality on η give us:   Ω q(x) = 1, we conclude by setting e exp(−η) = Z.

Proof of Theorem 1 1) Projected Langevin.  Let  us  define  the  following  continuous  processes  
by
interpolation of Xk and YK (Piecewise constant):


dX˜

= P  (U˜ (X˜)dt + √2λdW )

t          Ω       t                                  t

where U˜t(X˜) = − Σ∞k=0 ∇ₓU (X˜kη )1t∈[kη,(k+1)η](t). Similarly let us define :

dY˜  = P  (G (Y˜)dt + √2λdW )

t          Ω       t                                  t

where Gt(Y˜) = −      ∞k=0 G(Y˜kη )1t∈[kη,(k+1)η](t).

It is easy to see that we have : Xk = X˜kη  and Yk = Y˜kη .


Let πT

and πT be the distributions of (X˜t)t∈[₀,T ] and (Y˜)t∈[₀,T ].


Note that :

˜

. ˜    ˜            √     

   1            ˜        ˜    ˜          Σ


Let

dYt = PΩ

Ut(Xt)dt +    2λ(dWt + √2λ (Gt(Yt) − Ut(Xt))dt)

˜                       1            ˜        ˜    ˜


Hence we have :

dWt = dWt + √2λ (Gt(Yt) − Ut(Xt))dt

dY˜t  = PΩ .U˜t(X˜) + √2λdW˜tΣ ,

Assume that X₀ = Y₀ there exists Q such that , XT = Q({Wt}t∈[₀,T ]) and YT = Q((W˜t)t∈[₀,T ]).


Let µX˜  be the law of X˜

t∈[0,T ]

. Same for µY˜ . The proof here is similar to the proof of Lemma 8 in

(Bubeck et al., 2015). By the data processing inequality we have:

KL    X˜       Y˜                                              ˜

(µT , µT ) ≤ KL(Wt∈[0,T ], Wt∈[0,T ]),

Now using Grisanov’s Theorem for change of measure of Brownian Motion (Theorem 3) we have:


˜                   1     ∫  T

	

˜        ˜    ˜

19


Under review as a conference paper at ICLR 2020

Consider T  = Kη, hence we have (with some abuse of notation we drop tilde as Yk = Y˜kη ):


˜        ˜             1 

∫  Kη

˜        ˜    ˜


T      T          4λ

 1 

=      E

0

KΣ−1 ∫  (k+1)η

ǁG(Ykη) − ∇ₓU (Xkη)ǁ   dt

4λ

k=0

K−1


η

=  4λ         E ǁG(Ykη

k=0

K−1

) − ∇ₓ

2

U (Xkη)ǁ


η

=  4λ         E ǁG(Ykη

k=0

η  KΣ−1 .

) − ∇ₓ

U (Ykη) + ∇ₓ

2

U (Ykη

) − ∇ₓ

U (Xkη

)ǁ2

₂Σ


≤  2λ

k=0

E ǁG(Ykη) − ∇ₓU (Ykη)ǁ

+ E ǁ∇ₓU (Ykη) − ∇ₓU (Xkη)ǁ

where in the last inequality we used the fact that   a     b  ²      2(  a  ² +   b  ²). Note that 
we have by
smoothness assumption on U :

ǁ∇ₓU (Ykη) − ∇ₓU (Xkh)ǁ   ≤ β   ǁXkh − Ykhǁ

Let R be the diameter of Ω, we can get a bound as follows:


˜        ˜              η

	

KΣ−1                                                              

K−1                                         


KL(µX , µY ) ≤        

E ǁG(Ykη) − ∇ₓU (Ykη)ǁ2 +β²

E ǁXkh − Ykhǁ2


η

≤  2λ

K−1

k=0

E ǁG(Ykη) − ∇ₓU (Ykη)ǁ2

+ Kβ²R²Σ

Now using Pinsker inequality we have:


˜        ˜                                ˜        ˜

			

η .KΣ−1                                                                                            
Σ

	


Hence for T  = Kη we have:

	

.    .K−1                                                                                       Σ 1

		

Proof of Theorem 1 2) Proximal LMC.  Let us define the following continuous processes by interpo-
lation of Xk and YK (Piecewise constant):

dX˜   = U˜ (X˜)dt + √2λdW

where U˜t(X˜)  =  − Σ∞k=0(∇ₓU (X˜kη ) +  ¹ (X˜kη  − PΩ(X˜kη )))1t∈[kη,(k+1)η](t).  Similarly let us


define :

dY˜t

= G (Y˜)dt + √2λdW

20


Under review as a conference paper at ICLR 2020

where Gt(Y˜) =         k∞=0(G(Y˜kη ) + ¹ (Y˜kη     PΩ(Y˜kη )))1t∈[kη,(k+1)η](t). Now applying 
Grisanov’s

Theorem for diffusions (Theorem 4) we have:


X˜       Y˜             1 

Σ∫  T  ¨      ˜              ˜  ¨     Σ


KL(µT , µT ) =      EP X

4λ     T

¨Ut(X) − Gt(X)¨  dt

 1     KΣ−1 ∫  (k+1)η ¨                                 ¨

 

η  K−1


=

4λ

k=0

K−1

E ¨G(X˜kη ) − ∇ₓU (X˜kη )¨


η

=  4λ         E ǁG(Xk) − ∇ₓ

k=0

U (Xk)ǁ2 .

Now using Pinsker inequality we have:

TV (µT , µT )² ≤ 2KL(µT , µT ).


Hence for T  = Kη we have:

X˜       Y˜

.      .K−1

	

X˜       Y˜

Σ 1

	

Proof of Theorem 2 .  S-PLMC. If we set λ  =  1, η  ≤  α/K², where α  =  1/(δ + β²R²), in this

Corollary we obtain that :  TV (µS−P LMC, µP LMC)  ≤  √1    . Assuming A, B and C we consider

η ≤ min(R²/K, α/K²), and K = Ω˜(ε−¹²d¹²). Now using the triangle inequality together with the

bounds in Eq.s 7 we have: TV (µS−P LMC, π) ≤ TV (µS−P LMC, µP LMC) + TV (µP LMC, π) ≤


ε +   ¹   .

K

K                                        K                    K

S-ProxLMC. We conclude with a similar argument for TV (µS−P ʳᵒˣLMC, π) using Eq.s 8. Consid-


ering η = min(γ(1 + β²γ²)−¹,   ¹ 2 ), and K = Ω˜(ε−⁶d⁵), we obtain (ε + √1

) approximation in

δK                                                                                 K

TV of the target Gibbs distribution.

Proof of Corollary 1.  Z-PLMC: We have:

˜     ˜        ‚u η .KΣ−1                                                                           
                         Σ

			

TV (µX , µY ) ≤ ,             E ǁGnU (Ykη) − ∇ₓU (Ykη)ǁ2  + Kβ2R2

Taking the expectation we have:

˜     ˜                     ‚u η .KΣ−1                                                              
                                      Σ

							


Eg  ...g

TV (µX , µY ) ≤ Eg ...g  ,             E ǁGnU (Ykη) − ∇ₓU (Ykη)ǁ2  + Kβ2R2


η     K−1

≤       λ

k=0

EY Eg1 ...gn

ǁGnU (Ykη) − ∇ₓU (Ykη)ǁ2  + Kβ2R2Σ

(Jensen inequality)


Note now that we have:

Eg  ...g

ǁGnU (Ykη) − ∇ₓU (Ykη)ǁ   ≤ δ, ∀Ykη.


For n ≥ .βν(d + 2)³/²

1       n

1     Σ2 /δ The rest of the proof is an application of Theorem 2.

+ (d + 1) 2 L

Z-ProxLMC. A similar argument holds.

21

