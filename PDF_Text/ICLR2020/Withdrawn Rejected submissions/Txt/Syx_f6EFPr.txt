Under review as a conference paper at ICLR 2020
Supervised learning with incomplete data via
sparse representations
Anonymous authors
Paper under double-blind review
Abstract
This paper addresses the problem of training a classifier on a dataset with missing
features and its application to a complete or incomplete test dataset. A supervised
learning method is developed to train a general classifier, such as a logistic regression
or a deep neural network, using only a limited number of observed entries (features),
assuming sparse representations of data vectors on an unknown dictionary. The
pattern of missing entries is independent of the sample and can be random or struc-
tured. The proposed method simultaneously learns the classifier, the dictionary and
the corresponding sparse representations of each input data sample. A theoretical
analysis is also provided comparing this method with the standard imputation
approach, which consists on performing data completion followed by training the
classifier based on their reconstructions. The limitations of this last “sequential”
approach are identified, and a description of how the proposed new “simultane-
ous” method can overcome the problem of indiscernible observations is provided.
Additionally, it is shown that, if it is possible to train a classifier on incomplete
observations so that its reconstructions are well separated by a hyperplane, then
the same classifier also correctly separates the original (unobserved) data samples.
Extensive simulation results are presented on synthetic and well-known reference
datasets that demonstrate the effectiveness of the proposed method compared to
traditional data imputation methods.
1	Introduction
In many machine learning applications sometimes the measurements are noisy or affected by artifacts,
resulting in incomplete data samples. Examples of this situation include: self-driving vehicle or
robot where objects in the view field can be partially occluded; recommendation systems built from
the information gathered by different users where not all the users have fully completed their forms;
or medical datasets where typically not all tests can be performed on all patients.
A classical approach of supervised learning with missing entries is to apply entry imputation as a
preprocessing step, followed by standard training based on these reconstructions (Little & Rubin,
2014). For example, some techniques compute missing entries using statistical methods, such
as the “mean”, “regression” and “multiple” imputation methods (Little & Rubin, 2014). Other
completion methods are based on machine learning ideas by estimating missing entries through
K -nearest neighbor (Batista & Monard, 2002), Self Organization Maps (SOM) (Fessant & Midenet,
2002), multilayer or recurrent neural networks (Yoon & Lee, 1999; Bengio & Gingras, 1995), tensor
completion algorithms (Sole-Casals et al., 2018) and others (Garcia-Laencina et al., 2009). In
(Huang et al., 2018), a technique for completing the matrix of features was proposed by imposing
low-rankness and incorporating the label information into the completion process. A different
1
Under review as a conference paper at ICLR 2020
approach is to avoid direct imputation of lost inputs and rely on a probabilistic model of input data,
learning model parameters through the Expectation Maximization (EM) algorithm and building a
Bayesian classification (Ghahramani & Jordan, 1993; Little & Rubin, 2014). However, the latter
“model-based” approach has the disadvantage that it requires a good probabilistic data model, which
is usually not available, especially for real-world applications such as those involving natural images.
On the other hand, during the last few years in the signal processing community, there has been a
rapid development of the Compressed Sensing (CS) theory and sparse coding algorithms that exploit
the redundancy of natural signals (Candes et al., 2006; Donoho, 2006; Eldar & Kutyniok, 2012; Gregor
& LeCun, 2010). A particular case of CS is the signal completion problem, i.e. the reconstruction of
signals from incomplete measurements. Several algorithms for matrix and tensor (multidimensional
signals) completion have been recently proposed relying on different low-dimensional manifold
models that are fit to the available entries, allowing for an extrapolation of the unobserved entries.
This category of models ranges from sparse representation of vectors (Aharon et al., 2006; Fadili
et al., 2008) and tensors (Caiafa & Cichocki, 2012) to low-rank matrix (Candes & Tao, 2010) and
tensor decompositions (Liu et al., 2012; Yuan et al., 2018). The success of sparse representations in
many signal processing applications motivated researchers to use dictionary learning techniques for
data classification (Mairal et al., 2008), either using class-specific dictionaries (Ramirez et al., 2010;
Sprechmann & Sapiro, 2010), or using a single one for all classes (Tosic & Frossard, 2011).
In this paper, the goal is to develop a method for training a classifier using incomplete data samples
with their labels and to identify the conditions under which such a classifier performs as good as
the ideal classifier, i.e. the one that could be obtained from complete observations. We propose to
use the sparse representation model for data vectors, and analyze the limitations of the sequential
approach, i.e. imputation followed by training (section 2). In section 3 we introduce the simultaneous
classification and coding approach, which consists of incorporating a sparse data representation
model into a cost function optimized for training the classifier and, at the same time, finding the best
sparse representation of the observed data. We provide theoretical conditions under which we can
guarantee that the obtained classifier is as good as the ideal classifier. In section 4, a computational
method is presented to train a classifier on incomplete data; in section 5, extensive experimental
results are presented using synthetic and well known benchmark datasets illustrating the effectiveness
of our proposed algorithms; and finally, in section 6, the main conclusions are outlined.
1.1	Preliminary definitions and problem formulation
We assume a supervised learning scenario with vector samples and their labels {xi , yi }, i = 1, 2, . . . I,
xi ∈ RN and yi ∈ {0, 1, . . . , C - 1} (C classes), so that we only have access to subsets of entries in
each data vector along with their labels, which is denoted by {xi, y%}, Xi ∈ RM with M < N.
The success of CS theory is based on the fact that sparse coding of natural signals is nearly
ubiquitous. It is found, for example, in the way that neurons encode sensory information (Olshausen
& Field, 1996; 1997; Lee et al., 2007). We define the set of all K -sparse vectors ΣLK = {s ∈
RL such that IlsIlO ≤ K} (containing at most K non-zero entries) and assume that data vectors Xi
admit K -sparse representations over an unknown dictionary D ∈ RN×L (L ≥ N), i.e. Xi = Dsi,
with si ∈ ΣLK. The columns di ∈ RN of a dictionary are called “atoms” because we can express any
data vector as a combination of at most K atoms.
Without losing generality, to simplify the mathematical treatment, we shuffle the entries of the full
vector Xi to have the observed values placed in the upper M positions, so that we can write:
xi = xi , with Zi ∈ R(N-M).	(1)
2
Under review as a conference paper at ICLR 2020
However, our results are valid for any pattern of missing entries for each data vector. Accordingly,
we define Di ∈ RN ×L as the shuffled version of D, such that it stacks in the top M rows those
corresponding to the observed entries:
「6.1	~	….	.......
Di = Di , with Di ∈ RM×L and Ei ∈ R(N-M)×L.
Ei
(2)
Some important uniqueness properties of sparse vector reconstructions are characterized by the
Restricted Isometry Property (RIP) of a matrix, which is defined as follows: a matrix A ∈ RN ×L
satisfies the RIP of order K if there exists δκ ∈ [0, 1) such that (1 - δκ)∣∣s∣∣2 ≤ IlAsIl 2 ≤ (1 + δκ)∣∣s∣∣2,
holds for all s ∈ ΣK. RIP was introduced in the context of CS by Candes and Tao (Candes & Tao,
2005) and characterizes matrices which are nearly orthonormal when operating on sparse vectors.
Let us assume that a perfect classifier pΘ(y | x) in a two classes scenario (yi ∈ {0, 1}), e.g. a
logistic regression or deep neural network, can be trained on the complete dataset {xi , yi}, such
that 0.5 < pΘ(yi = 1 | xi) ≤ 1.0 and 0 ≤ pΘ(yi = 0 | xi) ≤ 0.5, ∀i = 1, 2, . . . , I, where Θ is the set of
trained parameters. We want to develop a method to estimate the classifier parameters Θ using only
the incomplete data {x%, yi} and to identify the conditions under which such classifier is compatible
with the ideal classifier. In the following section we will show that, when data imputation based on
sparsity followed by training is used, some limitations are clearly identified.
2	The problem of training after data imputation
First, we will provide some intuition about the limitation of the sparsity-based imputation or
“sequential” approach through a toy example. We consider the classification of hand-written digit
images belonging to two classes: the numbers “3” and “8”. We will assume that they admit 2-sparse
representations over a dictionary and only the right halves of these digits are observed. Let us
consider two example vectors xi and xj belonging to classes “3” and “8”, respectively, as shown in
Figure 1 (a-d). Here, we are clearly faced with the indiscernible observations problem because our
observations of two vectors from different classes are identical (Xi = Xj). It is obvious that at least
two possible 2-sparse representations for the observed data exist, let,s say Xi = Disi = Disi, with
si = si. Since the solution is not unique, we are not able to correctly reconstruct the original data
vectors based only on the observations and the sparsity assumption.
If the K-sparse representation of the observations Xi are unique, then Xi can be perfectly recon-
structed from the incomplete observations and the classifier can be successfully trained using these
reconstructions, but unfortunately this is not the case most of the times. In the particular case where
the dictionary D is known in advance, there are some conditions on the sampling patterns based on
properties of matrices such as coherence, spark or RIP that can guarantee a correct reconstruction
(Eldar & Kutyniok, 2012). However, these conditions are difficult to meet in practice. Moreover,
in the more general case where the dictionary D is unknown and needs to be learned from data,
it is even more difficult to obtain reconstructions of partially observed data vectors that are well
separated. For example, even in the case where complete data vectors are well separated, it could
happen that their reconstructions become difficult to separate, leading to suboptimal classifiers.
In this paper, motivated by the limitation of the sequential approach, we propose to incorporate the
class information of incomplete samples in order to learn simultaneously their sparsest representations
and the classifier, such that the classification error of the reconstructions is minimized.
3
Under review as a conference paper at ICLR 2020
3	The simultaneous classification and coding approach
In the previous section, we showed that sparsity-based imputation methods using only the information
on available entries are prone to fail because the non-uniqueness of solutions can make the training
of a good classifier an impossible task. It is interesting to note that we could solve this problem
by incorporating from the beginning the information of the class to which the incomplete data
vectors belong to. Let us consider the toy example of previous section, for which we assumed
the classes are linearly separable and each data vector admits a K -sparse representation over a
dictionary D ∈ RN ×L . A two-dimensional simplified visualization for this example is provided in
Figure 1(e). When We apply a sparsity-based reconstruction algorithm on observations Xi, and Xj
such that Xi = Xj and, assuming for now that dictionary D is known, we search for the K-sparse
representation that is compatible with the measurements, i.e. we find S ∈ ΣK such that Xi = Dis.
In this case, there are at least two equally acceptable solutions si and sj corresponding to vectors
Xi and Xj , respectively. If we were to make the wrong decision of assigning sj to Xi and si to Xj ,
then the resulting set of reconstructed vectors would not be linearly separable, which is illustrated
in Figure 1(f), therefore it would be impossible to find a linear classifier. The key observation
here is that, in order to assign the proper sparse representation we could also look at the label
information yi and yj , and make the solution unique by discarding any K -sparse representation
resulting in not separable vector reconstructions. This suggests that we would need to train the
classifier and find the proper representation not only as sparse as possible but also providing the
best separation of classes. In other words, we should simultaneously learn the optimal classifier
and find sparse representation of observations. Next, we theoretically analyze the case of using a
logistic regression classifier and demonstrate that, if we are able to train a classifier on incomplete
observations such that their sparse representations are well separated by a hyperplane, then the
same classifier correctly separates the original (unobserved) data vectors.
3.1	Theoretical guarantees in the logistic regression case
Let us consider the case of a logistic regression classifier (Hastie et al., 2009) where the set of
parameters Θ = {w, b} are a vector w ∈ RN and a scalar (bias) b ∈ R, and a perfect classifier exists
if there is a hyperplane that separates both classes, i.e., for each data vector Xi: f(Xi) = wTXi +b > 0
when yi = 1 and f (Xi) ≤ 0 if yi = 0. We investigate the conditions under which a perfect classifier
can be trained using incomplete data samples and their labels. By using the same shuffling as in
equation (1), we can write the partition of the shuffled coefficients wi ∈ RN as follows:
, where ui ∈ R(N-M).	(3)
Since the classifier is based on the feature f (Xi) = WTXi + b = WTXi + UTZi + b, the vector
ui ∈ R(N-M) contains the weights associated with unobserved entries zi . The following definition
introduces a way to measure the amount of discriminative information associated with missing
entries.
Definition 3.1. Discriminative Missing Information (DMI) μ%: Given a classifier {w, b} and
a partition of observed/missing entries in sample vector Xi such that the shuffled vector of coefficients
wi can be partitioned as in eq. (3), we define the DMIfor sample Xi as: μ% = ∣∣u∕∣ι = EN= -M |ui(m)|.
Intuitively, if the DMI is small enough, the classification will not strongly depend on the unobserved
entries zi and then a classifier could be trained from incomplete data samples. The following theorem
establishes precise conditions under which the existence of a classifier obtained from incomplete
W i
ui
Wi
4
Under review as a conference paper at ICLR 2020
Figure 1: An indiscernible observation toy example: (a) 4 out L dictionary elements di (atoms). (b) Digit
“3” and “8” can be represented by combining only two atoms in the dictionary (2-sparse representations). (c)
Atoms with their left-halves occluded di. (d) An occluded digit “3” or “8” admits more than one 2-sparse
representation, for example, they can be expressed as the sum of occluded atoms d1 and d2, or d3 and d4.
(e) Simplified two-dimensional representation of samples xT = [x1, x2] ∈ R2 from “3” and “8” classes that
are linearly separable where incomplete observations are taken by observing only one entry. Note that xi
and xj belong to different classes but their observations are identical. (f) A wrong reconstruction of data
vectors, i.e. Xi = Xi and Xj = Xj can make the set of reconstructed vectors not linearly separable.
data can be guaranteed and thus, a perfect classifier can be successfully trained. The proof can be
found in Appendix A
Theorem 3.2. Given a dataset {xi,yi}, i = 1, 2,...,I with normalized data vectors (∣∣x∕∣ ≤ 1)
admitting a K -sparse representation over a dictionary D ∈ RN ×L with unit-norm columns and
satisfying the RIP of order K with constant δK , and suppose that, we have obtained an alternative
dictionary D' ∈ RN×L also satisfying the RIP of order K with constant δκ such that, for the
incomplete observation X i ∈ R M, the K -sparse representation solution is non-unique, i.e. ∃s i, Si ∈
ΣK such that Xi = DiSi = Disi, where lɔi, Di ∈ RM×L are the matrices containing the observed
rows of D and Di, respectively; Si ∈ RL is the vector of coefficients of the true data, i.e. Xi = DiSi
and Si provides a plausible reconstruction through Xi = DiSi with ∣∣Xi∣∣ ≤ 1. Ifa perfect classifier
{wi, b} of the reconstruction Xi exists such that
e > 2Kμi/(1 - δκ),	⑷
with e > 0 being the minimum distance of reconstructed data sample to the separating hyperplane,
then the full data vector Xi is also perfectly separated with this classifier, in other words: f(Xi) =
wiT Xi + b > 0 (≤ 0) ifyi = 1 (yi = 0).
It is interesting to note that, supposing that we have obtained a classifier from incomplete data,
if we are able to evaluate condition (4) then we can figure out whether such a classifier is optimal
by testing the condition of the theorem. However, It is well known that, in general, the RIP
constant is difficult to compute in practice. When the dictionary is highly uncorrelated1 , i.e. if
ρK < 1, then the RIP constant can be written in terms of the correlation coefficients ρ as follows:
δK = (K - 1)ρ (Eldar & Kutyniok, 2012), and ρ is easy to compute. Our theorem provides useful
insights of the problem. For example, we it suggests the following conditions are desirable: (a) very
well separated reconstructed vectors (large e); (b) very sparse model (small K); (C) small norm
variability of transformed K -sparse vectors through D (small δK), which can be interpreted as a
1 Correlation coefficient is the maximum absolute correlation between any two columns in a normalized
dictionary.
5
Under review as a conference paper at ICLR 2020
quasi-orthonormal basis behaviour (low correlation, for example); and (d), small weights assigned to
unobserved entries of Xi (small DMI μi).
4	The proposed method
Here, we propose to combine the training of the classifier together with the learning of a dictionary
and optimal sparse representations such that reconstructed data vectors are compatible with the
observed entries and, at the same time, well separated. To do that we propose to minimize the
following global cost function, where We consider that x, X and m are now the original unruffled
vectors and the pattern of missing features is not the same for each data sample i :
1I
J(Θ,D, S i ) = ιf{J0(Θ, X i,yi) + λ ι J ι(D, S i) + λ 2 J2(s i)},	(5)
i=1
where Θ contains the classifier parameters, i.e. the vector of coefficients and bias for a logistic
regression model, or the vector of weights in a deep neural network classifier architecture; D ∈ RN ×L
(L ≥ N ) is a dictionary and Si ∈ RL are the representation coefficients such that the reconstructed
data vectors are Xi = Dsi. J0(Θ, Xi,y%) is a measure of the classification error for sample i. Typically,
we use the crossentropy measure, i.e. J0(Θ, X, yi) = - log(pyΘi (X)), where pyΘi (Xi) is the probability
assigned by the classifier to sample Xi as belonging to class yi . J1 (D, Si) is a measure of the error
associated with the sparse representation when it is restricted to the observed entries, and is defined
as follows: J 1(D, Si) = M∣∣mi * (xi — Dsi)∣∣2, where * stands for the entry-wise product, mi ∈ RN is
the observation mask for sample i, with mi (n) ∈ {0, 1} such that mi (n) = 1 or 0 if data entry Xi (n)
is available or missing, respectively. J2 (S i) = NIIS i ∣ι is the £「norm whose minimization promotes
the sparsity of the representation (Candes & Tao, 2005). Finally, the hyper-parameters λ 1 and λ2
allow to give more or less importance to the representation accuracy and its sparsity, with respect
to the classification error. Intuitively, minimizing equation (5) favors solutions that not only have
sparse representations compatible with observed entries, but also provides reconstructions that are
best separated in the given classes.
To minimize the cost function in equation (5) we propose to alternate between the optimization
over {Θ, D} and Si (i = 1, 2, . . . , I). As usual, we adopt a first order (gradient based) search of
minima, where gradients of functions J0(∙), Jι(∙) and J2(∙) are easily derived. Note that in the
case of feedforward architectures, the back-propagation technique can be used for the first term
J0(∙). It is also noted that function J2(∙) is not differentiable at zero, so we need to avoid zero
crossing in every update. In the case where the given test dataset is also incomplete we can use
the dictionary learned during the training phase to find the sparsest representation for the given
observations, compute the corresponding full vector reconstructions and apply the classifier to them.
The algorithms are presented in Appendix B.
5 Experimental results
We synthetically generated I = 11, 000 (10, 000 training +1, 000 test) K -sparse data vectors Xi ∈ R100
using a dictionary D ∈ R100×200 obtained from a Gaussian distribution with normalized atoms,
i.e. ∣∣D(: ,l) ∣ = 1, ∀ l. A random hyperplane {w ,b} with W ∈ R N, b ∈ R was randomly chosen
dividing data vectors into two classes according to the sign of the expression wT Xi + b, which
defines the value of label yi . We also controlled the degree of separation between the two classes by
regenerating all data vectors with distances to the hyperplane lower than a pre-specified threshold
d. For each case we generated 10 realizations using different masks and input data. We applied
our simultaneous method, with hyperparameters λ1 and λ2 tuned via cross-validation, to train a
6
Under review as a conference paper at ICLR 2020
logistic regression classifier on the training dataset with randomly distributed missing entries, and
compared the obtained Test Accuracy against the following standard sequential methods: Seq.
Sparse: reconstructions are obtained by finding the sparsest representation compatible with the
observations solving a LASSO problem; Zero Fill (ZF): missing entries are filled with zeros, which
is equivalent to ignore unknown values; Mean Unsupervised (MU): missing entries are filled
with the mean computed on the available values in the same position in the rest of data samples;
Mean Supervised (MS): as in the previous case but the mean is computed on the samples of the
same class vectors only; K-Nearest Neighbor (KNN): as in the previous case but the mean is
computed on the K-Nearest Neighbors of the same class only. Since the objective here is to compare
the performance of obtained classifiers, we computed the accuracy (mean ± standard error of the
mean - s.e.m.) on the test dataset using all the methods for two cases of degree of separation between
classes (d = 0.0, 0.2), two levels of sparsity (K = 4, 32) and missing values in the training dataset
ranging from 25% to 95% as shown in Fig. 2. Our results show that the simultaneous algorithm
clearly outperforms all the sequential methods. A t-test was performed to evaluate the statistical
significance of the difference between our algorithm (Simul) and MS. It is interesting to note that,
when classes has some degree of separation (d = 0.2), using simple methods as computing the mean
or filling with zeros, can give acceptable results but not better than our simultaneous algorithm.
o Simult.
-∙- Seq. Sp.
—ZF
v MU
MSMS
T- KNN-10
□ KNN-20
T- KNN-50
□ KNN-100
* p < 0.05
K=4, d =0.0	K=32, d=0.0	K=4, d=0.2	K=32, d =0.2
ι.o
0.9
0.8
0.7
0.6
0.5
0.25	0.50	0.75	0.25	0.50	0.75	0.25	0.50	0.75	0.25	0.50	0.75
Missing entries	Missing entries	Missing entries	Missing entries
Figure 2: Experimental results on synthetic dataset using our “simultaneous” (red thick solid line) compared
to various “sequential” methods. Obtained test accuracy (mean ± s.e.m computed over 10 realizations) as
a function of the percentage of missing entries for separation of classes d = 0.0, 0.2 and levels of sparsity
K = 4, 32. Statistical significance for the Simul-MS difference is shown (p < 0.05).
We also applied our algorithm to three popular computer vision datasets: MNIST (LeCun et al.,
1989) and Fashion (Xiao et al., 2017) consisting of 70,000 images (60,000 train + 10,000 test) each;
and CIFAR10 (Krizhevsky & Hinton, 2009) having 60,000 images (50,000 train + 10,000 test).
MNIST/Fashion datasets contains 28 × 28 gray scale images while CIFAR10 dataset is built upon
32 × 32 × 3 color images of different objects. The corresponding data sample size is N = 28 × 28 = 784
for MNIST/Fashion and N = 32 × 32 × 3 = 3, 072 for CIFAR10. We considered a dictionary of size
784 × 784 (MNIST/Fashion) and 1, 024 × 1, 024 (CIFAR10) and applied our simultaneous algorithm
to learn the classifier on incomplete data for these datasets using uniform random missing masks
at several levels of missing entries (25%, 50% and 75%) and for 50% random partial occlusions for
MNIST/Fashion. We used a logistic regression classifier (single layer neural network) and a 4-layer
convolutional neural network (CNN4) for the MNIST/Fashion dataset using batch normalization
(BN) in the Fashion dataset, while for CIFAR10 dataset, a residual neural network (Resnet-18, (He
et al., 2016)) was implemented. Table 1, columns 3rd through 10th, show the accuracy obtained in
the test step, when the model was trained on incomplete data and applied to incomplete as well as
7
Under review as a conference paper at ICLR 2020
Dataset	Classifier	Random missing entries %Train / %Test						Occlusion %Train / %Test		Baseline %Train / %Test
		75/75	50/50	25/25	75/0	50/0	25/0	50/50	50/0	070
MNIST	Log. Reg.	90.45	93.68	94.14	91.94	93.44	94.43	-	-	9195
	CNN4	94.62	98.34	98.94	98.14	98.94	98.95	88.55	-91.37-	98:95
Fashion	CNN4+BN	83.71	86.09	86.38	86.39	87.11	87.04	81.73	82.47	90776
CIFAR10	Resnet18	43.73	47.27	47.15	44.07	45.87	46.45	-	-	86.22
Table 1: Test accuracy: columns 3rd through 10th show the results with incomplete training data and
incomplete/complete test data. The baseline results were obtained by training the model on complete data.
to complete test data. The right-most column shows the baseline results obtained by training the
model on complete training dataset using the implementations found in 2 and 3 . We implemented
our algorithm in Pytorch 1.0.0 and ran the experiments on a single GPU. Note that for the logistic
regression classifier, we obtained better results when training with incomplete data rather than
using complete data. Also, it is highlighted that training on incomplete data, with 50% or less
random missing entries, provides the same test accuracy as training on complete data for MNIST
dataset. The hyper-parameters λ1 and λ2 were adjusted by cross-validation through a grid-search,
as shown in Appendix C. In Fig. 3 we present some selected examples comparing the original images
in the MNIST/Fashion test dataset, their observations using random masks and partial occlusions,
and reconstructions computed using their sparse representation over a dictionary learned from the
incomplete training data. Additional visual examples are provided in Appendix C.
6 Conclusions
We demonstrated that assuming a sparse representation model for input data vectors allows the
successful learning of a general classifier on incomplete data. We analyzed the limitations of
the classical imputation approach and demonstrated through experiments that our simultaneous
algorithm always outperforms sequential methods for various cases such as LASSO, zero-filling,
supervised/unsupervised mean and KNN based methods. Our approach is conceptually similar
to the work in Ghahramani & Jordan (1993) but, instead of using a probabilistic model, we use
sparse coding, which is known to fit very well for natural signals. It is also noted that in Huang
& Aviyente (2006), sparse representations of data vectors were incorporated in the optimization
process for maximizing the Fisher discriminant in a context of classification. However, no theoretical
guarantee was presented in that paper and our approach can be applied to any deep learning classifier
architecture by simply incorporating a sparse data model, and adding two regularization terms to
the objective function: one to measure the accuracy of the sparse model to explain the observations,
and another to favor sparsity of the data representation, which is based on the £ 1 norm.
2
https://github.com/pytorch/examples/tree/master/mnist
3
https://github.com/kuangliu/pytorch-cifar
8
Under review as a conference paper at ICLR 2020
References
M Aharon, M Elad, and A Bruckstein. K-SVD: An Algorithm for Designing Overcomplete Dictio-
naries for Sparse Representation. Signal Processing, IEEE Transactions on, 54(11):4311-4322,
2006.
Gustavo E A P A Batista and Maria Carolina Monard. A Study of K-Nearest Neighbour as an
Imputation Method. HIS, 2002.
Yoshua Bengio and Francois Gingras. Recurrent Neural Networks for Missing or Asynchronous Data.
NIPS, 1995.
Cesar F Caiafa and A Cichocki. Computing Sparse representations of multidimensional signals using
Kronecker bases. Neural Computation, pp. 186-220, December 2012.
E.J. Candes and T Tao. Decoding by Linear Programming. Institute of Electrical and Electronics
Engineers. Transactions on Information Theory, 51(12):4203-4215, December 2005.
Emmanuel J Candes and Terence Tao. The Power of Convex Relaxation: Near-Optimal Matrix
Completion. Institute of Electrical and Electronics Engineers. Transactions on Information Theory,
56(5):2053-2080, April 2010.
Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete
and inaccurate measurements. Communications on pure and applied mathematics, 59(8):1207-1223,
2006.
D L Donoho. Compressed sensing. Institute of Electrical and Electronics Engineers. Transactions
on Information Theory, 52(4):1289-1306, March 2006.
YC Eldar and Gitta Kutyniok. Compressed Sensing: Theory and Applications. New York: Cambridge
Univ. Press, 20:12, 2012.
M J Fadili, J-L Starck, and F Murtagh. Inpainting and Zooming Using Sparse Representations. The
Computer Journal, 52(1):64-79, February 2008.
Francoise Fessant and Sophie Midenet. Self-Organising Map for Data Imputation and Correction in
Surveys. Neural Computing and Applications, 10(4):300-310, 2002.
Pedro J Garcia-Laencina, Jose-Luis Sancho-Gomez, and Anibal R Figueiras-Vidal. Pattern classifica-
tion with missing data: a review. Neural Computing and Applications, 19(2):263-282, September
2009.
Zoubin Ghahramani and Michael I Jordan. Supervised learning from incomplete data via an EM
approach. NIPS, 1993.
Karol Gregor and Yann LeCun. Learning Fast Approximations of Sparse Coding. ICML, 2010.
Trevor Hastie, Robert Tibshirani, and Jerome H Friedman. The elements of statistical learning -
data mining, inference, and prediction, 2nd Edition. Springer series in statistics, 2009.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR, pp.
770-778. IEEE, 2016.
Ke Huang and Selin Aviyente. Sparse Representation for Signal Classification. NIPS, 2006.
9
Under review as a conference paper at ICLR 2020
Sheng-Jun Huang, Miao Xu, Ming-Kun Xie, Masashi Sugiyama, Gang Niu, and Songcan Chen.
Active Feature Acquisition with Supervised Matrix Completion. arXiv.org, pp. arXiv:1802.05380,
February 2018.
A Krizhevsky and G Hinton. Learning multiple layers of features from tiny images. PhD thesis,
Toronto University, 2009.
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E
Hubbard, and Lawrence D Jackel. Handwritten Digit Recognition with a Back-Propagation
Network. NIPS, 1989.
Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y Ng. Efficient sparse coding algorithms. In
Advances in Neural Information Processing Systems, pp. 801-808. Stanford University, Palo Alto,
United States, December 2007.
Roderick J A Little and Donald B Rubin. Statistical Analysis with Missing Data. John Wiley &
Sons, August 2014.
Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. Tensor Completion for Estimating
Missing Values in Visual Data. IEEE Transactions on Pattern Analysis and Machine Intel ligence,
35(1):208-220, November 2012.
Julien Mairal, Jean Ponce, Guillermo Sapiro, Andrew Zisserman, and Francis R Bach. Supervised
Dictionary Learning. In Advances in Neural Information Processing Systems (NIPS), 2008.
B A Olshausen and D J Field. Emergence of simple-cell receptive field properties by learning a
sparse code for natural images. Nature, 381(6583):607-609, June 1996.
BA Olshausen and DJ Field. Sparse coding with an overcomplete basis set: A strategy employed by
V1? Vision research, 37(23):3311-3325, 1997.
Ignacio Ramirez, Pablo Sprechmann, and Guillermo Sapiro. Classification and clustering via
dictionary learning with structured incoherence and shared features. In Proceedings of the IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3501-3508.
University of Minnesota System, United States, IEEE, August 2010.
J Sole-Casals, C F Caiafa, Q Zhao, and A Cichocki. Brain-ComPuter Interface with Corrupted EEG
Data: a Tensor Completion Approach. Cognitive Computation, 10(6):1062-1074, July 2018.
Pablo Sprechmann and Guillermo Sapiro. Dictionary learning and sparse coding for unsupervised
clustering. In ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing
- Proceedings, pp. 2042-2045. University of Minnesota System, United States, IEEE, November
2010.
Ivana Tosic and Pascal Frossard. Dictionary Learning. IEEE Signal Processing Magazine, 28(2):
27-38, March 2011.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Bench-
marking Machine Learning Algorithms. arXiv.org, pp. arXiv:1708.07747, August 2017.
Song Yee Yoon and Soo Young Lee. Training algorithm with incomplete data for feed-forward neural
networks. Neural Processing Letters, 10(3):171-179, January 1999.
Longhao Yuan, Chao Li 0013, Danilo P Mandic, Jianting Cao, and Qibin Zhao. Tensor Ring
Decomposition with Rank Minimization on Latent Space - An Efficient Approach for Tensor
Completion. arXiv, cs.LG:arXiv:1809.02288, 2018.
10
Under review as a conference paper at ICLR 2020
A Proof of Theorem 3.2
Proof. Let us first consider the case with yi = 1. We assume that we are able to find a perfect
classifier on the reconstructed full data vector such that
f(x i) = W T Xi + b> e> 0,
(6)
where xi = Di Si. Now, We note that the K-sparse representation solution given the observation is
non-unique, i.e. ∃s %, Si ∈ Σ K, such that X i = D i S i = Di Si. We assume that S i is the true K -sparse
vector such that the unobserved full vector is Xi = DiSi and Si provides an alternative reconstruction
through Di which explains the observations but Xi = Xi.
Analogously to equation (1), we can find a partition of the reconstructed vector Xi as follows:
X i
zi
L ~ ~l
D i
同
s%,
(7)
where Zi ∈ R(N-M), and we want to prove that f (Xi) > C implies f (xi) > 0, i.e. the original data
vector Xi (unobserved) is also correctly classified by the logistic regression with parameters {Wi , b}.
To that end we need to evaluate the following expression:
f (x i) = W T X i + b = W T X i + U T Z i + b,	(8)
where equation (3) was used. If we add and substract UTZi and arrange the terms in the previous
equation, we get:
f (Xi) = WTXi+ UTzi+ b + UT(zi- zi),	⑼
=f (Xi) + UTeiSi - UTEis	(Io)
The first term in the right-hand side of last equation is f (X i) > c> 0 so if we show that |u T E i S i —
UTEisi | < c, then the proof will be complete. To do so, we can write:
|UTEiSi - UTEiSi| ≤ |UTEiSi| + |UTEiSi|,	(II)
and
N-M	N	N-M	N
|uTEiSi| = I 工 Ui(m)工 Ei(m,n)si(n)∣ ≤ 工 |ui(m)| 工 |Ei(m,n)||si(n)|.	(12)
m=1	n=1	m=1	n=1
Since we assumed normalized vectors ∣∣x∕∣ ≤ 1, by applying the left-hand side of the RIP we
obtain: IISi∣∣ ≤ 1 /(1 — δκ), which implies that |Si(n)| ≤ 1 /(1 — δκ). Now, taking into ac-
count that |Ei (m, n)| ≤ 1 (columns of D are unit-norm) and using the fact that Si ∈ ΣLK
we obtain: |uTEiSi| ≤ .KK) EN-M |ui(m)|; and, similarly, we can obtain that |uTEisi| ≤
(iKκ)ENr二M |ui(m)|. Putting everything together into equation (11) we get:
2K	N-M
|UTEiSi- UTEisi| ≤ (1- δ )工 |Ui(m)| < C	(13)
( — K) m=1
where we used that EN-M |ui(m) | < (12KK), which completes the proof for the case yi = 1.
Similarly, it is strait-forward to prove that, if yi = 0, having f (Xi) < — E < 0 implies that
f (xi) < 0.	□
11
Under review as a conference paper at ICLR 2020
B Algorithms
Here, the pseudocode of the algorithms discussed in the paper are presented. In Algorithm 1, the
simultaneous classification and coding method is described. It consists in the iterative alternation
between the update of the classifier’s parameters together with the dictionary, and the update of
the sparse coefficients for the representation of data observations. Once the classifier is trained, we
are able to apply it to incomplete test data by using Algorithm 2, where for fixed Θ and D, we need
to find the corresponding sparse coefficients si , compute the full data vector estimations and, finally,
apply the classifier.
The standard sparsity-based imputation method is presented in Algorithm 3 (sequential approach),
which consists of learning first the optimal D and sparse coefficients si compatible with the incomplete
observations (dictionary learning and coding phase), followed by the training phase, where the
classifier is tuned in order to minimize the classification error of the reconstructed input data vectors
X i = Ds i.
Algorithm 1 : Simultaneous classification and coding (training on incomplete data)
Require: Incomplete data vectors and their labels {Xi,yi}, i = 1, 2,...,I, hyper-parameters α, λ 1 and λ2,
number of iterations Niter and update rate σ
Ensure: Classifier parameters Θ and sparse representation of full data vectors Xi = Dsi, ∀i
1:	Initialize Θ, D, si , ∀i randomly
2:	for n ≤ N do
3:	Fix si, update Θ and D:
4:	Θ = Θ - σ J, D = D - σ J
5:	Normalize columns of matrix D
6:	Fix Θ and D, update si , ∀i:
7:	∆ = — σ J, ∀i
8:	if si(j)[si(j) + ∆i(j)] < 0 then
9:	∆i(j) = -si(j); avoid zero crossing
10:	end if
11:	si = si + ∆i, ∀i
12:	end for
13:	return Θ, D, Si, Xi = Dsi, ∀i
Algorithm 2 : Testing on incomplete data
Require: Incomplete data vectors {Xi}, i = 1, 2,...,I, classifier parameters Θ, dictionary D, hyper-
parameters λ1 and λ2 , number of iterations Niter and update rate σ
Ensure: Class assigned to each vector yi and sparse representation of full data vectors Xi = Dsi, ∀i
1: Sparse coding stage: for fixed dictionary D find sparse representations of observations Xi
2: Initialize si , ∀i randomly
3: for n ≤ Niter do
4:	δ * i 11 = - σ [λ 1J + λ 2 J ], ∀i
5:	if si(j)[si(j) + ∆i(j)] < 0 then
6:	∆i(j) = -si(j); avoid zero crossing
7:	end if
8:	si = si - σ∆i , ∀i
9: end for
10: Xi = Dsi, ∀i; Compute reconstructions of unobserved vector data Xi
11: Classification stage: apply classifier to reconstructions Xi
12: yi = argmaxy(pΘ(X))
13: return Θ, yi, Si, Xi, ∀i
12
Under review as a conference paper at ICLR 2020
Algorithm 3 : Sequential approach (imputation method)
Require: Incomplete data vectors and their labels {Xi,yi}, i = 1, 2,...,I, hyper-parameters α, λ 1 and λ2,
number of iterations Niter and update rate σ
Ensure: Classifier parameters Θ and sparse representation of full data vectors Xi = Dsi, ∀i
1:	Initialize D, si , ∀i randomly
2:	Dictionary learning and coding stage: update D and si
3:	for n ≤ N do
4:	D = D - σ J
∂D
5:	Normalize columns of matrix D
6:	δi = -σ[λ 1J + λ2 J], ∀i
7:	if si(j)[si(j) + ∆i(j)] < 0 then
8:	∆i(j) = -si(j); avoid zero crossing
9:	end if
10:	si = si + ∆i, ∀i
11:	end for
12:	Xi = Dsi, ∀i; Compute reconstructions of unobserved vector data Xi
13:	Training stage: update Θ
14:	for n ≤ N do
15:	Θ = Θ — σd∂Θ; Update Θ:
16:	end for
17:	return Θ, D, Si, Xi = Dsi, ∀i
C Additional experimental results
C.1 Comparison to sequential methods on benchmark datasets
In Table 2, we compare our simultaneous algorithm (Simul) with the following standard sequential
methods: Zero Fill - ZF, Mean Supervised - MS, KNN-10, KNN-20, KNN-50 and KNN-100. We
computed the Test Accuracy on incomplete data using random masks for MNIST and CIFAR10
datasets. It is noted that the bests result using standard sequential method were obtained with
KNN-10 or KNN-20. However, the simultaneous algorithm outperforms KNN in all the cases
increasing the performance by approximately 10.% for MNIST and CIFAR10, when the percentage
of random missing features is 75%.
C.2 Hyperparameter tunning
In Table 3 and Figure 4 we present the results of the grid search for hyper-parameter tuning on
MNIST and CIFAR10 datasets. We fit our model to the training dataset for a range of values of
parameters λ1 and λ2 and apply it to the test data set. Figure 4 shows the accuracy obtained for the
test dataset with different classifiers and levels of missing entries. We chose the hyper-parameters
values such that the test accuracy is maximum, as shown in Table 3.
C.3 Additional visual results
To visually evaluate our results, additional selected examples of original (complete) images of the
test dataset in MNIST and Fashion, together with their given incomplete observations and obtained
reconstructions, are shown in Figure 5 and Figure 6
13
Under review as a conference paper at ICLR 2020
MNIST (CNN4)							
Missing	ZF	MS	KNN10	KNN20	KNN50	KNN100	SimuL
-75%-	84.14	83.69	88.10	-87.68-	-86.77-	-867T1-	98.14
50%	89.65	88.40	91.22	90.94	90.72	90.68	98.94
CIFAR10 (Resnet18)							
Missing	ZF	MS	KNN10	KNN20	KNN50	KNN100	SimuL
-75%-	18.7	21.98	33.10	34.78	-29.56-	-30.55-	44.07
50%	41.35	17.79	31.54	36.94	31.16	30.78	45.87
Table 2: Test Accuracy obtained on MNIST and CIFAR10 dataset using standard sequential methods and
compared to our simultaneous algorithm.
DataSet	Classifier	Random missing entries						Occlusion	
		75%		50%		25%		50%	—	
		λ1	ʌ 2	λ 1	ʌ 2	λ1	ʌ 2	λ 1	λ 2
MNIST	Log. Reg.	0.32	1.28	0.64	1.28	0.64	1.28	-	-
	CNN4 一	1.28	1.28	2.56	1.28	5.12	1.28	10.24	10.24
CIFAR10	ReSnet18	0.024	0.008	0.032	0.004	0.032	0.01	-	-
Table 3: Hyper-parameter tuning: crossvalidated hyperparameters λ1 and λ2 obtained for MNIST and
CIFAR10 datasets with the classifiers used in our experiments.
14
Under review as a conference paper at ICLR 2020
%
5
2
%
5
151BO5W-O5
(ZY)booɪ
.6ΦH .60-1，IS-NW
(<boOI
寸NN，IgNW

(<boOI
8匚əusəH，0<
75%
∩-0.9448
0.9300
0.910C
0.9000
0.8900
-0.8700
. 0.6000
■o.oooC
π 0.9894
0.9880
0.9750
0.9200
-0.8000
H 0.6000
B- 0.4000
・ 0.0800
πθ.4715
0.4620
0.4400
0.3800
0.3000
1-0.2000
∣-0.1000
∙θ.O8OO
-3	-2	-1	0	1
Iog(Ai)
ɪ
og(
1
og(
Figure 4: Testing accuracy in the grid search for hyper-parameter tuning: λι and λ2 were chosen through
crossvalidation by maximizing the testing accuracy in all datasets with different levels of missing entries:
25%, 50% and 75%.
15
Under review as a conference paper at ICLR 2020
Random mask	Occlusion
(75% missing entries) (50% missing entries)
16
Under review as a conference paper at ICLR 2020
Random mask
Occlusion
(75% missing entries)
(50% missing entries)
original observed reconst. original observed reconst.
Figure 6: Reconstructions of incomplete test Fashion dataset vectors by applying our simultaneous
classification and coding algorithm using the CNN4 architecture with Batch Normalization (BN).
17