Under review as a conference paper at ICLR 2020
Can AltQ Learn Faster: Experiments and
Theory
Anonymous authors
Paper under double-blind review
Ab stract
Differently from the popular Deep Q-Network (DQN) learning, Alternating Q-
learning (AltQ) does not fully fit a target Q-function at each iteration, and is
generally known to be unstable and inefficient. Limited applications of AltQ mostly
rely on substantially altering the algorithm architecture in order to improve its
performance. Although Adam appears to be a natural solution, its performance
in AltQ has rarely been studied before. In this paper, we first provide a solid
exploration on how well AltQ performs with Adam. We then take a further step to
improve the implementation by adopting the technique of parameter restart. More
specifically, the proposed algorithms are tested on a batch of Atari 2600 games and
exhibit superior performance than the DQN learning method. The convergence rate
of the slightly modified version of the proposed algorithms is characterized under
the linear function approximation. To the best of our knowledge, this is the first
theoretical study on the Adam-type algorithms in Q-learning.
1 Introduction
Q-learning (Watkins & Dayan, 1992) is one of the most important model-free reinforcement learning
(RL) problems, which has received considerable research attention in recent years (Bertsekas &
Tsitsiklis, 1996; Even-Dar & Mansour, 2003; Hasselt, 2010; Lu et al., 2018; Achiam et al., 2019).
When the state-action space is large or continuous, parametric approximation of the Q-function is often
necessary. One remarkable success of parametric Q-learning in practice is its combination with deep
learning, known as the Deep Q-Network (DQN) learning (Mnih et al., 2013; 2015). It has been applied
to various applications in computer games (Bhatti et al., 2016), traffic control (Arel et al., 2010),
recommendation systems (Zheng et al., 2018; Zhao et al., 2018), chemistry research (Zhou et al.,
2017), etc. Its on-policy continuous variant (Silver et al., 2014) has also led to great achievements in
robotics locomotion (Lillicrap et al., 2016).
The DQN algorithm is performed in a nested-loop manner, where the outer loop follows an one-step
update of the Q-function (via the empirical Bellman operator for Q-learning), and the inner loop
takes a supervised learning process to fit the updated (i.e., target) Q-function with a neural network.
In practice, the inner loop takes a sufficiently large number of iterations under certain optimizer (e.g.
stochastic gradient descent (SGD) or Adam) to fit the neural network well to the target Q-function.
In contrast, a conventional Q-learning algorithm runs only one SGD step in each inner loop, in
which case the overall Q-learning algorithm updates the Q-function and fits the target Q-function
alternatively in each iteration. We refer to such a Q-learning algorithm with alternating updates
as Alternating Q-learning (AltQ). Although significantly simpler in the update rule, AltQ is well
known to be unstable and have weak performance (Mnih et al., 2016). This is in part due to the
fact that the inner loop does not fit the target Q-function sufficiently well. To fix this issue, Mnih
et al. (2016) proposed a new exploration strategy and asynchronous sampling schemes over parallel
computing units (rather than the simple replay sampling in DQN) in order for the AltQ algorithm to
achieve comparable or better performance than DQN. As another alternative, Knight & Lerner (2018)
proposed a more involved natural gradient propagation for AltQ to improve the performance. All
these schemes require more sophisticated designs or hardware support, which may place AltQ less
advantageous compared to the popular DQN, even with their better performances. This motivates us
to ask the following first question.
1
Under review as a conference paper at ICLR 2020
•	Q1: Can we design a simple and easy variant of the AltQ algorithm, which uses as simple setup as
DQN and does not introduce extra computational burden and heuristics, but still achieves better
and more stable performance than DQN?
In this paper, we provide an affirmative answer by introducing novel lightweight designs to AltQ
based on Adam. Although Adam appears to be a natural tool, its performance in AltQ has rarely
been studied yet. Thus, we first provide a solid exploration on how well AltQ performs with Adam
(Kingma & Ba, 2014), where the algorithm is referred to as AltQ-Adam. We then take a further step
to improve the implementation of AltQ-Adam by adopting the technique of parameter restart (i.e.,
restart the initial setting of Adam parameters every a few iterations), and refer to the new algorithm
as AltQ-AdamR. This is the first time that restart is applied for improving the performance of RL
algorithms although restart has been used for conventional optimization before.
In a batch of 23 Atari 2600 games, our experiments show that both AltQ-Adam and AltQ-AdamR
outperform the baseline performance of DQN by 50% on average. Furthermore, AltQ-AdamR
effectively reduces the performance variance and achieves a much more stable learning process.
In our experiments for the linear quadratic regulator (LQR) problems, AltQ-AdamR converges
even faster than the model-based value iteration (VI) solution. This is a rather surprising result
given that the model-based VI has been treated as the performance upper bound for the Q-learning
(including DQN) algorithms with target update (Lewis & Vrabie, 2009; Yang et al., 2019).
Regarding the theoretical analysis of AltQ algorithms, their convergence guarantee has been ex-
tensively studied (Melo et al., 2008; Chen et al., 2019b). More references are given in Section 1.1.
However, all the existing studies focus on the AltQ algorithms that take a simple SGD step. Such
theory is not applicable to the proposed AltQ-Adam and AltQ-AdamR that implement the Adam-type
update. Thus, the second intriguing question we address here is described as follows.
•	Q2: Can we provide the convergence guarantee for AltQ-Adam and AltQ-AdamR or their slightly
modified variants (if these two algorithms do not always converge by nature)?
It is well known in optimization that Adam does not always converge, and instead, a slightly
modified variant AMSGrad proposed in Reddi et al. (2018) has been widely accepted as an
alternative to justify the performance of Adam-type algorithms. Thus, our theoretical analysis
here also focuses on such slightly modified variants AltQ-AMSGrad and AltQ-AMSGradR of
the proposed algorithms. We show that under the linear function approximation (which is the
structure that the current tools for analysis of Q-learning can handle), both AltQ-AMSGrad and
AltQ-AMSGradR converge to the global optimal solution under standard assumptions for Q-
learning. To the best of our knowledge, this is the first non-asymptotic convergence guarantee
on Q-learning that incorporates Adam-type update and momentum restart. Furthermore, a slight
adaptation of our proof provides the convergence rate for the AMSGrad for conventional strongly
convex optimization which has not been studied before and can be of independent interest.
Notations We use ∣∣χk := ∣∣χk2 = √XTχ to denote the '2 norm of a vector x, and use ∣∣χ∣∣∞ =
max∣xi∣ to denote the infinity norm. When x, y are both vectors, x/y, xy, x2, √x are all calculated
i
in the element-wise manner, which will be used in the update of Adam and AMSGrad. We denote
[n] = 1, 2, . . . , n, and bxc ∈ Z as the largest integer such that bxc ≤ x < bxc + 1.
1.1 Related Work
Empirical performance of AltQ: AltQ algorithms that strictly follow the alternating updates are
rarely used in practice, particularly in comparison with the well-accepted DQN learning and its
improved variants of dueling network structure (Wang et al., 2016), double Q-learning (Hasselt, 2010)
and variance exploration and sampling schemes (Schaul et al., 2015). Mnih et al. (2016) proposed the
asynchronous one-step Q-learning that is conceptually close to AltQ with competitive performance
against DQN. However, the algorithm still relies on a slowly moving target network like DQN,
and the multi-thread learning also complicates the computational setup. Lu et al. (2018) studied
the problem of value overestimation and proposed the non-delusional Q-learning algorithm that
employed the so-called pre-conditioned Q-networks, which is also computationally complex. Knight
& Lerner (2018) proposed a natural gradient propagation for AltQ to improve the performance, where
the gradient implementation is complex. In this paper, we propose two simple and computationally
efficient schemes to improve the performance of AltQ.
2
Under review as a conference paper at ICLR 2020
Theoretical analysis of AltQ: Since proposed in Watkins & Dayan (1992), Q-learning has aroused
great interest in theoretic analysis. The line of theoretic research of AltQ that are most relevant to our
study lies in the Q-learning with function approximation. A large number of works study Q-learning
with linear function approximation such as Bertsekas & Tsitsiklis (1996); Devraj & Meyn (2017);
Zou et al. (2019); Chen et al. (2019b); Du et al. (2019), to name a few. More recently, convergence of
AltQ with neural network parameterization was given in Cai et al. (2019), which exploits the linear
structure of neural networks in the overparamterized regime for analysis. It is worth noting that all
the existing analysis of AltQ with function approximation considers the simple SGD update, whereas
our analysis in this paper focuses on the more involved Adam-type updates.
Convergence analysis of Adam: Adam was proposed in Kingma & Ba (2014) and has achieved a
great success in training deep neural networks. Kingma & Ba (2014) and Reddi et al. (2018) provided
regret bounds under the online convex optimization framework for Adam and AMSGrad, respectively.
However, Tran et al. (2019) pointed out errors in the proofs of the previous two papers and corrected
them. Recently, convergence analysis of Adam and AMSGrad in nonconvex optimization was
provided in Zou et al. (2018); Zhou et al. (2018); Chen et al. (2019a); Phuong & Phong (2019), in
which the Adam-type algorithms were guaranteed to converge to a stationary point. To the best of our
knowledge, our study is the first convergence analysis of the Adam-type of algorithms for Q-learning.
2 Preliminaries
We consider a Markov decision process with a considerably large or continuous state space S ⊂ RM
and action space A ⊂ RN with a non-negative bounded reward function R : S × A → [0, Rmax]. We
define U(s) ⊂ A as the admissible set of actions at state s, and π : S → A as a feasible stationary
policy. We seek to solve a discrete-time sequential decision problem with γ ∈ (0, 1) as follows:
∞
X γtR(st, π(st))
t=0
maximize Jπ (s0) = EP
π
subject to st+ι 〜P(∙∣st,at).
(1)
Let J?(s) := Jπ? (s) be the optimal value function when applying the optimal policy π?. The
corresponding optimal Q-function can be defined as
Q?(s, a) := R(s, a) + YEP J?(s0),	(2)
where s0 〜P(∙∣s, a) and We use the same notation hereafter when no confusion arises. In other
words, Q?(s, a) is the reward of an agent who starts from state s and takes action a at the first step
and then follows the optimal policy π? thereafter.
2.1	AltQ Algorithm
This paper focuses on the Alternating Q-learning (AltQ) algorithm that uses a parametric function
Q(s, a; θ) to approximate the Q-function with a parameter θ of finite and relatively small dimension.
The update rule of AltQ-learning is given by
00
T Q(s, a; θt) = R(s, a) + γ max Q(s0, a0; θt);
a0∈U(s0)
θt+1 = θt - αt
(Qt(S,a; θt) - TQ(s,a; θt))
(3)
(4)
where αt is the step size at time t. It is immediate from the equations that AltQ performs the update
by taking one step temporal target update and one step parameter learning in an alternating fashion.
2.2	DQN Algorithm
As DQN is also included in this work for performance comparison. We recall the update of DQN
in the following as reference. Differently from AltQ, DQN updates the parameters in a nested loop.
Within the t-th inner loop, DQN first obtains the target Q-function as in Equation (5), and then uses a
3
Under review as a conference paper at ICLR 2020
neural network to fit the target Q-function by running Y steps of a certain optimization algorithm
as Equation (6). The update rule of DQN is given as follows.
TQ(S, a； θ0) = R(s,a) + Y maχ、Q(s0,a'； θ0),	(5)
a0∈U(s0)
θγ = Optimizer(θt0, TQ^(s, a; θ0)),	(6)
where Optimizer can be SGD or Adam for example, and Equation (6) is thus a supervised learning
process with TQ(s, a; θ0)) as the "supervisor”. At the t-th outer loop, DQN performs the so-called
target update as
θt0+1 = (1 -τ)θt0 +τθtY.	(7)
In practice, when one of the momentum-based optimizers is adopted for Equation (6), such as Adam,
it is only initialized once at the beginning of the first inner loop. The historical gradient terms then
accumulate throughout multiple inner loops with different targets. While this stabilizes the DQN
training empirically, it is still lack of theoretical understanding on how the optimizer affects the
training with various moving targets. As we will discuss in detail in Section 5, the analysis of AltQ
with Adam can potentially shed light on such ambiguity and inspire future work for this matter.
Note that AltQ and DQN mainly differ at how the Q-function evolves after each step of sampling.
A fair comparison between the algorithms should be made without introducing dramatic difference
on gradient propagation (Knight & Lerner, 2018), policy structure, exploration and sampling strate-
gies (Mnih et al., 2016). In practice, the vanilla AltQ is often slow in convergence and unstable with
high variance. To improve the performance, we propose to incorporate Adam and restart schemes,
which are easy to implement and yield improved performance than DQN.
3	Accelerated Alternating Q-learning Algorithms
In this section, we first describe how to incorporate Adam to the AltQ algorithm, and then introduce
a novel implementation scheme to improve the performance of AltQ with Adam.
AltQ with Adam-type update We propose a new AltQ algorithm with Adam-type update (AltQ-
Adam) as described in Algorithm 1. Its update is similar to the well-known Adam (Kingma & Ba,
2014). The iterations evolve by updating the exponentially decaying average of historical gradients
(mt) and squared historical gradients (vt). The hyper-parameters β1, β2 are used to exponentially
decrease the rate of the moving averages. The difference between Algorithm 1 and the standard Adam
in supervised learning is that in AltQ, there is no fixed target to “supervise” the learning process.
The target is always moving along with iteration t, leading to more noisy gradient estimations. The
proposed algorithm sheds new light on the possibility of using Adam to deal with such unique
challenge brought by RL.
AltQ-Adam with momentum restart We also introduce the restart technique to AltQ-Adam and
propose AltQ-AdamR as Algorithm 2. Traditional momentum-based algorithms largely depend on
the historical gradient direction. When part of the historical information is incorrect, the estimation
error tends to accumulate. The restart technique can be employed to deal with this issue. One way to
restart the momentum-based methods is to initialize the momentum at some restart iteration. That is,
at restart iteration r, we reset mr , vr, i.e., mr = 0, vr = 0, which yields θr+1 = θr. It is an intuitive
implementation technique to adjust the trajectory from time to time, and can usually help mitigate
the aforementioned problem while keeping fast convergence property. For the implementation, we
execute the restart periodically with a period r. It turns out that the restart technique can significantly
improve the numerical performance, which can be seen in Section 4.
4	Empirical Performance
We empirically evaluate the proposed algorithms in this section. The linear quadratic regulator (LQR)
is a direct numerical demonstration of the convergence analysis under linear function approximation
which will be discussed in the next section. Atari 2600 game (Bellemare et al., 2013; Brockman
et al., 2016), a classic benchmark for DQN evaluations, is also used to show the effectiveness of
the proposed algorithms for complicated tasks. In practice, we also make a small adjustment to
the proposed algorithms. That is, we re-scale the loss term of L(θt) := Qt(s, a; θt) - T Q(s, a; θt)
4
Under review as a conference paper at ICLR 2020
Algorithm 1 AltQ-Adam
1	: Input: η, θ1, β1, β2, , γ, m0 = 0, v0 = 0.
2	: for t = 1, 2, . . . , K do
3	:	T Q(S, a; θt) = R(S, a) + γ maxa0 Q(S0,a0; θt)
4	gt = (Qt(S, a; θt) - TQ(S, a; θt)J ∂θtQt(S, a; θt)
5	:	mt = (1 - β1)mt-1 + β1gt
6	:	vt = (1 - β2 )vt-1 + β2gt2
7	θt+1 = θt - η √⅛
8	: end for
9	: Output: θK
Algorithm 2 AltQ-AdamR
1:	Input: η, θ1 , β1 , β2, , γ, m0 = 0, v0 = 0, r.
2:	for t = 1, 2, . . . , K do
3:	if mod(t, r) = 0 then
4:	mt = 0, vt = 0
5:	end if
6:	T Q(s, a; θt) = R(s, a) + γ maxa0 Q(s0, a0; θt)
7:	gt = (Qt(S, a; θt) - TQ(S, a; θt)J ∂θt Qt(S, a; θt )
8:	mt = (1 - β1)mt-1 + β1gt
9:	vt = (1 - β2)vt-1 + β2gt2
10:	θt+ι = θt-η √m⅛
11:	end for
12:	Output: θK
in Equation (4) as L(θt) = τ2L(θt) with some scaling factor T ∈ (0,1], which is beneficial for
stabilizing the learning process.
We find that in both experiments, AltQ-AdamR outperforms both AltQ-Adam and DQN in terms
of convergence speed and variance reduction. Compared with DQN in the empirical experiments of
Atari games, under the same hyper-parameter settings, AltQ-Adam and AltQ-AdamR improve the
performance of DQN by 50% on average.
4.1	Linear Quadratic Regulator
We numerically validate the proposed algorithms through an infinite-horizon discrete-time LQR
problem whose background can be found in Appendix A.1. A typical model-based solution (with
known dynamics), known as the discrete-time algebraic Riccati equation (DARE), is adopted to derive
the optimal policy ut? = -K?xt. The performance of the learning algorithm is then evaluated at each
step of iterate t with the Euclidean norm kKt - K? k. The performance result for each method is
averaged over 10 trials with different random seeds. All algorithms share the same set of random seeds
and are initialized with the same θ0 . The hyper-parameters of the learning settings are also consistent
and further details are shown in Table 1. Note that for all the implementations, we also adopt the
double Q-update (Hasselt, 2010) to help prevent over-estimations of the Q-value. The performance
results are seen in Figure 1. Here we highlight main observations from the LQR experiments.
•	AltQ-AdamR outperforms DARE In ideal cases where data sampling perfectly emulates the
system dynamics and the target is accurately learned in each inner loop, DARE for LQR would
become equivalent to the DQN-like update if the neural network is replaced with a parameterzied
linear function. In practice, such ideal conditions are difficult to satisfy, and hence the actual
Q-learning with target update is usually far slower (in terms of number of steps of target updates)
than DARE. Note that AltQ-AdamR performs significantly well and even converges faster than
DARE, and thus implies it is faster than the most well-performing Q-learning with target update.
•	AltQ-AdamR outperforms AltQ-Adam Overall, under the same batch sampling scheme and
restart period, AltQ-AdamR achieves a faster convergence and smaller variance than AltQ-Adam.
5
Under review as a conference paper at ICLR 2020
Figure 1: LQR experiments with performance Figure 2: Atari game experiment with perfor-
evaluated in terms of policy loss kKt - K?k2. mance normalized and averaged over 23 games.
Table 1: Hyper-parameters for LQR experiments
Step size T	Adam βι	Adam β2	Restart period r Stop criterion Y
0.0001	0.01	0.9	0.999	100	kKi - K?k2 ≤ 10-4	1
4.2 Atari Games
We apply the proposed AltQ algorithms to more challenging tasks of deep convolutional neural
network playing a group of Atari 2600 games. The particular DQN we train to compare against
adopts the dueling network structure (Wang et al., 2016), double Q-learning setup (Van Hasselt et al.,
2016), -greedy exploration and experience replay (Mnih et al., 2013). Adam is also adopted, without
momentum restart, as the optimizer for the inner-loop supervised learning process. AltQ-Adam and
AltQ-AdamR are implemented using the identical setup of network construction, exploration and
sampling strategies.
We test all the three algorithms with a batch of 23 Atari games. The choice of 10 million steps of
iteration is a common setup for benchmark experiments with Atari games. Although this does not
guarantee the best performance in comparison with more time-consuming training with 50 million
steps or more, it is sufficient to illustrate different performances among the selected methods. The
software infrastructure is based on the baseline implementation of OpenAI. Selections of the hyper-
parameters are listed in Table 2. We summarize the results in Figure 2. The overall performance is
illustrated by first normalizing the return of each method with respect to the results obtained from
DQN, and then averaging the performance of all 23 games to obtain the mean return and standard
deviation. Considering we use a smaller buffer size than common practice, DQN is not consistently
showing improved return over all tested games. Therefore, the self-normalized average return of
DQN in Figure 2 is not strictly increasing from 0 to 100%.
Overall, both AltQ-Adam and AltQ-AdamR achieve significant improvement in comparison with
the DQN results. While AltQ-Adam is suffering from a higher variance, periodic restart (AltQ-
AdamR) resolves the issue efficiently with an on-par performance on average and far smaller variance.
Specifically, in terms of the maximum average return, AltQ-Adam and AltQ-AdamR perform no
worse then DQN on 17 and 20 games respectively out of the 23 games being evaluated.
5 Convergence Analysis
In this section, we characterize the convergence guarantee for the proposed AltQ-learning algorithms.
Furthermore, like most of the related papers, we focus on convergence analysis under the linear
approximation class. Understanding the analytical behavior in the linear case is an important stepping
stone to understand general cases such as deep neural network. A linear approximation of the
Q-function Q(s, a; θ) can be written as
Q(S, a; θ) = Φ(s,a)Tθ,	(8)
where θ ∈ Rd , and Φ : S × A → Rd is a vector function of size d, and the elements of Φ represent
the nonlinear kernel (feature) functions.
6
Under review as a conference paper at ICLR 2020
Table 2: Hyper-parameters for Atari games experiments of DQN, AltQ-Adam and AltQ-AdamR
Step size 0.0001	Scale factor T 0.0001	Adam β1 Adam β2 0.9	0.999	Restart period r 104	Buffer size 105
γ 0.99	Batch size B 32	Total training steps K 107	Target update frequency (DQN only) 104	
5.1	Modification of Algorithms
Although Adam has obtained great success as an optimizer in deep learning, it is well known that
Adam by nature is non-convergent even for simple convex loss functions (Reddi et al., 2018). Instead, a
slightly modified version called AMSGrad (Reddi et al., 2018) is widely used to study the convergence
property of the Adam-type algorithms. Compared with the update rule of Adam, AMSGrad makes
the sequence Vt,i increasing along the time step t for each entry i ∈ [d]. Here, We apply the update
rule of AMSGrad to the AltQ algorithm and refer to such an algorithm as AltQ-AMSGrad. Algorithm
3 describes AItQ-AMSGrad in detail, where ∏d V1/4 (θ0) = min Iw1/4 (θ0 - θ)∣∣. Correspondingly,
We introduce AltQ-AMSGradR which applies the same update rule as Algorithm 3, but resets mt, Vt
with a period of r, i.e., mt = 0,Vt = 0, ∀t = kr,k = 1, 2, ∙∙∙.
Algorithm 3 AltQ-AMSGrad
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
Input: α, λ, Θ1,β1, β2,m0 = 0,Vo = 0.
for t = 1, 2, . . . , T do
αt = √αt, β1t = β1λt
gt = φT (st, at)θt - r(st, at) - ma0xφT(st+1,
mt = (1 - β1t)mt-1 + β1tgt
Vt = (1 - β2)Vt-l + β2g2
Vt = max(Vt-i,Vt), Vt = diag(vι, ...,Vd)
θt+ι = πd Vy (θt - atVt 2 mt)
end for
Output： 1 PT=ι θt
a0
)θt φ(st, at)
5.2	Main results
Our theoretical analysis here focuses on the slight variants, AltQ-AMSGrad and AltQ-AMSGradR.
Before stating the theorems, we first introduce some technical assumptions for our analysis.
Assumption 1. At each iteration t, the noisy gradient is unbiased and uniformly bounded, i.e.
gt = gt + ξt with Eξt = 0 where gt = E[gt] ,and Ilgtk < G∞,∀t .Thus IlgtI∣∞ < G∞ and
kgtk2 < G2∞.
Assumption 2. (Chen et al., 2019b, Lemma 6.7) The equation g(θ) = 0 has a unique solution θ?,
which implies that there exists a c > 0, such that for any θ ∈ Rd we have
(θ -θ*)t g(θ) ≥ C ∣θ -θ*k2.	(9)
Assumption 3. The domain D ⊂ Rd of approximation parameters is a ball originating at θ = 0 with
bounded diameter containing θ?. That is, there exists D∞, such that Iθm - θnI < D∞, ∀θm, θn ∈ D,
and θ? ∈ D.
Assumption 1 is standard in the theoretical analysis of Adam-type algorithms (Chen et al., 2019a;
Zhou et al., 2018). Under linear function approximation and given Assumption 3 and bounded r(∙),
Assumption 1 is almost equivalent to the assumption of bounded φ(∙) which is commonly taken in
related RL work (Tsitsiklis & Van Roy, 1997; Bhandari et al., 2018). Assumption 2 has been proved
as a key technical lemma in Chen et al. (2019b) under certain assumptions. Such an assumption
appears to be the weakest in the existing studies of the theoretic guarantee for Q-learning with
function approximation.
7
Under review as a conference paper at ICLR 2020
We next provide the convergence results of AltQ-AMSGrad and AltQ-AMSGradR under linear
function approximation in the following two theorems.
Theorem 1.	(Convergence of AltQ-AMSGrad) Suppose at = √, βιt = βιλt and δ = β1∕β2 with
δ,λ ∈ (0,1) for t = 1, 2,... in Algorithm 3. Given Assumptions 1 〜3, the outputof AltQ-AMSGrad
satisfies:
E kθout - θ*k≤ B + 早 + B3√1 +log T XX E k"ik,	(10)
T T	T	i=1
Where BI = 2αG∞D⅛) + 2αcβ1G∞D1-λ)2 + kθι — θ?k2 ,B2 =	， and B3 =
______α(1+βι)
2c(1-βι)2(1-δ)√1-β2 .
In Theorem 1, B1, B2, B3 in the bound in Equation (10) are constants and independent of time.
Therefore, under the choice of the stepsize and hyper-parameters in Algorithm 3, AltQ-AMSGrad
achieves a convergence rate of O (√τ) when Pd=ι kgi：T,i k << √T which is justified in DUchi
et al. (2011).
Remark 1. Our proof of convergence here has two major differences from that for AMSGrad in Reddi
et al. (2018): (a) The two algorithms are quite different. AltQ-AMSGrad is a Q-learning algorithm
alternatively finding the best policy, whereas AMSGrad is an optimizer for conventional optimization
and does not have alternating nature. (b) Our analysis is on the convergence rate whereas Reddi et al.
(2018) provides regret bound. In fact, a slight modification of our proof also provides the convergence
rate of AMSGrad for conventional strongly convex optimization, which can be of independent interest.
Moreover, our proof avoids the theoretical error in the proof in Reddi et al. (2018) pointed out by Tran
et al. (2019).
In the following theorem, we provide the convergence result for AltQ-AMSGradR.
Theorem 2.	(Convergence of AltQ-AMSGradR) Under the same condition of Theorem 1, the output
of AltQ-AMSGradR satisfies:
E kθout - θ? k≤ B + B2 √1 +log T XX E kgnτ,ik + B (√T + bXXc kɪ
i=1	k=1
ι bT/rc (G D2	_
十 ^ ^X ( ———∞√kr + 2 + 4c(1 - βI)E kθkr - θ*k2
Tα
k=0
(11)
where Bl =______β1D∞G∞_______ BD =________α(1+β1)______ and BQ = dG∞D∞
where BI = 2αc(1-βι)(1-λ)2 ,B2 = 2c(1-βι)2(1-δ)√1-β2, and B3 = 2αc(1-βι).
Theorem 2	indicates that for AltQ-AMSGradR to enjoy a convergence rate of O (√T) the restart
period r needs to be sufficiently large and Pid=ι kgi：T,ik << √. In practice as demonstrated by
the experiments in Section 4, AltQ-AMSGradR typically performs well, not necessarily under the
theoretical conditions.
6 Conclusion
We propose two types of the accelerated AltQ algorithms, and demonstrate their superior performance
over the state-of-the-art through a linear quadratic regulator problem and a batch of 23 Atari 2600
games.
Notably, Adam is not the only scheme in the practice for general optimization. Heavy ball (Ghadimi
et al., 2015) and Nesterov (Nesterov, 2013) are also popular momentum-based methods. When
adopting such methods in AltQ-learning for RL problems, however, we tend to observe a less stable
learning process than AltQ-Adam. This is partially caused by the fact that they optimize over a
shorter historical horizon of updates than Adam. Furthermore, the restart scheme provides somewhat
remarkable performance in our study. It is thus of considerable future interest to further investigate
the potential of such a scheme. One possible direction is to develop an adaptive restart mechanism
with changing period determined by an appropriately defined signal of restart. This will potentially
relieve the effort in hyper-parameter tuning of finding a good fixed period.
8
Under review as a conference paper at ICLR 2020
References
Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep
Q-learning. arXiv preprint arXiv:1903.08894, 2019.
Itamar Arel, Cong Liu, T Urbanik, and A.G. Kohls. Reinforcement learning-based multi-agent system
for network traffic signal control. IET Intelligent TransPort Systems, 4(2):128-135, 2010.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Dimitri P. Bertsekas and John N Tsitsiklis. Neuro-Dynamic Programming, volume 5. Athena
Scientific, 1996.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. arXiv PrePrint arXiv:1806.02450, 2018.
Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N. Siddharth, and Philip H.S.
Torr. Playing doom with slam-augmented deep reinforcement learning. arXiv PrePrint
arXiv:1612.00380, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv PrePrint arXiv:1606.01540, 2016.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning
converges to global optima. arXiv PrePrint arXiv:1905.10027, 2019.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of Adam-type
algorithms for non-convex optimization. In International Conference on Learning RePresentations,
2019a.
Zaiwei Chen, Sheng Zhang, Thinh T. Doan, Siva Theja Maguluri, and John-Paul Clarke. Finite-time
analysis of Q-learning with linear function approximation. arXiv PrePrint arXiv:1905.11425,
2019b.
Adithya M Devraj and Sean P Meyn. Fastest convergence for Q-learning. arXiv PrePrint
arXiv:1707.03770, 2017.
Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efficient Q-learning with func-
tion approximation via distribution shift error checking oracle. arXiv PrePrint arXiv:1906.06321,
2019.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, Jul 2011.
Eyal Even-Dar and Yishay Mansour. Learning rates for Q-learning. Journal of Machine Learning
Research, 5:1-25, Dec 2003.
Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the
heavy-ball method for convex optimization. In Proceeding of IEEE EuroPean Control Conference
(ECC), pp. 310-315, 2015.
Hado V Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems, pp.
2613-2621, 2010.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv PrePrint
arXiv:1412.6980, 2014.
Ethan Knight and Osher Lerner. Natural gradient deep Q-learning. arXiv PrePrint arXiv:1803.07482,
2018.
F. L. Lewis and D. Vrabie. Reinforcement learning and adaptive dynamic programming for feedback
control. IEEE Circuits and Systems Magazine, 9(3):32-50, Third 2009. doi: 10.1109/MCAS.2009.
933854.
9
Under review as a conference paper at ICLR 2020
Frank L Lewis and Kyriakos G Vamvoudakis. Reinforcement learning for partially observable dy-
namic processes: Adaptive dynamic programming using measured output data. IEEE Transactions
on Systems, Man, and Cybernetics, PartB (Cybernetics), 41(1):14-25, 2011.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016.
Tyler Lu, Dale Schuurmans, and Craig Boutilier. Non-delusional Q-learning and value-iteration. In
Proceedings of the Thirty-second Conference on Neural Information Processing Systems (NeurIPS-
18), pp. 9971-9981, Montreal, QC, 2018.
Borislav Mavrin, Hengshuai Yao, and Linglong Kong. Deep reinforcement learning with decorrelation.
arXiv preprint arXiv:1903.07765, 2019.
Francisco S. Melo, Sean P. Meyn, and M. Isabel Ribeiro. An analysis of reinforcement learning
with function approximation. In Proceedings of the 25th International Conference on Machine
Learning, ICML ’08, pp. 664-671, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles
Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane
Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature,
518(7540):529, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Proceedings of The 33rd International Conference on Machine Learning, volume 48,
pp. 1928-1937. PMLR, 20-22 Jun 2016.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Springer
Science & Business Media, 2013.
Tran Thi Phuong and Le Trieu Phong. On the convergence proof of AMSGrad and a new version.
arXiv preprint arXiv:1904.03590, 2019.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
International Conference on Learning Representations, 2018.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In ICML, 2014.
Phuong Thi Tran et al. On the convergence proof of AMSGrad and a new version. IEEE Access, 7:
61706-61716, 2019.
John N. Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. In M. C. Mozer, M. I. Jordan, and T. Petsche (eds.), Advances in Neural Information
Processing Systems 9, pp. 1075-1081. MIT Press, 1997.
Kyriakos G Vamvoudakis. Q-learning for continuous-time linear systems: A model-free infinite
horizon optimal control approach. Systems & Control Letters, 100:14-20, 2017.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-
learning. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
10
Under review as a conference paper at ICLR 2020
Draguna Vrabie, O. Pastravanu, Murad Abu-Khalaf, and Frank L. Lewis. Adaptive optimal control
for continuous-time linear systems based on policy iteration. Automatica, 45(2):477-484, 2009.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas.
Dueling network architectures for deep reinforcement learning. In Proceedings of the 33rd
International Conference on International Conference on Machine Learning - Volume 48, ICML’16,
pp. 1995-2003. JMLR.org, 2016.
Christopher J.C.H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3-4):279-292, 1992.
Zhuora Yang, Yuchen Xie, and Zhaoran Wang. A theoretical analysis of deep Q-learning. arXiv
preprint arXiv:1901.00137, 2019.
Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep reinforce-
ment learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on
Recommender Systems, pp. 95-103, 2018.
Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and
Zhenhui Li. DRN: A deep reinforcement learning framework for news recommendation. In
Proceedings of the 2018 World Wide Web Conference on World Wide Web, pp. 167-176, 2018.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of adaptive
gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
Zhenpeng Zhou, Xiaocheng Li, and Richard N Zare. Optimizing chemical reactions with deep
reinforcement learning. ACS Central Science, 3(12):1337-1344, 2017.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for conver-
gences of Adam and RMSProp. arXiv preprint arXiv:1811.09358, 2018.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA and Q-learning
with linear function approximation. arXiv preprint arXiv:1902.02234, 2019.
11
Under review as a conference paper at ICLR 2020
Supplementary Materials
A Further Details and Results on Experiments
We discuss more details on the experiment setup and provide further results that are not included in
Section 4.
A.1 Linear Quadratic Regulator
The linear quadratic regulator (LQR) problem is of great interest for control community where Lewis
et al. applies PQL to both discrete-time problems (Lewis & Vrabie, 2009; Lewis & Vamvoudakis,
2011) and continuous-time problems (Vamvoudakis, 2017; Vrabie et al., 2009).
We empirically validate the proposed algorithms through an infinite-horizon discrete-time LQR
problem defined as
∞
minimize	J = ɪ2 (XTQxt + UTRut + 2xTNut),
π	t=0
subject to	xt+1 = Axt + But ,
where ut = π(xt).
A typical model-based solution (with known A and B) considers the problem backwards in time and
iterates a dynamic equation known as the discrete-time algebraic Riccati equation (DARE):
P =ATPA- (ATPB+N)(R+BTPB)-1(BTPA+NT)+Q,	(12)
with the cost-to-go P being positive definite. The optimal policy satisfies ut? = -K?xt with
K? = (R + BTPB)-1(NT + BTPA).	(13)
For experiments, we parameterize a quadratic Q-function with a matrix parameter H in the form of
x
T
Q(x, u; H)
u	Hux
Hxu
Huu
(14)
x
x
u
The corresponding linear policy satisfies u = -Kx, and K = Hu-u1Hux. The performance of the
learning algorithm is then evaluated at each step of iterate i with the Euclidean norm kKi - K? k2 .
A.2 Atari Games
We list detailed experiments of the 23 Atari games evaluated with the proposed algorithms in Figure 3.
All experiments are executed with the same set of two random seeds. Each task takes about 20-hour
of wall-clock time on a GPU instance. All three methods being evaluated share similar training time.
AltQ-Adam and AltQ-AdamR can be further accelerated in practice with a more memory-efficient
implementation considering the target network is not required. We keep our implementation of
proposed algorithms consistent with the DQN we are comparing against. Other techniques that are
not included in this experiment are also compatible with AltQ-Adam and AltQ-AdamR, such like
asynchronous exploration (Mnih et al., 2013) and training with decorrelated loss (Mavrin et al., 2019).
Overall, AltQ-Adam significantly increases the performance by over 100% in some of the tasks
including Asterix, BeamRider, Enduro, Gopher, etc. However, it also illustrates certain instability
with complete failure on Amidar and Assault. This is mostly caused by the sampling where we are
using a relevantly small buffer size with 10% of the common configured size in Atari games with
experience replay. Notice that those failures tend to appear when the -greedy exploration has evolved
to a certain level where the immediate policy is effectively contributing to the accumulated experience.
This potentially amplifies the biased exploration that essentially leads to the observed phenomenon.
Interstingly, AltQ-AdamR that incorporates the restart scheme resolves the problem of high variance
of average return brought by AltQ-Adam and provides a more consistent performance across the task
12
Under review as a conference paper at ICLR 2020
domain. This implies that momentum restart effectively corrects the accumulated error and stabilizes
the training process.
Figure 3: Experiment results of 23 Atari games with DQN, AltQ-Adam and AltQ-AdamR
___AItQ-Adam
---AItQ-AdamR
13
Under review as a conference paper at ICLR 2020
Task	DQN	AltQ-Adam	AltQ-AdamR
Alien	1529	∏25	1587
Amidar	269	313	551
Assault	1925	260	2097
Asteroids	1147	1394	1069
Asterix	11794	22413	17064
BeamRider	5728	10210	6458
Bowling	60	45	30
CrazyClimber	116422	102731	121770
Enduro	866	1671	1291
DemonAttack	5729	10485	9273
DoubleDunk	-14	-12	-15
FishingDerby	29.01	-4	19
Gopher	6066	16863	9508
Gravitar	316	551	518
Jamesbond	663	899	756
Pitfall	-76	-20	-7
Pong	20.68	20.79	20.74
Qbert	13453	12487	14352
Robotank	56	34	42
Seaquest	3652	6121	6624
Spaceinvaders	923	1528	1036
Tennis	-17	20	-5
Tutankham	159	194	191
Table 3: Best empirical return of 23 Atari games with DQN, AltQ-Adam and AltQ-AdamR
B Proof of Theorem 1
Different from the regret bound for AMSGrad obtained in Reddi et al. (2018), our analysis is on
the convergence rate. In fact, a slight modification of our proof also provides the convergence rate
for AMSGrad for conventional strongly convex optimization, which can be of independent interest.
Moreover, our proof avoids the theoretical error in the proof in Reddi et al. (2018) pointed out
by (Tran et al., 2019). Before proving the theorems, we first provide some useful lemmas.
Lemma 1. (ZhouetaL,2018,Lemma A.1)Let {gt,mt,Vt} for t = 1, 2,... be sequences generated
by Algorithm 3 and gt = E[gt]. UnderAssumptionl, kgtk ≤ G∞,∣∣mtk ≤ G∞,∣∣Vt∣∣ ≤ G∞.
Lemma 2. (Reddi et al., 2018, Lemma 2) Let {mt,Vt} for t = 1, 2,... be sequences generated by
Algorithm 3. Given αt, β1t, β2 as specified in Theorem l, we have
T	2	d	u T 1
XαtM2mtB ≤ (i-βι)(i -δ)√1-β2 X kg1:T，iktX 7
≤
α √1 + log T
(1 - βι)(1 - δ)√Γ->
d
X kg1:T,i k .
i=1
(15)
Lemma 3. Let at = √t and βιt = βιλt for t = 1, 2,.... Then
X包≤
αt
t=1	t
βl
α(1 - λ)2 .
(16)
Proof. The proof is based on taking the standard sum of geometric sequences.
Bit =	β1t√t
O^ at	ʌZ	ɑ
t=1	t	t=1
T
≤X
t=1
β1λt-1t	β1	1
α
α (1 - λ)
XT λt-1 - TλT	≤
β1
α(1 — λ)2
(17)
□
14
Under review as a conference paper at ICLR 2020
With the lemmas above, we are ready to prove Theorem 1. Observe that
θt+1 = πd ^ 1/4 (θt - αt^Vt 2 mt) = min ∣∣^Vt1/4 (θt - αt^Vt 2 mt - θ
Clearly Π0 V1/4 (θ?) = θ? due to Assumption 3. We start from the update of θt when t ≥ 2.
M"-"I
=∣∣πd,vj/4VV1/4 (%- θ?- αtVt 2 mt) ||
≤ ||%1/4 (θt-θ*- αtVt-1 mt)∣∣2
=∣∣%1"(θt - θ?)∣∣2 + ∣∣αt匕T/4mt『-2αt(θt - θ*)Tmt
=∣∣^Vt1/4 (θt —	θ?) ∣∣ + ∣∣αt Vt 1/4 mt ∣∣ -	2αt(θt — e*)T (IeItmt-ι +(I—βιt)gt)
≤	∣∣V1∕4(θt —	θ?)∣∣2 + ∣∣αth-"mt∣∣2 +	atβιt (V ∣∣V1∕4(θt — θ?)∣∣2 +	at ∣%τ∕4mt-i
—	2at(1 — βιt)(θt — θ*)τgt
≤) ∣∣vj4(θt - θ*)∣∣2 + ∣∣at %τ∕4mt∣∣2+βιt ∣∣vv4(θt - θ? )∣∣2+a2βιt ∣ 忆 Y4mt-ι∣∣2
—	2at(1 — βιt)(θt — θ*)τgt,
where (i) follows from the Cauchy-Schwarz inequality, and (ii) holds because Vt+ι,i ≥ Vt,i, ∀t, ∀i.
Next, we take the expectation over all samples used up to time step t on both sides, which still
preserves the inequality. Since we consider i.i.d. sampling case, by letting Ft be the filtration of all
the sampling up to time t, we have
E [(θt — θ*)Tgt] = E [E [(θt — θ*)Tgt] ∣Ft-ι] = E [(θt — θ*)T@].
(18)
Thus we have
E∣^4(θt+ι - θ?))2
≤ E ∣∣V1∕4(θt - θ*)∣∣2 + atE ∣忖τ∕4mt∣∣2 + βιtE ∣∣Vv4(θt - θ?)∣∣2 + α2βιtE ∣∣^4mt-i∣∣2
— 2at(1 — βιt)E [(θt — θ*)Tgt]
=E ∣%"(θt - θ?)∣∣2 + a22E ∣忖τ∕4mt∣∣2 + βιtE ∣∣Vv4(θt - θ?)∣∣2 + a2βιtE ∣∣^4mt-i∣∣2
— 2at(1 — βιt)E[(θt-θ*)T gt]
≤)E∣∣V1∕4(θt-θ? )∣∣2 +a2E∣%τ∕4mt∣∣2 + βιtE∣∣Vt1∕4(θt-θ*)∣∣2 +a2βιtE∣ 忆 Y4 mt-i∣∣2
— 2atc(1 — βιt)E kθt — θ*k2
≤ E 忖∕4(θt - θ*)∣∣2 + a2E ∣∣%τ∕4mt∣∣2 + βιtE 忖∕4(θt - θ*)∣∣2 + a2βιE 归二Y4m-1『
—	2atc(1 — βι)E kθt — θ*k2
≤ E ∣∣V1∕4(θt - θ*)∣∣2 + atE ∣忖τ∕4mt∣∣2 + G∞D∞βιt + a2βιE ∣∣^4mt-i∣∣2
—	2atc(1 — β1)E kθt — θ*k2 ,
where (i) follows from Equation (18), (ii) follows due to Assumption 2 and 1 — β1t > 0, (iii)
follows from βιt < βι < 1 and E∣∣θt — θ*∣∣2 > 0, and (iv) follows from ∣∣Vt1"(θt — θ*)∣∣ ≤
∣∣ Vt1" ∣∣ l∣θt — θ*∣∣2 ≤ G∞D∞ by Lemma 1 and Assumption 3. We note that (iii) is the key step to
15
Under review as a conference paper at ICLR 2020
avoid the error in the proof in Reddi et al. (2018), where we can directly bound 1 - β1t, which is
impossible in Reddi et al. (2018). By rearranging the terms in the above inequality and taking the
summation over time steps, we have
T
2c(1- βι) XE∣∣θt-θ*k2
t=2
T
≤x a (E
T
+ XαtE
t=2
T
≤ X ɪ (E
t=2	t
T
+ XαtE
t=2
≤X α (E
t=2	t
T
恒"B-*)∣∣ -EI恒/4(θt+ι-θ*)∣∣)+X
t=2
T
VtT4mt∣I + X αtβιE∣ 忆 1/4mt-i||
t=2
T
忖"B-*)|| -EIvt1/4R+ι-θ*)∣∣ )+ X
t=2
T
VtT/4mt∣I + X αt-iβιE∣∣%-Y4mt-i∣∣
t=2
T
恒/4(θt-θ*)∣∣ -E∣∣%1/4(θt+ι-θ*)∣∣ )+X
t=2
β1tG∞D∞2
αt
β1tG∞D∞2
αt
β1tG∞D∞2
αt
T2
+ (1 + βι) X αtE∣∣%τ∕4mt∣∣ ,
t=1
where (i) follows from αt < αt-1. With further adjustment of the first term in the right hand side of
the last inequality, we can then bound the sum as
T
2c(1-β1)XEkθt-θ*k2
t=2
TT
≤XαE (W4(θt-θ*)∣∣ - ∣恒/4(θt+ι-θ*)∣∣)+X
β1tG∞D∞2
αt
T2
+ (1+ βι) XαtE∣忖τ∕4mt∣∣
t=1
E∣∣V1∕4(θ2-θ*)∣∣2 T
—-----------+ TE
α2
2	t=3
(忖 ∕4(θt-θ*)∣∣2	∣∣%γ4(θt-θ*)∣∣2
----------------------------------
αt
αt-1
\
—
E∣∣V⅛∕4(θτ +1-Θ?)∣∣2
αT
+ X βltG∞D∞ +(1 + βι) X atE ∣∣VT1∕4mt
t=2	αt	t=1
EHV1∕4(θ2 -θ?)∣2 + X E (Pd= v1∕2(θt,i-θ*)2 - Pd=I v1-1,i(θt,i-θ*)2
α2	t=3
-E 归"+1- "U + X — + (1 + βι) X atE Wι∕4mt∣∣2
t=2	αt	t=1
αt
αt-1
αt
αT
α2
Td
+ XX E(θt,i - θi*)2
(霍-工
∖ at	at-i
E∣∣V⅛∕4(θτ +1-Θ?)∣∣2
αT
—
+ X β1tG∞D∞ +(1 + βι) X atE IwT∕4mt
t=2	αt	t=1
16
Under review as a conference paper at ICLR 2020
So far we just rearrange the terms in the series sum. Next, we are ready to obtain the upper bound.
T
2c(1- βι) XE∣∣θt-θ*k2
t=2
(i)
≤
α2
Td
+ D∞2 XXE
t=3 i=1
区-底
∖ at	at-i
EllV1∕4(θτ+ι-θ?)∣∣2
αT
+ X lβ"G"D +(1 + βι) X atE ∣∣%-1∕4mt
t=2 αt	t=1
α2
d V1/2 T
D∞ XEɪ + X
i=1	t=2
β1tG∞D∞2
αt
T
+ (1 + βι) X atE∣∣¼-1/4
t=1
mt ∣∣2
≤) G∞D∞ + dG∞D∞√T + βιG∞D∞
一 a?	a	a(1 — λ)2
a(1 + βι)√1 + log T
(1 - βι)(1 - δ)√Γ-W
d
X E kg1:T,i k
i=1
(19)
1/2	1/2
vv
where (i) follows from Assumption 3 and because -0i∙ > ：-1：, and (ii) follows from Lemmas 1 - 3.
Finally, applying the Jensen’s inequality yields
1T
Ekθout-θ*k2 ≤ T EEkθt-θ*k2.
t=1
(20)
We conclude our proof by further applying the bound in Equation (19) to Equation (20).
+
17
Under review as a conference paper at ICLR 2020
C Proof of Theorem 2
To prove the convergence for AltQ-AMSGradR, the major technical development beyond the proof
of Theorem 1 lies in dealing with the parameter restart. More specifically, the moment approximation
terms are reset every r steps, i.e., mkr = Vkr = 0 for k = 1,2,..., which implies θkr+ι = θkr
for k = 1, 2, . For	technical convenience, we define θ0 = θ1. Using the arguments similar
to Equation (19), in a time window that does not contain a restart (i.e. kr ≤ S ≤ (k + 1)r - 1) we
have
S
2c(1 — βι) X Ekθt-θ*k2
t=kr
≤i≤ G∞D∞
αkr+2
+ dG∞D∞ √S +
α
(1- β1)(1- δ√τw XE kgkr+1Sik 11JX_ 11
i=1	t=kr+1
Sβ
+ G∞D∞ ɪ2 +^~ + 2c(1 - β1) (Ekθkr+1- θ*k2 + E kθkr - θ?『
t=kr+2 αt
=)G∞D∞√kr+ + dG∞D∞√S +
αα
(1 - β1)(i — δ)√T-β2 XE kgkr+1Sik t 上 1t
i=1	t=kr+1
Sβ
+ g∞d∞ ɪ2 ~~ + 4C(I — βI)Ekθkr — θ*∣∣2 ,
αt
t=kr+2	t
where (i) follows from Equation (19) and (ii) follows from θkr+1 = θkr due to the definition of
restart. Then we take the summation over the total time steps and obtain
T
2c(1 — β1) XEkθt-θ*k2
t=1
(∖T∕rC	kr-1	T
X X	Eket — e*k2 + X E kθt — θ?k2 — E kθo — θ?k2
k=1 t=(k-1)r	t=∖T /rcr
bT/rc /g D2	,_____ 、	bT/rc dG D2 ,_________
≤ X (-∞r∞ √kr+2 + 4c(1 — β1)Ekθkr — θ*k2) + X ∞ ∞ VkT-i
k=0	α	k=1 α
+ d-∞D∞√T .___α(1 + β1)_
α + (1 - β1 )(1 - 6)S-^
bT /rc d
E
g(k-1)r+1:kr-1,i
k=1 i=1
kr-1
X
t=(k-1)r+1
1
t
∖
+ (1 — β1X1 + β‰ X EET/rcfUt=bXr+1 t
bT/rc	kr-1	T
+ -∞D∞ X X 包 +-∞D∞	X 包
αt	αt
k=1 t=(k-1)r+2	t=bT /rcr+2
bT/rc (G D2	_____ 、	bT/rc
≤ E	-∞r∞√kr+2+4c(1 — β1)Ekθkr — θ*k2 + E
k=0	α	k=1
d-∞ D∞
α
+ d-∞ D∞ √t +	α(1 + β1)
ɑ 十厂而1—而『2
bT /rc d
E
g(k-1)r+1:kr-1,i
k=1 i=1
kr-1
X
t=(k-1)r+1
1
t
∖
+ (1-β1；((1+ ；：1—户2 X EugbT""娟 t t=bXr+1 t + -∞ d∞ X at .
18
Under review as a conference paper at ICLR 2020
We can bound the term G∞D∞ PT=I 普 by Lemma 3. Next, we bound another key term in the
above inequality. We first observe that ∀k ≥ 2, ∀i ∈ [d],
g(k-1)r+1:kr-1,i t
kr-1
X
t=(k-1)r+1
(i)	u
≤ g(k-1)r+1:kr-1,i t
kr-1	I	ΓΓ
X	t+ lgkr,il∖lk
t=(k-1)r+1
(ii) I	I u
≤ Ig(k-1)r+1:kr,i I t
kr
x I，
t=(k-1)r+1
(21)
1
t
where (i) holds due to ∣gt,i |，1 > 0 and (ii) follows from the CaUchy-SchWarz inequality. Then We
have
bT /r」 d	u
Ig(k-1)r+1:kr-1,i I t
k=1 i=1
kr-1
X
t=(k-1)r+1
du
+ IgbT/r」r+1:T,i I
i=1
T
X
t=bT /r」r+1
⑴ bT/r」d	T	bT/rC d
≤ EΣ>kr,il" kr +Σ £||g(kf + 1：kl,i||\
k=1 i=1	k=1 i=1
kr-1
X
t=(k-1)r+1
du
+ IgbT/r」r+1:T,i I
i=1
T
X
t=bT /r」r+1
bT/r」 d	u
ΣΣ∣Ig(k-1)r+1:kr-1,i I t
k=1 i=1
kr-1
X
t=(k-1)r+1
1 + lgkr,il Vk1r
du
+ IgbT/r」r+1:T,i I
i=1
T
X
t=bT /r」r+1
1
t
1
t
1
t
1
t
1
t
(ii) bT/rc d	u
≤	Ig(k-1)r+1:kr,i I
k=1 i=1
kr
X t+
t=(k-1)r+1
du
IgbT/r」r+1:T,i I
i=1
d	bT/rc	u
g(k-1)r+1:kr,it
i=1	k=1
kr 1	u
E t + IlgbT/r」r+1：T,i Il t
t=(k-1)r+1
t
T
X
t=bT /r」r+1
1
t
T
X
t=bT /r」r+1
1
d
T
(iii)	u T 1
≤	kg1：T，ik t t，
i=1
t=1
where (i) follows from ∣gkr,i∣ y kr, ∀k ≥ 1, ∀i ∈ [d], (ii) follows from Equation (21) and (iii) holds
due to the Cauchy-Schwarz inequality. Then we have
19
Under review as a conference paper at ICLR 2020
T
2c(1-β1) X E∣∣θt-θ*∣∣2
t=1
LT/rJ /G D2	,_____ 、	LT/rJ
≤ E	-∞r∞ √kr+2 + 4c(1 - β1)E∣∣θkr-叫2 + E
k=0 ' Q	) k = 1
dG∞d∞ √⅛r-ι
α
+dG∞D∞√T+
α(1 + βι)
LT/rJ d
EEEIlg(k-1)r:kr-1」|
k=1 i=1
T
kr-1 ι
x 1
t=(k-1)r
α(1 + β1)
T
i=1
t= ∖T∕r]r
t + G∞
+
α
(1 - βι)(i - δ)√T-^2
d
(1 - βι)(1 - δ)√Γ-12
£E	T"S∕H
LT∕rj (GD	,_____ 、	LT/rJ
≤ E ∖~d∞√ √kr + 2 + 4c(1 - β1)E∣∣θkr-叫2) + E
k=0 '	k	k=1
dG∞d∞ √⅛r-ι
α
d
T
T
+dG∞D∞√T+
α(1 + βι)
d	Tl	TR
XEkg1：T,ik t X t+-∞d∞ X U
t=1
t=1
-t
α
(1 - βι)(1 - δ)√1-12
(i)	LT∕rj /- d2	,___ 、	LT∕rj
≤ X	-∞D∞√⅛T+2 + 4c(1-β1)Ekθkr-叫2 + X
k=0 α	k=1
dG∞D∞ √⅛r-ι
α
+
dG∞D∞ √T + «(1 + βι)p d(1 + IOg T)
XiP Il Il I β1-∞D∞
NEkgLT,ik + α(1- λ)2，
α
(1 - βι)(1 - δ)√1-12
where (i) follows from Lemma 2 and Lemma 3.
Finally, applying the Jensen,s inequality and the above bound, we obtain
E∣∣%ut-θ*k2
1T
≤ T EEkθt-θ*k2
t=1
1	LT/rJ ( — d2	∖	1 LT/rJ d— n2
≤ ɪ X(	g∞d∞、√⅛r+2 + 2E∣∣θkr-θ*k2) + ɪ X	∞d∞、√⅛r-ι
≤ T k⅛ <2cα(1-β1)V	+ + k kr k + + T 士 2cα(1-β1)V
+ d-∞D∞√T + α(1+ β1)Pd(1+log T) X E k	β1-∞D∞
+ 2cα(1 - β1) + 2c(1 - β1)2(1 - δ)√1-β2 J kg1：T,ik + 2ca(1 - β1)(1 - λ)2，
which concludes the proof.
20